{"cell_type":{"f541939c":"code","e445890d":"code","888c414b":"code","34ef401a":"code","2addc172":"code","1d15841f":"code","b4956b7b":"code","6446cc56":"code","cdad9504":"code","46dc5142":"code","487f5bfb":"code","5a9c5340":"code","4526f771":"code","162e685e":"code","a7268856":"code","b030b55d":"code","659b6ed5":"code","ba8853ae":"code","e6006b3a":"code","be0993d8":"code","ce2cd80d":"code","6412a436":"code","24be6e32":"markdown","92118bca":"markdown","50cd3fc7":"markdown","c4e6c0ee":"markdown","d8666c5b":"markdown","1a236a4f":"markdown","483d4a20":"markdown","96c5cdc3":"markdown","f479013b":"markdown","cf5725a8":"markdown","98b66cf2":"markdown","cca68f46":"markdown","ea065700":"markdown","ec99363c":"markdown","28d0d1d3":"markdown","2c71901c":"markdown","57f5f3be":"markdown","522b4024":"markdown","255dca40":"markdown","46edee04":"markdown","45d8d682":"markdown","8da3ee16":"markdown","42ecac30":"markdown","4f35b6f5":"markdown","8257959d":"markdown","c4d6b8e6":"markdown","79cae453":"markdown"},"source":{"f541939c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nfrom IPython.display import HTML\n\nfrom os import listdir\nprint(listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n# Any results you write to the current directory are saved as output.","e445890d":"HTML('<iframe width=\"800\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/m_dBwwDJ4uo\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","888c414b":"train = pd.read_csv(\"..\/input\/train.csv\", nrows=10000000,\n                    dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\ntrain.head(5)","34ef401a":"train.rename({\"acoustic_data\": \"signal\", \"time_to_failure\": \"quaketime\"}, axis=\"columns\", inplace=True)\ntrain.head(5)","2addc172":"for n in range(5):\n    print(train.quaketime.values[n])","1d15841f":"fig, ax = plt.subplots(2,1, figsize=(20,12))\nax[0].plot(train.index.values, train.quaketime.values, c=\"darkred\")\nax[0].set_title(\"Quaketime of 10 Mio rows\")\nax[0].set_xlabel(\"Index\")\nax[0].set_ylabel(\"Quaketime in ms\");\nax[1].plot(train.index.values, train.signal.values, c=\"mediumseagreen\")\nax[1].set_title(\"Signal of 10 Mio rows\")\nax[1].set_xlabel(\"Index\")\nax[1].set_ylabel(\"Acoustic Signal\");","b4956b7b":"fig, ax = plt.subplots(3,1,figsize=(20,18))\nax[0].plot(train.index.values[0:50000], train.quaketime.values[0:50000], c=\"Red\")\nax[0].set_xlabel(\"Index\")\nax[0].set_ylabel(\"Time to quake\")\nax[0].set_title(\"How does the second quaketime pattern look like?\")\nax[1].plot(train.index.values[0:49999], np.diff(train.quaketime.values[0:50000]))\nax[1].set_xlabel(\"Index\")\nax[1].set_ylabel(\"Difference between quaketimes\")\nax[1].set_title(\"Are the jumps always the same?\")\nax[2].plot(train.index.values[0:4000], train.quaketime.values[0:4000])\nax[2].set_xlabel(\"Index from 0 to 4000\")\nax[2].set_ylabel(\"Quaketime\")\nax[2].set_title(\"How does the quaketime changes within the first block?\");","6446cc56":"test_path = \"..\/input\/test\/\"","cdad9504":"test_files = listdir(\"..\/input\/test\")\nprint(test_files[0:5])","46dc5142":"len(test_files)","487f5bfb":"sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample_submission.head(2)","5a9c5340":"len(sample_submission.seg_id.values)","4526f771":"fig, ax = plt.subplots(4,1, figsize=(20,25))\n\nfor n in range(4):\n    seg = pd.read_csv(test_path  + test_files[n])\n    ax[n].plot(seg.acoustic_data.values, c=\"mediumseagreen\")\n    ax[n].set_xlabel(\"Index\")\n    ax[n].set_ylabel(\"Signal\")\n    ax[n].set_ylim([-300, 300])\n    ax[n].set_title(\"Test {}\".format(test_files[n]));","162e685e":"train.describe()","a7268856":"fig, ax = plt.subplots(1,2, figsize=(20,5))\nsns.distplot(train.signal.values, ax=ax[0], color=\"Red\", bins=100, kde=False)\nax[0].set_xlabel(\"Signal\")\nax[0].set_ylabel(\"Density\")\nax[0].set_title(\"Signal distribution\")\n\nlow = train.signal.mean() - 3 * train.signal.std()\nhigh = train.signal.mean() + 3 * train.signal.std() \nsns.distplot(train.loc[(train.signal >= low) & (train.signal <= high), \"signal\"].values,\n             ax=ax[1],\n             color=\"Orange\",\n             bins=150, kde=False)\nax[1].set_xlabel(\"Signal\")\nax[1].set_ylabel(\"Density\")\nax[1].set_title(\"Signal distribution without peaks\");","b030b55d":"stepsize = np.diff(train.quaketime)\ntrain = train.drop(train.index[len(train)-1])\ntrain[\"stepsize\"] = stepsize\ntrain.head(5)","659b6ed5":"train.stepsize = train.stepsize.apply(lambda l: np.round(l, 10))","ba8853ae":"stepsize_counts = train.stepsize.value_counts()\nstepsize_counts","e6006b3a":"from sklearn.model_selection import TimeSeriesSplit\n\ncv = TimeSeriesSplit(n_splits=5)","be0993d8":"window_sizes = [10, 50, 100, 1000]\nfor window in window_sizes:\n    train[\"rolling_mean_\" + str(window)] = train.signal.rolling(window=window).mean()\n    train[\"rolling_std_\" + str(window)] = train.signal.rolling(window=window).std()","ce2cd80d":"fig, ax = plt.subplots(len(window_sizes),1,figsize=(20,6*len(window_sizes)))\n\nn = 0\nfor col in train.columns.values:\n    if \"rolling_\" in col:\n        if \"mean\" in col:\n            mean_df = train.iloc[4435000:4445000][col]\n            ax[n].plot(mean_df, label=col, color=\"mediumseagreen\")\n        if \"std\" in col:\n            std = train.iloc[4435000:4445000][col].values\n            ax[n].fill_between(mean_df.index.values,\n                               mean_df.values-std, mean_df.values+std,\n                               facecolor='lightgreen',\n                               alpha = 0.5, label=col)\n            ax[n].legend()\n            n+=1\n","6412a436":"train[\"rolling_q25\"] = train.signal.rolling(window=50).quantile(0.25)\ntrain[\"rolling_q75\"] = train.signal.rolling(window=50).quantile(0.75)\ntrain[\"rolling_q50\"] = train.signal.rolling(window=50).quantile(0.5)\ntrain[\"rolling_iqr\"] = train.rolling_q75 - train.rolling_q25\ntrain[\"rolling_min\"] = train.signal.rolling(window=50).min()\ntrain[\"rolling_max\"] = train.signal.rolling(window=50).max()\ntrain[\"rolling_skew\"] = train.signal.rolling(window=50).skew()\ntrain[\"rolling_kurt\"] = train.signal.rolling(window=50).kurt()","24be6e32":"A window size of 50 looks good enough to cover most fluctuations in sufficient detail without averaging out important signals. \n\n### Statistical features\n\nNow, that we have found a first choice for the window size, let's compute some basic rolling statistical features:\n\n* mean \n* standard deviation\n* 25% quartile\n* 50% quartile (median)\n* 75% quartile\n* interquartile range (75%-25%)\n* min\n* max\n* skewness \n* kurtosis\n\nJust for curiosity let's keep the mean and std for the window sizes we have excluded from above. Perhaps we can see later that these features were more important than the 50-size-window-features. ","92118bca":"## The end ;-) \n\nI hope my kernel has supported you in getting started with the data and perhaps it has also provided some ideas to try out. If you like it, you can make me happy with a comment and\/or an upvote. Thank you! :-)","50cd3fc7":"## Setting up a validation strategy\n\nOk, I think that is part is difiicult. In my opinion it does not make sense to split the rows of the train data randomly to obtain validation data. This is temporal data and in the test segments we probably don't have any future values given. In the description you can see that:\n\n> The training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.\n\nHmm. What does that mean for us? Let's collect some scenarios:\n\n### Scenario A\n\n* The **cycles in the train data are independent of each other**. \n* If there was already an earthquake **does not change the material** in its fault behaviors. \n* Let's suppose the **test data uses the same experimental setup**. \n\nIf this is true it's not important that we need a second (or multiple) experiment to generate test data. We could only do one experiment and cut out segments to produce different data snippets. As this does not fit well to the description of the data **this scenario is not likely**.\n\n### Scenario B\n\n* The cycles depend on each other. There is temporal correlation of future signals with past ones. \n* The **material changes somehow from cycle to cylce** and perhaps future cycles are clearly different from past ones.\n* The **test data** uses the **same experimental setup** but to prevent leakage there were **at least one new experiment** done to produce segments that may depend on each other, e.g some of them may have temporal correlations. \n* To make it more tricky there are **only some snippets given in the test data whereas others are simply dropped**. \n\nIn this case we need a model that is able to capture the temporal dependence of cycles and it should be able to make nice predictions for different cycles. As we need to make predictions for several different cycle-phases it could be fruitful to use some kind of rolling window validation. \n\n### Scenario C\n\n* The **cylces depend on each other**. There is temporal correlation of future signals with past ones.\n* The state of the **material changes from cycle to cycle**.\n* The **test data uses experimental setups that are different from the train data generation** process. In the worst case we have multiple different experiments that produce test data. \n* To make it more tricky there are **only some snippets given in the test data whereas others are simply dropped**. \n\nThis is my personal worst case and this would probably cause high differences between scores of validation and test data. \n\n### How to split now?\n\nTo start with this competition **I prefer scenario B** and I haven't done it before but there is a scikit-learn implementation that may help us now. But before we keep going on... there is one more topic that should be considered: **The test set does not contain any earthquakes. Hence it's perhaps not a good idea to include them during training and validation**. In training they would cause extreme target outliers of quaketime. Our model may always try to match its predictions with these targets and this would hinder learning for the relevent parts, for those kind of predictions we need for the test set. ","c4e6c0ee":"Still strange! **The stepsize within two quaketimes is often given by either -1, -1,1 or -2 ns**. In addition we have **stepsizes close to -1,0955 ms or -0,9955 ms** :-) Do you see it? It's something centered at -1 ms. As we have one earthquake in train the **heavy stepsize of 11.5 is the step between the earthquake and a new cycle** that goes on until the next earthquake occurs.","d8666c5b":"### Take-Away\n\n* In the first plot the we can see that **the earthquake causes very high outlier values**. This way we can't say much about the signal distribution close to zero. \n* The second plot shows the signal at the median and mean around 4. We can see that it **looks very gaussian and balanced. It seems that the signal is somehow discrete.**","1a236a4f":"How many segments do we have?","483d4a20":"## A jump into train explorations","96c5cdc3":"### Take-Away\n\nVery interesting!\n\n* The first plot shows that the quaketime seems to stay almost constant up to index 4000. Then a steep decrease occurs. Afterwards this kind of pattern is repeated.\n* The second plot reveals that the second jump of the quaketime is larger than the first.\n* The third plot shows that the quaketime within such a \"constant\" block is not really constant but linear decreasing even though with very small numbers.","f479013b":"Ok. How does the signal of the test data look like?","cf5725a8":"* We can see that the mean is shifted towards higher values due to the earthquake. In addition we can see that the 25% up to 75% quartils are looking very discrete.\n* Looking at the quaketime we can't say much about it.","98b66cf2":"Aha! We can see that they are not the same and that pandas has rounded them off. And we can see that the time seems to decrease. Let's plot the time to get more familiar with this pattern:","cca68f46":"### The stepsize\n\nBy computing the difference between to quaketimes we obtain some kind of stepsize that is probably equal within blocks and can show us the jump strength between different quaketimes. The following code computes differences first and drops the last row of train such that we can add the stepsize to the data. I think we won't loose fruitful information this way.","ea065700":"## Loading packages","ec99363c":"We can see two columns: Acoustic data and time_to_failure. The further is the seismic singal and the latter corresponds to the time until the laboratory earthquake takes place. Ok, personally I like to rename the columns as typing \"acoustic\" every time is likely for me to produce errors:","28d0d1d3":"## What is this competition all about?\n\n* Given seismic signals we are asked to predict the time until the onset of laboratory earthquakes.\n* The training data is a single sequence of signal and seems to come from one experiment alone.\n* In contrast the test data consists of several different sequences, called segments, that may correspond to different experiments. The regular pattern we might find in the train set does not match those of the test segments. \n* For each test data segment with its corresponding seg_id we are asked to predict it's single time until the lab earthquake takes place.","2c71901c":"### Test data\n\nWe can find multiple segments of sequences in the test folder. Let's peek at their names:","57f5f3be":"We can see that the quaketime of these first rows seems to be always the same. But is this really true?","522b4024":"### Take-Away\n\n* We can see only one time in 10 Mio rows when quaketime goes to 0. This is a timepoint where an earthquake in the lab occurs. \n* There are many small oscillations until a heavy peak of the signal occurs. Then it takes some time with smaller oscillations and the earthquake occurs.\n\n\nIf we take a look at the first 50000 indizes we can see that there is a second pattern of quaketime that may has something to do with the resolution of the experimental equipment:","255dca40":"## A question collection\n\nEven though there is only one signal feature and one target column, there is so much we need to understand and explore. Before we add more and more features, it's probably better to work with what is given so far. I'm afraid of feeling puzzled too fast. ;-) And to prevent that feeling even further, I like to collect the questions that draw circles in my mind:\n\n### Questions for the train set\n\n* Why do we have this low resolution jumps and why are they different? Is there some periodicity that may correlate with the signal? Would it be helpful to reconstruct that for test segments?\n* Why do we only have 16 earthquakes and such a high resolution of signal inbetween?\n\n\n### Questions for the test set\n\n* Are all segments of the test set of the same length?\n* Are they similar in their distributions or in the strength and time period between strong peaks?\n* Can we find some groups of similar test set segments?\n","46edee04":"### First conclusion\n\nThank you to @pete **who pointet out that there are 16 earthquakes in the train set and that they really occur when quaketime goes to 0**. It's still interesting why we have **several blocks and jumps of quaketime on low resolution** where the time decreases linear with a very small stepsize.\n\nCurrently I'm not sure why these jumps between our target quaketime occur between values with only small scaled differences.  **What do you think? Let's me know if you like in the commets ;-) **\n\nOk, I feel more familiar with the train data right now. Let's turn to the test data before switching to more explorations.","45d8d682":"Does this match with the number of seg_ids in the sample submission?","8da3ee16":"### The train signal distribution","42ecac30":"## What is an earthquake in the lab?\n\nCurrently I don't know how an earthquake in the laboratory works and as I like to know I googled around and found this nice video that shows how such a lab looks like. If you like, feel free to take a look at it. I'm still on my journey to understand the problem. ","4f35b6f5":"### Take-Away\n\n* These test segment examples differ a lot in the occurences of small peaks that seem to be similar to those in the train data before and after the heavy signal peak that occured some time before the lab earthquake took place. \n* They probably came from the same experiment but do neither form a continuous signal nor directly follow after the train data. ","8257959d":"## Let's get familiar with the data!\n\n### Training data\n\nThe total size of the train data is almost 9 GB and we don't want to wait too long just for a first impression, let's load only some rows: ","c4d6b8e6":"## Rolling features\n\nI sometimes get stuck in too much details that may not be neccessary or are more fun to explore by yourself. As this is just a starter, I like to continue with some ideas and visualisations that have been used in initial work of Bertrand Rouet-Leduc and the LANL-group. One of the ideas was to use features extracted by a rolling window approach. Let's do the same and make some visualisations what goes on with these features until the first lab earthquake occurs.\n\n### Window size\n\nI don't know in adcance which kind of window size would be an appropriate choice and I think it's an hyperparameter we should try to optimize. But to start, let's try out some different sizes and the mean and standard deviation to select one that may be sufficient to play around:","79cae453":"In the end we can see that the probes that are used are put under some kind of **normal pressure but there is a shear stress working on it as well**. Then, after some time, the probe splits. If you take a look at the additional material given, you can see that we have: \n\n### 3 kind of plates\n\n* 2 plates left and right that are under normal pressure: Forces are acting with 90 degree on the plate, pushing the two plates together. \n* In the middle we find a third plate which is separated by some granular material. This plate moves downwards with constant velocity. \n\nI'm not sure if I understand this right, but it seems that this granular material is the \"rock\" that can split and load again to produce this kind of lab earthquakes in repetitive cycles. Even though the train set contains continuous data it contains several such splits (earthquakes)."}}