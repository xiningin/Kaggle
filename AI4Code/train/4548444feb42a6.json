{"cell_type":{"0e463878":"code","1d3261cd":"code","b08c56cf":"code","350d82b9":"code","99abcc26":"code","a5ffd006":"code","25714790":"code","3f12733b":"code","1c3f8a06":"code","de1b2a49":"code","f9ea9e5f":"code","237da939":"code","a8270c69":"code","73c776d1":"code","105c47cd":"code","a2510fbd":"code","be39a328":"code","76979504":"code","204135f8":"code","55d49f62":"code","257b31e5":"code","46e7e07b":"code","1034d7ed":"code","0cd15437":"code","57a04ce1":"code","7d775817":"code","0dc1701a":"markdown","ecd1930b":"markdown","6b4c4d57":"markdown","28ea1237":"markdown","88cf79a2":"markdown","2a0f6fb9":"markdown","ba303f71":"markdown","36e6ef56":"markdown","14d5bb3c":"markdown","ca86e29c":"markdown","f446626d":"markdown","ca6e140d":"markdown"},"source":{"0e463878":"!git clone https:\/\/github.com\/idstcv\/GPU-Efficient-Networks.git","1d3261cd":"cd .\/GPU-Efficient-Networks","b08c56cf":"import GENet","350d82b9":"cd ..\/","99abcc26":"# Python library to interact with the file system.\nimport os\n\n# Software library written for data manipulation and analysis.\nimport pandas as pd\n\n# fastai library for computer vision tasks\nfrom fastai.vision.all import *\n\n# Python library for image augmentation\nimport albumentations as A\n","a5ffd006":"path = Path('..\/input\/cassava-leaf-disease-classification')","25714790":"train_df = pd.read_csv(path\/'train.csv')\ntrain_df","3f12733b":"train_df['image_id'] = train_df['image_id'].map(lambda x : path \/'train_images'\/x )\ntrain_df.head()","1c3f8a06":"# obtain the input images.\ndef get_x(r):\n    return r['image_id']\n\n# obtain the targets.\ndef get_y(r):\n    return r['label']","de1b2a49":"'''AlbumentationsTransform will perform different transforms over both\n   the training and validation datasets ''' \nclass AlbumentationsTransform(RandTransform):\n    \n    '''split_idx is None, which allows for us to say when we're setting our split_idx.\n       We set an order to 2 which means any resize operations are done first before our new transform. '''\n    split_idx, order = None, 2\n    \n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    # Inherit from RandTransform, allows for us to set that split_idx in our before_call.\n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    # If split_idx is 0, run the trainining augmentation, otherwise run the validation augmentation. \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","f9ea9e5f":"def get_train_aug(size): \n    \n    return A.Compose([\n            # allows to combine RandomCrop and RandomScale\n            A.RandomResizedCrop(size,size),\n            \n            # Transpose the input by swapping rows and columns.\n            A.Transpose(p=0.5),\n        \n            # Flip the input horizontally around the y-axis.\n            A.HorizontalFlip(p=0.5),\n        \n            # Flip the input horizontally around the x-axis.\n            A.VerticalFlip(p=0.5),\n        \n            # Randomly apply affine transforms: translate, scale and rotate the input.\n            A.ShiftScaleRotate(p=0.5),\n        \n            # Randomly change hue, saturation and value of the input image.\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        \n            # Randomly change brightness and contrast of the input image.\n            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        \n            # CoarseDropout of the rectangular regions in the image.\n            A.CoarseDropout(p=0.5),\n        \n            # CoarseDropout of the square regions in the image.\n            A.Cutout(p=0.5) ])\n\ndef get_valid_aug(size): \n    \n    return A.Compose([\n    # Crop the central part of the input.   \n    A.CenterCrop(size, size, p=1.),\n    \n    # Resize the input to the given height and width.    \n    A.Resize(size,size)], p=1.)","237da939":"'''The first step item_tfms resizes all the images to the same size (this happens on the CPU) \n   and then batch_tfms happens on the GPU for the entire batch of images. '''\n# Transforms we need to do for each image in the dataset\nitem_tfms = [Resize(256), AlbumentationsTransform(get_train_aug(256), get_valid_aug(256))]\n\n# Transforms that can take place on a batch of images\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]","a8270c69":"def get_data(bs=32, data_df=train_df):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       splitter=RandomSplitter(seed=42), # split data into training and validation subsets.\n                       get_x=get_x, # obtain the input images.\n                       get_y=get_y, # obtain the targets.\n                       item_tfms = item_tfms,\n                       batch_tfms = batch_tfms)\n    return dblock.dataloaders(data_df,bs=bs)\n\ndls = get_data()","73c776d1":"# We can call show_batch() to see what a sample of a batch looks like.\ndls.show_batch()","105c47cd":"model = GENet.genet_large(pretrained=True, root='..\/input\/genetparam\/')","a2510fbd":"# Group together some dls, a model, and metrics to handle training\nlearn = Learner(dls, model, metrics = accuracy) ","be39a328":"# Choosing a good learning rate\nlearn.lr_find()","76979504":"# We can use the fine_tune function to train a model with this given learning rate\nlearn.fine_tune(4, base_lr=0.0012022644514217973)","204135f8":"# Plot training and validation losses.\nlearn.recorder.plot_loss()","55d49f62":"# Interpretation methods for classification models.\ninterp = ClassificationInterpretation.from_learner(learn)\n\n# Show images in top_losses along with their prediction, actual, loss, and probability of actual class.\ninterp.plot_top_losses(5, nrows=5)","257b31e5":"sample = pd.read_csv(path\/'sample_submission.csv')\nsample","46e7e07b":"_sample = sample.copy()\n_sample['image_id'] = _sample['image_id'].map(lambda x:path\/'test_images'\/x)\ntest_dl = dls.test_dl(_sample)","1034d7ed":"_sample.head()","0cd15437":"test_dl.show_batch()","57a04ce1":"a, _ = learn.tta(dl=test_dl, n=8)\npred = a.argmax(dim=1).numpy()\nsample['label'] = pred","7d775817":"sample.to_csv('submission.csv',index=False)","0dc1701a":"The Albumentation code has been borrowed from Fastai [docs](https:\/\/docs.fast.ai\/tutorial.albumentations.html). It's very common to use different transforms on the training dataset versus the validation dataset. Lets see how!","ecd1930b":"# Load training data","6b4c4d57":"# Test Time Augmentation (TTA)\nSimilar to what Data Augmentation is doing to the training set, the purpose of Test Time Augmentation is to perform random modifications to the test images. Thus, instead of showing the regular, \u201cclean\u201d images, only once to the trained model, we will show it the augmented images several times. We will then average the predictions of each corresponding image and take that as our final guess.\n\nThe reason why it works is that, by averaging our predictions, on randomly modified images, we are also averaging the errors. The error can be big in a single vector, leading to a wrong answer, but when averaged, only the correct answer stand out.","28ea1237":"# Model Definition\n\n* There are three pre-trained models, GENet-large\/normal\/small.\n* GENet_large (31M param)\n* GENet_normal (21M param)\n* GENet_small (8.17M)\n\n    * GENet-large\/normal\/small use different input image resolutions.\n    * GENet-large, size = 256\n    * GENet-normal, size = 192\n    * GENet-small, size = 192\n    ","88cf79a2":"# Load GENets","2a0f6fb9":"# Import Libraries","ba303f71":"# GENet : GPU Efficient Network\n\n![](https:\/\/raw.githubusercontent.com\/idstcv\/GPU-Efficient-Networks\/master\/misc\/genet_acc_speed_curve.jpg)\n\nThe proposed design space is optimized for fast GPU inference. In this space, it uses a semi-automatic NAS to\nhelp us design GPU-Efficient Networks. GENets use full convolutions in low-level stages and depth-wise\nconvolution and\/or bottleneck structure in high-level stages. \n\nThis design is inspired by the observation that convolutional kernels in the high-level stages are more likely to have low intrinsic rank and different types of convolutions have different kinds of efficiency on GPU.","36e6ef56":"### 85% accuracy in 5 epochs! Not Bad!!","14d5bb3c":"# Make Submission file","ca86e29c":"# Create Dataloaders","f446626d":"## *Upvote the kernel if you found it insightful!*","ca6e140d":"# Albumentations\nAlbumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.\n\n* Albumentations supports all common computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, and pose estimation.\n* The library provides a simple unified API to work with all data types: images (RBG-images, grayscale images, multispectral images), segmentation masks, bounding boxes, and keypoints.\n* The library contains more than 70 different augmentations to generate new training samples from the existing data.\n* Albumentations is fast.\n\n* Installation:- pip install -U albumentations"}}