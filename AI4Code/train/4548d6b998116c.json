{"cell_type":{"154e9dc6":"code","f5d1e56f":"code","2f86762a":"code","89c0c0c0":"code","13450adb":"code","0d2ab089":"code","858908f9":"code","6a22078f":"code","9acb92e0":"code","39f5489a":"code","a41cc8e4":"code","0cf649ca":"code","86bc7749":"code","2e8779a3":"code","5e823882":"code","249b644b":"code","c4acec65":"code","b2ddfce6":"code","94271d92":"code","0fb05317":"code","5c91d1b5":"code","ed33e805":"code","e72091bb":"code","5b39562f":"code","95315b2c":"code","aeec3a63":"code","16891ae9":"code","5a57c38e":"code","b6bb3605":"code","7b1f1f49":"code","2bba6bc1":"code","5d124a0d":"code","8003cc49":"code","11ee1d33":"code","83bbe679":"code","bbacdb7b":"code","74339915":"code","60eb5edf":"markdown","9f70ebcd":"markdown","cb08933b":"markdown","2c9bc7a5":"markdown","97e35134":"markdown","cdb85829":"markdown","f4bf215a":"markdown","63fd9060":"markdown","fa726a06":"markdown","c0cf097e":"markdown","161004b5":"markdown","366733ee":"markdown","95c5ffef":"markdown","d6ba7701":"markdown"},"source":{"154e9dc6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","f5d1e56f":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","2f86762a":"train.head()","89c0c0c0":"trainY = train.median_house_value \ntrain = train.drop(columns=[\"median_house_value\"])\ntrain.shape","13450adb":"train[\"persons\/room\"] = train[\"population\"]\/train[\"total_rooms\"]\ntest[\"persons\/room\"] = test[\"population\"]\/test[\"total_rooms\"]\ntrain[\"persons\/room\"]","0d2ab089":"train[\"persons\/bedroom\"] = train[\"population\"]\/train[\"total_bedrooms\"]\ntest[\"persons\/bedroom\"] = test[\"population\"]\/test[\"total_bedrooms\"]\ntrain[\"persons\/bedroom\"]","858908f9":"train.hist(bins=200, figsize=(25,20))","6a22078f":"train = train.drop(columns=[\"Id\"])","9acb92e0":"plt.xlabel('Persons per Room')\ntrain[\"persons\/room\"].hist(bins=200, range=(0,5))\ntest[\"persons\/room\"].hist(bins=200, range=(0,5))","39f5489a":"plt.xlabel('Persons per Bedroom')\ntrain[\"persons\/bedroom\"].hist(bins=200, range=(0,10))\ntest[\"persons\/bedroom\"].hist(bins=200, range=(0,10))","a41cc8e4":"import seaborn\nplt.figure(figsize=(10,10))\nplt.title(\"Matriz de correla\u00e7\u00e3o\")\nseaborn.heatmap(train.corr(), annot=True, linewidths=0.2)","0cf649ca":"train = train.drop(columns=[\"persons\/room\"])\ntest = test.drop(columns=[\"persons\/bedroom\"])","86bc7749":"from sklearn import tree\nregression_tree = tree.DecisionTreeRegressor(criterion='mse', splitter='best', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,  min_impurity_decrease=0.0, presort=True)\nmodel = regression_tree.fit(train, trainY)\nregression_tree.predict(train)","2e8779a3":"# from IPython.display import Image  \n# from sklearn import tree\n# import pydotplus\n\n# dot_data = tree.export_graphviz(regression_tree, out_file=None, \n#                                 feature_names=train.columns)\n\n# graph = pydotplus.graph_from_dot_data(dot_data)  \n\n# # Mostrar grafo e salvar um pdf\n# graph.write_pdf(\"Tree.pdf\")\n# Image(graph.create_png())","5e823882":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.show()","249b644b":"plt.figure(figsize=(12,8))\nplot_learning_curve(regression_tree,\"Regression Tree Learning Curve\", train, trainY)","c4acec65":"regression_tree = tree.DecisionTreeRegressor(criterion='mse', splitter='best', min_samples_split=2, min_samples_leaf=1, max_depth=7, min_weight_fraction_leaf=0.0,  min_impurity_decrease=0.0, presort=True)\nmodel = regression_tree.fit(train, trainY)\nregression_tree.predict(train)","b2ddfce6":"plt.figure(figsize=(12,8))\nplot_learning_curve(regression_tree,\"Regression Tree Learning Curve\", train, trainY)","94271d92":"regression_tree.score(train, trainY)","0fb05317":"from sklearn.linear_model import LinearRegression\nLR = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\nLR.fit(train, trainY)\nLR.score(train, trainY)","5c91d1b5":"plt.figure(figsize=(12,8))\nplot_learning_curve(LR,\"Linear Regression Learning Curve\", train, trainY)","ed33e805":"testId = test[\"Id\"]\ntest = test.drop(columns=[\"Id\"])\npredictions = LR.predict(test)\n#predictions.to_csv('predictions.csv')\npredictions","e72091bb":"predict = pd.DataFrame(index=testId)\npredict[\"median_house_value\"] = predictions\npredict.to_csv('predictions.csv')","5b39562f":"from sklearn.ensemble import AdaBoostRegressor\nboost = AdaBoostRegressor(base_estimator=LR, n_estimators=100, learning_rate=1.0, loss='linear', random_state=None)\nboost.fit(train, trainY)\nboost.score(train, trainY)","95315b2c":"plt.figure(figsize=(12,8))\nplot_learning_curve(boost,\"AdaBoost Learning Curve\", train, trainY)","aeec3a63":"predictions = boost.predict(test)\npredict = pd.DataFrame(index=testId)\npredict[\"median_house_value\"] = predictions\npredict.to_csv('predictions.csv')","16891ae9":"from sklearn import neural_network\n\nneural_net = neural_network.MLPRegressor(hidden_layer_sizes=(100,),\n                                       activation='relu', solver='adam',\n                                       learning_rate='adaptive', max_iter=800,\n                                       learning_rate_init=0.01, warm_start = True, alpha=0.01)\nneural_net.fit(train, trainY)\n\nprint (\"Neural Net score: \", str(neural_net.score(train, trainY)*100), \"%\")\nneural_net","5a57c38e":"plt.figure(figsize=(12,8))\nplot_learning_curve(neural_net,\"Neural Network Learning Curve\", train, trainY) #62","b6bb3605":"from catboost import CatBoostRegressor\n\nregressor = CatBoostRegressor(loss_function='RMSE')\nregressor.fit(train, trainY)\nregressor.score(train, trainY)","7b1f1f49":"from sklearn.ensemble import RandomForestRegressor\nnum=100\nforest = RandomForestRegressor(n_estimators=num, criterion='mse', min_samples_split=5, \n                      min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_features=\"sqrt\", \n                     random_state=None, verbose=0, warm_start=True)\nforest.fit(train, trainY)\nprint (forest.score(train, trainY))\npredictions = forest.predict(test)","2bba6bc1":"plt.figure(figsize=(12,8))\nplot_learning_curve(forest,\"Random Forest Learning Curve\", train, trainY)","5d124a0d":"from sklearn.ensemble import GradientBoostingRegressor\nboosting = GradientBoostingRegressor()\n# boosting = GradientBoostingClassifier(loss='deviance', learning_rate=0.02, n_estimators=100, subsample=1.0, \n#                                       criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, \n#                                       min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n#                                       min_impurity_split=None, init=forest, random_state=None, max_features=None,\n#                                       verbose=0, max_leaf_nodes=None, warm_start=True, presort='auto',\n#                                       validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n\n\n\nboosting.fit(train, trainY)\nprint (boosting.score(train, trainY))\n\npredictions = boosting.predict(test)","8003cc49":"plt.figure(figsize=(12,8))\nplot_learning_curve(boosting,\"Gradient Boosting Learning Curve\", train, trainY)","11ee1d33":"predict = pd.DataFrame(index=testId)\npredict[\"median_house_value\"] = predictions\npredict.to_csv('predictions.csv')","83bbe679":"from sklearn.ensemble import AdaBoostRegressor\nadaboost1 = AdaBoostRegressor(base_estimator=neural_net, n_estimators=10, learning_rate=0.01, random_state=None)\nadaboost2 = AdaBoostRegressor(base_estimator=forest, n_estimators=10, learning_rate=0.01, random_state=None)\n\n\nadaboost1.fit(train, trainY)\nadaboost2.fit(train, trainY)\n\nprint (\"AdaBoost on Neural Network score: \", str(adaboost1.score(train, trainY)*100), \"%\")\nprint (\"AdaBoost on Random Forest score: \", str(adaboost2.score(train, trainY)*100), \"%\")","bbacdb7b":"predictions = adaboost2.predict(test)\npredict = pd.DataFrame(index=testId)\npredict[\"median_house_value\"] = predictions\npredict.to_csv('predictions.csv')","74339915":"predictions = neural_net.predict(test)\nprint(predictions)\npredict = pd.DataFrame(index=testId)\npredict[\"median_house_value\"] = predictions\npredict.to_csv('predictions.csv')","60eb5edf":"# Boosting:\n\n* Gradient Boosting Regressor:","9f70ebcd":"Com a an\u00e1lise dos histogramas, fica evidente que a *feature* **Id** n\u00e3o apresenta relev\u00e2ncia alguma na regress\u00e3o, portanto esta ser\u00e1 removida","cb08933b":"# Random Forest:","2c9bc7a5":"# Atividade 3 - California House Pricing\n### Atividade a ser entregue \u00e0 disciplina PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es ","97e35134":"# Gerando Modelos de Regressor:\n\n## \u00c1rvore de Regress\u00e3o","cdb85829":"# Submiss\u00e3o do arquivo gerado pelo classificador com melhor performance:","f4bf215a":"## Neural Network: \nDentre os par\u00e2metros passados na cria\u00e7\u00e3o da rede neural, temos:\n* Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: Fun\u00e7\u00e3o Logistica (sigmoide)\n* Otimiza\u00e7\u00e3o dos Pesos pela t\u00e9cnica de Gradiente Estoc\u00e1stico Descendente\n* Expoente para diminuir taxa de aprendizado \"power_t\"\n* max_iter: for stochastic solvers, is the number of epochs (how many times each data point will be used), not the number of gradient steps\n* shuffle: Whether to shuffle samples in each iteration. Only used when solver=\u2019sgd\u2019 or \u2018adam\u2019.\n* random_state: inicializa\u00e7\u00e3o dos pesos e bias\n* momentum: Momentum for gradient descent update\n","63fd9060":"### Em nosso Dataset, temos os seguintes features:\n* Id - Identifica\u00e7\u00e3o dos locais\n* latitude: Latitude do local (em graus)\n* longitude: Longitude do local\n* median_age: Mediana das idades dos im\u00f3veis no local\n* total_rooms: Contagem do n\u00famero de c\u00f4modos das casas na regi\u00e3o\n* total_bedrooms: Contagem do total de quartos das casas na regi\u00e3o\n* population: Popula\u00e7\u00e3o na regi\u00e3o\n* households: N\u00famero total de casas na regi\u00e3o\n* median_income: Mediana da renda das pessoas na regi\u00e3o\n* median_house_value: Vari\u00e1vel Target.\n\n#### Al\u00e9m dessas features, uma an\u00e1lise qualitativa da situa\u00e7\u00e3o faz com que novas features como **\"persons\/room\" = population\/total_rooms** e **\"persons\/bedroom\" = population\/total_bedrooms** possam ser \u00fateis.  Como ser\u00e1 explorado abaixo na Engenharia de Features\n\n## Engenharia de Features:","fa726a06":"Com a an\u00e1lise da matriz de correla\u00e7\u00e3o, percebe-se que a *feature* **persons\/room** est\u00e1 extremamente relacionada \u00e0 **persons\/bedroom**, portanto manter as duas n\u00e3o \u00e9 de muita ultilidade","c0cf097e":"A curva acima evidencia um **overffitting** extremo do regressor! Uma poss\u00edvel solu\u00e7\u00e3o \u00e9 diminuir a profundidade da \u00e1rvore de regress\u00e3o para um classificador mais adequado.","161004b5":"* AdaBoost Regressor:","366733ee":"### Regress\u00e3o Linear","95c5ffef":"<img src=\"http:\/\/www.imagens.usp.br\/wp-content\/uploads\/EP.jpg\" alt=\"logo\" width=\"500\"\/>","d6ba7701":"## An\u00e1lise de Features:\n####     Abaixo ser\u00e1 realizada uma an\u00e1lise qualitativa da distribui\u00e7\u00e3o das features na base com histogramas e a Matriz de Correla\u00e7\u00e3o"}}