{"cell_type":{"4c836d11":"code","fa75f425":"code","20e601ec":"code","c22b4bd8":"code","aff37ac6":"code","a05dea07":"code","05f222a0":"code","f30d8766":"code","c2103f01":"code","a66ef0b9":"code","9e337a37":"code","c7f15fae":"code","296765a6":"code","9d0db9d2":"code","9e21804b":"code","f2899656":"code","61208c70":"code","289d6ee5":"code","91f5ded1":"code","1376be8a":"code","a7533f88":"code","aace4a39":"code","ef7739af":"code","0e133b47":"code","8d2f666c":"code","e8c8add3":"code","85f160e4":"code","59690298":"code","b5bb07f8":"code","47b83b26":"code","c6b10445":"code","28e746af":"code","50077a37":"code","b4f9e4bc":"code","c2632018":"code","6016efe8":"code","11c8c487":"code","80835dac":"markdown","c1e1fe3c":"markdown","ee9dc34d":"markdown","e6d7f351":"markdown","f165b000":"markdown","713cc044":"markdown","5ef10a11":"markdown","515c4327":"markdown","1049c024":"markdown","1312a319":"markdown","7a226046":"markdown","a0dfff05":"markdown","4fbab3c0":"markdown"},"source":{"4c836d11":"import pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata1 = pd.read_csv('..\/input\/ramen-ratings\/ramen-ratings.csv')\n","fa75f425":"data1.head()","20e601ec":"data1.shape","c22b4bd8":"data1.info()","aff37ac6":"'''Missing Value Chart'''\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(13, 3))\ndata1.isnull().mean(axis=0).plot.barh()\nplt.title(\"Ratio of missing values per columns\")","a05dea07":"'''Unique Columns'''\n\ndef unique_counts(data):\n   for i in data.columns:\n       count = data[i].nunique()\n       print(i, \": \", count)\nunique_counts(data1)","05f222a0":"data1['Top Ten'].unique()","f30d8766":"'''Handling Missing Values'''\n\nvar = [\"Top Ten\"]\nfor i in var:\n    data1[i].fillna(0, inplace=True)\n    \ndata1.dropna(inplace=True) #removing all missing values","c2103f01":"import plotly.express as px\n\ncustom_aggregation = {}\ncustom_aggregation[\"Style\"] = \"count\"\ndata2 = data1.groupby(\"Style\").agg(custom_aggregation)\n\ndata2.columns = [\"Count\"]\ndata2['Style'] = data2.index\n\nfig = px.bar(data2, x='Style', y=\"Count\", color=\"Style\", title=\"Number of Ramen Style\")\nfig.show()","a66ef0b9":"fig = px.box(data1, x=\"Style\", y=\"Review #\", color=\"Style\", boxmode=\"overlay\", title=\"Style x Review Boxplot\")\nfig.update_traces(quartilemethod=\"inclusive\")\nfig.show()","9e337a37":"custom_aggregation = {}\ncustom_aggregation[\"Country\"] = \"count\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\n\ndata2.columns = [\"Count\"]\ndata2['Country'] = data2.index\n\nfig = px.bar(data2, x='Country', y=\"Count\", color=\"Country\", title=\"Number of Country\")\nfig.show()","c7f15fae":"data1['Stars'][data1['Stars'] == 'Unrated'] = 0\ndata1['Stars'] = data1['Stars'].astype(float)\n\ncustom_aggregation = {}\ncustom_aggregation[\"Stars\"] = \"mean\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\n\ndata2.columns = [\"Stars\"]\ndata2['Country'] = data2.index\ndata2['Stars'] = data2['Stars'].round(decimals=1)\n\nfig = px.bar(data2, x='Country', y=\"Stars\", color=\"Country\", title=\"Stars Rating in Each Country\")\nfig.show()","296765a6":"custom_aggregation = {}\ncustom_aggregation[\"Variety\"] = \"count\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\n\ndata2.columns = [\"Count\"]\ndata2['Country'] = data2.index\n\nfig = px.bar(data2, x='Country', y=\"Count\", color=\"Country\", title=\"Ramen Variety in Each Country\")\nfig.show()","9d0db9d2":"'''Making TF-IDV'''\n\nimport nltk, warnings\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom string import digits, punctuation\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\nX = data1[\"Variety\"].unique()\nstemmer = nltk.stem.porter.PorterStemmer()\nstopword = nltk.corpus.stopwords.words('english')\n\ndef stem_and_filter(doc):\n    tokens = [stemmer.stem(w) for w in analyzer(doc)]\n    return [token for token in tokens if token.isalpha()]\n\nanalyzer = TfidfVectorizer().build_analyzer()\nCV = TfidfVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter, min_df=0.00, max_df=0.3)  # we remove words if it appears in more than 30 % of the corpus (not found stopwords like Box, Christmas and so on)\nTF_IDF_matrix = CV.fit_transform(X)\nprint(\"TF_IDF_matrix :\", TF_IDF_matrix.shape, \"of\", TF_IDF_matrix.dtype)","9e21804b":"'''TF-IDV Embedded'''\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\n\nsvd = TruncatedSVD(n_components = 100)\nnormalizer = Normalizer(copy=False)\nTF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\nTF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\nprint(\"TF_IDF_embedded :\", TF_IDF_embedded.shape, \"of\", TF_IDF_embedded.dtype)","f2899656":"'''Silhoutte Scoring'''\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\n\nscore_tfidf = []\nx = list(range(5, 155, 10))\n\nfor n_clusters in x:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n    kmeans.fit(TF_IDF_embedded)\n    clusters = kmeans.predict(TF_IDF_embedded)\n    silhouette_avg = silhouette_score(TF_IDF_embedded, clusters)\n    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n    score_tfidf.append(silhouette_avg)\n    \nplt.figure(figsize=(20,16))\nplt.subplot(2, 1, 1)\nplt.plot(x, score_tfidf, label=\"TF-IDF matrix\")\nplt.title(\"Evolution of the Silhouette Score\")\nplt.legend()","61208c70":"'''KMeans Clustering'''\n\nn_clusters = 70\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=0)\nproj = kmeans.fit_transform(TF_IDF_embedded)\nclusters = kmeans.predict(TF_IDF_embedded)\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"ACP with 135 clusters\", fontsize=\"20\")","289d6ee5":"'''TSNE Visualization'''\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\nproj = tsne.fit_transform(TF_IDF_embedded)\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"Visualization of the clustering with TSNE\", fontsize=\"20\")","91f5ded1":"'''WordClouding'''\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport random\n\nplt.figure(figsize=(20,8))\nwc = WordCloud()\n\nfor num, cluster in enumerate(random.sample(range(60), 12)) :\n    plt.subplot(3, 4, num+1)\n    wc.generate(\" \".join(X[np.where(clusters==cluster)]))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(\"Cluster {}\".format(cluster))\n    plt.axis(\"off\")\n    \nplt.figure()","1376be8a":"'''Clustering Using KMeans from Selected Feature'''\n\nfeature = ['Review #', 'Stars']\ncls_ = data1[feature]\n\nfrom sklearn.cluster import KMeans\nnc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in nc]\nscore = [kmeans[i].fit(cls_).score(cls_) for i in range(len(kmeans))]\nplt.plot(nc,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","a7533f88":"kmeans = KMeans(n_clusters=4, random_state=0).fit(cls_)\ncls_['Cluster'] = kmeans.labels_\ndata1 = pd.merge(data1,cls_)","aace4a39":"dict_article_to_cluster = {article : cluster for article, cluster in zip(X, clusters)}\ncluster = data1['Variety'].apply(lambda x : dict_article_to_cluster[x])","ef7739af":"data1 = pd.concat([data1,cluster], axis=1)","0e133b47":"data1.columns = [\"Review\", \"Brand\", \"Variety\", \"Style\", 'Country', 'Stars', 'Top Ten', 'Cluster', 'Label'] #changing name columns\ndata1['Review'] = data1['Review'].astype(int)","8d2f666c":"data1.head()","e8c8add3":"cls_0 = data1[data1['Cluster']==0]\nprint(f'Mean stars rating in Cluster 0 : {round(cls_0.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 0 : {round(cls_0.Review.mean(),2)}')\nprint(cls_0.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_0.groupby('Label').size().sort_values(ascending=False)[0:4])","85f160e4":"\nwc = WordCloud()\n\nimg1 = cls_0.loc[cls_0['Label'] == 15]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_0.loc[cls_0['Label'] == 4]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_0.loc[cls_0['Label'] == 53]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_0.loc[cls_0['Label'] == 10]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","59690298":"cls_1 = data1[data1['Cluster']==1]\nprint(f'Mean stars rating in Cluster 1 : {round(cls_1.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 1 : {round(cls_1.Review.mean(),2)}')\nprint(cls_1.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_1.groupby('Label').size().sort_values(ascending=False)[0:4])","b5bb07f8":"\nimg1 = cls_1.loc[cls_1['Label'] == 47]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_1.loc[cls_1['Label'] == 38]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_1.loc[cls_1['Label'] == 39]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_1.loc[cls_1['Label'] == 2]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","47b83b26":"cls_2 = data1[data1['Cluster']==2]\nprint(f'Mean stars rating in Cluster 2 : {round(cls_2.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 2 : {round(cls_2.Review.mean(),2)}')\nprint(cls_2.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_2.groupby('Label').size().sort_values(ascending=False)[0:4])","c6b10445":"\nimg1 = cls_2.loc[cls_2['Label'] == 23]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_2.loc[cls_2['Label'] == 54]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_2.loc[cls_2['Label'] == 15]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_2.loc[cls_2['Label'] == 39]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","28e746af":"cls_3 = data1[data1['Cluster']==3]\nprint(f'Mean stars rating in Cluster 3 : {round(cls_3.Stars.mean(),2)}')\nprint(f'Mean review in Cluster 3 : {round(cls_3.Review.mean(),2)}')\nprint(cls_3.groupby('Stars').size()[0:3].sort_values(ascending=False))\nprint(cls_3.groupby('Label').size().sort_values(ascending=False)[0:4])","50077a37":"\nimg1 = cls_3.loc[cls_3['Label'] == 26]\ntext1 = str(img1.Variety)\nwordcloud1 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text1)\n\nimg2 = cls_3.loc[cls_3['Label'] == 39]\ntext2 = str(img2.Variety)\nwordcloud2 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text2)\n\nimg3 = cls_3.loc[cls_3['Label'] == 30]\ntext3 = str(img3.Variety)\nwordcloud3 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text3)\n\nimg4 = cls_3.loc[cls_3['Label'] == 49]\ntext4 = str(img4.Variety)\nwordcloud4 = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text4)\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(50,20))\nax1.imshow(wordcloud1, interpolation=\"bilinear\")\nax2.imshow(wordcloud2, interpolation=\"bilinear\")\nax3.imshow(wordcloud3, interpolation=\"bilinear\")\nax4.imshow(wordcloud4, interpolation=\"bilinear\")\n\nax1.axis(\"off\")\nax2.axis(\"off\")\nax3.axis(\"off\")\nax4.axis(\"off\")","b4f9e4bc":"fig = px.box(data1, x=\"Cluster\", y=\"Review\", color=\"Cluster\", boxmode=\"overlay\", title=\"Review Boxplot by Cluster\")\nfig.update_traces(quartilemethod=\"inclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","c2632018":"fig = px.box(data1, x=\"Cluster\", y=\"Stars\", color=\"Cluster\", boxmode=\"overlay\", title=\"Review Boxplot by Cluster\")\nfig.update_traces(quartilemethod=\"inclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","6016efe8":"import plotly.graph_objs as go\nimport warnings\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")\n\ncustom_aggregation = {}\ncustom_aggregation[\"Stars\"] = \"mean\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\ndata2.columns = [\"Stars\"]\n\ntemp = data2\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\n\ndata = dict(type='choropleth',\n            locations = countries.index,\n            locationmode = 'country names', \n            z = data2['Stars'],\n            text = countries.index, \n            colorbar = {'title':'Rating'},\n            colorscale=[\n            [0, \"rgb(8, 29, 88)\"], \n            [0.125, \"rgb(37, 52, 148)\"], \n            [0.25, \"rgb(34, 94, 168)\"], \n            [0.375, \"rgb(29, 145, 192)\"], \n            [0.5, \"rgb(65, 182, 196)\"], \n            [0.625, \"rgb(127, 205, 187)\"], \n            [0.75, \"rgb(199, 233, 180)\"], \n            [0.875, \"rgb(237, 248, 217)\"], \n            [1, \"rgb(255, 255, 217)\"]],    \n\n            reversescale = False)\n\nlayout = dict(title='Ramen Rating per Country',\n\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n\nchoromap = go.Figure(data = [data], layout = layout)\n\niplot(choromap, validate=False)","11c8c487":"custom_aggregation = {}\ncustom_aggregation[\"Review\"] = \"mean\"\ndata2 = data1.groupby(\"Country\").agg(custom_aggregation)\ndata2.columns = [\"Review\"]\n\ntemp = data2\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\n\ndata = dict(type='choropleth',\n            locations = countries.index,\n            locationmode = 'country names', \n            z = data2['Review'],\n            text = countries.index, \n            colorbar = {'title':'Nb. of Review'},\n            colorscale=[\n            [0, \"rgb(8, 29, 88)\"], \n            [0.125, \"rgb(37, 52, 148)\"], \n            [0.25, \"rgb(34, 94, 168)\"], \n            [0.375, \"rgb(29, 145, 192)\"], \n            [0.5, \"rgb(65, 182, 196)\"], \n            [0.625, \"rgb(127, 205, 187)\"], \n            [0.75, \"rgb(199, 233, 180)\"], \n            [0.875, \"rgb(237, 248, 217)\"], \n            [1, \"rgb(255, 255, 217)\"]],    \n            reversescale = False)\n\nlayout = dict(title='Ramen Review per Country',\n\ngeo = dict(showframe = True, projection={'type':'mercator'}))\n\nchoromap = go.Figure(data = [data], layout = layout)\n\niplot(choromap, validate=False)","80835dac":"1. Cluster 0\n    - Mean stars rating: 3.74\n    - Mean review: 1602\n    - Variety: 24, 25, 39, 55\n\n*Notes: I only take 4 variety (label) to represent ramen variety for each cluster","c1e1fe3c":"- For the text generation,transformation and clustering i learned alot from this kernel: ttps:\/\/www.kaggle.com\/miljan\/customer-segmentation. Kindly check and upvote his kernel!\n- To transforming text into feature i used TfidfVectorizer, after get the matriks we will perform dimensionality reduction using truncated SVD (aka LSA). This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD).\n- And then we will try Silhoutte Scoring to search the best number of cluster and visualize our clustering using TSNE\n- Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other (dzone.com)\n\n![](https:\/\/miro.medium.com\/max\/700\/1*cUcY9jSBHFMqCmX-fp8BvQ.jpeg) (www.towardsdatascience.com)\n\n- For the variety we get 70 cluster which the best to represent number of variety of ramen in this dataset (based on Silhoutte Scoring)\n- And then try to look up the popular word in cluster (randomly) using WordCloud","ee9dc34d":"- The most country which have ramen review is Japan, USA and South Korea\n- For the stars rating, i use mean rating in each country and search for the highest rating which is Brazil, Sarawak, Malaysia and Indonesia\n- For the ramen variety, Japan, USA and South Korea have most ramen variety","e6d7f351":"- I try to make clustering again based on Stars and Review feature using KMeans Clustering\n- The best number of cluster to represent the Stars and Review is 4 cluster\n- And then combine the result into our dataset","f165b000":"- For the data shape, we have 2580 rows, and 7 columns\n- For the data type, all columns labeled as object. Later we must change feature such as Review and Stars to numerical\n- Top ten feature is feature which have most missing values\n- We will ignore the Top Ten because it doesn't contain complete informasion about top ten ramen each year. For the missing values, i fill it with 0\n- We check duplicate and drop it\n- For other feature that have missing values, i simply drop it. Because the size is not too much.","713cc044":"2. Cluster 1\n    - Mean stars rating: 3.21\n    - Mean review: 317\n    - Variety: 47, 38, 39, 2","5ef10a11":"Hello \ud83d\ude4c, welcome to my notebook. In this notebook we will try to learn Interactive Visualization and Clustering. Also try to make Geo-Map to better visualization about Ramen Stars and Review distribution around the World. Feel free if you have any question or suggestion! Thank you!\n\n![](https:\/\/realfood.tesco.com\/media\/images\/1400x919-BeefRamen-db0570eb-10cf-437b-a497-6cd26837ee2d-0-1400x919.jpg) (www.realfood.tesco.com)","515c4327":"- The most counted ramen style in this dataset is Ramen with Pack style\n- The most reviewed ramen style in this dataset is Ramen with Cup style","1049c024":"- We try to check the distribution of Review and Stars of our clustering\n- It seems that our clustering seperate the data very well!\n- And then try to make Ramen Star Rating and Review Geo-Map","1312a319":"3. Cluster 2\n    - Mean stars rating: 3.93\n    - Mean review: 2253\n    - Variety: 23, 54, 15, 39","7a226046":"Dont' Forget to Upvote! Thank you!:)","a0dfff05":"4. Cluster 3\n    - Mean stars rating: 3.71\n    - Mean review: 957\n    - Variety: 26, 39, 30, 49","4fbab3c0":"- Context: The Ramen Rater is a product review website for the hardcore ramen enthusiast (or \"ramenphile\"), with over 2500 reviews to date. This dataset is an export of \"The Big List\" (of reviews), converted to a CSV format.\n\n- Content: Each record in the dataset is a single ramen product review. Review numbers are contiguous: more recently reviewed ramen varieties have higher numbers. Brand, Variety (the product name), Country, and Style (Cup? Bowl? Tray?) are pretty self-explanatory. Stars indicate the ramen quality, as assessed by the reviewer, on a 5-point scale; this is the most important column in the dataset!"}}