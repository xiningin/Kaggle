{"cell_type":{"58791e66":"code","f3de9c7a":"code","49f6e845":"code","3b8d0fcc":"code","653f2484":"code","db87ef9b":"code","db3572ba":"code","555ce70c":"code","f8a12209":"code","fec2086e":"code","6c27127b":"code","47f1b2a7":"code","433d48c7":"code","b5339ea0":"code","0ae9c8a0":"code","e26f2d0f":"code","4110fcce":"code","b5cbc2cd":"code","aa8dbb4b":"code","9099c56a":"code","b42b14bc":"code","3f4be934":"code","eae6333f":"code","310fc6b1":"code","49d8d8d5":"code","afaf9493":"code","98758023":"code","6d0ac037":"code","c88acc0f":"code","0b2c55d5":"code","1a9ccebe":"code","ee5fd294":"code","82247e86":"code","b262bc7c":"code","ca311995":"code","e62d68bd":"code","e7bc7cac":"code","01293c35":"code","49bc58c8":"code","d849375e":"code","9b37dbd2":"code","ebf173fd":"code","3e5962d9":"code","61c05af1":"code","4a9edbcf":"code","156ee786":"code","3da67615":"code","5d36656d":"code","544de7b0":"code","ae8e1d00":"code","de5ccb76":"code","b2fbfb8f":"code","12fafc30":"code","b39ef15c":"code","9ddc8281":"code","f243fcbc":"code","f6db8e03":"code","505fc660":"code","a1ab15f6":"code","bc7c2548":"code","8014d461":"code","852ec326":"code","f59c191e":"code","3ae28a08":"markdown","fa24b004":"markdown","b6129fcc":"markdown","84b09c6b":"markdown","07d39ac2":"markdown","2af72ae8":"markdown","893e9fd0":"markdown","9fdf9b6b":"markdown","c3acf493":"markdown","79584141":"markdown","dfea7561":"markdown","f5a28fcd":"markdown","236e7e05":"markdown","43e3bcea":"markdown","2a286a7f":"markdown","8f424b74":"markdown","e2492b8a":"markdown","34ef1aad":"markdown"},"source":{"58791e66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3de9c7a":"gs = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","49f6e845":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","3b8d0fcc":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","653f2484":"train.describe()","db87ef9b":"train.isnull().sum()","db3572ba":"test.isnull().sum()","555ce70c":"pd.pivot_table(train, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","f8a12209":"#distributions for all numeric variables \nfor i in train[['Age','SibSp','Parch','Fare']].columns:\n    plt.hist(train[i])\n    plt.title(i)\n    plt.show()","fec2086e":"sns.barplot(train['Survived'].value_counts().index,train['Survived'].value_counts()).set_title('Survived')\nplt.show()","6c27127b":"pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count')","47f1b2a7":"sns.barplot(train['Pclass'].value_counts().index,train['Pclass'].value_counts()).set_title('Pclass')\nplt.show()","433d48c7":"pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count')","b5339ea0":"sns.barplot(train['Sex'].value_counts().index,train['Sex'].value_counts()).set_title('Sex')\nplt.show()","0ae9c8a0":"pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count')","e26f2d0f":"sns.barplot(train['Embarked'].value_counts().index,train['Embarked'].value_counts()).set_title('Embarked')\nplt.show()","4110fcce":"sns.barplot(train['Cabin'].value_counts().index,train['Cabin'].value_counts()).set_title('Cabin')\nplt.show()","b5cbc2cd":"sns.barplot(train['Ticket'].value_counts().index,train['Ticket'].value_counts()).set_title('Ticket')\nplt.show()","aa8dbb4b":"#Check correlation between our features.\nprint(train.corr())\nsns.heatmap(train.corr())","9099c56a":"#Creating variable for total family.\ntrain['Fam'] = train['SibSp'] + train['Parch']\ntrain.head()","b42b14bc":"#Get cabin Letters from the cabin column.\n\ntrain['cabin_adv'] = train.Cabin.apply(lambda x: str(x)[0])\n\n#comparing surivial rate by cabin\nprint(train.cabin_adv.value_counts())\npd.pivot_table(train,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')\n","3f4be934":"#make things simple drop T since theres only 1 row.\n\ntrain[train['cabin_adv'] == 'T'].index\ntrain.drop(index = 339, inplace = True)\ntrain.cabin_adv.value_counts()","eae6333f":"#Getting the numbers and letters out of the tickets column\ntrain['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain['ticket_letters'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n\n#lets us view all rows in dataframe through scrolling. This is for convenience \npd.set_option(\"max_rows\", None)\ntrain['ticket_letters'].value_counts()","310fc6b1":"#survival rate across different tyicket types \npd.pivot_table(train,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')","49d8d8d5":"#feature engineering on person's title \ntrain.Name.head(50)\ntrain['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#mr., ms., master. etc\n\ntrain['name_title'].value_counts()","afaf9493":"pd.pivot_table(train,index='Survived',columns='name_title', values = 'Ticket', aggfunc='count')","98758023":"test['Fam'] = test['SibSp'] + test['Parch']\ntest['cabin_adv'] = test.Cabin.apply(lambda x: str(x)[0])\ntest['numeric_ticket'] = test.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntest['ticket_letters'] = test.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\ntest.Name.head(50)\ntest['name_title'] = test.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n# test.head()\ntest['cabin_adv'].value_counts()","6d0ac037":"#impute nulls for continuous data \ntrain.Age = train.Age.fillna(train.Age.median())\ntrain.Fare = train.Fare.fillna(train.Fare.median())\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \ntrain.dropna(subset=['Embarked'],inplace = True)\ntrain.drop(columns=['Name', 'PassengerId', 'Cabin'], inplace = True)\ntrain.head()","c88acc0f":"#impute nulls for continuous data \ntest.Age = test.Age.fillna(train.Age.median())\ntest.Fare = test.Fare.fillna(train.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \ntest.dropna(subset=['Embarked'],inplace = True)\npassid = test['PassengerId'].copy()\ntest.drop(columns=['Name', 'PassengerId', 'Cabin'], inplace = True)\ntest.head()","0b2c55d5":"df = pd.concat([train.assign(ind=1), test.assign(ind=0)])\ndf_hot = pd.get_dummies(df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked', 'Fam', 'cabin_adv', 'numeric_ticket', 'ticket_letters', 'name_title', 'ind']])\n#notice that I left out our target column.\ndf_hot.head()","1a9ccebe":"test_hot, train_hot = df_hot[df_hot[\"ind\"].eq(0)], df_hot[df_hot[\"ind\"].eq(1)]\n","ee5fd294":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ntrain_hot[['Age','SibSp','Parch','Fare', 'Fam']]= scale.fit_transform(train_hot[['Age','SibSp','Parch','Fare', 'Fam']])\ntrain_hot.head()","82247e86":"scale = StandardScaler()\ntest_hot[['Age','SibSp','Parch','Fare', 'Fam']]= scale.fit_transform(test_hot[['Age','SibSp','Parch','Fare', 'Fam']])\ntest_hot.head()","b262bc7c":"Y_train = train['Survived']\nY_train.head()","ca311995":"print(train_hot.shape)\nprint(test_hot.shape)\nprint(Y_train.shape)\ntrain_hot.head()","e62d68bd":"test_hot.head()","e7bc7cac":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","01293c35":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1, learning_rate = 0.5, min_child_weight = 0.03)\ncv = cross_val_score(xgb,train_hot,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","49bc58c8":"rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=2)\ncv = cross_val_score(rfc,train_hot,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d849375e":"svc = SVC(probability = True)\ncv = cross_val_score(svc,train_hot,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9b37dbd2":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train_hot, Y_train, test_size=0.33, random_state=42)","ebf173fd":"svc = SVC(probability = True)\nsvc.fit(X_train, y_train)","3e5962d9":"xgb = XGBClassifier(random_state =1, learning_rate = 0.5, min_child_weight = 0.03)\nxgb.fit(X_train, y_train)","61c05af1":"from keras.models import Sequential\nfrom keras.layers import Dense\ndl1 = Sequential()\ndl1.add(Dense(1002, activation='relu'))\ndl1.add(Dense(512, activation='relu'))\ndl1.add(Dense(1, activation='sigmoid'))\n# compile the keras model\ndl1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\ndl1.fit(X_train, y_train, epochs=150, batch_size=10)\n# evaluate the keras model\n_, accuracy = dl1.evaluate(X_train, y_train)\nprint('Accuracy: %.2f' % (accuracy*100))","4a9edbcf":"dl2 = Sequential()\ndl2.add(Dense(1002, activation='relu'))\ndl2.add(Dense(512, activation='relu'))\ndl2.add(Dense(256, activation='relu'))\n# dl2.add(Dense(128, activation='relu'))\ndl2.add(Dense(1, activation='sigmoid'))\n# compile the keras model\ndl2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\ndl2.fit(X_train, y_train, epochs=150, batch_size=10)\n# evaluate the keras model\n_, accuracy = dl2.evaluate(X_train, y_train)\nprint('Accuracy: %.2f' % (accuracy*100))","156ee786":"knn = KNeighborsClassifier(algorithm= 'auto', n_neighbors = 7,weights= 'uniform')\nknn.fit(X_train, y_train)","3da67615":"lr = LogisticRegression(max_iter = 2000,\n              solver = 'liblinear')\nlr.fit(X_train, y_train)","5d36656d":"rf = RandomForestClassifier(random_state = 1)\nrf.fit(X_train, y_train)","544de7b0":"#Valid preds\nvalid_preds1 = svc.predict(X_valid)\nvalid_preds2 = xgb.predict(X_valid)\nvalid_preds_ur3 = dl1.predict(X_valid)\nvalid_preds3 = [round(x[0]) for x in valid_preds_ur3]\nvalid_preds_ur4 = dl2.predict(X_valid)\nvalid_preds4 = [round(x[0]) for x in valid_preds_ur4]\nvalid_preds5 = knn.predict(X_valid)\nvalid_preds6 = lr.predict(X_valid)\nvalid_preds7 = rf.predict(X_valid)","ae8e1d00":"valid_preds6","de5ccb76":"valid_preds7","b2fbfb8f":"from sklearn.metrics import accuracy_score\nprint(\"SVC:\", accuracy_score(y_valid, valid_preds1))\nprint(\"XGB:\", accuracy_score(y_valid, valid_preds2))\nprint(\"DL1:\", accuracy_score(y_valid, valid_preds3))\nprint(\"DL2:\", accuracy_score(y_valid, valid_preds4))\nprint(\"KNN:\", accuracy_score(y_valid, valid_preds5))\nprint(\"LR:\", accuracy_score(y_valid, valid_preds5))\nprint(\"RF:\", accuracy_score(y_valid, valid_preds5))","12fafc30":"#Test preds\ntest_preds1 = svc.predict(test_hot)\ntest_preds2 = xgb.predict(test_hot)\ntest_preds_ur3 = dl1.predict(test_hot)\ntest_preds3 = [round(x[0]) for x in test_preds_ur3]\ntest_preds_ur4 = dl2.predict(test_hot)\ntest_preds4 = [round(x[0]) for x in test_preds_ur4]\ntest_preds5 = knn.predict(test_hot)\ntest_preds6 = lr.predict(test_hot)\ntest_preds7 = rf.predict(test_hot)","b39ef15c":"test_preds6","9ddc8281":"test_preds7","f243fcbc":"stacked_predictions = np.column_stack([valid_preds1,valid_preds2,valid_preds3,valid_preds4, valid_preds5, valid_preds6, valid_preds7])\nstacked_test_predictions = np.column_stack([test_preds1,test_preds2,test_preds3,test_preds4, test_preds5, test_preds6, test_preds7])","f6db8e03":"stacked_predictions","505fc660":"stacked_test_predictions","a1ab15f6":"from sklearn.linear_model import LinearRegression\n#Making our Final predictions of ensemble.\nmeta_model = LinearRegression()\nmeta_model.fit(stacked_predictions, y_valid)\nmeta_predictions_ur = meta_model.predict(stacked_test_predictions)\npreds = [round(x) for x in meta_predictions_ur]\npreds","bc7c2548":"#Checking out the sample submission\ngs.head()","8014d461":"df = pd.DataFrame({'PassengerId': passid,\n                  'Survived' : preds})\ndf.Survived = df.Survived.astype(int)\nprint(df.shape)\ndf.head()","852ec326":"df['Survived'].sum()","f59c191e":"df.to_csv('submission_final.csv', index =False)","3ae28a08":"Stacking our Predictions to pass on to the meta model.","fa24b004":"Lets get our data **One Hot Encoded**.\nWe treated to training and test set separately until now, but its just simpler to get the one hot encodings if the datasets are concatenated.","b6129fcc":"Training our Level 2 meta model on the stacked predictions of our Level 1 models.","84b09c6b":"Prediciting on test set.","07d39ac2":"# Data Exploration\n\n**Lets get some insights about our data.**","2af72ae8":"Predicting on Validation Set.","893e9fd0":"# Ensemble\nTrying out and tuning different models, tuning on validation set and ensembling them. We will ensemble our model by **stacking**\n","9fdf9b6b":"# Feature Engineering ","c3acf493":"Now lets also apply our Feature Engineering changes to test data.","79584141":"Lets also handle **Null** values in our **test** data.","dfea7561":"Our best feature currently seems to be 'Fare' according to the correlation map as it has high correlation with our target ie. 'Survived'","f5a28fcd":"Lets handle **Null** values in our **train** data.","236e7e05":"Feel free to make any suggestions that will help make this notebook better. \nThe EDA and Feature Engineering was inspired by [Kenjee's Notebook on Titanic](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example). ","43e3bcea":"Validation scores of our models.","2a286a7f":"Importing our Data from Kaggle.","8f424b74":"Training Our **First-Level Base Models**.","e2492b8a":"Lets also check out cabin and ticket features even though the columns wont make much sense on their own without any feature engineering.","34ef1aad":"# Model\n\nSeems like we are all set on the data.\nLets try a few models to see how they perform on our data\n"}}