{"cell_type":{"b765ed43":"code","0bb1c21d":"code","0f04f77f":"code","5fbb74bc":"code","38a54d62":"code","20f32954":"code","f4ffe7b5":"code","344b51ff":"code","3ed46e0a":"code","94a73a72":"code","eddd4b18":"code","05376a37":"code","cf8259a0":"code","b3efeec5":"code","626c38a2":"code","5c044ce0":"code","ebe0d008":"code","270f4b8f":"code","62ae2131":"code","97a3aced":"code","51c657d8":"code","91b822ba":"code","23e69a8a":"code","fce6b0b3":"code","3804a399":"code","09464a87":"code","b62ce183":"code","9c6e16bb":"markdown","2b8ae8a0":"markdown","1e379d1e":"markdown","d56c9535":"markdown","b043f4ed":"markdown","24e2b287":"markdown","dab0fd04":"markdown","3e691da8":"markdown"},"source":{"b765ed43":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n!pip install transformers==3.1.0\n!pip install nlp","0bb1c21d":"import pytorch_lightning as pl\nimport argparse\nimport numpy as np \nimport pandas as pd ","0f04f77f":"from transformers import RobertaConfig,RobertaForSequenceClassification,RobertaTokenizer,AdamW,set_seed\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport nlp\nfrom collections import defaultdict\nfrom matplotlib import pyplot as plt\nimport os","5fbb74bc":"set_seed(6)","38a54d62":"import torch\ntorch.__version__","20f32954":"import transformers\ntransformers.__version__","f4ffe7b5":"# model = MultiLabelClassificationModel('bert', 'bert-base-dutch-cased\/bertje-base', num_labels=47, args={'n_gpu': 1, 'train_batch_size':8, 'gradient_accumulation_steps':16, 'learning_rate': 1e-4, 'num_train_epochs': 1, 'max_seq_length': 256, 'fp16': False, 'reprocess_input_data': True, 'use_cached_eval_features': False, 'evaluate_during_training': True, 'evaluate_during_training_verbose': True, 'output_dir': 'outputs\/'})","344b51ff":"\ntokenizer = RobertaTokenizer.from_pretrained('..\/input\/roberta-transformers-pytorch\/roberta-base')\nmodel = RobertaForSequenceClassification.from_pretrained('..\/input\/roberta-transformers-pytorch\/roberta-base',  num_labels=2)\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nlabels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\noutputs = model(**inputs, labels=labels)\nprint(\"loss :\",outputs[0])\nlogits = outputs[1]\nprint(\"\\n\\n\\n\",logits,\"\\n\\n\")\nprob = torch.softmax(logits, dim=1)\npred = torch.argmax(prob, dim=1) \nprint(pred,\"\\n\\n\",prob)","3ed46e0a":"tokenizer.get_vocab()","94a73a72":"dataset = nlp.load_dataset('boolq')\nval_df_tr = pd.DataFrame()\ntest_df_tr = pd.DataFrame()\n\nval_df_fl = pd.DataFrame()\ntest_df_fl = pd.DataFrame()\n\ntrain_df =pd.DataFrame(dataset['train'])\ndf = pd.DataFrame(dataset['validation'])\n\nval_df = pd.concat([df[df['answer']== True][:1016],df[df['answer']== False][:618]])\n\ntest_df = pd.concat([df[df['answer']== True][1016:],df[df['answer']== False][618:]])\ntrain_df.to_csv('\/kaggle\/working\/train.csv')\nval_df.to_csv('\/kaggle\/working\/val.csv')\ntest_df.to_csv('\/kaggle\/working\/test.csv')","eddd4b18":"dat =[{\n\t\"answer\":\"True\",\n\t\"passage\": \"YES Bank loan fraud: CBI charges Rana Kapoor with criminal conspiracy.\",\n\t\"question\":\"Does Yes bank involved in any fraud\",\n\t\"link\":\"https:\/\/www.business-standard.com\/article\/current-affairs\/yes-bank-loan-fraud-cbi-charges-rana-kapoor-with-criminal-conspiracy-120062501509_1.html\"\n},\n    {\n\t\"answer\":\"False\",\n\t\"passage\": \"Reserve Bank of India imposes monetary penalty on Oriental Bank of Commerce.\",\n\t\"question\":\"Oriental Bank of Commerce was not penalized by Reserve Bank of India\",\n\t\"link\":\"https:\/\/m.rbi.org.in\/commonman\/English\/Scripts\/PressReleases.aspx?Id=2868\"\n},\n{\n\t\"answer\":\"True\",\n\t\"passage\": \"YES Bank loan fraud: CBI charges Rana Kapoor with criminal conspiracy.\",\n\t\"question\":\"Rana Kapoor is charged with criminal conspiracy\",\n\t\"link\":\"https:\/\/www.business-standard.com\/article\/current-affairs\/yes-bank-loan-fraud-cbi-charges-rana-kapoor-with-criminal-conspiracy-120062501509_1.html\"\n},\n{\n\t\"answer\":\"True\",\n\t\"passage\": \"The Reserve Bank of India Tuesday slapped Rs 1 crore fine on Yes Bank for non-compliance of directions on Swift messaging software.The private sector lender in a regulatory filing said. The Reserve Bank of India (RBI) has levied an aggregate penalty of Rs 10 million (Rs 1 crore) on the bank for non-compliance of regulatory directions observed during assessment of implementation of SWIFT-related operational controls.\",\n\t\"question\":\"Yes Bank is fined by Reserve Bank of India\",\n\t\"link\":\"https:\/\/www.businesstoday.in\/current\/corporate\/reserve-bank-of-india-rbi-yes-bank-swift-messaging-software-pnb-fraud-rbi-levies-rs-1-crore-penalty-on-yes-bank-for-non-compliance-of-instructions-related-to-swift\/story\/324546.html\"\n},\n{\n\t\"answer\":\"True\",\n\t\"passage\": \"\"\"The latest disclosure under SEBI Prohibition of Insider Trading regulations was made by N S KANNAN in ICICI Bank Ltd. where Acquisition of 75000 Equity Shares done at an average price of Rs. 153.0 was reported to the exchange on Oct. 1st 2020.There were no SAST disclosures made for ICICI Bank Ltd.Insider trades are disclosures under SEBI (Prohibition of Insider Trading) Regulations. 2015 ([Regulation 7 (2) with 6(2)] made by corporate insiders: promoters. officers. directors. employees and large shareholders who are buying and selling stock in their own companies.\"\"\",\n\t\"question\":\"is N S KANNAN engaged in Insider trading\",\n\t\"link\":\"https:\/\/trendlyne.com\/equity\/insider-trading-sast\/all\/ICICIBANK\/584\/icici-bank-ltd\/#:~:text=Insider%20Trading%20%26%20SAST%20disclosures%20for,an%20average%20price%20of%20Rs.&text=There%20were%20no%20SAST%20disclosures%20made%20for%20ICICI%20Bank%20Ltd.\"\n}\n,\n{\n\t\"answer\":\"True\",\n\t\"passage\": \"\"\"The latest disclosure under SEBI Prohibition of Insider Trading regulations was made by N S KANNAN in ICICI Bank Ltd. where Acquisition of 75000 Equity Shares done at an average price of Rs. 153.0 was reported to the exchange on Oct. 1st 2020.There were no SAST disclosures made for ICICI Bank Ltd.Insider trades are disclosures under SEBI (Prohibition of Insider Trading) Regulations. 2015 ([Regulation 7 (2) with 6(2)] made by corporate insiders: promoters. officers. directors. employees and large shareholders who are buying and selling stock in their own companies.\"\"\",\n\t\"question\":\"is N S KANNAN engaged in Insider trading\",\n\t\"link\":\"https:\/\/trendlyne.com\/equity\/insider-trading-sast\/all\/ICICIBANK\/584\/icici-bank-ltd\/#:~:text=Insider%20Trading%20%26%20SAST%20disclosures%20for,an%20average%20price%20of%20Rs.&text=There%20were%20no%20SAST%20disclosures%20made%20for%20ICICI%20Bank%20Ltd.\"\n},\n\n{\n\t\"answer\":\"False\",\n\t\"passage\": \"Swiss banking giant UBS has been fined \u20ac3.7bn (\u00a33.2bn; $4.2bn) in a French tax fraud case.A court in Paris found that the bank had illegally helped French clients hide billions of euros from French tax authorities between 2004 and 2012.\",\n\t\"question\":\"UBS not involved in tax fraud\",\n\t\"link\":\"https:\/\/www.bbc.com\/news\/business-47305227\"\n},\n{\n\t\"answer\":\"True\",\n\t\"passage\": \"\"\"After completing an over year-long investigation and under various sections of the\nForex law, the highest ever FEMA show-cause notice has been issued today. The ED has charged the\nfirm under the FEMA for resorting to unauthorised foreign exchange dealings, holding of foreign\nexchange outside India, and willful siphoning off a whopping amount of Rs 7,220 crore as export\nproceedings. At present, the firm\u2019s three promoter brothers \u2013 Nilesh Parekh, Umesh Parekh, and\nKamlesh Parekh \u2013 are also being probed by CBI and ED. Earlier, the firm was allegedly defrauding a\nconsortium of 25 banks to the tune of Rs 2,672 crore by availing credit facilities in the form of\nworking capital loans and discounting of export bills\"\"\",\n\t\"question\":\"Umesh Parekh involved in foreign exchange dealings\",\n\t\"link\":\"\"\n},\n\n]\nda_df = pd.DataFrame(dat)\nda_df.to_csv(\"test_new.csv\")","05376a37":"da_df.count()","cf8259a0":"from random import randint","b3efeec5":"print(\"passage:\\n\",test_df.iloc[1]['passage'],\"\\n\\n\\n\")\nprint(\"question:\\n\",test_df.iloc[1]['question'],\"\\n\\n\\n\")\nprint(\"answer:\\n\",test_df.iloc[1]['answer'],\"\\n\\n\\n\")","626c38a2":"\n# train_df = pd.concat([train_df.head(100),train_df.tail(100)])\n# train_df.to_csv('\/kaggle\/working\/train.csv')\n# val_df = pd.concat([val_df.head(25),val_df.tail(25)])\n# val_df.to_csv('\/kaggle\/working\/val.csv')\n\n# test_df = pd.concat([test_df.head(25),test_df.tail(25)])\n# test_df.to_csv('\/kaggle\/working\/test.csv')","5c044ce0":"print(\"train_df :\",train_df.count())\nprint(\"val :\",val_df.count())\nprint(\"Test :\",test_df.count())","ebe0d008":"class BoolqDataset(Dataset):\n    def __init__(self,dir_path,type_,tokenizer):\n        self.df = pd.read_csv(dir_path+type_+\".csv\")\n        self.df['answer_']=self.df['answer'].apply(lambda x : 1 if x==True else 0)\n        self.tokenizer = tokenizer\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,index):\n        passage = self.df.iloc[index]['passage']\n        question = self.df.iloc[index]['question']\n        label = self.df.iloc[index]['answer_']\n        seq_len = len(self.tokenizer.tokenize(passage))+len(self.tokenizer.tokenize(question))\n        return {'passage':passage,'question':question,'label':label,\"seq_len\":seq_len}\ndef collate_fn_seq_len(batch,params):\n    input_ids =[]\n    attention_mask = []\n    target_ids = []\n    \n    max_seq_len = max([iter_data['seq_len'] for iter_data in batch]) \n    max_seq_len = 512\n    for iter_data in batch:\n#         print(\"question :\",iter_data['question'],\"\\npassage:\",iter_data['passage'],\"\\nGT :\",iter_data['label'],\"\\n\\n\\n\")\n        encoding = params['tokenizer'].encode_plus(iter_data['question'],iter_data['passage'],return_tensors=\"pt\",max_length=max_seq_len,pad_to_max_length=True,truncation=True)\n        input_ids.append(encoding['input_ids'])\n        attention_mask.append(encoding['attention_mask'])\n        target_ids.append(torch.tensor(iter_data['label'],dtype=torch.long))\n    return {'input_ids':(torch.stack(input_ids)).squeeze(),'attention_mask':(torch.stack(attention_mask)).squeeze(),\"target_ids\":(torch.stack(target_ids)).squeeze()}","270f4b8f":"\nbq = BoolqDataset('\/kaggle\/working\/','test',tokenizer)\ndl = DataLoader(bq,batch_size=2,drop_last=True,collate_fn= lambda b, params={'tokenizer':tokenizer}: collate_fn_seq_len(b, params),shuffle=False)","62ae2131":"for batch_ in dl:\n    s=\"\"\n#     print(batch_['target_ids'])\n    print(batch_['input_ids'].size())\n    #print(tokenizer.convert_ids_to_tokens(batch_['input_ids'][0]))\n        \n    break\n    ","97a3aced":"!zip roberta-large.zip ..\/input\/roberta-transformers-pytorch\/roberta-large","51c657d8":"configuration = RobertaConfig()\nargs= dict(\n    data_dir=\"\/kaggle\/working\/\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='..\/input\/roberta-transformers-pytorch\/roberta-large',\n    tokenizer_name_or_path='..\/input\/roberta-transformers-pytorch\/roberta-large',\n    max_seq_length=512,\n    learning_rate=1e-6,\n    weight_decay=0.0,\n    adam_epsilon=1e-5,\n    warmup_steps=0,\n    train_batch_size=4,\n    eval_batch_size=4,\n    test_batch_size=4,\n    num_train_epochs=3,\n    shuffle=True\n)\n\nparams = argparse.Namespace(**args)\nconfiguration.max_position_embeddings = params.max_seq_length+2\nconfiguration.num_labels= 2\nconfiguration.type_vocab_size=1\nconfiguration.vocab_size=50265\n\n#Uncomment below for Large Model\nconfiguration.hidden_size=1024\nconfiguration.num_attention_heads=16\nconfiguration.num_hidden_layers=24\nconfiguration.intermediate_size=4096\n\n\n\n\n# old 12 and \nconfiguration","91b822ba":"\nclass ClassificationModel():\n    def __init__(self,hparams):\n        super(ClassificationModel, self).__init__()\n        self.hparams = hparams\n\n        self.model = RobertaForSequenceClassification.from_pretrained(hparams.model_name_or_path,config=configuration)\n        self.tokenizer = RobertaTokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n        \n","23e69a8a":"news_classification = ClassificationModel(params)","fce6b0b3":"device=\"cuda\"\ntest_model = news_classification.model.from_pretrained('..\/input\/my-model\/save_model\/')#,num_labels=2)\n\ntest_model.to(device)\ntest_model.eval()","3804a399":"\n# from tqdm import tqdm \n# for batch_ in tqdm(dl, desc =\"Status \"): \n    \n#     logits = test_model(batch_['input_ids'].to(device),batch_[\"attention_mask\"].to(device))  \n#     prob = torch.softmax(logits[0], dim=1)\n#     pred = torch.argmax(prob, dim=1)   \n \n#     print(\"pred :\",pred)\n    ","09464a87":"# tensor([1, 1]) (tensor([[-2.0116,  1.0465],\n#         [-1.9102,  1.7220]], grad_fn=<AddmmBackward>),)\n# tensor([1, 1])","b62ce183":"from tqdm import tqdm\npred_l=[]\ntrg_l=[]\nfor batch_ in tqdm(dl, desc =\"Status \"):    \n    logits = test_model(batch_['input_ids'].to(device),batch_[\"attention_mask\"].to(device))  \n    prob = torch.softmax(logits[0], dim=1)\n#     print(\"prob :\",prob)\n    pred = torch.argmax(prob, dim=1)   \n#     print(\"pred :\",pred)\n    pred_l.append(pred)\n    trg_l.append(batch_[\"target_ids\"])\n    \nprint(\"Prediction End.....\")\n\npr=[]\ntr=[]\nfor f,j in zip(pred_l,trg_l):\n    for ff in f:\n        pr.append(ff.item())\n    for ff in j:\n        tr.append(ff.item())\n    \ncnt=0\nfor i,j in zip(tr,pr):\n    if i==j:\n        cnt+=1\ncnt\/len(pr)\n","9c6e16bb":"Test Loader","2b8ae8a0":"### Model","1e379d1e":"### Loading the saved model","d56c9535":"###  Test Model","b043f4ed":"### Data Loader","24e2b287":"## Boolq Data","dab0fd04":"###  Example","3e691da8":"### Model Config"}}