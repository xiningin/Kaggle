{"cell_type":{"f11789cd":"code","831fad24":"code","25d28f8d":"code","e03af6e1":"code","6a2d4d25":"code","c05f4a82":"code","abb33e0c":"code","f52bb489":"code","954f1a27":"code","3d4e03db":"code","0c4423fd":"code","ce2c256f":"markdown","e699c204":"markdown","29291f5f":"markdown","0ffd9831":"markdown","de10c84e":"markdown"},"source":{"f11789cd":"import pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.dates import date2num, num2date\nfrom matplotlib import dates as mdates\nfrom matplotlib import ticker\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\nfrom scipy import stats as sps\nfrom scipy.interpolate import interp1d\n\nfrom IPython.display import clear_output\n\n%config InlineBackend.figure_format = 'retina'","831fad24":"#import data\ncases_df = pd.read_csv('\/kaggle\/input\/covid19-challenges\/test_data_canada.csv')\ncases_df['date'] = pd.to_datetime(cases_df['date'])\nprovince_df = cases_df.groupby(['province', 'date'])['id'].count()\nprovince_df.index.rename(['region', 'date'], inplace=True)\nhr_df = cases_df.groupby(['region', 'date'])['id'].count()\ncanada_df = pd.concat((province_df, hr_df))","25d28f8d":"prov_name = 'Ontario'\n\ndef prepare_cases(cases):\n    # modification - Isha Berry et al.'s data already come in daily\n    #new_cases = cases.diff()\n    new_cases = cases\n\n    smoothed = new_cases.rolling(7,\n        win_type='gaussian',\n        min_periods=1,\n        # Alf: switching to right-aligned instead of centred to prevent leakage of\n        # information from the future\n        #center=True).mean(std=2).round()\n        center=False).mean(std=2).round()\n    \n    zeros = smoothed.index[smoothed.eq(0)]\n    if len(zeros) == 0:\n        idx_start = 0\n    else:\n        last_zero = zeros.max()\n        idx_start = smoothed.index.get_loc(last_zero) + 1\n    smoothed = smoothed.iloc[idx_start:]\n    original = new_cases.loc[smoothed.index]\n    \n    return original, smoothed\n\ncases = canada_df.xs(prov_name).rename(f\"{prov_name} cases\")\n\noriginal, smoothed = prepare_cases(cases)\n\n\n# We create an array for every possible value of Rt\nR_T_MAX = 12\nr_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)\n\n# Gamma is 1\/serial interval\n# https:\/\/wwwnc.cdc.gov\/eid\/article\/26\/6\/20-0357_article\nGAMMA = 1\/4\n\ndef get_posteriors(sr, window=7, min_periods=1):\n    lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))\n\n    # Note: if you want to have a Uniform prior you can use the following line instead.\n    # I chose the gamma distribution because of our prior knowledge of the likely value\n    # of R_t.\n    \n    # prior0 = np.full(len(r_t_range), np.log(1\/len(r_t_range)))\n    prior0 = np.log(sps.gamma(a=3).pdf(r_t_range) + 1e-14)\n\n    likelihoods = pd.DataFrame(\n        # Short-hand way of concatenating the prior and likelihoods\n        data = np.c_[prior0, sps.poisson.logpmf(sr[1:].values, lam)],\n        index = r_t_range,\n        columns = sr.index)\n\n    # Perform a rolling sum of log likelihoods. This is the equivalent\n    # of multiplying the original distributions. Exponentiate to move\n    # out of log.\n    posteriors = likelihoods.rolling(window,\n                                     axis=1,\n                                     min_periods=min_periods).sum()\n    posteriors = np.exp(posteriors)\n\n    # Normalize to 1.0\n    posteriors = posteriors.div(posteriors.sum(axis=0), axis=1)\n    \n    return posteriors\n\nposteriors = get_posteriors(smoothed)\n\ndef highest_density_interval(pmf, p=.95):\n    \n    # If we pass a DataFrame, just call this recursively on the columns\n    if(isinstance(pmf, pd.DataFrame)):\n        return pd.DataFrame([highest_density_interval(pmf[col]) for col in pmf],\n                            index=pmf.columns)\n    \n    cumsum = np.cumsum(pmf.values)\n    best = None\n    for i, value in enumerate(cumsum):\n        for j, high_value in enumerate(cumsum[i+1:]):\n            if (high_value-value > p) and (not best or j<best[1]-best[0]):\n                best = (i, i+j+1)\n                break\n            \n    low = pmf.index[best[0]]\n    high = pmf.index[best[1]]\n    return pd.Series([low, high], index=['Low', 'High'])\n\n\nhdis = highest_density_interval(posteriors)\n\nmost_likely = posteriors.idxmax().rename('ML')\n\n# Look into why you shift -1\nresult = pd.concat([most_likely, hdis], axis=1)\n\n#result.tail()\n\n\n#from pandas.plotting import register_matplotlib_converters\n#register_matplotlib_converters()\n\ndef plot_rt(result, ax, state_name):\n    \n    ax.set_title(f\"{prov_name}\")\n    \n    # Colors\n    ABOVE = [1,0,0]\n    MIDDLE = [1,1,1]\n    BELOW = [0,0,0]\n    cmap = ListedColormap(np.r_[\n        np.linspace(BELOW,MIDDLE,25),\n        np.linspace(MIDDLE,ABOVE,25)\n    ])\n    color_mapped = lambda y: np.clip(y, .5, 1.5)-.5\n    \n    index = result['ML'].index.get_level_values('date')\n    values = result['ML'].values\n    \n    # Plot dots and line\n    ax.plot(index, values, c='k', zorder=1, alpha=.25)\n    ax.scatter(index,\n               values,\n               s=40,\n               lw=.5,\n               c=cmap(color_mapped(values)),\n               edgecolors='k', zorder=2)\n    \n    # Aesthetically, extrapolate credible interval by 1 day either side\n    lowfn = interp1d(date2num(index),\n                     result['Low'].values,\n                     bounds_error=False,\n                     fill_value='extrapolate')\n    \n    highfn = interp1d(date2num(index),\n                      result['High'].values,\n                      bounds_error=False,\n                      fill_value='extrapolate')\n    \n    extended = pd.date_range(start=pd.Timestamp('2020-03-01'),\n                             end=index[-1]+pd.Timedelta(days=1))\n    \n    ax.fill_between(extended,\n                    lowfn(date2num(extended)),\n                    highfn(date2num(extended)),\n                    color='k',\n                    alpha=.1,\n                    lw=0,\n                    zorder=3)\n\n    ax.axhline(1.0, c='k', lw=1, label='$R_t=1.0$', alpha=.25);\n    \n    # Formatting\n    ax.xaxis.set_major_locator(mdates.MonthLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n    ax.xaxis.set_minor_locator(mdates.DayLocator())\n    \n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.1f}\"))\n    ax.yaxis.tick_right()\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.margins(0)\n    ax.grid(which='major', axis='y', c='k', alpha=.1, zorder=-2)\n    ax.margins(0)\n    ax.set_ylim(0.0,3.5)\n    ax.set_xlim(pd.Timestamp('2020-03-01'), result.index.get_level_values('date')[-1]+pd.Timedelta(days=1))\n    fig.set_facecolor('w')\n    \nresults = {}\n\nprovinces_to_process = canada_df.loc[['Waterloo']]\n\nfor prov_name, cases in provinces_to_process.groupby(level='region'):\n    clear_output(wait=True)\n    print(f'Processing {prov_name}')\n    new, smoothed = prepare_cases(cases)\n    print('\\tGetting Posteriors')\n    try:\n        posteriors = get_posteriors(smoothed)\n    except:\n        display(cases)\n    print('\\tGetting HDIs')\n    hdis = highest_density_interval(posteriors)\n    print('\\tGetting most likely values')\n    most_likely = posteriors.idxmax().rename('ML')\n    result = pd.concat([most_likely, hdis], axis=1)\n    results[prov_name] = result.droplevel(0)\n    \nclear_output(wait=True)\nprint('Done.')","e03af6e1":"# fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*3))\nfig, ax = plt.subplots(figsize=(600\/72,400\/72))\n\nplot_rt(result, ax, prov_name)\n\nfig.tight_layout()\nfig.set_facecolor('w')\nax.set_title(f'Real-time $R_t$ for {prov_name}')\nax.set_ylim(.5,3.5)\nax.xaxis.set_major_locator(mdates.WeekdayLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))","6a2d4d25":"waterloo_df = cases_df[((cases_df['province']=='Ontario')&(cases_df['region']=='Waterloo'))]\nwaterloo_cases_df = waterloo_df.groupby(['date'])['id'].count().to_frame()\nwaterloo_cases_rt_df = pd.DataFrame({'ML':result['ML'].values,'Low':result['Low'].values,'High':result['High'].values,'new cases':waterloo_cases_df['id'].values},index=result['ML'].index.get_level_values('date'))\n\n#create dataframe for Waterloo region based on R_t calculated (see plot above for R_t)\nwaterloo_cases_rt_df","c05f4a82":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import TimeseriesGenerator\n\nvalues = waterloo_cases_rt_df.values\nvalues = values.astype(np.float32)\n\nscaler = StandardScaler()\nscaled = scaler.fit_transform(values)\n\nepochs_to_predict = 3 #predict 3 days into the future\nX = scaled[:][:-epochs_to_predict] # remove data from last 3 days\ny = scaled[:,3][epochs_to_predict:] #target\/label column, remove data from first 3 days\n\n#split into train and test sets\ntrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.10, random_state=42, shuffle = False)\n\n#generate time series for training and testing datasets\ntrain_generator = TimeseriesGenerator(trainX, trainY, length=epochs_to_predict, sampling_rate=1, batch_size=epochs_to_predict)\ntest_generator = TimeseriesGenerator(testX, testY, length=epochs_to_predict, sampling_rate=1, batch_size=epochs_to_predict)\n\ntrain_X, train_y = train_generator[0]\ntest_X, test_y = test_generator[0]\n\ntrain_samples = train_X.shape[0]*len(train_generator)\ntest_samples = test_X.shape[0]*len(test_generator)\n\nX_train = np.reshape(trainX, (trainX.shape[0],trainX.shape[1],1))\nX_test = np.reshape(testX, (testX.shape[0],testX.shape[1],1))","abb33e0c":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n#define the architecture of the model\ndef create_model():\n    model = Sequential()\n    model.add(LSTM(32,input_shape=(4,1),return_sequences=True,activation='relu'))\n    model.add(LSTM(64,return_sequences=True))\n    model.add(LSTM(128))\n    model.add(Dense(1))\n    model.summary()\n    return model","f52bb489":"model=create_model()\nlr_reduce =tf.keras.callbacks.ReduceLROnPlateau('val_loss',patience=3,factor=0.3,min_lr=1e-3)\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['mae'])","954f1a27":"#train the model\nmodel.fit(X_train,trainY, epochs=100, batch_size=1, validation_data=(X_test, testY),validation_freq=10,callbacks=[lr_reduce])","3d4e03db":"from math import floor\n#predict\npredicts=[waterloo_cases_rt_df['new cases'][-1]]\nnewdata=scaled.copy()\n\nfor i in range(epochs_to_predict):\n  predict=model.predict(newdata[-4:,3].reshape(1,4,1))\n  predicts.append(floor(abs(predict)))\n  newdata = newdata[:-1,:]\n\n#new predicted cases starting from the second index\nprint(predicts)  ","0c4423fd":"import datetime\nimport numpy as np\n#extend dates by 3 days\na = result['ML'].index.get_level_values('date')\na[-1]+ datetime.timedelta(days=1)\ndate_rng = pd.date_range(start=a[-1], end=a[-1]+datetime.timedelta(days=epochs_to_predict), freq='D')\n\n#plot\nfig, ax = plt.subplots(figsize=(800\/72,600\/72))\nplt.plot(waterloo_cases_df)\nplt.plot(date_rng,predicts)\nplt.gcf().autofmt_xdate()\nplt.legend(['Original', 'Prediction'], loc='best',fontsize=14)\nax.set_title(f'Predicted New Cases Over {epochs_to_predict} Days for {prov_name} Region',fontsize=14)\nplt.xlabel('Days',fontsize=14)\nplt.ylabel('# of New Cases',fontsize=14)","ce2c256f":"Code in this block comes from Alf Whitehead's notebook (https:\/\/www.kaggle.com\/freealf\/estimation-of-rt-from-cases)","e699c204":"Code in this block comes from Alf Whitehead's notebook (https:\/\/www.kaggle.com\/freealf\/estimation-of-rt-from-cases)\n\nIt is claimed in Adam's notebook that a consistent measure of $R_t$ < 1 over a couple of days indicates that the current measures in place to control the spread of covid19 for a region are performing well and that restrictions can begin to ease slowly soon, otherwise, for $R_t$ > 1, it suggests that tighter controls are needed to contain the spread of the virus.\n\nThe red points plotted in the figure below indicate the most likely $R_t$ value calculated, while the grey area around them indicates the standard deviation of those points.","29291f5f":"Code in this block comes from Alf Whitehead's notebook (https:\/\/www.kaggle.com\/freealf\/estimation-of-rt-from-cases)","0ffd9831":"### Conclusion\n\nAlthough our LSTM model is not tuned well (there is quite a bit of loss), we do see a downward trend of new cases that is reflected from the values of $R_t$ calculated and used in training the model. The main constraint in training our model comes from the lack of data points. Shallow layers were used in the building of the model to preserve as much as possible the features from the time series data. Some improvements in tuning the model could be made with more time, but the performance gain will be minimal. If the dataset is small, prediction power will be lacking greatly.\n\nIt is interesting to note from the two plots that both show a downward trend in $R_t$ and new cases, thus it does suggest a strong correlation that makes the $R_t$ metric credible and should be used in assessing whether restrictions can be slowly eased. Note, however the number of cases does not distribute evenly for a population; in the case of Waterloo right now, most of the cases come from long term care homes or retirement homes.","de10c84e":"# Predicting new cases from calculation of $R_t$ in Waterloo Region\n\nThis notebook attempts to predict the number of new cases in the Waterloo region based off the effective reproduction number $R_t$. The calculation and analysis of $R_t$ directly comes from this notebook owned by Alf Whitehead: https:\/\/www.kaggle.com\/freealf\/estimation-of-rt-from-cases; all code\/functions to calculate and visualize $R_t$ are borrowed from that notebook, however, predictions made on new case counts are made by the author of this notebook. The purpose of this excercise is to explore whether the $R_t$ metric can provide a reasonable prediction of new cases in the Waterloo region using a LSTM (long short-term memory) through multivariate time series forecasting. More information about the notion of $R_t$ can be found in this [blog post](http:\/\/systrom.com\/blog\/the-metric-we-need-to-manage-covid-19\/).\n\nData used in this notebook:\n* [Interventions Data](https:\/\/howsmyflattening.ca\/#\/data) from HowsMyFlattening Team\n\n$R_t$ is the effective reproduction number for any time $t$; the current gold standard to determine how contagious an infectious disease is known as the $R_0$ number where $t$ = 0. [Expert epidemiologists](https:\/\/www.nytimes.com\/2020\/04\/06\/opinion\/coronavirus-end-social-distancing.html) claim that the effective reproduction number provides a real-time metric to evaluate current efforts by a region\/country in controlling the pandemic. Adam's notebook goes into details on how $R_t$ is methodologically estimated from daily cases within a region. This metric will be used as additional data in attempting to predict new cases in the Waterloo region using LSTM network."}}