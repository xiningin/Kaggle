{"cell_type":{"4bab72fb":"code","f6524082":"code","0745dead":"code","16fa47ab":"code","c42b53b3":"code","3537766f":"code","91c2f8b7":"code","02a0fb31":"code","20c982af":"code","ffedf6d7":"code","6937e300":"code","0e611506":"code","3fc4d86e":"code","4a8c4563":"code","59b52bf3":"code","5715157a":"code","243f71a5":"code","68eae0dc":"code","edaecc4a":"code","97584113":"code","cfadc446":"code","77fb7c24":"code","4afedb97":"code","737afad5":"code","50e74bae":"code","c1a5471a":"code","3c416baa":"markdown"},"source":{"4bab72fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f6524082":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score","0745dead":"train = pd.read_csv(\"..\/input\/mnist_train.csv\")\ntest = pd.read_csv(\"..\/input\/mnist_test.csv\")","16fa47ab":"print(train.shape)\nprint(test.shape)\nprint(train.columns)\nprint(train[:2])\nprint(test[:2])\nprint(train[:10][\"label\"])\nprint(test[:10][\"label\"])","c42b53b3":"train = np.array(train)\ntest = np.array(test)","3537766f":"print(train[10:15])\nprint(test[10:15])","91c2f8b7":"train_x = train[:,1:]\ntrain_y = pd.get_dummies(train[:,0])\ntest_x = test[:,1:]\ntest_y = pd.get_dummies(test[:,0])","02a0fb31":"print(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\nprint(train_x[10:12])\nprint(train_y[10:12])\nprint(test_x[10:12])\nprint(test_y[10:12])","20c982af":"#NETWORK PARAMETERS\nn_steps = 28\nn_inputs = 28\nn_neurons = 150\nn_outputs = 10","ffedf6d7":"train_x = train_x.reshape(-1,n_steps,n_inputs)\ntest_x = test_x.reshape(-1,n_steps,n_inputs)","6937e300":"print(train_x.shape)\nprint(test_x.shape)\nprint(train_x[0:1])\nprint(test_x[0:1])","0e611506":"train_X , train_y = shuffle(train_x , train_y)\ntest_X , test_y = shuffle(test_x , test_y)","3fc4d86e":"tf.reset_default_graph()","4a8c4563":"# Training Parameters\nlearning_rate = 0.001\ntraining_iters = 100\nbatch_size = 150\ndisplay_step = 200","59b52bf3":"X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\ny = tf.placeholder(tf.float32,[None,n_outputs])","5715157a":"def RNN(X):\n    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n    outputs , states = tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n    out = tf.layers.dense(states, n_outputs)\n    out = tf.nn.softmax(out)\n    return out\n    ","243f71a5":"prediction = RNN(X)","68eae0dc":"# Define loss and optimizer\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)","edaecc4a":"# Evaluate model (with test logits, for dropout to be disabled)\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()","97584113":"sess = tf.Session()\nsess.run(init) \n\nfor i in range(training_iters):\n    for batch in range(len(train_X)\/\/batch_size):\n        batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n        batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n\n        sess.run(train_op, feed_dict={X: batch_x, y: batch_y})\n            # Calculate batch loss and accuracy\n        loss = sess.run([loss_op], feed_dict={X: batch_x, y: batch_y})\n    \n    predTest = sess.run(prediction , feed_dict={X:test_X})\n\n    p = np.argmax(predTest,1)\n    t = np.argmax(np.array(test_y),1)\n\n    acc = accuracy_score(p,t)\n    print(\"Iter \"+str(i)+\" Out of\",training_iters , \" Loss= \",loss, \"acc=\",acc )\n            \n#     acc = sess.run([accuracy], feed_dict={X: batch_x, y: batch_y})\n        \n#     print(\"Step \" + str(i) + \",        Batch Loss= \",loss, \",       Training Accuracy= \",acc)\n    \nprint(\"Optimization Finished!\")","cfadc446":"while(True):\n    r = np.random.randint(9000)\n    test_img = np.reshape(test_X[r], (28,28))\n    plt.imshow(test_img, cmap=\"gray\")\n    test_pred = sess.run(prediction, feed_dict = {X:[test_X[r]]})\n    print(\"Model : I think it is :    \",np.argmax(test_pred))\n    plt.show()\n    \n    if input(\"Enter n to exit\")=='n':\n        break\nclear_output();","77fb7c24":"wrong = test_X[t!=p]\nwrong.shape","4afedb97":"a,b,c = wrong.shape","737afad5":"while(True):\n    r=np.random.randint(a)\n    plt.imshow(wrong[r].reshape((28,28)),cmap=\"gray\")\n    test_pred_1=sess.run(prediction, feed_dict = {X:[wrong[r]]})\n    print(\"Model : I think it is :    \",np.argmax(test_pred_1))\n    plt.show()\n    \n    if input(\"Enter n to exit\")=='n':\n        break\nclear_output();","50e74bae":"p = np.argmax(predTest,1)\nprint(p)\nt = np.argmax(np.array(test_y),1)\nprint(t)\nacc = accuracy_score(p,t)\nprint(acc*100)","c1a5471a":"print(\"Saving Weights\")\nsaver = tf.train.Saver()\nsaver.save(sess,\"weights_\"+str(i)+\"\/weights.ckpt\")\nprint(\"Weights Saved\")","3c416baa":"#### We will treat each image as a sequence of 28 rows of 28 pixels each (since each MNIST image is 28 \u00d7 28 pixels). We will use cells of 150 recurrent neurons, plus a fully connected layer containing 10 neurons (one per class) connected to the output of the last time step, followed by a softmax layer"}}