{"cell_type":{"f14cfecf":"code","3407a59e":"code","2bebaeb8":"code","3cc81ba7":"code","a17e5d9a":"code","3a60650c":"code","d08406be":"code","d522ae67":"code","64798fa9":"code","490385f3":"code","6b2dedcc":"code","717a0962":"code","f0fb68e0":"code","5d1bb214":"code","dd3f5491":"code","73a2f683":"code","84e41740":"code","e5c7dd0d":"code","d92b5c7b":"code","63e4af04":"code","dbcd183a":"code","a3ef36fa":"code","4a3ff68e":"code","34f693b0":"code","90b995e2":"code","f05d9a6c":"code","69033071":"code","e07d7c4e":"code","f10fae3a":"code","e7f2eefa":"code","6a0151de":"code","3fac1763":"code","def83c96":"markdown","97303bcd":"markdown","428b141c":"markdown","475cd19d":"markdown","5f1150e0":"markdown","a765c9f2":"markdown","18d2b0f2":"markdown","fa6f1878":"markdown","ef709f6f":"markdown","7a0ad213":"markdown","e854feda":"markdown","bfaaf057":"markdown","2dba9052":"markdown","76da9be6":"markdown","fd05ce74":"markdown","47348376":"markdown","ab33b6c4":"markdown","884fa0c5":"markdown","c7236f09":"markdown","6ce1caf3":"markdown","c0f7e677":"markdown","ae186a06":"markdown","2613baa2":"markdown","be1a3ed6":"markdown","c6fc8676":"markdown","7c19fe28":"markdown","e9619c0b":"markdown","39e1fd63":"markdown","8a212351":"markdown","2f5256e2":"markdown","b0f4dfa4":"markdown","c18baa84":"markdown","d6be8527":"markdown","9a477223":"markdown","5cbbdb41":"markdown","7f5697af":"markdown","12089770":"markdown"},"source":{"f14cfecf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nsns.set() #make the graphs prettier","3407a59e":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ndata_cleaner = [train, test]","2bebaeb8":"train","3cc81ba7":"sns.countplot('Survived',data=train)","a17e5d9a":"plt.figure(figsize=(10,8))\nsns.heatmap(train.corr(),cmap='coolwarm',annot=True)","3a60650c":"sns.countplot('Sex',hue='Survived',data=train)","d08406be":"sns.countplot('Pclass',hue='Survived',data=train)","d522ae67":"for data in data_cleaner:\n    print(data.isnull().sum())\n    print('\\n')","64798fa9":"for data in data_cleaner:\n    plt.figure(figsize=(8,6))\n    sns.heatmap(data.isnull(),cmap='viridis')","490385f3":"sns.boxplot(x='Pclass',y='Age',data=train)","6b2dedcc":"age_ref = pd.DataFrame(data=[train.groupby('Pclass')['Age'].mean()],columns=train['Pclass'].unique())\nage_ref","717a0962":"def fill_age(pclass,age):\n    if pd.isnull(age):\n        return float(age_ref[pclass])\n    else:\n        return age\n\nfor data in data_cleaner:\n    data['Age'] = train.apply(lambda x: fill_age(x['Pclass'],x['Age']), axis=1)","f0fb68e0":"def fill_fare(fare):\n    if pd.isnull(fare):\n        return train['Fare'].mean()\n    else:\n        return fare\n    \ndef fill_embark(embark):\n    if pd.isnull(embark):\n        return train['Embarked'].mode().iloc[0]\n    else:\n        return embark\n    \nfor data in data_cleaner:\n    data['Fare'] = train.apply(lambda x: fill_fare(x['Fare']), axis=1)\n    data['Embarked'] = train.apply(lambda x: fill_embark(x['Embarked']), axis=1)","5d1bb214":"for data in data_cleaner:\n    data.drop(['Cabin'],axis=1,inplace=True)","dd3f5491":"for data in data_cleaner:\n    print(data.isnull().sum())\n    print('\\n')","73a2f683":"train","84e41740":"train['Name']","e5c7dd0d":"title_list = list()\nfor data in data_cleaner:\n    for title in data['Name']:\n        title = title.split('.')[0].split(',')[1]\n        title_list.append(title)\n    \n    data['Title'] = title_list\n    title_list = list()","d92b5c7b":"for data in data_cleaner:\n    print(data['Title'].value_counts())\n    print('\\n')","63e4af04":"train['Title'] = train['Title'].replace([ ' Don', ' Rev', ' Dr', ' Mme',' Ms', ' Major', ' Lady', ' Sir', ' Mlle', ' Col', ' Capt',\n       ' the Countess', ' Jonkheer'], 'Others')\ntrain['Title'].value_counts()","dbcd183a":"test['Title'] = test['Title'].replace([ ' Don', ' Rev', ' Dr', ' Mme',' Ms', ' Major', ' Lady', ' Sir', ' Mlle', ' Col', ' Capt',\n       ' the Countess', ' Jonkheer',' Dona'], 'Others')\ntest['Title'].value_counts()","a3ef36fa":"sns.catplot(x=\"SibSp\",kind=\"count\", data=train, height=4.7, aspect=2.45)","4a3ff68e":"sns.catplot(x=\"SibSp\", y=\"Survived\", kind=\"bar\", data=train, height=4, aspect=3).set_ylabels(\"Survival Probability\")","34f693b0":"sns.catplot(x=\"Parch\", y='Survived',kind=\"bar\", data=train, height=4.5, aspect=2.5)","90b995e2":"def get_size(df):\n    if df['SibSp'] + df['Parch'] + 1 == 1:\n        return 'Single'\n    if df['SibSp'] + df['Parch'] + 1 > 1:\n        return 'Small'\n    if df['SibSp'] + df['Parch'] + 1 > 4:\n        return 'Big'\n    \nfor data in data_cleaner:\n    data['FamilySize'] = data.apply(get_size,axis=1)\n\nfor data in data_cleaner:\n    data['IsAlone'] = 1 \n    data['IsAlone'].loc[data['FamilySize'] != 'Single'] = 0","f05d9a6c":"sex = pd.get_dummies(train['Sex'],drop_first=True)\nembark = pd.get_dummies(train['Embarked'],drop_first=True)\ntitle = pd.get_dummies(train['Title'],drop_first=True)\nPclass = pd.get_dummies(train['Pclass'],drop_first=True)\nFamilySize = pd.get_dummies(train['FamilySize'],drop_first=True)\n\nsex2 = pd.get_dummies(test['Sex'],drop_first=True)\nembark2 = pd.get_dummies(test['Embarked'],drop_first=True)\ntitle2 = pd.get_dummies(test['Title'],drop_first=True)\nPclass2 = pd.get_dummies(test['Pclass'],drop_first=True)\nFamilySize2 = pd.get_dummies(test['FamilySize'],drop_first=True)\n\nfor data in data_cleaner:\n    data.drop(['Sex','Embarked','Name','Ticket','PassengerId','Title','FamilySize'],axis=1,inplace=True)\n    \ntrain = pd.concat([sex,embark,train,title,FamilySize],axis=1)\ntest = pd.concat([sex2,embark2,test,title2,FamilySize2],axis=1)","69033071":"X = train.drop('Survived',axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","e07d7c4e":"scaler = MinMaxScaler()\n\nscaler.fit(X_train)\n\nscaler.transform(X_train)\nscaler.transform(X_test)\nscaler.transform(test)","f10fae3a":"logistic_model = LogisticRegression()\n\nlogistic_model.fit(X_train, y_train)\n\ny_pred = logistic_model.predict(X_test)","e7f2eefa":"print(classification_report(y_test,y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test,y_pred))","6a0151de":"sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap='plasma')","3fac1763":"predictions = logistic_model.predict(test)\npred_list = [int(x) for x in predictions]\n\ntest2 = pd.read_csv(\"..\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId': test2['PassengerId'], 'Survived': pred_list})\noutput.to_csv('Titanic_with_logistic.csv', index=False)","def83c96":"Taking a closer look at the Name, it can be easily observed that each name has a special title encapsulated within it, which can tell a great deal about a person, like his or hers social status, mattering a lot about hers or his survival.","97303bcd":"It seems that most travellers travel as single.","428b141c":"Okay, that's pretty much it.\n \nUndoubtedly, there are still tons of places to improve, from feature engineering to tuning the parameters. I will really appreciate for any comment below for advice and suggestions, or just a thanks.","475cd19d":"Presented in the visualiztion below, the survival chance of a passenger with 1 or 2 siblings\/spouses and 1,2 or 3 parents\/children is significantly higher than than for a single passenger or a passenger with a large family.","5f1150e0":"Inspired by such features, we can add another FamilySize column and IsAlone column.","a765c9f2":"Next, let's deal with Fare and Embark. Since they both only miss a few, so we may simply fill the fare based on the mean and Embark on the mode.","18d2b0f2":"# 7. Model evaluation","fa6f1878":"Let's visualize those missing values in a heatmap.","ef709f6f":"For this part, we will evaluate our model using classification report and confusion matrix. Then, let's save our prediction into a csv. file and be ready for the submission!\n\nAs you can see, this model performs amazingly and surprisingly well for such amount of code, about 82% accuracy, which leads us to top 4%!!!","7a0ad213":"# **Content","e854feda":"# # Simple Logistic with Titanic - 81.3% test score: Quick yet effective For Beginners\n\n","bfaaf057":"# 5. Data preparation for modelling","2dba9052":"Hi there! \nI am totally new to data science and machine learning, and it is my very first kernal!\n\nThis kernal is really straightforward yet still get a pretty decent score, currently ranking top8%.\n\nFeel free to point out any mistakes or places to improve -- I am thrilled to learn more!","76da9be6":"**Train Test Split","fd05ce74":"#  3. Data cleaning","47348376":"# 1. Import libraries and data","ab33b6c4":"Some titles only have a handful of occurences, so we can replace them as Other.","884fa0c5":"First, let's deal with the Age column.\nFrom the EDA above and boxplot below, we can tell that Age is correlated with Pclass, which makes sense, since most rich people (in first class) tend to be older. So we will fill the age based on the class that person is in.","c7236f09":"After trying several different models and even Artificial Neural Network(ANN) with extensive hyperparameter analysis, it turns out that the logistic regression performs the best, which is extremely simple but effective.\n\nYou may consider to tune the parameter of the logistic regression, but for now, we just leave it default.","6ce1caf3":"**2. Family size**\n","c0f7e677":"Since there are already tons of brilliant kernals for EDA about this dataset, so this kernel will not go into much depth. Instead, we will focus more on the missing values and other useful relationships among data.","ae186a06":"Revisiting our training data, we will try to extract some features based on the existing coloums, a.k.a, feature engineering.\n\n**1. Title**\n\nFor the categorial column, it seems that we gain some useful information from the Name column, but for the Ticket column, there is really not much to do(you may explore it if you want, but here, let's just drop it)\n\n","2613baa2":"# 6. Modelling: Logistic regression","be1a3ed6":"1. Import libraries and data\n2. Exploratory data analysis(EDA)\n3. Data cleaning \n4. Feature engineering\n5. Data preparation for modelling\n6. Modelling: Logistic regression\n7. Model evaluation","c6fc8676":"As for the Cabin, it's too much to fill, so we can just remove them all.","7c19fe28":"**Scale our data\n\nRemember not to fit our X_test and final test set, in order to prevent data leakage.","e9619c0b":"Such process isn't really necessary, since titanic has a relatively small dataset. But we will still include it so that we can better validate our model.","39e1fd63":"Okay, we've roughly finished cleaning our data!","8a212351":"In this dataset, we have some missing values. Some of them only misses a small quantity, such as Embarked, while others are almost completely lost, such as Cabin. \nGiven the situation and for the sake of simplicity, we will fill in reasonable values for those missing a few and just drop others.","2f5256e2":"# 2. Exploratory data analysis(EDA)","b0f4dfa4":"First, let's convert the categorical data into dummy variables, split the data into train set and test set, and scale the data using MinMaxScaler to feed our model.","c18baa84":"Some common python libraries for data analysis and plotting are imported - you already can tell how straightforward the model is by the number of libraries imported.\n\nA data_cleaner is created so that it will be more convenient to clean and fill the data.","d6be8527":"**Get dummies","9a477223":"# Please upvote this kernal if you find it helpful!! \n# Thank you so much!!!","5cbbdb41":"Let's first take a look at the train dataset and then learn the correlations between 'Survived' column and other columns ","7f5697af":"Let's extract some grab the title out by using the quick and dirty split() method and then add a column called Title.","12089770":"# 4. Feature engineering"}}