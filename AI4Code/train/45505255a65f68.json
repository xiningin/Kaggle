{"cell_type":{"0fd14f47":"code","e08ec878":"code","b615b859":"code","e6190601":"code","0edaa707":"code","28bff4e6":"code","edaac9dc":"code","57925b00":"code","f486c41e":"code","b7d5f639":"code","e6d9467c":"code","83fd601d":"markdown","4117ec7b":"markdown","db80c932":"markdown","7b510689":"markdown","ed2ebce5":"markdown","7f2561f6":"markdown","795992e7":"markdown","b6ca427a":"markdown","b2e322b3":"markdown","4426849e":"markdown","acf79863":"markdown","1f41c0f7":"markdown","3b3ad1d8":"markdown","eb065f51":"markdown","05ddab78":"markdown","21a9c610":"markdown"},"source":{"0fd14f47":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf","e08ec878":"data_train = pd.read_csv('..\/input\/train.csv')\ndata_test = pd.read_csv('..\/input\/test.csv')\n\nprint(f\"There are {len(data_train)} example images.\")\ndata_train.head()","b615b859":"x, y = data_train.drop('label', axis=1), data_train['label']\n\nx_train = x[1000:]\ny_train = y[1000:]\n\nx_validate = x[:1000]\ny_validate = y[:1000]\n\nx_test = data_test","e6190601":"example_image, example_label = x_train.iloc[0], y_train.iloc[0]\npixels = int(example_image.size ** 0.5)\nplt.imshow(np.reshape(example_image.values, [pixels, pixels]))\nplt.title(f\"Image of the number {example_label}\")\nplt.show()","0edaa707":"# model = tf.keras.models.Sequential([\n#     tf.keras.layers.InputLayer(input_shape=(pixels * pixels,), name = 'Block0-Input'),\n#     tf.keras.layers.Reshape((pixels, pixels, 1), name = 'Block0-Reshape'),\n#     # Pad, Convolve, Batch Normalize, ReLU\n#     tf.keras.layers.ZeroPadding2D((3, 3), name = 'Block1-Padding'),\n#     tf.keras.layers.Conv2D(32, (7, 7), strides = (1, 1), name = 'Block1-Convolution'),\n#     tf.keras.layers.BatchNormalization(axis = 3, name = 'Block1-Normalization'),\n#     tf.keras.layers.Activation('relu', name = 'Block1-Activation'),\n#     # Convolve, Batch Normalize, ReLU, Pool    \n#     tf.keras.layers.Conv2D(32, (7, 7), strides = (1, 1), name = 'Block2-Convolution'),\n#     tf.keras.layers.BatchNormalization(axis = 3, name = 'Block2-Normalization'),\n#     tf.keras.layers.Activation('relu', name = 'Block2-Activation'),\n#     tf.keras.layers.MaxPooling2D((2, 2), name= 'Block2-MaxPool'),\n#     # Flatten, Fully Connected, Sigmoid\n#     tf.keras.layers.Flatten(name = 'Block3-Flatten'),\n#     tf.keras.layers.Dense(10, activation='softmax', name='Block3-Dense'),\n# ])\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.InputLayer(input_shape=(pixels * pixels,), name = 'Block0-Input'),\n    tf.keras.layers.Dense(1000, activation='relu', name='Block1-Dense'),\n    tf.keras.layers.Dense(2000, activation='relu', name='Block2-Dense'),\n    tf.keras.layers.Dense(1000, activation='relu', name='Block3-Dense'),\n    tf.keras.layers.Dense(500, activation='relu', name='Block4-Dense'),\n    tf.keras.layers.Dense(500, activation='relu', name='Block5-Dense'),\n    tf.keras.layers.Dense(10, activation='softmax', name='Block6-Output'),\n])\n\nmodel.summary()","28bff4e6":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","edaac9dc":"model.fit(x = x_train, y = y_train, epochs=10, batch_size=20, validation_data=(x_validate, y_validate))","57925b00":"results = model.predict(x_test)\npredictions = np.argmax(results, axis=1)\nprint(x_test.shape, predictions.shape)","f486c41e":"with open('submission.csv', 'w') as file:\n    file.write('ImageId,Label\\n')\n    for i in range(len(predictions)):\n        file.write(str(i+1) + ',' + str(predictions[i]) + '\\n')","b7d5f639":"predictions_validate = np.argmax(model.predict(x_validate), axis=1)\nincorrects = np.reshape(np.nonzero(predictions_validate != y_validate), [-1])\nprint(\"You got\", len(incorrects), \"images wrongly classified in Validation.\")\nfigure, axes = plt.subplots(4, 4, figsize=(15, 15))\naxes = np.reshape(axes, (-1))\nfor i in range(min(len(incorrects), 16)):\n    axes[i].imshow(np.reshape(x_validate.iloc[incorrects[i]].values, [pixels, pixels]), cmap='hot')\n    axes[i].set_title(str(y_validate.iloc[incorrects[i]]) + \" was classified as \" + str(predictions_validate[incorrects[i]]))","e6d9467c":"# Plot training & validation accuracy values\nplt.plot(model.history.history['acc'], color='IndianRed')\nplt.plot(model.history.history['val_acc'], color='RoyalBlue')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(model.history.history['loss'], color='IndianRed')\nplt.plot(model.history.history['val_loss'], color='RoyalBlue')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","83fd601d":"Now let's see how our model learnt what it did.","4117ec7b":"## Storing the Answers","db80c932":"This is how to make the model. Our model is very simple, it just has Dense (aka. Fully Connected) layers. The names are optional, just for our convinience. ReLU is used as activation inside and softmax at the output (since it's like applying a sigmoid to many outputs and then normalizing it, not exactly though), so we have our 10 outputs now.","7b510689":"Given the data, it's time to split it into training and validation dataset. We also need to separate the X and y values and store them as arrays. With this, our data preprocessing step is done.\n\nThe `df.drop('column_name', axis=1)` command removes one column, so we will have the remaining columns which are the values of the pixels for the image.\nThe `df['column_name']` just gets that one label since we need to use it as the correct output value.\nThen we do indexing, the first 1000 examples are called the validation examples and all others are used as training.","ed2ebce5":"## Defining the Model","7f2561f6":"# More Visualizations","795992e7":"Let's write the answers that we have into the `submissions.csv` file.","b6ca427a":"Let's see a few images of the digits our model got wrong.","b2e322b3":"We keep **Stochastic Gradient Descent** as out optimizer.\n**Sparse Categorical Entropy** as the loss function, since only one of the 10 logits will turn out to be correct.\nWe want to keep a track of **Percentage Accuracy**, that is just for our benefit, so that we can see it improve.","4426849e":"Let's just see our image, the first image in our training set. We are using `plt.imshow` to convert the array of numbers into an actual digit. Before we show the image, we convert the array of size `(784,)` to an image of size `(28, 28)` so that it actually looks like an image to matplotlib.","acf79863":"Finally, let's actual fit, for 3 epochs (i.e. 3 times repetition) over all the 41000 training images, and keep testing over the 1000 validation images. The backward pass will not happen on the validation images.","1f41c0f7":"# Classifying Images of Digits","3b3ad1d8":"First, let's start by importing some libraries.\n* Numpy: The library for managing arrays fast (in C++, accelerated processing), and using normal math notation to process arrays. \n* Pandas: Built on top of numpy, this helps in getting some more spreadsheet like behavior, managing CSV files, keeping column headings, etc.\n* Matplotlib: Plotting library, normal graphs and showing images, everything.\n* Tensorflow: Our big machine learning library, which has the keras library as a submodule.","eb065f51":"We can just call `model.predict` to get all our predictions. Taking `np.argmax` gives us the value of the digit, since the output of the model was a vector of confidences. Eg. `[0, 0, 0.7, 0.3, 0, 0, 0, 0, 0, 0]` implies that the model thinks that an image has `70%` probability of being a 2 and a `30%` change of being a `3`. The `np.argmax` will return 2, the max probability number.","05ddab78":"Pandas can read CSV files (comma separated values, these are files where there are many rows, and each row has all the columns separated by commas, so like a single sheet in a spreadsheet software). Having read it, it converts it into a format that is easy to make an array out of, and is easy to print and analyze and generally play around with.\n\nYou can see that by using the `df.head()` command, we can see the first 5 columns as an example of what the dataset looks like.","21a9c610":"## Data Preprocessing"}}