{"cell_type":{"e5f5fcd9":"code","ecc7816c":"code","4c42f078":"code","6c9fa3b1":"code","37587500":"code","d61a6341":"code","96223a0a":"code","e19c8566":"code","1e663942":"code","250f2811":"code","073aeaec":"code","dbff4139":"code","efbb2298":"code","629b8a62":"code","33e51226":"code","0a9d184b":"code","94ce2e2e":"code","a35124f3":"code","6ac291fc":"code","f7ebe424":"code","073e30a8":"code","611292b0":"code","eb92a0e2":"code","fc8d9cf1":"code","14e289b4":"code","61591571":"code","9dde55a1":"code","d0844523":"code","f3c09fd9":"code","54a9d74b":"code","3d344ff2":"code","db25c397":"code","465d9f26":"markdown"},"source":{"e5f5fcd9":"!cp -r ..\/input\/efficientnetpytorch\/ .\/efficientnetpytorch\n!pip install .\/efficientnetpytorch\/\n!rm -r .\/efficientnetpytorch","ecc7816c":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import init\nimport torchvision\nfrom torchvision import transforms\nimport albumentations as A\nfrom efficientnet_pytorch import EfficientNet\nimport gc\nimport cv2\nfrom tqdm import tqdm\nimport sklearn.metrics\nimport json\nimport functools\nimport itertools\nimport random","4c42f078":"MEAN = [0.5, 0.5, 0.5]\nSTD = [0.5, 0.5, 0.5]\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nBATCH_SIZE = 8\nEPOCH = 1\nTQDM_DISABLE = True","6c9fa3b1":"device = torch.device(\"cuda\")","37587500":"def load_images(paths):\n    all_images = []\n    for path in paths:\n        image_df = pd.read_parquet(path)\n        images = image_df.iloc[:, 1:].values.reshape(-1, 137, 236).astype(np.uint8)\n        del image_df\n        gc.collect()\n        all_images.append(images)\n    all_images = np.concatenate(all_images)\n    return all_images","d61a6341":"font_data = pd.read_csv('..\/input\/bengaliai-cv19-font\/font.csv')\nfont_images = load_images([\n    '..\/input\/bengaliai-cv19-font\/font_image_data_0.parquet',\n    '..\/input\/bengaliai-cv19-font\/font_image_data_1.parquet',\n    '..\/input\/bengaliai-cv19-font\/font_image_data_2.parquet',\n    '..\/input\/bengaliai-cv19-font\/font_image_data_3.parquet',\n])\nnp.save('font_images.npy', font_images)\ndel font_images\ngc.collect()","96223a0a":"train_data = pd.read_csv('..\/input\/bengaliai-cv19\/train.csv')\nmulti_diacritics_train_data = pd.read_csv('..\/input\/bengaliai-cv19\/train_multi_diacritics.csv')\ntrain_data = train_data.set_index('image_id')\nmulti_diacritics_train_data = multi_diacritics_train_data.set_index('image_id')\ntrain_data.update(multi_diacritics_train_data)\ntrain_images = load_images([\n    '..\/input\/bengaliai-cv19\/train_image_data_0.parquet',\n    '..\/input\/bengaliai-cv19\/train_image_data_1.parquet',\n    '..\/input\/bengaliai-cv19\/train_image_data_2.parquet',\n    '..\/input\/bengaliai-cv19\/train_image_data_3.parquet',\n])","e19c8566":"font_images = np.load('font_images.npy')","1e663942":"!rm .\/font_images.npy","250f2811":"class GraphemeDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, data, images, transform=None, num_grapheme_root=168, num_vowel_diacritic=11, num_consonant_diacritic=8):\n        self.data = data\n        self.grapheme_root_list = np.array(data['grapheme_root'].tolist(), dtype=np.int64)\n        self.vowel_diacritic_list = np.array(data['vowel_diacritic'].tolist(), dtype=np.int64)\n        self.consonant_diacritic_list = np.array(data['consonant_diacritic'].tolist(), dtype=np.int64)\n        self.num_grapheme_root = num_grapheme_root\n        self.num_vowel_diacritic = num_vowel_diacritic\n        self.num_consonant_diacritic = num_consonant_diacritic\n        self.images = images\n        self.transform = transform\n            \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        grapheme_root = self.grapheme_root_list[idx]\n        vowel_diacritic = self.vowel_diacritic_list[idx]\n        consonant_diacritic = self.consonant_diacritic_list[idx]\n        label = (grapheme_root*self.num_vowel_diacritic+vowel_diacritic)*self.num_consonant_diacritic+consonant_diacritic\n        np_image = self.images[idx].copy()\n        out_image = self.transform(np_image)\n        return out_image, label\n    ","073aeaec":"class Albumentations:\n    def __init__(self, augmentations):\n        self.augmentations = A.Compose(augmentations)\n    \n    def __call__(self, image):\n        image = self.augmentations(image=image)['image']\n        return image\n        ","dbff4139":"preprocess = [\n    A.CenterCrop(height=137, width=IMG_WIDTH),\n    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n]\n\naugmentations = [\n    A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], always_apply=True),\n    A.imgaug.transforms.IAAAffine(shear=5, mode='constant', cval=255, always_apply=True),\n    A.ShiftScaleRotate(rotate_limit=5, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], mask_value=[255, 255, 255], always_apply=True),\n    A.RandomCrop(height=IMG_HEIGHT, width=IMG_WIDTH, always_apply=True),\n]\n\n\ntrain_transform = transforms.Compose([\n    np.uint8,\n    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n    np.uint8,\n    Albumentations(preprocess + augmentations),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD),\n#     transforms.ToPILImage(),\n])\nvalid_transform = transforms.Compose([\n    np.uint8,\n    transforms.Lambda(lambda x: np.array([x, x, x]).transpose((1, 2, 0)) ),\n    np.uint8,\n    Albumentations(preprocess),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD),\n#     transforms.ToPILImage(),\n])","efbb2298":"hand_dataset = GraphemeDataset(train_data, train_images, valid_transform)\nfont_dataset = GraphemeDataset(font_data, font_images, train_transform)","629b8a62":"class ResnetGenerator(nn.Module):\n    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling\/upsampling operations.\n    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https:\/\/github.com\/jcjohnson\/fast-neural-style)\n    \"\"\"\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        \"\"\"Construct a Resnet-based generator\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):       # add ResNet blocks\n\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult \/ 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult \/ 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        \"\"\"Standard forward\"\"\"\n        return self.model(input)\n\n\nclass ResnetBlock(nn.Module):\n    \"\"\"Define a Resnet block\"\"\"\n\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Initialize the Resnet block\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https:\/\/arxiv.org\/pdf\/1512.03385.pdf\n        \"\"\"\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        \"\"\"Construct a convolutional block.\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        \"\"\"\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        \"\"\"Forward function (with skip connections)\"\"\"\n        out = x + self.conv_block(x)  # add skip connections\n        return out","33e51226":"class NLayerDiscriminator(nn.Module):\n    \"\"\"Defines a PatchGAN discriminator\"\"\"\n\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        \"\"\"Construct a PatchGAN discriminator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        \"\"\"\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        \"\"\"Standard forward.\"\"\"\n        return self.model(input)","0a9d184b":"def init_weight(net, init_gain):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            init.normal_(m.weight.data, 0.0, init_gain)\n            if hasattr(m, 'bias'):\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n    net.apply(init_func)\n","94ce2e2e":"class ImagePool():\n    \"\"\"This class implements an image buffer that stores previously generated images.\n    This buffer enables us to update discriminators using a history of generated images\n    rather than the ones produced by the latest generators.\n    \"\"\"\n\n    def __init__(self, pool_size):\n        \"\"\"Initialize the ImagePool class\n        Parameters:\n            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n        \"\"\"\n        self.pool_size = pool_size\n        if self.pool_size > 0:  # create an empty pool\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        \"\"\"Return an image from the pool.\n        Parameters:\n            images: the latest generated images from the generator\n        Returns images from the buffer.\n        By 50\/100, the buffer will return input images.\n        By 50\/100, the buffer will return images previously stored in the buffer,\n        and insert the current images to the buffer.\n        \"\"\"\n        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:       # by another 50% chance, the buffer will return the current image\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)   # collect all the images and return\n        return return_images","a35124f3":"class GANLoss(nn.Module):\n    \"\"\"Define different GAN objectives.\n    The GANLoss class abstracts away the need to create the target label tensor\n    that has the same size as the input.\n    \"\"\"\n\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n        \"\"\" Initialize the GANLoss class.\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        \"\"\"\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        self.gan_mode = gan_mode\n        if gan_mode == 'lsgan':\n            self.loss = nn.MSELoss()\n        elif gan_mode == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode in ['wgangp']:\n            self.loss = None\n        else:\n            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n\n    def get_target_tensor(self, prediction, target_is_real):\n        \"\"\"Create label tensors with the same size as the input.\n        Parameters:\n            prediction (tensor) - - tpyically the prediction from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n        Returns:\n            A label tensor filled with ground truth label, and with the size of the input\n        \"\"\"\n\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(prediction)\n\n    def __call__(self, prediction, target_is_real):\n        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n        Parameters:\n            prediction (tensor) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n        Returns:\n            the calculated loss.\n        \"\"\"\n        if self.gan_mode in ['lsgan', 'vanilla']:\n            target_tensor = self.get_target_tensor(prediction, target_is_real)\n            loss = self.loss(prediction, target_tensor)\n        elif self.gan_mode == 'wgangp':\n            if target_is_real:\n                loss = -prediction.mean()\n            else:\n                loss = prediction.mean()\n        return loss","6ac291fc":"class BengalModel(nn.Module):\n    def __init__(self, backbone, hidden_size=2560, class_num=168*11*7):\n        super(BengalModel, self).__init__()\n        self.backbone = backbone\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(hidden_size, class_num)\n        self.ln = nn.LayerNorm(hidden_size)\n\n        \n    def forward(self, inputs):\n        bs = inputs.shape[0]\n        feature = self.backbone.extract_features(inputs)\n        feature_vector = self._avg_pooling(feature)\n        feature_vector = feature_vector.view(bs, -1)\n        feature_vector = self.ln(feature_vector)\n\n        out = self.fc(feature_vector)\n        return out   \n    ","f7ebe424":"norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\ngenerator_a = ResnetGenerator(input_nc=3, output_nc=3, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\ngenerator_b = ResnetGenerator(input_nc=3, output_nc=3, ngf=64, norm_layer=norm_layer, use_dropout=False, n_blocks=9)\n\ndiscriminator_a = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=norm_layer)\ndiscriminator_b = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=norm_layer)\nbackbone = EfficientNet.from_name('efficientnet-b0')\nclassifier = BengalModel(backbone, hidden_size=1280, class_num=168*11*8)\ninit_weight(generator_a, 0.02)\ninit_weight(generator_b, 0.02)\ninit_weight(discriminator_a, 0.02)\ninit_weight(discriminator_b, 0.02)","073e30a8":"discriminator_loss = GANLoss('lsgan', target_real_label=1.0, target_fake_label=0.0)\nclassifier_loss = nn.CrossEntropyLoss()","611292b0":"classifier.load_state_dict(torch.load('..\/input\/cyclegan-classifier-results\/best.pth'))","eb92a0e2":"class CycleGan(nn.Module):\n    \n    def __init__(self, \n                 generator_a, generator_b, discriminator_a, discriminator_b, classifier, \n                 discriminator_loss, classifier_loss, \n                 lambda_a, lambda_b, lambda_cls,\n                 device):\n        super(CycleGan, self).__init__()\n        self.generator_a = generator_a\n        self.generator_b = generator_b\n        self.discriminator_a = discriminator_a\n        self.discriminator_b = discriminator_b\n        self.classifier = classifier.eval()\n        CycleGan.set_requires_grad(self.classifier, requires_grad=False)\n        self.discriminator_loss = discriminator_loss\n        self.classifier_loss = classifier_loss\n        self.reconstruct_loss = nn.L1Loss()\n        self.device = device\n        \n        self.image_pool_a = ImagePool(50)\n        self.image_pool_b = ImagePool(50)\n        \n        self.lambda_a = lambda_a\n        self.lambda_b = lambda_b\n        self.lambda_cls = lambda_cls\n        \n        self.real_images_a = None\n        self.real_images_b = None\n        self.labels_a = None\n        self.labels_b = None\n        self.fake_images_a = None\n        self.fake_images_b = None\n        self.rec_images_a = None\n        self.rec_images_b = None\n        self.generator_a = torch.nn.DataParallel(self.generator_a)\n        self.generator_b = torch.nn.DataParallel(self.generator_b)\n        self.discriminator_a = torch.nn.DataParallel(self.discriminator_a)\n        self.discriminator_b = torch.nn.DataParallel(self.discriminator_b)\n        self.to(device)\n        \n    def forward(self):\n        self.fake_images_a = self.generator_a(self.real_images_b)\n        self.fake_images_b = self.generator_b(self.real_images_a)\n        self.rec_images_a = self.generator_a(self.fake_images_b)\n        self.rec_images_b = self.generator_b(self.fake_images_a)\n    \n        \n    @staticmethod\n    def set_requires_grad(nets, requires_grad=False):\n        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n        Parameters:\n            nets (network list)   -- a list of networks\n            requires_grad (bool)  -- whether the networks require gradients or not\n        \"\"\"\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n                    \n                    \n    def generator_step(self):\n        CycleGan.set_requires_grad([self.discriminator_a, self.discriminator_b], False)\n        \n        loss_a = self.discriminator_loss(self.discriminator_a(self.fake_images_a), True)\n        loss_b = self.discriminator_loss(self.discriminator_b(self.fake_images_b), True)\n        cycle_a = self.reconstruct_loss(self.rec_images_a, self.real_images_a)*self.lambda_a\n        cycle_b = self.reconstruct_loss(self.rec_images_b, self.real_images_b)*self.lambda_b\n        cls_loss = self.classifier_loss(self.classifier(self.fake_images_b), self.labels_a)*self.lambda_cls\n        \n        loss = loss_a + loss_b + cycle_a + cycle_b + cls_loss\n        loss.backward()\n        CycleGan.set_requires_grad([self.discriminator_a, self.discriminator_b], True)\n        return loss, loss_a, loss_b, cycle_a, cycle_b, cls_loss\n        \n    def discriminator_step(self):\n        pred_real_a = self.discriminator_a(self.real_images_a)\n        loss_real_a = self.discriminator_loss(pred_real_a, True)\n        fake_images_a = self.image_pool_a.query(self.fake_images_a).detach()\n        pred_fake_a = self.discriminator_a(fake_images_a)\n        loss_fake_a = self.discriminator_loss(pred_fake_a, False)\n        \n        pred_real_b = self.discriminator_b(self.real_images_b)\n        loss_real_b = self.discriminator_loss(pred_real_b, True)\n        fake_images_b = self.image_pool_b.query(self.fake_images_b).detach()\n        pred_fake_b = self.discriminator_b(fake_images_b)\n        loss_fake_b = self.discriminator_loss(pred_fake_b, False)\n        \n        loss = (loss_real_a + loss_fake_a)\/2 + (loss_real_b + loss_fake_b)\/2\n        loss.backward()\n        return loss, loss_real_a, loss_fake_a, (loss_real_a + loss_fake_a)\/2, loss_real_b, loss_fake_b, (loss_real_b+loss_fake_b)\/2\n    \n    def set_input(self, images_a, images_b, labels_a, labels_b):\n        self.real_images_a = images_a.to(self.device)\n        self.real_images_b = images_b.to(self.device)\n        self.labels_a = labels_a\n        self.labels_b = labels_b","fc8d9cf1":"model = CycleGan(generator_a=generator_a,\n                generator_b=generator_b,\n                discriminator_a=discriminator_a,\n                discriminator_b=discriminator_b,\n                classifier=classifier,\n                discriminator_loss=discriminator_loss,\n                classifier_loss=classifier_loss,\n                lambda_a=10.0,\n                lambda_b=10.0,\n                lambda_cls=1.0,\n                device=device\n                )\n","14e289b4":"hand_sampler = torch.utils.data.RandomSampler(hand_dataset, True, int(max(len(hand_dataset), len(font_dataset)))*(EPOCH))\nfont_sampler = torch.utils.data.RandomSampler(font_dataset, True, int(max(len(hand_dataset), len(font_dataset)))*(EPOCH))\n","61591571":"hand_loader = torch.utils.data.DataLoader(\n    hand_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    num_workers=1, \n    pin_memory=True, \n    drop_last=True, \n    sampler=hand_sampler)\nfont_loader = torch.utils.data.DataLoader(\n    font_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    num_workers=1, \n    pin_memory=True, \n    drop_last=True, \n    sampler=font_sampler)","9dde55a1":"hand_loader_iter = iter(hand_loader)\nfont_loader_iter = iter(font_loader)","d0844523":"def train_step(model, a_iter, b_iter, generator_optimizer, discriminator_optimizer, generator_scheduler, discriminator_scheduler, device):\n    a_image, a_label = next(a_iter)\n    b_image, b_label = next(b_iter)\n    a_image = a_image.to(device)\n    b_image = b_image.to(device)\n    a_label = a_label.to(device)\n    b_label = b_label.to(device)\n    model.set_input(a_image, b_image, a_label, b_label)\n    model.forward()\n    generator_optimizer.zero_grad()\n    generator_loss, generator_loss_a, generator_loss_b, cycle_a, cycle_b, cls_loss = model.generator_step()\n    generator_optimizer.step()\n    discriminator_optimizer.zero_grad()\n    discriminator_loss, loss_real_a, loss_fake_a, discriminator_loss_a, loss_real_b, loss_fake_b, discriminator_loss_b = model.discriminator_step()\n    discriminator_optimizer.step()\n    generator_scheduler.step()\n    discriminator_scheduler.step()\n    return generator_loss, generator_loss_a, generator_loss_b, cycle_a, cycle_b, cls_loss, discriminator_loss, loss_real_a, loss_fake_a, discriminator_loss_a, loss_real_b, loss_fake_b, discriminator_loss_b","f3c09fd9":"generator_optimizer = torch.optim.Adam(itertools.chain(generator_a.parameters(), generator_b.parameters()), lr=0.0002, betas=(0.5, 0.999))\ndiscriminator_optimizer = torch.optim.Adam(itertools.chain(discriminator_a.parameters(), discriminator_b.parameters()), lr=0.0002, betas=(0.5, 0.999))","54a9d74b":"num_step_per_epoch = len(hand_loader)\/\/EPOCH\ntrain_steps = num_step_per_epoch*EPOCH\nWARM_UP_STEP = train_steps*0.5\n\ndef warmup_linear_decay(step):\n    if step < WARM_UP_STEP:\n        return 1.0\n    else:\n        return (train_steps-step)\/(train_steps-WARM_UP_STEP)\ngenerator_scheduler = torch.optim.lr_scheduler.LambdaLR(generator_optimizer, warmup_linear_decay)\ndiscriminator_scheduler = torch.optim.lr_scheduler.LambdaLR(discriminator_optimizer, warmup_linear_decay)","3d344ff2":"class LossAverager:\n    def __init__(self, prefix):\n        self.prefix = prefix\n        self.generator_loss = []\n        self.generator_loss_a = []\n        self.generator_loss_b = []\n        self.cycle_a = []\n        self.cycle_b = []\n        self.cls_loss = []\n        self.discriminator_loss = []\n        self.loss_real_a = []\n        self.loss_fake_a = []\n        self.discriminator_loss_a = []\n        self.loss_real_b = []\n        self.loss_fake_b = []\n        self.discriminator_loss_b = []\n    \n    def append(self, generator_loss, generator_loss_a, generator_loss_b, cycle_a, cycle_b, cls_loss, discriminator_loss, loss_real_a, loss_fake_a, discriminator_loss_a, loss_real_b, loss_fake_b, discriminator_loss_b):\n        self.generator_loss.append(generator_loss.item())\n        self.generator_loss_a.append(generator_loss_a.item())\n        self.generator_loss_b.append(generator_loss_b.item())\n        self.cycle_a.append(cycle_a.item())\n        self.cycle_b.append(cycle_b.item())\n        self.cls_loss.append(cls_loss.item())\n        self.discriminator_loss.append(discriminator_loss.item())\n        self.loss_real_a.append(loss_real_a.item())\n        self.loss_fake_a.append(loss_fake_a.item())\n        self.discriminator_loss_a.append(discriminator_loss_a.item())\n        self.loss_real_b.append(loss_real_b.item())\n        self.loss_fake_b.append(loss_fake_b.item())\n        self.discriminator_loss_b.append(discriminator_loss_b.item())\n\n    def average(self):\n        metric = {}\n        for key, value in self.__dict__.items():\n            if isinstance(value, list):\n                metric[self.prefix+'\/'+key] = sum(value)\/len(value)\n        return metric","db25c397":"log = []\n\n\nfor epoch in range(EPOCH):\n    model.train()\n    model.classifier.eval()\n    loss_averager = LossAverager('train')\n    for i in tqdm(range(num_step_per_epoch)):\n        losses = train_step(model, hand_loader_iter, font_loader_iter, generator_optimizer, discriminator_optimizer, generator_scheduler, discriminator_scheduler, device)\n        loss_averager.append(*losses)\n    metric = loss_averager.average()\n    metric['epoch'] = epoch\n    model.eval()\n    log.append(metric)\n    torch.save(generator_b.state_dict(), 'generator.pth')\n    with open('log.json', 'w') as fout:\n        json.dump(log , fout, indent=4)\n    ","465d9f26":"## CycleGAN Model\n\n\nThe code below is copied from https:\/\/github.com\/junyanz\/pytorch-CycleGAN-and-pix2pix\n\n```\nCopyright (c) 2017, Jun-Yan Zhu and Taesung Park\nAll rights reserved.\n\nFor pix2pix software\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\nAll rights reserved.\n```"}}