{"cell_type":{"d5cf5686":"code","ad3592a8":"code","3ee3a679":"code","0628c1b2":"code","955fce7c":"code","7f432585":"code","b04009af":"code","f15ba744":"code","affd1609":"code","4e289b1b":"code","e9f13db1":"code","3441bdee":"code","2ddb4e13":"code","fbc99746":"code","45434b3c":"code","1789d59b":"code","75bc4e67":"code","b92fa2e9":"code","b24c6dd2":"code","e45b6875":"code","34ef03c2":"code","3cd24356":"code","9f1e854c":"code","e3bc12ad":"code","d7597431":"code","64025dba":"code","8a59433d":"code","12ae29d4":"code","065e9271":"code","88281313":"code","c6c46c2f":"code","d3e60058":"markdown","9f5046e5":"markdown","e0308bc8":"markdown","0ec8e6f6":"markdown","c0c3591e":"markdown","c22e2e2b":"markdown","4701baf4":"markdown","5616e556":"markdown"},"source":{"d5cf5686":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"whitegrid\")","ad3592a8":"df = pd.read_csv(r'..\/input\/sms-spam-collection-dataset\/spam.csv', encoding = 'latin-1')\ndf.head()","3ee3a679":"#Find Missing Values\nplt.figure(figsize=(12,8))\nsns.heatmap(df.isnull(), cmap = 'viridis', yticklabels = False, cbar = False)","0628c1b2":"df.dropna(how=\"any\", inplace=True, axis=1)\ndf.columns = ['label', 'message']\ndf.head()","955fce7c":"df.describe()","7f432585":"df.groupby('label').describe()","b04009af":"sns.countplot(df['label'])","f15ba744":"# convert label to a numerical variable\ndf['label_num'] = df.label.map({'ham':0, 'spam':1})\ndf.head()","affd1609":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\ndef word_cloud(data, title=None):\n    cloud = WordCloud(background_color = 'black',\n                     stopwords = stopwords,\n                     max_words = 200,\n                     max_font_size = 40,\n                     scale = 3).generate(str(data))\n    fig = plt.figure(figsize=(15,15))\n    plt.axis('off')\n    if title:\n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.25)\n        plt.imshow(cloud)\n        plt.show()","4e289b1b":"word_cloud(df[df['label_num']==1]['message'],'Most Repeated words in spam messages')","e9f13db1":"word_cloud(df[df['label_num']==0]['message'],'Most Repeated words in Ham messages')","3441bdee":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nsms_len=df[df['label_num']==1]['message'].str.len()\nax1.hist(sms_len,color='red')\nax1.set_title('spam messages')\nsms_len=df[df['label_num']==0]['message'].str.len()\nax2.hist(sms_len,color='green')\nax2.set_title('Ham messages')\nfig.suptitle('Characters in sms')","2ddb4e13":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nsms_words = df[df['label_num']==1]['message'].str.split().map(lambda x: len(x))\nax1.hist(sms_words, color='red')\nax1.set_title('Spam Messages')\nsms_words = df[df['label_num']==0]['message'].str.split().map(lambda x: len(x))\nax2.hist(sms_words, color='green')\nax2.set_title('Ham Messages')\nfig.suptitle('Words in a Sms')","fbc99746":"df[df.label=='ham'].describe()","45434b3c":"df[df.label=='spam'].describe()","1789d59b":"import string\nfrom nltk.corpus import stopwords\n\ndef text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', '\u00fc', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","75bc4e67":"df.head()","b92fa2e9":"df['clean_msg'] = df.message.apply(text_process)\ndf.head()","b24c6dd2":"X = df.clean_msg\ny = df.label_num\nprint(X.shape)\nprint(y.shape)","e45b6875":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=1, stratify = y)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","34ef03c2":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","3cd24356":"from sklearn.pipeline import Pipeline\npipeline_lr=Pipeline([('bow1', CountVectorizer(analyzer=text_process)),\n                      ('tfidf1', TfidfTransformer()),\n                     ('lr_classifier',LogisticRegression(random_state=0))])","9f1e854c":"pipeline_dt=Pipeline([('bow2', CountVectorizer(analyzer=text_process)),\n                      ('tfidf2', TfidfTransformer()),\n                     ('dt_classifier',DecisionTreeClassifier())])","e3bc12ad":"pipeline_rf=Pipeline([('bow3', CountVectorizer(analyzer=text_process)),\n                      ('tfidf3', TfidfTransformer()),\n                     ('rf_classifier',RandomForestClassifier())])","d7597431":"pipeline_nb=Pipeline([('bow4', CountVectorizer(analyzer=text_process)),\n                      ('tfidf4', TfidfTransformer()),\n                     ('naive_classifier',MultinomialNB())])","64025dba":"pipeline_svm=Pipeline([('bow5', CountVectorizer(analyzer=text_process)),\n                      ('tfidf5', TfidfTransformer()),\n                     ('svm_classifier',SVC())])","8a59433d":"## Lets make the list of pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_nb, pipeline_svm]","12ae29d4":"best_score=0.0\nbest_classifier=0\nbest_pipeline=\"\"","065e9271":"pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Random Forest', 3: 'Naive-Baies Classifier', 4: 'SVM Classifier'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)","88281313":"from sklearn.metrics import f1_score\nfor i,model in enumerate(pipelines):\n    predictions = model.predict(X_test)\n    score = f1_score(y_test, predictions, average='macro')\n    print(\"{} Test F1_Score: {}\".format(pipe_dict[i], score))","c6c46c2f":"for i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_score:\n        best_score=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best f1_score:{}'.format(pipe_dict[best_classifier]))","d3e60058":"## Number of characters in a sms","9f5046e5":"# Exploratory Data Analysis","e0308bc8":"## Visualizing Most Repeated Words using WordCloud","0ec8e6f6":"# Model Building Using Pipelines","c0c3591e":"# Reading Dataset","c22e2e2b":"It is an Imbalaanced Dataset, so f1_score will be the best metric for evaluation","4701baf4":"Through just basic EDA we've been able to discover a trend that spam messages tend to have more characters.","5616e556":"# Importing Libraries"}}