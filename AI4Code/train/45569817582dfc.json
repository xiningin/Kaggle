{"cell_type":{"4a2ff506":"code","47aa6512":"code","34f97f27":"code","a4575705":"code","0ab35536":"code","69b84dd2":"code","be7e843e":"code","926b2c99":"code","a1c54303":"code","4406880d":"code","95115af4":"code","45147e40":"code","9fc6f3e9":"code","dbf22817":"code","38a49a46":"code","66711b01":"code","5a91965c":"code","f6b9f61f":"code","ec45bf4e":"code","ca2a4aac":"code","df098cbb":"code","d7e46921":"code","027879c5":"code","783125be":"code","4dc82bfb":"code","49100597":"code","7100a6db":"code","00eeca41":"code","5177d761":"code","2fbbd327":"code","47aeb3bf":"code","e9f36940":"code","f398a953":"code","ed5a005a":"code","cb618c49":"code","44bc97ac":"code","7d8f73a9":"code","28a9a619":"code","ba2928b1":"markdown","4e911b3f":"markdown","de473038":"markdown","7becf8a3":"markdown","0c7f4e67":"markdown","354717c4":"markdown","fe340984":"markdown","c9a5f124":"markdown","5e450e10":"markdown","c6eeff69":"markdown","83fab5f5":"markdown"},"source":{"4a2ff506":"# Importing Libraries\n\n# filtering out the warnings after cell execution\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# General Commonly Used Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing Libraries\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# EDA\nfrom sklearn import base\n\n# Feature Engineering and Selection\nfrom sklearn.utils import class_weight\n\n# Modeling & Accuracy Metrics\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\n\n# Validation and Hyperparameter Tuning\nfrom sklearn.model_selection import KFold, cross_val_score as cvs, GridSearchCV\n\n# Utility Library\nfrom collections import Counter\nimport os","47aa6512":"# Kaggle cwd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","34f97f27":"# Importing Datasets\n\ntrain_set = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-janatahack-customer-segmentation\/Train_aBjfeNk.csv\", verbose = -1)\ntest_set = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-janatahack-customer-segmentation\/Test_LqhgPWU.csv\", verbose = -1)\nid_cols = test_set['ID']\ntrain_set.head(5)","a4575705":"# Spliting Train Dataset into Train and Train-Remain (For further Validation and Test Spliting)\ntrain, test = train_test_split(train_set, test_size = 0.3, random_state = 0, shuffle = True)","0ab35536":"# Education\n\ntrain[\"Profession\"].fillna(\"Others\", inplace = True)\ntest[\"Profession\"].fillna(\"Others\", inplace = True)\ntest_set[\"Profession\"].fillna(\"Others\", inplace = True)","69b84dd2":"# Ever Married\n\n## Train\ngg = train.index[((train.Ever_Married.isnull()) & (train.Family_Size == 1.0))].tolist()\ntrain.at[gg, 'Ever_Married'] = 'No'\ntrain.Ever_Married.fillna('Yes', inplace = True)\n\n## Validation\ngg = test.index[((test.Ever_Married.isnull()) & (test.Family_Size == 1.0))].tolist()\ntest.at[gg, 'Ever_Married'] = 'No'\ntest.Ever_Married.fillna('Yes', inplace = True)\n\n## Test\ngg = test_set.index[((test_set.Ever_Married.isnull()) & (test_set.Family_Size == 1.0))].tolist()\ntest_set.at[gg, 'Ever_Married'] = 'No'\ntest_set.Ever_Married.fillna('Yes', inplace = True)","be7e843e":"# Family Size\n\ntrain['Family_Size'].fillna(round(train.Family_Size.mean()), inplace = True)\ntest['Family_Size'].fillna(round(train.Family_Size.mean()), inplace = True)\ntest_set['Family_Size'].fillna(round(train.Family_Size.mean()), inplace = True)","926b2c99":"# Profession\n\ntrain['Profession'].fillna('Others', inplace = True)\ntest['Profession'].fillna('Others', inplace = True)\ntest_set['Profession'].fillna('Others', inplace = True)","a1c54303":"# Graduation\n\n## Train\ngg = train.index[((train.Graduated.isnull()) & (train.Age <= 24) & (train.Family_Size == 1.0))].tolist()\ntrain.at[gg, 'Graduated'] = 'No'\ntrain.Graduated.fillna('Yes', inplace = True)\n\n## Validation\ngg = test.index[((test.Graduated.isnull()) & (test.Age <= 24) & (test.Family_Size == 1.0))].tolist()\ntest.at[gg, 'Graduated'] = 'No'\ntest.Graduated.fillna('Yes', inplace = True)\n\n## Test\ngg = test_set.index[((test_set.Graduated.isnull()) & (test_set.Age <= 24) & (test_set.Family_Size == 1.0))].tolist()\ntest_set.at[gg, 'Graduated'] = 'No'\ntest_set.Graduated.fillna('Yes', inplace = True)","4406880d":"# Work_Experience\n\ntrain.Work_Experience.fillna(round(train.Work_Experience.mean()), inplace = True)\ntest.Work_Experience.fillna(round(train.Work_Experience.mean()), inplace = True)\ntest_set.Work_Experience.fillna(round(train.Work_Experience.mean()), inplace = True)","95115af4":"# Var_1\n\ntrain.Var_1.fillna(train.Var_1.mode()[0], inplace = True)\ntest.Var_1.fillna(train.Var_1.mode()[0], inplace = True)\ntest_set.Var_1.fillna(train.Var_1.mode()[0], inplace = True)","45147e40":"train.isnull().mean(), test.isnull().mean(), test_set.isnull().mean()","9fc6f3e9":"# Determining Output Leakage -> Keeping the IDs\nleak = list(set(train_set.ID) & set(test_set.ID))\nlen(leak)","dbf22817":"# Storing the index in Train Set\nss = list()\nfor i in leak:\n    ss.append(train_set.index[train_set.ID == i][0])\nprint(len(ss))\n\n# Storing the Values for the Respective Indexes\nop_values = list()\nfor i in ss:\n    op_values.append(train_set.iloc[i, -1])\nprint(len(op_values))","38a49a46":"# Label Encoding\n\nl = LabelEncoder()\n\n## Gender\ntrain.loc[:, 'Gender'] = l.fit_transform(train.loc[:, 'Gender'])\ntest.loc[:, 'Gender'] = l.fit_transform(test.loc[:, 'Gender'])\ntest_set.loc[:, 'Gender'] = l.fit_transform(test_set.loc[:, 'Gender'])\n\n## Ever Married\ntrain.loc[:, 'Ever_Married'] = l.fit_transform(train.loc[:, 'Ever_Married'])\ntest.loc[:, 'Ever_Married'] = l.fit_transform(test.loc[:, 'Ever_Married'])\ntest_set.loc[:, 'Ever_Married'] = l.fit_transform(test_set.loc[:, 'Ever_Married'])\n\n## Graduated\ntrain.loc[:, 'Graduated'] = l.fit_transform(train.loc[:, 'Graduated'])\ntest.loc[:, 'Graduated'] = l.fit_transform(test.loc[:, 'Graduated'])\ntest_set.loc[:, 'Graduated'] = l.fit_transform(test_set.loc[:, 'Graduated'])","66711b01":"## Segmentation - Target Variable\n\ntrain.loc[:, 'Segmentation'] = l.fit_transform(train.loc[:, 'Segmentation'])\ntest.loc[:, 'Segmentation'] = l.fit_transform(test.loc[:, 'Segmentation'])","5a91965c":"# Defining K-Fold Target Encoding Class for Train (K-Fold as for Regularization)\n\nclass KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n\n    def __init__(self, colname, targetName, n_fold = 5):\n\n        self.colnames = colname\n        self.targetName = targetName\n        self.n_fold = n_fold\n\n    def fit(self, x, y = None):\n        return self\n\n    def transform(self, x):\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in x.columns)\n        assert(self.targetName in x.columns)\n\n        mean_of_target = x[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold, shuffle = False, random_state=0)\n\n        col_mean_name = 'tgt_' + self.colnames\n        x[col_mean_name] = np.nan\n\n        for tr_ind, val_ind in kf.split(x):\n            x_tr, x_val = x.iloc[tr_ind], x.iloc[val_ind]\n            x.loc[x.index[val_ind], col_mean_name] = x_val[self.colnames].map(x_tr.groupby(self.colnames)[self.targetName].mean())\n\n        x[col_mean_name].fillna(mean_of_target, inplace = True)\n\n        return x","f6b9f61f":"# Defining K-Fold Target Encoding Class for Validation (K-Fold as for Regularization) [Mapping from Train]\n\nclass KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n    \n    def __init__(self, train, colNames, encodedName):\n        \n        self.train = train\n        self.colNames = colNames\n        self.encodedName = encodedName\n         \n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X):\n\n        mean = self.train[[self.colNames, self.encodedName]].groupby(self.colNames).mean().reset_index() \n        \n        dd = {}\n        for index, row in mean.iterrows():\n            dd[row[self.colNames]] = row[self.encodedName]\n\n        X[self.encodedName] = X[self.colNames]\n        X = X.replace({self.encodedName: dd})\n\n        return X","ec45bf4e":"# K-Fold Target Encoding\n\n## Profession\n\n### Train\ntargetc = KFoldTargetEncoderTrain('Profession', 'Segmentation', n_fold = 5)\ntrain = targetc.fit_transform(train)\n\n### Validation\ntargetc = KFoldTargetEncoderTest(train, 'Profession', 'tgt_Profession')\ntest = targetc.fit_transform(test)\n\n### Validation\ntargetc = KFoldTargetEncoderTest(train, 'Profession', 'tgt_Profession')\ntest_set = targetc.fit_transform(test_set)\n\n## Var_1\n\n### Train\ntargetc = KFoldTargetEncoderTrain('Var_1', 'Segmentation', n_fold = 5)\ntrain = targetc.fit_transform(train)\n\n### Validation\ntargetc = KFoldTargetEncoderTest(train, 'Var_1', 'tgt_Var_1')\ntest = targetc.fit_transform(test)\n\n### Test\ntargetc = KFoldTargetEncoderTest(train, 'Var_1', 'tgt_Var_1')\ntest_set = targetc.fit_transform(test_set)","ca2a4aac":"# Weighted Encoding\n\n## Spending Score\nspend_enc = {\"Low\" : 0, \"Average\" : 1, \"High\": 2}\n\ntrain['Spending_Score'] = train['Spending_Score'].map(spend_enc)\ntest['Spending_Score'] = test['Spending_Score'].map(spend_enc)\ntest_set['Spending_Score'] = test_set['Spending_Score'].map(spend_enc)","df098cbb":"# Dropping off Redundant Features\n\ntrain.drop(['Profession', 'Var_1', 'ID'], axis = 1, inplace = True)\ntest.drop(['Profession', 'Var_1', 'ID'], axis = 1, inplace = True)\ntest_set.drop(['Profession', 'Var_1', 'ID'], axis = 1, inplace = True)","d7e46921":"# Reducing Skewness\n\n## Train   \ntrain.Age = stats.boxcox(train.Age)[0]\ntrain.Family_Size += 1   \ntrain.Family_Size = stats.boxcox(train.Family_Size)[0] \ntrain.tgt_Var_1 = stats.boxcox(train.tgt_Var_1)[0]    \ntrain.tgt_Profession = stats.boxcox(train.tgt_Profession)[0]\ntrain.Work_Experience = np.cbrt(train.Work_Experience)\n\n## Validation   \ntest.Age = stats.boxcox(test.Age)[0]\ntest.Family_Size += 1   \ntest.Family_Size = stats.boxcox(test.Family_Size)[0]  \ntest.tgt_Var_1 = stats.boxcox(test.tgt_Var_1)[0]   \ntest.tgt_Profession = stats.boxcox(test.tgt_Profession)[0] \ntest.Work_Experience = np.cbrt(test.Work_Experience)\n\n## Test  \ntest_set.Age = stats.boxcox(test_set.Age)[0] \ntest_set.Family_Size += 1\ntest_set.Family_Size = stats.boxcox(test_set.Family_Size)[0]      \ntest_set.tgt_Var_1 = stats.boxcox(test_set.tgt_Var_1)[0]  \ntest_set.tgt_Profession = stats.boxcox(test_set.tgt_Profession)[0] \ntest_set.Work_Experience = np.cbrt(test_set.Work_Experience)","027879c5":"# Dividing into independent and dependent features\n\n## Train\nx = train.drop(['Segmentation'], axis = 1)\ny = train.Segmentation.values.reshape(-1, 1)\n\n## Validation\nxx = train.drop(['Segmentation'], axis = 1)\nyy = train.Segmentation.values.reshape(-1, 1)","783125be":"# Standard Scaling\n\nsc_x = StandardScaler()\nx_scale = sc_x.fit_transform(x)\nxx_scale = sc_x.fit_transform(xx)\nt_scale = sc_x.fit_transform(test_set)","4dc82bfb":"# Determining Class Weights\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(y.reshape(-1, )), y.reshape(-1, ))\nim_weight = dict(enumerate(class_weights))\nim_weight","49100597":"# CatBoost Classifier\n\n# Fitting CatBoost Classifier to the Training Set\nclassifier = CatBoostClassifier(random_state = 0, eval_metric = 'Accuracy', class_weights = im_weight, od_type = \"Iter\", thread_count = -1)\nclassifier.fit(x_scale, y, eval_set = (xx_scale, yy))\ny_pred = classifier.predict(x_scale)\n\n## Classification Report - Train\nprint(classification_report(y, y_pred))\n\n# Applying k-fold Cross Validation Score\nfrom sklearn.model_selection import cross_val_score as cvs\naccuracies = cvs(estimator = classifier, X = xx_scale, y = yy, cv = 10, scoring = 'accuracy', n_jobs = -1)\nprint(accuracies.mean())\nprint(accuracies.std())\n\n## Classification Report - Validation\nprint(classification_report(yy, classifier.predict(xx_scale)))","7100a6db":"# Setting up the Dictionary of Hyper-Paramters\n\nhyperparams = {\n    \"eval_metric\" : [\"Accuracy\"],              # Evaluation Metric\n    \"random_state\" : [0],                      # Random State to retain the same configuration\n    \"iterations\" : [100, 200, 500, 1000],      # Iterations is an alis for 'n_estimators' -> Maximum no of Trees\n    \"learning_rate\" : [0.03, 0.1, 0.001],      # Learning Rate of our Model\n    \"l2_leaf_reg\" : [3.0, 1.0, 5.0],           # L2 Regularization Parameter of our Cost Function to reduce overfitting\n    \"depth\" : [6, 7, 8, 9, 10],                # Depth of our Trees\n    \"class_weights\" : [im_weight],             # Class Weights\n    \"od_type\" : [\"Iter\"],                      # Type of overfitting detector\n    \"od_wait\" : [50, 100],                     # The No. of Iterations to continue the training after the iteration with the optimal metric value\n    \"task_type\": [\"CPU\"],                      # Processing Unit\n}","00eeca41":"# Using Grid Search CV Method to find out the Best Set of Hyper-Parameters\n\nclassifier = CatBoostClassifier()\nclass_cv = GridSearchCV(classifier, hyperparams, verbose = 1, scoring = ['accuracy'], n_jobs = -1, cv = 5, refit = 'accuracy')\nclass_cv.fit(x_scale, y, eval_set = (xx_scale, yy))","5177d761":"# Dictionary of the Best Parameters\nclass_cv.best_params_","2fbbd327":"# Fitting CatBoost Classifier to the Training Set\n\nclassifier = CatBoostClassifier(**class_cv.best_params_)\nclassifier.fit(x_scale, y)\n\ny_pred = classifier.predict(xx_scale)\n\nprint(classification_report(y, y_pred))","47aeb3bf":"# Fitting CatBoost Classifier to the Test Set\nclassifier = CatBoostClassifier(**class_cv.best_params_)\nclassifier.fit(x_scale, y, eval_set = (xx_scale, yy))\ny_pred = classifier.predict(t_scale)","e9f36940":"# Creating Submission Dataframe\nsubmission = pd.DataFrame()\nsubmission['ID'] = id_cols\nsubmission['Segmentation'] = y_pred\nsubmission","f398a953":"# Maping the integer values with the objects\nspend_enc = {0 : 'A', 1 : 'B', 2: 'C', 3 : 'D'}\nsubmission['Segmentation'] = submission['Segmentation'].map(spend_enc)","ed5a005a":"# Checking out number of Each Prediction \nCounter(submission.Segmentation)","cb618c49":"# Taking out the index in test Set\nss = list()\nfor i in leak:\n    ss.append(submission.index[submission.ID == i][0])\nlen(ss)\n\n# Imputing the Values\nfor (index, replacement) in zip(ss, op_values):\n    submission.Segmentation[index] = replacement","44bc97ac":"# Checking out number of Each Prediction post mutation\nCounter(submission.Segmentation)","7d8f73a9":"# Checking out Submission Dataframe\nsubmission.head(5)","28a9a619":"# Generating Submission File\nsubmission.to_csv(\"Final_Submission.csv\", index = False)","ba2928b1":"# Janatahack Customer Segmentation","4e911b3f":"# Data Cleaning","de473038":"# Keeping the Data for Output Leakage (Dirty :p)","7becf8a3":"Applying Dirty Method Imputation :p","0c7f4e67":"Note : This is a very vague notebook and contains no feature engineering or selection and everything is done through raw features with hyperparameter tuning.\n\nPublic Leaderboard Score : 0.94952380952381\t\nPrivate Leaderboard Score : 0.946100190234623","354717c4":"# Submission","fe340984":"# End","c9a5f124":"# Hyperparameter Tuning","5e450e10":"# Data Preprocessing","c6eeff69":"# Problem Statement\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. \n\nYou are required to help the manager to predict the right group of the new customers.\n\n<b> Evaluation Metric <\/b>\nThe evaluation metric for this hackathon is Accuracy Score.\n\nCompetition Link : [Analytics Vidya](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-customer-segmentation\/#ProblemStatement)","83fab5f5":"# Modelling"}}