{"cell_type":{"1a076e02":"code","458f19ba":"code","5c72a343":"code","e99231f0":"code","6e699c16":"code","bf1baed1":"code","d496d102":"markdown","35ca3c9d":"markdown"},"source":{"1a076e02":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\npd.set_option('display.max_columns', None)\n\nimport plotly\nplotly.offline.init_notebook_mode()\n\nfrom sklearn.datasets import load_boston\ndata = load_boston()\ndf = pd.concat([pd.DataFrame(data['data'], columns = data['feature_names']), \n                pd.DataFrame(data['target'], columns=['MEDV'])], axis=1)","458f19ba":"import matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression\nimport plotly.express as px\nX = df.drop(['MEDV'], axis=1)\ny = df['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","5c72a343":"###\n# Code adopted from https:\/\/machinelearningmastery.com\/feature-selection-for-regression-data\/\n###\nfs = SelectKBest(score_func=mutual_info_regression, k='all')\n# learn relationship from training data\nfs.fit(X_train, y_train)\n# transform train input data\nX_train_fs = fs.transform(X_train)\n# transform test input data\nX_test_fs = fs.transform(X_test)\n\n_dict_features={'Features':[], 'Score':[]}\n\nfor score_feature, column in zip(fs.scores_, df.columns):\n    _dict_features['Features'].append(column)\n    _dict_features['Score'].append(score_feature)\n    \n_df=pd.DataFrame(_dict_features)\n\n# plot the scores\nfig = px.bar(_df, x='Features', y='Score', title=\"Visualization of the dependence of the target variable on features\")\nfig.update_layout(height=500, width=950)\nfig.show()","e99231f0":"#Preprocessors\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n#Algorithms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n#Metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score \n#Additional\nfrom sklearn.pipeline import make_pipeline\nfrom joblib import Parallel, delayed","6e699c16":"#Function for getting a best model by MAE\ndef evaluating_scores(pipeline, X, y, scoring_mae='neg_mean_absolute_error', scoring_r2='r2', cv=5):\n    #Get values from Preprocessors and Regressors\n    preprocessors_names=list(pipeline.named_steps.values())[0]\n    regressors_names=list(pipeline.named_steps.values())[1]\n    #Get MAE on Cross Validation\n    score_mae=-cross_val_score(pipeline, X, y, scoring=scoring_mae, cv=cv).mean()\n    #Returning a dict to transform in DataFrame\n    return dict(preprocessor=preprocessors_names, regressor=regressors_names, score_mae=score_mae)\n\n\ndef find_optimum(X, y):\n    #Set Preprocessors\n    preprocessors=[MinMaxScaler(), \n                   StandardScaler(), \n                   RobustScaler()]\n    #Set Algorithms\n    regressors=[RandomForestRegressor(random_state=42),\n                KNeighborsRegressor(),\n                LinearRegression(),\n                XGBRegressor(random_state=42)] \n    \n    #A list for pipelines\n    _list_est=[]\n    \n    #Get all possible pipelines using Brute-Force Search\n    for preprocessor in preprocessors:\n        for regressor in regressors:\n            _list_est.append(make_pipeline(preprocessor, regressor))\n            \n    #Computing scores for all pipelines using all computing resources\n    computed=Parallel(n_jobs=-1)(delayed(evaluating_scores)(pipeline, X, y) for pipeline in _list_est)\n    \n    #Returning a transformed DataFrame from a dict with evaluated scores\n    return round(pd.DataFrame(computed).sort_values('score_mae').reset_index(drop=True), 3)","bf1baed1":"results=find_optimum(X, y)\nresults","d496d102":"# Getting Best Model","35ca3c9d":"# Feature Selection"}}