{"cell_type":{"3e7db4f1":"code","5aff0d10":"code","f5aac493":"code","fe35a8bb":"code","32778dd9":"code","785ce69a":"code","f2d73552":"code","25071db3":"code","4f854011":"code","63c0e7ea":"code","07682065":"code","b6ac68f9":"code","9177ffae":"code","a572b898":"code","e8931714":"code","f674340b":"code","23009a8d":"code","c5ff677f":"code","f521c4d6":"code","f2d94afc":"code","3b040bdc":"code","67be9261":"code","9e8190cd":"code","c2901b01":"code","e12b53ad":"code","735efba7":"code","aa798fcb":"code","da6314bb":"code","400a7401":"code","9c5dab3f":"code","415caa77":"code","1e6ada5b":"code","7fd69056":"code","34754799":"code","8eb4390b":"code","a2cb71fd":"code","8391dd6f":"code","7a5e2b18":"code","bfbd667f":"code","93445faf":"code","a4e7d904":"code","579c4968":"code","357229ee":"code","6cf64eaf":"code","a287e3a7":"code","ee3747a6":"code","1238967c":"code","e72fb9c3":"code","29e0b9ba":"code","5df747bb":"code","aafcd692":"code","e199d7bc":"code","c317ed0a":"code","b2d4e345":"code","9994cddf":"code","abe9f0ba":"code","3beb60ea":"code","7f896054":"code","5ed523ae":"code","decb54f1":"code","6167cacd":"code","6c473440":"code","041fbc49":"code","ce31e57e":"code","4b795f56":"markdown","e8e083d1":"markdown","2bf92340":"markdown","1565d426":"markdown","b9537719":"markdown","a7446f5b":"markdown","31ed5da6":"markdown","49c09799":"markdown","f502f252":"markdown","8f1f7e62":"markdown","153daa63":"markdown","5680ce61":"markdown","b5d0e856":"markdown","57de95d3":"markdown","ce028cf1":"markdown","ad5151fd":"markdown","7809234a":"markdown","f652eadd":"markdown","73accbbf":"markdown","05f212d4":"markdown","ec1577c3":"markdown","00851c48":"markdown","8e20404b":"markdown","e1b90309":"markdown"},"source":{"3e7db4f1":"# necessary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()\n%matplotlib inline","5aff0d10":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv') # reading the data","f5aac493":"data.head()","fe35a8bb":"data.describe()","32778dd9":"# let's see how data is distributed for every column.\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\n\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","785ce69a":"# replacing zero values with the mean of the columnn\n\ndata['BMI'] = data['BMI'].replace(0, data['BMI'].mean())\ndata['BloodPressure'] = data['BloodPressure'].replace(0, data['BloodPressure'].mean())\ndata['Glucose'] = data['Glucose'].replace(0, data['Glucose'].mean())\ndata['Insulin'] = data['Insulin'].replace(0, data['Insulin'].mean())\ndata['SkinThickness'] = data['SkinThickness'].replace(0, data['SkinThickness'].mean())","f2d73552":"# again checking the data distribution\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\n\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","25071db3":"fig, ax = plt.subplots(figsize = (15, 10))\nsns.boxplot(data = data, width = 0.5, ax = ax, fliersize = 3)\nplt.show()","4f854011":"outlier = data['Pregnancies'].quantile(0.98)\n# removing the top 2% data from the pregnancies column\ndata = data[data['Pregnancies']<outlier]\n\noutlier = data['BMI'].quantile(0.99)\n# removing the top 1% data from BMI column\ndata = data[data['BMI']<outlier]\n\noutlier = data['SkinThickness'].quantile(0.99)\n# removing the top 1% data from SkinThickness column\ndata = data[data['SkinThickness']<outlier]\n\noutlier = data['Insulin'].quantile(0.95)\n# removing the top 5% data from Insulin column\ndata = data[data['Insulin']<outlier]\n\noutlier = data['DiabetesPedigreeFunction'].quantile(0.99)\n# removing the top 1% data from DiabetesPedigreeFunction column\ndata = data[data['DiabetesPedigreeFunction']<outlier]\n\noutlier = data['Age'].quantile(0.99)\n# removing the top 1% data from Age column\ndata = data[data['Age']<outlier]","63c0e7ea":"# again checking the data distribution\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\n\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","07682065":"plt.figure(figsize = (16, 8))\n\ncorr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nsns.heatmap(corr, mask = mask, annot = True, fmt = '.2g', linewidths = 1)\nplt.show()","b6ac68f9":"X = data.drop(columns = ['Outcome'])\ny = data['Outcome']","9177ffae":"# splitting the data into testing and training data.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","a572b898":"# scaling the data \n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e8931714":"# fitting data to model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlr = LogisticRegression()\n\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\n\nlr_train_acc = accuracy_score(y_train, lr.predict(X_train))\nlr_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Logistic Regression Model is {lr_train_acc}\")\nprint(f\"Test Accuracy of Logistic Regression Model is {lr_test_acc}\")","f674340b":"# confusion matrix \n\nconfusion_matrix(y_test, y_pred)","23009a8d":"# classification report\n\nprint(classification_report(y_test, y_pred))","c5ff677f":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nknn_train_acc = accuracy_score(y_train, knn.predict(X_train))\nknn_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of KNN Model is {knn_train_acc}\")\nprint(f\"Test Accuracy of KNN Model is {knn_test_acc}\")","f521c4d6":"# confusion matrix \n\nconfusion_matrix(y_test, y_pred)","f2d94afc":"# classification report\n\nprint(classification_report(y_test, y_pred))","3b040bdc":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\ny_pred = svc.predict(X_test)\n\nsvc_train_acc = accuracy_score(y_train, svc.predict(X_train))\nsvc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of SVC Model is {svc_train_acc}\")\nprint(f\"Test Accuracy of SVC Model is {svc_test_acc}\")","67be9261":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","9e8190cd":"# classification report\n\nprint(classification_report(y_test, y_pred))","c2901b01":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\ny_pred = dtc.predict(X_test)\n\ndtc_train_acc = accuracy_score(y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decision Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decision Tree Model is {dtc_test_acc}\")","e12b53ad":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","735efba7":"# classification report\n\nprint(classification_report(y_test, y_pred))","aa798fcb":"# hyper parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(dtc, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","da6314bb":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","400a7401":"dtc = grid_search.best_estimator_\n\ny_pred = dtc.predict(X_test)\n\ndtc_train_acc = accuracy_score(y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decesion Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decesion Tree Model is {dtc_test_acc}\")","9c5dab3f":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","415caa77":"# classification report\n\nprint(classification_report(y_test, y_pred))","1e6ada5b":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'gini', max_depth = 3, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 4, n_estimators = 180)\nrand_clf.fit(X_train, y_train)\n\ny_pred = rand_clf.predict(X_test)\n\nrand_clf_train_acc = accuracy_score(y_train, rand_clf.predict(X_train))\nrand_clf_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Random Forest Model is {rand_clf_train_acc}\")\nprint(f\"Test Accuracy of Random Forest Model is {rand_clf_test_acc}\")","7fd69056":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","34754799":"# classification report\n\nprint(classification_report(y_test, y_pred))","8eb4390b":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 5, verbose = 1)\ngrid_search.fit(X_train, y_train)","a2cb71fd":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","8391dd6f":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME', learning_rate = 0.001, n_estimators = 120)\nada.fit(X_train, y_train)\n\nada_train_acc = accuracy_score(y_train, ada.predict(X_train))\nada_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Ada Boost Model is {ada_train_acc}\")\nprint(f\"Test Accuracy of Ada Boost Model is {ada_test_acc}\")","7a5e2b18":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","bfbd667f":"# classification report\n\nprint(classification_report(y_test, y_pred))","93445faf":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search = GridSearchCV(gb, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","a4e7d904":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","579c4968":"gb = GradientBoostingClassifier(learning_rate = 0.1, loss = 'deviance', n_estimators = 150)\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\n\ngb_train_acc = accuracy_score(y_train, gb.predict(X_train))\ngb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Gradient Boosting Classifier Model is {gb_train_acc}\")\nprint(f\"Test Accuracy of Gradient Boosting Classifier Model is {gb_test_acc}\")","357229ee":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","6cf64eaf":"# classification report\n\nprint(classification_report(y_test, y_pred))","a287e3a7":"sgbc = GradientBoostingClassifier(learning_rate = 0.1, subsample = 0.9, max_features = 0.75, loss = 'deviance',\n                                  n_estimators = 100)\n\nsgbc.fit(X_train, y_train)\n\ny_pred = sgbc.predict(X_test)\n\nsgbc_train_acc = accuracy_score(y_train, sgbc.predict(X_train))\nsgbc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of SGB Model is {sgbc_train_acc}\")\nprint(f\"Test Accuracy of SGB Model is {sgbc_test_acc}\")","ee3747a6":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","1238967c":"# classification report\n\nprint(classification_report(y_test, y_pred))","e72fb9c3":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(iterations = 30, learning_rate = 0.1)\ncat.fit(X_train, y_train)\n\ny_pred = cat.predict(X_test)","29e0b9ba":"\ncat_train_acc = accuracy_score(y_train, cat.predict(X_train))\ncat_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Cat Boost Classifier Model is {cat_train_acc}\")\nprint(f\"Test Accuracy of Cat Boost Classifier Model is {cat_test_acc}\")","5df747bb":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, max_depth = 3, n_estimators = 10)\nxgb.fit(X_train, y_train)\n\ny_pred = xgb.predict(X_test)\n\nxgb_train_acc = accuracy_score(y_train, xgb.predict(X_train))\nxgb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of XGB Model is {xgb_train_acc}\")\nprint(f\"Test Accuracy of XGB Model is {xgb_test_acc}\")","aafcd692":"# let's divide our dataset into training set and holdout set by 50% \n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val_train, test, val_test = train_test_split(X, y, test_size = 0.5, random_state = 355)","e199d7bc":"# let's split the training set again into training and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(train, test, test_size = 0.2, random_state = 355)","c317ed0a":"# using Logistic Regression and SVM algorithm as base models.\n# Let's fit both of the models first on the X_train and y_train data.\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","b2d4e345":"svm = SVC()\nsvm.fit(X_train, y_train)","9994cddf":"predict_val1 = lr.predict(val_train)\npredict_val2 = svm.predict(val_train)","abe9f0ba":"predict_val = np.column_stack((predict_val1, predict_val2))","3beb60ea":"predict_test1 = lr.predict(X_test)\npredict_test2 = svm.predict(X_test)","7f896054":"predict_test = np.column_stack((predict_test1, predict_test2))","5ed523ae":"rand_clf = RandomForestClassifier()\nrand_clf.fit(predict_val, val_test)","decb54f1":"stacking_acc = accuracy_score(y_test, rand_clf.predict(predict_test))\nprint(stacking_acc)","6167cacd":"# confusion matrix\n\nconfusion_matrix(y_test, rand_clf.predict(predict_test))","6c473440":"# classification report\n\nprint(classification_report(y_test, rand_clf.predict(predict_test)))","041fbc49":"models = ['Logistic Regression', 'KNN', 'SVC', 'Decision Tree', 'Random Forest','Ada Boost', 'Gradient Boosting', 'SGB', 'XgBoost', 'Stacking', 'Cat Boost']\nscores = [lr_test_acc, knn_test_acc, svc_test_acc, dtc_test_acc, rand_clf_test_acc, ada_test_acc, gb_test_acc, sgbc_test_acc, xgb_test_acc, stacking_acc, cat_test_acc]\n\nmodels = pd.DataFrame({'Model' : models, 'Score' : scores})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","ce31e57e":"plt.figure(figsize = (18, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.show()","4b795f56":"## Support Vector Classifier","e8e083d1":"## Random Forest Classifier","2bf92340":"Also, we can see that Glucose, Insulin, Skin Thickness, BMI and Blood Pressure which have value as 0. That's not possible. We can either remove such data or simply replace it with their respective mean values.","1565d426":"## Gradient Boosting Classifier","b9537719":"## Boosting","a7446f5b":"Let's get the prediction of all the base models on test set X_set.","31ed5da6":"## K Neighbors Classifier (KNN)","49c09799":"Let's use the Stacked data 'predict_val' and val_test as the input feature for meta_model i.e. randomforest classifier.","f502f252":"## Decision Tree Classifier","8f1f7e62":"We can see that there is some skewness in data.","153daa63":"Let's get the predictions of all the base models on the validation set val_train.","5680ce61":"### Stochastic Gradient Boosting (SGB)","b5d0e856":"### We can see \"Cat Boost\" and \"Stacking\" is giving best results.","57de95d3":"Now we have dealt with the 0 values and data looks better.But, there still are outliers present in some columns. Let's deal with them.","ce028cf1":"### Ada Boost Classifier","ad5151fd":"## Stacking","7809234a":"### Extreme Gradient Boosting (XGBoost) ","f652eadd":"Let's check the accuracy of our meta_model using predict_test and y_test.","73accbbf":"Let's stack the prediction values for validation set together as 'predict_set'","05f212d4":"#### Accuracy score increases a lot after use of stacking.","ec1577c3":"### If you like this kernel, please do a upvote.","00851c48":"Let's stack the prediction values for validation set together as 'predict_val'","8e20404b":"*It seems that there are no missing values in our data. Great, let's see the distribution of data:*","e1b90309":"### Cat Boost Classifier"}}