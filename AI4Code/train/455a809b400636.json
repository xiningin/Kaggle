{"cell_type":{"dc67d350":"code","809df4d1":"code","3fa9e35e":"code","d9d8f258":"code","0e080194":"code","2392f146":"code","f3997d35":"code","5d5843a5":"code","4263b213":"code","992b9dda":"code","fbec87ab":"code","75b94075":"code","eb27b810":"code","03807a8f":"code","67a2b71d":"code","2886e273":"markdown","a16c2a47":"markdown","9a158249":"markdown","227b9a77":"markdown","1a359d41":"markdown","8afb5322":"markdown","48e42de4":"markdown","698b4802":"markdown","b90240b4":"markdown","6b852f7c":"markdown","b713aeee":"markdown","b01858df":"markdown","009dbab0":"markdown","7508c1d5":"markdown","cdb5d062":"markdown","0fed2346":"markdown","752f5aa4":"markdown","d6ae4d83":"markdown","3c192295":"markdown","c457e958":"markdown","b01f245c":"markdown","a8c6e5cf":"markdown"},"source":{"dc67d350":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# Define the number of inputs and outputs\nout_nodes = 2\nin_nodes = 2\n\nmodel = Sequential()    # This function allows you to create a sequantial model (a stack) to which you can add as many dense layers as you wish.\n\n# Define a hidden layer with single perceptron.\ndense_layer = Dense(out_nodes, activation='sigmoid', kernel_initializer=\"Ones\", bias_initializer=\"Ones\")  # An activation function in a neural network provides non-linearity to the data which is important for learning features from the input data, else the learning will stop at a particular stage and leads to a dying neuron problem.\n\nmodel.add(dense_layer)","809df4d1":"import numpy as np\n\n# Generate some random data\ntrain_data = np.random.random((1000, 100))\ntrain_labels = np.random.randint(2, size=(1000, 1))\ntest_data = np.random.random((100, 100))\ntest_labels = np.random.randint(2, size=(100, 1))\n\nunits = 32\n\nmodel = Sequential()\n\nmodel.add(Dense(units, activation='relu', input_dim=100))       # Input dimension should be equal to the number of features\nmodel.add(Dense(units, activation='relu'))\n\n# The output should be a single outcome so one Dense layer is defined with a single unit.\nmodel.add(Dense(1, activation='sigmoid'))","3fa9e35e":"from tensorflow.keras.optimizers import Adam, SGD, Adadelta, Adagrad, Adamax, Nadam, RMSprop\n\nadam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nsgd = SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\nadad = Adadelta(lr=1.0,rho=0.95,epsilon=None,decay=0.0)\nadag = Adagrad(lr=0.01,epsilon=None,decay=0.0)\nadamax = Adamax(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.0)\nnadam = Nadam(lr=0.002,beta_1=0.9,beta_2=0.999,epsilon=None,schedule_decay=0.004)\nrms = RMSprop(lr=0.001,rho=0.9,epsilon=None,decay=0.0)\n\nloss = ['sparse_categorical_crossentropy','mean_squared_error','mean_absolute_error',\n        'categorical_crossentropy','categorical_hinge']\n\nmetrics = ['accuracy','precision','recall']","d9d8f258":"# Compile the above created model\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])  # Optimises the learning by updating the weights with Stochastic Gradient Descent method.\n\n# Train the model by fitting the train data to the model we compiled in the above line. This is stored in a variable because the output of 'fit' function is a history class which consists of 4 key, value pairs for accuracy, val_accuracy, loss, val_loss\nhistory = model.fit(train_data, train_labels, epochs=30, batch_size=128)\n_, train_acc = model.evaluate(train_data, train_labels, verbose=1)\n_, test_acc = model.evaluate(test_data, test_labels, verbose=1)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","0e080194":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nmnist_fashion = tf.keras.datasets.fashion_mnist\n(x_train, y_train),(x_test, y_test) = mnist_fashion.load_data()\n\n# split training set into training set and validation set using train_test_split provided by scikit-learn \nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=77)\n\nnum_classes = 10   # The items in the dataset are to be classified into 1 of the 10 classes.\n\nprint(x_train.shape, x_val.shape, x_test.shape)","2392f146":"import matplotlib.pyplot as plt\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nplt.figure(figsize=(6,6))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(x_train[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[y_train[i]])\nplt.show()","f3997d35":"from tensorflow.keras.layers import Flatten\n\nmodel = Sequential()\nmodel.add(Flatten())  # This function flattens the input data\n\n# Feel free to play around with different parameters here like number of units in each layer or switching the activation function or increasing\/decreasing the number of layers.\nmodel.add(Dense(512, activation='relu'))    \n\nmodel.add(Dense(256, activation='relu')) \n\nmodel.add(Dense(128, activation='relu'))\n\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Dense(10, activation='softmax'))   # The number of units in the last layer should always be the number of classes in which we have to classify our input data.","5d5843a5":"model.compile(optimizer=adam, loss=loss[0], metrics=metrics[0])","4263b213":"batch_size = 128\nepochs = 50\n\nhistory = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_val, y_val))  # You can set verbose to 1 to get the status of your model training, 2 to get one line per epoch, here I kept it 0 to keep the notebook precise. \n\n_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n_, test_acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","992b9dda":"def plot_history(history):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()","fbec87ab":"plot_history(history)","75b94075":"from  tensorflow.keras import regularizers\n\n# Build the model\nmodel = Sequential()\nmodel.add(Flatten())\n\n# Add l2 regularizer (with 0.001 as regularization value) as kernel regularizer \nmodel.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n\n# Add l2 regularizer (with 0.001 as regularization value) as kernel regularizer \nmodel.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n\n# Add l2 regularizer (with 0.001 as regularization value) as kernel regularizer \nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n\n# Add l2 regularizer (with 0.001 as regularization value) as kernel regularizer \nmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n\n# Add a Dense layer with number of neurons equal to the number of classes, with softmax as activation function\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model created above.\nmodel.compile(optimizer=adam, loss=loss[0], metrics=metrics[0])\n\n\n# Fit the model created above to training and validation sets.\nhistory = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_val, y_val))\n\n# Call the plot_history function to plot the obtained results\nplot_history(history)\n\n# Evaluate the results\n_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n_, test_acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","eb27b810":"from tensorflow.keras.layers import Dropout\n\n# Build the model\nmodel = Sequential()\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))   # Add a dropout layer with 0.3 probability\nmodel.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))   # Add a dropout layer with 0.3 probability\nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))   # Add a dropout layer with 0.3 probability\nmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dense(10, activation='softmax'))\n \nmodel.compile(optimizer=adam, loss=loss[0], metrics=metrics[0])\n\nhistory = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_val, y_val))\n\nplot_history(history)\n\n_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n_, test_acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","03807a8f":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport os\n\nos.mkdir('my_checkpoint_dir')\n\n# Early stopping, for more refer documentation here: https:\/\/keras.io\/callbacks\/\nes_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)","67a2b71d":"model = Sequential()\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer=adam, loss=loss[0], metrics=metrics[0])\n\nhistory = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(x_val, y_val), callbacks=[es_callback])  # The callbacks parameter of the fit() function is responsible to handle the Early Stopping.\n\nplot_history(history)\n\n_, train_acc = model.evaluate(x_train, y_train, verbose=1)\n_, test_acc = model.evaluate(x_test, y_test, verbose=1)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","2886e273":"#### Listed below are different optimizers, loss functions and metrics. For more information, you can always refer to the documentations [here](http:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers), [here](http:\/\/keras.io\/losses\/) and [here](http:\/\/www.tensorflow.org\/api_docs\/python\/tf\/metrics)","a16c2a47":"#### By now we have created our first single layer perceptron. But the work might not be visible yet.","9a158249":"### Compile the model <a class=\"anchor\" id=\"ctm\"><\/a>","227b9a77":"## Voila! We have created our first mutiple layer deep neural network which is actually learning from the data and giving you results.","1a359d41":"# About Keras\n<img src=\"https:\/\/drive.google.com\/uc?id=1qUd5pI-C3DOFbJs8tGos9OCKZmPnNZYB\" width=\"400px\"><br>\nKeras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow.<br>\nIt allows for easy and fast prototyping and supports both convolutional networks and recurrent networks.<br>\nIts most important features are: user friendliness, modularity, easy extensibility","8afb5322":"### Early Stopping <a class=\"anchor\" id=\"es\"><\/a>\n\nThe training could be stopped at an epoch where there isn't any improvement of accuracy or loss for a specific number of rounds. These specific number of rounds is called 'patience'. Even if we declare a total of 1,000 epochs, the training will stop according to the patience once it finds no improvement in the accuracy.<br>\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1prBt0w2oWIviRDjOSRKP2TglDGUr-EGZ\" width=\"400px\"><br><br>","48e42de4":"### Dropout <a class=\"anchor\" id=\"drop\"><\/a>\n\nDropout is another most commonly used regularization techniques for neural networks.<br>\nDropout, applied as a hidden layer, randomly drops out a number of output features of the layer during training.<br>\n<img src=\"https:\/\/drive.google.com\/uc?id=1PcGnn9cdM5JQk2nxRHAFPZAtBj1fNMvk\" width=\"600px\"><br>\n\nThe \"dropout rate\" is the fraction of the features that are being dropped; it is usually set between 0.2 and 0.5.\n\nAt test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.","698b4802":"### This notebook is a tutorial to get beginners started with building first deep neural network using Keras and Tensorflow. Feel free to suggest edits and shortcomings of the notebook. Do show your support if you like the notebook.","b90240b4":"# Conclusion <a class=\"anchor\" id=\"conc\"><\/a>\n\nThere we go! We have been able to optimise this model to reduce the problem of over-fitting as much as possible. Still there are a lot of things which could be done to improve the accuracy of the model since the current model only gives an accuracy of ~87 on training set and ~85 on the test set. The parameters haven't been tuned yet which could open doors to more accurate models. But we will end this notebook here and leave you all to play with the parameters and improve the scores achieved above! Cheers!  ","6b852f7c":"# Table of contents","b713aeee":"# Deep Neural Network on Fashion MNIST data. <a class=\"anchor\" id=\"dnn\"><\/a>\nNow, it's time to work on a real data and build a more robust deep neural network.\nWe will use the Fashion MNIST dataset for our purpose.<br>\nFashion-MNIST is a dataset of Zalando\u2019s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples.\nEach example is a 28\u00d728 grayscale image, associated with a label from 10 classes.","b01858df":"### Plot some sample images from the dataset. <a class=\"anchor\" id=\"psi\"><\/a>","009dbab0":"## Building a multilayer perceptron. <a class=\"anchor\" id=\"mlp\"><\/a>\nMLPs are fully connected with each node connected each node in the other layer. A diagram below gives a taste of what an MLP looks like.\n\n<img src=\"https:\/\/drive.google.com\/uc?id=16Uxs11_l8xl58978mRsxgEtdaTSG5KGb\" width=\"400px\"><br>\n\n#### We will build a MLP with 2 hidden dense layers and another dense layer for the output.","7508c1d5":"1. [Building a single Layer Perceptron](#slp) <br>\n    1.1 [Activation Functions](#af) <br>\n2. [Building a multilayer Perceptron](#mlp) <br>\n    2.1 [Configurations of a model](#cm)<br>\n3. [Deep Neural Network on Fashion MNIST data](#dnn)<br>\n    3.1 [Plot sample images from the dataset](#psi)<br>\n    3.2 [Build a DNN](#bdnn)<br>\n    3.3 [Compile the model](#ctm)<br>\n    3.4 [Train the model](#tm)<br>\n    3.5 [Plot the training history visualization](#pthv)<br>\n4. [Managing Model Overfitting](#mmo)<br>\n    4.1 [Weight Regularization](#wr)<br>\n    4.2 [Dropout](#drop)<br>\n    4.3 [Early Stopping](#es)<br>\n5. [Conclusion](#conc)","cdb5d062":"#### Seems the model is overfitting here. An ideal loss plot should look similar to this.\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQHXfXuIksn-3ejS8UFeawUppJTmN2endFI6w&usqp=CAU\" width=\"460px\" align=\"center\">\n\n#### But at least we have managed to build our first deep neural network. Hurray! We have come a long way.","0fed2346":"### Weight Regularization <a class=\"anchor\" id=\"wr\"><\/a>\n\nOne of the most common way is to put constraints on the complexity of a network by forcing its weights only to take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights.\n\nWeight regularization could be done with 2 different techniques:\n  * L1 regularization or Lasso Regression\n  * L2 regularization or Ridge Regression\n  \nIn tf.keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments.","752f5aa4":"### Configurations of a model <a class=\"anchor\" id=\"cm\"><\/a>\n\n**Loss functions** are used to compare the network's predicted output  with the real output, in each pass of the backpropagations algorithm<br>\nCommon loss functions are: mean-squared error, cross-entropy, and so on...<br><br>\n**Metrics** are used to evaluate a model; common metrics are precision, recall, accuracy, auc,..<br>\n\nThe **Optimizer** determines the update rules of the weights. \nBelow is an idea on how different optimizers work. (Image Courtesy: University of Genoa) <br>\n\n<img src=\"https:\/\/drive.google.com\/uc?id=151nqtkDry6XPXYJvo3VqCHlwcB9XDXbk\" width=\"460px\" align=\"center\">\n<img src=\"https:\/\/drive.google.com\/uc?id=1UfM-0QruDiOgy8cSxLmZHx3FI4pzLeDQ\" width=\"360px\" align=\"center\">","d6ae4d83":"### Train your model <a class=\"anchor\" id=\"tm\"><\/a>","3c192295":"### Build a deep neural network on this dataset. <a class=\"anchor\" id=\"bdnn\"><\/a>","c457e958":"# Building a single layer perceptron <a class=\"anchor\" id=\"slp\"><\/a>\n<img src=\"https:\/\/drive.google.com\/uc?id=1cvJa9WfOier2NQIJGHpqDNGn-RlczU1x\" width=\"500px\"><br>\nA sigle layer perceptron can be thought of as a single learning unit in the network. This node will get inputs from the input layer, perform the learning task along with activation function. <br>\n### Some most commonly used activation functions are shown here. <a class=\"anchor\" id=\"af\"><\/a>\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1Rr_1OJeORL6qmGFplC0E2hFj-KapFGN0\" width=\"600px\"><br>","b01f245c":"### Plot the training history visualisation <a class=\"anchor\" id=\"pthv\"><\/a>","a8c6e5cf":"# Managing Model Overfitting! <a class=\"anchor\" id=\"mmo\"><\/a>\n\nNow that we have trained our very first model, it's time to optimise the model training dealing with the problem of overfitting. \n\nOverfitting could be handled in many ways. Some most common ways we will see here include:\n* Weight Regularization\n* Dropout\n* Early Stopping or Callbacks"}}