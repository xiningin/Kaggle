{"cell_type":{"8a7803a5":"code","7f25160f":"code","fb41db7e":"code","015171a0":"code","4563012a":"code","157da6a1":"code","e5b08cb9":"code","6d2dcf7c":"code","3bdd5eeb":"code","48c4b90b":"code","3f13169e":"code","b1cd8a14":"code","ca1d8483":"code","368c918a":"code","2f16100f":"code","c3a02427":"code","059f4cde":"code","5fb2d36e":"code","6edb83d9":"code","6984f714":"code","3bf574c9":"code","4cef0514":"code","b12422f8":"code","fa0aa209":"code","712139c3":"markdown","08012152":"markdown","79a38933":"markdown","0454690d":"markdown","6674a4e8":"markdown","5caa04ac":"markdown","1d12e0a0":"markdown","139de66f":"markdown","4e04a04f":"markdown","97c61505":"markdown","13785239":"markdown","1c8bee27":"markdown","19108198":"markdown","d83f26e0":"markdown","592ed2aa":"markdown","4c3ed60d":"markdown","adfe0503":"markdown","b2f498cf":"markdown","4b5c582b":"markdown","4d465398":"markdown"},"source":{"8a7803a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nfrom matplotlib.patches import Polygon\nimport seaborn as sns\nfrom itertools import cycle\n\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f25160f":"# Read in the data\nINPUT_DIR = '..\/input\/m5-forecasting-accuracy'\ncalendar = pd.read_csv(f'{INPUT_DIR}\/calendar.csv')\nsales_train_validation = pd.read_csv(f'{INPUT_DIR}\/sales_train_validation.csv')\nsample_submission = pd.read_csv(f'{INPUT_DIR}\/sample_submission.csv')\nsell_prices = pd.read_csv(f'{INPUT_DIR}\/sell_prices.csv')\n#sample_submission = sample_submission.set_index('id')\n","fb41db7e":"#Explain the data of CALENDAR\n#calendar.csv (1969 rows; 14 columns) contains information about the dates the products are sold\n#1)date in y-m-d formate. 2)wm_yr_wk:the id of the week the date belongs to\n#3)weekday: The type of the day (Monday, Teusday ...)\n#4)wday: the id of the weekday starting from Saturday 5)month 6)year\n#7)d: the day id: d_1, d_2,....d_1941 (but in the file, there are 1969 unique values?)\n#8)event_name_1 9)event_type_1 10)event_name_2 11)event_type_2 => If the date includes events: name and type of these events.\n#12)snap_CA 13)snap_TX 14)snap_WI: a binary variable (0 or 1) indicating whether the stores of CA, TX or WI \n#allow SNAP purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n#SNAP: Supplement Nutrition Assistance Program. \n#SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products\n#In many states, the monetary benefits are dispersed to people across 10 days of the month \n#and on each of these days 1\/10 of the people will receive the benefit on their card\ncalendar.head()","015171a0":"#Explain the data of SALES PRICE\n#sales_prices.csv (~6.84m rows; 4 columns) contains information about the price of the products sold per store and date\n#1) store_id: The id of the store where the product is sold\n#2) item_id: The id of the product\n#3) wm_yr_wk: The id of the week\n#4) sell_price: The price of the product for the given week\/store. The price is provided per week (average acrros seven days).\n#If not available, this means that the products was not sold during the examined week.\n#Noted that although prices are constant at weekly basis, they may change through time (both training and test set)\nsell_prices.head()","4563012a":"#Explain the data of SALES TRAIN \n# sales_train_validation.csv (30.49k rows; 1919 columns) Contain the historical daily unit sales data per product and store\n# 1)id: id of the product\n# 2)item_id: The id of the product\n# 3)dept_id: The id of the department the product belong to\n# 4)cat_id: The id of the category the product belong to\n# 5)store_id: The id of the store where the product is sold\n# 6)state_id: The State where the store is located.\n# 7)d_1, d_2, ...d_1941: The number of units solds at day_i, starting from 2011-01-29\nsales_train_validation.head()","157da6a1":"#Seeing what is in the sample_submission.csv file & what do we have to do?\n#The columns represent 28 forecast days. We will fill these forecast days with our predictions\n#The rows represent a specific item. This id tells us the item type, state, and store.\nsample_submission.head()","e5b08cb9":"pd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","6d2dcf7c":"# getting the sales data columns ['d_1','d_2','d_3'...,'d_1913']\ndata_columns = [c for c in sales_train_validation.columns if 'd_' in c] \n\n# Below we are chaining the following steps in pandas:\n# 1. Select the item.\n# 2. Set the id as the index, Keep only sales data columns\n# 3. Transform so it's a column\n# 4. Plot the data\n# choose the item id 'FOODS_3_090_CA_3_validation'\nsales_train_validation.loc[sales_train_validation['id'] == 'FOODS_3_090_CA_3_validation'] \\\n    .set_index('id')[data_columns] \\\n    .T \\\n    .plot(figsize=(15, 5),\n          title='FOODS_3_090_CA_3 sales by \"d\" number',\n          color=next(color_cycle))\nplt.legend('')\nplt.show()","3bdd5eeb":"# Get the sales data of an item ('FOODS_3_090_CA_3_validation')\nexample1 = sales_train_validation.loc[sales_train_validation['id'] == 'FOODS_3_090_CA_3_validation'][data_columns].T\n# Rename, since 'FOODS_3_090_CA_3_validation' have the index 8412\nexample1 = example1.rename(columns={8412:'FOODS_3_090_CA_3'}) \n# make the index \"d\"\nexample1 = example1.reset_index().rename(columns={'index': 'd'})\nexample1.head()","48c4b90b":"# Merge information from calendar to the sales data\nexample1 = example1.merge(calendar, how='left', validate='1:1')\nexample1.head()","3f13169e":"# Select more top selling examples\nexample2 = sales_train_validation.loc[sales_train_validation['id'] == 'HOBBIES_1_234_CA_3_validation'][data_columns].T\nexample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctly\nexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample2 = example2.merge(calendar, how='left', validate='1:1')\n\nexample3 = sales_train_validation.loc[sales_train_validation['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][data_columns].T\nexample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctly\nexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample3 = example3.merge(calendar, how='left', validate='1:1')","b1cd8a14":"examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']\nexample_df = [example1, example2, example3]\nfor i in [0, 1, 2]:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n    example_df[i].groupby('wday').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: day of week',\n              lw=5,\n              color=color_pal[0],\n              ax=ax1)\n    example_df[i].groupby('month').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: month',\n              lw=5,\n              color=color_pal[4],\n\n              ax=ax2)\n    example_df[i].groupby('year').mean()[examples[i]] \\\n        .plot(kind='line',\n              lw=5,\n              title='average sale: year',\n              color=color_pal[2],\n\n              ax=ax3)\n    fig.suptitle(f'Trends for item: {examples[i]}',\n                 size=20,\n                 y=1.1)\n    plt.tight_layout()\n    plt.show()","ca1d8483":"twenty_examples = sales_train_validation.sample(20, random_state=529) \\\n        .set_index('id')[data_columns] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\nfig, axs = plt.subplots(10, 2, figsize=(15, 20))\naxs = axs.flatten()\nax_idx = 0\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(title=item,\n                              color=next(color_cycle),\n                              ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","368c918a":"sales_train_validation.groupby('cat_id').count()['id'] \\\n    .sort_values() \\\n    .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')\nplt.show()\nsales_train_validation['cat_id'].unique()","2f16100f":"past_sales = sales_train_validation.set_index('id')[data_columns] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\nfor i in sales_train_validation['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(sales_train_validation['cat_id'].unique())\nplt.show()\n","c3a02427":"past_sales_clipped = past_sales.clip(0, 1)\nfor i in sales_train_validation['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    (past_sales_clipped[items_col] \\\n        .mean(axis=1) * 100) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Inventory Sale Percentage by Date',\n              style='.')\nplt.ylabel('% of Inventory with at least 1 sale')\nplt.legend(sales_train_validation['cat_id'].unique())\nplt.show()","059f4cde":"store_list = sell_prices['store_id'].unique()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(90).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(store_list)\nplt.show()","5fb2d36e":"fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(7).mean() \\\n        .plot(alpha=1,\n              ax=axes[ax_idx],\n              title=s,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n# plt.legend(store_list)\nplt.suptitle('Weekly Sale Trends by Store ID')\nplt.tight_layout()\nplt.show()","6edb83d9":"print('The lowest sale date was:', past_sales.sum(axis=1).sort_values().index[0],\n     'with', past_sales.sum(axis=1).sort_values().values[0], 'sales')\nprint('The lowest sale date was:', past_sales.sum(axis=1).sort_values(ascending=False).index[0],\n     'with', past_sales.sum(axis=1).sort_values(ascending=False).values[0], 'sales')","6984f714":"# ----------------------------------------------------------------------------\n# Author:  Nicolas P. Rougier\n# License: BSD\n# ----------------------------------------------------------------------------\n\ndef calmap(ax, year, data):\n    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n\n    # Month borders\n    xticks, labels = [], []\n    start = datetime(year,1,1).weekday()\n    for month in range(1,13):\n        first = datetime(year, month, 1)\n        last = first + relativedelta(months=1, days=-1)\n\n        y0 = first.weekday()\n        y1 = last.weekday()\n        x0 = (int(first.strftime(\"%j\"))+start-1)\/\/7\n        x1 = (int(last.strftime(\"%j\"))+start-1)\/\/7\n\n        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n              (x0+1,  0), (x0+1,  y0) ]\n        xticks.append(x0 +(x1-x0+1)\/2)\n        labels.append(first.strftime(\"%b\"))\n        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n                       linewidth=1, zorder=20, clip_on=False)\n        ax.add_artist(poly)\n    \n    ax.set_xticks(xticks)\n    ax.set_xticklabels(labels)\n    ax.set_yticks(0.5 + np.arange(7))\n    ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    ax.set_title(\"{}\".format(year), weight=\"semibold\")\n    \n    # Clearing first and last day from the data\n    valid = datetime(year, 1, 1).weekday()\n    data[:valid,0] = np.nan\n    valid = datetime(year, 12, 31).weekday()\n    # data[:,x1+1:] = np.nan\n    data[valid+1:,x1] = np.nan\n\n    # Showing data\n    ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=-1, vmax=1,\n              cmap=\"RdYlBu_r\", origin=\"lower\", alpha=.75)","3bf574c9":"from sklearn.preprocessing import StandardScaler\nsscale = StandardScaler()\npast_sales.index = pd.to_datetime(past_sales.index)\nfor i in sales_train_validation['cat_id'].unique():\n    fig, axes = plt.subplots(3, 1, figsize=(20, 8))\n    items_col = [c for c in past_sales.columns if i in c]\n    sales2013 = past_sales.loc[past_sales.index.isin(pd.date_range('31-Dec-2012',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\n    calmap(axes[0], 2013, vals.reshape(53,7).T)\n    sales2014 = past_sales.loc[past_sales.index.isin(pd.date_range('30-Dec-2013',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2014.values.reshape(-1, 1)))\n    calmap(axes[1], 2014, vals.reshape(53,7).T)\n    sales2015 = past_sales.loc[past_sales.index.isin(pd.date_range('29-Dec-2014',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2015.values.reshape(-1, 1)))\n    calmap(axes[2], 2015, vals.reshape(53,7).T)\n    plt.suptitle(i, fontsize=30, x=0.4, y=1.01)\n    plt.tight_layout()\n    plt.show()","4cef0514":"fig, ax = plt.subplots(figsize=(15, 5))\nstores = []\nfor store, d in sell_prices.query('item_id == \"FOODS_3_090\"').groupby('store_id'):\n    d.plot(x='wm_yr_wk',\n          y='sell_price',\n          style='.',\n          color=next(color_cycle),\n          figsize=(15, 5),\n          title='FOODS_3_090 sale price over time',\n         ax=ax,\n          legend=store)\n    stores.append(store)\n    plt.legend()\nplt.legend(stores)\nplt.show()","b12422f8":"sell_prices['Category'] = sell_prices['item_id'].str.split('_', expand=True)[0]\nfig, axs = plt.subplots(1, 3, figsize=(15, 4))\ni = 0\nfor cat, d in sell_prices.groupby('Category'):\n    ax = d['sell_price'].apply(np.log1p) \\\n        .plot(kind='hist',\n                         bins=20,\n                         title=f'Distribution of {cat} prices',\n                         ax=axs[i],\n                                         color=next(color_cycle))\n    ax.set_xlabel('Log(price)')\n    i += 1\nplt.tight_layout()","fa0aa209":"thirty_day_avg_map = sales_train_validation.set_index('id')[data_columns[-30:]].mean(axis=1).to_dict()\nfcols = [f for f in sample_submission.columns if 'F' in f]\nfor f in fcols:\n    sample_submission[f] = sample_submission['id'].map(thirty_day_avg_map).fillna(0)\n    \nsample_submission.to_csv('submission_average_30d.csv', index=False)","712139c3":"This file is created by Bang Nguyen for beginner of the M5 competition to easily understand and start making your first notebook to solve the problem.\n\nI do collect some codes from other notebooks.\n\nGive me an UPVOTE, if it is useful!\n\nThank you!","08012152":"**Rollout of items being sold**\n\nSince, some items come into supply that previously did not exist. Similarly some items stop being sold completely.\n\nLets plot the sales, but only count if item is selling or not selling.(0 -> not selling, >0 -> selling)\n\nThis plot shows us that many items are being slowly introduced into inventory, so many of them will not register a sale at the begining of the provided data.","79a38933":"#From the \"M5 competitors Guide Final\" file.\n\nThe M5 dataset involves the unit sales organize in the form of grouped time series.\n\nMore specifically, the data set involves that unit sales of **3,049 products** classified in **3 product categories** (Hobbies, Foods, and Household) and **7 product departments**.\n\nThe products are sold across **ten stores**, located in **three States** (CA, TX and WI).\n\n\nLevel \nid\tAggregation Level\tNumber of series\n\n1\tUnit sales of all products, aggregated for all stores\/states\t1\n\n2\tUnit sales of all products, aggregated for each State\t3\n\n3\tUnit sales of all products, aggregated for each store \t10\n\n4\tUnit sales of all products, aggregated for each category\t3\n\n5\tUnit sales of all products, aggregated for each department\t7\n\n6\tUnit sales of all products, aggregated for each State and category\t9\n\n7\tUnit sales of all products, aggregated for each State and department\t21\n\n8\tUnit sales of all products, aggregated for each store and category\t30\n\n9\tUnit sales of all products, aggregated for each store and department\t70\n\n10\tUnit sales of product x, aggregated for all stores\/states\t3,049\n\n11\tUnit sales of product x, aggregated for each State\t9,147\n\n12\tUnit sales of product x, aggregated for each store\t30,490\n\n![M5serires.png](attachment:M5serires.png)\n\nThe historical data range from **2011-01-29 ** to **2016-06-19** => the selling history of 1,941 days\/5.4 years.\n\nThe dataset includes these files: calendar.csv; sales_train_validation.csv; sell_prices.csv.(and sample_submission.csv)","0454690d":"**THE AVERAGE SALES ON WEEK, MONTH, AND YEAR **","6674a4e8":"# The Base-line submission\n**1. Naive:** A random walk model, defined as Y_n+1=Y_n, i = 1, 2, 3,...,h. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation.\n\n**2. Seasonal Naive (sNaive) ** :Like Naive, but this time the forecasts of the model are equal to the last known observation of the same period in order for it to capture possible weekly seasonal variations. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. ","5caa04ac":"**THE OBSERVATION ON COMBINED SALES OVER TIME by TYPE**\n\nPlot the total demand over time for each type","1d12e0a0":"THE OBSERVATION ON DIFFENT ITEMS\n* It is common to see an item unavailable for a period of time.\n* Some items only sell 1 or less in a day, making it very hard to predict.\n* Other items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these","139de66f":"# **THE BASE-LINE MODEL**","4e04a04f":"# **DATA EXPLORATION**","97c61505":"The average value from the past 30 days","13785239":"Some interesting things to note from these heatmaps:\n\n* Food tends to have lower number of purchases as the month goes on. Could this be because people get their paychecks early in the month?\n* Household and Hobby items sell much less in January - after the Holiday season is over.\n* Cleary weekends are more popular shopping days regardless of the item category.","1c8bee27":"WHAT EXACTLY ARE WE TRYING TO PREDICT?","19108198":"**DATA ANALYSIS**\n\nLets see how the sales of an item look across 1751 days of the training data\nThese codes are from\n> https:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration","d83f26e0":"**Sales by Store**","592ed2aa":"Looking at the same data a different way, we can plot a rolling 7 day total demand count by store. Note clearly that some stores have abrupt changes in their demand, it could be that the store expanded or a new competitor was built near by. Either way this is imporant to note when creating predictive models about demand pattern.","4c3ed60d":"# Sales Heatmap Calendar\nIt appears that walmarts are closed on Chirstmas day. The highest demand day of all the data was on Sunday March 6th, 2016. What happened on this day you may ask... well the Seventh Democratic presidential candidates debate hosted by CNN and held in Flint, Michigan... I doubt that impacted sales though :D","adfe0503":"****IMPORT PACKAGES**","b2f498cf":"**Requirements**: The number of forecasts required, both for point (**ACCURACY**) and probabilistic (**UNCERTAINTY**) forecasts, is **h=28 days**\n\nThe performance measures are **first computed for each series** separately by averaging their values across the forecasting horizon \nand then **averaged again across the series** in a weighted fashion to obtain the final scores.\n\n**Point forcasts**: The accuracy of the point forecasts will be evaluated using the **Root Mean Squared Scaled Error (RMSSE)** \n\n![RMSSE.png](attachment:RMSSE.png)\n\nwhere Y_t is the actual future value of the examined time series at point t, (Y_t ) \u0302 the generated forecast, n the length of the training sample (number of historical observations), and h the forecasting horizon.\n\n**PROBABILISTIC forcasts**: The accuracy of the point forecasts will be evaluated using the **Weighted Scaled Pinball Loss (WSPL)** ","4b5c582b":"Calmap function","4d465398":"# Sales Prices\nWe are given historical sale prices of each item. Lets take a look at our example item from before.\n\n* It looks to me like the price of this item is growing.\n* Different stores have different selling prices."}}