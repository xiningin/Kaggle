{"cell_type":{"e96e4235":"code","bc603c8c":"code","caa06618":"code","f71c8354":"code","bded28b0":"code","c95c49d2":"code","2dafa9ab":"code","dc255bb6":"code","385a55ff":"code","bbe2baaa":"code","cf09dc20":"code","3afaf248":"code","a8708ac9":"code","5ced740b":"code","ad606821":"code","a813e7e0":"code","ff5fc466":"code","908eba23":"code","caad6ad8":"code","38f6b799":"code","9ad14892":"code","5efdf6ca":"code","56a784c7":"code","9ad0d705":"code","d33b548c":"code","188f1015":"code","a92a3369":"code","03c92825":"code","047687ff":"code","b512b247":"markdown","06b30911":"markdown","cf2ed488":"markdown","00e71847":"markdown","cefe59c5":"markdown","d8dd853b":"markdown","976ededd":"markdown","ecd412f0":"markdown","d7f9aebf":"markdown"},"source":{"e96e4235":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport time\nimport os\nfrom tqdm.notebook import tqdm\n\n!pip install -q segmentation-models-pytorch\n!pip install -q torchsummary\n\nfrom torchsummary import summary\nimport segmentation_models_pytorch as smp\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","bc603c8c":"IMAGE_PATH = '..\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/original_images\/'\nMASK_PATH = '..\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/label_images_semantic\/'","caa06618":"n_classes = 23 \n\ndef create_df():\n    name = []\n    for dirname, _, filenames in os.walk(IMAGE_PATH):\n        for filename in filenames:\n            name.append(filename.split('.')[0])\n    \n    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n\ndf = create_df()\nprint('Total Images: ', len(df))","f71c8354":"#split data\nX_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\nX_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n\nprint('Train Size   : ', len(X_train))\nprint('Val Size     : ', len(X_val))\nprint('Test Size    : ', len(X_test))","bded28b0":"img = Image.open(IMAGE_PATH + df['id'][100] + '.jpg')\nmask = Image.open(MASK_PATH + df['id'][100] + '.png')\nprint('Image Size', np.asarray(img).shape)\nprint('Mask Size', np.asarray(mask).shape)\n\n\nplt.imshow(img)\nplt.imshow(mask, alpha=0.6)\nplt.title('Picture with Mask Appplied')\nplt.show()","c95c49d2":"class DroneDataset(Dataset):\n    \n    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n        self.patches = patch\n        self.mean = mean\n        self.std = std\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n        \n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = Image.fromarray(aug['image'])\n            mask = aug['mask']\n        \n        if self.transform is None:\n            img = Image.fromarray(img)\n        \n        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n        img = t(img)\n        mask = torch.from_numpy(mask).long()\n        \n        if self.patches:\n            img, mask = self.tiles(img, mask)\n            \n        return img, mask\n    \n    def tiles(self, img, mask):\n\n        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768) \n        img_patches  = img_patches.contiguous().view(3,-1, 512, 768) \n        img_patches = img_patches.permute(1,0,2,3)\n        \n        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n        \n        return img_patches, mask_patches","2dafa9ab":"mean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\nt_train = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(), \n                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n                     A.GaussNoise()])\n\nt_val = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(),\n                   A.GridDistortion(p=0.2)])\n\n#datasets\ntrain_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\nval_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n\n#dataloader\nbatch_size= 3 \n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)               ","dc255bb6":"model = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=23, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])","385a55ff":"model","bbe2baaa":"def pixel_accuracy(output, mask):\n    with torch.no_grad():\n        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n        correct = torch.eq(output, mask).int()\n        accuracy = float(correct.sum()) \/ float(correct.numel())\n    return accuracy","cf09dc20":"def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n    with torch.no_grad():\n        pred_mask = F.softmax(pred_mask, dim=1)\n        pred_mask = torch.argmax(pred_mask, dim=1)\n        pred_mask = pred_mask.contiguous().view(-1)\n        mask = mask.contiguous().view(-1)\n\n        iou_per_class = []\n        for clas in range(0, n_classes): #loop per pixel class\n            true_class = pred_mask == clas\n            true_label = mask == clas\n\n            if true_label.long().sum().item() == 0: #no exist label in this loop\n                iou_per_class.append(np.nan)\n            else:\n                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n                union = torch.logical_or(true_class, true_label).sum().float().item()\n\n                iou = (intersect + smooth) \/ (union +smooth)\n                iou_per_class.append(iou)\n        return np.nanmean(iou_per_class)","3afaf248":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n    torch.cuda.empty_cache()\n    train_losses = []\n    test_losses = []\n    val_iou = []; val_acc = []\n    train_iou = []; train_acc = []\n    lrs = []\n    min_loss = np.inf\n    decrease = 1 ; not_improve=0\n\n    model.to(device)\n    fit_time = time.time()\n    for e in range(epochs):\n        since = time.time()\n        running_loss = 0\n        iou_score = 0\n        accuracy = 0\n        #training loop\n        model.train()\n        for i, data in enumerate(tqdm(train_loader)):\n            #training phase\n            image_tiles, mask_tiles = data\n            if patch:\n                bs, n_tiles, c, h, w = image_tiles.size()\n\n                image_tiles = image_tiles.view(-1,c, h, w)\n                mask_tiles = mask_tiles.view(-1, h, w)\n            \n            image = image_tiles.to(device); mask = mask_tiles.to(device);\n            #forward\n            output = model(image)\n            loss = criterion(output, mask)\n            #evaluation metrics\n            iou_score += mIoU(output, mask)\n            accuracy += pixel_accuracy(output, mask)\n            #backward\n            loss.backward()\n            optimizer.step() #update weight          \n            optimizer.zero_grad() #reset gradient\n            \n            #step the learning rate\n            lrs.append(get_lr(optimizer))\n            scheduler.step() \n            \n            running_loss += loss.item()\n            \n        else:\n            model.eval()\n            test_loss = 0\n            test_accuracy = 0\n            val_iou_score = 0\n            #validation loop\n            with torch.no_grad():\n                for i, data in enumerate(tqdm(val_loader)):\n                    #reshape to 9 patches from single image, delete batch size\n                    image_tiles, mask_tiles = data\n\n                    if patch:\n                        bs, n_tiles, c, h, w = image_tiles.size()\n\n                        image_tiles = image_tiles.view(-1,c, h, w)\n                        mask_tiles = mask_tiles.view(-1, h, w)\n                    \n                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n                    output = model(image)\n                    #evaluation metrics\n                    val_iou_score +=  mIoU(output, mask)\n                    test_accuracy += pixel_accuracy(output, mask)\n                    #loss\n                    loss = criterion(output, mask)                                  \n                    test_loss += loss.item()\n            \n            #calculatio mean for each batch\n            train_losses.append(running_loss\/len(train_loader))\n            test_losses.append(test_loss\/len(val_loader))\n\n\n            if min_loss > (test_loss\/len(val_loader)):\n                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss\/len(val_loader))))\n                min_loss = (test_loss\/len(val_loader))\n                decrease += 1\n                if decrease % 5 == 0:\n                    print('saving model...')\n                    torch.save(model, 'Unet-Mobilenet_v2_mIoU-{:.3f}.pt'.format(val_iou_score\/len(val_loader)))\n                    \n\n            if (test_loss\/len(val_loader)) > min_loss:\n                not_improve += 1\n                min_loss = (test_loss\/len(val_loader))\n                print(f'Loss Not Decrease for {not_improve} time')\n                if not_improve == 7:\n                    print('Loss not decrease for 7 times, Stop Training')\n                    break\n            \n            #iou\n            val_iou.append(val_iou_score\/len(val_loader))\n            train_iou.append(iou_score\/len(train_loader))\n            train_acc.append(accuracy\/len(train_loader))\n            val_acc.append(test_accuracy\/ len(val_loader))\n            print(\"Epoch:{}\/{}..\".format(e+1, epochs),\n                  \"Train Loss: {:.3f}..\".format(running_loss\/len(train_loader)),\n                  \"Val Loss: {:.3f}..\".format(test_loss\/len(val_loader)),\n                  \"Train mIoU:{:.3f}..\".format(iou_score\/len(train_loader)),\n                  \"Val mIoU: {:.3f}..\".format(val_iou_score\/len(val_loader)),\n                  \"Train Acc:{:.3f}..\".format(accuracy\/len(train_loader)),\n                  \"Val Acc:{:.3f}..\".format(test_accuracy\/len(val_loader)),\n                  \"Time: {:.2f}m\".format((time.time()-since)\/60))\n        \n    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n               'train_miou' :train_iou, 'val_miou':val_iou,\n               'train_acc' :train_acc, 'val_acc':val_acc,\n               'lrs': lrs}\n    print('Total time: {:.2f} m' .format((time.time()- fit_time)\/60))\n    return history","a8708ac9":"max_lr = 1e-3\nepoch = 15\nweight_decay = 1e-4\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\nsched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n                                            steps_per_epoch=len(train_loader))\n\nhistory = fit(epoch, model, train_loader, val_loader, criterion, optimizer, sched)","5ced740b":"torch.save(model, 'Unet-Mobilenet.pt')","ad606821":"def plot_loss(history):\n    plt.plot(history['val_loss'], label='val', marker='o')\n    plt.plot( history['train_loss'], label='train', marker='o')\n    plt.title('Loss per epoch'); plt.ylabel('loss');\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()\n    \ndef plot_score(history):\n    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()\n    \ndef plot_acc(history):\n    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(), plt.grid()\n    plt.show()","a813e7e0":"plot_loss(history)\nplot_score(history)\nplot_acc(history)","ff5fc466":"class DroneTestDataset(Dataset):\n    \n    def __init__(self, img_path, mask_path, X, transform=None):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n      \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n        \n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = Image.fromarray(aug['image'])\n            mask = aug['mask']\n        \n        if self.transform is None:\n            img = Image.fromarray(img)\n        \n        mask = torch.from_numpy(mask).long()\n        \n        return img, mask\n\n\nt_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\ntest_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)","908eba23":"def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    model.eval()\n    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n    image = t(image)\n    model.to(device); image=image.to(device)\n    mask = mask.to(device)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        score = mIoU(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0)\n    return masked, score","caad6ad8":"def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    model.eval()\n    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n    image = t(image)\n    model.to(device); image=image.to(device)\n    mask = mask.to(device)\n    with torch.no_grad():\n        \n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        acc = pixel_accuracy(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0)\n    return masked, acc","38f6b799":"image, mask = test_set[3]\npred_mask, score = predict_image_mask_miou(model, image, mask)","9ad14892":"def miou_score(model, test_set):\n    score_iou = []\n    for i in tqdm(range(len(test_set))):\n        img, mask = test_set[i]\n        pred_mask, score = predict_image_mask_miou(model, img, mask)\n        score_iou.append(score)\n    return score_iou","5efdf6ca":"mob_miou = miou_score(model, test_set)","56a784c7":"def pixel_acc(model, test_set):\n    accuracy = []\n    for i in tqdm(range(len(test_set))):\n        img, mask = test_set[i]\n        pred_mask, acc = predict_image_mask_pixel(model, img, mask)\n        accuracy.append(acc)\n    return accuracy","9ad0d705":"mob_acc = pixel_acc(model, test_set)","d33b548c":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\nax1.imshow(image)\nax1.set_title('Picture');\n\nax2.imshow(mask)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask)\nax3.set_title('UNet-MobileNet | mIoU {:.3f}'.format(score))\nax3.set_axis_off()","188f1015":"image2, mask2 = test_set[4]\npred_mask2, score2 = predict_image_mask_miou(model, image2, mask2)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\nax1.imshow(image2)\nax1.set_title('Picture');\n\nax2.imshow(mask2)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask2)\nax3.set_title('UNet-MobileNet | mIoU {:.3f}'.format(score2))\nax3.set_axis_off()","a92a3369":"image3, mask3 = test_set[6]\npred_mask3, score3 = predict_image_mask_miou(model, image3, mask3)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\nax1.imshow(image3)\nax1.set_title('Picture');\n\nax2.imshow(mask3)\nax2.set_title('Ground truth')\nax2.set_axis_off()\n\nax3.imshow(pred_mask3)\nax3.set_title('UNet-MobileNet | mIoU {:.3f}'.format(score3))\nax3.set_axis_off()","03c92825":"print('Test Set mIoU', np.mean(mob_miou))","047687ff":"print('Test Set Pixel Accuracy', np.mean(mob_acc))","b512b247":"# What is Semantic Segmentation?\nSemantic segmentation refers to the process of linking each pixel in an image to a class label. These labels could include a person, car, flower, piece of furniture, etc., just to mention a few.\nWe can think of semantic segmentation as image classification at a pixel level. For example, in an image that has many cars, segmentation will label all the objects as car objects. However, a separate class of models known as instance segmentation is able to label the separate instances where an object appears in an image. This kind of segmentation can be very useful in applications that are used to count the number of objects, such as counting the amount of foot traffic in a mall.","06b30911":"# Please upvote the kernel if you found it insightful!","cf2ed488":"# Evaluation","00e71847":"# Dataset","cefe59c5":"## Result","d8dd853b":"# Training","976ededd":"# Preprocessing","ecd412f0":"# Model","d7f9aebf":"# Import Libraries"}}