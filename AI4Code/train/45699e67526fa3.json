{"cell_type":{"90039497":"code","a571db57":"code","ed075ee9":"code","44af8396":"code","04eac23c":"code","01b13aa3":"code","47741594":"code","e1635888":"code","bd84c9cb":"code","c8f0ac3f":"code","4c136114":"code","d5006d8a":"code","417c7f94":"code","f792356b":"code","f31fd5c0":"code","b1e65c19":"code","9016c628":"code","ca171574":"code","20b17be2":"code","098c128b":"code","e66e0dfc":"code","67461f45":"code","bb8eac16":"markdown","c3ffb1c9":"markdown","d04b351c":"markdown","b3d869fb":"markdown","91c4b08b":"markdown","820695c2":"markdown","46242d30":"markdown","ab3551c4":"markdown","b7618bcb":"markdown","7d503e49":"markdown"},"source":{"90039497":"!pip install kaggle-environments --upgrade","a571db57":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport PIL.Image\n\nimport tensorflow as tf\nimport logging\n\nfrom sklearn import preprocessing\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import evaluate, make\nfrom kaggle_environments.envs.halite.helpers import *\n","ed075ee9":"seed=123\ntf.compat.v1.set_random_seed(seed)\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\nlogging.disable(sys.maxsize)\nglobal ship_","44af8396":"env = make(\"halite\", debug=True)\nenv.run([\"random\"])\nenv.render(mode=\"ipython\",width=800, height=600)","04eac23c":"env.configuration","01b13aa3":"env.specification","47741594":"env.specification.reward","e1635888":"env.specification.action","bd84c9cb":"env.specification.observation","c8f0ac3f":"def getDirTo(fromPos, toPos, size):\n    fromX, fromY = divmod(fromPos[0],size), divmod(fromPos[1],size)\n    toX, toY = divmod(toPos[0],size), divmod(toPos[1],size)\n    if fromY < toY: return ShipAction.NORTH\n    if fromY > toY: return ShipAction.SOUTH\n    if fromX < toX: return ShipAction.EAST\n    if fromX > toX: return ShipAction.WEST\n\n# Directions a ship can move\ndirections = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n\n# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\nship_states = {}\n\n# Returns the commands we send to our ships and shipyards\ndef simple_agent(obs, config):\n    size = config.size\n    board = Board(obs, config)\n    me = board.current_player\n    # If there are no ships, use first shipyard to spawn a ship.\n    if len(me.ships) == 0 and len(me.shipyards) > 0:\n        me.shipyards[0].next_action = ShipyardAction.SPAWN\n\n    # If there are no shipyards, convert first ship into shipyard.\n    if len(me.shipyards) == 0 and len(me.ships) > 0:\n        me.ships[0].next_action = ShipAction.CONVERT\n    \n    for ship in me.ships:\n        if ship.next_action == None:\n            \n            ### Part 1: Set the ship's state \n            if ship.halite < 200: # If cargo is too low, collect halite\n                ship_states[ship.id] = \"COLLECT\"\n            if ship.halite > 500: # If cargo gets very big, deposit halite\n                ship_states[ship.id] = \"DEPOSIT\"\n                \n            ### Part 2: Use the ship's state to select an action\n            if ship_states[ship.id] == \"COLLECT\":\n                # If halite at current location running low, \n                # move to the adjacent square containing the most halite\n                if ship.cell.halite < 100:\n                    neighbors = [ship.cell.north.halite, ship.cell.east.halite, \n                                 ship.cell.south.halite, ship.cell.west.halite]\n                    best = max(range(len(neighbors)), key=neighbors.__getitem__)\n                    ship.next_action = directions[best]\n            if ship_states[ship.id] == \"DEPOSIT\":\n                # Move towards shipyard to deposit cargo\n                direction = getDirTo(ship.position, me.shipyards[0].position, size)\n                if direction: ship.next_action = direction\n                \n    return me.next_actions","4c136114":"trainer = env.train([None, \"random\"])\nobservation = trainer.reset()\nwhile not env.done:\n    my_action = simple_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation = trainer.step(my_action)[0]\n    print(\"Reward gained\",observation.players[0][0])","d5006d8a":"env.render(mode=\"ipython\",width=800, height=600)","417c7f94":"def ActorModel(num_actions,in_):\n    common = tf.keras.layers.Dense(128, activation='ReLU')(in_)\n    common = tf.keras.layers.Dense(128, activation='ReLU')(common)\n    common = tf.keras.layers.Dense(num_actions, activation='softmax')(common)\n    \n    return common","f792356b":"def CriticModel(in_):\n    common = tf.keras.layers.Dense(128)(in_)\n    common = tf.keras.layers.ReLU()(common)\n    common = tf.keras.layers.Dense(1)(common)\n    \n    return common","f31fd5c0":"input_ = tf.keras.layers.Input(shape=[441,])\nmodel = tf.keras.Model(inputs=input_, outputs=[ActorModel(5,input_),CriticModel(input_)])\nmodel.summary()","b1e65c19":"optimizer = tf.keras.optimizers.Adam(lr=7e-4)","9016c628":"huber_loss = tf.keras.losses.Huber()\naction_probs_history = []\ncritic_value_history = []\nrewards_history = []\nrunning_reward = 0\nepisode_count = 0\nnum_actions = 5\neps = np.finfo(np.float32).eps.item()\ngamma = 0.99  # Discount factor for past rewards\nenv = make(\"halite\", debug=True)\ntrainer = env.train([None,\"random\"])","ca171574":"le = preprocessing.LabelEncoder()\nlabel_encoded = le.fit_transform(['NORTH', 'SOUTH', 'EAST', 'WEST', 'CONVERT'])\nlabel_encoded","20b17be2":"def getDirTo(fromPos, toPos, size):\n    fromX, fromY = divmod(fromPos[0],size), divmod(fromPos[1],size)\n    toX, toY = divmod(toPos[0],size), divmod(toPos[1],size)\n    if fromY < toY: return ShipAction.NORTH\n    if fromY > toY: return ShipAction.SOUTH\n    if fromX < toX: return ShipAction.EAST\n    if fromX > toX: return ShipAction.WEST\n\n# Directions a ship can move\ndirections = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n   \ndef decodeDir(act_):\n    if act_ == 'NORTH':return directions[0]\n    if act_ == 'EAST':return directions[1]\n    if act_ == 'SOUTH':return directions[2]\n    if act_ == 'WEST':return directions[3]\n    \n# Will keep track of whether a ship is collecting halite or carrying cargo to a shipyard\nship_states = {}\nship_ = 0\ndef update_L1():\n    ship_+=1\n# Returns the commands we send to our ships and shipyards\ndef advanced_agent(obs, config, action):\n    size = config.size\n    board = Board(obs, config)\n    me = board.current_player \n    act = le.inverse_transform([action])[0]\n    global ship_\n    \n   # If there are no ships, use first shipyard to spawn a ship.\n    if len(me.ships) == 0 and len(me.shipyards) > 0:\n        me.shipyards[ship_-1].next_action = ShipyardAction.SPAWN\n\n    # If there are no shipyards, convert first ship into shipyard.\n    if len(me.shipyards) == 0 and len(me.ships) > 0 and ship_==0:\n        me.ships[0].next_action = ShipAction.CONVERT   \n    try: \n        if act=='CONVERT':\n            me.ships[0].next_action = ShipAction.CONVERT\n            update_L1()\n            if len(me.ships)==0 and len(me.shipyards) > 0:\n                me.shipyards[ship_-1].next_action = ShipyardAction.SPAWN\n        if me.ships[0].halite < 200:\n            ship_states[me.ships[0].id] = 'COLLECT'\n        if me.ships[0].halite > 800:\n            ship_states[me.ships[0].id] = 'DEPOSIT' \n\n        if ship_states[me.ships[0].id] == 'COLLECT': \n            if me.ships[0].cell.halite < 100:\n                me.ships[0].next_action = decodeDir(act)\n        if ship_states[me.ships[0].id] == 'DEPOSIT':\n            # Move towards shipyard to deposit cargo\n            direction = getDirTo(me.ships[0].position, me.shipyards[ship_-1].position, size)\n            if direction: me.ships[0].next_action = direction\n    except:\n        pass\n                \n    return me.next_actions","098c128b":"while not env.done:    \n    state = trainer.reset()\n    episode_reward = 0\n    with tf.GradientTape() as tape:\n        for timestep in range(1,env.configuration.episodeSteps+200):\n            # of the agent in a pop up window.\n            state_ = tf.convert_to_tensor(state.halite)\n            state_ = tf.expand_dims(state_, 0)\n            # Predict action probabilities and estimated future rewards\n            # from environment state\n            action_probs, critic_value = model(state_)\n            critic_value_history.append(critic_value[0, 0])\n            \n            # Sample action from action probability distribution\n            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n            action_probs_history.append(tf.math.log(action_probs[0, action]))\n            \n            # Apply the sampled action in our environment\n            action = advanced_agent(state, env.configuration, action)\n            state = trainer.step(action)[0]\n            gain=state.players[0][0]\/5000\n            rewards_history.append(gain)\n            episode_reward += gain\n            \n            if env.done:\n                state = trainer.reset() \n        # Update running reward to check condition for solving\n        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n\n        # Calculate expected value from rewards\n        # - At each timestep what was the total reward received after that timestep\n        # - Rewards in the past are discounted by multiplying them with gamma\n        # - These are the labels for our critic\n        returns = []\n        discounted_sum = 0\n        for r in rewards_history[::-1]:\n            discounted_sum = r + gamma * discounted_sum\n            returns.insert(0, discounted_sum)\n        # Normalize\n        returns = np.array(returns)\n        returns = (returns - np.mean(returns)) \/ (np.std(returns) + eps)\n        returns = returns.tolist()\n        # Calculating loss values to update our network\n        history = zip(action_probs_history, critic_value_history, returns)\n        actor_losses = []\n        critic_losses = []\n        for log_prob, value, ret in history:\n            # At this point in history, the critic estimated that we would get a\n            # total reward = `value` in the future. We took an action with log probability\n            # of `log_prob` and ended up recieving a total reward = `ret`.\n            # The actor must be updated so that it predicts an action that leads to\n            # high rewards (compared to critic's estimate) with high probability.\n            diff = ret - value\n            actor_losses.append(-log_prob * diff)  # actor loss\n\n            # The critic must be updated so that it predicts a better estimate of\n            # the future rewards.\n            critic_losses.append(\n                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n            )\n        # Backpropagation\n        loss_value = sum(actor_losses) + sum(critic_losses)\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        \n        # Clear the loss and reward history\n        action_probs_history.clear()\n        critic_value_history.clear()\n        rewards_history.clear()\n        \n    # Log details\n    episode_count += 1\n    if episode_count % 10 == 0:\n        template = \"running reward: {:.2f} at episode {}\"\n        print(template.format(running_reward, episode_count))\n\n    if running_reward > 550:  # Condition to consider the task solved\n        print(\"Solved at episode {}!\".format(episode_count))\n        break","e66e0dfc":"while not env.done:\n    state_ = tf.convert_to_tensor(state.halite)\n    state_ = tf.expand_dims(state_, 0)\n    action_probs, critic_value = model(state_)\n    critic_value_history.append(critic_value[0, 0])\n    action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n    action_probs_history.append(tf.math.log(action_probs[0, action]))\n    action = advanced_agent(state, env.configuration, action)\n    state = trainer.step(action)[0]","67461f45":"env.render(mode=\"ipython\",width=800, height=600)","bb8eac16":"## Analyzing the environment\nLets take a tour of our environment and its settings first.","c3ffb1c9":"## The game begins\nSo lets train our model with respect to random actions and see what happens...","d04b351c":"## Results\nThe Yellow ships and shipyards are controlled by our trained actor-critic model and the red ship and shipyards are trained against the random predicting agent.","b3d869fb":"## The Actor-Critic model","91c4b08b":"## Designing game AI with Reinforcement learning","820695c2":"## Rules of the Game\nTo go through details about the rules of the game I would recommend the notebook [Getting started with Halite](https:\/\/www.kaggle.com\/alexisbcook\/getting-started-with-halite) by [Alexis Cook](https:\/\/www.kaggle.com\/alexisbcook). She has elabored and explained the rules of the game very well in her notebook.","46242d30":"## What is Actor-Critic agent?\nNow, before jumping into the concept of Actor-Critic agent, I would recommend you to have some basic knowledge about Q-Learning, followed by deep Q-Learning because without these two you won't understand the significance and necessity Actor-Critic agent.\n<br>\nIn sort,<br>\nAs an agent takes actions and moves through an environment, it learns to map the observed state of the environment to two possible outputs:\n* Recommended action: A probabiltiy value for each action in the action space. The part of the agent responsible for this output is called the **actor**.\n* Estimated rewards in the future: Sum of all rewards it expects to receive in the future. The part of the agent responsible for this output is the **critic**.\n\nAgent and Critic learn to perform their tasks, such that the recommended actions from the actor maximize the rewards.<br>\nSource - [Keras.io](https:\/\/keras.io\/examples\/rl\/actor_critic_cartpole\/)","ab3551c4":"## Introduction\nIn this notebook, we are going to design a neural network to simulate a game through A.I. The game is **Halite** by **Two Sigma**. It is a resource management game where you build and control a small armada of ships. Your algorithms determine their movements to collect halite, a luminous energy source. The most halite at the end of the match wins, but it's up to you to figure out how to make effective and efficient moves. You control your fleet, build new ships, create shipyards, and mine the regenerating halite on the game board.<br>\nWe would be using Actor-Critic agent, as our base reinforcement learning model. The purpose of the agent would be to predict moves to control the direction of the ship to collect halite and deposit them in the shipyard.","b7618bcb":"## Implementation","7d503e49":"## Encoding our moves"}}