{"cell_type":{"6896a279":"code","5344c207":"code","45033c04":"code","5b5ea679":"code","2b976589":"code","474549f5":"code","439bf399":"code","467c4749":"code","ea0017ab":"code","a0c41738":"code","28cd22e6":"code","ce34387e":"code","3bafe96e":"code","123420ec":"code","9010376e":"code","375331f4":"code","977e4d43":"code","64aadc93":"code","b5ad6512":"code","574b7a1e":"code","2b08ba62":"code","f7f012fb":"code","0b06ddc9":"markdown","083df410":"markdown","f1ceccc5":"markdown","e5d13b92":"markdown","6122bdb7":"markdown","ea080dfb":"markdown","74bba493":"markdown"},"source":{"6896a279":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","5344c207":"train = pd.read_csv(\"..\/input\/bank-note-authentication-uci-data\/BankNote_Authentication.csv\")","45033c04":"train","5b5ea679":"x = train.drop([\"class\"],axis=1)\ny = train['class']","2b976589":"y.value_counts()","474549f5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","439bf399":"# splitting the dataset into train and test dataset with 4:1 ratio (80%-20%)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .2, random_state = 26,stratify=y)","467c4749":"from sklearn.linear_model import LogisticRegression\n\n# Create instance of model\nlreg = LogisticRegression()\n# Pass training data into model\nlreg.fit(x_train, y_train)","ea0017ab":"# Getting prediciton on x_test\ny_pred_lreg = lreg.predict(x_test)","a0c41738":"# Scoring our model\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n# Confusion Matrix\nprint('Logistic Regression')\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_lreg))\nprint('--'*50)\n\n# Classification Report\nprint('Classification Report')\nprint(classification_report(y_test,y_pred_lreg))\n\n\n# Accuracy of our model\nprint('--'*50)\nlogreg_accuracy = round(accuracy_score(y_test, y_pred_lreg) * 100,8)\nprint('Accuracy = ', logreg_accuracy,'%')\n","28cd22e6":"%%time\nfrom sklearn.svm import SVC\n# Instantiate the model\nsvc = SVC()\n# Fit the model on training data\nsvc.fit(x_train, y_train)","ce34387e":"# Getting the predictions for x_test\ny_pred_svc = svc.predict(x_test)","3bafe96e":"print('Support Vector Classifier')\nprint('\\n')\n# Confusion matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_svc))\nprint('--'*50)\n\n# Classification report\nprint('Classification Report')\nprint(classification_report(y_test, y_pred_svc))\n\n# Accuracy\nprint('--'*50)\nsvc_accuracy = round(accuracy_score(y_test, y_pred_svc)*100,8)\nprint('Accuracy = ', svc_accuracy,'%')","123420ec":"%%time\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# in knn we need to select a value of nearest neighbour, for now lets use a for loop. If accuarcy\n# is better than other models then we would search for optimal parameter\n\nerror_rate = []\n\nfor i in range (2,15):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(x_train, y_train)\n    pred_i = knn.predict(x_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\n# Plot error rate\nplt.figure(figsize = (10,6))\nplt.plot(range(2,15), error_rate, color = 'blue', linestyle = '--', marker = 'o', \n        markerfacecolor = 'green', markersize = 10)\n\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.show()","9010376e":"# now using above data to train with n_neighbors having least error rate\n\nn_value = 0\nmin_error = float('inf')\nfor idx,error in enumerate(error_rate):\n    if min_error>error:\n        min_error=error\n        n_value=idx+2\n\nknn = KNeighborsClassifier(n_neighbors = n_value)\n# Fit new KNN on training data\nknn.fit(x_train, y_train)","375331f4":"# Predict KNN\ny_pred_knn_op = knn.predict(x_test)","977e4d43":"print('K-Nearest Neighbors(KNN)')\nprint('k =',n_value)\n\n# Confusion Matrix\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred_knn_op))\n\n# Classification Report\nprint('--'*50)\nprint('Classfication Report',classification_report(y_test, y_pred_knn_op))\n\n# Accuracy\nprint('--'*50)\nknn_op_accuracy =round(accuracy_score(y_test, y_pred_knn_op)*100,8)\nprint('Accuracy = ',knn_op_accuracy,'%')","64aadc93":"from sklearn.ensemble import RandomForestClassifier\n\n# Create model object\nrfc = RandomForestClassifier(n_estimators = 250,n_jobs=-1)\n# Fit model to training data\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)","b5ad6512":"print('Random Forest')\n# Confusion matrix\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_rfc))\n\n# Classification report\nprint('--'*50)\nprint('Classification Report')\nprint(classification_report(y_test, y_pred_rfc))\n\n# Accuracy\nprint('--'*50)\nrf_accuracy = round(accuracy_score(y_test, y_pred_rfc)*100,8)\nprint('Accuracy = ', rf_accuracy,'%')","574b7a1e":"from xgboost import XGBClassifier\n\n# Create model object\nxgb = XGBClassifier(n_jobs=-1)\n\n# Fit model to training data\nxgb.fit(x_train, y_train)\ny_pred_xgb = xgb.predict(x_test)","2b08ba62":"print('XGBoost Classifer')\n# Confusion matrix\nprint('\\n')\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test, y_pred_xgb))\n\n# Classification report\nprint('--'*50)\nprint('Classification Report')\nprint(classification_report(y_test, y_pred_xgb))\n\n# Accuracy\nprint('--'*50)\nxgb_accuracy = round(accuracy_score(y_test, y_pred_xgb)*100,8)\nprint('Accuracy = ', xgb_accuracy,'%')","f7f012fb":"models = pd.DataFrame({\n     'Model': ['Logistic Regression', 'Linear SVC', \n               'K-Nearest Neighbors', 'Random Forest','XGBoost Classifier'],\n    'Score': [logreg_accuracy, svc_accuracy, \n               knn_op_accuracy, rf_accuracy,xgb_accuracy]})\nmodels.sort_values(by='Score', ascending=False)","0b06ddc9":"### K-NEAREST NEIGHBORS","083df410":"**We have a accuracy of 99.90%**","f1ceccc5":"### LINEAR SUPPORT VECTOR CLASSIFIER","e5d13b92":"### XGBoost Classifier","6122bdb7":"### Logistic Regression","ea080dfb":"## Training on different algorithms","74bba493":"### RANDOM FOREST"}}