{"cell_type":{"c4faf2e7":"code","0c9f7faa":"code","7f8c4f85":"code","9679e2ea":"code","61150282":"code","7c577cf5":"code","32a1139f":"code","5c720216":"code","97e4f8c7":"code","a1ee5237":"code","d11d510a":"code","5623b4b5":"code","82a65115":"code","3d5e7219":"code","bafb351a":"code","65bca86a":"code","608e9343":"code","11fab715":"code","46a636e8":"code","866827a4":"code","a4cbdf01":"code","c776d738":"code","075bb5d5":"code","14d8e4b6":"code","e953139e":"code","4162f88c":"code","86c22a9c":"code","1dbbdd70":"code","ab82ca08":"code","a36826be":"code","b1de9fde":"code","1a8daf34":"code","284af6a3":"code","34bdb995":"code","7dd4980d":"code","535efc80":"code","f452e6e7":"code","ec216745":"code","33f11f3a":"code","c0912c00":"code","43b03813":"code","922b4031":"code","33179209":"code","73c721a3":"code","74e01625":"code","81ceaf23":"code","bdf07086":"markdown","06f3378d":"markdown","a425df93":"markdown","a79b8ad8":"markdown","2bc280cb":"markdown","dbc72261":"markdown","df287eba":"markdown","8100fb11":"markdown","7cad85b9":"markdown","05384b67":"markdown","a077953d":"markdown","8c8ce32f":"markdown","ca28138e":"markdown","5a50b3d8":"markdown","dc0d7c2f":"markdown","60b62e64":"markdown","238359a2":"markdown","0cdeef7e":"markdown","80647e47":"markdown","ac1a20f7":"markdown","4e4cf91f":"markdown","7c7d0d12":"markdown","f832d692":"markdown","b7ab0ffc":"markdown","d541f832":"markdown","dbe6f5b7":"markdown","61a4594f":"markdown","d00997d3":"markdown","81293eaa":"markdown","713fbfe7":"markdown","bcc4c26e":"markdown","37eadf62":"markdown","057bbf04":"markdown","0642a641":"markdown","af6ee6df":"markdown","714a307c":"markdown","2b8d54e4":"markdown","5b088fc3":"markdown","12ba22ce":"markdown","a00eecf3":"markdown","df905530":"markdown","c99abf64":"markdown","aa893f22":"markdown","2ae0f38b":"markdown","7443fccc":"markdown","ce4a061d":"markdown"},"source":{"c4faf2e7":"import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm,skew\nimport ast\nfrom sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder\nfrom scipy.special import boxcox1p,inv_boxcox1p,boxcox\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\nfrom sklearn.base import BaseEstimator,TransformerMixin,RegressorMixin,clone\nfrom sklearn.metrics import mean_squared_log_error","0c9f7faa":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Shape of train set = {}, shape of test set = {}\".format(train.shape,test.shape))","7f8c4f85":"train.info()","9679e2ea":"test.index=test.index+3000\n\ndf=pd.concat([train.drop(\"revenue\",axis=1),test]).drop(\"id\", axis=1)\ny_train=train[\"revenue\"]\nprint(\"Shape of df = {}, shape of y_train = {}\".format(df.shape,y_train.shape))","61150282":"def visualize_distribution(y):\n    sns.distplot(y,fit=norm)\n    mu,sigma=norm.fit(y)\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})\".format(mu,sigma)])\n    plt.title(\"Distribution of revenue\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    \ndef visualize_probplot(y):\n    stats.probplot(y,plot=plt)\n    plt.show()","7c577cf5":"visualize_distribution(y_train)\nvisualize_probplot(y_train)","32a1139f":"y_train=boxcox1p(y_train,0.2)","5c720216":"visualize_distribution(y_train)\nvisualize_probplot(y_train)","97e4f8c7":"features_to_fix=[\"belongs_to_collection\", \"genres\", \"production_companies\", \"production_countries\",\\\n                 \"Keywords\"]\n\nfor feature in features_to_fix:\n    df.loc[df[feature].notnull(),feature]=\\\n    df.loc[df[feature].notnull(),feature].apply(lambda x : ast.literal_eval(x))\\\n    .apply(lambda x : [y[\"name\"] for y in x])","a1ee5237":"df.loc[df[\"cast\"].notnull(),\"cast\"]=df.loc[df[\"cast\"].notnull(),\"cast\"].apply(lambda x : ast.literal_eval(x))\ndf.loc[df[\"crew\"].notnull(),\"crew\"]=df.loc[df[\"crew\"].notnull(),\"crew\"].apply(lambda x : ast.literal_eval(x))","d11d510a":"df[\"cast_len\"] = df.loc[df[\"cast\"].notnull(),\"cast\"].apply(lambda x : len(x))\ndf[\"crew_len\"] = df.loc[df[\"crew\"].notnull(),\"crew\"].apply(lambda x : len(x))\n\ndf[\"production_companies_len\"]=df.loc[df[\"production_companies\"].notnull(),\"production_companies\"]\\\n.apply(lambda x : len(x))\n\ndf[\"production_countries_len\"]=df.loc[df[\"production_countries\"].notnull(),\"production_countries\"]\\\n.apply(lambda x : len(x))\n\ndf[\"Keywords_len\"]=df.loc[df[\"Keywords\"].notnull(),\"Keywords\"].apply(lambda x : len(x))\ndf[\"genres_len\"]=df.loc[df[\"genres\"].notnull(),\"genres\"].apply(lambda x : len(x))\n\ndf['original_title_letter_count'] = df['original_title'].str.len() \ndf['original_title_word_count'] = df['original_title'].str.split().str.len() \ndf['title_word_count'] = df['title'].str.split().str.len()\ndf['overview_word_count'] = df['overview'].str.split().str.len()\ndf['tagline_word_count'] = df['tagline'].str.split().str.len()","5623b4b5":"df.loc[df[\"homepage\"].notnull(),\"homepage\"]=1\ndf[\"homepage\"]=df[\"homepage\"].fillna(0)  # Note that we only need to know if the film has a webpage or not!\n\ndf[\"in_collection\"]=1\ndf.loc[df[\"belongs_to_collection\"].isnull(),\"in_collection\"]=0\n\ndf[\"has_tagline\"]=1\ndf.loc[df[\"tagline\"].isnull(),\"has_tagline\"]=0\n\ndf[\"title_different\"]=1\ndf.loc[df[\"title\"]==df[\"original_title\"],\"title_different\"]=0\n\ndf[\"isReleased\"]=1\ndf.loc[df[\"status\"]!=\"Released\",\"isReleased\"]=0","82a65115":"release_date=pd.to_datetime(df[\"release_date\"])\ndf[\"release_year\"]=release_date.dt.year\ndf[\"release_month\"]=release_date.dt.month\ndf[\"release_day\"]=release_date.dt.day\ndf[\"release_wd\"]=release_date.dt.dayofweek\ndf[\"release_quarter\"]=release_date.dt.quarter","3d5e7219":"df.loc[df[\"cast\"].notnull(),\"cast\"]=df.loc[df[\"cast\"].notnull(),\"cast\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"order\"]<6]) ","bafb351a":"df[\"Director\"]=[[] for i in range(df.shape[0])]\ndf[\"Producer\"]=[[] for i in range(df.shape[0])]\ndf[\"Executive Producer\"]=[[] for i in range(df.shape[0])]\n\ndf[\"Director\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Director\"])\n\ndf[\"Producer\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Producer\"])\n\ndf[\"Executive Producer\"]=df.loc[df[\"crew\"].notnull(),\"crew\"]\\\n.apply(lambda x : [y[\"name\"] for y in x if y[\"job\"]==\"Executive Producer\"])","65bca86a":"df=df.drop([\"imdb_id\",\"original_title\",\"overview\",\"poster_path\",\"tagline\",\"status\",\"title\",\\\n           \"spoken_languages\",\"release_date\",\"crew\"],axis=1)","608e9343":"mis_val=((df.isnull().sum()\/df.shape[0])*100).sort_values(ascending=False)\nmis_val=mis_val.drop(mis_val[mis_val==0].index)\nprint(mis_val)","11fab715":"to_empty_list=[\"belongs_to_collection\",\"Keywords\",\"production_companies\",\"production_countries\",\\\n              \"Director\",\"Producer\",\"Executive Producer\",\"cast\",\"genres\"]\n\nfor feature in to_empty_list:\n    df[feature] = df[feature].apply(lambda d: d if isinstance(d, list) else [])","46a636e8":"to_zero=[\"runtime\",\"release_month\",\"release_year\",\"release_wd\",\"release_quarter\",\"release_day\"]+\\\n[\"Keywords_len\",\"production_companies_len\",\"production_countries_len\",\"crew_len\",\"cast_len\",\"genres_len\",\n    \"tagline_word_count\",\"overview_word_count\",\"title_word_count\"]\n\nfor feat in to_zero:\n    df[feat]=df[feat].fillna(0)","866827a4":"df['_budget_popularity_ratio'] = df['budget']\/df['popularity']\ndf['_releaseYear_popularity_ratio'] = df['release_year']\/df['popularity']\ndf['_releaseYear_popularity_ratio2'] = df['popularity']\/df['release_year']","a4cbdf01":"mis_val=((df.isnull().sum()\/df.shape[0])*100).sort_values(ascending=False)\nmis_val=mis_val.drop(mis_val[mis_val==0].index)\nprint(mis_val)","c776d738":"numeric=[feat for feat in df.columns if df[feat].dtype!=\"object\"]\n\nskewness=df[numeric].apply(lambda x : skew(x)).sort_values(ascending=False)\nskew=skewness[skewness>2.5]\nprint(skew)","075bb5d5":"high_skew=skew[skew>10].index\nmedium_skew=skew[skew<=10].index\n\nfor feat in high_skew:\n    df[feat]=np.log1p(df[feat])\n\nfor feat in medium_skew:\n    df[feat]=df[feat]=boxcox1p(df[feat],0.15)","14d8e4b6":"skew=df[skew.index].skew()\nprint(skew)","e953139e":"plt.figure(figsize=(15,40))\nfor i,feat in enumerate(skew.index):\n    plt.subplot(7,2,i+1)\n    sns.distplot(df[feat],fit=norm)\n    mu,sigma=norm.fit(df[feat])\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})\".format(mu,sigma)])\n    plt.title(\"Distribution of \"+feat)\n    plt.ylabel(\"Frequency\")\nplt.show()","4162f88c":"lbl=LabelEncoder()\nlbl.fit(df[\"release_year\"].values)\ndf[\"release_year\"]=lbl.transform(df[\"release_year\"].values)","86c22a9c":"to_dummy = [\"belongs_to_collection\",\"genres\",\"original_language\",\"production_companies\",\"production_countries\",\\\n           \"Keywords\",\"cast\",\"Director\",\"Producer\",\"Executive Producer\"]","1dbbdd70":"limits=[4,0,0,35,10,40,10,5,10,12] \n\nfor i,feat in enumerate(to_dummy):\n    mlb = MultiLabelBinarizer()\n    s=df[feat]\n    x=pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_, index=df.index)\n    y=pd.DataFrame(mlb.fit_transform(s),columns=mlb.classes_, index=df.index).sum().sort_values(ascending=False)\n    rare_entries=y[y<=limits[i]].index\n    x=x.drop(rare_entries,axis=1)\n    df=df.drop(feat,axis=1)\n    df=pd.concat([df, x], axis=1, sort=False)","ab82ca08":"print(\"The final total number of features is {}\".format(df.shape[1]))","a36826be":"ntrain=train.shape[0]\n\ntrain=df.iloc[:ntrain,:]\ntest=df.iloc[ntrain:,:]\nprint(\"The shape of train DataFrame is {} and the shape of the test DataFrame is {}\".format(train.shape,test.shape))","b1de9fde":"model_xgb=xgb.XGBRegressor(max_depth=5,\n                           learning_rate=0.1, \n                           n_estimators=2000, \n                           objective='reg:linear', \n                           gamma=1.45, \n                           verbosity=3,\n                           subsample=0.7, \n                           colsample_bytree=0.8, \n                           colsample_bylevel=0.50)","1a8daf34":"model_lgb=lgb.LGBMRegressor(n_estimators=10000, \n                             objective=\"regression\", \n                             metric=\"rmse\", \n                             num_leaves=20, \n                             min_child_samples=100,\n                             learning_rate=0.01, \n                             bagging_fraction=0.8, \n                             feature_fraction=0.8, \n                             bagging_frequency=1, \n                             subsample=.9, \n                             colsample_bytree=.9,\n                             use_best_model=True)","284af6a3":"model_cat = cat.CatBoostRegressor(iterations=10000,learning_rate=0.01,depth=5,eval_metric='RMSE',\\\n                              colsample_bylevel=0.7,\n                              bagging_temperature = 0.2,\n                              metric_period = None,\n                              early_stopping_rounds=200)","34bdb995":"n_folds=5\n\ndef cross_val(model):\n    cr_val=np.sqrt(-cross_val_score(model,train.values,y_train.values,scoring=\"neg_mean_squared_log_error\",cv=5))\n    return cr_val","7dd4980d":"#print \"\\n mean XGB score = {:.3f} with std = {:.3f}\".format(cross_val(model_xgb).mean(),cross_val(model_xgb).std())","535efc80":"#print \"\\n mean LGB score = {:.3f} with std = {:.3f}\".format(cross_val(model_lgb).mean(),cross_val(model_lgb).std())","f452e6e7":"#print \"\\n mean LGB score = {:.3f} with std = {:.3f}\".format(cross_val(model_cat).mean(),cross_val(model_cat).std())","ec216745":"def msle(y,y_pred):\n    return np.sqrt(mean_squared_log_error(y,y_pred))","33f11f3a":"ti=time.time()\nmodel_lgb.fit(train.values,y_train)\nprint(\"Number of minutes of training of model_lgb = {:.2f}\".format((time.time()-ti)\/60))\n\nlgb_pred_train=model_lgb.predict(train.values)\nprint(\"Mean square logarithmic error of lgb model on whole train = {:.4f}\".format(msle(y_train,lgb_pred_train)))","c0912c00":"ti=time.time()\nmodel_xgb.fit(train.values,y_train)\nprint(\"Number of minutes of training of model_xgb = {:.2f}\".format((time.time()-ti)\/60))\n\nxgb_pred_train=model_xgb.predict(train.values)\nprint(\"Mean square logarithmic error of xgb model on whole train = {:.4f}\".format(msle(y_train,xgb_pred_train)))","43b03813":"ti=time.time()\nmodel_cat.fit(train.values,y_train,verbose=False)\nprint(\"Number of minutes of training of model_cal = {:.2f}\".format((time.time()-ti)\/60))\n\ncat_pred_train=model_cat.predict(train.values)\ncat_pred_train[cat_pred_train<0]=0\nprint(\"Mean square logarithmic error of cat model on whole train = {:.4f}\".format(msle(y_train,cat_pred_train)))","922b4031":"c = np.array([0.333334,0.333333,0.333333])\n\nprint(\"The sum of the entries of c is {}\".format(c.sum()))\n\ntrain_pred=xgb_pred_train*c[0]+lgb_pred_train*c[1]+cat_pred_train*c[2]\nprint(\"Mean square logarithmic error of chosen model on whole train = {:.4f}\".format(msle(y_train,train_pred)))","33179209":"plt.figure(figsize=(30,10))\nplt.plot(y_train[:500],label=\"y_train\")\nplt.plot(train_pred[:500],label=\"train_pred\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of first 500 entries of train set\",fontsize=24)\nplt.show()","73c721a3":"plt.figure(figsize=(30,10))\nplt.plot(y_train.values[-500:],label=\"y_train\")\nplt.plot(train_pred[-500:],label=\"train_pred\")\nplt.legend(fontsize=15)\nplt.title(\"Real and predicted revenue of last 500 entries of train set\",fontsize=24)\nplt.show()","74e01625":"lgb_pred=model_lgb.predict(test)\nxgb_pred=model_xgb.predict(test.values)\ncat_pred=model_cat.predict(test)","81ceaf23":"pred=inv_boxcox1p((xgb_pred*c[0]+lgb_pred*c[1]+cat_pred*c[2]),0.2)\n\nsub=pd.DataFrame({\"id\":np.arange(test.shape[0])+3001,\"revenue\":pred})\nsub.to_csv(\"my_submission.csv\",index=False)","bdf07086":"The features of *to_empty_list* are lists and consequently their missing values will be replaced with empty lists","06f3378d":"**Now it is very important to delete all useless features which will not be used in the analysis**","a425df93":"It turns out that the boxcox transformation with coefficient 0.2 is the one which makes the probability plot closer to linear: we thus apply it to *y_train*.","a79b8ad8":"**Now it is the best time to add new features, since they will not be affected by the missing values of other features**","2bc280cb":"## Part 1.4: Handle missing values and add even more new features","dbc72261":"Finally, let's visualize the distribution of these features","df287eba":"As we did in Part 1.1 for the target variable, it is now important to modify the numeric features in order to make them as close as possible to the normal distribution.\n\nFirst of all let's select the numeric features which has a high skewness","8100fb11":"We train the model on the whole *train* DataFrame and compute the loss function using the function *msle*","7cad85b9":"The features of *features_to_fix* will be transformed in lists, since we will only need the informations regarding the name.","05384b67":"# Simple tutorial of TMDB Box Office Competition using XGB, LGB and CatGB","a077953d":"## Part 1.1: Modify the target variable","8c8ce32f":"Finally, after having obtained all the dummy variables we needed, we can separate *df* back in *train* and *test*","ca28138e":"**New features from** *release_date*","5a50b3d8":"When computing the predictions we must remember to apply the inverse of the tranformation we applied on *y_train* in Part 1.1","dc0d7c2f":"**New features involving lengths**","60b62e64":"## Get dummy variables and separate train and test set","238359a2":"We want our model to perform as good as possible and in order to do so the target variable of *y_train* must be as close as possible to the normal distribution (and its probability plot as close as linear as possible). \n\nWe will thus check which power transformation of boxcox fulfills this task better.","0cdeef7e":"It is also useful to feed the feature *release_year* to the label encoder","80647e47":"## Part 2.3: Make the predictions and write them to the submission form","ac1a20f7":"Now it is time to create new useful features for our model to use","4e4cf91f":"We are going to get dummy variables from all the features which are not numeric","7c7d0d12":"## Part 1.6: Modify numeric features","f832d692":"First of all let's print the list of percentages of missing values in descending order","b7ab0ffc":"Finally, our prediction will be the weighted mean (using the coefficients of c) of the predictions of the three models. Clearly, the sum of the entries of c must be equal to 1.","d541f832":"From *crew* we create new features *Director*, *Producer* and *Executive Producer*","dbe6f5b7":"**Of course, before submitting the prediction we will need to remember to perform the inverse transformation!**","61a4594f":"# Part 2: Build the model and make the predictions","d00997d3":"# Part 0: Import libraries and read databases","81293eaa":"Notice that there are many non-numeric features, which we will want to transform into dummy variables. In order to do so we will need to cancatenate *train* and *test* datasets into a unique dataset *df*.\n\nWe will also drop the feature *id*, since it is useless for the analysis.","713fbfe7":"Some features are in a JSON-like format and will need to be converted to dictionaries or lists before being used. We will do it with *ast.literal_eval()*.","bcc4c26e":"## Part 2.2: Train the models","37eadf62":"Instead, the features *cast* and *crew* will be transformed in dictionaries","057bbf04":"## Part 2.1: Perform cross-validation","0642a641":"## Part 1.3: Create new features and drop some useless ones","af6ee6df":"Before training the models on the whole *train* DataFrame, it is interesting to perform some cross validation, in order to get an idea of how well the models are performing.\n\nHere the cross validations are commented since they require a lot of time","714a307c":"## Part 1.2: Fix the features with JSON-like formatting","2b8d54e4":"The missing values of time and length will be replaced with 0","5b088fc3":"Finally, we can see there are no missing values!","12ba22ce":"**Features underlining important characteristics of films**","a00eecf3":"In this Kernel we will see how to perform feature engineering (transform features, handle missing values and add new features) and model selection and validation for the competition \"TMDB Box Office\" in a very simple way.\n\nI am also very grateful to user Serigne for its Kernel about the House Prices competition: it taught me a lot and I really recommend it to you.","df905530":"# Part 1: Feature Engineering","c99abf64":"Indeed, we can se the skewness is really improved:","aa893f22":"In this Kernel we will use XGB,LGBM and CatBoost regressors, since they are the state of the art in prediction of tabular data","2ae0f38b":"Notice that the entries of the features of the list *to_dummy* are lists: in each feature, for each element of each list we want to get a dummy variable only if such element is \"famous\" enough, which is if it appears in a sufficient number of lists. In order to do so we use the list *limits*, whose entries are, for each feature, the minimum numbers of appearences which an element must have in order to get a dummy variable","7443fccc":"**Modify** *cast* **and** *crew*\n\nFor each film we only want to consider the six most important actors (*order* is the order of importance in ascending order starting from 0)","ce4a061d":"Let's reduce the  skewness of these features by applying a power transformation"}}