{"cell_type":{"7115a238":"code","d1db4510":"code","76b2c282":"code","9c3292bd":"code","1df632fc":"code","5490bdc6":"code","547744f3":"code","572c593b":"code","60ac078c":"code","e7642e5d":"code","b1c72939":"code","4b3edfbc":"code","3729ab87":"code","db5b26a9":"code","296fd0b8":"code","82dbbb4d":"code","cf7cf0a6":"code","8cd0f851":"code","05a6a243":"code","1ba58943":"code","bd298f17":"code","6ca12b3d":"code","f5e18660":"code","bd72fe05":"code","d1c439ba":"code","8475bdfc":"code","1ead4261":"code","99887e76":"code","f173fffc":"code","f5e2ae2a":"code","738c8a49":"code","6cc791d9":"code","9b999970":"code","2b3ae3ba":"code","7819bc16":"code","dfb6c57c":"code","23622d57":"markdown","be65fded":"markdown","fcd435d7":"markdown","15ed12e8":"markdown","3f423fb1":"markdown","4b8254bf":"markdown","414bba85":"markdown","7749cbbf":"markdown"},"source":{"7115a238":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1db4510":"df = pd.read_csv(\"\/kaggle\/input\/health-insurance-cost-prediction\/insurance.csv\")","76b2c282":"df.head()","9c3292bd":"df.describe()","1df632fc":"df.info() ","5490bdc6":"df.smoker.unique()","547744f3":"df.region.unique()","572c593b":"df.sex.unique()","60ac078c":"import seaborn as sns\nsns.lmplot(x=\"bmi\",y=\"charges\",hue=\"sex\",data=df)","e7642e5d":"sns.lmplot(x=\"children\",y=\"charges\",hue=\"sex\",data=df)","b1c72939":"plt.figure(figsize=(8,6))\nax=sns.heatmap(df.corr(),annot=True,vmin=-1,vmax=1)","4b3edfbc":"sns.pairplot(df) #lets check any linear connection btw features and target values","3729ab87":"df.columns","db5b26a9":"import statsmodels.formula.api as smf\n# Sadece n\u00fcmerik de\u011fi\u015fkenlerle model a\u00e7\u0131klanam\u0131yor.\nmodel=smf.ols('charges~ age + bmi + children',data=df)\nmodel.fit().summary() #underfitting ! UPS!","296fd0b8":"import matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.concat([df, pd.get_dummies(df[\"sex\"],prefix=\"sex\")], axis=1)\ndf = pd.concat([df, pd.get_dummies(df[\"region\"],prefix=\"region\")], axis=1)\ndf = pd.concat([df, pd.get_dummies(df[\"smoker\"],prefix=\"smoker\")], axis=1)\ndf.head()","82dbbb4d":"df.drop([ \"region\", \"smoker\", \"sex\"], axis = 1, inplace = True)","cf7cf0a6":"plt.figure(figsize=(8,6))\nax=sns.heatmap(df.corr(),annot=True,vmin=-1,vmax=1)","8cd0f851":"df.drop(columns=[\"smoker_yes\",\"sex_female\"],inplace=True) # Avoid from multicol.","05a6a243":"model=smf.ols('charges~ age + bmi + children + sex_male + region_northeast + region_northwest + region_southeast + region_southwest + smoker_no',data=df)\nmodel.fit().summary() # More data reduces bias.","1ba58943":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge,Lasso,RidgeCV,LassoCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\nX, y = df.drop('charges',axis=1), df['charges']\n\n#Simple Validation\nX, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=10)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.25, random_state=3) # 60-20-20","bd298f17":"lreg = LinearRegression()\nlreg.fit(X_train,y_train)\n\npred = lreg.predict(X_val)\n\n#mean square error\nmse = np.mean((pred - y_val)**2)\n\n#r2\n\nlreg.score(X_val, y_val)","6ca12b3d":"mse","f5e18660":"coefs=pd.DataFrame(X_train.columns)\ncoefs[\"Coefficient Estimate\"]=pd.Series(lreg.coef_)\ncoefs","bd72fe05":"#Standardizasyon\nreg=Ridge(alpha=0.05)\n\nscaler=StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train.values)\nX_val_scaled=scaler.transform(X_val.values)\nX_test_scaled=scaler.transform(X_test.values)\n\nreg.fit(X_train_scaled,y_train)\n\npred = reg.predict(X_val_scaled)\n\n#mean square error\nmse = np.mean((pred - y_val)**2)\n\n#r2\n\nreg.score(X_val_scaled, y_val) #Ridge has better r-score and mse than linear reg.Lets check polynomial","d1c439ba":"mse","8475bdfc":"coefs=pd.DataFrame(X_train.columns)\ncoefs[\"Coefficient Estimate\"]=pd.Series(reg.coef_)\ncoefs","1ead4261":"poly=PolynomialFeatures(degree=2)\nlm_poly=LinearRegression()\n\nX_train_poly=poly.fit_transform(X_train.values)\nX_val_poly=poly.transform(X_val.values)\nX_test_poly=poly.transform(X_test.values)\n\nlm_poly.fit(X_train_poly,y_train)\n\npred = lm_poly.predict(X_val_poly)\n\n#mean square error\nmse = np.mean((pred - y_val)**2)\n\n#r2\n\nlm_poly.score(X_val_poly, y_val)","99887e76":"mse","f173fffc":"coefs=pd.DataFrame(X_train.columns)\ncoefs[\"Coefficient Estimate\"]=pd.Series(lm_poly.coef_)\ncoefs","f5e2ae2a":"lass=Ridge(alpha=0.05)\n\n\nlass.fit(X_train_scaled,y_train)\n\npred = lass.predict(X_val_scaled)\n\n#mean square error\nmse = np.mean((pred - y_val)**2)\n\n#r2\n\nlass.score(X_val_scaled, y_val)","738c8a49":"coefs=pd.DataFrame(X_train.columns)\ncoefs[\"Coefficient Estimate\"]=pd.Series(lass.coef_)\ncoefs","6cc791d9":"ridge_cv=RidgeCV(alphas=(0.0001,0.0005,0.001,0.01,0.005,0.05),normalize=True,cv=kf).fit(X_train,y_train)\nridge_cv_pred=ridge_cv.predict(X_val)\n\n#mean square error\nmse = np.mean((ridge_cv_pred - y_val)**2)\n\n#r2\n\nridge_cv.score(X_val, y_val)","9b999970":"lasso_cv=LassoCV(alphas=(0.0001,0.0005,0.001,0.01,0.005,0.05),normalize=True,cv=kf).fit(X_train,y_train)\nlasso_cv_pred=lasso_cv.predict(X_val)\n\n#mean square error\nmse = np.mean((lasso_cv_pred - y_val)**2)\n\n#r2\n\nlasso_cv.score(X_val, y_val)","2b3ae3ba":"print(f'Linear Regression val R^2: {lreg.score(X_val, y_val):.3f}')\nprint(f'Poly Regression val R^2: {lm_poly.score(X_val_poly, y_val):.3f}')\nprint(f'Ridge Regression val R^2: {reg.score(X_val_scaled, y_val):.3f}')\nprint(f'Lasso Regression val R^2: {lass.score(X_val_scaled, y_val):.3f}')","7819bc16":"poly=PolynomialFeatures(degree=2)\nlm_poly=LinearRegression()\n\nX=poly.fit_transform(X.values)\n\n\nlm_poly.fit(X,y)\n\npred = lm_poly.predict(X_test_poly)\n\n#mean square error\nmse = np.mean((pred - y_test)**2)\n\n#r2\n\nlm_poly.score(X_test_poly, y_test)","dfb6c57c":"from sklearn.model_selection import cross_val_score,KFold\nkf=KFold(n_splits=5,shuffle=True,random_state=100)\ncross_val_score(lm_poly,X,y,cv=kf,scoring=\"r2\") # train+val dataset","23622d57":"# Lasso Regression","be65fded":"# RidgeCV","fcd435d7":"# Cross Validation","15ed12e8":"# Ridge Regression","3f423fb1":"# **Simple Linear Regression**","4b8254bf":"# Degree 2 Polynomial Regression","414bba85":"The winner is Polynomial Regression.\n# Our final model","7749cbbf":"# OLS Model"}}