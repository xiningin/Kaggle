{"cell_type":{"73a9a913":"code","58287eb7":"code","1b40e937":"code","bf9c0a24":"code","afcb05db":"code","6578368e":"code","4d9075b2":"code","06722373":"code","082bd930":"code","928e425b":"code","7384973a":"code","4021525d":"code","e1a3f6b4":"code","9dcb1a37":"code","3ff9c160":"code","5c6d94b0":"code","0ce035e8":"code","9f72e1cd":"code","85129f9d":"code","af6c1a7c":"code","0d3079e1":"code","b4c0d55e":"code","c65f09bf":"code","02c98eb1":"code","d6b547b1":"code","1cc98647":"code","521d5e18":"code","5032e8e0":"code","d674a8c6":"code","605ed881":"code","9724ca57":"code","7a421c3b":"code","6911bd24":"code","54acea3e":"code","dbc0ad13":"code","2aff02ea":"code","e55f5fdb":"code","5fb27cf9":"code","e06b97d0":"code","46587ef7":"code","0717bdae":"code","38919a49":"code","22d10d35":"markdown","af4c1d92":"markdown","4030cf80":"markdown","76b472b0":"markdown","5f759d68":"markdown","5a81b757":"markdown","435979b8":"markdown","4f4a82e3":"markdown","956307ef":"markdown","f869db83":"markdown","f3486759":"markdown","b3360b3b":"markdown","63097890":"markdown","10e3a09a":"markdown","d5d5c660":"markdown","08a7ec2a":"markdown","f8b08952":"markdown","787e898f":"markdown","e7b92bf3":"markdown","1636b91d":"markdown","fb38bd83":"markdown","18e02ca1":"markdown","74d423e8":"markdown","1294e0c6":"markdown","e26f6999":"markdown","5348d70a":"markdown","b697d7b5":"markdown","ac4fb321":"markdown","1e54f5b6":"markdown"},"source":{"73a9a913":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nimport os\nprint(os.listdir(\"..\/input\"))","58287eb7":"plt.rc('axes', lw = 1.5)\nplt.rc('xtick', labelsize = 14)\nplt.rc('ytick', labelsize = 14)\nplt.rc('xtick.major', size = 5, width = 3)\nplt.rc('ytick.major', size = 5, width = 3)","1b40e937":"# open the training dataset\ndataTrain = pd.read_csv('..\/input\/train.csv')\ndataTrain.head()","bf9c0a24":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndataTrain = dataTrain.select_dtypes(include=numerics)\ndataTrain.head()","afcb05db":"dataTrain = dataTrain[['GarageArea','SalePrice']]\ndataTrain.head()","6578368e":"dataTrain.isnull().values.any()","4d9075b2":"# Take a look at the data. \nplt.plot('GarageArea','SalePrice',data=dataTrain, marker = 'o', linestyle = '')\nplt.ylabel('Sale Price (dollars)', fontsize = 18)\nplt.xlabel('Garage Area (square feet)', fontsize = 18)\nplt.show()","06722373":"# format training data\nxTrain = dataTrain['GarageArea'].values.reshape(-1,1) # as_matrix is deprecated since version 0.23.0\nyTrain = dataTrain['SalePrice'].values.reshape(-1,1)\nxTrain","082bd930":"# Transform the input features\nPoly = PolynomialFeatures(degree = 10, include_bias = False)\nxTrainPoly = Poly.fit_transform(xTrain)","928e425b":"from sklearn.preprocessing import StandardScaler\n# standardization\nscaler = StandardScaler()\nxTrainPolyStan = scaler.fit_transform(xTrainPoly)\nscaler.scale_, scaler.mean_","7384973a":"# linear regression\nreg = LinearRegression()\nreg.fit(xTrainPolyStan, yTrain)\n\n# predict\nxFit = np.linspace(0,1500,num=200).reshape(-1,1)\nxFitPoly = Poly.transform(xFit)\nxFitPolyStan = scaler.transform(xFitPoly)\nyFit = reg.predict(xFitPolyStan)\n\n# plot\nplt.plot(xFit,yFit, lw=3, color='r', zorder = 2)\nplt.plot('GarageArea','SalePrice',data=dataTrain, marker = 'o', color = 'b', linestyle = '', zorder = 1)\nplt.ylabel('Sale Price (dollars)', fontsize = 18)\nplt.xlabel('Garage Area (square feet)', fontsize = 18)\nplt.show()","4021525d":"from sklearn.linear_model import Ridge","e1a3f6b4":"i=0\nls = ['-','--',':']\ncolor = ['r','g','orange']\n\nfor a in [0,2,2000]:\n    ridgeReg = Ridge(alpha=a)\n    ridgeReg.fit(xTrainPolyStan, yTrain)\n\n    # predict\n    xFit = np.linspace(0,1500,num=200).reshape(-1,1)\n    xFitPoly = Poly.transform(xFit)\n    xFitPolyStan = scaler.transform(xFitPoly)\n    yFit = ridgeReg.predict(xFitPolyStan)\n    \n    # plot\n    plt.plot(xFit,yFit, lw=3, color=color[i], zorder = 2, label= \"alpha = \" + str(a),linestyle=ls[i])\n    i = i + 1\n    \nplt.plot('GarageArea','SalePrice',data=dataTrain, marker = 'o', color = 'b', linestyle = '', zorder = 1)\nplt.ylabel('Sale Price (dollars)', fontsize = 18)\nplt.xlabel('Garage Area (square feet)', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","9dcb1a37":"from sklearn.linear_model import Lasso","3ff9c160":"i=0\nls = ['-','--',':']\ncolor = ['r','g','orange']\n\nfor a in [0.1,1,10]:\n    lassoReg = Lasso(alpha=a)\n    lassoReg.fit(xTrainPolyStan, yTrain)\n\n    # predict\n    xFit = np.linspace(0,1500,num=200).reshape(-1,1)\n    xFitPoly = Poly.transform(xFit)\n    xFitPolyStan = scaler.transform(xFitPoly)\n    yFit = lassoReg.predict(xFitPolyStan)\n    \n    # plot\n    plt.plot(xFit,yFit, lw=3, color=color[i], zorder = 2, label= \"alpha = \" + str(a),linestyle=ls[i])\n    i = i + 1\n    \nplt.plot('GarageArea','SalePrice',data=dataTrain, marker = 'o', color = 'b', linestyle = '', zorder = 1)\nplt.ylabel('Sale Price (dollars)', fontsize = 18)\nplt.xlabel('Garage Area (square feet)', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","5c6d94b0":"from sklearn.linear_model import SGDRegressor","0ce035e8":"sgd = SGDRegressor(loss='squared_loss', penalty='l1', alpha=0.1)\nyTrain = yTrain.ravel() # format required by sgd\nsgd.fit(xTrainPolyStan, yTrain)\n\n# predict\nxFit = np.linspace(0,1500,num=200).reshape(-1,1)\nxFitPoly = Poly.transform(xFit)\nxFitPolyStan = scaler.transform(xFitPoly)\nyFit = sgd.predict(xFitPolyStan)\n\nplt.plot(xFit,yFit, lw=3, color='r', zorder = 2, label= \"alpha = 0.1\",linestyle='-')\nplt.plot('GarageArea','SalePrice',data=dataTrain, marker = 'o', color = 'b', linestyle = '', zorder = 1)\nplt.ylabel('Sale Price (dollars)', fontsize = 18)\nplt.xlabel('Garage Area (square feet)', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","9f72e1cd":"from sklearn.linear_model import ElasticNet","85129f9d":"yTrain = yTrain.reshape(-1,1)\nelasticReg = ElasticNet(alpha = 0.1, l1_ratio = 0.5)\nelasticReg.fit(xTrainPolyStan, yTrain)\n\n# predict\nxFit = np.linspace(0,1500,num=200).reshape(-1,1)\nxFitPoly = Poly.transform(xFit)\nxFitPolyStan = scaler.transform(xFitPoly)\nyFit = elasticReg.predict(xFitPolyStan)\n\nplt.plot(xFit,yFit, lw=3, color='r', zorder = 2, label= \"alpha = 0.1\",linestyle='-')\nplt.plot('GarageArea','SalePrice',data=dataTrain, marker = 'o', color = 'b', linestyle = '', zorder = 1)\nplt.ylabel('Sale Price (dollars)', fontsize = 18)\nplt.xlabel('Garage Area (square feet)', fontsize = 18)\nplt.legend(fontsize = 14)\nplt.show()","af6c1a7c":"from sklearn.metrics import mean_squared_error","0d3079e1":"dataTrain = pd.read_csv('..\/input\/train.csv')\ndataTrain.head()","b4c0d55e":"# Obtain training data\nxTrain = dataTrain[['OverallQual','LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea', 'OpenPorchSF']].values\nyTrain = dataTrain['SalePrice'].values.reshape(-1,1)","c65f09bf":"# Transform the data\npoly2 = PolynomialFeatures(degree = 4, include_bias = False)\nxTrainPoly = poly2.fit_transform(xTrain)\nscaler = StandardScaler()\nxTrainPolyStan = scaler.fit_transform(xTrainPoly)\n\n# Fit the data\nelasticReg = ElasticNet(alpha = 0.1, l1_ratio = 0.85)\nelasticReg.fit(xTrainPolyStan, yTrain)\n\n# evaluate performance on training set\nyTrainHat = elasticReg.predict(xTrainPolyStan)\n\n# calculate rmse based on log(price)\nmse = mean_squared_error(np.log(yTrain), np.log(yTrainHat))\nrmse = np.sqrt(mse)\nprint(rmse)","02c98eb1":"x = np.linspace(0,800000,num=1000)\nplt.plot(yTrainHat, yTrain,marker='o', linestyle = '', zorder = 1, color='b')\nplt.plot(x, x, linestyle = '-',color='red',zorder=2,lw=3)\nplt.xlabel('predicted sale price (dollars)', fontsize = 18)\nplt.ylabel('actual sale price (dollars)', fontsize = 18)\nplt.show()","d6b547b1":"dataTest = pd.read_csv('..\/input\/test.csv')\ndataTest.head()","1cc98647":"dataTest = dataTest[['Id','OverallQual','LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea','OpenPorchSF']]\ndataTest.isnull().any()","521d5e18":"# fill the nans with respective means.\ndictMs = {'TotalBsmtSF':dataTest['TotalBsmtSF'].mean(skipna=True),\n          'GarageArea':dataTest['GarageArea'].mean(skipna=True)}\ndataTest = dataTest.fillna(value=dictMs)\ndataTest.isnull().any()","5032e8e0":"xTest = dataTest[['OverallQual','LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GarageArea', 'OpenPorchSF']].values\nxTestPoly = poly2.transform(xTest)\nxTestPolyStan = scaler.transform(xTestPoly)\nyTestHat = elasticReg.predict(xTestPolyStan)","d674a8c6":"sub = pd.DataFrame()\nsub['Id'] = dataTest['Id']\nsub['SalePrice'] = yTestHat\nsub.to_csv('submission.csv',index=False)","605ed881":"# Transform and standardize the data\npoly2 = PolynomialFeatures(degree = 4, include_bias = False)\nxTrainPoly = poly2.fit_transform(xTrain)\nscaler = StandardScaler()\nxTrainPolyStan = scaler.fit_transform(xTrainPoly)\nyTrainLog = np.log(yTrain)","9724ca57":"# shuffle data and split into training set and validation set\nfrom sklearn.utils import shuffle\nxShuffled, yShuffled = shuffle(xTrainPolyStan, yTrainLog)\n\ntrain_ratio = 0.8\nmTrain = np.int(len(xShuffled[:,0])*train_ratio) # 1168\nprint(\"Training sample size is: \", mTrain)\n\nX_train_stan = xShuffled[0:mTrain]\nY_train = yShuffled[0:mTrain].ravel()\nX_val_stan = xShuffled[mTrain:]\nY_val = yShuffled[mTrain:].ravel()","7a421c3b":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import SGDRegressor\nfrom copy import deepcopy","6911bd24":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nsgdReg = SGDRegressor(n_iter=1, warm_start = True, penalty=None, learning_rate = 'constant', eta0=0.00001)\n\nmse_val_min = float(\"inf\")\nbest_epoch = None\nbest_model = None\nrmse_train = []\nrmse_val = []\n\nn_no_change = 0\n\nfor epoch in range(1,100000):\n    sgdReg.fit(X_train_stan, Y_train)\n    Y_train_predict = sgdReg.predict(X_train_stan)\n    train_error = mean_squared_error(Y_train_predict,Y_train)\n    rmse_train.append(np.sqrt(train_error))\n    Y_val_predict = sgdReg.predict(X_val_stan)\n    val_error = mean_squared_error(Y_val_predict, Y_val)\n    rmse_val.append(np.sqrt(val_error))\n    \n    if val_error < mse_val_min:\n        n_no_change = 0\n        mse_val_min = val_error\n        best_epoch = epoch\n        best_model = deepcopy(sgdReg)\n    else:\n        n_no_change = n_no_change + 1\n    \n    if n_no_change >= 1000:\n        print('Time to stop!')\n        print('num epoch =', epoch)\n        print('best epoch = ', best_epoch)\n        break","54acea3e":"# plot rmse\nfig,ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,4))\nplt.subplots_adjust(wspace=0.5)\n\nax[0].plot(rmse_train, label = 'training')\nax[0].plot(rmse_val, label = 'validation')\nax[0].set_xlabel('epoch', fontsize = 18)\nax[0].set_ylabel('RMSE', fontsize = 18)\nax[0].legend(fontsize=14)\n\nax[1].plot(rmse_train[best_epoch-3:best_epoch+2], label = 'training')\nax[1].plot(rmse_val[best_epoch-3:best_epoch+2], label = 'validation')\nax[1].set_xlabel('epoch', fontsize = 18)\nax[1].set_ylabel('RMSE', fontsize = 18)\nax[1].set_xticks([0,1,2,3,4])\nxticklabels = [str(e) for e in range(best_epoch-2,best_epoch+3)]\nax[1].set_xticklabels(xticklabels)\nax[1].plot(2,rmse_val[best_epoch-1],marker='o',color='r')\nax[1].text(2,rmse_val[best_epoch-1]-0.001,'minimum',color='r',fontsize=14)\nax[1].legend(fontsize=14)\nplt.show()","dbc0ad13":"# total rmse on the train + validation sets\nyTrainHatLog = best_model.predict(xTrainPolyStan)\nprint(np.sqrt(mean_squared_error(yTrainHatLog,yTrainLog)))","2aff02ea":"# plot the train + validation sets' predicted sale price vs actual sale price\nplt.plot(np.exp(yTrainHatLog),np.exp(yTrainLog), marker = 'o', linestyle='', color = 'b')\nx = np.linspace(0,800000,num=1000)\nplt.plot(x, x, linestyle = '-',color='red',zorder=2,lw=3)\nplt.xlabel('predicted sale price (dollars)', fontsize = 18)\nplt.ylabel('actual sale price (dollars)', fontsize = 18)\nplt.show()","e55f5fdb":"from sklearn.model_selection import KFold","5fb27cf9":"xShuffled, yShuffled = shuffle(xTrainPolyStan, yTrainLog)\n\nsgdReg = SGDRegressor(n_iter=1, warm_start = True, penalty=None, learning_rate = 'constant', eta0=0.00001)\n\nround_num = 0\nbest_epoch = None\nbest_model = None\nrmse_train = []\nrmse_val = []\n\nkf = KFold(n_splits=5)\n\nfor train_index, val_index in kf.split(xShuffled):\n    round_num = round_num + 1\n    print(\"Round #\", round_num)\n    X_train_stan, X_val_stan = xShuffled[train_index], xShuffled[val_index]\n    Y_train, Y_val = yShuffled[train_index].ravel(), yShuffled[val_index].ravel()\n    \n    print(\"Running...\")\n    mse_val_min = float(\"inf\")\n    n_no_change = 0\n\n    for epoch in range(1,100000):\n        sgdReg.fit(X_train_stan, Y_train)\n        Y_train_predict = sgdReg.predict(X_train_stan)\n        train_error = mean_squared_error(Y_train_predict,Y_train)\n        rmse_train.append(np.sqrt(train_error))\n        Y_val_predict = sgdReg.predict(X_val_stan)\n        val_error = mean_squared_error(Y_val_predict, Y_val)\n        rmse_val.append(np.sqrt(val_error))\n    \n        if val_error < mse_val_min:\n            n_no_change = 0\n            mse_val_min = val_error\n            best_epoch = epoch\n            best_model = deepcopy(sgdReg)\n        else:\n            n_no_change = n_no_change + 1\n    \n        if n_no_change >= 1000:\n            print('Time to stop!')\n            print('num epoch =', epoch)\n            print('best epoch = ', best_epoch,', from round #', round_num)\n            break","e06b97d0":"fig,ax = plt.subplots(nrows = 1, ncols = 1, figsize = (6,4))\nplt.subplots_adjust(wspace=0.5)\n\nax.plot(rmse_train, label = 'training')\nax.plot(rmse_val, label = 'validation')\nax.set_xlabel('epoch', fontsize = 18)\nax.set_ylabel('RMSE', fontsize = 18)\nax.legend(fontsize=14)\nplt.show()","46587ef7":"# total rmse on the train + validation sets\nyTrainHatLog = best_model.predict(xTrainPolyStan)\nprint(np.sqrt(mean_squared_error(yTrainHatLog,yTrainLog)))","0717bdae":"# plot the train + validation sets' predicted sale price vs actual sale price\nplt.plot(np.exp(yTrainHatLog),np.exp(yTrainLog), marker = 'o', linestyle='', color = 'b')\nx = np.linspace(0,800000,num=1000)\nplt.plot(x, x, linestyle = '-',color='red',zorder=2,lw=3)\nplt.xlabel('predicted sale price (dollars)', fontsize = 18)\nplt.ylabel('actual sale price (dollars)', fontsize = 18)\nplt.show()","38919a49":"# test set (data preprocessing - see Elastic Net part)\nyTestHatLog = best_model.predict(xTestPolyStan)\nsub = pd.DataFrame()\nsub['Id'] = dataTest['Id']\nsub['SalePrice'] = np.exp(yTestHatLog)\nsub.to_csv('submission2.csv',index=False)","22d10d35":"**So, which model should we choose in practice?** <br\/>\nRidge regression: a good default. However, if sparse features are expected, ridge should be replaced by lasso or elastic net. <br\/>\nLasso regression: good for sparse feature selection. However, if the number of features is greater than the number of training samples, or when there are strongly correlated features, ridge or elastic net should be used. <br\/>\nElastic net: versatile since the ratio parameter r is tunable. A 50% ratio of l1 and l2-penalty can be a good default, too.","af4c1d92":"Since we have not talked about catgorical data, we will focus only on numeric features here. <br\/>","4030cf80":"## Lasso Regression\nLeast Absolute Shrinkage and Selection Operator Regression - LASSO <br\/>\nCost function: J(theta) = MSE(theta) + alpha (|theta_1| + |theta_2| + ... + |theta_n|). The penalty is proportional to the **l1-norm** of theta. <br\/>\n\nThe advantage of Lasso over ridge regression lies in the diamond shape of contour of the l1-norm penalty, which leads to some of the thetas being eliminated (set to 0) quickly. This means the Lasso regression can perform automatic feature selection, when ridge regression cannot. If you have the book *Hands-on Machine Learning with Scikit-Learn & Tensorflow*, Figure 4-19 gives a more detailed explanation of the feature selection capability of Lass0. You can also understand the difference of ridge and Lasso regression by understanding that, ridge's l2-penalty heavily penalizes large thetas, but has nearly no penalization for small thetas (due to the square), whereas Lasso's l1-penalty gives appropriate penalization to even small thetas. <br\/>","76b472b0":" Let's plot predicted sale price and actual sale price:","5f759d68":"Check to see if there are any missing values?","5a81b757":"These dataset are from the House Prices competition, yet the purpose of this Notebook is to learn and experiment with regularized linear models. <br\/>\nWe will first choose just one independent variable for visualization purpose, and try out different regularized linear models. Later we will expand the models to multiple independent variables. <br\/>\nData description is available here: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data","435979b8":"Let's say we have reasons to believe the OverallQual, LotArea, TotalBsmtSF, 1stFlrSF, 2ndFlrSF,  GarageArea, and OpenPorchSF are some of the most relevant features for sale price prediction, and we want to build a four-degree (including interactions) linear model with elastic net regularization to predict sale price (**this is, of course, a huge simiplification of the actual problem, but here we just want to show how the regularized linear models work, and how well they can perform with limited information** ). Here is how we do it:","4f4a82e3":"Here I used log(yTrain) as the target values in training, as it seems to perform better than yTrain.","956307ef":"# Single Variable Models (for illustration purposes)\n**We simplify the data for the purpose of illustration and introduction of the regularized models. **<br\/>\nLater we will include multiple variables in the models.","f869db83":"## Elastic Net","f3486759":"Notice that I used n_no_change >= 1000 as the stopping criterion to be on the safe side. A smaller number is doable.","b3360b3b":"As you can see, the three alpha tested gives very similar fits. As mentioned earlier, Lasso regression tends to return sparse theta vector, with many least important features eliminated (set to 0). Even when alpha is small, such elimination can happen, leading to similar fits for certain range of alphas.\n\nAs mentioned in Scikit-Learn's [documentaion](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html), Lasso function is not advised to use with alpha = 0. In such cases, LinearRegression should be used instead. <br\/>\n\nStochastic gradient descent can be used for any type of optimization problem. Here we show the example of lasso regression using SGDRegressor from Scikit-Learn package.","63097890":"No missing values, so we don't have to worry much about data cleaning.","10e3a09a":"Ridge regression is sensitive to the input features, therefore **standardization is usually recommended** before Ridge regression. <br\/>\nSome useful info here: https:\/\/stats.stackexchange.com\/questions\/111017\/question-about-standardizing-in-ridge-regression <br\/>\nHere, standardization has already been carried out (see above), so we will go straight to training.","d5d5c660":"Previously (see my Machine Learning 1 kernel), we standardized input features ourselves, but this can also be easily done through **Scikit-learn**:","08a7ec2a":"In general, the larger alpha is, the \"flatter\" the fit will be. Eventually, as alpha approaches infinity, the prediction y_hat will just be a constant, since all thetas (except the intercept) will be regularized to zero. <br\/>\n\nIn theory, ridge regression with alpha = 0 should give the same result as regular linear regression, but sometimes that is not the case. See one post here: https:\/\/stackoverflow.com\/questions\/40570370\/difference-between-linearregression-and-ridgealpha-0. <br\/>\nThe post describes a polynomial model where ridge regression overflowed, but linear regression did not. (we don't have this problem here.)","f8b08952":"Let's first do a degree 10 linear regression model **without** regularization.","787e898f":"## Elastic Net\nElastic net is somewhere between ridge regression and lasso regression. The cost function is: <br\/>\nJ(theta) = MSE(theta) + r lasso_penalty + (1-r) ridge_penalty. ","e7b92bf3":"I did submit this result and my model's RMSE on the test set is 0.17055. For a simple regularized linear model with arbitrarily chosen indenpendent variables, this RMSE is good enough. <br\/>\nBelow I will talk about early stopping method, which ends up producing a similar but slightly worse RMSE on the test set.","1636b91d":"For sklearn 0.20, early stopping is added to SGDRegressor. You can run the following code to achieve what we did above:\n> sgdReg = SGDRegressor(penalty=None, learning_rate = 'constant', eta0=0.00001, early_stopping = True, n_iter_no_change = 500, max_iter=100000) <br\/>\nsgdReg.fit(xShuffled, yShuffled)","fb38bd83":"# Multi-variable models (for actual prediction)\nHere, we try to predict house prices with an elastic net model with fourth order features. <br\/>\nLet's look at the training set again:\n\n","18e02ca1":"With Scikit-Learn, you can actually do SGD with early stopping in one line, but we will write a more detailed version here for better understanding of the algorithm:","74d423e8":"Due to the random nature of SGD algorithm, you will have different results every time you run the code. Here is a plot of RMSE for one of my runs: <br\/>\n<img src=\"https:\/\/imgur.com\/6XyZLSa.png\" width=\"300px\"\/>\nThis is to illustrate how using k-fold splitting can help with model performance. As you can see, a big drop of RMSE takes place when we switch to the second fold (epoch=100000 in this case).","1294e0c6":"## SGD and Early Stopping\nWe have introduced stochastic gradient descent (SGD) in the previous machine learning tutorial [Machine Learning 1 - Regression, Gradient Descent](https:\/\/www.kaggle.com\/fengdanye\/machine-learning-1-regression-gradient-descent). For iterative learning algorithm like SGD, a good regularization technique is **early stopping**. To use this technique, we split the available data for training into a **training** set and a **validation** set: <br\/>\n<img src=\"https:\/\/imgur.com\/anuQyV9.jpg\" width=\"500px\"\/>\nThen, for each epoch, we train the model on the training set and calculate RMSE for both the training set and the validation set. At first, both RMSE_training (blue curve below) and RMSE_validation (red curve below) should be overall decreasing. But starting from a certain epoch, the RMSE_validation will start increasing, indicating the model is now overfitting to the training set:\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fc\/Overfitting.png\" width=\"300px\"\/>\nThe early stopping technique simply stops the training when RMSE_validation reaches its minimum. For SGD, randomness is present and early stopping will stop the training when RMSE_validation is above its minimum for a certain time and roll back to the parameters that give the minimum RMSE_validation value. <br\/>\nLet's try SGD with early stopping on our dataset.","e26f6999":"If you run the above code multiple times, you will notice that the algorithm's performance fluctuate a bit with different shuffling results. Some selected training sets may lead to much worse RMSE than others. Let's try to improve the model with k-fold splitting.","5348d70a":"## Ridge Regression\nRegular linear regression has the form of: J(theta) = MSE(theta) <br\/>\n\nRidge regression appy a regularization term proportional to the **square of l2-norm** of feature weights (not including the intercept). A common expression is: <br\/>\nJ(theta) = MSE(theta) + alpha 1\/2 (theta_1^2 + theta_2^2 + ... + theta_n^2)\n\nThe corrsponding expression for gradient of theta and the optimal solution for theta will change, due to the additonal term. We can also use the **Scikit-Learn** package to do ridge regression.","b697d7b5":"**Table of Content:** <br\/>\nThis notebook is an illustration of how regularized linear models work, using the house prices **training** set. <br\/>\nI did submit my prediction just to see how it performs on the test set, but the purpose is **not** to enter competition, but to teach the various regularization methods. \n\n* Single-variable models (for illustration purposes)\n    * Ridge regression\n    * Lasso regression\n    * Elastic net\n* Multi-variable models\n    * Elastic net\n    * SGD and early stopping (with k-fold splitting)","ac4fb321":"Thought with lower training RMSE, the SGD + early stopping model actually does not perform as well as the elastic net model. I got RMSE=0.18 for one of my submissions on this SGD model. My speculation:\n* SGD model gives different model each time, due to its random nature. Ensemble learning should help with this probelm.\n* SGD model is overfitting to the training + validation sets, therefore perform not as well on the test set. **Early stopping really only prevents overfitting to the training set (the one with validation set taken out)**. Including a regularization term should help with this problem.","1e54f5b6":"Not bad. Let's try the test set."}}