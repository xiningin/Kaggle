{"cell_type":{"b85205b0":"code","4267a3cc":"code","e0cf6839":"code","3409aed8":"code","4389ce7f":"code","97bb5ba9":"code","fa1f3d64":"code","c7f0cacb":"code","708ff6bc":"code","48a99225":"code","77343d6e":"code","a28b143c":"code","9215549c":"code","b269213d":"code","445aa2bc":"code","943855ff":"code","5346ae4b":"code","8c071b2b":"markdown"},"source":{"b85205b0":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nimport tensorflow as tf\nfrom tensorflow.keras import layers,regularizers,Sequential,backend,callbacks,optimizers,metrics, Model,losses\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.decomposition import PCA","4267a3cc":"# Import train data, drop sig_id, cp_type\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nnon_ctl_idx = train_features.loc[train_features['cp_type']!='ctl_vehicle'].index.to_list()\ntrain_features = train_features.drop(['sig_id','cp_type'],axis=1)\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_scored = train_targets_scored.drop('sig_id',axis=1)\nlabels_train = train_targets_scored.values\n\n# Drop training data with ctl vehicle\n\ntrain_features = train_features.iloc[non_ctl_idx]\nlabels_train = labels_train[non_ctl_idx]\n\n# Import test data\n\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest_features = test_features.drop(['sig_id'],axis=1)","e0cf6839":"# Label Encoder for categorical cp_dose\n\ncat = 'cp_dose'\nle = preprocessing.LabelEncoder()\nle.fit(train_features[cat])\ntrain_features[cat] = le.transform(train_features[cat])\n\n# Transform categorical\n\ntest_features[cat] = le.transform(test_features[cat])","3409aed8":"# Quantile Transformer for gene and cell features\n\nscaler = preprocessing.QuantileTransformer(output_distribution='normal')\n\n# Scale train data\ndata_train = scaler.fit_transform(train_features.iloc[:,2:])\n\n# Scale Test data\ndata_test = scaler.transform(test_features.drop('cp_type',axis=1).iloc[:,2:])\n\n# Standard Scaling for Dose\/Time\n\nscaler = preprocessing.StandardScaler()\ndata_train =  np.concatenate((scaler.fit_transform(train_features.iloc[:,:2]),data_train),axis=1)\ndata_test =  np.concatenate((scaler.transform(test_features.drop('cp_type',axis=1).iloc[:,:2]),data_test),axis=1)","4389ce7f":"n_features = data_train.shape[1]\nn_labels = labels_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]","97bb5ba9":"# Autoencoder to create compressed features\n\n# Cells\ncs = train_features.columns.str.contains('c-')\ncells_train = data_train[:,cs]\ncells_test = data_test[:,cs]\nencoding_dim = 256\nactivation = 'swish'\ndropout_noise = 0.15\ndropout = 0.1\nclass Autoencoder(Model):\n    def __init__(self, n_inputs, encoding_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = Sequential([\n            layers.BatchNormalization(),\n            layers.Dropout(dropout_noise),\n            layers.Dense(0.5*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(0.75*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(encoding_dim,activation=activation),\n        ])\n        self.decoder = Sequential([\n            layers.Dense(0.75*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(0.5*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(n_inputs)\n        ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \ncells_autoencoder = Autoencoder(cs.sum(),encoding_dim)\ncells_autoencoder.compile(optimizer=optimizers.Adam(learning_rate=1E-5), loss='mse')\nae_reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1E-5)\nae_early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=16, restore_best_weights=True)\ndef scheduler(epoch,lr):\n    if epoch%32<17:\n        lr += (0.3*1E-2)\/16\n    else:\n        lr -= (0.3*1E-2)\/16\n    return lr\n\nlr_scheduler = callbacks.LearningRateScheduler(scheduler)\nhist = cells_autoencoder.fit(cells_train,cells_train,batch_size=128, verbose=0, validation_data = (cells_test,cells_test), epochs=256, shuffle=True,\n                             callbacks=[ae_early_stopping,lr_scheduler])\n\ncells_autoencoder.compile(optimizer=optimizers.Adam(learning_rate=5*1E-5), loss='mse')\n\ncells_autoencoder.fit(cells_train,cells_train,batch_size=128, verbose=0, validation_data = (cells_test,cells_test), epochs=256, shuffle=True,\n                             callbacks=[ae_early_stopping,ae_reduce_lr])\ncells_autoencoder.save('CellsAE')","fa1f3d64":"plt.figure(figsize=(12,8))\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.yscale('log')\n# plt.yticks(ticks=[1,1E-1])\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","c7f0cacb":"# Genes\ngs = train_features.columns.str.contains('g-')\ngenes_train = data_train[:,gs]\ngenes_test = data_test[:,gs]\nencoding_dim = 1600\nactivation = 'swish'\ndropout_noise = 0.15\ndropout = 0.1\nclass Autoencoder(Model):\n    def __init__(self, n_inputs, encoding_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = Sequential([\n            layers.BatchNormalization(),\n            layers.Dropout(dropout_noise),\n            layers.Dense(0.5*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(0.75*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(encoding_dim,activation=activation),\n        ])\n        self.decoder = Sequential([\n            layers.Dense(0.75*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(0.5*encoding_dim,activation=activation),\n            layers.BatchNormalization(),\n            layers.Dropout(dropout),\n            layers.Dense(n_inputs)\n        ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \ngenes_autoencoder = Autoencoder(gs.sum(),encoding_dim)\ngenes_autoencoder.compile(optimizer=optimizers.Adam(learning_rate=1E-5), loss='mse')\nae_reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1E-5)\nae_early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=1E-5, patience=16, restore_best_weights=True)\n\nhist = genes_autoencoder.fit(genes_train,genes_train,batch_size=128, verbose=0, validation_data = (genes_test,genes_test), epochs=256, shuffle=True, \n                             callbacks=[ae_early_stopping,lr_scheduler])\n\ngenes_autoencoder.compile(optimizer=optimizers.Adam(learning_rate=5*1E-5), loss='mse')\ngenes_autoencoder.fit(genes_train,genes_train,batch_size=128, verbose=0, validation_data = (genes_test,genes_test), epochs=256, shuffle=True, \n                             callbacks=[ae_early_stopping])\ngenes_autoencoder.save('GenesAE')","708ff6bc":"plt.figure(figsize=(12,8))\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.yscale('log')\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])","48a99225":"ae_cells_train = cells_autoencoder.encoder(cells_train).numpy()\nae_cells_test = cells_autoencoder.encoder(cells_test).numpy()\nautoencoder_error = mean_squared_error(cells_test,cells_autoencoder.decoder(ae_cells_test).numpy())\nprint(\"Cells reconstruction error is \" + str(autoencoder_error))","77343d6e":"# Comparison with PCA\n\npca = PCA(n_components=64)\npca.fit(cells_train)\npca_error = mean_squared_error(cells_test,pca.inverse_transform(pca.transform(cells_test)))\nprint('PCA Reconstruction Error for Cells is ' + str(pca_error))","a28b143c":"ae_genes_train = genes_autoencoder.encoder(genes_train).numpy()\nae_genes_test = genes_autoencoder.encoder(genes_test).numpy()\nautoencoder_error = mean_squared_error(genes_test,genes_autoencoder.decoder(ae_genes_test).numpy())\nprint(\"Genes reconstruction error is \" + str(autoencoder_error))","9215549c":"# Comparison with PCA\n\npca = PCA(n_components=512)\npca.fit(genes_train)\npca_error = mean_squared_error(genes_test,pca.inverse_transform(pca.transform(genes_test)))\nprint('PCA Reconstruction Error for Genes is ' + str(pca_error))","b269213d":"# Replace data with encoded data\n\ndata_train = np.concatenate((data_train[:,~(cs+gs)],ae_genes_train,ae_cells_train),axis=1)\ndata_test = np.concatenate((data_test[:,~(cs+gs)],ae_genes_test,ae_cells_test),axis=1)","445aa2bc":"# Train\n\nn_labels = train_targets_scored.shape[1]\nn_features = data_train.shape[1]\nn_train = data_train.shape[0]\nn_test = data_test.shape[0]\n\n\n# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# Evaluation Metric with clipping and no label smoothing\n\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n\n# Generate Seeds\n\nn_seeds = 3\nseeds = [34,9,18]\n\n# Training Loop\n\nn_folds = 5\ny_pred = np.zeros((n_test,n_labels))\noof = tf.constant(0.0)\nhists = []\nbias = tf.keras.initializers.Constant(np.log(labels_train.mean(axis=0)))\nfor seed in seeds:\n    fold = 0\n    mskf = MultilabelStratifiedKFold(n_splits=n_folds,shuffle=True,random_state=seed)\n    for train, test in mskf.split(data_train,labels_train):\n        X_train = data_train[train]\n        X_test = data_train[test]\n        y_train = labels_train[train]\n        y_test = labels_train[test]\n\n        # Define NN Model\n\n        model = Sequential()\n        model.add(layers.Dropout(0.3))\n        model.add(layers.Dense(1536))\n        model.add(layers.Activation('elu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.7))\n        model.add(layers.Dense(1024))\n        model.add(layers.Activation('elu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(512))\n        model.add(layers.Activation('elu'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n        model.add(layers.Dense(n_labels,activation='sigmoid',bias_initializer=bias))\n        model.compile(optimizer=optimizers.Adam(learning_rate=1E-5), loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics=['binary_crossentropy',logloss])\n        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_logloss', factor=0.3, patience=5, mode='min', min_lr=1E-5)\n        early_stopping = callbacks.EarlyStopping(monitor='val_logloss', min_delta=1E-5, patience=24, mode='min',restore_best_weights=True)\n        def scheduler(epoch,lr):\n            if epoch%16<9:\n                lr += np.exp(-int(epoch\/16))*(0.5*1E-2)\/16\n            else:\n                lr -= np.exp(-int(epoch\/16))*(0.5*1E-2)\/16\n            return lr\n\n        lr_scheduler = callbacks.LearningRateScheduler(scheduler)\n        hist = model.fit(X_train,y_train, batch_size=128, epochs=192,verbose=0,validation_data = (X_test,y_test),callbacks=[lr_scheduler, early_stopping])\n        \n        model.compile(optimizer=optimizers.Adam(learning_rate=1E-5), loss=losses.BinaryCrossentropy(label_smoothing=0.001), metrics=['binary_crossentropy',logloss])\n        model.fit(X_train,y_train, batch_size=128, epochs=192,verbose=0,validation_data = (X_test,y_test),callbacks=[early_stopping])\n\n        hists.append(hist)\n        \n        # Save Model\n        model.save('AutoEncoded_seed_'+str(seed)+'_fold_'+str(fold))\n\n        # OOF Score\n        y_val = model.predict(X_test)\n        oof += logloss(tf.constant(y_test,dtype=tf.float32),tf.constant(y_val,dtype=tf.float32))\/(n_folds*n_seeds)\n\n        # Run prediction\n        y_pred += model.predict(data_test)\/(n_folds*n_seeds)\n\n        fold += 1","943855ff":"# Analysis of Training\n\ntf.print('OOF score is ',oof)\n\nplt.figure(figsize=(12,8))\n\nhist_trains = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_train = (hists[i]).history['logloss']\n    hist_trains.append(hist_train)\n    hist_lens.append(len(hist_train))\nhist_train = []\nfor i in range(min(hist_lens)):\n    hist_train.append(np.mean([hist_trains[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_train)\n\nhist_vals = []\nhist_lens = []\nfor i in range(n_folds*n_seeds):\n    hist_val = (hists[i]).history['val_logloss']\n    hist_vals.append(hist_val)\n    hist_lens.append(len(hist_val))\nhist_val = []\nfor i in range(min(hist_lens)):\n    hist_val.append(np.mean([hist_vals[j][i] for j in range(n_folds*n_seeds)]))\n\nplt.plot(hist_val)\n\nplt.yscale('log')\nplt.xlabel('Epochs')\nplt.ylabel('Average Logloss')\nplt.legend(['Training','Validation'])\n","5346ae4b":"# Prediction Clipping Thresholds\n\np_min = 0.0005\np_max = 0.9995\n\n# Generate submission file, Clip Predictions\n\nsub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsub.iloc[:,1:] = np.clip(y_pred,p_min,p_max)\n\n# Set ctl_vehicle to 0\nsub.iloc[test_features['cp_type'] == 'ctl_vehicle',1:] = 0\n\n# Save Submission\nsub.to_csv('submission.csv', index=False)","8c071b2b":"# Experiment: Denoising Autoencoder for FE\n\n\n## Best LB so far (using only autoencoder features to generate predictions): V8 (LB 0.01879)"}}