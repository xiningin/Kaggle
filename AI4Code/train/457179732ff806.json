{"cell_type":{"cc4167c0":"code","efde17f6":"code","f40b7029":"code","5ed32aa3":"code","0ca0d813":"code","0582b0b1":"code","72e33cc2":"markdown","8575537f":"markdown","191c8c7f":"markdown","22daf328":"markdown"},"source":{"cc4167c0":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom scipy import stats","efde17f6":"def mu(x):\n    return np.sin(np.pi * x)\n\n\ndef sigma(x):\n    return 0.5 + np.where(x > 0, 1, 2) * np.abs(x)\n\n\nX = np.random.uniform(-3, 3, (5000, 1))\ny = np.random.normal(mu(X), sigma(X))\nX_out = np.concatenate([np.random.uniform(-5, -3, (1000, 1)), np.random.uniform(3, 5, (1000, 1))], 0)\ny_out  = np.random.normal(mu(X_out), sigma(X_out))\n\nplt.figure(figsize=(16, 8))\nplt.scatter(X, y, alpha=0.5, color='tab:blue')\nplt.scatter(X_out, y_out, alpha=0.5, color='tab:orange');","f40b7029":"def QuantileLoss(perc, delta=1e-4):\n    perc = np.array(perc).reshape(-1)\n    perc.sort()\n    perc = perc.reshape(1, -1)\n    def _qloss(y, pred):\n        I = tf.cast(y <= pred, tf.float32)\n        d = K.abs(y - pred)\n        correction = I * (1 - perc) + (1 - I) * perc\n        # huber loss\n        huber_loss = K.sum(correction * tf.where(d <= delta, 0.5 * d ** 2 \/ delta, d - 0.5 * delta), -1)\n        # order loss\n        q_order_loss = K.sum(K.maximum(0.0, pred[:, :-1] - pred[:, 1:] + 1e-6), -1)\n        return huber_loss + q_order_loss\n    return _qloss","5ed32aa3":"perc_points = [0.01, 0.25, 0.5, 0.75, 0.99]\nmodel = keras.Sequential([\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(5)\n])\nmodel.compile(optimizer=keras.optimizers.Adam(2e-3), loss=QuantileLoss(perc_points))\nmodel.fit(X, y, epochs=100, verbose=0)","0ca0d813":"xx = np.linspace(X_out.min(), X_out.max(), 500)\npred = model.predict(xx)\n\nplt.figure(figsize=(16, 8))\nplt.scatter(X, y, alpha=0.5, color='tab:blue')\nplt.scatter(X_out, y_out, alpha=0.5, color='tab:orange')\nplt.plot(xx, pred, color='tab:red', linestyle='--')\nplt.xlabel('X')\nplt.ylabel('y');","0582b0b1":"xs = np.array([-4, -2, 0, 2, 4]).reshape(-1, 1)\nfor i in range(xs.shape[0]):\n    x0 = xs[i:(i + 1), :]\n    mu0, sigma0 = mu(x0).squeeze(), sigma(x0).squeeze()\n    z = np.linspace(mu0 - 4 * sigma0, mu0 + 4 * sigma0, 100)\n    p = stats.norm(mu0, sigma0).cdf(z).reshape(-1)\n    plt.figure(figsize=(10, 5))\n    plt.plot(z, p, color='tab:blue', label='CDF')\n    plt.scatter(model.predict(x0), perc_points, color='tab:red', label='NN')\n    plt.title(f\"x = {x0.squeeze()}\")\n    plt.legend()\n    plt.ylim(0, 1)","72e33cc2":"Now we visualize the real cumulative distribution in various points VS the one estimated by the model","8575537f":"We build a synthetic dataset","191c8c7f":"Then we define the quantile-loss function. The huber-like loss is preferable to the absolute error since it is differentiable","22daf328":"# Deep Quantile Regression in Keras"}}