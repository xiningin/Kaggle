{"cell_type":{"4842f015":"code","b4dfb011":"code","ff59c9b1":"code","b2f09453":"code","9a5bec6e":"code","2de2fc01":"code","3aad86fd":"code","68beea43":"code","04318f21":"code","18953069":"code","11b4d892":"code","01f4ebc6":"code","0df21298":"code","8984b2c3":"code","df7e0016":"code","ee750892":"code","b755ca42":"code","d6296397":"code","ca404737":"code","f66da4bc":"code","7c53b788":"code","36966df9":"code","0b65170f":"code","17966e49":"code","a743cd7d":"code","d77a1dd3":"code","241ea5f2":"code","0d114ccd":"code","ee06d87a":"code","eb74b02f":"code","f7dc95e5":"code","24f7bd7b":"code","15395895":"code","23beb245":"code","a3d587ba":"code","f355c0b0":"code","e8bad87c":"code","8d22d544":"code","aee42791":"code","01deb634":"code","1db14695":"code","fc63657b":"code","e6f116ae":"code","cd6d5896":"code","965b946c":"code","0d6f37bf":"code","e42df693":"code","741bda23":"code","d6785866":"code","c94d9064":"code","6f5f74d7":"markdown","f31afb2d":"markdown","9b7feb4a":"markdown","a4e5c01c":"markdown","6c86476f":"markdown","38bd1f6c":"markdown","5693fbc5":"markdown","471cd91b":"markdown","51fab874":"markdown","0f5eadbc":"markdown","1e0e3772":"markdown","eaded84f":"markdown","a07f1fc4":"markdown","65d21032":"markdown","7d649d45":"markdown","f56dbc89":"markdown","f74b1caa":"markdown","42c0a56b":"markdown","238bba42":"markdown","b7139420":"markdown","4885b315":"markdown","67625497":"markdown","2e62b1fe":"markdown","65a4ca93":"markdown","9136357e":"markdown","37d1ee0a":"markdown","296cd54c":"markdown"},"source":{"4842f015":"import gc\nimport time\nimport pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\n\nimport lightgbm as lgb\n\n#Path to save the output\npath_analysis = 'C:\\\\Users\\\\maxwi\\\\Python\\\\Kaggle\\\\Mechanism of action\\\\data_analysis\\\\'","b4dfb011":"# Load the Data\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')","ff59c9b1":"#Function to reduce memory usage.\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b2f09453":"train_features = reduce_mem_usage(train_features)\ntrain_targets_scored = reduce_mem_usage(train_targets_scored)\ntest_features = reduce_mem_usage(test_features)","9a5bec6e":"display(train_features)\nprint()\n\n#Verify missing.\ntrain_features_missing = pd.DataFrame(train_features.isnull().sum())\nprint('Quantity of missing values:', train_features_missing[0].sum())","2de2fc01":"display(train_targets_scored)\nprint()\n\n#Verify missing.\ntrain_targets_scored_missing = pd.DataFrame(train_targets_scored.isnull().sum())\nprint('Quantity of missing values:', train_targets_scored_missing[0].sum())","3aad86fd":"display(test_features)\nprint()\n\n#Verify missing.\ntest_features_missing = pd.DataFrame(test_features.isnull().sum())\nprint('Quantity of missing values:', test_features_missing[0].sum())","68beea43":"'''\nBefore start studying the variables, lets separate train_features and train_targets_scored into train_set and validation_set.\nThis is important to avoid overfitting.\n'''\ntrain_features_set, validation_features_set = train_test_split(train_features, test_size = 0.2, random_state = 1).copy()\n\ntrain_labels_set = train_targets_scored.iloc[train_features_set.index].copy()\nvalidation_labels_set = train_targets_scored.iloc[validation_features_set.index].copy()\n\n#reset index. We don't really need a index as a number. We can identify our observations by the column 'sig_id'\ntrain_features_set.reset_index(inplace = True)\ntrain_features_set.drop('index', axis = 1, inplace = True)\n\nvalidation_features_set.reset_index(inplace = True)\nvalidation_features_set.drop('index', axis = 1, inplace = True)\n\ntrain_labels_set.reset_index(inplace = True)\ntrain_labels_set.drop('index', axis = 1, inplace = True)\n\nvalidation_labels_set.reset_index(inplace = True)\nvalidation_labels_set.drop('index', axis = 1, inplace = True)","04318f21":"#It looks like we have a lot of variables g-x and c-x.\n#Lets first verify the names of our variables.\nfeatures_df = pd.DataFrame(train_features_set.columns)\nfeatures_df['first_2_letter'] = features_df[0].str.extract(r\"(.{2})\", expand = True)\nfeatures_df['first_2_letter'].value_counts()","18953069":"#Lets see the variable cp_type\ntrain_features_cp_type_bar_graph = pd.DataFrame(train_features_set['cp_type'].value_counts())\n\nax = train_features_cp_type_bar_graph.plot(kind='bar', figsize=(15,10), width = 0.58, rot = 0,\n                                           align='center', color = 'LightGray', edgecolor = None)\n\ntotal = 0\nfor bars in ax.patches:\n    total += bars.get_height()\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height\/total:.2%}', (x + width\/2, y + 100 + height), ha = 'center')","11b4d892":"#Nonetheless, lets verify if the control group really has no MoA.\n#first, lets sum how many MoAs each observations has.\ntrain_labels_set['sum_MoA'] = train_labels_set.drop(['sig_id'], axis = 1).astype(bool).sum(axis = 1)\n#Now, we sum the total MoA of each obsevartion in control\ntrain_labels_set[train_features_set['cp_type'] == 'ctl_vehicle']['sum_MoA'].sum()","01f4ebc6":"#Lets see if, despite having no MoA, the control group has value for the other variables.\ntrain_features_set[train_features_set['cp_type'] == 'ctl_vehicle']","0df21298":"# Remove control observations.\ntrain_features_without_control = train_features_set[train_features_set['cp_type'] != 'ctl_vehicle'].copy()\ntrain_labels_without_control = train_labels_set.iloc[train_features_without_control.index].copy()\nprint('Observations left to train our model:', len(train_features_without_control))\n\n# Only control observations.\ntrain_features_only_control = train_features_set[train_features_set['cp_type'] == 'ctl_vehicle'].copy()\ntrain_labels_only_control = train_labels_set.iloc[train_features_only_control.index].copy()\n\n#reset index. \ntrain_features_without_control.reset_index(inplace = True)\ntrain_features_without_control.drop('index', axis = 1, inplace = True)\n\ntrain_labels_without_control.reset_index(inplace = True)\ntrain_labels_without_control.drop('index', axis = 1, inplace = True)\n\ntrain_features_without_control.reset_index(inplace = True)\ntrain_features_without_control.drop('index', axis = 1, inplace = True)\n\ntrain_labels_only_control.reset_index(inplace = True)\ntrain_labels_only_control.drop('index', axis = 1, inplace = True)","8984b2c3":"'''\nFor the validation and test, we will remove just the column 'cp_type'.\nHowever, we will not delete the observations. \nInstead, we will create a separate data frame with 'cp_type' so we can set all MoA equal to zero for these observations.\n'''\n\nvalidation_features_without_control = validation_features_set.drop(['cp_type'], axis = 1).copy()\nvalidation_features_control = validation_features_set[['sig_id', 'cp_type']].copy()\n\ntest_features_without_control = test_features.drop(['cp_type'], axis = 1).copy()\ntest_features_control = test_features[['sig_id', 'cp_type']].copy()","df7e0016":"#Now, lets study the variable cp_time\ntrain_features_cp_time_bar_graph = pd.DataFrame(train_features_without_control['cp_time'].value_counts()).sort_index()\n\nax = train_features_cp_time_bar_graph.plot(kind='bar', figsize=(15,10), width = 0.58, rot = 0,\n                                           align = 'center', color = 'LightGray', edgecolor = None)\n\ntotal = 0\nfor bars in ax.patches:\n    total += bars.get_height()\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height\/total:.2%}', (x + width\/2, y + 50 + height), ha = 'center')","ee750892":"#Compute value_counts just to see how many bins we should plot at the histogram.\ntrain_labels_without_control_cp_time_hist_24 = train_labels_without_control[train_features_without_control['cp_time'] == 24]['sum_MoA'].value_counts().sort_index()\ntrain_labels_without_control_cp_time_hist_24","b755ca42":"train_labels_without_control_cp_time_hist_48 = train_labels_without_control[train_features_without_control['cp_time'] == 48]['sum_MoA'].value_counts().sort_index()\ntrain_labels_without_control_cp_time_hist_48","d6296397":"train_labels_without_control_cp_time_hist_72 = train_labels_without_control[train_features_without_control['cp_time'] == 72]['sum_MoA'].value_counts().sort_index()\ntrain_labels_without_control_cp_time_hist_72","ca404737":"#Lets transform these tables in histograns to make it easier to the eyes.\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(3,1,1)\nax1 = train_labels_without_control[train_features_without_control['cp_time'] == 24]['sum_MoA'].hist(density = True,\n                histtype = 'bar', bins = 7, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('cp_time = 24')\nplt.ylim(top=0.6)\n\nax2 = fig.add_subplot(3,1,2)\nax2 = train_labels_without_control[train_features_without_control['cp_time'] == 48]['sum_MoA'].hist(density = True,\n                histtype = 'bar', bins = 7, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('cp_time = 48')\nplt.ylim(top=0.6)\n\nax3 = fig.add_subplot(3,1,3)\nax3 = train_labels_without_control[train_features_without_control['cp_time'] == 24]['sum_MoA'].hist(density = True,\n                histtype = 'bar', bins = 7, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('cp_time = 72')\nplt.ylim(top=0.6)","f66da4bc":"#Study of the variable cp_dose\ntrain_features_cp_dose_bar_graph = pd.DataFrame(train_features_without_control['cp_dose'].value_counts()).sort_index()\n\nax = train_features_cp_dose_bar_graph.plot(kind='bar', figsize=(15,10), width = 0.58, rot = 0,\n                                           align = 'center', color = 'LightGray', edgecolor = None)\n\ntotal = 0\nfor bars in ax.patches:\n    total += bars.get_height()\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height\/total:.2%}', (x + width\/2, y + 50 + height), ha = 'center')","7c53b788":"#Our data is well balanced in relation to drug dosage.\n#Lets see if the number of MoA is equilibrated too.\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(2,1,1)\nax1 = train_labels_without_control[train_features_without_control['cp_dose'] == 'D1']['sum_MoA'].hist(density = True,\n                histtype = 'bar', bins = 7, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('cp_dose = D1')\nplt.ylim(top=0.6)\n\nax2 = fig.add_subplot(2,1,2)\nax2 = train_labels_without_control[train_features_without_control['cp_dose'] == 'D2']['sum_MoA'].hist(density = True,\n                histtype = 'bar', bins = 7, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('cp_dose = D2')\nplt.ylim(top=0.6)","36966df9":"#Lets see if the realation between cp_time and cp_dose is equilibrated too.\ntrain_cp_time_cp_dose_sunburst = train_features_without_control.groupby(['cp_time', 'cp_dose'])['sig_id'].count().reset_index()\ntrain_cp_time_cp_dose_sunburst.columns = ['cp_time', 'cp_dose', 'count']\n\nfig = px.sunburst(\n    train_cp_time_cp_dose_sunburst,\n    path = ['cp_time','cp_dose'],\n    values = 'count',\n    title = 'Sunburst chart: cp_time, cp_dose',\n    color = 'count', color_continuous_scale = 'Blues',\n)\n\nfig.update_traces(go.Sunburst(textinfo= 'label + percent entry'))\n    \nfig.show()\n","0b65170f":"#Another way to see the same information is with a tree map\nfig =px.treemap(\n    train_cp_time_cp_dose_sunburst,\n    path = ['cp_dose', 'cp_time'],\n    values = 'count',\n    title = 'Tree map: cp_time, cp_dose',\n    color = 'count', color_continuous_scale = 'Blues',\n)\n\nfig.update_layout(template = 'seaborn')\nfig.data[0].textinfo = 'label + percent entry'\nfig.show()","17966e49":"#Computing the standard deviation for 'g_' and 'c_' variables.\ndescribe_g_c_variables = train_features_without_control.drop(['cp_time'], axis = 1).describe()\nstd_g_c = describe_g_c_variables.iloc[2]\nprint('Describe the standard deviation of g and c variables:')\ndisplay(std_g_c.describe())\nprint()\n\n#Histogram\nprint('Histogram:')\nfig = plt.figure(figsize=(15,10))\n\nax = std_g_c.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g and c variables')\nplt.ylim(top = 2)","a743cd7d":"#Computing the standard deviation for 'g_' variables.\nmask_g = train_features_without_control.columns.str.contains('^g-*')\n\ndescribe_g_variables = train_features_without_control.loc[:,mask_g].describe()\nstd_g = describe_g_variables.iloc[2]\nprint('Describe the standard deviation of g variables:')\ndisplay(std_g.describe())\nprint()\n\n#Histogram\nprint('Histogram:')\nfig = plt.figure(figsize=(15,10))\n\nax = std_g.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","d77a1dd3":"#Computing the standard deviation for 'c_' variables.\nmask_c = train_features_without_control.columns.str.contains('^c-[0-99]')\n\ndescribe_c_variables = train_features_without_control.loc[:,mask_c].describe()\nstd_c = describe_c_variables.iloc[2]\nprint('Describe the standard deviation of c variables:')\ndisplay(std_c.describe())\nprint()\n\n#Histogram\nprint('Histogram:')\nfig = plt.figure(figsize=(15,10))\n\nax = std_c.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","241ea5f2":"#Summarizing the comparison between g and c variables with a box plot.\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(2,2,1)\nax1 = sns.boxplot(data = std_g, color = 'LightGray')\nplt.xlabel('standard of g variables')\nplt.ylim(bottom = 0.5, top = 2.5)\n\n\nax2 = fig.add_subplot(2, 2, 2)\nax2 = sns.boxplot(data = std_c, color = 'LightGray')\nplt.xlabel('standard of c variables')\nplt.ylim(bottom = 0.5, top = 2.5)\n","0d114ccd":"describe_g_variables_without_control = train_features_without_control.loc[:,mask_g].describe()\nstd_g_without_control = describe_g_variables_without_control.iloc[2]\nprint('Describe the standard deviation of g variables without control:')\ndisplay(std_g_without_control.describe())\nprint()\ndescribe_g_variables_only_control = train_features_only_control.loc[:,mask_g].describe()\nstd_g_only_control = describe_g_variables_only_control.iloc[2]\nprint('Describe the standard deviation of g variables with only control:')\ndisplay(std_g_only_control.describe())\nprint()\n\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(2,1,1)\nax1 = std_g_without_control.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables without control')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax2 = fig.add_subplot(2,1,2)\nax2 = std_g_only_control.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables with only control')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","ee06d87a":"describe_c_variables_without_control = train_features_without_control.loc[:,mask_c].describe()\nstd_c_without_control = describe_c_variables_without_control.iloc[2]\nprint('Describe the standard deviation of c variables without control:')\ndisplay(std_c_without_control.describe())\nprint()\ndescribe_c_variables_only_control = train_features_only_control.loc[:,mask_c].describe()\nstd_c_only_control = describe_c_variables_only_control.iloc[2]\nprint('Describe the standard deviation of c variables with only control:')\ndisplay(std_c_only_control.describe())\nprint()\n\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(2,1,1)\nax1 = std_c_without_control.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables without control')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax2 = fig.add_subplot(2,1,2)\nax2 = std_c_only_control.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables with only control')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","eb74b02f":"train_g_variables = train_features_without_control.loc[:,mask_g]\ndescribe_g_variables_D1 = train_g_variables[np.in1d(train_features_without_control['cp_dose'], 'D1')].describe()\nstd_g_D1 = describe_g_variables_D1.iloc[2]\nprint('Describe the standard deviation of g variables, dosage = \"D1\":')\ndisplay(std_g_D1.describe())\nprint()\nstd_g_D2 = train_g_variables[np.in1d(train_features_without_control['cp_dose'], 'D2')].describe()\nstd_g_D2 = std_g_D2.iloc[2]\nprint('Describe the standard deviation of g variables, dosage = \"D2\":')\ndisplay(std_g_D2.describe())\nprint()\n\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(2,1,1)\nax1 = std_g_D1.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables, dosage = \"D1\":')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax2 = fig.add_subplot(2,1,2)\nax2 = std_g_D2.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables, dosage = \"D2\":')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","f7dc95e5":"train_c_variables = train_features_without_control.loc[:,mask_c]\ndescribe_c_variables_D1 = train_c_variables[np.in1d(train_features_without_control['cp_dose'], 'D1')].describe()\nstd_c_D1 = describe_c_variables_D1.iloc[2]\nprint('Describe the standard deviation of c variables, dosage = \"D1\":')\ndisplay(std_c_D1.describe())\nprint()\nstd_c_D2 = train_c_variables[np.in1d(train_features_without_control['cp_dose'], 'D2')].describe()\nstd_c_D2 = std_c_D2.iloc[2]\nprint('Describe the standard deviation of c variables, dosage = \"D2\":')\ndisplay(std_c_D2.describe())\nprint()\n\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(2,1,1)\nax1 = std_c_D1.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables, dosage = \"D1\":')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax2 = fig.add_subplot(2,1,2)\nax2 = std_c_D2.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables, dosage = \"D2\":')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","24f7bd7b":"train_g_variables = train_features_without_control.loc[:,mask_g]\nstd_g_24 = train_g_variables[np.in1d(train_features_without_control['cp_time'], 24)].describe()\nstd_g_24 = std_g_24.iloc[2]\nprint('Describe the standard deviation of g variables, duration = 24:')\ndisplay(std_g_24.describe())\nprint()\nstd_g_48 = train_g_variables[np.in1d(train_features_without_control['cp_time'], 48)].describe()\nstd_g_48 = std_g_48.iloc[2]\nprint('Describe the standard deviation of g variables, duration = 48:')\ndisplay(std_g_48.describe())\nprint()\nstd_g_72 = train_g_variables[np.in1d(train_features_without_control['cp_time'], 72)].describe()\nstd_g_72 = std_g_72.iloc[2]\nprint('Describe the standard deviation of g variables, duration = 72:')\ndisplay(std_g_48.describe())\nprint()\n\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(3,1,1)\nax1 = std_g_24.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables, duration = 24:')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax2 = fig.add_subplot(3,1,2)\nax2 = std_g_48.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables, duration = 48:')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax3 = fig.add_subplot(3,1,3)\nax3 = std_g_72.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of g variables, duration = 72:')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","15395895":"train_c_variables = train_features_without_control.loc[:,mask_c]\nstd_c_24 = train_c_variables[np.in1d(train_features_without_control['cp_time'], 24)].describe()\nstd_c_24 = std_c_24.iloc[2]\nprint('Describe the standard deviation of c variables, duration = 24:')\ndisplay(std_c_24.describe())\nprint()\nstd_c_48 = train_c_variables[np.in1d(train_features_without_control['cp_time'], 48)].describe()\nstd_c_48 = std_c_48.iloc[2]\nprint('Describe the standard deviation of c variables, duration = 48:')\ndisplay(std_c_48.describe())\nprint()\nstd_c_72 = train_c_variables[np.in1d(train_features_without_control['cp_time'], 72)].describe()\nstd_c_72 = std_c_72.iloc[2]\nprint('Describe the standard deviation of c variables, duration = 72:')\ndisplay(std_c_48.describe())\nprint()\n\nprint('Histogram:')\nfig = plt.figure(figsize=(15,25))\n\nax1 = fig.add_subplot(3,1,1)\nax1 = std_c_24.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables, duration = 24:')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax2 = fig.add_subplot(3,1,2)\nax2 = std_c_48.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables, duration = 48:')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)\n\nax3 = fig.add_subplot(3,1,3)\nax3 = std_c_72.hist(density = True,\n                histtype = 'bar', bins = 10, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('std of c variables, duration = 72:')\nplt.xlim(left = 0.5, right = 2.5)\nplt.ylim(top = 2.5)","23beb245":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef correlation_variables(db, variable_type, n, correlation_cut):\n    \n    if variable_type == 'c':\n        mask_type = db.columns.str.contains('^c-[0-99]')\n    elif variable_type == 'g':\n        mask_type = db.columns.str.contains('^g-*')\n        \n    db_variables = db.loc[:,mask_type]\n\n    #Correlation\n    corr_matrix = db_variables.corr()\n    corr_table = corr_matrix.abs().unstack()\n    labels_to_drop = get_redundant_pairs(db_variables)\n    corr_table = corr_table.drop(labels = labels_to_drop).sort_values(ascending = False)\n\n    print('Variable type: ' + variable_type)\n    print()\n    print(\"Top Absolute Correlations\")\n    print(corr_table[: n])\n    print()\n    print(\"Least Absolute Correlations\")\n    print(corr_table[len(corr_table) - n: ])\n    print()\n    print(\"Variables with correlation higher than\", correlation_cut, ':', len(corr_table[abs(corr_table) > correlation_cut]))\n    print(\"Percent:\", round(len(corr_table[abs(corr_table) > correlation_cut]) \/ len(corr_table), 4))\n    print()\n\n    size_x = 20     #This is a good size to visualise the heatmap saved as .png\n    size_y = 20\n    plt.figure(figsize = (size_x, size_y))\n    sns.set(font_scale = 1.5)\n\n    ax = sns.heatmap(corr_matrix, annot = False, linewidth = 0.2, cmap='coolwarm')\n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n\n    plt.tight_layout()\n    plt.savefig(path_analysis + 'variable_type_' + variable_type + '.png')\n\n    plt.show() ","a3d587ba":"correlation_variables(train_features_without_control, 'g', 10, 0.5)","f355c0b0":"correlation_variables(train_features_without_control, 'c', 10, 0.8)","e8bad87c":"# PCA\ndef pca_variabels(db, variable_type, interval_explained_variance_low, interval_explained_variance_high):\n    '''\n    pca_variabels(train_features_without_control, 'g', 340, 350)\n    '''\n    if variable_type == 'c':\n        mask_type = db.columns.str.contains('^c-[0-99]')\n    elif variable_type == 'g':\n        mask_type = db.columns.str.contains('^g-*')\n\n    db_variables = db.loc[:,mask_type]\n    pca = PCA(n_components = len(db_variables.columns),random_state = 4)\n    pca.fit(db_variables)\n\n    explained_variance = []\n\n    sum_variance = 0\n    num_variables = 0\n    for variance in pca.explained_variance_ratio_:\n        sum_variance += variance\n        num_variables += 1\n        explained_variance.append([num_variables, sum_variance])\n\n    print(\"Explained variance:\")\n    display(explained_variance[interval_explained_variance_low - 1: interval_explained_variance_high])\n    \n    return pd.DataFrame(data = explained_variance)","8d22d544":"#plot PCA: components Vs. explained \ndef graph_PCA(db, rotation_x, pace):\n    '''\n    loss_train_validation(db = log_loss_hp, rotation_x = 0, save_to = save_file_final)\n    '''\n    fig, ax = plt.subplots(figsize=(20, 15))\n    x = db[0]\n    y = db[1]\n\n    ax.xaxis.set_ticks(x)\n    ax.plot(x, y, color='gray', zorder = 1, linewidth = 0.5, linestyle = 'solid')\n    # add two layers of points to create an illusion of a discontinuous line. \"zorder\" specifies plotting order\n    ax.scatter(x, y, s = 64, color = 'white', zorder = 2)\n    ax.scatter(x, y, s = 8, color = 'gray', zorder = 3)\n\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n\n    ax.xaxis.set_ticks(list(range(1, len(x), pace)))\n    ax.set_facecolor('xkcd:white')\n    \n    plt.xticks(rotation = rotation_x)","aee42791":"pca_variabels(train_features_without_control, 'g', 340, 350)","01deb634":"pca_variabels(train_features_without_control, 'g', 160, 170)","1db14695":"explained_variance_g = pca_variabels(train_features_without_control, 'g', 65, 75)","fc63657b":"graph_PCA(explained_variance_g, 45, 50)","e6f116ae":"explained_variance_c = pca_variabels(train_features_without_control, 'c', 5, 15)","cd6d5896":"graph_PCA(explained_variance_c, 45, 10)","965b946c":"#Now that we already studied our exogenous variable, lets see our endogenous ones.\ntrain_labels_without_control","0d6f37bf":"print('Describe the endogenous varibles:')\ndisplay(train_labels_without_control['sum_MoA'].describe())\nprint()\n\n#Histogram\nprint('Histogram:')\nfig = plt.figure(figsize=(15,10))\nax = train_labels_without_control['sum_MoA'].hist(density = True,\n                histtype = 'bar', bins = 7, align = 'mid', orientation = 'vertical',\n                color = 'LightGray', edgecolor = None)\nplt.grid(b = None)\nplt.xlabel('Mechanism of action')\nplt.ylim(top=0.6)\n","e42df693":"#Box plot\nprint('Box plot:')\nfig = plt.figure(figsize=(10,10))\n\nax = sns.boxplot(data = train_labels_without_control['sum_MoA'], color = 'LightGray')\nplt.xlabel('Mechanism of action')","741bda23":"#Types of MoA\nmechanism_of_action_type_aux = train_labels_without_control.drop(['sig_id', 'sum_MoA'], axis = 1).columns\nmechanism_of_action_type_aux_split = mechanism_of_action_type_aux.str.split('_')\n\nmechanism_of_action_type = []\nfor i in range(len(mechanism_of_action_type_aux_split)):\n    #Words that appear only once\n    if mechanism_of_action_type_aux_split[i][-1] in ['b', 'stimulant', 'medium', 'local', 'donor',' sensitizer',\n                                                    'laxative','anticonvulsant','secretagogue', 'antibiotic',\n                                                    'antiprotozoal','antifolate','antimalarial', 'immunosuppressant',\n                                                    'antioxidant','antifungal','scavenger', 'antiviral',\n                                                    'steroid','antihistamine','diuretic', 'analgesic',\n                                                    'antiarrhythmic','anti-inflammatory']:\n        mechanism_of_action_type.append(mechanism_of_action_type_aux[i])\n    else:\n        mechanism_of_action_type.append(mechanism_of_action_type_aux_split[i][-1])\n\n\nmechanism_of_action_type_df = pd.DataFrame(data = mechanism_of_action_type)\nmechanism_of_action_type_df_barh_graph = mechanism_of_action_type_df[0].value_counts().sort_values()\nmechanism_of_action_type_df_barh_graph.plot(kind='barh', figsize=(15,13), width = 0.58, rot = 0,\n                                           align = 'center', color = 'LightGray', edgecolor = None)","d6785866":"sum_MoA = pd.DataFrame(data = train_labels_without_control.drop(['sig_id', 'sum_MoA'], axis = 1).sum().sort_values(ascending = False), columns = ['Count'])\nsum_MoA['Percent'] = round(sum_MoA['Count'] \/ len(train_labels_without_control), 4)\nsum_MoA.head(10)","c94d9064":"#Correlation between MoAs.\nn = 10\ndb_variables = train_labels_without_control.drop(['sig_id', 'sum_MoA'], axis = 1).copy()\ncorr_matrix = db_variables.corr()\ncorr_table = corr_matrix.abs().unstack()\nlabels_to_drop = get_redundant_pairs(db_variables)\ncorr_table = corr_table.drop(labels = labels_to_drop).sort_values(ascending = False)\nprint(\"Top Absolute Correlations:\")\nprint(corr_table[: n])\nprint()\nprint(\"Least Absolute Correlations:\")\nprint(corr_table[len(corr_table) - n: ])\nprint()\n\nsize_x = 20     #This is a good size to visualise the heatmap saved as .png\nsize_y = 20\nplt.figure(figsize = (size_x, size_y))\nsns.set(font_scale = 1.5)\n\nax = sns.heatmap(corr_matrix, annot = False, linewidth = 0.2, cmap='coolwarm')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\nplt.tight_layout()\nplt.savefig(path_analysis + 'MoA_correlation.png')\n\nplt.show() ","6f5f74d7":"We do not have so many mechanism of action correlated with another. This supports a strategy of one model per MoA.\\\nHowever, we still have some correlation, so it would be better to do only one model considering all MoA at the same time.\\\nThe difference is that one model per MoA can be easily done using LightGBM or any other model like a Logit, but a multi label model with one observations being able to assume more than one classification demands a fancier model like a Feedforward neural network.","f31afb2d":"As we can see, we have 772 gene expression variables (g), and 100 cell viability variables (c).\\\nThe 3 'cp' are cp_type, cp_time, and cp_dose.\n\ncp_type indicates if the person was in the treated group (cp_vehicle) or in the control group. Drugs in the control group (ctrl_vehicle) does not have any  Mechanism of Action.\n\ncp_time indicate the treatment duration  (24, 48, 72 hours).\n\ncp_dose indicates the dose (high or low, D1 or D2).\n\n\nIt would be very helpfull if we had information about how the dose and treatment duration affect the 'g' and 'c' variables and the mechanism of action. For example, given that we already have the 'g' and 'c' variables, does it really matter the dosage and the duration? Does a high dosage imply that the 'g' and 'c' variables will have a bigger impact in the pacient so the mechanism of action will be different.\\\nTo solve this problem we can design new variable, e. g., multiply all 'g' and 'c' variables by 2 when we have a high dosage, or make differents models by dosage and treatment duration.\nGiven that we do not even know if high dosage is D1, as it looks like by the way they described the variable, or D2, as one could infer that D1 mens one dosage and D2 means 2 dosages, it is preferable to do differents models. Nonetheless, we only have 24k observations, so if we won't have a good number of observations to make so many models.","9b7feb4a":"It is a good news that we don't have any missing values.\\\nOn the other hand, we don't have even 25 thousands of observations to work with.","a4e5c01c":"This shows us that it may be important to remove these observations for our training. \\\nIf we keep these observations, we only have more observations with no information or, even worst, a lot of observation with misleading informations since we do not know if the 'g' and 'c' variables represents a drug with no effect in the human body or the original drug.\\\nHowever, to have sure about our decision we must study the distributions of the variables in the control and treatment group.\\\nIf there are no change in the distribution of variables between control and treatment, it is crucial to remove the control group from our train dataset.","6c86476f":"We can see that the distribution of genes and cell variables are different for the control and treatment group, especially for the cell variables.\\\nLets repeat the same study considering the difference in the dosage variable,","38bd1f6c":"We can see that we have 206 differents classifications of mechanism of action.\nWe already know that one drug may have more than one MoA, but lets plot again the histogram of the number of mechanisms of action for each drug without clustering by variable.","5693fbc5":"Despite the correlation heatmap not showing high correlations between variables, insteady of using 772 'g' variables, we can use only 343 components and still explain a little more than 90% of our data variation.\\\nIf we acept a variation of 80% we can use only 166 components.\\\nIf we really want to cut the number of variables, we can use only 72 components and still explain 70% of the variation in the 'g' variables.","471cd91b":"Almost 8% of our observations are from the control group. \nWe should withdraw these observations from our database to train our model. However, we must put a flag in our model to classify all MoA as zero when classifying a control observation. ","51fab874":"For the genes variable we cannot see a significant difference in the distribution when considering the duration of the treatment. However, for the cell variables, there are a visible difference.","0f5eadbc":"Now, lets see the correlations.","1e0e3772":"The duration of each treatment is well distributed in our data.\nLets verify if treatments with longer durations presents more MoA.","eaded84f":"With the 'c' variables, we can use only 10 components and still explain more than 90% of the variation of the 'c' variables.","a07f1fc4":"On the other hand, for the 'c' variables, we can see that most variables (more than 85%) have a correlation higher than 80%.\\\nEven the least correlated variables have a correlation higher than 60%. Therefore, we can do a PCA to use less variables and increase the performance of our model.","65d21032":"There is a small change in the distribution of the genes and cell variables when considering the difference in the dosage. This may imply that the dosage variable is not so importante for our model.\\\nLets repeat the study considering the duration of the treatment.","7d649d45":"For now, to continue our data study and train our model, we should use \\\ntrain_features_without_control, and\\\ntrain_labels_without_control\n\nTo do the hyperparametrization and the holdout, we should use\\\nvalidation_features_without_control and validation_features_control, and\\\ntest_features_without_control and test_features_control","f56dbc89":"As we can see, even without the control group, about 35% of the drugs in our database does not have any mechanism of action.\nIt would be nice if we had access to more information about our database, or if we could talk with someone of the field to try to understand if is normal to have so many drugs without any MoA. \n\nWe have differents types of mechanisms of action, lets try to understand them a little.","f74b1caa":"From the box plot above, it is clear that the cell viability variables have a higher variability than the gene expression variables.\nLets compare the distribution of these genes and cell variables with the database with only control observations.","42c0a56b":"https:\/\/www.kaggle.com\/c\/lish-moa\/overview\n\nThe Connectivity Map, a project within the Broad Institute of MIT and Harvard, together with the Laboratory for Innovation Science at Harvard (LISH), presents this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.\n\nWhat is the Mechanism of Action (MoA) of a drug? And why is it important?\n\nIn the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\nHow do we determine the MoAs of a new drug?\n\nOne approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n\nIn this competition, you will have access to a unique dataset that combines gene expression and cell viability data. The data is based on a new technology that measures simultaneously (within the same samples) human cells\u2019 responses to drugs in a pool of 100 different cell types (thus solving the problem of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for more than 5,000 drugs in this dataset.\n\nAs is customary, the dataset has been split into testing and training subsets. Hence, your task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.\n\nHow to evaluate the accuracy of a solution?\n\nBased on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.\n\nIf successful, you\u2019ll help to develop an algorithm to predict a compound\u2019s MoA given its cellular signature, thus helping scientists advance the drug discovery process.","238bba42":"We can see that some variables have much more variance than others. Probably, these variables will be more important in the model. However, we do not have any variable without variance.\n\nLets repeat this study but separating the g variables from the c ones.","b7139420":"Again, it is a good news that we don't have any missing values, so we do not need to worry if a missing means no information or just another '0'.","4885b315":"Most of our mechanisms of action are inhibitors.\n\nConsidering that one drug can have more than one MoA, again it would be nice to talk with someone of the field to know, for example, if it is possible to one drug to be a inhibitor and an activator at the same time.\n\nLets now verify if our data base is unbalanced. Lets count how many drugs of each MoA we have.","67625497":"Our train and test file are in the same format with the same variables. There is no problem here.\n\nOur train_features and train_targets_scored are already in the same order, so we won't have any problem at modeling. We do not need to loose time joining the tables.\n\nWe can see that we have 206 different Mechanisms of Action. One approach is to run 206 different LightGBM.\\\nThe downside of this approach is time. However, in the 'lightgbm test' notebook, we could see that this methodology can be donne in less tha 9 hours (our limit to run this model in Kaggle's notebook). \n\nIf we have time, we will try a Feedforward neuron network also. ","2e62b1fe":"As we can see, the number of mechanisms of action does not change according the duration of the treatment.","65a4ca93":"We can notice that all groups of treatment duration and drug dosage have the same proportion in our database.\nThe segment (D1, 48) have a little more participation, but a difference of only 2% does not seen to be relevant enouth to treat it diferently from the other segments.\\\nIf we decide to do one model by dosage and duration, we will have only 2812 (17576*0.16) to work with in each model.\n\nAll the other variables(772 gene expression variables and 100 cell viability variables) are continuous variables and there isn't much to analize. We already know that there isn't any missing in our database, but we can also see if any of these variables is actually a constant (what would not be usefull for modeling).\\\nIt is also interisting to see the correlations among 'g' variables and the correlations amog the 'c' variables. If they are highly correlated, we can perform a principal component analysis. This will allow our model to be more efficint, especially considering the small number of observations that we have.","9136357e":"A drug can have from none to 7 different mechanisms of action.\nFor some reason, we do not have any observation with 6 MoA.\nAfter, when we train out model, we can considerer that drugs have a limited number of mechanisms of action and, for example, consider only the 10 most probables mechanisms.","37d1ee0a":"For the 'g' variables, we can see that they are not highly correlated. Less than 5% has a correlation higher than 0.5 or less than -0.5.\nThis imply that we won't be able to reduce very much the number of variables using PCA.","296cd54c":"Unfortunately, as we can see, most of our MoA has less than 4% of positive labels.\\\nThis unbalanced data may b a problem in modeling."}}