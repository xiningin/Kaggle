{"cell_type":{"1716e00e":"code","203bf192":"code","9eb7e743":"code","b58f04f8":"code","ddbc6dff":"code","25b700bd":"code","e0ba49e2":"code","3e880dec":"code","bc78c927":"code","aec57877":"code","e987ad59":"code","408234bf":"code","31a798ac":"code","79bd6b09":"code","0d64cc01":"code","0e49b70d":"code","9e29a880":"code","9dd3e560":"code","2480f57e":"code","705b061e":"code","a6c04b88":"code","5cf34e80":"code","365bef9a":"code","de039b7a":"code","7eafc6e2":"code","eeaf537e":"code","961fcf1d":"code","0b1619b7":"code","1a0128fc":"code","bad8e9d8":"code","59777aa6":"code","f471b904":"code","a16ba6cf":"code","7a45200c":"code","70b795cb":"code","5d5e7829":"code","08b070e9":"code","6d0bc161":"code","bc861e89":"code","e46b372c":"code","71e8e5df":"code","640ec3ad":"code","543cbea1":"code","647d181b":"code","127fba46":"code","39b159fb":"code","f08def2b":"code","18d932cf":"code","e2f53473":"code","fee18f47":"code","b94d9731":"code","0e67152c":"code","f095c381":"code","643ac018":"code","fc0aa854":"code","7b6a8040":"code","b3ee0421":"code","d2683396":"code","95a7b781":"code","0f7862b6":"code","4c9e2201":"code","d7e1b777":"code","1b7e0d1b":"code","4cc0e07a":"code","2d7ce01c":"code","906bb749":"code","ed9cb87d":"code","995f9213":"code","3153ee11":"code","4004adf2":"code","7583269f":"code","4cab154d":"code","b9869803":"code","b0f0a82c":"code","bf20ceb2":"code","905b45bf":"code","f59159e8":"code","87a616c6":"code","580624c5":"code","fe5b8091":"code","814802dc":"code","f14b445f":"code","e65cabe3":"code","5dcee3bf":"code","c2bc9b6f":"code","e8c76adc":"code","2aed11b3":"code","d4702264":"code","2a78823d":"code","0ef30af8":"code","731ce62f":"code","be5b4344":"code","9a9eda6b":"code","2f39b11d":"code","d38cb1b4":"code","94a5704d":"code","1d547a6c":"code","9466e740":"code","06b366e4":"code","f8330c48":"code","a08bf594":"code","59279c19":"code","a53f1025":"code","85d9222f":"code","49a25947":"code","dae80213":"code","9afb0ce8":"code","6510a771":"code","32389d2e":"code","0ba60c84":"code","2719d1fc":"code","dd92c79d":"code","f369fff6":"code","0a9302d0":"code","353029c1":"code","6a767f37":"code","9447a273":"code","48031907":"code","eb7c80f3":"code","0ddf2eba":"code","0cb35bb1":"code","0a67db1c":"code","652eaeef":"code","ec6ca8cf":"code","960c69b7":"code","9cb638b1":"code","4d5fc93c":"code","033dd9db":"code","51642b08":"code","6eeb715c":"code","dae2180c":"code","07cd411f":"code","82baab30":"code","d4fc6848":"code","932fb9d5":"code","685df0a7":"code","78c7fca9":"code","6dce683b":"code","a29184e1":"code","46f500f9":"code","8c955dfb":"code","22c4264d":"code","957f82b0":"code","388e3493":"code","1fb2d88e":"code","2fb22ad3":"code","048002e9":"code","88e815b9":"code","9aab4a79":"code","e03b7896":"code","68d3377c":"code","c214b912":"code","eb9ba94d":"code","91fab6b7":"code","4b9ccb80":"code","9aa17249":"code","c081f988":"code","5010d934":"code","2a8f7f97":"code","3c7fb6b4":"code","143f11e7":"code","6c86486e":"code","ca7e97b9":"code","07d51a29":"code","7f0ac7a9":"code","bd4b3e9f":"code","765d0d26":"code","c5bc7f43":"code","fdc4a02c":"code","63cf04ca":"code","986bbbe6":"code","26f07ad1":"code","95061b0c":"markdown","15dc7f35":"markdown","2205c218":"markdown","de50fa97":"markdown","99f98220":"markdown","27c2c7e3":"markdown","ce1e044a":"markdown","0f14681c":"markdown","909061bf":"markdown","f054d091":"markdown","8d0c4542":"markdown","ee3b20be":"markdown","143c85a9":"markdown","61217261":"markdown","15b5759a":"markdown","94f65a74":"markdown","0f7af18d":"markdown","f35bd142":"markdown","a8677300":"markdown","bc9378ac":"markdown","a850e2e7":"markdown","fbe27a41":"markdown","97ae9d6e":"markdown","adc17db7":"markdown","e84c2110":"markdown","2908029f":"markdown","5107a97c":"markdown","df84587a":"markdown","b378f1c7":"markdown","f9bc7b18":"markdown","965c830e":"markdown","5f61cc99":"markdown","940ddfe8":"markdown","7f08fb69":"markdown","b5c1c8c1":"markdown","6339af62":"markdown","069cc437":"markdown","e545a4f7":"markdown","b0a15b2a":"markdown"},"source":{"1716e00e":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","203bf192":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test =  pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_sample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_sample_submission.shape)","9eb7e743":"# df_train.head(2)\n# df_test.head(2)\n# df_sample_submission.head(2)","b58f04f8":"df_train.columns","ddbc6dff":"df_train.dtypes.value_counts()","25b700bd":"plt.figure(figsize=(12,8))\nsns.distplot(df_train['SalePrice'])","e0ba49e2":"# .corr() = compute pairwise correlation of columns, excluding NA\/null values\n# int64      35\n# float64     3\n\ncm = df_train.corr()\n\nplt.subplots(figsize=(12,8))\nsns.heatmap(cm)","3e880dec":"cm.nlargest(15, 'SalePrice')['SalePrice']","bc78c927":"f = plt.subplots(figsize=(12,6))\nsns.boxplot(x='OverallQual',y='SalePrice',data=df_train)","aec57877":"f = plt.subplots(figsize=(12,6))\nsns.scatterplot(x='GrLivArea',y='SalePrice',data=df_train)","e987ad59":"f = plt.subplots(figsize=(12,6))\nsns.boxplot(x='GarageCars',y='SalePrice',data=df_train)","408234bf":"f = plt.subplots(figsize=(12,6))\nsns.scatterplot(x='GarageArea',y='SalePrice',data=df_train)","31a798ac":"f = plt.subplots(figsize=(12,6))\nsns.scatterplot(x='TotalBsmtSF',y='SalePrice',data=df_train)","79bd6b09":"f = plt.subplots(figsize=(12,6))\nsns.scatterplot(x='1stFlrSF',y='SalePrice',data=df_train)","0d64cc01":"f = plt.subplots(figsize=(15,8))\nsns.boxplot(x='YearBuilt',y='SalePrice',data=df_train)","0e49b70d":"plt.figure(figsize=(10,8))\nsns.heatmap(df_train.isnull())","9e29a880":"# .isnull() gives back the same df but with booleans (True if NAN)\n# .sum() sums up all the True's per column \n\ndf_train.isnull().sum().sort_values(ascending=False).head(21)","9dd3e560":"percentage_missing = (df_train.isnull().sum())\/(1460)","2480f57e":"percentage_missing.sort_values(ascending=False).head(10)","705b061e":"type((df_train['LotFrontage'].iloc[1]))","a6c04b88":"sns.scatterplot(df_train['LotFrontage'],df_train['SalePrice'])","5cf34e80":"cm = df_train.corr()","365bef9a":"cm.nlargest(5, 'LotFrontage')['LotFrontage']","de039b7a":"sns.regplot(df_train['1stFlrSF'],df_train['LotFrontage'])","7eafc6e2":"from pylab import *\nfrom scipy import stats","eeaf537e":"the_x = df_train['1stFlrSF'].to_numpy()\nthe_y = df_train['LotFrontage'].to_numpy()\nmask = ~np.isnan(the_x) & ~np.isnan(the_y)\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(the_x[mask], the_y[mask])\n\nprint('slope = ' + str(slope))\nprint('intercept = '+ str(intercept))","961fcf1d":"def predict_LotFrontage(x):\n    return slope * x + intercept\n\nfitLine = predict_LotFrontage(the_x)\n\nplt.scatter(the_x,the_y)\nplt.plot(the_x, fitLine, c='r')\nplt.xlabel('1stFlrSF')\nplt.ylabel('LotFrontage')\nplt.show()","0b1619b7":"def replace_nans_LotFrontage(a):\n    \n    first_floor_sf = a[0]\n    lot_frontage = a[1]\n    \n    if pd.isnull(lot_frontage):\n        return predict_LotFrontage(first_floor_sf)    \n    else:\n        return lot_frontage","1a0128fc":"df_train['LotFrontage'] = df_train[['1stFlrSF','LotFrontage']].apply(replace_nans_LotFrontage,axis=1)\ndf_test['LotFrontage'] = df_test[['1stFlrSF','LotFrontage']].apply(replace_nans_LotFrontage,axis=1)","bad8e9d8":"df_train['LotFrontage'].isnull().sum()","59777aa6":"df_test['LotFrontage'].isnull().sum()","f471b904":"sns.regplot(df_train['1stFlrSF'],df_train['LotFrontage'])","a16ba6cf":"the_x = df_train['1stFlrSF'].to_numpy()\nthe_y = df_train['LotFrontage'].to_numpy()\nmask = ~np.isnan(the_x) & ~np.isnan(the_y)\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(the_x[mask], the_y[mask])\n\nprint('slope = ' + str(slope))\nprint('intercept = '+ str(intercept))","7a45200c":"percentage_missing = (df_train.isnull().sum())\/(1460)\npercentage_missing.sort_values(ascending=False).head(10)","70b795cb":"cm = df_train.corr()\ncm.nlargest(5, 'GarageYrBlt')['GarageYrBlt']","5d5e7829":"sns.scatterplot(df_train['YearBuilt'],df_train['GarageYrBlt'])","08b070e9":"sns.regplot(df_train['YearBuilt'],df_train['GarageYrBlt'])","6d0bc161":"the_x = df_train['YearBuilt'].to_numpy()\nthe_y = df_train['GarageYrBlt'].to_numpy()\nmask = ~np.isnan(the_x) & ~np.isnan(the_y)\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(the_x[mask], the_y[mask])\n\nprint('slope = ' + str(slope))\nprint('intercept = '+ str(intercept))","bc861e89":"def predict_GarageYrBlt(x):\n    return slope * x + intercept\n\nfitLine = predict_LotFrontage(the_x)\n\nplt.scatter(the_x,the_y)\nplt.plot(the_x, fitLine, c='r')\nplt.xlabel('YearBuilt')\nplt.ylabel('GarageYrBlt')\nplt.show()","e46b372c":"def replace_nans_GarageYrBlt(a):\n    \n    YearBuilt = a[0]\n    GarageYrBlt = a[1]\n    \n    if pd.isnull(GarageYrBlt):\n        return predict_GarageYrBlt(YearBuilt)    \n    else:\n        return GarageYrBlt","71e8e5df":"df_train['GarageYrBlt'] = df_train[['YearBuilt','GarageYrBlt']].apply(replace_nans_GarageYrBlt,axis=1)\ndf_test['GarageYrBlt'] = df_test[['YearBuilt','GarageYrBlt']].apply(replace_nans_GarageYrBlt,axis=1)","640ec3ad":"df_train['GarageYrBlt'].isnull().sum()","543cbea1":"df_test['GarageYrBlt'].isnull().sum()","647d181b":"the_x = df_train['YearBuilt'].to_numpy()\nthe_y = df_train['GarageYrBlt'].to_numpy()\nmask = ~np.isnan(the_x) & ~np.isnan(the_y)\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(the_x[mask], the_y[mask])\n\nprint('slope = ' + str(slope))\nprint('intercept = '+ str(intercept))","127fba46":"percentage_missing = (df_train.isnull().sum())\/(1460)\npercentage_missing.sort_values(ascending=False).head(10)","39b159fb":"cm = df_train.corr()\ncm.nlargest(15, 'SalePrice')['SalePrice']","f08def2b":"plt.figure(figsize=(10,8))\nsns.heatmap(df_train.isnull())","18d932cf":"still_missing = (df_train.isnull().sum())\/(1460)\nstill_missing[still_missing>0].sort_values(ascending=False)","e2f53473":"columns_missing_data = pd.DataFrame(still_missing[still_missing>0])\ncolumns_missing_data.index.name = 'predictor'","fee18f47":"print(df_train.shape)\nprint(df_test.shape)","b94d9731":"df_train = df_train.drop((columns_missing_data.index),axis=1)\ndf_test = df_test.drop((columns_missing_data.index),axis=1)","0e67152c":"print(df_train.shape)\nprint(df_test.shape)","f095c381":"plt.figure(figsize=(10,8))\nsns.heatmap(df_train.isnull())","643ac018":"df_train.isnull().sum().max()","fc0aa854":"plt.figure(figsize=(10,8))\nsns.heatmap(df_test.isnull())","7b6a8040":"df_test[df_test['MSZoning'].isnull()]","b3ee0421":"print(df_test['MSZoning'].value_counts())\nprint(df_test['MSZoning'].isnull().sum())","d2683396":"#Impute the values using scikit-learn SimpleImpute Class\n\nfrom sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer( strategy='most_frequent')\n\ntrial_array = df_test['MSZoning'].values.reshape(-1, 1)\n\n# Fit the imputer on all the MSZoning categorical values\nimp_mean.fit(trial_array)\n# Impute all missing values in the MSZoning column with the most_frequent categorical feature\ndf_test['MSZoning'] = imp_mean.transform(trial_array)","95a7b781":"print(df_test['MSZoning'].value_counts())\nprint(df_test['MSZoning'].isnull().sum())","0f7862b6":"#Do this for all the other categorical features\n\ncat_columns = ['Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType']\n\nfor column in cat_columns:\n    \n    # Fit the imputer \n    imp_mean.fit(df_test[str(column)].values.reshape(-1, 1))\n    # Impute all missing values with the most_frequent categorical feature in each respective column\n    df_test[str(column)] = imp_mean.transform(df_test[str(column)].values.reshape(-1, 1))","4c9e2201":"plt.figure(figsize=(10,8))\nsns.heatmap(df_test.isnull())","d7e1b777":"#Might as wel do the same for the numerical columns.. Since so few datapoints are missing..\n\nnum_columns = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']\n\nfor column in num_columns:\n    \n    # Fit the imputer \n    imp_mean.fit(df_test[str(column)].values.reshape(-1, 1))\n    # Impute all missing values the most_frequent value in each respective column\n    df_test[str(column)] = imp_mean.transform(df_test[str(column)].values.reshape(-1, 1))\n\n","1b7e0d1b":"plt.figure(figsize=(10,8))\nsns.heatmap(df_test.isnull())","4cc0e07a":"df_test.shape","2d7ce01c":"cm = df_train.corr()\ncm.nlargest(6,'SalePrice')['SalePrice']","906bb749":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15, 10))\nsns.scatterplot(df_train['OverallQual'],df_train['SalePrice'],ax=ax1)\nsns.distplot(df_train['OverallQual'],ax=ax2)\nsns.scatterplot(df_train['GrLivArea'],df_train['SalePrice'],ax=ax3)\nsns.distplot(df_train['GrLivArea'],ax=ax4)","ed9cb87d":"#decided to remove all 4 points>4000\n\ndf_train[df_train['GrLivArea']>4000]","995f9213":"df_train = df_train.drop(df_train[df_train['Id'] == 524].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 692].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 1183].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)","3153ee11":"df_train[df_train['GrLivArea']>4000]","4004adf2":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15, 10))\nsns.scatterplot(df_train['OverallQual'],df_train['SalePrice'],ax=ax1)\nsns.distplot(df_train['OverallQual'],ax=ax2)\nsns.scatterplot(df_train['GrLivArea'],df_train['SalePrice'],ax=ax3)\nsns.distplot(df_train['GrLivArea'],ax=ax4)","7583269f":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15, 10))\nsns.scatterplot(df_train['GarageCars'],df_train['SalePrice'],ax=ax1)\nsns.distplot(df_train['GarageCars'],ax=ax2)\nsns.scatterplot(df_train['GarageArea'],df_train['SalePrice'],ax=ax3)\nsns.distplot(df_train['GarageArea'],ax=ax4)\n","4cab154d":"df_train[df_train['GarageArea']>1220]","b9869803":"df_train = df_train.drop(df_train[df_train['Id'] == 582].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 1062].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 1191].index)","b0f0a82c":"df_train[df_train['GarageArea']>1220]","bf20ceb2":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15, 10))\nsns.scatterplot(df_train['GarageCars'],df_train['SalePrice'],ax=ax1)\nsns.distplot(df_train['GarageCars'],ax=ax2)\nsns.scatterplot(df_train['GarageArea'],df_train['SalePrice'],ax=ax3)\nsns.distplot(df_train['GarageArea'],ax=ax4)\n","905b45bf":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (15, 5))\nsns.scatterplot(df_train['TotalBsmtSF'],df_train['SalePrice'],ax=ax1)\nsns.distplot(df_train['TotalBsmtSF'],ax=ax2)","f59159e8":"df_train[df_train['TotalBsmtSF']>2500]","87a616c6":"df_train = df_train.drop(df_train[df_train['Id']==333].index)\ndf_train = df_train.drop(df_train[df_train['Id']==441].index)\ndf_train = df_train.drop(df_train[df_train['Id']==497].index)\ndf_train = df_train.drop(df_train[df_train['Id']==1045].index)\ndf_train = df_train.drop(df_train[df_train['Id']==1374].index)","580624c5":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (15, 5))\nsns.scatterplot(df_train['TotalBsmtSF'],df_train['SalePrice'],ax=ax1)\nsns.distplot(df_train['TotalBsmtSF'],ax=ax2)","fe5b8091":"df_train.dtypes.value_counts()","814802dc":"df_train.columns.to_series().groupby(df_train.dtypes).groups","f14b445f":"df_test.columns.to_series().groupby(df_test.dtypes).groups","e65cabe3":"print(df_train.isnull().sum().max())\nprint(df_test.isnull().sum().max())","5dcee3bf":"print(df_train.shape)\nprint(df_test.shape)","c2bc9b6f":"all_data = pd.concat((df_train,df_test))\n\nfor column in all_data.select_dtypes(include=[np.object]).columns:\n\n    print(column, all_data[column].unique())","e8c76adc":"from pandas.api.types import CategoricalDtype","2aed11b3":"# The following was not working:\n\n    # all_data = pd.concat((df_train,df_test))\n\n    # for column in all_data.select_dtypes(include=[np.object]).columns:\n    #     df_train[column] = df_train[column].astype('category', categories = all_data[column].unique())\n    #     df_test[column] = df_test[column].astype('category', categories = all_data[column].unique())\n\n# Workaround: https:\/\/stackoverflow.com\/questions\/37952128\/pandas-astype-categories-not-working","d4702264":"all_data = pd.concat((df_train,df_test))\n\nfor column in all_data.select_dtypes(include=[np.object]).columns:\n    df_train[column] = df_train[column].astype(CategoricalDtype(categories = all_data[column].unique()))\n    df_test[column] = df_test[column].astype(CategoricalDtype(categories = all_data[column].unique()))","2a78823d":"df_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test) ","0ef30af8":"print(df_train.shape)\nprint(df_test.shape)","731ce62f":"X = df_train.drop('SalePrice',axis=1)\ny = df_train['SalePrice']","be5b4344":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)","9a9eda6b":"from sklearn.linear_model import LinearRegression\nlrm = LinearRegression(normalize=False)\nlrm.fit(X_train,y_train)","2f39b11d":"coeff_df = pd.DataFrame(lrm.coef_,X.columns,columns=['Coefficient'])\ncoeff_df.sort_values(by='Coefficient', ascending=False)\n\n#Interpreting the coefficients:\n#Holding all other features fixed, a 1 unit increase in a certain predictor is associated with an increase\/decrease of 'Coefficient' dollar","d38cb1b4":"predictions = lrm.predict(X_test)","94a5704d":"plt.figure(figsize=(10,8))\n\n#predictions\nax = sns.scatterplot(y_test,predictions)\nax.set(xlabel=\"y_test\", ylabel = \"predictions\")\n\n#perfect predictions\nplt.plot(y_test,y_test,'-r')","1d547a6c":"plt.figure(figsize=(10,8))\nsns.distplot((y_test-predictions))\nplt.xlabel(\"Prediction Error [Dollar]\")\n_ = plt.ylabel(\"Count\")\n","9466e740":"residuals_sklearn = (y_test-predictions)","06b366e4":"print('1\u03c3 sklearn = '+str(np.std(residuals_sklearn)))\nprint('2\u03c3 sklearn = '+str(2*np.std(residuals_sklearn)))\nprint('mean sklearn  = '+str(np.mean(residuals_sklearn)))\nprint('median sklearn = '+str(np.median(residuals_sklearn)))","f8330c48":"from sklearn import metrics","a08bf594":"mae_sklearn =metrics.mean_absolute_error(y_test,predictions)\nmse_sklearn =metrics.mean_squared_error(y_test,predictions) \nrmse_sklearn =np.sqrt(metrics.mean_squared_error(y_test,predictions))","59279c19":"print('MAE sklearn:',mae_sklearn)\nprint('MSE sklearn:',mse_sklearn)\nprint('RMSE sklearn:',rmse_sklearn)","a53f1025":"predictions_scikit = lrm.predict(df_test)","85d9222f":"predictions_scikit","49a25947":"print(type(predictions_scikit))","dae80213":"predictions_scikit.shape","9afb0ce8":"Ids = df_test['Id'].values.reshape(1459,1)","6510a771":"Ids_scikit = df_test['Id'].values.reshape(1459,1)\nsubmission_scikit = pd.DataFrame(data={'Id':Ids[0:,0], 'SalePrice':predictions_scikit[0:]}, index=Ids_scikit[0:,0])","32389d2e":"submission_scikit","0ba60c84":"submission_scikit.to_csv(r'submission_sklearn_house_prices.csv')","2719d1fc":"plt.figure(figsize=(15,6))\nsns.scatterplot(x=submission_scikit['Id'],y=submission_scikit['SalePrice'])","dd92c79d":"X = df_train.drop('SalePrice',axis=1).values #.values because TF may complain. TF can't work with Pandas series or DF. It passes a numeric array.  \ny = df_train['SalePrice'].values","f369fff6":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=101)","0a9302d0":"from sklearn.preprocessing import MinMaxScaler\n\n# Transform features by scaling each feature to a given range.\n# This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n# The transformation is given by:\n# X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\n# X_scaled = X_std * (max - min) + min\n\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train) #fit and transform in one step possible with MinMaxScaler\n# fit = Compute the minimum and maximum to be used for later scaling.\n# transform = Scale features of X according to feature_range (default feature_range=(0, 1))\n\nX_test = scaler.transform(X_test) #don't fit, because we don't want to assume prior info about the testset","353029c1":"print(X_train.shape)\nprint(X_test.shape)","6a767f37":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam","9447a273":"model_all_data = Sequential()\nmodel_all_data.add(Dense(217,activation='relu')) # (217 features in training data)\nmodel_all_data.add(Dense(109,activation='relu'))\nmodel_all_data.add(Dense(50,activation='relu'))\nmodel_all_data.add(Dense(25,activation='relu'))\nmodel_all_data.add(Dense(12,activation='relu'))\nmodel_all_data.add(Dense(1)) #output neuron = predicted price\nmodel_all_data.compile(optimizer='adam',loss='mse')\nmodel_all_data.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=128,epochs=300) \n\n#validation data:\n#after each epoch we will check our loss on the testdata \n#like this we can see how well the model perform on our test data and training data\n#weights and biases are not affected by the test data! (Keras is not going to update your model based on test data)\n#In this way we can see if we are overfitting at some point (when validation loss starts to increase)\n\n\n#batch size:\n#The smaller the batch size, the longer the training is going to take, but the less likely you are going to overfit to the data.\n#Instead you are passing in these small batches with all different predictor situations.\n\n#epoch:\n#an arbitrary cutoff, generally defined as \"one pass over the entire dataset\",\n#used to separate training into distinct phases, which is useful for logging and periodic evaluation.","48031907":"#Compare training vs. test performance\nlosses = pd.DataFrame(model_all_data.history.history)\nlosses.plot(figsize=(12,6))\nplt.xlabel(\"Epoch\")\n_ = plt.ylabel(\"Loss\")","eb7c80f3":"from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\npredictions = model_all_data.predict(X_test)\n\nplt.figure(figsize=[12,8])\n# The predictions\nplt.scatter(y_test,predictions)\nplt.xlabel(\"y_test\")\n_ = plt.ylabel(\"Predictions\")\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r') ","0ddf2eba":"mae_tf =metrics.mean_absolute_error(y_test,predictions)\nmse_tf =metrics.mean_squared_error(y_test,predictions) \nrmse_tf =np.sqrt(metrics.mean_squared_error(y_test,predictions))","0cb35bb1":"print('MAE tf:',mae_tf)\nprint('MSE tf:',mse_tf)\nprint('RMSE tf:',rmse_tf )","0a67db1c":"print('MAE improvement of tf wrt sklearn: '+str(mae_sklearn-mae_tf))\nprint('MSE improvement of tf wrt sklearn: '+str(mse_sklearn-mse_tf))\nprint('RMSE improvement of tf wrt sklearn: '+str(rmse_sklearn-rmse_tf))","652eaeef":"print('TF is: ~'+str(rmse_sklearn-rmse_tf) +' dollars less wrong in predicting')","ec6ca8cf":"#Explained variance regression score function\n#Best possible score is 1.0, lower values are worse.\n\nexplained_variance_score(y_test,predictions)","960c69b7":"plt.figure(figsize=(10,8))\nsns.distplot((y_test-predictions))\nplt.xlabel(\"Prediction Error [dollar]\")\n_ = plt.ylabel(\"Count\")\n","9cb638b1":"residuals = y_test-predictions","4d5fc93c":"print('1\u03c3 tf = '+str(np.std(residuals)))\nprint('2\u03c3 tf = '+str(2*np.std(residuals)))\nprint('mean tf = '+str(np.mean(residuals)))\nprint('median tf = '+str(np.median(residuals)))","033dd9db":"print('1\u03c3 sklearn = '+str(np.std(residuals_sklearn)))\nprint('2\u03c3 sklearn = '+str(2*np.std(residuals_sklearn)))\nprint('mean sklearn  = '+str(np.mean(residuals_sklearn)))\nprint('median sklearn = '+str(np.median(residuals_sklearn)))","51642b08":"test_df = df_train.drop('SalePrice',axis=1)","6eeb715c":"test_df.iloc[0]","dae2180c":"single_house = test_df.iloc[0]","07cd411f":"single_house = scaler.transform(single_house.values.reshape(-1,217))","82baab30":"model_all_data.predict(single_house)","d4fc6848":"df_train['SalePrice'].head(1)","932fb9d5":"print('Prediction is: ' +str((model_all_data.predict(single_house))-(df_train['SalePrice'].iloc[0]))+' dollar off from the real sale price')","685df0a7":"scaler = MinMaxScaler()\ndf_test_final = scaler.fit_transform(df_test) ","78c7fca9":"predictions_all_data_df_test = model_all_data.predict(df_test_final)","6dce683b":"submission_all_data_df_test = pd.DataFrame(data={'Id':Ids[0:,0], 'SalePrice':predictions_all_data_df_test[0:,0]}, index=Ids[0:,0])","a29184e1":"plt.figure(figsize=(15,6))\nsns.scatterplot(x=submission_all_data_df_test['Id'],y=submission_all_data_df_test['SalePrice'],color='g')\n","46f500f9":"submission_all_data_df_test.to_csv(r'submission_all_data_TF_house_prices.csv')","8c955dfb":"plt.figure(figsize=(10,8))\nsns.distplot(df_train['SalePrice'])","22c4264d":"df_train_reduced = df_train[df_train['SalePrice']<350000]","957f82b0":"df_train_reduced.shape","388e3493":"X = df_train_reduced.drop('SalePrice',axis=1).values #.values because TF may complain. TF can't work with Pandas series or DF. It passes a numeric array.  \ny = df_train_reduced['SalePrice'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=101)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train) \n\nX_test = scaler.transform(X_test) #don't fit, because we don't want to assume prior info about the testset\n\nprint(X_train.shape)\nprint(X_test.shape)","1fb2d88e":"model = Sequential()\nmodel.add(Dense(217,activation='relu')) # (217 features in training data)\nmodel.add(Dense(109,activation='relu'))\nmodel.add(Dense(50,activation='relu'))\nmodel.add(Dense(25,activation='relu'))\nmodel.add(Dense(12,activation='relu'))\nmodel.add(Dense(1)) #output neuron = predicted price\nmodel.compile(optimizer='adam',loss='mse')\nmodel.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=128,epochs=300) \n\n","2fb22ad3":"#Compare training vs. test performance\nlosses = pd.DataFrame(model.history.history)\nlosses.plot(figsize=(12,6))\nplt.xlabel(\"Epoch\")\n_ = plt.ylabel(\"Loss\")","048002e9":"predictions = model.predict(X_test)","88e815b9":"plt.figure(figsize=[12,8])\n# The predictions\nplt.scatter(y_test,predictions)\nplt.xlabel(\"y_test\")\n_ = plt.ylabel(\"Predictions\")\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r') ","9aab4a79":"explained_variance_score(y_test,predictions)","e03b7896":"residuals_tf_ted = y_test-predictions","68d3377c":"print('1\u03c3 tf red = '+str(np.std(residuals_tf_ted)))\nprint('2\u03c3 tf red = '+str(2*np.std(residuals_tf_ted)))\nprint('mean tf red = '+str(np.mean(residuals_tf_ted)))\nprint('median tf red = '+str(np.median(residuals_tf_ted)))","c214b912":"plt.figure(figsize=(10,8))\nsns.distplot((y_test-predictions))\nplt.xlabel(\"Prediction Error [dollar]\")\n_ = plt.ylabel(\"Count\")","eb9ba94d":"mae_tf_red =metrics.mean_absolute_error(y_test,predictions)\nmse_tf_red =metrics.mean_squared_error(y_test,predictions) \nrmse_tf_red =np.sqrt(metrics.mean_squared_error(y_test,predictions))","91fab6b7":"print('MAE tf red:',mae_tf_red)\nprint('MSE tf red:',mse_tf_red)\nprint('RMSE tf red:',rmse_tf_red )","4b9ccb80":"# RMSE sklearn: 28351.633756075065\n# RMSE tf: 23625.95715406359\n# RMSE tf reduced: 20810.27072576839","9aa17249":"df_test.shape","c081f988":"df_test['Id']","5010d934":"df_test.head()","2a8f7f97":"scaler = MinMaxScaler()\ndf_test_final = scaler.fit_transform(df_test) ","3c7fb6b4":"final_predictions = model.predict(df_test_final)","143f11e7":"print(type(final_predictions))\nprint(final_predictions.shape)","6c86486e":"Ids = df_test['Id'].values.reshape(1459,1)","ca7e97b9":"print(type(Ids))\nprint(Ids.shape)","07d51a29":"submission = pd.DataFrame(data={'Id':Ids[0:,0], 'SalePrice':final_predictions[0:,0]}, index=Ids[0:,0])","7f0ac7a9":"import os","bd4b3e9f":"os.getcwd()","765d0d26":"submission","c5bc7f43":"submission.to_csv(r'submission_house_prices.csv')","fdc4a02c":"plt.figure(figsize=(15,6))\nsns.scatterplot(x=submission['Id'],y=submission['SalePrice'])","63cf04ca":"plt.figure(figsize=(15,6))\nsns.scatterplot(x=submission_scikit['Id'],y=submission_scikit['SalePrice']) #sklearn\nsns.scatterplot(x=submission['Id'],y=submission['SalePrice'],color='r') #TF","986bbbe6":"plt.figure(figsize=(15,6))\nsns.scatterplot(x=submission_all_data_df_test['Id'],y=submission_all_data_df_test['SalePrice'],color='g')\nsns.scatterplot(x=submission['Id'],y=submission['SalePrice'],color='r')","26f07ad1":"# np.sqrt(np.mean((y-y_pred)**2))","95061b0c":"# Convert categorical variable into dummy\/indicator variables","15dc7f35":"2 negative predictions and more scattered predictions from 250.000 dollar onwards.","2205c218":"The straight line makes sense for people building the garage at the same time when building the house <br>\nPoints left from the straight line is when people build the garage afterwards <br><br>\nSome samples where the garage was build before the house?<br>\nDid they start with the garage and finish the house some time later?<br>\nOr is it just an error in registering?\n","de50fa97":"**Sample size **\n\nn_training = 1460<br>\nn_test = 1459<br>\n\n**Predictors**\n\np_training = 81<br>\np_test = 80 (without sales_price)","99f98220":"27 categorical variables","27c2c7e3":"1\u03c3 = 68% of the area of a normal distribution<br> 2\u03c3 =Approximately 95% of the area of a normal distribution ","ce1e044a":"## To be scaled first!","0f14681c":"# Train model on y_train SalePrice < 350000 dollar and see if RMSE improves on test data","909061bf":"## Keras model (all data)","f054d091":"Different results on train and test df for pd.get_dummies?!\n\ndf_train = pd.get_dummies(df_train) gives (1448, 218)<br>\ndf_test = pd.get_dummies(df_test) gives (1447, 203)<br>\n\nFound this article: https:\/\/dzone.com\/articles\/pandasscikit-learn-get-dummies-testtrain-sets<br>\n\nBasically I need to change the object Dtype to categorical.\n\n","8d0c4542":"## TotalBsmtSF\nRemove points > 2500","ee3b20be":"Remove predictors with remaining missing data from both train and test df","143c85a9":"## Predictions","61217261":"# TF\/Keras","15b5759a":"Plot both again ","94f65a74":"Slope and intercept before filling NAN's<br>\n-slope = 0.028743868469171666<br>\n-intercept = 36.75196977580098<br>\n\nSlope and intercept barely changed due to adding 17.7% missing data. <br>","0f7af18d":"1. If it is obvious that the outlier is due to incorrectly entered or measured data, you should drop the outlier\n2. If the outlier does not change the results but does affect assumptions, you may drop the outlier.  \n3. More commonly, the outlier affects both results and assumptions. \n   In this situation, it is not legitimate to simply drop the outlier.  \n   You may run the analysis both with and without it, but you should state how the results changed accordingly.\n4. If the outlier creates a significant association, you should drop the outlier and should not report any significance from your analysis.\n\nSource: https:\/\/www.theanalysisfactor.com\/outliers-to-drop-or-not-to-drop\/\n","f35bd142":"# Comparison","a8677300":"Kind of linear..<br>\nSo seems usefull.<br>\nWith what does it correlate? ","bc9378ac":"# Sales price distribution","a850e2e7":"## Predictions on test data with sklearn-linear regression","fbe27a41":"Which of the still missing vaues has a high correlation with SalePrice?\n\nPoolQC = no         \nMiscFeature = no     \nAlley= no            \nFence = no <br>\nFireplaceQu = no     \nGarageQual = no      \nGarageCond = no      \nGarageFinish = no  \nGarageType = no      \nBsmtExposure = no   \n\n\n","97ae9d6e":"\ntrain.csv - the training set <br>\ntest.csv - the test set <br>\ndata_description.txt - full description of each column<br>\nsample_submission.csv - a benchmark submission (LR on year and month of sale, lot square footage, and number of bedrooms)\n","adc17db7":"Slope and intercept before filling NAN's<br>\n-slope = 0.6938592237258078<br>\n-intercept = 609.5505956613179\n\nSlope and intercept barely changed due to adding 5% missing data. <br>\n    ","e84c2110":"# **Predictions on test data**","2908029f":"## OverallQual \n1. Two outliers for quality = 10 (are the outliers that are removed for GrLivArea)\n\n## GrLivArea:\n1. Two outliers in the right bottom corner. \n2. Positive skewness.<br>\n3. How could I interpret these two outliers? Maybe a large farm? I want to understand more about the SalePrice of a \"typical American house\", filtering out farms seems like a legitimate thing to do.","5107a97c":"### TTS and scaling","df84587a":"# **Missing data**","b378f1c7":"# Sklearn - linear regression ","f9bc7b18":"## TTS","965c830e":"Correlation LotFrontage-SalePrice?<br>","5f61cc99":"# Outliers: To Drop or Not to Drop","940ddfe8":"Submission File Format (1459)\n\n- The file should contain a header and have the following format:\n\n- Id,SalePrice\n- 1461,169000.1\n- 1462,187724.1233\n- 1463,175221\n- etc.","7f08fb69":"## GarageCars\nDecrease in saleprice for garagecars = 4 might be again due to a farmhouse\n\n## GarageArea\nRemove >1220","b5c1c8c1":"LinearRegression fits a linear model with coefficients b0 and b1 to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n\n![image.png](attachment:image.png)","6339af62":"I could have a look at ALL the independent predictors\/variables and look for outliers.<br>\nBut let's limit it down to the top 5 correlated (with SalePrice) independent predictors\/variables.","069cc437":"[](http:\/\/)**Rule of thumb**: >15% of data is missing = remove predictor.","e545a4f7":"## Model evalutation","b0a15b2a":"## Creating and Training the Model"}}