{"cell_type":{"6fe33dfc":"code","7b31073e":"code","99ec5622":"code","af09a30a":"code","99522857":"code","d2140920":"code","5edc5d54":"code","43fba37d":"code","d42edfc1":"code","8cf5f8de":"markdown","b9bb00c3":"markdown","9210263d":"markdown","13908dcd":"markdown","428d50ac":"markdown","9feb772f":"markdown"},"source":{"6fe33dfc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b31073e":"! pip install git+https:\/\/github.com\/openai\/CLIP.git\n! pip install imutils","99ec5622":"! nvidia-smi","af09a30a":"import torch\nimport clip\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport glob, random\nimport imutils, os, time\nimport matplotlib.pyplot as plt","99522857":"def get_ramdom_path():\n        file_path = '\/kaggle\/input\/animal-image-datasetdog-cat-and-panda\/animals\/'\n\n        first_path = os.listdir(file_path + 'animals\/')\n        class_path = random.choice(first_path)\n        if class_path != 'animals' and 'images':\n            rfile = os.path.join(file_path, class_path)\n            img_list = os.listdir(rfile)\n            sel_img = random.choice(img_list)\n            rfile= rfile+'\/'+ sel_img\n            return class_path, rfile","d2140920":"# plot images\nn = 5\nplt.figure(figsize= (20,10))\n\nfor i in range(n):\n    ax = plt.subplot(2, n, i+1)\n    _, img = get_ramdom_path()\n    img = cv2.imread(img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    ax = plt.subplot(2, n, i+1+n)\n    _, img = get_ramdom_path()\n    img = cv2.imread(img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()","5edc5d54":"def clip_predict_vit(rfile):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        model_use = \"ViT-B\/32\"\n        # model_use = \"RN50x16\" # ViT-B\/32, RN50x16\n        model, preprocess = clip.load(model_use, device=device)\n        image = preprocess(Image.open(rfile)).unsqueeze(0).to(device)\n\n        class_list = ['cats', 'dogs', 'panda']\n\n        start_time = time.time()\n        text = clip.tokenize(class_list).to(device)\n\n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            text_features = model.encode_text(text)\n\n            logits_per_image, logits_per_text = model(image, text)\n            probs = np.array(logits_per_image.softmax(dim=-1).cpu().numpy())\n\n        label = np.argmax(probs[0])\n        end_time = time.time()\n        #print(probs[0][label])\n        print(\"ViT inference time :\", end_time-start_time,\"seconds\")\n\n        return class_list[label], rfile\n    \ndef clip_predict_res(rfile):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        model_use = \"RN50x16\"\n        # model_use = \"RN50x16\" # ViT-B\/32, RN50x16\n        model, preprocess = clip.load(model_use, device=device)\n        image = preprocess(Image.open(rfile)).unsqueeze(0).to(device)\n\n        class_list = ['cats', 'dogs', 'panda']\n\n        start_time = time.time()\n        text = clip.tokenize(class_list).to(device)\n\n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            text_features = model.encode_text(text)\n\n            logits_per_image, logits_per_text = model(image, text)\n            probs = np.array(logits_per_image.softmax(dim=-1).cpu().numpy())\n\n        label = np.argmax(probs[0])\n        end_time = time.time()\n        #print(probs[0][label])\n        print(\"ResNet inference time :\", end_time-start_time,\"seconds\")\n\n        return class_list[label], rfile","43fba37d":"if __name__ == \"__main__\":\n    vit = 0\n    res = 0\n\n    for i in range(50):\n        gt, rfile = get_ramdom_path()\n        \n        print('stage: ', i)\n        print(\"Ground Truth :\", gt)\n        \n        predict_vit, rfile_vit = clip_predict_vit(rfile)\n        print(\"Predict Class ViT : \", predict_vit)\n        \n        predict_res, rfile_res = clip_predict_res(rfile)\n        print(\"Predict Class ResNet : \", predict_res, \"\\n\")\n\n        if gt == predict_vit:\n            vit+=1\n        if gt == predict_res:\n            res+=1","d42edfc1":"print(\"ViT-B\/32 Accurcy:\", vit\/50)\nprint(\"ResNet50 Accurcy:\", res\/50)","8cf5f8de":"## We will be using OpenAI's CLIP Model for this classification task","b9bb00c3":"<img src= \"https:\/\/syncedreview.com\/wp-content\/uploads\/2021\/05\/openai-cover.png\" alt =\"openai\" style='width: 500px;'>","9210263d":"<img src= \"https:\/\/miro.medium.com\/max\/3662\/1*tg7akErlMSyCLQxrMtQIYw.png\" alt =\"clip_Arch\" style='width: 1200px;'> ","13908dcd":"### Wonderful performance! thx to OpenAI Team :)","428d50ac":"#### In this work, we will test ViT, ResNet Image Encoder's performance each and it's inference time.","9feb772f":"## Model Archetecture\n#### Learn over 400 million images and text related to those images by crawling on the Internet.\n#### Due to these characteristics, we can use CLIP as a zero-shot prediction model.\n#### OpenAI provides image encoders of both ViT and ResNet."}}