{"cell_type":{"4409b2c7":"code","dea112fa":"code","3ce01226":"code","8a860923":"code","6a668324":"code","8b4779fa":"code","acf19485":"code","eeed5bed":"code","e6a8fe6e":"code","4ae614dd":"code","cde49cb2":"code","61acc7a7":"code","9eb258bd":"code","a826cf09":"code","58b998b1":"code","d6975ac9":"code","91a4ee23":"code","6fbe87d6":"code","4fda04f8":"code","a9e95e24":"code","128c613f":"code","7525d467":"code","9aab3eb9":"code","54a9091f":"code","6de5bd2d":"code","e02e5fa0":"code","326cca49":"code","71e5721d":"code","a543cbef":"code","40f7c413":"code","a1ad10ac":"code","1cac68a6":"code","e22fb98e":"code","a9f6e69f":"code","52251dfe":"code","3140f985":"markdown","0b1692e1":"markdown","e0d56650":"markdown","ad2613f0":"markdown","4321e2cc":"markdown","881fc26f":"markdown","b6b67d94":"markdown","9b612ff3":"markdown","68e97b44":"markdown","94517e2c":"markdown","168aed9f":"markdown"},"source":{"4409b2c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dea112fa":"import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.cbook as cbook\nimport datetime","3ce01226":"ufo_data = pd.read_csv('..\/input\/ufo-and-elections\/merged_ufo_elect_data.csv', low_memory=False)","8a860923":"filt_objs = []\nfor i, obj in ufo_data.iterrows():\n    if pd.notna(obj[\"year\"]) and pd.notna(obj[\"totalvotes\"]) and pd.notna(obj[\"state_x\"]):\n        filt_objs.append(obj)","6a668324":"df = pd.DataFrame(filt_objs)","8b4779fa":"states = set()\nfor i, obj in df.iterrows():\n    states.add(obj[\"state_x\"])\n    \nyears = set()\nfor i, obj in df.iterrows():\n    years.add(obj[\"year\"])","acf19485":"year_sums = []\nyears = sorted(list(years))\nfor year in years:\n    year_sum = 0\n    state_dic = set()\n    for i, obj in df.iterrows():\n        if int(obj[\"year\"]) == int(year):\n            if obj[\"state_x\"] not in state_dic and pd.notna(obj[\"totalvotes\"]):\n                year_sum += int(obj[\"totalvotes\"])\n                state_dic.add(obj[\"state_x\"])\n                \n    year_sums.append(year_sum)","eeed5bed":"year_counts = {}\n\nsightings = []\n\nfor i, obj in ufo_data.iterrows():\n    if pd.notna(obj[\"date\"]):\n        sightings.append(obj)\n\naccounts = pd.DataFrame(sightings)","e6a8fe6e":"for i, obj in accounts.iterrows():\n    if obj[\"year\"] not in year_counts.keys():\n        year_counts[obj[\"year\"]] = 1\n    else:\n        year_counts[obj[\"year\"]] += 1","4ae614dd":"tups = sorted(year_counts.items()) \n# print(tups)\nx, y = zip(*tups)\nplt.rcParams[\"figure.figsize\"] = (18,7)\nplt.xlabel(\"frequency of sightings by last election year\", fontsize=20)\nplt.bar(x, y, color=[\"green\", \"blue\"], alpha=.8, width=1)","cde49cb2":"# data from https:\/\/en.wikipedia.org\/wiki\/Voter_turnout_in_the_United_States_presidential_elections#:~:text=Note%3A%20The%20Bipartisan%20Policy%20Center,62.3%25%3B%20and%202012%2057.5%25.\nturnout = [ (1944,  48026000), (1948,  48834000), (1952,61552000),\n           (1956,62027000), (1960, 68836000), (1964,70098000), (1968, 73027000),\n           (1972, 77625000), (1976, 81603000), (1980, 86497000),(1984, 92655000),\n           (1988, 91587000), (1992,104600000), (1996,96390000), (2000, 105594000),\n           (2004, 122349000), (2008, 131407000), (2012, 129235000), (2016, 138847000)\n          ]\n\nyear_turn, num_turn = zip(*turnout)","61acc7a7":"plt.rcParams[\"figure.figsize\"] = (14,8)\nplt.xlabel(\"election turnout by year\", fontsize=20)\nplt.bar(year_turn, num_turn, color=[\"red\", \"white\", \"blue\"], width=3, edgecolor=\"red\", alpha=.7)","9eb258bd":"visits = []\nfor tup in tups:\n    if tup[0] in year_turn:\n        visits.append(tup[1])","a826cf09":"elect = []\ni = 0\nwhile i < len(visits):\n    row = [visits[i], year_turn[i], num_turn[i]]\n    elect.append(row)\n    i += 1\n    \nelect_df = pd.DataFrame(elect, columns=[\"visits\", \"year\", \"turnout_total\"])","58b998b1":"elect_df","d6975ac9":"corelation = elect_df.corr()\nprint(corelation)","91a4ee23":"shape_counts = {} # cleaning out junk\nfor i, obj in accounts.iterrows():\n    if obj[\"shape\"] != \"other\" and obj[\"shape\"] != \"nan\" and obj[\"shape\"] != \"unknown\" and obj[\"shape\"] != \"changing\" and type(obj[\"shape\"]) == str:\n        if obj[\"shape\"] not in shape_counts.keys():\n            shape_counts[obj[\"shape\"]] = 1\n        else:\n            shape_counts[obj[\"shape\"]] += 1\n\n# print(shape_counts)\nshape_x = list(shape_counts.keys())\nshape_y = list(shape_counts.values())\nshape_x = [str(shape) for shape in shape_x]\n# print(shape_y)\n# print(shape_x)\nplt.rcParams[\"figure.figsize\"] = (22,6)\nplt.xticks(rotation=90, fontsize=32)\nplt.ylabel(\"sightings\", fontsize=28)\nplt.bar(shape_x, shape_y, alpha=.7)\nplt.savefig(\"sightings_shape_hist.png\")","6fbe87d6":"circular = [\"circle\", \"sphere\", \"egg\", \"oval\", \"round\", \"disk\"]\ntriangular_pointed = [\"triangle\", \"delta\", \"diamond\", \"pyramid\", \"chevron\"]\nlight_based = [\"light\", \"fireball\", \"flash\", \"flare\"]\n\ndef shape_map(shape): # grouping like shapes \n    if shape in circular:\n        return \"circular\"\n    elif shape in triangular_pointed:\n        return \"triangular\/pointed\"\n    elif shape in light_based:\n        return \"light based\"\n    else:\n        return shape\n    \n\nshape_year = {}\nfor i, obj in accounts.iterrows():\n    if obj[\"shape\"] != \"other\" and obj[\"shape\"] != \"nan\" and obj[\"shape\"] != \"unknown\" and obj[\"shape\"] != \"changing\" and type(obj[\"shape\"]) == str:\n        if obj[\"year\"] not in shape_year.keys():\n            shape_year[int(obj[\"year\"])] = {shape_map(obj[\"shape\"]) : 1}\n        elif shape_map(obj[\"shape\"]) not in shape_year[int(obj[\"year\"])].keys():\n            shape_year[int(obj[\"year\"])][shape_map(obj[\"shape\"])] = 1\n        else:\n            shape_year[int(obj[\"year\"])][shape_map(obj[\"shape\"])] += 1\n\n\nshape_counts_grouped = {}\nfor i, obj in accounts.iterrows():\n    if obj[\"shape\"] != \"other\" and obj[\"shape\"] != \"nan\" and obj[\"shape\"] != \"unknown\" and obj[\"shape\"] != \"changing\" and type(obj[\"shape\"]) == str:\n        if shape_map(obj[\"shape\"]) not in shape_counts_grouped.keys():\n            shape_counts_grouped[shape_map(obj[\"shape\"])] = 1\n        else:\n            shape_counts_grouped[shape_map(obj[\"shape\"])] += 1\n            \nshape_x = list(shape_counts_grouped.keys())\nshape_y = list(shape_counts_grouped.values())\nshape_x = [str(shape) for shape in shape_x]\n# print(shape_y)\n# print(shape_x)\nplt.rcParams[\"figure.figsize\"] = (22,6)\nplt.xticks(rotation=90, fontsize=32)\nplt.ylabel(\"sightings\", fontsize=28)\nplt.bar(shape_x, shape_y, alpha=.7)\nplt.savefig(\"sightings_shape_hist.png\")","4fda04f8":"import collections\nordered_by_year = collections.OrderedDict(sorted(shape_year.items()))\n\n\ndf_shape_year = pd.DataFrame(ordered_by_year)\ndf_shape_year.head()","a9e95e24":"# cleaning\ndf_shape_year = df_shape_year.fillna(0)","128c613f":"\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (18,12)\nsns.set(font_scale=1.8)\nax = sns.heatmap(df_shape_year.loc[:,1952:],cmap='viridis', robust=True)\nplt.savefig(\"shape_year_heatmap.jpg\")","7525d467":"## dividing each shape counts per year by that year's overall sightings\nfor i, obj in df_shape_year.iterrows():\n    #print(obj)\n    for j, count in enumerate(obj):\n        year = df_shape_year.columns[j]\n        \n        counts_for_year = year_counts[year]\n        df_shape_year.loc[i, df_shape_year.columns[j]] = count\/counts_for_year","9aab3eb9":"plt.rcParams[\"figure.figsize\"] = (18,12)\nsns.set(font_scale=1.8)\nax = sns.heatmap(df_shape_year.loc[:,1960:],cmap='viridis', vmax=.4)\nplt.savefig(\"shape_year_heatmap.jpg\")\n","54a9091f":"import numpy as np\nimport random\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.cbook as cbook\nimport datetime\nimport nltk\nimport re\nimport string\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer \nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom wordcloud import WordCloud\n\ndf = pd.read_csv('..\/input\/ufo-and-elections\/samsclean.csv')","6de5bd2d":"df.head()","e02e5fa0":"def year_pre_post(x):\n        if x < 1996:\n            return \"pre 96\"\n        else:\n            return \"post 96\"","326cca49":"df[\"last_election_year\"] = df[\"last_election_year\"].apply(lambda x: year_pre_post(x))","71e5721d":"#cleaning and removing stop words\n\n    \ndf.head()\nnltk.download('stopwords')\nstop_words = stopwords.words(\"english\")\n\ndef text_preproc(x):\n    if type(x) == str:\n        # filtering out stop words\n        x = x.encode('ascii', 'ignore').decode()\n        x = ' '.join([word for word in x.split(' ') if word not in stop_words and word != \"pd\"])\n        \n        return x\n    else:\n        return \"\"\n\ndf['comments'] = df['comments'].apply(lambda x: text_preproc(x))  \ndf['comments'] = df['comments'].apply(lambda x: text_preproc(x))\n    \ndf.head()","a543cbef":"vectorizer = TfidfVectorizer(stop_words='english') # suspiciously accurate\n#vectorizer = CountVectorizer(stop_words='english')\nX = vectorizer.fit_transform(df[\"comments\"])","40f7c413":"\n\ntraining_set = df.sample(frac = 0.8) \n\ntest_set = df.drop(training_set.index) \n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, average_precision_score, f1_score, classification_report\n\ntraining_X = vectorizer.transform(training_set[\"comments\"])\ntest_vect = vectorizer.transform(test_set[\"comments\"]) # vectorizing train and test comments\n\n\n","a1ad10ac":"clf_year = MultinomialNB().fit(training_X, training_set[\"last_election_year\"])\npredicted_year = clf_year.predict(test_vect)\n\nprint(predicted_year)\npre_predictions = 0\nfor year in predicted_year:\n    if year == \"pre 96\":\n        pre_predictions += 1\nprint(pre_predictions)\n\n\nprint(classification_report(test_set[\"last_election_year\"], predicted_year))","1cac68a6":"# with svm\n# LinearSVC\nclf_year_svm = LinearSVC().fit(training_X, training_set[\"last_election_year\"])\npredicted_year_svm = clf_year_svm.predict(test_vect)\n\nprint(predicted_year_svm)\npre_predictions_svm = 0\nfor year in predicted_year_svm:\n    if year == \"pre 96\":\n        pre_predictions_svm += 1\nprint(pre_predictions_svm)\n\n\nprint(classification_report(test_set[\"last_election_year\"], predicted_year_svm))","e22fb98e":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier(n_estimators=200, max_depth=8).fit(training_X, training_set[\"last_election_year\"])\n\npredicted_year_gb = gb_clf.predict(test_vect)\n\n\n\nprint(classification_report(test_set[\"last_election_year\"], predicted_year_gb))","a9f6e69f":"pre_95_comments = \"\"\npost_95_comments = \"\"\nnum_pre = 0\nnum_post = 0\n\nfor i, obj in df.iterrows():\n    if obj[\"last_election_year\"] == \"pre 96\":\n        pre_95_comments +=  obj[\"comments\"]\n        num_pre += 1\n    else:\n        num_post += 1\n        post_95_comments += obj[\"comments\"]\n        \n##### pre_95_comments = re.sub(r'lights', ' ', pre_95_comments)\npre_95_comments = ' '.join([word for word in pre_95_comments.split(' ')])\npost_95_comments = ' '.join([word for word in post_95_comments.split(' ')])\n\nwordcloud_pre = WordCloud(max_words=20).generate(pre_95_comments) \nprint(\"pre\", num_pre)\nprint(\"post\", num_post)\n\nplt.imshow(wordcloud_pre) ","52251dfe":"wordcloud_post = WordCloud(max_words=20).generate(post_95_comments) \nplt.imshow(wordcloud_post)","3140f985":"Lets try naive bayes","0b1692e1":"Not so hot. But f score for sgd and svc are both above .5 and wordclouds seem to indicate a difference in pre and post 95 comments.","e0d56650":"\"Light\", \"Sky\" and \"Object\" become much more popular post 1995 than beforehand. Thats kind of interesting because those seem to be among the most vague descriptors for ufo sightings. For more conclusions, and analyses see the rest of our project at: https:\/\/github.com\/KWolley\/CSPB4502_UFO_PresidentialElections ","ad2613f0":"Above is a pre 95 comments wordcloud\nBelow is post 95","4321e2cc":"Lets make a training and test set, and vectorize the comments","881fc26f":"A little better, but not very good pre 96 recall","b6b67d94":"See https:\/\/github.com\/KWolley\/CSPB4502_UFO_PresidentialElections for full project.\nUsing some data sets built by teammates that cleaned ufo-sightings and merging with a us elections data set: https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/VOQCHQ","9b612ff3":"Wordclouds:","68e97b44":"Plotting the heatmap by fraction of all frequencies for a given year seems to show that \ncircular stops being the dominant shape in the mid\/late 90s, and light based becomes more popular\n\n- Are there other pattern shifts around that time?\n\nLets try classifying the comments attribute which describes ufo sightings to see if there is a difference between pre and post mid 90s comments\n","94517e2c":"NOT GOOD! due to a class imbalance issue where we have many more post 96 comments than pre 96 our model learned to just always predict that a comment is post 96. Lets try some other classifiers","168aed9f":"### Time to aggregate like shapes"}}