{"cell_type":{"4e954c90":"code","c41c45f8":"code","02621ec5":"code","4a57469e":"code","60f0b972":"code","724f2e18":"code","af1f1bef":"code","46088f49":"code","3bed4bbf":"code","46d00554":"code","45b50e03":"code","824e5f78":"code","ad7ea6ef":"code","ce35735e":"code","3a442cd6":"code","0dbd556b":"code","b5bd9e40":"code","9fca5e74":"code","67aa5661":"code","fab36212":"code","e4464af8":"code","5a605583":"code","afcf4e6c":"code","03396a5d":"code","87e443ea":"code","c2e820be":"code","61566143":"code","e8180224":"code","01919a92":"code","b6b17c67":"code","b6d91a3b":"code","8ecd5618":"code","0c3046f2":"code","2dbd749a":"code","7ea39a05":"code","9cd456fc":"code","6de492a5":"code","6204d64e":"code","649863a1":"code","d7130b8d":"code","7548df5f":"code","9ac2c211":"code","c7cdbbc8":"markdown","7713c150":"markdown","f004ca71":"markdown","bfcfcaeb":"markdown","061e7666":"markdown","2aaadf06":"markdown","bb6c0f01":"markdown","c20361a7":"markdown","4d6bcd1e":"markdown","d195c288":"markdown","85f32b58":"markdown","673ad38b":"markdown","547423d6":"markdown","8806607e":"markdown","7dff7c93":"markdown","6626bc18":"markdown","91ab95a6":"markdown","ee5485e3":"markdown","edfa187b":"markdown","69da0ae0":"markdown","b479ce31":"markdown","0721c5c5":"markdown","56775575":"markdown","0e6f109d":"markdown","1d0aadf5":"markdown","5e2ee0cc":"markdown","810cab52":"markdown","6edcdd5d":"markdown","f925f32d":"markdown","1a3c4175":"markdown","c000e68a":"markdown","3e918a28":"markdown","62dcd2a2":"markdown","3aa7bb03":"markdown","c4572ddc":"markdown","3bc49917":"markdown","a7792d09":"markdown","ad4fb095":"markdown","7773e1eb":"markdown","24ec062d":"markdown","bd72b4f3":"markdown","02df7e51":"markdown","c4beb353":"markdown","18ee5fee":"markdown","eb4e85d2":"markdown","ab1e8ad9":"markdown","d08d1e05":"markdown","04e022de":"markdown","a9ed5c63":"markdown","c07cda1d":"markdown"},"source":{"4e954c90":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c41c45f8":"age  = [23,27,24,23,34,28,23,27,36,38]","02621ec5":"mean_age = np.mean(age)\nprint(\"Mean:\" ,mean_age)","4a57469e":"median_age = np.median(age)\nprint(\"Median:\" ,median_age)","60f0b972":"mode_age = stats.mode(age)\nprint(\"Mode:\" ,mode_age[0][0])","724f2e18":"print(\"Range: \", (np.max(age)-np.min(age)))","af1f1bef":"print(\"Variance: \", (np.var(age)))\nvar = sum((age - np.mean(age))**2)\/len(age)\nprint(\"Variance with Formula: \", var)","46088f49":"print(\"Standard Deviation: \", np.std(age))\nstd = np.sqrt(sum((age - np.mean(age))**2)\/len(age))\nprint(\"Standard deviation with Formula: \", std)","3bed4bbf":"import matplotlib.pyplot as plt\n\ny = np.random.uniform(5,8,100)\nx1 = np.random.uniform(10,20,100)\nx2 = np.random.uniform(0,30,100)\n\nplt.scatter(x1,y,color = \"blue\")\nplt.scatter(x2,y,color = \"red\")\nplt.xlim([-1,31])\nplt.ylim([2,11])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nprint(\"x1 mean: {} and median: {}\".format(np.mean(x1),np.median(x1)))\nprint(\"x2 mean: {} and median: {}\".format(np.mean(x2),np.median(x2)))","46d00554":"x1_range = (np.max(x1)-np.min(x1))\nx1_variance = (np.var(x1))\nx1_std = (np.std(x1))\n\nx2_range = (np.max(x2)-np.min(x2))\nx2_variance = (np.var(x2))\nx2_std = (np.std(x2))\n\nx1_dispersion = [x1_range, x1_variance, x1_std]\nx2_dispersion = [x2_range, x2_variance, x2_std]\n\ndf = pd.DataFrame([x1_dispersion,x2_dispersion],columns= ['Range','Variance','Std'], index= ['x1','x2'])\ndf","45b50e03":"plt.figure(figsize=(6,4))    \nplt.plot(['Range','Variance','Std'], x1_dispersion, color = \"blue\")\nplt.plot(['Range','Variance','Std'], x2_dispersion, color  = \"red\")\nplt.title(\"Dispersion\", size = 14)\nplt.show()","824e5f78":"plt.style.use(\"ggplot\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata.head()","ad7ea6ef":"data_abnormal = data[data[\"class\"] == \"Abnormal\"]\ndata_normal = data[data[\"class\"] == \"Normal\"]\ndesc = data_abnormal.pelvic_incidence.describe()\nQ1 = desc[4]\nQ3 = desc[6]\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"Anything outside this range is an outlier: (\" , lower_bound ,\",\" , upper_bound,\")\")\ndata_abnormal[data_abnormal.pelvic_incidence < lower_bound].pelvic_incidence\nprint(\"Outliers: \" , data_abnormal[(data_abnormal.pelvic_incidence < lower_bound) | (data_abnormal.pelvic_incidence > upper_bound)].pelvic_incidence.values)","ce35735e":"melted_data  = pd.melt(data,id_vars = \"class\", value_vars = ['pelvic_incidence'])\nsns.boxplot(x = \"variable\", y = \"value\", hue = \"class\", data = melted_data)\nplt.show()","3a442cd6":"f,ax=plt.subplots(figsize = (8,8))\n# corr() is actually pearson correlation\nsns.heatmap(data.corr(),\n            annot= True,\n            linewidths=0.5,\n            fmt = \".2f\",\n            vmax = 1,\n            vmin = -1,\n            ax=ax,\n            annot_kws = {'size': 14},\n            cmap =\"coolwarm\")\nplt.xticks(rotation=70, size = 12)\nplt.yticks(rotation=0, size = 12)\nplt.title('Correlation Map',size = 14)\nplt.show()","0dbd556b":"plt.figure(figsize = (15,10))\nsns.jointplot(data.pelvic_incidence,data.sacral_slope,kind=\"reg\")\nsns.jointplot(data.pelvic_radius,data.sacral_slope,kind=\"reg\")\nplt.show()","b5bd9e40":"sns.set(style = \"white\")\ndf = data.loc[:,[\"pelvic_incidence\",\"sacral_slope\",\"lumbar_lordosis_angle\"]]\ng = sns.PairGrid(df,diag_sharey = False,)\ng.map_lower(sns.kdeplot,cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot,lw =3)\nplt.show()","9fca5e74":"np.cov(data.pelvic_incidence,data.sacral_slope)\nprint(\"Covariance between Pelvic Incidence and Sacral Slope: \",data.pelvic_incidence.cov(data.sacral_slope))\nprint(\"Covariance between Pelvic Incidence and Pelvic Radius: \",data.pelvic_incidence.cov(data.pelvic_radius))\nfig, axs = plt.subplots(1, 2, figsize = (10,4))\naxs[0].scatter(data.pelvic_incidence, data.sacral_slope)\naxs[1].scatter(data.pelvic_radius, data.pelvic_incidence)\nplt.show()","67aa5661":"p1 = data.loc[:,[\"pelvic_incidence\",\"sacral_slope\"]].corr(method= \"pearson\")\np2 = data.sacral_slope.cov(data.pelvic_incidence)\/(data.sacral_slope.std()*data.pelvic_incidence.std())\nprint('Pearson Correlation: ')\nprint(p1)\nprint('Pearson Correlation: ',p2)","fab36212":"sns.jointplot(data.sacral_slope,data.pelvic_incidence,kind=\"reg\")\nplt.show()","e4464af8":"ranked_data = data.rank() \nspearman_corr = ranked_data.loc[:,[\"pelvic_incidence\",\"sacral_slope\"]].corr(method= \"pearson\")\nprint(\"Spearman's Correlation: \")\nprint(spearman_corr)","5a605583":"mean_diff = data_abnormal.pelvic_incidence.mean() - data_normal.pelvic_incidence.mean()    # m1 - m2\nvar_abnormal = data_abnormal.pelvic_incidence.var()\nvar_normal = data_normal.pelvic_incidence.var()\nvar_pooled = (len(data_abnormal)*var_normal +len(data_normal)*var_abnormal ) \/ float(len(data_abnormal)+ len(data_normal))\neffect_size = mean_diff\/np.sqrt(var_pooled)\nprint(\"Effect Size:\",effect_size)","afcf4e6c":"import math as math\nwords = [\"A\",\"B\",\"C\"]\np = int(math.factorial(len(words)) \/ math.factorial(len(words)-2))\nprint(p)","03396a5d":"x = np.random.random_integers(10,size=100000)\nplt.hist(x)\nplt.show()","87e443ea":"import random\nmean_sample = []\nfor i in range(1000):\n    sample = random.randrange(5,10)\n    mean_sample.append(np.mean(random.sample(list(x),sample)))\nplt.hist(mean_sample,bins = 50, color = \"red\")\nplt.show()","c2e820be":"plt.hist(x,alpha = 0.5,density=True)\nplt.hist(mean_sample,bins = 50,alpha = 0.5,color = \"red\",density=True)\nplt.title(\"Central Limit Theorem\")\nplt.show()","61566143":"statistic, p_value = stats.ttest_rel(data.pelvic_incidence,data.sacral_slope)\np_value = round(p_value,3)\nprint('p-value: ',p_value)\nif p_value == 0: \n    print(\"Reject null hypothesis, alternate hypothesis is correct, relationship between pelvic incidence and sacral slope is not zero.\")\nelse:\n    print(\"Fail to reject null hypothesis, relationship between pelvic incidence and sacral slope is zero.\")","e8180224":"s1 = np.array([14.67230258, 14.5984991 , 14.99997003, 14.83541808, 15.42533116,\n       15.42023888, 15.0614731 , 14.43906856, 15.40888636, 14.87811941,\n       14.93932134, 15.04271942, 14.96311939, 14.0379782 , 14.10980817,\n       15.23184029])\nprint(\"mean 1: \", np.mean(s1))\nprint(\"standart deviation 1: \", np.std(s1))\nprint(\"variance 1: \", np.var(s1))\ns2 = np.array([15.23658167, 15.30058977, 15.49836851, 15.03712277, 14.72393502,\n       14.97462198, 15.0381114 , 15.18667258, 15.5914418 , 15.44854406,\n       15.54645152, 14.89288726, 15.36069141, 15.18758271, 14.48270754,\n       15.28841374])\nprint(\"mean 2: \", np.mean(s2))\nprint(\"standart deviation 2: \", np.std(s2))\nprint(\"variance 2: \", np.var(s2))\n# visualize with pdf\nimport seaborn as sns\nsns.kdeplot(s1)\nsns.kdeplot(s2)\nplt.show()","01919a92":"t_val = np.abs(np.mean(s1)-np.mean(s2))\/np.sqrt((np.var(s1)\/len(s1))+(np.var(s2)\/len(s2)))\nprint(\"t-value: \", t_val)","b6b17c67":"critical_value = 2.04\nprint(\"Null hypothesis: There is no statistically significant difference between these two distributions.\")\nif t_val > critical_value:\n    print(\"t value > critical value\")\n    print(\"Reject Null Hypothesis\")\nelse:\n    print(\"t value < critical value\")\n    print(\"Fail to reject Null Hypothesis \")","b6d91a3b":"from scipy.stats import shapiro,levene\ndata = pd.read_csv(\"..\/input\/students-performance-in-exams\/StudentsPerformance.csv\")\ndata[data['parental level of education'].isin([\"bachelor's degree\",'high school'])]. \\\ngroupby('parental level of education').agg({'math score':'mean'})","8ecd5618":"test_stat, p = shapiro(data[data['parental level of education'] == \"bachelor's degree\"]['math score'])\nprint('Test Stat: {}'.format(round(test_stat,4)))\nprint('p-value: {}'.format(round(p,4)))\nif p < 0.05:\n    print('p < 0.05 --> Reject Null Hypothesis, data are not normally distributed.')\nelse:\n    print('p > 0.05 --> Cannot reject Null Hypothesis, data are normally distributed.')","0c3046f2":"test_stat, p = shapiro(data[data['parental level of education'] == 'high school']['math score'])\nprint('Test Stat: {}'.format(round(test_stat,4)))\nprint('p-value: {}'.format(round(p,4)))\nif p < 0.05:\n    print('p < 0.05 --> Reject Null Hypothesis, data are not normally distributed.')\nelse:\n    print('p > 0.05 --> Cannot reject Null Hypothesis, data are normally distributed.')","2dbd749a":"test_stat, p = levene(data[data['parental level of education'] == \"bachelor's degree\"]['math score'],\n                      data[data['parental level of education'] == 'high school']['math score'])\n\nprint('Test Stat: {}'.format(round(test_stat,4)))\nprint('p-value: {}'.format(round(p,4)))\n\nif p < 0.05:\n    print('p < 0.05 --> Reject Null Hypothesis, variances are not equal.')\nelse:\n    print('p > 0.05 --> Cannot reject Null Hypothesis, variances are equal.')","7ea39a05":"test_stat, p = stats.ttest_ind(data[data['parental level of education'] == \"bachelor's degree\"]['math score'],\n                               data[data['parental level of education'] == 'high school']['math score'],\n                               equal_var = True)\n\nprint('Test Stat: {}'.format(round(test_stat,4)))\nprint('p-value: {}'.format(round(p,4)))\n\nif p < 0.05:\n    print('p < 0.05 --> Reject Null Hypothesis, population means are not the same.')\n    \nelse:\n    print('p > 0.05 --> Cannot reject Null Hypothesis, population means are the same.')","9cd456fc":"middle_school = np.array([51.36372405, 44.96944041, 49.43648441, 45.84584407, 45.76670682,\n       56.04033356, 60.85163656, 39.16790361, 36.90132329, 43.58084076])\nhigh_school = np.array([56.65674765, 55.92724431, 42.32435143, 50.19137162, 48.91784081,\n       48.11598035, 50.91298812, 47.46134988, 42.76947742, 36.86738678])\nuniversity = np.array([60.03609029, 56.94733648, 57.77026852, 47.29851926, 54.21559389,\n       57.74008243, 50.92416154, 53.47770749, 55.62968872, 59.42984391])\n\nprint(\"Middle school Mean: \",np.mean(middle_school))\nprint(\"High school Mean: \",np.mean(high_school))\nprint(\"University Mean: \",np.mean(university))\ntotal_mean = (np.mean(middle_school) + np.mean(high_school) + np.mean(university))\/3\nprint(\"Total Mean: \",np.mean(total_mean))\n\nsns.kdeplot(middle_school)\nsns.kdeplot(high_school)\nsns.kdeplot(university)\nplt.show()","6de492a5":"stats.f_oneway(middle_school, high_school, university)","6204d64e":"f_value = stats.f_oneway(middle_school, high_school, university)[0]\nprint(\"F value:\", f_value)","649863a1":"critical_value = 5.4881\nif f_value > critical_value:\n    print(\"Reject to Null Hypothesis (f_value > critical_value)\")\nelse:\n    print(\"Fail to reject Null Hypothesis (critical_value > f_value)\")","d7130b8d":"observation = np.array([5,7,9,4,1,10,6])\nprint(\"Total: \",np.sum(observation))\nexpected = np.sum(observation)\/ len(observation)\nprint(\"Expected: \",expected)\nchi_value = np.sum(((observation - expected)**2)\/expected)\nprint(\"Chi_value: \",chi_value)","7548df5f":"from scipy.stats import chi2\ncrit_value = chi2.isf(0.05,6)\nprint(\"Critical value: \", crit_value)","9ac2c211":"if crit_value > chi_value:\n    print(\"Fail to reject Null Hypothesis (crit_value > chi_value)\")\nelse:\n    print(\"Reject Null Hypothesis (chi_value > crit_value)\")","c7cdbbc8":"<a id = \"28\"><\/a>\n## A\/B Testing\nA\/B testing (also known as bucket testing or split-run testing) is a user experience research methodology. A\/B tests consist of a randomized experiment with two variants, A and B. It includes application of statistical hypothesis testing or \"two-sample hypothesis testing\" as used in the field of statistics. A\/B testing is a way to compare two versions of a single variable, typically by testing a subject's response to variant A against variant B, and determining which of the two variants is more effective.\n\n![image.png](attachment:031e0bfb-17f6-4bf7-9c76-19c8f9529962.png)","7713c150":"<img src=\"https:\/\/media.giphy.com\/media\/l378c04F2fjeZ7vH2\/giphy.gif\">","f004ca71":"<a id = \"19\"><\/a>\n## Permutation\nA permutation of a set is, loosely speaking, an arrangement of its members into a sequence or linear order, or if the set is already ordered, a rearrangement of its elements. The word \"permutation\" also refers to the act or process of changing the linear order of an ordered set.\n\nPermutations differ from combinations, which are selections of some members of a set regardless of order. For example, written as tuples, there are six permutations of the set {1,2,3}, namely: (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2), and (3,2,1). These are all the possible orderings of this three-element set. Anagrams of words whose letters are different are also permutations: the letters are already ordered in the original word, and the anagram is a reordering of the letters. The study of permutations of finite sets is an important topic in the fields of combinatorics and group theory.\n\n\n$$\\Huge P(n,r)  = \\frac {n!} {(n-r)! } $$","bfcfcaeb":"<a id = \"22\"><\/a>\n# Statistics\nStatistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.","061e7666":"<a id = \"4\"><\/a>\n# Central Tendency\nCentral tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s.\n\nThe most common measures of central tendency are the arithmetic mean, the median, and the mode. A middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote \"the tendency of quantitative data to cluster around some central value.\"\n\nThe central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysis may judge whether data has a strong or a weak central tendency based on its dispersion.","2aaadf06":"### Levene Test for Equality of Variances\n\nLevene's test is used to test if k samples have equal variances. Equal variances across samples is called homogeneity of variance. Some statistical tests, for example the analysis of variance, assume that variances are equal across groups or samples. The Levene test can be used to verify that assumption.","bb6c0f01":"<a id = \"26\"><\/a>\n## Hypothesis Testing\nHypothesis testing refers to the process of making inferences or educated guesses about a particular parameter. This can either be done using statistics and sample data, or it can be done on the basis of an uncontrolled observational study.\n\nWhen a predetermined number of subjects in a hypothesis test prove the \"alternative hypothesis,\" then the original hypothesis (the \"null hypothesis\") is overturned or \"rejected.\" You must decide the level of statistical significance in your hypothesis, as you can never be 100 percent confident in your findings. First, let's examine the steps to test a hypothesis. Then, we'll enjoy some examples of hypothesis testing.\n\n## How to test a Hypothesis?\nAt this point, you'll already have a hypothesis ready to go. Now, it's time to test your theory. Remember, a hypothesis is a statement regarding what you believe might happen. These are the steps you'll want to take to see if your suppositions stand up:\n\n1. **State your null hypothesis.** The null hypothesis is a commonly accepted fact. It's the default, or what we'd believe if the experiment was never conducted. It's the least exciting result, showing no significant difference between two or more groups. Researchers work to nullify or disprove null hypotheses.\n2. **State an alternative hypothesis.** You'll want to prove an alternative hypothesis. This is the opposite of the null hypothesis, demonstrating or supporting a statistically significant result. By rejecting the null hypothesis, you accept the alternative hypothesis.\n3. **Determine a significance level.** This is the determiner, also known as the alpha (\u03b1). It defines the probability that the null hypothesis will be rejected. A typical significance level is set at 0.05 (or 5%). You may also see 0.1 or 0.01, depending on the area of study.\n    If you set the alpha at 0.05, then there is a 5% chance you'll find support for the alternative hypothesis (thus rejecting the null hypothesis) when, in truth, the null hypothesis is actually true and you were wrong to reject it.\n    In other words, the significance level is a statistical way of demonstrating how confident you are in your conclusion. If you set a high alpha (0.25), then you'll have a better shot at supporting your alternative hypothesis, since you don't need to find as big a difference between your test groups. However, you'll also have a bigger chance at being wrong about your conclusion.\n4. **Calculate the p-value.** The p-value, or calculated probability, indicates the probability of achieving the results of the null hypothesis. While the alpha is the significance level you're trying to achieve, the p-level is what your actual data is showing when you calculate it. A low p-value offers stronger support for your alternative hypothesis.\n5. **Draw a conclusion.** If your p-value meets your significance level requirements, then your alternative hypothesis may be valid and you may reject the null hypothesis. In other words, if your p-value is less than your significance level (e.g., if your calculated p-value is 0.02 and your significance level is 0.05), then you can reject the null hypothesis and accept your alternative hypothesis.\n\n## Hypothesis Testing Example\nLet's take those five steps and look at a couple of real-world scenarios.\n\n### Peppermint Essential Oil\nEssential oils are becoming more and more popular. Chamomile, lavender, and ylang-ylang are commonly touted as anxiety remedies. Perhaps you'd like to test the healing powers of peppermint essential oil. Your hypothesis might go something like this:\n\n1. **Null hypothesis** - Peppermint essential oil has no effect on the pangs of anxiety.\n2. **Alternative hypothesis** - Peppermint essential oil alleviates the pangs of anxiety.\n3. **Significance level** - The significance level is 0.25 (allowing for a better shot at proving your alternative hypothesis).\n4. **P-value** - The p-value is calculated as 0.05.\n5. **Conclusion** - After providing one group with peppermint oil and the other with a placebo, you gauge the difference between the two based on self-reported levels of anxiety. Based on your calculations, the difference between the two groups is statistically significant with a p-value of 0.05, well below the defined alpha of 0.25. You conclude that your study supports the alternative hypothesis that peppermint essential oil can alleviate the pangs of anxiety.","c20361a7":"![image.png](attachment:image.png)\n\nIf the chi-square value is less than the critical value, there is a high correlation between observation and expected values.\n\n- 6.4> 3.8 so reject the null hypothesis.","4d6bcd1e":"<a id = \"32\"><\/a>\n## Chi-Square Analysis Example\nThere are 7 washing machines with equal probability of deterioration in the laundry. So expected = failure rate should be same for all washing machines.\n\n- Washing machines are independent of each other.\n- Observations: 1(5), 2(7), 3(9), 4(4), 5(1), 6(10), 7(6)\n- Null Hypothesis: observation values in this way makes sense with a statistically 95% probability.\n- Total Deterioration: 42\n- Expected Value: 42 \/ 7 = 6\n- Degrees of Freedom: 7 - 1 = 6","d195c288":"**Example:** The population may be \"ALL people living in the US.\" A sample data set contains a part, or a subset, of a population. The size of a sample is always less than the size of the population from which it is taken.","85f32b58":"<a id = \"12\"><\/a>\n# Central Tendency and Dispersion","673ad38b":"<a id = \"5\"><\/a>\n## Mean\nThere are several kinds of mean in mathematics, especially in statistics. For a data set, the arithmetic mean, also called the expected value or average, is the central value of a discrete set of numbers: specifically, the sum of the values divided by the number of values.","547423d6":"<a id = \"27\"><\/a>\n## T-Distribution\nStudent's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a normally-distributed population in situations where the sample size is small and the population's standard deviation is unknown. It was developed by English statistician William Sealy Gosset under the pseudonym \"Student\".\n\nThe t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. The Student's t-distribution also arises in the Bayesian analysis of data from a normal family.","8806607e":"<a id = \"21\"><\/a>\n## Intersection, Unions and Complements\n- The **intersection** of two sets A and B, denoted by A \u2229 B, is the set containing all elements of A that also belong to B (or equivalently, all elements of B that also belong to A).    \n- In set theory, the **union** (denoted by \u222a) of a collection of sets is the set of all elements in the collection. It is one of the fundamental operations through which sets can be combined and related to each other.","7dff7c93":"![image.png](attachment:image.png)","6626bc18":"<a id = \"3\"><\/a>\n# Population and Sample\nA population is the entire group that you want to draw conclusions about. A sample is the specific group that you will collect data from. The size of the sample is always less than the total size of the population. In research, a population doesn't always refer to people.","91ab95a6":"Spearman's correlation is little higher than Pearson correlation.\n- If relationship between distributions are non linear, spearman's correlation tends to better estimate the strength of relationship.\n- Pearson correlation can be affected by outliers. Spearman's correlation is more robust.","ee5485e3":"![image.png](attachment:image.png)","edfa187b":"<a id = \"1\"><\/a>\n# Data\nData are characteristics or information, usually numerical, that are collected through observation. In a more technical sense, data are a set of values of qualitative or quantitative variables about one or more persons or objects, while a datum (singular of data) is a single value of a single variable.\n\nThere are 2 types of data:\n- **Continous:**  Continuous data is data that can take any value. Height, weight, temperature and length are all examples of continuous data. Some continuous data will change over time; the weight of a baby in its first year or the temperature in a room throughout the day.\n- **Categorical:**  Categorical variables represent types of data which may be divided into groups. Examples of categorical variables are race, sex, age group, and educational level.","69da0ae0":"<a id = \"29\"><\/a>\n# ANOVA (Analysis of Variance)\nAnalysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors. The systematic factors have a statistical influence on the given data set, while the random factors do not. Analysts use the ANOVA test to determine the influence that independent variables have on the dependent variable in a regression study.\n\n\nThe t- and z-test methods developed in the 20th century were used for statistical analysis until 1918, when Ronald Fisher created the analysis of variance method. ANOVA is also called the Fisher analysis of variance, and it is the extension of the t- and z-tests. The term became well-known in 1925, after appearing in Fisher's book, \"Statistical Methods for Research Workers.\" It was employed in experimental psychology and later expanded to subjects that were more complex.\n\n- For example, are the exam anxiety of middle school, high school and university students different from each other? We will answer the question with ANOVA.\n- Null Hypothesis: Exam concerns same","b479ce31":"<a id = \"17\"><\/a>\n# Effect Size\nEffect size is a number measuring the strength of the relationship between two variables in a statistical population, or a sample-based estimate of that quantity. It can refer to the value of a statistic calculated from a sample of data, the value of a parameter of a hypothetical statistical population, or to the equation that operationalizes how statistics or parameters lead to the effect size value. Examples of effect sizes include the correlation between two variables, the regression coefficient in a regression, the mean difference, or the risk of a particular event (such as a heart attack) happening. Effect sizes complement statistical hypothesis testing, and play an important role in power analyses, sample size planning, and in meta-analyses. The cluster of data-analysis methods concerning effect sizes is referred to as estimation statistics.\n\n$$\\huge  \\frac {M_1 - M_2} {\\sqrt{\\frac {S_1^2} {n_1} + \\frac {S_2^2} {n_2} }} $$","0721c5c5":"<a id = \"7\"><\/a>\n## Mode\nThe mode is the value that appears most often in a set of data values. If X is a discrete random variable, the mode is the value x (i.e, X = x) at which the probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled.","56775575":"<a id = \"15\"><\/a>\n# Pearson Correlation Coefficient\nPearson correlation coefficient is the test statistics that measures the statistical relationship, or association, between two continuous variables.  It is known as the best method of measuring the association between variables of interest because it is based on the method of covariance.  It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship.","0e6f109d":"<a id = \"16\"><\/a>\n# Spearman Rank Coefficient\nThe Spearman's rank-order correlation is the nonparametric version of the Pearson product-moment correlation. Spearman's correlation coefficient, (\u03c1, also signified by rs) measures the strength and direction of association between two ranked variables.","1d0aadf5":"<a id = \"14\"><\/a>\n# Bivariate Data and Covariance\n**Bivariate data** is data on each of two variables, where each value of one of the variables is paired with a value of the other variable. Typically it would be of interest to investigate the possible association between the two variables. The association can be studied via a tabular or graphical display, or via sample statistics which might be used for inference. The method used to investigate the association would depend on the level of measurement of the variable. ","5e2ee0cc":"<p style=\"padding: 10px;\n          color: Black;\n          font-weight: bold;\n          text-align: center;\n          font-size:240%;\">\nStatistical Learning Tutorial\n\n<\/p>","810cab52":"<a id = \"10\"><\/a>\n## Variance\nVariance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of numbers is spread out from their average value.\n\n$$\\Huge \\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\mu)^2} {n} $$","6edcdd5d":"<a id = \"23\"><\/a>\n## Sampling\nQuality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt for the samples to represent the population in question. Two advantages of sampling are lower cost and faster data collection than measuring the entire population.\n\nEach observation measures one or more properties (such as weight, location, colour) of observable bodies distinguished as independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. Acceptance sampling is used to determine if a production lot of material meets the governing specifications.\n\n![image.png](attachment:image.png)","f925f32d":"<a id = \"9\"><\/a>\n## Range\nThe range of a set of data is the difference between the largest and smallest values.","1a3c4175":"<a id = \"24\"><\/a>\n## Central Limit Theorem\nThe central limit theorem states that if you have a population with mean \u03bc and standard deviation \u03c3 and take sufficiently large random samples from the population with replacement , then the distribution of the sample means will be approximately normally distributed.\n\nAll this is saying is that as you take more samples, especially large ones, your graph of the sample means will look more like a normal distribution.\n\nHere\u2019s what the Central Limit Theorem is saying, graphically. The picture below shows one of the simplest types of test: rolling a fair die. The more times you roll the die, the more likely the shape of the distribution of the means tends to look like a normal distribution graph.","c000e68a":"![image.png](attachment:image.png)","3e918a28":"Permutations are the different ways in which a collection of items can be arranged. For example: The different ways in which the alphabets A, B and C, taken 2 at a time, can be arranged is 3!\/(3-2)! = 3!\/1! = 6 ways. (AB, AC, BA, BC, CA, CB)","62dcd2a2":"### Shapiro-Wilks Normality Test\n\nThe normality assumption is more important when the two groups have small sample sizes than for larger sample sizes.\n\nNormal distributions are symmetric, which means they are \u201ceven\u201d on both sides of the center. Normal distributions do not have extreme values, or outliers. You can check these two features of a normal distribution with graphs. Earlier, we decided that the body fat data was \u201cclose enough\u201d to normal to go ahead with the assumption of normality. The figure below shows a normal quantile plot for men and women, and supports our decision.","3aa7bb03":"<a id = \"18\"><\/a>\n# Probability\nProbability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1\/2 (which could also be written as 0.5 or 50%).","c4572ddc":"<a id = \"6\"><\/a>\n## Median\nThe median is the value separating the higher half from the lower half of a data sample, a population, or a probability distribution. For a data set, it may be thought of as \"the middle\" value. The basic feature of the median in describing data compared to the mean (often simply described as the \"average\") is that it is not skewed by a small proportion of extremely large or small values, and therefore provides a better representation of a \"typical\" value.","3bc49917":"# Credits\n\nhttps:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35a.htm\n\nhttps:\/\/www.jmp.com\/en_ch\/statistics-knowledge-portal\/t-test\/two-sample-t-test.html\n\nhttps:\/\/analyse-it.com\/docs\/user-guide\/distribution\/continuous\/normality-hypothesis-test","a7792d09":"<a id = \"25\"><\/a>\n## Standard Error\nThe standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution or an estimate of that standard deviation. If the statistic is the sample mean, it is called the standard error of the mean (SEM). \n\nThe sampling distribution of a mean is generated by repeated sampling from the same population and recording of the sample means obtained. This forms a distribution of different means, and this distribution has its own mean and variance. Mathematically, the variance of the sampling distribution obtained is equal to the variance of the population divided by the sample size. This is because as the sample size increases, sample means cluster more closely around the population mean.\n\nTherefore, the relationship between the standard error of the mean and the standard deviation is such that, for a given sample size, the standard error of the mean equals the standard deviation divided by the square root of the sample size. In other words, the standard error of the mean is a measure of the dispersion of sample means around the population mean.\n\nIn regression analysis, the term \"standard error\" refers either to the square root of the reduced chi-squared statistic, or the standard error for a particular regression coefficient (as used in, say, confidence intervals).","ad4fb095":"<a id = \"13\"><\/a>\n# Quartiles\nQuartile is a type of quantile which divides the number of data points into four parts, or quarters, of more-or-less equal size. The data must be ordered from smallest to largest to compute quartiles; as such, quartiles are a form of order statistic. The three main quartiles are as follows:\n\n- **The first quartile (Q1)** is defined as the middle number between the smallest number (minimum) and the median of the data set. It is also known as the lower or 25th empirical quartile, as **25%** of the data is below this point.\n- **The second quartile (Q2)** is the median of a data set; thus **50%** of the data lies below this point.\n- **The third quartile (Q3)** is the middle value between the median and the highest value (maximum) of the data set. It is known as the upper or 75th empirical quartile, as **75%** of the data lies below this point.\n    \nAlong with the minimum and maximum of the data (which are also quartiles), the three quartiles described above provide a five-number summary of the data. This summary is important in statistics because it provides information about both the center and the spread of the data. Knowing the lower and upper quartile provides information on how big the spread is and if the dataset is skewed toward one side. Since quartiles divide the number of data points evenly, the range is not the same between quartiles (i.e., Q3-Q2 \u2260 Q2-Q1) and is instead known as the interquartile range (IQR). While the maximum and minimum also show the spread of the data, the upper and lower quartiles can provide more detailed information on the location of specific data points, the presence of outliers in the data, and the difference in spread between the middle 50% of the data and the outer data points.","7773e1eb":"<a id = \"31\"><\/a>\n# Chi-Square Analysis\nA chi-squared test, also written as \u03c72 test, is a statistical hypothesis test that is valid to perform when the test statistic is chi-squared distributed under the null hypothesis, specifically Pearson's chi-squared test and variants thereof. Pearson's chi-squared test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table.\n\nIn the standard applications of this test, the observations are classified into mutually exclusive classes. If the null hypothesis that there are no differences between the classes in the population is true, the test statistic computed from the observations follows a \u03c72 frequency distribution. The purpose of the test is to evaluate how likely the observed frequencies would be assuming the null hypothesis is true.\n\nTest statistics that follow a \u03c72 distribution occur when the observations are independent and normally distributed, which assumptions are often justified under the central limit theorem. There are also \u03c72 tests for testing the null hypothesis of independence of a pair of random variables based on observations of the pairs.\n\nChi-squared tests often refers to tests for which the distribution of the test statistic approaches the \u03c72 distribution asymptotically, meaning that the sampling distribution (if the null hypothesis is true) of the test statistic approximates a chi-squared distribution more and more closely as sample sizes increase.\n\nFor example, let's give an example, we throw money in the air 10 times. It comes to 9 tails and 1 head.\n- Our question is: 9 times there is no chance of tails or if this money is inclined to tails? so is it biased (you can also consider it fraudulent)\n- Null hypothesis: For a fair coin, it makes sense to get 9 tails out of 10 shots with a statistically 95% probability (confidence level 0.05).","24ec062d":"1. [Data](#1)\n2. [Level of Measurements](#2)\n3. [Population and Sample](#3)\n4. [Central Tendency](#4)\n    - [Mean](#5)   \n    - [Median](#6)     \n    - [Mode](#7)\n5. [Dispersion](#8)   \n    - [Range](#9)   \n    - [Variance](#10)   \n    - [Standard Deviation](#11)\n6. [Central Tendency and Dispersion](#12)\n7. [Quartiles](#13)\n8. [Bivariate Data and Covariance](#14)\n9. [Pearson Correlation Coefficient](#15)\n10. [Spearman Rank Coefficient](#16)\n11. [Effect Size](#17)\n12. [Probability](#18)   \n    - [Permutation](#19)   \n    - [Combination](#20)\n    - [Intersection, Unions and Complements](#21)\n13. [Statistics](#22)\n    - [Sampling](#23)\n    - [Central Limit Theorem](#24)\n    - [Standard Error](#25)\n    - [Hypothesis Testing](#26)\n    - [T-Distribution](#27)\n    - [A\/B Testing](#28)\n14. [ANOVA (Analysis of Variance)](#29)\n    - [F-Distribution](#30)\n15. [Chi-Square Analysis](#31)\n    - [Chi-Square Analysis Example](#32)","bd72b4f3":"<a id = \"2\"><\/a>\n# Level of Measurements\nLevel of measurement or scale of measure is a classification that describes the nature of information within the values assigned to variables. Psychologist Stanley Smith Stevens developed the best-known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines. Other classifications include those by Mosteller and Tukey, and by Chrisman.\n\n**Nominal Measurement:** A nominal variable is one of the 2 types of categorical variables and is the simplest among all the measurement variables. Some examples of nominal variables include gender, Name, phone, etc.   \n**Ordinal Measurement:** Examples of ordinal variables include: socio economic status (\u201clow income\u201d,\u201dmiddle income\u201d,\u201dhigh income\u201d), education level (\u201chigh school\u201d,\u201dBS\u201d,\u201dMS\u201d,\u201dPhD\u201d), income level (\u201cless than 50K\u201d, \u201c50K-100K\u201d, \u201cover 100K\u201d), satisfaction rating (\u201cextremely dislike\u201d, \u201cdislike\u201d, \u201cneutral\u201d, \u201clike\u201d, \u201cextremely like\u201d).  \n**Interval Measurement:** An interval scale is one where there is order and the difference between two values is meaningful. Examples of interval variables include: temperature (Farenheit), temperature (Celcius), pH, SAT score (200-800), credit score (300-850).  \n**Ratio Measurement:** The most common examples of ratio scale are height, money, age, weight etc. With respect to market research, the common examples that are observed are sales, price, number of customers, market share etc.","02df7e51":"<a id = \"11\"><\/a>\n## Standard Deviation\nThe standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.\n\n$$\\Huge \\sigma = {\\sqrt{\\frac{\\sum_{}(x-\\mu)^2}{N}}} $$","c4beb353":"## Hypothesis Testing with Our Data\n**Null hypothesis:** relationship between pelvic incidence and sacral slope is zero.   \n**Alternate hypothesis:** relationship between pelvic incidence and sacral slope is not zero.   \n\nLet's find p-value.","18ee5fee":"<a id = \"30\"><\/a>\n##  F-Distribution\nIn probability theory and statistics, the F-distribution, also known as Snedecor's F distribution or the Fisher\u2013Snedecor distribution (after Ronald Fisher and George W. Snedecor) is a continuous probability distribution that arises frequently as the null distribution of a test statistic, most notably in the analysis of variance (ANOVA), e.g., F-test.\n\n- F value < critical value --> fail to reject null hypothesis\n- F value > critical value --> reject null hypothesis\n- Degrees of freedom for groups: Number of groups - 1   \n3 - 1 = 2\n- Degrees of freedom for error: (number of rows - 1)* number of groups   \n(10 - 1) * 3 = 27","eb4e85d2":"<a id = \"8\"><\/a>\n# Dispersion\nDispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the **variance**, **standard deviation**, and **interquartile range**.","ab1e8ad9":"For the tails in our example:   \n- expected frequency = 5\n- observed frequency = 9\n\nFor heads: \n\n- expected frequency = 5\n- observed frequency = 1\n\nChi-Sqaure Value: 6.4","d08d1e05":"<a id = \"20\"><\/a>\n## Combination\nA combination is a selection of items from a collection, such that the order of selection does not matter (unlike permutations). For example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. More formally, a k-combination of a set S is a subset of k distinct elements of S. If the set has n elements, the number of k-combinations is equal to the binomial coefficient.\n\n$$\\Huge C(n,r)  = \\frac {n!} { r!(n-r)! } $$","04e022de":"![image.png](attachment:image.png)","a9ed5c63":"**Covariance** is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values (that is, the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (that is, the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.\n\n$$\\Huge cov_{x,y}=\\frac{\\sum_{}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{n}$$","c07cda1d":"### Two-Samples T-Test\n\nThe two-sample t-test (also known as the independent samples t-test) is a method used to test whether the unknown population means of two groups are equal or not.\n\n**Is this the same as an A\/B test?**   \nYes, a two-sample t-test is used to analyze the results from A\/B tests.\n\n**When can I use the test?**   \nYou can use the test when your data values are independent, are randomly sampled from two normal populations and the two independent groups have equal variances."}}