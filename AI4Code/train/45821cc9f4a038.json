{"cell_type":{"63526224":"code","643e30f4":"code","71dd3d98":"code","5c1635f4":"code","f9b81a33":"code","9fb3b541":"code","9c65cf5f":"code","903c3c1c":"code","c2246593":"code","9befc9fa":"code","4dc4fb39":"code","42424a37":"code","0c7535ac":"code","206173aa":"code","638b9a8b":"code","182dc7ef":"code","60138ffc":"code","e5cde6ad":"code","03ece3f3":"code","23a64df1":"code","cb8a77a3":"code","cb10feed":"code","d3cd399b":"code","fcacd1cb":"code","99886249":"code","3bb16df5":"code","b9bb9f0b":"code","e114cce6":"code","bd818071":"code","666249d3":"code","033694cd":"code","d896d5d3":"code","1281ef76":"code","691ccd1e":"markdown","48d951f1":"markdown","c1c7cb0e":"markdown","8c7a2cae":"markdown","d3d34d28":"markdown","ba4bcae4":"markdown","3b461a0b":"markdown","6bf52fe3":"markdown","ec3d5fd6":"markdown","e9af3c1e":"markdown","97346e09":"markdown","c1176c08":"markdown","f0ff6139":"markdown","9937d8bf":"markdown","171e9212":"markdown","32a2eafd":"markdown","9c1c5485":"markdown","c8857b5c":"markdown","f384aed2":"markdown","e47f4c53":"markdown","3c6159ec":"markdown","9b6e2790":"markdown","045734c2":"markdown"},"source":{"63526224":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 8\nSUBMIT = True","643e30f4":"import time\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd \n\n# Testing, Scoring\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocssing\nfrom functools import partial\nfrom sklearn.base import clone\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder \nfrom category_encoders import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Models \nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","71dd3d98":"# Load data, drop columns with missing target\ntrain = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\") \ntest = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")\n    \n# Remove rows with missing target\ntrain.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# Columns of interest\nfeatures = [x for x in train.columns if x not in ['SalePrice','Id']]\ncategorical = [x for x in features if train[x].dtype == \"object\"]\nnumerical = [x for x in features if  train[x].dtype in ['int64', 'float64']]\nhigh_cardinality = [x for x in categorical if train[x].nunique() >= 10]\nlow_cardinality = [x for x in categorical if x not in high_cardinality]","5c1635f4":"# Function for comparing different preprocessing approaches\ndef score(preprocessing = None):\n    \n    # Get train\/test split for scoring\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        train[features], train['SalePrice'], \n        random_state = RANDOM_SEED\n    )\n    \n    # Apply preprocessing, if applicable\n    if preprocessing:\n        X_train, X_valid = preprocessing(X_train, X_valid)\n        \n    # Train random forest model\n    model = RandomForestRegressor(random_state = RANDOM_SEED)\n    model.fit(X_train, y_train)\n    valid_preds = model.predict(X_valid)\n    \n    return round(mean_absolute_error(y_valid, valid_preds), 2)","f9b81a33":"# Missing Values Strategy 1: Drop columns w\/ missing values\ndef drop_missing(X_train, X_valid):\n    \n    # Get names of columns with missing values\n    na_cols = [col for col in numerical if X_train[col].isnull().any()]\n    X_train = X_train[numerical].drop(na_cols, axis=1)\n    X_valid = X_valid[numerical].drop(na_cols, axis=1)\n    \n    return X_train, X_valid","9fb3b541":"# Test Strategy 1: Drop Columns with NAs\nprint('Strategy 1: Drop Columns with NAs:',score(preprocessing = drop_missing),'MAE\\n')","9c65cf5f":"# Missing Values Strategy 2: Impute missing values (numerical data only)\ndef impute_numerical(X_train, X_valid, strategy ='mean', drop = True):\n    \n    columns = [x for x in X_train.columns if x in numerical]\n    if drop:\n        X_train, X_valid = X_train[columns].copy(), X_valid[columns].copy()\n    else:\n        X_train, X_valid = X_train.copy(), X_valid.copy()\n\n    # impute NAs for numerical cols\n    imputer = SimpleImputer(strategy=strategy)\n    X_train[columns] = imputer.fit_transform(X_train[columns])\n    X_valid[columns] = imputer.transform(X_valid[columns])\n    \n    return X_train, X_valid\n\n# Missing Values Strategy 2.5: Impute missing values (categorical data only)\ndef impute_categorical(X_train, X_valid, strategy ='constant'):\n    \n    columns = [x for x in X_train.columns if x in categorical]\n    X_train, X_valid = X_train.copy(), X_valid.copy()\n    \n    # assert valid strategy\n    assert strategy in ['constant','most_frequent']\n    \n    # impute NAs for categorical columns\n    imputer = SimpleImputer(strategy = strategy, fill_value = 'None')\n    X_train[columns] = imputer.fit_transform(X_train[columns])\n    X_valid[columns] = imputer.transform(X_valid[columns])\n    \n    return X_train, X_valid","903c3c1c":"# Test Strategy 2.0: Impute using the mean value\npreprocessing = impute_numerical\nprint('Strategy 2.0 (Impute w\/ Mean):', score(preprocessing),'MAE\\n')\n    \n# Test Strategy 2.1: Impute using the median value\npreprocessing = partial(impute_numerical, strategy = 'median')\nprint('Strategy 2.1 (Impute w\/ Median):', score(preprocessing),'MAE\\n')\n    \n# Test Strategy 2.2: Impute using the most frequent value\npreprocessing = partial(impute_numerical, strategy = 'most_frequent')\nprint('Strategy 2.2 (Impute w\/ Mode):', score(preprocessing),'MAE\\n')","c2246593":"# Missing Values Strategy 3: Impute missing values w\/ indicator (numerical data only) \ndef impute_numerical_extended(X_train, X_valid, strategy ='mean'):\n    \n    columns = [x for x in X_train.columns if x in numerical]\n    X_train, X_valid = X_train[columns].copy(), X_valid[columns].copy()\n    \n    # 1. Get numerical columns w\/ NAs\n    na_cols = [x for x in numerical if X_train[x].isnull().any()]\n    \n    # 2. Add indicator column for missing values\n    for col in na_cols:\n        X_train[col + '_was_NA'] = X_train[col].isnull().astype(int)\n        X_valid[col + '_was_NA'] = X_valid[col].isnull().astype(int)\n    \n    # 3. Fit on training data, apply to validation set\n    imputer = SimpleImputer(strategy=strategy)\n    X_train = imputer.fit_transform(X_train)\n    X_valid = imputer.transform(X_valid)\n    \n    return X_train, X_valid\n\n# Missing Values Strategy 3.5: Impute missing values w\/ indicator (categorical data only)\ndef impute_categorical_extended(X_train, X_valid, strategy ='constant'):\n    \n    assert strategy in ['constant','most_frequent']\n    columns = [x for x in X_train.columns if x in categorical]\n    X_train, X_valid = X_train[columns].copy(), X_valid[columns].copy()\n\n    # 1. Determine categorical features with missing values\n    na_cols = [col for col in categorical if X_train[col].isnull().any()]\n    \n    # 2. Add indicator columns for imputed values\n    for col in na_cols:\n        X_train[col + '_was_NA'] = X_train[col].isnull().astype(int)\n        X_valid[col + '_was_NA'] = X_valid[col].isnull().astype(int)\n    \n    # 3. Fit imputer on training data, apply to validation set\n    imputer = SimpleImputer(strategy=strategy, fill_value = 'None')\n    X_train = imputer.fit_transform(X_train)\n    X_valid = imputer.transform(X_valid)\n    \n    return X_train, X_valid","9befc9fa":"# Test Strategy 2.0: Impute using the mean value\npreprocessing = impute_numerical_extended\nprint('Strategy 2.0 (Extended Impute w\/ Mean):', score(preprocessing),'MAE\\n')\n    \n# Test Strategy 2.1: Impute using the median value\npreprocessing = partial(impute_numerical_extended, strategy = 'median')\nprint('Strategy 2.1 (Extended Impute w\/ Median):', score(preprocessing),'MAE\\n')\n    \n# Test Strategy 2.2: Impute using the most frequent value\npreprocessing = partial(impute_numerical_extended, strategy = 'most_frequent')\nprint('Strategy 2.2 (Extended Impute w\/ Mode):', score(preprocessing),'MAE\\n')","4dc4fb39":"def drop_categorical(X_train, X_valid):\n    \n    # Drop categorical columns\n    X_train = X_train.drop(categorical, axis=1)\n    X_valid = X_valid.drop(categorical, axis=1)\n    \n    # Drop NA columns\n    na_cols = [col for col in numerical if X_train[col].isnull().any()]\n    X_train = X_train.drop(na_cols, axis=1)\n    X_valid = X_valid.drop(na_cols, axis=1)\n    \n    return X_train, X_valid","42424a37":"# Strategy 1: Drop all categorical variables\npreprocessing = drop_categorical\nprint(\"Strategy 1 (Drop categorical variables):\", score(preprocessing),'MAE\\n')","0c7535ac":"# Strategy 2: Ordinal Encoding\ndef ordinal_encoding(X_train, X_valid, verbose = True):\n    \n    # 0. Drop NA columns\n    na_cols = [col for col in features if train[col].isnull().any()]\n    X_train = X_train.drop(na_cols, axis=1)\n    X_valid = X_valid.drop(na_cols, axis=1)\n    \n    # 1. Columns which share the same values in training and validation sets\n    columns = [col for col in X_train.columns if col in categorical]\n    good_label_cols = [col for col in columns if set(X_train[col]) == set(X_valid[col])]\n    bad_label_cols = list(set(columns)-set(good_label_cols))\n    if verbose: print('Columns encoded:', len(good_label_cols))\n    if verbose: print('Columns dropped:', len(bad_label_cols), end = '\\n\\n')\n\n    # 2. Drop categorical columns that will not be encoded\n    X_train = X_train.drop(bad_label_cols, axis=1)\n    X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n    # 3. Apply ordinal encoder to \"good\" columns\n    ordinal_encoder = OrdinalEncoder()\n    X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\n    X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])     \n    \n    return X_train, X_valid","206173aa":"# Strategy 2: Ordinal Encoding\npreprocessing = ordinal_encoding\nprint(\"Strategy 2 (Ordinal Encoding):\", score(preprocessing), 'MAE\\n')","638b9a8b":"# Strategy 3: One-Hot Encoding\ndef one_hot_encoding(X_train, X_valid, verbose = True):\n    \n    # 0. Drop NA columns\n    na_cols = [col for col in features if train[col].isnull().any()]\n    X_train = X_train.drop(na_cols, axis=1)\n    X_valid = X_valid.drop(na_cols, axis=1)\n    \n    # 1. Determine low cardinality (few unique values) categorical variables\n    columns = [col for col in X_train.columns if col in categorical]\n    good_cols = [col for col in columns if set(X_train[col]) == set(X_valid[col])]\n    low_cols = [col for col in good_cols if X_train[col].nunique() < 10]\n    bad_cols = list(set(columns)-set(low_cols))\n    if verbose: print('Columns encoded:', len(low_cols))\n    if verbose: print('Columns dropped:', len(bad_cols), end = '\\n\\n')\n    \n    # 2. Drop categorical columns that will not be encoded\n    X_train = X_train.drop(bad_cols, axis=1)\n    X_valid = X_valid.drop(bad_cols, axis=1)\n    \n    # 3. Apply one-hot encoding to low cardinality columns\n    encoder = OneHotEncoder(cols=low_cols, use_cat_names=True)\n    X_train = encoder.fit_transform(X_train)\n    X_valid = encoder.transform(X_valid)\n    \n    return X_train, X_valid","182dc7ef":"# Strategy 3: One-Hot Encoding\npreprocessing = one_hot_encoding\nprint(\"Strategy 3 (One-Hot Encoding):\", score(preprocessing), 'MAE\\n')","60138ffc":"def basic_strategy(X_train, X_test):\n    \n    # 1. Determine relevant categorical columns\n    good_cols = [col for col in categorical if set(X_train[col]) == set(X_test[col])]\n    low_cols = [col for col in good_cols if X_train[col].nunique() < 10]\n    high_cols = list(set(good_cols)-set(low_cols))\n    bad_cols = list(set(categorical)-set(good_cols))\n    \n    # 1.2 Drop irrelevant columns\n    X_train = X_train.drop(bad_cols, axis=1)\n    X_test = X_test.drop(bad_cols, axis=1)\n    \n    # 2. Impute using median value for numerical data\n    X_train, X_test = impute_numerical(\n        X_train, X_test, strategy = 'median', drop = False\n    )\n    \n    # 2.1 Impute constant string for categorical data\n    X_train, X_test = impute_categorical(\n        X_train, X_test, strategy = 'constant'\n    )\n    \n    # 3. One-Hot encode low cardinality columns\n    onehot_encoder = OneHotEncoder(cols = low_cols, use_cat_names=True)\n    X_train = onehot_encoder.fit_transform(X_train)\n    X_test = onehot_encoder.transform(X_test)\n    \n    # 3. Ordinal encode high cardinality columns\n    ordinal_encoder = OrdinalEncoder()\n    X_train[high_cols] = ordinal_encoder.fit_transform(X_train[high_cols])\n    X_test[high_cols] = ordinal_encoder.transform(X_test[high_cols]) \n    \n    return X_train, X_test","e5cde6ad":"def make_submission(preprocessing):\n    \n    # Load data\n    X_train, y_train = train[features].copy(), train['SalePrice'].copy()\n    X_test = test[features].copy()\n    \n    # Preprocessing\n    X_train, X_test = preprocessing(X_train, X_test)\n\n    # Create submission\n    model = RandomForestRegressor(random_state = RANDOM_SEED)\n    model.fit(X_train, y_train)\n    test_preds = model.predict(X_test)\n    \n    output = pd.DataFrame({'Id': test.Id,'SalePrice': test_preds})\n    output.to_csv(preprocessing.__name__ + ' submission.csv', index=False)","03ece3f3":"# Competition Strategy 1\npreprocessing = basic_strategy\n\n# Get validation score\nprint(\"Competition Strategy 1:\", score(preprocessing), 'MAE\\n')\n\n# Make submission\nif SUBMIT: \n    make_submission(preprocessing)\n    print('Created Submission.\\n')","23a64df1":"# Function for comparing pipelines\ndef score_pipeline(sklearn_pipeline):\n    \n    # Get train\/test split for scoring\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        train[features], train['SalePrice'], \n        random_state = RANDOM_SEED\n    )\n        \n    # Clones the input pipeline (creates new instance)\n    pipeline = clone(sklearn_pipeline)\n    pipeline.fit(X_train, y_train)\n    valid_preds = pipeline.predict(X_valid)\n    \n    return round(mean_absolute_error(y_valid, valid_preds), 2)","cb8a77a3":"# Basic pipeline from the notes, as is.\ndef basic_pipeline():\n    \n    # 1. Preprocessing for numerical data\n    numerical_transformer = SimpleImputer(strategy='constant')\n\n    # 2. Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder())\n    ])\n\n    # 3. Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical),\n            ('cat', categorical_transformer, categorical)\n        ])\n    \n    # 4. The model, which is the same as in the scoring_function\n    model = RandomForestRegressor(random_state = RANDOM_SEED)\n    \n    return Pipeline(steps=[('preprocessor', preprocessor),('model', model)])","cb10feed":"pipeline = basic_pipeline()\nprint(\"Basic Pipeline:\", score_pipeline(pipeline), ' MAE\\n')","d3cd399b":"# Slightly better pipeline\ndef improved_pipeline():\n    \n    # 1. Preprocessing for numerical data\n    numerical_transformer = SimpleImputer(strategy='median')\n\n    # 2. Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value = 'None')),\n        ('onehot', OneHotEncoder())\n    ])\n\n    # 3. Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical),\n            ('cat', categorical_transformer, categorical)\n        ])\n    \n    # 4. The model, which is the same as in the scoring_function\n    model = RandomForestRegressor(random_state = RANDOM_SEED)\n    \n    return Pipeline(steps=[('preprocessor', preprocessor),('model', model)])","fcacd1cb":"pipeline = improved_pipeline()\nprint(\"Improved Pipeline:\", score_pipeline(pipeline), ' MAE\\n')","99886249":"# Testing a RandomForestRegressor for various parameters using cross-validation\ndef get_cross_val_score(num_folds, num_trees):\n    \n    # Get data\n    X, y = train[numerical].copy(), train['SalePrice'].copy()\n    \n    # Default imputer (np.nan and mean value)\n    pipeline = Pipeline(steps=[\n        ('preprocessor', SimpleImputer()),\n        ('model', RandomForestRegressor(n_estimators = num_trees, random_state = RANDOM_SEED))\n    ])\n    \n    # multiply by -1 so that it matches scores from previous sections\n    scores = -1 * cross_val_score(\n        pipeline, X, y, \n        cv=num_folds, \n        scoring='neg_mean_absolute_error'\n    )\n    return round(scores.mean(), 2)","3bb16df5":"# Test 50 through 300 estimators\ndef test_estimators():\n    num_folds = 3\n    for num_trees in range(50,450,50):\n        print(\n            f'n_estimators: {num_trees}\\t {num_folds}-fold MAE:', \n            get_cross_val_score(num_folds, num_trees)\n        )","b9bb9f0b":"test_estimators()","e114cce6":"def score_xgboost(preprocessing, xgboost_model):\n    \n    # Drop high cardinality categorical variables\n    X = train[features].drop(high_cardinality, axis=1)\n    y = train['SalePrice'].copy()\n    \n    # Data structure for storing scores and times\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    \n    # K-fold cross-validation\n    kfold = KFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n        \n        # Training and Validation Sets\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        \n        # Preprocessing\n        X_train, X_valid = preprocessing(X_train, X_valid)\n        \n        # Create model\n        start = time.time()\n        model = clone(xgboost_model)\n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds=5,\n            eval_set=[(X_valid, y_valid)], \n            verbose=False\n        )\n        \n        # validation predictions\n        valid_preds = np.ravel(model.predict(X_valid))\n        scores[fold] = mean_absolute_error(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} MAE: {round(scores[fold], 5)} in {round(end-start,2)}s.')\n        time.sleep(0.5)\n    return round(scores.mean(), 2)","bd818071":"def preprocessing(X_train, X_valid):\n    \n    X_train, X_valid = impute_numerical(X_train, X_valid, drop = False)\n    X_train, X_valid = impute_categorical(X_train, X_valid)\n    X_train, X_valid = one_hot_encoding(X_train, X_valid, verbose = False)\n    \n    return X_train, X_valid","666249d3":"# Baseline XGBoost model with default settings\nmodel = XGBRegressor(random_state = RANDOM_SEED)\nprint(\"\\nMAE for XGBoost (Baseline):\", score_xgboost(preprocessing, model), end = '\\n\\n')\n    \n# Improved hyperparameters\nmodel2 = XGBRegressor(\n    random_state = RANDOM_SEED,\n    n_estimators = 500,\n    learning_rate = 0.05\n)\nprint(\"\\nMAE for XGBoost (Better):\", score_xgboost(preprocessing, model2), end = '\\n\\n')\n    \n# Worse hyperparameters\nmodel3 = XGBRegressor(\n    random_state = RANDOM_SEED, \n    n_estimators = 750, \n    learning_rate = 0.001\n)\nprint(\"\\nMAE for XGBoost (Worse):\", score_xgboost(preprocessing, model3), end = '\\n\\n')\n","033694cd":"# We use the ordinal encoder from the category encoders library\nfrom category_encoders import OrdinalEncoder\n\n# Preprocessing function which operates on test data\ndef preprocessing(X_train, X_valid, X_test):\n    \n    # Impute NAs for numerical cols\n    columns = [x for x in X_train.columns if x in numerical]\n    imputer = SimpleImputer(strategy = 'median')\n    X_train[columns] = imputer.fit_transform(X_train[columns])\n    X_valid[columns] = imputer.transform(X_valid[columns])\n    X_test[columns] = imputer.transform(X_test[columns])\n    \n    # Impute NAs for categorical cols\n    columns = [x for x in X_train.columns if x in categorical]\n    imputer = SimpleImputer(strategy = 'constant', fill_value = 'None')\n    X_train[columns] = imputer.fit_transform(X_train[columns])\n    X_valid[columns] = imputer.transform(X_valid[columns])\n    X_test[columns] = imputer.transform(X_test[columns])\n    \n    # Ordinally encode categorical columns\n    encoder = OrdinalEncoder(cols = columns)\n    X_train = encoder.fit_transform(X_train)\n    X_valid = encoder.transform(X_valid)\n    X_test = encoder.transform(X_test)\n    \n    return X_train, X_valid, X_test","d896d5d3":"def generate_submission(submit = False):\n    \n    # Drop high cardinality categorical variables\n    X = train[features].drop(high_cardinality, axis=1)\n    y = train['SalePrice'].copy()\n    \n    # Data structure for storing scores and times\n    test_preds = np.zeros((test.shape[0],))\n    scores = np.zeros(NUM_FOLDS)\n    times = np.zeros(NUM_FOLDS)\n    \n    # K-fold cross-validation\n    kfold = KFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n        \n        # Training and Validation Sets\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        X_test = test[features].drop(high_cardinality, axis=1)\n        \n        # Preprocessing\n        X_train, X_valid, X_test = preprocessing(X_train, X_valid, X_test)\n        \n        # Create model\n        start = time.time()\n        model = XGBRegressor(    \n            random_state = RANDOM_SEED,\n            n_estimators = 1000,\n            learning_rate = 0.05\n        )\n        model.fit(\n            X_train, y_train, \n            early_stopping_rounds = 10,\n            eval_set=[(X_valid, y_valid)], \n            verbose=False\n        )\n        \n        # validation predictions\n        valid_preds = np.ravel(model.predict(X_valid))\n        test_preds += model.predict(X_test) \/ NUM_FOLDS\n        scores[fold] = mean_absolute_error(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} MAE: {round(scores[fold], 5)} in {round(end-start,2)}s.')\n        time.sleep(0.5)\n    \n    if submit:\n        output = pd.DataFrame({'Id': test.Id,'SalePrice': test_preds})\n        output.to_csv(preprocessing.__name__ + ' submission.csv', index=False)\n        print('\\nCreated Submission.\\n')","1281ef76":"# change to submit = True to generate output\ngenerate_submission(SUBMIT)","691ccd1e":"## 4.3 Improved Pipeline\n\nThis is a tweaked version of the pipeline in the exercise which performs a bit better. The differences with the notes are as follows:\n\n1. We use median imputation for the numerical data \n2. We use a constant value for the categorical imputer","48d951f1":"# Lesson 6: XGBoost\n\nNote: The examples in the [XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost) notes discard high-cardinality categorical data. We create our own function for performing cross-validation so we can use `early_stopping_rounds`, which isn't possible with `cross_val_score`.\n\n## 6.1 XGBoost Scoring Function","c1c7cb0e":"# Lesson 3: Categorical Variables\n\nNote: the strategies in the [Categorical Variables](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) notes assume that all columns with missing data have been removed.\n\n1. Drop Categorical Variables\n2. Label Encode\n3. One-Hot Encode\n4. Both Label Encoding and One-Hot Encoding\n\n## Strategy 1: Drop Categorical Variables","8c7a2cae":"# Lesson 4: Pipelines\n\nNote: the strategies from the [Pipelines](https:\/\/www.kaggle.com\/alexisbcook\/pipelines) notebook assumes that all high cardinality (> 10) categorical data has been discarded. \n\n## 4.1 Scoring Function\n\nSimilar to the scoring function used in the previous section but accepts a scikit-learn [pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) as input.","d3d34d28":"## Strategy 3: One-Hot Encoding\n\nNote: I use the [category_encoders](https:\/\/contrib.scikit-learn.org\/category_encoders\/onehot.html) library rather than sklearn for this encoder. In my opinion it is more convenient and easier to use.","ba4bcae4":"## Imports","3b461a0b":"## Strategy 2: Imputing Missing Values\n\nNote: The lecture notes only covers numerical data, we include an imputer for categorical data as well.","6bf52fe3":"# Competition Submission 1\n\nMy first submission which uses only the basic techniques (e.g. no pipelines, XGBoost) from the [Missing Values](https:\/\/www.kaggle.com\/alexisbcook\/missing-values) and [Categorical Variables](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) notebooks. In particular, we do the following:\n\n1. Impute numerical data NAs with median\n2. Impute categorical data NAs with constant value (placeholder)\n3. One-Hot encode low cardinality data\n4. Ordinally encode high cardinality data\n5. Train random forest using the full training data.","ec3d5fd6":"## Submission Function","e9af3c1e":"# Lesson 2: Missing Values\n\nNote: The strategies from the [Missing Values](https:\/\/www.kaggle.com\/alexisbcook\/missing-values) notebook assume that all of the categorical data has been removed.\n\n1. Drop columns containing missing values\n2. Impute missing values using various strategies (e.g. mean, median, most-frequent)\n\n## Strategy 1: Drop columns with NAs","97346e09":"## 2. Generate Submission","c1176c08":"## Scoring Function","f0ff6139":"I hope you found this useful I plan to make a similar notebook from the [Feature Engineering](https:\/\/www.kaggle.com\/learn\/feature-engineering) course, which will build off of this notebook.","9937d8bf":"# Lesson 5: Cross Validation\n\nNote: The [Cross Validation](https:\/\/www.kaggle.com\/alexisbcook\/cross-validation) examples use only numerical data.\n\n## 5.1 Scoring\n\nThe following function performs k-fold cross-validation using a random forest model with a specified number of trees.","171e9212":"# Reference Notebook: Intermediate Machine Learning\n\nThis notebook is an attempt to summarize the coding techniques covered in the [Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) course (both the notes and the exercises) into one notebook for easier reference. If you find this useful, I have a similar notebook for the follow up course on [Feature Engineering](https:\/\/www.kaggle.com\/rsizem2\/kaggle-learn-reference-feature-engineering).","32a2eafd":"## Loading Data","9c1c5485":"## 6.2 XGBoost Preprocessing","c8857b5c":"## Strategy 2: Ordinal Encoding\n\nTransforms categorical columns to integer columns with values from 0 to N-1 where N is the number of unique values. \n\n**Note:** This is called \"label encoding\" in the notes, but the [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder) is intended for transforming the target variable (for classification problems) whereas the [OrdinalEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) is for transforming categorical variables.","f384aed2":"## 5.2 Testing Number of Estimators\n\nWe test various values for the number of estimators in a random forest model using 3-fold cross-validation:","e47f4c53":"## Strategy 2.5: Imputation (extended)\n\nThis approach was covered in the notes but not used for the exercise, we include it for the sake of completeness. In this approach missing values are imputed and for each column with missing values a new column is added with True\/False values indicating which row had missing values.","3c6159ec":"# Competition Submission\n\nIn previous versions of this notebook, I used knowledge about the columns to encode certain categorical columns in a way that preserves their natural ordering. I decided to keep things simple and use more naive feature engineering in order to keep with the spirit of the course. For the final submission, I will do the following:\n\n1. Impute missing numerical data with the median\n2. Import missing categorical data with a placeholder\n3. Encode all categorical data with an `OrdinalEncoder`\n\n## 1. Preprocessing\n","9b6e2790":"## Competition Strategy","045734c2":"## 4.2 Basic Pipeline\n\nThe pipeline from the notes does the following:\n\n1. Imputes missing numerical values with a constant\n2. Imputes categorical missing values with the most frequent value\n3. One-Hot encodes the categorical variables"}}