{"cell_type":{"a073f725":"code","aa69d607":"code","66364d3e":"code","2fa68b6d":"code","d63a46a1":"code","4e338534":"code","b33b3f30":"code","64e1c60b":"code","93c77c39":"markdown","188957f4":"markdown"},"source":{"a073f725":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa69d607":"#load data\nX = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ny=X['Survived']\nX.drop(columns=['Survived'],inplace=True)\nX_test=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","66364d3e":"def add_new_features(df):\n    map_dict={'Jonkheer':'Noble','the Countess':'Noble','Sir':'Noble','Lady':'Noble','Don':'Noble','Dona':'Noble',\n         'Major':'Military','Jonkheer':'Military','Col':'Military',\n          'Mlle':'Miss','Miss':'Miss','Mr':'Mr','Mrs':'Mrs',\n          'Master':'Master','Dr':'Dr','Capt':'Capt'\n         }\n    df['Familyname']=df.Name.str.split(',',n=1,expand=True)[0]\n    df['Title']=df.Name.str.split(',',n=1,expand=True)[1].str.split('.',n=1,expand=True)[0]\n    df['Title']=df['Title'].str.strip()\n    df['Title']=df.Title.map(map_dict)\n\n    df['husband_names']=df.Name.str.split(',',n=1,expand=True)[1].str.split('.',n=1,expand=True)[1].str.split('(',n=1,expand=True)[0]\n    df['husband_names']=df.Familyname+' '+df.husband_names.str.strip()\n    df['married']=df.groupby('husband_names')['husband_names'].transform('count')\n    #X['married']=X[X.husband_names.isin(X.husband_names.value_counts()[X.husband_names.value_counts()==2].index)]\n    df['married']=(df['married']==2)\n    #X['married']=X['married'].fillna(0)\n    #if married=1, if not married=0\n\n    #Let's create another one of child with parent :D\n    df['child']=df.groupby('Familyname')['Familyname'].transform('count')\n    df['child']= (df.child>2)&(df.married==0)\n    return df\n\nX=add_new_features(X)\nX_test=add_new_features(X_test)\nprint(X.columns)\nprint(X_test.columns)","2fa68b6d":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBClassifier\n\ncat_cols=['Pclass', 'Sex',  \n       'Ticket',  'Cabin', 'Embarked', 'Familyname', 'Title',\n       'husband_names', 'married', 'child']\nnum_cols=['Age','SibSp', 'Parch','Fare']\n\n#Processing for the categorical data\ncategorical_transformer=Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                        ('onehot',OneHotEncoder(sparse=False,handle_unknown='ignore'))\n                                       ])\n#numerical data\nnumerical_transformer=Pipeline(steps=[('imputer',SimpleImputer(strategy='mean'))\n                                     ])\n#make my preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols),\n        ('cat', categorical_transformer, cat_cols)\n    ])\n\nmy_model = XGBClassifier(n_estimators=500)\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_model)\n                     ])","d63a46a1":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X[cat_cols+num_cols], y, \n                                                    train_size=0.8, test_size=0.2,\n                                                    random_state=0)\n\nmy_pipeline.fit(X_train,y_train)\nprint(\"XGB train accuracy: %0.3f\" % my_pipeline.score(X_train, y_train))\nprint(\"XGB test accuracy: %0.3f\" % my_pipeline.score(X_valid, y_valid))\n","4e338534":"#Permutation importance\nfrom sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\n\nresult = permutation_importance(my_pipeline, X_valid, y_valid, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_valid.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","b33b3f30":"#Removing all columns worse than 'Embarked'\ncat_cols2=['Pclass', 'Sex',  \n         'Cabin', 'Embarked', 'Title']\nnum_cols2=['Age', 'Fare']\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols2),\n        ('cat', categorical_transformer, cat_cols2)\n    ])\n\n\nmy_model = XGBClassifier(n_estimators=500)\nX_train, X_valid, y_train, y_valid = train_test_split(X[cat_cols2+num_cols2], y, \n                                                    train_size=0.8, test_size=0.2,\n                                                    random_state=0)\n\nmy_pipeline2 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_model)\n                     ])\nmy_pipeline2.fit(X_train,y_train)\nprint(\"XGB train accuracy: %0.3f\" % my_pipeline2.score(X_train, y_train))\nprint(\"XGB test accuracy: %0.3f\" % my_pipeline2.score(X_valid, y_valid))","64e1c60b":"cat_cols3=['Pclass', 'Sex', 'Title']\nnum_cols3=['Age']\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols3),\n        ('cat', categorical_transformer, cat_cols3)\n    ])\n\n#let's start with n_estimators=500. Will finetune later\nmy_model = XGBClassifier(n_estimators=500)\nX_train, X_valid, y_train, y_valid = train_test_split(X[cat_cols3+num_cols3], y, \n                                                    train_size=0.8, test_size=0.2,\n                                                    random_state=0)\n\nmy_pipeline3 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_model)\n                     ])\nmy_pipeline3.fit(X_train,y_train)\nprint(\"XGB train accuracy: %0.3f\" % my_pipeline3.score(X_train, y_train))\nprint(\"XGB test accuracy: %0.3f\" % my_pipeline3.score(X_valid, y_valid))\n\n#Permutation importance\nfrom sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\n\nresult = permutation_importance(my_pipeline3, X_valid, y_valid, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_valid.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","93c77c39":"Loading the data","188957f4":"Time to create our XGboost model - and the associated pipelines"}}