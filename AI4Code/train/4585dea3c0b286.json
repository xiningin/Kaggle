{"cell_type":{"30c530b9":"code","a27794ce":"code","d8c64dfe":"code","926148f2":"code","290fbe6a":"code","ee452459":"code","bc9ba533":"code","6836eb27":"code","3aefbe20":"code","e95b7e91":"code","60810663":"code","5c39e495":"code","6b9ceac2":"code","cbd9eaf8":"code","70c61ad3":"code","d8eabcb1":"markdown","d648f9a6":"markdown","de78ce04":"markdown","667e4869":"markdown"},"source":{"30c530b9":"import os # accessing directory structure\nprint(os.listdir('..\/input\/property-sales'))","a27794ce":"# We will clean, explore and visualise the raw data first\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\ndf=pd.read_csv('..\/input\/property-sales\/raw_sales.csv')\ndf=df[df.propertyType=='house'] #Let's limit the scope of this problem to houses only\ndf['datesold'] = pd.to_datetime(df['datesold'])\ndf=df.drop(columns=['postcode', 'propertyType'])\ndf = df[np.abs(df.price - df.price.mean()) <= (5.0 * df.price.std())] # Clean the outliers\n%matplotlib inline \nplt.figure(figsize=(15,5))\nplt.plot_date(df['datesold'], df['price'], xdate=True, markersize=1)","d8c64dfe":"# We will group and visualise the data by the number of bedrooms\nimport seaborn\nfrom  matplotlib import pyplot\ndf=df[df.bedrooms>1] # 0 and 1 bedrooms are not relevant to houses and should be discarded\n_bedrooms=df['bedrooms'].unique().sort()\nfg = seaborn.FacetGrid(data=df, hue='bedrooms', hue_order=_bedrooms, aspect=2, height=8)\nfg.map(pyplot.scatter, 'datesold', 'price', alpha=.7, s=5).add_legend()\n#This data may be useful for forecasts via a neural network.","926148f2":"#The data in ma_lga_12345.csv has been resampled to quartely intervals with a median aggregator outside of this notebook\n#We will load it and visualise it first\ndf=pd.read_csv('..\/input\/property-sales\/ma_lga_12345.csv')\ndf=df[df.type=='house'] #Let's limit the scope of this problem to houses only\ndf['saledate'] = pd.to_datetime(df['saledate'])\ndf.tail()","290fbe6a":"#Pivot the data so we can feed it into the model\ndf=df.pivot(index='saledate', columns='bedrooms', values='MA').interpolate(method='linear', limit_direction='both')\ndf.tail()","ee452459":"#Plot the data\n%matplotlib inline \ndf.plot(figsize=(15,5))\n#It is evident that 2 bedroom curve before 2009 is not an accurate representation of the actual median price.\n#It is not possible for a 2 bedroom median price to be above that of 3 bedroom median price.\n#This is due to low number of sales in that timeframe, which skews the calculated median price.","bc9ba533":"# Let's see what we can do to correct the overlapping lines for 2 and 3 br data\ndef separate_series(df):\n    columns = list(df) \n    for col in columns: \n        if col== columns[-1]:\n            break\n        #Calculate average difference between 2 and 3 bedrooms for the recent  1\/3 of the dataframe\n        diff_mean= (df[col+1][:-int(len(df)\/3)]-df[col][:-int(len(df)\/3)]).mean()\n        #Where 2 br price is higher than that of 3 br, replace it with 3 br price minus the diff\n        #do_they_intersect = False if df[col].loc[df[col] >= df[col+1], ].empty else True\n        #if do_they_intersect:\n        df.loc[df[col] > df[col+1]-diff_mean, col] = df[col+1]-diff_mean\n    return df\ndf=separate_series(df)\ndf.plot(figsize=(15,5))","6836eb27":"#if you are after monthly frequency we can resample the quarterly data to monthly by\ndf_monthly = df.resample('M').interpolate(method='linear', limit_direction='both').astype(int)\n#if you are after weekly or daily just replace 'M' with 'W' or 'D' respectively\ndf_monthly.tail()\ndf_monthly.plot(figsize=(15,5))","3aefbe20":"#Coint Johansen test for all # bedrooms columns\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\ncoint_johansen(df,-1,1).eig","e95b7e91":"#Split the data into train and test\ntrain = df[:int(0.9*(len(df)))]\ntest = df[int(0.9*(len(df))):]\n\n#Fit the model\nfrom statsmodels.tsa.vector_ar.var_model import VAR\n\nmodel = VAR(endog=train, freq='Q-DEC')\nmodel_fit = model.fit()\n\n#Forecast based on train data\nforecast = model_fit.forecast(model_fit.endog, steps=len(test))","60810663":"#Convert forecast data to a dataframe we can use\ncols = df.columns\npred = pd.DataFrame(index=test.index, data=forecast,columns=[cols])\npred=pred.astype(int)\npred.tail()","5c39e495":"#Plot actuals (df) and forecast (pred) on the same chart\nax = df.plot()\npred.plot(ax=ax,figsize=(15,5))","6b9ceac2":"#Show percentage difference between the last period of the forecast and actual series. Less is better.\n((pred.iloc[-1].values-pred.iloc[0].values)\/df.iloc[-1].values)*100","cbd9eaf8":"#Here is the mean absolute percentage error\nimport numpy as np\nfor col in df.columns:\n    print (str(col) +' bedrooms ' + str(np.mean(np.abs((df[col].iloc[-len(pred):].values - pred[[col]].values) \/ df[col].iloc[-len(pred):].values)) * 100))","70c61ad3":"#Forecast actuals\nmodel = VAR(endog=df, freq='Q-DEC')\nmodel_fit = model.fit()\nforecast_period=8\nprediction = model_fit.forecast(model_fit.endog, steps=forecast_period)\ncols = df.columns\nforecast_index = pd.DatetimeIndex(start ='2019-09-30', freq ='Q', periods=forecast_period) \npred = pd.DataFrame(index=forecast_index, data=prediction,columns=[cols])\nax = df.plot()\npred.plot(ax=ax,figsize=(15,5))","d8eabcb1":"We will stop here for now and move on to resampled quarterly data","d648f9a6":"**Next** is up to you. What can you do to improve accuracy of this forecast?","de78ce04":"**Dataset 1:** raw_sales.csv contains individual sales data. There are aproximately 30,000 sales recorded in the period between years 2008-2019.<br>\n**Dataset 2:** ma_lga_12345.csv contains data resampled using Median Price Moving Average (MA) in Quarterly Intervals.<br>\n<br>\n**Objective:** Forecast MA for 8 future intervals for all # of bedrooms series using a multivariate forecasting model of your choice.<br>\n<br>\n**Guiding Point 1:** Try at least 3 different models and recommend one that yields the best results. It is ok to include a variant of the VAR model or improve the VAR parameters in this notebook in one instance of the three. One of the models should use a neural network.<br>\n**Guiding Point 2:** The forecast should be done on the 90% train and validated against the 10% test set. Test set should be witheld from the model and only be used to produce the MAPE scores.<br> \n**Guiding Point 3:** The train forecast should be compared with test set and MAPE values presented for every model must be below those of the VAR model values in this notebook. ","667e4869":"Can you improve it so it is under 5% for all series without over-fitting the model?"}}