{"cell_type":{"ba29f348":"code","7962c81a":"code","ea1f3fb6":"code","3374ce1e":"code","4eb966e3":"code","70684c01":"code","c6d14550":"code","12afe52e":"code","b2d7be40":"code","2ece02bb":"code","015878e9":"code","57b2d967":"code","65d8db7d":"code","a0f6ead3":"code","107ffc45":"code","d397c513":"code","f35a9971":"code","997b28a3":"code","3f8c7407":"code","3046e184":"code","b4660c23":"code","6dd556a6":"code","991e22e1":"code","04ccfeed":"code","a0a8b0a2":"code","e948eb90":"markdown","b361aa7b":"markdown","30d2079f":"markdown","660c11f4":"markdown","3817f2b5":"markdown","40759065":"markdown","8831bf95":"markdown","b5485836":"markdown","fa542832":"markdown","2dabaa8c":"markdown"},"source":{"ba29f348":"for line in open('\/kaggle\/input\/liardataset\/README', 'r').readlines():\n    print(line.strip('\\n'))","7962c81a":"import pandas as pd # https:\/\/pandas.pydata.org\/","ea1f3fb6":"def read_dataframe(tsv_file: str) -> pd.DataFrame:\n    \n    # creates a \"dataframe\" or \"df\" for short. This is similar to a 2-D python dict.\n    df = pd.read_csv(tsv_file, delimiter='\\t', dtype=object)\n    \n    # replaces all \"null\" or \"NaN\" values with an empty string\n    df.fillna(\"\", inplace=True)\n    \n    # labels the columns in the dataset using the data dictionary described in the README\n    df.columns = [\n        'id',                # Column 1: the ID of the statement ([ID].json).\n        'label',             # Column 2: the label.\n        'statement',         # Column 3: the statement.\n        'subjects',          # Column 4: the subject(s).\n        'speaker',           # Column 5: the speaker.\n        'speaker_job_title', # Column 6: the speaker's job title.\n        'state_info',        # Column 7: the state info.\n        'party_affiliation', # Column 8: the party affiliation.\n        \n        # Column 9-13: the total credit history count, including the current statement.\n        'count_1', # barely true counts.\n        'count_2', # false counts.\n        'count_3', # half true counts.\n        'count_4', # mostly true counts.\n        'count_5', # pants on fire counts.\n        \n        'context' # Column 14: the context (venue \/ location of the speech or statement).\n    ]\n    \n    return df\n\n#create a dataframe from the training data\ndf = read_dataframe('\/kaggle\/input\/liardataset\/train.tsv')","3374ce1e":"df.info()","4eb966e3":"df.head(10)","70684c01":"def print_row(input_df: pd.DataFrame, index: int) -> None:\n    \"\"\"Most relevant columns for today's analysis\"\"\"\n    print(f\"speaker: {input_df.iat[index, 4]}\")\n    print(f\"subject(s): {input_df.iat[index, 3]}\")\n    print(f\"statement: {input_df.iat[index, 2]}\")\n    print(f\"label: {input_df.iat[index, 1]}\")","c6d14550":"print_row(df, 1)","12afe52e":"print_row(df, 10000)","b2d7be40":"import seaborn as sns # https:\/\/seaborn.pydata.org\/\nsns.set()\n\n\ndef label_bar_chart(input_df: pd.DataFrame, title: str = \"LIAR Dataset\") -> None:\n    \n    # computes frequencies of labels and converts to percentages\n    label_frequencies = input_df['label'].value_counts(normalize=True)\n    \n    def multiply_100(x):\n        return x * 100\n    \n    # \"apply\" is a handy way to call a function on every row of data.\n    label_frequencies = label_frequencies.apply(multiply_100)\n    \n    # bar chart ordering and  colors for readability.\n    labels = ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']\n    colors = [\n        'orangered', # pants-fire\n        'coral', # false\n        'salmon', # barely-true\n        'peachpuff', # half-true\n        'skyblue', # mostly-true\n        'deepskyblue' # true\n    ]\n    \n    label_frequencies = label_frequencies.reindex(index = labels)\n    \n    \n    # creates a horizontal bar chart with a descriptive title\n    axis = label_frequencies.plot(kind='barh', figsize=(12, 8), color=colors)\n    axis.set_title(f\"distribution of label values ({title}, sample_size={len(input_df)})\", size=20);\n    \n\n# create bar chart over labels for the entire LIAR dataset.\nlabel_bar_chart(df)","2ece02bb":"num_unique_speakers = df['speaker'].nunique()\navg_statments = len(df) \/ num_unique_speakers\n\nprint(f\"Unique speakers in dataset: {num_unique_speakers}\")\nprint(f\"Average statements made per speaker: {avg_statments}\")","015878e9":"# 'barack-obama', 'bernie-s', 'mitt-romney', 'hillary-clinton', 'tom-cotton', 'north-korea', 'donald-trump', 'joe-biden', 'mitch-mcconnell'\n# list(df['speaker'].unique())\n\ndef speaker_bar_chart(input_df: pd.DataFrame, speaker: str) -> pd.DataFrame:\n    \n    # filters the input_df\n    speaker_df = input_df[ input_df['speaker'] == speaker ]\n    speaker_df.reset_index(inplace=True)\n    \n    # calls the bar chart function we created earlier\n    label_bar_chart(speaker_df, title=speaker)\n    \n    return speaker_df\n    \n\n# create a bar chart over labels for a single speaker. Store the returned dataframe to a variable.\nret_df = speaker_bar_chart(df, 'barack-obama')","57b2d967":"# ret_df[ ret_df['label'] == 'pants-fire' ]\n# ret_df","65d8db7d":"# print_row(ret_df, 0)","a0f6ead3":"# this creates a new dataframe that only contains the label and subjects columns.\nsubject_df = df[['label', 'subjects']].copy()\n\n# # start by turning the comma-separated values into a list of values\ndef csv_to_list(x):\n    return x.split(\",\")\n\nsubject_df['subjects'] = subject_df['subjects'].apply(csv_to_list)\n\nsubject_df = subject_df.explode('subjects')\n\nsubject_df.head()","107ffc45":"num_unique_subjects = subject_df['subjects'].nunique()\n\nprint(f\"Unique subjects in dataset: {num_unique_subjects}\")","d397c513":"# 'energy', 'poverty', 'taxes', 'climate-change', 'obama-birth-certificate'\n# list(subject_df['subjects'].unique())\n\ndef subject_bar_chart(input_df: pd.DataFrame, subject: str) -> pd.DataFrame:\n    \n    # filters the input_df\n    ret_df = input_df[ input_df['subjects'] == subject ]\n    ret_df.reset_index(inplace=True)\n    \n    # calls the bar chart function we created earlier\n    label_bar_chart(ret_df, title=subject)\n    \n    return ret_df\n    \n\n# create a bar chart over labels for a single subject. Store the returned dataframe to a variable.\nret_df = subject_bar_chart(subject_df, 'poverty')","f35a9971":"df.head()","997b28a3":"from typing import Dict\n\n\ndef score_label(label: str) -> float:\n    scores = {\n        'true': 2,\n        'mostly-true': 1,\n        'half-true': 0,\n        'barely-true': -1,\n        'false': -2,\n        'pants-fire': -3\n    }\n    return scores[label]\n\n\ndef model_speaker_statements(input_df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n    \n    tot = {}\n    count = {}\n    \n    # for each row in the input_df\n    for _, row in input_df.T.items():\n        \n        # grab the pertinent information\n        speaker = row['speaker']\n        score = score_label(row['label'])\n        words = row['statement'].lower().split()\n        \n        # prevents double counting of words\n        uniques = []\n        \n        for word in words:\n            # if its a word we've already seen, skip processing\n            if word in uniques:\n                continue\n                    \n            uniques.append(word)\n            \n            if speaker in tot:\n                if word in tot[speaker]:\n                    tot[speaker][word] += score\n                    count[speaker][word] += 1\n                else:\n                    tot[speaker][word] = score\n                    count[speaker][word] = 1\n                    \n            else:\n                tot[speaker] = {}\n                tot[speaker][word] = score\n                \n                count[speaker] = {}\n                count[speaker][word] = 1\n                \n    \n    # compute averages and return model\n    for speaker in tot:\n        for word in tot[speaker]:\n            tot[speaker][word] = tot[speaker][word] \/ count[speaker][word]\n            \n    return tot\n\n\ndef score_speaker_statement(model: Dict[str, Dict[str, float]], speaker: str, phrase: str) -> float:\n        \n    idx = 0\n    score = 0\n    for word in phrase.split():\n        word = word.lower()\n        \n        if word in model[speaker]:\n            score += model[speaker][word]\n            idx += 1\n            \n    if not idx:\n        return None\n    \n    return score \/ idx\n                                        \n\ndef sentiment_speaker_statement(model: Dict[str, Dict[str, float]], speaker: str, phrase: str) -> str:\n\n    score = score_speaker_statement(model, speaker, phrase)\n\n    if score is None:\n        return None\n    \n    if score <= 0:\n        return False\n    \n    return True","3f8c7407":"from IPython.display import Image\n\n# build a model from the liar dataset\nmodel = model_speaker_statements(df)","3046e184":"outcome = sentiment_speaker_statement(model, 'joe-biden', \"A new, independent study put out last week found that at least 55 of our largest corporations used various loopholes to pay zero federal income tax in 2020.\")\nprint(f\"Model outcome: {outcome}\\n\")\n\nImage(\"..\/input\/liarscreenshots\/Screen Shot 2021-04-18 at 10.27.43 PM.png\") # https:\/\/www.politifact.com\/personalities\/joe-biden\/","b4660c23":"outcome = sentiment_speaker_statement(model, 'donald-trump', \"The unemployment rate for Wisconsin workers has reached historic lows. It\u2019s never been this low before, ever, ever, ever.\")\nprint(f\"Model outcome: {outcome}\\n\")\n\nImage(\"..\/input\/liarscreenshots\/Screen Shot 2021-04-18 at 10.56.43 PM.png\") # https:\/\/www.politifact.com\/personalities\/donald-trump\/","6dd556a6":"outcome = sentiment_speaker_statement(model, 'mitch-mcconnell', \"Clearly, the Obama administration did not leave any kind of game plan for something like this.\")\nprint(f\"Model outcome: {outcome}\\n\")\n\nImage(\"..\/input\/liarscreenshots\/Screen Shot 2021-04-18 at 10.31.34 PM.png\") # https:\/\/www.politifact.com\/personalities\/mitch-mcconnell\/","991e22e1":"outcome = sentiment_speaker_statement(model, 'kamala-harris', \"The cost of living is going up, but paychecks aren't keeping up.\")\nprint(f\"Model outcome: {outcome}\\n\")\n\nImage(\"..\/input\/liarscreenshots\/Screen Shot 2021-04-18 at 11.06.44 PM.png\")","04ccfeed":"outcome = sentiment_speaker_statement(model, 'bernie-s', \"Trade agreements like NAFTA and permanent normal trade relations with China, which forced American workers to compete against people making pennies an hour, has resulted in the loss of 160,000 jobs here in Michigan.\")\nprint(f\"Model outcome: {outcome}\\n\")\n\nImage(\"..\/input\/liarscreenshots\/Screen Shot 2021-04-18 at 11.08.40 PM.png\")","a0a8b0a2":"import json\n\nwith open('\/kaggle\/working\/model.json', 'w') as fp:\n    json.dump(model, fp)","e948eb90":"# LIAR Dataset\n* https:\/\/www.politifact.com\/\n* https:\/\/paperswithcode.com\/about\n* https:\/\/paperswithcode.com\/paper\/liar-liar-pants-on-fire-a-new-benchmark\n* https:\/\/paperswithcode.com\/dataset\/liar\n\n_LIAR is a publicly available dataset for fake news detection. A decade-long of 12.8K manually labeled short statements were collected in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. The LIAR dataset4 includes 12.8K human labeled short statements from POLITIFACT.COM\u2019s API, and each statement is evaluated by a POLITIFACT.COM editor for its truthfulness._","b361aa7b":"# Data Ingestion\n\n`pandas` is an open-source python library built for data manipulation and analysis. It is part of the standard library for many teams of data scientists and engineers. Sponsored by [NumFOCUS](https:\/\/numfocus.org\/), `pandas` is used by academics researchers and private industry for importing and cleansing data, transforming data, creating visualizations, time-series analysis, machine learning, etc... (the list goes on...)\n\n`pandas` introduces new types (e.g. `pandas.DataFrame`, `pandas.Series`) that have special syntax for data manipulation that are not shared with python's builtin types (e.g. `list`, `dict`). Some of the new syntax can look jarring at first, but is _lingua franca_ for many data researchers.\n\n","30d2079f":"We can filter a `pandas.DataFrame` by specific values. However, it uses non-native python syntax which can look pretty foreign at first, ","660c11f4":"<br \/>\n\n## Speaker Analysis\nLet's take a look at the speaker column now. In order for this data to be super useful, we want a large breadth of speakers. Otherwise, the dataset might be too narrow for general applicability. It might also be nice to look at label distribution for a given speaker. \n\n`pandas` has a lot of builtin methods to help slice and dice datasets. `pandas.Series.nunique()` gives us the number of unique values for a column. We can treat `pandas.DataFrame` like a list and call `len(df)` to get the total number of rows. \n","3817f2b5":"Now we can perform the same analysis on the subject column, ","40759065":"<br \/>\n\n## Subject Analysis\nLet's do for subjects what we did for speakers. Our `label_bar_chart` function doesn't care how the input data is sliced (as long as it has a \"labels\" column!), so we can re-use that here. \n\n**Note**: There can be multiple subjects per row of data, so we'll need to account for that. We need to _transform_ our dataset, using `pandas`, in order to feed it into `label_bar_chart`. \n\n\nHandling columns of data that have multiple values is such a common operation in data analysis that there is a builtin function for this: `pandas.DataFrame.explode`!","8831bf95":"# Revisiting the Sentiment Analysis Lab\nOften, the work of a data scientist takes them beyond analysis and into the realm of \"modeling\". A \"model\" is a representation of data that typically helps answer a particular question. You've already encountered modeling this semester in two labs: \"Diagnosing Heart Disease\" and \"Sentiment Analysis\". \n\nIn the \"Sentiment Analysis\" lab, you labeled words and phrases as \"Positive\", \"Negative\", or \"Unknown\" by building a model from a movie reviews dataset. We are going to apply the same modeling technique here to predict **Truthiness**.\n","b5485836":"# Analysing the data\nWith a `pandas.DataFrame` in hand, we can start asking questions about the data. This is called \"Exploratory Data Analysis\" (a.k.a. \"EDA\") and is a critical step in a data scientist's workflow. It is important because we need to understand the nature of our data in order to draw conclusions from it. Some questions we'll answer today, \n\n* What is the distribution of labels in this data? How many \"mostly-true\", \"half-true\", etc? \n* How many distinct subjects are there? Distinct speakers?\n\n<br \/>\n\n## Label Distribution & \"Truthiness\"\n_These definitions were taken from  [PolitiFact's \"truth-o-meter\" methodology](https:\/\/www.politifact.com\/article\/2018\/feb\/12\/principles-truth-o-meter-politifacts-methodology-i\/#Truth-O-Meter%20ratings) page_\n\n1. **true** \u2013 The statement is accurate and there\u2019s nothing significant missing.\n2. **mostly-true** \u2013 The statement is accurate but needs clarification or additional information.\n3. **half-true** \u2013 The statement is partially accurate but leaves out important details or takes things out of context.\n4. **barely-true** \u2013 The statement contains an element of truth but ignores critical facts that would give a different impression.\n5. **false** \u2013 The statement is not accurate.\n6. **pants-fire** \u2013 The statement is not accurate and makes a ridiculous claim. a.k.a. \"Liar, Liar, Pants on Fire!\"\n\n<br \/>\n\n`pandas` has builtin methods that wrap `matplotlib` visualization libraries. Using these builtin methods, it is fairly straightforward to make charts from a `pandas.DataFrame`.","fa542832":"<br \/>\n\n## Spot Check\n\nNow let's generate our model and test it against some recent statements that are not a part of our training dataset.","2dabaa8c":"## Exporting our model"}}