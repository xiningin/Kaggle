{"cell_type":{"cf4154cc":"code","4df04375":"code","67f9841f":"code","463e3bc6":"code","4456c008":"code","027f8db7":"code","509b9d84":"code","3a00af41":"code","de76bbfa":"code","4e0e428b":"code","1132d9af":"code","66861c67":"code","8b1c738c":"code","e54c50be":"code","56aa5eb4":"code","29da6d8b":"code","27b59605":"code","3e622ec1":"code","fab16fb9":"code","a0551de0":"code","67a9ec7f":"code","27424256":"code","e387a0dc":"code","7803b388":"code","06cd8532":"code","a3a8bf96":"code","f8ba8c97":"markdown","9d3752e5":"markdown","226d391f":"markdown","5fb832b2":"markdown","1e583cbd":"markdown","ee9042be":"markdown","a1be85d3":"markdown","9c690bd4":"markdown","28a1f93d":"markdown","ff689244":"markdown","0857994b":"markdown","1d511a2e":"markdown","dc625922":"markdown","a2462345":"markdown","a1bcca40":"markdown","0e55345c":"markdown","f3de317b":"markdown","81f14eda":"markdown","62bdb67c":"markdown","470644ea":"markdown"},"source":{"cf4154cc":"pip install pmdarima","4df04375":"import pandas as pd\nfrom pandas.plotting import lag_plot\nimport numpy as np \nfrom datetime import datetime  \nimport matplotlib.pyplot as plt \nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acovf,acf,pacf,pacf_yw,pacf_ols\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.tsa.stattools import adfuller,kpss,coint,bds,q_stat,grangercausalitytests,levinson_durbin\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.ar_model import AR,ARResults\nfrom statsmodels.tsa.arima_model import ARIMA, ARMA\nfrom pmdarima import auto_arima\nfrom statsmodels.tools.eval_measures import mse, rmse, meanabs\nimport statsmodels.api as sm\nimport warnings\nfrom pylab import rcParams\nfrom fbprophet import Prophet\nfrom statsmodels.tools.eval_measures import rmse\nfrom fbprophet.diagnostics import cross_validation, performance_metrics\nfrom fbprophet.plot import plot_cross_validation_metric\nimport seaborn as sns\nimport cufflinks as cf\nimport plotly.offline\n\n\n\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n%matplotlib inline\nrcParams['figure.figsize'] = 22,12    # to set a common figsize for all plots\nwarnings.filterwarnings('ignore')\n","67f9841f":"def adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print(f'Augmented Dickey-Fuller Test: {title}')\n    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out[f'critical value ({key})']=val\n        \n    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n        print('\\n\\n')\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")\n        print('\\n\\n')\n\n","463e3bc6":"data = pd.read_csv('..\/input\/electricity-prices-germany-epex\/Electricity_Prices_Germany_EPEX.csv', index_col='dates', parse_dates=True)\ndata['dates'] = data.index # we will use dates column later in the process, here we ensure it's of datetime format\ndata.index.freq = 'D' # daily prices \ndata['week day'] = data.index.day_name() # weekdays for informative plottings\ndata['daily mean'] = data.mean(axis=1) # means of daily prices (24h)","4456c008":"plt.figure(1)\nlag_plot(data['daily mean']);  # upward trend of y_t wrt lagged y_t-1 supports the idea of non-stationarity\nplt.figure(2)\nplot_acf(data['daily mean'],title='Autocorrelation',lags = 31); # There are a large number of lags before ACF values drop off and falls within Confidence Interval\nplt.figure(3)\ndecompose = seasonal_decompose(data['daily mean'],model='additive') \ndecompose.plot(); # plot decompose.trend\/seasonal\/resid separately if you wish to take a closer look\nadf_test(data['daily mean'],title='ADF test of  Daily Mean values') # to be totally ensured upon Augmented Dickey-Fuller test's results","027f8db7":"# Differencing over daily means and log-transformed daily means \ndata['daily mean 1-diff'] = diff(data['daily mean'], k_diff=1)\ndata['logged'] = data['daily mean'].apply(lambda x : np.log(x)) \ndata['logged 1-diff'] = diff(data['logged'], k_diff=1)\ndata['daily mean 1-diff'].fillna(method='bfill',inplace=True)\ndata['logged 1-diff'].fillna(method='bfill',inplace=True)\n\n\nplt.figure(5)\ndata[['daily mean','daily mean 1-diff','logged','logged 1-diff']].plot()\n\n# finally we can check whether our data has truly become stationary\n\nadf_test(data['daily mean 1-diff'],title='ADF test of  Daily Mean 1-Differenced')\nadf_test(data['logged 1-diff'],title='ADF test of Logged Daily Mean 1-Differenced')\n\n# and let's print the entire dataset\nprint(data)","509b9d84":"plt.figure(1)\ncorr = data.corr()# plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True)); ","3a00af41":"figure1, axes1 = plt.subplots(nrows=1, ncols=2)\nfigure2, axes2 = plt.subplots(nrows=1, ncols=2)\nfigure3, axes3 = plt.subplots(nrows=1, ncols=2)\nfigure4, axes4 = plt.subplots(nrows=1, ncols=2)\n\naxes1[0].hist(data['h1']);\naxes1[1].hist(data['h7']);\nfigure1.tight_layout();\n\naxes2[0].hist(data['h8']);\naxes2[1].hist(data['h17']);\nfigure2.tight_layout();\n\naxes3[0].hist(data['h18']);\naxes3[1].hist(data['h21']);\nfigure3.tight_layout();\n\naxes4[0].hist(data['h22']);\naxes4[1].hist(data['h24']);\nfigure4.tight_layout();\n","de76bbfa":"# Grid Search\ndmpd_tr = [True,False]\nseasonal_periods = [4, 7, 21, 31,90, 365]\nim = ['estimated','heuristic','legacy-heuristic']\nperiod = 365 #[7, 14, 31, 90, 365] \n#data = ['daily mean 1-diff','daily mean','logged 1-diff','logged']\n\nmetrics = []\ntotal = len(dmpd_tr)*len(seasonal_periods)*len(im)\n\nfor i in dmpd_tr:\n    for j in seasonal_periods:\n        for k in im:\n            #for l in period:\n                #for m in list(range(0,total)):\n                    train = data['daily mean'].iloc[:-period]\n                    test =  data['daily mean'].iloc[-period:]\n                \n                    fitted_model = ExponentialSmoothing(train, trend='add',seasonal='add',damped_trend = i,seasonal_periods = j,freq = 'D',initialization_method = k).fit(optimized=True)\n                    test_predictions = fitted_model.forecast(period).rename('Holt-Winters Forecast')\n                    idx = 0\n                    plt.figure(idx)\n                    test.plot(legend=True,label='TEST',c='blue',xlim=['2018-01-01','2018-12-31'])\n                    test_predictions.plot(legend=True,label='PREDICTION',xlim=['2018-01-01','2018-12-31']) #c='orange',\n                    idx = idx + 1\n\n                    # Evaluation\n                    print(test.describe())\n                    #print('MAE:',meanabs(test,test_predictions),'\\n\\n')\n                    #print('MSE:', mse(test,test_predictions),'\\n\\n')\n                    print('For parameters: \\n  damped_trend {} \\n seasonal_periods {} \\n initialization_method {} \\n period {} \\n  '.format(i,j,k,period))\n                    print('RMSE:', rmse(test,test_predictions),'\\n\\n')\n","4e0e428b":"period = 365\ntrain = data['daily mean'].iloc[:-period]\ntest =  data['daily mean'].iloc[-period:]\n\n\nfitted_model = ExponentialSmoothing(train, trend='add',seasonal='add',damped_trend = True,seasonal_periods = 365,freq = 'D', initialization_method = 'legacy-heuristic').fit(optimized=True)\ntest_predictions = fitted_model.forecast(period).rename('Holt-Winters Forecast')\ntest.plot(legend=True,label='TEST',c='blue',xlim=['2018-01-01','2018-12-31'])\ntest_predictions.plot(legend=True,label='PREDICTION',c='orange',xlim=['2018-12-01','2018-12-31'])\n\n# Evaluation\nprint(test.describe())\nprint('MAE:',meanabs(test,test_predictions),'\\n\\n')\nprint('MSE:', mse(test,test_predictions),'\\n\\n')\nprint('RMSE:', rmse(test,test_predictions),'\\n\\n')\n","1132d9af":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nauto_arima(data['daily mean'],error_action='trace').summary()  # SARIMAX(1, 1, 2) is recommended","66861c67":"''' GRID SEARCH SARIMA '''\n\nimport itertools \n\nperiod = 31 #[7,14,21,28,31,90,365]\n\np = q = [1,2,7] # p, d, and q can be either 0, 1, or 2\nd = [1,2]\npdq = list(itertools.product(p,d,q)) # gets all possible combinations of p, d, and q \np2 = q2 = [1,2,3] # second set of p's, d's, and q's\nd2 = [1,2,3]\npdq2 = list(itertools.product(p2,d2,q2)) # simular too code above but for seasonal parameters\nseason = 12\npdqs2 = [(c[0], c[1], c[2], season) for c in pdq2]\n\n\nprd = period\n#for prd in period:\nfor order in pdq:\n    for seaorder in pdqs2:\n            \n            train = data['daily mean'].iloc[:-prd]\n            test =  data['daily mean'].iloc[-prd:]\n            \n            \n            model = SARIMAX(train, order=order, seasonal_order=seaorder,enforce_stationarity=False,enforce_invertibility=False) #,trend='c',enforce_invertibility=False)\n            results = model.fit()\n            #results.plot_diagnostics()\n            print(results.summary(),'\\n\\n\\n')\n\n            # Obtain predicted values\n            start=len(train)\n            end=len(train)+len(test)-1\n            predictions = results.predict(start=start, end=end, dynamic=False, typ ='levels').rename('SARIMA(1,1,2) Predictions')\n            #Passing dynamic=False means that forecasts at each point are generated using the full history up to that point (all lagged values)\n\n            # Plotting predictions\n            idx = 1\n            plt.figure(idx)\n            test.plot(legend=True)\n            predictions.plot(legend=True)\n            idx = idx + 1\n\n            # Evaluate the model\n            #error1 = mse(test, predictions)\n            error2 = rmse(test, predictions)\n\n            #print(f'SARIMA(1,1,2) MSE Error: {error1:11.10}')\n            print('SARIMA(1,1,2) RMSE Error: {}'.format(error2))\n            \n            print('These results are for the following set of parameters \\n period {} \\n order {} \\n seasonal_order {} \\n'.format(prd,order,seaorder))","8b1c738c":"period = 31\ntrain = data['daily mean'].iloc[:-period]\ntest =  data['daily mean'].iloc[-period:]\n\nmodel = SARIMAX(train, order=(5,1,2), seasonal_order=(1,2,4,7),enforce_stationarity=False,enforce_invertibility=False) #,trend='c',enforce_invertibility=False)\nresults = model.fit()\n#results.plot_diagnostics()\nprint(results.summary())\n\n# Obtain predicted values\nstart=len(train)\nend=len(train)+len(test)-1\npredictions = results.predict(start=start, end=end, dynamic=False, typ ='levels').rename('SARIMA(1,1,2) Predictions')\n#Passing dynamic=False means that forecasts at each point are generated using the full history up to that point (all lagged values)\n\n# Plotting predictions\nplt.figure(2)\ntest.plot(legend=True)\npredictions.plot(legend=True)\n\n# Evaluate the model\nerror1 = mse(test, predictions)\nerror2 = rmse(test, predictions)\n\nprint(f' MSE Error: {error1}')\nprint(f' RMSE Error: {error2}')","e54c50be":"from sklearn.preprocessing import MinMaxScaler\n\nperiod = 31 \nscaler = MinMaxScaler()\n\ndata_fbprophet = data[['dates', 'daily mean']].copy()\ndata_fbprophet.index = np.arange(len(data_fbprophet))\ndata_fbprophet.columns = ['ds','y']\ndata_fbprophet['ds'] = pd.to_datetime(data_fbprophet['ds'])\nprint(data_fbprophet)\n\nprint(' \\n Total length of our dataset is {} so we will take last {} points as a test data, and the rest {} as a train data \\n\\n'.format(len(data_fbprophet),period,len(data_fbprophet)-period))\n\ntest = pd.DataFrame(data=data_fbprophet.iloc[-period:],index=data_fbprophet.iloc[-period:].index)\ntrain = pd.DataFrame(data=data_fbprophet.iloc[0:-period],index=data_fbprophet.iloc[0:-period].index)\n\n# scaler.fit(np.array(train['y']).reshape(2,1)) \n# traintemp = scaler.transform(train['y']) \n# testtemp = scaler.transform(test['y'])\n# test['y'] = testtemp.reshape(-1,1)\n# train['y'] = traintemp.reshape(-1,1)\n# print(train,'\\n\\n',test)\n","56aa5eb4":"from sklearn.model_selection import ParameterGrid\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\nparams_grid = {'seasonality_mode':('multiplicative','additive'),\n               'changepoint_prior_scale':[0.1,0.2,0.3,0.4,0.5],\ngrid = ParameterGrid(params_grid)\ncnt = 0\nfor p in grid:\n    cnt = cnt+1\n\nprint('Total Possible Models',cnt)\n\nstrt='2018-01-01'\nend='2018-12-31'\nmodel_parameters = pd.DataFrame(columns = ['MAPE','Parameters'])\nfor p in grid:\n    test = pd.DataFrame()\n    print(p)\n    np.random.seed(0)\n    train_model =Prophet(changepoint_prior_scale = p['changepoint_prior_scale'],\n                         seasonality_mode = p['seasonality_mode'],\n                         weekly_seasonality=True,\n                         daily_seasonality = True,\n                         yearly_seasonality = True,\n                         interval_width=0.95)\n    train_model.fit(train)\n    train_forecast = train_model.make_future_dataframe(periods=period, freq='D')#,include_history = False)\n    train_forecast = train_model.predict(train_forecast)\n    test=train_forecast[['ds','yhat']]\n    #Actual = data_fbprophet[(data_fbprophet['ds']>strt) & (data_fbprophet['ds']<=end)]\n    MAPE = mean_absolute_percentage_error(data_fbprophet['y'],abs(test['yhat']))\n    print('Mean Absolute Percentage Error(MAPE)------------------------------------',MAPE)\n    model_parameters = model_parameters.append({'MAPE':MAPE,'Parameters':p},ignore_index=True)","29da6d8b":"m = Prophet(\n    seasonality_mode = 'additive',\n    growth='linear',\n    #changepoint_prior_scale=30,\n    #seasonality_prior_scale=35,\n    daily_seasonality=False,\n    weekly_seasonality=False,\n    yearly_seasonality=False,\n    ).add_seasonality(\n    name='weekly',\n    period=7,\n    fourier_order=20\n    ).add_seasonality(\n    name = 'daily',\n    period = 1,\n    fourier_order = 15\n    )","27b59605":"Mean Absolute Percentage Error(MAPE)------------------------------------ 34.27883835814246\n{'changepoint_prior_scale': 0.4, 'holidays_prior_scale': 0.5, 'n_changepoints': 200, 'seasonality_mode': 'additive'}","3e622ec1":"# you can also specify seasonality_mode = 'multiplicative'\nm.fit(train)\n\n# placeholder to hold our future predictions \nfuture = m.make_future_dataframe(periods=period,freq='D') #!! predict 24 MONTHS(! specified) values head of the future\nforecast = m.predict(future) # go ahead and predict the future\nprint(forecast[['ds','yhat_lower','yhat_upper','yhat']].tail(period))\n\nplt.figure(1)\nm.plot(forecast); # plots the whole data + forecast using Facebook Prophet's built-in plotter\n#plt.xlim('','') # to zoom in our forecasts\n\nplt.figure(2)\nm.plot_components(forecast);","fab16fb9":"# alternative plotting \n\nax = forecast.plot(x='ds',y='yhat',label='Predictions',legend=True)\ntest.plot(x='ds',y='y',label='True Test Data',legend=True,ax=ax)","a0551de0":"# evaluating our model accuracy\n\n#def MAPE(y_true,y_pred):\n#    ''' Take in true and predicted values and calculate the MAPE score'''\n#    y_true, y_pred = np.array(y_true),np.array(y_pred)\n#    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\npredictions = forecast.iloc[-period:]['yhat']\nprint('RMSE score is {}'.format(rmse(predictions,test['y'])))\n\n# cross-validation (model evaluation) using built-in Prophet tools\n# details are explained on the Prophet Diagnostics Documentation\n\n# INITIAL TRAINING PERIOD\ninitial = 2 * 365 \ninitial = str(initial) + 'days'\n\n# PERIOD LENGTH WE ARE GOING TO PERFORM CROSS-VALIDATION\n\nperiod = 2 * 365 \nperiod = str(period) + 'days'\n\n# HORIZON OF PREDICTION \nhorizon = 365 # forecast one year ahead\nhorizon = str(horizon) + 'days'\n\ndf_cv = cross_validation(m, initial=initial,period=period,horizon=horizon)\nprint(df_cv.head())\nprint(performance_metrics(df_cv))\nplt.figure(1)\nplot_cross_validation_metric(df_cv,metric='rmse');\nplt.figure(2)\nplot_cross_validation_metric(df_cv, metric='mape');\n\n#MAPE_baseline = MAPE(df_cv.y,df_cv.yhat)\n#print(MAPE)","67a9ec7f":"from sklearn.preprocessing import MinMaxScaler\n\n# train, test new split (look at indicies)\nperiod = 31\ntrain = pd.DataFrame(index=data.index[0:-period],data=data['daily mean'].iloc[0:-period])\ntest = pd.DataFrame(index=data.index[-period:],data=data['daily mean'].iloc[-period:])\n\nscaler = MinMaxScaler()\nscaler.fit(train) \nscaled_train = scaler.transform(train) \nscaled_test = scaler.transform(test)\n\nfrom keras.preprocessing.sequence import TimeseriesGenerator\n\n# define generator\nn_input = 30\nn_features = 1\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)\n# What does the first batch look like?\nX,y = generator[0]\n#print(f'Given the Array: \\n{X.flatten()}')\n#print(f'Predict this y: \\n {y}')\n\n# Creating the model \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', input_shape=(n_input, n_features)))  # how to choose 100 ?\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nprint(model.summary(),'\\n\\n')\n\nmodel.fit_generator(generator,epochs=10)\nmodel.history.history.keys()\nloss_per_epoch = model.history.history['loss']\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch)","27424256":"# Evaluation on test data\ntest_predictions = []\n\nfirst_eval_batch = scaled_train[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n    \n# invert scaling \ntrue_predictions = scaler.inverse_transform(test_predictions)\ntest['Predictions'] = true_predictions\ntest.plot()","e387a0dc":"df3 = pd.read_csv('..\/Data\/samples.csv',index_col=0,parse_dates=True)\ndf3.index.freq = 'MS'\ndf3[['a','d']].plot(figsize=(16,5));","7803b388":"df3['a'].iloc[2:].plot(figsize=(16,5),legend=True);\ndf3['d'].shift(2).plot(legend=True);","06cd8532":"# Add a semicolon at the end to avoid duplicate output\ngrangercausalitytests(df3[['a','d']],maxlag=3);","a3a8bf96":"# Add a semicolon at the end to avoid duplicate output\ngrangercausalitytests(df3[['b','d']],maxlag=3);","f8ba8c97":"It's hard to tell from this overlay but <tt>df['d']<\/tt> almost perfectly predicts the behavior of <tt>df['a']<\/tt>.<br>\nTo see this more clearly (spoiler alert!), we will shift <tt>df['d']<\/tt> two periods forward.","9d3752e5":"To make plots visually more readable, I plotted first and last hours within the ranges listed above. You can check those in between \nto be ensured that distribution is the same. These plots are important since they illustrate which hours are correlated with the others. ","226d391f":"## STATISTICAL MODELS (Holt-Winters,AR, ARIMA, SARIMA,VAR)","5fb832b2":"# Facebook Prophet Model\n\nhttps:\/\/towardsdatascience.com\/implementing-facebook-prophet-efficiently-c241305405a3\n\n","1e583cbd":"## ARIMA(p,d,q) --> need some help with seasonal_order parameter tuning","ee9042be":"### Run the test\nThe function takes in a 2D array [y,x] and a maximum number of lags to test on x. Here our y is column 'a' and x is column 'd'. We'll set maxlags to 3.","a1be85d3":"We will perform much more detailed EDA soon, it was just the good time to look at these plots.\nNow we add a few more columns with detrended and deseasonalized values.","9c690bd4":"# NON-STATISTICAL MODELS (RNN-LSTM, Facebook Prophet)","28a1f93d":"From the heatmap and related plots below, we can make a few potentially useful observations:\n\n- h1 to h7 are highly correlated, which means we can rely more on h1-h7 range while predicting the day-ahead price at these hours. Also, from histogram distributions visualized for each hour, one can see that distributions for h1-h7 hours is right-skewed with some negative outliers.\n- similarly, h8-h11 is another highly correlated range of hours, with plus\/minus 3 hours range for each being the most impactful. \n- h11-h17 is another highly correlated range of hours, with plus\/minus 5 hours range for each being the most impactful.Also, from histogram distributions visualized for each hour, one can see that distributions for h8-h17 hours is centered (neither right nor left skewed)\n- looking closely at h18-h24 range, we can see that the price at a particular hour is highly correlated with only the previous and the next hour (namely, plus\/minus 1 hour). From 18-21 histograms are left-skewed, with some positive outliers. \n- Hours 22-24 are again right-skewed like h1-h7, which might signify the relationship between h1-h24, although that's not evident from the heatmap. It makes sense for h1 and h24 to be related (?)\n- daily mean is the most correlated with hour 10, 11, 9\/12, 13,14,15,16,etc.\n\nNote that, however, these observations are useful in case we do hourly forecasts, rather than daily mean forecasts.","ff689244":"# Granger Causality Tests\nThe <a href='https:\/\/en.wikipedia.org\/wiki\/Granger_causality'>Granger causality test<\/a> is a a hypothesis test to determine if one time series is useful in forecasting another. While it is fairly easy to measure correlations between series - when one goes up the other goes up, and vice versa - it's another thing to observe changes in one series correlated to changes in another after a consistent amount of time. This <em>may<\/em> indicate the presence of causality, that changes in the first series influenced the behavior of the second. However, it may also be that both series are affected by some third factor, just at different rates. Still, it can be useful if changes in one series can predict upcoming changes in another, whether there is causality or not. In this case we say that one series \"Granger-causes\" another.\n\nIn the case of two series, $y$ and $x$, the null hypothesis is that lagged values of $x$ do <em>not<\/em> explain variations in $y$.<br>\nIn other words, it assumes that $x_t$ doesn\u2019t Granger-cause $y_t$.\n\nThe stattools <tt><strong>grangercausalitytests<\/strong><\/tt> function offers four tests for granger non-causality of 2 timeseries\n\nFor this example we'll use the samples.csv file, where columns 'a' and 'd' are stationary datasets.","0857994b":"### Triple Exponential Smoothing \/ Holt-Winters Method","1d511a2e":"## My custom functions ","dc625922":"Before we detrend and deseasonalize our data, let's look at these components separately. We can observe a clear indication of additive trend and different seasonalities, also relying on Autocorrelation plot, as well as Augmented Dickey-Fuller test (ADF). ","a2462345":"# Brief Intro\nWe will use 2016-2018 price data from the German electricity market. Our analysis is based on three years of hourly energy prices data for the German electricity market, i.e.: from 01.01.2016 to 31.12.2018. We will consider the daily average for \neach day, for a total of 1096 observations. We will try to accurately estimate test data (2018 prices) given the train data (2016,2017 prices). We will perform rigorous EDA and apply various models to achieve desirable results. \n\nWe will take two different approaches: estimating hourly prices on the individual basis (running a model 24 times for every hour data) and estimating the daily mean of prices (running a model single time for the data).\n","a1bcca40":"There is obvious weekly seasonality\/periodicity observed for each month. Vary the weeks to be ensured, and do that for every month to notice that they have distinc seasonality patterns that repeat only throught a single month. Also, this seasonality holds every year. ","0e55345c":"# Exploratory Data Analysis (EDA)\n\nUpon plotting one can clearly see that the given data is non-stationary, with weekly, quarterly and yearly seasonalities, as well as additive\/linear trend. A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \u201cstationarised\u201d) through the use of mathematical transformations. A stationarised series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past. Another reason for trying to stationarise a time series is to be able to obtain meaningful sample statistics such as means, variances, and correlations with other variables. Such statistics are useful as descriptors of future behavior only if the series is stationary. \n\n- Trend represents an increase or decrease in time-series value over time. If we notice that the value of measurement over time is increasing or decreasing then we can say that it has an upward or downward trend. We can notice some additive trend for every year. \n\n- Seasonality represents variations in measured value which repeats over the same time interval regularly. If we notice that particular variations in value are happening every week, month, quarter or half-yearly then we can say that time series has some kind of seasonality. How to remove seasonality from time-series data? Differencing a time-series.\n\nWe will apply Log Transformation + Differencing to detrend and deseasonalize our data, therefore making it stationary.","f3de317b":"# LSTM (Recurrent Neural Networks)","81f14eda":"## Preparing data","62bdb67c":"# Importing Libraries","470644ea":"Essentially we're looking for extremely low p-values, which we see at lag 2.<br>\nBy comparison, let's compare two datasets that are not at all similar, 'b' and 'd'."}}