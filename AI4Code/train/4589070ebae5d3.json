{"cell_type":{"f9474882":"code","4a779258":"code","8895cdec":"code","159a2ece":"code","7d8f3035":"code","b2f8dcdc":"code","12c15efa":"code","77aa1dcb":"code","2a16e889":"code","b1eddce8":"code","b06dfe50":"code","5be3df5a":"code","97d88d54":"code","7eb4b060":"code","4d135c1a":"code","b7df1760":"code","32367ec4":"code","4ee08209":"code","5b4cca14":"code","dc97c495":"code","b0119b16":"code","848174f2":"code","d88f7ee8":"code","6a968032":"markdown","8174555b":"markdown","99e91fa3":"markdown","22f7ad1e":"markdown","510c873c":"markdown","b12d5997":"markdown","e8a05c0b":"markdown","c3f79128":"markdown","51b11ed4":"markdown"},"source":{"f9474882":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\nimport pandas as pd\nimport datetime as dt\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nwidth = 0.75\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nimport string\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom datetime import date, timedelta\nimport operator \nimport re\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\nimport spacy #load spacy\nnlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n#stops = stopwords.words(\"english\")\nfrom tqdm import  tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom IPython.display import IFrame\nfrom IPython.core.display import display, HTML\n\n# Any results you write to the current directory are saved as output.","4a779258":"data = pd.read_csv(\"..\/input\/taylor_swift_lyrics.csv\",encoding = \"latin1\")\n","8895cdec":"data.head()","159a2ece":"def get_features(df):    \n    data['lyric'] = data['lyric'].apply(lambda x:str(x))\n    data['total_length'] = data['lyric'].apply(len)\n    data['capitals'] = data['lyric'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])\/float(row['total_length']),\n                                axis=1)\n    data['num_words'] = data.lyric.str.count('\\S+')\n    data['num_unique_words'] = data['lyric'].apply(lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] \/ df['num_words']  \n    return df","7d8f3035":"sns.set(rc={'figure.figsize':(11.7,8.27)})\ny1 = data[data['year'] == 2017]['lyric'].str.len()\nsns.distplot(y1, label='2017')\ny2 = data[data['year'] == 2014]['lyric'].str.len()\nsns.distplot(y2, label='2014')\ny3 = data[data['year'] == 2012]['lyric'].str.len()\nsns.distplot(y3, label='2012')\ny4 = data[data['year'] == 2010]['lyric'].str.len()\nsns.distplot(y4, label='2010')\ny5 = data[data['year'] == 2008]['lyric'].str.len()\nsns.distplot(y5, label='2008')\ny6 = data[data['year'] == 2006]['lyric'].str.len()\nsns.distplot(y6, label='2006')\nplt.title('Year Wise - Lyrics Lenght Distribution (Without Preprocessing)')\nplt.legend();\n","b2f8dcdc":"train = get_features(data)\ndata_pair = data.filter(['year','total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique'],axis=1)","12c15efa":"data.head().T","77aa1dcb":"sns.pairplot(data_pair,hue='year',palette=\"husl\");","2a16e889":"contraction_mapping_1 = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" ,\n                       \"Isn't\":\"is not\", \"\\u200b\":\"\", \"It's\": \"it is\",\"I'm\": \"I am\",\"don't\":\"do not\",\"did't\":\"did not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n                       \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n                       \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n","b1eddce8":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","b06dfe50":"def get_features(df):    \n    data['Clean_Lyrics'] = data['Clean_Lyrics'].apply(lambda x:str(x))\n    data['total_length'] = data['Clean_Lyrics'].apply(len)\n    data['capitals'] = data['Clean_Lyrics'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])\/float(row['total_length']),\n                                axis=1)\n    data['num_words'] = data.lyric.str.count('\\S+')\n    data['num_unique_words'] = data['Clean_Lyrics'].apply(lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] \/ df['num_words']  \n    return df","5be3df5a":"data['Clean_Lyrics'] = data['lyric'].apply(lambda x: clean_contractions(x, contraction_mapping_1))\n#Stopwords\ndata['Clean_Lyrics'] = data['Clean_Lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n#Re-calculate the features\ntrain = get_features(data)","97d88d54":"data.head().T","7eb4b060":"sns.set(rc={'figure.figsize':(11.7,8.27)})\ny1 = data[data['year'] == 2017]['Clean_Lyrics'].str.len()\nsns.distplot(y1, label='2017')\ny2 = data[data['year'] == 2014]['Clean_Lyrics'].str.len()\nsns.distplot(y2, label='2014')\ny3 = data[data['year'] == 2012]['Clean_Lyrics'].str.len()\nsns.distplot(y3, label='2012')\ny4 = data[data['year'] == 2010]['Clean_Lyrics'].str.len()\nsns.distplot(y4, label='2010')\ny5 = data[data['year'] == 2008]['Clean_Lyrics'].str.len()\nsns.distplot(y5, label='2008')\ny6 = data[data['year'] == 2006]['Clean_Lyrics'].str.len()\nsns.distplot(y6, label='2006')\nplt.title('Year Wise - Lyrics Lenght Distribution (After Preprocessing)')\nplt.legend();\n","4d135c1a":"data['year'].value_counts()","b7df1760":"def ngram_extractor(text, n_gram):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Function to generate a dataframe with n_gram and top max_row frequencies\ndef generate_ngrams(df, col, n_gram, max_row):\n    temp_dict = defaultdict(int)\n    for question in df[col]:\n        for word in ngram_extractor(question, n_gram):\n            temp_dict[word] += 1\n    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n    temp_df.columns = [\"word\", \"wordcount\"]\n    return temp_df\n\ndef comparison_plot(df_1,df_2,col_1,col_2, space):\n    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n    \n    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0], color=\"skyblue\")\n    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1], color=\"skyblue\")\n\n    ax[0].set_xlabel('Word count', size=14, color=\"green\")\n    ax[0].set_ylabel('Words', size=18, color=\"green\")\n    ax[0].set_title('Top words in 2017 Lyrics', size=18, color=\"green\")\n\n    ax[1].set_xlabel('Word count', size=14, color=\"green\")\n    ax[1].set_ylabel('Words', size=18, color=\"green\")\n    ax[1].set_title('Top words in 2008 Lyrics', size=18, color=\"green\")\n\n    fig.subplots_adjust(wspace=space)\n    \n    plt.show()","32367ec4":"Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 1, 10)\nLyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 1, 10)\ncomparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)","4ee08209":"Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 2, 10)\nLyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 2, 10)\ncomparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)","5b4cca14":"Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 3, 10)\nLyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 3, 10)\ncomparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)","dc97c495":"import scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\ndata['parsed'] = data.Clean_Lyrics.apply(nlp)\n","b0119b16":"corpus = st.CorpusFromParsedDocuments(data,\n                             category_col='album',\n                             parsed_col='parsed').build()\n","848174f2":"html = st.produce_scattertext_explorer(corpus,\n          category='reputation',\n          category_name='reputation',\n          not_category_name='1989',\n          width_in_pixels=600,\n          minimum_term_frequency=5,\n          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n          )","d88f7ee8":"filename = \"reputation-vs-1989.html\"\nopen(filename, 'wb').write(html.encode('utf-8'))\nIFrame(src=filename, width = 800, height=700)\n","6a968032":"Basic Lyric Understanding ","8174555b":"Thank you :)","99e91fa3":"## Trigram Lyrics Anaysis 2017 vs 2008 ","22f7ad1e":"## Expanding English language contractions ","510c873c":"### Pairplot allows us to see both distribution of single variables and relationships between two variables.","b12d5997":"Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics. ","e8a05c0b":"## Bigram Lyrics Anaysis 2017 vs 2008 ","c3f79128":"## Ngram Lyrics Anaysis 2017 vs 2008 ","51b11ed4":"## Text EDA using lyrics from Taylor Swift :)\n\n**The dataset contains follwing fields\n**\n* Album name\n* Track title\n* Track number\n* Lyric text\n* Line number of the lyric in the track\n* Year of release of the album"}}