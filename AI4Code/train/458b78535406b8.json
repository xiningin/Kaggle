{"cell_type":{"64653639":"code","eca53d46":"code","c5c4b708":"code","d486b582":"code","307bc274":"code","8abc832d":"code","f6759cc6":"code","a6c4bf20":"code","9989f0b4":"code","5a34632c":"code","0b0b428a":"code","bb42d298":"code","5eaf9582":"code","deaa811c":"code","5f7fec7d":"code","6d755223":"code","ea7a8a61":"code","429bcf42":"code","73389f93":"code","c97cffc4":"code","01192810":"code","1332e688":"code","0c07977e":"code","13d25efa":"code","d6ac980d":"code","147c1dbe":"code","69897126":"code","d85275e0":"code","397e7fc2":"code","c7c571a6":"code","22483861":"code","4471d8f0":"code","16755843":"code","ecc4e200":"code","08dfbd99":"code","5c5653cc":"code","36870c67":"code","d1119afc":"code","4d1aace2":"code","47594529":"code","83c8bb61":"code","d9290df8":"code","668bcab7":"code","909f9c9a":"code","7db1c1b0":"code","0d1a06dd":"code","904f4c00":"code","daa5fcff":"code","1bf225ac":"code","72b6081f":"code","068cd5e1":"code","ac1d1275":"code","0a4570e1":"code","62a290ae":"code","1bf67e58":"code","34546bb3":"code","a0f1a5b5":"code","a18aed6f":"code","86ae0374":"code","22cb8bbc":"code","582dcdd4":"code","f94500cb":"markdown","95519d33":"markdown","b0e69ee1":"markdown","6260bce6":"markdown","d1600a44":"markdown","6849a344":"markdown","4ba50ab4":"markdown","a6522f65":"markdown","ac5fa2d9":"markdown","ac49f690":"markdown","b558b6c8":"markdown","7fbc329e":"markdown","137c4540":"markdown","e479b76f":"markdown","b9c9ed9b":"markdown","331502d1":"markdown","970ff008":"markdown","ed8fd665":"markdown","2ff6e062":"markdown","603e08d4":"markdown","e7f092fd":"markdown","90bd0177":"markdown","cddcceb3":"markdown","8dac24e3":"markdown","caf8420e":"markdown"},"source":{"64653639":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nfrom sklearn.metrics import explained_variance_score as EVS\nfrom sklearn.metrics import mean_squared_log_error\nimport pydot\nfrom IPython.display import Image\nfrom sklearn.externals.six import StringIO\nfrom sklearn.model_selection import train_test_split","eca53d46":"df_temp = pd.read_csv('\/kaggle\/input\/american-census-2019-person\/pppub19\/pppub19.csv')\ndf_temp.shape","c5c4b708":"df_temp = df_temp[['ERN_VAL', 'A_HRSPAY', 'WSAL_VAL', 'PEARNVAL', 'DIV_VAL',\n                   'RNT_VAL', 'DSAB_VAL', 'ED_VAL', 'FIN_VAL', 'INT_VAL',\n                   'OI_VAL', # income variables\n                   'A_AGE', 'PRDTRACE', 'A_SEX', 'A_HGA', 'A_MARITL',\n                   'PEHSPNON', 'MIG_ST', 'MIG_DIV', # demographics\n                   'A_LFSR', 'A_USLHRS', 'A_CLSWKR', 'A_MJIND', \n                   'A_MJOCC', # employment variables\n                   'MOOP', 'HEA']] # health","d486b582":"# Plotting distribution of the labor force\n\ndf_temp['A_LFSR'] = df_temp['A_LFSR'].replace(7, 5)\n\nlabels = ['Children or Armed Forces', 'Working' ,'With job, not at work',\n          'Unemployed, looking for work', 'Unemployed, on layoff', \n          'Not in labor force']\nax = df_temp['A_LFSR'].plot(kind='hist', figsize=(22,8), fontsize=13, \n                         bins = np.arange(len(labels)+1)-0.5, rwidth = 0.5)\nax.set_title(\"Employment status histogram\", fontsize=22)\nax.set_ylabel(\"Frequency\", fontsize=15);\nplt.xlabel('Employment categories', fontsize = 14)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# Showing normalized distribution of the labor force\ndf_temp['A_LFSR'].value_counts(normalize = True)\n\n# We can see that only a very small fraction of the sample is in the labor force\n# but not under 'working'. So let's continue with this category alone.","307bc274":"df_temp1 = df_temp.loc[(df_temp['A_LFSR'] == 1)]\n\nlabels = ['Not in universe', 'Private', 'Federal government',\n          'State government', 'Local government', 'Self-employed-incorporated',\n          'Self-employed-not incorporated', 'Without pay']\ndf_temp1['A_CLSWKR'].plot(kind='hist', rwidth = 0.4,  figsize = (27, 7), \n                       bins = np.arange(len(labels)+1)-0.5, fontsize = 12)\nplt.xlabel('Worker class categories', fontsize = 14)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels)\nfig = plt.show\n\ndf_temp1['A_CLSWKR'].value_counts(normalize = True)\n\n### df_temp1 = df_temp.loc[(df_temp['ERN_VAL'] > 0) & (df_temp['ERN_SRCE'] == 1)]\n# df_temp1.head\n\n# Worker class can be roughly divided into 3 groups - private (76%), \n# government (~14%) & self-employed (~10%). We decided to focus on the private\n# sector","8abc832d":"df_temp1 = df_temp1.loc[(df_temp1['A_CLSWKR'] == 1)]\ndf_temp1.shape\n\n# After dropping the other categories we now have about 62,000 records","f6759cc6":"# Examination of continuous variables\ndf_continuous = df_temp1[['A_AGE', 'A_USLHRS', # Age & hours \n               'WSAL_VAL', 'ERN_VAL', 'A_HRSPAY', 'PEARNVAL', # Income from work\n               'DSAB_VAL', 'ED_VAL', 'FIN_VAL', # Income from grants\n               'DIV_VAL', 'RNT_VAL', 'INT_VAL', # additional income sources\n               'OI_VAL', # Overall additional income\n               'MOOP']] # Total medical out of pocket expenditures\ndf_continuous.describe(percentiles = [.01, .05, .1, .25, .5, .75, .9, .95,\n                                       .98, .99])\n\n# We can see that A_HRSPAY, DIV_VAL, RNT_VAL, DSAB_VAL, ED_VAL, FIN_VAL & OI_VAL\n# are mostly '0'. So we decided to drop them.\n\n# Also, ERN_VAL and PEARNVAL have negative values, which should only be possible \n# for self-employed. WSAL_VAL has a '0' value, which is something we don't want.\n# So, we crop these observations.\n\n# Another thing to notice is the huge jump in ERN_VAL, WSAL_VAL & PEARNVAL\n# from 99th percentile to max. It's a multiplcation by a factor of 4-5, \n# while the increase from 98th to 99th percentile is a multiplcation of about\n# a factor of 1.4.","a6c4bf20":"# Dropping useless variables and showing scatterplots\ndf_continuous1 = df_continuous.drop(columns = ['DIV_VAL', 'RNT_VAL', 'DSAB_VAL',\n                                               'ED_VAL', 'FIN_VAL', 'OI_VAL',\n                                               'A_HRSPAY', 'INT_VAL'])\n\ndf_continuous1 = df_continuous1.loc[df_continuous1['WSAL_VAL'] > 0]\ndf_continuous1 = df_continuous1.loc[df_continuous1['ERN_VAL'] > 0]\ndf_continuous1 = df_continuous1.loc[df_continuous1['PEARNVAL'] > 0]\n\nfig = sns.pairplot(df_continuous1)\ndf_continuous1.describe(percentiles = \n                         [.01, .05, .1, .25, .5, .75, .9, .95, .99])\n\n# We can see that DIV_VAL, RNT_VAL & DSAB_VAL are correlated, though they have \n# outliers, especially in the lower values.\n# We should note that this graph is after a log transformation. And we can see \n# in the histograms that the vast majority of the people are in the extreme \n# left side of the graph. \n\n# We can see that the min for the income variables is 6, while percentile 1 is\n# over 1,000. So lets drop the 1st percentile. We can also see that the max\n# for the income variables and for MOOP is extremely high compared to the 99th\n# percentile. So let's drop the 100th percentile. This is also true for MOOP\n\n\n# ******************************************************************* #\n\n# ERN_VAL - How much did (you) earn from this (primary) employer before deductions\n# WSAL_VAL - total wage and salary earnings\n# PEARNVAL - total persons earnings\n# MOOP - Total medical out of pocket expenditures","9989f0b4":"# Dropping outliers of VAL features and of MOOP feature\n\ndf_continuous1['to_drop'] = 0\n\n# 1st and 100th percentiles of WSAL_VAL\ndf_continuous1.loc[df_continuous1['WSAL_VAL'] >= \n                    df_continuous1.WSAL_VAL.quantile(.99), 'to_drop'] = 1\ndf_continuous1.loc[df_continuous1['WSAL_VAL'] <= \n                    df_continuous1.WSAL_VAL.quantile(.01), 'to_drop'] = 1\n\n# 1st and 100th percentiles of ERN_VAL\ndf_continuous1.loc[df_continuous1['ERN_VAL'] >= \n                    df_continuous1.ERN_VAL.quantile(.99), 'to_drop'] = 1\ndf_continuous1.loc[df_continuous1['ERN_VAL'] <= \n                    df_continuous1.ERN_VAL.quantile(.01), 'to_drop'] = 1\n\n# 1st and 100th percentiles of PEARNVAL\ndf_continuous1.loc[df_continuous1['PEARNVAL'] >= \n                    df_continuous1.PEARNVAL.quantile(.99), 'to_drop'] = 1\ndf_continuous1.loc[df_continuous1['PEARNVAL'] <= \n                    df_continuous1.PEARNVAL.quantile(.01), 'to_drop'] = 1\n\n# 100th percentiles of MOOP\ndf_continuous1.loc[df_continuous1['MOOP'] >=\n                    df_continuous1.MOOP.quantile(.99), 'to_drop'] = 1\n\n# people over 79 years old \n# df_interval_min.loc[df_interval_min['A_AGE'] >= 80, 'A_AGE'] = np.nan\n\ndf_continuous1 = df_continuous1.loc[df_continuous1['to_drop'] == 0]\ndf_continuous1 = df_continuous1.drop(columns = (['to_drop']))\nfig = sns.pairplot(df_continuous1)\ndf_continuous1.describe(percentiles = \n                         [.01, .05, .1, .25, .5, .75, .9, .95, .99])\n\n# We can see now that the VAL features are less correlated than before, but they\n# still represent the same thing, more or less. So we'll only keep ERN_VAL. \n# Also, values higher than 79 in the A_AGE are categorical. So we'll drop them\n# too. In addition,we'll drop records in which A_USLHRS is 0 or lower, as these \n# are categorical values. Finally, the distributions of ERN_VAL and MOOP are \n# very skewed. We'll calculate their square root and see if we can get a nicer \n# distribution. ","5a34632c":"# Plotting the histogram of ERN_VAL\n\nplt.subplots(figsize=(10, 9))\nsns.kdeplot(df_continuous1['ERN_VAL'], shade=True);","0b0b428a":"# Plotting the histogram of MOOP\n\nplt.subplots(figsize=(10, 9))\nsns.kdeplot(df_continuous1['MOOP'], shade=True);","bb42d298":"# Dropping categorical data and calculating square roots\n\n#df_continuous1 = df_continuous1.drop(columns = ['WSAL_VAL', 'PEARNVAL'])\ndf_continuous1 = df_continuous1.loc[df_continuous1['A_AGE'] < 80]\ndf_continuous1 = df_continuous1.loc[df_continuous1['A_USLHRS'] > 0]\ndf_continuous1['sqrt_ern_val'] = df_continuous1['ERN_VAL'] ** 0.5\ndf_continuous1['sqrt_moop'] = df_continuous1['MOOP'] ** 0.5\n\nfig = sns.pairplot(df_continuous1)\ndf_continuous1.describe(percentiles = \n                         [.01, .05, .1, .25, .5, .75, .9, .95, .99])\n\n# The data looks much nicer now, especially sqrt_ern_val.\n# We now have about 55,000 records.","5eaf9582":"# Plotting the histogram of sqrt_ern_val\n\nplt.subplots(figsize=(10, 9))\nsns.kdeplot(df_continuous1['sqrt_ern_val'], shade=True);","deaa811c":"# Plotting the histogram of sqrt_ern_val\n\nplt.subplots(figsize=(10, 9))\nsns.kdeplot(df_continuous1['sqrt_moop'], shade=True);","5f7fec7d":"### Recreating df \n\ndf_temp = pd.read_csv('\/kaggle\/input\/american-census-2019-person\/pppub19\/pppub19.csv')\ndf_temp2 = df_temp[['ERN_VAL','A_AGE', 'PRDTRACE', 'A_SEX', 'A_HGA', 'A_MARITL',\n                   'PEHSPNON', 'A_LFSR', 'A_USLHRS', 'A_CLSWKR', 'A_MJOCC', \n                   'MOOP', 'HEA']]\n\ndf_temp2 = df_temp2.loc[(df_temp['A_LFSR'] == 1)]\ndf_temp2 = df_temp2.loc[(df_temp2['A_CLSWKR'] == 1)]\n\n# Dropping unnecessary columns\ndf_temp2 = df_temp2.drop(columns = ['A_LFSR', 'A_CLSWKR'])\n\n# Cropping '0's and negatives from ERN_VAL\ndf_temp2 = df_temp2.loc[df_temp2['ERN_VAL'] > 0]\n\ndf_temp2['to_drop'] = 0\n\n# Selecting 1st and 100th percentiles of ERN_VAL\ndf_temp2.loc[df_temp2['ERN_VAL'] > \n                    df_temp2.ERN_VAL.quantile(.99), 'to_drop'] = 1\ndf_temp2.loc[df_temp2['ERN_VAL'] < \n                    df_temp2.ERN_VAL.quantile(.01), 'to_drop'] = 1\n\n# Selecting 100th percentiles of MOOP\ndf_temp2.loc[df_temp2['MOOP'] > df_temp2.MOOP.quantile(.99), 'to_drop'] = 1\n\n# Dropping selected rows\ndf_temp2 = df_temp2.loc[df_temp2['to_drop'] == 0]\ndf_temp2 = df_temp2.drop(columns = (['to_drop']))\n\n# Dropping ages 80+\ndf_temp2 = df_temp2.loc[df_temp2['A_AGE'] < 80]\n\n# Dropping less than 1 hours of work\ndf_temp2 = df_temp2.loc[df_temp2['A_USLHRS'] > 0]\n\n# Calculating square roots\ndf_temp2['sqrt_ern_val'] = df_temp2['ERN_VAL'] ** 0.5\ndf_temp2['sqrt_moop'] = df_temp2['MOOP'] ** 0.5\n\ndf = df_temp2\ndf.describe()\n\n# We now have 55,507 records and 13 features","6d755223":"df['A_MJOCC'].plot(kind='hist', rwidth = 0.8, figsize = (10, 5))\nplt.show()\ndf['A_MJOCC'].value_counts(normalize=True)\n\n# There are a lot of occupation types and no obvious way to group them.\n# Lets look at the median income of the categories\n\n# ******************************************\n# Legend is too long to be shown on plot so it is listed here\n\n# 1 = Management, business, and financial occupations\n# 2 = Professional and related occupations\n# 3 = Service occupations\n# 4 = Sales and related occupations\n# 5 = Office and administrative support occupations\n# 6 = Farming, fishing, and forestry occupations\n# 7 = Construction and extraction occupations\n# 8 = Installation, maintenance, and repair occupations\n# 9 = Production occupations\n# 10 = Transportation and material moving occupations","ea7a8a61":"fig = df.groupby('A_MJOCC')['ERN_VAL'].median().plot(figsize = (10, 6))\n\n# It seems that categories 1 (16.8%) and 2 (20.1%) have a higher median\n# income than the rest (63.1%). This finding makes sense. So let's a\n# new occupation variable, based on this grouping.","429bcf42":"# Creating occupation variable:\n# 1 - Management, business, and financial occupations\n# 2 - Professional and related occupations\n# 3 - Other\n\noccupation_dict = {1: 0, 2: 1, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 2, 9: 2, 10: 2}\ndf['occupation'] = df['A_MJOCC'].replace(occupation_dict)\ndf['occupation'].value_counts(normalize = True)","73389f93":"## Set dummy variables for occupation \n\ndf['occ_management'] = np.where(df['occupation'] == 0, 1, 0)\ndf['occ_professional'] = np.where(df['occupation'] == 1, 1, 0)\ndf['occ_other'] = np.where(df['occupation'] == 2, 1, 0)\n## df['occ_other'].value_counts(normalize = True)","c97cffc4":"# We already saw that the working hours variables has a major peak at 40.\n# So lets divide it into 3 categories. This is done arbitrarily.\ndf['hours_cat'] = 2\ndf.loc[df['A_USLHRS'] < 36, 'hours_cat'] = 1\ndf.loc[df['A_USLHRS'] > 44, 'hours_cat'] = 3\ndf['hours_cat'].value_counts(normalize = True)","01192810":"df.groupby('hours_cat')['ERN_VAL'].median()\n\n# Obviously, median income differs by working hours","1332e688":"## Set dummy variables for working hours \ndf['less_than_36_hours'] = np.where(df['hours_cat'] == 1, 1, 0) \ndf['between_36_and_44_hours'] = np.where(df['hours_cat'] == 2, 1, 0) \ndf['more_than_44_hours'] = np.where(df['hours_cat'] == 3, 1, 0)  ","0c07977e":"df['A_SEX'].value_counts(normalize=True)\n# 1 - Male, 2 - Female\n# We can see that we have a little bit more males than females\n# (Sorry for using the terms males and females and not Men \/ Women. That's how the dictionary specifies them)","13d25efa":"df['PRDTRACE'].value_counts(normalize=True) \n\n# 1 - White only, 2 - Black only, 4 - Asian only\n\n# From first glance it seems that white greatly predominates everything else\n# However, we found out that 91% of latino's are labeled as white","d6ac980d":"# Are you Spanish, Hispanic, or Latino?\n# 1 - Yes\n# 2 - No\ndf['PEHSPNON'].value_counts(normalize=True) ","147c1dbe":"# Race by latino. We found out that many Latino's are classified as white\npd.crosstab(df.PRDTRACE, df.PEHSPNON, normalize = 'columns')","69897126":"# We decided to label 'white latinos' as 'latinos'. \n# Creating a new race variable:\n# 0 - White, 1 - Latino, 2 - Black, 3 - Asian, 4 - Other\n\nrace_dict = {1: 0, 2: 2, 3: 4, 4: 3, 5: 4, 6: 4, 7: 4, 8: 4, 9: 4, 10: 4, 11: 4,\n             12: 4, 13: 4, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, \n             21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4}\n\ndf['race_cat'] = df['PRDTRACE'].replace(race_dict)\n\n# Now assigning white latinos as '1'\ndf.loc[(df['PEHSPNON'] == 1) & (df['PRDTRACE'] == 1), 'race_cat'] = 1\n\n# Plotting the graph\nlabels = ['White', 'Latino', 'Black', 'Asian', 'Other']\nax = df['race_cat'].plot(kind='hist', figsize=(14,6), fontsize=13, \n                         bins = np.arange(len(labels)+1)-0.5, rwidth = 0.5)\nax.set_title(\"Race histogram\", fontsize=22)\nax.set_ylabel(\"Frequency\", fontsize=15);\nplt.xlabel('Race categories', fontsize = 14)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show","d85275e0":"df.groupby('race_cat')['ERN_VAL'].median()\n\n# We can see that giving 'white latinos' their own category makes a lot of sense\n# as their median income is significantly lower than 'regular' whites. However, \n# we decided to create a binary race variable because the sample simply didn't \n# contain enough people from the other races. ","397e7fc2":"# Creating a new, binary, variable - white vs not white\ndf['white'] = np.where(df['race_cat'] == 0, 1, 0)\ndf['white'].value_counts(normalize=True)\n\n# 0 - Not white\n# 1 - White","c7c571a6":"df.groupby('white')['ERN_VAL'].median()","22483861":"# 2 - Married - AF spouse present (AF is an airforce program to assist the \n#     spouses of the soldiers)\n# 3 - Married - spouse absent (exc.separated)\n# 4 - Widowed\n# 5 - Divorced\n# 6 - Separated\n# 7 - Never married\ndf['A_MARITL'].value_counts(normalize=True)","4471d8f0":"# We decided to create a new family status variable with 0 - married (53.6%), \n# 1 - never married (31.5%), 2 - divorced (9.6%) & 3 - other (5.3%)\nmarital_dict = {1: 0, 2: 3, 3: 3, 4: 3, 5: 2, 6: 3, 7: 1}\ndf['marital_cat'] = df['A_MARITL'].replace(marital_dict)\n\n# Plotting the graph\nlabels = ['Married', 'Never married', 'Divorced', 'Other']\nax = df['marital_cat'].plot(kind='hist', figsize=(14,6), fontsize=13, \n                            bins = np.arange(len(labels)+1)-0.5, rwidth = 0.4)\nax.set_title(\"Family status histogram\", fontsize=22)\nax.set_ylabel(\"Frequency\", fontsize=15);\nplt.xlabel('Family status categories', fontsize = 14)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show","16755843":"# Now lets create 3 dummy variables based on family status\ndf['married'] = 0\ndf['never_married'] = 0\ndf['divorced'] = 0\n\ndf.loc[df['marital_cat'] == 0, 'married'] = 1\ndf.loc[df['marital_cat'] == 1, 'never_married'] = 1\ndf.loc[df['marital_cat'] == 2, 'divorced'] = 1\n\n# dropping people with other status\ndf = df.loc[df['marital_cat'] != 3]\ndf.shape","ecc4e200":"df['A_HGA'].value_counts(normalize=True) \n\n# 39 - High school graduate\n# 43 - Bachelor's degree\n# 40 - Some college but no degree\n# 44 - Master's degree\n# 42 - Associate degree in college - academic program\n# 41 - Associate degree in college - occupation\/vocation program\n\n### To be presented in a graph\n\n# By aggregating these results into the categories of 'less than high school\n# degree', 'high school degree but less than b.a 'b.a.\/m.a' we cover 96.4%\n# of the population. This leaves out professional schools (1.6%), which varies \n# a lot, and doctorates (2.2%). ","08dfbd99":"# Lets create a new education variable with 0 - less than high school diploma\n# (9.2%), 1 - high_school_no_ba (56.5%), 2 - ba (23.2%) & 3 - ma_doc_pro (9.2%)\neducation_dict = {39: 1, 43: 2, 40: 1, 44: 3, 42: 1, 41: 1, 46: 3, 37: 0, \n                  45: 3, 36: 0, 35: 0, 38:0, 33: 0, 34: 0, 32: 0, 31: 0}\ndf['educ_cat'] = df['A_HGA'].replace(education_dict)\n\n# Plotting the graph\nlabels = ['Less than high school diploma', 'High_school_no_B.A.',\n          'B.A.', 'M.A. \/ Dr. \/ Pro']\nax = df['educ_cat'].plot(kind='hist', figsize=(14,6), fontsize=13,\n                         bins = np.arange(len(labels)+1)-0.5, rwidth = 0.4)\nax.set_title(\"Education level histogram\", fontsize=22)\nax.set_ylabel(\"Frequency\", fontsize=15);\nplt.xlabel('Education categories', fontsize = 14)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show","5c5653cc":"# Now lets create 3 dummy variables based on education\ndf['less_than_high_school'] = 0\ndf['high_school_no_ba'] = 0\ndf['ba'] = 0\ndf['ma_dr_pro'] = 0\n\ndf.loc[df['educ_cat'] == 0, 'less_than_high_school'] = 1\ndf.loc[df['educ_cat'] == 1, 'high_school_no_ba'] = 1\ndf.loc[df['educ_cat'] == 2, 'ba'] = 1\ndf.loc[df['educ_cat'] == 3, 'ma_dr_pro'] = 1","36870c67":"df['HEA'].value_counts(normalize=True) \n\n# 1 - Excellent\n# 2 - Very good \n# 3 - Good\n# 4 - Fair\n# 5 - Poor\n\n# This distribution is not shown in a chart because that would require changing \n# the values of the feature","d1119afc":"# Let's create a dummy variable based on health with 'good health' (1+2) and \n# 'bad health' (3+4+5). We realize that 'good' and 'fair' don't logically fit\n# into the 'bad' category. However, we believe that whoever chooses 3 in a scale \n# of 5 is not in a very good shape.\n\ndf.loc[df['HEA'] <= 2, 'good_health'] = 1\ndf.loc[df['HEA'] > 2, 'good_health'] = 0","4d1aace2":"# Income by sex\ninc_by_sex = df.groupby(['A_SEX'])['ERN_VAL'].median()\nlabels = ['Male', 'Female']\nax = inc_by_sex.plot(kind='bar', figsize=(10,6), fontsize=13, width = 0.3)\nax.set_title(\"Median income by sex\", fontsize=22)\nax.set_ylabel(\"Median income\", fontsize=15);\nplt.xlabel('Sex categories', fontsize = 14)\nplt.ylim(0, 80000)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# There is a difference in the median income by sex","47594529":"# Plotting a chart of income by occupation\ninc_by_occupation = df.groupby(['occupation'])['ERN_VAL'].median()\nlabels = ['Management', 'professional', 'Other']\nax = inc_by_occupation.plot(kind='bar', figsize=(10,6), fontsize=13, width = 0.3)\nax.set_title(\"Median income by occupation\", fontsize=22)\nax.set_ylabel(\"Median income\", fontsize=15);\nplt.xlabel('Occupation categories', fontsize = 14)\nplt.ylim(0, 80000)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# There is a big difference in the median income by occupation","83c8bb61":"# Plotting a chart of income by white\ninc_by_white = df.groupby(['white'])['ERN_VAL'].median()\nlabels = ['Not white', 'White']\nax = inc_by_white.plot(kind='bar', figsize=(10,6), fontsize=13, width = 0.3)\nax.set_title(\"Median income by white\", fontsize=22)\nax.set_ylabel(\"Median income\", fontsize=15);\nplt.xlabel('White categories', fontsize = 14)\nplt.ylim(0, 80000)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# There is a difference in the median income by white\/not white","d9290df8":"# Plotting a chart of income by family status\ninc_by_family = df.groupby(['marital_cat'])['ERN_VAL'].median()\nlabels = ['Married', 'Never Married', 'Divorced']\nax = inc_by_family.plot(kind='bar', figsize=(10,6), fontsize=13, width = 0.3)\nax.set_title(\"Median income by family status\", fontsize=22)\nax.set_ylabel(\"Median income\", fontsize=15);\nplt.xlabel('family status categories', fontsize = 14)\nplt.ylim(0, 80000)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# There is a difference in median income by family status","668bcab7":"# Plotting a chart of income by education level\ninc_by_educ = df.groupby(['educ_cat'])['ERN_VAL'].median()\nlabels = ['No high school diploma', 'High school diploma no B.A.', 'B.A.',\n          'M.A. \/ Dr. \/ Pro']\nax = inc_by_educ.plot(kind='bar', figsize=(15,8), fontsize=13, width = 0.3)\nax.set_title(\"Median income by education level\", fontsize=22)\nax.set_ylabel(\"Median income\", fontsize=15);\nplt.xlabel('Education level categories', fontsize = 14)\nplt.ylim(0, 100000)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# We can see that this aggregation to education levels makes a lot of sense\n# income wise","909f9c9a":"# Plotting a chart of income by health level\ninc_by_health = df.groupby(['good_health'])['ERN_VAL'].median()\nlabels = ['Bad health', 'Good health']\nax = inc_by_health.plot(kind='bar', figsize=(10,6), fontsize=13, width = 0.3)\nax.set_title(\"Median income by health level\", fontsize=22)\nax.set_ylabel(\"Median income\", fontsize=15);\nplt.xlabel('Health level categories', fontsize = 14)\nplt.ylim(0, 80000)\ny_pos = np.arange(len(labels))\nplt.xticks(y_pos, labels, rotation='horizontal')\nfig = plt.show\n\n# There seems to be a slight difference in the median income by health","7db1c1b0":"# Average income by white and sex\nfig = sns.barplot(x=\"white\", y=\"ERN_VAL\", hue=\"A_SEX\", data=df)\n\n# We can see that while white men earn on average a lot, all others earn little","0d1a06dd":"# Average income by married and sex\nfig = sns.barplot(x=\"married\", y=\"ERN_VAL\", hue=\"A_SEX\", data=df)\n\n# We can see that married men earn on average more than the rest","904f4c00":"# Average income by sex and education\nfig = sns.barplot(x=\"A_SEX\", y=\"ERN_VAL\", hue=\"educ_cat\", data=df)\n\n# Notice that women in the M.A. \/ Dr. \/ Pro category earn about as much as men \n# in the B.A. category","daa5fcff":"# Average income by occupation and sex\nfig = sns.barplot(x=\"occupation\", y=\"ERN_VAL\", hue=\"A_SEX\", data=df)","1bf225ac":"df_for_model = df[['sqrt_ern_val', 'A_AGE', 'A_SEX', 'white', \n                   'occ_management', 'occ_professional', 'occ_other', \n                   'less_than_36_hours', 'between_36_and_44_hours',\n                   'more_than_44_hours', 'married', 'never_married', 'divorced',\n                   'MOOP', 'less_than_high_school', 'high_school_no_ba', 'ba',\n                   'ma_dr_pro', 'good_health']]\ndf_for_model.shape","72b6081f":"X = df_for_model.drop(columns = ['sqrt_ern_val'], axis=1)\ny = df_for_model['sqrt_ern_val']","068cd5e1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=123456,\n                                                    shuffle=True)","ac1d1275":"# We set the min leaf sample to be about 1% of the entire sample\nmodel = DecisionTreeRegressor(min_samples_leaf=500)\nmodel.fit(X_train, y_train)","0a4570e1":"dot_data = StringIO()  \nexport_graphviz(model, out_file=dot_data, feature_names=X_test.columns,\n                leaves_parallel=True)  \ngraph = pydot.graph_from_dot_data(dot_data.getvalue())[0] \nImage(graph.create_png(), width=1000) ","62a290ae":"for feature, importance in zip(X_test.columns, model.feature_importances_):\n    print('{:12}: {}'.format(feature, importance))\n\n# Feature importance can be misleading. Still, it seems that age is very\n# important. Occupation, working hours and MOOP also seem important","1bf67e58":"y_train_pred = model.predict(X_train)\ny_train_pred","34546bb3":"plt.figure(figsize = (16,9))\nax = sns.scatterplot(x=y_train, y=y_train_pred)\nax.set_title(\"Prediction figure - training\", fontsize=22)\nax.set_ylabel(\"Actual square root of the income\", fontsize=15);\nplt.xlabel('Predicted square root of the income', fontsize = 14)\nfig = ax.plot(y_train, y_train, 'r')","a0f1a5b5":"# Calaculating RMSLE\ny_train_squared = y_train ** 2\ny_train_pred_squared = y_train_pred ** 2\nrmsle = np.sqrt(mean_squared_log_error(y_train_squared, y_train_pred_squared))\nprint(\"#### Decision Tree Performance:  ####\")\nprint(\"Root Mean Squared Logarithmic Error =\", round(rmsle, 2))","a18aed6f":"y_test_pred = model.predict(X_test)","86ae0374":"plt.figure(figsize = (16,9))\nax = sns.scatterplot(x=y_test, y=y_test_pred)\nax.set_title(\"Prediction figure - test\", fontsize=22)\nax.set_ylabel(\"Actual square root of the income\", fontsize=15);\nplt.xlabel('Predicted square root of the income', fontsize = 14)\nfig = ax.plot(y_test, y_test, 'r')","22cb8bbc":"y_test_squared = y_test ** 2\ny_test_pred_squared = y_test_pred ** 2\nrmsle = np.sqrt(mean_squared_log_error(y_test_squared, y_test_pred_squared))\nprint(\"#### Decision Tree Performance:  ####\")\nprint(\"Root Mean Squared Logarithmic Error =\", round(rmsle, 2))","582dcdd4":"# Plotting the histogram of sqrt_ern_val\nplt.subplots(figsize=(15, 10))\nsns.kdeplot(y_test, label=\"Test data\");\nsns.kdeplot(y_train, label=\"Train data\");\nsns.kdeplot(y_test_pred, label=\"Prediction data\");\n\n# The precition data is slightly less skewed but has a much larger kurtosis.\n# Still, this is expected.","f94500cb":"## Create df for model","95519d33":"## Prediction using the trained model","b0e69ee1":"Examination of the Labor Force Status distribution","6260bce6":"## Demographic features analysis: sex, race, education, marital status & health conditions","d1600a44":"> **Examine Race distribution**","6849a344":"## Import files","4ba50ab4":"> Keeping only the 'working' category","a6522f65":"> **Examining education distribution**","ac5fa2d9":"> **Add code lines for the tree model**","ac49f690":"# The Model - we chose to use a Decision Tree","b558b6c8":"## Income by multiple variables","7fbc329e":"## Occupation and working hours analysis","137c4540":"## Visualizing the tree","e479b76f":"> **Examine Health Status**","b9c9ed9b":"## Accuracy report with test data","331502d1":"## Examine income by demographics","970ff008":"# American census 2019 Project\nhttps:\/\/www.census.gov\/data\/datasets\/time-series\/demo\/cps\/cps-asec.html [link text](https:\/\/)","ed8fd665":"## Exploratory Data Analysis (EDA)","2ff6e062":"> **Working hours Recode (A_USLHRS)**","603e08d4":"## Continuous variables analysis","e7f092fd":"> **Examine Sex distribution**","90bd0177":"## Check Employment variables","cddcceb3":"## Evaluate the performance of the model","8dac24e3":"## Feature Importance","caf8420e":"> **Major Occupation Recode (A_MJOCC)**"}}