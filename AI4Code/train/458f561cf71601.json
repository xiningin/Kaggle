{"cell_type":{"028b4070":"code","7597305d":"code","2ddc8f68":"code","d5ef4209":"code","241f4f89":"code","0c680dff":"code","785f355c":"code","eef56052":"code","a7f6652a":"code","5657bfa1":"code","6148c481":"code","cbecb847":"code","a4d1128d":"code","e0b0c370":"code","19987374":"code","8dce506d":"code","fe098041":"code","3cf524d2":"code","39f02609":"code","e59520e5":"code","345813c5":"code","b6eb98d2":"code","43ab3e8f":"code","a4089744":"code","fdb6d95c":"code","291595c8":"code","76cc7b43":"code","515168cb":"code","ceabc445":"code","e39e8512":"code","d98e6ff0":"code","709f588d":"code","9193879f":"code","7b495110":"code","edce46a2":"code","7e76a940":"code","0999619a":"code","2487b0b7":"code","7a5298ff":"code","4cf89f58":"code","451d8312":"code","d9f5bd3b":"code","20db521b":"code","507e3018":"code","386b7be5":"code","5fc92947":"code","f7661f85":"code","21357462":"code","fd14e2dc":"code","f6403750":"code","be7f652f":"code","4d8f7fb1":"code","3e5f40c3":"code","f10f7965":"code","724b1115":"code","2970a96a":"code","8ce7e825":"code","fbf5cdd3":"code","91bc791e":"code","bb795912":"code","7a0ebf59":"code","29295257":"code","ff2d25df":"code","610d4f4a":"code","dae918ae":"code","27160ef1":"code","44e4d822":"code","52213b3f":"code","e5c74716":"code","1a8f6152":"code","fe62576a":"code","85567999":"markdown","69562973":"markdown","8cd359fb":"markdown","f4c4fe0d":"markdown","043cb8ee":"markdown","ce7e8f4f":"markdown","68ccfecc":"markdown","78c0f9f7":"markdown","f6d14d8d":"markdown","f4812198":"markdown","1c344356":"markdown","d45ed502":"markdown","444d14b6":"markdown","efd45351":"markdown","196fd314":"markdown","4507a600":"markdown","29c88990":"markdown","c4280ce3":"markdown","6e75251a":"markdown","5afdf21b":"markdown","b47b915a":"markdown","fc3ab148":"markdown"},"source":{"028b4070":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\nimport statistics\nfrom sklearn.preprocessing import scale\nimport warnings\n\n%matplotlib inline\n\n\npd.options.display.max_columns = None\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7597305d":"# Importion des donn\u00e9es\ndf = pd.read_csv('\/kaggle\/input\/titanicdataset-traincsv\/train.csv')\ndf = df.drop(columns = ['PassengerId'])","2ddc8f68":"df.columns","d5ef4209":"data = df.copy()","241f4f89":"data.isna().sum()","0c680dff":"data.dtypes.value_counts()","785f355c":"sb.heatmap(data.isna(), cbar=False)","eef56052":"(data.isna().sum()\/data.shape[0]).sort_values()","a7f6652a":"Age = data['Age'] # on remplace les valeurs manquantes par la median\nAge[Age.isna() == True] = Age.median()","5657bfa1":"data = data.drop(columns=['Cabin','Name', 'Ticket']) # on supprime les variables inutiles","6148c481":"data.isna().sum()","cbecb847":"data.dtypes","a4d1128d":"for col in data.select_dtypes('float64'):\n    plt.figure()\n    sb.distplot(data[col])","e0b0c370":"for col in data.select_dtypes('object'):\n    print(col, data[col].unique())","19987374":"for col in data.select_dtypes('object'):\n    plt.figure()\n    data[col].value_counts().plot.pie()","8dce506d":"survecu = data[data['Survived'] == 1]\nnonsurvecu = data[data['Survived'] ==  0]","fe098041":"sb.countplot(x ='Age', hue = 'Survived', data = data)","3cf524d2":"sb.countplot(x ='Fare', hue = 'Survived', data = data)","39f02609":"pd.crosstab(data['Survived'], data['Embarked'])\/data.shape[0]","e59520e5":"pd.crosstab(data['Survived'], data['Sex'])\/data.shape[0]","345813c5":"pd.crosstab(data['Survived'], data['Parch'])\/data.shape[0]","b6eb98d2":"pd.crosstab(data['Survived'], data['Pclass'])\/data.shape[0]","43ab3e8f":"pd.crosstab(data['Survived'], data['SibSp'])\/data.shape[0]","a4089744":"for col in ['Sex','Embarked','Pclass','Parch','SibSp']:\n    plt.figure()\n    sb.heatmap(pd.crosstab(data['Survived'], data[col]), annot=True, fmt = 'd')","fdb6d95c":"df = pd.read_csv('\/kaggle\/input\/titanicdataset-traincsv\/train.csv')\ndf = df.drop(columns = ['PassengerId'])","291595c8":"from sklearn.model_selection import train_test_split","76cc7b43":"Age = df['Age']\nAge[Age.isna() == True] = Age.median()\ndf['Age'] = Age","515168cb":"Embarked = df['Embarked']\nEmbarked[Embarked.isna() == True] = Embarked.mode()\ndf['Embarked'] = Embarked","ceabc445":"trainset, testset = train_test_split(df, test_size = 0.2, random_state = 0)","e39e8512":"trainset['Survived'].value_counts()","d98e6ff0":"testset.shape","709f588d":"df = df.drop(columns=['Name','Cabin','Ticket'])","9193879f":"for col in df.select_dtypes('object').columns:\n    print(col)","7b495110":"def encodage(df):\n    \n    code  = {'S':1, \n             'C':2, \n             'Q':3, \n             'female':1,\n             'male':2}\n    for col in df.select_dtypes('object').columns:\n        df[col] = df[col].map(code)  \n        \n    return df","edce46a2":"def preprocessing(df):\n    \n    df = encodage(df)\n    \n    X = df.drop(columns=['Survived','Cabin','Name','Ticket'], axis = 1)\n    y = df['Survived']\n    print(y.value_counts())\n    return X,y","7e76a940":"X_train, y_train = preprocessing(trainset)","0999619a":"X_test, y_test = preprocessing(testset)","2487b0b7":"X_train.isna().sum()","7a5298ff":"Age = X_train['Age'] # on remplace les valeurs manquantes par la mediane\nAge[Age.isna() == True] = Age.median()\nX_train['Age'] = Age","4cf89f58":"Embarked = X_train['Embarked']\nEmbarked[Embarked.isna() == True] = 1\nX_train['Embarked'] = Embarked","451d8312":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler","d9f5bd3b":"preprocessor = make_pipeline(PolynomialFeatures(2, include_bias=False) ,SelectKBest(f_classif, k = 4))","20db521b":"RandomForest = make_pipeline(preprocessor, RandomForestClassifier(random_state=0))\nAdaBoost = make_pipeline(preprocessor, AdaBoostClassifier(random_state=0))\nSVM = make_pipeline(preprocessor, StandardScaler(), SVC(random_state=0))\nKNN = make_pipeline(preprocessor, StandardScaler(), KNeighborsClassifier())\nLogReg = make_pipeline(preprocessor, StandardScaler(), LogisticRegression())","507e3018":"list_model = {'RandomForest': RandomForest, 'AdaBoost': AdaBoost, \n              'SVM': SVM, 'KNN': KNN, 'LogisticReg' : LogReg}","386b7be5":"from sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import learning_curve","5fc92947":"def evaluation(model):\n    model.fit(X_train,y_train)\n    ypred = model.predict(X_test)\n    print(confusion_matrix(y_test,ypred))\n    print(classification_report(y_test,ypred))\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                               cv = 4, scoring = 'f1',\n                                               train_sizes = np.linspace(0.1,1,10))\n    plt.figure(figsize=(12,8))\n    plt.plot(N, train_score.mean(axis = 1))\n    plt.plot(N, val_score.mean(axis = 1))","f7661f85":"from sklearn.metrics import f1_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import learning_curve","21357462":"def evaluation(model):\n    model.fit(X_train,y_train)\n    ypred = model.predict(X_test)\n    print(confusion_matrix(y_test,ypred))\n    print(classification_report(y_test,ypred))\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                               cv = 4, scoring = 'f1',\n                                               train_sizes = np.linspace(0.1,1,10))\n    plt.figure(figsize=(12,8))\n    plt.plot(N, train_score.mean(axis = 1))\n    plt.plot(N, val_score.mean(axis = 1))","fd14e2dc":"for name, model in list_model.items():\n    print(name)\n    evaluation(model)","f6403750":"SVM","be7f652f":"hyper_param = {'svc__gamma': [1e-3, 1e-4], \n               'svc__C': [1,10,100,1000],\n              'pipeline__polynomialfeatures__degree': [2,3,4]}","4d8f7fb1":"grid = RandomizedSearchCV(SVM, hyper_param, scoring='recall', cv=4, n_iter=10)","3e5f40c3":"grid.fit(X_train,y_train)","f10f7965":"print(grid.best_params_)\nypred = grid.predict(X_test)\nprint(classification_report(y_test,ypred))","724b1115":"evaluation(grid.best_estimator_)","2970a96a":"from sklearn.metrics import precision_recall_curve","8ce7e825":"precision, recall, threshold = precision_recall_curve(y_test,grid.best_estimator_.decision_function(X_test))","fbf5cdd3":"precision, recall, threshold = precision_recall_curve(y_test,grid.best_estimator_.decision_function(X_test))\nplt.plot(threshold,recall[:-1], label = 'precision')\nplt.legend()","91bc791e":"LogReg","bb795912":"model_final = LogisticRegression()","7a0ebf59":"model.fit(X_train,y_train)","29295257":"param = {'tol' :np.linspace(0.00001,1,5), 'C': [1.0,2,3,4,5]}","ff2d25df":"Grid = RandomizedSearchCV(model_final, param, cv = 4, n_iter=10)","610d4f4a":"Grid.fit(X_train,y_train)","dae918ae":"print(Grid.best_params_)\nypredL = Grid.predict(X_test)\nprint(classification_report(y_test,ypredL))","27160ef1":"confusion_matrix(y_test,ypredL)","44e4d822":"confusion_matrix(y_test,ypred)","52213b3f":"ypredk = KNN.predict(X_test)","e5c74716":"confusion_matrix(y_test,ypredk)","1a8f6152":"evaluation(model_final)","fe62576a":"print(model.predict(X_test))","85567999":"On remplace les valeurs manquantes de la variable 'Age' par la mediane","69562973":"### Optimisation","8cd359fb":"### Pr\u00e9cision Recall Curve","f4c4fe0d":"### Proc\u00e9dure d'\u00e9valuation ","043cb8ee":"### Relation Target\/variable","ce7e8f4f":"### Analyse de la forme des donn\u00e9es.","68ccfecc":"On met de c\u00f4t\u00e9 une partie des donn\u00e9es pour tester notre mod\u00e8le (***test_set***) et une autre partie pour l'enta\u00eener (***training_set***). \n\n*   Le ***training set***, qui va nous permettre d\u2019entra\u00eener notre mod\u00e8le et sera utilis\u00e9 par l\u2019algorithme d\u2019apprentissage.\n*  Le **testing_set**, qui permet de mesurer l\u2019erreur du mod\u00e8le final sur des donn\u00e9es qu\u2019il n\u2019a jamais vues. \n\nles donn\u00e9es sont s\u00e9par\u00e9es avec les proportions suivantes : *80 % pour le training set et 20 % pour le testing set*.\n","78c0f9f7":"#### Cr\u00e9ation des sous ensemble : Survi et non survi","f6d14d8d":"D'apr\u00e8s cette table de fr\u00e9quence, on voit que la moiti\u00e9 des voyageurs n'ayant pas survecu \u00e9taient des hommes. On remaque aussi une faible mortalit\u00e9 pour le genre f\u00e9minine.","f4812198":"### Histogrammes des variables quantitatives","1c344356":"### Train-Test-Encodage-Nettoyage","d45ed502":"### Mod\u00e9lisation\nOn met en place des mod\u00e8les de machine learning candidat afin d'en choisir un pour mod\u00e9liser notre probl\u00e8me.","444d14b6":"## Pr\u00e9paration des donn\u00e9es\n- Objectif: Mettre les donn\u00e9es dans un format propice au ML\n  -  Train\/Test\n  - Encodage\n  - Nettoyage des NaN","efd45351":"### Analyse de fond","196fd314":"Selon la table de probabilit\u00e9, on voit que les voyageurs de classe 3 n'ont, en majorit\u00e9 pas survecu.","4507a600":"Parmi les voyageurs qui ont ambarqu\u00e9 \u00e0 Southampton, 48% n'ont pas survecu, alors que seuls 24% ont survecu.","29c88990":"### Encodage","c4280ce3":"Dans ce jeu des donn\u00e9es, il nous manque 177 valeurs dans la colonne 'Age', 687 valeurs de la variable 'Cabin' et 2 valeurs de 'Embarked'.","6e75251a":"# Analyse de donn\u00e9es Titanic\n\n## 1. Visualisation de donn\u00e9es en utilisant PowerBI\n![dashboard](https:\/\/dochub.com\/heiko11\/bDa8NX3RdX5jmAER2zA6Ey\/bi-png)","5afdf21b":"On d\u00e9finit une fonction encodage, qui nous permet de remplacer les valeurs des variables Embarked et Sex en des valeurs num\u00e9riques et une fonction preprocessing pour la pr\u00e9paration des donn\u00e9es.","b47b915a":"Pour la variable **Cabin**, plus de 77% des valeurs sont manquantes et 20% valeurs de la colonne **Age**","fc3ab148":"## 2. Analyse de donn\u00e9es en utilisant les librairies de machine learning en python.\n\n### Demarche:\n*   Definir un objectif mesurable : \n> Objectif : pr\u00e9dire si un passager aurait surv\u00e9cu ou pas.\\\n> M\u00e9trique : F1 -> 50% et Recall -> 70%. \\\n> Pr\u00e9cision : permet de r\u00e9duire au maximum le nombre de faux positifs.\\\n> Recall (sensibilit\u00e9) : permet de r\u00e9duire au maximum le nombre de faux n\u00e9gatifs.\\\n> Score F1.\n\n*   AED (Analyse Exploratoire des Donn\u00e9es)\n*   Pr\u00e9paration des donn\u00e9es.\n*   Mod\u00e9lisation"}}