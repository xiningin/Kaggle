{"cell_type":{"a9836fd8":"code","1bdb7fb0":"code","d6004a30":"code","4cc9b5ac":"code","05a5c8fd":"code","f4be8d28":"code","be1e6a9d":"code","7e426108":"code","f1708325":"code","d2240683":"code","469dd6ad":"code","f27d9ff4":"code","e2b24bd7":"code","c75779aa":"code","c65f075c":"code","1469b40d":"code","1c7b4167":"code","4a9817f8":"markdown","d60ec7bc":"markdown","61bec237":"markdown","ba455686":"markdown","18e79208":"markdown","872bc99f":"markdown","1a0a1e5a":"markdown","23b0af95":"markdown","133d651b":"markdown","aa9654f2":"markdown","54a9f72e":"markdown","c6acf8fd":"markdown"},"source":{"a9836fd8":"import numpy as np \nimport pandas as pd\nfrom collections import Counter\nimport pickle\n#!pip install gensim\n#!pip install pyLDAvis\nimport gensim\nfrom gensim import corpora\nimport pyLDAvis.gensim\nimport nltk\n#Download the nltk dependencies \n#nltk.download('punkt')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('wordnet')\n#nltk.download('stopwords')\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \nfrom nltk.corpus import stopwords\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npunc=\"!#$%&'()*+-\/:;<=>?@[\\]^_`{|}~@.\"\nstop_words = list(set(stopwords.words('english')))+['dont']\n","1bdb7fb0":"df_train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n#df_test=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","d6004a30":"df_train.tail(2)","4cc9b5ac":"text_data=list(df['text'])","05a5c8fd":"#Removal of punctuation tokens\nfor i in range(len(text_data)):\n    text_data[i]=text_data[i].translate(str.maketrans('', '', punc))\n    \n#Tokeniztion of data\nword_list=[]\nfor i in range(0,len(text_data)):\n    word_list += nltk.word_tokenize(text_data[i]) ","f4be8d28":"print(*word_list[:20])","be1e6a9d":"words_lemma_list=[]\n\nfor i in range(0,len(word_list)):\n    word=lemmatizer.lemmatize(word_list[i].lower())\n    if(word not in stop_words and len(word)>2):\n        words_lemma_list.append(word)","7e426108":"print(*words_lemma_list[:20])","f1708325":"pos_list=[]\nfor i in range(0,len(words_lemma_list)):\n    pos_list+=nltk.pos_tag([words_lemma_list[i]])","d2240683":"print(*pos_list[20:25])","469dd6ad":"nouns=[]\nfor i in range(len(pos_list)):\n    if((pos_list[i][1] in ['NN','NNS'])):\n        nouns.append(pos_list[i][0])","f27d9ff4":"print(\"Nouns in the dataset :\",*nouns[:10])\nprint(\"Number of Nouns in dataset :\",len(nouns))\nprint(\"Number of distinct Nouns in the dataset : \",len(set(nouns)))","e2b24bd7":"print(\"Most common Nouns in the dataset : \\n\",*Counter(nouns).most_common(5))","c75779aa":"nouns=[[nouns[i]] for i in range(0,len(nouns))]","c65f075c":"dictionary = corpora.Dictionary(nouns)\ncorpus = [dictionary.doc2bow(text) for text in nouns]\n\n#Use pickle files\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","1469b40d":"NUM_TOPICS = 3\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\nldamodel.save('model5.gensim')\ntopics = ldamodel.print_topics(num_words=10)\nfor topic in topics:\n    print(topic)","1c7b4167":"dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n\nlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","4a9817f8":"We now have the LDA distibution for each topic, let's now visualize it in using pyLDAvis","d60ec7bc":"After punctuation tokens removal and word tokenization, our list of words looks like this.","61bec237":"# 1.1 Pre-processing Input\n\n## 1.1.1 Punctuation tokens\nWe will need to remove punctuation tokens from our text. \n\n## 1.1.2 Work Tokenization\nWe are using NLTK's word tokenizer for this task. It splits the string by ' ' and returns individual tokens.  \nThere are many other tokenizers available. I have personally used Spacy's tokenizer as well and it has given me good results.","ba455686":"List of words after lemmatizaion and reducing it to lower case.","18e79208":"There are a few parameters:\n1. Number of Topics - Based on your use case, you can change the number of topics.\n2. Passes - Number of passes you want to the algorithm to run for. ","872bc99f":"LDA creates a dictionary of words from the input, and converts the input into document vectors. ","1a0a1e5a":"Tuples of tokens and their corresponding POS tag","23b0af95":"# 1.3 POS Tagging\nPOS or Parts of Speech Tagging is identifying what is the part of speech of the given string.   \nSince we are performing Topic Modelling, we will consider only Nouns for our task.   \nA POS tag of ['NN','NNS'] corresponds to a Noun.","133d651b":"# 1.2 Lemmatization of strings\nLemmatizaion is the task of reducing a string to its base form.  \nThis helps us in groups words that have the same base form or lemma and will provide more meaning in the coming steps.","aa9654f2":"We will be using the **'text'** column of the Training Data for our task.","54a9f72e":"# Introduction to Latent Dirichlet Allocation \n I've made this notebook to showcase the capability of Latent Dirichlet Allocation( LDA ).  \n I have used this dataset's training data to demonstarate LDA and how to implement it using   \n <b> Gensim and pyLDAvis <\/b>.\n    \nWe will use LDA to perform Topic modelling.  \nTopic modelling refers to the task of identifying topics that best describes a set of documents.     \nThese topics will only emerge during the topic modelling process (therefore called latent).   \nTo tell briefly, LDA imagines a fixed set of topics. Each topic represents a set of words.   \nAnd the goal of LDA is to map all the documents to the topics in a way, such that the words  \nin each document are mostly captured by those imaginary topics.","c6acf8fd":"This is one of my first notebooks on Kaggle. I hope you gained some insight about how to implement LDA   \nand increased your curiosity about Topic Modelling.\n\nI have not included spell-check in the pre-processing step, but do check out my other notebook,  \nwhere I have implemented spell-check functions to handle all types of spelling mistakes.  \n[https:\/\/www.kaggle.com\/amarananth\/spellcheck-python](http:\/\/)    \n\nIf you want to learn more and the math behind LDA, one of my references for the textual information in the beginning.\n[https:\/\/towardsdatascience.com\/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158](http:\/\/)\n\n"}}