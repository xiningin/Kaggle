{"cell_type":{"a514bf68":"code","4e839f68":"code","2be8b05c":"code","0e88e1cb":"code","dd53da27":"code","84587a47":"code","1df87baa":"code","d9051f32":"code","1a55b572":"code","8d70ef9f":"code","07c2f30f":"code","10bfb51e":"code","eb8946ef":"code","21ec1286":"code","66039894":"code","3451e4ec":"code","ce0f4527":"code","6a892205":"code","d7bffaa3":"code","0123a655":"code","3d8858a9":"code","002b526e":"code","eb4c76d4":"code","c17db64e":"code","f83d4ff7":"code","ac8aafc0":"code","e67128a8":"code","ae92ddcf":"code","8e1d5834":"code","d0da5b45":"code","475d50fc":"code","d823007d":"code","33a25d60":"code","c2161946":"code","b37e7a96":"code","f04c0ebe":"code","7ab3e2df":"code","afcc7544":"code","2b51feb8":"code","ec7259b2":"code","a0eb2b98":"code","0cfa2e5c":"code","64b2cc56":"code","9ff50616":"code","bb207426":"code","bf78eba7":"code","f7df8f88":"code","1959c354":"code","d51e16d9":"code","c14dd8c2":"code","15b02dca":"code","7f6c75ba":"code","e3fb213c":"code","8d9df894":"code","4dd8a913":"code","ad0a7537":"code","c549085c":"code","d4988171":"code","1dd1672e":"code","63293a27":"code","c809814d":"code","166796c1":"code","c5cbabed":"code","12d9b9d8":"code","99e896d1":"code","01cff0c0":"code","960c7eb7":"markdown","6a8523d8":"markdown","c1b3408e":"markdown","dca682b3":"markdown","ba474157":"markdown","0cda78f8":"markdown","607db325":"markdown","f34ce16c":"markdown","bc024106":"markdown","74936fdf":"markdown","a8ba4ed5":"markdown","4b06f6d8":"markdown","d0e561b9":"markdown","f5cfc232":"markdown","82c6a382":"markdown"},"source":{"a514bf68":"import pandas as pd\nimport numpy as np #","4e839f68":"df_holidays_events = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv')\ndf_oil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv')\ndf_stores = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ndf_trans = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv')\n\ndf_train = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv', index_col='id')\ndf_test = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv', index_col='id')","2be8b05c":"df_oil1 = df_oil.copy()\ndf_oil1 = df_oil1.ffill()\n\n# copying of train data and merging other data\ndf_train1 = df_train.merge(df_oil1, on = 'date', how='left')\ndf_train1 = df_train1.merge(df_stores, on = 'store_nbr', how='left')\ndf_train1 = df_train1.merge(df_trans, on = ['date', 'store_nbr'], how='left')\n\ndate_data = pd.to_datetime(df_train1['date'])\ndf_train1['year'] = date_data.dt.year\ndf_train1['month'] = date_data.dt.month\ndf_train1['week'] = date_data.dt.isocalendar().week\ndf_train1['quarter'] = date_data.dt.quarter\ndf_train1['day_of_week'] = date_data.dt.day_name()","0e88e1cb":"# copying of test data and merging other data\ndf_test1 = df_test.merge(df_oil1, on = 'date', how='left')\ndf_test1 = df_test1.merge(df_stores, on = 'store_nbr', how='left')\ndf_test1 = df_test1.merge(df_trans, on = ['date', 'store_nbr'], how='left')\n\ndate_data = pd.to_datetime(df_test1['date'])\ndf_test1['year'] = date_data.dt.year\ndf_test1['month'] = date_data.dt.month\ndf_test1['week'] = date_data.dt.isocalendar().week\ndf_test1['quarter'] = date_data.dt.quarter\ndf_test1['day_of_week'] = date_data.dt.day_name()","dd53da27":"nasum = df_train1.isnull().sum()\n# amount of NANs before\nnasum","84587a47":"nasum = df_test1.isnull().sum()\n# amount of NANs before\nnasum","1df87baa":"df_train1.dcoilwtico = df_train1.dcoilwtico.ffill()\ndf_train1.dcoilwtico = df_train1.dcoilwtico.bfill()\n\ndf_train1['trans_mean_family'] = df_train1.groupby(['store_nbr','family'])['transactions'].transform('mean')\ndf_train1['transactions'] = df_train1['transactions'].fillna(df_train1['trans_mean_family'])","d9051f32":"df_test1.dcoilwtico = df_test1.dcoilwtico.ffill()\ndf_test1.dcoilwtico = df_test1.dcoilwtico.bfill()\n\ndf_test1['trans_mean_family'] = df_test1.groupby(['store_nbr','family'])['transactions'].transform('mean')\ndf_test1['transactions'] = df_test1['transactions'].fillna(df_test1['trans_mean_family'])","1a55b572":"nasum = df_train1.isnull().sum()\n# amount of NANs after\nnasum","8d70ef9f":"print(df_train1.columns)\nprint(df_train1.dtypes)","07c2f30f":"for col in df_train1.columns:\n    if col != 'sales':\n        if df_train1[col].dtype == 'float64':\n            df_train1[col] = df_train1[col].astype('float32')\n            df_test1[col] = df_test1[col].astype('float32')\n        if df_train1[col].dtype == 'int64':\n            df_train1[col] = df_train1[col].astype('int8')\n            df_test1[col] = df_test1[col].astype('int8')\n\nprint(df_train1.dtypes)","10bfb51e":"holidays_count_date = pd.DataFrame(df_holidays_events.date.value_counts(), dtype='int8')\nholidays_count_date = holidays_count_date.reset_index()\nholidays_count_date.rename(columns = {'index': 'date', 'date': 'count'}, inplace = True)\nholidays_count_date","eb8946ef":"df_train1.head()","21ec1286":"df_train1 = pd.merge(df_train1, holidays_count_date, how=\"left\", on=\"date\")\ndf_train1 = df_train1.fillna(0)\ndf_test1 = pd.merge(df_test1, holidays_count_date, how=\"left\", on=\"date\")\ndf_test1 = df_test1.fillna(0)\ndf_train1.head()","66039894":"train_gp = df_train1.sort_values('date').groupby(['family', 'store_nbr', 'date'], as_index=False)\ntrain_gp1 = train_gp.agg({'sales':['mean']})\ntrain_gp1.columns = ['family', 'store_nbr', 'date', 'sales']\n\n#test_gp = df_test1.sort_values('date').groupby(['family', 'store_nbr', 'date'], as_index=False)\n#test_gp1 = test_gp.agg({'sales':['mean']})\n#test_gp1.columns = ['family', 'store_nbr', 'date', 'sales']\n\ntrain_gp1.head()","3451e4ec":"look_back = 3","ce0f4527":"def shift_series(data1, look_back=3):\n    for i in range(look_back):\n        data1[f'shift t-{i+1} sales'] = data1['sales'].shift(i+1)\n        \n    data1 = data1.drop(['sales'], axis=1)\n\n    return data1\n\ntrain_gp1 = shift_series(train_gp1, look_back=look_back)\n#test_gp1 = shift_series(test_gp1, look_back=look_back)\ntrain_gp1 = train_gp1.fillna(0)\n#test_gp1 = test_gp1.fillna(0)\ntrain_gp1.head()","6a892205":"#df_train1 = df_train1.merge(train_gp1, on =['family', 'store_nbr', 'date'], how='left')\n#df_test1 = df_test1.merge(test_gp1, on =['family', 'store_nbr', 'date'], how='left')\ndf_train1","d7bffaa3":"#look_forward = 15","0123a655":"#pred_gp = df_train1.sort_values('date').groupby(['family', 'store_nbr', 'date'], as_index=False)\n#pred_gp1 = pred_gp.agg({'sales':['mean']})\n#pred_gp1.columns = ['family', 'store_nbr', 'date', 'sales']\n#pred_gp1.head()","3d8858a9":"def predict_shift_series(data1, look_forward=16):\n    for i in range(look_forward):\n        data1[f'shift t+{i+1} sales'] = data1['sales'].shift(i-1)\n        \n    data1 = data1.drop(['sales'], axis=1)\n\n    return data1\n\n#pred_gp1 = predict_shift_series(pred_gp1, look_forward=look_forward)\n#pred_gp1 = pred_gp1.fillna(0)\n#pred_gp1.head()\n","002b526e":"#y = df_train1.sort_values('date').groupby(['family', 'store_nbr', 'date'], as_index=False)\n#y = y.agg({'sales':['mean']})\n#y.columns = ['family', 'store_nbr', 'date', 'sales']\n#y = y.merge(pred_gp1, on =['family', 'store_nbr', 'date'], how='left')\n#y","eb4c76d4":"#y = y.sort_values('date').groupby(['date', 'store_nbr', 'family'], as_index=False)\n#y = y.agg({'sales':['mean']})\n#y.columns = ['date', 'store_nbr', 'family', 'sales']\n#y","c17db64e":"df_train1['date'] = pd.to_datetime(df_train1['date'])\ndf_test1['date'] = pd.to_datetime(df_test1['date'])\ndf_train1.dtypes","f83d4ff7":"cat_cols = [cname for cname in df_train1.columns if df_train1[cname].dtype == 'object']\ncat_cols","ac8aafc0":"# Prepare dataset to train network (supervised learning)\ny = pd.DataFrame(df_train1.sales)\ndf_train1 = df_train1.drop(['sales'], axis=1)","e67128a8":"df_train1 = df_train1.drop('date', axis=1)\ndf_test1 = df_test1.drop('date', axis=1)","ae92ddcf":"from sklearn import preprocessing\nenc = preprocessing.LabelEncoder()\nfor col in cat_cols:\n    df_train1[col] = enc.fit_transform(df_train1[col].astype(str))\n    df_test1[col] = enc.fit_transform(df_test1[col])","8e1d5834":"import numpy as np\ntrain_data = np.array(df_train1)\ntest_data = np.array(df_test1)","d0da5b45":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\ntrain_data = qt.fit_transform(train_data)\ntest_data = qt.transform(test_data)\n\nqty = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\ny = qty.fit_transform(y)","475d50fc":"from sklearn.preprocessing import MinMaxScaler\nx_scaler = MinMaxScaler()\ndataset_train_sc = x_scaler.fit_transform(train_data)\ndataset_test_sc = x_scaler.transform(test_data)\n\ny_scaler = MinMaxScaler()\ny_sc = y_scaler.fit_transform(y)","d823007d":"import tensorflow as tf","33a25d60":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","c2161946":"from tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\n#from tensorflow.keras.engine.input_layer import Input\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import BatchNormalization","b37e7a96":"input_width = len(dataset_train_sc[0])\ninput_width","f04c0ebe":"def model_builder(lr):\n    \"\"\"\u041c\u043e\u0434\u0435\u043b\u044c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\"\"\"\n    inputA = keras.Input(shape=(input_width))\n    line = Reshape((input_width,1))(inputA)\n    line = Conv1D(filters=1024, kernel_size=2, padding='same', activation='relu')(line)\n    line = BatchNormalization()(line)\n    line = Dropout(0.1)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    #line = Conv1D(filters=1024, kernel_size=1, padding='same', activation='relu')(line)\n    #line = BatchNormalization()(line)\n    #line = Dropout(0.1)(line)\n    \n    #line = MaxPooling1D(pool_size=2)(line)\n    \n    line = Flatten()(line)\n    \n    line = Dense(1024, activation='relu')(line)\n    line = Dense(512, activation='relu')(line)\n    line = Dense(256, activation='relu')(line)\n    #line = Dropout(0.3)(line)\n    \n    outputA = Dense(units=1)(line)\n    model = Model(inputs=inputA, outputs=outputA)\n    #model = keras.models.load_model('models\/model2')\n    model.compile(\n        #loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=\"auto\", name=\"mean_squared_logarithmic_error\"),\n        loss = 'msle',\n        optimizer = Adam(lr=lr), metrics=['mae'],)\n    return model","7ab3e2df":"lr=0.001\nwith strategy.scope():\n    model = model_builder(lr)\n#with strategy.scope():\n#    model = load_model('..\/input\/ss-tsf\/best.h5')","afcc7544":"model.summary()","2b51feb8":"checkpoint_filepath = 'best.h5'\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor='val_loss',\n    mode='min',\n    verbose=1,\n    save_best_only=True)","ec7259b2":"N_split = int(0.05 * len(dataset_train_sc))\ndataset_sc_TRAIN = dataset_train_sc[:-N_split, :]\ndataset_sc_VAL = dataset_train_sc[-N_split:, :]\ny_TRAIN = y_sc[:-N_split]\ny_VAL = y_sc[-N_split:]","a0eb2b98":"from tensorflow.keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.0001, verbose=1, mode='min')","0cfa2e5c":"batch_size = 8192\nEPOCHS = 100\n#EPOCHS = 1\nmodel.fit(dataset_sc_TRAIN, y_TRAIN, validation_data=(dataset_sc_VAL, y_VAL), batch_size=batch_size, epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], shuffle=True)\n","64b2cc56":"with strategy.scope():\n    model = load_model('best.h5')\n#with strategy.scope():\n#    model = load_model('..\/input\/ss-tsf\/best_6.3584e-04-162.h5')","9ff50616":"preds = model.predict(dataset_test_sc)\npseudo_y_TRAIN = np.concatenate([y_TRAIN, preds], axis=0)\npseudo_x_TRAIN = np.concatenate([dataset_sc_TRAIN, dataset_test_sc], axis=0)\nlen(pseudo_y_TRAIN)","bb207426":"batch_size = 8192\nEPOCHS = 1\n#EPOCHS = 1\nmodel.fit(pseudo_x_TRAIN, pseudo_y_TRAIN, validation_data=(dataset_sc_VAL, y_VAL), batch_size=batch_size, epochs=EPOCHS, callbacks=[save_model_callback, reduce_lr], shuffle=True)\n","bf78eba7":"big_x = np.concatenate([dataset_train_sc, dataset_test_sc], axis=0)\nbig_y = np.concatenate([y_sc, preds], axis=0)\n","f7df8f88":"preds = model.predict(dataset_test_sc)\npreds1 = y_scaler.inverse_transform(preds)\npreds1 = qty.inverse_transform(preds1)\nfor i in range(len(preds1)): preds1[i] = 0 if preds1[i] < 0 else preds1[i]\nsales = pd.DataFrame({'Id': df_test.index,'sales': preds1[:,0]})","1959c354":"sales = sales.set_index('Id')","d51e16d9":"sales","c14dd8c2":"df_train","15b02dca":"df_test_sales = pd.merge(df_test, sales, left_index=True, right_index=True)\ndf_test_sales = df_test_sales.fillna(0)\ndf_test_sales","7f6c75ba":"df_test_sales2 = df_test_sales.sort_values('date').groupby(['family', 'store_nbr', 'date'], as_index=False)\ndf_test_sales2 = df_test_sales2.agg({'sales':['mean']})\ndf_test_sales2.columns = ['family', 'store_nbr', 'date', 'sales']\n","e3fb213c":"df_test_sales2","8d9df894":"look_back = 3\ny_shifted_test = shift_series(df_test_sales2, look_back=look_back)\ny_shifted_test = y_shifted_test.fillna(0)","4dd8a913":"y_shifted_test","ad0a7537":"y_shifted_sorted_id = pd.merge(df_test, y_shifted_test, how='left', on=['date', 'store_nbr', 'family'])\ny_shifted_sorted_id = y_shifted_sorted_id.fillna(0)\ny_shifted_sorted_id.index = np.arange(3000888, 3000888+len(y_shifted_sorted_id))\ny_shifted_sorted_id\n#y_shifted_sorted_id = y_shifted.sort_values('date').groupby(['date', 'store_nbr', 'family'], as_index=False)\n#y_shifted_sorted_id = y_shifted_sorted_id.agg({'sales':['mean']})\n#y_shifted_sorted_id.columns = ['family', 'store_nbr', 'date', 'sales']\n#y_shifted_sorted_id","c549085c":"cat_cols = ['family']\nfor col in cat_cols:\n    #df_train1[col] = enc.fit_transform(df_train1[col].astype(str))\n    y_shifted_sorted_id[col] = enc.fit_transform(y_shifted_sorted_id[col])\n#y_shifted_test.index = np.arange(3000888, 3000888+len(y_shifted_test))","d4988171":"y_shifted_sorted_id = y_shifted_sorted_id.drop('date', axis=1)\ny_shifted_sorted_id","1dd1672e":"df_test1.index = np.arange(3000888, 3000888+len(df_test1))\ndf_test1","63293a27":"df_test_shifted_sales = pd.merge(df_test1, y_shifted_sorted_id, how='left', left_index=True, right_index=True)\ndf_test_shifted_sales = df_test_shifted_sales.drop(['store_nbr_y', 'family_y', 'onpromotion_y'], axis=1)\ndf_test_shifted_sales.rename(columns = {'store_nbr_x': 'store_nbr', 'family_x': 'family', 'onpromotion_x': 'onpromotion'}, inplace = True)\ndf_test_shifted_sales","c809814d":"df_train1 = df_train1.merge(train_gp1, on =['family', 'store_nbr', 'date'], how='left')\ndf_test1 = df_test1.merge(test_gp1, on =['family', 'store_nbr', 'date'], how='left')\ndf_train1","166796c1":"train_data = np.array(df_train1)\ntest_data = np.array(df_test_shifted_sales)","c5cbabed":"#df_train1 = df_train1.merge(train_gp1, on =['family', 'store_nbr', 'date'], how='left')\n#df_test1 = df_test1.merge(test_gp1, on =['family', 'store_nbr', 'date'], how='left')\ndf_train1","12d9b9d8":"with strategy.scope():\n    model = load_model('best.h5')\n#with strategy.scope():\n#    model = load_model('..\/input\/ss-tsf\/best_6.3584e-04-162.h5')","99e896d1":"preds = model.predict(dataset_test_sc)\npreds1 = y_scaler.inverse_transform(preds)\npreds1 = qty.inverse_transform(preds1)\nfor i in range(len(preds1)): preds1[i] = 0 if preds1[i] < 0 else preds1[i]","01cff0c0":"output = pd.DataFrame({'Id': df_test.index,'sales': preds1[:,0]})\npath = 'sample_submission.csv'\noutput.to_csv(path, index=False)\noutput ","960c7eb7":"# Transform and Normalise Data","6a8523d8":"# Find Columns for Label Encoding","c1b3408e":"# Fill NA","dca682b3":"# Load Data","ba474157":"# Prepare Neural Network from Tensorflow\/Keras ","0cda78f8":"# Pseudo Labeling","607db325":"# Change types to Reduce Memory Usage","f34ce16c":"# 1","bc024106":"# Merge Datasets and Add Some New Features from Date","74936fdf":"# Predict and Submit","a8ba4ed5":"train\/test split","4b06f6d8":"# Make Shifts","d0e561b9":"Repeat real training","f5cfc232":"# Stay Tuned. Work is going on. Please, UPVOTE","82c6a382":"# Count holidays daily"}}