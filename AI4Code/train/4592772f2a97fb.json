{"cell_type":{"152749b9":"code","44dac999":"code","5463618f":"code","2e32d82b":"code","26472997":"code","4d4cd953":"code","b8108133":"code","aa1a43a2":"code","7c3e79d0":"code","4b68cd9a":"code","91043b81":"code","2947b0fe":"code","99dc1f6f":"code","4217a133":"code","615170f0":"code","1547be66":"code","f06e7dc6":"code","36e1fe8b":"code","6a85901e":"markdown","d086b9c4":"markdown","7abed7bb":"markdown","3dd14b52":"markdown","06e65073":"markdown","be218928":"markdown"},"source":{"152749b9":"import pandas as pd\nimport networkx as nx\nfrom node2vec import Node2Vec\nfrom cytoolz import sliding_window\nfrom collections import Counter\n\ndf = pd.read_csv(\"..\/input\/titles.csv\")\ndf.fillna(\"\", inplace=True)","44dac999":"from nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom cytoolz import isdistinct, topk, sliding_window, memoize\nfrom operator import itemgetter\nfrom itertools import product\n\nstops = set(stopwords.words('english'))\nstops.update(set(punctuation))\n\n@memoize\ndef int_to_roman(x):\n    \"\"\"\n    Normalizing titles like software engineer 3\n    \n    Also filters out numbers that are not likely part of a seniority description, i.e. 2000 \n    \"\"\"\n    \n    if not x.isnumeric():\n        return x\n    x = int(x)\n    ints = (1000, 900,  500, 400, 100,  90, 50,  40, 10,  9,   5,  4,   1)\n    nums = ('M',  'CM', 'D', 'CD','C', 'XC','L','XL','X','IX','V','IV','I')\n    result = []\n    for i in range(len(ints)):\n        count = int(x \/ ints[i])\n        result.append(nums[i] * count)\n        x -= ints[i] * count\n    result = ''.join(result).lower()\n    if any([n in result for n in ['M', 'C', 'D', 'X', 'L']]):\n        return \"\"\n    return ''.join(result).lower()","5463618f":"@memoize\ndef tokenize(x:str):\n    return wordpunct_tokenize(x)\n\ndef lowercase(x:list):\n    return [token.lower() for token in x]\n\ndef remove_stopwords(x:list, stopwords=stops):\n    return [token for token in x if token not in stopwords]\n\ndef is_truthy(x):\n    if x:\n        return True\n    return False\n\ndef preprocess_title(x:str):\n    if pd.isna(x) or x == \"\":\n        return []\n    tokens = tokenize(x)\n    tokens = lowercase(tokens)\n    tokens = remove_stopwords(tokens)\n    if not tokens:\n        return []\n    tokens = [int_to_roman(token) for token in tokens]\n    tokens = list(filter(is_truthy, tokens))\n    return tokens","2e32d82b":"# Get title counts\n\ntitle_counts = Counter(df.stack().apply(lambda x: \" \".join(preprocess_title(x))).values)\ndel title_counts['']","26472997":"def get_gram_counts(tokens, best_grams):\n    results = [] \n    seen = set()\n    if not best_grams:\n        gram_product = product(tokens, repeat=2)\n    else:\n        gram_product = product(tokens, best_grams)\n    for title_grams in gram_product:\n        if not isdistinct(title_grams):\n            continue\n        title = \" \".join(title_grams)\n        if title in seen:\n            continue\n        else:\n            seen.add(title)\n        count = title_counts.get(title, 0)\n        results.append((title, count))\n    return sorted(results, key=itemgetter(1), reverse=True)\n\n\n@memoize\ndef optimize_title(x:str, topn=3, title_counts=title_counts):\n    tokens = preprocess_title(x)\n    if not tokens:\n        return x\n    if len(tokens)==1:\n        return tokens[0]\n    \n    starting_score = title_counts.get(x, 1)\n    best_ngrams = [(token, title_counts.get(token, 0)) for token in tokens]\n    gram_counter = 2\n    while gram_counter <= len(tokens): # Continue chaining tokens to get the highest score\n        gram_counts = get_gram_counts(tokens, [token for token, score in best_ngrams])\n        best_ngrams.extend(gram_counts)\n        best_ngrams = list(topk(topn, best_ngrams, key=itemgetter(1)))\n        if not any([g in best_ngrams for g in gram_counts]):  # The most recent get_gram_counts did not 'make the cut'\n            best_ngram_found = topk(1, best_ngrams, key=itemgetter(1))[0]\n            best_ngram, best_ngram_score = best_ngram_found\n            if best_ngram_score > starting_score:\n                return best_ngram\n            else:\n                return x\n        gram_counter += 1\n    return best_ngrams[0][0]\n\n","4d4cd953":"for title, count in sorted(title_counts.items(), key=itemgetter(1))[:50]:\n    optimum_title = optimize_title(title)\n    optimum_count = title_counts[optimum_title]\n    print(\"{}, Count: {}  =======> {}, Count: {}\".format(title, count, optimum_title, optimum_count))","b8108133":"%%time\ng = nx.DiGraph()\ntitle2optimum = {}\nfor i, row in df.iterrows():\n    row_titles = [row[c] for c in df.columns]\n    for title in row_titles:\n        optimized_title = optimize_title(title)\n        if title not in title2optimum:\n            title2optimum[title] = optimized_title\n    row_titles = [optimize_title(x) for x in row_titles]\n    row_titles = list(filter(lambda x: x, row_titles))\n    row_titles = reversed(row_titles)\n    \n    for prior_title, new_title in sliding_window(2, row_titles):\n        if g.has_edge(prior_title, new_title):\n                g.edges[(prior_title, new_title)]['weight'] += 1\n        else:\n            g.add_edge(prior_title, new_title, weight=1)","aa1a43a2":"# What edges have a weight of 1?\nweight_filter = lambda x: [(n1, n2) for n1, n2, weight in g.edges.data('weight') if weight ==x]\nweight_filter_edges = weight_filter(1)\ng.remove_edges_from(weight_filter_edges)  # Remove those edges\n\n# What nodes are now unconnected?\ndegree_filter = lambda x: [node for node, degree in dict(g.degree()).items() if degree <= x]\ndegree_filter_nodes = degree_filter(0)\ng.remove_nodes_from(degree_filter_nodes) # Remove those nodes\n\n# We had an expectation that each job history should be at least 3 titles long. After pruning the graph we will likely have nodes that are now shorter than 3 and should be removed\ntwo_degree_nodes = degree_filter(2)","7c3e79d0":"two_degree_nodes","4b68cd9a":"import operator\nfrom operator import ge as greater_or_equal\nfrom operator import le as less_or_equal\n\nug = g.to_undirected()\n\ndef filter_by_chain_length(node, chain_length, op, g=g):\n    chain_counter = 0\n    n_ancestors = len(nx.ancestors(g, node))\n    chain_counter += n_ancestors\n    if op == operator.ge:  # we may be able to skip a step\n        if op(chain_counter, chain_length):\n            return True\n    \n    n_descendants = len(nx.descendants(g, node))\n    chain_counter += n_descendants\n    return op(chain_counter, chain_length)","91043b81":"short_chain_nodes = list(filter(lambda x: filter_by_chain_length(x, 4, less_or_equal), two_degree_nodes))\n\nprint(\"Removing {} Short Chain Nodes\".format(len(short_chain_nodes)))\ng.remove_nodes_from(short_chain_nodes)","2947b0fe":"two_degree_nodes = [x for x in two_degree_nodes if x not in short_chain_nodes]\nassert not list(filter(lambda x: filter_by_chain_length(x, 4, less_or_equal), two_degree_nodes))","99dc1f6f":"# Making a temp folder for Joblib. Parallel execution can quickly eat up Kaggle's 16GB of memory\n\nimport os\n\ntemp_folder = r\"\/kaggle\/working\/temp_folder\"\n\nif not os.path.isdir(temp_folder):\n    os.makedirs(temp_folder)\n","4217a133":"node2vec = Node2Vec(g, dimensions=64, walk_length=5, num_walks=200, workers=4, temp_folder=temp_folder, p=0.01)  # Use temp_folder for big graphs","615170f0":"model = node2vec.fit(min_count=2, window=3)","1547be66":"model.wv.save_word2vec_format('titles.wv')\nmodel.save('titles_node2vec.model')\n","f06e7dc6":"import pickle\nwith open('title2optimum.pkl', 'wb') as pfile:\n    pickle.dump(title2optimum, pfile)","36e1fe8b":"for title, _ in Counter(title_counts).most_common(100):\n    if not g.edges(title):\n        continue\n    print(title.center(80, \"=\"))\n    sims = model.wv.most_similar(title)\n    for sim_title, score in sims:\n        print(\"\\t{} : {}\".format(sim_title, score))\n    print(\"\")","6a85901e":"### We build the graph with the expectation of modifying it\n1. Removing Edges by filtering on weight \n2. Finding Nodes with a degree == 0 (unconnected)\n","d086b9c4":"### Requiring minimum chain length\n1. We want to exclude isoloated histories","7abed7bb":"The following are broken out so that we can memoize wordpunct_tokenize","3dd14b52":"### Normalizing\n\nI noticed a tendency for uncommon job titles to **include**, rather than exclude additional words. Usually, these have some form of a common title embedded.\n\nE.g. Technical Leader, Datacenter\n\nBy dynamically locating the most common string within a job title we can hopefully preserve its essence while removing \"noise\"   ","06e65073":"### Exploring Job Titles\n\nJob Titles attempt to capture one's scope of work in one line of text. This leads to seemingly infinite variablility which makes large scale analysis difficult.\n\nIn this notebook, I will explore the applications of Node2Vec to this domain problem ","be218928":"Some examples of optimize_title"}}