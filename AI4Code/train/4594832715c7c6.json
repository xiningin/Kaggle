{"cell_type":{"766b3389":"code","e3346d06":"code","c8260c29":"code","78bf6ee8":"code","ae4a880c":"code","fd8fc9b1":"code","51cd2767":"code","ae4f86dd":"code","39314aa4":"code","9c4d103d":"code","64140ab3":"code","c3b0c5e8":"code","9b5c964d":"code","a17fec63":"code","70dc352f":"code","862a4f2e":"code","151363f1":"code","433a70e8":"code","b1a203d7":"markdown","5f080163":"markdown","3834e7e7":"markdown","565707c9":"markdown","aec1e95d":"markdown","72a65d71":"markdown","276580dd":"markdown","3635e2dc":"markdown","f18f6f84":"markdown","909e23b6":"markdown","57270741":"markdown","4843ad9a":"markdown","0b3eb2ff":"markdown","39c679e1":"markdown","38feeee9":"markdown","92b59034":"markdown","d1d9d99e":"markdown","663610d3":"markdown","5b7d824b":"markdown","79d8ab7d":"markdown","0dbc3b5f":"markdown"},"source":{"766b3389":"import numpy as np\nimport pandas as pd\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import Ridge\n\nimport datetime\nimport gc","e3346d06":"train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv')\ntrain['Province_State'].fillna('', inplace=True)\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['day'] = train.Date.dt.dayofyear\ntrain['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv')\ntest['Province_State'].fillna('', inplace=True)\ntest['Date'] = pd.to_datetime(test['Date'])\ntest['day'] = test.Date.dt.dayofyear\ntest['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n\nday_min = train['day'].min()\ntrain['day'] -= day_min\ntest['day'] -= day_min\n\nmin_test_val_day = test.day.min()\nmax_test_val_day = train.day.max()\nmax_test_day = test.day.max()\nnum_days = max_test_day + 1\n\nmin_test_val_day, max_test_val_day, num_days","c8260c29":"data = train\ndel train, test \ngc.collect()","78bf6ee8":"cases = np.log1p(data.pivot(index='geo', columns='day', values='ConfirmedCases'))\ndeaths = np.log1p(data.pivot(index='geo', columns='day', values='Fatalities'))\nnum_geo = cases.shape[0]","ae4a880c":"max_date = max_test_val_day\ndelta = 8\ngeos = ['Italy_', 'France_', 'Spain_', 'China_Hubei', 'Germany_', 'Afghanistan_']\nfor geo in geos:\n    print(geo)\n    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n    ax.plot(cases.loc[geo, :max_date].values)\n    ax.plot(deaths.loc[geo, delta:max_date].values)\n    ax.plot(cases.loc[geo, :max_date-delta].values - deaths.loc[geo, delta:max_date].values)\n    plt.show()","fd8fc9b1":"max_date = max_test_val_day\ngeo = 'Spain_'\nfor delta in range(6, 11):\n    print(delta)\n    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n    ax.plot(cases.loc[geo, :max_date].values)\n    ax.plot(deaths.loc[geo, delta:max_date].values)\n    ax.plot(cases.loc[geo, :max_date-delta].values - deaths.loc[geo, delta:max_date].values)\n    plt.show()","51cd2767":"def get_dataset(start_pred, num_train, lag_period):\n    days = np.arange( start_pred - num_train + 1, start_pred + 1)\n    lag_cases = np.vstack([cases.iloc[:, d - lag_period : d] for d in days])\n    lag_deaths = np.vstack([deaths.iloc[:, d - 1 : d] for d in days])\n    target_deaths = np.vstack([deaths.iloc[:, d : d + 1] for d in days])\n    return lag_cases, lag_deaths, target_deaths","ae4f86dd":"def val_score(true, pred):\n    return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\ndef fit_eval(lr, data, fit):\n    lag_cases, lag_deaths, target_deaths = data\n    \n    X = np.hstack([lag_cases, lag_deaths])\n    y = target_deaths\n    if fit:\n        lr.fit(X, y)\n    y_pred = lr.predict(X)\n    score = val_score(y, y_pred)\n    return score","39314aa4":"def train_model(train_data, valid_data):\n    lr = Ridge(fit_intercept=False)        \n    train_score = fit_eval(lr, train_data, fit=True)\n    valid_score = fit_eval(lr, valid_data, fit=False)   \n    print('train: %0.4f' %  train_score,\n          'val : %0.4f' %  valid_score,\n          )\n    return train_score, valid_score","9c4d103d":"num_train = 5\nlag_period = 14\n\nstart_val = min_test_val_day\nlast_train = start_val - 1\ntrain_data = get_dataset(last_train, num_train, lag_period)\nvalid_data = get_dataset(start_val, 1, lag_period)\n_ = train_model(train_data, valid_data)\n        \n            ","64140ab3":"last_val = max_test_val_day\nlast_train = min_test_val_day - 1\nnum_val = last_val - last_train\n\ntrain_data = get_dataset(last_train, num_train, lag_period)\nvalid_data = get_dataset(last_val, num_val, lag_period)\n_ = train_model(train_data, valid_data)\n        ","c3b0c5e8":"def evaluate():\n    train_scores = []\n    valid_scores = []\n\n    for start_val_delta in range(3, -8, -3):\n        last_val = max_test_val_day\n        last_train = min_test_val_day - 1 + start_val_delta\n        num_val = last_val - last_train\n        print(num_val, end='  ')\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(last_val, num_val, lag_period)\n        train_score, valid_score = train_model(train_data, valid_data)\n        train_scores.append(train_score)\n        valid_scores.append(valid_score)\n\n    print('avg train: %0.4f val: %0.4f' % (np.mean(train_scores), np.mean(valid_scores), ))\n    \nevaluate()","9b5c964d":"def fit_eval(lr, data, fit):\n    lag_cases, lag_deaths, target_deaths = data\n    X = np.hstack([lag_cases, lag_deaths])\n    y = target_deaths\n    if fit:\n        lr.fit(X, y)\n    y_pred = lr.predict(X)\n    y_prev = lag_deaths[:, -1:]  # new\n    y_pred = np.maximum(y_pred, y_prev) # new\n    score = val_score(y, y_pred)\n    return score\n\nevaluate()","a17fec63":"def get_dataset(start_pred, num_train, case_lag_start, case_lag_end):\n    days = np.arange( start_pred - num_train + 1, start_pred + 1)\n    lag_cases = np.vstack([cases.iloc[:, d - case_lag_start : d - case_lag_end ] for d in days]) # modified\n    lag_deaths = np.vstack([deaths.iloc[:, d - 1 : d] for d in days]) \n    target_deaths = np.vstack([deaths.iloc[:, d : d + 1] for d in days])\n    return lag_cases, lag_deaths, target_deaths\n\ndef evaluate(case_lag_start, case_lag_end ):\n    train_scores = []\n    valid_scores = []\n\n    for start_val_delta in range(3, -8, -3):\n        last_val = max_test_val_day\n        last_train = min_test_val_day - 1 + start_val_delta\n        num_val = last_val - last_train\n        print(num_val, end='  ')\n        train_data = get_dataset(last_train, num_train, case_lag_start, case_lag_end)\n        valid_data = get_dataset(last_val, num_val, case_lag_start, case_lag_end)\n        train_score, valid_score = train_model(train_data, valid_data)\n        train_scores.append(train_score)\n        valid_scores.append(valid_score)\n\n    print('avg train: %0.4f val: %0.4f' % (np.mean(train_scores), np.mean(valid_scores), ))","70dc352f":"    \ncase_lag_start, case_lag_end = 15,2\nevaluate(case_lag_start, case_lag_end )","862a4f2e":"countries = [g.split('_')[0] for g in cases.index]\ncountries = pd.factorize(countries)[0]\ncountries = countries.reshape((-1, 1))\nohe = OneHotEncoder(sparse=False)\ngeo_ids_base = ohe.fit_transform(countries)\ngeo_ids_base.shape","151363f1":"def get_dataset(start_pred, num_train, case_lag_start, case_lag_end):\n    days = np.arange( start_pred - num_train + 1, start_pred + 1)\n    lag_cases = np.vstack([cases.iloc[:, d - case_lag_start : d - case_lag_end ] for d in days]) # modified\n    lag_deaths = np.vstack([deaths.iloc[:, d - 1 : d] for d in days]) \n    target_deaths = np.vstack([deaths.iloc[:, d : d + 1] for d in days])\n    geo_ids = np.vstack([geo_ids_base for d in days]) # modified\n    return lag_cases, lag_deaths, target_deaths, geo_ids # modified\n\ndef fit_eval(lr, data, fit):\n    lag_cases, lag_deaths, target_deaths, geo_ids = data # modified\n    X = np.hstack([lag_cases, lag_deaths, geo_ids]) # modified\n    y = target_deaths\n    if fit:\n        lr.fit(X, y)\n    y_pred = lr.predict(X)\n    y_prev = lag_deaths[:, -1:]  \n    y_pred = np.maximum(y_pred, y_prev) \n    score = val_score(y, y_pred)\n    return score\n\nevaluate(case_lag_start, case_lag_end )","433a70e8":"countries = [g.split('_')[0] for g in cases.index]\ncountries = pd.factorize(countries)[0]\ncountries = countries.reshape((-1, 1))\nohe = OneHotEncoder(sparse=False)\ngeo_ids_base = ohe.fit_transform(countries)\ngeo_ids_base = 0.2 * geo_ids_base  # new\n\nevaluate(case_lag_start, case_lag_end )","b1a203d7":"We now see some improvement of the validation score.","5f080163":"Similarly, we should use more than one training fold. ","3834e7e7":"Plots with delta equal to 7 looks almost as good.  other values see a degradation of the curve, as shown for Spain:","565707c9":"Let us now look at lag cases values.  Our current model depends on previous day confirmed cases which looks very unlikely.  What we should do is to restrict lag cases.  Let's introduce two more control parameters for that.","aec1e95d":"This modeling has been used as part of our currently top submission.","72a65d71":"Testing on one validation day is a bit brittle, we should test on all the validation days we have.  For this we just need to get a larger validation dataset.  ","276580dd":"First some imports","3635e2dc":"Let's now turn data into matrices with geos as rows, amd days as columns.  For this we use the convenient pivot from pandas.  We will also apply log1p rescaling so that the competition metric becomes root mean squared error.","f18f6f84":"Improvement is too small to show, but good to have.  A way to improve Ridge model is to tune its regularization weight.  Increasing it helps a bit:","909e23b6":"This notebook shows how fatalities can be predicted as a lag of confirmed cases.  This model confirms clinical observation about duration of disease in case of fatal outcome.","57270741":"Reading data.  We introduce and normalize days since data starting date, as well as a geo column for all country\/province pairs.  Last, we define our validation data to be data from first test day to last train day included.","4843ad9a":"Using same train\/valid split we now get a validation score degradation whiole trainings core improves a lot.  This is a symptom of overfiting to the features we just added.  A way to cope with this is to add more regularization of the coefficients for the added country features.  \n\nOne simple trick is to divide the features by a constant. Mechanically, the same model will need to multiply their coefficients by the same value. And regulariwzation cost for these coefficients will also be multiplied by the square of this value (roughly speaking). Let's try it.\n","0b3eb2ff":"Our main procedure trains a model, then computes its score on the validation data.  It takes as input a training dataset and a validation dataset.\n","39c679e1":"Some grid search led to these values.","38feeee9":"Allright, how can we model the different we noticed across countries?  One way is just to add an indicator variable for each geo.  We need to create it, and update our utility functions","92b59034":"From now on we will just work with training data.  When submitting to the competition we of course need to make predictions for the full test set.  This part is left out of the introductory notebook but can be seen in one of our two selected submissions.","d1d9d99e":"Now, let's make some plots to see if fatalities curve is following or not the confirmed case curves.  The following assumes a 8 day delay between when people are confirmed case and when they die, for those who die.  We see that the green curve is rather flat, which confirms there is a linear dependancy between when people are diagnosed and when they die. We also see that the proportion of fatalities is not the same across countries: the green curve height varies from geo to geo.","663610d3":"Let's try it for the train\/validation split we discussed earlier. The values for `num_train` and `lag_period` were found via grid search in previous runs of the final model.","5b7d824b":"Let's model this.  The first thing to do is to create datasets for training and validation.  A dataset will be defined by a `start_pred` day.  It will have `lag_period` past confirmed cases data prior to that day, as well as previous day cumulated deaths, and fatailities target for the day.  We further refine this by allowing the creation of `num_train` datasets for several days, and concatenating them.  ","79d8ab7d":"We are predicting a cumul i.e. a function that increases over time.  Therefore, we should make our predictions at least as large as the previous say.  A simple tweak in our utility function does the job.","0dbc3b5f":"We can now define our procedure for training our model.  We will use scikit-learn linear regression.  More specifically we will use the L2 regularized version of it, called Ridge.  Our procedure takes as input a dataset and a Ridge model.  It fits it to the data then computes a score.  We also add an optoin to only compute the score without fitting the model.  This will be useful for validation."}}