{"cell_type":{"ee4494d0":"code","9d9caf41":"code","f7e69d7d":"code","79ef099c":"code","1f69d663":"code","a6d5bdb5":"code","1c88c510":"code","335a11eb":"code","87c34fa2":"markdown","8b876919":"markdown","7cc49acb":"markdown","cdb237b4":"markdown","2841c58f":"markdown","0681fa0d":"markdown","deec9ac1":"markdown","0dfdb86b":"markdown","c4ff5c47":"markdown","1ed62a7e":"markdown","4d828520":"markdown","5bba624b":"markdown","ee9924d8":"markdown"},"source":{"ee4494d0":"# libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy\nimport os\nimport plotly.express as px\nfrom scipy import stats\n\n# read in data provided by kaggle for the competition\nsource_filesinfolder = os.listdir('..\/input\/m5-forecasting-accuracy')\nsource_filenames = [file.replace('.csv', '') for file in source_filesinfolder]\nsource_data_path = '\/kaggle\/input\/m5-forecasting-accuracy'\nfor file in source_filenames:\n    globals()[file] = pd.read_csv(f'{source_data_path}\/{file}.csv')   ","9d9caf41":"print(source_filenames)\nfor file in source_filenames:\n    print(globals()[file])","f7e69d7d":"# Get a dataset with info on each item\nitem_info = sales_train_validation[[\"dept_id\",\"cat_id\",\"item_id\"]].drop_duplicates()\n\n# Join category onto sell_prices\nsell_prices_for_eda = (\n    sell_prices\n    .merge(\n        item_info\n        , how = \"left\"\n        , on = \"item_id\"\n    )\n)\n\n# Summarise by item & store\nitem_store_summaries = (\n    sell_prices_for_eda\n    .groupby(['item_id', 'store_id' ,'dept_id', 'cat_id'])['sell_price']\n    .agg(\n        price_mode = lambda x: stats.mode(x)[0][0]\n        , price_mean = 'mean'\n        , price_min = 'min'\n        , price_max = 'max'\n        , price_sd = np.std\n        , price_sum = 'sum'\n        , price_size = 'size'\n    )\n    .reset_index()\n    .assign(max_discount_from_peak = lambda x: 1 - x['price_min']\/x['price_max'])\n)\n\n# Summarise by item\nitem_summaries = (\n    item_store_summaries\n    .groupby(['item_id','dept_id'])['price_mode']\n    .agg(\n        price_mode__nationwide_mean = 'mean'\n        , min = 'min'\n        , max = 'max'\n    )\n    .reset_index()\n    .assign(price_mode__nationwide_range = lambda x: x['max'] - x['min'])\n)\n\n","79ef099c":"# Summarise by time\ntime_dept_summaries = (\n    sell_prices_for_eda\n    .groupby(['wm_yr_wk','dept_id'])['sell_price']\n    .agg(\n        perct_25th = lambda x: np.percentile(x, q = 25)\n        , perct_75th = lambda x: np.percentile(x, q = 75)\n        , perct_95th = lambda x: np.percentile(x, q = 95)\n        , median = 'median'\n        , mean = 'mean'\n    )\n    .reset_index()\n    .merge(\n        calendar.groupby('wm_yr_wk').first().reset_index()[['wm_yr_wk','date']].rename(columns={'date':'start_of_week'})\n        , how = \"left\"\n        , on = \"wm_yr_wk\"\n    )\n)","1f69d663":"px.histogram(\n    item_info\n    , x = \"dept_id\"\n    , title = 'Count of unique items per department'\n).show()","a6d5bdb5":"px.box(\n    item_store_summaries\n    , x = \"dept_id\"\n    , y = \"price_mode\"\n    , color = \"store_id\"\n    , title = 'Distributions of standard (mode) item prices across dept and store'\n).show()","1c88c510":"px.box(\n    item_summaries\n    , x = 'dept_id'\n    , y = 'price_mode__nationwide_mean'\n    #, color = \"store_id\"\n    , title = 'Distributions of item prices across dept'\n).show()","335a11eb":"px.line(\n    time_dept_summaries\n    , x = \"start_of_week\"\n    , y = \"mean\"\n    , color = \"dept_id\"\n    , title = 'Mean price per department over time'\n).show()","87c34fa2":"## Quick Peek at the Datasets","8b876919":"### Distributions of item prices (nationwide mean of the store mode prices) across departments\n\nThat's a mouthful, but this should be the fairest perception of a price distribution within a department","7cc49acb":"# High level price and sale info\n\nWe've got over 3k items, so can't easily look at each individually, but let's look at some high level info on sales, prices within categories, and prices across time & stores.\n\nWe'll calculate some item level summaries\n","cdb237b4":"### ... *to be continued* ...\n\nwe've touchced on the pricing, but should look to also gain some perspective on sales, events & SNAP^ windows\n\n> ^The United States federal government provides a nutrition assistance benefit called the Supplement Nutrition Assistance Program (SNAP).  SNAP provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products.  In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1\/10 of the people will receive the benefit on their card.  More information about the SNAP program can be found here: https:\/\/www.fns.usda.gov\/snap\/supplemental-nutrition-assistance-program","2841c58f":"And now let's visualise some of this...","0681fa0d":"# Introduction\n\nIn this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n\n*Note that this is the **point estimate** competition (there is also a complementary competition running which is concerned with predicting the uncertainty distribution).*\n\n### Evaluation\nThis competition uses a Weighted Root Mean Squared Scaled Error (RMSSE):\n![](https:\/\/i.imgur.com\/uqhsf3d.png)\n![](https:\/\/i.imgur.com\/B1hglCf.png)\n\n### Submission\nEach row contains an id that is a concatenation of an item_id and a store_id, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard). You are predicting 28 forecast days (F1-F28) of items sold for each row. For the validation rows, this corresponds to d_1914 - d_1941, and for the evaluation rows, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)\n\n# Understanding the Data\n\nThe data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details split as follows:\n\n![](https:\/\/i.imgur.com\/C5hASXe.png)\n\nLet's take a deeper dive!\n\n# Exploratory Analysis\n\n## Setup","deec9ac1":"Pretty consistent across stores, but there will inevitably be some differences for individual items.","0dfdb86b":"### Median price per dept over time","c4ff5c47":"### Distributions of standard (mode) item prices across dept and store\n","1ed62a7e":"Some strange fluctuations here - probably due to additions and removal of items (would need to verify).","4d828520":"# Some opening remarks\n\nI consider myself a competent R coder; however, this is my first proper foray into python so please forgive any bad practices (and feel free to draw my attention to them).\n\nFull discloser: I have copied a few imgur links from the [It is time for M5. Going step by step](https:\/\/www.kaggle.com\/artgor\/it-is-time-for-m5-going-step-by-step) notebook as they're really useful!\n","5bba624b":"and summaries of prices over time","ee9924d8":"### Number of items per department"}}