{"cell_type":{"7c08c00a":"code","1e285638":"code","7599cfec":"code","dbfe5e08":"code","3ed670b5":"code","98cdc4b9":"code","e648cb75":"code","ca61de8b":"code","ae14e51c":"code","1e5fc06f":"code","9a7af6f4":"code","f9ef92ff":"code","ff13986b":"code","0c082b4d":"code","71fc0bfc":"code","3ef4902a":"code","622e03c1":"code","10ed2082":"code","d6551c62":"code","977787cf":"code","5e9557ff":"code","afaeab7c":"code","837a5b4e":"code","5e772879":"code","f5f7944b":"code","d5e880b4":"code","8861dfc0":"code","134e3ccc":"code","032e085b":"code","9dd9ebf9":"code","29c7e659":"code","5eb3c83d":"code","692e8ddc":"code","e8a4dec6":"code","31fa6463":"code","5db6a855":"code","87c1f3ea":"code","1d48eb13":"code","6eb9f970":"code","790232a6":"code","556ec8f4":"code","45b3c0b9":"code","35a63c8a":"code","53422ea9":"code","f6f152ee":"code","a66aaa84":"code","55bc49a4":"code","cf298949":"code","af61566d":"code","101ab33b":"code","fc9380b9":"code","d1a108bd":"code","554141e6":"code","73667956":"code","a0162c59":"code","b1f2d32a":"code","356df55b":"code","4234693c":"code","03f064ea":"code","f4cfcb76":"code","f01401b0":"code","6f33e7ab":"markdown","e0d1bc8c":"markdown","dd3f5c3e":"markdown","59413e64":"markdown","2bba50e7":"markdown","52c87ea2":"markdown","92b5dda8":"markdown","b9063811":"markdown","ab2ac2d9":"markdown","c16c0a55":"markdown","4f694c3d":"markdown","6f0ddf6c":"markdown","f4153051":"markdown","92017649":"markdown","a5650823":"markdown","10948f72":"markdown","6c62e1bd":"markdown","2fb0f59c":"markdown","0e680182":"markdown","e3e5ad04":"markdown","113e6056":"markdown","e7be7151":"markdown","e69677f4":"markdown","3b4ab062":"markdown","b25b87f1":"markdown","24f5fe6c":"markdown","258cc312":"markdown","cc6d2b33":"markdown","49765f57":"markdown","e96c0e18":"markdown"},"source":{"7c08c00a":"#IMPORT THE LIBRARIES....\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport shap\n\npd.set_option(\"display.max_columns\",None)\npd.set_option(\"display.max_rows\",None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1e285638":"# data Load\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","7599cfec":"train.isnull().sum()[train.isnull().sum() != 0]","dbfe5e08":"# train data 'id' remove\ntrain_df = train.drop(['Id', 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], axis=1)\ntest_df = test.drop(['Id', 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], axis=1)","3ed670b5":"def encode_target_smooth(data, target, categ_variables, smooth):\n    \"\"\"    \n    Apply target encoding with smoothing.\n    \n    Parameters\n    ----------\n    data: pd.DataFrame\n    target: str, dependent variable\n    categ_variables: list of str, variables to encode\n    smooth: int, number of observations to weigh global average with\n    \n    Returns\n    --------\n    encoded_dataset: pd.DataFrame\n    code_map: dict, mapping to be used on validation\/test datasets \n    defaul_map: dict, mapping to replace previously unseen values with\n    \"\"\"\n    train_target = data.copy()\n    code_map = dict()    # stores mapping between original and encoded values\n    default_map = dict() # stores global average of each variable\n    \n    for v in categ_variables:\n        prior = data[target].mean()\n        n = data.groupby(v).size()\n        mu = data.groupby(v)[target].mean()\n        mu_smoothed = (n * mu + smooth * prior) \/ (n + smooth)\n        \n        train_target.loc[:, v] = train_target[v].map(mu_smoothed)        \n        code_map[v] = mu_smoothed\n        default_map[v] = prior        \n    return train_target, code_map, default_map","98cdc4b9":"Tr_mean = pd.concat([train_df.loc[:, train_df.dtypes == object],train_df['SalePrice']], axis =1)\nTe_mean = test_df.loc[:, test_df.dtypes== object]","e648cb75":"cat_vars = ['MSZoning', 'Street', 'LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual',\n            'ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','GarageType','GarageFinish',\n            'GarageQual','GarageCond','PavedDrive','SaleType','SaleCondition']\n\ntrain_target_smooth, target_map, default_map = encode_target_smooth(Tr_mean , 'SalePrice', cat_vars, 500)\ntest_target_smooth = Te_mean.copy()\nfor v in cat_vars:\n    test_target_smooth.loc[:, v] = test_target_smooth[v].map(target_map[v])","ca61de8b":"no_Tr_mean = train_df.loc[:, train_df.dtypes != object]  ##object\uc544\ub2cc train features\nno_Tr_mean = no_Tr_mean.drop(['SalePrice'],axis =1)\nno_Te_mean = test_df.loc[:, train_df.dtypes != object]   ##object\uc544\ub2cc test features\n\ndf_train = pd.concat([train_target_smooth,no_Tr_mean], axis = 1)\ndf_test = pd.concat([test_target_smooth,no_Te_mean], axis = 1)","ae14e51c":"df_train = df_train.fillna(df_train.mean())\ndf_test = df_test.fillna(df_test.mean())","1e5fc06f":"# data segmentation\nX = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","9a7af6f4":"from sklearn.model_selection import GridSearchCV\n\ndef get_best_params(model, params):\n    grid_model = GridSearchCV(model, param_grid=params, scoring='neg_mean_squared_error', cv=5)\n    grid_model.fit(X_train, y_train)\n    rmse = np.sqrt(-1* grid_model.best_score_)\n    print('Optimal mean RMSE value:', np.round(rmse, 4))\n    print('Optimal parameters:', grid_model.best_params_)\n    \n    return grid_model.best_estimator_","f9ef92ff":"lgbm_params = {'n_estimators':[3300]}\n\nlgbm_reg = LGBMRegressor(n_estimators=3300, bagging_fraction=0.7, learning_rate=0.1,\n                         max_depth=4, subsample=0.7, feature_fraction=0.9, boosting_type='gbdt',\n                         colsample_bytree=0.5, reg_lambda=5, n_jobs=-1)\n\nbest_lgbm = get_best_params(lgbm_reg, lgbm_params)","ff13986b":"best_lgbm","0c082b4d":"# lgbm \nlgbm_model = LGBMRegressor(bagging_fraction=0.7, colsample_bytree=0.5, feature_fraction=0.9,\n              max_depth=4, n_estimators=3300, reg_lambda=5, subsample=0.7, random_state=0)","71fc0bfc":"# model training\nlgbm_model.fit(X_train, y_train)","3ef4902a":"# RMSE function\ndef calculate_rmse(actual, prediction):\n    sum = 0\n    for a, p in zip(actual, prediction):\n        sum += (a - p)**2\n    mse = sum\/len(actual)\n    rmse = np.sqrt(mse)\n    return rmse","622e03c1":"# x_valid data predict\nvalid_prediction = lgbm_model.predict(X_valid)","10ed2082":"# valid data rmse\ncalculate_rmse(y_valid, valid_prediction)","d6551c62":"# test data predict\nlgbm_prediction = lgbm_model.predict(df_test)","977787cf":"submission_lgbm = pd.DataFrame({'Id':test['Id'], 'SalePrice':lgbm_prediction})\nsubmission_lgbm.head()","5e9557ff":"explainer = shap.TreeExplainer(lgbm_model) # Tree model Shap Value \nshap_values = explainer.shap_values(X_valid) # Shap Values ","afaeab7c":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[1,:], X_valid.iloc[1,:])","837a5b4e":"shap.force_plot(explainer.expected_value, shap_values, X_valid)","5e772879":"shap.summary_plot(shap_values, X_valid)","f5f7944b":"shap.summary_plot(shap_values, X_valid, plot_type = \"bar\")","d5e880b4":"# data segmentation\nX = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","8861dfc0":"from sklearn import ensemble\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\nmseOos = []\nnTreeList = range(50, 500, 10)\n\nfor iTrees in nTreeList:\n    depth = None\n    maxFeat = 4 #try diff value\n    wineRFModel = ensemble.RandomForestRegressor(n_estimators=iTrees,\n                    max_depth=depth, max_features=maxFeat,\n                    oob_score=False, random_state=531)\n    wineRFModel.fit(X_train, y_train)\n    #\ub370\uc774\ud130 \uc138\ud2b8\uc5d0 \ub300\ud55c MSE \ub204\uc801\n    prediction = wineRFModel.predict(X_train)\n    mseOos.append(mean_squared_error(y_train, prediction))\nprint(\"MSE\")\nprint(mseOos)","134e3ccc":"regr = RandomForestRegressor(max_depth=1000, random_state=1000,\n                          n_estimators=150)\nregr.fit(X_train, y_train)","032e085b":"# RMSE function\ndef calculate_rmse(actual, prediction):\n    sum = 0\n    for a, p in zip(actual, prediction):\n        sum += (a - p)**2\n    mse = sum\/len(actual)\n    rmse = np.sqrt(mse)\n    return rmse","9dd9ebf9":"valid_prediction = regr.predict(X_valid)","29c7e659":"calculate_rmse(y_valid, valid_prediction)","5eb3c83d":"RFR_prediction = regr.predict(df_test)","692e8ddc":"submission_RFR = pd.DataFrame({'Id':test['Id'], 'SalePrice':RFR_prediction})\nsubmission_RFR.head()","e8a4dec6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score # \uc815\ud655\ub3c4 \ud568\uc218\n\n# sample 1000, tree depth - 1000(max)\nRF = RandomForestClassifier(n_estimators=1000, max_depth=1000,random_state=0)\nRF.fit(X_train,y_train)","31fa6463":"# RMSE function\ndef calculate_rmse(actual, prediction):\n    sum = 0\n    for a, p in zip(actual, prediction):\n        sum += (a - p)**2\n    mse = sum\/len(actual)\n    rmse = np.sqrt(mse)\n    return rmse","5db6a855":"valid_prediction = RF.predict(X_valid)","87c1f3ea":"calculate_rmse(y_valid, valid_prediction)","1d48eb13":"RF_prediction = RF.predict(df_test)","6eb9f970":"submission_RF = pd.DataFrame({'Id':test['Id'], 'SalePrice':RF_prediction})\nsubmission_RF.head()","790232a6":"# data segmentation\nX = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","556ec8f4":"import xgboost\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score\n\nxgb_model = xgboost.XGBRegressor(n_estimators=1000, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=0.9,colsample_bylevel=0.8, max_depth=20)\n\nxgb_model.fit(X_train,y_train)","45b3c0b9":"xgboost.plot_importance(xgb_model)","35a63c8a":"valid_prediction = xgb_model.predict(X_valid)","53422ea9":"calculate_rmse(y_valid, valid_prediction)","f6f152ee":"xgb_model_prediction = xgb_model.predict(df_test)","a66aaa84":"# submission\nsubmission_XGBoost = pd.DataFrame({'Id':test['Id'], 'SalePrice':xgb_model_prediction})\nsubmission_XGBoost.head()","55bc49a4":"# data segmentation\nX = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","cf298949":"from catboost import CatBoostRegressor\nfrom catboost import Pool","af61566d":"catboost_model = CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    X_train, y_train,\n    eval_set=(X_valid, y_valid)\n)","101ab33b":"print('Model params:', catboost_model.get_params())","fc9380b9":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, catboost_model.get_feature_importance(Pool(X_train, label=y_train)))), columns=['Feature','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","d1a108bd":"valid_prediction = catboost_model.predict(X_valid)\ncalculate_rmse(y_valid, valid_prediction)","554141e6":"catboost_model_prediction = catboost_model.predict(df_test)","73667956":"# submission\nsubmission_catboost = pd.DataFrame({'Id':test['Id'], 'SalePrice':catboost_model_prediction})\nsubmission_catboost.head()","a0162c59":"# Dataset that will be the train set of the ensemble model.\n# \nfirst_level = pd.DataFrame(catboost_model.predict(X_train), columns=['catboost'])\nfirst_level['xgbm'] = xgb_model.predict(X_train)\n#first_level['random_forest'] = RF.predict(X_train)    \n#first_level['RF_regression'] = regr.predict(X_train)\nfirst_level['lgbm'] = lgbm_model.predict(X_train)\nfirst_level['label'] = y_train.values\nfirst_level.head(20)","b1f2d32a":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(catboost_model.predict(df_test), columns=['catboost'])\nfirst_level_test['xgbm'] = xgb_model.predict(df_test)\n#first_level_test['random_forest'] = RF.predict(df_test)\n#first_level_test['RF_regression'] = regr.predict(df_test)\nfirst_level_test['lgbm'] = lgbm_model.predict(df_test)\nfirst_level_test.head()","356df55b":"RF_meta_model = RandomForestClassifier(n_estimators=1000, max_depth=1000,random_state=0)","4234693c":"# Drop label from dataset.\nfirst_level.drop('label', axis=1, inplace=True)\nRF_meta_model.fit(first_level, y_train)","03f064ea":"ensemble_pred = RF_meta_model.predict(first_level)\nfinal_predictions = RF_meta_model.predict(first_level_test)","f4cfcb76":"submission_meta_model = pd.DataFrame({'Id':test['Id'], 'SalePrice':final_predictions})\nsubmission_meta_model.head()","f01401b0":"submission_meta_model.to_csv('.\/submission.csv', index=False)","6f33e7ab":"<h4>Remove features with more than half null values, exclude meaningless columns such as ID","e0d1bc8c":"<div style=\"background-color:rgba(0, 255, 255, 0.6);border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">import<\/center><\/h1>\n<\/div>","dd3f5c3e":"**RFregression**\n1. rmse : 32369.00146910789","59413e64":"**meta_model**","2bba50e7":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>RFregression Modeling<\/center><\/h1>\n<\/div>","52c87ea2":"# Submission","92b5dda8":"<h3> I tried various things, but the best performance was achieved by stacking two XGboost and LightGMB.","b9063811":"<h3>Check for null values","ab2ac2d9":"<div style=\"background-color:rgba(255,255, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Model Stacking<\/center><\/h1>\n<\/div>","c16c0a55":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>lightGBM Modeling<\/center><\/h1>\n<\/div>","4f694c3d":"<div style=\"background-color:rgba(0, 0, 255, 0.6);border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Smoothing<\/center><\/h1>\n<\/div>","6f0ddf6c":"<h3>Find the optimal parameters","f4153051":"![\uc2a4\ud0dc\ud0b9.PNG](attachment:f3db081a-8b44-41cf-ba5c-0293d436dc37.PNG)","92017649":"<h4>Null value averaging","a5650823":"**XGBoost**\n1. rmse : 26497.32500764802","10948f72":"<h3>Extract only object type","6c62e1bd":"(smoothing)**reference**  : https:\/\/www.kaggle.com\/dustinthewind\/making-sense-of-mean-encoding<br><br>\n(stacking)**reference**  : https:\/\/www.kaggle.com\/dimitreoliveira\/model-stacking-feature-engineering-and-eda#notebook-container","2fb0f59c":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>CatBoost Modeling<\/center><\/h1>\n<\/div>","0e680182":"<div style=\"background-color:rgba(0, 255, 255, 0.6);border-radius:5px;display:fill;\">\n    <h1><center style =\"margin-left : 20px;\">Plan<\/center><\/h1>\n<\/div>","e3e5ad04":"<h3>Features Importance","113e6056":"<div style=\"background-color:rgba(0,255, 2, 0.5);border-radius:5px;display:fill\">\n    <h1><center>Log<\/center><\/h1>\n<\/div>","e7be7151":"**catBoost**\n\n  1. rmse : 31123.49221255535","e69677f4":"<h3>feture importance<\/h3>\n(just see the code)","3b4ab062":"**After pre-processing and data analysis, I will create and stack several models to create an optimal model.**","b25b87f1":"<h3>Apply smoothing","24f5fe6c":"<h3>Features Variable Importance","258cc312":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>RFclassifier Modeling<\/center><\/h1>\n<\/div>","cc6d2b33":"**RF**\n1. rmse :  32599.806800051516\n\n","49765f57":"**LGBM**\n1. submission : 0.13561       \n2. submission : 0.13519\n3. submission : 0.13485\n4. submission : 0.13337 rmse : 27881.5164532458\n","e96c0e18":"<div style=\"background-color:rgba(255, 69, 0, 0.5);border-radius:5px;display:fill\">\n    <h1><center>XGBoost Modeling<\/center><\/h1>\n<\/div>"}}