{"cell_type":{"98db8519":"code","4ea9c2fc":"code","309971f4":"code","6d904c99":"code","c610c850":"code","7c1c145c":"code","879f4431":"code","642fafce":"code","f35aa015":"code","ced1738e":"code","f8bcdf6f":"code","2a4b10d6":"code","1391d651":"code","1a27493c":"code","6d657c58":"code","2a19ea23":"code","ae2dd226":"code","89195647":"code","90c6e37c":"code","040ae02f":"code","63ffe30c":"code","82c51e55":"code","7b035795":"code","408dbbac":"code","e9a10b10":"code","11635e2c":"code","21cbfe6a":"code","105d932a":"code","778163e3":"code","42382598":"code","a35dd344":"code","7faac728":"code","49c8182c":"code","4db6b3b6":"code","e7fc7167":"code","038c5411":"code","010fa302":"markdown","5803c542":"markdown","55eb9533":"markdown","12602565":"markdown","36e77dda":"markdown","c960d29e":"markdown","6b5f6cde":"markdown","8cc02c28":"markdown","5411d94b":"markdown","0bc5699a":"markdown","ce43ac70":"markdown","541d9fc9":"markdown","ac31e4ea":"markdown","577846cf":"markdown","2fb67776":"markdown","73f698a1":"markdown","24453869":"markdown","56f52b27":"markdown","b1b79a20":"markdown","45dba016":"markdown","e4dfaf18":"markdown","d63d717a":"markdown","3575825d":"markdown","7723d1ae":"markdown","3b4316d4":"markdown","26e494b8":"markdown","ca4e7ead":"markdown","7c21b6fd":"markdown"},"source":{"98db8519":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ea9c2fc":"df=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","309971f4":"df.head()","6d904c99":"df.info()","c610c850":"df.isnull().sum()","7c1c145c":"df.drop(['id',df.columns[-1]],axis=1,inplace=True)","879f4431":"df.isnull().sum()","642fafce":"df['diagnosis']=df['diagnosis'].astype('category').cat.as_ordered()","f35aa015":"df.corrwith(df['diagnosis'].cat.codes)","ced1738e":"df.info()","f8bcdf6f":"Y=df['diagnosis'].cat.codes","2a4b10d6":"X=df[df.columns[1:-1]]","1391d651":"X=(X-X.mean())\/X.std()","1a27493c":"X","6d657c58":"X.shape","2a19ea23":"import torch as T","ae2dd226":"X=T.from_numpy(X.to_numpy())","89195647":"X=X.float()","90c6e37c":"Y=T.from_numpy(Y.to_numpy().copy())","040ae02f":"import torch.nn as nn\nclass Network(nn.Module):\n    def __init__(self,i_dim,o_dim):\n        super(Network,self).__init__()\n        self.layer1=nn.Sequential(\n                    nn.Linear(i_dim,128),\n                    nn.ReLU(),\n                    nn.Linear(128,64),\n                    nn.Tanh()\n        \n        \n        )\n        self.layer2=nn.Sequential(\n                    nn.Linear(64,16),\n                    nn.ReLU()\n                    \n        )\n        self.layer3=nn.Linear(16,o_dim)\n        self.output=nn.LogSoftmax()\n        \n    def forward(self,X):\n        X=self.layer1(X)\n        X=self.layer2(X)\n        X=self.layer3(X)\n        return self.output(X)","63ffe30c":"net=Network(X.shape[1],2)","82c51e55":"print(net)","7b035795":"creterion1=nn.CrossEntropyLoss()\noptim=T.optim.Adam(net.parameters())","408dbbac":"!pip install torchviz\n\nfrom torchviz import make_dot","e9a10b10":"make_dot(net(X))","11635e2c":"from sklearn.model_selection import train_test_split as ttt\nX_train,X_test,Y_train,Y_test=ttt(X,Y,test_size=0.1)","21cbfe6a":"Y_train","105d932a":"losses=[]\nfor e in range(1000):\n    optim.zero_grad()\n    y_hat=net.forward(X_train.float())\n    loss=creterion1(y_hat,Y_train.long())\n    loss.backward()\n    optim.step()\n    if e%10==0:\n        losses.append(loss.item())\n        print('loss at epoch {} is {}'.format(e,loss.item()))","778163e3":"import matplotlib.pyplot as plt\nplt.scatter(range(len(losses)),losses)","42382598":"def predict(model,X):\n    y_hat=model.forward(X)\n    y_hat=T.exp(y_hat)\n    return y_hat.max(axis=1)","a35dd344":"y_hat=predict(net,X_test)\n","7faac728":"y_hat","49c8182c":"acc=sum(y_hat[1]==Y_test).float()\/len(y_hat[1])","4db6b3b6":"acc","e7fc7167":"from sklearn.metrics import confusion_matrix as cm","038c5411":"cm(y_hat[1].numpy(),Y_test.numpy())","010fa302":"split the dataset for test and train","5803c542":"Actually we can also visualize this process","55eb9533":"Lets create Our Model","12602565":"perform training for 1000 epochs","36e77dda":"Lets have a look at Data","c960d29e":"Lets create a instance of model","6b5f6cde":"we pass paramaters of our network to adam. This can be understood as creating a link so that when we call our model and graph is formed for backprop , our optimizer will automatically upgrade the parameters for our network","8cc02c28":"So we dont need id column and it appears there is a column which is all Nan. we will remove them by using df.drop","5411d94b":"so how our loss actually looks","0bc5699a":"Lets have a final look at df to make sure we have correct Dtypes","ce43ac70":"Lets check what is correaltion between 'diagonisis' column and other columns of dataframe","541d9fc9":"Here we can also drop columns with very low correlation","ac31e4ea":"pandas Series can be converted into tensor by first converting into numpy and then using torch function to_numpy","577846cf":"Lets calculate accuracy of our model","2fb67776":"But we are still working with pandas module.We would need to convert our input into torch tensors.Now lets do that","73f698a1":"So now our data is clean. So we are going to build a classifier. lets convert diagonsis to category class. Category class provides us a handy function to convert data into category classes.","24453869":"Final look at X","56f52b27":"This Notebook is Just a very simple implementation of Neural Network to classify Cancer Cells using pytorch\n\nLets first load basic libraries. We will use pandas, torch modules only.","b1b79a20":"Not copy diagonisis column to a variable Y. we are coping them as classes","45dba016":"So how many does we misclassified","e4dfaf18":"lets normalize the X. we will use builtin pandas functions.","d63d717a":"Note: we use .copy() as series.to_numpy returns a view hence we need a copy so we dont mess up original data in some way","3575825d":"Now to decide loss function and what optimizer to use\n\nWe can use Binary classifier or CrossEntropyLoss. We will use CrossEntropyLoss as we are treating Y as class types.\nfor optimizer we will use adams optimizer","7723d1ae":"for prediction purposes","3b4316d4":"So this is our flow of data. As pytorch is dynamic, There is no pre determined graph . but we can use backward graph created during each operation(due to backprop). Above graph is that\n                    ","26e494b8":"Model is simple:\nSequential Layer in pytorch just means applying each operation or layer in sequence. it also helps in writing forward function\nfirst hidden layer is Linear Layer i.e. output is calculated as Y=X*theta+bias where theta is weights. followed by a ReLu activation with 128 nodes\n2nd layer is a linear layer followed by tanh activation. with 64 nodes\n3rd layer is Linear layer followed by Relu layer with 16 nodes\nand output layer is Linear layer with Log softmax activation with 2 nodes.This is also our no. of classes \n\n\n","ca4e7ead":"We  also need to convert X to float (it is already in float but to be sure) otherwise it will show error that mat3 need to be in float","7c21b6fd":"Copy input parameters to variable X "}}