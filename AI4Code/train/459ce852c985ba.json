{"cell_type":{"a8522eb6":"code","78c1265b":"code","a1461ee1":"code","1edac729":"code","3b30c69c":"code","a88b3d26":"code","03f7fd37":"code","b6f9f14c":"code","ba6cf622":"code","9d6913a2":"code","619f8760":"code","3d6b4c12":"code","732f06f0":"code","d4ab97ba":"code","a1952da6":"code","3ffa34e4":"code","07775991":"code","2918b537":"code","13783725":"code","3f54e197":"code","85020112":"code","81793774":"code","d3082642":"code","19902714":"markdown","331b3aaa":"markdown","386742b9":"markdown","3013a68e":"markdown","dc0ce67b":"markdown","a25c9444":"markdown","5fc3e347":"markdown","eb813eb5":"markdown","17b060f3":"markdown","3c86c7fc":"markdown","ff2a1ce3":"markdown","f5fc8ef1":"markdown","270b791b":"markdown","b4537be1":"markdown","8b2855f8":"markdown","6739e984":"markdown","5b09450e":"markdown","6c4f9a74":"markdown","1a7eeb97":"markdown","8bfd4419":"markdown"},"source":{"a8522eb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78c1265b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix   \nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n# Disable warnings \nimport warnings\nwarnings.filterwarnings('ignore')","a1461ee1":"train = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/test.csv')","1edac729":"#create a function which shows the information about the dataset\ndef dataoveriew(df, message):\n    print(f'{message}:\\n')\n    print(\"Rows:\", df.shape[0])\n    print(\"\\nNumber of features:\", df.shape[1])\n    print(\"\\nFeatures:\")\n    print(df.columns.tolist())\n    print(\"\\nMissing values:\", df.isnull().sum().values.sum())\n    print(\"\\nUnique values:\")\n    print(df.nunique())\n    print(\"\\nDatatypes of the variables:\")\n    print(df.dtypes)\ndataoveriew(train, 'Overiew of the training dataset')","3b30c69c":"#Test dataset Information\ndataoveriew(test, 'Overiew of the testing dataset')","a88b3d26":"import plotly.offline as py # visualization\npy.init_notebook_mode(connected=True) \nimport plotly.graph_objs as go # visualization\ntrace = go.Pie(labels = train[\"churn\"].value_counts().keys().tolist(),\n               values = train[\"churn\"].value_counts().values.tolist(),\n               marker = dict(colors = ['LightSkyBlue','MediumPurple'],\n                             line = dict(color = \"white\", width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Customer churn in training data\",\n                        plot_bgcolor = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)\n\n","03f7fd37":"# Copied code from seaborn examples\n# https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(train.corr(), mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show();","b6f9f14c":"train.describe().T","ba6cf622":"# Transformation of training data\n#Normalize Numeric Columns for training  data\nnumeric = train[['account_length','number_vmail_messages', 'total_day_minutes',\n       'total_day_calls', 'total_day_charge', 'total_eve_minutes',\n       'total_eve_calls', 'total_eve_charge', 'total_night_minutes',\n       'total_night_calls', 'total_night_charge', 'total_intl_minutes',\n       'total_intl_calls', 'total_intl_charge',\n       'number_customer_service_calls']]\nfrom sklearn.preprocessing import MinMaxScaler\n# fit scaler on training data\nnorm = MinMaxScaler().fit(numeric)\n# transform training data\nnormal_numeric_data = norm.transform(numeric)\ncols = numeric.columns.values\n# normalized data to dataframe\nnormal_numeric_data = pd.DataFrame(data = normal_numeric_data, columns = cols)\nnormal_numeric_data.head(5)\n#Standardizing Categorical Columns of training data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nCategorical = train[['international_plan','voice_mail_plan','churn']]\nCategorical = Categorical.apply(LabelEncoder().fit_transform)\nCategorical.head(5)\n#Create Dummy Variables separately for state and area code\nstate_dum = pd.get_dummies(train['state'])\narea_dum = pd.get_dummies(train['area_code'])\n#Concatenating  numeriacal and categorical dataframe\nFinal_train =  pd.concat([state_dum,area_dum,normal_numeric_data, Categorical], axis=1)\nFinal_train.head()","9d6913a2":"#Normalize Numeric Columns for testing  data\nnumeric_t = test[['account_length','number_vmail_messages', 'total_day_minutes',\n       'total_day_calls', 'total_day_charge', 'total_eve_minutes',\n       'total_eve_calls', 'total_eve_charge', 'total_night_minutes',\n       'total_night_calls', 'total_night_charge', 'total_intl_minutes',\n       'total_intl_calls', 'total_intl_charge',\n       'number_customer_service_calls']]\nfrom sklearn.preprocessing import MinMaxScaler\n# fit scaler on testing data\nnorm_t = MinMaxScaler().fit(numeric_t)\n# transform testing data\nnormal_numeric_data_t = norm_t.transform(numeric_t)\ncols_t = numeric_t.columns.values\n# normalized data to dataframe\nnormal_numeric_data_t = pd.DataFrame(data = normal_numeric_data_t, columns = cols_t)\nnormal_numeric_data_t.head(5)\n#Standardizing Categorical Columns of testing data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\nCategorical_t = test[['international_plan','voice_mail_plan']]\nCategorical_t = Categorical_t.apply(LabelEncoder().fit_transform)\nCategorical_t.head(5)\n# Create dummy variables separately for state and area code\nstate_dumt = pd.get_dummies(test['state'])\narea_dumt = pd.get_dummies(test['area_code'])\n#Concatenating  numeriacal and categorical dataframe\nFinal_test =  pd.concat([state_dumt,area_dumt,normal_numeric_data_t, Categorical_t], axis=1)\nFinal_test.head()","619f8760":"# features X and the target y variable \nX=Final_train.drop('churn',axis=1).copy()\ny=Final_train['churn'].copy()","3d6b4c12":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 13,stratify=y)","732f06f0":"from imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state=0)\nX_train_Smote, y_train_Smote = smt.fit_sample(X_train, y_train)\nX_train.shape, y_train.shape\n#Shape of X_train & y_train","d4ab97ba":"#oversampled for minority class\nX_train_Smote.shape, y_train_Smote.shape\n#Shape of X_train of SMOTE and y_train of SMOTE","a1952da6":"from catboost import CatBoostClassifier\nCat_model1 = CatBoostClassifier()\n\nCat_model1.fit(\n    X_train, y_train,\n    #cat_features=Categorical,\n    eval_set=(X_test, y_test),\n    verbose=False,\n    plot=True\n)\npreds=Cat_model1.predict(X_test)\n#classification Report \nCat_clf = classification_report(y_test, preds)\nprint(Cat_clf)\nCatboost_Accuracy = (preds == y_test).sum().astype(float) \/ len(preds)*100\nprint('Catboost accuracy score is:',Catboost_Accuracy)\n# Create the Confusion matrix\nplot_confusion_matrix(Cat_model1, \n                      X_test, \n                      y_test,\n                      display_labels=['Not_Churn', 'Churn']);","3ffa34e4":"from catboost import CatBoostClassifier\nCat_Smote = CatBoostClassifier(iterations=1000,\n    random_seed=40)\n\nCat_Smote.fit(\n    X_train_Smote, y_train_Smote,\n    eval_set=(X_test, y_test),\n    verbose=False,\n    plot=True\n)\npreds=Cat_Smote.predict(X_test)\n# classification Report\nCat_clf_S = classification_report(y_test, preds)\nprint(Cat_clf_S)\nCat_Smote_Accuracy = (preds == y_test).sum().astype(float) \/ len(preds)*100\nprint('Catboost accuracy score with Smote:',Cat_Smote_Accuracy)\n# Create the Confusion matrix\nplot_confusion_matrix(Cat_Smote, \n                      X_test, \n                      y_test,\n                      display_labels=['Not_Churn', 'Churn']);","07775991":"import xgboost as xgb\nfrom xgboost import XGBClassifier\ngbm = xgb.XGBClassifier(random_seed=40)\nclf_xgb = xgb.XGBClassifier()\nclf_xgb.fit(X_train, \n            y_train, \n            verbose=True, \n            early_stopping_rounds=10,\n            eval_metric='aucpr',\n            eval_set=[(X_test, y_test)])\npreds = clf_xgb.predict(X_test)\nxgb_CR = classification_report(y_test, preds)\nprint(xgb_CR)\nxgb_accuracy = (preds == y_test).sum().astype(float) \/ len(preds)*100\nprint(\"XGBoost's prediction accuracy without SMOTE: %3.2f\" % (xgb_accuracy))\nplot_confusion_matrix(clf_xgb, \n                      X_test, \n                      y_test,\n                      display_labels=[\"Not Churn\", \"Churn\"])","2918b537":"xgb_Smote = xgb.XGBClassifier(random_seed=40)\nxgb_Smote.fit(X_train_Smote, \n            y_train_Smote, \n            verbose=True, \n            early_stopping_rounds=10,\n            eval_metric='aucpr',\n            eval_set=[(X_test, y_test)])\npreds = xgb_Smote.predict(X_test)\n\nxgbsm_CR = classification_report(y_test, preds)\nprint(xgbsm_CR)\n\nxgb_accuracy_Smote = (preds == y_test).sum().astype(float) \/ len(preds)*100\n\nprint(\"XGBoost's prediction accuracy with SMOTE:\", (xgb_accuracy_Smote))\nplot_confusion_matrix(xgb_Smote, \n                      X_test, \n                      y_test,\n                      display_labels=[\"Not Churn\", \"Churn\"])","13783725":"import plotly.graph_objects as go\nModel=['Catboost', 'Catboost_Smote', 'XGBoost', 'XGBoost_Smote']\nAcc=[Catboost_Accuracy,Cat_Smote_Accuracy,xgb_accuracy,xgb_accuracy_Smote]\nfig = go.Figure([go.Bar(x=Model, y=Acc)])\nfig.update_layout(title_text='Accuracy scores of the Models')\nfig.show()","3f54e197":"CBC = CatBoostClassifier()\nparameters = {'depth'         : [2,3,4],\n              'learning_rate' : [0.03,0.05,0.1],\n              'iterations'    : [300,400,600]}\nfrom sklearn.model_selection import GridSearchCV\nGrid_CBC = GridSearchCV(estimator=CBC, param_grid = parameters, cv = 3, n_jobs=-1)\nGrid_CBC.fit(X_train, y_train)\nprint(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\",Grid_CBC.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\",Grid_CBC.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\",Grid_CBC.best_params_)","85020112":"from catboost import CatBoostClassifier\nCBC_Gridcv = CatBoostClassifier(\n    iterations=300,\n    depth=4,random_seed=40,\n    learning_rate=0.1,\n    verbose=10,\n    custom_loss=['AUC', 'Accuracy']\n    \n)\nCBC_Gridcv.fit(\n    X_train, y_train,\n    eval_set=(X_test, y_test),\n    verbose=False,\n    plot=True\n)\nprint('Model is fitted: ' + str(CBC_Gridcv.is_fitted()))\nprint('Model params:')\nprint(CBC_Gridcv.get_params())\n#Predicsions after hyperparameter tunning\npreds2=CBC_Gridcv.predict(X_test)\n \n\nCBC_Gridcv_clf = classification_report(y_test, preds)\nprint(CBC_Gridcv_clf)\nplot_confusion_matrix(CBC_Gridcv, \n                      X_test, \n                      y_test,\n                      display_labels=['Not_Churn', 'Churn']);\nCat_Accuracy_grid = (preds2 == y_test).sum().astype(float) \/ len(preds2)*100\n\nprint('Catboost accuracy score after hyperparameter tunning:', (Cat_Accuracy_grid))","81793774":"predictor_cols = X_test.columns\n# Use the model to make the  prediction\nprediction = CBC_Gridcv.predict(Final_test[predictor_cols])\nprint(prediction)","d3082642":"my_submission = pd.DataFrame({'id': test.id, 'churn': prediction})\n\n#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Submission_churn.csv'\n\nmy_submission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","19902714":"3.Model building with XGboost","331b3aaa":"8.Comparision of Models Performance","386742b9":"#### 1. Import libraries","3013a68e":"#### Conclusion:\nThere is a slight increment in performance score after hyperparameter tuning giving the score of 96.80,which means 96.80% of customers were accurately identified as churners or no-churners by this model.","dc0ce67b":"2.Catboost- SMOTE in trainning data and tested without SMOTE","a25c9444":"#### 5.Splitting the dataset into the Training set and Testing set","5fc3e347":"Transformation of testing data","eb813eb5":"Correlation between the varaiables","17b060f3":"9.Catboost with actual data & hyperparameter tunning using GridSearchCV","3c86c7fc":"1.Catboost","ff2a1ce3":"#### 2. Data Overview - Traing Data & Testing Data","f5fc8ef1":"#### 7.Model Building","270b791b":"#### 6.SMOTE Oversampling teqnique for imbalance dataset","b4537be1":"4.XGboost - SMOTE only in trainning data","8b2855f8":"#### The redboxes indicates positive correlation coefficient of that particular variables which means increase in the first variable would correspond to an increase in the second variable, thus implying a direct relationship between the variables. The light  grey boxes with negative sign indicates negative correlation coefficient of that particular variables which means inverse relationship whereas one variable increases, the second variable decreases.","6739e984":"### 3. EDA","5b09450e":"#### 14.1% of customer churn in training data","6c4f9a74":"#### 4.Transformation of Training & Testing Data","1a7eeb97":"#### Description of the training data","8bfd4419":"There is no much score differences between these 4 models but when comparing of all Catboost without SMOTE is giving the highest performance score of 96.70.This means 96.70% of customers were accurately identified as churners or no-churners."}}