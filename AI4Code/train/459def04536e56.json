{"cell_type":{"3b4a007d":"code","7f851c6e":"code","a3ab834e":"code","3d1ee0a6":"code","bd6a8ade":"code","6bbb7767":"code","2a1fc1c8":"code","87f75b88":"code","2c4ebaac":"code","d875ea2a":"code","26b08469":"code","51731a72":"code","0903928a":"code","b452de83":"code","2531e270":"code","2959ff36":"code","5ab65b63":"code","40ec8083":"code","c78a1c39":"code","72e648fb":"code","8f1e377d":"code","79a1b89e":"code","f59b63fa":"code","3751b6b0":"code","c5ac5f6b":"code","47a710a7":"code","c5be294b":"code","7d1773f4":"code","e3c4afb0":"code","3910c870":"code","712235e3":"code","af57429a":"code","f64c0da7":"code","5dcb23a8":"code","91bd6cdc":"code","cdd125aa":"code","6e940c76":"code","201a0f4d":"code","0f4ffb68":"code","33a2583e":"code","0db33461":"code","d3b50ef3":"code","2bb94004":"code","97207c52":"code","97811070":"code","0a637164":"code","9b2c72ca":"code","d91933f7":"code","7ea54f40":"code","93a43f27":"code","62c07d36":"code","49c7b87e":"markdown","ddca0526":"markdown","abc4fbf9":"markdown","6754b50c":"markdown","902a7747":"markdown","37df2b1f":"markdown","38ac8539":"markdown","b188be51":"markdown","9575e672":"markdown","b6dd1bbf":"markdown","a778be98":"markdown","a956ff03":"markdown","270f2be3":"markdown","eae9dce2":"markdown","5ad70060":"markdown","82d22d44":"markdown","4eab7e84":"markdown","4296d206":"markdown","041754b2":"markdown","0cb1663a":"markdown","2e6d4618":"markdown","4ab9bea5":"markdown"},"source":{"3b4a007d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#sklearn\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn import metrics, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')","7f851c6e":"df = pd.read_csv('..\/input\/ssd-reviews\/ssd_reviews.csv', index_col=0)","a3ab834e":"df.head()","3d1ee0a6":"df.info()","bd6a8ade":"fig, ax = plt.subplots(figsize = (14, 10))\nsns.heatmap(df.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='viridis')\nax.set_title('Customers Reviews')\nplt.show()","6bbb7767":"df['date'] = pd.to_datetime(df['date'])","2a1fc1c8":"df['rating_stars'].value_counts()","87f75b88":"df_cons = df[['cons']].dropna()\ndf_cons['positive'] = 0","2c4ebaac":"# a lot of values droped around 700 row in cons\ndf_cons.drop(df_cons[df_cons['cons'].isin(['none', 'none so far', 'non'])].index, inplace=True)","d875ea2a":"df_cons.rename(columns={'cons':'pros_and_cons'}, inplace=True)","26b08469":"#To make our data balanced betwwen'pros' and 'cons', Here in 'pros' we took only 1562 row because a lot of rows droped in 'cons'\ndf_pros = df[['pros']][:1562].dropna()","51731a72":"df_pros.rename(columns={'pros':'pros_and_cons'}, inplace=True)","0903928a":"df_pros['positive'] = 1","b452de83":"merged_df = pd.merge(left=df_pros, right=df_cons, left_on=['pros_and_cons', 'positive'],\n                     right_on=['pros_and_cons', 'positive'], how='outer')","2531e270":"merged_df","2959ff36":"merged_df['positive'].value_counts()","5ab65b63":"X = merged_df.drop('positive', axis=1)\ny = merged_df['positive']","40ec8083":"bow_f = CountVectorizer(stop_words='english').fit(X['pros_and_cons'])","c78a1c39":"print(\"After eliminating stop words: \", len(bow_f.get_feature_names()))","72e648fb":"bow_transform = bow_f.transform(X['pros_and_cons'])","8f1e377d":"count_vect_df = pd.DataFrame(bow_transform.todense(), columns=bow_f.get_feature_names())\nnp.sum(count_vect_df).sort_values(ascending=False)[0:20]","79a1b89e":"X_train, X_test, y_train, y_test = train_test_split(bow_transform, y, test_size=0.3, random_state=42)","f59b63fa":"#8\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('train score' , logreg.score(X_train, y_train))\nprint('test score' , logreg.score(X_test, y_test))\ny_pred = logreg.predict(X_test)","3751b6b0":"confusion_matrix(y_test, y_pred)","c5ac5f6b":"tfidf_vectoriser = TfidfVectorizer(stop_words='english')\ntfidf_f = tfidf_vectoriser.fit(X['pros_and_cons'])\ntfidf_transform = tfidf_f.transform(X['pros_and_cons'])","47a710a7":"tf_X_train, tf_X_test, y_train, y_test = train_test_split(tfidf_transform, y, test_size=0.3)","c5be294b":"tf_logreg = LogisticRegression(C=3.2)\ntf_logreg.fit(tf_X_train, y_train)\nprint('train score' , tf_logreg.score(tf_X_train, y_train))\nprint('test score' , tf_logreg.score(tf_X_test, y_test))\n\ny_pred = tf_logreg.predict(tf_X_test)","7d1773f4":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(tf_logreg, tfidf_transform, y, cv=cv).mean()","e3c4afb0":"confusion_matrix(y_test, y_pred)","3910c870":"knn_classifier = KNeighborsClassifier()  \nknn_classifier.fit(tf_X_train, y_train)\nprint(knn_classifier.score(tf_X_train, y_train))\nprint (knn_classifier.score(tf_X_test, y_test))\ny_pred = knn_classifier.predict(tf_X_test)","712235e3":"confusion_matrix(y_test, y_pred)","af57429a":"tree= DecisionTreeClassifier()\ntree.fit(tf_X_train, y_train)\nprint('test score' , tree.score(tf_X_train, y_train))\nprint('test score' , tree.score(tf_X_test, y_test))\ny_pred = tree.predict(tf_X_test)","f64c0da7":"confusion_matrix(y_test, y_pred)","5dcb23a8":"svm_linear = svm.SVC(kernel='linear')\nsvm_linear.fit(tf_X_train, y_train)\nprint('Train : ', svm_linear.score(tf_X_train, y_train))\nprint('Test: ', svm_linear.score(tf_X_test, y_test))\ny_pred = svm_linear.predict(tf_X_test)","91bd6cdc":"confusion_matrix(y_test, y_pred)","cdd125aa":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_linear, tfidf_transform, y, cv=cv).mean()","6e940c76":"svm_rbf = svm.SVC(kernel='rbf',C=1, probability=True)\nsvm_rbf.fit(tf_X_train, y_train)\nprint('Train : ', svm_rbf.score(tf_X_train, y_train))\nprint('Test: ', svm_rbf.score(tf_X_test, y_test))\ny_pred = svm_rbf.predict(tf_X_test)","201a0f4d":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_rbf, tfidf_transform, y, cv=cv).mean()","0f4ffb68":"confusion_matrix(y_test, y_pred)","33a2583e":"svm_poly = svm.SVC(kernel='poly', C=.7)\nsvm_poly.fit(tf_X_train, y_train)\nprint('Train : ', svm_poly.score(tf_X_train, y_train))\nprint('Test: ', svm_poly.score(tf_X_test, y_test))\ny_pred = svm_poly.predict(tf_X_test)","0db33461":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(svm_poly, tfidf_transform, y, cv=cv).mean()","d3b50ef3":"confusion_matrix(y_test, y_pred)","2bb94004":"randomF = RandomForestClassifier()\nrandomF.fit(tf_X_train, y_train)\nprint('Train score :',randomF.score(tf_X_train, y_train))\nprint('Ttest score :',randomF.score(tf_X_test, y_test))\ny_pred = randomF.predict(tf_X_test)","97207c52":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(randomF, tfidf_transform, y, cv=cv).mean()","97811070":"confusion_matrix(y_test, y_pred)","0a637164":"gnb = GaussianNB(var_smoothing=0.11) \ngnb.fit(tf_X_train.toarray(), y_train) \nprint('Train score :',gnb.score(tf_X_train.toarray(), y_train))\nprint('Ttest score :',gnb.score(tf_X_test.toarray(), y_test))\ny_pred = gnb.predict(tf_X_test.toarray())","9b2c72ca":"confusion_matrix(y_test, y_pred)","d91933f7":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(gnb, tfidf_transform.toarray(), y, cv=cv).mean()","7ea54f40":"mnb = MultinomialNB(alpha=0.22) \nmnb.fit(tf_X_train.toarray(), y_train) \nprint('Train score :',mnb.score(tf_X_train.toarray(), y_train))\nprint('Ttest score :',mnb.score(tf_X_test.toarray(), y_test))\ny_pred = mnb.predict(tf_X_test.toarray())","93a43f27":"confusion_matrix(y_test, y_pred)","62c07d36":"tfidf_comments = tfidf_f.transform(['expensive'])\nsvm_rbf.predict(tfidf_comments)\nround(svm_rbf.predict_proba(tfidf_comments)[0][1], 5)","49c7b87e":"# Data Exploratory","ddca0526":"Prepare pros before merging","abc4fbf9":"# Analysing Samsung Internal SSD Reviews\n","6754b50c":"# Problem statement","902a7747":"#### Number of null values in each column","37df2b1f":"# Modeling","38ac8539":"Merging 'pros' and 'cons'","b188be51":"And here our final data before modeling","9575e672":"## TF-IDF\nLet's see if TF-IDF improves the accuracy.","b6dd1bbf":"##### <span style=\"color:blue\">The Support Vector Machine(svm) result is the best<\/span>","a778be98":"Prepare cons before merging","a956ff03":"# Feature Engineering","270f2be3":"#### Merging 'pros' and cons in one column in a new dataframe","eae9dce2":"# Conclusion\nAfter we analysed Samsung internal ssd customers reviews, And after we build a sentiment analysis model, And after we test our model with cross validation We got the following:<br>\n- Our model can classifie any new ssd review with 88% Accuracy\n\n<br>\nAnd finally after we test a lot of models the model we chose to be the best model in this case is: The Support Vector Machine(svm) with 'rbf' kernal.\n\nI hope you enjoyed reading this notebook, Have a great day.","5ad70060":"Below we are droping any row contains ('none', 'none so far', 'non') because these are not cons, So basicly what the customer meant by writing 'none' here in 'cons' that there is no cons","82d22d44":"**Looks like the product doing very well!, The most majority of customers rated the product with 5 stars, And this might be a problem in normal cases because the sentiment analysis model will be bias to the good reviews, Why? because they are above 80% of our data, But like I said this is in a normal case scenario,<br><br>\nThe good thing here that we can do a simple trick to fix this problem also this trick will double our data, Intresting yes? Alright to discover this trick look at the feature engineering part.**","4eab7e84":"Here we were provided with a dataset of Samsung internal ssd customers reviews, And we will try to understand if the customer is happy or not by doing sentiment analysis to the customers reviews.<br><br><br>","4296d206":"![download.jpg](attachment:download.jpg)","041754b2":"<b>Test manually<\/b>","0cb1663a":"We can see below our data now balanced","2e6d4618":"#### Trick explaining\n\nWe saw before how the 'overall_review' column missing a lot of data, So we can't use it and expect a good result.<br>\nWhat we will do insted is merging 'pros' column with 'cons' column to be in a single column, And what I meeant be merging them is by creating a new column called \"pros_and_cons\" in a new dataframe and create another column with this column called positive, which will contain 0 and 1, The value will be 1 if the row in \"pros_and_cons\" column contains pros, And will contain 0 if the row in \"pros_and_cons\" column contains cons<br>\n\nAnd how this is will double our data? by puting 'cons' column under 'pros' column in our new column \"pros_and_cons\", so it will be 4000 row insted of 2000 row.","4ab9bea5":"We can see below that overall_review column has a lot of null values, Also we can see that we have 2 columns called 'pros' and 'cons' and they are almost has no null values, So in the feature engineering section I will explain how we can benefit from that"}}