{"cell_type":{"29c21a59":"code","be869e8e":"code","3e61599e":"code","252077cd":"code","7524d476":"code","5fc37d96":"code","5a8dd653":"code","bc9ad11d":"code","f98ddae8":"code","574f7ca5":"code","742fc8fd":"code","b633210f":"code","acb28a7c":"code","27edd81a":"code","2b60d3d2":"code","ea63b680":"code","adefab22":"code","87b94030":"code","b531fc5d":"code","f73e0906":"code","f7b8faac":"code","6e70a753":"code","642e7d98":"code","7eebdc75":"code","c251af3f":"code","b90a072a":"code","ba03d4df":"code","c16e3bd1":"code","285998a3":"code","4cd73530":"code","bb910def":"code","ee88f4b2":"code","7df7cacd":"code","37e7b658":"code","472c1092":"code","becef0c2":"markdown","857fec35":"markdown","882b1954":"markdown","202eb838":"markdown","431453af":"markdown","7a78ba3e":"markdown","df547f02":"markdown","19653a42":"markdown","2077bf1b":"markdown","1750d902":"markdown","fa718b10":"markdown","5e0fafa5":"markdown","35063c67":"markdown","1d1b35ba":"markdown","1933158a":"markdown","e6f84d14":"markdown","793aadff":"markdown","8d950e6c":"markdown","c5588b5d":"markdown","af33078c":"markdown","c9cbfb95":"markdown","e72acadc":"markdown","3e2eee53":"markdown","d5fda910":"markdown"},"source":{"29c21a59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn as sk\nimport matplotlib as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","be869e8e":"columns = ['Id', 'age', 'workclass', 'final_weight', 'education', 'education_num', 'marital_status',\n            'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n            'native_country', 'income']\n#with this we put headers in the data, instaead of it's column 0 being the header\n\nadult = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv', names = columns, sep=',', engine='python',\n                     na_values=\"?\").drop(0, axis = 0).reset_index(drop = True)\n\nadult.head()","3e61599e":"adult.describe(include = 'all')","252077cd":"num_columns = ['age', 'final_weight', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\ncat_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n\nfor col in num_columns:\n    adult[col] = pd.to_numeric(adult[col])\n    \nsns.set()\nsns.pairplot(adult, vars = num_columns, hue = 'income', palette = 'husl', dropna = True)","7524d476":"for col in cat_columns:\n    adult[col] = pd.Categorical(adult[col])\n    \nnum_columns = ['age', 'final_weight', 'education_num', 'hours_per_week', 'capital_gain'] #updating the list because we'll use these atributes\n\nfor numeric in num_columns:\n    for col in cat_columns:\n        sns.set()\n        sns.catplot(x = col, y = numeric, height = 6, aspect = 1.75, data=adult);","5fc37d96":"cat_columns = ['marital_status', 'race', 'sex', 'native_country', 'income'] #updating the categorical atributes used\n#workclass doesn't seen to change much the results in the income, as well as ocupation and relationship. \n#The education column isn;t useful because there's the education num column being used already","5a8dd653":"#the numeric values will be dealt by analysing the frequency, standard deviation and speculating some inferences\nfor col in num_columns:\n    print(col + \":\")\n    print(adult[col].describe())\n    print()\n#the categorical values will be dealt by inference and commom sense","bc9ad11d":"#just getting the data of women and men separately\nmen, women = [], []\n\nfor index, row in adult.iterrows():\n    if row['sex'] == \"Male\":\n        men.append(row)\n    else:\n        women.append(row)\nwomen = pd.DataFrame(women, columns = columns)\nmen = pd.DataFrame(men, columns = columns) \n\nprint(women.describe())\nprint(men.describe())","f98ddae8":"values = {'age': 39, 'education.num': 9, 'hours.per.week': 40, 'sex': 'Male', 'race': 'Black', 'marital_status': 'Married-civ-spouse',\n          'native_country': 'Hungary', 'capital_gain': 40000} \nadult.fillna(values)\nadult['final_weight'] = adult.apply(lambda row: ('2.411530e+05' if row['sex']== 'Male' else '2.283315e+05') if np.isnan(row['final_weight']) else row['final_weight'],\n    axis=1)","574f7ca5":"#the columns we will create are: 'graduated', 'maried' and 'american', using the columns 'education_num', 'marital_status' and 'native_country'\ngraduated = []\nfor i in range(len(adult)): \n    if adult['education_num'][i] <= 8:\n        graduated.append(0)\n    elif adult['education_num'][i] <= 12:\n        graduated.append(1)\n    else:\n        graduated.append(2)\nadult['graduated'] = graduated\n\nmaried = []\nfor i in range(len(adult)): \n    if adult['marital_status'][i] == 'Married-civ-spouse' or adult['marital_status'][i] == 'Maried-AF-spouse':\n        maried.append(1)\n    else:\n        maried.append(0)\nadult['maried'] = maried\n\namerican = []\nfor i in range(len(adult)): \n    if adult['native_country'][i] == 'United-States':\n        american.append(1)\n    else:\n        american.append(0)\nadult['american'] = american\n\nadult.head()","742fc8fd":"#In order to use Male or Female (Sex) data, we will create a new column that is a numeric view of this categorical data\nMale_or_Female = []\nfor i in range(len(adult)): \n    if adult[\"sex\"][i] == \"Male\":\n        Male_or_Female.append(1)\n    else:\n        Male_or_Female.append(0)\n    \nadult[\"Male_or_Female\"] = Male_or_Female","b633210f":"#before moving on to the classifiers it is important to use the same methods on the testing data\n\nadultTest = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/test_data.csv', names = columns, sep=',', engine='python',\n                     na_values=\"?\").drop(0, axis = 0).reset_index(drop = True)\n\nvalues = {'age': 39, 'education.num': 9, 'hours.per.week': 40, 'sex': 'Male', 'race': 'Black', 'marital_status': 'Married-civ-spouse',\n          'native_country': 'Hungary'}\nadultTest.fillna(values)\nadultTest['final_weight'] = adultTest.apply(lambda row: ('2.411530e+05' if row['sex']== 'Male' else '2.283315e+05') if pd.isnull(row['final_weight']) else row['final_weight'],\n    axis=1)\n\ngraduated = []\nfor i in range(len(adultTest)): \n    num = int(adultTest['education_num'][i])\n    if num <= 8:\n        graduated.append(0)\n    elif num <= 12:\n        graduated.append(1)\n    else:\n        graduated.append(2)\nadultTest['graduated'] = graduated\n\nmaried = []\nfor i in range(len(adultTest)): \n    if adultTest['marital_status'][i] == 'Married-civ-spouse' or adultTest['marital_status'][i] == 'Maried-AF-spouse':\n        maried.append(1)\n    else:\n        maried.append(0)\nadultTest['maried'] = maried\n\namerican = []\nfor i in range(len(adultTest)): \n    if adultTest['native_country'][i] == 'United-States':\n        american.append(1)\n    else:\n        american.append(0)\nadultTest['american'] = american\n\nMale_or_Female = []\nfor i in range(len(adultTest)): \n    if adultTest[\"sex\"][i] == \"Male\":\n        Male_or_Female.append(1)\n    else:\n        Male_or_Female.append(0)\n    \nadultTest[\"Male_or_Female\"] = Male_or_Female","acb28a7c":"#now, just separating the X and Y\nadult = adult[columns[:-1] + ['graduated', 'maried', 'american', 'Male_or_Female', 'income']] #reordering\ndataset = adult.to_numpy()\nadultX = dataset[:,:-1]\nadultY = dataset[:,len(dataset[0])-1]\nadultX = pd.DataFrame(adultX)\nadultY = pd.DataFrame(adultY)\n\n#and now for the testing data\nadultTest = adultTest[columns[:-1] + ['graduated', 'maried', 'american', 'Male_or_Female']] #reordering\ndatasetTest = adultTest.to_numpy()\nadultTestX = datasetTest[:,:]\nadultTestX = pd.DataFrame(adultTestX)","27edd81a":"#Now,transforming categorical data into numeric data\n#Just remembering we're suign the columns: ['age', 'final_weight', 'education_num', 'hours_per_week', 'maried', \n#'race', 'Male_or_Female', 'American', 'income']\n        \nfor index,row in adultX.iterrows():\n    if row[9] == 'White':\n        row[9] = 0\n    elif row[9] == 'Asian-Pac-Islander':\n        row[9] = 1\n    elif row[9] == 'Amer-Idian-Eskimo':\n        row[9] = 2\n    elif row[9] == 'Black':\n        row[9] = 3\n    else:\n        row[9] = 4\n        \nfor index, row in adultTestX.iterrows():\n    if row[9] == 'White':\n        row[9] = 0\n    elif row[9] == 'Asian-Pac-Islander':\n        row[9] = 1\n    elif row[9] == 'Amer-Idian-Eskimo':\n        row[9] = 2\n    elif row[9] == 'Black':\n        row[9] = 3\n    else:\n        row[9] = 4\n        \nfor index, row in adultY.iterrows():\n    if row[0] == '>50K':\n        row[0] = 1\n    else:\n        row[0] = 0\n        \nadultY = adultY.astype('int')","2b60d3d2":"print(adultX.head())\nadultX = adultX[[1, 3, 9, 11, 13, 15, 16, 17, 18]]\nadultTestX = adultTestX[[1, 3, 9, 11, 13, 15, 16, 17, 18]]\n#separating only the attributes that were treated\n\n#the testing dataframe had and error that some numbers were considered strings\nfor index,row in adultTestX.iterrows():\n    for i in [1,3,9,11, 13,15,16,17,18]:\n        row[i] = int(row[i])","ea63b680":"#for svm me will standardize the data\nnormalized_X = (adultX - adultX.mean())\/adultX.std()\nnormalized_TestX = (adultTestX - adultTestX.mean())\/adultTestX.std()\n\nfrom sklearn.svm import SVC\nSVM = SVC(C=1.0, kernel='rbf', gamma='scale', coef0=0.0, shrinking=True, probability=False,\n                 tol=0.001, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)","adefab22":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(SVM, normalized_X, adultY.to_numpy().ravel(), cv=10)\nprint(\"scores = {}\". format(scores))\nprint(\"accuracy = {}\".format(scores.mean()))","87b94030":"SVM.fit(normalized_X, adultY.to_numpy().ravel())","b531fc5d":"results_SVM = SVM.predict(normalized_TestX)","f73e0906":"ResultsSVM = []\nfor i in range(len(results_SVM)):\n    if results_SVM[i] == 1:\n        ResultsSVM.append('>50K')\n    else:\n        ResultsSVM.append('<=50K')\n\nid_index = pd.DataFrame({'Id' : list(range(len(ResultsSVM)))})\nincome = pd.DataFrame({'income' : ResultsSVM})\nresultSVM = income\nresultSVM","f7b8faac":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nRandomForest = RandomForestClassifier(n_estimators=750, criterion='gini', max_depth=11, min_samples_split=2,\n                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,\n                                      min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, \n                                      random_state=None, verbose=0, warm_start=False, class_weight=None)","6e70a753":"scores = cross_val_score(RandomForest, normalized_X, adultY.to_numpy().ravel(), cv=10)\nprint(\"scores = {}\". format(scores))\nprint(\"accuracy = {}\".format(scores.mean()))","642e7d98":"RandomForest.fit(normalized_X, adultY.to_numpy().ravel())","7eebdc75":"results_RandomForest = RandomForest.predict(normalized_TestX)\nResultsRandomForest = []\nfor i in range(len(results_RandomForest)):\n    if results_RandomForest[i] == 1:\n        ResultsRandomForest.append('>50K')\n    else:\n        ResultsRandomForest.append('<=50K')\n\nid_index = pd.DataFrame({'Id' : list(range(len(ResultsRandomForest)))})\nincome = pd.DataFrame({'income' : ResultsRandomForest})\nresultRandomForest = income\nresultRandomForest","c251af3f":"LogisticRegression = sk.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n                                   class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', \n                                   verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)","b90a072a":"scores = cross_val_score(LogisticRegression, normalized_X, adultY.to_numpy().ravel(), cv=10)\nprint(\"scores = {}\". format(scores))\nprint(\"accuracy = {}\".format(scores.mean()))","ba03d4df":"LogisticRegression.fit(normalized_X, adultY.to_numpy().ravel())","c16e3bd1":"results_LogisticRegression = LogisticRegression.predict(normalized_TestX)\nResultsLogisticRegression = []\nfor i in range(len(results_LogisticRegression)):\n    if results_LogisticRegression[i] == 1:\n        ResultsLogisticRegression.append('>50K')\n    else:\n        ResultsLogisticRegression.append('<=50K')\n\nid_index = pd.DataFrame({'Id' : list(range(len(ResultsLogisticRegression)))})\nincome = pd.DataFrame({'income' : ResultsLogisticRegression})\nresultLogisticRegression = income\nresultLogisticRegression","285998a3":"resultSVM.to_csv(\"submissionSVM.csv\", index = True, index_label = 'Id')\nresultRandomForest.to_csv(\"submissionRandomForest.csv\", index = True, index_label = 'Id')\nresultLogisticRegression.to_csv(\"submissionLogisticRegression.csv\", index = True, index_label = 'Id')","4cd73530":"train = pd.read_csv('..\/input\/atividade-3\/train.csv', na_values='?').reset_index(drop = True)\ntest = pd.read_csv('..\/input\/atividade-3\/test.csv', na_values='?').reset_index(drop = True)\ntrain.head()","bb910def":"train.describe()","ee88f4b2":"columns = ['longitude', 'latitude', 'median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\nYcolumn = 'median_house_value'","7df7cacd":"trainX = train[columns]\ntrainY = train[[Ycolumn]]","37e7b658":"#we will use a KNN and a Random Forest, testing the classifiers\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nRandomForestHousehold = RandomForestClassifier(n_estimators=400, criterion='gini', max_depth=9, min_samples_split=2,\n                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,\n                                      min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, \n                                      random_state=None, verbose=0, warm_start=False, class_weight=None)\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(RandomForestHousehold, trainX , trainY.to_numpy().ravel() , cv=5)\nprint(\"scores = {}\". format(scores))\nprint(\"accuracy = {}\".format(scores.mean()))","472c1092":"KNN = sk.neighbors.KNeighborsClassifier(n_neighbors=15, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski',\n                                       metric_params=None, n_jobs=None)\n\nscores = cross_val_score(KNN, trainX , trainY.to_numpy().ravel(), cv=5)\nprint(\"scores = {}\". format(scores))\nprint(\"accuracy = {}\".format(scores.mean()))","becef0c2":"<font size=\"4\">OK, we will put the mean for the atributes age and hours per week, since it's standard deviation isn't high.\nNow, for the final weight we will assume that people won't tell the census about it because either they are up the mean, and this depends biologically if it's a male or female, because of society imposed standards, so we'll check if it is a woman or a man and treat with this information.\nFor the education num we can assume it's beacause they've quitted school early,so we'll assume the first quart as it's number.\nFor the capital gain we will assume 40000.\n<\/font>","857fec35":"<font size=\"5\">**Now we'll be analying the data**<\/font>","882b1954":"The Random Forest got the highest accuracy but also took a lot of time to run, it wasn't so easy to implement but it wasn't rocket science. It has the best overall results because of it's high accuracy. It also used a lot of memory. The interpretability isn't its best part, because when a lot of tress are used together it makes things harder to understand.\n","202eb838":"<font size=\"4\">Now we'll be analying the categorical data<\/font>","431453af":"<font size=\"3\">So, the three classifiers chosen have been SVM, Random Forest and Logistic Regression because all of them are relatively easy to implement, and are also very commonly used by professionals. They're avalible in the sklearning packet and have lots of online tutorials that can help users to get started.<\/font>","7a78ba3e":"<font size=\"5\">**Let's firstly get the data**<\/font>","df547f02":"<font size=\"4\">SVM<\/font>","19653a42":"The SVM wasn't fast to run and it used a lot of memory, but it was easy to implement once all the data was normalized and treated. The accuracy was high but not too much. Overall this classifier is used when more complex system is needed, a lot of data is avaliable and requires no further explanation.","2077bf1b":"<font size=\"5\">Conclusion<\/font>","1750d902":"The logistic regression is (almost) a must try, because it is the simplest and has huge accuracy when considering its use of memory and time to run. It didn't had a high accuracy, considering the others, but it had by far the lowest run time and the loest memory use. It's also very simple to interpret and be understood.\n","fa718b10":"<font size=\"5\">Extra Activity: CaliforniaHousing<\/font>","5e0fafa5":"<font size=\"4\">Random Forest<\/font>","35063c67":"<font size=\"5\">**Now we'll be analying dealing with missing values and preparating the data**<\/font>","1d1b35ba":"<font size=\"4\">Firstly the missing data <\/font>","1933158a":"since it has few attributes we will use all of them to train our classifier. Also, it will be a 'simple' classifier for time purposes and for the lack of a big amount of data","e6f84d14":"<font size=\"4\">We see that the most import ones are age, final_weight, education_num, capital_gain and hours_per_week <\/font>","793aadff":"The tests run in the KNN and in the RandomForest show that the Random Forest got a higher accuracy (as in the adult database) but used more memory and took longer to run than the KNN. The KNN got good results despite its simplicity","8d950e6c":"<font size=\"4\">Logistic Regression<\/font>","c5588b5d":"<font size=\"4\">Analysing the data<\/font>","af33078c":"<font size=\"4\">For the categorical data: \n    The marital status will be filled with the most frequent one (Married-civ-spouse), as seen in the .describe() method above;\n    The race will be filled with Black because this could be a source of racism;\n    The sex will be filled with man, since it's the most frequent and the ones that didn't answer this question wouldn't identify with eitheer;\n    The native country will be replaced with a country other than USA, because this could be a away of avoiding xenophobia.\n<\/font>","c9cbfb95":"<font size=\"5\">Classifiers<\/font>","e72acadc":"<font size=\"4\">Conclusion<\/font>","3e2eee53":"<font size=\"5\">Now, we will create auxiliar columns to use, based on inference and aspects analysed earlier<\/font>","d5fda910":"<font size=\"4\">We can see that some variables have less interference in the object of study of this evaluation, so this means that those with high variance won't be usefull when traing the classifier, especially beacuse it isn't a huge amount of data, therefore it would interfere negatively in the traning process.<\/font>"}}