{"cell_type":{"318ea4eb":"code","d3987e1a":"code","0f43326f":"code","3c18d219":"code","210f2cbf":"code","c8f16ec1":"code","9f49c840":"code","153c4ba6":"code","770d7377":"code","5816404c":"code","9d9da7e0":"code","8a169675":"code","2d1b74b1":"code","eecfe7d9":"code","afde136e":"code","338c7e9b":"code","499c2ac0":"code","2c52df3f":"code","b689e298":"code","4da6e8a2":"code","1fbbc6e0":"code","3e4720ea":"code","7b597183":"code","f8396127":"code","84c62c93":"code","7de17fb0":"code","3ea5d34f":"code","cb061569":"code","2c5bb412":"code","05ef3839":"code","da1c8ca9":"code","7b4c961d":"code","e452c811":"code","7528f230":"code","165aede6":"code","8f06d10d":"code","912fb8ed":"code","62233aed":"code","a9a66fa2":"code","51c37f17":"code","a8036133":"code","3aa79b5d":"code","5aa1c816":"code","98028296":"code","1cc3cd1d":"code","14fc33e4":"code","c29a6e99":"code","06d749d8":"code","8a542a9f":"code","e0787706":"code","1ebad706":"code","ab4bd65c":"code","bf734990":"code","b151f450":"code","63c2fde3":"code","b5025c60":"code","97f0b41f":"code","4b13c517":"code","f134e8d9":"code","10525b86":"code","2a2414dd":"code","0966c6df":"code","ea6d3fb9":"code","6e9f0798":"code","fdd96837":"code","070455ad":"code","61d86285":"code","1caca7f5":"code","73d4aa7a":"markdown","0cc64d22":"markdown","edb9cb8f":"markdown","4a054647":"markdown","e7ab8821":"markdown","c4762d73":"markdown","685a5534":"markdown","b6b72178":"markdown","b0d895fd":"markdown","10c08732":"markdown","c82cdbf9":"markdown","676121ba":"markdown","0a5bd80a":"markdown","3a2d0e20":"markdown","6c8efa79":"markdown","744f8779":"markdown","98a99320":"markdown","968444e1":"markdown","bc6cc7ee":"markdown","5c716d33":"markdown","611ae97e":"markdown","19ead27f":"markdown","0172f0ff":"markdown","6adcfd82":"markdown","276cb7f8":"markdown","cec76c46":"markdown","240044f0":"markdown","aeb84575":"markdown","5c6727a8":"markdown","3269ec11":"markdown","87735afb":"markdown","44ca82c6":"markdown","370aa4ff":"markdown","4a77c0fb":"markdown","457855e0":"markdown","345da3d3":"markdown","4e514245":"markdown","ca9deb08":"markdown","a3238711":"markdown","b47495b0":"markdown","8551278f":"markdown","d323fc83":"markdown","1acc119b":"markdown","9bd81df2":"markdown","1fd5ecc2":"markdown","24bd8555":"markdown","35d1a822":"markdown","11a04f2d":"markdown","fdfa67ee":"markdown","32ac5dd2":"markdown","581c504c":"markdown"},"source":{"318ea4eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import ensemble\nfrom sklearn import tree\nfrom sklearn import linear_model\nimport os, datetime, sys, random, time\nimport seaborn as sns\nimport xgboost as xgs\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom mlxtend import classifier\nplt.style.use('fivethirtyeight')\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats, special\nimport shap\nimport catboost as ctb","d3987e1a":"trainingData = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv')\ntestData = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv')","0f43326f":"trainingData.head()","3c18d219":"trainingData.info()","210f2cbf":"trainingData.describe()","c8f16ec1":"print(trainingData.shape)\nprint(testData.shape)","9f49c840":"testData.head()","153c4ba6":"testData.info()","770d7377":"testData.describe()","5816404c":"finalTrain = trainingData.copy()\nfinalTest = testData.copy()","9d9da7e0":"finalTest.drop('SeriousDlqin2yrs', axis=1, inplace = True)","8a169675":"trainID = finalTrain['Unnamed: 0']\ntestID = finalTest['Unnamed: 0']\n\nfinalTrain.drop('Unnamed: 0', axis=1, inplace=True)\nfinalTest.drop('Unnamed: 0', axis=1, inplace=True)","2d1b74b1":"fig, axes = plt.subplots(1,2,figsize=(12,6))\nfinalTrain['SeriousDlqin2yrs'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=axes[0])\naxes[0].set_title('SeriousDlqin2yrs')\n#ax[0].set_ylabel('')\nsns.countplot('SeriousDlqin2yrs',data=finalTrain,ax=axes[1])\naxes[1].set_title('SeriousDlqin2yrs')\nplt.show()","eecfe7d9":"fig = plt.figure(figsize=[30,30])\nfor col,i in zip(finalTrain.columns,range(1,13)):\n    axes = fig.add_subplot(7,2,i)\n    sns.regplot(finalTrain[col],finalTrain.SeriousDlqin2yrs,ax=axes)\nplt.show()","afde136e":"print(\"Unique values in '30-59 Days' values that are more than or equal to 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                       ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                    ['NumberOfTimes90DaysLate']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                           ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                        ['NumberOfTimes90DaysLate']))\n\n\nprint(\"Proportion of positive class with special 96\/98 values:\",\n      round(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]['SeriousDlqin2yrs'].sum()*100\/\n      len(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]['SeriousDlqin2yrs']),2),'%')","338c7e9b":"finalTrain.loc[finalTrain['NumberOfTime30-59DaysPastDueNotWorse'] >= 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 13\nfinalTrain.loc[finalTrain['NumberOfTime60-89DaysPastDueNotWorse'] >= 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 11\nfinalTrain.loc[finalTrain['NumberOfTimes90DaysLate'] >= 90, 'NumberOfTimes90DaysLate'] = 17","499c2ac0":"print(\"Unique values in 30-59Days\", np.unique(finalTrain['NumberOfTime30-59DaysPastDueNotWorse']))\nprint(\"Unique values in 60-89Days\", np.unique(finalTrain['NumberOfTime60-89DaysPastDueNotWorse']))\nprint(\"Unique values in 90Days\", np.unique(finalTrain['NumberOfTimes90DaysLate']))","2c52df3f":"print(\"Unique values in '30-59 Days' values that are more than or equal to 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                       ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                    ['NumberOfTimes90DaysLate']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                           ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                        ['NumberOfTimes90DaysLate']))","b689e298":"finalTest.loc[finalTest['NumberOfTime30-59DaysPastDueNotWorse'] >= 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 19\nfinalTest.loc[finalTest['NumberOfTime60-89DaysPastDueNotWorse'] >= 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 9\nfinalTest.loc[finalTest['NumberOfTimes90DaysLate'] >= 90, 'NumberOfTimes90DaysLate'] = 18\n\nprint(\"Unique values in 30-59Days\", np.unique(finalTest['NumberOfTime30-59DaysPastDueNotWorse']))\nprint(\"Unique values in 60-89Days\", np.unique(finalTest['NumberOfTime60-89DaysPastDueNotWorse']))\nprint(\"Unique values in 90Days\", np.unique(finalTest['NumberOfTimes90DaysLate']))","4da6e8a2":"print('Debt Ratio: \\n',finalTrain['DebtRatio'].describe())\nprint('\\nRevolving Utilization of Unsecured Lines: \\n',finalTrain['RevolvingUtilizationOfUnsecuredLines'].describe())","1fbbc6e0":"quantiles = [0.75,0.8,0.81,0.85,0.9,0.95,0.975,0.99]\n\nfor i in quantiles:\n    print(i*100,'% quantile of debt ratio is: ',finalTrain.DebtRatio.quantile(i))","3e4720ea":"finalTrain[finalTrain['DebtRatio'] >= finalTrain['DebtRatio'].quantile(0.95)][['SeriousDlqin2yrs','MonthlyIncome']].describe()","7b597183":"finalTrain[(finalTrain[\"DebtRatio\"] > finalTrain[\"DebtRatio\"].quantile(0.95)) & (finalTrain['SeriousDlqin2yrs'] == finalTrain['MonthlyIncome'])]","f8396127":"finalTrain = finalTrain[-((finalTrain[\"DebtRatio\"] > finalTrain[\"DebtRatio\"].quantile(0.95)) & (finalTrain['SeriousDlqin2yrs'] == finalTrain['MonthlyIncome']))]\nfinalTrain","84c62c93":"finalTrain[finalTrain['RevolvingUtilizationOfUnsecuredLines']>10].describe()","7de17fb0":"finalTrain[finalTrain['RevolvingUtilizationOfUnsecuredLines']>13].describe()","3ea5d34f":"finalTrain = finalTrain[finalTrain['RevolvingUtilizationOfUnsecuredLines']<=13]\nfinalTrain","cb061569":"def MissingHandler(df):\n    DataMissing = df.isnull().sum()*100\/len(df)\n    DataMissingByColumn = pd.DataFrame({'Percentage Nulls':DataMissing})\n    DataMissingByColumn.sort_values(by='Percentage Nulls',ascending=False,inplace=True)\n    return DataMissingByColumn[DataMissingByColumn['Percentage Nulls']>0]\n\nMissingHandler(finalTrain)","2c5bb412":"finalTrain['MonthlyIncome'].fillna(finalTrain['MonthlyIncome'].median(), inplace=True)\nfinalTrain['NumberOfDependents'].fillna(0, inplace = True)","05ef3839":"MissingHandler(finalTrain)","da1c8ca9":"MissingHandler(finalTest)","7b4c961d":"finalTest['MonthlyIncome'].fillna(finalTrain['MonthlyIncome'].median(), inplace=True)\nfinalTest['NumberOfDependents'].fillna(0, inplace = True)","e452c811":"MissingHandler(finalTest)","7528f230":"print(finalTrain.shape)\nprint(finalTest.shape)","165aede6":"fig = plt.figure(figsize = [15,10])\nmask = np.zeros_like(finalTrain.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(finalTrain.corr(), cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9), mask = mask, annot=True, center = 0)\nplt.title(\"Correlation Matrix (HeatMap)\", fontsize = 15)","8f06d10d":"SeriousDlqIn2Yrs = finalTrain['SeriousDlqin2yrs']\n\nfinalTrain.drop('SeriousDlqin2yrs', axis = 1 , inplace = True)\n","912fb8ed":"finalData = pd.concat([finalTrain, finalTest])\n\nfinalData.shape","62233aed":"#New Features\n\nfinalData['MonthlyIncomePerPerson'] = finalData['MonthlyIncome']\/(finalData['NumberOfDependents']+1)\nfinalData['MonthlyIncomePerPerson'].fillna(0, inplace=True)\n\nfinalData['MonthlyDebt'] = finalData['MonthlyIncome']*finalData['DebtRatio']\nfinalData['MonthlyDebt'].fillna(finalData['DebtRatio'],inplace=True)\nfinalData['MonthlyDebt'] = np.where(finalData['MonthlyDebt']==0, finalData['DebtRatio'],finalData['MonthlyDebt'])\n\nfinalData['isRetired'] = np.where((finalData['age'] > 65), 1, 0)\n\nfinalData['RevolvingLines'] = finalData['NumberOfOpenCreditLinesAndLoans']-finalData['NumberRealEstateLoansOrLines']\n\nfinalData['hasRevolvingLines']=np.where((finalData['RevolvingLines']>0),1,0)\n\nfinalData['hasMultipleRealEstates'] = np.where((finalData['NumberRealEstateLoansOrLines']>=2),1,0)\n\nfinalData['incomeDivByThousand'] = finalData['MonthlyIncome']\/1000","a9a66fa2":"finalData.shape","51c37f17":"MissingHandler(finalData)","a8036133":"columnList = list(finalData.columns)\ncolumnList\n\nfig = plt.figure(figsize=[20,20])\nfor col,i in zip(columnList,range(1,19)):\n    axes = fig.add_subplot(6,3,i)\n    sns.distplot(finalData[col],ax=axes, kde_kws={'bw':1.5}, color='purple')\nplt.show()","3aa79b5d":"def SkewMeasure(df):\n    nonObjectColList = df.dtypes[df.dtypes != 'object'].index\n    skewM = df[nonObjectColList].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending = False)\n    skewM=pd.DataFrame({'skew':skewM})\n    return skewM[abs(skewM)>0.5].dropna()\n\nskewM = SkewMeasure(finalData)\nskewM","5aa1c816":"for i in skewM.index:\n    finalData[i] = special.boxcox1p(finalData[i],0.15) #lambda = 0.15\n    \nSkewMeasure(finalData)","98028296":"fig = plt.figure(figsize=[20,20])\nfor col,i in zip(columnList,range(1,19)):\n    axes = fig.add_subplot(6,3,i)\n    sns.distplot(finalData[col],ax=axes, kde_kws={'bw':1.5}, color='purple')\nplt.show()","1cc3cd1d":"trainDF = finalData[:len(finalTrain)]\ntestDF = finalData[len(finalTrain):]\nprint(trainDF.shape)\nprint(testDF.shape)","14fc33e4":"xTrain, xTest, yTrain, yTest = model_selection.train_test_split(trainDF.to_numpy(),SeriousDlqIn2Yrs.to_numpy(),test_size=0.3,random_state=2020)","c29a6e99":"lgbAttributes = lgb.LGBMClassifier(objective='binary', n_jobs=-1, random_state=2020, importance_type='gain')\n\nlgbParameters = {\n    'max_depth' : [2,3,4,5],\n    'learning_rate': [0.05, 0.1,0.125,0.15],\n    'colsample_bytree' : [0.2,0.4,0.6,0.8,1],\n    'n_estimators' : [400,500,600,700,800,900],\n    'min_split_gain' : [0.15,0.20,0.25,0.3,0.35], #equivalent to gamma in XGBoost\n    'subsample': [0.6,0.7,0.8,0.9,1],\n    'min_child_weight': [6,7,8,9,10],\n    'scale_pos_weight': [10,15,20],\n    'min_data_in_leaf' : [100,200,300,400,500,600,700,800,900],\n    'num_leaves' : [20,30,40,50,60,70,80,90,100]\n}\n\nlgbModel = model_selection.RandomizedSearchCV(lgbAttributes, param_distributions = lgbParameters, cv = 5, random_state=2020)\n\nlgbModel.fit(xTrain,yTrain.flatten(),feature_name=trainDF.columns.to_list())","06d749d8":"bestEstimatorLGB = lgbModel.best_estimator_\nbestEstimatorLGB","8a542a9f":"bestEstimatorLGB = lgb.LGBMClassifier(colsample_bytree=0.4, importance_type='gain', max_depth=5,\n               min_child_weight=6, min_data_in_leaf=600, min_split_gain=0.25,\n               n_estimators=900, num_leaves=50, objective='binary',\n               random_state=2020, scale_pos_weight=10, subsample=0.9).fit(xTrain,yTrain.flatten(),feature_name=trainDF.columns.to_list())","e0787706":"yPredLGB = bestEstimatorLGB.predict_proba(xTest)\nyPredLGB = yPredLGB[:,1]","1ebad706":"yTestPredLGB = bestEstimatorLGB.predict(xTest)\nprint(metrics.classification_report(yTest,yTestPredLGB))","ab4bd65c":"metrics.confusion_matrix(yTest,yTestPredLGB)","bf734990":"LGBMMetrics = pd.DataFrame({'Model': 'LightGBM', \n                            'MSE': round(metrics.mean_squared_error(yTest, yTestPredLGB)*100,2),\n                            'RMSE' : round(np.sqrt(metrics.mean_squared_error(yTest, yTestPredLGB)*100),2),\n                            'MAE' : round(metrics.mean_absolute_error(yTest, yTestPredLGB)*100,2),\n                            'MSLE' : round(metrics.mean_squared_log_error(yTest, yTestPredLGB)*100,2), \n                            'RMSLE' : round(np.sqrt(metrics.mean_squared_log_error(yTest, yTestPredLGB)*100),2),\n                            'Accuracy Train' : round(bestEstimatorLGB.score(xTrain, yTrain) * 100,2),\n                            'Accuracy Test' : round(bestEstimatorLGB.score(xTest, yTest) * 100,2),\n                            'F-Beta Score (\u03b2=2)' : round(metrics.fbeta_score(yTest, yTestPredLGB, beta=2)*100,2)},index=[1])\n\nLGBMMetrics","b151f450":"fpr,tpr,_ = metrics.roc_curve(yTest,yPredLGB)\nrocAuc = metrics.auc(fpr, tpr)\nplt.figure(figsize=(12,6))\nplt.title('ROC Curve')\nsns.lineplot(fpr, tpr, label = 'AUC for LightGBM Model = %0.2f' % rocAuc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","63c2fde3":"lgb.plot_importance(bestEstimatorLGB, importance_type='gain')","b5025c60":"X = pd.DataFrame(xTrain, columns=trainDF.columns.to_list())\n\nexplainer = shap.TreeExplainer(bestEstimatorLGB)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values[1], X)","97f0b41f":"xgbAttribute = xgs.XGBClassifier(tree_method='gpu_hist',n_jobs=-1, gpu_id=0)\n\nxgbParameters = {\n    'max_depth' : [2,3,4,5,6,7,8],\n    'learning_rate':[0.05,0.1,0.125,0.15],\n    'colsample_bytree' : [0.2,0.4,0.6,0.8,1],\n    'n_estimators' : [400,500,600,700,800,900],\n    'gamma':[0.15,0.20,0.25,0.3,0.35],\n    'subsample': [0.6,0.7,0.8,0.9,1],\n    'min_child_weight': [6,7,8,9,10],\n    'scale_pos_weight': [10,15,20]\n    \n}\n\nxgbModel = model_selection.RandomizedSearchCV(xgbAttribute, param_distributions = xgbParameters, cv = 5, random_state=2020)\n\nxgbModel.fit(xTrain,yTrain.flatten())","4b13c517":"bestEstimatorXGB = xgbModel.best_estimator_\nbestEstimatorXGB","f134e8d9":"bestEstimatorXGB = xgs.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4, gamma=0.25, gpu_id=0,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.125, max_delta_step=0, max_depth=5,\n              min_child_weight=9,\n              monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n              n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=10, subsample=1,\n              tree_method='gpu_hist', validate_parameters=1, verbosity=None).fit(xTrain,yTrain.flatten())","10525b86":"yPredXGB = bestEstimatorXGB.predict_proba(xTest)\nyPredXGB = yPredXGB[:,1]\n\nyTestPredXGB = bestEstimatorXGB.predict(xTest)\nprint(metrics.classification_report(yTest,yTestPredXGB))","2a2414dd":"metrics.confusion_matrix(yTest,yTestPredXGB)","0966c6df":"XGBMetrics = pd.DataFrame({'Model': 'XGBoost', \n                            'MSE': round(metrics.mean_squared_error(yTest, yTestPredXGB)*100,2),\n                            'RMSE' : round(np.sqrt(metrics.mean_squared_error(yTest, yTestPredXGB)*100),2),\n                            'MAE' : round(metrics.mean_absolute_error(yTest, yTestPredXGB)*100,2),\n                            'MSLE' : round(metrics.mean_squared_log_error(yTest, yTestPredXGB)*100,2), \n                            'RMSLE' : round(np.sqrt(metrics.mean_squared_log_error(yTest, yTestPredXGB)*100),2),\n                            'Accuracy Train' : round(bestEstimatorLGB.score(xTrain, yTrain) * 100,2),\n                            'Accuracy Test' : round(bestEstimatorLGB.score(xTest, yTest) * 100,2),\n                            'F-Beta Score (\u03b2=2)' : round(metrics.fbeta_score(yTest, yTestPredXGB, beta=2)*100,2)},index=[2])\n\nXGBMetrics","ea6d3fb9":"fpr,tpr,_ = metrics.roc_curve(yTest,yPredXGB)\nrocAuc = metrics.auc(fpr, tpr)\nplt.figure(figsize=(12,6))\nplt.title('ROC Curve')\nsns.lineplot(fpr, tpr, label = 'AUC for XGBoost Model = %0.2f' % rocAuc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","6e9f0798":"bestEstimatorXGB.get_booster().feature_names = trainDF.columns.to_list()\nxgs.plot_importance(bestEstimatorXGB, importance_type='gain')","fdd96837":"# resolve a conflict\/bug with latest version of XGBoost and SHAP\nmybooster = bestEstimatorXGB.get_booster()\nmodel_bytearray = mybooster.save_raw()[4:]\ndef myfun(self=None):\n    return model_bytearray\n\nmybooster.save_raw = myfun\n\n\nX = pd.DataFrame(xTrain, columns=trainDF.columns.to_list())\n\nexplainer = shap.TreeExplainer(mybooster)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X)","070455ad":"frames = [LGBMMetrics, XGBMetrics]\nTrainingResult = pd.concat(frames)\nTrainingResult.T","61d86285":"lgbProbs = bestEstimatorLGB.predict_proba(testDF)\nlgbDF = pd.DataFrame({'ID': testID, 'Probability': lgbProbs[:,1]})\nlgbDF.to_csv('submission.csv', index=False)","1caca7f5":"lgbDF","73d4aa7a":"Hence the delinquency probabilities.","0cc64d22":"# Model Training\n\n## Train-Validation Split\n\nWe will currently split the train and validation sets into a 70-30 proportion.","edb9cb8f":"**FEATURE IMPORTANCE USING SHAP**","4a054647":"From the above distribution plots, we can see that majority of our data is skewed in either of the directions. We can only see Age forming close to normal distribution. Let's check the skewness values of each column","e7ab8821":"Step 2: Checking for **DebtRatio** and **RevolvingUtilizationOfUnsecuredLines**.","c4762d73":"# GIVE ME SOME CREDIT\n\nThis notebook is created 9 years after this competition ended. The main aim of this project is to predict the probabily whether a customer will default in the future given his record present in the dataset. We will be using **predict_proba** to determine the delinquency probabilities of the customer.\n\nThe Highlights of the notebook are:\n\n- **Exploratory Data Analysis**\n    - **Outlier Analysis\n    - **Null Handling\n    - **Distribution Analysis\n    - **Skewness Reduction (using Box Cox Transformation)\n- **Feature Engineering**\n- **LightGBM using RandomizedSearchCV (Classification)**\n    - **Evaluation Metrics**\n        - Mean Squared Error\n        - Root Mean Squared Error\n        - Mean Absolute Error\n        - Mean Squared Logarithmic Error\n        - Root Mean Square Logarithmic Error\n        - Accuracy on Training Set\n        - Accuracy on Test Set\n        - F-Beta Score (Beta = 2)\n        - F1 Score\n        - Precision\n        - Recall\n        - Confusion Matrix\n        - AUC Curve\n    - **Probability Prediction on Validation Sets**\n    - **Delinquency Prediction on Validation Sets**\n    - **Feature Importances**\n        - Summary Plot\n        - SHAP Analysis\n- **XGBoost using RandomizedSearchCV (Classification)**\n    - **Evaluation Metrics**\n        - Mean Squared Error\n        - Root Mean Squared Error\n        - Mean Absolute Error\n        - Mean Squared Logarithmic Error\n        - Root Mean Square Logarithmic Error\n        - Accuracy on Training Set\n        - Accuracy on Test Set\n        - F-Beta Score (Beta = 2)\n        - F1 Score\n        - Precision\n        - Recall\n        - Confusion Matrix\n        - AUC Curve\n    - **Probability Prediction on Validation Sets**\n    - **Delinquency Prediction on Validation Sets**\n    - **Feature Importances**\n        - Summary Plot\n        - SHAP Analysis\n        \n        \nLet's begin with importing the libraries we will be requiring for this notebook","685a5534":"Since, we need to predict the probability of Delinquency in the test data, we need to remove the additional column from it first.","b6b72178":"Similar to the training data, we have 19.71% and 2.56% nulls for MonthlyIncome and NumberOfDependents respectively.","b0d895fd":"We can see from the following that when records in column 'NumberOfTime30-59DaysPastDueNotWorse' are more than 90, the other columns that records number of times payments are past due X days also have the same values. We will classify these as special labels since the proportion of positive class is abnormally high at 54.65%.\n\nThese 96 and 98 values can be viewed as accounting errors. Hence, we would replace them with the maximum value before 96 i.e. 13, 11 and 17","10c08732":"Some of the observations are:\n\n- There are 150,000 rows for 11 features in our data.\n- We see in the training data, that all the datatypes belong to a numeric class i.e. **int** and **float**.\n- Columns **MonthlyIncome** and **NumberOfDependents** have some null values","c82cdbf9":"From the above graphs we can observe:\n\n- In the columns **NumberOfTime30-59DaysPastDueNotWorse** , **NumberOfTime60-89DaysPastDueNotWorse** and **NumberOfTimes90DaysLate**, we see delinquency range beyond 90 which is common across all 3 features.\n- There are some unusually high values for **DebtRatio** and **RevolvingUtilizationOfUnsecuredLines**.\n\nStep 1: Fixing the columns **NumberOfTime30-59DaysPastDueNotWorse** , **NumberOfTime60-89DaysPastDueNotWorse** and **NumberOfTimes90DaysLate**","676121ba":"Here you can see a massive difference between the 75th Quantile and the Max Value. Let's explore this in a greater depth.\n\n**Debt Ratio**","0a5bd80a":"Some of the observations on the testing data:\n\n- The total rows for our 11 features are 101,503. \n- Like the Training Data (as it should be), we observe numeric class's datatypes i.e. **int** and **float**.\n- Nulls were observed for features **MonthlyIncome** and **NumberOfDependents** just like the training data.","3a2d0e20":"Applying Similar Analysis for the Testing Data","6c8efa79":"Rechecking Nulls","744f8779":"The ratio of negative to positive delinquency outliers are found to be 93.3% to 6.7%, which is approximately a ratio of 14:1. Therefore, our dataset is highly imbalanced. We cannot rely on the accuracy scores to predict the model's success. Many other evaluation metrics would be considered here. But more on that later.\n\nNow let's move on the Outlier Analysis section of our EDA. Here we will remove potential outliers which might affect our predictive modelling.","98a99320":"The Skewness have reduced on a much higher scale now that the Box Cox Transformation is applied. Let's check the distribution plots for individual columns again:","968444e1":"Since, these values exist in Test Set as well. Therefore, replacing them with maximum values before 96 and 98 i.e. 19, 9 and 18.","bc6cc7ee":"### Imbalance Ratio\n\nSince we have a total data of 150,000. There are high chances that it can be an imbalanced dataset. Therefore, checking the positive and negative delinquency ratio.","5c716d33":"**FEATURE IMPORTANCE**","611ae97e":"### Additional EDA\n\nLet's study a few more things about the dataset to get more familiar with it.\n\n**CORRELATION MATRIX**","19ead27f":"Here we can observe:\n\n- Out of 7501 customers who have debt ratio greater than 95% i.e. the number of times their debt is higher than their income, only 379 have Monthly Income values.\n- The Max for Monthly Income is 1 and Min is 0 which makes us wonder that are data entry errors. Let's check whether the Serious Delinquency in 2 years and Monthly Income values are equal.","0172f0ff":"Rechecking Nulls","6adcfd82":"The Skewness is massively high for all the columns. We would apply Box Cox Transformation with **\u03bb = 0.15** in order to reduce this skewness.","276cb7f8":"As you can see there is a huge rise in quantile post 81%. So, our main aim would be to check the potential outliers beyond 81% quantiles. However, since our data is 150,000, let's consider 95% and 97.5% quantiles for our further analysis.","cec76c46":"Also as mentioned above, let's take the ID column i.e. **Unnamed: 0** and store it in seperate variables.","240044f0":"Therefore, we have 19.76% and 2.59% Nulls for MonthlyIncome and NumberOfDependents respectively. ","aeb84575":"Despite owing thousands, these 238 people do not show any default which means this might be another error. Even if it is not an error, these numbers will add huge bias and variance to our final predictions. Therefore, the best decision is to remove these values.","5c6727a8":"From here we can conclude that the column **Unnamed: 0** will have no significance in the predictive modelling because it represents ID of the customer,","3269ec11":"**FEATURE IMPORTANCE USING SHAP**","87735afb":"Hence, our suspects are true and there are 331 out of 379 rows where Monthly Income is equal to the Serious Delinquencies in 2 years. Hence we will remove these 331 outliers from our analysis as their current values aren't useful for our predictive modelling and will add to the bias and variance.\n\nThe reason behind this, is we have 331 rows where the debt ratio is massive compared to the customer's income and they arent't scrutinized for defaulting which is nothing but a data entry error.","44ca82c6":"### LGBM Submission\n\nSince, we can see our LGBM performs better, we will submit this. (Late Submission)","370aa4ff":"Setting the best estimator from RandomizedSearchCV","4a77c0fb":"We have now added new features to our dataset. Next, we will perform a skewness check on our data by analysing the distributions of individual columns and perform Box Cox Transformation to reduce the skewness.\n\n# Skewness Check and Box Cox Transformation\n\nLet's check the distribution of each values first","457855e0":"From the correlation heatmap above, we can see the most correlated values to **SeriousDlqin2yrs** are **NumberOfTime30-59DaysPastDueNotWorse** , **NumberOfTime60-89DaysPastDueNotWorse** and **NumberOfTimes90DaysLate**.\n\nNow let's move to the Feature Engineering section of our Notebook\n\n# Feature Engineering\n\nLet's first combine the train and test sets to add features on both the data and conduct further analyses. We will split them later before Model Testing.","345da3d3":"Let's create a copy of our two datasets, so the changes we are gonna make forward does not affect the original data.","4e514245":"The outliers are now handled. Next, we will move on to handling the missing data, as we observed at the start of this notebook that MonthlyIncome and NumberOfDependents had null values.\n\n### Null Handling\n\n- Since MonthlyIncome is an integer value, we will replace the nulls with the median values.\n- Number of Dependents can be characterized as a categorical variable, hence if customers have NA for number of dependents, it means that they do not have any dependents. Therefore, we fill them with zeros.","ca9deb08":"**FEATURE IMPORTANCE**","a3238711":"Saving the best estimator from RandomSearchCV","b47495b0":"Performing similar analysis on the Test Data.","8551278f":"Adding some new features:\n\n- **MonthlyIncomePerPerson**: Monthly Income divided by the number of dependents\n\n- **MonthlyDebt**: Monthly Income multiplied by the Debt Ratio\n\n- **isRetired**: Person whose monthly income is 0 and age is greater than 65 (Assumed Retirement Age)\n\n- **RevolvingLines**: Difference between Number of Open Credit Lines and Loans and Number of Real Estate Lines and Loans\n\n- **hasRevolvingLines**: If RevolvingLines exists then 1 else 0\n\n- **hasMultipleRealEstates**: If the Number of Real Estates is greater than 2\n\n- **incomeDivByThousand**: Monthly Income divided by 1000. Fraud might be more likely for these or it might signal the person is in a new job and hasn\u2019t had a percent raise in pay yet. Both groups signal higher risk.","d323fc83":"Here if you see the difference between the 50th and 75 quantile for Revolving Utilization of Unsecured Lines, you'll observe that there is a massive increase from 13 to 1891.25. Since 13 seems like a reasonable ratio too (but way too high), let's check how many of these counts lie above 13.","1acc119b":"Performing a similar analysis on the Test Set.","9bd81df2":"### Outlier Analysis","1fd5ecc2":"# Exploratory Data Analysis\n\nLet's first try to identify the column by column datatypes and null values.","24bd8555":"## XGBoost\n\n**Hyperparameter Tuning**","35d1a822":"As you can see, our graphs look much better now.","11a04f2d":"## LightGBM\n\n**Hyperparameter Tuning**","fdfa67ee":"**Revolving Utilization of Unsecured Lines**\n\nThis field basically represents the ratio of the amount owed by the credit limit of a customer. A ratio higher than 1 is considered to be a serious defaulter. A Ratio of 10 functionally also seems possible, let's see how many of these customers have the Revolving Utilization of Unsecured Lines greater than 10.","32ac5dd2":"**ROC AUC**","581c504c":"**ROC AUC**"}}