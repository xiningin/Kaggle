{"cell_type":{"5dde2d59":"code","5dcc4516":"code","8ddfc5af":"code","244c0ea8":"code","38a0a769":"code","b2998501":"code","f4d47df1":"code","4317975a":"code","22e7662e":"code","03ac34e5":"code","0df33c36":"code","270b4bff":"code","634cb24f":"code","2057fada":"code","d854e40d":"code","c17e91d4":"code","adfbac8a":"code","6e60f6a4":"code","4e7ef457":"code","fe727eea":"code","ed0bdb79":"code","2f5ff8b0":"code","6a179d34":"code","628ecaf9":"code","d5e72359":"code","228aa449":"code","327a405b":"code","855da6d7":"code","cd19d619":"code","fd77f5d4":"code","506b939b":"code","308b4262":"code","8de3e44e":"code","fe99e7f5":"code","8a87ce54":"code","46b2ff38":"code","cfbd8d27":"code","43c9e61b":"code","717823fb":"code","73ed031b":"code","c4d1d607":"code","7b7bc063":"markdown","bf383d53":"markdown","11e2ba45":"markdown","1dd8a4e9":"markdown","b5c3507e":"markdown","cba89148":"markdown","ec3720ef":"markdown","7dad2220":"markdown","f302ce52":"markdown","3d62dca2":"markdown","b1fac7e4":"markdown","4928b3f1":"markdown","a870eff6":"markdown","2b146f3c":"markdown","96118da3":"markdown"},"source":{"5dde2d59":"%%time\n# Import the Rapids suite here - takes abot 2 mins\n\n#import sys\n#!cp ..\/input\/rapids\/rapids.0.17.0 \/opt\/conda\/envs\/rapids.tar.gz\n#!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n#sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n#sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\n#sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n#!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","5dcc4516":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport gc\nimport sys\nimport random\nfrom tqdm.notebook import * \nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport pandas as pd,numpy as np\n\n\nimport cudf\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.svm import SVC, SVR\nfrom cuml.neighbors import KNeighborsClassifier, NearestNeighbors\n\nfrom cuml.metrics import roc_auc_score\nfrom cuml.preprocessing import LabelEncoder\nfrom cuml.experimental.preprocessing import MinMaxScaler\nfrom cuml.linear_model import MBSGDClassifier as cumlMBSGDClassifier\nfrom cuml.naive_bayes import MultinomialNB\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 500)\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings('ignore')","8ddfc5af":"DATA_PATH = \"..\/input\/homedataformlcourse\/\"\n\nNA = -9999999999\n\nTARGET = 'SalePrice'\nKEY = 'Id'","244c0ea8":"os.listdir(DATA_PATH)","38a0a769":"def read(name):\n    df = pd.read_csv(DATA_PATH+f'{name}.csv')\n    try:\n        df = df.drop('Unnamed: 0',axis=1)\n    except:\n        pass\n    return df\n\n\n\ndef converte(df,cols):\n    for c in tqdm_notebook(cols):\n        if df[c].dtypes!='object':\n            df[c] = df[c].fillna(NA)\n            df[c] = df[c].astype(int)\n            df[c] = df[c].astype(str)\n            df.loc[df[c]==str(NA),c] = 'Missing'\n    return df\n\n\ndef cat_na(train,test,cols):\n    \n    for c in tqdm_notebook(cols):\n        diff = [x for x in test[c].unique() if x not in train[c].unique() if x==x]\n        if len(diff)>0:\n            test.loc[test[c].isin(diff),c] = 'Missing'\n        \n        train_val = train[c].value_counts()\n        train_val = train_val[train_val<=10].index.tolist()\n        \n        test_val = test[c].value_counts()\n        test_val = test_val[test_val<=10].index.tolist()\n        \n        diff =  [x for x in test_val if x in train_val]\n        test.loc[test[c].isin(diff),c] = 'Missing'\n        train.loc[train[c].isin(diff),c] = 'Missing' \n            \n    return test","b2998501":"train_df = read('train')\ntest_df = read('test')","f4d47df1":"corr = train_df.corr().stack().reset_index(name=\"correlation\")\ng = sns.relplot(\n    data=corr,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=10, sizes=(50, 250), size_norm=(-.2, .8))\ng.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(.02)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)\nfor artist in g.legend.legendHandles:\n    artist.set_edgecolor(\".7\") ","4317975a":"# Run only once\n#!pip install dabl","22e7662e":"import dabl\ndabl.plot(train_df,'SalePrice')","03ac34e5":"test_df.head()","0df33c36":"# Fill in the line below: get names of columns with missing values\ncols_with_missing = [col for col in test_df.columns\n                     if test_df[col].isnull().any()]\n# Fill in the lines below: drop columns in training and validation data\ntest_df.drop(cols_with_missing, axis=1,inplace=True)\ntrain_df.drop(cols_with_missing, axis=1,inplace=True)","270b4bff":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\n## Categorical features --\n#train_df.drop(,inplace=True)\ncategoricals = [cname for cname in train_df.columns if\n                    train_df[cname].nunique() < 50 and \n                    train_df[cname].dtype in ['object','bool']]\n                    #train_df[cname].dtype in ['object', 'bool']]\n## Numerical features:\n#numericals = [x for x in train_df.columns if x not in categoricals+[KEY,TARGET]]\nnumericals = [x for x in train_df.columns if train_df[x].dtype in ['int64','float64']\n             and x not in [KEY]]\n\nnumerical_transformer = SimpleImputer(strategy='constant')\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Final ensemble of features --\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numericals),\n        ('cat', categorical_transformer, categoricals)\n])","634cb24f":"for dv in [train_df,test_df]:\n    dv = converte(dv,categoricals)","2057fada":"test_df = cat_na(train_df,test_df,categoricals)","d854e40d":"data = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\ndata[categoricals] = data[categoricals].fillna('Missing')\ndata[numericals] = data[numericals].fillna(data[numericals].median())\ndata[categoricals] = data[categoricals].astype(str)","c17e91d4":"data","adfbac8a":"print(data[categoricals+numericals].isna().sum().sum())","6e60f6a4":"#data = pd.get_dummies(data[[TARGET,'Id']+categoricals+numericals])\ndata = pd.get_dummies(data[['Id']+numericals+categoricals])\ndata2 = pd.get_dummies(data[['Id']+numericals])","4e7ef457":"data.head()","fe727eea":"## Num + Cat \nfeatures = [x for x in data.columns if x not in [TARGET,'Id']]\n## Only Numerical for testing purposes\nfeatures2 = [x for x in data2.columns if x not in [TARGET,'Id']]","ed0bdb79":"len(features)","2f5ff8b0":"for c in tqdm(features):\n    data[c] = (data[c]-data[c].mean())\/data[c].std()","6a179d34":"data[features].isna().sum()","628ecaf9":"len(features)","d5e72359":"## Important\ntrain_df1=data[0:len(train_df.Id)-1].copy()\ntest_df1=data[len(train_df.Id):].copy()","228aa449":"def run_xgb(param,dtrain,dval,features,target,num_round,es):\n    \n    trn_data = xgb.DMatrix(dtrain[features], label=dtrain[target])\n    val_data = xgb.DMatrix(dval[features], label=dval[target])\n    \n    evallist = [(trn_data, 'train'),(val_data, 'validation')]\n    bst = xgb.train(param, trn_data, num_round, evallist,\n                            early_stopping_rounds=es,verbose_eval=10)\n    \n    pred_test = bst.predict(dtrain)\n    pred_oof = bst.predict(val_data)\n    \n    return pred_oof, pred_test","327a405b":"param_xgb = {\"random_stat\":0,\n        \"gpu_id\":-1,\n        \"predictor\":\"gpu_predictor\",\n        \"n_jobs\":0, \n        \"num_parallel_tree\":1,\n        \"colsample_bytree\":0.6,\n        \"max_depth\":2,\n        \"learning_rate\":0.02, \n        \"max_delta_step\":0,\n        \"subsample\":0.8,\n        \"eta\":0.05,\n        \"scale_pos_weight\":1,\n        \"tree_method\":\"gpu_hist\", \n        \"validate_parameters\":1,\n        \"objective\":\"reg:squarederror\",\n        \"verbosity\":None\n        }\n\nclass Config:\n    num_round = 10_000\n    es = 100\n    target = TARGET\n    features = features\n    \n    k = 10\n    random_state = 42\n    selected_folds = [0,1,2,3,4,5]\n    name = 'ml_rapids_'\n    param_xgb = param_xgb","855da6d7":"%%time\nimport xgboost as xgb\n# Compute shap values using GPU with xgboost\n\nparams1 = {\"random_stat\":0,\n        \"gpu_id\":-1,\n        \"predictor\":\"gpu_predictor\",\n        \"n_jobs\":0, \n        \"num_parallel_tree\":1,\n        \"colsample_bytree\":0.6,\n        \"max_depth\":7,\n        \"learning_rate\":0.02, \n        \"max_delta_step\":0,\n        \"subsample\":0.6,\n        \"eta\":0.5,\n        \"scale_pos_weight\":1,\n        \"tree_method\":\"gpu_hist\", \n        \"validate_parameters\":1,\n        \"verbosity\":None\n        }\n\nimport math\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n#### A function to calculate errors ####\ndef root_mean_squared_log_error(y_valid, y_preds):\n    \"\"\"Calculate root mean squared error of log(y_true) and log(y_pred)\"\"\"\n    if len(y_preds)!=len(y_valid): return 'error_mismatch'\n    y_preds_new = [math.log(x) for x in y_preds]\n    y_valid_new = [math.log(x) for x in y_valid]\n    return mean_squared_error(y_valid_new, y_preds_new, squared=False)\n#########################################","cd19d619":"# GPU accelerated training\nfrom sklearn.model_selection import train_test_split\ntarget=TARGET\nXT, XV, YT, YV = train_test_split(train_df1[features], \n                                  train_df1[target], random_state=0)\ndtrain = xgb.DMatrix(XT, \n                     label=YT, \n                     feature_names=features)\ndval = xgb.DMatrix(XV, \n                     label=YV, \n                     feature_names=features)\nwatchlist = [(dtrain, 'train'), (dval, 'val')]\n#evals=watchlist,early_stopping_rounds=2\n%time modelXGB = xgb.train(params1,dtrain,10)\n## Chosing the best features --\npredXGB = modelXGB.predict(dval)\nprint('RMSLE:', root_mean_squared_log_error(YV, predXGB))","fd77f5d4":"k=10\nnum_round = 10_000\nes = 100\ntarget = TARGET\nskf = StratifiedKFold(n_splits=k, random_state=42)\nsplit = list(skf.split(X=XT, y=YT))","506b939b":"for i, (train_idx, val_idx) in enumerate(split):\n        print(f\"\\n-------------   Fold {i + 1} \/ {k}  -------------\\n\")\n        \n        dtrain = train_df1.iloc[train_idx]\n        dval = train_df1.iloc[val_idx]\n        ####\n        dtrain = xgb.DMatrix(dtrain[features], \n                     label=dtrain[target], \n                     feature_names=features)\n        dval = xgb.DMatrix(dval[features], \n                     label=dval[target], \n                     feature_names=features)\n        \n        # Xgb\n        modelXGB = xgb.train(params1,dtrain,10)\n        oof_xgb,pred_xgb = modelXGB.predict(dval), modelXGB.predict(dtrain)\n        \n#        run_xgb(param_xgb,dtrain,dval,test_df1,\n#                                   features,target,num_round,es)","308b4262":"def run_xgb2(param,dtrain,dval,features,target,num_round,es):\n    \n    global models, train_scores\n    trn_data = xgb.DMatrix(dtrain[features], label=dtrain[target])\n    val_data = xgb.DMatrix(dval[features], label=dval[target])\n    \n    evallist = [(trn_data, 'train'),(val_data, 'validation')]\n    fitted_model = xgb.train(param, trn_data, num_round, evallist,\n                             early_stopping_rounds=es,verbose_eval=None)\n    models.append(fitted_model)\n    train_predictions = fitted_model.predict(val_data)\n    train_score = root_mean_squared_log_error(dval[target], \n                                              train_predictions)\n    train_scores.append(train_score)   \n    return train_score","8de3e44e":"%%time\n# collecting the fitted models and model performance\nmodels = []\ntrain_scores = []\nk=15\nnum_round = 15_000\nes = 100\ntarget = TARGET\nfeatures = features\nskf = StratifiedKFold(n_splits=k, random_state=42)\nsplit = list(skf.split(X=XT, y=YT))\n\n\nfor i, (train_idx, val_idx) in enumerate(split):\n        print(f\"\\n-------------   Fold {i + 1} \/ {k}  -------------\\n\")\n        \n        dtrain = train_df1.iloc[train_idx]\n        dval = train_df1.iloc[val_idx]\n        \n        # Xgb\n        results = run_xgb2(param_xgb,dtrain,dval,\n                                   features,target,num_round,es)","fe99e7f5":"train_scores","8a87ce54":"train_scores.index(min(train_scores))","46b2ff38":"train_scores.index(min(train_scores))\n# Extract the best model, based in the mean_squared_log_error\nIndexMin=train_scores.index(min(train_scores))\n# The model -\nbestModel=models[IndexMin]","cfbd8d27":"import plotly.express as px\nn_calls=len(train_scores)\nmetrics = pd.DataFrame(train_scores)\nmetrics.loc[:,'dataset'] = [\"train_score\"]*n_calls\nmetrics.loc[:,'Iteration Number'] = list(range(1,n_calls+1))\nmetrics.columns = [\"MSLE\", \"dataset\", \"Iteration Number\"]\nfig = px.line(metrics, x=\"Iteration Number\", y=\"MSLE\", color=\"dataset\")\nfig.show()","43c9e61b":"%%time\n# We can use the shap package\nimport shap\n\n# Compute shap values using GPU with xgboost\n# model.set_param({\"predictor\":\"cpu_predictor\"})\n# GPU accelerated training\ndtrainshap = xgb.DMatrix(train_df1[features], \n                     label=train_df1[target], \n                     feature_names=features)\n\nmodelXGB.set_param({\"predictor\": \"gpu_predictor\"})\nshap_values = modelXGB.predict(dtrainshap, pred_contribs=True)\n\n\n\n# shap will call the GPU accelerated version as long as the predictor parameter is set to \"gpu_predictor\"\n#modelXGB.set_param({\"predictor\": \"gpu_predictor\"})\nexplainer = shap.TreeExplainer(modelXGB)\nshap_values = explainer.shap_values(train_df1[features])\n\n# visualize the first prediction's explanation\nshap.force_plot(\n    explainer.expected_value,\n    shap_values[0, :],\n    train_df1[features].loc[0, :],\n    feature_names=features,\n    matplotlib=True\n)","717823fb":"# Show a summary of feature importance\nshap.summary_plot(shap_values, \n                  train_df1[features], \n                  plot_type=\"bar\",\n                  feature_names=features)","73ed031b":"len(features)","c4d1d607":"# make predictions which we will submit. \ntest_data = xgb.DMatrix(test_df1[features])\ntest_preds = bestModel.predict(test_data)\n#test_preds = modelXGB.predict(test_data)\n# The lines below shows how to save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': test_df.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submissionV1.csv', index=False)","7b7bc063":"# Some running on GPU","bf383d53":"# Thank you for the upvoting :)","11e2ba45":"# Feature importance:","1dd8a4e9":"Small modifications from several other's ideas.\n\n1. Removing missing data from the test data.\n2. Combining numerical + categorical data.\n3. Trying to implement some GPU facilities.","b5c3507e":"# Send to competition version 1","cba89148":"# Make the most of the GPU!","ec3720ef":"# Some exploration of the data","7dad2220":"# Finally writing down the test file","f302ce52":"# This is a notebook to send to the competition of housing (using GPUs)","3d62dca2":"# Data of the Housing competition","b1fac7e4":"# Preprocessing functions","4928b3f1":"**Checking...**","a870eff6":"# Time to run an example","2b146f3c":"Run only once to get the RAPIDs suite.","96118da3":"# After preprocessing, split again, train\/test"}}