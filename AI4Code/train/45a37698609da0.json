{"cell_type":{"25c67ea2":"code","07bab668":"code","188d97d4":"code","b1da09cf":"code","ad65414d":"code","44c8e644":"code","3552342e":"code","c5c42c30":"code","2c11012e":"code","d8a2068b":"code","e044b57e":"markdown","b5ba176d":"markdown","a2aab6b0":"markdown","4677eb0e":"markdown","5964811f":"markdown","9fb5eda1":"markdown","cfb39784":"markdown","f537cbd4":"markdown","de7cc07e":"markdown","62fa5863":"markdown"},"source":{"25c67ea2":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(42)\nX = 2 * np.random.rand(100,1)\nY = 10 + 4 * X+np.random.randn(100,1) \n\nplt.style.use('seaborn')\nplt.scatter(X, Y, color='black')\nplt.title('Training Data')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()","07bab668":"n = float(len(X)) # number of data pair\ninit_b = 0\ninit_m = 0\n\ndef compute_cost_function_point_by_point(m, b, X, Y):\n    totalError = 0\n    \n    for i in range(len(X)):\n        x = X[i,0]\n        y = Y[i,0]\n        totalError += (y - (m * x + b)) ** 2 \n        \n    return totalError \/ n\n\ncompute_cost_function_point_by_point(init_m,init_b,X,Y)","188d97d4":"learning_rate = 0.01","b1da09cf":"def compute_gradient_step_by_step(m,b,X,Y,learning_rate):\n    b_gradient = 0\n    m_gradient = 0\n    for i in range(len(X)):\n        x = X[i,0]\n        y = Y[i,0]\n        \n        b_gradient += -(2\/n) * (y - ((m * x) + b))\n        m_gradient += -(2\/n) * x * (y - ((m * x) + b))\n    new_b = b - (learning_rate * b_gradient)\n    new_m = m - (learning_rate * m_gradient)\n    return [new_b,new_m]","ad65414d":"num_iteration = 1000\nmin_step_size = 0.001","44c8e644":"def gradient_descent(X,Y,starting_b,starting_m,learning_rate, num_iteration):\n    b = starting_b\n    m = starting_m\n    i = 0\n    while i in range(num_iteration) or (b < min_step_size and m < min_step_size):\n        b, m = compute_gradient_step_by_step(m, b, X, Y, learning_rate)\n        i += 1\n    return [b,m]","3552342e":"def calculate_predicted_values(X, opt_b, opt_m):\n    return X*opt_m + opt_b","c5c42c30":"[g_b, g_m] = gradient_descent(X,Y,init_b,init_m,learning_rate, num_iteration)","2c11012e":"opt_X = calculate_predicted_values(X, g_b, g_m)","d8a2068b":"plt.style.use('seaborn')\nplt.scatter(X, Y, color='black')\nplt.plot(X, opt_X)\nplt.title('Linear Regression with Gradient Descent ')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.show()","e044b57e":"Let's start with the cost error function, and we will use the Mean Sum Error (MSE). Given a set of values $y_i$, and a set of predicted values $\\widehat{y_i}$.\nFirst we find the error of each data in the 2 sets ($y_i - \\widehat{y_i}$), sum up the differences and find the average.\n$$J_{m,b} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\widehat{y_i})^2 $$\n","b5ba176d":"## Understanding Gradient Descent with Linear Regression","a2aab6b0":"In this case, the predicted values $\\widehat{y_i}$ can be found using the linear regression model, and the MSE becomes:\n$$J_{m,b} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - (mx_i + b))^2 $$","4677eb0e":"And you can see that the result of the cost function is at ~195.1. It means that the error value of the predictive model is at ~195.1 when $m = 0$ and $b = 0$. Remember the goal of this experiment is to minimize this cost function. Now we don't know if it is the best estimation this model can do. So what do we do? We try with different parameter values.\nBut how would we do that?\n \nThis is when Gradient Descent and a bit of Calculus concept come in. One way to find any local minimal value is to find the point where the gradient converges. Since we have 2 parameters, we will take 2 partial derivatives.","5964811f":"Learning rate is a hyperparameter, usually ranged between 0 and 1, and it controls how quickly the model is adapted to the problem, or the step size of each iteration. The smaller the error is, the smaller step it will take toward convergence.","9fb5eda1":"Gradient Descent is a optimization algorithm that is used in most Machine Learning models. It starts with a set of initial parameter values and iteratively moving these set of parameter values that minimize a cost function. We achieve the best-fit parameter values by taking the partial derivatives of the parameters. \n\nWe can use Linear Regression to demonstrate how Gradient Descent can fit a line to a set of data by finding the optimal values (minimized cost function) for the 2 parameters: Intercept and the Slope. The linear regression predictive model is  \n\n$$ y = mx + b $$\n\nwhere m and b are the parameters.","cfb39784":"### A Linear Regression Example\nLet set up a pair of data sets (X,Y) and each set has 100 data points and it will look something like this:","f537cbd4":"$$\\frac{\\partial J_{m,b}}{\\partial m} = -\\frac{2}{N} \\sum_{i=1}^{N} x_i (y_i - (mx_i + b)) $$\n\n$$\\frac{\\partial J_{m,b}}{\\partial b} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (mx_i + b)) $$\n\nEach iteration, we take a step in the negative gradient and eventually we willa reach a point where it converges. \n\nLet look at a implementation below.","de7cc07e":"And it is common practice to run at least 1000 iteration or stop with the gradient step size is less than 0.001","62fa5863":"The following is MSE implementation when the parameter values are 0 (init_b, init_m). For demonstration, it is implemented iteratively using a loop."}}