{"cell_type":{"65604c0e":"code","ba8d86d0":"code","70ba57aa":"code","b1b68217":"code","57328538":"code","681d1a23":"code","3a0b1fab":"code","fad145fd":"code","b8da0bee":"code","d913cdea":"code","e4aeafba":"code","12477269":"code","8add0b37":"code","cbb78020":"code","290787b3":"code","1ead0d04":"code","4ee7a5f5":"code","5d4c63e4":"code","ae5f5c00":"code","4058feba":"code","392190bb":"code","a14a534a":"code","5d08c13c":"code","c92886f1":"code","43c279a8":"code","d8649e07":"code","e564dd9f":"code","a84ccd41":"code","4beff924":"code","685baaa4":"code","b0f1dba8":"code","7594cb51":"code","4990213a":"code","13dadab0":"code","74912059":"code","5f75169c":"code","2b23ace9":"code","f3eca3b8":"code","c33a5f11":"markdown","61d886c9":"markdown","52b1312e":"markdown","11b788ab":"markdown","1e9d21d6":"markdown","22a970bc":"markdown","47914ab2":"markdown","20386223":"markdown","07efdad7":"markdown","8ac298c1":"markdown","22d81fa8":"markdown","844760db":"markdown","2b085751":"markdown","49c1deab":"markdown","80266eb2":"markdown","0115ea17":"markdown","40a076f8":"markdown","93706888":"markdown","1d1785e6":"markdown","f7a21de8":"markdown","609a83dc":"markdown","2c84dbc5":"markdown","e4dd7d66":"markdown","a4ac685e":"markdown","48462dcb":"markdown","b4d56dfa":"markdown","61b56971":"markdown","be62ac4c":"markdown","29eda3e1":"markdown","a3e2f49e":"markdown","32fea0cc":"markdown","74a328d1":"markdown","a2f8adb3":"markdown","8f6410e2":"markdown","ab2c664e":"markdown","e5cf8b06":"markdown","afb0e56a":"markdown","a79cf6af":"markdown","6b347687":"markdown"},"source":{"65604c0e":"pip install -U scikit-learn","ba8d86d0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\nimport keras_tuner as kt","70ba57aa":"# Set Global Random Seed (to get stable results)\n\nfrom numpy.random import seed\nseed(42)\n\ntf.random.set_seed(42)","b1b68217":"# Read in source data\ndata = pd.read_csv(\"\/kaggle\/input\/beer-profile-and-ratings-data-set\/beer_profile_and_ratings.csv\")","57328538":"data.info()","681d1a23":"# Calculate number of unique beer with fewer than 25 ratings\nlen(data[data['number_of_reviews']<25])","3a0b1fab":"len(data[data['number_of_reviews']<25])\/len(data)","fad145fd":"test_data_temp = data[data['number_of_reviews']<25]\ntrain_data_temp = data.drop(test_data_temp.index, axis=0)\n\n\ntrain_data, test_data_additional = train_test_split(train_data_temp, test_size=0.1, random_state=1)\ntest_data = test_data_temp.append(test_data_additional)\n\n\nprint(\"# rows in training set:\", len(train_data), \"\\n\" \n      \"# rows in test set:\", len(test_data))","b8da0bee":"# Finalize training set\ndf_train = train_data.drop([\n    'Name',\n    'Brewery',\n    'Beer Name (Full)', \n    'Description',\n    'review_aroma', \n    'review_appearance', \n    'review_palate', \n    'review_taste', \n    'number_of_reviews'\n    ], axis=1)\n\n# Finalize test set\ndf_test = test_data.drop([\n    'Name',\n    'Brewery',\n    'Beer Name (Full)', \n    'Description',\n    'review_aroma', \n    'review_appearance', \n    'review_palate', \n    'review_taste', \n    'number_of_reviews'\n    ], axis=1)\n\n# Peak at finalized training set\ndf_train.head()","d913cdea":"# Features related to tasting profile\ntasting_profile_feat = ['Astringency', 'Body', 'Alcohol', 'Bitter', 'Sweet', \n                        'Sour', 'Salty', 'Fruits', 'Hoppy', 'Spices', 'Malty']\n\n\n# Scale feature values accross each row\ndef scale_profile_feat(df, tasting_profile_cols):\n    scaler = MinMaxScaler()\n    scaled_profile_feat = pd.DataFrame(scaler.fit_transform(df[tasting_profile_cols].T).T, columns=tasting_profile_cols)\n    scaled_profile_feat.index = df.index\n    df = df.drop(tasting_profile_cols, axis=1)\n    df = pd.concat([df, scaled_profile_feat], axis=1)\n    return df\n\ndf_train = scale_profile_feat(df_train, tasting_profile_feat)\ndf_test = scale_profile_feat(df_test, tasting_profile_feat)","e4aeafba":"# Use ordinal encoder to quickly encode categorical features\ndef label_encode(X_train, X_test=None):\n    if X_test is None:\n        # Make copy to avoid changing original data\n        X_train = X_train.copy()\n        # Ordinal encoding for categorical columns\n        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=X_train.select_dtypes([\"object\"]).nunique()[0])\n        for colname in X_train.select_dtypes([\"object\"]):\n            X_train[[colname]] = encoder.fit_transform(X_train[[colname]])\n            X_train[[colname]] = X_train[[colname]].astype('int64')\n        return X_train\n\n    else:\n        # Make copy to avoid changing original data\n        X_train = X_train.copy()\n        X_test = X_test.copy()\n        # Ordinal encoding for categorical columns\n        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=X_train.select_dtypes([\"object\"]).nunique()[0])\n        for colname in X_train.select_dtypes([\"object\"]):\n            X_train[[colname]] = encoder.fit_transform(X_train[[colname]])\n            X_train[[colname]] = X_train[[colname]].astype('int64')\n            X_test[[colname]] = encoder.transform(X_test[[colname]])\n            X_test[[colname]] = X_test[[colname]].astype('int64')\n        return X_train, X_test","12477269":"# Training set\nX = df_train.drop(['review_overall'], axis=1)\ny = df_train['review_overall']","8add0b37":"def get_cross_validation_score(X, y, model):\n    # Make copy to avoid changing original data\n    X = X.copy()\n    # Label encoding for categoricals\n    ## (Without determined train-test split, can possibly cause data leakage\n    ## causing lower score in test set later,\n    ## but ignore for now since only done for quick prototyping)\n    X = label_encode(X)\n    # Get cross_validation scores\n    log_y = np.log(y)\n    scores = cross_val_score(\n        model, X, log_y, cv=8, scoring='neg_mean_squared_error'\n    )\n    scores = np.mean(abs(scores))\n    scores = np.sqrt(scores)\n    return scores\n\n\nmodels = [DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), XGBRegressor()]\n\nmodel_scores = []\nfor model in models:\n    model_score = get_cross_validation_score(X, y, model)\n    model_scores.append(model_score)\n    print(model,'RMSE score:', model_score)\n    print('\\n')\n    \nprint(\"Best score (RMSE):\", min(model_scores))","cbb78020":"# Test set\nX_test = df_test.drop(['review_overall'], axis=1)\ny_test = df_test['review_overall']\n\n\n# Quick label encoding\nX, X_test = label_encode(X, X_test)\n\n\n# Use best model as baseline model\nbaseline_model = RandomForestRegressor(random_state=1)\nbaseline_model.fit(X, np.log(y))\ny_baseline_pred = np.exp(baseline_model.predict(X_test))\n\n\n# Evaluate baseline model prediction\ndef evaluate_model(y_test, y_pred):\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"MAE:\", mae, \"\\n\", \"MSE:\", mse, \"\\n\", \"RMSE:\", rmse)\n\nprint(\"Baseline model score:\")\nevaluate_model(y_test, y_baseline_pred)","290787b3":"def preprocess_data(X_train, X_test):\n    # Applying MinMaxScaler on numeric features\n    X_train, X_test = apply_scaler(X_train, X_test)\n    # Preprocessing categorical features\n    X_train, X_test = group_beer_style(X_train, X_test)\n    X_train, X_test = label_encode(X_train, X_test)\n    return X_train, X_test","1ead0d04":"# All numeric features\nfeatures_num = ['ABV', 'Min IBU', 'Max IBU', 'Astringency', 'Body', 'Alcohol',\n                'Bitter', 'Sweet', 'Sour', 'Salty', 'Fruits', 'Hoppy', 'Spices',\n                'Malty']\n\n\n# Categorical feature\nfeatures_cat = ['Style']","4ee7a5f5":"def apply_scaler(X_train, X_test):\n    scaler = MinMaxScaler()\n    X_train[features_num] = scaler.fit_transform(X_train[features_num]) \n    X_test[features_num] = scaler.transform(X_test[features_num])\n    return X_train, X_test","5d4c63e4":"def group_beer_style(X_train, X_test):\n    X_train['Style Group']=X_train['Style'].apply(lambda x: x.split('-')[0])\n    X_train = X_train.drop(['Style'], axis=1)\n    X_test['Style Group']=X_test['Style'].apply(lambda x: x.split('-')[0])\n    X_test = X_test.drop(['Style'], axis=1)\n    return X_train, X_test","ae5f5c00":"# Test set\nX_test = df_test.drop(['review_overall'], axis=1)\ny_test = df_test['review_overall']\n\n\n# Full training set\nX = df_train.drop(['review_overall'], axis=1)\ny = df_train['review_overall']\n\n\n# Data preprocessing\nX, X_test = preprocess_data(X, X_test)\n\n\n# Quick look at final training data\nX.head()","4058feba":"# Split validation set from training set\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=1)","392190bb":"# Training configuration\nBATCH_SIZE = 2 ** 8 #256\nEPOCHS = 350\n\n\n#Defining callbacks\nearly_stopping = callbacks.EarlyStopping(\n    patience=5, # how many epochs to wait before stopping\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    restore_best_weights=True\n)","a14a534a":"features_cat = ['Style Group']\n\n# Apply one hot encoding to categorical feature\ndef one_hot_encode(X_train, X_test):\n    # One-hot encoding for categorical columns\n    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    OH_cols_X_train = pd.DataFrame(encoder.fit_transform(X_train[features_cat]))\n    OH_cols_X_test = pd.DataFrame(encoder.transform(X_test[features_cat]))\n    # One-hot encoding removed index; put it back\n    OH_cols_X_train.index = X_train.index\n    OH_cols_X_test.index = X_test.index\n    # Remove categorical columns (will replace with one-hot encoding)\n    OH_X_train = X_train.drop(features_cat, axis=1)\n    OH_X_test = X_test.drop(features_cat, axis=1)\n    # Add one-hot encoded categorical columns to numerical features\n    OH_X_train = pd.concat([OH_X_train, OH_cols_X_train], axis=1)\n    OH_X_test = pd.concat([OH_X_test, OH_cols_X_test], axis=1)\n    return OH_X_train, OH_X_test\n\nOH_X_train, OH_X_val = one_hot_encode(X_train, X_val)","5d08c13c":"# Model Configuration\nUNITS = 2 ** 6 #64\nACTIVATION = 'relu'\nDROPOUT = 0.1\nOPTIMIZER = 'adam'","c92886f1":"# Define model\ninputs = keras.Input(shape=(OH_X_train.shape[1],))\nx = layers.Dense(UNITS, activation=ACTIVATION)(inputs)\nx = layers.Dense(UNITS, activation=ACTIVATION)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n# Compile model\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='mse',\n    metrics=['mae']\n)\n\n\n# Fit model (and save training history)\nhistory = model.fit(\n    OH_X_train, y_train,\n    validation_data = (OH_X_val, y_val),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n    verbose=0\n)\n\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot(title='MSE')\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot(title='MAE')\n\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation MAE: {:0.4f}\")\\\n      .format(history_frame['val_loss'].min(), \n              history_frame['val_mae'].min()))","43c279a8":"# Separate categorical features (for embedding) from numerical features\nX_train_style = pd.DataFrame(X_train.pop('Style Group'))\nX_val_style = pd.DataFrame(X_val.pop('Style Group'))","d8649e07":"# Define model with embedding layer\n\ncateg_input = keras.Input(shape=(1,), dtype='float32', name=\"style_group_input\")\nembedding = layers.Embedding(input_dim=44, output_dim=5, input_length=1)(categ_input)\ncateg_out = layers.Flatten()(embedding)\ncateg_output = layers.Dense(1, activation='relu', name=\"style_group_out\")(categ_out)\n\nnum_input = keras.Input(shape=(X_train.shape[1],), name=\"main_input\")\nx = keras.layers.concatenate([num_input, categ_output])\nx = layers.Dense(UNITS, activation=ACTIVATION)(x)\nx = layers.Dense(UNITS, activation=ACTIVATION)(x)\nnum_output = layers.Dense(1, name=\"main_output\")(x)\n\nmodel = keras.Model(\n    inputs=[num_input, categ_input], \n    outputs=[num_output, categ_output]\n)\n\n\n# Compile Model\nmodel .compile(\n    optimizer=OPTIMIZER,\n    loss='mse',\n    metrics=['mae'],\n)\n\n\n# Fit model\nhistory = model.fit(\n    [X_train, X_train_style],\n    [y_train, y_train],\n    validation_data = ([X_val,X_val_style], [y_val, y_val]),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n    verbose=0\n)\n\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n\n# Plot training history\nhistory_frame.loc[0:, ['main_output_loss', 'val_main_output_loss']].plot(title='MSE')\nhistory_frame.loc[0:, ['main_output_mae', 'val_main_output_mae']].plot(title='MAE')\n\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation MAE: {:0.4f}\")\\\n      .format(history_frame['val_main_output_loss'].min(), \n              history_frame['val_main_output_mae'].min()))","e564dd9f":"# Training configuration\nBATCH_SIZE = 2 ** 8 #256\nEPOCHS = 350\n\n\n#Defining callbacks\nearly_stopping = callbacks.EarlyStopping(\n    patience=5, # how many epochs to wait before stopping\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)","a84ccd41":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make","4beff924":"# Define model\n\ncateg_input = keras.Input(shape=(1,), dtype='float32', name=\"style_group_input\")\nembedding = layers.Embedding(input_dim=44, output_dim=5, input_length=1)(categ_input)\ncateg_out = layers.Flatten()(embedding)\ncateg_output = layers.Dense(1, activation='relu', name=\"style_group_out\")(categ_out)\n\nnum_input = keras.Input(shape=(X_train.shape[1],), name=\"main_input\")\nx = keras.layers.concatenate([num_input, categ_output])\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = layers.BatchNormalization()(x)\nnum_output = layers.Dense(1, name=\"main_output\")(x)\n\nmodel = keras.Model(\n    inputs=[num_input, categ_input], \n    outputs=[num_output, categ_output]\n)\n\n\n# Compile Model\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='mse',\n    metrics=['mae'],\n)\n\n\n# Fit model\nhistory = model.fit(\n    [X_train, X_train_style],\n    [y_train, y_train],\n    validation_data = ([X_val,X_val_style], [y_val, y_val]),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping, lr_schedule],\n    verbose=0\n)\n\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n\n# Plot training history\nhistory_frame.loc[0:, ['main_output_loss', 'val_main_output_loss']].plot(title='MSE')\nhistory_frame.loc[0:, ['main_output_mae', 'val_main_output_mae']].plot(title='MAE')\n\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation MAE: {:0.4f}\")\\\n      .format(history_frame['val_main_output_loss'].min(), \n              history_frame['val_main_output_mae'].min()))","685baaa4":"# A KerasTuner model-building function\n\ndef build_model(hp):\n    # Search for best model configuration\n    EMBEDDING_OUTPUT_DIM = hp.Int(name=\"EMBEDDING_OUTPUT_DIM\", min_value=2, max_value=6, step=1)\n    UNITS = hp.Int(name=\"UNITS\", min_value=58, max_value=64, step=2)\n    ACTIVATION = hp.Choice(name=\"ACTIVATION\", values=[\"relu\", \"tanh\"])\n    DROPOUT = hp.Float(name=\"DROPOUT\", min_value = 0.1, max_value=0.3)\n    OPTIMIZER = hp.Choice(name=\"OPTIMIZER\", values=[\"adam\", \"rmsprop\"])\n\n  \n    # Assemble model\n    categ_input = keras.Input(shape=(1,), dtype='float32', name=\"style_group_input\")\n    embedding = layers.Embedding(input_dim=44, output_dim=EMBEDDING_OUTPUT_DIM, input_length=1)(categ_input)\n    categ_out = layers.Flatten()(embedding)\n    categ_output = layers.Dense(1, activation='relu', name=\"style_group_out\")(categ_out)\n\n    num_input = keras.Input(shape=(X_train.shape[1],), name=\"main_input\")\n    x = keras.layers.concatenate([num_input, categ_output])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = layers.BatchNormalization()(x)\n    num_output = layers.Dense(1, name=\"main_output\")(x)\n\n    model = keras.Model(\n        inputs=[num_input, categ_input], \n        outputs=[num_output, categ_output]\n    )\n\n    # Compile Model\n    model.compile(\n        optimizer=OPTIMIZER,\n        loss='mse',\n        metrics=['mae']\n    )\n\n    return model","b0f1dba8":"# Define a tuner\n\ntuner = kt.BayesianOptimization(\n    build_model,\n    objective=kt.Objective(\"val_main_output_loss\", direction=\"min\"),\n    directory=\"my_dir\",\n    max_trials=50,\n    executions_per_trial=2,\n    overwrite=True\n)","7594cb51":"tuner.search_space_summary()","4990213a":"callbacks = [callbacks.EarlyStopping(\n    patience=5, # how many epochs to wait before stopping\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    restore_best_weights=True), \n    callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001)\n]","13dadab0":"# Takes a long time\ntuner.search(\n    [X_train, X_train_style],\n    [y_train, y_train],\n    validation_data = ([X_val,X_val_style], [y_val, y_val]),\n    batch_size= BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=2\n    )","74912059":"# Get the optimal hyperparameters (top 5)\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\ntop_n = 5\nbest_hps = tuner.get_best_hyperparameters(top_n)","5f75169c":"def get_best_epoch(hp):\n    model = build_model(hp)\n    callbacks = [keras.callbacks.EarlyStopping(\n        min_delta=0.001,\n        patience=10),\n        keras.callbacks.ReduceLROnPlateau(\n        patience=0,\n        factor=0.2,\n        min_lr=0.001)\n    ]\n    history = model.fit(\n        [X_train, X_train_style],\n        [y_train, y_train],\n        validation_data = ([X_val,X_val_style], [y_val, y_val]),\n        batch_size= BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=0,\n    )\n    val_loss_per_epoch = history.history[\"val_loss\"]\n    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n    print(f\"Best epoch: {best_epoch}\")\n    return best_epoch","2b23ace9":"# Separate categorical features (for embedding) from numerical features\nX_style = pd.DataFrame(X.pop('Style Group'))\nX_test_style = pd.DataFrame(X_test.pop('Style Group'))","f3eca3b8":"# Compare top 5 models after tuning\n\nfor hp in best_hps:\n    # Print model order\n    print(f\"Tuned Model No. {best_hps.index(hp)+1}\")\n\n    # Model Configuration\n    EMBEDDING_OUTPUT_DIM = hp.get('EMBEDDING_OUTPUT_DIM')\n    UNITS = hp.get('UNITS')\n    ACTIVATION = hp.get('ACTIVATION')\n    DROPOUT = hp.get('DROPOUT')\n    OPTIMIZER = hp.get('OPTIMIZER')\n\n    # Best training epoch\n    EPOCHS = get_best_epoch(hp)\n\n    # Build Model\n    categ_input = keras.Input(shape=(1,), dtype='float32', name=\"style_group_input\")\n    embedding = layers.Embedding(input_dim=44, output_dim=EMBEDDING_OUTPUT_DIM, input_length=1)(categ_input)\n    categ_out = layers.Flatten()(embedding)\n    categ_output = layers.Dense(1, activation='relu', name=\"style_group_out\")(categ_out)\n\n    num_input = keras.Input(shape=(X.shape[1],), name=\"main_input\")\n    x = keras.layers.concatenate([num_input, categ_output])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = layers.BatchNormalization()(x)\n    num_output = layers.Dense(1, name=\"main_output\")(x)\n\n    model = keras.Model(\n        inputs=[num_input, categ_input], \n        outputs=[num_output, categ_output]\n    )\n\n\n    # Compile Model\n    model.compile(\n        optimizer=OPTIMIZER,\n        loss='mse',\n        metrics=['mae'],\n    )\n\n\n    # Fit model (notice the increased number of epochs)\n    model.fit(\n        [X, X_style],\n        [y, y],\n        batch_size=BATCH_SIZE,\n        epochs=int(EPOCHS * 1.45),\n        verbose=0)\n\n    # Evaluate model\n    predictions = model.predict([X_test, X_test_style])\n    y_predicted = pd.DataFrame(predictions[0])\n    evaluate_model(y_test, y_predicted)\n    print(\"\\n\")","c33a5f11":"Our neural network will now receive two separate inputs: one is the beer style data (the categorical feature that which we will convert into an embedding), and the other is a vector of the rest of the features (all numeric features).","61d886c9":"Before we can train on the full training data, though, there\u2019s one last parameter we need to settle: the optimal number of epochs to train for. Typically, we want to train the new models for longer than we did during the search: using an aggressive patience value in the EarlyStopping callback saves time during the search, but it may lead to under-fit models--so we use the validation set to find the best epoch.","52b1312e":"*Have questions or comments? Share them on the comments section!*","11b788ab":"## Imports and Configuration","1e9d21d6":"## Optimize Model Architecture","22a970bc":"## Define Model","47914ab2":"## Pre-preprocessing","20386223":"## Load Preprocessed Data","07efdad7":"## Hyperparameter Tuning with KerasTuner","8ac298c1":"### Baseline NN","22d81fa8":"We have one categorical feature that describes the style of each beer. This feature has 111 unique levels, making it a high-cardinality feature. We're going to reduce the feature cardinality by grouping each style into 44 bigger style categories (different types of stouts will be labeled as \"Stout\", while different types of IPAs are simple labeled as \"IPA\", etc); thankfully, this is quite easy with the data we have:","844760db":"You can display an overview of the search space via `search_space_summary()`:","2b085751":"This notebook describes the process of creating a deep learning model that predicts the **overall review score** of different beers based on their specifications and tasting profiles. It covers the model construction process using Keras Functional API (including creating an embedding layer for categorical feature), and model hyperparameter-tuning using KerasTuner.\n\n*Note: This notebook attempts to solve a regression problem. If you're interested in learning how to build a content-based beer recommendation system, check out [this notebook](https:\/\/www.kaggle.com\/ruthgn\/creating-a-beer-recommender-deployment).*","49c1deab":"The next step is to define a \u201ctuner.\u201d Schematically, you can think of a tuner as a for loop that will repeatedly do the following:\n* Pick a set of hyperparameter values\n* Call the model-building function with these values to create a model\n* Train the model and record its metrics\n\nKerasTuner has several built-in tuners available\u2014RandomSearch, BayesianOptimization, and Hyperband. Let\u2019s try BayesianOptimization, a tuner that attempts to make smart predictions for which new hyperparameter values are likely to perform best given the outcomes of previous choices:","80266eb2":"## Introduction","0115ea17":"## Quick Prototyping","40a076f8":"## Scaling Numeric Features","93706888":"## Embedding Categorical data","1d1785e6":"## Dealing With Categorical Feature","f7a21de8":"Finally, let\u2019s launch the search.","609a83dc":"## Reviewing the Full Dataset","2c84dbc5":"### Scaling Values Across Tasting Profile Features","e4dd7d66":"One powerful technique for optimizing the performance of deep learning models for tabular data is to use embeddings for categorical variables. You can read more about this approach [in this article](https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/).\n\n>[Embeddings for categorical variables] allow for relationships between categories to be captured. [For example], [p]erhaps Saturday and Sunday have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status.\n\nNeural network embeddings overcome the limitations of two of the most common methods for representing categorical variables: ordinal encoding and one-hot encoding. The main problem with ordinal encoding is that sometimes it can bring a natural order on the different classes that doesn't translate into the real world (for instance, the algorithm thinks the class 1 is less important than the class 2, just because 2 is bigger than 1). On the other hand, one-hot encoding has two major drawbacks:\n- It could create too many columns for categorical variables with a lot of unique classes\u2014some algorithms aren\u2019t too good for handle this. The dimensionality of the transformed vector can easily become unmanageable.\n- The mapping is completely uninformed: \u201csimilar\u201d categories are not placed closer to each other in embedding space.","a4ac685e":"### Label Encoding for Categorical Feature","48462dcb":"# Part 4 - Predictions and Evaluation","b4d56dfa":"Finally, we are going to train on the full dataset for just a bit longer than this epoch count, since we're training on more data; roughly 45% more in our case:","61b56971":"# Part 1 - Preliminaries","be62ac4c":"Let's run the best model on the test set to get our baseline score.","29eda3e1":"### Establish Baseline","a3e2f49e":"When building a deep learning model, you have to make many seemingly arbitrary decisions: How many units or should go in each layer? Should you use relu as activation, or a different function? How much dropout should you use? And so on. These architecture-level parameters are called *hyperparameters* (to distinguish them from the parameters of a model, which are trained via backpropagation).\n\nKerasTuner lets you replace hard-coded hyperparameter values, such as units=32, with a range of possible choices, such as `Int(name=\"units\", min_value=58, max_value=64, step=2)`. This set of choices in a given model is called the search space of the hyperparameter tuning process. To specify a search space, define a model-building function (see the next cell). It takes an `hp` argument, from which you can sample hyperparameter ranges, and it\nreturns a compiled Keras model.","32fea0cc":"Our best deep learning model performs similarly (if not slightly better) compared to our baseline RandomForestRegressor model. \"Traditional\" methods like linear and tree models have been the \u201cgo to\u201d models for tabular data, but even when other shallow models come out slightly ahead in raw performance, where deep learning comes out ahead is flexibility--DL could tackle problems with tabular data that includes columns with unstructured text, for example, where traditional ML methods would require a lot more processing. For example, although in this particular analysis we did not use each beer's full label description listed in our source data's 'Description' column (because of missing entires), DL allows us to use the unstructured text from such column as an input feature without manual feature engineering--it\u2019s much easier to create a DL system that can handle a very broad range of tabular data.\n\nAdditionally, traditional ML model can require tons of feature engineering and tweaking, for instance, when the structure of you data table changes, while DL can produce an adequate model without so much feature engineering--this is another example for when DL may be better option even if its raw performance in terms of accuracy isn\u2019t as good as that of a popular ML algorithm like XGBoost.","74a328d1":"A good chunk of beers in our entire dataset have only been reviewed by fewer than 25 people--making their overall ratings score less of a reliable metric when compared to that in beers that have been reviewed by a lot more users. \n\nThese beers with `number_of_reviews` < 25 make good candidates for our test set for two reasons:\n- Training on them will introduce more noise to our model.\n- Predicted rating score generated by our upcoming model for these beers can potentially a better representation for the overall beer favorability, given the highly-sensitive (thus less reliable) nature of the original metric (`review_overall`) that was generated from a small and biased sample pool.\n\n\nNonetheless, we also want to add more diversity to our test set to be able to fairly evaluate our prediction model at the end. We will add more rows to our test set to finalize our train-test split:","a2f8adb3":"Our dataset contains 25 unique columns that include tasting profiles and and consumer reviews for 3197 unique beers from 934 different breweries.","8f6410e2":"## Create Training and Test Sets","ab2c664e":"_____\n\n## Acknowledgement\n\nSome text in the Hyperparameter Tuning section is copied from Fran\u00e7ois Chollet's book\u2014[Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python-second-edition).","e5cf8b06":"# Part 3 - Model Building","afb0e56a":"### Select Features\/Columns for Modelling","a79cf6af":"# Part 2 - Data Preprocessing for Neural Network","6b347687":"Later, we will create embeddings for this categorical variable to be processed by our neural network.\n\n*Note: Reducing the cardinality of categorical features is **not** a requirement for training an NN-based prediction model for tabular data that includes them. In fact, one of the advantages of deep learning for tabular data is that it allows us the use some techniques that deal with high-cardinality categorical features really well (resulting in predictive performance trumping that of tree-based ML models popular for structured data). In this particular analysis, we're arbitrarily reducing the cardinality of one categorical feature for purposes related to the intended use of the generated predictions (as seen in this [demo app](https:\/\/share.streamlit.io\/ruthgn\/beer-recommender\/main\/beer-recommender-app.py)--see available beer style options for user input).*"}}