{"cell_type":{"9d6b94c7":"code","65ebcc08":"code","d4e9b5d3":"code","49caeddb":"code","b767b2b6":"code","94898d2c":"code","039fca04":"code","7e06dd7e":"code","0fd74d89":"code","1d610384":"code","aa14e8ef":"code","66ce3520":"code","111d4152":"code","8abc9259":"code","e0b2654b":"code","e0fa670f":"code","8d30addd":"code","78c3775d":"code","1a1e6e1f":"code","1d2996d7":"code","2509b9de":"code","255eb0c3":"code","83ab6d4e":"code","86f7663d":"code","f9447104":"code","2cc59ffa":"code","d2c4e4ac":"code","dd454534":"code","59e79f0f":"code","075b5844":"code","e55f62cf":"code","8a8d6350":"markdown","d5487aad":"markdown","e2edd0fa":"markdown","78fd7552":"markdown","0a8a6ce8":"markdown","3161f130":"markdown","6ff7a73d":"markdown","81f6e6c8":"markdown","0fb92897":"markdown","841ac830":"markdown","7c15ceb8":"markdown","f23c2349":"markdown","fbe197d1":"markdown","d8eecac2":"markdown","563d5fe2":"markdown","7288b5e6":"markdown","cde80b27":"markdown","6e77007c":"markdown","49e379c5":"markdown","0fd873fb":"markdown","cb7b8415":"markdown","8c1e1976":"markdown","ea6d2e8c":"markdown","68f8c7b2":"markdown","70d7a688":"markdown","70d0a6fb":"markdown","39be388c":"markdown","01d2e1e9":"markdown","f31e2d80":"markdown","1e2b8279":"markdown","6c7d412c":"markdown","c44172ba":"markdown","4eaf5cc0":"markdown","0962ce42":"markdown","a2b89e2e":"markdown","807f48af":"markdown","f5d5b97d":"markdown","102f6885":"markdown","8065ef8b":"markdown","76927088":"markdown","673a7c89":"markdown","70dc01bf":"markdown","9c390c87":"markdown","8ba6c8e5":"markdown","bf070727":"markdown","80f6e5d5":"markdown","921ae391":"markdown","6aafdd2e":"markdown"},"source":{"9d6b94c7":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","65ebcc08":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","d4e9b5d3":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","49caeddb":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","b767b2b6":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","94898d2c":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","039fca04":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","7e06dd7e":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","0fd74d89":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","1d610384":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","aa14e8ef":"%%writefile memory_patterns.py\n\nimport random\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n\n\n# maximum steps in the pattern\nsteps_max = 3\n# minimum steps in the pattern\nsteps_min = 3\n# maximum amount of steps until reassessment of effectiveness of current memory patterns\nmax_steps_until_memory_reassessment = random.randint(80, 120)\n\n# current memory of the agent\ncurrent_memory = []\n# list of 1, 0 and -1 representing win, tie and lost results of the game respectively\n# length is max_steps_until_memory_reassessment\nresults = []\n# current best sum of results\nbest_sum_of_results = 0\n# how many times each action was performed by opponent\nopponent_actions_count = [0, 0, 0]\n# memory length of patterns in first group\n# steps_max is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = steps_max * 2\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(steps_max, steps_min - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    \n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    global results\n    global best_sum_of_results\n    # action of my_agent\n    my_action = None\n    \n    # if it's not first step, add opponent's last action to agent's current memory\n    # and reassess effectiveness of current memory patterns\n    if obs[\"step\"] > 0:\n        # count opponent's actions\n        opponent_actions_count[obs[\"lastOpponentAction\"]] += 1\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        results.append(get_step_result_for_my_agent(current_memory[-2], current_memory[-1]))\n        \n        # if there is enough steps added to results for memery reassessment\n        if len(results) == max_steps_until_memory_reassessment:\n            results_sum = sum(results)\n            # if effectiveness of current memory patterns has decreased significantly\n            if results_sum < (best_sum_of_results * 0.5):\n                # flush all current memory patterns\n                best_sum_of_results = 0\n                results = []\n                for group in groups_of_memory_patterns:\n                    group[\"memory_patterns\"] = []\n            else:\n                # if effectiveness of current memory patterns has increased\n                if results_sum > best_sum_of_results:\n                    best_sum_of_results = results_sum\n                del results[:1]\n    \n    # search for my_action in memory patterns\n    for group in groups_of_memory_patterns:\n        # if length of current memory is bigger than necessary for a new memory pattern\n        if len(current_memory) > group[\"memory_length\"]:\n            # get momory of the previous step\n            previous_step_memory = current_memory[:group[\"memory_length\"]]\n            previous_pattern = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n            if previous_pattern == None:\n                previous_pattern = {\n                    \"actions\": previous_step_memory.copy(),\n                    \"opp_next_actions\": [\n                        {\"action\": 0, \"amount\": 0, \"response\": 1},\n                        {\"action\": 1, \"amount\": 0, \"response\": 2},\n                        {\"action\": 2, \"amount\": 0, \"response\": 0}\n                    ]\n                }\n                group[\"memory_patterns\"].append(previous_pattern)\n            # if such pattern already exists\n            for action in previous_pattern[\"opp_next_actions\"]:\n                if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                    action[\"amount\"] += 1\n            # delete first two elements in current memory (actions of the oldest step in current memory)\n            del current_memory[:2]\n            \n            # if action was not yet found\n            if my_action == None:\n                pattern = find_pattern(group[\"memory_patterns\"], current_memory, group[\"memory_length\"])\n                # if appropriate pattern is found\n                if pattern != None:\n                    my_action_amount = 0\n                    for action in pattern[\"opp_next_actions\"]:\n                        # if this opponent's action occurred more times than currently chosen action\n                        # or, if it occured the same amount of times and this one is choosen randomly among them\n                        if (action[\"amount\"] > my_action_amount or\n                                (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                            my_action_amount = action[\"amount\"]\n                            my_action = action[\"response\"]\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n    \n    current_memory.append(my_action)\n    return my_action","66ce3520":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n        \n# looks for the same pattern in history and returns the best answer to the most possible counter strategy\nclass pattern_matching(agent):\n    def __init__(self, steps = 3, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        self.steps = steps\n        \n    def history_step(self, history):\n        if len(history) < self.steps + 1:\n            return self.initial_step()\n        \n        next_step_count = np.zeros(3) + self.init_value\n        pattern = [history[i][self.step_type] for i in range(- self.steps, 0)]\n        \n        for i in range(len(history) - self.steps):\n            next_step_count = (next_step_count - self.init_value)\/self.decay + self.init_value\n            current_pattern = [history[j][self.step_type] for j in range(i, i + self.steps)]\n            if np.sum([pattern[j] == current_pattern[j] for j in range(self.steps)]) == self.steps:\n                next_step_count[history[i + self.steps][self.step_type]] += 1\n        \n        if next_step_count.max() == self.init_value:\n            return self.initial_step()\n        \n        if  self.deterministic:\n            step = np.argmax(next_step_count)\n        else:\n            step = np.random.choice([0,1,2], p = next_step_count\/next_step_count.sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n        \n# if we add all agents the algorithm will spend more that 1 second on turn and will be invalidated\n# right now the agens are non optimal and the same computeations are repeated a lot of times\n# the approach can be optimised to run much faster\nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n    \n#     'random_transitison_matrix_decay2': transition_matrix(False, False, decay = 1.001),\n#     'random_self_trans_matrix_decay2': transition_matrix(False, True, decay = 1.001),\n#     'random_transitison_tensor_decay2': transition_tensor(False, False, decay = 1.001),\n#     'random_self_trans_tensor_decay2': transition_tensor(False, True, decay = 1.001),\n    \n#     'determenistic_transitison_matrix_decay2': transition_matrix(True, False, decay = 1.001),\n#     'determenistic_self_trans_matrix_decay2': transition_matrix(True, True, decay = 1.001),\n#     'determenistic_transitison_tensor_decay2': transition_tensor(True, False, decay = 1.001),\n#     'determenistic_self_trans_tensor_decay2': transition_tensor(True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_1': pattern_matching(1, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_1': pattern_matching(1, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_1': pattern_matching(1, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_1': pattern_matching(1, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_2': pattern_matching(2, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_2': pattern_matching(2, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_2': pattern_matching(2, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_2': pattern_matching(2, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_3': pattern_matching(3, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_3': pattern_matching(3, False, True, decay = 1.001),\n    'determenistic_pattern_matching_decay_3': pattern_matching(3, True, False, decay = 1.001),\n    'determenistic_self_pattern_matching_decay_3': pattern_matching(3, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_4': pattern_matching(4, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_4': pattern_matching(4, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_4': pattern_matching(4, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_4': pattern_matching(4, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_5': pattern_matching(5, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_5': pattern_matching(5, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_5': pattern_matching(5, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_5': pattern_matching(5, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_6': pattern_matching(6, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_6': pattern_matching(6, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_6': pattern_matching(6, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_6': pattern_matching(6, True, True, decay = 1.001),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 3 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    # load history\n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    # we can use it for analysis later\n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","111d4152":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","8abc9259":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","e0b2654b":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [1,2,0],\n    \"opponent\":   [0,1],\n    \"rotn\":       [0,1],\n}\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 1000, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions          = list(range(configuration.signs))  # [0,1,2]\n    last_action      = history['action'][-1]\n    prev_opp_action  = history['opponent'][-1]\n    opponent_action  = observation.lastOpponentAction if observation.step > 0 else 2\n    rotn             = (opponent_action - prev_opp_action) % configuration.signs\n\n    history['opponent'].append(opponent_action)\n    history['rotn'].append(rotn)\n    \n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency   = Counter(history['rotn'])\n    action_frequency = Counter(zip(history['action'], history['rotn'])) \n    move_weights     = [   move_frequency.get(n, 1) \n                         + action_frequency.get((last_action,n), 1) \n                         for n in range(configuration.signs) ] \n    guess            = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency  = Counter(zip(history['guess'], history['rotn']))\n    guess_weights    = [ guess_frequency.get((guess,n), 1) \n                         for n in range(configuration.signs) ]\n    prediction       = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    pred_frequency   = Counter(zip(history['prediction'], history['rotn']))\n    pred_weights     = [ pred_frequency.get((prediction,n), 1) \n                         for n in range(configuration.signs) ]\n    expected         = random.choices( population=actions, weights=pred_weights, k=1 )[0]\n\n    \n    # Slowly decay to 50% pure randomness as the match progresses\n    pure_random_chance = observation.step \/ (configuration.episodeSteps * 2)\n    if random.random() < pure_random_chance:\n        action = random.randint(0, configuration.signs-1)\n        is_pure_random_chance = True\n    else:\n        # Play the +1 counter move\n        # action = (expected + 1) % configuration.signs                  # without rotn\n        action = (opponent_action + expected + 1) % configuration.signs  # using   rotn\n        is_pure_random_chance = False\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('step                      = ', observation.step)\n    print('opponent_action           = ', opponent_action)\n    print('guess,      move_weights  = ', guess,      move_weights)\n    print('prediction, guess_weights = ', prediction, guess_weights)\n    print('expected,   pred_weights  = ', expected,   pred_weights)\n    print('action                    = ', action)\n    print('pure_random_chance        = ', f'{100*pure_random_chance:.2f}%', is_pure_random_chance)\n    print()\n    \n    return action","e0fa670f":"%%writefile high_performance_rps_dojo.py\n\n\nimport secrets\nimport math\nimport numpy as np\n\ndef get_score(S, A1, A2):\n    return (S + A1 - A2 + 1) % S - 1\n\nclass Submission:\n    K = 25\n    def __init__(self):\n        self.B = 0\n        # Jitter - steps before next non-random move\n        self.Jmax = 2\n        self.J2 = (self.Jmax+1)**2\n        self.J = int(math.sqrt(secrets.randbelow(self.J2)))\n        # Depth - number of previous steps taken into consideration\n        self.Dmin = 1\n        self.Dmax = 3\n        self.DL = self.Dmax-self.Dmin+1\n        self.HL = 3\n        self.HText = ['Opp',  'Me', 'Score']\n        self.Depth = np.arange(self.DL)\n        self.Hash = np.zeros((self.HL, self.DL), dtype=int)\n        self.G = 2\n        self.R = 0.4\n        self.RG = (1-self.R) * self.G\n        self.Threshold = 0.2\n        \n    def split_idx(self, idx):\n        d = idx % self.DL\n        idx \/\/= self.DL\n        h2 = idx % self.HL\n        idx \/\/= self.HL\n        h1 = idx % self.HL\n        idx \/\/= self.HL\n        return d, h1, h2, idx\n    \n    def next_action(self, T, A, S):\n        B, HL, DL, Dmin, Dmax = self.B, self.HL, self.DL, self.Dmin, self.Dmax\n        SD = S**self.DL\n        PR = None\n        if T == 0:\n            self.Map = np.zeros((S, SD**2, HL, HL, DL))\n            self.SList = np.arange(S)[:,None,None,None]\n            self.Predicts = np.full((HL, HL, DL), S, dtype=int)\n            self.Attempts = np.zeros((HL, HL, DL), dtype=int)\n            self.Scores = np.zeros((S, HL, HL, DL))\n            self.OrgID = np.ogrid[:S, :HL, :HL, :DL]\n            self.Hash2 = self.Hash[None,:] + SD*self.Hash[:,None]\n        else:\n            C = get_score(S, A, B) + 1\n            print(T, f'{B}-{A} {1-C}')\n            ABC = np.array([A, B, C])[:,None]\n            Depth, Hash, Hash2, Map, SList, OrgID, Predicts, Attempts, Scores = self.Depth, self.Hash, self.Hash2, self.Map, self.SList, self.OrgID, self.Predicts, self.Attempts, self.Scores\n            # Update Moves Map by previous move and previous Hash\n            Map *= 0.99\n            Map[OrgID[0], Hash2, OrgID[1], OrgID[2], OrgID[3]] += (T > Depth + Dmin) * (SList == A)\n            # Update Hash by previous move\n            Hash[:] \/\/= S\n            Hash[:] += ABC[:HL] * S**Depth\n            Hash2[:] = Hash[None,:] + SD*Hash[:,None]\n            \n            # Update prediction scores by previous move\n            PB = Predicts < S\n            Attempts[:] = Attempts + PB\n            Scores[:] += PB * get_score(S, Predicts + SList, A)\n            #print(T, Scores.T[0])\n            # Update prediction scores by previous move\n            PR = Map[OrgID[0], Hash2, OrgID[1], OrgID[2], OrgID[3]]\n            Sum = np.sum(PR, axis=0)\n            Predicts[:] = (np.max((Sum >= self.G) * (PR >= Sum * self.R + self.RG) * (SList + 1), axis=0) - 1) % (S + 1)\n\n        self.B = np.random.choice(S)\n        if self.J > 0:\n            self.J -= 1\n        else:\n            sc = np.where(self.Predicts < S, self.Scores \/ (self.Attempts + 5), 0).ravel()\n            idx = np.argmax(sc)\n            if sc[idx] > self.Threshold:\n                self.Scores.ravel()[idx] -= 1\/3\n                Raw = self.Predicts.ravel()\n                L = len(Raw)\n                p = None\n                s = 0\n                if PR is not None:\n                    p = PR.ravel().reshape((3,-1))[:, idx % L]\n                    s = np.sum(p)\n                if s > 0 and np.random.choice(3) > 0:\n                    p \/= s\n                    self.B = (np.random.choice(S, p=p) + idx \/\/ L) % S\n                    parts = self.split_idx(idx)\n                    print(T, f'Weighted {parts[0]+self.Dmin}: {self.HText[parts[1]]}-{self.HText[parts[2]]}+{parts[3]}', p, self.B)\n                else:\n                    self.B = (Raw[idx % L] + idx \/\/ L) % S\n                    parts = self.split_idx(idx)\n                    print(T, f'Direct {parts[0]+self.Dmin}: {self.HText[parts[1]]}-{self.HText[parts[2]]}+{parts[3]}', self.Scores.ravel()[idx], self.B)\n                self.J = int(math.sqrt(secrets.randbelow(self.J2)))\n        return self.B\n\nsubmission = Submission()\n\ndef agent(observation, configuration):\n    T = observation.step\n    A = observation.lastOpponentAction if T > 0 else None\n    S = configuration.signs\n    try:\n        return int(submission.next_action(T, A, S))\n    except Exception as e:\n        print(T, f'Failed', e)\n        return int(np.random.choice(S))","8d30addd":"%%writefile geometry.py\n\n\nimport operator\nimport numpy as np\nimport cmath\nfrom typing import List\nfrom collections import namedtuple\nimport traceback\nimport sys\n\n\nbasis = np.array(\n    [1, cmath.exp(2j * cmath.pi * 1 \/ 3), cmath.exp(2j * cmath.pi * 2 \/ 3)]\n)\n\n\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\n\ndef find_all_longest(seq, max_len=None) -> List[HistMatchResult]:\n    \"\"\"\n    Find all indices where end of `seq` matches some past.\n    \"\"\"\n    result = []\n\n    i_search_start = len(seq) - 2\n\n    while i_search_start > 0:\n        i_sub = -1\n        i_search = i_search_start\n        length = 0\n\n        while i_search >= 0 and seq[i_sub] == seq[i_search]:\n            length += 1\n            i_sub -= 1\n            i_search -= 1\n\n            if max_len is not None and length > max_len:\n                break\n\n        if length > 0:\n            result.append(HistMatchResult(i_search_start + 1, length))\n\n        i_search_start -= 1\n\n    result = sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\n    return result\n\n\ndef probs_to_complex(p):\n    return p @ basis\n\n\ndef _fix_probs(probs):\n    \"\"\"\n    Put probs back into triangle. Sometimes this happens due to rounding errors or if you\n    use complex numbers which are outside the triangle.\n    \"\"\"\n    if min(probs) < 0:\n        probs -= min(probs)\n\n    probs \/= sum(probs)\n\n    return probs\n\n\ndef complex_to_probs(z):\n    probs = (2 * (z * basis.conjugate()).real + 1) \/ 3\n    probs = _fix_probs(probs)\n    return probs\n\n\ndef z_from_action(action):\n    return basis[action]\n\n\ndef sample_from_z(z):\n    probs = complex_to_probs(z)\n    return np.random.choice(3, p=probs)\n\n\ndef bound(z):\n    return probs_to_complex(complex_to_probs(z))\n\n\ndef norm(z):\n    return bound(z \/ abs(z))\n\n\nclass Pred:\n    def __init__(self, *, alpha):\n        self.offset = 0\n        self.alpha = alpha\n        self.last_feat = None\n\n    def train(self, target):\n        if self.last_feat is not None:\n            offset = target * self.last_feat.conjugate()   # fixed\n\n            self.offset = (1 - self.alpha) * self.offset + self.alpha * offset\n\n    def predict(self, feat):\n        \"\"\"\n        feat is an arbitrary feature with a probability on 0,1,2\n        anything which could be useful anchor to start with some kind of sensible direction\n        \"\"\"\n        feat = norm(feat)\n\n        # offset = mean(target - feat)\n        # so here we see something like: result = feat + mean(target - feat)\n        # which seem natural and accounts for the correlation between target and feat\n        # all RPSContest bots do no more than that, just in a hidden way\n        \n        result = feat * self.offset\n\n        self.last_feat = feat\n\n        return result\n    \n    \nclass BaseAgent:\n    def __init__(self):\n        self.my_hist = []\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.outcome_hist = []\n        self.step = None\n\n    def __call__(self, obs, conf):\n        try:\n            if obs.step == 0:\n                action = np.random.choice(3)\n                self.my_hist.append(action)\n                return action\n\n            self.step = obs.step\n\n            opp = int(obs.lastOpponentAction)\n            my = self.my_hist[-1]\n\n            self.my_opp_hist.append((my, opp))\n            self.opp_hist.append(opp)\n\n            outcome = {0: 0, 1: 1, 2: -1}[(my - opp) % 3]\n            self.outcome_hist.append(outcome)\n\n            action = self.action()\n\n            self.my_hist.append(action)\n\n            return action\n        except Exception:\n            traceback.print_exc(file=sys.stderr)\n            raise\n\n    def action(self):\n        pass\n\n\nclass Agent(BaseAgent):\n    def __init__(self, alpha=0.01):\n        super().__init__()\n\n        self.predictor = Pred(alpha=alpha)\n\n    def action(self):\n        self.train()\n\n        pred = self.preds()\n\n        return_action = sample_from_z(pred)\n\n        return return_action\n\n    def train(self):\n        last_beat_opp = z_from_action((self.opp_hist[-1] + 1) % 3)\n        self.predictor.train(last_beat_opp)\n\n    def preds(self):\n        hist_match = find_all_longest(self.my_opp_hist, max_len=20)\n\n        if not hist_match:\n             return 0\n\n        feat = z_from_action(self.opp_hist[hist_match[0].idx])\n\n        pred = self.predictor.predict(feat)\n\n        return pred\n    \n    \nagent = Agent()\n\n\ndef call_agent(obs, conf):\n    return agent(obs, conf)","78c3775d":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","1a1e6e1f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make, evaluate","1d2996d7":"env = make(\n    \"rps\", \n    configuration={\n        \"episodeSteps\": 1000\n    }\n)","2509b9de":"# Battle example: invert_my_last_action vs copy_opponent_agent\nenv.run(\n    [\"statistical_prediction.py\", \"hit_the_last_own_action.py\"]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","255eb0c3":"# Battle example: decision_tree_classifier vs decision_tree_classifier\nenv.run(\n    [\"decision_tree_classifier.py\", \"decision_tree_classifier.py\"]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","83ab6d4e":"evaluate(\n    \"rps\", \n    [\"rock.py\", \"scissors.py\"], \n    configuration={\"episodeSteps\": 1000}\n)","86f7663d":"evaluate(\n    \"rps\", \n    [\"geometry.py\", \"decision_tree_classifier.py\"], \n    configuration={\"episodeSteps\": 1000}\n)","f9447104":"list_names = [\n    \"rock\", \n    \"paper\", \n    \"scissors\",\n    \"hit_the_last_own_action\",  \n    \"copy_opponent\", \n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\",\n    \"statistical_prediction\",\n    \"high_performance_rps_dojo\",\n    \"geometry\",\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nn_agents = len(list_names)\n\nscores = np.zeros((n_agents, n_agents), dtype=np.int)","2cc59ffa":"print(\"Simulation of battles. It can take some time...\")\n\nfor ind_agent_1 in range(len(list_names)):\n    for ind_agent_2 in range(ind_agent_1 + 1, len(list_names)):\n        print(\n            f\"LOG: {list_names[ind_agent_1]} vs {list_names[ind_agent_2]}\", \n            end=\"\\r\"\n        )\n        \n        current_score = evaluate(\n            \"rps\", \n            [list_agents[ind_agent_1], list_agents[ind_agent_2]], \n            configuration={\"episodeSteps\": 1000}\n        )\n        \n        scores[ind_agent_1, ind_agent_2] = current_score[0][0]\n        scores[ind_agent_2, ind_agent_1] = current_score[0][1]\n    \n    print()","d2c4e4ac":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=list_names,\n)\n\n\nplt.figure(figsize=(9, 9))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap='coolwarm', linewidths=1, \n    linecolor='black', fmt=\"d\",\n)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","dd454534":"%%writefile your_agent.py\n\n# Example of the simple agent\ndef your_agent(observation, configuration):\n    return 0","59e79f0f":"scores = np.zeros((len(list_names), 1), dtype=int)","075b5844":"for ind_agent_1 in range(len(list_names)):\n\n    current_score = evaluate(\n        \"rps\", \n        [\"your_agent.py\", list_agents[ind_agent_1]], \n        configuration={\"episodeSteps\": 1000}\n    )\n\n    scores[ind_agent_1, 0] = current_score[0][0]","e55f62cf":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=[\"your_agent\"],\n)\n\n\nplt.figure(figsize=(2, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap=\"coolwarm\", linewidths=1, linecolor=\"black\", \n    fmt=\"d\", vmin=-500, vmax=500,\n)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(rotation=360, fontsize=15);","8a8d6350":"Copy from kernel [Rock, Paper, Scissors with Memory Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns)","d5487aad":"<a id=\"102\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Test Your Own Agent<center><h2>\n","e2edd0fa":"We need to import the library for creating environments and simulating agent battles","78fd7552":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","0a8a6ce8":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Paper action","3161f130":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py    \n\nAlways uses Rock action","6ff7a73d":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns<center><h2>\n\n\n","81f6e6c8":"Copy from kernel [Decision Tree Classifier](https:\/\/www.kaggle.com\/alexandersamarin\/decision-tree-classifier?scriptVersionId=46415861)","0fb92897":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nCopy the last action of the opponent","841ac830":"Copy from kernel [Rock Paper Scissors - Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction)","7c15ceb8":"# Rock Paper Scissors - Agents Comparison \n\nThis notebook contains a lot of different agents from different sources.    \nIn the **Comparison In Battle** section, we also added a comparison in a fair battle of each agent with each in 1 round with 1000 steps.","f23c2349":"Copy from kernel [(Not so) Markov \u26d3\ufe0f](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov)","fbe197d1":"<a id=\"9\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>\n\n\n","d8eecac2":"Copy from kernel [Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy)\n\nNash Equilibrium Strategy (always random)","563d5fe2":"<a id=\"14\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Decision Tree Classifier<center><h2>\n\n","7288b5e6":"<a id=\"10\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Markov Agent<center><h2>\n\n\n","cde80b27":"<a id=\"7\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Counter Reactionary<center><h2>\n\n","6e77007c":"<a id=\"101\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Comparison In Battle<center><h2>\n","49e379c5":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Scissors action","0fd873fb":"Copy from kernel [High-performance RPS Dojo](https:\/\/www.kaggle.com\/elvenmonk\/high-performance-rps-dojo\/)","cb7b8415":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22838\/logos\/header.png?t=2020-11-02-21-55-44)","8c1e1976":"<a id=\"15\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical Prediction<center><h2>\n\n","ea6d2e8c":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nHit the last action of the opponent","68f8c7b2":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","70d7a688":"<a id=\"5\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Copy Opponent<center><h2>","70d0a6fb":"<a id=\"8\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical<center><h2>\n\n\n","39be388c":"Copy from kernel [RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix)","01d2e1e9":"<a id=\"12\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>","f31e2d80":"<a id=\"16\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: High Performance RPS Dojo<center><h2>\n\n\n","1e2b8279":"Get score for 2 agents in the battle","6c7d412c":"<a id=\"17\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Geometry<center><h2>\n\n\n","c44172ba":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)","4eaf5cc0":"Copy from kernel [RPS Geometry \ud83e\udd87](https:\/\/www.kaggle.com\/superant\/rps-geometry?scriptVersionId=51928653)","0962ce42":"Simulating battles. It can take some time","a2b89e2e":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation","807f48af":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Markov Agent](#10)\n* [Agent: Memory Patterns](#11)\n* [Agent: Multi Armed Bandit](#12)\n* [Agent: Opponent Transition Matrix](#13)\n* [Agent: Decision Tree Classifier](#14)\n* [Agent: Statistical Prediction](#15)\n* [Agent: High Performance RPS Dojo](#16)\n* [Agent: Geometry](#17)\n\n    \n* [Example Using The kaggle_environments For Testing Agents](#100)\n* [Comparison In Battle](#101)\n* [Test Your Own Agent](#102)","f5d5b97d":"<a id=\"3\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Paper<center><h2>","102f6885":"Copy from kernel [Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents)","8065ef8b":"<a id=\"2\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Rock<center><h2>","76927088":"<a id=\"100\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Example Using The kaggle_environments For Testing Agents<center><h2>","673a7c89":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>","70dc01bf":"Create the array for battle results","9c390c87":"Visualize the result","8ba6c8e5":"Let's start simulating the battle invert_my_last_action vs copy_opponent_agent","bf070727":"<a id=\"4\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Scissors<center><h2>","80f6e5d5":"Create your agent in the cell below, and save it to the **your_agent.py** file, and your agent will fight against all existing agents in this kernel","921ae391":"<a id=\"13\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Opponent Transition Matrix<center><h2>\n","6aafdd2e":"<a id=\"6\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Reactionary<center><h2>\n"}}