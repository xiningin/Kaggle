{"cell_type":{"c87c2937":"code","47138efa":"code","e4bec143":"code","75acbf21":"code","a7ad1a79":"code","8df52d94":"code","e0e76e3f":"code","56351cc1":"code","6da35426":"code","c1931dc7":"code","c17e99b9":"code","66f79765":"code","5861812d":"code","6b2bba4f":"code","f76c70f4":"code","4853c751":"code","23b9964d":"code","228a4240":"code","2f73c40a":"code","e1819160":"code","0ef9121b":"code","284dc83c":"code","c1cbcd0c":"code","1af243a9":"code","aedb6d07":"code","04e6a952":"code","decfd872":"code","9a9b929c":"code","743e3e9e":"code","d5e24fa6":"code","485dcacd":"code","9d00a75b":"code","0b5cd0c5":"code","08049b30":"code","b1367a77":"code","e88f7eae":"code","ba76df17":"code","e6a10210":"code","3fb51ebd":"code","cbf7aa29":"code","a4238257":"code","6baaf10f":"markdown","cfe1e750":"markdown","8ac4beda":"markdown","9c7bcc62":"markdown","6f265f26":"markdown","9ac5c4c0":"markdown","4282efd1":"markdown","b6923dd8":"markdown","61461741":"markdown","7eb9520e":"markdown","73bba471":"markdown","585e968d":"markdown","b2325ee0":"markdown","8a1dd269":"markdown","2f939e0d":"markdown","35ef09a1":"markdown"},"source":{"c87c2937":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47138efa":"import nltk\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom datetime import datetime\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as L\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import load_model\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint  \nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.models import Sequential\nimport matplotlib.pyplot as plt\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom tqdm import tqdm","e4bec143":"nltk.download('stopwords')\n","75acbf21":"df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","a7ad1a79":"df.isnull().sum()\n","8df52d94":"df.head()\n","e0e76e3f":"df['sentiment'].value_counts()\n","56351cc1":"def encode_sentiment(sentiment) -> int:\n  encoder = {\"positive\" : 1 , \"negative\": 0 ,\"Positive\":1 , \"Negative\" : 0}\n  if sentiment!=None:\n    return encoder[sentiment]\n","6da35426":"def decode_sentiment(sentiment) -> str:\n  decoder = {1: \"positive\" , 0: \"negative\"}\n  if sentiment!=None:\n    return decoder[sentiment]","c1931dc7":"train_df = df\n","c17e99b9":"\ntrain_df['new_sentiment'] = train_df['sentiment'].apply(encode_sentiment)","66f79765":"train_df = train_df.sample(frac=1)\n","5861812d":"def process_text(text) -> str:\n  #Convert string \n  process_text = str(text)\n  #Convert string to lower\n  process_text = process_text.lower()\n  #Removing html tags\n  process_text = re.sub(\"<.*?>\",\" \",process_text)\n  #Removing all digits and having only letters\n  process_text = re.sub(\"[^a-zA-Z]\",\" \",process_text)\n  #Removing all stop words\n  process_text = process_text.split(\" \")\n  process_text = \" \".join([word for word in process_text if word not in stopwords.words(\"english\")])\n\n  return process_text","6b2bba4f":"tqdm.pandas()\n","f76c70f4":"train_df['processed_text'] = train_df['review'].progress_apply(process_text)","4853c751":"positive_sentiment = train_df.loc[train_df['new_sentiment']==1]\nnegative_sentiment = train_df.loc[train_df['new_sentiment']==0]","23b9964d":"positive_sentiment.head()\n","228a4240":"positive_string = ' '.join([word for word in positive_sentiment['processed_text'].values])\n# Display the generated image:\nwordcloud = WordCloud().generate(positive_string)","2f73c40a":"plt.figure(figsize=(10,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","e1819160":"negative_string = ' '.join([word for word in negative_sentiment['processed_text'].values])\n# Display the generated image:\nnegative = WordCloud().generate(negative_string)","0ef9121b":"plt.figure(figsize=(10,8))\nplt.imshow(negative, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","284dc83c":"train = train_df[['processed_text','new_sentiment']]\n","c1cbcd0c":"text = train['processed_text'].values\nsentiment = train['new_sentiment'].values","1af243a9":"tokenizer = Tokenizer()\n","aedb6d07":"tokenizer.fit_on_texts(text)","04e6a952":"print(f\"Total vocab :  {len(tokenizer.word_index)+1}\")","decfd872":"sequence_text = tokenizer.texts_to_sequences(text)\n","9a9b929c":"sequence_text = pad_sequences(sequence_text,padding=\"post\")","743e3e9e":"x_train , x_test , y_train ,y_test = train_test_split(sequence_text,sentiment,\n                                                      test_size=0.3,random_state=11)","d5e24fa6":"\nprint(f\"\"\"\nX Train :{x_train.shape}\nX TEST : {x_test.shape}\nY TRAIN : {y_train.shape}\nY TEST  : {y_test.shape}\n\"\"\")\n","485dcacd":"VOCAB_SIZE = len(tokenizer.word_index)+1\nEMBEDDING_VEC = 30","9d00a75b":"model = tf.keras.Sequential([\n    L.Embedding(VOCAB_SIZE,EMBEDDING_VEC, input_length=x_train.shape[1]),\n    L.Bidirectional(L.LSTM(128,return_sequences=True)),\n    L.GlobalMaxPool1D(),\n    L.Dropout(0.4),\n    L.Dense(128, activation=\"relu\"),\n    L.Dropout(0.4),\n    L.Dense(2)\n    ])\nmodel.compile(loss=SparseCategoricalCrossentropy(from_logits=True),\n              optimizer='adam',metrics=['accuracy']\n             )","0b5cd0c5":"model.fit(x_train,y_train , validation_data=(x_test,y_test),batch_size=32\n          ,epochs=2)","08049b30":"predictions = model.predict_classes(x_test)","b1367a77":"print(f\"The accuracy of the model is : {accuracy_score(y_test,predictions)*100}%\")","e88f7eae":"plt.figure(figsize=(10,8))\nsns.heatmap(confusion_matrix(y_test,predictions),annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")","ba76df17":"print(classification_report(y_test,predictions))","e6a10210":"decoded = [decode_sentiment(i) for i in predictions]","3fb51ebd":"sns.countplot(decoded)\nplt.title(\"Total number of positive and negative predictions over 15000 samples\")","cbf7aa29":"sample_data = [[\"\"\"I know everyone wants to compare this to the Animated version, but don't. Take it as it comes and you will thoroughly enjoy it. It does stay pretty faithful to the animated version I think. Will Smith as the genie could never be the Robyn Williams genie, but I don't think he tries to. He does fantastically well in his own right. Absolutely loved the Prince Ali song where Aladdin enters the city as the prince. Brilliantly colorful spectacle captured really well. Jafar missed a little for me as had lost the smarmy-ness of the animated version. The songs were great and the Aladdin and Jasmine characterization was pretty spot on. \nI think kids would love this and I would definitely recommend it. \"\"\"],\n              [ \"\"\"\n               Everything felts small and Bollywood, for being the middle east. felt like I was watching a b film remake of Arabian Nights, not Aladdin from the Disney family. \n               way to kill a childhood of mine...save your money for Lion King.\n               \"\"\"],\n               [\"\"\"\n                This movie sucks\n               \"\"\"],\n               ]","a4238257":"for i in sample_data:\n    process = process_text(i)\n    process = tokenizer.texts_to_sequences([process])\n    process = pad_sequences(process,padding='post')\n    prediction = int(model.predict_classes(process))\n    print(f\"Predicted : {decode_sentiment(prediction)}\")","6baaf10f":"<h2>Encoder \/ Decoder function<\/h2>","cfe1e750":"<h2>Making sample predictions<\/h2>","8ac4beda":"<h2>Tokenize<\/h2>\n\nWe will now tokenize the words and fit them on to the text. The tokenizer will assign an index to every word. In which the sentences will be soon replaced by the index when we convert them into sequence","9c7bcc62":"<h3>For positive review<\/h3>","6f265f26":"In this kernel we will be predicting the sentiment of the text using word embedding's & LSTM. The dataset contains over 50,000 reviews out of which 25,000 of them are positive & 25,000 are negative. \n\nAlso , Check out the django application of this dataset in my [Github](https:\/\/github.com\/ratheeshaditya\/IMDB-Sentiment-Prediction)\n\nThe notebook will be divided into these sections , \n\n* Importing the required library's\n* Loading up the data & understanding it\n* Creating encoder & decoder function's for our labels\n* Process the text\n* Analyze the common words used in both positive and negative review's\n* Tokenize the text and convert the text into sequence of integers\n* Build the model\n* Evaluation of the model\n* Make prediction's on newer\/custom text","9ac5c4c0":"<h2>Making predictions<\/h2>","4282efd1":"<h2>Import the required library's<\/h2>","b6923dd8":"As we can see our model is prone to predicting more false Positives than false negatives , but overall it has made a good balance between the two classes! Meaning our model makes more 'Positive' predictions on the review's than negatives","61461741":"<h2>Building the model<\/h2>","7eb9520e":"<h2>Word cloud<\/h2>","73bba471":"<h3>For negative review<\/h3>","585e968d":"<h2>Preprocessing the text<\/h2>","b2325ee0":"For this dataset , we will be \n\n*  Converting the string to lower case\n*  Since HTML tags are present in the dataset we will create a regex to remove that as well\n*  Removing all the special characters & digits present , (removed the html tags first so that as a group , the tags are removed and the noise of the text will be reduced , or else we will be left with unwanted tags without the '< >' \n*  Removing stopwords\n","8a1dd269":"Converting the text into series of integers (basically replaced by the index of the tokenized words)","2f939e0d":"<h2>Confusion Matrix<\/h2>","35ef09a1":"<h2>Overall prediction of the classes<\/h2>"}}