{"cell_type":{"c53b15bc":"code","fa61a741":"code","e6d89531":"code","3105a8ef":"code","1ee47aba":"code","3506735b":"code","c4cf515b":"code","77bb3e0a":"code","e7178228":"code","89f2750d":"code","a9bcc6b6":"code","4e693616":"markdown","fb0b7f51":"markdown","9d95126a":"markdown","c3277c43":"markdown","ab508c9e":"markdown","0733c1e4":"markdown","02926311":"markdown","43a03060":"markdown","eba6ddf2":"markdown","02b875e5":"markdown","6f2b8bb4":"markdown","c5cbe34d":"markdown"},"source":{"c53b15bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa61a741":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ncolors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])","e6d89531":"from sklearn.datasets import make_classification\nX, y = make_classification(n_classes=2, class_sep=0.5,\nweights=[0.05, 0.95], n_informative=2, n_redundant=0, flip_y=0,\nn_features=2, n_clusters_per_class=1, n_samples=1000, random_state=10)","3105a8ef":"plt.gca().set_title('data')\nplt.scatter(X[:,0], X[:,1],c=vectorizer(y))","1ee47aba":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","3506735b":"np.bincount(y_train)","c4cf515b":"from sklearn.linear_model import LogisticRegression\n#Initalize the classifier\nclf = LogisticRegression(random_state=0)\n#Fitting the training data\nclf.fit(X_train, y_train)\n#Predicting on test\ny_pred=clf.predict(X_test)","77bb3e0a":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","e7178228":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X_train, y_train)","89f2750d":"np.bincount(y_res)","a9bcc6b6":"from sklearn.metrics import classification_report\nclf1 = LogisticRegression(random_state=0)\n#Fitting the training data\nclf1.fit(X_res, y_res)\n#Predicting on test\ny_pred1=clf1.predict(X_test)\nprint(confusion_matrix(y_test, y_pred1))\nprint(classification_report(y_test, y_pred1))","4e693616":"### Sampling_strategy\n* float sampling_strategy='auto', an indicator how much majority\/minority mix will be created, applicable for binary classification\n*  specify the class targeted by the resampling\n\n### k_neighbors\n* number of nearest neighbours to used to construct synthetic samples\n\n","fb0b7f51":"![image.png](attachment:image.png)","9d95126a":"![image.png](attachment:image.png)","c3277c43":"* Often the distribution of the classes are skewed\n* For a binary classification, we typically call the one class occuring more as the majority class and the other as the minority or rare class.  Example: Fraud Detection\n* The classifier ends up being dominated by the majority class\n* The baseline methods are oversampling the minority class or undersampling the majority class","ab508c9e":"## Class Imbalance","0733c1e4":"![image.png](attachment:image.png)","02926311":"## Creating a sample dataset","43a03060":"## Visualize the data","eba6ddf2":"## Fitting a logistic classifier","02b875e5":"## Regular housekeeping splitting in train,test","6f2b8bb4":"## Some other strategies are \n* ADASYN\n* BorderLine SMOTE\n* KMeansSMOTE","c5cbe34d":"## Basic Housekeeping"}}