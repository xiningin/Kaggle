{"cell_type":{"96b8159f":"code","00798091":"code","a0805c9e":"code","56899c03":"code","8b5ef2db":"code","ef727fbf":"code","d45ac95e":"code","022e0a1c":"code","65a2176a":"code","3b7148af":"code","24ad1298":"code","7a60f1b9":"markdown","3e5b1a8c":"markdown","b13676ec":"markdown","493a3a91":"markdown","f7d443aa":"markdown"},"source":{"96b8159f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","00798091":"!pip install praw","a0805c9e":"!pip install config","56899c03":"import praw # Import PRAW - python reddit API wrapper\nimport config\nimport time\nimport csv\nimport random\nimport os\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud","8b5ef2db":"# fetch the comments from a subreddit\n\ndef bot_login():\n    print(\"Logging in...\")\n    r = praw.Reddit(username = config.username,  #config.py is the sperate file that consist your username, password, client id and client secret through reddit API\n    password = config.password,\n    client_id = config.client_id,\n    client_secret = config.client_secret,\n    user_agent = \"you can write any version here\")\n    print(\"Logged in!\")\n\n    return r\n\ndef top_post_from_subreddit(r):\n    top_post=[]\n    for submission in r.subreddit('india').top(limit=20):  # write Indiaspeaks when you want to run for that subreddit\n        top_post.append(submission.title)\n        for i in range(0,20):\n            try:\n                print(submission.comments[i].body)\n                with open(\"comments.txt\",\"a\") as f:    # Save the comments in the file\n                    f.write(submission.comments[i].body+\"\\n\\n\")\n            except UnicodeEncodeError:\n                pass\n    return top_post\n\n\nr = bot_login()\nposts = top_post_from_subreddit(r)\nprint(posts)\n","ef727fbf":"# Defining a function to clean up the text\n\nimport re\n\ndef clean(text):\n    sms = re.sub('[^a-zA-Z]',' ',text) #clearing all the non-alphanumeric text\n    sms = sms.lower() #converting the text into lower text\n    sms = sms.split() #spliting them into words\n    sms = ' '.join(sms) #join them by space\n    return sms","d45ac95e":"def removestopwords(text): # remove all the stopwords from the text\n    text = text.split()\n    return [words for words in text if words not in stopwords.words('english')]","022e0a1c":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n# lemmatize string\ndef lemmatize_word(text): \n    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n    return lemmas","65a2176a":"file = []\nwith open('comments.txt','r') as f:\n    print(type(f))\n    for line in f:\n        line = line.strip() #remove the newline from the file\n        file.append(line)\n    \ntest = clean(str(file))\nprint(test)    \n\ntest2 = removestopwords(test)\nprint(test2)\n\ntest3 = lemmatize_word(test2)\ntest3 = \" \".join(str(x) for x in test3)\nprint(test3)","3b7148af":"with open('comments_final.txt','a') as f: # save the final comments in file\n    f.write(test3)","24ad1298":"dataset = open(\"comments_final.txt\", \"r\").read()\n\ndef create_word_cloud(string):\n    word_cloud2 = WordCloud(width=1600, height=800,collocations = False, background_color = 'white').generate(string)\n    # Display the generated Word Cloud\n\n    plt.imshow(word_cloud2)\n    plt.axis(\"off\")\n    plt.show()\n\n\ncreate_word_cloud(dataset)  ","7a60f1b9":"#### Process of collecting Data :\n1. I collected the top 20 comments from All time top 20 post from both of these subreddits. \n2. Then I preprocess the data by removing unwanted comments e.g. bot comments, mod comments, unwanted links, deleted comments, spams etc. \n3. Further refine the comments by removing the stop words such as 'I','Me','My','Our' etc which are pretty common and doesn't add much to the information. \n4. Lemmatization of words. ex - gone, going, went -> go. \n5. Save the final comments in the file. \n6. created the word cloud of the words. \n","3e5b1a8c":"This project of about the sentiment Analysis of 2 of the famous subreddit related to India of opposite ideologies.<br>\n<a href='https:\/\/www.reddit.com\/r\/india\/'> r\/india <\/a> which is known as a left wing, and <a href='https:\/\/www.reddit.com\/r\/IndiaSpeaks\/'> r\/IndiaSpeaks<\/a> which is known as Right wing","b13676ec":"#### End result - \n\nIf you want to look at better resolution image here they are - \n<a href=\"https:\/\/drive.google.com\/file\/d\/17f63CAKcRH14yMbFei91vuIhftxRAQ4T\/view?usp=sharing\"> r\/india <\/a>\n<a href=\"https:\/\/drive.google.com\/file\/d\/11p8QWzlNWXn89yBF24C-4F4rTZBMcS6c\/view?usp=sharing\"> r\/IndiaSpeaks <\/a>\n\n\n![WhatsApp Image 2021-12-08 at 5.51.10 PM.jpeg](attachment:34c2cc71-0292-40af-81a3-f9bbc0335db1.jpeg)","493a3a91":"## This project is still a Work in Progress!\n**Thank you for reading my notebook! This is my first notebook post. Let me know if you have some questions or any error you found in my code :)**","f7d443aa":"# Sentiment Analysis of Two of the Famous subreddit of India."}}