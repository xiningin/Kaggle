{"cell_type":{"4e10473b":"code","ed830fe3":"code","a40c90b2":"code","219cbfa1":"code","b7a43ff2":"code","0463d78d":"code","4c030e2e":"code","bf6d4daf":"code","33902454":"code","819147fc":"code","96f8b4ee":"code","e12e3bb0":"code","e3bbe19d":"code","22f18e1d":"code","e52f7081":"code","a47bab00":"code","ca7ff3e2":"code","3545ed57":"code","688d0367":"code","82b77945":"code","5d699479":"code","48c7d312":"code","179eb3d6":"code","abd057a5":"code","499ef6bf":"code","4ee11e4e":"code","fdae0b1f":"code","14559516":"code","37da4fc3":"code","e978e9d6":"code","fb9c2c58":"code","28d148c8":"code","f2e4653a":"code","7e506566":"code","30f0faa9":"markdown","d2c93d33":"markdown","ed23894a":"markdown","169c589c":"markdown","b6bea68b":"markdown"},"source":{"4e10473b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\npd.options.display.max_colwidth = 200\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed830fe3":"os.listdir('\/kaggle\/input\/nlp-getting-started')","a40c90b2":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","219cbfa1":"train.head()","b7a43ff2":"train.shape","0463d78d":"test.shape","4c030e2e":"def missing_data(df):\n    total = df.isnull().sum()\n    percentage = round(total \/ df.shape[0] *100)\n    missing = pd.concat([total, percentage], axis=1, keys= ['Total', 'Percent']).sort_values(by='Percent', ascending = False)\n    missing = missing[missing['Total'] > 0]\n    return missing","bf6d4daf":"missing_data(train)","33902454":"train.info()","819147fc":"train.keyword.value_counts().sum()","96f8b4ee":"wpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef normalize_document(doc):\n    # lower case and remove special characters\\whitespaces\n    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = wpt.tokenize(doc)\n    # filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = ' '.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)","e12e3bb0":"norm_corpus = normalize_corpus(train['text'])","e3bbe19d":"norm_corpus_test = normalize_corpus(test['text'])","22f18e1d":"# from sklearn.feature_extraction.text import CountVectorizer\n# # CountVectorizer(ngram_range=(2,2))\n# cv = CountVectorizer(min_df=0., max_df=1.)\n# cv_matrix = cv.fit_transform(norm_corpus)\n# cv_matrix = cv_matrix.toarray()\n# cv_matrix","e52f7081":"# cv_matrix_test = cv.transform(norm_corpus_test)\n\n# cv_matrix_test = cv_matrix_test.toarray()","a47bab00":"# # get all unique words in the corpus\n# vocab = cv.get_feature_names()\n# # show document feature vectors\n# pd.DataFrame(cv_matrix, columns=vocab)","ca7ff3e2":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n# tv_matrix = tv.fit_transform(norm_corpus)\n# tv_matrix = tv_matrix.toarray()\n\n# vocab = tv.get_feature_names()\n# pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)","3545ed57":"# tv_matrix_test = tv.transform(norm_corpus_test)\n# tv_matrix_test = tv_matrix_test.toarray()\n\n# vocab = tv.get_feature_names()\n# pd.DataFrame(np.round(tv_matrix_test, 2), columns=vocab)","688d0367":"\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n# tv_matrix = tv.fit_transform(norm_corpus)\n# tv_matrix = tv_matrix.toarray()","82b77945":"# clf = linear_model.RidgeClassifier()","5d699479":"\n# from sklearn.metrics.pairwise import cosine_similarity\n\n# similarity_matrix = cosine_similarity(tv_matrix)\n# similarity_df = pd.DataFrame(similarity_matrix)\n# similarity_df","48c7d312":"# scores = model_selection.cross_val_score(clf, tv_matrix, train[\"target\"], cv=3, scoring=\"f1\")\n# scores","179eb3d6":"# clf.fit(tv_matrix, train[\"target\"])","abd057a5":"# sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","499ef6bf":"# sample_submission[\"target\"] = clf.predict(tv_matrix_test)","4ee11e4e":"# sample_submission.to_csv(\"submission4.csv\", index=False)","fdae0b1f":"from nltk.corpus import gutenberg\nfrom string import punctuation\n\n# bible = gutenberg.sents('bible-kjv.txt') \nbible = norm_corpus\n# remove_terms = punctuation + '0123456789'\n\n# norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n# norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n# norm_bible = filter(None, normalize_corpus(norm_bible))\nnorm_bible = [tok_sent for tok_sent in bible if len(tok_sent.split()) > 2]\n\nprint('Total lines:', len(bible))\nprint('\\nSample line:', bible[10])\nprint('\\nProcessed line:', norm_bible[10])","14559516":"from keras.preprocessing import text\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(norm_bible)\nword2id = tokenizer.word_index\n\n# build vocabulary of unique words\nword2id['PAD'] = 0\nid2word = {v:k for k, v in word2id.items()}\nwids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n\nvocab_size = len(word2id)\nembed_size = 100\nwindow_size = 2 # context window size\n\nprint('Vocabulary Size:', vocab_size)\nprint('Vocabulary Sample:', list(word2id.items())[:10])","37da4fc3":"def generate_context_word_pairs(corpus, window_size, vocab_size):\n    context_length = window_size*2\n    for words in corpus:\n        sentence_length = len(words)\n        for index, word in enumerate(words):\n            context_words = []\n            label_word   = []            \n            start = index - window_size\n            end = index + window_size + 1\n            \n            context_words.append([words[i] \n                                 for i in range(start, end) \n                                 if 0 <= i < sentence_length \n                                 and i != index])\n            label_word.append(word)\n\n            x = sequence.pad_sequences(context_words, maxlen=context_length)\n            y = np_utils.to_categorical(label_word, vocab_size)\n            yield (x, y)\n            \n            \n# Test this out for some samples\ni = 0\nfor x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n    if 0 not in x[0]:\n        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n    \n        if i == 10:\n            break\n        i += 1","e978e9d6":"\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\n\n# build CBOW architecture\ncbow = Sequential()\ncbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\ncbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\ncbow.add(Dense(vocab_size, activation='softmax'))\ncbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n# view model summary\nprint(cbow.summary())\n\n# visualize model structure\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n                 rankdir='TB').create(prog='dot', format='svg'))","fb9c2c58":"for epoch in range(1, 6):\n    loss = 0.\n    i = 0\n    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n        i += 1\n        loss += cbow.train_on_batch(x, y)\n        if i % 100000 == 0:\n            print('Processed {} (context, word) pairs'.format(i))\n\n    print('Epoch:', epoch, '\\tLoss:', loss)\n    print()","28d148c8":"weights = cbow.get_weights()[0]\nweights = weights[1:]\nprint(weights.shape)\n\n# pd.DataFrame(weights, index=list(id2word.values())[1:]).head()","f2e4653a":"train.sample(20)","7e506566":"from sklearn.metrics.pairwise import euclidean_distances\n\n# compute pairwise distance matrix\ndistance_matrix = euclidean_distances(weights)\nprint(distance_matrix.shape)\n\n# view contextually similar words\nsimilar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n                   for search_term in ['disaster', 'earthquake', 'flood', 'fire', 'hurricane', 'bombing', 'crime','crash']}\n\nsimilar_words","30f0faa9":" ## Continuous Bag of Words","d2c93d33":"# Remove Punctuation, Stopwords","ed23894a":"## TFIDF","169c589c":"## Word2Vec","b6bea68b":"## Update logs of the Kernel\n\n* evaluation through f1-score\n* Created a quick and dirty model on first try with just BOW (CountVectorizer) and fed it to RidgeClassifier, got me around 0.78 accuracy\n* On second try, removed punctuation, lower the words, filtered stop words, then used CountVectorizer to created vector and then fed it to RidgeClassifier, performed a bit bad compared to first try with 0.77 accuracy. Why?\n* On third try using bi-grams with uni-grams improved it a bit, to .79, jumped from 1200s to 651. can't increase n grams, making the model complex.\n* Using TFIDF helped, submission scored 0.79957, which is an improvement of your previous score of 0.79926. 635 from 651\n* bi-grams with TFIDF got worse. uni-gram + bi-gram took a toll on kaggle processor.\n* Testing how can I implement context based methods to predict the labels "}}