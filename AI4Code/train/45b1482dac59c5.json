{"cell_type":{"3908c6eb":"code","419f215e":"code","9a917285":"code","6082396d":"code","1eecebea":"code","7b472ccb":"code","52286dac":"code","294f0c10":"code","336a2115":"code","efe267cc":"code","e6211c18":"code","dd3ec2a5":"code","b3509f7f":"code","84c86a04":"code","7b5faa54":"code","08b58dd9":"code","1d03e890":"code","6431568c":"code","62ecac76":"code","e02ef599":"code","24489f4d":"code","0f1f9c7b":"code","7d8be3c7":"code","11cbb704":"code","c95f5b87":"code","2c1fdf57":"code","6481969f":"code","b4b9a4d2":"code","e68963da":"code","6ea1bcf9":"markdown","7f6caa80":"markdown","c944df3f":"markdown","54f0aa47":"markdown","db7c6f3f":"markdown","ec0e0231":"markdown","0079d3f8":"markdown","71080f04":"markdown","b861f38c":"markdown","05586259":"markdown","765cb3f3":"markdown","0af12429":"markdown"},"source":{"3908c6eb":"# from IPython.display import clear_output\n!pip install -q -U autokeras\n# clear_output()\n\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\n\n# from category_encoders import MEstimateEncoder\n# from sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\n# from sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nimport autokeras as ak\nimport tensorflow as tf\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","419f215e":"# -----------------------------------------------------------------\n# Some parameters to config \nMAX_TRIAL = 3 # speed trial any%\nEPOCHS = 37\n\n# not used\nBATCH_SIZE = 2048 # large enough to fit RAM\nACTIVATION = 'swish'\nLEARNING_RATE = 0.000965713 # Optimal lr is about half the maximum lr\nLR_FACTOR = 0.5 # LEARNING_RATE * LR_FACTOR = New Learning rate on ReduceLROnPlateau\nES_PATIENCE = 10\nRLRP_PATIENCE = 5\nDROPOUT = 0.15\n\nRANDOM_STATE = 31\nVERBOSE = 1\n\n# The dataset is too huge for free contrainer. Sampling it for more fun!\nSAMPLE = 11426 # [1468136, 2262087, 195712, 377, 1, 11426, 62261] # 4000000 total rows\nVALIDATION_SPLIT = 0.2\n\n# Admin\nID = \"Id\" # Id id x X index\nINPUT = \"..\/input\/tabular-playground-series-dec-2021\"","9a917285":"def load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=ID)\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=ID)\n    # Merge the splits so we can process them together\n#     df = pd.concat([df_train, df_test])\n    # Preprocessing\n#     df = clean(df)\n#     df = encode(df)\n    df_train = impute(df_train)\n    df_test = impute(df_test)\n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    # Reform splits\n#     df_train = df.loc[df_train.index, :]\n#     df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n","6082396d":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","1eecebea":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","7b472ccb":"df_train, df_test = load_data()","52286dac":"# Peek at the values\ndisplay(df_train)\n# display(df_test)\n\n# Display information about dtypes and missing values\n# display(df_train.info())\n# display(df_test.info())","294f0c10":"target_col = df_train.columns.difference(df_test.columns)[0]\nX_raw = df_train.drop(columns=target_col)\ny_raw = df_train[target_col]\n\nX_test_raw = df_test.iloc[:,:]\ntarget_col","336a2115":"from sklearn.model_selection import train_test_split\n# Check NA\nmissing_val = X_raw.isnull().sum()\nprint(missing_val[missing_val > 0])\n\n# For small testing batch\n# X_raw, x_val, y, y_val = train_test_split(X_raw, y, train_size = VALIDATION_SPLIT, random_state = RANDOM_STATE)\n# X_raw = X_raw.sample(n=SAMPLE, random_state=RANDOM_STATE)\n# y_raw = y_raw.sample(n=SAMPLE, random_state=RANDOM_STATE)\n# x_test = x_test.sample(n=SAMPLE, random_state=RANDOM_STATE)","efe267cc":"sampling_key, sampling_count = np.unique(y_raw, return_counts=True)\nsampling_count[sampling_count > SAMPLE] = SAMPLE\nzip_iterator = zip(sampling_key, sampling_count)\nsampling_params = dict(zip_iterator)\n\nundersample = RandomUnderSampler(\n    sampling_strategy=sampling_params)\n\nX_raw, y_raw = undersample.fit_resample(X_raw, y_raw)","e6211c18":"np.unique(y_raw, return_counts=True)","dd3ec2a5":"transformer_all_cols = make_pipeline(\n    StandardScaler(),\n    MinMaxScaler(feature_range=(0, 1))\n)\n\npreprocessor = make_column_transformer(\n    (transformer_all_cols, df_test.columns[:]),\n)","b3509f7f":"X_train = preprocessor.fit_transform(X_raw)\nX_test = preprocessor.transform(X_test_raw)","84c86a04":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(y_raw)\n# y_train = y_raw","7b5faa54":"import gc\ngc.collect()","08b58dd9":"# Search for the best model with EarlyStopping.\ncbs = [\n    tf.keras.callbacks.EarlyStopping(\n                                    patience=ES_PATIENCE,\n                                    min_delta=0,\n                                    monitor='val_accuracy',\n                                    mode='max',\n                                    restore_best_weights=True,       \n                                    baseline=None,\n                                    verbose=VERBOSE,\n                                    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n                                    factor=LR_FACTOR,\n                                    patience=RLRP_PATIENCE,\n                                    monitor='val_loss',\n                                    mode='min',\n                                    verbose=VERBOSE,\n                                    )\n]","1d03e890":"# Initialize the structured data classifier.\nclf = ak.StructuredDataClassifier(\n    overwrite=False, max_trials=MAX_TRIAL,seed=RANDOM_STATE,\n#     optimizer= 'adam',\n#     loss='categorical_crossentropy',\n#     metrics=['accuracy'],\n)  # It tries MAX_TRIAL different models.\n# Feed the structured data classifier with training data.\nhistory = clf.fit(\n                X_train,\n                y_train,\n                # Split the training data and use the last 15% as validation data.\n                validation_split=VALIDATION_SPLIT,\n                epochs=EPOCHS,\n                callbacks=cbs,\n                verbose=VERBOSE,\n)\n\n","6431568c":"history1 = clf.evaluate(X_train, y_train)","62ecac76":"history1","e02ef599":"model = clf.export_model()\nmodel.summary()","24489f4d":"# Predict with the best model.\npredicted_y = clf.predict(X_test)\n# predicted_y = le.inverse_transform(clf.predict(X_test))\n","0f1f9c7b":"# Auto Keras converted y from int to string bug\npredicted_y = le.inverse_transform(predicted_y.astype(np.int16))","7d8be3c7":"# output = pd.DataFrame({ID: df_test.index, target_col: predicted_y[:,0]})\n\noutput = pd.read_csv(INPUT + \"\/sample_submission.csv\")\noutput[target_col] = predicted_y\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\noutput","11cbb704":"# history.history","c95f5b87":"# summarize history for accuracy, loss\nplt.plot(history.history['accuracy'], label='acc (training data)')\nplt.plot(history.history['loss'], label='loss (training data)')\n# plt.plot(history.history['val_acc'])\nplt.title('model accuracy\/loss')\n# plt.ylabel('acc\/loss')\nplt.xlabel('epoch')\n# plt.legend(['acc', 'loss'], loc='upper left')\nplt.show()\n","2c1fdf57":"# history.history","6481969f":"predicted_y","b4b9a4d2":"np.unique(output[target_col], return_counts=True)","e68963da":"# Plot the distribution of the test predictions\nfig, ax =plt.subplots(1,2,figsize=(10,4))\nsns.countplot(x=predicted_y, ax=ax[0], orient=\"h\").set_title(\"Prediction\")\n# Plot the distribution of the training set\nsns.countplot(x = df_train[target_col], ax=ax[1], orient=\"h\").set_title(\"Training labels\")\nfig.show()","6ea1bcf9":"# Train Model and Create Submissions #\n\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file","7f6caa80":"## Load Data ##\n\nAnd now we can call the data loader and get the processed data splits:","c944df3f":"To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/notebooks) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","54f0aa47":"# Hyperparameter Tuning #\n\nAt this stage, you might like to do auto hyperparameter tuning with AutoKeras before creating your final submission.\nAutoKeras: An AutoML system based on Keras. It is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible to everyone.\n\nBy default, AutoKeras use the last 20% of training data as validation data. As shown in the example below, you can use validation_split to specify the percentage.","db7c6f3f":"### Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.","ec0e0231":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The data we used in the course was a bit simpler than the competition data. For the competition dataset, we'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","0079d3f8":"You can also export the best model found by AutoKeras as a Keras Model.","71080f04":"# Resampling\n\nAuto Keras y categories calculation wrong when cat 5 is missing etc","b861f38c":"## Scaler transformer\nBy using RobustScaler(), we can remove the outliers. No good for this dataset test.\n![](https:\/\/github.com\/furyhawk\/kaggle_practice\/blob\/main\/images\/Scalers.png?raw=true)","05586259":"# Parameters\n","765cb3f3":"TPS always have huge dataset.","0af12429":"# Introduction #\nWelcome. Let automate machine learning as much as possible.\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!<\/strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit<\/strong> button in the top right corner.\n<\/blockquote>\n\nBugs:\nAK - No val_loss, val_accuracy from history.\n\n## Imports and Configuration ##\n\nWe'll start by importing the packages we used in the exercises and setting some notebook defaults. Unhide this cell if you'd like to see the libraries we'll use:"}}