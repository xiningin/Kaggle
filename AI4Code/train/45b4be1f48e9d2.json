{"cell_type":{"8e97113b":"code","ef48c5ea":"code","c5371307":"code","2dc4f14b":"code","9320b77c":"code","e8e7103a":"code","8a04f064":"code","78a386cf":"code","3492450c":"code","5acdee76":"code","814c6e81":"code","8a9a3ce2":"code","2e92fea4":"code","07b5cce1":"code","d00d459b":"code","69f1fe6a":"code","cc023991":"code","fa605b9a":"code","b59e4b00":"code","0ea1c189":"code","ca4e8e18":"code","abfc56ba":"code","81c5ef7c":"code","d4f4b568":"code","aaa3863d":"code","bb0a35a6":"code","dc909b25":"code","f8d3e07d":"code","76a4d1a4":"code","378ef807":"code","cd8c5122":"code","e9da199d":"code","ae7078df":"code","e287de9b":"code","be1fe438":"markdown","f00de39e":"markdown","8ce6a184":"markdown","2bfb972b":"markdown","8c498f6a":"markdown","61edd745":"markdown","5db59258":"markdown","ffd22132":"markdown","35584ef0":"markdown","e99d5ed8":"markdown","a4763282":"markdown","7b2d52db":"markdown","eeaefc41":"markdown","d7bd8c07":"markdown","7557fc86":"markdown"},"source":{"8e97113b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef48c5ea":"# !pip install pyLDAvis==2.1.2","c5371307":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.corpora import Dictionary\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import models","2dc4f14b":"df_tweets = pd.read_csv('\/kaggle\/input\/trump-tweets\/realdonaldtrump.csv')\ndf_tweets.head()","9320b77c":"# px.imshow(df_tweets.isnull())","e8e7103a":"df_tweets.content","8a04f064":"# tweets=[tweet.split() for tweet in df_tweets.content]","78a386cf":"from nltk.corpus import stopwords\nimport re\nimport spacy\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","3492450c":"# Convert to list\ndata = df_tweets.content.values.tolist()\n\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\ndata[:1]","5acdee76":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","814c6e81":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","8a9a3ce2":"# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","2e92fea4":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)","07b5cce1":"# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])","d00d459b":"tweets=data_lemmatized","69f1fe6a":"id2word = Dictionary(tweets)\n# id2word","cc023991":"corpus = [id2word.doc2bow(tweet) for tweet in tweets]\n# corpus","fa605b9a":"# Build LDA model\nlda_model = LdaModel(corpus=corpus,\n                   id2word=id2word,\n                   num_topics=10, \n                   random_state=0,\n                   chunksize=100,\n                   alpha='auto',\n                   per_word_topics=True)","b59e4b00":"lda_model.print_topics()\n# doc_lda = lda_model[corpus]","0ea1c189":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=tweets, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","ca4e8e18":"import pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","abfc56ba":"list(lda_model.get_document_topics(corpus[1]))","81c5ef7c":"lda_model.show_topic(0)","d4f4b568":"def find_dom_topics(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n        dominant_topic = row[0]\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        sent_topics_df = sent_topics_df.append(pd.Series([int(dominant_topic[0]), round(dominant_topic[1],4)]), ignore_index=True)\n\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_dominant_topic = find_dom_topics(ldamodel=lda_model, corpus=corpus, texts=tweets)\n\n# Format\ndf_dominant_topic = df_dominant_topic.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","aaa3863d":"# Group top 5 sentences under each topic\ndf_topics_doc = pd.DataFrame()\n\ndf_dominant_topic_grpd = df_dominant_topic.groupby('Dominant_Topic')\n\nfor i, grp in df_dominant_topic_grpd:\n    df_topics_doc = pd.concat([df_topics_doc, \n                                             grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(1)], \n                                            axis=0)\n\n# Reset Index    \ndf_topics_doc.reset_index(drop=True, inplace=True)\n\n# Format\ndf_topics_doc.columns = ['Document_No', 'Topic_Num', \"Topic_Perc_Contrib\", \"Text\"]\n\n# Show\ndf_topics_doc","bb0a35a6":"df_dominant_topic['Dominant_Topic'].hist(bins=10)","dc909b25":"# Number of Documents for Each Topic\ntopic_counts = df_dominant_topic['Dominant_Topic'].value_counts()\n\n# Percentage of Documents for Each Topic\ntopic_contribution = round(topic_counts\/topic_counts.sum(), 4)\n\n# Concatenate Column wise\ndf_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n\n# Change Column names\ndf_topics.columns = ['Num_Documents', 'Perc_Documents']\n\n# Show\ndf_topics","f8d3e07d":"df_topics.sort_index().Num_Documents.plot.bar()","76a4d1a4":"df_topics.sort_index().Perc_Documents.plot.bar()","378ef807":"tfidf = models.TfidfModel(corpus, smartirs='ntc')\ntfidf_corpus=[]\nfor doc in tfidf[corpus]:\n   tfidf_corpus.append([(id, np.around(freq,decimals=2)) for id, freq in doc])","cd8c5122":"# Build LDA model\nlda_model2 = LdaModel(corpus=tfidf_corpus,\n                   id2word=id2word,\n                   num_topics=10, \n                   random_state=0,\n                   chunksize=100,\n                   alpha='auto',\n                   per_word_topics=True)","e9da199d":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model2.log_perplexity(tfidf_corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda2 = CoherenceModel(model=lda_model2, texts=tweets, dictionary=id2word, coherence='c_v')\ncoherence_lda2 = coherence_model_lda2.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda2)","ae7078df":"lda_model2.print_topics()","e287de9b":"vis2 = pyLDAvis.gensim.prepare(lda_model2, tfidf_corpus, id2word)\nvis2","be1fe438":"# Step:4 tfidf base LDA model","f00de39e":"## 2.4 Visualize topics","8ce6a184":"## 1.4 Generate lemma","2bfb972b":"## 1.3 Remove stop words and prepare bigram","8c498f6a":"## 2.1 Generate dictionary\/id2word mapping & corpus from data which generates countvector","61edd745":"## 2.3 Evaluate Model","5db59258":"## 1.2 Prepare bigram\/trigram","ffd22132":"## 1.1 Tokenization","35584ef0":"## 3.1 Finding the dominant topic in each sentence","e99d5ed8":"# Step 1: Preprocessing","a4763282":"# Step:2 Prepare inital LDA model","7b2d52db":"## 2.2 build model ","eeaefc41":"# Steps 3: Questions","d7bd8c07":"## 3.2 Find the most representative document for each topic","7557fc86":"## 3.3 Topic distribution across documents"}}