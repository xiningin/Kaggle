{"cell_type":{"f78b9dfa":"code","b4cfb487":"code","ca9a844e":"code","6bee769c":"code","8a4abe33":"code","563ef34c":"code","d4bca313":"code","5622e748":"code","4fa8d7ae":"code","dd5920da":"code","696b672b":"code","8ff43075":"code","c8c1abef":"code","0ca65fce":"code","be51ce38":"code","66998ff1":"code","6692d178":"code","c5161fd5":"code","2c04b050":"code","1b3c767e":"code","02d79e8e":"code","5fb701ea":"code","d9830701":"code","982b2170":"code","b2c11ec5":"code","5e4919bd":"code","a178f710":"code","5048ebda":"code","cfad5372":"code","5b5829c8":"code","25de8264":"code","8c80e234":"code","2c2f18ee":"code","d6348d96":"code","229628e4":"code","ea08e197":"code","0c71ef78":"code","abc34017":"code","0efc05bf":"code","37e964ad":"code","9029c984":"code","a55c281e":"code","0d8b1551":"code","53009aa9":"code","3073186d":"code","33954e51":"code","a37ded31":"code","d2397fa1":"code","31fa6651":"code","acb78d05":"code","5da2fb9b":"code","f221f5e3":"code","bb7d5009":"code","894271c8":"code","f3790a0f":"code","bd54e2a4":"code","23180173":"code","512a3af8":"code","4a9dda4f":"code","c611fc1d":"code","5396a612":"code","1f1ccd37":"code","596397b1":"code","fba60e75":"markdown","9c1a14c2":"markdown","8d45a236":"markdown","3a928dcd":"markdown","7e6e2deb":"markdown","b52a76b3":"markdown","668425f1":"markdown","71981de2":"markdown","280afd90":"markdown","d4935036":"markdown","e3ce22c9":"markdown","6287bea7":"markdown","af0cdfe6":"markdown","311a5382":"markdown","41692198":"markdown","e6d8af4d":"markdown","cbbc894a":"markdown","c9a94257":"markdown","fdd70945":"markdown","6a79a692":"markdown","817d34d9":"markdown","9e31915c":"markdown","3fd0de08":"markdown","4b5793e2":"markdown","ba39e3d8":"markdown","20be3ec0":"markdown","cc34e4ad":"markdown","c6ef337d":"markdown","901b6e6e":"markdown","bf178eef":"markdown","c6d551a1":"markdown","500d94ff":"markdown","4f3a1040":"markdown","0be667e2":"markdown","7009285a":"markdown","6fe9f61c":"markdown","684a5c3b":"markdown","527359a1":"markdown","ba54c1d6":"markdown","57457d40":"markdown","032ee9a7":"markdown","80123047":"markdown","46237dd0":"markdown","9b0355ed":"markdown","0515f70f":"markdown","1d3c26ef":"markdown","04bebb7c":"markdown","b01a1680":"markdown","3eb637b0":"markdown","30ce27df":"markdown","717764ec":"markdown","a0c36b31":"markdown","58004568":"markdown","d56f8b36":"markdown","eeb47963":"markdown","f29ad722":"markdown","ee9b5845":"markdown","a88bf6da":"markdown","bd60e4d7":"markdown","72bdc23e":"markdown","209c2460":"markdown","dc8ea444":"markdown","cd21eba9":"markdown","39dba026":"markdown","ee50c4cf":"markdown","7023ec92":"markdown","7722c2d3":"markdown","1ed44ece":"markdown","6acfd4c0":"markdown"},"source":{"f78b9dfa":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport datetime as dt\nimport time\nimport math\nimport copy\nfrom scipy.stats import iqr\nimport collections\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\n#!pip install category_encoders\nimport category_encoders as ce\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import NearestNeighbors\nimport plotly.graph_objects as go\n#!pip install umap\n#!pip install umap-learn\nimport umap.umap_ as umap\nfrom sklearn.cluster import KMeans\nfrom matplotlib.colors import ListedColormap\n!pip install hdbscan --no-build-isolation --no-binary :all:\nfrom hdbscan import HDBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.cluster.hierarchy import dendrogram, linkage","b4cfb487":"def plot_3d(component1,component2,component3, color, x,y,z, size=900):\n  fig = go.Figure(data=[go.Scatter3d(\n          x=component1,\n          y=component2,\n          z=component3,\n          mode='markers',\n          marker=dict(\n              size=10,\n              color=color,                \n              colorscale=[\"firebrick\", 'mediumseagreen' ,'darkmagenta', \"steelblue\",'orange' ],   \n              opacity=1,\n              line_width=1\n          )\n      )])\n  # tight layout\n  fig.update_layout(width=size,height=size, scene_camera = dict(eye=dict(x=x, y=y, z=z)))\n  fig.layout.template = 'plotly_white'\n  fig.show()","ca9a844e":"from sklearn.metrics import davies_bouldin_score\ndef get_kmeans_score(data, center):\n    kmeans = KMeans(n_clusters=center)\n    model = kmeans.fit_predict(data)\n    score = davies_bouldin_score(data, model)\n    return score","6bee769c":"customers = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/raularellano\/Customers\/main\/customers.csv\")\ncampaigns = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/raularellano\/Customers\/main\/campaigns.csv\")\ncamp2cust= pd.read_csv(\"https:\/\/raw.githubusercontent.com\/raularellano\/Customers\/main\/campaigns_to_customers_launched.csv\")","8a4abe33":"customers_train, customers_test = train_test_split(customers, test_size=0.3, random_state=123)\n\nc2c_train = camp2cust[camp2cust.customer.isin(customers_train['customer_id'])]\nc2c_test = camp2cust[camp2cust.customer.isin(customers_test['customer_id'])]\nprint(\"La variable c2c_train contiene {} registros, que suponen un {} del total \\nLa variable c2c_test contiene {} registros, que suponen un {} del total\".format(c2c_train.shape[0], c2c_train.shape[0]\/camp2cust.shape[0], c2c_test.shape[0], c2c_test.shape[0]\/camp2cust.shape[0]))","563ef34c":"customers.head()","d4bca313":"customers.describe()","5622e748":"pd.isnull(customers[[x for x in customers.columns[pd.isna(customers).any()].tolist()]]).sum()","4fa8d7ae":"print(customers.Education.value_counts())\ncustomers.Marital_Status.value_counts()","dd5920da":"customers_clean = customers.drop(columns=['customer_id', 'Z_Revenue', 'Z_CostContact', 'Year_Birth'])\ncustomers_clean['Education_off'] = customers['Education']\ncustomers_clean['Education'] = customers['Education'].apply(lambda id: {'Basic': 1,'2n Cycle': 2, 'Graduation': 3,'Master': 4, 'PhD': 5}[str(id)])\ncustomers_clean['Dt_Customer'] = ((int(time.time())) - (pd.to_datetime(customers['Dt_Customer']).astype(int)\/10**9))\/(60*60*24*365)\ncustomers_clean['Edad'] = 2022-customers['Year_Birth']\ncustomers_clean['Marital_Status_off'] = customers_clean['Marital_Status']\ncustomers_clean['Marital_Status'] = customers_clean['Marital_Status'].apply(lambda id: {'Married': 'Married', 'Together': 'Together', 'Single': 'Single', 'Divorced': 'Divorced',\n                                                                            'Widow': 'Widow', 'Alone': 'Single', 'YOLO': 'Single', 'Absurd': 'Single'}[str(id)])\ncustomers_clean = ce.OneHotEncoder(cols = ['Marital_Status']).fit(customers_clean).transform(customers_clean)\ncustomers_clean = customers_clean.drop(columns=['Marital_Status_5'])\ncustomers_clean['Income'] = customers_clean['Income'].fillna(customers_clean.Income.mean(), inplace=False)\npd.isnull(customers_clean[[x for x in customers_clean.columns[pd.isna(customers_clean).any()].tolist()]]).sum()","696b672b":"customers_clean.head()","8ff43075":"customers_clean_scaled = StandardScaler().fit_transform(customers_clean.select_dtypes(np.number))","c8c1abef":"from sklearn.decomposition import PCA\npca_tsne = PCA()\npca_result_tsne = pca_tsne.fit(customers_clean_scaled)\ntsne = TSNE(n_components=3, perplexity = math.sqrt(len(customers_clean_scaled))).fit_transform(customers_clean_scaled)","0ca65fce":"plot_3d(tsne[:,0], tsne[:,1], tsne[:,2], 'cadetblue', 1.5, 1.5, 0)","be51ce38":"neighbors = NearestNeighbors()\nneighbors_fit = neighbors.fit(customers_clean_scaled)\ndistances, indices = neighbors_fit.kneighbors(customers_clean_scaled)\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.figure(figsize=(10,10))\nplt.plot(distances)\nplt.show()","66998ff1":"clustering = DBSCAN(eps=5.5, min_samples=np.shape(customers_clean_scaled)[1]*2).fit(customers_clean_scaled)\ny_pred_x = clustering.fit_predict(customers_clean_scaled)\n\nplot_3d(tsne[:,0], tsne[:,1], tsne[:,2], y_pred_x, 1.5, 0, 0)","6692d178":"summary = pd.DataFrame({'Variance' : pca_result_tsne.explained_variance_ratio_,\n                          'VarianceCum' : pca_result_tsne.explained_variance_ratio_.cumsum(),\n                          'Eigenvalue' : pca_result_tsne.explained_variance_,\n                          'PC' : np.arange(1, len(pca_result_tsne.explained_variance_ratio_)+1)})\n\nplt.figure(num = 3, figsize=(20,15))\n\nplt.subplot(211)\nplt.bar(data = summary,\n         height = 'Variance', x = 'PC', color=\"steelblue\")\nsns.lineplot(data = summary,\n         y = 'VarianceCum', x = 'PC', color=\"red\")\nfor i, v in enumerate(round(summary.VarianceCum,3)):\n    plt.text(summary.PC.tolist()[i], v+0.01, str(v), ha='right', \n             fontsize=12, color = 'red')\nplt.title('Variance and Cumulative Variance of each PC', size=15)\n\nplt.subplot(212)\nplt.bar(data = summary,\n         height = 'Eigenvalue', x = 'PC', color=\"steelblue\")\nfor i, v in enumerate(round(summary.Eigenvalue,1)):\n    plt.text(summary.PC.tolist()[i], v+.1, str(v), ha='center', \n             fontsize=12, color = 'black')\nplt.title('Eigenvalues of each PC', size=15)\nplt.show()","c5161fd5":"correlation = customers_clean.loc[:, customers_clean.columns]\ncorr_matrix_p = correlation.select_dtypes(include = ['int', 'float']).corr(method = 'pearson')\n\nmask_p = np.zeros_like(corr_matrix_p)\nmask_p[np.triu_indices_from(mask_p)]=True\n\nplt.figure(num = 3, figsize=(20,15))\nsns.heatmap(corr_matrix_p, annot=True, annot_kws={\"size\": 10}, mask = mask_p)\nplt.show()","2c04b050":"from sklearn.decomposition import PCA \nPCA = PCA(n_components = 9)\npca = PCA.fit(customers_clean_scaled)\ntestComponents = pca.transform(customers_clean_scaled)\ncustomers_clean_rec = pca.inverse_transform(testComponents)","1b3c767e":"MSE = np.array([mean_squared_error(customers_clean_scaled[i,], customers_clean_rec[i,]) for i in range(len(customers_clean_scaled))])\nplt.figure(num = 3, figsize=(30,2))\nsns.boxplot(x=MSE)\nplt.axvline(x = np.quantile(MSE, .75) + 1.5*iqr(MSE), color = 'red')\nplt.show()","02d79e8e":"print(np.sort([mean_squared_error(customers_clean_scaled[i,], customers_clean_rec[i,]).round(2) for i in range(len(customers_clean_scaled))]).tolist()[-39:])\ncollections.Counter(y_pred_x)","5fb701ea":"np.corrcoef(customers['Complain'], y_pred_x)","d9830701":"customers_clean['Outlier'] = y_pred_x\ncustomers_clean_Preg2 = customers_clean[customers_clean.Outlier.isin([0])].drop(columns=['Outlier'])\ncustomers_clean2_scaled = StandardScaler().fit_transform(customers_clean_Preg2.select_dtypes(np.number))\n\npca_result_tsne = pca_tsne.fit(customers_clean2_scaled)\ntsne2 = TSNE(n_components=3, perplexity = math.sqrt(len(customers_clean2_scaled))).fit_transform(customers_clean2_scaled)","982b2170":"plot_3d(tsne2[:,0], tsne2[:,1], tsne2[:,2], 'purple', 1.5, 1.5, 0) \nplot_3d(tsne[:,0], tsne[:,1], tsne[:,2], [np.nan if y_pred_x[x] == -1 else 0 for x in range(len(y_pred_x))], 1.5, 0, 0)","b2c11ec5":"correlation = customers_clean_Preg2.loc[:, customers_clean_Preg2.columns]\ncorr_matrix_p = correlation.select_dtypes(include = ['int', 'float']).corr(method = 'pearson')\n\nmask_p = np.zeros_like(corr_matrix_p)\nmask_p[np.triu_indices_from(mask_p)]=True\n\nplt.figure(num = 3, figsize=(20,15))\nsns.heatmap(corr_matrix_p, annot=True, annot_kws={\"size\": 10}, mask = mask_p)\nplt.show()","5e4919bd":"scaler_customers = StandardScaler().fit(customers_clean_Preg2.select_dtypes(np.number).drop(columns=['Complain']))\ncustomers_clean2_scaled = scaler_customers.transform(customers_clean_Preg2.select_dtypes(np.number).drop(columns=['Complain']))\n\ndistortions = []\nK = range(1,15)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(customers_clean2_scaled)\n    distortions.append(kmeanModel.inertia_)","a178f710":"plt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()\n\n#from yellowbrick.cluster import KElbowVisualizer\n#model = KMeans()\n\n#visualizer = KElbowVisualizer(model, k=(2,30),metric='calinski_harabasz', timings= True)\n#visualizer.fit(customers_clean2_scaled)        \n#visualizer.show()        \n\n#scores = []\n#centers = list(range(2,30))\n#for center in centers:\n #   scores.append(get_kmeans_score(customers_clean2_scaled, center))\n    \n#plt.plot(centers, scores, linestyle='--', marker='o', color='b');\n#plt.xlabel('K');\n#plt.ylabel('Davies Bouldin score');\n#plt.title('Davies Bouldin score vs. K')","5048ebda":"kmeans_cust = KMeans(n_clusters=3, random_state=123, max_iter = 10000)\ny_pred_clientes = kmeans_cust.fit_predict(customers_clean2_scaled)\n\nprojected = pca.fit_transform(customers_clean2_scaled)","cfad5372":"plot_3d(projected[:,0], projected[:,1], projected[:,2], y_pred_clientes, 1, 1.5, 1)\nplot_3d(tsne2[:,0], tsne2[:,1], tsne2[:,2], y_pred_clientes, 1, 1.5, 1)\n\nreducer = umap.UMAP(random_state = 42, n_components = 3)\nembedding = reducer.fit_transform(customers_clean2_scaled, y = y_pred_clientes)\n\nplot_3d(reducer.embedding_[:,0], reducer.embedding_[:,1], reducer.embedding_[:,2], y_pred_clientes, 1, -1, 1.5)","5b5829c8":"data = projected\ncmap = ListedColormap(sns.xkcd_palette(['denim blue', 'medium green', 'gold', 'cyan', 'purple']))\n\nclusters = [0]*4\ncentroids = [0]*4\nfor i in range(2, 6):\n  alg = KMeans(n_clusters=i, random_state=123, max_iter = 10000)\n  clusters[i-2] = alg.fit_predict(customers_clean2_scaled)\n  centroids[i-2] = [item[[0,1]] for item in pca.transform(alg.cluster_centers_)]\n\n\nplt.figure(figsize=(30,12))\nplt.subplot(231)\nplt.scatter(data[:,0], data[:,1], s=20, cmap=cmap)\nplt.title('Sample Data', fontsize=14)\n\nplt.subplot(232)\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\n\nplt.subplot(233)\ncentroide = pd.DataFrame(centroids[0])\nplt.scatter(data[:,0], data[:,1], c=clusters[0], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('2 Clusters', fontsize=14)\n\nplt.subplot(234)\ncentroide = pd.DataFrame(centroids[1])\nplt.scatter(data[:,0], data[:,1], c=clusters[1], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('3 Clusters', fontsize=14)\n\nplt.subplot(235)\ncentroide = pd.DataFrame(centroids[2])\nplt.scatter(data[:,0], data[:,1], c=clusters[2], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('4 Clusters', fontsize=14)\n\nplt.subplot(236)\ncentroide = pd.DataFrame(centroids[3])\nplt.scatter(data[:,0], data[:,1], c=clusters[3], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('5 Clusters', fontsize=14)\nplt.show()\n\ncentroides_5_customers = alg.cluster_centers_","25de8264":"customers_clean_Preg2['Cluster_Customer'] = y_pred_clientes+1\ncustomers_clean_Preg2","8c80e234":"campaigns.head()","2c2f18ee":"campaigns.describe()","d6348d96":"campaigns_clean = campaigns\ncampaigns_clean.index = campaigns.campaign_id\ncampaigns_clean = campaigns.drop(columns=['campaign_id', 'name'])\ncampaigns_clean['discount'] = np.round(campaigns.discount\/campaigns.price, 2)\ncampaigns_clean.head()","229628e4":"scaler_campaigns = StandardScaler().fit(campaigns_clean.select_dtypes(np.number))\ncampaigns_clean_scaled = scaler_campaigns.transform(campaigns_clean.select_dtypes(np.number))","ea08e197":"correlation = campaigns_clean.loc[:, campaigns_clean.columns]\ncorr_matrix_p = correlation.select_dtypes(include = ['int', 'float']).corr(method = 'pearson')\n\nmask_p = np.zeros_like(corr_matrix_p)\nmask_p[np.triu_indices_from(mask_p)]=True\n\nplt.figure(figsize=(28,14))\n\nplt.subplot(231)\nsns.heatmap(corr_matrix_p, annot=True, annot_kws={\"size\": 19}, mask = mask_p)\nplt.subplot(232)\nsns.kdeplot(campaigns_clean.prob_drink, shade=True, color = 'blue')\nsns.kdeplot(campaigns_clean.prob_food, shade=True, color = 'darkorange')\nplt.xlabel('Prob_drink (b) & Prob_food (o)')\nplt.subplot(233)\nsns.histplot(x=campaigns_clean.price, stat=\"count\", edgecolor='black')\nplt.subplot(234)\nplt.scatter( x=campaigns_clean.prob_drink, y=campaigns_clean.prob_food)\nplt.subplot(235)\nplt.scatter( x=campaigns_clean.prob_drink, y=campaigns_clean.prob_food, c = campaigns_clean.outdate, cmap = 'hot')\nplt.subplot(236)\nplt.scatter( x=campaigns_clean.prob_drink, y=campaigns_clean.prob_food, c = campaigns_clean.price, cmap = 'plasma')\nplt.show()\n\nsorted = campaigns_clean.sort_values(by=['price'], ascending = False)\n\nfig, axes = plt.subplots(ncols=2, sharey=True,figsize=(22.35,10))\naxes[0].barh(y=range(len(sorted)),edgecolor = 'black', width=sorted.discount, color='orange')\naxes[1].barh(y=range(len(sorted)),edgecolor = 'black', width=sorted.price)\naxes[0].set(title='% de Descuento')\naxes[1].set(title='Precio de Venta')\naxes[0].invert_xaxis()\nfig.tight_layout()\nfig.subplots_adjust(wspace=0.01)\nplt.show()","0c71ef78":"from sklearn.decomposition import PCA \nPCA = PCA(n_components = 5)\npca = PCA.fit(campaigns_clean_scaled)\nprojected2 = pca.fit_transform(campaigns_clean_scaled)\n\ndata = projected2\ncmap = ListedColormap(sns.xkcd_palette(['denim blue', 'medium green', 'gold', 'cyan', 'purple']))\n\nclusters = [0]*4\ncentroids = [0]*4\nfor i in range(2, 6):\n  alg = KMeans(n_clusters=i, random_state=123, max_iter = 10000)\n  clusters[i-2] = alg.fit_predict(campaigns_clean_scaled)\n  centroids[i-2] = [item[[0,1]] for item in pca.transform(alg.cluster_centers_)]\n\n\nplt.figure(figsize=(30,12))\nplt.subplot(231)\nplt.scatter(data[:,0], data[:,1], s=20, cmap=cmap)\nplt.title('Sample Data', fontsize=14)\n\nplt.subplot(232)\ndistortions = []\nK = range(1,15)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(campaigns_clean_scaled)\n    distortions.append(kmeanModel.inertia_)\n\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\n\nplt.subplot(233)\ncentroide = pd.DataFrame(centroids[0])\nplt.scatter(data[:,0], data[:,1], c=clusters[0], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('2 Clusters', fontsize=14)\n\nplt.subplot(234)\ncentroide = pd.DataFrame(centroids[1])\nplt.scatter(data[:,0], data[:,1], c=clusters[1], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('3 Clusters', fontsize=14)\n\nplt.subplot(235)\ncentroide = pd.DataFrame(centroids[2])\nplt.scatter(data[:,0], data[:,1], c=clusters[2], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('4 Clusters', fontsize=14)\n\nplt.subplot(236)\ncentroide = pd.DataFrame(centroids[3])\nplt.scatter(data[:,0], data[:,1], c=clusters[3], s=20, cmap=cmap)\nplt.plot(centroide[0], centroide[1], 'ro', markersize=15)\nplt.title('5 Clusters', fontsize=14)\nplt.show()\n\ncentroides_5_campaigns = alg.cluster_centers_","abc34017":"kmeans_camp = KMeans(n_clusters=4, random_state=123, max_iter = 10000)\ny_pred_campaign = kmeans_camp.fit_predict(campaigns_clean_scaled)\n\ncampaigns_clean['Cluster_Campaigns'] = y_pred_campaign+1\ncampaigns_clean","0efc05bf":"plot_3d(data[:,0], data[:,1], data[:,2], campaigns_clean.Cluster_Campaigns, 0, 0, 2)","37e964ad":"c2c_train = c2c_train.loc[:, c2c_train.columns != 'Unnamed: 0']\nc2c_train.head()","9029c984":"campaigns_ready = campaigns_clean[['Cluster_Campaigns']]\ncustomers_ready = customers_clean_Preg2[['Cluster_Customer']]\n\njoin_train = c2c_train.set_index('campaign')\njoin_train = join_train.join(campaigns_ready).join(customers_ready, on = 'customer').reset_index().rename(columns={'index': 'campaign'})\njoin_train['unos'] = [1]*len(join_train)","a55c281e":"join_train.head()","0d8b1551":"grouped = join_train.groupby(['Cluster_Campaigns', 'Cluster_Customer']).sum()\nresultado_train = grouped.result\/grouped.unos\nresultado_train","53009aa9":"c2c_test = c2c_test.loc[:, c2c_test.columns != 'Unnamed: 0']\nc2c_test.index = c2c_test.campaign\nc2c_test.head()","3073186d":"join_test = c2c_test\njoin_test = join_test.join(campaigns_ready).join(customers_ready, on = 'customer').reset_index()\njoin_test['unos'] = [1]*len(join_test) ","33954e51":"grouped = join_test.groupby(['Cluster_Campaigns', 'Cluster_Customer']).sum()\nresultado_test = grouped.result\/grouped.unos\nresultado_test","a37ded31":"math.sqrt(mean_squared_error(resultado_train, resultado_test))","d2397fa1":"centroides_5_customer = scaler_customers.inverse_transform(centroides_5_customers)\n\nPreg61 = pd.DataFrame(centroides_5_customer, columns = customers_clean_Preg2.select_dtypes(np.number).drop(columns=['Complain']).columns[0:23])\nPreg61['Cluster'] = kmeans_cust.fit_predict(centroides_5_customers)+1\nPreg61","31fa6651":"resultado_train = pd.DataFrame(resultado_train).reset_index()","acb78d05":"sns.heatmap(pd.crosstab(resultado_train.Cluster_Campaigns, resultado_train.Cluster_Customer, values = np.array(resultado_train.iloc[:,2]), aggfunc = 'mean'),\n            cmap=\"hot_r\", annot=True, cbar=False)\nplt.show()","5da2fb9b":"centroides_5_campaign = scaler_campaigns.inverse_transform(centroides_5_campaigns)\nPreg62 = pd.DataFrame(centroides_5_campaign, columns = campaigns_clean.columns[0:5])\nPreg62['Cluster'] = kmeans_camp.fit_predict(centroides_5_campaigns)+1\nPreg62","f221f5e3":"sns.heatmap(pd.crosstab(resultado_train.Cluster_Campaigns, resultado_train.Cluster_Customer, values = np.array(resultado_train.iloc[:,2]), aggfunc = 'mean'),\n            cmap=\"hot_r\", annot=True, cbar=False)\nplt.show()","bb7d5009":"#c2c_train, c2c_test = train_test_split(camp2cust, test_size=0.2, random_state=123)\n#c2c_train, c2c_validation = train_test_split(c2c_train, test_size=0.25, random_state=123)","894271c8":"customers_Extra = customers.drop(columns=['Response', 'customer_id', 'Z_Revenue', 'Z_CostContact'])\ncustomers_Extra['Income'] = customers_Extra['Income'].fillna(customers_Extra.Income.mean(), inplace=False) \ncustomers_Extra['Marital_Status'] = customers_Extra['Marital_Status'].apply(lambda id: {'Married': 'Married', 'Together': 'Together', 'Single': 'Single', 'Divorced': 'Divorced',\n                                                                            'Widow': 'Widow', 'Alone': 'Single', 'YOLO': 'Single', 'Absurd': 'Single'}[str(id)])\ncustomers_Extra = ce.OneHotEncoder(cols = ['Marital_Status']).fit(customers_Extra).transform(customers_Extra)\ncustomers_Extra['Education'] = customers['Education'].apply(lambda id: {'Basic': 1,'2n Cycle': 2, 'Graduation': 3,'Master': 4, 'PhD': 5}[str(id)])\ncustomers_Extra['Edad'] = 2022-customers_Extra['Year_Birth']\ncustomers_Extra['Dt_Customer'] = ((int(time.time())) - (pd.to_datetime(customers['Dt_Customer']).astype(int)\/10**9))\/(60*60*24*365)\ncustomers_Extra = customers_Extra.drop(columns=['Year_Birth', 'Marital_Status_5'])\n\ncustomers_Extra.head()","f3790a0f":"customers_Extra['Outlier'] = y_pred_x\ncustomers_clean_extra = customers_Extra[customers_Extra.Outlier.isin([0])].drop(columns=['Outlier'])\ncustomers_clean2_scaled = StandardScaler().fit_transform(customers_clean_extra.select_dtypes(np.number))","bd54e2a4":"from sklearn.decomposition import PCA\nPCA = PCA(n_components = 11)\npca = PCA.fit(customers_clean2_scaled)\nprojected = pca.fit_transform(customers_clean2_scaled)","23180173":"def obtain_RMSE(cluster_clientes, cluster_campaigns):\n   clus_cust = customers_clean_extra\n   clus_cust['Cluster_Customer'] = cluster_clientes + 1\n   clus_camp = campaigns_clean\n   clus_camp['Cluster_Campaigns'] = cluster_campaigns + 1\n\n   join_train = c2c_train.set_index('campaign')\n   join_train = join_train.join(clus_camp).join(clus_cust, on = 'customer').reset_index()\n   join_train['unos'] = [1]*len(join_train) \n   \n   grouped = join_train.groupby(['Cluster_Campaigns', 'Cluster_Customer']).sum()\n   resultado_train = grouped.result\/grouped.unos\n\n   join_test = c2c_test\n   join_test = join_test.join(clus_camp).join(clus_cust, on = 'customer').reset_index()\n   join_test['unos'] = [1]*len(join_test) \n\n   grouped = join_test.groupby(['Cluster_Campaigns', 'Cluster_Customer']).sum()\n   resultado_test = grouped.result\/grouped.unos\n\n   return math.sqrt(mean_squared_error(resultado_train, resultado_test))","512a3af8":"kmeans_cust = KMeans(n_clusters=3, random_state=123, max_iter = 10000)\ny_pred_clientes = kmeans_cust.fit_predict(customers_clean2_scaled)\n\nkmeans_camp = KMeans(n_clusters=4, random_state=123, max_iter = 10000)\ny_pred_campaign = kmeans_camp.fit_predict(campaigns_clean_scaled)","4a9dda4f":"clusterer = AgglomerativeClustering(n_clusters=3)\nclusters_cust = clusterer.fit_predict(customers_clean2_scaled)\n\nclusterer = AgglomerativeClustering(n_clusters=4)\nclusters_camp = clusterer.fit_predict(campaigns_clean_scaled)","c611fc1d":"clustering2 = DBSCAN(eps = 3.5, min_samples=np.shape(customers_clean2_scaled)[1]*2).fit(customers_clean2_scaled)\ny_pred_cus = clustering2.fit_predict(customers_clean2_scaled)\n\nclustering2 = DBSCAN(eps = 0.9, min_samples=np.shape(campaigns_clean_scaled)[1]*2).fit(campaigns_clean_scaled)\ny_pred_camp = clustering2.fit_predict(campaigns_clean_scaled)","5396a612":"gmm = GaussianMixture(n_components=3, max_iter =1000000,covariance_type = 'full',random_state = 123)\ny_pred_customers = gmm.fit_predict(customers_clean2_scaled)\n\ngmm = GaussianMixture(n_components=4, max_iter =1000000,covariance_type = 'full',random_state = 123)\ny_pred_campa = gmm.fit_predict(campaigns_clean_scaled)","1f1ccd37":"hdbscan = HDBSCAN(min_cluster_size=50, min_samples=5)\npreds_2_cus = hdbscan.fit_predict(customers_clean2_scaled)\n\nhdbscan = HDBSCAN(min_cluster_size=20, min_samples=5)\npreds_2_camp = hdbscan.fit_predict(campaigns_clean_scaled)","596397b1":"plt.figure(figsize=(30,25))\n\nplt.subplot(551)\nplt.text(0.5, 0.6, \"K-MEDIAS\", ha='center', fontsize=30, color='black')\nplt.text(0.5, 0.4, \"RSME: {} \".format(round(obtain_RMSE(y_pred_clientes, y_pred_campaign),5)),ha='center', fontsize=20, color='grey')\nplt.subplot(552)\ndistortions = []\nK = range(1,15)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(customers_clean2_scaled)\n    distortions.append(kmeanModel.inertia_)\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('Codo de Clientes')\n\nplt.subplot(553)\nplt.scatter(projected[:,0], projected[:,1], c=y_pred_clientes, s=20, cmap=cmap)\nplt.title('3 Clusters Kmedias', fontsize=14)\n\nplt.subplot(554)\nplt.scatter(data[:,0], data[:,1], c=y_pred_campaign, s=20, cmap=cmap)\nplt.title('4 Clusters Kmedias', fontsize=14)\n\nplt.subplot(555)\ndistortions = []\nK = range(1,15)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(campaigns_clean_scaled)\n    distortions.append(kmeanModel.inertia_)\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('Codo Campa\u00f1as')\n\nplt.subplot(556)\nplt.text(0.5, 0.6, \"JERARQUICO\", ha='center', fontsize=30, color='black')\nplt.text(0.5, 0.4, \"RSME: {} \".format(round(obtain_RMSE(clusters_cust, clusters_camp),5)),ha='center', fontsize=20, color='grey')\n\nplt.subplot(557)\nZ = linkage(customers_clean2_scaled, 'ward')\ndendrogram(Z, labels=clusters_cust, orientation='top', leaf_rotation=0., leaf_font_size=8., truncate_mode='level', p=3, color_threshold = 80)\nsns.despine()\nplt.axhline(y=80, c='k')\nplt.title('Dendograma Clientes')\n\nplt.subplot(558)\nplt.scatter(projected[:,0], projected[:,1], c=clusters_cust, s=20, cmap=cmap)\nplt.title('3 Clusters Jer\u00e1rquico', fontsize=14)\n\nplt.subplot(559)\nplt.scatter(data[:,0], data[:,1], c=clusters_camp, s=20, cmap=cmap)\nplt.title('4 Clusters Jer\u00e1rquico', fontsize=14)\n\nplt.subplot(5,5,10)\nZ = linkage(campaigns_clean_scaled, 'ward')\ndendrogram(Z, labels=clusters_camp, orientation='top', leaf_rotation=0., leaf_font_size=8., truncate_mode='level', p=4, color_threshold = 7)\nsns.despine()\nplt.axhline(y=7, c='k')\nplt.title('Dendograma Campa\u00f1as')\n\nplt.subplot(5,5,11)\nplt.text(0.5, 0.6, \"DBSCAN\", ha='center', fontsize=30, color='black')\nplt.text(0.5, 0.4, \"RMSE: {} \".format(round(obtain_RMSE(y_pred_cus, y_pred_camp),5)),ha='center', fontsize=20, color='grey')\n\nplt.subplot(5,5,12)\nneighbors_fit = neighbors.fit(customers_clean2_scaled)\ndistances, indices = neighbors_fit.kneighbors(customers_clean2_scaled)\ndistances = np.sort(distances, axis=0)                           \ndistances = distances[:,1]\nplt.plot(distances)\nplt.title('Epsilon Clientes')\n\nplt.subplot(5,5,13)\nplt.scatter(projected[:,0], projected[:,1], c=y_pred_cus, s=20, cmap=cmap)\nplt.title('2 Clusters DBSCAN', fontsize=14)\n\nplt.subplot(5,5,14)\nplt.scatter(data[:,0], data[:,1], c=y_pred_camp, s=20, cmap=cmap)\nplt.title('4 Clusters DBSCAN', fontsize=14)\n\nplt.subplot(5,5,15)\nneighbors_fit = neighbors.fit(campaigns_clean_scaled)\ndistances, indices = neighbors_fit.kneighbors(campaigns_clean_scaled)\ndistances = np.sort(distances, axis=0)                           \ndistances = distances[:,1]\nplt.plot(distances)\nplt.title('Epsilon Campa\u00f1as')\n\nplt.subplot(5,5,16)\nplt.text(0.5, 0.6, \"GAUSSIAN MIX\", ha='center', fontsize=30, color='black')\nplt.text(0.5, 0.4, \"RMSE: {} \".format(round(obtain_RMSE(y_pred_customers, y_pred_campa),5)),ha='center', fontsize=20, color='grey')\n\nplt.subplot(5,5,18)\nplt.scatter(projected[:,0], projected[:,1], c=y_pred_customers, s=20, cmap=cmap)\nplt.title('3 Clusters Gaussian Mixture', fontsize=14)\n\nplt.subplot(5,5,19)\nplt.scatter(data[:,0], data[:,1], c=y_pred_campa, s=20, cmap=cmap)\nplt.title('4 Clusters Gaussian Mixture', fontsize=14)\n\nplt.subplot(5,5,21)\nplt.text(0.5, 0.6, \"HDBSCAN\",ha='center', fontsize=30, color='black')\nplt.text(0.5, 0.4, \"RMSE: {} \".format(round(obtain_RMSE(preds_2_cus, preds_2_camp),5)),ha='center', fontsize=20, color='grey')\n\nplt.subplot(5,5,23)\nplt.scatter(projected[:,0], projected[:,1], c=preds_2_cus, s=20, cmap=cmap)\nplt.title('3 Clusters HDBSCAN', fontsize=14)\n\nplt.subplot(5,5,24)\nplt.scatter(data[:,0], data[:,1], c=preds_2_camp, s=20, cmap=cmap)\nplt.title('4 Clusters HDBSCAN', fontsize=14)\n\nplt.show()","fba60e75":"# <font size=\"6\"> Pr\u00e1ctica Final <\/font>\n### Ra\u00fal Arellano L\u00f3pez","9c1a14c2":"# <font size=\"5\"> Pregunta 1 <\/font>\n**Para hacer una posterior evaluaci\u00f3n de los m\u00e9todos desarrollados, divide el dataset campaigns_to_customers_launched.csv en training, validation y test.**","8d45a236":"# <font size=\"5\"> Pregunta 6.2 <\/font>\n**Genera una campa\u00f1a nueva simulada. \u00bfA que cl\u00faster de clientes le ofrecer\u00edas la campa\u00f1a con el objetivo de maximizar el \u00e9xito?.**","3a928dcd":"# <font size=\"5\"> Pregunta 2.1 <\/font>\n**Realiza una transformaci\u00f3n adecuada del dataset customers.csv para su posterior procesamiento. (Tips: recuerda eliminar ids, transformar variables discretas, y cambiar fechas a d\u00edas hasta el d\u00eda actual, por ejemplo.)**\n","7e6e2deb":"# <font size=\"5\"> Pregunta 2 <\/font>\n### Realiza las acciones adecuadas sobre customers.csv:","b52a76b3":"#### HDBSCAN\n\n**He calculado, sobre los datos tratados, el cluster al que pertenece cada cliente o campa\u00f1a seg\u00fan el algoritmo HDBSCAN, al ser la primera vez que implementaba este algoritmo, he obtenido informacion de la siguiente <a href=\"https:\/\/rubialesalberto.medium.com\/clustering-con-dbscan-y-hdbscan-con-python-y-sus-hiperpar%C3%A1metros-en-sklearn-8728283b96ac\">publicaci\u00f3n<\/a>.**","668425f1":"**Me parec\u00eda interesante remarcar una relaci\u00f3n que he visto terminando la pr\u00e1ctica, y es la fuerte correlaci\u00f3n entre los individuos fuera de rango y aquellos que se han quejado (Complain).**\n\nEsta relaci\u00f3n **tiene mucho sentido**, ya que aquellos **individuos que no encajan entre nuestros clientes habituales** se han podido sentir tratados de una forma diferente y por lo tanto **quejarse**, lo que explicar\u00eda la correlaci\u00f3n entre las personas que se quejan y aquellas que toman valores at\u00edpicos. Por lo tanto, **el grupo de outliers agrupados podr\u00eda ser un grupo de clientes insatisfechos.** *(tras un par de comprobaciones, a medida que el n\u00famero de outliers calculado por el DBSCAN se acercam\u00e1s al numero de individuos que se ha quejado, mayor es la correlaci\u00f3n, lo que indica que la relacion entre los individuos que se quejan y aquellos outliers m\u00e1s extremos, es pr\u00e1cticamente perfecta, lo que refuerza la hip\u00f3tesis de negocio que he planteado).*","71981de2":"#### K-Medias\n**He calculado, sobre los datos tratados, el cluster al que pertenece cada cliente o campa\u00f1a seg\u00fan el algoritmo de K-medias, teniendo en cuenta el \u00f3ptimo n\u00famero de clusters que tiene en cada caso seg\u00fan el m\u00e9todo del codo que se presenta en el cuadro final**\n\nHe guardado la informaci\u00f3n resultante en dos variables para emplearlas en el resumen.","280afd90":"# <font size=\"5\"> Pregunta 2.4 <\/font>\n**Haz un clustering haciendo uso de K-means, de la base de datos de customers. Recuerda optimizar el valor adecuado de componentes (seleccionar adecuadamente la K).**","d4935036":"**Lo siguiente que hago es una funci\u00f3n que calcule el Root Mean Square Error.**\n\nCon esta funci\u00f3n, consigo simplificar el proceso de predicci\u00f3n del \u00e9xito para cada uno de las combinaciones de cluster cliente - cluster campa\u00f1a tanto para train y para test, asi como el calculo del RMSE\n\n* **Toma como imput el cluster al que pertenece cada cliente y campa\u00f1a** seg\u00fan el algoritmo de clustering de cada caso.\n\n* Despues, **guarda ese dato en una nueva columna de los datos tratados anteriormente de clientes y de campa\u00f1as**\n\n* A continuaci\u00f3n, **hace el doble join entre los clusters de clientes, la tabla de resultados de train y los clusters de campa\u00f1as**. Seguidamente, repite el proceso pero con la tabla de resultados de test.\n\n* Lo siguiente que hace es **agrupar los datos y calcular laprobabilidad de \u00e9xito de cada combinacion cluster cliente - cluster campa\u00f1a**, tanto para los datos de entrenamiento como para los datos de test. \n\n* Por ultimo, **devuelve la raiz cuadrada del Error Cuadr\u00e1tico Medio para poder comparar los valores en cada uno de los 5 algoritmos de clustering**\n\n***Esta funci\u00f3n me ha permitido simplificar mi siguiente c\u00f3digo, ya que simplemente he tenido que calcular el cluster al que pertenece cada cliente y campa\u00f1a en cada uno de los 5 algoritmos de clustering para despues aplicar la funcion y obtener el RMSE automaticamente, evitando escribir una gran cantidad de c\u00f3digo y evitando tambien posibles fallos.***","e3ce22c9":"**En esta pr\u00e1ctica voy a agrupar cada cliente y cada campa\u00f1a en un cluster utilizando los algoritmos de clustering \u00f3ptimos en cada caso y teniendo en cuenta el procesamiento previo de los datos.**\n\nConcretamente, voy a **tratar los datos** de clientes y de campa\u00f1as para hacerlos *aptos* a los modelos de agrupaci\u00f3n, analizando la **estructura de los datos**, tomando decisiones de tratamiento, viendo las posibles **agrupaciones previas**. Para ello **utilizar\u00e9 t\u00e9cnicas estad\u00edsticas sencillas y otras herramientas como el *PCA,* el *T-SNE*, o el *UMAP*.** Eliminar\u00e9 los valores at\u00edpicos y dejar\u00e9 los datos limpios.\n\nUna vez tengo los datos correctamente tratados, aplicar\u00e9 sendos **modelos de agrupaci\u00f3n** para ambas tablas. De esta maner\u00e1 obtendr\u00e9 el cluster al que pertenece cada individuo y cliente. \n\nUna vez he llegado hasta aqui, **unir\u00e9 los datos a la tabla de resultados de test y de entranamiento, y calcular\u00e9 la probabilidad de \u00e9xito al aplicarle una campa\u00f1a perteneceiente a un cluster a un cliente que pertenezca a un cluster concreto.** Por \u00faltimo, **utilizar\u00e9 el RMSE para calcular el grado de precisi\u00f3n del modelo.**\n\nPor \u00faltimo, **generar\u00e9 cinco clientes y cinco campa\u00f1as completamente nuevos, y analizar\u00e9 el cluster al que pertenecen y la campa\u00f1a o cliente con el que deber\u00edan relacionarse para obtener el mayor \u00e9xito posible**\n\nComo Extra, **realizar\u00e9 (adem\u00e1s del K medias ya aplicado) otros 4 modelos de agrupaci\u00f3n: *Jer\u00e1rquico, DBSCAN, Gaussian Mixture y HDBSCAN* para obtener el modelo que mejor se ajusta a cada uno de los dos tipos de datos.**\n\n***A lo largo de esta pr\u00e1ctica, ir\u00e9 planteando algunas cuestiones de negocio que me parecen relevante mencionar y que he obtenido al desarrollar los ejercicios.***","6287bea7":" A continuaci\u00f3n, voy a **analizar los valores at\u00edpicos realizando un An\u00e1lisis de Componentes Principales**. Para ello, voy a realizar lo siguiente:\n\n * **1\u00ba Voy a aplicar el PCA al conjunto de datos directamente**, incluyendo los valores fuera de rango\n \n * **2\u00ba Obtengo las coordenadas de todos los individuos en TODAS las CP.** Los individuos que no sean Outliers, ser\u00e1n correctamente representados en las primeras CP (ya que son aquellas que representan el grueso de la poblaci\u00f3n) pero aquellos cuyos valores sean m\u00e1s extremos, no estar\u00e1s correctamente representados en estas primeras componentes, sino en las \u00faltimas (ya que las \u00faltimas CP son las que mejor representan los valores at\u00edpicos)\n \n * **3\u00ba Me quedar\u00e9 con las CP que expliquen el 70% de la poblaci\u00f3n** (9 componentes, ya que son aquellos con autovalor mayor de 1) y que representen correctamente a los valores m\u00e1s homog\u00e9neos y mal a los individuos extremos\n \n * **4\u00ba Aplicar\u00e9 la inversa del PCA con 9 CP** (que contiene el 70% de la informaci\u00f3n) **sobre el total de individuos**. De esta manera, obtendr\u00e9 las nuevas Componentes Principales en las que estar\u00e1n correctamentente representados los indviduos dentro de valores normales, y mal representados los individuos fura de rango\n \n * **5\u00ba Por \u00faltimo, aplico el Error Cuadr\u00e1tico Medio entre la coordenada que tiene el individuo en la primera CP al aplicar en un primer momento el PCA y el valor que toma al realizar la inversa del PCA.** Aquellos individuos con mayor MSE ser\u00e1n valores fuera de rango","af0cdfe6":"# <font size=\"5\"> Pregunta 2.3 <\/font>\n**Puede ser interesante realizar un an\u00e1lisis descriptivo con alg\u00fan mapa de correlaci\u00f3n y un t-sne, pca o similar para ver como se distribuyen los datos y eliminar outliers (recuerda eliminarlos tambi\u00e9n de la tabla que relaciona clientes y campa\u00f1as)**\n","311a5382":"Siguiendo el mismo proceso que en la Pregunta 2, **he generado un gr\u00e1fico de codo para analizar el n\u00famero \u00f3ptimo de grupos en los que se pueden dividir las campa\u00f1as, as\u00ed como un An\u00e1lisis de Componentes Principales para poder  ver gr\u00e1ficamente en 2D la mayor cantidad de informaci\u00f3n posible.** En este caso, **se ve claramnte como se generan 4 clusters de campa\u00f1as, que ya se pod\u00edan ver en los datos del PCA.**\n\n*Podr\u00eda parecer que igual con 3 clusters es suficiente, pero es la tercera CP (que refleja mayor informaci\u00f3n) en la que se ve como claramente son grupos distintos. Por lo tanto, podemos decir que la variable que discrimina si un inividuo pertenece a uno de estos dos grupos o al otro est\u00e1 fuertemente correlacionada con la tercera Componente Principal.*","41692198":"**Voy a hacer primero un TSNE** para ver como se agrupan los datos seg\u00fan este modelo y si hay individuos que no se agrupan correctamente.\n\nDespu\u00e9s, para identificar estos individuos, relizar\u00e9 un **An\u00e1lisis de Componentes Principales** y obtendr\u00e9 cual de los individuos es un **outlier** seg\u00fan el `mean_squared_error` entre la proyecci\u00f3n en las CP y su predicci\u00f3n al reducir la dimensi\u00f3n (guardando el 80% de la informaci\u00f3n)","e6d8af4d":"### Cuadro Resumen\n\nEn el siguiente cuadro, he querido recoger de un golpe la informaci\u00f3n que he obtenido en esta pregunta 7, con 16 gr\u00e1ficas distintas y calculando 5 RMSE gracias a la funci\u00f3n que he creado previamente. ","cbbc894a":"Un ejemplo, quedar\u00eda como: ***La probabilidad de que un individuo perteneciente a un cluster de cliente 1 acepte una campa\u00f1a de un cluster de campa\u00f1as 1 es de un 18.32%*** (con train era del 18.91%, por lo que es bastante parecido)\n\nPara analizar la semejanza de las dos distribuciones de probabilidad, y por tanto ver si el modelo sirve tanto para predecir la probabilidad de clientes de entrenamiento y de test y que no hay sobreajuste, aplico el **Error Cuadratico Medio y le hago la raiz cuadrada para que sea m\u00e1s sencillo de analizar**. El utilizar la ra\u00edz cuadrada lo he sacado de la siguiente <a href=\"https:\/\/towardsdatascience.com\/what-are-the-best-metrics-to-evaluate-your-regression-model-418ca481755b \">publicaci\u00f3n.<\/a> \n\n**Al ser la raiz cuadrada, estar\u00eda definido en t\u00e9rminos de Desviaci\u00f3n T\u00edpicay podr\u00edamos decir algo as\u00ed como que las desviaciones t\u00edpicas de ambas distribuciones var\u00edan muy poco (0.02556) por lo que son pr\u00e1cticamente identicas. La utilidad real del RMSE es comparar con otros modelos, lo que reservo para la Pregunta 7.**","c9a94257":"Efectivamente, **hay 39 outliers identificados por el DBSCAN seg\u00fan los inputs introducidos.** (M\u00e1s adelante comentar\u00e9 este resultado comparandolo con otras t\u00e9cnicas de identificaci\u00f3n de valores at\u00edpicos)\n\nRealmente, seg\u00fan lo que se ve en la gr\u00e1fica (y recordando que en el T-SNE las distancias INTRA-grupo son reales pero las INTER-grupos  son irreales) **podr\u00eda identificar dos tipos de valores at\u00edpicos teniendo en cuenta el problema de negocio que se nos plantea:**\n\n* **Valores extremos individuales**, es decir, aquellos que se alejan de los valores no at\u00edpicos de forma individual. Ser\u00edan clientes que nada tienen que ver con el negocio y que han participado en las campa\u00f1as pero siendo completamente heterog\u00e9neos al conjunto de clientes habituales del negocio.\n\n* **Valores extremos agrupados**, es decir, aquellos resentes en el grupo minoritario de valores at\u00edpicos. Estos individuos no son lo suficientemente representativos en n\u00famero como para pertenecer a los clientes habituales o crear un nuevo grupo de clientes. Puede deberse a que la estrategia del negocio se enfoque a ofrecer su producto a un nuevo segmento de cliente y esta estrategia se acabe de implantar de tal manera que, pese a que hay una cantidad de clientes a los que la empresa se est\u00e9 dirigiendo, su n\u00famero no es suficientemente alto como para dejar de ser heterog\u00e9neos a los clientes habituales o generar un nuevo grupo de clientes habituales2.","fdd70945":"**Para hacer la primera parte de esta pregunta, he seguido el mismo proceso que en la pregunta anterior, pero aplicado sobre los datos de test.**","6a79a692":"Los resultados de los 5 clientes simulados ser\u00e1n: \n\n* **Cliente Simulado 1**, pertenece al cluster de clientes 3, por lo que, con un 37% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer una campa\u00f1a perteneciente al cuarto cluster de campa\u00f1as\n\n* **Cliente Simulado 2**, pertenece al cluster de clientes 2, por lo que, con un 47% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer una campa\u00f1a perteneciente al cuarto cluster de campa\u00f1as\n\n* **Cliente Simulado 3**, pertenece al cluster de clientes 2, por lo que, con un 47% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer una campa\u00f1a perteneciente al cuarto cluster de campa\u00f1as\n\n* **Cliente Simulado 4**, pertenece al cluster de clientes 1, por lo que, con un 32% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer una campa\u00f1a perteneciente al tercer cluster de campa\u00f1as\n\n* **Cliente Simulado 5**, pertenece al cluster de clientes 1, por lo que, con un 47% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer una campa\u00f1a perteneciente al segundo cluster de campa\u00f1as\n\nCon los resultados obtenidos, se me ocurre hacer un an\u00e1lisis de la eficiencia de cada uno de los clusters de campa\u00f1as **(aplicando teor\u00eda de juegos de Nash)**. Las campa\u00f1as que pertenezcan a los clusters 1 o 2 nunca van a ser escogidas como optimas para ofrecer a los clientes seg\u00fan la agrupaci\u00f3n en 3 clusters de clientes, por lo que le podr\u00edamos decir al director de marketing que elimine las campa\u00f1as que pertenecen a estos clusters, ya que antes de aplicarlas, ser\u00eda mejor aplicar las de otros clusters (realmente podr\u00eda haber campa\u00f1as en estos clusters que tengan mejor resultados sobre los clientes que algunas de las que pertenecen a los clusters dominantes, pero me parec\u00eda un an\u00e1lisis interesante de marcar)\n\n> Si estamos ante un cliente de cluster 1, lo optimo seria una campa\u00f1a de cluster de campa\u00f1as 3 ya que es la oferta con mayor probabilidad de \u00e9xito\n\n> Si estamos ante un cliente de cluster 2, lo optimo seria una campa\u00f1a de cluster de campa\u00f1as 4 ya que es la oferta con mayor probabilidad de \u00e9xito\n\n> Si estamos ante un cliente de cluster 3, lo optimo seria una campa\u00f1a de cluster de campa\u00f1as 4 ya que es la oferta con mayor probabilidad de \u00e9xito\n","817d34d9":"\nPrimero, veo que **pr\u00e1cticamente todas las columnas son num\u00e9ricas, a excepcion de el nivel de educaci\u00f3n `Education` y el estado civil `Marital_Status`.** Adem\u00e1s, hay columnas que contienen la identidad de los consumidores, por lo que deber\u00eda eliminarlas, otras que contienen un numero constante (`Z_CostContact` o `Z_Revenue`) y otra, `DT_Customer` en formato de fecha, cuando deber\u00eda ser un n\u00famero.\n","9e31915c":"# <font size=\"5\"> Pregunta 6 <\/font>\n### Genera datos simulados:","3fd0de08":"A continuaci\u00f3n realizo el **tratamiento de datos**, muy similar al de la pr\u00e1ctica, y elimino los **outliers**, **escalo los datos** y realizo el **PCA** para poder graficar los resultados de los algoritmos.\n\n*Me he planteado aplicar los algoritmos de clustering sobre el PCA directamente, de esta manera, la informaci\u00f3n repetida en varias columnas no tendr\u00eda tanto efecto sobre el cluster, pero los RMSE que arrojaban los modelos eran un tanto superiores,por lo que he decidido aplicarlo directamente sobre los datos tratados y escalados, pero no sobre las CP.*","4b5793e2":"Por ahora no ser\u00eda necesario escalar, ya que para la Pregunta 3.3. sigo utilizando los datos originales. De todos modos, como lo voy a necesitar en la Pregunta 3.4, escalo ahora con la funci\u00f3n `StandardScaler()`","ba39e3d8":"**Si establecemos que un MSE por encima de 0.9** (debido a la distribuci\u00f3n de este c\u00e1lculo, que hace que un valor de 0.9 se encuentre por encima de 1,5 veces el Recorrido Intecturt\u00edlico m\u00e1s el cuartil 3 (que podr\u00eda ser una definici\u00f3n de outlier)) **encontramos que unos 38 valores se encuentran por encima de 0.9 en MSE.** Si compaamos con que **el DBSCAN ha determinado en el grupo de Outliers** (aquellos individuos que no ha sido capaz de agrupar en otros grupos) **39 individuos**, podr\u00edamos tenerlos en cuenta como individuos at\u00edpicos y eliminarlos del an\u00e1lisis (siempre y cuendo hayamos establecido los par\u00e1metros del DBSCAN de forma correcta, cosa que creo que he hecho).","20be3ec0":"En este ultimo ejercicio, al ser voluntario, **he querido complicarlo lo m\u00e1ximo posible y buscar la mejor solucion para el problema de negocio que se nos ha planteado.** Por ello, he realizado una transformaci\u00f3n de los datos un tanto distinta y sobre esta he aplicado 5 algoritmos clustering distintos:\n\n* **1.- K means**\n* **2.- Agrupaci\u00f3n Jer\u00e1rquica**\n* **3.-DBSCAN**\n* **4.- Gaussian Mixture**\n* **5.- HDBSCAN**\n\n**Estos 5 algoritmos los he aplicado teniendo en cuenta el optimo de inputs en cada caso** (*epsilon y minimo en la muestra para el DBSCAN, k optimo en el K medias o la altura de corte en el jer\u00e1rquico*) **repitiendo el an\u00e1lisis para train y para test para poder obtener el (Root) Error Cuadr\u00e1tico Medio (RMSE) de cada algoritmo, comparando las predicciones que hace para los clientes y campa\u00f1as de test y train.**\n\nLa estructura de mi respuesta es la siguiente:\n\n* **Primero**, he tratado los datos y he creado una funci\u00f3n para obtener el RMSE que ahora comentar\u00e9\n\n* **Segundo**, he calculado la predicci\u00f3n del cluster de pertenencia para cada cliente y campa\u00f1a seg\u00fan los 5 algoritmos mencionados \n\n* **Tercero**, he analizado el resultado sobre los datos de entrenamiento y de test, obteniendo la prediccion del \u00e9xito de aplicar cada cluster de campa\u00f1as a cada cluster de clientes.\n\n* **Cuarto**, caluclar el Root Mean Square Error para cada algoritmo de clustering\n\n* **Quinto**, hacer un cuadro resumen con todo el proceso anterior, para ver y comparar de una sola vez todo el an\u00e1lisis.","cc34e4ad":"# <font size=\"5\"> Pregunta 5 <\/font>\n**Eval\u00faa como de preciso son los modelos que hemos creado, contrast\u00e1ndolos contra el set de test. Para ello, simplemente tienes que calcular el error cuadr\u00e1tico medio, restando y elevando al cuadrado (c\u00f3mo haciamos para detectar outliers con PCA), el resultado de cada campa\u00f1a que aparece en test menos la probabilidad de esa campa\u00f1a teniendo en cuenta el cl\u00faster al que pertenece el cliente y el cl\u00faster al que pertenece la campa\u00f1a.**","c6ef337d":"#### Agrupaci\u00f3n Jer\u00e1rquica\n\n**He calculado, sobre los datos tratados, el cluster al que pertenece cada cliente o campa\u00f1a seg\u00fan el algoritmo jer\u00e1rquico de agrupaci\u00f3n, teniendo en cuenta el \u00f3ptimo n\u00famero de clusters que tiene en cada caso seg\u00fan el m\u00e9todo del codo que se presenta en el cuadro final. Adem\u00e1s, he analizado el dendograma resultante para campa\u00f1as y para clientes en el resumen.**\n\nHe guardado la informaci\u00f3n resultante en dos variables para emplearlas en el resumen.","901b6e6e":"**He creado una serie de gr\u00e1ficos seg\u00fan lo que creo que se adapta mejor a cada tipo de dato para analizar distintas cosas que me ayuden a la hora de generar los clusters:**\n\n* **Matriz de correlaciones** entre las distintas variables de la tabla\n\n* **Comparaci\u00f3n entre la distribuci\u00f3nde `prob_food` y `prob_drink`**, permite ver bimodalidad\n\n* **Histograma de la distribuci\u00f3n del precio**\n\n* **Gr\u00e1fico de dispersi\u00f3n entre `prob_food` y `prob_drink`,** para ver si podr\u00eda haber grupos\n\n* **Gr\u00e1fico de dispersi\u00f3n entre `prob_food` y `prob_drink`, relacionado por `duraci\u00f3n`**, para ver si podr\u00eda haber grupos\n\n* **Gr\u00e1fico de dispersi\u00f3n entre `prob_food` y `prob_drink`, relacionado con `precio`**, para ver si podr\u00eda haber grupos\n\n* **Histogramas de comparaci\u00f3n de distribuci\u00f3n de precio  de descuento.**","bf178eef":"# <font size=\"5\"> Pregunta 3.1 <\/font>\n**Transformaci\u00f3n del dataset si es necesario.**\n","c6d551a1":"Para realizar la pr\u00e1ctica, **he dividido el conjunto de datos en train y test** (si bien es cierto que me plante\u00e9 la posibilidad de dividir los datos en train, test y validation siguiendo esta <a href=\"https:\/\/towardsdatascience.com\/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c \">publicaci\u00f3n<\/a>, me parec\u00eda que en este planteamiento acad\u00e9mico no tendr\u00eda mucha utilidad generar una tabla de validaci\u00f3n)\n\n**Para dividir entre entrenamiento y test, ten\u00eda un problema: Si directamente dividia la tabla de resultados en test y entrenamiento, tendr\u00eda clientes que pertenecer\u00edan tanto a test como a train** (ya que a un mismo individuo, se le ofrecen distintas campa\u00f1as). Para evitar este problema, decid\u00ed **dividir el total de individuos en train y test con la funci\u00f3n `train_test_split()`** y luego usarlos para filtrar en la tabla de resultados los registros que se guardan como entrenamiento y los que se guardan como prueba, en funcion del cliente que intervenga. La division de la tabla de clientes no tiene m\u00e1s utilidad que esta a lo largo de la pr\u00e1ctica, como ya se ver\u00e1, he utilizado el conjunto de clientes a partir de aqui. \n\nUna vez que tengo la tabla de resultados correctamente dividida en train y test, voy a comentar como afectan los siguientes desarrollos a esta divisi\u00f3n:\n\n El tratamiento de datos y la agrupacion por clusters de las preguntas 2 y 3 las he hecho sobre el conjunto de datos total, haciendo la divisi\u00f3n en train y test en la pregunta 4 al filtrar aquellos que pertenecen a test y los que pertenecen a train para calcular en cada caso una probabilidad de \u00e9xito distinta.\n\n Pese a que en la tabla de resultados de train y de test se encuentren los outliers que voy a quitar en el ejercicio 2, al hacer el `inner join` con los valores no outlier, no tendr\u00e9 en cuenta aquellos que est\u00e9n fuera de rango para la predicci\u00f3n.\n\n *Me gustar\u00eda haber podido aplicar la tabla de validaci\u00f3n en la Pregunta 7 para analizar algunas hip\u00f3tesis de negocio que he planteado, pero la estructura de los datos hace que en los datos de train, al ser de tan solo el 60% del global, no genere el mismo numero de clusters que train o validaci\u00f3n, imposibilitando la comparaci\u00f3n de los distintos modelos. De todas formas, he dejado el c\u00f3digo para esclarecer la mejor forma de obtener los tres conjuntos de datos seg\u00fan la publicaci\u00f3n mencionada. Podr\u00e1 hacer un muestreo estratificado para evitar este problema, pero ya ser\u00eda m\u00e1s supervisado*\n\n","500d94ff":"Los resultados de los 5 clientes simulados ser\u00e1n: \n\n* **Campa\u00f1a Simulada 1**, pertenece al cluster de campa\u00f1as 4, por lo que, con un 47% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer a un cliente perteneciente al segundo cluster de clientes\n\n* **Campa\u00f1a Simulada 2**, pertenece al cluster de campa\u00f1as 2, por lo que, con un 19% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer a un cliente perteneciente al primer cluster de clientes\n\n* **Campa\u00f1a Simulada 3**, pertenece al cluster de campa\u00f1as 3, por lo que, con un 32% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer a un cliente perteneciente al primer cluster de clientes\n\n* **Campa\u00f1a Simulada 4**, pertenece al cluster de campa\u00f1as 1, por lo que, con un 27% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer a un cliente perteneciente al segundo cluster de clientes\n\n* **Campa\u00f1a Simulada 5**, pertenece al cluster de campa\u00f1as 1, por lo que, con un 27% de probabilidad de \u00e9xito, se le deber\u00e1 ofrecer a un cliente perteneciente al segundo cluster de clientes\n\nCon los resultados obtenidos, se me ocurre hacer un an\u00e1lisis de la eficiencia de cada uno de los clusters de clientes (**aplicando teor\u00eda de juegos de Nash**). Realmente, no hay ningun cluster de campa\u00f1as que se deba enfocar directamente al tercer cluster de clientes, ya que siempre tendr\u00e1 mayor probabilidad de \u00e9xito enfocarlo a clientes pertenecientes a los clusters de clientes uno o dos.\n\n> Si estamos ante una campa\u00f1a de cluster 1, lo optimo seria enfocarlo a un cliente del cluster 2 ya que es el enfoque de mayor probabilidad de \u00e9xito\n\n> Si estamos ante una campa\u00f1a de cluster 2, lo optimo seria enfocarlo a un cliente del cluster 1 ya que es el enfoque de mayor probabilidad de \u00e9xito\n\n> Si estamos ante una campa\u00f1a de cluster 3, lo optimo seria enfocarlo a un cliente del cluster 1 ya que es el enfoque de mayor probabilidad de \u00e9xito\n\n> Si estamos ante una campa\u00f1a de cluster 4, lo optimo seria enfocarlo a un cliente del cluster 2 ya que es el enfoque de mayor probabilidad de \u00e9xito","4f3a1040":"# <font size=\"5\"> Pregunta 4 <\/font>\n**Haciendo uso del dataset de entrenamiento de campaigns_to_customers_launched.csv, calcula el valor esperado de \u00e9xito para cl\u00faster de campa\u00f1as mostrado a cada cl\u00faster de clientes. Este valor esperado, podr\u00eda representar, de forma aproximada, por cada combinaci\u00f3n de clusters campa\u00f1a-cliente, la probabilidad de \u00e9xito al ofrecer una campa\u00f1a al azar de un cl\u00faster de campa\u00f1as a un cliente al azar de un cl\u00faster de clientes.**","0be667e2":"#### Gaussian Mixture\n\n**He calculado, sobre los datos tratados, el cluster al que pertenece cada cliente o campa\u00f1a seg\u00fan el modelo de Gaussian Mixture, teniendo en cuenta el \u00f3ptimon\u00famero de clusters seg\u00fan la gr\u00e1fica del codo. He guardado la informaci\u00f3n resultante en dos variables para emplearlas en el resumen.**","7009285a":"# <font size=\"5\"> Pregunta 6.1 <\/font>\n**Genera un cliente simulado. \u00bfDe que cl\u00faster de campa\u00f1as le ofrecer\u00edas al azar una campa\u00f1a con el objetivo de maximizar el \u00e9xito?.**","6fe9f61c":"Tal y como se puede ver, **tanto 4 como 5 clusters ser\u00eda un tanto ineficiente** (teniendo en cuenta la definicion de la gr\u00e1fica del codo, incluir un cuarto o quinto cluster a penas supondr\u00eda un cambio, lo que es menos eficiente que aplicar 2 o 3 clusters)\n\nRespecto la diferencia entre 2 y 3 clusters, **realmente no tengo un dato que determine que es mejor utilizar 3 que 2 clusters, pero teniendoen cuenta el objetivo del an\u00e1lisis, utilizar 3 clusters ajusta mucho mejor los individuos para poder calcular la probablidad de \u00e9xito de las campa\u00f1as**, por lo que es el objetivo del problema de negocio lo que me lleva a decantarme por 3 clusters (si bien, soy consciente de que podr\u00eda estar incurriendo en sobre ajuste con esta decisi\u00f3n).\n\nPor \u00faltimo, **agrego el cluster al que pertenece cada individuo a la tabla limpia y sin outliers de datos para poder emplearlo en los siguientes ejercicios.**","684a5c3b":"# <font size=\"5\"> Pregunta 2.2 <\/font>\n**\u00bfDebes normalizar los datos?. Si es as\u00ed, realiza la normalizaci\u00f3n.**","527359a1":"**Una vez tengo los datos limpios, los escalo con la funcion `StandardScaler()`** y calculo el n\u00famero de clusters que mejor se adapta a estos datos seg\u00fan elm\u00e9todo de Kmedias. **A dem\u00e1s del criterio del codo,he utilizado otros criterios m\u00e1s elaborados que he encontrado en esta <a href=\"https:\/\/towardsdatascience.com\/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad\">publicaci\u00f3n.<\/a>** (*debido a la gran necesidad de tiempo que llevan algunos modelos para poder aplicarse, no los he procesado*).\n\n**El resultado es que se deber\u00edan de coger entre 2 o 3 Clusters de clientes. Para decidier el valor exacto, m\u00e1s adelante he preparado una simulaci\u00f3n con la distribuci\u00f3n de los individuos en cada grupo en funci\u00f3n del n\u00famero de clusters y el centroide de cada grupo repitiendo la operacion entre 2 y 5 grupos**","ba54c1d6":"Para generar datos simulados, **voy a utilizar los centroides generados en los Kmedias de los ejercicios 2 y 3. Como he llegado a generar hasta 5 centroides para clientes y 5 para campa\u00f1as, voy a analizar un total de 10 valores simulados.**\n\n*Al ser centroides, la puntuaci\u00f3n que toman en algunas variables no es factible en la realidad (por ejemplo, Income si que tomar\u00e1 valores factiles al ser una variable continua, pero el numero de hijos no, ya que puede ser que tenga por ejemplo 0.43 hijos). De todos modos, si que son valores factibles matem\u00e1ticamente, por lo que los voy a aplicar de esta manera.*","57457d40":"A continuaci\u00f3n, **voy a realizar un DBSCAN para asignar a cada registro un grupo de pertenencia, y as\u00ed poder certificar que el T-SNE se ha aplicado correctamente**\n\nPara aplicar el algor\u00edtmo de clustering, lo primero que hago es **establecer el *n\u00famero m\u00ednimo* de individuos necesario para generar un grupo**. He decidido que este numero sea igual a **el n\u00famero de variables que contienen los datos por dos** (`min_samples=np.shape(x)[1]*2`) y no un n\u00famero menor para ser bastante conservador a la hora de determinar un valor outlier (Un individuo necesitar\u00e1 estar cerca de muchos otros para poder pertenecer a un grupo y no ser denominado como outlier)\n\nLo segundo que decido es **el valor de *epsilon***, es decir, el radio del c\u00edculo centrado en el individuo que va a establecer si dos individuos est\u00e1n suficientemente cercanos como para pertenecer al mismo grupo o si son muy lejanos, seg\u00fan el criterio de el vecino m\u00e1s cercano. Para ello, he creado las distancias entre los individuos seg\u00fan el criterio indicado, y he realizado una gr\u00e1fica de codo para determinar el valor \u00f3ptimo de epsilon. He decidido que tenga un valor de 5.5, tal y como se ve en la gr\u00e1fica.\n\nEn ambas medidas he intentado ser bastante conservador, en la medida de lo posible, para que los outlier sean realmente valores extremos y no posibles nuevos entrantes a la empresa a los que se les est\u00e1 comenzando a aplicar una camp\u00f1a para analizar el resultado o por que la empresa estuviese buscando diversificar su linea de productos.\n\n**Todo ello lo he realizado debido a la informaci\u00f3n de la siguiente <a href=\"https:\/\/medium.com\/@tarammullin\/dbscan-parameter-estimation-ff8330e3a3bd\">publicaci\u00f3n.<\/a>**","032ee9a7":"# <font size=\"5\"> Pregunta 3.3 <\/font>\n**Quiz\u00e1 sea interesante un peque\u00f1o an\u00e1lisis descriptivo.**","80123047":"Por lo tanto, **lo siguiente que hago es eliminar los individuos extremos**, utilizando la funcion `isin()` y `drop()` para que elimine aquellos registros cuyo individuo pertenezca a la definicion de outlier dada por el DBSCAN y que coincide con el an\u00e1lisis de CP teniendo en cuenta el Error Cuadr\u00e1tico Medio. **Los he eliminado de la tabla original para repetir el T-SNE y corroborar que efectivamente se han eliminado tanto los indviduos agrupados en elgrupo de outliers como aquellos que erancompletamente extremos.**","46237dd0":"#### DBSCAN\n\n**He calculado, sobre los datos tratados, el cluster al que pertenece cada cliente o campa\u00f1a seg\u00fan el algoritmo de DBSCAN, teniendo en cuenta el \u00f3ptimo epsilon que tiene en cada caso seg\u00fan el m\u00e9todo del codo que se presenta en el cuadro final, y el numero minimo de muestras como dos veces la longitud de los datos como se explica en elDBSCAN aplicado en la pregunta 2 de la pr\u00e1ctica.**\n\nHe guardado la informaci\u00f3n resultante en dos variables para emplearlas en el resumen.","9b0355ed":"Con 9 CP explicamos el 70% de la informaci\u00f3n, por lo que va a ser el n\u00ba de CP que vamos a utilizar para generar el modelo sobre el que calcularemos la inversa. **En esta inversa, los valores que m\u00e1s se desv\u00eden de los registros originales ser\u00e1n aquellos que no se explican correctamente en las CP que explican la mayor parte de la varianza, sino que se explicar\u00e1n en las CP marginales, por lo que ser\u00e1n individuos fuera de rango.**\n\n*Por otra parte, la primera vez que realic\u00e9 el PCA (no en esta, donde he llevado a cabo una serie de transformaciones adicionales) encontr\u00e9 situaci\u00f3n muy interesante: las \u00faltimas 4 CP no explicaban nada de informaci\u00f3n, lo que quiere decir que hay una alta correlaci\u00f3n entre 3 variables y que deber\u00edan ser extra\u00eddas del an\u00e1lisis. Al realizar una tabla de correlaciones, vi que las variables Z_Revenue y Z_CostContact recog\u00edan el mismo valor para cada individuo, por lo que es superflua para el an\u00e1lisis. Adem\u00e1s, no hab\u00eda eliminado la ultima de las variables dummie, por lo que, pese a no estar correladas al 100%, existia colinealidad entre las variables. Esta \u00faltima dummie no contiene informaci\u00f3n adicional ya que se puede calcular como la diferencia respecto al resto de dummies.*","0515f70f":"**De esta manera confirmo que la unica variable con NAs es Income, con 24 registros no disponibles.** Realmente, es una de las variables que mas senciallamente se puede imputar un valor como la media sin asumir que estemos cometiendo un gran error","1d3c26ef":"**El grueso del tratamiento de datos es el siguiente:**\n\n* **Primero elimino cuatro variables:** La variable `ID` ya que no aporta informaci\u00f3n adicional al index de la tabla, las variables `Z_Revenue` y `Z_CostContact` ya que recogen el mismo valor para cada individuo, por lo que es superflua para el an\u00e1lisis (de hecho, me he dado cuenta de esta situaci\u00f3n al aplicar un PCA y ver que algunas CP no explicaban informaci\u00f3n (varianza 0)) y `Year_Birth` ya que la voy a sustituir directamente por la edad del cliente. Por otra parte, elimino la quinta variable Dummie generada del One Hot Encoder ya que genera colinealidad entre los datos y no es necesaria, ya que la informaci\u00f3n la contienen el resto de dummies.\n\n* **Para la variable de `Educaci\u00f3n`**, paso de las 5 categor\u00edas a una **escala del 1 al 5 segun el nivel de educaci\u00f3n.** He decidido tratar as\u00ed la variable educaci\u00f3n por dos razones: **si que existe una jerarqu\u00eda en la distribuci\u00f3n de las categor\u00edas** y por que **las variables dummie no me parece que representen correctamente esta dimensi\u00f3n** (si tienes un Doctorado o un Master, tendr\u00e1s una carrera tambi\u00e9n, pero no aparecer\u00e1s como graduado en la dummie de `Graduation`) y por que un gran numero de variables explicativas del mismo rasgo (en el caso de hacer dummies) afectar\u00edan en gran medida a un analisis cluster si no se hace una reducci\u00f3n dimensional previa. Mantengo en la tabla la variable original por si la necesito m\u00e1s adelante para hacer un an\u00e1lisis descriptivo.\n\n* **La variable `DT_Customer` la he pasado al n\u00famero de a\u00f1os** (con decimales) que hace desde que es cliente hasta la fecha de hoy. Lo he logrado pasando la fecha original a tiempo `UNIX` y restando ese valor a la fecha actual en UNIX. Por \u00faltimo, he dividido este valor entre el n\u00famero de segundos que tiene un a\u00f1o *(60x60x24x365)* para tener el valor en a\u00f1os y no en segundos.\n\n* **La variable `Year_Birth` ha sido restada al a\u00f1o actual** para crear la variable `Edad`.\n\n* **Para la variable `Maritial_Status`, decido aplicar un `One Hot Encoder`**, pero realizando un tratamiento previo:\n\n Las categor\u00edas Casad@ (`Married`), Enparejad@ (`Together`), Solter@ (`Single`), Divorciad@ (`Divorced`) y Viud@ (`Widow`) tienen una frecuencia y una coherencia suficiente como para usarlas directamente, pero las categor\u00edas Sol@ (`Alone`), \"Solo Se Vive una Vez\" (You Only Live Once aka `YOLO`) y Absurdo (`Absurd`) son efectivamente absurdas y **encajan en la categor\u00eda de Solter@**. Por lo tanto, **pasaremos de 8 a 5 categor\u00edas.** Despu\u00e9s, aplico el `OneHotEncoder()` para generar *Dummies* y elimino la ultima para evitar colinealidad.\n\n* **Para tratar los valores NA de `Income`**, los completo con la media de esta variable (*la proporci\u00f3n de NAs, el 1%, permite que esta sustituci\u00f3n no altere en gran medida a la variable*) y certifico que ya no hay NAs en la tabla.","04bebb7c":"Por ultimo, **para certificar que efectivamente son 3 el numero de clusters qu deberia utilizar, he utilizado un bucle for para repetir 5 veces el k medias con distinto numero de clusters. De esta manera, puedo ver gr\u00e1ficamente como se agrupan los individuos (teniendo las 2 primeras componentes principales como ejes) en funcion del n\u00famero de clusters que indico, as\u00ed como la posicion del centroide de cada cluster.**\n\n*Tengo en cuenta que 2 clusters que se solapen en este gr\u00e1fico no representan al 100% la realidad del an\u00e1lisis (puede ser que, aun que parezcan muy juntos en este gr\u00e1fico, la variable que diferencia la pertenencia a un cluster o a otro no est\u00e9 correctamente representada en las dos primeras componentes principales, por lo que, aunque pareciese solapado, puede ser que no lo estuviese si se tuviesen en cuenta m\u00e1s dimensiones). Sin embargo, es la mejor representaci\u00f3n posible en dos dimensiones.*","b01a1680":"Aqu\u00ed, veo la **frecuencia de los distintos valores de las variables categ\u00f3ricas** que han escapado del an\u00e1lisis de variables numericas previo.\n* En la variable `Education` se distribuyen de tal manera que no considero que haga falta un gran cambio para aplicar los modelos, m\u00e1s all\u00e1 de cambiar a num\u00e9rica.\n* La variable `Marital_Status` si que tiene categor\u00edas con muy baja frecuencia, y que como explico a continuaci\u00f3n, pueden agruparse con otras categor\u00edas mayoritarias.","3eb637b0":"# <font size=\"5\"> Pregunta 7 <\/font>\n### Realiza una optimizaci\u00f3n de tus modelos. \n\n**Haciendo uso de los conocimientos de diferentes t\u00e9cnicas de clustering que conoces, y sus diferentes par\u00e1metros. Prueba diferentes combinaciones (todas las que quieras) para clusterizar tanto las campa\u00f1as y los clientes y eval\u00faalas contra el dataset de validaci\u00f3n. Qu\u00e9date con aquella que tenga el error cuadr\u00e1tico m\u00e1s peque\u00f1o.  Una vez elegida la mejor combinaci\u00f3n, eval\u00faala contra el dataset de test. TIPS: puedes incluso combinar el hacer un PCA primero antes de aplicar clustering.**","30ce27df":"Por \u00faltimo, aplico el `groupby()` agrupando por la combinaci\u00f3n de `Cluster_Campaigns` y `Cluster_Customer` y aplicando la funci\u00f3n `sum()` (as\u00ed obtengo una suma para cada una de las combinaciones de las 2 variables en cada una del resto de variables).\n\nPor \u00faltimo, **lo \u00fanico que me queda hacer es dividir la suma de resultados positivos en una combinaci\u00f3n de Cluster_Customers - Cluster_Campaigns entre la suma del total de registros (de unos) de esa combinaci\u00f3n, calculando de esta manera la probabilidad condicionada.**\n\nUn ejemplo, quedar\u00eda como: ***La probabilidad de que un individuo perteneciente a un cluster de cliente 1 acepte una campa\u00f1a de un cluster de campa\u00f1as 1 es de un 18.91%***","717764ec":"Los datos se componen de distintas variables que contienen la `identificaci\u00f3n de la campa\u00f1a`, la probabilidad de que pertenezca a campa\u00f1a de un producto de `comida` o `bebida` (*Calculado mediante NLP tipo TF-IDF, LDA o similar*), el `numero de dias que dur\u00f3 la campa\u00f1a`, el ` precio original del producto`, el `descuento en euros` y el `nombre de la campa\u00f1a`","a0c36b31":"En este ejercicio voy a **implementar el algoritmo de Kmedias sobre los atos de customers limpios y escalados directamente**\n\nPara ello, lo primero que hago es un peque\u00f1o **an\u00e1lisis de correlaciones** para comprobar que la variable `Complain` no puede ser utilizada en el an\u00e1lisis (lo m\u00e1s seguro es que gran cantidad de los outliers tomasen el valor 1 en esta variable. Por ello, es necesario eliminarla del an\u00e1lisis antes de comenzar con el K medias","58004568":"# <font size=\"5\"> Pregunta 3 <\/font>\n### Realiza las acciones adecuadas sobre campaigns.csv:","d56f8b36":"**En esta tabla se puede ver los valores que toma cada una de las 5 campa\u00f1as generadas**, son valores factibles matematicamente y en este caso tambien realmente, ademas de pertenecer al espacio dimensional.\n\nHe utilizado **los 5 centroides de la ultima estimaci\u00f3n del K medias que he hecho** (con 5 clusters), de tal manera que **son puntos factibles, pero que no son el centroide exacto de los 4 grupos, as\u00ed tengo algo de variedad y no simplemente un individuo para cada uno de los 4 grupos** (es decir, la predicci\u00f3n que hago ahora, la someto a 4 grupos, siendo que los centroides se han determinado con 5 grupos). **Si cogiese los centroides de 4 grupos, tambien ser\u00edan factibles, pero no estar\u00eda prediciendo nada nuevo, me dar\u00eda que pertenece cada uno a un grupo (al ser el centroide de ese grupo)**\n\nHe utilizado el Kmedias de la pregunta 3 para predecir el cluster al que pertenece cada una de ellas y he registrado esa variable en la ultima columna.\n\n**A continuaci\u00f3n he creado una tabla de doble entrada que representa todas las combinaciones posibles de cluster-campa\u00f1a y las probabilidades de \u00e9xito de cada combinacion segun los datos de entrenamiento** (la diferencia con los datos de test es minima,por lo que podr\u00eda haberlos usado con resultados muy similares","eeb47963":"Lo primero que veo con la funcion `describe()` es que la variable `Income` tiene *NAs* al tener tan solo 2216 valores de los 2240 registros totales, por lo que **lo tratar\u00e9 a continuaci\u00f3n.**\n\nConfirmo que las variables `Z_CostContact` y `Z_Revenue` no tienen ningun tipo de varianza, al tomar valores constantes 3 y 11 por lo que **las eliminar\u00e9 para el an\u00e1lisis.**\n\nEl resto de variables tiene distribuciones mas o menos correctas, **existiendo algun outlier que eliminar\u00e9 m\u00e1s adelante en la pr\u00e1ctica.**","f29ad722":"# <font size=\"5\"> Pregunta 3.4 <\/font>\n**Haz un clustering haciendo uso de K-means, de la base de datos de customers. Recuerda optimizar el valor adecuado de componentes.**","ee9b5845":"# <font size=\"5\"> Pregunta 3.2 <\/font>\n**Normaliza si es necesario.**","a88bf6da":"**Primero, voy a realizar un T-SNE sobre los datos que he codificado en el ejercicio anterior.** Me parece adecuada al ser una herramienta de visualizaci\u00f3n de conjuntos de datos de alta dimensi\u00f3n,como creo que es el caso, y para identificar posibles outliers y eliminarlos del an\u00e1lisis para que no condicionen los algoritmos de Kmedias y su comparaci\u00f3n con los registros de test.\n\nAntes de todo, **aplico un PCA sencillo sobre los datos** (una vez escalados), para poder seleccionar las 3 variables que mayor informaci\u00f3n van a explicar de los datos, y asi graficar el T-SNE de la mejor maner posible. \n\nRealizo el T-SNE y lo grafico, como se puede ver en el gr\u00e1fico, **aparecen una serie de grupos correctamente diferenciados** (debemos recordar que las distancias entre individuos intragrupo son reales, pero entre grupos, se escoge la distancia que m\u00e1s convenga a la gr\u00e1fica, por lo que son irreales).**Para establecer el perplexity del T-SNE, he tenido en cuenta la siguiente  <a href=\"https:\/\/towardsdatascience.com\/how-to-tune-hyperparameters-of-tsne-7c0596a18868 \">publicaci\u00f3n<\/a>.** (raiz cuadrada del n\u00famero de registros con el que se construye el T-SNE, (2.240 ^ 1\/2 = 47), por lo que la perplexity es igual a 47.\n\nA priori, **puedo identificar 3 o 4 grupos principales, unos tantos m\u00e1s de un tama\u00f1o menor y valores que no pertenecen a ninguno de los grupos y que, por tanto, presupongo como at\u00edpicos, a falta de confirmaci\u00f3n por otros algoritmos, as\u00ed como un grupo muy peque\u00f1o que podr\u00eda ser 'el grupo de Outliers'**\n\nEn principio, **no puedo separar por grupos al ser un an\u00e1lisis no supervisado**, pero m\u00e1s adelante utilizo algoritmos que me indican al grupo al que pertence cada individuo en funci\u00f3n de una serie de inputs (DBSCAN) y **vuelvo a graficar el T-SNE, pero teniendo en cuenta el grupo al que se supone que pertenece cada individuo seg\u00fan el DBSCAN, para comprobar que, efectivamente, los grupos generados por el TSNE se contrastan con el DBSCAN.**","bd60e4d7":"Para tratar los datos, he **definido como indice el numero de identificai\u00f3n de cada campa\u00f1a** (realmente no har\u00eda falta por que es igual al indice que ya habia, pero de esta manera, ante nuevas campa\u00f1as o datos desordenados, el c\u00f3digo seguir\u00eda teniendo buen resultado). **He eliminado las columnas de `campign_id` y `name` ya que refer\u00edan la misma informaci\u00f3n** que ya est\u00e1 en el \u00edndice de la tabla. Por \u00faltimo, he **establecido el descuento que se hace en cada campa\u00f1a como un porcentaje respecto del precio original**, y no como un valor monetario","72bdc23e":"Por \u00faltimo, creo una nueva columna, en los datos originales,que determine el cluster al que pertenece cada campa\u00f1a para poderlo utilizar en los siguientes ejercicios.","209c2460":"A continuaci\u00f3n, **cojo la tabla que enfrenta clientes y campa\u00f1as de entrenamiento, y elimino la columna `unamed` ya que no es necesaria para este an\u00e1lisis.** Adem\u00e1s, de las dos tablas de clientes (y el cluster al que pertenecen) y campa\u00f1as (y el cluster al que pertenecen) **extraigo tan solo las columnas que contienen la pertenencia a un cluster y utilizo el \u00edndice de cada uno de estos dos vectores para saber el cliente o campa\u00f1a al que pertenece cada registro.**\n\n**Lo siguiente que hago es un `join()` entre el dataset de resultados de entrenamiento y el cluster al que pertenece cada campa\u00f1a y cliente haciendo un doble `join()`.**\n\nPor \u00faltimo hago un `reset_index()` para evitar errores ya que tengo una columna con la identidad del cliente y una columna con la identidad de la campa\u00f1a que se aplica en cada caso.\n\n**Por \u00faltimo una columna con todo unos que me ser\u00e1 muy \u00fatil para ahorrar tiempo al hacer el group by.**\n\n*Nota: No hay que preocuparse por los outliers, por que, como ya he comentado al realizar la divisi\u00f3n entre train y test, al hacer el inner join de los individuos de train (que si que contiene outliers) y de la tabla de los clusters para cada uno de todos los clientes (que no contiene ya outliers), los individuos que est\u00e1n presentes en ambas tablas (los que no son outliers) se mantendr\u00e1n, mientras que aquellas que tan solo est\u00e1n presentes en una de las tablas (los fuera de rango) quedar\u00e1n fuera del join.*","dc8ea444":"En este ejercicio voy a hacer un proceso similar al del anterior pero cambiando de datos:\n\n* **Primero**, voy a tratar los datos, eliminando aquellos que no aporten informacion o que est\u00e9n repetidos\n* **Segundo**, escalar las variables para poder aplicar los modelos de clustering\n* **Tercero**, an\u00e1lisis descriptivo de los datos en profundidad (en este caso hay menos variables, por lo que es factible profundizar)\n* **Cuarto**, aplicar el K medias para obtener los distintos clustrs de campa\u00f1as de marketing, teniendo en cuenta el numero de grupos optimo.","cd21eba9":"Lo primero que hago, es **analizar la estructura de los datos** para luego poder tomar mejores decisiones respecto al tratamiento previo necesario para realizar los algoritmos de clustering o los modelos de reduccion dimensional o visualizaci\u00f3n.","39dba026":"**Para poder aplicar el PCA y para evitar que el K medias no tome unas variables como m\u00e1s relevantes que otras, es necesario escalar los datos**, por lo que utilizo la funcion `StandardScaler()` para ello.","ee50c4cf":"**El primero de los algoritmos que he aplicado ha sido el K medias con un error cuadratico medio de 0,029 lo que es un valor realmente bajo.**\n\n* He hecho **dos gr\u00e1ficas de codo, una para clientes y otra para campa\u00f1as.** Se parecen mucho a las realizadas en la pr\u00e1ctica, ya que los datos son similares.  \n\n Se puede ver como **el \u00f3ptimo de cluster de clientes es 3 y el \u00f3ptimo de cluster de campa\u00f1as es 4** por lo tanto, son el n\u00famero que utilizo para realizar el clustering.\n\n* En el centro, se pueden ver las **comparaciones de los distintos gr\u00e1ficos de sedimentaci\u00f3n.** En el caso de los clusters de K medias de clientes **se ve perfectamente como se distribuyen los grupos de forma homog\u00e9nea entre s\u00ed y heterog\u00e9nea con el resto de puntos, sin generar ning\u00fan tipo de punto solapado.**\n\n Est\u00e1 superposici\u00f3n de puntos es incluso menor en el caso de los clusters de campa\u00f1as, es m\u00e1s, como ahora veremos, **todos los algoritmos de clustering determinan la misma distribuci\u00f3n de los puntos en cuatro clases de campa\u00f1as**\n\n\n**El siguiente de los algoritmos de clustering que he aplicado es el modelo de agrupaci\u00f3n jer\u00e1rquica. El error cuadratico medio que arroja es de 0,020, es decir, menor que en el caso del Kmedias. A\u00fan as\u00ed sigue siendo mayor que otros algoritmos como ahora veremos.**\n\n* En este caso he realizado **dos dendrogramas, uno para clientes y otro para campa\u00f1as.** Se puede ver en el dendograma de clientes como, **cortando a una altura de 80, logramos generar tres clusters** que se agrupan a distintas alturas. En el caso del primero de ellos a una altura de unos 40, en el caso del segundo, una altura de unos 50 y en el caso el tercero, a una altura de unos 70.\n\n* Por otro lado, el dendograma de campa\u00f1as permite **cortarlo a una altura incluso de 5 para generar 4 clusters diferenciados**. En este caso, se agrupan a una altura de entorno a 3. \n\n* Gr\u00e1ficamente, vemos como **en la sedimentaci\u00f3n de los grupos de clientes, s\u00ed que se superponen unos a otros.** Tenemos que recordar, que en esta gr\u00e1fica tan solo se presentan las dos primeras componentes principales, por lo que puede ser que la discriminaci\u00f3n entre grupos se de por una variable que no est\u00e9 correctamente representada en estas componentes principales, lo que explicar\u00eda que pese, a que graficamente parece estar peor agrupado, realmente arroje un error cuadratico medio menor.\n\n* En el caso de la sedimentaci\u00f3n de campa\u00f1as, como ya he dicho, **se agrupa en cuatro grupos al igual que en todos los algoritmos**, ya que est\u00e1n suficientemente separados y son muy homog\u00e9neos intragrupo y muy heterog\u00e9neos intergrupos\n\n\n**El siguiente algoritmo que he aplicado, ha sido el DBSCAN. En este caso he obtenido el menor error cuadratico medio de todos 0,013 lo que se puede deber a que tan solo se han generado dos clusters en los datos de clientes.**\n\n* En un primer lugar voy a comentar las **gr\u00e1ficas de codo que determinan el \u00e9psilon qu\u00e9 se utiliza en cada uno de los algoritmos que he implementado.**\n \n* El epsilon del codo de clientes que genera el cambio hacia la verticalidad se da en **3.5**, por lo tanto, es el \u00e9psilon que he utilizado para agrupar los puntos, es decir, el radio del c\u00edrculo que determina si un punto pertenece a un grupo o a otro es de 3.5.\n\n* En el caso del epsilon de las campa\u00f1as, la verticalidad se consigue con un valor de **0.9**, que ha sido el radio que he utilizado para el algoritmo de clustering.\n\n* Graficamente, se ve como **en el caso de los clientes tan solo se han generado dos grupos con esta combinaci\u00f3n de \u00e9psilon y n\u00famero m\u00ednimo de datos, lo cual puede ser el condicionante para generar un error cuadratico medio menor que en el caso de algoritmos**. En el caso de campa\u00f1as, seguimos como en el resto de algoritmos.\n\n\n**En el caso de el algoritmo Gaussiano, he obtenido un error cuadratico medio de 0,025 lo cual es un tanto mayor al anterior.**\n\n* No he necesitado realizar ning\u00fan an\u00e1lisis previo, ya que el n\u00famero de clusters es el mismo que utilizado en el k-medias a traves de las graficas del codo.\n\n* Gr\u00e1ficamente, **en la sedimentaci\u00f3n de los individuos, veo como se superponen los grupos**. Esto puede generar un mayor error cuadratico medio o simplemente deberse a que la discriminaci\u00f3n entre un grupo u otro no est\u00e1 correctamente representada en las dos primeras componentes principales.\n\n* En el caso de los campa\u00f1as, **no hay sorpresa**, y se agrupan igual que en el resto de algoritmos\n\n\n**Por \u00faltimo, me parec\u00eda interesante aplicar un HDBSCAN, ya que nunca he aplicado uno y me parece que pueda ajustarse correctamente a la estructura de los datos.**\n\n* **He obtenido un error cuadratico medio de 0,027 lo cual es bastante bajo**, teniendo en cuenta que es menor que el de k-medias.\n\n* En este caso, para poder implementarlo, he utilizado la librer\u00eda `HDBSCAN` siguiendo las pautas que se indican en la <a href=\"https:\/\/rubialesalberto.medium.com\/clustering-con-dbscan-y-hdbscan-con-python-y-sus-hiperpar%C3%A1metros-en-sklearn-8728283b96ac\">publicaci\u00f3n<\/a> que ya he mencionado.\n\n* Graficamente se ve como **la sedimentaci\u00f3n de clientes es bastante parecida a la del DBSCAN a excepci\u00f3n de incluir otro grupo m\u00e1s que podr\u00edan ser considerados como individuos fuera de rango.**\n\n* En el caso de las campa\u00f1as se sigue agrupando igual que en otros algoritmos\n\n\n**Para hacer una conclusi\u00f3n de el algoritmo \u00f3ptimo para tratar cada dato, he tenido en cuenta que la estructura que posee cada una de las dos tablas de clientes y de campa\u00f1as, es distinta, por lo que el algoritmo que mejor se adapte a cada tipo de dato es distinto.**\n\n**Por lo tanto y teniendo en cuenta el resultado que arroja cada uno de los algoritmos, creo que en el caso de los datos de campa\u00f1as cualquiera de ellos puede agruparlos correctamente ya que no hay ninguna diferencia. Esto se debe a que los datos son suficientemente homog\u00e9neos dentro de cada grupo y heterog\u00e9neos para discriminarse de otros grupos. Finalmente creo que el K-medias es el m\u00e1s f\u00e1cil de implementar y m\u00e1s sencillo determinar sus inputs \u00f3ptimos (es decir, simplemente con la gr\u00e1fica del codo) por lo que para la agrupaci\u00f3n de campa\u00f1as utilizar\u00eda el K-medias.**\n\n**En el caso de los datos de clientes, me parece que el algoritmo que mejor separa los grupos es el DBSCAN con dos clusters, ya que es el que minimiza el error cuadratico medio, si bien es cierto que elimina uno de los grupos lo cual perjudicar\u00eda algunas decisiones de negocio.**\n\n**Pensandolo ahora, puede ser que el \u00f3ptimo de clusters de clientes a agrupar en el Kmedias de la practica sean 2 y no 3, esto explicar\u00eda que en la pregunta 6, haya un cluster de clientes al que, de primeras, no se le ofrecer\u00eda ninguna campa\u00f1a (ya que no generar\u00eda tanto \u00e9xito como ofreciendosela a otro cluster de clientes).**\n\n**Puede ser que simplemente haya 2 clusters de clientes, de tal manera que los que pertenecen a el grupo ineficiente, se integrar\u00edan en los otros clusters.**\n\n**Pese a esto, he de decir que hay campa\u00f1as concretas cuyo resultado de \u00e9xito es \u00f3ptimo al ofrecerse al cluster que podr\u00eda ser eliminado, por lo que si bien, no podemos decir que estos indviduos deber\u00edan agruparse en otro de los dos clusters, dentro de este, se generar\u00eda un subgrupo concreto sobre el que tendr\u00eda un mejor efecto ciertas campa\u00f1as concretas.**\n\n**A nivel de negocio, podemos decir que la empresa tiene 4 tipos de campa\u00f1a muy bien diferenciados, es decir, podr\u00edamos determinar que hay cuatro lineas de producto correctamente diferenciadas.**\n\n**Sin embargo, a la hora de hablar de los clientes a los que ofrecemos esas l\u00edneas de producto, encontramos que, o bien hay tres grupos generales de clientes los cuales van a tener un impacto diferente cada una de las campa\u00f1as que ofrezcamos, o bien hay tan solo dos grupos de clientes a los que se enfocar\u00e1n unas u otras campa\u00f1as y dentro de uno de esos grupos de clientes hay un subgrupo, que no es suficientemente numeroso como para ser un tercer grupo, pero si como para poder enfocar ciertas campa\u00f1as a un nicho de mercado.**\n\n**Adem\u00e1s, teniendo en cuenta la similitud entre los outliers que hemos eliminado en la pr\u00e1ctica, podr\u00edamos llegar a decir que la empresa est\u00e1 buscando enfocarse a un nuevo tipo de cliente, completamente distinto al habitual pero que generar\u00eda un grupo de mercado nuevo.**\n","7023ec92":"Para poder graficar r\u00e1pidamente los individuos almacenando la mayor informaci\u00f3n posible, he hecho un **An\u00e1lisis de Componentes Principales**. De esta manera, podr\u00e9 utilizar las coordenadas de cada individuo en las CP que almacenen mayor informaci\u00f3n para representar los clusters de la manera m\u00e1s realista posible.\n\nA continuaci\u00f3n se presentan **los individuos posicionados en sus coordenaas respecto las 3 primeras CP y agrupados seg\u00fan el cluster al que pertenecen teniendo en cuenta 3 clusters que, como explicar\u00e9 m\u00e1s adelante, es la opci\u00f3n que voy a tomar.**\n\n**Tambi\u00e9n aplico la agrupaci\u00f3n al posicionamiento de cada individuo seg\u00fan el T-SNE que he realizado previamente, para ver si los grupos que se han predecido antes corresponden m\u00e1s o menos con los clusters. Como se puede ver, los cluesters se situan ordenadamente dentro del TSNE tambi\u00e9n**\n\nPor \u00faltimo, **hago un UMAP para representar los clusters seg\u00fan los grupos que genera este sistema de visualizacion y efectivamente es coherente con la agrupaci\u00f3n del Kmedias.**","7722c2d3":"De los anteriores gr\u00e1ficos, puedo extraer el siguiente an\u00e1lisis:\n\n* **Matriz de correlaciones entre las distintas variables de la tabla:**\n \n Hay correlaciones muy fuertes entre las variables, especialmente entre las de **probabilidad y la duraci\u00f3n** por un lado y **el precio y el descuento** por otro. Es por estas correlaciones, por lo que he definido las siguientes gr\u00e1ficas\n\n* **Comparaci\u00f3n entre la distribuci\u00f3nde prob_food y prob_drink, permite ver bimodalidad:**\n\n Ambas distribuciones tienen **una moda** (prob_drink en torno al 0.3 y prob_food entrono al 0.6) y un **punto con frecuencia bastante alta** (prob_drink en torno al 0.9 y prob_food entrono al 0.1) lo que puede ser un **primer indicio de la existencia de grupos y deque estas dos variables son utiles para determinar la pertenencia a un grupo o a otro.**\n\n* **Histograma de la distribuci\u00f3n del precio:**\n\n Claramente se puede apreciar como **hay 3 grupos de precio completamente diferenciados** (de 25 a 50 euros, de 90 a 115 euros y de 180 a 210 euros) sin tener valores fuera de estos grupos, lo que es una clara evidencia de que es una variable perfecta para aplicar al algoritmo de clustering.\n\n* **Gr\u00e1fico de dispersi\u00f3n entre prob_food y prob_drink, para ver si podr\u00eda haber grupos:**\n\n Se pueden ver en torno a **3 grupos de valores**, aquellas campa\u00f1as 'puras', ya sea de bebida(abajo a la derecha) o de comida (arriba a la izquierda) y aquellas que son mixtas o que el NLP no ha sabido clasificar correctamente (en el centro) que quizas pertenezcan a una tercera categor\u00eda (productos de limpieza por ejemplo)\n\n* **Gr\u00e1fico de dispersi\u00f3n entre prob_food y prob_drink, relacionado por duraci\u00f3n, para ver si podr\u00eda haber grupos:**\n\n Mismo gr\u00e1fico anterior pero teniendo en cuenta la duraci\u00f3n de la campa\u00f1a. Se ve como **las campa\u00f1as de comida, toman valores m\u00e1s oscuros y, por tanto, duran m\u00e1s tiempo y las campa\u00f1as de bebida toman valores m\u00e1s claros y, por tanto, duran menos tiempo.**\n\n* **Gr\u00e1fico de dispersi\u00f3n entre prob_food y prob_drink, relacionado con precio, para ver si podr\u00eda haber grupos:**\n\n **Se puede ver perfectamente como simplemente esta agrupaci\u00f3n ya podr\u00eda suponer una agrupacion por clusters de por si.** Se ve como cad auno de lo 3 grupos descritos en esta gr\u00e1fica pertenece a un grupo de precio distinto de los vistos en el hitograma.\n\n* **Histogramas de comparaci\u00f3n de distribuci\u00f3n de precio  de descuento:**\n\n Por \u00faltimo, me parec\u00eda muy interesante ver este gr\u00e1fico debido a la fuerte correlacion negativa entre el precio y el descuento de una campa\u00f1a (ya tenia esta idea en la cabeza cuando vi que estas dos variables eran exactamente iguales en una primera tabla de datos). Se puede ver como claramente **las campa\u00f1as de productos con un mayor precio tienen un descuento muy inferior a aquellas campa\u00f1as con productos m\u00e1s baratos, cuyo descuento es mayor.** Es muy relevante lo significativo de estas variables, y como pueden servir como discriminatorias a la hora de agrupar en el K medias.","1ed44ece":"Como se puede apreciar, **me queda una tabla con la siguiente estructura:**\n\n* **Customer**: con la identificaci\u00f3n de cada cliente en el registro correspondiente (tiene tan solo los ID de los clientes de train excluyendo los outliers)\n\n* **Cluster_customer**: Con el cluster al que pertenece cada cliente seg\u00fan el K medias aplicado ( del 1 al 3)\n\n* **Result**: Resultado de aplicar la campa\u00f1a correspondiente a ese cliente en concreto, pudiendo ser 1 o 0\n\n* **Campaign**: Identificaci\u00f3n de la campa\u00f1a en el registro correspondiente (del 0 al 99)\n\n* **Cluster_campaign**: Cluster al que pertenece cada campa\u00f1a en funci\u00f3n del K medias realizado (del 1 al 4)\n\n* **Unos**: Vector de unos que servir\u00e1 en el groupby() para contar sin necesidad de realizar otras funciones.","6acfd4c0":"**En esta tabla se puede ver los valores que toma cada uno de los 5 individuos generados**, son valores factibles matematicamente pero no realmente, ya que si que pertenecen al espacio dimensional.\n\nHe utilizado los 5 centroides de la ultima estimaci\u00f3n del K medias que he hecho (con 5 clusters), de tal manera que **son puntos factibles, pero que no son el centroide exacto de los 3 grupos, as\u00ed tengo algo de variedad y no simplemente un individuo para cada uno de los 3 grupos** (es decir, la predicci\u00f3n que hago ahora, la someto a 3 grupos, siendo que los centroides se han determinado con 5 grupos). **Si cogiese los centroides de 3 grupos, tambien ser\u00edan factibles, pero no estar\u00eda prediciendo nada nuevo, me dar\u00eda que pertenece cada uno a un grupo (al ser el centroide de ese mismo grupo)**\n\nHe utilizado el Kmedias de la pregunta 2 para predecir el cluster al que pertenece cada uno de ellos y he registrado esa variable en la ultima columna.\n\n**A continuaci\u00f3n he creado una tabla de doble entrada que representa todas las combinaciones posibles de cluster-campa\u00f1a y las probabilidades de \u00e9xito de cada combinacion segun los datos de entrenamiento** (la diferencia con los datos de test es minima,por lo que podr\u00eda haberlos usado con resultados muy similares"}}