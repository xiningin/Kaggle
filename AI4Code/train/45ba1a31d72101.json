{"cell_type":{"6e4a742e":"code","dd45ee7b":"code","97bdf782":"code","6aa042a3":"code","a67a980e":"code","fcf76118":"code","9641370e":"code","5d866da3":"code","ae748b84":"code","1e8ae379":"code","45183d8b":"code","0059a999":"code","fe421e11":"code","6ebfda91":"code","60cf44a8":"code","1e2dd7a9":"code","807f90f4":"code","30559192":"code","c2751dcb":"code","b65ad0a9":"code","3faec83e":"code","182fcb3b":"code","b838a21d":"code","464c7562":"code","7a7177f2":"markdown","0d22e200":"markdown","91fd34ea":"markdown","2bff8b92":"markdown","1374fedc":"markdown","133b5ed3":"markdown","47a0b230":"markdown","7007731b":"markdown","1aaf7aa7":"markdown","6bd861d5":"markdown","7afa2bba":"markdown","45b8529d":"markdown","c449a6a9":"markdown","7342132f":"markdown","1a288e90":"markdown","3cc66c82":"markdown","bced52c0":"markdown","858fbae3":"markdown","b0a8ed48":"markdown","9a585682":"markdown","b2c08670":"markdown","fcfcf93a":"markdown","cd9cb128":"markdown","f5cd074d":"markdown","c88a6ee9":"markdown","39192642":"markdown","b4d1cdcd":"markdown","191fc57b":"markdown","23210f21":"markdown","29ebcae4":"markdown","6fa8f1fe":"markdown"},"source":{"6e4a742e":"!pip install pyttsx3 #used for speech output\n!pip install SpeechRecognition #used for speech input\n!pip install pyspellchecker #spelling checker","dd45ee7b":"import nltk\nfrom spellchecker import SpellChecker\nimport urllib\nimport bs4 as bs\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport random\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport string \nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport numpy as np\nimport os\nfrom pandas import DataFrame\nimport pyttsx3 \nimport speech_recognition as sr\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('popular', quiet=True) ","97bdf782":"page1=requests.get('https:\/\/www.timeanddate.com\/weather\/india') #switch on the Internet option from right side window","6aa042a3":"\n\ndef temp(topic):\n    \n    page = page1\n    soup = BeautifulSoup(page.content,'html.parser')\n\n    data = soup.find(class_ = 'zebra fw tb-wt zebra va-m')\n\n    tags = data('a')\n    city = [tag.contents[0] for tag in tags]\n    tags2 = data.find_all(class_ = 'rbi')\n    temp = [tag.contents[0] for tag in tags2]\n\n    indian_weather = pd.DataFrame(\n    {\n        'City':city,\n        'Temperature':temp\n    }\n    )\n    \n    df = indian_weather[indian_weather['City'].str.contains(topic.title())] \n    \n    return (df['Temperature'])","a67a980e":"def wiki_data(topic):\n    \n    topic=topic.title()\n    topic=topic.replace(' ', '_',1)\n    url1=\"https:\/\/en.wikipedia.org\/wiki\/\"\n    url=url1+topic\n\n    source = urllib.request.urlopen(url).read()\n\n    # Parsing the data\/ creating BeautifulSoup object\n    soup = bs.BeautifulSoup(source,'lxml')\n\n    # Fetching the data\n    text = \"\"\n    for paragraph in soup.find_all('p'):\n        text += paragraph.text\n\n    import re\n    # Preprocessing the data\n    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n    text = re.sub(r'\\s+',' ',text)\n    text = text.lower()\n    text = re.sub(r'\\d',' ',text)\n    text = re.sub(r'\\s+',' ',text)\n    \n    \n    return (text)","fcf76118":"def rem_special(text):\n    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n    return(text.translate(remove_punct_dict))\n\nsample_text=\"I am sorry! I don't understand you.\"\nrem_special(sample_text)","9641370e":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize \n\ndef stemmer(text):\n    words = word_tokenize(text) \n    for w in words:\n        text=text.replace(w,PorterStemmer().stem(w))\n    return text\n\nstemmer(\"He is Eating. He played yesterday. He will be going tomorrow.\")","5d866da3":"lemmer = WordNetLemmatizer()\ndef LemTokens(tokens):\n    return [lemmer.lemmatize(token) for token in tokens]\n\nsample_text=\"rocks corpora better\" #default noun\nLemTokens(nltk.word_tokenize(sample_text))","ae748b84":"from nltk.tokenize.toktok import ToktokTokenizer\ntokenizer = ToktokTokenizer()\n\nstopword_list = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\nremove_stopwords(\"This is a sample sentence, showing off the stop words filtration.\")","1e8ae379":"import spacy \nspacy_df=[]\nspacy_df1=[]\ndf_spacy_nltk=pd.DataFrame()\nnlp = spacy.load('en_core_web_sm') \n  \n# Process whole documents \nsample_text = (\"The heavens are above. The moral code of conduct is above the civil code of conduct\") \ndoc = nlp(sample_text) \n  \n# Token and Tag \nfor token in doc:\n    spacy_df.append(token.pos_)\n    spacy_df1.append(token)\n\n\ndf_spacy_nltk['origional']=spacy_df1\ndf_spacy_nltk['spacy']=spacy_df\n#df_spacy_nltk","45183d8b":"import spacy \nnlp = spacy.load('en_core_web_sm') \n\ndef ner(sentence):\n    doc = nlp(sentence) \n    for ent in doc.ents: \n        print(ent.text, ent.label_) \n    \n\nsentence = \"A gangster family epic set in 1919 Birmingham, England; centered on a gang who sew razor blades in the peaks of their caps, and their fierce boss Tommy Shelby.\"\nner(sentence) ","0059a999":"from textblob import TextBlob\n\ndef senti(text):\n    testimonial = TextBlob(text)\n    return(testimonial.polarity)\n\nsample_text=\"This apple is good\"\nprint(\"polarity\",senti(sample_text))\nsample_text=\"This apple is not good\"\nprint(\"polarity\",senti(sample_text))","fe421e11":"\nspell = SpellChecker()\n\n\ndef spelling(text):\n    splits = sample_text.split()\n    for split in splits:\n        text=text.replace(split,spell.correction(split))\n        \n    return (text)\n    \n    \nsample_text=\"hapenning elephnt texte luckno sweeto\"\nspelling(sample_text)","6ebfda91":"#TOkenisation\nprint(nltk.sent_tokenize(\"Hey how are you? I am fine.\"))\nprint(nltk.word_tokenize(\"Hey how are you? I am fine.\"))","60cf44a8":"city = {} \ncity[\"bangalore\"]=[\"bangalore\",\"bengaluru\",\"blr\"]\ncity[\"lucknow\"] = [\"lucknow\", \"lko\"]\ncity[\"delhi\"]=[\"new delhi\",\"ndls\",\"delhi\"]","1e2dd7a9":"def city_name(sentence):\n    for word in sentence.split():\n        for key, values in city.items():\n            \n            if word.lower() in values:\n                return(key)\n                \n    \ncity_name(\"blr\")","807f90f4":"from sklearn.feature_extraction.text import TfidfVectorizer\ndocumentA = 'This is about Messi'\ndocumentB = 'This is about TFIDF'\nvectorizer = TfidfVectorizer()\nvectors = vectorizer.fit_transform([documentA, documentB])\nfeature_names = vectorizer.get_feature_names()\ndense = vectors.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)\ndf","30559192":"def LemNormalize(text):\n    text=rem_special(text) #remove special char\n    text=text.lower() # lower case\n    text=remove_stopwords(text) # remove stop words\n    \n    return LemTokens(nltk.word_tokenize(text))","c2751dcb":"#Generating answer\ndef response(user_input):\n    \n    ToGu_response=''\n    sent_tokens.append(user_input)\n    \n    \n    \n    word_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')   \n    all_word_vectors = word_vectorizer.fit_transform(sent_tokens)  \n    \n   \n    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors) \n    idx=similar_vector_values.argsort()[0][-2]\n    \n\n    matched_vector = similar_vector_values.flatten()\n    matched_vector.sort()\n    vector_matched = matched_vector[-2]\n    \n    if(vector_matched==0):\n        ToGu_response=ToGu_response+\"I am sorry! I don't understand you.\"\n        return ToGu_response\n    else:\n        ToGu_response = ToGu_response+sent_tokens[idx]\n        return ToGu_response\n","b65ad0a9":"#topic=str(input(\"Please enter the city name you want to ask queries for: \")) #Uncomment this line to take input from command prompt or jupyter notebook.\ntopic=\"bangalore\" # sample city\n\ntopic=city_name(topic) # fetch city name in case of invalid input or any discrepancy in the city name\n\ntext=wiki_data(topic) # fetch wiki data about city\n\nsent_tokens = nltk.sent_tokenize(text)# converts to list of sentences \nword_tokens = nltk.word_tokenize(text)# converts to list of words\nweather_reading=(temp(topic)).iloc[0] #fetch weather","3faec83e":"# greetings Keyword matching\nGREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\nGREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n\ndef greeting(sentence):\n    for word in sentence.split():\n        if word.lower() in GREETING_INPUTS:\n            return random.choice(GREETING_RESPONSES)","182fcb3b":"PLACES_INPUTS = (\"places\", \"monuments\", \"buildings\",\"places\", \"monument\", \"building\")\n\nimport spacy \nnlp = spacy.load('en_core_web_sm') \n\ndef ner(sentence):\n    places_imp=\"\"\n    doc = nlp(sentence) \n    for ent in doc.ents: \n        if (ent.label_==\"FAC\"):\n            #print(ent.text, ent.label_) \n            places_imp=places_imp+ent.text+\",\"+\" \"\n            \n    return(places_imp)\n    \n\nplaces_imp=ner(text) \n\n\ns=places_imp\nl = s.split() \nk = [] \nfor i in l: \n  \n    # If condition is used to store unique string  \n    # in another list 'k'  \n    if (s.count(i)>1 and (i not in k)or s.count(i)==1): \n        k.append(i) \n\nPLACES_RESPONSES = ' '.join(k)\n\ndef places(sentence):\n    for word in sentence.split():\n        if word.lower() in PLACES_INPUTS:\n            return (PLACES_RESPONSES)","b838a21d":"WEATHER_INPUTS = (\"weather\", \"temp\", \"temperature\")\n\nWEATHER_RESPONSES =weather_reading\n\ndef weather(sentence):\n    for word in sentence.split():\n        if word.lower() in WEATHER_INPUTS:\n            return (WEATHER_RESPONSES)","464c7562":"continue_dialogue=True\nprint(\"ToGu: Hello\")\n#speak(\"Hello\")\n\nwhile(continue_dialogue==True):\n    user_input = input(\"User:\")\n    user_input=user_input.lower()\n    user_input=spelling(user_input) #spelling check\n    print(\"Sentiment score=\",senti(user_input)) #sentiment score\n    \n    if(user_input!='bye'):\n        if(user_input=='thanks' or user_input=='thank you' ):\n            print(\"ToGu: You are welcome..\")\n            #speak(\" You are welcome\")\n            \n        else:\n            if(greeting(user_input)!=None):\n                tmp=greeting(user_input)\n                print(\"ToGu: \"+tmp)\n                #speak(tmp)\n                \n            elif(weather(user_input)!=None):\n                tmp=weather(user_input)\n                print(\"ToGu: \"+tmp)\n                #speak(tmp)\n                \n                \n            elif(places(user_input)!=None):\n                tmp=places(user_input)\n                print(\"ToGu: Important places are \"+tmp)\n                #speak(\"Important places are\")\n                #speak(tmp)\n                \n            else:\n                print(\"ToGu: \",end=\"\")\n                temp=response(user_input)\n                print(temp) \n                #speak(temp)\n                sent_tokens.remove(user_input)\n                \n\n    else:\n        continue_dialogue=False\n        print(\"ToGu: Goodbye.\")\n        #speak(\"goodbye\")\n        \n\n\n","7a7177f2":"# Conclusion\nThank you for going through full notebook. Feel free to write comments and suggestions. If you want any particular topic to be added in this notebook plz let me know.\n\n# **Upvote my notebook.**\n\n*Links*\n\n[Github](https:\/\/github.com\/manzoormahmood\/Tourist-Guide-Chatbot)\n\n[Youtube](https:\/\/youtu.be\/DQl79__3xKU)\n\n[Linkedin](https:\/\/www.linkedin.com\/in\/manzoor-bin-mahmood\/)","0d22e200":"### Tokenization\nIt is the process of breaking sentence or word into tokens.","91fd34ea":"### Places\nThis piece of code extracts the city details from corpus using name entity recognition(NER). As told earlier \"FAC\" is used for airports, city. This feature from spcay is used to find monuments and important tourist places in city.","2bff8b92":"### Pre-processing all","1374fedc":"### Stemming\nStrip suffixes from the end of the word. \n\nEating --> Eat\n\nGoing --> Go","133b5ed3":"# Chatbot Tourist Guide (ToGu)\n\nView the source code on [Github](https:\/\/github.com\/manzoormahmood\/Tourist-Guide-Chatbot)\n\nWatch the working chatbot video on [Youtube](https:\/\/youtu.be\/DQl79__3xKU)\n\nLets get connected on [Linkedin](https:\/\/www.linkedin.com\/in\/manzoor-bin-mahmood\/)\n\n### Upvote my work if you like it.","47a0b230":"### Removing special char\nUsing this function all the special char are removed.","7007731b":"### Sentiment Analysis\nThe sentiment returns polarity. The polarity score is a float within the range [-1.0, 1.0]. ","1aaf7aa7":"### Name Entity Recognition (NER)\nspaCy supports the following entity types:\n\nPERSON\n\nNORP (nationalities, religious and political groups)\n\nFAC (buildings, airports etc.)\n\nORG (organizations)\n\nGPE (countries, cities etc.)\n\nLOC (mountain ranges, water bodies etc.)\n\nPRODUCT (products)\n\nEVENT (event names)\n\nWORK_OF_ART (books, song titles)\n\nLAW (legal document titles)\n\nLANGUAGE (named languages)\n\nDATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL and CARDINAL.\n\n","6bd861d5":"### Importing lib","7afa2bba":"### Chat\nThis is the main chat function. Sentiment score is for finding the sentiment of the user input. According to user sentiment we can reply to the user. \n\nspeak() function is used for speech output from chatbot. (Run on your local system)\n","45b8529d":"## Text Cleaning\nNow comes the most important part of nlp i.e. text cleaning. Without this we can't get useful results.\n* Removing special char\n* Stemming\n* Lemmatization\n* Stop words\n* Part of speech (POS)\n* Name entity recognition (NER)\n* Sentiment Analysis\n* Spelling checker\n* Tokenization\n* Creating dictionary for city names","c449a6a9":"### Lemmatization\nDetermine that the two words have same root.\n\nCorpus and Corpora--> Corpus (root)","7342132f":"### Weather details about city\nWeather details have been obtained from website using BeautifulSoup as a scrapping tool. Alternatively, other websites can also be scrapped for getting more detailed information about the weather. ","1a288e90":"## What is a chatbot and its application\nA chatbot is an artificial intelligence software that can simulate a conversation with a user in natural language. A chatbot is used for interactions between people and services to enhance customer experience.\n\nOne of the important use of chatbot is as a tourist guide. It will help in providing information about an unknown city to the user. Information like important monuments in the city, famous foods, travel mode, etc. can be found easily.","3cc66c82":"*Note: TF-IDF is basic for word embedding with less accuracy. Other methods like CBOW, skip-gram can be used for better accuracy.If this notebook gets good response then I will add frequency also.*","bced52c0":"## Text Gathering\nThe time-consuming part of a chatbot is making the text corpus. For making a chatbot, details regarding the city have to be obtained. Information like weather details, monuments, food, travel mode, etc may be needed as a text corpus.\n\nFor this chatbot, we have obtained weather information from timeanddate.com and other details are obtained from Wikipedia.","858fbae3":"### Scrape wiki for city details\n\nScrape city details from wiki. Again using BeautifulSoup.","b0a8ed48":"## Generating answer\nThis section combines all the codes and generated the output.","9a585682":"### Creating dictionary for cities\nFor chatbot the user may input city name in many form. For example \"bangalore\" can be referred as \"bangalore\", \"bengaluru\" or \"blr\". The chatbot shoud be able to identify to which city the user is referring to.","b2c08670":"### Cosine similarity","fcfcf93a":"### TF-IDF","cd9cb128":"### Part of Speech (POS)\nThe part-of-speech tag signifies whether the word is a noun, adjective, verb, and so on. Refer [this](https:\/\/spacy.io\/usage\/linguistic-features) page for more details","f5cd074d":"### Stop Words\nStop words are very frequently appearing words like 'a' , 'the', 'is'.","c88a6ee9":"## Word Embedding\nWord embedding is the representation of word so that it can be feed as input the machine learning models. ML model take input only in form of numerical values, so words are converted to vector form.\n\nThere are two type of word embedding. \n1. Frequency based: Count vector, TF-IDF\n2. Prediction based. Continous bag of words (CBOW), skip-gram.","39192642":"### Get city name\nTake input from user. Then fetch wiki data and weather details.","b4d1cdcd":"### Weather\nGetting weathe details when user inputs any input among variable WEATHER_INPUTS.","191fc57b":"### Greeting\nGenerate greeting when user give any input among the variable GREETING_INPUTS.","23210f21":"### Spelling checker\nCheck for any incorrect spelling in text and find the correct spelling.\n","29ebcae4":"## Setting up Environment\n\n### Installing lib not included in kaggle(Using Internet)\n* pyttsx3 is used for getting answer from chatbot as a speech.\n* SpeechRecognition is used for speech input from user.\n* pyspellchecker is used for spelling checking ","6fa8f1fe":"## Step for getting started with any kind of chatbot.\n\n* Setting up Environment\n* Text Gathering\n* Text cleaning\n* Word Embedding\n* Generating answer\n* Conversation"}}