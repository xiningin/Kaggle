{"cell_type":{"cc2593bf":"code","f7477270":"markdown","53014bd4":"markdown","0cccb90d":"markdown","95825ed6":"markdown","e2952d38":"markdown","574e9cf6":"markdown","989fab82":"markdown","4bfbe6b2":"markdown","50ea6ee5":"markdown","2bfe1de1":"markdown","5ec2f078":"markdown","cd98d17f":"markdown","157598de":"markdown","73c56cf8":"markdown","f58f36b0":"markdown","a45ac12e":"markdown","fa9530cb":"markdown","a3ee0c90":"markdown","9df3692a":"markdown","f4756034":"markdown","ce96de9b":"markdown","53895d90":"markdown","8881db59":"markdown","954ed8b2":"markdown","2a471dd7":"markdown","e4fa4fd4":"markdown"},"source":{"cc2593bf":"def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n    # first layer\n    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    # second layer\n    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n    if batchnorm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    \n    return x\n  \ndef get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n    # Contracting Path\n    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n    p1 = MaxPooling2D((2, 2))(c1)\n    p1 = Dropout(dropout)(p1)\n    \n    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n    p2 = MaxPooling2D((2, 2))(c2)\n    p2 = Dropout(dropout)(p2)\n    \n    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n    p3 = MaxPooling2D((2, 2))(c3)\n    p3 = Dropout(dropout)(p3)\n    \n    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n    p4 = MaxPooling2D((2, 2))(c4)\n    p4 = Dropout(dropout)(p4)\n    \n    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n    \n    # Expansive Path\n    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n    u6 = concatenate([u6, c4])\n    u6 = Dropout(dropout)(u6)\n    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n    \n    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n    u7 = concatenate([u7, c3])\n    u7 = Dropout(dropout)(u7)\n    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n    \n    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n    u8 = concatenate([u8, c2])\n    u8 = Dropout(dropout)(u8)\n    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n    \n    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n    u9 = concatenate([u9, c1])\n    u9 = Dropout(dropout)(u9)\n    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n    \n    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n    model = Model(inputs=[input_img], outputs=[outputs])\n    return model","f7477270":"![UNet](https:\/\/miro.medium.com\/max\/1800\/1*Rmo71TyPxvLLM4FnlH5MDg.jpeg)","53014bd4":"# Key Learnings from this Notebook\n\n* What is UNet Architecture?\n* Working and Breakdown of UNet Architecture.\n* Practicle Implementation of UNet.\n* Applications of UNet Architecture.","0cccb90d":"# Applications of UNet Architecture","95825ed6":"In the original paper, the UNET is described as follows:\n\n![](https:\/\/miro.medium.com\/max\/875\/1*OkUrpDD6I0FpugA_bbYBJQ.png)\n\n\nIf you did not understand, its okay. I will try to describe this architecture much more intuitively. Note that in the original paper, the size of the input image is 572x572x3, however, we will use input image of size 128x128x3. Hence the size at various locations will differ from that in the original paper but the core components remain the same.\n\nBelow is the detailed explanation of the architecture:\n\n![](https:\/\/miro.medium.com\/max\/1250\/1*yzbjioOqZDYbO6yHMVpXVQ.jpeg)","e2952d38":"Code for UNet Architecure can be easily implemented in python from scratch, like so: ","574e9cf6":"Before diving into working of UNet, let's first learn about what exactly is Convolution and Dec-convolution operations.\n\nJust hang tight because if you love Deep Learning, this is going to be very entertaining ;)","989fab82":"**Short summary on working of UNet**\n\nIn short if I had to explain the working of UNet, it will go something like this:\n\nImage --> Convolution Network (Down-samples the image by convolution. Convolution in its core, reduces the dimensionality of the image by applying the various filters. On different levels of convolutions, we get images that are more fine-grained. Image becomes more and more fine grained with the level of convolution) --> Deconvolution Network (Upsamples the image by de-convolution operations. If you remember the Deconvolution operation explained above, it upsample the image by the use of appropriately sized de-convolution matrix. Then it makes a skip connection to its corresponding level of convolution layer and performs a convolution operation.) --> Segmented Image.\n\nThe image below would help cement the working of UNet Architecture:\n\n![image.png](attachment:image.png)","4bfbe6b2":"The transposed convolution operation forms the same connectivity as the normal convolution but in the backward direction.\nWe can use it to conduct up-sampling. Moreover, the weights in the transposed convolution are learnable. So we do not need a predefined interpolation method.\n\nEven though it is called the transposed convolution, it does not mean that we take some existing convolution matrix and use the transposed version. The main point is that the association between the input and the output is handled in the backward fashion compared with a standard convolution matrix (one-to-many rather than many-to-one association).\n\nAs such, the transposed convolution is not a convolution. But we can emulate the transposed convolution using a convolution. We up-sample the input by adding zeros between the values in the input matrix in a way that the direct convolution produces the same effect as the transposed convolution. You may find some article explains the transposed convolution in this way. However, it is less efficient due to the need to add zeros to up-sample the input before the convolution.\n","50ea6ee5":"# Working and Breakdown of UNet Architecture.","2bfe1de1":"This notebook is a one stop destination to learn everything about **UNet Architecture**.","5ec2f078":"# Practicle Implementation of UNet.","cd98d17f":"# Convolution Operation\n\nLet\u2019s use a simple example to explain how convolution operation works. Suppose we have a 4x4 matrix and apply a convolution operation on it with a 3x3 kernel, with no padding, and with a stride of 1. As shown further below, the output is a 2x2 matrix.\n\n![](https:\/\/miro.medium.com\/max\/875\/1*NoXQbZqPnxSnjdAwo93XcQ.png)\n\n\nThe convolution operation calculates the sum of the element-wise multiplication between the input matrix and kernel matrix. Since we have no padding and the stride of 1, we can do this only 4 times. Hence, the output matrix is 2x2.\n\n\n\n![](https:\/\/miro.medium.com\/max\/875\/1*M33WSDDeOSx6nbUZ0sbkxQ.png)\n\n\nOne important point of such convolution operation is that the positional connectivity exists between the input values and the output values.\n\nFor example, the top left values in the input matrix affect the top left value of the output matrix.\nMore concretely, the 3x3 kernel is used to connect the 9 values in the input matrix to 1 value in the output matrix. A convolution operation forms a many-to-one relationship. Let\u2019s keep this in mind as we need it later on.","157598de":"# Convolution Matrix\n\n\nWe can express a convolution operation using a matrix. It is nothing but a kernel matrix rearranged so that we can use a matrix multiplication to conduct convolution operations.\n\n![](https:\/\/miro.medium.com\/max\/243\/1*0wFFJUNHLRPd3r8R6WT8ng.png)\n\nWe rearrange the 3x3 kernel into a 4x16 matrix as below:\n\n![](https:\/\/miro.medium.com\/max\/875\/1*LKnTr_0k409vOjgj2h4-vg.png)\n\nThis is the convolution matrix. Each row defines one convolution operation. If you do not see it, the below diagram may help. Each row of the convolution matrix is just a rearranged kernel matrix with zero padding in different places.\n\n![](https:\/\/miro.medium.com\/max\/875\/1*yLMY-HCEGg2r7IHevR48oA.png)\n\n\nTo use it, we flatten the input matrix (4x4) into a column vector (16x1).\n\n![](https:\/\/miro.medium.com\/max\/500\/1*0_oqO0AFZBigpBxPcJ7c_A.png)\n\nWe can matrix-multiply the 4x16 convolution matrix with the 16x1 input matrix (16 dimensional column vector).\n\n![](https:\/\/miro.medium.com\/max\/875\/1*ql2ZxrS_h8D7KHNCrGndug.png)\n\nThe output 4x1 matrix can be reshaped into a 2x2 matrix which gives us the same result as before.\n\n![](https:\/\/miro.medium.com\/max\/174\/1*YZwIXPPyb_AsKmxn_em42Q.png)\n\nIn short, a convolution matrix is nothing but an rearranged kernel weights, and a convolution operation can be expressed using the convolution matrix.\n\nSo what?\n\nThe point is that with the convolution matrix, you can go from 16 (4x4) to 4 (2x2) because the convolution matrix is 4x16. Then, if you have a 16x4 matrix, you can go from 4 (2x2) to 16 (4x4).","73c56cf8":"# References\n\n* https:\/\/arxiv.org\/abs\/1505.04597\n* https:\/\/towardsdatascience.com\/understanding-semantic-segmentation-with-unet-6be4f42d4b47#:~:text=8.-,UNET%20Architecture%20and%20Training,for%20Bio%20Medical%20Image%20Segmentation.&text=Thus%20it%20is%20an%20end,accept%20image%20of%20any%20size.\n* https:\/\/naokishibuya.medium.com\/up-sampling-with-transposed-convolution-9ae4f2df52d0","f58f36b0":"# Complete UNet Architecure Overview","a45ac12e":"You can find a lot of applications of UNet online about image segmentations. But I will tell you an application that I myself had a chance to work on in my professional career.\nThat is: Segmentation model to seperate out vocals from songs, so that we are left with only pure music(instrumentation signal).\n\nTry to implement this on you own, I swear it will be a lot of fun !! ;)","fa9530cb":"# DeConvolution Layer in Neural Networks\n\n**Deconvolution Network for Segmentation**\n\nWe now discuss two main operations, unpooling and deconvolution, in our deconvolution network in details.\n\n**Unpooling**\n\nPooling in convolution network is designed to filter noisy activations in a lower layer by abstracting activations in a receptive field with a single representative value. Although it helps classification by retaining only robust activations in upper layers, spatial information within a receptive field is lost during pooling, which may be critical for precise localization that is required for semantic segmentation.\n\nTo resolve such issue, we employ unpooling layers in deconvolution network, which perform the reverse operation of pooling and reconstruct the original size of activations as illustrated in Figure 3. To implement the unpooling operation, we follow the similar approach proposed in [24, 25]. It records the locations of maximum activations selected during pooling operation in switch variables, which are employed to place each activation back to its original pooled location. This unpooling strategy is particularly useful to reconstruct the structure of input object as described.\n\n\n**Deconvolution**\n\nThe output of an unpooling layer is an enlarged, yet sparse activation map. The deconvolution layers densify the sparse activations obtained by unpooling through convolution-like operations with multiple learned filters. However, contrary to convolutional layers, which connect multiple input activations within a filter window to a single activation, deconvolutional layers associate a single input activation with multiple outputs, as illustrated in Figure 3. The output of the deconvolutional layer is an enlarged and dense activation map. We crop the boundary of the enlarged activation map to keep the size of the output map identical to the one from the preceding unpooling layer.\n\nThe learned filters in deconvolutional layers correspond to bases to reconstruct shape of an input object. \nTherefore, similar to the convolution network, a hierarchical structure of deconvolutional layers are used to capture different level of shape details. The filters in lower layers tend to capture overall shape of an object while the class-specific fine details are encoded in the filters in higher layers. In this way, the network directly takes class-specific shape information into account for semantic segmentation, which is often ignored in other approaches based only on convolutional layers.","a3ee0c90":"# Going Backward\n\nNow, suppose we want to go the other direction. We want to associate 1 value in a matrix to 9 values in another matrix. It\u2019s a one-to-many relationship. This is like going backward of convolution operation, and it is the core idea of transposed convolution.\nFor example, we up-sample a 2x2 matrix to a 4x4 matrix. The operation maintains the 1-to-9 relationship.\n\n![](https:\/\/miro.medium.com\/max\/875\/1*4a4OjlszAvi7-vqjOT0PoA.png)\n\nBut how do we perform such operation?\n\nTo talk about how, we need to define the convolution matrix and the transposed convolution matrix.\n","9df3692a":"# Please remember to **upvote** the notebook if you like the content. It is always a great motivation :)","f4756034":"UNet model can be created like above. You can also modify the architecture as per your experimentation needs as compare the results.","ce96de9b":"Points to note:\n\n* 2@Conv layers means that two consecutive Convolution Layers are applied\n\n* c1, c2, \u2026. c9 are the output tensors of Convolutional Layers\n\n* p1, p2, p3 and p4 are the output tensors of Max Pooling Layers\n\n* u6, u7, u8 and u9 are the output tensors of up-sampling (transposed convolutional) layers\n\n* The left hand side is the contraction path (Encoder) where we apply regular convolutions and max pooling layers.\n\n* In the Encoder, the size of the image gradually reduces while the depth gradually increases. Starting from 128x128x3 to 8x8x256\n\n* This basically means the network learns the \u201cWHAT\u201d information in the image, however it has lost the \u201cWHERE\u201d information\n\n* The right hand side is the expansion path (Decoder) where we apply transposed convolutions along with regular convolutions\n\n* In the decoder, the size of the image gradually increases and the depth gradually decreases. Starting from 8x8x256 to 128x128x1\n\n* Intuitively, the Decoder recovers the \u201cWHERE\u201d information (precise localization) by gradually applying up-sampling\n\n* To get better precise locations, at every step of the decoder we use skip connections by concatenating the output of the transposed convolution layers with the feature maps from the Encoder at the same level:\n  u6 = u6 + c4\n  u7 = u7 + c3\n  u8 = u8 + c2\n  u9 = u9 + c1\n  \n  After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output\n\n* This is what gives the architecture a symmetric U-shape, hence the name UNET\n\n* On a high level, we have the following relationship:\n  Input (128x128x1) => Encoder =>(8x8x256) => Decoder =>Ouput (128x128x1)\n  Below is the Keras code to define the above model:\n","53895d90":"The architecture consists of a **contracting path** to capture context and a **symmetric expanding** path that enables precise localization. \n\nThe above statement precisely in very simple words explains the whole idea behind the UNet. UNet breaks the images down with the Convolution operations and re-creates it again with De-convolution operations.","8881db59":"**Now that the basics are clear, we can get back to the UNet Architecture**","954ed8b2":"# Transposed Convolution Matrix\n\nWe want to go from 4 (2x2) to 16 (4x4). So, we use a 16x4 matrix. But there is one more thing here. We want to maintain the 1 to 9 relationship.\n\nSuppose we transpose the convolution matrix C (4x16) to C.T (16x4). We can matrix-multiply C.T (16x4) with a column vector (4x1) to generate an output matrix (16x1). The transposed matrix connects 1 value to 9 values in the output.\n\n![](https:\/\/miro.medium.com\/max\/875\/1*JDAuBt3aS9mz3aQQ7JKYKA.png)\n\nThe output can be reshaped into 4x4.\n\n![](https:\/\/miro.medium.com\/max\/310\/1*STkqLI87Q8qlO1gxpG6sJA.png)\n\nWe have just up-sampled a smaller matrix (2x2) into a larger one (4x4). The transposed convolution maintains the 1 to 9 relationship because of the way it lays out the weights.\n\nNB: the actual weight values in the matrix does not have to come from the original convolution matrix. What\u2019s important is that the weight layout is transposed from that of the convolution matrix.","2a471dd7":"U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. \n\nThe U-Net architecture stems from the so-called \u201c**fully convolutional network**\u201d first proposed by Long, Shelhamer, and Darrell.\n\nThe main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. What's more, a successive convolutional layer can then learn to assemble a precise output based on this information.\n\nOne important modification in U-Net is that there are a large number of feature channels in the **upsampling** part, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting part, and **yields a u-shaped architecture**. The network only uses the valid part of each convolution without any fully connected layers. To predict the pixels in the border region of the image, the **missing context is extrapolated** by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.","e4fa4fd4":"# What is UNet Architecture?"}}