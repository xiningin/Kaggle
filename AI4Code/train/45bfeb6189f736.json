{"cell_type":{"dd0b192c":"code","9334a3c7":"code","5007e235":"code","fa45336b":"code","a5051576":"code","be1bd4a1":"code","91ef3318":"code","605c52e1":"code","c6e41b54":"code","ae16020b":"code","62e1eb79":"code","43dc1d5c":"code","cdf17a1a":"code","0d56d9b3":"code","c55c9f34":"code","f274dc71":"code","3ac1af90":"code","e824111c":"code","78296351":"code","f5cafa7d":"code","a5b30993":"code","a4dcdc80":"markdown","431448ba":"markdown","09b49dc1":"markdown","5ee4408d":"markdown","a3031bed":"markdown","b0dba173":"markdown","c0d21533":"markdown","566bee6c":"markdown","4cfd00f6":"markdown","86415585":"markdown","4dd33019":"markdown","0e2463f5":"markdown","9a47b50c":"markdown","e1d3e97e":"markdown","0fac148d":"markdown"},"source":{"dd0b192c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9334a3c7":"import csv\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","5007e235":"def get_data(filename):\n    with open(filename) as training_file:\n        training_reader = csv.reader(training_file, delimiter=',')\n        image = []\n        labels = []\n        line_count = 0\n        for row in training_reader:\n            if line_count == 0:\n                line_count +=1\n            else:\n                labels.append(row[0])\n                temp_image = row[1:785]\n                image_data_as_array = np.array_split(temp_image, 28)\n                image.append(image_data_as_array)\n                line_count += 1\n        images = np.array(image).astype('float')\n        labels = np.array(labels).astype('float')\n        print(f'Processed {line_count} lines.')\n\n    return images, labels","fa45336b":"training_images, training_labels = get_data(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\ntesting_images, testing_labels = get_data(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")\n\nprint(\"Total Training images\", training_images.shape)\nprint(\"Total Training labels\",training_labels.shape)\nprint(\"Total Testing images\",testing_images.shape)\nprint(\"Total Testing labels\",testing_labels.shape)","a5051576":"labels = {0 : \"T-shirt\/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}","be1bd4a1":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(8, 8),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img = training_images[i].reshape(28,28)\n    ax.imshow(img, cmap = 'gray')\n    title = labels[training_labels[i]]\n    ax.set_title(title, fontsize = 15)\nplt.tight_layout(pad=0.5)\nplt.show()","91ef3318":"# Display the distribution of each letter\n\nvc = pd.Series(training_labels).value_counts()\nplt.figure(figsize=(20,5))\nsns.barplot(x = sorted(vc.index), y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category\", fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.show()","605c52e1":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.998):\n      print(\"\\nReached 99.8% accuracy so cancelling training!\")\n      self.model.stop_training = True","c6e41b54":"training_images=training_images.reshape(60000, 28, 28, 1)\ntraining_images=training_images \/ 255.0\ntesting_images = testing_images.reshape(10000, 28, 28, 1)\ntesting_images=testing_images\/255.0","ae16020b":"# Define the model\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape = (28,28,1)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation = 'softmax')])","62e1eb79":"model.summary()","43dc1d5c":"# Compiling the Model\n\nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n             optimizer = tf.keras.optimizers.Adam(),\n              metrics = ['accuracy'])","cdf17a1a":"callbacks = myCallback()\nhistory = model.fit(train_datagenerator,\n                    validation_data = validation_datagenerator,\n                    steps_per_epoch = len(training_labels)\/\/128,\n                    epochs = 100,\n                    validation_steps = len(testing_labels)\/\/128,\n                    callbacks = [callbacks])","0d56d9b3":"import matplotlib.pyplot as plt\nfig.set_size_inches(16,12)\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","c55c9f34":"model.evaluate(testing_images, testing_labels, verbose=0)","f274dc71":"tf.keras.utils.plot_model(model,\n                          to_file=\"model.png\",\n                          show_shapes=True,\n                          show_dtype=False,\n                          show_layer_names=True,\n                          rankdir=\"TB\",                          \n                          expand_nested=True,\n                          dpi=200)","3ac1af90":"from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n\n# Predict the label of the test_images\n\npred = model.predict(testing_images)\npred = np.argmax(pred,axis=1)\n\n# Get the accuracy score\nacc = accuracy_score(testing_labels,pred)\n\n# Display the results\nprint(f'## {acc*100:.2f}% accuracy on the test set')","e824111c":"print(classification_report(testing_labels, pred))","78296351":"# Display confusion matrix\n\ncf_matrix = confusion_matrix(testing_labels, pred, normalize='true')\nplt.figure(figsize = (20,15))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(testing_labels)), yticklabels = sorted(set(testing_labels)),cbar=False)\nplt.title('Normalized Confusion Matrix\\n', fontsize = 23)\nplt.xlabel(\"Predicted Classes\",fontsize=15)\nplt.ylabel(\"True Classes\",fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15,rotation=0)\nplt.show()","f5cafa7d":"correct = np.nonzero(pred == testing_labels)[0]\nplt.figure(figsize=(9, 9))\ni = 0\nfor c in correct[:9]:\n    plt.subplot(3,3,i+1)\n    plt.imshow(testing_images[c].reshape(28,28), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted:{}, Actual:{}\".format(pred[c], testing_labels[c]))\n    plt.tight_layout()\n    i += 1","a5b30993":"incorrect = np.nonzero(pred != testing_labels)[0]\nplt.figure(figsize=(9, 9))\ni = 0\nfor c in incorrect[:6]:\n    plt.subplot(3,3,i+1)\n    plt.imshow(testing_images[c].reshape(28,28), cmap=\"gray\", interpolation='none')\n    plt.title(\"Predicted:{}, Actual:{}\".format(labels[pred[c]], labels[testing_labels[c]]))\n    plt.tight_layout()\n    i += 1","a4dcdc80":"## 7. Plotiing the losses","431448ba":"<h2> 2. Importing the libraries <\/h2> ","09b49dc1":"<h2> 11. References <\/h2>\n\n[1] Fashion MNIST, An MNIST-like dataset of 70,000 28x28 labeled fashion images, https:\/\/www.kaggle.com\/zalando-research\/fashionmnist\n\n[2] DanB, CollinMoris, Deep Learning From Scratch, https:\/\/www.kaggle.com\/dansbecker\/deep-learning-from-scratch\n\n[3] DanB, Dropout and Strides for Larger Models, https:\/\/www.kaggle.com\/dansbecker\/dropout-and-strides-for-larger-models\n\n[4] BGO, CNN with Keras, https:\/\/www.kaggle.com\/bugraokcu\/cnn-with-keras\n\n[5] NAIN, EagerFMINST, https:\/\/www.kaggle.com\/aakashnain\/eagerfmnist\n\n[6] Why Dropounts prevent overfitting in Deep Neural Networks, https:\/\/medium.com\/@vivek.yadav\/why-dropouts-prevent-overfitting-in-deep-neural-networks-937e2543a701\n\n[7] Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https:\/\/www.cs.toronto.edu\/~hinton\/absps\/JMLRdropout.pdf","5ee4408d":"Alternatively, you can clone this GitHub repository; the dataset appears under data\/fashion. This repo also contains some scripts for benchmark and visualization.\n\n> git clone git@github.com:zalandoresearch\/fashion-mnist.git <\n\n<h3> Let's begin !! <\/h3>","a3031bed":"## 6. Model Training","b0dba173":"<h2> 10. Results Visualization <\/h2>","c0d21533":"As you can see that there are 10 categories present in the labels, and each one is having equal no of images in the data. This is really balanced dataset. Not quite a real world scenario.\n\nNow we need to add another dimension in our images so that we can process it for the ImageDataGenerator and do the Image Augmentation Read more here","566bee6c":"## 4. EDA and Data Visualization","4cfd00f6":"## 8. Visualise the model","86415585":"<h2> 3. Reading the data <\/h2>","4dd33019":"<img src= \"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/758717\/1310306\/datasets\/fashion.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210626%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210626T180922Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=aeecdc26028875b14ad4f75b889f8ed576a01519953f3a28fa51d8d07a2a79e47678e0d82079105bc8b109cf5254394b6625fc0c3e6743ce180f8f9f5350db9b16c66a0c0065d02d9046e0efc2ee0353a4aea66cbec0f5b465529c09ea784836f9a553c3ae654c7651fb3a2cb451ec7f78501c8a2745b6b8c1720bbea1e82682a076b98b75b38591cac44f0a6a934b0e8f12cecd03fed0f09aa5b01884c5231d09eba67db7650af39106c7d307cdcd99820f7b05a7a929ff7f1075d4a8e7e44d0760353c3e5b95396a1080a58637b7631b3bc36db2b98e1c7b287a6abd1ded521e3d8af9082cda63566cfc8ee82d0efa44a1825db16a496d404cea8a63365872\" alt =\"MNIST\" style='width: 800px;'>","0e2463f5":"<h1> Fashion MNIST with Keras <\/h1>\n\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n\nThe training and test data sets have 785 columns.\n\nThe first column consists of the class labels (see above), and represents the article of clothing.\n\nThe rest of 784 columns (1-785) contain the pixel-values of the associated image.\n\nEach training and test example is assigned to one of the following labels:\n\n    Label\tDescription\n    0\tT-shirt\/top\n    1\tTrouser\n    2\tPullover\n    3\tDress\n    4\tCoat\n    5\tSandal\n    6\tShirt\n    7\tSneaker\n    8\tBag\n    9\tAnkle boot","9a47b50c":"<h2>1 . Listing Directory <\/h2>","e1d3e97e":"## 9. Model Evaluation","0fac148d":"## 5. Defining Keras Model"}}