{"cell_type":{"38e4e324":"code","f5f34968":"code","db96cfe6":"code","84cc1750":"code","3615fabb":"code","229af76a":"code","41b24264":"code","c0b1a087":"code","38fad1c6":"code","03363988":"code","9acb8dfc":"code","6bf7fb1a":"code","a60fd87d":"code","de6d87d5":"code","59b9b6c3":"code","b2e998ba":"markdown","bc5b0644":"markdown","0ba53fa6":"markdown"},"source":{"38e4e324":"# Importing essential Libraries\n\nimport numpy as np\nimport pandas as pd\n\nfrom urllib.request import urlopen\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f5f34968":"# Reading train and test data\n\ndf = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest_data  = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\n\nprint('Shape of the training data : ', df.shape)\nprint('Shape of the testing data : ', test_data.shape, '\\n')\n\n# Dropping column 'row_id'\ndf = df.drop('row_id', axis = 1)\ndf.date = pd.to_datetime(df.date)\ndf.head()","db96cfe6":"def EDA(df):\n    \n    print('\\033[1m' +'EXPLORATORY DATA ANALYSIS :'+ '\\033[0m\\n')\n    print('\\033[1m' + 'Shape of the data (rows, columns):' + '\\033[0m')\n    print(df.shape, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'All columns from the dataframe :' + '\\033[0m')\n    print(df.columns, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Datatpes and Missing values:' + '\\033[0m')\n    print(df.info(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    for col in df.columns:\n        if df[col].dtype == 'object':\n            print('\\033[1m' + 'Total Unique values in {} :'.format(col) + '\\033[0m',len(df[col].unique()))\n            print('\\t\\033[1m' + 'Categories in {} :'.format(col) + '\\033[0m', df[col].unique())\n    print('\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Summary statistics for the data :' + '\\033[0m')\n    print(df.describe(include='all'), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n        \n    print('\\033[1m' + 'Memory used by the data :' + '\\033[0m')\n    print(df.memory_usage(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Number of duplicate values :' + '\\033[0m')\n    print(df.duplicated().sum())\n          \nEDA(df)","84cc1750":"mmm = Image.open(urlopen(\"https:\/\/i.postimg.cc\/4drz84FH\/fooled-by-the-accuracy.png\"))\nmmm","3615fabb":"training_set = df[(df['country'] == 'Finland') & (df['store'] == 'KaggleMart') & (df['product'] == 'Kaggle Mug')]\n\ntraining_set = training_set[['num_sold']].values\n\nplt.figure(figsize=(8,4), dpi=120)\nplt.plot(training_set)\nplt.grid(True)\nplt.title('Finland KaggleMart Mug sales')\nplt.ylabel('num_sold')\nplt.xlabel('days')\nplt.show()","229af76a":"# Parameters\n\nnum_epochs = 2000\nlearning_rate = 0.01\n\ninput_size = 1\nhidden_size = 2\nnum_layers = 1\n\nnum_classes = 1\n\n#train_size = len(y) - 365\nseq_length = 365","41b24264":"def sliding_windows(data, seq_length):\n    '''\n    The function will accept the raw input data and will return a list of tuples.\n    In each tuple, the first element will contain list of 365 items corresponding \n    to the number of sales days in a year, the second tuple element will contain \n    one item i.e. the num_sold in 365+1st day.\n    '''\n    x = []\n    y = []\n\n    for i in range(len(data) - seq_length-1):\n        _x = data[i : (i + seq_length)]\n        _y = data[i + seq_length]\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x), np.array(y)","c0b1a087":"# Initializing scaler\nsc = MinMaxScaler()","38fad1c6":"mmm = Image.open(urlopen(\"https:\/\/i.postimg.cc\/mkLbWCnv\/fooled-by-the-accuracy-2.png\"))\nmmm","03363988":"class LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n        \n        self.lstm = nn.LSTM(input_size=input_size,\n                            hidden_size=hidden_size,\n                            num_layers=num_layers,\n                            batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        state = Variable(torch.zeros(self.num_layers,\n                                   x.size(0),\n                                   self.hidden_size))\n        \n        cell = Variable(torch.zeros(self.num_layers,\n                                   x.size(0),\n                                   self.hidden_size))\n        \n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (state, cell))        \n        h_out = h_out.view(-1, self.hidden_size)        \n        out = self.fc(h_out)\n        \n        return out","9acb8dfc":"lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)","6bf7fb1a":"mmm = Image.open(urlopen(\"https:\/\/i.postimg.cc\/Sx2m4yZx\/fooled-by-the-accuracy-3.png\"))\nmmm","a60fd87d":"models_dict = {}\nfor c in df.country.unique():\n    for s in df.store.unique():\n        for p in df[\"product\"].unique():\n            \n            data = df[(df.country == c) & (df.store == s) & (df[\"product\"] == p)][[\"date\", \"num_sold\"]]\n            \n            # Transformation\n            training_set  = data[['num_sold']].values            \n            training_data = sc.fit_transform(training_set)\n            x, y = sliding_windows(training_data, seq_length)\n            train_size = len(y) - 365# Change if you want validation set\n            \n            dataX = Variable(torch.Tensor(np.array(x)))\n            dataY = Variable(torch.Tensor(np.array(y)))\n\n            trainX = Variable(torch.Tensor(np.array(x[0:train_size]))) \n            trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n\n            testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n            testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n            \n            model_name = f\"model_{c}_{s}_{p}\"\n            print(\"Training model: \", model_name)          \n\n            # Training\n            for epoch in range(num_epochs):\n                \n                outputs = lstm(trainX)\n                loss = criterion(outputs, trainY)\n                \n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()                \n\n            print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n            models_dict[model_name] = lstm.state_dict()\n\n            # Evaluating\n\n            with torch.no_grad():\n                lstm.eval()\n                train_predict = lstm(dataX)\n\n            data_predict = train_predict.data.numpy()\n            dataY_plot = dataY.data.numpy()\n\n            data_predict = sc.inverse_transform(data_predict)\n            dataY_plot = sc.inverse_transform(dataY_plot)\n\n            plt.figure(figsize=(12,4), dpi=100)\n            plt.axvline(x=train_size, c='g', linestyle='--', label='test split')\n            plt.plot(dataY_plot, label = 'Actual')\n            plt.plot(data_predict, label = 'Predicted')\n            plt.ylabel('num_sold')\n            plt.legend(loc=\"upper left\")\n            plt.suptitle(f'Time-Series Prediction : {c}-> {s}-> {p}')\n            plt.box(False)\n            plt.show()\n","de6d87d5":"# Accessing saved models\n\nmodels_dict['model_Finland_KaggleMart_Kaggle Mug']","59b9b6c3":"# Saving models \nnp.save('models.npy', models_dict)","b2e998ba":"## Acknowledgements\n* https:\/\/machinelearningmastery.com\/time-series-forecasting-long-short-term-memory-network-python\/\n* https:\/\/stackabuse.com\/time-series-prediction-using-lstm-with-pytorch-in-python\/\n* https:\/\/www.kaggle.com\/rnepal2\/tps-how-does-the-very-new-neuralprophet-do","bc5b0644":"### Thank you for reading :)","0ba53fa6":"* Link for Part 1 : https:\/\/www.kaggle.com\/toomuchsauce\/happy-new-year-tps-understanding-the-data-pt1"}}