{"cell_type":{"73c53572":"code","f7e26d6e":"code","7a2ecb72":"code","0625ca97":"code","60f84d16":"code","06441b6b":"code","2f61fb95":"code","ae42d80f":"code","55501b84":"code","a3f5a105":"code","dd519798":"code","d77bfda7":"code","38c90eee":"code","1c04e042":"code","819f049a":"code","f19d5f19":"code","368a36ee":"code","24c9543c":"code","e5176de7":"code","760c3009":"code","e4229b03":"code","ca94f653":"code","2f0ce79c":"code","70b15c5b":"code","0d25f4e6":"code","505e67c0":"code","4c01b95c":"code","fda15cf5":"code","7e4d8318":"code","df112486":"code","10a88121":"code","d5fa76da":"code","1bdb95a7":"code","604c7113":"code","f45c8474":"code","34c5642d":"code","f3f083c2":"code","806377f5":"code","2aebcfa2":"code","f26ae376":"code","1f4b30d3":"code","5da3cece":"code","f242d7d3":"code","4a4e552b":"code","1ff1bde6":"code","9aa1e7a7":"code","2261765c":"code","5511c820":"code","2d34aaf6":"code","48a6cb01":"code","76e26e04":"code","aaa1dc6a":"code","06c11e16":"code","f7375b3a":"code","3347f85c":"code","0e8571d9":"code","8bf63405":"code","c6c94a4d":"code","26a8f2f2":"markdown","930e1e15":"markdown","9a24d8b4":"markdown","c09bfc46":"markdown","2867bec0":"markdown","49bd559b":"markdown","cef036ca":"markdown","00e5c3d2":"markdown","e746f1b6":"markdown","d995a886":"markdown","4b4792b4":"markdown","a3cbed53":"markdown","35e0f807":"markdown","c1f87104":"markdown","f7c2080a":"markdown","2a38d362":"markdown","34c28b28":"markdown","0694fb9f":"markdown","dd351dc3":"markdown","b561336f":"markdown","98bc5330":"markdown","0d34d0c4":"markdown","b569e6ab":"markdown","b802ea68":"markdown","6c0e9945":"markdown","d17f849c":"markdown","e6a5e581":"markdown","363e0eb7":"markdown","36592b5b":"markdown","c8ec689b":"markdown","84dcd587":"markdown","a6b9976b":"markdown","d4d5f231":"markdown","44d37f98":"markdown","3e6c2d14":"markdown","7e5e0034":"markdown","df0470ef":"markdown","fa231a7a":"markdown","a3adae61":"markdown","11880eac":"markdown","1da9ba42":"markdown","2a917a09":"markdown","4268897c":"markdown","d485e659":"markdown","32d8d42a":"markdown"},"source":{"73c53572":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\n\n#sklearn imports\nfrom sklearn.preprocessing import OrdinalEncoder\n#imputer\nfrom fancyimpute import KNN\nfrom fancyimpute import IterativeImputer\n\n#visulizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#statistics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nlocal=False\nif local is False:\n    import os\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","f7e26d6e":"if local == False:\n    traindf=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n    testdf=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nelse:\n    traindf=pd.read_csv(\"train.csv\")\n    testdf=pd.read_csv(\"test.csv\")\nprint(traindf.shape)\nprint(testdf.shape)","7a2ecb72":"testdf.head()","0625ca97":"training_ids = traindf[\"Id\"]\ntesting_ids = testdf[\"Id\"]\ntraindf_cp = traindf.copy()\ntestdf_cp = testdf.copy()\ndependent_data = traindf[[\"Id\",\"SalePrice\"]]\ntraindf_cp.drop(\"SalePrice\",axis=1,inplace=True)\nprint(traindf_cp.shape)\nprint(testdf_cp.shape)\nif traindf_cp.shape[1]==testdf_cp.shape[1]:\n    house_data = pd.concat([traindf_cp,testdf_cp],axis=0)\n    house_data = house_data.reset_index(drop=True)\n    house_data.fillna(np.nan,inplace=True)\n    print(house_data.shape)","60f84d16":"house_data.info()","06441b6b":"def show_missing_info(house_data):\n    missing_info = house_data.isna().sum().reset_index(drop=False)\n    missing_info.columns = [\"column\",\"rows\"]\n    missing_info[\"missing_pct\"] = (missing_info[\"rows\"]\/house_data.shape[0])*100\n    missing_info = missing_info[missing_info[\"rows\"]>0].sort_values(by=\"missing_pct\",ascending=False)\n    return missing_info\nmissing_df = show_missing_info(house_data)\nmissing_df","2f61fb95":"msno.bar(house_data,labels=house_data.columns.tolist())","ae42d80f":"msno.matrix(house_data,labels=house_data.columns.tolist())","55501b84":"delete_rows_cols = missing_df[missing_df[\"rows\"]<20][\"column\"].tolist()\nhouse_data.dropna(axis=0,how=\"any\",subset=delete_rows_cols,inplace=True)\nprint(house_data.shape)","a3f5a105":"house_data.drop(columns=[\"PoolQC\",\"MiscFeature\",\"Alley\"],axis=1,inplace=True)","dd519798":"house_data_columns = house_data.columns","d77bfda7":"categorical_columns = \"Fence,FireplaceQu,GarageFinish,GarageQual,GarageCond,GarageType,BsmtExposure,BsmtCond,BsmtQual,BsmtFinType2,BsmtFinType1,MasVnrType\".split(\",\")","38c90eee":"def ordinal_encoding(data):\n    #empty dictionary ordinal_enc_dict\n    ordinal_enc_dict = {}\n    for col_name in categorical_columns:\n        # Create Ordinal encoder for col\n        ordinal_enc_dict[col_name] = OrdinalEncoder()\n        col = data[col_name]\n\n        # Select non-null values of col\n        col_not_null = col[col.notnull()]\n        reshaped_vals = col_not_null.values.reshape(-1, 1)\n        encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n\n        # Store the values to non-null values of the column in users\n        data.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)\n    return data,ordinal_enc_dict","1c04e042":"house_data_encoded,encoded_dict = ordinal_encoding(house_data.copy(deep=True))","819f049a":"house_data_encoded[categorical_columns].head()","f19d5f19":"for i in categorical_columns:\n    print(house_data_encoded[i].unique())\n    print(\"-\"*40)","368a36ee":"def impute_categorical_features(data,encoded_dict):\n    # Create KNN imputer\n    KNN_imputer = KNN()\n    data.iloc[:, :] = np.round(KNN_imputer.fit_transform(data))\n    for col_name in categorical_columns:\n        reshaped = data[col_name].values.reshape(-1, 1)\n        data[col_name] = encoded_dict[col_name].inverse_transform(reshaped)\n    return data","24c9543c":"house_data_imputed = impute_categorical_features(house_data_encoded[categorical_columns],encoded_dict)","e5176de7":"house_data.drop(columns=categorical_columns,inplace=True,axis=1)\nhouse_data = pd.concat([house_data,house_data_imputed],axis=1)\nhouse_data = house_data[house_data_columns]","760c3009":"house_data[categorical_columns].head()","e4229b03":"missing_df = show_missing_info(house_data)\nmissing_df","ca94f653":"missing_columns = missing_df[\"column\"].tolist()","2f0ce79c":"tmpdf = house_data[missing_columns]\ntmpdf_index = tmpdf.index","70b15c5b":"MICE_imputer = IterativeImputer()\nhouse_data_mice = MICE_imputer.fit_transform(tmpdf)","0d25f4e6":"imputed_df = pd.DataFrame(house_data_mice,columns=missing_columns,index=tmpdf_index)","505e67c0":"house_data.drop(columns=missing_columns,inplace=True,axis=1)\nhouse_data = pd.concat([house_data,imputed_df],axis=1)\nprint(house_data.shape)","4c01b95c":"missing_df = show_missing_info(house_data)\nmissing_df","fda15cf5":"pp_house_data_train = house_data[house_data[\"Id\"].isin(training_ids.tolist())]\npp_house_data_test = house_data[house_data[\"Id\"].isin(testing_ids.tolist())]","7e4d8318":"sales_price = dependent_data[dependent_data[\"Id\"].isin(training_ids.tolist())]\npp_house_data_train.insert(0,\"SalePrice\",sales_price[\"SalePrice\"])","df112486":"pp_house_data_train.to_csv(\"pp_train.csv\",index=False)\npp_house_data_test.to_csv(\"pp_test.csv\",index=False)","10a88121":"train_df = pd.read_csv(\"pp_train.csv\")\ntrain_df.head()","d5fa76da":"area_features = \"LotFrontage,LotArea,MasVnrArea,BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,1stFlrSF,2ndFlrSF,LowQualFinSF,GrLivArea,GarageArea,WoodDeckSF,OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch,PoolArea\".split(\",\")\nyears_and_dates = \"YearBuilt,YearRemodAdd,GarageYrBlt,MoSold,YrSold\".split(\",\")\nquantitiative_descrete_columns = \"FullBath,HalfBath,BsmtFullBath,BsmtHalfBath,TotRmsAbvGrd,Fireplaces,GarageCars,MiscVal\".split(\",\")","1bdb95a7":"categorical_features = list(set(list(train_df.columns[2:])) - set(area_features+years_and_dates+quantitiative_descrete_columns))","604c7113":"for i in area_features+quantitiative_descrete_columns+years_and_dates:\n    train_df[i] = train_df[i].astype(float)","f45c8474":"train_df[years_and_dates].describe()","34c5642d":"train_df[quantitiative_descrete_columns].describe()","f3f083c2":"corr = train_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nfig, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(corr,mask=mask,center=0, linewidths=0, annot=True, fmt=\".2f\")","806377f5":"qc_correlation_with_saleprice = train_df[[\"SalePrice\"]+area_features].corr()[\"SalePrice\"][1:]\nfig = plt.subplots(figsize=(16, 5))\nplt.bar(qc_correlation_with_saleprice.index,qc_correlation_with_saleprice)\nplt.xticks(rotation=90)\nplt.title(\"quantitative continues features correlation with sales price\")\nplt.xlabel(\"category\")\nplt.ylabel(\"correlation\")\nplt.grid()\nplt.show()","2aebcfa2":"qd_correlation_with_saleprice = train_df[[\"SalePrice\"]+quantitiative_descrete_columns].corr()[\"SalePrice\"][1:]\nfig = plt.subplots(figsize=(16, 5))\nplt.bar(qd_correlation_with_saleprice.index,qd_correlation_with_saleprice)\nplt.xticks(rotation=90)\nplt.title(\"quantitiative descrete features correlation with sales price\")\nplt.xlabel(\"category\")\nplt.ylabel(\"correlation\")\nplt.grid()\nplt.show()","f26ae376":"cn_correlation_with_saleprice = train_df[[\"SalePrice\"]+categorical_features].corr()[\"SalePrice\"][1:]\nfig = plt.subplots(figsize=(16, 5))\nplt.bar(cn_correlation_with_saleprice.index,cn_correlation_with_saleprice)\nplt.xticks(rotation=90)\nplt.title(\"Categorical features correlation with sales price\")\nplt.xlabel(\"category\")\nplt.ylabel(\"correlation\")\nplt.grid()\nplt.show()","1f4b30d3":"fig,ax = plt.subplots(6,3,figsize=(15,15))\nsplits = np.split(np.array(area_features), 6)\nfor k,sp in enumerate(splits):\n    for i,col in enumerate(sp):\n        ax[k,i].scatter(train_df[col],train_df[\"SalePrice\"])\n        ax[k,i].set_title(col)\n\nplt.tight_layout()\nplt.show()","5da3cece":"fig,ax = plt.subplots(4,2,figsize=(15,15))\nsplits = np.split(np.array(quantitiative_descrete_columns), 4)\nfor k,sp in enumerate(splits):\n    for i,col in enumerate(sp):\n        ax[k,i].scatter(train_df[col],train_df[\"SalePrice\"])\n        ax[k,i].set_title(col)\n    \nplt.tight_layout()\nplt.show()","f242d7d3":"def plot_selected_columns_scatter(selected_columns):\n    for i in selected_columns:\n        fig,ax = plt.subplots(figsize=(15,4))\n        plt.scatter(train_df[i],train_df[\"SalePrice\"])\n        plt.title(f\"{i} vs Sale Price\")\n        plt.xlabel(i)\n        plt.ylabel(\"Sale Price\")\n        plt.grid()\n        plt.show()","4a4e552b":"selected_columns_qc = qc_correlation_with_saleprice[qc_correlation_with_saleprice>0.5].sort_values(ascending=False).index\nselected_columns_qd = qd_correlation_with_saleprice[qd_correlation_with_saleprice>0.5].sort_values(ascending=False).index\nplot_selected_columns_scatter(selected_columns_qc.tolist()+selected_columns_qd.tolist())","1ff1bde6":"outliers = {\"GrLivArea\":{\"sales_price\":200000,\"value\":4000},\n            \"GarageArea\":{\"sales_price\":300000,\"value\":1200},\n            \"TotalBsmtSF\":{\"sales_price\":200000,\"value\":6000},\n            \"1stFlrSF\":{\"sales_price\":200000,\"value\":4000}}\nprint(train_df.shape)\nfor col in selected_columns_qc:\n    ol = outliers.get(col)\n    train_df = train_df[~((train_df[\"SalePrice\"] < ol[\"sales_price\"]) & (train_df[col] > ol[\"value\"]))]\nprint(train_df.shape)","9aa1e7a7":"plot_selected_columns_scatter(selected_columns_qc)","2261765c":"fig,ax = plt.subplots(figsize=(10,5))\nsns.distplot(train_df[\"SalePrice\"])","5511c820":"original_sale_prices = traindf[\"SalePrice\"]\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\nfig,ax = plt.subplots(figsize=(10,5))\nsns.distplot(train_df[\"SalePrice\"])","2d34aaf6":"train_df.drop(\"Id\",axis=1,inplace=True)","48a6cb01":"train_df = pd.get_dummies(train_df,columns=categorical_features,drop_first=True)\ntrain_df = train_df.apply(pd.to_numeric)","76e26e04":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale","aaa1dc6a":"def prepare_data(data):\n    X = data.drop(\"SalePrice\",axis=1)\n    y = data[\"SalePrice\"]\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n    return X_train,X_test,y_train,y_test","06c11e16":"def fit_baseline_model(X_train,X_test,y_train,y_test):\n    model = LinearRegression()\n    model.fit(X_train,y_train)\n    print(f\"Training set R2 {model.score(X_train,y_train)}\")\n    y_pred = model.predict(X_test)\n    print(f\"R2 score {r2_score(y_test,y_pred)}\")\n    print(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")\n    return y_pred","f7375b3a":"def show_pred_and_test(y_test,y_pred):\n    plt.figure(figsize=(20,5))\n    plt.plot(y_test.values,label=\"Actual\")\n    plt.plot(y_pred,label=\"Predicted\")\n    plt.legend()\n    plt.show()","3347f85c":"X_train,X_test,y_train,y_test = prepare_data(train_df)\ny_pred = fit_baseline_model(X_train,X_test,y_train,y_test)\nshow_pred_and_test(y_test,y_pred)","0e8571d9":"from sklearn.ensemble import BaggingRegressor","8bf63405":"X_train,X_test,y_train,y_test = prepare_data(train_df)\nbag_reg = BaggingRegressor(LinearRegression(),\n                          n_estimators=200,\n                          bootstrap=True,\n                          max_samples=0.7,\n                          n_jobs=-1,\n                          oob_score=True)\nbag_reg.fit(X_train,y_train)\nprint(f\"Out of bag score {bag_reg.oob_score_}\")\nprint(f\"Training set R2 {bag_reg.score(X_train,y_train)}\")\ny_pred = bag_reg.predict(X_test)\nprint(f\"R2 score {r2_score(y_test,y_pred)}\")\nprint(f\"RMSE {np.sqrt(mean_squared_error(y_test,y_pred))}\")","c6c94a4d":"show_pred_and_test(y_test,y_pred)","26a8f2f2":"our target variable follows right skewed distribution.Lets transform this to normal distribution using log transformation.Since our target variable and predictions are large numbers getting, root_mean_squared_error of log transformed values will not punish for larger errors.","930e1e15":"### Explore target variable","9a24d8b4":"**GrLivArea**,**GarageArea**,**1stFkrSF**,**TotalBsmtSF** are having significant positive correlation (>0.6) with SalePrice.","c09bfc46":"custom funtion to show the percentage of missing rows","2867bec0":"#### Imputing\/Drop missing data","49bd559b":"summary of years and dates","cef036ca":"Split Merged dataset to original training and Testing datasets","00e5c3d2":"### Ongoing kernal","e746f1b6":"1. 1. ####  Plot the missingness","d995a886":"As we can see,we don't have any highly correlated pair (correlation >90)","4b4792b4":"combine train and test data to explore missing values and further analysis","a3cbed53":"We just added bagging using linear regression and our model has improved.Lets deep dive in to ensemble using some advance ensemble techniques.","35e0f807":"According to the summary of the dates and years data,we can see there are no abnormal data points.All features are in valid range.","c1f87104":"### **1. Find\/Manage missing values**","f7c2080a":"No surprise,overall Quality is a significant feature for price of the property. BUT do we have multicollinearity? Lets run Variable Inflation Factors (VIF) later after the baseline model.","2a38d362":"We can see there are possible outliers of these important features,this will hurt the model.lets remove outliers","34c28b28":"Lets try to improve current linear regression using knowledge of ensemble learning.\nLets sample data without replacement","0694fb9f":"### Explore data and Identify Outliers","dd351dc3":"> #### Explore data by Data type","b561336f":"#### Remove outliers","98bc5330":"we have imputed all missing data..it's time to deep dive in to further analysis","0d34d0c4":"### Find high correlated features and plot to identify possible outliers","b569e6ab":"Correlations of  SalePrice  with Categorical Nominal features","b802ea68":"lets see how our target variable is looks like.","6c0e9945":"#### Impute Categorical features using KNN Imputer","d17f849c":"### Basic Data analysis\n1.  Find\/Manage missing values\n    * Plot missingness\n    * Drop \/Impute missing values\n2. Explore Data by category wise\n    * Quantitative Discrete data\n3. Explore correlations\n    * Correlations with SalePrice with Quantitative continues features\n    * Correlations with SalePrice with Quantitative continues features\n    * Correlations with SalePrice with Quantitative continues features\n    \n4. Identify\/Remove Outliers \n5. Explore dependent variable\n\n### Regression Analysis\n6. Implement Baseline Regression\n7. Advance Regression methods","e6a5e581":"Lets explore correlations\n* Check for highly correlated features.Highly correlated features are does not add extra information to the dataset.so we can remove those features and reduce the dimentionality","363e0eb7":"Lets take these important features and check if there are possible outliers,before that lets plot and check how features behaves with SalePrice.","36592b5b":"use ordinal_encoding function to encode categories","c8ec689b":"House area and other area features are always has huge impact on cost of the property.So we explore area features separately","84dcd587":"1. Select feaures that missing rows are less than 20 and delete those rows\n2. Drop PoolQC,MiscFeature,Alley features from the dataset because of over 90% of the data is missing and imputing will add bias to the model\n\n3. keep Fence(Fence quality) feature and Impute missing rows since it is considerable effect to target variable ","a6b9976b":"### prepare data for machine learning","d4d5f231":"#### Impute Continues missing values using Multiple Imputation by Chained Equations (MICE)","44d37f98":":\n### Goal is to perform better than baseline model,which is rmse less than 0.117","3e6c2d14":"## Lets explore !!!!!!!!","7e5e0034":"### baseline model using all features","df0470ef":"print information of the dataset,so we can get idea about default data types and missing values","fa231a7a":"Impute categorical features using KNN algorithm","a3adae61":"simple function to define range and keep only data within that range","11880eac":"### Overview of Quantitative Discrete data","1da9ba42":"### Correlation Analysis with SalePrice","2a917a09":"### Overview of date type data","4268897c":"Correlations of  SalePrice  with Quantitative Descrete features","d485e659":"**FullBath**,**TotRmsAbvGrd**,**GarageCars** are having significant correlation with SalePrice","32d8d42a":"let's split our dataset to subsets of each data type.This will easy to explore,plot and identify correlations.We will reduce dimentions of the dataset later after we implement baseline model."}}