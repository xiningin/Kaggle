{"cell_type":{"85f1e20a":"code","a5af1528":"code","c11b1766":"code","ff078ba2":"code","4e0518e9":"code","4e664881":"code","42dd4404":"code","6b63e892":"code","8e130502":"code","3d38ba29":"code","b362c21a":"code","d22b58f1":"code","581af9fa":"code","750bbca9":"code","e2154a1f":"code","de22e082":"code","00fc6d11":"code","58fecec0":"code","318a6153":"code","36ddfaff":"code","7f1a6f93":"code","a3dc8713":"code","dce0a3fb":"code","87fa99a7":"code","ee4b24bc":"code","601557ae":"code","0352999a":"code","d7762237":"code","6561f75e":"code","fb435ae5":"code","98e18706":"code","40b6c5c0":"code","ae2a617f":"markdown","5fb318ad":"markdown","bce343a2":"markdown","dd924e56":"markdown","3a46cc5a":"markdown","6162204b":"markdown","23ddda29":"markdown"},"source":{"85f1e20a":"import os\nimport gc\nimport sys\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,scale\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom sklearn.cluster import KMeans\n\n#k fold\nfrom sklearn.model_selection import StratifiedKFold,train_test_split,KFold\n#metrics\nfrom sklearn.metrics import balanced_accuracy_score,brier_score_loss\n\n\n\n#deep learning\n# import tensorflow as tf \n# from tensorflow.keras import layers\n# from tensorflow.keras import Model\n# from tensorflow.keras import backend as K\n\n\n\n#training\nfrom catboost import CatBoostClassifier as cb\n\n\n#hyperparam optimization\nimport optuna\nfrom optuna import Trial\n\n\nimport warnings \nwarnings.filterwarnings('ignore')","a5af1528":"train=pd.read_csv('..\/input\/geoffrey-hinton-fellowship-hackathon-2\/Training Data.csv')\ntarget=pd.read_csv('..\/input\/geoffrey-hinton-fellowship-hackathon-2\/Training Data Target.csv')\ntest=pd.read_csv('..\/input\/geoffrey-hinton-fellowship-hackathon-2\/Test Data.csv')\n\ntrain.shape","c11b1766":"train=train[train['user_id'].isin(target['user_id'])]\ntrain.shape","ff078ba2":"categories=train.category.unique()\ncategories","4e0518e9":"print('Number of Unique Users {}'.format(train['user_id'].nunique()))","4e664881":"def plot_count(df,col,title):\n    plt.subplots(figsize=(16,8))\n    sns.countplot(df[col])\n    plt.xticks(rotation=45)\n    plt.title(title)\n    plt.show()\n    \n    \nplot_count(train,'category','training data categories')","42dd4404":"plot_count(train,'category','training target categories')","6b63e892":"# K means\n#finding ideal k by elbow method:\n\ndef find_ideal_k(m,df,cols):\n    '''find ideal k for clustering using elbw method'''\n    \n    \n    X_temp=scale(df[cols].copy())\n    inertia=[]\n    K=[i for i in range(2,m)]\n    for k in K:\n        km=KMeans(n_clusters=k)\n        km.fit(X_temp)\n        inertia.append(km.inertia_)\n    \n    plt.figure(figsize=(16,8))\n    plt.plot(K,inertia)\n    plt.xlabel('K')\n    plt.ylabel('inertia')\n    plt.show()\n    \n    del X_temp;gc.collect()\n\n    \n#category columns\n# find_ideal_k(40,X,categories)   \n# find_ideal_k(40,X,aov_cols)   \n","8e130502":"class feature_transformer:\n    '''make features out of data provided'''\n    \n    def __init__(self,\n                 test_run=False):\n        self.test_run=test_run\n        self.super_categories=['Education_Hobbies_work','Electronics','Home','Personal','Kids']\n        \n        \n    def agg_features(self,df):\n        agg_cols=['median','min','max','sum','count','mean','std']\n        \n        #train\n        aov_features=pd.DataFrame(df.groupby('user_id').agg({'aov':agg_cols})).droplevel(0,axis=1)\n        aov_features.reset_index(inplace=True)\n        aov_features.rename({k:f'{k}_aov' for k in agg_cols},axis=1,inplace=True)        \n\n        \n        self.aov_cols=list({k:f'{k}_aov' for k in agg_cols}.values())\n        \n        return aov_features\n    \n    \n    def vectorize(self,df):\n        '''return a dataframe with the customer interaction with products'''\n        df1=pd.DataFrame()\n        df1['user_id']=df['user_id'].unique()\n        categories=df['category'].unique()\n        df1[categories]=0\n\n\n        df1[[f'{cat}_spend'for cat in categories]]=0     \n               \n\n        for user_id in df1.user_id:\n            df_user=df[df['user_id']==user_id]\n\n\n            for ids,row in df_user.iterrows():\n                #number of times user has bought a particular category\n                df1.loc[df1['user_id']==user_id,row['category']]+=1 \n\n                #sum of money user has spend on that category\n                df1.loc[df1['user_id']==user_id,row['category']+'_spend']+=row['aov']            \n\n        return df1\n    \n    def avg_spend(self,row):\n        '''avg spend for each category'''\n        categories=list(self.categories)\n        categories.extend(list(self.super_categories))\n        \n        for cat in categories:\n            if row[cat]!=0:\n                row[cat + '_avg' + '_spend']=row[cat + '_spend']\/row[cat]\n            else:\n                row[cat + '_avg' + '_spend']=0\n                \n        return row\n    \n\n    \n    def sup_categ(self,\n                  df):\n        '''count of Items purchased from these super categories'''\n        \n        sup_categories=self.super_categories\n        \n        Education_Hobbies_work=['Back to School','Painiting Supplies','Laptops',\n                               'Books','Ereaders','Board Games','Gaming']\n        \n        Electronics=['Phones','TVs','Gaming','Laptops','Ereaders']\n        Personal=['Fitness','Fashion','Consumer Durables','Beauty Products']\n        Home=['Home Decor','Consumer Durables','TVs','Groceries',\n              'Kitchen cleaning Supplies','Pet Supplies']\n        Kids=['Board Games','Back to School','Toys','Gaming']\n        \n        for cat in sup_categories:\n            #count in super cat\n            df[f'{cat}']= df[eval(cat)].sum(axis=1)\n            \n            #amount spend in each super cat\n            df[f'{cat}_spend']= df[[str(i)+'_spend' for i in eval(cat)]].sum(axis=1)\n            \n            \n        return df\n             \n\n    def cluster_features(self,\n                         X,X_test,\n                         num_init=50):\n        \n        categories=[f'{c}_spend' for c in self.categories]\n        aov_cols=self.aov_cols\n        sup_categories=[f'{c}_spend' for c in self.super_categories]\n\n        #categories \n        km_pipe=KMeans(n_clusters=15,n_init=num_init,init='k-means++')\n\n        X['Cluster_categories']=km_pipe.fit_predict(X[categories])\n        X_test['Cluster_categories']=km_pipe.predict(X_test[categories])\n        \n        #super Categories\n        km_pipe=KMeans(n_clusters=10,n_init=num_init,init='k-means++')\n        X['Cluster_Sup_cat']=km_pipe.fit_predict(X[sup_categories])\n        X_test['Cluster_Sup_cat']=km_pipe.predict(X_test[sup_categories])\n        \n        \n\n        #aov cols\n        km_pipe=Pipeline([('scale',StandardScaler()) ,\n                          ('kmeans',KMeans(n_clusters=10,n_init=num_init,init='k-means++'))])\n\n        X['Cluster_aov']=km_pipe.fit_predict(X[aov_cols])\n        X_test['Cluster_aov']=km_pipe.predict(X_test[aov_cols])    \n\n        return X,X_test\n    \n    def transform(self,\n                 train_df,\n                 test_df):\n        self.categories=train_df['category'].unique()\n        \n        if self.test_run:\n            train_df=train_df.sample(1000)\n            test_df=test_df.sample(1000)\n            \n        \n#       train features\n        X=self.vectorize(train_df)\n        X=self.sup_categ(X)\n        X=X.merge(self.agg_features(train_df),on='user_id',how='inner')\n        X=X.apply(lambda x: self.avg_spend(x),axis=1)\n        X=X.fillna(0)\n        \n        \n        #test features        \n        X_test=self.vectorize(test_df)\n        X_test=self.sup_categ(X_test)\n        X_test=X_test.merge(self.agg_features(test_df),on='user_id',how='inner')\n        X_test=X_test.apply(lambda x: self.avg_spend(x),axis=1)\n        X_test=X_test.fillna(0)\n        \n        \n        #cluster\n        X,X_test=self.cluster_features(X=X,\n                                      X_test=X_test)\n\n        \n        return X,X_test\n    ","3d38ba29":"%%time\n\n#make features\n    \nfeature_transformer=feature_transformer(test_run=False)\n\nX,X_test=feature_transformer.transform(train_df=train,\n                                       test_df=test)","b362c21a":"\n#merge target\nX=X.merge(target[['user_id','category']],on='user_id',how='inner')\n\n\n#reset index\nX.reset_index(inplace=True,drop=True)\nX_test.reset_index(inplace=True,drop=True)\nX.shape","d22b58f1":"#encoding target category\nlabel_enc=LabelEncoder()\nX['category']=label_enc.fit_transform(X['category'])\nclasses=label_enc.classes_                        \n\n#num_classes\nnum_classes=len(classes)\nnum_classes","581af9fa":"# train data \ny=X.pop('category')\n\ntr_class=y.unique()\n\n\n#cal class weights if required\nweights = compute_class_weight(class_weight='balanced', \n                               classes=tr_class, \n                               y=y)\n\nclass_weights = dict(zip(tr_class, weights))\n\n\nassert set(X.columns)==set(X_test.columns), 'columns donot match'","750bbca9":"#user ids\ntrain_user_ids=X.pop('user_id')\ntest_user_ids=X_test.pop('user_id')","e2154a1f":"#columns with categorical features\n\ncategorical_columns=['Cluster_categories','Cluster_aov','Cluster_Sup_cat']","de22e082":"X['Cluster_categories'].plot(kind='hist')\n","00fc6d11":"def objective(trial:Trial):\n    \n    #splitting training data \n    x_train,x_test,y_train,y_test=train_test_split(X,y,random_state=7,\n                                                  train_size=0.7, stratify=y)\n    \n    #hyperparam_grid\n    params={   'verbose'        : 0,\n               'loss_function'  :'MultiClass',\n               'classes_count'  : len(classes),\n               'depth'          :trial.suggest_int('depth',4,8),\n               'learning_rate'  :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n               'l2_leaf_reg'    :trial.suggest_loguniform('l2_leaf_reg', 1e-2, 10.0),\n               'random_strength':trial.suggest_uniform('random_strength',1e-2,0.3),\n               'max_bin'        :trial.suggest_int('max_bin',64,254),\n#                'grow_policy'    :trial.suggest_categorical('grow_policy',\n#                                                            ['SymmetricTree','Depthwise','Lossguide']),\n               'iterations'     :trial.suggest_int('iterations',1000,2000),\n#                'max_leaves'     :trial.suggest_int('max_leaves',2,64),\n               \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.1, 0.6),\n#                \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n               \"bootstrap_type\": 'MVS',\n#                                  trial.suggest_categorical(\"bootstrap_type\",\n#                                                            [\"Bayesian\", \"MVS\",'Bernoulli']),\n               'eval_metric': 'MultiClass'\n                }\n    \n    \n    try:\n        model = cb(**params)\n\n        model.fit(x_train,y_train,\n                 eval_set=[(x_test,y_test)],\n                 verbose=0,\n                 cat_features=categorical_columns,\n                 early_stopping_rounds=300)\n#                  class_weights=cw)\n\n        preds=model.predict(x_test)\n\n        acc= balanced_accuracy_score(y_test,preds)\n        \n        return acc\n\n    except Exception as e:\n        print(e)\n        return None\n   \n    ","58fecec0":"def get_best_params(time_out=9000):\n    sampler = optuna.samplers.TPESampler(seed=7)  # Make the sampler behave in a deterministic way.\n    study=optuna.create_study(direction='maximize',sampler=sampler)\n    study.optimize(objective, n_trials=300, timeout=time_out)\n    \n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    \n    return study.best_trial.params\n\n# best_params=get_best_params()","318a6153":"#29\/7\/21\nbest_params={\n             'verbose'        : 0,\n             'loss_function'  :'MultiClass',\n             'classes_count'  : len(classes),\n             \"bootstrap_type\": 'MVS',\n             'depth': 4, \n             'learning_rate': 0.06365747563313634,\n             'l2_leaf_reg': 0.14164990508678563,\n             'random_strength': 0.18177681941882445,\n             'max_bin': 254,\n             'iterations': 3000,\n#             1505\n             'colsample_bylevel': 0.4732653286911665}\n\n\n# 26\/7\/21\n# best_params={\n#              'verbose'        : 0,\n#              'loss_function'  :'MultiClass',\n#              'classes_count'  : len(classes),\n#              \"bootstrap_type\": 'MVS',\n#              'depth': 5,\n#              'learning_rate': 0.09962164243695339,\n#              'l2_leaf_reg': 1.2575960096355996,\n#              'random_strength': 0.07176128132724813,\n#              'max_bin': 220, \n#              'iterations': 1206, \n#              'colsample_bylevel': 0.5598653634090404}\n\n# 25\/7\/21\n# best_params={\n#              'verbose'        : 0,\n#              'loss_function'  :'MultiClass',\n#              'classes_count'  : len(classes),\n#              \"bootstrap_type\": 'MVS',\n#              'depth': 4, \n#              'learning_rate': 0.07240696360883225, \n#              'l2_leaf_reg': 0.37748668450031264, \n#              'random_strength': 0.14114940406399043, \n#              'max_bin': 170, 'iterations': 2148, \n#              'colsample_bylevel': 0.41308547541216933}","36ddfaff":"def k_fold_predict(k,\n                  params=best_params):\n    \n    \n    skf=StratifiedKFold(n_splits=k)\n    \n    mean_preds=np.zeros(shape=(X_test.shape[0],len(classes)))\n    train_check=np.zeros(shape=(X.shape[0],len(classes)))\n    \n    \n\n    for train_idx,val_idx in skf.split(X,y):\n        x_t,x_v=X.iloc[train_idx],X.iloc[val_idx]\n        y_t,y_v=y.iloc[train_idx],y.iloc[val_idx]\n        \n        model=cb(**params)        \n        model.fit(x_t,y_t,\n                 cat_features=categorical_columns)\n        \n        print('Validation score {}'.format(balanced_accuracy_score(\n                                            y_v,model.predict(x_v))))\n        \n        #test predictions\n        mean_preds+=model.predict(X_test,prediction_type='Probability')\n        \n        #training preds\n        train_check+=model.predict(X,prediction_type='Probability')\n    \n    mean_preds=mean_preds\/k\n    train_check=train_check\/k\n    \n    return mean_preds,train_check","7f1a6f93":"%%time\npreds,train_ch=k_fold_predict(30)\n\npreds=pd.DataFrame(preds)\ntrain_preds=pd.DataFrame(train_ch)","a3dc8713":"preds.rename(columns={i:classes[i] for i in range(len(classes))},inplace=True)\npreds.head()","dce0a3fb":"# mean predictions for each class\nnp.mean(preds,axis=0)","87fa99a7":"# mean predictions for each class(training_data)\npd.DataFrame({'predicted_ratio':np.mean(train_preds,axis=0),\n              'actual_ratio': y.value_counts(1)})","ee4b24bc":"def get_top_cat(row,classes):\n    '''get the top 3 predicted categories'''\n    prob,top_3 = zip(*sorted(zip(row.values,classes),reverse=True)[:3])\n    top_3=', '.join(top_3)\n    \n    return top_3\n","601557ae":"get_top_cat(preds.iloc[0],classes)","0352999a":"get_top_cat(preds.iloc[180],classes)","d7762237":"df_preds=pd.DataFrame()\ndf_preds['user_id']=test_user_ids\ndf_preds['pred3']=0","6561f75e":"for i in range(len(preds)):\n    df_preds.loc[i,'pred3']=get_top_cat(row=preds.iloc[i,:],classes=classes)","fb435ae5":"df_preds.head()","98e18706":"df_preds.to_csv('submission.csv',index=False)","40b6c5c0":"\n# def mean_relevance_rank(top_category,predicted_categories):\n#     '''get a mean relevance rank for a given prediction.'''\n#     mrr=float(1\/(1+i) for i in range(len(predicted_categories.split(','))) \\\n#         if predicted_categories.split(',')[i]==top_category else 0)\n    \n#     return mrr\n\n# def precision_rank(top_category,predicted_categories):\n#     '''return a precision rank for given prediction'''\n#     pr=1 if top_catgory in predicted_categories else 0\n    \n#     return pr\n","ae2a617f":"# **OPTUNA Hyperparam optimization**","5fb318ad":"**Training Data**","bce343a2":"# Model Fitting and Prediction","dd924e56":"# Resources\n* [ https:\/\/developers.google.com\/machine-learning\/recommendation\/overview\/types ]\n* [ https:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system ]\n* [ https:\/\/www.kaggle.com\/kanncaa1\/recommendation-systems-tutorial ]\n* [ https:\/\/www.featuretools.com\/demos\/ ]","3a46cc5a":"**Predicting Top Three categories based on predicted Probablities**","6162204b":"# **Feature Transformer**","23ddda29":"**Creating Features**"}}