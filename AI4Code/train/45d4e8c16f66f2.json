{"cell_type":{"e945ebb6":"code","c3a0aa88":"code","633d1881":"code","74547886":"code","b1e02b0f":"code","35f63a52":"code","eb499883":"code","9fc3a8e5":"code","4cd0c88c":"code","03561e0a":"code","6c4478d9":"code","5c782167":"code","cda070b0":"code","b02ab876":"code","a797ab6e":"code","b3f489de":"code","9784aa1d":"code","98e184ee":"code","90f362d9":"code","ccb43f6f":"code","343ef20f":"code","cddcf8ab":"code","02b5dc0b":"code","844f50a6":"code","cfd77926":"code","8532c013":"code","2554786b":"code","c0789dcd":"code","9c381767":"code","e5b05e2b":"code","4c06865f":"markdown","6f88dc47":"markdown","c3fa465e":"markdown","52a72075":"markdown","ce461033":"markdown","3d74ae0b":"markdown","f29af363":"markdown","32a98aab":"markdown","5fa84731":"markdown","2f455d3a":"markdown"},"source":{"e945ebb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport random\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport featuretools as ft\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom fastai import *\nfrom fastai.tabular import *\nfrom fastai.basic_data import DataBunch\n# from tqdm import tqdm_notebook\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n%reload_ext autoreload\n%autoreload 2","c3a0aa88":"indir = '..\/input'","633d1881":"df = pd.read_csv(os.path.join(indir, 'train.csv'))\ndf_summary = df.describe(); df_summary","74547886":"test_df = pd.read_csv(os.path.join(indir, 'test.csv')).set_index('ID_code')\ntest_df.describe()","b1e02b0f":"# get the features list\nfeatures = list(test_df.columns)\nlen(features)","35f63a52":"dmean, dmin, dmax = df_summary.loc['mean'],df_summary.loc['min'], df_summary.loc['max'] \ndrange = dmax - dmin","eb499883":"df.loc[:, features] = (df[features] - dmin[features])\/drange[features]\ndf.describe()","9fc3a8e5":"test_df.loc[:, features] = (test_df[features] - dmin[features])\/drange[features]\ntest_df.describe()","4cd0c88c":"def augment_df(df):\n    for feature in features:\n        df[f'sq_{feature}'] = df[feature]**2\n        df[f'repo_{feature}'] = df[feature].apply(lambda x: 0 if x==0 else 1\/x)\n        df[f'repo_sq_{feature}'] = df[f'repo_{feature}']**2\n        df[f'cube_{feature}'] = df[feature]**3\n#         df[f'repo_cube_{feature}'] = df[f'repo_{feature}']**3\n#         df[f'p4_{feature}'] = df[feature]**4\n#         df[f'sin_{feature}'] = sin(df[feature])\n#         df[f'exp_{feature}'] = exp(df[feature])\n#         df[f'log_{feature}'] = df[f'sq_{feature}'].apply(lambda x: 0 if x==0 else log(x))\n    \n    df['min'] = df[features].min(axis=1)\n    df['mean'] = df[features].mean(axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['sum'] = df[features].sum(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['std'] = df[features].std(axis=1)\n    df['var'] = df[features].var(axis=1)\n    df['abs_sum'] = df[features].abs().sum(axis=1)\n    df['abs_mean'] = df[features].abs().mean(axis=1)\n    df['abs_median'] = df[features].abs().median(axis=1)\n    df['abs_std'] = df[features].abs().std(axis=1)\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    \n    df['sq_kurt'] = df[[f'sq_{feature}' for feature in features]].kurt(axis=1)\n    ","03561e0a":"%%time\naugment_df(df)\naugment_df(test_df)","6c4478d9":"features = list(test_df.columns[:-12])\nstats_features = list(test_df.columns[-12:])\nnum_features = len(features)\nnum_features","5c782167":"### Feature Understanding","cda070b0":"def view_dist(df, columns, row=10, col=10):\n    fig, axes = plt.subplots(10,10,figsize=(30,30))\n    axes = axes.flatten()\n    for col, ax in zip(columns, axes):\n        sns.kdeplot(df.loc[df.target==0, col], ax=ax, color='r', label='0')\n        sns.kdeplot(df.loc[df.target==1, col], ax=ax, color='b', label='1')\n        ax.legend()\n        ax.set_title(f'{col}')\n        \n    plt.show()    ","b02ab876":"# view_dist(df, features[:100])","a797ab6e":"# seed = 2019\n# train_samples = df.sample(frac=0.95, random_state=seed)\n# valid_samples = df.drop(train_samples.index)","b3f489de":"random.seed(31415926)\nvalid_idx = random.sample(list(df.index.values), int(len(df)*0.2) )\ntrain_idx = df.drop(valid_idx).index","9784aa1d":"# verify that positive sample distribution in validation set is similar to that of the whole data\ndf.iloc[valid_idx].target.sum() \/ len(valid_idx) , df.target.sum() \/ len(df)","98e184ee":"class roc(Callback):\n    '''\n    Updated on March 28 2019 to reflect new change in FastAI's Callback\n    ROC_AUC metric callback for fastai. Compute ROC score over each batch and returns the average over batches.\n    TO DO: rolling average\n    '''\n    def on_epoch_begin(self, **kwargs):\n        self.total = 0\n        self.batch_count = 0\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        preds = F.softmax(last_output, dim=1)\n        # roc_auc_score does not work on batches which does not contain both classes.\n        try:\n            roc_score = roc_auc_score(to_np(last_target), to_np(preds[:,1]))\n            self.total += roc_score\n            self.batch_count += 1\n        except:\n            pass\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        return add_metrics(last_metrics, self.total\/self.batch_count)","90f362d9":"BATCH_SIZE = 2048","ccb43f6f":"# data = TabularDataBunch.from_df(path='.', df=df, \n#                                 dep_var='target', \n#                                 valid_idx=valid_samples.index, \n#                                 cat_names=[], \n#                                 cont_names=features, \n#                                 procs=[tabular.transform.Normalize],\n#                                 test_df=test_df)\n\n#learner = tabular_learner(data, layers=[200,100], ps=[0.5,0.2], metrics=[accuracy, roc()])\n\n#learner.lr_find()\n#learner.recorder.plot()","343ef20f":"def train_and_eval_tabular_learner(train_df,\n                                   train_features, \n                                   valid_idx,\n                                   add_noise=False,\n                                   lr=0.02, epochs=1, layers=[200, 50], ps=[0.5, 0.2], name='learner'):\n    \n    data = TabularDataBunch.from_df(path='.', df=train_df, \n                                    dep_var='target', \n                                    valid_idx=valid_idx, \n                                    cat_names=[], \n                                    cont_names=train_features, \n                                    bs=BATCH_SIZE,\n                                    procs=[],\n                                    test_df=test_df)\n    learner = tabular_learner(data, layers=layers, ps=ps, metrics=[roc()])\n\n    learner.fit_one_cycle(epochs, lr)\n\n    # learner.save(name,with_opt=False)\n        \n    # run prediction on validation set\n    valid_predicts, _ = learner.get_preds(ds_type=DatasetType.Valid)\n    valid_probs = np.array(valid_predicts[:,1])\n    valid_targets = train_df.loc[valid_idx].target.values\n    valid_score = roc_auc_score(valid_targets, valid_probs)\n    \n    # run prediction on test    \n    test_predicts, _ = learner.get_preds(ds_type=DatasetType.Test)\n    test_probs = to_np(test_predicts[:, 1])\n\n    return valid_score, valid_probs, test_probs","cddcf8ab":"%%time\nsub_features = []\nvalid_scores = []\nvalid_predictions = []\npredictions = []\nnum_learner = 1000\nnum_epochs = 5\nsaved_model_prefix = 'learner'\n\nfor i in range(num_learner):\n    print('training model {:}'.format(i))\n    sub_features.append(random.sample(list(features), int(num_features*0.5)) + stats_features)\n    name = f'{saved_model_prefix}_{i}'\n\n    score, valid_probs, test_probs = train_and_eval_tabular_learner(df,\n                                                                    sub_features[-1], \n                                                                    valid_idx, \n                                                                    epochs=num_epochs, \n                                                                    lr=0.01, \n                                                                    name=name)\n    \n    valid_scores.append(score)\n    valid_predictions.append(valid_probs)\n    predictions.append(test_probs)","02b5dc0b":"print(valid_scores)","844f50a6":"# roc_auc_score on validation set\naverage_valid_predicts = sum(valid_predictions)\/len(valid_predictions)\nvalid_auc_score = roc_auc_score(df.iloc[valid_idx].target, average_valid_predicts); valid_auc_score","cfd77926":"from sklearn.metrics import roc_curve\n\nfpr, tpr, _ = roc_curve(y_true=df.iloc[valid_idx].target,y_score=average_valid_predicts)\nplt.figure(figsize=(9,9))\nplt.plot(fpr, tpr)\nplt.show()","8532c013":"# this is if we want to average only on the models that score more than average\n# predicts = np.zeros(predictions[0].shape)\n# counts = 0\n# for i in range(num_epochs):\n#     if valid_scores[i] > average_valid_score:\n#         predicts += predictions[i]\n#         counts += 1\n        \n# print(\"number of models: {:}\".format(counts))\n\n# predicts = sum(predictions)\/counts","2554786b":"test_df['target'] = sum(predictions)\/len(valid_predictions)","c0789dcd":"# add timestamp to submission\nfrom datetime import datetime\nnow = datetime.now()\nmodel_time = now.strftime(\"%Y%m%d-%H%M\")","9c381767":"test_df[['target']].to_csv(f'submission_fastai_ensemble_{model_time}_{valid_auc_score}.csv')","e5b05e2b":"from IPython.display import FileLink\nFileLink(f'submission_fastai_ensemble_{model_time}_{valid_auc_score}.csv')","4c06865f":"## Data Load and Exploration","6f88dc47":"## Test and Submit","c3fa465e":"First we want to find the correct learning rate for this dataset\/problem. This only needs to run once.\nThe *optimal* learning rate found is 0.01","52a72075":"## Visualize ROC on the Validation Set","ce461033":"## Normalize Data\nWe could let FastAI normalize the data automatically, but we choose to do so manually for more flexibility","3d74ae0b":"This is the main train and evaluate function. Since we are training multiple learners, we choose to save the model to harddisk and load them later if needed.","f29af363":"## Feature Engineering\nWe don't really do anything special but polynomial features","32a98aab":"## Split training data into train and validation sets","5fa84731":"Grab a statistic summary of the training set. We may use this later in adding noises to the data during training","2f455d3a":"## FastAI Tabular Learner\nWe start off with the default learner from FastAI"}}