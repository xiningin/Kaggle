{"cell_type":{"8cf6cd45":"code","1e145366":"code","fa1eaa96":"code","aff41367":"code","371b330e":"code","0a9de4ec":"code","4828bae6":"code","243a1e97":"code","15d15a33":"code","5a2aaf73":"code","fefe829d":"code","f9b6dad5":"code","21cd0740":"code","3aee04a3":"code","450b3de4":"code","d2491064":"code","039d8556":"code","2f6ce4af":"code","aa7a607b":"code","0afd0dce":"code","065a3659":"code","a71568e7":"code","2caa4da2":"code","ba5f1a91":"code","de253c21":"code","b43d97d9":"code","196d04cb":"code","6cc0b902":"code","fe5c6481":"code","21339140":"code","0a9a413c":"code","c4fb6661":"code","726dcfc7":"code","91d7a4c0":"code","3330fd71":"code","6d0eeb75":"code","03a66312":"code","8b8199de":"code","4ada9138":"code","acb486e7":"code","25c9fb78":"code","1ccf4f2a":"code","c60fd000":"code","64f0a3d5":"code","e758f4b9":"code","14513a4b":"code","43b4b171":"code","d5332e3c":"code","ae806e16":"code","fbeff6e0":"code","31ead821":"code","2cd1dadf":"code","e0febcdf":"code","0f449850":"code","887eac0b":"code","cadcdf8a":"code","2a0a38db":"code","47bbbdd7":"code","cd0dcf6b":"code","35cb6375":"code","fbadb7f7":"code","69c88396":"code","cd94c161":"code","18579482":"code","eb083e94":"code","b1b4c642":"code","28e8fe10":"code","363fa6cb":"code","3b7216e8":"code","4d5821a8":"code","d7c00fe2":"code","244ad213":"code","e9e5de5e":"code","2bdb527d":"code","e039ce3a":"code","0544aabe":"code","29dba5a0":"code","0499d347":"code","6e562393":"code","644d9213":"code","157ab212":"code","85a222ac":"code","452b1206":"code","c5d68d0d":"code","4f81aa01":"code","114d735c":"code","dcb9496d":"code","33f438ab":"code","4b944120":"code","7a3aea59":"code","f4dceb5b":"code","eb82474c":"code","d757975f":"code","04bf196e":"code","386838eb":"code","47db60bd":"code","d2e3347b":"code","71eb0e6c":"code","7405779c":"code","201061d5":"code","51de2d77":"code","d0392d2a":"code","ffc27f95":"code","a5d41a4b":"code","e3eb7c9b":"code","6833bc60":"code","83b609c4":"code","bf69dfb3":"code","24cb4f27":"code","21461f03":"code","b6384cbb":"code","1ede7602":"code","9b10e785":"code","07085004":"code","d4624be5":"code","0002ee0e":"code","4f90b23e":"code","b4f54af7":"code","e63c893e":"markdown","9c142bc1":"markdown","874b6774":"markdown","7e282b73":"markdown","c46fc2ca":"markdown","4fc9930c":"markdown","26d37c9f":"markdown","d6caa045":"markdown","6e14164c":"markdown","dfa9aec4":"markdown","964cdd81":"markdown","613ba091":"markdown","fba5e966":"markdown","ff73d9db":"markdown","ea303b18":"markdown","d8491fed":"markdown","cafa237a":"markdown","dd928a47":"markdown","176d0c4c":"markdown","9747e09c":"markdown","77169ed6":"markdown","5a17bbc1":"markdown","03933ab1":"markdown","3901efd0":"markdown","56257f86":"markdown","fc971535":"markdown","dac977e5":"markdown","3e68b8c8":"markdown","bd391b42":"markdown","3411dd7b":"markdown","667d72f2":"markdown","75d00632":"markdown","23902769":"markdown","3e442d89":"markdown","43398d3f":"markdown","ea3c1169":"markdown","2bde318a":"markdown","305344ea":"markdown","2acc3d63":"markdown","8a093fb5":"markdown","07a8dfdf":"markdown","81b0dce7":"markdown","d7873319":"markdown","76b5c3a4":"markdown","e31fb11f":"markdown"},"source":{"8cf6cd45":"#Data Structures\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\n\n#Sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Others\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","1e145366":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","fa1eaa96":"data = pd.read_csv(\"..\/input\/dataset\/train.csv\")\nunseen = pd.read_csv(\"..\/input\/dataset\/test.csv\")\nsample = pd.read_csv(\"..\/input\/dataset\/sample.csv\")\ndata_dict = pd.read_csv(\"..\/input\/dataset\/data_dictionary.csv\")\n\nprint(data.shape)\nprint(unseen.shape)\nprint(sample.shape)\nprint(data_dict.shape)","aff41367":"data.head()","371b330e":"data_dict","0a9de4ec":"# Checking the total Churn rate as per the Dataframe\n\nChurn_rate = (sum(data['churn_probability'])\/len(data['churn_probability']))*100\nChurn_rate","4828bae6":"#Checking for repeated values in dataset by checking unique values in  columns\n\nprint(data['id'].nunique())","243a1e97":"# Dropping Columns with more than 70% Null Values\n\ndrop_cols = []\n\nfor i in  data:\n\n    if data[i].isnull().sum()\/len(data)*100 >= 70 :\n        drop_cols.append(i)\n        \nlen(drop_cols)","15d15a33":"drop_cols","5a2aaf73":"data = data.drop(drop_cols , 1)\ndata.shape","fefe829d":"data.info(object)","f9b6dad5":"print(round((data.isnull().sum() \/ len(data)) * 100 , 2))","21cd0740":"missing_data_percent = data.isnull().any()\nimpute_cols = missing_data_percent[missing_data_percent.gt(0)].index\nimpute_cols","3aee04a3":"imp = SimpleImputer(strategy='constant', fill_value=0)\ndata[impute_cols] = imp.fit_transform(data[impute_cols])","450b3de4":"print(round((data.isnull().sum() \/ len(data)) * 100 , 2))","d2491064":"data.nunique()","039d8556":"one_val_cols = ['circle_id',\n 'loc_og_t2o_mou',\n 'std_og_t2o_mou',\n 'loc_ic_t2o_mou',\n 'last_date_of_month_6',\n 'last_date_of_month_7',\n 'last_date_of_month_8', 'std_og_t2c_mou_6',\n 'std_og_t2c_mou_7',\n 'std_og_t2c_mou_8' ,'std_ic_t2o_mou_6',\n 'std_ic_t2o_mou_7',\n 'std_ic_t2o_mou_8']","2f6ce4af":"data = data.drop(one_val_cols , 1)\ndata.shape","aa7a607b":"data.info()","0afd0dce":"# Changing \"Object\" Dtype column to DateTime Dtype\nobject_col_data = data.select_dtypes(include=['object'])\n\n# convert to datetime\nfor i in object_col_data.columns:\n    data[i] = pd.to_datetime(data[i])\n\n    \ndata.info()","065a3659":"# Checking Desribe\n\ndata.describe(include=\"all\")","a71568e7":"# create box plot for  6th, 7th and 8th month\n\ndef box_plot(attribute):\n    plt.figure(figsize=(20,16))\n    df = data\n    plt.subplot(2,3,1)\n    sns.boxplot(data=df, y=attribute+\"_6\",x=\"churn_probability\",hue=\"churn_probability\",\n                showfliers=False,palette=(\"plasma\"))\n    plt.subplot(2,3,2)\n    sns.boxplot(data=df, y=attribute+\"_7\",x=\"churn_probability\",hue=\"churn_probability\",\n                showfliers=False,palette=(\"plasma\"))\n    plt.subplot(2,3,3)\n    sns.boxplot(data=df, y=attribute+\"_8\",x=\"churn_probability\",hue=\"churn_probability\",\n                showfliers=False,palette=(\"plasma\"))\n    plt.show()","2caa4da2":"box_plot('total_rech_amt')","ba5f1a91":"sns.boxplot(data=data, y=data['aon'],x=\"churn_probability\",hue=\"churn_probability\",\n                showfliers=False,palette=(\"plasma\"))\nplt.show()","de253c21":"box_plot('arpu')","b43d97d9":"box_plot('max_rech_amt')","196d04cb":"box_plot(\"total_rech_num\")","6cc0b902":"last_day_rech_amt = data.columns[data.columns.str.contains('day')]","fe5c6481":"last_day_rech_amt.tolist()","21339140":"box_plot('last_day_rch_amt')","0a9a413c":"usage_2g_and_3g = data.columns[data.columns.str.contains('2g|3g',regex=True)]","c4fb6661":"percentage_3g_2g_null_check = 100*data.loc[:,usage_2g_and_3g].isnull().sum()\/len(data.loc[:,usage_2g_and_3g])\ndf = pd.DataFrame(percentage_3g_2g_null_check)\ndf.rename(columns={0:'Null_Percentage'}, inplace=True)\ndf = pd.DataFrame(df.Null_Percentage)\ndisplay(df)","726dcfc7":"box_plot(\"vol_2g_mb\")","91d7a4c0":"sns.distplot(data.vol_2g_mb_6)","3330fd71":"box_plot(\"vol_3g_mb\")","6d0eeb75":"box_plot(\"monthly_2g\")","03a66312":"sns.displot(data.monthly_2g_6)","8b8199de":"monthly_subcription_2g_3g = data.columns[data.columns.str.contains('monthly_2g|monthly_3g',regex=True)]\nmonthly_subcription_2g_3g.tolist()","4ada9138":"def mean_bar_chart(df,columns_list):\n    df_0 = df[df.churn_probability==0].filter(columns_list)\n    df_1 = df[df.churn_probability==1].filter(columns_list)\n\n    mean_df_0 = pd.DataFrame([df_0.mean()],index={'Non Churn'})\n    mean_df_1 = pd.DataFrame([df_1.mean()],index={'Churn'})\n\n    frames = [mean_df_0, mean_df_1]\n    mean_bar = pd.concat(frames)\n\n    mean_bar.T.plot.bar(figsize=(10,5),rot=0)\n    plt.show()\n    \n    return mean_bar","acb486e7":"mean_bar_chart(data, monthly_subcription_2g_3g)","25c9fb78":"# let's check Volume based cost \nvbc_column = data.columns[data.columns.str.contains('vbc_',regex=True)]\nvbc_column.tolist()","1ccf4f2a":"# Renaming month named vbc columns to 6,7,8,9 format\ndata.rename(columns={'jun_vbc_3g':'vbc_3g_6','jul_vbc_3g':'vbc_3g_7','aug_vbc_3g':'vbc_3g_8'}, inplace=True)","c60fd000":"vbc_column = data.columns[data.columns.str.contains('vbc_3g',regex=True)]\nvbc_column.tolist()","64f0a3d5":"box_plot('vbc_3g')","e758f4b9":"mean_bar_chart(data , vbc_column)","14513a4b":"box_plot(\"arpu\")","43b4b171":"tot_arpu_cols = [i for i in data.columns if 'arpu' in i]\n\ntot_arpu_cols = list(tot_arpu_cols)\nprint(tot_arpu_cols)\n","d5332e3c":"mean_bar_chart(data , tot_arpu_cols)","ae806e16":"data.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'] , axis=1 , inplace=True)","fbeff6e0":"data.shape","31ead821":"# Creating a copy of dataset for PCA\n\ndata_PCA = data.copy()","2cd1dadf":"# Saggregating Target variable from independent variables\nX = data_PCA.drop(['churn_probability'], axis=1)\ny = data_PCA['churn_probability']\n\n\ndata_PCA.drop('churn_probability', axis=1, inplace=True)","e0febcdf":"X.head()","0f449850":"print(X.shape)\nprint(y.shape)","887eac0b":"#Instantiating an object\nscaler = StandardScaler()\n\n#list of numerical Columns\nnumerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n\n#Fititng the scaler\nX[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n\nX.head()","cadcdf8a":"X.info()\n","2a0a38db":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=100)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","47bbbdd7":"pip install imblearn","cd0dcf6b":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_tr,y_tr = sm.fit_resample(X_train,y_train)\n\n","35cb6375":"print(X_tr.shape)\nprint(y_tr.shape)","fbadb7f7":"# import PCA\nfrom sklearn.decomposition import PCA\npca = PCA(random_state=100)\n\n# apply PCA on train data\npca.fit(X_tr)","69c88396":"X_tr_pca = pca.fit_transform(X_tr)\nprint(X_tr_pca.shape)\n\nX_test_pca = pca.transform(X_test)\nprint(X_test_pca.shape)","cd94c161":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlr_pca = LogisticRegression(C=1e9)\nlr_pca.fit(X_tr_pca, y_tr)\n\n# make the predictions\ny_pred = lr_pca.predict(X_test_pca)\n\n# convert prediction array into a dataframe\ny_pred_df = pd.DataFrame(y_pred)","18579482":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Printing confusion matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy of the logistic regression model with PCA: \",accuracy_score(y_test,y_pred))","eb083e94":"# scree plot to check the variance explained by different PCAs\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('no of principal components')\nplt.ylabel('explained variance - cumulative')\nplt.show()","b1b4c642":"np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n","28e8fe10":"# PCA with 52 components\npca_52 = PCA(n_components=52)\n\ndf_tr_pca_52 = pca_52.fit_transform(X_tr)\nprint(df_tr_pca_52.shape)\n\ndf_test_pca_52 = pca_52.transform(X_test)\nprint(df_test_pca_52.shape)","363fa6cb":"# Let's run the model using the selected variables\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlr_pca1 = LogisticRegression()\nlr_pca1.fit(df_tr_pca_52, y_tr)\n\n# Predicted probabilities\ny_pred52 = lr_pca1.predict(df_test_pca_52)\n\n# Converting y_pred to a dataframe which is an array\ndf_y_pred = pd.DataFrame(y_pred52)\n\nprint(\"Accuracy with 52 PCAs: \",accuracy_score(y_test,y_pred52))","3b7216e8":"print(confusion_matrix(y_test,y_pred52))","4d5821a8":"# Feature reduction using RFE\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nfrom sklearn.feature_selection import RFE\n\n# running RFE with 20 variables as output\nrfe = RFE(lr, 20)   \nrfe = rfe.fit(X_tr, y_tr)","d7c00fe2":"rfe.support_","244ad213":"list(zip(X_tr.columns, rfe.support_, rfe.ranking_))","e9e5de5e":"#list of RFE supported columns\ncol = X_tr.columns[rfe.support_]\ncol","2bdb527d":"X_rfe = pd.DataFrame(data=X_tr).iloc[:, rfe.support_]\ny_rfe = y_tr","e039ce3a":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=1)\n\nlr.fit(X_rfe, y_rfe)","0544aabe":"X_test_rfe = pd.DataFrame(data=X_test).iloc[:, rfe.support_]\n\ny_pred = lr.predict(X_test_rfe)\n\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","29dba5a0":"print('Accuracy of Logistic Regression Model on test set is ',lr.score(X_test_rfe, y_test))","0499d347":"print(classification_report(y_test, y_pred))","6e562393":"data_tree = data.copy()","644d9213":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, train_size=0.7, random_state=100)\n\n#Applying Smote\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE()\nX_tr,y_tr = sm.fit_resample(X_train,y_train)\nprint(X_tr.shape)\nprint(y_tr.shape)","157ab212":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt1 = DecisionTreeClassifier(max_depth=5)\ndt1.fit(X_tr, y_tr)","85a222ac":"y_train_pred = dt1.predict(X_tr)\ny_test_pred = dt1.predict(X_test)","452b1206":"print(\"The Accuracy from Decision tree Model is: \" , accuracy_score(y_tr, y_train_pred))\n","c5d68d0d":"# The evaluation metrics of our default model\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Making predictions\nX_test = pd.DataFrame(data=X_test)\ny_pred1 = dt1.predict(X_test)\ny_train_pred = dt1.predict(X_train)\n\n# Printing classification report\nprint(classification_report(y_test, y_pred1))","4f81aa01":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydotplus, graphviz","114d735c":"# plotting tree with max_depth=5\ndot_data = StringIO()  \n\nexport_graphviz(dt1, out_file=dot_data, filled=True, rounded=True,\n                feature_names=X.columns, \n                class_names=['Not_Churned', \"Churned\"])\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","dcb9496d":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': [3, 5] , 'min_samples_leaf': [500 , 1000]}\n\n# instantiate the model\ndt2 = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ngrid_search = GridSearchCV(estimator=dt2, param_grid=parameters, \n                    cv=n_folds, verbose=1, \n                   scoring=\"accuracy\")","33f438ab":"grid_search.fit(X_tr, y_tr)","4b944120":"# scores of GridSearch CV\nscore = grid_search.cv_results_\nscore_df = pd.DataFrame(score)\n\nscore_df.head()","7a3aea59":"score_df.nlargest(5,\"mean_test_score\")","f4dceb5b":"grid_search.best_estimator_","eb82474c":"dt_best = grid_search.best_estimator_","d757975f":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, dt_best.predict(X_test)))","04bf196e":"# plotting tree with best estimators\ndot_data = StringIO()  \n\nexport_graphviz(dt_best, out_file=dot_data, filled=True, rounded=True,\n                feature_names=X.columns, \n                class_names=['Not_Churned', \"Churned\"])\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","386838eb":"y_train_pred_best_dt = dt_best.predict(X_tr)\ny_test_pred_best_dt = dt_best.predict(X_test)","47db60bd":"print(\"The Accuracy from Decision tree Model is: \" , accuracy_score(y_tr, y_train_pred_best_dt))","d2e3347b":"sample.head()","71eb0e6c":"unseen.head()","7405779c":"unseen.shape","201061d5":"#Checking for repeated values in dataset by checking unique values in  columns\n\nprint(unseen['id'].nunique())","51de2d77":"# Dropping Columns with more than 70% Null Values\n\ndrop_cols = []\n\nfor i in  unseen:\n\n    if unseen[i].isnull().sum()\/len(unseen)*100 >= 70 :\n        drop_cols.append(i)\n        \nlen(drop_cols)","d0392d2a":"unseen = unseen.drop(drop_cols , 1)\nunseen.shape","ffc27f95":"print(round((unseen.isnull().sum() \/ len(unseen)) * 100 , 2))","a5d41a4b":"missing_data_percent2 = unseen.isnull().any()\nimpute_cols2 = missing_data_percent2[missing_data_percent2.gt(0)].index\nimpute_cols2","e3eb7c9b":"imp2 = SimpleImputer(strategy='constant', fill_value=0)\nunseen[impute_cols2] = imp2.fit_transform(unseen[impute_cols2])","6833bc60":"print(round((unseen.isnull().sum() \/ len(unseen)) * 100 , 2))","83b609c4":"one_val_cols = ['circle_id',\n 'loc_og_t2o_mou',\n 'std_og_t2o_mou',\n 'loc_ic_t2o_mou',\n 'last_date_of_month_6',\n 'last_date_of_month_7',\n 'last_date_of_month_8', 'std_og_t2c_mou_6',\n 'std_og_t2c_mou_7',\n 'std_og_t2c_mou_8' ,'std_ic_t2o_mou_6',\n 'std_ic_t2o_mou_7',\n 'std_ic_t2o_mou_8']","bf69dfb3":"unseen = unseen.drop(one_val_cols , 1)\nunseen.shape","24cb4f27":"unseen.info()","21461f03":"# Changing \"Object\" Dtype column to DateTime Dtype\nobject_col_data = unseen.select_dtypes(include=['object'])\n\n# convert to datetime\nfor i in object_col_data.columns:\n    unseen[i] = pd.to_datetime(unseen[i])\n\n    \nunseen.info()","b6384cbb":"unseen.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8'] , axis=1 , inplace=True)","1ede7602":"# Checking Desribe\n\nunseen.describe(include=\"all\")","9b10e785":"submission_data = unseen.set_index('id')\nsubmission_data.head()","07085004":"#Instantiating an object\nscaler = StandardScaler()\n\n#list of numerical Columns\nnumerical_cols = submission_data.select_dtypes(include=['float64', 'int64']).columns\n\n#Fititng the scaler\nsubmission_data[numerical_cols] = scaler.fit_transform(submission_data[numerical_cols])\n\nsubmission_data.head()","d4624be5":"submission_data = submission_data.reset_index()\nsubmission_data.head()","0002ee0e":"unseen['churn_probability'] =  dt_best.predict(unseen)","4f90b23e":"\noutput_id = submission_data['id']\noutput_churn = unseen['churn_probability']\n\noutput = pd.concat([output_id , output_churn] , axis=1)\noutput.head()","b4f54af7":"output.to_csv('unseen_test_final.csv',index=False)","e63c893e":"# 2. Data Understanding, Preparation, and Pre-Processing ","9c142bc1":"# 1. Loading dependencies & datasets\n\nLets start by loading our dependencies. We can keep adding any imports to this cell block, as we write mode and mode code.","874b6774":"This model has an accuracy of 88%. we will use this model for our prediction on unseen data.","7e282b73":"#### Treating rest of the null values ","c46fc2ca":"Most Important Featured from Decision Tree came out to be:\n\n    total_ic_mou_8\n    total_rech_amt_8\n    roam_og_mou_8\n    spl_ic_mou_8","4fc9930c":"**Analysis:** There is a drop in monthly subscription for churned customers in 7th Month.\n\n**Insight:** Decrement in the usage of internet is a strong factor indicating Chrun of he customer","26d37c9f":"**Conclusions from the PCA modelling:**\n\nModel has 80% Accuracy 52 features can explain 90% variance in the dataset","d6caa045":"Our dataset now has zero null values","6e14164c":"52 columns explains 90% of the variance, lets apply PCA with 52 components","dfa9aec4":"Box Plots for the the Internet consumption variabled gives no information.","964cdd81":"**Analysis:** We can see a gradual drop in the total recharge amount for churned customers in the 7th Month (Action Phase) and a Sudden drop in the month of August (Churn Phase).","613ba091":"# 0. Problem statement\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business\ngoal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n\nIn this competition, your goal is *to build a machine learning model that is able to predict churning customers based on the features provided for their usage.*\n\n**Customer behaviour during churn:**\n\nCustomers usually do not decide to switch to another competitor instantly, but rather over a\nperiod of time (this is especially applicable to high-value customers). In churn prediction, we\nassume that there are three phases of customer lifecycle :\n\n1. <u>The \u2018good\u2019 phase:<\/u> In this phase, the customer is happy with the service and behaves as usual.\n\n2. <u>The \u2018action\u2019 phase:<\/u> The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. It is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\n3. <u>The \u2018churn\u2019 phase:<\/u> In this phase, the customer is said to have churned. In this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month (September) is the \u2018churn\u2019 phase.","fba5e966":"## Decision Trees","ff73d9db":"**`Scaling`** ","ea303b18":"### Hyperparameter Tuning","d8491fed":"**Analysis:** Observed a huge drop in max recharge amount for customers that will churn in the action phase. Drop in max recharge amount indicated towards possibility of churning customer.","cafa237a":"# Creating submission file","dd928a47":"**Analysis:** Significantly it showing that volume based cost for 3G is much lower for Churned customers as compared to Non-Churned Customers and also there is a gradual drop in vbc in 8th month","176d0c4c":"# 4. Modelling","9747e09c":"## PCA","77169ed6":"There is no duplicate values in the data set.","5a17bbc1":"The given dataset has nearly 10% churn cases. The dataset is imbalanced.","03933ab1":"There are 30 Columns that have more than 70% null values. As they shall not provide enough information to our model, hence we will drop them.","3901efd0":"**Plan Of Action:**\n1. Total minutes usage of Incoming calls is the most important feature that can be leveraged to predict the possible churning of a high value customer\n\n\n2. Total Recharge Amount, Average Revenue per User, Local incoming T2m & Local outgoin T2T calls are amongst other variables\n\n\n3. Behaviour of Volume Based Cost is not a strong indicator of Churn\n\n\n4. Other considerble features are Roaming variables\n\n\n**Service Insights:**\n\n1. Very Less Amount of High Value customers are churning which is a good service indicator\n\n","56257f86":"We are given data for 3 Months(June, July & August)\n\n`The 'GOOD' Phase ` : June\n\n`The 'Action' Phase` :  July\n\n`The 'CHURN' Phase` : August","fc971535":"**Dropping off columns with ecactly 1 Unique value as it will not provide any Variance \/ Information to the Model**","dac977e5":"**`Test-Train Split`** ","3e68b8c8":"# Business Insights & Inferences","bd391b42":"The submission file should contain churn_probability values that have to be predicted for the unseen data provided (test.csv)\n","3411dd7b":"Our dataset now has zero null values","667d72f2":"**Analysis:** No customer using 2G data","75d00632":"We are going to follow the process called CRISP-DM.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/639px-CRISP-DM_Process_Diagram.png\" style=\"height: 400px; width:400px;\"\/>\n\nAfter Business and Data Understanding via EDA, we want to prepare data for modelling. Then evaluate and submit our predictions.","23902769":"__________________________________________","3e442d89":"**Analysis:** There is a tremendous drops for Average Revenue Per User in 8th month for churned customers","43398d3f":"The End","ea3c1169":"#### Treating rest of the null values ","2bde318a":"As there is High Class Imbalance, we shall use SMOTE to treat the same.","305344ea":"**Conclusions from the Logistic Regression modelling:**\n\nModel Accuracy is 79%\n\nConfusion matix clearly shows that the model has drawback in predicting churn as high false positives","2acc3d63":"We will begin our model building process with Principal Component Analysis as there is a very high number of variables in out dataset","8a093fb5":"**Dropping off columns with ecactly 1 Unique value as it will not provide any Variance \/ Information to the Model**","07a8dfdf":"Not getting any insight from box plot","81b0dce7":"## Steps for model building process:\n\n* Loading Data\n* Data Understanding, Preparation, and Pre-Processing :\n\n* Exploratory Data Analysis\n* Feature Engineering and Variable Transformation\n* Model Selection, Model Building, and  Prediction\n","d7873319":"# 3. Exploratory Data Analysis","76b5c3a4":"**Analysis**: We are getting a huge drop in 7th month recharge amount for churned customers.","e31fb11f":"## Logistic Regression using RFE"}}