{"cell_type":{"b46a3d10":"code","e578afa2":"code","9c714603":"code","6b2467ba":"code","488daad6":"code","33258623":"code","a17e8a54":"code","d2a55f45":"code","ad95ebb8":"code","ec4623ae":"code","f0d06b6e":"code","0b05b88b":"code","4f3bf291":"code","ca872f31":"code","0c908609":"code","1fbf1401":"code","ad4b4bf8":"markdown","41e8ecd1":"markdown","27257dd9":"markdown","d1faad9d":"markdown","137e1b38":"markdown"},"source":{"b46a3d10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e578afa2":"# import datasets\ntrain_file = '..\/input\/tabular-playground-series-nov-2021\/train.csv'\ntest_file = '..\/input\/tabular-playground-series-nov-2021\/test.csv'\nsub_file = '..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv'","9c714603":"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\nsubmission = pd.read_csv(sub_file)\n\n","6b2467ba":"train_df.head()","488daad6":"print(f'Number of rows: {train_df.shape[0]}')\nprint(f'Number of columns: {train_df.shape[1]}')\nprint(f'No of missing values: {sum(train_df.isna().sum())}')","33258623":"train_df.describe()","a17e8a54":"test_df.head()","d2a55f45":"submission.head()","ad95ebb8":"train_df.loc[:, 'f0':'f99'].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Greens')\\\n                            .background_gradient(subset=['25%'], cmap='Spectral')\\\n                            .background_gradient(subset=['50%'], cmap='seismic')\\\n                            .background_gradient(subset=['75%'], cmap='viridis')\\\n                            .background_gradient(subset=['mean'], cmap='cubehelix')\\\n                            .background_gradient(subset=['min'], cmap='Reds')\\\n                            .background_gradient(subset=['max'], cmap='Blues')","ec4623ae":"missing_train_df = pd.DataFrame(train_df.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'target']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\n\nmissing_train_percent_df = missing_train_df.copy()\nmissing_train_percent_df['count'] = missing_train_df['count']\/train_df.shape[0]\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\n\nmissing_test_percent_df = missing_test_df.copy()\nmissing_test_percent_df['count'] = missing_test_df['count']\/test_df.shape[0]\n\nfeatures = [feature for feature in train_df.columns if feature not in ['id', 'target']]\nmissing_train_row = train_df[features].isna().sum(axis=1)\nmissing_train_row = pd.DataFrame(missing_train_row.value_counts()\/train_df.shape[0]).reset_index()\nmissing_train_row.columns = ['no', 'count']\n\nmissing_test_row = test_df[features].isna().sum(axis=1)\nmissing_test_row = pd.DataFrame(missing_test_row.value_counts()\/test_df.shape[0]).reset_index()\nmissing_test_row.columns = ['no', 'count']","f0d06b6e":"cat_features =[]\nnum_features =[]\n\nfor col in train_df.columns:\n    if train_df[col].dtype=='float64':\n        num_features.append(col)\n    else:\n        cat_features.append(col)\nprint('Catagoric features: ', cat_features)\ndisplay(len(cat_features))\nprint('Numerical features: ', num_features)\ndisplay(len(num_features))","0b05b88b":"#Drop the Target Column\ntraining_data = train_df.drop(\"target\", axis=1)\n#Save the value of Target for usage\ntraining_label = train_df[\"target\"].copy()","4f3bf291":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nlgbm_pipeline = Pipeline([\n    ('imputer',SimpleImputer(strategy=\"most_frequent\")),\n    ('std_scaler', StandardScaler()),\n])\ntraining_prepared = lgbm_pipeline.fit_transform(training_data)","ca872f31":"import lightgbm as lgb\nlgbm_cls = lgb.LGBMClassifier()\nlgbm_cls.fit(training_prepared,training_label)","0c908609":"testing_prepared = lgbm_pipeline.fit_transform(test_df)\ntest_predictions = lgbm_cls.predict(testing_prepared)","1fbf1401":"submission = pd.read_csv(sub_file)\nsubmission['target'] = list(map(float, test_predictions))\nsubmission.to_csv('submission.csv', index=False)","ad4b4bf8":"# Introduction","41e8ecd1":"# Preparation","27257dd9":"# EDA","d1faad9d":"# Features","137e1b38":"# Modeling"}}