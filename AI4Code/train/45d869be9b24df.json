{"cell_type":{"b6a04284":"code","49c214d3":"code","6c1d4725":"code","b769000f":"code","07e8580b":"code","af62ca75":"code","264d23f7":"code","02149aac":"code","632d959c":"code","adda17de":"code","21b2f298":"code","042f7863":"code","a644560c":"code","147f2191":"code","111c44fb":"code","6c67db74":"code","5a5c39f0":"code","0604db29":"code","f261c863":"code","4cf7a105":"code","23ce10db":"code","0f43371e":"code","aae91790":"code","eaf6a1ae":"code","0e0c8f17":"code","a91d6a0c":"code","8bffe35b":"code","af1f5bdd":"code","e4a8db6b":"code","29142584":"code","4369f318":"code","ffc1fea3":"code","d8029c2f":"code","9e2098ad":"code","c1a15588":"code","5fa237f0":"code","99075647":"code","05a40b63":"code","07b9eba2":"code","7765cce1":"code","a0a36dcb":"code","33f715ec":"code","d3d37ef6":"code","a38b5042":"code","9ce486ca":"code","30fee357":"code","03eab1a6":"code","b88dd2d2":"code","ccf5791a":"code","2041e9f0":"code","76fe1e37":"code","8704ae8e":"code","fc21947b":"code","5a62912c":"markdown","d4edf178":"markdown","0072f104":"markdown","4bfa882e":"markdown","abec3b0d":"markdown","c00d71df":"markdown","e41c9333":"markdown","316c4282":"markdown"},"source":{"b6a04284":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Here we are just importing which are important for starting with.. and will add-on once I need more when reaching towards Modeling and Prdiction.","49c214d3":"# Get File Path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6c1d4725":"train_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')","b769000f":"train_data.head()","07e8580b":"print(train_data.shape, test_data.shape, sample_submission.shape)","af62ca75":"test_data['id'].head()","264d23f7":"test_data['id'].tail()","02149aac":"sample_submission['id'].head()","632d959c":"train_data.info()","adda17de":"# Lets get the % of each null values.\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)\n# Cool.. No NaN Values in train_data","21b2f298":"# Lets get the % of each null values.\ntotal = test_data.isnull().sum().sort_values(ascending=False)\npercent_1 = test_data.isnull().sum()\/test_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)\n# Cool.. No NaN Values in test_data","042f7863":"#Using Pearson Correlation\n\nplt.figure(figsize=(20,10))\ncor = train_data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","a644560c":"#Correlation with output variable\ncor_target = abs(cor[\"target\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features\n\n# Seems none of the numeric feature have much correlation with our target variable.\n# Correlation coefficients whose magnitude are between 0.5 and 0.7 indicate variables which can be considered moderately correlated. Correlation coefficients whose magnitude are between 0.3 and 0.5 indicate variables which have a low correlation.","147f2191":"#Get list of categorical variables\ns = (train_data.dtypes == 'object')\ntrain_data_cat_var = list(s[s].index)\n\ns = (test_data.dtypes == 'object')\ntest_data_cat_var = list(s[s].index)\n\nprint(\"Categorical variables from train_data:\", train_data_cat_var)\nprint(\"-\"*30)\nprint(\"Categorical variables from test_data:\", test_data_cat_var)","111c44fb":"#train_data['bin_3'].unique() \n#train_data['bin_3'].value_counts() \n#train_data['bin_3'].unique().sum()\n#train_data.groupby('bin_3').size()\nlen(train_data['bin_3'].unique())","6c67db74":"# write a function to get the count of distinct value in each categorical value\ndef get_Unique_Count(list_cat_var) :\n    cat_dict = dict()\n    for i in list_cat_var:\n        cat_dict[i] = len(train_data[i].unique())\n    return cat_dict","5a5c39f0":"print(get_Unique_Count(list(train_data_cat_var))) \nprint(get_Unique_Count(list(test_data_cat_var))) ","0604db29":"# Dropping off un-used features.\ntrain_data.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5'], axis = 1, inplace = True)\ntest_data.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5'], axis = 1, inplace = True)","f261c863":"# removing un-used features from our categorical features.\nprint(len(train_data_cat_var))\ntrain_data_cat_var = [ele for ele in train_data_cat_var if ele not in  ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5']]\nprint(len(train_data_cat_var))","4cf7a105":"print(len(test_data_cat_var))\ntest_data_cat_var = [ele for ele in test_data_cat_var if ele not in  ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5']]\nprint(len(test_data_cat_var))","23ce10db":"# Lets transform the Categorical Features into Number using get_dummies function (One Hot Encoding)\nfinal_train_data = pd.get_dummies(train_data, columns=train_data_cat_var, drop_first=True)\nprint(final_train_data.shape, train_data.shape)\nfinal_train_data.head()","0f43371e":"final_test_data = pd.get_dummies(test_data, columns=test_data_cat_var, drop_first=True)\nprint(final_test_data.shape, test_data.shape)\nfinal_test_data.head()","aae91790":"# Defining Feature and Target.\n#print (final_train_data.columns)\nfeatures = final_train_data.drop(['target'], axis = 1).columns\ntarget = final_train_data[\"target\"]\nprint(\"Features\", features)\nprint('--'*10)\nprint (\"Target\", target.head())","eaf6a1ae":"# split the train_data into 2 DF's aka X_train, X_test, y_train, y_test.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(final_train_data[features], target, test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","0e0c8f17":"# test_data \nX_test_df  = final_test_data[features].copy()\nX_test_df.head()","a91d6a0c":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","8bffe35b":"# ROC and AUR Curve related importing the libraries\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, classification_report","af1f5bdd":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred_lr = logreg.predict(X_test)\n#print(Y_pred_lr)\n\n","e4a8db6b":"logreg_score = round(logreg.score(X_train, y_train) * 100, 2)\nprint(\"Score (LogisticRegression)\", logreg_score)\n","29142584":"logreg_accuracy_score = round(accuracy_score(y_test, Y_pred_lr) * 100, 2)\nprint(\"Accuracy Score (LogisticRegression)\", logreg_accuracy_score)","4369f318":"logreg_confusion_matrix = confusion_matrix(y_test, Y_pred_lr)\nlogreg_confusion_matrix","ffc1fea3":"logreg_roc_auc = roc_auc_score(y_test, Y_pred_lr)\nlogreg_roc_auc","d8029c2f":"# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_logreg, tpr_logreg, threshold_logreg = roc_curve(y_test,logreg.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_logreg)\nprint('True Positive Rate : ', tpr_logreg)\nprint('Threshold : ', threshold_logreg)","9e2098ad":"# Plotting the ROC Curve\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","c1a15588":"# Support Vector Machines\n\n#svc = SVC(gamma='auto')\n#svc.fit(X_train, y_train)\n#Y_pred_svc = svc.predict(X_test)\n\n\n","5fa237f0":"#svc_roc_auc = roc_auc_score(y_test, Y_pred_svc)\n#print('ROC AUR Score for SVC Model : ', svc_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\n#fpr_svc, tpr_svc, threshold_svc = roc_curve(y_test,svc.predict_proba(X_test)[:,1])\n#print('False Positive Rate : ', fpr_svc)\n#print('True Positive Rate : ', tpr_svc)\n#print('Threshold : ', threshold_svc)","99075647":"# Plotting the ROC Curve for Logistic Regression and SVC Model\n'''\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\nplt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()\n'''","05a40b63":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred_knn = knn.predict(X_test)","07b9eba2":"knn_roc_auc = roc_auc_score(y_test, Y_pred_knn)\nprint('ROC AUR Score for KNN Model : ', knn_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_knn, tpr_knn, threshold_knn = roc_curve(y_test,knn.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_knn)\nprint('True Positive Rate : ', tpr_knn)\nprint('Threshold : ', threshold_knn)","7765cce1":"# Plotting the ROC Curve for Logistic Regression ; SVC ; KNN Model\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\n#plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot(fpr_knn, tpr_knn, label = 'KNN Model (aread = %0.2f)' %knn_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","a0a36dcb":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred_gnb = gaussian.predict(X_test)","33f715ec":"gnb_roc_auc = roc_auc_score(y_test, Y_pred_gnb)\nprint('ROC AUR Score for Gaussian Naive Bayes Model : ', gnb_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_gnb, tpr_gnb, threshold_gnb = roc_curve(y_test,gaussian.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_gnb)\nprint('True Positive Rate : ', tpr_gnb)\nprint('Threshold : ', threshold_gnb)","d3d37ef6":"# Plotting the ROC Curve for Logistic Regression ; SVC ; KNN; Gaussian Naive Bayes Model\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\n#plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot(fpr_knn, tpr_knn, label = 'KNN Model (aread = %0.2f)' %knn_roc_auc)\nplt.plot(fpr_gnb, tpr_gnb, label = 'Gaussian Naive Bayes Model (aread = %0.2f)' %gnb_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","a38b5042":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)","9ce486ca":"rf_roc_auc = roc_auc_score(y_test, Y_pred_rf)\nprint('ROC AUR Score for Gaussian Naive Bayes Model : ', rf_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_rf, tpr_rf, threshold_rf = roc_curve(y_test,random_forest.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_rf)\nprint('True Positive Rate : ', tpr_rf)\nprint('Threshold : ', threshold_rf)","30fee357":"# Plotting the ROC Curve for Logistic Regression ; SVC ; KNN; Gaussian Naive Bayes Model\nplt.figure(figsize = (10, 10))\nplt.plot(fpr_logreg, tpr_logreg, label = 'Log Reg Model (aread = %0.2f)' %logreg_roc_auc)\n#plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot(fpr_knn, tpr_knn, label = 'KNN Model (aread = %0.2f)' %knn_roc_auc)\nplt.plot(fpr_gnb, tpr_gnb, label = 'G N Bayes Model (aread = %0.2f)' %gnb_roc_auc)\nplt.plot(fpr_rf, tpr_rf, label = 'R F Model (aread = %0.2f)' %rf_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","03eab1a6":"modelling_score = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'ROC AUR Score': [0, knn_roc_auc, logreg_roc_auc, \n              rf_roc_auc, gnb_roc_auc, 0, \n              0, 0, 0]})","b88dd2d2":"modelling_score.sort_values(by='ROC AUR Score', ascending=False)","ccf5791a":"# Predicting on actual test_data\nY_pred_test_df = random_forest.predict(X_test_df)\nY_pred_test_df ","2041e9f0":"X_test_df.head()","76fe1e37":"submission = pd.DataFrame( { 'id': X_test_df.id , 'target': Y_pred_test_df } )","8704ae8e":"print(\"Submission File Shape \",submission.shape)\nsubmission.head()","fc21947b":"submission.to_csv( '\/kaggle\/working\/submission1.csv' , index = False )","5a62912c":"From above we see that there are some categorical variables which has more than 10 unique value such as nom_5; nom_6; nom_7; nom_8; nom_9; ord_3; ord_4; ord_5. So we will not be using these to transform.\nWill just transoform remaining as we have limited counts, also it is recommended to transform any categorical variable if the max unique is less than 15, but here we will stick max to 10.","d4edf178":"# Introduction\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform.\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","0072f104":"# Handling Categorical Features","4bfa882e":"# Identifying best Model from above","abec3b0d":"# Submission","c00d71df":"# Correlation Heatmap","e41c9333":"# Modeling","316c4282":"From my previous challenge... i experienced that the test and train categorical variables may have different set of values.. so better to check at first. If found we can merge the data-set and then transform them."}}