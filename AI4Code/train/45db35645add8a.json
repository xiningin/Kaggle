{"cell_type":{"af68aac6":"code","36918d09":"code","191edde6":"code","0ee7eabd":"code","f9735422":"code","5ee023da":"code","1735377b":"code","a9e86caa":"code","d885609b":"code","ae4b0eee":"code","7ffc1403":"code","1be536ce":"markdown","d1f03e19":"markdown","d44a5719":"markdown","cd227db8":"markdown","c50b1109":"markdown","557c9c1b":"markdown","eeea824d":"markdown","662470f8":"markdown","69656bbd":"markdown","49803d1f":"markdown","cd9edf65":"markdown","d99772a8":"markdown"},"source":{"af68aac6":"import tensorflow as tf","36918d09":"#1\ndef pos_weight(pred_tensor, pos_tensor, neg_weight=1, pos_weight=1):\n    # neg_weight for when pred position < target position\n    # pos_weight for when pred position > target position\n    gap = tf.cast(tf.math.argmax(pred_tensor, axis=-1),tf.float32) - pos_tensor\n    gap = tf.cast(gap, tf.float32)\n    return tf.where(gap < 0, -neg_weight * gap, pos_weight * gap)","191edde6":"inputs = tf.constant([[0.1, 0.1, 0.1, 0.1, 0.8]], dtype=tf.float32) # pred as 4\ntargets = tf.constant([3.0])\n\npos_weight(inputs, targets, 1, 1)","0ee7eabd":"inputs = tf.constant([[0.8, 0.1, 0.1, 0.1, 0.1]]) # pred as 0\ntargets = tf.constant([3.0])\n\npos_weight(inputs, targets, 1, 1)","f9735422":"from tensorflow import keras","5ee023da":"inputs = tf.constant([[0.1, 0.1, 0.1, 0.1, 0.8]]) # pred as 4\ntargets = tf.broadcast_to(tf.constant([[3]],dtype=tf.float32),inputs.shape)\n#2\nloss_func = keras.losses.CategoricalCrossentropy()\nprint(loss_func(inputs,targets))","1735377b":"#3\ndef loss_fn(y_true, y_pred):\n    start_positions = y_true[0]\n    end_positions = y_true[1]\n\n    start_logits = y_pred[0]\n    end_logits = y_pred[1]\n\n\n\n\n    loss_fct =  keras.losses.CategoricalCrossentropy(reduction = tf.keras.losses.Reduction.NONE) # do reduction later\n    \n    loss_fnc_start_positions=tf.broadcast_to(start_positions,start_logits.shape)\n    loss_fnc_end_positions=tf.broadcast_to(end_positions,end_logits.shape)\n\n    start_loss = loss_fct(start_logits, loss_fnc_start_positions) * pos_weight(start_logits, start_positions, 1, 1)\n\n    end_loss = loss_fct(end_logits, loss_fnc_end_positions) * pos_weight(end_logits, end_positions, 1, 1)\n\n    start_loss = tf.math.reduce_mean(start_loss,keepdims = True)\n\n    end_loss = tf.math.reduce_mean(end_loss,keepdims = True)\n    \n    total_loss = (start_loss + end_loss)\n    return total_loss","a9e86caa":"# argmax pred for the end is 3, target is 3\nstart = tf.constant([[0.1, 0.1, 0.1, 0.8, 0.1]])\nstart_target = tf.constant([1.0])\n\nend = tf.constant([[0.1, 0.1, 0.1, 0.8, 0.1]])\nend_target = tf.constant([3.0])","d885609b":"loss_fn([start_target,end_target], [start,end])","ae4b0eee":"start = tf.constant([[0.1, 0.1, 0.8, 0.1, 0.1]])\nstart_target = tf.constant([1.0])\n\nend = tf.constant([[0.1, 0.1, 0.1, 0.8, 0.1]])\nend_target = tf.constant([3.0])","7ffc1403":"loss_fn([start_target,end_target], [start,end])","1be536ce":"**This Loss function works perfectly fine in Keras, except, that it does an error in the validation data ( or in the second fold ). This error is related to dimensions ( I think this might be from the pos_weight function : \n'tf.cast(tf.math.argmax(pred_tensor, axis=-1),tf.float32) - pos_tensor'\nsome thing that has to do with the axis).\nPlease if you find a solution or a better version of this loss function make sure to publish it, thanks !**","d1f03e19":"**That's it! I hope this notebook was constructive. I myself am a beginner so please if I did anything wrong or I gave an incomplete or wrong information make sure to comment so I can edit the notebook and correct it**","d44a5719":"**![](http:\/\/)You can find the original notebook in PyTorch  [here](https:\/\/www.kaggle.com\/laevatein\/tweat-the-loss-function-a-bit)**","cd227db8":"We get 3.8626509 as a result !! \nIt is pretty close to the original result ( 3.5881 )","c50b1109":"**Below is my personal try.\nI insist that this try is not correct and still buggy ( in the validation set ) So if you find a fix for it make you sure to comment below \ud83d\ude01**","557c9c1b":"Same here, we have 1.9313254 ( Keras ) and 1.7940 ( PyTorch )","eeea824d":"#  **Welcome!**","662470f8":"**\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a**","69656bbd":"But I don't think this can modify anything in our process. So, let's continue :) ","49803d1f":"As you can notice here the value of the Categorical CrossEntropy loss ( 1.9313254 ) is not the same from the value in the original notebook with PyTorch ( 1.7940 ) even though the vectors are the same.\nInterestingly, I looked it up and it turns out its because its not the same mathematical formula ( Please check this information and comment below if I made a mistake or if you need to add something.\nKeras Categorical Crossentropy formula loss :\n![Keras Categorical Crossentropy formula](https:\/\/cloud.githubusercontent.com\/assets\/18217467\/25556005\/1bb51402-2d12-11e7-87ef-a9bad097a858.png)\n\n\n\nPyTorch Cross entropy loss formula:\n![PyTorch Cross entropy loss formula](https:\/\/i.stack.imgur.com\/bdrd8.png)","cd9edf65":"**And now, the main loss function ( which is a combination of #1 and # 2 )**","d99772a8":"**This is my try for the custom loss function thats been going trending in the discussion ( In Keras)**"}}