{"cell_type":{"59aa5265":"code","78953d23":"code","962ab24d":"code","56203af6":"code","fe218765":"code","13a00243":"code","2fed8ef8":"code","6a49cf9a":"code","24fd442a":"code","5f190d43":"code","fed4b47e":"code","aa5d64eb":"code","027a932a":"code","5501f3ee":"code","631c0e06":"code","bbb01fb6":"code","e7ecb0e5":"code","1b6ca81c":"code","b49f7fc8":"code","fa0ae884":"code","6b573c2a":"code","7ea2ea49":"code","eb825807":"code","a0ea927c":"code","529a6e12":"code","85fb624b":"code","85c46106":"code","257fcfd0":"code","e5ce0f93":"code","10fe11b9":"code","684e8d9a":"code","7bf7c4ae":"code","b5cdce3f":"code","a36b58af":"code","74299a83":"code","f80d63a2":"code","adcdbdec":"code","28a9fb05":"code","9a6ee7b8":"code","3080043d":"code","5bf5349e":"code","cb6e4d8d":"code","8108ec10":"code","2ea22018":"code","9d99a23e":"code","30484989":"code","189e198b":"code","80f6820a":"code","dac2ac8a":"code","a5321b9f":"code","ca05df9a":"code","c8abb896":"code","c8e4675b":"code","798418fb":"code","6e4e5331":"markdown","99eeb63b":"markdown","d025141d":"markdown","34a000cb":"markdown","bf77790c":"markdown","7834e743":"markdown","94301382":"markdown","4bcc5900":"markdown","f7821b2f":"markdown","62a10448":"markdown","f0f8a8df":"markdown","91c37d19":"markdown","87fefaa7":"markdown","a6fc4553":"markdown","eebdbc4d":"markdown","1bcfded9":"markdown","9fca7880":"markdown","385cb021":"markdown","26708e41":"markdown","9c9071b3":"markdown","9e1e2aa4":"markdown","65a8f733":"markdown","f8039747":"markdown"},"source":{"59aa5265":"#importing necessary libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport seaborn\nimport warnings\nwarnings.filterwarnings(\"ignore\",category= DeprecationWarning)\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","78953d23":"#Reading the data\ndata = pd.read_csv('\/kaggle\/input\/reddit-india-flair-detection\/datafinal.csv')\n\ndataf = data.copy()\ndata","962ab24d":"data.columns","56203af6":"data = data.drop(['score','url','comms_num','author','timestamp'],axis=1)","fe218765":"data.head()","13a00243":"data['title'][0]","2fed8ef8":"data['body'][0]","6a49cf9a":"data['combined_features'][0]","24fd442a":"data['comments'][0]","5f190d43":"data = data.drop(['combined_features'],axis=1)\ndata.head()","fed4b47e":"data.info()","aa5d64eb":"data.describe()","027a932a":"data['flair'].unique()","5501f3ee":"data.groupby('flair')['title'].describe()","631c0e06":"data.groupby('flair')['id'].describe()","bbb01fb6":"data.groupby('flair')['body'].describe()","e7ecb0e5":"data.groupby('flair')['comments'].describe()","1b6ca81c":"#Dropping the rows corresponding to date-time flairs\nf = data['flair'].dropna()\nregx = re.compile(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{4} [\\d]{1,2}:[\\d]{1,2}\")\nfor __ in f:\n    #print(flair)\n    x = regx.search(__)\n    if x is not None:\n        #print(x.group())\n        d = data[data.flair == x.group()]\n        #print(d)\n        data = data.drop(d.index)","b49f7fc8":"data['flair'].unique()","fa0ae884":"data[data['flair'] == np.nan].describe()","6b573c2a":"data = data.dropna(subset=['flair'])","7ea2ea49":"data.info()","eb825807":"data['text'] = data['title'].astype(str) + data['body'].astype(str) + data['comments'].astype(str)","a0ea927c":"data_final = data[['flair','id','text']]\ndata_final.head()","529a6e12":"data_final.describe()","85fb624b":"data_final.groupby('flair')['text'].describe()","85c46106":"data_final['text'] = data_final['text'].str.replace(\"[^a-zA-Z0-9 \\n.]\",\" \")","257fcfd0":"\"\"\"Now we have clean data!!!\"\"\"\ndata_final.head(10) ","e5ce0f93":"\"\"\"\n    1. Removing all punctuation\n    2. Removing stop-words\n    3. Returns a clean text\n\"\"\"\ndef clean_txt(mess):\n    \n    nonpunc = [char for char in mess if char not in string.punctuation] #list of strings which are non-punc\n    \n    nonpunc = \"\".join(nonpunc) #join back to form the whole string\n    \n    return [word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]    ","10fe11b9":"data_final['text'] = data_final['text'].apply(clean_txt)","684e8d9a":"from nltk import WordNetLemmatizer\n\nle = WordNetLemmatizer()\n\ndata_final['text'] = data_final['text'].apply(lambda x : [le.lemmatize(word) for word in x])\ndata_final['text'] = data_final['text'].apply(lambda x : \" \".join(x))\ndata_final['text']","7bf7c4ae":"qwe = data_final['text'].copy()\nqwe","b5cdce3f":"from sklearn.feature_extraction.text import CountVectorizer","a36b58af":"bow_transformer = CountVectorizer(analyzer = clean_txt).fit(data_final['text'])","74299a83":"text_bow = bow_transformer.transform(data_final['text'])","f80d63a2":"print(f'Shape of sparse matrix is {text_bow.shape}')\nprint(f'Length of dictionary is {len(bow_transformer.vocabulary_)}')\nprint(f'Number of non=zero occusrances is {text_bow.nnz}')\nsparsity = (text_bow.nnz\/(text_bow.shape[0]*text_bow.shape[1]))*100\nprint('Sparsity :', sparsity)","adcdbdec":"from sklearn.feature_extraction.text import TfidfTransformer","28a9fb05":"tfidf_transformer = TfidfTransformer().fit(text_bow)\ntext_tfidf = tfidf_transformer.transform(text_bow)","9a6ee7b8":"print(f'Shape of tfidf of text is {text_tfidf.shape}')","3080043d":"from sklearn.naive_bayes import MultinomialNB","5bf5349e":"model = MultinomialNB().fit(text_tfidf,data_final['flair'])","cb6e4d8d":"predictions = model.predict(text_tfidf)","8108ec10":"from sklearn.metrics import classification_report","2ea22018":"print(classification_report(data_final['flair'],predictions))","9d99a23e":"from sklearn.pipeline import Pipeline","30484989":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=clean_txt)),\n    ('tfidf',TfidfTransformer()),\n    ('classifier',MultinomialNB()),\n])","189e198b":"from sklearn.model_selection import train_test_split","80f6820a":"text_train, text_test, flair_train, flair_test = train_test_split(data_final['text'],data_final['flair'])","dac2ac8a":"pipeline.fit(text_train,flair_train)","a5321b9f":"predictions = pipeline.predict(text_test)","ca05df9a":"print(classification_report(flair_test,predictions))","c8abb896":"print(text_test)","c8e4675b":"ids = [data_final.iloc[int(i)]['id'] for i in text_test.index]\nPredicted_df = pd.DataFrame({'ID':ids,'Text':text_test,'PredictedFlair':predictions}).reset_index(drop=True)","798418fb":"Predicted_df","6e4e5331":"Note that some of the flairs are date values. Let's explore it a bit to see what it is...","99eeb63b":"### Removing Punctuation, Stopwords and Tokenization","d025141d":"There are 18 unique flairs.","34a000cb":"##### Note that combined features column is actually combination text of three columns as title,comments, url and combined_features.","bf77790c":"### Vectorization","7834e743":"There are some nan values in flair. Exploring it....","94301382":"# Prediction \n\n## Pipeline","4bcc5900":"# Traning a Model (Naive Bayes)","f7821b2f":"##### Conclusion: Devised a model which can predict flair if given a text.","62a10448":"# Data Cleaning","f0f8a8df":"##### Removing Special characters","91c37d19":"# Train-test split","87fefaa7":"#### Dropping unwanted columns.","a6fc4553":"I am dropping that column too for ease of analysis.","eebdbc4d":"### Prediction","1bcfded9":"### Lemmatization","9fca7880":"### TfIdf","385cb021":"##### Note: There are no significant data related to nan flair value. Hence we can drop it.","26708e41":"Seems like there is no significant details present related to those 'date-time' flairs. So let's drop those entries.","9c9071b3":"# Model Evaluation","9e1e2aa4":"##### Combine [title,body and comments] for text-processing","65a8f733":"# Text Preprocessing","f8039747":"### Input "}}