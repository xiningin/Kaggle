{"cell_type":{"eddc26c3":"code","0e3a79fb":"code","c1ad55f4":"code","dd541676":"code","5b0303d0":"code","d7d138e1":"code","2b06a352":"code","2da3540d":"code","c8f1fb6b":"code","65e27465":"markdown","8f00c2d0":"markdown","54e6b1ea":"markdown","8cd1d4d4":"markdown","ce81a9c5":"markdown","d7143c4e":"markdown","a5e3afa2":"markdown"},"source":{"eddc26c3":"#Import pandas, tensorflow e keras\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nimport keras\nfrom keras import regularizers\nfrom keras.utils import to_categorical\nfrom keras import models\nfrom keras import layers\nfrom keras import backend as K\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#   for filename in filenames:\n#       print(os.path.join(dirname, filename))\n#Lettura dati\ndf = pd.read_csv(\"\/kaggle\/input\/forest-cover-type-prediction\/train.csv\")\ndfT = pd.read_csv(\"\/kaggle\/input\/forest-cover-type-prediction\/test.csv\")","0e3a79fb":"#Selezioniamo le caratteristiche\nx = df[df.columns[1:55]]\nxT = dfT[dfT.columns[1:55]]\n#Selezioniamo le etichette (8) \ny = df.Cover_Type\n#Split data into train and test \nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.7, random_state =  90)","c1ad55f4":"# Normalize Training Data \nscaler = preprocessing.StandardScaler()\nscaler.fit(x_train.values[:,0:10])\nx_train_norm = scaler.transform(x_train.values[:,0:10])\nx_test_norm = scaler.transform(x_test.values[:,0:10])\nx_sub = scaler.transform(xT.values[:,0:10])\nx_train_norm=numpy.concatenate((x_train_norm,x_train.values[:,10:]),axis=1)\nx_test_norm=numpy.concatenate((x_test_norm,x_test.values[:,10:]),axis=1)\nx_sub=numpy.concatenate((x_sub,xT.values[:,10:]),axis=1)","dd541676":"def l0_reg(weight_matrix):\n    temp = K.abs(weight_matrix)>0.005\n    if_true = tf.reduce_sum(tf.cast(temp, tf.float32))\n    return if_true","5b0303d0":"modelF = models.Sequential()\nmodelF.add(layers.Dense(32,name=\"Layer_1\",activation='relu',input_dim=54,kernel_initializer='he_normal',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.08)))\nmodelF.add(layers.BatchNormalization())\nmodelF.add(layers.Dense(16,name=\"Layer_2\",activation='relu'))\nmodelF.add(layers.Dense(64,name=\"Layer_22\",activation='relu'))\nmodelF.add(layers.BatchNormalization())\nmodelF.add(layers.Dense(64,name=\"Layer_23\",activation='relu'))\nmodelF.add(layers.BatchNormalization())\nmodelF.add(layers.Dense(16,name=\"Layer_4\",activation='relu'))\nmodelF.add(layers.Dense(8,name=\"Layer_5\",activation='softmax'))\nmodelF.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodelF.summary()","d7d138e1":"Net4 = modelF.fit(\n x_train_norm, y_train,\n epochs= 400, batch_size = 256,\n validation_data = (x_test_norm, y_test))","2b06a352":"_, train_acc = modelF.evaluate(x_train_norm, y_train, verbose=0)\n_, test_acc = modelF.evaluate(x_test_norm, y_test, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss during training\nplt.rcParams['figure.figsize'] = (12.0, 9.0)\nplt.subplot(211)\nplt.title('Loss')\nplt.plot(Net4.history['loss'], label='train')\nplt.plot(Net4.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy during training\nplt.subplot(212)\nplt.title('Accuracy')\nplt.plot(Net4.history['acc'], label='train')\nplt.plot(Net4.history['val_acc'], label='test')\nplt.legend()\nplt.show()","2da3540d":"test_predictions=modelF.predict_classes(x_sub, batch_size=256, verbose=0)","c8f1fb6b":"solutions = pd.DataFrame({'Id':dfT.Id, 'Cover_Type':test_predictions})\nsolutions.to_csv('submission.csv',index=False)","65e27465":"Creiamo una rete a 5 Livelli in cui abbiamo definito l'architettura, il tipo di inizializzazione dei parametri.","8f00c2d0":"## Normalizziamo la rete come da teoria\n\\begin{equation}\n  x^{(i)} = x^{(i)}-\\frac{1}{m}\\sum_{i=1}^{m} x^{(i)} \\\\\n  x^{(i)} = \\frac{x^{(i)}}{\\frac{1}{m}\\sum_{i=1}^{m} {x^{(i)}}^2}\n\\end{equation}","54e6b1ea":"### Creiamo una nuova funzione di regolarizzazione da testare ;-) ","8cd1d4d4":"Verifichiamo l'efficacia della normalizzazione dei Dati in questo esempio tratto da Kaggle [Kaggle competition](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction).\n\nForest Cover Type Prediction<br>\nUse cartographic variables to classify forest categories\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3936\/logos\/front_page.png)","ce81a9c5":"> ## Creiamo la rete Neurale con Keras con Doppio regolarizzatore","d7143c4e":"Grafichiamo delle curve per la valutazione","a5e3afa2":"# Esempio normalizzazione DataSet"}}