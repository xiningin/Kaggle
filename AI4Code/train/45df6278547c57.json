{"cell_type":{"688a024b":"code","0d7fb955":"code","a001cc9f":"code","f1b56801":"code","429ac416":"code","a109712a":"code","c7f70461":"code","e6187626":"code","365810ee":"code","8dd5fa89":"code","0aa8591e":"code","055d873e":"code","9ecb287b":"code","ed31a5cb":"code","e3270adb":"code","0499679f":"code","a3985bcd":"code","d423b4d2":"code","24cc6617":"code","073b5389":"code","103bc81c":"code","67f0a1d0":"code","11218638":"code","a3971b44":"code","0f45fb30":"code","e780bb35":"code","8dc2756e":"code","73e12d49":"code","9b781047":"code","6dd0a845":"code","8717367f":"code","ab13132c":"code","d1a0ecca":"code","69d13c47":"code","cb41c250":"code","a14509fc":"code","0d109e7e":"code","6e1f7b77":"code","111c5039":"code","be68d4a8":"code","907c9e0d":"code","a1cdc058":"code","42576562":"code","a9708f5b":"code","9af632e8":"code","26ce77f4":"code","501e4ae9":"code","dff55b08":"code","56ddf669":"code","3ec8e21e":"code","459fe6cc":"code","06eb7bae":"code","502d69a5":"code","a508337a":"code","019a14dd":"code","a2ff0319":"code","82ab8ede":"code","cee5105f":"code","4e7cc7dc":"code","9a3464b7":"code","19c3e5ec":"code","6c6ad1ee":"code","d240262e":"code","62d91ec2":"code","a93dbc37":"markdown","2cf78de6":"markdown","708eacee":"markdown","d5bcd379":"markdown","80f69092":"markdown","8b587ea3":"markdown","c968312e":"markdown","e34e220c":"markdown","b64dd593":"markdown","9407d6b6":"markdown","84cd27be":"markdown","e5548074":"markdown","18d04bf1":"markdown","4b114c04":"markdown","7681bd0c":"markdown","aa59807f":"markdown","b94e89db":"markdown","cc58b52e":"markdown","2f5b4419":"markdown","3d595e94":"markdown","a43732a9":"markdown","e8170946":"markdown","6b21eadd":"markdown","dc72d9c9":"markdown","17c6a139":"markdown","d0f136a4":"markdown","00b5c718":"markdown","e3074ef5":"markdown","b5695804":"markdown","bb4a127b":"markdown","c62bc503":"markdown","b9c279b4":"markdown","3ac699f5":"markdown","1e841205":"markdown","e08ebf81":"markdown","218cba06":"markdown","a96f1a7f":"markdown","ae57e6f7":"markdown","0c74eb65":"markdown","61a68ec7":"markdown","1d2c52be":"markdown","a39eda00":"markdown","c25e6990":"markdown","7038d49e":"markdown","eb9f13d5":"markdown","7b6b2b8b":"markdown","c4d92f56":"markdown","65a312b0":"markdown"},"source":{"688a024b":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly\nimport plotly.offline as py\npy.init_notebook_mode()\n\n\nimport time\nfrom tqdm import tqdm_notebook as tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0d7fb955":"train = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv\")\ntrain.head()","a001cc9f":"train.shape","f1b56801":"calendar = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/calendar.csv\")\ncalendar.head()","429ac416":"sell_prices = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/sell_prices.csv\")\nsell_prices.head()","a109712a":"series_cols = train.columns[train.columns.str.contains(\"d_\")].values\nlevel_cols = train.columns[train.columns.str.contains(\"d_\")==False].values","c7f70461":"train.head(1)","e6187626":"sns.set_palette(\"colorblind\")\n\nfig, ax = plt.subplots(5,1,figsize=(20,28))\ntrain[series_cols].sum().plot(ax=ax[0])\nax[0].set_title(\"Top-Level-1: Summed product sales of all stores and states\")\nax[0].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"state_id\")[series_cols].sum().transpose().plot(ax=ax[1])\nax[1].set_title(\"Level-2: Summed product sales of all stores per state\");\nax[1].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"store_id\")[series_cols].sum().transpose().plot(ax=ax[2])\nax[2].set_title(\"Level-3: Summed product sales per store\")\nax[2].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"cat_id\")[series_cols].sum().transpose().plot(ax=ax[3])\nax[3].set_title(\"Level-4: Summed product sales per category\")\nax[3].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"dept_id\")[series_cols].sum().transpose().plot(ax=ax[4])\nax[4].set_title(\"Level-4: Summed product sales per product department\")\nax[4].set_ylabel(\"Unit sales of all products\");","365810ee":"submission = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-uncertainty\/sample_submission.csv\")\nsubmission.head(10)","8dd5fa89":"submission.shape","0aa8591e":"np.random.choice(submission.id.values, replace=False, size=15)","055d873e":"def find_quartil(l):\n    \n    if \"0.005\" in l:\n        return 0.005\n    elif \"0.025\" in l:\n        return 0.025\n    elif \"0.165\" in l:\n        return 0.165\n    elif \"0.25\" in l:\n        return 0.25\n    elif \"0.5\" in l:\n        return 0.5\n    elif \"0.75\" in l:\n        return 0.75\n    elif \"0.835\" in l:\n        return 0.835\n    elif \"0.975\" in l:\n        return 0.975\n    elif \"0.995\" in l:\n        return 0.995\n    else:\n        return 0\n    \ndef find_state(l):\n    if \"CA\" in l:\n        return \"CA\"\n    elif \"TX\" in l:\n        return \"TX\"\n    elif \"WI\" in l:\n        return \"WI\"\n    else:\n        return \"Unknown\"\n    \ndef find_category(l):\n    if \"FOODS\" in l:\n        return \"foods\"\n    elif \"HOBBIES\" in l:\n        return \"hobbies\"\n    elif \"HOUSEHOLD\" in l:\n        return \"household\"\n    else:\n        return \"Unknown\"","9ecb287b":"submission_eda = pd.DataFrame(submission.id, columns=[\"id\"])\nsubmission_eda.loc[:, \"lb_type\"] = np.where(submission.id.str.contains(\"validation\"), \"validation\", \"evaluation\")\nsubmission_eda.loc[:, \"u\"] = submission.id.apply(lambda l: find_quartil(l))\nsubmission_eda.loc[:, \"state\"] = submission.id.apply(lambda l: find_state(l))\nsubmission_eda.loc[:, \"category\"] = submission.id.apply(lambda l: find_category(l))","ed31a5cb":"sns.set_palette(\"husl\")\n\nfig, ax = plt.subplots(3,3,figsize=(20,20))\nsns.countplot(submission_eda.u, ax=ax[0,0]);\nsns.countplot(submission_eda.lb_type, ax=ax[0,1]);\nsns.countplot(submission_eda.state, ax=ax[1,0]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"validation\"].state, ax=ax[1,1]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"evaluation\"].state, ax=ax[1,2]);\nsns.countplot(submission_eda.category, ax=ax[2,0]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"validation\"].category, ax=ax[2,1]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"evaluation\"].category, ax=ax[2,2]);\nfor n in range(1,3):\n    ax[n,2].set_title(\"in evaluation\")\n    ax[n,1].set_title(\"in validation\")","e3270adb":"def spl_denominator(train_series):\n    N = len(train_series)\n    sumup = 0\n    for n in range(1, N):\n        sumup += np.abs(train_series[n]-train_series[n-1])\n    return sumup\/(N-1)","0499679f":"def spl_numerator(dev_series, Q, u):\n    sumup = 0\n    for m in range(len(dev_series)):\n        if Q[m] <= dev_series[m]:\n            sumup += (dev_series[m] - Q[m])*u\n        else:\n            sumup += (Q[m] - dev_series[m])*(1-u)\n    return sumup","a3985bcd":"def spl(train_series, dev_series, Q, u):\n    h = len(dev_series)\n    spl_denomina = spl_denominator(train_series)\n    spl_numera = spl_numerator(dev_series, Q, u)\n    \n    return spl_numera\/(h*spl_denomina)","d423b4d2":"idx = 1000","24cc6617":"train[level_cols].iloc[idx]","073b5389":"train.loc[train.item_id==\"HOUSEHOLD_1_445\"].store_id.unique()","103bc81c":"plt.figure(figsize=(20,5))\nplt.plot(train[series_cols].iloc[idx].values, 'o')\nplt.title(\"Item 445 daily sales in shop CA_1\");\nplt.xlabel(\"observed days\")\nplt.ylabel(\"Unit sales\");","67f0a1d0":"timeseries = train[series_cols].iloc[idx].values\nh = 28\n\ntrain_timeseries = timeseries[0:len(timeseries)-h]\ndev_timeseries = timeseries[(len(timeseries)-h)::]\n\nprint(len(train_timeseries), len(dev_timeseries))","11218638":"naive_val = train_timeseries[-1]\nnaive_Q = np.ones(dev_timeseries.shape) * naive_val\nnaive_Q","a3971b44":"spl(train_timeseries, dev_timeseries, naive_Q, 0.5)","0f45fb30":"naive_val","e780bb35":"residuals = train_timeseries - naive_val","8dc2756e":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(residuals, ax=ax[0], kde=False)\nax[0].set_xlabel(\"residuals\")\nax[0].set_ylabel(\"frequency\");\nax[0].set_title(\"Distribution of residuals\");","73e12d49":"np.mean(residuals)","9b781047":"std_dev = np.std(residuals)\nstd_h = np.ones(dev_timeseries.shape)\n\nfor h in range(1, 29):\n    std_h[h-1] = std_dev * np.sqrt(h)","6dd0a845":"std_h","8717367f":"y_lower = np.ones(len(std_h))\ny_upper = np.ones(len(std_h))\nfor h in range(len(std_h)):\n    low_val = naive_Q[h] - 2.58 * std_h[h]\n    if low_val < 0:\n        y_lower[h] = 0\n    else:\n        y_lower[h] = low_val\n    y_upper[h] = naive_Q[h] + 2.58 * std_h[h]","ab13132c":"plt.figure(figsize=(20,5))\nplt.plot(y_lower, c=\"r\", label=\"0.005 boundary\")\nplt.plot(y_upper, c=\"g\", label=\"0.995 boundary\")\nplt.plot(naive_Q, 'o', c=\"b\", label=\"predicted value\")\nplt.title(\"Computing 99% PI for one timeseries of level 12\");\nplt.xlabel(\"time horizont h=28 days\")\nplt.ylabel(\"Unit sales\");\nplt.legend();","d1a0ecca":"timeseries = train[series_cols].sum().values\nlen(timeseries)","69d13c47":"train_timeseries = timeseries[0:-28]\neval_timeseries = timeseries[-28::]\nprint(len(train_timeseries), len(eval_timeseries))\ndays = np.arange(1, len(series_cols)+1)","cb41c250":"plt.figure(figsize=(20,5))\nplt.plot(days[0:-28], train_timeseries, label=\"train\")\nplt.plot(days[-28::], eval_timeseries, label=\"validation\")\nplt.title(\"Top-Level-1: Summed product sales of all stores and states\");\nplt.legend()\nplt.xlabel(\"Day\")\nplt.ylabel(\"Unit sales\");","a14509fc":"dates = calendar.iloc[0:len(timeseries)].date.values\ndf = pd.DataFrame(dates, columns=[\"ds\"])\ndf.loc[:, \"y\"] = timeseries\ndf.head()","0d109e7e":"train_df = df.iloc[0:-28]\ntrain_df.shape","6e1f7b77":"eval_df = df.iloc[-28::]\neval_df.shape","111c5039":"uncertainty_interval_width = 0.25","be68d4a8":"m = Prophet(interval_width=uncertainty_interval_width)\nm.fit(train_df)\nfuture = m.make_future_dataframe(periods=28)\nforecast = m.predict(future)\nforecast.head()","907c9e0d":"col_int = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\nforecast[col_int].head()","a1cdc058":"plt.plot(forecast.iloc[-28::].yhat.values, 'o', label=\"predicted yhat\")\nplt.plot(eval_df.y.values, 'o-', label=\"target\")\nplt.legend();","42576562":"fig = plot_plotly(m, forecast)  \npy.iplot(fig)","a9708f5b":"uncertainty_interval_width = 0.25","9af632e8":"f_cols = [col for col in submission.columns if \"F\" in col]","26ce77f4":"submission_val = submission[submission.id.str.contains(\"validation\")].copy()","501e4ae9":"def plugin_total_predictions():\n    \n    for uncertainty_interval_width in [0.005, 0.025, 0.165, 0.25]:\n        upper = 1-uncertainty_interval_width\n        lower = uncertainty_interval_width\n    \n        m = Prophet(interval_width=uncertainty_interval_width)\n        m.fit(df)\n        future = m.make_future_dataframe(periods=28)\n        forecast = m.predict(future)\n    \n        submission_val.loc[\n            (submission_val.id.str.contains(\"Total\")) & (submission_val.id.str.contains(str(lower))),f_cols\n        ] = np.round(forecast.yhat_lower.values[-28::])\n    \n        submission_val.loc[\n            (submission_val.id.str.contains(\"Total\")) & (submission_val.id.str.contains(str(upper))),f_cols\n        ] = np.round(forecast.yhat_upper.values[-28::])\n    \n    submission_val.loc[\n        (submission_val.id.str.contains(\"Total\")) & (submission_val.id.str.contains(str(0.5))),f_cols\n    ] = forecast.yhat.values[-28::]\n    \n    return submission_val","dff55b08":"submission_val = plugin_total_predictions()\nsubmission_val.loc[submission_val.id.str.contains(\"Total\")]","56ddf669":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","3ec8e21e":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","459fe6cc":"class MyLSTM(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, batch_size, num_layers=1, output_dim=1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size=self.input_dim,\n                            hidden_size=self.hidden_dim,\n                            num_layers=self.num_layers,\n                            dropout = 0.25)\n        self.linear = nn.Linear(self.hidden_dim, output_dim)\n        \n    def init_hidden(self):\n        self.h_zero = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n        self.c_zero = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n    \n    def forward(self, x):\n        lstm_output, (h_n, c_n) = self.lstm(x.view(len(x), self.batch_size, -1),\n                                           (self.h_zero, self.c_zero))\n        last_time_step = lstm_output.view(self.batch_size, len(x), self.hidden_dim)[-1]\n        pred = self.linear(last_time_step)\n        return pred\n    \n\ndef train_model(model, data_dict, lr=1e-4, num_epochs=500):\n    \n    loss_fun = torch.nn.MSELoss(reduction=\"mean\")\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    train_losses = np.zeros(num_epochs)\n    phases = [\"train\", \"eval\"]\n    losses_dict = {\"train\": [], \"eval\": []}\n    predictions_dict = {\"train\": [], \"eval\": [] }\n    \n    for n in range(num_epochs):\n        \n        for phase in phases:\n            \n            x = data_dict[phase][\"input\"].to(device, dtype=torch.float)\n            y = data_dict[phase][\"target\"].to(device, dtype=torch.float)\n            \n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n        \n            optimiser.zero_grad()\n            \n            model.init_hidden()\n            y_pred = model(x)\n            \n            if n == (num_epochs-1):\n                predictions_dict[phase] = y_pred.float().cpu().detach().numpy()\n            \n            loss = loss_fun(y_pred.float(), y)\n            losses_dict[phase].append(loss.item())\n            \n            if n % 50 == 0:\n                print(\"{} loss: {}\".format(phase, loss.item()))\n            \n            if phase == 'train':\n                loss.backward()\n                optimiser.step()\n        \n    return losses_dict, predictions_dict\n\ndef create_sequences(timeseries, seq_len):\n    inputs = []\n    targets = []\n    \n    max_steps = len(timeseries) - (seq_len+1)\n    \n    for t in range(max_steps):\n        x = timeseries[t:(t+seq_len)]\n        y = timeseries[t+seq_len]\n        inputs.append(x)\n        targets.append(y)\n    \n    return np.array(inputs), np.array(targets)","06eb7bae":"diff_series = np.diff(timeseries)\ntrain_size = np.int(0.7 * len(diff_series))\ntrain_diff_series = diff_series[0:train_size]\neval_diff_series = diff_series[train_size::]\nscaler = MinMaxScaler(feature_range=(-1,1))\nscaled_train = scaler.fit_transform(train_diff_series.reshape(-1, 1))\nscaled_eval = scaler.transform(eval_diff_series.reshape(-1,1))","502d69a5":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(scaled_train, '-o', c=\"b\")\nax[1].plot(scaled_eval, '-o', c=\"g\")\nax[0].set_title(\"Single preprocessed top timeseries in train\")\nax[1].set_title(\"Single preprocessed top timeseries in eval\");\nax[0].set_xlabel(\"Days in dataset\")\nax[1].set_xlabel(\"Days in dataset\")\nax[0].set_ylabel(\"$\\Delta y$ scaled\")\nax[1].set_ylabel(\"$\\Delta y$ scaled\");","a508337a":"seq_len = 400\ninput_dim = 1\nhidden_dim = 128\nnum_epochs = 600\nlr=0.0005\n\n\nx_train, y_train = create_sequences(scaled_train, seq_len)\nx_eval, y_eval = create_sequences(scaled_eval, seq_len)\n\nx_train = torch.from_numpy(x_train).float()\ny_train = torch.from_numpy(y_train).float()\nx_eval = torch.from_numpy(x_eval).float()\ny_eval = torch.from_numpy(y_eval).float()\n\ndata_dict = {\"train\": {\"input\": x_train, \"target\": y_train},\n             \"eval\": {\"input\": x_eval, \"target\": y_eval}}","019a14dd":"model = MyLSTM(input_dim=input_dim,\n               hidden_dim=hidden_dim,\n               batch_size=seq_len)\nmodel = model.to(device)","a2ff0319":"run_training = True\nif run_training:\n    losses_dict, predictions_dict = train_model(model, data_dict, num_epochs=num_epochs, lr=lr)","82ab8ede":"if run_training:\n    \n    fig, ax = plt.subplots(3,1,figsize=(20,20))\n    ax[0].plot(losses_dict[\"train\"], '.-', label=\"train\", c=\"red\")\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"MSE\")\n    ax[0].plot(losses_dict[\"eval\"], '.-', label=\"eval\", c=\"blue\");\n    ax[0].legend();\n\n    ax[1].plot(predictions_dict[\"train\"], '-o', c=\"red\")\n    ax[1].plot(y_train, '-o', c=\"green\")\n    ax[1].set_title(\"Fitted and true values of y in train\");\n    ax[1].set_ylabel(\"Unit sales y\");\n    ax[1].set_xlabel(\"Number of days in train\");\n\n    ax[2].plot(predictions_dict[\"eval\"], '-o', c=\"red\")\n    ax[2].plot(y_eval, '-o', c=\"green\")\n    ax[2].set_title(\"Predicted and true values of y in eval\");\n    ax[2].set_xlabel(\"Number of days in eval\");\n    ax[2].set_ylabel(\"Unit sales y\");","cee5105f":"from statsmodels.graphics.tsaplots import plot_acf\n\nif run_training:\n    \n    train_residuals = y_train-predictions_dict[\"train\"]\n    eval_residuals = y_eval-predictions_dict[\"eval\"]\n    \n    fig, ax = plt.subplots(2,2,figsize=(20,10))\n    sns.distplot(train_residuals, ax=ax[0,0], color=\"red\")\n    sns.distplot(eval_residuals, ax=ax[0,1], color=\"green\")\n    ax[0,0].set_title(\"Train residuals\")\n    ax[0,1].set_title(\"Eval residuals\")\n    ax[0,0].set_xlabel(\"$y_{true} - y_{pred}$\")\n    ax[0,1].set_xlabel(\"$y_{true} - y_{pred}$\")\n    ax[0,0].set_ylabel(\"density\")\n    ax[0,1].set_ylabel(\"density\")\n    \n    plot_acf(train_residuals, ax=ax[1,0])\n    plot_acf(eval_residuals, ax=ax[1,1])","4e7cc7dc":"sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_train), replace=True)\nsampled_residuals = sampled_residuals.reshape(-1,1)\nnew_response = predictions_dict[\"train\"] + sampled_residuals","9a3464b7":"fig, ax = plt.subplots(2,2,figsize=(20,10))\nax[0,0].plot(predictions_dict[\"train\"][0:200], 'o-', color=\"purple\")\nax[0,0].set_title(\"Original fitted values $y_{pred}$ in \")\nax[0,0].set_xlabel(\"200 example days\")\nax[0,0].set_ylim(-0.4, 0.4)\nax[0,0].set_ylabel(\"$y_{fitted}$\")\n\nax[0,1].plot(new_response[0:200,0], 'o-', color=\"orange\")\nax[0,1].set_title(\"Response values $y^{*}$ using sampled residuals\");\nax[0,1].set_xlabel(\"200 example days\")\nax[0,1].set_ylabel(\"$y^{*}$\");\nax[0,1].set_ylim(-0.4, 0.4)\n\nax[1,0].plot(sampled_residuals[0:200], 'o-', color=\"cornflowerblue\")\nax[1,0].set_title(\"Sampled residuals\")\nax[1,0].set_xlabel(\"200 example days\")\nax[1,0].set_ylabel(\"$\\epsilon$\")\n\nax[1,1].plot(y_train[0:200], 'o-', color=\"firebrick\")\nax[1,1].set_title(\"True values $y_{train}$\")\nax[1,1].set_xlabel(\"200 example days\")\nax[1,1].set_ylabel(\"$y_{train}$\");\n","19c3e5ec":"responses = []\nfor n in range(100):\n    # sample residuals using the historical residuals found in train\n    sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_eval), replace=True)\n    sampled_residuals = sampled_residuals.reshape(-1,1)\n    # create a synthetic future timeseries of eval by adding sampled residuals\n    new_response = predictions_dict[\"eval\"] + sampled_residuals\n    # reverse the scaling\n    new_response = scaler.inverse_transform(new_response)\n    # concat the first value of the evaluation series and the response series\n    new_response = np.hstack((timeseries[train_size], new_response[:,0]))\n    # reverse the differnciation (trend removal) using cumsum\n    new_response = np.cumsum(new_response)\n    # save the future timeseries\n    responses.append(new_response)\n    \nresponses = np.array(responses)\nresponses.shape","6c6ad1ee":"y_eval.shape","d240262e":"median_series = np.median(responses, axis=0)\neval_series = scaler.inverse_transform(y_eval)\neval_series = np.cumsum(np.hstack((timeseries[train_size-1], eval_series[:,0])))\nlow_q = 0.25\nup_q = 0.75","62d91ec2":"plt.figure(figsize=(20,5))\nplt.plot(np.arange(0, len(median_series)), median_series, 'o-', label=\"median predicted series\")\nplt.plot(eval_series, '.-', color=\"cornflowerblue\", label=\"true eval series\")\nlower = np.quantile(responses, low_q, axis=0)\nupper = np.quantile(responses, up_q, axis=0)\nplt.fill_between(np.arange(0, len(median_series)), lower, upper, alpha=0.5)\nplt.title(\"Prediction interval {}% of eval timeseries\".format((up_q-low_q)*100));\nplt.xlabel(\"Days in eval\")\nplt.ylabel(\"Unit sales\");\nplt.legend();","a93dbc37":"## Loading data <a class=\"anchor\" id=\"data\"><\/a>","2cf78de6":"### Insights\n\n* As the last known value was 0 and this value is also the only possible minimum value, we do not observe a normal distribution of residuals!\n* Furthermore the mean of our residuals is not zero. In this case they are called biased and it's also an indicator that our model does not suite well.\n* I have decided to compute this example for the bottom level 12. In contrast to top-level time series the corresponding series do not show nice and clear periodic patterns that would likely yield more normally distributed residuals. \n* Consequently we may conclude that some methods and models could be better suited for top or low-level series in the hierarchy but not for both. This is something to keep in mind.","708eacee":"# The Weighted Scaled Pinball loss <a class=\"anchor\" id=\"loss\"><\/a>\n\n## The formula <a class=\"anchor\" id=\"formula\"><\/a>\n\nFor each time series and for each quantile the **Scaled Pinball loss** can be computed by:\n\n$$ SPL(u) = \\frac{1}{h} \\cdot \\frac{1}{\\frac{1}{n-1} \\cdot \\sum_{t=2}^{n} |Y_{t} - Y_{t-1}|} \\cdot \\sum_{t=n+1}^{n+h}\n\\begin{cases} \n    (Y_{t} - Q_{t}(u))\\cdot u & \\text{if } Y_{t} \\geq Q_{t}(u) \\\\\n    (Q_{t}(u) - Y_{t})\\cdot (1-u)       & \\text{if } Y_{t} < Q_{t}(u)\n\\end{cases} $$\n\nwhereas:\n\n* $Y_{t}$ is the actual true future value of the time series at point $t$\n* $u$ is the considered quantile\n* $Q_{t}$ is the generated forecast for quantile $u$\n* $h$ is the forecasting horizon (28 days)\n* $n$ is the length of the training sample (number of historical observations)\n\nAfter computing this loss for all 42840 time series and for all requested quantiles, the **Weighted Scaled Pinball** loss is computed as follows:\n\n$$ WSPL = \\sum_{i=1}^{42840} \\cdot w_{i} \\cdot \\frac{1}{9} \\sum_{j=1}^{9}SPL(u_{j})$$\n\nIn the M5 competiton we have 12 aggregation levels and as all hierarchical levels are equally weighted the weights should be $w_{i}=\\frac{1}{12}$. The total number of time series is higher than what is given in train as all levels up to the top aggregation are included.\n\n\n\n## Playing with the loss implementation <a class=\"anchor\" id=\"loss_implementation\"><\/a>\n\nLet's pick a single timeseries to get started with the loss and its implementation. As I just like to get started it's a bit quick and dirty and surely not the best way to write it down. ;-) \n","d5bcd379":"## How can we generate forecasts for grouped timeseries? <a class=\"anchor\" id=\"forecasts_ts\"><\/a>\n\n* Our training data consists of 30490 timeseries. They belong to the bottom-level 12: Unit sales of product x, aggregated for each store.\n* A simple method to generate forecasts for all levels is to focus only on the bottom level. All of its predictions are then summed up to create the forecasts of all levels up to the top. This is called the bottom-up approach. \n* As you can see [here](https:\/\/otexts.com\/fpp2\/bottom-up.html), there are many more approaches one could use, for example top-down or middle-out. ","80f69092":"## Fitting the model to the top-level series <a class=\"anchor\" id=\"fitting_lstm\"><\/a>","8b587ea3":"### Insights\n\n* Each quartile u has exactily $2*42840 = 85680$ requests. The total number of all 12 level timeseries is 42840.\n* We have the same number of validation and evaluation requests and this explains the factor 2.\n* It seems that really all 12 aggregation levels are represented in the submission id. This is not clear yet and can be shown with further EDA (work in progress).","c968312e":"As far as I currently know Prophet likes to have the dates that we can find in our calendar dataframe:","e34e220c":"### Calendar information","b64dd593":"### Insights\n\n* We can see that our median series is often close to the true eval timeseries.\n* But the predictions become more uncertain with increasing time horizont.\n* We were not able to cover heavy, periodic outliers so far. \n* Nonetheless I'm still impressed that the bootstrapped residuals seem to work not so bad as I expected it. :-)","9407d6b6":"### Sales Training Data","84cd27be":"## Prediction intervals and quartiles <a class=\"anchor\" id=\"PIs\"><\/a>\n\nReading in the competition guideline, we can find that we are asked to make predictions for the median and four prediction intervals (PI): 50%, 67%, 95% and 99%. They belong to the following quartiles:\n\n* 99% PI - $u_{1} = 0.005$ and $u_{9} = 0.995$\n* 95% PI - $u_{2} = 0.025$ and $u_{8} = 0.975$\n* 67% PI - $u_{3} = 0.165$ and $u_{7} = 0.835$\n* 50% PI - $u_{4} = 0.25$ and $u_{6} = 0.75$\n* median - $u_{5} = 0.5$","e5548074":"# \"What is meant by probabilistic forecasting?\"... <a class=\"anchor\" id=\"prob_forecasting\"><\/a>\n\nWhen I read about this competition this was one of the first questions that came into my mind. I know a few probabilistic models and methods and I have done some time series analysis before but I haven't directly got in touch with probabilistic timeseries analysis so far. The M5 competition is a good way to close this gap and to learn something new. To start I like to follow a question driven approach...\n\n## What is a grouped time series? <a class=\"anchor\" id=\"grouped_ts\"><\/a>\n\n* Reading the competiton guideline we can find out that we have to deal with grouped time series of unit sales data. \n* They show a hierarchy of different aggregation levels that are weighted equally in the loss functions. \n* When working with grouped time series it's common to compute forecasts only for disaggregated time series and to add them up the same way the aggregation is performed for all remaining time series. \n* In Chapter 10 of [Forecasting - Principles and Practice](https:\/\/otexts.com\/fpp2\/hierarchical.html) we can find even more information about how to do this \"forecasting aggregation\".","18d04bf1":"As we are asked to predict a time window of 28 days, the easiest way to go now is to use the last 28 days for validation: ","4b114c04":"## Setting up LSTM <a class=\"anchor\" id=\"lstm_setup\"><\/a>","7681bd0c":"### Sell prices information","aa59807f":"# Facebook's Prophet <a class=\"anchor\" id=\"prophet\"><\/a>\n\n\n## Model description\n\n[Prophet](https:\/\/facebook.github.io\/prophet\/) is a decomposable time series model with 3 main model components and one error term:\n\n$$y(t) = g(t) + s(t) + h(t) + \\epsilon_{t}$$\n\n* trend g(t) - non-periodic changes of the value\n* seasonality s(t) - periodic changes (e.g. weekly and yearly) \n* holidays h(t) - effect of holidays (e.g. irregular patterns over one or more days)\n* $\\epsilon_{t}$ - error term that describes any idiosyncratic changes (assumed to be normally distributed)\n\n\n## Uncertainty estimates\n\nBy default it returns uncertainty intervals of the predicted value $y_{hat}$ consisting of three different sources:\n* uncertainty in the trend,\n* uncertainty in the seasonality estimates,\n* additional observation noise\n\nTo compute the uncertainty in the trend it is assumed that the average frequency and magnitude of trend changes will be the same in the future as observed in the history. This trend changes are projected forward into the future and by computing their distribution uncertainty intervals are obtained. **By default Prophet only returns uncertainty in the trend and observation noise!**","b94e89db":"### Insights\n\n* The residuals in train only show a significant autocorrelation with their previous, 1-lag timepoint. \n* That's great as we are close to uncorrelated residuals that were assumed when using bootstrapped residuals.","cc58b52e":"# Where to go next? <a class=\"anchor\" id=\"next\"><\/a>\n\nI'm going to continue with the following topics:\n\n* More about Prophet and personal adjustments - computing PIs\n* Conclusion of what I have learnt by writing this kernel","2f5b4419":"# Preparing to start <a class=\"anchor\" id=\"prepare\"><\/a>\n\n## Loading packages <a class=\"anchor\" id=\"packages\"><\/a>","3d595e94":"## Aggregation levels <a class=\"anchor\" id=\"sub_aggregation_levels\"><\/a>","a43732a9":"Ok, the rest now is simple. We have seen that we can compute PIs using our multipliers:\n\n$$y_{lower, h} = y - c \\cdot \\sigma_{h}$$\n\n$$y_{upper, h} = y + c \\cdot \\sigma_{h}$$\n\n* c = 2.58 for 99% PI\n* c = 1.96 for 95% PI\n* c ~ 0.95 for 67% PI\n* c = 0.67 for 50% PI","e8170946":"Let's take a look at the daily sales of this series:","6b21eadd":"## How does the hierarchy look like? <a class=\"anchor\" id=\"hierarchy_ts\"><\/a>\n\nIn the competition guideline we can find that the hierarchy consits of 12 levels. Let's try to reconstruct some of them:\n\n1. The top is given by the unit sales of all products, aggregated for all stores\/states. \n2. Unit sales of all products, aggregated for each state.\n3. Unit sales of all products, aggregated for each store.\n4. Unit sales of all products, aggregated for each category.\n5. Unit sales of all products, aggregated for each department.\n\n...\n\nOk, time for a vis: ;-)","dc72d9c9":"## The top timeseries - preprocessing <a class=\"anchor\" id=\"preprocessing\"><\/a>","17c6a139":"# The submission format <a class=\"anchor\" id=\"submission\"><\/a>\n\n## Intro <a class=\"anchor\" id=\"intro\"><\/a>\n\n* We have 28 F-columns as we are predicting daily sales for the next 28 days. \n* We are asked to make uncertainty estimates for these days.","d0f136a4":"Then we choose the last known value of the train timeseries as predictions for all asked 28 time points: ","00b5c718":"# M5 - Sales Uncertainty Prediction\n\n1. [Sources and guidelines](#sources)\n2. [Preparing to start](#prepare)\n    * [Loading packages](#packages)\n    * [Loading data](#data)\n3. [\"What is meant by probabilistic forecasting?\"...](#prob_forecasting)\n    * [What is a grouped time series?](#grouped_ts)\n    * [How does the hierarchy look like?](#hierarchy_ts)\n    * [How can we generate forecasts for grouped timeseries?](#forecasts_ts)\n4. [The submission format](#submission)\n    * [Intro](#intro)\n    * [Prediction intervals and quartiles](#PIs)\n    * [Aggregation levels](#sub_aggregation_levels)\n    * [Submission EDA](#submission_eda)\n5. [The Weighted Scaled Pinball loss](#loss)\n    * [The formula](#formula)\n    * [Playing with the loss implementation](#loss_implementation)\n6. [The Naive method](#naive)\n    * [Prediction intervals for the Naive method](#prediction_intervals_naive)\n    * [Computing the loss for one timeseries of level 12](#loss_example)\n7. [Facebook's Prophet](#prophet)\n8. [LSTM and bootstrapped residuals](#lstm_bootstrapped_res)\n    * [Basic idea](#basic_idea)\n    * [Setting up LSTM](#lstm_setup)\n    * [Fitting the model to the top-level series](#fitting_lstm)\n    * [Check residuals for autocorrelation ](#residuals_checkup)\n    * [Computing PIs using bootstrapped residuals](#bootstrapped_PIs)\n9. [Where to go next?](#next)","e3074ef5":"Now, we need to split our single row data into a training and validation (dev) part. I decided to use the same period of time (28 days) for validation:","b5695804":"Browsing through the submission ids, we can see that we are given values of $u_{i}$ and information about the aggregation type like:\n\n* the state id\n* the department id\n* the item id\n* the store id\n\nIt's a bit confusing that missing states are not represented by X. This makes splitting the id for EDA a bit more complicated. :-( Furthermore there is no clear separator. The $ \\_ $ sign is also present in the department id. **It seems that one asked aggregation always consists of 3 ids. In cases of counts smaller than 3, we can observe X as placeholder.**  ","bb4a127b":"Let's use the timeseries of total unit sales as an example again. For preprocessing we should remove the trend and scale the values.","c62bc503":"Ok, we can see that this series represents all daily sales of item 445 in the CA_1 store. This item is part of the household_1 department and is also present as a row in the other shops:","b9c279b4":"I will only use one as an example: c=2.58 for 99% PI:","3ac699f5":"# Sources and guidelines <a class=\"anchor\" id=\"sources\"><\/a>\n\n* [M5 Competition guideline](https:\/\/mofc.unic.ac.cy\/m5-competition\/)\n* [M5 github repository](https:\/\/github.com\/Mcompetitions\/M5-methods\/tree\/master\/validation)\n* [Forecasting - Principles and Practice (by Rob J Hyndman and George Athanasopoulos)](https:\/\/otexts.com\/fpp2\/)","1e841205":"## Submission EDA <a class=\"anchor\" id=\"submission_eda\"><\/a> ","e08ebf81":"I'm not sure if this whole stuff makes sense. Personally I feel a strong need for Bayesian ML oand [credible intervals](https:\/\/en.wikipedia.org\/wiki\/Credible_interval). What I miss most is a much more detailed mathematical description of Prophet in the documentation. Only using the model without a deeper understanding of what is going on feels very sloppy and dangerous. :-(","218cba06":"* In the first submission row we are asked to make precitions for the top level 1 (unit sales of all products, aggregated for all stores\/states)\n* The next 3 rows represent level 2.\n* Followed by level 3. \n* This may goes on and on until the bottom level 12 is reached? Probably not as there seem to be only 3 combinations of ids.\n* Some rows contain aggregations at different levels. An X indicates the absence of an second aggregration level.\n* The prediction interval can be validation (related to the public leaderboard) or evaluation (related to the private leaderboard).","a96f1a7f":"## Submission for validation\n\nRemember that we are asked to predict the following intervals PI:\n\n* 99% PI - $u_{1} = 0.005$ and $u_{9} = 0.995$\n* 95% PI - $u_{2} = 0.025$ and $u_{8} = 0.975$\n* 67% PI - $u_{3} = 0.165$ and $u_{7} = 0.835$\n* 50% PI - $u_{4} = 0.25$ and $u_{6} = 0.75$\n* median - $u_{5} = 0.5$\n\nNow let's fit the whole training data and predict for the validation timeperiod of the submission file. We have to set the interval in advance and personally it feels a bit overcomplicated to do so for each requested interval. But as I still need to understand Prophet in its details I'm going the following way:  ","ae57e6f7":"In this case all predictions are zero. Let's compute the loss for the median:","0c74eb65":"## Computing PIs using bootstrapped residuals <a class=\"anchor\" id=\"bootstrapped_PIs\"><\/a>\n\nThe idea of computing PIs using bootstrapped residuals is as follows:\n\n1. Fit the model to your data to obtain the fitted values $\\hat{y}_{i}$ and the forcasting errors $\\epsilon_{i} = y_{true, i} - \\hat{y}_{i}$.\n2. Randomly sample a residual $\\epsilon_{i}$ of the distribution of all $\\epsilon_{j}$ to generate a new response variable $y^{*}$ using the fitted value: $y^{*} = \\hat{y}_{i} + \\epsilon_{i}$. \n3. Doing this repeatively we obtain many different, synthetic values for future predictions that we can use to compute prediction intervals.\n\nLet's take a look at a single example first:","61a68ec7":"## Check residuals for autocorrelation <a class=\"anchor\" id=\"residuals_checkup\"><\/a>","1d2c52be":"## Example - Total unit sales prediction\n\nLet's sum up all unit sales given in our training data to obtain the top level time series of total unit sales of all stores and states:","a39eda00":"## Residual analysis \n\nOk, let's start with the computation of residuals for our example time series. As we assumed that the last known value is valid for all future values, I would choose this one also as fitted value for all training data points in the past:","c25e6990":"# LSTM and bootstrapped residuals <a class=\"anchor\" id=\"lstm_bootstrapped_res\"><\/a>\n\n## Basic idea <a class=\"anchor\" id=\"basic_idea\"><\/a>\n\n* Here we are using the Frequentist perspective of probability:\n    * The model parameters $w$ are assumed to be fixed and we estimate it using our estimator. \n    * The estimation depends on the dataset D we observe and consequently we can obtain error bars for our estimated parameters $w_{est}$ by considering multiple datasets.\n    * One way to do this is by creating new datasets, for example with bootstrapping.\n* We are creating new datasets by using bootstrapped residuals:\n    * We only assume uncorrelated residuals that need not be normally distributed.","7038d49e":"### Insights\n\n* It has become much clearer how these levels are aggregated by performing groupby- and summing up the sales.\n* We can already observe nice periodic patterns. ","eb9f13d5":"We need to compute PIs for evaluation and validation data. In my case, I have splitted the original training timeseries into my own eval and train part. Consequently instead of computing response series for the training data, we need to do this for the evaluation data. While doing so we also need to reverse the preprocessing:","7b6b2b8b":"## Computing the loss for one timeseries of level 12 <a class=\"anchor\" id=\"loss_example\"><\/a>","c4d92f56":"### Computing prediction intervals\n\nNow we need to choose a benchmark to compute how the predictions become more unsecure when time moves on. As I'm on the \"naive\" way, I like to use $\\sigma_{h} = \\sigma \\cdot \\sqrt{h}$. Let's do it for this single time series example:","65a312b0":"# The Naive method <a class=\"anchor\" id=\"naive\"><\/a>\n\nTo really compute the loss for our example we need to make quantile forecasts $Q_{t}(u)$ for a given quantile $u$. So far we haven't setup a model, but we can start easily using the naive method described in the competition guideline:\n\n$Y_{n+i} = Y_{n}$\n\nfor $i=1,2,...h$.\n\nIt's used for predicting series of the lowest level of the hierarchy. You can see that this approach just assumes the last known daily sale value for all requested predictions of the series. \n\n\n## Prediction intervals for the Naive method <a class=\"anchor\" id=\"prediction_intervals_naive\"><\/a>\n\n\n### Assuming normally distributed forcasting errors\n\nBut how can we compute prediction intervals for this method? If we always assume the last known value for the next points of the time period with horizont h, there would be no distribution per time point that could tell us something about uncertainty. I really start to like [Forecasting - Principles and Practice](https:\/\/otexts.com\/fpp2\/). Take a look at the chapter [\"Prediction intervals\"](https:\/\/otexts.com\/fpp2\/prediction-intervals.html#prediction-intervals). Here we can read that one way to go is to assume normally distributed forcasting errors:\n\n$$\\epsilon(y) \\sim N(\\sigma_{h})$$\n\nwhereas $\\sigma_{h}$ stands for the estimated standard deviation of future time step h. A prediction interval is then computed as a multiple of this standard deviation:\n\n$$y_{lower, h} = y - c \\cdot \\sigma_{h}$$\n\n$$y_{upper, h} = y + c \\cdot \\sigma_{h}$$\n\nThe factor $c$ is called **multiplier** and often such values are used for the requested prediction intervals:\n\n* c = 2.58 for 99% PI\n* c = 1.96 for 95% PI\n* c ~ 0.95 for 67% PI\n* c = 0.67 for 50% PI\n\nIf you like to read more about prediction intervals, I found this [Wikipedia article](https:\/\/en.wikipedia.org\/wiki\/Prediction_interval) useful as well.\n\n### Prediction intervals for multi-step time horizonts\n\nIn our case we are asked to compute uncertainty estimates for a time period of 28 days. It's intuitive that our predictions become more uncertain the greater the time step $h$ of our time horizont. Therefore we can assume that $\\sigma_{h}$ increases with h. \n\n\nIn contrast to one-step predictions we can't just use the standard deviation $\\sigma$ of residuals $\\epsilon_{t} = y_{true, t} - y_{fitted, t}$ of all time points $t$ between our observations and fitted values of the training data. Instead we could use some common benchmark methods:\n\n* **Mean forcasts: $\\sigma_{h} = \\sigma \\cdot \\sqrt{1 + \\frac{1}{T}}$ **\n* **Naive forcasts:** $\\sigma_{h} = \\sigma \\cdot \\sqrt{h}$\n* **Seasonal naive forcasts:** $\\sigma_{h} = \\sigma \\cdot \\sqrt{k+1}$ with k as the integer part of $\\frac{(h-1)}{m}$ and $m$ as the seasonal period\n* **Drift forcasts:** $\\sigma_{h} = \\sigma \\cdot \\sqrt{h \\cdot \\left(\\frac{1+h}{T}\\right) }$\n\nIn these cases $T$ stands for the total time span in the training data and $h$ for our prediction time horizont (in our case 28 days). All methods assume that we have given uncorrelated residuals. Consequently we need to perform a residual analysis to check if this is true! :-) \n\n\nBut before doing so, we need to compute an example of the Naive method! "}}