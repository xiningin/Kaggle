{"cell_type":{"740c3c01":"code","0a7220ee":"code","2dc86e58":"code","8ca2fa96":"code","863c4c54":"code","84ee8049":"code","0f32afb8":"code","637b308a":"code","aea1e204":"code","046b9516":"code","8d373955":"code","cae044a2":"code","89001772":"code","66279b95":"code","3d52bf7f":"code","dcf9c0c3":"code","840d4349":"code","31a70e94":"code","ee9656ec":"code","26e916bb":"code","b0e4af4f":"code","ae2d9baf":"code","268a157e":"code","41713450":"code","f18171fb":"code","b1ecfd7f":"code","edb8ea8c":"code","bea8ca3f":"code","251887c4":"code","c87963d1":"code","a80ba96b":"code","0489be77":"code","99431c31":"code","ba218ad8":"code","93ad5045":"code","155ad6b4":"code","816ce378":"markdown","354203e5":"markdown","12984695":"markdown","1ad94a07":"markdown","dc5ece4c":"markdown","0ef57322":"markdown","4839edbe":"markdown","c71d5756":"markdown","ad59ee36":"markdown","e0769aa8":"markdown","c03a0967":"markdown","f73fdce7":"markdown","360b9fcc":"markdown","3c8e0b90":"markdown"},"source":{"740c3c01":"import pandas as pd","0a7220ee":"import seaborn as sn\nimport matplotlib.pyplot as plt","2dc86e58":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV","8ca2fa96":"from sklearn.metrics import accuracy_score, log_loss, roc_auc_score","863c4c54":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost","84ee8049":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f32afb8":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","637b308a":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","aea1e204":"train.isna().sum()","046b9516":"train.drop('Cabin', axis = 1, inplace = True) #seems to be useless and lots of nans\ntest.drop('Cabin', axis = 1, inplace = True)","8d373955":"train.drop(['Ticket', 'Name', 'Embarked'], axis = 1, inplace = True) #seems to be useless\ntest.drop(['Ticket', 'Name', 'Embarked'], axis = 1, inplace = True) ","cae044a2":"corr_matrix = train.corr()\nsn.heatmap(corr_matrix, annot=True)\nplt.show()","89001772":"train.drop(['Fare'], axis = 1, inplace = True)\ntest.drop(['Fare'], axis = 1, inplace = True)","66279b95":"train['Age'].hist()","3d52bf7f":"train['Age'].fillna(train['Age'].median(), inplace = True) \ntrain['Age'].hist()","dcf9c0c3":"test['Age'].fillna(train['Age'].median(), inplace = True) ","840d4349":"train.head()","31a70e94":"train['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntest['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntrain.head()","ee9656ec":"y = train['Survived']\nX = train.drop(['Survived', 'PassengerId'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","26e916bb":"#for hyperparameters\n\ndef get_best_params(model, params): \n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    search = GridSearchCV(model, params, n_jobs=-1, cv=cv, scoring=\"accuracy\").fit(X_train, y_train)\n    best_params = search.best_estimator_.get_params()\n    return best_params, search.best_estimator_","b0e4af4f":"def get_score(model): \n    return accuracy_score(y_test, model.predict(X_test))","ae2d9baf":"models = {}","268a157e":"params = {\n    'n_neighbors' : list(range(1,5)),\n    'weights'     : ['uniform', 'distance'],\n    'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size'   : list(range(1,10)),\n    'p'           : [1,2]\n}\nknn = KNeighborsClassifier()\nbest_params = get_best_params(knn, params)[0]\nbest_knn = get_best_params(knn, params)[1]\nprint(best_params)","41713450":"models['knn'] = get_score(best_knn)\nprint(f'{round(get_score(best_knn)*100, 1)} %')","f18171fb":"params = {\n    'C' : [x \/ 2 for x in range(1,11)],\n    'kernel' : ['poly', 'rbf'],\n}\nsvc = SVC()\nbest_params = get_best_params(svc, params)[0]\nbest_svc = get_best_params(svc, params)[1]\nprint(best_params)","b1ecfd7f":"models['svc'] = get_score(best_svc)\nprint(f'{round(get_score(best_svc)*100, 1)} %')","edb8ea8c":"params = {'splitter': ['best', 'random'],\n           'max_depth': [7, 8, 9],\n           'min_samples_split': [2, 3, 4],\n           'min_samples_leaf': [1, 2, 3]}\n\ndt = DecisionTreeClassifier()\nbest_params = get_best_params(dt, params)[0]\nbest_dt = get_best_params(dt, params)[1]\nprint(best_params)","bea8ca3f":"models['dt'] = get_score(best_dt)\nprint(f'{round(get_score(best_dt)*100, 1)} %')","251887c4":"params = {'n_estimators': [100, 200, 300],\n           'max_depth': [7, 8, 9],\n           'min_samples_split': [2, 3, 4],\n           'min_samples_leaf': [1, 2, 3]}\n\nrfc = RandomForestClassifier()\nbest_params = get_best_params(rfc, params)[0]\nbest_rfc = get_best_params(rfc, params)[1]\nprint(best_params)","c87963d1":"models['rfc'] = get_score(best_rfc)\nprint(f'{round(get_score(best_rfc)*100, 1)} %')","a80ba96b":"params = {'learning_rate' : [0.3] ,\n          'max_depth': [3], \n          'min_child_weight' : [2, 3]}\n\n\nxgb = xgboost.XGBClassifier(use_label_encoder = False, eval_metric = 'logloss')\nbest_params = get_best_params(xgb, params)[0]\nbest_xgb = get_best_params(xgb, params)[1]\nprint(best_params)","0489be77":"models['xgb'] = get_score(best_xgb)\nprint(f'{round(get_score(best_xgb)*100, 1)} %')","99431c31":"df_models = pd.DataFrame.from_dict(data = models, orient = \"index\", columns = [\"acc\"])\ndf_models.sort_values(by='acc', ascending = False)","ba218ad8":"roc_auc_score(y_test, best_xgb.predict(X_test))","93ad5045":"roc_auc_score(y_test, best_rfc.predict(X_test))","155ad6b4":"features = ['Pclass','Sex','Age','SibSp','Parch']\npredictions = best_xgb.predict(test[features])\n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv(\"submission.csv\", index=False)\nprint(\"Submission saved!\")","816ce378":"We can pay attention to the relatively large correlation of the columns Pclass and Fare\n\nWith such data, there is a suspicion of duplication of features\n\nAlso, if we leave the Fare feature in the future, we will face some problems like data normalization and scaling\n\nSo the simplest desicion in this case will be to drop this column =)","354203e5":"### Features engineering","12984695":"#### Useless columns","1ad94a07":"Column \"Sex\" doesn't look good for our future classification  \nSo we can use simple binary coding for this","dc5ece4c":"The histogram does not look the best, but due to the small amount of data and the relevance of the picture to real data, we will leave it as it is","0ef57322":"#### NaN problems","4839edbe":"Here it is necessary to say a few words about the choice of the algorithm. In this dataset, we do not see a particular distribution or clear linear relationship in border of classes (there is only a relatively good correlation with the Pclass). Therefore, in my opinion, algorithms based on Bayes' theorem or Linear Regression will clearly show the result worse than other algorithms. So, let's try KNN (LVQ maybe after that), SVM, Random Forest, Decision Tree and Boosting.","c71d5756":"### Data view and cleaning ","ad59ee36":"### Imports","e0769aa8":"#### Fixing Age column","c03a0967":"And here the XGboost shows the best result","f73fdce7":"### Modeling","360b9fcc":"It will be useful to compare the two winners using the auc roc","3c8e0b90":"### Hi, Kaggle!\n\nHope that this notebook can be useful for someone"}}