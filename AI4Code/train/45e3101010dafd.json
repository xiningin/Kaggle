{"cell_type":{"dd1be378":"code","13e78ffb":"code","678a53eb":"code","7a8cf42a":"code","b0cb24e2":"code","2be32bc9":"code","d9a973cb":"code","a1a22431":"code","f91175a1":"code","bcc63f6c":"code","963462cd":"code","f7676f03":"code","c218bdd1":"code","5a53078f":"code","32976c9f":"code","b87acc59":"code","0916f1f1":"code","9139e779":"code","aa3d1c6e":"code","41c17c26":"code","bc62de72":"code","5e22f98b":"code","2ded7c6c":"code","e844991c":"code","e4a1f8d0":"code","7a411c18":"code","a0075766":"code","96d7ab45":"code","7ae5d12d":"code","0cbdb48a":"code","a7065994":"code","5ef870f5":"code","903e3dfe":"code","e3bcaf8d":"code","df861b1a":"code","0bc7551d":"code","7444089d":"code","cb826f26":"code","a7129f25":"code","a2ceb00a":"code","c7ae806c":"code","afeda13a":"code","f5e6d819":"code","9aa57888":"code","87b9627f":"code","4877d5d7":"code","1fd55d11":"code","0c651a6b":"code","a947b844":"code","4588a33f":"code","bc447b28":"code","61abe2fd":"code","e91d00be":"code","9aca23a5":"code","15f8015c":"code","4f1c8a4d":"code","20b6b1b2":"code","3f67cfa2":"code","032add16":"code","733bd6f9":"code","dc9a84cc":"code","3bc585f3":"code","840bdb2d":"code","5d0798a5":"code","ecdc47e1":"code","3ede9bf4":"code","b0d2fcca":"code","3fb0175f":"code","089f0dd0":"code","5bfc18ae":"code","0ad7335d":"code","06b96586":"code","94f889ba":"code","010a7ad8":"code","4fb0295f":"code","0af11c66":"code","9a314a2c":"code","0c140b83":"code","120eebd6":"markdown","501eaf5a":"markdown","70e2eba1":"markdown","7d5a5029":"markdown","7f11660d":"markdown","d6ce3c69":"markdown","0653454e":"markdown","33ea249c":"markdown","63e08fdd":"markdown","55bd1713":"markdown","e5e06a8b":"markdown","b7e6b08e":"markdown","e2131c96":"markdown","c7d0535e":"markdown","04d00e03":"markdown"},"source":{"dd1be378":"#Importing Required Library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#SMOTE to balance the Imbalance Data\nfrom imblearn.over_sampling import SMOTE\n\n#for Spliting Data and Hyperparameter Tuning \nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#Importing Machine Learning Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom catboost import CatBoostClassifier\n    \n#Bagging Algo\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.neural_network import MLPClassifier\n\n#To tranform data\nfrom sklearn import preprocessing\n\n#statistical Tools\nfrom sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\n#Setting Format\npd.options.display.float_format = '{:.5f}'.format\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","13e78ffb":"train = pd.read_csv(\"..\/input\/lt-vehicle-loan-default-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/lt-vehicle-loan-default-prediction\/test.csv\")","678a53eb":"test.shape,train.shape","7a8cf42a":"train.loan_default.value_counts().plot(kind='bar')","b0cb24e2":"#Lets looks at data description\ninfo = pd.read_csv(\"..\/input\/lt-vehicle-loan-default-prediction\/data_dictionary.csv\")\ninfo","2be32bc9":"train.describe().T","d9a973cb":"train.info()","a1a22431":"#Replacing all the Spaces with '_'\ntrain.columns = train.columns.str.replace('.','_')","f91175a1":"train.isna().sum()\n\n#So only Employment Type data is missing","bcc63f6c":"#Data Correlation\nplt.figure(figsize=(12,8))\nsns.heatmap(train.corr())","963462cd":"#Lets Look at few columns\n\ncolumns_unique = ['UniqueID','MobileNo_Avl_Flag',\n         'Current_pincode_ID','Employee_code_ID',\n         'NO_OF_INQUIRIES','State_ID',\n         'branch_id','manufacturer_id','supplier_id']\n\n\nunique_col = train[columns_unique]","f7676f03":"unique_col.head()","c218bdd1":"#Looking at all unique values\nfor i in unique_col.columns:\n    print(i,\" : distinct_value\")\n    print(unique_col[i].nunique(),\" : No. of unique Items\")\n    #print(unique_col[i].unique())\n    print(\"-\"*30)\n    print(\"\")","5a53078f":"unique_col.hist(bins=5, figsize=(16,12))\nplt.show()","32976c9f":"def columns_drop(data):\n    data.drop(unique_col,axis=1,inplace=True)","b87acc59":"columns_drop(train)","0916f1f1":"#Now we have 2 Columns named \"AVERAGE_ACCT_AGE\" & \"CREDIT_HISTORY_LENGTH\".\n#They have AplhNumeric Values Lets change them to Months\n\ndef change_col_month(col):\n    year = int(col.split()[0].replace('yrs',''))\n    month = int(col.split()[1].replace('mon',''))\n    return year*12+month\n\ndef months_transformation(data):\n    data['CREDIT_HISTORY_LENGTH'] = data['CREDIT_HISTORY_LENGTH'].apply(change_col_month)\n    data['AVERAGE_ACCT_AGE'] = data['AVERAGE_ACCT_AGE'].apply(change_col_month)","9139e779":"months_transformation(train)","aa3d1c6e":"train.head()","41c17c26":"#plot = data.iloc[:test.shape[0]]\nplot = train[train['AVERAGE_ACCT_AGE']<175]\nsns.lineplot(x=train['AVERAGE_ACCT_AGE'],y=train['loan_default'])","bc62de72":"#plot = data.iloc[:test.shape[0]]\nplot = train[train['CREDIT_HISTORY_LENGTH']<200]\nsns.lineplot(x=train['CREDIT_HISTORY_LENGTH'],y=train['loan_default'])","5e22f98b":"train.PERFORM_CNS_SCORE_DESCRIPTION.value_counts()","2ded7c6c":"def replace_not_scored(n):\n    #here we are spliting letters before '-'.\n    score=n.split(\"-\")\n    \n    if len(score)!=1:\n        return score[0]\n    else:\n        return 'N'\n\ndef transform_CNS_Description(data):\n    data['CNS_SCORE_DESCRIPTION']=data['PERFORM_CNS_SCORE_DESCRIPTION'].apply(replace_not_scored).astype(np.object)\n    \n    #Now Transform CNS Score Description data into Numbers\n\n    sub_risk = {'N':-1, 'K':0, 'J':1, 'I':2, 'H':3, 'G':4, 'E':5,'F':6, 'L':7, 'M':8, 'B':9, 'D':10, 'A':11, 'C':12}\n\n    data['CNS_SCORE_DESCRIPTION'] = data['CNS_SCORE_DESCRIPTION'].apply(lambda x: sub_risk[x])\n    \ntransform_CNS_Description(train)","e844991c":"train.head()","e4a1f8d0":"def transform_PERFORM_CNS_SCORE_DESCRIPTION(data):\n    #Replacing all the values into Common Group\n\n    data['PERFORM_CNS_SCORE_DESCRIPTION'].replace({'C-Very Low Risk':'Very Low Risk',\n                                                 'A-Very Low Risk':'Very Low Risk',\n                                                 'D-Very Low Risk':'Very Low Risk',\n                                                 'B-Very Low Risk':'Very Low Risk',\n                                                 'M-Very High Risk':'Very High Risk',\n                                                 'L-Very High Risk':'Very High Risk',\n                                                 'F-Low Risk':'Low Risk',\n                                                 'E-Low Risk':'Low Risk',\n                                                 'G-Low Risk':'Low Risk',\n                                                 'H-Medium Risk':'Medium Risk',\n                                                 'I-Medium Risk':'Medium Risk',\n                                                 'J-High Risk':'High Risk',\n                                                 'K-High Risk':'High Risk'},\n                                                  inplace=True)\n\n    #Transformin them into Numeric Features\n\n    risk_map = {'No Bureau History Available':-1, \n                  'Not Scored: No Activity seen on the customer (Inactive)':-1,\n                  'Not Scored: Sufficient History Not Available':-1,\n                  'Not Scored: No Updates available in last 36 months':-1,\n                  'Not Scored: Only a Guarantor':-1,\n                  'Not Scored: More than 50 active Accounts found':-1,\n                  'Not Scored: Not Enough Info available on the customer':-1,\n                  'Very Low Risk':4,\n                  'Low Risk':3,\n                  'Medium Risk':2, \n                  'High Risk':1,\n                  'Very High Risk':0}\n\n    data['PERFORM_CNS_SCORE_DESCRIPTION'] = data['PERFORM_CNS_SCORE_DESCRIPTION'].map(risk_map)\n\ntransform_PERFORM_CNS_SCORE_DESCRIPTION(train)","7a411c18":"train.head()","a0075766":"sns.countplot(x = train['PERFORM_CNS_SCORE_DESCRIPTION'])","96d7ab45":"train.Employment_Type.value_counts()","7ae5d12d":"defa = pd.crosstab(train['Employment_Type'], train['loan_default'])\nprint(defa)","0cbdb48a":"def fill_employment_type(data):\n    data['Employment_Type'] = data['Employment_Type'].fillna('Salaried')\n    employment_map = {'Self employed':0, 'Salaried':1, 'Not_employed':-1}\n\n    data['Employment_Type'] = data['Employment_Type'].apply(lambda x: employment_map[x])\nfill_employment_type(train)","a7065994":"import scipy.stats as stats\nchi_sq, p_value, deg_freedom, exp_freq = stats.chi2_contingency(defa)\nprint('Chi Square Statistics',chi_sq)\nprint('p-value',p_value)\nprint('Degree of freedom',deg_freedom)","5ef870f5":"sns.countplot(x = train['Employment_Type'])","903e3dfe":"pri_columns = ['PRI_NO_OF_ACCTS','SEC_NO_OF_ACCTS',\n           'PRI_ACTIVE_ACCTS','SEC_ACTIVE_ACCTS',\n           'PRI_OVERDUE_ACCTS','SEC_OVERDUE_ACCTS',\n           'PRI_CURRENT_BALANCE','SEC_CURRENT_BALANCE',\n           'PRI_SANCTIONED_AMOUNT','SEC_SANCTIONED_AMOUNT',\n           'PRI_DISBURSED_AMOUNT','SEC_DISBURSED_AMOUNT',\n           'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT']\n\npri_df = train[pri_columns]","e3bcaf8d":"def new_col(data):\n    #Creating and Sorting Columns\n\n    data['NO_OF_ACCTS'] = data['PRI_NO_OF_ACCTS'] + data['SEC_NO_OF_ACCTS']\n\n    data['ACTIVE_ACCTS'] = data['PRI_ACTIVE_ACCTS'] + data['SEC_ACTIVE_ACCTS']\n\n    data['OVERDUE_ACCTS'] = data['PRI_OVERDUE_ACCTS'] + data['SEC_OVERDUE_ACCTS']\n\n    data['CURRENT_BALANCE'] = data['PRI_CURRENT_BALANCE'] + data['SEC_CURRENT_BALANCE']\n\n    data['SANCTIONED_AMOUNT'] = data['PRI_SANCTIONED_AMOUNT'] + data['SEC_SANCTIONED_AMOUNT']\n\n    data['DISBURSED_AMOUNT'] = data['PRI_DISBURSED_AMOUNT'] + data['SEC_DISBURSED_AMOUNT']\n\n    data['INSTAL_AMT'] = data['PRIMARY_INSTAL_AMT'] + data['SEC_SANCTIONED_AMOUNT']\n    \n    data.drop(pri_columns, axis=1, inplace=True)\n\nnew_col(train)","df861b1a":"new_columns = ['NO_OF_ACCTS', 'ACTIVE_ACCTS', 'OVERDUE_ACCTS', 'CURRENT_BALANCE',\n       'SANCTIONED_AMOUNT', 'DISBURSED_AMOUNT', 'INSTAL_AMT']\n\nfor i in new_columns:\n    print(i,\" : distinct_value\")\n    print(train[i].nunique(),\" : No. of unique Items\")\n    #print(data[i].unique())\n    print(\"-\"*30)\n    print(\"\")","0bc7551d":"sns.scatterplot(data=train['ACTIVE_ACCTS'])","7444089d":"sns.scatterplot(data=train['NO_OF_ACCTS'])","cb826f26":"sns.scatterplot(data=train['OVERDUE_ACCTS'])","a7129f25":"sns.scatterplot(data = train['CURRENT_BALANCE'])","a2ceb00a":"def mode_impute_outlier(data):\n    li = list(data['ACTIVE_ACCTS'].sort_values()[-3:].index)\n    data['ACTIVE_ACCTS'][li] = int(data.drop(li)['ACTIVE_ACCTS'].mode())\n    li = list(data['NO_OF_ACCTS'].sort_values()[-4:].index)\n    data['NO_OF_ACCTS'][li] = int(data.drop(li)['NO_OF_ACCTS'].mode())\n    li = list(data['OVERDUE_ACCTS'].sort_values()[-10:].index)\n    data['OVERDUE_ACCTS'][li] = int(data.drop(li)['OVERDUE_ACCTS'].mode())\n    li = list(data['CURRENT_BALANCE'].sort_values()[-15:].index)\n    data['CURRENT_BALANCE'][li] = int(data.drop(li)['CURRENT_BALANCE'].mode())","c7ae806c":"mode_impute_outlier(train)","afeda13a":"train.head()","f5e6d819":"train.Date_of_Birth.min(), train.Date_of_Birth.max()","9aa57888":"df_age = train[['disbursed_amount', 'asset_cost', 'ltv', 'Date_of_Birth','DisbursalDate','loan_default']]\ndf_age.tail()","87b9627f":"def age(dob):\n    yr = int(dob[-2:])\n    if yr >=0 and yr < 20:\n        return yr + 2000\n    else:\n         return yr + 1900\n        \ndf_age['Date_of_Birth'] = df_age['Date_of_Birth'].apply(age)\ndf_age['DisbursalDate'] = df_age['DisbursalDate'].apply(age)\ndf_age['Age']=df_age['DisbursalDate']-df_age['Date_of_Birth']\ndf_age=df_age.drop(['DisbursalDate','Date_of_Birth'],axis=1)\n\ndf_age.head()","4877d5d7":"def calculate_age(data):\n    data['Date_of_Birth'] = data['Date_of_Birth'].apply(age)\n    data['DisbursalDate'] = data['DisbursalDate'].apply(age)\n    # Age of applicant when he\/she applied for Loan\n    data['Age'] = data['DisbursalDate'] - data['Date_of_Birth']\n    data = data.drop( ['DisbursalDate', 'Date_of_Birth'], axis=1)\n    \ncalculate_age(train)","1fd55d11":"train.describe().T","0c651a6b":"transformed = []\ntransformed_with_one = []\nnot_transformed = []\n\ndef column_to_transform(data):\n\n    num_col = ['disbursed_amount', 'asset_cost', 'ltv', 'PERFORM_CNS_SCORE',\n            'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',\n           'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'AVERAGE_ACCT_AGE',\n           'NO_OF_ACCTS', 'ACTIVE_ACCTS', 'OVERDUE_ACCTS', 'CURRENT_BALANCE',\n           'SANCTIONED_AMOUNT', 'DISBURSED_AMOUNT', 'INSTAL_AMT', 'Age']\n    \n    num_col_data = data[num_col]\n    \n    def transformation_boxcox(num_col_data):\n    \n        from scipy.stats import boxcox\n\n        for i in num_col:\n            if num_col_data[i].min() > 0:\n                num_col_data[i] = boxcox(num_col_data[i])[0]\n                transformed.append(i)\n            elif num_col_data[i].min() == 0:\n                num_col_data[i] = boxcox(num_col_data[i]+1)[0]\n                transformed_with_one.append(i)\n            else:\n                num_col_data[i] = num_col_data[i]\n                not_transformed.append(i)\n        print(\"Successful\")\n    \n    transformation_boxcox(data)\n\ncolumn_to_transform(train)","a947b844":"train.describe().T","4588a33f":"def data_processing(data):\n    test.columns = test.columns.str.replace('.','_')\n    columns_drop(data)\n    months_transformation(data)\n    transform_CNS_Description(data)\n    transform_PERFORM_CNS_SCORE_DESCRIPTION(data)\n    fill_employment_type(data)\n    new_col(data)\n    mode_impute_outlier(data)\n    calculate_age(data)\n    column_to_transform(data)\n    preprocessing.RobustScaler()\n    scaler.transform(data)\n    return data.shape","bc447b28":"X = train.drop(['loan_default'], axis=1)\ny = train['loan_default']","61abe2fd":"X.head()","e91d00be":"smote = SMOTE()\nX_tf,y_tf = smote.fit_resample(X,y)\nX_tf.shape, y_tf.shape","9aca23a5":"scaler = preprocessing.RobustScaler()\nX_tf = scaler.fit_transform(X_tf)\n\n# Split the data into training and testing sets \nx_train,x_test,y_train,y_test = train_test_split(X_tf,y_tf,test_size = .1, random_state = 3300)\n\nprint(x_train.shape[0], x_test.shape[0])","15f8015c":"accuracy = {}\nroc_r = {}\n\ndef train_model(model, model_name):\n    print(model_name)\n    \n    # Fitting model\n    model = model.fit(x_train, y_train)\n    pred = model.predict(x_test)\n    \n    #Model accuracy\n    acc = accuracy_score(y_test, pred)*100\n    accuracy[model_name] = acc\n    print('accuracy_score',acc)\n    print('precision_score',precision_score(y_test, pred)*100)\n    print('recall_score',recall_score(y_test, pred)*100)\n    print('f1_score',f1_score(y_test, pred)*100)\n    \n    \n    #ROC Score\n    roc_score = roc_auc_score(y_test, pred)*100\n    roc_r[model_name] = roc_score\n    print('roc_auc_score',roc_score)\n    \n    # Confusion matrix\n    print('confusion_matrix')\n    print(pd.DataFrame(confusion_matrix(y_test, pred)))\n    \n    #ROC Score\n    fpr, tpr, threshold = roc_curve(y_test, pred)\n    roc_auc = auc(fpr, tpr)*100\n    \n    #ROC Plot\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","4f1c8a4d":"knn = KNeighborsClassifier(weights='distance', algorithm='auto', n_neighbors=15, n_jobs=4)\n\ntrain_model(knn, 'K Nearest Neighbour')","20b6b1b2":"lr = LogisticRegression(C=5.0, solver='saga')\n\ntrain_model(lr, 'Logistic Regression')","3f67cfa2":"dtc = DecisionTreeClassifier(criterion='gini', splitter='random', max_depth=25, min_samples_split=4,\n                            min_samples_leaf=2)\n\ntrain_model(dtc, 'Decision Tree Classifier')","032add16":"bnb = BernoulliNB()\n\ntrain_model(bnb, 'Bernolli Naive Bayes')","733bd6f9":"rfc = RandomForestClassifier(n_estimators = 1500, n_jobs=-1, max_depth=15, \n                             min_samples_split=5, min_samples_leaf=3)\n\ntrain_model(rfc, 'Random Forest Classifier')","dc9a84cc":"lgbm = LGBMClassifier(n_estimators=720, n_jobs=-1, max_depth=15, min_child_weight=5, \n                      min_child_samples=5, num_leaves=10, learning_rate=0.15)\n\ntrain_model(lgbm, 'LGBMClassifier')","3bc585f3":"cat = CatBoostClassifier(verbose = 0)\n\ntrain_model(cat, \"Cat Boost\")","840bdb2d":"mlp = MLPClassifier(hidden_layer_sizes = (200,3), activation = 'relu', solver = 'adam', learning_rate = 'adaptive',\n                   max_iter = 1000)\n\ntrain_model(mlp, 'Multi-layer Perceptron Classifier')","5d0798a5":"xgb = XGBClassifier(n_estimators = 1500, nthread  = 4, max_depth = 15, min_child_weight = 5, learning_rate=0.1)\n\ntrain_model(xgb, 'XGBClassifier')","ecdc47e1":"xgbr = XGBRFClassifier(n_estimators = 2000, nthread  = 4, max_depth = 10, min_child_weight = 4, learning_rate=0.1)\n\ntrain_model(xgbr, 'XGBRFClassifier')","3ede9bf4":"gbc = GradientBoostingClassifier(n_estimators=1000, min_samples_split=5, max_depth=15)\n\ntrain_model(gbc, 'GradientBoostingClassifier')","b0d2fcca":"ada = AdaBoostClassifier(n_estimators=1000, learning_rate=0.1)\n\ntrain_model(ada, 'AdaBoostClassifier')","3fb0175f":"'''from sklearn.ensemble import StackingClassifier\n\nestimator = [('Lgbr', lgbm), ('xgb', xgb), ('gbc', gbc), ('mlp', mlp)]\n\nsc = StackingClassifier(estimators = estimator, final_estimator = lgbm, n_jobs=-1)\n\ntrain_model(sc, 'StackingClassifier')'''","089f0dd0":"# Predicted values\ny_head_lr = lr.predict(x_test)\ny_head_knn = knn.predict(x_test)\ny_head_xgb = xgb.predict(x_test)\ny_head_nb = bnb.predict(x_test)\ny_head_dtc = dtc.predict(x_test)\ny_head_rfc = rfc.predict(x_test)\ny_head_lgbm = lgbm.predict(x_test)\ny_head_ada = ada.predict(x_test)\ny_head_gbc = gbc.predict(x_test)\ny_head_mlp = mlp.predict(x_test)\ny_head_cat = cat.predict(x_test)","5bfc18ae":"cm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_xgb = confusion_matrix(y_test,y_head_xgb)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rfc = confusion_matrix(y_test,y_head_rfc)\ncm_lgbm = confusion_matrix(y_test,y_head_lgbm)\ncm_ada = confusion_matrix(y_test,y_head_ada)\ncm_gbc = confusion_matrix(y_test,y_head_gbc)\ncm_mlp = confusion_matrix(y_test,y_head_mlp)\ncm_cat = confusion_matrix(y_test,y_head_cat)","0ad7335d":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(4,3,5)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\nplt.subplot(4,3,6)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,2)\nplt.title(\"XGB Confusion Matrix\")\nsns.heatmap(cm_xgb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,3)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,1)\nplt.title(\"Random Forest Gini Confusion Matrix\")\nsns.heatmap(cm_rfc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,7)\nplt.title(\"LightGB Confusion Matrix\")\nsns.heatmap(cm_lgbm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,8)\nplt.title(\"Ada Boost Confusion Matrix\")\nsns.heatmap(cm_ada,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,9)\nplt.title(\"Gradient boost Classifier Confusion Matrix\")\nsns.heatmap(cm_gbc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,10)\nplt.title(\"Multi-layer Perceptron Classifier Confusion Matrix\")\nsns.heatmap(cm_mlp,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,11)\nplt.title(\"Cat boost Classifier Confusion Matrix\")\nsns.heatmap(cm_cat,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\n\nplt.show()","06b96586":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,5))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nplt.xticks(rotation = 90)\nsns.barplot(x=list(accuracy.keys()), y=list(accuracy.values()), palette=\"cubehelix\")\nplt.show()","94f889ba":"cat.fit(X_tf, y_tf)","010a7ad8":"submission = pd.DataFrame()\nsubmission['UniqueID'] = test['UniqueID']","4fb0295f":"test.head()","0af11c66":"data_processing(test)\ntest.head()","9a314a2c":"submission['loan_default'] = cat.predict(test)","0c140b83":"submission.head()","120eebd6":"Date_of_Birth = Date of birth of the customer\t\n\nDisbursal_Date = Date of disbursement\n\nDisbursement means the payment of money from a fund.","501eaf5a":"# Transforming Data","70e2eba1":"SMOTE is python library which is used when the data is imbalanced.","7d5a5029":"UniqueID = It is provided to every customer so its Unique and will always be different\n\nMobileNo_Avl_Flag = Whether person provided Mobile No. Doesn't tell us if loan will default\n\nCurrent_pincode_ID = It is Customers address we don't need that for Prediction\n\nEmployee_code_ID = Employee ID is not required as it doesn't related with Loan_defualt\n\nNO_OF_INQUIRIES = No. of Inquiries to loan doesn't help us to determine wheather loan will default or not\n\nState_ID = It is where loan is availed and doesn't add much to prediction to loan default\n\nbranch_id = Branch ID isn't relevent to Data Processing\n\nmanufacturer_id = Manufacturer ID doesn't add much too data\n\nsupplier_id = Supplier ID doesn't add much too data\n","7f11660d":"# Now we will predict on Test Data","d6ce3c69":"# Lets take a look at Date of Birth Column","0653454e":"# Now train it with whole Traning dataset","33ea249c":"# Traning Our Model","63e08fdd":"# Treating Missing Values","55bd1713":"# Balance Data using SMOTE","e5e06a8b":"# Transforming Primary and Secondary Accounts","b7e6b08e":"# Transform CNS Score And Create New Columns","e2131c96":"# Digging Few Columns for Insight","c7d0535e":"Now lets look at CNS Score Description\n\n\n","04d00e03":"# Visualization and Treating Outliers"}}