{"cell_type":{"48270ef1":"code","2c12d5c1":"code","a0cc7850":"code","12757338":"code","e8c4f9a0":"code","5638c96f":"code","78277aa8":"code","15a9fa24":"code","b75032f1":"code","36c3e0fc":"code","6511e2c0":"code","6f6bee5d":"code","a112a6d6":"code","9b2dcf38":"markdown","bd6283c6":"markdown","f29b1160":"markdown","162e33eb":"markdown","b0716033":"markdown","6a977fec":"markdown","58afe462":"markdown","9c834dd4":"markdown","9ad70fc9":"markdown"},"source":{"48270ef1":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\nfrom kaggle_datasets import KaggleDatasets","2c12d5c1":"tf.config.set_soft_device_placement(True)\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a0cc7850":"batch_size = strategy.num_replicas_in_sync * 128\nmodes = [\"training\", \"inference\"]\nmode = modes[1]\nif mode == modes[0]:\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path(\"ump-tf-record-time-series-split-10-fold\")","12757338":"if mode == modes[0]:\n    paths = tf.io.gfile.glob(GCS_DS_PATH + '*\/*.tfrecords')\n    train_paths = []\n    for path in paths:\n        for i in range(9):\n            if f\"fold_{i}.tfrecords\" in path:\n                train_paths.append(path)\n    valid_paths = []\n    for path in paths:\n        if f\"fold_9.tfrecords\" in path:\n            valid_paths.append(path)","e8c4f9a0":"def decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )\n\ndef preprocess(item):\n    return tf.reshape(item[\"features\"], [batch_size, 300]), tf.reshape(item[\"target\"], [batch_size])\n\ndef make_dataset(file_paths, batch_size=1024, mode=\"train\"):\n    ds = tf.data.TFRecordDataset(file_paths)\n    ds = ds.map(decode_function)\n    options = tf.data.Options()\n    if mode == \"train\":\n        ds = ds.shuffle(batch_size)\n        options.experimental_deterministic = False\n    ds = ds.with_options(options) \n    ds = ds.batch(batch_size, drop_remainder=True)\n    ds = ds.map(preprocess)\n    ds = ds.cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","5638c96f":"%%time\nif mode == modes[0]:\n    train_ds = make_dataset(train_paths, batch_size=batch_size)\n    valid_ds = make_dataset(valid_paths, mode=\"valid\", batch_size=batch_size)","78277aa8":"def correlation(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum \/ n\n    ymean = ysum \/ n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov \/ tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef correlationLoss(x,y, axis=-2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum \/ n\n    ymean = ysum \/ n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov \/ tf.sqrt(xsqsum * ysqsum)\n    sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis=axis) \/ n \/ tf.sqrt(ysqsum \/ n)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (0.01 * sqdif)) , dtype=tf.float32 )\n","15a9fa24":"def get_model(strategy):\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            tf.keras.Input((300,), dtype=tf.float32, name=\"features\"),\n            layers.Dense(512, activation='swish', kernel_regularizer=\"l2\"),\n            layers.Dense(256, activation='swish'),\n            layers.Dense(256, activation='swish', kernel_regularizer=\"l2\"),\n            layers.Dense(128, activation='swish'),\n            layers.Dense(32, activation='swish', kernel_regularizer=\"l2\"),\n            layers.Dense(1, name=\"target\")\n        ])\n        rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n        model.compile(optimizer=tf.optimizers.Adam(3e-4), loss=correlationLoss, metrics=[\"mae\", \"mape\", rmse, correlation])\n        return model","b75032f1":"keras.backend.clear_session()\nmodel = get_model(strategy)\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","36c3e0fc":"%%time\nmodels = []\ncheckpoint = keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\nearly_stop = keras.callbacks.EarlyStopping(patience=10)\nwith strategy.scope():\n    if mode == modes[0]:\n        history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n        model.load_weights(\"model.h5\")\n        for metric in [\"loss\", \"mae\", \"rmse\", \"correlation\"]:\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n            plt.title(metric.upper())\n            plt.show()\n    else:\n        model.load_weights(\"..\/input\/ubiquant-market-prediction-on-tpu-output\/model.h5\")\nmodels.append(model)","6511e2c0":"if mode == modes[0]:\n    y_vals = []\n    for _, y in valid_ds:\n        y_vals += list(y.numpy().reshape(-1))\n    y_val = np.array(y_vals)\n    y_pred = model.predict(valid_ds)\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val[:y_pred.shape[0]])[0]\n    print(f\"Pearson Correlation: {pearson_score}\")","6f6bee5d":"def inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds).reshape(-1)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","a112a6d6":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfeatures = [f\"f_{i}\" for i in range(300)]\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df['target'] = inference(models, test_df[features])\n    env.predict(sample_prediction_df) ","9b2dcf38":"## Submission","bd6283c6":"## Import dataset","f29b1160":"## Model Evaluation","162e33eb":"Let's see what the Model looks like.","b0716033":"## Modeling","6a977fec":"## Distribution Strategy","58afe462":"## Make Tensorflow dataset\nNow I am using first 9 folds as training set and last fold as validation set. You can experiment different data spliting strategy.","9c834dd4":"#  Ubiquant Market Prediction on TPU\nIn this notebook I will create a Ubiquant Market Prediction Model. I will train the Model on TPU using TF-Record dataset. The notebook contains Two Modes: training modes and inference model. During Training Mode, you need to switch this Accelerator to TPU and enable Internet Access. During inference Mode, it's not allowed to submit the notebook using TPU, so you need to switch the Acceleartor to GPU or CPU and close the Internet Access. To see how the TF-Record dataset is created, you can have look at my notebook [Create TF-Record for UMP dataset](https:\/\/www.kaggle.com\/lonnieqin\/create-tf-record-for-ump-dataset).\n## Import Packages","9ad70fc9":"## Configurations"}}