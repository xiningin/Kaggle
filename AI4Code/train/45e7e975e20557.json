{"cell_type":{"c2b3383d":"code","58928472":"code","73592d10":"code","35c42d43":"code","1b87b577":"code","8928a30f":"code","d3e09d3c":"code","904c34d6":"code","37a03b2b":"code","6dd868a5":"code","14a2f03d":"code","8be93cf7":"code","668fd911":"code","79297b60":"code","c314c1cd":"code","3e680629":"code","1abff64f":"code","cc85ab6f":"code","722a721e":"code","9ea66f9a":"code","3064343f":"code","063c89a5":"code","3139e7ea":"code","0b5d8f04":"code","7dd25daf":"code","33682f67":"code","b0e653ab":"code","543ade43":"code","510191dd":"code","ce45102f":"code","e8e74ad7":"code","a05d2924":"code","4c5d66f7":"code","fc6960f4":"code","9ccd6b1c":"code","86fe4f22":"code","78899327":"code","a037c23d":"code","bc43f687":"code","4c4930ed":"code","7ef87ab6":"code","f959d2ab":"code","c252b97a":"code","8d29ac04":"code","ec1a4b91":"code","59fa3320":"code","2f9c4352":"code","888c38c0":"code","00a06a1b":"code","76e3d34b":"code","0c11e93e":"code","44dc7e31":"code","85111910":"code","32ed8c59":"code","34f60958":"code","0793e83b":"markdown","d254d92d":"markdown","31eaf3b7":"markdown","d003fdef":"markdown","ce64753b":"markdown","c9290052":"markdown","319f4d62":"markdown","27cd251a":"markdown","02a378f4":"markdown","dfd4f392":"markdown","1c81b16f":"markdown","c34b6ac2":"markdown","1a172899":"markdown","9f7fb359":"markdown","6371eac3":"markdown","b9e51144":"markdown","2be401c0":"markdown","fced4f93":"markdown","f5e28c2f":"markdown","392d8258":"markdown","ee4bedf8":"markdown","87896b81":"markdown","3120e882":"markdown","d9ef4dca":"markdown","d91584eb":"markdown","68e0b913":"markdown","c8300dd0":"markdown","f150f0a6":"markdown","2b6115ba":"markdown","9909619e":"markdown","6f620bfd":"markdown","646f4017":"markdown","1b326070":"markdown","927aee82":"markdown","79653e64":"markdown","596736c6":"markdown","d7aa340e":"markdown","91bb40cb":"markdown","34e21207":"markdown","0a8ab7cc":"markdown","0f0aefa0":"markdown","059494f3":"markdown","81ebd671":"markdown","e80ee062":"markdown","a21c40a3":"markdown","202a5db3":"markdown","33e5d8ca":"markdown","e9ca7372":"markdown","e9759214":"markdown","ad5f0556":"markdown","d7ef2687":"markdown","2ca958fd":"markdown","2735c6de":"markdown"},"source":{"c2b3383d":"# importing Python modules\nimport os\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\nprint(\"Python environment: {}\".format(sys.executable))\n\nimport pandas as pd \nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\n#from openpyxl import load_workbook\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport plotly_express as px\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nimport matplotlib.pyplot as plt # for plotting\n%matplotlib inline\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nprint(\"seaborn version: {}\". format(sns.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nfrom IPython.display import display\npd.options.display.max_columns = None\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport datetime\nfrom datetime import datetime\nfrom dateutil.parser import parse\nfrom time import time\n\n# to make this notebook's output identical at every run\nnp.random.seed(42)\n\nprint(\"Imported required Python packages\")","58928472":"# scikit-learn modules\nimport sklearn\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n# sklearn modules for preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n# from imblearn.over_sampling import SMOTE  # SMOTE\n# sklearn modules for ML model selection\nfrom sklearn.model_selection import train_test_split  # import 'train_test_split'\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Libraries for data modelling\nfrom sklearn import svm, tree, linear_model, neighbors\nfrom sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor # import RandomForestRegressor\nfrom sklearn.ensemble  import AdaBoostClassifier\nfrom sklearn.ensemble  import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\n\n# Common sklearn Model Helpers\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\n# from sklearn.datasets import make_classification\n\n# sklearn modules for performance metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import r2_score, make_scorer, mean_squared_error\nprint(\"scikit-learn libraries imported successfully\")\n\n# Other ML algorithms\nfrom lightgbm import LGBMRegressor\nprint(\"lightgbm imported\")\nimport xgboost as xgb\nprint(\"xgboost imported\")\nfrom mlxtend.regressor import StackingCVRegressor, StackingRegressor\nprint(\"StackingRegressor imported\")","73592d10":"import warnings\nwarnings.simplefilter('ignore')\n#warnings.simplefilter(action='ignore', category=FutureWarning)","35c42d43":"# Input data files are available in the \"..\/input\/\" directory.\nprint(os.listdir(\"..\/input\"))\n# Any results written to the current directory are saved as output.","1b87b577":"# importing the supplied dataset and storing it in a dataframe\ntraining = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n# making copies of original datasets for rest of this kernel\ndf_train = training.copy()\ndf_test = test.copy()\nprint(df_train.shape, df_test.shape)","8928a30f":"#drop target variable from training dataset\ntarget = df_train['SalePrice']  #target variable\ndf_train = df_train.drop('SalePrice', axis=1) \n\nprint(\"Training: {}, Target: {}, Test: {}\".format(df_train.shape, target.shape, df_test.shape))","d3e09d3c":"df_train_exp = df_train.copy() #make a copy of the training dataset for EDA purposes\nprint(df_train_exp.shape) ","904c34d6":"df_train_exp.head()","37a03b2b":"print(\"{} Numerical columns, {} Categorial columns\".format(\n    list(df_train_exp.select_dtypes(include=[np.number]).shape)[1],\n    list(df_train_exp.select_dtypes(include = ['object']).shape)[1]))","6dd868a5":"# let's break down the columns by their type (i.e. int64, float64, object)\ndf_train_exp.columns.to_series().groupby(df_train_exp.dtypes).groups","14a2f03d":"#list of columns with missing values\nprint(\"{} columns have missing values:\".format(\n    len(df_train_exp.columns[df_train_exp.isna().any()].tolist())))\ndf_train_exp.columns[df_train_exp.isna().any()].tolist()","8be93cf7":"df_train_exp.describe() # let's have a look at variable types in our dataframe","668fd911":"df_train_exp.hist(figsize=(18,18))\nplt.show()","79297b60":"# Testing for normal distribution hypothesis in numerical features\ntest_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\nnumerical_features = [f for f in df_train_exp.columns if df_train_exp.dtypes[f] != 'object']\nnormal = pd.DataFrame(df_train_exp[numerical_features])\nnormal = normal.apply(test_normality)\nprint(not normal.any())","c314c1cd":"# Calculate correlations\ncorr = training.corr(method='spearman')\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n# Heatmap\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr,\n            vmax=.5,\n            mask=mask,\n            #annot=True, \n            fmt='.2f',\n            linewidths=.2, cmap=\"YlGnBu\");","3e680629":"# Find correlations with the target and sort\ncorrelations = training.corr(method='spearman')['SalePrice'].sort_values(ascending=False)\ncorrelations_abs = correlations.abs()\nprint('\\nTop 10 correlations (absolute):\\n', correlations_abs.head(11))","1abff64f":"target_exp = target.copy() #make copy for exploratory purposes","cc85ab6f":"# let's see if there are any missing values (i.e. NA)\nprint(\"There are {} NA values in 'SalePrice'\".format(target_exp.isnull().values.sum()))","722a721e":"y = target_exp\nplt.figure(1); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)\nplt.ylabel('Frequency')\nprint(\"Skewness: %f\" % target_exp.skew())\n# get mean and standard deviation\n(mu, sigma) = norm.fit(target_exp)\nprint('Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma))","9ea66f9a":"# let's get some stats on the 'SalePrice' variable\nprint(\"Statistics for the supplied house prices training dataset:\\n\")\nprint(\"Minimum price: ${:,.2f}\".format(np.min(target_exp)))\nprint(\"Maximum price: ${:,.2f}\".format(np.max(target_exp)))\nprint(\"Mean price: ${:,.2f}\".format(np.mean(target_exp)))\nprint(\"Median price ${:,.2f}\".format(np.median(target_exp)))\nprint(\"Standard deviation of prices: ${:,.2f}\".format(np.std(target_exp)))","3064343f":"#  To get a visual of the outliers, let's plot a box plot.\nsns.boxplot(y = target)\nplt.ylabel('SalePrice (Log)')\nplt.title('Price');\n\n# count number of outliers after transformation is applied\nQ1 = target.quantile(0.25)\nQ3 = target.quantile(0.75)\nIQR = Q3 - Q1\nprint(\"IQR value: {}\\n# of outliers: {}\".format(\n    IQR,\n    ((target < (Q1 - 1.5 * IQR)) | (target > (Q3 + 1.5 * IQR))).sum()))","063c89a5":"#applying log transformation to the Target Variable\ntarget_tr = np.log1p(target)\n\n# let's plot a histogram with the fitted parameters used by the function\nsns.distplot(target_tr , fit=norm);\n(mu, sigma) = norm.fit(target_tr)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.title('Price (Log)');\nprint(\"Skewness: %f\" % target_tr.skew())","3139e7ea":"#  To get a visual of the outliers, let's plot a box plot.\nsns.boxplot(y = target_tr)\nplt.ylabel('SalePrice (Log)')\nplt.title('Price');\n\n# count number of outliers after transformation is applied\nQ1 = target_tr.quantile(0.25)\nQ3 = target_tr.quantile(0.75)\nIQR = Q3 - Q1\nprint(\"IQR value: {}\\n# of outliers: {}\".format(\n    IQR,\n    ((target_tr < (Q1 - 1.5 * IQR)) | (target_tr > (Q3 + 1.5 * IQR))).sum()))","0b5d8f04":"perc_na = (df_train.isnull().sum()\/len(df_train))*100\nratio_na = perc_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Values Ratio' :ratio_na})\nprint(missing_data.shape)\nmissing_data.head(20)","7dd25daf":"def house_pipeline_v1(dataframe,\n                      impute_method = \"median\",\n                      feature_transform = \"yes\",\n                      feature_scaling = \"RobustScaler\", \n                      feature_selection = \"yes\"):\n    # 0. initialising dataframe\n    df_pipe = dataframe.copy()\n    print(\"Dataframe loaded.\")\n    \n    # Drop redundant columns\n    df_pipe.drop(['Id'], axis=1, inplace=True) # drop Id column\n    print(\"Dropped redundant column 'Id'.\")\n\n    # column types variables\n    numeric_features = list(df_pipe.select_dtypes(\n        include=[np.number]).columns.values)\n    categ_features = list(df_pipe.select_dtypes(\n        include=['object']).columns.values)\n    for col in numeric_features:\n        df_pipe[col] = df_pipe[col].astype(float)\n\n    # 1. Handling missing values\n    # replacing NaNs in categorical features with \"None\"\n    df_pipe[categ_features] = df_pipe[categ_features].apply(\n        lambda x: x.fillna(\"None\"), axis=0)\n\n    # imputing numerical features\n    for col in (\"LotFrontage\", 'GarageYrBlt', 'GarageArea', 'GarageCars'):\n        df_pipe[col].fillna(0.0, inplace=True)\n        \n    if impute_method == \"median\": # replacing NaNs in numerical features with the median\n        df_pipe[numeric_features] = df_pipe[numeric_features].apply(\n            lambda x: x.fillna(x.median()), axis=0)\n        print(\"Missing values imputed with median.\")\n    \n    elif impute_method == \"mean\": # replacing NaNs in numerical features with the mean\n        df_pipe[numeric_features] = df_pipe[numeric_features].apply(\n            lambda x: x.fillna(x.mean()), axis=0)\n        print(\"Missing values imputed with mean.\")\n\n    # 2. Feature Engineering\n    # Examples: Discretize Continous Feature;\n    #           Decompose Features;\n    #           Add Combination of Feature\n    df_pipe['YrBltAndRemod']=df_pipe['YearBuilt']+df_pipe['YearRemodAdd']\n    df_pipe['TotalSF']=df_pipe['TotalBsmtSF'] + df_pipe['1stFlrSF'] + df_pipe['2ndFlrSF']\n\n    df_pipe['Total_sqr_footage'] = (df_pipe['BsmtFinSF1'] + df_pipe['BsmtFinSF2'] +\n                                     df_pipe['1stFlrSF'] + df_pipe['2ndFlrSF'])\n\n    df_pipe['Total_Bathrooms'] = (df_pipe['FullBath'] + (0.5 * df_pipe['HalfBath']) +\n                                   df_pipe['BsmtFullBath'] + (0.5 * df_pipe['BsmtHalfBath']))\n\n    df_pipe['Total_porch_sf'] = (df_pipe['OpenPorchSF'] + df_pipe['3SsnPorch'] +\n                                  df_pipe['EnclosedPorch'] + df_pipe['ScreenPorch'] + \n                                 df_pipe['WoodDeckSF'])\n    print(\"Feature enginering: added combination of features.\")\n    \n    df_pipe['haspool'] = df_pipe['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df_pipe['has2ndfloor'] = df_pipe['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df_pipe['hasgarage'] = df_pipe['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df_pipe['hasbsmt'] = df_pipe['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df_pipe['hasfireplace'] = df_pipe['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    print(\"Feature enginering: added boolean features.\")\n    \n    # 3. Feature Transformations (log(x), sqrt(x), x^2, etc.)\n    # Transform numerical features that should be considered as strings \n    df_pipe['MSSubClass'] = df_pipe['MSSubClass'].apply(str)\n    df_pipe['YrSold'] = df_pipe['YrSold'].astype(str)\n    df_pipe['MoSold'] = df_pipe['MoSold'].astype(str)\n    df_pipe['YrBltAndRemod'] = df_pipe['YrBltAndRemod'].astype(str)\n    print(\"Transformed numerical features that should be considered as strings.\")\n    \n    numeric_features = list(df_pipe.select_dtypes(\n        include=[np.number]).columns.values)\n    categ_features = list(df_pipe.select_dtypes(\n        include=['object']).columns.values)\n    \n    if feature_transform == \"yes\":\n        # Transform all numerical columns with skewness factor > 0.5\n        skew_features = df_pipe[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n        high_skew = skew_features[skew_features > 0.5]\n        skew_index = high_skew.index\n        for i in skew_index:\n            df_pipe[i] = boxcox1p(df_pipe[i], boxcox_normmax(df_pipe[i]+1))\n        print(\"Transformed numerical columns with high skewness factor.\")\n    elif feature_transform == \"no\":\n        pass\n\n    # 4. Label Encoding\n    df_pipe = pd.get_dummies(df_pipe)\n    print(\"Label Encoding: from {} cols to {} cols.\".format(\n        dataframe.shape[1], df_pipe.shape[1]))\n\n    # 5. Feature Scaling\n    #cols = df_pipe.select_dtypes([np.number]).columns\n    if feature_scaling == 'MinMaxScaler':\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        for col in numeric_features:\n            df_pipe[[col]] = scaler.fit_transform(df_pipe[[col]])\n        print(\"Performed feature Scaling with MinMaxScaler.\")\n\n    elif feature_scaling == 'StandardScaler':\n        scaler = StandardScaler()\n        for col in numeric_features:\n            df_pipe[[col]] = scaler.fit_transform(df_pipe[[col]])\n        print(\"Performed feature Scaling with StandardScaler.\")\n\n    elif feature_scaling == \"RobustScaler\":\n        scaler = RobustScaler()\n        for col in numeric_features:\n            df_pipe[[col]] = scaler.fit_transform(df_pipe[[col]])\n        print(\"Performed feature Scaling with RobustScaler.\")\n    \n    # 6. Feature Selection\n    ## let's remove columns with little variance (to reduce overfitting)\n    overfit = []\n    for i in df_pipe.columns:\n        counts = df_pipe[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df_pipe) * 100 > 99.9: # the threshold is set at 99.9%\n            overfit.append(i)\n    overfit = list(overfit)\n    # let's make sure to keep data processing columns needed later on\n    try:\n        overfit.remove('Dataset_Train')\n        overfit.remove('Dataset_Test')\n    except:\n        pass\n    df_pipe.drop(overfit, axis=1, inplace=True)\n    print(\"To prevent overfitting, {} columns were removed.\".format(len(overfit)))\n    \n    ## Summary\n    print(\"Shape of transformed dataset: {} (original: {})\".format(df_pipe.shape, dataframe.shape))\n    return df_pipe","33682f67":"def target_transf(target, \n                  transform=\"log\"):\n    \n    if transform == \"log\":\n        target_tranf = np.log1p(target)\n        print(\"Target feature transformed with natural logarithm.\")\n    \n    elif transform == \"sqrt\":\n        target_tranf = np.sqrt(target)\n        print(\"Target feature transformed with sqrt.\")\n    \n    elif transform == \"square\":\n        target_tranf = np.square(target)\n        print(\"Target feature transformed with square.\")\n    \n    print(\"Shape of transformed target: {}\".format(target_tr.shape))\n    return target_tranf","b0e653ab":"# Test pipeline\ndf_train_test = house_pipeline_v1(df_train)\nprint(\"\\n\")\ntarget_tr = target_transf(target)","543ade43":"# let's check that we no longer have any missing values\nperc_na = (df_train_test.isnull().sum()\/len(df_train_test))*100\nratio_na = perc_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'missing_ratio' :ratio_na})\nmissing_data = missing_data.drop(missing_data[missing_data.missing_ratio == 0].index)\nmissing_data.head(5)","510191dd":"# target feature transformed\nsns.distplot(target_tr , fit=norm);\n(mu, sigma) = norm.fit(target_tr)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.title('Price (Log)');\nprint(\"Skewness: %f\" % target_tr.skew())","ce45102f":"## Feature Scaling\ncol_eda = list(correlations_abs.index)\ndf_train_scal = df_train_test.filter(col_eda, axis=1).copy()\ndf_train_scal.hist(figsize=(18,18))\nplt.show()","e8e74ad7":"# Copy dataframes prior to data processing\ndf_train_pipeline = df_train.copy()\ndf_test_pipeline = df_test.copy()\n# Concat dataframes\ndf_train_pipeline[\"Dataset\"] = \"Train\"\ndf_test_pipeline[\"Dataset\"] = \"Test\"\n# Concat dataframes\ndf_joined = pd.concat([df_train_pipeline, df_test_pipeline], \n                      sort=False)\ndf_joined = df_joined.reset_index(drop=True) # reset index\nprint(\"Joined Dataframe shape: {}\".format(df_joined.shape))","a05d2924":"df_joined_ml = house_pipeline_v1(df_joined,\n                                 impute_method = \"median\",\n                                 feature_transform = \"yes\",\n                                 feature_scaling = \"RobustScaler\", \n                                 feature_selection = \"yes\")\nprint(\"----\\n\")\ntarget_ml = target_transf(target)\nprint(\"----\\n\")\nprint(\"Transformed Joined Dataframe shape: {}, and target shape: {}\".format(\n    df_joined_ml.shape, target_ml.shape))","4c5d66f7":"# Extract Training data from joined transformed dataset\ndf_train_ml = df_joined_ml[df_joined_ml['Dataset_Train']==1].copy()\n# Remove redundant features\ndf_train_ml.drop(['Dataset_Train'], axis=1, inplace=True)\ndf_train_ml.drop(['Dataset_Test'], axis=1, inplace=True)\n# Reset index\ndf_train_ml = df_train_ml.reset_index(drop=True) \nprint(df_train_ml.shape)","fc6960f4":"# Extract Testing data from joined transformed dataset\ndf_test_ml = df_joined_ml[df_joined_ml['Dataset_Test']==1].copy()\n# Remove redundant features\ndf_test_ml.drop(['Dataset_Train'], axis=1, inplace=True)\ndf_test_ml.drop(['Dataset_Test'], axis=1, inplace=True)\n# Reset index\ndf_test_ml = df_test_ml.reset_index(drop=True)\nprint(df_test_ml.shape)","9ccd6b1c":"X_train, X_test, y_train, y_test = train_test_split(df_train_ml,\n                                                    target_ml,\n                                                    test_size=0.2,\n                                                    stratify=df_train_ml['OverallQual'],\n                                                    random_state=42)","86fe4f22":"print(\"Training Data Shape: {}\".format(df_train_ml.shape))\nprint(\"X_train Shape: {}\".format(X_train.shape))\nprint(\"X_test Shape: {}\".format(X_test.shape))","78899327":"# selection of algorithms to consider\nmodels = []\nmodels.append(('Ridge Regression', Ridge(alpha=1.0)))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('Random Forest', RandomForestRegressor(\n    n_estimators=100, random_state=7)))\nmodels.append(('Lasso', Lasso(random_state=42)))\nmodels.append(('XGBoost Regressor', xgb.XGBRegressor(objective='reg:squarederror', \n                                                     random_state=42)))\nmodels.append(('Gradient Boosting Regressor', GradientBoostingRegressor()))\nmodels.append(('LGBM Regressor',LGBMRegressor(objective='regression')))\nmodels.append(('SVR',SVR()))\n\n# set table to table to populate with performance results\nrmse_results = []\nnames = []\ncol = ['Algorithm', 'RMSE Mean', 'RMSE SD']\ndf_results = pd.DataFrame(columns=col)\n\n# evaluate each model using cross-validation\nkfold = model_selection.KFold(n_splits=5, shuffle = True, random_state=7)\ni = 0\nfor name, model in models:\n    # -mse scoring\n    cv_mse_results = model_selection.cross_val_score(\n        model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n    # calculate and append rmse results\n    cv_rmse_results = np.sqrt(-cv_mse_results)\n    rmse_results.append(cv_rmse_results)\n    names.append(name)\n    df_results.loc[i] = [name,\n                         round(cv_rmse_results.mean(), 4),\n                         round(cv_rmse_results.std(), 4)]\n    i += 1\ndf_results.sort_values(by=['RMSE Mean'], ascending=True).reset_index(drop=True)","a037c23d":"fig = plt.figure(figsize=(15, 8))\nfig.suptitle('Algorithm RMSE Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(rmse_results)\nax.set_xticklabels(names)\nplt.show();","bc43f687":"import xgboost as xgb\nxgb_regressor = xgb.XGBRegressor(random_state=42)","4c4930ed":"# start = time() # Get start time\n# cv_sets_xgb = ShuffleSplit(random_state = 10) # shuffling our data for cross-validation\n# parameters_xgb = {'n_estimators':range(2000, 8000, 500), \n#              'learning_rate':[0.05,0.060,0.070], \n#              'max_depth':[3,5,7],\n#              'min_child_weight':[1,1.5,2]}\n# scorer_xgb = make_scorer(mean_squared_error)\n# grid_obj_xgb = RandomizedSearchCV(xgb_regressor, \n#                                  parameters_xgb,\n#                                  scoring = scorer_xgb, \n#                                  cv = cv_sets_xgb,\n#                                  random_state= 99)\n# grid_fit_xgb = grid_obj_xgb.fit(X_train, y_train)\n# xgb_opt = grid_fit_xgb.best_estimator_\n\n# end = time() # Get end time\n# xgb_time = (end-start)\/60 # Calculate training time\n# print('It took {0:.2f} minutes for RandomizedSearchCV to converge to optimised parameters for the RandomForest model'.format(xgb_time))\n# ## Print results\n# print('='*20)\n# print(\"best params: \" + str(grid_fit_xgb.best_estimator_))\n# print(\"best params: \" + str(grid_fit_xgb.best_params_))\n# print('best score:', grid_fit_xgb.best_score_)\n# print('='*20)","7ef87ab6":"# XGBoost with tuned parameters\nxgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\nxgb_opt = xgb.XGBRegressor(learning_rate=0.01,\n                           n_estimators=6000,\n                           max_depth=4,\n                           min_child_weight=0,\n                           gamma=0.6,\n                           subsample=0.7,\n                           colsample_bytree=0.7,\n                           objective='reg:squarederror',\n                           nthread=-1,\n                           scale_pos_weight=1,\n                           seed=27,\n                           reg_alpha=0.00006,\n                           random_state=42)","f959d2ab":"gbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)","c252b97a":"lightgbm = LGBMRegressor(objective='regression', \n                         num_leaves=6,\n                         learning_rate=0.01, \n                         n_estimators=7000,\n                         max_bin=200, \n                         bagging_fraction=0.8,\n                         bagging_freq=4, \n                         bagging_seed=8,\n                         feature_fraction=0.2,\n                         feature_fraction_seed=8,\n                         min_sum_hessian_in_leaf = 11,\n                         verbose=-1,\n                         random_state=42)","8d29ac04":"# start = time() # Get start time\n# rf_regressor = RandomForestRegressor(random_state=42)\n# cv_sets = ShuffleSplit(random_state = 4) # shuffling our data for cross-validation\n# parameters = {'n_estimators':range(5, 950, 5), \n#               'min_samples_leaf':range(20, 40, 5), \n#               'max_depth':range(3, 5, 1)}\n# scorer = make_scorer(mean_squared_error)\n# n_iter_search = 10\n# grid_obj = RandomizedSearchCV(rf_regressor, \n#                               parameters, \n#                               n_iter = n_iter_search, \n#                               scoring = scorer, \n#                               cv = cv_sets,\n#                               random_state= 99)\n# grid_fit = grid_obj.fit(X_train, y_train)\n# rf_opt = grid_fit.best_estimator_\n# end = time() # Get end time\n# rf_time = (end-start)\/60 # Calculate training time\n# print('It took {0:.2f} minutes for RandomizedSearchCV to converge to optimised parameters for the RandomForest model'.format(rf_time))\n# ## Print results\n# print('='*20)\n# print(\"best params: \" + str(grid_fit.best_estimator_))\n# print(\"best params: \" + str(grid_fit.best_params_))\n# print('best score:', grid_fit.best_score_)\n# print('='*20)","ec1a4b91":"# RandomForest with tuned parameters\nrf_reg = RandomForestRegressor(n_estimators=100, \n                               random_state=7)\nrf_opt = RandomForestRegressor(n_estimators=1200,\n                               max_depth=15,\n                               min_samples_split=5,\n                               min_samples_leaf=5,\n                               max_features=None,\n                               oob_score=True,\n                               random_state=42)","59fa3320":"rf_imp = RandomForestRegressor(n_estimators=1200,\n                               max_depth=15,\n                               min_samples_split=5,\n                               min_samples_leaf=5,\n                               max_features=None,\n                               oob_score=True,\n                               random_state=42)\nrf_imp.fit(X_train, y_train)\nimportances = rf_imp.feature_importances_\ndf_param_coeff = pd.DataFrame(columns=['Feature', 'Coefficient'])\nfor i in range(len(X_train.columns)-1):\n    feat = X_train.columns[i]\n    coeff = importances[i]\n    df_param_coeff.loc[i] = (feat, coeff)\ndf_param_coeff.sort_values(by='Coefficient', ascending=False, inplace=True)\ndf_param_coeff = df_param_coeff.reset_index(drop=True)\nprint(\"Top 10 features:\\n{}\".format(df_param_coeff.head(10)))\n\nimportances = rf_imp.feature_importances_\nindices = np.argsort(importances)[::-1] # Sort feature importances in descending order\nnames = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances\nplt.figure(figsize=(15, 7)) # Create plot\nplt.title(\"Top 10 Most Important Features\") # Create plot title\nplt.bar(range(10), importances[indices][:10]) # Add bars\nplt.xticks(range(10), names[:10], rotation=90) # Add feature names as x-axis labels\n#plt.bar(range(X_train.shape[1]), importances[indices]) # Add bars\n#plt.xticks(range(X_train.shape[1]), names, rotation=90) # Add feature names as x-axis labels\nplt.show() # Show plot","2f9c4352":"kfolds = KFold(n_splits=5, shuffle=True, random_state=7)\nrcv_alphas = np.arange(14, 16, 0.1)\nridge = RidgeCV(alphas=rcv_alphas, \n                cv=kfolds)","888c38c0":"svr = SVR(C= 20, \n          epsilon= 0.008, \n          gamma=0.0003)","00a06a1b":"# selection of algorithms to consider\nstart = time() # Get start time\nmodels = []\nmodels.append(('Ridge Regression', ridge))\nmodels.append(('Random Forest', rf_opt))\nmodels.append(('XGBoost Regressor', xgb_opt))\nmodels.append(('Gradient Boosting Regressor', gbr))\nmodels.append(('LGBM Regressor',lightgbm))\nmodels.append(('SVR',svr))\nmodels.append(('StackingRegressor',StackingRegressor(regressors=(gbr,\n                                                                 xgb_opt,\n                                                                 lightgbm,\n                                                                 rf_opt,\n                                                                 ridge, \n                                                                 svr),\n                                                     meta_regressor=xgb_opt,\n                                                     use_features_in_secondary=False)))\n\n# set table to table to populate with performance results\nrmse_results = []\nnames = []\ncol = ['Algorithm', 'RMSE Mean', 'RMSE SD']\ndf_results = pd.DataFrame(columns=col)\n\n# evaluate each model using cross-validation\nkfold = model_selection.KFold(n_splits=5, shuffle = True, random_state=7)\ni = 0\nfor name, model in models:\n    print(\"Evaluating {}...\".format(name))\n    # -mse scoring\n    cv_mse_results = model_selection.cross_val_score(\n        model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n    # calculate and append rmse results\n    cv_rmse_results = np.sqrt(-cv_mse_results)\n    rmse_results.append(cv_rmse_results)\n    names.append(name)\n    df_results.loc[i] = [name,\n                         round(cv_rmse_results.mean(), 4),\n                         round(cv_rmse_results.std(), 4)]\n    i += 1\nend = time() # Get end time\neval_time = (end-start)\/60 # Calculate training time\nprint('Evaluation completed.\\nIt took {0:.2f} minutes to evaluate all models using a 5-fold cross-validation.'.format(eval_time))\ndf_results.sort_values(by=['RMSE Mean'], ascending=True).reset_index(drop=True)","76e3d34b":"fig = plt.figure(figsize=(20, 8))\nfig.suptitle('Algorithm RMSE Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(rmse_results)\nax.set_xticklabels(names)\nplt.show()","0c11e93e":"stack_gen = StackingCVRegressor(regressors=(gbr,\n                                            xgb_opt,\n                                            lightgbm,\n                                            rf_opt,\n                                            ridge, \n                                            svr),\n                                meta_regressor=xgb_opt,\n                                use_features_in_secondary=False)","44dc7e31":"print('Fitting models to the training data:')\nstart = time() # Get start time\n\nprint('xgboost....')\nxgb_model_full_data = xgb_opt.fit(df_train_ml, target_ml)\nprint('GradientBoosting....')\ngbr_model_full_data = gbr.fit(df_train_ml, target_ml)\nprint('lightgbm....')\nlgb_model_full_data = lightgbm.fit(df_train_ml, target_ml)\nprint('RandomForest....')\nrf_model_full_data = rf_opt.fit(df_train_ml, target_ml)\nprint('Ridge....')\nridge_model_full_data = ridge.fit(df_train_ml, target_ml)\nprint('SVR....')\nsvr_model_full_data = svr.fit(df_train_ml, target_ml)\nprint('Stacking Regression....')\nstack_gen_model = stack_gen.fit(np.array(df_train_ml), np.array(target_ml))\n\nend = time() # Get end time\nfitting_time = (end-start)\/60 # Calculate training time\nprint('Fitting completed.\\nIt took {0:.2f} minutes to fit all the models to the training data.'.format(fitting_time))","85111910":"def blend_models_predict(X):\n    return ((0.25 * stack_gen_model.predict(np.array(X))) + \\\n            (0.25 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * svr_model_full_data.predict(X)) + \\\n            (0.15 * lgb_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X))+ \\\n            (0.05 * xgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) \n           )","32ed8c59":"# Generate predictions from the blend\ny_pred_final = np.floor(np.expm1(blend_models_predict(df_test_ml)))","34f60958":"# Generate submission dataframe\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': y_pred_final})\n\n# Exporting submission to CSV\nmy_submission.to_csv('submission-080719_v1.csv', index=False)","0793e83b":"### 5.1 Data Prep for ML","d254d92d":"This step is often referred to as **data wrangling** which includes implementing **data architectures for storage and processing**, developing data **governance standards** for quality and control, **data extraction** (i.e. ETL and web scraping), and **data cleaning** to identify aberrant, missing, or outlier data points.","31eaf3b7":"> It is important to deploy **descriptive and graphical statistics** to look for potential **problems**, **patterns**, **classifications**, **correlations** and **comparisons** in the dataset. In addition, **data categorization** (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.","d003fdef":"### 4.4 Pipeline Validation","ce64753b":"In this stage, we will clean our data by:<br> \n1. Data Cleaning: Outliers and Missing Values, <br>\n2. Feature Selection, <br>\n3. Feature Engineering, <br>\n4. Label Encoding and Feature Scaling","c9290052":"# Kaggle Competition - House Prices: Advanced Regression Techniques","319f4d62":"#### 6.4 Random Forest Regressor","27cd251a":"> Let's get the RandomForestRegression model's assessment of the Top 5 most important features","02a378f4":"### 5.2 Baseline Results","dfd4f392":"> XGBoost is short for \u201cExtreme Gradient Boosting\u201d and is popular algorithm on Kaggle. It is fast to implement but time-consuming to tune the hyper-parameters so below, I have included the code for hyperparameter tuning but did not run it on Kaggle.","1c81b16f":"**Contents:**\n* [1. Framing the Problem](#first-bullet)\n* [2. Get the Data](#second-bullet)\n* [3. Exploratory Data Analysis](#third-bullet)\n* [4. Data Preparation](#fourth-bullet)\n* [5. Maching Learning Models](#fifth-bullet)\n* [6 . Fine-Tuning ML Hyper-Parameters](#sixth-bullet)\n* [7 . Blending ML Algorithms](#seventh-bullet)\n* [8 . Submission](#eigth-bullet)","c34b6ac2":"### 4.3 Pre-processing Pipeline","1a172899":"> We note that their are quite a few non-numerical variables, which would result in a large number of columns in our training dataframe once we implement one-hot encoding prior to fitting our model.","9f7fb359":"----\n<a class=\"anchor\" id=\"third-bullet\"><\/a>\n## 3. Exploratory Data Analysis","6371eac3":"### 4.1 Log Transformation: Target Feature","b9e51144":"---\n<a class=\"anchor\" id=\"third-bullet\"><\/a>\n### 3.3 Target Feature: SalePrice","2be401c0":"#### 6.1 XGBoost","fced4f93":"---\n<a class=\"anchor\" id=\"fourth-bullet\"><\/a>\n## 4. Data Preparation","f5e28c2f":"Author: Hamza Bendemra <br>\nPython version: 3.6.8 <br>","392d8258":"----\n<a class=\"anchor\" id=\"sixth-bullet\"><\/a>\n### 6. Fine-Tuning ML Hyper-Parameters","ee4bedf8":"#### 6.2 Gradient Boosting Regressor","87896b81":"----\n<a class=\"anchor\" id=\"second-bullet\"><\/a>\n## 2. Get the Data","3120e882":"> According to the our analysis, the top variables with the most percentage of missing values (>15%) are:\n- PoolQC with 99.5%\n- MiscFeature with 96.30%\n- Alley\twith 93.77%\n- Fence\twith 80.75%\n- FireplaceQu with 47.26%\n- LotFrontage with 17.740%","d9ef4dca":"In this ML project, we will train an ML model to predict a home's value based on supplied feature vectors. <br>\nWe will also look at what are the **leading predictors to determine home's value**. <br>\n\nML category: **Supervised Learning (offline)** <br>\nProblem Type: **Regression** <br>\nTarget outcome type: **Numerical** <br>\nData Type: **Tabular** (CSV files) <br>\nPerformance Measure: **Root-Mean-Squared-Error (RMSE)** between the logarithm of the predicted value and the logarithm of the observed sales price. <br>\nPerformance Requirement: **minimize RMSE** <br>","d91584eb":"### 3.1 Quick EDA","68e0b913":"Let's focus on our training dataset and see which variables have missing values.","c8300dd0":"----\n<a class=\"anchor\" id=\"seventh-bullet\"><\/a>\n## 7. Blending ML Algorithms with StackingCVRegressor","f150f0a6":"----\n<a class=\"anchor\" id=\"fifth-bullet\"><\/a>\n## 5. Machine Learning Models","2b6115ba":"**A few observations can be made based on the EDA so far**:\n- The trainig dataset has 19 features (out 80) with missing values\n- Numerical features histograms' clearly show that some features are tail-heavy; indeed several distributions are right- or left-skewed (e.g. OpenPorchSF, 2ndFlrSF). Data transformation methods may be required to approach a normal distribution prior to fitting a model to the data.\n- Feature \"Id\" is likely to be a unique identifier for the homes recorded given the feature's quasi-uniform distribution, and should be removed in pre-processing.","9909619e":"### 3.2 Correlation Map","6f620bfd":"#### 6.6 SVR","646f4017":"### 2.2 Import data","1b326070":"**We have 1460 observations of 80 variables in the training dataframe**. The variables are described below: <br>\n\n> **SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict. <br><br>\nMSSubClass: The building class <br>\nMSZoning: The general zoning classification <br>\nLotFrontage: Linear feet of street connected to property <br>\nLotArea: Lot size in square feet <br>\nStreet: Type of road access <br>\nAlley: Type of alley access <br>\nLotShape: General shape of property <br>\nLandContour: Flatness of the property <br>\nUtilities: Type of utilities available <br>\nLotConfig: Lot configuration <br>\nLandSlope: Slope of property <br>\nNeighborhood: Physical locations within Ames city limits <br>\nCondition1: Proximity to main road or railroad <br>\nCondition2: Proximity to main road or railroad (if a second is present) <br>\nBldgType: Type of dwelling <br>\nHouseStyle: Style of dwelling <br>\nOverallQual: Overall material and finish quality <br>\nOverallCond: Overall condition rating <br>\nYearBuilt: Original construction date <br>\nYearRemodAdd: Remodel date <br>\nRoofStyle: Type of roof <br>\nRoofMatl: Roof material <br>\nExterior1st: Exterior covering on house <br>\nExterior2nd: Exterior covering on house (if more than one material) <br>\nMasVnrType: Masonry veneer type <br>\nMasVnrArea: Masonry veneer area in square feet <br>\nExterQual: Exterior material quality <br>\nExterCond: Present condition of the material on the exterior <br>\nFoundation: Type of foundation <br>\nBsmtQual: Height of the basement <br>\nBsmtCond: General condition of the basement <br>\nBsmtExposure: Walkout or garden level basement walls <br>\nBsmtFinType1: Quality of basement finished area <br>\nBsmtFinSF1: Type 1 finished square feet <br>\nBsmtFinType2: Quality of second finished area (if present) <br>\nBsmtFinSF2: Type 2 finished square feet <br>\nBsmtUnfSF: Unfinished square feet of basement area <br>\nTotalBsmtSF: Total square feet of basement area <br>\nHeating: Type of heating <br>\nHeatingQC: Heating quality and condition <br>\nCentralAir: Central air conditioning <br>\nElectrical: Electrical system <br>\n1stFlrSF: First Floor square feet <br>\n2ndFlrSF: Second floor square feet <br>\nLowQualFinSF: Low quality finished square feet (all floors) <br>\nGrLivArea: Above grade (ground) living area square feet <br>\nBsmtFullBath: Basement full bathrooms <br>\nBsmtHalfBath: Basement half bathrooms <br>\nFullBath: Full bathrooms above grade <br>\nHalfBath: Half baths above grade <br> \nBedroom: Number of bedrooms above basement level <br> \nKitchen: Number of kitchens <br> \nKitchenQual: Kitchen quality <br> \nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms) <br> \nFunctional: Home functionality rating\nFireplaces: Number of fireplaces <br>\nFireplaceQu: Fireplace quality <br>\nGarageType: Garage location <br>\nGarageYrBlt: Year garage was built <br>\nGarageFinish: Interior finish of the garage <br>\nGarageCars: Size of garage in car capacity <br>\nGarageArea: Size of garage in square feet <br>\nGarageQual: Garage quality <br>\nGarageCond: Garage condition <br>\nPavedDrive: Paved driveway <br>\nWoodDeckSF: Wood deck area in square feet <br>\nOpenPorchSF: Open porch area in square feet <br>\nEnclosedPorch: Enclosed porch area in square feet <br>\n3SsnPorch: Three season porch area in square feet <br>\nScreenPorch: Screen porch area in square feet <br>\nPoolArea: Pool area in square feet <br>\nPoolQC: Pool quality <br>\nFence: Fence quality <br>\nMiscFeature: Miscellaneous feature not covered in other categories <br>\nMiscVal: $Value of miscellaneous feature <br>\nMoSold: Month Sold <br>\nYrSold: Year Sold <br>\nSaleType: Type of sale <br>\nSaleCondition: Condition of sale <br>","927aee82":"The data is provided by the Kaggle competition in the form or of two CSV files. <br>\nA **training** and **testing** dataset are provided.","79653e64":"----\n<a class=\"anchor\" id=\"first-bullet\"><\/a>\n## 1. Framing the Problem","596736c6":"> As we can see above, applying a log-transformation has significantly taken our distribution closer to a normal distribution. The skewness measure is also much closer to zero, with a valuoe of 0.121335.","d7aa340e":"#### 6.3 LGBM Regressor","91bb40cb":"> Let's spent some time getting to know our target variable 'SalePrice'.","34e21207":"Stacking regression is an ensemble learning technique to combine multiple regression models via a meta-regressor. The individual regression models are trained based on the complete training set; then, the meta-regressor is fitted based on the outputs -- meta-features -- of the individual regression models in the ensemble.\n\n**StackingCVRegressor** extends the standard stacking algorithm (implemented as StackingRegressor) using out-of-fold predictions to prepare the input data for the level-2 regressor.","0a8ab7cc":"> Cross-validation can be used (as shown below) to find optimum hyperparameters. Due to the time needed for RandomizedSearchCV to provide results, I've commended out the next cell but left the code FYI.","0f0aefa0":"#### 6.7 Model Performance Review","059494f3":"### 4.2 Missing Values","81ebd671":"> Focusing solely on the **'SalePrice' feature** (bottom row), we can see **strong correlations with various variables** including OverallQual, GrLivArea, and GarageCars - all three of these intuitively make sense that they would strongly affect a home's price. However, let's not forget that Pearson's **correlation factor only picks up linear relationships**, more complex relationships surely exist between other features and the target feature.","e80ee062":"**Remarks:**\n> As we can see on the plot, the distribution of 'SalePrice' is right-skewed which is reflected in the skewness factor which is positive. We would like to get the **skewness factor as close to zero as possible** (i.e. a normal or Gaussian distribution).\n\n> This can accomplished by either removing outliers or **transforming the variable**. Removing outliers may be tricky as expertise in real estate is needed to assess whether whether outliers should be removed or not. Applying transformations is typically a safer option if it can deliver the desired outcome. In the case of positive skewness, **log transformation** does the trick.","a21c40a3":"> I hope you enjoyed this notebook. If you found it helpful, don't hesitate to  upvote - it would be very much appreciated :-)\n<br>\n> I wish all the best to the rest of this amazing Kaggle community! ","202a5db3":"#### 6.5 Ridge Regression","33e5d8ca":"> Let's take a look at a correltion map. I'll then focus on the top most strongly correlated features with the target feature. Note: when the variables are not normally distributed or the relationship between the variables is not linear (as is the case here), it is more appropriate to use the **Spearman rank correlation method** rather than the default Pearson's method.","e9ca7372":"### 2.1 Import Python libraries","e9759214":">  Splitting Training Dataframe prior to training ML algorithms using cross-validation. I've chosen to \"stratify\" my training sets as per the feature with the stronges correlation with the target label.","ad5f0556":"With **79 explanatory variables** describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. Great for **practicing skills** such as: <br>\n- Creative feature engineering\n- Advanced regression techniques like random forest and gradient boosting","d7ef2687":"----\n<a class=\"anchor\" id=\"eigth-bullet\"><\/a>\n## 8. Submission","2ca958fd":"Submitted datasets include: <br>\n- train.csv - the training set\n- test.csv - the test set\n- data_description.txt - full description of each column","2735c6de":"First of all, I would like to acknowledge some great kernels out there that have influenced my approach in this kernel. I highly recommend checking them out also:\n- [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne \n- [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by Pedro Marcelino\n- [Blend&Stack LR&GB = 0.10649 {House Prices} v57](https:\/\/www.kaggle.com\/itslek\/blend-stack-lr-gb-0-10649-house-prices-v57) by Nanashi"}}