{"cell_type":{"f93eec63":"code","c18bb90b":"code","9b610d50":"markdown"},"source":{"f93eec63":"import tensorflow as tf\n\nclass QRNN():\n\n    def __init__(self, in_size, size, conv_size=2):\n        self.kernel = None\n        self.batch_size = -1\n        self.conv_size = conv_size\n        self.c = None\n        self.h = None\n        self._x = None\n        if conv_size == 1:\n            self.kernel = QRNNLinear(in_size, size)\n        elif conv_size == 2:\n            self.kernel = QRNNWithPrevious(in_size, size)\n        else:\n            self.kernel = QRNNConvolution(in_size, size, conv_size)\n\n    def _step(self, f, z, o):\n        with tf.variable_scope(\"fo-Pool\"):\n            # f,z,o is batch_size x size\n            f = tf.sigmoid(f)\n            z = tf.tanh(z)\n            o = tf.sigmoid(o)\n            self.c = tf.multiply(f, self.c) + tf.multiply(1 - f, z)\n            self.h = tf.multiply(o, self.c)  # h is size vector\n\n        return self.h\n\n    def forward(self, x):\n        length = lambda mx: int(mx.get_shape()[0])\n\n        with tf.variable_scope(\"QRNN\/Forward\"):\n            if self.c is None:\n                # init context cell\n                self.c = tf.zeros([length(x), self.kernel.size], dtype=tf.float32)\n\n            if self.conv_size <= 2:\n                # x is batch_size x sentence_length x word_length\n                # -> now, transpose it to sentence_length x batch_size x word_length\n                _x = tf.transpose(x, [1, 0, 2])\n\n                for i in range(length(_x)):\n                    t = _x[i] # t is batch_size x word_length matrix\n                    f, z, o = self.kernel.forward(t)\n                    self._step(f, z, o)\n            else:\n                c_f, c_z, c_o = self.kernel.conv(x)\n                for i in range(length(c_f)):\n                    f, z, o = c_f[i], c_z[i], c_o[i]\n                    self._step(f, z, o)\n\n        return self.h\n\n\nclass QRNNLinear():\n\n    def __init__(self, in_size, size):\n        self.in_size = in_size\n        self.size = size\n        self._weight_size = self.size * 3  # z, f, o\n        with tf.variable_scope(\"QRNN\/Variable\/Linear\"):\n            initializer = tf.random_normal_initializer()\n            self.W = tf.get_variable(\"W\", [self.in_size, self._weight_size], initializer=initializer)\n            self.b = tf.get_variable(\"b\", [self._weight_size], initializer=initializer)\n\n    def forward(self, t):\n        # x is batch_size x word_length matrix\n        _weighted = tf.matmul(t, self.W)\n        _weighted = tf.add(_weighted, self.b)\n\n        # now, _weighted is batch_size x weight_size\n        f, z, o = tf.split(_weighted, num_or_size_splits=3, axis=1)  # split to f, z, o. each matrix is batch_size x size\n        return f, z, o\n\n\nclass QRNNWithPrevious():\n\n    def __init__(self, in_size, size):\n        self.in_size = in_size\n        self.size = size\n        self._weight_size = self.size * 3  # z, f, o\n        self._previous = None\n        with tf.variable_scope(\"QRNN\/Variable\/WithPrevious\"):\n            initializer = tf.random_normal_initializer()\n            self.W = tf.get_variable(\"W\", [self.in_size, self._weight_size], initializer=initializer)\n            self.V = tf.get_variable(\"V\", [self.in_size, self._weight_size], initializer=initializer)\n            self.b = tf.get_variable(\"b\", [self._weight_size], initializer=initializer)\n\n    def forward(self, t):\n        if self._previous is None:\n            self._previous = tf.get_variable(\"previous\", [t.get_shape()[0], self.in_size], initializer=tf.random_normal_initializer())\n\n        _current = tf.matmul(t, self.W)\n        _previous = tf.matmul(self._previous, self.V)\n        _previous = tf.add(_previous, self.b)\n        _weighted = tf.add(_current, _previous)\n\n        f, z, o = tf.split(_weighted, num_or_size_splits=3, axis=1)  # split to f, z, o. each matrix is batch_size x size\n        self._previous = t\n        return f, z, o\n\n\nclass QRNNConvolution():\n\n    def __init__(self, in_size, size, conv_size):\n        self.in_size = in_size\n        self.size = size\n        self.conv_size = conv_size\n        self._weight_size = self.size * 3  # z, f, o\n\n        with tf.variable_scope(\"QRNN\/Variable\/Convolution\"):\n            initializer = tf.random_normal_initializer()\n            self.conv_filter = tf.get_variable(\"conv_filter\", [conv_size, in_size, self._weight_size], initializer=initializer)\n\n    def conv(self, x):\n        # !! x is batch_size x sentence_length x word_length(=channel) !!\n        _weighted = tf.nn.conv1d(x, self.conv_filter, stride=1, padding=\"SAME\", data_format=\"NWC\")\n\n        # _weighted is batch_size x conved_size x output_channel\n        _w = tf.transpose(_weighted, [1, 0, 2])  # conved_size x  batch_size x output_channel\n        _ws = tf.split(_w, num_or_size_splits=3, axis=2) # make 3(f, z, o) conved_size x  batch_size x size\n        return _ws","c18bb90b":"import os\nimport unittest\nimport time\nimport functools\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom tensorflow import nn\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\ndef qrnn_forward(X, size, n_class, batch_size, conv_size):\n    in_size = int(X.get_shape()[2])\n\n    qrnn = QRNN(in_size=in_size, size=size, conv_size=conv_size)\n    hidden = qrnn.forward(X)\n\n    with tf.name_scope(\"QRNN-Classifier\"):\n        W = tf.Variable(tf.random_normal([size, n_class]), name=\"W\")\n        b = tf.Variable(tf.random_normal([n_class]), name=\"b\")\n        output = tf.add(tf.matmul(hidden, W), b)\n\n        return output\n \ndef baseline_forward(X, size, n_class):\n        shape = X.get_shape()\n        seq = tf.transpose(X, [1, 0, 2]) \n\n        with tf.name_scope(\"LSTM\"):\n            lstm_cell = LSTMCell(size, forget_bias=1.0)\n            outputs, states = nn.dynamic_rnn(time_major=True, cell=lstm_cell, inputs=seq, dtype=tf.float32)\n\n        with tf.name_scope(\"LSTM-Classifier\"):\n            W = tf.Variable(tf.random_normal([size, n_class]), name=\"W\")\n            b = tf.Variable(tf.random_normal([n_class]), name=\"b\")\n            output = tf.matmul(outputs[-1], W) + b\n\n        return output\n\ndef check_by_digits(graph, qrnn=-1, baseline=False, random=False):\n    digits = load_digits()\n    horizon, vertical, n_class = (8, 8, 10)  # 8 x 8 image, 0~9 number(=10 class)\n    size = 128  # state vector size\n    batch_size = 128\n    images = digits.images \/ np.max(digits.images)  # simple normalization\n    target = np.array([[1 if t == i else 0 for i in range(n_class)] for t in digits.target])  # to 1 hot vector\n    learning_rate = 0.001\n    train_iter = 1000\n    summary_dir = os.path.dirname(\".\/summary\")\n\n    with tf.name_scope(\"placeholder\"):\n        X = tf.placeholder(tf.float32, [batch_size, vertical, horizon])\n        y = tf.placeholder(tf.float32, [batch_size, n_class])\n\n    if qrnn > 0:\n        pred = qrnn_forward(X, size, n_class, batch_size, conv_size=qrnn)\n        summary_dir += \"\/qrnn\"\n    elif baseline:\n        pred = baseline_forward(X, size, n_class)\n        summary_dir += \"\/lstm\"\n    else:\n        pred = random_forward(X, size, n_class)            \n        summary_dir += \"\/random\"\n        \n    with tf.name_scope(\"optimization\"):\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n\n    with tf.name_scope(\"evaluation\"):\n        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    with tf.name_scope(\"summary\"):\n        tf.summary.scalar(\"loss\", loss)\n        tf.summary.scalar(\"accuracy\", accuracy)\n        merged = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(summary_dir, graph)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(train_iter):\n            indices = np.random.randint(len(digits.target) - batch_size, size=batch_size)\n            _X = images[indices]\n            _y = target[indices]\n            sess.run(optimizer, feed_dict={X: _X, y: _y})\n\n            if i % 100 == 0:\n                _loss, _accuracy, _merged = sess.run([loss, accuracy, merged], feed_dict={X: _X, y: _y})\n                writer.add_summary(_merged, i)\n                print(\"Iter {}: loss={}, accuracy={}\".format(i, _loss, _accuracy))\n            \n        with tf.name_scope(\"test-evaluation\"):\n            acc = sess.run(accuracy, feed_dict={X: images[-batch_size:], y: target[-batch_size:]})\n            print(\"Testset Accuracy={}\".format(acc))\n    \n\nwith timer(\"QRNN\"):\n    with tf.Graph().as_default() as qrnn:\n        check_by_digits(qrnn, qrnn=5)\n    \nwith timer(\"LSTM\"):\n    with tf.Graph().as_default() as ltsm:\n        check_by_digits(ltsm, baseline=True)\n        \n","9b610d50":"This kernel illustrates a basic Tensorflow implementation of the [Quasi-Recurrent Neural Networks](https:\/\/arxiv.org\/abs\/1611.01576).\n\nThe original implementation from the authors is based on [PyTorch](https:\/\/github.com\/salesforce\/pytorch-qrnn) but was challenging to run on GPU kernel (because it requires some libraries like cupy).\n\nThe code below has been based on [this](https:\/\/github.com\/icoxfog417\/tensorflow_qrnn\/) implementation.\n\nThe dataset used here is the UCI ML hand-written digits datasets from scikit-learn. "}}