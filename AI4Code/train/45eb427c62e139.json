{"cell_type":{"9952e3b3":"code","f4428ddd":"code","01f2ede2":"code","29fed80a":"code","138dbd98":"code","2f014401":"code","c0cf2f65":"code","c5ff5fa2":"code","9561b321":"code","73f8534c":"code","a259ae1a":"markdown","a0f33082":"markdown","9a2a8707":"markdown","5a6ce871":"markdown","8ee92bad":"markdown","3562ae01":"markdown"},"source":{"9952e3b3":"# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex3 import *\nprint(\"Setup Complete\")","f4428ddd":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('..\/input\/train.csv', index_col='Id') \nX_test = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","01f2ede2":"X_train.head()","29fed80a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","138dbd98":"# Columns that will be one-hot encoded\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == 'object']\n\nnumerical_cols = [col for col in X_train.columns if X_train[col].dtype != 'object']\n\nlow_cardinality_cols = [col for col in X_train.columns if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nlabel_mean = y_train.mean()\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be ordinal encoded', high_cardinality_cols)\nprint('\\nColumns that will not be encoded', numerical_cols)\nprint('\\nThe average house price: ', label_mean)\n\n","2f014401":"from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Creating model pipeline\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\n\noneHotEncoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nordinalEncoder = OrdinalEncoder()\n\nnumerical_transformer = SimpleImputer(strategy='mean')\n\nlow_card_transformer = Pipeline(steps=[\n    ('imputer', categorical_imputer),\n    ('one-hot-encoder', oneHotEncoder)\n])\n\nhigh_card_transformer = Pipeline(steps=[\n    ('imputer', categorical_imputer),\n    ('ordinal_encoder', ordinalEncoder) \n])\n\ncolumnTransformer = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('low_card', low_card_transformer, low_cardinality_cols),\n    ('high_card', high_card_transformer, high_cardinality_cols)\n])","c0cf2f65":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators=2700, learning_rate=0.0078)\n\npipeline = Pipeline(steps=[\n    ('preprocessing', columnTransformer),\n    ('model', model)\n])\ncolumnTransformer.fit(X_train)\npipeline.fit(X_train, y_train, model__verbose=True, model__early_stopping_rounds=30, \n             model__eval_set=[(columnTransformer.transform(X_train), y_train)], \n            model__eval_metric=['mae'])\n","c5ff5fa2":"mae_error = mean_absolute_error(y_valid, pipeline.predict(X_valid))\nprint(\"Mean Abosulte Error from test data in Model Pipeline (XGBoost):\")\nprint(mae_error)\nprint(\"Accuracy from test data in Model Pipeline (XGBoost):\")\nprint((1 - mae_error\/label_mean) * 100)","9561b321":"model = RandomForestRegressor(n_estimators=100)\nmodel.fit(OH_X, y)\npreds = model.predict(OH_X_test)\n\noutput = pd.DataFrame({'Id': OH_X_test.index, 'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","73f8534c":"# (Optional) Your code here","a259ae1a":"\n\n# Setup\n\n","a0f33082":"Run the next code cell to get the MAE for this approach.","9a2a8707":"Use the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. ","5a6ce871":"We display the first five rows of the data.","8ee92bad":"In this exercise, we will work with data from the Housing Prices Competition \n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)\n\nThe next code cell loads the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.","3562ae01":"Notice that the dataset contains both numerical and categorical variables.  We'll need to encode the categorical data before training a model."}}