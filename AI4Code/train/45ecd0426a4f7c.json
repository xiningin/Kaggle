{"cell_type":{"89dd60c5":"code","100984ac":"code","800faa84":"code","6c7a41c8":"code","c862f433":"code","396a72a7":"code","49d4c7e3":"code","99a897c4":"markdown","4f688324":"markdown","58579255":"markdown","8bc93761":"markdown","a6d92aa0":"markdown","38f60906":"markdown","a636d374":"markdown","11132976":"markdown"},"source":{"89dd60c5":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Veriyi oku\ndata = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\n\n# Tahmin edicilerin alt k\u00fcmesini se\u00e7in \ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\n\n# Hedef se\u00e7in\ny = data.Price\n\n# Verileri e\u011fitim ve do\u011frulama k\u00fcmelerine ay\u0131r\u0131n \nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","100984ac":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, y_train)","800faa84":"from sklearn.metrics import mean_absolute_error\n\npredictions = my_model.predict(X_valid)\nprint(\"Ortalama Mutlak Hata (MAE): \" + str(mean_absolute_error(predictions, y_valid)))","6c7a41c8":"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train)","c862f433":"my_model = XGBRegressor(n_estimators=500)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)],\n             verbose=False)","396a72a7":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","49d4c7e3":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","99a897c4":"Ayr\u0131ca tahminlerde bulunup ve modeli de\u011ferlendirece\u011fiz. ","4f688324":"Bu kursta, **XGBoost** ile modelleri nas\u0131l olu\u015fturaca\u011f\u0131n\u0131z\u0131 ve optimize edece\u011finizi \u00f6\u011freneceksiniz. Bu y\u00f6ntem bir\u00e7ok Kaggle yar\u0131\u015fmas\u0131na h\u00fckmediyor ve \u00e7e\u015fitli veri k\u00fcmelerinde son teknoloji sonu\u00e7lar elde ediyor.\n\n# Giri\u015f\n\nBu kursun \u00e7o\u011fu i\u00e7in, bir\u00e7ok karar a\u011fac\u0131n\u0131n tahminlerinin ortalamas\u0131n\u0131 alarak tek bir karar a\u011fac\u0131ndan daha iyi performans sa\u011flayan rastgele orman y\u00f6ntemiyle tahminler yapt\u0131n\u0131z.\n\nRastgele orman y\u00f6ntemini \"topluluk y\u00f6ntemi\" olarak adland\u0131r\u0131yoruz. Tan\u0131m olarak, **topluluk y\u00f6ntemleri** birka\u00e7 modelin (\u00f6rne\u011fin, rastgele ormanlar durumunda birka\u00e7 a\u011fa\u00e7) tahminlerini birle\u015ftirir.\n\nArd\u0131ndan, XGBoost verilen ba\u015fka bir topluluk y\u00f6ntemi hakk\u0131nda bilgi edinece\u011fiz.\n\n# XGBoost\n\n**XGBoost**, modelleri bir toplulu\u011fa yinelemeli olarak eklemek i\u00e7in d\u00f6ng\u00fclerden ge\u00e7en bir y\u00f6ntemdir.\n\nToplulu\u011fu, tahminleri olduk\u00e7a naif olabilen tek bir modelle ba\u015flatarak ba\u015flar. (Tahminleri \u00e7\u0131lg\u0131nca yanl\u0131\u015f olsa bile, toplulu\u011fa sonraki eklemeler bu hatalar\u0131 giderecektir.)\n\nArd\u0131ndan d\u00f6ng\u00fcy\u00fc ba\u015flat\u0131yoruz:\n- \u0130lk olarak, veri setindeki her g\u00f6zlem i\u00e7in tahminler olu\u015fturmak i\u00e7in mevcut toplulu\u011fu kullan\u0131r\u0131z. Bir tahmin yapmak i\u00e7in, topluluktaki t\u00fcm modellerden gelen tahminleri ekliyoruz.\n- Bu tahminler, bir kay\u0131p fonksiyonunu hesaplamak i\u00e7in kullan\u0131l\u0131r (\u00f6rne\u011fin, [ortalama kare hatas\u0131](https:\/\/tr.xcv.wiki\/wiki\/Mean_squared_error) gibi).\n- Ard\u0131ndan, toplulu\u011fa eklenecek yeni bir modele uymas\u0131 i\u00e7in kay\u0131p fonksiyonunu kullan\u0131yoruz. Spesifik olarak, bu yeni modeli toplulu\u011fa eklemek kayb\u0131 azaltacak \u015fekilde model parametrelerini belirliyoruz.\n- Son olarak, yeni modeli toplulu\u011fa ekliyoruz ve ...\n- ... tekrar et!\n\n![resim](https:\/\/i.hizliresim.com\/bclikwj.png)\n\n\n# \u00d6rnek\n\nE\u011fitim ve do\u011frulama verilerini `X_train`, `X_valid`, `y_train`, ve `y_valid`'e y\u00fckleyerek ba\u015fl\u0131yoruz. ","58579255":"### `early_stopping_rounds`\n\n`early_stopping_rounds`, `n_estimators` i\u00e7in ideal de\u011feri otomatik olarak bulman\u0131n bir yolunu sunar. Erken durdurma, `n_estimators` i\u00e7in zor durakta olmasak bile, do\u011frulama puan\u0131n\u0131n iyile\u015fmesi durdu\u011funda modelin yinelemeyi durdurmas\u0131na neden olur. `n_estimators` i\u00e7in y\u00fcksek bir de\u011fer ayarlamak ve ard\u0131ndan yinelemeyi durdurmak i\u00e7in en uygun zaman\u0131 bulmak i\u00e7in `early_stopping_rounds` kullanmak ak\u0131ll\u0131cad\u0131r.\n\nRastgele \u015fans bazen do\u011frulama puanlar\u0131n\u0131n iyile\u015fmedi\u011fi tek bir tura neden oldu\u011fundan, durmadan \u00f6nce ka\u00e7 tane d\u00fcz bozulmaya izin verilece\u011fi i\u00e7in bir say\u0131 belirtmeniz gerekir. `early_stopping_rounds=5`\" ayar\u0131 makul bir se\u00e7imdir. Bu durumda, 5 d\u00fcz rauntluk k\u00f6t\u00fcle\u015fen do\u011frulama puanlar\u0131ndan sonra dururuz.\n\n`early_stopping_rounds`u kullan\u0131rken, do\u011frulama puanlar\u0131n\u0131 hesaplamak i\u00e7in baz\u0131 verileri de ay\u0131rman\u0131z gerekir - bu, `eval_set` parametresini ayarlayarak yap\u0131l\u0131r.\n\nYukar\u0131daki \u00f6rne\u011fi erken durdurmay\u0131 i\u00e7erecek \u015fekilde de\u011fi\u015ftirebiliriz: ","8bc93761":"Daha sonra t\u00fcm verilerinizle bir model fit etmek isterseniz, erken durdurma ile \u00e7al\u0131\u015ft\u0131rd\u0131\u011f\u0131n\u0131zda en uygun buldu\u011funuz de\u011fere `n_estimators`'\u0131 ayarlay\u0131n.\n\n### `learning_rate`\n\nHer bir bile\u015fen modelinden tahminleri basit\u00e7e toplayarak tahminler almak yerine, eklemeden \u00f6nce her modelden gelen tahminleri k\u00fc\u00e7\u00fck bir say\u0131yla (**\u00f6\u011frenme oran\u0131** olarak bilinir) \u00e7arpabiliriz.\n\nBu, toplulu\u011fa ekledi\u011fimiz her a\u011fac\u0131n bize daha az yard\u0131mc\u0131 oldu\u011fu anlam\u0131na gelir. B\u00f6ylece, fazla overfitting yapmadan `n_estimators` i\u00e7in daha y\u00fcksek bir de\u011fer belirleyebiliriz. Erken durdurma kullan\u0131rsak uygun a\u011fa\u00e7 say\u0131s\u0131 otomatik olarak belirlenir.\n\nGenel olarak, k\u00fc\u00e7\u00fck bir \u00f6\u011frenme oran\u0131 ve \u00e7ok say\u0131da tahmin edici, daha do\u011fru XGBoost modelleri verecektir, ancak ayn\u0131 zamanda, d\u00f6ng\u00fc boyunca daha fazla yineleme yapt\u0131\u011f\u0131 i\u00e7in modelin e\u011fitilmesi daha uzun s\u00fcrecektir. Varsay\u0131lan olarak, XGBoost `learning_rate=0.1` de\u011ferini ayarlar.\n\nLearning_rate'i yani \u00f6\u011frenme oran\u0131n\u0131 de\u011fi\u015ftirmek i\u00e7in kod: ","a6d92aa0":"Bu \u00f6rnekte, xgboost k\u00fct\u00fcphanesi ile \u00e7al\u0131\u015faca\u011f\u0131z. **XGBoost**, performans ve h\u0131za odaklanan \u00e7e\u015fitli ek \u00f6zelliklerle birlikte gradyan art\u0131rman\u0131n bir uygulamas\u0131 olan **a\u015f\u0131r\u0131 gradyan art\u0131rma** anlam\u0131na gelir. (_Scikit-learn, degrade art\u0131rman\u0131n ba\u015fka bir s\u00fcr\u00fcm\u00fcne sahiptir, ancak XGBoost'un baz\u0131 teknik avantajlar\u0131 vard\u0131r._)\n\nSonraki kod h\u00fccresinde, XGBoost i\u00e7in scikit-learn API'sini i\u00e7e aktar\u0131yoruz ([`xgboost.XGBRegressor`](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn) ). Bu, t\u0131pk\u0131 scikit-learn'de yapt\u0131\u011f\u0131m\u0131z gibi bir model olu\u015fturmam\u0131za ve fit etmemize izin verir. \u00c7\u0131kt\u0131da g\u00f6rece\u011finiz gibi, `XGBRegressor` s\u0131n\u0131f\u0131n\u0131n bir\u00e7ok ayarlanabilir parametresi var. ","38f60906":"### `n_jobs`\n\u00c7al\u0131\u015fma zaman\u0131n\u0131n \u00f6nemli oldu\u011fu daha b\u00fcy\u00fck veri k\u00fcmelerinde, modellerinizi daha h\u0131zl\u0131 olu\u015fturmak i\u00e7in paralelli\u011fi kullanabilirsiniz. `n_jobs` parametresini makinenizdeki \u00e7ekirdek say\u0131s\u0131na e\u015fit olarak ayarlamak yayg\u0131n bir uygulamad\u0131r. Daha k\u00fc\u00e7\u00fck veri k\u00fcmelerinde bu yard\u0131mc\u0131 olmaz.\n\nOrtaya \u00e7\u0131kan model daha iyi olmayacak, bu nedenle montaj s\u00fcresi i\u00e7in mikro optimizasyon, genellikle dikkat da\u011f\u0131tmaktan ba\u015fka bir \u015fey de\u011fildir. Ancak, aksi takdirde `fit` komutu s\u0131ras\u0131nda uzun s\u00fcre bekleyece\u011finiz b\u00fcy\u00fck veri k\u00fcmelerinde kullan\u0131\u015fl\u0131d\u0131r.\n\n\u0130\u015fte de\u011fi\u015ftirilmi\u015f \u00f6rnek: ","a636d374":"# Sonu\u00e7\n\n[XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/), standart tablo verileriyle \u00e7al\u0131\u015fmak i\u00e7in \u00f6nde gelen bir yaz\u0131l\u0131m k\u00fct\u00fcphanesidir. Dikkatli parametre ayarlama ile son derece hassas modelleri e\u011fitebilirsiniz. \n\n# Devam et\n\n**[Veri s\u0131z\u0131nt\u0131s\u0131](https:\/\/www.kaggle.com\/bolsogansiz\/veri-s-z-nt-s)** hakk\u0131nda bilgi edinmeye devam edin. Bu, bir veri bilimcisinin anlamas\u0131 gereken \u00f6nemli bir konudur ve modellerinizi ince ve tehlikeli \u015fekillerde mahvetme potansiyeline sahiptir! ","11132976":"# Parametre Ayarlama\n\nXGBoost, do\u011frulu\u011fu ve e\u011fitim h\u0131z\u0131n\u0131 \u00f6nemli \u00f6l\u00e7\u00fcde etkileyebilecek birka\u00e7 parametreye sahiptir. Anlaman\u0131z gereken ilk parametreler \u015funlard\u0131r:\n\n### `n_estimators`\n`n_estimators`, yukar\u0131da a\u00e7\u0131klanan modelleme d\u00f6ng\u00fcs\u00fcnden ka\u00e7 kez ge\u00e7ilece\u011fini belirtir. Toplulu\u011fa dahil etti\u011fimiz modellerin say\u0131s\u0131na e\u015fittir.\n\n- \u00c7ok _d\u00fc\u015f\u00fck bir de\u011fer, _underfitting_'e neden olur, bu da hem e\u011fitim verileri hem de test verileri \u00fczerinde yanl\u0131\u015f tahminlere yol a\u00e7ar.\n- \u00c7ok _y\u00fcksek_ bir de\u011fer, e\u011fitim verilerinde do\u011fru tahminlere, ancak test verilerinde yanl\u0131\u015f tahminlere neden olan _overfitting_'e neden olur (_\u00f6nemsedi\u011fimiz \u015fey budur_).\n\nTipik de\u011ferler 100-1000 aras\u0131ndad\u0131r, ancak bu, a\u015fa\u011f\u0131da tart\u0131\u015f\u0131lan `learning_rate`  parametresine \u00e7ok ba\u011fl\u0131d\u0131r.\n\nTopluluktaki model say\u0131s\u0131n\u0131 ayarlamak i\u00e7in kod: "}}