{"cell_type":{"01bcf08d":"code","f462551f":"code","433141bf":"code","eea1c101":"code","37e83fc3":"code","df46807a":"code","f39c0055":"code","61c1d76c":"code","dad444a0":"code","835c6243":"code","c2ac8cae":"code","f4d18d89":"code","f0edf30c":"code","78ebc06d":"code","3e63a279":"code","c32fdef3":"code","febfc27a":"code","f8ad1550":"code","96b3d1ee":"code","7c760324":"code","f8737ac1":"code","27a2d643":"code","68e1c74c":"code","dac59771":"code","cdc7beb0":"code","17af28a6":"code","b9ce9c6b":"code","0a4d0f59":"code","42a1895e":"markdown","e3c0f1b6":"markdown","1bcb24fe":"markdown","d0f6f277":"markdown","41ec0da0":"markdown","8cf49afc":"markdown","bf916fff":"markdown","b70dad5a":"markdown","93d9c14f":"markdown","623dd9a7":"markdown","df908654":"markdown","5a7ba9a1":"markdown","e4c9ac61":"markdown","8be1eb07":"markdown","e3be6674":"markdown","87f40f78":"markdown","086f9876":"markdown","d809fff4":"markdown","b3bbbca6":"markdown"},"source":{"01bcf08d":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport shap\n\nfrom scipy.stats import chi2_contingency\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import f_classif,mutual_info_classif,SelectKBest,chi2, SelectFromModel","f462551f":"# using datatable for faster loading\n\ntrain = dt.fread(r'..\/input\/tabular-playground-series-nov-2021\/train.csv').to_pandas()\ntest = dt.fread(r'..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas()","433141bf":"print(train.info())\nprint(test.info())\n\n# The train data has 600k entries, target being boolean and rest 100 continuous variables","eea1c101":"train.describe()\n\n# on first look, f2 seems different compared to others, in terms of range.","37e83fc3":"train.head()\n\n# different distributions of columns, it seems. We may need to scale later.","df46807a":"# no missing values in the datasets\n\nprint('missing values in Train data: ',train.isna().sum().sum())\nprint('missing values in Test data: ',test.isna().sum().sum())","f39c0055":"# checking for duplicates in the data\n\nprint('number of duplicates in train: ',len(train.drop_duplicates())-len(train))\nprint('number of duplicates in test: ',len(test.drop_duplicates())-len(test))","61c1d76c":"train.nunique().sort_values(ascending=True)\n\n# no categorical variables other than our target - but might explore binning them later.","dad444a0":"# variables have low correlation with the target (though not as low as we saw in TPS October)\n\ntrain.corr()['target'].sort_values(ascending=False)","835c6243":"# checking if the variables are correlated with each other - they are NOT\n\nsns.set(rc = {'figure.figsize':(12,8)})\nsns.heatmap(train.corr())","c2ac8cae":"# plotting our target variable - very balanced\n\nsns.countplot(train['target'])","f4d18d89":"# plotting all the features' distribution\n# 3 types - bimodal, spiked, right-skewed\n# Maybe scaling needed and outliers need to be dealt with\n\ncolumns = 10\nrows = 10\nf=0\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\nfor i,ax_row in enumerate(ax_array):\n    for j,axes in enumerate(ax_row):\n        axes.set_title('f'+str(f))\n        col = 'f'+str(f)\n        sns.set(rc = {'figure.figsize':(14,14)})\n        g2 = sns.kdeplot(train[col],ax=axes)\n        g2.set(ylabel=None)\n        g2.set(xticklabels=[])\n        g2.set(yticklabels=[])\n        f=f+1\nplt.show()","f0edf30c":"# seeing which features may have outliers - spiked and skewed ones have plenty\n\ncolumns = 10\nrows = 10\nf=0\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\nfor i,ax_row in enumerate(ax_array):\n    for j,axes in enumerate(ax_row):\n        axes.set_title('f'+str(f))\n        axes.set_yticklabels([])\n        axes.set_xticklabels([])\n        col = 'f'+str(f)\n        sns.set(rc = {'figure.figsize':(14,14)})\n        g2 = sns.boxplot(train[col],ax=axes)\n        g2.set(ylabel=None)\n        g2.set(xticklabels=[])\n        g2.set(yticklabels=[])\n        f=f+1\nplt.show()","78ebc06d":"# using selectkbest to get top 30 features - f_classif\n\nX = train.drop(['id','target'],axis=1)\ny = train['target']\n\nselector = SelectKBest(score_func=f_classif,k=30)\nselector.fit(X,y)\n\nmask = selector.get_support()\nnew_features = [] # The list of your K best features\n\nfor bool, feature in zip(mask, X.columns):\n    if bool:\n        new_features.append(feature)\n        \nprint(new_features)","3e63a279":"# top 30 variables as per absolute correlation\n\nvars = pd.DataFrame(np.abs(train.drop(['id'],axis=1).corr()['target']).sort_values(ascending=False).head(31)).index.to_list()\nprint(vars)","c32fdef3":"plt.figure(figsize=(10,4))\ntrain.var().sort_values(ascending=True).head(10).plot(kind='bar')\n\n# f73 and f21 have very low variance, but f21 seems an important variable","febfc27a":"plt.figure(figsize=(10,4))\ntrain.apply(lambda x: np.std(x)\/np.mean(x)).sort_values(ascending=False).head(10).plot(kind='bar',color='g')\n\n# all are spike\/skew variables , with high COV compared to others.","f8ad1550":"# from borutashap (no feature engineering done)\n\nluca_variables = set(['f1', 'f10', 'f11', 'f14', 'f15', 'f16', 'f17', 'f2', 'f20', 'f21', 'f22','f24',\n'f25', 'f26', 'f27', 'f28', 'f3', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f4', 'f40', 'f41',\n'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f5','f50', 'f51', 'f53', 'f54', 'f55', 'f57', \n'f58', 'f59', 'f60', 'f61', 'f62', 'f64', 'f66','f67', 'f70', 'f71', 'f76','f77', 'f8', 'f80', 'f81',\n'f82', 'f83', 'f87', 'f89', 'f9', 'f90', 'f91', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98'])\n\n# from my selectkbest (top 30)\n\nmy_features_f_classif = set(['f3','f8','f10','f17','f21','f22','f24','f25','f26','f27','f34',\n               'f40','f41','f43','f44','f47','f50','f54','f55','f57','f60','f66',\n               'f71','f80','f81','f82','f91','f96','f97','f98'])\n\n# from lofo notebook (taking top 30)\n\nlofo_features = set(['f34','f55','f8','f43','f91','f71','f80','f27','f50','f41','f97','f66','f57',\n                'f22','f25','f96','f81','f82','f21','f24','f26','f54','f98','f40','f60','f3','f17',\n                'f95','f5','f45'])\n\n# top 30 as per absolute correlation with target\n\ncor_features = set(['f34', 'f55', 'f43', 'f71', 'f80', 'f91', 'f8', 'f27', 'f97', 'f50', 'f41', 'f57',\n                    'f25', 'f22', 'f66', 'f96', 'f81', 'f82', 'f21', 'f40', 'f24', 'f60', 'f98', 'f3',\n                    'f54', 'f44', 'f26', 'f47', 'f17', 'f10'])","96b3d1ee":"# common useful features - 30 in number, as per my 2 methods\n\nuseful_features = my_features_f_classif | cor_features\nprint(useful_features)","7c760324":"# using columns of original dataset\n\ncolumns = train.drop(['id','target'],axis=1).columns","f8737ac1":"train['mean'] = train[columns].mean(axis=1)\ntrain['sum'] = train[columns].sum(axis=1)\ntrain['min'] = train[columns].min(axis=1)\ntrain['max'] = train[columns].max(axis=1)\ntrain['std'] = train[columns].std(axis=1)\ntrain['var'] = train[columns].var(axis=1)","27a2d643":"# these features are not adding any value it seems, so will drop them for now\n\nnp.abs(train.corr()['target']).sort_values(ascending=False).head(10)","68e1c74c":"# I may change number of clusters and the features used to get to them later\n# Their distribution plots look nice - Gaussian type, but I may be wrong in first impressions :P\n\nn_clusters = 6\ncluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters)]\nkmeans = KMeans(n_clusters=n_clusters, n_init=50, max_iter=500, random_state=42)\n\nX_cd = kmeans.fit_transform(train[useful_features])\nX_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=train.index)\ntrain = train.join(X_cd)\n\nfig = plt.figure(figsize = (10,5))\nsns.kdeplot(data=train[cluster_cols])\n\nplt.show()","dac59771":"cluster_cols.append('target')\ntrain[cluster_cols].corr()\n\n# no significant correlation with the target, will check mutual information later","cdc7beb0":"from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nsc = RobustScaler() # Scaling Data for PCA\n\nX_scaled = sc.fit_transform(X)\npca = PCA().fit(X_scaled)\n\nplt.rcParams[\"figure.figsize\"] = (20,6)\nfig, ax = plt.subplots()\nxi = np.arange(1, 101, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 101, step=1))\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.90, '95% cut-off threshold', color = 'red', fontsize=16)\nplt.axhline(y=0.99, color='g', linestyle='-')\nplt.text(0.5, 1.0, '99% cut-off threshold', color = 'green', fontsize=16)\n\nax.grid(axis='x')\nplt.tight_layout()\nplt.show()","17af28a6":"# As seen earlier, f14 has the least unique values among all.\n# Now, it may not be categorical at all, but I wanted to check how it may look binned\n# Original and 50 bins look identical...\n\ncolumns = 4\nrows = 1\nplt.rcParams[\"figure.figsize\"] = (12,4)\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\ndata = pd.DataFrame(train['f14'])\ndata['Binned_10'] = pd.cut(data['f14'],bins=10,labels=False)\ndata['Binned_20'] = pd.cut(data['f14'],bins=20,labels=False)\ndata['Binned_50'] = pd.cut(data['f14'],bins=50,labels=False)\n\nax_array[0][0].set_title('Original')\nsns.kdeplot(data['f14'],ax=ax_array[0][0])\nax_array[0][1].set_title('10 Bins')\nsns.kdeplot(data['Binned_10'],ax=ax_array[0][1])\nax_array[0][2].set_title('20 Bins')\nsns.kdeplot(data['Binned_20'],ax=ax_array[0][2])\nax_array[0][3].set_title('50 Bins')\nsns.kdeplot(data['Binned_50'],ax=ax_array[0][3])","b9ce9c6b":"# I am taking f2 (skewed) for experiments\n# Log Transformation might work for skewed - BUT HAVE TO THINK OF DEALING WITH NEGATIVE VALUES\n# BoxCox and Square Root will fail because data has to be positive\n# exp fails because it leads to infinitely high values\n# Has outliers, so I can go for Robust Scaling\n\ncolumns = 4\nrows = 1\nplt.rcParams[\"figure.figsize\"] = (12,4)\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\ndata = train['f2']\n\nax_array[0][0].set_title('Original')\nsns.kdeplot(data,ax=ax_array[0][0])\nax_array[0][1].set_title('Logged')\nsns.kdeplot(np.log1p(data),ax=ax_array[0][1]) # used 1p to deal with zeroes\nax_array[0][2].set_title('Reciprocal')\nsns.kdeplot(1\/data,ax=ax_array[0][2])\nax_array[0][3].set_title('Square Root')\nsns.kdeplot(data**0.5,ax=ax_array[0][3])","0a4d0f59":"# I am taking f1 (bimodal) for relevant transformations\n# square root looks decent. There are no outliers in these variables, hence no robust scaling needed.\n\ncolumns = 4\nrows = 1\nplt.rcParams[\"figure.figsize\"] = (12,4)\nfig, ax_array = plt.subplots(rows, columns, squeeze=False)\ndata = train['f1']\n\nax_array[0][0].set_title('Original')\nsns.kdeplot(data,ax=ax_array[0][0])\nax_array[0][1].set_title('Logged')\nsns.kdeplot(np.log1p(data),ax=ax_array[0][1]) # used 1p to deal with zeroes\nax_array[0][2].set_title('Exponential')\nsns.kdeplot(np.exp(data),ax=ax_array[0][2])\nax_array[0][3].set_title('Square Root')\nsns.kdeplot(data**0.5,ax=ax_array[0][3])","42a1895e":"<div style=\"background-color:rgba(15, 159, 21, 0.5);\">\n    <h1><center>Importing Libraries and Data<\/center><\/h1>\n<\/div>","e3c0f1b6":"<div style=\"background-color:rgba(15, 159, 21, 0.5);\">     \n    <h1><center>Feature Selection<\/center><\/h1>\n<\/div>","1bcb24fe":"<div style=\"background-color:rgba(15, 159, 21, 0.5);\">     <h1><center>Basic EDA<\/center><\/h1>\n<\/div>","d0f6f277":"# Binning Continuous Variables","41ec0da0":"<div style=\"background-color:rgba(15, 159, 21, 0.5);\">\n    <h1><center>Basic Data Check<\/center><\/h1>\n<\/div>","8cf49afc":"# PCA Check","bf916fff":"Credits to the following beautiful notebook by Bex - https:\/\/www.kaggle.com\/bextuychiev\/model-explainability-with-shap-only-guide-u-need\/notebook\n\nI have also used the following one from Luca as reference - https:\/\/www.kaggle.com\/lucamassaron\/feature-selection-by-boruta-shap\n\nFollowing is a good notebook on LOFO - https:\/\/www.kaggle.com\/frankmollard\/lofo-importance-correlations-tps-nov-21\n\nI am doing a simple SelectKBest (30 variables) in this data and then taking common ones from the above methods, to see what variables truly stand out.\n\nWill update if I add another set and once I try **mutual information with my engineered variables**","b70dad5a":"# Variance and Coefficient of Variance Check","93d9c14f":"<div style=\"background-color:rgba(15, 159, 21, 0.5);\">     \n    <h1><center>Feature Engineering Ideas<\/center><\/h1>\n<\/div>","623dd9a7":"# SelectKBest","df908654":"PCA should be used mainly for variables which are strongly correlated. If the relationship is weak between variables, PCA does not work well to reduce data. In general, if most of the correlation coefficients are smaller than 0.3, PCA will not help. It might not help in our case too then.\n\nAs per the plot below, if we consider going ahead with PCA and explain **95% variance**-\n1. Standard Scaling - we can take **94-95 components**\n2. Robust Scaling - we can take **42-43 components**\n3. MinMax Scaling - we can take **44-45 components**\n\nThe current plot is set for MinMax. I think Standard Scaling won't help much here, if we do want to reduce the number of features going into a model.\n","5a7ba9a1":"# Selection","e4c9ac61":"<div style=\"background-color:rgba(15, 159, 21, 0.5);\">     \n    <h1><center>Summary<\/center><\/h1>\n<\/div>","8be1eb07":"# Absolute Correlation","e3be6674":"I have referred to the following notebook by Kaveh - https:\/\/www.kaggle.com\/kavehshahhosseini\/tps-oct-2021-pca-and-kmeans-feature-eng\/notebook","87f40f78":"I have the following take-aways from this exercise-\n\n1. I will have to **check MI scores** to determine whether clustering and PCAs will help or not.\n2. **Robust Scaling** the skewed variables should be ok.\n3. Standard or MinMax Scaling or **taking square root** (if no negatives) of the bimodal variables.\n4. Binning will not help here I think.\n5. Creating row-wise stats will also not help much. Will wait for MI scores in my first few models.\n\nNext up-\n1. Create **basic models with feature engineering** ideas picked from here.\n2. Have **read about NNs** doing really well in this competition - will learn that.\n3. MI scores have to be checked once I have the complete cluster and PCA data added to my train.","086f9876":"# Creating Clusters","d809fff4":"# Transforming the Bimodal and Skewed Variables","b3bbbca6":"# Creating variables using Row-wise Statistics"}}