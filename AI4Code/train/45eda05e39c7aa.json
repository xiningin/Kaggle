{"cell_type":{"b2305dc2":"code","4eeaffea":"code","9deae287":"code","8af4d849":"code","13d9db3b":"code","ac4a2895":"code","fce5df6d":"code","d85e965a":"code","6daf3aa2":"code","c13c1935":"code","97948a39":"code","8bd45910":"code","8ff45420":"code","cc3d2300":"code","0245f550":"code","d31f2cb5":"code","bf273b36":"code","f03bd97b":"code","cb10d3ab":"code","63940bea":"code","c449bae0":"code","7a05f1a3":"code","0ac2febf":"code","1ce22d3f":"code","44f4e32e":"code","21f56399":"code","824b9f7b":"code","f5596c71":"code","151a9a3f":"code","c8f8d118":"code","ebf0b87d":"code","84873f70":"code","ff6f43c5":"markdown","139000c6":"markdown","06904a8c":"markdown","ca3f4fb2":"markdown","2c134e06":"markdown","8352152b":"markdown","35a561a6":"markdown","303f457c":"markdown","c9cfedfb":"markdown","9a508d17":"markdown","75b15451":"markdown","3b225a25":"markdown","819b6bcb":"markdown","473fa13d":"markdown","18208769":"markdown","8adb0f88":"markdown","d58e8730":"markdown","1eadcd0f":"markdown","8e9ba1f3":"markdown","9aea49ff":"markdown","106d10d7":"markdown","701963c5":"markdown","e57f1bb3":"markdown","de69f674":"markdown","75472bcc":"markdown","a4e601a7":"markdown","b5fa4812":"markdown","822d86ae":"markdown","c532450e":"markdown","450cdb60":"markdown","e3c7ba3e":"markdown","aa999509":"markdown","ff07193f":"markdown","a8a9378e":"markdown","bb377582":"markdown","ee516a38":"markdown","c883c79a":"markdown","9701cff0":"markdown","3d080a56":"markdown","375f40f6":"markdown","72d7bb7a":"markdown","2e7a9847":"markdown","c23c4439":"markdown","38212aba":"markdown","b913104d":"markdown","32056406":"markdown","77ca68d4":"markdown","d43caa74":"markdown","39bbb94f":"markdown","1526d4f6":"markdown","fdeeb762":"markdown","d4218912":"markdown","8f0eeb71":"markdown","3d96ab4b":"markdown"},"source":{"b2305dc2":"# data processing and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# algorithm\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n# training\u8bad\u7ec3\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score","4eeaffea":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","9deae287":"sns.distplot(train.SalePrice)","8af4d849":"sns.distplot(np.log(train.SalePrice + 1))","13d9db3b":"all_data = pd.concat((train.drop([\"SalePrice\"], axis=1), test))\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nplt.figure(figsize=(12, 6))\nplt.xticks(rotation=\"90\")\nsns.barplot(x=all_data_na.index, y=all_data_na)","ac4a2895":"all_data[all_data.PoolArea != 0][[\"PoolArea\", \"PoolQC\"]]","fce5df6d":"all_data[all_data.MiscVal > 10000][[\"MiscFeature\", \"MiscVal\"]]","d85e965a":"all_data[(all_data.GarageType.notnull()) & (all_data.GarageYrBlt.isnull())][[\"Neighborhood\", \"YearBuilt\", \"YearRemodAdd\", \"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"GarageCars\", \"GarageArea\", \"GarageQual\", \"GarageCond\"]]","6daf3aa2":"train.loc[[332, 948]][[\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinSF1\", \"BsmtFinType2\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]]","c13c1935":"test.loc[[27, 580, 725, 757, 758, 888, 1064]][[\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinSF1\", \"BsmtFinType2\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]]","97948a39":"plt.scatter(train.Utilities, train.SalePrice)","8bd45910":"y = train[\"SalePrice\"]\ny = np.log(y+1)","8ff45420":"# PoolQC\ntest.loc[960, \"PoolQC\"] = \"Fa\"\ntest.loc[1043, \"PoolQC\"] = \"Gd\"\ntest.loc[1139, \"PoolQC\"] = \"Fa\"\n \n# Garage\ntest.loc[666, \"GarageYrBlt\"] = 1979\ntest.loc[1116, \"GarageYrBlt\"] = 1979\n \ntest.loc[666, \"GarageFinish\"] = \"Unf\"\ntest.loc[1116, \"GarageFinish\"] = \"Unf\"\n \ntest.loc[1116, \"GarageCars\"] = 2\ntest.loc[1116, \"GarageArea\"] = 480\n \ntest.loc[666, \"GarageQual\"] = \"TA\"\ntest.loc[1116, \"GarageQual\"] = \"TA\"\n \ntest.loc[666, \"GarageCond\"] = \"TA\"\ntest.loc[1116, \"GarageCond\"] = \"TA\"","cc3d2300":"# PoolQC\ntrain = train.fillna({\"PoolQC\": \"None\"})\ntest = test.fillna({\"PoolQC\": \"None\"})\n \n# Alley\ntrain = train.fillna({\"Alley\": \"None\"})\ntest = test.fillna({\"Alley\": \"None\"})\n \n# FireplaceQu\ntrain = train.fillna({\"FireplaceQu\": \"None\"})\ntest = test.fillna({\"FireplaceQu\": \"None\"})\n \n# LotFrontage\ntrain = train.fillna({\"LotFrontage\": 0})\ntest = test.fillna({\"LotFrontage\": 0})\n \n# Garage\ntrain = train.fillna({\"GarageType\": \"None\"})\ntest = test.fillna({\"GarageType\": \"None\"})\ntrain = train.fillna({\"GarageYrBlt\": 0})\ntest = test.fillna({\"GarageYrBlt\": 0})\ntrain = train.fillna({\"GarageFinish\": \"None\"})\ntest = test.fillna({\"GarageFinish\": \"None\"})\ntest = test.fillna({\"GarageCars\": 0})\ntest = test.fillna({\"GarageArea\": 0})\ntrain = train.fillna({\"GarageQual\": \"None\"})\ntest = test.fillna({\"GarageQual\": \"None\"})\ntrain = train.fillna({\"GarageCond\": \"None\"})\ntest = test.fillna({\"GarageCond\": \"None\"})\n \n# Bsmt\ntrain = train.fillna({\"BsmtQual\": \"None\"})\ntest = test.fillna({\"BsmtQual\": \"None\"})\ntrain = train.fillna({\"BsmtCond\": \"None\"})\ntest = test.fillna({\"BsmtCond\": \"None\"})\ntrain = train.fillna({\"BsmtExposure\": \"None\"})\ntest = test.fillna({\"BsmtExposure\": \"None\"})\ntrain = train.fillna({\"BsmtFinType1\": \"None\"})\ntest = test.fillna({\"BsmtFinType1\": \"None\"})\ntrain = train.fillna({\"BsmtFinSF1\": 0})\ntest = test.fillna({\"BsmtFinSF1\": 0})\ntrain = train.fillna({\"BsmtFinType2\": \"None\"})\ntest = test.fillna({\"BsmtFinType2\": \"None\"})\ntest = test.fillna({\"BsmtFinSF2\": 0})\ntest = test.fillna({\"BsmtUnfSF\": 0})\ntest = test.fillna({\"TotalBsmtSF\": 0})\ntest = test.fillna({\"BsmtFullBath\": 0})\ntest = test.fillna({\"BsmtHalfBath\": 0})\n \n# MasVnr\ntrain = train.fillna({\"MasVnrType\": \"None\"})\ntest = test.fillna({\"MasVnrType\": \"None\"})\ntrain = train.fillna({\"MasVnrArea\": 0})\ntest = test.fillna({\"MasVnrArea\": 0})\n \n# MiscFeature,Fence,Utilities\ntrain = train.drop([\"Fence\", \"MiscFeature\", \"Utilities\"], axis=1)\ntest = test.drop([\"Fence\", \"MiscFeature\", \"Utilities\"], axis=1)\n \n# other\ntest = test.fillna({\"MSZoning\": \"RL\"})\ntest = test.fillna({\"Exterior1st\": \"VinylSd\"})\ntest = test.fillna({\"Exterior2nd\": \"VinylSd\"})\ntrain = train.fillna({\"Electrical\": \"SBrkr\"})\ntest = test.fillna({\"KitchenQual\": \"TA\"})\ntest = test.fillna({\"Functional\": \"Typ\"})\ntest = test.fillna({\"SaleType\": \"WD\"})","0245f550":"train_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[: train.shape[0]]\ntest_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[train.shape[0]:]","d31f2cb5":"rr = Ridge(alpha=10)\nrr.fit(train_dummies, y)\nnp.sqrt(-cross_val_score(rr, train_dummies, y, cv=5, scoring=\"neg_mean_squared_error\")).mean()","bf273b36":"y_pred = rr.predict(train_dummies)\nresid = y - y_pred\nmean_resid = resid.mean()\nstd_resid = resid.std()\nz = (resid - mean_resid) \/ std_resid\nz = np.array(z)\noutliers1 = np.where(abs(z) > abs(z).std() * 3)[0]\noutliers1","f03bd97b":"plt.figure(figsize=(6, 6))\nplt.scatter(y, y_pred)\nplt.scatter(y.iloc[outliers1], y_pred[outliers1])\nplt.plot(range(10, 15), range(10, 15), color=\"red\")","cb10d3ab":"er = ElasticNet(alpha=0.001, l1_ratio=0.58)\ner.fit(train_dummies, y)\nnp.sqrt(-cross_val_score(rr, train_dummies, y, cv=5, scoring=\"neg_mean_squared_error\")).mean()","63940bea":"y_pred = er.predict(train_dummies)\nresid = y - y_pred\nmean_resid = resid.mean()\nstd_resid = resid.std()\nz = (resid - mean_resid) \/ std_resid\nz = np.array(z)\noutliers2 = np.where(abs(z) > abs(z).std() * 3)[0]\noutliers2","c449bae0":"plt.figure(figsize=(6, 6))\nplt.scatter(y, y_pred)\nplt.scatter(y.iloc[outliers2], y_pred[outliers2])\nplt.plot(range(10, 15), range(10, 15), color=\"red\")","7a05f1a3":"outliers = []\nfor i in outliers1:\n    for j in outliers2:\n        if i == j:\n            outliers.append(i)\noutliers","0ac2febf":"train = train.drop([30, 88, 142, 277, 328, 410, 462, 495, 523, 533, 581, 588, 628, 632, 681, 688, 710, 714, 728, 774, 812, 874, 898, 916, 968, 970, 1181, 1182, 1298, 1324, 1383, 1423, 1432, 1453])\ny = train[\"SalePrice\"]\ny = np.log(y+1)","1ce22d3f":"train_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[: train.shape[0]]\ntest_dummies = pd.get_dummies(pd.concat((train.drop([\"SalePrice\", \"Id\"], axis=1), test.drop([\"Id\"], axis=1)), axis=0)).iloc[train.shape[0]:]","44f4e32e":"gbr = GradientBoostingRegressor(max_depth=4, n_estimators=150)\ngbr.fit(train_dummies, y)\nnp.sqrt(-cross_val_score(gbr, train_dummies, y, cv=5, scoring=\"neg_mean_squared_error\")).mean()","21f56399":"xgbr = XGBRegressor(max_depth=5, n_estimators=400)\nxgbr.fit(train_dummies, y)\nnp.sqrt(-cross_val_score(xgbr, train_dummies, y, cv=5, scoring=\"neg_mean_squared_error\")).mean()","824b9f7b":"lsr = Lasso(alpha=0.00047)\nlsr.fit(train_dummies, y)\nnp.sqrt(-cross_val_score(lsr, train_dummies, y, cv=5, scoring=\"neg_mean_squared_error\")).mean()","f5596c71":"rr = Ridge(alpha=13)\nrr.fit(train_dummies, y)\nnp.sqrt(-cross_val_score(rr, train_dummies, y, cv=5, scoring=\"neg_mean_squared_error\")).mean()","151a9a3f":"train_predict = 0.1 * gbr.predict(train_dummies) + 0.3 * xgbr.predict(train_dummies) + 0.3 * lsr.predict(train_dummies) + 0.3 * rr.predict(train_dummies)","c8f8d118":"plt.figure(figsize=(6, 6))\nplt.scatter(y, train_predict)\nplt.plot(range(10, 15), range(10, 15), color=\"red\")","ebf0b87d":"q1 = pd.DataFrame(train_predict).quantile(0.0042)\npre_df = pd.DataFrame(train_predict)\npre_df[\"SalePrice\"] = train_predict\npre_df = pre_df[[\"SalePrice\"]]\npre_df.loc[pre_df.SalePrice <= q1[0], \"SalePrice\"] = pre_df.loc[pre_df.SalePrice <= q1[0], \"SalePrice\"] *0.99\ntrain_predict = np.array(pre_df.SalePrice)\nplt.figure(figsize=(6, 6))\nplt.scatter(y, train_predict)\nplt.plot(range(10, 15), range(10, 15), color=\"red\")","84873f70":"test_predict = 0.1 * gbr.predict(test_dummies) + 0.3 * xgbr.predict(test_dummies) + 0.3 * lsr.predict(test_dummies) + 0.3 * rr.predict(test_dummies)\nq1 = pd.DataFrame(test_predict).quantile(0.0042)\npre_df = pd.DataFrame(test_predict)\npre_df[\"SalePrice\"] = test_predict\npre_df = pre_df[[\"SalePrice\"]]\npre_df.loc[pre_df.SalePrice <= q1[0], \"SalePrice\"] = pre_df.loc[pre_df.SalePrice <= q1[0], \"SalePrice\"] *0.96\ntest_predict = np.array(pre_df.SalePrice)\nsample_submission[\"SalePrice\"] = np.exp(test_predict)-1\nsample_submission.to_csv(\"1.csv\", index=False)","ff6f43c5":"**4. It is known from Titanic that it is useful to create some new features. Many of the kernels in the house price have also created some new features, but it is not useful for this model.**","139000c6":"**Missing values for the remaining features are filled with mode values**","06904a8c":"**MasVnrArea**\uff1a**0**","ca3f4fb2":"**LotFrontage**\uff1aThis is the object that many Kernels focus on, such as predicted by a useful algorithm, group by Neighborhood and filled with MICE. After my observation,** I think the more reasonable filling method is filled with LotConfig and Neighborhood**, but unfortunately, these filling methods have no effect on my model. **What really works for this model is 0**.","2c134e06":"**Use Ridge to find outliers**","8352152b":"**GBDT**","35a561a6":"***feature engineering***","303f457c":"You will see that the bottom point is not on the red line like the top point, indicating that the bottom point may not be predictive. You can manually adjust it and use the quantile to select the predicted value you want to adjust and adjust it. The parameters of the quantile and the multiples of the resizing can be adjusted to find the parameters that will give the best score.","c9cfedfb":"**FireplaceQu**\uff1aAccording to the file, you can use **None to fill** in missing values.","9a508d17":"**2\u3001Special example missing value filling**","75b15451":"dummies","3b225a25":"**Utilities**\uff1aThere is no NoSeWa in the test set, basically no effect on SalePrice, **delete**.","819b6bcb":"**Lasso**","473fa13d":"**1. It is very important to fill in missing values, which can effectively improve the score. You can try different filling methods. For example, the filling of three special missing value examples of PoolQC can try different combinations.**","18208769":"**3\u3001Missing value filling**","8adb0f88":"***Load Data***","d58e8730":"**There are many ways to fill in missing values. You can try more, explore more, and find the best filling method for your model.**","1eadcd0f":"**Fence**\uff1aAs MiscFeature, **delete**.","8e9ba1f3":"**Manually modify the predicted value** (Refer to [Top 10 (0.10943): stacking, MICE and brutal force](https:\/\/www.kaggle.com\/agehsbarg\/top-10-0-10943-stacking-mice-and-brutal-force), very effective)","9aea49ff":"**The point where the two algorithms predict poor results is used as the outlier**","106d10d7":"**Combined model** (I also tried stacking, which can improve the score, but the effect is not ideal)","701963c5":"The output format here is a bit problematic, and the outliers are manually deleted later.","e57f1bb3":"**PoolQC**\uff1aPoolQC has too many missing values. According to the file, there is no swimming pool. You can use **None to fill in missing values**.In theory, the PoolArea corresponding to the missing value of PoolQC should be 0, but after observing a few PoolAreas, it is found that the test set **960, 1043, 1139** are very special. These three examples cannot be filled with None.Of the 13 data, Ex appeared 4 times, Gd appeared 4 times, and Fa appeared 2 times. According to the idea of average distribution, **there are two missing values that need to be filled with Fa**.","de69f674":"***Module Imports***","75472bcc":"**2. The exploration of outliers, the outliers proposed above are not the best for this model, you can try to add new outliers, or reduce the old outliers, there will be some outlier combination for you to increase your score again.**","a4e601a7":"**Explore outliers with ElasticNet**","b5fa4812":"**Predict and submit**","822d86ae":"**Delete outliers**","c532450e":"**2\u3001Missing value**\n\uff08refer to [Top 2% of LeaderBoard - Advanced FE](https:\/\/www.kaggle.com\/laurenstc\/top-2-of-leaderboard-advanced-fe)\uff09","450cdb60":"The cross-validation score is metaphysical. Sometimes the score is increased here, but it will fall after the submission; the score is dropped here, but it is increased after the submission.","e3c7ba3e":"**3. The search for redundant features, that is the first time I built the simplest model, I found it by chance. At that time, I manually tried which feature is good, which feature is bad, and found two bad features, which have been used until the end and surprisingly useful all the time. Deleting these two features will increase the score of this model again.**","aa999509":"**MiscFeature**\uff1a**delete**. According to the file, you can use None to fill in missing values.I also explored the relationship between MiscFeature and its corresponding MiscVal and GarageType, tried some filling methods, and even determined that the example in the test set 1089 should be filled with Gar2, but eventually deleting the feature works better for my model.","ff07193f":"It can be seen that SalePrice deviates from the normal distribution.We can correct this with a simple log transformation.","a8a9378e":"*Splicing data and visualizing the number of missing values*","bb377582":"**Observe the prediction effect of our model on the training set**","ee516a38":"If you are Chinese, you can check out my [blog](https:\/\/blog.csdn.net\/qq_33758867\/article\/details\/87794848) .","c883c79a":"dummies","9701cff0":"**1\u3001correct SalePrice with a simple log transformation.**","3d080a56":"**1\u3001SalePrice distribution**","375f40f6":"**Bsmt**\uff1aAccording to the file, **fill with None or 0**. There are also some special examples below that should not be filled like this, I have tried other filling methods, but in the end I chose to fill with None or 0.","72d7bb7a":"**Garage**\uff1aAccording to the file, **GarageType,GarageFinish,GarageQual,GarageCond fill None, GarageYrBlt,GarageCars,GarageArea fill 0**\u3002But by observing, **there are some unusual examples in the test set that can't be filled like this. It will be filled with median and mode**. Of course, I also tried the filling method that I think is more reasonable, for example, by filling with Neighborhood and GarageType, but the effect is not ideal.","2e7a9847":"**MasVnrType**\uff1a**None**","c23c4439":"***data analysis***","38212aba":"**4\u3001Explore outliers and delete**(The method of exploring outliers is based on [Top 7% using ElasticNet with Interactions](https:\/\/www.kaggle.com\/jack89roberts\/top-7-using-elasticnet-with-interactions). I used Ridge and ElasticNet to train the training set and predict the training set. Find out the samples with unpredictable prediction results in the two algorithms as outliers)","b913104d":"you can see that I finally chose a multiple of 0.96, which allowed me to achieve a good score. This submission can get a score of 0.11052, reaching top 2%.","32056406":"***summarize***","77ca68d4":"**Finally, delete the outliers *[30, 88, 410, 462, 495, 523, 588, 628, 632, 874, 898, 968, 970, 1182, 1298, 1324, 1432]*, delete the features *LandSlope, Exterior2nd* allows this model reached 0.10955, top 1%.**","d43caa74":"**Alley**\uff1aAccording to the file, you can use **None to fill** in missing values.","39bbb94f":"**Ridge**","1526d4f6":"**MSZoning**\uff1a**Fill with mode value RL**. I also tried to fill by MSSubClass, but it didn't work.","fdeeb762":"**XGB**","d4218912":"***Modeling*** (Use GBDT, XGBOOST, Lasso, Ridge, and combine them)","8f0eeb71":"This is a combination of prediction models for the training set. The training set is first combined with the prediction model because I am going to manually modify the predicted values.","3d96ab4b":"**5. Skew and PCA which many kernels mentioned are not useful for this model.**"}}