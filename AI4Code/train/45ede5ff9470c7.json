{"cell_type":{"e11e4b76":"code","e6a7fd0e":"code","5ff24389":"code","f214ab9d":"code","2037588c":"code","9751ee10":"code","94c304d9":"code","82ad4d18":"code","3b727eee":"code","b576cb27":"code","4248aa61":"code","858ad93d":"code","c28ca5b9":"code","d6662516":"code","51863922":"code","23c75b2d":"code","6acabbc6":"code","89149142":"code","1251eb6e":"code","5825b033":"markdown","d821b819":"markdown","399d1b4e":"markdown","64af0de8":"markdown","ecfc7679":"markdown"},"source":{"e11e4b76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n##Let's import necessary libraries and import the dataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nforest_train_data = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv',index_col='Id')\nforest_test_data = pd.read_csv('\/\/kaggle\/input\/learn-together\/test.csv',index_col='Id')\nforest_train_data.head()\n# Any results you write to the current directory are saved as output.","e6a7fd0e":"forest_train_data.dtypes","5ff24389":"forest_test_data.dtypes","f214ab9d":"#next check the range\nforest_train_data.describe()","2037588c":"forest_test_data.describe()","9751ee10":"forest_train_data.columns","94c304d9":"forest_test_data.columns","82ad4d18":"# Get names of columns with missing values\ntrain_cols_with_missing = [col for col in forest_train_data.columns\n                     if forest_train_data[col].isnull().any()]\ntest_cols_with_missing = [col for col in forest_test_data.columns\n                     if forest_test_data[col].isnull().any()]\n\nprint(\"Missing columns in Training Data: \",train_cols_with_missing)\nprint(\"Missing columns in Test Data: \",test_cols_with_missing)\n\n","3b727eee":"forest_train_data['Mean_Hillshade']=forest_train_data['Hillshade_9am']+ forest_train_data['Hillshade_Noon']+forest_train_data['Hillshade_3pm']\/3\nforest_test_data['Mean_Hillshade']=forest_test_data['Hillshade_9am']+ forest_test_data['Hillshade_Noon']+forest_test_data['Hillshade_3pm']\/3\n\nforest_train_data['Mean_Hydrology']=forest_train_data['Horizontal_Distance_To_Hydrology']+ forest_train_data['Vertical_Distance_To_Hydrology']\/2\nforest_test_data['Mean_Hydrology']=forest_test_data['Horizontal_Distance_To_Hydrology']+ forest_test_data['Vertical_Distance_To_Hydrology']\/2\n\n","b576cb27":"forest_train_data.head()","4248aa61":"#Checking the frequency of soil type\nfor i in range(1,41):\n    print(\"Soil Type \"+str(i)+\"   \"+str(forest_train_data[\"Soil_Type\"+str(i)].sum()))","858ad93d":"#removing all the features with low frequency\nforest_train_data.drop([ 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology','Soil_Type7','Soil_Type8','Soil_Type9','Soil_Type15','Soil_Type19','Soil_Type25','Soil_Type26','Soil_Type27','Soil_Type28','Soil_Type34','Soil_Type36','Soil_Type37','Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm'],axis=1,inplace=True)\nforest_test_data.drop([ 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology','Soil_Type7','Soil_Type8','Soil_Type9','Soil_Type15','Soil_Type19','Soil_Type25','Soil_Type26','Soil_Type27','Soil_Type28','Soil_Type34','Soil_Type36','Soil_Type37','Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm'],axis=1,inplace=True)","c28ca5b9":"\n'''\nforest_train_data['Soil'] = (forest_train_data.iloc[:, 14:54] == 1).idxmax(1)\nforest_train_data['Soil'].unique()\n\nforest_test_data['Soil']=(forest_test_data.iloc[:,14:]==1).idxmax(1)\n\ntemp_train = forest_train_data\ntemp_test = forest_test_data\n\nsoil_OH_cols = [col for col in forest_train_data.iloc[:,14:54].columns]\nforest_train_data.drop(soil_OH_cols,axis=1,inplace=True)\nforest_test_data.drop(soil_OH_cols,axis=1,inplace=True)\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encode = LabelEncoder()\nforest_train_data['Soil_Type'] = label_encode.fit_transform(forest_train_data['Soil'])\nforest_test_data['Soil_Type'] = label_encode.fit_transform(forest_test_data['Soil'])\n\nforest_train_data.drop('Soil',axis=1,inplace=True)\nforest_test_data.drop('Soil',axis=1,inplace=True)\n\n'''\n###Pull Cover_Type in the end\ntemp_cover_type = forest_train_data['Cover_Type']\nforest_train_data.drop('Cover_Type',axis=1,inplace=True)\nforest_train_data['Cover_Type']=temp_cover_type\nforest_train_data.head()\nforest_train_data.shape","d6662516":"for i in range(1,5):\n    print(str(forest_train_data[\"Wilderness_Area\"+str(i)].sum()))","51863922":"'''# to avoid dummy trap\nforest_train_data.drop(['Wilderness_Area2'],axis=1,inplace=True)\nforest_test_data.drop(['Wilderness_Area2'],axis=1,inplace=True)\n'''","23c75b2d":"from sklearn.ensemble  import RandomForestClassifier\n\nmodel1 = RandomForestClassifier(n_estimators=50,random_state=0)\nmodel2 = RandomForestClassifier(n_estimators=100,random_state=0)\nmodel3 = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)\nmodel4 = RandomForestClassifier(n_estimators=200,min_samples_split=20,random_state=0)\nmodel5 = RandomForestClassifier(n_estimators=100,max_depth=7,random_state=0)\n\nmodels = [model1,model2,model3,model4,model5]","6acabbc6":"X = forest_train_data.iloc[:,:-1]\ny = forest_train_data.Cover_Type\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,random_state=1)\n\nfrom sklearn.metrics import mean_absolute_error\n\ndef best_model(iowa_model,x_train=x_train,x_test=x_test,y_train=y_train,y_test=y_test):\n    iowa_model.fit(x_train,y_train)\n    predictions = iowa_model.predict(x_test)\n    err=mean_absolute_error(y_test,predictions)\n    ax1 = sns.distplot(y_test, hist=False, color=\"r\", label=\"Actual Value\")\n    sns.distplot(predictions, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\n    plt.title('Actual vs Fitted Values for Price')\n    plt.xlabel('y')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()\n    return err\nmin_model_mae = 100\nbest_model_name = ''\nfor i in range(0,len(models)):\n    best_model_mae = best_model(models[i])\n    print(\"model MAE: \",best_model_mae)\n    if best_model_mae < min_model_mae:\n        min_model_mae = best_model_mae\n        best_model_name = i\nprint(\"best model is: \",best_model_name)","89149142":"#predict using best model\niowa_model = models[best_model_name]\niowa_model.fit(X,y)\npredictions = iowa_model.predict(forest_test_data)","1251eb6e":"output = pd.DataFrame({'Id':forest_test_data.index,\n                      'Cover_Type':predictions\n                      })\noutput.to_csv('submission.csv',index=False)","5825b033":"Attempt to convert One Hot Encoding to Label Encoding. But doesn't improve the score","d821b819":"Okay, so all the features have expected data types.","399d1b4e":"check if all features have expected datatypes","64af0de8":"Checking for missing values","ecfc7679":"Calculating Mean of Hillshade"}}