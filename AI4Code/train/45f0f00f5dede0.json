{"cell_type":{"f34f4e1d":"code","a6cbe25e":"code","e7c5a93e":"code","5428c0db":"code","253dcb90":"code","12580135":"code","f8a00d17":"code","2663741c":"code","02796516":"code","8f5fc52b":"code","fa44b406":"code","18798f14":"code","eb4f8950":"code","fa8e445f":"code","f4a40c3d":"code","8c4344ee":"code","8bba2100":"code","d1c67dc2":"code","050c9f45":"code","2d032d7e":"code","ba39fc9c":"code","86e7aa09":"code","e04bd59c":"code","374dfa92":"code","b71dad50":"code","15ca0b77":"markdown","c1d301f1":"markdown","99ecd91a":"markdown","58b7054d":"markdown","dec6a6a5":"markdown","99865ae3":"markdown","79ba79ec":"markdown","57ea40f5":"markdown","402dd946":"markdown","fef80a5d":"markdown","c1681f50":"markdown","8f79c9c0":"markdown","46f82326":"markdown","41e0cfcb":"markdown","1aa8ecaa":"markdown","9cf93c12":"markdown","21620a75":"markdown","f5baf57b":"markdown"},"source":{"f34f4e1d":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Scikit-Learn modules\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import KernelPCA, PCA\nfrom sklearn.model_selection import  train_test_split, GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.pipeline import Pipeline\n\n# others\nfrom skimage.measure import block_reduce","a6cbe25e":"%matplotlib inline\n_ = plt.style.use('ggplot')\n_ = plt.style.available","e7c5a93e":"molecule_data = pd.read_csv('..\/input\/roboBohr.csv', header=0, dtype=np.float64, usecols=list(range(1,1276))+[1277])\nmolecule_data.head()","5428c0db":"pre_X = molecule_data.drop(columns=['Eat'])\nzero_mask_X = (pre_X==0)\nprint(\"{0:.2f} % of cells were actually padded zero\"\n      .format(100.0 * zero_mask_X.values.flatten().sum() \/ (pre_X.shape[0]*pre_X.shape[1])))\nprint(\"--- --- --- \")\nprint(\"Turning them into np.nan\")\npre_X[zero_mask_X] = np.nan\nprint(\"DONE!\")\nprint(\"--- --- --- \")\nX = StandardScaler().fit_transform(pre_X)\nprint(\"Scaling finished, slice of new feature data\")\nprint(X[:8])\nprint('--- --- --- ')\nprint('Target values:')\ny = molecule_data['Eat'].values\nprint(y)","253dcb90":"# let us learn some stats and info about dataset\nprint(\"There are {} entries with {} features\".format(X.shape[0], X.shape[1]))\nprint(\"--- --- --- \")\nprint(\"The statistical information about each feature (column)\")\nmolecule_stats = pd.DataFrame(X).describe()\nprint(molecule_stats)","12580135":"feature_indices = [0,1,2,300,500,700,-3,-2,-1]\nchosen_features = ([ pd.DataFrame(X[:,i]).dropna().values for i in feature_indices])\n### PLOTTING time ###\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(111)\nax.set_yscale('linear')\nax.set_xlabel('Value of feature')\nax.set_ylabel('Feature: top-rightmost, bottom-leftmost')\nax.tick_params(bottom=False, left=False, labelleft=False)\n# print(chosen_features)\n_ = ax.boxplot(chosen_features, showmeans=True,showcaps=True, showfliers=True,vert=False)","f8a00d17":"np.random.seed(0)\ndist_gauss = np.random.normal(size=1000)\ndist_gamma = np.random.gamma(shape=1.5, scale=1.5,size=1000)\ndist_exp = np.random.exponential(size=1000)\n\nfig = plt.figure(figsize=(10,5))\nwith plt.xkcd():\n    plt.rcParams.update({'font.size':'10'})\n    ax_gauss = fig.add_subplot(131)\n    ax_gauss.tick_params(left=False, labelleft=False)\n    ax_gamma = fig.add_subplot(132)\n    ax_gamma.tick_params(left=False, labelleft=False)\n    ax_exp = fig.add_subplot(133)\n    ax_exp.tick_params(left=False, labelleft=False)\n    ax_gauss.hist(dist_gauss,density=True,bins=17)\n    ax_gauss.set_title(\"Normal(Gauss) distribution\")\n    ax_gamma.hist(dist_gamma,density=True,bins=17)\n    ax_gamma.set_title(\"Gamma distribution\")\n    ax_exp.hist(dist_exp,density=True,bins=17)\n    ax_exp.set_title(\"Exponential distribution\")\n\n    ","2663741c":"##############################\n### Put missing values back ### \n##############################\n\nX[zero_mask_X] = 0\n\n###########\n### PCA ###\n###########\n# try different number of components and see how well they explain variance\nN_PCA=50\np = PCA(n_components=N_PCA).fit(X)\nns = list(range(N_PCA))\n\nplt.figure()\nplt.plot(ns, [ p.explained_variance_ratio_[n] for n in ns], \n         'r+', label=\"Explained variance - single feature\")\nplt.plot(ns, [ p.explained_variance_ratio_.cumsum()[n] for n in ns], \n         'b*', label=\"Explained variance - cumulative\")\n_ = plt.legend()","02796516":"# from analyzing the graph we can see that about 25 components\n# are enough to explain 96% variation in data. We can keep them\n\n# Another thing that grinds my gears\n# how well does new PCs explain energy levels?\nX_reduced = p.transform(X)[:,:25]\nplt.style.use('grayscale')\nfig = plt.figure(figsize=(10,10))\naxs = fig.subplots(5,5,sharex=True,sharey=True)\naxs = np.array(axs).flatten()\nfor ax in axs[[0,5,10,15,20]]:\n    ax.set_ylabel(\"Energy Level\")\nfor i in range(25):\n    ax = axs[i]\n    ax.scatter(X_reduced[:,i],y,s=0.1, alpha=0.2)\n    ax.set_xlabel(\"PC-{}\".format(i+1), labelpad=2)\n    ax.tick_params(left=False, bottom=False)","8f5fc52b":"plt.style.use('ggplot')\nplt.set_cmap(\"magma\")\nplt.figure(figsize=(8,8))\nplt.xlabel(\"PC 1\")\nplt.ylabel(\"PC 2\")\nplt.tick_params(left=False, bottom=False, labelbottom=False, labelleft=False)\n_ = plt.scatter(X_reduced[:,0], X_reduced[:,1], c=y,s=5,alpha=0.2)","fa44b406":"##################\n### KERNEL PCA ###\n##################\n\nmask_random = np.random.randint(0,X.shape[0],size=2000)\nkp = KernelPCA(n_components=100,kernel='sigmoid', gamma=0.5, max_iter=250).fit(X[mask_random])\nprint(\"PCA is trained on kernel\")\nprint('--- --- --- ')\nX_reduced = kp.transform(X)[:,:50]\nplt.style.use('grayscale')\nfig = plt.figure(figsize=(16,8))\naxs = fig.subplots(5,10,sharex=True, sharey=True)\naxs = np.array(axs).flatten()\nfor i in range(50):\n    ax = axs[i]\n    ax.scatter(X_reduced[:,i],y,s=0.1, alpha=0.2)\n    ax.tick_params(left=False, bottom=False)","18798f14":"plt.style.use('ggplot')\nplt.set_cmap(\"magma\")\nplt.figure(figsize=(16,8))\nplt.subplot(121)\nplt.xlabel(\"PC 1\")\nplt.ylabel(\"PC 2\")\nplt.tick_params(left=False, bottom=False, labelbottom=False, labelleft=False)\n_ = plt.scatter(X_reduced[:,0], X_reduced[:,1], c=y,s=5,alpha=0.2)\n\nplt.subplot(122)\nplt.xlabel(\"PC 2\")\nplt.ylabel(\"PC 3\")\nplt.tick_params(left=False, bottom=False, labelbottom=False, labelleft=False)\n_ = plt.scatter(X_reduced[:,1], X_reduced[:,2], c=y,s=5,alpha=0.2)","eb4f8950":"X_train, X_test, y_train, y_test = train_test_split(X_reduced[:,:2], y)","fa8e445f":"R_clf = Ridge(alpha=10).fit(X_train, y_train)\nR_clf.score(X_test, y_test)","f4a40c3d":"# pretty embarassing, maybe try quadratic regression?\nR_clf = Ridge(alpha=100).fit(PolynomialFeatures(degree=3).fit_transform(X_train), y_train)\nR_clf.score(PolynomialFeatures(degree=3).fit_transform(X_test), y_test)","8c4344ee":"# much better but not promising, KNN still better\n# how about Decision Tree and a bit more components\nX_train, X_test, y_train, y_test = train_test_split(X_reduced[:,:10], y)","8bba2100":"dt_clf = DecisionTreeRegressor(max_depth=25).fit(X_train, y_train)\ndt_clf.score(X_test, y_test)","d1c67dc2":"#NICE!!! well maybe Linear models (with poly features) are good fit too\nR_clf = Ridge(alpha=0.05).fit(PolynomialFeatures(degree=3).fit_transform(X_train), y_train)\nR_clf.score(PolynomialFeatures(degree=3).fit_transform(X_test), y_test)","050c9f45":"# Not that bad, Ridge would probably have better generalization, however trees have another trick up their sleeve\n# RANDOM FORESTS, this is basically a way to overcome overfitting and have better generalization\nforest_reg = RandomForestRegressor(n_estimators=100,max_depth=30).fit(X_train, y_train)\nforest_reg.score(X_test, y_test)","2d032d7e":"# Seems like we are pushing it to limits and hitting wall, there is another Ensemble model for trees -> ExtremeTrees!!!\nex_tree = ExtraTreesRegressor(n_estimators=100, max_depth=22).fit(X_train, y_train)\nex_tree.score(X_test, y_test)","ba39fc9c":"# do not forget to seperate validation and test data\nX_inter, X_test, y_inter, y_test = train_test_split(X,y,test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_inter,y_inter,test_size=0.2)","86e7aa09":"# KIDDING! I will do everythin manually, changing every parameter by hand to get good accuracy\nN_BOOTSRAP = 2000\nGAMMA = 0.0005\nscores = []\nnp.random.seed(0)\nfor i in range(5):\n    X_train, X_val, y_train, y_val = train_test_split(X_inter,y_inter,test_size=0.2)\n    kp = (KernelPCA(n_components=10, kernel='sigmoid', gamma=GAMMA, max_iter=250)\n          .fit(X_train[np.random.randint(0,X_train.shape[0],size=N_BOOTSRAP)]))\n    X_train_reduce = kp.transform(X_train)\n    X_val_reduce = kp.transform(X_val)\n    ex_regr = ExtraTreesRegressor(n_estimators=100, max_depth=25).fit(X_train_reduce, y_train)\n    r2_score = ex_regr.score(X_val_reduce, y_val)\n    scores.append(r2_score)","e04bd59c":"print(\"Score on 5 different cross-vals:\\n{}\\n\".format(scores))\nprint(\"Average {}\".format(np.mean(scores)))","374dfa92":"%%time\nkp = (KernelPCA(n_components=10, kernel='sigmoid', gamma=0.0005, max_iter=250)\n       .fit(X_inter[np.random.randint(0,X_inter.shape[0],size=2000)]))\nX_train_reduce = kp.transform(X_inter)\nX_test_reduce = kp.transform(X_test)\nex_regr = ExtraTreesRegressor(n_estimators=100, max_depth=25).fit(X_train_reduce, y_inter)","b71dad50":"ex_regr.score(X_test_reduce, y_test)","15ca0b77":"## Time to calculate test score\n\nONE LAST TIME AND WE ARE DONE!","c1d301f1":"Since we scaled the data mean values are nearly zero (e-16) and variance is 1.  Seems like max values tend to be more on the outlier size than min values. To investigate it further, let us make boxplots of first and last three feature columns","99ecd91a":"Let us take a step back and look at the hidden structure that got discovered in the dataset simply applying a kernel.\n>**MESMERIZING!**\n\nAs we can see applying logistic kernel does not explain the variance as good as normal PCA (see the PC-20 in the plot, it is still trying to explain some variance).  However we do not really need variance, because gamma distribution is not well explained by variance anyway. From the look of plots, KernelPCA seems like better way of reduction. Enough pictures, time to actually learn...","58b7054d":"**Well we tried a lot today, and results are beautiful low dimensional structure and good regression learner. I think we can take a rest. Peace! **\n\nIf you find this work interesting, feel free to fork and tweak parameter and maybe build a proper pipeline and cross-validation procedure for easier reproducebility ","dec6a6a5":"\n~~> Why does it happen?~~\n\n~~It turns out, after skimming the article associated with dataset, because some molecules have atoms less than 50, extra 0s are padded (guess where, to rightmost features!). That is why the mean is close to zero while there is no **real** zero value feature. This also affects StandarScaler too. To overcome it, let us turn 0 values into NAN~~","99865ae3":"We will not create sklearn Pipeline, because that would make our KernelPCA perform full scale fitting which requires a lot (A LOT) of time. Instead I will create function chaining and pre-trained kernels and use them as a pipeline. Wish me good luck","79ba79ec":"Just as expected first few principal components produce more well defined shapes. The latter components essentially \"melts\" dataset into a grid where the energy level practically is not dependent on the value at all. Just because everyone does it let us make 2D heatmap where color is dependent on target value using our new axis. Also if you try hard with your imagination, it is possible to see PC1 and PC2 structures in the image","57ea40f5":"As we can see absolute values of cells do vary a lot. Rightmost values are essentially zero. For that reason Let us normalize the data. SKlearn StandarScaler is good one to help us here. It will center our data around 0-mean and variance of 1. \n\nPS. After playing with some data I learnt that there are padded 0s to data. If you want to see story of how I noticed it, simply comment out the line where I get rid of them and read the sentence I have striked through on one of next cells","402dd946":"This is a lot of variance which is unaccounted. But soon we will find out it is not that important.","fef80a5d":"# Visualization","c1681f50":"## Experimentation\n\nThis section I will just throw different ML eggs on the data wall and hope one sticks\n\nFor the first try I will be bold, and use only 2 PC components and fit a linear model. This should give us good idea how the model would work, if it goes well, we can build a pipeline, do gridsearch, crossval etc. to optimize the parameters, if not, there are many things to do, no worries. Just for the baseline, **KNN with 4 PC (non-kernel) has an R2 score of ..96!** This is the number we are trying to beat.\n\nPS. after little try-and-error I decided to give up on pipelining, and went on with manually doing things. ","8f79c9c0":"### KERNEL PCA\n\nI am not comfortable with using PCA as a hammer to everything. Particularly in this situation there is (kind of reason) to reject it. And also because I want to apply some kernels to dataset just because it is so cool. Beware not to apply to whole dataset though, because it is so expensive. Bootstrapping the dataset and fitting on it works fine and A LOT faster. Just a side note I use sigmoid, because it seems to work. Have tried rbf, but results were not good, and cosine pretty much gives the same results as sigmoid.","46f82326":"We see that first features (leftmost) follow sort of gamma distribution. There is little refresher on distributions on next cell. ~~Going more right stretches distribution more into exponential territory. For the rightmost one even can argue that there are two groups of values.~~ There fewer rightmost values which means less outliers\nThis actually can be a little problematic if we happen to use PCA dimensionality reduction. Main point of PCA is that it preserves variance in the data, however, that is not the case in gamma distribution. Nevertheless, we can still give it a try since it can still give satisfactoery results","41e0cfcb":"# Dimensionality reduction\n\n1275 is awful a lot, let us reduce it using Principal Component Analysis so curse of dimensionality does not annoy us","1aa8ecaa":"# Using data science to predict ground energy level\n\nPS. Sorry for rather ugly start of notebook and all those naughty terminal prints, they serve a good purpose, I promis. You will see hidden beauty in the end","9cf93c12":"HAHA!!! I do not know what do you call a model that performs better on test phase than validation phase (opposite of overfitting), but seems like we have created one of those. And it takes only 20 seconds to train. YAY!","21620a75":"# LET US Build ","f5baf57b":"**WOW** we have been able to increase to R2 score of .993, better than PCA+KNN. We have been able to squeeze few juice out of  trees in the forest. Extreme trees have slightly better performance because they take randomness one step further. The score we have *.993* is a bit too optimistic. During experimentation we ignored the fact that our KernelPCA chose points used all of X data to construct new components. This probably resulted in data leakage, because it has seen test data already. But we already suspect KernelPCA->RandomTrees is good candidate. So let us build a pipeline, and build model without leakage this time."}}