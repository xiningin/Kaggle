{"cell_type":{"cfaf02cb":"code","0cb79af9":"code","26a40bbc":"code","de828014":"code","e7b25346":"code","d328afe9":"code","f2f1b3bf":"code","0eaad097":"code","53b3917d":"code","97c8280f":"code","4df9d337":"code","79f4ae0d":"code","5f1c0ece":"code","e8305f5b":"code","ed9f1bf3":"code","348541a2":"code","6bd92837":"code","c05151bd":"code","962fd7dc":"code","bed4df6e":"code","d55478fc":"code","0efb7df6":"code","f06e9606":"code","7208a4aa":"markdown","c979d10f":"markdown","83aee881":"markdown","054bfd2a":"markdown"},"source":{"cfaf02cb":"\nimport numpy as np \nimport pandas as pd \nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense, Dropout\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0cb79af9":"BASE = '\/kaggle\/input\/wave2hackathon\/'\nkabini = pd.read_csv(BASE + '\/kabini.csv', parse_dates=True, index_col = \"FLOW_DATE\")\nKRS = pd.read_csv(BASE + '\/KRS.csv',parse_dates=True, index_col = \"FLOW_DATE\")\nhemavathi = pd.read_csv(BASE + '\/Hemavathi.csv', parse_dates=True, index_col = \"FLOW_DATE\")\nharangi = pd.read_csv(BASE + '\/Harangi.csv', parse_dates=True, index_col = \"FLOW_DATE\")","26a40bbc":"df = hemavathi.drop(['RESERVOIR' ,'SL_NO', 'UNIQUE_KEY'], axis=1)\n\ndf['INFLOW_CUSECS'] = df['INFLOW_CUSECS'].replace(['&nbsp;'],'0.0')\ndf['INFLOW_CUSECS'] = df['INFLOW_CUSECS'].astype(float)\ndf['RES_LEVEL_FT'] = df['RES_LEVEL_FT'].replace(['&nbsp;'],'0.0')\ndf['RES_LEVEL_FT'] =df['RES_LEVEL_FT'].astype(float)","de828014":"df.isnull().sum()","e7b25346":"df.dtypes","d328afe9":"df.head()","f2f1b3bf":"df['YEAR'].dtypes","0eaad097":"df_main = df[(df['YEAR'] >= 2011) & (df['YEAR'] < 2020)]","53b3917d":"print( df_main['YEAR'].unique(), df_main['YEAR'].max(), df_main['YEAR'].min())","97c8280f":"df_main.head()","4df9d337":"df_main.columns","79f4ae0d":"train_dates = pd.to_datetime(df_main.index)\n\n#Variables for training\ncols = list(df_main)[2:6]\n\ndf_for_training = df_main[cols].astype(float)\n\ndf_for_training.head()\n\n# df_for_plot=df_for_training.tail(5000)\n# df_for_plot.plot.line()\n\n#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n","5f1c0ece":"# normalize the dataset\nscaler = StandardScaler()\nscaler = scaler.fit(df_for_training)\ndf_for_training_scaled = scaler.transform(df_for_training)","e8305f5b":"cols = list(df_main)[2:6]\nprint(cols)","ed9f1bf3":"df_for_training.shape[1]","348541a2":"train_dates","6bd92837":"#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n#In this example, the n_features is 2. We will make timesteps = 3. \n#With this, the resultant n_samples is 5 (as the input data has 9 rows).\ntrainX = []\ntrainY = []\n\nn_future = 1   # Number of days we want to predict into the future\nn_past = 14     # Number of past days we want to use to predict the future\n\nfor i in range(n_past, len(df_for_training_scaled) - n_future +1):\n    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n\ntrainX, trainY = np.array(trainX), np.array(trainY)\n\nprint('trainX shape == {}.'.format(trainX.shape))\nprint('trainY shape == {}.'.format(trainY.shape))\n\n","c05151bd":"# define Autoencoder model\n\nmodel = Sequential()\nmodel.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\nmodel.add(LSTM(32, activation='relu', return_sequences=False))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(trainY.shape[1]))\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","962fd7dc":"\n# fit model\nhistory = model.fit(trainX, trainY, epochs=5, batch_size=16, validation_split=0.3, verbose=1, )\n\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()","bed4df6e":"n_future=2750  #Redefining n_future to extend prediction dates beyond original n_future dates...\nforecast_period_dates = pd.date_range(list(train_dates)[-1], periods=n_future, freq='1d').tolist()\n\nforecast = model.predict(trainX[-n_future:]) #forecast \n\n#Perform inverse transformation to rescale back to original range\n#Since we used 5 variables for transform, the inverse expects same dimensions\n#Therefore, let us copy our values 5 times and discard them after inverse transform\nforecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)\ny_pred_future = scaler.inverse_transform(forecast_copies)[:,0]\n","d55478fc":"df['FLOW_DATE'] =  pd.to_datetime(hemavathi.index)","0efb7df6":"df.head()","f06e9606":"# Convert timestamp to date\nforecast_dates = []\nfor time_i in forecast_period_dates:\n    forecast_dates.append(time_i.date())\n    \ndf_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'PRESENT_STORAGE_TMC':y_pred_future})\ndf_forecast['Date']=pd.to_datetime(df_forecast['Date'])\n\n\noriginal = df[['FLOW_DATE', 'PRESENT_STORAGE_TMC']]\noriginal['Date']=pd.to_datetime(original['FLOW_DATE'])\noriginal = original.loc[original['Date']]\n\nsns.lineplot(original['Date'], original['PRESENT_STORAGE_TMC'])\nsns.lineplot(df_forecast['Date'], df_forecast['PRESENT_STORAGE_TMC'])","7208a4aa":"## Pre model preparationsss","c979d10f":"## Splitting the dataset from 2011 to 2019. \n\n### Saving the data of year 2020 as unseen data { since the problem is for real time prediction we need to do this }\n\n#### Kabini dataset starts from the third month of 2011. (* that's a issue)\n\nrest are quite good","83aee881":"## dataset preprocessing","054bfd2a":"## Preparing Model "}}