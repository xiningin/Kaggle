{"cell_type":{"031f8ac6":"code","5d90a49a":"code","0b9cc670":"code","f2c8ed5c":"code","90d64b6c":"code","eeaa3a52":"code","7244cba9":"code","3dfe567a":"code","c43d2838":"code","55524be6":"code","39e0f00f":"code","2eb164e9":"code","475ff80f":"code","4afb51c9":"code","8b595d1d":"code","dc7ef3e2":"code","5186079d":"code","61f1f669":"code","f9e17b72":"code","0990fade":"code","337315a6":"code","74c5049e":"code","9f2b4ba2":"code","1aab29e9":"code","38f4f59c":"code","a9669b4d":"code","b74f7800":"code","cc291660":"code","b160e1ae":"code","be92069c":"code","05fec79b":"code","216cac52":"code","48d509a3":"markdown","6ec9436e":"markdown","8e1ce3ae":"markdown","40ce8d2e":"markdown","8bc72ba1":"markdown","eb9909b8":"markdown","14f0804c":"markdown","9996ec1d":"markdown","83cc54b3":"markdown","af994314":"markdown","d2c03f19":"markdown","b0f00b70":"markdown","70857d55":"markdown"},"source":{"031f8ac6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5d90a49a":"#Store train and test data in dataframes\nX_full =pd.read_csv('..\/input\/titanic\/train.csv')\nX_test_full = pd.read_csv('..\/input\/titanic\/test.csv')","0b9cc670":"#Look into how many nulls and datatypes\nX_full.info()","f2c8ed5c":"#Look at numerical columns' distributions\nX_full.describe()","90d64b6c":"#Create separate lists for numerical columns and categorical columns\nnumerical_cols = X_full[['Age','SibSp','Parch','Fare']]\ncategorical_cols = X_full[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","eeaa3a52":"#Create histograms for numeric variables to get sense of distribution\nfor col in numerical_cols.columns:\n    plt.hist(numerical_cols[col])\n    plt.title(col)\n    plt.show()","7244cba9":"#Look at average values for numerical variables that survived vs didn't to spot any trends\npd.pivot_table(X_full, index = 'Survived', values = numerical_cols)","3dfe567a":"pd.pivot_table(X_full, index = 'Survived', values = numerical_cols, aggfunc = 'median')","c43d2838":"#Plotting barcharts to check the distribution of the categorical variables\nfor col in categorical_cols.columns:\n    sns.barplot(categorical_cols[col].value_counts().index,categorical_cols[col].value_counts()).set_title(col)\n    plt.show()","55524be6":"#Finding counts for useful categorical variables that survived vs perished to notice any useful trends\nprint(pd.pivot_table(X_full, index = 'Survived', columns = 'Pclass', values = 'PassengerId', aggfunc = 'count'))\nprint(pd.pivot_table(X_full, index = 'Survived', columns = 'Sex', values = 'PassengerId', aggfunc = 'count'))\nprint(pd.pivot_table(X_full, index = 'Survived', columns = 'Embarked', values = 'PassengerId', aggfunc = 'count'))","39e0f00f":"#Let's take the first letter from Cabin to make it more useful(returns n if null)\n\nX_full['cabin_letter'] = X_full['Cabin'].apply(lambda x: str(x)[0])\npd.pivot_table(X_full, index = 'Survived', columns = 'cabin_letter', values = 'PassengerId', aggfunc = 'count')","2eb164e9":"#Let's create a column for whether the ticket has just a number (vs. letters and\/or numbers)\nX_full['numerical_ticket'] = X_full.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\npd.pivot_table(X_full, index = 'Survived', columns = 'numerical_ticket', values = 'PassengerId', aggfunc = 'count')","475ff80f":"#Let's create a column for a person's title. (Comes from Name column after the comma before the period)\n#x_full name apply lambda x: post comma and a space until next space\nX_full['Title'] = X_full['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\npd.pivot_table(X_full, index = 'Survived', columns = 'Title', values = 'PassengerId', aggfunc = 'count')","4afb51c9":"#Create new features for test data\nX_test_full['cabin_letter'] = X_test_full['Cabin'].apply(lambda x: str(x)[0])\nX_test_full['numerical_ticket'] = X_test_full.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nX_test_full['Title'] = X_test_full['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","8b595d1d":"#Update numerical and categorical column lists:\nnumerical_cols = X_full[['Age','SibSp','Parch','Fare']]\ncategorical_cols = X_full[['Survived','Pclass','Sex','Ticket','Cabin','Embarked', 'cabin_letter', 'numerical_ticket', 'Title']]","dc7ef3e2":"#Create a combined dataset with test and train so that we can do our categorical encoding with a combined set to make sure we have the same columns in each\n#Also creating a means to separate the data back out in test and train data after we are done encoding\nX_full['train_test'] = 1\nX_test_full['train_test'] = 0\nX_test_full['Survived'] = np.NaN\nall_data = pd.concat([X_full,X_test_full])","5186079d":"#Drop the two null embarked\nX_full.dropna(subset=['Embarked'],inplace = True)\nX_full.info()","61f1f669":"#Drop columns not using\nall_data = all_data.drop(['Name', 'Ticket', 'Cabin'],axis=1)","f9e17b72":"#Convert the categoricals into dummy variables (similar to OneHotEncoder)\nall_data = pd.get_dummies(all_data, columns = ['Pclass','Sex','Embarked', 'cabin_letter', 'numerical_ticket', 'Title'])\nall_data.head()","0990fade":"#Split back to train and test again\nX_train = all_data[all_data.train_test == 1].drop(['train_test'], axis =1)\ny_train = X_train.Survived\nX_train = X_train.drop('Survived',axis=1)\nX_test = all_data[all_data.train_test == 0].drop(['train_test'], axis =1)\nX_test = X_test.drop('Survived', axis=1)","337315a6":"X_train.info()","74c5049e":"X_test.head()","9f2b4ba2":"#Impute Nulls for Age and Fare for Train Data\nX_train.Age = X_train.Age.fillna(X_train.Age.mean())\nX_train.Fare = X_train.Fare.fillna(X_train.Fare.mean())","1aab29e9":"#Impute Nulls for Age and Fare for Test Data\nX_test.Age = X_test.Age.fillna(X_test.Age.mean())\nX_test.Fare = X_test.Fare.fillna(X_test.Fare.mean())","38f4f59c":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier","a9669b4d":"#Xtreme Gradient Boosting Model\nxgb = XGBClassifier(random_state =1)\nxgb.fit(X_train,y_train)\nval_score = cross_val_score(xgb,X_train,y_train,cv=5).mean()\nprint(val_score)","b74f7800":"#Gaussian Naive Bayes Model\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\nval_score = cross_val_score(gnb,X_train,y_train,cv=5).mean()\nprint(val_score)","cc291660":"#Logistic Regression Model\nlr = LogisticRegression(max_iter=2000)\nlr.fit(X_train,y_train)\nval_score = cross_val_score(lr,X_train,y_train,cv=5).mean()\nprint(val_score)","b160e1ae":"#Support Vector Classifier Model\nsvc = SVC(probability = True)\nsvc.fit(X_train,y_train)\nval_score = cross_val_score(svc,X_train,y_train,cv=5).mean()\nprint(val_score)","be92069c":"# Create predictions\nxgb_preds = xgb.predict(X_test).astype(int)\ngnb_preds = gnb.predict(X_test).astype(int)\nlr_preds = lr.predict(X_test).astype(int)\nsvc_preds = svc.predict(X_test).astype(int)","05fec79b":"#Store predictions in two column dataframe with Passenger ID\nsubmission_xgb = pd.DataFrame({'PassengerId':X_test['PassengerId'],'Survived':xgb_preds})\nsubmission_gnb = pd.DataFrame({'PassengerId':X_test['PassengerId'],'Survived':gnb_preds})\nsubmission_lr = pd.DataFrame({'PassengerId':X_test['PassengerId'],'Survived':lr_preds})\nsubmission_svc = pd.DataFrame({'PassengerId':X_test['PassengerId'],'Survived':svc_preds})","216cac52":"#Convert dataframes to csv\nsubmission_xgb.to_csv('submission_xgb.csv', index=False)\nsubmission_gnb.to_csv('submission_gnb.csv', index=False)\nsubmission_lr.to_csv('submission_lr.csv', index=False)\nsubmission_svc.to_csv('submission_svc.csv', index=False)","48d509a3":"Non-Null Cabin Letters have higher survival rates","6ec9436e":"Doesn't seem to make a huge difference whether the ticket is numeric or not","8e1ce3ae":"Looks like we do have some strong relationships here. \n\nSex seems to be an important indicator: females tend to have a very high survival rate while men do not. Maybe the men are being gentlemen and helping the women to safety first? I never saw the movie Titanic but I'm pretty sure that's what happened.\n\nIt also seems like passengers in first class fared better than those in the lower two. Maybe they had secret exits? Or better swim gear?\n\nAnd the less popular ports C and Q definitely fare better than S. ","40ce8d2e":"## Data Preprocessing\n- Drop null values from embarked (only using this approach because there are just 2)\n- Drop irrelevant variables (Name, Ticket, Cabin, PassengerID)\n- Create imputer for numerical variables and dummy variable columns for categorical variables","8bc72ba1":"## Model Building","eb9909b8":"It may be worth exploring normalizing these non-normal distributions.","14f0804c":"Some observations:\n- About 1\/3 of passengers survived\n- The most popular class is 3rd and it's about equal between 1st and 2nd\n- Almost 2\/3 of passengers are male\n- Most people embarked from Port S, C has around 150 and Q around 75.\n- Ticket and Cabin variables are hard to decipher info from in their current states. Maybe there's more work to do to uncover useful information there","9996ec1d":"Yep, it does look like there is a relationship there. Let's make a note of that.\n\nMoving onto the categorical variables:","83cc54b3":"## Exploring Data","af994314":"Only one that is popping out to me is Fare. Those who survived on average had more than 2X the price of ticket. Is there some connection between having a better ticket and getting off the boat safely?\n\nLet's check out the medians:","d2c03f19":"# Titanic Machine Learning Project\nIn this notebook, I will be documenting my process as I work through the Titanic Machine Learning Competition. The goal of this is to create a model that determines whether passengers survived or perished in the Titanic based on characteristics given about them. This is the first real machine learning model I've created, and I learned a lot of what I'm doing from the mini courses here on Kaggle in addition to other Kagglers like [Ken Jee](https:\/\/www.kaggle.com\/kenjee).\n\nI ended up scoring a 78% which put me in the top 11 percentile. Not bad!\n\n### My Process\n- Explore data\n- Feature engineering\n- Data preprocessing\n- Model building","b0f00b70":"Very tough night for Misters. And the Captain perished!","70857d55":"## Feature Engineering\nLet's see if we can create any useful variables out of Cabin, Tickets, or Title"}}