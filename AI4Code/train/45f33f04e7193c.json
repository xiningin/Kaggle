{"cell_type":{"513d48cb":"code","955ff6ca":"code","b76dbe9e":"code","22e8ed1f":"code","48c9eb95":"code","c88389fb":"code","21aac7bf":"code","b546dabe":"code","f40fb59e":"code","cb408d12":"code","1161cbe5":"code","43df8abf":"code","7b32e55f":"code","a404baf2":"code","ea96273d":"code","59c930eb":"code","67b93915":"code","3a57a508":"code","4b52d284":"code","fcdbc11b":"code","3d3b614b":"code","326cae52":"code","762ba652":"code","62fd3f9f":"code","5f438c87":"code","53b8fbe3":"code","d76e3cd7":"markdown","340ec58c":"markdown","dd6954c9":"markdown","c0bb50fa":"markdown","70150788":"markdown","d195b86c":"markdown"},"source":{"513d48cb":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize, pos_tag, pos_tag_sents\nimport re\nimport gensim\nimport gensim.corpora as corpora\nimport gensim.models.ldamodel as ldamodel\nfrom gensim.summarization import summarize\nimport pyLDAvis.gensim\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.probability import FreqDist\nfrom IPython.utils.text import columnize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport textwrap\n\n%matplotlib notebook\nplt.style.use('seaborn-darkgrid')","955ff6ca":"# Loading the UN General Debates Dataset\ndf = pd.read_csv('..\/input\/un-general-debates\/un-general-debates.csv'\n                ).drop('session', axis=1)\n# The dataset represents country names as 3-letter ISO-alpha3 Codes.\n# To convert these codes into country names, I merged df with the \n# UN country names dataset, which I obtained from:\n# https:\/\/unstats.un.org\/unsd\/methodology\/m49\/overview\/ \n# This dataset also specifies the region (continent), which I will need later.\ncountry_names = pd.read_excel('..\/input\/un-country-names\/UNSD  Methodology.xlsx')\ndf = pd.merge(df, country_names[['Region Name','Country or Area','ISO-alpha3 Code']],\n             how='left', left_on='country', right_on='ISO-alpha3 Code')\ndf.drop('ISO-alpha3 Code',axis=1, inplace=True) #removing a duplicate column\ndf.head()","b76dbe9e":"# For convenience (processing speed), I will reduce the size of the dataset by:\n# 1) Looking at debates that happened this century (years 2001-2015)\ndf = df.loc[df.year > 2000]\n# 2) Limiting the countries of interest to Asian countries:\ndf = df.loc[df['Region Name'] == 'Asia']\n# 3) Removing countries absent from any debates this century:\nalways_present = [index for (index,value) in \n                      df.country.value_counts().items() \n                  if value==15] # for 15 debates in the 21st century\ndf = df.loc[df['country'].isin(always_present)]\n\n# The reduced dataframe (number of unique values per column):\ndf.nunique()","22e8ed1f":"# Preparing texts for analysis:\ndef preprocess_text(text):\n    text = re.sub('[^a-z]+',' ', text.lower()) # remove all non-letter characters\n    tokens = word_tokenize(text) # returns a list of individual words\n    # Removing unhelpfull, ubiquitous words ('stop words', e.g. \u2018the\u2019, \u2018is\u2019, \u2018are\u2019):\n    tokens = [token for token in tokens if len(token) > 4 and\n             token not in nltk.corpus.stopwords.words('english')]\n    # Lemmatizing removes inflectional endings and gets the root word (lemma):\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n    return lemmas\n\n# processed text as list of lemmas:\ndf['lemmatized_tokens'] = df['text'].apply(preprocess_text)\n# processed text in continous form:\ndf['lemmatized_text'] = [' '.join(x) for x in df['lemmatized_tokens']]\n# calculating the number of times each words appears in a speech:\ndf['freq_dist'] = df['lemmatized_tokens'].apply(FreqDist)\ndf[['text','lemmatized_tokens','lemmatized_text']].head()","48c9eb95":"# I will need a picture to prettify my figure.\n# This picture is a black silhouette on a white background.\n# I have it saved as a table, which I will load into a numpy array:\npeace_dove = np.genfromtxt('..\/input\/peace-dove-picture\/peace_dove.csv',\n                           delimiter=',', dtype='uint8')\n\n# Mearging all speeches into a single text:\ntext = df.lemmatized_text.sum()\n\n# Plotting:\nfig = plt.figure(figsize = (10,10))\n# I used Word Cloud to represents the frequency of words with font size.\n# These words can be printed within a shape, in this provided by my picture\nwordcloud = WordCloud(background_color='white',\n                      mask=peace_dove,\n                      max_words=2000).generate(text);\nplt.imshow(wordcloud, interpolation='bilinear') # plotting the image\nplt.axis(\"off\")\n# tight_layout automatically adjusts subplot params\n# so that the subplot(s) fits in to the figure area:\nplt.tight_layout()","c88389fb":"# For each word count in how many documents does it appear:\n\ntexts = df.lemmatized_tokens.tolist() # I will be working on lemmatized words\nall_words = list(set(df.lemmatized_tokens.sum())) # All unique words\n\ntexts = [list(set(t)) for t in texts] # Unique words in each text\nword_in_texts = {} # keys=unique words, values=the number of texts in which they appear\nfor word in all_words:\n    word_in_texts[word] = 0 \n    for text in texts:\n        if word in text:\n            word_in_texts[word] += 1\n\n# This is a slow implementation. Any suggestions on how to speed it up?","21aac7bf":"no_of_speeches = len(texts)\n\nfig, ax = plt.subplots(nrows=2, ncols=1) # figure with two subplots\n\n# converting the dicrionary to a list of tupples and sorting them from most common,\n# to rarest words\nwit = sorted(list(word_in_texts.items()), key=lambda x: x[1], reverse=True)\n# for plotting: numbering words from most to liest common:\nx_wit = [x for x in range(len(wit))]\ny_wit = [y for (x,y) in wit]\nax[0].plot(x_wit, y_wit, color='slateblue')\nax[0].fill_between(x_wit, y_wit, color=\"slateblue\", alpha=0.3)\nax[0].set_title('All words')\nax[0].set_ylabel('Speeches')\n\n# very few words appear in most documents\n# on the other hand out of >14,000 unique words\n# about 10,000 occur in no more than 10 documents\n\nwit_10 = [x for x in wit if x[1]>10] # remove words appearing in 10 or less documents\nx_wit_10 = [x for x in range(len(wit_10))]\ny_wit_10 = [y for (x,y) in wit_10]\nax[1].plot(x_wit_10, y_wit_10, color='slateblue')\nax[1].fill_between(x_wit_10, y_wit_10, color=\"slateblue\", alpha=0.3)\nax[1].set_title('Words that appear in over 10 speeches')\nax[1].set_ylabel('Speeches')\nplt.tight_layout()","b546dabe":"# By removing words that appear in at most 10 speeches, I end up with a much more\n# managable number of >4,000 words.\n# Let's display every 25th of those >4,000 words to get a feeling of\n# how word's characted changes with it's commonness.\n# Subjectively, I find words that occur in >200 speeches (i.e. over 30% of speeches)\n# too general to be useful, and will remove them from further analysis.\n\nprint(columnize([str(x) for x in wit_10[0::25]])) # for a prettier display,\n# I put the list into multiple columns","f40fb59e":"# LDA model\n\ndef find_topics(no_above, no_below, num_topics, data_frame):\n    texts = data_frame['lemmatized_tokens'].tolist()\n    id2word = corpora.Dictionary(texts)\n    # ignore words with document frequency > than the given threshold\n    # keep tokens which are contained in at least no_below documents\n    id2word.filter_extremes(no_above=no_above, no_below=no_below)\n    # Gensim creates a unique id for each word.\n    # The produced corpus is a mapping of (word_id, word_frequency).\n    corpus = [id2word.doc2bow(text) for text in texts]\n    \n    # Estimating LDA model parameters on the corpus.\n    lda = ldamodel.LdaModel(corpus, num_topics=num_topics,\n                        passes=50, random_state=0,\n                        id2word=id2word)\n    return lda, corpus, id2word","cb408d12":"# The results are very sensitive to the number o topics, with some unintuitional results.\n# 14 topics may e.g. give more similar results to 11 topics, than to 13.\n# The results are also hugely sensitive to no_above. Including ubiquitous words resulted in\n# pretty much just one topic, simmilar to the peace dove picture, repeted in different\n# variations. As I played with the parameters, I captured a lot of interesting topics,\n# but unfortunatelly never all of them at once.\n\nnum_topics = 12\nlda_30, corpus_30, id2word_30 = find_topics(0.3, 10, num_topics, df)\nfor topic in lda_30.show_topics():\n    print('topic', topic[0])\n    print(textwrap.fill(topic[1], 75))","1161cbe5":"# pyLDAvis is a neat, interactive way to visualize an LDA model\n\n# Among other things, it shows the overlap between topic, hence may help to pick \n# the number of topics that we want to find. \n# LDA is often used to clasify new documents based of topic,\n# thus overlaps are better avoided, e.g. by limiting the number of topics.\n# In this case however, I'm more interested in finding what is being discussed,\n# than how to classify the speeches, hence I can accept some overlap if I that's\n# the price for also finding more original topics.\n\n# pyLDAvis also shows which words contribute to which topic and by how much\n# Note that the numbers assigned to topics here, do not correspond to those\n# from the figure above, or other plots to follow!\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_30, corpus_30, id2word_30)\nvis","43df8abf":"# I wished to know which topics does each of our speeches contain.\n# Here, I create a dataframe with the list of topics contributing to each speech in\n# one column ('topics'), and the main topic in another column ('main topic')\n\ndef text_topics(ldamodel, corpus):\n    topics_df = df.drop(['country','text','Region Name'], axis=1, inplace=False)\n    topics_df['topics'] = [sorted(ldamodel.get_document_topics(corpus[i]), \n                               key=lambda x: x[1], reverse=True) \n                        for i in range(df.shape[0])]\n    topics_df['Main topic'] = [x[0][0] for x in topics_df.topics]   \n    return topics_df","7b32e55f":"# Did topic distribution change over years?\n# Did any topics become more\/less popular?\n# For each topic I counted in how many speeches was it the main one\n# in each year.\n# Although the counts fluctuate from year to year, and topic 9 clearly\n# peaks in 2008, non of the topics suddenly appears or dissapears\n# within the time frame. I wonder whether this is an artifact of the\n# method or representation of facts?\n\ndef plot_by_year(ldamodel, corpus):\n    topics_df = text_topics(ldamodel, corpus)\n    \n    # Listing words contribution to each topic for the\n    # purpose of graph labelling:\n    topics = ldamodel.show_topics(formatted=False, num_topics=num_topics)\n    topics_words = [(tp[0], [wd[0] for wd in tp[1]])[:] for tp in topics]\n    \n    # A dataframe with years as rows and each topic as a column.\n    # The value in row x and column y is the number of speeches in year x\n    # with main topic y.\n    df1 = pd.crosstab(topics_df['year'], topics_df['Main topic'])\n    df1.plot.bar(subplots=True, sharey=True, figsize=(9,12),\n            use_index=True, legend=False, title=topics_words,\n            color='slateblue', width=0.7)\n    plt.tight_layout()\n\nplot_by_year(lda_30, corpus_30)","a404baf2":"# Do countries have their favourite topics?\n# (The answer is very, very much yes)\n\n# I couted the number of times country x gave speech with main topic y\n# and ploted the results as a heatmap.\n# Each country gave exacly 15 speeches in the selected time frame.\n# Interestingly most countries give 13-15 speeches on the same topic.\n\ndef plot_by_country(ldamodel, corpus):\n    topics_df = text_topics(ldamodel, corpus)\n    # A dataframe with countries as rows and each topic as a column.\n    # The value in row x and column y is the number of speeches given by country x\n    # with main topic y.\n    df2 = pd.crosstab(topics_df['Country or Area'], topics_df['Main topic'])\n    df2.index.name = None\n    f = plt.figure(figsize=(8, 10))\n    ax = sns.heatmap(df2, cmap='bone', cbar=False, annot=True)\n    plt.tight_layout()\n    \nplot_by_country(lda_30, corpus_30)","ea96273d":"# While the above approach gives some interesting insights, there are\n# things I would like to improve upon.\n# Mostly, there is the matter of resolution. I would like to\n# get to more detailed, specific issues.\n# One of the approaches I tried was to split each speech into individual\n# paragraphs and then treat each of those paragraphs as a separate text.\n# I did it on the assumpiton that each paragraph is likely to have\n# one predominant topic, which may be easier for the LDA to fish out.\n\ndef texts_to_paragraphs(text):\n    text = text.lower().split('\\n\\n') # create a list of paragraphs\n    text = [par for par in text if len(par) > 1] # removing empty paragraphs\n    text = [re.sub('[^a-z]+',' ', par) for par in text] # remove all non-letter characters\n    tokens = [word_tokenize(par) for par in text] # returns a list of individual words\n    # Removing unhelpfull, ubiquitous words ('stop words', e.g. \u2018the\u2019, \u2018is\u2019, \u2018are\u2019):\n    tokens = [[token for token in par if len(token) > 4 and\n             token not in nltk.corpus.stopwords.words('english')] for par in tokens]\n    # Lemmatizing removes inflectional endings and gets the root word (lemma):\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmas = [[lemmatizer.lemmatize(token) for token in par] for par in tokens]\n    return lemmas\n\nall_paragraphs = texts_to_paragraphs(df.text.sum())\n\n# Building an LDA model:\nid2word_par = corpora.Dictionary(all_paragraphs)\nid2word_par.filter_extremes(no_above=0.1, no_below=20) # Having more text, I can lower\n# the no_above value\ncorpus_par = [id2word_par.doc2bow(par) for par in all_paragraphs]\n\nlda_par = ldamodel.LdaModel(corpus_par, num_topics=12,\n                    passes=50, random_state=0,\n                    id2word=id2word_par)\n\nfor topic in lda_par.show_topics():\n    print('topic', topic[0])\n    print(textwrap.fill(topic[1], 75))","59c930eb":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_par, corpus_par, id2word_par)\nvis","67b93915":"# Dividing texts into paragraphs identified some specific topics, e.g. grouping\n# China with Yemeni affairs. Unfortunately, not being a specialist in global politics,\n# I cannot judge how much sense such grouping makes. Still, a quick internet search suggests\n# that we may be onto something.\n\n# Before I move on to other methods, I would like to test one more approach for building\n# LDA models.\n\n# This time, instead of increasing the number of texts, I will apply the models to only\n# a selected group of speeches at a time.\n# Specifically, I will collect all speeches given by a single country and run the LDA model\n# only on them. In this way, I am hoping to find some specific issues in which those countries\n# are particularly interested, rather than the general issues the entire World talks about.\n# I am showing a single example below.\n\ndf_country = df.loc[df['Country or Area'] == 'China']\nnum_topics = 3\n# There are only 15 speeches in this dataframe so no no_below filtering and just a\n# mild no_above filter:\nlda_country, corpus_country, id2word_country = find_topics(0.8, 1, num_topics, df_country)\nfor topic in lda_country.show_topics():\n    print('topic', topic[0])\n    print(textwrap.fill(topic[1], 75))","3a57a508":"# ... and using individual paragraphs:\ncountry_paragraphs = texts_to_paragraphs(df_country.text.sum())\n\n# Building an LDA model:\nid2word_c = corpora.Dictionary(country_paragraphs)\nid2word_c.filter_extremes(no_above=0.5, no_below=1) # Having more texts, I can lower\n# the no_above value\ncorpus_c = [id2word_c.doc2bow(par) for par in country_paragraphs]\n\nlda_c = ldamodel.LdaModel(corpus_c, num_topics=6,\n                    passes=50, random_state=0,\n                    id2word=id2word_c)\n\nfor topic in lda_c.show_topics():\n    print('topic', topic[0])\n    print(textwrap.fill(topic[1], 75))","4b52d284":"# I will need to split each speech into sentences.\n# For compatibility with my lists of words,\n# I lowercased and lemmatized all words in sentences.\ndef to_sentences_lemmas(text):\n    sentences = sent_tokenize(text.lower()) # returns a list of sentences\n    # Lemmatize each word:\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmas = [[lemmatizer.lemmatize(word) for word in word_tokenize(sentence)]\n              for sentence in sentences]\n    return lemmas\n\n# a list of sentences in which each sentence is itself a list:\ndf['sentences_lemmas'] = df.text.apply(to_sentences_lemmas)\n# a list of sentences in a continous form:\ndf['sentences'] = df.text.str.replace('\\n+',' ').apply(sent_tokenize)\ndf[['sentences_lemmas','sentences']].head()","fcdbc11b":"# Let's see how long are the speeches (measured in number of sentences):\nplt.violinplot([len(i) for i in df.sentences.tolist()], showmeans=True,\n               showextrema=False);","3d3b614b":"# I think that shortening the text tenfold seems a good compromise between\n# reducing the length and retaining information.\n# This will give me mostly 5-10 sentence summaries.\n\ndef summarize(x):\n    m = x['freq_dist'].most_common(1)[0][1] # get the highest count\n    scores = [sum([x['freq_dist'][word]\/m for word in sentence]\n                 ) for sentence in x['sentences_lemmas']]\n    indexes = [i for i in range(len(scores))]\n    zipped = list(zip(scores,indexes))\n    zipped = sorted(zipped, key=lambda x: x[0], reverse=True)\n    # how many sentences to return?\n    n = int(len(indexes)\/10)\n    # get the index of sentences with n highest scores:\n    i = [zipped[item][1] for item in range(n)]\n    return [x['sentences'][index] for index in i]\n\ndf['summary'] = df.apply(summarize, axis=1)","326cae52":"# Let's see how well it worked:\nprint(df.iloc[3]['year'], df.iloc[3]['Country or Area'])\nprint('\\nOriginal lemmatized text:', str(len(df.iloc[3]['sentences'])), 'sentences')\nfor sentence in df.iloc[3]['sentences']:\n    print(textwrap.fill(sentence, 75))\nprint('\\nSummary:', str(len(df.iloc[3]['summary'])), 'sentences')\nfor sentence in df.iloc[3]['summary']:\n    print(textwrap.fill(sentence, 75))","762ba652":"# The analyser returns a dictionary of scores, in the form:\n# {'neg': 0.152, 'neu': 0.848, 'pos': 0.0, 'compound': -0.5267}\n# 'compound' represents the general sentiment of each sentence.\n# Each word in the lexicon is associated with a strength of sentiment,\n# and this information is reflected in the 'compound' score.\n\ndef strongest_sentiments(text):\n    analyser = SentimentIntensityAnalyzer()\n    scores = [analyser.polarity_scores(sentence)['compound'] for sentence in text]\n    indexes = [i for i in range(len(scores))]\n    zipped = list(zip(scores,indexes))\n    zipped_positive = sorted(zipped, key=lambda x: x[0], reverse=True)\n    zipped_negative = sorted(zipped, key=lambda x: x[0], reverse=False)\n    # how many sentences to return?\n    n = int(len(indexes)\/250)\n    # get the index of sentences with n highest scores:\n    most_positive = [zipped_positive[item][1] for item in range(n)]\n    most_negative = [zipped_negative[item][1] for item in range(n)]\n    return [text[index] for index in most_positive], [text[index] \n                                                      for index in most_negative]\n\nall_sentences = df.sentences.sum()\npositive , negative = strongest_sentiments(all_sentences)","62fd3f9f":"# ten most positive sentences\nfor sentence in positive[:10]:\n    print(textwrap.fill(sentence, 75), '\\n')","5f438c87":"# ten most negative sentences\nfor sentence in negative[:10]:\n    print(textwrap.fill(sentence, 75), '\\n')","53b8fbe3":"# And a visual...\ntext_positive = ' '.join(positive)\ntext_negative = ' '.join(negative)\n\n# Plotting:\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8))\nwordcloud_positive = WordCloud(background_color='white',\n                      max_words=2000).generate(text_positive);\nax[0].imshow(wordcloud_positive, interpolation='bilinear')\nax[0].axis(\"off\")\n\nwordcloud_negative = WordCloud(background_color='black',\n                      max_words=2000).generate(text_negative);\nax[1].imshow(wordcloud_negative, interpolation='bilinear')\nax[1].axis(\"off\")\n\nplt.tight_layout()","d76e3cd7":"**Topic modeling**\n\nThe above picture shows a lot of grand and insping words, but somewhat lacks in concretes...\n\nTo fish for more down to earth topics, I applied a popular type of topic modeling: the Latent Dirichlet allocation model.\n\nLDA is an unsupervised learning model that takes as input our speeches and the number of topics that we wish to find.\n\nIt assumes that documents are created by picking a small number of topics,\n\nwhich then \"generate\" words with certain probabilities.\n\nAs output:\n\n1. For each of our speeches, we will see which topics it is composed of, and to what extent.\n\n2. For each topic, we see a bag of words that describe it. \n\nThese are the words that the topic is most likely to \"generate\" and their corresponding probabilities.\n\nThe LDA model is not immune to the blight of ubiquitous words, such as those from the image above.\n\nThis can he help by manually deciding that, e.g. words present in >80% documents will be discarded.\n\nSimilarly, one can remove words present in too few documents.\n\nHence, before I apply the gensim toolkit LDA model, I will have a look at the commonness of each word\n\nand decide what thresholds to apply.","340ec58c":"**Sentiment analysis**\n\nI decided to use sentiment analysis to fish for the most emotionally-loaded (and hence potentially important) issues.\n\nI pooled all sentences from all speeches together and, like when summarizing, gave each sentence a score.\n\nThis time, the score is based on the strength of sentments expressed in the sentence.\n\nI then picked the sentences with the highest scores to represent positively-associated topics, \n\nand sentences with lowest scores to represent negative sentiments.\n\nUnsurprisingly, I found the negative sentiments more informative.\n\nI used VADER (Valence Aware Dictionary and sEntiment Reasoner), a is a lexicon \n\nand rule-based python sentiment analysis tool.","dd6954c9":"This approach gave a good summary of those topics on which the speaker dwelled on the longest.\n\nHowever, it missed issues mentioned more briefly, e.g. the matter of Palestine (5 sentences).\n\nAny ideas for better approaches? Please, share!","c0bb50fa":"Let's move to an entirely different approach for extracting information from texts.\n\n**Summarizing**\n\nThere are plenty of approaches and automated tools for summarization.\n\nThe method I picked I very simple, but I like the outcomes.\n\nI got the idea from:  https:\/\/stackabuse.com\/text-summarization-with-nltk-in-python\/","70150788":"In this exploration of the UN General Debates I took a \"let the texts speak for themselves\" approach.\n\nMy goal was to discover topics of interest for Asian countries.\n\nI was particularly interested in local issues which I never heard about.\n\nSince I did not want to let my preconceptions affect the outcomes, I decided against coming up with my\n\nown list of subjects to quary (such as 'terrorism', 'hiv' and so on).\n\nInstead, I tried a number of approaches, such as summarising, sentiment analysis,  and building \n\nword clouds and LDA models, to get to the meat of the speeches.\n\n\nI will be very gratefull for any ideas on how to improve\/expand the search!","d195b86c":"**Word Cloud**\n\nA fun way of finding the most common words in all the speeches."}}