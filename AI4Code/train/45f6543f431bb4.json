{"cell_type":{"90b9d6d8":"code","c144d0ce":"code","9595890b":"code","1934e468":"code","7233ba89":"code","9f1a0748":"code","30e7c8e6":"code","b31fcc67":"code","cd1f9c20":"code","6f877569":"code","8e4dd89f":"code","afbfc4c2":"code","349dcf99":"code","690f0e6c":"code","acd7ff3f":"code","4b4b0dd1":"code","e0c4157d":"code","4f04ced0":"code","8f29c5da":"code","c2c3e396":"code","96bc49b3":"code","eab07c3f":"code","f6fe1074":"code","b8057a0e":"code","a0fa8053":"code","96cc23fd":"code","01dce0cf":"code","b8b8eec4":"code","9a8453f3":"code","3443b2ee":"code","81202b6a":"code","5dcac874":"code","d50ea4ce":"code","34f5cdfc":"code","ee1fc35a":"code","f7609d6b":"code","1a2154c4":"code","1126dd5b":"code","12dae3b6":"code","e1a40f27":"code","3f09be0e":"code","21942f99":"code","3610ee1d":"code","d538bc37":"code","49e38df0":"code","afaf90eb":"code","ea4081c8":"code","2e1e3606":"code","81cddf78":"markdown","84965a8a":"markdown","90c00f39":"markdown","621814c7":"markdown","face30df":"markdown","ad8fd4a3":"markdown","e430920b":"markdown","a1184abb":"markdown","343866d3":"markdown"},"source":{"90b9d6d8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c144d0ce":"attrition = pd.read_csv(\"..\/input\/employee\/train.csv\")","9595890b":"attrition.head() #Top 5 Records","1934e468":"attrition.isnull().any()","7233ba89":"attrition.dtypes","9f1a0748":"categorical = attrition.select_dtypes(include = 'object')\nprint(categorical.columns)","30e7c8e6":"numerical = attrition.select_dtypes(include=['float64','int64'])","b31fcc67":"print((numerical.columns))","cd1f9c20":"sns.kdeplot(attrition['Age'])","6f877569":"sns.distplot(attrition['Age'])","8e4dd89f":"fig, ax = plt.subplots(5,2, figsize=(9,9))\nsns.distplot(attrition['TotalWorkingYears'], ax = ax[0,0])\nsns.distplot(attrition['MonthlyIncome'], ax = ax[0,1])\nsns.distplot(attrition['YearsAtCompany'], ax = ax[1,0])\nsns.distplot(attrition['DistanceFromHome'], ax = ax[1,1])\nsns.distplot(attrition['YearsWithCurrManager'], ax = ax[2,0])\nsns.distplot(attrition['YearsSinceLastPromotion'], ax = ax[2,1])\nsns.distplot(attrition['PercentSalaryHike'], ax = ax[3,0])\nsns.distplot(attrition['YearsAtCompany'], ax = ax[3,1])\nsns.distplot(attrition['YearsSinceLastPromotion'], ax = ax[4,0])\nsns.distplot(attrition['TrainingTimesLastYear'], ax = ax[4,1])\nplt.tight_layout()\nplt.show()","afbfc4c2":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'BusinessTravel')","349dcf99":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'Department')","690f0e6c":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'EducationField')","acd7ff3f":"bins = [0, 18, 35, 60, np.inf]\nlabels = ['Student', 'Freshers\/junior', 'Senior', 'Retired']\nattrition['AgeGroup'] = pd.cut(attrition[\"Age\"], bins, labels = labels)\nsns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'AgeGroup')","4b4b0dd1":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'Gender')","e0c4157d":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'JobRole')","4f04ced0":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'Over18')","8f29c5da":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'OverTime')","c2c3e396":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'MaritalStatus')","96bc49b3":"sns.factorplot(data = attrition, kind = 'count', aspect = 3, size = 5, x = 'Attrition')","eab07c3f":"cor_mat = attrition.corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)]=False\nfig = plt.gcf()\nfig.set_size_inches(60,12)\nsns.heatmap(data = cor_mat, mask = mask, square = True, annot = True, cbar = True)","f6fe1074":"attrition.columns","b8057a0e":"continious = ['Age',  'DailyRate', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'TotalWorkingYears', 'YearsAtCompany' ]","a0fa8053":"for var in continious:\n    #boxplot\n    plt.figure(figsize = (10,5))\n    plt.subplot(1,2,1)\n    fig = attrition.boxplot(column = var)\n    fig.set_ylabel(var)\n    \n    #histogram\n    plt.subplot(1,2,2)\n    fig = attrition[var].hist(bins = 20)\n    fig.set_ylabel('No. of Employees')\n    fig.set_xlabel(var)\n    \n    plt.show()\n    ","96cc23fd":"attrition['TotalWorkingYears'].describe()","01dce0cf":"categorical.head()","b8b8eec4":"attrition_cat = pd.get_dummies(categorical)","9a8453f3":"attrition_cat.head()","3443b2ee":"numerical.head()","81202b6a":"attrition_final = pd.concat([numerical,attrition_cat], axis=1)","5dcac874":"attrition_final.head()","d50ea4ce":"attrition_final = attrition_final.drop('Attrition', axis = 1)","34f5cdfc":"attrition_final","ee1fc35a":"target = attrition['Attrition']","f7609d6b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report","1a2154c4":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(attrition_final ,target, test_size = 0.2, random_state = 0)","1126dd5b":"x_train.shape","12dae3b6":"x_test.shape","e1a40f27":"model = RandomForestClassifier()\nmodel.fit(x_train,y_train)\nmodel_predictions = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test, model_predictions))\nprint(classification_report(y_test, model_predictions))","3f09be0e":"model = LogisticRegression()\nmodel.fit(x_train,y_train)\nmodel_predictions = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test, model_predictions))\nprint(classification_report(y_test, model_predictions))","21942f99":"model = DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\nmodel_predictions = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test, model_predictions))\nprint(classification_report(y_test, model_predictions))","3610ee1d":"model = KNeighborsClassifier()\nmodel.fit(x_train,y_train)\nmodel_predictions = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test, model_predictions))\nprint(classification_report(y_test, model_predictions))","d538bc37":"model = SVC()\nmodel.fit(x_train,y_train)\nmodel_predictions = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test, model_predictions))\nprint(classification_report(y_test, model_predictions))","49e38df0":"from imblearn.over_sampling import SMOTE","afaf90eb":"oversampler = SMOTE(random_state = 12, sampling_strategy = 1.0)\nsmote_train, smote_target = oversampler.fit_sample(x_train,y_train)","ea4081c8":"smote_train.shape","2e1e3606":"model = RandomForestClassifier()\nmodel.fit(smote_train,smote_target)\nmodel_predictions = model.predict(x_test)\nprint(\"Accuracy: \", accuracy_score(y_test, model_predictions))\nprint(classification_report(y_test, model_predictions))","81cddf78":"# Introduction\nAttrition is a problem that impacts all businesses, irrespective of geography, industry and size of the company. Employee attrition leads to significant costs for a business, including the cost of business disruption, hiring new staff and training new staff. As such, there is great business interest in understanding the drivers of, and minimizing staff attrition. Let us therefore turn to our predictive modelling capabilities and see if we can predict employee attrition on this IBM dataset.\n\nThis notebook is structured as follows:\n\n1. **Exploratory Data Analysis:** In this section, we explore the dataset by taking a look at the feature distributions, how correlated one feature is to the other and create some Seaborn and Plotly visualisations\n2. **Feature Engineering and Categorical Encoding:** Conduct some feature engineering as well as encode all our categorical features into dummy variables\n3. **Implementing Machine Learning models:** We implement a Random Forest and a Gradient Boosted Model after which we look at feature importances from these respective models\n\nLet's Go.","84965a8a":"**For Futhur Analysis we can seperate the numerical and categorical columns**","90c00f39":"### Correlation of Features\n\nThe next tool in a data explorer's arsenal is that of a correlation matrix. By plotting a correlation matrix, we have a very nice overview of how the features are related to one another. For a Pandas dataframe, we can conveniently use the call **.corr** which by default provides the Pearson Correlation values of the columns pairwise in that dataframe.\n\nIn this correlation plot, I will use the the Plotly library to produce a interactive Pearson correlation matrix via the Heatmap function as follows:","621814c7":"### Distribution of the dataset\n\nGenerally one of the first few steps in exploring the data would be to have a rough idea of how the features are distributed with one another. To do so, I shall invoke the familiar **kdeplot** function from the Seaborn plotting library and this generates bivariate plots as follows:","face30df":"In collaboration with [Samruddhi Mhatre](https:\/\/www.kaggle.com\/samruddhim)","ad8fd4a3":"**Data quality checks**\n\nTo look for any null values, we can just invoke the **isnull** call as follows","e430920b":"Build Basline Models","a1184abb":"<center><h1><b>IBM Employee Attrition Analysis and Prediction<\/b><\/h1><\/center>\n\n![image.png](attachment:image.png)","343866d3":"# 1. Exploratory Data Analysis"}}