{"cell_type":{"f2ff6ce6":"code","fbb0eb17":"code","091e4563":"code","60e39da8":"code","0b33f68b":"code","1ef8956b":"code","94944004":"code","9f571c4e":"code","b2d8a521":"code","8f6d094a":"code","4cdc8e7e":"markdown","6a1a1891":"markdown","4502e3ea":"markdown","752eec99":"markdown","d9dcb2f0":"markdown","47a499c5":"markdown","e7727dfa":"markdown","ff244386":"markdown","f6bc4698":"markdown","c28c3702":"markdown","73f13fdc":"markdown"},"source":{"f2ff6ce6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.metrics import f1_score,roc_auc_score\nimport matplotlib.pyplot as plt\n\nseed = 1301","fbb0eb17":"path = \"..\/input\/titanic\/train_and_test2.csv\"\ndata = pd.read_csv(path)\n\ndata.head()","091e4563":"###to get train and test\ndef split_data(data,target,drop,test_size=0.2,seed=seed):\n    \n    return train_test_split(data.dropna().drop(target+drop,axis=1),\n                            data.dropna()[target],\n                            test_size=test_size,\n                            random_state = seed)","60e39da8":"drop,target  = ['Passengerid'],['2urvived']\n\n#split the data\nX_train,X_test,y_train,y_test = split_data(data,target,drop=drop)","0b33f68b":"tcl = DecisionTreeClassifier(max_depth=3,random_state=seed)\ntcl.fit(X_train,y_train)\ny_pred = tcl.predict(X_test)\ny_prob = tcl.predict_proba(X_test)[:,1]\nprint(f\"f1 score: {f1_score(y_test,y_pred):.2f}\")\nprint(f\"AUC score: {roc_auc_score(y_test,y_prob):.2f}\")","1ef8956b":"class gradient_booster():\n    def __init__(self,loss,gradient_loss,max_depth,nu):\n        self.max_depth = max_depth #max depth of all learners\n        self.nu = nu #learning rate\n        self.loss = loss #loss function to be optimized\n        self.gradient_loss = gradient_loss #gradient of the loss function\n        self.learners = [] #list with all the learners\n        self.loss_history = [] #loss through the process\n    \n    def fit(self,X,y,epochs):\n        base_learner = DecisionTreeClassifier(max_depth=self.max_depth,random_state=seed)\n        base_learner.fit(X,y)\n        initial_probs = base_learner.predict_proba(X)[:,1]\n        \n        probs = initial_probs\n        preds = initial_probs\n        loss_history = [] \n        self.learners = [base_learner]\n        target = y\n        \n        for i in range(epochs):\n            target = -gradient_loss(y,probs)\n            rt = DecisionTreeRegressor(max_depth=self.max_depth,random_state=seed) #regressor tree\n            rt.fit(X,target)\n            self.learners.append(rt)\n            preds += self.nu*rt.predict(X) #these are not probabilities!!!\n            probs = 1 \/ (1 + np.exp(-preds)) #these are probabilities\n            self.loss_history.append(loss(y,probs))\n            \n        return self\n    \n    def predict_proba(self,X):\n        try:\n            preds  = self.learners[0].predict_proba(X)[:,1]\n            \n            for m in self.learners[1:]:\n                preds += self.nu*m.predict(X)\n\n            return 1 \/ (1 + np.exp(-preds))\n        \n        except NotFittedError:\n            print(\"Model not fitted yet\")","94944004":"def loss(y,p):\n    return -2.0 * np.mean(y * np.log(p\/(1-p)) - np.logaddexp(0.0, np.log(p\/(1-p)))) ","9f571c4e":"def gradient_loss(y,p):\n    return (p-y)\/(p*(1-p))","b2d8a521":"booster = gradient_booster(loss=loss,gradient_loss=gradient_loss,max_depth=3,nu=0.01)\nX, y, epochs = X_train.values, y_train.values.ravel(), 50\nbooster.fit(X,y,epochs)\ny_prob = booster.predict_proba(X_test)\ny_pred = 1*(y_prob>0.5)\nprint(f\"f1 score: {f1_score(y_test,y_pred):.2f}\")\nprint(f\"AUC score: {roc_auc_score(y_test,y_prob):.2f}\")","8f6d094a":"fig,ax = plt.subplots(1,1,figsize=(6,6))\nplt.plot(range(epochs),booster.loss_history)\nplt.show()","4cdc8e7e":"Now we will construct a gradient booster class implementing the previous algorithm. Let's code it first and then talk a little bit about the details","6a1a1891":"### What is gradient boosting?\n\nIf you're inside the world of machine learning, it's for sure you have heard about gradient boosting algorithms such as xgboost or lightgbm. Indeed, gradient boosting represents the state-of-start for a lot of machine learning task, but how does it work? We'll try to answer this question specifically for the case of gradient boosting for trees which is the most popular case up today","4502e3ea":" For me, understanding gradient boosting is all about undestarding its link to gradient descent. Remember gradient descent is the algorithm to minimize a loss function $L(\\theta)$ by sustracting the gradient to the parameters\n\n$$\n\\theta = \\theta - \\frac{\\partial L(\\theta)}{\\partial \\theta}\n$$\n\n At first glance it doesn't make much sense; trees are based on a split-gain function (Gini, entropy), not a loss function. Moreover, what would be $\\theta$?\n \n Recall $\\theta = (\\theta_1,\\ldots,\\theta_n)$ are the learned parameters we use for making predictions, which are the paramters of the loss functio. In gradient boosting, we consider the loss function as a function of the predictions instead, so we want to find $\\min_{p}L(y,p)$ and the way to achieve that is analogous to gradient descent, i.e, updating the predictions on the opposite direction of the gradients. But how can you update the predicitions? I mean, once the tree is built it has a fixed structure. \n \n Here comes the idea of additive modeling, in which you add (sequentially in this case) the predictions of several models in order to get a better performance. The first tree makes predictions on the original target, then, with the second tree, we try to minimize the loss function adding something which is not exactly minus the gradient loss, as in gradient descent, but instead we add *predictions on the gradients loss*, i.e, we fit a tree over with target the gradient loss. So, if $X,y$ represents the original data and \n$p = \\mbox{predict}(X,y)$ \n\n$$\np = p-\\mbox{predict}\\left(X,\\dfrac{\\partial L(y,p)}{\\partial p}\\right)\n$$ \n\nThis equation would be the *gradient descent* version of trees. Maybe this is nothing new for you, but I hope it gives you a better or new way of seeing gradient boosting.","752eec99":"I intenionally picked a classification problem over a regression because has some additional technical details worth understanding.\n\n1. Note that the base learner is a classifier tree but the next learners are regressors due to the gradients being a continuous target.\n2. The outputs of the regressor trees aren't probabilities and therefore the cumulative predictions `preds` neither are. That's why we pass them to the sigmoid function.\n3. Finally, and this stand for the general case, we added a shrinkage parameter `nu` because of the same reasons we add it to gradient descent: the gradients can become crazy and overshoot the minimum.","d9dcb2f0":"Let's fit a base learner for the data.","47a499c5":"Next we define the loss which is the classic binary deviance","e7727dfa":"### References\n1. https:\/\/explained.ai\/gradient-boosting\/\n2. https:\/\/web.stanford.edu\/~hastie\/Papers\/ESLII.pdf","ff244386":"We will work with a modified version of our familiar titanic dataset. This is a ready-to-train version: doesn't not contains missings and categorical data has been encoded.\n(shout out to the person who made public this data)","f6bc4698":"Let's implement the booster and see how it performs","c28c3702":"Nice! We got decent increment in auc and a good amount of f1. To finish this notebook we'll plot the classic loss vs epochs plot to verify that the loss is decresing with every addition of a new tree.","73f13fdc":"Now let's get our hands on some data and see a woking example"}}