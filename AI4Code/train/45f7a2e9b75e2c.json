{"cell_type":{"56273ae8":"code","ac556923":"code","1662f6f2":"code","2a5bf1bd":"code","34834046":"code","70462f62":"code","7daee544":"code","b23e5f07":"code","46b162f5":"code","ad3fdc22":"code","f614e7e5":"code","9eca33c0":"code","b1c69e78":"code","44ca75c6":"code","5a883779":"code","6fed0b50":"code","84d526d0":"code","fca358d8":"code","bb766bb8":"code","5f0b6129":"code","d4b9826d":"code","4333f51e":"code","6bc375ef":"code","8b63cd7d":"code","ddd0739a":"code","d5e120ec":"code","5b059671":"code","820b2849":"code","d27d8acc":"code","f60f728e":"code","d1a2eedb":"code","4d3f5543":"code","a1c8094b":"code","ffeb731c":"code","ae138e69":"code","b7f057f8":"code","db9ea553":"code","c4743f0f":"code","49780d71":"code","d3ff8338":"code","7db10ac5":"code","6fcc1657":"code","b5c85763":"code","2f4544d7":"code","edbff857":"code","a146db5f":"code","6c4dfdd9":"code","d0070fc8":"code","eb8d8daa":"code","c6d41594":"code","a0423d9d":"code","78306961":"code","8aff358e":"code","61f7e4c9":"code","648d870a":"code","6d897ac1":"code","12fe5a5a":"code","73c02809":"code","0c445418":"code","464a25c4":"code","d217a65c":"code","30c416f3":"code","ad2a67b5":"code","83c843db":"code","a6f25f74":"code","9e0c062a":"code","df8e3b3d":"code","79f2fabc":"code","651ff9f1":"code","7c90bb54":"code","392f112f":"code","cab79eb1":"code","d7787f65":"code","200ba924":"code","4950cf40":"code","82b5b74f":"code","9675c00b":"code","d26163c7":"code","1788171e":"code","f777ef34":"code","ea081165":"code","a8334e84":"code","e37170fd":"code","e4b70ec1":"code","56b92da0":"code","62db1589":"code","9476dd1d":"markdown","6b436f40":"markdown","bf642f62":"markdown","2c6466cb":"markdown","7c658e33":"markdown","83c31c52":"markdown","d0fd1b4c":"markdown","18f52315":"markdown","8256a4b9":"markdown","6a84087f":"markdown","46273d2f":"markdown","6381bd0b":"markdown","6d4f7c8d":"markdown","6e2c8b09":"markdown","e8c23691":"markdown","24267e6e":"markdown","cb3f8123":"markdown","f0759070":"markdown","4e72658a":"markdown","8380bed8":"markdown","b21ebe21":"markdown","c5cc086b":"markdown","1e635f44":"markdown","292c1a34":"markdown","d836f9d2":"markdown","e9d9787a":"markdown","72dc97a0":"markdown"},"source":{"56273ae8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport datetime as dt\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport itertools\n%matplotlib inline\n\n# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438:\nfrom sklearn.ensemble import RandomForestRegressor # \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nfrom sklearn import metrics # \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0432\u043a\u0438:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ac556923":"# \u0437\u0430\u0444\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u0432\u0435\u0440\u0441\u0438\u044e \u043f\u0430\u043a\u0435\u0442\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b:\n!pip freeze > requirements.txt","1662f6f2":"# \u0414\u043b\u044f \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0437\u0430\u0434\u0430\u0434\u0438\u043c:\n# - \u043e\u0431\u0449\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0445 \u0447\u0438\u0441\u0435\u043b\nRANDOM_SEED = 42\n# - \u043e\u0431\u0449\u0443\u044e \u0442\u0435\u043a\u0443\u0449\u0443\u044e \u0434\u0430\u0442\u0443\nCURRENT_DATE = pd.to_datetime('20\/11\/2020')\n","2a5bf1bd":"DATA_DIR = '\/kaggle\/input\/sf-dst-restaurant-rating\/'\ndf_train = pd.read_csv(DATA_DIR+'\/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'\/sample_submission.csv')","34834046":"#\u0414\u0430\u0442\u0430\u0441\u0435\u0442, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0433\u043e\u0440\u043e\u0434\u0430\u0445\ndf_city = pd.read_csv(\"\/kaggle\/input\/world-cities\/worldcities.csv\")\ndf_city.head()","70462f62":"# \u041f\u0443\u0442\u044c \u043a \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0435\u043c\u0443 \u0441\u043b\u043e\u0432\u0430 \u0441 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u043e\u0439 \u043e\u043a\u0440\u0430\u0441\u043a\u043e\u0439\ndf_pos_words = pd.read_table(\"..\/input\/opinion-lexicon\/opinion_lexicon\/positive-words.txt\", comment=';', header=None)\n# \u041f\u0443\u0442\u044c \u043a \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0435\u043c\u0443 \u0441\u043b\u043e\u0432\u0430 \u0441 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u043e\u043a\u0440\u0430\u0441\u043a\u043e\u0439\ndf_neg_words = pd.read_table(\"..\/input\/opinion-lexicon\/opinion_lexicon\/negative-words.txt\", comment=';', header=None)\npos_words_list = df_pos_words[0].to_list()\nneg_words_list = df_neg_words[0].to_list()","7daee544":"def get_season(date):\n   '''\n   \u0412\u0440\u0435\u043c\u044f \u0433\u043e\u0434\u0430 \u0438\u0437 \u0434\u0430\u0442\u044b\n   '''\n   if (pd.isna(date)):\n       return \"OTHER\"\n   month = date.month\n   if (month > 11 or month <= 3):\n      return \"WINTER\"\n   elif (month == 4 or month == 5):\n      return \"SPRING\"\n   elif (month >=6 and month <= 9):\n      return \"SUMMER\"\n   else:\n      return \"FALL\"","b23e5f07":"def round_of_rating(number):\n    \"\"\"\n    \u041e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u0435 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0441 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\u044e \u0434\u043e 0.5\n    \"\"\"\n    return np.round(number * 2) \/ 2","46b162f5":"def get_Weighed_Rank_RK(row):\n    '''\n    \u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u044e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\n    '''\n    Weighed_Rank = row.Ranking \/ row['Restaurants_Count']\n    return Weighed_Rank","ad3fdc22":"def get_Weighed_Rank(CityMinMax:pd.DataFrame ,row):\n    '''\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u043e\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443\n    '''\n    city_min = CityMinMax[CityMinMax.City == row.City]['min'].iloc[0]\n    city_max = CityMinMax[CityMinMax.City == row.City ]['max'].iloc[0]\n    Weighed_Rank = round(1 - (row.Ranking-city_min) \/ (city_max-city_min),3)\n    return Weighed_Rank","f614e7e5":"def cleanup_string(str_in):\n    '''\n    \u0427\u0438\u0441\u0442\u0438\u043c \u0442\u0435\u043a\u0441\u0442 \u0432 review \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0439 \u0434\u0435\u0441\u0435\u0440\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438\n    '''\n    try:      \n        #middle\n        str = str_in.replace(\"', \\\"\",\"\u215e\").replace(\"', '\",\"\u215e\").replace(\"\\\", '\",\"\u215e\").replace(\"\\\", \\\"\",\"\u215e\")# \", \n        str = str.replace(\"\\\", \\\"\\\"\",\"\u215e\").replace(\"\\\"\\\", '\",\"\u215e\").replace(\"\\\", \\'\",\"\u215e\").replace(\"\\\"\\\", \\'\",\"\u215e\")\n        str = str.replace(\"\\', \\'\",\"\u215e\")\n        #left\n        str = str.replace(\"[['\",\"\u2264\").replace(\"['\",\"\u215b\")\n        #right\n        str = str.replace(\"']]\",\"\u2265\").replace(\"']\",\"\u215d\")\n        #cleanups\n        str = str.replace('\\'', ' ').replace('\\\"', ' ').replace('\\'', ' ').replace('\"', ' ')     \n        str = str.replace(\"\\\\\", \" \").replace(\"[[`\", \"\u2264\").replace('\\'\"', '\\'').replace('\\'\\\"', '\\'')\n        str = str.replace('\"\\'', '\\'').replace('\\\"\\'', '\\'').replace(\"[''\" ,\"\u2264\").replace(\"[\\'\\'\" ,\"\u2264\")\n        str = str.replace('\\'', ' ').replace('\\\"', ' ')\n        str = str.replace('\\'', ' ').replace('\\\"', ' ')\n        str = str.replace('\\'', ' ').replace('\"', ' ')\n        #middle\n        str = str.replace(\"\u215e\", \"', '\")\n        #left\n        str = str.replace(\"\u2264\", \"[['\").replace(\"\u215b\", \"['\").replace('[[ ', '[[ \\'')\n        #right\n        str = str.replace(\"\u2265\" ,\"']]\").replace(\"\u215d\", \"']\").replace(' ]', ' \\']')\n        str = str.replace(', nan]', '\\', \\'nan\\']').replace('[nan, ', '[\\'nan\\', \\'')\n    except Exception:\n        print('<----',str_in,'---->')\n    return str","9eca33c0":"def get_reviews(rev):\n    '''\n    \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c review \u0432 \u0432\u0438\u0434\u0435:\n    review['reviews_txt'][1] - list of reviews\n    review['reviews_dt'][1] - list of reviews dates\n    '''\n    if  not pd.isna(rev): \n        #\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043a \u0444\u043e\u0440\u043c\u0430\u0442\u0443 json \u0434\u043b\u044f \u0443\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u043f\u0430\u0440\u0441\u0438\u043d\u0433\u0430\n        rev = str(rev).replace(\"'\",'\"')\n        rev = rev.replace('], [', '], \"reviews_dt\": [')\n        rev = '{ \"reviews_txt\":' + rev + '}'\n        rev = rev.replace('[[','[').replace(']]',']')\n        d = json.loads(rev)\n\n        d['reviews_dt'] = [dt.datetime.strptime(date, '%m\/%d\/%Y').date() if len(date.split('\/')[2])==4 \\\n            else dt.datetime.strptime(date, '%m\/%d\/%y').date() for date in d['reviews_dt']]\n        return d\n    else:\n        return {}","b1c69e78":"def rev_time_delta(rev):\n    '''\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u0432\u0440\u0435\u043c\u044f \u043c\u0435\u0436\u0434\u0443 review \u0432 \u0434\u043d\u044f\u0445\n    '''\n    if (pd.notna(rev)):\n        reviews_dt_list = get_reviews(rev)['reviews_dt']\n        if reviews_dt_list:\n            return (max(reviews_dt_list) - min(reviews_dt_list)).days\n        else:\n            return dt.timedelta(days=3650).days\n    else:\n        return dt.timedelta(days=3650).days","44ca75c6":"def get_cuisines(cuisines):\n    '''\n    \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0432\u0438\u0434\u0435:\n    cuisines[0] - list of cusines\n    \u0435\u0441\u043b\u0438 \u0431\u044b\u043b NaN, \u0442\u043e \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f 'Regional Cusine' -\u043a\u0430\u043a \u0441\u0430\u043c\u0430\u044f \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u0432 \u0440\u0435\u0433\u0438\u043e\u043d\u0435\/\u0433\u043e\u0440\u043e\u0434\u0435\/\u0441\u0442\u0440\u0430\u043d\u0435\n    '''\n    if cuisines == 'NaN': return ['Regional Cusine']\n    if  cuisines:\n        cuisines = str(cuisines).replace(\"'\",'\"')\n        return json.loads(cuisines)\n    else:\n        return ['Regional Cusine']","5a883779":"allCusines = []\ndef cuisine_styles_count(row):\n    '''\n    \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c\n    '''\n    global allCusines\n    cusines = get_cuisines(row['Cuisine_Style'])\n    \n    if row['Cuisine_Style'] != 'NaN':    \n        cusines = get_cuisines(row['Cuisine_Style'])\n        allCusines.extend(cusines)\n        cuisines_count =len(cusines)\n    else:\n        cuisines_count = 1\n\n    return cuisines_count","6fed0b50":"def get_city_population_and_country(df:pd.DataFrame, df_city:pd.DataFrame):\n    '''\n    \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043f\u0443\u043b\u044f\u0446\u0438\u044e \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c, \u0430 \u0442\u0430\u043a \u0436\u0435 ISO \u043a\u043e\u0434 \u0441\u0442\u0440\u0430\u043d\u044b \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443 (\u0438\u0437 \u0432\u043d\u0435\u0448\u043d\u0438\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432)\n    '''\n    population_city_dict = {}\n    country_city_dict = {}\n    cities = df['City'].unique()\n\n    for city in cities:\n    \n        vals = (df_city[df_city.city.str.contains(city)].population \/ 1000000).max()\n        vals = 0.3 if np.isnan(vals) else vals\n    \n        population_city_dict[city] = vals\n        country =  df_city[df_city.city.str.contains(city)][df_city[df_city.city.str.contains(city)].population == df_city[df_city.city.str.contains(city)].population.max()].iso2\n        country = -1 if country.shape[0] < 1 else country.keys()[0]\n        country_city_dict[city] = country\n\n    population_city_dict['Luxembourg'] = 0.613894 \n    population_city_dict['Brussels'] = 2.115468 \n    population_city_dict['Geneva'] = 0.686562 \n    population_city_dict['Oporto'] = 0.214349 \n    population_city_dict['Ljubljana'] = 0.279631\n\n\n\n    country_city_dict['Luxembourg'] = 442 \n    country_city_dict['Brussels'] = 56 \n    country_city_dict['Geneva'] = 756 \n    country_city_dict['Oporto'] = 620 \n    country_city_dict['Ljubljana'] = 705\n    \n    #\u042d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u0435\u0435\n    population_city_dict = {    'London': 8.173900,\n    'Paris': 2.240621,\n    'Madrid': 3.155360,\n    'Barcelona': 1.593075,\n    'Berlin': 3.326002,\n    'Milan': 1.331586,\n    'Rome': 2.870493,\n    'Prague': 1.272690,\n    'Lisbon': .547733,\n    'Vienna': 1.765649,\n    'Amsterdam': .825080,\n    'Brussels': .144784,\n    'Hamburg': 1.718187,\n    'Munich': 1.364920,\n    'Lyon': .496343,\n    'Stockholm': 1.981263,\n    'Budapest': 1.744665,\n    'Warsaw': 1.720398,\n    'Dublin': .506211 ,\n    'Copenhagen': 1.246611,\n    'Athens': 3.168846,\n    'Edinburgh': .476100,\n    'Zurich': .402275,\n    'Oporto': .221800,\n    'Geneva': .196150,\n    'Krakow': .756183,\n    'Oslo': .673469,\n    'Helsinki': .574579,\n    'Bratislava': .413192,\n    'Luxembourg': .576249,\n    'Ljubljana': .277554\n    }\n    return population_city_dict, country_city_dict","84d526d0":"def get_capital_city_dict(df_in:pd.DataFrame, df_city:pd.DataFrame):\n    '''\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0433\u043e\u0440\u043e\u0434 == \u0441\u0442\u043e\u043b\u0438\u0446\u0430 \u0438\u043b\u0438 \u043d\u0435\u0442\n    '''\n    capital_city_dict = {}\n    cities = df_in['City'].unique()\n\n    df_city['isCapital'] = df_city.capital.apply(lambda x: True if x == \"primary\" else False)\n\n    for city in cities:\n        vals = df_city[df_city.city.str.contains(city)] \\\n            [df_city[df_city.city.str.contains(city)].population == df_city[df_city.city.str.contains(city)].population.max()].isCapital\n        capital_city_dict[city] = vals.values[0] if len(vals) > 0 else False\n    return capital_city_dict","fca358d8":"def is_cuisine_top_N(cs):\n    '''\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c, \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0445 \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c,\u0434\u043b\u044f \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0445 Other\n    '''\n    c = get_cuisines(cs)\n    c = set(c)\n\n    shared_cousines=()\n    shared_cousines=c.intersection(topNcusines)\n\n    if len(shared_cousines) != len(c):\n        shared_cousines = list(shared_cousines)\n        shared_cousines.extend(['Other'])\n\n    return list(shared_cousines)","bb766bb8":"def createWordList(line):\n    wordList2 =[]\n    wordList1 = line.split()\n    for word in wordList1:\n        cleanWord = \"\"\n        for char in word:\n            if char in '!,.?\":;0123456789':\n                char = \"\"\n            cleanWord += char\n        wordList2.append(cleanWord.lower())\n    return wordList2","5f0b6129":"def count_positive_words_proportion(reviews):\n    '''\n    \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n    '''\n    pos_words_count = 0\n    txts=get_reviews(reviews)['reviews_txt']\n    txt = ' '.join(txts)\n    #print(type(txt))\n    words = createWordList(txt)\n    \n    words_count = len(words) if len(words) > 0 else 1\n    words_count = 1\n    for word in words:\n        if word in pos_words_list:\n            #print(word)\n            pos_words_count +=1  \n    return np.round(pos_words_count \/ words_count,2)\n\ndef count_negative_words_proportion(reviews):\n    '''\n    \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n    '''\n    neg_words_count = 0\n    txts=get_reviews(reviews)['reviews_txt']\n    txt = ' '.join(txts)\n    #print(type(txt))\n    words = createWordList(txt)\n    \n    words_count = len(words) if len(words) > 0 else 1\n    words_count = 1\n    for word in words:\n        if word in neg_words_list:\n            #print(word)\n            neg_words_count +=1  \n    return np.round(neg_words_count \/ words_count,2)\n\ndef list_positive_words(reviews): \n    '''\n    \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n    '''\n    txts = get_reviews(reviews)['reviews_txt']\n    txt = ' '.join(txts)\n    words = createWordList(txt)\n    \n    words_count = len(words) if len(words) > 0 else 1\n    words_count = 1\n    pos_words_in_review=set(words).intersection(pos_words_list)\n    if (len(pos_words_in_review) == 0):\n        return np.NAN\n    else:\n        return list(pos_words_in_review)\n\ndef list_negative_words(reviews): \n    '''\n    \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n    '''\n    txts = get_reviews(reviews)['reviews_txt']\n    txt = ' '.join(txts)\n    words = createWordList(txt)\n    \n    words_count = len(words) if len(words) > 0 else 1\n    words_count = 1\n    neg_words_in_review=set(words).intersection(neg_words_list)\n    if (len(neg_words_in_review) == 0):\n        return np.NAN\n    else:\n        return list(neg_words_in_review) ","d4b9826d":"df_train.info()","4333f51e":"df_train.head(5)","6bc375ef":"# \u0412\u0410\u0416\u041d\u041e! \u0434\u0440\u044f \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0432 \u043e\u0434\u0438\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n\ndf_train['Train'] = True # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ndf_test['Train'] = False # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\ndf_test['Rating'] = 0 # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f Rating, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c","8b63cd7d":"data.columns = list(map(lambda x: x.replace(\" \",\"_\"), data.columns))\ndata.info()","ddd0739a":"# \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043d\u0443\u0436\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0441\u0442\u043e\u043b\u0431\u0446\u044b\ndf = data.drop('URL_TA',axis=1).copy()","d5e120ec":"df.head(5)","5b059671":"df.Reviews[1]","820b2849":"df.info()\nprint(\"\\n\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432:\")\nfor col in df.columns:\n    if data[col].isna().sum() > 0:\n        print(col, \"\\t:\", data[col].isna().sum())","d27d8acc":"df.nunique(dropna=False)","f60f728e":"df['Train']","d1a2eedb":"df[df.Train][['Ranking', 'Rating', 'Number_of_Reviews']].hist(figsize=(20, 10), bins=100);\nplt.tight_layout()","4d3f5543":"df[df.Train]['Restaurant_id'].apply(lambda x: x.split('_')[1]).astype(int).hist(figsize=(10,5), bins=100)\nplt.tight_layout()\n# Restaurant_id \u041e\u0447\u0435\u043d\u044c \u043f\u043e\u0445\u043e\u0436 \u043d\u0430 Ranking","a1c8094b":"df[df.Train]['ID_TA'].apply(lambda x: x.replace('d','')).astype(int).hist(figsize=(10, 5), bins=100)\nplt.tight_layout()\n# \u0412\u0438\u0434\u043d\u043e \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0433\u0440\u0443\u043f\u043f - \u043d\u0430 ID - \u0442\u043e\u0447\u043d\u043e \u043d\u0435 \u043f\u043e\u0445\u043e\u0436\u0435. ","ffeb731c":"df['ID_TA'] = df['ID_TA'].apply(lambda x: x[1:]).astype(int)","ae138e69":"# Reviews - \u0443\u0431\u0438\u0440\u0430\u0435\u043c NaN \u0438 \"\u043f\u0440\u0438\u0447\u0435\u0441\u044b\u0432\u0430\u0435\u043c\" \u0442\u0435\u043a\u0441\u0442 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \ndf['Reviews_txt_NaN'] = df['Reviews'].apply(lambda x: x ==  '[[], []]')\n\ndf['Reviews'] = df['Reviews'].fillna('[[], []]')\ndf['Reviews'] = df['Reviews'].apply(lambda x: cleanup_string(x))","b7f057f8":"# \u041f\u0435\u0440\u0435\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c Price Range \u0438 \u0443\u0434\u0430\u043b\u044f\u0435\u043c NaN\ncleanup_nums = {'Price_Range': {\"$\": 1, \"$$ - $$$\": 2, \"$$$$\": 3, np.NaN: 2}}\n# \u0447\u0430\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \"$$ - $$$\", \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u043b\u0438 \u043d\u0430 2\ndf['Price_Range_NaN'] = df['Price_Range'].isna()\ndf.replace(cleanup_nums, inplace=True)","db9ea553":"# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c Cuisines Count, \u0441\u0430\u043c\u0443\u044e \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0443\u044e \u043a\u0443\u0445\u043d\u044e, \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043a-\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0438 \u0443\u0441\u0442\u0440\u0430\u043d\u044f\u0435\u043c NaN\ndf['Cuisine_Style_NAN'] = df['Cuisine_Style'].isna()\ndf['Cuisine_Style'] = df['Cuisine_Style'].fillna('NaN')\ndf['Cuisines_Count'] = df.apply(cuisine_styles_count, axis=1)\n\nmost_popular_cusine = pd.Series(allCusines).value_counts().index[0]\naverage_cousines_count = np.round(df['Cuisines_Count'].mean())","c4743f0f":"# Number of Reviews\ndf['Number_of_Reviews_NAN'] = df['Number_of_Reviews'].isna()\nreplace_val = df['Number_of_Reviews'].mean()\nreplace_val = np.round(replace_val)\ndf['Number_of_Reviews'] = df['Number_of_Reviews'].fillna(replace_val)","49780d71":"df.sample(5)","d3ff8338":"plt.rcParams['figure.figsize'] = (10,5)\ndf[df.Train]['Ranking'].hist(bins=100)","7db10ac5":"df[df.Train]['City'].value_counts(ascending=True).plot(kind='barh')","6fcc1657":"df[df.Train]['Ranking'][df[df.Train]['City'] =='London'].hist(bins=100)","b5c85763":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0442\u043e\u043f 10 \u0433\u043e\u0440\u043e\u0434\u043e\u0432\nfor x in (df[df.Train]['City'].value_counts())[0:10].index:\n    df[df.Train]['Ranking'][df[df.Train]['City'] == x].hist(bins=100)\nplt.show()","2f4544d7":"df[df.Train]['Rating'].value_counts(ascending=True).plot(kind='barh')","edbff857":"df[df.Train]['Ranking'][df[df.Train]['Rating'] == 5].hist(bins=100)","a146db5f":"df[df.Train]['Ranking'][df[df.Train]['Rating'] < 4].hist(bins=100)","6c4dfdd9":"plt.rcParams['figure.figsize'] = (15,15)\nsns.heatmap(df[df.Train].drop(['Train'], axis=1).corr(), square=True,\n            annot=True, fmt=\".1f\", linewidths=1, cmap=\"cool\")\nplt.tight_layout()","d0070fc8":"population_city_dict = {}\ncountry_city_dict = {}\n# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0438 \u043f\u043e\u043f\u0443\u043b\u044f\u0446\u0438\u0438 \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c, \u0430 \u0442\u0430\u043a \u0436\u0435 ISO \u043a\u043e\u0434 \u0441\u0442\u0440\u0430\u043d\u044b \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443\npopulation_city_dict, country_city_dict = get_city_population_and_country(df, df_city)","eb8d8daa":"# \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u0441\u0442\u0440\u0430\u043d\u0443 \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\ndf['Country'] = df[\"City\"].apply(lambda x: country_city_dict[x])","c6d41594":"# \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043a-\u0432\u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\nrestorants_in_city = df.groupby('City')['Ranking'].count().to_dict()\ndf['Restaurants_Count'] = df['City'].map(restorants_in_city)","a0423d9d":"# \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u0435 (\u0432 \u0442\u044b\u0441. \u0447\u0435\u043b) \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\ndf['Population'] = df[\"City\"].map(population_city_dict)","78306961":"# \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043a-\u0432\u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u043d\u0430 1000 \u0447\u0435\u043b \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\ndf['Restaurants_for_Population'] = df.Restaurants_Count \/ (df['Population']*1000)","8aff358e":"# \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0438 \u0433\u043e\u0440\u043e\u0434 \u0441\u0442\u043e\u043b\u0438\u0446\u0435\u0439 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\ncapital_city_dict = get_capital_city_dict(df, df_city)\ndf['isCapital'] = df[\"City\"].apply(lambda x: capital_city_dict[x])","61f7e4c9":"# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u043f\u043e\u0437\u0438\u0446\u0438\u044e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0441\u0440\u0435\u0434\u0438 \u0432\u0441\u0435\u0445 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0433\u043e\u0440\u043e\u0434\u0430\ndf['Weighed_Rank'] = df.apply(lambda x: get_Weighed_Rank_RK(x), axis=1)\n\nCityMinMax = df.groupby('City')['Ranking'].agg([min,max])\nCityMinMax =CityMinMax.reset_index()\ndf['Weighed_Rank_min_max'] = df.apply(lambda x: get_Weighed_Rank(CityMinMax, x), axis=1)","648d870a":"# \u0424\u043b\u0430\u0433\u0438 (1\/0) isMostPopCusine - \u0435\u0441\u0442\u044c \u043b\u0438 \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0441\u0430\u043c\u0430\u044f \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u043a\u0443\u0445\u043d\u044f; isMultyCusine - \u043a-\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u0438\u043b\u0438 \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0436\u0435 \u0447\u0435\u043c \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c\ndf['isMostPopCusine'] = df.Cuisine_Style.apply(lambda x: 1 if most_popular_cusine in x else 0 )\ndf['isMultyCusine'] = df.Cuisines_Count.apply(lambda x: 1 if  x >= average_cousines_count else 0 )","6d897ac1":"# RevTimeDelta - \u0432\u0440\u0435\u043c\u044f \u043c\u0435\u0436\u0434\u0443 review \u0432 \u0434\u043d\u044f\u0445\ndf['RevTimeDelta'] = df.Reviews.apply(rev_time_delta)","12fe5a5a":"# NewestReviewDate - \u0432\u0440\u0435\u043c\u044f, \u043f\u0440\u043e\u0448\u0435\u0434\u0448\u0435\u0435 \u0441\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e review\ndf['NewestReviewDate'] = df.Reviews.apply(lambda x: get_reviews(x)['reviews_dt'])\ndf['NewestReviewDate'] = df.NewestReviewDate.apply(lambda x: sorted(x,reverse=True)[0] if len(x)!=0 else pd.NaT)\ndf['NewestReviewDate'] = df.NewestReviewDate.fillna(dt.date(1970,1,1))\ndf['NewestReviewDate'] = df.NewestReviewDate.apply(lambda x: (CURRENT_DATE.date()-x).total_seconds()\/\/86400)","73c02809":"df['NewestReviewSeason'] = df.Reviews.apply(lambda x: get_reviews(x)['reviews_dt'])\ndf['NewestReviewSeason'] = df.NewestReviewSeason.apply(lambda x: sorted(x,reverse=True)[0] if len(x)!=0 else pd.NaT)\n\ndf['NewestReviewSeason'] = df.NewestReviewSeason.apply(lambda x: get_season(x))","0c445418":"#  'TxtReviewsCount' - \u043a-\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432, \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043d\u043e \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0443\u0442\ndf['TxtReviewsCount'] = df.Reviews.apply(lambda x: len(get_reviews(x)['reviews_txt']))","464a25c4":"# \u041a-\u0432\u043e \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\ndf['PositiveWords'] = df.Reviews.apply(lambda x: count_positive_words_proportion(x))\n# \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\ndf['PositiveWordsList'] = df.Reviews.apply(lambda x: list_positive_words(x))","d217a65c":"cusines_in_city={}\ncusines_count_in_city={}\nfor city_name, group in df.groupby('City'):\n    #\u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0441\u0435\u0440\u0438\u044e \u0441\u043e \u0441\u043f\u0438\u0441\u043a\u0430\u043c\u0438 \u043a\u0443\u0445\u043e\u043d\u044c \u0434\u043b\u044f \u0433\u0440\u0443\u043f\u043f\n    cusines = group['Cuisine_Style'].apply(get_cuisines)\n    #\u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0433\u0440\u0443\u043f\u043f\u0435\n    cusines_list = list(itertools.chain.from_iterable(cusines))\n    #\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0438 \u0437\u0430\u043f\u0438\u0448\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u0443\u0445\u043d\u0438 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 Counter\n    cusines_in_city[city_name] = Counter(cusines_list)    \n\nfor city_name in cusines_in_city.keys():\n    cusines_count_in_city[city_name] = len(cusines_in_city[city_name])\n  \ndf['Cusines_Count_In_City'] = df.City.map(cusines_count_in_city)","30c416f3":"# \u041e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0434\u043e\u043b\u044f \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435\ndf['Weighed_Cuisines_Count'] = df['Cuisines_Count'] \/ df['Cusines_Count_In_City']  ","ad2a67b5":"# \u0421\u0430\u043c\u0430\u044f \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u043a\u0443\u0445\u043d\u044f \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\ndf['Most_Common_Cusine_in_City'] = df['City'].apply(lambda x: cusines_in_city[x].most_common(1)[0][0])\n# \u0417\u0430\u043c\u0435\u043d\u0438\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u043d\u0430 \u0441\u0430\u043c\u0443\u044e \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0443\u044e\ndf['Cuisine_Style'] = df.apply(lambda x: x['Cuisine_Style'] if x['Cuisine_Style_NAN'] ==False else [x['Most_Common_Cusine_in_City']], axis=1)","83c843db":"# \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043c\u0435\u0441\u0442\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435 \u043e\u0442 \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u044f\ndf['Weighed_Rank_by_Population'] = df['Weighed_Rank']  \/ df['Population'] ","a6f25f74":"df['PositiveWords_in_Reviews'] = df['PositiveWords'] \/ df['Number_of_Reviews']","9e0c062a":"# \u041a\u0430\u043a \u0447\u0430\u0441\u0442\u043e \u0432 \u0433\u043e\u0440\u043e\u0434\u0435 \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0442\u0437\u044b\u0432\u044b\ndf['NRP'] = df['Number_of_Reviews'] \/ df['Population']","df8e3b3d":"# \u0420\u0430\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0447\u0430\u0441\u0442\u043e\u0442\u044b \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\ndf['WRR'] =  df['Weighed_Rank']  *  df['NRP'] \ndf['Relative_Price_Range'] = df['Price_Range'] \/ df['Weighed_Rank']","79f2fabc":"# \u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0446\u0435\u043d\u0430 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\nprice_in_city_dict = df.groupby('City')['Price_Range'].mean().to_dict()\ndf['Price_in_City'] = df['City'].map(price_in_city_dict)","651ff9f1":"# \u0421\u043e\u043a\u0440\u0430\u0449\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u043e N - \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445, \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 Other\nN=30 #!!!\n\ns = df['Cuisine_Style'].apply(lambda x: get_cuisines(x))\nslist =[]\nfor x in s:\n    slist.extend(x)\ntopNcusines = set(pd.Series(slist).value_counts()[:N].index)  \ndf['Cuisine_top_N'] =df['Cuisine_Style'].apply(lambda x: is_cuisine_top_N(x))","7c90bb54":"# \u0421\u0435\u0442\u0435\u0432\u043e\u0439 ID \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 (\u043f\u043e\u0445\u043e\u0436\u0435, \u0447\u0442\u043e \u044d\u0442\u043e ID \u0444\u0440\u0430\u043d\u0448\u0438\u0437\u044b, \u0435\u0441\u043b\u0438 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u0435\u0435 1-\u0433\u043e \u0440\u0430\u0437\u0430 - isNetworkRestorant)\nimport warnings; warnings.simplefilter('ignore')\ndf['Restaurant_net_id'] = df['Restaurant_id'].apply(lambda x: x.split('_')[1])\nNetworkRestorants = df[df['Restaurant_net_id'].isin(df['Restaurant_net_id'].value_counts()[df['Restaurant_net_id'].value_counts()>2].index)]\nNetworkRestorants['isNetworkRestorant'] = True\ndf['isNetworkRestorant'] = NetworkRestorants['isNetworkRestorant']\ndf['isNetworkRestorant'] = df['isNetworkRestorant'].fillna(False)","392f112f":"# \u041f\u043e\u043c\u0435\u0447\u0430\u0435\u043c, \u0432\u0445\u043e\u0434\u0438\u0442 \u043b\u0438 \u0433\u043e\u0440\u043e\u0434 \u0432 N-top \u0433\u043e\u0440\u043e\u0434\u043e\u0432, \u0435\u0441\u043b\u0438 \u0414\u0410, \u0442\u043e \u043f\u0438\u0448\u0435\u043c \u0435\u0433\u043e \u043d\u0430\u0437\u0430\u0432\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u041d\u0415\u0422 -Other\ntop_Cityes = df['City'].value_counts()[0:10].index.to_list()\ndf['TopCityes'] = df.City.apply(lambda x: x if x in top_Cityes else 'Other_City')","cab79eb1":"df[['Ranking', 'Rating', 'Number_of_Reviews', 'City', 'Price_Range',\n        'Restaurant_id',  'Country',\n       'Restaurants_Count', 'Population', 'Restaurants_for_Population',\n       'Weighed_Rank',  'Cuisines_Count',\n       'RevTimeDelta', 'NewestReviewDate',\n       'TxtReviewsCount', 'Weighed_Rank_by_Population', 'PositiveWords','PositiveWords_in_Reviews',\n        'WRR', 'Price_in_City', 'Restaurant_net_id']].hist(figsize=(20, 20), bins=100);\nplt.tight_layout()","d7787f65":"table_corr = abs(df.drop(['Train'],axis=1).corr().round(2))\nplt.rcParams['figure.figsize'] = (30,30)\nsns.heatmap(table_corr, vmin=0, vmax=1, cmap=\"cool\", linewidths=1,annot=True)","200ba924":"# \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c Dummies: city, price_range, country_range, Cuisine top N\n\ndff = pd.get_dummies(df['Cuisine_top_N'].apply(pd.Series).stack()).sum(level=0)\ndf_mcc = pd.get_dummies(df['Most_Common_Cusine_in_City'], prefix = 'MCC_')\ndf_city_dum = pd.get_dummies(df['City'], prefix = 'City_')\ndf_city_Top = pd.get_dummies(df['TopCityes'], prefix = 'City_Top', dummy_na=True)\ndf_price_range = pd.get_dummies(df['Price_Range'], prefix = 'Price_') \ndf_country_range = pd.get_dummies(df['Country'], prefix = 'Country_') \ndf_season_range = pd.get_dummies(df['NewestReviewSeason'], prefix = 'Season')\n\ndf['PositiveWordsList'] = df['PositiveWordsList'].fillna('PositiveWords_NAN')\ndf_positive_words_range = pd.get_dummies(df['PositiveWordsList'].apply(pd.Series).stack(), dummy_na=False).sum(level=0)\n\ndf1 = pd.concat([df,dff], axis=1)\ndf1 = pd.concat([df1,df_city_dum], axis=1)\ndf1 = pd.concat([df1,df_price_range], axis=1)\ndf1 = pd.concat([df1,df_country_range], axis=1)\ndf1 = pd.concat([df1,df_mcc], axis=1)\ndf1 = pd.concat([df1,df_positive_words_range], axis=1)\n\n#df1 = pd.concat([df1,df_season_range], axis=1)\n\ncols_cuisine_style = dff.columns\ncols_city = df_city_dum.columns\ncols_price_range =  df_price_range.columns\ncols_country_range =  df_country_range.columns\ncols_mcc =  df_mcc.columns\ncols_positive_words = df_positive_words_range.columns\n#cols_season = df_season_range.columns","4950cf40":"# \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0418 \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043d\u0430 \u0447\u0430\u0441\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\ncolumns = [ 'isMultyCusine', 'Price_Range_NaN', 'Cuisine_Style_NAN', 'Number_of_Reviews_NAN', 'Number_of_Reviews',\n     'Restaurants_Count', 'Cuisines_Count', 'RevTimeDelta', 'NewestReviewDate', 'PositiveWords',  'Weighed_Rank',\n     'Ranking', 'Weighed_Rank_by_Population',  'Cusines_Count_In_City', 'WRR', 'ID_TA', 'Weighed_Rank_min_max',\n     'Price_in_City', 'isCapital', 'Population', 'Restaurants_for_Population' ]\n\ncolumns.extend(cols_price_range.tolist())\ncolumns.extend(cols_cuisine_style.tolist())\ncolumns.extend(cols_city.tolist())\ncolumns.extend(cols_country_range.tolist())\ncolumns.extend(cols_positive_words.tolist())\n\n#\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043d\u0430 \u0447\u0430\u0441\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nX = df1[df1.Train][columns]\n\ny = df1[df1.Train]['Rating']","82b5b74f":"class preprocessor():\n    def __init__(self, df_Train, df_Test, Sample_Submission):\n        '''\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438'''\n        self.df_train = df_Train\n        self.df_test = df_Test\n        self.sample_submission = Sample_Submission\n        self.pos_words_list = df_pos_words[0].to_list()\n        self.neg_words_list = df_neg_words[0].to_list()\n        self.allCusines = []\n        self.df_train['Train'] = True # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\n        self.df_test['Train'] = False # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\n        self.df_test['Rating'] = 0 # \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f Rating, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e \u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\n        self.data = df_test.append(df_train, sort=False).reset_index(drop=True) # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c\n        self.data.columns = list(map(lambda x: x.replace(\" \",\"_\"), data.columns))\n        self.df = data.drop('URL_TA',axis=1).copy()\n\n    \n    def get_df(self):\n        '''\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442'''\n        return self.df\n\n    def clean_and_fill(self):\n        '''\u041e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442'''\n        self.df['ID_TA'] = self.df['ID_TA'].apply(lambda x: x[1:]).astype(int)\n\n        # Reviews - \u0443\u0431\u0438\u0440\u0430\u0435\u043c NaN \u0438 \"\u043f\u0440\u0438\u0447\u0435\u0441\u044b\u0432\u0430\u0435\u043c\" \u0442\u0435\u043a\u0441\u0442 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \n        self.df['Reviews_txt_NaN'] = self.df['Reviews'].apply(lambda x: x ==  '[[], []]')\n        self.df['Reviews'] = self.df['Reviews'].fillna('[[], []]')\n        self.df['Reviews'] = self.df['Reviews'].apply(lambda x: cleanup_string(x))\n\n        # Number of Reviews\n        self.df['Number_of_Reviews_NAN'] = self.df['Number_of_Reviews'].isna()\n        replace_val = df['Number_of_Reviews'].mean()\n        replace_val = np.round(replace_val)\n        self.df['Number_of_Reviews'] = self.df['Number_of_Reviews'].fillna(replace_val)\n\n        # \u041f\u0435\u0440\u0435\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c Price Range \u0438 \u0443\u0434\u0430\u043b\u044f\u0435\u043c NaN\n        cleanup_nums = {'Price_Range': {\"$\": 1, \"$$ - $$$\": 2, \"$$$$\": 3, np.NaN: 2}}\n        # \u0447\u0430\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f \"$$ - $$$\", \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u043b\u0438 \u043d\u0430 2\n        self.df['Price_Range_NaN'] = self.df['Price_Range'].isna()\n        self.df.replace(cleanup_nums, inplace=True)\n\n        # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c Cuisines Count, \u0441\u0430\u043c\u0443\u044e \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0443\u044e \u043a\u0443\u0445\u043d\u044e, \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043a-\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0438 \u0443\u0441\u0442\u0440\u0430\u043d\u044f\u0435\u043c NaN\n        self.df['Cuisine_Style_NAN'] = self.df['Cuisine_Style'].isna()\n        self.df['Cuisine_Style'] = self.df['Cuisine_Style'].fillna('NaN')\n        self.df['Cuisines_Count'] = self.df.apply(cuisine_styles_count, axis=1)\n        self.most_popular_cusine = pd.Series(allCusines).value_counts().index[0]\n        self.average_cousines_count = np.round(df['Cuisines_Count'].mean())\n\n    def feature_engineering(self):\n        '''\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0432 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442'''\n        population_city_dict = {}\n        country_city_dict = {}\n        \n        # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0438 \u043f\u043e\u043f\u0443\u043b\u044f\u0446\u0438\u0438 \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c, \u0430 \u0442\u0430\u043a \u0436\u0435 ISO \u043a\u043e\u0434 \u0441\u0442\u0440\u0430\u043d\u044b \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443\n        population_city_dict, country_city_dict = get_city_population_and_country(self.df, df_city)\n\n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u0441\u0442\u0440\u0430\u043d\u0443 \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\n        self.df['Country'] = self.df[\"City\"].apply(lambda x: country_city_dict[x])\n\n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043a-\u0432\u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\n        restorants_in_city = self.df.groupby('City')['Ranking'].count().to_dict()\n        self.df['Restaurants_Count'] = self.df['City'].map(restorants_in_city)\n\n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u0435 (\u0432 \u0442\u044b\u0441. \u0447\u0435\u043b) \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\n        self.df['Population'] = self.df[\"City\"].map(population_city_dict)\n\n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043a-\u0432\u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u043d\u0430 1000 \u0447\u0435\u043b \u0434\u043b\u044f \u0433\u043e\u0440\u043e\u0434\u0430 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\n        self.df['Restaurants_for_Population'] = self.df.Restaurants_Count \/ (self.df['Population']*1000)\n\n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0438 \u0433\u043e\u0440\u043e\u0434 \u0441\u0442\u043e\u043b\u0438\u0446\u0435\u0439 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435\n        capital_city_dict = get_capital_city_dict(self.df, df_city)\n        self.df['isCapital'] = self.df[\"City\"].apply(lambda x: capital_city_dict[x])\n\n        # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u043f\u043e\u0437\u0438\u0446\u0438\u044e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0441\u0440\u0435\u0434\u0438 \u0432\u0441\u0435\u0445 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0433\u043e\u0440\u043e\u0434\u0430\n        self.df['Weighed_Rank'] = self.df.apply(lambda x: get_Weighed_Rank_RK(x), axis=1)\n\n        CityMinMax = self.df.groupby('City')['Ranking'].agg([min,max])\n        CityMinMax = CityMinMax.reset_index()\n        self.df['Weighed_Rank_min_max'] = self.df.apply(lambda x: get_Weighed_Rank(CityMinMax, x), axis=1)\n\n        # \u0424\u043b\u0430\u0433\u0438 (1\/0) isMostPopCusine - \u0435\u0441\u0442\u044c \u043b\u0438 \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0441\u0430\u043c\u0430\u044f \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u043a\u0443\u0445\u043d\u044f; isMultyCusine - \u043a-\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u0438\u043b\u0438 \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0436\u0435 \u0447\u0435\u043c \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c\n        self.df['isMostPopCusine'] = self.df.Cuisine_Style.apply(lambda x: 1 if self.most_popular_cusine in x else 0 )\n        self.df['isMultyCusine'] = self.df.Cuisines_Count.apply(lambda x: 1 if  x >= self.average_cousines_count else 0 )\n\n        # RevTimeDelta - \u0432\u0440\u0435\u043c\u044f \u043c\u0435\u0436\u0434\u0443 review \u0432 \u0434\u043d\u044f\u0445\n        self.df['RevTimeDelta'] = self.df.Reviews.apply(rev_time_delta)\n\n        # NewestReviewDate - \u0432\u0440\u0435\u043c\u044f, \u043f\u0440\u043e\u0448\u0435\u0434\u0448\u0435\u0435 \u0441\u043e \u043c\u043e\u043c\u0435\u043d\u0442\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e review\n        self.df['NewestReviewDate'] = self.df.Reviews.apply(lambda x: get_reviews(x)['reviews_dt'])\n        self.df['NewestReviewDate'] = self.df.NewestReviewDate.apply(lambda x: sorted(x,reverse=True)[0] if len(x)!=0 else pd.NaT)\n        self.df['NewestReviewDate'] = self.df.NewestReviewDate.fillna(dt.date(1970,1,1))\n        self.df['NewestReviewDate'] = self.df.NewestReviewDate.apply(lambda x: (CURRENT_DATE.date()-x).total_seconds()\/\/86400)\n\n        self.df['NewestReviewSeason'] = self.df.Reviews.apply(lambda x: get_reviews(x)['reviews_dt'])\n        self.df['NewestReviewSeason'] = self.df.NewestReviewSeason.apply(lambda x: sorted(x,reverse=True)[0] if len(x)!=0 else pd.NaT)\n        self.df['NewestReviewSeason'] = self.df.NewestReviewSeason.apply(lambda x: get_season(x))\n\n        #  'TxtReviewsCount' - \u043a-\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432, \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043d\u043e \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0443\u0442\n        self.df['TxtReviewsCount'] = self.df.Reviews.apply(lambda x: len(get_reviews(x)['reviews_txt']))\n\n        # \u041a-\u0432\u043e \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n        self.df['PositiveWords'] = self.df.Reviews.apply(lambda x: count_positive_words_proportion(x))\n        # \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n        self.df['PositiveWordsList'] = self.df.Reviews.apply(lambda x: list_positive_words(x))\n        self.df['PositiveWordsList'] = self.df['PositiveWordsList'].fillna('PositiveWords_NAN')\n        self.df['PositiveWords_in_Reviews'] = self.df['PositiveWords'] \/ df['Number_of_Reviews']\n\n        # \u041a-\u0432\u043e \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n        self.df['NegativeWords'] = self.df.Reviews.apply(lambda x: count_negative_words_proportion(x))\n        # \u0421\u043f\u0438\u0441\u043e\u043a \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445\n        self.df['NegativeWordsList'] = self.df.Reviews.apply(lambda x: list_negative_words(x))\n        self.df['NegativeWordsList'] = self.df['NegativeWordsList'].fillna('NegativeWords_NAN')\n        self.df['NegativeWords_in_Reviews'] = self.df['NegativeWords'] \/ df['Number_of_Reviews']\n\n        cusines_in_city={}\n        cusines_count_in_city={}\n        for city_name, group in self.df.groupby('City'):\n            #\u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0441\u0435\u0440\u0438\u044e \u0441\u043e \u0441\u043f\u0438\u0441\u043a\u0430\u043c\u0438 \u043a\u0443\u0445\u043e\u043d\u044c \u0434\u043b\u044f \u0433\u0440\u0443\u043f\u043f\n            cusines = group['Cuisine_Style'].apply(get_cuisines)\n            #\u041f\u043e\u043b\u0443\u0447\u0438\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0433\u0440\u0443\u043f\u043f\u0435\n            cusines_list = list(itertools.chain.from_iterable(cusines))\n            #\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0438 \u0437\u0430\u043f\u0438\u0448\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u0443\u0445\u043d\u0438 \u0432 \u043a\u0430\u0436\u0434\u043e\u0439 \u0433\u0440\u0443\u043f\u043f\u0435 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 Counter\n            cusines_in_city[city_name] = Counter(cusines_list)    \n        for city_name in cusines_in_city.keys():\n            cusines_count_in_city[city_name] = len(cusines_in_city[city_name])\n        self.df['Cusines_Count_In_City'] = self.df.City.map(cusines_count_in_city)\n\n        # \u041e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0434\u043e\u043b\u044f \u043a\u0443\u0445\u043e\u043d\u044c \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435\n        self.df['Weighed_Cuisines_Count'] = self.df['Cuisines_Count'] \/ self.df['Cusines_Count_In_City']  \n        # \u0421\u0430\u043c\u0430\u044f \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0430\u044f \u043a\u0443\u0445\u043d\u044f \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\n        self.df['Most_Common_Cusine_in_City'] = self.df['City'].apply(lambda x: cusines_in_city[x].most_common(1)[0][0])\n        # \u0417\u0430\u043c\u0435\u043d\u0438\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u043d\u0430 \u0441\u0430\u043c\u0443\u044e \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u0443\u044e\n        self.df['Cuisine_Style'] = self.df.apply(lambda x: x['Cuisine_Style'] if x['Cuisine_Style_NAN'] ==False else [x['Most_Common_Cusine_in_City']], axis=1)\n\n        # \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043c\u0435\u0441\u0442\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435 \u043e\u0442 \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u044f\n        self.df['Weighed_Rank_by_Population'] = self.df['Weighed_Rank']  \/ self.df['Population'] \n\n\n        # \u041a\u0430\u043a \u0447\u0430\u0441\u0442\u043e \u0432 \u0433\u043e\u0440\u043e\u0434\u0435 \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u043e\u0442\u0437\u044b\u0432\u044b\n        self.df['NRP'] = self.df['Number_of_Reviews'] \/ self.df['Population']\n        # \u0420\u0430\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0447\u0430\u0441\u0442\u043e\u0442\u044b \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\n        self.df['WRR'] =  self.df['Weighed_Rank']  *  self.df['NRP'] \n\n        self.df['Relative_Price_Range'] = self.df['Price_Range'] \/ self.df['Weighed_Rank']\n\n        # \u0421\u0440\u0435\u0434\u043d\u044f\u044f \u0446\u0435\u043d\u0430 \u0432 \u0433\u043e\u0440\u043e\u0434\u0435\n        price_in_city_dict = self.df.groupby('City')['Price_Range'].mean().to_dict()\n        self.df['Price_in_City'] = self.df['City'].map(price_in_city_dict)\n\n        # \u0421\u043e\u043a\u0440\u0430\u0449\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043a\u0443\u0445\u043e\u043d\u044c \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0434\u043e N - \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445, \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 Other\n        N=30 #!!!\n\n        s = self.df['Cuisine_Style'].apply(lambda x: get_cuisines(x))\n        slist =[]\n        for x in s:\n            slist.extend(x)\n        self.topNcusines = set(pd.Series(slist).value_counts()[:N].index)  \n        self.df['Cuisine_top_N'] =self.df['Cuisine_Style'].apply(lambda x: is_cuisine_top_N(x))\n\n\n\n        # \u0421\u0435\u0442\u0435\u0432\u043e\u0439 ID \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 (\u043f\u043e\u0445\u043e\u0436\u0435, \u0447\u0442\u043e \u044d\u0442\u043e ID \u0444\u0440\u0430\u043d\u0448\u0438\u0437\u044b, \u0435\u0441\u043b\u0438 \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u0442\u0441\u044f \u0431\u043e\u043b\u0435\u0435 1-\u0433\u043e \u0440\u0430\u0437\u0430 - isNetworkRestorant)\n        import warnings; warnings.simplefilter('ignore')\n        self.df['Restaurant_net_id'] = self.df['Restaurant_id'].apply(lambda x: x.split('_')[1])\n        NetworkRestorants = self.df[self.df['Restaurant_net_id'].isin(self.df['Restaurant_net_id'].value_counts()[self.df['Restaurant_net_id'].value_counts()>2].index)]\n        NetworkRestorants['isNetworkRestorant'] = True\n        self.df['isNetworkRestorant'] = NetworkRestorants['isNetworkRestorant']\n        self.df['isNetworkRestorant'] = self.df['isNetworkRestorant'].fillna(False)\n\n        # \u041f\u043e\u043c\u0435\u0447\u0430\u0435\u043c, \u0432\u0445\u043e\u0434\u0438\u0442 \u043b\u0438 \u0433\u043e\u0440\u043e\u0434 \u0432 N-top \u0433\u043e\u0440\u043e\u0434\u043e\u0432, \u0435\u0441\u043b\u0438 \u0414\u0410, \u0442\u043e \u043f\u0438\u0448\u0435\u043c \u0435\u0433\u043e \u043d\u0430\u0437\u0430\u0432\u043d\u0438\u0435, \u0435\u0441\u043b\u0438 \u041d\u0415\u0422 -Other\n        top_Cityes = self.df['City'].value_counts()[0:10].index.to_list()\n        self.df['TopCityes'] = self.df.City.apply(lambda x: x if x in top_Cityes else 'Other_City')\n    \n    def dummies(self):\n        '''\u0414\u043e\u0431\u0430\u0432\u0438\u0442\u044c dummies \u0432 \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442'''\n\n        df_Cuisine_top_N = pd.get_dummies(self.df['Cuisine_top_N'].apply(pd.Series).stack()).sum(level=0)\n        df_mcc = pd.get_dummies(self.df['Most_Common_Cusine_in_City'], prefix = 'MCC')\n        df_city_dum = pd.get_dummies(self.df['City'], prefix = 'City')\n        df_price = pd.get_dummies(self.df['Price_Range'], prefix = 'Price') \n        df_country = pd.get_dummies(self.df['Country'], prefix = 'Country') \n        df_season = pd.get_dummies(df['NewestReviewSeason'], prefix = 'Season')\n        df_positive_words = pd.get_dummies(self.df['PositiveWordsList'].apply(pd.Series).stack(), dummy_na=False).sum(level=0)\n        df_negative_words = pd.get_dummies(self.df['NegativeWordsList'].apply(pd.Series).stack(), dummy_na=False).sum(level=0)\n\n\n        #object_columns = []\n        #for s in (self.df.columns):\n        #    if self.df[s].dtypes == 'object':\n        #        object_columns.append(s)\n        #        print(s, self.df[s].dtypes)\n        #object_columns = [s for s in self.df.columns if (self.df[s].dtypes == 'object')]\n\n        self.df = pd.concat([self.df, df_Cuisine_top_N], axis=1)\n        self.df = pd.concat([self.df, df_city_dum], axis=1)\n        self.df = pd.concat([self.df, df_price], axis=1)\n        self.df = pd.concat([self.df, df_country], axis=1)\n        self.df = pd.concat([self.df, df_mcc], axis=1)\n        self.df = pd.concat([self.df, df_positive_words], axis=1)\n        self.df = pd.concat([self.df, df_negative_words], axis=1)\n        self.df = pd.concat([self.df, df_season], axis=1)\n\n        self.cols_cuisine_style = df_Cuisine_top_N.columns\n        self.cols_city = df_city_dum.columns\n        self.cols_price =  df_price.columns\n        self.cols_country =  df_country.columns\n        self.cols_mcc =  df_mcc.columns\n        self.cols_positive_words = df_positive_words.columns\n        self.cols_negative_words = df_negative_words.columns\n        self.cols_season = df_season.columns\n      \n        #object_columns = self.df.dtypes[self.df.dtypes == 'object'].keys()\n        #self.df.drop(object_columns, axis = 1, inplace=True)\n\n        # \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0418 \u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043d\u0430 \u0447\u0430\u0441\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n        self.columns = [ 'isMultyCusine', 'Price_Range_NaN', 'Cuisine_Style_NAN', 'Number_of_Reviews_NAN', 'Number_of_Reviews',\n             'Restaurants_Count', 'Cuisines_Count', 'RevTimeDelta', 'NewestReviewDate', 'PositiveWords',  'Weighed_Rank',\n             'Ranking', 'Weighed_Rank_by_Population',  'Cusines_Count_In_City', 'WRR', 'ID_TA', 'Weighed_Rank_min_max',\n             'Price_in_City', 'isCapital', 'Population', 'Restaurants_for_Population', \n             'Reviews_txt_NaN','isMostPopCusine', 'TxtReviewsCount', 'NegativeWords', 'NegativeWords_in_Reviews', 'PositiveWords_in_Reviews',\n             'Weighed_Cuisines_Count', 'Relative_Price_Range',  'isNetworkRestorant' ]\n        \n        self.columns.extend(self.cols_price.tolist())\n        self.columns.extend(self.cols_cuisine_style.tolist())\n        self.columns.extend(self.cols_city.tolist())\n        self.columns.extend(self.cols_country.tolist())\n        self.columns.extend(self.cols_mcc.tolist())\n        self.columns.extend(self.cols_positive_words.tolist())\n        self.columns.extend(self.cols_negative_words.tolist())\n        self.columns.extend(self.cols_season.tolist())\n\n    def Run(self):\n        '''\u0412\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0432\u0441\u0435 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u044f'''\n        self.clean_and_fill()\n        self.feature_engineering()\n        self.dummies()\n    \n    def get_X_y(self):\n        '''\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043d\u0430 \u0447\u0430\u0441\u0442\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438'''\n        X = self.df[self.df.Train][self.columns]\n        y = self.df[self.df.Train]['Rating']\n        return (X,y)\n","9675c00b":"prepro = preprocessor(df_train, df_test, sample_submission)\nprepro.Run()\ndf1 = prepro.df\nX, y = prepro.get_X_y()\ncolumns = prepro.columns","d26163c7":"x_cols = set(X.columns)\nf_cols = set(df1.columns)\nf_cols - x_cols","1788171e":"# \u041d\u0430\u0431\u043e\u0440\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u043c\u0435\u0442\u043a\u043e\u0439 \"train\" \u0431\u0443\u0434\u0443\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \"test\" - \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.\n# \u0414\u043b\u044f \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c 20% \u043e\u0442 \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n\n# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u043e\u0434\u0435\u043b\u044c (\u041d\u0410\u0421\u0422\u0420\u041e\u0419\u041a\u0418 \u041d\u0415 \u0422\u0420\u041e\u0413\u0410\u0415\u041c)\nregr = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nregr.fit(X_train, y_train)\n\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0430 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.\n# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e y_pred\ny_pred = regr.predict(X_test)\ny_pred_old = y_pred.copy()\ny_pred = round_of_rating(y_pred) \n\n# \u0421\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (y_pred) \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438 (y_test), \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043e\u043d\u0438 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f\n# \u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f Mean Absolute Error (MAE) \u0438 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0442 \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445.\nprint('MAE \u0441 \u043e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u0435\u043c:', metrics.mean_absolute_error(y_test, y_pred), 'MAE \u0431\u0435\u0437 \u043e\u043a\u0440\u0443\u0433\u043b\u0435\u043d\u0438\u044f:', metrics.mean_absolute_error(y_test, y_pred_old) )","f777ef34":"plt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(regr.feature_importances_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh')","ea081165":"# \u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0438 \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 (Train == False)\nX_submission = df1[df1.Train == False][columns]\ny_pred_submission = round_of_rating(regr.predict(X_submission))","a8334e84":"# \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438 Restaurant_id -- Rating\nsubmission_df = pd.DataFrame()\nsubmission_df['Restaurant_id'] = df1[df1.Train == False]['Restaurant_id']\nsubmission_df['Rating'] = y_pred_submission\nsubmission_df.head(15)","e37170fd":"print('\u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442')\ndf[df.Train].Rating.value_counts()","e4b70ec1":"print('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0432 4 \u0440\u0430\u0437\u0430')\nsubmission_df.Rating.value_counts()*4","56b92da0":"h = df[df.Train].Rating.hist()\nf = submission_df.Rating.hist()\nfig = h.get_figure()\nfig = f.get_figure()","62db1589":"# \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043f\u043e\u0445\u043e\u0436\u0438 - \u041e\u041a!\n# \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\nsubmission_df.to_csv('submission.csv', index=False)","9476dd1d":"# 3. FUNC","6b436f40":"## \u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c dummies","bf642f62":"## 6.2 \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439","2c6466cb":"**\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432**","7c658e33":"\u041f\u0435\u0440\u0432\u043e\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0434\u0435\u0441\u044f\u0442\u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0445 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e:\n\n- **Restaurant_id** \u2014 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \/ \u0441\u0435\u0442\u0438 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432;\n- **City** \u2014 \u0433\u043e\u0440\u043e\u0434, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u0441\u044f \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d;\n- **Cuisine Style** \u2014 \u043a\u0443\u0445\u043d\u044f \u0438\u043b\u0438 \u043a\u0443\u0445\u043d\u0438, \u043a \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043c\u043e\u0436\u043d\u043e \u043e\u0442\u043d\u0435\u0441\u0442\u0438 \u0431\u043b\u044e\u0434\u0430, \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u043c\u044b\u0435 \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435;\n- **Ranking** \u2014 \u043c\u0435\u0441\u0442\u043e, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0437\u0430\u043d\u0438\u043c\u0430\u0435\u0442 \u0434\u0430\u043d\u043d\u044b\u0439 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d \u0441\u0440\u0435\u0434\u0438 \u0432\u0441\u0435\u0445 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432 \u0441\u0432\u043e\u0435\u0433\u043e \u0433\u043e\u0440\u043e\u0434\u0430;\n- **Rating** \u2014 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c TripAdvisor (\u0438\u043c\u0435\u043d\u043d\u043e \u044d\u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c);\n- **Price Range** \u2014 \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d \u0446\u0435\u043d \u0432 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435;\n- **Number of Reviews** \u2014 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0435;\n- **Reviews** \u2014 \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u0434\u0432\u0443\u0445 \u043e\u0442\u0437\u044b\u0432\u0430\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430;\n- **URL_TA** \u2014 URL \u0441\u0442\u0440\u0430\u043d\u0438\u0446\u044b \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u043d\u0430 TripAdvisor;\n- **ID_TA** \u2014 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432 \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 TripAdvisor.","83c31c52":"# 9. Class","d0fd1b4c":"# 7. \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043d\u043e\u0432\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u043f\u043e\u043a\u0430 \u0431\u0435\u0437 dummies)\n\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0432\u043d\u0435\u0448\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435","18f52315":"# 2. DATA","8256a4b9":"# 10. Model ","6a84087f":"# 8. \u041e\u0442\u0441\u043c\u043e\u0442\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","46273d2f":"# 5. \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 NAN \u0438 \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","6381bd0b":"# \u041f\u0440\u043e\u0435\u043a\u0442 \u21163. \u041e \u0432\u043a\u0443\u0441\u043d\u043e\u0439 \u0438 \u0437\u0434\u043e\u0440\u043e\u0432\u043e\u0439 \u043f\u0438\u0449\u0435 \n**[DSPR-60] SF Predict TripAdvisor Rating Matsera Maxim**","6d4f7c8d":"## 6.1 \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","6e2c8b09":"# 11. Submission","e8c23691":"**\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u043a-\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439**","24267e6e":"\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e Ranking \u0438\u043c\u0435\u0435\u0442 \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435, \u043f\u0440\u043e\u0441\u0442\u043e \u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0433\u043e\u0440\u043e\u0434\u0430\u0445 \u0431\u043e\u043b\u044c\u0448\u0435 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432, \u0438\u0437-\u0437\u0430 \u043c\u044b \u044d\u0442\u043e\u0433\u043e \u0438\u043c\u0435\u0435\u043c \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0435.  \n\u0414\u043b\u044f \u0443\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c Ranking, \u0442\u043e \u0435\u0441\u0442\u044c \u043f\u0440\u043e\u0432\u0435\u0434\u0451\u043c min-max \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 (Min-Max Scaling)","cb3f8123":"\u0423 \u043d\u0430\u0441 \u043c\u043d\u043e\u0433\u043e \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0434\u043e\u0442\u044f\u0433\u0438\u0432\u0430\u044e\u0442 \u0438 \u0434\u043e 2500 \u043c\u0435\u0441\u0442\u0430 \u0432 \u0441\u0432\u043e\u0435\u043c \u0433\u043e\u0440\u043e\u0434\u0435, \u0430 \u0447\u0442\u043e \u0442\u0430\u043c \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c?","f0759070":"\u041f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u043c, \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u0443\u044e\u0449\u0438\u043c \u0441 Rating \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f Ranking. \u041e\u043d, \u0432 \u0441\u0432\u043e\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c, \u0443\u0436\u0435 \u0438\u043c\u0435\u0435\u0442 \u0441\u043b\u0430\u0431\u0443\u044e \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044e \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0441\u043e \u0432\u0441\u0435\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 ","4e72658a":"\u041a\u0430\u043a \u0432\u0438\u0434\u0438\u043c, \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u043e\u0447\u0438\u0441\u0442\u043a\u0438 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438.","8380bed8":"**\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u043a-\u0432\u043e \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 (NaN)**","b21ebe21":"## 6.3 \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430","c5cc086b":"# 6. EDA \u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445\n\u041d\u0430 \u044d\u0442\u043e\u043c \u044d\u0442\u0430\u043f\u0435 \u043c\u044b \u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438, \u0438\u0449\u0435\u043c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438, \u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0438, \u0432\u044b\u0431\u0440\u043e\u0441\u044b \u0438\u043b\u0438 \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438.\n\u0412 \u043e\u0431\u0449\u0435\u043c \u0446\u0435\u043b\u044c \u044d\u0442\u043e\u0433\u043e \u044d\u0442\u0430\u043f\u0430 \u043f\u043e\u043d\u044f\u0442\u044c, \u0447\u0442\u043e \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u043d\u0430\u043c \u0434\u0430\u0442\u044c \u0438 \u043a\u0430\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0432\u0437\u0430\u0438\u043c\u043e\u0441\u0432\u044f\u0437\u0430\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439.\n\u041f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435, \u0431\u043e\u043b\u0435\u0435 \u0441\u0438\u043b\u044c\u043d\u044b\u0435 \u0438, \u0442\u0435\u043c \u0441\u0430\u043c\u044b\u043c, \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0430\u0448\u0443 \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u0443\u0447\u0448\u0435.","1e635f44":"# 4. Data Info","292c1a34":"\u0420\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u044b \u0441 \u043c\u0435\u043d\u044c\u0448\u0438\u043c \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u043e\u043c \u0440\u0430\u0441\u043f\u043e\u043b\u0430\u0433\u0430\u044e\u0442\u0441\u044f \u043d\u0430 \u0431\u043e\u043b\u0435\u0435 \u0445\u0443\u0434\u0448\u0438\u0445 \u043c\u0435\u0441\u0442\u0430\u0445 \u0432 \u0440\u0435\u0439\u0442\u0438\u043d\u0433\u0435 \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0443, \u0447\u0442\u043e \u043b\u043e\u0433\u0438\u0447\u043d\u043e","d836f9d2":"## \u0412 \u044d\u0442\u043e\u043c \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u043d\u0430\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u043e\u0438\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0440\u0435\u0441\u0442\u043e\u0440\u0430\u043d\u0430 \u0432\u00a0TripAdvisor\n**\u041f\u043e \u0445\u043e\u0434\u0443 \u0437\u0430\u0434\u0430\u0447\u0438:**\n* \u041f\u0440\u043e\u043a\u0430\u0447\u0430\u0435\u043c\u00a0\u0440\u0430\u0431\u043e\u0442\u0443 \u0441 pandas\n* \u041d\u0430\u0443\u0447\u0438\u043c\u0441\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 Kaggle Notebooks\n* \u041f\u043e\u0439\u043c\u0435\u043c \u043a\u0430\u043a \u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n* \u041d\u0430\u0443\u0447\u0438\u043c\u0441\u044f \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 (Nan)\n* \u041f\u043e\u0437\u043d\u0430\u043a\u043e\u043c\u0438\u043c\u0441\u044f \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0432\u0438\u0434\u0430\u043c\u0438 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n* \u041d\u0435\u043c\u043d\u043e\u0433\u043e \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c\u00a0[Feature Engineering](https:\/\/ru.wikipedia.org\/wiki\/\u041a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435_\u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432) (\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438)\n* \u0418 \u0441\u043e\u0432\u0441\u0435\u043c \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0437\u0430\u0442\u0440\u043e\u043d\u0435\u043c ML\n* \u0418 \u043c\u043d\u043e\u0433\u043e\u0435 \u0434\u0440\u0443\u0433\u043e\u0435...   ","e9d9787a":"## 6.4 \u041a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044f \u0438\u043c\u0435\u044e\u0449\u0438\u0445\u0441\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","72dc97a0":"# 1. Import"}}