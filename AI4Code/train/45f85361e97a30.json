{"cell_type":{"ca438392":"code","8da3b070":"code","8a76713f":"code","43c2396e":"code","db0b4229":"code","15252384":"code","efe9339a":"code","9f4122f3":"code","e3ff3801":"code","bab36877":"code","7cf754da":"code","192d60b9":"code","a0beb50d":"code","e0fa659d":"code","26aed632":"code","07a23721":"code","02537389":"code","3f9f71f2":"code","242687d0":"code","2fcd4963":"code","49c6cb84":"code","58185137":"code","3f87c00e":"code","a0589657":"markdown","a4949a70":"markdown","9bf28fe4":"markdown","c14a5d92":"markdown","3a81b999":"markdown","1bf45ea5":"markdown"},"source":{"ca438392":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8da3b070":"\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import f1_score, recall_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter","8a76713f":"data= pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","43c2396e":"data.dropna(inplace= True)","db0b4229":"data.shape","15252384":"print(len(data[data['stroke']==1]))","efe9339a":"# Encoding certain categorical variables\nobtype=['gender','ever_married','work_type','Residence_type','smoking_status']\ngen_encode=LabelEncoder()\nmar_encode=LabelEncoder()\nwork_encode=LabelEncoder()\nres_encode=LabelEncoder()\nsmo_encode=LabelEncoder()\ndata['gender']=gen_encode.fit_transform(data['gender'])\ndata['ever_married']=mar_encode.fit_transform(data['ever_married'])\ndata['work_type']=work_encode.fit_transform(data['work_type'])\ndata['Residence_type']=res_encode.fit_transform(data['Residence_type'])\ndata['smoking_status']=smo_encode.fit_transform(data['smoking_status'])","9f4122f3":"data= data.sample(frac=1).reset_index(drop=True)","e3ff3801":"data.head(3)","bab36877":"y =data['stroke']\nX= data.drop(['id','stroke'], axis=1)","7cf754da":"oversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","192d60b9":"print(len(y), sum(y))","a0beb50d":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=42)\n","e0fa659d":"\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier(n_estimators= 200,objective= 'binary:logistic', seed=42)\nmodel.fit(X_train, y_train)\n#xgb=XGBClassifier()\n#xgb.fit(X_train, y_train )\ny_pred = model.predict(X_test)\nprint(f1_score(y_test, y_pred, average = 'binary'))","26aed632":"import os\nimport numpy as np\nimport pandas as pd\nimport random, math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle as skshuffle\nfrom sklearn.model_selection import train_test_split as sk_train_test_split\nfrom tensorflow.keras.layers import Convolution1D, Dense, Flatten, GlobalAveragePooling1D, AveragePooling2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, GlobalAveragePooling2D, Conv2D\nfrom tensorflow.keras.layers import Dropout, AveragePooling2D, LSTM, GRU\nfrom sklearn.metrics import mean_squared_error\nfrom time import time\nfrom tensorflow.keras.callbacks import TensorBoard\nimport tensorflow as tf\n\nfrom tensorflow.keras import regularizers\n\nos.environ[\"PYTHONHASHSEED\"] = \"0\"\nnp.random.seed(64)\nrandom.seed(64)\ntf.compat.v1.set_random_seed(64)\nfrom keras import backend as K\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph())\nfrom tensorflow.compat.v1.keras import backend as K\nK.set_session(sess)","07a23721":"n_features= X_train.shape[1]\nX_train= np.array(X_train).reshape(len(X_train), 1, n_features)\nX_test= np.array(X_test).reshape(len(X_test), 1, n_features)","02537389":"model2 = Sequential()\nmodel2.add(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[2])))\n# model.add(Dropout(0.2))\nmodel2.add(LSTM(25, return_sequences=True))\nmodel2.add(LSTM(10, return_sequences=False))\n\n\nmodel2.add(Dense(1, activation='sigmoid'))\nfrom tensorflow.keras.optimizers import SGD\n# model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['binary_accuracy'])\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel2.summary()\n# fit model\nmodel2.fit(X_train, y_train, epochs=20, batch_size=10, shuffle=True)","3f9f71f2":"predictions= model2.predict_classes(X_test)","242687d0":"print(f1_score(y_test, predictions, average = 'binary'))","2fcd4963":"X_train = X_train.reshape(len(X_train), n_features)\nX_test= X_test.reshape(len(X_test), n_features)","49c6cb84":"input_shape = (n_features,)\nmodel3= Sequential()\nmodel3.add(Dense(350, input_shape=input_shape, activation='relu'))\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dense(1, activation='sigmoid'))\n\n# Configure the model and start training\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel3.fit(X_train, y_train, epochs=20, batch_size=10, shuffle=True )","58185137":"prediction= model3.predict_classes(X_test)","3f87c00e":"print(f1_score(y_test, prediction, average = 'binary'))","a0589657":"MLP model implementation:","a4949a70":"\n\nIn this note book, I have investigated the performance of three calssifiers (XGB, Stacked LSTM, and MLP) using SMOTE algorithm due to the exsiting imbalanced classe.\n","9bf28fe4":"Generating Train- Test sets: ","c14a5d92":"Applying SMOTE algorithm:","3a81b999":"XGB model implementation:","1bf45ea5":"Stacked LSTM model implementation: "}}