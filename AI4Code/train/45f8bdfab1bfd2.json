{"cell_type":{"7a9f18a3":"code","3bff30d4":"code","2f8c581e":"code","b20769aa":"code","79bad787":"code","fedcba2c":"code","bb7736af":"code","1308a089":"code","609ae0d6":"code","eacb3122":"code","4ebaa8e8":"code","e93e9464":"code","7ac3ca88":"code","f0f6c02d":"code","90025606":"code","ac83f945":"code","ff0edd96":"code","f28cbbfa":"code","8e5228ab":"code","69c0d99b":"code","16aea400":"code","775a3752":"code","c37fb0ec":"code","c84bef0e":"code","402d47d4":"code","90d2dbe3":"code","6d2168f5":"code","d302bb6f":"code","59d81885":"code","679ec5e0":"code","80fe2ee9":"code","76fa278b":"code","2f32ab8c":"code","0d69361f":"code","ad7935c4":"code","4a0168c2":"code","7d9229db":"code","fabaae9c":"code","f6150f9d":"code","de3ddf80":"code","e2d2aef6":"code","b77e3cb0":"code","a402f6ca":"code","f702ca6a":"code","df36b938":"code","a5cd694f":"code","7a17360c":"code","6b23c7f2":"code","b4a353be":"code","a9e3ce54":"code","52b65a78":"code","cc528a24":"code","f1cf0c55":"code","f7a8eeca":"code","0e3bd34f":"code","748dc350":"code","04dc1cb5":"code","d2881004":"code","1cdd2db2":"code","df44cdb4":"code","e5bf55f6":"code","200fb984":"code","04a8b349":"code","51139f98":"code","f4c21a7f":"code","b7167020":"code","fe21f838":"code","f76d6d50":"code","46d3dc84":"code","9d4d46f8":"code","a9bb1cf6":"code","4f564bf8":"code","92f70d2d":"code","d19af862":"code","a15b14a5":"code","2a647f48":"code","97bf9784":"code","e076f60e":"code","84d24f23":"code","81066889":"code","3c7be84f":"code","f6cb5501":"code","82756944":"code","44d92bbe":"code","85370691":"code","7a23dca4":"code","59eb8a75":"code","a8f5e10a":"code","53731af4":"code","2a022dee":"code","027def05":"code","5352336c":"code","80748f87":"code","9bc67dac":"code","9d62f77f":"code","26809de2":"code","deb5416f":"code","589232c4":"code","e3eda065":"code","62619e4a":"code","57dba584":"code","84c6457a":"code","981e1e70":"code","7a2ff6b1":"code","c5ef46b3":"code","51d99d58":"code","edea0ae8":"code","aa9ca615":"code","3623e406":"code","a8afe102":"code","8d36abda":"code","bdbe3789":"code","f88f9d44":"code","c4ea4656":"code","65e58d7a":"code","2b9ff6ef":"code","62cae3c6":"code","9aafbac9":"code","f1267c58":"code","a43a19a2":"code","fba0c1c4":"markdown","c8a85f5e":"markdown","b94087db":"markdown","e470618c":"markdown","1d3e0ea5":"markdown","994e4804":"markdown","5a4c75e1":"markdown","33ea2d63":"markdown","7573d8a3":"markdown","47f76c2f":"markdown","7f5e43b4":"markdown","fd3ab83f":"markdown","079f7189":"markdown","6f49d715":"markdown","0cb61d9c":"markdown","d72c0aee":"markdown","22f6b036":"markdown","79cf8627":"markdown","fcd0f392":"markdown","2bb54c13":"markdown","14fc6806":"markdown","2ce47378":"markdown","fc876203":"markdown","a5a09e18":"markdown","9d63e1c3":"markdown","07fba1e2":"markdown","a35c5ab5":"markdown","922f7635":"markdown","799e18c5":"markdown","82cec3a6":"markdown","3b848bf6":"markdown","83c51eaa":"markdown","f248ddc1":"markdown","1448b454":"markdown","99c6ce2d":"markdown","26f58703":"markdown","3dd2305f":"markdown","18e9bf27":"markdown","d162b7d1":"markdown","54072f5a":"markdown","4a0069a4":"markdown","119a1d1b":"markdown","9f08c015":"markdown","3e421dd7":"markdown","f11af676":"markdown","782efe27":"markdown","6c1645cd":"markdown","417e978c":"markdown","582879ce":"markdown","6e92033e":"markdown","20b0ef18":"markdown","df555a12":"markdown","e1b162f2":"markdown","9286c492":"markdown","530bf8df":"markdown","bd007031":"markdown","aa256b05":"markdown","4a18f4ca":"markdown","27a4796f":"markdown","147d6bc7":"markdown"},"source":{"7a9f18a3":"# Parameters for my workflow for more easeful experimenting (instead of commenting out code)\n# Also gives a great overall picture of what I'm including\n\nVERBOSITY = 0\nHIDE_CODE_CELLS = False\nIGNORE_WARNINGS = True\n\n# Baseline model before any major cleaning or feature engineering\nDO_BASELINE_MODEL = True\n\n# Data Cleaning & Class Imbalance\nVALIDATE_DATA = False\n\nMERGE_ON_COLS = ['installation_id', 'game_session']  # This indicates what constitutes an 'observation'\nRESAMPLE_TRUE_DATA = False\nSHOW_CLASS_BALANCE = False\nCOMPUTE_CLASS_WEIGHTS = False\nIMPUTE_VALUES = True\nIMPUTE_FILLNA_VAL = 0\nIMPUTE_METHOD = 'fill_na'\n\n# Semi Supervised Learning\nLABEL_UBLABELLED_DATA = True\n\n# Exploratory Data Analysis\n\nRESAMPLE_EDA_DATA = True\nDO_PANDAS_PROFILE = False\nEXPORT_PANDAS_PROFILE = False\n\n# Exploratory Clustering\n\nEXPLORE_WITH_HIERARCHICAL_C = False\nHIERARCHICAL_METHOD = ''\nEXPLORE_WITH_PCA = False\nEXPLORE_WITH_TSNE = True\nEXPLORE_WITH_DBSCAN = False\nDO_CO_OCCURENCE_MATRIX = False\nENSEMBLE_CLUSTERS = False\ncluster_parameter_dict = {}\n\n# Explanatory Data Analysis\nSHOW_EXPLANATORY_DATA = False\n\n# Feature Engineering\nBINARIZE_FEATURES = False\nBIN__NUMERICAL_FEATURES = False\nENCODE_CATEGORICAL = False\nGROUPBY_COUNTS_ON_TARGET = True  # ?\nGENERATE_CATEGORICAL_COUNTS = True\nTARGET_ENCODE_CATEGORIES = False\nCALCULATE_POLYNOMIAL_FEATURES = False\nCALCULATE_SIMILARITY_FEATURES = False\nEMBED_PREDICTIONS_AS_FEATURES = False\nEMBED_TEST_FIT_CLUSTER_ON_TRAIN_FEATURE = False\nADD_NOISY_FEATURE = False\nFEATURES_WITH_POTENTIAL_RELATIONSHIPS = []\n\n# NLP Engineering\nENGINEER_NLP_FEATURES = False\nPSEUDO_LABEL = False\n\n# Time Series Engineering\nCREATE_DATETIME_FEATURES = False\nENGINEER_TIME_SERIES_FEATURES = True\n\n# Feature Preprocessing\nSCALE_METHOD = \"min_max\"\nSCALE_FEATURES = True\n\n# Feature Selection\nREDUCE_TO_N_FEATURES = 30  # \nUSE_BEST_FEATURES_ONLY = False\nSELECT_WITH_PCA = True\nPCA_N_COMPONENTS = 30\nSELECT_WITH_LDA = False\nLDA_N_COMPONENTS = REDUCE_TO_N_FEATURES\nN_FEATURES = 30\n\n# Cross Validation and Model Validation\nMIMIC_KAGGLE_TRAIN_TEST = False\nDO_TRAIN_TEST_SPLIT = True\nTRAIN_TEST_PERCENTAGE = 0.75\nDO_KFOLD = True\nCOMPETITION_METRIC = 'kappa_quadratic'\nPLOT_DECISION_REGIONS = False\n\n# Models\nDO_LINEAR_REGRESSION = False\nDO_LOGISTIC_REGRESSION = False\n\nDO_RANDOM_FOREST = False\nDO_CATBOOST = True\nDO_LIGHTBOOST = True\nDO_XGBOOST = True\n\n# Deep Learning\nUSE_DEEP_LEARNING = False\nUSE_AUTO_KERAS = False\nNN_LOSS_FUNCTION = 'categorical_cross_entropy'\nNN_OPTIMIZER = 'adam'\nNN_N_LAYERS = 8\nNN_BATCH_SIZE = 32\nNN_N_NODES = 100\n\n# Ensembling\nUSE_VOTING_CLASSIFIER = False\nUSE_STACKING = False\n\n# Hyperparameter Tuning\nDO_GRID_SEARCH = False\nUSE_TPOT = False\nUSE_AUTO_KERAS = False\n\n# Submission\nCREATE_SUBMISSION = True\nSUBMISSION_CLF = 'cat_clf'\n","3bff30d4":"import itertools\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport scipy\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (16,12)\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n\nimport seaborn as sns\nsns.set()  # This make matplotlib plots look like seaborn plots.\nsns.set_context(\"talk\")\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Stats\nfrom scipy.stats import zscore\n\n# EDA and Unsupervised Exploring and Feature Visualization\nimport pandas_profiling\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nimport missingno as msno  # Missing Data\nfrom yellowbrick.features import Manifold, ParallelCoordinates\nfrom yellowbrick.datasets import load_occupancy\n\n# Class Balancing\nfrom imblearn.under_sampling import ClusterCentroids, NearMiss\n\n# Resampling\nfrom sklearn.utils import resample\n\n# Feature Processing and Engineering\nfrom sklearn.feature_extraction.text import CountVectorizer  # NLP\nfrom sklearn.preprocessing import PolynomialFeatures, label_binarize, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n# Models \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\n# Ensembling\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier\nfrom sklearn.pipeline import make_pipeline\n\n# Dimensionality Reduction\nif SELECT_WITH_PCA == True | EXPLORE_WITH_PCA == True:\n    from sklearn.decomposition import PCA\n\nfrom sklearn.manifold import TSNE\n\n# Model Validation and Metrics\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics.cluster import homogeneity_score\n\n\n# from imblearn.ensemble import BalancedRandomForestClassifier\n\n#AutoML\nfrom tpot import TPOTClassifier\n\n# Yellowbrick Reports\nfrom yellowbrick.classifier import ClassBalance, ClassificationReport, ConfusionMatrix, ROCAUC, DiscriminationThreshold\nfrom yellowbrick.model_selection import FeatureImportances, LearningCurve, RFECV\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\n\n# Optimization\nfrom numba import jit \n\n","2f8c581e":"def save_current_fig(filename):\n    # Save a figure to the output path.\n    \n    plt.gcf()\n    plt.savefig(filename)","b20769aa":"# Data Import\nSAMPLE_SIZE = 500000000000\nsample_size = SAMPLE_SIZE # Remove this redundancy\nCHUNK_SIZE = 2000\nEXPORE_MEM_TYPES = False\nREDUCE_MEMORY = True\nLOAD_ON_KAGGLE = True\n\n# Data Import Specifics\ndatafiles = ['sample_submission', 'train_labels', 'train', 'test']\ndataframe_names = ['sample_submission', 'train_labels', 'train', 'test']  # Remove this redundancy!\n\ncsv_filenames = map(lambda  x: str(x) + '.csv', datafiles)\nindex_columns = [None, 'installation_id', 'installation_id', 'installation_id']\ndatetime_cols = [None, 'timestamp', 'timestamp', 'timestamp']\nthepath = '\/kaggle\/input\/data-science-bowl-2019\/'\n\nCATEGORICAL_VARIABLES = []\nNUMERICAL_VARIABLES = []\nORDINAL_VARIABLES = []\nTARGET_VARIABLE = \"accuracy_group\"","79bad787":"\"\"\"\n# Load Preserving Ram\n# Still overloads the ram!!!!\n# I Think test_df = test_df.append() is the problem.  \n# Just using append wasn't doing anything.\n\n# Use a with statement to close after done with the chunk?\n\ndf_dict = {}\nfilepath = '\/kaggle\/input\/data-science-bowl-2019\/train.csv'\n\n# Read one column to get the column names.\ndf_info = pd.read_csv(filepath, nrows = 1, index_col = 'installation_id')\ncol_names = df_info.columns\n\nread_chunk = pd.read_csv(filepath, \n                         chunksize = 50000, \n                         index_col = 'installation_id')\n\n\nwhile True:\n    try:\n        df_dict.update(next(read_chunk))\n    except StopIteration:\n        break\n\ndf = pd.DataFrame.from_dict(data = df_dict)\nprint(df.shape)\n\"\"\"","fedcba2c":"def read_csv(datafiles, path = thepath):\n    return [pd.read_csv(str(path + datafiles[df_i] + '.csv'), \n                        nrows = sample_size,\n                        index_col=index_columns[df_i])\n            for df_i in range(len(datafiles))]\n\n# Unpack\nsample_submission, train_labels, train, test = read_csv(datafiles)\n","bb7736af":"# Convert to datetimes.\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","1308a089":"# Updated function for converting memory types.  DRY method.   \n# Not active yet.\n\nmem_updates_dict = {'train_labels': {'title':'category',\n                               'num_correct': 'np.int8',\n                               'num_incorrect': 'np.int8',\n                               'accuracy': 'np.float16',\n                               'accuracy_group': 'np.int8'},\n                      'train': {'type':'category',\n                               'world':'category',\n                               'event_count':'np.int8',\n                               'event_code':'np.int8',\n                               'game_time':'np.int8'},\n                      'test': {'type':'category',\n                               'world':'category',\n                               'event_count':'np.int8',\n                               'event_code':'np.int8',\n                               'game_time':'np.int8'}}\n\ndef convert_memtypes(memdict):\n    pass\n","609ae0d6":"# Convert memory types WET method.\n\ntrain_labels['title'] = train_labels['title'].astype('category')\ntrain_labels['num_correct'] = train_labels['num_correct'].astype(np.int16)\ntrain_labels['num_incorrect'] = train_labels['num_incorrect'].astype(np.int16)\ntrain_labels['accuracy'] = train_labels['accuracy'].astype(np.float16)\ntrain_labels['accuracy_group'] = train_labels['accuracy_group'].astype(np.int16)\n\ntrain['type'] = train['type'].astype('category')\ntest['type'] = test['type'].astype('category')\n\ntrain['world'] = train['world'].astype('category')\ntest['world'] = test['world'].astype('category')\n\ntrain['event_code'] = train['event_code'].astype('category')\ntest['event_code'] = test['event_code'].astype('category')\n\n# train['event_count'] = train['event_count'].astype(np.int8)\n# test['event_count'] = test['event_count'].astype(np.int8)\n\ntrain['game_time'] = train['game_time'].astype(np.int)\ntest['game_time'] = test['game_time'].astype(np.int)\n","eacb3122":"def extract_time_features(df):\n# Inspired by Gabriel Preda: https:\/\/www.kaggle.com\/gpreda\/2019-data-science-bowl-eda\n    df['date'] = df['timestamp'].dt.date.astype('category')\n    df['month'] = df['timestamp'].dt.month.astype('category')\n    df['hour'] = df['timestamp'].dt.hour.astype('category')\n    df['year'] = df['timestamp'].dt.year.astype('category')\n    df['day_of_week'] = df['timestamp'].dt.dayofweek.astype('category')\n    df['week_of_year'] = df['timestamp'].dt.weekofyear.astype('category')\n    df['day_of_year'] = df['timestamp'].dt.dayofyear.astype('category')\n    df['quarter'] = df['timestamp'].dt.quarter.astype('category')\n    df['is_month_start'] = df['timestamp'].dt.is_month_start.astype('category')\n    return df","4ebaa8e8":"#train = extract_time_features(train)\n#test = extract_time_features(test)\n#train.head()","e93e9464":"def remove_outliers(df, low_q = 0, high_q = 1):  # Does better without removing outliers\n    \n    # Chose 0 and 1 to bypass without changing code everywhere.\n    # Check to see if outliers exist.  Will return an error if they don't.\n    # Right now, nothing should be passed in the test set\n    # In the future, update so that only df.index.isin(train.index)\n    # Are removed.\n    \n    print('Original shape', df.shape)\n    outliers_removed = pd.DataFrame()\n    \n    for column in df.columns:\n        # Do this only if column is numeric\n        \n        try:\n            q1 = df[column].quantile(low_q)\n            q3 = df[column].quantile(high_q)\n            mask = df[column].between(q1, q3, inclusive=True)\n\n            iqr = df.loc[mask, column]\n            outliers_removed[column] = iqr\n        except:\n            print('Did not remove outliers from' + column)\n        finally:\n            print('After removing outliers: ', outliers_removed.shape)\n            \n    return outliers_removed\n\n\n","7ac3ca88":"REMOVE_0_GAME_TIME = False # Does better when not removed.\n\nif REMOVE_0_GAME_TIME == True:\n    # Remove game times of 0 since these will not likely contain useful information.\n    train_is_game_time_0 = train['game_time'] == 0\n    train_is_not_game_time_0 = train['game_time'] != 0\n    train = train[train_is_not_game_time_0]\n\n    print('Training set:')\n    print(train_is_game_time_0.sum())\n    print(train.shape[0])\n    print(round(train_is_game_time_0.sum() \/ train.shape[0] * 100, 2), '% of entries in train have 0 game time')\n    print('\\n')\n\n    # Does the test set have any 0 game time entries?\n    print('Testing set:')\n    test_is_game_time_0 = test['game_time'] == 0\n    print(test_is_game_time_0.sum())\n    print(test.shape[0])\n    print(round(test_is_game_time_0.sum() \/ test.shape[0] * 100, 2), '% of entries in test have 0 game time')\n\n    # Should we just assign these entries as accuracy of 0?\n\n    # Maybe they just left the games on and weren't actually playing?\n    train_gt_is_not_too_long = train.game_time < train.game_time.quantile(.9999)\n    train = train[train_gt_is_not_too_long]\n\n    # Do note remove from test set!!!\n    # test_gt_is_not_too_long = test.game_time < test.game_time.quantile(.9999)\n    # test = test[test_gt_is_not_too_long]\n\n    # train = remove_outliers(train, low_q = 0, high_q = .9999)  # this should remove the 'problem' data points.\n\n    # Do not remove from test set!!!\n    # test = remove_outliers(test, low_q = 0, high_q = .9999)  # this should remove the 'problem' data points.","f0f6c02d":"def groupby_categorical_counts(df, df_name, by_col, on_cols, stat_features = True, verbose = 1):\n    # Inputs a categorical column in a dataframe and \n    # Returns the count of that column of each unique value.\n    # on_cols must be a list of cols.  If one col, pass ['col'].\n    \n    added_features = []\n    total_features = 0\n    \n    for i in range(len(on_cols)):\n        \n        on_col = on_cols[i]  # Set the column to the i'th element in the list.\n        \n        # Do Groupby value_counts.\n        if verbose == 1:\n            print(\"Creating groupby counts\", by_col, \"on\", on_col, 'in dataframe', df_name)\n\n        added_feature = df.groupby(by_col)[on_col].value_counts().unstack().fillna(0)\n        n_new_features = len(added_feature.columns)\n        \n        added_features.append(added_feature)\n        total_features = total_features + n_new_features\n        \n        if verbose == 1:\n            print(\"Added: \", n_new_features, \"features from\", df_name, \"Filled Na with 0 \\n\")        \n    \n    if verbose == 1:\n        print(\"Total features returned:\", total_features)\n        \n    return added_features","90025606":"# BASELINE_WITH_SCALING = True\nCATEGORICAL_COLS_TO_GROUPBY = ['event_code', \n                               'title', 'type', \n                               'world', 'event_id']\n\n# add_later = [, 'is_month_start', 'quarter']","ac83f945":"if DO_BASELINE_MODEL == True:\n    print('Creating categorical features for the training set')    \n    tr_baseline_event_code, tr_baseline_title, tr_baseline_type, tr_baseline_world, tr_baseline_event_id = groupby_categorical_counts(df = train, \n                                                                                df_name = 'train', \n                                                                                by_col = 'installation_id', \n                                                                                on_cols = CATEGORICAL_COLS_TO_GROUPBY)","ff0edd96":"if DO_BASELINE_MODEL == True:\n    print('\\n\\n Creating categorical features for the test set')\n    test_baseline_event_code, test_baseline_title, test_baseline_type, test_baseline_world, test_baseline_event_id = groupby_categorical_counts(df = test, \n                                                                            df_name = 'test', \n                                                                            by_col = 'installation_id', \n                                                                            on_cols = CATEGORICAL_COLS_TO_GROUPBY)","f28cbbfa":"# Work only with the assessments for now. Training data.\n\nis_assessment_train = train['type'] == 'Assessment'\ntrain = train[is_assessment_train]\n\n# All other assessments.\n\n\n# Work only with the assessments for now. Training data.\nis_assessment_test = test['type'] == 'Assessment'\ntest = test[is_assessment_test]\n\n# Find the strings that indicate 'correct and incorrect' assessments\nis_correct = '\"correct\":true'\nis_incorrect = '\"correct\":false'\n\ntrain['num_correct'] = train.loc[:, 'event_data'].str.find(is_correct)\ntrain['num_correct'] = train['num_correct'] >= 0\ntrain['num_correct'] = train['num_correct'].astype(int)\n\ntrain['num_incorrect'] = train.loc[:, 'event_data'].str.find(is_incorrect)\ntrain['num_incorrect'] = train['num_incorrect'] >= 0\ntrain['num_incorrect'] = train['num_incorrect'].astype(int)\n\ntest['num_correct'] = test.loc[:, 'event_data'].str.find(is_correct)\ntest['num_correct'] = test['num_correct'] >= 0\ntest['num_correct'] = test['num_correct'].astype(int)\n\ntest['num_incorrect'] = test.loc[:, 'event_data'].str.find(is_incorrect)\ntest['num_incorrect'] = test['num_incorrect'] >= 0\ntest['num_incorrect'] = test['num_incorrect'].astype(int)\n\n# Collect bird train assessments\n# is_bird_train = train['event_code'] == 4110\n# bird_assess_train = pd.DataFrame(train[is_bird_train])\n\n# Collect bird train assessments\n# is_bird_test = test['event_code'] == 4110\n# bird_assess_test = pd.DataFrame(test[is_bird_test])","8e5228ab":"# Aggregate the num_correct and num_incorrect by user, game sesion, and title\ntr_baseline_num_correct = train.groupby(['installation_id', 'game_session', 'title'])['num_correct'].sum().fillna(0).astype(int)\ntr_baseline_num_incorrect = train.groupby(['installation_id', 'game_session', 'title'])['num_incorrect'].sum().fillna(0).astype(int)","69c0d99b":"# Make dataframes out of these for concatenation next.\ntr_num_correct_df = pd.DataFrame(tr_baseline_num_correct.unstack().fillna(0).astype(int))\ntr_num_incorrect_df = pd.DataFrame(tr_baseline_num_incorrect.unstack().fillna(0).astype(int))","16aea400":"# This is validated.  Calculate the accuracy per game and then do statistics on it.\ntrain_accuracy_per_game = tr_baseline_num_correct \/ (tr_baseline_num_correct + tr_baseline_num_incorrect)","775a3752":"train_accuracy_avg = train_accuracy_per_game.groupby(['installation_id']).mean().fillna(0).rename('accuracy', inplace = True).astype(float)\ntrain_accuracy_std = train_accuracy_per_game.groupby(['installation_id']).std().fillna(0).rename('accuracy_std', inplace = True).astype(float)\ntrain_accuracy_std = train_accuracy_per_game.groupby(['installation_id']).std().fillna(0).rename('accuracy_std', inplace = True).astype(float)\n\ntrain_accuracy_min = train_accuracy_per_game.groupby(['installation_id']).min().fillna(0).rename('accuracy_min', inplace = True).astype(float)\ntrain_accuracy_max = train_accuracy_per_game.groupby(['installation_id']).max().fillna(0).rename('accuracy_max', inplace = True).astype(float)","c37fb0ec":"# Sum totals of num correct and num incorrect.\ntr_baseline_num_correct = tr_baseline_num_correct.groupby(['installation_id']).sum().fillna(0).rename('num_correct', inplace = True)\ntr_baseline_num_incorrect = tr_baseline_num_incorrect.groupby(['installation_id']).sum().fillna(0).rename('num_incorrect', inplace = True)","c84bef0e":"train_accuracy_per_game.head()","402d47d4":"# Other statistics on accuracy\ntrain_accuracy_gmean = train_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.gmean).fillna(0).rename('accuracy_gmean', inplace = True).astype(float)\ntrain_accuracy_skew = train_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.skew).fillna(0).rename('accuracy_skew', inplace = True).astype(float)\ntrain_accuracy_kurt = train_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.kurtosis).fillna(0).rename('accuracy_kurtosis', inplace = True).astype(float)\n\ntrain_statistics_cols = [train_accuracy_gmean, train_accuracy_skew, train_accuracy_kurt]\n","90d2dbe3":"tr_baseline_num_correct_eventid = train.groupby(['installation_id','event_id'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntr_baseline_num_correct_eventid.columns = tr_baseline_num_correct_eventid.columns.map(''.join).str.strip('')\n\ntest_baseline_num_correct_eventid = test.groupby(['installation_id','event_id'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntest_baseline_num_correct_eventid.columns = test_baseline_num_correct_eventid.columns.map(''.join).str.strip('')","6d2168f5":"train_num_correct_per_title = train.groupby(['installation_id', 'title'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntrain_num_correct_per_title.columns = train_num_correct_per_title.columns.map('_'.join).str.strip('_')\n\ntest_num_correct_per_title = test.groupby(['installation_id', 'title'])['num_correct','num_incorrect'].sum().unstack(fill_value = 0)\ntest_num_correct_per_title.columns = test_num_correct_per_title.columns.map('_'.join).str.strip('_')","d302bb6f":"tr_num_correct_per_world = train.groupby(['installation_id', 'world'])['num_correct','num_incorrect'].sum().unstack().fillna(0)\ntr_num_correct_per_world.columns = tr_num_correct_per_world.columns.map('_'.join).str.strip('_')\n\ntest_num_correct_per_world = test.groupby(['installation_id', 'world'])['num_correct','num_incorrect'].sum().unstack().fillna(0)\ntest_num_correct_per_world.columns = test_num_correct_per_world.columns.map('_'.join).str.strip('_')\ntr_num_correct_per_world.head()","59d81885":"test_baseline_num_correct = test.groupby(['installation_id', 'game_session', 'title'])['num_correct'].sum().fillna(0).astype(int)\ntest_baseline_num_incorrect = test.groupby(['installation_id', 'game_session', 'title'])['num_incorrect'].sum().fillna(0).astype(int)\n\ntest_accuracy_per_game = test_baseline_num_correct \/ (test_baseline_num_correct + test_baseline_num_incorrect)\n\n\ntest_accuracy_avg = test_accuracy_per_game.groupby(['installation_id']).mean().fillna(0).rename('accuracy', inplace = True).astype(float)\ntest_accuracy_std = test_accuracy_per_game.groupby(['installation_id']).std().fillna(0).rename('accuracy_std', inplace = True).astype(float)\ntest_accuracy_min = test_accuracy_per_game.groupby(['installation_id']).min().fillna(0).rename('accuracy_min', inplace = True).astype(float)\ntest_accuracy_max = test_accuracy_per_game.groupby(['installation_id']).max().fillna(0).rename('accuracy_max', inplace = True).astype(float)\n\ntest_baseline_num_correct = test_baseline_num_correct.groupby(['installation_id']).sum().fillna(0).rename('num_correct', inplace = True)\ntest_baseline_num_incorrect = test_baseline_num_incorrect.groupby(['installation_id']).sum().fillna(0).rename('num_incorrect', inplace = True)\n\ntest_accuracy_gmean = test_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.gmean).fillna(0).rename('accuracy_gmean', inplace = True).astype(float)\ntest_accuracy_skew = test_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.skew).fillna(0).rename('accuracy_skew', inplace = True).astype(float)\ntest_accuracy_kurt = test_accuracy_per_game.groupby(['installation_id']).agg(scipy.stats.kurtosis).fillna(0).rename('accuracy_kurtosis', inplace = True).astype(float)\n\ntest_statistics_cols = [test_accuracy_gmean, test_accuracy_skew, test_accuracy_kurt]","679ec5e0":"# Add and sort the event_id columns for training and test sets.\n\nprint('Are there missing event ID counts in the test variable to be concatenated later?')\nprint('train_baseline_event_id,shape', tr_baseline_event_id.shape)\nprint('test_baseline_event_id,shape', test_baseline_event_id.shape)\n\nprint('\\nAdding to test event_id counts')\n\n# Loop through missing columns and broadcast them.\nevent_ids_add_to_test = list(set(tr_baseline_event_id).difference(set(test_baseline_event_id)))\nfor col in event_ids_add_to_test:\n    test_baseline_event_id[col] = 0\n\nprint('train_baseline_event_id,shape', tr_baseline_event_id.shape)\nprint('test_baseline_event_id,shape', test_baseline_event_id.shape)\n\nprint('\\nAdding to test num_correct_event_id')\n\nprint('tr_baseline_num_correct_eventid', tr_baseline_num_correct_eventid.shape)\nprint('test_baseline_num_correct_eventid', test_baseline_num_correct_eventid.shape)\n\nnum_correct_event_ids_add_to_test = list(set(tr_baseline_num_correct_eventid).difference(set(test_baseline_num_correct_eventid)))\nfor col in num_correct_event_ids_add_to_test:\n    test_baseline_num_correct_eventid[col] = 0\n    \nprint('tr_baseline_num_correct_eventid', tr_baseline_num_correct_eventid.shape)\nprint('test_baseline_num_correct_eventid', test_baseline_num_correct_eventid.shape)\n\nprint('\\nSorting the Columns...')\ntr_baseline_event_id.sort_index(axis=1, inplace=True)\ntest_baseline_event_id.sort_index(axis=1, inplace=True)\ntr_baseline_num_correct_eventid.sort_index(axis=1, inplace=True)\ntest_baseline_num_correct_eventid.sort_index(axis=1, inplace=True)\nprint('Complete')","80fe2ee9":"def convert_to_accuracy_group(the_series):\n    if the_series == 0:\n        return 0\n    elif the_series > 0 and the_series < 0.5:\n        return 1\n    elif the_series > 0 and the_series >= 0.5 and the_series < 0.75:\n        return 2\n    elif the_series >= 0.75:\n        return 3","76fa278b":"y_train_estimate = train_accuracy_avg.apply(convert_to_accuracy_group).rename('accuracy_group_estimate', inplace = True).fillna(0).astype(int)\ny_test_estimate = test_accuracy_avg.apply(convert_to_accuracy_group).rename('accuracy_group_estimate', inplace = True).fillna(0).astype(int)","2f32ab8c":"# Calculate - 'Got the Third One'\n# if event_count == 3 and type == 'Assessment':\n# Check if success\n# \"won first assessment.\"","0d69361f":"# Calculate - Only got it after the third one.\n# else won after third try.","ad7935c4":"from sklearn.metrics import f1_score\n\ndef aggregate_targets_on_id(y):\n    # Aggregate by the mode.  \n    # Preserves structure of distribution most.\n    \n    mode_agg = lambda x: scipy.stats.mode(x)[0]\n    aggregation_function = mode_agg\n\n    # Just aggregate the y_train data. \n    return y.groupby(['installation_id'])['accuracy_group'].agg(aggregation_function)","4a0168c2":"# Aggregate targets\nif DO_BASELINE_MODEL == True:\n    print('Aggregating training set with the MODE so there is one row per ID')\n    train_y_baseline = aggregate_targets_on_id(train_labels)\n    print(train_y_baseline.shape)\n    print('assigning this to y')\n    \n    # Code smell - organize this so I don't need to assign it twice.\n    y = train_y_baseline\n    y_train = train_y_baseline","7d9229db":"# Concatenate all features engineering to train dataframe.\n\nif DO_BASELINE_MODEL == True:\n    print('Concatenating engineered features to the train_baseline')\n    \n    train_features = [tr_num_correct_per_world, tr_baseline_event_code, \n                      tr_baseline_title, tr_baseline_type, tr_baseline_world, \n                      tr_baseline_event_id, tr_baseline_num_correct, \n                      tr_baseline_num_incorrect, tr_baseline_num_correct_eventid, \n                      train_accuracy_avg, train_accuracy_std, train_accuracy_min, \n                      train_accuracy_max, train_num_correct_per_title, y_train_estimate,\n                      train_accuracy_gmean, train_accuracy_skew, train_accuracy_kurt]\n    \n    train_baseline = pd.concat(train_features, axis = 1)\n    train_baseline.fillna(0, inplace = True)\n    print(train_baseline.shape)\n    train_baseline.head()\n","fabaae9c":"# Concatenate Test Set\n\nif DO_BASELINE_MODEL == True:\n    # print('\\nRemoving Outliers...')\n    # train_baseline = train_baseline.fillna(0)\n    # train_baseline = remove_outliers(train_baseline)\n    \n\n    print('\\nConcatenating engineered features to the test_baseline')\n    test_baseline = pd.concat([test_num_correct_per_world, test_baseline_event_code, \n                               test_baseline_title, test_baseline_type, \n                               test_baseline_world, test_baseline_event_id, \n                               test_baseline_num_correct, test_baseline_num_incorrect, \n                               test_baseline_num_correct_eventid, test_accuracy_avg, \n                               test_accuracy_std, test_accuracy_min, test_accuracy_max, \n                               test_num_correct_per_title, y_test_estimate,\n                               test_accuracy_gmean, test_accuracy_skew, test_accuracy_kurt], axis = 1)\n\n    print('test_baseline.shape' , test_baseline.shape)","f6150f9d":"cols_in_train = list(train_baseline.columns)\nlen(cols_in_train)","de3ddf80":"print(test_baseline.shape)\ntest_baseline = test_baseline[cols_in_train]\nprint(test_baseline.shape)","e2d2aef6":"    print('\\nStoring unlabelled data in the training seet as: train_baseline_labelled for semi-supervised learning')\n    train_baseline_labelled = train_baseline[train_baseline.index.isin(train_y_baseline.index.unique())]\n    train_unlabelled = train_baseline[~train_baseline.index.isin(train_y_baseline.index.unique())]\n    \n    print('train_unlabelled shape', train_unlabelled.shape)\n    print('train_labelled shape', train_baseline_labelled.shape)\n    \n    # print('Using generated targets')\n    # train_y_baseline = y_train_estimate\n    \n    # print('\\nOnly including samples in train which have accuracy group labels in train_labels')\n    # print(train_y_baseline.shape)\n    # train_y_baseline = train_y_baseline[train_y_baseline.index.isin(train_baseline.index.unique())]\n    # print(train_y_baseline.shape)","b77e3cb0":"import missingno as msno\n\n# Impute test with 0's for now.\ntest_baseline = test_baseline.fillna(0)\n# ax = msno.matrix(test_baseline.fillna(0))\n","a402f6ca":"def impute_missing_vals(df):\n    pass","f702ca6a":"train_baseline_labelled.head()","df36b938":"train_unlabelled.head()","a5cd694f":"# Assign current working dataset as training set X\nX_train = train_baseline_labelled\nX_test = test_baseline\n\nSCALE_FEATURES = False\n\nif SCALE_FEATURES == True:\n    print('Generating scaled features as X_scaled_train \\ntest as X_scaled_test')\n    \n    X_scaled_train = StandardScaler()\n    X_scaled_train = X_scaled_train.fit_transform(X_train)\n    print('Sample from X_scaled_train', X_scaled_train[0][:5])\n    \n    X_scaled_test = StandardScaler()\n    X_scaled_test = X_scaled_test.fit_transform(test_baseline)\n    print('Sample from X_scaled_test', X_scaled_test[0][:5])\n    \nelse:\n    print('X_train and X_test assigned as unscaled feature data')","7a17360c":"from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, roc_auc_score\n\ndef fit_baseline_classifiers(classifiers, X, y, use_polynomial = False, use_scaled = False, do_kfold = False, graph_results = False):\n    \n    if use_polynomial == True:\n        polyX_train = PolynomialFeatures(degree = 2, interaction_only = True)\n        X = polyX_train.fit_transform(X)\n    \n    if use_scaled == True:\n        print('Scaling for Classification using MinMax\\n')\n        X_scaled = MinMaxScaler()\n        X_scaled = X_scaled.fit_transform(X)\n        X = X_scaled\n    \n    # For collecting and graphing results at the end.\n    all_accuracies = {}\n    all_balanced_acc = {}\n    all_validation_accuracies = {}\n    all_acc_differences = {}\n    all_quadratic_kappas = {}\n    all_f1_scores = {}\n    \n    # Loop across each classifier.\n    for clf_name, classifier in classifiers:\n        \n        # Split for validation scoring.\n        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, shuffle = True)\n        \n        # Fit the classifier on the labels.\n        classifier.fit(X_train, y_train)\n        \n        # Predict targets after fitting.\n        y_pred = classifier.predict(X_test)\n        \n        # Use .score method if it exists.\n        try:\n            training_set_accuracy = round(classifier.score(X_train, y_train), 2)\n        except:\n            training_set_accuracy = None\n            print(clf_name, \"doesn't have an internal accuracy metric set up\")\n        \n        # Compute metrics\n        accuracy = round(accuracy_score(y_test, y_pred), 2)\n        balanced_acc = round(balanced_accuracy_score(y_test, y_pred), 2)\n        recall_sc = round(recall_score(y_test, y_pred, average = 'weighted'), 2)\n        precision_sc = round(precision_score(y_test, y_pred, average = 'weighted'))\n        quadratic_kappa = round(cohen_kappa_score(y_test, y_pred, weights=\"quadratic\"))\n        the_f1_score = round(f1_score(y_test, y_pred, average = 'weighted'))\n        # roc_auc_sc = round(roc_auc_score(y_test, y_pred, average = None))\n        \n        # Collect all accuracy scores and return the highest one?\n        print(clf_name)\n        print('Accuracy score - training set:', training_set_accuracy)\n        print('Accuracy score - validation set:', accuracy)\n        print('Difference of  - training and val sets:', round(training_set_accuracy - accuracy, 2))\n        print('Balanced accuracy is', balanced_acc)\n        print('Recall (What proportion of actual positives was predicted correctly?):', recall_sc)\n        print('Precision (What proportion of positive predictions was actually correct?):', precision_sc)\n        print('F1 Score - validation set: ', the_f1_score)\n        # print('Area under the ROC curve of', clf_name, 'is', roc_auc_sc)\n        print('Quadratic Kappa - validation set - is: ', quadratic_kappa)\n        \n        if do_kfold == True:\n            print('Cross Val Score of', clf_name, ' is: ', cross_val_score(classifier, X, y, cv=3))\n        \n        print('\\n')\n        \n        if graph_results == True:\n            pass\n            # Graph this!\n    ","6b23c7f2":"if DO_BASELINE_MODEL == True:\n    # 'Linear' Classifiers\n    dum_clf = ('Dummy_Classifier', DummyClassifier())\n    logreg_clf = ('Logistic_Regression', LogisticRegression(solver = 'lbfgs', \n                                                            multi_class = 'auto', \n                                                            max_iter = 100))\n    svc_clf = ('SVC', SVC(gamma = 'scale'))\n    svc_poly = ('SVC_Poly', SVC(kernel = 'poly', degree = 3, C = 5, coef0 = 1, gamma = 'scale'))\n    \n    # Tree Classifiers\n    lb_clf = ('LightGBM', lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                             objective = 'multiclass',\n                                             is_unbalance = True,\n                                             lambda_l1 = 8))\n    rf_clf = ('Random_Forest', RandomForestClassifier(n_estimators = 500,\n                                                      min_impurity_decrease =.0065,\n                                                      class_weight = 'balanced_subsample'))\n    xgb_clf = ('XGBoost', xgb.XGBClassifier(reg_alpha = 10))\n    cb_clf = ('Catboost', cb.CatBoostClassifier(verbose=0))\n    \n    # Lists of these classifiers\n    scaled_classifiers = [dum_clf,logreg_clf, svc_clf,]\n    tree_classifiers = [lb_clf, rf_clf, xgb_clf]\n    quarantined_classifiers = [cb_clf]\n    all_classifiers = scaled_classifiers + tree_classifiers + quarantined_classifiers\n    \n    # Function call to fit.  Set the first position to what classifier list you want to use.\n    fit_baseline_classifiers(tree_classifiers, X_train, y_train, use_scaled = True, do_kfold = False)\n    ","b4a353be":"DO_FAST_AI = False:\n    \nif DO_FAST_AI = True:\n    from fastai.tabular import *\n\n    dep_var = 'accuracy_group'\n    cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n    cont_names = ['education-num', 'hours-per-week', 'age', 'capital-loss', 'fnlwgt', 'capital-gain']\n    procs = [FillMissing, Categorify, Normalize]\n    \n    learn = tabular_learner(data, layers=[200,100], metrics=accuracy)\n    learn.fit(5, 1e-2)\n    learn.save('mini_train')\n    \n    learn.show_results()\n","a9e3ce54":"DO_BASELINE_REPORT = False\nDO_VALIDATION_CURVE = False\nDO_LEARNING_CURVE = False\nDO_CV_SCORE = False\nDO_MUTUAL_INFORMATION = False","52b65a78":"if DO_BASELINE_REPORT == True:\n    \n    from yellowbrick.model_selection import ValidationCurve, LearningCurve, CVScores, RFECV\n    \n    # Reporting parameters applicable to all reports\n    DO_BASELINE_REPORT = True\n    b_clf_to_study_name = \"xgb\"\n    b_clf_to_study = xgb.XGBClassifier(max_depth = 22, \n                                       reg_alpha = 3)\n    cv = StratifiedKFold(n_splits=12)\n","cc528a24":"if DO_BASELINE_REPORT == True & DO_VALIDATION_CURVE == True:\n\n    parameter_to_study = 'reg_lambda'\n    param_range_study = np.arange(1, 15, 2)\n\n    # Validation curve to see performance changes on a particular parameter\n    val_curve = ValidationCurve(b_clf_to_study,\n                               param_name = parameter_to_study,\n                               param_range = param_range_study,\n                               n_jobs = -1)\n    val_curve.fit(X_train, y_train)\n    val_curve.finalize()\n    val_curve.show()\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'val_curve.jpg'))\n    except:\n        pass","f1cf0c55":"if DO_BASELINE_REPORT == True & DO_LEARNING_CURVE == True:\n    \n    train_sizes = np.linspace(0.3, 1.0, 10) # in percentages\n    \n    # Learning curve to see performance changes with increased sample size.\n    cv = StratifiedKFold(n_splits=12)\n    \n    learn_curve = LearningCurve(b_clf_to_study, \n                                scoring='f1_weighted',\n                                cv = cv,\n                                train_sizes=train_sizes, \n                                n_jobs=-1)\n    \n    learn_curve.fit(X_train, y_train)\n    learn_curve.finalize()\n    learn_curve.show()\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'learn_curve.jpg'))\n    except:\n        pass","f7a8eeca":"if DO_BASELINE_REPORT == True & DO_CV_SCORE == True:\n    \n    # CV Score to see performance changes changes within a fold dramatically.\n    cv = StratifiedKFold(n_splits=12)\n    \n    cv_scores = CVScores(b_clf_to_study, \n                                scoring='f1_weighted',\n                                cv = cv)\n    \n    cv_scores.fit(X_train, y_train)\n    cv_scores.finalize()\n    cv_scores.show()\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'cv_scores.jpg'))\n    except:\n        pass","0e3bd34f":"if DO_BASELINE_REPORT == True & DO_MUTUAL_INFORMATION == True:\n    from sklearn.feature_selection import mutual_info_classif\n    \n    mutual_info_rep = mutual_info_classif(X_train, y_train)\n                        \n    fig, ax = plt.subplots()\n    \n    mutual_info_df = pd.DataFrame({'feature                  m      rd':X_train.columns,\n                                  'bits_shared':mutual_info_rep}).set_index('feature')\n    mutual_info_df_top25 = mutual_info_df.sort_values(by = 'bits_shared',\n                          ascending = False)[:25]\n    \n    mutual_info_df_top25.plot.barh(ax = ax)\n    \n    try:\n        save_current_fig(str(b_clf_to_study_name + '_' + 'mutual_information.jpg'))\n    except:\n        pass    ","748dc350":"def report_feature_importances(clf_name, classifier, X, y):\n    viz = FeatureImportances(classifier)\n    viz.fit(X, y)\n    viz.show()\n    save_current_fig(str(clf_name + 'feature_imp.jpg'))\n    \n    saved_feature_importances = pd.DataFrame(list(zip(viz.features_, viz.feature_importances_)))\n    \n    feature_importances_df = pd.DataFrame()\n    feature_importances_df[str(clf_name + '_features')] = viz.features_\n    \n    feature_importances_df[str(clf_name + '_importances')] = viz.feature_importances_\n    feature_importances_df.sort_values(inplace = True, by = str(clf_name + '_importances'), ascending = False)\n    \n    feature_importances_df.to_csv(str(clf_name +' feat_imp.csv'), index=False)\n    \n    return feature_importances_df","04dc1cb5":"xgb_feat = xgb.XGBClassifier(reg_alpha = 10)\n\nxgb_feat.fit(X_train, y)\nxgb_feature_importances = report_feature_importances('XGBoost', xgb_feat, X_train, y)\n","d2881004":"print('top100_xgb has the filtered feature importances of X_train')\nprint('X_train still has all of the features')\n\ntop100_xgb = xgb_feature_importances[:100]\ntop100_xgb_names = top100_xgb['XGBoost_features'].reset_index(drop = True)\n\nprint('\\nSample of top 25 feature importances')\nprint(top100_xgb_names.head(25))\n\n# xgb_clf[1] is the classifier in the named tuples of classifiers above.\nxgb_feat = xgb_clf[1]\nX_top100 = X_train.loc[:,top100_xgb_names]\n\nxgb_feat.fit(X_top100 , y)\nxgb_feature_importances = report_feature_importances('XGBoost', xgb_feat, X_top100, y)","1cdd2db2":"# Filter the best 100 features in the test set and assign to test_baseline_top100\ntest_baseline_top100 = test_baseline.loc[:,top100_xgb_names]\nprint('Top 100 features filtered in test set and assigned to test_baseline_top100')\nprint('test_baseline still has all the features.')\n\ntest_baseline_top100.head()","df44cdb4":"# Check shape of everything:\nbs_X_train, bs_y_train, bs_X_test = X_train, y, X_test\nprint(bs_X_train.shape, bs_y_train.shape, bs_X_test.shape)\n","e5bf55f6":"#Are any columns missing from the training and testing set?\nprint('Are any columns missing from the training and testing set?')\nprint(list(set(bs_X_train).difference(set(bs_X_test))))","200fb984":"# Create a submission dataframe from the best baseline.\n# Fit to best 100 feature set or whole thing.\n# Set the X training set to top100_xgb or X_train\n# y set to y or estimated targets.\n\n# classifier[1] is the named classifier in the named tuples\n# Converted to an np.array because of 'JSON column names' problem.\nbaseline_submission_clf = rf_clf[1]\nbaseline_submission_clf.fit(bs_X_train, bs_y_train)","04a8b349":"baseline_submision_preds = baseline_submission_clf.predict(bs_X_test.fillna(0)).astype(int)\nprint(baseline_submision_preds[:50])","51139f98":"baseline_100_submission_clf = rf_clf[1]\nbaseline_100_submission_clf.fit(X_top100, bs_y_train)\n","f4c21a7f":"baseline_100_submision_preds = baseline_100_submission_clf.predict(test_baseline_top100.fillna(0)).astype(int)\nprint(baseline_100_submision_preds[:50])","b7167020":"DO_GRID_SEARCH = False","fe21f838":"if DO_GRID_SEARCH == True:\n    from sklearn.model_selection import GridSearchCV\n\n    param_grid = {\n        'num_leaves': [31, 127],\n        'reg_alpha': [0.1, 0.5],\n        'min_data_in_leaf': [30, 50, 100, 300, 400],\n        'lambda_l1': [0, 1, 1.5],\n        'lambda_l2': [0, 1]\n        }\n\n    grid_kfold = KFold(n_splits = 3, shuffle = True, random_state = 42).split(X = X, y = y)\n    gsearch = GridSearchCV(estimator = baseline_submission_clf, param_grid = param_grid, cv = grid_kfold, verbose = 2)\n    lgb_model = gsearch.fit(X=X, y=y)\n    \n    print(lgb_model.best_params_, lgb_model.best_score_)\n","f76d6d50":"MERGE_TRAIN_AND_LABELS = False","46d3dc84":"# If you wanted to graph things to see their relationships with the targets\n\nif MERGE_TRAIN_AND_LABELS == True:\n    annotated_train = train_labels.merge(train, on = MERGE_ON_COLS)\n    annotated_train.shape","9d4d46f8":"DO_EDA = False","a9bb1cf6":"if DO_EDA == True:\n    # Create a drop function here\n    # IF DO_DROP_COLUMNS == True\n    # DROPPED_COLS == [\"event_id\", \"game_session\", \"timestamp\"]\n\n    dropped_cols = [\"game_session\", \"timestamp\"]\n\n    train.drop(dropped_cols, axis = 1, inplace = True)\n    test.drop(dropped_cols, axis = 1, inplace = True)\n    \n    # Get rid of training samples that don't have labels.\n    # Check https:\/\/www.kaggle.com\/erikbruin\/data-science-bowl-2019-eda-and-baseline\n    train = train[train.index.isin(train_labels.index.unique())]\n    train.shape\n    \n    # Is this redundant?\n    annotated_train = annotated_train[annotated_train.index.isin(train_labels.index.unique())]\n    annotated_train.shape","4f564bf8":"ENGINEER_NLP_FEATURES = False","92f70d2d":"\n\nif ENGINEER_NLP_FEATURES == True:\n    event_stream = train['event_data']\n    \n    # Ignore these words b\/c they are redundant categories.\n    # What do 'coordinates' do?\n    stop_word_list = ['event_code', 'game_time', 'event_count', 'game_time', 'title', 'type', 'world', 'media_type', 'audio', 'duration', 'total_duration']\n\n    important_words = ['description', 'identifier']\n    count_vec = CountVectorizer(stop_words = stop_word_list,\n                               token_pattern = ':\\D+:',\n                               max_df = 1000,\n                               min_df = 2)\n\n    # BUG: Still catches digits and stopwords...\n    # Make this into an 'apply' function or a for loop that adds this to each installation ID.\n    ##### count_vec.fit_transform(event_stream)\n\n    ##### print(count_vec.vocabulary_)\n    \n    # Export this for faster processing time?","d19af862":"N_CLUSTERS = 4  # For kmeans and embedding.","a15b14a5":"# Group all samples of test and unlabeled + labeled train under total_samples\ntotal_samples = pd.concat([X_train, train_unlabelled, X_test])\ntotal_samples.shape\ntotal_index = total_samples.index","2a647f48":"PSEUDO_LABEL = False\n","97bf9784":"\nif PSEUDO_LABEL == False:\n    pass\n    \nif PSEUDO_LABEL == True:\n    train_unlabelled_top100 = train_unlabelled.loc[:, top100_xgb_names]\n\n    semi_preds = pd.DataFrame()\n    semi_preds['installation_id'] = train_unlabelled_top100.index\n    semi_preds.set_index('installation_id', inplace = True)\n\n    lr_semi = LogisticRegression(verbose = 1)\n    lr_semi.fit(X_top100, y)\n\n    semi_preds['lr_cat'] = lr_semi.predict(train_unlabelled_top100)\n    print(semi_preds.head())\n\n    lr_semi_prob = pd.DataFrame(lr_semi.predict_proba(train_unlabelled_top100))\n    lr_semi_prob['installation_id'] = train_unlabelled_top100.index\n    lr_semi_prob.set_index('installation_id', inplace = True)\n\n    lr_semi_colnames = ['lr_prob_0', 'lr_prob_1', 'lr_prob_2', 'lr_prob_3']\n    lr_semi_prob.columns = lr_semi_colnames\n\n    # print(lr_semi_prob.shape)\n    print(lr_semi_prob.head())\n\n    confidence_threshold = 0.9\n\n    \"\"\"\n    # Skip Logreg\n    lr_is_confident0_mask = lr_semi_prob.loc[:, 'lr_prob_0'] > confidence_threshold\n    lr_confident0s = lr_semi_prob[lr_is_confident0_mask]\n    print('Logistic Regression found', lr_confident0s.shape[0], 'rows with above threshold confidence for class 0')\n\n    lr_is_confident1_mask = lr_semi_prob.loc[:, 'lr_prob_1'] > confidence_threshold\n    lr_confident1s = lr_semi_prob[lr_is_confident1_mask]\n    print('Logistic Regression found', lr_confident1s.shape[0], 'rows with above threshold confidence for class 1')\n\n    lr_is_confident2_mask = lr_semi_prob.loc[:, 'lr_prob_2'] > confidence_threshold\n    lr_confident2s = lr_semi_prob[lr_is_confident2_mask]\n    print('Logistic Regression found', lr_confident2s.shape[0], 'rows with above threshold confidence for class 2')\n\n    lr_is_confident3_mask = lr_semi_prob.loc[:, 'lr_prob_3'] > confidence_threshold\n    lr_confident3s = lr_semi_prob[lr_is_confident3_mask]\n    print('Logistic Regression found', lr_confident3s.shape[0], 'rows with above threshold confidence for class 3')\n    \"\"\"\n\n    # Predict for labels\n    xgb_semi = xgb.XGBClassifier(verbose = 1, reg_alpha = 5)\n    xgb_semi.fit(X_top100, y)\n\n    xgb_semi_preds = xgb_semi.predict(train_unlabelled_top100)\n    print(xgb_semi_preds[:5])\n\n    xgb_semi_prob = pd.DataFrame(xgb_semi.predict_proba(train_unlabelled_top100))\n    xgb_semi_prob['installation_id'] = train_unlabelled_top100.index\n    xgb_semi_prob.set_index('installation_id', inplace = True)\n\n    xgb_semi_colnames = ['xgb_prob_0', 'xgb_prob_1', 'xgb_prob_2', 'xgb_prob_3']\n    xgb_semi_prob.columns = xgb_semi_colnames\n    xgb_semi_prob.head()\n\n    confidence_threshold = 0.9\n\n    xgb_is_confident0_mask = xgb_semi_prob.loc[:, 'xgb_prob_0'] > confidence_threshold\n    xgb_confident0s = xgb_semi_prob[xgb_is_confident0_mask]\n    print('XGBoost found', xgb_confident0s.shape[0], 'rows with above threshold confidence for class 0')\n\n    xgb_is_confident1_mask = xgb_semi_prob.loc[:, 'xgb_prob_1'] > confidence_threshold\n    xgb_confident1s = xgb_semi_prob[xgb_is_confident1_mask]\n    print('XGBoost found', xgb_confident1s.shape[0], 'rows with above threshold confidence for class 1')\n\n    xgb_is_confident2_mask = xgb_semi_prob.loc[:, 'xgb_prob_2'] > confidence_threshold\n    xgb_confident2s = xgb_semi_prob[xgb_is_confident2_mask]\n    print('XGBoost found', xgb_confident2s.shape[0], 'rows with above threshold confidence for class 2')\n\n    xgb_is_confident3_mask = xgb_semi_prob.loc[:, 'xgb_prob_3'] > confidence_threshold\n    xgb_confident3s = xgb_semi_prob[xgb_is_confident3_mask]\n    print('XGBoost found', xgb_confident3s.shape[0], 'rows with above threshold confidence for class 3')\n\n    print(xgb_confident0s.head())\n    print(xgb_confident3s.head())\n\n    # Define new X_train style dataframes with just the confident threes and zeroes\n    confident_zeroes = train_unlabelled.loc[xgb_confident0s.index, top100_xgb_names]\n    confident_threes = train_unlabelled.loc[xgb_confident3s.index, top100_xgb_names]\n    print(confident_zeroes.shape)\n    print(confident_threes.shape)\n\n    # Concatenate  0's\n    X_train = pd.concat([X_top100, confident_zeroes], axis = 0)\n    X_train.shape\n\n    # Concatenate 3's\n    X_train = pd.concat([X_train, confident_threes], axis = 0)\n    X_train.shape\n\n    print(y.shape)\n    y = pd.DataFrame(y)\n    print(y.head())\n\n    y_confident_zeroes = pd.DataFrame(confident_zeroes.index)\n    y_confident_zeroes.set_index('installation_id', inplace = True, drop = True)\n    y_confident_zeroes['accuracy_group'] = 0\n\n    print(y_confident_zeroes.shape)\n    print(y_confident_zeroes.head())\n\n    y = pd.concat([y, y_confident_zeroes], axis = 0)\n    print(y.shape)\n    print(y.head())\n\n    y_confident_threes = pd.DataFrame(confident_threes.index)\n    y_confident_threes.set_index('installation_id', inplace = True, drop = True)\n    y_confident_threes['accuracy_group'] = 3\n    print(y_confident_threes.shape)\n\n    y = pd.concat([y, y_confident_threes], axis = 0)\n    print(y.shape)\n    print(y.head())\n    print(X_train.shape)","e076f60e":"if EMBED_TEST_FIT_CLUSTER_ON_TRAIN_FEATURE == True:\n    # Create a function for this\n    \n    # Store the X index\n    X_index = X.index\n    test_index = test_baseline.index\n    # Does labelling the predictions lead to better models?\n\n    # Scale the testing data\n    test_scaled = MinMaxScaler()\n    test_scaled = test_scaled.fit_transform(test_baseline.fillna(0))\n\n    # The training data is already scaled at \n\n    # Fit kmeans to the scaled test data.\n    kmeans_test = KMeans(n_clusters = N_CLUSTERS, random_state=0)\n    kmeans_test.fit(test_scaled)\n\n    # Predict the clusters that would result with characteristics similar to the training set.\n    kmeans_preds_on_train = kmeans_test.predict(X_scaled)\n\n    # Categorically encode these categories.\n    kmeans_preds_cat_encoded_train = pd.get_dummies(kmeans_preds_on_train, prefix='kmeans_')\n    kmeans_preds_cat_encoded_train.index = X_index\n\n    # Do the same thing with the test set.\n    kmeans_preds_on_test = kmeans_test.predict(test_scaled)\n\n    kmeans_preds_cat_encoded_test = pd.get_dummies(kmeans_preds_on_test, prefix='kmeans_')\n    kmeans_preds_cat_encoded_test.index = test_index\n    \n    train = pd.concat([X, kmeans_preds_cat_encoded_train], axis = 1)\n    test = pd.concat([test_baseline, kmeans_preds_cat_encoded_test], axis = 1)\n    \n    train.head()","84d24f23":"# Reassign inputs for unsupervised learning: \nX_train, y_train, X_test = X_train, y, test_baseline\nprint(X_train.shape, y_train.shape, X_test.shape)","81066889":"# Scale the data for cluster analysis\ntrain_scaled = MinMaxScaler()\ntrain_scaled = train_scaled.fit_transform(X_train)\n\n# Scale the testing data.\n# test_preprocessed = X_test.fillna(0).astype(int)\n\ntest_scaled = MinMaxScaler()\ntest_scaled = test_scaled.fit_transform(X_test)\n\n# Scale everything\ntotal_samples_scaled = MinMaxScaler()\ntotal_samples_scaled = total_samples_scaled.fit_transform(total_samples)","3c7be84f":"if EXPLORE_WITH_HIERARCHICAL_C == True:\n    from scipy.cluster.hierarchy import dendrogram\n\n    def get_hc_distances(X, verbose = 0):\n        distances = linkage(X, method=\"centroid\", metric=\"euclidean\")\n        return distances\n\n    def plot_dendrogram(distances):\n        dn = dendrogram(distances)\n\n    def create_hc_clusters(distances):\n        hc_clusters = fcluster(distances, 4, criterion=\"distance\")\n        return hc_clusters\n\n    if EXPLORE_WITH_HIERARCHICAL_C == True:\n        distances = get_hc_distances(train_scaled_eda)\n        plot_dendrogram(distances)\n\n        # Save the figure\n        save_current_fig('Dendrogram.jpg')\n\n        # Clusters not working yet.\n        hc_clusters = create_hc_clusters(distances)\n        hc_clusters[:50]\n    ","f6cb5501":"def scatterplot_of_2pca(X, X_pca, labels, use_matplotlib = True, use_plotly = False):\n    X_pca = pd.DataFrame(X_pca, index = X.index)\n\n    if use_matplotlib == True:\n        plt.scatter(x=X_pca.loc[:,0], y=X_pca.loc[:,1], c = labels, cmap = 'RdBu')\n        plt.title('Scatterplot of 2 highest principal components')\n        plt.show()\n\n    if use_plotly == True:\n        fig = px.scatter(X_pca.loc[:,0], y = X_pca.loc[:,1], color = y_train)\n        fig.update_layout(title = 'Scatterplot of 2 highest principal components')\n        fig.show()\n    \n    save_current_fig('Scattplot_2_pcas_train.jpg')","82756944":"DO_PCA = False","44d92bbe":"\nif DO_PCA == True:\n    PCA_N_COMPONENTS = 100\n\n    # Instantiate components of the pipeline\n    scaler = StandardScaler()\n    pca = PCA(n_components = PCA_N_COMPONENTS)\n\n    # Make and fit the pipeline\n    pipeline = make_pipeline(scaler, pca)\n    pipeline.fit(total_samples.fillna(0))   \n\n    total_pca = pca.transform(total_samples_scaled)\n\n    # Show the explained variances of the PCA features.\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_ratio_)\n    plt.title(\"Principal Component Analysis of Total Set\")\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n\n    print('Total variance with', PCA_N_COMPONENTS, 'components is', pca.explained_variance_ratio_[:PCA_N_COMPONENTS].sum())\n    save_current_fig(\"Principal_Component_Analysis_Total_Set.jpg\")\n    \n    scatterplot_of_2pca(total_samples, total_pca, labels = None)\n    \n    total_pca_df = pd.DataFrame(total_pca, index = total_index)\n\n    X_tr_pca_labelled = total_pca_df.loc[train_y_baseline.index.unique(), :]\n    X_tr_pca_unlabelled = total_pca_df.loc[train_unlabelled.index.unique(), :]\n    X_test_pca = total_pca_df.loc[test_baseline.index.unique(), :]\n    \n    X_tr_pca_labelled.head()","85370691":"if DO_PCA == True:\n    pca_xgb = xgb.XGBClassifier()\n    pca_xgb.fit(X_tr_pca_labelled, y)\n    \n    unlabelled_pca_preds = pca_xgb.predict(X_tr_pca_unlabelled)\n    unlabelled_preds_df = pd.DataFrame(unlabelled_pca_preds, index = train_unlabelled.index, columns = ['accuracy_group'])\n    unlabelled_preds_df.head()\n    \n    # Concatenate the unlabelled preds to total_y","7a23dca4":"if DO_PCA == True:\n    PCA_N_COMPONENTS = 100\n\n    # Instantiate components of the pipeline\n    scaler = StandardScaler()\n    pca = PCA(n_components = PCA_N_COMPONENTS)\n\n    # Make and fit the pipeline\n    pipeline = make_pipeline(scaler, pca)\n    pipeline.fit(X_train.fillna(0))   # On the whole thing or just the training set?\n\n    X_train_pca = pca.transform(train_scaled)\n\n    # Show the explained variances of the PCA features.\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_ratio_)\n    plt.title(\"Principal Component Analysis of Train Set\")\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n\n    print('Total variance with', PCA_N_COMPONENTS, 'components is', pca.explained_variance_ratio_[:PCA_N_COMPONENTS].sum())\n    save_current_fig(\"Principal_Component_Analysis_Train_Set.jpg\")\n    \n    # This Decodes the encoded compressed PCA information.  You can use this for your models.\n    X_train_PCA_inverse = pca.inverse_transform(X_train_pca)\n    X_train_PCA_inverse = pd.DataFrame(data = X_train_PCA_inverse, index = X_train.index)\n    \n    scatterplot_of_2pca(X_train, X_train_pca, y_train)","59eb8a75":"# Instantiate components of the pipeline\nDO_PCA_ON_TEST = False\n\nif DO_PCA_ON_TEST == True:\n    scaler = StandardScaler()\n    pca = PCA(n_components = PCA_N_COMPONENTS)\n\n    # Make and fit the pipeline\n    pipeline = make_pipeline(scaler, pca)\n\n    pipeline.fit(X_test.fillna(0))\n\n    X_test_pca = pca.transform(X_test.fillna(0))\n\n    # Show the explained variances of the PCA features.\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_)\n    plt.title(\"Principal Component Analysis of Test Set\")\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n\n    print('Total variance with', PCA_N_COMPONENTS, 'components is', pca.explained_variance_ratio_[:PCA_N_COMPONENTS].sum())\n    save_current_fig(\"Principal_Component_Analysis_Test_Set.jpg\")\n\n    scatterplot_of_2pca(X_test, X_test_pca, None)\n\n    X_test_PCA_inverse = pca.inverse_transform(X_test_pca)\n    X_test_PCA_inverse = pd.DataFrame(data = X_test_PCA_inverse, index = X_test.index)\n\n    fit_baseline_classifiers(tree_classifiers, X_train_pca, y)\n","a8f5e10a":"DO_KERNEL_PCA = True\nPCA_N_COMPONENTS = 50\n\nif DO_KERNEL_PCA == True:\n    from sklearn.decomposition import KernelPCA\n    \n    def do_kernel_pca():\n        pass\n    \n    #Apply kernel to train\n    \n    kernel_pca_train = KernelPCA(n_components = PCA_N_COMPONENTS, kernel = 'linear', fit_inverse_transform = True)\n    \n    kernel_pca_train_arr = kernel_pca_train.fit_transform(train_scaled)\n    kernel_pca_train_df = pd.DataFrame(data = kernel_pca_train_arr, index = X_train.index)\n    \n    kernel_pca_train_inverse = kernel_pca_train.inverse_transform(kernel_pca_train_arr)\n    kernel_pca_train_inverse_df = pd.DataFrame(data = kernel_pca_train_inverse, index = X_train.index)\n    \n    # Apply kernel to test\n    kernel_pca_test = KernelPCA(n_components = PCA_N_COMPONENTS, kernel = 'rbf', fit_inverse_transform = True)\n    \n    kernel_pca_test_arr = kernel_pca_test.fit_transform(test_scaled)\n    kernel_pca_test_df = pd.DataFrame(data = kernel_pca_test_arr, index = X_test.index)\n    \n    kernel_pca_test_inverse = kernel_pca_test.inverse_transform(kernel_pca_test_arr)\n    kernel_pca_test_inverse_df = pd.DataFrame(data = kernel_pca_test_inverse, index = X_test.index)","53731af4":"print(X_train.shape)\nprint(kernel_pca_train_df.shape)\nprint(y.shape)","2a022dee":"if DO_KERNEL_PCA == True:\n    scatterplot_of_2pca(X = X_train, \n                        X_pca = kernel_pca_train_df, \n                        labels = y)","027def05":"print(kernel_pca_test_df.shape)\nprint(X_test.shape)","5352336c":"if DO_KERNEL_PCA == True:\n    scatterplot_of_2pca(X = X_test, \n                        X_pca = kernel_pca_test_df, \n                        labels = None)","80748f87":"if DO_KERNEL_PCA == True:\n    fit_baseline_classifiers(tree_classifiers, kernel_pca_train_df, y)\n    kernel_pca_train_df.head(2)","9bc67dac":"xgb_pca = xgb.XGBClassifier()\nxgb_pca.fit(kernel_pca_train_df, y)","9d62f77f":"pca_preds = xgb_pca.predict(kernel_pca_test_df)\npca_preds[200:600]\n","26809de2":"EXPLORE_WITH_TSNE = False\n\nif EXPLORE_WITH_TSNE == True:\n    from sklearn.manifold import TSNE\n    \n    tsne_model = TSNE(learning_rate = 250, verbose = 1, perplexity = 500)\n    print(tsne_model)\n    \n    tsne_transformed = tsne_model.fit_transform(kernel_pca_train_df)\n    \n    # Define the x and y axes of the TSNE plot.\n    tsne_xs = tsne_transformed[:,0]\n    tsne_ys = tsne_transformed[:,1]\n\n    # Plot the TSNE\n    plt.scatter(tsne_xs, tsne_ys, c = y.values, cmap = 'RdBu')\n    plt.title('T-SNE Clusters of Training Dataset - Axes do not have meaning')\n    plt.show()\n    save_current_fig('T_SNE_of_Training_Dataset.jpg')","deb5416f":"if EXPLORE_WITH_TSNE == True:\n    from sklearn.manifold import TSNE\n    \n    tsne_model = TSNE(learning_rate = 250, verbose = 1, perplexity = 500)\n    print(tsne_model)\n    \n    tsne_transformed = tsne_model.fit_transform(total_pca)\n    \n    # Define the x and y axes of the TSNE plot.\n    tsne_xs = tsne_transformed[:,0]\n    tsne_ys = tsne_transformed[:,1]\n\n    # Plot the TSNE\n    plt.scatter(tsne_xs, tsne_ys)\n    plt.title('T-SNE Clusters of All datapoints - Axes do not have meaning')\n    plt.show()\n    save_current_fig('T_SNE_of_Total_Dataset.jpg')","589232c4":"EXPLORE_WITH_KMEANS = True\n\nfrom sklearn.metrics.cluster import homogeneity_score\n\ndef explore_with_kmeans(X, data_name, ground_truth = None, n_clusters = N_CLUSTERS):\n    # Scale before using with minmax or standardscaler.\n    \n    kmeans = KMeans(n_clusters = n_clusters, random_state=0)\n    kmeans.fit(X)\n    kmeans_preds = kmeans.predict(X)\n\n    clusters = kmeans_preds\n\n    # KMeans Distribution\n    # Be nice if this was normalized?\n    \n    kmeans_filename = 'Count_Plot_'+ str(data_name) + '_Cluster_Counts.jpg'\n    \n    sns.countplot(x = clusters).set_title(kmeans_filename)\n    plt.plot()\n    \n    save_current_fig(kmeans_filename)\n    \n    print('KMEANS Intertia of', data_name, ':', kmeans.inertia_)\n    # ADD - Write intertia to report.\n    \n    # Homogeneity of classes y (ground_truth) is given\n    try:\n        print('Homogeneity score is:', homogeneity_score(ground_truth, kmeans_preds))\n    except:\n        pass\n\n","e3eda065":"explore_with_kmeans(X = kernel_pca_train_df, \n                    data_name = 'Train', \n                    ground_truth = y_train, \n                    n_clusters = 4)","62619e4a":"explore_with_kmeans(X = kernel_pca_test_df, \n                    data_name = 'Test')","57dba584":"# Which train and test set would you like to use?\n# Use train and test for \"original\" dataframe\n# Use X_train_poly and X_test_poly for polynomial features.\n# X_train = train\n# X_test = test\n\ny_train = y_train[y_train.index.isin(X_train.index.unique())]\ny_train.shape\n\nprint('Make sure features have good shape')\nprint(X_train.shape, '\\n', y_train.shape, '\\n', X_test.shape)","84c6457a":"# Train test splitting.\n# Use this if doing original dataframe\n# If you use this you must retrain classifiers from scratch so as not to overfit.\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train, y_train, stratify=y_train, shuffle = True)\nprint('Splits', X_train_s.shape, y_train_s.shape, X_test_s.shape, y_test_s.shape)\n","981e1e70":"# Feature Selection + Hyperparameter Tuning Functions\n\n# Recursive Feature Elimination... Return Best Features\ndef recursive_feature_elimination(classifier, X_train, y_train):\n    rec_feat_elim = RFECV(classifier, C=1)\n    rec_feat_elim.fit(X_train, y_train) \n    rec_feat_elim.show()\n\n# Bayesian Hyperparamter Tuning\n\n# TPOT AutoML  Tuning\n# Add y_test as a parameter when doing train_test splits and validation.\ndef tpot_automl(X_train, y_train, X_test, generations=4, population_size=20, verbosity=3):\n    tpot = TPOTClassifier(generations=generations, \n                          population_size=population_size, \n                          cv=5,\n                          random_state=42, \n                          verbosity=verbosity)\n    \n    tpot.fit(X_train, y_train)\n    \n    # tpot.score(X_test, y_test)\n    \n    tpot_preds = tpot.predict(X_test)\n    \n    print(tpot.fitted_pipeline_)\n    tpot.export('tpot_pipeline.py')\n    \n    return tpot_preds","7a2ff6b1":"# This is just the instances, we will have to refit them to the splits to not overfit.\nUSE_SAME_CLASSIFIERS_FOR_END = True\n\nif USE_SAME_CLASSIFIERS_FOR_END == True:\n    pass\nelse:\n    # Random Forest Classifier\n    rf_clf = RandomForestClassifier(n_estimators = 500,\n                                   max_depth = 7,\n                                   class_weight='balanced',\n                                   n_jobs=-1)\n\n    # XGBoost Classifier\n    # xgb_data_matrix = xgb.DMatrix(data = X_train_s, label = y_train_s)\n    xgb_clf = xgb.XGBClassifier(n_jobs=-1,\n                                num_feature = 30,\n                                nfold=5)\n\n    # LightGBM Classifier\n    lb_clf = lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                             objective = 'multiclass',\n                                             is_unbalance = True,\n                                             lambda_l1 = 8)\n\n    # Catboost Classifier\n    cb_clf = cb.CatBoostClassifier(verbose=0)\n\n    #Bagging Classifier\n    bag_clf = BaggingClassifier(base_estimator=cb_clf, n_estimators=10, random_state=0, n_jobs=-1)\n\n    #TPOT Classifier\n\n    # Old List of classifiers\n    ## classifiers = [('Random Forest', rf_clf), ('XGBoost', xgb_clf), ('Bagging Classifier', bag_clf)]\n\n    # Just the three best.\n    classifiers = [('XGBoost', xgb_clf), ('LightGBM', lgb_clf), ('Catboost', cb_clf)]\n    \n    # Empty list to return predictions\n    preds = pd.DataFrame()","c5ef46b3":"def kfold_loop(classifiers, X_train, y_train):\n    str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n    fold = 0\n    \n    for train_index, test_index in str_kf.split(X_train, y_train):\n        # Obtain training and testing folds\n        \n        cv_X_train, cv_X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n        cv_y_train, cv_y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n        \n        print('Fold: {}'.format(fold))\n        print('CV train shape: {}'.format(cv_X_train.shape))\n        \n        fit_classifiers(classifiers, cv_X_train, cv_y_train, cv_X_test, cv_y_test)\n        \n        fold += 1\n","51d99d58":"# kfold_loop(classifiers, X_train, y_train)","edea0ae8":"# Run Functions Without Polynomail Features\n# Remove 'baseline' from classifier name.\n\nprint('Running classifiers after feature modifications, compared to baseline')\nfit_baseline_classifiers(tree_classifiers, np.array(X_train_s), y_train_s)\n","aa9ca615":"LB_FEAT_IMP = False\n\nif LB_FEAT_IMP == True:  \n    lb_clf_feat = lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                                 objective = 'multiclass',\n                                                 is_unbalance = True,\n                                                 lambda_l1 = 8)\n\n    lb_clf_feat.fit(X_train_s, y_train_s)\n    lgb.plot_importance(lb_clf_feat, max_num_features = 50, figsize = (20,20), title = 'Feature Importance of LightGBM Model')\n    save_current_fig('Feature_importance_of_lightGBM')","3623e406":"def plot_2d_decision_regions(X, y, clf):\n    return plot_decision_regions(X = X, y = y , clf = clf)","a8afe102":"# Load the two variables\n\nvar1 = 'accuracy'\nvar2 = 'accuracy_min'\n\nX_plot_decision = pd.DataFrame(X_train_s[var1].fillna(0))  # Create first variabel column\nX_plot_decision[var2] = X_train_s[var2].fillna(0)  # Create second variable column\n\nX_plot_decision = np.array(X_plot_decision)\ny_plot_decision = np.array(y_train_s)\n\nxgb_dec_clf = xgb.XGBClassifier()\nxgb_dec_clf.fit(X_plot_decision, y_train_s)\n\n# See if two dimensions is enough to converge on a decision region.\nax = plot_2d_decision_regions(X_plot_decision, y_plot_decision, xgb_dec_clf)\n# print(accuracy_score())\nplt.title('Decision Regions of ' + str(var1) + ' and ' + str(var2))\nsave_current_fig('Decision_Region.jpg')","8d36abda":"# Reporting Functions\ndef display_confusion_matrix(classifier, X_train, y_train, X_test, y_test):\n    try:\n        cm = ConfusionMatrix(classifier)\n        cm.fit(X_train, y_train)\n        cm.score(X_test, y_test)\n        cm.show() \n    except:\n        pass\n\n\ndef display_classification_report(classifier, X_train, y_train, X_test, y_test):\n    try:\n        cr = ClassificationReport(classifier)\n        cr.fit(X_train, y_train)\n        cr.score(X_test, y_test)\n        cr.show()\n    except:\n        pass\n    \ndef display_ROC(classifier, X_train, y_train, X_test, y_test):\n    try:\n        roc = ROCAUC(classifier)\n        roc.fit(X_train, y_train)\n        roc.score(X_test, y_test)\n        roc.show()\n    except:\n        pass\n    \ndef display_feature_importances(classifier, X_train, y_train):\n    try:\n        feature_imp = FeatureImportances(classifier)\n        feature_imp.fit(X_train, y_train)\n        print(list(feature_imp.features_))\n    except:\n        pass","bdbe3789":"# Display important reportings here\n# Ideally modify functions for subplots\n\ndef report_all(classifiers, X_train_display, y_train_display, X_test_display, y_test_display):\n    for clf_name, classifier in classifiers:\n\n        display_confusion_matrix(classifier, X_train_display, y_train_display, X_test_display, y_test_display)\n        display_classification_report(classifier, X_train_display, y_train_display, X_test_display, y_test_display)\n        display_ROC(classifier, X_train_display, y_train_display, X_test_display, y_test_display)\n\n        # Not all classifiers will have this.\n        try:\n            display_feature_importances(display_feature_importances, X_train_display, y_train_display)\n        except:\n            print('did not compute feature importance for: ', clf_name)","f88f9d44":"# Set the report to the features and then display\nX_train_display, y_train_display, X_test_display, y_test_display = X_train_s, y_train_s, X_test_s, y_test_s\n\n# Report all regular features\n## Remove Catboost b\/c it is causing problems in the reporting.\nclassifiers_report = tree_classifiers.copy()\n\nreport_all(classifiers_report, X_train_display, y_train_display, X_test_display, y_test_display)","c4ea4656":"# Set the report to the regular features and then display\n## X_train_display, y_train_display, X_test_display, y_test_display = X_train_s, y_train_s, X_test_s, y_test_s\n\n# Report all regular features\n# This throws an error because I fit the same classifiers and now they have different features.\n# I'd have to run this before and after I fit them unless I have the functions return values.\n\n## report_all(classifiers, X_train_display, y_train_display, X_test_display, y_test_display)","65e58d7a":"USE_DEEP_LEARNING = True","2b9ff6ef":"if USE_DEEP_LEARNING == True:\n    from keras.utils import to_categorical\n    from keras import models, layers\n    from keras.callbacks import EarlyStopping, ModelCheckpoint\n    from keras.optimizers import SGD\n    \n    print('Splits', X_train_s.shape, y_train_s.shape, X_test_s.shape, y_test_s.shape)\n\n    # DL Parameters\n    var_input_shape = (X_train_s.shape[1], )\n    layer_num = 400\n    activation = 'relu'\n    epochs = 40\n    batch_size = 20\n    \n    # For scaling\n    X_train_s_imp = X_train_s.fillna(0)\n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train_s_imp)\n    X_train_dl = X_train_scaled\n    print('Scaled the X training split')\n    print(X_train_dl[1][:5])\n\n    X_test_s_imp = X_test_s.fillna(0)\n    scaler_test = MinMaxScaler()\n    X_test_scaled = scaler_test.fit_transform(X_test_s_imp)\n    X_test_dl = X_test_scaled\n    print('Scaled the X test split')\n    print(X_test_dl[1][:5])\n\n\n    # To_Categorical so output shape is (n,4)\n    y_train_dl = to_categorical(np.array(y_train_s))\n    print('Categorically encoded targets of training data.')\n    print(y_train_dl[1])\n\n    y_test_dl = to_categorical(np.array(y_test_s))\n    print('Categorically encoded targets of testing data.')\n    print(y_test_dl[1])\n\n    print('Splits', X_train_dl.shape, y_train_dl.shape, X_test_dl.shape, y_test_dl.shape)\n\n\n\n    # # This standard model has 3 layers with ____ nodes with \n    # a dropout layer that gets rid of 50% of the learning\n    # To encourage the learning to 'spread' throughout\n    # the network.\n    \n\n    network = models.Sequential()\n\n    network.add(layers.Dense(layer_num, input_shape = var_input_shape, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n    network.add(layers.Dense(layer_num, activation='relu'))\n    network.add(layers.Dropout(0.5))\n\n    # Final layer\n    network.add(layers.Dense(4, activation='softmax'))\n\n    sgd = SGD(lr=0.005, momentum=0.4, nesterov=True)\n    \n    # Categorical cross entropy for multi-class problems\n    network.compile(optimizer = sgd,\n                   loss = 'categorical_crossentropy',\n                   metrics=['accuracy'])\n\n    # early_stopping = EarlyStopping(monitor='val_accuracy', patience = 8)\n\n    model_save = ModelCheckpoint('best_model.hdf5',\n                                save_best_only=True)\n\n    dl_history = network.fit(X_train_dl, \n                             y_train_dl, \n                             epochs = epochs, \n                             batch_size=batch_size, \n                             verbose=2,\n                            validation_data=(X_test_dl, y_test_dl),\n                            callbacks = [model_save])","62cae3c6":"if USE_DEEP_LEARNING == True:\n    plt.figure()\n    plt.plot(dl_history.history['accuracy'])\n    plt.plot(dl_history.history['val_accuracy'])\n    plt.title('Keras Model Accuracy on Training set and Validation Set')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train','Test'])\n    plt.show()\n\n    save_current_fig('Keras_Model_Acc_Training_vs_validation_set.jpg')","9aafbac9":"if USE_DEEP_LEARNING == True:\n    # The submission file has more training samples,\n    # So should do better.  Process that.\n    # Process the submission training set.\n\n    X_train_submit_dl = X_train.fillna(0)\n    scaler_train_submit = MinMaxScaler()\n    X_submit_scaled = scaler_train_submit.fit_transform(X_train_submit_dl)\n    print('Scaling X of the entire TRAINING set for ensembling with neural network')\n    print(X_submit_scaled[1][:5])\n\n\n    X_test_submit_dl = X_test.fillna(0)\n    test_scaler_submit = MinMaxScaler()\n    X_test_submit_scaled = test_scaler_submit.fit_transform(X_test_submit_dl)\n    print('Scaling X of the entire TEST set for ensembling with neural network')\n    print(X_test_submit_scaled[1][:5])\n\n\n    y_train_submit_dl = to_categorical(np.array(y_train))\n    print('Categorically encode targets of ALL the training set.')\n    print(y_train_submit_dl[1])\n\n\n    layer_num = layer_num\n\n    network_submit = models.Sequential()\n\n    network_submit.add(layers.Dense(layer_num, input_shape = var_input_shape, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n    network_submit.add(layers.Dropout(0.5))\n    network_submit.add(layers.Dense(layer_num, activation='relu'))\n\n    # Final layer\n    network_submit.add(layers.Dense(4, activation='softmax'))\n\n    # Categorical cross entropy for multi-class problems\n    network_submit.compile(optimizer = sgd,\n                   loss = 'categorical_crossentropy',\n                   metrics=['accuracy'])\n\n    # early_stopping = EarlyStopping(monitor='val_accuracy', patience = 8)\n\n    model_save = ModelCheckpoint('best_model.hdf5',\n                                save_best_only=True)\n\n    network_submit.fit(X_submit_scaled, \n                       y_train_submit_dl, \n                       epochs = epochs, \n                       batch_size=batch_size, \n                       verbose=2,\n                        callbacks = [model_save])\n\n    print(X_test_submit_scaled[1][:5])","f1267c58":"if USE_DEEP_LEARNING == True:\n    dl_preds = network_submit.predict_classes(X_test_submit_scaled)\n    dl_preds.shape\n\n    model_prediction['NN_preds'] = dl_preds\n    model_prediction.head()\n\n    print(model_prediction.head(100))","a43a19a2":"# Define the best results here for the submission\n# This should NOT be the splits, but the whole thing.\nX_train_submit, y_train_submit, X_test_submit = X_train, y_train, X_test\n\n# CHOOSE WHICH SUBMISSION PREDICTIONS TO USE HERE\nfinal_submission_preds = dl_preds\nfinal_submission_preds_name = \"Neural Network Preds\"\n\nprint(X_train_submit.shape, y_train_submit.shape, X_test_submit.shape)\n\n# Create the empty dataframe and copy the indices of the test set.\nmodel_prediction = pd.DataFrame()\nmodel_prediction['installation_id'] = X_test_submit.index.astype(str)\n\nprint(model_prediction.shape)\n\n# Choose the final model here\nsubmission_model = lgb.LGBMClassifier(min_gain_to_split = 0.9,\n                                     objective = 'multiclass',\n                                     is_unbalance = True,\n                                     lambda_l1 = 8)\n\n# Fit the model and create the predictions.\nsubmission_model.fit(np.array(X_train_submit), y_train_submit)\n\n# IF FITTING HERE: Predict on the test set from the fitted model.\n# Otherwise use another pred output from a different model.\n# like baseline_preds or PCA_preds.\n\n# MODEL\n# submission_preds = submission_model.predict(X_test_submit)\n\nsubmission_preds = final_submission_preds\n\n# load this into the submission dataframe. \nmodel_prediction['accuracy_group'] = submission_preds.astype(int)\n\n# Create a csv file out of these predictions.\nmodel_prediction.to_csv('submission.csv', index=False)\n\n# Confirm everythin looks ok.\nprint('Submission file created from', final_submission_preds_name)\nprint(model_prediction.head(30))\nprint(model_prediction.shape)\nprint(model_prediction.info())\nprint(model_prediction['accuracy_group'].value_counts())\n\n# Check output file was created in right spot.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fba0c1c4":"## Semi-Supervised Confidence Labelling With LogReg, XGB ##","c8a85f5e":"## Update memory types ##","b94087db":"### Scale features for models that require scaling  ###","e470618c":"## NLP Feature Engineering ##","1d3e0ea5":"## If Loading The Data From Saved External Dataframe ##","994e4804":"**Predict accuracy group on unlabelled data**","5a4c75e1":"** Fit to Experimental Validation Sets**","33ea2d63":"### Do a quick look at the distributions of important factors ###","7573d8a3":"## Baseline Submission and 100 Features Submission ##","47f76c2f":"**Baseline submission with all features **","7f5e43b4":"**Reporting code - Consider removing? **","fd3ab83f":"# Questions I am Exploring #\n\nLooking for some support with the community.  I can't quite figure out why:\n* My PCA based model is spitting out all 3's.  \n* My model performance is so bad on the public leaderboard (0.1-0.25, not close to what I am getting in CV).\n* My submissions no longer work.\n","079f7189":"### Instantiate All Classifiers and Fit ###","6f49d715":"### Post Submission Reports for Modification ###","0cb61d9c":"## PCA ##","d72c0aee":"## TESTING Disabled - Grid Search for LightGBM Submission ##","22f6b036":"## Manifold Visualizations After PCA ##","79cf8627":"**Load all at once**","fcd0f392":"# Cleaning and Processing Data after Baseline #","2bb54c13":"### Impute Missing Values ###","14fc6806":"### Create Datetime Features + Do Timeseries Statistics ###","2ce47378":"# File Submission #","fc876203":"** Preprocess Submission X_test for Neural Network**","a5a09e18":"### Preliminary cleaning of outliers in original sets before aggregating ###","9d63e1c3":"## If Loading The Data in Real Time ##","07fba1e2":"## Deep Learning ##","a35c5ab5":"**Submission with just 100 features**","922f7635":"## Feature Engineering ##","799e18c5":"** Model does better if we leave 0 game times in**","82cec3a6":"**Stratified KFold Loop**","3b848bf6":"# FastAI Baseline #","83c51eaa":"### Fit Models After Feature Selection, Semi-Supervised Labelling, etc. ###","f248ddc1":"**PCA on everything**","1448b454":"## Reduce test_baseline to just the 100 top features. ##","99c6ce2d":"** Load feature importances of the baseline so we can reduce the dimensionality with feature selection. **","26f58703":"### Kernel PCA ###","3dd2305f":"**Loading Data in chunks and processing per ID to save ram**","18e9bf27":"### Remove features that are not represented in the train set ###","d162b7d1":"** Create a dataframe with all labelled training data merged on installation ID and game_session **","54072f5a":"## Encode Clusters With Semi-Supervised Learning ##","4a0069a4":"### Hierarchical Clustering Exploration ###","119a1d1b":"## Unsupervised Exploration and Visualization ##\n","9f08c015":"# Reporting Pipeline for One Classifier #","3e421dd7":"### Concatenate num correct and num incorrect and calculate accuracy like train_labels ###","f11af676":"### Feature Selection ###\n\n* Use principal component analysis (PCA) to determine features that explain the most variance.\n* Use Nonlinear PCA\n* Use random forests to determine feature importances.","782efe27":"### Do some Groupby's on the whole dataset before I focus in on assessments ###","6c1645cd":"### Plot Decision Regions with several high-importance variables ###","417e978c":"Display some visuals here.","582879ce":"**PCA on the test set**","6e92033e":"Best parameters so far\n* xgb max depth = ~22\n* reg_alpha = 3\n* reg_lambda\n* n_estimators\n* learning_rate\n* booster\n* gamma \n* importance_type","20b0ef18":"### Split into labelled and unlabelled train for semi-supervised learning ###","df555a12":"Feature elimination and tpot automl","e1b162f2":"### MinMax Scale Data for Clustering (Hierarchical, Kmeans, Other) ###","9286c492":"**PCA on the training set **","530bf8df":"**Metrics Helper Functions**","bd007031":"### T-SNE Grid Search Experiment ###","aa256b05":"### Lightboost Feature Importances ###","4a18f4ca":"### Create submission file with nonlinear pca since it did well? ###","27a4796f":"## Concatenate everything to train\/test_baseline DataFrames","147d6bc7":"### Train Test Splits - Several Splits to test Comparative Accuracy ##"}}