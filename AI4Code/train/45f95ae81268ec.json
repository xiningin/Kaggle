{"cell_type":{"390794b4":"code","ea51dd57":"code","6edd81c8":"code","ce51c8b8":"code","5ba23b31":"code","c179b4e2":"code","115b6f9e":"code","ece2f3eb":"code","2383980a":"code","eddd4a04":"code","2d7e1863":"code","05dfbff6":"code","b4934b76":"code","6b380ae8":"code","e8355939":"code","d723f5fe":"code","09dad589":"code","781833cc":"code","d334be91":"code","eb8f28e2":"code","800d569c":"code","37bfb2ef":"code","76e14226":"code","322b6438":"code","8e988ad1":"code","39515a0e":"markdown","f8de2baa":"markdown","69ff4493":"markdown","7daae7cc":"markdown","09b133a8":"markdown","c018b49b":"markdown","512ae7d1":"markdown","9bff1d2c":"markdown","52c5779a":"markdown","6916be2f":"markdown","da2ffe7f":"markdown","23227cba":"markdown","fa794222":"markdown","47433a26":"markdown","64df5629":"markdown","c425e930":"markdown","410ba160":"markdown","340784c4":"markdown"},"source":{"390794b4":"# !pip install fastai\n# import libraries\nimport missingno as msno\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \nimport pandas as pd\nimport numpy as np\nfrom functools import partial\nimport io\nimport os","ea51dd57":"import matplotlib.pyplot as plt\nimport seaborn as sns","6edd81c8":"train = pd.read_csv(\"\/kaggle\/input\/textdb3\/fake_or_real_news.csv\")","ce51c8b8":"train","5ba23b31":"train_df = train[[\"text\",\"label\"]]","c179b4e2":"train_df = train_df[:1000]","115b6f9e":"train_df","ece2f3eb":"data_lm = (TextList.from_df(train_df)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.20)\n           #We randomly split and keep 20% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=128))\ndata_lm.save('tmp_lm')","2383980a":"data_lm.show_batch()","eddd4a04":"# Language model AWD_LSTM\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","2d7e1863":"print('Model Summary:')\nprint(learn.layer_groups)","05dfbff6":"learn.fit_one_cycle(1, 1e-2)\nlearn.save('lm_hyper')","b4934b76":"learn.save_encoder('ft_enc')\n","6b380ae8":"data_clas = (TextList.from_df(train_df, cols=[\"text\"], vocab=data_lm.vocab)\n             .split_by_rand_pct(0.3)\n             .label_from_df('label')\n             .databunch(bs=32))\n\ndata_clas.save('tmp_class')\n","e8355939":"help(learn)","d723f5fe":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n","09dad589":"learn.load_encoder('ft_enc')","781833cc":"learn.freeze_to(-1)\nlearn.summary()","d334be91":"torch.cuda.empty_cache()","eb8f28e2":"help(learn)","800d569c":"learn.fit_one_cycle(1, 1e-1)","37bfb2ef":"learn.save('stage1')","76e14226":"from fastai.vision import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6,6), dpi=60)","322b6438":"interp = TextClassificationInterpretation.from_learner(learn)\ninterp.show_top_losses(10)","8e988ad1":"learn.export()\nlearn.model_dir = \"\/kaggle\/working\"\nlearn.save(\"stage-1\",return_path=True)","39515a0e":"<a id=\"18\"><\/a>\n<font color=\"blue\" size=+2.5><b>Feedback and Support<\/b><\/font>\n<br\/>\n* Your feedback is much appreciated\n* Please UPVOTE if you LIKE this notebook\n* Comment if you have any doubts or you found any errors in the notebook","f8de2baa":"Now we can create a language model based on the architecture \n[AWD_LSTM](https:\/\/docs.fast.ai\/text.models.html#AWD_LSTM)","69ff4493":"..\/input\/textdb3\/fake_or_real_news.csv","7daae7cc":"<a id=\"23\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.3 Save and Load Model<\/b><\/font>\n<br\/>\n","09b133a8":"<a id=\"14\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4.1 Loading Data For Text Classification <\/b><\/font>\n","c018b49b":"<a id=\"15\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4.2 Loading Text Classification Model <\/b><\/font>\n","512ae7d1":"**Now, the training cycle is repeated: lr_find, freeze except last layer,..., unfreeze the model and saving the final trained model.**","9bff1d2c":"**Lets train our language model. First, we call lr_find to analyze and find an optimal learning rate for our problem, then we fit or train the model for a few epochs. Finally we unfreeze the model and runs it for a few more epochs. So we have a encoder trained and ready to be used for our classifier and it is recorded on disk.**","52c5779a":"<a id=\"22\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.1 Interpret the results<\/b><\/font>\n<br\/>","6916be2f":"<a id=\"5\"><\/a>\n<font color=\"blue\" size=+2.5><b>2.2 Library Import<\/b><\/font>","da2ffe7f":"<a id=\"10\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.5 Finding LR <\/b><\/font>\n","23227cba":"**we will have to load the encoder previously trained (the language model).**","fa794222":"<a id=\"9\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.4 Model Summary <\/b><\/font>\n","47433a26":"<a id=\"24\"><\/a>\n<font color=\"blue\" size=+2.5><b>5.4 Sources<\/b><\/font>\n<br\/>\n* [Fastai MOOC](https:\/\/course.fast.ai\/)\n* [Fastai library](https:\/\/docs.fast.ai\/)","64df5629":"<a id=\"6.1\"><\/a>\n# <font color=\"blue\" size=+2.5><b>Data Loading<\/b><\/font>","c425e930":"<a id=\"12\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.7 Saving Model After Training <\/b><\/font>","410ba160":"<a id=\"13\"><\/a>\n<font color=\"blue\" size=+2.5><b> 4. Building and Training a Text Classifier <\/b><\/font>","340784c4":"<a id=\"11\"><\/a>\n<font color=\"blue\" size=+2.5><b> 3.6 HyperParameter Tuning For Model Training <\/b><\/font>"}}