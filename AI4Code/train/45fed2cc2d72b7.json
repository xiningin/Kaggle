{"cell_type":{"8adf71f3":"code","d3427b34":"code","21891a85":"code","2be3f4bd":"code","b555cbd2":"code","18f0ce39":"code","a42c3d00":"code","4f092d42":"code","bf35d7f4":"code","9ed40ac7":"code","8630e19c":"code","433f968e":"code","286afd45":"code","6bc8b13d":"code","98019fa1":"code","e67a53df":"code","b3c9e873":"code","d337b9fa":"code","d7adff70":"code","6dbe2409":"code","f0624ae0":"code","254763bc":"code","050ca8e1":"code","4a1cf9cd":"code","12a817a9":"code","f3c044ab":"code","557c884b":"code","0ab7dacd":"code","93674ea4":"code","809019f4":"code","2b9c63df":"code","5060b977":"code","79b6b861":"code","73008f2e":"code","3d813f35":"code","956d05de":"code","1721d2dc":"code","0649a40a":"code","29891885":"code","145c67dd":"code","9e931d85":"code","d44fd360":"code","89fe8006":"code","b997df6c":"code","fc9d7172":"code","75d8a4d5":"code","efcd682d":"code","16c2b95d":"code","dbab3c37":"code","8d1d564d":"code","38fd7309":"code","6c44b52e":"code","3b98f3e4":"code","6a8835a3":"code","0ccaa014":"code","66a70144":"code","bdbc2bc2":"code","052d2bba":"code","83097a75":"markdown","c0ffca68":"markdown","d1d87906":"markdown","5f5cf803":"markdown","62027c0a":"markdown","9f3b2704":"markdown","cf48ee44":"markdown","47ce6e10":"markdown","4dd54c3f":"markdown","17159d0c":"markdown","a20e37a5":"markdown","98c921d1":"markdown","778f62e1":"markdown","08e93b17":"markdown","4a2241ef":"markdown","dfd59f85":"markdown","0a7f63a9":"markdown","270c0788":"markdown","3cf0f64e":"markdown","c8a58994":"markdown","37303719":"markdown","22efa9d7":"markdown","3b32f7e0":"markdown","25d6d7ab":"markdown","404ff5a8":"markdown","91154bd5":"markdown","bce3164a":"markdown","5ae487a0":"markdown","c2a7b2f4":"markdown"},"source":{"8adf71f3":"# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d3427b34":"# setting sns backgroud style\nsns.set_style('darkgrid')","21891a85":"# import warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2be3f4bd":"# Read the data set in `heart_data`\nheart_data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\nheart_data.head()","b555cbd2":"heart_data.shape","18f0ce39":"heart_data.isnull().sum()","a42c3d00":"heart_data.describe()","4f092d42":"heart_data.info()","bf35d7f4":"heart_data.nunique()","9ed40ac7":"heart_data.thall.value_counts()","8630e19c":"heart_data.slp.value_counts()","433f968e":"# output percentages of 1s and 0s\nheart_data.output.value_counts(normalize=True)*100","286afd45":"k = ['sex', 'cp', 'restecg']\nfor i in k:\n    print(heart_data[i].value_counts())","6bc8b13d":"cols_list = list(heart_data.columns)\ncols_list.pop(cols_list.index('output'))\ncols_list","98019fa1":"# boxplot of the data set\nplt.figure(figsize=[10,8])\nheart_data.boxplot()\nplt.show()","e67a53df":"# boxplot of cholestrol\nplt.figure(figsize=[8,6])\nplt.boxplot(heart_data.chol)\nplt.xlabel(\"Cholestrol\")\nplt.show()","b3c9e873":"heart_data.chol.quantile([0.25, 0.50, 0.75, 0.99, 0.998, 1.00])","d337b9fa":"# heart_data[heart_data.chol > heart_data.chol.quantile([0.99]).values[0]]","d7adff70":"# # dropping rows greater than 99%tile cholestrol\n# heart_data = heart_data[heart_data.chol <= heart_data.chol.quantile([0.99]).values[0]]","6dbe2409":"heart_data.shape","f0624ae0":"# boxplot of the data set\nplt.figure(figsize=[10,8])\nheart_data.boxplot()\nplt.show()","254763bc":"# check outliers using boxplot for trtbps\nplt.figure(figsize=[8,6])\nplt.boxplot(heart_data.trtbps)\nplt.show()","050ca8e1":"heart_data.head()","4a1cf9cd":"# age distribution among outputs\nplt.figure(figsize=[20,6])\nplt.subplot(1,2,1)\nsns.distplot(heart_data[heart_data.output == 1]['age'], color='g')\nplt.subplot(1,2,2)\nsns.distplot(heart_data[heart_data.output == 0]['age'], color='b')\n\nplt.show()","12a817a9":"# check 'outputs'\n\nsns.countplot(data=heart_data, x='output')\nplt.show()","f3c044ab":"# check outputs based on sex\n\n# plt.figure(figsize=[10,6])\nsns.countplot(data=heart_data, x='sex', hue='output')\nplt.show()","557c884b":"# check Chest pain\n\nplt.figure(figsize=[10,6])\nsns.countplot(data=heart_data, x='cp', hue='output')\nplt.show()","0ab7dacd":"# check resting electrocardiographic\n\nplt.figure(figsize=[10,6])\nsns.countplot(data=heart_data, x='restecg', hue='output')\nplt.show()","93674ea4":"# pairplot on numerical columns\nall_cols = heart_data.columns\nsns.pairplot(heart_data[all_cols], hue='output')\nplt.show()","809019f4":"# correlation heatmat\nplt.figure(figsize=[15,15])\nsns.heatmap(heart_data.corr(), annot=True, cmap='RdYlGn')\nplt.show()","2b9c63df":"heart_data.head()","5060b977":"heart_data.shape","79b6b861":"# importing train test split library\nfrom sklearn.model_selection import train_test_split","73008f2e":"heart_data.columns","3d813f35":"X = heart_data.drop('output', axis=1)\ny = heart_data.output\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)","956d05de":"# num_cols = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak',]\nnum_cols = list(heart_data.columns)\nnum_cols.pop(num_cols.index('output'))\n\n# importing scaling library\nfrom sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import MinMaxScaler\n\n# create scaler object\nscaler = StandardScaler()\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])","1721d2dc":"from sklearn.linear_model import LogisticRegression","0649a40a":"loglm = LogisticRegression()\n\nloglm.fit(X_train, y_train)","29891885":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, plot_roc_curve","145c67dd":"# evaluation function\ndef evaluation(model):\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    print(\"=\"*50)\n    print(confusion_matrix(y_train, y_train_pred))\n    print(classification_report(y_train, y_train_pred))\n    print(\"Accuracy of TRAIN data:\",accuracy_score(y_train, y_train_pred))\n    print(\"=\"*50)\n    print(confusion_matrix(y_test, y_test_pred))\n    print(classification_report(y_test, y_test_pred))\n    print(\"Accuracy of TEST data:\",accuracy_score(y_test, y_test_pred))\n    print(\"=\"*50)\n    \n    # Plot ROC_AUC Curve\n    plot_roc_curve(model, X_test, y_test)\n    plt.title('ROC_AUC Curve', fontsize=16)\n    plt.show()","9e931d85":"evaluation(loglm)","d44fd360":"# importing library\nfrom sklearn.neighbors import KNeighborsClassifier","89fe8006":"# iterating knn model for all values of k from 1-39\nerror_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","b997df6c":"# ploting the error_rate\n\nplt.figure(figsize=[10,6])\nplt.plot(range(1,40), error_rate, color='blue', linestyle='dashed',\n         marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","fc9d7172":"# building knn and fit the model\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X_train, y_train)","75d8a4d5":"# Evaluation\nevaluation(knn)","efcd682d":"# import \nfrom sklearn.ensemble import RandomForestClassifier","16c2b95d":"# using GridSearchCV\nfrom sklearn.model_selection import GridSearchCV","dbab3c37":"params = {\n    'max_depth': [5, 10, 20, 30, 50],\n    'min_samples_leaf': [10, 20, 30, 50, 100],\n    'max_features': [2, 5, 8, 11, 13],\n    'n_estimators': [10, 30, 50, 100, 200]}\n\nrfm_basic = RandomForestClassifier(random_state=42, oob_score=True)\n\ngrid_search = GridSearchCV(estimator=rfm_basic, param_grid=params,\n                          cv=5, n_jobs=-1, verbose=1, scoring=\"accuracy\")","8d1d564d":"%%time\ngrid_search.fit(X_train, y_train)","38fd7309":"rfm_best = grid_search.best_estimator_\nrfm_best","6c44b52e":"# prediction and model evaluation\nevaluation(rfm_best)","3b98f3e4":"# import\nfrom xgboost import XGBClassifier","6a8835a3":"xgb = XGBClassifier(n_jobs=-1, tree_method='gpu_hist')\n\nxgb.fit(X_train, y_train)","0ccaa014":"# prediction and model evaluation\nevaluation(xgb)","66a70144":"# hyperparameters\nxgb_model = XGBClassifier(n_jobs=-1, tree_method='gpu_hist')\n\nparameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n              'max_depth': [2, 4, 6, 8, 10],\n              'min_child_weight': [3, 7, 11, 19, 25],\n              'n_estimators': [50, 100, 150, 200, 300, 500]}\n\nscorer = metrics.make_scorer(metrics.roc_auc_score,\n                             greater_is_better=True,\n                             needs_proba=True,\n                             needs_threshold=False)\n\nclf_xgb = GridSearchCV(estimator=xgb_model,\n                                       param_grid=parameters,\n                                       n_jobs=-1,\n                                       cv=3,\n                                       scoring=scorer,\n                                       refit=True)\n\nclf_xgb.fit(X_train, y_train)","bdbc2bc2":"print(clf_xgb.best_params_)\nprint(clf_xgb.best_score_)\nprint(clf_xgb.best_estimator_)","052d2bba":"# prediction and evaluation\nevaluation(clf_xgb.best_estimator_)","83097a75":"## Get the data","c0ffca68":"## Data Visualization","d1d87906":"- **Accuracy for XGBoost model is 84%**","5f5cf803":"**Box Plots of all columns except <i>output<\/i> column**","62027c0a":"## XGBoost Model","9f3b2704":"**NOTE: Check for the steps at the end of the notebook**","cf48ee44":"## Building Logistic Regression Model","47ce6e10":"### Prediction","4dd54c3f":"### Choosing a K Value \n\nLet's go ahead and use the elbow method to pick a good K Value:","17159d0c":"#### Let's implement KNN again at k=15","a20e37a5":"- **Accuracy for Logistic Regression model is 85%**","98c921d1":"- We can see that error rate is low at k=15.","778f62e1":"**Steps:**\n- We have inspected the data for null and dtypes\n- Cleaned the data as there are outliers present\n- Analysed the data for insights\n- Model Building and prediction:\n    - 1. Logistic Regression. Accuracy: 85%\n    - 2. K Nearest Neighbour. Accuracy: 90%\n    - 3. Random Forest Classifier. Accuracy: 82.4%\n    - 4. XGBoost Model. Accuracy: 84%","08e93b17":"## Data Preprocessing","4a2241ef":"### Using Hyper-Parameter Tuning","dfd59f85":"### Model Evaluation","0a7f63a9":"### Scaling numerical columns","270c0788":"## Random Forest Classifier Model","3cf0f64e":"Lets remove outliers, i.e., remove Cholestrol level data greater than 99%tile","c8a58994":"## Spliting to Train and Test","37303719":"## Data Inspection","22efa9d7":"## Accuracy is highest with 90% using K-NN model","3b32f7e0":"# **Heart Attack Analysis & Prediction**","25d6d7ab":"## About this dataset\n\n- Age : Age of the patient\n- Sex : Sex of the patient\n- exang: exercise induced angina (1 = yes; 0 = no)\n- ca: number of major vessels (0-3)\n- cp : Chest Pain type chest pain type\n    - Value 1: typical angina\n    - Value 2: atypical angina\n    - Value 3: non-anginal pain\n    - Value 4: asymptomatic\n- trtbps : resting blood pressure (in mm Hg)\n- chol : cholestoral in mg\/dl fetched via BMI sensor\n\n- fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n- rest_ecg : resting electrocardiographic results\n    - Value 0: normal\n    - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n    - thalach : maximum heart rate achieved\n\n- target : 0= less chance of heart attack 1= more chance of heart attack\n","404ff5a8":"## So, finally from the above model we could see that K-NN model has the best Accuracy Score with 90%","91154bd5":"## Building K-Nearset Neighbors Model","bce3164a":"### Hyper- parameter tuning","5ae487a0":"- **Accuracy for K-Nearest Neighbors model is 90%**","c2a7b2f4":"- **Accuracy for Random Forest Classifier model is 82.4%**"}}