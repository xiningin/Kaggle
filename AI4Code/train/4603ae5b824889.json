{"cell_type":{"9bc20088":"code","c0ae359d":"code","c4650d12":"code","e83aafb1":"code","d818945e":"code","c84d5104":"code","2a693180":"code","4f1c898f":"code","0536091f":"code","c21f77ff":"code","a3f75078":"code","a65e1d96":"code","7f671d21":"code","3885f3d4":"code","fbeb9d46":"code","81858844":"code","fae1b787":"code","e1087b88":"code","8b2d733a":"code","7d787bf3":"code","5b9a11e3":"code","e9ee247c":"code","ef8c3c19":"code","59d058f5":"code","f3597174":"code","9f06a108":"code","a9525b02":"code","996fed80":"code","881bd1c6":"code","83776ac1":"code","fde68c4f":"code","caa3df61":"code","6aae13b3":"code","9dca2f37":"code","9f82a694":"code","cc127d87":"code","65bf8e94":"code","147bf12e":"code","ef5862d9":"code","c27d85e5":"code","2b08ca18":"code","8a0f0059":"code","b7ca7dcb":"code","b9dd327d":"code","76dca52b":"code","b6ee298c":"code","a41ac554":"markdown","99a9485a":"markdown","01ecd49b":"markdown"},"source":{"9bc20088":"!pip install pyspark","c0ae359d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4650d12":"from pyspark.sql import SparkSession\n","e83aafb1":"spark  = SparkSession.builder.appName('Dataframe').getOrCreate()","d818945e":"# Read the datasets\ndf_housing = spark.read.option('header', 'true').csv(os.path.join(dirname, 'housing.csv'), inferSchema = True)","c84d5104":"#Showing first 20 rows\ndf_housing.head(3)","2a693180":"#Check the schema\ndf_housing.printSchema()","4f1c898f":"#print column names\ndf_housing.columns","0536091f":"#Checking data entries for each column\ndf_housing.select(['longitude',\n 'latitude',\n 'housing_median_age',\n 'total_rooms',\n 'total_bedrooms']).describe().show()","c21f77ff":"df_housing.select(['population',\n 'households',\n 'median_income',\n 'median_house_value',\n 'ocean_proximity']).describe().show()","a3f75078":"#column overview\npd.DataFrame(df_housing.dtypes, columns = ['Column Name','Data type'])\n","a65e1d96":"df_housing.select(['longitude']).show()","7f671d21":"#Check any missing value\nfor column in df_housing.columns:\n    print(column, df_housing.filter(col(column).cast(\"float\").isin([None,np.nan])).count())\n\n#no null values in the dataframe","3885f3d4":"# number and percentage of unique values \nfor column in df_housing.columns:\n    print(column, format(df_housing.select(column).distinct().count(), ',d'), \n          \"%.2f\" % (df_housing.select(column).distinct().count()\/df_housing.select(column).count()*100))","fbeb9d46":"df_housing = df_housing.withColumnRenamed('median_house_value','price')","81858844":"#Data imputation\ndf_housing.na.drop()","fae1b787":"type(df_housing['price'])","e1087b88":"#Checking if the prices are normally distributed\nsns.distplot(df_housing.select('price').toPandas(), color=\"skyblue\")\ndf_housing.select(F.skewness('price'), F.kurtosis('price')).show()","8b2d733a":"#Housing prices greater than 500,000 (expensive houses)\nprint(\"No of houses: %i\" % df_housing.select('price').count())\nprint(\"No of houses greater than $500000 are: %i\" % df_housing.filter(df_housing[\"price\"] > 500000).count())","7d787bf3":"#Distribution of prices\nsns.set_style(\"darkgrid\")\nsns.histplot(df_housing.select('price').toPandas(), bins = 10)","5b9a11e3":"#Average price of house\nimport matplotlib.pyplot as plt\ndf1 = df_housing.groupby('total_rooms').avg().sort('total_rooms').select(['total_rooms','avg(price)'])\ndf_p = df1.toPandas()\nplt.figure(figsize = (15, 8))\nsns.scatterplot(x = df_p['total_rooms'], y = df_p['avg(price)'] )","e9ee247c":"#Adding a column of per-capita income to the dataframe\n\ndf_housing = df_housing.withColumn('per_capita_income', df_housing['median_income']*10000\/df_housing['population'])","ef8c3c19":"#per_capita_income distribution \ng = sns.histplot(df_housing.select('per_capita_income').toPandas())\ng.set(xlim = (0, 500))\n","59d058f5":"#Per-capita-income and prices of the home\ndf_p = df_housing.toPandas()\nsns.scatterplot(x = df_p['per_capita_income'], y = df_p['price'])\n\n#A lot of data has near $100 per-capita income - data is skewed towards zero. ","f3597174":"#Counting per capita that are less than $100\ncount_blocks = df_housing.filter('per_capita_income <  100').count()\/df_housing.select('per_capita_income').count()*100\nprint(\"Percentage of blocks below $100 per capita: %2f\" % count_blocks)","9f06a108":"#Checking unique values in ocean_proximity\ndf_housing.select('ocean_proximity').distinct().show()","a9525b02":"#Where does wealthy people live?\ndf_i = df_housing.groupby('ocean_proximity').agg({'median_income' : 'avg'})\ndf_p = df_i.toPandas()\nsns.barplot(x = df_p['ocean_proximity'], y = df_p['avg(median_income)']*10000)\n\n#Houses that are less than 1 hour to ocean where most wealthy people wants to live","996fed80":"#Label-encoding for the \"ocean_proximity\" column\nfrom pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\") \ndf_housing = indexer.fit(df_housing).transform(df_housing)\ndf_housing = df_housing.drop('ocean_proximity')\ndf_housing.select('ocean_proximity_index').show(3)","881bd1c6":"#Removing na values to ensure correlation method works properly\nmean = df_housing.select(F.mean('total_bedrooms')).collect()[0][0]\ndf_housing = df_housing.na.fill({'total_bedrooms': mean})","83776ac1":"#Checking if na values exist in 'total_bedrooms' columns\ndf_housing.filter(col('total_bedrooms').isNull()).show()","fde68c4f":"print(\"THe min price of home\" , df_housing.agg({'price': 'min'}).collect()[0][0])\nprint(\"THe max price of home\" , df_housing.agg({'price': 'max'}).collect()[0][0])","caa3df61":"range_category =  np.linspace(100000, 550000, num = 11)","6aae13b3":"from pyspark.ml.feature import Bucketizer\nbucketizer = Bucketizer(splits= range_category, inputCol = 'price', outputCol = 'bucket_price')\nbucketed = bucketizer.setHandleInvalid(\"keep\").transform(df_housing)\nbucketed.select('bucket_price').show(5)\ndf_housing  = df_housing.withColumns('bucket_price', bucketed['bucket_price']*100000)\n","9dca2f37":"from pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\n\n# convert to vector column first\nassembler = VectorAssembler(inputCols=df_housing.columns, outputCol=\"features\")\ndf_vector = assembler.transform(df_housing).select(\"features\")\n\n# get correlation matrix\nmatrix = Correlation.corr(df_vector, 'features')\ncorrmatrix = matrix.collect()[0][0].toArray().tolist()\n\n#Converst to pandas dataframe\ndf_corr = pd.DataFrame(corrmatrix, columns = df_housing.columns, index = df_housing.columns)\n\n#plot correlation matrix by using seaborn\nsns.heatmap(df_corr)\n","9f82a694":"df_housing.columns","cc127d87":"#Drop non-correlated columns\ndf_model = df_housing.select(['housing_median_age','total_rooms', 'median_income','price'])\ndf_model.show(3)","65bf8e94":"#Checking normal distribution of selected fetures\n#housing_median_age\n\nsns.distplot(df_housing.select('housing_median_age').toPandas(), color=\"skyblue\")\ndf_housing.select(F.skewness('housing_median_age'), F.kurtosis('housing_median_age')).show()\n\n#the housing_median_age is normally distributed","147bf12e":"#Checking normal distribution of selected fetures\n#total_rooms\n\nsns.distplot(df_housing.select('total_rooms').toPandas(), color=\"skyblue\")\ndf_housing.select(F.skewness('total_rooms'), F.kurtosis('total_rooms')).show()\n\n#the total_rooms is not normally distributed","ef5862d9":"#Using lograthimic scale to normalize the data\n\ndf_model = df_model.withColumn(\"total_rooms_log\", F.log10(col(\"total_rooms\")))\n\nsns.distplot(df_model.select('total_rooms_log').toPandas(), color=\"skyblue\")\ndf_model.select(F.skewness('total_rooms_log'), F.kurtosis('total_rooms_log')).show()\n\n#The distribution is now lograthmic distributed","c27d85e5":"#Checking normal distribution of selected fetures\n#median_income\n\nsns.distplot(df_housing.select('median_income').toPandas(), color=\"skyblue\")\ndf_housing.select(F.skewness('median_income'), F.kurtosis('median_income')).show()\n\n#the median_income is normally distributed","2b08ca18":"#Assembling features\nfeature_assembly = VectorAssembler(inputCols = ['housing_median_age','total_rooms_log', 'median_income'], outputCol = 'features')\noutput = feature_assembly.transform(df_model)\noutput.show(3)","8a0f0059":"#Normalizing the features\nfrom pyspark.ml.feature import StandardScaler\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=True, withMean=False)\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(output)\n\n# Normalize each feature to have unit standard deviation.\nscaledOutput = scalerModel.transform(output)\nscaledOutput.show(3)","b7ca7dcb":"#Selecting input and output column from output\ndf_model_final = scaledOutput.select(['price', 'scaledFeatures'])\ndf_model_final.show(3)","b9dd327d":"from pyspark.ml.regression import LinearRegression \n\n#test train split\ndf_train, df_test = df_model_final.randomSplit([0.75, 0.25])\nregressor = LinearRegression(featuresCol = 'scaledFeatures', labelCol = 'price')\nregressor = regressor.fit(df_train)","76dca52b":"#MSE for the train data\n\npred_results = regressor.evaluate(df_train)\nprint(\"The MSE for the model is: %2f\"% pred_results.meanAbsoluteError)\nprint(\"The r2 for the model is: %2f\"% pred_results.r2)","b6ee298c":"#Checking train performance\npred_results = regressor.evaluate(df_test)\nprint(\"The MSE for the model is: %2f\"% pred_results.meanAbsoluteError)\nprint(\"The r2 for the model is: %2f\"% pred_results.r2)","a41ac554":"**Perliminary analysis**","99a9485a":"**Linear regression to predict prices**","01ecd49b":"**This notebook aims to analyze housing price by using PySpark libraries.**\n\n- Cleaning the data\n- Exploratory data analysis\n- Created new features\n- Corrleation analysis\n- Converted categorial data to numerical\n- Linear regression for house price prediction\n- Hyperparameter tuning"}}