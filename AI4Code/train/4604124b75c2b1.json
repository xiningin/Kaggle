{"cell_type":{"6bbd17e4":"code","735c75a5":"code","d0ddbf6e":"code","0ec85719":"code","d1338bf5":"code","beca2dbf":"code","ce3e66cd":"code","3bfad7cb":"code","af2951ad":"code","a1702357":"code","3a42d5f1":"code","7c121f52":"code","1114bfe4":"code","e5c8106c":"code","4e182d22":"code","d4a89568":"code","3354f2f5":"code","acaf425a":"code","44ce566e":"code","d6a63daf":"code","7ddb417e":"code","77708a29":"code","b4d38291":"code","e8fafe37":"code","597025c0":"code","f100ac52":"code","ab95317e":"code","5f2f9c46":"code","76f7f883":"code","fa230bc4":"code","043907b0":"code","f7f55f53":"code","44a581dc":"code","e9b9e4b5":"code","e35a3456":"code","54daabd1":"code","49f7726e":"code","c48bb1bb":"code","fbebebf4":"code","b5b99f02":"code","a4130ffd":"code","3c344d81":"code","74ad5cd7":"code","345c2977":"code","e7222188":"code","4e2a1b0b":"code","a5489159":"code","8be544a2":"code","acd18ba5":"code","eff2f817":"code","7f39473d":"code","86731978":"code","98305874":"code","719e86be":"code","39ac4d28":"code","08b3bd0b":"code","bd9ec9ac":"code","23f02cf6":"code","6e67f644":"code","1e981f85":"code","8ccf345b":"code","85fc0b44":"code","4d364fea":"code","63b19e0a":"code","af8c2a91":"code","a51a4a41":"code","36e84600":"code","bafb8a4e":"code","e596aba6":"code","be776626":"code","4ca61703":"code","8f9ee7ba":"code","bc165d65":"code","e2978aea":"code","4bbf9df1":"code","0a6604a4":"code","e763d04f":"code","1b90e2ad":"code","f21d6e85":"code","768a4a91":"code","84997fba":"code","a3eabe0a":"markdown","090a56fa":"markdown","2b41d0a0":"markdown","eb9ba6b7":"markdown","f9ebef16":"markdown","8db0921e":"markdown","377d5dc3":"markdown","ca59166e":"markdown","0cc9ca62":"markdown","6ed9238f":"markdown","5e22d767":"markdown","1c2346be":"markdown","2f399621":"markdown","db00fa65":"markdown","ad1ef32c":"markdown","09833b0e":"markdown","21ad2ea3":"markdown","d840ece3":"markdown","e22b9d74":"markdown","7be39553":"markdown","17ff2294":"markdown","fe9be756":"markdown","913c24ce":"markdown","fbc9b632":"markdown","61ac9b56":"markdown","b1cb3ce4":"markdown","d3f128ab":"markdown","1335464b":"markdown","67413791":"markdown","a274f231":"markdown","bc1b269a":"markdown","811bfc2d":"markdown","728a5e23":"markdown","50774c5b":"markdown","947bba8e":"markdown","e6d32e80":"markdown","24c4abc1":"markdown","9ab9b9c4":"markdown","12bd8fad":"markdown","f3d8b4cb":"markdown"},"source":{"6bbd17e4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","735c75a5":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","d0ddbf6e":"# Loading training data for exploration\ndata_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","0ec85719":"data_train.head()","d1338bf5":"data_train.info()","beca2dbf":"data_train.describe()","ce3e66cd":"# Splitting columns into numerical and categorical\ncategorical_cols = list(data_train.select_dtypes(exclude=['number','bool_']).columns)\nnumerical_cols = [c for c in data_train.columns if c not in categorical_cols]\nnumerical_cols.remove('PassengerId')\n\nprint('Categorical Variables: {}'.format(categorical_cols))\nprint('Numerical Variables: {}'.format(numerical_cols))","3bfad7cb":"# Plotting the single variable distributions of all numerical data\nfig, axs = plt.subplots(2, 3, figsize=(16, 9))\naxs = axs.flatten()\n\nfor col, ax in zip(numerical_cols, axs):\n    sns.histplot(data_train, x=col, ax=ax)","af2951ad":"# Looking at the correlations\nsns.heatmap(\n    data_train[numerical_cols].corr(),\n    annot = True,\n    fmt = '.2f',\n    cmap = 'coolwarm'\n)","a1702357":"data_train['Cabin'].head(10)","3a42d5f1":"data_train['Cabin'].dropna().head(20)","7c121f52":"# Cabin group (first letter of the cabin)\ndef get_cabin_group(cabin):\n    ''' Returns cabin group for a given cabin value'''\n    if isinstance(cabin, str):\n        cabin_split = cabin.split()\n        return cabin_split[0][0]\n    else:\n        return 'none'","1114bfe4":"# Apllying get_cabin_group to Cabin data\ndata_train['CabinGroup'] = data_train['Cabin'].map(get_cabin_group)","e5c8106c":"# Plotting  survival rates for each CabinGroup\nsns.barplot(x='CabinGroup', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')","4e182d22":"# Cabin Number\ndef get_cabin_number(cabin):\n    ''' Returns number of cabins for each Cabin value. \n        Note: Missing Cabin will be considered as CabinNumber 0. '''\n    if isinstance(cabin, str):\n        cabin_split = cabin.split()\n        return len(cabin_split)\n    else:\n        return 0","d4a89568":"# Applying get_cabin_number for Cabin data. \ndata_train['CabinNumber'] = data_train['Cabin'].map(get_cabin_number)","3354f2f5":"# Plotting  survival rates for each CabinNumber\nsns.barplot(x='CabinNumber', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')","acaf425a":"# Has Cabin\ndef get_has_cabin(cabin):\n    ''' Returns 1 if passenger has a Cabin and 0 if not. '''\n    return 1 if isinstance(cabin, str) else 0","44ce566e":"# Applygin get_has_cabin to Cabin data\ndata_train['HasCabin'] = data_train['Cabin'].map(get_has_cabin)","d6a63daf":"# Plotting  survival rates for each HasCabin\nsns.barplot(x='HasCabin', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')","7ddb417e":"data_train['Ticket'].head(20)","77708a29":"def get_is_number(ticket):\n    ''' return flag for ticket that are numbers. '''\n    try:\n        float(ticket)\n        return 1\n    except:\n        return 0","b4d38291":"# Applying get_is_number to Ticket data\ndata_train['IsTicketNumber'] = data_train['Ticket'].map(get_is_number)","e8fafe37":"# Plotting survival rates for each IsNumber\nsns.barplot(x='IsTicketNumber', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by IsTicketNumber')","597025c0":"data_train['Name'].head(10)","f100ac52":"# Extract title from Name data\ntitle_dict = {\n    'Mr.': 'Mr',\n    'Mrs.': 'Mrs',\n    'Ms.': 'Ms',\n    'Master.': 'Ms',\n    'Miss.': 'Miss',\n    'Dr.': 'Dr',\n    'Rev.': 'Rev'\n}\n\ndef get_title_from_name(name):\n    ''' Return NameTitle for each given Name. '''\n    for word in name.split():\n        if word[-1] != '.':\n            continue\n        return title_dict[word] if word in title_dict.keys() else 'Other'\n\n    print('Warning: NameTitle was not extracted properly. ')\n    return None","ab95317e":"# Applying get_title_from_name to Name data\ndata_train['NameTitle'] = data_train['Name'].map(get_title_from_name)","5f2f9c46":"data_train['NameTitle'].value_counts()","76f7f883":"# Plotting survival rates for each NameTitle\nsns.barplot(x='NameTitle', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by NameTitle')","fa230bc4":"data_train['Sex'].value_counts()","043907b0":"# Plotting survival rates for each Sex\nsns.barplot(x='Sex', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Sex')","f7f55f53":"data_train['Embarked'].value_counts()","44a581dc":"# Plotting survival rates for each Embarked\nsns.barplot(x='Embarked', y='Survived', data=data_train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Embarked')","e9b9e4b5":"def process_data(data):\n    '''\n    Parameters\n    ----------\n    data: DataFrame\n        Raw data (train or test)\n        \n    Returns\n    -------\n    DataFrame\n        Processed data.\n    '''\n    df_out = pd.DataFrame()\n\n    # Copying PassengerId\n    df_out['PassengerId'] = data['PassengerId']\n    \n    # Copying Survived (if existing)\n    if 'Survived' in data.columns:\n        df_out['Survived'] = data['Survived']\n    \n    ## Numerical data\n    # Pclass - one-hot encoding\n    df_Pclass = pd.get_dummies(\n        data['Pclass'],\n        drop_first=False\n    )\n    df_out['IsFirstClass'] = df_Pclass[1]\n    df_out['IsSecondClass'] = df_Pclass[2]\n\n    # Age\n    df_out['Age'] = data['Age']\n    df_out['Age'].fillna(df_out['Age'].mean(), inplace=True)\n    \n    # SibSp\n    df_out['SibSp'] = data['SibSp']\n    df_out['SibSp'].fillna(df_out['SibSp'].mean(), inplace=True)\n\n    # Parch\n    df_out['Parch'] = data['Parch']\n    df_out['Parch'].fillna(df_out['Parch'].mean(), inplace=True)\n    \n    # Fare\n    df_out['NormFare'] = np.log(data['Fare'] + 1)\n    df_out['NormFare'].fillna(df_out['NormFare'].mean(), inplace=True)\n    \n    ## Categorical data\n    # Name\n    df_NameTitle = data['Name'].map(get_title_from_name)\n    df_NameTitleDummies = pd.get_dummies(\n        df_NameTitle,\n        prefix='IsTitle',\n        prefix_sep='',\n        drop_first=True\n    )\n    for var in df_NameTitleDummies.columns:\n        df_out[var] = df_NameTitleDummies[var]\n    \n    # Cabin\n    df_out['HasCabin'] = data['Cabin'].map(get_has_cabin)\n\n    # Sex\n    df_SexDummies = pd.get_dummies(\n        data['Sex'],\n        prefix='Is',\n        prefix_sep='_',\n        drop_first=True\n    )\n    for var in df_SexDummies.columns:\n        df_out[var] = df_SexDummies[var]\n    \n    # Embarked\n    df_EmbarkedDummies = pd.get_dummies(\n        data['Embarked'],\n        prefix='Embarked',\n        prefix_sep='',\n        drop_first=True\n    )\n    for var in df_EmbarkedDummies.columns:\n        df_out[var] = df_EmbarkedDummies[var]\n    \n    return df_out","e35a3456":"# Loading raw data (train and test)\nraw_data_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nraw_data_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","54daabd1":"# Process train and test raw data\nprocessed_data_train = process_data(raw_data_train)\nprocessed_data_test = process_data(raw_data_test)","49f7726e":"processed_data_train.head()","c48bb1bb":"processed_data_test.head()","fbebebf4":"# Listing variables\n\nvariables = list(processed_data_test.columns)\nvariables.remove('PassengerId')\n\nprint('Number of trainable variables = {}'.format(len(variables)))\nfor i, var in enumerate(variables):\n    print('{}. {}'.format(i, var))","b5b99f02":"# Correlations\nfig = plt.figure(figsize=(20, 20))\nres = sns.heatmap(\n    processed_data_train.corr(),\n    annot = True,\n    fmt = '.2f',\n    cmap = 'coolwarm'\n)\nres.set_yticklabels(res.get_ymajorticklabels(), fontsize = 18)\nres.set_xticklabels(res.get_xmajorticklabels(), fontsize = 18)","a4130ffd":"X = processed_data_train.drop(columns=['PassengerId', 'Survived'])\ny = processed_data_train['Survived']","3c344d81":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    validation_curve,\n    learning_curve\n)\nfrom copy import copy","74ad5cd7":"X_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=3\n)","345c2977":"scaler = StandardScaler().fit(X_train)\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)","e7222188":"def perform_grid_search(X_in, y_in, classifier, **parameters):\n    '''\n    Parameters\n    ----------\n    X_in: DataFrame\n        X data\n    y_in: DataFrame\n        y data\n    classifier: sklearn class\n        Classsifier to be evaluated\n    **parameters\n        Parameters to be scanned by the grid search\n        \n    Returns\n    -------\n    (best estimator, best parameters)\n    '''\n    gs = GridSearchCV(\n        classifier(),\n        param_grid=parameters,\n        n_jobs=-1,\n        cv=5\n    )\n    gs.fit(X_in, y_in)\n    print('BestParameters = {}'.format(gs.best_params_))\n    print('BestScore = {:.4f}'.format(gs.best_score_)) \n    return copy(gs.best_estimator_), gs.best_params_","4e2a1b0b":"def get_test_cv_score_and_plot_learning_curve(X_in, y_in, classifier, ax=None):\n    ''' Raise and plot learning curve and return the test score with its error.\n    \n    Parameters\n    ----------\n    X_in: DataFrame\n        X data\n    y_in: DataFrame\n        y data\n    classifier: sklearn class\n        Classsifier to be evaluated\n    ax: plt.Axis\n        Optional\n\n    Returns\n    -------\n    (mean test score, std test score)\n    '''\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator=classifier,\n        X=X_in,\n        y=y_in,\n        train_sizes=np.linspace(0.1, 1, 10),\n        cv=5\n    )\n    train_score_mean = np.mean(train_scores, axis=1)\n    test_score_mean = np.mean(test_scores, axis=1)\n    train_score_std = np.std(train_scores, axis=1)\n    test_score_std = np.std(test_scores, axis=1)\n    \n    if ax is None:\n        ax = plt.gca()\n\n    COLORS = {'train': 'r', 'test': 'b'}\n    \n    ax.plot(train_sizes, train_score_mean, color=COLORS['train'], linestyle='none',\n            marker='o', label='train')\n    plt.fill_between(train_sizes, train_score_mean + train_score_std,\n                     train_score_mean - train_score_std, alpha=0.15,\n                     color=COLORS['train'])\n    ax.plot(train_sizes, test_score_mean, color=COLORS['test'], linestyle='none',\n            marker='s', label='test')\n    plt.fill_between(train_sizes, test_score_mean + test_score_std,\n                     test_score_mean - test_score_std, alpha=0.15,\n                     color=COLORS['test'])\n    ax.legend()\n    ax.set_xlabel('train size')\n    ax.set_ylabel('score')\n    \n    score_text = 'ValidationScore = {:.3f}+\/-{:.3f}'.format(test_score_mean[-1],\n                                                      test_score_std[-1])\n    ax.set_title(score_text)\n    return test_score_mean[-1], test_score_std[-1]","a5489159":"results = dict()","8be544a2":"from sklearn.linear_model import LogisticRegression","acd18ba5":"results['LR'] = dict()","eff2f817":"results['LR']['estimator'], results['LR']['pars'] = perform_grid_search(\n    X_train_std,\n    y_train,\n    LogisticRegression,\n    C=[0.01, 0.1, 1.0, 10, 100],\n    class_weight=['balanced', None],\n    penalty=['l2']\n)","7f39473d":"results['LR']['validation_score'] = get_test_cv_score_and_plot_learning_curve(\n    X_train_std,\n    y_train,\n    results['LR']['estimator']\n)","86731978":"results['LR']['test_score'] = results['LR']['estimator'].score(X_test_std, y_test)","98305874":"from sklearn.svm import SVC","719e86be":"results['SVM'] = dict()","39ac4d28":"results['SVM']['estimator'], results['SVM']['pars'] = perform_grid_search(\n    X_train_std,\n    y_train,\n    SVC,\n    C=[0.01, 0.1, 1, 10, 100],\n    #kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n    kernel=['rbf'],\n    class_weight=['balanced', None]\n)","08b3bd0b":"results['SVM']['validation_score'] = get_test_cv_score_and_plot_learning_curve(\n    X_train_std,\n    y_train,\n    results['SVM']['estimator']\n)","bd9ec9ac":"results['SVM']['test_score'] = results['SVM']['estimator'].score(X_test_std, y_test)","23f02cf6":"from sklearn.ensemble import RandomForestClassifier","6e67f644":"results['RF'] = dict()","1e981f85":"results['RF']['estimator'], results['RF']['pars'] = perform_grid_search(\n    X_train_std,\n    y_train,\n    RandomForestClassifier,\n    max_depth=[4, 5, 6]\n)","8ccf345b":"results['RF']['validation_score'] = get_test_cv_score_and_plot_learning_curve(\n    X_train_std,\n    y_train,\n    results['RF']['estimator']\n)","85fc0b44":"results['RF']['test_score'] = results['RF']['estimator'].score(X_test_std, y_test)","4d364fea":"# Plotting feature importances\nfeat_imp = pd.DataFrame({\n    'importance': results['RF']['estimator'].feature_importances_,\n    'feature': X_train.columns}\n)\norder_reversed = feat_imp.sort_values('importance').feature[::-1]\nsns.barplot(\n    x='importance',\n    y='feature',\n    data=feat_imp,\n    order=order_reversed\n)","63b19e0a":"from sklearn.ensemble import GradientBoostingClassifier","af8c2a91":"results['GB'] = dict()","a51a4a41":"results['GB']['estimator'], results['GB']['pars'] = perform_grid_search(\n    X_train_std,\n    y_train,\n    GradientBoostingClassifier,\n    max_depth=[3, 4, 5, 6]\n)","36e84600":"results['GB']['validation_score'] = get_test_cv_score_and_plot_learning_curve(\n    X_train_std,\n    y_train,\n    results['GB']['estimator']\n)","bafb8a4e":"results['GB']['test_score'] = results['GB']['estimator'].score(X_test_std, y_test)","e596aba6":"from sklearn.neighbors import KNeighborsClassifier","be776626":"results['KNN'] = dict()","4ca61703":"results['KNN']['estimator'], results['KNN']['pars'] = perform_grid_search(\n    X_train_std,\n    y_train,\n    KNeighborsClassifier,\n    n_neighbors=[3, 4, 5, 6, 7, 8],\n    weights=['uniform', 'distance']\n)","8f9ee7ba":"results['KNN']['validation_score'] = get_test_cv_score_and_plot_learning_curve(\n    X_train_std,\n    y_train,\n    results['KNN']['estimator']\n)","bc165d65":"results['KNN']['test_score'] = results['KNN']['estimator'].score(X_test_std, y_test)","e2978aea":"print('Model | Validation Score | Test Score') \nfor key, value in results.items():\n    print('{} | {:.3f}+\/-{:.3f} | {:.3f}'.format(\n        key, \n        value['validation_score'][0],\n        value['validation_score'][1],\n        value['test_score']\n    ))","4bbf9df1":"best_clf = results['LR']['estimator']","0a6604a4":"# Preparing data\nX_test = processed_data_test.drop(columns='PassengerId')\nX_test_std = scaler.transform(X_test)","e763d04f":"# Calculating predictions\ny_pred = best_clf.predict(X_test_std)","1b90e2ad":"# Preparing output\ndata_submission = pd.DataFrame()\ndata_submission['PassengerId'] = processed_data_test['PassengerId']\ndata_submission['Survived'] = y_pred","f21d6e85":"data_submission.head()","768a4a91":"data_submission.to_csv('submission.csv', index=False)","84997fba":"print(data_submission['Survived'].sum() \/ len(data_submission))","a3eabe0a":"***\n#### Conclusion\n- *Embarked* is potentially relevant for the classification.\n- Because there are only 3 values, I will keep it as it is and encode it in the next section.\n***","090a56fa":"<a id='data_processing'><\/a>\n## 2. Processing the data (cleaning and feature engeneering)","2b41d0a0":"#### 3.2.2 SVM","eb9ba6b7":"### 1.2.3. Name","f9ebef16":"***\n#### Notes\n- Multiple patterns (single numbers, letter patterns + numbers ...)\n- I will try to extract information by separating passengers with a ticket that is a number fromthe others (number + letter pattern). \n***","8db0921e":"***\n#### Conclusion\n- Again, *CabinNumber* **is not significantly correlated** with the surviving probability, the distributions is quite flat. It seems thought that 0 Cabin has a significantly lower survival rate than *CabinNumber* > 0. I will follow up on that by having a look at the correlation between passengers that have or not a *Cabin*.\n***","377d5dc3":"### 1.2 Exploring categorical data","ca59166e":"***\n#### Notes\n- There are 5 categorical and 5 numerical variables (+ the labels). These two groups of variables will be treated separetly in the next subsections.\n- For numerical variables, I will plot the distribution and check for outliers, need for normalization, and possible correlations with the label.\n- For categorical variables, I will check for possibility\/necessity of encoding, correlations with labels and relevance.\n***","0cc9ca62":"### 1.1 Exploring numerical data","6ed9238f":"#### 3.2.4 Gradient Boosting (GB)","5e22d767":"***\nAll the conclusions from the previous section will be incorporated into one function called process_data. The missing values for the numerical variables will be filled with the means. This is not very relevant since the missing value rates are very small for the numerical variables, apart from *Age*. For *Age*, it still seems reasonable to replace the missing valuesfor its mean.\n***","1c2346be":"***\n#### Conclusion\nThe validation scores look **very similar** for all models. Basically they all agree within uncertainties. There is no strong reason to pick any of these models since the perfomance is very similar and they are all relatively simple algorithms.\nI choose the **Logistic Regression** to apply at the test data. It is a easy model to explain and very simple in terms of hyper-parameters to tune. The learning curve also seemed well behaved, no sign of overtraining. \n***","2f399621":"# Titanic\nThis is my second attempt at the Titanic dataset. Some of the improvements were due to  ideas taken from [Ken Jee](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example) and [Jason Chong](https:\/\/www.youtube.com\/watch?v=GSk-EEu1zkA). My goals include becoming more familiar with seaborn and sklearn, which means that there are a number of unnecessary steps that were added for the sake of learning.\n\nCode can also be found [here](https:\/\/github.com\/RaulRPrado\/Titanic-Kaggle).\n\n***\n### The notebook is structured as:\n\n[1. Exploring the data](#data_exploration)\n\n[2. Processing the data (cleaning and feature engeneering)](#data_processing)\n\n[3. Building and evaluating models](#model_building)\n\n[4. Selecting the best model](#model_selection)\n\n[5. Applying best model to test data](#model_applying)","db00fa65":"<a id='model_applying'><\/a>\n## 5. Applying best model to test data","ad1ef32c":"#### 3.2.1 Logistic Regression (LR)","09833b0e":"<a id='model_building'><\/a>\n## 3. Building and evaluating models ","21ad2ea3":"***\nI will first compare the validation and the test score of all models.\n***","d840ece3":"### 3.2 Buiilding models\n***\nThe plan is to test a number of classification models and select one based on their performance.\n\n#### List of classifiers\n- Logistic Regression\n- SVM\n- Random Forest\n- Gradient Boosting\n- KNN\n\n#### Steps\n- For each classifier, a grid search will be first performed, scanning a non-extensive grid of parameters. For the sake of simplicity, I will not focus on performing the most complete grid search, but I will only choose a reasonable range of the most important parameters. The best estimator for it classifier and the best parameters will be store into the dict *results*. The grid search will be done with the function *perform_grid_search* defined below.\n- A learning curve will be raised in order to validate the model. \n- The validation score will be evaluated by using the calculations done to raise the learning curve. Both the learning curve and the test score steps will be done with the function *get_test_cv_score_and_plot_learning_curve*. The validation scores and its error will be stored into the dict *results*.\n***","e22b9d74":"### 2.1 Processing data\n\n***\n#### Relevant steps\n- *Pclass* is one-hot encoded into *IsFirstClass* and *IsSecondClass*.\n- *Fare* is log normalized and replaced by *NormFare*.\n- *Name* is turned into *NameTitle*, which is one-hot encoded into *IsTitleMr* etc.\n- *Cabin* is replaced by *HasCabin*.\n- *Sex* is one-hot encoded and replaced by *Is_male* only.\n- *Embarked* is one-hot encoded. We are left with *EmbarkedS* and *EmbarkedQ*.\n***","7be39553":"<a id='model_selection'><\/a>\n## 4. Selecting the best model","17ff2294":"#### 3.2.5 KNN","fe9be756":"***\n#### Notes\n- Unsurprsingly, *Is_male* is strongly correlated with *IsTitleMr*.\n***","913c24ce":"***\n#### Notes\n- Large fraction of missing data.\n- Follows the pattern \"single letter + number\".\n- There may be multiple cabins for the same passenger.\n***\n\n***\n#### Ideas\n- The first letter (cabin group) could carry relevant information.\n- The number of cabins could carry relevant information.\n***","fbc9b632":"<a id='data_exploration'><\/a>\n## 1. Exploring the data\n\nIn this section I will take a broad look at the data and collect ideas for the data processing, which will come later.","61ac9b56":"***\n#### Conclusion\n- *HasCabin* seems to **carry some information** about the survival probability. I will keep *HasCabin* as a final variable for the classification.\n***","b1cb3ce4":"#### 1.2.5. Embarked","d3f128ab":"***\n#### Notes\n- *Name* is of course a specific variable for each passenger. The only possible information we can extract is the **title**.\n- I found a list of may different titles, from which several appered only 1 or 2 times in the dataset. I then selected only the ones which appeared a number of times (Mr, Mrs, Ms, Miss, Dr, Rev) and grouped all the remanining ones into the code Other. \n***","1335464b":"***\n#### Conclusions\n- *IsTicketNumber* has no information about survival rates. I will **drop completely** the *Ticket* variable.\n***","67413791":"### 2.2 Listing variables","a274f231":"#### 1.2.4. Sex","bc1b269a":"#### 1.2.1 Cabin","811bfc2d":"### 2.3 Correlation Matrix","728a5e23":"***\n#### Notes\n- Negative correlation between *Pclass* and *Fare* because higher class (smaller *Pclass*) tickets should be more expensive.\n- Positive correlation between *Parch* and *SibSp* because both are related to the family size. Meaning that families travelling together will be higher on both variables.\n***","50774c5b":"#### 1.2.2 Ticket","947bba8e":"***\n#### Conclusion\n- *CabinGroup* **is not significantly correlated** with the surviving probability, the distributions is quite flat. The *CabinGroup* would need to be one-hot encoded later, which means 9 more variables. To avoid having to deal with too many variables I will not use *CabinGroup* for the classification. \n***","e6d32e80":"***\n#### Conclusion\n- *Sex* is a very relevant variable and will be kept as it it.\n***","24c4abc1":"***\n#### Conclusion\n- There are differences in survival rates between *NameTitle*. In particular, Mr shows a much lower survival rate than the rest and Rev shows 0 survival probability. I will keep this variable for the classification.\n***","9ab9b9c4":"### 3.1 Preparing data\n***\n#### Steps\n- Selecting columns and creating X and y arrays\n- Sppliting data into train and test\n- Rescaling data using StandardScaler\n***","12bd8fad":"#### 3.2.3 Random Forest (RF)","f3d8b4cb":"***\n#### Ideas\n- *Pclass* is a code for the class of the passenger (1 - first class, 2 - second class ...). Therefore, the value of Pclass does not carry the actual ordinal information and the relationship between the classes. The one to treat this kind of variable is by **(one-hot) encoding** them into extra variables which indicates which class the passenger was into. In this case, I will encode Pclass into the variables **IsFirstClass** and **IsSecondClass**. The third class variable is not necessary because the same information can be obtained by the previous ones ( \"IsThirdClass == 1\" is equivalent to \"IsFirstClass == 0 and IsSecondClass == 0\").\n- *Fare* is a very skewed distribution, therefore it may be beneficial to normalize it through a log function.\n- *Age* is quite well behaved, gaussian-like distribution - nothing to do.\n- *SibSp* and *Parch* are also well behaved, but discrete distributions. They are also skewed, therefore it could be beneficial to perform a log normalization on them. \n***"}}