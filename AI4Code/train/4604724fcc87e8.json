{"cell_type":{"407346e0":"code","c6a7d992":"code","48896954":"code","09b15789":"code","cec20fec":"code","737fe0ec":"code","bcf42070":"code","5fbbfd01":"markdown","731b0faf":"markdown","d58a8ab9":"markdown","0ec150e8":"markdown","ee8de42c":"markdown","5dca4fa9":"markdown","c9daaef8":"markdown"},"source":{"407346e0":"import numpy as np      # linear algebra\nimport pandas as pd     # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf # This is where the real magic happens! :-)\nfrom PIL import Image   # For image augmentation","c6a7d992":"def augment_images(data_source, batch_index, batch_size):\n    max_rotation_degrees = 10\n    max_crop_pixels      = 6\n\n    if data_source == 'train':\n        max_index = train_images.shape[0] - 1\n    \n        if batch_index + batch_size < max_index:\n            max_index = batch_index + batch_size\n\n        Y = train_labels.iloc[batch_index:max_index].to_numpy()\n        X = np.array(train_images[batch_index:max_index], copy=True)\n\n        for i in range(0, max_index - batch_index):\n            rotation = max_rotation_degrees - (np.random.rand(1) * max_rotation_degrees * 2)\n\n            x1 = int(np.random.rand(1) * max_crop_pixels)\n            x2 = 28 - int(np.random.rand(1) * max_crop_pixels)\n\n            y1 = int(np.random.rand(1) * max_crop_pixels)\n            y2 = 28 - int(np.random.rand(1) * max_crop_pixels)\n\n            img_data = X[i].reshape(28,28).astype(np.int8)\n            img = Image.fromarray(img_data)\n            img = img.rotate(rotation)\n            img = img.crop((x1, y1, x2, y2))\n            X[i] = np.asarray(img.resize((28,28), Image.ANTIALIAS)).reshape(28, 28, 1)\n\n    else:\n        max_index = val_images.shape[0] - 1\n    \n        if batch_index + batch_size < max_index:\n            max_index = batch_index + batch_size\n\n\n        image_batch = val_images[batch_index:max_index]\n\n        Y = val_labels.iloc[batch_index:max_index].to_numpy()\n        X = np.asarray(val_images[batch_index:max_index])\n        \n    return X, Y","48896954":"def batch_images_generator(data_source, batch_size, steps_per_epoch):\n    current_step = 0\n    while True:\n        yield augment_images(data_source, batch_size * current_step, steps_per_epoch)\n        if current_step < steps_per_epoch:\n            current_step += 1\n        else:\n            current_step = 0","09b15789":"def build_model():\n    dropout_level = 0.15\n    \n    model_input    = tf.keras.layers.Input(shape=(28,28,1))\n    \n    model_cnn1a    = tf.keras.layers.Conv2D(60, kernel_size=(5,5), activation='relu')(model_input)\n    model_batchnorm1a = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(model_cnn1a)\n    model_dropout1a = tf.keras.layers.Dropout(dropout_level)(model_batchnorm1a)\n    model_cnn2a     = tf.keras.layers.Conv2D(50, kernel_size=(13,13), activation='relu')(model_dropout1a)\n\n    model_cnn1b    = tf.keras.layers.Conv2D(60, kernel_size=(9,9), activation='relu')(model_input)\n    model_batchnorm1b = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(model_cnn1b)\n    model_dropout1b = tf.keras.layers.Dropout(dropout_level)(model_batchnorm1b)\n    model_cnn2b     = tf.keras.layers.Conv2D(50, kernel_size=(9,9), activation='relu')(model_dropout1b)\n\n    model_cnn1c    = tf.keras.layers.Conv2D(60, kernel_size=(13,13), activation='relu')(model_input)\n    model_batchnorm1c = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(model_cnn1c)\n    model_dropout1c = tf.keras.layers.Dropout(dropout_level)(model_batchnorm1c)\n    model_cnn2c     = tf.keras.layers.Conv2D(50, kernel_size=(5,5), activation='relu')(model_dropout1c)\n\n    model_concat    = tf.keras.layers.Concatenate()([model_cnn2a, model_cnn2b, model_cnn2c])\n    model_batchnorm2 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(model_concat)\n    model_dropout2 = tf.keras.layers.Dropout(dropout_level)(model_batchnorm2)\n    model_cnn3     = tf.keras.layers.Conv2D(50, kernel_size=(5,5), activation='relu')(model_dropout2)\n    model_dropout3 = tf.keras.layers.Dropout(dropout_level)(model_cnn3)\n    \n    model_flatten = tf.keras.layers.Flatten()(model_dropout3)\n    model_batchnorm3 = tf.keras.layers.BatchNormalization()(model_flatten)\n    model_dense1  = tf.keras.layers.Dense(40)(model_batchnorm3)\n    model_dropout2 = tf.keras.layers.Dropout(dropout_level)(model_dense1)\n    model_dense2  = tf.keras.layers.Dense(30)(model_dropout2)\n    model_dropout2 = tf.keras.layers.Dropout(dropout_level)(model_dense2)\n    model_out     = tf.keras.layers.Dense(10, activation='softmax')(model_dropout2)\n    \n    model = tf.keras.Model(inputs=model_input, outputs=model_out)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0005), loss='categorical_crossentropy', metrics='acc')\n    \n    return model","cec20fec":"train_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df  = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ntrain_test_split_count = 42000\ntrain_images = train_df.to_numpy()[:train_test_split_count,1:].reshape(train_test_split_count,28,28,1)\nval_images   = train_df.to_numpy()[train_test_split_count:,1:].reshape(len(train_df)-train_test_split_count,28,28,1)\n\ntest_images  = test_df.to_numpy().reshape(len(test_df),28,28,1)\n\ntrain_labels = pd.get_dummies(train_df['label']).iloc[:train_test_split_count]\nval_labels   = pd.get_dummies(train_df['label']).iloc[train_test_split_count:]","737fe0ec":"model = build_model()\nmodel.summary()\n\nbatch_size = 64\ntrain_steps_per_epoch = int(train_images.shape[0] \/ batch_size)\nval_steps_per_epoch = int(val_images.shape[0] \/ batch_size)\n\ntrain_generator = batch_images_generator('train', batch_size, train_steps_per_epoch)\nval_generator = batch_images_generator('val', batch_size, val_steps_per_epoch)","bcf42070":"model.fit(train_generator, epochs=75, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator, \\\n          validation_steps=val_steps_per_epoch)\nsubmission_df = pd.DataFrame()\nsubmission_df['ImageId'] = test_df.index + 1\nsubmission_df['Label'] = np.argmax(model.predict(test_images), axis=1)\nsubmission_df.to_csv('submission.csv', index=False)","5fbbfd01":"Load and split the data. train_test_split_count = 42000 for all train, no validation","731b0faf":"Build the model and the data generators","d58a8ab9":"Batch data generator used for image augmentation","0ec150e8":"Image augmentation called by the batch data generator","ee8de42c":"Function for building the CNN model. It has same input to three different paths of CNN with different filter sizes. The general idea is that they each pick up specific patterns, inspired by the inception network.","5dca4fa9":"Import the libraries needed","c9daaef8":"Fit the model and create the submission file"}}