{"cell_type":{"989d7d30":"code","8a3b7e18":"code","be641bf3":"code","bc04e2a0":"code","498a9dca":"code","ad65c75a":"code","f17fd18f":"code","80531418":"code","a7073528":"code","c7a794b8":"code","6170285d":"code","3425ce68":"code","72c202c0":"code","ce48e08b":"code","e8d765c5":"code","73bf293b":"code","82ea6b76":"code","ac49b84e":"code","4f550724":"code","ad59e55f":"code","256a0230":"code","c49ab028":"code","49ca9f4e":"markdown","fdfc20c2":"markdown","ca440a7b":"markdown","ac05cd55":"markdown","794b4f61":"markdown","1b6f99ce":"markdown","3bafa591":"markdown","6a3a51af":"markdown","ff6eab9e":"markdown","0f5f52be":"markdown","a6bb8f81":"markdown","c46d140c":"markdown","cfaa1a82":"markdown","b064a8c9":"markdown","e6f84390":"markdown","37fd17b1":"markdown","1d0d89c1":"markdown","c4c4c599":"markdown","da4f9ac1":"markdown","c45322dc":"markdown","e9b0b4cd":"markdown","aa5d7efe":"markdown","023bd20d":"markdown","9825b816":"markdown","4f6130d4":"markdown","e6b860d8":"markdown"},"source":{"989d7d30":"#importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8a3b7e18":"#reading the dataset\ndata = pd.read_csv('\/kaggle\/input\/usa-cers-dataset\/USA_cars_datasets.csv')","be641bf3":"#look at dataset\ndata.head(2)","bc04e2a0":"#Removing the unnamed: 0 column \ndata = data.drop(['Unnamed: 0'], axis = 1)","498a9dca":"data.head(2)","ad65c75a":"#Exploring the dataset information\ndata.info()","f17fd18f":"#checking dimension of dataset\ndata.shape","80531418":"#importing library for visualising dataset and plotting the  histogram for  price attributes\nimport seaborn as sns\ndata['price'].hist(grid = False)","a7073528":"# Checking the skewness of Price column of dataset\ndata['price'].skew()","c7a794b8":"#density plot\nsns.distplot(data['price'], hist = True)","6170285d":"# Checking the skewness of mileage column of dataset\ndata['mileage'].skew() ","3425ce68":"sns.distplot(data['mileage'], hist = True)","72c202c0":"#performing the log transformation using numpy\nlog_mileage = np.log(data['mileage'])\nlog_mileage","ce48e08b":"#checking the skewness after the log-transformation\nlog_mileage.skew()","e8d765c5":"#calculating the square root for data['mileage'] column\nsqrt_mileage = np.sqrt(data['mileage'])\nsqrt_mileage","73bf293b":"#calculation skewness after calculating the square root \nsqrt_mileage.skew()","82ea6b76":"#visualising by density plot\nsns.distplot(sqrt_mileage, hist = True)","ac49b84e":"#calculating the cube root for the column data['mileage'] column\n\ncube_root_mileage = np.cbrt(data['mileage'])\ncube_root_mileage","4f550724":"#calculation skewness after calculating the cube root \ncube_root_mileage.skew()","ad59e55f":"#visualising by density plot\nsns.distplot(cube_root_mileage, hist = True)","256a0230":"#calculating the reciprocal for the column data['mileage'] column\nrecipr_mileage = np.reciprocal(data['mileage'])\nrecipr_mileage","c49ab028":"recipr_mileage.skew()","49ca9f4e":"### 2. Root Transformation\n\n#### 2.1 Square root Transfomation","fdfc20c2":"#### 2.2 cube root Transformation","ca440a7b":"### What does skewness tells us?\n\nSkewness of a data indicates the **direction and relative magnitude of a distribution's deviation from the normal distribution.** Skewness considers the extremes of the dataset rather than concentrating only on the average. Investors need to look at the extremes while judging the return from market as they are less likely to depend on the average value to work out.\n\nMany **model assumes normal distribution** but in reality data points may not be perfectly symmetric. If the data are skewed, then this kind of **model will always underestimate the skewness risk**.The more the data is skewed the less accurate the model will be.","ac05cd55":"* The square root means __x__ to __x^(1\/2)__ = __sqrt(x)__, is a transformation with a moderate effect on distribution shape. it is __weaker than the logarithm and the cube root__. \n\n* It is also used for reducing right skewness, and also has the advantage that it can be __applied to zero values__. \n\n* Note that the square root of an area has the units of a length. It is commonly applied to counted data, especially if the values are mostly rather small.","794b4f61":"__Note:__ As we can see the skewed values lies between the 0.5 to 1 range. so, data is moderately skewed and right skewed(but it's fine to train the model with it). Let's explore another attribute.","1b6f99ce":"__Note:__ In logrithm transformation we got the __nan__ because of zero, and in the square root tranformation it has reduced the skewed values from __7.07 to 1.66.__ but now in cube root transformation the skewed values __reduced to 0.68.__ and it is very much near to zero compare to 1.66 and 7.07.","3bafa591":"# Handling With Highly Skewed Data Set","6a3a51af":"### 3. reciprocals Transformation\n\n* The reciprocal, x to 1\/x, with its sibling the negative reciprocal, __x to -1\/x__, is a very strong transformation with a drastic effect on distribution shape. \n\n* It can not be applied to zero values. Although it can be applied to __negative values__, it is not useful unless all values are positive.\n\nFor Example: we might want to multiply or divide the results of taking the reciprocal by some constant, such as 100 or 1000, to get numbers that are easy to manage, but that itself has no effect on skewness or linearity.","ff6eab9e":"### About Dataset\n\nThis dataset tells the details about the cars. The car Dataset contain the following attributes:\n\n* index: Unnamed: 0(index values)\n* price: The sale price of the vehicle in the ad\n* brand: The brand of car\n* model: model of the vehicle\n* year: The vehicle registration year\n* title_status: This feature included binary classification, which are clean title vehicles and salvage insurance\n* mileage: miles traveled by vehicle\n* color: Color of the vehicle\n* vinThe:  vehicle identification number is a collection of 17 characters (digits and capital letters)\n* lot: A lot number is an identification number assigned to a particular quantity or lot of material from a single manufacturer. For cars, a lot number is combined with a serial number to form the Vehicle Identification Number.\n* state: The location in which the car is being available for purchase\n* country: The location in which the car is being available for purchase\n* condition: Time","0f5f52be":"Here,\n* skew of raw data is positive and greater than 1, right tail of the data is skewed.\n* skew of raw data is negative and less than 1, left tail of the data is skewed.","a6bb8f81":"### Reasons for using transformations\n\nThere are many reasons for transformation. \n\n        1. Convenience\n        2. Reducing skewness\n        3. Equal spreads\n        4. Linear relationships\n        5. Additive relationships\n\n__1. Convenience:__ A transformed scale may be as natural as the original scale and more convenient for a specific purpose.\n    for example- percentage rather than the original data.\n    \n__2. Reducing skewness:__ A transformation may be used to reduce skewness.  A distribution that is symmetric or nearly so is often easier to handle and interpret than a skewed distribution. \n    \n        - To handle the right skewness, we use:\n                - logarithms (best for it)\n                - roots[square root and cube root] (good)\n                - reciprocals (weak)\n    \n        - To handle left skewness, we use:\n                - squares \n                - cubes\n                - higher powers.\n \n__3. Equal spreads:__ A transformation may be used to produce approximately equal spreads, despite marked variations in level, which again makes data easier to __handle and interpret__. Each data set or subset having about the same spread or variability is a condition called __homoscedasticity__ and it's opposite is called __heteroscedasticity__\n\n\n__4. Linear relationships:__ When looking at relationships between variables,it is often far easier to think about patterns that are approximately linear than about patterns that are highly curved. This is vitally important when using linear regression, which amounts to fitting such patterns to data.\n\n__5. Additive relationships__ Relationships are often easier to analyse when additive rather than (say) multiplicative. So\n\n        y = a + bx\n\nIn which two terms __a__ and __bx__ are added is easier to deal with, than\n\n\n        y = ax^b\n\nIn which two terms __a__ and __x^b__ are multiplied. Additivity is a vital issue in analysis of variance (in Stata, anova, oneway, etc.).\n\n","c46d140c":"### Why log? \n\n* The normal distribution is widely used in basic research studies to model continuous outcomes. Unfortunately, the symmetric bell-shaped distribution often does not adequately describe the observed data from research projects. Quite often data arising in real studies are so skewed that standard statistical analyses of these data yield invalid results. \n\n* Many methods have been developed to test the normality assumption of observed data. When the distribution of the continuous data is non-normal, transformations of data are applied to make the data as __\"normal\"__ as possible and, thus, __increase the validity of the associated statistical analyses.__\n\n* Popular use of the log transformation is to __reduce the variability of data__, especially in data sets that include outlying observations. Again, contrary to this popular belief, log transformation can often __increase \u2013 not reduce \u2013 the variability of data whether or not there are outliers.__\n\n### Why not?\n\n* Using transformations in general and log transformation in particular can be quite problematic. If such an approach is used, the researcher must be mindful about its limitations, particularly when interpreting the relevance of the analysis of transformed data for the hypothesis of interest about the original data.\n\n* As from the below graph we can see that at zero it get deviated & hence log transformation doesn't work for negative zero values.","cfaa1a82":"__Note:__ In previous case we got the __nan__ because of zero, but the square root tranformation has reduced the skewed values __from 7.07 to 1.66.__ which is very much nearer to zero compare to 7.07.","b064a8c9":"![log10.png](attachment:log10.png)","e6f84390":"###  1. log Transformation\n\nThe log transformation is widely used in research to deal with skewed data. It is the best method to handle the right skewed data.","37fd17b1":"__Note:__ It's giving output as  __nan__ because there are some values as the zero. In reciprocal transformation, __it's good deal with negative numbers not with zero.__","1d0d89c1":"### Skewness\nSkewness of a distribution is defined as the lack of symmetry. In a symmetrical distribution, the Mean, Meadian and Mode are equal to each other.The normal distribution has a skewness of 0.\nSkewness tell us about where most of the values are concentrated on an ascending scale.\n\n\n__Now, the question is when we can say our data is mderately skewed or heavily skewed?__\n  \n  The thumb rule is:\n  * If the skewness is between -0.5 to +0.5 then we can say data is fairly symmetrical.\n  * If the skewness is between -1 to -0.5 or 0.5 to 1 then data is moderately skewed.\n  * And if the skewness is less than -1 and greater than +1 then our data is heavily skewed.","c4c4c599":"### To Handle Right Skeweness","da4f9ac1":"# Normal Distributed Data Set\n\n![1.png](attachment:1.png)\n\n\n# Positve & Negative Skewed\n\n\n![2.png](attachment:2.png)\n\n![3.png](attachment:3.png)\n\n# Get more clearity\n\n![4.jpg](attachment:4.jpg)\n\n# Let's make it more simply\n\n![5.png](attachment:5.png)","c45322dc":"__Note:__ As we can see the skewed values lies between -1 and greater than +1 then our data is heavily skewed. so, data is heavily skewed. data['mileage'] is right skewed by looking at the graph and skewed values.","e9b0b4cd":"# We can analyze different technique to tackle with highly positive skewed data points & soon we will see with negatively skewed data point in next notebook.\n\n# If you found it useful do upvote!!","aa5d7efe":"## Steps to do transformation\n\n__1.__  Draw a graph(histogram and density plot) of the data to see how far patterns in data match the simplest ideal patterns.\n\n    \n__2.__ check the range the data. because Transformations will have little effect if the range is small.\n\n__3.__ check the skewness by statistical methods(decide right and left skweness).\n\n__4.__ apply the methods (explained in detail below) to handle the skewness based on the skewed values.","023bd20d":"It's giving us __nan__ because there are some values as the zero. In log transformation, it deals with only the positive and negative numbers not with zero. The log is range in between (- infinity to infinity) but greater or less than zero. For better understanding you can check the log graph below:\n\n\n\n__Note:__ In the graph, the line at zero is deviated towards the positive infinity. so, If you are getting zeros inside the data, refer root Transformation.","9825b816":"## How to handle these skewed data?\n\n### Transformation\nIn data analysis transformation is the replacement of a variable by a function of that variable: for example, replacing a variable x by the square root of x or the logarithm of x. In a stronger sense, a transformation is a replacement that changes the shape of a distribution or relationship.","4f6130d4":"### Types of Skewness\n\n* __Positive skewness:__ In simple words, if the skewness is greater than 0 then the distribution is positively skewed. The tail on the right side of the distribution will be longer or flatter. If the data is positively skewed than most of values will be concentrated below the average value of the data.\n\n\n* __Negative skewness:__ If the skewness is less than 0 then the distribution is negatively skewed. For negatively skewed data, most of the values will be concentrated above the average value and tail on the left side of the distribution will be longer of flatter.","e6b860d8":"* The cube root means __x to x^(1\/3)__. This is a fairly strong transformation with a substantial effect on distribution shape, \n\n* It is __weaker than the logarithm but stronger than the square root__ transformation.\n\n* It is also used for reducing right skewness, and has the advantage that it can be __applied to zero and negative values__.  Note that the cube root of a volume has the units of a length. It is commonly applied to rainfall data."}}