{"cell_type":{"f9bd5ad0":"code","1ea4927b":"code","1ce8f29d":"code","3fa7c821":"code","2f9c5595":"code","675daab1":"code","46f4776d":"code","8c2ed029":"code","5bf4c8df":"code","eebac5d3":"code","4c1bbe06":"code","6ba0be10":"code","3d9da95e":"code","a262e3ff":"code","adde6041":"code","171d7b9c":"code","ecd33e80":"code","b6fd8279":"code","b6f063c7":"code","2fea8628":"code","b0cd75f1":"code","be8e52d8":"code","21d158de":"code","ceb91c8c":"markdown","2e3cac9e":"markdown","f4c62cce":"markdown","a753e980":"markdown","00984598":"markdown","27e65506":"markdown"},"source":{"f9bd5ad0":"import numpy as np\nimport pandas as pd\n\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import Conv1D, Input, Dense, Add, Multiply\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import models\nimport tensorflow_addons as tfa\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","1ea4927b":"tf.__version__","1ce8f29d":"train_ori = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest_ori = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","3fa7c821":"train_ori","2f9c5595":"print(f'Length of TRAIN dataset: {len(train_ori)}')\nprint(f'Length of TEST dataset: {len(test_ori)}')\nprint('')\nprint('Missing values in TRAIN dataset')\nfor i in train_ori.iloc[:, 0:-1].columns.tolist():\n    print(f'{i}: {train_ori[i].isna().sum()}')\nprint('')\nprint('Missing values in TEST dataset')\nfor i in test_ori.iloc[:, 0:-1].columns.tolist():\n    print(f'{i}: {test_ori[i].isna().sum()}')\nprint('')\nprint(f'Number of breaths in train dataset: {train_ori[\"breath_id\"].nunique()}')\nprint(f'Number of breaths in test dataset: {test_ori[\"breath_id\"].nunique()}')\nprint(f'The number of observations for each breath: {train_ori[\"breath_id\"].value_counts().reset_index()[\"breath_id\"].unique()[0]}')","675daab1":"train_initial = train_ori[train_ori['time_step']==0]\ntrain_initial","46f4776d":"total_mean = train_initial['pressure'].mean()\ntotal_std = train_initial['pressure'].std()\nprint('total mean: {}'.format(total_mean))\nprint('total std: {}'.format(total_std))","8c2ed029":"initial_stat_mean = train_initial.groupby(['R', 'C']).mean()\ninitial_stat_std = train_initial.groupby(['R', 'C']).std()","5bf4c8df":"initial_stat = pd.DataFrame({'R': [5, 5, 5, 20, 20, 20, 50, 50, 50],\n                            'C': [10, 20, 50, 10, 20, 50, 10, 20, 50],\n                            'mean': initial_stat_mean['pressure'].values,\n                            'std': initial_stat_std['pressure'].values})\ninitial_stat","eebac5d3":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    #df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    #df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    #df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    #df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    #df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    #df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    #df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    #df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train_ori)\ntest = add_features(test_ori)","4c1bbe06":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","6ba0be10":"train.columns","3d9da95e":"RS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)","a262e3ff":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, test.shape[-1])","adde6041":"train.shape","171d7b9c":"shape_ = train.shape[1:]","ecd33e80":"targets = targets[:,:,np.newaxis]","b6fd8279":"EPOCH = 300\nBATCH_SIZE = 128\nLR = 1e-3","b6f063c7":"def conv_block(x: tf.Tensor, filters: int, kernel_size: int):\n    \"\"\"\n    Implements convolution block with residual connection.\n    :param x: Input tensor.\n    :param filters: Number of filters in convolution layer.\n    :param kernel_size: Filter size.\n    :return: Output tensor.\n    \"\"\"\n    x = Conv1D(filters=filters,\n               kernel_size=1,\n               padding='same')(x)\n    res_x = x\n    x = Conv1D(filters=filters,\n               kernel_size=kernel_size,\n               padding='same', activation='relu')(x)\n    x = Conv1D(filters=filters,\n               kernel_size=kernel_size,\n               padding='same', activation='relu')(x)\n    x = Conv1D(filters=filters,\n               kernel_size=kernel_size,\n               padding='same', activation='relu')(x)\n    res_x = Add()([res_x, x])\n    return res_x\n\ndef wave_block(x: tf.Tensor, filters: int, kernel_size: int, n: int):\n    \"\"\"\n    Implements wavenet block.\n    :param x: Input tensor.\n    :param filters: Number of kernels.\n    :param kernel_size: Filter size.\n    :param n: Number of dilation rates for convolutions.\n    :return: Output tensor.\n    \"\"\"\n    dilation_rates = [2 ** i for i in range(n)]\n    x = Conv1D(filters=filters,\n               kernel_size=1,\n               padding='same')(x)\n    res_x = x\n    for dilation_rate in dilation_rates:\n        tanh_out = Conv1D(filters=filters,\n                          kernel_size=kernel_size,\n                          padding='same',\n                          activation='tanh',\n                          dilation_rate=dilation_rate)(x)\n        sigm_out = Conv1D(filters=filters,\n                          kernel_size=kernel_size,\n                          padding='same',\n                          activation='sigmoid',\n                          dilation_rate=dilation_rate)(x)\n        x = Multiply()([tanh_out, sigm_out])\n        x = Conv1D(filters=filters,\n                   kernel_size=1,\n                   padding='same')(x)\n        res_x = Add()([res_x, x])\n    return res_x","2fea8628":"def get_model():\n    inp = Input(shape_)\n    x = conv_block(inp, 16, 3)\n    x = wave_block(x, 16, 3, 6)\n    x = conv_block(x, 32, 3)\n    x = wave_block(x, 32, 3, 4)\n    x = conv_block(x, 64, 3)\n    x = wave_block(x, 64, 3, 2)\n    x = conv_block(x, 128, 3)\n    x = wave_block(x, 128, 3, 1)\n    x = keras.layers.Activation('swish')(x)\n    out = Dense(1, name='out')(x)\n    model = models.Model(inputs=inp, outputs=out)\n    opt = Adam(lr=LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss='mae', optimizer=opt)\n    return model","b0cd75f1":"#submission_tmp = submission.copy()","be8e52d8":"# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n#with tpu_strategy.scope():\nkf = KFold(n_splits=5, shuffle=True, random_state=2021)\ntest_preds = []\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n    if (fold != 0) and (fold != 1):\n        continue\n    submission_tmp = submission.copy()\n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n    X_train, X_valid = train[train_idx], train[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n    X_valid = tf.convert_to_tensor(X_valid, dtype=tf.float32)\n    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n    y_valid = tf.convert_to_tensor(y_valid, dtype=tf.float32)\n    model = get_model()\n\n    scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/BATCH_SIZE), 1e-5)\n    lr = LearningRateScheduler(scheduler, verbose=1)\n\n    #es = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1, mode=\"min\", restore_best_weights=True)\n    sv = tf.keras.callbacks.ModelCheckpoint(\n    'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n    save_weights_only=True, mode='min', save_freq='epoch')\n\n    history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, sv])\n    model.load_weights('fold-%i.h5'%fold)\n    pred = model.predict(test).squeeze().reshape(-1, 1).squeeze()\n    submission_tmp['pressure'] = pred\n    submission_tmp.to_csv('submission_{}.csv'.format(fold), index=False)\n    test_preds.append(pred)\n    \n    plt.figure(figsize=(15, 5))\n    plt.plot(\n        np.arange(len(history.history[\"loss\"])),\n        history.history[\"loss\"],\n        \"-o\",\n        label=\"Train Loss\",\n        color=\"#2ca02c\")\n    plt.plot(\n        np.arange(len(history.history[\"loss\"])),\n        history.history[\"val_loss\"],\n        \"-o\",\n        label=\"Val Loss\",\n        color=\"#d62728\")\n    \n    x = np.argmin(history.history[\"val_loss\"])\n    y = np.min(history.history[\"val_loss\"])\n    x1 = np.argmin(history.history[\"loss\"])\n    y1 = np.min(history.history[\"loss\"])\n    \n    xdist = plt.xlim()[1] - plt.xlim()[0]\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n\n    plt.scatter(x, y, s=200, color=\"#d62728\")\n    plt.text(x - 0.03 * xdist, y + 0.05 * ydist, \"min val_loss:{:.4f}\".format(y), size=14)\n    plt.scatter(x1, y1, s=200, color=\"#2ca02c\")\n    plt.text(x1 - 0.03 * xdist, y1 + 0.05 * ydist, \"min loss:{:.4f}\".format(y1), size=14)\n\n    plt.xlabel(\"Epoch\", size=14)\n    plt.ylabel(\"Loss\", size=14)\n\n    plt.legend()\n    plt.savefig(f\"fig{fold}.png\")\n    plt.show()","21d158de":"submission[\"pressure\"] = sum(test_preds)\/2\nsubmission.to_csv('submission.csv', index=False)","ceb91c8c":"# Training","2e3cac9e":"# Imports","f4c62cce":"# Add Features","a753e980":"# Define Model\n- https:\/\/github.com\/stdereka\/liverpool-ion-switching\/blob\/f44c57e88a3e7889720c88710135f2c7d31b416e\/model\/nn.py#L116","00984598":"# Load Data and some statistics","27e65506":"# Configuration"}}