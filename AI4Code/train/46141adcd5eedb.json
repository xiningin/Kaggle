{"cell_type":{"436b1e02":"code","0cad7263":"code","ac1b748e":"code","22d979fc":"code","537e0408":"code","3597e7c8":"code","994d8d8a":"code","f8ff047f":"code","2fe07b96":"code","e02b9411":"code","4710dedf":"code","3b101cb1":"code","90ccd11f":"code","bac1964b":"code","02fcf96d":"code","4f769f1e":"code","2c0c847f":"code","40767128":"code","3c0824b1":"code","c33a250d":"code","87ad6b15":"code","74ce8578":"code","aec7edd6":"code","02ab642c":"code","1d3eeb74":"code","d8c5d213":"code","07e3dba0":"code","43c6c516":"code","fb09421f":"code","406944c1":"code","e5424be5":"code","99780ece":"code","005bcfba":"code","6d8fd372":"code","9defb465":"code","78174e60":"code","eaae1951":"code","88008499":"code","71b745a2":"code","700a6ca2":"code","792c00b3":"code","d9a7b620":"code","82752f35":"code","4724952c":"code","5a90ed77":"code","ce7a2c50":"code","ad729755":"code","86f8016d":"code","1ed98942":"code","0a5aa7a0":"code","f3549b18":"code","af6802f0":"code","2d966f83":"code","f1c0281e":"code","c4a4f144":"code","b9dbdfa9":"markdown","e946af98":"markdown","5445fb5f":"markdown","3a7276fc":"markdown","e7ba71e3":"markdown","d42fd526":"markdown","36c8ec75":"markdown","f3e325a1":"markdown","d209373b":"markdown","46d52d30":"markdown","01199bcd":"markdown","4f35a7a4":"markdown","857e4696":"markdown","867d2fd6":"markdown","d40c52b6":"markdown"},"source":{"436b1e02":"import os\nimport tweepy\nimport pandas as pd\nfrom datetime import datetime\nimport pickle","0cad7263":"consumer_key = ''\nconsumer_secret = ''\naccess_key = ''\naccess_secret = ''","ac1b748e":"#authorize twitter, initialize tweepy\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_key, access_secret)\napi = tweepy.API(auth)","22d979fc":"def get_all_tweets(screen_name, api):\n    '''\n    Getting all tweets beyond the limit of tweepy    \n    Args-\n        screen_name: Screen name (not handles) of the twitter user\n        api: The tweepy api\n    Return-\n        alltweets: List of all the tweets w.r.t the user \n    '''\n    #Twitter only allows access to a users most recent 3240 tweets with this method    \n    #initialize a list to hold all the tweepy Tweets\n    alltweets = []  \n    \n    #make initial request for most recent tweets (200 is the maximum allowed count)\n    new_tweets = api.user_timeline(screen_name = screen_name,count=200, tweet_mode='extended',include_rts=True)\n    \n    #save most recent tweets\n    alltweets.extend(new_tweets)\n    \n    #save the id of the oldest tweet less one\n    oldest = alltweets[-1].id - 1\n    \n    #keep grabbing tweets until there are no tweets left to grab\n    while len(new_tweets) > 0:\n        print(f\"getting tweets before {oldest} tweet\")\n        \n        #all subsiquent requests use the max_id param to prevent duplicates\n        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest,tweet_mode='extended')\n        \n        #save most recent tweets\n        alltweets.extend(new_tweets)\n        \n        #update the id of the oldest tweet less one\n        oldest = alltweets[-1].id - 1\n        \n        print(f\"...{len(alltweets)} tweets downloaded so far\")\n    \n    return alltweets","537e0408":"# Screen names and saving the tweets. \nscreen_name_1 = 'bhutanisanyam1' \nscreen_name_2 = 'ctdsshow'\n\ntweet_pkl_fldr = \"data\/tweets\"\nif not os.path.exists(tweet_pkl_fldr):\n    os.makedirs(tweet_pkl_fldr)\n\ntweets_pkl = os.listdir(tweet_pkl_fldr)\nkaggle_data_dir = \"data\/ctds_bundle_archive\"\nexternal_data_dir = \"data\/\"","3597e7c8":"screen_names = [screen_name_1, screen_name_2]","994d8d8a":"# Added a check so as to not run the tweet getter again! \nfor sn in screen_names:\n    if sn +'.pkl' not in tweets_pkl:\n        print (\"Fetching tweet from \", sn)\n        all_tweets = get_all_tweets(sn, api)\n        print ('')\n    \n        print (\"Saving twitter data as \"+ sn + \".pkl\" +\" pickle file\")\n        with open(os.path.join(tweet_pkl_fldr, sn +'.pkl'),\"wb\") as op:\n            pickle.dump(all_tweets, op)\n            \n    print (\"Data available..\")\n    \nprint (\"Done!\")\n    \ntweets_pkl = os.listdir(tweet_pkl_fldr)    ","f8ff047f":"sanyam_tweets = []\n\nwith open(os.path.join(tweet_pkl_fldr, tweets_pkl[0]),\"rb\") as op:\n    sanyam_tweets = pickle.load(op)\n    print(\"Number of tweets by Sanyam - \", len(sanyam_tweets))\n    \nctds_tweets = []\n\nwith open(os.path.join(tweet_pkl_fldr, tweets_pkl[1]),\"rb\") as op:\n    ctds_tweets = pickle.load(op)\n    print(\"Number of tweets by CTDS - \", len(ctds_tweets))","2fe07b96":"endDate = datetime(2020, 6, 20, 0, 0, 0) #Stopping analysis at June 20th, 2020\nstartDate = datetime(2019, 7, 20, 0, 0, 0) #Starting analysis at July 20th, 2020\n\ntweets_sanyam = []\nfor tweet in sanyam_tweets:\n    if startDate < tweet.created_at < endDate:\n        tweets_sanyam.append(tweet)\n        \nprint (\"Tweets that can be considered from Sanyam -\", len(tweets_sanyam))\n\ntweets_ctds = []\nfor tweet in ctds_tweets:\n    if startDate < tweet.created_at < endDate:\n        tweets_ctds.append(tweet)\n        \nprint (\"Tweets that can be considered from CTDS -\", len(tweets_ctds))","e02b9411":"sanyam_alltweet_list = [[tweet.id_str, tweet.created_at, tweet.full_text, tweet.entities['user_mentions'],\n                    tweet.retweet_count, tweet.favorite_count] for tweet in tweets_sanyam]\n\nctds_alltweet_list = [[tweet.id_str, tweet.created_at, tweet.full_text, tweet.entities['user_mentions'],\n                    tweet.retweet_count, tweet.favorite_count] for tweet in tweets_ctds]\n\nsanyam_df = pd.DataFrame(sanyam_alltweet_list)\nctds_df = pd.DataFrame(ctds_alltweet_list)\n\ncolumns = [\"tweet_id\", \"created_date\", \"full_text\", \"user_mentions\", \"retweet_count\", \"fav_count\"]\n\nsanyam_df.columns = ctds_df.columns = columns\nctds_df.to_csv(os.path.join(external_data_dir, \"ctds_all_tweet.csv\"), index=False)\nsanyam_df.to_csv(os.path.join(external_data_dir, \"sanyam_all_tweet.csv\"), index=False)","4710dedf":"converse = [\"talk\", \"interview\",\"episode\"]\n\nctds_sanyam = [tweet for tweet in tweets_sanyam if any(i in tweet.full_text.lower() for i in converse)]\nctds_ctds = [tweet for tweet in tweets_ctds if any(i in tweet.full_text.lower() for i in converse)]","3b101cb1":"# sanyam_df['created_date'] = pd.to_datetime(sanyam_df['created_date']).dt.date\n# ctds_tweet_df['created_date'] = pd.to_datetime(ctds_tweet_df['created_date'])","90ccd11f":"print(\"Tweets on CTDS from Sanyam - \", len(ctds_sanyam))\nprint(\"Tweets on CTDS from CTDS - \", len(ctds_ctds))","bac1964b":"import re\n\nno_ama_sanyam = [tweet for tweet in ctds_sanyam if not re.search(r\"\\bama\\b\", tweet.full_text.lower())]\nno_ama_ctds = [tweet for tweet in ctds_ctds if not re.search(r\"\\bama\\b\", tweet.full_text.lower())]\nprint(\"Tweets inviting AMA from Sanyam - \", len(ctds_sanyam)-len(no_ama_sanyam))\nprint(\"Tweets inviting AMA from CTDS - \", len(ctds_ctds)-len(no_ama_ctds))","02fcf96d":"def ctds_mention(user_mentions):\n    '''\n    To reduce duplicates, removing the tweets from Sanyam that might have CTDS mentions\n    '''\n    ctds = [True if i['screen_name'] == 'ctdsshow' else False for i in user_mentions]\n    return any(ctds)","4f769f1e":"no_ctds_sanyam = [i for i in no_ama_sanyam if not ctds_mention(i.entities['user_mentions'])]\nprint (\"Sanyam's tweets on CTDS show with not @ctdsshow mention - \", len(no_ctds_sanyam))","2c0c847f":"# This might be a mistake!\nsanyam_tweet_list = [[tweet.id_str, tweet.created_at, tweet.full_text, tweet.entities['user_mentions'],\n                    tweet.retweet_count, tweet.favorite_count] for tweet in no_ama_sanyam]\n\nctds_tweet_list = [[tweet.id_str, tweet.created_at, tweet.full_text, tweet.entities['user_mentions'],\n                    tweet.retweet_count, tweet.favorite_count] for tweet in no_ama_ctds]","40767128":"# Moving to a df, master df for now\nsanyam_tweet_df = pd.DataFrame(sanyam_tweet_list)\nctds_tweet_df = pd.DataFrame(ctds_tweet_list)\n\ncolumns = [\"tweet_id\", \"created_date\", \"full_text\", \"user_mentions\", \"retweet_count\", \"fav_count\"]\n\nsanyam_tweet_df.columns = ctds_tweet_df.columns = columns","3c0824b1":"sanyam_tweet_df.shape, ctds_tweet_df.shape","c33a250d":"ctds_tweet_df['user_mentions'] = ctds_tweet_df['user_mentions'].apply(lambda x: ','.join([i['screen_name'] for i in x])) \nsanyam_tweet_df['user_mentions'] = sanyam_tweet_df['user_mentions'].apply(lambda x: ','.join([i['screen_name'] for i in x])) ","87ad6b15":"sanyam_tweet_df.shape, ctds_tweet_df.shape","74ce8578":"ctds_tweet_df.to_csv(os.path.join(external_data_dir, \"ctds_tweet.csv\"), index=False)\nsanyam_tweet_df.to_csv(os.path.join(external_data_dir, \"sanyam_tweet.csv\"), index=False)","aec7edd6":"import os\nimport tweepy\nimport pandas as pd\nfrom datetime import datetime\nimport pickle","02ab642c":"# Reading all the data that is presently available\nexternal_data_dir = \"data\"\nkaggle_data_dir = \"data\/ctds_bundle_archive\"\n\nepisode_df = pd.read_csv(os.path.join(kaggle_data_dir, 'Episodes.csv'))\ndescription_df = pd.read_csv(os.path.join(kaggle_data_dir, 'Description.csv'))\n\nctds_tweet_df = pd.read_csv(os.path.join(external_data_dir, \"ctds_tweet.csv\"))\nsanyam_tweet_df = pd.read_csv(os.path.join(external_data_dir, \"sanyam_tweet.csv\"))","1d3eeb74":"# df = sanyam_tweet_df[~sanyam_tweet_df[\"full_text\"].str.contains(\"|\".join(converse))]","d8c5d213":"no_tweet = episode_df[episode_df.heroes_twitter_handle.isnull()]\nno_handle_dates = no_tweet['release_date'].values\n\nprint (\"Number of episodes where there were no twitter handles mentioned - \", no_tweet.shape[0])","07e3dba0":"# Formatting dataes for filtering\nepisode_df['release_date'] = pd.to_datetime(episode_df['release_date'])\nsanyam_tweet_df['created_date'] = pd.to_datetime(sanyam_tweet_df['created_date'])\nctds_tweet_df['created_date'] = pd.to_datetime(ctds_tweet_df['created_date'])","43c6c516":"episode_df['release_date_only'] = episode_df['release_date'].dt.date","fb09421f":"# Creating an event date, something like a launch date\nsanyam_tweet_df['event_date'] = sanyam_tweet_df['created_date'].dt.date\nctds_tweet_df['event_date'] = ctds_tweet_df['created_date'].dt.date","406944c1":"# Tweepy is sensitive\ncorrected_handels = {\n                     \"jfpuget\":\"JFPuget\", \"guggersylvain\":\"GuggerSylvain\",\\\n                     \"sanhestpasmoi\":\"SanhEstPasMoi\",\"arnocandel\":\"ArnoCandel\",\\\n                     \"giba1\":\"Giba1\",\"stacknet_\":\"StackNet_\",\"johnmillertx\":\"JohnMillerTX\",\\\n                     \"walterreade\":\"WalterReade\",\"scitator\":\"Scitator\",\"mark_a_landry\":\"Mark_a_Landry\",\\\n                     \"madeupmasters\":\"MadeUpMasters\"\n                    }","e5424be5":"# all_interviewees = episode_df[\"heroes_twitter_handle\"]\n# nan_elems = all_interviewees.isnull()\n# twitter_users = all_interviewees[~nan_elems]\n\n# all_handles = []\n# for i in twitter_users.values:\n#     if \"|\" not in i:\n#         all_handles.append(i)\n#     else:\n#         handles = [i.strip() for i in i.split(\"|\")]\n#         all_handles.extend(handles)\n\n# all_handles = [corrected_handels[i] if i in corrected_handels.keys() else i for i in all_handles]\n# user_details = api.lookup_users(screen_names=all_handles)\n# hero_followers = {i.screen_name:i.followers_count for i in user_details}\n\n# with open(\"data\/hero_followers.pkl\",\"wb\") as op:\n#     pickle.dump(hero_followers, op)","99780ece":"# Saving it again, do not want to re-do\nwith open(os.path.join(external_data_dir,\"hero_followers.pkl\"),\"rb\") as op:\n    hero_followers = pickle.load(op)","005bcfba":"def get_total_fcount(hero):\n    follower_count = 0\n    if not pd.isnull(hero):\n        try:\n            if \"|\" not in hero:\n                follower_count += hero_followers[hero]     \n            else:\n                heros = [i.strip() for i in hero.split(\"|\")]\n                for hero in heros:\n                    try:\n                        follower_count += hero_followers[hero]\n                    except:\n                        if hero in corrected_handels.keys():\n                            follower_count += hero_followers[corrected_handels[hero]]\n                        else:\n                            continue\n        except KeyError:\n            if hero in corrected_handels.keys():\n                follower_count += hero_followers[corrected_handels[hero]]     \n            else:\n                print (hero)\n        \n    return follower_count","6d8fd372":"episode_df['hero_follower_count'] = episode_df[\"heroes_twitter_handle\"].apply(lambda x: get_total_fcount(x))","9defb465":"def separate_str(s):\n    if \"|\" not in s:\n        return [s.lower()]\n    else:\n        print (\"more than one interviewee present: \", s)\n        return [j.lower().strip() for j in s.split(\"|\")]\n    \nepisode_df['episode_hero'] = [separate_str(i) if isinstance(i,str) else '' for i in episode_df['heroes_twitter_handle'].values.tolist()]","78174e60":"# episode_df['episode_hero'] ","eaae1951":"episode_df.shape, sanyam_tweet_df.shape","88008499":"sanyam_tweet_df['episode_hero_tweet'] = [i.lower().split(\",\") if isinstance(i,str) else '' for i in sanyam_tweet_df['user_mentions']] \nctds_tweet_df['episode_hero_tweet'] = [i.split(\",\") if isinstance(i,str) else '' for i in ctds_tweet_df['user_mentions']] ","71b745a2":"sanyam_tweet_df.shape, ctds_tweet_df.shape","700a6ca2":"# df = sanyam_tweet_df[~sanyam_tweet_df[\"full_text\"].str.contains(\"|\".join(converse))]","792c00b3":"def split_to_rows(df, col_name):\n    '''Can be done JUST because data is small!'''\n    temp_series = df[col_name].apply(pd.Series).reset_index()\\\n                              .melt(id_vars='index').dropna()[['index', 'value']]\\\n                              .set_index('index')\n\n    df = pd.merge(temp_series, df, left_index=True, right_index=True)\n    del df[col_name]\n    df.rename(columns = {'value':col_name}, inplace = True) \n\n    df.reset_index(inplace=True)\n    del df[\"index\"]\n    \n    return df","d9a7b620":"episode_df = split_to_rows(episode_df, 'episode_hero')\nsanyam_tweet_df = split_to_rows(sanyam_tweet_df, 'episode_hero_tweet')\nctds_tweet_df = split_to_rows(ctds_tweet_df, 'episode_hero_tweet')","82752f35":"sanyam_tweet_df.shape, ctds_tweet_df.shape","4724952c":"# df = sanyam_tweet_df[~sanyam_tweet_df[\"episode_hero_tweet\"].isin(['bhutanisanyam1','ctdsshow'])]\nsanyam_tweet_df = sanyam_tweet_df[~sanyam_tweet_df[\"user_mentions\"].isnull()]","5a90ed77":"sanyam_tweet_df.shape","ce7a2c50":"episode_df[\"start_date\"] = episode_df[\"release_date_only\"] - pd.DateOffset(1)\nepisode_df[\"end_date\"] = episode_df[\"release_date_only\"] + pd.DateOffset(1)\n\nepisode_df[\"start_date\"] = pd.to_datetime(episode_df[\"start_date\"]).dt.date\nepisode_df[\"end_date\"] = pd.to_datetime(episode_df[\"end_date\"]).dt.date","ad729755":"episode_df = episode_df.assign(key=1)\nsanyam_tweet_df = sanyam_tweet_df.assign(key=1)\ndf_merge = pd.merge(episode_df, sanyam_tweet_df, on='key').drop('key',axis=1)","86f8016d":"df_merge = df_merge.query('event_date >= start_date and event_date <= end_date')","1ed98942":"df_merge_sanyam = df_merge[df_merge[\"episode_hero_tweet\"].str.lower() ==df_merge[\"episode_hero\"].str.lower()]\ndf_merge_sanyam.shape","0a5aa7a0":"df_merge_sanyam.drop_duplicates(subset=[\"tweet_id\"], keep='first', inplace=True)\n\nagg_columns = ['fav_count','retweet_count']\nagg_functions = {i:('first' if i not in agg_columns else 'sum') for i in df_merge_sanyam.columns}\n\ndf_merge_sanyam = df_merge_sanyam.groupby([\"episode_id\",\"heroes\"]).agg(agg_functions)\ndf_merge_sanyam.shape","f3549b18":"episode_df = episode_df.assign(key=1)\nctds_tweet_df = ctds_tweet_df.assign(key=1)\ndf_merge = pd.merge(episode_df, ctds_tweet_df, on='key').drop('key',axis=1)","af6802f0":"df_merge = df_merge.query('event_date >= start_date and event_date <= end_date')","2d966f83":"df_merge_ctds = df_merge[df_merge[\"episode_hero_tweet\"].str.lower() ==df_merge[\"episode_hero\"].str.lower()]\ndf_merge_ctds.drop_duplicates(subset=[\"tweet_id\"], keep='first', inplace=True)\n\nagg_columns = ['fav_count','retweet_count']\nagg_functions = {i:('first' if i not in agg_columns else 'sum') for i in df_merge_ctds.columns}\n\ndf_merge_ctds = df_merge_ctds.groupby([\"episode_id\",\"heroes\"]).agg(agg_functions)\ndf_merge_ctds.shape","f1c0281e":"df_merge_ctds.to_csv(os.path.join(external_data_dir,\"ctds_episode_tweets.csv\"),index=None)\ndf_merge_sanyam.to_csv(os.path.join(external_data_dir,\"sanyam_episode_tweets.csv\"),index=None)","c4a4f144":"no_tweet = episode_df[episode_df.heroes_twitter_handle.isnull()]\nno_tweet.to_csv(os.path.join(external_data_dir,\"no_tweets.csv\"),index=None)","b9dbdfa9":"###### Credit Links\nhttps:\/\/stackoverflow.com\/questions\/50217968\/pandas-split-list-in-column-into-multiple-rows\nhttps:\/\/gist.github.com\/jaymcgrath\/367c521f1dd786bc5a05ec3eeeb1cb04  \nhttps:\/\/gist.github.com\/yanofsky\/5436496","e946af98":"#### Extracting and saving the required info from all the tweets","5445fb5f":"#### A: Getting tweets from heroes to just get the follower's count. \n@TODO: Re-tweet from heroes? Will\/won't affect?","3a7276fc":"Making sure episode hero and the tweets allign","e7ba71e3":"#### Restricting the tweet period within the analysis time","d42fd526":"#### B: Missed schedules to be removed","36c8ec75":"#### C: Merge Data!\n\nEpisodes + Sanyam tweets","f3e325a1":"Episodes + CTDS tweets","d209373b":"### Step-3: Merging Episodes Data With Twitter data","46d52d30":"### Step-1: Getting Twitter Data","01199bcd":"#### D: Save to file because I am sure I will not know why the above steps worked","4f35a7a4":"#### C: Save to file and to hell with my pressumptions.","857e4696":"#### B: To leave or not to leave AMA?\n@TODO : AMA gives som important info? How to check the relation? \nBut for now, I will remove those tweets. Q: Why am I doing this? Remember the reason that struck while dining. Answer amma later.","867d2fd6":"### STEP-2: Filtering\n\n#### A: Get only tweets related to the show\n@TODO: Very bad way of filtering tweets that are related to the show. There must be a better way than this!","d40c52b6":"Some episodes have multiple guests, so just adding the follower count"}}