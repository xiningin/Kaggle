{"cell_type":{"484fdbcb":"code","1c3474f2":"code","9640c55c":"code","3b52543e":"code","9f63b439":"code","9ca7fdd6":"code","0709e414":"code","ff0c803a":"code","a5f4f7cb":"code","559062e0":"code","5d4de045":"code","3a3c62a6":"code","4152b939":"code","3039048b":"code","22fcb723":"code","e839743f":"code","30f5e476":"code","968618dd":"code","b1b48c77":"code","714da8d2":"code","578063e2":"code","fe7eda11":"code","a7378d88":"code","26f11c6c":"code","c78cff2f":"code","07cc7bfa":"code","a101a489":"code","5555576d":"code","0916810a":"code","2be64953":"code","49c9fd9f":"code","25354e5c":"code","5fa92b66":"code","889c388b":"code","6058101e":"code","fbe4dfc6":"code","9c01f97d":"code","317458de":"code","763921b1":"code","cf65b23a":"code","021bba98":"code","5148547b":"code","c300e0a6":"code","5fdce265":"code","f86c518e":"code","9d30c392":"code","6ba6c012":"code","e660ada8":"code","1450b44c":"code","ed878b83":"markdown","7c7a778a":"markdown","a4b720bb":"markdown","31ff458b":"markdown","2017f416":"markdown","600da802":"markdown","9bfd11b4":"markdown","58d7726c":"markdown","84ade23f":"markdown"},"source":{"484fdbcb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c3474f2":"# making use of LGBM imputation and neural network \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')","9640c55c":"## reading the data and creating a extra column \n\ntrain = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nss = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n\ntrain[\"isTrain\"] = True\ntest[\"isTrain\"] = False\n\ntt = pd.concat([train, test]).reset_index(drop=True).copy()","3b52543e":"train.shape","9f63b439":"train.head()","9ca7fdd6":"FEATURES = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n    \"audio_mode\",\n    \"speechiness\",\n    \"tempo\",\n    \"time_signature\",\n    \"audio_valence\",\n]","0709e414":"len(FEATURES)","ff0c803a":"ncounts = pd.DataFrame([train.isna().mean(), test.isna().mean()]).T\nncounts = ncounts.rename(columns={0: \"train_missing\", 1: \"test_missing\"})\n\nncounts.query(\"train_missing > 0\").plot(\n    kind=\"barh\", figsize=(8, 5), title=\"% of Values Missing\"\n)\nplt.show()","a5f4f7cb":"nacols = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n]","559062e0":"## creating a column which contains the no. of misising values in each columns\ntt[\"n_missing\"] = tt[nacols].isna().sum(axis=1)\ntrain[\"n_missing\"] = train[nacols].isna().sum(axis=1)\ntest[\"n_missing\"] = test[nacols].isna().sum(axis=1)","5d4de045":"train_missing_tag_df = train[nacols].isna()\ntrain_missing_tag_df.columns = \\\n    [f\"{c}_missing\" for c in train_missing_tag_df.columns]","3a3c62a6":"test_missing_tag_df = test[nacols].isna()\ntest_missing_tag_df.columns = \\\n    [f\"{c}_missing\" for c in test_missing_tag_df.columns]","4152b939":"train = pd.concat([train, train_missing_tag_df], axis=1)\ntest = pd.concat([test,test_missing_tag_df],axis=1)","3039048b":"train.head()","22fcb723":"ax = train['danceability'] \\\n    .plot(kind='hist', bins=50,\n          title='distribution of song_duration')\nax.axvline(train['danceability'].mean(),\n           color='black')\nax.axvline(train['danceability'].median(),\n           color='orange')","e839743f":"!rm -r kuma_utils\n\n","30f5e476":"!git clone https:\/\/github.com\/analokmaus\/kuma_utils.git","968618dd":"import sys\nsys.path.append(\"kuma_utils\/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer","b1b48c77":"%%time\nlgbm_imtr = LGBMImputer(n_iter=400, verbose=True)\n\ntrain_lgbmimp = lgbm_imtr.fit_transform(train[FEATURES])\ntest_lgbmimp = lgbm_imtr.transform(test[FEATURES])\n\n\n# Create LGBM Train\/Test imputed dataframe\ntrain_lgbmimp_df = pd.DataFrame(train_lgbmimp, columns=FEATURES)\ntest_lgbmimp_df = pd.DataFrame(test_lgbmimp,columns=FEATURES)","714da8d2":"train_lgbmimp_df.head()","578063e2":"test_lgbmimp_df.head()","fe7eda11":"train_new = pd.concat([train_lgbmimp_df, train_missing_tag_df,train['song_popularity']], axis=1)\ntest_new = pd.concat([test_lgbmimp_df, test_missing_tag_df],axis=1)\n","a7378d88":"test_new.shape","26f11c6c":"train_new.shape","c78cff2f":"train_new.head()\n","07cc7bfa":"train_new['song_popularity'].head()","a101a489":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Dropout,Dense,Flatten,Conv1D\nfrom tensorflow.keras.optimizers import Adam","5555576d":"X =train_new.drop(['song_popularity'],axis=1)\ny=train_new['song_popularity']","0916810a":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","2be64953":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y)","49c9fd9f":"X_train.head()","25354e5c":"train_new['song_popularity'].head()","5fa92b66":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\n","889c388b":"y_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","6058101e":"X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)","fbe4dfc6":"scaler2=StandardScaler()\ntrain_new = train_new.drop(['song_popularity'],axis=1)\ntrain_new=scaler2.fit_transform(train_new)\ntest_new=scaler2.transform(test_new)","9c01f97d":"test_new = test_new.reshape(test_new.shape[0],test_new.shape[1],1)","317458de":"print(X_train.shape)\nprint(X_test.shape)","763921b1":"from tensorflow.keras.layers import Dense,Activation,Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping","cf65b23a":"\nmodel=Sequential()\nmodel.add(Conv1D(32,2,activation='relu',padding='same',input_shape=X_train[0].shape))\nmodel.add(Conv1D(64,2,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64,2,activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1,activation='sigmoid'))\n\n","021bba98":"model.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])","5148547b":"model.summary()","c300e0a6":"history = model.fit(X_train,y_train,epochs=30,validation_data=(X_test,y_test))","5fdce265":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  plt.plot(epochRange,history.history['accuracy'])\n  plt.plot(epochRange,history.history['val_accuracy'])\n  plt.title('Model Accuracy')\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()\n\n  plt.plot(epochRange,history.history['loss'])\n  plt.plot(epochRange,history.history['val_loss'])\n  plt.title('Model Loss')\n  plt.xlabel('Epoch')\n  plt.ylabel('Loss')\n  plt.legend(['Train','Validation'],loc='upper left')\n  plt.show()","f86c518e":"plotLearningCurve(history,30)\n","9d30c392":"#model.predict(X_test)\n\n","6ba6c012":"prediction = model.predict(test_new)","e660ada8":"submission = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')","1450b44c":"submission['song_popularity'] = prediction\nsubmission.to_csv('submission_Convo.csv', index=False)\nsubmission.head()","ed878b83":"### creating a missing tag in the data set","7c7a778a":"# making use of LGBM imputation and neural network and LGBM Regressor to predict","a4b720bb":"### concatenating the data set with missing_tag so that model can learn and generalise","31ff458b":"### the neural network model","2017f416":"### the data set after the imputation","600da802":"### Conclusion: the model performance is not very great. It seems that the missing_tag didn't have any effect and is of no use. LB score : ***to be updated soon***","9bfd11b4":"### making the data to come on the same scale so that neural network performance is enhanced","58d7726c":"## PLEASE UPVOTE IF YOU FIND THIS HELPFUL!","84ade23f":"### the LGBM imputer in work:"}}