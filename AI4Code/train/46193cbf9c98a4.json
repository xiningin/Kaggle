{"cell_type":{"d5973260":"code","0fa97193":"code","ccfc8649":"code","248035ca":"code","0b8ce063":"code","1b671475":"code","204dfde4":"code","52f9355e":"code","81a97d24":"code","527e5616":"code","fb2d4d2b":"code","68cd9ea6":"code","234fac45":"code","e27d1fc3":"code","09ce2482":"code","50ac8c4f":"code","171ffefb":"code","4980fbb9":"code","09cce385":"code","c17fcd52":"code","c68666d3":"code","c9b31ae5":"code","c3ce6105":"code","27ca0f87":"code","6257ed6b":"code","10b4fd7a":"code","e76e0e1f":"markdown","a765e538":"markdown","dbdb0e7b":"markdown","199d5238":"markdown","af200f23":"markdown","ece2a54c":"markdown","9223d342":"markdown","ae42788a":"markdown","0783fba2":"markdown","31a41fa8":"markdown","db51fec2":"markdown","dc661954":"markdown","c81e5531":"markdown","064ef2f4":"markdown","cd46d808":"markdown","30f53225":"markdown","87eaed84":"markdown","aec224ac":"markdown","d2077cef":"markdown","39c516f2":"markdown","7f558eef":"markdown","8c7ec8c7":"markdown","ad06a0fc":"markdown"},"source":{"d5973260":"# K-Means, CURE, DBSCAN\n!pip install pyclustering\nimport csv\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pyclustering.cluster.cure import cure\nfrom pyclustering.cluster import cluster_visualizer\nfrom pyclustering.samples.definitions import SIMPLE_SAMPLES\nfrom pyclustering.samples.definitions import FCPS_SAMPLES\nfrom pyclustering.utils import read_sample\nfrom pyclustering.utils import timedcall\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nfrom mpl_toolkits import mplot3d\nfrom sklearn import metrics\nfrom sklearn.cluster import DBSCAN\n%matplotlib inline\n\ndf=pd.read_csv('..\/input\/ecommerce-data\/data.csv',encoding='iso-8859-1')\ndf['InvoiceDate'] =  pd.to_datetime(df['InvoiceDate'], format='%m\/%d\/%Y %H:%M')","0fa97193":"df.describe()","ccfc8649":"import matplotlib.pyplot as plt\n\nplt.figure()\ndf.boxplot()","248035ca":"df=df.dropna().reset_index()\ndf = df[df.Quantity <=10000]\ndf = df[df.Quantity >=0]\ndf=df.sort_values(['Quantity'],ascending=False)\ndf.shape","0b8ce063":"import matplotlib.pyplot as plt\n\nplt.figure()\ndf.boxplot('Quantity')","1b671475":"sns.set(style=\"darkgrid\")\nf, ax = plt.subplots(figsize=(25, 5))\nax = sns.countplot(x=\"Country\", data=df)","204dfde4":"dfuk=df[df['Country']=='United Kingdom']\n\n","52f9355e":"dfukg = (dfuk.groupby(['CustomerID','Country'],as_index=False)\n          .agg({'InvoiceNo':'nunique', 'StockCode':'nunique','UnitPrice':'mean','Quantity':'sum'}))\ndfukg\ndfukg.reset_index()\ndfukg['avgitems']=dfukg['Quantity']\/dfukg['InvoiceNo']\ndb=dfukg[['InvoiceNo','UnitPrice','avgitems']]\n\n","81a97d24":"import numpy as np\n\ni=0 \nwhile i<=len(db)-1:\n    quartile_1, quartile_3 = np.percentile(db['avgitems'], [25, 75])\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr *1.5 )\n    upper_bound = quartile_3 + (iqr *1.5)\n    \n    if db.loc[i,'avgitems']> upper_bound:\n        db.loc[i,'outlier']=1\n    elif db.loc[i,'avgitems']< lower_bound:\n        db.loc[i,'outlier']=1\n    else:\n        db.loc[i,'outlier']=0\n    i=i+1\n    \ndb","527e5616":"ax = sns.countplot(x=\"outlier\", data=db)","fb2d4d2b":"db=db[db['outlier']==0]\ndb= db.drop(columns=['outlier'])\ndb","68cd9ea6":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,12))\nvisualizer.fit(db)  \nvisualizer.show()        ","234fac45":"kmeans = KMeans(5)\nkmeans.fit(db)\nidentified_clusters = kmeans.fit_predict(db)\ndata_with_clusters = db.copy()\ndata_with_clusters['Cluster'] = identified_clusters\nprint(kmeans.cluster_centers_)\nprint(identified_clusters)\n\nsns.set(style=\"darkgrid\")\nf, ax = plt.subplots(figsize=(25, 5))\nax = sns.countplot(x=\"Cluster\", data=data_with_clusters)\ndata_with_clusters.groupby(['Cluster']).count()\nfig = plt.figure()\nax = plt.axes(projection='3d')\nxline=data_with_clusters['InvoiceNo']\nyline=data_with_clusters['avgitems']\nzline=data_with_clusters['UnitPrice']\n\nax.scatter3D(xline, zline,yline,c=data_with_clusters['Cluster'])\nax.view_init(60, 60)","e27d1fc3":"fig = plt.figure()\nax = plt.axes(projection='3d')\nxline=data_with_clusters['InvoiceNo']\nyline=data_with_clusters['avgitems']\nzline=data_with_clusters['UnitPrice']\n\nax.scatter3D(xline, zline,yline,c=data_with_clusters['Cluster'])\nax.view_init(60, 60)","09ce2482":"data_with_clusters[data_with_clusters['Cluster']==4]","50ac8c4f":"kmeans.cluster_centers_\n","171ffefb":"scaler = StandardScaler()\nx_scaled=scaler.fit(db)\nx_scaled = scaler.fit_transform(db)\nx_scaled","4980fbb9":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,12))\nvisualizer.fit(x_scaled)  \nvisualizer.show()     ","09cce385":"kmeans_scaled = KMeans(4)\nkmeans_scaled.fit(x_scaled)\nclusters_scaled = db.copy()\nclusters_scaled['cluster_pred']=kmeans_scaled.fit_predict(x_scaled)\nprint(identified_clusters)\nsns.set(style=\"darkgrid\")\nprint(kmeans.cluster_centers_)\nf, ax = plt.subplots(figsize=(25, 5))\nax = sns.countplot(x=\"cluster_pred\", data=clusters_scaled)\nclusters_scaled.groupby(['cluster_pred']).count()","c17fcd52":"fig = plt.figure()\nax = plt.axes(projection='3d')\nxline=clusters_scaled['InvoiceNo']\nyline=clusters_scaled['avgitems']\nzline=clusters_scaled['UnitPrice']\n\nax.scatter3D(xline, zline,yline,c=clusters_scaled['cluster_pred'])\nax.view_init(35, 60)","c68666d3":"def template_clustering(number_clusters, path, number_represent_points=1, compression=0.5, draw=True, ccore_flag=True):\n    sample = read_sample(path)\n    \n    cure_instance = cure(sample, number_clusters, number_represent_points, compression, ccore_flag)\n    (ticks, _) = timedcall(cure_instance.process)\n    \n    clusters = cure_instance.get_clusters()\n    representors = cure_instance.get_representors()\n    means = cure_instance.get_means()\n    print('clusters:',means)\n    print(\"Sample: \", path, \"\\t\\tExecution time: \", ticks, \"\\n\")\n    print([len(cluster) for cluster in clusters])\n\n    if draw is True:\n        visualizer = cluster_visualizer()\n\n        visualizer.append_clusters(clusters, sample)\n\n        for cluster_index in range(len(clusters)):\n            visualizer.append_cluster_attribute(0, cluster_index, representors[cluster_index], '*', 10)\n            visualizer.append_cluster_attribute(0, cluster_index, [ means[cluster_index] ], 'o')\n\n        visualizer.show()\n   \n\n\n\n        \nrec = db.to_records(index=False)\ndb.to_csv(r'\/kaggle\/working\/pandas.txt', header=None, index=None, sep=' ', mode='a')\npath= '\/kaggle\/working\/pandas.txt'\ntemplate_clustering(5,path)\n","c9b31ae5":"def template_clustering(number_clusters, path, number_represent_points=1, compression=0.5, draw=True, ccore_flag=True):\n    sample = read_sample(path)\n    \n    cure_instance = cure(sample, number_clusters, number_represent_points, compression, ccore_flag)\n    (ticks, _) = timedcall(cure_instance.process)\n    \n    clusters = cure_instance.get_clusters()\n    representors = cure_instance.get_representors()\n    means = cure_instance.get_means()\n    print('clusters:',means)\n    print(\"Sample: \", path, \"\\t\\tExecution time: \", ticks, \"\\n\")\n    print([len(cluster) for cluster in clusters])\n\n    if draw is True:\n        visualizer = cluster_visualizer()\n\n        visualizer.append_clusters(clusters, sample)\n\n        for cluster_index in range(len(clusters)):\n            visualizer.append_cluster_attribute(0, cluster_index, representors[cluster_index], '*', 10)\n            visualizer.append_cluster_attribute(0, cluster_index, [ means[cluster_index] ], 'o')\n\n        visualizer.show()\n   \n\n\ndtype = [('Col1','int32'), ('Col2','float32'), ('Col3','float32')]\nindex = ['Row'+str(i) for i in range(1, len(x_scaled)+1)]\n\nx_sc1 = pd.DataFrame(x_scaled, index=index)\n\nrec = x_sc1.to_records(index=False)\nx_sc1.to_csv(r'\/kaggle\/working\/pa2ndas.txt', header=None, index=None, sep=' ', mode='a')\n\npath= '\/kaggle\/working\/pa2ndas.txt'\ntemplate_clustering(4,path)\n","c3ce6105":"\nstscaler = StandardScaler().fit(db)\ndb11 = stscaler.transform(db)\ndbsc = DBSCAN(eps = .5, min_samples = 5).fit(db11)\nclusters_scaled = db.copy()\nclusters_scaled['cluster_pred']=dbsc.fit_predict(db11)\nclusters_scaled\nax = sns.countplot(x=\"cluster_pred\", data=clusters_scaled)\nclusters_scaled.groupby(['cluster_pred']).count()\n","27ca0f87":"fig = plt.figure()\nax = plt.axes(projection='3d')\nxline=clusters_scaled['InvoiceNo']\nyline=clusters_scaled['avgitems']\nzline=clusters_scaled['UnitPrice']\n\nax.scatter3D(xline, zline,yline,c=clusters_scaled['cluster_pred'])\nax.view_init(35, 60)","6257ed6b":"dbsc = DBSCAN(eps = .5, min_samples = 5).fit(db)\ndata_with_clusters = db.copy()\ndata_with_clusters['cluster_pred']=dbsc.fit_predict(data_with_clusters)\ndata_with_clusters\nax = sns.countplot(x=\"cluster_pred\", data=data_with_clusters)\ndata_with_clusters.groupby(['cluster_pred']).count()\nfig = plt.figure()\nax = plt.axes(projection='3d')\nxline=data_with_clusters['InvoiceNo']\nyline=data_with_clusters['avgitems']\nzline=data_with_clusters['UnitPrice']\n\nax.scatter3D(xline, zline,yline,c=data_with_clusters['cluster_pred'])\nax.view_init(35, 60)","10b4fd7a":"fig = plt.figure()\nax = plt.axes(projection='3d')\nxline=data_with_clusters['InvoiceNo']\nyline=data_with_clusters['avgitems']\nzline=data_with_clusters['UnitPrice']\n\nax.scatter3D(xline, zline,yline,c=data_with_clusters['cluster_pred'])\nax.view_init(35, 60)","e76e0e1f":"## Data Analysis & Preproccessing","a765e538":"### Elbow Method","dbdb0e7b":"### Outliers","199d5238":"### Scale Data","af200f23":"## Cure Algorithm at Normalized Data","ece2a54c":"### Exploring Dataset","9223d342":"### Mark and Removing the outliers","ae42788a":"#### Dimitris Chortarias Data Scientist\n\n","0783fba2":"## K-Means with not normalized data","31a41fa8":"### Elbow","db51fec2":"## DBSCAN ","dc661954":"## DBSCAN with Normalized Data","c81e5531":"### Countries Contribution at Dataset ","064ef2f4":"## K-Means with  normalized data","cd46d808":"# Data Mining Project @ E-commerce Data","30f53225":"## Cure Algorithm","87eaed84":"We are going to run the project based on how much times does the customer bought, average price of the items that he buys and quantity per buy. ","aec224ac":"### Kmeans","d2077cef":"### Selecting only UK due to sample size","39c516f2":"### Removing Null Values & filter Data on Quantity to be Positive","7f558eef":"### Creating the metrics per Customers due to create the customer Segmentation","8c7ec8c7":"## Loading Data & Packages","ad06a0fc":"### Kmeans"}}