{"cell_type":{"1f365eea":"code","9300721b":"code","a09cdb43":"code","dc87b8ad":"code","8a4f6801":"code","edfe679c":"code","c4199e6b":"code","5de26e87":"code","298afe85":"code","0423b32d":"code","3e4ec19e":"code","ea8def58":"code","5cfdcc44":"code","d80b01a8":"code","363a166b":"code","9cc71b7d":"code","3db7db0c":"code","1c8bb450":"code","52b3a2ec":"code","1eefd525":"code","76633441":"code","bf0a6941":"markdown","a5a7706f":"markdown","13018520":"markdown"},"source":{"1f365eea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9300721b":"#!ls '..\/input\/cityofla\/CityofLA\/Job Bulletins'","a09cdb43":"import re\nimport numpy as np\nimport nltk\nimport string\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\n\ndef load_jobopening_dataset():\n\n    data_path = '..\/input\/cityofla\/CityofLA\/Job Bulletins'\n    seed = 123\n    # Load the data\n    texts = []\n    positions = []\n    content_array=[]\n    for fname in sorted(os.listdir(data_path)):\n        if fname.endswith('.txt'):\n            with open(os.path.join(data_path, fname),\"rb\") as f:\n                texts.append(str(f.read()))\n                positions.append((re.split(' (?=class)', fname))[0])\n        \n    print(len(texts))\n    print(len(positions))\n\n    return (texts,positions)","dc87b8ad":"job_data, positions = load_jobopening_dataset()","8a4f6801":"job_data[0].replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\")\n#.split(\"b'\")[1]","edfe679c":"\n\nexclude = set(string.punctuation) \nwpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words('english')\nnewStopWords = ['city','los','angele','angeles']\nstop_words.extend(newStopWords)\ntable = str.maketrans('', '', string.punctuation)\n\nlemma = WordNetLemmatizer()\nporter = PorterStemmer()\n\ndef normalize_document(doc):\n    # tokenize document\n    doc = doc.replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\\\'s\",\"\").replace(\"\\t\",\" \") #.split(\"b'\")[1]\n    #doc = re.sub(r'([t])\\1+', '', doc, re.I)\n    tokens = doc.split()\n    # remove punctuation from each word\n    tokens = [w.translate(table) for w in tokens]\n    # convert to lower case\n    lower_tokens = [w.lower() for w in tokens]\n    #remove spaces\n    stripped = [w.strip() for w in lower_tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter stopwords out of document\n    filtered_tokens = [token for token in words if token not in stop_words]\n    #apply Lemmatization\n    #normalized = \" \".join(lemma.lemmatize(word) for word in filtered_tokens)\n    doc = ' '.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)","c4199e6b":"norm_positions=[]\nfor text_sample in positions:\n    norm_positions.append(normalize_document(text_sample))","5de26e87":"norm_corpus=[]\nfor text_sample in job_data:\n    norm_corpus.append(normalize_document(text_sample))","298afe85":"norm_positions[0]","0423b32d":"norm_corpus[0]","3e4ec19e":"#job_positions=[]\n#for i in range(len(norm_corpus)):\n#    text = norm_corpus[i]\n#    text = text.replace(\"tclass\",\"class\")\n#    text = text.replace(\"ttclass\",\"class\")\n#    text = re.sub(r'(.)\\1{2,}', '', text, re.I) #r'([t])\\1+'\n#    res = re.split(' (?=class)', text)\n#    print(res[0])","ea8def58":"#len(job_positions)","5cfdcc44":"def get_num_words_per_sample(sample_texts):\n    num_words = [len(s.split()) for s in sample_texts]\n    return np.median(num_words)","d80b01a8":"print(get_num_words_per_sample(norm_corpus))","363a166b":"import matplotlib.pyplot as plt\ndef plot_sample_length_distribution(sample_texts):\n    plt.hist([len(s.split()) for s in sample_texts], 50)\n    plt.xlabel('Length of a sample')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution')\n    plt.show()","9cc71b7d":"plot_sample_length_distribution(norm_corpus)","3db7db0c":"from collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef plot_frequency_distribution_of_ngrams(sample_texts,\n                                          ngram_range=(1, 3),\n                                          num_ngrams=30):\n    \"\"\"Plots the frequency distribution of n-grams.\n\n    # Arguments\n        samples_texts: list, sample texts.\n        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n            Min and mplt are the lower and upper bound values for the range.\n        num_ngrams: int, number of n-grams to plot.\n            Top `num_ngrams` frequent n-grams will be plotted.\n    \"\"\"\n    # Create args required for vectorizing.\n    kwargs = {\n            'ngram_range': ngram_range,\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': 'word',  # Split text into word tokens.\n    }\n    vectorizer = CountVectorizer(**kwargs)\n\n    vectorized_texts = vectorizer.fit_transform(sample_texts)\n\n    # This is the list of all n-grams in the index order from the vocabulary.\n    all_ngrams = list(vectorizer.get_feature_names())\n    num_ngrams = min(num_ngrams, len(all_ngrams))\n    # ngrams = all_ngrams[:num_ngrams]\n\n    # Add up the counts per n-gram ie. column-wise\n    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n\n    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n        zip(all_counts, all_ngrams), reverse=True)])\n    ngrams = list(all_ngrams)[:num_ngrams]\n    counts = list(all_counts)[:num_ngrams]#num_ngrams\n\n    idx = np.arange(num_ngrams)\n    plt.figure(figsize=(30,20)) \n    plt.bar(idx, counts, width=0.6, color='b')\n    plt.xlabel('N-grams',fontsize=\"18\")\n    plt.ylabel('Frequencies',fontsize=\"18\")\n    plt.title('Frequency distribution of n-grams',fontsize=\"36\")\n    plt.xticks(idx, ngrams, rotation=45, fontsize=18)\n    plt.yticks(fontsize=18)\n    plt.show()\n","1c8bb450":"plot_frequency_distribution_of_ngrams(norm_corpus)","52b3a2ec":"plot_frequency_distribution_of_ngrams(norm_positions)","1eefd525":"full_norm_corpus=' '.join(norm_corpus)\n\nfrom wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud().generate(full_norm_corpus)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","76633441":"full_norm_corpus=' '.join(norm_positions)\n\nfrom wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud().generate(full_norm_corpus)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","bf0a6941":"**Some words that might scare off applicants are : written test**\n\n**Good points : equal employer, disability**","a5a7706f":"**Shows Supervisor jobs, chief jobs, officer, engineering, inspector are most prevalent, followed by electrician, analyst,associate**\n\n**Some issues are use of terms like worker, operator , maybe these can be changed for better reach**","13018520":"**Observation here is : the words 'may', 'candidates', 'examination' , 'applicants' are occurring too many times. This probably will make the job ad very very formal, and scare away applicants.**********\n\n**The good point is that the word 'disability' is making in the top 20 most frequent words, which means the job ads are inclusive.**"}}