{"cell_type":{"5ec043f3":"code","1cbb5ba3":"code","94166057":"code","d295a9b9":"code","8671fab7":"code","f905398a":"code","d53e785f":"code","8166f171":"code","5e1c487f":"code","c5d7fab2":"code","35e4bbf5":"code","8246d01e":"code","894af8c9":"code","a82a0f70":"code","02ff2c5d":"code","521945a9":"code","1cc790f9":"code","14be9dee":"code","25130baf":"code","693e89c0":"code","a93b359a":"code","3999b057":"code","b3ff475f":"code","76232460":"code","2eae94f2":"code","4eb436ff":"code","df281dc4":"code","0abc4f6f":"code","9a0f6827":"code","d06b5e84":"code","84d3579c":"code","9c2c61e8":"code","89d4a7e9":"code","4f19982c":"code","54fbfe58":"code","7cec56cd":"code","c5de7334":"code","16b8ccdb":"code","7dfa89c9":"code","b285326b":"code","dc62325f":"code","50c5763b":"code","f143c05c":"code","08bcc907":"code","b2ecc785":"code","8800c212":"code","34ea5459":"code","c48b81a7":"code","40001ddc":"code","686ae242":"code","e51a27d4":"code","efdc240b":"markdown","597d9911":"markdown","6a2d34a4":"markdown","5903df1c":"markdown","5272ae41":"markdown","19799004":"markdown","5339027b":"markdown","436df375":"markdown","a4e7e760":"markdown","c0e5a843":"markdown","325b086d":"markdown","9cd081bf":"markdown","eea3845f":"markdown","a3242338":"markdown","29629ef8":"markdown"},"source":{"5ec043f3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')","1cbb5ba3":"# Data visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Performance metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV","94166057":"# Review the train dataset\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()","d295a9b9":"# Data types\ntrain.dtypes","8671fab7":"# Separate into X and y variables\nX = train.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ny = train['Survived'].values","f905398a":"# Add the dummy values. Drop the first value to show the minimum cohort's required\nX = pd.get_dummies(X, drop_first=True)\nX.head()","d53e785f":"# Check the missing values\nX.isnull().sum()","8166f171":"# Apply the average value for the missing values\nX['Age'].fillna(X['Age'].mean(), inplace=True)\nX.isnull().sum()","5e1c487f":"# Scale the features\nX_scaled = scale(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(\"Mean of Unscaled Features: {}\".format(np.mean(X))) \nprint(\"Standard Deviation of Unscaled Features: {}\".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(\"Mean of Scaled Features: {}\".format(np.mean(X_scaled))) \nprint(\"Standard Deviation of Scaled Features: {}\".format(np.std(X_scaled)))","c5d7fab2":"# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))","35e4bbf5":"# Lets understand the performance of the k-NN classifer across a range of clusters\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 12)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","8246d01e":"# Lets understand the Confusion matrix and classifcation report\n\n# Instantiate a k-NN classifier\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","894af8c9":"# Setup the pipeline steps\nsteps = [('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier())]\n        \n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the training set\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))","a82a0f70":"# Instantiate a logisitic regression\nlogreg = LogisticRegression(solver='sag')\n\n# Fit the model\nlogreg.fit(X_train, y_train)\n\n# Make a prediction\ny_pred = logreg.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","02ff2c5d":"# Compute predicted probabilities\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","521945a9":"# Compute predicted probabilities\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores\ncv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))","1cc790f9":"# Hyperparameter Tuning\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate a logistic regression classifier\nlogreg = LogisticRegression(solver='sag')\n\n# Instantiate the GridSearchCV object\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv.best_score_))","14be9dee":"# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier\nlogreg = LogisticRegression(solver='sag')\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate the GridSearchCV object\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))","25130baf":"# Setup the parameters and distributions to sample\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","693e89c0":"# Setup the pipeline steps\nsteps = [\n#         ('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n        ('SVM', SVC())]\n\n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))","a93b359a":"# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Instantiate the GridSearchCV object\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))","3999b057":"# Setup the pipeline steps\nsteps = [('scaler', StandardScaler()),\n    ('NB', BernoulliNB())]\n\n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))","b3ff475f":"# Setup the pipeline steps\nsteps = [('scaler', StandardScaler()),\n    ('NB', GaussianNB())]\n\n# Create the pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))","76232460":"# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('RF', RandomForestClassifier())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))","2eae94f2":"# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('RF', RandomForestClassifier())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'RF__n_estimators':list(range(1,50))}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Instantiate the GridSearchCV object\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_score_))","4eb436ff":"class ModelBuild():\n    # Constructor\n    def __init__(self, X, y, model=LogisticRegression(solver='liblinear',multi_class='auto')):\n        self.X = X\n        self.y = y\n        self.model = model\n    \n    # Method to perform the train test split\n    def _train_test_split(self):\n        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    # Method to set the pipeline\n    def _pipeline(self):\n        steps = [('scaler', StandardScaler()),\n                 ('model_name', self.model)]\n        return Pipeline(steps)\n    \n    # Method to run all steps\n    def model_build(self):\n        if __name__ == \"__main__\":\n            X_train, X_test, y_train, y_test = self._train_test_split()\n            pipeline = self._pipeline()\n            fit = pipeline.fit(X_train, y_train)\n            return print(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))\n            ","df281dc4":"ModelBuild(X, y).model_build()","0abc4f6f":"ModelBuild(X, y, model=RandomForestClassifier()).model_build()","9a0f6827":"ModelBuild(X, y, model=DecisionTreeClassifier()).model_build()","d06b5e84":"ModelBuild(X, y, model=GaussianNB()).model_build()","84d3579c":"class FeatureSelection(ModelBuild):\n    \n    # Inherit the ModelBuild features\n    def __init__(self, X, y, model=RandomForestClassifier()):\n        super().__init__(X, y, model=RandomForestClassifier())\n        self.X = X\n        self.y = y\n        self.model = model\n    \n    # Method to evaluate list of models\n    def rfe_model(self):\n        model_dict = dict()\n        for i in range(2, 8):\n            rfe = RFE(estimator=self.model, n_features_to_select=i)\n            model = DecisionTreeClassifier()\n            model_dict[str(i)] = Pipeline(steps=[('rfe', rfe), ('mod', model)])\n        return model_dict\n    \n    # Method to evaluate the models\n    def eval_model(self, model):\n        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=6)\n        scores = cross_val_score(model, self.X, self.y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n        return scores\n    \n    # Lets understand the features being selected\n    def feature_select(self, n_feature):\n        rfe = RFE(estimator=self.model, n_features_to_select=n_feature)\n        rfe.fit(self.X, self.y)\n#         for i in range(X.shape[1]):\n        for i, col in enumerate(X.columns):\n            print('Column: %s, Selected %s, Rank: %.3f' % (col, rfe.support_[i], rfe.ranking_[i]))   \n    \n    # Method to run all steps\n    def feature_selection(self):\n        if __name__ == \"__main__\":\n            models = self.rfe_model()\n            results, names = list(), list()\n            for name, model in models.items():\n                scores = self.eval_model(model)\n                results.append(scores)\n                names.append(name)\n                print(f'{name}, mean_score: {np.mean(scores)}, std_score: {np.std(scores)}')\n                box_plt = plt.boxplot(results, labels=names, showmeans=True)\n            return box_plt","9c2c61e8":"box = FeatureSelection(X, y).feature_selection()\nplt.show()","89d4a7e9":"box = FeatureSelection(X, y, model=DecisionTreeClassifier()).feature_selection()\nplt.show()","4f19982c":"features = FeatureSelection(X, y, model=DecisionTreeClassifier()).feature_select(5)","54fbfe58":"# X = X.loc[:, ['Pclass', 'Age', 'SibSp', 'Fare', 'Sex_male']]\nX = X.loc[:, ['Pclass', 'Age', 'Fare', 'Sex_male']]\nX.head()","7cec56cd":"# Scale the features\nX_scale = StandardScaler(X)","c5de7334":"box = FeatureSelection(X, y, model=DecisionTreeClassifier()).feature_selection()\nplt.show()","16b8ccdb":"class HyperTuning(FeatureSelection):\n    \n    # Inherit the ModelBuild and FeatureSelection\n    def __init__(self, X, y, **kwargs):\n        super().__init__(X, y, **kwargs)\n        self.model_params = {\n    'svm': {\n        'model': SVC(),\n        'params' : {\n            'C': [50, 10, 1.0, 0.1, 0.01],\n            'kernel': ['rbf', 'poly', 'sigmoid'],\n            'gamma' : ['scale']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [10, 100, 1000],\n            'max_features' : ['sqrt', 'log2']\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(),\n        'params': {\n            'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n            'penalty' : ['l2'],\n            'C': [100, 10, 1.0, 0.1, 0.01]\n        }\n    },\n    'k-nearest neighbour': {\n        'model': KNeighborsClassifier(),\n        'params': {\n            'n_neighbors' : range(1, 21, 2),\n            'weights' : ['uniform', 'distance'],\n            'metric' : ['euclidean', 'manhattan', 'minkowski']\n        }\n    },\n    'naive_bayes_gaussian': {\n        'model': GaussianNB(),\n        'params': {}\n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            \n        }\n    }     \n}\n    \n    # Method to run the hyper params\n    def hyper_params(self, X_train, y_train):\n        scores = list()\n        for model_name, model_param in self.model_params.items():\n            cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n            clf = GridSearchCV(model_param['model'], model_param['params'], cv=3)\n            clf.fit(X_train, y_train)\n            scores.append({\n                'model':model_name,\n                'best_score':clf.best_score_,\n                'best_params':clf.best_params_\n            })\n        return scores\n    \n    # Method to run all steps\n    def hyper_tuning(self):\n        if __name__ == \"__main__\":\n            X_train, X_test, y_train, y_test = self._train_test_split()\n            scores = self.hyper_params(X_train, y_train)\n            df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n            df = df.sort_values('best_score', ascending=False, ignore_index=True)\n            return df","7dfa89c9":"features = HyperTuning(X, y, model=DecisionTreeClassifier()).feature_select(5)","b285326b":"HyperTuning(X, y).model_build()","dc62325f":"# Not working properly - need to review\n# df = HyperTuning(X_scale, y).hyper_tuning()\ndf = HyperTuning(X, y).hyper_tuning()","50c5763b":"df","f143c05c":"HyperTuning(X, y, model=RandomForestClassifier(max_features='log2', n_estimators=100)).model_build()","08bcc907":"# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('RF', RandomForestClassifier(max_features='log2', n_estimators=100))]\n\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit to the training set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(pipeline.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))","b2ecc785":"# Review the test dataset\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","8800c212":"# Check column data types\ntest_data.dtypes","34ea5459":"# Keep columns\ntest = test_data.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked'], axis=1)\ntest.head()","c48b81a7":"# Add the dummy values. Drop the first value to show the minimum cohort's required\ntest = pd.get_dummies(test, drop_first=True)\ntest.head()","40001ddc":"# Check for missing values\ntest.isnull().sum()","686ae242":"# Apply the average value for the missing values\ntest['Age'].fillna(test['Age'].mean(), inplace=True)\ntest['Fare'].fillna(test['Fare'].mean(), inplace=True)\ntest.isnull().sum()","e51a27d4":"# Make predictions\ntest_predictions = pipeline.predict(test)\noutput = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived':test_predictions.astype(int)})\noutput.to_csv('my_submission.csv', index=False)","efdc240b":"## Make the predictions","597d9911":"# Decision Tree","6a2d34a4":"# k-NN classifier","5903df1c":"# Naive Bayes - Gaussian","5272ae41":"# Creating Class statement","19799004":"Popular algorithms that can be used for binary classification include:\n\n* Logistic Regression\n* k-Nearest Neighbors\n* Decision Trees\n* Support Vector Machine\n* Naive Bayes\n\nIncluding a Random Forest Classifier as well.\n***\nAiming to convert code into Class statements to allow for more automated coding.\n***\nPerform Feature selection\n***\nHyperparameter Tuning","5339027b":"# Hyperparameter Tuning","436df375":"# Naive Bayes - Bernoulli","a4e7e760":"# Feature selection","c0e5a843":"# Random Forest","325b086d":"# Journey into the unknown","9cd081bf":"Going to keep these five variables moving forward","eea3845f":"# Logistic Regression","a3242338":"### Lets run the random forest","29629ef8":"# SVC"}}