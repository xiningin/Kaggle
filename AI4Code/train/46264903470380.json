{"cell_type":{"dad6a3ab":"code","23510b61":"code","cd16eee5":"code","c172e23e":"code","f4c277ed":"code","9c76b434":"code","98c44feb":"code","4a5f344a":"code","ec334add":"code","a799ea9a":"code","f86e1017":"code","d715e4b6":"code","ad14ec1d":"code","8fe15d36":"code","f877f69f":"code","94cce008":"code","7859b5cd":"code","84cd6b61":"code","8859352b":"code","969e52e1":"code","0a2bb8a4":"code","a3d9c2dd":"code","415f1159":"markdown","dab38502":"markdown","b581d006":"markdown","bf279a5a":"markdown","28b09491":"markdown","3069754e":"markdown","ea9f5112":"markdown","8b1069b2":"markdown","93380de2":"markdown","e3226b3b":"markdown","e126c0c8":"markdown","922312b0":"markdown","07f33995":"markdown","80cbe5b5":"markdown","228b2a7f":"markdown","f66291ee":"markdown","56722f73":"markdown"},"source":{"dad6a3ab":"''' Global Configuration Settings '''\nclass CFG:\n    \n    def __init__(self):\n        self.n_epochs = 200\n        self.seed = 221\n\ncfg = CFG()","23510b61":"import numpy as np # linear algebra\nfrom numpy import argmax\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom keras import backend as K\nimport keras\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom tqdm.keras import TqdmCallback\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow as tf\ntf.get_logger().setLevel('INFO')\nimport tensorflow.keras.layers as layers\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport random as rn\nimport logging\ntf.get_logger().setLevel(logging.ERROR)\n\n# various library seeds\nos.environ['PYTHONHASHSEED'] = '0'                      \nnp.random.seed(cfg.seed)\nrn.seed(cfg.seed)\ntf.random.set_seed(cfg.seed)","cd16eee5":"path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/'\nos.chdir(path)\ntrain_path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/train\/'\ntest_path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/test\/'\nvalid_path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/valid\/'\n\nclass read_data:\n    \n    def __init__(self):\n        \n        train_path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/train\/'\n        test_path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/test\/'\n        valid_path = '\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/valid\/'\n        \n        # Training Dataset\n        self.train = {}; self.valid = {}; self.test = {}\n        with open(train_path+'train_X.npy','rb') as f:\n            self.train['X'] = np.load(f)\n        f.close()\n        with open(train_path+'train_y.npy','rb') as f:\n            self.train['y'] = np.load(f)\n        f.close()\n        with open(train_path+'train_w.npy','rb') as f:\n            self.train['w'] = np.load(f)\n        f.close()\n        with open(train_path+'train_ids.npy','rb') as f:\n            self.train['ids'] = np.load(f,allow_pickle=True)\n        f.close()\n        \n        # For Validation during Training\n        with open(valid_path+'valid_X.npy','rb') as f:\n            self.valid['X'] = np.load(f)\n        f.close()\n        with open(valid_path+'valid_y.npy','rb') as f:\n            self.valid['y'] = np.load(f)\n        f.close()\n        with open(valid_path+'valid_w.npy','rb') as f:\n            self.valid['w'] = np.load(f)\n        f.close()\n        with open(valid_path+'valid_ids.npy','rb') as f:\n            self.valid['ids'] = np.load(f,allow_pickle=True)\n        f.close()\n        \n        # For Inference\n        with open(test_path+'test_X.npy','rb') as f:\n            self.test['X'] = np.load(f)\n        f.close()\n        with open(test_path+'test_y.npy','rb') as f:\n            self.test['y'] = np.load(f)\n        f.close()\n        with open(test_path+'test_w.npy','rb') as f:\n            self.test['w'] = np.load(f)\n        f.close()\n        with open(test_path+'test_ids.npy','rb') as f:\n            self.test['ids'] = np.load(f,allow_pickle=True)\n        f.close()\n        \n    # Add chromatic accessibility feature\n    def add_chrom(self):\n        \n        span_accessibility = {}\n        for line in open('\/kaggle\/input\/bioinformatics\/machine_learning\/JUND_TF\/accessibility.txt'):\n            fields = line.split()\n            span_accessibility[fields[0]] = float(fields[1])\n    \n        # find correct value for txt & save array for later load\n        def dicarray_df(case):\n            \n            if(case is 'train'):\n                case_id = self.train\n            elif(case is 'test'):\n                case_id = self.test\n            elif(case is 'valid'):\n                case_id = self.valid\n            \n            ldf = pd.DataFrame([k, *v] for k, v in case_id.items())\n            ldf = ldf.T\n            ldf.columns = ['X','y','w','ids']\n            ldf.drop(0,axis=0,inplace=True)\n            ldf.reset_index(drop=True,inplace=True)\n            ldf[\"chrom\"] = ldf[\"ids\"].map(span_accessibility)\n#             print(os.getcwd())\n            os.chdir('\/kaggle\/working\/')\n            np.save(f'.\/chrom_{case}.npy',ldf['chrom'].values)\n    \n        # match & save array\n        dicarray_df('train')\n        dicarray_df('test')\n        dicarray_df('valid')\n\n        array = np.load('.\/chrom_train.npy')\n        self.train['chrom'] = array\n#         indx = self.train['X'].shape[0]\n#         temp = self.train['X'].reshape(indx,404)\n#         fm = np.concatenate((temp,self.train['chrom'][:,None]),axis=1)\n#         self.train['X'] = fm\n        \n        array = np.load('.\/chrom_test.npy')\n        self.test['chrom'] = array\n#         indx = self.test['X'].shape[0]\n#         temp = self.test['X'].reshape(indx,404)\n#         fm = np.concatenate((temp,self.test['chrom'][:,None]),axis=1)\n#         self.test['X'] = fm\n    \n        array = np.load('.\/chrom_valid.npy')\n        self.valid['chrom'] = array\n#         indx = self.valid['X'].shape[0]\n#         temp = self.valid['X'].reshape(indx,404)\n#         fm = np.concatenate((temp,self.valid['chrom'][:,None]),axis=1)\n#         self.valid['X'] = fm\n        \ndata = read_data() # read data","c172e23e":"print(f\"Training - Feature Matrix: {data.train['X'].shape}\")\nprint(f\"Training - Target Vector: {data.train['y'].shape}\")\nprint(f\"Training - Sample Weighting: {data.train['w'].shape}\\n\")\n\nprint(f\"Validation - Feature Matrix: {data.valid['X'].shape}\")\nprint(f\"Validation - Target Vector: {data.valid['y'].shape}\")\nprint(f\"Validation - Sample Weighting: {data.valid['w'].shape}\\n\")\n\nprint(f\"Test - Feature Matrix: {data.test['X'].shape}\")\nprint(f\"Test - Target Vector: {data.test['y'].shape}\")\nprint(f\"Test - Sample Weighting: {data.test['w'].shape}\")","f4c277ed":"data_all_X = np.concatenate([data.train['X'],data.valid['X'],data.test['X']],axis=0)\ndata_all_y = np.concatenate([data.train['y'],data.valid['y'],data.test['y']],axis=0)\ndata_all_w = np.concatenate([data.train['w'],data.valid['w'],data.test['w']],axis=0)\ndata_all_ids = np.concatenate([data.train['ids'],data.valid['ids'],data.test['ids']],axis=0)\n\nprint(data_all_X.shape)\nprint(data_all_y.shape)\nprint(data_all_w.shape)\nprint(data_all_ids.shape)","9c76b434":"# Test Sequence you want to OHE for ML problems\nseq = 'ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGA' +\\\n      'TCTGTTCTCTAAACGAACTTTAAAATCTGTGTGGCTGTCACTC' + \\\n      'GGCTGCATGCTTAGTGCACTCACGCAGTATAATTAATAACTAATTACTGTCGTTGAC' +\\\n      'AGGACACGAGTAACTCGTCTATCTTCTGCAGGCTGCTTACGGT'  + \\\n      'TTCGTCCGTGTTGCAGCCGATCATCAGCACATCTAGGTTTCGTCCGGGTGTGACCGA' +\\\n      'AAGGTAAGATGGAGAGCCTTGTCCCTGGTTTCAACGAGAAAA' + \\\n      'CACACGTCCAACTCAGTTTGCCTGTTTTACAGGTTCGCGACGTGCTCGTAC'\n\n# Function for when you want to prepare DNA sequence feature for ML applications\ndef dnaseq_features(seq,start=0,n_segs=101,seq_name=None):\n    \n    print(f\"Input Sequence Length: {len(seq)}\")\n    remaind = len(seq)%n_segs\n    if(remaind is not 0):\n        last_id = len(seq) - remaind\n    print(f\"# Bases cut-off: {int(remaind)}\")\n    \n    upd_seq = seq[start:last_id]\n    \n    print(f\"Updated sequence length: {len(upd_seq)}\")\n    print(f\"# Segments: {int(len(upd_seq)\/n_segs)} created\")\n    if(seq_name is None):\n        seq_name = 'seq'\n    \n    # store sequence subsets in a dictionary\n    dic_seq = {}\n    for i in range(0,3):\n        a = int(i*n_segs) ; b = int(i*n_segs)+n_segs \n        identifier = f\"{seq_name}_{a}:{b}\"\n        dic_seq[identifier] = upd_seq[a:b]\n        \n    lst_seq = dic_seq.values()\n    index = list(dic_seq.keys())\n    \n    # One hot encode\n        \n    ii=-1\n    for data in lst_seq:\n\n        ii+=1; abc = 'acgt'.upper()\n\n        char_to_int = dict((c, i) for i, c in enumerate(abc))\n        int_enc = [char_to_int[char] for char in data]\n\n        ohe = []\n        for value in int_enc:\n            base = [0 for _ in range(len(abc))]\n            base[value] = 1\n            ohe.append(base)\n        np_mat = np.array(ohe)\n        np_mat = np.expand_dims(np_mat,axis=0)\n\n        if(ii is not 0):\n            matrix = np.concatenate([np_mat,matrix],axis=0)\n        else:\n            matrix = np_mat\n        \n    return matrix,index\n\ndna_ohe_feat,dna_ohe_index = dnaseq_features(seq)\nprint(f'\\n{type(dna_ohe_feat)}')\nprint(f'DNA OHE features: {dna_ohe_feat.shape}')\nprint(f'Index: {dna_ohe_index[:10]}')","98c44feb":"# Target Value Counts\ntarg_weight = pd.DataFrame(np.concatenate([data.train['y'],data.train['w']],axis=1),columns=['y','w'])\ntarg_weight['y'].value_counts()\ntarg_weight['y'].value_counts()","4a5f344a":"fig = go.Figure()\nx = np.array([i for i in range(10000)])\nw = data.train['w'][:10000][:,0]\ny = data.train['y'][:10000][:,0]\n\nfig.add_trace(go.Scatter(x=x,y=w,mode='lines',name='weight'))\nfig.add_trace(go.Scatter(x=x,y=y,mode='lines',name='target'))\nfig.update_yaxes(range=[0, 2])\nfig.update_layout(template='plotly_white',title='WEIGHT & TARGET RELATION',height=350,showlegend=True)\nfig.update_layout(margin={\"r\":30,\"t\":80,\"l\":30,\"b\":30})\nfig.update_layout(font=dict(family='sans-serif',size=12))\nfig.show()","ec334add":"targ_weight.groupby('y').sum()","a799ea9a":"''' FUNCTIONS FOR PLOTTING KERAS HISTORY RESULTS '''\n\n# Function to plot all metrics side by side (defined above)\ndef plot_keras_metric(history,title_id):\n\n    # Palettes\n    lst_color = ['#F55AA2','#323232','#004379','#032B52']\n    metric_id = ['loss','auc'] # change your metrics to plot here\n    fig = make_subplots(rows=1, cols=len(metric_id),subplot_titles=metric_id)\n\n    jj=0;\n    for metric in metric_id:     \n\n        jj+=1\n        fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],\n                                 y=history.history[metric],\n                                 name=f'train_{metric}',\n                                 line=dict(color=lst_color[0]),mode='lines'),\n                      row=1,col=jj)\n        \n        fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],\n                                 y=history.history['val_'+metric],\n                                 name=f'val_{metric}',\n                                 line=dict(color=lst_color[1]),mode='lines'),\n                      row=1,col=jj)\n\n        # difference between training\/validation metrics\n        if(metric is not 'loss'):\n            diff = abs(np.array(history.history[metric]) \\\n                       - np.array(history.history['val_'+metric]))\n            fig.add_trace(go.Bar(x=[i for i in range(1,cfg.n_epochs+1)],\n                                 y=diff,name='metric diff.',\n                                 marker_color=lst_color[3],\n                                 opacity=0.15,showlegend=False)\n                          ,row=1,col=jj)\n\n    # Plot Aesthetics\n    fig.update_layout(yaxis=dict(range=[0,1]),yaxis_range=[0,1],margin=dict(b=10),\n                      height=400,showlegend=False,template='plotly_white',\n                      font=dict(family='sans-serif',size=14),\n                      hovermode=\"x\",title=f'<b>Training History<\/b> | {title_id}')\n    \n    fig['layout']['yaxis'].update(title='', range=[0,5], autorange=True,type='log')\n    fig['layout']['yaxis2'].update(title='', range=[0, 1.1], autorange=False)    \n    fig.show()","f86e1017":"''' Convolution 1D CNN Model '''\n# function conv1d_model w\/ argument to drop .Dropout\n\ndef conv1d_model(drop_id=True,\n                 in_shape=(101,4)):\n\n    if(drop_id is False):\n        model = keras.models.Sequential([\n            keras.layers.Conv1D(15, \n                                kernel_size=10,\n                                padding=\"same\", \n                                activation=\"relu\", \n                                input_shape=in_shape),    \n            keras.layers.Dropout(0.5),\n            keras.layers.Conv1D(15, \n                                kernel_size=10,\n                                padding=\"same\", \n                                activation=\"relu\"),\n            keras.layers.Dropout(0.5),\n            keras.layers.Conv1D(15, \n                                kernel_size=10,\n                                padding=\"same\", \n                                activation=\"relu\"),\n            keras.layers.Dropout(0.5),  \n            keras.layers.Flatten(),\n            keras.layers.Dense(1,activation='sigmoid')])\n        \n        return model\n    \n    if(drop_id is True):\n        model = keras.models.Sequential([\n            keras.layers.Conv1D(15, \n                                kernel_size=10,\n                                padding=\"same\", \n                                activation=\"relu\", \n                                input_shape=in_shape),    \n            keras.layers.Conv1D(15, \n                                kernel_size=10,\n                                padding=\"same\", \n                                activation=\"relu\"),\n            keras.layers.Conv1D(15, \n                                kernel_size=10,\n                                padding=\"same\", \n                                activation=\"relu\"),\n            keras.layers.Flatten(),\n            keras.layers.Dense(1,activation='sigmoid')])\n        return model\n    \ntest_seq = conv1d_model(drop_id=True)","d715e4b6":"''' KERAS METRICS '''\n# Keras offers quite a few metrics we can call in compile\n\nMETRICS = [\n    #   keras.metrics.TruePositives(name='tp'),\n    #   keras.metrics.FalsePositives(name='fp'),\n    #   keras.metrics.TrueNegatives(name='tn'),\n    #   keras.metrics.FalseNegatives(name='fn'), \n#       keras.metrics.BinaryAccuracy(name='accuracy'),\n#       keras.metrics.Precision(name='precision'),\n#       keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc')\n    #   keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\n# Compile Neural Network\ntest_seq.compile(\n    optimizer='sgd',\n    loss='binary_crossentropy',\n    metrics=METRICS)","ad14ec1d":"# Set Model\nmodel1 = conv1d_model(drop_id=True)\n\n# Set Metrics\nMETRICS = [keras.metrics.AUC(name='auc')]\n\n# Compile Neural Network\nmodel1.compile(\n    optimizer='sgd',\n    loss='binary_crossentropy',\n    metrics=METRICS)\n\n# Train Neural Network\nhist1 = model1.fit(x=data.train['X'],              # feature matrix   \n                   y=data.train['y'],              # target vector \n                   sample_weight=data.train['w'],  # weight vector\n            validation_data=(data.valid['X'],\n                             data.valid['y']), # validation data\n            epochs = cfg.n_epochs,  # number of iterations\n            batch_size = 512,      # batch size\n            callbacks = [TqdmCallback(verbose=0)], # tqdm for keras\n            verbose=0) # turn of training messagaes","8fe15d36":"# Evaluate on Training \/ Test Sets\nmodel1.evaluate(data.train['X'],data.train['y'])\nmodel1.evaluate(data.valid['X'],data.valid['y'])\n# Inference on Unseen Data\nmodel1.evaluate(data.test['X'],data.test['y'])","f877f69f":"plot_keras_metric(hist1,'No Dropout Model')","94cce008":"# Set Model\nmodel2 = conv1d_model(drop_id=False)\n\n# Set Metrics\nMETRICS = [keras.metrics.AUC(name='auc')]\n\n# Compile Neural Network\nmodel2.compile(\n    optimizer='sgd',\n    loss='binary_crossentropy',\n    metrics=METRICS)\n\n# Train Neural Network\nhist2 = model2.fit(x=data.train['X'],              # feature matrix   \n                   y=data.train['y'],              # target vector \n                   validation_data=(data.valid['X'],\n                                    data.valid['y']), # validation data\n            epochs = cfg.n_epochs,  # number of iterations\n            batch_size = 512,      # batch size\n            verbose=0) # turn of training messages","7859b5cd":"# Evaluate on Training \/ Test Sets\nmodel2.evaluate(data.train['X'],data.train['y'])\nmodel2.evaluate(data.valid['X'],data.valid['y'])\n# Inference on Unseen Data\nmodel2.evaluate(data.test['X'],data.test['y'])","84cd6b61":"plot_keras_metric(hist2,'No Sample Weighting Model')","8859352b":"# Set Model\nmodel3 = conv1d_model(drop_id=False)\n\n# Set Metrics\nMETRICS = [keras.metrics.AUC(name='auc')]\n\n# Compile Neural Network\nmodel3.compile(\n    optimizer='sgd',\n    loss='binary_crossentropy',\n    metrics=METRICS)\n\nmodel3.summary()","969e52e1":"# Train Neural Network\nhist3 = model3.fit(x=data.train['X'],              # feature matrix   \n                   y=data.train['y'],              # target vector \n                   sample_weight=data.train['w'],  # weight vector\n            validation_data=(data.valid['X'],\n                             data.valid['y']), # validation data\n            epochs = cfg.n_epochs,  # number of iterations\n            batch_size = 512,      # batch size\n            callbacks = [TqdmCallback(verbose=0)], # tqdm for keras\n            verbose=0) # turn of training messages","0a2bb8a4":"# Evaluate on Training \/ Test Sets\nmodel3.evaluate(data.train['X'],data.train['y'])\nmodel3.evaluate(data.valid['X'],data.valid['y'])\n# Inference on Unseen Data\nmodel3.evaluate(data.test['X'],data.test['y'])","a3d9c2dd":"plot_keras_metric(hist3,'Baseline Transcription Factor Model')","415f1159":"### **SUMMARY : <span style='color:#F55AA2'>BASELINE INVESTIGATIVE MODELS<\/span>**\n- As was expected; the model started to show significant difference between training & validation results, the evaluation reaching only a **validation AUC** of 0.504, which could suggest that the addition of dropout layers in the neural network model were justified\n- When we didn't use weighting for each sample, we have a very small difference between training & validation results, which is what we want, however the model learns very slowly & only reaches a **validation AUC** of 0.58, this could be an indicator that sample imbalance is an important issue, and weighting them was justified","dab38502":"### **SUMMARY : <span style='color:#F55AA2'>TRAINING BASELINE MODEL<\/span>**\n- We can see that the model performs much better when we take into consideration both, **sample weighting** & **overfitting**.\n- The **AUC** of both training & validation data is very close, which is what we wanted & we have an improvement in model performance, reaching a **validation AUC** of 0.76.","b581d006":"- Using <code>groupby<\/code>, we can confirm that the **class weighting** of sample are completely balanced as well","bf279a5a":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.5 | TARGET IMBALANCE<\/b><\/p>\n<\/div>\n\nFirst of all, let's inspect our preprocessed data, starting with the **target variable distribution**\n\n- As we can see our imbalance ratio is quite high, only about 1200 sequences **at which transcription factors bind to our DNA sequences** (samples) in the training set.\n- Nevertheless we have plenty of <b>True Positives<\/b>, unlike in notebook **[Identifying Antibiotic Resistant Bacteria](https:\/\/www.kaggle.com\/shtrausslearning\/identifying-antibiotic-resistant-bacteria)**, where we only had a handful of such cases in the entire dataset.\n- One approach we can use when our data is imbalanced in Keras, is custom **sample\/class** balancing.","28b09491":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | MORE REALISTIC VIEW ON RELATION<\/b><\/p>\n<\/div>\n\n### **MORE COMPLEX REALITY - MORE COMPLETE PICTURE**\n\n- The above picture is quite simple, however in reality there is a bit more going on in the described process\n- We should appreciate **how many dependendies** and **unknowns still exist in the process** mentioned above\n- Thus an approach called **<span style='color:#F55AA2'>deep learning<\/span>** can be used; we will try to extract a model from the data we have, that can explain the complex process to be described below\n\nLet's consider DNA molecules (called chromosomes), as the process of transcription & translation differs slightly for both (have different dependencies):\n\n> - In **<span style='color:#F55AA2'>Bacteria<\/span>** - have **<span style='color:#F55AA2'>small genomes<\/span>**, DNA exists as simple & **free floating molecule**\n> - In **<span style='color:#F55AA2'>Eukaryotes<\/span>** - (amoebas, humans and everything in between) have much **<span style='color:#F55AA2'>larger genomes<\/span>**\n\n### **UNWINDING DNA**\n\n- To fit inside a cell:\n> - Each chromosome must be packed into a very small space\n> - This is accomplished by **winding it around proteins** called; **<span style='color:#F55AA2'>histones<\/span>**\n> - But -> If all the DNA are tightly packed away, how can it be transcribed? Answer: It can't\n\n\n- **<span style='color:#F55AA2'>Before a gene can be transcribed<\/span>**:\n> - The stretch of DNA containing it first **<span style='color:#F55AA2'>must be unwound<\/span>**\n\n\n- How does the cell know **<span style='color:#F55AA2'>which DNA to unwind<\/span>**? \n> - Firstly, the answer is still poorly understood\n> - Secondly, it is believed to involve various types of **<span style='color:#F55AA2'>chemical modifications to the histone molecules<\/span>**\n> - As well as **<span style='color:#F55AA2'>proteins that recognise particular modification to the histone molecules<\/span>**\n> - Clearly, **there is a regulatory mechanism involved**, but many of the details are still unknown\n\n### **CHEMICAL MODIFICATION OF DNA**\n\nDNA itself can be **<span style='color:#F55AA2'>chemically modified<\/span>** through a process; **<span style='color:#F55AA2'>methylation<\/span>**\n\n- The more **<span style='color:#F55AA2'>highly a stretch of DNA is methylated<\/span>**, the **less likely it is to be transcribed**\n- So this is another **regulatory mechanism** the cell can use to **control the production of proteins**\n- But how does it control which regions of DNA are methylated? (Also poorly understood)\n\n### **EVEN MORE FACTORS TO CONSIDER**\n\n- Previously mentioned; **particular stretch of DNA corresponds to a particular protein**; correct for **<span style='color:#F55AA2'>bacteria<\/span>**\n\nHowever for **<span style='color:#F55AA2'>eukaryotes<\/span>**, the situation is more complex\n- After the DNA is transcribed into RNA, the **<span style='color:#F55AA2'>RNA often is edited<\/span>** to remove sections and connect\/splice the remaining parts (**<span style='color:#F55AA2'>exons<\/span>**) back together again\n- The **<span style='color:#F55AA2'>resultant RNA sequence<\/span>** that finally get translated into a protein may be different from the original DNA\n- Additionally, many genes have multiple splice variants - different ways of removing sections to form the final sequence\n- Thus a single stretch of DNA can actually code for several different proteins\n\n### **FUNDAMENTALLY MORE COMPLEX PICTURE OF RNA**\n- Conventional picture; RNA is viewed as **<span style='color:#F55AA2'>just an information carrier<\/span>**\n- The job of **translating mRNA** (tRNA) -> **Proteins** is performed by **<span style='color:#F55AA2'>ribosomes<\/span>**:\n> - Complicated **molecules made partly of proteins & RNA**\n- **<span style='color:#F55AA2'>tRNA<\/span>** | **Another key role in translation** is performed by molecules called **transfer RNAs**\n> - Molecules which define the genetic code recognising patterns of the three bases in mRNA & adding the correct amino acid to the growing protein\n\n\n- Over half a century, we knew that there were at least three types of RNA:\n> - **<span style='color:#F55AA2'>mRNA<\/span>**\n> - **<span style='color:#F55AA2'>ribosomal RNA<\/span>**\n> - **<span style='color:#F55AA2'>tRNA<\/span>**\n\n\n### <b>DISCOVERY OF MORE RNA TYPES<\/b>\n\n- The **DNA** must contain some form of instruction about how to make all of these RNA types\n- In recent times even more types of RNA have been discovered, which affect the process outlined above:\n\n**<span style='color:#F55AA2'>Micro RNA (miRNA)<\/span>**\n> - Short sections of RNA which bind to messenger RNA & **prevent it from being translated into proteins**\n> - This is an important regulatory mechanism in some type of animals, especially mammals\n  \n**<span style='color:#F55AA2'>Short interfering RNA (siRNA)<\/span>**\n> - Another type of RNA that binds to mRNA & **prevents it from being translated**\n> - It is similar to miRNA, however siRNAs are **double stranded** (unlike miRNA) & some functionality is different\n\n**<span style='color:#F55AA2'>Ribozymes<\/span>**\n> - RNA molecules that can act as enzymes to catalyse chemical reactions. \n> - Chemistry is the foundation of everything that happens in a living cell, so catalysts are vital to life.\n> - Usually this job is done by proteins, but we now know it sometimes is done by the RNA.\n\n**<span style='color:#F55AA2'>Riboswitches<\/span>**\n> - RNA molecules that consist of two parts; one part - acts as a messenger RNA, while the other part is capable\n> - of binding to a small molecule. When it binds, it can either enable or prevent translation of the mRNA.\n> - yet another regulatory mechanism by which protein production can be adjusted based on the concentration of particular small molecules in the cell.\n\nSo DNA is more than just a **string of encoded protein sequences**:\n> - It also contains RNA sequences, plus binding sites for TF & other **regulatory molecules**,\n> - Instructions for how messenger RNA should be spliced, plus various **chemical modifications** that influence how it is wound around histones and which genes get transcribed.\n\n### <b>AFTER TRANSLATION COMPLEXITIES<\/b>\n\nConsider what happens **after the ribosome finishes translating** the mRNA into a protein:\n\n- Some proteins can **<span style='color:#F55AA2'>spontaneously fold into the correct 3D shape<\/span>**, but **many require help from other proteins**; **<span style='color:#F55AA2'>chaperpnes<\/span>**\n- It's also very common for proteins to **<span style='color:#F55AA2'>need additional chemical modifications<\/span>** after they are translated\n- Then the **translated protein must be transported to the correct location** in the cell to do its job and finally degrade when it is no longer needed\n- Each of these processes is controlled by **additional regulatory mechanisms** and involves **interactions with lots of other molecules**\n\n### **<span style='color:#F55AA2'>DEEP LEARNING<\/span> FOR COMPLEX PROCESS MODELING**\n- We have huge amounts of **<span style='color:#F55AA2'>raw sequence data<\/span>** & **<span style='color:#F55AA2'>places of attachment<\/span>** in the sequence, generated by a process that's highly complex & not completly understood\n- There must exist so many factors that influence transcription & we want to discover subtle patterns buried in the data, a problem deep learning should excels at, so that is the tool we will be using in this notebool\n\nReference - **[Deep Learning for the Life Sciences](https:\/\/www.oreilly.com\/library\/view\/deep-learning-for\/9781492039822\/)**","3069754e":"# <b>3 <span style='color:#F55AA2'> | <\/span> TRANSCRIPTION FACTORS<\/b>\n\nThis problem focuses on **<span style='color:#F55AA2'>Transcription Factors<\/span>**, so let's outline\/expand on them\n\n### <b>WHAT ARE THEY?<\/b>\n\n- **<span style='color:#F55AA2'>Transcription Factors<\/span>** (TF) are **<span style='color:#F55AA2'>proteins that bind to DNA<\/span>**\n- When they bind, they **influence the probability of nearby genes being** **<span style='color:#F55AA2'>transcribed into RNA<\/span>**\n\n### <b>HOW DOES TF KNOW WHERE TO BIND?<\/b>\n\nLike so much of genomics, we have a simplistic generalisation, followed by lots of complications:\n\n> - To a first approximation, **<span style='color:#F55AA2'>every transcription factor<\/span>** has a specific DNA sequence called its **<span style='color:#F55AA2'>binding site motif<\/span>** that it binds to. \n> - These binding sites tend to be short & usually 10 bases or less\n> - Whenever a transcription factor's motif appears in the genome, it will bind to it\n\n### <b>WHAT HAPPENS IN PRACTICE<\/b>\n\nConsidering the sequence only:\n\n> - Motifs are not completely specific. A TF may be able to **<span style='color:#F55AA2'>bind to many similar but not identical sequences<\/span>** <br>\n> - **<span style='color:#F55AA2'>Some bases within the motif may be more important<\/span>** than others <br>\n> - This is often modeled as a position weight matrix that specifies how much preference the TF has for each possible base at each position within the motif <br>\n\n> - Of course, that assumes every position within the motif is independent, which is not always true <br>\n> - Sometimes even the **<span style='color:#F55AA2'>length of a motif can vary<\/span>** \n> - Although binding is primarily determined by the bases within the motif, the **<span style='color:#F55AA2'>DNA to either side of it can also have some influence<\/span>**\n\nOther aspects of the DNA can also be important:\n\n> - Many TFs are influenced by the **<span style='color:#F55AA2'>physical shape of the DNA<\/span>**, such as how tightly the double helix is twisted <br>\n> - If the **<span style='color:#F55AA2'>DNA is methylated<\/span>**, that can influence TF binding\n> - And remember that most DNA in eukaryotes is tightly packed away, wound around histones\n> - TFs can only bind to the portions that have been unwound\n\n### <b>OTHER DEPENDENCIES<\/b>\n\n> - Other molecules also play important roles. **<span style='color:#F55AA2'>TFs often interact with other molecules<\/span>**, and those **interactions can affect DNA binding**\n> -  For example, a TF may bind to a second molecule to form a complex, and that complex then binds to a different DNA motif than the TF would on its own.\n> - Biologists have spent decades untangling these details and **designing models for TF binding**\n\n### **<span style='color:#F55AA2'>DEEP LEARNING<\/span> & TF BINDING LOCATION PREDICTION**\n\n- As we can see, the process of TF binding is **highly complex** & thus a neural network approach may be suitable for such a task\n- We will simply use the sequence data & **<span style='color:#F55AA2'>create a model for TF binding<\/span>** from the data directly","ea9f5112":"![](https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/8cc1eeaa-4046-4c4a-ae93-93d656f68688\/deu2v16-6addadbf-945b-427b-a8e8-39b12a6c1839.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGV1MnYxNi02YWRkYWRiZi05NDViLTQyN2ItYThlOC0zOWIxMmE2YzE4MzkuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.2dmpyhHG2U812JKfmA0xiWKqgm5axemnBn9nBUoEoOY)\n\n# <b>1 <span style='color:#F55AA2'> | <\/span> INTRODUCTION<\/b>\n\n### **COMPLEX BIOLOGICAL PROCESS**\n\n- In notebook, **[Custom Class | Biological Sequence Operations](https:\/\/www.kaggle.com\/shtrausslearning\/custom-class-biological-sequence-operations)**, a brief & simplistic picture to the relation between **<span style='color:#F55AA2'>DNA<\/span>**,**<span style='color:#F55AA2'>RNA<\/span>** & **<span style='color:#F55AA2'>Proteins<\/span>** was provided.\n- Here, we'll provide almost identical information with the exception of a new term called **<span style='color:#F55AA2'>Transcription Factors<\/span>**, which is relevant to the problem\n- It was noted that the process of converting a **<span style='color:#F55AA2'>nucleotide sequence<\/span>** into an **<span style='color:#F55AA2'>amino acid<\/span>** chain containing **<span style='color:#F55AA2'>proteins<\/span>** is quite uncomplicated and straightforward:\n> - In reality, the relation is **much more complex** as we'll see below as will be described in **<span style='color:#F55AA2'>Section 1.3<\/span>**\n> - Such a complex process is not straightforward to understand, especially for people not up to date with all details surrounding the process **<span style='color:#F55AA2'>transcription<\/span>** & **<span style='color:#F55AA2'>translation<\/span>**\n\n### **ML APPLICATAION**\n\n- What we can do is attempt to utilise deep learning in order to model a relation for our phenomenon associated with the above biological process:\n> - Our model **<span style='color:#F55AA2'>model will attemt to predict places in the DNA<\/span>** at which a so called **<span style='color:#F55AA2'>transcription factor will attach itself<\/span>**\n> - It's quite important to understand where this can occur, as it has a direct relation & impact on **<span style='color:#F55AA2'>protein generation<\/span>**\n\n- When referring to **deep learning**, what we are of course doing:\n> - Inserting lots of data we aren't quite familar with & would like the model to find patterns \n> - In this case do some classification; **<span style='color:#F55AA2'>binary classification<\/span>**\n\n### **NOTEBOOK WORKFLOW**\n\n#### **<span style='color:#F55AA2'>INTRODUCTION<\/span>**\n> - **<span style='color:#F55AA2'>DNA,RNA & PROTEINS RELATION<\/span>** - Introducing the background, what field does this problem relate to\n> - **<span style='color:#F55AA2'>MORE REALISTIC VIEW ON RELATION<\/span>** - In reality, lots of factors affect what we want to model, perhaps too complex? If so, should we try deep learning?\n> - **<span style='color:#F55AA2'>TRANSCRIPTION FACTORS<\/span>** - Specifically the proteins, whose behaviour we are intersted in modeling since they are one of the critical influensors in the process of **transcription** & hence **translation**\n\n#### **<span style='color:#F55AA2'>EXPERIMENTAL DATA<\/span>**\n- There probably isn't much to explore in the data itself, the DNA sequence has been preprocessed via **[OHE](https:\/\/en.wikipedia.org\/wiki\/One-hot)** & split into **training**, **validation**, **test** sets and uploaded as a dataset\n- I've added additional functions that can be used on a DNA sequence, so that it can be prepared in the same way as the data used here:\n> - Described in **<span style='color:#F55AA2'>Section 2.2<\/span>** and function added in **<span style='color:#F55AA2'>Section 2.4<\/span>**; <code>dnaseq_features<\/code>\n> - For custom splitting of data, <code>np.save<\/code> can be used on <code>data_all_X<\/code>, ...","8b1069b2":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.2 | METRICS, LOSS FUNCTION & OPTIMISER<\/b><\/p>\n<\/div>\n\n- As with any neural network, we'll need to set an evalution metric we'll monitor during training, a loss function & optimiser.\n- We'll be using the same settings for all models, so let's define it first.\n\n#### **<span style='color:#F55AA2'>EVALUATION METRIC<\/span>**\n\n> - Having defined our neural network, we should pay attention to the **metrics we use to assess our model**\n> - In classification, it's quite standard to look at **<span style='color:#F55AA2'>ROC<\/span>** curves to assess the true positive rate & false positive rate\n> - A single metric, **<span style='color:#F55AA2'>AUC<\/span>** can be used to access how much the curve hugs to top left sides of the graph, an indicator of the area under the curve & having a max value of 1\n\n#### **<span style='color:#F55AA2'>LOSS FUNCTION<\/span>**\n> - For the loss function, let's use **<span style='color:#F55AA2'>binary_crossentropy<\/span>**, which is commonly used for binary classification\n\n#### **<span style='color:#F55AA2'>OPTIMISER<\/span>**\n> - For the optimiser, we'll use **<span style='color:#F55AA2'>stochastic gradient descent<\/span>** (<code>'sgd'<\/code>), which is commonly used as well","93380de2":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b> [OPTIONAL] REASSEMBLING MATRIX<\/b><\/p>\n<\/div>\n\n- If you want to split the data in a slightly different way, the split data can of course easily be reassembled & resplit using for example; **[train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)**","e3226b3b":"# <b>6 <span style='color:#F55AA2'> | <\/span> BASELINE INVESTIGATIVE MODELS <\/b>\n\nThere are two interesting factors that potentially affect our model, as outlined above:\n> - Model 1 - The effect of <code>.Dropout<\/code> (Dropout removed)\n> - Model 2 - The effect **Sample Weighting** (No sample weighting applied)\n\n- For the first test (**<span style='color:#F55AA2'>model 1<\/span>**);\n> The settings are the same as the above baseline model model, except we'll use <code>drop_id<\/code> = **False** & **weighting** of samples is identical to the baseline model as well\n- For the second test (**<span style='color:#F55AA2'>model 2<\/span>**);\n> The settings are the same as the above baseline model model, except we'll use <code>drop_id<\/code> = **True** & there is not **weighting** of samples this time\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.1 | NO DROPOUT LAYER MODEL<\/b><\/p>\n<\/div>\n\n- The first being <code>.dropout<\/code>, which is quite commonly used in the construction of a neural network to **prevent overfitting**\n- Let's see what the actual effect on accuracy is, if we remove the dropout layers, a model that overfits should have a much higher evaluation metric on training data than on evaluation data\n- <code>plot_keras_metric<\/code> also displays the difference between the training & evaluation metric (**metric diff.**), so it's easier to see the evolution overfitting as we train the model","e126c0c8":"# <b>5 <span style='color:#F55AA2'> | <\/span> DEFINING BASELINE <span style='color:#F55AA2'>TF<\/span> BINDING MODEL<\/b>\n\n- We will aim to train our model on the <b>training data<\/b> & use the <b>validation data<\/b> to validate the model as we train it\n- Additionally, an inference on the subset <b>test data<\/b> will be done using <code>.evaluate<\/code>, to see how well the model actually **<span style='color:#F55AA2'>adapts to unseen data<\/span>**\n- First of all, let's build a **<span style='color:#F55AA2'>binary classifier<\/span>**, as we are interested in only a two way outcome; **a binding site** or **not a binding site**\n\n### **WHAT WE NEED FROM THE MODEL**\n- As we saw above, we have an **<span style='color:#F55AA2'>unbalanced target distribution<\/span>** & for that reason, we should assume **sample balancing** would be beneficial\n- Also, as with most neural networks, we should **<span style='color:#F55AA2'>refrain from overfitting our data<\/span>** unless we are certain data we will test our model on follows the exact same tendencies as the training data\n- However as the sequence alone doesn't encode all the factors that can actually affect our result, so we'll need to strike a balance.\n\n### **BASELINE MODEL & ITS VARIATION**\n\nThere are a few things we ought to look at (investigative models):\n- **<span style='color:#F55AA2'>Model 1<\/span>** - The effect of <code>.Dropout<\/code> (Dropout removed) <br>\n- **<span style='color:#F55AA2'>Model 2<\/span>** - The effect **Sample Weighting** (No sample weighting applied) <br>\n\n<b>Model 1 & 2<\/b> are looked at first, to understand how critical the **overfitting** & **weighting** is in this problem, then we can confirm if our precautions were justified in **Model 3**\n\n- **<span style='color:#F55AA2'>Model 3<\/span>** - Standard baseline model for Transcription Factor Binding Prediction<\/mark>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.1 | MODEL SETUP<\/b><\/p>\n<\/div>\n\n- Our model won't be too complicated, we will use a **<span style='color:#F55AA2'>Convolution Neural Network (CNN)<\/span>**, <code>Conv1D<\/code>, the subclass of the popular <code>Conv2D<\/code>, as our data is sequential.\n- One of the requirements of our model is that we definitely **don't want to overfit our training data** & the addition of <code>.Dropout<\/code> layers in the model is one approach we can take to prevent this\n- We probably will end up seeing lots of lots of oscillation during training because of these layers","922312b0":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.6 | TARGET & SAMPLE WEIGHTS RELATION<\/b><\/p>\n<\/div>\n\n- In Keras, we have the option to add <b><span style='color:#F55AA2'>custom sample balance<\/span><\/b> using <code>sample_weight<\/code> for **training** & <code>validation_data<\/code> (X,y,weight) for **validation**\n- We can define any weighting we like for **True positive** samples & from the figure below we can see that larger weights are applied to the minority class.","07f33995":"# <b>4 <span style='color:#F55AA2'> | <\/span> EXPERIMENTAL DATA<\/b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | DESCRIPTION<\/b><\/p>\n<\/div>\n\n- **<span style='color:#F55AA2'>The Transcription Factor<\/span>** we are looking into: **<span style='color:#F55AA2'>JUND<\/span>** | [ebi.ac.uk](https:\/\/www.ebi.ac.uk\/interpro\/entry\/InterPro\/IPR029823\/)\n\n> Transcription factor JunD is a member of the transcription factor activator protein (AP)-1 family, comprising Jun (c-Jun, JunB, and JunD), Fos (c-Fos, FosB, Fra1, and Fra2), ATF (ATFa, ATF-2 and ATF-3) and JDP (JDP-1 and JDP-2) family members [1]. They are basic leucine zipper transcription factors that play a central role in regulating gene transcription in various biological processes [2]. This entry also includes transcription factor AP-1 (Jra) from Drosophila, which recognizes and binds to the enhancer heptamer motif 5'-TGA[CG]TCA-3' and has a role in dorsal closure [cite:PUB00083247],[cite:PUB00083248],[cite:PUB00083249] .\n\n- An experiment was done to **<span style='color:#F55AA2'>identify every place in the human genome where it binds<\/span>**, we store this data in the the **target variable**\n- To keep things managable, only sequence data from **<span style='color:#F55AA2'>chomosome 22<\/span>** (one of the smallest) is included in the data (50 million **bases\/characters** long)\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | GENERATING FEATURE MATRIX FOR MODEL<\/b><\/p>\n<\/div>\n\n### <b>SPLITTING GENOME INTO SUBSEQUENCES<\/b>\n- The **<span style='color:#F55AA2'>full chomosome<\/span>** split up into **<span style='color:#F55AA2'>segments<\/span>**, each **101 bases\/characters long**\n- Each segment subsequently representing an addition to the index in the feature matrix as we divide our entire sequence.\n- Each segment has been labelled to indicate whether it does or doesnt include a <b><span style='color:#F55AA2'>sites where JUND bind<\/span><\/b> (<b><span style='color:#F55AA2'>obtained experimentally<\/span><\/b>) \n\nBelow is an example sequnce which is split into 101 bases & each **DNA segment** (cut by <b><span style='color:#F55AA2'>|<\/span><\/b>) contains a combination of four bases (**ACGT**)  \n\n>...\n>**<span style='color:#F55AA2'>|<\/span>**ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGTAGATCTGTTCTCTAAA\n>CGAACTTTAAAATCTGTGTGGCTGTCACTC**<span style='color:#F55AA2'>|<\/span>**GGCTGCATGCTTAGTGCACTCACGCAGTATAATTAATAAC\n>TAATTACTGTCGTTGACAGGACACGAGTAACTCGTCTATCTTCTGCAGGCTGCTTACGGT**<span style='color:#F55AA2'>|<\/span>**TTCGTCCGTG\n>TTGCAGCCGATCATCAGCACATCTAGGTTTCGTCCGGGTGTGACCGAAAGGTAAGATGGAGAGCCTTGTC\n>CCTGGTTTCAACGAGAAAA**<span style='color:#F55AA2'>|<\/span>**CACACGTCCAACTCAGTTTGCCTGTTTTACAGGTTCGCGACGTGCTCGTAC\n>GTGGCTTTGGAGACTCCGTGGAGGAGGTCTTATCAGAGGCACGTCAACA**<span style='color:#F55AA2'>|<\/span>**TCTTAAAGATGGCACTTGTGG\n>CTTAGTAGAAGTTGAAAAAGGCGTTTTGCCTCAACTTGAACAGCCCTATGTGTTCATCAAACGTTCGGAT\n>GCTCGAAC**<span style='color:#F55AA2'>|<\/span>**TGCACCTCATGGTCATGTTATGGTTGAGCTGGTAGCAGAACTCGAAGGCATTCAGTACGGTC\n>GTAGTGGTGAGACACTTGGTGTCCTTGTCCCTCATGTG**<span style='color:#F55AA2'>|<\/span>**GGCGAAATACCAGTGGCTTACCGCAAGGTTCT\n>TCTTCGTAAGAACGGTAATAAAGGAGCTGGTGGCCATAGTTACGGCGCCGATCTAAAGTCATTTGACT**<span style='color:#F55AA2'>|<\/span>**TA\n>...\n\n### <b>CREATING FEATURES FOR EACH SAMPLE<\/b>\n\n- **Sample features are represented with OHE**; for each base, we have 4 numbers, one of which is set to 1 & rest 0:\n> - With one of the four numbers is set to 1 indicates whether the base is an **<span style='color:#F55AA2'>A<\/span>**,**<span style='color:#F55AA2'>C<\/span>**,**<span style='color:#F55AA2'>G<\/span>** or **<span style='color:#F55AA2'>T<\/span>** (so we have (X,4))\n> - Eg. **Segment\/Sample 1** -> Sequence made of 6 **bases**(characters) : **ACTGTG** -> (1,0,0,0,0,0),(0,1,0,0,0,0),(0,0,1,0,1,0),(0,0,0,1,0,1)\n- For **each sample**, we subsequently have a **feature vector** of size (101,4) **(number of bases**,**OHE of each base)**\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.3 | TARGET & SAMPLE WEIGHTS<\/b><\/p>\n<\/div>\n\n- Description information about our target variable, as well as an additional <b>weighting component<\/b> we can use for both training\/validation.\n- Keras allows us to give a weighting to either all <b><span style='color:#F55AA2'>samples<\/span><\/b> or <b><span style='color:#F55AA2'>classes<\/span><\/b> during training, this is of course useful if our data is imbalanced.\n\n> - **<span style='color:#6936F2'>SAMPLE LABELS<\/span>** - Single number for the label (<b><span style='color:#F55AA2'>0 - doesn't contain<\/span><\/b>, <b><span style='color:#F55AA2'>1 - contains binding site<\/span><\/b>)\n> - **<span style='color:#6936F2'>SAMPLE WEIGHT<\/span>** - Single number for the <b><span style='color:#F55AA2'>weights of each sample<\/span><\/b> (sequence section)\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.4 | READING PREPROCESSED DATA<\/b><\/p>\n<\/div>\n\n- For convenience the data has already been preprocessed (by the exp authors) and stored in <code>.npy<\/code> format\n- We can use <code>read_data<\/code> to read our data or rearrange it in any way after it has been read as you can add more methods to the class.\n- For this problem, we'll use a standard **train\/test** strategy for validation as we'll be using neural networks & this is a demonstration only.","80cbe5b5":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>[OPTIONAL] SEQUENCE TO FEATURE MATRIX<\/b><\/p>\n<\/div>\n\n- Given a full **<span style='color:#F55AA2'>nucleotide sequence<\/span>**, we wish to split into segments and One-Hot-Encode. \n- We can use use function <code>dnaseq_features<\/code> to obtain the necessary feature matrix and use it in the same format as the data read in this problem","228b2a7f":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.2 | NO SAMPLE WEIGHTING MODEL<\/b><\/p>\n<\/div>\n\n- Lastly, let's check the effect of not including **sample weighting**, so <code>sample_weight<\/code> is not used","f66291ee":"# <b>7 <span style='color:#F55AA2'> | <\/span> TRAINING BASELINE <span style='color:#F55AA2'>TF<\/span> BINDING MODEL<\/b>\n\n### <b>PREPARING & TRAINING THE MODEL<\/b>\n- As we saw from the results in the previous two explatory models, if we don't add **dropout** and **weighting**, our results seem very fair at best\n- Let's check how well the standard transcription factor model model performs, taking into cosideration both of the these seemingly necessary parts of the model  ","56722f73":"# <b>2 <span style='color:#F55AA2'> | <\/span> DNA,RNA & PROTEIN RELATION<\/b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | SIMPLIFIED RELATION<\/b><\/p>\n<\/div>\n\n### **BASIC TERMS USED**\n\n- **[PROTEINS](https:\/\/learn.genetics.utah.edu\/content\/basics\/proteins\/)** | Do almost all the work in a cell & are polymers and made up of **amino acids**\n- **[AMINO ACIDS](https:\/\/www.sciencedirect.com\/topics\/materials-science\/amino-acids)** | There are 20 main amino acids (IUPAC) which have a wide range of properties & functions\n- **[DNA](https:\/\/medlineplus.gov\/genetics\/understanding\/basics\/dna\/)** | One of the main functions of a DNA is to record the sequence of amino acids for an organisms proteins:\n> - Particular stretches of DNA directly correspond to particular proteins <br>\n> - Each sequence of **three DNA bases (codon)** corresponds to an **amino acid**\n- **[RNA](https:\/\/www.genome.gov\/genetics-glossary\/RNA-Ribonucleic-Acid)** | Going from **<span style='color:#F55AA2'>DNA -> Proteins<\/span>** involves another molecule:\n> - **<span style='color:#F55AA2'>RNA<\/span>** | Yet another polymer and is chemically similar to DNA. It has four bases that can be chaned together in arbitrary order\n> - **RNA**, serves as an intermediate reprentation to **carry information from one part of the cell to another** <br>\n\n### **CREATING A PROTEIN**\n> - **[STEP1]** Firstly, the DNA sequence is <mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.9\">transcribed<\/mark> into an equivalent RNA sequence\n> - **[STEP2]** Secondly, the RNA molecule is <mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:0.9\">translated<\/mark> into a protein molecule <br>\n- The **RNA** molecule that carries information is called a messenger **<span style='color:#F55AA2'>RNA (mRNA)<\/span>**\n\n### **PROTEIN FORMATION** : **WHEN ARE THEY MADE?**\n\nThe above tells us how a protein is made, but not when & where. A human cell has many thousands of different proteins it can make\n\n- There must exist a **regulatory mechanism** to control **which proteins are made** & **when**, it can't be random\/all the time.\n- In the conventional picture, this is done by **special proteins**; **<span style='color:#F55AA2'>transcription factors (TF)<\/span>**\n- Each TF **<span style='color:#F55AA2'>recognises<\/span>** & **<span style='color:#F55AA2'>binds to a particular DNA sequence<\/span>**\n- Depending on the type of TF & the location where it binds:\n> It can either increase or decrease the **<span style='color:#F55AA2'>rate at which nearby genes are transcribed<\/span>**\n\n### **SIMPLE PICTURE TO BEGIN WITH**\n\nSo we have a very straightforward and simple generalisation:\n\n- The job of the DNA is to **<span style='color:#F55AA2'>encode proteins<\/span>** \n- Stretches of DNA (genes) code for proteins using a simple & well defined code\n- DNA is converted to RNA (which **serves only as an information carrier**) \n- RNA is then **<span style='color:#F55AA2'>converted into proteins<\/span>**  "}}