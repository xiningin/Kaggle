{"cell_type":{"76aa6bf7":"code","de2a2342":"code","9c474df7":"code","a86e3352":"code","45c67e33":"code","bf4e7e2d":"code","ab75de26":"code","e37f2452":"code","b23451c4":"code","d8f92a53":"code","99595a81":"code","041fb7a3":"code","b4aa9a0d":"code","41261275":"code","d1f727f6":"code","5cc65bc8":"code","87cbdd55":"code","a9b28953":"markdown"},"source":{"76aa6bf7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\n%matplotlib inline\npd.set_option(\"display.max_rows\",None)","de2a2342":"src = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00291\/airfoil_self_noise.dat\",sep=\"\\t\",header=None,\n                 names=[\"frequency\",\"angle_of_attack\",\"chord_length\",\"fs_velocity\",\"disp_thickness\",\"sound_pressure\"])\nsrc.head()","9c474df7":"src.describe()","a86e3352":"src.isnull().sum() # Checking for missing values","45c67e33":"src.plot(kind=\"box\",figsize=(15,5)) # Plotting box plot for finding outliers","bf4e7e2d":"# Function to find outliers using IQR method \/ Box-whisker method\ndef find_outliers(x):\n    q1 = x.quantile(0.25)\n    q3 = x.quantile(0.75)\n    iqr = q3 - q1\n    minimum = q1 - (1.5 * iqr)\n    maximum = q3 + (1.5 * iqr)\n    return x[(x < minimum) | (x > maximum)]","ab75de26":"#Dropping the outliers sice it is a quick and dirty way to bulid an ANN.\n#Dropping the outliers is not the ideal way to do in practice.\nfor j in range(20):\n    for col in src.columns:\n        src.loc[src[col].isin(find_outliers(src[col])), col] = np.nan\n    src.dropna(inplace=True)\n\n#Box plot shows there are no outliers left\nsrc.plot(kind=\"box\",figsize=(15,5))","e37f2452":"# Forming input and output arrays\nX = src.iloc[:,:-1].values\ny = src.iloc[:,-1].values","b23451c4":"#Train, test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=11)\n\nprint(\"Dataset shape: \"+str(src.shape))\nprint(\"Training set shape: \"+str(X_train.shape))\nprint(\"Test set shape: \"+str(X_test.shape))","d8f92a53":"#It is prudent to apply the transforms separately on training and testing sets\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)","99595a81":"np.random.seed(11) #setting seed to ensure model reproducibility\ntf.random.set_seed(11)\n#Building the ANN with some random structure\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=8,activation=\"relu\")) # Hidden Layer 1 with 8 neurons and ReLU activation\nann.add(tf.keras.layers.Dense(units=8,activation=\"relu\")) # Hidden Layer 2 with 8 neurons and ReLU activation\nann.add(tf.keras.layers.Dense(units=1)) # output layer\n#since it is a regression problem, we need not use activation function in output layer\n\nann.compile(optimizer=\"adam\",loss=\"mean_squared_error\") #using MSE as loss function\n\n#Fitting the model\nmodel_log = ann.fit(X_train,y_train,epochs=500,batch_size=32) #model log helps to track the steps in model fitting process","041fb7a3":"#Plotting the loss function during the fitting process\npd.Series(model_log.history[\"loss\"]).plot()\nplt.title(\"Plotting the loss function per epoch\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")","b4aa9a0d":"#Applying scaler to X_test\nnp.random.seed(11) #setting seed to ensure model reproducibility\ntf.random.set_seed(11)\nX_test = scaler.transform(X_test)\npred = ann.predict(X_test,batch_size=32)\npred = pred.flatten()\npred_df = pd.DataFrame({\"Actual\":y_test,\"Prediction\":pred}) # dataframe showing actual vs predictions\npred_df","41261275":"pred_df.plot(kind=\"kde\")","d1f727f6":"mean_squared_error(y_test,pred) #mean squared error of model on test set. Can alsp use model.evaluate()","5cc65bc8":"np.random.seed(11) #setting seed to ensure model reproducibility\ntf.random.set_seed(11)\nmean_squared_error(y_train,ann.predict(X_train)) #mean squared error of model on training set ","87cbdd55":"#Since the error on training set and test set is close. It ensures the model is not overfit.\n#This model was built in less than an hour. The model accuracy can be improved by tuning the hyperparameters and trying different model architectures","a9b28953":"<h1>Implementation of Artificial Neural Network for Regression<\/h1>\n<p>Data sourced from UCI Machine Learning Repository<\/p>"}}