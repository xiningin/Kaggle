{"cell_type":{"05d72ebe":"code","8d3fb2f7":"code","6643fa42":"code","4e4ef4a5":"code","59cc3348":"code","bce166e9":"code","e17e15ed":"markdown","c4c50b9b":"markdown","6c392224":"markdown","0528456b":"markdown","25eadaf4":"markdown","6794be7b":"markdown"},"source":{"05d72ebe":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# code from https:\/\/github.com\/KellerJordan\/ResNet-PyTorch-CIFAR10\/blob\/master\/model.py\nclass IdentityPadding(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super(IdentityPadding, self).__init__()\n        \n        self.pooling = nn.MaxPool2d(1, stride=stride)\n        self.add_channels = out_channels - in_channels\n    \n    def forward(self, x):\n        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n        out = self.pooling(out)\n        return out\n    \n    \nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n        super(ResidualBlock, self).__init__()\n        self.down_sample = down_sample\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                               stride=stride, padding=1, bias=False) \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n                               stride=1, padding=1, bias=False) \n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.stride = stride\n        \n        self.dropout = nn.Dropout(0.2)\n        \n        if down_sample:\n            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n        else:\n            self.down_sample = None\n\n\n    def forward(self, x):\n        shortcut = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.dropout(out)\n\n        if self.down_sample is not None:\n            shortcut = self.down_sample(x)\n\n        out += shortcut\n        out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, num_layers, block, num_classes=7):\n        super(ResNet, self).__init__()\n        self.num_layers = num_layers\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # feature map size = 112x112x16\n        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n        # feature map size = 56x56x32\n        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n        # feature map size = 28x28x64\n        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n\n        # output layers\n        self.avg_pool = nn.MaxPool2d(28, stride=1)\n        self.fc_out = nn.Linear(64, num_classes)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def get_layers(self, block, in_channels, out_channels, stride):\n        if stride == 2:\n            down_sample = True\n        else:\n            down_sample = False\n        \n        layers_list = nn.ModuleList(\n            [block(in_channels, out_channels, stride, down_sample)])\n            \n        for _ in range(self.num_layers - 1):\n            layers_list.append(block(out_channels, out_channels))\n\n        return nn.Sequential(*layers_list)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layers_2n(x)\n        x = self.layers_4n(x)\n        x = self.layers_6n(x)\n        \n        x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_out(x)\n        return x\n\n\ndef resnet():\n    block = ResidualBlock\n    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n    model = ResNet(3, block) \n    return model\n","8d3fb2f7":"from torch.hub import load_state_dict_from_url\nfrom torchvision.models import ResNet\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel \/\/ reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel \/\/ reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None,\n                 *, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n        \n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n        \n        out = self.dropout(out)\n        \n        \n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n            \n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None,\n                 *, reduction=16):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes * 4, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\n        \n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.se(out)\n        \n\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef se_resnet18(num_classes=7):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet34(num_classes=7):\n    \"\"\"Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet50(num_classes=7, pretrained=False):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    if pretrained:\n        model.load_state_dict(load_state_dict_from_url(\n            \"https:\/\/github.com\/moskomule\/senet.pytorch\/releases\/download\/archive\/seresnet50-60a8950a85b2b.pkl\"))\n    return model\n\n\ndef se_resnet101(num_classes=7):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet152(num_classes=7):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n","6643fa42":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport argparse\nfrom tensorboardX import SummaryWriter\n","4e4ef4a5":"import pickle\nimport random\nfrom PIL import Image\n\nprint('==> Preparing data..')\n\ncropped_image_list = []\nlabel_list = []\nimg_path = '\/kaggle\/input\/bts-crop2\/cropped\/'\n\nwith open('\/kaggle\/input\/bts-crop2\/result.pickle', 'rb') as f:\n    data = pickle.load(f)\n    \ntransforms_train = transforms.Compose([\n    transforms.RandomCrop(112, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\ntransforms_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\n#number of train set and test set\nnum_data=len(data.index)\nnum_test_data = int(num_data\/10)\nnum_train_data = num_data - num_test_data\n\nclasses = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\nclass_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n  \nfor i in data.index:\n    img = Image.open(img_path+data[0][i])\n    cropped_image_list.append(img)\n    \n    label = class_dict[data[1][i]]\n    label_list.append(label)\n    \n\n#dataset making by random select\ndataset_train = []\ndataset_test = []\n\n#random select test data index\nrand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n\n# class dataset() and __init__() looks good for calling and managing self variables\n# but This time, tried to make simple without class\n\ncount_test=0\ncount_train=0\nfor i in np.arange(num_data):\n    if i in rand:\n        img = transforms_test(cropped_image_list[i])\n        dataset_test.append([img,label_list[i]])\n    else:\n        img = transforms_train(cropped_image_list[i])\n        dataset_train.append([img,label_list[i]])\n        \n#dataset_test = np.asarray(dataset_test)\n#dataset_train = np.asarray(dataset_train)\n","59cc3348":"class FocalLoss(nn.Module):\n\n    def __init__(self, gamma=0):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.ce = torch.nn.CrossEntropyLoss()\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        loss = (1 - p) ** self.gamma * logp\n        return loss.mean()","bce166e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nexp_id=0\nlr = 0.1\nbatch_size = 16\nbatch_size_test=2\nnum_worker=1\nresume = None\nlogdir = '\/kaggle\/output\/'\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size, \n                          shuffle=True, num_workers=num_worker)\ntest_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n                         shuffle=False, num_workers=num_worker)\n\n# bts class\nclasses = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n\nprint('==> Making model..')\n\n#net = resnet()\nnet = se_resnet18()\nnet = net.to(device)\nnum_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\nprint('The number of parameters of model is', num_params)\n# print(net)\n\nif resume is not None:\n    checkpoint = torch.load('.\/save_model\/' + str(exp_id))\n    net.load_state_dict(checkpoint['net'])\n\n#criterion = nn.CrossEntropyLoss()\ncriterion = FocalLoss(gamma=2.0).to(device)\n#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\noptimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n\ndecay_epoch = [2400, 3600]\nstep_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n                                 milestones=decay_epoch, gamma=0.1)\nwriter = SummaryWriter(logdir)\n\n\ndef train(epoch, global_steps):\n    net.train()\n\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n        global_steps += 1\n        step_lr_scheduler.step()\n        inputs = inputs.to(device)\n        \n        targets = targets.to(device)\n        outputs = net(inputs)\n        \n        loss = criterion(outputs, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n        \n    acc = 100 * correct \/ total\n    print('train epoch : {} [{}\/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n           epoch, batch_idx, len(train_loader), train_loss\/(batch_idx+1), acc))\n\n    writer.add_scalar('log\/train error', 100 - acc, global_steps)\n    return global_steps\n\n\ndef test(epoch, best_acc, global_steps):\n    net.eval()\n\n    test_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            \n            \n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    acc = 100 * correct \/ total\n    print('test epoch : {} [{}\/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n           epoch, batch_idx, len(test_loader), test_loss\/(batch_idx+1), acc))\n\n    writer.add_scalar('log\/test error', 100 - acc, global_steps)\n    \n    if acc > best_acc:\n        print('==> Saving model..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('save_model'):\n            os.mkdir('save_model')\n        torch.save(state, '.\/save_model\/ckpt.pth')\n        best_acc = acc\n\n    return best_acc\n\n\nif __name__=='__main__':\n    best_acc = 0\n    epoch = 0\n    global_steps = 0\n    \n    if resume is not None:\n        test(epoch=0, best_acc=0)\n    else:\n        while True:\n            epoch += 1\n            global_steps = train(epoch, global_steps)\n            best_acc = test(epoch, best_acc, global_steps)\n            print('best test accuracy is ', best_acc)\n            \n            if global_steps >= 4800:\n                break\n\n\n\n# Any results you write to the current directory are saved as output.","e17e15ed":"SE Resnet","c4c50b9b":"Import dataset","6c392224":"**Import files**","0528456b":"Focal loss\nhttps:\/\/github.com\/foamliu\/InsightFace-v2\/blob\/master\/focal_loss.py","25eadaf4":"from https:\/\/github.com\/moskomule\/senet.pytorch\/blob\/master\/senet\/se_resnet.py","6794be7b":"Set resnet models"}}