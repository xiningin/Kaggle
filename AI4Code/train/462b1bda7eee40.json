{"cell_type":{"4cbc2074":"code","15577774":"code","3a788462":"code","853b5bd7":"code","aa8eed0f":"code","795703fb":"code","f8537ea8":"code","2143f356":"code","9d8ae494":"code","4317e6fd":"code","475d58f0":"code","55dcbea3":"code","9d951204":"code","101cc56d":"code","c63aa114":"code","b007ce3c":"code","f6800300":"code","f854977a":"code","034c9e1e":"code","4c63df8e":"code","7d0113c1":"code","a25d8f45":"code","39e04a53":"code","36e1ac28":"code","6a5d8a5d":"code","dcb5b1e7":"code","17cb8ee4":"code","6323ab8d":"code","e0b09d45":"code","4ba24f50":"code","270d5c5c":"code","9a1499fb":"markdown","0fc04ff3":"markdown","2a7d2372":"markdown","aae30575":"markdown","5182a033":"markdown","3c810b21":"markdown","2fa1947b":"markdown","3820fc22":"markdown","9403187e":"markdown","8fa96f22":"markdown","d059f93a":"markdown","743df761":"markdown","5fa4bbba":"markdown","494b876f":"markdown","58de2d7a":"markdown","900a22e5":"markdown","34a16e74":"markdown","77901be7":"markdown","e85aec29":"markdown","ff4f0625":"markdown","8cde7696":"markdown","e6b324a6":"markdown","e9ab7cf9":"markdown","1ef3c6de":"markdown","9da2775b":"markdown","91a2f2d1":"markdown","b136ae05":"markdown","be1eeea4":"markdown","d1457d42":"markdown","e1643f4a":"markdown"},"source":{"4cbc2074":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","15577774":"# !pip install chart_studio\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/ import string\nfrom pandas_profiling import ProfileReport\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec \nfrom gensim.models import KeyedVectors \nimport matplotlib.pyplot as plt\nimport pickle\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport seaborn as sns\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom plotly import tools\n# import chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom collections import Counter # suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"ticks\", color_codes=True)\n","3a788462":"BASE_PATH = '..\/input\/tweet-sentiment-extraction\/'\n\ntrain_df = pd.read_csv(BASE_PATH + 'train.csv')\ntest_df = pd.read_csv( BASE_PATH + 'test.csv')\nsubmission_df = pd.read_csv( BASE_PATH + 'sample_submission.csv')","853b5bd7":"print(\"Number of data points in train data frame\", train_df.shape)\nprint(\"The attributes of train data :\", train_df.columns.values)\nprint('-'*50)\nprint(\"Number of data points in test data frame\", train_df.shape)\nprint(\"The attributes of test data :\", test_df.columns.values)","aa8eed0f":"train_profile = ProfileReport(train_df, title='Train Data Profiling Report', html={'style':{'full_width':True}})","795703fb":"train_profile.to_file(output_file=\"train_profile.html\")\ntrain_profile.to_notebook_iframe()","f8537ea8":"test_profile = ProfileReport(test_df, title='Test Data Profiling Report', html={'style':{'full_width':True}})","2143f356":"test_profile.to_file(output_file=\"test_profile.html\")\ntest_profile.to_notebook_iframe()","9d8ae494":"def jaccard_similarity(text1, text2):\n    intersection = set(text1).intersection(set(text2))\n    union = set(text1).union(set(text2))\n    return len(intersection)\/len(union)","4317e6fd":"str1 = 'President greets the press in Chicago'\nstr2 = 'Obama speaks in Illinois'","475d58f0":"jaccard_similarity(str1, str2)","55dcbea3":"nltk.jaccard_distance(set(str1), set(str2))","9d951204":"1 - nltk.jaccard_distance(set(str1), set(str2))","101cc56d":"train_df.sentiment.value_counts()","c63aa114":"sns.catplot(x=\"sentiment\", kind=\"count\", palette=\"ch:.25\", data=train_df);","b007ce3c":"test_df.sentiment.value_counts()","f6800300":"sns.catplot(x=\"sentiment\", kind=\"count\", palette=\"ch:.25\", data=test_df);","f854977a":"# https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python\n\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    \n    # Create a word cloud image\n    wordcloud = WordCloud(background_color=color,\n                   stopwords = stopwords,\n                   max_words = max_words,\n                   max_font_size = max_font_size,\n                   random_state = 42,\n                   mask=mask,\n                   width=200,\n                   height=100,\n                   contour_width=2, \n                   contour_color='firebrick')\n    \n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  ","034c9e1e":"train_df = train_df.dropna()\n\nneutral_text = train_df.loc[train_df['sentiment'] == 'neutral', 'text'].append(test_df.loc[test_df['sentiment'] == 'neutral'])\npositive_text = train_df.loc[train_df['sentiment'] == 'positive', 'text'].append(test_df.loc[test_df['sentiment'] == 'positive'])\nnegative_text = train_df.loc[train_df['sentiment'] == 'negative', 'text'].append(test_df.loc[test_df['sentiment'] == 'negative'])\n","4c63df8e":"## util to create masked image compatible for WordCloud\nwine_mask = np.array(Image.open(\"..\/input\/wine-mask\/wine_mask.png\"))\n\ndef transform_format(val):\n    if val == 0:\n        return 255\n    else:\n        return val\n    \n# Transform your mask into a new one that will work with the function:\ntransformed_wine_mask = np.ndarray((wine_mask.shape[0],wine_mask.shape[1]), np.int32)\n\nfor i in range(len(wine_mask)):\n    transformed_wine_mask[i] = list(map(transform_format, wine_mask[i]))","7d0113c1":"plot_wordcloud(neutral_text, transformed_wine_mask, max_words=1000, max_font_size=120, title = 'Word Cloud of Neutral tweets', title_size=50)","a25d8f45":"plot_wordcloud(positive_text,transformed_wine_mask, max_words=1000, max_font_size=100, \n               title = 'Word Cloud of Positive tweets', title_size=50)","39e04a53":"plot_wordcloud(negative_text,transformed_wine_mask, max_words=1000, max_font_size=100, \n               title = 'Word Cloud of Negative tweets', title_size=50)","36e1ac28":"def plot_text_features(data):\n    \n    fig = go.Figure()\n    for val in data:\n        fig.add_trace(go.Histogram(x=val['x'],name = val['label']))\n\n    # Overlay both histograms\n    fig.update_layout(barmode='stack')\n    # Reduce opacity to see both histograms\n    fig.update_traces(opacity=0.75)\n    fig.show()\n    ","6a5d8a5d":"train_num_words = train_df['text'].apply(lambda x: len(str(x).split(' ')))\ntest_num_words = test_df['text'].apply(lambda x: len(str(x).split(' ')))\nselected_text_num_words = train_df['selected_text'].apply(lambda x: len(str(x).split(' ')))\n\n\ndata_num_words = [\n    {'x': train_num_words, 'label': 'Num of words in text of train data'},\n    {'x': test_num_words, 'label': 'Num of words in text of test data'},\n    {'x': selected_text_num_words, 'label': 'Num of words in selected text'},\n]\n\nplot_text_features(data_num_words)","dcb5b1e7":"train_num_chars = train_df['text'].apply(lambda x: len(x))\ntest_num_chars = test_df['text'].apply(lambda x: len(x))\nselected_text_num_chars = train_df['selected_text'].apply(lambda x: len(x))\n\n\ndata_num_chars = [\n    {'x': train_num_chars, 'label': 'Num of chars in text of train data'},\n    {'x': test_num_chars, 'label': 'Num of chars in text of test data'},\n    {'x': selected_text_num_chars, 'label': 'Num of chars in selected text'},\n]\n\nplot_text_features(data_num_chars)","17cb8ee4":"train_num_uniq_words = train_df['text'].apply(lambda x: len(set(str(x).split(' '))))\ntest_num_uniq_words = test_df['text'].apply(lambda x: len(set(str(x).split(' '))))\nselected_text_num_uniq_words = train_df['selected_text'].apply(lambda x: len(set(str(x).split(' '))))\n\n\ndata_num_uniq_words = [\n    {'x': train_num_uniq_words, 'label': 'Num of unique words in text of train data'},\n    {'x': test_num_uniq_words, 'label': 'Num of unique words in text of test data'},\n    {'x': selected_text_num_uniq_words, 'label': 'Num of unique words in selected text'},\n]\n\nplot_text_features(data_num_uniq_words)","6323ab8d":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english')) \n\n\ntrain_num_stop_words = train_df['text'].apply(lambda x: len([w for w in word_tokenize(x) if w in stop_words]))\ntest_num_stop_words = test_df['text'].apply(lambda x: len([w for w in word_tokenize(x) if w in stop_words]))\nselected_text_num_stop_words = train_df['selected_text'].apply(lambda x: len([w for w in word_tokenize(x) if w in stop_words]))\n\n\ndata_num_stop_words = [\n    {'x': train_num_stop_words, 'label': 'Num of stop words in text of train data'},\n    {'x': test_num_stop_words, 'label': 'Num of stop words in text of test data'},\n    {'x': selected_text_num_stop_words, 'label': 'Num of stop words in selected text'},\n]\n\nplot_text_features(data_num_stop_words)","e0b09d45":"from string import punctuation\n\n\n\ntrain_num_puncs = train_df['text'].apply(lambda x: len([w for w in word_tokenize(x) if w in punctuation]))\ntest_num_puncs = test_df['text'].apply(lambda x: len([w for w in word_tokenize(x) if w in punctuation]))\nselected_text_num_puncs = train_df['selected_text'].apply(lambda x: len([w for w in word_tokenize(x) if w in punctuation]))\n\n\ndata_num_puncs = [\n    {'x': train_num_puncs, 'label': 'Num of punctuation in text of train data'},\n    {'x': test_num_puncs, 'label': 'Num of punctuation in text of test data'},\n    {'x': selected_text_num_puncs, 'label': 'Num of punctuation in selected text'},\n]\n\nplot_text_features(data_num_puncs)\n","4ba24f50":"neutral_text = train_df.loc[train_df['sentiment'] == 'neutral', 'text'].append(test_df.loc[test_df['sentiment'] == 'neutral'])\npositive_text = train_df.loc[train_df['sentiment'] == 'positive', 'text'].append(test_df.loc[test_df['sentiment'] == 'positive'])\nnegative_text = train_df.loc[train_df['sentiment'] == 'negative', 'text'].append(test_df.loc[test_df['sentiment'] == 'negative'])\n\nneutral_text_num_words = neutral_text['text'].apply(lambda x: len(str(x).split(' ')))\npositive_text_num_words = positive_text['text'].apply(lambda x: len(str(x).split(' ')))\nnegative_text_num_words = negative_text['text'].apply(lambda x: len(str(x).split(' ')))\n\n\ndata_num_words = [\n    {'x': neutral_text_num_words, 'label': 'Num of words in neutral text'},\n    {'x': positive_text_num_words, 'label': 'Num of words in positive text'},\n    {'x': negative_text_num_words, 'label': 'Num of words in negative text'},\n]\n\nplot_text_features(data_num_words)\n\n","270d5c5c":"# test_df['selected_text'] = test_df['text']\n# test_df.loc[test_df.sentiment != 'neutral', 'selected_text'] = test_df.loc[test_df['sentiment'] != 'neutral', 'text'].apply(lambda x: \" \".join(x.strip().split(' ')[-5:]))\n\nsubmission_df['selected_text'] = test_df['text']\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head(10))","9a1499fb":"## 4. Analyzing Train Data\n\n### 4.1 distribution of train data","0fc04ff3":"### 1.1 About Tweet Sentiment Extraction Dataset\n\nThe data folder contains following files, all in csv format\n\n**Files**\n* `train.csv` - the training set\n* `test.csv` - the test set\n* `sample_submission.csv` - a sample submission file in the correct format\n\n**Columns**\n* `textID` - unique ID for each piece of text\n* `text` - the text of the tweet\n* `sentiment` - the general sentiment of the tweet\n* `selected_text` - [train only] the text that supports the tweet's sentiment","2a7d2372":"### 4.3 Word clouds of `neutral`, `positive` and `negative` words","aae30575":"* This shows that nltk has exact same implementation as ours\n* str1 and str2 has `jaccard_distance of 0.36363636363636365` and `jaccard_similarity of 0.6363636363636364`\n","5182a033":"`jacard_similarity = 1 - jacard_distance`","3c810b21":"### 4.8 Number of punctuations in text and selected_text","2fa1947b":"> Note: From above plots it is clear that both train and test data has similar distribution","3820fc22":"### 4.6 Plotting number of unique words in text and selected_text","9403187e":"### 4.4 Plotting number of words in text and selected_text","8fa96f22":"### 1.3 Competition metric:\n\nThe metric in this competition is the word-level Jaccard score.\nJaccard similarity or intersection over union is defined as size of intersection divided by size of union of two sets. Let\u2019s take example of two sentences:\n\n`Sentence 1: AI is our friend and it has been friendly`\n\n`Sentence 2: AI and humans have always been friendly`\n\nIn order to calculate similarity using Jaccard similarity, we will first perform lemmatization to reduce words to the same root word. In our case, \u201cfriend\u201d and \u201cfriendly\u201d will both become \u201cfriend\u201d, \u201chas\u201d and \u201chave\u201d will both become \u201chas\u201d. Drawing a Venn diagram of the two sentences we get:\n\n![](https:\/\/miro.medium.com\/max\/926\/1*NSK8ERXexyIZ_SRaxioFEg.png)\n\nPlease read [this](https:\/\/medium.com\/@adriensieg\/text-similarities-da019229c894) article for better understanding","d059f93a":"## 4.9 number of words in text category wise","743df761":"> Observations\"\n    * All text columns has most number of stop words in range 0-15.","5fa4bbba":"> Observations:\n    * From above plot we can see that number of characters in test and train set was in same range.\n    * In selected text the range flows from 3 to 138 Characters.\n\n","494b876f":"## 5. Training Model\n\n\nAbout to come","58de2d7a":"### 4.5 Plotting number of characters in text and selected_text","900a22e5":"> Observations:\n    * Number of punctuations varies from 0 to 100\n    * Most of the values lie between 0 to 10","34a16e74":"### 1.2 Reading Data","77901be7":"> Observations:\n    * We can see that number of unique words in train and test sets range from 1 to 30. \n    * In selected text most number of unique words lie between 1 to 30\n\n","e85aec29":"# Tweet Sentiment Extraction\n\n\n## 1. Introduction\n\n* With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\n* The goal of this competition is to extract those word or phrases which determines the sentiment of whole tweet.\n\n> Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n","ff4f0625":"> Note: train_df contains following interesting information:\n    * Text column do not have any duplicate values\n    * There is only 2 missing value in entire data frame, which is quite good\n    * The sentiment column has 3 possible values which are: `positive`, `negative` or `neutral`\n","8cde7696":"Little Help taken from these kernels\n\n* https:\/\/www.kaggle.com\/ratan123\/sentiment-extraction-understanding-metric-eda\n\n\nNote: If you like my work, please, upvote \u263a","e6b324a6":"> Note: test_df contains following interesting information:\n    * Text column do not have any duplicate values\n    * There is no missing value in test data frame.\n    * The sentiment column has 3 possible values which are: `positive`, `negative` or `neutral`","e9ab7cf9":"* **test profile report**","1ef3c6de":"## 6. Simple Submission","9da2775b":"> Note: Our train data is not symmetric as it has more neutral points as compared to positive or negative.[](http:\/\/)","91a2f2d1":"> Observations:\n    We can observe from above histogram plot that the number of words in train text and test text ranges from 1 to 30.Selected text words mostly fall in range of 1-25.\n\n","b136ae05":"## 2. Profiling Dataframes\n\nFor profiling i am using [pandas-profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling) library\n\n\n* **train profile report**","be1eeea4":"### 4.2 distribution of test data","d1457d42":"## 3. Understanding Competition Metric\n\nWe can define our own function for jaccard similarity or can simply use nltk library which contain predefined `jaccard_distance` function","e1643f4a":"### 4.7 Number of stop words in text and selected_text"}}