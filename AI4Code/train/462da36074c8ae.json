{"cell_type":{"61513fa4":"code","63a1cd93":"code","9a020a44":"code","97b6494a":"code","0b9bc1fc":"code","89c562cb":"code","ba8c433d":"code","1e3ae7be":"code","815a282d":"code","4a8d11eb":"code","2dc26441":"code","3f27d6e7":"code","71efe2d0":"code","0302cf2d":"code","2af8754f":"code","44c546a9":"code","1a522a26":"code","efb92ec6":"code","5835382c":"code","a3c17a14":"code","bd751674":"code","e4db8dc5":"code","5b92bb19":"code","ac01548e":"code","e207df19":"code","f0d73120":"code","ef2b598e":"code","a1d79dda":"code","ee1af001":"code","80e74fa7":"code","a5e3b138":"code","009bb495":"code","0850b510":"code","0bb4afbe":"code","92113382":"code","cb945e6e":"code","a02ec67f":"code","b0318397":"code","3d0d530f":"code","6b321319":"code","f048b164":"code","6fa5c9fe":"code","4004928d":"code","58e9906c":"code","0be09434":"code","f4c2fbe0":"code","b2c76389":"code","9807f38b":"code","a9dea86f":"code","98269d7f":"code","76c7dc6e":"code","f83e1b96":"code","2fc7bf9d":"code","099992cd":"code","885cd8f2":"code","ec87f0e1":"code","88015a8d":"code","0162e464":"code","10e46800":"code","ad5eb47c":"code","6db4524d":"code","3df79e60":"code","5cb9adcd":"code","7b15cc42":"code","ddc275d5":"code","c4400836":"code","c2e4ff99":"code","096cc710":"code","31760a4f":"code","458e2305":"code","2b45e052":"code","d29a61d3":"code","f205f4ff":"code","b36ba6e5":"code","71d1f4e8":"code","b2bb201b":"code","c00d737b":"code","edd9af5e":"code","cfb8bbb0":"code","8446dfd4":"code","020ac888":"code","7384a495":"code","cff4de6b":"code","8c70ef19":"code","1600b52f":"code","896be1d4":"code","e7ef3efa":"code","79ac59d1":"code","f0778ba3":"code","ccfa9c11":"code","8ca0f4a9":"code","d00e7ed6":"code","229ddf32":"markdown","1b610548":"markdown","0ceaa5e3":"markdown","bc6de63b":"markdown","1b19034f":"markdown","84f68e72":"markdown","44753765":"markdown","66145fe5":"markdown","b8974213":"markdown","1d4b396b":"markdown","bf3d1fee":"markdown","bf910b4b":"markdown","f30d5c80":"markdown","3b2edc84":"markdown","a3ea6a1e":"markdown","757646a3":"markdown","d7197d33":"markdown","ab1a7401":"markdown","4e07f9aa":"markdown","5f6226bb":"markdown","7d3f0f94":"markdown","ece36186":"markdown","22253cbb":"markdown","01434683":"markdown","2464a5b5":"markdown","700a6436":"markdown","8c3c7fa5":"markdown","c777882d":"markdown","a15b98c4":"markdown","3344d02f":"markdown","ab50c7ce":"markdown","80f8b76e":"markdown","214d1a33":"markdown","73736f8b":"markdown","5a26b024":"markdown","90d57874":"markdown","3a7455a2":"markdown","a29e1275":"markdown","e22236a7":"markdown","e4ca2a82":"markdown","aab48251":"markdown","a5610416":"markdown","d9873176":"markdown","2526286b":"markdown","a69f1660":"markdown","9eb4712a":"markdown","6433d51a":"markdown","8036f36b":"markdown","47499a05":"markdown","1f027a1f":"markdown"},"source":{"61513fa4":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","63a1cd93":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","9a020a44":"import os\nimport copy\nimport tqdm\nimport pickle\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\n\nfrom pickle import load,dump\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.cluster import KMeans\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor","97b6494a":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_drug = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')","0b9bc1fc":"#Just making a copy of train and test features\ntrain_features2=train_features.copy()\ntest_features2=test_features.copy()","89c562cb":"print('Training Features Samples')\ndisplay(train_features.head(3))\nprint('Training Features Description')\ndisplay(train_features.describe())","ba8c433d":"print('Training Features Samples')\ndisplay(test_features.head(3))\nprint('Training Features Description')\ndisplay(test_features.describe())","1e3ae7be":"train_missing = train_features.isnull().sum().sum()\ntest_missing = test_features.isnull().sum().sum()\nif train_missing & test_missing == 0:\n    print(\"Train and Test Files have no missing values\")\nelse: print(\"Train and Test Files have missing values\")    ","815a282d":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=train_features, alpha=0.85)\nplt.title('Train: Control and treated samples', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=train_features, alpha=0.85)\nplt.title('Train: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","4a8d11eb":"plt.figure(figsize=(10,5))\nsns.countplot( train_features['cp_time'], color='violet')\nplt.title(\"Train: Treatment duration \", fontsize=15, weight='bold')\nplt.show()","2dc26441":"GENES = [g for g in train_features.columns if g.startswith(\"g-\")]\nprint(f\"Number of gene features: {len(GENES)}\")\nCELLS = [c for c in train_features.columns if c.startswith(\"c-\")]\nprint(f\"Number of cell features: {len(CELLS)}\")","3f27d6e7":"plt.figure(figsize=(16,16))\nsns.set_style('whitegrid')\ngene_choice=np.random.choice(len(GENES),16)\nfor i,col in enumerate(gene_choice):\n    plt.subplot(4,4,i+1)\n    plt.hist(train_features.loc[:,GENES[col]],bins=100,color=\"blue\")\n    plt.title(GENES[col])","71efe2d0":"plt.figure(figsize=(16,16))\nsns.set_style('whitegrid')\ncell_choice=np.random.choice(len(CELLS),16)\nfor i,col in enumerate(cell_choice):\n    plt.subplot(4,4,i+1)\n    plt.hist(train_features.loc[:,CELLS[col]],bins=100,color=\"green\")\n    plt.title(CELLS[col])","0302cf2d":"def treated(a):\n    treated= a[a['cp_type']=='trt_cp']\n    return treated","2af8754f":"cells = treated(train_features)[CELLS]\nplt.figure(figsize=(15,6))\nsns.heatmap(cells.corr(),cmap='coolwarm',alpha=0.75)\nplt.title('Correlation: Cell viability', fontsize=15, weight='bold')\nplt.show()","44c546a9":"def corrs(data,col1=\"Cell 1\",col2=\"Cell 2\",rows=5,thresh=0.9,pos=[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53]):\n    #Correlation between genes\n    corre=data.corr()\n    #Unstack the dataframe\n    s=corre.unstack()\n    so=s.sort_values(kind='quicksort',ascending=False)\n    #Create new dataframe\n    so2=pd.DataFrame(so).reset_index()\n    so2=so2.rename(columns={0:'correlation','level_0':col1,'level_1':col2})\n    #Filter out the coef 1 correlation between the same drugs\n    so2=so2[so2['correlation']!=1]\n    #Drop pair duplicates\n    so2=so2.reset_index()\n    pos=pos\n    so3=so2.drop(so2.index[pos])\n    so3=so3.drop('index',axis=1)\n    #Show the first 10 high correlations\n    cm = sns.light_palette(\"pink\", as_cmap=True)\n    s = so3.head(rows).style.background_gradient(cmap=cm)\n    print(f\"{len(so2[so2['correlation']>thresh])\/2} {col1} pairs have +{thresh} correlation.\")\n    return s","1a522a26":"corrs(cells,'cell 1','cell 2',rows=8)","efb92ec6":"genes = treated(train_features)[GENES]\nplt.figure(figsize=(15,6))\nsns.heatmap(cells.corr(),cmap='coolwarm',alpha=0.75)\nplt.title('Correlation: GENE viability', fontsize=15, weight='bold')\nplt.show()","5835382c":"corrs(genes,'Gene 1', 'Gene 2',rows=8)","a3c17a14":"target_cols_scored=[col for col in train_targets_scored.columns if col not in ['sig_id']]\ntarget_cols_nonscored=[col for col in train_targets_nonscored.columns if col not in ['sig_id']]","bd751674":"sns.distplot(train_targets_scored[target_cols_scored].sum(axis=1),color='orange')\nplt.title(\"The Scored targets distribution\", fontsize=15, weight='bold')\nplt.show()","e4db8dc5":"fig = plt.figure(figsize=(12, 60))\n\nsns.barplot(x=train_targets_scored[target_cols_scored].sum(axis=0).sort_values(ascending=False).values,\n            y=train_targets_scored[target_cols_scored].sum(axis=0).sort_values(ascending=False).index)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Training Set Scored Targets Classification Counts', size=18, pad=18, weight='bold')\nplt.show()","5b92bb19":"plt.figure(figsize=(15,6))\nsns.heatmap(train_targets_scored[target_cols_scored].corr(),cmap='hot',alpha=0.75)\nplt.title('Correlation between scored targets:', fontsize=15, weight='bold')\nplt.show()","ac01548e":"corrs(train_targets_scored[target_cols_scored],'Target 1', 'Target 2',rows=5)","e207df19":"fig = plt.figure(figsize=(12, 100))\n\nsns.barplot(x=train_targets_nonscored[target_cols_nonscored].sum(axis=0).sort_values(ascending=False).values,\n            y=train_targets_nonscored[target_cols_nonscored].sum(axis=0).sort_values(ascending=False).index)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Training Set Scored Targets Classification Counts', size=18, pad=18, weight='bold')\nplt.show()","f0d73120":"plt.figure(figsize=(15,6))\nsns.heatmap(train_targets_nonscored[target_cols_nonscored].corr(),cmap='hot',alpha=0.75)\nplt.title('Correlation between Non-scored targets:', fontsize=15, weight='bold')\nplt.show()","ef2b598e":"corrs(train_targets_nonscored[target_cols_nonscored],'Target 1', 'Target 2',rows=8)","a1d79dda":"all_targets=train_targets_scored.merge(train_targets_nonscored, on='sig_id',how='left')\ncorrs(all_targets[target_cols_nonscored+target_cols_scored],'Target 1', 'Target 2',rows=8)","ee1af001":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=test_features,alpha=0.85)\nplt.title('Test: Control and treated samples', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=test_features,  alpha=0.85)\nplt.title('Test: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","80e74fa7":"plt.figure(figsize=(10,5))\nsns.countplot( test_features['cp_time'], color='violet')\nplt.title(\"Train: Treatment duration \", fontsize=15, weight='bold')\nplt.show()","a5e3b138":"genes2 = treated(test_features)[GENES]\nfig=plt.figure(figsize=(15,6))\n#first row first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.heatmap(genes2.corr(),cmap='coolwarm',alpha=0.9)\nplt.title('Test: Gene expression correlation', fontsize=15, weight='bold')\n\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.heatmap(genes.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Train: Gene expression correlation', fontsize=15, weight='bold')\nplt.show()","009bb495":"corrs(genes2,'Gene 1', 'Gene 2',rows=8)","0850b510":"cells2 = treated(test_features)[CELLS]\nfig=plt.figure(figsize=(15,6))\n#first row first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.heatmap(genes2.corr(),cmap='coolwarm',alpha=0.9)\nplt.title('Test: Cell viability correlation', fontsize=15, weight='bold')\n\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.heatmap(genes.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Train: Cell viability correlation', fontsize=15, weight='bold')\nplt.show()","0bb4afbe":"corrs(cells2,'Cells 1', 'Cells 2',rows=8)","92113382":"fig = plt.figure(figsize=(10,5))\n\nsns.barplot(x=train_drug['drug_id'].value_counts().values[:10],\n            y=train_drug['drug_id'].value_counts().index[:10],palette = \"Blues\")\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.xlabel('')\nplt.ylabel('')\nplt.title('Most frequent drugs', size=18, pad=18, weight='bold')\nplt.show()","cb945e6e":"print('First observation:')\nprint(f\"Number of rows of the Control vehicle is {len(train_features[train_features['cp_type']=='ctl_vehicle'])}\")\nprint(f\"Number of rows of the Drug cacb2b860 is {train_drug.drug_id.value_counts()[0]}\")","a02ec67f":"drug_count = train_drug[['drug_id']].value_counts().to_frame()\ndrug_count = drug_count.rename(columns={0:'drug_count'})\ndrug_count2=drug_count['drug_count'].value_counts().to_frame().reset_index()\ndrug_count2=drug_count2.rename(columns={'index': 'Samples per drug', 'drug_count':'Number of Drugs'})\ndrug_count2[:12]","b0318397":"train_features['target'] = 0\ntest_features['target'] = 1\n\nX = pd.concat([train_features.loc[:,GENES+CELLS], test_features.loc[:,GENES+CELLS]]).reset_index(drop=True)\ny = pd.concat([train_features.loc[:, 'target'], test_features.loc[:, 'target']]).reset_index(drop=True)","3d0d530f":"k=5\nskf=StratifiedKFold(n_splits=k,random_state=721991,shuffle=True)\nscores=[]\noof_predictions = pd.DataFrame(np.zeros((X.shape[0], 1)), columns=['target'])\nfeature_importance = pd.DataFrame(np.zeros((X.shape[1], k)), columns=[f'Fold_{i}_Importance' for i in range(1, k + 1)], index=X.columns)\nparameters = {\n    'num_iterations': 500,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 5, \n    'learning_rate': 0.05,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.9,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'binary',\n    'seed': 721991,\n    'feature_fraction_seed': 721991,\n    'bagging_seed': 721991,\n    'drop_seed': 721991,\n    'data_random_seed': 721991,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'auc',\n    'n_jobs': -1,   \n}\nprint('Running LightGBM Adversarial Validation Model\\n' + ('-' * 45) + '\\n')\nfor fold,(train_idx,val_idx) in enumerate(skf.split(X, y), 1):\n    trn_data=lgb.Dataset(X.iloc[train_idx,:],label=y.iloc[train_idx])\n    val_data=lgb.Dataset(X.iloc[val_idx,:],label=y.iloc[val_idx])\n    model=lgb.train(parameters,trn_data,valid_sets=[trn_data, val_data], verbose_eval=50)\n    feature_importance.iloc[:,fold-1]=model.feature_importance(importance_type='gain')\n    predictions = model.predict(X.iloc[val_idx, :], num_iteration=model.best_iteration)\n    oof_predictions.loc[val_idx, 'target'] = predictions\n    score=roc_auc_score(y.iloc[val_idx],predictions)\n    scores.append(score)\n    print(f'\\nFold {fold} - ROC AUC Score {score:.6}\\n')\noof_score = roc_auc_score(y, oof_predictions)\nprint(f'\\n{\"-\" * 30}\\nLightGBM Adversarial Validation Model Mean ROC AUC Score {np.mean(scores):.6} [STD:{np.std(scores):.6}]')\nprint(f'LightGBM Adversarial Validation Model OOF ROC AUC Score: {oof_score:.6}\\n{\"-\" * 30}')\n\nplt.figure(figsize=(20, 20))\nfeature_importance['Mean_Importance'] = feature_importance.sum(axis=1) \/ k\nfeature_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=feature_importance.index[:50], data=feature_importance[:50])\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('LightGBM Adversarial Validation Model Top 50 Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel X, y, oof_predictions, feature_importance, parameters, scores, oof_score","6b321319":"#seeding Everything\nseed = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(seed)","f048b164":"plt.figure(figsize=(15,4))\nsns.set_style('whitegrid')\ngene_choice=[0,1,2,3]\nfor i,col in enumerate(gene_choice):\n    plt.subplot(1,4,i+1)\n    plt.hist(train_features.loc[:,GENES[col]],bins=100,color=\"blue\")\n    plt.title(GENES[col])","6fa5c9fe":"plt.figure(figsize=(15,4))\nsns.set_style('whitegrid')\ncell_choice=[0,1,2,3]\nfor i,col in enumerate(cell_choice):\n    plt.subplot(1,4,i+1)\n    plt.hist(train_features.loc[:,CELLS[col]],bins=100,color=\"green\")\n    plt.title(CELLS[col])","4004928d":"qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ntrain_features[GENES+CELLS] = qt.fit_transform(train_features[GENES+CELLS])\ntest_features[GENES+CELLS] = qt.transform(test_features[GENES+CELLS])","58e9906c":"plt.figure(figsize=(15,4))\nsns.set_style('whitegrid')\ngene_choice=[0,1,2,3]\nfor i,col in enumerate(gene_choice):\n    plt.subplot(1,4,i+1)\n    plt.hist(train_features.loc[:,GENES[col]],bins=100,color=\"blue\")\n    plt.title(GENES[col])","0be09434":"plt.figure(figsize=(15,4))\nsns.set_style('whitegrid')\ncell_choice=[0,1,2,3]\nfor i,col in enumerate(cell_choice):\n    plt.subplot(1,4,i+1)\n    plt.hist(train_features.loc[:,CELLS[col]],bins=100,color=\"green\")\n    plt.title(CELLS[col])","f4c2fbe0":"#GENES\nn_comp = 600  #<--Update\n\n#pca_g = PCA(n_components=n_comp, random_state=42)\n#data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n#gpca= (pca_g.fit(data[GENES]))\n\ngpca= load(open('..\/input\/moa-tabnet-last\/gpca.pkl', 'rb')) #<--Loading saved features using pickle load feature\ntrain2= (gpca.transform(train_features[GENES]))\ntest2 = (gpca.transform(test_features[GENES]))\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)\n\n#dump(gpca, open('gpca.pkl', 'wb')) #<--Saving created features for just inference kernels","b2c76389":"#CELLS\nn_comp = 50  #<--Update\n\n#pca_c = PCA(n_components=n_comp, random_state=42)\n#data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n#cpca= (pca_c.fit(data[CELLS]))\n\ncpca= load(open('..\/input\/moa-tabnet-last\/cpca.pkl', 'rb'))\ntrain2= (cpca.transform(train_features[CELLS]))\ntest2 = (cpca.transform(test_features[CELLS]))\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)\n\n#dump(cpca, open('cpca.pkl', 'wb'))","9807f38b":"print(f'Number of features before Variance thresholding - {train_features.shape[1]}')","a9dea86f":"c_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\nmask = (train_features[c_n].var() >= 0.85).values #<-- In this I have kept threshold value equal to 0.85\ntmp = train_features[c_n].loc[:, mask]\ntrain_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\ntmp = test_features[c_n].loc[:, mask]\ntest_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)","98269d7f":"print(f'Number of features after Variance thresholding - {train_features.shape[1]}')","76c7dc6e":"from sklearn.cluster import KMeans\ndef fe_cluster_genes(train, test, n_clusters_g = 22, SEED = 42):\n    \n    features_g = GENES\n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n  #      data = pd.concat([train_, test_], axis = 0)\n  #      kmeans_genes = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n  #      dump(kmeans_genes, open('kmeans_genes.pkl', 'wb'))\n        kmeans_genes = load(open('..\/input\/moa-tabnet-last\/kmeans_genes.pkl', 'rb'))\n        train[f'clusters_{kind}'] = kmeans_genes.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_genes.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_genes(train_features2,test_features2)","f83e1b96":"def fe_cluster_cells(train, test, n_clusters_c = 4, SEED = 42):\n    \n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'c', n_clusters = n_clusters_c):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n #       data = pd.concat([train_, test_], axis = 0)\n #       kmeans_cells = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n #       dump(kmeans_cells, open('kmeans_cells.pkl', 'wb'))\n        kmeans_cells = load(open('..\/input\/moa-tabnet-last\/kmeans_cells.pkl', 'rb'))\n        train[f'clusters_{kind}'] = kmeans_cells.predict(train_.values)\n        test[f'clusters_{kind}'] = kmeans_cells.predict(test_.values)\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster_cells(train_features2,test_features2)","2fc7bf9d":"train_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)","099992cd":"# Applying K-means clustering on pca generated features\ndef fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n  #      data=pd.concat([train,test],axis=0)\n  #      kmeans_pca = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n  #      dump(kmeans_pca, open('kmeans_pca.pkl', 'wb'))\n        kmeans_pca = load(open('..\/input\/moa-tabnet-last\/kmeans_pca.pkl', 'rb'))\n        train[f'clusters_pca'] = kmeans_pca.predict(train.values)\n        test[f'clusters_pca'] = kmeans_pca.predict(test.values)\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)","885cd8f2":"train_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]","ec87f0e1":"train_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]","88015a8d":"# some important g-features\ngsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","0162e464":"def fe_stats(train, test):\n    \n    # statistical Features\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        # Using pair of highly correlative features\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-26'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        df['c26_c38'] = df['c-26'] * df['c-38']\n        df['c90_c13'] = df['c-90'] * df['c-13']\n        df['c85_c31'] = df['c-85'] * df['c-31']\n        df['c63_c42'] = df['c-63'] * df['c-42']\n        df['c94_c11'] = df['c-94'] * df['c-11']\n        df['c94_c60'] = df['c-94'] * df['c-60']\n        df['c55_c42'] = df['c-55'] * df['c-42']\n        df['g37_c50'] = df['g-37'] * df['g-50']\n        \n        # Making Polynomial Features\n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)","10e46800":"train_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]","ad5eb47c":"train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)","6db4524d":"#Extract unique elements per column\ncols2 = train_targets_nonscored.columns.to_list() # specify the columns whose unique values you want here\nuniques2 = {col: train_targets_nonscored[col].nunique() for col in cols2}\nuniques2=pd.DataFrame(uniques2, index=[0]).T\nuniques2=uniques2.rename(columns={0:'count'})\nuniques2= uniques2.drop('sig_id', axis=0)\nprint(f\"{len(uniques2[uniques2['count']==1])} targets without ANY mechanism of action in the nonscored dataset\")","3df79e60":"nonmoacols=uniques2[uniques2['count']==1].index\ntrain_targets_nonscored_columns = [col for col in list(train_targets_nonscored.columns) if col not in nonmoacols]\ntrain_targets_nonscored=train_targets_nonscored[train_targets_nonscored_columns]","5cb9adcd":"train = train_features.merge(train_targets_nonscored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_nonscored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n\ntrain = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)\n\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()\ntarget=target[target_cols]\n\ntrain = pd.get_dummies(train, columns=['cp_time','cp_dose'])\ntest_ = pd.get_dummies(test, columns=['cp_time','cp_dose'])\n\nfeature_cols = [c for c in train.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['sig_id']]","7b15cc42":"train_features_tabnet=train[feature_cols]\nX_test_tabnet=test_[feature_cols].values","ddc275d5":"class TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","c4400836":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","c2e4ff99":"class NNModel(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(NNModel, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.dense1(x), 1e-3)\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","096cc710":"#class SmoothBCEwLogits(_WeightedLoss):\n#    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n#        super().__init__(weight=weight, reduction=reduction)\n#        self.smoothing = smoothing\n#        self.weight = weight\n#        self.reduction = reduction\n\n#    @staticmethod\n#    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n#        assert 0 <= smoothing < 1\n#        with torch.no_grad():\n#            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n#        return targets\n\n#    def forward(self, inputs, targets):\n#        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n#            self.smoothing)\n#        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n#        if  self.reduction == 'sum':\n#            loss = loss.sum()\n#        elif  self.reduction == 'mean':\n#            loss = loss.mean()\n\n#        return loss","31760a4f":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nNFOLDS = 7\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=2048\nBATCH_SIZE=256","458e2305":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = NNModel(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    model.load_state_dict(torch.load(f\"..\/input\/moa-pretrained-non-scored-targets-as-meta-features\/SEED{seed}_FOLD{fold}_nonscored.pth\",map_location=torch.device(DEVICE)))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return predictions","2b45e052":"def run_k_fold(NFOLDS, seed):\n    predictions = np.zeros((len(test), len(target_cols)))\n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        predictions += pred_ \/ NFOLDS\n    return predictions","d29a61d3":"SEED = [0,1,2,3,4,5,6] \npredictions = np.zeros((len(test), len(target_cols)))\nfor seed in SEED:\n    predictions_ = run_k_fold(NFOLDS, seed)\n    predictions += predictions_ \/ len(SEED)\ntest_[target_cols] = predictions","f205f4ff":"train = train.merge(train_targets_scored, on='sig_id')\ntarget = train[train_targets_scored.columns]\ntarget_cols = target.drop('sig_id', axis=1).columns.values.tolist()","b36ba6e5":"feature_cols_NN = [c for c in train.columns if c not in target_cols]\nfeature_cols_NN = [c for c in feature_cols_NN if c not in ['sig_id','kfold']]","71d1f4e8":"num_features=len(feature_cols_NN)\nnum_targets=len(target_cols)\nhidden_size=2048","b2bb201b":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    x_test = test_[feature_cols_NN].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = NNModel(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n    \n    model.load_state_dict(torch.load(f\"..\/input\/moa-pretrained-non-scored-targets-as-meta-features\/SEED{seed}_FOLD{fold}_scored.pth\",map_location=torch.device(DEVICE)))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return predictions","c00d737b":"def run_k_fold(NFOLDS, seed):\n    predictions = np.zeros((len(test), len(target_cols)))\n    for fold in range(NFOLDS):\n        pred_ = run_training(fold, seed)\n        predictions += pred_ \/ NFOLDS\n    return predictions","edd9af5e":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2,3,4,5,6]  #<-- Update\npredictions = np.zeros((len(test), len(target_cols)))\nfor seed in SEED:\n    predictions_ = run_k_fold(NFOLDS, seed)\n    predictions += predictions_ \/ len(SEED)\npreds_pretrain = predictions ","cfb8bbb0":"preds_pretrain","8446dfd4":"class ResNetModel(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(ResNetModel, self).__init__()\n        self.hidden_size = [2048, 1500, 1250, 1000]\n        self.dropout_value = [0.3, 0.3, 0.3, 0.3]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n        \n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x","020ac888":"num_features = len(feature_cols)\nnum_features","7384a495":"def run_training(fold_id, seed_id):\n    \n    seed_everything(seed_id)\n    \n    model = ResNetModel(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"..\/input\/pytorch-transfer-learning-last\/SCORED_ONLY_SEED{seed_id}_FOLD{fold_id}_.pth\",map_location=torch.device(DEVICE)))\n    model.to(DEVICE)                    \n\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=128, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return predictions","cff4de6b":"def run_k_fold(NFOLDS, seed_id):\n    predictions = np.zeros((len(test), len(target_cols)))\n    for fold_id in range(NFOLDS):\n        pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ \/ NFOLDS   \n    return predictions","8c70ef19":"SEED = [0,1,2,3,4,5,6]\npredictions = np.zeros((len(test), len(target_cols)))\nfor seed_id in SEED:\n    predictions_ = run_k_fold(NFOLDS, seed_id)\n    predictions += predictions_ \/ len(SEED)\npreds_transfer=predictions","1600b52f":"preds_transfer","896be1d4":"test_cv_preds_1 = []\n\nNB_SPLITS = 7\nSEED = [0,1,2,3,4,5,6]\nfor s in SEED:\n    for fold_nb, (train_idx, val_idx) in enumerate(MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = s).split(train_features_tabnet, target)):\n        \n        model1 = TabNetRegressor()\n        ### Predict on test ###\n        model1.load_model(f\"..\/input\/moa-tabnet-train-inference\/TabNet_seed_{s}_fold_{fold_nb+1}.zip\")\n        preds_test_1 = model1.predict(X_test_tabnet)\n        test_cv_preds_1.append(1 \/ (1 + np.exp(-preds_test_1)))\n\ntest_preds_all_1 = np.stack(test_cv_preds_1)\npreds_tabnet_1 = test_preds_all_1.mean(axis = 0)","e7ef3efa":"preds_tabnet_1","79ac59d1":"#test_cv_preds_2 = []\n\n#NB_SPLITS = 7\n#SEED = [0,1,2,3,4,5,6]\n#for s in SEED:\n#    for fold_nb, (train_idx, val_idx) in enumerate(MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = s).split(train_features_tabnet, target)):\n        \n#        model1 = TabNetRegressor()\n#        model2 = TabNetRegressor()\n        ### Predict on test ###\n#        model2.load_model(f\"..\/input\/moa-tabnet-last\/TabNet_seed_{s}_fold_{fold_nb+1}.zip\")\n#        preds_test_2 = model2.predict(X_test_tabnet)\n#        test_cv_preds_2.append(1 \/ (1 + np.exp(-preds_test_2)))\n\n#test_preds_all_2 = np.stack(test_cv_preds_2)\n#preds_tabnet_2 = test_preds_all_2.mean(axis = 0)","f0778ba3":"#preds_tabnet_2","ccfa9c11":"predictions_ = 0.333333*preds_pretrain + 0.333333*preds_tabnet_1 + 0.333334*preds_transfer # + 0.33*preds_tabnet_2","8ca0f4a9":"df = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\npublic_id = list(df['sig_id'].values)\ntest_id = list(test_features['sig_id'].values)\nprivate_id = list(set(test_id)-set(public_id))\ndf_submit = pd.DataFrame(index = public_id+private_id, columns=target_cols)\ndf_submit.index.name = 'sig_id'\ndf_submit[:] = 0\ndf_submit.loc[test.sig_id,:] = predictions_\ndf_submit.loc[test_features[test_features.cp_type=='ctl_vehicle'].sig_id]= 0\ndf_submit.to_csv('submission.csv',index=True)","d00e7ed6":"df_submit","229ddf32":"* It seems we have 2 outliers here: **nfkb_inhibitor** and **proteasome_inhibitor.** \n\n* We can see many target labels in the plot: inhibitor, agonist, antagonist.\n***\n* **Observations:**\n\n* 1.It seems we have 2 outliers here: **nfkb_inhibitor** and **proteasome_inhibitor.** \n* We can see many target labels in the plot: inhibitor, agonist, antagonist.\n* **nfkb_inhibitor and proteasome_inhibitor** have +0.9 correlation and are highly presented in the samples.\n* **Kit_inhibtor** is highly correlated with 2 targets: **pdgfr_inhibitor and flt3_inhibitor**.\n\n***\n\n## Nonscored targets:\n\nIn this section, we will have a look over the dataset provided that will not be used in the score. This dataset has 402 MoAs *(more than the 206 MoAs in the targets_scored dataset that will be used in the score).* ","1b610548":"* All the targets are present in at least one sample.\n* The presence of the targets is very low in the samples (Mostly less than 0.75%).\n* Some targets *(outliers)* have a higher presence in comparison with the rest of targets with a percentage in the range (3%, 4%).\n\n> **Let's have a look over some of these targets**","0ceaa5e3":"* **Many high correlations between c- features. This is something to be taken into consideration in feature engineering.**\n***","bc6de63b":"***\n# Ensemble\n>### Pytorch Pre-train (Using Predictions on Non-scored Targets as Meta-Features)","1b19034f":"**Let's have a quick glance at Test Features**","84f68e72":"Target features are categorized into two groups; scored and non-scored target features, and features in both of those groups are binary. The competition score is based on the scored target features but non-scored group can still be used for model evaluation, data analysis and feature engineering.\n\n> ## Scored targets:\nThis is a multi-label classification, we have 207 MoA and we have to find out the mechanism of action of the 5000 drugs that were treated in the `sig_id` samples. A single sample treated with a drug can have many active targets, in other words, one drug can have more than 1 mechanism of action, so we have to predict the mechanisms of action of each drug.\n\n*We will filter the **train_targets_scored** dataset and keep just the treated rows (we discard the control rows because they are not treated with the drugs).*","44753765":"**\n***Observation in this EDA:***\n\nCell and Gene viability should range between the integers 0 and 1. Here, we have values in the range -10 and 6 because the data were z-scored and then normalized using a procedure called [quantile normalization](https:\/\/clue.io\/connectopedia\/glossary#Q).\n\n*A high negative cell viability measure reflects a high fraction of killing [@by the host](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/191487).* In other words:\n* High negative values = High number of dead cells\n* High positive values = High number of living cells.\n***","66145fe5":">### Variance Threshold\nAnother approach for feature selection is eliminating features that have lower variance than a predefined threshold. Variance is the average of the squared differences from the mean, so it measures how far the data points are spread out from the mean. If the variance is low or close to zero, then a feature is approximately constant and will not improve the performances of the models. In that case, it should be removed.","b8974213":">### Pytorch Transfer learning (Using All Targets)","1d4b396b":"**Quite Good Correlation can be found in Non-scored Targets.**\n***\nNow let's check out correlation in **(scored targets + non-scored targets) i.e. All targets** \n## All Targets:","bf3d1fee":"#### Predicting Scored Targets for pretrain model","bf910b4b":"***\n> ### Dimensionality Reduction (PCA)\nPCA is a linear transformation that projects the data into another space, where vectors of projections are defined by variance of the data. PCA results can be evaluated with reconstruction error and cumulative percent variance.\n\nFor Finding the optimal number of components for PCA of genes and cells please refer this [notebook](https:\/\/www.kaggle.com\/kushal1506\/deciding-n-components-in-pca)","f30d5c80":"We can see several high positive and negative correlations between some genes, same as in the train set","3b2edc84":"> ### Cells correlation\n**let's see the correlation between gene expression features. (in the treated samples, no control)**","a3ea6a1e":">### Genes correlation:\n**let's see the correlation between gene expression features. (in the treated samples, no control)**","757646a3":"* **Test set cell features have better correlation than Train set**\n* **The order of the cells correlation is different in the test set!**\n***","d7197d33":">### Assigning weights to predictions: \n\n**These Weights are manually decided.**\n\n* To decide Weights based on differnet Models OOF-Score...Please refer this [kernel](https:\/\/www.kaggle.com\/gogo827jz\/optimise-blending-weights-with-bonus-0).","ab1a7401":"So, Drug cacb2b860 is the control vehicle, this explains it's high presence in the train set. The second most frequent drugs is 87d714366 with 718 rows!","4e07f9aa":"> ### Distribution of Cells:","5f6226bb":"**After applying Quantile Transformation , obtained Uniformaity in Train set is easily noticiable.**","7d3f0f94":"***\n> ### Genes Expression and Cells Viability Features","ece36186":"># Exploartory Data Analysis:\n># Overview: Features\n* Features **g-** signify gene expression data.\n* Features **c-** signify cell viability data.\n* **cp_type** indicates samples treated with a compound, **trt_cp** samples treated with the compounds. \n* **cp_vehicle** or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs.\n* **cp_time** and **cp_dose** indicate treatment duration (24, 48, 72 hours) and dose (high or low).","22253cbb":"Adversarial validation model yields approx. **0.525** ROC AUC score which suggests that training and public test set are similar. Features at the top of importance plot have higher gain, because they have different means in training and public test set due to distribution tail extremities. This could be related to small sample size of public test set, and it's not necessarily have to be expected in private test set.\n***","01434683":"**Train set before Transformation**","2464a5b5":">### Using statistical Features","700a6436":"## **Objective and Metric:**\n\nThis is a multi-label binary classification problem, and metric used for the evaluation is mean columnwise log loss. For every row, a probability that the sample had a positive response for each target, has to be predicted. For $N$ rows and $M$ targets, there will be $N\u00d7M$ predictions. Submissions are scored by the log loss:\n\n$\\Large \\text{log loss} = - \\frac{1}{M}\\sum_{m=1}^{M} \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_{i,m} \\log(\\hat{y}_{i,m}) + (1 - y_{i,m}) \\log(1 - \\hat{y}_{i,m})\\right]$\n\n* $N$ is the number of rows ($i=1,\u2026,N$)\n* $M$ is the number of targets ($m=1,\u2026,M$)\n* $\\large \\hat{y}_{i,m}$ is the predicted probability of the ith row and mth target\n* $\\large y_{i,m}$ is the ground truth of the ith row and mth target (1 for a positive response, 0 otherwise)\n* $log()$ is the natural logarithm\n\nSubmitted predicted probabilities are replaced with $max(min(p,1-10^{-15}),10^{-15})$.\n***\n## Importing Necesseties:","8c3c7fa5":"> ## Cell viability:","c777882d":"#### Predicting Non-scored Targets","a15b98c4":"> ### Distribution of Genes:","3344d02f":"# Mechanisms of Action (MoA) Prediction:\n***\n![image1](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-forum-message-attachments\/o\/inbox%2F846065%2F283d7731734a680ad0abe9579059dacc%2FNew%20Project.png?generation=1600472356251411&alt=media)\n\nIn this competition, we are supposed to develop models **to determine the mechanism of action of a new drug based on the gene expression and cell viability information.** For this competition, you will have access to a unique dataset that combines gene expression and cell viability data in addition to the MoA annotations of more than 5,000 drugs.\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">PROJECT CONTENT<\/h3>\n    \n> ####  1- Exploratory Data Analysis\n> ####  2- Feature Engineering\n> ####  3. Models Ensemble\n    \n## If you find this kernel useful, Please Upvote it , it motivates me to write more Quality content :)    ","ab50c7ce":">### Checking for Missing Values","80f8b76e":"Everything seems similar to the train set:\n* The doses are equally applied.\n* Very few control samples.\n* Same treatment duration 24h, 48h and 72h.\n\n**Good news!** It seems that both train and test datasets are similar in terms of *experimental conditions.* The variation would be in the gene expression and cell viability since the samples used in the test set are different than the train set.\n\nLet's see how different are those samples!\n\n> ## Gene expression:","214d1a33":"**Great!** This nonscored dataset seems promising. \n\n### Note:- All Targets can be used for Transfer learning\n***\n# Test features:\nAfter understanding the relationship between the features and the labels, we move on to the test set to understand the features and their relationship with the train features.\n> ## Categorial Features:","73736f8b":"So U can notice that many features are removed that have variance less than 0.85.\n***\n>### K-Means Clustering\nThe objective of K-means is simple: group similar data points together and discover underlying patterns. To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.You\u2019ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.\nEvery data point is allocated to each of the clusters through reducing the in-cluster sum of squares.\nIn other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.\nThe \u2018means\u2019 in the K-means refers to averaging of the data; that is, finding the centroid.\n\nThis is a multi-Label problem so applying K-means maybe beneficial.\n\nHere's a short video to refer K-means Clustering. [YouTube Video](https:\/\/www.youtube.com\/watch?v=4b5d3muPQmA&t=0s&index=2&list=PLlcyUWQOcKAgDvemP5qJp0kcvWU5n-d7V)","5a26b024":">### Extracting and removing 71 Non-scored targets without any MoA.","90d57874":"Horizontal and vertical white lines in non-scored target correlations occur due to NaNs.\n\n### Non-Scored Targets with the highest MoA correlation","3a7455a2":"**Let's have a quick glance at Train Features**","a29e1275":"# Train Features:\n> ### Categorial Features :\n#### There are 3 Categorial features:- `Treatement time, dose and type.`\n#### We can observe:\n\n>#### Most of the observed treatments are compound for both datasets meanwhile control pertubation are 7-8% for train test set respectively. We can say it's balanced between train test sets.\n\n>#### Treatment durations are commonly distributed with 48 hour ones slightly more than the rest. Again it's pretty balanced for both datasets.\n\n>#### Doses are evenly distributed, first dose is slightly more than D2 in both datasets.Both datasets are balanced.\n","e22236a7":"**Observations:**\n* 2774 drugs out of 3700 drugs have 6 rows that correspond to 2 doses and 3 treatment times. \n* Only 64 drugs have 12 samples, I was expecting more drugs to be profiled twice.\n* Only 3 drugs have 18 sample, the drugs were profiled 3 times.\n***\n### Conclusion of EDA:\n* Train and Test Set share Quite same Distribution.\n* Cell Features are highly correlation among themselves (can be used to create new features).\n* Data is quite skewed , so some statistical features can be also be created.\n* Drug_id can play an important role for cross-validaton strategy as explained by this [topic](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/195195).\nThe Cross Validation strategy i have used for submission is Multi-label stratified Kfold. \n* All Targets are highly correlative can be used for transfer learning.\n* About 71 Non-scoredtargets don't have mechanism of action so can be dropped while pre-training.\n***\n# Feature Engineering:\n> ## Adversarial Validation and Feature Importance:\n\nTo study about Adversarial Validation please refer --> [PAGE](https:\/\/towardsdatascience.com\/adversarial-validation-ca69303543cd) | \n[YouTube Video](https:\/\/www.youtube.com\/watch?v=7cUCDRaIZ7I&feature=youtu.be)\n\nCategorical features; `cp_type`, `cp_time` and `cp_dose` are omitted in adversarial validation. Only gene expression and cell viability features are used. Basically we going to replace our targets for both datasets (0 for train and 1 for test), then we going build a classifier which tries to predict which observation belongs to train and which one belongs to test set. If datasets randomly selected from similar roots it should be really hard for the classifier to separate them. But if there is systematic selection differences between train and test sets then classifier should be able to capture this trend. So we want our models score lower for the next section (0.50 AUC) because higher detection rate means higher difference between train and test datasets, so let's get started...","e4ca2a82":">### Tabnet-1 and Tabnet-2\n* About Tabnet: [Discussion-What can TabNet see!](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/194203)","aab48251":"* It seems we have 2 outliers here: **nfkb_inhibitor** and **proteasome_inhibitor.** \n\n* We can see many target labels in the plot: inhibitor, agonist, antagonist.\n***\n### Correlation between scored targets:","a5610416":"## Submission","d9873176":"## References:\n>#### Exploratory Data Analysis: \n* [Mechanisms of Action (MoA) Prediction - EDA](https:\/\/www.kaggle.com\/gunesevitan\/mechanisms-of-action-moa-prediction-eda)\n* [Drugs MoA classification: EDA](https:\/\/www.kaggle.com\/amiiiney\/drugs-moa-classification-eda)\n\n>#### Feature Engineering: \n* [Discussion](https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/192211)\n\n>#### Code: \n* [MoA-tabnet-Train+Inference](https:\/\/www.kaggle.com\/kushal1506\/moa-tabnet-train-inference)\n* [MoA|Pretrained Non-scored Targets as Meta-Features](https:\/\/www.kaggle.com\/kushal1506\/moa-pretrained-non-scored-targets-as-meta-features)\n* [Pytorch_Transfer_Learning](https:\/\/www.kaggle.com\/kushal1506\/pytorch-transfer-learning-last\/data) ","2526286b":"***\n## End Notes:\nKaggle always provide a lot of days for a competition which one can utilize to learn and grow.As Promised I have presented my first model,along with explanation,you can read spacy's documentation and Rohit singh's kernel as all the code comes from their.If you understand any part of code feel free to comment and ask,I will try to resolve it.\n\n#### Thank You Kaggle Community for the enormous love and appreciation  \n\n## I hope you Liked my kernel. An upvote is a gesture of appreciation and encouragement that fills me with energy to keep improving my efforts ,be kind to show one ;-)","a69f1660":"> ## Drug IDs:\n`Train_drug.csv` contains anonymous drug ids of every signature id in training set. There are 3289 unique drugs while there are 23814 unique signatures. This means some drugs are used more than 6 times (2 different doses x 3 different durations). This data can be useful for cross-validations and outlier detection.\n\n#### Most frequent drugs in the drug set:","9eb4712a":"* **One high correlation between g- features. This is something to be taken into consideration in feature engineering.**\n***\n\n\n# Targets *(MoA)*:","6433d51a":"## Reading data:","8036f36b":"***\n> ## Feature Scaling\n\nIt can seen from the prior visualizations that continuous features are between different ranges. Feature scaling can both improve model performance and speed up convergence at the same time.In this competition,`Quantile Transformer` Played an important role for scaling of GENE and CELL features reason being this method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.Zero centric data perform better in algorithms like PCA, on the other hand min max scaled data may perform better in neural networks.\n\nMin max scaler and standard scaler are heavily affected by outliers, however gaussian rank scaler yields more robust results. Gaussian rank scaled data have symmetrical tails unlike others.\n\nHere's a quick exciting video about Quantile Normalization-[YouTube Video](https:\/\/www.youtube.com\/watch?v=ecjN6Xpv6SE)","47499a05":">#### Label Smoothing:\n`Label smoothing`, in a nutshell, is a way to make our model more robust so that it generalizes well. \nIn this Competition,Label Smoothing played an important role in climbing the LB and improving our models.This is an Inference kernel so label smoothing is not used in this kernel.\n\nTo have a better Understanding about Label smoothing, please refer this [Page](https:\/\/medium.com\/@nainaakash012\/when-does-label-smoothing-help-89654ec75326).\n* Note:- Label Smoothing is applied on training loss function not on validation loss function.","1f027a1f":"Most of the targets have 0 correlation. It is worth recalling that the presence of active targets in the samples in very low (mainly 1 or 2 targets per sample).\n\nHowever, we notice some yellow dots *(high correlation)* between some targets. Let's have a closer look over these targets.\n\n### Scored Targets with the highest MoA correlation"}}