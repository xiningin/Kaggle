{"cell_type":{"8c18bb95":"code","296d7a37":"code","8a14a7a0":"code","8a83ac2a":"code","5bdf5742":"code","e25e118b":"code","186ba16b":"code","1b020903":"markdown","cac6a95f":"markdown","22ffed2b":"markdown","d64f2697":"markdown","3bd7ff4f":"markdown","489e771f":"markdown","2a5f50ac":"markdown","3514d340":"markdown","28b60b64":"markdown"},"source":{"8c18bb95":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom tqdm import tqdm","296d7a37":"from skimage.transform import resize\n#print(os.listdir(\"..\/input\/best-artworks-of-all-time\"))\nw, h = 300, 300\nprint(\"Reading all CSV files...\")\nroot_dir = \"..\/input\/best-artworks-of-all-time\/\"\nimages_dir = root_dir + 'images\/images\/'\n\n#Get all authors\ndf = pd.read_csv(root_dir + \"artists.csv\")\ndf.replace(' ', '_', regex=True, inplace=True)\nall_authors = list(df.name.values)\nall_paintings = list(df.paintings)\n\nfrom random import choice as rc\nfrom random import randint as ri\n\ndef read_input(n):\n    for _ in range(n):\n        path = 'nonexistent'\n        while not os.path.exists(path):\n            auth = rc(all_authors)\n            n = ri(1, all_paintings[all_authors.index(auth)])\n            path = images_dir + auth + '\/' + auth+'_'+str(n)+'.jpg'\n        #print('Current Author: %s\\tNumber: %i' %(auth, n), file=sys.stderr) #For debugging\n        image = plt.imread(path)\n        new_image = resize(image, (w, h), anti_aliasing=True)\n        new_image = new_image\/256\n        yield new_image.flatten()","8a14a7a0":"from tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\n\ndef adam_optimizer():\n    return Adam(lr=0.00005, beta_1=0.5)\n\ndef create_generator():\n    generator = Sequential()\n    generator.add(Dense(units=512, input_dim=100))\n    generator.add(LeakyReLU(0.2))\n    generator.add(Dense(units=1024))\n    generator.add(LeakyReLU(0.2))\n    generator.add(Dense(units=w*h*3, activation='sigmoid'))\n    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n    return generator\n\ng = create_generator()\ng.summary()\n\ndef create_discriminator():\n    discriminator=Sequential()\n    discriminator.add(Dense(units=1024,input_dim=w*h*3))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n    discriminator.add(Dense(units=512))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n    discriminator.add(Dense(units=256))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dense(units=1, activation='sigmoid'))\n    \n    discriminator.compile(loss='mse', optimizer=adam_optimizer())\n    return discriminator\n\nd = create_discriminator()\nd.summary()","8a83ac2a":"def create_gan(discriminator, generator):\n    discriminator.trainable=False\n    gan_input = Input(shape=(100,))\n    x = generator(gan_input)\n    gan_output= discriminator(x)\n    gan= Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss='binary_crossentropy', optimizer='adam')\n    return gan\ntest = create_gan(d,g)\ntest.summary()\ndel test, d, g","5bdf5742":"def plot_generated_images(epoch, generator, examples=20, dim=(5,4), figsize=(5,4), to_file=False):\n    noise= np.random.normal(loc=0, scale=1, size=[examples, 100])\n    generated_images = generator.predict(noise)\n    generated_images = generated_images.reshape(examples,w,h,3)\n    plt.figure(figsize=figsize)\n    for i in range(generated_images.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generated_images[i], interpolation='nearest')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.savefig('gan_generated_image %d.png' %epoch)\n    if to_file:\n        for i in range(examples):\n            fname = 'image_%i.jpg' %i\n            plt.imsave(fname, generated_images[i])\n","e25e118b":"def load_data(n=8):\n    x_train = np.array([x for x in read_input(n)])\n    y_train = np.array([1 for x in range(n)])\n    return x_train, y_train","186ba16b":"def training(epochs=1, batch_size=16):\n    # Creating GAN\n    generator= create_generator()\n    discriminator= create_discriminator()\n    gan = create_gan(discriminator, generator)\n    previous_batch, _ = load_data(batch_size)\n    for e in range(1,epochs+1 ):\n        print(\"Epoch %d\" %e)\n    \n        #Loading the data\n        #To increase the number of samples, we load new data for every epoch\n        X_train, y_train = load_data(batch_size)\n        batch_count = X_train.shape[0] \/ batch_size\n    \n        for _ in tqdm(range(batch_size)):\n        #generate  random noise as an input  to  initialize the  generator\n            noise= np.random.normal(0,1, [batch_size, 100])\n            \n            # Generate fake MNIST images from noised input\n            generated_images = generator.predict(noise)\n            \n            # Get a random set of  real images\n            image_batch = X_train\n            \n            #Construct different batches of  real and fake data \n            #print(image_batch, file=sys.stderr)\n            #print(image_batch.shape, generated_images.shape, file=sys.stderr)\n            if len(image_batch.shape) != len(generated_images.shape):\n                print('Error in shape!')\n                image_batch = previous_batch\n            else:\n                previous_batch = image_batch\n            X= np.concatenate([image_batch, generated_images])\n            \n            # Labels for generated and real data\n            y_dis=np.zeros(2*batch_size)\n            y_dis[:batch_size]=1.0\n            \n            #Pre train discriminator on  fake and real data  before starting the gan. \n            discriminator.trainable=True\n            discriminator.train_on_batch(X, y_dis)\n            \n            #Tricking the noised input of the Generator as real data\n            noise= np.random.normal(0,1, [batch_size, 100])\n            y_gen = np.ones(batch_size)\n            \n            # During the training of gan, \n            # the weights of discriminator should be fixed. \n            #We can enforce that by setting the trainable flag\n            discriminator.trainable=False\n            \n            #training  the GAN by alternating the training of the Discriminator \n            #and training the chained GAN model with Discriminator\u2019s weights freezed.\n            gan.train_on_batch(noise, y_gen)\n            \n        if e%25==0:# e == 1 or e % 5 == 0 or e:\n            plot_generated_images(e, generator)\n    #We will now save a full size image batch of the test\n    plot_generated_images(0, generator, 20, (5,4), (5,4), True)\n    #Now, we'll save our models to preserve them\n    #generator.save('generator.h5')\n    #discriminator.save('discriminator.h5')\n    #gan.save('GAN.h5')\n\ntraining(4000, 16)","1b020903":"<h1>Training<\/h1>\nAnd then, Define and call the function to train our Neural Networks!","cac6a95f":"<h1>Verification<\/h1>\nTo further see the results of the training, we will need to view a randomized image by the generator. Thus, we need this function to do this for us.","22ffed2b":"<h1>Network Creation<\/h1>\nNow, we have read a part of the whole data. We will be needing the other artworks for further training, but I will not cover it in this Notebook.\n\nNext, we would be declaring our Generator and Discriminator Networks. Since our input image are all resized to 512x512x3 (that is width x height x RGB) pixels, the output of the generator should also be 512x512x3.","d64f2697":"Hello Eveyone~! This is your friendly neighborhood programmer, Seraph, about to challenge a type of Neural Network called a Generative Adversarial Network or GAN for short.\n\nIn this Notebook, we will be doing a test to see if a GAN network will be able to recreate, or probably, overcome, renowned artists' skilled paintings and artworks.\n\nThen, without further ado, let's begin!","3bd7ff4f":"Credits to Renu Khandelwal for some of the codes\nhttps:\/\/medium.com\/datadriveninvestor\/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3","489e771f":"<h1> Read and Load Data <\/h1>\nFirst, we need to read the datasets and prepare them for feeding into the NN. We would only need the images as we will be developing a GAN.","2a5f50ac":"Here, since the Kaggle notebook has a limited RAM of about 16GB, we will be resizing our images to 256x256 to minimize the RAM usage, then, we'll turn the RGB values into the range of (-128, 128) by subtracting 128 from all values.\n\nFurthermore, to save on RAM, we will turn the input getter into a GENERATOR that yields values only when needed.","3514d340":"Finally, we'll be doing the Training! But first, we need to create a function that loads our data into the Network.","28b60b64":"Next, we will be declaring our GAN function"}}