{"cell_type":{"9ebaf8be":"code","49ea5e0e":"code","5e9f1b5b":"code","b82df234":"code","33f4d686":"code","54076a62":"code","65bd6058":"code","5ae9b3b8":"code","1b7b8c5f":"code","564e5a12":"code","23f3daf6":"code","ac6fcac6":"code","280151e0":"code","0120285f":"code","4fda29ba":"code","a11d6f60":"code","322ce985":"code","dcc16c9b":"code","a620e7dc":"code","c4ffa5b9":"code","ebd524f5":"code","59f68f9a":"code","65f8cc9f":"code","2ee45dec":"markdown","3ff4562c":"markdown","a9dcbf5a":"markdown","5e2ca7eb":"markdown","874be859":"markdown"},"source":{"9ebaf8be":"import os\nimport gc\nimport sys\nimport time\nimport shutil\n\nimport random\nimport pickle\n\nfrom ast import literal_eval\nfrom tqdm import tqdm as print_progress\nfrom glob import glob\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","49ea5e0e":"os.environ['TF_KERAS'] = '1'\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","5e9f1b5b":"from tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal\nfrom tensorflow.keras.utils import plot_model","b82df234":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","33f4d686":"pip install stellargraph","54076a62":"pip install gradient-centralization-tf","65bd6058":"label_encoder = pickle.load(open('..\/input\/hotel-comment\/label_encoder.pkl', 'rb'))\nlabels = list(label_encoder.classes_)\nlen(labels)","5ae9b3b8":"labels_matrix = np.load('..\/input\/we-for-magnet-trainset-1\/labels_embeddings.npy')\nlabels_matrix.shape","1b7b8c5f":"import sklearn\nfrom ast import literal_eval\nfrom tensorflow.keras.utils import Sequence, to_categorical\n\n\nclass DataGenerator(Sequence):\n\n    def __init__(self,\n                 data_root,\n                 labels_fixed: np.array,\n                 max_seq_len: int=512,\n                 batch_size: int = 256, \n                 shuffle: bool = True):\n        \n        self.data_root = data_root\n        self.num_labels = labels_fixed.shape[-2]\n        self.max_seq_len = max_seq_len\n        self.labels_fixed = labels_fixed\n        self.embedding_dim = labels_fixed.shape[-1]\n        \n        # list of files containing both word-embeddings and multi-labels\n        if isinstance(self.data_root, str):\n            self.files = glob(os.path.join(self.data_root, 'sample_*.npz'))\n        elif isinstance(self.data_root, (list, tuple)):\n            self.files = []\n            for data_dir in self.data_root:\n                self.files += glob(os.path.join(data_dir, 'sample_*.npz'))\n                \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.indices = np.array(list(range(len(self.files))))\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"\n        Denotes the number of batches per epoch\n        \"\"\"\n        n_samples = len(self.files)\n        return n_samples\/\/self.batch_size + (0 if n_samples%self.batch_size==0 else 1)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate one batch of data\n        \"\"\"\n        # Generate indexes of the batch\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)\n        indices = self.indices[start_index:end_index]\n\n        # Generate data\n        wb_batch = []\n        mtl_batch = []\n        for idx in indices:\n            \n            sample_file = self.files[idx]\n            \n            # Load word embeddings\n            wb_pad = np.zeros((self.max_seq_len, self.embedding_dim))\n            wb = np.load(sample_file)['emb']\n            wb_pad[:wb.shape[0],:] = wb\n            wb_batch += [wb_pad]\n            \n            # Load multi-labels\n            mtl = np.load(sample_file)['mtl']\n            mtl = mtl[:self.num_labels]\n            mtl_batch += [self.smooth_labels(mtl)]\n        return [np.array(wb_batch), self.labels_fixed], np.array(mtl_batch)\n\n    def smooth_labels(self, labels, factor=0.1):\n        # smooth the labels\n        labels *= (1 - factor)\n        labels += (factor \/ labels.shape[-1])\n        return labels\n\n    def on_epoch_end(self):\n        \"\"\"\n        Update indices after each epoch\n        \"\"\"\n        if self.shuffle:\n            self.indices = sklearn.utils.shuffle(self.indices)","564e5a12":"dset_paths = [\n    ['..\/input\/we-for-magnet-trainset-1\/training', \n     '..\/input\/we-for-magnet-trainset-2\/training', \n     '..\/input\/we-for-magnet-trainset-3\/training', \n     '..\/input\/we-for-magnet-trainset-4\/training', \n     '..\/input\/we-for-magnet-trainset-8\/training',],\n    ['..\/input\/we-for-magnet-valset-1\/valuating', \n     '..\/input\/we-for-magnet-valset-2\/valuating',],\n]\ndata_generator = dict()\nfor dataset, dset_path in zip(['training', 'valuating'], dset_paths):\n    data_generator[dataset] = DataGenerator(data_root=dset_path, \n                                            labels_fixed=labels_matrix, \n                                            batch_size=128, \n                                            shuffle=True if dataset=='training' else False)","23f3daf6":"len(data_generator['training']), len(data_generator['valuating'])","ac6fcac6":"X = data_generator['training'][0]\nprint(X[0][0].shape)\nprint(X[0][1].shape)\nprint(X[1].shape)","280151e0":"class CyclicLR(Callback):\n    \"\"\"\n    This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with some constant frequency, \n        as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.\n    \n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"halving\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exponential\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n\n    For more detail, please read the paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {original, halving, exponential}.\n            Default 'original'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n    def __init__(self, base_lr=0.001, max_lr=0.1, step_size=2000., mode='original',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'halving':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exponential':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n            else:\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None, new_step_size=None):\n        \"\"\"\n        Resets cycle iterations.\n            Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        new_lr = self.clr()\n        K.set_value(self.model.optimizer.lr, new_lr)","0120285f":"class Adjacency(Layer):\n\n    def __init__(self, nodes=1, weights=None, init_method='identity'):\n        super(Adjacency, self).__init__()\n\n        self.shape = (1, nodes, nodes)\n\n        if weights is not None:\n            assert weights.shape==(nodes, nodes), \\\n                f'Adjacency Matrix must have shape ({nodes}, {nodes})' + \\\n                f' while its shape is {weights.shape}'\n            w_init = tf.convert_to_tensor(weights)\n        else:\n            init_method = init_method.lower()\n            if init_method == 'identity':\n                initializer = tf.initializers.Identity()\n            elif init_method in ['xavier', 'glorot']:\n                initializer = tf.initializers.GlorotNormal()\n            w_init = initializer(shape=(nodes, nodes))\n\n        self.w = tf.Variable(\n            initial_value=tf.expand_dims(w_init, axis=0), \n            dtype=\"float32\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.convert_to_tensor(self.w)\n\n    def compute_output_shape(self):\n        return self.shape","4fda29ba":"import gctf\nfrom stellargraph.layer import GraphAttention\nfrom stellargraph.utils import plot_history\n\n\ndef buil_MAGNET(n_labels,\n                embedding_dim: int,\n                sequence_length: int=512, \n                lstm_units: int=64,\n                dropout_rates=[0.3, 0.2],\n                attention_heads=[4, 2],\n                adjacency_matrix=None,\n                adjacency_generation='xavier', # 'identity' or 'xavier' or 'glorot'\n                feed_text_embeddings=True, # if False, add additional Embedding layer\n                text_embeddings_matrix=None, # initialized weights for text Embedding layer\n                feed_label_embeddings=True, # if False, add additional Embedding layer\n                label_embeddings_matrix=None, # initialized weights for label Embedding layer\n                ) -> Model:\n\n    if isinstance(attention_heads, int):\n        attention_heads = [attention_heads, attention_heads]\n    if not isinstance(attention_heads, (list, tuple)):\n        raise ValueError('`attention_heads` must be INT, LIST or TUPLE')\n\n    # 1. Sentence Representation\n    if feed_text_embeddings:\n        sentence_model = Sequential(name='sentence_model')\n        sentence_model.add(Dropout(dropout_rates[0], input_shape=(sequence_length, embedding_dim), name='word_embeddings'))\n        word_inputs, word_embeddings = sentence_model.inputs, sentence_model.outputs\n    else:\n        word_inputs = Input(shape=(sequence_length, ), name='word_inputs')\n        embedding_args = {\n            'input_dim': sequence_length,\n            'output_dim': embedding_dim,\n            'name': 'word_embeddings'\n        }\n        if text_embeddings_matrix is not None \\\n            and text_embeddings_matrix.shape==(sequence_length, embedding_dim):\n            embedding_args['weights'] = [text_embeddings_matrix]\n        word_embeddings = Embedding(**embedding_args)(word_inputs)\n        word_embeddings = Dropout(dropout_rates[0], name='WE_dropout')(word_embeddings)\n    \n    forward_rnn = LSTM(units=lstm_units, return_sequences=True, name='forward_rnn')\n    backward_rnn = LSTM(units=lstm_units, return_sequences=True, name='backward_rnn', go_backwards=True)\n    bidir_rnn = Bidirectional(layer=forward_rnn, backward_layer=backward_rnn, merge_mode=\"concat\", name='bidir_rnn')\n    \n    sentence_repr = bidir_rnn(word_embeddings)\n    sentence_repr = K.mean(sentence_repr, axis=1)\n    # print(f\"sentence_repr: {K.int_shape(sentence_repr)}\")\n\n    # 2. Labels Representation\n    if feed_label_embeddings:\n        label_inputs = Input(batch_shape=(1, n_labels, embedding_dim), name='label_embeddings')\n        label_embeddings = label_inputs\n    else:\n        label_inputs = Input(batch_shape=(1, n_labels), name='label_inputs')\n        embedding_args = {'input_dim': n_labels,\n                          'output_dim': embedding_dim,\n                          'name': 'label_embeddings'}\n        if label_embeddings_matrix is not None \\\n            and label_embeddings_matrix.shape==(n_labels, embedding_dim):\n            embedding_args['weights'] = [label_embeddings_matrix]\n        label_embeddings = Embedding(**embedding_args)(label_inputs)\n        label_embeddings = Dropout(rate=dropout_rates[0], name='LE_dropout')(label_embeddings)\n    label_embeddings = Dense(units=embedding_dim\/\/2, name='label_embeddings_reduced')(label_embeddings)\n    # print(f\"label_inputs: {K.int_shape(label_inputs)}\")\n\n    label_correlation = Adjacency(nodes=n_labels, \n                                  weights=adjacency_matrix,\n                                  init_method=adjacency_generation)(label_embeddings)\n    # print(f\"label_correlation: {K.int_shape(label_correlation)}\")\n\n    label_attention = GraphAttention(units=embedding_dim\/\/2\/\/attention_heads[0],\n                                     activation='tanh',\n                                     attn_heads=attention_heads[0],\n                                     in_dropout_rate=dropout_rates[1],\n                                     attn_dropout_rate=dropout_rates[1], )([label_embeddings, label_correlation])\n    # print(f\"label_attention: {K.int_shape(label_attention)}\")\n\n    label_residual = Add(name='label_residual')([label_attention, label_embeddings])\n    # print(f\"label_residual: {K.int_shape(label_residual)}\")\n\n    label_repr = GraphAttention(units=2*lstm_units,\n                                activation='tanh',\n                                attn_heads_reduction='average',\n                                attn_heads=attention_heads[1],\n                                in_dropout_rate=dropout_rates[1],\n                                attn_dropout_rate=dropout_rates[1], )([label_residual, label_correlation])\n\n    label_repr = K.sum(label_repr, axis=0, keepdims=False)\n    # print(f\"label_repr: {K.int_shape(label_repr)}\")\n\n    # 3. Prediction\n    prediction = tf.einsum('Bk,Nk->BN', sentence_repr, label_repr)\n    prediction = sigmoid(prediction)\n    # print(f\"prediction: {K.int_shape(prediction)}\")\n\n    return Model(inputs=[word_inputs, label_inputs], outputs=prediction, name='MAGNET')","a11d6f60":"def noam_scheme(global_step, init_lr, warmup_steps=16):\n    \"\"\"\n    Noam scheme learning rate decay\n        init_lr: (scalar) initial learning rate. \n        global_step: (scalar) current training step\n        warmup_steps: (scalar) During warmup_steps, learning rate increases until it reaches init_lr.\n    \"\"\"\n    step = tf.cast(global_step+1, dtype=tf.float32, name=\"global_step\")\n    return init_lr * (warmup_steps**0.5) * tf.minimum(step*(warmup_steps**-1.5), step**-0.5)","322ce985":"def weighted_cross_entropy(y_true, y_pred, pos_weight=1.69):\n    losses = y_true * -K.log(y_pred) * pos_weight + (1-y_true) * -K.log(1-y_pred)\n    losses = K.clip(losses, 0.0, 11.27)\n    return K.mean(losses)","dcc16c9b":"import gctf\nfrom stellargraph.utils import plot_history\n\n\nclass MAGNET:\n\n    def __init__(self, n_labels: int, embedding_dim: int, model_ckpt=None):\n\n        self.embedding_dim = embedding_dim\n\n        # Build model(s)\n        print(f\"\\n\\n\\nBuilding MAGNET ...\\n\\n\\n\")\n        self.model = buil_MAGNET(n_labels, embedding_dim=embedding_dim, sequence_length=512, lstm_units=32)\n        self.model.summary()\n        \n        # Load weights\n        if model_ckpt:\n            try:\n                print(f\"Try to load checkpoint @{model_ckpt} ...\")\n                self.model.load_weights(model_ckpt)\n            except:\n                print(f\"\\t==> Loading Fail !!!\")\n            \n    def compile(self, model_saved: str, logs_path: str, schedule_step: int, verbose: int=1):\n                    \n        # Compile optimizer, loss & metric functions\n        print(f\"Compiling MAGNET using \\n\\tgrad-centralized ADAM, \\n\\ttop-k Accuracy, \\n\\tweighted Cross-Entropy \\n...\")\n        self.model.compile(optimizer=gctf.optimizers.adam(learning_rate=0.00169), \n                           # optimizer=Adam(learning_rate=0.001), \n                           metrics=[\"accuracy\", TopKCategoricalAccuracy(k=3)],\n                           loss=weighted_cross_entropy)\n\n        # Define Callbacks\n        return [\n            # TensorBoard(log_dir=logs_path),\n            # ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=verbose),\n            CyclicLR(mode='exponential', base_lr=1e-7, max_lr=1e-3, step_size=schedule_step),\n            ModelCheckpoint(filepath=model_saved, monitor='accuracy', save_weights_only=True, save_best_only=False, save_freq='epoch'),\n            # LearningRateScheduler(noam_scheme),\n            # EarlyStopping(monitor='val_accuracy', mode='max', restore_best_weights=True, min_delta=1e-7, patience=7, verbose=verbose),\n        ]\n\n    def finetune(self, train_generator, val_generator, model_saved: str, logs_path: str, n_loops: int=3, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator) \/\/ 2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Define part(s) of layers for fine-tuning\n        graph_layers = ['adjacency', 'graph_attention', 'graph_attention_1']\n        word_layers = ['bidir_rnn', 'label_embeddings_reduced']\n        train_histories = []\n\n        ######################################\n        #             FINE-TUNING            #\n        ######################################\n\n        print(f\"[Fine-tuning MAGNET]\")\n        train_args = {\n            'generator': train_generator,\n            'steps_per_epoch': len(train_generator),\n            'validation_data': val_generator,\n            'validation_steps': len(val_generator),\n            'callbacks': custom_callbacks\n        }\n        for l in range(n_loops):\n            \n            print(f\"Training loop {l+1}\")\n\n            # Step 1: Train ALL layers\n            for layer in self.model.layers:\n                layer.trainable = True\n\n            print(f\"\\tStep 1: Training ALL layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*10, epochs=l*10+3, **train_args)\n            train_histories.append(train_history)\n\n            # Step 2: Train GRAPH layers\n            for layer in self.model.layers:\n                layer.trainable = True if layer.name in graph_layers else False\n\n            print(f\"\\tStep 2: Training GRAPH layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*10+3, epochs=l*10+5, **train_args)\n            train_histories.append(train_history)\n\n            # Step 3: Train EMBEDDING layers\n            for layer in self.model.layers:\n                layer.trainable = True if layer.name in word_layers else False\n\n            print(f\"\\tStep 3: Training EMBEDDING layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*10+5, epochs=l*10+7, **train_args)\n            train_histories.append(train_history)\n\n            # Step 4: Train ALL layers\n            for layer in self.model.layers:\n                layer.trainable = True\n\n            print(f\"\\tStep 4: Training ALL layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*10+7, epochs=l*10+10, **train_args)\n            train_histories.append(train_history)\n\n            # Reduce learning rate\n            # custom_callbacks[0].base_lr \/= 1.69\n            # custom_callbacks[0].max_lr \/= 1.69\n\n        return train_histories\n\n    def train(self, train_generator, val_generator, \n                    model_saved: str, logs_path: str,\n                    max_epochs: int=50, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator) \/\/ 2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Training\n        train_history = self.model.fit_generator(generator=train_generator,\n                                                 steps_per_epoch=len(train_generator),\n                                                 validation_data=val_generator,\n                                                 validation_steps=len(val_generator),\n                                                 callbacks=custom_callbacks, \n                                                 epochs=max_epochs,\n                                                 initial_epoch=0)\n        return train_history\n\n    def load_weights(self, weight_path: str):\n        self.model.load_weights(weight_path)\n\n    def predict(self, label_embeddings: np.array, sent_embeddings: np.array):\n        sent_embeddings = np.reshape(sent_embeddings, (1, 512, self.embedding_dim))\n        preds = self.model.predict([sent_embeddings, label_embeddings]).tolist()\n        return preds[0]","a620e7dc":"N_LABELS, embedding_dim = labels_matrix.shape[-2:]\n\nmodel = MAGNET(n_labels=N_LABELS, \n               embedding_dim=embedding_dim, \n               model_ckpt='..\/input\/magnet-distiluse-checkpoints\/catCE\/ep025_acc0.432_val_acc0.471_topk0.733_val_topk0.779.h5')","c4ffa5b9":"models_path = '\/kaggle\/working\/models'\nif not os.path.isdir(models_path):\n    os.makedirs(models_path)\n\nlogs_path = '\/kaggle\/working\/logs'\nif not os.path.isdir(logs_path):\n    os.makedirs(logs_path)\n    \npred_dir = '\/kaggle\/working\/predictions'\nif not os.path.isdir(pred_dir):\n    os.makedirs(pred_dir)\n    \nmodel_format = 'ep={epoch:03d}_acc={accuracy:.3f}_val_acc={val_accuracy:.3f}_topk={top_k_categorical_accuracy:.3f}_val_topk={val_top_k_categorical_accuracy:.3f}.h5'\nmodel_saved =  os.path.join(models_path, model_format)","ebd524f5":"train_histories = model.finetune(data_generator['training'], \n                                 data_generator['valuating'], \n                                 model_saved=model_saved, \n                                 logs_path=logs_path, \n                                 n_loops=1, \n                                 verbose=1)","59f68f9a":"for t_i, train_history in enumerate(train_histories):\n    with open(f'train_history_{t_i}.hst', 'wb') as f_writer:\n        pickle.dump(train_history.history, f_writer)","65f8cc9f":"os.chdir(r'\/kaggle\/working')\ndir_path = '\/kaggle\/working\/'\nshutil.make_archive(dir_path+\"data\", 'zip', dir_path)","2ee45dec":"# **Load Model**","3ff4562c":"# **Train**","a9dcbf5a":"# **Load data**","5e2ca7eb":"# **Import Libraries**","874be859":"# **Data Generator**"}}