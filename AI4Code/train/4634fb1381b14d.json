{"cell_type":{"d024baa4":"code","3433ec36":"code","4b4736ca":"code","ded017b4":"code","608ec2c1":"code","5484f710":"code","47260a8c":"code","be404cef":"code","6e31eefb":"code","6e50f65a":"code","5e0a1d6f":"code","1b941779":"code","936e5d51":"code","42a9be6a":"code","7bc7dc48":"code","057c3f25":"code","56060252":"code","fc0d45d7":"code","ff116b38":"code","d6ecb598":"code","7ab56744":"code","eb9c94fe":"code","4fb843ec":"code","bb0efcdc":"code","42a4cfb8":"code","5a86c077":"code","3b1e7199":"code","d5c30745":"code","406f25f5":"code","20972951":"code","b45c3f64":"code","382ee7da":"code","3d537a90":"code","17b8f6d7":"code","8e9ef0b5":"code","39247512":"code","6f610aa7":"code","33d738dc":"code","1b93d15a":"code","422a1c37":"code","663b40c7":"code","2c58e068":"markdown","01d486fc":"markdown","f417e9fb":"markdown","f560ab05":"markdown","96deb7ca":"markdown","ca523fdc":"markdown","e1ac2212":"markdown","5838bcb9":"markdown","335d0004":"markdown","bcc1f9c4":"markdown","4d6e7c2e":"markdown","6cb15dbf":"markdown","0e757907":"markdown","e61c4849":"markdown","7f3c4b91":"markdown","ae6550f1":"markdown","144d27a0":"markdown","0deace5d":"markdown","e3943e10":"markdown","1935f33d":"markdown","3f1fab88":"markdown","dcfbf49d":"markdown","5266cfe6":"markdown","e2567436":"markdown","a2567add":"markdown","6e62ac5d":"markdown","d2b2b8c6":"markdown","040db261":"markdown","839b659b":"markdown","703030d0":"markdown","ac1838fb":"markdown"},"source":{"d024baa4":"# Bread & butter tool for ML & DL\nimport tensorflow as tf\n\n# basic math ops & data handling\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"dark_background\")","3433ec36":"(X_train, y_train,), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# scale the values to be within [0, 1] \n# since max pixel value is 255\nX_train = X_train \/ 255.0\nX_test = X_test \/  255.0\n\n# printing out the shapes\nX_train.shape, X_test.shape","4b4736ca":"# clear residual layers from memory\ntf.keras.backend.clear_session()\n\n# creating the input layer\ninp = tf.keras.layers.Input(shape=(28, 28))\n\n# flatten the inputs \nflat_inp = tf.keras.layers.Flatten()(inp)\n\n# Dense layers: we use ELU activation & appropriate kernel Init\nhidden1 = tf.keras.layers.Dense(units=64, activation='elu', kernel_initializer='he_normal')(flat_inp)\nhidden2 = tf.keras.layers.Dense(units=64, activation='elu', kernel_initializer='he_normal')(hidden1)\n\n# We concatenate the first dense layer with output from second dense layer \nconc = tf.keras.layers.Concatenate()([flat_inp, hidden2])\n\n# drop out layer, to prevent overfitting\ndropout1 = tf.keras.layers.Dropout(0.5)(conc)\n\n# final hidden dense layer\nhidden3  = tf.keras.layers.Dense(units=128, activation='elu', kernel_initializer='he_normal')(dropout1)\n\n# the output layer making predictions\nop = tf.keras.layers.Dense(10, activation='softmax')(hidden3)\n\n# create the functional api model\nmodel = tf.keras.models.Model(name='Deep_N_Wide_Model', inputs=[inp], outputs=[op])\n\n# compile the model\nmodel.compile(\n    loss='sparse_categorical_crossentropy', \n    optimizer=tf.keras.optimizers.Adam(0.001),\n    metrics=tf.keras.metrics.SparseCategoricalAccuracy(name='SCAcc')\n)\n\n# let's print out the summary of model\nmodel.summary()","ded017b4":"tf.keras.utils.plot_model(model, show_shapes=True)","608ec2c1":"hist = model.fit(\n    X_train, y_train, \n    validation_data=(X_test, y_test),\n    \n    # combining Early stopping with max epochs \n    # to arrive at best possible model\n    epochs=100,\n    callbacks=tf.keras.callbacks.EarlyStopping(\n        patience=2, monitor='val_SCAcc', mode='max'),\n    \n    # let's NOT print all the clutter\n    # If you don't have the patience to \n    # wait for long, set this to 1 to see \n    # live model training stats\n    verbose=0,\n)\n\nprint (\n    \"Performance Summary after {} epochs:\\n\\\n     Best Train Accuracy reached: {:.2f}\\n\\\n     Best Val Accuracy reached: {:6.2f}\".format(\n        len(hist.epoch), \n        hist.history['SCAcc'][-1], \n        hist.history['val_SCAcc'][-1])\n)","5484f710":"# visualise the models training performance\nhist_df = pd.DataFrame(hist.history)\n\nf, ax = plt.subplots(ncols=2, figsize=(20, 10))\nax = ax.ravel()\n\nhist_df[['loss', 'val_loss']].plot(\n    ylim=[0, 0.15], xlim=[0, len(hist.epoch)],\n    xlabel='Epochs', ylabel='Loss',\n    title='Epoch vs Loss', ax=ax[0])\n\nhist_df[['SCAcc', 'val_SCAcc']].plot(\n    ylim=[0.95, 1], xlim=[0, len(hist.epoch)], \n    xlabel='Epochs', ylabel='Accuracy',\n    title='Epoch vs Accuracy', ax=ax[1]);","47260a8c":"class my_cross_entropy(tf.keras.losses.Loss):\n    '''Cross entropy loss, loss layers inherit from keras.losses.loss class.\n    Requires one hot encoded targets (categorical cross entropy)'''\n    \n    def __init__(self, epsilon=1e-12, **kwargs):\n        '''You would see super().__init__(**kwargs) in almost every custom \n        object we would be building we do this, so that tf.keras takes care \n        of handling methods & attributes we may have failed to overload ourselves \n        (it's like setting to default behavior of that class)'''\n        super().__init__(**kwargs)\n        \n        # we use epsilon, a tiny miniscule value\n        # to prevent the loss from becoming NAN\n        self.epsilon = epsilon\n    \n    def call(self, y_true, y_pred):\n        '''The main function of loss class that determines what the layer is supposed to do.\n        Remember to use only `tf.keras.Backend` functions or tf functions. Others would slow \n        down the model terribly since internally all layers are converted to graphs. '''\n        \n        pred = tf.clip_by_value(y_pred, self.epsilon, 1.0 - self.epsilon)\n        N = pred.shape[0]\n        ce = -tf.reduce_sum(tf.math.log(pred) * y_true) \/ N\n        \n        return ce\n        \n    def get_config(self):\n        '''This is a must if we hope to save our model someday. \n        tf.keras.models.Model.save() requires this. Any new values we \n        had used that needs saving comes here. We over ride the super class\n        config method and add our custom objects here.'''\n        \n        return {**super().get_config(), \"epsilon\": self.epsilon}","be404cef":"class my_s_cross_entropy(tf.keras.losses.Loss):\n    '''Same as above but sparse_crossentropy loss.\n    Takes in sparse targets (spare_categorical_crossentropy).'''\n    \n    def __init__(self, epsilon=1e-12, **kwargs):\n        'Same as above, override parent class, define new variables here'\n        super().__init__(**kwargs)\n        self.epsilon = epsilon\n    \n    def call(self, y_true, y_pred):\n        'Function that determines the behaviour of the layer'\n        \n        # one hot encode the targets and squeeze any dimensions of value 1\n        y_true = tf.squeeze(tf.one_hot(y_true, depth=y_pred.shape[-1]))\n        \n        # exactly copied from above loss snippet\n        pred = tf.clip_by_value(y_pred, self.epsilon, 1.0 - self.epsilon)\n        N = pred.shape[0]\n        \n        # we add the epsilon value since the model loss \n        # hits NAN when trained for longer periods\n        ce = -tf.reduce_sum(tf.math.log(pred + 1e-9) * y_true) \/ N\n        \n        return ce\n        \n    def get_config(self):\n        'Override to save any custom objects we had made'\n        return {**super().get_config(), \"epsilon\": self.epsilon}","6e31eefb":"# assume to be the model's predictions\npredictions = np.array([[0.05 , 0.95, 0],\n                        [0.1, 0.8, 0.1]], dtype=np.float32)\n\n# sparse targets (actual)\ns_targets = np.array([[1], [2]], dtype=np.float32)\n\n# one hot encoded targets (actual)\ntargets = np.array([[0., 1., 0.], [0., 0., 1.]], dtype=np.float32)","6e50f65a":"cce = tf.keras.losses.CategoricalCrossentropy()\nprint (\"Keras categorical_cross_entropy:\", cce(targets, predictions).numpy())\nprint (\"Our categorical_cross_entropy:\", my_cross_entropy()(targets, predictions).numpy())","5e0a1d6f":"sce = tf.keras.losses.SparseCategoricalCrossentropy()\nprint (\"Keras sparse_categorical_cross_entropy:\", sce(s_targets, predictions).numpy())\nprint (\"Our sparse_categorical_cross_entropy:\", my_s_cross_entropy()(s_targets, predictions).numpy())","1b941779":"class my_scacc_metric(tf.keras.metrics.Metric):\n    '''To create a metric, we need to override the tf.keras.metrics.Metric class.\n    Here we are creating a stateful metric, which keeps updating itself with streaming scores.\n    No new ARGS to init, hence get_config is not needed here.\n    \n    Note: Even though we create new varibles in it, we don't require saving metrics. \n    If you ever need to save the metric, say a special value which the metric uses, \n    then simply create a get_config() and override it.       \n    '''\n    \n    def __init__(self, **kwargs):\n        '''Init as before, notice we have no new arguments we are explictly \n        overriding or using. It is implictly passed as **kwargs to be handled by super.\n        '''\n        super().__init__(**kwargs)\n        \n        # intialize total & count to calculate the mean when required\n        self.total = self.add_weight(\"total\", initializer='zeros')\n        self.count = self.add_weight(\"count\", initializer='zeros')\n        \n    def update_state(self, y_true, y_pred, sample_weights=None):\n        '''Gets called inside the loop for EVERY STEP OF EPOCH. We compute the metric\n        & update the total values so that later on we can utilize them. \n        \n        This makes more sense for metrics such as precision.'''\n        \n        # calculate Sparse Categrical accuracy\n        metrics = self.calc_SCAcc(y_true, y_pred)\n        \n        # update the total scores along with count for later purpose\n        self.total.assign_add(tf.reduce_sum(metrics))\n        self.count.assign_add(tf.cast(tf.size(y_true), dtype=tf.float32))\n    \n    def calc_SCAcc(self, y_true, y_pred):\n        '''Helper function to compute the metrics given a batch.'''\n        \n        scores = tf.cast(tf.equal(\n            tf.cast(tf.reduce_max(y_true, axis=-1), dtype=tf.float32), \n            tf.cast(tf.argmax(y_pred, axis=-1), dtype=tf.float32),\n        ), dtype=tf.float32)\n        \n        return scores\n    \n    def result(self):\n        '''Gets called at the END of each epoch, write the function accordingly. \n        Here, it calculates the mean.'''\n        \n        # I am fully aware that we could use a simple tf.reduce_mean(metrics) :)\n        return self.total \/ self.count","936e5d51":"targets = [[2], [1]]\npredictions = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\ntemp = my_scacc_metric(), tf.keras.metrics.SparseCategoricalAccuracy()\ntemp[0].update_state(targets, predictions)\ntemp[1].update_state(targets, predictions)\n\n# Our metric Vs tf.keras\ntemp[0].result().numpy(), temp[1].result().numpy()","42a9be6a":"class my_elu(tf.keras.layers.Layer):\n    def __init__(self, alpha=1., **kwargs):\n        '''As always contains the super class init. Or any new attribute defined comes here.\n        This is our custom implementation of exponential linear unit function.'''\n        \n        super().__init__(**kwargs)\n        self.alpha = alpha\n        \n    def call(self, z):\n        'Call defines the behaviour of the layer.'\n        \n        z = tf.cast(z, dtype=tf.float32)\n        return tf.where(z > 0, z, self.alpha * (tf.exp(z) - 1))\n            \n    def get_config(self):\n        '''Required only when we need to save config of our custom model & \n        when we have defined new attributes in init that needs saving'''\n        \n        return {**super().get_config(), \"alpha\": self.alpha}","7bc7dc48":"temp = tf.constant([-1., -9., 10., 20.])\n\n# ours vs tf.keras\nmy_elu()(temp).numpy(), tf.keras.layers.Activation('elu')(temp).numpy()","057c3f25":"def my_softmax(z, axis=-1):   \n    '''As stated before, we dont have to over ride classes all the time. \n    Sometimes a simple python function written with tf functions would suffice.'''\n    \n    # calculate the number of dimensions\n    # ndim = len(tf.shape(z))\n    ndim = z.ndim\n    \n    # we compute only with dimensions are >= 2, throw error otherwise\n    if ndim >= 2:\n        e = tf.exp(z - tf.reduce_max(z, axis=axis, keepdims=True))\n        s = tf.reduce_sum(e, axis=axis, keepdims=True)\n        return e \/ s\n    \n    else:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D. '\n                         'Received input: %s' % z)","56060252":"temp = tf.constant([[1., 2., 3., 6.], [2., 4., 5., 6.], [3., 8., 7., 6.]])\n\n# Custom Vs tf.keras\nmy_softmax(temp).numpy(), tf.keras.activations.softmax(temp).numpy()","fc0d45d7":"def my_glorot_init(shape, dtype=tf.float32):\n    'Simple python function suffices'\n    \n    stddev = tf.sqrt(2. \/ (shape[0] + shape[1]))\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)","ff116b38":"def my_he_init(shape, dtype=tf.float32):\n    'Simple python function suffices'\n    \n    stddev = tf.sqrt(2. \/ shape[0])\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)","d6ecb598":"# works without throwing any errors?\nmy_glorot_init([1, 10]).numpy()","7ab56744":"my_he_init([1, 10]).numpy()","eb9c94fe":"class my_dense(tf.keras.layers.Layer):\n    '''For layers, we need to override the tf.keras.layers.Layer.'''\n    \n    def __init__(self, units, activation=None, initializer=None, **kwargs):\n        'Define those attributes we would be using inside other functions.'\n        \n        # units is the number of units in the layer\n        self.units = units\n        \n        # super function to handle those values we might have missed\n        # \"Take on the default behaviour, unless explictly over rided\"\n        super().__init__(**kwargs)\n        \n        # specifying default activation\n        if not activation: \n            self.activation = my_elu()\n        else:\n            self.activation = activation\n            \n        # specifying default initializer\n        if not initializer:\n            self.my_init = my_he_init\n        else:\n            self.my_init = initializer        \n        \n    def build(self, batch_input_shape):\n        '''Main role of the build() is to create the layers variables -> weights and biases.\n        It is called the first time the layer is built. '''\n        \n        self.kernel = self.add_weight(\n            name='kernel', \n            shape=[batch_input_shape[-1], self.units], \n            initializer=self.my_init)\n        \n        self.bias = self.add_weight(name='bias', shape=[self.units], initializer='zeros')  \n        \n        # we call super() at the only the end this is \n        # to let keras know that the layer has been built\n        super().build(batch_input_shape)\n        \n    def call(self, x):\n        '''Performs what the layer is supposed to do \n        -> Matrix multiplication of input X with the kernel \n        -> Add the above output with bias term.'''\n        \n        # @ -> matrix multiplication (overridden for tensors by default)\n        return self.activation(x @ self.kernel + self.bias)\n    \n    def compute_output_shape(self, batch_input_shape):\n        '''Simply returns the shape of the tensors output, which is same as input\n        except the final dimension that is replaced with number of units in the layer.\n        \n        Output must be of TensorShape datatype.\n        \n        Note: batch_input_shape is of type `tf.TensorShape` which can be converted \n        to list with function as_list().\n        '''\n\n        return tf.TensorShape(batch_input_shape.as_list()[:-1], [self.units])\n    \n    def get_config(self):\n        'Override to be able to save the custom object later on.'\n        \n        parent_configs = super().get_config()\n        return {\n            **parent_configs, \n            \"units\": self.units,\n            \"activation\": tf.keras.activations.serialize(self.activation),\n            \"initializer\": tf.keras.initializers.serialize(self.my_init),\n        }","4fb843ec":"# default dense layer\ntemp = my_dense(units=10)\ntemp(tf.random.normal(shape=(1,5)))","bb0efcdc":"temp = my_dense(units=10, activation=my_softmax, initializer=my_glorot_init)\ntemp(tf.random.normal(shape=(1,5)))","42a4cfb8":"class my_dropout(tf.keras.layers.Layer):\n    def __init__(self, drate, **kwargs):\n        \n        # drate denotes the dropout rate probability\n        self.drate = drate\n        \n        # call parent to handle misc stuffs\n        super().__init__(**kwargs)\n        \n    def call(self, inputs, training=None):\n        '''Dropout layer is a special layer, it requires that this layer knows if the model is in \n        training or the testing phase. Its behaviour is changed accordingly. Note that dropouts are \n        applied only during the train phase.'''\n        \n        # if model is in training phase, apply dropouts\n        if training: \n            mask = tf.random.uniform(shape=inputs.shape) > self.drate\n            return tf.where(mask, inputs, 0) \/ (1 - self.drate)\n        \n        # else return op as such\n        else:\n            return inputs\n        \n    def get_config(self):\n        'We override to be able to save the dropout rates'\n        return {**super().get_config(), \"drate\": self.drate}","5a86c077":"# Manually setting the training scope environment\n# to stimulate model training.\n# Note that op shape must equal input shape\nwith tf.keras.backend.learning_phase_scope(1):\n    temp = my_dropout(0.2)(tf.random.normal(shape=(10,)))\n    print (temp.numpy())","3b1e7199":"class my_flatten(tf.keras.layers.Layer):\n    def call(self, x):\n        'Simply reshape the input to required shape (Flatten)'\n        return tf.reshape(x, shape=[x.shape[0], -1])\n    \n    def compute_output_shape(self, batch_input_shape):\n        'Output shape must be in TensorShape format.'\n        return tf.TensorShape([batch_input_shape[0]], [tf.reduce_prod(batch_input_shape[1:])])","d5c30745":"# working good? Check last dimension\nmy_flatten()(X_train[:5].astype(np.int32)).shape","406f25f5":"class my_conc(tf.keras.layers.Layer):\n    def call(self, inputs):\n        '''Note that inputs no matter how many come in through the \n        variable inputs, we need to do some tuple unpacking ourselves'''\n        \n        a, b = inputs\n        return tf.concat([a, b], axis=1)\n    \n    def compute_output_shape(self, batch_input_shapes):\n        'Output shape must be in TensorShape format.'\n        return tf.TensorShape(\n            batch_input_shapes[0][:-1],\n            [batch_input_shapes[0][-1] + batch_input_shapes[1][-1]])","20972951":"# working as expected? The last dimension should be concatenated\nmy_conc()([tf.random.uniform(shape=(1, 3)), tf.random.uniform(shape=(1, 7))]).shape","b45c3f64":"class my_mnist_model(tf.keras.models.Model):\n    def __init__(self, **kwargs):\n        '''The init function contains all the hyperparameter that the model takes.\n        Furthermore here we define all the layers, its attributes, etc as well.'''\n        \n        super().__init__(**kwargs)\n        \n        # The model architecture we had build \n        # previously using subclassing API\n        self.flatten = my_flatten()\n        self.hidden1 = my_dense(units=64)\n        self.hidden2 = my_dense(units=64)\n        self.conc = my_conc()\n        self.dropout = my_dropout(drate=0.5)\n        self.hidden3 = my_dense(units=128)\n        self.op = my_dense(units=10, initializer=my_glorot_init, activation=my_softmax)\n        \n    def call(self, inputs, training=None):\n        '''The computation you wish the model to perform comes here.\n        This looks identical to subclassing API, however the bonus is \n        that we could add in For loops, if statements, etc and so much more\n        inside this function.\n        \n        training -> Useful when your model contains layers whose behaviour depends on \n        scope, i.e whether the model is in training or testing phase.'''\n        \n        flatten = self.flatten(inputs)\n        hidden1 = self.hidden1(flatten)\n        hidden2 = self.hidden2(hidden1)\n        conc = self.conc([flatten, hidden2])\n        dropout = self.dropout(conc, training=training)\n        hidden3 = self.hidden3(dropout)\n        op = self.op(hidden3)\n        \n        return op              ","382ee7da":"# stimulate the model training environment, to check how \n# model would actually work during training\nwith tf.keras.backend.learning_phase_scope(1):\n    temp = my_mnist_model()\n    print (temp(X_train[:50].astype(np.float32)).shape)","3d537a90":"def print_status_bar(iteration, total, loss, metrics=None, bar_size=45):\n    '''This function prints out the loss + other metrics (if specified) one below another.\n    iteration -> Epoch\n    total     -> Total Epochs\n    loss      -> Model loss for that epoch\n    metrics   -> Calculated metrics'''\n    \n    metrics = ' - '.join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n    end = \"\" if iteration < total else \"\\n\"\n    \n    # \\r ensures that the output is printed in the same line\n    # print (\"\\r{}\/{} - \".format(iteration, total) + metrics, end=end)\n    \n    # \\r ensures that the output is printed in the same line\n    # Change bar_size to get larger or smaller bars\n    print (\"\\r[{}{}] - \"\n           .format((\"=\" * int(iteration\/total*bar_size) + \">\"), \n                   (\".\" * int((total-iteration)\/total*bar_size))\n           ) + metrics, end=end)","17b8f6d7":"class my_ES(object):\n    def __init__(self, patience=2, mode='max'):\n        self.check = \"0\" if mode=='max' else \"np.inf\"\n        self.mode = mode\n        self.patience = patience\n        \n        # count the number of times metric hasn't improved\n        self.p = 0\n        \n    def continue_training(self, metric_value):\n        # this is a make shift function, do suggest yours in the comments\n        # feel free to fork and change it if you wish to\n        curr = eval(f\"{self.mode}({self.check}, {metric_value})\")\n        if self.check != curr:\n            self.p = 0\n            self.check = curr\n            return True\n        else:\n            self.p += 1\n            return False if self.p >= self.patience else True","8e9ef0b5":"# clear the residual memory from all layers\n# we had created and tested so far\ntf.keras.backend.clear_session()\n\n# creating the model\nmy_model = my_mnist_model()\n\n# We use the inbuilt optimizer (FOR NOW)\noptimizer = tf.keras.optimizers.Adam(0.001)\n\n# save all metrics to a list\nmetrics = [my_scacc_metric()]\n\n# loss function the model would be using\nloss_fn = my_s_cross_entropy()\n\n# save the loss, metrics for each epoch to a dict\ncustom_hist = {}\n\n# Early Stopping Checker Object\nes = my_ES(mode='max', patience=2)\n\n# We use this to compute the loss of the mean and \n# to keep track of it in an efficient manner\n# since we are already familiar with functions \n# reset_states(), result(), update_state(), etc\n# We reuse this metric for both TRAIN and VAL\nmean_loss = tf.keras.metrics.Mean(name='Loss_Mean')\n\n# number of epochs & batch size\nn_epochs = 20\nbatch_size = 32\n\n# we need to calc steps per epoch, since \n# we are using custom loops for training\nn_steps_train = len(X_train) \/\/ batch_size\nn_steps_test = len(X_test) \/\/ batch_size\n\n# Little bit of preprocessing \n# before fiting to model\nX_Train = X_train.astype(np.float32)\ny_Train = y_train.reshape(-1, 1)\nX_Test = X_test.astype(np.float32)\ny_Test = y_test.reshape(-1, 1)","39247512":"# for n epochs\nfor epoch in range(1, n_epochs + 1):\n    \n    if epoch != 1:\n        print (\"\\n\\n\")\n    print (\"Epoch {}\/{}:\".format(epoch, n_epochs))\n    \n    # shuffle the training set for each epoch\n    ids = tf.random.shuffle(tf.range(start=0, limit=len(X_train)))\n    \n    # for each step of an epoch (TRAIN)\n    for step in range(1, n_steps_train + 1):\n        \n        # obtain the train batch for that step\n        temp_ids = ids[(step-1)*batch_size: step*batch_size]\n        X_batch, y_batch = X_Train[temp_ids], y_Train[temp_ids] \n        \n        with tf.GradientTape() as tape:\n            \n            # make predictions with the model\n            y_pred = my_model(X_batch, training=True)\n            \n            # our loss function doesn't return one loss per instance.\n            # Instead, it returns the loss summed. To be able to apply \n            # different class weights, rewrite the function to return\n            # one loss per instance, then apply the appropriate weights\n            # and use tf.reduce_sum to calculate the main loss\n            main_loss = loss_fn(y_batch, y_pred)\n            \n            # add computed loss to model.losses\n            loss = tf.add_n([main_loss] + my_model.losses)\n        \n        # compute gradient of loss with regard to each `trainable variable`\n        gradients = tape.gradient(loss, my_model.trainable_variables)\n        \n        # perform gradient descent step to model's trainable variables from computed gradients\n        optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n        \n        # update the loss for each step\n        mean_loss(loss)\n        \n        # compute metrics for each step predictions\n        for metric in metrics:\n            metric(y_batch, y_pred)\n            \n        # display progress bar for each TRAIN step (continuously overwritten)\n        # print_status_bar(step * batch_size, len(y_Train), mean_loss, metrics)\n        \n    # display progress metrics at end of epoch, uncomment above line\n    # if you wish to see the progress clearly for each step\n    print_status_bar(step * batch_size, len(y_Train), mean_loss, metrics)   \n      \n    # reset metrics & loss at the end of each epoch\n    # also save it so that we can analyse it later\n    for metric in [mean_loss] + metrics:\n        custom_hist[metric.name] = custom_hist.get(metric.name, []) + [metric.result().numpy()]\n        metric.reset_states()\n    \n    # model validation block, for each step of epoch (VAL)\n    for step in range(1, n_steps_test + 1):\n        \n        # obtain the Val batch for that step\n        X_batch = X_Test[(step - 1)*batch_size: step * batch_size]\n        y_batch = y_Test[(step - 1)*batch_size: step * batch_size]\n        \n        # make predictions on val dataset\n        y_pred = my_model(X_batch, training=False)\n        \n        # compute loss on val dataset & update loss\n        val_loss = loss_fn(y_batch, y_pred)\n        mean_loss(val_loss)\n        \n        # compute other metrics for VAL\n        for metric in metrics:\n            metric(y_batch, y_pred)\n            \n        # display progress bar for each VAL step (continuously overwritten)\n        # print_status_bar(step * batch_size, len(y_Test), mean_loss, metrics)\n        \n    # display progress metrics at end of epoch, uncomment above line\n    # if you wish to see the progress clearly for each step\n    print_status_bar(step * batch_size, len(y_Test), mean_loss, metrics) \n    \n    # reset metrics at the end of validation block\n    # also save it so that we can analyse it later\n    for metric in [mean_loss] + metrics:\n        custom_hist[\"val_\" + metric.name] = custom_hist.get(\"val_\" + metric.name, []) + [metric.result().numpy()]\n        metric.reset_states()\n        \n    # Early Stopping Checker (on VAL data)\n    if not es.continue_training(custom_hist['val_my_scacc_metric'][-1]):\n        print (f\"\\n\\n<======== EARLY STOPPING AFTER {epoch} EPOCHS ========>\")\n        break","6f610aa7":"# visualise the model's training performance\nhist_df = pd.DataFrame(custom_hist)\n\nf, ax = plt.subplots(ncols=2, figsize=(20, 10))\nax = ax.ravel()\n\nhist_df[['Loss_Mean', 'val_Loss_Mean']].plot(\n    ylim=[0, 0.15], xlim=[0, hist_df.shape[0]],\n    xlabel='Epochs', ylabel='Loss',\n    title='Epoch vs Loss', ax=ax[0])\n\nhist_df[['my_scacc_metric', 'val_my_scacc_metric']].plot(\n    ylim=[0.95, 1], xlim=[0, hist_df.shape[0]], \n    xlabel='Epochs', ylabel='Accuracy',\n    title='Epoch vs Accuracy', ax=ax[1]);","33d738dc":"# saving model preds for later use\npreds = my_model(X_Test)\n\nprint (\"Tf.keras Model: {:8.5}\\nOur Custom Model: {:.5}\".format(\n    tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_Test, model.predict(X_Test))).numpy(),\n    tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_Test, preds)).numpy())\n)","1b93d15a":"# saving the model weights\nmy_model.save_weights(\".\/custom_model\", save_format=\"tf\")","422a1c37":"# create the model exactly like we did before\n# luckily we have a function for that ;)\ntemp = my_mnist_model()\n\n# fit on a sample batch to help\n# model initialize its weights\ntemp(X_train[:1].astype(np.float32))\n\n# load the weights from saved dir\ntemp.load_weights(\".\/custom_model\")\n\n# throws an error if deviations are large\n# preds is the predictions made by custom model\n# see cell: 'The Ultimate Test'\nnp.testing.assert_allclose(preds, temp(X_Test), atol=1e-6)","663b40c7":"# Double check with validation score\ntf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_Test, temp(X_Test))).numpy()","2c58e068":"#### Concatenate layer:\n\nAgain easy to implement, however care should be taken with the `compute_output_shape`.","01d486fc":"##### Verify the outputs of keras implementation with ours:\n\nNote that sparse_categorical_cross_entropy and categorical_cross_entropy values must be equal to one another. They are essentially the same made to work on *different* types of input.","f417e9fb":"### Activations:\nMetrics are working just fine, let's move on to creating our own `softmax` & `elu` activations. Activations behind the hood are merely layers. So we can create activation functions by overriding the `tf.keras.layers.Layer`. Alternatively we could also over ride `tf.keras.activations.Activation` as well.\n\n#### ELU first:","f560ab05":"##### Check if the layer runs smoothly:\n\nTo be able to compare with tf.keras's layers, we may require setting seeds. This will be done in the future.","96deb7ca":"##### Works without throwing errors?\n\nInitializers cannot be compared with tf.keras's since they produce random values. If anyone knows of a method to compare, please feel free to let me know ^.^\n\nPerhaps could be done by setting a random seed? ","ca523fdc":"### Let's create the model we created above from scratch using subclassing API:\n\nYou: But I already know all this :| <br>\nMe: Well yeah.. But we needed this base model before we got started with fancier stuffs. \n\nNow that we are done with the boring part, let's get to  business :)\n\n#### Custom Loss functions:","e1ac2212":"##### Works without throwing errors?","5838bcb9":"##### The ultimate test:\n\nLet's compare the performance of tf.keras model with our custom model.","335d0004":"#### Let's train our model:","bcc1f9c4":"#### Model Training (Custom loop)\nWe have come a very long way from creating custom layers, activations to loses. We are almost at the conclusion of this notebook. The ultimate test would be to verify if our model performs as well as the Functional API did.\n\nBefore we write logic for custom loops, let's write a function that would print out the metrics during training:","4d6e7c2e":"#### A note on Steps, Batch_size & Epochs: \nA model is trained for a specified number of **epochs**. Each epoch runs completely through the training set, simply put, one epoch is one complete iteration through the train dataset. Each iteration through the training set is termed as one **step**. The number of training steps per epoch varies based on the **batch_size**. A batch size is number of examples that the model sees in one step.\n\n#### Model Training begins:\n\nFor custom training loops, we create tf.GradientTape context, which monitors all operations inside that involve a variable. We can later ask this tape to compute the gradients (derivatives) with respect to other variables. It is incredibly efficient at this and goes through all the variables just once for the computation. \n\nWe can compute the gradients using the gradient() method, after which the tape is destroyed. Sometimes, we may need to call gradient multiple times for which we need use the following command to make the tape 'persistent': ```tf.GradientTape(persistent=True)```.\n\nAlways put only the *Strict Minimum* inside tf.GradientTape to save memory.\n\n`Note`: I am yet to completely understand the mechanics of tf.GradientTape and autodiff. You might want to read the book I had mentioned to understand the concepts better.","6cb15dbf":"### Create the model using the functional API:\nLet's implement a 'Deep and Wide' neural network to add a little complexity for our custom model, Since a simple Neural network won't be much of a fun. A sample deep and wide Neural network's architecture is as follows:\n\n![](https:\/\/ibb.co\/5T3rnjd)\n\n![wide%20and%20deep.png](attachment:wide%20and%20deep.png)\n\nWe would be making slight tweaks to this architecture.","0e757907":"### Custom layers:\n\nNow let's move to the most import part of creating layers. Luckily, We already have an understanding of how this works when we saw the implementation of `elu` activation function.\n\nCustom layers are especially useful when we have an architecture which contains a lot of repetitive blocks such as ResNet.\n\nBelow we implement the following layers:\n1. Dense layer (Weights + Bias)\n2. Dropout layer (No weights)\n3. Flatten layer (No weights)\n\n#### Dense Layer:","e61c4849":"### Initializers:\nLet's now move on to create the intializers: \n1. `glorot_normal` (for softmax layer) \n2. `he_normal` (for elu layers)","7f3c4b91":"Perhaps an visual flowchart might help us better see the model's architecture:","ae6550f1":"##### Ensure & verify that it runs without complaining:","144d27a0":"##### Loading the Model:","0deace5d":"#### Dropout Layer:\n\nSince it has no weights or bias it is much simpler to create. We don't require to override the `build()` or the `compute_output_shape()`.","e3943e10":"### The Objective:\n\n- People shun keras due its lack of flexibility and control. Although loved by all for its simplicity, people often feel they have to outgrow this awesome package if they ever hope to do some serious deep learning.\n- People shun tensorflow owing to its complexity. Pytorch owing to its more 'Pythonic' Syntax is more popular. \n\nThe bold objective of this notebook is to contradict (atleast in part) both the above statements and to demonstrate the flexibility that tensorflow provides in creating custom layers, activations, losses, etc. This was inspired by chapter 12 (Custom models and Training with Tensorflow) of the book: **Hands-On Machine Learning with Scikit-Learn and TensorFlow, Aurelien Geron**. I highly recommend reading this book for every aspiring data scientist out there. A special thanks to Krish Naik and this [video](https:\/\/www.youtube.com\/watch?v=AuqZ4recf0s) for having introduced me to this awesome book!\n\nBefore we start with building our custom models and all the fancy stuffs I had promised, let's first create a simple MNIST classifier like we usually do with `tf.keras`, as a baseline which we would like to replicate with our own version of layers, activations & custom training loops.\n\n### Load the dataset\nWe start by loading the dataset we would be working with which is of course MNIST dataset:\n","1935f33d":"##### Working without complaining?\n\nWe are not yet done and we need to create simple helpers to print the progress of our model. Further more we need to write the logic for EarlingStopping. \n\nLet's check however if the model runs without throwing any error. Let's also verify if the output comes out as expected:","3f1fab88":"#### Flatten Layer:\n\nSimplest of all the layers we would be using. It doesn't even have any states we require saving. A simple `Call()` would suffice. However `compute_output_shape()` is the most important function here.","dcfbf49d":"## Conclusion:\nOur model has stood the ultimate challenge and has emerged glorious! Although this is a very simple MNIST classifier, the aim of this notebook as initially stated, is to demonstrate the ease with which we can implement custom loops, layers, etc and the extent of control that tf.keras offers to those who are willing to put in the effort. All the prior knowledge that we need is how to over ride the classes of `tf.keras`. \n\nDespite the control that it offers, the training time for our custom loop is much higher than training time for our functional API model, this was partly due to python loops we had written & perhaps due to inefficiencies in our code.\n\nThis notebook is far from perfect and probably has plenty of glaring flaws. If you notice any please feel free to let me know in the comments below.\n\nThis notebook is still under development, I will update them as I learn more about this awesome ecosystem.\n\n### TODO:\n- Custom optimizer for custom model\n- Custom Scheduler for both func API and custom model\n- Comparing the init and layer outputs, setting seed\n- Add notes for GPU training\n- Reconstruction loss for functional API and custom model, requires two inputs (More challenging)","5266cfe6":"#### Saving & loading a model with custom objects:\n\nFor saving models with custom layers, losses, metrics:\n\nWe can simply use the tf.keras.models.Model.save(\"path_to_save\"). <br>\nWhen reloading the model, we can use tf.keras.models.load_model(\"path\", custom_objects={\"custom_obj_name\": custom_obj, ...)<br>\n\nHowever whenever subclassing API is involved (tf.keras.models.Model is overridden), the [recommended way](https:\/\/colab.research.google.com\/drive\/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=rvGCpyX72HOC) according to keras author franchois Chollet is:\n\n    Use `save_weights` to create a TensorFlow SavedModel checkpoint, which will contain the value of all variables associated with the model:\n        1. The layers' weights\n        2. The optimizer's state\n        3. Any variables associated with stateful model metrics (if any)\n        \n    To restore your model, you will need access to the code that created the model object.\n\n    Note that in order to restore the optimizer state and the state of any stateful  metric, you should compile the model (with the exact same arguments as before) and call it on some data before calling `load_weights`.\n\n\n##### Saving the Model:\nLet's see how this is done in our case:","e2567436":"Prepare the field for model training:","a2567add":"### Importing necessary packages:","6e62ac5d":"Working good, let's move on to create the **SparseCategoricalAccuracy** Metric\n\n### Metrics:\n\nGenerally a simple function utilizing tf functions can be used as a metric. A overridden class metric only makes sense when we need a *Streaming or stateful metric*. However for purposes of demonstration we have over ridden the tf.keras.metrics.Metric.\n    \nRefer [here](https:\/\/stackoverflow.com\/questions\/56551282\/what-does-stateful-mean-in-tensorflow-metrics-in-my-case) for an example of what stateful metric is supposed to mean.","d2b2b8c6":"##### Verify that the activation runs fine:","040db261":"Let's also write a Simple Class to implement early stopping:\n\nNote that this not an implementation of `tf.keras.callbacks.Callback`, and this cannot be passed as callback for a normal fit argument of keras model. This is pure python code to help us stop the model training if it hasn't improved for long.","839b659b":"#### Now comes Softmax activation:\n\nTo understand more about softmax function & it's implementation check this awesome [link](https:\/\/medium.com\/data-science-bootcamp\/understand-the-softmax-function-in-minutes-f3a59641e86d).\n\nSoftmax turn logits into probabilities by taking the exponents of each output and then normalizing them with the sum of all the such exponents so the entire output vector adds up to one. We take the `exp(x)` for normalization instead of `x` since, when the logits are negative, adding them together does not give us the correct normalization. The same negative logits when applied with `exp()` always produces a value between 0 and inifinity. And adding them for normalization would make more sense.\n\nSimple softmax function expressed as numpy would be as follows:\n\n    np.exp(x) \/ np.sum(np.exp(x), axis=0)\n    \nHowever for [numerical stability](https:\/\/stackoverflow.com\/questions\/34968722\/how-to-implement-the-softmax-function-in-python) we may write the function as follows:\n\n    np.exp(x - x.max()) \/ np.exp(x - x.max()).sum(axis=0)","703030d0":"##### Verify our metric against tf.keras's metric:","ac1838fb":"### Model creation:\n\nWe are done creating the layers, Hurray!! \n\nLet's now create the model using the subclass api. To create a custom model, of course we have to override the `tf.keras.models.Model`, duh!\n\nThis part about creating models using the subclassing API is explained in depth in *chapter 11* of the same book. "}}