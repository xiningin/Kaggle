{"cell_type":{"c736d5a6":"code","458e5a4c":"code","51d59637":"code","eacb7fcb":"code","8ec01da5":"code","98e51f77":"code","a0c174ee":"code","3c03f6d0":"code","e6b66a60":"code","9a13d531":"code","13009b85":"code","ab4f86ca":"code","f6f81d72":"code","40da1e13":"code","36ae5772":"code","855acae7":"code","3f8a3d94":"code","c68ef7ee":"code","18e57077":"code","48dc663b":"code","fef76e82":"code","ba5f237c":"code","ff224e58":"code","ca84b57c":"code","a66b19bb":"code","14b1a4f2":"code","914dbe0b":"code","e6667a75":"code","3466391a":"code","a8146b5c":"code","910b6527":"code","cba37d4e":"code","c51f9531":"code","4408e89c":"code","a3c89a7e":"code","6ac2de79":"code","c25a1b7d":"markdown","a2c1ef20":"markdown","2ae40efb":"markdown","71b555ed":"markdown","bb5338cd":"markdown","5a4e5058":"markdown","94b04ed9":"markdown","442af1e9":"markdown","915a44db":"markdown","febd2e2b":"markdown","0ac0b8df":"markdown","78ff735b":"markdown","6b63ff69":"markdown","36981c6b":"markdown","baf77294":"markdown","6c56ec65":"markdown","eddc7390":"markdown","da66a358":"markdown","7c5e9206":"markdown","98b10824":"markdown","b87f69ef":"markdown"},"source":{"c736d5a6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dropout\n\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","458e5a4c":"train_df = pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')","51d59637":"train_df.head()","eacb7fcb":"np.max(train_df.id)","8ec01da5":"# Drop Nan Values\nX = train_df.dropna()\n\n# Get training data\nX = train_df.drop('label', axis = 1)\n\n#  Get target label\ny = train_df['label']","98e51f77":"messages = X.copy()","a0c174ee":"messages.reset_index(inplace = True)","3c03f6d0":"nltk.download('stopwords')","e6b66a60":"# Dataset Preprocessing\nps = PorterStemmer()\n\ncorpus = []\n\nfor i in range(0, len(messages)):\n    # replace with space words other than a-1, A-Z\n    \n    review = re.sub('[^a-zA-Z]', ' ', str(messages['title'][i]))\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","9a13d531":"# vocabulray size\nvoc_size = 5000","13009b85":"onehot_repr = [one_hot(words, voc_size) for words in corpus]\nnormal_repr = [(words, voc_size) for words in corpus]","ab4f86ca":"print(\"Normal Representation:\",normal_repr[0][0])\nprint(\"One Hot Representation:\",onehot_repr[0])","f6f81d72":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","40da1e13":"class MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads !=0:\n             raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim \/\/ num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n    \n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b = True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score \/ tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n    \n    def seperate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm = [0, 2, 1, 3])\n    \n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)\n        key = self.key_dense(inputs)\n        value = self.value_dense(inputs)\n        \n        query = self.seperate_heads(query, batch_size)\n        key = self.seperate_heads(key, batch_size)\n        value = self.seperate_heads(value, batch_size)\n        \n        attention, weights = self.attention(query, key, value)\n        \n        attention = tf.transpose(attention, perm=[0,2,1,3])\n        \n        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n        \n        output = self.combine_heads(concat_attention)\n        \n        return output","36ae5772":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n    \n    def call(self, inputs, training):\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training = training)\n        \n        out1 = self.layernorm1(inputs + attn_output)\n        \n        ffn_output = self.ffn(out1)\n        \n        fnn_output = self.dropout2(ffn_output, training = training)\n        return self.layernorm2(out1 + ffn_output)","855acae7":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim = vocab_size, output_dim = embed_dim)\n        self.pos_emb = layers.Embedding(input_dim = maxlen, output_dim = embed_dim)\n    \n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit = maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        \n        x = self.token_emb(x)\n        \n        return x + positions","3f8a3d94":"# making all sentences of same length\nmaxlen = 20\n\n# vocabulray size\nvocab_size = 5000\n\nembedded_docs = pad_sequences(onehot_repr, padding = 'pre', maxlen = maxlen)#","c68ef7ee":"print(\"One Hot Representation:\",onehot_repr[0])\nprint(\"Padding:\",embedded_docs[0])","18e57077":"embed_dim = 40\nnum_heads = 4\nff_dim = 32\n\ninputs = layers.Input(shape = (maxlen,))\nembedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(10, activation='relu')(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = keras.Model(inputs = inputs, outputs = outputs)","48dc663b":"model.summary()","fef76e82":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","ba5f237c":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","ff224e58":"X_final = np.array(embedded_docs)\ny_final = np.array(y)","ca84b57c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)","a66b19bb":"model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10, batch_size = 32)","14b1a4f2":"model.history.history","914dbe0b":"import matplotlib.pyplot as plt\n\n# visualizing losses and accuracy\n%matplotlib inline\n\ntrain_loss = model.history.history['loss']\nval_loss = model.history.history['val_loss']\ntrain_acc = model.history.history['accuracy']\nval_acc = model.history.history['val_accuracy']\n\nepochs = range(len(train_acc))\n\nplt.plot(epochs,train_acc,'r', label='Training Accuracy')\nplt.plot(epochs,val_acc,'b', label='Validation Accuracy')\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.legend()\nplt.figure()","e6667a75":"def return_x_y(X):\n    \n    # Drop Nan Values\n    X = X.fillna(0)\n    \n    messages = X.copy()\n\n    messages.reset_index(inplace = True)\n\n    # Dataset Preprocessing\n    ps = PorterStemmer()\n\n    corpus = []\n\n    for i in range(0, len(messages)):\n        # replace with space words other than a-1, A-Z\n\n        review = re.sub('[^a-zA-Z]', ' ', str(messages['title'][i]))\n        review = review.lower()\n        review = review.split()\n\n        review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n        review = ' '.join(review)\n        corpus.append(review)\n\n    # vocabulray size\n    voc_size = 5000\n\n    onehot_repr = [one_hot(words, voc_size) for words in corpus]\n\n    # Embedding Representation\n    # making all sentences of same length\n    sent_length = 20\n    embedded_docs = pad_sequences(onehot_repr, padding = 'pre', maxlen = sent_length)\n\n    X_final = np.array(embedded_docs)\n    \n    \n    return X_final, X","3466391a":"test_df = pd.read_csv('\/kaggle\/input\/fake-news\/test.csv')\nX_test,X_test_drop = return_x_y(test_df)","a8146b5c":"y_pred_test = model.predict(X_test)","910b6527":"submission_data = pd.read_csv('\/kaggle\/input\/fake-news\/submit.csv')","cba37d4e":"len(X_test_drop['id']), len(y_pred_test)","c51f9531":"df_sub = pd.DataFrame()\ndf_sub['id'] = X_test_drop['id']\ndf_sub['label'] = y_pred_test","4408e89c":"# Converting float to Integer\ndf_sub['label'] = df_sub['label'].apply(lambda x:0 if x<=0.5 else 1)","a3c89a7e":"df_sub.to_csv('gender_submission.csv', index=False)","6ac2de79":"#Model Save\nmodel.save_weights('model_weights.h5')\nmodel.save('model_keras.h5')","c25a1b7d":"# Constructing Transformer model","a2c1ef20":"# Embedding Representation","2ae40efb":"# Reading Dataset","71b555ed":"# Submission data","bb5338cd":"**The evolution of the information and communication technologies has dramatically increased the number of people with access to the Internet, which has changed the way the information is consumed. As a consequence of the above, fake news have become one of the major concerns because its potential to destabilize governments, which makes them a potential danger to modern society. An example of this can be found in the US. electoral campaign, where the term \"fake news\" gained great notoriety due to the influence of the hoaxes in the final result of these.**\n\n<img src=\"https:\/\/ichef.bbci.co.uk\/news\/904\/cpsprodpb\/15B3B\/production\/_99919888_fakenews4.gif\"\/>","5a4e5058":"# Fake News","94b04ed9":"# Implement a Transformer block as a layer","442af1e9":"# Model Training","915a44db":"# Data Preprocessing\n\n1. Putting features to 'X' and Target\/Label to 'Y'\n2. From the gained text except the alphabet (A-Z, a-z).. all others are removed\n3. Gained Text is Lowered and StopWords is removed\n\n**After all this operation below operation is done.(Generalized Version may change little in different project)**\n<img src=\"https:\/\/raw.githubusercontent.com\/milan400\/kaggle\/master\/lstm%20data%20preprocessing.png\"\/>","febd2e2b":"# Implement Embedding Layer\n* Two seperate embedding layers, one for tokens, one for token index (positions).","0ac0b8df":"# Visualizing Train,Test--->Accuracy,Loss","78ff735b":"# OneHot Representation","6b63ff69":"# Importing Library","36981c6b":"# Create Classifier model using Transfomer layer\n\n* Transformer layer outputs one vector for each time step of our input sequence. Here, we take the mean across all time steps and use a feed forward network on top of it to classify text\n\n","baf77294":"# Total number of messages","6c56ec65":"# Model Visualization","eddc7390":"# Reading and Preprocessing Test data","da66a358":"# Setup","7c5e9206":"# Save model","98b10824":"**Splitting Data into train and test**","b87f69ef":"# Implement multi head self attention as a keras layer"}}