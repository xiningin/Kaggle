{"cell_type":{"6bbc22c1":"code","c998f888":"code","c729413c":"code","5ada338c":"code","e04fbea1":"code","6eb077fd":"code","4af8c2de":"code","b8fde7d5":"code","b2dbc4d3":"code","3ed8aea3":"code","ffbab151":"code","8f0b304b":"code","7785a523":"code","aebc94fa":"code","36e2b2bc":"code","af80b68c":"code","dca0a09f":"code","a1ae08cd":"code","ab6fca93":"code","e3e1cdf7":"code","db99db8d":"code","0fbafb0d":"code","20c9a5ed":"code","c7f4964c":"code","d47cc90c":"code","784408e9":"code","e06a70b0":"code","b016aa87":"code","0971e91f":"code","7d545075":"code","068094e2":"code","ef3f7041":"code","40c68f2b":"code","61530981":"code","7c3d6687":"code","0be93d4a":"code","27153dff":"code","9be5e266":"code","20f4bc1e":"code","6388d132":"code","bb71c312":"code","040c61d7":"code","2e1b3a6c":"code","9d1b8aba":"code","3f9366dd":"code","0e0bf6da":"code","5cd6230a":"code","06e46ad9":"code","6007d8e8":"code","3c146e33":"code","2524732b":"markdown","7bb2fc34":"markdown","de87b0e7":"markdown","9df1c9f6":"markdown","2fadc259":"markdown","4a14fba4":"markdown","6313cebe":"markdown","ae159ac0":"markdown","04860de2":"markdown","103d3b3a":"markdown","33cb8cdd":"markdown","913587b5":"markdown","b2630043":"markdown"},"source":{"6bbc22c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c998f888":"import pandas as pd\ndata = pd.read_csv(\"\/kaggle\/input\/loan-data-set\/loan_data_set.csv\")\nprint(\"Dataset load successfullly!\")","c729413c":"#see head of data\ndata.head()","5ada338c":"#shape of data ---> 614 rows and 13 columns\ndata.shape","e04fbea1":"data.info()","6eb077fd":"#It only predicts the numerical data\ndata.describe()","4af8c2de":"#To describe categorical columns\ndata.describe(include='object')","b8fde7d5":"#get numerocal columns names\nnumericalCols = data.select_dtypes(['float64','int64']).columns\nnumericalCols","b2dbc4d3":"#get Categorical columns names\ncategoricalCols = data.select_dtypes(['object']).columns\ncategoricalCols","3ed8aea3":"#it is used to check the cross-tabulation of two or more factors\npd.crosstab(data['Credit_History'], data['Loan_Status'])","ffbab151":"#draw a boxplot to check the outliers\ndata.boxplot(column=['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', \n                     'Loan_Amount_Term'])\n'''As we can see that ApplicantIncome and CoapplicantIncome columns has large Standard Deviation and more outliers.\nwhile LoanAmount and Loan_Amount_Term columns has less Standard Deviation and less outliers.'''","8f0b304b":"#By using Histogram, lets  check the distribution of Numerical columns one by one.\ndata['ApplicantIncome'].hist(bins=20)\n'''See! Data is left skewed. We have to convert it normal Distribution'''","7785a523":"data['ApplicantIncome'].hist(bins=20)\n'''Again, data is left skewed'''","aebc94fa":"data['LoanAmount'].hist(bins=20)\n'''It looks like normal distributed.'''","36e2b2bc":"data['Loan_Amount_Term'].hist(bins=20)\n'''Not showing any ditribution, hard to say any thing.'''","af80b68c":"#tell how  many entries are missing in each columns\ndata.isnull().sum()","dca0a09f":"#for categorical columns use to fillup the values using mode \ndata['Gender']= data['Gender'].fillna(data['Gender'].mode()[0])\ndata['Married']= data['Married'].fillna(data['Married'].mode()[0])\ndata['Dependents']= data['Dependents'].fillna(data['Dependents'].mode()[0])\ndata['Self_Employed']= data['Self_Employed'].fillna(data['Self_Employed'].mode()[0])","a1ae08cd":"'''numerical columns has not normally distributed. So, median is better for imputation.'''\n#for categorical columns use to fillup the values using median \ndata['LoanAmount']=data['LoanAmount'].fillna(data['LoanAmount'].median())\ndata['Loan_Amount_Term']=data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].median())\n#Credit_History has only two unique values 0 and 1, it's better to use mode for imputation.\ndata['Credit_History']=data['Credit_History'].fillna(data['Credit_History'].mode()[0])","ab6fca93":"data.isnull().sum()\n'''See! all the missing values are filled successfully'''","e3e1cdf7":"#import libraries\nimport numpy as np","db99db8d":"data['ApplicantIncome'].hist(bins=20)\n'''ApplicantIncome before normalization'''","0fbafb0d":"#apply normalization using log\ndata['ApplicantIncome_log']= np.log(data['ApplicantIncome'])","20c9a5ed":"data['ApplicantIncome_log'].hist(bins=20)\n'''ApplicantIncome_log before normalization'''","c7f4964c":"data['CoapplicantIncome'].hist(bins=20)\n#data['CoapplicantIncome_log']= np.log(data['CoapplicantIncome'])\n#data['CoapplicantIncome_log'].hist(bins=20)","d47cc90c":"# create a new variable of totalIncome \ndata['totalIncome']= data['CoapplicantIncome'] + data['ApplicantIncome']\ndata['totalIncome'].hist(bins=20)\n'''totalIncome before normalization'''\n#normalize it\ndata['totalIncome_log']= np.log(data['totalIncome'])","784408e9":"data['totalIncome_log'].hist(bins=20)\n'''totalIncome after normalization'''","e06a70b0":"data['LoanAmount'].hist(bins=20)\n'''LoanAmount Before Normalization'''","b016aa87":"data['LoanAmount_log']= np.log(data['LoanAmount'])","0971e91f":"data['LoanAmount_log'].hist(bins=20)\n'''LoanAmount Before Normalization'''","7d545075":"data['Loan_Amount_Term'].hist(bins=20)\n'''Loan_Amount_Term Before Normalization'''","068094e2":"data['Loan_Amount_Term_log']= np.log(data['Loan_Amount_Term'])","ef3f7041":"data['Loan_Amount_Term_log'].hist(bins=20)\n'''Loan_Amount_Term Before Normalization'''","40c68f2b":"data.describe()","61530981":"data.shape","7c3d6687":"data.columns","0be93d4a":"data.head()","27153dff":"#selection of columns\nX=data.iloc[:,np.r_[1:6,10:12,13,15:18]]\ny=data.iloc[:,12]","9be5e266":"#train-test-split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)","20f4bc1e":"X_train","6388d132":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","bb71c312":"from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = X_train.copy()\nlabel_X_test = X_test.copy()\n\n# Apply Ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\nlabel_X_test[object_cols] = ordinal_encoder.transform(X_test[object_cols])","040c61d7":"# Make copy to avoid changing original data \nlabel_y_train = y_train.copy()\nlabel_y_test = y_test.copy()\n\n#apply encoder on y split variables\nlabel_encoder = LabelEncoder()\nlabel_y_train =  label_encoder.fit_transform(label_y_train)\nlabel_y_test =  label_encoder.fit_transform(label_y_test)","2e1b3a6c":"from sklearn.tree import DecisionTreeClassifier\nDTClassifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTClassifier.fit(label_X_train, label_y_train)","9d1b8aba":"#prediction of data\npred = DTClassifier.predict(label_X_test)\npred","3f9366dd":"from sklearn import metrics\nprint(\"Acuracy of Decision Tree model is : \", metrics.accuracy_score(pred, label_y_test))","0e0bf6da":"from sklearn.naive_bayes import GaussianNB\nNBClassifier = GaussianNB()\nNBClassifier.fit(label_X_train, label_y_train)\nNB_pred = NBClassifier.predict(label_X_test)\nprint(\"Acuracy of Naive Bayes model is : \", metrics.accuracy_score(NB_pred, label_y_test))","5cd6230a":"from sklearn.ensemble import RandomForestClassifier\n\nRFClassifier = RandomForestClassifier(n_estimators=8, random_state=0)\nRFClassifier.fit(label_X_train, label_y_train)\nRFpred = RFClassifier.predict(label_X_test)","06e46ad9":"from sklearn.svm import SVC\nclf = SVC(kernel='sigmoid', C=1)\nclf.fit(label_X_train, label_y_train)\nclf_pred = clf.predict(label_X_test)\nprint(\"Acuracy of Support vector Machine model is : \", metrics.accuracy_score(clf_pred, label_y_test))","6007d8e8":"#logistic model\nfrom sklearn.linear_model import LogisticRegression\nlogistic_model = LogisticRegression()\nlogistic_model.fit(label_X_train, label_y_train)\nlog_pred = logistic_model.predict(label_X_test)","3c146e33":"print(\"Acuracy of Randon Forest model is : \", metrics.accuracy_score(RFpred, label_y_test))\nprint(\"Acuracy of Decision Tree model is : \", metrics.accuracy_score(pred, label_y_test))\nprint(\"Acuracy of Naive Bayes model is : \", metrics.accuracy_score(NB_pred, label_y_test))\nprint(\"Acuracy of Logistic Regression model is : \", metrics.accuracy_score(log_pred, label_y_test))\nprint(\"Acuracy of Support vector Machine model is : \", metrics.accuracy_score(clf_pred, label_y_test))","2524732b":"**Apply Decision Tree models**","7bb2fc34":"**Apply Naive Baise Model**","de87b0e7":"**convert categorical columns values into numerical**","9df1c9f6":"****","2fadc259":"*handle skewed numerical columns*","4a14fba4":"**Preprocessing of Data**\n\nAs, we see there are missing values in our data and some columns needs to normalized.\n\n*handle missing values*\n","6313cebe":"**Accuracy of all applied models**\n\nWe, can see that SVM model has high accuracy than all other models","ae159ac0":"**Logistic Regression Model**","04860de2":"**Apply Random Forest CLassifier model**","103d3b3a":"**Support Vector machine**","33cb8cdd":"Let's know our data","913587b5":"**Accuracy of model**","b2630043":"**selection of varaibles and train test split**"}}