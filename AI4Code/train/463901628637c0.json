{"cell_type":{"8e642568":"code","3513a111":"code","5ade32b0":"code","e4dcc694":"code","63823e15":"code","5e4d6040":"code","c04136e7":"code","66d078c1":"code","8f951cdb":"code","123779c8":"code","8587cadb":"code","418a78fe":"code","d63c31bc":"code","83589fb5":"code","7bd51551":"code","9853436b":"code","bdea2053":"code","089f741d":"code","562cde75":"code","97840f52":"code","5df249c6":"code","240055a3":"code","83a4a179":"code","8b7043db":"code","1407eff9":"code","36d3919a":"code","2bc4be67":"code","7536c1d2":"markdown","7faf2301":"markdown","4f0a4330":"markdown","6a8d1ca8":"markdown","db96f7ce":"markdown","43b79083":"markdown"},"source":{"8e642568":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport cv2\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","3513a111":"# atfter loading the BERT in the input-directory install BERT \n!pip install \/kaggle\/input\/bert-for-tf2\/py-params-0.8.2\/py-params-0.8.2\/\n!pip install \/kaggle\/input\/bert-for-tf2\/params-flow-0.7.4\/params-flow-0.7.4\/\n!pip install \/kaggle\/input\/bert-for-tf2\/bert-for-tf2-0.13.2\/bert-for-tf2-0.13.2\/\n!pip install sentencepiece","5ade32b0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport tensorflow as tf \nimport tensorflow_hub as hub # importing tools\/keras.layers etc. from internet\n\nimport keras\nfrom tensorflow.keras.layers import Dense, Input,LeakyReLU, Dropout, Softmax\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\n\nimport bert\n\nimport matplotlib.pyplot as plt \n\nimport re # regular expression operations, for cleaning text techniques","e4dcc694":"# load the competition data\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","63823e15":"# first look at the train dataset, there are 5 cloumns, only the \"text\" and \"target\" columns contains are of interest.\n# the keyword-column and the location-column may be important but contain some NaN-entries\ntrain_df = train_df.astype({\"id\" : int, \"target\" : int, \"text\" : str})\ntrain_df.head(5)","5e4d6040":"# look at the test_df\ntest_df = test_df.astype({\"id\" : int, \"text\" : str})\ntest_df.head(5)","c04136e7":"# three example of what is not a disaster\ntrain_df[train_df[\"target\"] == 0][\"text\"].head(3)# the first three df-entries-\"text\"-column with target ==0","66d078c1":"# And one that is a disaster:\ntrain_df[train_df[\"target\"] == 1][\"text\"].head(3)","8f951cdb":"# helpful function for cleaning the text with regular experessions\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # remove https? links\n    text = re.sub(r'#', '', text) # remove hashtags by keeping the hashtag text\n    text = re.sub(r'@\\w+', '', text) # remove @usernames\n    text = re.sub(r'\\n',' ', text) # remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # remove leading, trailing, and extra spaces\n    #text = re.sub(r'\\-','',text)\n    #text = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\=\\\/\\|\\'\\(\\)\\[\\]]\",\" \",text).split()) # remove punctuation\n    #text = remove_emoji(text)\n    return text\n\n# helpful function for extract hashtags, usernames and weblinks from tweets\ndef find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_usernames(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'\n\n# function for pereproceeding the hole text\ndef preprocess_text(df):\n    df['clean_text'] = df['text'].apply(lambda x: clean_text(x)) # cleaning the text\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x)) # extracting the hashtags\n    df['usernames'] = df['text'].apply(lambda x: find_usernames(x)) # extracting the @username(s)\n    df['links'] = df['text'].apply(lambda x: find_links(x)) # extracting http(s)-links\n    return df \n    \n# preprocessing the 'text'-column in df and extending with additional columns \n# 'clean_text', 'hashtags', 'usernames' and 'links'\ntrain_df = preprocess_text(train_df)\ntest_df = preprocess_text(test_df)\nprint(train_df)","123779c8":"#there are missing values in training set in 'keyword'- and 'location'-columns\ntrain_df.isnull().sum()","8587cadb":"train_df.fillna(' ')\ntest_df.fillna(' ')\n#train_df['text_final'] = train_df['clean_text']+' '+ train_df['keyword']+' '+ train_df['location']+' '+ train_df['hashtags']\n#test_df['text_final'] = test_df['clean_text']+' '+ test_df['keyword']+' '+ test_df['location']+' '+ train_df['hashtags']\ntrain_df['text_final'] = train_df['clean_text']+' '+ train_df['keyword']#+' '+ train_df['hashtags']\ntest_df['text_final'] = test_df['clean_text']+' '+ test_df['keyword']#+' '+ train_df['hashtags']\n\ntrain_df['lowered_text'] = train_df['text_final'].str.lower()\ntest_df['lowered_text'] = test_df['text_final'].str.lower()\n","418a78fe":"# encoding text for bert in an bert-compatible format like: [CLS]..text..[SEP][PAD][PAD] etc.\n#  cls_token='[CLS]', sep_token='[SEP]', pad_token='[PAD]'=[0], mask_token='[MASK]',\n# pass the text, the tokenizer from BERT an a max_len of the sequences \n\ndef bert_encode(texts, tokenizer, max_len):  # length of encoded sequences \n    # prepare empty np-arrays for the token, mask and segments\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:  # for every text-sequence\n        text = tokenizer.tokenize(text)# transform text-sequence into token-sequence\n          \n        text = text[:max_len-2]# cut the token-sequence at the end\n        \n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] # insert [CLS]-token at the beginning of sequence and a [SEP]-token at the end\n        \n        pad_len = max_len - len(input_sequence) # determine the length of the [PAD]-sequences to add on short input-sequences\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) # transforms token to token-id\n        tokens += [0] * pad_len # concatenate the missing space as [0]-PAD-token\n       \n        pad_masks = [1] * len(input_sequence) + [0] * pad_len # pad_mask of the form 11111...00000 with 111 for input, 000 for rest\n        segment_ids = [0] * max_len # segment_id of the form 00000...000\n        \n        all_tokens.append(tokens) # concatenate the token-sequences\n        all_masks.append(pad_masks) # concatenate the padding-masks\n        all_segments.append(segment_ids) # concatenate the segment-ids\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments) # return all","d63c31bc":"BertTokenizer = bert.bert_tokenization.FullTokenizer\n# note that the internet must be accessible for this notebook, to download the bert-layer\n# load a pretrained, trainable bert-layer as Keras.layer from the tensorflow-Hub\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_cased_L-12_H-768_A-12\/1\",trainable=True)\n\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() # \nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n","83589fb5":"# encoding the input- und test-features for BERT\ntrain_input = bert_encode(train_df.lowered_text.values.astype(str), tokenizer, max_len=512) # final input-data\ntest_input = bert_encode(test_df.lowered_text.values.astype(str), tokenizer, max_len=512)# final test-data\ntrain_labels = train_df.target.values # final target-data","7bd51551":"# define a model by pass a bert-layer and a finite sequence-lenght as parameters\n# to the function\ndef build_model(bert_layer, max_len): # etc. max_len=512, bert encoder works with sequences of finite lenght\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :] # for sentence classification, we\u2019re only  interested \n    #in BERT\u2019s output for the [CLS] token,\u00a0\n    \n    hidden1 = Dense(128, activation='relu')(clf_output) #128\n    out = Dense(1, activation='sigmoid')(hidden1) \n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","9853436b":"model = build_model(bert_layer, max_len=512)\nmodel.summary()","bdea2053":"history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=5, #5\n    batch_size=16\n)\n\npredictions1 = model.predict(test_input)\nprint(predictions1[0:30])","089f741d":"# try another model by using the google universal sentence encoder from https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/1 \nembedding = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")\n\nembedded_xtrain = embedding(train_df['clean_text']).numpy()\nembedded_xtest = embedding(test_df['clean_text']).numpy()\ntarget = train_df[\"target\"].to_numpy()\n\n# prepare a support vector maschine with radial basis funtion kernels\nfrom sklearn import svm\nmodel2 = svm.SVR(kernel='rbf',gamma='auto')\nmodel2.fit(embedded_xtrain,target)\n\npredictions2 = model2.predict(embedded_xtest)\npredictions2 = np.mat(predictions2)\npredictions2 = predictions2.T\nprint(predictions2[0:30])\n","562cde75":"sequence_lenght = 512","97840f52":"# try another model by using the google universal sentence encoder https:\/\/tfhub.dev\/google\/universal-sentence-encoder-lite\/2\nUSElite2_embedding = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\") #\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-lite\/2\")\n\nUSElite2_embedded_xtrain = USElite2_embedding(train_df['clean_text']).numpy()\nUSElite2_embedded_xtest = USElite2_embedding(test_df['clean_text']).numpy()\nUSElite2_target = train_df[\"target\"].to_numpy() # no embedding\nUSElite2_embedded_xtest.shape\n\nUSE_for_m4_xtrain = USElite2_embedded_xtrain\nUSE_for_m4_xtest = USElite2_embedded_xtest\nUSE_for_m4_target = USElite2_target","5df249c6":"from sklearn.model_selection import train_test_split\nUSElite2_x_train, USElite2_x_test, USElite2_y_train, USElite2_y_test = train_test_split(\n    USElite2_embedded_xtrain,\n    USElite2_target,\n    test_size=0.1,\n    random_state=0,\n    shuffle=True\n)\nprint(USElite2_x_train.shape)\nprint(USElite2_y_train.shape)\nprint(USElite2_x_test.shape)\nprint(USElite2_y_test.shape)\nprint(USElite2_x_train)","240055a3":"def make_my_model():\n    input = keras.layers.Input(shape=(sequence_lenght,1), dtype='float32')\n    \n    #Conv1D-layer expected shape (Batchsize,Width,Channels)\n   \n    next_layer = keras.layers.Conv1D(265,kernel_size = 10, activation = \"relu\",padding=\"valid\",strides = 1)(input)\n    next_layer = keras.layers.MaxPooling1D(pool_size=2)(next_layer)\n    \n    next_layer = keras.layers.Conv1D(64,kernel_size = 5, padding=\"valid\", strides = 1)(next_layer)\n    next_layer = keras.layers.LeakyReLU(alpha=0.1)(next_layer)\n    next_layer = keras.layers.MaxPooling1D(pool_size=3, strides=1)(next_layer)\n    \n    next_layer = keras.layers.Flatten()(next_layer)\n    \n    next_layer = keras.layers.Dense(64)(next_layer)\n    next_layer = keras.layers.LeakyReLU(alpha=0.1)(next_layer)\n    \n    #next_layer = keras.layers.Dropout(0.2)(next_layer)\n    \n    #next_layer = keras.layers.LeakyReLU(alpha=0.1)(next_layer)\n    \n    output = keras.layers.Dense(1, activation=\"sigmoid\")(next_layer)\n      \n    return keras.Model(inputs=input, outputs=output)","83a4a179":"# Reshaping the inputs. The conv1d-Layer needs (batchsize x lenght x dim=1)\n# shape[0]=batchsize=6090, shape[1]=length=512, dim=1\nUSElite2_x_train = np.reshape(USElite2_x_train, (USElite2_x_train.shape[0], USElite2_x_train.shape[1],1))\nUSElite2_y_train = np.reshape(USElite2_y_train, (USElite2_y_train.shape[0],1))\n# shape[0]=batchsize=1523, shape[1]=length=512, dim=1\nUSElite2_x_test = np.reshape(USElite2_x_test, (USElite2_x_test.shape[0], USElite2_x_test.shape[1],1))\nUSElite2_y_test = np.reshape(USElite2_y_test, (USElite2_y_test.shape[0],1))\n\nmodel3 = make_my_model()\nmodel3.compile(\"adam\", loss = \"binary_crossentropy\", metrics = [\"acc\"])\nmodel3.summary()\n\nmodel3.fit(\n    USElite2_x_train,\n    USElite2_y_train,\n    batch_size = 128,\n    epochs = 15,\n    validation_data = (USElite2_x_test,USElite2_y_test)\n)\n\nUSElite2_embedded_xtest = np.reshape(USElite2_embedded_xtest, (USElite2_embedded_xtest.shape[0],USElite2_embedded_xtest.shape[1],1))\npredictions3 = model3.predict(USElite2_embedded_xtest)\npredictions3\nprint(predictions3[0:30])","8b7043db":"# prepare a support vector maschine with radial basis function kernels\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\n\n\nmodel4 = svm.SVR(kernel='rbf', gamma='auto')\nmodel4.fit(USE_for_m4_xtrain ,USE_for_m4_target)\n\npredictions4 = model4.predict(USE_for_m4_xtest)\npredictions4 = np.mat(predictions4)\npredictions4 = predictions4.T\nprint(predictions4[0:30])","1407eff9":"print(((0.5*predictions1+0.5*predictions2+0.1*predictions3+0.3*predictions4)*0.8)[0:30])","36d3919a":"((0.5*predictions1+0.5*predictions2+0.1*predictions3+0.3*predictions4)*0.8).round().astype(int)","2bc4be67":"submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nprint(submission)\nsubmission['target'] =((0.5*predictions1+0.5*predictions2+0.1*predictions3+0.3*predictions4)*0.8).round().astype(int)\nprint(submission)\nsubmission.to_csv('submission.csv', index=False)","7536c1d2":"**Model 1: input-text  ===> Encoding for bert ==> BERT  ===> Classifier(FeedForward-Network with 'softmax'-output-layer)**","7faf2301":"**Model 3: USE large\/5 Embedding + MLP **","4f0a4330":"pred1=BERT, pred2=USE+SVM, pred3=USElarge+ConV1D, pred4=USElarge+SVM  ","6a8d1ca8":"**Model 2: USE + support vector machine**","db96f7ce":"In this notebook I try my first application of the BERT concept for NLP. As I understood I first have to clean up the data and then get it into a form that can be processed by BERT, so that the input sequences can be processed by a BERT layer that I have loaded from the tensorflow hub. I try to learn from the notebook https:\/\/www.kaggle.com\/hassanamin\/bert-nlp-real-or-not\n\nIn total I combine four different techniques of NLP with a clear focus on support vector machines.\n\npred1=BERT+Dense(1), pred2=USE+SVM, pred3=USElarge+ConV1D, pred4=USElarge+SVM  \n\nI try to learn from the notebook https:\/\/www.kaggle.com\/hassanamin\/bert-nlp-real-or-not\n","43b79083":"**Model 4**"}}