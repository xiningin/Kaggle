{"cell_type":{"7f07ec5c":"code","8182af03":"code","700a7486":"code","841c813b":"code","84c11e8c":"code","0b6cd74d":"code","e4b38d5b":"code","66cf7d4f":"code","2aa8a739":"code","7027173c":"code","0afffb87":"code","28bb69fd":"code","6914950c":"code","f5f1fda8":"markdown","641d8cfb":"markdown","9a3cd132":"markdown","5c621715":"markdown","c2c8eeb5":"markdown","bb266baa":"markdown","a590e4eb":"markdown","23d7d4e5":"markdown","5ce69085":"markdown","c185771d":"markdown"},"source":{"7f07ec5c":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport transformers\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","8182af03":"kaggle = '\/kaggle\/input\/undersampling-xlmtokenized\/'\nx_train = np.load(f'{kaggle}x_train.npy')\ny_train = np.load(f'{kaggle}y_train.npy')\n\nx_valid = np.load(f'{kaggle}x_valid.npy')\ny_valid = np.load(f'{kaggle}y_valid.npy')\n\nx_test = np.load(f'{kaggle}x_test.npy')","700a7486":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","841c813b":"# Global Variables\nBATCHSIZE = 128\nEPOCHS = 10\nAUTO = tf.data.experimental.AUTOTUNE\nNUMCORES = strategy.num_replicas_in_sync","84c11e8c":"def build(transformer, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = Input(shape=(maxlen,), dtype=tf.int32, name=\"input_word_ids\")\n    cls_token = transformer(input_ids)[0][:,0,:]\n    out = tf.reshape(cls_token, [-1, cls_token.shape[1], 1])\n    out = Dropout(0.25)(out)\n    out = Conv1D(100, 3, padding='valid', activation='relu', strides=1)(out)\n    out = BatchNormalization(axis=2)(out)\n    out = Conv1D(100, 4, padding='valid', activation='relu', strides=1)(out)\n    out = BatchNormalization(axis=2)(out)\n    out = Conv1D(100, 5, padding='valid', activation='relu', strides=1)(out)\n    out = BatchNormalization(axis=2)(out)\n    out = GlobalMaxPooling1D()(out)\n    out = Dropout(0.5)(out)\n    out = Dense(1, activation='sigmoid')(out)\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), \n                  loss=BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy', AUC()])\n    return model","0b6cd74d":"train_dataset = (\n    tf.data.Dataset\n        .from_tensor_slices((x_train, y_train))\n        .repeat()\n        .shuffle(x_train.shape[0])\n        .batch(BATCHSIZE)\n        .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n        .from_tensor_slices((x_valid, y_valid))\n        .batch(BATCHSIZE)\n        .cache()\n        .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n        .from_tensor_slices(x_test)\n        .batch(BATCHSIZE)\n)","e4b38d5b":"%%time\nwith strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained('jplu\/tf-xlm-roberta-large')\n    model = build(transformer_layer, maxlen=192)\nmodel.summary()","66cf7d4f":"n_steps = 680\n\n# Treat every instance of class 1 as 20 instances of class 0.\n# 5% of training data is composed of toxic data.\n# CLASS_WEIGHT = [1., 20.]\ntraining_history = model.fit(train_dataset, \n                             steps_per_epoch=n_steps, \n                             validation_data=valid_dataset, \n                             epochs=EPOCHS)","2aa8a739":"n_steps = x_valid.shape[0] \/\/ 128\n\n# Treat every instance of class 1 as 7 instances of class 0.\n# 15% of validation data is composed of toxic data\nCLASS_WEIGHT = [1., 7.]\nvalid_history = model.fit(valid_dataset.repeat(), \n                          steps_per_epoch=n_steps, \n                          epochs=4,\n                          class_weight=CLASS_WEIGHT)","7027173c":"import matplotlib.pyplot as plt\n\n\nplt.plot(training_history.history['auc'])\nplt.plot(training_history.history['val_auc'])\nplt.title('Model AUC vs. Epoch')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Valid'], loc='upper left')\nplt.show()","0afffb87":"plt.plot(training_history.history['accuracy'])\nplt.plot(training_history.history['val_accuracy'])\nplt.title('Model Accuracy vs. Epoch')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Valid'], loc='upper left')\nplt.show()","28bb69fd":"plt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\nplt.title('Model Loss vs. Epoch')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Valid'], loc='upper left')\nplt.show()","6914950c":"sub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\nsub['toxic'] = model.predict(test_dataset, verbose=1)[0:len(sub['toxic'])]\nsub.to_csv('submission.csv', index=False)","f5f1fda8":"Helper function to build model.","641d8cfb":"Loading training\/validation\/test data.","9a3cd132":"## Plotting Training History","5c621715":"# Pytorch sucks am I right (the sequel)","c2c8eeb5":"## Test Model\n","bb266baa":"Load model into the TPU","a590e4eb":"Create TF dataset objects.","23d7d4e5":"## Train Model","5ce69085":"### TPU Configuration","c185771d":"After saturating the learning potential of the model on english data only, we can train it for a few more epochs on the validation set to fine tune the translation process."}}