{"cell_type":{"d146d1d0":"code","3b813add":"code","eda039fe":"code","4424e54e":"code","6a5bd669":"code","a442fe27":"code","2288dfc6":"code","719e48bb":"code","f4a3e2f1":"code","a1f915a4":"code","d0599bef":"code","5e699be2":"code","0d81cce3":"code","ade8f2ab":"code","a67da4cb":"code","b8cab931":"code","1a86749f":"code","9c5f002c":"code","75bf7d9f":"code","4d106f71":"code","ec4f2223":"code","39276a1a":"code","95138e11":"code","9b282fb0":"code","ad753a3b":"code","22c5b31d":"code","cf342f9e":"code","f337971c":"code","cf516174":"code","a38b3305":"code","cb928128":"code","86fc6113":"code","fbe2ca89":"code","6d4c726f":"code","c9c7407e":"code","279d944b":"code","62644e6b":"code","86512ba0":"code","0ebb530b":"code","7d5ab580":"markdown","75566348":"markdown","a7cb2ff1":"markdown","70370480":"markdown"},"source":{"d146d1d0":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\n\n\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3b813add":"df_train = pd.read_csv(os.path.join(dirname, 'banki_ru_train.csv'))\ndf_test = pd.read_csv(os.path.join(dirname, 'banki_ru_test.csv'))\ndf_unlabeled = pd.read_csv(os.path.join(dirname, 'banki_ru_unlabeled.csv')) ","eda039fe":"print('Data shapes:\\n -train: {}\\n -test: {}\\n -unlabeled: {}'.format(\n    df_train.shape, df_test.shape, df_unlabeled.shape))","4424e54e":"df_train.head()","6a5bd669":"df_test.head()","a442fe27":"df_unlabeled.head()","2288dfc6":"df_train = df_train.iloc[:50000,:]\ndf_train['text'] = df_train[\"title\"] + \" \" + df_train[\"text\"]\ndf_train = df_train.drop('title', axis=1)\ndf_train.head()","719e48bb":"import re\n\n\ndel_symbols = '[\\\\\\\\\\'[\\]!\"$%&()*+,-.\/:;<=>?@^_`{|}~\u00ab\u00bb\\n]'\nto_del_symbols = lambda text: re.sub(del_symbols, ' ', text)\nto_del_digits = lambda text: ' '.join([w for w in text.split() if not w.isdigit()])\n\ndef to_tokenize(df):\n    df[\"without_punctuation_text\"] = df[\"text\"].apply(to_del_symbols)\n    df['without_numbers_text'] = df['without_punctuation_text'].apply(to_del_digits)\n    df[\"tokenized_text\"] = df['without_numbers_text']\n    return df","f4a3e2f1":"to_tokenize(df_train)","a1f915a4":"df_train = df_train.drop([\"without_numbers_text\", \"without_punctuation_text\"], axis=1)\ndf_train.head()","d0599bef":"df_train[\"lowercase_text\"] = df_train[\"tokenized_text\"].apply(lambda text: text.lower())\ndf_train.head()","5e699be2":"# import nltk\n# from nltk.corpus import stopwords\n\n# nltk.download(\"stopwords\")\n# stop_words = stopwords.words(\"russian\")\n# print(stop_words)","0d81cce3":"# to_del_sw = lambda text: ' '.join([w for w in text.split() if w not in stop_words])\n# df_train[\"without_sw\"] = df_train[\"lowercase_text\"].apply(to_del_sw)\n# df_train.head()","ade8f2ab":"n_words = len(set(\" \".join(df_train[\"lowercase_text\"]).split()))\nn_words","a67da4cb":"pip install pymorphy2","b8cab931":"import pymorphy2\nfrom pymorphy2 import MorphAnalyzer\n\n\ndef to_lemmatize(df):\n    all_words = \" \".join(df[\"lowercase_text\"]).split()\n#     all_word_list = all_word_str.split()\n    unique_words = np.unique(all_words)\n    lemmatized_words = {}\n    lemmatizer = MorphAnalyzer()\n    for word in unique_words:\n        lemmatized_words[word] = lemmatizer.normal_forms(word)[0]\n    lemm_func = lambda text: ' '.join([lemmatized_words[word] for word in text.split()])\n    df['lemmatized_text'] = df['lowercase_text'].apply(lemm_func)\n    return df","1a86749f":"%%time\nto_lemmatize(df_train)","9c5f002c":"df_train[\"transformed_text\"] = df_train[\"lemmatized_text\"]\ndf_train = df_train.drop([\"tokenized_text\", \"lowercase_text\", \"lemmatized_text\"], axis=1)\ndf_train.head()","75bf7d9f":"n_words = len(set(\" \".join(df_train[\"transformed_text\"]).split()))\nn_words","4d106f71":"from sklearn.feature_extraction.text import CountVectorizer\n\n\nvectorizer = CountVectorizer(min_df=50, ngram_range=(1,5))\nX = vectorizer.fit_transform(df_train[\"transformed_text\"])\ny = df_train[\"target\"].values\nX.shape","ec4f2223":"vocabulary = list(pd.Series(vectorizer.vocabulary_).keys())\nvocabulary[:15]","39276a1a":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_dev, y_train, y_dev = train_test_split(X, y, stratify=y, test_size=0.2)\nprint(\"-----Dimensions------\")\nprint(\"X_train: \", X_train.shape)\nprint(\"X_dev: \", X_dev.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_dev: \", y_dev.shape)","95138e11":"from sklearn.ensemble import RandomForestClassifier\n\n\nclf = RandomForestClassifier(oob_score=True)\nclf.fit(X_train, y_train)","9b282fb0":"from sklearn.metrics import accuracy_score\n\n\ny_dev_pred = clf.predict(X_dev)\nacc = accuracy_score(y_dev, y_dev_pred)\nprint('Accuracy score: ', acc)","ad753a3b":"feature_imporances = pd.Series(clf.feature_importances_, \n                               index=vocabulary).sort_values(ascending=False)\nfeature_imporances","22c5b31d":"imp_features = feature_imporances[feature_imporances > 0.0001].index\nimp_features.shape","cf342f9e":"imp_features_indexes = [i for i in range(len(vocabulary)) if vocabulary[i] in imp_features]\nX_train_decr = X_train[:, imp_features_indexes]\nX_dev_decr = X_dev[:, imp_features_indexes]\nrf_clf = RandomForestClassifier(oob_score=True)\nrf_clf.fit(X_train_decr, y_train)","f337971c":"acc_decr = accuracy_score(y_dev, rf_clf.predict(X_dev_decr))\nprint('Accuracy score after feature selection: ', acc_decr)","cf516174":"df_test = pd.read_csv(os.path.join(dirname, 'banki_ru_test.csv'))\ndf_test = df_test.iloc[:30000,:]\ndf_test.rename(columns = {'title':'text'}, inplace = True)\n\nto_tokenize(df_test)\n\ndf_test = df_test.drop([\"without_numbers_text\", \"without_punctuation_text\"], axis=1)\ndf_test.head()","a38b3305":"df_test[\"lowercase_text\"] = df_test[\"tokenized_text\"].apply(lambda text: text.lower())\n\ndf_test = to_lemmatize(df_test)\ndf_test.head()","cb928128":"df_test[\"transformed_text\"] = df_test[\"lemmatized_text\"]\ndf_test = df_test.drop([\"tokenized_text\", \"lowercase_text\", \"lemmatized_text\"], axis=1)\ndf_test.head()","86fc6113":"X_test = vectorizer.transform(df_test[\"transformed_text\"])\nX_test_decr = X_test[:, imp_features_indexes]\ny_test = df_test[\"target\"].values\nprint(\"Dimension X_test_decr: \", X_test_decr.shape)","fbe2ca89":"y_test_pred = rf_clf.predict(X_test_decr)\nacc_test = accuracy_score(y_test, y_test_pred)\nprint('Test accuracy score: ', acc_test)","6d4c726f":"df_test['prediction'] = y_test_pred\ndf_test[df_test['prediction'] == 1][:60]","c9c7407e":"df_unlabeled = df_unlabeled.iloc[:20000,:]\ndf_unlabeled['text'] = df_unlabeled[\"title\"] + \" \" + df_unlabeled[\"text\"]\ndf_unlabeled = df_unlabeled.drop('title', axis=1)\n\nto_tokenize(df_unlabeled)\n\ndf_unlabeled = df_unlabeled.drop([\"without_numbers_text\", \"without_punctuation_text\"], axis=1)\ndf_unlabeled.head()","279d944b":"df_unlabeled[\"lowercase_text\"] = df_unlabeled[\"tokenized_text\"].apply(lambda text: text.lower())\n\ndf_unlabeled = to_lemmatize(df_unlabeled)\n\ndf_unlabeled[\"transformed_text\"] = df_unlabeled[\"lemmatized_text\"]\ndf_unlabeled = df_unlabeled.drop([\"tokenized_text\", \"lowercase_text\", \"lemmatized_text\"], axis=1)\ndf_unlabeled.head()","62644e6b":"X_unlabeled = vectorizer.transform(df_unlabeled[\"transformed_text\"])\nX_unlabeled_decr = X_unlabeled[:, imp_features_indexes]\nprint(\"Dimension X_unlabeled: \", X_unlabeled_decr.shape)","86512ba0":"y_unlabeled_pred = rf_clf.predict(X_unlabeled_decr)","0ebb530b":"df_unlabeled['prediction'] = y_unlabeled_pred\ndf_unlabeled[df_unlabeled['prediction'] == 1].iloc[:60, 2:]","7d5ab580":"## Tokenization","75566348":"## ----------------------------------------------------","a7cb2ff1":"## Preprocessing","70370480":"## Vectorization"}}