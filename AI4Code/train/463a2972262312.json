{"cell_type":{"f6e3c92b":"code","9fff94b7":"code","da8165ca":"code","93a6f49c":"code","46622a6a":"code","b2a35b51":"code","50a5c18f":"code","67b52258":"code","24cf5eec":"code","cd9330e0":"code","da2cd6bd":"code","0de0a12e":"code","b97bbe29":"code","4bcc8ae0":"code","e2c8d966":"code","5dade458":"code","630ca1e0":"code","37a4bc53":"code","8ba37d33":"code","36fdc03c":"code","2518f182":"code","0dcd9ba9":"code","e20d7678":"code","89bfe592":"code","1831cc7a":"code","1355cf63":"code","97be3461":"code","1fc8ec83":"code","add587ca":"code","6c7a0824":"code","a0893d99":"code","97bb139d":"code","445ca1f8":"code","fb367632":"code","166c6a6b":"code","56dff560":"code","a21d5f0c":"code","4af25f89":"code","7248e77a":"code","00c5beff":"code","84a220af":"code","d6c3f493":"code","96e08e60":"markdown","21eff168":"markdown","fda4e04f":"markdown","d94ba551":"markdown","e6a93ded":"markdown","53b59ca9":"markdown","1e3bd479":"markdown","8db67e71":"markdown","dd186e2f":"markdown","f6fba89b":"markdown","489afedb":"markdown","447515df":"markdown","98b40df1":"markdown","1bedefe9":"markdown","e0f479d7":"markdown","e012a674":"markdown","ce298f3c":"markdown","168171ee":"markdown","d4b23bb7":"markdown","ead4d7d7":"markdown","e8005d76":"markdown","210445d2":"markdown","fbf6186b":"markdown","801a0856":"markdown","74be387a":"markdown","11f6cf17":"markdown","9ca89740":"markdown","df6c44b0":"markdown","d5a2ec7d":"markdown","66e35c31":"markdown","2e3cdce4":"markdown"},"source":{"f6e3c92b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fff94b7":"#Loading in training data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/train.csv\")\n\n#Check data\ntrain_data.head()","da8165ca":"#Check data types since there are multiple data mismatch columns\ntrain_data.dtypes","93a6f49c":"#Check unique values for data\nprint(train_data['Start Date'].value_counts())\nprint(train_data['End Date'].value_counts())\nprint(train_data['Group'].value_counts())\nprint(train_data['MMWR Week'].value_counts())\nprint(train_data['Year'].value_counts())\nprint(train_data['Month'].value_counts())\nprint(train_data['HHS Region'].value_counts())\nprint(train_data['Race and Hispanic Origin Group'].value_counts())\nprint(train_data['Age Group'].value_counts())\nprint(train_data['Footnote'].value_counts())","46622a6a":"region_train_data = train_data[train_data['HHS Region'] != 'United States']\nunitedStates_train_data = train_data[train_data['HHS Region'] == 'United States']\n\nprint(region_train_data['COVID-19 Deaths'].sum())\nprint(unitedStates_train_data['COVID-19 Deaths'].sum())\n","b2a35b51":"train_data = unitedStates_train_data\n#Check for any missing data\nmissingDataTrain = train_data.isna().sum()\nprint(missingDataTrain)","50a5c18f":"train_data.head()","67b52258":"#Load test_data\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-1\/test.csv\")\n\n#Check data, should be without survived column\ntest_data.head()","24cf5eec":"print(test_data['Start Date'].value_counts())\nprint(test_data['End Date'].value_counts())\nprint(test_data['MMWR Week'].value_counts())\nprint(test_data['Group'].value_counts())\nprint(test_data['Year'].value_counts())\nprint(test_data['Month'].value_counts())\nprint(test_data['HHS Region'].value_counts())\nprint(test_data['Race and Hispanic Origin Group'].value_counts())\nprint(test_data['Age Group'].value_counts())","cd9330e0":"test_data.dtypes","da2cd6bd":"train_data = train_data[train_data['Group'] == 'By Week']\n#Check for any missing data\nmissingDataTrain = train_data.isna().sum()\nprint(missingDataTrain)\nprint(train_data.head())","0de0a12e":"train_data = train_data.drop('Footnote', axis = 1)\ntrain_data = train_data.drop('Total Deaths', axis = 1)\ntrain_data = train_data.drop('Year', axis = 1)\ntrain_data = train_data.drop('Month', axis = 1)\ntrain_data = train_data.drop('End Date', axis = 1)\ntrain_data = train_data.drop('Week-Ending Date', axis = 1)\ntrain_data = train_data.drop('Data As Of', axis = 1)\ntrain_data = train_data.drop('Group', axis = 1)\ntrain_data = train_data.drop('id', axis = 1)\ntrain_data = train_data.drop('HHS Region', axis = 1)\n\nprint(train_data.head())","b97bbe29":"train_data['Start Date'] = pd.factorize(train_data['Start Date'])[0]\n#train_data['Start Date'] = train_data.groupby('Start Date').ngroup()\nprint(train_data)","4bcc8ae0":"g = sns.FacetGrid(train_data, col='Race and Hispanic Origin Group', row='Age Group')\ng.map(sns.regplot, 'Start Date', 'COVID-19 Deaths')","e2c8d966":"#See mean, std, min, max of current train_data\nprint(train_data.describe())","5dade458":"#Check for any missing data\nmissingDataTrain = train_data.isna().sum()\nprint(missingDataTrain)","630ca1e0":"#Check unique values for data\nprint(train_data['Race and Hispanic Origin Group'].value_counts())\nprint(train_data['MMWR Week'].value_counts())\nprint(train_data['Age Group'].value_counts())","37a4bc53":"#train_data[train_data['MMWR Week'] == 53] = 1\n#print(train_data['MMWR Week'].value_counts())","8ba37d33":"#Check for any missing data\nmissingDataTrain = train_data.isna().sum()\nprint(missingDataTrain)","36fdc03c":"#Finding outliers\nfig, axs = plt.subplots(1,2, figsize = (20,10))\ntrain_data['COVID-19 Deaths'].plot(ax = axs[0])\ntrain_data.plot(kind = 'box', ax = axs[1])","2518f182":"\nfig, axs = plt.subplots(2,4, figsize = (30,10))\ntrain_data.plot(x = 'Start Date', y = 'COVID-19 Deaths',ax = axs[0][0], title = 'Overall')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Hispanic'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[0][1], title = 'Hispanic')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic American Indian or Alaska Native'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[0][2], title = 'Non-Hispanic American Indian or Alaska Native')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Asian'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[0][3], title = 'Non-Hispanic Asian')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic More than one race'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][0], title = 'Non-Hispanic More than one race')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Native Hawaiian or Other Pacific Islander'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][1], title = 'Non-Hispanic Native Hawaiian or Other Pacific Islander')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic White'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][2], title = 'Non-Hispanic White')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Unknown'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][3], title = 'Unknown')","0dcd9ba9":"#absolute_differences_from_mean = np.abs(train_data['COVID-19 Deaths'] - np.mean(train_data['COVID-19 Deaths']))\n#train_data['COVID-19 Deaths'] = train_data['COVID-19 Deaths'].where(absolute_differences_from_mean > (np.std(train_data['COVID-19 Deaths']) * 5), np.nanmean(train_data['COVID-19 Deaths']))","e20d7678":"series = train_data['COVID-19 Deaths']\nabsolute_differences_from_mean = np.abs(series - np.mean(series))\nseries[absolute_differences_from_mean > (np.std(train_data['COVID-19 Deaths'] )*3)] = np.nanmean(series)\ntrain_data.mask(absolute_differences_from_mean > (np.std(train_data['COVID-19 Deaths'] )* 3),np.nanmean(train_data['COVID-19 Deaths']))","89bfe592":"fig, axs = plt.subplots(2,4, figsize = (30,10))\ntrain_data.plot(x = 'Start Date', y = 'COVID-19 Deaths',ax = axs[0][0], title = 'Overall')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Hispanic'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[0][1], title = 'Hispanic')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic American Indian or Alaska Native'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[0][2], title = 'Non-Hispanic American Indian or Alaska Native')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Asian'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[0][3], title = 'Non-Hispanic Asian')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic More than one race'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][0], title = 'Non-Hispanic More than one race')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic Native Hawaiian or Other Pacific Islander'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][1], title = 'Non-Hispanic Native Hawaiian or Other Pacific Islander')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Non-Hispanic White'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][2], title = 'Non-Hispanic White')\ntrain_data[train_data['Race and Hispanic Origin Group'] == 'Unknown'].plot(x = 'Start Date', y= 'COVID-19 Deaths', ax = axs[1][3], title = 'Unknown')","1831cc7a":"#Showcasing Data points after removing some outliers\nfig, axs = plt.subplots(1,2, figsize = (20,10))\ntrain_data['COVID-19 Deaths'].plot(ax = axs[0])\ntrain_data.plot(kind = 'box', ax = axs[1])","1355cf63":"train_cat =train_data.select_dtypes(include = 'object')\ntrain_cat_prepped = train_cat.merge(pd.get_dummies(train_cat, drop_first= False), left_index=True, right_index=True)\ntrain_cat_prepped.drop('Race and Hispanic Origin Group', axis = 1,inplace = True)\ntrain_cat_prepped.drop('Age Group', axis = 1,inplace = True)\nprint(train_cat_prepped.head())\nprint(train_cat_prepped.shape)","97be3461":"train_numerics = train_data.select_dtypes(exclude = 'object')\ntrain_cols = train_numerics.columns\ntrain_numerics.columns = train_cols\nprint(train_numerics.head())\nprint(train_numerics.shape)","1fc8ec83":"train_prep_data = pd.merge(train_cat_prepped, train_numerics, left_index = True, right_index = True)\ntrain_prep_data = train_prep_data.set_index('Start Date')\n#train_prep_data = pd.get_dummies(train_prep_data, prefix = ['MMWR'], columns=['MMWR Week'], drop_first = True)\n#train_prep_data['COVID-19 Deaths']= train_prep_data['COVID-19 Deaths'].replace(0,train_prep_data['COVID-19 Deaths'].median())\n#train_prep_data['t_startDate1'] = train_prep_data.index + 1\n#train_prep_data['t_startDate2'] = train_prep_data['t_startDate1']**2\ntrain_prep_data['COVID-19 Deaths'] = train_prep_data['COVID-19 Deaths'].astype(int)\ntrain_prep_data['MMWR Week'] = train_prep_data['MMWR Week'].astype(int)\nprint(train_prep_data.shape)\nprint(train_prep_data.dtypes)\n","add587ca":"#Check for any missing data\nmissingDataTrain = train_prep_data.isna().sum()\nprint(missingDataTrain)","6c7a0824":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\n\n\nX = train_prep_data.drop('COVID-19 Deaths', axis = 1)\ny = train_prep_data['COVID-19 Deaths']\n\n\n# Create time-series cross-validation object\ncv = TimeSeriesSplit(n_splits=50)\nmodel = LinearRegression()\n\npipeline = Pipeline([('model', model)])\nresults1= np.empty(50, dtype=object) \n# Iterate through CV splits\nfig, ax = plt.subplots(1,3,figsize = (30,10))\nfor ii, (tr, tt) in enumerate(cv.split(X, y)):\n    \n    pipeline.fit(X.iloc[tr], y.iloc[tr])\n    \n    # Generate predictions on the test data and collect\n    prediction = np.maximum(model.predict(X.iloc[tt]),0.)\n    \n    mse = mean_squared_error(y.iloc[tt], prediction)\n    rmse = np.sqrt(mse)\n    results1[ii] = rmse\n    \n    \n    # Plot the training data on each iteration, to see the behavior of the CV\n    ax[0].plot(tr, ii + y.iloc[tr])\n    ax[0].set_title('Training Data')\n    ax[1].plot(tt, ii + prediction)\n    ax[1].set_title('Predicted Data')\n    ax[2].plot(tt, ii + y.iloc[tt])\n    ax[2].set_title('Testing Data')\n    \n    \nfig.suptitle('Training data and Predicted data displayed')\nplt.show()\n\nsns.displot(results1)\ncv_results = cross_validate(\n        pipeline,\n        X,\n        y,\n        cv=cv,\n        scoring=[\"neg_root_mean_squared_error\"],\n    )\n\n\nrmse = -cv_results[\"test_neg_root_mean_squared_error\"]\nprint(f\"Root Mean Squared Error: {rmse.mean():.3f} +\/- {rmse.std():.3f}\")","a0893d99":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\n\nX = train_prep_data.drop('COVID-19 Deaths', axis = 1)\n\ny = train_prep_data['COVID-19 Deaths']\n\n# Create time-series cross-validation object\ncv = TimeSeriesSplit(n_splits=50)\nmodel = Ridge(alpha = 1.0)\nscaler = StandardScaler()\npipeline = Pipeline([('scaler', scaler),  ('model', model)])\n\n\nresults2= np.empty(50, dtype=object) \n# Iterate through CV splits\nfig, ax = plt.subplots(1,3,figsize = (30,10))\nfor ii, (tr, tt) in enumerate(cv.split(X, y)):\n    \n    pipeline.fit(X.iloc[tr], y.iloc[tr])\n    \n    # Generate predictions on the test data and collect\n    prediction = model.predict(X.iloc[tt])\n    \n    mse = mean_squared_error(y.iloc[tt], prediction)\n    rmse = np.sqrt(mse)\n    results2[ii] = rmse\n    \n    \n    # Plot the training data on each iteration, to see the behavior of the CV\n    ax[0].plot(tr, ii + y.iloc[tr])\n    ax[0].set_title('Training Data')\n    ax[1].plot(tt, ii + prediction)\n    ax[1].set_title('Predicted Data')\n    ax[2].plot(tt, ii + y.iloc[tt])\n    ax[2].set_title('Testing Data')\n    \n    \nfig.suptitle('Training data and Predicted data displayed')\nplt.show()\n\nsns.displot(results2)\ncv_results = cross_validate(\n       pipeline,\n       X,\n       y,\n       cv=cv,\n       scoring=[\"neg_root_mean_squared_error\"],\n   )\n\nrmse = -cv_results[\"test_neg_root_mean_squared_error\"]\nprint(f\"Root Mean Squared Error: {rmse.mean():.3f} +\/- {rmse.std():.3f}\")\n","97bb139d":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nX = train_prep_data.drop('COVID-19 Deaths', axis = 1)\ny = train_prep_data['COVID-19 Deaths']\n\n# Create time-series cross-validation object\ncv = TimeSeriesSplit(n_splits=50)\nmodel = Lasso(alpha=0.1, normalize=True, positive = True)\nscalar = StandardScaler()\n\npipeline = Pipeline([('transformer', scalar), ('estimator', model)])\n\nresults3= np.empty(50, dtype=object) \n# Iterate through CV splits\nfig, ax = plt.subplots(1,3,figsize = (30,10))\nfor ii, (tr, tt) in enumerate(cv.split(X, y)):\n    \n    pipeline.fit(X.iloc[tr], y.iloc[tr])\n    \n    # Generate predictions on the test data and collect\n    prediction = model.predict(X.iloc[tt])\n    \n    mse = mean_squared_error(y.iloc[tt], prediction)\n    rmse = np.sqrt(mse)\n    results3[ii] = rmse\n    \n    \n    # Plot the training data on each iteration, to see the behavior of the CV\n    ax[0].plot(tr, ii + y.iloc[tr])\n    ax[0].set_title('Training Data')\n    ax[1].plot(tt, ii + prediction)\n    ax[1].set_title('Predicted Data')\n    ax[2].plot(tt, ii + y.iloc[tt])\n    ax[2].set_title('Testing Data')\n    \n    \nfig.suptitle('Training data and Predicted data displayed')\nplt.show()\n\nsns.displot(results3)\ncv_results = cross_validate(\n        pipeline,\n        X,\n        y,\n        cv=cv,\n        scoring=[\"neg_root_mean_squared_error\"],\n    )\n\nrmse = -cv_results[\"test_neg_root_mean_squared_error\"]\nprint(f\"Root Mean Squared Error: {rmse.mean():.3f} +\/- {rmse.std():.3f}\")","445ca1f8":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nX = train_prep_data.drop('COVID-19 Deaths', axis = 1)\ny = train_prep_data['COVID-19 Deaths']\n\n\n# Create time-series cross-validation object\ncv = TimeSeriesSplit(n_splits=50)\nmodel = ElasticNet(alpha = 0.1)\nscalar = StandardScaler()\n\npipeline = Pipeline([('transformer', scalar), ('estimator', model)])\n\nresults4= np.empty(50, dtype=object) \n# Iterate through CV splits\nfig, ax = plt.subplots(1,3,figsize = (30,10))\nfor ii, (tr, tt) in enumerate(cv.split(X, y)):\n    \n    pipeline.fit(X.iloc[tr], y.iloc[tr])\n    \n    # Generate predictions on the test data and collect\n    prediction = model.predict(X.iloc[tt])\n    \n    mse = mean_squared_error(y.iloc[tt], prediction)\n    print(mse)\n    rmse = np.sqrt(mse)\n    results4[ii] = rmse\n    \n    \n    # Plot the training data on each iteration, to see the behavior of the CV\n    ax[0].plot(tr, ii + y.iloc[tr])\n    ax[0].set_title('Training Data')\n    ax[1].plot(tt, ii + prediction)\n    ax[1].set_title('Predicted Data')\n    ax[2].plot(tt, ii + y.iloc[tt])\n    ax[2].set_title('Testing Data')\n    \n    \nfig.suptitle('Training data and Predicted data displayed')\nplt.show()\n\nsns.displot(results4)\ncv_results = cross_validate(\n        pipeline,\n        X,\n        y,\n        cv=cv,\n        scoring=[\"neg_root_mean_squared_error\"],\n    )\nrmse = -cv_results[\"test_neg_root_mean_squared_error\"]\nprint(f\"Root Mean Squared Error: {rmse.mean():.3f} +\/- {rmse.std():.3f}\")","fb367632":"results = pd.DataFrame({'ols': list(results1), 'ridge': list(results2), 'lasso': list(results3), 'elastic': list(results4)})\nprint(results.shape)\nprint(results.describe())","166c6a6b":"test_data = test_data.drop('Total Deaths', axis = 1)\ntest_data = test_data.drop('Year', axis = 1)\ntest_data = test_data.drop('Month', axis = 1)\ntest_data = test_data.drop('End Date', axis = 1)\ntest_data = test_data.drop('Week-Ending Date', axis = 1)\ntest_data = test_data.drop('Data As Of', axis = 1)\ntest_data = test_data.drop('Group', axis = 1)\ntest_id = test_data.id\ntest_data = test_data.drop('id', axis = 1)\ntest_data = test_data.drop('HHS Region', axis = 1)\nprint(test_data.head())","56dff560":"test_data['Start Date'] = pd.factorize(test_data['Start Date'])[0]\nprint(test_data.head())","a21d5f0c":"test_cat =test_data.select_dtypes(include = 'object')\ntest_cat_prepped = test_cat.merge(pd.get_dummies(test_cat, drop_first= False), left_index=True, right_index=True)\ntest_cat_prepped.drop('Race and Hispanic Origin Group', axis = 1,inplace = True)\ntest_cat_prepped.drop('Age Group', axis = 1,inplace = True)\nprint(test_cat_prepped.head())\nprint(test_cat_prepped.shape)","4af25f89":"test_numerics = test_data.select_dtypes(exclude = 'object')\ntest_cols = test_numerics.columns\ntest_numerics.columns = test_cols\nprint(test_numerics.head())\nprint(test_numerics.shape)","7248e77a":"test_prep_data = pd.merge(test_cat_prepped, test_numerics, left_index = True, right_index = True)\ntest_prep_data = test_prep_data.set_index('Start Date')\n#test_prep_data['t_startDate1'] = test_prep_data.index + 1\n#test_prep_data['t_startDate2'] = test_prep_data['t_startDate1']**2\n#test_prep_data = pd.get_dummies(test_prep_data, prefix = ['MMWR'], columns=['MMWR Week'], drop_first = True)\nprint(test_prep_data.shape)\nprint(test_prep_data.dtypes)","00c5beff":"#See mean, std, min, max of current train_data\nprint(test_prep_data.describe())","84a220af":"#Check for any missing data\nmissingDataTest = test_prep_data.isna().sum()\nprint(missingDataTest)","d6c3f493":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = train_prep_data.drop('COVID-19 Deaths', axis = 1)\ny_train = train_prep_data['COVID-19 Deaths']\n\nX_test = test_prep_data\n\nmodel = ElasticNet(alpha = 0.1)\nscalar = StandardScaler()\n\npipeline = Pipeline([('transformer', scalar), ('estimator', model)])\n\nmodel.fit(X_train, y_train)\npredictions =np.maximum(model.predict(X_test), 0.).astype(int)\n\n\n\noutput = pd.DataFrame({'id': test_id, 'COVID-19 Deaths': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","96e08e60":"**Drop any unecessary columns**","21eff168":"**Finding patterns**","fda4e04f":"**Creating numerical values for the categorical data**","d94ba551":"# Submission Model","e6a93ded":"**Build Model and Evaluate**","53b59ca9":"# Evaluating Test Data","1e3bd479":"# Evaluating Training Data","8db67e71":"##  Ordinary Least Squares model","dd186e2f":"**Check that for HHS Region, United States covers the other regions**","f6fba89b":"**Loading Testing Data**","489afedb":"Setting Start Date as an index for the train_data","447515df":"**Merg Categorical and Numerical Data together**","98b40df1":"* Footnote is dropped as it does not correlate to any of the data or to covid deaths\n* Total Deaths is dropped as it does not provide enough information to use with covid deaths\n* Year and Month is dropped because for year it only has 2020 so it does not help with predicting years after that and Month is not as specific as week\n* End date is dropped as we will be using the start date as the date time for the time series\n* Week-Ending Date is the same as End Date so it's dropped\n* Group is dropped once we have the train data organized by week\n* HHS Region is dropped because all the Region will be is the United States","1bedefe9":"**Change all 53 values in the MMWR Week to 1 as that indicates a new year**","e0f479d7":"**In order to handle outliers will replace any values in the COVID 19 deaths with a mean difference greater than 3 * the STD of the data with the mean of the data**","e012a674":"## Handling Covid Deaths Outliers","ce298f3c":"# Loading Training and Testing Data","168171ee":"**Build Model**","d4b23bb7":"## Seperating numerical and categorical data","ead4d7d7":"## Ridge Regression Model","e8005d76":"## Summary Statistics For Models","210445d2":"## Notice that the test data is grouped by week, so the train data should also resemble that","fbf6186b":"**Build Model**","801a0856":"**Checking for missing data**","74be387a":"## Elastic Net Regression Model","11f6cf17":"**Build Model**","9ca89740":"## Evaluating Models","df6c44b0":"## Lasso Regression Model","d5a2ec7d":"**Build and Submit Model**","66e35c31":"**Finding Outliers in the data**","2e3cdce4":"Based on the results above, we can conclude that the United States accumlates the covid death from the regions, so we really don't need the other regions and can stick with just the United States"}}