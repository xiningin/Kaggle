{"cell_type":{"1f316a3a":"code","1255000e":"code","d81ff117":"code","5eaeaaa5":"code","05411131":"code","6a87aeb2":"code","6bf383d6":"code","a65125f4":"code","96f4b1f0":"code","425e32e8":"code","c60d6326":"code","62f392e8":"code","dd0c3ec1":"code","d4096709":"code","9374caa7":"code","cebc64f8":"code","e254a33f":"code","94cc09e0":"code","5256c8c1":"code","970fd6ca":"code","f5395b02":"code","4f7a5057":"code","b29948df":"code","79790436":"code","55ab5821":"code","ee8db4ea":"code","db0de151":"code","2fa2cca4":"code","c6eab72c":"code","0efba42d":"code","4e706029":"code","141d0228":"code","fdb13c5e":"code","8a81d64a":"code","c3cedb0e":"code","fcc090b4":"code","5ee14aeb":"code","b58b2b97":"code","3bd76b97":"code","280cdf6b":"code","10889e68":"code","efc30dbe":"code","f5714b3e":"code","63d35b20":"code","ccb35eee":"code","79d900db":"code","b90b42df":"code","c7e053d0":"code","193bc04b":"code","8ee960cf":"code","2c713b85":"code","be06ad0c":"code","de8bc7a7":"code","9a363712":"code","4c9e8e23":"code","d34f8767":"code","537dadb6":"code","0f68ae0b":"code","e33f3ce4":"code","e4d798af":"code","f255f840":"code","45e6f939":"code","d9edc4f2":"code","ed067829":"code","f6d31634":"code","1450e353":"code","7ae044e0":"code","255fe896":"code","3ac844f1":"code","fa1fc498":"code","dd103908":"code","95ab580d":"code","3c466357":"code","ed0b2094":"code","976fee64":"code","9d6c0fd3":"code","95368d1d":"code","58b086be":"code","90be6961":"markdown","6bde64f7":"markdown","e80de5f6":"markdown","007ecccb":"markdown","1a211579":"markdown","73d00bc8":"markdown","5a6a4972":"markdown","15f9b4b7":"markdown","93f76851":"markdown","7b255468":"markdown","454a2557":"markdown","a025e6a2":"markdown","641880cf":"markdown","31c418a2":"markdown","cbf617f6":"markdown","f1b5047d":"markdown","a9b30bb9":"markdown","c6d70898":"markdown","77bc2f4f":"markdown","5d21fb22":"markdown","6668dba9":"markdown","89c4890b":"markdown","80131cfc":"markdown","ec54fd56":"markdown","a987d192":"markdown","69c8b636":"markdown","70d15fee":"markdown","d378ea1f":"markdown","41542d3d":"markdown","6af8cb4b":"markdown","ed63d68c":"markdown","cdeaaccb":"markdown","deadaa05":"markdown","288a329b":"markdown","9c85e44e":"markdown","a8fd238c":"markdown","658386a0":"markdown","59858cf3":"markdown","3e6316ef":"markdown","dc5adcce":"markdown"},"source":{"1f316a3a":"from sklearn.datasets import load_boston\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np","1255000e":"boston = load_boston()\ncancer = load_breast_cancer()\n\nbos_df = pd.DataFrame(boston['data'],columns = boston['feature_names'])\nbos_df['target'] = boston['target']\n\ncan_df = pd.DataFrame(cancer['data'],columns = cancer['feature_names'])\ncan_df['target'] = cancer['target']","d81ff117":"def train_test_clf(variable, target, test_size):\n    X_train, X_test, y_train, y_test = train_test_split(variable, target, stratify = target, test_size = test_size, random_state = 0)\n    \n    return X_train, X_test, y_train, y_test\n\ndef train_test_reg(variable, target, test_size):\n    X_train, X_test, y_train, y_test = train_test_split(variable, target, test_size = test_size, random_state = 0)\n    \n    return X_train, X_test, y_train, y_test","5eaeaaa5":"from sklearn.neighbors import KNeighborsRegressor","05411131":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = KNeighborsRegressor()\n\nreg.fit(X_train, y_train)","6a87aeb2":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","6bf383d6":"from sklearn.neighbors import KNeighborsClassifier","a65125f4":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\nclf = KNeighborsClassifier()\n\nclf.fit(X_train, y_train)","96f4b1f0":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","425e32e8":"from sklearn.svm import SVR","c60d6326":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = SVR()\n\nreg.fit(X_train, y_train)","62f392e8":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","dd0c3ec1":"from sklearn.svm import SVC","d4096709":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\nclf = SVC()\n\nclf.fit(X_train, y_train)","9374caa7":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","cebc64f8":"from sklearn.linear_model import LinearRegression","e254a33f":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = LinearRegression()\n\nreg.fit(X_train, y_train)","94cc09e0":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","5256c8c1":"from sklearn.linear_model import Ridge","970fd6ca":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = Ridge()\n\nreg.fit(X_train, y_train)","f5395b02":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","4f7a5057":"from sklearn.linear_model import Lasso","b29948df":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = Lasso()\n   \nreg.fit(X_train, y_train)","79790436":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","55ab5821":"from sklearn.linear_model import ElasticNet","ee8db4ea":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = ElasticNet()\n   \nreg.fit(X_train, y_train)","db0de151":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","2fa2cca4":"from sklearn.linear_model import LogisticRegression","c6eab72c":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = LogisticRegression()\n   \nclf.fit(X_train, y_train)","0efba42d":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","4e706029":"from sklearn.naive_bayes import GaussianNB","141d0228":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = GaussianNB()\n   \nclf.fit(X_train, y_train)","fdb13c5e":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","8a81d64a":"from sklearn.ensemble import GradientBoostingRegressor","c3cedb0e":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = GradientBoostingRegressor()\n   \nreg.fit(X_train, y_train)","fcc090b4":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","5ee14aeb":"from sklearn.ensemble import GradientBoostingClassifier","b58b2b97":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = GradientBoostingClassifier()\n   \nclf.fit(X_train, y_train)","3bd76b97":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","280cdf6b":"from sklearn.ensemble import AdaBoostRegressor","10889e68":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = AdaBoostRegressor()\n   \nreg.fit(X_train, y_train)","efc30dbe":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","f5714b3e":"from sklearn.ensemble import AdaBoostClassifier","63d35b20":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = AdaBoostClassifier()\n   \nclf.fit(X_train, y_train)","ccb35eee":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","79d900db":"import xgboost as xgb","b90b42df":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = xgb.XGBRegressor()\n   \nreg.fit(X_train, y_train)","c7e053d0":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","193bc04b":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = xgb.XGBClassifier()\n   \nclf.fit(X_train, y_train)","8ee960cf":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","2c713b85":"import lightgbm as lgb","be06ad0c":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = lgb.LGBMRegressor()\n   \nreg.fit(X_train, y_train)","de8bc7a7":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","9a363712":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = lgb.LGBMClassifier()\n   \nclf.fit(X_train, y_train)","4c9e8e23":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","d34f8767":"import catboost as cb","537dadb6":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = cb.CatBoostRegressor()\n   \nreg.fit(X_train, y_train)","0f68ae0b":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","e33f3ce4":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = cb.CatBoostClassifier()\n   \nclf.fit(X_train, y_train)","e4d798af":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","f255f840":"from sklearn.tree import DecisionTreeRegressor","45e6f939":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = DecisionTreeRegressor()\n   \nreg.fit(X_train, y_train)","d9edc4f2":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","ed067829":"from sklearn.tree import DecisionTreeClassifier","f6d31634":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = DecisionTreeClassifier()\n   \nclf.fit(X_train, y_train)","1450e353":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","7ae044e0":"from sklearn.ensemble import ExtraTreesRegressor","255fe896":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = ExtraTreesRegressor()\n   \nreg.fit(X_train, y_train)","3ac844f1":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","fa1fc498":"from sklearn.ensemble import ExtraTreesClassifier","dd103908":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = ExtraTreesClassifier()\n   \nclf.fit(X_train, y_train)","95ab580d":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","3c466357":"from sklearn.ensemble import RandomForestRegressor","ed0b2094":"X_train, X_test, y_train, y_test = train_test_reg(np.array(bos_df.drop(columns = 'target')), np.array(bos_df['target']), 0.25)\n\n\nreg = RandomForestRegressor()\n   \nreg.fit(X_train, y_train)","976fee64":"print(f'Training set score : {reg.score(X_train, y_train)}')\nprint(f'Test set score : {reg.score(X_test,y_test)}')","9d6c0fd3":"from sklearn.ensemble import RandomForestClassifier","95368d1d":"X_train, X_test, y_train, y_test = train_test_clf(np.array(can_df.drop(columns = 'target')), np.array(can_df['target']), 0.25)\n\n\nclf = RandomForestClassifier()\n   \nclf.fit(X_train, y_train)","58b086be":"print(f'Training set score : {clf.score(X_train, y_train)}')\nprint(f'Test set score : {clf.score(X_test,y_test)}')","90be6961":"### 6-1-1. Gradient Boosting Regressor","6bde64f7":"# 8. Extra Tree(Ensemble)\n\n    - Important parameters\n        - n_estimators\n        - pre prunning\n        \n    - Advantages\n        - Increased model randomness to increase bias and reduce variance\n        - Performance similar to RF\n        - cost less than RF\n        \n    - Disadvantages\n        - Low generalize","e80de5f6":"# 9. Random Forest(Ensemble)\n    \n    - Important parameters\n        - n_estimators : tree number\n        - pre prunnung\n            - max_features\n            - max_depth\n        - learning_rate\n        \n    - Advantages\n        - low parameters tuning high performances\n        - Normalization, no scale required\n        - CPU parallel processing possible : n_jobs\n        \n    - Disadvantages\n        - Does not work high dimension and sparing data like text\n        - use a lot of memory\n            - training, predict low speed\n","007ecccb":"# 1. KNN\n\n    - Important parameters\n        - Distance method\n        - K : neighbor number","1a211579":"## 1-1. KNN regressor","73d00bc8":"- ***Train score is better than test score -> overfitting***","5a6a4972":"### 6-1-2. Gradient Boosting Classifier","15f9b4b7":"## 9-2. Random Forest Classifier","93f76851":"### 6-5-1. Catboost Regressor","7b255468":"## 8-1. ExtraTree Regressor","454a2557":"### 6-4-1. LGBM Regressor","a025e6a2":"# 2. SVM\n    - Important parameters\n        - C : The smaller the value, the simpler the model.\n        - gamma : RBF kernel\n        - Use L2 Regularation\n\n    - Advantages\n        - Works well with all data, including low and high dimensional data sets\n    \n    - Disadvantages\n        - Doesn't work well with lots of samples\n            - 10,000 samples works well\n            - 100,000 low performances\n        - Sensitivity parameters\n        - blackbox model\n        - require scale","641880cf":"# 4. Ridge Regression\n    - Important parameters\n        - alpha : L2 Regulation\n            - The larger the value, the simpler the model.\n        - This model use if it is judged that there are many important variables\n            - It makes it close to 0, but since there is no case that the variable is not used by making the weight of the variable 0 at all\n        - solver = 'sag' : very large dataset","31c418a2":"## 6-5. Catboost","cbf617f6":"# 3. Linear Regression(OLS)","f1b5047d":"### 6-3-1. Xgboost Classifier","a9b30bb9":"## 7-2. Decision Tree Classifier","c6d70898":"## 6-4. Light GBM","77bc2f4f":"# Library & Prepare Dataset","5d21fb22":"### 6-2-1. Ada Boost Classifier","6668dba9":"### 6-5-1. Catboost Classifier","89c4890b":"### 6-3-1. Xgboost Regressor","80131cfc":"## 7-1. Decision Tree Regressor","ec54fd56":"# 6. Boosting","a987d192":"# 5. Lasso Regression\n    - Important parameters\n        - alpha : L1 Regulation\n            - The larger the value, the simpler the model.\n        - This model not use if it is judged that there are many important variables\n            - Because we are making it close to 0 and assigning some variable weights to 0","69c8b636":"## 2-1. Support Vector Classifier","70d15fee":"# 7. Logistic Regression\n    - Important parameters\n        - C\n        - Use L2 Regularation\n        - solver = 'sag' : very large dataset\n        \n    - Advantages\n        - The important characteristics used in the model and the model can be described.","d378ea1f":"## 6-1. Gradient Boosting(Ensemble)\n    - Important parameters\n        - n_estimators\n        - learning_rate\n            -> It is good practice to optimize the learning_rate after finding the appropriate n_estimators.\n        - max_depth\n        - max_leaf_nodes\n        \n    - Advantages\n        - adaption classification, squence data \n    \n    - Disadvantages\n        - The parameters need to be finely adjusted and the training time is long\n        - Doesn't work well with sparse high-dimensional data\n","41542d3d":"### 6-2-1. Ada Boost Regressor","6af8cb4b":"### 6-4-2. LGBM Classifier","ed63d68c":"## 2-1. Support Vector Regressor","cdeaaccb":"## 8-2. ExtraTree Classifier","deadaa05":"## 6-3. Xgboost","288a329b":"## 6-2. Ada Boost","9c85e44e":"# 8. Naive Bayesian\n\n    - Important parameters\n        - alpha : smaller -> model complex\n        \n    - Advantages\n        - train, predict high speed\n        - Easy to understand the training process\n        - Works well on sparse high-dimensional data and is not parameter sensitive","a8fd238c":"- ***This notebook is meaningful in creating base modeling for data to be analyzed quickly using the currently widely commercialized ML Model.***\n- ***All parameters used in the model use default values \u200b\u200bas the purpose is to check the baseline with various models.***\n- ***We will use the boston and breast cancer data to cover both classification and regression problems.***","658386a0":"# 6. ElasticNet\n    - Important parameters\n        - alpha : L1, L2 Regularation","59858cf3":"# 7. Decision tree\n\n    - Important parameters\n        - pre prunning : reduce overfitting\n            - max_depth\n            - max_leaf_nodes\n            - min_samples_leaf\n\n    - Advantages\n        - Easy model visualization\n        - free data scaling\n            - Normalization, no scale required\n            \n    - Disadvantages\n        - Poor generalization performance\n            - Because there are many overfitting problems that cannot be solved by pre-pruning\n","3e6316ef":"## 9-1. Random Forest Regressor","dc5adcce":"## 1-2. KNN Classifier"}}