{"cell_type":{"c39d89a9":"code","6a024546":"code","479f0ff5":"code","b9ecf230":"code","1a30c4e7":"code","968140b7":"code","f20e89e1":"code","b70d2c4a":"code","21fd467e":"code","1c6f8fd4":"code","d5a88dde":"code","3f47fb69":"code","52ef21f3":"code","8c42eca4":"code","0c705069":"code","b941e1f1":"code","aef4e6bb":"code","54699282":"code","8b258d89":"code","20def786":"code","23061138":"code","9a80817d":"code","99344c6a":"code","0bdb9a21":"code","480160af":"code","ec07293c":"code","8d94f3ce":"code","937642b5":"code","179543b1":"code","14776e65":"code","4879e3bc":"code","73665882":"code","2f971f6c":"code","25fa593e":"code","cd5f8bc7":"code","ef7dfd5d":"code","19e45317":"code","068fb863":"code","b1161c92":"code","7b851ab6":"code","a8ffc15c":"code","fa19f582":"code","f0ed0ffc":"code","dcf5abe4":"code","c6e4b4f0":"code","dc52744f":"code","5f3e1d86":"code","652979cd":"code","45b95f37":"code","53c7be00":"code","ebb35617":"code","5a2f0b73":"code","7234ee89":"code","1db136ec":"code","f3eeca53":"code","be2bcc37":"code","e89540bf":"code","cb9de089":"code","1daa5176":"code","93d6ca57":"code","acbb1a82":"code","119d0565":"code","7cc59c2a":"code","bc69e366":"code","ad0e5683":"code","13f790e3":"code","ec039d1f":"code","f0a4c86d":"code","45582596":"code","bc3bcaf4":"code","270c4e01":"code","cbfe9853":"code","d1520135":"markdown","abe4b0bd":"markdown","b631efe0":"markdown","88d13418":"markdown","10d3b743":"markdown","80b0faba":"markdown","b992637f":"markdown","cfb506a7":"markdown","64d5c354":"markdown","07f312e4":"markdown","5b8302fe":"markdown","303d7592":"markdown","7ebd91e9":"markdown","3a01b8df":"markdown","61b4e708":"markdown","df64b399":"markdown","9a57c90f":"markdown","b0775a8f":"markdown","337bd662":"markdown","38dc21ae":"markdown","4c9c1c52":"markdown","d4ae033b":"markdown","a6213a92":"markdown","09c55fdf":"markdown","dac4d681":"markdown","911d0447":"markdown","9eec29d2":"markdown","22c71e63":"markdown","fb91b7ee":"markdown","580eaa46":"markdown","7bcb7423":"markdown","f83f9746":"markdown","9b978895":"markdown","3330ef51":"markdown","7aa7b2e1":"markdown","4bdca10c":"markdown","42b80580":"markdown","92d3ea38":"markdown","21021269":"markdown","e081757f":"markdown","594c6d97":"markdown","c980a398":"markdown","36340e0c":"markdown","d40eaad5":"markdown","3d80c0f7":"markdown","b119ff9c":"markdown","7fdd56f4":"markdown","4cc5d23e":"markdown"},"source":{"c39d89a9":"# General tools\nimport pandas as pd\nimport numpy as np\nimport os, math\nfrom collections import Counter\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nmorancolor=sns.color_palette(['#6a2202', '#bc7201', '#e5ab09', '#22180d', '#0f1a26','#241c24', '#745656', '#c7b44f', '#977f48', '#392c23'])\nplt.style.use(\"fivethirtyeight\")\nsns.set_palette(morancolor)\n\nplt.rcParams['font.family']='serif'\nplt.rcParams['figure.dpi'] =100 # high resolution\n\n# Manage warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data Preprocessing\nfrom sklearn.preprocessing import QuantileTransformer,scale,normalize,minmax_scale\n\n# Model Selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\n# Statistical\nimport scipy.stats\nimport statsmodels.api as sm","6a024546":"df=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.drop_duplicates(inplace=True)\ndf.dropna(how='all',inplace=True)\ndf.sample(3)","479f0ff5":"df=df.rename(columns={\"DiabetesPedigreeFunction\":\"Dpf\"})","b9ecf230":"df.info()","1a30c4e7":"df.describe()","968140b7":"df.isnull().sum().sort_values(ascending=False)","f20e89e1":"df.min()","b70d2c4a":"def despine():\n    sns.despine(top=1,bottom=1,right=1,left=1)\n    \ndef title(title,fontsize=13):\n    plt.title(title,fontweight='bold',fontsize=fontsize)","21fd467e":"def countplot(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10,color=None):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.countplot(df[col],color=color)\n        for p in ax.patches:\n            ax.annotate(f\"{p.get_height()\/df[col].shape[0]*100:.2f}%\",xy=[p.get_x(),p.get_height()],fontsize=annotsize)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')","1c6f8fd4":"countplot(df,['Outcome'],annotsize=10,cut=1,w=7)\ntitle('Distribution of Outcome',fontsize=17)","d5a88dde":"from scipy.stats import skew\ndef kdeall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,meanskew=True,kdecut=0,legendsize=10,xlabelsize=13,loc='best'):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if meanskew==True:\n            sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,label=f'Skewness: {skew(df.dropna(subset=[col])[col]):.2f}',lw=3)\n            plt.axvline(df.dropna(subset=[col])[col].mean(),label='mean',color='#22180d',lw=1.5)\n            plt.axvline(df.dropna(subset=[col])[col].median(),label='median',ls='--',color='#22180d',lw=1.5)\n            plt.legend(fontsize=legendsize,loc=loc)\n        else: sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,lw=3)\n        sns.rugplot(df.dropna(subset=[col])[col])\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)","3f47fb69":"def kde2(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,xlabel=12):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.kdeplot(data=df.dropna(subset=[col,target]),x=col,hue=target,cut=0)\n        sns.rugplot(data=df.dropna(subset=[col,target]),x=col,hue=target)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabel)","52ef21f3":"def boxall(df,lst,hspace=.5,wspace=.25,cut=3,h=7,w=10,target=None):\n    f=plt.figure(figsize=(w,h))\n    for i,col in enumerate(lst):\n        f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if target==None: sns.boxplot(y=df[col],showmeans=True)\n        else: sns.boxplot(x=df[target],y=df[col],showmeans=True)\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')\n        sns.despine(top=1,bottom=1,left=1,right=1)\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)","8c42eca4":"def barall(df,lst,target,hspace=.5,wspace=.25,cut=3,h=7,w=10,color=None):\n    f=plt.figure(figsize=(w,h))\n    for i,col in enumerate(lst):\n        f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.barplot(y=df[target],x=df[col],color=color)\n        plt.xlabel(col,fontweight='bold')\n        sns.despine(top=1,bottom=1,left=1,right=1)\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)","0c705069":"numcol=df.columns.tolist()[:-1]\nkdeall(df,numcol,h=10)","b941e1f1":"def fillna(df,lst,target):\n    for col in lst:\n        missing_ind=df[df[col]==0].index.tolist()\n        fill={i:df[col][df[target]==df[target].iloc[i]].mean() for i in missing_ind}\n        for ind,x in fill.items(): df.loc[ind,col]=x","aef4e6bb":"lst=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\nfillna(df,lst,'Outcome')","54699282":"df.min()","8b258d89":"kde2(df,numcol,h=10,target='Outcome')","20def786":"boxall(df,numcol,h=10,cut=3,w=10,target='Outcome')","23061138":"countplot(df,[\"Pregnancies\"],color='#6a2202',annotsize=8,cut=1)","9a80817d":"f,ax=plt.subplots(figsize=(8,5))\nsns.pointplot(df.Pregnancies,df.Outcome)\ndespine()","99344c6a":"mask = np.triu(np.ones_like(df.corr()))\nf=plt.figure(figsize=(9,5))\nsns.heatmap(df.corr(),cmap='RdBu',vmax=1,vmin=-1,mask=mask,annot=True,fmt='.2f',annot_kws={\"size\":9},cbar=False)\nplt.yticks(fontsize=11)\nplt.xticks(fontsize=11,rotation=40)\nplt.show()","0bdb9a21":"def outlier(df,features,fence=1.5,n=1):\n    outliers=[]\n    for col in features:\n        q1=np.percentile(df[col],25)\n        q3=np.percentile(df[col],75)\n        iqr=q3-q1\n        outliers_list_col=df[(df[col]<q1-iqr*fence)|(df[col]>q3+iqr*fence)].index\n        outliers.extend(outliers_list_col)\n    outliers=Counter(outliers)\n    multiple_outliers=list(key for key,value in outliers.items() if value>=n)\n    return multiple_outliers","480160af":"mostimp=[\"Glucose\",\"BMI\",\"Age\"]\ndf.describe()[mostimp]","ec07293c":"kdeall(df,mostimp,h=3,cut=3)\nboxall(df,mostimp,h=3,cut=3)","8d94f3ce":"df.iloc[outlier(df,['BMI'])]","937642b5":"df.iloc[outlier(df,['Age'])]","179543b1":"kdeall(df,numcol,h=10)","14776e65":"for i in [\"Pregnancies\",\"Insulin\",\"Dpf\",\"Age\",\"SkinThickness\"]:\n    df[i]=normalize(df[i].values.reshape(-1,1),norm='max',axis=0)","4879e3bc":"df.BloodPressure=scale(df.BloodPressure.values).reshape(-1,1)","73665882":"for i in [\"Glucose\",\"BMI\"]: df[i]=minmax_scale(df[i].values.reshape(-1,1))","2f971f6c":"kdeall(df,numcol,h=10)","25fa593e":"X=df.drop(columns='Outcome')\ny=df.Outcome\nX.shape,y.shape","cd5f8bc7":"from sklearn.model_selection import StratifiedKFold\ndef check(clf,X,y=y,n_splits=10):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=0,shuffle=True)\n    k_tracc,k_teacc=[],[]\n    for (tr,te) in kf.split(X,y):\n        clf.fit(X.iloc[tr],y.iloc[tr])\n        k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n        k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n    print(f\"Train score: {np.mean(k_tracc)}\")\n    print(f\"Test score: {np.mean(k_teacc)}\")\n    return clf","ef7dfd5d":"def selectmoran(modellst,X,y=y,n_splits=10,random_state=0):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=random_state,shuffle=True)\n    namelst,imp,tr_acc,te_acc=[],[],[],[]\n    for clf in modellst:\n        namelst.append(type(clf).__name__)\n        k_tracc, k_teacc, k_f1, k_imp=[],[],[],[]\n        for (tr,te) in kf.split(X,y): # train, test index\n            clf.fit(X.iloc[tr],y.iloc[tr])\n            k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n            k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n            if hasattr(clf,\"feature_importances_\"): k_imp.append(clf.feature_importances_)\n        tr_acc.append(np.mean(k_tracc))\n        te_acc.append(np.mean(k_teacc))\n        if len(k_imp)==0: imp.append(False)\n        else: imp.append(np.mean(k_imp,axis=0))\n    score=pd.DataFrame({'Model':namelst,'Train_accuracy':tr_acc,'Test_accuracy':te_acc}).sort_values('Test_accuracy',ascending=False)\n    return score,imp","19e45317":"def plotscoring(score,title,w=7,h=5,alpha=.97,axvline=.8,yticksize=12):\n    f,ax=plt.subplots(figsize=(w,h))\n    print(f\"Mean accuracy for all models: {np.mean(score.Test_accuracy)}\\n\")\n    print(score)\n    sns.barplot(x=score.Test_accuracy,y=score.Model,alpha=alpha,color='#bc7201')\n    sns.barplot(x=-score.Train_accuracy,y=score.Model,alpha=alpha,color='#6a2202')\n    ax.set_xlim(-1,1)\n    plt.axvline(x=0,color='black')\n    plt.xlabel('Train\/ Test accuracy')\n    plt.ylabel('')\n    plt.title(f\"Accuracy score for {title}\",fontweight='bold')\n    plt.axvline(x=axvline,ls=':')\n    plt.yticks(fontsize=yticksize)\n    despine()","068fb863":"from sklearn.model_selection import GridSearchCV\ndef grid(clf,params,X,y=y,cv=5):\n    grid=GridSearchCV(clf,params,cv=cv)\n    grid.fit(X,y)\n    print(f\"Best score: {grid.best_score_}\")\n    print(f\"Best params: {grid.best_params_}\")\n    return grid.best_estimator_","b1161c92":"def plotting_importances(score,imp,X,w=8,h=3,rotation=90,xsize=10):\n    for (a,b) in zip(score.Model,imp):\n        if b is not False:\n            ind=np.argsort(b)[::-1]\n            cols=X.columns\n            plt.figure(figsize=(w,h))\n            plt.title(f\"Feature importances via {a}\",fontweight='bold',fontsize=13)\n            plt.bar(range(X.shape[1]),b[ind])\n            plt.xticks(range(X.shape[1]),cols[ind],rotation=rotation,fontsize=xsize)\n            plt.xlim([-1,X.shape[1]])\n            plt.tight_layout()","7b851ab6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier,BaggingRegressor,GradientBoostingClassifier,AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\n\nmodellst=[LogisticRegression(),\n          ExtraTreesClassifier(),RandomForestClassifier(),\n          BaggingRegressor(base_estimator=DecisionTreeClassifier(),n_estimators=50),\n          GradientBoostingClassifier(),\n         XGBClassifier(),\n         LGBMClassifier(),\n         CatBoostClassifier(),\n         SVC()]","a8ffc15c":"from sklearn.decomposition import PCA\ndef reduct_pca(X,w=8,h=3):\n    pca=PCA(n_components=X.shape[1],random_state=0)\n    Xpca=pca.fit_transform(X)\n    f=plt.figure(figsize=(w,h))\n    plt.bar(range(1,X.shape[1]+1),pca.explained_variance_ratio_,label='individual explained variance')\n    plt.step(range(1,X.shape[1]+1), np.cumsum(pca.explained_variance_ratio_),where='mid',label='cumulative explained variance')\n    plt.xticks(range(1,X.shape[1]+1))\n    plt.xlabel('# of principal components')\n    plt.ylabel('explained variance ratio',fontsize=13)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12,loc='best')\n    despine()\n    return Xpca","fa19f582":"Xpca=pd.DataFrame(reduct_pca(X)[:,:4])\nXpca.shape","f0ed0ffc":"import statsmodels.api as sm\ndef select_by_pvalue(target,df,fence=.05,w=7,h=5,textw=.5,texth=.2):\n    mod=sm.OLS(df[target],df.drop(target,axis=1))\n    fii=mod.fit()\n    sub=fii.summary2().tables[1][\"P>|t|\"].sort_values()\n    sub.plot(kind='barh',figsize=(w,h))\n    pvalue_set=sub[sub<=fence].index.tolist()\n    plt.axvline(x=fence,c='black')\n    plt.figtext(textw, texth, 'dropped',fontsize=15, fontweight='bold')\n    return pvalue_set","dcf5abe4":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif(x):\n    X=add_constant(x)\n    return pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])],index=X.columns).sort_values(ascending=False)","c6e4b4f0":"pval_set=select_by_pvalue(\"Outcome\",df)\npval_set","dc52744f":"vif(X)","5f3e1d86":"vif(X[pval_set])","652979cd":"scorefull,impfull=selectmoran(modellst,X)","45b95f37":"plotscoring(scorefull,'All features',axvline=.9)","53c7be00":"plotting_importances(scorefull,impfull,X,rotation=20)","ebb35617":"pval_set","5a2f0b73":"scorepval,imppval=selectmoran(modellst,X[pval_set])","7234ee89":"plotscoring(scorepval,'P-value subset',axvline=.9)","1db136ec":"plotting_importances(scorepval,imppval,X[pval_set],rotation=20)","f3eeca53":"scorepca,imppca=selectmoran(modellst,Xpca)","be2bcc37":"plotscoring(scorepca,'PCA subset',axvline=.9)","e89540bf":"eda_set=[\"Glucose\",\"BMI\",\"Age\",\"Pregnancies\",\"Insulin\"]\nscoreeda,impeda=selectmoran(modellst,X[eda_set])","cb9de089":"plotscoring(scoreeda,'EDA subset',axvline=.9)","1daa5176":"plotting_importances(scoreeda,impeda,X[eda_set],rotation=20)","93d6ca57":"def plot_meanscore(meanscore_set,axvline=.9):\n    print(meanscore_set)\n    f=plt.figure(figsize=(8,3))\n    meanscore_set.Test_accuracy[::-1].plot(kind='barh',color='#0f1a26',alpha=.95)\n    (-1*meanscore_set.Train_accuracy[::-1]).plot(kind='barh',alpha=.95)\n    plt.axvline(x=0,c='black')\n    plt.xlim([-1,1])\n    plt.axvline(x=axvline,ls=':')\n    despine()","acbb1a82":"meanscore=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [scorefull,scoreeda,scorepca,scorepval]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [scorefull,scoreeda,scorepca,scorepval]]},\n             index=['All features','EDA subset','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)\n\nmxscore=pd.DataFrame({'Test_accuracy':[np.max(i.Test_accuracy) for i in [scorefull,scoreeda,scorepca,scorepval]],\n             'Train_accuracy':[np.max(i.Train_accuracy) for i in [scorefull,scoreeda,scorepca,scorepval]]},\n             index=['All features','EDA subset','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)","119d0565":"plot_meanscore(meanscore)\ntitle('Mean accuracy score',fontsize=17)","7cc59c2a":"plot_meanscore(mxscore)\ntitle('Max accuracy score',fontsize=17)","bc69e366":"plotscoring(pd.concat([scorefull,scoreeda,scorepca,scorepval]).groupby('Model').aggregate(np.mean).reset_index().sort_values('Test_accuracy',ascending=False),'M',axvline=.9)\ntitle('Mean accuracy score by Classifier',fontsize=17)","ad0e5683":"plotscoring(scoreeda,'EDA subset',axvline=.9)","13f790e3":"tuningeda=[GradientBoostingClassifier(criterion='mse', learning_rate=0.05),\n          CatBoostClassifier(leaf_estimation_method='Gradient',learning_rate=.1),\n          AdaBoostClassifier(base_estimator=RandomForestClassifier(), learning_rate=0.5),\n          LGBMClassifier(boosting_type='dart', subsample=0.8),\n          XGBClassifier(booster='dart',max_depth=3,subsample=.8,n_estimators=277),\n          RandomForestClassifier(min_samples_leaf=2, n_estimators=200)]","ec039d1f":"score_tueda,_=selectmoran(tuningeda,X[eda_set])","f0a4c86d":"plotscoring(score_tueda,'Hypertuning - EDA subset',axvline=.9)","45582596":"plotscoring(scorefull,'All features',axvline=.9)","bc3bcaf4":"tuningfull=[LGBMClassifier(boosting_type='dart', subsample=0.8, n_estimators=150, max_depth=3),\n           CatBoostClassifier(leaf_estimation_method='Gradient',learning_rate=.3)]","270c4e01":"score_tuf,_=selectmoran(tuningfull,X)","cbfe9853":"plotscoring(score_tuf,'Hypertuning - All features',axvline=.9)","d1520135":"#### 5.1.2 Categorical variables\nWe have 2 categorical variables: Pregnancies and our target variable - Outcome. Earlier, we've check the target variable balance","abe4b0bd":"# Data Dictionary\n    - Pregnancies: Number of times pregnant\n    - Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n    - BloodPressure: Diastolic blood pressure (mm Hg)\n    - SkinThickness: Triceps skin fold thickness (mm)\n    - Insulin: 2-Hour serum insulin (mu U\/ml)\n    - BMI: Body mass index (weight in kg\/(height in m)^2)\n    - DiabetesPedigreeFunction: Diabetes pedigree function\n    - Age: Age (years)\n    - Outcome (our target variable): Class variable (0 or 1) 268 of 768 are 1, the others are 0","b631efe0":"### 5.2 Multivariate analysis","88d13418":"## 2. Data Completeness","10d3b743":"## 4. Filling in missing values","80b0faba":"## 7. Features scaling","b992637f":"## 6. Outliers detection ","cfb506a7":"- The most common number of pregnancies among women is between 0 and 2\n- Surprisingly, some women get pregnant more than 10 times","64d5c354":"## 5. Exploratory Data Analysis (EDA)","07f312e4":"- The probability of having diabetes for females in our samples is approximately 35%\n- The average times of pregnancies is 3, the maximum of pregnancies is 17 times\n- **Some features have a minimum value of 0, which is not possible. There may be some missing values in our dataset**","5b8302fe":"- Some features have a **minimum values of 0** (For example, Glucose, BloodPressure, SkinThickness, etc), which is **a sign of missing data**. We should replace these null values with the mean of the corresponding features by grouping our target variable","303d7592":"# Diabetes\nDiabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\n**The most common types of diabetes are type 1, type 2, and gestational diabetes.**\n- **Type 1 diabetes**: If you have type 1 diabetes, your body **does not make insulin**. Your immune system attacks and destroys the cells in your pancreas that make insulin. Type 1 diabetes is usually diagnosed in **children and young adults**, although it can appear at any age. People with type 1 diabetes need to take insulin every day to stay alive.\n- **Type 2 diabetes**: If you have type 2 diabetes, your body **does not make or use insulin well**. You can develop type 2 diabetes at any age, even during childhood. However, this type of diabetes occurs most often in **middle-aged and older** people. Type 2 is the **most common** type of diabetes.\n- **Gestational diabetes**: Gestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you\u2019ve had gestational diabetes, you have a **greater chance of developing type 2** diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes.\n- **Other types of diabetes**: Less common types include **monogenic diabetes**, which is an inherited form of diabetes, and cystic fibrosis-related diabetes External \n\n**What health problems can people with diabetes develop?**\n- Over time, high blood glucose leads to problems such as heart disease, stroke, kidney disease, eye problems, dental disease, nerve damage, foot problems, etc.","7ebd91e9":"- Our EDA subset has better performance than other subsets, with a max test accuracy of about 90%\n- PCA subset doesn't perform as well as other subsets","3a01b8df":"- We should identify outliers for our most important features, including: Glucose, BMI and Age. First, let's take a look at distribution of these features","61b4e708":"## 1. Importing data","df64b399":"**Conclusion**: After hypertuning parameters, we get a test accuracy of about 90.6% for both full features set and EDA subset","9a57c90f":"- No missing values, but some features have a minimum values of 0 (For example, Glucose, BloodPressure, SkinThickness, etc), which is a sign of missing data","b0775a8f":"- Age: 75% of the women in our samples are under the age of 41, the outliers belong to the elderly (approximately 70 years old)","337bd662":"# 10. Hypeturning parameters","38dc21ae":"- So we choose our EDA subset and all features set to perform hypertuning parameters","4c9c1c52":"- The features that have a clear impact on our target variable (in order) are: Glucose, BMI, Age and Pregnancies\n- The positive correlation between Age and Pregnancies shows that older women tend to get more pregnant\n- We also find that several features are significantly correlated (like BMI and SkinThickness, Insulin and Glucose, etc). In the following step, we should try dimensionality reduction\n- Age: As age increases, the number of pregnancies, plasma glucose concentration and blood pressure also increase\n- Glucose: As the glucose level increases, the insulin level and BMI also increase","d4ae033b":"## 9.5 Conclusion","a6213a92":"#### 5.1.1 Continuous variables\n- Note: Pregnancies is a categorical variable, but I'll put it here anyway","09c55fdf":"- In general, women having more pregnancies are more likely to develop diabetes","dac4d681":"## 3. Checking target imbalance","911d0447":"## 9.2 P-value subset","9eec29d2":"# Content\n1. Importing data\n2. Data completeness\n3. Checking target imbalance\n4. Filling in missing values\n5. Exploratory data analysis (EDA)\n    - 5.1 Univariate analysis\n    - 5.2 Multivariate analysis\n    - 5.3 Summary\n6. Outliers detection\n7. Features scaling\n8. Feature selection\n    - 8.1 Dimensionality reduction via PCA\n    - 8.2 Using P-value\n9. Modeling\n    - 9.1 All features\n    - 9.2 P-value subset\n    - 9.3 PCA subset\n    - 9.4 EDA subset\n    - 9.5 Conclusion\n10. Hypertuning parameters\n    - 10.1 EDA subset\n    - 10.2 All features","22c71e63":"![pima%20indians-3.jfif](attachment:pima%20indians-3.jfif)","fb91b7ee":"## 10.1 EDA subset","580eaa46":"- **Pregnancies**: the distribution of pregnancies has positive skewness. The average times of pregnancies is 2 times. The boxplot shows that there are some outliers to the right. We can see that females with **more than 5 times of pregnancies are more likely to develop diabetes** (probably because **gestational diabetes** can be seen in these women)\n- **Glucose**: the distribution of glucose if nearly bell-shaped. **There is a significant difference in glucose levels between people without diabetes and those with diabetes**. The average level of plasma glucose for healthy women and diabetic patient are approximately 110 and 140, respectively. We can see that people with **higher blood glucose levels** are more likely to develop diabetes\n- **BloodPressure**: also nearly normal distribution. There is **not significant difference** between the BloodPressure of a diabetic and a non-diabetic woman\n- **SkinThickness**: **not much differences**, but people with triceps skin folds thicker than 57mm have a higher probability of developing diabetes\n- **Insulin**: right-skewed, we can see that diabetics have **higher insulin levels** than non-diabetics\n- **BMI**: the distribution is nearly bell-shaped, healthy women have an average BMI of 30, while women with diabetes have a higher BMI of about 37\n- **DiabetesPedigreeFunction**: skew to the right and there are some outliers. The boxplot shows that a diabetic person usually has Dpf value close to 0.5 (50%)\n- **Age**: right-skewed. We can see that most of the women in our data set are between the ages of 20 and 40 (the oldest is 80 years old). Diabetes is more likely to occur in the **middle-aged and older age group**","7bcb7423":"### 5.1 Univariate analysis","f83f9746":"## 9.3 PCA subset","9b978895":"# 9. Modeling","3330ef51":"- For BMI: Outliers belong to women whose BMI is quite high compared to the mean BMI of 30","7aa7b2e1":"- Approximate 98% of the variance was explained by the top 4 principal components","4bdca10c":"- The **most important features** (in order) are: **Glucose, BMI, Age and Pregnancies**\n- Diabetes is more prominent in females with more pregnancies\n- Higher plasma glucose, higher blood pressure, and higher BMI found in diabetic patient\n- A person with diabetes usually has a Dpf value close to 0.5, that is, if one has another family member with diabetes, he has a 50 percent probability of developing diabetes\n- Age plays an important role in diabetes risk, middle-aged and older women are more likely to develop diabetes","42b80580":"## 8. Feature selection","92d3ea38":"## 10.2 All features","21021269":"# Object\nCan you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\nData source: https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\n\nReference:\n- https:\/\/www.kaggle.com\/ohseokkim\/diabetes-three-ensemble-models\/notebook\n- https:\/\/www.kaggle.com\/yogidsba\/diabetes-prediction-eda-model","e081757f":"- **All features are numeric**\n- Looks like we don't have missing values","594c6d97":"## 8.1 Dimensionality reduction via PCA","c980a398":"- The target variable distribution is **not really well balanced**, the rate of patients with diabetes is nearly twice as high as the rate of patients without diabetes.","36340e0c":"## 9.1 All features","d40eaad5":"## 8.2 Using P-value","3d80c0f7":"- Outliers appear in BMI and Age features","b119ff9c":"### 5.3 Summary","7fdd56f4":"## 9.4 EDA subset","4cc5d23e":"> *The Pima (or Akimel O'odham, also spelled Akimel O\u02bcotham, \"River People,\" formerly known as Pima) are a group of Native Americans living in an area consisting of what is now central and southern Arizona, as well as northwestern Mexico in the states of Sonora and Chihuahua. The majority population of the two current bands of the Akimel O'odham in the United States are based in two reservations: the Keli Akimel O\u02bcotham on the Gila River Indian Community (GRIC) and the On'k Akimel O'odham on the Salt River Pima-Maricopa Indian Community (SRPMIC).*"}}