{"cell_type":{"5c6e3ff8":"code","c57ed2f8":"code","76893781":"code","7527b674":"code","af4d65d9":"code","a0c6b211":"code","6ef24303":"code","462d7f4b":"code","5f67bb03":"code","0e7b85a4":"code","ac549cf7":"code","470382d4":"code","98ba78b6":"code","1b2fb1f4":"code","826c2337":"code","f0c7663c":"code","32e76b41":"code","c4a7dbaf":"code","3100bd41":"code","7695b92e":"code","16ceb525":"code","f3f6175c":"code","f89c2ece":"code","9556c99b":"code","630b38d9":"code","2839f96d":"code","20771ada":"code","91b61df4":"code","9578c820":"code","b5efed56":"code","dee67383":"code","f7ed93b4":"code","7fe25aa6":"markdown","c6cf0314":"markdown","c733dfc7":"markdown"},"source":{"5c6e3ff8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, GRU, Bidirectional, LSTM\nfrom keras.optimizers import SGD\nimport math\nfrom sklearn.metrics import mean_squared_error\nimport keras","c57ed2f8":"def make_gru_network():\n    regressorGRU = Sequential()\n    # First GRU layer with Dropout regularisation\n    \n    regressorGRU.add(GRU(units = 30, return_sequences = False, input_shape=(1,2), activation='tanh'))\n    regressorGRU.add(Dropout(0.3))\n    \n    # The output layer\n    regressorGRU.add(Dense(units=1))\n    return regressorGRU","76893781":"model = make_gru_network()","7527b674":"model.compile(optimizer='rmsprop', loss='mean_squared_error')","af4d65d9":"x_train = np.array([1,2,2,3,3,4,4,5])\ny_train = np.array([\n    3,\n    4,\n    5,\n    6\n])","a0c6b211":"x_train = x_train.reshape(4,1,2)\nx_train","6ef24303":"model.fit(x_train, y_train, epochs = 30, batch_size = 2)","462d7f4b":"test_set = np.array([[[1,2]]])","5f67bb03":"model.predict(test_set)","0e7b85a4":"# Load dataset from csv file\ndataset = pd.read_csv('..\/historical_stock_prices.csv', index_col='date', parse_dates=['date'])","ac549cf7":"# Separate train\/test\ntrainSet = dataset['2015':'2017'].sort_values(by=['ticker','date'])\ntestSet = dataset['2018':].sort_values(by=['ticker','date'])","470382d4":"trainSet.head()","98ba78b6":"# All symbols\nsymbols = trainSet.ticker.unique()","1b2fb1f4":"timesteps = 60\ndef make_samples(\n    data_1,\n    s):\n    \n    stop_append = {}\n    for i in prediction_intervals:\n        stop_append.update({i:False}) \n    l = len(data_1)\n    \n    for i in range(timesteps, l):\n        x_1 = data_1[i-timesteps: i, 0]\n        for j in stop_append:\n            if not stop_append[j]:\n                if i+j-1 < l:\n                    y = data_1[i+j-1,0]\n                    y = y.reshape(-1,1)\n                    x_1 = x_1.reshape(-1,1)\n                    \n                    sc = MinMaxScaler(feature_range=(0,1))\n                    sc.partial_fit(x_1)\n                    sc.partial_fit(y)\n                    \n                    train_set[s]['x_1'][j].append(sc.transform(x_1))\n                    train_set[s]['y'][j].append(sc.transform(y))\n                    \n                    if y == data_1[l-1,0]:\n                        stop_append[j] =True","826c2337":"# Define train_set\ntrain_set = {}\nprediction_intervals = [1,3,5,10]\nt = 1\nfor s in symbols:\n    train_set.update({s:{\n        'x_1':{},\n        'y':{},\n    }})\n    for i in prediction_intervals:\n        train_set[s]['x_1'][i] = []\n        train_set[s]['y'][i] = []\n            \n    data_1 = trainSet.loc[trainSet['ticker'] == s][['adj_close']].values\n        \n    make_samples(\n        data_1, \n        s)\n    if t == 2000:\n        break\n    t += 1","f0c7663c":"# Define earlystopping callback function\nes = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5, mode='min', restore_best_weights=True)","32e76b41":"def make_gru_network():\n    regressorGRU = Sequential()\n    # First GRU layer with Dropout regularisation\n    \n    regressorGRU.add(GRU(units = 30, return_sequences = False, input_shape=(1,60), activation='tanh'))\n    regressorGRU.add(Dropout(0.3))\n    \n    # The output layer\n    regressorGRU.add(Dense(units=1))\n    return regressorGRU","c4a7dbaf":"# Training model with one feature in train_set\nmodel = {}\nmodel_history = {}\nfor i in prediction_intervals:\n    model[i] = make_gru_network()\n    model[i].compile(optimizer='rmsprop', loss='mean_squared_error')\n    \n    n_epoch = 100\n    x_train = []\n    y_train = []\n    \n    for s in train_set:\n        for j in range (0, len(train_set[s]['x_1'][i])):\n            x = [\n                train_set[s]['x_1'][i][j],\n            ]\n            x_train.append(x)\n        for j in train_set[s]['y'][i]:\n            y_train.append(j)\n    X_train, Y_train = np.array(x_train), np.array(y_train)\n    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n    Y_train = Y_train.reshape(Y_train.shape[0])\n    \n    print(X_train.shape, Y_train.shape)\n    print('Fitting prediction interval {}  model'.format(i))\n    model_history[i] = model[i].fit(X_train, Y_train, epochs = n_epoch, batch_size = 6000, \n                                    validation_split = 0.3,callbacks=[es])","3100bd41":"for i in prediction_intervals:\n    plt.plot(model_history[i].history['val_loss'])\n    plt.plot(model_history[i].history['loss'])\n    plt.title('Model Loss of Interval {}'.format(i))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['val_loss', 'loss'], loc='upper right')\n    plt.show()","7695b92e":"# Calculate Mean squared_error\ndef return_mse(test,predicted):\n    mse = mean_squared_error(test, predicted)\n    return mse","16ceb525":"# Plotting graph\ndef plotting_graph(y_test, y_pred, ticker, des):\n    plt.plot(y_test, color='green', label='Actual adj_close value')\n    plt.plot(y_pred, color='red', label='Predicted adj_close value')\n    plt.title('Prediction of {} on {}'.format(ticker, des))\n    plt.xlabel('Time steps')\n    plt.ylabel('adj_close value')\n    plt.legend()\n    plt.show()","f3f6175c":"# Define test_set\ndef make_test_data(s):\n    scaler = {s:{}}\n    test_set = {s:{}}\n    test_set.update({s:{\n            'x_1':{},\n            'y':{},\n        }})\n    for i in prediction_intervals:\n            test_set[s]['x_1'][i] = []\n            test_set[s]['y'][i] = []\n            scaler[s][i] = []\n\n    data_1 = testSet.loc[testSet['ticker'] == s][['adj_close']].values\n    \n    stop_append = {}\n    for i in prediction_intervals:\n        stop_append.update({i:False}) \n    l = len(data_1)\n\n    for i in range(timesteps, l):\n        x_1 = data_1[i-timesteps: i, 0]\n        for j in stop_append:\n            if not stop_append[j]:\n                if i+j-1 < l:\n                    y = data_1[i+j-1,0]\n                    y = y.reshape(-1,1)\n                    x_1 = x_1.reshape(-1,1)\n                    \n                    sc = MinMaxScaler(feature_range=(0,1))\n                    sc.partial_fit(x_1)\n                    sc.partial_fit(y)\n                    scaler[s][j].append(sc)\n                    \n                    test_set[s]['x_1'][j].append(sc.transform(x_1))\n                    test_set[s]['y'][j].append(y)\n                    \n                    if y == data_1[l-1,0]:\n                        stop_append[j] =True\n                    \n    return test_set, scaler","f89c2ece":"def make_prediction(data, s, scaler):\n    mse_val = {}\n    y_true_val = {}\n    y_pred_val = {}\n    \n    for i in prediction_intervals:\n        x_test = []\n        for j in range (0, len(data[s]['x_1'][i])):\n            x = [\n                data[s]['x_1'][i][j],\n            ]\n            x_test.append(x)\n\n        X_test = np.array(x_test)\n        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n        \n        result = model[i].predict(X_test)\n        \n        y_pred = []\n        for j in range(0,len(result)):\n            y = result[j]\n            y = y.reshape(-1,1)\n            k = scaler[s][i][j].inverse_transform(y)\n            y_pred.append(k[0][0])\n                    \n        y_test = data[s]['y'][i]\n        y_true = np.array(y_test)\n        y_true = y_true.reshape(y_true.shape[0])\n    \n        y_true_val[i] = y_true\n        y_pred_val[i] = y_pred\n\n        mse_val[i] = return_mse(y_true, y_pred) \n    return y_true_val, y_pred_val, mse_val","9556c99b":"s = 'AAPL'\ndata, sc = make_test_data(s)\ny_true_val, y_pred_val, mse_val = make_prediction(data, s, sc)\n\nfor i in mse_val:\n    print('Test Set -> MSE of {} inveral {}: {}'.format(s, i, mse_val[i]))","630b38d9":"for i in prediction_intervals:\n    plotting_graph(y_true_val[i], y_pred_val[i], s, 'Test Set interval {}'.format(i))","2839f96d":"test_symbols = ['A', 'ACER', 'MSFT', 'ABC', 'AAPL']","20771ada":"all_mse = {}\nfor s in test_symbols:\n    all_mse[s] = {}\n    data, sc = make_test_data(s)\n    y_true_val, y_pred_val, mse_val = make_prediction(data, s, sc)\n    for i in prediction_intervals:\n        all_mse[s][i] = mse_val[i]\n        all_mse[s][i] = mse_val[i]","91b61df4":"all_mse_1 = []\nall_mse_3 = []\nall_mse_5 = []\nall_mse_10 = []\nfor s in all_mse:\n    all_mse_1.append(round(all_mse[s][1], 2))\n    all_mse_3.append(round(all_mse[s][3], 2))\n    all_mse_5.append(round(all_mse[s][5], 2))\n    all_mse_10.append(round(all_mse[s][10], 2))","9578c820":"# MSE of interval 1\nall_mse_1","b5efed56":"# MSE of interval 3\nall_mse_3","dee67383":"# MSE of interval 5\nall_mse_5","f7ed93b4":"# MSE of interval 10\nall_mse_10","7fe25aa6":"# MSE of 5 symbols","c6cf0314":"# Test our model","c733dfc7":"## Predicting test sample"}}