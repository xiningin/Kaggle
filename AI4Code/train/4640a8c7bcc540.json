{"cell_type":{"dc6a3e02":"code","72de735d":"code","0f8e80e1":"code","b18f8872":"code","2d00af91":"code","6a1637af":"code","01babfd3":"code","cd9dcfe5":"code","425d27f0":"code","2c1768b1":"code","8c4cb72e":"code","7461316c":"code","a500f9a0":"code","835de504":"code","b29f4c14":"code","68a851f9":"code","6cfdbe6a":"code","94ac01ab":"code","8152300b":"code","ac93e8c1":"markdown","15286563":"markdown","b4bae4fd":"markdown","852f10f7":"markdown","d4584290":"markdown","c1a61597":"markdown","caf7d928":"markdown","f3ffa785":"markdown","782bfe87":"markdown","2ca33dd5":"markdown","b90979f7":"markdown","fde15199":"markdown","605db8d2":"markdown","8a3bc48a":"markdown","e78e5082":"markdown","7829e4f9":"markdown","6be92755":"markdown","a1922224":"markdown"},"source":{"dc6a3e02":"!pip install transformers","72de735d":"import re\nimport string\nimport torch\nimport transformers\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom sklearn.manifold import TSNE\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n\ndf = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\ndf.head()","0f8e80e1":"df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], inplace=True, axis=1)\n\n#Adding new features\ndf[\"num_words\"] = df[\"v2\"].apply(lambda s: len(re.findall(r'\\w+', s))) # Count the number of words in the message\ndf[\"message_len\"] = df[\"v2\"].apply(len) # get the length of the text message\n\ndf[\"v1\"].replace({\"ham\": 0, \"spam\":1}, inplace=True)\n\ndf.rename({\"v1\": \"is_spam\", \"v2\": \"message\"},axis=1, inplace=True)","b18f8872":"def clean_sentence(s):\n    \"\"\"Given a sentence remove its punctuation and stop words\"\"\"\n    \n    stop_words = set(stopwords.words('english'))\n    s = s.translate(str.maketrans('','',string.punctuation)) # remove punctuation\n    tokens = word_tokenize(s)\n    cleaned_s = [w for w in tokens if w not in stop_words] # removing stop-words\n    return \" \".join(cleaned_s[:10]) # using the first 10 tokens only\n\ndf[\"message\"] = df[\"message\"].apply(clean_sentence)","2d00af91":"# Loading pretrained model\/tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")","6a1637af":"tokenized = df[\"message\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\ntokenized","01babfd3":"max_len = tokenized.apply(len).max() # get the length of the longest tokenized sentence\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values]) # padd the rest of the sentence with zeros if the sentence is smaller than the longest sentence\npadded","cd9dcfe5":"attention_mask = np.where(padded != 0, 1, 0)\nattention_mask","425d27f0":"input_ids = torch.tensor(padded)  # create a torch tensor for the padded sentences\nattention_mask = torch.tensor(attention_mask) # create a torch tensor for the attention matrix\n\nwith torch.no_grad():\n    encoder_hidden_state = model(input_ids, attention_mask=attention_mask)","2c1768b1":"X = encoder_hidden_state[0][:,0,:].numpy()\nX = np.hstack((X, df[[\"num_words\", \"message_len\"]].to_numpy().reshape(-1, 2))) # addind the the engineered features from the beginning\ny = df[\"is_spam\"]","8c4cb72e":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)","7461316c":"X_embedded = TSNE(n_components=2, random_state=42).fit_transform(X_train)\nX_embedded.shape","a500f9a0":"# creating the dataframe for plotting\ndef creat_plotting_data(data, labels=y_train):\n    \"\"\"Creates a dataframe from the given data, used for plotting\"\"\"\n    \n    df = pd.DataFrame(data)\n    df[\"is_spam\"] = labels.to_numpy()\n    df.rename({0:\"v1\", 1:\"v2\", 768:\"num_words\", 769: \"message_len\"}, axis=1, inplace=True)\n    return df\n\n# creating the dataframes for plotting\nplotting_data = creat_plotting_data(X_train)\nplotting_data_embedded = creat_plotting_data(X_embedded)","835de504":"plt.figure(figsize=(16, 10))\nax = sns.scatterplot(x=\"v1\", y=\"v2\", hue=\"is_spam\", data=plotting_data_embedded)\nax.set(title = \"Spam messages are generally closer together due to the BERT embeddings\")\nplt.show()","b29f4c14":"f,ax = plt.subplots(figsize=(16,10))\nsns.kdeplot(plotting_data.loc[plotting_data.is_spam == 1, \"num_words\"], shade=True, label=\"Spam\")\nsns.kdeplot(plotting_data.loc[plotting_data.is_spam == 0, \"num_words\"], shade=True, label=\"Ham\", clip=(0, 35)) # removing observations with message length above 35 because there is an outlier\nax.set(xlabel = \"Number of words\", ylabel = \"Density\",title = \"Spam messages have more words than ham messages\")\nplt.show()","68a851f9":"f,ax = plt.subplots(figsize=(16,10))\nsns.kdeplot(plotting_data.loc[plotting_data.is_spam == 1, \"message_len\"], shade=True, label=\"Spam\")\nsns.kdeplot(plotting_data.loc[plotting_data.is_spam == 0, \"message_len\"], shade=True, label=\"Ham\", clip=(0, 250)) # removing observations with message length above 250 because there is an outlier\nax.set(xlabel = \"Message length\", ylabel = \"Density\",title = \"Spam messages are longer than ham messages, concentrated on 150 characters\")\nplt.show()","6cfdbe6a":"rf_classifier = RandomForestClassifier(n_estimators=1500, class_weight=\"balanced\", n_jobs=-1, random_state=42) # Create a baseline random forest (no cross-validation, no hyperparameter tuning)\nrf_classifier.fit(X_train, y_train)\npreds = rf_classifier.predict(X_test)","94ac01ab":"fig = plt.figure(figsize=(10,4))\nheatmap = sns.heatmap(data = pd.DataFrame(confusion_matrix(y_test, preds)), annot = True, fmt = \"d\", cmap=sns.color_palette(\"Reds\", 50))\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.ylabel('Ground Truth')\nplt.xlabel('Prediction')\nplt.show()","8152300b":"print(f\"\"\"Accuray: {round(accuracy_score(y_test, preds), 5) * 100}%\nROC-AUC: {round(roc_auc_score(y_test, preds), 5) * 100}%\"\"\")\nprint(classification_report(y_test, preds))","ac93e8c1":"Each point in this plot represents a sentence that is colorized according to it being spam or not. This shows us that the embeddings that BERT created have succesfully encoded our data in such a way that spam massages are all closer together.\n\nSome outliers exist but it may be due to the embedding being based on only 10 tokens or just so really good spam massages.\n\n## Do spam messages have more words than ham messages?","15286563":"This plots shows two things:\n\n- Spam messages are, generally, **longer** than ham messages (which is to be expected since they have more words)\n- There seems to be a common value of around 150 characters for spam messages","b4bae4fd":"This plot shows us that yes Spam messages, generally, have **more** words than ham messages\n\n## Is there any pattern in the length of the messages?","852f10f7":"The first element in `encoder_hidden_state` is the one we want. This matrix is of shape (number_of_sentences, max_sentence_length, 768). From this matrix we are only interested in the first column of every row, which is the column with the \"[CLS]\" embedding, the embedding used for classification","d4584290":"Great! Our extremely simple random forest with no cross validation and no hyper-parameter tunning was able to achieve a 97% precision on the positive class while retrieving 76% of all positive observations. Given that we only used the 10 first non-stop words to create our embedding I'd say this is a job well done for BERT, the spam detector!\n\n### Future Work\n\nThis kernel serves more as a tutorial to get BERT going for classification and what can you do with it. You can try embedding a larger number of words, maybe add n-grams to the messages as well and create an embedding for them. You can also tune the hyper-parameters of the random forest or use any other classification model you like!\n\nHope this helps :)","c1a61597":"Now that we have our features and labels defined we can beging the modeling part of the problem","caf7d928":"We can see the confusion matrix for the test set","f3ffa785":"# Data cleaning and feature engineering\n\nWe can discard the unnamed columns and replace \"ham\" with 0 and \"spam\" with 1\n\nTwo new features are engineered:\n- `num_words` -> Indicates the number of words in the message\n- `message_len` -> The length of the message","782bfe87":"# EDA\n\nFirst lets look at the manifold distribution of the data to see how did our embeddings fair in encoding the messages. We'll use T-SNE to reduce the dimensions in the data down to just 2 so we can plot them","2ca33dd5":"When cleaning the sentences punctuation is removed as well as english stopwords. For memory size reasons (as well as just for the challenge of it (:) only the first 10 non-stopword tokens are used to detect spam ","b90979f7":"## How are messages distributed in manifold space?","fde15199":"Finally we can send our input data to BERT to get the classification embeddings for the messages","605db8d2":"Each row in tokenized is an array of numbers that represents a sentence in our dataset. Note that every sentence starts with 101, that's the ID for the \"[CLS]\" token\n\n### Padding\nSince some sentences are longer than others will need to padd them so they all have the same size","8a3bc48a":"### Tokenizer\n\nSince machine learning\/deep learning models only work with numbers we need to convert every word to a number. We'll use DistilBERTTokenizer for this. This will separete the words in the sentence, add special tokens (\"[CLS]\" and \"[SEP]\") and replace every word for an ID","e78e5082":"## Loading DistilBERT\n\nWe'll load the uncased base model (the smaller model). This model is easier to train in a \"normal\" computer and since it is uncased we do not have to worry about case sensitivity!","7829e4f9":"# BERT, the spam detector that uses just 10 words \n## Is 10 words enough to detect spam? (spoiler alert: it is)\n\nThe idea of this kernel is to use BERT (actually HuggingFace's [DistilBERT](https:\/\/github.com\/huggingface\/transformers)) to create good word embeddings for our data so we can use them in a downstream classifier. Since BERT creates very large embeddings that will not fit into memory the idea is to use only the first 10 non-stop words from every text to see if it is enough to detect spam.\n\n[This excellent tutorial](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) was followed to get BERT running\n\nInstalling HuggingFaces transformers package","6be92755":"# Baseline Random Forest\n\nThe goal of this kernel was mainly to study if 10 non-stop words would be enough to classify spam messages, not really get the greatest performance. That being said, a basic random forest is created to see how well our embeddings help out in the classification task","a1922224":"But now since we've added these zeros we'll have to create another matrix in which to tell BERT what should it pay attention and what should it ignore. Everything that is a zero we tell BERT to ignore it"}}