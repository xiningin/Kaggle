{"cell_type":{"d81f81c3":"code","ee3eee2e":"code","06b36da6":"code","b5c67902":"code","0e4aa505":"code","06f9b8db":"code","3a25449f":"code","6e72d39e":"code","a7947d50":"code","6ce7934d":"code","783e8f7e":"code","6869dedd":"code","48b58d1f":"code","c3bcca8e":"code","c3a36336":"markdown","7cd0719f":"markdown","1fc2f0cb":"markdown","3a9284dd":"markdown","ad93c1d1":"markdown","938a9f78":"markdown","225b08bc":"markdown","383bb6d7":"markdown","63f60c25":"markdown","9beb5472":"markdown","0c933fb3":"markdown","e442a781":"markdown"},"source":{"d81f81c3":"## for data\nimport pandas as pd\nimport numpy as np\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## for statistical tests\nimport scipy\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n# Scale data \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC,LinearSVC\nfrom xgboost import XGBClassifier\n# NLP\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer","ee3eee2e":"train_df = pd.read_csv('..\/input\/titanic-yw\/train.csv')\ntest_df  = pd.read_csv('..\/input\/titanic-yw\/test.csv')\nall_data = pd.concat ([train_df,test_df])\ntrain_df.head(50)","06b36da6":"!pip install git+https:\/\/github.com\/shakedzy\/dython.git","b5c67902":"train_df['Title'] = train_df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ntest_df['Title'] = test_df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]","0e4aa505":"train_df['Title'].replace(['Mme', 'Ms', 'Lady','Mrs', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ntest_df['Title'].replace(['Mme', 'Ms', 'Lady','Mrs',  'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ntrain_df['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer','Rev','Dr'], 'Mr', inplace=True)\ntest_df['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer','Rev','Dr'], 'Mr', inplace=True)\ntrain_df['Title'].replace(['Master'], 'Child', inplace=True)\ntest_df['Title'].replace(['Master'], 'Child', inplace=True)","06f9b8db":"train_df['combined'] = train_df['Title'].astype(str)+ ' '+train_df['Ticket'].astype(str)\ntest_df['combined'] =  test_df['Title'].astype(str)+ ' '+test_df['Ticket'].astype(str)","3a25449f":"X_train = train_df['combined']\ny= train_df['Survived']\nX_test = test_df['combined']","6e72d39e":"## This is how our feature looks like \nX_train\n##\n0               Mr A\/5 21171\n1              Miss PC 17599\n2      Miss STON\/O2. 3101282\n3                Miss 113803\n4                  Mr 373450\n               ...          \n886                Mr 211536\n887              Miss 112053\n888          Miss W.\/C. 6607\n889                Mr 111369\n890                Mr 370376\nName: combined, Length: 891, dtype: object ##","a7947d50":"vectorizer = TfidfVectorizer(ngram_range = (1,1), sublinear_tf= True, use_idf=True)\nX_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)","6ce7934d":"print(vectorizer.vocabulary_)","783e8f7e":"## Classifiers \n\nnb = MultinomialNB()\nlr = LogisticRegression()\ndt = tree.DecisionTreeClassifier(random_state = 1)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state=0)\nsvm = LinearSVC(class_weight = 'balanced')\nsvm2 = SVC(kernel='rbf')\nxgb = XGBClassifier(random_state =1)\n\ncv1 = cross_val_score(nb,X_tfidf,y,cv=10)\ncv2 = cross_val_score(lr,X_tfidf,y,cv=10)\ncv3 = cross_val_score(dt,X_tfidf,y,cv=10)\ncv4 = cross_val_score(knn,X_tfidf,y,cv=10)\ncv5 = cross_val_score(rf,X_tfidf,y,cv=10)\ncv6 = cross_val_score(svm,X_tfidf,y,cv=10)\ncv7 = cross_val_score(svm2,X_tfidf,y,cv=10)\ncv8 = cross_val_score(xgb,X_tfidf,y,cv=10)\n\nprint('\\n', cv1)\nprint('MultiNB',cv1.mean())\n\nprint('\\n', cv2)\nprint('LR',cv2.mean())\n\nprint('\\n', cv3)\nprint('Tree',cv3.mean())\n\nprint('\\n', cv4)\nprint('KNN',cv4.mean())\n\nprint('\\n', cv5)\nprint('RF',cv5.mean())\n\nprint('\\n', cv6)\nprint('SVM',cv6.mean())\n\nprint('\\n', cv7)\nprint('SVM-RBF',cv7.mean())\n\nprint('\\n', cv8)\nprint('XGB',cv8.mean())","6869dedd":" [0.86666667 0.8988764  0.79775281 0.87640449 0.86516854 0.82022472\n 0.83146067 0.82022472 0.86516854 0.86516854]\nSVM 0.8507116104868914","48b58d1f":"svm.fit(X_tfidf,y)\ny_pred =svm.predict(X_test_tfidf)\nbase_submission = pd.DataFrame(data={'PassengerId': test_df.PassengerId, 'Survived': y_pred})\nbase_submission.to_csv('base_submission.csv', index=False)","c3bcca8e":"y_pred =clf.predict(X_test_tfidf)\nbase_submission = pd.DataFrame(data={'PassengerId': test_df.PassengerId, 'Survived': y_pred})\nbase_submission.to_csv('base_submission.csv', index=False)","c3a36336":"Came across this amazing Library here (Dython) https:\/\/github.com\/shakedzy\/dython and thought to try it on the Categorical features.","7cd0719f":"**Using TFIDF to vectorize the Text.\nTerm Frequency \u2014 Inverse Document Frequency (TFIDF) is a technique for text vectorization based on the Bag of words (BoW) model. It performs better than the BoW model as it considers the importance of the word in a document.**","1fc2f0cb":"The large gap between CV scores and the LeaderBoard scores are explained here -> https:\/\/www.kaggle.com\/carlmcbrideellis\/overfitting-and-underfitting-the-titanic\nThere is certainly overfitting happening with all models, and more apparent in complex(i.e., advanced) models like XGB and this is because the dataset is small! no way to escape this...\n\nOverall, I'm happy with my small little code and good accuracy & I believe more can be done to improve the LB score either by adding more text features (like ticket length, lastName, ..) or by model tuning!","3a9284dd":"**SVM was the Best! with 0.85 training accuracy.**","ad93c1d1":"![image.png](attachment:9ac8d43d-36de-4060-92af-1b2906fa581b.png)","938a9f78":"**As can be seen, Ticket and Cabin have high association with our target (Survived) so we will not drop them**.\n**I ended up not using Cabin in this notebook ...**","225b08bc":"**Submitting to Kaggle...**","383bb6d7":"**Extracting the *Titles* from the *Name* feature.****","63f60c25":"**Combining Titles and Ticket (as is) as one single feature!**","9beb5472":"![image.png](attachment:ac379600-cbb0-4c36-b8f5-f3ad541e3794.png)","0c933fb3":"This was an NLP project that I recently finished and I thought to try it on the Titanic Dataset and the result was impressive!!\nNot impressive in terms of Leader Board accuracy, but in terms of simplicity and minimal feature engineering done.\n\nNo EDA is done here since there are lots of amazing visualizations out there","e442a781":"**We start by importing all necessary packages**"}}