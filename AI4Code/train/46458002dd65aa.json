{"cell_type":{"78943bba":"code","e8c89416":"code","316bddc8":"code","9f53323b":"code","40aee769":"code","7d3ee7f9":"code","35588790":"code","0336c0db":"code","a0778557":"code","8ed6fb6e":"code","a9ce3b02":"code","9bbc0709":"code","51f22805":"code","206b9264":"code","cbd0f147":"code","7ce50ed8":"code","cf656894":"markdown","b6802f34":"markdown","80685fba":"markdown","48473f7e":"markdown"},"source":{"78943bba":"# imports\n\nfrom gensim.models import KeyedVectors\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing import text_dataset_from_directory\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Embedding, Dense, Input, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom gensim.models.callbacks import CallbackAny2Vec\nfrom gensim.models import Word2Vec, KeyedVectors\n\nimport utils","e8c89416":"# Importing & preprocessing the dataset\n\ntrain_ds = text_dataset_from_directory('..\/input\/imbd-reviews-dataset\/aclImdb\/train')\ntest_ds = text_dataset_from_directory('..\/input\/imbd-reviews-dataset\/aclImdb\/test')\n\ndfTrain = pd.DataFrame(train_ds.unbatch().as_numpy_iterator(), columns=['text', 'label'])\ndfTest = pd.DataFrame(test_ds.unbatch().as_numpy_iterator(), columns=['text', 'label'])\n_, xts = train_test_split(dfTest, stratify=dfTest['label'], test_size=0.25)\n\ndfTrain['text'] = dfTrain['text'].map(lambda x: x.decode())\nxts['text'] = xts['text'].map(lambda x: x.decode())","316bddc8":"pd.options.display.max_colwidth = 200\ndfTrain=dfTrain[dfTrain.label!=2].reset_index(drop=True)\ndfTrain.sample(n=5)","9f53323b":"print(dfTrain.loc[0, 'text'])","40aee769":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(dfTrain['text'].tolist())\ntrain_sequences = tokenizer.texts_to_sequences(dfTrain['text'].tolist())\ntest_sequences = tokenizer.texts_to_sequences(xts['text'].tolist())\n\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","7d3ee7f9":"print(train_sequences[0])","35588790":"print([tokenizer.index_word[k] for k in train_sequences[0]])","0336c0db":"MAX_SEQUENCE_LENGTH = max([max(map(len, train_sequences)), max(map(len, test_sequences))])\nMAX_SEQUENCE_LENGTH","a0778557":"train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)","8ed6fb6e":"#print([tokenizer.index_word.get(k, '<PAD>') for k in train_data[0]])","a9ce3b02":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","9bbc0709":"class MetricCallback(CallbackAny2Vec):\n    \"\"\"\n    Callback to print loss after each epoch\n    \"\"\"\n    def __init__(self, every=10):\n        self.myloss = []\n        self.epoch = 0\n        self.every = every\n\n    def on_epoch_end(self, model):\n        loss = model.get_latest_training_loss()\n        if self.epoch == 0:\n            self.myloss.append(loss)\n        else:\n            self.myloss.append(loss - self.loss_previous_step)\n        if self.epoch % self.every == 0:\n            print(f'Loss after epoch {self.epoch}: {self.myloss[-1]}')  # NOQA: T001\n        self.epoch += 1\n        self.loss_previous_step = loss\n\n\ndef plot_arrows(starts, ends, wv, estimator=PCA, **kwargs):\n    if len(starts) != len(ends):\n        raise ValueError('starts and ends must be the same length.')\n    fig, ax = plt.subplots(figsize=kwargs.pop('figsize', (8, 8)))\n    X = wv[starts + ends]  # NOQA: N806\n    x_red = estimator(n_components=2).fit_transform(X)\n    plt.scatter(*x_red.T)\n    for i, word in enumerate(starts + ends):\n        plt.annotate(word, x_red[i])\n    xstart = x_red[:len(starts)]\n    xend = x_red[len(starts):]\n    for i, (start, end) in enumerate(zip(starts, ends)):\n        x1, y1 = xstart[i]\n        x2, y2 = xend[i]\n        plt.arrow(x1, y1, x2 - x1, y2 - y1)\n\n\ndef plot_vectors(words, model, estimator=TSNE, **kwargs):\n    names = []\n    vectors = []\n    for word in words:\n        if word in model.wv:\n            names.append(word)\n            vectors.append(model.wv[word])\n\n    X = np.r_[vectors]  # NOQA: N806\n    x_red = estimator(n_components=2).fit_transform(X)\n    fig, ax = plt.subplots(figsize=kwargs.pop('figsize', (16, 16)))  # NOQA: E912\n    ax.scatter(*x_red.T)\n\n    for i, word in enumerate(names):\n        plt.annotate(word, x_red[i])\n\n\ndef make_embedding_layer(model, tokenizer, MAX_SEQUENCE_LENGTH):  # NOQA: N803\n    word_index = tokenizer.word_index\n    if isinstance(model, Word2Vec):\n        wv = model.wv\n    elif isinstance(model, KeyedVectors):\n        wv = model\n    embedding_matrix = np.zeros((len(word_index) + 1, wv.vector_size))\n    for word, i in word_index.items():\n        try:\n            vector = wv.get_vector(word, False)\n            embedding_matrix[i] = vector\n        except KeyError:\n            continue\n    el = Embedding(\n        len(word_index) + 1, wv.vector_size, weights=[embedding_matrix],\n        input_length=MAX_SEQUENCE_LENGTH, trainable=False\n    )\n    return el","51f22805":"glove_wiki = KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False, no_header=True)","206b9264":"embedding_layer = make_embedding_layer(glove_wiki, tokenizer, MAX_SEQUENCE_LENGTH)\n\nglove_model = Sequential([\n    Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32'),\n    embedding_layer,\n    GlobalAveragePooling1D(),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nglove_model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])","cbd0f147":"glove_history = glove_model.fit(\n    train_data, dfTrain['label'].values,\n    validation_data=(test_data, xts['label'].values),\n    batch_size=32, epochs=30\n)","7ce50ed8":"#Plotting the results\n\nplt.figure(figsize=[15,5])\n\nplt.subplot(1,2,1)\nplt.plot(glove_history.history['accuracy'], label='All Wiki')\nplt.plot(glove_history.history['val_accuracy'], label='All Wiki')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(glove_history.history['loss'], label='All Wiki')\nplt.plot(glove_history.history['val_loss'], label='All Wiki')\nplt.legend()\nplt.show()","cf656894":"# Train a classifier with Word Embeddings","b6802f34":"## Tokenize the text","80685fba":"## Exploring the data","48473f7e":"# Classify movie reviews from IMDB into positive or negative sentiment."}}