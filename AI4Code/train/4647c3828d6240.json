{"cell_type":{"278d8052":"code","1e6bcd47":"code","27e7feb1":"code","a8e52ca7":"code","d7b94c17":"code","fdd9b036":"code","45e6e75d":"code","b3dbecaf":"code","a04b4d7b":"code","c22c1e78":"code","43503420":"code","b79facf8":"code","09f49f0d":"code","8cfa4b38":"code","2883c57d":"code","5dd437dc":"code","02e8e5a6":"code","b55113b0":"code","dcfdc784":"code","723393c9":"code","4fe831f7":"code","cf66fe52":"code","e4bb7484":"code","9c96cbf8":"code","8bba9d75":"code","014c0aec":"code","b849818d":"code","da764d63":"code","f2f0ec5d":"code","f59fcc1e":"code","0e8d93db":"code","1660f08a":"code","3d3d73df":"code","a159fb5a":"code","6d36237a":"code","50a0e517":"code","dff78a0e":"code","67c6e02a":"markdown","81f1a966":"markdown"},"source":{"278d8052":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nsns.set(style='white', context='notebook', palette='deep')","1e6bcd47":"dataset = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndataset.head()","27e7feb1":"col = dataset.columns","a8e52ca7":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(dataset,2,[col])","d7b94c17":"dataset.loc[Outliers_to_drop]","fdd9b036":"dataset.isnull().sum()","45e6e75d":"plt.figure(figsize= (15,10))\nsns.heatmap(dataset.corr(), annot = True, fmt = '.2', cmap = 'coolwarm')","b3dbecaf":"dataset.head()","a04b4d7b":"g = sns.distplot(dataset[\"ejection_fraction\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"ejection_fraction\"].skew()))\ng = g.legend(loc=\"best\")","c22c1e78":"dataset[\"ejection_fraction\"] = dataset[\"ejection_fraction\"].map(lambda i: np.log(i) if i > 0 else 0)","43503420":"g = sns.distplot(dataset[\"ejection_fraction\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"ejection_fraction\"].skew()))\ng = g.legend(loc=\"best\")","b79facf8":"g = sns.distplot(dataset[\"creatinine_phosphokinase\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"creatinine_phosphokinase\"].skew()))\ng = g.legend(loc=\"best\")","09f49f0d":"dataset[\"creatinine_phosphokinase\"] = dataset[\"creatinine_phosphokinase\"].map(lambda i: np.log(i) if i > 0 else 0)\ng = sns.distplot(dataset[\"creatinine_phosphokinase\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"creatinine_phosphokinase\"].skew()))\ng = g.legend(loc=\"best\")","8cfa4b38":"g = sns.distplot(dataset[\"platelets\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"platelets\"].skew()))\ng = g.legend(loc=\"best\")","2883c57d":"g = sns.distplot(dataset[\"serum_creatinine\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"serum_creatinine\"].skew()))\ng = g.legend(loc=\"best\")","5dd437dc":"dataset[\"serum_creatinine\"] = dataset[\"serum_creatinine\"].map(lambda i: np.log(i) if i > 0 else 0)\ng = sns.distplot(dataset[\"serum_creatinine\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"serum_creatinine\"].skew()))\ng = g.legend(loc=\"best\")","02e8e5a6":"g = sns.distplot(dataset[\"serum_sodium\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"serum_sodium\"].skew()))\ng = g.legend(loc=\"best\")","b55113b0":"g = sns.distplot(dataset[\"time\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"time\"].skew()))\ng = g.legend(loc=\"best\")","dcfdc784":"x = dataset.drop('DEATH_EVENT', axis = 1)\ny = dataset['DEATH_EVENT']","723393c9":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)","4fe831f7":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nsc.fit(x_train)\nx_train = sc.transform(x_train)\nx_test = sc.transform(x_test)","cf66fe52":"kfold = StratifiedKFold(n_splits=10)","e4bb7484":"from xgboost import XGBClassifier","9c96cbf8":"random_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(CatBoostClassifier(iterations = 2000, depth = 3))\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(XGBClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, x_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"CatBoostClassifier\",\"LogisticRegression\",\"XGBClassifier\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","8bba9d75":"DTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(x_train,y_train)\n\nada_best = gsadaDTC.best_estimator_","014c0aec":"gsadaDTC.best_score_","b849818d":"ExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsExtC.fit(x_train,y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","da764d63":"RFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsRFC.fit(x_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","f2f0ec5d":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300, 400, 500],\n              'learning_rate': [0.1, 0.05, 0.01, 0.001],\n              'max_depth': [3, 4, 5, 8],\n              'min_samples_leaf': [100,150, 200],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsGBC.fit(x_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","f59fcc1e":"SVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsSVMC.fit(x_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","0e8d93db":"cat = CatBoostClassifier()\ncat_param_grid = {'iterations': [1000, 2000, 3000],\n                 'depth': [3, 5, 7],\n                 'learning_rate': [0.1, 0.01, 0.05, 0.001]}\n\ngscat = GridSearchCV(cat,param_grid = cat_param_grid, cv= 5, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngscat.fit(x_train, y_train)\ncat_best = gscat.best_estimator_\n                  \ngscat.best_score_","1660f08a":"def plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gscat.best_estimator_,\"CatBoost learning curves\",x_train,y_train,cv=kfold)","3d3d73df":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best), ('cat', cat_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(x_train, y_train)","a159fb5a":"test = pd.Series(votingC.predict(x_test), name=\"DEATH_EVENT\")","6d36237a":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\naccuracy_score(y_test, test)","50a0e517":"cm = confusion_matrix(y_test, test)\nplt.figure(figsize=(5,5))\nsns.heatmap(cm,annot = True, cmap=plt.cm.Blues)\nplt.title(\"Random Forest Model - Confusion Matrix\")\nplt.xticks(range(2), [\"Heart Not Failed\",\"Heart Fail\"], fontsize=16)\nplt.yticks(range(2), [\"Heart Not Failed\",\"Heart Fail\"], fontsize=16)\nplt.show()","dff78a0e":"test.to_csv(\"voting.csv\",index=False)","67c6e02a":"Checking for null vaalues","81f1a966":"Checking outliers"}}