{"cell_type":{"a38e0d5c":"code","87aaca9d":"code","42b1adb9":"code","b8f2c53d":"code","079306bd":"code","a616669f":"code","a1f939e4":"code","18d62971":"code","fa719660":"code","421b761d":"code","29729051":"code","fd29f8c3":"code","eaba8df4":"code","243bce74":"code","7e2dc290":"code","d46d4c89":"code","95c3be45":"code","608c6df5":"code","ea03d18c":"code","2fd88d76":"code","f941e759":"code","5d374ece":"code","48e0c9d5":"code","64719a3c":"code","c99e9b36":"code","bb9900b9":"code","33359506":"code","3fa2a831":"code","4337ab5f":"markdown","455d413c":"markdown","2d31e615":"markdown","7fa97a2f":"markdown","8badb228":"markdown","8d525e93":"markdown","08e68e41":"markdown","4504254d":"markdown","cb736b57":"markdown"},"source":{"a38e0d5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","87aaca9d":"data_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","42b1adb9":"data_train.head()","b8f2c53d":"data_train.info()","079306bd":"# using seaborn to beutify our plots\nimport seaborn as sns\nsns.set()","a616669f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorr = data_train[['Survived', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']].corr()\nplt.figure(figsize = (12,6))\nsns.heatmap(corr, cmap=\"Blues\", annot=True, linewidths=2)\nplt.show()","a1f939e4":"fig, axes = plt.subplots(nrows=1, ncols=2)\ndata_train['Pclass'].value_counts().plot(kind='bar', ax=axes[0], figsize=(15,10), xlabel='Pclass')\ndata_train['Pclass'].value_counts().plot(kind='pie', ax=axes[1], figsize=(15,5), xlabel='Pclass')\nplt.show()","18d62971":"fig, axes = plt.subplots(nrows=1, ncols=2)\ndata_train[['Pclass', 'Survived']].value_counts().plot(kind='bar', ax=axes[0], figsize=(15,10))\ndata_train[['Pclass', 'Survived']].value_counts().plot(kind='pie', ax=axes[1], figsize=(15,5), title=\"(Pclass, Survived)\", ylabel=\"\")\nplt.show()","fa719660":"fig, axes = plt.subplots(nrows=1, ncols=2)\ndata_train[['Pclass', 'Embarked']].value_counts().plot(kind='bar', ax=axes[0], figsize=(15,10))\ndata_train[['Sex', 'Survived']].value_counts().plot(kind='pie', ax=axes[1], ylabel='(Sex, Survived)', figsize=(15,5))\n\nplt.show()","421b761d":"data_train['Cabin'].unique()","29729051":"data_train['Family'] = data_train['Name'].apply(lambda x: x.split(',')[0].strip())","fd29f8c3":"data_train['Family'].unique().shape","eaba8df4":"from math import isnan\ndef encode_categorical(df):\n    choosen_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_S', 'Embarked_C', 'cabinCount', 'Mr', 'Mrs', 'Miss']\n    df['Embarked_S'] = (df['Embarked'] == 'S').astype(int)\n    df['Embarked_C'] = (df['Embarked'] == 'C').astype(int)\n    df['Sex'] = (df['Sex'] == 'female').astype(int)\n    \n    df['cabinCount'] = df['Cabin'].apply(lambda x: 0 if type(x)==float and isnan(x) else len(x.split(' ')))\n    df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n    df['Mr'] = (df['Title'] == 'Mr').astype(int)\n    df['Mrs'] = np.logical_or(df['Title'] == 'Mrs', np.logical_and(df['Sex'] == 'female', df['Age'] > 28)).astype(int)\n    df['Miss'] = np.logical_and(df['Title'] != 'Mrs', df['Sex'] == 'female').astype(int)\n    \n    return df[choosen_features]\n    ","243bce74":"X_train = encode_categorical(data_train)","7e2dc290":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')\nX_train_imputed = pd.DataFrame(imputer.fit_transform(X_train))\nX_train_imputed.columns = X_train.columns\nX_train_imputed.index = X_train.index","d46d4c89":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed))\nX_train_scaled.columns = X_train_imputed.columns\nX_train_scaled.index = X_train_imputed.index","95c3be45":"Y_train = data_train['Survived']","608c6df5":"from statistics import median\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val_classifiers(X, Y, clfs, labels=None):\n    scores = []\n    best_median_score = 0\n    best_clf_idx = 0\n    for i in range(len(clfs)):\n        score = cross_val_score(clfs[i], X, Y)\n        scores.append(score)\n        m = median(score)\n        if m > best_median_score:\n            best_median_score = m\n            best_clf_idx = i\n    \n    if labels is None:\n        labels = clfs\n        \n    scores_df = pd.DataFrame(columns=labels, data=np.array(scores).T)\n    return scores_df, best_clf_idx \n\n","ea03d18c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nsvc_clf = SVC(kernel='linear', C=1)\nforest_clf = RandomForestClassifier(random_state=42)\nscores_df, best_median = cross_val_classifiers(X_train_scaled, Y_train, [svc_clf, forest_clf])\n\nprint(best_median)\nscores_df","2fd88d76":"scores_df.describe()","f941e759":"def preprocess_data(df):\n    df = encode_categorical(df)\n    df_imputed = pd.DataFrame(imputer.transform(df))\n    df_imputed.columns = df.columns\n    df_imputed.index = df.index\n    df = scaler.transform(df_imputed)\n    return df","5d374ece":"from sklearn.model_selection import GridSearchCV\n\ndef gridSearchSVM(X, Y):\n    param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n    grid = GridSearchCV(SVC(random_state=42), param_grid,refit=True, verbose=2)\n    grid.fit(X, Y)\n    return grid","48e0c9d5":"# clf = gridSearchSVM(X_train_scaled, Y_train)","64719a3c":"# print(clf.best_params_)","c99e9b36":"best_params = {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}","bb9900b9":"clf = SVC(random_state=42)\nclf.set_params(**best_params)\nclf.fit(X_train_scaled, Y_train)\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nX_test = preprocess_data(data_test)\n","33359506":"# pred_out = clf.predict(X_test)\npred_out = clf.predict(X_test)","3fa2a831":"out_df = pd.DataFrame({'PassengerId': data_test['PassengerId'], 'Survived': pred_out})\nout_df.to_csv('submission.csv',index=False)\n","4337ab5f":"Hello everyone,\n\nIn this series of articles I will go through the famous kaggle competition [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)\n\nAt first, what are the main steps we should go through in this project?\n\n# TODOs:\n\n1. Understand the problem (look at the big picture)\n2. Get the data\n3. Explore the data and get insights\n4. Prepare the data \n5. Explore different models and choose one\n6. Fine-tune your model\n\nToday, we will go through the first three points.","455d413c":"### Checking data types and missing values","2d31e615":"### Analyzing Histograms of Categorical variables","7fa97a2f":"# 2. Get the data\n\nUsing Kaggle online notebooks this is so easy, kaggle automatically downloads the data for us and we just need to import it using pandas:","8badb228":"### Checking correlations between variables","8d525e93":"We can classify our variables into numerical and categorical variables:\n- Numerical:\n    - Age\n    - Fare\n    - SibSp\n    - Parch\n- Categorical:\n    - Sex\n    - Cabin\n    - Embarked\n    - Pclass\n    - Title (Extracted from name, e.g.: Mr, Miss, Mrs..)","08e68e41":"\n# 3. Explore the data and get insights\n\nExploring the data specially via visualization helps to understand it better, know variables and how they may affect each others.\n\n### As a start, let's see sample of the training data","4504254d":"\n### Now let's understand what variables means\n\n- PassengerId, Survived (prediction target), Name, Sex, Age, Ticket, Fare, Cabin are self-explantory.\n- Pclass: passenger class, values = (1, 2, 3) \n- SibSp: # of siblings \/ spouses aboard the Titanic\t\n- Parch: # of parents \/ children aboard the Titanic\n- embarked: Port of Embarkation, values = (C, Q, S)\n    - C = Cherbourg, Q = Queenstown, S = Southampton","cb736b57":"# 1. Understand the problem\n\n*What is the objective?* Despite this question appears so easy, but it is very important..\n\nHere, the objective is so clear: **use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.**\n\nSo, we need to build a predictive model with:\n\n- Input: passenger data (ie name, age, gender, socio-economic class, etc).\n- Output: what passengers were more likely to survive."}}