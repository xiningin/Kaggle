{"cell_type":{"5adddd47":"code","75682356":"code","5470e856":"code","192a2d11":"code","a804cd9f":"code","0d08803f":"code","b5d60917":"code","e5c5ee15":"markdown","da12062c":"markdown","d09cc15c":"markdown","dbe6bea1":"markdown","1f0de02e":"markdown","95f5ea83":"markdown","e0c34384":"markdown"},"source":{"5adddd47":"import os\nimport sys\nimport torch\nimport random\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport pandas as pd\nimport time\n\n# From https:\/\/github.com\/graykode\/gpt-2-Pytorch couldn't find a pip version\n# I uploaded this gpt-2-Pytorch library as a dataset, so it would permanently\n# reside in the input folder, which allowed the notebook commit sucessfully.\nos.chdir('\/kaggle\/input\/gpt2pytorch\/gpt-2-Pytorch')\nsys.path.insert(1, '\/kaggle\/input\/gpt2pytorch\/gpt-2-Pytorch\/')\n\nfrom GPT2.model import (GPT2LMHeadModel)\nfrom GPT2.utils import load_weight\nfrom GPT2.config import GPT2Config\nfrom GPT2.sample import sample_sequence\nfrom GPT2.encoder import get_encoder\n\n# set pandas preview to use full width of browser\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n","75682356":"view_local_files = False\n\nif view_local_files is True:\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","5470e856":"# https:\/\/www.kaggle.com\/bkkaggle\/generate-your-own-text-with-openai-s-gpt-2-117m\n\nstate_dict = torch.load(\n    '..\/..\/..\/input\/gpt2pytorch-modelbin\/gpt2-pytorch_model.bin',\n    map_location='cpu' if not torch.cuda.is_available() else None)\n\nseed = random.randint(0, 2147483647)\nnp.random.seed(seed)\ntorch.random.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load Model\nenc = get_encoder()\nconfig = GPT2Config()\nmodel = GPT2LMHeadModel(config)\nmodel = load_weight(model, state_dict)\nmodel.to(device)\nmodel.eval()\n\ndef force_period(text):\n    \"\"\"If input string does not end with common punctuation,\n    a period is added to the end. credit:\n    https:\/\/stackoverflow.com\/a\/41402588\n    A dangling word at the end of a sentence that doesn't\n    end with punctuation causes GPT-2 to go off topic.\n    \"\"\"\n    if text[-1] not in ['!', ',', '.', '\\n']:\n        text += '.'\n    \n    return text\n\ndef clean(text):\n    \"\"\"Removes various characters and string patterns\n    generated by GPT-2.\n    \"\"\"\n    text = text.replace('\\n', ' ').replace('<|endoftext|>', '').strip()\n    \n    return text\n\ndef text_generator(state_dict,\n                   text,\n                   match_length=True,\n                   match_length_multiplier=2,\n                   length=50,\n                   temperature=0.5,\n                   top_k=30):\n    \"\"\"code by TaeHwan Jung(@graykode)\n    Original Paper and repository here : https:\/\/github.com\/openai\/gpt-2\n    GPT2 Pytorch Model : https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\n    Modifications by John David Parsons for the Kaggle \"Real or Not?\" competition\n    Depends on external GPT2 variables initialized outside of this function.\n    \n    Args:\n        text: sentence to begin with.\n        length: number of words to generate, only read if match_length is False\n        temperature: 0=deterministic, 1.0 is wildly creative and risks going off topic\n        \n    Returns:\n        A string of GPT-2 generated text, based on the input text.\n    \"\"\"\n    \n    text = force_period(text)\n    \n    # very short texts benefit from a longer multiplier\n    if len(text) < 30:\n        match_length_multiplier += 1\n    \n    # very long texts do not need as much multiplier\n    if len(text) > 120 and match_length_multiplier > 1:\n        match_length_multiplier -= 1\n    \n    if match_length is True:\n        length = len(word_tokenize(text)) * match_length_multiplier\n\n    # max tweet length is 280 characters, estimating a max of 50 words\n    length = min(length, 50)\n    unconditional = False\n\n    context_tokens = enc.encode(text)\n\n    out = sample_sequence(\n        model=model,\n        length=length,\n        context=context_tokens if not unconditional else None,\n        start_token=enc.encoder['<|endoftext|>'] if unconditional else None,\n        batch_size=1,\n        temperature=temperature,\n        top_k=top_k,\n        device=device)\n    out = out[:, len(context_tokens):].tolist()\n\n    text = enc.decode(out[0])\n    text = clean(text)\n\n    return text\n\n\ndef get_fake_tweets(df, num_samples=10):\n    \"\"\"Generates fake text similar to the original. NOTE:\n    enabling the GPU will speed up execution by about 2x.\n    60 rows took 85 seconds on a CPU, 45 seconds on a GPU\n\n    Args:\n        df: A pandas dataframe with columns 'text' and 'target'\n        num_samples: number of rows to generate\n\n    Returns:\n        A pandas dataframe containing only the new generated\n        text. The dataframe has the following columns:\n        'original_text', 'fake_text', 'target'\n    \"\"\"\n    \n    start_time = time.time()\n    expanded_rows = []\n\n    for i, row in df.sample(num_samples).iterrows():\n        row_original_text = row['text']\n        row_target = row['target']\n\n        generated_text = text_generator(state_dict, row_original_text)\n        expanded_row = [row_original_text, generated_text, row_target]\n        expanded_rows.append(expanded_row)\n\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\n\n    expanded_df = pd.DataFrame(\n        expanded_rows, columns=['original_text', 'fake_text', 'target'])\n\n    return expanded_df","192a2d11":"#train_df = pd.read_csv('..\/..\/..\/input\/nlp-getting-started\/train.csv')\ntrain_df = pd.read_csv('..\/..\/..\/input\/tweet-cleaner\/train_df_clean.csv')\n\ntrain_df = train_df[['text', 'target']]\n\ntrain_df","a804cd9f":"# iloc of interesting test tweets\n# 5725 = rescuing bodies in the water\n# 333 = Windows is ethics armageddon\n# 5678 = Dog buried alive\n# 7611 = e-bike crash\n# 7 = fire in the woods\n\ntest_tweet = train_df.iloc[7611]['text']\n#test_tweet = 'Wow, it is super stormy out right now. The lightning woke me up :\/'\ngenerated_tweet = text_generator(state_dict, test_tweet, match_length_multiplier=2)\n\nprint('ORIGINAL: ' + test_tweet)\nprint('GPT-2: ' + generated_tweet)","0d08803f":"# num_samples=3000 took around 40 min with the GPU\nfaked_df = get_fake_tweets(train_df, num_samples=10)\nfaked_df = faked_df[['fake_text', 'target']]\nfaked_df.columns = ['text', 'target']\nfaked_df.to_csv('..\/..\/..\/working\/faked_df.csv', index=False)\nfaked_df","b5d60917":"train_df_combined = pd.concat([train_df, faked_df])\ntrain_df_combined.to_csv('..\/..\/..\/working\/train_df_combined.csv', index=False)","e5c5ee15":"### Helper to view file paths of imported data","da12062c":"Starting with the original training data, randomly sample rows to use as the source material for generating new fake tweets. The same original tweet will result in different fake tweets, so it is safe to sample the same row multiple times. get_fake_tweets returns a dataframe with the following columns: 'original_text', 'fake_text', 'target'. Select just the 'fake_text' column and rename to 'text' so the new rows can be concatenated to the original training dataframe. If you want to double the size of your training data, set num_samples to over 9000 and come back in a few hours...","d09cc15c":"Finally, save the newly expanded training data to CSV. Download this file and plug it in to your existing pipeline as a bigger training set!","dbe6bea1":"Load data","1f0de02e":"# Example usage\n\nDo a test run of the text_generator method. It's a lot of fun to set test_tweet to your own prose!","95f5ea83":"# Using GPT-2 to generate fake tweets for the Real or Not competition\n\nModels benefit from larger training datasets. When working with images, the original dataset is often augmented by creating transformations of each image. For example, a single image of a chinchilla can be augmented in the following ways:\n\n<img src=\"https:\/\/raw.githubusercontent.com\/aleju\/imgaug-doc\/master\/readme_images\/examples_grid.jpg\" width=\"450px\">\n\n(credit: https:\/\/github.com\/aleju\/imgaug)\n\nThis notebook is my attempt at doing a similar type of augmentation with **text**. Real tweets (after being cleaned by my other notebook [Tweet Cleaner](https:\/\/www.kaggle.com\/jdparsons\/tweet-cleaner)) are sent to the GPT-2 \"small\" 117M model as the starter text, and the output is assigned the same real-or-not label as the original text. The GPT-2 code is a modified version of the code found in this notebook: https:\/\/www.kaggle.com\/bkkaggle\/generate-your-own-text-with-openai-s-gpt-2-117m.\n\nHere are some example results - good, bad, and hilarious. The generated text for disasters seems to pick up a \"news\" style of writing, while non-disaster text stays more casual. The GPT-2 default settings seem to result in lots of image credits and twitter links, which isn't ideal. **If you have any ideas for improving this notebook, please let me know in the comments!**\n\n| Original Text | Fake Text | Label |\n| :------------ | :-------------- | :----- |\n| Forest fire near La Ronge Sask. Canada |  The fire was brought under control by a single vehicle on Sunday afternoon.  The fire started at about 11:50 p.m | 1 |\n| Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries. | PITTSBURGH (AP) \u2014 A car collided with an e-bike on the side of a street Saturday night in Little Portugal and injured a woman, police said.  Police investigating after an e-bike collided with a | 1 |\n| My car is so fast | I can get to work or just leave town. I'm not going anywhere.\" | 0 |\n| My car is so fast | Just like any other car, I start moving quickly and the sound is amazing. I really | 0 |\n| These boxes are ready to explode! Exploding Kittens finally arrived! gameofkittens #explodingkittens\u0089\u00db_ https:\/\/t.co\/TFGrAyuDC5 | twitter.com\/U4YK5XgXqTThis is a rush transcript. Copy may not be in its final form.  AMY GOODMAN: Let's go back to the recent story in the New York Times that New | 0 |\n| major rainstorm happening! I'm gonna lie down and drift away to the storming around me for a little while. bebacksoon. | The best way to get your dog and pet out of a storm is to make sure that you have the shelter in place. In some cases, you can even make your dog and pet stay in a park, in a shelter. | 1 |\n| I went to pick up my lunch today and the bartender was holding my change hostage because he wanted my number. ?? | At this point I was telling him I didn't want to pay up, so I called back and told him to go out and get it for me. He said I would pay up, but then I called my parents and told them I didn't | 1 (seems misclassified) |\n| @AmirKingKhan you would have been annihilated so you might as well thank @FloydMayweather | Mark Cuban (@MarkCuban) July 11, 2016  What's more, he didn't just win the fight.  When he won the heavyweight title, he was the first fighter in UFC history to knock out a UFC | 0 |\n| Aftershock \u0089\u00e3\u00a2 (2010) Full\u0089\u00e3\u00a2 Streaming - YouTube http:\/\/t.co\/vVE3UsesGf | I think I'm going to be using it right now. I'd like to watch it on my phone just to see if its worth it.  Reply \u00b7 Report PostIt is hard | 0 |\n| That horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time | We have seen all the time that 3G has been an issue, it is our most critical aspect, we do not want to see this happen to our children and our grandchildren when it comes to 3G. | 1 (seems misclassified) |\n| Accident on I-24 W #NashvilleTraffic. Traffic moving 8m slower than usual. https:\/\/t.co\/0GHk693EgJ | NMA Traffic (@NMATraffic) December 14, 2017  The car was spotted in the parking lot by several people in the area at 7:45 p.m., but all were not injured.  There was no word | 1 |\n\n---\n\nCode linted via http:\/\/pep8online.com\/ and https:\/\/yapf.now.sh\/ to follow the Google python style guide (mostly).","e0c34384":"# Functions to interract with GPT-2"}}