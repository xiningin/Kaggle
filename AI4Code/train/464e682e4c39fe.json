{"cell_type":{"2a68d3ed":"code","300d0d73":"code","91dcee74":"code","11277d6b":"code","64f4f2bf":"code","cf32de74":"code","74f04d02":"code","d6d609a6":"code","4b660632":"code","59565500":"code","2f732050":"code","37baa853":"code","5d5c8257":"code","06405609":"code","3b1901a7":"code","cae35479":"code","1bdbe7a9":"markdown","c8af7540":"markdown","445be280":"markdown","d2ac5e44":"markdown","8ccd931b":"markdown","6124938b":"markdown","838e7f26":"markdown","b4451d78":"markdown","29c85436":"markdown","19953531":"markdown","97c521a0":"markdown","aac15ad5":"markdown","c25fc842":"markdown","09b2e1cb":"markdown","79c93949":"markdown","94723545":"markdown","ddfeb8ae":"markdown","1fe3c8fd":"markdown","436741bc":"markdown","0279fdcc":"markdown","3dc8f00b":"markdown","a0802205":"markdown","b6dda93c":"markdown"},"source":{"2a68d3ed":"!pip install --upgrade ngboost\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.plotting import parallel_coordinates\n\nimport os\nimport cv2\nimport math\n\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport wandb\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import skew\nfrom ngboost import NGBRegressor\n\nfrom sklearn.decomposition import PCA \nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE \nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom pprint import pprint\n\nimport tensorflow.compat.v2 as tf\ntf.enable_v2_behavior()\n\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\n\nsns.reset_defaults()\n#sns.set_style('whitegrid')\n#sns.set_context('talk')\nsns.set_context(context='talk',font_scale=0.7)\n\ntfd = tfp.distributions\n\n\n# Set Style\nsns.set_style(\"white\")\nmpl.rcParams['xtick.labelsize'] = 16\nmpl.rcParams['ytick.labelsize'] = 16\nmpl.rcParams['axes.spines.left'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\nplt.rcParams.update({'font.size': 17})\n","300d0d73":"train=pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest=pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\n\nROOT_PATH = Path('..\/input\/petfinder-pawpularity-score')\nTRAIN_IMGS_PATH = ROOT_PATH \/ 'train\/'\ncolumns = train.columns.tolist()\ncolumns.insert(1, 'image')\n\n","91dcee74":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')\n    \nCONFIG = dict(competition = 'PetFinder',_wandb_kernel = 'tensorgirl')","11277d6b":"feature_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n       'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\nY = train['Pawpularity']\ntrain_features = train[feature_columns]\ntest_features = test[feature_columns]\n\nnegloglik = lambda y, rv_y: -rv_y.log_prob(y)\n\nx = train_features.to_numpy(\"float64\")\ny = Y.to_numpy(\"float64\")\ntest = test_features.to_numpy(\"float64\")\n","64f4f2bf":"plt.figure(figsize=(15, 7))\nplt.subplot(121)\nsns.kdeplot(train.Pawpularity , color = \"#E4916C\")\nplt.subplot(122)\nsns.boxplot(train.Pawpularity , color = \"#BCE6EF\")","cf32de74":"# code copied from https:\/\/www.kaggle.com\/aakashnain\/which-features-to-use-and-why\nfeatures = train.columns[1:-1].tolist()\nnum_cols = 2\nnum_rows = len(features) \/\/ num_cols\n\n\nfig, axs = plt.subplots(num_rows,\n                        num_cols,\n                        figsize=(20, 15),\n                        sharex=False,\n                        sharey=True\n                       )\n\nfor i, feature in enumerate(features):\n    _ = sns.kdeplot(data=train,\n                 x=\"Pawpularity\",                 \n                 ax=axs[i \/\/ num_cols, i % num_cols],\n                 hue=feature,\n                 palette =sns.color_palette([\"#E4916C\", \"#BCE6EF\"])\n                )\nplt.show()","74f04d02":"df_train = train_features.melt(value_vars=feature_columns)\nplt.figure(figsize = (15, 7))\nsns.countplot(data=df_train, y=\"variable\", hue=\"value\" , palette =sns.color_palette([\"#E4916C\", \"#BCE6EF\"]))\nplt.show()","d6d609a6":"tsne = TSNE(n_components=2, random_state=4)\nt_train = tsne.fit_transform(train_features)\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    x=t_train[:, 0], y=t_train[:, 1],\n    hue=train['Pawpularity'],\n    alpha=0.3\n)\nplt.show()","4b660632":"t_train = pd.DataFrame(t_train, index=train.Id, columns=[\"c\"+str(c) for c in range(2)])\nkm = KMeans(n_clusters=2, random_state= 4).fit(t_train)\ny_km = km.predict(t_train)\nt_train[\"cluster\"] = y_km\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    data=t_train,\n    x=\"c0\", y=\"c1\",\n    hue=\"cluster\",\n    alpha=0.3,\n    palette =sns.color_palette([\"#E4916C\", \"#BCE6EF\"])\n)\nplt.show()","59565500":"n_comp = 2\npca = PCA(n_components=n_comp, svd_solver='full', random_state=4)\nX_pca = pca.fit_transform(train_features)\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    data=X_pca,\n    x=X_pca[:, 0], y=X_pca[:, 1],\n    hue=train['Pawpularity'],\n    alpha=0.3\n    )\nplt.show()","2f732050":"# Initialize a W&B run to log images\nrun = wandb.init(project='Pawpularity', config=CONFIG, anonymous=anony) # W&B Code 1\n\ndata_at = wandb.Table(columns=columns) # W&B Code 2\n\nfor i in tqdm(range(len(train))):\n    row = train.loc[i]\n    img_id = row.Id\n\n    data_at.add_data(img_id,                                            \n                     wandb.Image(f'{TRAIN_IMGS_PATH}\/{img_id}.jpg'),\n                     *tuple(row.values[1:])) # W&B Code 3\n\nwandb.log({'Raw Petfinder data': data_at}) # W&B Code 4\nwandb.finish() # W&B Code 5","37baa853":"# This is just to display the W&B run page in this interactive session.\n#from IPython import display\n\n# we create an IFrame and set the width and height\n#iF = display.IFrame(run.url, width=1080, height=720)\n#iF","5d5c8257":"negloglik = lambda y, rv_y: -rv_y.log_prob(y)","06405609":"# Build model.\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)),\n])\n\n# Do inference.\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)\nmodel.fit(x,y, epochs=1000, verbose=False);\n\n# Profit.\n[print(np.squeeze(w.numpy())) for w in model.weights];\nyhat = model(test)\nassert isinstance(yhat, tfd.Distribution)","3b1901a7":"\nX_train, X_test, Y_train, Y_test = train_test_split(train[feature_columns], Y, test_size=0.2)\n\nngb = NGBRegressor().fit(X_train, Y_train)\nY_preds = ngb.predict(X_test)\nY_dists = ngb.pred_dist(X_test)\n\n# test Mean Squared Error\ntest_MSE = mean_squared_error(Y_preds, Y_test)\nprint('Test MSE', test_MSE)\n\n","cae35479":"rmse = math. sqrt(test_MSE)","1bdbe7a9":"# **<span style=\"color:#e76f51;\">t-SNE<\/span>**","c8af7540":"# **<span style=\"color:#AB51E9;\">Target Variable Distribution<\/span>**","445be280":"# **<span style=\"color:#AB51E9;\">Frequency Distribution of Meta Features<\/span>**","d2ac5e44":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [PetFinder - Popularity Score Project on W&B Dashboard]\n(https:\/\/wandb.ai\/usharengaraju\/Pawpularity)\n> \n> - To get the API key, create an account in the [website](https:\/\/wandb.ai\/site) .\n> - Use secrets to use API Keys more securely ","8ccd931b":"# <h1 style='background:#AB51E9; border:0; color:black'><center> PetFinder.my - Pawpularity Contest <\/center><\/h1> ","6124938b":"# **<span style=\"color:#e76f51;\">Simple and modular approach.<\/span>**\n\nThe NGBoost algorithm is simple to use. It has three abstract modular components that are chosen as configuration:\n\n\n**Base Learner**\n\nThe most common choice is Decision Trees, which tend to work well on structured inputs.\n\n**Probability Distribution**\n\nThe distribution needs to be compatible with the output type. For e.g. Normal distribution for real valued outputs, Bernoulli for binary outputs.\n\n**Scoring rule**\n\nMaximum Likelihood Estimation is an obvious choice. More robust rules such as Continuous Ranked Probability Score are also suitable.\nThe above choices can be mixed and matched to be customized for the specific prediction problem at hand.\n\n![](https:\/\/drive.google.com\/uc?id=1sgb1BJKIH8PN4NhuXQ1tcxkfO7T1RMOT)","838e7f26":"# **<span style=\"color:#e76f51;\">Predictive Uncertainty Estimation in the real world.<\/span>**\n\nEstimating the uncertainty in the predictions of a machine learning model is crucial for production deployments in the real world. In addition to making accurate predictions, we also want a correct estimate of uncertainty along with each prediction. When model predictions are part of an automated decision-making workflow or production line, predictive uncertainty estimates are important for determining manual fallback alternatives or for human inspection and intervenion.\n\nProbabilistic prediction (or probabilistic forecasting), which is the approach where the model outputs a full probability distribution over the entire outcome space, is a natural way to quantify those uncertainties.\n\nComparison between the point predictions vs probabilistic predictions \n\n![](https:\/\/drive.google.com\/uc?id=1gurE8gafuf5m9xM7Rg7zhbxT0_qdTHGN)","b4451d78":"### [Check out the W&B Tables $\\rightarrow$](https:\/\/wandb.ai\/anony-mouse-139969\/Pawpularity\/runs\/27ip81sk)\n\n![img](https:\/\/i.imgur.com\/cV9ycET.gif)","29c85436":"We can use a variety of standard continuous and categorical and loss functions with this model of regression. Mean squared error loss for continuous labels, for example, means that P(y | x, w) is a normal distribution with a fixed scale (standard deviation). Cross-entropy loss for classification means that P(y | x, w) is the categorical distribution.","19953531":"# **<span style=\"color:#e76f51;\">Competitive performance in both uncertainty estimates and traditional metrics.<\/span>**\n\nNGBoost requires far less expertise to use than competing methods, and performs as well on common benchmarks. NGBoost has particularly strong performance on smaller data sets.\n\n![](https:\/\/drive.google.com\/uc?id=1vuz4OymodyDBxbxwT5sMyjYDBs429pNN)","97c521a0":"# **<span style=\"color:#e76f51;\">K - Nearest Neighbours<\/span>**","aac15ad5":"![](https:\/\/drive.google.com\/uc?id=1fZu2LfKginyy8jxhMlCRtUNbkSGKbu98)","c25fc842":"# **<span style=\"color:#e76f51;\">NGBoost brings predictive uncertainty estimation to Gradient Boosting.<\/span>**\n\nGradient Boosting methods have generally been among the top performers in predictive accuracy over structured or tabular input data.\n\nNGBoost enables predictive uncertainty estimation with Gradient Boosting through probabilistic predictions (including real valued outputs). With the use of Natural Gradients, NGBoost overcomes technical challenges that make generic probabilistic prediction hard with gradient boosting.\n\n![](https:\/\/drive.google.com\/uc?id=1v7ootml1mOBt32SjQPRo2UcrMWDIn7QF)\n\n[Github](https:\/\/github.com\/stanfordmlgroup\/ngboost)","09b2e1cb":"# **<span style=\"color:#AB51E9;\">TensorFlow Probability<\/span>**  \n\n[Source :](https:\/\/blog.tensorflow.org\/2019\/03\/regression-with-probabilistic-layers-in.html)\n\nRegression is one of the most basic techniques that a machine learning practitioner can apply to prediction problems However, many analyses based on regression omit a proper quantification of the uncertainty in the predictions, owing partially to the degree of complexity required. To start to quantify the uncertainty, a particularly elegant way of posing the problem is to write the regression model as P(y | x, w), the probability distribution of labels (y), given the inputs (x) and some parameters (w). We can fit this model to the data by maximizing the probability of the labels, or equivalently, minimizing the negative log-likelihood loss: -log P(y | x). In Python:\n\n","79c93949":"# **<span style=\"color:#AB51E9;\">NGBoost: Natural Gradient Boosting for Probabilistic Prediction<\/span>**\n[Website](https:\/\/stanfordmlgroup.github.io\/projects\/ngboost\/)\n\n[Paper](https:\/\/arxiv.org\/pdf\/1910.03225.pdf)\n\nNGBoost generalizes gradient boosting to probabilistic regression by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm.NGBoost matches or\nexceeds the performance of existing methods for probabilistic prediction while offering additional\nbenefits in flexibility, scalability, and usability.\n","94723545":"PetFinder.my is Malaysia\u2019s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.\n\nCurrently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.\n\n# **<span style=\"color:#AB51E9;\">Goal<\/span>**\n \nThe goal is to analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos.\n\n# **<span style=\"color:#AB51E9;\">Data<\/span>**\n\n**Training Data**\n\n> - ```train\/``` - Folder containing training set photos of the form {id}.jpg, where {id} is a unique Pet Profile ID.\n> - ```train.csv``` - Metadata (described below) for each photo in the training set as well as the target, the photo's Pawpularity score. The Id column gives the photo's unique Pet Profile ID corresponding the photo's file name.\n\n**Example Test Data**\n\nIn addition to the training data, we include some randomly generated example test data to help you author submission code. When your submitted notebook is scored, this example data will be replaced by the actual test data (including the sample submission).\n\n> - ```test\/``` - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\n> - ```test.csv``` - Randomly generated metadata similar to the training set metadata.\nsample_submission.csv - A sample submission file in the correct format.\n\n**Photo Metadata**\n\nThe train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n\n> - ```Focus``` - Pet stands out against uncluttered background, not too close \/ far.\n> - ```Eyes``` - Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear.\n> - ```Face``` - Decently clear face, facing front or near-front.\n> - ```Near``` - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n> - ```Action``` - Pet in the middle of an action (e.g., jumping).\n> - ```Accessory``` - Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash.\n> - ```Group``` - More than 1 pet in the photo.\n> - ```Collage``` - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n> - ```Human``` - Human in the photo.\n> - ```Occlusion``` - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n> - ```Info``` - Custom-added text or labels (i.e. pet name, description).\n> - ```Blur``` - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.","ddfeb8ae":"# **<span style=\"color:#AB51E9;\">Distribution of Meta Features vs Target <\/span>**","1fe3c8fd":"# **<span style=\"color:#e76f51;\">The natural gradient makes learning efficient and effective.<\/span>**\n\nOur key innovation is in employing the natural gradient to perform gradient boosting by casting it as a problem of determining the parameters of a probability distribution.\n\nOrdinary gradients can be highly unsuitable for learning multi-parameter probability distributions (such as the Normal distribution). The training dynamics with the use of natural gradients tends to be much more stable and result in a better fit, as seen in the probabilistic regression example above.\n\n![](https:\/\/drive.google.com\/uc?id=11okOcrrcFK8ErqqGchNL1xa9GSCt95mt)","436741bc":"# Work in progress \ud83d\udea7","0279fdcc":"# **<span style=\"color:#AB51E9;\">References<\/span>**\n\nhttps:\/\/arxiv.org\/pdf\/1910.03225.pdf\n\nhttps:\/\/stanfordmlgroup.github.io\/projects\/ngboost\/\n\n@kooose  https:\/\/www.kaggle.com\/kooose\/eda-by-t-sne\n\n@aakashnain https:\/\/www.kaggle.com\/aakashnain\/which-features-to-use-and-why\n\n@ayuraj Wandb Content","3dc8f00b":"# **<span style=\"color:#e76f51;\">PCA<\/span>**","a0802205":"# **<span style=\"color:#AB51E9;\">Visualize Dataset Interactively using W&B Tables<\/span>**\n\nIt only requires 5 lines of extra code to get the power of W&B Tables. \n\n1. You first need to initialize a W&B run using `wandb.init` API. This step is common for any W&B Logging.\n2. Create a `wandb.Table` object. Imagine this to be an empty Pandas Dataframe. \n3. Iterate through each row of the `train.csv` file and `add_data` to the `wandb.Table` object. Imagine this to be appending new rows to your Dataframe. \n4. Log the W&B Tables using `wandb.log` API. You will use this API to log almost anything to W&B.\n5. In a Juypter like interactive session, you need to call `wandb.finish` to close the initialized W&B run. \n\nSource : Content copied from Ayush notebook\n","b6dda93c":"# **<span style=\"color:#AB51E9;\">Clustering<\/span>**"}}