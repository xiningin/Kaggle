{"cell_type":{"64d5d9e7":"code","999030d1":"code","ab76350f":"code","680d5d74":"code","cb84112f":"code","ac499fd4":"code","9e316c39":"code","1226091b":"code","f0c270a7":"code","4cafc6e3":"code","2cecc833":"code","5ca95bc1":"code","7c4c6115":"code","76677317":"code","84f6c1c9":"code","a33bc957":"code","b88579aa":"code","e0799bdd":"code","5022b5d5":"markdown","68e21297":"markdown","eceb5fd8":"markdown","8e0643c0":"markdown","1e1a99f8":"markdown","d2e0ed0f":"markdown","305b8dc5":"markdown"},"source":{"64d5d9e7":"# converts midi files to wav files into order to play them\nprint('installing fluidsynth...')\n!apt-get install fluidsynth --assume-yes --fix-missing > \/dev\/null\nprint('done!')\n\n# update the version of music21, used to parse midi files\n!pip install music21==6.3.0","999030d1":"import os, random, math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport torchaudio.transforms as T\nfrom music21 import midi\nfrom IPython.display import Audio()\n\nfrom typing import Optional, Tuple\nfrom torch import Tensor","ab76350f":"def get_df(\n    maestro_path = '\/kaggle\/input\/themaestrodatasetv2\/maestro-v2.0.0\/',\n    perf_path = '\/kaggle\/input\/the-maestro-dataset-performance-events\/maestro-v2.0.0\/',\n):\n    df = pd.read_csv(f'{maestro_path}\/maestro-v2.0.0.csv')\n    for col in ['canonical_composer', 'canonical_title', 'split', 'year']:\n        df[col] = df[col].astype('category')\n    df['mp3_path'] = maestro_path + df.audio_filename.str.split('.').str.get(0) + '.mp3'\n    df['midi_path'] = maestro_path + df.midi_filename\n    df['perf_path'] = perf_path + df.midi_filename + '.pt'\n    return df\n\n\nget_df().head()","680d5d74":"class DataframeDataset(torch.utils.data.Dataset):\n    # df is a dataframe that has a path column (path_col) for each file\n    # features is a list of columns in the dataframe that will be added to items\n    # preload_device can be cpu\/cuda\n    def __init__(self, df, features=[], path_col='path', preload_device=None):\n        self.df = df.copy()\n        for feature in features:\n            if self.df[feature].dtype.name == 'category':\n                # convert categories to indices\n                self.df[feature] = self.df[feature].cat.codes\n\n        self.features = features\n        self.path_col = path_col\n        self.preloaded_data = None\n        if preload_device is not None:\n            self.preloaded_features = []\n            self.preloaded_data = []\n            for i, row in self.df.iterrows():\n                self.preloaded_features.append({\n                    k: torch.tensor(v).to(preload_device)\n                    for k, v in row[features].to_dict().items()\n                })\n                self.preloaded_data.append(\n                    self._get_data(row[self.path_col]).to(preload_device)\n                )\n\n    def __len__(self):\n        return(len(self.df))\n\n    def __getitem__(self, index):\n        if self.preloaded_data is None: \n            row = self.df.iloc[index]\n            features = row[self.features].to_dict()\n            data = self._get_data(row[self.path_col])\n        else:\n            features = self.preloaded_features[index]\n            data = self.preloaded_data[index]\n        return {**features, 'data': data}\n\n    def _get_data(self, path):\n        raise NotImplementedError()\n\n\nclass PerformanceDataset(DataframeDataset):\n    def __init__(self, df, features=[], path_col='perf_path', preload_device=None, transform=None):\n        self.transform = transform\n        super().__init__(df, features, path_col, preload_device)\n\n    def _get_data(self, path):\n        return torch.jit.load(path).state_dict()['0'].long()\n\n    def __getitem__(self, index):\n        item = super().__getitem__(index)\n        if self.transform is not None:\n            item['data'] = self.transform(item['data'])\n        return item","cb84112f":"class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, x):\n        for transform in self.transforms:\n            x = transform(x)\n        return x\n    \n    \nclass RandomCrop1D:\n    def __init__(self, size, pad_if_needed=True, fill=0):\n        self.size = size\n        self.pad_if_needed = pad_if_needed\n        self.fill = fill\n    def __call__(self, x):\n        x_len = x.shape[-1]\n        len_diff = x_len - self.size\n        pad_needed = len_diff < 0\n        if pad_needed and not self.pad_if_needed:\n            raise Exception('too small')\n        start = random.randint(0, max(0, len_diff))\n        crop = x[..., start:start+self.size]\n        if pad_needed:\n            return F.pad(crop, pad=(0, max(0, -len_diff)), value=self.fill)\n        return crop\n    \n\nclass AugmentPitch:\n    def __init__(self, half_steps=3, in_place=True):\n        self.half_steps = half_steps\n        self.in_place = in_place\n\n    def __call__(self, x):\n        if not self.in_place:\n            x = x.clone()\n        pitch_transposition = random.randint(-self.half_steps, self.half_steps)\n        note_ons = (x >= 0) * (x < 128)\n        x[note_ons] = torch.clamp(x[note_ons] + pitch_transposition, 0, 127)\n        note_offs = (x >= 128) * (x < 256)\n        x[note_offs] = torch.clamp(x[note_offs] + pitch_transposition, 128, 255)\n        return x\n\n\nclass AugmentTime:\n    def __init__(self, time_pct=2.5, num_steps=5, in_place=True):\n        self.time_pct = time_pct \/ 100\n        self.in_place = in_place\n        self.num_steps = num_steps\n\n    def __call__(self, x):\n        if not self.in_place:\n            x = x.clone()\n        time_stretch = (1 - self.time_pct * 2) + random.randint(0, self.num_steps - 1) * self.time_pct\n        time_shifts = (x >= 256) * (x < 356)\n        x[time_shifts] = torch.clamp((x[time_shifts] - 256) * time_stretch, 0, 99).long() + 256\n        return x","ac499fd4":"df = get_df()\n\ntrain_dataset = PerformanceDataset(\n    df[df.split == 'train'],\n    features=['canonical_composer'],\n    transform=Compose([\n        RandomCrop1D(2048, fill=388),\n        AugmentPitch(in_place=False),\n        AugmentTime(),\n    ]),\n    preload_device='cuda',\n)\n\nval_dataset = PerformanceDataset(\n    df[df.split == 'validation'],\n    features=['canonical_composer'],\n    transform=Compose([\n        RandomCrop1D(2048, fill=388),\n    ]),\n    preload_device='cuda',\n)\n\ntrain_dataset[0]","9e316c39":"def read_midi(filepath):\n  mf = midi.MidiFile()\n  mf.open(filepath)\n  mf.read()\n  mf.close()\n  return mf\n\n\ndef write_midi(mf, filename = 'tempmidi.mid'):\n  mf.open(filename, attrib='wb')\n  mf.write()\n  mf.close()\n  return filename\n\n\ndef midi_to_wav(filename):\n  # linux ships with a default midi font\n  !fluidsynth -F $filename\\.wav -r 16000 -i -n -T wav \/usr\/share\/sounds\/sf2\/FluidR3_GM.sf2 $filename > \/dev\/null\n  return filename + '.wav'\n\n\ndef play_midi(mf, secs):\n  data, rate = torchaudio.load(midi_to_wav(write_midi(mf)))\n  display(Audio(data[0, :16000 * secs], rate=rate))\n\n    \nclass MidiToPerformanceConverter:\n    def __init__(self):\n        self.event_to_idx = {}\n        for i in range(128):\n          self.event_to_idx['note-on-' + str(i)] = i\n        for i in range(128):\n          self.event_to_idx['note-off-' + str(i)] = i + 128\n        for i in range(100):\n          self.event_to_idx['time-shift-' + str(i + 1)] = i + 128 + 128\n        for i in range(32):\n          self.event_to_idx['velocity-' + str(i)] = i + 128 + 128 + 100\n        self.idx_to_event = list(self.event_to_idx.keys())\n        self.num_channels = len(self.idx_to_event)\n\n        \n    def midi_to_idxs(self, mf):\n      event_to_idx = self.event_to_idx\n      ticks_per_beat = mf.ticksPerQuarterNote\n      # The maestro dataset uses the first track to store tempo data\n      tempo_data = next(e for e in mf.tracks[0].events if e.type == midi.MetaEvents.SET_TEMPO).data\n      # tempo data is stored at microseconds per beat (beat = quarter note)\n      microsecs_per_beat = int.from_bytes(tempo_data, 'big')\n      millis_per_tick = microsecs_per_beat \/ ticks_per_beat \/ 1e3\n\n      idxs = []\n      started = False\n      previous_t = None\n      is_pedal_down = False\n      notes_to_turn_off = set()\n      notes_on = set()\n\n      # The second track stores the actual performance\n      for e in mf.tracks[1].events:\n        #if started and e.type == 'DeltaTime' and e.time > 0:\n        if e.type == 'DeltaTime' and e.time > 0:\n          # event times are stored as ticks, so convert to milliseconds\n          millis = e.time * millis_per_tick\n\n          # combine repeated delta time events\n          t = millis + (0 if previous_t is None else previous_t)\n\n          # we can only represent a max time of 1 second (1000 ms)\n          # so we must split up times that are larger than that into separate events\n          while t > 0:\n            t_chunk = min(t, 1000)\n            idx = event_to_idx['time-shift-' + str(math.ceil(t_chunk \/ 10))]\n            if previous_t is None:\n              idxs.append(idx)\n            else:\n              idxs[-1] = idx\n              previous_t = None\n            t -= t_chunk\n          previous_t = t_chunk\n\n        elif e.type == midi.ChannelVoiceMessages.NOTE_ON:\n          if e.velocity == 0:\n            if is_pedal_down:\n              notes_to_turn_off.add(e.pitch)\n            elif e.pitch in notes_on:\n              idxs.append(event_to_idx['note-off-' + str(e.pitch)])\n              notes_on.remove(e.pitch)\n              previous_t = None\n          else:\n            if e.pitch in notes_to_turn_off:\n              idxs.append(event_to_idx['note-off-' + str(e.pitch)])\n              notes_to_turn_off.remove(e.pitch)\n              notes_on.remove(e.pitch)\n\n            # midi supports 128 velocities, but our representation only allows 32\n            idxs.append(event_to_idx['velocity-' + str(e.velocity \/\/ 4)])\n            idxs.append(event_to_idx['note-on-' + str(e.pitch)])\n            notes_on.add(e.pitch)\n            started = True\n            previous_t = None\n\n        elif e.type == midi.ChannelVoiceMessages.CONTROLLER_CHANGE and e.parameter1 == 64: # sustain pedal\n          # pedal values greater than 64 mean the pedal is being held down,\n          # otherwise it's up\n          if is_pedal_down and e.parameter2 < 64:\n            is_pedal_down = False\n            for pitch in notes_to_turn_off:\n              idxs.append(event_to_idx['note-off-' + str(pitch)])\n              notes_on.remove(pitch)\n            notes_to_turn_off = set()\n            previous_t = None\n          elif not is_pedal_down and e.parameter2 >= 64:\n            is_pedal_down = True\n            previous_t = None\n\n      return idxs\n\n\n    def make_note(self, track, pitch, velocity):\n      e = midi.MidiEvent(track, type=midi.ChannelVoiceMessages.NOTE_ON, channel=1)\n      e.pitch = int(pitch)\n      e.velocity = int(velocity)\n      return e\n\n\n    def idxs_to_midi(self, idxs, verbose=False):\n      if type(idxs) == torch.Tensor:\n        idxs = idxs.detach().numpy()\n\n      mf = midi.MidiFile()\n      mf.ticksPerQuarterNote = 1024\n\n      # The maestro dataset uses the first track to store tempo data, and the second\n      # track to store the actual performance. So follow that convention.\n      tempo_track = midi.MidiTrack(0)\n      track = midi.MidiTrack(1)\n      mf.tracks = [tempo_track, track]\n\n      tempo = midi.MidiEvent(tempo_track, type=midi.MetaEvents.SET_TEMPO)\n      # temp.data is the number of microseconds per beat (per quarter note)\n      # So to set ticks per millis = 1 (easy translation from time-shift values to ticks),\n      # tempo.data must be 1e3 * 1024, since ticksPerQuarterNote is 1024 (see above)\n      tempo.data = int(1e3 * 1024).to_bytes(3, 'big')\n\n      end_of_track = midi.MidiEvent(tempo_track, type=midi.MetaEvents.END_OF_TRACK)\n      end_of_track.data = ''\n      tempo_track.events = [\n        # there must always be a delta time before each event\n        midi.DeltaTime(tempo_track, time=0),\n        tempo,\n        midi.DeltaTime(tempo_track, time=0),\n        end_of_track\n      ]\n\n      track.events = [midi.DeltaTime(track, time=0)]\n\n      # set to 0 initially in case no velocity events occur before the first note\n      current_velocity = 0\n      notes_on = set()\n      errors = {'is_on': 0, 'is_not_on': 0}\n\n      for idx in idxs:\n        if 0 <= idx < 128: # note-on\n          pitch = idx\n          if pitch in notes_on:\n            if verbose:\n              print(pitch, 'is already on')\n            errors['is_on'] += 1\n            continue\n          if track.events[-1].type != 'DeltaTime':\n            track.events.append(midi.DeltaTime(track, time=0))\n          track.events.append(self.make_note(track, pitch, current_velocity))\n          notes_on.add(pitch)\n\n        elif 128 <= idx < 256: # note-off\n          pitch = idx - 128\n          if pitch not in notes_on:\n            if verbose:\n              print(pitch, 'is not on')\n            errors['is_not_on'] += 1\n            continue\n          if track.events[-1].type != 'DeltaTime':\n            track.events.append(midi.DeltaTime(track, time=0))\n          track.events.append(self.make_note(track, pitch, 1))\n          notes_on.remove(pitch)\n\n        elif 256 <= idx < 356: # time-shift\n          t = (1 + idx - 256) * 10\n          if track.events[-1].type == 'DeltaTime':\n            # combine repeated delta times\n            track.events[-1].time += t\n          else:\n            track.events.append(midi.DeltaTime(track, time=t))\n\n        else: # velocity\n          current_velocity = (idx - 356) * 4\n\n      if verbose:\n        print('remaining notes left on:', notes_on)\n\n      if track.events[-1].type != 'DeltaTime':\n        track.events.append(midi.DeltaTime(track, time=0))\n      track.events.append(end_of_track)\n\n      return mf, errors","1226091b":"def display_audio(title, audio):\n    plt.title(title)\n    plt.imshow(torch.log10(T.MelSpectrogram(n_mels=80)(audio) + 1e-6)[0])\n    plt.show()\n    display(Audio(audio, rate=16000))","f0c270a7":"idxs = train_dataset[0]['data'].to('cpu')\n\nconverter = MidiToPerformanceConverter()\nmf = converter.idxs_to_midi(idxs)[0]\nmidi_filename = write_midi(mf)\nwav_filename = midi_to_wav(midi_filename)\nmidi_to_wav_audio = torchaudio.load(wav_filename)\nmidi_to_wav_audio = midi_to_wav_audio[0][:1, :16000*10]\n\ndisplay_audio('performance events to midi to wav', midi_to_wav_audio)","4cafc6e3":"class PerformanceRNNComposerConditioning(nn.Module):\n    def __init__(self, num_embedding, num_composers, composer_embed_dim=2, hidden_dim=128, num_layers=3):\n        super(PerformanceRNNComposerConditioning, self).__init__()\n        self.composer_embedding = nn.Embedding(num_embeddings=num_composers, embedding_dim=composer_embed_dim)\n        self.input_embedding = nn.Embedding(num_embeddings=num_embedding, embedding_dim=hidden_dim)\n        self.lstm = nn.LSTM(input_size=hidden_dim+composer_embed_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n        self.output_linear = nn.Linear(in_features=hidden_dim, out_features=num_embedding)\n\n    # x is (batch, seq)\n    # composers is (batch)\n    # returns (batch, seq, num_embedding)\n    def forward(\n        self,\n        input: Tuple[torch.Tensor, torch.Tensor],\n        state: Optional[Tuple[torch.Tensor, torch.Tensor]]=None,\n    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n        (x, c) = input\n        x = self.input_embedding(x) # batch, seq, hidden\n        batch_size, seq_len, _ = x.shape\n        c = self.composer_embedding(c.unsqueeze(-1)) # batch, 1, c_embed\n        c = c.expand(batch_size, seq_len, c.shape[-1])\n        x = torch.cat([x, c], dim=2) # batch, seq, hidden+c_embed\n        x, state = self.lstm(x, state) # (batch, seq, hidden)\n        x = self.output_linear(x)\n        return x, state\n\n    # prime is (batch, prime_seq)\n    # composer is (batch,)\n    # returns (batch, prime_seq + steps)\n    def forward_seq(self, prime, composer, steps, greedy):\n        if greedy:\n            sample = lambda x: torch.argmax(x, axis=2)\n        else:\n            sample = lambda x: torch.distributions.Categorical(torch.softmax(x, axis=2)).sample()\n\n        x, state = self.forward((prime, composer))\n        output = sample(x)\n        for _ in range(steps - 1):\n            x, state = self.forward((output[:, -1:], composer), state)\n            x = sample(x)\n            output = torch.cat([output, x], axis=1)\n\n        return torch.cat([prime, output], axis=1)\n    \n    \ndef prepare_batch(batch):\n    data = batch['data']\n    return (data[..., :-1], batch['canonical_composer']), data[..., 1:]\n\n\ncel = torch.nn.CrossEntropyLoss()\ndef lossfn(prediction, y):\n    return cel(prediction.transpose(1, 2), y)\n\n\nDEVICE = 'cuda'\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\nmodel = PerformanceRNNComposerConditioning(converter.num_channels + 1, 10, 2, hidden_dim=512).to(DEVICE)\n\n(x, c), y = prepare_batch(next(iter(train_dataloader)))\nx = x.to(DEVICE)\ny = y.to(DEVICE)\nprediction = model((x, c))[0]\nloss = lossfn(prediction, y)\n\ny_seq = model.forward_seq(x[:, :1], c, 10, greedy=False)\ny_seq_greedy = model.forward_seq(x[:, :1], c, 10, greedy=True)\nx.shape, y.shape, prediction.shape, loss.shape, y_seq.shape, y_seq_greedy.shape","2cecc833":"def train(\n    model,\n    optimizer,\n    lossfn,\n    train_dataloader,\n    val_dataloader,\n    train_cb,\n    val_cb,\n    prepare_batch,\n    num_epochs,\n):\n    for epoch in range(num_epochs):\n        print(f'## EPOCH {epoch} - train - {len(train_dataloader)} batches ##')\n        model.train()\n        for i, batch in enumerate(train_dataloader):\n            x, y = prepare_batch(batch)\n            prediction = model(x)[0]\n            loss = lossfn(prediction, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_cb(i, prediction, y, loss)\n\n        print(f'## EPOCH {epoch} - validation - {len(val_dataloader)} batches ##')\n        model.eval()\n        with torch.no_grad():\n            for i, batch in enumerate(val_dataloader):\n                x, y = prepare_batch(batch)\n                prediction = model(x)[0]\n                loss = lossfn(prediction, y)\n                val_cb(i, prediction, y, loss)","5ca95bc1":"def make_cb(dataloader):\n    losses = []\n    acc = []\n    num_batches = len(dataloader)\n    def cb(i, prediction, y, loss):\n        losses.append(loss)\n        acc.append((torch.argmax(prediction, -1) == y).float().mean())\n        #print(i, end=' ' if i < num_batches - 1 else '\\n')\n    return losses, acc, cb\n\n\n# linearly interpolate b to match a's length, for displaying val\/train loss\ndef interpolate(a, b):\n    return F.interpolate(\n        torch.tensor(b)[None, None],\n        len(a),\n        mode='linear',\n        align_corners=True,\n    )[0, 0]","7c4c6115":"num_composers = len(get_df()['canonical_composer'].cat.codes.unique())\nmodel = PerformanceRNNComposerConditioning(converter.num_channels + 1, num_composers=num_composers, composer_embed_dim=16, hidden_dim=512).to('cuda')\noptimizer = torch.optim.Adam(model.parameters())\n\ntrain_losses, train_acc, train_cb = make_cb(train_dataloader)\nval_losses, val_acc, val_cb = make_cb(val_dataloader)","76677317":"train(\n    model,\n    optimizer,\n    lossfn,\n    train_dataloader,\n    val_dataloader,\n    train_cb,\n    val_cb,\n    prepare_batch,\n    num_epochs=10,\n)","84f6c1c9":"plt.figure(figsize=(12, 4)) \nplt.subplot(1, 2, 1)\nplt.title('loss')\nplt.plot(train_losses, label='train')\nplt.plot(interpolate(train_losses, val_losses), label='validation')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.title('accuracy')\nplt.ylim(0, 1)\nplt.plot(train_acc, label='train')\nplt.plot(interpolate(train_acc, val_acc), label='validation')\nplt.legend()\nplt.show()","a33bc957":"def f(x, y, n):\n    fig, ax = plt.subplots()\n    ax.scatter(x, y)\n    for i, txt in enumerate(n):\n        ax.annotate(txt, (x[i], y[i]))\n\ncomposers = get_df()['canonical_composer']\nnames = list(composers.dtype.categories)\nidxs = list(composers.cat.codes.value_counts().head(15).index)\ncomposer_embeddings = model.composer_embedding(torch.tensor(idxs).long())\nx = composer_embeddings[:, 0]\ny = composer_embeddings[:, 1]\nn = []\nfor i in idxs:\n    n.append(names[i])\n\nf(x, y, n)","b88579aa":"def convert_and_display(title, idxs):\n    mf = converter.idxs_to_midi(idxs.to('cpu'))[0]\n    midi_filename = write_midi(mf)\n    wav_filename = midi_to_wav(midi_filename)\n    midi_to_wav_audio = torchaudio.load(wav_filename)\n    midi_to_wav_audio = midi_to_wav_audio[0][:1]\n    display_audio(title, midi_to_wav_audio)\n    plt.title('events')\n    plt.plot(idxs.squeeze().to('cpu'))\n    plt.show()\n\n\n# first 256 events is the first ~20 secs\nidxs = train_dataset.preloaded_data[0][:256]\ncomposers = train_dataset.preloaded_features[0]['canonical_composer']\n\n# ground truth\nconvert_and_display('ground truth', idxs)\n\n# greedy prediction\nprime = idxs[:64] # first 64 events is the first ~5 secs\nprime = prime[None] # add batch\nprediction = model.forward_seq(prime, composers, steps=256 - 64, greedy=True)\nconvert_and_display('greedy prediction', prediction[0])\n\n# sampling prediction\nprediction = model.forward_seq(prime, composers, steps=256 - 64, greedy=False)\nconvert_and_display('sample prediction', prediction[0])","e0799bdd":"model.eval()\nmodel.to('cpu')\ntorch.jit.script(model).save('performance_rnn.pt')\nmodel_loaded = torch.jit.load('performance_rnn.pt')\nmodel_loaded((torch.zeros(16, 1).long(), torch.zeros(16).long()))[0].shape","5022b5d5":"# Export model","68e21297":"## Data augmentation\n\nWe can expand the dataset by adding some augmentations, including random cropping, increasing\/decreasing pitch, and tweaking time delays.","eceb5fd8":"# Train\n\nTrain the model using the performance data loaded above, recording train\/validation performance.","8e0643c0":"# Inference\n\nNow that we've trained a model, use it to generate music! One way to do this is to pass some events from an existing piece into the model and ask it to predict subsequent events. We can then compare the ground truth to the generated version.","1e1a99f8":"## Convert data into MIDI and WAV files\n\nVisualize and listen to performance tensors by converting them in MIDI\/WAV. This code will be used to convert the output of a model that is trained to output the perforamnce representation into audible music.","d2e0ed0f":"# Model\n\nCreate the PerformanceRNN model, which embedds performance events into an internal embedding, learns sequences autoregressively, and then projects its internal representation back to a distribution over possible events. \n\nThe model has a forward method to predict the next event given a sequence, as well as a forward_seq method that efficiently predicts N subsequent elements into the future by feeding its own predicted back into itself in a loop.","305b8dc5":"# Data\n\nCreate a dataframe and pytorch dataset that to load the performance data for training. This combines the Maestro dataset with another dataset that contains the preprocessed performance files."}}