{"cell_type":{"f1ae5675":"code","8ee090e9":"code","58866a52":"code","b230e501":"code","31e302dd":"code","540beab3":"code","cf46b69c":"code","ee7ea4d1":"code","9058bdcc":"code","2b22b4a0":"code","85189cb8":"code","cbdd88ec":"code","8b56ab1f":"code","ea1c108d":"code","28f03559":"code","6273f72c":"code","e8a7302d":"code","aacb2a8f":"code","4825c487":"code","db4e18c8":"code","5b004201":"code","cb141326":"code","5ab273df":"code","e5c65d5b":"code","ccd38381":"code","64739b26":"code","cb6bfeec":"code","ba513cb5":"code","d018ab6f":"code","2e031d61":"code","4c9bf908":"code","b0e66e10":"code","82d16696":"code","f999d3c5":"code","e523d861":"code","e7fc6e19":"code","476e52c0":"code","6182e39d":"code","5c16dd6b":"code","c73ddd76":"code","c7791544":"code","037ed0e0":"code","24ef558e":"code","63f33d1f":"code","9b85dd46":"code","bebd1761":"code","701d78a7":"code","5e0ab86d":"code","83759501":"code","a5a2e106":"code","9a5724e1":"code","3a278c59":"code","01fcc0d9":"code","da25a959":"code","ef994617":"code","556fdbd5":"code","5df3d481":"code","8edc7872":"code","b714dcb9":"code","345bbced":"code","b02a23fa":"code","195232e4":"code","2718d58b":"code","de141392":"code","9ee3ce17":"code","414598ff":"code","9c27d4ac":"code","b5ed02cc":"code","8e25b1ad":"code","d917093a":"code","7fb4a765":"code","156cb8af":"code","bc40d22a":"code","258100cd":"code","3311fbc0":"code","e53e8fc8":"code","c88c8d1b":"code","a2f9fa8a":"code","fd53666e":"code","651a95ef":"code","0c463cf7":"code","3e516df1":"code","1995ad4c":"code","202bdd3b":"code","40a3c197":"code","1324eb65":"code","87bd52fb":"code","3791bff8":"code","e7add285":"code","5a7854c3":"code","ec9019fc":"code","ac91d585":"code","6a1b2b32":"code","5a3004a1":"code","ecd59d7f":"code","a1958837":"code","dc5de51a":"code","7c086811":"code","8aa6bfe8":"code","0a801317":"code","3bd6835e":"code","ea235685":"code","98b95019":"code","4620a2f1":"code","72146ea0":"markdown","71330f91":"markdown","ebdb6743":"markdown","d57afc1e":"markdown","6abac34d":"markdown","743d5b9e":"markdown","5b484bd8":"markdown","5a6dca9c":"markdown","8c950fac":"markdown","64904e62":"markdown","ca61ba00":"markdown","9054eb21":"markdown","c2f7d651":"markdown","37dc9de3":"markdown","9ba5d2ad":"markdown","120afbf6":"markdown","3815af21":"markdown","03538f48":"markdown","1f56eb61":"markdown","ab5336c6":"markdown","851401e2":"markdown","20144d12":"markdown","188f0c07":"markdown","8cccd455":"markdown","4b1c8fd6":"markdown","7f0c8984":"markdown","85437e11":"markdown","c0518683":"markdown","f45caa7b":"markdown","e789dc44":"markdown","7aec4a78":"markdown","f3559e95":"markdown","75775208":"markdown","8c922ba8":"markdown","738557ca":"markdown","0fffd206":"markdown","548665d1":"markdown","a081e812":"markdown","45947c9f":"markdown","049c3fce":"markdown","2f8d70f4":"markdown"},"source":{"f1ae5675":"import numpy as np \nimport pandas as pd\nimport os\nimport glob\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport statistics as stat\nfrom wordcloud import WordCloud, STOPWORDS\nfrom plotly.subplots import make_subplots\nimport folium\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\n%matplotlib inline","8ee090e9":"districts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")","58866a52":"#we define a couple of scripts that we shall implement\ndef null_values(df):\n    \n    #get the number of null values in the system\n    try:\n        print(\" #### Calculating Missing Values ##### \\n\")\n        for i in tq(range(1),desc=\"Fetching data\"):\n        \n            \n            \n            sumofNull=df.isna().sum()\n            percentage=sumofNull\/len(df)*100\n        print(\" #### Done #### \\n \")\n            \n            \n        \n        print(\"#### Creating data frame #### \\n \")\n        \n        for i in tq(range(1),desc=\"Creating DataFrame\"):\n            valuesdf=pd.DataFrame(data=[sumofNull,percentage])\n            valuesdf=valuesdf.T\n            valuesdf.columns=[\"Total Missing\",\"Percentage Missing\"]\n        print(\" #### Done #### \")\n        \n\n        return valuesdf\n            \n    except Exception as e:\n        \n        print(\" !!! Error File Not Found  !!!!! \\n\")\n        print(\" !!! Program Failed !!!!! \\n\")\n        \n        print(\"Safely exiting the program\")\n        sys.exit(1)\n\ndef dropDuplicates(data):\n        try:\n            \n            for i in tq(range(1),desc=\"Detecting duplicates\"):\n                data=data.drop_duplicates()\n            for i in tq(range(1),desc=\"Report on Duplicates \"):\n                dupCount=len(data)-len(data.drop_duplicates())\n                \n            print (\"There are {} duplicates in the dataset\\n\".format(dupCount))\n            logging.info(\"Number of duplicates in the datset are {} \".format(dupCount))\n\n            \n\n            return data \n        except Exception as e:\n\n            print(\"The following error occured {} \".format(e.__class__))","b230e501":"districts_info.info()","31e302dd":"import sys\nfrom tqdm.notebook import tqdm_notebook as tq\nnull_values(districts_info)","540beab3":"#we get the state abbrevations to use in potting graphs\nstate_abb = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_info['state_abb'] = districts_info['state'].map(state_abb)","cf46b69c":"def count_plot(data,colr,title):\n    plt.figure(figsize=(10,8))\n    ax=sns.countplot(x=data,palette=colr,order=data.value_counts().index)\n    plt.xticks(rotation=90)\n    plt.title(title)\n    for p in ax.patches:\n        ax.text (p.get_x() + p.get_width()  \/ 2,p.get_height()+ 0.75,p.get_height(), fontsize = 11)\n#         ax.text('%{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))","ee7ea4d1":"import plotly.graph_objects as go\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Count of districts in the available States\",\n    title_font = dict(\n            family = \"monospace\",\n            size = 25,\n            color = \"black\"\n            ),\n    geo_scope = 'usa'\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations = districts_info['state_abb'].value_counts().to_frame().reset_index()['index'],\n        zmax = 1,\n        z = districts_info['state_abb'].value_counts().to_frame().reset_index()['state_abb'],\n        locationmode = 'USA-states',\n        marker_line_color = 'white',\n        geo = 'geo',\n        colorscale = \"cividis\", \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","9058bdcc":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncount_plot(districts_info['state'],'RdYlGn',\"State representation\")","2b22b4a0":"plt.figure(figsize = (15, 8))\nsns.set_style(\"white\")\na = sns.barplot(data = districts_info['state'].value_counts().reset_index(), x = 'state', y = 'index', color = '#90afc5')\nplt.xticks([])\nplt.yticks(fontsize = 14, color = '#283655')\nplt.ylabel('')\nplt.xlabel('')\n\na.spines['left'].set_linewidth(1.5)\nfor w in ['right', 'top', 'bottom']:\n    a.spines[w].set_visible(False)\n    \nfor p in a.patches:\n    width = p.get_width()\n    plt.text(0.5 + width, p.get_y() + 0.55 * p.get_height(), f'{int(width)}',\n             ha = 'center', va = 'center',  fontsize = 15, color = '#283655')\n\nplt.show()","85189cb8":"count_plot(districts_info['locale'],'Blues','locale representation') ","cbdd88ec":"distcrict_copy=districts_info.copy()\ndistricts_info.dropna(inplace=True)\ndistricts_info['pct_black\/hispanic']=districts_info['pct_black\/hispanic'].apply(lambda x :float(x.split(',')[0][1:])+0.1)\n\ndistricts_info['pct_free\/reduced']=districts_info['pct_free\/reduced'].apply(lambda x :float(x.split(',')[0][1:])+0.1)\ndistricts_info=districts_info.reset_index()\ndistricts_info.drop(labels='index',inplace=True,axis=1)\ndistricts_info","8b56ab1f":"districts_info['pp_total_raw']=districts_info['pp_total_raw'].apply(lambda x :float(x.split(',')[0][1:])+1000)\ndistricts_info['county_connections_ratio']=districts_info['county_connections_ratio'].apply(lambda x: float(x.split(',')[0][1:])+0.1)\n\ndistricts_info","ea1c108d":"import numpy as np\nstate_pct=districts_info.groupby('state').agg({'pct_black\/hispanic':np.mean,'pct_free\/reduced':np.mean,'pp_total_raw':np.mean})\nstate_pct=state_pct.reset_index()","28f03559":"#is there a relationship between the two ratios?\nsns.heatmap(state_pct.corr(),annot=True)","6273f72c":"# sample funnel graph showing the percentage of black\/hispanic students in each state\nimport plotly.express as px\nfig = px.funnel(state_pct, x='pct_black\/hispanic', y='state')\nfig.show()","e8a7302d":"fig = px.bar_polar(state_pct, r=\"pct_black\/hispanic\", theta=\"state\", color=\"pct_free\/reduced\", template=\"plotly_dark\",\n            color_discrete_sequence= px.colors.sequential.Plasma_r)\nfig.show()","aacb2a8f":"fig = px.bar_polar(state_pct, r=\"pp_total_raw\", theta=\"state\", template=\"plotly_dark\",\n            color_discrete_sequence= px.colors.sequential.Plasma_r)\nfig.show()","4825c487":"locale_df=districts_info.groupby('locale').agg({'pct_black\/hispanic':np.mean,'pct_free\/reduced':np.mean})\nlocale_df=locale_df.reset_index()\nplt.figure(figsize=(15,9))\nsns.barplot(x=\"locale\", y=\"pct_black\/hispanic\", data=locale_df,\n                 palette=\"Blues_d\")\nplt.title(\"Percentage of blacks and hispanic in the locales\")\nplt.xticks(rotation=90)","db4e18c8":"locale_df.head()","5b004201":"product_info = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\nproduct_info.head()","cb141326":"cloud = WordCloud(width=1440, height=1080,stopwords={'nan'}).generate(\" \".join(product_info['Product Name'].astype(str)))\nplt.figure(figsize=(15, 10))\nplt.imshow(cloud)\nplt.axis('off')","5ab273df":"# fill missing with ffill method for columns (pct_free\/reduced , pp_total_raw )\n\ndef fix_missing_ffill(df, col):\n    df[col] = df[col].fillna(method='ffill')\n    return df[col]\n# fill missing with ffill method for columns (pct_free\/reduced , pp_total_raw )\n\ndef fix_missing_ffill(df, col):\n    df[col] = df[col].fillna(method='ffill')\n    return df[col]","e5c65d5b":"product_info.info()","ccd38381":"product_info.isna().sum()","64739b26":"msno.bar(product_info,color='#924893', sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","cb6bfeec":"# we can use fowward fil of back file to fill the 20 missing values in sector , primary function main and sub function \nproduct_info['Sector(s)'] = fix_missing_ffill(product_info, 'Sector(s)')\nproduct_info['Primary Essential Function'] = fix_missing_ffill(product_info,'Primary Essential Function')","ba513cb5":"product_info['primary_function_main'] = product_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproduct_info['primary_function_sub'] = product_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproduct_info['primary_function_sub'] = product_info['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\nproduct_info.drop(\"Primary Essential Function\", axis=1, inplace=True)","d018ab6f":"product_info","2e031d61":"product_info.isna().sum()","4c9bf908":"product_info = product_info.dropna()","b0e66e10":"product_info","82d16696":"def plot_count(df:pd.DataFrame, column:str,title:str) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.countplot(data=df, x=column) \n    plt.xticks(rotation=75, fontsize=14)\n    plt.title(title,font=\"Serif\", size=20)\n    plt.show()\ndef group_donut(grouped_data,title: str):\n    grouped_data.plot.pie(subplots=True,figsize=(18, 9),autopct=\"%.1f%%\",pctdistance=0.85)\n    # add a circle at the center to transform it in a donut chart\n    my_circle=plt.Circle( (0,0), 0.7, color='white')\n    p=plt.gcf()\n    p.gca().add_artist(my_circle)\n    plt.title(title,font=\"Serif\", size=20)\n    plt.show()\ndef bar_p(df:pd.DataFrame, column:str,title:str):\n    plt.figure(figsize=(18, 9))\n    sns.countplot(y=column, data=product_info, order=df[column].value_counts().head(10).index,color = \"#a265b8\")\n    plt.title(title,font=\"Serif\", size=20)\n    plt.show()","f999d3c5":"product_info['Provider\/Company Name'].value_counts()","e523d861":"bar_p(product_info,'Provider\/Company Name','Showing 10 providers')","e7fc6e19":"Data = ['Google LLC','Houghton Mifflin Harcourt','Microsoft']\ntop3 = product_info.loc[product_info['Provider\/Company Name'].isin(Data)]\ntop3.head()","476e52c0":"top = top3.groupby(['Provider\/Company Name'])['Product Name'].value_counts().groupby(level=0).head(10)\ntop","6182e39d":"c1=c2=c3=0\nfor s in product_info[\"Sector(s)\"]:\n    if(not pd.isnull(s)):\n        s = s.split(\";\")\n        for i in range(len(s)):\n            sub = s[i].strip()\n            if(sub == 'PreK-12'): c1+=1\n            if(sub == 'Higher Ed'): c2+=1\n            if(sub == 'Corporate'): c3+=1\n\nfig, ax  = plt.subplots(figsize=(16, 8))\nfig.suptitle('Sector Distribution', size = 30, font=\"Serif\")\nexplode = (0.05, 0.05, 0.05)\nlabels = ['PreK-12','Higher Ed','Corporate']\nsizes = [c1,c2, c3]\nax.pie(sizes, explode=explode,startangle=60, labels=labels,autopct='%1.2f%%', pctdistance=0.7, colors=[\"#ff228a\",\"#20b1fd\",\"#ffb703\"])\nax.add_artist(plt.Circle((0,0),0.4,fc='white'))\nplt.show()","5c16dd6b":"c1=c2=c3=0\n\nfor s in product_info[\"primary_function_main\"]:\n    if(not pd.isnull(s)):\n        c1 += s.count(\"CM\")\n        c2 += s.count(\"LC\")\n        c3 += s.count(\"SDO\")\n\nfig, ax  = plt.subplots(figsize=(16, 8))\nfig.suptitle('Primary Essential Function', size = 20, font=\"Serif\")\nexplode = (0.05, 0.05, 0.05)\nlabels = ['CM','LC','SDO']\nsizes = [c1, c2, c3]\nax.pie(sizes, explode=explode,startangle=60, labels=labels,autopct='%1.2f%%', pctdistance=0.7, colors=[\"#18ff9f\",\"#2cfbff\",\"#ffb703\"])\nax.add_artist(plt.Circle((0,0),0.4,fc='white'))\nplt.show()","c73ddd76":"product_info['primary_function_sub'].value_counts()","c7791544":"bar_p(product_info,'primary_function_sub','plot showing top 10 used sub functions')","037ed0e0":"path = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'\nfiles = glob.glob(os.path.join(path, \"*.csv\"))","24ef558e":"PATH = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' \ndistricts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    temp.append(df)\n    \n    \nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)\n","63f33d1f":"engagement.shape","9b85dd46":"engagement.isna().sum()","bebd1761":"engagement.head(10)","701d78a7":"percent_missing_val_engagement = (engagement.isnull().sum().sort_values(ascending = False)\/len(engagement))*100\npercent_missing_val_engagement","5e0ab86d":"msno.bar(engagement, color='#25c28b', sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.title(\"Number of non-null entries in engagement dataset\", size=20)\nplt.show()","83759501":"engagement.info()","a5a2e106":"convert_dict = {'district_id': 'int64'\n               }\nengagement = engagement.astype(convert_dict)\nengagement['time'] = pd.to_datetime(engagement['time'])","9a5724e1":"engagement.info()","3a278c59":"district = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\nmapping_state = dict(district[['district_id', 'state']].values)\nmapping_locale = dict(district[['district_id', 'locale']].values)\nengagement['state'] = engagement['district_id'].map(mapping_state)\nengagement['locale'] = engagement['district_id'].map(mapping_locale)\nengagement.head(10)","01fcc0d9":"engagement_state = engagement[['pct_access', 'engagement_index', 'state']]\nengagement_locale = engagement[['pct_access', 'engagement_index', 'locale']]","da25a959":"plt.figure(figsize=(15, 10))\nplt.ticklabel_format(style='plain')\nsns.countplot(y=\"state\",data=engagement_state,order=engagement_state.state.value_counts().index,palette=\"rocket_r\",linewidth=4)\nplt.title(\"Statewise Digital connectivity distribution in 2020\",font=\"Georgia\", size=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel('Count', fontsize=20)\nplt.ylabel('State', fontsize=20)\nplt.show()","ef994617":"plt.figure(figsize=(10, 6))\nplt.ticklabel_format(style='plain')\nsns.countplot(y=\"locale\",data=engagement_locale,order=engagement_locale.locale.value_counts().index,palette=\"mako\",linewidth=3)\nplt.title(\"Locale-wise Digital connectivity distribution in 2020\",font=\"Georgia\", size=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel('Count', fontsize=20)\nplt.ylabel('Locale', fontsize=20)\nplt.show()","556fdbd5":"fig, ax  = plt.subplots(figsize=(15, 10))\nexplode = (0.04, 0.04, 0.04, 0.04)\nlabels = list(engagement_locale.locale.value_counts().index)\nsizes = engagement_locale.locale.value_counts().values\npatches, texts, autotexts = ax.pie(sizes, explode=explode, startangle=60, labels=labels, autopct='%1.0f%%', pctdistance=0.7, colors=[\"#367382\",\"#3998af\",\"#16b5dc\",\"#a8c2c9\"])\ntexts[0].set_fontsize(20)\ntexts[1].set_fontsize(20)\ntexts[2].set_fontsize(20)\ntexts[3].set_fontsize(20)\nautotexts[0].set_fontsize(20)\nautotexts[1].set_fontsize(20)\nautotexts[2].set_fontsize(20)\nautotexts[3].set_fontsize(20)\nax.add_artist(plt.Circle((0,0),0.6,fc='white'))\nfont = {'fontname':'Georgia'}\nplt.title('Locale-wise Digital connectivity distribution in 2020', fontsize = 20, **font)\nplt.show()\n\n\n","5df3d481":"engagement.time.nunique()","8edc7872":"plt.figure(figsize=(15,5))\n\nsns.histplot(engagement.groupby('district_id').time.nunique(), bins=30, color = '#b3a42f')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlabel('Number of days', fontsize=20)\nplt.ylabel('Number of district', fontsize=20)\nplt.title('Districts with unique days of engagement ', fontsize = 20)\n","b714dcb9":"lp_id_virtual = product_info[product_info.primary_function_sub == 'Virtual Classroom']['LP ID'].unique()\nplt.rcParams.update({'font.size': 14,})\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\n\nfor product_id in lp_id_virtual:\n    dummy = engagement[engagement.lp_id == product_id].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=dummy.time, y=dummy.pct_access, label=product_info[product_info['LP ID'] == product_id]['Product Name'].values[0])\nplt.legend()\nplt.title('Variation of pct_access over time for Virtual Classroom products', fontsize = 20)\nplt.show()","345bbced":"engagement['weekday'] = pd.DatetimeIndex(engagement['time']).weekday\nengagement_updated = engagement[engagement.weekday < 5]","b02a23fa":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\n\nfor product_id in lp_id_virtual:\n    dummy = engagement_updated[engagement_updated.lp_id == product_id].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=dummy.time, y=dummy.pct_access, label=product_info[product_info['LP ID'] == product_id]['Product Name'].values[0])\nplt.legend()\nplt.title('Variation of pct_access over time for Virtual Classroom products', fontsize = 20)\nplt.show()","195232e4":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor product_id in lp_id_virtual:\n    dummy = engagement_updated[engagement_updated.lp_id == product_id].groupby('time').engagement_index.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=dummy.time, y=dummy.engagement_index, label=product_info[product_info['LP ID'] == product_id]['Product Name'].values[0])\nplt.legend()\nplt.title('Variation of engagement_index over time for Virtual Classroom products', fontsize = 20)\nplt.show()","2718d58b":"engagement.lp_id.head()","de141392":"lp_id_digital = product_info[product_info.primary_function_sub == 'Digital Learning Platforms']['LP ID'].unique()","9ee3ce17":"len(lp_id_digital)","414598ff":"lp_id_digital = [36692, 92993, 71279, 25559, 64998, 61441]","9c27d4ac":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\n\nfor product_id in lp_id_digital:\n    dummy = engagement_updated[engagement_updated.lp_id == product_id].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=dummy.time, y=dummy.pct_access, label=product_info[product_info['LP ID'] == product_id]['Product Name'].values[0])\nplt.legend()\nplt.title('Variation of pct_access over time for TOP 6 most accessed Digital Learning platforms', fontsize = 20)\nplt.show()","b5ed02cc":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\n\nfor product_id in lp_id_digital:\n    dummy = engagement_updated[engagement_updated.lp_id == product_id].groupby('time').engagement_index.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=dummy.time, y=dummy.engagement_index, label=product_info[product_info['LP ID'] == product_id]['Product Name'].values[0])\nplt.legend()\nplt.title('Variation of engagement_index over time for TOP 6 most accessed Digital Learning platforms', fontsize = 20)\nplt.show()","8e25b1ad":"!pip install causalnex","d917093a":"! apt install python3-dev graphviz libgraphviz-dev pkg-config -y","7fb4a765":"!pip install pygraphviz","156cb8af":"del districts_info\ndel engagement","bc40d22a":"districts_info = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\npath = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n#     if df.time.nunique() == 366:\n    temp.append(df)\n\nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)","258100cd":"districts_info.dropna(inplace=True)","3311fbc0":"districts_info_edited = districts_info.copy()\n\ndistricts_info_edited['pct_black\/hispanic'] = districts_info['pct_black\/hispanic'].apply(lambda x: (x.replace('[', '')).split(','))\ndistricts_info_edited['pct_free\/reduced'] = districts_info['pct_free\/reduced'].apply(lambda x: (x.replace('[', '')).split(','))\ndistricts_info_edited['pp_total_raw'] = districts_info['pp_total_raw'].apply(lambda x: (x.replace('[', '')).split(','))\ndistricts_info_edited.drop(columns=['county_connections_ratio'],inplace=True)\nfor i in ['pct_black\/hispanic','pct_free\/reduced','pp_total_raw']:\n    districts_info_edited[i] = districts_info_edited[i].apply(lambda x: (float(x[0])+float(x[1]))\/2)\ndistricts_info_edited","e53e8fc8":"df_engagement_district = pd.merge(engagement, districts_info_edited, on='district_id')\ndf_merged = pd.merge(df_engagement_district, product_info, left_on='lp_id',right_on=\"LP ID\")\ndf_merged.drop(columns=['LP ID'],inplace=True)\ndf_merged.head()","c88c8d1b":"del engagement","a2f9fa8a":"df_merged.head()","fd53666e":"df_merged.shape","651a95ef":"df_merged = df_merged.dropna().reset_index(drop=True)","0c463cf7":"df_merged.columns","3e516df1":"causal_data = df_merged[['pct_access', 'engagement_index','locale', \n                         'pct_black\/hispanic', 'pct_free\/reduced',\n                        'pp_total_raw', 'Provider\/Company Name',\n                        'Sector(s)', 'primary_function_main',\n                        'primary_function_sub']].copy()\ncausal_data.head()","1995ad4c":"del df_merged","202bdd3b":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ncausal_data[['locale', 'Provider\/Company Name', 'Sector(s)', 'primary_function_main',\n       'primary_function_sub']] = causal_data[['locale', 'Provider\/Company Name', 'Sector(s)', 'primary_function_main',\n       'primary_function_sub']].apply(le.fit_transform)\ncausal_data.head()","40a3c197":"col_dict = {'pct_black\/hispanic': 'pct_black_hispanic',\n        'pct_free\/reduced': 'pct_free_reduced',\n        'Provider\/Company Name': 'Provider_Company_Name',\n           'Sector(s)': 'Sector'}\ncausal_data.rename(columns=col_dict,\n          inplace=True)\ncausal_data.head()","1324eb65":"causal_data.shape","87bd52fb":"from IPython.display import Image\nfrom causalnex.plots import plot_structure, NODE_STYLE, EDGE_STYLE\nfrom causalnex.structure.notears import from_pandas, from_pandas_lasso","3791bff8":"sm_100000 = from_pandas(causal_data.iloc[:100000,:], w_threshold=0.8)\nsm_100000.remove_edge('primary_function_sub','Sector') \nviz = plot_structure(\n    sm_100000,\n    graph_attributes={\"scale\": \"2.0\", 'size':2.5},\n    all_node_attributes=NODE_STYLE.WEAK,\n    all_edge_attributes=EDGE_STYLE.WEAK)\nImage(viz.draw(format='png'))","e7add285":"locale_map = {'City': 0, 'Rural': 1, 'Suburb': 2, 'Town': 3}\nProvider_map = {' A&E Television Networks, LLC': 0,\n ' Autodesk, Inc': 1,\n ' Tes Global Ltd ': 2,\n 'ABC digital': 3,\n 'ABCya.com, LLC': 4,\n 'Achieve3000': 5,\n 'Actively Learn': 6,\n 'Adobe Inc': 7,\n 'Adobe Inc.': 8,\n 'Age of Learning, Inc ': 9,\n 'Amazon.com, Inc. ': 10,\n 'Amplify Education, Inc.': 11,\n 'Answers': 12,\n 'Association for Supervision and Curriculum Development': 13,\n 'Bartleby': 14,\n 'BetterLesson': 15,\n 'Big Ideas Learning': 16,\n 'Blackboard Inc': 17,\n 'Blindside Networks': 18,\n 'Blooket LLC': 19,\n 'Boom Learning (a dba of Omega Labs Inc.)': 20,\n 'Box': 21,\n 'BrainPOP LLC': 22,\n 'Brainly': 23,\n 'Breakout, Inc': 24,\n 'CK-12 Foundation': 25,\n 'Cable News Network': 26,\n 'Calculator.com': 27,\n 'Calendly': 28,\n 'Canva': 29,\n 'Canvas Talent, Inc. ': 30,\n 'Capstone': 31,\n 'Cengage Learning': 32,\n 'Chegg': 33,\n 'Cisco': 34,\n 'ClassDojo, Inc.': 35,\n 'ClassLink': 36,\n 'Clever': 37,\n 'Code.org': 38,\n 'CommonLit': 39,\n 'Constructive Media': 40,\n 'ContentKeeper Technologies': 41,\n 'CoolMath.com LLC': 42,\n 'Course Hero': 43,\n 'Cult of Pedagogy': 44,\n 'Curiosity Media Inc': 45,\n 'Curriculum Associates': 46,\n 'DeltaMath': 47,\n 'Desmos': 48,\n 'Dictionary.com': 49,\n 'Didax Education': 50,\n 'Discovery Communications': 51,\n 'Discovery Education': 52,\n 'Disney': 53,\n 'DocuSign Inc': 54,\n 'Doodle Ltd': 55,\n 'DotDash': 56,\n 'Dreambox Learning': 57,\n 'Dropbox': 58,\n 'Duolingo': 59,\n 'EBSCO Industries, Inc': 60,\n 'EDpuzzle Inc.': 61,\n 'Edgenuity Inc.': 62,\n 'Edmentum': 63,\n 'Education.com': 64,\n 'Educational Testing Service': 65,\n 'Edulastic': 66,\n 'Ellevation': 67,\n 'Enchanted Learning': 68,\n 'Encyclopaedia Britannica, Inc.': 69,\n 'Enotes.com': 70,\n 'Epic Creations, Inc.': 71,\n 'EqsQuest Ltd': 72,\n 'Eventbrite': 73,\n 'Evite': 74,\n 'ExploreLearning, LLC': 75,\n 'Facebook': 76,\n 'Flipgrid': 77,\n 'Flippity': 78,\n 'Flocabulary': 79,\n 'Formative': 80,\n 'Frontline Education': 81,\n 'Funbrain Holdings, LLC': 82,\n 'Future US Inc': 83,\n 'Gale Cengage': 84,\n 'Generation Genius, Inc.': 85,\n 'GeoGebra': 86,\n 'Gimkit': 87,\n 'GitHub': 88,\n 'GloWorld': 89,\n 'Global Compliance Network Inc': 90,\n 'GoGuardian': 91,\n 'Goodreads': 92,\n 'Google LLC': 93,\n 'Grammarly': 94,\n 'Hapara': 95,\n 'HealthTeacher': 96,\n 'Heinemann, a division of Greenwood Publishing Group LLC': 97,\n 'Hewlett-Packard': 98,\n 'Hobsons': 99,\n 'Hooda Math': 100,\n 'Houghton Mifflin Harcourt': 101,\n 'HowStuffWorks': 102,\n 'Hulu, LLC': 103,\n 'IAC': 104,\n 'ITHAKA': 105,\n 'IXL Learning': 106,\n 'Imagine Easy Solutions': 107,\n 'Imagine Learning': 108,\n 'Infinite Campus': 109,\n 'InnerSloth': 110,\n 'Instagram': 111,\n 'Instructure, Inc. ': 112,\n 'Issuu': 113,\n 'Istation': 114,\n 'Jakub Koziol': 115,\n 'John Wiley and Sons Inco': 116,\n 'KQED': 117,\n 'Kahoot! AS': 118,\n 'Kaleido AI GmbH': 119,\n 'Kami Limited': 120,\n 'Khan Academy': 121,\n 'Lakeshore Learning Materials': 122,\n 'Lazel Inc.': 123,\n 'LearnZillion': 124,\n 'Learning A-Z': 125,\n 'Legends of Learning': 126,\n 'Lexia Learning': 127,\n 'Library of Congress': 128,\n 'LinkedIn': 129,\n 'LitCharts LLC': 130,\n 'LogMeIn': 131,\n 'Loom, Inc': 132,\n 'Lumen Learning': 133,\n 'MIND Research Institute': 134,\n 'MIT Media Lab': 135,\n 'MakeMusic, Inc.': 136,\n 'MarketWatch': 137,\n 'Massachusetts Institute of Technology': 138,\n 'MasteryConnect': 139,\n 'Math Playground': 140,\n 'Math Worksheets 4 Kids': 141,\n 'Mathsisfun.com': 142,\n 'Mathway': 143,\n 'McGraw-Hill PreK-12': 144,\n 'Measurement Incorporated ': 145,\n 'Merriam-Webster': 146,\n 'Michael Dayah': 147,\n 'Microsoft': 148,\n 'Microsoft Education': 149,\n 'MobyMax': 150,\n 'Multiplication.com': 151,\n 'Mystery Science, Inc': 152,\n 'NASA': 153,\n 'National Center for Families Learning': 154,\n 'Nature America, Inc': 155,\n 'Nearpod Inc.': 156,\n 'Netflix': 157,\n 'Neuron Fuel': 158,\n 'New York State Education Department': 159,\n 'Newsela': 160,\n 'Nitrolabs Limited': 161,\n 'NoRedInk': 162,\n 'NoodleTools': 163,\n 'Online Writing Lab': 164,\n 'OverDrive': 165,\n 'PBS': 166,\n 'Padlet': 167,\n 'Pandora Media, LLC': 168,\n 'Panorama Education': 169,\n 'Pear Deck': 170,\n 'Performance Matters': 171,\n 'Pew Research Center ': 172,\n 'Physics Classroom, LLC': 173,\n 'Pixlr': 174,\n 'PowerSchool Group LLC': 175,\n 'Prezi Inc.': 176,\n 'ProQuest': 177,\n 'Purch': 178,\n 'Qualtrics': 179,\n 'Quaver Music': 180,\n 'Quill.org': 181,\n 'Quizizz': 182,\n 'Quizlet': 183,\n 'Quora': 184,\n 'Read Theory': 185,\n 'ReadWorks': 186,\n 'ReadWriteThink.org': 187,\n 'Remind101': 188,\n 'Renaissance Learning': 189,\n 'Renaissance Learning, Inc.': 190,\n 'RoomRecess.com': 191,\n 'SAG-AFTRA Foundation': 192,\n 'SMARTeacher Inc.': 193,\n 'Sandbox Networks': 194,\n 'Savvas Learning Company | Formerly Pearson K12 Learning': 195,\n 'Sched LLC': 196,\n 'Scholastic Inc': 197,\n 'School Loop': 198,\n 'School Specialty Inc': 199,\n 'SchoolTube': 200,\n 'Schoology': 201,\n 'Screencast-O-Matic': 202,\n 'Screencastify, LLC': 203,\n 'Securly Inc ': 204,\n 'Seesaw Learning Inc': 205,\n 'SharpSchool': 206,\n 'Shmoop University, Inc': 207,\n 'Showbie Inc ': 208,\n 'SignUpGenius': 209,\n 'SlidesCarnival': 210,\n 'Snap Inc.': 211,\n 'SoundCloud': 212,\n 'Southern Poverty Law Center': 213,\n 'SparkNotes': 214,\n 'Spotify Ltd': 215,\n 'Spotify USA Inc': 216,\n 'Starfall Education': 217,\n 'Studies Weekly': 218,\n 'Study.com': 219,\n 'StudyPad Inc.': 220,\n 'SurveyMonkey': 221,\n 'TEACHERSPAYTEACHERS': 222,\n 'TED Conferences': 223,\n 'Teach TCI': 224,\n 'TeachMe': 225,\n 'Teaching.com': 226,\n 'Technological Solutions, Inc. (TSI)': 227,\n 'Texthelp, Inc.': 228,\n 'The College Board': 229,\n 'The Common Application, Inc.': 230,\n 'The Internet Archive': 231,\n 'The Math Learning Center': 232,\n 'The New York Times': 233,\n 'The Pennsylvania State Universtity': 234,\n 'The University of Utah ': 235,\n 'The Wikimedia Foundation': 236,\n 'ThingLink': 237,\n 'Time USA, LLC ': 238,\n 'Tools for Schools, Inc. (Book Creator)': 239,\n 'Toy Theater': 240,\n 'TumbleBooks': 241,\n 'Tumblr ': 242,\n 'Turnitin': 243,\n 'TurtleDiary LLC': 244,\n 'TypingClub': 245,\n 'US Geological Survey': 246,\n 'US Holocaust Museum': 247,\n 'United States National Archives': 248,\n 'University of Colorado': 249,\n 'Utah Education Network': 250,\n 'Vector Solutions': 251,\n 'Vespr': 252,\n 'ViewPure': 253,\n 'Vimeo': 254,\n 'Vitzo Ltd': 255,\n 'VocabularySpellingCity': 256,\n 'Vooks, Inc.': 257,\n 'Wakelet': 258,\n 'Washington Post': 259,\n 'WeAreTeachers': 260,\n 'WeVideo, Inc.': 261,\n 'Weebly': 262,\n 'West Corporation': 263,\n 'Whiteboard.fi': 264,\n 'Wistia': 265,\n 'Wix.com, Inc': 266,\n 'WordPress': 267,\n 'WordReference.com': 268,\n 'World Book, Inc': 269,\n 'World Wildlife Fund': 270,\n 'WyzAnt': 271,\n 'XtraMath': 272,\n 'Yegros Educational LLC DBA Conjuguemos': 273,\n 'ZOOM VIDEO COMMUNICATIONS, INC.': 274,\n 'Zearn': 275,\n 'Zendesk': 276,\n 'iCivics Inc': 277,\n 'iHeartRadio': 278,\n 'iStockphoto LP': 279,\n 'mrdonn.org': 280,\n 'musictheory.net': 281,\n 'online-stopwatch.com': 282}\nsector_map = {'Corporate': 0,\n 'Higher Ed; Corporate': 1,\n 'PreK-12': 2,\n 'PreK-12; Higher Ed': 3,\n 'PreK-12; Higher Ed; Corporate': 4}\nprimary_function_main_map = {'CM': 0, 'LC': 1, 'LC\/CM\/SDO': 2, 'SDO': 3}\nprimary_function_sub_map = {'Admissions, Enrollment & Rostering': 0,\n                         'Career Planning & Job Search': 1,\n                         'Classroom Engagement & Instruction': 2,\n                         'Content Creation & Curation': 3,\n                         'Courseware & Textbooks': 4,\n                         'Data, Analytics & Reporting': 5,\n                         'Digital Learning Platforms': 6,\n                         'Environmental, Health & Safety (EHS) Compliance': 7,\n                         'Human Resources': 8,\n                         'Large-Scale & Standardized Testing': 9,\n                         'Learning Management Systems (LMS)': 10,\n                         'Online Course Providers & Technical Skills Development': 11,\n                         'Other': 12,\n                         'School Management Software': 13,\n                         'Sites, Resources & Reference': 14,\n                         'Study Tools': 15,\n                         'Teacher Resources': 16,\n                         'Virtual Classroom': 17}","5a7854c3":"discretised_data = causal_data.iloc[:100000,:].copy()","ec9019fc":"discretised_data[\"locale\"] = discretised_data[\"locale\"].map({y:x for x,y in locale_map.items()})\ndiscretised_data[\"Provider_Company_Name\"] = discretised_data[\"Provider_Company_Name\"].map({y:x for x,y in Provider_map.items()})\ndiscretised_data[\"Sector\"] = discretised_data[\"Sector\"].map({y:x for x,y in sector_map.items()})\ndiscretised_data[\"primary_function_main\"] = discretised_data[\"primary_function_main\"].map({y:x for x,y in primary_function_main_map.items()})\ndiscretised_data[\"primary_function_sub\"] = discretised_data[\"primary_function_sub\"].map({y:x for x,y in primary_function_sub_map.items()})","ac91d585":"data_vals = {col: causal_data[col].unique() for col in causal_data.columns}\nfor i in discretised_data.columns[[0,1,3,4,5]]:\n  map  = {v: 'low' if v <= (discretised_data[str(i)].max()-discretised_data[str(i)].min())\/2\n            else 'high' for v in data_vals[str(i)]}\n  discretised_data[str(i)] = discretised_data[str(i)].map(map)","6a1b2b32":"del causal_data","5a3004a1":"discretised_data.head(30)","ecd59d7f":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\nfrom causalnex.inference import InferenceEngine\nfrom causalnex.evaluation import roc_auc\nfrom causalnex.evaluation import classification_report\nfrom causalnex.network import BayesianNetwork","a1958837":"bn = BayesianNetwork(sm_100000)","dc5de51a":"bn = bn.fit_node_states(discretised_data)","7c086811":"bn = bn.fit_cpds(discretised_data, method=\"BayesianEstimator\", bayes_prior=\"K2\")","8aa6bfe8":"ie1 = InferenceEngine(bn)","0a801317":"print(\"distribution before do\", ie1.query()[\"pct_free_reduced\"])\nprint(\"marginal engagement\", ie1.query()[\"engagement_index\"])\nie1.do_intervention(\"pct_free_reduced\",\n                   {'low': 1.0,\n                    'high': 0.0})\n\nprint(\"distribution after do\", ie1.query()[\"pct_free_reduced\"])\nprint(\"updated marginal engagement\", ie1.query()[\"engagement_index\"])","3bd6835e":"ie2 = InferenceEngine(bn)","ea235685":"print(\"pct_black_hispanic distribution before intervention\", ie2.query()[\"pct_black_hispanic\"])\nprint(\"marginal engagement\", ie2.query()[\"engagement_index\"])\nie2.do_intervention(\"pct_black_hispanic\",\n                   {'low': 0.0,\n                    'high': 1.0})\nprint(\"distribution after do\", ie2.query()[\"pct_black_hispanic\"])\nprint(\"updated marginal engagement\", ie2.query()[\"engagement_index\"])","98b95019":"ie3 = InferenceEngine(bn)","4620a2f1":"print(\"pp_total_raw distribution before intervention\", ie3.query()[\"pp_total_raw\"])\nie3.do_intervention(\"pp_total_raw\",\n                   {'low': 1.0,\n                    'high': 0.0})\nprint(\"distribution after do\", ie3.query()[\"pp_total_raw\"])","72146ea0":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Converting the data type of \"time\" column from \"object\" to \"int64\" <\/span><\/div>","71330f91":"In the original distribution, 55% of the data had low rollout of free food programs. We include an intervention to make this 100% low rollout of free food programs.","ebdb6743":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">The below bar graph shows the number of non null entries in each column for the \"engagement\" dataset. <\/span><\/div>","d57afc1e":"### To keep track of the file names as it is the only reference to the district (The 4-digit file name represents district_id which can be used to link to district information in district_info.csv), we have to create a list of dataframes with the filenames in a \"district_id\" column and then we can concatenate the items of the list into one dataframe.\n\n### Here, we have used list comprehension to make a list of dataframes and then used concat function to combine the list items into one dataframe.","6abac34d":"## Loading the dataset and reading its contents\nLet's load the district data.. ","743d5b9e":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Mapping the district_id to get associated state and locale and then adding it to the engagement dataset as individual columns. <\/span><\/div>","5b484bd8":"## Importing libraries that are required for this Analysis <\/span><\/div>","5a6dca9c":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">We see that with the engagement dataset containing weekends as well as weekdays data, the graph shows variation like the above with ripples. Since, people usually don't like to work on weekends which can be substantiated by the above graph, we can remove weekend data from our dataset.<\/span><\/div>","8c950fac":"If the number of students were black or hispanic is 100% low as opposed to 18% from the original distribution, the level of engagement would increase by five percent.","64904e62":"### Discretize the data for input in Bayesian model","ca61ba00":"# Data we are using\nWe are using data provided in this kaggle competition providing a set of daily edtech engagement data from over 200 school districts in 2020, and we will also leverage other publicly available data sources in your analysis. Initial provided data includes three basic sets of files:\n\n\n\n1.   The engagement_ data folder is based on LearnPlatform\u2019s Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district.\n2.   The products_info.csv file includes information about the characteristics of the top 372 products with most users in 2020.\n3. The districts_info.csv file includes information about the characteristics of school districts, including data from NCES and FCC.\nThe definitions of each column in the three data sets are detailed in the README file.\n\n\nIn addition to the files provided, we intend to use other public data sources such as COVID-19 US State Policy database, KIDS Count, and KFF.","9054eb21":"From the quick summary above we can tell we have quite a number of null values. From the description the null values are meant to create a form of anonymity,hence most of the data is structurally missing.\n\nLets calculate the missing values","c2f7d651":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Let's perform exploratory data analysis on the datasets that we have here. But before we do that, we have to change the data type of \"time\" column in \"engagement\" dataset from \"object\" to \"int\" so that we can perform some analysis on the dataset. Moreover, we have to add more columns into \"engagement\" dataset and map the \"district_id\" with \"state\" and \"locale\". <\/span><\/div>","37dc9de3":"A great number of the education insituions are located in the Suburbs but does this result in bettter education How do they compare to the other locales?\n\nWe aggeregate by state and find the percentages features. But first a quick look at the column we do realize that the column is made of intervals.\n\nA quick intro to interval notation:\n\n]a,b[ := {: a<x <b } : open Real interval\n\n[a,b[ := {a<= x <b} : Half-open on the right\n\n]a,b] := {a<xb<=b} : Half-open on the left\n\nwe address this by spliting by the , and taking the values\n\nWe however have to impute the values first before carrying on with the analysis","9ba5d2ad":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Getting a picture of digital connectivity and engagement across states and locale in 2020.<\/span><\/div>","120afbf6":"# <div style=\"text-align: Left\"><span style=\"color:#08838b; font-family:Georgia;\">Exploratory Data Analysis<\/span><\/div>","3815af21":"## Engagement Data\n\n### The engagement data are aggregated at school district level, and each file in the folder engagement_data represents data from one school district. The 4-digit file name represents district_id which can be used to link to district information in district_info.csv. The lp_id can be used to link to product information in product_info.csv.<\/span><\/div>\n\n   ","03538f48":"# <div style=\"text-align: Left\"><span style=\"color:#08838b; font-family:Georgia;\">Questions that tackle the above problem statement <\/span><\/div>\n\n<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">    \n<ul>\n<li>What is the picture of digital connectivity and engagement in 2020?<\/li>\n<li>What is the effect of the COVID-19 pandemic on online and distance learning, and how might this also evolve in the future?<\/li>\n<li>How does student engagement with different types of education technology change over the course of the pandemic?<\/li>\n<li>How does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race\/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?<\/li>\n<li>Do certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?<\/li>\n<\/ul><\/span><\/div>","1f56eb61":"If the rate at which free meals were provided was 100% low as opposed to 55% from the original distribution, the level of engagement would remain the same. This can be explained by the fact that in 2020 most students were learning remotely and had no access to the free meals provided in schools.","ab5336c6":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">From the above plot, we can see that 'Connecticut', 'Utah', 'Illinois', 'Massachusetts' and 'California' are the top five states with maximum number of counts\/records in the engagement dataset.<\/span><\/div>","851401e2":"Unique values","20144d12":"# Basics about the data\n\n*   There are a total of 233 School Districts available within the data, all around USA. A school district is a special-purpose district that operates local public primary and secondary schools in various nations.\n\n*   There are a total of 372 distinct Educational Technology Products, such as tools like Canva, educational apps like Duolingo, reading sites like Goodreads, or social pages like Facebook.\n*    The data was collected between 01.01.2020 (a few months before the pandemic hit) until 31.12.2020. This will give a full year overview of a before the pandemic and after usage.\n* Summing up all data collected for all the district we end up with ~ 22.3M datapoints, which in the Data Science World would be called Big Data.\nFrom the 22M datapoints, around 24% have missing values in the engagement_index feature. Nevertheless, the pct_access feature is fully available.\n* Around half of the pupils have at least one page load on a product and on a given day.\n* There are almost 168 page loads per 1k students on a given product and on a given day. This means activity close to none for some of the pupils.","188f0c07":"We can discover that if the state data is missing then locale and pc_black\/hispanic records are probably also missing. We see that ppl_total_raw has roughly 49% null values which isn't that good for our anlysis","8cccd455":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">From the above histogram, we can see that majority of districts have engagement data for 366 days while some have data for less than 10 days or so.<\/span><\/div>","4b1c8fd6":"Texas has the highest number of black\/hispanic ratio accounting for 60%, Florida comes second with 50%","7f0c8984":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Now the graph shows better variation of pct_access data without ripples which were earlier present because of weekend data in the dataset.<\/span><\/div>","85437e11":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">The above plot tells that, Suburb records the maximum count of digital activity.<\/span><\/div>","c0518683":"## Causal graph","f45caa7b":"# Loading the dataset and reading its contents \n\n### Let's load the engagement data first. Since, the engagement_data folder contains 233 csv files and each file in the folder engagement_data represents data from one school district, we have to join these csv files into one file so that we can work on it.<\/span><\/div>\n\n### The first step is to create a list of all the csv files stored in the folder engagement_data.","e789dc44":"New york seems to have the highest number of expenditure per pupil, we also do notice that the states that had higher pct_ofblacks\/hispanic has a lower expenditure per pupil\n\nCan this be affected by the locale of a district , lets viusialize and see if it affects that","7aec4a78":"### Train Bayesian Model","f3559e95":"Let's load the product data.","75775208":"\n## Product information data\n> The product file products_info.csv includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy.\n\n* LP ID - the unique identifier of the product. URL\n* Product Name\n* Provider\/Company Name\n* Sector(s) - sector of education where the product is used.\n* Primary Essential Function - the basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled.","8c922ba8":"# <div style=\"text-align: Left\"><span style=\"color:#08838b; font-family:Georgia;\">Problem Statement<\/span><\/div>\n<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow.<\/span><\/div>","738557ca":"Since the engagement and district dataset have 80,000,000 points we will randomly select 10% of the data points for graphing the causal graph. To investigate the elements that directly affect the engagement index, we build an initial causal tree using a portion of the data (100,000 records). As the number of data points grows, the graph becomes more stable, but it also demands more computing power. Over the three graphs that were plotted, the factors that directly effect engagement stayed the same. When an intervention is placed on the variable pct free\/reduced, we fit a conditional model and examine the change in the engagement index.sm = construct_structural_model(model_df, tabu_parent_nodes=[\"engagement_index\"]) In [36]:","0fffd206":"Quick review of the polar chart we spot that it doesn't have any clean relationship between pct_black\/hispanic and pct_free\/reduced","548665d1":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Let's understand the variation of \"pct_access\" data and \"engagement_index\" data over time.<\/span><\/div>","a081e812":" We can see the size of dataframe named engagement after combining data from all the 233 csv files. <\/span><\/div>","45947c9f":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">We have 366 unique dates when the data was recorded. <\/span><\/div>","049c3fce":"Connecticut has the most number of district representation with 30 district counts in the dataset closely followed by Utah","2f8d70f4":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">Let's find out the percentage of missing\/null values in out dataset for each dataframe. we will use missingno library to visualize missing values. <\/span><\/div>"}}