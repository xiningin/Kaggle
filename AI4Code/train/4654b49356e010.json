{"cell_type":{"f0c43ed1":"code","ac3b0019":"code","6dc21d24":"code","116ec099":"code","a06c6bf7":"code","6653242b":"code","ff32d52d":"code","acaa542d":"code","8cd46eed":"code","d6bf5212":"code","b543fe4c":"code","ef89108f":"code","783092fa":"code","81efa167":"code","29cd89f2":"code","c88b7ddd":"code","32e233b8":"code","bf40a165":"code","22e617e1":"code","73545955":"code","b6e99a59":"code","352c61e5":"code","946d08ea":"code","69c8a301":"code","872a8ba4":"code","f0b9e8fd":"code","aeeb4071":"code","041ad264":"markdown","22546164":"markdown","ced43ba5":"markdown","a16149f6":"markdown","3924924f":"markdown","3123bfad":"markdown","4d087a1b":"markdown","4336da18":"markdown","353705c0":"markdown","367a478b":"markdown","13ff48e3":"markdown","751f94eb":"markdown","a5448378":"markdown","cdd2683b":"markdown","c7548b84":"markdown"},"source":{"f0c43ed1":"import numpy as np, pandas as pd, matplotlib.pyplot as plt\nimport joblib\nimport optuna\nimport sklearn \n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report","ac3b0019":"# load data\ntrain = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\n# view data\ntrain.head()","6dc21d24":"# remove ID column from set\ntrain = train.iloc[:, 1:]\ntrain.head()","116ec099":"# check for missing values\ntrain.isnull().values.any()","a06c6bf7":"# summary\ntrain.describe()","6653242b":"# dimensions of data set \nprint(train.shape) # 55 columns\n# column names\nprint(train.columns)","ff32d52d":"# create cat, num, and y\nX_cat = train.iloc[:, 10:54].values\nX_num = train.iloc[:, 0:10].values\ny = train.iloc[:, -1].values","acaa542d":"# scale\/standardizing numerical columns\n# scaler object\nscaler = StandardScaler()\n# fit to training data\nscaler.fit(X_num)\n# scale num columns\nX_num = scaler.transform(X_num)\n\n# shape\nprint(f'Categorical Shape: {X_cat.shape}')\nprint(f'Numerical Shape: {X_num.shape}')\nprint(f'Label Shape: {y.shape}')","8cd46eed":"# combine num and cat\nX = np.hstack((X_num, X_cat))\nprint(X.shape)","d6bf5212":"# PCA to find the number of components\npca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA Number of Components for Cumulative Variance')","b543fe4c":"# PCA\npca = PCA(n_components = 10)\npca.fit(X)","ef89108f":"# print components\nprint(pca.components_)\n\n# print variances\nprint(pca.explained_variance_)","783092fa":"# best model \n\nlr_model = LogisticRegression(random_state = 1, \n                              penalty = 'none', \n                              max_iter = 500, \n                              solver = 'saga')\nlr_model.fit(X, y)","81efa167":"%%time\n\n# optuna hyperparameter tuning\ndef objective(trial):\n    max_depth = trial.suggest_int('max_depth', 2, 50)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 32)\n    dt_clf = DecisionTreeClassifier(random_state = 1, max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n    return sklearn.model_selection.cross_val_score(dt_clf, X, y, n_jobs = -1, cv = 10).mean()\n    \ndt_study = optuna.create_study(direction='maximize')\ndt_study.optimize(objective, n_trials=100)\ndt = dt_study.best_trial\nprint('Accuracy: {}'.format(dt.value))\nprint(\"Best hyperparameters: {}\".format(dt.params))","29cd89f2":"# dt best model\ndt_model = DecisionTreeClassifier(random_state = 1, \n                                  max_depth = dt_study.best_trial.params['max_depth'], \n                                  min_samples_leaf = dt_study.best_trial.params['min_samples_leaf'])\ndt_model.fit(X, y)","c88b7ddd":"%%time\n\n# optuna hyperparameter tuning\ndef objective(trial):\n    n_estimators = trial.suggest_int('n_estimators', 100, 150)\n    max_depth = trial.suggest_int('max_depth', 20, 50)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n    rf_clf = RandomForestClassifier(random_state = 1, n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n    return sklearn.model_selection.cross_val_score(rf_clf, X, y, n_jobs = -1, cv = 10).mean()\n    \nrf_study = optuna.create_study(direction='maximize')\nrf_study.optimize(objective, n_trials=20)\nrf = rf_study.best_trial\nprint('Accuracy: {}'.format(rf.value))\nprint(\"Best hyperparameters: {}\".format(rf.params))","32e233b8":"# best model\nrf_model = RandomForestClassifier(random_state = 1, \n                                  n_estimators = rf_study.best_trial.params['n_estimators'], \n                                  max_depth = rf_study.best_trial.params['max_depth'], \n                                  min_samples_leaf = rf_study.best_trial.params['min_samples_leaf'])\n\nrf_model.fit(X, y)","bf40a165":"%%time\n\n# optuna hyperparameter tuning\ndef objective(trial):\n    max_depth = trial.suggest_int('max_depth', 30, 50)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n    tree_clf = ExtraTreesClassifier(random_state = 0, n_estimators = 200, max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n    return sklearn.model_selection.cross_val_score(tree_clf, X, y, n_jobs = -1, cv = 10).mean()\n    \ntree_study = optuna.create_study(direction='maximize')\ntree_study.optimize(objective, n_trials=20)\ntree = tree_study.best_trial\nprint('Accuracy: {}'.format(tree.value))\nprint(\"Best hyperparameters: {}\".format(tree.params))","22e617e1":"# best model\ntree_model = ExtraTreesClassifier(random_state = 1, \n                                  n_estimators = 200, \n                                  max_depth = tree_study.best_trial.params['max_depth'], \n                                  min_samples_leaf = tree_study.best_trial.params['min_samples_leaf'])\n\ntree_model.fit(X, y)","73545955":"%%time\n\n# optuna hyperparameter tuning\ndef objective(trial):\n    max_depth = trial.suggest_int('max_depth', 10, 20)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 15, 20)\n    gradb_clf = GradientBoostingClassifier(random_state = 0, max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n    return sklearn.model_selection.cross_val_score(gradb_clf, X, y, n_jobs = -1, cv = 10).mean()\n    \ngradb_study = optuna.create_study(direction='maximize')\ngradb_study.optimize(objective, n_trials = 5)\ngradb = gradb_study.best_trial\nprint('Accuracy: {}'.format(gradb.value))\nprint(\"Best hyperparameters: {}\".format(gradb.params))","b6e99a59":"%%time\n# best model\ngradb_model = GradientBoostingClassifier(random_state = 0,\n                                         max_depth = gradb_study.best_trial.params['max_depth'], \n                                         min_samples_leaf = gradb_study.best_trial.params['min_samples_leaf'])\n\ngradb_model.fit(X, y)","352c61e5":"%%time\n# xgb classifier\nxgb_clf = XGBClassifier(random_state = 0, max_depth = 10)\nxgb_model = xgb_clf.fit(X, y)\nxgb_model.score(X, y)","946d08ea":"%%time\n\n# optuna hyperparameter tuning\ndef objective(trial):\n    n_estimators = trial.suggest_int('n_estimators', 1, 15)\n    adab_clf = AdaBoostClassifier(random_state = 0, n_estimators = n_estimators)\n    return sklearn.model_selection.cross_val_score(adab_clf, X, y, n_jobs = -1, cv = 10).mean()\n    \nadab_study = optuna.create_study(direction='maximize')\nadab_study.optimize(objective, n_trials = 10)\nadab = adab_study.best_trial\nprint('Accuracy: {}'.format(adab.value))\nprint(\"Best hyperparameters: {}\".format(adab.params))","69c8a301":"# best model\nadab_model = AdaBoostClassifier(random_state = 0, \n                                n_estimators = adab_study.best_trial.params['n_estimators'])\n\nadab_model.fit(X, y)","872a8ba4":"%%time \n# create ensemble classifier \nensemble_model = VotingClassifier(\n    estimators = [('tree', tree_model), \n                  ('rf', rf_model), \n                  ('gradb', gradb_model), \n                  ('xgb', xgb_model)],\n    voting = 'hard'\n)\n\n# fit\nensemble_model.fit(X, y)\n\n# print training accuracy\nprint('Logistic Regression Accuracy', lr_model.score(X, y))\nprint('Decision Tree Accuracy', dt_model.score(X, y))\nprint('Random Forest Accuracy', rf_model.score(X, y))\nprint('Extra Trees Accuracy', tree_model.score(X, y))\nprint('Gradient Boosting Accuracy', gradb_model.score(X, y))\nprint('Extra Gradient Boosting Accuracy', xgb_model.score(X, y))\nprint('AdaBoost Accuracy', adab_model.score(X, y))\nprint('Ensemble Accuracy:', ensemble_model.score(X, y))","f0b9e8fd":"# save scaler\njoblib.dump(scaler, 'forest_cover_scaler.joblib')","aeeb4071":"joblib.dump(rf_model, 'rf_model_2.joblib')\njoblib.dump(tree_model, 'tree_model_2.joblib')\njoblib.dump(gradb_model, 'gradb_model_2.joblib')\njoblib.dump(xgb_model, 'xgb_model_2.joblib')\njoblib.dump(adab_model, 'adab_model_2.joblib')\njoblib.dump(ensemble_model, 'ensemble_model_2.joblib')\nprint('Model written to file.')","041ad264":"# PCA","22546164":"# EDA","ced43ba5":"# Extreme Gradient Boosting","a16149f6":"%%time\n\n# optuna hyperparameter tuning\ndef objective(trial):\n      solver = trial.suggest_categorical('solver', ['saga', 'lbfgs'])\n      lr_clf = LogisticRegression(random_state = 1, penalty = 'none', max_iter = 500, solver = solver)\n      return sklearn.model_selection.cross_val_score(lr_clf, X, y, n_jobs = -1, cv = 10).mean()\n    \nlr_study = optuna.create_study(direction='maximize')\nlr_study.optimize(objective, n_trials=3)\nlr = lr_study.best_trial\nprint('Accuracy: {}'.format(lr.value))\nprint(\"Best hyperparameters: {}\".format(lr.params))","3924924f":"# Save Preprocessor and Models","3123bfad":"# Decision Tree","4d087a1b":"# Gradient Boosting Classifier","4336da18":"# Import Statements","353705c0":"# Random Forest","367a478b":"# Extra Tree Classifier","13ff48e3":"# AdaBoost ","751f94eb":"# Preprocessing","a5448378":"# Model Selection","cdd2683b":"# Logistic Regression","c7548b84":"# Load Data"}}