{"cell_type":{"9f6fd128":"code","d90956db":"code","49279ab1":"code","36957981":"code","c2b6c7c7":"code","6ee1be61":"code","61edc2bf":"code","494b3c67":"code","623d424f":"code","ce239040":"code","c7fe0267":"code","c1ef8f61":"code","f61d14a9":"code","864a3ed7":"code","7e345850":"code","f4922022":"code","53548d42":"code","274f6af3":"code","741a3488":"code","efcfe683":"code","d47d4a3b":"code","258cfd73":"code","96c91b58":"code","c2642067":"code","1a83a23d":"code","7e631af2":"code","7d66269e":"code","1a4c460a":"code","89ed5a3a":"code","eb5bb76d":"code","fc1af332":"code","50e8fe2e":"code","8aa93873":"code","d5d0a3fa":"code","570b55e0":"code","61b0647c":"code","db09ea74":"code","c4b99e12":"code","5295b0e9":"code","84b4680b":"code","f34a61a0":"code","5ebec1a8":"code","4a6bbd47":"code","e70f3a15":"code","a064542c":"code","0fac952a":"code","c85d533b":"code","37c3735a":"code","826e743e":"code","791e60c5":"code","56881a4a":"code","b7fee3f5":"code","db1b23af":"code","4c49b114":"code","c868871c":"code","1dfb8780":"code","175e1ccd":"code","e394addb":"code","715c89c6":"code","1502aae9":"code","8aaaff10":"code","7ef37ec7":"code","67716b3e":"code","74f10305":"code","20cacdcc":"code","ddb43b37":"code","65962cfd":"code","02c5fc6a":"markdown","f8a842b7":"markdown","3d1a77c7":"markdown","344206f5":"markdown","ade30feb":"markdown","1503da77":"markdown","6a1b4867":"markdown","7bcbd662":"markdown","5dabfc2c":"markdown","4cb12eda":"markdown","e979db15":"markdown","b838612f":"markdown","54082b43":"markdown","250f1808":"markdown","ed3d77d5":"markdown","f8102985":"markdown","7998ecc5":"markdown","d1dd2e86":"markdown","b3a52089":"markdown","2f4c634c":"markdown","121f0d9d":"markdown","3465d716":"markdown","4436553b":"markdown","dd0f8dcb":"markdown","8662c1ce":"markdown","99313e39":"markdown","6eed1ca2":"markdown","005aa68d":"markdown","fa237ec3":"markdown","eb25405a":"markdown","c5753410":"markdown","80766479":"markdown","f3f09f3c":"markdown","fc8d86b7":"markdown","a795771d":"markdown","3d367b13":"markdown","4c4dc612":"markdown","a7d1ad5c":"markdown","b39fbb5f":"markdown","9b7f1969":"markdown","9befbe74":"markdown","a2a46aa9":"markdown","472f11a4":"markdown","3438846a":"markdown","51352390":"markdown","3dda6487":"markdown","f10db5ff":"markdown","367ebe1b":"markdown","3511e69f":"markdown","f8707d32":"markdown","0b708f6b":"markdown","5e1808d5":"markdown","c9a0b9ac":"markdown","e29343c2":"markdown","f5b2f7fe":"markdown","c6c0644d":"markdown","401e9925":"markdown"},"source":{"9f6fd128":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d90956db":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('white')\nplt.rcParams[\"patch.force_edgecolor\"] = True\n","49279ab1":"from plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.graph_objs as go\n# import chart_studio.plotly as py\n\ninit_notebook_mode(connected=True)\ncf.go_offline(connected=True)\n\n# Set global theme\ncf.set_config_file(world_readable=True, theme='pearl')\n","36957981":"df_original = pd.read_csv(\"\/kaggle\/input\/hourly-weather-surface-brazil-southeast-region\/sudeste.csv\", )\n","c2b6c7c7":"sample_df = df_original.sample(300000,random_state=101)","6ee1be61":"\nprint(sample_df.iloc[:,:10].head(1))\nprint(\"==\"*20)\nprint(sample_df.iloc[:,10:20].head(1))\nprint(\"==\"*20)\n\nprint(sample_df.iloc[:,20:].head(1))","61edc2bf":"sample_df.drop(['wsnm','wsid','inme'],inplace=True, axis=1)","494b3c67":"# Padding of columns\n# i.e  1->01, 2-> 02, so on.\nsample_df.mo = sample_df.mo.astype(str).str.zfill(2)\nsample_df.da = sample_df.da.astype(str).str.zfill(2)","623d424f":"sample_df['Date'] =  sample_df[['yr','mo','da']].astype(str).agg(\"-\".join,axis=1)","ce239040":"sample_df['Date'] = pd.to_datetime(sample_df['Date'], format='%Y-%m-%d')","c7fe0267":"sample_df.drop(['mdct','yr','da','hr','mo','date'], axis= 1, inplace=True)","c1ef8f61":"sample_df.set_index(\"Date\", inplace=True,drop=True)","f61d14a9":"null_cols = sample_df.columns[sample_df.isnull().sum()>0]\nnot_null_cols = sample_df.columns[sample_df.notnull().sum()==300000]\nnull_df= sample_df[null_cols]\nnot_null_df= sample_df[not_null_cols]\n","864a3ed7":"def customized_heatmap(df):\n    corr_df = df.corr()\n#     print(corr_df)\n    missing_df =corr_df.iloc[1:,:-1].copy()  \n#     print(missing_df)\n\n    \n    # Get only half portion of corr_df to avoid df, so create mask    \n    mask = np.triu(np.ones_like(missing_df), k=1)\n    \n     \n    # plot a heatmap of the values\n    plt.figure(figsize=(20,14))\n    ax = sns.heatmap(missing_df, vmin=-1, vmax=1, cbar=False,\n                     cmap='coolwarm', mask=mask, annot=True)\n    \n    # format the text in the plot to make it easier to read\n    for text in ax.texts:\n        t = float(text.get_text())\n        if -0.25 < t < 0.25:\n            text.set_text('')\n        else:\n            text.set_text(round(t, 2))\n        text.set_fontsize('x-large')\n    plt.xticks( size='x-large')\n    plt.yticks(rotation=0, size='x-large')\n#     plt.savefig(\"Heatmap DF\")\n    plt.show()\n    ","7e345850":"customized_heatmap(null_df)","f4922022":"null_counts = null_df.isnull().sum()\nnull_counts_pct = (null_counts \/ sample_df.shape[0])*100\n\nnull_pct_df = pd.DataFrame({'null_counts': null_counts, 'null_pct': null_counts_pct})\n\nprint(null_pct_df.T.astype(int))","53548d42":"#Plot by sorting the values by gust\nsorted_df_by_temp = sample_df[['gust','wdsp',\"temp\"]].sort_values(['temp'] )\nsorted_df_by_gust = sample_df[['gust','wdsp',\"temp\"]].sort_values(['gust'] )","274f6af3":"plt.figure(figsize=(10,5))\nsns.heatmap(sorted_df_by_gust.isnull(),cmap='coolwarm', cbar=False, yticklabels=False);\n","741a3488":"plt.figure(figsize=(10,5))\nsns.heatmap(sorted_df_by_temp.isnull(),cmap='coolwarm', cbar=False, yticklabels=False);","efcfe683":"mask_gust_wdsp = sample_df['gust'].notnull() & sample_df['wdsp'].isnull()","d47d4a3b":"# Create interval from mask\nsample_df[mask_gust_wdsp].gust.value_counts(bins=3)","258cfd73":"# Create three mean \nfirst_range_mean_wdsp_by_gust= sample_df[sample_df['gust']<7.5]['wdsp'].mean()\nsecond_range_mean_wdsp_by_gust=sample_df[((sample_df['gust']>=7.5) &(sample_df['gust']<15.0))]['wdsp'].mean()\nthird_range_mean_wdsp_by_gust=sample_df[sample_df['gust']>=15.0]['wdsp'].mean()","96c91b58":"#math is imported to check nan, np.nan wont work\ndef fill_wdsp_by_gust(col): \n    \n    \n#Initialize relevant cols\n    gust = col[0]\n    wdsp = col[1]\n    \n    # If the value is nan\n    #Assign by ranges declared above\n    import math\n    if (math.isnan(wdsp)):\n        # Make sure gust in not nan\n        if math.isnan(gust):\n            pass\n        elif (gust<7.5):\n            return first_range_mean_wdsp_by_gust\n        elif (gust>=7.5 ) and (gust<15.0):\n            return second_range_mean_wdsp_by_gust\n        elif (gust>=15.0):\n            return third_range_mean_wdsp_by_gust\n          #if not nan return as it is\n    else:\n        return wdsp\n    ","c2642067":"sample_df['wdsp'] = sample_df[['gust','wdsp']].apply(fill_wdsp_by_gust,axis=1)\n","1a83a23d":"sample_df.dropna(subset=['tmax','temp','tmin'],inplace=True)","7e631af2":"print(sample_df['temp'].value_counts(bins=5).sort_index())\n","7d66269e":"# this funciton will take two cols, temp and column to fill values in this order\n# the second argument is the context\n# From correlation table it seems temperature has high correlation with many columns so, context will clarify which value to fill \n#by column name\n#math is imported to check nan, np.nan wont work\n\n# Create five conditions\ncond_1 = sample_df[sample_df['temp']<5.96]\ncond_2 = sample_df[((sample_df['temp']>=5.96) & (sample_df['temp']<15.32))]\ncond_3= sample_df[((sample_df['temp']>=15.32) & (sample_df['temp']<24.68))  ]\ncond_4 = sample_df[((sample_df['temp']>=24.68) & (sample_df['temp']<34.04))  ]\ncond_5 = sample_df[sample_df['temp']>=34.04]\n\n\n# Create five ranges of mean according to above interval for windspeed\n\nfirst_range_mean_wdsp_by_temp=cond_1['wdsp'].mean()\nsecond_range_mean_wdsp_by_temp= cond_2['wdsp'].mean()\nthird_range_mean_wdsp_by_temp= cond_3['wdsp'].mean()\nfourth_range_mean_wdsp_by_temp= cond_4['wdsp'].mean()\nfifth_range_mean_wdsp_by_temp= cond_5['wdsp'].mean()\n\n\n# Create five ranges of mean according to above interval for gust\n\nfirst_range_mean_gust_by_temp= cond_1['gust'].mean()\nsecond_range_mean_gust_by_temp=  cond_2['gust'].mean()\nthird_range_mean_gust_by_temp= cond_3['gust'].mean()\nfourth_range_mean_gust_by_temp=  cond_4['gust'].mean()\nfifth_range_mean_gust_by_temp=  cond_5['gust'].mean()\n\n\n# Create five ranges of mean according to above interval for solar radiation\n\nfirst_range_mean_radiation_by_temp= cond_1['gbrd'].mean()\nsecond_range_mean_radiation_by_temp=cond_2['gbrd'].mean()\nthird_range_mean_radiation_by_temp= cond_3['gbrd'].mean()\nfourth_range_mean_radiation_by_temp=  cond_4['gbrd'].mean()\nfifth_range_mean_radiation_by_temp=  cond_5['gbrd'].mean()\n\n\n\ndef fill_missing_by_temp(col, context):\n    import math\n    #Initialize relevant cols\n    temp = col[0]\n    col_1_val = col[1]\n    \n    # Divide the task by context\n    #Either for windspeed or for gust\n    \n    if context == \"wdsp\":\n      \n        # If the value is nan\n        #Assign by ranges declared above\n        if math.isnan(col_1_val):\n            if(temp<5.96):\n                return first_range_mean_wdsp_by_temp\n            elif(temp>=5.96) and (temp<15.32):\n                return second_range_mean_wdsp_by_temp\n            elif(temp>=15.32) and (temp<24.68):\n                return third_range_mean_wdsp_by_temp\n            elif(temp>=24.68) and (temp<34.04):\n                return fourth_range_mean_wdsp_by_temp\n            elif(temp>=34.04):\n                return fifth_range_mean_wdsp_by_temp\n            #if not nan return as it is\n        else:\n            return col_1_val\n        \n    elif context==\"gbrd\":\n         # If the value is nan\n        #Assign by ranges declared above\n        if math.isnan(col_1_val):\n            if(temp<5.96):\n                return first_range_mean_radiation_by_temp\n            elif(temp>=5.96) and (temp<15.32):\n                return second_range_mean_radiation_by_temp\n            elif(temp>=15.32) and (temp<24.68):\n                return third_range_mean_radiation_by_temp\n            elif(temp>=24.68) and (temp<34.04):\n                return fourth_range_mean_radiation_by_temp\n            elif(temp>=34.04):\n                return fifth_range_mean_radiation_by_temp\n            #if not nan return as it is\n        else:\n            return col_1_val\n        \n    else:\n         # If the value is nan\n        #Assign by ranges declared above\n        if math.isnan(col_1_val):\n            if(temp<5.96):\n                return first_range_mean_gust_by_temp\n            elif(temp>=5.96) and (temp<15.32):\n                return second_range_mean_gust_by_temp\n            elif(temp>=15.32) and (temp<24.68):\n                return third_range_mean_gust_by_temp\n            elif(temp>=24.68) and (temp<34.04):\n                return fourth_range_mean_gust_by_temp\n            elif(temp>=34.04):\n                return fifth_range_mean_gust_by_temp\n            #if not nan return as it is\n        else:\n            return col_1_val\n    \n        \n    \n    ","1a4c460a":"sample_df['wdsp'] = sample_df[['temp','wdsp']].apply(fill_missing_by_temp,context =\"wdsp\",axis=1)\nsample_df['gust'] = sample_df[['temp','gust']].apply(fill_missing_by_temp,context= \"gust\", axis=1)\n\n","89ed5a3a":"#Plot by sorting the values by temp\ngbrd_df_by_temp = sample_df[['temp',\"gbrd\" ]].sort_values(['temp'] )\n","eb5bb76d":"plt.figure(figsize=(10,5))\nsns.heatmap(gbrd_df_by_temp[['temp',\"gbrd\"]].isnull(),cmap='coolwarm', cbar=False, yticklabels=False);\n","fc1af332":"sample_df['gbrd'] = sample_df[['temp','gbrd']].apply(fill_missing_by_temp,context= \"gbrd\", axis=1)\n","50e8fe2e":"# First drop hmin and hmax na values\n\nsample_df.dropna(subset=['hmax','hmin'],inplace=True)","8aa93873":"print(null_pct_df.T.astype(int))","d5d0a3fa":"# Check out intervals\n\nsample_df.hmdy.value_counts(bins=3).sort_index()","570b55e0":"# Create three mean \nfirst_range_mean_dwep_by_humidity= sample_df[sample_df['hmdy']<33.333]['dewp'].mean()\nsecond_range_mean_dwep_by_humidity=sample_df[((sample_df['hmdy']>=33.333) &(sample_df['hmdy']<66.667))]['dewp'].mean()\nthird_range_mean_dwep_by_humidity=sample_df[sample_df['hmdy']>= 66.667 ]['dewp'].mean()","61b0647c":"#math is imported to check nan, np.nan wont work\ndef fill_dewp_by_humidity(col): \n    \n    \n#Initialize relevant cols\n    hmdy = col[0]\n    dewp = col[1]\n    \n    # If the value is nan\n    #Assign by ranges declared above\n    import math\n    if math.isnan(dewp):\n        if (hmdy<33.333):\n            return first_range_mean_dwep_by_humidity\n        elif (hmdy>=33.333 ) and (hmdy<66.667):\n            return second_range_mean_dwep_by_humidity\n        elif (hmdy>=66.667):\n            return third_range_mean_dwep_by_humidity\n          #if not nan return as it is\n    else:\n        return dewp\n    ","db09ea74":"sample_df['dewp'] = sample_df[[\"hmdy\", \"dewp\"]].apply(fill_dewp_by_humidity, axis=1)\nsample_df['dmin'] = sample_df[[\"hmdy\", \"dmin\"]].apply(fill_dewp_by_humidity, axis=1)\nsample_df['dmax'] = sample_df[[\"hmdy\", \"dmax\"]].apply(fill_dewp_by_humidity, axis=1)","c4b99e12":"# New df without prcp\ndf = sample_df[['elvt', 'lat', 'lon','city', 'prov', 'stp', 'smax', 'smin', 'gbrd', 'temp',\n       'dewp', 'tmax', 'dmax', 'tmin', 'dmin', 'hmdy', 'hmax', 'hmin', 'wdsp',\n       'wdct', 'gust']].copy()\n\n","5295b0e9":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\n#Make a copy from the original df first\n\nnormed_df = df.copy()\n# EXtract cols with non-string values\nfloat_cols = df.columns[df.dtypes ==\"float64\" ].tolist()\n# Normed df \nnormed_df[float_cols] =scaler.fit_transform(normed_df[float_cols])","84b4680b":"df['prov'].value_counts(normalize=True).plot.pie(figsize=(8,10),autopct = '%.1f%%',\n                                                 labels=['Minas Gerais','S\u00e3o Paulo','Rio de Janeiro','Esp\u00edrito Santo'])\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.title('Weather Data by Province');\n# plt.savefig(\"Provinces Proportions\")\n\n","f34a61a0":"gr_by_city = normed_df[['prov','city', 'stp', 'smax', 'smin', 'gbrd', 'temp',\n                  'dewp', 'tmax', 'dmax', 'tmin', 'dmin', 'hmdy', \n                  'hmax', 'hmin', 'wdsp','wdct', 'gust']].groupby('city').mean()\n\ngr_by_prov = normed_df[['prov','city', 'stp', 'smax', 'smin', 'gbrd', 'temp',\n                             'dewp', 'tmax', 'dmax', 'tmin', 'dmin','hmdy', \n                        'hmax', 'hmin', 'wdsp','wdct', 'gust']].groupby(['prov']).mean()","5ebec1a8":"gr_by_city.columns","4a6bbd47":"gr_by_yr_prov = normed_df.groupby([normed_df.index.year,'prov']).mean().unstack(level=0)","e70f3a15":"layout = dict(xaxis_title=\"City\",\n              yaxis_title=\"Frequency Normalized\",\n              title=\"Avg. Temp by City\")\ngr_by_city[('temp')].iplot(kind=\"bar\", layout=layout);","a064542c":"\n# Change name for better labels in legend\n# 'stp', 'smax', 'smin', 'gbrd', 'temp', 'dewp', 'tmax', 'dmax', 'tmin',\n#        'dmin', 'hmdy', 'hmax', 'hmin', 'wdsp', 'wdct', 'gust'\ncols_to_plot =['stp','gbrd', 'temp', 'dewp', 'hmdy', 'wdsp', 'gust']\ncols_changed_name ={\"Air Pressure\",\"Solar Radiation\", \"Temperature\",\"Dew Point\",\"Humidity\", \"Windspeed\",\"Gust\"}\n\n\ntemp_df = gr_by_prov[cols_to_plot]\n\ntemp_df.columns = cols_changed_name\n","0fac952a":"layout = dict(xaxis = dict( tickvals =[\"ES\",\"MG\",\"RJ\",\"SP\"],\n                           ticktext=['Esp\u00edrito Santo','Minas Gerais','Rio de Janeiro','S\u00e3o Paulo']),\n             legend_title_text='Weather Factors',\n              xaxis_title=\"Provinces\",\n              yaxis_title=\"Frequency Normalized\",\n              title = \"Avg Weather Factors by Provinces\",)\ntemp_df.iplot(kind=\"bar\",  layout=layout);\n#names = [\"Air pressure\",\"Solar Radiation\", \"Temperature\", \"Dewpoint\", \"Humidity\",\"Windspeed\",\"Gust\"])\n\n          ","c85d533b":"# sample_df[['prcp',\"city\"]].groupby(['city', df.index.year]).mean().unstack(level=1)\n#\ntemp_yearly=pd.pivot_table(data= sample_df,index=\"city\", columns=sample_df.index.year,values=\"temp\")\n# temp_yearly_normed=pd.pivot_table(data= normed_df,index=\"city\", columns=sample_df.index.year,values=\"temp\")\n\n","37c3735a":"l= dict(showlegend=False, title=\"Yearly Average Temperature\", xaxis_title=\"Year\", yaxis_title=\"Frequecny\")\ntemp_yearly.iplot(kind=\"box\",layout=l)","826e743e":"\n# temp_yearly.iplot(secondary_y = min_values)\nlayout= dict(yaxis_title=\"Frequency\",xaxis_title=\"City\", title=\"Average Yearly Temperature by Cities\",)\ntemp_yearly.iplot(kind=\"scatter\", mode=\"lines+markers\", layout=layout)","791e60c5":"from sklearn.preprocessing import MinMaxScaler","56881a4a":"scaler = MinMaxScaler()\n\n# Take a copy of sample_df\nsample_df_norm = sample_df.copy()\n\n#Columns to normalize\ncols_to_norm = ['elvt', 'lat', 'lon', 'prcp', 'stp', 'smax', 'smin','gbrd', 'temp', 'dewp', 'tmax', 'dmax', 'tmin', 'dmin',\n                'hmdy', 'hmax','hmin', 'wdsp', 'wdct', 'gust']\n\n#Normalize numeric scales\nsample_df_norm.loc[:,cols_to_norm] = scaler.fit_transform(sample_df.loc[:,cols_to_norm])","b7fee3f5":"### Seasonal Fluctuations- Tempearature\n\n# Assuming 4 mnths seasons\n# Lets obsorve seasoal fluctuation\nlayout= dict(yaxis_title=\"Frequency\",xaxis_title=\"Years\", title=\"Average Seasonal Temperature Distribution\",)\nsample_df_norm[['temp','tmin',\"tmax\"]].resample('M').mean().rolling(4).mean().iplot(kind=\"bar\",layout=layout )\n","db1b23af":"temperature_df=sample_df[['city','temp','tmax','tmin']]","4c49b114":"def get_temp_by_choice(yr,mo,c):\n    # Get temperature record by given month and year\n    temp_m_y_c=temperature_df[(temperature_df.index.year==yr) & (temperature_df.index.month==mo) & (temperature_df.city == c)]\n    \n    \n    # Extract given city from  above record\n    temp_max = temp_m_y_c.max()\n    temp_min = temp_m_y_c.min()\n    print(temp_max)\n    print(temp_min)\n    print(\"==\"*20)\n#     return temp_max, temp_min","c868871c":"get_temp_by_choice(2008,10,\"Vi\u00e7osa\")\nget_temp_by_choice(2008,10,\"Valen\u00e7a\")\nget_temp_by_choice(2008,10,\"Montalv\u00e2nia\")\nget_temp_by_choice(2008,10,\"Itaobim\")","1dfb8780":"timeline={\"Weekly\":\"W\",\"Monthly\":\"M\",\"Yearly\":\"Y\"}\n\n\ndef temp_analysis(cities, time_line=\"Yearly\"):\n    \"\"\"provide timeline among weekly, monthly or yearly. The default value is Yearly.\n     Function takes  two  arguments \n     1. cities, which is list of cites\n     2. time_line, which has three values, weekly, yearly, or monthly\n    for eg: temp_analysis(['Vi\u00e7osa','Mantena','Formiga',\"S\u00e3o Jo\u00e3o del Rei\",\"Juiz de Fora\" ], \"Yearly\")\n\n\"\"\"\n    print(temp_analysis.__doc__)\n    \n    c =pd.DataFrame()\n    # Extract given city from  above record\n    for city in cities:\n        c= c.append(temperature_df[temperature_df['city'] == city])\n    \n    # Resample it on average\n    #title() takes care of uppercase and lowercase confusion\n    d=c.resample(timeline[time_line.title()]).mean()\n    print(\"Temperature of cities {}\".format(cities))\n\n   \n    # Plot the data\n    layout= dict( title='Avg Temperatures {}'.format(time_line.upper()), xaxis_title=\"Years\", yaxis_title=\"Temperatures\")\n    d[['tmax','tmin','temp']].iplot(kind=\"bar\", layout=layout)\n","175e1ccd":"temp_analysis(['Vi\u00e7osa','Mantena','Formiga',\"S\u00e3o Jo\u00e3o del Rei\",\"Juiz de Fora\" ], \"Weekly\")","e394addb":"# Assuming 4 mnths seasons\n# Lets obsorve seasoal fluctuation\nlayout= dict(yaxis_title=\"Frequency\",xaxis_title=\"Years\", title=\"Average Seasonal Pressure Distribution\",)\nsample_df_norm[['stp','smin',\"smax\"]].resample('M').mean().rolling(4).mean().iplot(kind=\"bar\",layout=layout )\n","715c89c6":"# df for airpressure only\nair_df=sample_df[['city','prov','stp','smax','smin']]","1502aae9":"def monthly_plot(m,y,cities):\n    '''  Function takes  three arguments month in number, year in int and list of cities to compare\n    for eg: monthly_plot(10,2008,['Vi\u00e7osa','Mantena','Formiga',\"S\u00e3o Jo\u00e3o del Rei\",\"Juiz de Fora\" ])\n\n\n    '''\n    print(monthly_plot.__doc__)\n  \n    # Get pressure record by given month and year\n    air_m_y=air_df[(air_df.index.month == m) & (air_df.index.year == y)]\n    air_m_y_c =pd.DataFrame()\n    # Extract given city from  above record\n    for c in cities:\n        air_m_y_c = air_m_y_c.append(air_m_y[air_m_y['city'] == c])\n    \n    grp_by_c = air_m_y_c.groupby('city').mean()\n    layout= dict( title='Avg Air pressure of city {}-{}'.format(m,y), xaxis_title=\"Cities\", yaxis_title=\"Air pressure\")\n    grp_by_c.iplot(kind=\"bar\",layout=layout)","8aaaff10":"# For prov \"MG\"\nminas_cities = air_df[air_df.prov==\"MG\"].city\n# lets just use 5 cities at random\nminas_cities[:5]","7ef37ec7":"monthly_plot(10,2008,['Vi\u00e7osa','Mantena','Formiga',\"S\u00e3o Jo\u00e3o del Rei\",\"Juiz de Fora\" ])","67716b3e":"gust_yr_city = sample_df_norm.pivot_table(columns=sample_df_norm.index.year,index='city',values=['gust'],aggfunc='mean').stack()\n","74f10305":"gusty_city= gust_yr_city.loc[gust_yr_city['gust']==gust_yr_city.gust.max()].index[0]\ngusty_city","20cacdcc":"customized_heatmap(normed_df)","ddb43b37":"# fig = plt.figure(figsize=(10,10))\nsns.jointplot(normed_df['hmdy'],normed_df['dewp']);\n# plt.savefig(\"Humidity vs Dewpoint\")","65962cfd":"fig = plt.figure(figsize=(10,10))\nsns.jointplot(normed_df['wdsp'],normed_df['hmdy'], kind='kde');\nplt.savefig(\"Windspped vs Humidty Density Plot\")","02c5fc6a":"Among the cities, the gusty city is \"Petr\u00f3polis\" in year 2007.","f8a842b7":"## Info On Some Columns\n\n\n1. Instant Air Temperature (celsius degrees) = temp\n2. Maximum Air Temperature (celsius degrees) = tmin\n3. Minimum Air Temperature (celsius degrees) = tmax\n4. Relative Humidity of Air (%) =hmdy\n5. Maximum Relative Air Humidity (%) =hmax\n6. Minimum Relative Air Humidity (%) = hmin\n7. Instant Dew Point (celsius degrees) = dewp\n8. Maximum Dew Point (celsius degrees)=dmax\n9. Minimum Dew Point Temperature (celsius degrees) = dmin\n10. Instant Air Atmospheric Pressure (millibars) =stp\n11. Maximum Air Atmospheric Pressure (millibars) = smax\n12. Minimum Air Atmospheric Pressure (millibars)= smin\n13. Instant Wind Speed (metres per second) = wdsp\n14. Wind Direction (radius degrees) = wdct\n15. Wind Gust Intensity (metres per second) = gust\n16. Solar radiation  =  gbrd\n17. Precipitation (milimetres) = prcp\n18. Elevation = elvt\n19. Observation Datetime = mdct\n20. Observation Date = date\n21. Station number (INMET number) for the location = inme\n22. The year (2000-2016) : yr\n23. The month (0-12) : mo\n24. The day (0-31): da\n25. The hour : hr\n\n*Not all the columns are mentioned in this list*","3d1a77c7":"*Temparature* column seems to have high correlation with many columns. So, lets create a function that will fill the values of columns with different interval of times and their mean with in that intervals. for this project lets try 5 intervals of temp.\n\nMoreover, since temp is gonna determine the other columns lets drop few rows that are null in the temp column.","344206f5":" ## Missing Data Correlation\n \n Separate null columns and non-null ones, also the dataframes.","ade30feb":"On average fluctation was noticeable during yr 2001 and 2006. Slightly however, there is constant sloppy decrease in temp from 2004 to 2006, lowest temperatures comparing to other years. \n\nFrom year 2010-2016 the temperatures are steady and warmer.","1503da77":"In 2016, which is the latest record, Guaruja,is the coldest city and \"Sao Goncalo\" is the hottest city. ","6a1b4867":"## Missing Data Visualization Using Heatmap ","7bcbd662":"Above heatmap shows obvious high correlation between similar variables like (temp,tmax and tmin), (stp,smax and smin) and so on. However, it shows higher correaltion btween variables of dew points with humidity. It can implied that more due forms for humidity. The jointplot \"Dew and Humidity\" also proves the point. Moreover, the aspect of temperature(temp,tmax and tmin) and airpressure (stp,smax and smin) also have high correlation between them.  \n\nIt appears aspects wind-speeds have very low correlatoin with humidity. Windspeed and Humidity plot below will make this more clear.  ","5dabfc2c":"# Sample Data\nDue to large data lets work with sampel data****","4cb12eda":"## Heatmap of Corr","e979db15":"If the weekly timeline is provided on zooming in the max temp week is From Jan 18-25,2015","b838612f":"Lets normalize the dataset first to neutralize different units","54082b43":"Create function that fills value by mask. ","250f1808":"# Data Analysis","ed3d77d5":"Reset Index as Date column and the check for duplicates data on same date in same city and remove them","f8102985":"In October of 2008, *Vi\u00e7osa* was the coldest and *Montalv\u00e2nia* was the hottest one.","7998ecc5":"### Gust and Windspeed (fig 1.2)","d1dd2e86":"Since there is no null values,under the assumption that all the data from various weather stations are valid and true, lets not filter the time of operation of weather stations and drop the wsids and inme, only city adn prov are enough.","b3a52089":"Lets apply the func to all dewp variables","2f4c634c":"### Dewpoint \n\n\nDew point has highest correlation with humyidity and then with temperature.Lets fill their values by dewp","121f0d9d":"## Airpressure","3465d716":"# Tidying Data","4436553b":"\nPlotting heatmap will not be fruitfull because the relative size of null values to the total lenght of df is quite low so, the heatmap will not show the null values. \nLets just use pct table we created earlier.\n\n ","dd0f8dcb":"There are five columns on time of observatoin. They are mdct, date,yr, month and hour, which are same but separated into several sctions. Among them Date and Hour columns are useful and represent all in some form, so others can be dropped. The conversion of date column to datetime gave format error.So, first do padding of year and day column then, combie as data.","8662c1ce":"Air pressure and temperature  seasonal dist show similar behaviour accoring to plot.","99313e39":"## Correlatoin of Null Columns in Heatmap (Fig 1.1)","6eed1ca2":"City *Maneta* is the only one with air pressure in the upper range around the year, while *Vicosa's* in the middle range and other 3 are in lower range.","005aa68d":"#### Columns and Measurement scale\n\nCity and Province are categorical, nominal values. All other columns are quantitative,continuous and ratio scale. ","fa237ec3":"## Temperature\n","eb25405a":"## Gusty City of All Time","c5753410":"Now we can extract temp records of our choice lets, try few.","80766479":"The columns gust and wdsp have high correlation since they are both properties of wind. Moreover, both seem to be affected by properties of temperatures positively. \n\nSimilary, Solar Radiation(GBRD) is also the affected by temperatures,gusts and windspeed respectively in order from higher to lower correlation. \n\nDewpoint also has higher correlatin with temperature, while the highest with properties of humidity(hmin, hmax).\n\nLets plot the properties that have higher correlation with each other in heatmap to visualize null rows.","f3f09f3c":"### Dew and Humidity","fc8d86b7":"Dataset is too big so lets import in a chunk and work with the smallest one at first","a795771d":"The radiation column has very high missing values and quite scattered. Lets apply temp fill function with context as gbrd.","3d367b13":"### Data share By Province","4c4dc612":"For columns with smaller null count the percentage is very less to be shown in rounded values. ","a7d1ad5c":"## Dealing with Time related columns","b39fbb5f":"The plot is denser on the left and botteom. So, it seems wind-speed and humidity are reversely proportional to each other. Increase in wind-speed causes decrease in humidity. Also both appear right skewed.","9b7f1969":"Lets check some cities from same province first, during same times.\n","9befbe74":"# Missing Values ","a2a46aa9":"Only column with null value is prcp, which is also a dependent column. Hence leave it at that for now. Create new df for analysis without prcp. Moreover, some analysis is faster using the normalized version so lets created a normalized dataframe too.\n","472f11a4":"Just for the sake of argument fill the missing values in dewp, dmax with average values","3438846a":"Creating a function that will plot the heatmap of any dataframe, plotting the correlation between the columns in the dataframes. Moreover, this customized heatmap will remove the repitition plotting only lower diagonal elements. Plot will also only dispaly  the columns correlaton that are >-0.25 and <0.25 which are generally considered high correlation.","51352390":"### Airpressure Dataframe","3dda6487":"After sorting the columns it seems, for all the missing values in gust there are missing values in windspeed. But according to correlation heatmap, since gust and windspeed have higher realtion than with temperature.\nLets fill the rows  where windspeed has null values but gust does not with weighted mean of windspeed by gust column. i.e create a mask where wdsp is null and gust in not null. Then divide that range again into three interval just to be more precise.\n\nAfter that apply  interval technique to fill remaining rows with wighted mean of temperature columns but five intervals this time. ","f10db5ff":"Year 2011 has least fluctuations, data in early 2000's are incomplete data so can't be compared properly. ","367ebe1b":"Looks like 2013-2016 have been periods of ups and downs. Average temperature  fell from about 20.5 to  16.5 from 2013 t0 2014 and again rose to about 22 degrees on average and theres slopy journey again to 2016.","3511e69f":"### Temperature dataframe","f8707d32":"### Solar Radiation\n\n\nSolar radiation has high correaltion with temp so, lets checkout those columns","0b708f6b":"It seems abot 50% of the data is related to *Minas Gerlas* province of Brazil","5e1808d5":"### Plot By Custom Choice\nThe following function will plot Average temperature of given list of citeis in  bar  plot some timeline of choice.","c9a0b9ac":"### Humidity and WindSpeed","e29343c2":"### Sketching the chart for specific air pressure values with year, month and city\n\nCreate a function which shows the data from given year, month and city name then plots it","f5b2f7fe":"## Missing data percentage calculation","c6c0644d":"# Variables and Correlations","401e9925":"Province *Espirito Santo* is taking the top spot on average weather factors by province. While *Minas Gerias* and *Rio de Janerio* both share similar triats. "}}