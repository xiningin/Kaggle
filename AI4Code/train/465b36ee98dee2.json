{"cell_type":{"cbb5869b":"code","0c4955a5":"code","e4b7c366":"code","50d2b36a":"code","6487cd71":"code","f9fb18e0":"code","e49fff64":"code","f45eaa96":"code","b6440865":"code","ce19a70f":"code","a4b5477e":"code","54cda156":"code","8cb2b3bb":"code","b2fbfa84":"code","2696015f":"code","a569314e":"code","9e640f6c":"code","467eeb45":"code","3f913ffc":"code","796d98c0":"code","ce5ca137":"code","419344b5":"code","594d59eb":"code","f0794599":"code","da21165c":"code","e99351f8":"code","609db238":"code","17da4c16":"code","1893b4f6":"code","8c46359c":"code","e17c63e2":"code","62192eb4":"code","a8bf5e5b":"code","b7a8308e":"code","f62ff04a":"code","9c77efd7":"code","b5e63d5a":"code","dea74592":"code","4927e460":"code","f9035391":"code","32df0d46":"code","f42abe18":"code","fb480780":"code","10debf14":"code","a2f81fcf":"code","bd8c7196":"code","98be5771":"code","1c02530d":"code","ef0084e8":"code","f05eb7d6":"code","d7af2f15":"code","b736b366":"code","36c6f32a":"code","c225837a":"code","620e8ec2":"code","df784f4e":"code","b1f62a78":"code","da409c7f":"code","fae072e1":"code","d60daa24":"code","8f60d99b":"code","a96a694f":"code","ff410e1d":"code","55af9509":"code","21ec52c1":"code","b0983209":"code","d3d07147":"code","5ecb125e":"code","06712a15":"code","e27651ea":"code","70778aec":"code","be0eb857":"code","1b5a9ddd":"code","c440fa14":"code","0881c6ac":"code","1bbdfde4":"code","405248e0":"code","99551804":"code","a885060e":"code","6de3197e":"code","b763b40c":"code","bf9db5f0":"code","456d2b54":"code","9895c4a0":"code","e2a11e48":"code","058a91d1":"code","99631f6f":"code","e8dc4467":"code","734fbaa4":"code","f1b7ad6e":"code","d9738b9c":"code","a4b32a4c":"code","42391148":"code","e57f3457":"code","e09685d7":"code","8e2a4c3e":"code","3748a04e":"code","e86dc40b":"code","41c6a41d":"code","4eebca0a":"code","911ad8e7":"code","3a884ad6":"code","e263eddd":"code","f0f1ae7a":"code","a0ba9f5e":"code","8ac1eb2a":"code","57fe50e0":"code","195e1614":"code","b15f459a":"code","ed518470":"code","29e1f461":"code","a84ca3ed":"code","b4e48a0f":"code","b36e6911":"code","8f9317d3":"code","423f5823":"code","782f8156":"code","482fde88":"code","e82c875d":"code","71ef6574":"code","ed66cc17":"code","edda1f38":"code","397d39bb":"code","30f85f1f":"code","de9c9627":"code","7e4fc7af":"code","8b024c78":"code","9e8ba4f4":"code","ce750b31":"code","3968bb2a":"code","f0d54b4f":"code","d563bfa9":"code","eef3d302":"code","081a0dcd":"code","c6c7f8b9":"code","614ea103":"code","b0dce6b8":"code","c316fc75":"code","35879c54":"code","53991cd2":"code","b90d8848":"code","f89f1483":"code","c3109804":"code","fd2c8033":"code","40f90eb5":"code","43077bb1":"code","3bee9499":"code","9da058f5":"code","71579a1d":"code","1e95882c":"code","910f4546":"code","245f8b97":"code","8b828287":"code","05ff1f51":"code","5820bbe6":"code","7333102d":"code","f73b2f33":"code","c67e3e32":"code","6bf6fbdc":"code","afbd209e":"code","8f9c2460":"code","862f9c91":"code","21aa293d":"code","3d6b42d7":"code","c6c81c70":"code","2e8116f2":"code","664fee2d":"code","4e48e98c":"code","138c8ce4":"code","2cdeab84":"code","fb6142b1":"code","ffcaef13":"code","361122aa":"code","8134c74b":"code","0b805db4":"code","1cabbffc":"code","9ca628ee":"code","5c143d44":"code","cd8b9730":"code","318607a3":"code","f24bba07":"code","c92c39a0":"code","4f0f85f0":"code","5961628f":"code","2714c2e7":"code","a6f8a87e":"code","7d9ef4dd":"code","48b88c41":"code","4040da9e":"code","ed20ba07":"code","6c766258":"code","47b8409e":"code","d451fa0b":"code","531b3450":"code","46deb3fd":"code","13657c5e":"code","d5711d9e":"code","830edcef":"code","fca9cd7f":"code","f14c84ab":"code","f4394efb":"code","a5fa1ec7":"code","3cec40f2":"code","6b9b9b83":"code","aad31331":"code","52e09b74":"code","8879f364":"code","b243b6d7":"code","3550407e":"code","7fabbb88":"code","8e53868f":"code","9c00e5ae":"code","e9e87381":"code","dc0379e7":"code","3df9df28":"code","83c8ed56":"code","30403a9b":"code","8d8d95c9":"code","d92bc869":"code","92cba1b1":"code","f070c2e3":"code","6611c994":"code","865270e2":"code","6b8f4cac":"code","407310b1":"code","48faf41d":"code","0ff9b3c6":"code","099ec035":"code","1d89c03d":"code","45848a45":"code","bd3249a3":"code","66e27e83":"code","e0b0ba81":"code","6047adbb":"code","0f0dd171":"code","1a67a300":"code","e48c35b3":"code","5fe583ad":"code","4983dedc":"code","6fc7a906":"code","9b4cdda2":"code","77b92e99":"markdown","a8cc383b":"markdown","9310b098":"markdown","ec98e576":"markdown","cf192fd3":"markdown","e11ef3fb":"markdown","c324a7b5":"markdown","ab88435f":"markdown","ba008f23":"markdown","6b692bac":"markdown","18addf13":"markdown","0e2b08a0":"markdown","b52ec265":"markdown","51aea475":"markdown","db35c33e":"markdown","17e7e5a6":"markdown","5d6911da":"markdown","641a67a6":"markdown","b1101313":"markdown","eed50d57":"markdown","c71233a4":"markdown","f8117cf7":"markdown","37ea5faa":"markdown","67207b56":"markdown","9085ac5e":"markdown","f2342cd5":"markdown","391ac245":"markdown","59be6521":"markdown","d4bad25d":"markdown","bcbd1e08":"markdown","8f76487f":"markdown","ff9d8b07":"markdown","cc5423ca":"markdown","025809b2":"markdown","0cb8090c":"markdown","f9be0a74":"markdown","b6cdf23b":"markdown","386c7b50":"markdown","74b3d816":"markdown","42fef069":"markdown","385db037":"markdown","594bdf85":"markdown","1f35e8a3":"markdown","a93c1171":"markdown","7e935b9d":"markdown","83ecc9ab":"markdown","b10535d7":"markdown","2daa8fe3":"markdown","15694d24":"markdown","d4aaef01":"markdown","84deab06":"markdown","b84b0d65":"markdown","f364d8c1":"markdown","46cb3900":"markdown","10f98688":"markdown","c563d2c7":"markdown","3a3cda88":"markdown","b5324244":"markdown","3d63fa48":"markdown","9276a83d":"markdown","fde25585":"markdown","dbfe8341":"markdown","8d01d835":"markdown","e6c905d8":"markdown","a09b7cbc":"markdown","256ca773":"markdown","b1be701c":"markdown","79312883":"markdown","234719f6":"markdown","ab0c739b":"markdown","7abd265a":"markdown","042e1beb":"markdown","e70133ce":"markdown","db0ab2d9":"markdown","b9283241":"markdown","3dcaa5eb":"markdown","3a4bbfab":"markdown","f00a3c85":"markdown","d85e856c":"markdown","fcca09d6":"markdown","6babc8eb":"markdown","e441e742":"markdown","31883f98":"markdown","ebbac224":"markdown","9e7caf99":"markdown","00ddd91f":"markdown","38564922":"markdown","b2b207e9":"markdown","002239ad":"markdown","f3b85be2":"markdown","7c50f632":"markdown","da479784":"markdown","393da044":"markdown","7a59d75f":"markdown","c357b736":"markdown","d4a26288":"markdown","5d96e59f":"markdown","3012ce67":"markdown","4749103a":"markdown","47edb8c0":"markdown","930bc4ad":"markdown","e4eb2dba":"markdown","a4041e3b":"markdown","4ac79dd5":"markdown","0ecb17e0":"markdown","a54c2ecf":"markdown","8b6a9ca8":"markdown","4ef3f1e7":"markdown","dc440ead":"markdown","1b838fc1":"markdown","e9ba71de":"markdown","cdb3d64d":"markdown","c4fcb5ba":"markdown","477fb198":"markdown","ba270898":"markdown","17957262":"markdown","80c4cf31":"markdown","9def8f17":"markdown"},"source":{"cbb5869b":"#import all the libraries and modules\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom scipy import stats \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# Importing RFE and LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm  \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\n\n# Supress Warnings\n#Enable autocomplete in Jupyter Notebook.\n%config IPCompleter.greedy=True\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\n\n### Set seaborn style\nsns.set(style=\"darkgrid\")\n\n## Set the max display columns to None so that pandas doesn't sandwich the output \npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 40)","0c4955a5":"### Let us create a utility function to generate a list of null values in different dataframes\n### We will utilize this function extensively througout the notebook. \ndef generateNullValuesPercentageTable(dataframe):\n    totalNullValues = dataframe.isnull().sum().sort_values(ascending=False)\n    percentageOfNullValues = round((dataframe.isnull().sum()*100\/len(dataframe)).sort_values(ascending=False),2)\n    columnNamesWithPrcntgOfNullValues = pd.concat([totalNullValues, percentageOfNullValues], axis=1, keys=['Total Null Values', 'Percentage of Null Values'])\n    return columnNamesWithPrcntgOfNullValues","e4b7c366":"### let us create a reuseable function that will help us in ploting our barplots for analysis\n\ndef generateBarPlot(dataframe, keyVariable, plotSize):\n    fig, axs = plt.subplots(figsize = plotSize)\n    plt.xticks(rotation = 90)\n    ax = sns.countplot(x=keyVariable, data=dataframe)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}'.format(height\/len(dataframe) * 100),\n                ha=\"center\") ","50d2b36a":"### Let us define a reusable function to carry out Bivariate analysis as well.\n\ndef generateBiplot(df,col,title,figsize,hue=None):\n    \n    sns.set_context('talk')\n    plt.rcParams[\"axes.labelsize\"] = 20\n    plt.rcParams['axes.titlesize'] = 22\n    plt.rcParams['axes.titlepad'] = 30\n    plt.figure(figsize=figsize)\n    \n    \n    temp = pd.Series(data = hue)\n    fig, ax = plt.subplots()\n    width = len(df[col].unique()) + 7 + 4*len(temp.unique())\n    fig.set_size_inches(width , 8)\n    plt.xticks(rotation=45)\n    plt.title(title)\n    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue,\n                       palette='magma')\n    \n    ### Also print the conversion accuracy of every field\n    convertcount=df.pivot_table(values='Lead Number',index=col,columns='Converted', aggfunc='count').fillna(0)\n    convertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\n    return print(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"),plt.show())\n        \n    plt.show()","6487cd71":"### Function to generate heatmaps\n\ndef generateHeatmaps(df, figsize):\n    plt.figure(figsize = figsize)        # Size of the figure\n    sns.heatmap(df.corr(),annot = True, annot_kws={\"fontsize\":7})\n","f9fb18e0":"### Function to generate ROC curves\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","e49fff64":"### Let us create a reusable function to calculate VIF values for our models\n\ndef vifCalculator(inputModel):\n    vif= pd.DataFrame()\n    X = inputModel\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif\n","f45eaa96":"### Checking if the data has been correctly loaded or not.\nleadScoreDataset = pd.read_csv('..\/input\/leads-dataset\/Leads.csv')\nleadScoreDataset.head()","b6440865":"### It is important to know the original conversion rate of the company before we proceed. Let us calculate that\noriginalConversionRate = round((sum(leadScoreDataset['Converted'])\/len(leadScoreDataset['Converted'].index))*100, 2)\nprint(\"The conversion rate of leads is: \",originalConversionRate)","ce19a70f":"leadScoreDataset.shape","a4b5477e":"leadScoreDataset.info()","54cda156":"leadScoreDataset.describe()","8cb2b3bb":"leadScoreDataset = leadScoreDataset.replace('Select', np.nan)\nleadScoreDataset.head()","b2fbfa84":"### Dropping rows with duplicate values based on unique 'Prospect ID' & for 'Lead Number' for each candidate\nprint('Are there NO duplicates present in Prospect Id column? ', sum(leadScoreDataset.duplicated('Prospect ID')) == 0)\nprint('Are there NO duplicates present in Lead Number Column? ', sum(leadScoreDataset.duplicated('Lead Number')) == 0)\n","2696015f":"generateNullValuesPercentageTable(leadScoreDataset)","a569314e":"### Dropping columns with null values over 70%\ndroppedColumns = leadScoreDataset.columns[leadScoreDataset.isnull().mean() > 0.70]\nleadScoreDatasetAfterDroppedColumns = leadScoreDataset.drop(droppedColumns, axis = 1)\n\nprint('The new shape of the dataset after dropping the columns is: ', leadScoreDatasetAfterDroppedColumns.shape)\n\n### analysing the dataframe is correct after dropping the columns\nleadScoreDatasetAfterDroppedColumns.head()","9e640f6c":"### Checking the number of unique values per column\nleadScoreDatasetAfterDroppedColumns.nunique()","467eeb45":"leadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.loc[:, leadScoreDatasetAfterDroppedColumns.nunique()!=1]\nleadScoreDatasetAfterDroppedColumns.shape","3f913ffc":"leadScoreDatasetAfterDroppedColumns.head()","796d98c0":"### Let us see the frequency of the different values present in the 'Lead Quality' column\nleadScoreDatasetAfterDroppedColumns['Lead Quality'].value_counts()","ce5ca137":"### Since 'Lead Quality' is based on an employees intuition, let us inpute any NAN values with 'Not Sure' and take counts again\nleadScoreDatasetAfterDroppedColumns['Lead Quality'] = leadScoreDatasetAfterDroppedColumns['Lead Quality'].replace(np.nan, 'Not Sure')\nleadScoreDatasetAfterDroppedColumns['Lead Quality'].value_counts()","419344b5":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Lead Quality', (10,10))","594d59eb":"### We will plot box plots and count plots repectively\n\nfig, axs = plt.subplots(2,2, figsize = (10,7.5))\nplt1 = sns.countplot(leadScoreDatasetAfterDroppedColumns['Asymmetrique Activity Index'], ax = axs[0,0])\nplt2 = sns.boxplot(leadScoreDatasetAfterDroppedColumns['Asymmetrique Activity Score'], ax = axs[0,1])\nplt3 = sns.countplot(leadScoreDatasetAfterDroppedColumns['Asymmetrique Profile Index'], ax = axs[1,0])\nplt4 = sns.boxplot(leadScoreDatasetAfterDroppedColumns['Asymmetrique Profile Score'], ax = axs[1,1])\nplt.tight_layout()","f0794599":"colsToDrop = ['Lead Quality', 'Asymmetrique Activity Index','Asymmetrique Activity Score','Asymmetrique Profile Index','Asymmetrique Profile Score']\nleadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop(colsToDrop,axis =1)\n\nleadScoreDatasetAfterDroppedColumns.head()","da21165c":"leadScoreDatasetAfterDroppedColumns.shape","e99351f8":"### Let us now assess the percentage of missing values in the remaining dataframe\ngenerateNullValuesPercentageTable(leadScoreDatasetAfterDroppedColumns)","609db238":"### Exploring 'City' column\n\nleadScoreDatasetAfterDroppedColumns.City.describe()","17da4c16":"leadScoreDatasetAfterDroppedColumns.City.value_counts(normalize=True)","1893b4f6":"## From the above we can see that the value 'Mumbai' has the most number of enteries\n## Let us plot the same\n\ngenerateBarPlot(leadScoreDatasetAfterDroppedColumns, 'City', (10,5))","8c46359c":"leadScoreDatasetAfterDroppedColumns.City = leadScoreDatasetAfterDroppedColumns.City.fillna('Mumbai')","e17c63e2":"leadScoreDatasetAfterDroppedColumns.City.value_counts(normalize=True)","62192eb4":"### Exploring 'Specialization' column which hs 36.58% null values\nleadScoreDatasetAfterDroppedColumns.Specialization.describe()","a8bf5e5b":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Specialization', (20,20))","b7a8308e":"### Replacing missing values with 'Others'\n\nleadScoreDatasetAfterDroppedColumns.Specialization = leadScoreDatasetAfterDroppedColumns.Specialization.fillna('Others')\n\nleadScoreDatasetAfterDroppedColumns.Specialization.value_counts(normalize=True)","f62ff04a":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Specialization', (20,20))","9c77efd7":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Specialization', 'Conversion based on Specialization',(40,30), 'Converted')","b5e63d5a":"### The last column with a high percentage of null values is Tags. Let us explore this column\n\nleadScoreDatasetAfterDroppedColumns.Tags.describe()","dea74592":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Tags', (20,20))","4927e460":"leadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop('Tags', axis=1)\n\nleadScoreDatasetAfterDroppedColumns.head()","f9035391":"leadScoreDatasetAfterDroppedColumns.shape","32df0d46":"### Let us check the null percentage of the dataframe now\ngenerateNullValuesPercentageTable(leadScoreDatasetAfterDroppedColumns)","f42abe18":"leadScoreDatasetAfterDroppedColumns['What matters most to you in choosing a course'].value_counts(normalize=True)","fb480780":"leadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop('What matters most to you in choosing a course', axis=1)\nleadScoreDatasetAfterDroppedColumns.head()","10debf14":"leadScoreDatasetAfterDroppedColumns.shape","a2f81fcf":"leadScoreDatasetAfterDroppedColumns['What is your current occupation'].value_counts(normalize=True)","bd8c7196":"leadScoreDatasetAfterDroppedColumns['What is your current occupation'] = leadScoreDatasetAfterDroppedColumns['What is your current occupation'].fillna('Unemployed')\nleadScoreDatasetAfterDroppedColumns['What is your current occupation'].value_counts(normalize=True)","98be5771":"leadScoreDatasetAfterDroppedColumns.Country.value_counts().head(5)","1c02530d":"leadScoreDatasetAfterDroppedColumns.Country = leadScoreDatasetAfterDroppedColumns.Country.fillna('India')\nleadScoreDatasetAfterDroppedColumns.Country.value_counts().head(5)","ef0084e8":"leadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop('Country', axis=1)\nleadScoreDatasetAfterDroppedColumns.head()","f05eb7d6":"leadScoreDatasetAfterDroppedColumns.shape","d7af2f15":"generateNullValuesPercentageTable(leadScoreDatasetAfterDroppedColumns)","b736b366":"### Imputing missing values in Lead Source column\nleadScoreDatasetAfterDroppedColumns['Lead Source'].value_counts()","36c6f32a":"### Imputing missing values with 'Google'\n\nleadScoreDatasetAfterDroppedColumns['Lead Source'] = leadScoreDatasetAfterDroppedColumns['Lead Source'].fillna('Google')\nleadScoreDatasetAfterDroppedColumns['Lead Source'].value_counts()","c225837a":"leadScoreDatasetAfterDroppedColumns['Lead Source'] =  leadScoreDatasetAfterDroppedColumns['Lead Source'].apply(lambda x:x.capitalize())\n\nleadScoreDatasetAfterDroppedColumns['Lead Source'].value_counts()","620e8ec2":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Lead Source', (15,10))","df784f4e":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Lead Source', 'Conversion based on Lead Source', (50,30),'Converted')","b1f62a78":"colsToReplace=['Click2call', 'Live chat', 'Nc_edm', 'Pay per click ads', 'Press_release',\n  'Social media', 'Welearn', 'Bing', 'Blog', 'Testone', 'Welearnblog_home', 'Youtubechannel']\nleadScoreDatasetAfterDroppedColumns['Lead Source'] = leadScoreDatasetAfterDroppedColumns['Lead Source'].replace(colsToReplace, 'Others')","da409c7f":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Lead Source', (15,10))","fae072e1":"leadScoreDatasetAfterDroppedColumns['Lead Source'].value_counts()","d60daa24":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Lead Source', 'Conversion based on Lead Source', (40,20),'Converted')","8f60d99b":"### Imputing values in Last Activity column\nleadScoreDatasetAfterDroppedColumns['Last Activity'].value_counts()","a96a694f":"#### Imputing the missing values with 'Email Opened'\nleadScoreDatasetAfterDroppedColumns['Last Activity'] = leadScoreDatasetAfterDroppedColumns['Last Activity'].fillna('Email Opened')\nleadScoreDatasetAfterDroppedColumns['Last Activity'].value_counts()","ff410e1d":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Last Activity', (15,10))","55af9509":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Last Activity', 'Conversion based on Last Activity', (40,30),'Converted')","21ec52c1":"### Imputing values in Page Views Per Visit column\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].value_counts().head(15)","b0983209":"leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].describe()","d3d07147":"#### Imputing the missing values with '2.0' which is the median value\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'] = leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].replace(np.nan,'2.0')\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].value_counts().head(15)","5ecb125e":"### Looks like 0.0 was incorrectly imputed. Let us correct the imputation\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'] =  leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].astype(float)\n\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].value_counts()","06712a15":"### It looks like there are a lot of outliers in the data, let us verify this using a boxplot\nfig, axs = plt.subplots(figsize = (10,10))\nsns.boxplot(leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'])","e27651ea":"### We will cap our data at the 1% & 95% mark so as to not lose any values or drop rows. \ncapValue = leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].quantile([0.01,0.95]).values\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'][leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'] <= capValue[0]] = capValue[0]\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'][leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'] >= capValue[1]] = capValue[1]\n\nleadScoreDatasetAfterDroppedColumns['Page Views Per Visit'].describe(percentiles=[0.01,.25, .5, .75, .90, .95, .99])","70778aec":"fig, axs = plt.subplots(figsize = (10,10))\nsns.boxplot(leadScoreDatasetAfterDroppedColumns['Page Views Per Visit'])","be0eb857":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Page Views Per Visit', (25,20))","1b5a9ddd":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Page Views Per Visit', 'Page Views Per Visit vs Conversion', (40,40), 'Converted')","c440fa14":"leadScoreDatasetAfterDroppedColumns.TotalVisits.describe()","0881c6ac":"### We will impute this value with the meadian value since the \n### mean and the median values are relatively close to each other\nleadScoreDatasetAfterDroppedColumns.TotalVisits = leadScoreDatasetAfterDroppedColumns.TotalVisits.fillna('3.0')\nleadScoreDatasetAfterDroppedColumns.TotalVisits.value_counts()","1bbdfde4":"### Looks like 3.0 was incorrectly imputed. Let us correct the imputation\nleadScoreDatasetAfterDroppedColumns['TotalVisits'] =  leadScoreDatasetAfterDroppedColumns['TotalVisits'].astype(float)\n\nleadScoreDatasetAfterDroppedColumns['TotalVisits'].value_counts()","405248e0":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'TotalVisits',(30,10))","99551804":"leadScoreDatasetAfterDroppedColumns['TotalVisits'].describe(percentiles=[0.01,.25, .5, .75, .90, .95, .99])","a885060e":"### There seem to be a large number of outliers. Let us check these using a boxplot and decide what to do next\nfig, axs = plt.subplots(figsize = (10,10))\n\nsns.boxplot(data=leadScoreDatasetAfterDroppedColumns.TotalVisits)\n","6de3197e":"### We will cap our data at the 1% & 95% mark so as to not lose any values or drop rows. \ncapValue = leadScoreDatasetAfterDroppedColumns.TotalVisits.quantile([0.01,0.95]).values\nleadScoreDatasetAfterDroppedColumns.TotalVisits[leadScoreDatasetAfterDroppedColumns.TotalVisits <= capValue[0]] = capValue[0]\nleadScoreDatasetAfterDroppedColumns.TotalVisits[leadScoreDatasetAfterDroppedColumns.TotalVisits >= capValue[1]] = capValue[1]\n\nleadScoreDatasetAfterDroppedColumns.TotalVisits.describe(percentiles=[0.01,.25, .5, .75, .90, .95, .99])","b763b40c":"fig, axs = plt.subplots(figsize = (10,10))\n\nsns.boxplot(data=leadScoreDatasetAfterDroppedColumns.TotalVisits)\n","bf9db5f0":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'TotalVisits', 'Total Visits vs Conversion', (40,20), 'Converted')","456d2b54":"### Assessing if there are any more missing values in the data\ngenerateNullValuesPercentageTable(leadScoreDatasetAfterDroppedColumns)","9895c4a0":"### We can also drop the column 'Prospect ID' as we already have an identifying column with unique values: 'Lead Number'\nleadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop('Prospect ID', axis=1)\nleadScoreDatasetAfterDroppedColumns.head()","e2a11e48":"### Checking the shape of the dataset before beginning any further analysis\nleadScoreDatasetAfterDroppedColumns.shape","058a91d1":"### Identifying the remaining columns and their datatypes before proceeding\nleadScoreDatasetAfterDroppedColumns.info()","99631f6f":"leadScoreDatasetAfterDroppedColumns.describe()","e8dc4467":"leadScoreDatasetAfterDroppedColumns['Lead Origin'].value_counts()","734fbaa4":"generateBarPlot(leadScoreDatasetAfterDroppedColumns,'Lead Origin', (10,10))","f1b7ad6e":"generateBiplot(leadScoreDatasetAfterDroppedColumns,'Lead Origin', 'Conversion Based on Lead Origin', (40,20),'Converted')","d9738b9c":"leadScoreDatasetAfterDroppedColumns['Do Not Email'].value_counts()","a4b32a4c":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Do Not Email',(10,10))","42391148":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Do Not Email', 'Do Not Email vs Conversion', (40,20),'Converted')","e57f3457":"leadScoreDatasetAfterDroppedColumns['Do Not Call'].value_counts()","e09685d7":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Do Not Call',(10,10))","8e2a4c3e":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Do Not Call', 'Conversions vs Do Not Call', (40,20),'Converted')","3748a04e":"leadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop(['Do Not Call', 'Do Not Email'], axis=1)\nleadScoreDatasetAfterDroppedColumns.shape","e86dc40b":"leadScoreDatasetAfterDroppedColumns.info()","41c6a41d":"leadScoreDatasetAfterDroppedColumns.Search.describe()","4eebca0a":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Search',(10,10))","911ad8e7":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Search', 'Search vs Conversion',(40,20), 'Converted')","3a884ad6":"leadScoreDatasetAfterDroppedColumns['Newspaper Article'].describe()","e263eddd":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Newspaper Article',(10,10))","f0f1ae7a":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Newspaper Article', 'Newspaper Article vs Conversion',(40,20), 'Converted')","a0ba9f5e":"leadScoreDatasetAfterDroppedColumns['X Education Forums'].value_counts()","8ac1eb2a":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'X Education Forums',(10,10))","57fe50e0":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'X Education Forums', 'X Education Forums vs Conversion', (40,20),'Converted')","195e1614":"leadScoreDatasetAfterDroppedColumns['Newspaper'].value_counts()","b15f459a":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Newspaper',(10,10))","ed518470":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Newspaper', 'Newspaper vs Conversion', (40,20),'Converted')","29e1f461":"leadScoreDatasetAfterDroppedColumns['Digital Advertisement'].value_counts()","a84ca3ed":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Digital Advertisement',(10,10))","b4e48a0f":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Digital Advertisement', 'Digital Advertisement vs Conversion', (40,20),'Converted')","b36e6911":"leadScoreDatasetAfterDroppedColumns['Through Recommendations'].value_counts()","8f9317d3":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Through Recommendations',(10,10))","423f5823":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Through Recommendations', 'Through Recommendations vs Conversion', (40,20),'Converted')","782f8156":"dropCols = ['Search', 'Newspaper', 'X Education Forums', 'Newspaper Article' , 'Digital Advertisement','Through Recommendations']\nleadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop(dropCols, axis=1)\nleadScoreDatasetAfterDroppedColumns.head()","482fde88":"leadScoreDatasetAfterDroppedColumns.shape","e82c875d":"leadScoreDatasetAfterDroppedColumns.info()","71ef6574":"leadScoreDatasetAfterDroppedColumns['A free copy of Mastering The Interview'].value_counts()","ed66cc17":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'A free copy of Mastering The Interview',(10,10))","edda1f38":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'A free copy of Mastering The Interview', 'A free copy of Mastering The Interview vs Conversion', (40,20),'Converted')","397d39bb":"leadScoreDatasetAfterDroppedColumns.shape","30f85f1f":"leadScoreDatasetAfterDroppedColumns['Last Notable Activity'].describe()","de9c9627":"leadScoreDatasetAfterDroppedColumns['Last Notable Activity'].value_counts()","7e4fc7af":"generateBarPlot(leadScoreDatasetAfterDroppedColumns, 'Last Notable Activity',(20,10))","8b024c78":"generateBiplot(leadScoreDatasetAfterDroppedColumns, 'Last Notable Activity', 'Last Notable Activity vs Conversion', (40,20),'Converted')","9e8ba4f4":"leadScoreDatasetAfterDroppedColumns = leadScoreDatasetAfterDroppedColumns.drop('Last Notable Activity', axis=1)\nleadScoreDatasetAfterDroppedColumns.head()","ce750b31":"leadScoreDatasetAfterDroppedColumns.shape","3968bb2a":"leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website'].describe()","f0d54b4f":"leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website'].value_counts()","d563bfa9":"### Let us generate a distplot to view the split of this data\nsns.distplot(leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website'])\nplt.show()","eef3d302":"leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website'] = leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website'].apply(lambda x: round((x\/60), 2))\nsns.distplot(leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website'])\nplt.show()","081a0dcd":"leadScoreDatasetAfterDroppedColumns.head()","c6c7f8b9":"# Let us split our dataframe to perform better analysis\ntimeSpentMoreThan1HourDF=leadScoreDatasetAfterDroppedColumns[leadScoreDatasetAfterDroppedColumns['Total Time Spent on Website']>=1.0]\ntimeSpentMoreThan1HourDF[\"Hours Spent\"]= timeSpentMoreThan1HourDF[\"Total Time Spent on Website\"].astype(int)\n\ntimeSpentMoreThan1HourDF.head()\n","614ea103":"### Let us generate a bivariate analysis bar plot to better understand our conversions\n\ngenerateBiplot(timeSpentMoreThan1HourDF, 'Hours Spent', 'Last Notable Activity vs Conversion', (40,40),'Converted')","b0dce6b8":"plt.figure(figsize=(40,40))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =timeSpentMoreThan1HourDF, x='TotalVisits',y='Total Time Spent on Website', hue ='Converted',orient='v')\nplt.title('Total Time Spent Vs Total Visits based on Conversion')\nplt.show()","c316fc75":"generateNullValuesPercentageTable(leadScoreDatasetAfterDroppedColumns)","35879c54":"leadScoreDatasetAfterDroppedColumns.head()","53991cd2":"### Assessing current dataframe\nleadScoreDatasetAfterDroppedColumns.info()","b90d8848":"### Let us assess the correlation between the existing variables to rule out any colinearity \ngenerateHeatmaps(leadScoreDatasetAfterDroppedColumns, (20,20))","f89f1483":"### First we will convert the Yes\/No values in the 'A free copy of Mastering The Interview' column to 1\/0\n\nleadScoreDatasetAfterDroppedColumns['A free copy of Mastering The Interview'] = leadScoreDatasetAfterDroppedColumns['A free copy of Mastering The Interview'].map(dict(Yes=1, No=0))\nleadScoreDatasetAfterDroppedColumns.head()","c3109804":"leadScoreDatasetAfterDroppedColumns.shape","fd2c8033":"### Creating dummies\ndummyCols = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City']\ndummyDataset = pd.get_dummies(leadScoreDatasetAfterDroppedColumns[dummyCols],drop_first=True)\ndummyDataset.head()","40f90eb5":"dummyDataset.shape","43077bb1":"### Combining dummies with the original dataset into a new dataset\n\ncombinedDummyDataset = leadScoreDatasetAfterDroppedColumns.copy()\ncombinedDummyDataset.head()","3bee9499":"combinedDummyDataset.shape","9da058f5":"### combining datasets\ncombinedDummyDataset = pd.concat([combinedDummyDataset, dummyDataset], axis=1)\ncombinedDummyDataset.head()","71579a1d":"combinedDummyDataset.shape","1e95882c":"### We will now drop the original columns and the columns that have 'Others' as a sub heading since we had \n### combined various values to create those columns\n\ndummiesColsToDrop = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City',\n                     'Lead Source_Others','Specialization_Others']\ncombinedDummyDataset = combinedDummyDataset.drop(dummiesColsToDrop, axis=1)\ncombinedDummyDataset.head()","910f4546":"combinedDummyDataset.shape","245f8b97":"combinedDummyDataset.info()","8b828287":"### First we will drop the Converted & Lead Number columns \n### We will create another copy of our model and use that for this\n\nX = combinedDummyDataset.drop(['Converted','Lead Number'], axis=1)\nX.head()","05ff1f51":"X.shape","5820bbe6":"### Adding the target variable 'Converted' to y\ny = combinedDummyDataset['Converted']\n\ny.head()","7333102d":"# Splitting the data into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","f73b2f33":"### Now let us begin scaling features. First let us assess our training dataset\nX_train.head()","c67e3e32":"### Scaling \nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\n\nX_train.head()","6bf6fbdc":"X_train.shape","afbd209e":"generateHeatmaps(combinedDummyDataset,(30,30))","8f9c2460":"### Dropping highly correlated variables\nX_train = X_train.drop(['Lead Origin_Lead Add Form', 'Lead Source_Facebook'], axis=1)\nX_test = X_test.drop(['Lead Origin_Lead Add Form', 'Lead Source_Facebook'], axis=1)","862f9c91":"X_train.head()","21aa293d":"X_train.shape","3d6b42d7":"X_test.head()","c6c81c70":"X_test.shape","2e8116f2":"generateHeatmaps(combinedDummyDataset[X_train.columns],(30,30))","664fee2d":"## Creating Logistic Regression Model\n\nlogisticRegressionModel = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogisticRegressionModel.fit().summary()","4e48e98c":"### RFE with 20 variables\nlogreg = LogisticRegression()\n\nrfe20 = RFE(logreg, 20)\nrfe20= rfe20.fit(X_train,y_train)\nrfe20.support_","138c8ce4":"list(zip(X_train.columns, rfe20.support_, rfe20.ranking_))","2cdeab84":"col = X_train.columns[rfe20.support_]","fb6142b1":"X_train.columns[~rfe20.support_]","ffcaef13":"X_train_cols = X_train[col]\nX_train_cols","361122aa":"X_train_sm = sm.add_constant(X_train_cols)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()\n","8134c74b":"### Checking VIF values\n\nvifCalculator(X_train_cols)","0b805db4":"X_train_cols = X_train_cols.drop('What is your current occupation_Housewife', axis=1)\nX_train_cols.columns","1cabbffc":"### Rerun the model with the selected variables\nX_train_sm = sm.add_constant(X_train_cols)\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","9ca628ee":"vifCalculator(X_train_cols)","5c143d44":"X_train_cols = X_train_cols.drop('Last Activity_Resubscribed to emails', axis=1)\nX_train_cols.columns","cd8b9730":"X_train_sm = sm.add_constant(X_train_cols)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","318607a3":"vifCalculator(X_train_cols)","f24bba07":"X_train_cols = X_train_cols.drop('Lead Origin_Lead Import', axis=1)\nX_train_cols.columns","c92c39a0":"X_train_sm = sm.add_constant(X_train_cols)\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","4f0f85f0":"vifCalculator(X_train_cols)","5961628f":"X_train_cols = X_train_cols.drop('What is your current occupation_Working Professional', axis=1)\nX_train_cols.columns \n","2714c2e7":"X_train_sm = sm.add_constant(X_train_cols)\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","a6f8a87e":"vifCalculator(X_train_cols)","7d9ef4dd":"X_train_cols = X_train_cols.drop('Specialization_Rural and Agribusiness', axis=1)\nX_train_cols.columns ","48b88c41":"X_train_sm = sm.add_constant(X_train_cols)\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","4040da9e":"vifCalculator(X_train_cols)","ed20ba07":"generateHeatmaps(X_train_sm, (20,20))","6c766258":"y_train_pred = res.predict(X_train_sm)\ny_train_pred.head()","47b8409e":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","d451fa0b":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Lead_Score_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","531b3450":"y_train_pred_final['Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\n\n\ny_train_pred_final.head()","46deb3fd":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_train_pred_final['Lead_Score'] = y_train_pred_final['Lead_Score'].astype(int)\n\n# Let's see the head\ny_train_pred_final.head()","13657c5e":"# Confusion matrix \n\nfrom sklearn import metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead )\nprint(confusion)\n","d5711d9e":"# Let's check the overall accuracy.\nprint(round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead),2))","830edcef":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","fca9cd7f":"# Let's see the sensitivity of our logistic regression model\nround((TP \/ float(TP+FN)),2)","f14c84ab":"# Let us calculate specificity\nround((TN \/ float(TN+FP)),2)","f4394efb":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob, drop_intermediate = False )","a5fa1ec7":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob)","3cec40f2":"# Let's create columns with different probability cutoffs \n\nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","6b9b9b83":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificty'])\n\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","aad31331":"# Let's plot accuracy sensitivity and specificity for various probabilities\n\nsns.set_style('whitegrid')\nsns.set_context('paper')\n\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificty'])\nplt.xticks(np.arange(0,1,step=.05), size=8)\nplt.yticks(size=12)\nplt.show()","52e09b74":"y_train_pred_final['Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map( lambda x: 1 if x > 0.33 else 0)\n\ny_train_pred_final.head()","8879f364":"round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead),2)","b243b6d7":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted_Hot_Lead )\nconfusion2","3550407e":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","7fabbb88":"# Let's see the sensitivity of our logistic regression model\nround(TP \/ float(TP+FN),2)","8e53868f":"# Let us calculate specificity\nround(TN \/ float(TN+FP),2)","9c00e5ae":"### Calculating Precision\nprecision =round(TP\/float(TP+FP),2)\nprecision","e9e87381":"### Calculating Recall\nrecall = round(TP\/float(TP+FN),2)\nrecall","dc0379e7":"### Let us generate the Precision vs Recall tradeoff curve \np ,r, thresholds=precision_recall_curve(y_train_pred_final.Converted,y_train_pred_final['Lead_Score_Prob'])\nplt.title('Precision vs Recall tradeoff')\nplt.plot(thresholds, p[:-1], \"g-\")    # Plotting precision\nplt.plot(thresholds, r[:-1], \"r-\")    # Plotting Recall\nplt.show()","3df9df28":"### The F statistic is given by 2 * (precision * recall) \/ (precision + recall)\n## The F score is used to measure a test's accuracy, and it balances the use of precision and recall to do it.\n### The F score can provide a more realistic measure of a test's performance by using both precision and recall\nF1 =2 * (precision * recall) \/ (precision + recall)\nround(F1,2)","83c8ed56":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","30403a9b":"X_train_cols.shape","8d8d95c9":"X_test = X_test[X_train_cols.columns]\n\nX_test.shape","d92bc869":"X_test.head()","92cba1b1":"X_test_sm = sm.add_constant(X_test)","f070c2e3":"y_test_pred = res.predict(X_test_sm)","6611c994":"y_test_pred[:10]","865270e2":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","6b8f4cac":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","407310b1":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","48faf41d":"# Removing index for both dataframes to append them side by side \n\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","0ff9b3c6":"# Appending y_test_df and y_pred_1\n\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","099ec035":"y_pred_final.head()","1d89c03d":"# Renaming the column \n\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Lead_Score_Prob'})","45848a45":"# Rearranging the columns\n\ny_pred_final = y_pred_final.reindex(['CustID','Converted','Lead_Score_Prob'], axis=1)","bd3249a3":"# Adding Lead_Score column\n\ny_pred_final['Lead_Score'] = round((y_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_pred_final['Lead_Score'] = y_pred_final['Lead_Score'].astype(int)","66e27e83":"# Let's see the head of y_pred_final\ny_pred_final.head()","e0b0ba81":"y_pred_final['Final_Predicted_Hot_Lead'] = y_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.33 else 0)","6047adbb":"y_pred_final.head()","0f0dd171":"# Let's check the overall accuracy.\nround(metrics.accuracy_score(y_pred_final.Converted, y_pred_final.Final_Predicted_Hot_Lead),2)","1a67a300":"confusion4 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Final_Predicted_Hot_Lead )\nconfusion4","e48c35b3":"TP = confusion4[1,1] # true positive \nTN = confusion4[0,0] # true negatives\nFP = confusion4[0,1] # false positives\nFN = confusion4[1,0] # false negatives","5fe583ad":"# Let's see the sensitivity of our logistic regression model\nround((TP \/ float(TP+FN)),2)","4983dedc":"# Let us calculate specificity\nround(TN \/ float(TN+FP),2)","6fc7a906":"### Generating table\nresultingTable = pd.merge(y_pred_final,leadScoreDataset,how='inner',left_on='CustID',right_index=True)\nresultingTable[['Lead Number','Lead_Score']].head()","9b4cdda2":"resultingTable.shape","77b92e99":"## Key Insights\n\n- Hot Leads are identified as 'Customers having lead score of 33 or above'\n- Sales Team of the company should first focus on the 'Hot Leads'  \n- Higher the Lead Score, higher the chances of conversion of 'Hot Leads' into 'Paying Customers'\n- The 'Cold Leads'(Customer having lead score < 33) should be focused after the Sales Team is done with the 'Hot Leads'","a8cc383b":"### Observation\n\n- The p value for Lead Origin_Lead Import is very high at 0.517 & above the threshold 0.05\n\nWe will be dropping Lead Origin_Lead Import in the next model","9310b098":"### Note\n\nOur Exploratory Data Analysis and outlier analysis is now complete.\n\nOur data is free of outliers and any issues that might have prevented our model from performing the best it can. \n\nLet us now asses the final data frame and create a new dataframe for the dummies. ","ec98e576":"### Observations\n\nFrom the above analysis we can conclude that:\n\n- People who make more than 10 visits are almost 50% likely to apply for a course\n- Only 15% of people who visited the website converted to student. This could imply that people weren't able to gather all the information they needed easily. Hence, decided not to opt for any course. ","cf192fd3":"# Lead Score Case Study\n\n### Problem Statement\n\n- An education company named X Education sells online courses to industry professionals.The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted.\n\n- X Education wants it's Data Analyst team to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers.\n\n- The company requires its Data Analyst team to build a model wherein they need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.","e11ef3fb":"#### A free copy of Mastering The Interview","c324a7b5":"## Performing Train - Test Split","ab88435f":"#### Through Recommendations","ba008f23":"### Observation\n\nThe model seems to be coming well under control however:\n\n- The p value for Specialization_Rural and Agribusiness is slightly above the threshold at 0.5\n\nWe will be dropping this variable in the next model","6b692bac":"#### Lead Origin","18addf13":"#### What matters most to you in choosing a course\t","0e2b08a0":"#### Observation\nWe have successfully capped the data at the 95%.\nFrom the above plots it is safe to infer that: \n\n- People who dont visit any pages have the highest count of conversion overall\n- Less than half the people who visit 2 pages on average convert to students","b52ec265":"#### Last Notable Activity \n\nFrom the data dictionary it looks like this column is very simmilar to the 'Last Activity' column. We will do some analysis on this and if found similar we will drop this column. ","51aea475":"### Observation\n\nWe shall impute the missing values with 'Mumbai' since it has the highest count. ","db35c33e":"#### Lead Source","17e7e5a6":"### Observation \n\n- The VIF values for all the variables in the model look to be under control\n- The p value for What is your current occupation_Housewife is very high at 0.999 & above the threshold 0.05\n\nWe will be dropping What is your current occupation_Housewife in the next model ","5d6911da":"### City","641a67a6":"### Observation \n\nThere is a lot of variation in the data and the number of null values is also very high at 45.65%. Therefore we will drop these columns. \n\nLet us now drop all the 5 columns: Lead quality, Asymmetrique Activity Index, Asymmetrique Activity Score, Asymmetrique Profile Index and Asymmetrique Profile Score. \n\n","b1101313":"### Observation\n\nModel 7 meets all our criteria:\n\n- The VIF values are under 3\n- The p values are under 0.05\n- The 15 selected features look significant\n\nLet us generate a heatmap to confirm that there is no multicollinearity ","eed50d57":"### Plotting ROC Curve\nAn ROC curve demonstrates several things:\n\nIt shows the trade-off between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\nThe closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\nThe closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","c71233a4":"### Observation\n\nAs you can see the shape of our data set has changed from 9240x35 after dropping the first 2 columns to 9240x30 after dropping the other 5. \n\nWe have also verified that the correct columns have been dropped visually above using the head() command.","f8117cf7":"### Observation\n\nNow that there are no missing values in the dataset we can proceed with our next analysis.","37ea5faa":"# End of File","67207b56":"### Importing Data","9085ac5e":"#### Note: \n\nFrom our analysis of the data dictionary we can see that the folloowing columns: Search, Newspaper Article, Education Forums, Newspaper , Digital Advertisement and Through Recommendations, are already represented in the 'Lead Source' column. \n\nWe will carry out basic univariate analysis on them and make a decision on if we need to drop them or not. ","f2342cd5":"### Observation\n\nAs we can confirm with our heatmap, there is no multicollinearity in the model.\n\nWe will take model 7 as our final model for the condition where RFE has 15 variables. Which is less than the original RFE count of 20.","391ac245":"#### Observation\n\nWe can clearly see that this column is heavily skewed towards better career prospects. Since it doesn't really provide any more information, we can drop this column and keep note that all candidates that take this course are looking to have a better career. ","59be6521":"### Note\nWe will be dropping the columns Do Not Email & Do Not Call as the data is highly skewed towards the No section. These two columns can be take as safe assumptions by company X that 99% of their prospective customers do not like to be called or receive emails. ","d4bad25d":"# 5. Conclusion\n\nWe conducted this case study for X Education with the objective of providing them insights to successfully convert leads they received into students of the platform. Our goal was to generate a machine learning based model that identifies the highest probability of converting customers to students. \n\nWe started this by assessing the data and identifying various columns, understanding what they mean by assessing the data dictionary. We initially saw that there were a large number of 'Select' values spread across the dataset. These values meant that the user had made no selection in those fields. We decided to replace these values with NaN values and treated them appropriately later. Then we conducted an analysis on the percentage of null columns in the dataset, tackling the ones with the highest percentage of null values first. We removed columns that had over 70% null values and for the remaining columns assessed them individually. \n\nWhile assessing our columns individually we found that various columns were actually summarized into one column already. Therefore, it did not make sense for us to keep these columns and we decided to drop them entirely. Examples of such columns are Search, Newspaper Article etc., they are already represented in the 'Lead Source' column. The  distribution represented in these individual columns was very well represented by the data in the Lead Source column. There were a few columns that had highly skewed data, i.e. data pointing in one direction only. The Country, What matters most to you in choosing a course, are a few examples. Most of the leads, 95% and above, mentioned that they were from India and were looking for better career prospects. We dropped these columns as well. \nThe tendency of skewed data to sway the model heavily towards its direction makes the model incapable of predicting the results correctly. \n\nWhile analyzing the various columns we performed univariate and bivariate analysis on them. Bivariate analysis was carried out with the Converted column as a benchmark. This analysis yielded some very important insights that we have mentioned below. Key being that the longer the user stayed on the website, the higher the chances of them converting.\n\nOf particular interest to us was the 'Total Time Spent on The Website' column. This column had highly varied data that we had to properly convert to correct metrics to make better sense of it. \n\nCertain columns had a large mix of values, some outliers and a small number of null values. We had to perform appropriate outlier & null value treatment for each of these. In certain cases such as that of the Specialization column, we could not have taken the Mode value to impute the null columns. This is because  we had to consider the fact that the mentioned options in the form might not have represented the applicants specialization correctly. We decided to club these values into one field and later assess them. \nIn the case of numerical columns like TotalVisits we imputed the null values with the median value. This is because the difference between the median and mean was very less. We also capped any outliers present instead of dropping them so that all the rows of data are retained. Capping was done with by replacing the lowest values with the 1%ile & highest with the 95%ile value in the column. \n\nHaving conducted our EDA and prepared our data free of any anomalies we proceeded to preparing our data for the Logistic Regression Model.\n\nWe created dummy variables from our final 12 variables and correctly dropped all the original columns, other category variables that we had created. Then we performed the train-test split using the 70-30 method for splitting. \nUsing the StandardScaler we scaled our numerical columns so that all the variables follow similar units. Scaling helps us in standardizing our dataset and preventing any features that have higher units to skew the model in its favor. We assessed the split datasets and plotted a correlation heatmap to identify any variables with high collinearity. We found 2 such variables and dropped them from both the training and test sets. \n\nFinally, we proceeded to creating our logistic regression model.\nFirstly, it is important that we define our model acceptance criteria:\n- The model does not over-fit\n- The model is simple enough to be understood\n- The model is built using significant features.\n- The VIF value is under 3 & the p value is under 0.05 for each feature\n- The accuracy, sensitivity and specificity of our model after test are at least 80%(+- 1% between all 3 parameters)\n\n\nWe created a basic logistic regression model with all our features from the scaled training dataset.\nThe GLM summary report from this model provided us the base benchmarks for our model. Based on the above criteria we performed RFE with 20 variables and began creating and the models. We eliminated any variables that had high p values and VIF values. Eventually we generated a model with 15 variables that we performed training and testing on.\n\nOn both our training and testing models we predicted the probability score for converting the leads and correctly added them to a new table along with the customer id. Once we had our scores and probability of converting a lead, we checked for the accuracy, specificity, sensitivity, precision and recall. Our model performed very well on these statistics. WE generated the ROC curve and the probability cutoff curve to find our optimal cutoff, which is 0.33. \n\nAssessing our model at the optimal cutoff we saw that our model satisfied our condition of 80% sensitivity. \nWe then compared then ran our model against our test set and achieved a similar result of 80% sensitivity.\nThus confirming our model is correct and completing the modeling. \n\nFinally, we generated the table which contains lead scores for the leads in the original dataset. These scores can now be used to convert leads to students of the platform. The bench mark being that the leads above a score of 33 have a higher chance of converting. The higher the score, the better the chances of conversion.\n\nThrough the course of this case study for X Education we have identified the following key aspects:\n\n- Most applicants would like to join a course to have better career prospects\n- X Education has the highest conversion rate of individuals who are referred to them\n- Overall it is safe to say that the more time the user spends on the website, the better their chances of becoming a student.\n- Hot Leads are identified as 'Customers having lead score of 33 or above'\n- Sales Team of the company should first focus on the 'Hot Leads'  \n- Higher the Lead Score, higher the chances of conversion of 'Hot Leads' into 'Paying Customers'\n- The 'Cold Leads'(Customer having lead score < 33) should be focused after the Sales Team is done with the 'Hot Leads'","bcbd1e08":"### Observation\n\nFrom the above scores we note that our model has a good overall relevancy, defined by Precision, at 71% & a great return of relevant results, defined by Recall, at 80%.\n\n\nFor the purposes of our model we will focus on the Recall result as we would like to miss out on any hot leads that are willing to be converted. ","8f76487f":"#### Note:\nNow that we have identified the above 12 columns, let us generate dummies for the same\n","ff9d8b07":"# 2. Data Preparation\n\nWe will begin our data prepartation for the Logistic Regression model by:\n\n- Creating Dummy variables for categorical columns\n- Removing repeated columns\n- Performing train-test split\n- Performing scaling","cc5423ca":"### Observation \n\nThe precision vs recall tradeoff value from the above graph is at 0.4","025809b2":"##### Note\n\nSince a mode value for this column is India, we can replace the missing values with India. Since this will potentially skew the data heavily in the model we will drop this column. ","0cb8090c":"#### Newspaper Article","f9be0a74":"### Observation\nAs we see above there are a lot of columns with 1 or two uniwue values. \n\nWe will remove the columns that have only one unique value, since they wont have anything to contribute to the model significantly. These columns are:\n- Get updates on DM Content\n- Update me on Supply Chain Content\n- I agree to pay the amount through cheque\n- Receive More Updates About Our Courses\n- Magazine\n\nIt is also important to note that there are no null values in these columns as seen from the null values table above.","b6cdf23b":"### Observations\n\nBased on the plots above we observe that:\n\n- Most students found X education via 'Google' search\n- However, most of the google search leads weren't converted to actual students of the platform\n- References had the highest number of conversions at 92%\n- Welingak website also had a significantly high number of conversions at 99%\n- Welearn & Nc_edm had 100% conversion but due to their low numbers overall it might not be a correct picture of the situation\n- No conversions were made through the youtube channel, blog, press releases, pay per click ads or Welearnblog_home\n\n\nLet us merge the columns with low numbers into a common category: 'Others'","386c7b50":"## 3. Modeling\n\nTo build our Logistic Regression based model we will do the following:\n\n- Perform GLM analysis on the base model\n- Use RFE to perform feature selection \n- Calculate VIF\n- Build a model\n- Assess parameters & repeat the previous two steps till we have a good model with no multicollinearity \n- Generate prediction probabilities for our existing dataset\n- Plot the ROC curve to assess our model\n- Check the accuracy, specificity, sensitivity, precision & recall of our model\n- Repeat the whole process till an agreeable model is created","74b3d816":"#### Observation\nThe data feels highly skewed and confirms our assumption that it is correctly represented in the Lead Source column","42fef069":"### Assessing the model","385db037":"### Observation\nWe still have a few columns that have a high number of null values. Let us explore those","594bdf85":"### Observations\nBased on the above we can see that the data is highly skewed. Therefore we will be dropping this column eventually","1f35e8a3":"### Observation\n\nBased on the above statistics for Accuracy(80%), Sensitivity(65%) and Specificity(90%) we can say that our trained model is currently highly specific but not very sensitive. Our objective is to create a highly sensitive model with 80% sensitivity. Let us find cut-off values using ROC curves to improve this.","a93c1171":"### Observation\n\nOur dummies dataset was correctly created and the columns have been respectively removed. \nNow that we have our final data we are ready to create our model.","7e935b9d":"#### Newspaper","83ecc9ab":"# 4. Generating Leads Table\n\nOur final objective is to generate a table with the probability of converting leads to students. \n\nWe will do this by mapping the lead score to the respective lead numbers present in our original dataset. ","b10535d7":"### Observation\n\n\n- The VIF values for all the variables in the model look to be under control\n- The p value for Last Activity_Resubscribed to emails is very high at 0.999 & above the threshold 0.05\n\nWe will be dropping Last Activity_Resubscribed to emails in the next model ","2daa8fe3":"### Observation\n\n- The p value for What is your current occupation_Working Professional is above the threshold at 0.440\n\nWe will be dropping this variable in the next model","15694d24":"#### Observation\nThe data feels highly skewed and confirms our assumption that it is correctly represented in the Lead Source Column","d4aaef01":"### Logistic Regression Model Acceptance Criteria\n\n- The model does not over-fit\n- The model is simple enough to be understood\n- The model is built using significant features.\n- The VIF value is under 3 & the p value is under 0.05 for each feature\n- The accuracy, sensitivity and specificity of our model after test are at least 80%(+- 1% between all 3 parameters)\n","84deab06":"##### We will now assess the data and convert any 'Select' values into NAN values so that they can be treated as null values.","b84b0d65":"### Feature Scaling","f364d8c1":"##### Note:\nLet us now take count of the number of unique values in the remaining columns to assess them and decide if they should be dropped or imputed","46cb3900":"#### Helper Functions\n\nBelow are a few helper functions that we will use extensively in our notebook. These have been moved to the top to prevent the flow of our analysis.","10f98688":"### Observation\n\nFrom the above curve we can see that the optimal cutoff is at 0.33. This is the point where all the parameters are equally balanced","c563d2c7":"### Tags","3a3cda88":"#### Observation\n\nFrom the above bar plot it looks like we have a lot of small categories within the tags section. Moreover these tags are added by the sales team and are therefore arbitrary in nature. We can safely drop this column as this doesn't provide a lot of insight. ","b5324244":"### Creating Dummies","3d63fa48":"# 1. Data Analysis and EDA","9276a83d":"### Precision and Recall Curve","fde25585":"#### Creating new column 'Predicted_Hot_Lead' \n(with value 1 if Lead_Score_Prob > 0.5 else 0)\n\n- Please Note that we have selected an arbitrary cut off of of 0.5 as of now And we will be selecting the final cutoff later below","dbfe8341":"#### Observation\n\nFrom our analysis we see that this column holds similar data represented in the Last Activity column. We will drop this one and keep the Last Activity column.","8d01d835":"#### Observation\nThe data in this column looks skewed, however, it also defines potential target market for company X.\nWe will impute the missing values with 'Unemployed' and drop the column if analysis further down deems it necessary","e6c905d8":"### Observation\n\nFrom the ROC curve we can say that the model will be able to provide us with a good result overall. ","a09b7cbc":"### Observation\n\nTo imporve the conversion rate X education should focus on providing incentives to referrals as well as improve the lead conversion through olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website.","256ca773":"#### What is your current occupation","b1be701c":"#### Education Forums","79312883":"### Observation\n\nWe have successfully removed the highly corellated variables from the traiing and test datasets. \n\nWe are now ready to train our model","234719f6":"From the provided data dictionary we know that 'Converted' is our target column. 1 means the lead was converted and 0 means it wasn't. We will use this information to generate bivariate plots for our features.  ","ab0c739b":"#### Observation\n\nBased on the above heatmap we can see that we don't have any highly correlated features. Therefore there is no multicollinearity in the dataset.","7abd265a":"####  Do Not Email","042e1beb":"#### Total Time Spent on Website","e70133ce":"#### Do Not Call","db0ab2d9":"##### Creating a dataframe with the actual 'Converted' flag and the predicted 'Lead_Score_Prob' probabilities","b9283241":"### Observation\n\nFrom the barplot above we can infer that:\n\n- Lead Add Form has the highest conversion rate at 92%\n- Quick Add Form has 100% conversion rate but it has only 1 entry, so it might not be that reliable as a lead to go on\n- API has the least amount of conversions","3dcaa5eb":"#### Let us again observe the null values in the dataframe","3a4bbfab":"#### Last Activity\n","f00a3c85":"#### Observation\n\nA large number of candidates, 64% didnt opt for any course even though the would like a free copy. \n60% of the candidates didn't opt for any course or the free book. It is safe to say that this column has no impact on the behaviour of the prospective candidates. We will drop this column.","d85e856c":"#### Search","fcca09d6":"### Observation\n\nWe observe that there are still a few columns with a high percentage of null values, i.e. above 30%. \n\nLet us explore these columns individually to decide if they are important for us to keep or not. ","6babc8eb":"### Model 7","e441e742":"### Model 4","31883f98":"#### Note\nSince this is a key variable for us it would make sense to check what percentage of candidates were converted depending on the number of hours they spent.","ebbac224":"We will carry out some exploratory data analysis and understand the data better. \n\n#### Steps taken:\n\n- Checking the shape, columns, datatypes etc. of the dataset\n- Assessing out of place values\n- Checking for duplicate values\n- Checking for null values\n- Dropping unnecessary columns\n\nAfter this basic analysis we will carry out analysis of columns on individual basis and make a decision based on: \n- Impute values in rows with less than 5 values missing\n- Perform univariate, bivariate and multivariate analysis on various columns\n\nOnce our data is prepared and free of any anomalies we will\n\n- Create dummies for our remaining columns\n","9e7caf99":"##### Creating new column 'Lead_Score'\nLead_Score would be equal to (Lead_Score_Prob * 100)","00ddd91f":"### Observation\n\n- The shape of the dataset is 9240x37\n- We see a large number of 'Select' values present in various columns in the dataset. These values correspond to the user having not made any selection.\n- There are 7 numerical columns and 30 categorical columns","38564922":"### Observation\n\nThe above looks like time spent was recorded in minutes rather than hours. We will convert the entire column into hours for ease of analysis","b2b207e9":"### TotalVisits","002239ad":"### Specialization","f3b85be2":"### Observation\n\nFrom the abover barplot we see that the  number of values for 'Not Sure' are considerable high at 63.14%. We will drop this column further down.\n\nLet us now look at the following columns: Asymmetrique Activity Index, Asymmetrique Profile Index,Asymmetrique Profile Score and Asymmetrique Activity Score. \n\nWe know from the data dictionary that these are scores assigned to a customer based on their activity and profile.","7c50f632":"### Observation\n\nBased on the plots above we can infer that:\n\n- People interacting with the portal usually send sms the most\n- Only 24% of people who visit the website convert to actual students\n","da479784":"#### Digital Advertisement","393da044":"### Observation\n\nBased on the F1 score we can say that our model is fairly accurate. Let us test this accuracy on the test set.","7a59d75f":"### Making Predictions on test set","c357b736":"### Observation \n\nFrom the above bar plot we can infer that:\n\n- The highest number of conversions happen when people are spending around 18 hours or above on the website\n- People who spent around 3 hours on the website didn't opt for any courses. \n- From the boxplot we can see better that the longer you stay on the website, the higher your chances of conversion as well.\n\nOverall it is safe to say that the more time the user spends on the website, the better their chances of becoming a student. ","d4a26288":"#### Page Views Per Visit","5d96e59f":"## Final Model Reporting & Equation\n\nlog odds is given by: log(P\/1-P) = c + B1X1 +B2X2 + B3X3 + .... + BnXn\n\n- log odds = 1.5252 +(1.0683 * Total Time Spent on Website) + (1.1104 * Lead Source_Olark chat) + (3.5638 * Lead Source_Reference) + (5.4885 * Lead Source_Welingak website) + (-1.3461 * Last Activity_Converted to Lead) + (-1.9256 * Last Activity_Email Bounced) + (-0.5873 * Last Activity_Email Link Clicked) + (-0.7636 * Last Activity_Form Submitted on Website) + (2.0298 * Last Activity_Had a Phone Conversation) + (-1.5187 * Last Activity_Olark Chat Conversation) + (-0.6397 * Last Activity_Page Visited on Website) +(1.0606 * Last Activity_SMS Sent)+(-2.7448 * What is your current occupation_Other)+(-2.4126 * What is your current occupation_Student)+(-2.8085 * What is your current occupation_Unemployed)\n","3012ce67":"### Observation\n\nFrom the resultant shape we can confirm that the number of rows in the final dataset are the same as that in the test data set. Therefore we can use the values from the 'resultingTable' dataset to pursue the leads.based on the key insights identified above. ","4749103a":"#### Country","47edb8c0":"### Observation:\n\nFrom the above we see that there are some columns with over 50% of null values.\n\nFirst we will create a new dataset and then drop those columns from the data set. We will then assess the dataset to check the kind of values other columns with high number of null values hold and make a decision on whether or not we should drop them or impute those values. ","930bc4ad":"### Observation\nThe data feels highly skewed and confirms our assumption that it is correctly represented in the Lead Source column.\n\nSince the data for these columns is already correctly represented in the 'Lead Source' column, we will drop these columns","e4eb2dba":"### Obseravation\nWe notice that the original conversion rate of company X is 38.54%.","a4041e3b":"### Observations\n\nNow that our dataset is clear of all the null values we can begin performing analysis on the remaining columns","4ac79dd5":"### Observation\nThe data feels highly skewed and confirms our assumption that it is correctly represented in the Lead Source column","0ecb17e0":"#### Observation\nThe data feels highly skewed and confirms our assumption that it is correctly represented in the Lead Source column","a54c2ecf":"### Observation\n\nWe have reduced a lot of columns and imputed missing values in a few of them. In the remaining columns we can safely impute the missing value with the mode value since it has less than 5% missing values.","8b6a9ca8":"### Generating predicted values on the training set","4ef3f1e7":"#### Finding Optimal Cutoff Point\n\n- Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","dc440ead":"### Establishing Correlations","1b838fc1":"From our initial analysis it looks like these columns are very similar. Let us do some univariate & bivariate analysis to confirm this. ","e9ba71de":"#### Note:\nSince there are 36% null values that haven't yet been accounted for, we will replace those with 'Others'. This is being done because the NaN values have the highest percentage of values that haven't been shown above. It simply means that the user did not have any option relevant to them in this field. ","cdb3d64d":"### Observation\n\nAs we can see above, when we are selecting the optimal cutoff = 0.33, the various performance parameters Accuracy, Sensitivity & Specificity are all 80%\n\nThis meets our objective of getting a highly sensitive model with 80% sensitivity","c4fcb5ba":"## Observation And Conclusion (on Test Set)\n\n- As we can see above when cut-off = 0.33, the various Model Performance parameters on test set are as per below\n- Sensitivity = 79%\n- Specificity = 80%\n- Accuracy = 80%%\n- All the 3 performance parameters on test set appear to be almost same with no much variation, so we are good with the modeling now.","477fb198":"### Observation\n\nBased on the above we can see that the data is highly skewed. Therefore we will be dropping this column eventually","ba270898":"#### Observation\n\nThe data feels highly skewed and confirms our assumption that it is correctly represented in the Lead Source Column","17957262":"### Model 6","80c4cf31":"### RFE\n\nWe currently have 56 variables which is very high to create a model. \n\nWe will use RFE to generate a model with 20 variables.","9def8f17":"### Model 5"}}