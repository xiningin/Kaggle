{"cell_type":{"c8b89dd5":"code","0ef96dc0":"code","91b8e225":"code","35580d71":"code","1440c754":"code","b70d9d68":"code","28bc43b6":"code","37ac31b9":"code","c94ba862":"code","876cd517":"code","aca11380":"code","9fba6978":"code","33a62b1f":"code","3b8c6e68":"code","614f3545":"code","28b6f87e":"code","202d0ce8":"code","64ddc508":"code","9c6e1fb6":"code","50e3fe12":"code","aca2c7d5":"code","a461adac":"code","d3122cdd":"code","b27e0ed6":"code","ac092909":"code","6343dac8":"code","a701a316":"code","927ca5e7":"code","4acb6013":"code","92cf898d":"code","d96b570f":"code","537dbac5":"code","ba3169aa":"code","66256240":"code","f4b496ec":"code","c7b8ae8e":"code","1f7f25d8":"code","1517ce6c":"code","c4af8e26":"code","b04542ad":"code","e7d632f5":"code","225c7628":"code","b9064680":"code","751be499":"code","e97854ba":"code","c00ca8a6":"code","5bc54d54":"code","0bd3bad3":"code","4c24577f":"code","dab63849":"code","a56fd5d3":"code","6a31f7f8":"code","9f91c1aa":"code","3d259b22":"code","d54ff1be":"code","2d612e8b":"code","1970cb21":"code","f277274c":"code","4032a17b":"code","f2d0fe60":"code","0fa9d031":"code","86c4823f":"code","c8e5cb38":"code","7d04867c":"code","610accc1":"code","eda9c4df":"code","31bfc9c8":"code","1a2a928f":"code","acaf1d78":"code","5cdb8464":"code","67d7f435":"code","cd5b26bb":"code","42671e0f":"code","dd34c527":"code","41f5a460":"code","22211322":"code","4443f2bc":"code","54e41039":"code","965dec20":"code","1499545e":"code","92ea9c33":"code","3b85455f":"code","e6677055":"code","a8c17131":"code","29003a57":"code","90d84fb2":"code","c3e83d3a":"markdown","df81d7a5":"markdown","851bdee6":"markdown","68742bc5":"markdown","3d3b037f":"markdown","009075d5":"markdown","413d2e7e":"markdown","7007d1f2":"markdown","7e40090a":"markdown","57b4bfe2":"markdown","2498c6ac":"markdown","975d6c15":"markdown","30ac1e73":"markdown","31935424":"markdown","5a3cd158":"markdown","bdb64984":"markdown","74c68905":"markdown","2f2afed4":"markdown","d344f6e8":"markdown","16e3313b":"markdown","3053fe79":"markdown","c3139cf9":"markdown","7d0885cf":"markdown","1ec510cb":"markdown","557c4827":"markdown","1260d76d":"markdown","c627cca8":"markdown","f554e80d":"markdown","aab863f4":"markdown","da1b0031":"markdown","383ce5be":"markdown","85878b9b":"markdown","ebe84886":"markdown","cbf13659":"markdown","59da9fb5":"markdown","1fe69573":"markdown","97a96dc0":"markdown"},"source":{"c8b89dd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ef96dc0":"# Necessary librarys\nimport os # it's a operational system library, to set some informations\nimport random # random is to generate random values\n\nimport pandas as pd # to manipulate data frames \nimport numpy as np # to work with matrix\nfrom scipy.stats import kurtosis, skew # it's to explore some statistics of numerical values\nimport statistics as st \n\nimport matplotlib.pyplot as plt # to graphics plot\nimport seaborn as sns # a good library to graphic plots\ncolor = sns.color_palette()\nimport squarify # to better understand proportion of categorys - it's a treemap layout algorithm\n\nfrom sklearn.preprocessing import StandardScaler \nfrom scipy import stats #The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n\n# Importing librarys to use on interactive graphs\nfrom plotly.offline import init_notebook_mode, iplot, plot \nimport plotly.graph_objs as go \n\nimport json # to convert json in df\nfrom pandas.io.json import json_normalize # to normalize the json file\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\n# to set a style to all graphs\nplt.style.use('fivethirtyeight')\ninit_notebook_mode(connected=True)\n","91b8e225":"columns = ['device', 'geoNetwork', 'totals', 'trafficSource'] # Columns that have json format\n\ndir_path = \"\/kaggle\/input\/ga-customer-revenue-prediction\/\" \n\n# p is a fractional number to skiprows and read just a random sample of the our dataset. \np = 0.07 # *** In this case we will use 50% of data set *** #\n\n#Code to transform the json format columns in table\ndef json_read(df):\n    #joining the [ path + df received]\n    data_frame = dir_path + df\n    \n    #Importing the dataset\n    df = pd.read_csv(data_frame, \n                     converters={column: json.loads for column in columns}, # loading the json columns properly\n                     dtype={'fullVisitorId': 'str'}, # transforming this column to string\n                     skiprows=lambda i: i>0 and random.random() > p)# Number of rows that w\n    for column in columns: #loop to finally transform the columns in data frame\n        #It will normalize and set the json to a table\n        column_as_df = json_normalize(df[column]) \n        # here will be set the name using the category and subcategory of json columns\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns] \n        # after extracting the values, let drop the original columns\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n        \n    # Printing the shape of dataframes that was imported     \n    print(f\"Loaded {os.path.basename(data_frame)}. Shape: {df.shape}\")\n    return df # returning the df after importing and transforming\n","35580d71":"%%time \n# %%time is used to calculate the timing of code chunk execution #\n\n# We will import the data using the name and extension that will be concatenated with dir_path\ndf_train = json_read(\"train.csv\") \n# The same to test dataset\ndf_test = json_read(\"test.csv\") ","1440c754":"# This command shows the first 5 rows of our dataset\ndf_train.head()","b70d9d68":"df_train.info()","28bc43b6":"#df_train.mean()","37ac31b9":"#df_train.median()","c94ba862":"#df_train.min()","876cd517":"#df_train.max()","aca11380":"#df_train.stdev()","9fba6978":"df_train[\"totals.transactionRevenue\"] = df_train[\"totals.transactionRevenue\"].astype('float')\ndf_train['Has Revenue?'] = df_train['totals.transactionRevenue'].apply(lambda x: 1 if x>0 else 0)","33a62b1f":"df_train.head()","3b8c6e68":"#df_train[\"totals.transactionRevenue\"] = df_train[\"totals.transactionRevenue\"].astype('float')\ngdf = df_train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()\n","614f3545":"nzi = pd.notnull(df_train[\"totals.transactionRevenue\"]).sum()\nnzr = (gdf[\"totals.transactionRevenue\"]>0).sum()\nprint(\"Number of instances in train set with non-zero revenue : \", nzi, \" and ratio is : \", nzi \/ df_train.shape[0])\nprint(\"Number of unique customers with non-zero revenue : \", nzr, \"and the ratio is : \", nzr \/ gdf.shape[0])","28b6f87e":"df_train['totals.transactionRevenue'][df_train['totals.transactionRevenue'].isna()]=0","202d0ce8":"discovering_consts = [col for col in df_train.columns if df_train[col].nunique() == 1]\nprint(\"Columns with just one value: \", len(discovering_consts), \"columns\")\nprint(\"Name of constant columns: \\n\", discovering_consts)\n","64ddc508":"df_train.head()","9c6e1fb6":"#dropping the constant colums\ndf_train = df_train.drop(columns = discovering_consts)\n#dropping \n#Next steps: \n#drop columns that are not meaningful that much\n#df_train.drop(columns = ['sessionId','visitId','trafficSource.referralPath','trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot','trafficSource.adwordsClickInfo.gclId','trafficSource.adContent'])\n\n#Converting fields to categorical of the data\n#Step 1: check the portion of the missing values (if missing value exceeds 90%\/80%, I should throw the field out)\n#Step 2: I need to check the number of occurances in order for each term, I want to see how fast it drops (). I decide a threshhold and I cut. \n#Step 3: \n\n#df_test = df_test.drop(columns = discovering_consts)\n\n","50e3fe12":"#drop columns that are not meaningful that much\ndf_train = df_train.drop(columns = ['sessionId','visitId','trafficSource.referralPath','trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot','trafficSource.adwordsClickInfo.gclId','trafficSource.adContent'])\n#df_test = df_test.drop(columns = ['sessionId','visitId','trafficSource.referralPath','trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot','trafficSource.adwordsClickInfo.gclId','trafficSource.adContent'])\n","aca2c7d5":"\n#Ask Yenson how to also count 'not available in demo dataset' and \"(not set)\"\n\n#percent_missing = (df_train.isnull().sum()+df_train['not available in demo dataset'].value_counts(normalize=True)) * 100 \/ len(df_train)\n#missing_value_df = pd.DataFrame({'column_name': df_train.columns,\n                                 #'percent_missing': percent_missing})\n#missing_value_df","a461adac":"df_train","d3122cdd":"print(df_train.apply(lambda col: col.unique()))","b27e0ed6":"#print(df_train.apply(lambda col: col.unique()))\nprint(df_train.nunique())\n","ac092909":"#drop visitStartTime that has large number of unique values and wouldn't contribute to the model\ndf_train = df_train.drop(columns = ['visitStartTime','geoNetwork.networkDomain'])\n#df_test = df_test.drop(columns = ['visitStartTime','geoNetwork.networkDomain'])\n","6343dac8":"df_train['logTotalRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x: np.log(1+x))","a701a316":"df_train.corr()\nsns.heatmap(df_train.corr());","927ca5e7":"#making a new data frame excluding fullVisitorId before \ndf_train_corr_set=df_train.drop(columns = ['fullVisitorId'])","4acb6013":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(df_train_corr_set.sample(n=10000))\nprofile.to_widgets()","92cf898d":"import pandas as pd\nimport phik\nfrom phik import resources, report\n\ndf_train_corr_set.sample(n=10000).phik_matrix()","d96b570f":"df_train.info()","537dbac5":"\nsource = df_train['trafficSource.source']\n\nsource_value_counts = source.value_counts(dropna=False)\ndisplay(source_value_counts) \nsource_value_counts.reset_index(drop=True)[:20].plot()\nplt.title(\"Source\")#reset_index is to convert indexes to numbers\n\nTop_5_Sources = source_value_counts[:5].index\nprint(Top_5_Sources)\n\n#transaction cities to missing value None\nsource[~source.isin(Top_5_Sources)] = None\nsource\n#write in the report: in the ideal world, I would consider merging some of the values\n#write a function ","ba3169aa":"print(df_train.nunique())","66256240":"#The method proved to be working, defining a function to accelerate the process\ndef plot_column(name,x):\n    myList = df_train[name]\n    value_counts = myList.value_counts(dropna=False)\n    display(value_counts) \n    value_counts.reset_index(drop=True)[:x].plot()\n    plt.title(name)\n    Top_x_Sources = value_counts[:x].index\n    print(Top_x_Sources)\n\n","f4b496ec":"def convert_to_none_after_threshold(name,y):\n    mycolumn = df_train[name]\n    column_value_counts = mycolumn.value_counts(dropna=False)\n    Top_y_Categories = column_value_counts[:y].index\n    print(Top_y_Categories)\n    mycolumn[~mycolumn.isin(Top_y_Categories)] = None\n    mycolumn","c7b8ae8e":"#def convert_to_none_after_threshold(name,y):\n    #mycolumn = df_train[name]\n    #threshold=y\n    #column_value_counts = mycolumn.value_counts(dropna=False)\n    #low_volumn_categories = mycolumn.value_counts()<threshold\n    #print(low_volumn_categories)\n    #mycolumn[mycolumn.isin(low_volumn_categories)] = None\n    #mycolumn","1f7f25d8":"plot_column('channelGrouping',20)\n\n","1517ce6c":"plot_column('device.browser',20)","c4af8e26":"convert_to_none_after_threshold('device.browser',11)","b04542ad":"#convert_to_none_after_threshold('device.browser',11)","e7d632f5":"plot_column('device.operatingSystem',20)","225c7628":"convert_to_none_after_threshold('device.operatingSystem',6)","b9064680":"plot_column('device.browser',20)","751be499":"df_train['device.operatingSystem']","e97854ba":"convert_to_none_after_threshold('device.operatingSystem',8)","c00ca8a6":"print(df_train.nunique())","5bc54d54":"plot_column('geoNetwork.region',20)\n","0bd3bad3":"plot_column('trafficSource.keyword',20)\n","4c24577f":"plot_column('geoNetwork.continent',20)","dab63849":"#get top 5 continent and convert the rest to none\nconvert_to_none_after_threshold('geoNetwork.continent',5)\n","a56fd5d3":"plot_column('geoNetwork.subContinent',20)","6a31f7f8":"#getting the top 10 subContiment and convert the rest to none\nconvert_to_none_after_threshold('geoNetwork.subContinent',8)","9f91c1aa":"plot_column('geoNetwork.metro',20)","3d259b22":"plot_column('geoNetwork.city',20)","d54ff1be":"#dimension_list=[col for col in df_train if type(col)!= int or type(col)!=float]\n#print(dimension_list)","2d612e8b":"#channelGrouping\n#plot_column('channelGrouping',20)\n","1970cb21":"df_train['month'] = df_train['date'].apply(lambda x: (x\/\/100)%100)\n#df_train['logTotalRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x: np.log(1+x))\ndisplay(df_train)","f277274c":"ax = df_train.plot.scatter('month','logTotalRevenue', figsize=(15, 5))\ndf_train[['month', 'logTotalRevenue']].groupby('month')['logTotalRevenue'].median().plot(ax=ax)\n#visually, it seems there is a trend\n#by plotting the mean and median, it doesn't seem there is a seasonability\n\n#hits vs. transaction: scatter plots and then compute correlation","4032a17b":"#totals.transactionRevenue correlation matrics\ndef horizontal_bar_chart(cnt_srs, color):\n    trace = go.Bar(\n        y=cnt_srs.index[::-1],\n        x=cnt_srs.values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n# Device Browser\ncnt_srs = df_train.groupby('device.browser')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(50, 171, 96, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(50, 171, 96, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(50, 171, 96, 0.6)')\n\n# Device Category\ncnt_srs = df_train.groupby('device.deviceCategory')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(71, 58, 131, 0.8)')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(71, 58, 131, 0.8)')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(71, 58, 131, 0.8)')\n\n# Operating system\ncnt_srs = df_train.groupby('device.operatingSystem')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(246, 78, 139, 0.6)')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10),'rgba(246, 78, 139, 0.6)')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10),'rgba(246, 78, 139, 0.6)')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.04, \n                          subplot_titles=[\"Device Browser - Count\", \"Device Browser - Non-zero Revenue Count\", \"Device Browser - Mean Revenue\",\n                                          \"Device Category - Count\",  \"Device Category - Non-zero Revenue Count\", \"Device Category - Mean Revenue\", \n                                          \"Device OS - Count\", \"Device OS - Non-zero Revenue Count\", \"Device OS - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Device Plots\")\npy.iplot(fig, filename='device-plots')","f2d0fe60":"# Continent\ncnt_srs = df_train.groupby('geoNetwork.continent')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(58, 71, 80, 0.6)')\n\n# Sub-continent\ncnt_srs = df_train.groupby('geoNetwork.subContinent')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"], 'orange')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"], 'orange')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"], 'orange')\n\n# Country\ncnt_srs = df_train.groupby('geoNetwork.country')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'blue')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'blue')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"Continent - Count\", \"Continent - Non-zero Revenue Count\", \"Continent - Mean Revenue\",\n                                          \"Sub Continent - Count\",  \"Sub Continent - Non-zero Revenue Count\", \"Sub Continent - Mean Revenue\",\n                                          \"Country - Count\", \"Country - Non-zero Revenue Count\", \"Country - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1500, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Geography Plots\")\npy.iplot(fig, filename='geo-plots')","0fa9d031":"# Channelgrouping\ncnt_srs = df_train.groupby('channelGrouping')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace1 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace2 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'rgba(58, 71, 80, 0.6)')\ntrace3 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'rgba(58, 71, 80, 0.6)')\n\n# Medium\ncnt_srs = df_train.groupby('trafficSource.medium')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace4 = horizontal_bar_chart(cnt_srs[\"count\"], 'orange')\ntrace5 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"], 'orange')\ntrace6 = horizontal_bar_chart(cnt_srs[\"mean\"], 'orange')\n\n# Source\ncnt_srs = df_train.groupby('trafficSource.source')['totals.transactionRevenue'].agg(['size', 'count', 'mean'])\ncnt_srs.columns = [\"count\", \"count of non-zero revenue\", \"mean\"]\ncnt_srs = cnt_srs.sort_values(by=\"count\", ascending=False)\ntrace7 = horizontal_bar_chart(cnt_srs[\"count\"].head(10), 'blue')\ntrace8 = horizontal_bar_chart(cnt_srs[\"count of non-zero revenue\"].head(10), 'blue')\ntrace9 = horizontal_bar_chart(cnt_srs[\"mean\"].head(10), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=3, vertical_spacing=0.08, horizontal_spacing=0.15, \n                          subplot_titles=[\"channelGrouping - Count\", \"channelGrouping - Non-zero Revenue Count\", \"channelGrouping - Mean Revenue\",\n                                          \"Medium - Count\",  \"Medium  - Non-zero Revenue Count\", \"Medium  - Mean Revenue\",\n                                          \"Source - Count\", \"Source - Non-zero Revenue Count\", \"Source - Mean Revenue\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 2)\nfig.append_trace(trace6, 2, 3)\nfig.append_trace(trace7, 3, 1)\nfig.append_trace(trace8, 3, 2)\nfig.append_trace(trace9, 3, 3)\n\nfig['layout'].update(height=1500, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Channel Plots\")\npy.iplot(fig, filename='channel-plots')","86c4823f":"fig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x = df_train['totals.hits'], y = df_train['logTotalRevenue'])\nplt.xlabel(\"Total Hits\")\nplt.ylabel(\"LogTotalRevenue\")\n\nplt.show()","c8e5cb38":"df_train[['totals.hits', 'totals.pageviews']] = df_train[['totals.hits', 'totals.pageviews']].apply(pd.to_numeric)\ndf_train.info()","7d04867c":"column_1=df_train['logTotalRevenue']\ncolumn_2=df_train['totals.hits']\ncorrelation = column_1.corr(column_2)\nprint(correlation)","610accc1":"fig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x = df_train['totals.pageviews'], y = df_train['logTotalRevenue'])\nplt.xlabel(\"Total Pageviews\")\nplt.ylabel(\"LogTotalRevenue\")\n\nplt.show()","eda9c4df":"column_1=df_train['logTotalRevenue']\ncolumn_2=df_train['totals.pageviews']\ncorrelation = column_1.corr(column_2)\nprint(correlation)","31bfc9c8":"#dropping columns that have too many missing values and 0 correlation with logTotalRevenue\ndf_train = df_train.drop(columns = ['geoNetwork.region','geoNetwork.metro','geoNetwork.city','trafficSource.keyword'])","1a2a928f":"df_train = df_train.drop(columns = ['month'])","acaf1d78":"df_train.describe(include='all')","5cdb8464":"#transform data in object format or string format to categorical type, as lgb won't accept object or string format\n\nobject_features = list(df_train.loc[:,df_train.dtypes == 'object'].columns.values)\nobject_features","67d7f435":"for feature in object_features:\n    df_train[feature] = pd.Series(df_train[feature], dtype=\"category\")","cd5b26bb":"var_columns = [col for col in df_train.columns if col not in ['fullVisitorId','logTotalRevenue','totals.transactionRevenue']]\nx = df_train.loc[:,var_columns] #independent variables\ny = df_train.loc[:,'logTotalRevenue'] #dependent variable\n\nx_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size=0.2)\n\nx_train.shape, x_valid.shape, y_train.shape, y_valid.shape","42671e0f":"train_data = lgb.Dataset(x_train,label=y_train,#categorical_feature=['channelGrouping', 'device.browser', 'device.operatingSystem', 'device.deviceCategory', 'geoNetwork.continent', 'geoNetwork.subContinent', 'geoNetwork.country', 'trafficSource.campaign', 'trafficSource.source', 'trafficSource.medium']\n)\nvalid_data = lgb.Dataset(x_valid, label=y_valid,#categorical_feature=['channelGrouping', 'device.browser', 'device.operatingSystem', 'device.deviceCategory', 'geoNetwork.continent', 'geoNetwork.subContinent', 'geoNetwork.country', 'trafficSource.campaign', 'trafficSource.source', 'trafficSource.medium']\n)","dd34c527":"parameters = {'objective': 'regression',#change\n              'metric': 'auc', #\n              'is_unbalance': 'true',\n              'boosting': 'gbdt',\n              'num_leaves': 63, #how many leaves will be there \n              'feature_fraction': 0.5,\n              'bagging_fraction': 0.5,\n              'bagging_freq': 20,\n              'learning_rate': 0.01,\n              'verbose': -1\n            }","41f5a460":"\nmodel_lgbm = lgb.train(parameters,\n                      train_data,\n                     valid_sets=valid_data,\n                      num_boost_round=500\n                      \n                     )\n\n\n","22211322":"y_train_pred = model_lgbm.predict(x_train)\ny_valid_pred = model_lgbm.predict(x_valid)\n\nprint(\"AUC Train: {:.4f}\\nAUC Valid: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))","4443f2bc":"print(y_valid)","54e41039":"print(y_valid_pred)","965dec20":"df_test.head()","1499545e":"#var_columns = [col for col in df_test.columns if col not in ['fullVisitorId','logTotalRevenue','totals.transactionRevenue']]\n#x = df_train.loc[:,var_columns] #independent variables\n#y = df_train.loc[:,'logTotalRevenue'] #dependent variable","92ea9c33":"#submission_df=pf.DataFrame()","3b85455f":"cm_lgbm = confusion_matrix(y_valid, y_valid_pred)\nsns.heatmap(cm_lgbm,annot=True)","e6677055":"#df_sample_submission = pd.read_csv('www.kaggle.com\/anniehou\/ga-sampe-submission')\n","a8c17131":"#x_test = df_test.loc[:,var_columns]\n#df_sample_submission['target'] = model_lgbm.predict(x_test)\n#df_sample_submission","29003a57":"import xgboost as xgb","90d84fb2":"\nmodel_xgboost = lgb.train(parameters,\n                      train_data,\n                     valid_sets=valid_data,\n                      num_boost_round=500\n                      \n                     )\n","c3e83d3a":"Specify the parameters for LightGBM","df81d7a5":"geoNetwork.metro has large volumn of missing value and will eventually to be dropped before modelling","851bdee6":"Adding logTotalRevenue as a new column","68742bc5":"# Loading the data and have a quick look","3d3b037f":"geoNetwork.city has large volumn of missing value and will eventually be dropped before modelling","009075d5":"# **geoNetwork.region**","413d2e7e":"# **channelGrouping**","7007d1f2":"* The majority of the zero and non-zero revenue traffic is coming from North America\n* Within North America, the majority of the zero and non-zero revenue traffic is coming from America","7e40090a":"trafficSource.keyword has too many missing values. Will eventually drop the column before modelling","57b4bfe2":"# **trafficSource.keyword**","2498c6ac":"# 1. First thing first, analyzing Has Revenue?\nSince we are predicting the natural log of sum of all transactions of the user, let us sum up the transaction revenue at user level and take a log and visulize in a scatter plot","975d6c15":"# Create a XGBoost Model and Evaluate Performance\n","30ac1e73":"# **geoNetwork.city**","31935424":"# **Relationship logTotalRevenue vs. totals.hits**","5a3cd158":"**totals.transactionRevenue vs. Device**","bdb64984":"* Device distribution looks similar on both the count and the count of non-zero revenue plots. And the majority of the traffic comes from Chrome for both zero and non-zero revenue traffic\n* For device category, desktop is getting way more traffic for both zero and non-zero traffic than mobile and desktop. And the average revenue on desktop is also higher\n* For device category, desktop is getting way more traffic for both zero and non-zero traffic than mobile and desktop. And the average revenue on desktop is also higher\n* For device operating system,  windows has the highest number of counts while macintosh has the highest non-zero revenue count.\n\n\n","74c68905":"# 2. Data Reprocessing","2f2afed4":"Before doing any further analysis, I'm going to find out the constant values that is not useful and save time on manipulating and processing","d344f6e8":"So the ratio of customers with revenue vs. customer with no revenue is 1.3% ","16e3313b":"**logTotalRevenue has 0 correlation with device.browser, device.operatingSystem,geoNetwork.continent,geoNetwork.subContinent,geoNetwork.country,trafficSource.source**\ngeoNetwork.region has very low correlation and too many missing values","3053fe79":"Before doing any further analysis, I'm going to convert the NaN value in totals.transactionRevenue to 0.","c3139cf9":"geoNetwork.region has too many missing values and the correlation is low (0.11). Will eventually drop the column before modelling","7d0885cf":"# **geoNetwork.metro**","1ec510cb":"This visual has confirmed 80\/20 rule that a small percentage of customers product most of the revenue. Therefore, the marketing team are in need of making smart marketing strategies to reach those high value customers. ","557c4827":"* Organic has the highest number of count, while referral has the highest number of non-zero revenue count.Indicating referall is the most efficient chanel in driving revenue for G Store\n","1260d76d":"# trafficSource.source","c627cca8":"channelGrouping only has 8 unique values although the plot is showing it's droping very fast","f554e80d":"**geoNetwork.continent **","aab863f4":"**totals.transactionRevenue vs. Geo**","da1b0031":"# # **Modeling**","383ce5be":"# Converting discrete into categorical variables. In order to  use one hot\nWhy use Light GBM: best in class performance, fast training time, I don't have to do one-hot\ntraining time: the time for building the model\nRunning time: the time to get the prediction\n\nAlternative: XGBoost, CatBoost (Try CatBoost if it doesn't require one-hot)\n\n","85878b9b":"# Create a Light GBM Model and evaluate performance","ebe84886":"# **geoNetwork.subContinent**","cbf13659":"Separate the data into independent and dependent variables","59da9fb5":"# **device.browser**","1fe69573":"**Seasonality?**\n**total transaction vs. Month**","97a96dc0":"# # Exploratory Data Analysis"}}