{"cell_type":{"e4839273":"code","23fd07ed":"code","de085e81":"code","220160cb":"code","4fdcaa11":"code","9df5d863":"code","8b6e8bb9":"code","6365536c":"code","fa32b1b8":"code","11b5f134":"code","8dbea94a":"code","16841744":"code","e7e2df0f":"code","aa728d6d":"code","d8f08275":"code","83464b20":"code","f4c7eefc":"code","99a6ce0c":"code","c4e61522":"code","cec03706":"code","379b6df1":"code","7c2dea8f":"code","0491e776":"markdown","fd016847":"markdown","843420d7":"markdown","65b5b6a6":"markdown","ce6380ff":"markdown","711e2d53":"markdown","66748df3":"markdown","1c93ce10":"markdown","e83aa452":"markdown","1aaba4eb":"markdown","b90e76b8":"markdown"},"source":{"e4839273":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport xgboost as xgb\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nimport os\nprint(os.listdir(\"..\/input\"))\nseed=5\n# Any results you write to the current directory are saved as output.","23fd07ed":"train_identity=pd.read_csv('..\/input\/train_identity.csv')\n# test_identity=pd.read_csv('..\/input\/test_identity.csv')\ntrain_transaction=pd.read_csv('..\/input\/train_transaction.csv')\n# test_transaction=pd.read_csv('..\/input\/test_transaction.csv')","de085e81":"train=pd.merge(train_transaction,train_identity,how='left',on='TransactionID')\n# test=pd.merge(test_transaction,test_identity,how='left',on='TransactionID')","220160cb":"del train_identity,train_transaction","4fdcaa11":"target=train['isFraud']\ntrain=train.drop(['isFraud','TransactionID'],axis=1)\n# test=test.drop('TransactionID',axis=1)","9df5d863":"from sklearn.model_selection import train_test_split\n\ntrain,val,target,val_y=train_test_split(train,target,test_size=0.5,random_state=5,stratify=target)","8b6e8bb9":"del val,val_y","6365536c":"train.info()","fa32b1b8":"train=train.fillna(-999)\n# test=test.fillna(-999)","11b5f134":"from sklearn.preprocessing import LabelEncoder\n\ncat_cols=[col for col in train.columns if train[col].dtype=='object']\nfor col in cat_cols:\n    le=LabelEncoder()\n    le.fit(list(train[col].values))\n    train[col]=le.transform(list(train[col].values))\n#     test[col]=le.transform(list(test[col].values))","8dbea94a":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","16841744":"train=reduce_mem_usage(train)\n# test=reduce_mem_usage(test)","e7e2df0f":"def get_feature_importance(data,target,shuffle=False):\n    y=target.copy()\n    if shuffle:\n        y=target.copy().sample(frac=1).reset_index(drop=True)\n    \n    xgb_params=dict(n_estimators=1000,\n                verbosity=0,\n                tree_method='gpu_hist',\n               colsample_bytree=0.8,\n               subsample=0.8,\n               learning_rate=0.05,\n               max_depth=5)\n    \n    clf=XGBClassifier(**xgb_params)\n    clf.fit(train,y)\n    \n    feat_imp=pd.DataFrame()\n    feat_imp['features']=train.columns\n    feat_imp['importances']=clf.feature_importances_\n    feat_imp['train_score']=roc_auc_score(y,clf.predict(train))\n    del clf\n    \n    return feat_imp","aa728d6d":"np.random.seed(5)\nactual_feat_imp=get_feature_importance(train,target,shuffle=False)","d8f08275":"import time\n\n\nnull_feat_imps=pd.DataFrame()\nstart=time.time()\nfor i in range(50):\n    start1=time.time()\n    null_imp=get_feature_importance(train,target,shuffle=True)\n    null_imp['round']=i+1\n    null_feat_imps=pd.concat([null_feat_imps,null_imp],axis=0)\n    del null_imp\n    end1=time.time()\n    epoch_time=(end1-start1)\/60\n    print(f'Round {i+1} completed in {epoch_time} mins')\n    print('-'*100)\n\n    \nend=time.time()\ntotal_time=(end-start)\/60\nprint(f'Total time taken : {total_time} mins')","83464b20":"def show_null_actual(feature):\n    plt.figure(figsize=(10,5))\n    a=plt.hist(null_feat_imps[null_feat_imps['features']==feature]['importances'],label='Null Importance')\n    plt.vlines(x=actual_feat_imp[actual_feat_imp['features']==feature]['importances'],ymin=0,ymax=np.max(a[0]),color='r',linewidth=10,label='Real Target')\n    plt.legend(loc='best')\n    plt.title(f'Acutal Importance vs Null Importance of {feature}')\n    plt.show()","f4c7eefc":"actual_feat_imp.to_csv('Actual_imp.csv',index=False)\nnull_feat_imps.to_csv('Null_imp.csv',index=False)","99a6ce0c":"show_null_actual('V1')","c4e61522":"feature_scores=[]\n\nfor feature in train.columns:\n    null_imp=null_feat_imps[null_feat_imps['features']==feature]['importances'].values\n    actual_imp=actual_feat_imp[actual_feat_imp['features']==feature]['importances'].values\n    score=np.log((1e-10 + actual_imp\/(1+np.percentile(null_imp,75))))[0]\n    feature_scores.append((feature,score))","cec03706":"feature_score_df=pd.DataFrame(feature_scores,columns=['Feature','Score']).sort_values('Score',ascending=False).reset_index(drop=True)\n\nplt.figure(figsize=(10,10))\nsns.barplot(x='Score',y='Feature',data=feature_score_df.iloc[:50,:])\nplt.title('Scores of all features')","379b6df1":"correlation_scores=[]\n\nfor feature in train.columns:\n    null_imp=null_feat_imps[null_feat_imps['features']==feature]['importances'].values\n    actual_imp=actual_feat_imp[actual_feat_imp['features']==feature]['importances'].values\n    corr_score=100*(null_imp < actual_imp).sum()\/null_imp.size\n    correlation_scores.append((feature,corr_score))","7c2dea8f":"correlation_df=pd.DataFrame(correlation_scores,columns=['Feature','Score']).sort_values('Score',ascending=False).reset_index(drop=True)\nplt.figure(figsize=(10,10))\nsns.barplot(x='Score',y='Feature',data=correlation_df.iloc[:50,:])\nplt.title('Scores of all features')\nplt.show()","0491e776":"**2. Here we just use the counts of how many null importances are lesser than the actual importances. For more important features, we expect the score to be more. **","fd016847":"I have used the same ways which Oliver used in his kernel to score features. But surprisingly, all features are scoring negative. But according to me, if a feature is really important then the distance between the mean of the null importances and actual importance must be more (I know 'more' is very subjective but we can try different thresholds) as compared to that of unimportant features. I have done this in another kernel because of memory issues : https:\/\/www.kaggle.com\/virajbagal\/distance-between-importances  I have used different thresholds and then fit XGB model  in that kernel. \n\nSo, accoring to me the hypothesis must be:\n1. The distance between mean of null importances and the actual importance must be more compared to that of unimportant features.\n2. The null importance of the important features must have high variance. \n\n\n**Please comment if I am wrong anywhere or whatever your thoughts are. \nThank you for reading my kernel. All the best for the competitoin. **","843420d7":"# Plotting null and actual importance.","65b5b6a6":"## Actual Importance","ce6380ff":"You can try this out for all other features. ","711e2d53":"To get an idea about which features might be important we do the following :\n1. I fit usual XGB model to the data as it is and get the actual importance of each feature.\n2. Then I shuffle only the 'isFraud' column and fit XGB model and get the importance of each faeture. We call this as null importance. I did this 50 times. \n3. Plot the actual importance and all the null importances.\n4. Define some way to get score for each feature.","66748df3":"## Feature Scores","1c93ce10":"This kernel is highly inspired by Oliver's kernel in the Home Credit Default Risk competition : https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances","e83aa452":"**1. Using the log of ratio of actual importance and 75th percentile of null importance. The logic here is, in case of important features, the actual importance must be substantially greater than the 75th percentile of null importances of that feature. So, the log of the ratio is expected to be more positive for more important features. But here, for every feature the score obtained is negative. That means for every feature, actual importance is lesser than (1 + the 75th percentile of null importances). This was shocking to me . **","1aaba4eb":"## Feature Selection IEEE Fraud","b90e76b8":"## Null Importances"}}