{"cell_type":{"b7fd45cf":"code","3feae6a3":"code","04b1dae2":"code","8b97dcd6":"code","51cf57aa":"code","bb14589c":"code","95e84593":"code","4542bef7":"code","422c0254":"code","aa130cc5":"code","1d446fcf":"code","853f3da3":"code","6de6c434":"code","2cf7f09d":"code","05d30179":"code","11e18097":"code","660f7141":"code","7a01063e":"code","43ed8bf0":"code","30e3a333":"code","7f4aaff1":"markdown","5d8d833a":"markdown","5109419b":"markdown","04642456":"markdown","7e62c87a":"markdown","737c8fe2":"markdown","3c5b1647":"markdown"},"source":{"b7fd45cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n","3feae6a3":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data =pd.read_csv(\"..\/input\/titanic\/test.csv\")\nprint(\"Number of rows and columns present in the train data is \", train_data.shape)\nprint(\"Number of rows and columns present in the test data is \", test_data.shape)\n","04b1dae2":"train_data.head()","8b97dcd6":"train_data.drop(['Name', 'Ticket'],axis=1,inplace=True)\ntest_data.drop(['Name', 'Ticket'],axis=1,inplace=True)\n","51cf57aa":"train_data.head()","bb14589c":"sns.countplot(train_data[\"Survived\"])\n\n# we can see that count of surived and not survied is diffrent and \n#we can say that data is imbalance and we need to balance the data in preprocessing ","95e84593":"sns.countplot(train_data[\"Pclass\"])\n# From the below graph we can say that number of people in 3rd class is highest with 500 people and then 1st class with 200 ","4542bef7":"sns.countplot(train_data[\"Sex\"])","422c0254":"plt.hist(train_data[\"Age\"],bins=5)","aa130cc5":"plt.hist(train_data[\"Fare\"],bins=10)","1d446fcf":"def TreatNullValue(data):\n    NulValPerdf=pd.DataFrame(data.isna().sum()\/data.shape[0])*100\n    threshold=float(input('Enter a value of threshold for null values: '))\n    nUnique=int(input('Enter the number of unique values to decide the categorical variable: '))\n    dropCol=[]\n    con=[]\n    cat=[]\n    disnum=[]\n    for i in NulValPerdf.index:\n        if NulValPerdf.at[i,0]>=threshold:\n            dropCol.append(i)\n            print(\"drop col\",dropCol)\n    data.drop(dropCol,axis=1,inplace=True)\n    for j in data.columns:\n        if (data[j].nunique()>=nUnique) and (data[j].dtype=='int64' or data[j].dtype=='float64'):\n            con.append(j)\n            data[j].fillna(value=data[j].median(), inplace=True)\n        elif (data[j].nunique()<nUnique) and (data[j].dtype=='int64' or data[j].dtype=='float64'):\n            disnum.append(j)\n        else:\n            cat.append(j)\n            data[j].fillna(value=data[j].value_counts().index[0],inplace=True)\n    return (con,cat,disnum,data)","853f3da3":"(con,cat,disnum,data)=TreatNullValue(train_data)\n(con_test,cat_test,disnum_test,data)=TreatNullValue(test_data)\n","6de6c434":"print(\"continuous data\",con)\nprint(\"categorical data\",cat)\nprint(\"discreate numerical\",disnum)\nprint(\"shape of the dataset\",train_data.shape)","2cf7f09d":"def encode(data):\n    dummies_dataset=pd.get_dummies(data[cat])\n    print(\"Shape of dummy dataset\",dummies_dataset.shape)\n    #dropping the cat data from main data set \n    data.drop(cat,axis=1,inplace=True)\n    for i in dummies_dataset:\n        data[i]=dummies_dataset[i]\n    print(\"shape of the train data after dummy encoding \",data.shape)\n    return(data)","05d30179":"train_data=encode(train_data)\ntest_data=encode(test_data)","11e18097":"test_data.head()","660f7141":"from imblearn.over_sampling import SMOTE\nx_train =train_data.drop(\"Survived\",axis=1)\ny_train=train_data[\"Survived\"]\nprint('Before oversampling, counts of label 1',sum(y_train==1))\nprint('Before oversampling, counts of label 0',sum(y_train==0))\nsm = SMOTE (random_state=2)\nX_train_res,y_train_res = sm.fit_sample(x_train,y_train.ravel())\nprint(\"After sampling shape of X train and Ytrain \", X_train_res.shape,y_train_res.shape)\nprint('After oversampling, counts of label 1',sum(y_train_res==1))\nprint('After oversampling, counts of label 0',sum(y_train_res==0))","7a01063e":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train_res, y_train_res)\ny_pred_class=logreg.predict(test_data)\ny_pred_class","43ed8bf0":"df = pd.DataFrame(y_pred_class,index=None,columns=['Survived'])","30e3a333":"export_csv = df.to_csv(\"gender_submission1.csv\",index=False)","7f4aaff1":"### Null value Treatement ","5d8d833a":"## Visualization of the data ","5109419b":"\n### Encoding of the data","04642456":"### Balancing of data","7e62c87a":"## Preprocessing","737c8fe2":"#  EDA ","3c5b1647":"# Titanic: Machine Learning from Disaster"}}