{"cell_type":{"3633b613":"code","00ee3b1e":"code","9ed75416":"code","a403378b":"code","cc384e1f":"code","59e042eb":"code","621fd2b9":"code","bc532b50":"code","46cea522":"code","879750d0":"code","5e5ed7e7":"code","3e8c9ddf":"code","5a81c8d8":"code","b42cae95":"code","ded9e128":"code","b1db02e2":"code","c139bf32":"code","f55b1b6c":"code","f6ae229c":"code","178c979a":"code","94be6d61":"code","957307ef":"code","71827b40":"code","d1696393":"code","292335de":"code","bd5b621c":"code","19ac6ca4":"code","e275ec39":"code","2a8a263e":"code","b307b7fa":"code","6ddfaded":"code","9dd32571":"code","19810f14":"code","6a0f65da":"code","43c16fb7":"code","cebcad5d":"code","25ea00a9":"code","c267aa05":"code","a883ac2d":"code","262704c3":"markdown","8b0435ca":"markdown","62374b70":"markdown","f237b180":"markdown","2945d672":"markdown","2ecacfbe":"markdown","2e42ca11":"markdown","b4321be6":"markdown","8d363cdf":"markdown","976081c2":"markdown","dce5878b":"markdown","5bf2a396":"markdown","713c12c9":"markdown","46a72c79":"markdown","57dca5be":"markdown"},"source":{"3633b613":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00ee3b1e":"train = pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\ntest  = pd.read_csv (\"\/kaggle\/input\/fake-news\/test.csv\")\nsubmit= pd.read_csv(\"\/kaggle\/input\/fake-news\/submit.csv\")\n\ntrain.head()","9ed75416":"# Veri setlerimizin boyutlar\u0131 ; \nprint(f\"Train Shape : {train.shape}\")\nprint(f\"Test Shape : {test.shape}\")\nprint(f\"Submit Shape : {submit.shape}\")","a403378b":"train.info()","cc384e1f":"train.isnull().sum()","59e042eb":"train.dtypes.value_counts()\n","621fd2b9":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","bc532b50":"plt.figure(figsize = (9,9))\nsorted_counts = train['label'].value_counts()\nplt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n       autopct='%1.1f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 18}, shadow = True,\n        colors = sns.color_palette(\"Paired\")[7:])\nplt.text(x = -0.35, y = 0, s = 'Total Value: {}'.format(train.shape[0]))\nplt.title('Distribution of News in the Data Set', fontsize = 16);","46cea522":"#We are deleting data with null values.\n#Bo\u015f de\u011fer ta\u015f\u0131yan verileri silme i\u015flemi ger\u00e7ekle\u015ftiriyoruz.\ntrain.dropna(inplace=True)","879750d0":"#After the deleted data, we rearrange the data set indexes.\n# Silinen verilerden sonra veri seti indexlerini tekrar d\u00fczenliyoruz.\ntrain.reset_index(inplace=True)","5e5ed7e7":"test.isnull().sum()","3e8c9ddf":"test['text'].fillna('TEST',inplace=True)","5a81c8d8":"test.text.isnull().sum()","b42cae95":"import string \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","ded9e128":"test.head()","b1db02e2":"def ozel_karakter (text):\n    punctation = string.punctuation\n    return text.translate(str.maketrans(\"\",\"\",punctation))\n\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: ozel_karakter(x))\ntrain[\"text\"] = train[\"text\"].str.lower()\ntrain[\"text\"] = train[\"text\"].str.split()\n# Test veri setine uygulanmas\u0131\ntest[\"text\"] = test[\"text\"].apply(lambda x: ozel_karakter(x))\ntest[\"text\"] = test[\"text\"].str.lower()\ntest[\"text\"] = test[\"text\"].str.split()","c139bf32":"def stop_words_temizle (text):\n    words = set(stopwords.words(\"english\"))\n    return [i for i in text if i not in words]\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x : stop_words_temizle(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x : stop_words_temizle(x))\n","f55b1b6c":"def lemmatizasyon (text):\n    lemma = WordNetLemmatizer()\n    return [lemma.lemmatize(word= i , pos=\"v\") for i in text]\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: lemmatizasyon(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: lemmatizasyon(x))","f6ae229c":"def gurultu_sil (text):\n    return (' '.join([w for w in text if len(w)>2]))\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x : gurultu_sil(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x : gurultu_sil(x))","178c979a":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer","94be6d61":"# Apply to Train Data\n#  E\u011fitim Verisine Uygulanmas\u0131\ntfidf_v=TfidfVectorizer(max_features=5200,ngram_range=(1,3))\n\nX= tfidf_v.fit_transform(train[\"text\"]).toarray()","957307ef":"# Apply to Test Data \n# Test Verisine Uygulanmas\u0131\nX_test_data = tfidf_v.fit_transform(test[\"text\"]).toarray()\ncount_df_test = pd.DataFrame(X_test_data, columns=tfidf_v.get_feature_names())\ncount_df_test.head()\n","71827b40":"y = train[\"label\"]\ntrain.head()","d1696393":"vectorize_data = pd.DataFrame(X , columns =tfidf_v.get_feature_names())","292335de":"vectorize_data.head()","bd5b621c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(vectorize_data, y, test_size=0.2, random_state=0)","19ac6ca4":"X_train.head()","e275ec39":"y_train.head()","2a8a263e":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport itertools\n\nclassifier=MultinomialNB()\nclassifier.fit(X_train, y_train)\nprediction = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test, prediction)\nprint(\"accuracy:   %0.3f\" % score)","b307b7fa":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\nLR.fit(X_train, y_train)\npred_LR = LR.predict(X_test)\n\nscore_LR = metrics.accuracy_score(y_test, pred_LR)\nprint(\"accuracy:   %0.3f\" % score_LR)","6ddfaded":"prediction_test = LR.predict(count_df_test)\n","9dd32571":"prediction_test","19810f14":"submit.head()\n","6a0f65da":"submit['label']=prediction_test","43c16fb7":"submit.to_csv('submission_first.csv', index=False)\nfrom IPython.display import FileLink\nFileLink(r'submission_first.csv')\n","cebcad5d":"pred_test_nb = classifier.predict(count_df_test)\n","25ea00a9":"submit['label']=pred_test_nb\n","c267aa05":"submit.head()\n","a883ac2d":"submit.to_csv('submission_second.csv',index=False)\nFileLink(r'submission_second.csv')","262704c3":"<a id=\"8\"><\/a> <br>\n\n# TFIDF VECTOR\u0130ZER","8b0435ca":"<a id=\"3\"><\/a> <br>\n# Data Preprocessing - Veri \u00d6n \u0130\u015fleme","62374b70":"<a id=\"1\"><\/a> <br>\n# Heuristic Data Analysis - Ke\u015fifsel Veri Analizi\n","f237b180":"<a id=\"5\"><\/a> <br>\n\n## Tokenization - Tokenizasyon","2945d672":"<a id=\"9\"><\/a> <br>\n\n# Train-Test Split ","2ecacfbe":"<a id=\"11\"><\/a> <br>\n\n# Logistic Regression","2e42ca11":"### Veri Seti \u0130\u00e7erisindeki Haberlerin Da\u011f\u0131l\u0131mlar\u0131.\n### Distribution of News in the Data Set.","b4321be6":"<a id=\"10\"><\/a> <br>\n\n# Multinomial Naive-Bayes","8d363cdf":"<a id=\"4\"><\/a> <br>\n\n## Cleaning Special Character - \u00d6zel Karakterlerin Temizlenmesi","976081c2":"<a id=\"7\"><\/a> <br>\n\n## Deleting Noise Data From Data Set - Veri Setinden G\u00fcr\u00fclt\u00fclerin Silinmesi\n\n\nBasically, after performing all the necessary operations in text processing, noise occurs in our data set, so I delete words that are too short.\n\n\nTemel olarak metin i\u015flemede gerekli b\u00fct\u00fcn i\u015flemleri ger\u00e7ekle\u015ftirdikten sonra veri setimizde g\u00fcr\u00fclt\u00fc olu\u015fur , bu y\u00fczden \u00e7ok k\u0131sa olan kelimeleri siliyorum.\n\n![](http:\/\/)![1_T0nY31vvtKC4fuQgEsudbw.png](attachment:1_T0nY31vvtKC4fuQgEsudbw.png)","dce5878b":"<a id=\"13\"><\/a> <br>\n\n# SUBM\u0130T","5bf2a396":"# Fake News Detected\n\n1. [Heuristic Data Analysis:](#1)\n    1. [Visualization](#2)\n1. [Data Preprocessing:](#3)\n    1. [Cleaning Special Character](#4)\n    1. [Tokenization](#5)\n    1. [Lemmatize](#6)\n    1. [Deleting Noise Data From Data Set](#7)\n    1. [TFIDF VECTOR\u0130ZER](#8)\n1. [Train-Test Split ](#9)    \n1. [Multinoimal Naive Bayes ](#10)    \n1. [Logistic Regression ](#11)    \n1. [Prediction Test ](#12)    \n1. [Submit ](#13)    \n   ","713c12c9":"<a id=\"12\"><\/a> <br>\n\n## Prediction Test ","46a72c79":"<a id=\"6\"><\/a> <br>\n\n## Lemmatize","57dca5be":"<a id=\"2\"><\/a> <br>\n## Visualization - G\u00f6rselle\u015ftirme"}}