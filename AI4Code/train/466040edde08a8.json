{"cell_type":{"0e783e63":"code","4e6c4bd6":"code","60a3b178":"code","83da3caa":"code","5f334c25":"code","200ed848":"code","a4fe45d5":"code","bce252e2":"code","53891fd4":"code","0937f996":"code","efefcb1c":"code","0169aaef":"code","975bb860":"code","5dc0212a":"code","615f1b61":"code","56e710b2":"code","5a5f54a0":"code","10dace47":"code","9ea6624e":"code","cc9a37e9":"code","605b50a8":"code","601418df":"code","7b5f94ee":"code","8374bd09":"code","dcb50077":"code","f73e5bda":"code","67980756":"code","cfcc58c1":"code","3ff5f13b":"code","8864757e":"code","842602ce":"code","cd2d99eb":"code","b78e53da":"code","fed8326e":"code","cf58b68e":"code","92c29ee6":"code","9afad598":"code","361e2b06":"code","2f378d5b":"code","96c071b6":"code","8ddbab7d":"code","8c9ff39c":"code","de23c44a":"code","5a3b1d44":"code","4005c2ce":"code","1b34368b":"code","e06164ba":"code","afe59052":"code","4ae283fa":"code","9a7f0c91":"code","f599885d":"code","0d869cda":"code","3bf529b4":"code","3cceaffe":"code","10e8b98b":"code","5f20d172":"code","699b4836":"code","3fb5c7cb":"code","db2b7443":"code","315b5d12":"code","7ffa77bb":"code","9a56a9e7":"code","ff41dbe1":"code","a94caaed":"code","8a5d83c7":"code","2901e390":"code","724ed7cb":"code","c92de8a5":"code","9057b112":"code","cd9d6b27":"code","f42f9b75":"code","a9143626":"code","c9803065":"code","b407977a":"code","d1af1e4c":"code","dcd003c2":"code","ef60c248":"code","bca9ba51":"code","e89c045a":"code","000f28b8":"code","a2fb0fd1":"code","2a0b6111":"code","e72fda04":"code","e9b625ed":"code","454bee3a":"code","5c390be9":"code","82965d5b":"code","b6baad9b":"code","e0b96f8b":"code","fbf9b0f1":"code","9db29503":"code","77750b5e":"code","39ab20e6":"code","0689c276":"code","1882cc4b":"code","d06f4bbe":"markdown","7fb9673e":"markdown","b8f82ad2":"markdown","06532c99":"markdown","689048fb":"markdown","40b23d44":"markdown","bf78fe33":"markdown","db8e2544":"markdown","385e6c95":"markdown","f9a1dd9b":"markdown","07379b35":"markdown","ed8d70f7":"markdown","4010f077":"markdown","01916ec6":"markdown","dcd82180":"markdown","866bb7b3":"markdown","43f5145b":"markdown","0f399480":"markdown","1a280e52":"markdown","991da2eb":"markdown","a8677a00":"markdown","9aa4d917":"markdown","7dfbea09":"markdown","e1c54a0e":"markdown","db3484b5":"markdown","866937c5":"markdown","cebcfeb2":"markdown","c576f0e8":"markdown","5d6ab6bc":"markdown","8b0f8123":"markdown","dd171c88":"markdown","441ca41f":"markdown","e561c8f0":"markdown"},"source":{"0e783e63":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport zipfile\n#import visualkeras\n\nfrom sklearn.model_selection import train_test_split","4e6c4bd6":"print(os.listdir(\"..\/input\/dogs-vs-cats-redux-kernels-edition\"))","60a3b178":"with zipfile.ZipFile('..\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip','r') as z:\n    z.extractall('.')\n    \nwith zipfile.ZipFile('..\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip','r') as z:\n    z.extractall('.')","83da3caa":"os.listdir('\/kaggle\/working\/train\/')[:10]","5f334c25":"len(os.listdir('\/kaggle\/working\/train\/')), len(os.listdir('\/kaggle\/working\/test\/'))","200ed848":"print('Sample of file in train directory: ',os.listdir('\/kaggle\/working\/train\/')[0])\nprint('Sample of file in test directory: ',os.listdir('\/kaggle\/working\/test\/')[0])","a4fe45d5":"print('Label of image: ',os.listdir('\/kaggle\/working\/train\/')[0].split('.')[0])\nprint('Index of image: ',os.listdir('\/kaggle\/working\/train\/')[0].split('.')[1])\nprint('Format of image: ',os.listdir('\/kaggle\/working\/train\/')[0].split('.')[2])","bce252e2":"IMG_SIZE = 120\nImages_train = []\nImages_label = []\nfor i in os.listdir('\/kaggle\/working\/train\/'):\n    label = i.split('.')[0]\n    if label == 'cat':\n        label = 0\n    elif label == 'dog':\n        label = 1\n    img = cv2.imread('\/kaggle\/working\/train\/'+i, cv2.IMREAD_COLOR)\n    img = cv2.resize(img,(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_CUBIC)\n    Images_train.append([np.array(img), np.array(label)])","53891fd4":"Images_train[0]","0937f996":"Images_train[0][0]","efefcb1c":"Images_train[0][1]","0169aaef":"import random\n\nrandom.shuffle(Images_train)","975bb860":"Images = np.array([i[0] for i in Images_train]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\nLabel = np.array([i[1] for i in Images_train])","5dc0212a":"Images.shape, Label.shape","615f1b61":"pd.DataFrame(Label).value_counts()","56e710b2":"plt.figure(figsize=(15,15))\nfor k in range(10):\n    plt.subplot(2, 5, k+1)\n    img=random.randint(0,25000)\n    plt.imshow(Images[img])\n    plt.title('DOG' if Label[img]==1 else 'CAT')","5a5f54a0":"X_train, X_val, Y_train, Y_val = train_test_split(Images, Label, test_size = 0.1)","10dace47":"X_train.shape, Y_train.shape, X_val.shape, Y_val.shape","9ea6624e":"#X_train=X_train\/255.0\n#X_val=X_val\/255.0","cc9a37e9":"import tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","605b50a8":"datagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False) ","601418df":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('val_accuracy')>0.98):\n      print(\"\\nReached 98% accuracy so cancelling training!\")\n      self.model.stop_training = True\n        \ncallbacks = myCallback()\n\nfrom keras.callbacks import ReduceLROnPlateau\nlr_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                 patience=1, \n                                 verbose=1, \n                                 factor=0.5, \n                                 min_lr=0.000001)\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_accuracy', \n                               min_delta=0.005,\n                               patience=3, \n                               verbose=1, \n                               mode='auto')","7b5f94ee":"optimizer = Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)","8374bd09":"model=Sequential()\nmodel.add(Conv2D(64,(3,3),strides=1,padding='Same',activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],3)))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3), strides=1,padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3), strides=1,padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\n\n#optimizer = SGD(learning_rate=0.01)\n#optimizer = RMSprop(learning_rate=0.001,rho=0.9,momentum=0.0,epsilon=1e-07)\n#optimizer = Adadelta(learning_rate=0.001,rho=0.95, epsilon=1e-07)\nmodel.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])","dcb50077":"datagen.fit(X_train)","f73e5bda":"history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                              validation_data=(X_val,Y_val), epochs=20, verbose=1,\n                              callbacks=[callbacks, lr_reduction])","67980756":"pd.DataFrame(history.history)","cfcc58c1":"def metrics_plot(history):\n  acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs = range(len(acc))\n\n  plt.plot(epochs, acc, 'r', label='Training accuracy')\n  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n  plt.title('Training and validation accuracy')\n  plt.legend()\n  plt.figure()\n\n  plt.plot(epochs, loss, 'r', label='Training Loss')\n  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n  plt.title('Training and validation loss')\n  plt.legend()\n\n  plt.show()","3ff5f13b":"metrics_plot(history)","8864757e":"from keras.models import load_model\n\nmodel.save('CNN_model.h5')","842602ce":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input","cd2d99eb":"from tensorflow.keras import Model","b78e53da":"model_RN=Sequential()\nmodel_RN.add(ResNet50(input_shape=(120,120,3),\n            include_top=False,\n            weights='imagenet',\n            pooling='max'))","fed8326e":"model_RN.summary()","cf58b68e":"model_RN.layers[0].trainable=False\nmodel_RN.summary()","92c29ee6":"model_RN.add(Dense(512,activation='relu'))\nmodel_RN.add(Dropout(0.2))\nmodel_RN.add(Dense(1,activation='sigmoid'))","9afad598":"model_RN.summary()","361e2b06":"model_RN.layers","2f378d5b":"model_RN.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","96c071b6":"history2 = model_RN.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                                  validation_data=(X_val,Y_val), epochs=20, verbose=1,\n                                  callbacks=[callbacks, lr_reduction, early_stopping])","8ddbab7d":"pd.DataFrame(history2.history)","8c9ff39c":"metrics_plot(history2)","de23c44a":"model_RN.save('ResNet_model.h5')","5a3b1d44":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input","4005c2ce":"model_VGG=Sequential()\nmodel_VGG.add(VGG16(input_shape=(120,120,3),\n                    include_top=False,\n                    pooling='max',\n                    weights='imagenet'))","1b34368b":"model_VGG.summary()","e06164ba":"model_VGG.layers[0].trainable=False\nmodel_VGG.summary()","afe59052":"model_VGG.add(Dense(512,activation='relu'))\nmodel_VGG.add(Dropout(0.2))\nmodel_VGG.add(Dense(1,activation='sigmoid'))","4ae283fa":"model_VGG.summary()","9a7f0c91":"model_VGG.layers","f599885d":"model_VGG.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","0d869cda":"history3 = model_VGG.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                                  validation_data=(X_val,Y_val), epochs=20, verbose=1,\n                                  callbacks=[callbacks, lr_reduction, early_stopping])","3bf529b4":"pd.DataFrame(history3.history)","3cceaffe":"metrics_plot(history3)","10e8b98b":"model_VGG.save('VGG_model.h5')","5f20d172":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input","699b4836":"model_EF=Sequential()\nmodel_EF.add(EfficientNetB0(input_shape=(120,120,3),\n                            include_top=False,\n                            pooling='max',\n                            weights='imagenet'))","3fb5c7cb":"model_EF.summary()","db2b7443":"model_EF.layers[0].trainable=False\nmodel_EF.summary()","315b5d12":"model_EF.add(Dense(512,activation='relu'))\nmodel_EF.add(Dropout(0.2))\nmodel_EF.add(Dense(1,activation='sigmoid'))","7ffa77bb":"model_EF.summary()","9a56a9e7":"model_EF.layers","ff41dbe1":"model_EF.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])","a94caaed":"history4 = model_EF.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                                  validation_data=(X_val,Y_val), epochs=10, verbose=1,\n                                  callbacks=[callbacks, lr_reduction, early_stopping])","8a5d83c7":"pd.DataFrame(history4.history)","2901e390":"metrics_plot(history4)","724ed7cb":"model_EF.save('EF_model.h5')","c92de8a5":"from keras.models import load_model","9057b112":"model1 = load_model('..\/input\/models-saved\/CNN_model.h5')\nmodel2 = load_model('..\/input\/models-saved\/ResNet_model.h5')\nmodel3 = load_model('..\/input\/models-saved\/VGG_model.h5')\nmodel4 = load_model('..\/input\/models-saved\/EF_model.h5')","cd9d6b27":"train_loss_cnn, train_acc_cnn = model1.evaluate(X_train,  Y_train, verbose=2)\ntest_loss_cnn, test_acc_cnn = model1.evaluate(X_val,  Y_val, verbose=2)","f42f9b75":"train_loss_rn, train_acc_rn = model2.evaluate(X_train,  Y_train, verbose=2)\ntest_loss_rn, test_acc_rn = model2.evaluate(X_val,  Y_val, verbose=2)","a9143626":"train_loss_vgg, train_acc_vgg = model3.evaluate(X_train,  Y_train, verbose=2)\ntest_loss_vgg, test_acc_vgg = model3.evaluate(X_val,  Y_val, verbose=2)","c9803065":"train_loss_ef, train_acc_ef = model4.evaluate(X_train,  Y_train, verbose=2)\ntest_loss_ef, test_acc_ef = model4.evaluate(X_val,  Y_val, verbose=2)","b407977a":"data = {'Scratch model':[train_acc_cnn,train_loss_cnn,test_acc_cnn,test_loss_cnn],\n        'ResNet50':[train_acc_rn,train_loss_rn,test_acc_rn,test_loss_rn],\n        'VGG16': [train_acc_vgg,train_loss_vgg,test_acc_vgg,test_loss_vgg],\n        'EfficientNet': [train_acc_ef,train_loss_ef,test_acc_ef,test_loss_ef]}\n \npd.DataFrame(data, index=['Train accuracy','Train loss','Val accuracy','Val loss'])","d1af1e4c":"model3.summary()","dcd003c2":"predicted_val_prob = model3.predict(X_val, batch_size=32)","ef60c248":"predicted_val_prob","bca9ba51":"Y_val_pred= np.round(predicted_val_prob)","e89c045a":"Y_val_pred","000f28b8":"from sklearn.metrics import classification_report\n\nreport = classification_report(Y_val, Y_val_pred)\n\nprint(report)","a2fb0fd1":"from sklearn.metrics import confusion_matrix\n\nf,ax = plt.subplots(figsize=(15, 15))\nconfusion_mtx = confusion_matrix(Y_val, Y_val_pred)\nsns.set(font_scale=1.4)\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\",ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix Validation set\")\nplt.show()","2a0b6111":"l = []\nfor i in range(len(Y_val_pred)):\n    if Y_val[i] != Y_val_pred[i]:\n        l.append(i)","e72fda04":"print('Number of misclassifications in validation dataset: ', len(l))","e9b625ed":"l[:20]","454bee3a":"plt.figure(figsize=(35,35))\nc = 1\nfor i in l[:20]:\n    plt.subplot(4,5, c)\n    plt.imshow(X_val[i])\n    plt.title('DOG:{}\\nTrue label:{}'.format(predicted_val_prob[i], Y_val[i])\n              if predicted_val_prob[i]>= 0.5 else 'CAT:{}\\nTrue label:{}'.format(predicted_val_prob[i],Y_val[i]))\n    plt.axis('off')\n    c = c+1","5c390be9":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import label_binarize","82965d5b":"metrics = []\nmodels = ['VGG16 model']\npredictions=[Y_val_pred]\n\nfor lab,i in zip(models, predictions):\n    precision, recall, fscore, _ = score(Y_val, i, average='weighted')\n    accuracy = accuracy_score(Y_val, i)\n    auc = roc_auc_score(label_binarize(Y_val, classes=[0,1]),\n                        label_binarize(i, classes=[0,1]),\n                        average='weighted')\n    metrics.append(pd.Series({'precision':precision, 'recall':recall,\n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, name=lab))\n    \nmetrics = pd.concat(metrics, axis=1)","b6baad9b":"metrics","e0b96f8b":"os.listdir('\/kaggle\/working\/test\/')[:20] ","fbf9b0f1":"print('Sample of image: ',os.listdir('\/kaggle\/working\/test\/')[0])\nprint('Index of image: ',os.listdir('\/kaggle\/working\/test\/')[0].split('.')[0])\nprint('Format of image: ',os.listdir('\/kaggle\/working\/test\/')[0].split('.')[1])","9db29503":"Images_test = []\nfor j in os.listdir('\/kaggle\/working\/test\/'):\n    index = j.split('.')[0]\n    img = cv2.imread('\/kaggle\/working\/test\/'+j, cv2.IMREAD_COLOR)\n    img = cv2.resize(img,(IMG_SIZE,IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n    Images_test.append([np.array(img), np.array(index)])","77750b5e":"X_test = np.array([j[0] for j in Images_test]).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\nIndex = np.array([j[1] for j in Images_test])","39ab20e6":"test_prediction = model3.predict(X_test, batch_size = 32)","0689c276":"submission=pd.DataFrame(test_prediction, columns=['label'], index=pd.Series(Index, name='id'))\nsubmission.head()","1882cc4b":"submission.to_csv('submission.csv')","d06f4bbe":"Using the os library we can print the files contained inside a directory, this should contain 3 files as can be seen below:","7fb9673e":"The following network was evaluated with four optimizers (Adam, SGD, RMSProp and Adadelta) and we got the highest train\/validation accuracy using Adam, as can be seen below:","b8f82ad2":"## Prediction of testing images:","06532c99":"Using the os.listdir function let's print the name of some images contained in the test folder: ","689048fb":"Let's see the classification report for this model, we can see the considerably high metrics, later we will in more detail why every model didn't achieve a perfect performance.","40b23d44":"## VGG16:","bf78fe33":"Now I'm going to load the 4 models saved and compute their corresponding metrics which should match those from the last epoch for each one.","db8e2544":"One we have finished the preprocessing step we have to reshape our images to a 4-dimentional array were (number of images, width, height, color channels), and our labels keep being the same:","385e6c95":"In order to create some changes in our images we can use the ImageDataGenerator for data augmentation, as these correspond to images of animals and we know they can be in different positions we can play with lots of arguments of this function below:","f9a1dd9b":"Our images in training dataset are labeled differently than we are used to see, in the current project the images contain their class in the name of the file. i.e. cat.11724.jpg means image of cat, index 11724, jpg format. For this reason we have to iterate through each image and assign its respective class according to the name as can be seen in the code below:","07379b35":"## ResNet50:\n\nEvery pre-trained network will make use of the weights belonging to imagenet.","ed8d70f7":"Once we have this probabilities we have to convert it to discrete values, either 1 or 0, such task can be achieved by using the np.round function:","4010f077":"Let's see the first instance of our training dataset preprocessed, this should contain 2 objects, the first one corresponds to the RGB image as numpy array and the second is the class 'label':","01916ec6":"The list 'l' is storing the indexes of those instances which were misclassified, having said that let's see the first 20 misclassifications and then show these images with their corresponding sigmoid output (probability):","dcd82180":"Given the computed metrics for every model loaded I will summarize and show a table which can facilitate comparing them:","866bb7b3":"Above we see only 20 misclassifications, what is more worrying is that some of these images had a relatively high probability of prediction (let's say ~0.8 and ~0.2), which is bad if we want to have a high accuracy when predicting the classes of testing images. One big reason could be that some of them belong to uncommon or less frequent breeds of dogs and cats, thus there are present only a few pictures of them. This problem could be solved by adding more images in order to balance the breeds in the training set. Another big problem is that some of the images do not contain only one animal or 'object', some of them contain humans close to animals, some contain cats and dogs in same image, some contain multiple images, etc. This without a doubt sidetracks the prediction of every model and for such images the method to use should be 'Object detection', one of the well-known models are YOLO's which as output of any image highlights each object contained drawing a bounding box and the class predicted with corresponding probability.\n\nAs I said in order to correct such problems YOLO approaches should be used and this is a challenge for a next project, which I will be working on.\n\nAs a final step let's compute and show the error metrics (recall, precision, f1-score, accuracy and AUC) for the best model.","43f5145b":"As the testing dataset does not contain the labels included in the name of the image we will see how they are presented printing one sample of them:","0f399480":"The numpy arrays representing the images will be stored in a new variable 'X_test' from which will be predicted the class. ","1a280e52":"test.zip and train.zip contains the images to use in our project, because of this we have to extract them using the library zipfile, the images extracted will appear in the output directory of kaggle '\/kaggle\/working\/', path which we have to take to explore each image.","991da2eb":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects\/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","a8677a00":"The model which had the best performance was the VGG16 with 119 misclassification out of 2500 instances, as I said before such number can change due to stochastic nature of these models, since now I will use VGG16 to predict the classes of instances stored in testing folder, but firstly let's plot a sample of the images misclassified and see if there is a pattern or general reason of this problem.","9aa4d917":"Let's create two constraints or 'callbacks' which can help us improve the training (ReduceLROnPlateau) and stop the training once it has reached a high threshold (Callback):","7dfbea09":"In the table above we can see the model build by scrath is the one with lowest performance, despite it is not too far from the others the difference is significant. About the pre-trained models again there is not a big difference, but something important to look and take into account is that when we trained these models the metrics were different than in the table above, the reason of such change could be the stochastic nature of convolutional networks have when evaluating on different batches. Having said that I will continue with VGG16 for having the best accuracy on training and validation sets, obviously you can choose whatever you want, but such model outstandingly!\n\nLet's see the summary of the model chosen:","e1c54a0e":"In the following line we will apply such idea were if the label corresponds to a cat the class will be '0', whereas if it's a dog the class will be '1', then the images will be read as RGB channels in resized to 150 pixels for width and height, to finally store the image and its class as one instance inside a list.","db3484b5":"# Modeling:\n\nThe following models will be built and compared using their corresponding error measurements:\n\n- Convolutional Neural Network by scratch.\n- Pre-trained ResNet50.\n- Pre-trained VGG16.\n- Pre-trained EfficientNetB0.\n\nLet's import some libraries useful in the process of building the first network by scratch:","866937c5":"## EfficientNetB0:\n\nImportant to mention that InceptionV3, Xception, ResNet152V2, DenseNet201 didn't work well, I mean reached no more than 0.75 in validation accuracy, I tried with EfficientNetB0 and finally I reached a satisfactory performance as you will see below:","cebcfeb2":"Above the list of images is huge and we would be better if we print the length of such lists to see how many images each folder contain:","c576f0e8":"The following function will extract the index of the image and the image as numpy array and save both as an instance in the list Images_test:","5d6ab6bc":"We can see the images look perfect and the next step is splitting into training and validation sets to be used in modeling process:","8b0f8123":"As the preprocessing was applied in the same order as the images were stored in the folder training we have to take into account that these were stored as the first 12500 images were cats and second 12500 were dogs, such sorting can make our model perform poorly and thus will be shuffled. ","dd171c88":"Remember that as this project is binary classification the last layer had sigmoid activation function, therefore our predicted output will have values of probabilities between 0-1, where 1 means 'Dog' and 0 means 'Cat':","441ca41f":"Let's print the list of images contained in the train folder, as we know this contain 25000 images which is huge and in order to just have an idea how they are named 10 samples of them will be shown:","e561c8f0":"Let's select 10 images randomly from our training dataset and show with their corresponding label:"}}