{"cell_type":{"090d9cfb":"code","ec3b5657":"code","a886930e":"code","3c179913":"code","7a9914a8":"code","cd9bab59":"code","a1a16156":"code","18b22936":"code","9047779a":"code","b8952420":"code","8f8ccb86":"code","3508b291":"code","2f712770":"code","d941fe14":"code","d500612e":"code","47769ff0":"code","7c894e0f":"code","bc238df3":"code","98e10d5f":"code","4802ba03":"code","d522ca6a":"code","d7f599ee":"code","8278fb5b":"code","b588efeb":"code","18455d2a":"code","cea4c2fc":"code","000f9783":"code","0003d01e":"code","8918a966":"code","8b3853bb":"code","62a505f9":"code","9e364d09":"code","48d7946c":"code","6c2d1445":"code","7c12907f":"markdown","db46072d":"markdown","986238bf":"markdown","9c19efb2":"markdown","b0597010":"markdown","f96cd148":"markdown","5e998227":"markdown","892f7a12":"markdown","eb5fa2eb":"markdown","7fa4bde6":"markdown","22bd0548":"markdown","45ed2fe9":"markdown","d426421a":"markdown"},"source":{"090d9cfb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec3b5657":"# First Read Dataset\nX = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ny = X['Survived']\nX.drop(['Survived' , 'PassengerId'] , axis = 1 , inplace = True)\nX_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint('X shape = ',X.shape)\nX.head(20)","a886930e":"#check nan values \nX.isna().sum()","3c179913":"X['Age'].isna().sum()","7a9914a8":"#check duplicate\nX.duplicated().sum()","cd9bab59":"# check datatype and get sum summary about data\nprint(X.info() , '\\n')\nX.describe().T","a1a16156":"# SibSp Feature \" \"\n\naxis=sns.barplot(x ='SibSp' , y = y , data=X)\naxis.set_title('Siblings\/Spouses v.s Survived')\naxis.set_xlabel('Siblings\/Spouses')\n\naxis.set_ylabel('Survived Percentage')\n\n# Conclusion :  people with one or two Siblings more survived\n# it sound an important feature","18b22936":"# Parch Feature \" \"\n\naxis=sns.barplot(x ='Parch' , y = y , data=X)\naxis.set_title('Num of Parent and children  v.s  Survived')\naxis.set_xlabel('Num of Parent and children ')\naxis.set_ylabel('Survived Percentage')\n\n# Conclusion :  people with (1,2,3) Parent or children  more survived\n# may be this people belongs to pClass = 1 or 2\n# it sound an important feature\n\nX[((X['Parch'] == 1) | (X['Parch'] == 3)) & (y == 0) ]['Pclass'].value_counts()","9047779a":"# Pclass Feature \" \"\n\naxis=sns.countplot(x ='Pclass' , hue = y , data=X)\naxis.set_title('Pclass  v.s  Survived')\naxis.set_xlabel('Pclass ')\n\n\n\naxis.legend(['Unsurvived',\"Survived\"] ,loc= \"upper left\")","b8952420":"# Embarked  Feature \" \"\n\naxis=sns.barplot(x ='Embarked' , y = y , data=X)\naxis.set_title('Twon  v.s  Survived')\naxis.set_xticklabels(['Southampton (S)' , 'Cherbourg (C)','Queenstown (Q)'])\naxis.set_xlabel('Town Name ')\naxis.set_ylabel('Survived Percentage')\n\n# Conclusion : most of people from 'Cherbourg (C) are survived , most of people from Southampton (S) and Queenstown unsurvived\n# it sound an important feature\n","8f8ccb86":"# Sex  Feature \" \"\n\naxis=sns.barplot(x ='Sex' , y = y , data=X)\naxis.set_title('Gender  v.s  Survived')\n# axis.set_xticklabels(['Southampton (S)' , 'Cherbourg (C)','Queenstown (Q)'])\naxis.set_xlabel('Gender')\naxis.set_ylabel('Survived Percentage')\n\n# Conclusion : female is more survived than male\n# it sound an important feature\n","3508b291":"# Age  Feature \" \"\n\n# Create bins for age ranges separation\nbins = [ 0, 5, 10, 15, 20, 30, 40,45,50,55,60 ,65,70 ,75,80, np.inf]\n\n# Label of age ranges\n# labels = [' 0-5', ' 5-12', ' 12-18', ' 18-24', '  24-35', ' 35-60', ' 60-']\nlabels = [ '0', '5', '10', '15', '20', '30', '40','45','50','55','60' ,'65','70' ,'75','80']\n\n\n# Assign labels to bins\nage = pd.cut(X[\"Age\"], bins, labels = labels)\n\n# axis=sns.barplot(age , y=y )\nplt.figure(figsize=(8,5))\naxis = sns.countplot(x = age , hue=  y , data = X)\naxis.set_title('Age v.s Survived')\n\naxis.set_xlabel('Age')\n# axis.set_ylabel('Survived Percentage')\n_=axis.set_xticklabels(labels,rotation  = 30)\n\n\n\n# Conclusion : babies are more survived than other ages\n# it sound an important feature\n","2f712770":"\nfig, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(X.corr(), vmin=-1, vmax=1,square=True,annot=True, cmap='BrBG')","d941fe14":"## Age have a high correlation with Pclass and Sex \n## compute missing value in age based on the median ofother age rows with pclass and Sex\n\n\n# X['Age'] = X.groupby(['Sex' , 'Pclass'])['Age'].apply(lambda x : x.fillna(x.median()))\n# X_test['Age'] = X_test.groupby(['Sex' , 'Pclass'])['Age'].apply(lambda x : x.fillna(x.median()))\n\n# # check nan values : \n# X_test.isna().sum()\n","d500612e":"# Age :\n\nsns.distplot(X[ y== 0]['Age']  ,label='Not Survived', color = '#e74c3c')\nsns.distplot(X[y == 1]['Age'] , label='Survived' ,color = '#2ecc71')\n\nplt.legend()\nplt.title('Distribtion of Survived with Age')","47769ff0":"# Age :\naxis = sns.distplot(X[y == 0]['Fare'] , hist = True ,label='Not Survived', color = '#e74c3c')\nsns.distplot(X[y == 1]['Fare'] , label='Survived' ,color = '#2ecc71')\nplt.legend()\n# plt.xlim([-5,200])\nplt.title('Distribtion of Survived with Fare')\n\nprint('Survived along with Fare {0}'.format(X[y == 1]['Fare'].count()))\nprint('Not Survived along with Fare {0}'.format(X[y == 0]['Fare'].count()))\n","7c894e0f":"# plot scatter with all numaric data\n\nfrom pandas.plotting import scatter_matrix\n\n\n# scatter_matrix(X, figsize=(12, 10) , alpha=0.5)\n# plt.subplots_adjust(wspace=.2, hspace=.3)","bc238df3":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MaxAbsScaler\n\n\n# Define NumericalTransformer To Imute missing data and drop\nclass NumericalTransformer(BaseEstimator , TransformerMixin):\n    \n    def __init__(self , scaler = MaxAbsScaler() ,strategy = 'mean'):\n        self.strategy = strategy\n        self.scaler = scaler\n        \n    \n    \n    def fit(self , df , y=None):\n        return self\n    \n    def transform(self , df , y= None):\n        \n        ## Add New Feature :\n#         df['family_size'] = df['Parch'] + df['SibSp'] + 1\n        \n        ## then Drop Parch , SibSp column \n#         df = df.drop(['Parch' , 'SibSp'] , axis = 1)\n\n        #impute missing value  with Mean Or Midean\n        imuter = SimpleImputer(strategy = self.strategy)\n        df = imuter.fit_transform(df)\n        \n        #scale data \n        df = self.scaler.fit_transform(df)\n        \n        return df\n        \n        ","98e10d5f":"# Define NumericalTransformer To Imute missing data and drop\nclass CategoricalTransformer(BaseEstimator , TransformerMixin):\n    \n    def __init__(self ,encoder = OneHotEncoder(handle_unknown='ignore') , strategy = 'most_frequent'):\n        self.strategy = strategy\n        self.encoder = encoder \n        \n    \n    \n    def fit(self , df , y=None):\n        return self\n    \n    def transform(self , df , y= None):\n        \n        # drop un-needed columns\n        df = df.drop(['Cabin' , 'Ticket' , 'Name'] , axis = 1)\n\n        #impute missing value  with Mean Or Midean\n        imuter = SimpleImputer(strategy = self.strategy)\n        df = imuter.fit_transform(df)\n        \n        #Encode Cat data  \n        df = self.encoder.fit_transform(df)\n        \n        return df","4802ba03":"np.random.seed(0)\nfrom sklearn.model_selection import train_test_split\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","d522ca6a":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nnumeric_featured = [col for col in X.columns  if X[col].dtype != 'object']\ncat_featured = [col for col in X.columns  if X[col].dtype == 'object']\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', NumericalTransformer(MaxAbsScaler() , 'mean'), numeric_featured),\n        ('cat', CategoricalTransformer(), cat_featured),\n    ])","d7f599ee":"## Just Try A simple Model to test our Work and pipline\n# SGDClassifier Classifier : \nfrom sklearn.linear_model import SGDClassifier\n# accuricy score to validate our result\nfrom sklearn.metrics import accuracy_score\n\nmodel = SGDClassifier(max_iter=1000 ,random_state=0)\n\n# create pipline\nmy_pipline = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model' , model)\n])\n\n# train model\nmy_pipline.fit(X_train , y_train)\n\n#predict y\ny_pred = my_pipline.predict(X_valid)\n\naccuracy_score(y_valid , y_pred)","8278fb5b":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRFClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score , GridSearchCV \nfrom xgboost import XGBClassifier\n\nall_models ={\n    \"RandomForestClassifier\" : RandomForestClassifier(n_estimators=1000, max_depth=5,random_state=0),\n    \"XGBClassifier\" : XGBClassifier(colsample_bytree= 0.6,gamma= .5, max_depth= 5\n                                    ,min_child_weight= 5,subsample= 1.0),\n    \"XGBRFClassifier\" : XGBRFClassifier(max_depth= None,n_estimators= 10),\n    \"SVC\" : SVC(kernel='rbf' , C = 10 , gamma=1),\n    \"LogisticRegression\" : LogisticRegression(),\n    \"AdaBoostClassifier\" :AdaBoostClassifier(),\n    \"SGDClassifier\" : SGDClassifier(max_iter=1000)\n}\n\n\nscores = []\n\nfor key , mdl in  all_models.items():\n    pip = Pipeline(steps=[\n        ('preprocessor',preprocessor),\n        ('model' , mdl) ])\n    \n    score = cross_val_score(pip ,X , y , cv = 3 ,  scoring=\"accuracy\").mean()\n    scores.append(score)\n    \nmodels_df = pd.DataFrame({'model': list(all_models.keys()) , 'score' : scores})  \nmodels_df.sort_values(by='score', ascending=False)\n","b588efeb":"## transfoem X to use it in GridSearch\n# create pipline\n\nX_transformed = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n]).fit_transform(X)\n","18455d2a":"# A parameter grid for XGBClassifier\n# params = {\n#         'min_child_weight': [1, 5, 10],\n#         'gamma': [0.5, 1, 1.5, 2, 5],\n#         'subsample': [0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5]\n#         }\n\n\n\n# grid = GridSearchCV(estimator=XGBClassifier(), \n#                     param_grid=params,\n#                     cv=3,\n#                     verbose=2,\n#                     )\n\n# xgbclassifer_better = grid.fit(X_transformed,y)\n# xgbclassifer_better.best_params_","cea4c2fc":"# create the grid XGBRFClassifier\n# n_estimators = [10, 100, 1000, 2000]\n# max_depth = [None, 5, 10, 20]\n# param_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\n\n# grid = GridSearchCV(estimator=RandomForestClassifier(), \n#                     param_grid=param_grid,\n#                     cv=3,\n#                     verbose=2)\n\n# xgbrf_classifier_better = grid.fit(X_transformed,y)\n# xgbrf_classifier_better.best_params_","000f9783":"# ## Grid Search for SVM\n\n# defining parameter range\n# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n#               'gamma': [1, 0.1, 0.01, 0.001 , .0001],\n#               'kernel': ['rbf']} \n  \n# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n  \n# # fitting the model for grid search\n# svm_better  = grid.fit(X_transformed, y)\n# svm_better.best_params_","0003d01e":"from sklearn.ensemble import VotingClassifier\n\n# select two top model of accuricy based on previous output\nbest = VotingClassifier([('svc' , all_models['SVC']) , ('xgbrf_classifier',all_models['XGBRFClassifier']), \n                         ('random_forest',all_models['RandomForestClassifier'])])\n\n#create pipline with best_model \nmy_pipline = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model' , best)\n])\nmy_pipline.fit(X_train , y_train)\n\npredicted = my_pipline.predict(X_valid)\n","8918a966":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix , roc_curve , auc ,f1_score\n\n\nM = confusion_matrix(y_valid, predicted)\ntn, fp, fn, tp = M.ravel() \nM\n# plotting the confusion matrix\nplot_confusion_matrix(my_pipline, X_valid, y_valid)\nprint('F1_score = ',f1_score(y_valid , predicted))\naccuracy_score(y_valid , predicted)\n","8b3853bb":"fpr, tpr, threshold =roc_curve(y_valid , predicted)\nroc_auc = auc(fpr, tpr)\n\n# method I: plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# method II: ggplot\nfrom ggplot import *\ndf = pd.DataFrame(dict(fpr = fpr, tpr = tpr))\nggplot(df, aes(x = 'fpr', y = 'tpr')) + geom_line() + geom_abline(linetype = 'dashed')\n","62a505f9":"X_test.head()","9e364d09":"passengerId = X_test['PassengerId']\nX_test.drop( 'PassengerId' , axis = 1  , inplace = True )\nX_test.head()","48d7946c":"y_predict = my_pipline.predict(X_test)","6c2d1445":"## submision file \n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = passengerId\nsubmission_df['Survived'] = y_predict\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","7c12907f":"### **Create Transformation Step using Pipline **","db46072d":"## Testing","986238bf":"### **Split data to train and validation**\n\n","9c19efb2":"### Voting Majority to enhance prediction ","b0597010":"### ** GridSearch result for XGBClassifier {colsample_bytree= 0.8,gamma= 1, max_depth= 5\n#### ,min_child_weight= 5,subsample= 0.8)} used i above fit **\n    \n    ","f96cd148":"### decision making after EDA : \n #### drop ['cabin','Ticket' , 'Name'] feature \n #### calculate the missing value in 'Age' and 'Embarked' feature\n #### scale data with diffrent approach\n #### encode cat features\n ","5e998227":"#### **GridSearch for  XGBRFClassifier {'max_depth': None, 'n_estimators': 1000}\nUsed In above fit  **","892f7a12":"### Model Selection Step : \n   ##### We Start With Simple Model ","eb5fa2eb":"#### **GridSearch result of SVM {C =10  , gamma = 1} , used in above fit **","7fa4bde6":"## Explore continous features:\n### Age , Fare","22bd0548":"## Exploration Step","45ed2fe9":"## **Dealing With Missing Value** : \n  #### Age :","d426421a":"### **Try multiple Models with Cross_val to select the best **"}}