{"cell_type":{"12872917":"code","9d3667e2":"code","598408bc":"code","08999920":"code","2e9a8934":"code","14b48725":"code","984fc74a":"code","d0cccded":"code","4dc660b7":"code","a0409080":"code","e03edf8b":"code","813a28d6":"code","e2aee262":"markdown","7481f177":"markdown","d4b2c807":"markdown","ac204a91":"markdown","c362d062":"markdown","e5f1a9c0":"markdown","d19cecd0":"markdown","aee48c95":"markdown","c93aec8b":"markdown","3bcc6a85":"markdown","316d3b13":"markdown","e19bf36a":"markdown","6b6da947":"markdown","474e1ece":"markdown","45bf1491":"markdown","194dc5c3":"markdown","69313c8e":"markdown","6e3d147d":"markdown","dd12909a":"markdown","2a9d2f99":"markdown"},"source":{"12872917":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets.samples_generator import make_blobs, make_circles\nfrom mpl_toolkits import mplot3d\nfrom ipywidgets import interact, fixed\n# Any results you write to the current directory are saved as output.\niris = datasets.load_iris()","9d3667e2":"X = iris.data[:, [2, 0]]\nX = X[:100]\ny = iris.target\ny = y[:100]\nsvm = SVC(C=0.5, kernel='linear')\nsvm.fit(X, y)\n\nfig = plt.figure(figsize=(16, 10))\n# Plotting decision regions\nplot_decision_regions(X, y, clf=svm, legend=2, zoom_factor=4.0)\n\n# Adding axes annotations\nplt.xlabel('petal length [cm]')\nplt.ylabel('sepal length [cm]')\nplt.title('SVM')\nplt.show()","598408bc":"fig = plt.figure(figsize=(16, 10))\nplt.scatter(x=[1,2,3,4,4,3.5,3.1,2.5,2.2, 4.9], y=[1,2,3,4,5,2,3,1,2,4], marker='o', s=100)\nplt.scatter(x=[5.5, 6, 7, 8, 7,6.4,7.7,7.6, 5.4,6], y=[4,3,5,6,7,6,7,5,6,7.7], marker='+', s=100)\nplt.plot([5.3,5], [0,8], 'ro-')","08999920":"fig = plt.figure(figsize=(16, 10))\nplt.scatter(x=[1,2,3,4,4,3.5,3.1,2.5,2.2, 4.9], y=[1,2,3,4,5,2,3,1,2,4], marker='o', s=100)\nplt.scatter(x=[5.5, 6, 7, 8, 7,6.4,7.7,7.6, 5.4,6, 7.25], y=[4,3,5,6,7,6,7,5,6,7.7, 4.4], marker='+', s=100)\nplt.plot([5.3,5], [0,8], 'ro-')\nplt.plot([5.15, 7.2], [4,4.4], '>-g', color='green')\nplt.text(4.95, 4, 'B', fontsize=22)\nplt.text(7.3, 4.4, 'A', fontsize=22)\nplt.plot([5.1, 7.2], [4+2,4.4+2], '>-g', color='blue')\nplt.text(6, 5.6, '$W$', fontsize=22)\nplt.text(6, 3.5, '$\\gamma^{(i)}$', fontsize=22)","2e9a8934":"c = plt.Circle((0.5, 0.5), 0.2, fill=False)\nfig, ax = plt.subplots(figsize=(16, 10))\nax.add_artist(c)\nplt.text(0.45, 0.5, '$||w|| = 1$', fontsize=22)","14b48725":"y = [1 for i in range(2450)] + [-1 for i in range(2550)]\nnp.random.shuffle(y)\nx = np.random.randint(1, 1000 + 1, size=5000)\nw = -1\nb = 1\ngamma_hat = (y * (w * x + b))\nfig, ax = plt.subplots(figsize=(16, 10))\nplt.scatter(x, gamma_hat)","984fc74a":"X, y = make_blobs(n_samples=50, centers=2,\n                  random_state=0, cluster_std=0.60)\n\nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n\nfig, ax = plt.subplots(figsize=(16, 10))\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# create grid to evaluate model\nx = np.linspace(xlim[0], xlim[1], 30)\ny = np.linspace(ylim[0], ylim[1], 30)\nY, X = np.meshgrid(y, x)\nxy = np.vstack([X.ravel(), Y.ravel()]).T\nP = model.decision_function(xy).reshape(X.shape)\n\n# plot decision boundary and margins\nax.contour(X, Y, P, colors='k',\n           levels=[-1, 0, 1], alpha=0.5,\n           linestyles=['--', '-', '--'])\n\n# plot support vectors\nax.scatter(model.support_vectors_[:, 0],\n           model.support_vectors_[:, 1],\n           s=300, linewidth=1, facecolors='none');\nax.set_xlim(xlim)\nax.set_ylim(ylim)","d0cccded":"fig, ax = plt.subplots(figsize=(16, 10))\ndef plot_svc_decision_function(model, ax=None, plot_support=True):\n    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # create grid to evaluate model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n    \n    # plot decision boundary and margins\n    ax.contour(X, Y, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    \n    # plot support vectors\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=1, facecolors='none');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\nX, y = make_circles(100, factor=.1, noise=.1)\n\nclf = SVC(kernel='linear').fit(X, y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf, plot_support=False)","4dc660b7":"fig, ax = plt.subplots(figsize=(16, 10))\nr = np.exp(-(X ** 2).sum(1))\n\ndef plot_3D(elev=30, azim=30, X=X, y=y):\n    ax = plt.subplot(projection='3d')\n    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('r')\n\nplot_3D(X=X, y=y)","a0409080":"fig, ax = plt.subplots(figsize=(16, 10))\nclf = SVC(kernel='rbf', C=1E6, gamma='auto')\nclf.fit(X, y)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf)\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n            s=300, lw=1, facecolors='none')","e03edf8b":"fig, ax = plt.subplots(figsize=(16, 10))\nclf = SVC(kernel='poly', degree = 2, C=1E6, gamma='auto')\nclf.fit(X, y)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf)\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n            s=300, lw=1, facecolors='none')","813a28d6":"fig, axs = plt.subplots(figsize=(16, 50), nrows = 8)\n\nfor i in range(8):\n    clf = SVC(kernel='poly', degree = i+3, C=1E6, gamma='auto')\n    clf.fit(X, y)\n    axs[i].set_title(\"Degree: \" + str(i+3))\n    axs[i].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n    plot_svc_decision_function(clf, ax=axs[i])\n    axs[i].scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n                s=300, lw=1, facecolors='none')","e2aee262":"### Functional margin\n* For a **training example** $(x^{(i)}, y^{(i)})$, we define the **functional margin** of $(w,b)$ with respect to the **training example**\n$$\n\\large{\\hat{\\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b)}\n$$\n\n* For a given **training set** $S = \\{ (x^{(i)}, y^{(i)}); i = 1,...,m\\}$ function margin of $(w,b)$ with respect to $S$ is the smallest functional margins of the individual training examples.\n\n$$\n\\large{\\hat{\\gamma} = \\mathop{min}_{\\textbf{i=1,...,m}}\\hat{\\gamma}^{(i)}}\n$$\n\nOn problem why functional margin is not a great extimator of confidence of predictions is because we can scale it by any constant and increase to any arbitary number, but still the predictions will be the same ${g(w^Tx+b)\\ =\\ g(2w^Tx+2b)}$.\n\nSo, Intuitively, we want some kind of normalization condition, such that we don't have this problem.","7481f177":"### Mathematical Interpretation\nIn machine learning all we want to do is learn a function $f(x)$ which can give us correct predictions $y$ for a given input $X$.\n* The function $f(x)$ is known as **hypothesis function** which is mostly denoted by $h(x)$.\n![image](https:\/\/i.imgur.com\/L3SHSc6.png)\n\n* Let us take an example of a linear function $ h_\\theta(x) = \\theta{_0} + \\theta{_1}{x_1}  + \\theta{_2}{x_2} + \\theta{_3}{x_3} + \\theta{_n}{x_n}$. Where $n$ in the dimension for the feature vector $X$.\n* Here we want to learn the parameters $\\theta_i : for\\ i\\ in \\ \\{0,1,2,3,...,n\\}$","d4b2c807":"# Sources\n* http:\/\/cs229.stanford.edu\/notes\/cs229-notes3.pdf\n* https:\/\/www.csie.ntu.edu.tw\/~cjlin\/talks\/kuleuven_svm.pdf\n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.07-support-vector-machines.html\n* https:\/\/stats.stackexchange.com\/questions\/122631\/how-does-the-phix-i-function-look-for-gaussian-rbf-kernel\n* https:\/\/stats.stackexchange.com\/questions\/80398\/how-can-svm-find-an-infinite-feature-space-where-linear-separation-is-always-p\/168309#168309\n* https:\/\/math.stackexchange.com\/questions\/1301585\/why-is-the-constraint-w-1-non-convex\/1301592\n* https:\/\/datascience.stackexchange.com\/questions\/6054\/in-svm-algorithm-why-vector-w-is-orthogonal-to-the-separating-hyperplane\n","ac204a91":"Geometric margin $\\gamma^{(i)}$ is defined as the perpendicular distance between the point A and B in the above figure. Now the question is:\n\nHow to find $\\gamma^{(i)}$?\n\n$\\frac{w}{||w||}$ is a unit-length vector pointing in the same direction as $w$.\n\nSince point A represents $x^{(i)}$, B is given $x^{(i)}-\\gamma^{(i)}.(w\/||w||)$. As $w\/||w||$ is a unit vector perpendicular from B to A. So using the negative of this as a multiplier we go to the opposite direction and scaling it with $\\gamma^{(i)}$ makes us reach B. \n\nWe know that point B lies on the line(hyperplane), so it must satisfy the equation of the hyperplane.\n$$\nw^T\\Big(x^{(i)} - \\gamma^{(i)}\\frac{w}{||w||}\\Big) + b = 0\n$$\n\nSolving the above equation for $\\gamma^{(i)}$ gives us \n$$\n\\large{\\gamma^{(i)} = \\Big(\\big( \\frac{w}{||w||}  \\big)^T x^{(i)} + \\frac{b}{||w||} \\Big)}\n$$\n\nThe above calculation was done for the case of a single positive training example $x{(i)}$. In general case\n\n* **geometric margin** of $(w,b)$ with respect to **training example** $(x^{(i)}, y^{(i)})$ is defined as\n$$\n\\large{\\gamma^{(i)} = y^{(i)}\\Big(\\big( \\frac{w}{||w||}  \\big)^T x^{(i)} + \\frac{b}{||w||} \\Big)}\n$$\n\n* **geometric margin** of $(w,b)$ with respect to training set $S = \\{ (x^{(i)}, y^{(i)}); i = 1,...,m\\}$ is the smallest of the geometric margin for indivisual training examples\n$$\n\\large{\\gamma = \\mathop{min}_{\\textbf{i=1,...,m}}\\gamma^{(i)}}\n$$\n","c362d062":"# Support Vector Machines (SVM)\n### Contents\n* Introduction\n* Margins\n    * Functional Margin\n    * Geometric Margin\n* Optimal Margin Classifier\n* Kernels\n* ~~Non-separable case~~\n* ~~Regularization~~\n    * ~~L1 Regularization~~\n    * ~~L2 Regularization~~\n* ~~Sequential Minimal Optimization (SMO) algorithm~~","e5f1a9c0":"Let's get rid of the non-convex constraint $||w|| = 1$. And redefine an equivalent optimization problem as:\n$$\n\\begin{aligned}\n& max_{\\hat{\\gamma}, w, b}\n& & \\frac{\\hat{\\gamma}}{||w||} \\\\\n& \\text{subject to}\n& & y^{(i)}(w^Tx^{(i)} + b) \\geq \\hat{\\gamma}, i = 1,...,m \\\\\n\\end{aligned}\n$$\n\nHere, we're going to maximize $\\frac{\\hat{\\gamma}}{||w||}$, subject to **functional margins** all being at least $\\hat{\\gamma}$. Since, geometric margin and functional margin are related with $\\gamma = \\frac{\\hat{\\gamma}}{||w||}$, this gives us the answer we want. We are essentially normalizing the functional margin so that scaling won't affect it. The only problem we have with this is that $\\frac{\\hat{\\gamma}}{||w||}$ is non-convex.","d19cecd0":"The points with the smallest margins are esactly the ones closese toe the decision boundary; here, these are the three points (one negative and two positive examples) that lie on the dashed lines parallel to the decision boundary.\n\nThus only three of the $\\alpha_i$'s will be non-zero at the optimal solution to our optimization problem. These three points are called the **support vectors** in this problem.","aee48c95":"## Kernels\nLet's say instead of using just $x$ we use $x$, $x^2$ and $x^3$ as the features to obtain a cubic function. To distinguish between the two sets of variables, we'll call the original input value the input **attributes** of the problem. When that is mapped to some new set of quantities the input features.\n\nWe will also let $\\phi$ denote the **feature mapping**, which maps from the attributes to the features. For instance in our example above, we had\n$$\n\\phi(x) = \\begin{bmatrix}\n           x \\\\\n           x^{2} \\\\\n           x^3\n          \\end{bmatrix}\n$$\n\nRather than applying SVMs using the original input attributes x, we may insted want to learn using some feature $\\phi(x)$. To do so, we simply need to go over our previous algorithm, and replace $x$ everywhere in it with $\\phi(x)$.\n\nSince the algorithm can be written entirely in terms of the inner products $\\langle x,z\\rangle$. Specifically, given a feature mapping $\\phi$, we define the corresponding **Kernel** to be\n$$\nK(x,z) = \\phi(x)^T\\phi(z).\n$$\n\nThen, everywhere we previously had $\\langle x,z\\rangle$ in our algorithm, we could simply replace it with $K(x,z)$, and our algorithm would now be learning using the features $\\phi$.\n\nNow, given $\\phi$, we could easily compute $K(x,z)$ by finding $\\phi(x)$ and $\\phi(z)$ and taking their inner product. But what's more interesting is that often, $K(x,z)$ may be very inexpensive to calculate, even though $\\phi(x)$ itself may be very expensive to calculate. In such settings, by using in our algorithm an efficient way to calculate $K(x,z)$, we can get SVMs to learn in the high dimensional feature space given by $\\phi$, but without ever having to explicitly find or represent vectors $\\phi(x)$.\n\nLets see an example. Suppose $x$, $z$ $\\in R^n$, and consider\n$$\nK(x,z) = (x^Tz)^2.\n$$\n\nWe can also write this as \n$$\nK(x,z) = \\Bigg( \\sum^{n}_{i=1}x_iz_i \\Bigg) \\Bigg(\\sum^{n}_{j=1}x_jz_j\\Bigg)\\\\\n= \\sum^{n}_{i=1}\\sum^{n}_{j=1} x_ix_jz_iz_j \\\\\n= \\sum^{n}_{i,j=1}(x_ix_j)(z_iz_j)\n$$\n\nThus we see that $K(x,z) = \\phi(x)^T\\phi(z)$, where as mapping $\\phi$ is given(shown here for the case of n=3) by\n$$\n\\phi(x) = \\begin{bmatrix}\n           x_1 x_1 \\\\\n           x_1 x_2 \\\\\n           x_1 x_3 \\\\\n           x_2 x_1 \\\\\n           x_2 x_2 \\\\\n           x_2 x_3 \\\\\n           x_3 x_1 \\\\\n           x_3 x_2 \\\\\n           x_3 x_3\n          \\end{bmatrix}\n$$\n\nNote that whereas calculating the high dimensional $\\phi(x)$ requires $O(n^2)$ time, finding $K(x,z)$ takes only $O(n)$ time. Linear in the dimension of the input attributes.","c93aec8b":"## Margins\nAs described above in the geometric interpretation we want to draw a line between the data such that we can differentiate between the two type of data. But which line is a best line for this case. we can clearly see that we can draw many lines which can differentiate the above data correctly. But which one is the best one. To answer this question let's take an example of logistic regression where we predict output based on the probability.\nSo we will predict $1$ if we get $h_{\\theta}(x) \\geq 0.5$ , $0$ otherwise. The larger the $h_{\\theta}(x)$ we can say with more confidence that our prediction is going to be correct and vice-versa.\n\n\n","3bcc6a85":"# Using Langrange Duality\nConsider the **primal** problem of the following form:\n$$\n\\begin{aligned}\n& min_{w}\n& & f(w) \\\\\n& \\text{subject to}\n& & g_i(w) \\leq 0, i = 1,...,k \\\\\n& & & h_i(w) = 0, i = 1,...,l \\\\\n\\end{aligned}\n$$\n\nTo, solve this we start by defining the **generalized Lagrangian**\n$$\n\\mathcal{L}(w, \\alpha, \\beta) = f(w) + \\sum_{i=1}^{k}\\alpha_ig_i(w) + \\sum_{i=1}^{l}\\beta_ih_i(w).\n$$\n\nHere $\\alpha_i's$ and $\\beta_i$'s are called the **Lagrange multipliers**. We would then find and set $\\mathcal{L}$'s partial derivatives to zero\n\n$$\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 0 ; \\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i} = 0; \\frac{\\partial \\mathcal{L}}{\\partial \\beta_i} = 0, $$\n\nand solve for $w, \\alpha, \\beta$.\n\nConsider the quantity\n$$ \\theta_{\\mathcal{P}}(w) =  \\mathop{max}_{\\alpha,\\beta:\\alpha_i\\geq0} \\mathcal{L}(w, \\alpha, \\beta).$$\n\nHere $\\mathcal{P}$ subscript stands for **primal**.\n\nLet some w be given, If w violates any of the primal constraints ($g_i(w) > 0\\ or\\ h_i(w) \\neq 0\\ for\\ some\\ i$). Then $\\theta_{\\mathcal{P}}(w) = \\infty$.\n\nConversely, if the constrainta are indeed satisfied for a particular value of $w$, then $\\theta_{\\mathcal{P}} = f(w)$.\n\nHence,\n$$\n\\theta_{\\mathcal{P}}(w) = \n\\begin{cases}\n      f(w) &if\\ w\\ satisfies\\ primal\\ constraints,\\\\\n      \\infty & otherwise\\\\\n\\end{cases} \n$$\n\nThus, $theta_{\\mathcal{P}}$ takes the same value as the objective function in our problem for all values of w that satisfies the primal constraints, and is positive infinity if the constraints are violated. Hence, if we consider the minimization problem\n\n$$ \\mathop{min}_{w} \\theta_{\\mathcal{P}}(w) = \\mathop{min}_{w} \\mathop{max}_{\\alpha,\\beta:\\alpha_i\\geq0} \\mathcal{L}(w, \\alpha, \\beta).$$\n\nWe see that it is the same problem (i.e. has the same solutions as) original, primal problem.\n\nWe also define the **value of primal problem** , \n$$\np^* = \\mathop{min}_{w} \\theta_{\\mathcal{P}}(w) \n$$\n\nNow, let's look at a different problem, We define\n$$\n\\theta_{\\mathop{D}}(\\alpha, \\beta) = \\mathop{min}_{w} \\mathcal{L}(w, \\alpha, \\beta).\n$$\n\nHere $\\mathop{D}$ subscript stands for **dual**. Note also that whereas in the definition of $\\theta_{\\mathcal{P}}$ we were optimining with respect to $\\alpha$, $\\beta$, here we are minimizing with respect to $w$.\n\nWe can now propose the **dual** optimization problem.\n$$ \\mathop{max}_{\\alpha,\\beta:\\alpha_i\\geq0} \\theta_{\\mathcal{D}}(\\alpha, \\beta) = \\mathop{max}_{\\alpha,\\beta:\\alpha_i\\geq0} \\mathop{min}_{w} \\mathcal{L}(w, \\alpha, \\beta).\n$$\n\nThis is exactly the same as our primal problem shown above, except that the order of the **max** and **min** are now exchanged.\n\nWe also define\n$$d^* = \\mathop{max}_{\\alpha,\\beta:\\alpha_i\\geq0} \\theta_{\\mathcal{D}}(\\alpha, \\beta)$$\nas the optimal **value** of the dual problem objective.\n\nIt can easily be shown that\n$$\nd^* \\leq p^*\n$$\nAs \"max min\" of a function is always less than or equal to the \"min max\".\n\nHowever, under some conditions $d^* = p^*$, Lets see what these conditions are.\n\n* Suppose $f$ and the $g_i$'s are convex, and the $h_i$'s are affine.\n* Suppose that the constraints $g_i$ are (strictly) feasible; this means that there exists some $w$ so that $g_i(w) < 0 \\text{ for all i}$.\n\nUnder the above assumptions, there must exist $w^*$, $\\alpha^*$, $\\beta^*$ so that $w^*$ is the solution to the primal problem, $\\alpha^*$, $\\beta^*$ are the solution to the dual problem, and moreover $p^*$ = $d^*$ = $\\mathcal{L}(w^*, \\alpha^*, \\beta^*)$.\n\nMoreover, $w^*$, $\\alpha^*$, $\\beta^*$ satisfy the **Karush-Kuhn-Tucker (KKT) conditions**, which are as follows:\n\n* Condition 1:\n$$\n\\frac{\\partial}{\\partial w_i}\\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0,\\ \\  i = 1,...,n\n$$\n\n* Condition 2:\n$$\n\\frac{\\partial}{\\partial \\beta_i}\\mathcal{L}(w^*, \\alpha^*, \\beta^*) = 0,\\ \\  i = 1,...,l\n$$\n\n* **Condition 3**:\n$$\n\\large{\\alpha_{i}^{*}g_i(w^*) = 0, i = 1,...,k}\n$$\n\n* Condition 4:\n$$\ng_i(w^*) \\leq 0, i = 1,...,k\n$$\n\n* Condition 5:\n$$\n\\alpha^* \\geq 0, i = 1,...,k\n$$\n\nMoreover, if some $w^*$, $\\alpha^*$, $\\beta^*$ satisfy the KKT conditions, then it is also a solution to the primal and dual problems.\n\nThe third condition is called the KKT **dual complementarity** condition. Specifically, it implies that if $\\alpha_i^* > 0$, then $g_i(w^*) = 0$. (i.e. the \"$g_i(w) \\leq 0$\" constraint is active, meaning it holds with equality rathere than with inequality.)\n","316d3b13":"### Gaussian kernel\nLet's talk about a slightly different view of kernels. Intutively, if $\\phi(x)$ and $\\phi(z)$ are close together, we might expect $K(x,z)$ to be large. Conversely, if $\\phi(x)$ and $\\phi(z)$ are far apart say nearly orthogonal to each other then $K(x,z) = \\phi(x)^T\\phi(x)$.\n\nSo we can think of $K(x,z)$ as some measurement of how similar $\\phi(x)$ and $\\phi(z)$ are, or of how similar are $x$ and $z$.\n\nGiven this intution, suppose that for some learning problem that you're working on, you've come up with some function $K(x,z)$ that you might think might be a resonable measure of how similar $x$ and $z$ are. For instance, you chose\n$$\nK(x,z) = exp\\big(-\\frac{||x-z||^2}{2\\sigma^2}\\big)\n$$\nThis is a reasonable measure of x and z's similarity, and is close to 1 when x and z are close, and near 0 when x and z are far apart.\nThis kernel corresponds to an **infinite dimensional feature mapping** $\\phi$. If x is 1-D we have,\n$$\n\\phi_{RBF}(x) = e^{-\\gamma x^2}\\big[1,\\sqrt{\\frac{2\\gamma}{1!}}x, \\sqrt{\\frac{(2\\gamma)^2}{2!}}x^2, \\sqrt{\\frac{(2\\gamma)^3}{3!}}x^3,\\ldots\\big]^T,\n$$\nwhere, $\\gamma = \\frac{1}{2\\sigma^2}$\nThis is known as **Radial Basis Function (RBF)**.\n\nLet's see what we can do with it:","e19bf36a":"## Introduction\n\nBefore starting with SVM, Let me talk about what we want to achieve from Machine Learning algorithms in general.\n\n### Geometric Interpretation\nBelow in the plot we see that from machine learning we need get a **line** inbetween the data which can differentiate between two categories in a good manner. For 3-D features it will be a **plane** and for higher dimensions it will be a **hyper-plane**.\nFor now think of these two categories of data as boxes and triangles. It doesn't matter what the data is about.","6b6da947":"Let's construct the Lagrangian for our optimization problem:\n$$\n\\mathcal{L}(w, b, \\alpha) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^{m}\\alpha_i [y^{(i)}(w^Tx^{(i)} + b) - 1]\n$$\n\nWe don't have $\\beta_i$'s as we don't have any equality constraints.\n\nLets find the dual form of the problem. To do so, we need to first minimize $\\mathcal{L}(w, b, \\alpha)$ with respect to $w$ and $b$(for fixed $\\alpha$), to get $\\theta_\\mathcal{D}$, which we'll do by setting the derivatives of $\\mathcal{L}$ with respect to $w$ and $b$ to zero. We have:\n$$\n\\nabla_w \\mathcal{L}(w, b, \\alpha) = w - \\sum_{i=1}^{m}\\alpha_iy^{(i)}x^{(i)} = 0\n$$\nThis implies that\n$$\nw = \\sum_{i=1}^{m}\\alpha_iy^{(i)}x^{(i)} \\hspace{50pt}(1)\n$$\nAs for the derivative with respect to b, we obtain\n$$\n\\frac{\\partial}{\\partial b_i} \\mathcal{L}(w, b, \\alpha) = \\sum_{i=1}^{m}\\alpha_iy^{(i)} = 0 \\hspace{50pt}(2)\n$$\nIf we take the definition of $w$ from equation (1) and plug that back into the Lagrangian, and simplify, we get\n$$\n\\mathcal{L}(w, b, \\alpha) = \\sum_{i=1}^{m}\\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m}y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)}, y^{(j)}\\rangle.\n$$\n\n\n$$\n\\begin{aligned}\n& max_{\\alpha}\n& & W(\\alpha) =  \\mathcal{L}(w, b, \\alpha) = \\sum_{i=1}^{m}\\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m}y^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)}, y^{(j)}\\rangle\\\\\n& \\text{subject to}\n& & \\alpha_i \\geq 0, i = 1,...,m \\\\\n& & & \\sum_{i=1}^{m}\\alpha_iy^{(i)} = 0 \\\\\n\\end{aligned}\n$$\n\nUsing some maths we can prove that conditions required for $p^* = d^*$ and the KKT conditions to hold are indeed satisfied in our optimization problem. Hence, we can **solve the dual** in lieu of solving the primal problem.\n\nSpecifically, in the dual problem above, we have a maximization problem in which the parameters are the $\\alpha_i$'s.\n","474e1ece":"# Optimal margin classifier\nAfter understanding the equations in the section before, we can easily say all we want from our decision boundary is to **maximize** the **geometric margin**.\n\nThis would mean we will have a classifier which seperates the positive and negative training examples with a \"gap\".\n\nSo, how do we propose that as a optimization problem:\n$$\n\\begin{aligned}\n& max_{\\gamma, w, b}\n& & \\gamma \\\\\n& \\text{subject to}\n& & y^{(i)}(w^Tx^{(i)} + b) \\geq \\gamma, i = 1,...,m \\\\\n& & & ||w|| = 1\n\\end{aligned}\n$$\n\nWhat that means is that I want to maximize $\\gamma$, subject to each training example having **functional margin** at least $\\gamma$. And the constraint $||w|| = 1$ ensures that the **functional margin** equals to the **geometric margin**. So, we are also guaranteed that all the **geometric margins** are at least $\\gamma$.\n\nThe problem with the above optimization equation is that the constraint $||w|| = 1$ is non-convex set. To see this let's see what this will give us in two dimensions. The constraint represents a set of points on the boundary of a circle (in n dimensions it will be a n-ball). ","45bf1491":"Let us suppose that we use some algorithm (SMO) to get optimal values of $\\alpha$'s i.e. find the $\\alpha$'s that maximize $W(\\alpha)$ subject to the constraints), the we can use Equation (1) to go back and find the optimal value of $w$'s as a function of the $\\alpha$'s. Having found $w^*$, by considering the primal problem, it is also straightforward to find the optimal value for the intercept term b as\n$$\nb^* = -\\frac{max_{i:y^{(i)}=-1}w^{*T}x^{(i)} + min_{i:y^{(i)}=1}w^{*T}x^{(i)}}{2}\n$$\n\nSuppose we've fit our model's parameters to a training set, and now wish to make a prediction at a new point input x. We would then calculate $w^Tx + b$, and predict $y = 1$ iff this quantity is bigger than zero. But using equation (1) this quantity can also be written as:\n$$\nw^Tx + b = \\Bigg(\\sum^{m}_{i=1} \\alpha_iy^{(i)}x^{(i)}\\Bigg)^T x + b \\\\\n\\large= \\sum^{m}_{i=1} \\alpha_iy^{(i)}\\langle x^{(i)}, x\\rangle + b\n$$\n\nWe know that **$\\alpha_i$'s are non-zero only for the support vectors** and we were also able to write the entire algorithm in terms of only inner products between input feature vectors.","194dc5c3":"# Geometric Margin\n","69313c8e":"### Theorem (Mercer):\nLet $K: R^n X R^n \\rightarrow R$ be given. Then $K$ to be a valid(Mercer) kernel, it is neccessary and sufficient that for any $\\{x^{(i)}, ..., x^{(m)}\\}, (m < \\infty)$, the corresponding kernel matrix is **symmetric positive semi-definite**.","6e3d147d":"### Notation\n* We will be considering a binary classification problem where $y \\in \\{-1,1\\}$.\n* Rather than using $\\theta$ we will be using $w$ and $b$ in its place. where $b$ is just $\\theta_0$ and the rest is represented by $w$.\n$$\nh_{w,b}(x) = g(w^Tx + b)\n$$\n\n$$g(z) = \\big\\{ 1\\ if\\ z \\geq\\ 0,\\ else\\ -1\\big\\}$$","dd12909a":"# Optimal margin classifiers\nWe previously, proposed the following (primal) optimization problem for finding the optimal margin classifier:\n$$\n\\large{\\begin{aligned}\n& min_{\\gamma, w, b}\n& & \\frac{1}{2}{||w||^2} \\\\\n& \\text{subject to}\n& & y^{(i)}(w^Tx^{(i)} + b) \\geq 1, i = 1,...,m \\\\\n\\end{aligned}}\n$$\n\nWe can write the constraints as\n$$\ng_i(w) = -y^{(i)}(w^Tx^{(i)} + b) + 1 \\leq 0, i = 1,...,m\n$$\n\nWe have one such constraint for each training example. Note that from the KKT dual complementarity condition, we will have $\\alpha_i > 0$ only for the training examples that have functional margin exactly equal to one (i.e. the ones corresponding to constraints that hold with equality, g_i(w) = 0). Consider the figure below, in which a maximum margin separating hyperplane is shown by the solid line.\n","2a9d2f99":"We know that we can scale **functional margin** with any scaling constraint $w$ and $b$ without changing anything. Now, we will introduce a scaling constraint $\\hat{\\gamma} = 1$. i.e. the functional margin of $w,b$ with respect to the training set must be $1$.\n\nThis is a scaling constraint and can be satisfied by rescaling $w,b$. \n\nNotice, that if $\\hat{\\gamma} = 1$ we will change the above objective function to $1\/||w||$. This is same as minimizing $||w||^2$.\n\nSo, we get a convex optimization problem:\n$$\n\\large{\\begin{aligned}\n& min_{\\gamma, w, b}\n& & \\frac{1}{2}{||w||^2} \\\\\n& \\text{subject to}\n& & y^{(i)}(w^Tx^{(i)} + b) \\geq 1, i = 1,...,m \\\\\n\\end{aligned}}\n$$\n\nThis optimization problem can be solved using any commercial quadratic programming (QP) code."}}