{"cell_type":{"1d86b9c9":"code","1d958d47":"code","152dfb30":"code","2e5b17da":"code","fd29afcc":"code","a8d8b2d9":"code","4caa2d40":"code","fa61455f":"code","5a864301":"code","36404c3b":"code","cefce2d2":"code","096bc1a6":"code","d602d1f3":"code","6b69ea66":"code","c7667118":"code","42d3a1c8":"code","0f193306":"code","4b56701e":"code","e6d9ab98":"code","25b3ce0f":"code","6b68445d":"code","ae5aedde":"code","2d0c020f":"code","83ae0c78":"code","1b9ea841":"code","aa64e6c5":"code","91b5d3e6":"code","11d1eab7":"code","3580493e":"code","0be9fea4":"code","19793e39":"code","b3b56de9":"code","40a5b9d0":"code","47e03de9":"code","3da1eb49":"code","c500f0c8":"code","90cf5a5b":"markdown","7d13fec7":"markdown","27eca7a2":"markdown","f80e6517":"markdown","2f10e06a":"markdown","5a0a09ef":"markdown","408d4513":"markdown","5e8c9962":"markdown","d72e1f75":"markdown","efa8f986":"markdown","9d515b84":"markdown","2c6131dd":"markdown","aadb4eaa":"markdown","c8878b94":"markdown","c9e03eba":"markdown","2bf25727":"markdown","5b24c685":"markdown","56a4dcb8":"markdown","c6993ad8":"markdown","c9e67a5d":"markdown","a846d7df":"markdown","7519c650":"markdown","c42df5ba":"markdown","57058275":"markdown","3c5f7fcc":"markdown","02126c89":"markdown","b6ce3ba9":"markdown","d4a6c89e":"markdown","3918f679":"markdown","38f51352":"markdown","2067b976":"markdown"},"source":{"1d86b9c9":"# standard library\nimport collections\nimport os\nimport random\nimport re\nimport string\nimport tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# third-party library\nimport matplotlib.pyplot as plt\nimport nltk.corpus # stopwords\nimport numpy as np \nimport pandas as pd \nfrom PIL import Image\nfrom plotly import graph_objs, express, figure_factory  # go. pe, ff\nimport seaborn as sns\nimport spacy.util # compounding, minibatch\nimport tensorflow as tf\nimport tokenizers\nimport transformers\nfrom sklearn.model_selection import StratifiedKFold\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n%matplotlib inline","1d958d47":"print('TF version', tf.__version__)","152dfb30":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","2e5b17da":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nss = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","fd29afcc":"# row \uc218, col \uc218 \ub77c\uace0 \uc0dd\uac01\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.\nprint(train.shape)\nprint(test.shape)","a8d8b2d9":"train.info()","4caa2d40":"train.dropna(inplace=True)","fa61455f":"test.info()","5a864301":"train.head()","36404c3b":"len(train.apply(lambda x: x.selected_text in x.text, axis=1))","cefce2d2":"train.describe()","096bc1a6":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","d602d1f3":"plt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=train)","6b69ea66":"fig = graph_objs.Figure(graph_objs.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","c7667118":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","42d3a1c8":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","0f193306":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain = train.merge(jaccard,how='outer')","4b56701e":"train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","e6d9ab98":"train.head()","25b3ce0f":"train.describe()","6b68445d":"hist_data = [train['Num_words_ST'],train['Num_word_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig, axes = plt.subplots(figsize=(20,10))\nsns.countplot(train['Num_words_ST'], ax=axes, color='blue', alpha=0.3, label='selected_text')\nsns.countplot(train['Num_word_text'], ax=axes, color='red', alpha=0.3, label='text')\naxes.legend()\nfig.show()","ae5aedde":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\", label='positive').set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\",label='negative')","2d0c020f":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)","83ae0c78":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\", label='positive').set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\", label='negative')\nplt.legend(labels=['positive','negative'])","1b9ea841":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","aa64e6c5":"# \uc800\uc790\uc640 \uc880 \ub2e4\ub974\uac8c unique sentiment \uc5d0 \ub300\ud55c \ud45c\uae30\ub97c \ub2e8\uc21c\ud55c 0,1,2 \ub85c \ud574\ubcf4\uace0\uc790 \ud569\ub2c8\ub2e4.\nunique_sentiment = train.sentiment.unique()\nprint(unique_sentiment)\nsentiment_id = collections.defaultdict(int)\nfor idx, sentiment in enumerate(unique_sentiment):\n    sentiment_id[sentiment] = idx","91b5d3e6":"shape0 = train.shape[0]\ninput_ids = np.ones((shape0,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((shape0,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((shape0,MAX_LEN),dtype='int32')\nsentiment = np.zeros((shape0,3), dtype='int32') \n\nfor k in range(train.shape[0]):\n    # text2 \uc758 \uc704\uce58\ub97c \ucc3e\uc544\uc11c idx \ub85c \uc815\uc758\ud558\uace0, text2 \uc704\uce58\uc5d0 1\uc774\ub77c\uace0 \ud45c\uc2dc\ud574\uc8fc\ub294 chars \ubc30\uc5f4\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1] == ' ': \n        chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # text1 \uc5d0 \ub300\ud574\uc11c \ud2b9\uc815 \ub2e8\uc5b4\uc758 \uae38\uc774\ub97c \ud45c\uae30\ud574\uc8fc\ub294 offsets \ubc30\uc5f4\uc744 \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4. (\ub2e8\uc5b4\uc758 \uc2dc\uc791, \ub2e8\uc5b4\uc758 \ub05d)\n    offsets = []\n    idx = 0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # enc.ids \ub97c \ud1b5\ud574\uc11c \uadf8\ub9bc\uc758 input_ids \ub97c \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4.\n    # attention_mask \ub294 input_ids \uc758 \uae38\uc774\ub9cc\ud07c\uc774\uae30 \ub54c\ubb38\uc5d0 \ud574\ub2f9 \ubd80\ubd84\ub4e4\uc744 1\ub85c \ucc44\uc6cc\uc90d\ub2c8\ub2e4.\n    input_ids[k,:len(enc.ids)+2] = [0] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+2] = 1\n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    sentiment[k,s_tok] = 1","11d1eab7":"target = 1042\nprint(input_ids[target,])\nprint(attention_mask[target,])\nprint(sentiment[target])","3580493e":"def build_roberta():\n    # input \ub4e4\uc744 \ubc1b\uc744 \uc218 \uc788\ub3c4\ub85d \ub9cc\ub4e7\ub2c8\ub2e4.\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    # \uae30\ubcf8\uc801\uc73c\ub85c transformers \uc548\uc5d0 roberta \uc5d0 \ub300\ud55c \uc124\uc815 \ubc0f \ubaa8\ub4c8\ub4e4\uc774 \ub4e4\uc5b4\uc788\uc2b5\ub2c8\ub2e4.\n    # \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 [huggingface](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html) \ub97c \ucc38\uace0\ud558\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n    config = transformers.RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model=transformers.TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5', config=config)\n    \n    x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n    \n    # x[0] \ub294 bert_model\uc758 bert \uc758 \uccab\ubc88\uc9f8 \uc544\uc6c3\ud48b\uc73c\ub85c (batch_size, MAX_LEN, 768) \ud06c\uae30\uc758 tensor \uc785\ub2c8\ub2e4.\n    # \uc6b0\ub9ac\ub294 \uac01 input token \uc5d0 \ub300\ud574\uc11c \ud574\ub2f9 token \uc774 start \uc778\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ud45c\ud604\ud558\ub294 \uac12\ub4e4\uc744 softmax \ud558\uc5ec \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n    x = tf.keras.layers.Dropout(0.1)(x[0])\n    x = tf.keras.layers.Conv1D(1,1)(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(3)(x)\n    x = tf.keras.layers.Activation('softmax')(x)\n\n    # x = sentiment\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n\n    return model","0be9fea4":"model = build_roberta()\nmodel.summary()","19793e39":"VER='v0'\nDISPLAY=1\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nbest_model = 0\nhistory = []\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    tf.keras.backend.clear_session()\n    model = build_roberta()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '\/kaggle\/working\/%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    history.append(model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [sentiment[idxT,]], \n        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [sentiment[idxV,]])))\n    \n    print('Loading model...')\n    model.load_weights('\/kaggle\/working\/%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting validation...')\n    sentiment[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    break","b3b56de9":"import matplotlib.pyplot as plt\n\nplt.plot(history[0].history['loss'])\nplt.plot(history[0].history['val_loss'])\nplt.title('Model Loss')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","40a5b9d0":"test0 = test.shape[0]\ninput_ids_t = np.ones((test0,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((test0,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((test0,MAX_LEN),dtype='int32')\n\nfor k in range(test0):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","47e03de9":"import glob\nsaved_model = glob.glob('\/kaggle\/working\/*.h5')\n\nmodel = build_roberta()\nmodel.load_weights(saved_model[0])\n\npreds = np.zeros((input_ids_t.shape[0],3))\n\nprint('Predicting Test...')\npreds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)","3da1eb49":"category_pred = [unique_sentiment[np.argmax(pred)] for pred in preds]","c500f0c8":"test['predicted_sentiment'] = category_pred\ntest[['textID','predicted_sentiment']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","90cf5a5b":"# Train roBERTa Model\n\n\uc6b0\ub9ac\ub294 [StratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html) \ud568\uc218\ub97c \ud1b5\ud574\uc11c input_ids \ub97c \ucd1d 5\uac1c\ub85c \ub098\ub20c\uac83\uc785\ub2c8\ub2e4. StratifiedKFold \ub294 \uad50\ucc28\uac80\uc99d(cross-validation) \uc744 \uc774\uc6a9\ud55c \ud559\uc2b5\ubc95\uc785\ub2c8\ub2e4. \uad50\ucc28 \uc720\ud6a8\uc131 \uac80\uc0ac \uc804\ub7b5\uc5d0 \ub530\ub77c \ub370\uc774\ud130 \uc9d1\ud569\uc744 \uc0dd\uc131\ud558\ub294\ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uc778\ub371\uc2a4\ub97c \uc0dd\uc131\ud558\ub294 \ub3c4\uad6c\uc785\ub2c8\ub2e4. \uac01\uac01\uc758 fold \uc5d0\uc11c \ucd5c\uace0\uc758 \uc131\ub2a5\uc744 \ub0b4\ub294 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc11c validation prediction \uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4.\n\n![image.png](attachment:image.png)\n\n[\ucc38\uace0](https:\/\/swlock.blogspot.com\/2019\/01\/scikit-learn-cross-validation-iterators.html?m=1)","7d13fec7":"# Build roBERTa Model\n\n\uc704\uc5d0\uc11c \ubaa8\ub378\uc774 \uc0ac\uc6a9\ud560 \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uc5c8\uc73c\ub2c8, \uc774\uc81c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\uc11c \ud559\uc2b5\uc744 \uc2dc\ucf1c\ubd05\ub2c8\ub2e4.\nconfig \ud30c\uc77c\uc744 \uc774\uc6a9\ud574\uc11c config \ub97c \ub85c\ub529\ud558\uace0, config \ub97c \uc0ac\uc6a9\ud574\uc11c reberta \ubaa8\ub378\uc744 \ub85c\ub529\ud569\ub2c8\ub2e4.\nbert_model \uc774 \ubf51\uc544\uc8fc\ub294 output \uc740 x \uc778\ub370, x \ub294 \ubc30\uc5f4 \ud615\ud0dc\ub85c \ub4e4\uc5b4\uc635\ub2c8\ub2e4. \n\n--- \n\uc544\ub798 \ubd80\ubd84\uc740 \ud655\uc778\uc774 \ud544\uc694\ud569\ub2c8\ub2e4!\n\n\uadf8 \uc911 \uccab\ubc88\uc9f8 \uc544\uc6c3\ud48b\uc740 (MAX_LEN, 768) \ud06c\uae30\uc758 \uc544\uc6c3\ud48b\uc778\ub370, \uc774\uac83\uc744 softmax \ud568\uc218 \ud615\ud0dc\ub85c (MAX_LEN,) \ud615\ud0dc\ub85c \ubf51\uc544\uc90d\ub2c8\ub2e4. \nReturn \uc740 [transformers.BertModel](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertModel) \uc744 \ucc38\uace0\ud574\ubcf4\uba74 \ub429\ub2c8\ub2e4.\n> A BaseModelOutputWithPoolingAndCrossAttentions (if return_dict=True is passed or when config.return_dict=True) or a tuple of torch.FloatTensor comprising various elements depending on the configuration (BertConfig) and inputs.\n\n> last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) \u2013 Sequence of hidden-states at the output of the last layer of the model.\n\nMAX_LEN \uc5d0 \ub300\ud574\uc11c softmax \ub97c \ud558\uae30 \ub54c\ubb38\uc5d0 x=sentiment \uc5d0 \ub300\ud55c \ud655\ub960 \ud615\ud0dc\ub85c \ub098\ud0c0\ub098\uac8c \ub429\ub2c8\ub2e4. \n\uc774 sentiment \uc5d0 \ub300\ud574\uc11c crossentropy\ub97c \uad6c\ud558\uae30 \uc704\ud574\uc11c [categorical_crossentropy](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/categorical_crossentropy) \ub97c loss \ub85c \uc124\uc815\ud574\uc90d\ub2c8\ub2e4.","27eca7a2":"# \ubaa9\uc801\n\n\uc774 \ub178\ud2b8\ubd81\uc744 \ud1b5\ud574\uc11c \uc800\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ubaa9\uc801\uc744 \uc774\ub8e8\uace0\uc790 \ud569\ub2c8\ub2e4.\n- EDA \ub97c \ud1b5\ud55c \ub370\uc774\ud130\uc14b \uc774\ud574\ud558\uae30.\n  - \ub370\uc774\ud130 \uc14b \ubd84\uc11d\uc744 \uc704\ud55c \uc2dc\uac01\ud654. (matplotlib, seaborn, plotly, word cloud)\n  - \ub370\uc774\ud130\uc14b\uc758 \ubd84\ud3ec \ubd84\uc11d.\n- Tensorflow \ubaa8\ub378\uc778 roBERTa \ub97c \ud1b5\ud574\uc11c \ud559\uc2b5\uc744 \uc2dc\ucf1c\ubcf4\uae30.","f80e6517":"Funnel-Chart \ub85c \ubcf4\uba74 \ub354 \ud655\uc2e4\ud558\uac8c \ubcf4\uc785\ub2c8\ub2e4.","2f10e06a":"# See train history\n","5a0a09ef":"## Meta-Feature \ub9cc\ub4e4\uae30","408d4513":"Sentiment \ubcc4\ub85c \ub370\uc774\ud130\ub4e4\uc744 \uc0b4\ud3b4\ubd05\uc2dc\ub2e4.","5e8c9962":"\uc774 \ub178\ud2b8\ubd81\uc740 2\uac00\uc9c0 \ub178\ud2b8\ubd81\uc744 \uc885\ud569\ud574\uc11c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4.\n- EDA \ubc0f \uc804\uccb4\uc801\uc778 \uad6c\uc870: [Twitter sentiment Extaction-Analysis,EDA and Model](https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model) \n  - \ud574\ub2f9 dataset \uc5d0\uc11c Most vote \ub97c \ubc1b\uc740 notebook \uc785\ub2c8\ub2e4.\n- \ubaa8\ub378 \uc0ac\uc6a9(Tensorflow): [TensorFlow roBERTa - [0.705]](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705)\n  - \uc800\ub294 Tensorflow \uc5d0 \uc775\uc219\ud55c\ub370, \ud2b9\ud788 \uc810\uc218\uac00 \uad1c\ucc2e\uc544\ubcf4\uc774\uace0, \ud2b9\uc815 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud55c \uac83\uc73c\ub85c \ubcf4\uc5ec \uc774 notebook \uc744 \uc120\ud0dd\ud588\uc2b5\ub2c8\ub2e4.\n\nData \ub85c\ub294 \ucd1d 3\uac00\uc9c0\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc624\ub978\ucabd\uc758 + Add data \uc5d0 \ub2e4\uc74c\uacfc \uac19\uc740 3\uac1c\ub97c \ub4f1\ub85d\ud558\uc138\uc694.\n* tweet-sentiment-extraction\n* tf-roberta\n* tse-spacy-model\n\n\ucd94\uac00\ud558\uac70\ub098 \uc5c5\ub370\uc774\ud2b8\ub41c \ub0b4\uc6a9\uc740 \ub530\ub85c \ud45c\uc2dc\ud560 \uc218 \uc788\ub3c4\ub85d \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","d72e1f75":"\uc774 \ub178\ud2b8\ubd81\uacfc\ub294 \ubcc4\uac1c\uc758 \uc791\uc5c5\uc774 \ub420 \uc218 \uc788\uaca0\uc9c0\ub9cc, \ub2e4\uc74c\uc73c\ub85c \uc815\ub9ac\ud560 \uc77c\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n* TPU \ub85c \ud559\uc2b5\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\uc11c \uc368\ubcfc\uae4c \ud569\ub2c8\ub2e4.\n* GPU \ub85c \ud559\uc2b5\ud55c \ubaa8\ub378\uc744 \uc800\uc7a5\ud574\ubcfc\uae4c \ud569\ub2c8\ub2e4. \ub2e4\uc74c\uc5d0 \ub2e4\uc2dc \uc4f8 \uc218 \uc788\ub3c4\ub85d\uc694.\n\n\uacc4\uc18d \ucd94\uac00\ub420 \uc608\uc815\uc785\ub2c8\ub2e4. TO BE CONTINUED.\n","efa8f986":"\ub450 \uac00\uc9c0 \uc7ac\ubbf8\uc788\ub294 \uac83\ub4e4\uc744 \ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n* positive, negative tweets \ub4e4\uc758 jaccard \uc810\uc218\ub294 \ub192\uc740 \ucca8\ub3c4(kurtosis) \ub97c \ubcf4\uc774\uace0 \uc788\uc73c\uba70, \ub450 \uacf3\uc5d0 \uc881\uace0 \uc870\ubc00\ud558\uac8c \ubd84\ud3ec\ud574\uc788\uc2b5\ub2c8\ub2e4.\n* Neutral tweets\ub294 \ucca8\ub3c4\uac00 \ub0ae\uc73c\uba70, 1\uc5d0 \uac70\uc758 \ubaa8\ub4e0 \uac12\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4.","9d515b84":"Meta-Features \uc758 distribution \uc744 \uc0b4\ud3b4\ubd05\uc2dc\ub2e4.","2c6131dd":"# \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub4e4\uc744 import \ud569\ub2c8\ub2e4.\n\n[PEP8](https:\/\/www.python.org\/dev\/peps\/pep-0008\/) \ucc38\uace0\n* import \ub294 \ud55c \uc904\uc529 \uc801\uc5b4\uc57c \ud55c\ub2e4.\n* \ud30c\uc77c\uc758 \ub9e8 \uc704\uc5d0 import \ub97c \uc801\uc5b4\uc57c \ud55c\ub2e4.\n* python \ud328\ud0a4\uc9c0\ub97c import \ud560\ub54c\ub294 \ucd1d 3\uac00\uc9c0\ub85c \uadf8\ub8f9\ud551\ud574\uc57c \ud55c\ub2e4.\n  * python \uc758 standard library\n  * third_party library\n  * \ub85c\uceec(Local application\/library) \uc5d0\uc11c \uac00\uc838\uc62c \ud328\ud0a4\uc9c0\ub4e4\n* wild import \ub294 \uc4f0\uc9c0 \ub9d0\uc544\uc57c \ud55c\ub2e4.\n\n\uc774\uac74 \ud604\uc5c5\uc5d0\uc11c\ub9cc \ud574\ub2f9\ud560 \uc218 \uc788\uc9c0\ub9cc, \ubcf4\ud1b5\uc740 \uac01 \uadf8\ub8f9\ubcc4\ub85c a->z \ub97c \uc2dc\ucf1c\uc11c \uc501\ub2c8\ub2e4.","aadb4eaa":"## \ub370\uc774\ud130\ub97c \ubcf4\uace0 \uc54c\uac8c\ub41c \uac83\ub4e4\n\n* \uc6b0\ub9ac\ub294 selected_text \uac00 text \uc758 subset \uc774\ub77c\ub294 \uac83\uc744 \ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n* \uc6b0\ub9ac\ub294 selected_text \uac00 \uc5f0\uc18d\ub41c \ub2e8\uc5b4\ub85c\ubd80\ud130 \uc0dd\uc131\ub418\uc5c8\uc74c\uc744 \uc54c\uc558\uc2b5\ub2c8\ub2e4. \uc5ec\ub7ec \ubb38\uc7a5\uc5d0\uc11c \uc77c\ubd80\ubd84\uc529 \ucd94\ucd9c\ub418\uc9c0\ub294 \uc54a\uc74c\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/138520 \ub97c \ubcf4\uba74 neutral tweet \uc740 selected_text \uc640 text \uac04\uc758 jaccard similarity \uac00 97% \ub77c\uace0 \ud569\ub2c8\ub2e4.\n* https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/138272 \ub97c \ubcf4\uba74 selected_text \uac00 \ub2e8\uc5b4 \uc0ac\uc774\uc5d0\uc11c \uc2dc\uc791\ud558\ub294 \uacbd\uc6b0\uac00 \uc885\uc885 \uc788\uc5b4\uc11c \ud56d\uc0c1 \ub9d0\uc774 \ub418\ub294\uac74 \uc544\ub2c8\uba70, \ud14c\uc2a4\ud2b8 \uc14b\uc5d0\uc11c\ub3c4 \uc774\uac83\uc774 \uc720\ud6a8\ud55c\uc9c0\ub294 \uc798 \ubaa8\ub974\uae30 \ub54c\ubb38\uc5d0 \ubb38\uc7a5\uc744 preprocessing \ud558\uac70\ub098 punctuation \uc744 \uc5c6\uc560\ub294\uac8c \uc88b\uc740 \uc194\ub8e8\uc158\uc778\uc9c0 \uc54c \uc218 \uc5c6\ub2e4\uace0 \ud569\ub2c8\ub2e4.","c8878b94":"\ub2e4\uc74c\uacfc \uac19\uc740 \ub450 \uac00\uc9c0 \uc815\ubcf4\ub97c \uc0ac\uc6a9\ud558\uba74 \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \ub0bc \uc218 \uc788\uc744 \uac70\ub77c\uace0 \ud569\ub2c8\ub2e4.\n* text \uc640 selected_text \uc0ac\uc774\uc758 \ub2e8\uc5b4 \uc22b\uc790 \ucc28\uc774\n* text \uc640 selected_text \uc0ac\uc774\uc758 jaccard similarity score.\n\n[jaccard similiarity](https:\/\/www.geeksforgeeks.org\/find-the-jaccard-index-and-jaccard-distance-between-the-two-given-sets\/)\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uacc4\uc0b0\ub420 \uc218 \uc788\ub2e4\uace0 \ud569\ub2c8\ub2e4.\n\n![image.png](attachment:image.png)","c9e03eba":"\uc544\ub798 \ub0b4\uc6a9\uc744 \uc774\ud574\ud558\uae30 \uc27d\ub3c4\ub85d \uacfc\uc815\uc744 \ud45c\ud604\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n```\n  Text  = \"Kaggle is a fun webplace!\", Selected_text=\"fun webplace!\", Sentiment = positive\n```\n\ntext1, text2 \ub97c split \ud558\uace0 \" \" \ub85c \ubd99\uc5ec\uc90d\ub2c8\ub2e4. text1 \uc740 \uc2dc\uc791\uc5d0 space \ub97c \ub123\uc5b4\uc90d\ub2c8\ub2e4. <br>\n```\n  text1 = \" Kaggle is a fun webplace!\"\n  text2 = \"fun webplace!\"\n```\n\ntext1 \uc5d0\uc11c text2 \uc758 \uc704\uce58(idx) \ub97c \ucc3e\uace0, chars \ubc30\uc5f4\uc744 \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4. enc \ub77c\ub294 \ubcc0\uc218\ub294 text1 \uc744 tokenizer \ub85c encode \ud55c \uac83 \uc785\ub2c8\ub2e4. \ucc38\uace0\ub85c '\u0120' \ub294 space \ub77c\uace0 \ubcf4\uc785\ub2c8\ub2e4.\n```\n  idx = 3\n  chars = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n  enc.tokens = ['\u0120k', 'agg', 'le', '\u0120is', '\u0120a', '\u0120fun', '\u0120web', 'place', '!']\n  enc.ids = [449, 7165, 459, 16, 10, 1531, 3748, 6406, 328]\n```\n\n\uc2e4\uc81c token \ub4e4\uc758 (\uc2dc\uc791, \uae38\uc774)\ub97c \uac00\uc9c0\ub294 offset \ubc30\uc5f4\uc744 \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4. <br>\n```\n  offset = [(0, 2), (2, 5), (5, 7), (7, 10), (10, 12), (12, 16), (16, 20), (20, 25), (25, 26)]\n```\n\ntext2 \uc5d0\uc11c text1 \uc758 \uc704\uce58\ub97c token \uc758 \uc704\uce58\ub85c \ud45c\uc2dc\ud560 \uc218 \uc788\ub294 tok \ubc30\uc5f4\uc744 \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4.\n```\n  tok = [5,6,7,8]\n```\n\n\uadf8\ub9ac\uace0 \uc704\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub97c \ud1b5\ud574\uc11c \uadf8\ub9bc\uc5d0\uc11c \ubcfc \uc218 \uc788\ub294 input \uc744 \ub9cc\ub4e4\uc5b4\uc900\ub2e4!","2bf25727":"\uae30\uc874 BERT \ub294 token, segment embedding, positional embedding \ub4f1\uc758 3\uac00\uc9c0\uc5d0 \ub300\ud574\uc11c input \uc744 \ubc1b\uc558\ub294\ub370 roberta \uc758 \uacbd\uc6b0\uc5d0\ub294 token, attention_mask, start_token, end_token \uc758 \ud615\ud0dc\ub85c \uc778\uc790\ub97c \ubc1b\uace0 \uc788\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\nsegment embedding \uc740 \ubb38\uc7a5\uacfc \ub2e4\uc74c \ubb38\uc7a5\uc5d0 \ub300\ud55c \uacbd\uacc4\ub97c \ud45c\uc2dc\ud558\uae30 \uc704\ud574\uc11c \uc0ac\uc6a9\ud588\ub358 \uac83\uc774\uace0, positional embedding \uc740 \ud2b9\uc815 \uc704\uce58\uc758 \ud1a0\ud070\uc774 \uc5b4\ub5a4 embedding \uc744 \uac16\ub294\uc9c0\uc5d0 \ub300\ud574\uc11c (512, 768) matrix \ub85c \ud45c\ud604\ub418\uc5c8\uc2b5\ub2c8\ub2e4. (\uc8fc\uc758! \ud2c0\ub9b4\ud655\ub960\uc774 \ub9e4\uc6b0 \ub192\uc2b5\ub2c8\ub2e4!) \uc774\uc640 \ube44\uc2b7\ud558\uac8c \ubb38\uc7a5\uc758 \uacbd\uacc4\ub97c \ud45c\uc2dc\ud558\uae30 \uc704\ud574\uc11c `<\/s><\/s>` \ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc73c\ub85c \ubcf4\uc774\ub294\ub370 positional \uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ub531\ud788 \uc8fc\uc5b4\uc9c0\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ubcf4\uc774\ub124\uc694.","5b24c685":"sentiment \uc758 distribution \uc744 \ubd05\uc2dc\ub2e4.","56a4dcb8":"# \ucc38\uace0\uc790\ub8cc\n\n**[Twitter sentiment Extaction-Analysis,EDA and Model](https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model)**\n* https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n  * WORDCLOUDS FUNCTION: \ub2e4\uc591\ud55c \uae00\uc790\ub4e4\uc744 \ud2b9\uc815 \uc774\ubbf8\uc9c0\uc758 \ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4 \ud45c\ud604\ud558\ub294 \uae30\ubc95\uc73c\ub85c \uba4b\uc9c4 \uadf8\ub9bc\uc744 \uc704\ud574\uc11c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n* https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-0-628-lb \n  * For understanding how to train spacy NER on custom inputs\n    \n**[TensorFlow roBERTa - [0.705]](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705)**\n* \bhttps:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds\n  * Tokenization logic \uc744 \ube4c\ub824\uc654\ub2e4\uace0 \ud569\ub2c8\ub2e4.\n* roBERTa \ub17c\ubb38\uc740 BERT \ub97c '\uc798' \ud559\uc2b5\uc2dc\ud0b4\uc73c\ub85c\uc368 BERT \uc774\ud6c4\uc5d0 \ub098\uc628 work \ub4e4 \ubcf4\ub2e4 \ub2e4\uc591\ud55c finetuning job \ub4e4\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc77c \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \uc544\ub798\ub294 roBERTa \ub17c\ubb38\uc758 4\uc7a5\uc5d0 \uc2e4\ub9b0 \uc2e4\ud5d8 \ub0b4\uc6a9\ub4e4\uc744 \uac00\uc838\uc628 \uac83\ub4e4\uc785\ub2c8\ub2e4.\n  * Static vs. Dynamic Masking \uc2e4\ud5d8: MLM \uc758 \ud2b9\uc131\uc744 \uc798 \uc0b4\ub824\uc11c \ubbf8\ub9ac \uac19\uc740 \uacf3\ub9cc masking \uc744 \ud558\uc9c0 \uc54a\uace0 on-the-fly \ub85c dynamic \ud558\uac8c input masking \uc744 \ud569\ub2c8\ub2e4.\n  * Model Input Format and Next Sentence Prediction: NSP(Next Sentence Prediction) \ub97c objective \ub85c \uc0ac\uc6a9\ud588\uc744\ub54c \uc131\ub2a5\uc774 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc5ec\uc8fc\uba70, Segment-pair, sentence-pair, full-sentences, doc-sentences \ub4e4\uc758 \ubc29\ubc95\uc744 \ud1b5\ud574\uc11c Sentence \ub97c \uc5b4\ub5bb\uac8c \uc798 \ubaa8\uc544\uc11c \ud559\uc2b5\uc744 \uc2dc\ucf30\uc744\ub54c \uc131\ub2a5\uc774 \uc88b\uc544\uc9c0\ub294 \uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uacb0\ub860\uc801\uc73c\ub85c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc740 NSP \ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc73c\uba70, \uac19\uc740 Document \uc758 \ubb38\uc7a5\ub4e4\ub9cc \uc798 \ubaa8\uc544\uc11c \ud559\uc2b5\uc2dc\ucf30\uc744\ub54c\uac00 \uc88b\uc740 \uacb0\uacfc\ub97c \ub0c8\uc2b5\ub2c8\ub2e4.\n  * Training with large batches: Batch \uc0ac\uc774\uc988\ub97c 2K, 8K \ub4f1\uacfc \uac19\uc774 \uc5c4\uccad \ud06c\uac8c \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4.\n  * Text Encoding: \uae30\uc874 BERT \uc5d0\uc11c\ub294 char-level \uc758 BPE \uc744 \uc0ac\uc6a9\ud558\uba70 30k \uc758 vocab \uc744 \uc0ac\uc6a9\ud588\uc9c0\ub9cc, roBERTa \uc5d0\uc11c\ub294 50k byte-level BPE \ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, \ucd94\uac00\uc801\uc778 input \uc5d0 \ub300\ud55c tokenization \uc774\ub098 preprocessing \uc744 \ud558\uc9c0 \uc54a\uc558\ub2e4\uace0 \ud569\ub2c8\ub2e4.\n  * \ub610\ud55c 5\uc7a5\uc5d0\uc11c\ub294 BERT-large \uc5d0\uc11c \uc0ac\uc6a9\ud55c 13GB \ub370\uc774\ud130\uc14b\ubcf4\ub2e4 \ud6e8\uc52c \ud070 496GB \uc815\ub3c4\uc758 \ud559\uc2b5 \ub370\uc774\ud130\uc14b\uc744 \uac01\uac01 8K batch\ub85c 500K step \uae4c\uc9c0 \ud559\uc2b5\uc2dc\ud0b4\uc73c\ub85c\uc368 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc774\ub8ec \ubc29\ubc95\uc785\ub2c8\ub2e4.","c6993ad8":"1\uac1c\uc5d0 \ub300\ud574\uc11c text, selected_text \uac00 null \uc774 \uc874\uc7ac\ud558\ub2c8\uae4c \uc5c6\uc560\uc90d\ub2c8\ub2e4.","c9e67a5d":"Training \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uc5b4\uc8fc\uae30 \uc704\ud574\uc11c \ub2e4\uc74c\uacfc \uac19\uc740 \uc791\uc5c5\uc744 \ud569\ub2c8\ub2e4.\n![image.png](attachment:image.png)","a846d7df":"# Tensorflow \ub97c \ud65c\uc6a9\ud574\uc11c roBERTa \uc2e4\ud589\ud558\uae30\n\n\uc5ec\uae30\ubd80\ud130\ub294 [TensorFlow roBERTa - 0.705](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705) \ub97c \ucc38\uace0\ud574\uc11c \uc791\uc131\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","7519c650":"# Submission","c42df5ba":"test \uc14b\uc740 \uad1c\ucc2e\ub124\uc694.","57058275":"\uadf8\ub9bc\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 neutral \uc5d0 \ub300\ud574\uc11c\ub294 \ucc28\uc774\uac00 \uac70\uc758 \uc5c6\uc74c\uc744 \ubcfc \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc774\ub294 jaccard_score \ub97c \ud655\uc778\ud558\uba74 \ub354 \uc798 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","3c5f7fcc":"# EDA\n\n[EDA](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section1\/eda11.htm) \ub780 Exploratory Data Analysis \uc758 \uc904\uc784\ub9d0\uc785\ub2c8\ub2e4. \ub370\uc774\ud130 \uc14b\uc5d0 \ub300\ud55c insight \ub97c \uac00\uc9c0\uae30 \uc704\ud574\uc11c \ub370\uc774\ud130\ub294 \ubd84\uc11d\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.\n* \ub370\uc774\ud130 \uc14b\uc5d0 \ub300\ud574\uc11c insight \ub97c \ucd5c\ub300\ud654 \ud558\uace0\n* \ub370\uc774\ud130 \uc14b\uc758 \uc0dd\uae40\uc0c8\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ub0b4\uba70\n* \uc911\uc694\ud55c \ubcc0\uc218\ub97c \ucc3e\uc544\uc11c \ucd94\ucd9c\ud574\ub0b4\uace0\n* outlier \ub098 anomalies \ub97c \ubc1c\uacac\ud574\ub0b4\uace0\n* \uc608\uc0c1\uc5d0 \ub300\ud55c \ud14c\uc2a4\ud2b8\ub97c \ud574\ubcf4\uba70\n* \uad49\uc7a5\ud788 \uac80\uc18c\ud55c \ubaa8\ub378(parsimonious model)\uc744 \ub9cc\ub4e4\uace0\n* optimal factor \ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294 \uacfc\uc815\uc774\ub77c\uace0 \ud569\ub2c8\ub2e4.\n\n\uac80\uc18c\ud55c \ubaa8\ub378\uc744 'explain data with a minimum number of parameters, or predictor variables' \ub77c\uace0 \ub9d0\ud558\uace0 \uc788\ub124\uc694. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 tensorflow \uc758 keras \ub97c \uc0ac\uc6a9\ud560 \uac83\uc774\ub77c... EDA \uc5d0\uc11c \ub370\uc774\ud130 \ubd84\uc11d\ub9cc\uc744 \ud558\uace0 \ub118\uc5b4\uac00\uaca0\uc2b5\ub2c8\ub2e4!","02126c89":"\ub9cc\ub4e4\uc5b4\uc9c4 \ub370\uc774\ud130\ub97c \ud655\uc778\ud574\ubd05\ub2c8\ub2e4.","b6ce3ba9":"27481 \uac1c\uc758 train set, 3534 \uac1c\uc758 test set \uc744 \uc81c\uacf5\ud558\ub124\uc694.","d4a6c89e":"\uc0dd\uac01\ubcf4\ub2e4 \ub9ce\uc740 \uac83\ub4e4\uc774 neutral \uac12\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.","3918f679":"selected_text \uc5f4\uc740 text \uc5f4\uc758 substring \uc778\uac83 \uac19\uc2b5\ub2c8\ub2e4.","38f51352":"\ub79c\ub364 \uceec\ub7ec\ub97c \ub9cc\ub4e4 \uc218 \uc788\ub3c4\ub85d \ud558\ub294 \ud568\uc218\ub97c \uc81c\uacf5\ud574\uc8fc\uace0\uc790 \ud558\ub124\uc694.","2067b976":"# \ub370\uc774\ud130 \uc77d\uae30\n\nkaggle \uc5d0\uc11c\ub294 \ubcf4\ud1b5 \ub370\uc774\ud130\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud574\uc11c pandas library \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4."}}