{"cell_type":{"037c4eea":"code","46ca32fd":"code","1662d7fe":"code","72800695":"code","ccb36598":"code","4f610127":"code","9525d170":"code","623e12cd":"code","4756af27":"code","078961d2":"code","a13446ac":"code","786a0846":"code","9f24b6ea":"code","5a580012":"code","fa313aa3":"code","52f0e805":"code","89536a6a":"code","bc7d549e":"code","8f86d01b":"code","d45958db":"code","e1ab8dc1":"code","0d00b734":"code","d3b178bf":"code","cd00fb6f":"code","39324f49":"code","6f7e2fb7":"code","a46045cc":"code","ff64ae1e":"code","fe2810cb":"code","6fa11570":"code","161d6ab5":"code","58efa21e":"code","a7e41765":"code","2ae78bbf":"code","3cd3ab16":"code","5fdda60b":"code","3e382e03":"code","92d7487a":"code","4aca989a":"code","14f1fa8f":"code","9647d263":"code","20d6179a":"code","93454e20":"code","c0bae22e":"code","3214503b":"code","30093e04":"code","543f4f8c":"code","e677073b":"code","2b05a42b":"code","18eed148":"code","ba7fb1d1":"code","f9ff959c":"code","bce4b3ed":"code","ba6079d4":"code","30a92062":"code","e1c327ee":"code","6c47d3a4":"code","408edac1":"code","510f7c55":"code","74920bd9":"code","98dc2487":"code","57851687":"code","790f2e2b":"code","42e0bc0d":"code","6085c101":"code","f1315770":"code","6f8f7abb":"code","cd69ee56":"code","0428e127":"code","5358ceda":"code","3ce83707":"code","28f1cc2a":"code","ef35e2e3":"code","54135b0d":"code","86af6c31":"code","ee3a6b88":"code","a6e709ba":"code","62caf643":"code","b951f482":"code","19198cd9":"code","aa1f2321":"code","5934cd5e":"markdown","fe468f05":"markdown","49ac62a9":"markdown","ccab7415":"markdown","cadeca20":"markdown","79e74590":"markdown","1ac256c8":"markdown"},"source":{"037c4eea":"pip install sorted_months_weekdays","46ca32fd":"pip install sort_dataframeby_monthorweek","1662d7fe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\nimport calendar as calendar\n\nfrom sorted_months_weekdays import *\n\nfrom sort_dataframeby_monthorweek import *\n\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\nimport os","72800695":"print(os.listdir('..\/input\/mount-rainier-weather-and-climbing-data\/'))","ccb36598":"#Import the Weather data ( Available only between 2014 and 2015)\ndbWeather=pd.read_csv('..\/input\/mount-rainier-weather-and-climbing-data\/Rainier_Weather.csv')","4f610127":"dbWeather.info()","9525d170":"dbWeather.isnull().values.any()","623e12cd":"dbWeather['Date'] = pd.to_datetime(dbWeather['Date'].str.strip(), format='%m\/%d\/%Y')","4756af27":"#Import the Climbing Statistics ( Available only between 2014 and 2015)\ndbclimbs=pd.read_csv('..\/input\/mount-rainier-weather-and-climbing-data\/climbing_statistics.csv')","078961d2":"dbclimbs.isnull().values.any()","a13446ac":"dbclimbs['Date'] = pd.to_datetime(dbclimbs['Date'].str.strip(), format='%m\/%d\/%Y')","786a0846":"dbclimbs.info()","9f24b6ea":"df=pd.DataFrame(columns=['Date','Battery Voltage AVG','Temperature AVG','Relative Humidity AVG','Wind Speed Daily AVG','Wind Direction AVG','Solar Radiation AVG','Route','Attempted','Succeeded','Success Ratio']) ","5a580012":"for index,row in dbclimbs.iterrows():\n    weatherRow=dbWeather.loc[dbWeather['Date'] == row['Date']]\n    df=df.append({'Date':row['Date'],'Battery Voltage AVG':weatherRow['Battery Voltage AVG'].to_string(index=False,header=False),'Temperature AVG':weatherRow['Temperature AVG'].to_string(index=False,header=False),'Relative Humidity AVG':weatherRow['Relative Humidity AVG'].to_string(index=False,header=False),'Wind Speed Daily AVG':weatherRow['Wind Speed Daily AVG'].to_string(index=False,header=False),'Wind Direction AVG':weatherRow['Wind Direction AVG'].to_string(index=False,header=False),'Solar Radiation AVG':weatherRow['Solare Radiation AVG'].to_string(index=False,header=False),'Route':row['Route'],'Attempted':row['Attempted'],'Succeeded':row['Succeeded'],'Success Ratio':row['Success Percentage']}, ignore_index=True)\n      ","fa313aa3":"df.head(10)","52f0e805":"df.info()","89536a6a":"cols = df.columns.drop(['Date','Route'])","bc7d549e":"#Converting object type attributes to Float except for route\ndf[cols] = df[cols].apply(pd.to_numeric, errors='coerce')","8f86d01b":"#Extract Month\ndf['Month']=pd.to_datetime(df['Date']).dt.month.apply(lambda x: calendar.month_name[x])\n\n","d45958db":"df['Month_Val']=pd.to_datetime(df['Date']).dt.month","e1ab8dc1":"#Extract Year\ndf['Year']=pd.to_datetime(df['Date']).dt.year","0d00b734":" df.isna().sum()","d3b178bf":"# After merging the Weather Data Set with the Climbing Dataset, it has been observed that there are many records of a day for whic the weather has not been captured\n# Imputing an average weather of the month do not make sense, instead i would like to capture the climbing statistics only for those days where the weather information is available\ndf.dropna(inplace=True)\n","cd00fb6f":"#Climbs by Year\ndbRainierByYear=df.groupby(['Year'])[\"Attempted\",\"Succeeded\"].sum().reset_index()","39324f49":"dbRainierByYear['Success %']=(dbRainierByYear['Succeeded']\/dbRainierByYear['Attempted'])*100","6f7e2fb7":"dbRainierByYear.head(20)","a46045cc":"dbRainierByYear.plot(x='Year',y=[\"Success %\"],kind='bar')\n\n","ff64ae1e":"#Climbs by Month\ndbRainierByMonth=df.groupby(['Month'])[\"Attempted\",\"Succeeded\"].sum().reset_index()","fe2810cb":"dbRainierByMonth['Success %']=(dbRainierByMonth['Succeeded']\/dbRainierByMonth['Attempted'])*100","6fa11570":"Sort_Dataframeby_Month(dbRainierByMonth,monthcolumnname='Month')","161d6ab5":"dbRainierByMonth.head(20)","58efa21e":"Sort_Dataframeby_Month(dbRainierByMonth,monthcolumnname='Month').plot(x='Month',y=['Attempted','Succeeded'],kind='bar')\n","a7e41765":"#Climbs by Route\ndbRainierByRoute=df.groupby(['Route'])[\"Attempted\",\"Succeeded\"].sum().reset_index()","2ae78bbf":"dbRainierByRoute.head(5)","3cd3ab16":"dbRainierByRoute.plot(x='Route',y=[\"Attempted\",\"Succeeded\"],kind='bar')","5fdda60b":"sns.pairplot(df, diag_kind='kde') ","3e382e03":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nX = df.iloc[:,0:20]  #independent columns\ny = df.iloc[:,-1]    #target column i.e price range\n#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","92d7487a":"# removing the attempted, succeeded, Month, Year and Date columns from the dataset so that the target column success can be strictly used for analysis\n# Removing Battery Voltage since it is redundant to Tempareture and Wind Direction since it doesnt seem to have a major impact on the overall summit attempts\n# Solar Radiation is linear with Temperate - Has many null values and outliers. Hence removing the column\n\ndf_model=df.drop(['Attempted','Succeeded','Year','Month','Date', 'Battery Voltage AVG','Wind Direction AVG','Solar Radiation AVG'], axis=1)","4aca989a":"# Wind Speed Average has 0 values in the data and need to be corrected. Windspeed can never be 0\n# Observing the data has given some insights, that Wind Speed has been 0  when the Relative Humidity is above 90 in 98% of the cases\n# Calculating the average windspeed when the relative humidity is over 89 but less than 99 gives - 21.17\n# Replace the 0 values of the wind speed to 21.17\ndf_model['Wind Speed Daily AVG']=df_model['Wind Speed Daily AVG'].replace([0], 21.17)\n","14f1fa8f":"df_model.info()\n","9647d263":"df_model.head(10)","20d6179a":" df_model.isna().sum()","93454e20":"sns.pairplot(df_model, diag_kind='hist')","c0bae22e":"df_new=pd.get_dummies(df_model)\n#df_new=df_model\n","3214503b":"df_new.info()","30093e04":"df_new.head(10)","543f4f8c":" df_new.isnull().sum()","e677073b":"# Converting the values to Z-Scores since the attributes values are in different ratios and types\n\ndf_new_z = df_new.apply(zscore)","2b05a42b":"df_new_z.describe()","18eed148":"#Split the data to train and test\n\ny = df_new_z['Success Ratio'].values\nX = df_new_z.drop('Success Ratio', axis=1).values\n\n","ba7fb1d1":"y","f9ff959c":"from sklearn import model_selection\n\ntest_size = 0.30 # taking 70:30 training and test set\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size)\n","bce4b3ed":"y","ba6079d4":"X_train","30a92062":"# Linear Regression\n\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)","e1c327ee":"model.coef_","6c47d3a4":"model.intercept_","408edac1":"model.score(X_test, y_test)\n# Very bad score.","510f7c55":"plt.scatter(X_test[:,0], y_test)\nplt.plot(X_test[:,0], y_predict, color='red')\nplt.show()","74920bd9":"from sklearn.svm import SVR\nmodel = SVR(gamma='scale', C=1.0, epsilon=0.2)\nmodel.fit(X, y)\ny_predict = model.predict(X_test)","98dc2487":"model.score(X_test, y_test)","57851687":"plt.scatter(X_test[:,0], y_test)\nplt.plot(X_test[:,0], y_predict, color='red')\nplt.show()","790f2e2b":"#Decision Tree and Random Forest\nfrom sklearn.tree import DecisionTreeRegressor  \n  \n# create a regressor object \nregressor = DecisionTreeRegressor(random_state = 3)  \n\n  \n# fit the regressor with X and Y data \nregressor.fit(X, y) ","42e0bc0d":"df1= pd.DataFrame()\n#df1['feature'] = df.drop(['not.fully.paid'], axis=1).columns\ndf1['feature'] = df_new_z.drop('Success Ratio', axis=1).columns\ndf1['Importance Index']= regressor.feature_importances_\nprint(df1.sort_values(by='Importance Index', ascending=False))","6085c101":"#Choosing only the top 12 columns from the dataset as per the feature importance notatin above\n\ndf_Impftrs=df_new.drop(columns=['Route_Liberty RIngraham Directge','Route_Kautz Cleaver','Route_Ptarmigan RIngraham Directge','Route_Ingraham Direct','Route_Gibralter Ledges','Route_Mowich Face','Route_Success Cleaver','Route_Sunset RIngraham Directge','Route_Tahoma Cleaver',\"Route_Fuhrer's Finger\",'Route_Gibralter Chute','Route_Curtis RIngraham Directge','Route_Tahoma Glacier','Route_Unknown','Route_Wilson Headwall','Route_Nisqually Glacier'])","f1315770":"df_Impftrs.head(10)","6f8f7abb":"df_Impftrs_z = df_Impftrs.apply(zscore)","cd69ee56":"#Split the data to train and test\n\ny = df_Impftrs_z['Success Ratio'].values\nX = df_Impftrs_z.drop('Success Ratio', axis=1).values\n","0428e127":"from sklearn import model_selection\n\ntest_size = 0.30 # taking 70:30 training and test set\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size)\n","5358ceda":"#Decision Tree and Random Forest\nfrom sklearn.tree import DecisionTreeRegressor  \n  \n# create a regressor object \nregressor = DecisionTreeRegressor(random_state = 0)  \n\n  \n# fit the regressor with X and Y data \nregressor.fit(X, y) ","3ce83707":"y_predict = regressor.predict(X_test)","28f1cc2a":"regressor.score(X_test, y_test)","ef35e2e3":"# Bagging - Ensemble\n\nfrom sklearn.ensemble import BaggingRegressor\n\nbgcl = BaggingRegressor(n_estimators=35)\n\n#bgcl = BaggingClassifier(n_estimators=50)\nbgcl = bgcl.fit(X, y)","54135b0d":"y_predict = bgcl.predict(X_test)\n\nprint(bgcl.score(X_test, y_test))\n\n","86af6c31":"# GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbcl = GradientBoostingRegressor(n_estimators = 50)\ngbcl = gbcl.fit(X, y)\ny_predict = bgcl.predict(X_test)\nprint(bgcl.score(X_test, y_test))\n","ee3a6b88":"#RandomForest\nfrom sklearn.ensemble import RandomForestRegressor\nrfcl = RandomForestRegressor(n_estimators = 50)\nrfcl = rfcl.fit(X, y)\ny_predict = rfcl.predict(X_test)\nprint(rfcl.score(X_test, y_test))\n","a6e709ba":"print('Parameters currently in use:\\n')\nprint(rfcl.get_params())\nprint(\"\\n\")\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 300, num = 9)] ## play with start and stop\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 20, num = 5)] ## change 10,20 and 2\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,15]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,10]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","62caf643":"# Use the random grid to search for best hyperparameters\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf_random = RandomizedSearchCV(estimator = rfcl, param_distributions = random_grid, n_iter = 200, cv = 3, \n                               verbose=2, random_state=50, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train,y_train)\nprint(\"Best Parameters are:\",rf_random.best_params_)","b951f482":"best_random = rf_random.best_estimator_\nbest_random.fit(X_train,y_train)\n\npredictions = best_random.predict(X_test)\n\nprint(best_random.score(X_test, y_test))\n\n#from sklearn.metrics import classification_report,confusion_matrix\n#print(classification_report(y_test,predictions))","19198cd9":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nregressor = ExtraTreesRegressor(n_estimators=700,min_samples_split= 5,max_depth= 50, min_samples_leaf= 5) ","aa1f2321":"regressor.fit(X_train, y_train)\npredictions = regressor.predict(X_test)\nprint(regressor.score(X_test, y_test))","5934cd5e":"# Analysis\n\nThe above plot shows that most of the climbs were attempted through the disappointment cleaver route and a few considerable number through the Emmons and Kaultz Galcier","fe468f05":"# Imputing variables","49ac62a9":"# Models","ccab7415":"# Pairplot analysis and correlation\n\n1. Clearly there is a correlation between Battery Volate and The Temperature but their correlation does not have an effect on the summit success\n2. The Number of attempts and success are clearly concentrated within a period of the year , during certain months, than on specific dates.\n3. Higher the temperature, higher is the solar radiation and so is the number of attempts and successes\n4. There seems to be a lot of Outliers in the data specifically in the attributed Solar Radiation\n5. As the temperature has increased, the humidity has decreased creating favourable conditions for climbing.\n6. As the temperateure has decreased the wind speed has decreased, indicating that during the colder weather the wind speed is usually high which is unfavourable for climbing\n\n\n","cadeca20":"# This dataset contains the climbing statistics and weather reports for year 2014 and 2015 for Mt Rainier\n\nField Description\n\n1. Date : They day of the record\n2. Battery Voltage AVG: The average voltage of the day captured\n3. Temperate AVG:  The average temperature of the day in Farenheit\n4. Relative Humidity: The value that depicts the average humidity of the day \n5. Wind Speed Daily Average: Average wind speed on that day in mph\n6. Wind Directon Average: Average direction of the wind in deg\n7. Solar Radiation AVG: The average solar radiation of the day in Watts\/square metre\n8. Route: There are several routes through which people attempt to climb Mt Rainier\n9. Attempted: Total number of people who attempted the climb\n10. Succeeded: Total number of people who have reach the summit\n11. Success Percentage: The ration of Succeeded to attempted (target) \n","79e74590":"# Hyperparameter Tuning of Random Forests","1ac256c8":"## Analysis\n1. The Above chart show that most number of the climbing attempts and the successes are during the months, May, June, July , August and September"}}