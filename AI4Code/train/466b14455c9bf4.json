{"cell_type":{"b4d3b0fc":"code","413e07e9":"code","1605bef7":"code","6387ec4b":"code","884700a0":"code","047268d2":"code","2e930347":"code","822ae03f":"code","2e363a7a":"code","9c1b75cc":"code","113dc786":"code","bcef443c":"code","170c3102":"code","9b4e970c":"code","25d16e30":"code","971e7b21":"code","2aeaaf13":"code","6475449b":"code","de047270":"code","bfad8894":"code","1d8bcd7c":"code","07a26abf":"code","8d14a33e":"code","ed373fc0":"code","b3ace2a9":"code","ef8aa707":"code","efbb26f8":"code","40c39465":"code","62693867":"code","f4981a03":"code","baf29ddd":"code","b18b4b3d":"code","a9a2d223":"code","5efae1aa":"code","3c74dbb9":"code","b48c9402":"code","9e570898":"code","58af6041":"code","bc201fbe":"code","6ce9e48d":"code","6058d4ca":"code","9d9ca3e6":"code","be71f1f3":"code","3ba962aa":"code","1094dfc5":"code","d542d5be":"code","1988458c":"code","66eb4ca2":"code","992abc8e":"code","538ebca7":"code","90a7857c":"code","688461dc":"code","93dc6e3c":"code","c4a110ae":"code","af39224a":"code","e63c0198":"code","2255b87e":"code","660605a6":"code","bd3c90ff":"code","6b97f7ae":"code","3650ab67":"code","3d8a1486":"code","498752c9":"code","14ccbc93":"code","73fbf510":"markdown","4e286a3a":"markdown","b8c15712":"markdown","11a107f1":"markdown","c0711c37":"markdown","53152f8f":"markdown","59eb1256":"markdown","10540316":"markdown","886164f7":"markdown","38b67df6":"markdown","3353018d":"markdown","900cc95a":"markdown","541e44cd":"markdown","9d9f4581":"markdown","5caa3e77":"markdown","f9d91d2b":"markdown","eed6f709":"markdown","6c917804":"markdown","a3e7d943":"markdown","bffe7794":"markdown","fc997467":"markdown","110856f2":"markdown","4c3a42f4":"markdown","13b4049b":"markdown","74733aae":"markdown","e758668e":"markdown","902ace74":"markdown","71342d39":"markdown","419ae97c":"markdown","ce34827c":"markdown","38bb2736":"markdown","3e28f90a":"markdown","da7a16c4":"markdown","cb30b434":"markdown","a45318d1":"markdown","17142389":"markdown","9a062110":"markdown","646f4b62":"markdown","c2124f07":"markdown","1675338f":"markdown","15eec5df":"markdown","2ab0522b":"markdown","b2e217a5":"markdown","3bc0d979":"markdown","0a94e6be":"markdown","58c48c89":"markdown","870ca3c5":"markdown","8b1de938":"markdown","c5ee5923":"markdown","8b1a3e1a":"markdown","f10b2928":"markdown","0172f18a":"markdown","d151c75f":"markdown","88723a43":"markdown","a5687ec9":"markdown","901076a7":"markdown","2b6fc902":"markdown","aabf7690":"markdown","bc552cdc":"markdown","a8840ba4":"markdown","1b0f95d6":"markdown","f3c25ae4":"markdown","9b7a307c":"markdown","ae4cb39a":"markdown","f9019c72":"markdown","0e63fed5":"markdown","bb9afae4":"markdown","dc574314":"markdown","f5eac93f":"markdown"},"source":{"b4d3b0fc":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt","413e07e9":"data_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","1605bef7":"# checking if files have been loaded correctly\ndata_train.head()","6387ec4b":"data_train.shape, data_test.shape","884700a0":"data_train['SalePrice'].describe()","047268d2":"# columns which have null values\ncolumns_having_nulls_train = data_train.columns[data_train.isna().any()].sort_values(ascending=False).to_series()\n\n# count of null values in those column\nvalues_train = data_train[columns_having_nulls_train].isna().sum().sort_values(ascending=False)\n\n# percent of null values in those columns\npercent_train = (data_train[columns_having_nulls_train].isna().sum()\/data_train.shape[0]).sort_values(ascending=False)\n\n# concat series and create a datframe to present the information cleanly\npd.concat([columns_having_nulls_train, values_train, percent_train], axis=1, sort=False, join='inner'\n          , keys=['Feature', 'Null Count', 'Percent Null']).sort_values(by=['Null Count'], ascending=False).reset_index(drop=True)","2e930347":"columns_having_nulls_test = data_test.columns[data_test.isna().any()].sort_values(ascending=False).to_series()\nvalues_test = data_test[columns_having_nulls_test].isna().sum().sort_values(ascending=False)\npercent_test = (data_test[columns_having_nulls_test].isna().sum()\/data_test.shape[0]).sort_values(ascending=False)\npd.concat([columns_having_nulls_test, values_test, percent_test], axis=1, sort=False, join='inner'\n          , keys=['Feature', 'Null Count', 'Percent Null']).sort_values(by=['Null Count'], ascending=False).reset_index(drop=True)","822ae03f":"#data_train.describe().T","2e363a7a":"plt.figure(figsize=(10,5))\ng = sns.scatterplot(x=\"LotArea\", y=\"SalePrice\", hue=\"GrLivArea\", data=data_train, palette=\"Set2\")","9c1b75cc":"# Let's create a new variable Total Floor Surface Area (1st + 2nd) and see how total surface are is related with SalePrice\ndata_train['TotalSF'] = data_train['1stFlrSF'] + data_train['2ndFlrSF']\nplt.figure(figsize=(10,5))\ng = sns.scatterplot(x=\"TotalSF\", y=\"SalePrice\", hue=\"OverallQual\", data=data_train, palette=\"Set2\")","113dc786":"g = sns.catplot(x=\"TotRmsAbvGrd\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=2, height=4, palette=\"Set2\")","bcef443c":"g = sns.catplot(x=\"OverallQual\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=1.6, height=4, palette=\"Set2\")","170c3102":"g = sns.catplot(x=\"OverallCond\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=1.6, height=4, palette=\"Set2\")","9b4e970c":"data_train['TotalBaths'] = data_train['FullBath'] + data_train['HalfBath']\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize = (15, 5))\nsns.boxplot(x=\"HalfBath\", y=\"SalePrice\", data=data_train, ax=ax1)\nsns.boxplot(x=\"FullBath\", y=\"SalePrice\", data=data_train, ax=ax2)\nsns.boxplot(x=\"TotalBaths\", y=\"SalePrice\", data=data_train, ax=ax3)","25d16e30":"sns.catplot(x=\"Utilities\", y=\"SalePrice\", kind=\"violin\", data=data_train, aspect=1, height=4)","971e7b21":"data_train[data_train['Utilities']=='NoSeWa'].shape[0]","2aeaaf13":"data_test['Utilities'].value_counts()","6475449b":"g = sns.catplot(x=\"YearBuilt\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=4, height=4)","de047270":"# Age before selling\ndata_train['AgeAtSale'] = data_train['YrSold'] - data_train['YearBuilt'] + data_train['MoSold']\/12\ndata_train['AgeAtSale'] = round(data_train['AgeAtSale'])\ndata_train['AgeAtSale'] = pd.qcut(data_train['AgeAtSale'], 4)\ng = sns.catplot(x=\"AgeAtSale\", y=\"SalePrice\", kind=\"violin\", data=data_train, aspect=4, height=4)","bfad8894":"sns.catplot(x=\"MSZoning\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=2, height=4, palette=\"Set2\")","1d8bcd7c":"sns.catplot(x=\"MSZoning\", y=\"TotalSF\", kind=\"box\", data=data_train, aspect=2, height=4, palette=\"Set2\")","07a26abf":"data_test['MSZoning'].unique()","8d14a33e":"sns.catplot(x=\"LandContour\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=2, height=4, palette=\"Set2\")","ed373fc0":"sns.catplot(x=\"ExterQual\", y=\"SalePrice\", kind=\"violin\", data=data_train, aspect=2, height=4, palette=\"Set2\")","b3ace2a9":"sns.catplot(x=\"BsmtQual\", y=\"SalePrice\", kind=\"violin\", data=data_train, aspect=2, height=4, palette=\"Set2\")","ef8aa707":"plt.figure(figsize=(10,5))\ng = sns.regplot(x=\"SalePrice\", y=\"GarageArea\", data=data_train)","efbb26f8":"data_train['SaleType_updated'] = data_train['SaleType']\ndata_train.loc[((data_train['SaleType'] == 'WD') | (data_train['SaleType'] == 'CWD') | (data_train['SaleType'] == 'VWD')), 'SaleType_updated'] = 'WD'\ndata_train.loc[((data_train['SaleType'] == 'Con') | (data_train['SaleType'] == 'ConLD') | (data_train['SaleType'] == 'ConLI') | (data_train['SaleType'] == 'ConLw')), 'SaleType_updated'] = 'Con'\nsns.catplot(x=\"SaleType_updated\", y=\"SalePrice\", kind=\"box\", data=data_train, aspect=1.5, height=4, palette=\"Set2\")","40c39465":"data_train.loc[(data_train['SaleType'] != 'New'), 'SaleType_updated'] = 'Other'\nsns.catplot(x=\"SaleType_updated\", y=\"SalePrice\", kind=\"violin\", data=data_train, aspect=1.5, height=4, palette=\"Set2\")","62693867":"sns.catplot(x=\"CentralAir\", y=\"SalePrice\", kind=\"violin\", data=data_train, aspect=1, height=4, palette=\"Set2\")","f4981a03":"corrmat = data_train.loc[:,~data_train.columns.isin(['TotalSF','TotalBaths'])].corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corrmat, vmax=.8, square=True);","baf29ddd":"corrmat = data_train[['TotalSF','TotalBaths','TotalBsmtSF','1stFlrSF','FullBath','GarageCars','GarageArea','TotRmsAbvGrd','GrLivArea','SalePrice']].corr()\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(corrmat, square=True, annot=True);","b18b4b3d":"cols_to_remove1 = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage']  #cols with lot of nulls\ncols_to_remove2 = ['GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea', 'TotalBsmtSF'] #correlated with other feature\ncols_to_remove3 = ['Utilities'] #not contributing to prediction\ncols_to_remove4 = ['OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', \n                   'HalfBath', 'BedroomAbvGrd', 'KitchenAbvGrd', 'WoodDeckSF', 'OpenPorchSF', 'EncliosedPorch', '3SsnPorch', \n                   'ScreenPorch', 'PoolArea', 'MiscVal']\n# 'MSSubclass', 'MoSold', 'YrSold' -- on hold","a9a2d223":"# from our understanding of data\ncategorical_cols = ['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope',\n                   'Neighborhood','Condition1','Condition2','BldgType','HouseStyle','OverallQual','OverallCond','RoofStyle',\n                   'RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual',\n                    'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical',\n                   'KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n                   'PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition',\n                    'MoSold']","5efae1aa":"# so what remains are the numerical columns\nnumerical_cols = list(set(data_train.columns.tolist()) - set(categorical_cols))","3c74dbb9":"data_train = data_train[~((data_train['TotalSF'] > 4000) & (data_train['SalePrice'] < 300000))]","b48c9402":"data_train.shape","9e570898":"data_train['GarageType'].fillna(value=data_train['GarageType'].mode()[0], inplace=True)\ndata_train['BsmtQual'].fillna(value=data_train['BsmtQual'].mode()[0], inplace=True)\ndata_train['Electrical'].fillna(value=data_train['Electrical'].mode()[0], inplace=True)\ndata_train['GarageQual'].fillna(value=data_train['GarageQual'].mode()[0], inplace=True)\ndata_train['MasVnrArea'].fillna(value=data_train['MasVnrArea'].mean(), inplace=True)","58af6041":"data_test['KitchenQual'].fillna(data_test['KitchenQual'].mode()[0], inplace=True)\ndata_test['GarageType'].fillna(data_test['GarageType'].mode()[0], inplace=True)\ndata_test['MSZoning'].fillna(data_test['MSZoning'].mode()[0], inplace=True)\ndata_test['BsmtQual'].fillna(data_test['BsmtQual'].mode()[0], inplace=True)\ndata_test['SaleType'].fillna(data_test['SaleType'].mode()[0], inplace=True)\ndata_test['GarageCars'].fillna(data_test['GarageCars'].mode()[0], inplace=True)\ndata_test['GarageQual'].fillna(data_test['GarageQual'].mode()[0], inplace=True)\ndata_test['MasVnrArea'].fillna(data_test['MasVnrArea'].mean(), inplace=True)\ndata_test['BsmtFinSF1'].fillna(data_test['BsmtFinSF1'].mean(), inplace=True)","bc201fbe":"data_train[['GarageType', 'MasVnrArea', 'BsmtQual', 'Electrical', 'GarageQual']].isna().any().sum()","6ce9e48d":"data_test[['KitchenQual', 'GarageType', 'MSZoning', 'MasVnrArea', 'BsmtQual', 'SaleType', 'BsmtFinSF1', 'GarageCars', 'GarageQual']].isna().any().sum()","6058d4ca":"# data_train['AgeAtSale'] - Already created\ndata_train['RemodAgeAtSale'] = data_train['YrSold'] - data_train['YearRemodAdd']\ndata_train['RemodAgeAtSale'] = round(data_train['RemodAgeAtSale'])\ndata_train['RemodAgeAtSale'] = pd.cut(data_train['RemodAgeAtSale'], 3)","9d9ca3e6":"data_train['IsMultiStory'] = data_train['2ndFlrSF'].apply(lambda x: 0 if x <= 0 else 1)","be71f1f3":"data_test['AgeAtSale'] = data_test['YrSold'] - data_test['YearBuilt'] + data_test['MoSold']\/12\ndata_test['AgeAtSale'] = round(data_test['AgeAtSale'])\ndata_test['AgeAtSale'] = pd.qcut(data_test['AgeAtSale'], 4)\n\ndata_test['TotalBaths'] = data_test['FullBath'] + data_test['HalfBath']\n\ndata_test['RemodAgeAtSale'] = data_test['YrSold'] - data_test['YearRemodAdd']\ndata_test['RemodAgeAtSale'] = round(data_test['RemodAgeAtSale'])\ndata_test['RemodAgeAtSale'] = pd.cut(data_test['RemodAgeAtSale'], 3)\n\ndata_test['IsMultiStory'] = data_test['2ndFlrSF'].apply(lambda x: 0 if x <= 0 else 1)","3ba962aa":"# 'Id' is kept so that we can create submission files at the end\nnum_cols_to_keep = ['Id','1stFlrSF', 'BedroomAbvGr', 'BsmtFinSF1', 'EnclosedPorch', 'Fireplaces', 'TotalBaths', 'FullBath', 'GarageCars', 'GrLivArea', 'KitchenAbvGr', 'LotArea', 'MasVnrArea', 'SalePrice']\ncat_cols_to_keep = ['BsmtQual', 'CentralAir', 'Electrical', 'ExterQual', 'GarageQual', 'GarageType', 'Heating', 'HouseStyle', 'KitchenQual', 'MSZoning', 'OverallQual', 'SaleType']\nnew_cols_we_created = ['IsMultiStory','AgeAtSale','RemodAgeAtSale']\ncols_to_keep = list(set(num_cols_to_keep)|set(cat_cols_to_keep)|set(new_cols_we_created))","1094dfc5":"data_train = data_train[cols_to_keep]\ndata_train.shape","d542d5be":"data_test = data_test[list(set(cols_to_keep) - {'SalePrice'})]\ndata_test.shape","1988458c":"data_train.loc[(data_train['Electrical'] != 'SBrkr'), 'Electrical'] = 0\ndata_train.loc[(data_train['Electrical'] == 'SBrkr'), 'Electrical'] = 1\n\ndata_train.loc[((data_train['KitchenQual'] == 'Fa') | (data_train['KitchenQual'] == 'TA')), 'KitchenQual'] = 'Fa'\n\ndata_train.loc[(data_train['SaleType'] != 'New'), 'SaleType'] = 0\ndata_train.loc[(data_train['SaleType'] == 'New'), 'SaleType'] = 1\n\ndata_train.loc[((data_train['GarageQual'] == 'Fa') | (data_train['GarageQual'] == 'Po')), 'GarageQual'] = 0\ndata_train.loc[(data_train['GarageQual'] != 'Poor'), 'GarageQual'] = 1\n\ndata_train.loc[((data_train['GarageType'] == 'Attchd') | (data_train['GarageType'] == 'BuiltIn') | (data_train['GarageType'] == 'Basment') | (data_train['GarageType'] == '2Types') ), 'GarageType'] = 1\ndata_train.loc[((data_train['GarageType'] == 'Detchd') | (data_train['GarageType'] == 'CarPort') ), 'GarageType'] = 0\n\ndata_train.loc[((data_train['Heating'] == 'Floor') | (data_train['Heating'] == 'Wall') | (data_train['Heating'] == 'Grav') | (data_train['Heating'] == 'OthW') ), 'Heating'] = 0\ndata_train.loc[((data_train['Heating'] == 'GasA') | (data_train['Heating'] == 'GasW') ), 'Heating'] = 1\n\ndata_train.loc[((data_train['HouseStyle'] == '1.5Unf') | (data_train['HouseStyle'] == '2.5Unf') ), 'HouseStyle'] = 0\ndata_train.loc[(data_train['HouseStyle'] != 'Unfinished'), 'HouseStyle'] = 1","66eb4ca2":"data_test.loc[(data_test['Electrical'] != 'SBrkr'), 'Electrical'] = 0\ndata_test.loc[(data_test['Electrical'] == 'SBrkr'), 'Electrical'] = 1\n\ndata_test.loc[((data_test['KitchenQual'] == 'Fa') | (data_test['KitchenQual'] == 'TA')), 'KitchenQual'] = 'Fa'\n\ndata_test.loc[(data_test['SaleType'] != 'New'), 'SaleType'] = 0\ndata_test.loc[(data_test['SaleType'] == 'New'), 'SaleType'] = 1\n\ndata_test.loc[((data_test['GarageQual'] == 'Fa') | (data_test['GarageQual'] == 'Po')), 'GarageQual'] = 0\ndata_test.loc[(data_test['GarageQual'] != 'Poor'), 'GarageQual'] = 1\n\ndata_test.loc[((data_test['GarageType'] == 'Attchd') | (data_test['GarageType'] == 'BuiltIn') | (data_test['GarageType'] == 'Basment') | (data_test['GarageType'] == '2Types') ), 'GarageType'] = 1\ndata_test.loc[((data_test['GarageType'] == 'Detchd') | (data_test['GarageType'] == 'CarPort') ), 'GarageType'] = 0\n\ndata_test.loc[((data_test['Heating'] == 'Floor') | (data_test['Heating'] == 'Wall') | (data_test['Heating'] == 'Grav') | (data_test['Heating'] == 'OthW') ), 'Heating'] = 0\ndata_test.loc[((data_test['Heating'] == 'GasA') | (data_test['Heating'] == 'GasW') ), 'Heating'] = 1\n\ndata_test.loc[((data_test['HouseStyle'] == '1.5Unf') | (data_test['HouseStyle'] == '2.5Unf') ), 'HouseStyle'] = 0\ndata_test.loc[(data_test['HouseStyle'] != 'Unfinished'), 'HouseStyle'] = 1","992abc8e":"data_train['BsmtQual'].replace({'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, inplace=True)\ndata_train['ExterQual'].replace({'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, inplace=True)\ndata_train['KitchenQual'].replace({'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, inplace=True)\ndata_train['CentralAir'].replace({'Y': 1, 'N': 2}, inplace=True)\n#---\ndata_test['BsmtQual'].replace({'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, inplace=True)\ndata_test['ExterQual'].replace({'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, inplace=True)\ndata_test['KitchenQual'].replace({'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}, inplace=True)\ndata_test['CentralAir'].replace({'Y': 1, 'N': 2}, inplace=True)","538ebca7":"data_train = pd.get_dummies(data_train, columns=['MSZoning','AgeAtSale','RemodAgeAtSale'])\ndata_test = pd.get_dummies(data_test, columns=['MSZoning','AgeAtSale','RemodAgeAtSale'])","90a7857c":"data_train.columns","688461dc":"data_train[num_cols_to_keep].describe()","93dc6e3c":"cols_to_standardize = ['1stFlrSF', 'BsmtFinSF1', 'EnclosedPorch', 'GrLivArea', 'LotArea', 'MasVnrArea']\ncol_scalar = MinMaxScaler()\nfor col in cols_to_standardize:\n    data_train[col] = col_scalar.fit_transform(data_train[col].values.reshape(-1,1))\n    data_test[col] = col_scalar.transform(data_test[col].values.reshape(-1,1))","c4a110ae":"data_train[cols_to_standardize].head()","af39224a":"data_test.shape","e63c0198":"cv_splits = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)","2255b87e":"kwargs = {\n    'LinearRegression':{'fit_intercept': True},\n    'Ridge': {'alpha': 1,\n              'max_iter': 1000,\n              'solver': 'auto',\n              'random_state': 42},\n    'Lasso': {'alpha': 1,\n              'max_iter': 1000,\n              #'precompute': 'auto',\n              'warm_start': False,\n              'selection':'cyclic',\n              'random_state': 42},\n    'KNeighborsRegressor': {'n_neighbors': 10,\n                            'algorithm': 'auto',\n                           'leaf_size': 10,\n                           'p': 2},\n    'SVR': {'kernel': 'poly',\n            'degree':5,\n            'C':10.0, \n            'epsilon':0.01, \n            'shrinking':True},\n    'GradientBoostingRegressor': {'loss':'huber',\n                                  'learning_rate':0.1,\n                                  'n_estimators':100,\n                                  'subsample': 1, \n                                  'max_depth':3, \n                                  'random_state':42, \n                                  'max_features':None, \n                                  'max_leaf_nodes':None, \n                                  'validation_fraction':0.1},\n    'MLPRegressor': {'hidden_layer_sizes':(64,32), \n                     'activation':'relu', \n                     'solver':'lbfgs', \n                     'alpha':0.025, \n                     'batch_size':'auto', \n                     'learning_rate':'adaptive', \n                     'learning_rate_init':0.001, \n                     'max_iter':500, \n                     'shuffle':True, \n                     'random_state':42, \n                     'momentum':0.8, \n                     'beta_1':0.9, 'beta_2':0.999, \n                     'epsilon':1e-08}\n}","660605a6":"algos = {\n    'LinearRegression':LinearRegression(**kwargs['LinearRegression']),\n    'Ridge':Ridge(**kwargs['Ridge']),\n    'Lasso':Lasso(**kwargs['Lasso']),\n    'KNeighborsRegressor':KNeighborsRegressor(**kwargs['KNeighborsRegressor']),\n    'SVR':SVR(**kwargs['SVR']),\n    'GradientBoostingRegressor':GradientBoostingRegressor(**kwargs['GradientBoostingRegressor']),\n    'MLPRegressor':MLPRegressor(**kwargs['MLPRegressor'])\n}","bd3c90ff":"cv_results = {'Algorithm':[],                     # algorithm name\n              'Mean Train MSE':[],                # Mean of training accuracy on all splits\n              'Mean Test MSE':[],                 # Mean of test accuracy on all splits\n              'Mean Train R2':[],\n              'Mean Test R2':[],\n              'Fit Time': []}                     # how fast the algorithm converges","6b97f7ae":"for alg_name,alg in algos.items():\n    cv_results['Algorithm'].append(alg_name)\n    \n    cross_val = model_selection.cross_validate(alg, \n                                               data_train.loc[:, ~data_train.columns.isin(['Id','SalePrice'])], \n                                               data_train['SalePrice'],\n                                               scoring = ['neg_mean_squared_error','r2'],\n                                               cv  = cv_splits,\n                                               return_train_score=True,\n                                               return_estimator=False\n                                              )\n    \n    cv_results['Mean Train MSE'].append(cross_val['train_neg_mean_squared_error'].mean())\n    cv_results['Mean Test MSE'].append(cross_val['test_neg_mean_squared_error'].mean())\n    cv_results['Mean Train R2'].append(cross_val['train_r2'].mean())\n    cv_results['Mean Test R2'].append(cross_val['test_r2'].mean())\n    cv_results['Fit Time'].append(cross_val['fit_time'].mean())\n    ","3650ab67":"cv_results_df = pd.DataFrame.from_dict(cv_results)\ncv_results_df.sort_values(by=['Mean Test R2'], inplace=True, ascending=False)\ncv_results_df","3d8a1486":"# store the predictions in a dictionary\ny_predicted = {}","498752c9":"for alg_name,alg in algos.items():\n    \n    alg.fit(data_train.loc[:, ~data_train.columns.isin(['Id','SalePrice'])], data_train['SalePrice'])\n    y_predicted[alg_name] = alg.predict(data_test.loc[:, ~data_test.columns.isin(['Id'])])\n    ","14ccbc93":"# create a dataframe and write to a csv file\nfor alg_name in algos.keys():\n    results_dict = {'Id':data_test['Id'].values.tolist(), 'SalePrice':list(y_predicted[alg_name])}\n    results_df = pd.DataFrame.from_dict(results_dict)\n    #results_df.to_csv(alg_name+'.csv', index=False)","73fbf510":"We will fill NA values in categorical columns with `Mode` and Numerical columns with `Mean` (You can try `Median` too)","4e286a3a":"Test dataset has the same categories which are present in training","b8c15712":"As expected, Sale Price of older houses is less then newer","11a107f1":"### Feature Engineering","c0711c37":"Interesting, there are only two types of utilities present in training dataset when there could be 4 possible values (as per data description sheet). Also the houses with _NoSeWa_ (Electricity and Gas Only) utility seem to be extremely less.  \nLet's see how many houses are there with _NoSeWa_ utility  \nand  \nHow many types of Utilities are present in test dataset","53152f8f":"Assigning the parameters to the algorithms","59eb1256":"From our visualization exercise we already know that-  \nOverallQual, MSZoning, ExterQual, BsmtQual, CentralAir contribute towards determining SalePrice but OverallCond, Utilities, LandContour, SaleType do not  \n\nWe will not consider __Neighborhood__ as it has a lot of catergories (you can try grouping values or use embedding if you don't want to remove this variable - I'll leave this up to you!)  \nOn a side note, I found this answer helpful if in future you are working on a problem and you are required to collapse some of the categorical variables having many levels (_fusion of categories_ is what it's called btw) - https:\/\/stats.stackexchange.com\/a\/237000  \n\n___*Before we move forward,___ Some of the points I have written below may not come obvious to you and you might wonder how I reached to that (deletion\/grouping) conclusion so I should clarify that I had separately visualized all those variables with SalePrice and then arrived at the conclusion whether to remove them from analysis, keep them as it is or keep them but group the categories. This was done to keep this notebook short. If you don't find some of them intutive, please go ahead fork the notebook and analyse them yourself and if you find something wrong with my analysis\/conclusions then please mention it in the comments :)\n\nWe will remove following variables as they do not help with prediction  \n* BldgType\n* We will remove Condition1 and Condition2 because the frequency of categories other than 'Normal' is very low and so this variable would like have an insignificant affect on SalePrice\n* ExterCond - we already have ExterQual which seem to be more correlated with sale price\n* Exterior1st and Exterior2nd\n* Foundation, Functional (frequency of some categories very low), LotConfig, LandSlope, LotShape, MasVnrType, MoSold, PavedDrive, RoofMatl (frequency of categories other than 'CompShg' is very low), RoofStyle, Street (frequency of 'Gravel' category is too low)\n\nFollowing variables are related, we will either remove some of them, group them or create new features from them-  \n* BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 - We will keep only __BsmtCond__\n* Remove GarageCond, GarageFinish - Keep __GarageQual__, __GarageType__\n* Remove HeatingQC and Keep __Heating__\n* Remove MSSubClass as it looks related to HouseStyle, keep __HouseStyle__\n* Remove SaleCondition as it is correlated with SaleType, keep __SaleType__ (we can keep any of these, I just tend to like SaleType more)\n\n\nFinally, we will level down (grouping\/fuse categories) these categorical variables-  \n* ___Electrical___ - group categories other than SBrkr (Standard Circuit Breakers & Romex) into one category\n* ___KitchenQual___ -  group 'Fa' and 'TA' category into only 1 ('Fa')\n* ___SaleType___ - in New (New) and Old (all other categories) house\n* ___GarageQual___ - Group into 'Good' (Ex, TA, Gd) and 'Poor' (Fa, Po)\n* ___GarageType___ - Group in Attached (Attchd, BuiltIn, Basment, 2Types) and Detached (Detchd, CarPort, )\n* ___Heating___ - Group - Gas (GasA, GasW) and Not-Gas (Floor, Wall, Grav, OthW)\n* ___HouseStyle___ - Group in 'Finished' (except 1.5Unf, 2.5Unf) and 'Unfinished' (1.5Unf, 2.5Unf)\n\n**To Summarize, we will remove below variables**  \nOverallCond, Utilities, LandContour, SaleCondition, Neighborhood, BldgType, Condition1, Condition2, ExterCond, Exterior1st, Exterior2nd, Foundation, Functional, LotConfig, LandSlope, LotShape, MasVnrType, MoSold, PavedDrive, RoofMatl, RoofStyle, Street, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageCond, GarageFinish, HeatingQC, MSSubClass\n\n**And now we are only left with these categorical variables-**  \n_'BsmtQual', 'CentralAir', 'Electrical', 'ExterQual', 'GarageQual', 'GarageType', 'Heating', 'HouseStyle', 'KitchenQual', 'MSZoning', 'OverallQual', 'SaleType'_  \n\nPhew! This was a very lengthy and time consuming exercise ","10540316":"Training dataset has 1460 records and test has almost equal 1459 records  \nThere are 80 features in the dataset","886164f7":"Setting values for hyper-parameters, these are manually adjusted but you can also use GridSearchCV for finding best combination of hyper parameter values","38b67df6":"### Predictions","3353018d":"Gradient Boosting regressor seems to perform best followed by MLP and Lasso  \nI believe Hyper-parameters can be further tuned to get better results","900cc95a":"Hmm.. what if we club categories further?","541e44cd":"The two right-most points (houses) seems like a outlier in the dataset (as suspected earlier). Their Ground living area (GrLivAre) is between 0-2000 square feet only and are still sold for way higher price.  \nPoints with very high lot area but less SalePrice seem like an outlier too but they might be Agriculture type (MSZoning) plots and so priced lower. It's just an assumption and could be incorrect.  \n\nWe will remove these upon further inspection.","9d9f4581":"How LotArea _(Lot size in square feet)_ is related with Sale Price","5caa3e77":"Does number of baths affect the SalePrice?","f9d91d2b":"Only 1 record with 'NoSeWa' utility","eed6f709":"Features having Medium to High Correlation with SalePrice which we missed -  \n`TotalBsmtSF` (highly correlated with 1stFlrSF), `FullBath`, `GarageCars` (highly co-realted with GarageArea)  \n\nCorrelated features -  \n* GarageYrBuilt with YearBuilt - We will keep YearBuilt as GarageYrBuilt has some null values\n* 1stFlrSF with TotalBsmtSF\n* TotalRmsAbvGrd with GrLivArea\n* TotalRmsAbvGrd with BedroomAbvGr but BedroomAbvGr is not correlated with SalePrice so we will not use this feature\nWe will which feature's correlation is higher with SalePrice and then remove other correlated feature","6c917804":"MSZoning - zoning classification (Agriculture, Commercial, Industrial, Residential) - seem to influence the house prices.  \nFV (Floating Village Residential) were sold for higher median price as compard to others. (May be they were bigger house?)  ","a3e7d943":"* MLP Regressor worked best on Kaggle test dataset (scored 0.15191)\n* GridSearchCV can be used to search for best parameters\n* We didn't check for ___normality___ of observed and dependent variables, in some studies it is mentioned that making your independent variables follow normal distribution does not have any significant impact on results but it is dataset dependent. We can run the model after taking log transform of variables (normal distibution) and check the score then to see if it improves\n* We already have run multiple algorithms, we can now do __stacking__ (mix of models, ensamble) to improve score further\n* Another important to do is to draw histogram of errors, check for __Heteroscedasticity__ and improve the model","bffe7794":"We will now visalize the relationship between target variable and features.  \nSince there are lot of features in this dataset so from my intution (which we will later see how wrong or correct is) I have ordered features as per their importance. We will start with the ones which I believe people consider as a major factor in deciding price of a house before buying  \n","fc997467":"What is min and max SalePrice ?","110856f2":"**Fuse Categories**","4c3a42f4":"Looping over each algorithm  \nNotice that for scoring we are using _neg_mean_squared_error, r2_","13b4049b":"**Let's begin!**","74733aae":"**Deleting features which are not required**","e758668e":"**Standardizing Numerical Features**","902ace74":"**Label\/One Hot Encoding**","71342d39":"Maximum price is $755000 which is way higher than the mean and 75th percentile price. I suspect some outliers in the data.  \nLet's move on to seeing how many nulls are there in the dataset","419ae97c":"**Creating new features**","ce34827c":"**Remove Outliers**  \nAs we saw in TotalSF vs SalePrice data that two houses have a very high Surface area but are priced quite low. We will remove points from our dataset","38bb2736":"### Data Exploration and Visualization","3e28f90a":"### Data Modeling","da7a16c4":"I'll continue working on improving this kernel as and when I get time  \nPlease upvote if you found it helpful (so that it reaches maximum people in need) and comment for any clarifications you need or you found something wrong in the kernel","cb30b434":"Remember, we will need to do the same for test dataset too","a45318d1":"Creating a python dictionary to hold the results","17142389":"There is no clear trend but the median selling price of newer houses is _generally_ higher than older ones","9a062110":"Again the same _5_ columns have a lot of null value","646f4b62":"HLS - Hillside - houses seemed to be priced bit higher, there is no high correlation though","c2124f07":"Now, Let's see how the average selling price varies with year they got built in","1675338f":"We will gently move towards Feature Engineering now and we will explore, update, eliminate some of the categorical features","15eec5df":"Those two houses with highest prices seem to follow the trend and do not seem like a complete outlier anymore. We will keep them in our dataset.  \n    Althouh we can see points with a lot high Surface area but lesser Sale Price. **Are they outliers?**","2ab0522b":"Let's first note down the columns we will remove before regression modeling (this is by using the knowledge we gained from data visualization we did just a while ago)","b2e217a5":"Now, Let's check how Sale Price varies with `TotRmsAbvGrd` - Total rooms above grade","3bc0d979":"Varies as the number of baths increase. Interesting to see that there are houses with 0 Baths too!","0a94e6be":"data_test also seems to have only one value for utility, we are fine!  \n  \nThis also tells us that `Utilities` feature is not at all important (in our dataset) in determining Sale Price of house","58c48c89":"Now, we need to find out useful categorical columns from below  \n\n_BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSSubClass, MSZoning, MasVnrType, MoSold, Neighborhood, OverallQual, PavedDrive, RoofMatl, RoofStyle, SaleCondition, Street_","870ca3c5":"Exterial Quality vs Sale Price  \n`Gd` -> Good  \n`TA` -> Average\/Typical  \n`Ex` -> Excellent  \n`Fa` -> Fair  \nThis expected and Self explanatory","8b1de938":"#### Importing required libraries","c5ee5923":"Contents-\n1. [Importing modules and data](#Importing-required-libraries)\n2. [Data Exploration and Visualization](#Data-Exploration-and-Visualization)\n3. [Feature Engineering](#Feature-Engineering)\n4. [Data Modeling](#Data-Modeling)\n5. [Predictions](#Predictions)\n6. [Final thoughts!](#Final-thoughts!)","8b1a3e1a":"### House Prices: Advanced Regression Techniques\nIn this competition we are provided with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, and we are challenged to predict the final price of each home  \n\nThis I believe is one of the best competition for Data Science beginners to learn Regression techniques, EDA, feature engineering and feature selection.  \n\nWithout much talk, let's get started!","f10b2928":"<u>Steps:-<\/u>\n* We will use K-Fold (for training, cross-validation)\n* We will train using _7_ most common algorithms and compare the test and train accuracy and their fit time-\n>- Linear Regression \n>- Ridge Regression\n>- Lasso Regression\n>- K-Neighbours\n>- MLPRegressor (neural network)\n>- Support vector machines\n>- GradientBoostingRegressor\n  \nYou can experiment with other algorithms of your choice in the similar way.","0172f18a":"**Observation -** Generally, Sale Price seems to be increasing as Total Rooms in House increases but no linear or constantly increasing relationship","d151c75f":"All NA values in train and test datasets are filled now","88723a43":"Out of curiosity, how many MSZoning categories are there in test dataset?","a5687ec9":"So we can see the relation quite clear now. Newer houses are priced higher as compared to old.","901076a7":"We will Standardize only columns which have too min\/max values quite high when compared to others. We won't standardize columns having max values = 1,2, .. 5 etc.","2b6fc902":"So the `Total Surface Area` vs `MSZoning` plot looks similar to `SalePrice` vs `MSZoning` plot.  \nMedian TotalSF in FV Zone is higher compared to others which explains higher sale prices of houses in that area.","aabf7690":"**Let's fill the NA values**","bc552cdc":"Now we need to one hot encode or label encode some of the categorical variable (why just some and not all? We converted some of them 0 and 1 already while _fusing_ categories)  \n\nWe only need to convert these categorical variables - _BsmtQual, CentralAir, ExterQual, KitchenQual, MSZoning, OverallQual, AgeAtSale, RemodAgeAtSale_\n\nWe will leave _OverallQual_ as it is, One Hot encode _MSZoning, AgeAtSale and RemodAgeAtSale_ and Label encode remaining variables  ","a8840ba4":"We will now encode convert categorical variable and check their correlation with SalePrice","1b0f95d6":"**Ok, let's move the next steps. We will now do the following**\n- Remove Outliers\n- Fill NA values\n- Create new features\n- Delete features which are not required for analysis\n- Fuse categories in some categorical features\n- Do OneHotEncoding for Categorical Features\n- Standardize Numerical Features","f3c25ae4":"We can see that 'YearBuilt', 'GrLivArea', 'GarageCars' have higher correlation with SalePrice as compared to 'GarageYrBlt', 'TotRmsAbvGrd', 'GarageArea' respectively so we will keep 'YearBuilt', 'GrLivArea', 'GarageCars' features.  \n1stFlrSF with TotalBsmtSF are equally correlated with SalePrice so we can keep any one of them. I'll keep 1stFlrSF","9b7a307c":"### Final thoughts!","ae4cb39a":"Columns - LotFrontage, Alley, FireplaceQu, PoolQC, Fence, MiscFeature have a lot of null values (> 20 %). Let's see what are these columns  \n\n`Alley` - Type of alley access to property - 3 unique values  \n`FireplaceQu` - Fireplace quality - 6 unique values  \n`PoolQC` - Pool quality - 5 unique values  \n`Fence` - Fence quality - 5 unique values  \n`MiscFeature` - Miscellaneous feature not covered in other categories - 6 unique values  \n\nWe will not use these features in our analysis\n  \nLet's also check _test_ dataset","f9019c72":"**Oservations:-**  \n\nFrom separate visuals-\n* SalePrice increases exponentially with increase in LotArea\n* SalePrice seems to be increasing linearly with TotalSF (1stFlrSF + 2ndFlrSF), though the variance in as SF increases\n* As Total rooms above ground increases, median Sale Price increases. Not quite true when rooms = 12 though\n* SalePrice increases exponentially as Overall Quality of house increases but the relation is not so strong with Overall Condition (OverallCond) of house\n* We do not have enough data to see relationship with Utility\n* Price of newer houses (YearBuilt) tend to be more than old. We can see this relationship using SaleType variable too.\n* Houses having central air conditioning are priced higher  \n\nFrom HeatMap-  \n* LotFrontage has ~17% null values and isn't highly correlated (correlation - 0.35) with SalePrice so we will remove it\n* _OverallCond, BsmtFinSF2, BsmtUnfSF, LowQualFinSF, BsmtFullBath, BsmtHalfBath, HalfBath, BedroomAbvGrd, KitchenAbvGrd, WoodDeckSF, OpenPorchSF, EncliosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal_ are not significantly correlated with SalePrice and hence we would not include them in modeling\n* We will not keep 'TotalSF' as it is exactly same as GrLivArea (Huh! didn't knew that before)\n* We will keep GrLivArea and remove '2ndFlrSF' to avoid _collinearity_\n* We will keep feature 'TotalBaths' as it is also quite correlated to SalePrice\n* MSSubclass is actually categorical variable but because it has numeric values it appeared in Heatmap\n* OverallCond is a categorical variable too but in our case we would be just fine if we treat it as numeric too\n* MoSold, YrSold are significantly correlated too and we don't expect MoSold (Month in which house was sold) to be correlated with SalePrice either. We will create a Age feature first and then can get rid of these.\n* We are yet to see correlation with some of the categorical features which we missed before and not included on HeatMap","0e63fed5":"Columns having nulls in training dataset - ['GarageType', 'MasVnrArea', 'BsmtQual', 'Electrical', 'GarageQual']  \nColumns having nulls in test dataset - ['KitchenQual', 'GarageType', 'MSZoning', 'MasVnrArea', 'BsmtQual', 'SaleType', 'BsmtFinSF1', 'GarageCars', 'GarageQual']","bb9afae4":"SalePrice directly (non-linearly) proportional to Overall Quality of House","dc574314":"So to summarize - we will keep only these numerical columns in our regression model  \n\n___'1stFlrSF', 'BedroomAbvGr', 'BsmtFinSF1', 'EnclosedPorch', 'Fireplaces', 'FullBath','TotalBaths, 'GarageCars', 'GrLivArea', 'KitchenAbvGr', 'LotArea', 'MasVnrArea', 'SalePrice', 'YearBuilt', 'YearRemodAdd', 'YrSold', 'MoSold'___  \n\nTo note- we will remove MoSold and YrSold feature after we create House Age feature. We will create an additional categorical feature from _2ndFlrSF_ variable - if it is zero then single story house else double story house  \nWe will create a House Age before sold After remodeling from YearRemodAdd and then remove YearRemodAdd variable  ","f5eac93f":"let's check the shape of data_train to confirm that they are removed"}}