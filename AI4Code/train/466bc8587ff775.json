{"cell_type":{"fac9f81c":"code","cc3962f6":"code","06667407":"code","e7378944":"code","018d0352":"code","147e803f":"code","1edd88ff":"code","de309dbd":"code","e3d8f14c":"code","874de4b4":"code","d4249b42":"code","f18f4f70":"code","14aafacd":"code","83891dec":"code","139ad0f8":"code","e2091b69":"code","423cfbca":"code","d863c0a9":"code","6aaabd0d":"code","50c42d0b":"code","1cda9c6f":"code","00a9417d":"code","c2f33a72":"code","89e8f26a":"code","805f00ba":"code","0e206d02":"code","9c6dd5c8":"code","5b4d0ec2":"code","d83071e0":"code","fe4387e2":"code","d38ed71b":"markdown","1ffd2ba9":"markdown","d7466a99":"markdown","6bee443c":"markdown","bd1fe09a":"markdown","a7b04721":"markdown","7e7aeea2":"markdown","8e7b7049":"markdown","5f49d990":"markdown","3ef8fedb":"markdown","eeddf0dd":"markdown","296b2168":"markdown"},"source":{"fac9f81c":"from distutils.dir_util import copy_tree\nimport os\nfrom pathlib import Path\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Dense, Flatten\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, \n    precision_score, recall_score, \n    roc_auc_score, roc_curve,\n    confusion_matrix\n)\nfrom mpl_toolkits.axes_grid1 import ImageGrid","cc3962f6":"# define hardcoded values used throughout this notebook\nBACTERIA_TXT = 'BACTERIA'\nVIRUS_TXT = 'VIRUS'\nVALIDATION_SET_FRAC = 0.2\nIMAGE_SIZE = [244, 244]\nBATCH_SIZE = 128\nEPOCHS = 2\nSEED = 14\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","06667407":"# make sure the val directory is empty, because it will be created when \n#  this notebook is run end to end, which would cause an error if it \n#  already exists\n! rm -R \/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/val","e7378944":"# copy everything to the working directory so that \n#   a new validation folder can be created, and files can be copied \n#   to it without errors\nfrom_dir = \"\/kaggle\/input\"\nto_dir = \"\/kaggle\/working\"\n\ncopy_tree(from_dir, to_dir)","018d0352":"# get the file names in each directory, and count how many files there are\n\ndef only_images(ls):\n    return [f for f in ls if f.endswith('.jpeg')]\n\n# get training set files and counts\ntrain_normal_dir = \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/train\/NORMAL\"\ntrain_pneumo_dir = \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/train\/PNEUMONIA\"\n\ntrain_normal_path, train_normal_dirs, train_normal_files = next(os.walk(train_normal_dir))\ntrain_pneumo_path, train_pneumo_dirs, train_pneumo_files = next(os.walk(train_pneumo_dir))\ntrain_normal_files = only_images(train_normal_files)\ntrain_pneumo_files = only_images(train_pneumo_files)\nrandom.shuffle(train_normal_files)\nrandom.shuffle(train_pneumo_files)\noriginal_train_image_count = len(train_normal_files) + len(train_pneumo_files)\n\n# create a validation set\nval_normal_dir = \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/val\/NORMAL\"\nval_pneumo_dir = \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/val\/PNEUMONIA\"\n\nos.mkdir(\"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/val\")\nos.mkdir(val_normal_dir)\nos.mkdir(val_pneumo_dir)\n\ntrain_normal_split_idx = int(np.ceil(VALIDATION_SET_FRAC * len(train_normal_files)))\ntrain_pneumo_split_idx = int(np.ceil(VALIDATION_SET_FRAC * len(train_pneumo_files)))\n\nval_normal_files = train_normal_files[-train_normal_split_idx:]\nval_pneumo_files = train_pneumo_files[-train_pneumo_split_idx:]\n\ntrain_normal_files = train_normal_files[:-train_normal_split_idx]\ntrain_pneumo_files = train_pneumo_files[:-train_pneumo_split_idx]\n\n# move the validation images to the newly created validation folders\nfor file in val_normal_files:\n    Path(train_normal_dir + \"\/\" + file).rename(val_normal_dir + \"\/\" + file)\nfor file in val_pneumo_files:\n    Path(train_pneumo_dir + \"\/\" + file).rename(val_pneumo_dir + \"\/\" + file)\n\ntrain_normal_count = len(train_normal_files)\ntrain_pneumo_count = len(train_pneumo_files)\ntrain_pneumo_bac_files = [f for f in train_pneumo_files if BACTERIA_TXT in f]\ntrain_pneumo_vir_files = [f for f in train_pneumo_files if VIRUS_TXT in f]\ntrain_bacteria_count = len(train_pneumo_bac_files)\ntrain_virus_count = len(train_pneumo_vir_files)\n\n# get validation set files and counts\nval_normal_count = len(val_normal_files)\nval_pneumo_count = len(val_pneumo_files)\nval_pneumo_bac_files = [f for f in val_pneumo_files if BACTERIA_TXT in f]\nval_bacteria_count = len(val_pneumo_bac_files)\nval_pneumo_vir_files = [f for f in val_pneumo_files if VIRUS_TXT in f]\nval_virus_count = len(val_pneumo_vir_files)\n\n# get test set files and counts\ntest_normal_dir = \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/test\/NORMAL\"\ntest_pneumo_dir = \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/test\/PNEUMONIA\"\n\ntest_normal_path, test_normal_dirs, test_normal_files = next(os.walk(test_normal_dir))\ntest_pneumo_path, test_pneumo_dirs, test_pneumo_files = next(os.walk(test_pneumo_dir))\ntest_normal_files = only_images(test_normal_files)\ntest_pneumo_files = only_images(test_pneumo_files)\n\ntest_normal_count = len(test_normal_files)\ntest_pneumo_count = len(test_pneumo_files)\ntest_pneumo_bac_files = [f for f in test_pneumo_files if BACTERIA_TXT in f]\ntest_bacteria_count = len(test_pneumo_bac_files)\ntest_pneumo_vir_files = [f for f in test_pneumo_files if VIRUS_TXT in f]\ntest_virus_count = len(test_pneumo_vir_files)\n\n# inspect\nassert original_train_image_count == train_normal_count + train_pneumo_count + val_normal_count + val_pneumo_count\nprint(f\"Training images: {train_normal_count + train_pneumo_count}\")\nprint(f\"Validation images: {val_normal_count + val_pneumo_count}\")\nprint(f\"Test images: {test_normal_count + test_pneumo_count}\")","147e803f":"# empty list to be filled with tuples of (img_path, pneumonia_label, pneumonia_type_label)\ntrain_data = []\n\n# fill the train_data list with tuples of the file path and labels at 2 levels\nfor file in train_normal_files:\n    train_data.append((train_normal_dir + \"\/\" + file, 0, 0))\nfor file in train_pneumo_bac_files:\n    train_data.append((train_pneumo_dir + \"\/\" + file, 1, 1))\nfor file in train_pneumo_vir_files:\n    train_data.append((train_pneumo_dir + \"\/\" + file, 1, 2))\n\n# turn the list of tuples into a Pandas dataframe\ntrain_data = pd.DataFrame(train_data, columns=['img_path', 'pneumonia_label', 'pneumonia_type_label'], index=None)\n\n# shuffle the data\ntrain_data = train_data.sample(frac=1.).reset_index(drop=True)\n\n# inspect\ntrain_data.head()","1edd88ff":"pneumonia_label_counts = train_data['pneumonia_label'].value_counts()\npneumonia_type_label_counts = train_data['pneumonia_type_label'].value_counts()\n\n# plot the distribution of labels\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=pneumonia_label_counts.index, y=pneumonia_label_counts.values)\nplt.title('Number of Training Samples by Pneumonia Label')\nplt.xlabel('Pneumonia Label')\nplt.ylabel('Number of Samples')\nplt.xticks(range(len(pneumonia_label_counts.index)), ['Normal (0)', 'Pneumonia (1)'])\nplt.show()\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=pneumonia_type_label_counts.index, y=pneumonia_type_label_counts.values)\nplt.title('Number of Training Samples by Pneumonia Type Label')\nplt.xlabel('Pneumonia Type Label')\nplt.ylabel('Number of Samples')\nplt.xticks(range(len(pneumonia_type_label_counts.index)), ['Normal (0)', 'Bacterial Pneumonia (1)', 'Viral Pneumonia (2)'])\nplt.show()","de309dbd":"# empty list to be filled with tuples of (img_path, pneumonia_label, pneumonia_type_label)\nval_data = []\n\n# fill the train_data list with tuples of the file path and labels at 2 levels\nfor file in val_normal_files:\n    val_data.append((val_normal_dir + \"\/\" + file, 0, 0))\nfor file in val_pneumo_bac_files:\n    val_data.append((val_pneumo_dir + \"\/\" + file, 1, 1))\nfor file in val_pneumo_vir_files:\n    val_data.append((val_pneumo_dir + \"\/\" + file, 1, 2))\n\n# turn the list of tuples into a Pandas dataframe\nval_data = pd.DataFrame(val_data, columns=['img_path', 'pneumonia_label', 'pneumonia_type_label'], index=None)\n\n# shuffle the data\nval_data = val_data.sample(frac=1.).reset_index(drop=True)\n\n# inspect\nval_data.head()","e3d8f14c":"pneumonia_label_counts = val_data['pneumonia_label'].value_counts()\npneumonia_type_label_counts = val_data['pneumonia_type_label'].value_counts()\n\n# plot the distribution of labels\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=pneumonia_label_counts.index, y=pneumonia_label_counts.values)\nplt.title('Number of Validation Samples by Pneumonia Label')\nplt.xlabel('Pneumonia Label')\nplt.ylabel('Number of Samples')\nplt.xticks(range(len(pneumonia_label_counts.index)), ['Normal (0)', 'Pneumonia (1)'])\nplt.show()\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=pneumonia_type_label_counts.index, y=pneumonia_type_label_counts.values)\nplt.title('Number of Validation Samples by Pneumonia Type Label')\nplt.xlabel('Pneumonia Type Label')\nplt.ylabel('Number of Samples')\nplt.xticks(range(len(pneumonia_type_label_counts.index)), ['Normal (0)', 'Bacterial Pneumonia (1)', 'Viral Pneumonia (2)'])\nplt.show()","874de4b4":"# empty list to be filled with tuples of (img_path, pneumonia_label, pneumonia_type_label)\ntest_data = []\n\nfor file in test_normal_files:\n    test_data.append((test_normal_path + \"\/\" + file, 0, 0))\nfor file in test_pneumo_bac_files:\n    test_data.append((test_pneumo_path + \"\/\" + file, 1, 1))\nfor file in test_pneumo_vir_files:\n    test_data.append((test_pneumo_path + \"\/\" + file, 1, 2))\n\n# turn the list of tuples into a Pandas dataframe\ntest_data = pd.DataFrame(test_data, columns=['img_path', 'pneumonia_label', 'pneumonia_type_label'], index=None)\n\n# shuffle the data\ntest_data = test_data.sample(frac=1.).reset_index(drop=True)\n\n# make sure the dataframe accounts for all desired files\nassert len(test_data) == test_normal_count + test_pneumo_count\nassert len(test_data[test_data['pneumonia_type_label'] == 0]) == test_normal_count\nassert len(test_data[test_data['pneumonia_type_label'] == 1]) == test_bacteria_count\nassert len(test_data[test_data['pneumonia_type_label'] == 2]) == test_virus_count\n\n# inspect\nprint(f\"Number of test samples: {len(test_data)}\")\ntest_data.head()","d4249b42":"pneumonia_label_counts = test_data['pneumonia_label'].value_counts()\npneumonia_type_label_counts = test_data['pneumonia_type_label'].value_counts()\n\n# plot the distribution of labels\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=pneumonia_label_counts.index, y=pneumonia_label_counts.values)\nplt.title('Number of Test Samples by Pneumonia Label')\nplt.xlabel('Pneumonia Label')\nplt.ylabel('Number of Samples')\nplt.xticks(range(len(pneumonia_label_counts.index)), ['Normal (0)', 'Pneumonia (1)'])\nplt.show()\n\nplt.figure(figsize=(10,8))\nsns.barplot(x=pneumonia_type_label_counts.index, y=pneumonia_type_label_counts.values)\nplt.title('Number of Test Samples by Pneumonia Type Label')\nplt.xlabel('Pneumonia Type Label')\nplt.ylabel('Number of Samples')\nplt.xticks(range(len(pneumonia_type_label_counts.index)), ['Normal (0)', 'Bacterial Pneumonia (1)', 'Viral Pneumonia (2)'])\nplt.show()","f18f4f70":"# show the pixel dimensions of the first 10 training samples (recall that they have been shuffled)\nsample_images = []\nfor i in train_data[:10]['img_path'].to_list():\n    img = cv2.imread(i)\n    print(img.shape)\n    sample_images.append(img)\n    \n# visually inspect the images with their type label\nfig, axs = plt.subplots(nrows=2, ncols=5, figsize=(30, 10))\nfor idx, img in enumerate(sample_images):\n    axs[idx\/\/5, idx%5].imshow(img, cmap='gray')\n    axs[idx\/\/5, idx%5].axis('off')\n    axs[idx\/\/5, idx%5].set_title(\"Pneumonia Type Label \" + str(train_data.loc[idx, 'pneumonia_type_label']))\nplt.show()","14aafacd":"# inspect pixel value range (should be 0-255)\nimg = cv2.imread(train_data.loc[0, 'img_path'])\nprint(\"Pixel value range:\", np.min(img), np.max(img))","83891dec":"def resize_image(img, size=(28,28)):\n    \"\"\"\n    Resizes an image to the given size.  \n    Code inspired by: https:\/\/stackoverflow.com\/questions\/44650888\/resize-an-image-without-distortion-opencv\n    \"\"\"\n    # get the image's original height and width\n    h, w = img.shape[:2]\n    \n    # get the image's original number of color channels\n    c = img.shape[2] if len(img.shape) > 2 else 1\n\n    # If the image is square already, simply resize it.\n    #   Otherwise find which is longer (h or w), and create a square \n    #   image of zeros with the length equal to the larger dimension.\n    #   Then paste the image into the square and resize the new image.\n    if h == w: \n        return cv2.resize(img, size, cv2.INTER_AREA)\n    else:\n        # find the longer dimension\n        longer_dim = h if h > w else w\n\n        # Determine which interpolation method to use.  If the longer \n        #   dimension is larger than half the sum of the new image's height and width, \n        #   then use INTER_AREA.  Otherwise use INTER_CUBIC (bicubic interpolation).\n        #   These methods take longer than others but ensure the least information loss.\n        if longer_dim > (size[0] + size[1]) \/\/ 2:\n            interpolation_method = cv2.INTER_AREA\n        else:\n            interpolation_method = cv2.INTER_CUBIC\n\n        # get the coordinates of the original image\n        x_pos = (longer_dim - w)\/\/2\n        y_pos = (longer_dim - h)\/\/2\n\n        # create an image of zeros, then copy the original\n        #   image into position\n        if len(img.shape) == 2:\n            mask = np.zeros((longer_dim, longer_dim), dtype=img.dtype)\n            mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n        else:\n            mask = np.zeros((longer_dim, longer_dim, c), dtype=img.dtype)\n            mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n\n        return cv2.resize(mask, size, interpolation_method)\n\n\n# test the resizing function on a single image\nimg = cv2.imread(train_data.loc[0, 'img_path'])\nnew_img = resize_image(img=img, size=tuple(IMAGE_SIZE))\nplt.imshow(img, cmap='gray')\nplt.show()\nplt.imshow(new_img, cmap='gray')\nplt.show()","139ad0f8":"# specify a pre-trained ResNet model and freeze the weights\nrn = ResNet50(\n    include_top=False,\n    weights='imagenet',  # image net weights are a good starting point\n    input_shape=IMAGE_SIZE + [3],\n    pooling=None,  # output will be a 4D tensor from the last convolutional block\n)\nrn.trainable = False\n\n# Keras image data generator returns classes one-hot encoded, \n#   so it needs to be mapped to the feature vectors from ResNet50's \n#   output (the 4D tensor from the last convolutional block).  This will \n#   ensure that the final layers can be trained on the new data, \n#   while the lower layers can remain as they are.\nnbr_classes = len(set(train_data['pneumonia_label']))\nx = Flatten()(rn.output)\nx = Dense(nbr_classes, activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=rn.input, outputs=x)\n\nmodel.summary()","e2091b69":"# create an instance of ImageDataGenerator\ngen_train = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.2,\n    brightness_range=(0.3, 1.0),\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest',\n    preprocessing_function=preprocess_input\n)\n\ngen_val = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\ngen_test = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n# create generators\ntrain_generator = gen_train.flow_from_directory(\n    \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/train\",\n    shuffle=True,\n    seed=SEED,\n    target_size=IMAGE_SIZE,\n    interpolation='bicubic',\n    batch_size=BATCH_SIZE,\n)\nvalid_generator = gen_val.flow_from_directory(\n    \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/val\",\n    target_size=IMAGE_SIZE,\n    interpolation='bicubic',\n    batch_size=BATCH_SIZE,\n)","423cfbca":"model.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nr = model.fit_generator(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=EPOCHS,\n    steps_per_epoch=int(np.ceil(len(train_normal_files+train_pneumo_files) \/ BATCH_SIZE)),\n    validation_steps=int(np.ceil(len(val_normal_files+val_pneumo_files) \/ BATCH_SIZE)),\n)","d863c0a9":"# plot the training loss\nplt.plot(r.history['loss'], label='Training Loss')\nplt.plot(r.history['val_loss'], label='Validation Loss')\nplt.title(\"Loss by Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","6aaabd0d":"# plot the training accuracy\nplt.plot(r.history['accuracy'], label='Training Accuracy')\nplt.plot(r.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"Classification Accuracy by Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","50c42d0b":"# create the test set generator that does not use data augmentation\ntest_generator = gen_test.flow_from_directory(\n    \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/test\",\n    shuffle=False,  # do not shuffle so the predictions can easily be mapped\n    class_mode=None,  # only get data, no labels\n    target_size=IMAGE_SIZE,\n    interpolation='bicubic',\n    batch_size=BATCH_SIZE,\n)\n\n# evaluate the model on the test set, without any augmented samples\npred_probs = model.predict_generator(\n    test_generator,\n    steps=int(np.ceil(len(test_normal_files+test_pneumo_files) \/ BATCH_SIZE))\n)\n\n# do the same thing for the training set\ntrain_generator2 = gen_train.flow_from_directory(\n    \"\/kaggle\/working\/labeled-chest-xray-images\/chest_xray\/train\",\n    shuffle=False,  # do not shuffle so the predictions can easily be mapped\n    class_mode=None,  # only get data, no labels\n    target_size=IMAGE_SIZE,\n    interpolation='bicubic',\n    batch_size=BATCH_SIZE,\n)\ntrain_pred_probs = model.predict_generator(\n    train_generator2,\n    steps=int(np.ceil(len(train_normal_files+train_pneumo_files) \/ BATCH_SIZE))\n)\n\npred_probs.shape","1cda9c6f":"# make sure the predicted labels are what is expected (i.e. make sure NORAML != 1)\ntest_generator.class_indices","00a9417d":"# prepare a dataframe with the predictions\ntrain_preds = np.argmax(train_pred_probs, axis=1)\npreds = np.argmax(pred_probs, axis=1)\n\ntrain_preds_df = pd.DataFrame({\n    'img_name': [f[f.rfind(\"\/\")+1:] for f in train_generator.filenames],\n    'prediction_prob': train_pred_probs[:, 1],\n    'prediction': train_preds\n})\n\ntest_preds_df = pd.DataFrame({\n    'img_name': [f[f.rfind(\"\/\")+1:] for f in test_generator.filenames],\n    'prediction_prob': pred_probs[:, 1],\n    'prediction': preds\n})\n\n# combine predictions with dataframes\ntrain_data['img_name'] = train_data['img_path'].apply(lambda x: x[x.rfind('\/')+1:])\ntrain_data = pd.merge(train_data, train_preds_df, how='left', on='img_name')\n\ntest_data['img_name'] = test_data['img_path'].apply(lambda x: x[x.rfind('\/')+1:])\ntest_data = pd.merge(test_data, test_preds_df, how='left', on='img_name')\ntest_data.head()","c2f33a72":"# print evaluation\nprint(\"Test Set Metrics\")\nprint(\"Accuracy:\", accuracy_score(test_data['pneumonia_label'], test_data['prediction']))\nprint(\"F1 Score:\", f1_score(test_data['pneumonia_label'], test_data['prediction']))\nprint(\"Precision:\", precision_score(test_data['pneumonia_label'], test_data['prediction']))\nprint(\"Recall:\", recall_score(test_data['pneumonia_label'], test_data['prediction']))\nprint(\"ROC AUC:\", roc_auc_score(test_data['pneumonia_label'], test_data['prediction']))\nprint(\"Confusion Matrix Format: \\n[TN, FP]\\n[FN, TP]\")\nprint(confusion_matrix(test_data['pneumonia_label'], test_data['prediction']))","89e8f26a":"accuracy_score(train_data['pneumonia_label'], train_data['prediction'])","805f00ba":"# plot the ROC curve\ntrain_fpr, train_tpr, train_thresholds = roc_curve(train_data['pneumonia_label'], train_data['prediction_prob'])\nfpr, tpr, thresholds = roc_curve(test_data['pneumonia_label'], test_data['prediction_prob'])\n\nplt.plot(\n    train_fpr, train_tpr, color='darkorange', \n    label=f\"Train ROC with AUC = {round(roc_auc_score(train_data['pneumonia_label'], train_data['prediction']), 2)}\"\n)\nplt.plot(\n    fpr, tpr, color='green', \n    label=f\"Test ROC with AUC = {round(roc_auc_score(test_data['pneumonia_label'], test_data['prediction']), 2)}\"\n)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","0e206d02":"# select layers to visualize the weights of\n# skip the input layer and zero padding, and stop before the flattening\n# note that the weights are from imagenet, so this will just explore imagenet weights\nlayers_to_visualize = [layer.output for layer in model.layers[2:-2]]\nlayer_names_to_visualize = [layer.name.split(\"\/\")[0] for layer in layers_to_visualize]\n\n# define a new model to generate the layers to visualize as outputs\nvis_model = Model(model.input, layers_to_visualize)\n\nprint(\"Layers to visualize:\", layer_names_to_visualize)","9c6dd5c8":"# define a class for gradient class activation mapping\n\nclass GradCAM:\n    def __init__(self, model, class_idx, layer_name=None):\n        \"\"\"\n        Initialize with params.  If no layer_name is provided, the \n        object will initialize with the final convoluational layer \n        in the network.  If the provided network uses a softmax \n        activation function in the final layer, the function is \n        reversed so that the gradients returned are not all zeros.\n        \n        :params:\n            model (TF model): the TensorFlow classification model with \n                convolutional layers\n            class_idx (int): the index of the class to be used for \n                activation mapping (usually the target class)\n            layer_name (str): the name of the convolutional layer to \n                get the class activation heatmap for\n        \"\"\"\n        if model.layers[-1].activation == tf.keras.activations.softmax:\n            self.model = self._reverse_softmax(model=model)\n        else:\n            self.model = model\n        self.class_idx = class_idx\n        self.layer_name = layer_name\n        if self.layer_name is None:\n            self.layer_name = self._find_final_conv_layer()\n    \n    def _reverse_softmax(self, model):\n        \"\"\"\n        Reverses the softmax activation function so that the logits are \n        return instead.  This prevents the gradients from being zero (they \n        have to be, because softmax always sums to 1), which would cause \n        the heatmap returned by this class to be all zeros.\n        \"\"\"\n        # store the final layer config and weights\n        config = model.layers[-1].get_config()\n        weights = [x.numpy() for x in model.layers[-1].weights]\n\n        # set the final layer's activation to linear, as this \n        #   essentially performs no transformation and is just the identity\n        # also rename this layer 'logits', because that is what would \n        #   otherwise be passed to the softmax activation function\n        config['activation'] = tf.keras.activations.linear\n        config['name'] = 'logits'\n        \n        # return the new model with the final layer as a linear activation, \n        #   using the same weights and config as the original\n        new_layer = tf.keras.layers.Dense(**config)(model.layers[-2].output)\n        new_model = tf.keras.Model(inputs=[model.input], outputs=[new_layer])\n        new_model.layers[-1].set_weights(weights)\n        \n        return new_model\n\n    def _find_final_conv_layer(self):\n        \"\"\"\n        Finds the final convolutional layer in the network by looping \n        over the layers, starting from the final one.\n        \"\"\"\n        for layer in reversed(self.model.layers):\n            # check to see if the layer has a 4D output\n            if len(layer.output_shape) == 4:\n                return layer.name\n        raise ValueError(\n            \"Could not find 4D convolutional layer. Cannot apply GradCAM.\"\n        )\n\n    def compute_heatmap(self, image, eps=1e-16):\n        \"\"\"\n        Computes the class activation heatmap for a layer.  \n        \n        To get the last convolutional layer's gradients, we first need to \n        create a model that maps the model input to the last convolutional \n        layer's weights, and the model's class prediction output.  That is \n        the grad_model defined in this method.  \n        \n        With the grad_model, the loss can be calculated and passed to the \n        gradient tape (the history of all gradients from TF's eager \n        execution).  The gradient tape can be used to find the last \n        convolutional layer's gradients.\n        \n        The conv output values can be multiplied by the gradients to \n        determine which feature maps in the layer are activating.  This \n        gives a sort of \"feature importance\" to the feature maps from the \n        conv layer.  These maps are called the guided_grads in this method.\n        \n        The weights of the guided_grads can be calculated by taking their \n        mean over the first 2 axes, producing a 1x1xN tensor over the feature \n        mapping.  The class activation maps can then be found by summing \n        the weights of the averaged guided_grads with the weighted original \n        convolutional feature maps.\n        \n        :params:\n            image (numpy array): the sample image to visualize the class \n                activation maps for\n            eps (float): small, non-zero number to avoid divide by 0 error \n                during normalization\n        \n        :return:\n            a single channel, grayscale image of where the layer activated \n            for a given image, where larger values correspond to more \n            activations\n        \"\"\"\n        # create the gradient model\n        grad_model = Model(\n            inputs=[self.model.inputs],\n            outputs=[\n                self.model.get_layer(self.layer_name).output,\n                self.model.output\n            ]\n        )\n        \n        # use TF's gradient history to find the final conv layer's gradients\n        with tf.GradientTape() as tape:\n            # cast the image tensor to a float-32 data type\n            inputs = tf.cast(image, tf.float32)\n            # pass the image through the gradient model\n            (conv_outputs, predictions) = grad_model(inputs)\n            # get the loss associated with the specific class index\n            loss = predictions[:, self.class_idx]\n        # use automatic differentiation to compute the gradients\n        grads = tape.gradient(loss, conv_outputs)\n        \n        # normalize the gradients, in case they are too close to 0\n        # grads = tf.Variable([[0.0]])  # uncomment to test this error message\n        if grads.numpy().max() == 0.:\n            raise ValueError(\n                \"Gradients are 0. Cannot apply GradCAM.\"\n            )\n        grads = grads \/ (grads.numpy().max()-grads.numpy().min())\n        \n        # cast all positive conv_outputs and gradients to binary float-32's\n        # this produces tensors of 0's and 1's that are float-32 type\n        cast_conv_outputs = tf.cast(conv_outputs > 0, \"float32\")\n        cast_grads = tf.cast(grads > 0, \"float32\")\n        \n        # multiply the conv outputs by the gradients to create 'guided gradients'\n        # 'guided_grads' will be 0 where the feature map is 0, leaving only the \n        # activated portions of the feature mapping, with their gradients\n        guided_grads = cast_conv_outputs * cast_grads * grads\n        \n        # the convolution and guided gradients have a batch dimension\n        #   that is not needed, so keep the feature maps and delete the batch\n        conv_outputs = conv_outputs[0]\n        guided_grads = guided_grads[0]\n        \n        # compute the class activation map, by summing the averaged guided_grads\n        #   with the weighted original conv feature maps\n        weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n        cam = tf.reduce_sum(tf.multiply(weights, conv_outputs), axis=-1)\n   \n        # get the dimensions of the input image\n        (w, h) = (image.shape[2], image.shape[1])\n        \n        # resize the class activation map to match the dimensions of the input image\n        heatmap = cv2.resize(cam.numpy(), (w, h))\n        \n        # normalize the heatmap to (0,1), then scale the values to range (0, 255)\n        numerator = heatmap - np.min(heatmap)\n        denominator = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numerator \/ denominator\n        heatmap = (heatmap * 255).astype(\"uint8\")\n        \n        return heatmap\n        \n    def overlay_heatmap(\n        self, image, heatmap=None, alpha=0.5, colormap=cv2.COLORMAP_INFERNO\n    ):\n        \"\"\"\n        Overlays the heatmap onto the image, mapping with the given color scheme\n        \n        :params:\n            alpha (float): opacity of the original image, from 0-1\n            colormap (cv2 object): options [MAGMA, INFERNO, PLASMA, VIRIDIS, CIVIDIS]\n        \n        :return:\n            a tuple of the color mapped heatmap and the overlayed image\n        \"\"\"\n        if heatmap is None:\n            # compute the heatmap, expanding the input image to 4D\n            heatmap = self.compute_heatmap(image=np.expand_dims(image, 0))\n        \n        # resize the heatmap to match the image\n        heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n        # apply the color mapping to the heatmap\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        # overlap the heatmap on the image\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n\n        return output","5b4d0ec2":"# produce the class activation map for a few images\ncams = []\nfor idx, i in enumerate(train_data[:10]['img_path'].to_list()):\n    img = cv2.imread(i)\n    img = resize_image(img=img, size=tuple(IMAGE_SIZE))\n    # create the CAM, using the predicted label as the class target\n    gcm = GradCAM(\n        model=model, \n        class_idx=train_data.loc[idx, 'prediction'], \n        layer_name=layer_names_to_visualize[-1]\n    )\n    cam = gcm.overlay_heatmap(image=img, alpha=0.7)\n    cams.append(cam)\n\nfig, axs = plt.subplots(nrows=2, ncols=5, figsize=(30, 10))\nfor idx, img in enumerate(cams):\n    axs[idx\/\/5, idx%5].imshow(img, cmap='gray')\n    axs[idx\/\/5, idx%5].axis('off')\n    axs[idx\/\/5, idx%5].set_title(f\"Class Activation Map, True Label: {str(train_data.loc[idx, 'pneumonia_label'])}, Predicted: {str(train_data.loc[idx, 'prediction'])}\")\nplt.show()","d83071e0":"def normalize_tensor(t):\n    numerator = t - np.min(t)\n    denominator = (t.max() - t.min()) + 1e-16\n    t = numerator \/ denominator\n    t = (t * 255).astype(\"uint8\")\n    return t","fe4387e2":"# get the activations for 1 image (re-use new_img from earlier in this notebook)\nactivations = vis_model.predict(np.expand_dims(new_img, 0))[0]\nnbr_activations = activations.shape[3]\n\n# collect and normalize all the activations\nactivation_imgs = []\nfor a in range(activations.shape[3]):\n    activation = normalize_tensor(activations[0, :, :, a])\n    activation_imgs.append(activation)\n\n# plot the activations in a giant grid\nnbr_cols = 4\nnbr_rows = int(np.ceil(nbr_activations\/nbr_cols))\n\nfig = plt.figure(figsize=(18, 14), dpi=100)\ngrid = ImageGrid(\n    fig, (0, 0, 18, 14),\n    nrows_ncols=(nbr_rows, nbr_cols),\n    axes_pad=0.1,\n)\nfor ax, im in zip(grid, activation_imgs):\n    ax.imshow(im)\nplt.show()","d38ed71b":"### Inspecting the Training Labels","1ffd2ba9":"### Inspecting the Image Attributes\n\nIf the images are of different sizes, they may need to be resized.  Resizing poses the risk of data loss (https:\/\/www.mathworks.com\/matlabcentral\/answers\/122799-what-exactly-happend-when-you-use-the-command-imresize), but there is nothing that can be done about that. ","d7466a99":"The model performs very well.  The test set ROC AUC is slightly lower, but the validation set is larger than the test set, and since its metrics did not plunge over time, the model likely has not overfit too terribly.  The model tends to predict more false positives than false negatives.  When dealing with disease diagnosis, that is a good thing, especially since x-rays are only 1 factor in diagnosing pneumonia.\n\n## Model Interpretation\n\nNow I will apply class activation mapping to visualize which regions of the images triggered each class prediction.  ","6bee443c":"The class below was adapted from Adrian Rosenbrock's excellent CAM tutorial: https:\/\/www.pyimagesearch.com\/2020\/03\/09\/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning\/, with help from StackOverflow when dealing with softmax activation functions in the final layer: https:\/\/stackoverflow.com\/questions\/58544097\/turning-off-softmax-in-tensorflow-models. ","bd1fe09a":"### Inspecting the Validation Lables","a7b04721":"These maps are fascinating.  You can see how the regions of the images that activated the most to trigger each classification are lighter in color.  The darker purple\/blue regions were not as useful.  The pink\/magenta regions were somewhat useful.  The yellow, organe, and white regions that look like they were taken from a topographic map show the areas that were the most useful.  It is interesting to see how the network learned to pay the most attention to the lungs, and that the regions that determine pneumonia versus normal are similar.  \n\nAt this point, I could try to use the CAMs to estimate the progression of pneumonia.  Perhaps the size, opacity, brightness, or some other features of the highlighted areas could indicate how advanced the pneumonia is.  Early detection is important in medicine.  However, without information on disease progression tied to this dataset, it would be impossible to verify how right or wrong I would be.  It would just be speculation.","7e7aeea2":"## Modeling with Data Augmentation\n\nHere I will make use of Tensorflow's data augmentation generator to apply augmentation to each batch.  This is an efficient way to augment the data without having to write the augmentations to disk or store it all in memory at one time.  \n\nFor the model, I have decided to try transfer learning with ResNet 50, using the ImageNet weights.  The ImageNet weights are a good starting point because of the size and diversity of the ImageNet dataset, meaning the network has likely learned features about images that could apply to a vast array of image types.  It is likely that they would work well for the chest x-ray images.  \n\nBut first, I will create an image resizing function, which may come in handy.","8e7b7049":"# Pneumonia Detection from Chest X-Rays\n\nThis dataset contains chest x-rays of 1-5 year old children from the Guangzhou Women and Children\u2019s Medical Center, in Guangzhou, China.  The x-rays show some children with normal lungs and some with pneumonia.\n\nThe dataset is very similar to Kaggle's RNSA Pneumonia Detection Challenge.  According to a popular notebook submitted by Guy Zahavi for that challenge, pneumonia cannot be diagnosed from chest x-rays alone, but an x-ray is an important factor in the diagnosis.  Furthermore, the x-rays do not necessary show pneumonia, but lung opacities that may indicate the presence of pneumonia (https:\/\/www.kaggle.com\/zahaviguy\/what-are-lung-opacities).  \n\nWhile the RNSA challenge focused on image segmentation, this dataset has no bounding boxes to train an image segmenter.  Instead, the dataset is more appropriate for image classification, or perhaps unsupervised image segmentation.  My goals for this notebook are as follows:\n\n1. I will fit an image classifier.  I will start with a CNN, and see how it performs, before trying other methods.  (Spoiler - it performs adequately.)  Rather than designing my own CNN, I will save time by transfer learning ResNet 50, which I picked arbitrarily because I have not used it before.  But I can use the ImageNet weights to give it a head start, and fine tune the final layers.\n2. I will explore the weights and activations of the CNN's inner layers to see what it learns.  More specifically, I want to apply class activation mapping to visualize which parts of an x-ray image are responsible for classifying it as having pneumonia.\n3. Since the dataset is small, I plan to use data augmentation.  But I want to apply augmentation in the most efficient way possible, by using a generator that augments the data on the fly, on a per batch basis.  So each batch of observations will contain augmented samples.  To keep things fast and efficient, I will not save the augmented images.  This will require them to be stored in memory, but the batch size should be small enough that it will not matter.\n\nBefore beginning, I like to ponder the problem that I am solving to determine its worth.  It seems that image segmentation, or identifying the particular areas in an x-ray that show evidence of pneumonia, should be more useful than simply classifying an x-ray as pneumonia or normal.  But I will stick to classification for now.  Maybe I will return to this dataset in the future and try unsupervised image segmentation.  It also seems like early detection of pneumonia would be more worthwhile than simply detecting it.  There is no indication of how advanced the pneumonia is, for these images.  So while I could make estimates about disease progression based on the size, opacity, or some other features of the regions identified by the class activation maps, there would be no way to validate those estimates.  So this notebook will be interesting, but it likely will be of no true value to the field of medicine.  To say that the model I will develop can detect pneumonia in any chest x-ray would be a stretch.  This is simply for my own learning and entertainment.","5f49d990":"## Data Exploration\n\nThe trick to exploring this dataset, since it is too large to fit into memory, is to make use of the file names and inspect a few sample images.  There is no validation set, so I will create one by randomly sampling from the training set.","3ef8fedb":"### Inspecting the Test Labels","eeddf0dd":"These images show the activations for each layer in the network.  It is interesting to see how different layers learned different transformations of the data.  Some layers, for instance, appeared to have learned edge detection, as you can see the shapes of bones.  Others appear to have recognized organs, as those regions are brighter than other parts of the feature map. ","296b2168":"The images appear to be of many different sizes.  They will need to be re-sized to be the same.  I will resize them to 224 x 224 pixels.  The larger ones will be reduced with interpolation, and the smaller ones will simply be zero padded.  I will resize so that aspect ratio is maintained, by zero padding in any dimension < 224 pixels.  That should minimize the reduction in image quality over the dataset.\n\nIt is also very difficult to see any differences between the images with different labels.  If I were to try to label these myself, I would be completely dumbfounded.  Hopefully the model will be better able to spot pneumonia."}}