{"cell_type":{"5f327df6":"code","24d69b98":"code","2c97927b":"code","f4fc36f0":"code","1c41acdd":"code","4ecb469e":"code","e864e7c1":"code","0ac4eab0":"code","5147717c":"code","4ddf8c54":"code","ec2ace34":"code","b393723b":"code","19e96966":"code","de08aa38":"markdown","4f111b78":"markdown","e97d60c1":"markdown","03c7bb69":"markdown","d90e7fb2":"markdown","71c7ad1e":"markdown","8e44c34f":"markdown","04c7ae40":"markdown","bb5acd8e":"markdown"},"source":{"5f327df6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","24d69b98":"import tensorflow as tf\ntf.__version__","2c97927b":"!pip install tensorflow-hub\n!pip install seaborn","f4fc36f0":"#@title Load the Universal Sentence Encoder's TF Hub module\nfrom absl import logging\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\n\nmodule_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\" #@param [\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\", \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\"]\nmodel = hub.load(module_url)\nprint (\"module %s loaded\" % module_url)\ndef embed(input):\n  return model(input)","1c41acdd":"word = \"Elephant\"\nsentence = \"I am a sentence for which I would like to get its embedding.\"\nparagraph = (\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n    \"the more 'diluted' the embedding will be.\")\nmessages = [word, sentence, paragraph]\n\n# Reduce logging output.\nlogging.set_verbosity(logging.ERROR)\n\nmessage_embeddings = embed(messages)\n\nfor i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n  print(\"Message: {}\".format(messages[i]))\n  print(\"Embedding size: {}\".format(len(message_embedding)))\n  message_embedding_snippet = \", \".join(\n      (str(x) for x in message_embedding[:3]))\n  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))","4ecb469e":"import matplotlib.pyplot as plt\n","e864e7c1":"def plot_similarity(labels, features, rotation):\n  corr = np.inner(features, features)\n  sns.set(font_scale=1.2)\n  fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n  \n  g = sns.heatmap(\n      corr,\n      xticklabels=labels,\n      yticklabels=labels,\n      vmin=0,\n      vmax=1,\n      cmap=\"YlOrRd\")\n  g.set_xticklabels(labels, rotation=rotation)\n  g.set_title(\"Semantic Textual Similarity\")\n\ndef run_and_plot(messages_):\n  message_embeddings_ = embed(messages_)\n  plot_similarity(messages_, message_embeddings_, 90)","0ac4eab0":"messages = [\n    # Smartphones\n    \"I like my phone\",\n    \"My phone is not good.\",\n    \"Your cellphone looks great.\",\n\n    # Weather\n    \"Will it snow tomorrow?\",\n    \"Recently a lot of hurricanes have hit the US\",\n    \"Global warming is real\",\n\n    # Food and health\n    \"An apple a day, keeps the doctors away\",\n    \"Eating strawberries is healthy\",\n    \"Is paleo better than keto?\",\n\n    # Asking about age\n    \"How old are you?\",\n    \"what is your age?\",\n    \n    # Dataset given by you related to car\n   'Driver monitoring system',\n 'excessive tire noise',\n 'automatic parking assist',\n 'car door assist',\n 'speed warning system',\n 'forward collision avoidance assist',\n 'simulated car crashes',\n 'speed warning system',\n 'emergency brake assist',\n 'vehicle fleet management',\n 'electric automotive brake systems',\n 'right interior noise solution',\n 'safety & driver assist',\n# Noise\n 'ping-pong is table tennis'\n]\n","5147717c":"run_and_plot(messages)\n","4ddf8c54":"import pandas\nimport scipy\nimport math\nimport csv\n\nsts_dataset = tf.keras.utils.get_file(\n    fname=\"Stsbenchmark.tar.gz\",\n    origin=\"http:\/\/ixa2.si.ehu.es\/stswiki\/images\/4\/48\/Stsbenchmark.tar.gz\",\n    extract=True)\nsts_dev = pandas.read_table(\n    os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"),\n    error_bad_lines=False,\n    skip_blank_lines=True,\n    usecols=[4, 5, 6],\n    names=[\"sim\", \"sent_1\", \"sent_2\"])\nsts_test = pandas.read_table(\n    os.path.join(\n        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n    error_bad_lines=False,\n    quoting=csv.QUOTE_NONE,\n    skip_blank_lines=True,\n    usecols=[4, 5, 6],\n    names=[\"sim\", \"sent_1\", \"sent_2\"])\n# cleanup some NaN values in sts_dev\nsts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]","ec2ace34":"sts_dev","b393723b":"sts_data = sts_dev #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n\ndef run_sts_benchmark(batch):\n  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n  scores = 1.0 - tf.acos(clip_cosine_similarities)\n  \"\"\"Returns the similarity scores\"\"\"\n  return scores\n\ndev_scores = sts_data['sim'].tolist()\nscores = []\nfor batch in np.array_split(sts_data, 10):\n  scores.extend(run_sts_benchmark(batch))\n\npearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\nprint('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n    pearson_correlation[0], pearson_correlation[1]))","19e96966":"sts_data = sts_test #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n\ndef run_sts_benchmark(batch):\n  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n  scores = 1.0 - tf.acos(clip_cosine_similarities)\n  \"\"\"Returns the similarity scores\"\"\"\n  return scores\n\ndev_scores = sts_data['sim'].tolist()\nscores = []\nfor batch in np.array_split(sts_data, 10):\n  scores.extend(run_sts_benchmark(batch))\n\npearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\nprint('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n    pearson_correlation[0], pearson_correlation[1]))","de08aa38":"From the above example we can know that all sentences of n size(0 to n) are encoded into fixed size vector of length of 512.","4f111b78":"# Public dataset","e97d60c1":"# Documentation\nhttps:\/\/tfhub.dev\/google\/universal-sentence-encoder-large","03c7bb69":"An example of Universal Sentence Encoder","d90e7fb2":"The embeddings produced by the Universal Sentence Encoder are approximately normalized. The semantic similarity of two sentences can be trivially computed as the inner product of the encodings.","71c7ad1e":"# Evaluation: STS (Semantic Textual Similarity) Benchmark\nThe STS Benchmark provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. Pearson correlation is then used to evaluate the quality of the machine similarity scores against human judgements.","8e44c34f":"# We can see from above heatmap than similar sentences has form a squares showimg similarity among them.\n","04c7ae40":"I had also entered a noise in last sentences and we can clearly see it does not have any correleation with any sentence.","bb5acd8e":"# Semantic Textual Similarity Task Example"}}