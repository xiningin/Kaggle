{"cell_type":{"a4e72dc8":"code","7a127d59":"code","c805434c":"code","2442e64b":"code","479b5833":"code","428da0b4":"code","96c96a1c":"code","70a58f56":"code","dd4a392c":"code","abbd7353":"code","006af483":"code","7a94e4cf":"code","e2e23fb7":"code","0fce8440":"code","e9043f37":"code","5359b3f5":"markdown","e730f15f":"markdown","e78b3a8e":"markdown","891f7a81":"markdown","7a55f371":"markdown","8096ab9b":"markdown","4c78643d":"markdown","817bc1c8":"markdown","a0303917":"markdown","7ebab15d":"markdown","2db1166e":"markdown","8223c81c":"markdown","d3a53224":"markdown","a90c3339":"markdown","1a871c6b":"markdown","852a6590":"markdown","5d58c9b8":"markdown"},"source":{"a4e72dc8":"import torch\nfrom torch import nn\n\nimport math\nimport matplotlib.pyplot as plt","7a127d59":"torch.manual_seed(111)","c805434c":"train_data_length = 1024\ntrain_data = torch.zeros((train_data_length, 2))\ntrain_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\ntrain_data[:, 1] = torch.sin(train_data[:, 0])\ntrain_labels = torch.zeros(train_data_length)\ntrain_set = [\n    (train_data[i], train_labels[i]) for i in range(train_data_length)\n]","2442e64b":"plt.plot(train_data[:, 0], train_data[:, 1], \".\")","479b5833":"train_data_length = 1024\ntrain_data = torch.zeros((train_data_length, 2))\ntrain_data[:, 0] = 2 * math.pi * torch.rand(train_data_length)\ntrain_data[:, 1] = torch.sin(train_data[:, 0])\ntrain_labels = torch.zeros(train_data_length)\ntrain_set = [\n    (train_data[i], train_labels[i]) for i in range(train_data_length)\n]","428da0b4":"plt.plot(train_data[:, 0], train_data[:, 1], \".\")","96c96a1c":"batch_size = 32\ntrain_loader = torch.utils.data.DataLoader(\n    train_set, batch_size=batch_size, shuffle=True\n)","70a58f56":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(2, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output","dd4a392c":"discriminator = Discriminator()","abbd7353":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, 2),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\ngenerator = Generator()","006af483":"lr = 0.001\nnum_epochs = 300\nloss_function = nn.BCELoss()","7a94e4cf":"optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=lr)\noptimizer_generator = torch.optim.Adam(generator.parameters(), lr=lr)","e2e23fb7":"for epoch in range(num_epochs):\n    for n, (real_samples, _) in enumerate(train_loader):\n        # Data for training the discriminator\n        real_samples_labels = torch.ones((batch_size, 1))\n        latent_space_samples = torch.randn((batch_size, 2))\n        generated_samples = generator(latent_space_samples)\n        generated_samples_labels = torch.zeros((batch_size, 1))\n        all_samples = torch.cat((real_samples, generated_samples))\n        all_samples_labels = torch.cat(\n            (real_samples_labels, generated_samples_labels)\n        )\n\n        # Training the discriminator\n        discriminator.zero_grad()\n        output_discriminator = discriminator(all_samples)\n        loss_discriminator = loss_function(\n            output_discriminator, all_samples_labels)\n        loss_discriminator.backward()\n        optimizer_discriminator.step()\n\n        # Data for training the generator\n        latent_space_samples = torch.randn((batch_size, 2))\n\n        # Training the generator\n        generator.zero_grad()\n        generated_samples = generator(latent_space_samples)\n        output_discriminator_generated = discriminator(generated_samples)\n        loss_generator = loss_function(\n            output_discriminator_generated, real_samples_labels\n        )\n        loss_generator.backward()\n        optimizer_generator.step()\n\n        # Show loss\n        if epoch % 10 == 0 and n == batch_size - 1:\n            print(f\"Epoch: {epoch} Loss D.: {loss_discriminator}\")\n            print(f\"Epoch: {epoch} Loss G.: {loss_generator}\")","0fce8440":"latent_space_samples = torch.randn(100, 2)\ngenerated_samples = generator(latent_space_samples)","e9043f37":"generated_samples = generated_samples.detach()\nplt.plot(generated_samples[:, 0], generated_samples[:, 1], \".\")","5359b3f5":"![layup0xwjl0pmuczv8zh.png](attachment:layup0xwjl0pmuczv8zh.png)","e730f15f":"# Difference between Discriminative and Generative models","e78b3a8e":"The GAN training process consists of a two-player minimax game in which D is adapted to minimize the discrimination error between real and generated samples, and G is adapted to maximize the probability of D making a mistake. Although, the dataset containing the real data is not labelled, the training process for G and D are done in a supervised way. At each step in the training, D and G have theirparameters updated. in the original GAN proposal, the parameters of D are updated k times, while the parameters of G are updated only once for each training step. However, to make the training simpler, you can consider k equal to 1. To train D, at each iteration you label some real samples taken from the training data as 1 and some generated samples provided by G as 0. This way, you can use a conventional supervised training framework to update the parameters of D in order to minimize a loss function. You can imagine the system composed of G and D as a single classification system that receives random samples as input and outputs the classification, which in this case can be interpreted as a probability.","891f7a81":"# Training the model","7a55f371":"**When G does a good enough job to fool D, the output probability should be close to 1. You could also use a conventional supervised training framework here: the dataset to train the classification system composed of G and D would be provided by random input samples, and the label associated with each input sample would be 1.\nDuring training, as the parameters of D and G are updated, it\u2019s expected that the generated samples given by G will more closely resemble the real data, and D will have more trouble distinguishing between real and generated data.**","8096ab9b":"**Generative adversarial networks are neural networks which is able to generate materials such as - images, music, speech or text, which is similar to what humans are able to produce. It is an active topic of research in recent times. In this tutorial, I will show you how GANs work and also will teach you to implement generative models.**","4c78643d":"![gan1.png](attachment:gan1.png)","817bc1c8":"# Architecture of Generative Adversarial Networks","a0303917":"Generative adversarial network includes two neural networks as mentioned earlier - generator and the discriminator. The generator estimates the probability distribution of the real samples in order to provide generated samples resembling real data. The discriminator, on the other hand, estimates the probability that a given sample came from the data rather than provided by the generator. These structures are called generative adversarial networks as the generator and discriminator are trained to compete against each other. The generator always tries to get better at fooling the discriminator and the discriminator tries to get better at identifying generated samples.- ","7ebab15d":"# Preparing the training data","2db1166e":"> For better understanding, GANs can be defined as machine learning systems which can learn to mimic a given distribution of data. It was first proposed by deep learning expert Ian Goodfellow and his team in this [paper](https:\/\/papers.nips.cc\/paper\/2014\/file\/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf).\n\n*GANs consists of 2 neural networks - one trained to generate data and the other one is trained to distinguish fake data from real data. The latter one provides the adversarial nature of the model.*","8223c81c":"Discriminative models are used for most supervised classification or regression problems. For example - you have used discriminative models to classify labelled images of handwritten digits from 0 to 9.\n\nGenerative models like GANs are trained to describe how a dataset is generated in terms of probabilistic model. By sampling from a generative model, you are able to generate new data. On the other hand, discriminative models are used for supervised learning. Generative models are often used with unlabelled dataset and can be seen as a form of unsupervised learning. We can actually use the dataset of handwriiten digits to train a generative model to generate new digits. During training, some algorithms are used to adjust the model's parameters to minimize a loss function and learn the probability distribution of the training set, and the model trained is used to generate new samples. To output new samples, generative models usually consider a stochastic, or random element that influences the samples generated by the model. The random samples used to drive the generator are obtained from a latent spae in which the vectors represent a kind of compressed form of the generative samples. Generative models can also be used for labelled datasets, but in general, discriminative models work better for classification. GANs have recieved a lot of attention in recent times, but they are not the only architecture that can be used as a generative model. There are other generative model architectures such as - Boltzmann machines, variational autoencoders, Hidden markov models, GPT-2, etc.","d3a53224":"# Implementing the discriminator","a90c3339":"# Let's start working with your first GAN","1a871c6b":"> Random generator seed ensures that the experiment can be replicated in any macine. The number 111 represents the random seed used to initialize the random number generator which is later used to initialize neural network's weights.","852a6590":"# Implementing the Generator","5d58c9b8":"# Let's check the sample generated"}}