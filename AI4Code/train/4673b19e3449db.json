{"cell_type":{"e4b08b33":"code","9410e925":"code","e4a18934":"code","43c613d5":"code","9ea94779":"code","677c43e9":"code","116fee3e":"code","36d1b053":"markdown","19997a58":"markdown","85f272a8":"markdown","c59d2074":"markdown","d6385763":"markdown","1ab979a1":"markdown","c855e39f":"markdown","71d1a3b3":"markdown","6a53c33c":"markdown","d0a45bc9":"markdown","0eb3b108":"markdown"},"source":{"e4b08b33":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.metrics as metrics\nfrom sklearn.naive_bayes import GaussianNB\n\ndata = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ndata.head(20)\n","9410e925":"data.columns","e4a18934":"# Set variables for the targets and features\ny = data['price_range']\nX = data.drop('price_range', axis=1)\n\n# Split the data into training and validation sets\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=7)","43c613d5":"# Create the classifier and fit it to our training data\nrforest = RandomForestClassifier(random_state=7, n_estimators=100)\nrforest.fit(train_X, train_y)\n\nsvm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nsvm.fit(train_X, train_y)\n\ngnb = GaussianNB()\ngnb.fit(train_X, train_y)\n\nimportances = rforest.feature_importances_\n# std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n#              axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(train_X.shape[1]):\n    idx = indices[f]\n    print(\"%d. %s (%f)\" % (f + 1, data.columns[idx], importances[indices[f]]))","9ea94779":"# Predict classes given the validation features\npred_rforest = rforest.predict(val_X)\n\n# Predict classes given the validation features\npred_svm = svm.predict(val_X)\n\n# Predict classes given the validation features\npred_gnb = gnb.predict(val_X)\n\n# Calculate the accuracy as our performance metric\naccuracy = metrics.accuracy_score(val_y, pred_rforest)\nprint(\"Accuracy Forrest: \", accuracy)\n\n# Calculate the accuracy as our performance metric\naccuracy1 = metrics.accuracy_score(val_y, pred_svm)\nprint(\"Accuracy SVM: \", accuracy1)\n\n# Calculate the accuracy as our performance metric\naccuracy2 = metrics.accuracy_score(val_y, pred_gnb)\nprint(\"Accuracy Naive Bayes: \", accuracy2)","677c43e9":"# Calculate the confusion matrix itself\nconfusion = metrics.confusion_matrix(val_y, pred_rforest)\nprint(f\"Confusion matrix random forest:\")\nfor row in confusion:\n    print((row \/ row.sum()).round(decimals=2)*100)\n\nconfusion = metrics.confusion_matrix(val_y, pred_svm)\nprint(f\"\\nConfusion matrix support vector machine:\")\nfor row in confusion:\n    print((row \/ row.sum()).round(decimals=2)*100)\n\nconfusion = metrics.confusion_matrix(val_y, pred_gnb)\nprint(f\"\\nConfusion matrix Naive Nayes:\")\nfor row in confusion:\n    print((row \/ row.sum()).round(decimals=2)*100)\n          \n# # Normalizing by the true label counts to get rates\n# print(f\"\\nNormalized confusion matrix:\")\n# for row in confusion:\n#     print((row \/ row.sum()).round(decimals=2)*100)","116fee3e":"probs = rforest.predict_proba(val_X)\nprint(probs)","36d1b053":"Creating and fitting the model is similar to what you've done before, except you'll use `RandomForestClassifier` instead of `RandomForestRegressor`.","19997a58":"# Your Turn \nTry **[some classification](https:\/\/www.kaggle.com\/kernels\/fork\/3685412)** yourself. It's not complicated given what you already know, and it will dramatically expand what types of use cases you can tackle.","85f272a8":"**[Machine Learning Home Page](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)**\n\n---\n","c59d2074":"# Classification\n\nSo far you've predicted numeric targets. This type of modeling is called **regression**, hence the \"Regressor\" part of `RandomForestRegressor`.\n\nAnother common problem you'll see is making a choice between mutually exclusive outcomes. For example, spam detection is predicting whether an email is \"spam\" or \"not spam\" based on the email's content. This type of modeling is called **classification**.\n\nThere are two types of classification: binary (choosing between two classes) and multiclass (choosing between more than two classes). In general there are different approaches to the two types of classification, but most multiclass models will also work for binary problems.\n\nIt's straightforward to build classification models using what you already know about scikit-learn. Instead of `RandomForestRegressor`, you will use `RandomForestClassifier`. \n\nAs an example of classification with `RandomForestClassifier`, I'll use a dataset of phone features to predict a phone's price range. The targets in the data have values:\n\n * 0 (low cost)\n * 1 (medium cost)\n * 2 (high cost)\n * 3 (very high cost)\n \nThe features are things like\n\n* battery_power: Total energy a battery can store in one time measured in mAh\n* blue: Has bluetooth or not\n* clock_speed: speed at which microprocessor executes instructions\n* dual_sim: Has dual sim support or not\n* fc: Front Camera mega pixels\n* four_g: Has 4G or not\n* ....\n\nHere is a quick overview of the data","d6385763":"We create our feature and targets the same as before using `train_test_split`. This part looks like what you've already seen.","1ab979a1":"## Confusion Matrix\n\nOur model did pretty well, correctly predicting around 86% of the price ranges in the validation data. It's often useful to look at where the model is failing with a **confusion matrix** which shows us how our model classified the inputs.","c855e39f":"This shows the probability the model assigns to each class. Often in business problems, decisions you make lead to different monetary returns. The expected return for a decision based on your classifier is the probability times the monetary return of that decision.\n\nConsider probabilities `[0.05 0.17 0.42 0.36]`. Assume the third option would result in \\\\$100 of profit while the fourth option would return \\\\$150 in profit. Then the expected monetary values are $0.42* \\$100 = \\$42$ and $0.36*\\$150 = \\$54$. Even though the third option has the highest probability, on average it would be better from a business perspective to choose the fourth option.","71d1a3b3":"## Class probabilities \n\nClassification models actually calculate a *probability distribution* over the classes. Using `model.predict` simply returns the class with the highest probability. This might not be ideal based on how the decision affects your metrics or downstream measures. To get the probabilities themselves, use the `.predict_proba` method.","6a53c33c":"It's a little easier to understand as a nice little figure like so:\n\n<img src=\"https:\/\/i.imgur.com\/idD0k8y.png\" alt=\"example confusion matrix\" width=400px>\n\nThe rows of the confusion matrix are the true class and the columns are the predicted class. The diagonal tells us how many of each class the model predicted correctly. The off-diagonals show where the model is making wrong predictions, where it is \"confused.\" For example, looking at the first column and second row, we classified four phones that were actually low cost as medium cost. We see for classes 0 and 3, the low cost and highest cost phones, our model works really well, above 90% accurate. However, our model is weaker for medium and high cost phones. Note that incorrect predictions are only between adjacent classes. The model doesn't confuse low cost and very high cost phones.","d0a45bc9":"The simplest metric for classification models is the **accuracy**, the fraction predictions that are correct. Scikit-learn provides `metrics.accuracy_score` to calculate this.","0eb3b108":"---\n**[Machine Learning Home Page](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum) to chat with other Learners.*"}}