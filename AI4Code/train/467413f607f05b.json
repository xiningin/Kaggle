{"cell_type":{"d24c2b8c":"code","8ec63b66":"code","831edaeb":"code","206f879a":"code","13111da6":"code","6bc6e367":"code","c86b3de7":"code","33ec5212":"code","5f0f896b":"code","ba08a599":"code","bb4c589b":"code","58e9bbbb":"code","42239c59":"markdown"},"source":{"d24c2b8c":"import os\nimport cv2\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nprint(\"Tensorflow version: \", tf.__version__)\n\nseed = 1234\nnp.random.seed(seed)\ntf.random.set_seed(seed)","8ec63b66":"# Path to the data directory\n# data_dir = Path(\"..\/input\/captcha-version-2-images\/samples\/\")\n# images = list(data_dir.glob(\"*.png\"))\ndata_dir = Path(\"..\/input\/laporan-katun\/\")\nimages = list(data_dir.glob(\"*.jpg\"))\nprint(\"Number of images found: \", len(images))\n\n# Let's take a look at some samples first. \n# Always look at your data!\nsample_images = images[:4]\n\n_,ax = plt.subplots(2,2, figsize=(5,3))\nfor i in range(4):\n    img = cv2.imread(str(sample_images[i]))\n    RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    print(\"Shape of image: \", RGB_img.shape)\n    ax[i\/\/2, i%2].imshow(RGB_img)\n    ax[i\/\/2, i%2].axis('off')\nplt.show()","831edaeb":"# Store all the characters in a set\ncharacters = set()\n\n# A list to store the length of each captcha\ncaptcha_length = []\n\n# Store image-label info\ndataset = []\n\n# Iterate over the dataset and store the\n# information needed\nfor img_path in images:\n    # 1. Get the label associated with each image\n    label = img_path.name.split(\".jpg\")[0]\n#     label = img_path.name.split(\".png\")[0]\n    # 2. Store the length of this cpatcha\n    captcha_length.append(len(label))\n    # 3. Store the image-label pair info\n    dataset.append((str(img_path), label))\n    \n    # 4. Store the characters present\n    for ch in label:\n        characters.add(ch)\n\n# Sort the characters        \ncharacters = sorted(characters)\n\n# Convert the dataset info into a dataframe\ndataset = pd.DataFrame(dataset, columns=[\"img_path\", \"label\"], index=None)\n\n# Shuffle the dataset\ndataset = dataset.sample(frac=1.).reset_index(drop=True)\n\n\nprint(\"Number of unqiue charcaters in the whole dataset: \", len(characters))\nprint(\"Maximum length of any captcha: \", max(Counter(captcha_length).keys()))\nprint(\"Characters present: \", characters)\nprint(\"Total number of samples in the dataset: \", len(dataset))\ndataset.head()","206f879a":"# Split the dataset into training and validation sets\ntraining_data, validation_data = train_test_split(dataset, test_size=0.1, random_state=seed)\n\ntraining_data = training_data.reset_index(drop=True)\nvalidation_data = validation_data.reset_index(drop=True)\n\nprint(\"Number of training samples: \", len(training_data))\nprint(\"Number of validation samples: \", len(validation_data))\n\n\n\n# Map text to numeric labels \nchar_to_labels = {char:idx for idx, char in enumerate(characters)}\n\n# Map numeric labels to text\nlabels_to_char = {val:key for key, val in char_to_labels.items()}\n\n\n\n# Sanity check for corrupted images\ndef is_valid_captcha(captcha):\n    for ch in captcha:\n        if not ch in characters:\n            return False\n    return True\n\n\n\n# Store arrays in memory as it's not a muvh big dataset\ndef generate_arrays(df, resize=True, img_height=30, img_width=128):\n    \"\"\"Generates image array and labels array from a dataframe.\n    \n    Args:\n        df: dataframe from which we want to read the data\n        resize (bool)    : whether to resize images or not\n        img_weidth (int): width of the resized images\n        img_height (int): height of the resized images\n        \n    Returns:\n        images (ndarray): grayscale images\n        labels (ndarray): corresponding encoded labels\n    \"\"\"\n    \n    num_items = len(df)\n    images = np.zeros((num_items, img_height, img_width), dtype=np.float32)\n    labels = [0]*num_items\n    \n    for i in range(num_items):\n        img = cv2.imread(df[\"img_path\"][i])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        \n        if resize: \n            img = cv2.resize(img, (img_width, img_height))\n        \n        img = (img\/255.).astype(np.float32)\n        label = df[\"label\"][i]\n        \n        # Add only if it is a valid captcha\n        if is_valid_captcha(label):\n            images[i, :, :] = img\n            labels[i] = label\n    \n    return images, np.array(labels)\n\n\n\n# Build training data\ntraining_data, training_labels = generate_arrays(df=training_data)\nprint(\"Number of training images: \", training_data.shape)\nprint(\"Number of training labels: \", training_labels.shape)\n\n\n# Build validation data\nvalidation_data, validation_labels = generate_arrays(df=validation_data)\nprint(\"Number of validation images: \", validation_data.shape)\nprint(\"Number of validation labels: \", validation_labels.shape)","13111da6":"class DataGenerator(keras.utils.Sequence):\n    \"\"\"Generates batches from a given dataset.\n    \n    Args:\n        data: training or validation data\n        labels: corresponding labels\n        char_map: dictionary mapping char to labels\n        batch_size: size of a single batch\n        img_width: width of the resized\n        img_height: height of the resized\n        downsample_factor: by what factor did the CNN downsample the images\n        max_length: maximum length of any captcha\n        shuffle: whether to shuffle data or not after each epoch\n    Returns:\n        batch_inputs: a dictionary containing batch inputs \n        batch_labels: a batch of corresponding labels \n    \"\"\"\n    \n    def __init__(self,\n                 data,\n                 labels,\n                 char_map,\n                 batch_size=16,\n                 img_width=180,\n                 img_height=30,\n                 downsample_factor=4,\n                 max_length=6,\n                 shuffle=True\n                ):\n        self.data = data\n        self.labels = labels\n        self.char_map = char_map\n        self.batch_size = batch_size\n        self.img_width = img_width\n        self.img_height = img_height\n        self.downsample_factor = downsample_factor\n        self.max_length = max_length\n        self.shuffle = shuffle\n        self.indices = np.arange(len(data))    \n        self.on_epoch_end()\n        \n    def __len__(self):\n        return int(np.ceil(len(self.data) \/ self.batch_size))\n    \n    def __getitem__(self, idx):\n        # 1. Get the next batch indices\n        curr_batch_idx = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n        \n        # 2. This isn't necessary but it can help us save some memory\n        # as not all batches the last batch may not have elements\n        # equal to the batch_size \n        batch_len = len(curr_batch_idx)\n        \n        # 3. Instantiate batch arrays\n        batch_images = np.ones((batch_len, self.img_width, self.img_height, 1),\n                               dtype=np.float32)\n        batch_labels = np.ones((batch_len, self.max_length), dtype=np.float32)\n        input_length = np.ones((batch_len, 1), dtype=np.int64) * \\\n                                (self.img_width \/\/ self.downsample_factor - 2)\n        label_length = np.zeros((batch_len, 1), dtype=np.int64)\n        \n        \n        for j, idx in enumerate(curr_batch_idx):\n            # 1. Get the image and transpose it\n            img = self.data[idx].T\n            # 2. Add extra dimenison\n            img = np.expand_dims(img, axis=-1)\n            # 3. Get the correpsonding label\n            text = self.labels[idx]\n            # 4. Include the pair only if the captcha is valid\n            if is_valid_captcha(text):\n                label = [self.char_map[ch] for ch in text]\n                batch_images[j] = img\n                batch_labels[j] = label\n                label_length[j] = len(text)\n        \n        batch_inputs = {\n                'input_data': batch_images,\n                'input_label': batch_labels,\n                'input_length': input_length,\n                'label_length': label_length,\n                }\n        return batch_inputs, np.zeros(batch_len).astype(np.float32)\n        \n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)","6bc6e367":"# Batch size for training and validation\nbatch_size = 16\n\n# Desired image dimensions\nimg_width=128\nimg_height=30 \n\n# Factor  by which the image is going to be downsampled\n# by the convolutional blocks\ndownsample_factor=4\n\n# Maximum length of any captcha in the data\nmax_length=6\n\n# Get a generator object for the training data\ntrain_data_generator = DataGenerator(data=training_data,\n                                     labels=training_labels,\n                                     char_map=char_to_labels,\n                                     batch_size=batch_size,\n                                     img_width=img_width,\n                                     img_height=img_height,\n                                     downsample_factor=downsample_factor,\n                                     max_length=max_length,\n                                     shuffle=True\n                                    )\n\n# Get a generator object for the validation data \nvalid_data_generator = DataGenerator(data=validation_data,\n                                     labels=validation_labels,\n                                     char_map=char_to_labels,\n                                     batch_size=batch_size,\n                                     img_width=img_width,\n                                     img_height=img_height,\n                                     downsample_factor=downsample_factor,\n                                     max_length=max_length,\n                                     shuffle=False\n                                    )","c86b3de7":"class CTCLayer(layers.Layer):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred, input_length, label_length):\n        # Compute the training-time loss value and add it\n        # to the layer using `self.add_loss()`.\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        self.add_loss(loss)\n        \n        # On test time, just return the computed loss\n        return loss\n\n\n\ndef build_model():\n    # Inputs to the model\n    input_img = layers.Input(shape=(img_width, img_height, 1),\n                            name='input_data',\n                            dtype='float32')\n    labels = layers.Input(name='input_label', shape=[max_length], dtype='float32')\n    input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n    label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n    \n       \n    # First conv block\n    x = layers.Conv2D(32,\n               (3,3),\n               activation='relu',\n               kernel_initializer='he_normal',\n               padding='same',\n               name='Conv1')(input_img)\n    x = layers.MaxPooling2D((2,2), name='pool1')(x)\n    \n    # Second conv block\n    x = layers.Conv2D(64,\n               (3,3),\n               activation='relu',\n               kernel_initializer='he_normal',\n               padding='same',\n               name='Conv2')(x)\n    x = layers.MaxPooling2D((2,2), name='pool2')(x)\n    \n    \n    # We have used two max pool with pool size and strides of 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing it to RNNs\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 4)*64)\n    x = layers.Reshape(target_shape=new_shape, name='reshape')(x)\n    x = layers.Dense(64, activation='relu', name='dense1')(x)\n    x = layers.Dropout(0.05)(x)\n    \n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128,\n                                         return_sequences=True,\n                                         dropout=0.01))(x)\n    x = layers.Bidirectional(layers.LSTM(64,\n                                         return_sequences=True,\n                                         dropout=0.05))(x)\n    \n    # Predictions\n    x = layers.Dense(len(characters)+1,\n              activation='softmax', \n              name='dense2',\n              kernel_initializer='he_normal')(x)\n    \n    # Calculate CTC\n    output = CTCLayer(name='ctc_loss')(labels, x, input_length, label_length)\n    \n    # Define the model\n    model = keras.models.Model(inputs=[input_img,\n                                       labels,\n                                       input_length,\n                                       label_length],\n                                outputs=output,\n                                name='ocr_model_v1')\n    \n    # Optimizer\n    sgd = keras.optimizers.SGD(learning_rate=0.006,\n                               decay=1e-6,\n                               momentum=0.9,\n                               nesterov=True,\n                               clipnorm=5)\n    \n    # Compile the model and return \n    model.compile(optimizer=sgd)\n    return model","33ec5212":"model = build_model()\nmodel.summary()","5f0f896b":"# Add early stopping\nes = keras.callbacks.EarlyStopping(monitor='val_loss',\n                                   patience=30,\n                                   restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(train_data_generator,\n                    validation_data=valid_data_generator,\n                    epochs=500,\n                    callbacks=[es])","ba08a599":"prediction_model = keras.models.Model(model.get_layer(name='input_data').input,\n                                        model.get_layer(name='dense2').output)\nprediction_model.summary()","bb4c589b":"# A utility to decode the output of the network\ndef decode_batch_predictions(pred):\n    pred = pred[:, :-2]\n    input_len = np.ones(pred.shape[0])*pred.shape[1]\n    \n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, \n                                        input_length=input_len,\n                                        greedy=True)[0][0]\n    \n    # Iterate over the results and get back the text\n    output_text = []\n    for res in results.numpy():\n        outstr = ''\n        for c in res:\n            if c < len(characters) and c >=0:\n                outstr += labels_to_char[c]\n        output_text.append(outstr)\n    \n    # return final text results\n    return output_text","58e9bbbb":"#  Let's check results on some validation samples\nfor p, (inp_value, _) in enumerate(valid_data_generator):\n    bs = inp_value['input_data'].shape[0]\n    X_data = inp_value['input_data']\n    labels = inp_value['input_label']\n    \n    preds = prediction_model.predict(X_data)\n    pred_texts = decode_batch_predictions(preds)\n    \n    \n    orig_texts = []\n    for label in labels:\n        text = ''.join([labels_to_char[int(x)] for x in label])\n        orig_texts.append(text)\n        \n    for i in range(bs):\n#         img = cv2.imread(\"..\/input\/captcha-version-2-images\/samples\/\"+orig_texts[i]+\".png\")\n        img = cv2.imread(\"..\/input\/laporan-katun\/\"+orig_texts[i]+\".jpg\")\n        plt.imshow(img)\n        plt.show()\n        print(f'Ground truth: {orig_texts[i]} \\t Predicted: {pred_texts[i]}')\n    break","42239c59":"**Please upvote if you liked the notebook.**"}}