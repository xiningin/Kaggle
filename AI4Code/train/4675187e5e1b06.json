{"cell_type":{"d3f6dfe3":"code","e379d122":"code","3ae2fdd9":"code","18f9105c":"code","b479520f":"code","7481df32":"code","8b6d14cf":"code","df018ef0":"code","201006b2":"code","1c274f22":"code","1a8dc0c8":"code","51791a80":"code","a5fa6680":"code","767995ed":"code","034f24ba":"code","3f90cd65":"code","9607e39e":"code","37b89d2d":"code","2fd5b5a4":"code","fb9c1013":"code","626df30c":"code","5d0b2219":"code","4592acd0":"code","3137112f":"code","57f490f6":"code","61ff84b3":"code","50c8d8e7":"code","af970410":"code","d26b8c39":"code","5d974f78":"code","b97267e4":"code","2a518a62":"code","ccca2cbc":"code","01a1ac97":"code","19977a3a":"code","12179bba":"code","9e2a483e":"code","46d177f7":"code","99d3f15e":"code","6cb47d35":"code","dd829e55":"code","0e2e7c8c":"code","4de66315":"markdown","e9847486":"markdown","9de5ef7c":"markdown","4ecb4c7b":"markdown","6c43748f":"markdown","b9622ea9":"markdown","e8119504":"markdown","de214652":"markdown","5e0365fc":"markdown","33dcfe72":"markdown","ed7202a7":"markdown","6cf49994":"markdown","44634695":"markdown","a3fa1119":"markdown","50cb2086":"markdown","e2590d50":"markdown","98e17d94":"markdown","913b2996":"markdown","76d27d13":"markdown","5b4007f5":"markdown","b2707e8c":"markdown","fcab5563":"markdown","3d19fa90":"markdown","0e7ac643":"markdown","4bdb3b5e":"markdown","bd2499ff":"markdown","adbe460d":"markdown","185b0b61":"markdown","67253e68":"markdown","3462d304":"markdown","f5536e60":"markdown","72c1e9b9":"markdown","400c9970":"markdown","c624c156":"markdown","c8cca1f4":"markdown","0badeb2c":"markdown"},"source":{"d3f6dfe3":"%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport zipfile\nimport cv2\nimport tensorflow as tf\nimport time\nimport dateutil\nimport sklearn.metrics as sm","e379d122":"PATH='..\/input\/appliances-energy-prediction\/KAG_energydata_complete.csv'\ndata = pd.read_csv(PATH)\ndata.columns = [x.lower() for x in data.columns]\ndata.head(5)","3ae2fdd9":"data.isnull().sum().sort_values(ascending=False)","18f9105c":"data.apply(lambda x: len(x.unique()))","b479520f":"data.describe()","7481df32":"data.info()","8b6d14cf":"sns.distplot(data[\"appliances\"])","df018ef0":"data = data[data['appliances'].between(data['appliances'].quantile(.0), data['appliances'].quantile(.99))]\nsns.boxplot(data[\"appliances\"],color=\"green\")","201006b2":"data[\"exact_date\"]=data['date'].str.split(' ').str[0]\n\ndata[\"hours\"]=(data['date'].str.split(':').str[0].str.split(\" \").str[1]).astype(str).astype(int)\ndata[\"seconds\"]=((data['date'].str.split(':').str[1])).astype(str).astype(int).mul(60)\n\ndata[\"week\"]=(data['date'].str.split(' ').str[0])\ndata[\"week\"]=(data['week'].apply(dateutil.parser.parse, dayfirst=True))\ndata[\"weekday\"]=(data['week'].dt.dayofweek).astype(str).astype(int)\ndata[\"week\"]=(data['week'].dt.day_name())\n\ndata['log_appliances'] = np.log(data.appliances)\ndata['hour*lights'] = data.hours * data.lights\ndata['hour_avg'] = list(map(dict(data.groupby('hours')[\"appliances\"].mean()).get, data.hours))\n\ndata.head(5)","1c274f22":"dates=data[\"exact_date\"].unique()\narranged_day = pd.Categorical(data[\"exact_date\"], categories=dates,ordered=True)\ndate_series = pd.Series(arranged_day)\ntable = pd.pivot_table(data,values=\"appliances\",index=date_series, aggfunc=[np.sum],fill_value=0)\ntable.plot(kind=\"bar\",figsize=(20, 7))\nplt.show()","1a8dc0c8":"days=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\narranged_day = pd.Categorical(data[\"week\"], categories=days,ordered=True)\nday_series = pd.Series(arranged_day)\ntable = pd.pivot_table(data,index=[\"hours\"],\n               values=\"appliances\",columns=day_series,\n               aggfunc=[np.sum],fill_value=0)\n\nfig, ax = plt.subplots(figsize=(20, 10))\nax.set_title('Heatmap : Appliances(wh)')\n\nheatmap = ax.pcolor(table)\n\nax.set_xlabel(\"Week Days\")\nax.set_ylabel(\"Hours\")\n\nplt.colorbar(heatmap)\nax.set_yticks(range(len(table.index)+1))\nax.set_xticks(range(len(table.columns)+1))\n\nplt.xlabel(\"Week\")\nplt.ylabel(\"Hours of Day\")\nplt.show()","51791a80":"table.plot.box(figsize=(20, 7))","a5fa6680":"# Data sets in 30 minitues and 1 hour basis\ndata['date'] = pd.to_datetime(data['date'])\ndata = data.set_index('date')\ndf_hour = data.resample('1H').mean()\ndf_30min =data.resample('30min').mean()","767995ed":"# Qualitative predictors \n'''We assume that we have low(high) energy load when the appliances consumption is lower(higher) \n   than a given point of the hourly average counsumption. This point is dependent of data time frequency \n   and the numbers below are set after several tryouts based on appliances' consumption standard deviation.\n'''\n\ndata['low_consum'] = (data.appliances+25<(data.hour_avg))*1\ndata['high_consum'] = (data.appliances+100>(data.hour_avg))*1\n\ndf_hour['low_consum'] = (df_hour.appliances+25<(df_hour.hour_avg))*1\ndf_hour['high_consum'] = (df_hour.appliances+25>(df_hour.hour_avg))*1\n\ndf_30min['low_consum'] = (df_30min.appliances+25<(df_30min.hour_avg))*1\ndf_30min['high_consum'] = (df_30min.appliances+35>(df_30min.hour_avg))*1","034f24ba":"# Plot of Mean Energy Consumption per Hour of a Day\n\ndata.groupby('hours')['appliances'].mean().plot(figsize=(10,8))\nplt.xlabel('Hour')\nplt.ylabel('Appliances consumption in Wh')\nticks = list(range(0, 24, 1))\nplt.title('Mean Energy Consumption per Hour of a Day')\n\nplt.xticks(ticks);","3f90cd65":"f, axes = plt.subplots(1, 2,figsize=(10,4))\n\nsns.distplot(df_hour.appliances, hist=True, color = 'blue',hist_kws={'edgecolor':'black'},ax=axes[0])\naxes[0].set_title(\"Appliance's consumption\")\naxes[0].set_xlabel('Appliances wH')\n\nsns.distplot(df_hour.log_appliances, hist=True, color = 'blue',hist_kws={'edgecolor':'black'},ax=axes[1])\naxes[1].set_title(\"Log Appliance's consumption\")\naxes[1].set_xlabel('Appliances log(wH)')","9607e39e":"\ncol = ['log_appliances', 'lights', 't1', 'rh_1', 't2', 'rh_2', 't3', 'rh_3', 't4',\n       'rh_4', 't5', 'rh_5', 't6', 'rh_6', 't7', 'rh_7', 't8', 'rh_8', 't9',\n       'rh_9', 't_out', 'press_mm_hg', 'rh_out', 'windspeed', 'visibility',\n       'tdewpoint','hours']\ncorr = data[col].corr()\nplt.figure(figsize = (18,18))\nsns.set(font_scale=1)\nsns.heatmap(corr, cbar = True, annot=True, square = True,cmap=\"RdYlGn\", fmt = '.2f', xticklabels=col, yticklabels=col)\nplt.show();","37b89d2d":"col = ['t6','t2', 'rh_6','lights','hours','t_out','windspeed','tdewpoint']\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(data[col])\nplt.show();","2fd5b5a4":"for cat_feature in ['weekday', 'hours']:\n    df_hour = pd.concat([df_hour, pd.get_dummies(df_hour[cat_feature])], axis=1)\n    df_30min = pd.concat([df_30min, pd.get_dummies(df_30min[cat_feature])], axis=1)\n    df = pd.concat([data, pd.get_dummies(data[cat_feature])], axis=1)","fb9c1013":"feature_set = ['low_consum','high_consum','hours','t6','rh_6','lights','hour*lights',\n               'tdewpoint','visibility','press_mm_hg','windspeed']","626df30c":"# to avoid warnings from standardscaler\ndf_hour.lights = df_hour.lights.astype(float)\ndf_hour.log_appliances = df_hour.log_appliances.astype(float)\ndf_hour.hour = df_hour.hours.astype(float)\ndf_hour.low_consum = df_hour.low_consum.astype(float)\ndf_hour.high_consum = df_hour.high_consum.astype(float)","5d0b2219":"# Creation of train\/test sets\ntest_size=.2\ntest_index = int(len(df_hour.dropna())*(1-test_size))\n\nX_train, X_test = df_hour[feature_set].iloc[:test_index,], df_hour[feature_set].iloc[test_index:,]\ny_train = df_hour.log_appliances.iloc[:test_index,]\n\ny_test =  df_hour.log_appliances.iloc[test_index:,]","4592acd0":"from sklearn.preprocessing import StandardScaler\n\n# Normalizing of X matrices for each model to mean = 0 and standard deviation = 1\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","3137112f":"from sklearn import linear_model\n\nlin_model = linear_model.LinearRegression()\nlin_model.fit(X_train,y_train)","57f490f6":"from sklearn import svm\n\nsvr_model = svm.SVR(gamma='scale')\nsvr_model.fit(X_train,y_train)","61ff84b3":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=100,random_state=1)            \nrf_model.fit(X_train, y_train)","50c8d8e7":"import xgboost as xgb\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor as cbr\n\nmodel_lgb = lgb.LGBMRegressor(num_leaves=41, n_estimators=200)\nmodel_lgb.fit(X_train, y_train)","af970410":"model_xgb = xgb.XGBRegressor(objective='reg:squarederror')\nmodel_xgb.fit(X_train, y_train)","d26b8c39":"model_cbr = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\nmodel_cbr.fit(X_train, y_train)","5d974f78":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import metrics\n\n# Function to evaluate the models\ndef evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    r_score = 100*r2_score(test_labels,predictions)\n    accuracy = 100 - mape\n    print(model,'\\n')\n    print('Average Error       : {:0.4f} degrees'.format(np.mean(errors)))\n    print('Variance score R^2  : {:0.2f}%' .format(r_score))\n    print('Accuracy            : {:0.2f}%\\n'.format(accuracy))","b97267e4":"evaluate(lin_model, X_test, y_test)\nevaluate(svr_model, X_test, y_test)\nevaluate(rf_model, X_test, y_test)\nevaluate(model_lgb, X_test, y_test)\nevaluate(model_xgb, X_test, y_test)\nevaluate(model_cbr, X_test, y_test)","2a518a62":"#instead of KFold I use TimeSeriesSplit (10 splits) due to time series data\ncv = TimeSeriesSplit(n_splits = 10)\n\nprint('Linear Model:')\nscores = cross_val_score(lin_model, X_train, y_train, cv=cv,scoring='neg_mean_absolute_error')\nprint(\"Accuracy: %0.2f (+\/- %0.2f) degrees\" % (100+scores.mean(), scores.std() * 2))\nscores = cross_val_score(lin_model, X_train, y_train, cv=cv,scoring='r2')\nprint(\"R^2: %0.2f (+\/- %0.2f) degrees\" % (scores.mean(), scores.std() * 2))\n\nprint('SVR Model:')\nscores = cross_val_score(svr_model, X_train, y_train, cv=cv,scoring='neg_mean_absolute_error')\nprint(\"Accuracy: %0.2f (+\/- %0.2f) degrees\" % (100+scores.mean(), scores.std() * 2))\nscores = cross_val_score(svr_model, X_train, y_train, cv=cv)\nprint(\"R^2: %0.2f (+\/- %0.2f) degrees\" % (scores.mean(), scores.std() * 2))\n\nprint('Random Forest Model:')\nscores = cross_val_score(rf_model, X_train, y_train, cv=cv,scoring='neg_mean_absolute_error')\nprint(\"Accuracy: %0.2f (+\/- %0.2f) degrees\" % (100+scores.mean(), scores.std() * 2))\nscores = cross_val_score(rf_model, X_train, y_train, cv=cv)\nprint(\"R^2: %0.2f (+\/- %0.2f) degrees\" % (scores.mean(), scores.std() * 2))\n\nprint('LGBMRegressor Model:')\nscores = cross_val_score(model_lgb, X_train, y_train, cv=cv,scoring='neg_mean_absolute_error')\nprint(\"Accuracy: %0.2f (+\/- %0.2f) degrees\" % (100+scores.mean(), scores.std() * 2))\nscores = cross_val_score(model_lgb, X_train, y_train, cv=cv)\nprint(\"R^2: %0.2f (+\/- %0.2f) degrees\" % (scores.mean(), scores.std() * 2))\n\nprint('XGBRegressor Model:')\nscores = cross_val_score(model_xgb, X_train, y_train, cv=cv,scoring='neg_mean_absolute_error')\nprint(\"Accuracy: %0.2f (+\/- %0.2f) degrees\" % (100+scores.mean(), scores.std() * 2))\nscores = cross_val_score(model_xgb, X_train, y_train, cv=cv)\nprint(\"R^2: %0.2f (+\/- %0.2f) degrees\" % (scores.mean(), scores.std() * 2))\n\nprint('CatBoostRegressor Model:')\nscores = cross_val_score(model_cbr, X_train, y_train, cv=cv,scoring='neg_mean_absolute_error')\nprint(\"Accuracy: %0.2f (+\/- %0.2f) degrees\" % (100+scores.mean(), scores.std() * 2))\nscores = cross_val_score(model_cbr, X_train, y_train, cv=cv)\nprint(\"R^2: %0.2f (+\/- %0.2f) degrees\" % (scores.mean(), scores.std() * 2))","ccca2cbc":"y1_pred = lin_model.predict(X_test)\ny2_pred = svr_model.predict(X_test)\ny3_pred = rf_model.predict(X_test)\ny4_pred = model_lgb.predict(X_test)\ny5_pred = model_xgb.predict(X_test)\ny6_pred = model_cbr.predict(X_test)","01a1ac97":"fig, axs = plt.subplots(1, 6, figsize=(16,4), sharey=True)\naxs[0].scatter(y1_pred,y_test-y1_pred)\naxs[0].set_title('Linear Regression')\naxs[1].scatter(y2_pred,y_test-y2_pred)\naxs[1].set_title('SVR')\naxs[2].scatter(y3_pred,y_test-y3_pred)\naxs[2].set_title('Random Forest')\naxs[3].scatter(y4_pred,y_test-y4_pred)\naxs[3].set_title('LGB')\naxs[4].scatter(y5_pred,y_test-y5_pred)\naxs[4].set_title('XGB')\naxs[5].scatter(y6_pred,y_test-y6_pred)\naxs[5].set_title('CBR')\nfig.text(0.06, 0.5, 'Residuals', ha='center', va='center', rotation='vertical')\nfig.text(0.5, 0.01,'Fitted Values', ha='center', va='center')","19977a3a":"fig, axs = plt.subplots(1, 6, figsize=(16,4), sharey=True)\naxs[0].scatter(y_test,y1_pred)\naxs[0].set_title('Linear Regression')\naxs[1].scatter(y_test,y2_pred)\naxs[1].set_title('SVR')\naxs[2].scatter(y_test, y3_pred)\naxs[2].set_title('Random Forest')\naxs[3].scatter(y_test, y4_pred)\naxs[3].set_title('LGB')\naxs[4].scatter(y_test, y5_pred)\naxs[4].set_title('XGB')\naxs[5].scatter(y_test, y6_pred)\naxs[5].set_title('CBR')\nfig.text(0.06, 0.5, 'Predictions', ha='center', va='center', rotation='vertical')\nfig.text(0.5, 0.01,'True Values', ha='center', va='center')","12179bba":"fig = plt.figure(figsize=(20,8))\nplt.plot(y_test[:100].values,label='Target value',color='b')\nplt.plot(y1_pred[:100],label='Linear Prediction ', linestyle='--', color='y')\nplt.plot(y2_pred[:100],label='SVR Prediction ', linestyle='--', color='g')\nplt.plot(y3_pred[:100],label='Random Forest', linestyle='--', color='r')\nplt.plot(y4_pred[:100],label='LGB', linestyle='--', color='black')\nplt.plot(y5_pred[:100],label='XGB', linestyle='--', color='orange')\nplt.plot(y6_pred[:100],label='CBR', linestyle='--', color='purple')\n\nplt.legend(loc=1)","9e2a483e":"from sklearn.model_selection import GridSearchCV, KFold\n\nparameters = {\n    'max_depth': [800,1000,1500],\n    'min_samples_leaf': [5,8,10],\n    'min_samples_split': [5,10,15],\n    'n_estimators': [40,60,100],\n    'random_state':[1]    \n}\n\ncv =cv\ngrid_model = GridSearchCV(RandomForestRegressor(), parameters, cv=cv)\n\ngrid_model = grid_model.fit(X_train, y_train)\nprint(grid_model.best_estimator_)\nprint(grid_model.best_params_)","46d177f7":"best_rf_model = grid_model.best_estimator_\ngrid_accuracy = evaluate(grid_model, X_test, y_test)\ny_best_pred = best_rf_model.predict(X_test)","99d3f15e":"# Calculate Confidence interval 95% for the predictions\nsum_errs = np.sum((y_test - y_best_pred)**2)\nstdev = np.sqrt(1\/(len(df_hour)-2) * sum_errs)\n\ninterval = 1.96 * stdev #95% CI\nlower, upper = y_best_pred - interval, y_best_pred + interval","6cb47d35":"fig = plt.figure(figsize=(20,8))\nplt.plot(y_test[:100].values,label='Target value',color='b')\n#plt.plot(y_pred,label='Best Tree Prediction ', linestyle='-', color='b')\nplt.plot(lower[:100],label='Lower Limit ', linestyle='--', color='r')\nplt.plot(upper[:100],label='Upper Limit ', linestyle='--', color='y')\nplt.title('Predicted Lower Limit and Upper Limit of best RF model')\n\nplt.legend(loc=1)","dd829e55":"factor_list = feature_set\nfactors = np.array(X_test)\n\nimportances = list(rf_model.feature_importances_)\n\nfactor_importances = [(factor, round(importance, 2)) \n    for factor, importance in zip(factor_list, importances)]\n\nfactor_importances = sorted(factor_importances, key = lambda X_test: X_test[1], reverse = True)\n \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in factor_importances];","0e2e7c8c":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nx_values = list(range(len(importances)))\nplt.bar(x_values, importances, orientation = 'vertical')\nplt.xticks(x_values, factor_list, rotation='vertical')\nplt.ylabel('Importance'); plt.xlabel('Variable')\nplt.title('Factors influencing energy consumption')\n","4de66315":"### Hour of the Day?","e9847486":"RF, LGB, XGB, CBR models appears to has mean random residuals close to 0 and constant standard deviation.","9de5ef7c":"Checking for Outliers and removing extreme 1% of the data.","4ecb4c7b":"### Pearson Correlation among the variables","6c43748f":"The Energy consumption is highly correlated with:\n    1. Hours : 0.34\n    2. Lights : 0.26\n    3. T2 : 0.22\n    4. T6 : 0.26\n    \nAlso all temperature values inside house are highly correlated with each other (> 0.8)","b9622ea9":"### Histogram of Appliance's consumption","e8119504":"Inside temperatures, outside temperatures and tdewpoint have linear relationship. These features will best suite for Linear regression modelling.","de214652":"### Factors influencing energy consumption","5e0365fc":"The distribution of power load is not normal as we have left asymetry, for this reason we shall use log(power load) which has closer to normal distribution for further analysis.","33dcfe72":"### Parameter Tuning","ed7202a7":"### Transforming categorical variables ","6cf49994":"### Linear dependencey evaluation","44634695":"Generated 3 data sets with time interval 10 minutes, 30 minutes, 1 hour respectively. Using the 1 hour data set for further analysis as it having less noise.","a3fa1119":"XGB model appears to be the one which predicts high and low values of energy consumption.","50cb2086":"#### Hour of the Day is the important influencing parameter for Energy consumption.","e2590d50":"### Prediction of each model vs Test data","98e17d94":"## Perform analysis & model development ","913b2996":"### Injesting new features to the dataset","76d27d13":"Weekends (Saturdays and Sundays) are observed to have high consumption of Electricity. (> 25% than Weekdays).","5b4007f5":"High Electricity consumption of >140Wh is observed during evening hours 16:00 to 20:00. At night hours from 23:00-6:00 the power load is below 50Wh, meaning that most appliances are off or standby. Between 9:00-13:00 the power load is >100Wh and after launch reduces again to <100Wh. At afternoon, the energy consumption ranges from 130-185Wh as family members are at home and many devices are on. ","b2707e8c":"## References\n    [1] Luis M. Candanedo, Veronique Feldheim, Dominique Deramaix, Data driven prediction models of energy use of appliances in a low-energy house, Energy and Buildings, Volume 140, 1 April 2017, Pages 81-97, ISSN 0378-7788.\n    [2] N. Arghira, L. Hawarah, S. Ploix, M. Jacomino, Prediction of appliances energy use in smart homes, Energy 48 (1) (2012) 128\u2013134.\n    [3] M. Muratori, M.C. Roberts, R. Sioshansi, V. Marano, G. Rizzoni, A highly resolved modeling technique to simulate residential power demand, Appl. Energy 107 (2013) 465\u2013473.\n    [4] Saleh Seyedzadeh, Farzad Pour Rahimian, Ivan Glesk & Marc Roper, Machine learning for estimation of building energy consumption and performance: a review, Visualization in Engineering volume 6, Article number: 5 (2018) ","fcab5563":"XBG is predicitng highs and lows better than other models. Overall Random Forest appears to closely fit with the test data.","3d19fa90":"## Observations and motivation for next steps","0e7ac643":"### Observations:\n    1. Hour of the Day is the most important influencing parameter for Energy consumption\n    2. XBG is predicitng highs and lows better than other models, \n    3. Overall Random Forest appears to closely fit with the test data\n    4. High Electricity consumption of >140Wh is observed during evening hours 16:00 to 20:00\n    5. Weekends (Saturdays and Sundays) are observed to have high consumption of Electricity. (> 25% than Weekdays)\n    6. Though light consumpstion appeared as highly correlated with Appliance electricity consumption, lights are having very low importance as a feature\n\n### Motivation for future steps:\n    1. Available data is only for 1 house, we learn important information if we analyse several houses \n    2. Further informations like House geometry, number of people residing at house over time may give few more insights\n    3. Need to capture data for several months to bring in seasonal effects on energy consumption\n    4. Optimal positioning and quality of sensors can be analysed for better data capturing\n    5. The predictions of appliances energy use could probably be better if the weather station was closer to the house\n    6. Noise and CO2 level in the room can also be an important data for improving predictions","4bdb3b5e":"### Day wise Electricity consumption","bd2499ff":"----------------------------------------------------------------------------------------------------------------\n### Inferences:\n     1. There are 29 columns - 1 date time column, 2 Integer columns and 26 Float column\n     2. Nearly 1 coulmn (Lights) is having less than 10 unique items, which can be considered as categorical column\n     3. There are NULL values in any of the given columns\n     4. Target, which is Appliances need to predicted","adbe460d":"## Modelling","185b0b61":"## Read data and basic data clean-up","67253e68":"Trying out 6 Regression models:\n    1. LinearRegression\n    2. SVR\n    3. RandomForestRegressor\n    4. LGBMRegressor\n    5. XGBRegressor\n    6. catboost","3462d304":"#### The Variance score of the model impoved from 65% to 68.26%.","f5536e60":"### Model Evaluation, Cross-validation & Selection","72c1e9b9":"## Feature Engineering","400c9970":"Random Forest Model is having the best Accuracy and CatBoost is having the highest R^2.","c624c156":"## Model performance on test data","c8cca1f4":"### Final predictions on test set based on best RF model","0badeb2c":"### Weekend vs Weekday?"}}