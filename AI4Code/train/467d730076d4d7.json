{"cell_type":{"63817d3d":"code","33513f0d":"code","c5a0b1fc":"code","db97de06":"code","414ee57c":"code","0e2e6a35":"code","fde6e4cf":"code","1d327303":"code","e3a3c917":"code","0d56fe1f":"code","2b6c2026":"code","e1d30c32":"markdown","2b7d3d42":"markdown","b6a38c19":"markdown","912d4e2e":"markdown","d3252670":"markdown","d4ef672a":"markdown","9fad4307":"markdown","a5952ede":"markdown","046dd356":"markdown","278bee3b":"markdown","7a43e0b6":"markdown"},"source":{"63817d3d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.cluster import KMeans\nsns.set_style(\"whitegrid\")","33513f0d":"dataset = pd.read_csv(\"..\/input\/UCI_Credit_Card.csv\")\nprint(dataset.head())\nprint(f\"Amount of samples: {dataset.shape[0]}\")","c5a0b1fc":"def plotDist(dataset):\n    features_to_plot = [\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"default.payment.next.month\"]\n    # Define integer to string mappings for a pretty graph\n    intToStr = {\"SEX\": {1: \"Male\", 2: \"Female\"},\n                \"EDUCATION\": {1: \"Graduate School\", 2: \"University\", 3: \"High School\", 4: \"Other\", 5: \"Unknown\", 6: \"Unknown\", 0: \"Unknown\"},\n                \"MARRIAGE\": {1: \"Married\", 2: \"Single\", 3: \"Other\", 0: \"Unknown\"},\n                \"default.payment.next.month\": {0: \"No\", 1: \"Yes\"}\n               }\n    \n    # Iterate the specified features\n    for f in features_to_plot:\n        count = {} # Use dictionary to count\n        for i, s in enumerate(dataset[f]):\n            # Manually replace the number with the nominal string for a pretty graph\n            if f in intToStr.keys():\n                s = intToStr[f][s]\n\n            if s in count.keys():\n                count[s] += 1\n            else:\n                count[s] = 1\n\n        values = np.array(list(count.values()))\n        keys = list(count.keys())\n        \n        # Plot graph\n        fig, ax = plt.subplots(figsize=(13, 4))\n        sns.barplot(keys, values)\n        plt.title(f)\n        plt.ylabel(\"Number of samples\")\n        plt.show()\n        \nplotDist(dataset)","db97de06":"# Normalize columns 12 to 23. BILL_AMT1 to BILL_AMT6 and PAY_AMT1 to PAY_AMT6\nnorm_columns = [\"LIMIT_BAL\", \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\",\n                \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"AGE\"]\nfor c in norm_columns:\n    max_val = np.max(dataset[c])\n    min_val = np.min(dataset[c])\n    \n    dataset[c] = (dataset[c] - min_val) * 2 \/ (max_val - min_val) - 1\n\ndataset.drop(columns=[\"ID\"], inplace=True)\nprint(dataset.head())","414ee57c":"def trainTestSplit(dataset, valid_per=0.1):\n    n_samples = dataset.shape[0] # Total number of samples\n    n_val = int(valid_per * n_samples)\n    \n    indices = np.arange(0, n_samples) # Generate a big array with all indices\n    np.random.shuffle(indices) # Shuffle the array, numpy shuffles inplace\n    \n    # Perform the splits\n    x_train = dataset.iloc[indices[n_val:], :-1].values # Last column is the feature we want to predict\n    y_train = dataset.iloc[indices[n_val:], -1].values\n    x_test = dataset.iloc[indices[:n_val], :-1].values\n    y_test = dataset.iloc[indices[:n_val], -1].values\n    \n    return x_train, y_train, x_test, y_test","0e2e6a35":"print(\"Getting new dataset split...\")\n# Get a dataset split for training and validation\nx_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.2)\n\nprint(f\"Training on {x_train.shape[0]} samples...\")\nprint(\"\\n#### Linear SVM Results ####\")\n# Create Linear SVM model\nlsvm = LinearSVC(max_iter=32000) # If we don't specify anything it assumed all classes have same weight\nlsvm.fit(x_train, y_train)\ny_pred = lsvm.predict(x_test)\nlinear_acc = lsvm.score(x_test, y_test)\nprint(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\n#### Polynomial SVM with Degree 3 Results ####\")\n# Create Polynomial SVM\nsvm = SVC(gamma='scale', kernel='poly', degree=3)\nsvm.fit(x_train, y_train)\npoly_acc = svm.score(x_test, y_test)\ny_pred = svm.predict(x_test)\nprint(f\"Polynomial SVM Acc: {poly_acc*100} %\")\nprint(classification_report(y_test, y_pred))","fde6e4cf":"print(f\"Training on {x_train.shape[0]} samples...\")\nprint(\"\\n#### Linear SVM Results ####\")\n# Create Linear SVM model\nlsvm = LinearSVC(max_iter=32000, class_weight=\"balanced\") # Compute weight based on sample count per class\nlsvm.fit(x_train, y_train)\ny_pred = lsvm.predict(x_test)\nlinear_acc = lsvm.score(x_test, y_test)\nprint(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\n#### Polynomial SVM with Degree 3 Results ####\")\n# Create Polynomial SVM\nsvm = SVC(gamma='scale', kernel='poly', degree=3, \n          class_weight=\"balanced\") # Compute weight based on sample count per class\nsvm.fit(x_train, y_train)\npoly_acc = svm.score(x_test, y_test)\ny_pred = svm.predict(x_test)\nprint(f\"Polynomial SVM Acc: {poly_acc*100} %\")\nprint(classification_report(y_test, y_pred))","1d327303":"def balanceTrainSet(x_train, y_train):\n    samples_per_class = np.bincount(y_train) # Count samples per class\n    dom_class = np.argmax(samples_per_class) # Max class index\n    min_class = np.argmin(samples_per_class) # Min class index\n    n_min = samples_per_class[min_class] # Number of samples in min class\n    \n    # Get indices for the dominant and the minor class\n    dom_indices = np.where(y_train == dom_class)[0]\n    min_indices = np.where(y_train == min_class)[0]\n    np.random.shuffle(dom_indices) # Shuffle dom_indices\n    # Contatenate both indices, using only the same number of indices from dom_indices as in min_indices\n    indices = np.concatenate([min_indices, dom_indices[:n_min]], axis=0)\n    np.random.shuffle(indices)\n    \n    # Build the new training set\n    new_x_train = x_train[indices]\n    new_y_train = y_train[indices]\n    \n    return new_x_train, new_y_train\n    \nbal_x_train, bal_y_train = balanceTrainSet(x_train, y_train)\n\nprint(f\"Training on {bal_x_train.shape[0]} samples...\")\nprint(\"\\n#### Linear SVM Results ####\")\n# Create Linear SVM model\nlsvm = LinearSVC(max_iter=32000) # If we don't specify anything it assumed all classes have same weight\nlsvm.fit(bal_x_train, bal_y_train)\ny_pred = lsvm.predict(x_test)\nlinear_acc = lsvm.score(x_test, y_test)\nprint(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\n#### Polynomial SVM with Degree 3 Results ####\")\n# Create Polynomial SVM\nsvm = SVC(gamma='scale', kernel='poly', degree=3)\nsvm.fit(bal_x_train, bal_y_train)\npoly_acc = svm.score(x_test, y_test)\ny_pred = svm.predict(x_test)\nprint(f\"Polynomial SVM Acc: {poly_acc*100} %\")\nprint(classification_report(y_test, y_pred))","e3a3c917":"N_ROUNDS = 10\nl_reports = [] # Linear SVM reports\np_reports = [] # Polynomial SVM reports\nl_accs = [] # Linear SVM accuracy history\np_accs = [] # Polynomial SVM accuracy history\nfor i in range(N_ROUNDS):\n    print(f\"### Round {i+1} ###\")\n    print(\"Getting new dataset split...\")\n    x_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.2)    \n    print(f\"Training on {x_train.shape[0]} samples...\")\n    \n    # Create a new Linear SVM model\n    lsvm = LinearSVC(max_iter=32000, class_weight=\"balanced\") # Compute weight based on sample count per class\n    lsvm.fit(x_train, y_train)\n    y_pred = lsvm.predict(x_test)\n    linear_acc = lsvm.score(x_test, y_test)\n    print(f\"Linear SVM Acc: {linear_acc*100} % - Validated on {y_test.shape[0]} samples\")\n    report = classification_report(y_test, y_pred, output_dict=True)\n    l_reports.append(report)\n    l_accs.append(linear_acc)\n\n    # Create Polynomial SVM\n    svm = SVC(gamma='scale', kernel='poly', degree=3, \n              class_weight=\"balanced\") # Compute weight based on sample count per class\n    svm.fit(x_train, y_train)\n    poly_acc = svm.score(x_test, y_test)\n    y_pred = svm.predict(x_test)\n    print(f\"Polynomial SVM Acc: {poly_acc*100} %\")\n    report = classification_report(y_test, y_pred, output_dict=True)\n    p_reports.append(report)\n    p_accs.append(poly_acc)\n    \nprint(\"### Finished ###\")\nprint(\"Linear SVM Results:\")\nprint(l_reports[0])\nmean_acc = np.mean(l_accs)\nmean_f1 = np.mean([r[\"weighted avg\"][\"f1-score\"] for r in l_reports])\nprint(f\"\\tMean Acc: {mean_acc*100}% -- Mean Weighted Avg F1-Score: {mean_f1*100}%\")\n\nprint(\"Polynomial SVM with Degree 3 Results:\")\nmean_acc = np.mean(p_accs)\nmean_f1 = np.mean([r[\"weighted avg\"][\"f1-score\"] for r in p_reports])\nprint(f\"\\tMean Acc: {mean_acc*100}% -- Mean Weighted Avg F1-Score: {mean_f1*100}%\")","0d56fe1f":"FEATURE_NAMES = [\"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\", \"PAY_0\", \"PAY_2\",\n                 \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\", \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\",\n                 \"BILL_AMT4\", \"BILL_AMT5\", \"BILLT_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\",\n                 \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\"]\n# Due to computing time constraints use a much smaller training dataset here\nx_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.85)\n\ndef trainModel(x_train, y_train, x_test, y_test):\n    svm = SVC(gamma='scale', kernel='poly', degree=3, \n              class_weight=\"balanced\")\n    svm.fit(x_train, y_train)\n    return svm.score(x_test, y_test)\n\nprint(f\"Will train on {x_train.shape[0]} samples and validate on {x_test.shape[0]} samples.\")\n# Train a baseline model for this data, including all the features\nbaseline_acc = trainModel(x_train, y_train, x_test, y_test)\nprint(f\"Baseline Acc: {baseline_acc*100}%\")\nprint(\"====================================\")\n\nremaining_features = np.arange(0, x_train.shape[1])\nfor i in range(x_train.shape[1]-1):\n    feat_names = [FEATURE_NAMES[r] for r in remaining_features]\n    print(f\"Remaining features: \", end=' ')\n    [print(feat, end=', ') for feat in feat_names]\n    print()\n    \n    best_acc = 0.0\n    least_impact_feature = 0\n    # Find feature with least impact on performance\n    for c in range(remaining_features.shape[0]):\n        # Test by removing each of the columns\n        curr_features = np.delete(remaining_features, c)\n        part_x_train = x_train[:, curr_features]\n        part_x_test = x_test[:, curr_features]\n        \n        acc = trainModel(part_x_train, y_train, part_x_test, y_test)\n        if acc > best_acc:\n            best_acc = acc\n            least_impact_feature = c\n\n    print(f\"Removing feature {FEATURE_NAMES[remaining_features[least_impact_feature]]} -- Had the least impact on performance (Acc: {best_acc*100} %)\")\n    remaining_features = np.delete(remaining_features, least_impact_feature)\n    print(\"====================================\")\n    \nprint(f\"Last feature is {FEATURE_NAMES[remaining_features[0]]}\")\npart_x_train = x_train[:, remaining_features]\npart_x_test = x_test[:, remaining_features]\nacc = trainModel(part_x_train, y_train, part_x_test, y_test)\nprint(f\"Accuracy: {acc*100} %\")","2b6c2026":"# Get a dataset split for clustering and validation\nx_train, y_train, x_test, y_test = trainTestSplit(dataset, 0.2)    \n\nn_clusters = [2, 4, 8, 16, 32]\nfor n in n_clusters:\n    print(f\"K-Means with {n} clusters:\")\n    # Create K-Means model\n    kmeans = KMeans(n_clusters=n)\n    kmeans.fit(x_train) # Fit to training data\n    # Get clusters from validation data\n    clustered = kmeans.predict(x_test)\n    # For each cluster, find the probability defaulting (that the client paid it next month)\n    # by comparing the number of samples for each class\n    highest_prob = 0.0\n    highest_c = 0\n    overall_acc = 0.0\n    for c in range(n):\n        # Retrieve samples that belong to the current cluster\n        indices = np.where(clustered == c)[0]\n        samples_in_cluster = [y_test[s] for s in indices]\n        # How many of those samples are of customers with default paid next month?\n        proportion = np.bincount(samples_in_cluster)\n        prob = proportion[1] \/ np.sum(proportion)\n        print(f\"[Cluster {c}] - Probability of paid credit in this cluster: {prob*100} %\")\n        if prob > highest_prob:\n            highest_c = c\n            highest_prob = prob\n            overall_acc = proportion[1] \/ y_test.shape[0]\n    \n    print(f\"Choosing the single cluster {highest_c} as default_paid yields an overall Accuracy of {overall_acc*100} %\")\n    \n    print()","e1d30c32":"### Loading the Dataset\nThe dataset contains the following features:\n* ID: ID of each client\n* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\n* SEX: Gender (1=male, 2=female)\n* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n* AGE: Age in years\n* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* PAY_2: Repayment status in August, 2005 (scale same as above)\n* PAY_3: Repayment status in July, 2005 (scale same as above)\n* PAY_4: Repayment status in June, 2005 (scale same as above)\n* PAY_5: Repayment status in May, 2005 (scale same as above)\n* PAY_6: Repayment status in April, 2005 (scale same as above)\n* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n* **default.payment.next.month: Default payment (1=yes, 0=no)** -> This is the value we want to predict","2b7d3d42":"### Balancing the training dataset and using same class weightings\n\nA third scenario we can try here is actually balancing the training dataset and using the same class weightings. This will be done by randomly dropping samples of the dominant class until we have the same number of samples per class.\n\nWe can observe that there's not much difference from using the class weightings from the previous run. In fact the results are so similar that both ways might be equivalent and the small changes might be just due differences in the dropped samples.","b6a38c19":"### Splitting the dataset for training and validation","912d4e2e":"# Taiwan Credit Card Defaults\nThis was originally an assignment for a Machine Learning class at National Taiwan Normal University. The goal of this assignment is to implement a machine learning system to analyze and make predictions about the Taiwan Credit Card defaults dataset. This dataset is one of the standard benchmark datasets in the UCI Machine Learning repository. The two goals of the assignment is to:\n\n1. predict a default on the next credit card payment using SVM\n2. cluster the customers into k groups using kMeans clustering\n\nI've wrote the code focusing more on readability than speed.\n\n### Importing modules","d3252670":"## Feature Selection\nHere, we remove the feature with the least impact on generalization accuracy one-by-one, until there's no remaining features to be removed. The goal is to find whether some features are irrelevant and how much they impact the accuracy of the model. This task will be performed on the same data throughout and using the polynomial of degree 3 model.\n\nThis reveals a very interesting pattern. Using even one feature we can achieve a very similar performance (even slightly better).","d4ef672a":"### Performing 10 Rounds of Cross Validation\nNext, let's perform 10 rounds of cross validation using the SVM method with class weightings relative to the number of samples per class. After 10 rounds the average accuracy and the mean average micro f1-score is presented for both SVM models. Micro F1-score is used because the classes are imbalanced.\n\nObserving the results we can conclude that the SVM model with polynomial kernel of degree 3 performs better than the Linear SVM model on this data.","9fad4307":"### Data normalization\nSVMs are guaranteed to converge at some point. However, when data is unscaled this can take some time. In this step, some of the features are rescaled to be within the range of [-1, 1]. In this step the ID column is also dropped from the dataset since it doesn't provide any relevant information for this task.","a5952ede":"### Assigning class weightings relative to sample number\n\nBy initializing the SVM classifiers with the parameter class_weights=\"balanced\" each class is assigned a weight based on its number of samples. The actual calculation is done by (n_samples \/ (n_classes * np.bincount(y)). Let's repeat the test with the same data.\n\nWe can see that the accuracy took a hit, however the recall for class 1 improved greatly. Nonetheless, the precision for predicting class 1 got much worse. I'm not sure why this is the case.","046dd356":"## Goal 1 -- Predicting next month payment\n\n### Assuming same class weightings.\n\nNow, let's train an SVM classifier to predict wether the credit default will be paid next month. In this step we assume the same weight for both classes, assuming that the dataset is balanced.\n\nWe can see that the accuracy seems reasonably good, however just looking at this metric masks an important issue that is happening. If take a look at recall values for class 1 we can see that it is VERY low. This means that the classifier is taking an \"easy\" route and just tends to classify most of the samples as class 0 (the dominant class), while underperforming on the other class. \n\nThis shows that for unbalanced datasets just looking at accuracy isn't enough. We can think of an extreme example were 99% of the samples are of class 0 and just 1% of class 1. In this scenario if the classifier always predicted class 0 for any sample it would achieve an accuracy of 99%.","278bee3b":"## Clustering customers with K-Means\nHere, K-Means is used to cluster the customers based on the features, without taking in account if the default was paid next month. Then, a single cluster is chosen to be considered as the class where the default was paid and the overall accuracy is calculated. The test is repeated for different cluster sizes.\n\nWe can see that using only 2 clusters yielded the best (although not good) result. This happens probably because as we use a larger number of clusters the data becomes more partitioned, i.e. there's less samples per cluster, so the accuracy takes a drop even though the probability is \"better\" in-cluster. An interesting extension of this is exhaustively searching through all combinations of clusters to find the one that yields the best overall accuracy.","7a43e0b6":"### Preliminar data analysis\n\nLet's check the balance of the dataset for each of the nominal features. Here we print the number of samples for each value for the features SEX, EDUCATION, MARRIAGE and AGE. Also, let's plot the feature we want to predict (default.payment.next.month) to check the balance of the dataset.\n\nWe can observe that we have a much large number of females than males in the dataset. Also most of the people have an university level education, are single and are around 30 years old.\nMost importantly, we can notice that the dataset is clearly not balanced for the feature we want to predict. SVMs are sensitive to unbalanced data, so we have to assign different class weightings based on the number of samples of each class."}}