{"cell_type":{"032073c9":"code","8267c6c8":"code","3f2dab54":"code","8ac4578b":"code","2482fbe6":"code","b2287247":"code","ca26b2aa":"code","eeee5acb":"code","957c2cd2":"code","2e25bb6b":"code","cf1c9a5e":"code","a7fc20e0":"code","60093d46":"code","da5c1b97":"code","ca894923":"code","84105a3d":"code","51cf0367":"code","afcb29c9":"code","3bc746c1":"code","22c7cafe":"code","e36fd547":"code","bcc073de":"code","6a109ec0":"code","2b975e73":"code","60534c2d":"code","31983d44":"code","3ab62e49":"code","21cd6fae":"code","f7732024":"code","48ef15d5":"code","46215c79":"code","1dffd5e4":"code","8abb71e1":"code","1ec77e09":"code","d897ef59":"code","9a28fa5d":"code","1c9052c6":"code","16de4f1d":"code","a59b16da":"code","affd8bc8":"code","f7c5687d":"code","9fd23ee1":"code","76987348":"code","52dc8c4e":"code","031d1ae0":"code","e75b123f":"code","f58d0c15":"code","c2f4a0ec":"code","4ed0fc2a":"code","78f83554":"code","a4bc58fd":"code","2163694c":"code","734de1ee":"code","e3388c29":"code","e82d6496":"code","c191bb41":"code","7c61d319":"code","c34364f2":"code","21a93f29":"code","81419652":"code","78b3665c":"code","e0a9288d":"code","3b553bfe":"code","ae953c9c":"code","f7d26bec":"code","8b62b86e":"code","02b948b0":"code","924db622":"code","f60a2617":"code","153c8f68":"code","6c722098":"code","e08cb771":"code","c6e58dd0":"code","b8f39c43":"code","18501fcf":"code","e7ff9124":"code","7134de56":"code","596c463a":"code","15404ba5":"code","e6dd3d42":"code","be634e69":"code","463189d1":"code","df0f5e4c":"code","14f30462":"code","8de3c774":"code","f32d2040":"code","c82c83db":"code","8a13d182":"code","9dc10bab":"code","4bb1b15d":"code","faa9fc96":"code","c946762f":"code","488e38c4":"code","3692bd11":"code","648cc14c":"code","5b9c1a4e":"code","db9ede8f":"code","80ae73a0":"code","3bbff807":"code","47afba3c":"code","a1a8d384":"code","d5cda248":"code","99522c02":"markdown","40872081":"markdown","a5b4ebfb":"markdown","e6d4d7fa":"markdown","73d2c7e8":"markdown","57ecdfc2":"markdown","376a65e2":"markdown","5116e4a7":"markdown","37608663":"markdown","14c7e496":"markdown","191bd1de":"markdown","6ebc92d9":"markdown","efd56924":"markdown","03b5095c":"markdown","cc50160b":"markdown","e2a1c634":"markdown","30e87023":"markdown","4fee8316":"markdown","cf5fb912":"markdown","3422e591":"markdown","52e00aa5":"markdown","a7ad2322":"markdown","70443572":"markdown","b5eb27c6":"markdown","2dfa4a07":"markdown","dd444c93":"markdown","b5ce6cc5":"markdown","6241ab0b":"markdown","ccadced9":"markdown","5729ccdb":"markdown","627ad649":"markdown","59dace6e":"markdown","93c90988":"markdown","1e39b03b":"markdown","2d8fc398":"markdown","fa4ba861":"markdown","e3f4ed1c":"markdown","4c4884c7":"markdown","985cf89e":"markdown","eeac60ca":"markdown","8a2dcae4":"markdown","3a9bf194":"markdown","b4513501":"markdown","6c5d19fc":"markdown","a316a7ae":"markdown"},"source":{"032073c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8267c6c8":"df = pd.read_csv('..\/input\/the-tweets-of-wisdom\/tweets.csv')","3f2dab54":"df.head()","8ac4578b":"df.describe()","2482fbe6":"df.info()","b2287247":"df.isna().any()","ca26b2aa":"df[df.tweet_content.isna()]","eeee5acb":"df[df.author_name.isna()]","957c2cd2":"df = df.dropna()","2e25bb6b":"df.isna().any()","cf1c9a5e":"print(len(df.author_name.unique()))","a7fc20e0":"df.handle.value_counts().head(10)","60093d46":"# let's take top 10 authors\ntop_ten_authors = df.handle.value_counts().head(10).index","da5c1b97":"top_ten_authors","ca894923":"plt.figure(figsize=(15,10))\nsns.distplot(df.author_name.value_counts())","84105a3d":"plt.figure(figsize=(15,10))\nsns.distplot(df.likes)","51cf0367":"plt.figure(figsize=(15,10))\nprint(df[df['author_name'] == 'Thomas Sowell'].likes.mean(), df[df['author_name'] == 'Thomas Sowell'].likes.std())\nsns.distplot(df[df['author_name'] == 'Thomas Sowell'].likes)","afcb29c9":"plt.figure(figsize=(15,10))\nprint(df[df['author_name'] == 'Thomas Sowell'].retweets.mean(), df[df['author_name'] == 'Thomas Sowell'].retweets.std())\nsns.distplot(df[df['author_name'] == 'Thomas Sowell'].retweets)","3bc746c1":"plt.figure(figsize=(10,8))\nsns.heatmap(df[['likes', 'retweets']].corr(), annot=True)","22c7cafe":"plt.figure(figsize=(10,10))\np  = pd.concat([df.groupby('handle').likes.mean(), df.groupby('handle').retweets.mean(), df.groupby('handle').likes.count()], axis=1 )\np.columns = ['Mean Likes per Author', 'Mean Retweets per Author' ,'Number of tweets per authors']\nsns.heatmap(p.corr() ,annot=True)","e36fd547":"df.head()","bcc073de":"plt.figure(figsize=(15,10))\n# Sort the dataframe by target\ntarget_0 = df[df['handle'] == 'ThomasSowell']\ntarget_1 = df[df['handle'] == 'TheAncientSage']\ntarget_2 = df[df['handle'] == 'orangebook_']\n\nsns.distplot(target_0[['likes']], hist=False)\nsns.distplot(target_1[['likes']], hist=False)\nsns.distplot(target_2[['likes']], hist=False)\n\nplt.show()","6a109ec0":"plt.figure(figsize=(15,10))\n# Sort the dataframe by target\ntarget_0 = df[df['handle'] == 'ThomasSowell ']\ntarget_1 = df[df['handle'] == 'TheAncientSage']\ntarget_2 = df[df['handle'] == 'orangebook_']\n\nsns.distplot(target_0[['retweets']], hist=False)\nsns.distplot(target_1[['retweets']], hist=False)\nsns.distplot(target_2[['retweets']], hist=False)\n","2b975e73":"fig, ax = plt.subplots(figsize=(20,10))\nsns.boxplot(x=\"handle\", y=\"likes\", data=df[df['handle'].isin(top_ten_authors)], ax=ax)\nplt.show()","60534c2d":"fig, ax = plt.subplots(figsize=(20,10))\nsns.violinplot(x=\"handle\", y=\"likes\", data=df[df['handle'].isin(top_ten_authors)], ax=ax)\nplt.show()","31983d44":"l  = top_ten_authors.to_list()\nl.remove('ThomasSowell')\nfig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(x=\"handle\", y=\"retweets\", data=df[df['handle'].isin(l)], ax=ax)\nplt.show()","3ab62e49":"l  = top_ten_authors.to_list()\nl.remove('ThomasSowell')\nfig, ax = plt.subplots(figsize=(15,10))\nsns.violinplot(x=\"handle\", y=\"likes\", data=df[df['handle'].isin(l)], ax=ax)\nplt.show()","21cd6fae":"# Ratio of Likes and Retweets\nfig, ax = plt.subplots(figsize=(15,10))\nsns.distplot(df.likes \/ (1 + df.retweets))","f7732024":"print('Mean of the ratio distribution', (df.likes \/ (1 + df.retweets)).mean(), '\\nStandard Deviation of the ratio distribution',(df.likes \/ (1 + df.retweets)).std())","48ef15d5":"sns.distplot(df[df.handle=='ThomasSowell'].likes)","46215c79":"print('Mean of the likes in ThomasSowell tweets', df[df.handle=='ThomasSowell'].likes.mean(), '\\nStandard Deviation of the likes in ThomasSowell tweets', df[df.handle=='ThomasSowell'].likes.std())","1dffd5e4":"fig, ax = plt.subplots(3, 3, figsize=(15,15))\nfor i in range(3):\n    for j in range(3):\n        sns.distplot(df[df.handle==top_ten_authors[i*3 + j  + 1]].likes, ax=ax[i][j])","8abb71e1":"for i in range(3):\n    for j in range(3):\n        print('Mean of the likes in', top_ten_authors[i*3 + j  + 1], 'tweets', df[df.handle==top_ten_authors[i*3 + j  + 1]].likes.mean(), '\\nStandard Deviation of the likes in', top_ten_authors[i*3 + j  + 1], 'tweets', df[df.handle==top_ten_authors[i*3 + j + 1]].likes.std(), '\\n')","1ec77e09":"plt.figure(figsize=(15,10))\ndis = df.groupby('handle').filter(lambda group: group.size > 10)\nsns.distplot(dis.groupby('handle').likes.mean() - dis.groupby('handle').likes.std())","d897ef59":"df.head()","9a28fa5d":"import matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter","1c9052c6":"df.created_at.head()","16de4f1d":"df.created_at = pd.to_datetime(df.created_at, format='%Y-%m-%d %H:%M:%S')","a59b16da":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(30, 15))\n\n# Add the x-axis and the y-axis to the plot\nax.plot(df.created_at,\n        df.likes, '-o',\n        color='purple')\n\n# Clean up the x axis dates (reviewed in lesson 4)\nax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\nax.xaxis.set_major_formatter(DateFormatter(\"%m\"))\n\nplt.show()","affd8bc8":"df['creation_month'] = df.created_at.dt.month\ndf['creation_day'] = df.created_at.dt.day\ndf['creation_year'] = df.created_at.dt.year\ndf['creation_hour'] = df.created_at.dt.hour","f7c5687d":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_month.unique()),\n        df.groupby('creation_month').likes.sum(), '-o',\n        color='purple')\n\nplt.show()","9fd23ee1":"plt.subplots(figsize=(15, 10))\nsns.countplot(df.creation_month)","76987348":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_month.unique()),\n        df.groupby('creation_month').likes.mean(), '-o',\n        color='purple')\n\nplt.show()","52dc8c4e":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_year.unique()),\n        df.groupby('creation_year').likes.sum(), '-o',\n        color='purple')\n\nplt.show()","031d1ae0":"plt.subplots(figsize=(15, 10))\nsns.countplot(df.creation_year)","e75b123f":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_year.unique()),\n        df.groupby('creation_year').likes.mean(), '-o',\n        color='purple')\n\nplt.show()","f58d0c15":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_hour.unique()),\n        df.groupby('creation_hour').likes.sum(), '-o',\n        color='purple')\n\nplt.show()","c2f4a0ec":"plt.subplots(figsize=(15, 10))\nsns.countplot(df.creation_hour)","4ed0fc2a":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_hour.unique()),\n        df.groupby('creation_hour').likes.mean(), '-o',\n        color='purple')\n\nplt.show()","78f83554":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_month, hue=df.creation_year)","a4bc58fd":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_day, hue=df.creation_year)","2163694c":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_hour, hue=df.creation_year)","734de1ee":"plt.subplots(figsize=(25, 15))\nsns.countplot(df.creation_day, hue=df.creation_month)","e3388c29":"sns.heatmap(df[['likes', 'creation_year', 'creation_month', 'creation_hour']].corr(), annot=True)","e82d6496":"fig, ax = plt.subplots(3, 3, figsize=(15,15))\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].plot(np.sort(df[df['handle'] == top_ten_authors[3 * i + j]].creation_year.unique()), df[df['handle'] == top_ten_authors[3 * i + j]].groupby('creation_year').likes.mean(), '-o', color='purple')","c191bb41":"from datetime import datetime","7c61d319":"(datetime.today() - df.groupby('handle').created_at.min()).dt.days","c34364f2":"sns.heatmap(pd.concat([df.groupby('handle').likes.mean(), (datetime.today() - df.groupby('handle').created_at.min()).dt.days], axis=1).corr(), annot=True)","21a93f29":"sns.heatmap(pd.concat([df.groupby('handle').likes.sum(), (datetime.today() - df.groupby('handle').created_at.min()).dt.days], axis=1).corr(), annot=True)","81419652":"from scipy import stats\nfrom scipy.ndimage.interpolation import shift\n","78b3665c":"## Outliers(\"Viral tweets\") are in yellow and else are in blue\nfig, ax = plt.subplots(3, 3, figsize=(30,15))\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].scatter(df[df['handle'] == top_ten_authors[3 * i + j]].created_at, df[df['handle'] == top_ten_authors[3 * i + j]].likes, facecolors='blue',alpha=.85, s=30)\n        ax[i][j].scatter(df[df['handle'] == top_ten_authors[3 * i + j]][(np.abs(stats.zscore(df[df['handle'] == top_ten_authors[3 * i + j]].likes)) > 3)].created_at, df[df['handle'] == top_ten_authors[3 * i + j]][(np.abs(stats.zscore(df[df['handle'] == top_ten_authors[3 * i + j]].likes)) > 3)].likes, color=\"yellow\")","e0a9288d":"def plot_avg_likes_between_viral_twts(author, ax):\n    t = df[df['handle'] == author].sort_values(by = 'created_at').reset_index()\n    idx = t[(np.abs(stats.zscore(t.likes)) > 3)].index\n    idx_created = t[(np.abs(stats.zscore(t.likes)) > 3)].created_at\n    x = t.created_at\n    y = [0 for i in range(t.shape[0])]\n    y2 = []\n    prev = 0\n    for i in idx:\n        m = t[prev:i].likes.mean()\n        for j in range(prev, i):\n            y[j] = m\n        y[i] = None\n        y2.append(t.iloc[i].likes)\n        prev = i + 1\n    ax.plot(x, y, linewidth=2)\n    ax.scatter(idx_created, y2, color=\"red\")\n    ax.set_title(author)","3b553bfe":"## The red ones are the viral tweets and the blue one is average of likes between two consecutive viral tweets\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        plot_avg_likes_between_viral_twts(top_ten_authors[3 * i + j], ax[i][j])","ae953c9c":"def change_in_likes_after_viral_twts(author, ax):\n    t = df[df['handle'] == author].sort_values(by = 'created_at').reset_index()\n    idx = t[(np.abs(stats.zscore(t.likes)) > 3)].index\n    delta_change = []\n    prev = 0\n    prev_del = 0\n    for i in idx:\n        m = t[prev:i].likes.mean() - prev_del\n        if(np.isnan(m) == False):\n            delta_change.append(m)\n            prev_del =  t[prev:i].likes.mean()\n        prev = i + 1\n    sns.distplot(np.array(delta_change) - shift(delta_change, 1, cval=0), ax=ax).set_title(author)","f7d26bec":"## The red ones are the viral tweets and the blue one is average of likes between two consecutive viral tweets\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        change_in_likes_after_viral_twts(top_ten_authors[3 * i + j], ax[i][j])","8b62b86e":"## Duration between two consecutive tweets distribution\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        sns.distplot((df[df['handle'] == top_ten_authors[3 * i + j]].sort_values(by = 'created_at').created_at.diff() \/ np.timedelta64(1, 'h')).dropna(), ax=ax[i][j])","02b948b0":"## Correlation between duration between two tweets and difference of likes both of them have. \nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        temp = df[df['handle'] == top_ten_authors[3 * i + j]].sort_values(by = 'created_at')[['created_at', 'likes']].diff().dropna()\n        sns.heatmap(pd.concat([temp.created_at.dt.seconds, temp.likes], axis=1).corr(), ax=ax[i][j], annot=True).set_title(top_ten_authors[3 * i + j])","924db622":"df['word_count'] = df.tweet_content.str.len()","f60a2617":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.distplot(df.word_count, ax=ax)","153c8f68":"# Create the plot space upon which to plot the data\nfig, ax = plt.subplots(figsize=(15, 10))\n\n\n# Add the x-axis and the y-axis to the plot\nax.plot(np.sort(df.creation_year.unique()),\n        df.groupby('creation_year').word_count.mean(), '-o',\n        color='purple')\n\nplt.show()","6c722098":"## The red ones are the viral tweets and the blue one is average of likes between two consecutive viral tweets\nfig, ax = plt.subplots(3, 3, figsize=(30,20))\nfor i in range(3):\n    for j in range(3):\n        ax[i][j].plot(np.sort(df[df.handle == top_ten_authors[3*i + j]].creation_year.unique()), df[df.handle == top_ten_authors[3*i + j]].groupby('creation_year').word_count.mean(), '-o', color='purple')\n        ax[i][j].set_title(top_ten_authors[3*i + j])","e08cb771":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(df[['word_count', 'likes']].corr(), ax=ax, annot=True)","c6e58dd0":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","b8f39c43":"def plot_wordcloud(text, ax, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), title = None, title_size=40, image_color=False):\n    \"\"\"\n    Function Credit: https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n    \"\"\"\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        ax.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        ax.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        ax.imshow(wordcloud);\n        ax.set_title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    ax.axis('off');\n        \ndef plot_the_author(name, ax=plt):\n    author_tweets = df[df.handle == name].tweet_content\n    plot_wordcloud('\\n'.join(author_tweets), max_words=600, max_font_size=120,  title = name + ' tweets', title_size=20, figure_size=(10,12), ax=ax)","18501fcf":"fig, ax = plt.subplots(figsize=(15, 10))\nplot_wordcloud('\\n'.join(df.tweet_content), ax=ax)","e7ff9124":"fig, ax = plt.subplots(figsize=(15, 10))\nviral_tweets_all = df[(np.abs(stats.zscore(df.likes)) > 3)].tweet_content\nplot_wordcloud('\\n'.join(viral_tweets_all), ax=ax)","7134de56":"## Correlation between duration between two tweets and difference of likes both of them have. \nfig, ax = plt.subplots(3, 3, figsize=(25,20))\nfor i in range(3):\n    for j in range(3):\n        plot_the_author(top_ten_authors[3 * i + j], ax=ax[i][j])","596c463a":"def plot_viral_twts_cloud(name, ax):\n    author_tweets = df[df.handle == name].tweet_content\n    plot_wordcloud('\\n'.join(author_tweets), max_words=600, max_font_size=120,  title = name + ' tweets', title_size=20, figure_size=(10,12), ax=ax[0])\n    t = df[df['handle'] == name]\n    viral_tweets_content = t[(np.abs(stats.zscore(t.likes)) > 3)].tweet_content\n    t = df[(df['handle'] == name) & ~((df.creation_year ==2019) & (df.creation_month==9))] # Trying to remove recent tweets\n    least_liked_tweets = t.sort_values(by = 'likes').head(10).tweet_content\n    plot_wordcloud('\\n'.join(viral_tweets_content), max_words=600, max_font_size=120,  title = name + ' viral tweets', title_size=20, figure_size=(10,12), ax=ax[1])\n    plot_wordcloud('\\n'.join(least_liked_tweets), max_words=600, max_font_size=120,  title = name + ' least liked tweets', title_size=20, figure_size=(10,12), ax=ax[2])","15404ba5":"fig, ax = plt.subplots(5, 3, figsize=(30,20))\nfor i in range(5):\n    plot_viral_twts_cloud(top_ten_authors[i], ax[i])","e6dd3d42":"import nltk\nfrom tqdm import tqdm\ntqdm.pandas()","be634e69":"df['tweet_tokens'] = df['tweet_content'].progress_apply(nltk.word_tokenize)","463189d1":"en_stopwords = set(nltk.corpus.stopwords.words('english'))\ndf['tweet_tokens'] = df['tweet_tokens'].progress_apply(lambda x: [item for item in x if item not in en_stopwords])","df0f5e4c":"#function to filter for ADJ\/NN bigrams\ndef rightTypes(ngram):\n    if '-pron-' in ngram or 't' in ngram:\n        return False\n    for word in ngram:\n        if word in en_stopwords or word.isspace():\n            return False\n    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n    tags = nltk.pos_tag(ngram)\n    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n        return True\n    else:\n        return False\n#filter bigrams\n#filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n#function to filter for trigrams\ndef rightTypesTri(ngram):\n    if '-pron-' in ngram or 't' in ngram:\n        return False\n    for word in ngram:\n        if word in en_stopwords or word.isspace():\n            return False\n    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    tags = nltk.pos_tag(ngram)\n    if tags[0][1] in first_type and tags[2][1] in third_type:\n        return True\n    else:\n        return False\n#filter trigrams\n#filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]","14f30462":"def get_bigram_trigrams(tokens, title):\n    bigrams = nltk.collocations.BigramAssocMeasures()\n    trigrams = nltk.collocations.TrigramAssocMeasures()\n    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n    trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n    #bigrams\n    bigram_freq = bigramFinder.ngram_fd.items()\n    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n    #trigrams\n    trigram_freq = trigramFinder.ngram_fd.items()\n    trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n    filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n    filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n    print(title)\n    print(filtered_bi.head(20))\n    print(filtered_tri.head(20))\n    return bigramFinder, trigramFinder, bigrams, trigrams","8de3c774":"%time generic_bigrams_finders, generic_trigrams_finders, generic_bigrams, generic_trigrams = get_bigram_trigrams(np.concatenate(df.tweet_tokens.to_list()), title=\"Extracting generic Bi\/Tri-grams\")","f32d2040":"top_five_author_bigram_finder = [None for i in range(5)]\ntop_five_author_trigram_finder = [None for i in range(5)]\ntop_five_author_bigrams = [None for i in range(5)]\ntop_five_author_trigrams = [None for i in range(5)]\nfor i in range(5):\n     top_five_author_bigram_finder[i], top_five_author_trigram_finder[i], top_five_author_bigrams[i], top_five_author_trigrams[i] = get_bigram_trigrams(np.concatenate(df[df.handle == top_ten_authors[i]].tweet_tokens.to_list()), title=top_ten_authors[i])","c82c83db":"top_five_author_bigram_finder_v = [None for i in range(5)]\ntop_five_author_trigram_finder_v = [None for i in range(5)]\ntop_five_author_bigrams_v = [None for i in range(5)]\ntop_five_author_trigrams_v = [None for i in range(5)]\nfor i in range(5):\n    t = df[df['handle'] == top_ten_authors[i]]\n    top_five_author_bigram_finder_v[i], top_five_author_trigram_finder_v[i], top_five_author_bigrams_v[i], top_five_author_trigrams_v[i] = get_bigram_trigrams(np.concatenate(t[(np.abs(stats.zscore(t.likes)) > 3)].tweet_tokens.to_list()), title=top_ten_authors[i])","8a13d182":"def get_pointwise_mi_scores(bigramFinder, trigramFinder, bigrams, trigrams, title):\n    #filter for only those with more than 20 occurences\n    bigramFinder.apply_freq_filter(20)\n    trigramFinder.apply_freq_filter(20)\n    bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n    trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)\n    print('Exploring Point wise Mututal Information in bigrams and trigrams of ' + title)\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(bigramPMITable.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(trigramPMITable.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    return bigramPMITable, trigramPMITable","9dc10bab":"_, __ = get_pointwise_mi_scores(generic_bigrams_finders, generic_trigrams_finders, generic_bigrams, generic_trigrams, \"all the tweets\")","4bb1b15d":"for i in range(5):\n    _, __ = get_pointwise_mi_scores(top_five_author_bigram_finder[i], top_five_author_trigram_finder[i], top_five_author_bigrams[i], top_five_author_trigrams[i], top_ten_authors[i] + \"\\'s tweets\")","faa9fc96":"def get_t_scores( bigramFinder, trigramFinder, bigrams, trigrams, title):\n    bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n    trigramTtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.student_t)), columns=['trigram','t']).sort_values(by='t', ascending=False)\n    #filters\n    filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n    filteredT_tri = trigramTtable[trigramTtable.trigram.map(lambda x: rightTypesTri(x))]\n    print('Exploring t scores between the words in bigrams and trigrams of ' + title)\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(filteredT_bi.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    print(filteredT_tri.head(10))\n    print('-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')\n    return filteredT_bi, filteredT_tri","c946762f":"_, __ = get_t_scores(generic_bigrams_finders, generic_trigrams_finders, generic_bigrams, generic_trigrams, \"all the tweets\")","488e38c4":"for i in range(5):\n    _, __ = get_t_scores(top_five_author_bigram_finder[i], top_five_author_trigram_finder[i], top_five_author_bigrams[i], top_five_author_trigrams[i], top_ten_authors[i] + \"\\'s tweets\")","3692bd11":"from textblob import TextBlob","648cc14c":"def  get_sentiments(text):\n    s = TextBlob(text).sentiment\n    return  s.polarity ,s.subjectivity","5b9c1a4e":"df['polarity'], df['subjectivity'] = zip(*df['tweet_content'].progress_apply(get_sentiments))","db9ede8f":"df.head()","80ae73a0":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.distplot(df.polarity, ax=ax)","3bbff807":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.distplot(df.subjectivity, ax=ax)","47afba3c":"sns.heatmap(df[['subjectivity', 'polarity', 'likes']].corr(), annot=True)","a1a8d384":"for author in top_ten_authors:\n    print(\"Mean Polarity of \"+ author + \" is \" + str(df[df['handle'] == author].polarity.mean()) + ' and standard deviation is ' + str(df[df['handle'] == author].polarity.std()))","d5cda248":"for author in top_ten_authors:\n    print(\"Mean subjectivity of \"+ author +  \" is \" + str(df[df['handle'] == author].subjectivity.mean())  + ' and standard deviation is ' + str(df[df['handle'] == author].subjectivity.std()))","99522c02":"** Well there is no significant correlation between the profile duration on twitter and mean tweet. Though there is a small positive correlation in total tweet vs duration. But it is expected. My question was does author's presence has any significance i.e does it matter that author created account in 2008 and the one is 2019. As we have saw before that older tweets has lesser likes and hence older tweets has less impact on users. Also, there is almost no correlation between average likes and age of author on twitter. Both of these evidence strongly suggests that it doesn't matter what the author age on twitter.**","40872081":"# Time to focus on time\n* Relation between time and likes.\n* Likes distribution wrt timestamp for each author\n* Does time of tweet has any relation with likes?\n* Does author's presence on twitter effects the likes author would get?\n* How mean likes change over the time. \n* How mean tweet count change over the time. \n* Graph between the duration between two tweets (everyday presence) and it's relation with the likes. \n* Does viral tweets matter?","a5b4ebfb":"** The standard deviation of polarity and subjectivity of each author from top 10 authors are close.**","e6d4d7fa":"# Understanding the Author, Likes and Retweets\n* How much likes and retweets are related. \n* Value count of tweets of each author. \n* Do more tweets increase the mean likes an author got?\n* Variation of likes for each author.\n* Definition of a \"viral\" tweet. ","73d2c7e8":"** Aren't they look like poisson distribution with mean and std as lambda ** \n\n** Let's plot distribution as the difference between mean and standard deviation **","57ecdfc2":"** Let me clear some things here. &lt;Q> and &lt;\/Q> are the tags I have used when a author retweets a tweet with a comment. The author's comment is followed by &lt;Q> tag and then the content of the retweet comes which is followed by &lt;\/Q>. **\n\n** Most frequent bigrams are just @ <author's name>. That is a let down ** \n\n** Also trigrams has most of the trigrams regarding workout which might have come from a same twitter accout. That is why individual analysis is necessary. \n\n** Let's analysis bi\/tri grams of top 5 authors **","376a65e2":"** Negligible correlation between word count and number of likes. Might be because 280 words are not too much and hence most of the people can read it easily. **","5116e4a7":"** Twittter acocunts are from different countries. Some of them are from USA, some are from India and the rest are from the Europe and Africa**\n\n** But the count of tweets decrease and increase from the 5th hour to 13th hour (8 hours). Now the way it is decreasing it looks like it is sleeping time for the majority of authors. **\n\n** Even in the total likes graph, the graph is lowest between (5-10). It implies even the followers are sleeping at the time of tweet** \n\n** Also at the 16-18 hours, both the tweets count as well as aggregated hit peak. If we assume 5-13 is the sleeping time, we can say that 16-18 is noon time which means the lunch time.**","37608663":"** You may have seen few tuples which qualitaatively convey no information but have highest correlation\/mutual information. To find out significance of each tuple, we'll use t score and score the bi and tri grams **","14c7e496":"** Removing Stop words **","191bd1de":"** ThomasSowell is leading by a large margin.**\n\n** It's amazing how ThomasSowell has such a flatter distribution.**\n\n** Let's throw ThomasSowell out of the game.**","6ebc92d9":"** Here is something interesting, Likes per tweet is highest in 2014 and then decreases. It might be because number of users has increased drastically. **","efd56924":"** It can be observed that a \"viral\" tweet increases the likes by a very little. But bunch of viral tweets in close time do increase likes on future tweets. This is expected behaviour but I was expecting the affect to be more. The affect is short term as well as  little as compare to the viralness of the tweet **","03b5095c":"** High Correlation between number of Retweets and likes which is expected **","cc50160b":"** In few of the cases, viral tweets increases the avg amount of likes one gets. But sometimes, it might decrease them which came as a surprise to me. Moreover in most of the cases, the average of the distribution tends to zero. I am start having doubts on viral tweets. I thought they are always good but statistically they have a negligible relevance over a long term. We've seen previously that author's tweeter age also does not matter neither the author's number of tweet. Does that mean, only your content determines the likes you got? I don't know but will investigate it soon. **","e2a1c634":"** Aren't all the word clouds seem alike. Most frequent words of all the top 10 authors are almost same. They talk about the same content using different language. This also tells us motivational people (who focuses on individual responsibility) are more of less uses the same formulla to preach. **\n\n** Let's focus on viral tweet's wordcloud now. We are plotting wordcloud of all the viral tweets as well as word cloud of the least liked tweets of the author ** ","30e87023":"** You might be expecting this because I am. Words like \"will\", \"time\", \"life\", \"people\", \"mind\" and many more words which focuses more on individual responsibility are in abundance. **","4fee8316":"#### ThomasSowell","cf5fb912":"** Here polarity is the sentiment of a tweet. It is float which lies between [-1,1] where -1 is for completely negative tweet and 1 for totally positive tweet. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]. **","3422e591":"** Does \"viral\" tweets impact average likes? Logically, viral tweets(Tweets whose number of likes are greater than interquartile range) invite more user. But does it statistically matter? **","52e00aa5":"** I believe bigrams and the trigrams are a great window to get gist of an author's work. If you want to know more about more authors, just run your copy  and check them out. ** \n\n** Let's move forward to sentiment analysis. The last but not the least fascinating topic of the notebook.  I'm going to use [TextBlob](https:\/\/github.com\/sloria\/TextBlob) library which contains state-of-the NLP methods and is very easy to use.**\n\n* Distribution of subjectivity and polarity. \n* The correlation between subjectivity, polarity and likes. \n* Author wise subjectivity and polarity analysis. \n* Variation of polarity and subjectivity over the time. Both holistic as well as individual analysis. ","a7ad2322":"** Older tweets has less likes than newer ones. One reason might be the growing rate of Twitter which means more users which means more likes **","70443572":"** Let's checkout bi\/tri-grams of all the viral tweets ** ","b5eb27c6":"** So many things to unfold. But you can see that words like \"political\", \"unions\", \"argument\" etc political terms helped ThomasSowell getting viral tweets **\n\n** The difference is apparent in a few cases and obsure in some. It is better if you observe them by yourself. Also let me know if you found something interesting about the plot in comments. **\n\n** I'm targeting bigrams and trigrams for analysis the tweets. **\n\nInspiraton: [Data cleaning, Data Processing & Data Analysis](https:\/\/www.kaggle.com\/hsankesara\/data-cleaning-data-processing-data-analysis)","2dfa4a07":"** Now let's talk about author's tweeter presence which I will estimate using duration between two tweets **","dd444c93":"## Contact Me\n\n[<img src=\"http:\/\/i.imgur.com\/0o48UoR.png\" width=\"35\">](https:\/\/github.com\/Hsankesara\/)    [<img src=\"https:\/\/i.imgur.com\/0IdggSZ.png\" width=\"35\">](https:\/\/www.linkedin.com\/in\/heet-sankesara-72383a152\/)     [<img src=\"http:\/\/i.imgur.com\/tXSoThF.png\" width=\"35\">](https:\/\/twitter.com\/TheSankesara)   [<img src=\"https:\/\/loading.io\/s\/icon\/vzeour.svg\" width=\"35\">](https:\/\/www.kaggle.com\/hsankesara) [<img src=\"https:\/\/image.flaticon.com\/icons\/svg\/2111\/2111505.svg\" width=\"35\">](https:\/\/medium.com\/@heetsankesara3)","b5ce6cc5":"# The End\n\nI tried to understand every aspect of the tweets and along the way discovered many surprising results. I hope that you like this notebook. Feel free to copy and edit it for your own exploration.  \n\nThanks for reading this kernel. If you have any question, advice or doubt, please write it in the comments. If you like the kernel, Please leave an upvote. ","6241ab0b":"** These tweets are NaN because the author has posted only an image. I'll remove them as they has no significance at all in my analysis. **\n\n\n** Only one tweet has an author with NaN. Gonna drop it also **","ccadced9":"** This is the living proof of exponential growth of Twitter after 2014 **\n","5729ccdb":"** The likes per tweet is highest for the tweets that are created in january. As we observed previously, tweets has very low rate of getting time as it gets older. I might be reaching here but since most of these tweets are motivational and january is the time for New year Resolution, there might be chances that people follow motivational tweets in January more. As there new year reolution, there engagement with the tweet also declines in Feb and March. **","627ad649":"### Author Wise Tweets-time Analysis","59dace6e":"** Too much outliers. That is strange. Might be the \"viral\" factor of the internet**\n\n** The distribution looks like Poisson distribution for each author. **\n","93c90988":"#### Distribution of top 2 to 10 authors","1e39b03b":"** Even many of the top authors have started a less than a year ago. I wonder if your duration on twitter can affect your likes. Let's find out**","2d8fc398":"** Viral tweets usually do not have same bigrams\/trigrams which makes sense as they are far lesser in number that chance of such collision reduces drastically. Plus the viral tweet is something which appeals to not only your niche group of followers but also a general crowd. **\n\n** Let's Analyse the bigrams and trigrams now. Let's now find the bigrams and trigrams with highest pointwise mututal information. The main intuition is that it measures how much more likely the words co-occur than if they were independent. However, it is very sensitive to rare combination of words.**","fa4ba861":"Twitter is one of the biggest social media platforms of our time. In twitter, one can find from the thoughts of their favorite celebrities to the policies of their government. In short, twitter is one of the major ways to connect with people of every field. There are many twitter accounts which tweet many wise things reelated to self improvement there. In these kernels, I am going to analyse more than 30K+ tweets from 40 different such acccounts. Let's move forward and see what we can find. \n\nI have scrapped tweets of mainly these twitter handles:\n\n@naval, @Via_Benjamin, @RortyWitt, @EdLatimore, @StoopToRise, @AymPlanet, @Wealth_Theory, @alanhliang, @TradingNirvana, @modestproposal1, CrazyPolymath, @SentientBonobo, @webdevMason, @stoic_dilettant, @AJA_Cortes, @martyrmade, @PresentWitness_, @millstoic, @z3nblack, @TheChuChu_, @orangebook_, @yawyr_vk, @Noahpinion, @ThomasSowell, @LifeMathMoney, @DejaRu22, @lawsofaurelius, @48_quotes, @shl, @cryptoseneca, @paulg, @TheCreativeFury, @Kpaxs, @TheAncientSage, @DeeperThrill, @mmay3r, @DrRalphNap, @TheStoicEmperor, @uncannyinsights\n\nThere are other authors also in the dataset because one of the above mentioned account retweeted their tweets. \n\nYou can find the script I wrote for scrapping all these tweets [here](https:\/\/github.com\/Hsankesara\/Tweets-Scrapper). You can also find the same dataset at Github. [Click here](https:\/\/github.com\/Hsankesara\/The-Tweets-of-Wisdom) to go there. \nLet's get started....","e3f4ed1c":"# Analysing the content inside the tweet\n\n* wordcount distribution and correlation with the likes.\n* Wordcloud for each author. \n* Understanding what makes some tweets viral and some not so much. \n* Bigrams and trigram analysis of viral tweets. \n* What are the sentiments of tweets. What kind  of sentiments make a tweet viral. \n* variation of author's word count and word cloud with respect to time. ","4c4884c7":"# Introduction\n* Describe the data\n* Understanding each column\n* Removing NAN","985cf89e":"** Technically, a negative correlation means smaller the duration, greater the difference between likes. Vice versa for the positive correlation. But most of the correlation is too close to zero. It implies that duration between two tweets cannot influence the likes of the latter one. **","eeac60ca":"** Oh! Mean revolves around zero with small deviation. We can estimate the curve as a poisson distribution. Let's see if we can use it in the future. **","8a2dcae4":"** A great rise in the number of tweets as well as sum of likes in the month of Aug and Sept **","3a9bf194":"** It can be easily seen that after 2017, the average tweet length jump drastically **\n\n** Most of the authors has a very little change in word count. Some even has decrease the average word count with respect to time. ** ","b4513501":"**Negligible Correlation between likes and number of tweets which came as a surprise. It means your content matter more than the number of tweets. Quality over Quantity**","6c5d19fc":"** Twitter has 280 word limit on a tweet. On 7 November 2017, twitter increased it's character limits from 124-280. It will be interesting to see that change in the dataset ** ","a316a7ae":"** Likes has no correlation with the polarity\/subjectivity and likes. Polarity and subjectivity has some positive correlation. **"}}