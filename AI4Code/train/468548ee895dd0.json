{"cell_type":{"18ada0d3":"code","980e4f2c":"code","04af7e2e":"code","5d009c35":"markdown","cc200a2b":"markdown"},"source":{"18ada0d3":"# https:\/\/github.com\/dragen1860\/TensorFlow-2.x-Tutorials\/tree\/master\/02-AutoGraph","980e4f2c":"import tensorflow as tf\n\nReLU_Layer = tf.keras.layers.Dense(100, input_shape=(784,), activation=tf.nn.relu)\nLogit_Layer = tf.keras.layers.Dense(10, input_shape=(100,))\n\n# X and y are labels and inputs","04af7e2e":"# SGD_Trainer = tf.optimizers.SGD(1e-2)\n\n# @tf.function\n# def loss_fn(inputs=X, labels=y):\n#     hidden = ReLU_Layer(inputs)\n#     logits = Logit_Layer(hidden)\n#     entropy = tf.nn.softmax_cross_entropy_with_logits(\n#         logits=logits, labels=labels)\n#     return tf.reduce_mean(entropy)\n\n# for step in range(1000):\n#     SGD_Trainer.minimize(loss_fn, \n#         var_list=ReLU_Layer.weights+Logit_Layer.weights)","5d009c35":"**TensorFlow 2.0:** Operations are executed directly and the computational graph is built on-the-fly. However, we can still write functions and pre-compile computational graphs from them like in TF 1.0 using the @tf.function decorator, allowing for faster execution.","cc200a2b":"## AutoGraph\nCompare static graph using @tf.function VS dynamic graph.\n\nAutoGraph helps you write complicated graph code using normal Python. Behind the scenes, AutoGraph automatically transforms your code into the equivalent TensorFlow graph code.\n\nLet's take a look at TensorFlow graphs and how they work."}}