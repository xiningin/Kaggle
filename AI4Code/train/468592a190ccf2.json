{"cell_type":{"e068ab2b":"code","7aed3604":"code","a0fdb4b5":"code","ba3326c4":"code","1f7797d4":"code","ceb50afa":"code","bdf9e6fa":"code","be53e15d":"code","62ed8b77":"code","42b6cb2b":"code","88621118":"code","2b17f456":"code","42068da5":"code","d1fd5b98":"code","448d25c2":"code","8519430b":"code","741ded5c":"code","c9b67862":"code","27530102":"code","f7083c21":"code","49a2f6df":"code","0c4492c3":"code","5c293a67":"code","f4c3486f":"code","6959fe5f":"code","113d9e22":"code","994c108c":"code","5723f116":"code","1fab9ee6":"code","aed25ef9":"code","871274ec":"code","b25a9869":"code","ea408114":"code","8e27019d":"code","e8b4b3b7":"code","b2505a2b":"code","6c0df00b":"code","a6b8e856":"code","2e2ed7a0":"code","cb45cb91":"code","48493b84":"code","2b5eff84":"code","d58a0011":"code","36769c07":"code","fb0a752c":"code","ef814e73":"markdown","5147dc7e":"markdown","a256da33":"markdown","e03e3bfb":"markdown","667e0f36":"markdown","bb2a266a":"markdown","df6ba14f":"markdown","ce94551c":"markdown","ff50e8b1":"markdown","b37a393d":"markdown","9b7e098a":"markdown","ad911251":"markdown","af10877e":"markdown","9181ed09":"markdown","0fdd94ab":"markdown","c7969d23":"markdown","070c50d2":"markdown","842f1945":"markdown","1e2e8808":"markdown","995b138b":"markdown","e3f524bf":"markdown","32632011":"markdown","cad06a5b":"markdown","9c2481c6":"markdown","4ee7d3da":"markdown","3e922620":"markdown","03eaa80f":"markdown","90a76fba":"markdown"},"source":{"e068ab2b":"import pandas as pd \nimport numpy as np \nfrom datetime import datetime\nimport sys\nimport ast\n\nimport plotly_express as px\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nimport networkx\nfrom networkx.algorithms.components.connected import connected_components\n\nimport json\nimport dask.bag as db\n\n\n","7aed3604":"import utils","a0fdb4b5":"ai_category_list=['stat.ML','cs.LG','cs.AI']\nrecords=db.read_text(\"\/kaggle\/input\/arxiv\/*.json\").map(lambda x:json.loads(x))\nai_docs = (records.filter(lambda x:any(ele in x['categories'] for ele in ai_category_list)==True))\nget_metadata = lambda x: {'id': x['id'],\n                  'title': x['title'],\n                  'category':x['categories'],\n                  'abstract':x['abstract'],\n                 'version':x['versions'][-1]['created'],\n                         'doi':x[\"doi\"],\n                         'authors_parsed':x['authors_parsed']}\n\ndata=ai_docs.map(get_metadata).to_dataframe().compute()\n\ndata.to_excel(\"AI_ML_ArXiv_Papers.xlsx\",index=False,encoding=\"utf-8\")","ba3326c4":"print(\"Number of Papers Related to AI and ML is \",data.shape[0])","1f7797d4":"data.head()","ceb50afa":"data['DateTime']=pd.to_datetime(data['version'])\ndata.head()","bdf9e6fa":"data=utils.extractDateFeatures(data,\"DateTime\")\ndata.head()","be53e15d":"data['num_authors']=data['authors_parsed'].apply(lambda x:len(x))","62ed8b77":"data['authors']=data['authors_parsed'].apply(lambda authors:[(\" \".join(author)).strip() for author in authors])\ndata.head()","42b6cb2b":"print(\"Number of Papers with No DOI \",data[pd.isnull(data['doi'])].shape[0])","88621118":"\npapers_over_years=data.groupby(['Year']).size().reset_index().rename(columns={0:'Number Of Papers Published'})\npx.line(x=\"Year\",y=\"Number Of Papers Published\",data_frame=papers_over_years,title=\"Growth of AI ML over the Years\")","2b17f456":"papers_published_over_days=data.groupby(['Date']).size().reset_index().rename(columns={0:'Papers Published By Date'})\npx.line(x=\"Date\",y=\"Papers Published By Date\",data_frame=papers_published_over_days,title=\"Average Papers Published Over Each Day\")","42068da5":"ai_authors=pd.DataFrame(utils.flattenList(data['authors'].tolist())).rename(columns={0:'authors'})\npapers_by_authors=ai_authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False).head(20)\npx.bar(x=\"Number of Papers Published\",y=\"authors\",data_frame=papers_by_authors.sort_values(\"Number of Papers Published\",ascending=True),title=\"Top 20 Popular Authors\",orientation=\"h\")","d1fd5b98":"data['is_bengio_author']=data['authors'].apply(lambda x:1 if \"Bengio Yoshua\" in x else 0)\nbengio_papers=data[data['is_bengio_author']==1]\nbengio_papers=bengio_papers.reset_index(drop=True)\n\nprint(\"Number of Papers by Bengio Yoshua on Arxiv is \",bengio_papers.shape[0])","448d25c2":"print(\"Bengio Yoshua Published His First Paper in \",min(bengio_papers['Date']))\nprint(\"Bengio Yoshua Published His Recent Paper in \",max(bengio_papers['Date']))\n","8519430b":"bengio_papers_by_year=bengio_papers.groupby(['Year']).size().reset_index().rename(columns={0:'Number of Papers Published'})\n\npx.bar(x=\"Year\",y=\"Number of Papers Published\",title=\"Papers by Bengio Yoshua Over Years\",data_frame=bengio_papers_by_year)","741ded5c":"print(\"Average Papers Published in a Year By Bengio Yoshua \",np.median(bengio_papers_by_year['Number of Papers Published']))","c9b67862":"titles=bengio_papers['title'].tolist()\nstop_words = set(stopwords.words('english')) \ntitles=[title.lower() for title in titles] ### Lower Casing the Title\ntitles=[utils.removeStopWords(title,stop_words) for title in titles]","27530102":"\nbigrams_list=[\" \".join(utils.generateNGram(title,2)) for title in titles]\ntopn=50\ntop_bigrams=utils.getMostCommon(bigrams_list,topn=topn)\ntop_bigrams_df=pd.DataFrame()\ntop_bigrams_df['words']=[val[0] for val in top_bigrams]\ntop_bigrams_df['Frequency']=[val[1] for val in top_bigrams]\npx.bar(data_frame=top_bigrams_df.sort_values(\"Frequency\",ascending=True),x=\"Frequency\",y=\"words\",orientation=\"h\",title=\"Top \"+str(topn)+\" Bigrams in Papers by Bengio Yoshua\")","f7083c21":"trigrams_list=[\" \".join(utils.generateNGram(title.replace(\":\",\"\"),3)) for title in titles]\ntopn=50\ntop_trigrams=utils.getMostCommon(trigrams_list,topn=topn)\ntop_trigrams_df=pd.DataFrame()\ntop_trigrams_df['words']=[val[0] for val in top_trigrams]\ntop_trigrams_df['Frequency']=[val[1] for val in top_trigrams]\ntop_trigrams_df=top_trigrams_df[top_trigrams_df[\"words\"]!=\"\"]\npx.bar(data_frame=top_trigrams_df.sort_values(\"Frequency\",ascending=True),x=\"Frequency\",y=\"words\",orientation=\"h\",title=\"Top \"+str(topn)+\" Trigrams in Papers by Bengio Yoshua\")","49a2f6df":"from pprint import pprint\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim ","0c4492c3":"'''\nThe tokenise function will lowercase, and tokenise the sentences\n'''\n\ndef tokenise(sentences):\n    return [gensim.utils.simple_preprocess(sentence, deacc=True,max_len=50) for sentence in sentences]\n","5c293a67":"tokenised_sentences=tokenise(bengio_papers['title'].tolist())\ntokenised_sentences[0]","f4c3486f":"nlp = spacy.load('en')","6959fe5f":"def lemmatise(sentence,stop_words,allowed_postags=None):\n    doc=nlp(sentence)\n    #print(sentence)\n    if allowed_postags!=None:\n        tokens = [token.lemma_ for token in doc if (token.pos_ in allowed_postags) and (token.text not in stop_words)]\n    if allowed_postags==None:\n        tokens= [token.lemma_ for token in doc if (token.text not in stop_words)]\n    return tokens","113d9e22":"stop_words = spacy.lang.en.stop_words.STOP_WORDS","994c108c":"sentences=[\" \".join(tokenised_sentence) for tokenised_sentence in tokenised_sentences]\nlemmatised_sentences=[lemmatise(sentence,stop_words) for sentence in sentences]\nlemmatised_sentences[0]","5723f116":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(lemmatised_sentences,min_count=2) \ntrigram = gensim.models.Phrases(bigram[lemmatised_sentences],min_count=2)  \n\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","1fab9ee6":"bigrams_words=[bigram_mod[sentence] for sentence in lemmatised_sentences]\n\ntrigrams_words=[trigram_mod[sentence] for sentence in bigrams_words]\n","aed25ef9":"id2word = corpora.Dictionary(trigrams_words)\ncorpus = [id2word.doc2bow(text) for text in trigrams_words]\n[(id2word[id], freq) for id, freq in corpus[0]] ","871274ec":"def compute_coherence_values(id2word, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=20,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","b25a9869":"models,coherence=compute_coherence_values(id2word,corpus,trigrams_words,limit=20,start=2,step=2)\nx = range(2, 20, 2)\nplt.plot(x, coherence)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","ea408114":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=6, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=20,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","8e27019d":"pprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","e8b4b3b7":"#pyLDAvis.enable_notebook()\n#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n#vis","b2505a2b":"print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=trigrams_words, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","6c0df00b":"def format_topics_sentences(texts,ldamodel=lda_model, corpus=corpus):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\n","a6b8e856":"df_topic_sents_keywords = format_topics_sentences(bengio_papers['title'].tolist(),ldamodel=lda_model, corpus=corpus)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","2e2ed7a0":"topic_counts=df_dominant_topic['Dominant_Topic'].value_counts().reset_index().rename(columns={'index':'Topic','Dominant_Topic':'Number of Documents'})\ntopic_counts['percentage_contribution']=(topic_counts['Number of Documents']\/topic_counts['Number of Documents'].sum())*100\ntopic_counts","cb45cb91":"# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\n\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n","48493b84":"sent_topics_df=pd.DataFrame()\nsent_topics_df['Text']=bengio_papers['title'].tolist()\nsent_topics_df['tsne_x']=tsne_lda[:,0]\nsent_topics_df['tsne_y']=tsne_lda[:,1]\nsent_topics_df['Topic_No']=topic_num\nsent_topics_df=pd.merge(sent_topics_df,df_dominant_topic,on=\"Text\")\nsent_topics_df.head()","2b5eff84":"px.scatter(x='tsne_x',y='tsne_y',data_frame=sent_topics_df,color=\"Topic_No\",hover_data=[\"Topic_Perc_Contrib\"])","d58a0011":"bengio_papers=pd.merge(bengio_papers,df_dominant_topic.rename(columns={'Text':'title'}),on='title')\n\nnum_topics=bengio_papers['Dominant_Topic'].nunique()\nauthors_df_list=[]\n\nfor topic_no in range(num_topics):\n    \n\n    temp=bengio_papers[bengio_papers['Dominant_Topic']==topic_no]\n    authors=pd.DataFrame(utils.flattenList(temp['authors'].tolist())).rename(columns={0:'authors'})\n    authors=authors[authors['authors']!=\"Bengio Yoshua\"]\n    papers_authors=authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False).head(10)\n    papers_authors['Topic No']=topic_no\n    authors_df_list.append(papers_authors)\n\nco_occurring_authors=pd.concat(authors_df_list)\n","36769c07":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go","fb0a752c":"fig = make_subplots(rows=3, cols=2)\nrow=1\ncol=1\nfor topic_no in range(num_topics):\n    \n    wp = lda_model.show_topic(topic_no)\n    topic_keywords = \", \".join([word for word, prop in wp])\n    temp=co_occurring_authors.loc[co_occurring_authors['Topic No']==topic_no].sort_values(\"Number of Papers Published\",ascending=True)\n\n    fig.add_trace(\n    go.Bar(\n        x=temp['Number of Papers Published'],\n        y=temp['authors'],\n        orientation='h',\n        name=\"Topic \"+str(topic_no)\n        #mode=\"markers+text\",\n        #text=[\"Text A\", \"Text B\", \"Text C\"],\n        #textposition=\"bottom center\"\n    ),\n    row=row, col=col)\n    if col%2==0:\n        row=row+1\n        col=1\n    else:\n        col=col+1\nfig.update_layout(height=1000, width=1200, title_text=\"Top 10 Authors With Whom Bengio Worked Across Different Topics\")\n\nfig.show()\n\n","ef814e73":"#### Creating Dictionary and Corpus ","5147dc7e":"From one published paper over each day, in the last one year there have been around 100 papers published each day. In Mar2013, there is a jump in number of papers published. Also, 2013 was the year, when the paper on Word2Vec was published - this was a new beginning in the field of NLP","a256da33":"# Introduction\n\nThe idea of this notebook is to understand the growth of ML and AI from research perspective. How has the focus of various subjects changed over time - is there a change.We will also analyse the work of Bengio Yoshua, who is considered the Godfather of AI\n\nWhat are the phenomenal papers that has impacted this field. Before we do that, let us extract papers related to AI and ML from the Arxiv Repository","e03e3bfb":"From 2010, there has been an exponential growth in this field - and this is continuously increasing over the period of time","667e0f36":"### Extracting the Date Time Information","bb2a266a":"### Cleaning the ***authors_parsed*** column\n\n\n1. Concatenating the authors first and last names.","df6ba14f":"There are topics related to GAN and adversial networks in relation to speech and images.There are topics related to hypergraph and Deep Reinforcement Learning as well.\n\nLet us now assign, each document to a Topics - a document may consists of more than one topic, but we will assign it the dominant topic","ce94551c":"\n###  What are the topics in which Bengio Yoshua has published papers in?\n\nTo look at topics at a broad level, we can Build a Frequency Bar Plot to understand key words used in the titles of the papers published.\n\nBefore we look at the top words in the Title, we will have to do some cleaning of the title - Removing Stop Words, Lower Casing the Words. Let us not do any stemming or lemmatization","ff50e8b1":"## Growth in Field of ML AI ","b37a393d":"As we can there has been a lot of Papers on Recurrent Neural Networks and Reinforcement Learning by Bengio Yoshua. Also, his research areas are also focussed on Neural Machine Translations and Understanding Stochastic Gradients. The top words also, show us that Bengio has worked on various topics in Deep Learning as part of his research - The next question arises is can we categorise his work? And also can we see who are the authors he works predominantly with for each of the categories we have identified","9b7e098a":"Around 6 topics seem a good number","ad911251":"# Data Preprocessing\n\nSome preprocessing steps that we need to perform are:\n\n1. Extract the Date Time information from version column\n\n\n2. The authors parsed information, first and last names need to be concatenated to get one name.\n\n3. Handling Missing DOI's\n\n4. We need to look for any possible duplication in the title names\n","af10877e":"# Extracting AI ML Papers from Arxiv Repo","9181ed09":"Though Bengio, had entered the field of AI ML in the 1990's the first paper published by him on Arxiv is in September of 2010 and his most recent paper is in August 2020. In 10 years, he has published 311 papers - Astounding Rate of Publication. It may be possible that his other papers are tagged into other categories on Arxiv that we are not considering for this analysis","0fdd94ab":"We can see that number of documents is each topic is almost equally distributed.. Let us use T-SNE to visualise the topics vs document distribution\n","c7969d23":"# Conclusion and Future Works\n\nIn this Analysis, we started off with analysing the set of AI and ML Papers in the Arxiv Repository. And then we explored the Worked of Bengio Yoshua. As a part of Future Work we can \n\n1. Use Abstract Information for more indepth topic Analysis\n2. Can we build a co-citation network and analyse similar authors\n3. We can build a Topic Model on Entire Dataset to understand how each topic has evolved over time","070c50d2":"The data contains the id, the title,the category the paper belongs to, the date when the version was created and list of authors ","842f1945":"### Missing DOI \n\nIn the Data, we can see that there are papers with no doi - Since Arxiv is a pre-print server, once the paper is published DOI is received. This DOI needs to be updated to Arxiv. In cases where there are no DOI - probably they were not published in any other journal or the author forgot to update the doi - hence there is no DOI available\n\n(Reference : https:\/\/academia.stackexchange.com\/questions\/62480\/why-does-arxiv-org-not-assign-dois)","1e2e8808":"# Analysing the Data\n\n1. How has the field of ML\/AI grown over the years?\n\n2. Who have been the most successful Authors?\n\n3. What are the different topics being spoken about  - and how this has changed over the years?\n\n4. Can we cluster papers based on their Abstract and Title? ","995b138b":"Across Topics, Bengio has published papers with Courville Aaron. The other authors are quite distinct across Topics.Courville Aaron is a part of LISA lab along with Bengiom. Except for Topic 2 which talks about self taught deep neural networks and causal networks, the top author with whom Bengio has published his papers with is Courville Aaraon","e3f524bf":"## Analyse the Papers published by Bengio Yoshua \n\nBengio Yoshua, is well known for his work on Artifical Neural Networks and Deep Learning. Bengio along with Geoffrey Hinton and Yann LeCun are reffered to as the \"Godfathers of AI\". Let us look at what kind of research Bengio has been involved him and understand his contributions to this field - that led him to win the Turing Award\n","32632011":"Aroung 88% of the papers have no DOI - the authors most probably didnt update this information. ","cad06a5b":"### Has Bengio Worked with Different Authors on Different Topics? Who is the most popular Co-Author across different topics?","9c2481c6":"### Topic Modelling to Understand Different Themes\n","4ee7d3da":"The topics are very well seperated as we can see from TSNE","3e922620":"## Who has published most papers in AI ML Space","03eaa80f":"**Bengio Yoshua, Canadian Scientist** who won the Turing Award in 2018 Leads the Popular Authors. There are also a lot of Chinese Authors in the Top 20 list\n","90a76fba":"#### Building Bigrams and Trigrams "}}