{"cell_type":{"4d6d228e":"code","03ab4989":"code","429e83b5":"code","aae046dc":"code","497c1854":"code","98b70d00":"code","c897419e":"code","86a42650":"code","a9dd83c6":"code","1ecd0e47":"code","aff607ad":"code","185d85ee":"code","e8668525":"code","e01b7a90":"code","2c243b91":"code","9514ec11":"code","0875cabc":"code","749287dd":"code","7b1c497b":"code","179bb0b5":"code","9a6dc964":"code","4cea053e":"code","c377fd2c":"code","2f4ad902":"code","b28d703e":"code","0e92a224":"code","26ca45ce":"code","54813097":"code","f0e53d40":"code","d386fb2b":"code","16a995fc":"code","94a6c243":"code","553d062a":"code","529f5376":"code","2e381678":"code","49b38af5":"code","84e858ca":"markdown","dc606c1d":"markdown","06f4cab4":"markdown","f174e8ac":"markdown","fe461ef7":"markdown","4f5fde85":"markdown","7770d67f":"markdown","885784fb":"markdown","ab9cedb7":"markdown","740d08be":"markdown","4e6d6fa2":"markdown","7833f009":"markdown","da7a53eb":"markdown","f1d053b5":"markdown","04a11188":"markdown","6617d9c5":"markdown","f8e32d8b":"markdown","4c0712a4":"markdown","abf84b71":"markdown","fab4fd86":"markdown"},"source":{"4d6d228e":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')","03ab4989":"train = pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest = pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\nsubmission = pd.read_csv(r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col='Id')","429e83b5":"display('Train data shape: {}'.format(train.shape))\ndisplay('Test data shape: {}'.format(test.shape))","aae046dc":"df_train = train.copy()\ndf_test = test.copy()\n\ny = df_train.SalePrice              \ndf_train.drop(['SalePrice'], axis=1, inplace=True)\ndf_train.head()","497c1854":"df_train.info()","98b70d00":"df_train.describe().style.format(\"{:.2f}\")","c897419e":"## source for this snippet is @https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html\n\ndef highlight_max(data, color='red'):\n    '''\n    highlight the maximum in a Series or DataFrame\n    '''\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n        is_max = data == data.max()\n        return [attr if v else '' for v in is_max]\n    else:  # from .apply(axis=None)\n        is_max = data == data.max().max()\n        return pd.DataFrame(np.where(is_max, attr, ''),\n                            index=data.index, columns=data.columns)","86a42650":"# zoom-in to few interesting features only\n# train data\ninteresting_features =['LotArea', 'OverallQual','TotRmsAbvGrd', 'GarageCars', 'YearRemodAdd', 'YearBuilt','GarageYrBlt', 'SalePrice']\nDF = train.describe(include=np.number)[interesting_features].style.format(\"{:.1f}\")\nDF.format({\"YearRemodAdd\": \"{:.0f}\",\"YearBuilt\": \"{:.0f}\",\"GarageYrBlt\": \"{:.0f}\"})","a9dd83c6":"# test data\ninteresting_features =['LotArea', 'OverallQual','TotRmsAbvGrd', 'GarageCars', 'YearRemodAdd', 'YearBuilt','GarageYrBlt']\nDF = test.describe(include=np.number)[interesting_features].style.format(\"{:.1f}\").apply(highlight_max, subset=['GarageYrBlt'], color='red')\nDF.format({\"YearRemodAdd\": \"{:.0f}\",\"YearBuilt\": \"{:.0f}\", \"GarageYrBlt\": \"{:.0f}\"})","1ecd0e47":"test[test['GarageYrBlt'] > 2010]['GarageYrBlt']","aff607ad":"display('Null values in each column: trian data')\ndf_train.isnull().sum().sort_values(ascending=False).head()","185d85ee":"display('Null values in each column: test data')\ndf_test.isnull().sum().sort_values(ascending=False).head()","e8668525":"# train_data\nnull_values_train = []\nfor col in df_train.columns:\n    if df_train[col].isna().sum() != 0:\n        pct_na = np.round((100 * (df_train[col].isna().sum())\/len(df_train)), 2)            \n        dict2 ={\n            'Features' : col,\n            'NA_train (count)': df_train[col].isna().sum(),\n            'NA_trian (%)': pct_na\n        }\n        null_values_train.append(dict2)\nDF1 = pd.DataFrame(null_values_train, index=None).sort_values(by='NA_train (count)',ascending=False)\n\n\n# test_data\nnull_values_test = []\nfor col in df_test.columns:\n    if df_test[col].isna().sum() != 0:\n        pct_na = np.round((100 * (df_test[col].isna().sum())\/len(df_test)), 2)            \n        dict1 ={\n            'Features' : col,\n            'NA_test (count)': df_test[col].isna().sum(),\n            'NA_test (%)': pct_na\n        }\n        null_values_test.append(dict1)\nDF2 = pd.DataFrame(null_values_test, index=None).sort_values(by='NA_test (count)',ascending=False)\n\npd.merge(DF1, DF2, how='outer', on=['Features']).style.format(None, na_rep=\"-\")","e01b7a90":"def null_value_percentage_plot(data):\n    '''Given a dataframe, this fuction calculates and plot the number of \n    null values in each columns as a percentage\n    \n    input: data\n    output: seaborn horizontal barplot\n    \n    '''\n    null_values = []\n    for col in data.columns:\n        if data[col].isna().sum() != 0:\n            pct_na = np.round((100 * (data[col].isna().sum())\/len(data)), 2)            \n            dict ={\n                'Column' : col,\n                'Null value': data[col].isna().sum(),\n                'Percent null value': pct_na\n            }\n            null_values.append(dict)\n\n    z = pd.DataFrame(null_values, index=None)    \n    fig = plt.figure(figsize=(12,8))\n    #fig.subplots_adjust(top=0.89)\n    #sns.set_style(\"dark\")\n    ax = sns.barplot(y=\"Column\", x=\"Percent null value\", \n                     data=z.sort_values('Percent null value', ascending=False).head(15),\n                     palette='gist_earth',\n                     orient='h')\n    ax.set_title(\"Pecent null value in columns (top 15 displayed)\", fontsize=20, y=1.05)\n    return ax","2c243b91":"null_value_percentage_plot(df_train)","9514ec11":"# separte the columns into numerical and categorical\n\ncat_features =[]\nnum_features =[]\n\nfor col in df_train.columns:\n    if df_train[col].dtype=='object':\n        cat_features.append(col)\n    else:\n        num_features.append(col)\n\n# group columns according to cardinality\n\nlow_cardinal_cols = []\nmed_cardinal_cols = []\nhigh_cardinal_cols = []\n\nfor col in cat_features:\n    if df_train[col].nunique() < 6:\n        low_cardinal_cols.append(col)\n    elif df_train[col].nunique() < 9:\n        med_cardinal_cols.append(col)\n    else:\n        high_cardinal_cols.append(col)\n\n# display the values\n\ndisplay(\"low_cardinal_cols\")\ndisplay(low_cardinal_cols)\ndisplay(\"med_cardinal_cols\")\ndisplay(med_cardinal_cols)\ndisplay(\"high_cardinal_cols\")\ndisplay(high_cardinal_cols)","0875cabc":"def count_plot_pct(df_train, df_test, cols, titleText, figsize=(24,26)):\n    L = len(cols)\n    nrow= int(np.ceil(L\/3))\n    ncol= 3\n    \n    remove_last= (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=figsize, facecolor=None, sharex=True)\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.97)\n    itr = 1\n    for col in cols:\n        total = float(len(df_train[cols])) \n        plt.subplot(nrow, ncol, itr)\n        ax = sns.countplot(y=col, color=\"#5F6664\", data=df_train[cols], alpha=0.5, label='train')\n        ax = sns.countplot(y=col, color=\"#600000\", data=df_test[cols], alpha =0.5, label='test')\n        ax.set_xlabel('') \n        sns.despine(top=True, right=True, left=False, bottom=False, offset=5, trim=False)\n        plt.legend()\n        itr += 1\n    plt.suptitle(titleText ,fontsize = 24, y=1.002)\n    fig.text(0.5, 0.085, 'counts', ha='center')\n    plt.show()    \n    ","749287dd":"count_plot_pct(df_train, df_test, low_cardinal_cols, \"Unique categories in low cardinal columns\")","7b1c497b":"count_plot_pct(df_train, df_test, med_cardinal_cols, \"Unique categories in medium cardinal columns\", figsize=(24, 20))","179bb0b5":"count_plot_pct(df_train, df_test, high_cardinal_cols, \"Unique categories in high cardinal columns\", figsize=(24, 16))","9a6dc964":"# train_data\nunique_cat_train = []\nfor col in df_train[cat_features]:\n    unique_train = df_train[col].nunique()  \n    dict1 ={\n        'Features' : col,\n        'Unique cats (train)': unique_train,        \n    }\n    unique_cat_train.append(dict1)\nDF1 = pd.DataFrame(unique_cat_train, index=None).sort_values(by='Unique cats (train)',ascending=False)\n\n# test_data\nunique_cat_test = []\nfor col in df_test[cat_features]:\n    unique_test = df_test[col].nunique()    \n    dict2 ={\n        'Features' : col,\n        'Unique cats (test)': unique_test,        \n    }\n    unique_cat_test.append(dict2)\nDF2 = pd.DataFrame(unique_cat_test, index=None).sort_values(by='Unique cats (test)',ascending=False)\n\npd.merge(DF1, DF2, how='outer', on=['Features']).style.format(None, na_rep=\"-\")","4cea053e":"plt.figure()\nfig, ax = plt.subplots(12, 3,figsize=(20, 46))\nfig.subplots_adjust(top=0.96)\nitr = 1\nfor feature in num_features:\n    plt.subplot(12, 3, itr)\n    ax = sns.histplot(df_train[feature], color=\"#ff9900\", label='train')\n    ax = sns.histplot(df_test[feature], color=\"#4da6ff\", label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    itr += 1\nplt.suptitle('Numerical features', fontsize=20)\nplt.show()","c377fd2c":"median_ = train['SalePrice'].median()\nmode_ = train['SalePrice'].mode()\nmean_ = train['SalePrice'].mean()\ndisplay(mode_, median_, mean_)","2f4ad902":"train['SalePrice'].describe()","b28d703e":"fig, axes = plt.subplots(1, 2, figsize=(20, 5))\nfig.suptitle('Normality Check of SalePrice distribution', fontsize=16)\n\nsns.histplot(y, bins=100,  kde=True, color='#214e69', ax=axes[0])\naxes[0].set_title('')\n\nsm.qqplot(y, line='s', ax=axes[1], color='#214e69')\naxes[1].set_title('')","0e92a224":"correlation_table = []\nfor cols in num_features:\n    y = train['SalePrice']\n    x = df_train[cols]\n    corr = np.corrcoef(x, y)[1][0]\n    dict ={\n        'Features': cols,\n        'Correlation coefficient' : corr,\n        'Feat_type': 'numerical'\n    }\n    correlation_table.append(dict)\ndF1 = pd.DataFrame(correlation_table)\nfig = plt.figure(figsize=(12,8))\nax = sns.barplot(x=\"Correlation coefficient\", y=\"Features\", \n                     data=dF1.sort_values(\"Correlation coefficient\", ascending=False),\n                     palette='Blues_r', alpha=0.75)\nax.set_title(\"Correlation of numerical features with SalePrice\", fontsize=20, y=1.05)","26ca45ce":"correlation_table= []\nfor cols in cat_features:\n    y = train['SalePrice']\n    X = train[cols]\n    corr = pd.concat((X, y), axis=1).apply(lambda x : pd.factorize(x)[0]).corr()\n    dict ={\n        'Features': cols,\n        'Correlation coefficient' : corr['SalePrice'][:].values[0],\n        'Feat_type': 'categorical'\n    }\n    correlation_table.append(dict)\ndF2 = pd.DataFrame(correlation_table)\nfig = plt.figure(figsize=(12,8))\nax = sns.barplot(x=\"Correlation coefficient\", y=\"Features\", \n                     data=dF2.sort_values(\"Correlation coefficient\", ascending=False),\n                     palette='Blues_r', alpha=0.75)\nax.set_title(\"Correlation of categorical features with SalePrice\", fontsize=20, y=1.05)","54813097":"# # the bove two correlations plots combined together\n\n# dF3 = pd.concat((dF1, dF2), axis=0)\n# fig = plt.figure(figsize=(12,20))\n# ax = sns.barplot(x=\"Correlation coefficient\", y=\"Features\", \n#                      data=dF3.sort_values(\"Correlation coefficient\", ascending=False),\n#                      alpha=0.75, hue='Feat_type')\n# ax.set_title(\"Correlation of all features with SalePrice\", fontsize=20, y=1.05)","f0e53d40":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=int))\nf, ax = plt.subplots(figsize=(18, 12))\ncmap = sns.color_palette(\"Spectral\", as_cmap=True)\nax= sns.heatmap(corr, mask= np.abs(mask) > 0.8, cmap=cmap, vmax=1.0, vmin=-1.0, center=0, annot=False,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 1.0})\nax.set_title('Correlation heatmap: numerical features', fontsize=20, y= 1.05)\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_ticks([-0.75, 0, 0.75])\ncolorbar.set_ticklabels(['negative_corr','Little_to_no_corr','positive_corr'])","d386fb2b":"f, ax = plt.subplots(figsize=(18, 12))\ncorr = pd.concat((df_train[cat_features], y), axis=1).apply(lambda x : pd.factorize(x)[0]).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.color_palette(\"Spectral\", as_cmap=True)\nax = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1, center=0, annot=False,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 1.0})\nax.set_title('Correlation heatmap: categorical features', fontsize=20, y= 1.05)\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_ticks([-0.75, 0, 0.75])\ncolorbar.set_ticklabels(['negative_corr','Little_to_no_corr','positive_corr'])","16a995fc":"# make a list of columns with a null-value, which is a re-cap\ncol_with_null_values = []\nfor col in df_train:\n    if df_train[col].isnull().any():\n        col_with_null_values.append(col)\ndisplay('Recap features with missing values')\ndisplay(col_with_null_values)\n# create a dataFrame of columns with null-values\ndf_train_nan = df_train[col_with_null_values].fillna('NaN')\ndf_train_nan","94a6c243":"# the code for customized color @line15 is adapted from \n# python-graph-gallery.com\n\ndef sns_boxplot(data, features, titleText='Title', ncol=4):\n    itr = 1    \n    L = len(features)    \n    nrow= int(np.ceil(L\/ncol))        \n    remove_last= (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(28, 18))\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.95) \n    \n    for feature in features:\n        plt.subplot(nrow, ncol, itr)\n        my_pal = {feature: \"#ff9900\" if feature == \"NaN\" else '#32718E' for feature in data[feature].unique()}\n        ax = sns.boxplot(x=data[feature], y=y, data=data, palette=my_pal)\n        plt.xlabel(feature, fontsize=10)\n        itr += 1\n    plt.suptitle(titleText, fontsize=24,)\n    plt.show()    \n","553d062a":"LisT0 = ['Alley',\n 'MasVnrType',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtFinType2',\n 'Electrical',\n 'FireplaceQu',\n 'GarageType',\n 'GarageFinish',\n 'GarageQual',\n 'GarageCond',\n 'PoolQC',\n 'Fence',\n 'MiscFeature']\nLisT1= ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']","529f5376":"sns_boxplot(df_train_nan, LisT0, titleText='The (Sale) Price of Missing Values', ncol=4)","2e381678":"def sns_scatter_bar_subplot(data, features, titleText='Title', ncol=3):\n    itr = 1    \n    L = len(features)    \n    nrow= int(np.ceil(L\/ncol))        \n    remove_last= (nrow * ncol) - L\n    \n    fig, ax = plt.subplots(nrow, ncol, figsize=(22, 18))\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.95) \n    \n    for feature in features:\n        plt.subplot(nrow, ncol, itr)\n        \n        if data[feature].nunique() <= 5:\n            ax = sns.boxplot(x=feature, y='SalePrice', data= data, color='#1eb169')\n        \n        elif data[feature].nunique() < 20:\n            ax = sns.barplot(x=feature, y='SalePrice', data= data, color='#600000')\n            \n        else:\n            ax = sns.scatterplot(data=data, x=feature, y=\"SalePrice\", palette='Blues')\n                    \n        plt.xlabel(feature, fontsize=10)\n        plt.grid()\n        itr += 1\n    plt.suptitle(titleText, fontsize=24, y=1.002)    \n    plt.show()    \n","49b38af5":"highCorrFeats = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF',\n                 'FullBath', 'TotRmsAbvGrd', 'YearBuilt','YearRemodAdd', 'SalePrice']\ndf_highCorr = train[highCorrFeats]\nsns_scatter_bar_subplot(df_highCorr, highCorrFeats[:-1], titleText='Features highly correlated with saleprice', ncol=2)","84e858ca":"## 1.2 Explore the data (EDA)","dc606c1d":"## Highly SalePrice-Predictive Features\n\nBelow are the top ten features highly (and positively) correlated with the sale price of the houses. Knowing these features of a house will play a vital role in how accurately one can estimate the sale price of a given house. ","06f4cab4":"# Introduction\n\nPredicting the sale price of a house based on its features is probabily one of the famous and most common example in ML study. Perhaps together with the titanic learning from disater dataset, house price predictions is the most explored dataset here in kaggle. They are the first dataset beginners use to start their DS and ML journey. So like many beginning ML students, I will take the chance to practice my data science skills using the Ames Iowa housing dataset.\n\nI will break down my work into three parts:\n\n- Part-1: **Exploratory data analysis (EDA)**: where we will attempt to do exploratory data analysis and study the data at hand\n- Part-2: **Pre-processing and Feature Engineering**: where we deal with missing values, drop trivial features (if any) and engineer additional potentially useful features \n- Part-3: **Modeling and Prediction**: where the magic happens, train a ML model and predict house sale price like a real estate agent would normally do :)\n","f174e8ac":"## 1.3 Categorical Features \nI grouped the columns into three groups according to their cardinality. This is just (for now) purely for plotting convienience.\n1. Low cardinals: cols with < 5 categories\n1. Medium cardinals: cols with < 9 categoriescate\n1. High cardinals: cols with >=9 categories","fe461ef7":"## 1.4 Numerical Features","4f5fde85":"<div class=\"alert alert-info\">\n  <strong>Notebook in progress....<\/strong> \n[publicly publishing a partially complete notebook helps me fight procrastination and finish it sooner]\n\n<\/div>","7770d67f":"## 1.6 Visualizing missing values (through SalePrice lense)\n\nVisualizing the null-values within each colomn relative to the other non-null values gives a perspective on what kind kind of values could be missing. Later in the data processing phase, it will help us guide imputing the missing values. \n\nHere first we identify the columns\/features with null-values, plot them aginst the sale prices and see where they stand.","885784fb":"## 1.3 Explore NaN values\n\n- 19 out of 78 features of the train data have at least one missing value. However, there are more (33) features missing at least one in the test data\n- Four columns (**PoolQC**, **MiscFeatures**, **Alley** and **Fence**) have more that 80% missing values\n- **FireplaceQu** has 47.26% missing values","ab9cedb7":"## 1.5 The target variable (Sale Price)\n\n- Sale price is not a **normally** distributed data. It is **right skewed**.\n- The average house price is sold at **180921** USD\n- The highest and lowest sale prices are **755000** and **34900** USD respectively.","740d08be":"## Setup","4e6d6fa2":"### Tell me about your *house*, I will tell you its *worth*.\n![](https:\/\/www.key-universal.com\/wp-content\/uploads\/2020\/07\/real-estate-agents-and-buyer-difference.png)","7833f009":"## Thank you very much for reading this notebook!","da7a53eb":"## 1.1 Load the data","f1d053b5":"### 1.5.2 Which features are correlated with eachother?","04a11188":"### 1.3.1 Similarity of unique entries in catagorical features (train vs test)\n\n<div class=\"alert alert-block alert-danger\">  \n>>> There are 11 categorical features where the unique values in train & test datasets aren't identical. We will deal with this issue in the data pre-processing phase.\n<\/div>\n","6617d9c5":"Observation:\n- Average lot area is 10516.8 square feet\n- Average total rooms excluding underground is 6.5 (~7 rooms)\n- On average houses were renovated in the mid 80's (1985 avg.), the latest being 2010 (the year of data collection). The oldest house was built in 1872\n- The mean sale price of a house is 180921 USD\n- A typical garage size takes in 2 cars\n<div class=\"alert alert-block alert-danger\">  \n>>> Test data has an error in data entry! The highlighted year (in the table above) is probabily 2007, surely not 2207! \n<\/div>\n","f8e32d8b":"# Part 3: Modeling & Predictions","4c0712a4":"### 1.5.1 Which features drive the sale price?","abf84b71":"# Part 1: Exploratory Data Analysis (EDA)","fab4fd86":"# Part 2: Data pre-processing and Feature Engineering"}}