{"cell_type":{"896bc18e":"code","ff896a70":"code","28373a21":"code","840e46c1":"code","f3e06007":"code","6a06e7f6":"code","84d94bea":"code","785f2f49":"code","ac609c27":"code","e4dc1300":"code","6811d8a2":"code","f1ac4f49":"code","a074a8fa":"code","6ba417ff":"code","616fa334":"code","140484aa":"code","13217e10":"code","b43dbe68":"code","fd7b6bae":"code","35d93e3f":"code","6cc5c2c6":"code","abea02d0":"code","e84180b3":"markdown","fd435d7a":"markdown","58dae712":"markdown","63f68eb9":"markdown","4ec752da":"markdown","2478deab":"markdown","3b32caa2":"markdown","1d245208":"markdown","be7e414b":"markdown","460927b5":"markdown","23acd244":"markdown","fd127cba":"markdown","9c8a61fd":"markdown","7111521a":"markdown","c12469d6":"markdown","f9931ad7":"markdown","a1a9d36f":"markdown","63d9aae7":"markdown","237235d4":"markdown","b8b55c0a":"markdown","d4b9a715":"markdown","4f70307f":"markdown","cc0ec7fd":"markdown","3b7d66ba":"markdown","93618696":"markdown","80c5c39f":"markdown"},"source":{"896bc18e":"# Please switch on the TPU before running these lines.\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","ff896a70":"# Imports required to use TPUs with Pytorch.\n# https:\/\/pytorch.org\/xla\/release\/1.5\/index.html\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm","28373a21":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n\nimport transformers\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom transformers import AdamW\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","840e46c1":"# Here goes some my tests!\ndata = pd.read_csv(\"..\/input\/porn-recognition\/data.csv\", index_col=0)\ndata['url'] = data['url'].str.replace('.', ' ')\ndata.head()","f3e06007":"MODEL_TYPE = 'xlm-roberta-large'\n\n\nL_RATE = 1e-5\nMAX_LEN = 256\n\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nNUM_CORES = os.cpu_count()\nRANDOM_STATE = 42\n\ntorch.manual_seed(RANDOM_STATE)\n\n\nNUM_CORES","6a06e7f6":"# USING TPU ON KAGGLE   \ndevice = xm.xla_device()\n\nprint(device)","84d94bea":"df = data.loc[ (data['target'] != -1) & (data['target'] != -2) ]\n\npercentage_df =  round( df.shape[0] \/100*70 )\n\ndf_train = df\ndf_val = df.iloc[percentage_df:len(df),:]\n\ndf_test = data.loc[data['target'] == -1 ]\n\ndf_train.head()","785f2f49":"df_val.head()","ac609c27":"df_test.head()","e4dc1300":"from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n\n# xlm-roberta-large\nprint('Loading XLMRoberta tokenizer...')\ntokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)","6811d8a2":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","f1ac4f49":"class CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'url']\n        sentence2 = self.df_data.loc[index, 'title']\n        sentence3 = self.df_data.loc[index, 'title_re']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,          # Sentences to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',  \n                    truncation=True # Return pytorch tensors.\n        )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'target'])\n        sample = (padded_token_list, att_mask, target)\n        return sample\n\n    def __len__(self):\n        return len(self.df_data)\n    \nclass TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'url']\n        sentence2 = self.df_data.loc[index, 'title']\n        sentence3 = self.df_data.loc[index, 'title_re']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,         # Sentence to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt', \n                    truncation=True\n            # Return pytorch tensors.\n        )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        sample = (padded_token_list, att_mask)\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)","a074a8fa":"model = XLMRobertaForSequenceClassification.from_pretrained(\n    MODEL_TYPE, \n    num_labels = 2, # The number of output labels. 2 for binary classification.\n)\n\n# Send the model to the device.\nmodel.to(device)","6ba417ff":"# Define the optimizer\noptimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8 \n            )","616fa334":"# Create the dataloaders.\n\ntrain_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","140484aa":"%%time\n\n\n# Set the seed.\nseed_val = 101\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n\n# For each epoch...\nfor epoch in range(0, NUM_EPOCHS):\n    \n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch + 1, NUM_EPOCHS))\n    \n\n    stacked_val_labels = []\n    targets_list = []\n\n    # ========================================\n    #               Training\n    # ========================================\n    \n    print('Training...')\n    \n    # put the model into train mode\n    model.train()\n    \n    # This turns gradient calculations on and off.\n    torch.set_grad_enabled(True)\n\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    for i, batch in enumerate(train_dataloader):\n        \n        train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n        \n        print(train_status, end='\\r')\n\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n\n        outputs = model(b_input_ids, \n                    attention_mask=b_input_mask,\n                    labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_train_loss = total_train_loss + loss.item()\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        \n        \n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        \n        \n        # Use the optimizer to update the weights.\n        \n        # Optimizer for GPU\n        # optimizer.step() \n        \n        # Optimizer for TPU\n        # https:\/\/pytorch.org\/xla\/\n        xm.optimizer_step(optimizer, barrier=True)\n\n    \n    print('Train loss:' ,total_train_loss)\n\n\n    # ========================================\n    #               Validation\n    # ========================================\n    \n    print('\\nValidation...')\n\n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n    \n    \n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n    \n\n    for j, batch in enumerate(val_dataloader):\n        \n        val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n        \n        print(val_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)      \n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask, \n                labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_val_loss = total_val_loss + loss.item()\n        \n\n        # Get the preds\n        preds = outputs[1]\n\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n\n    \n    # Calculate the validation accuracy\n    y_true = targets_list\n    y_pred = np.argmax(stacked_val_preds, axis=1)\n    \n    val_acc = f1_score(y_true, y_pred)\n    \n    \n    print('Val loss:' ,total_val_loss)\n    print('Val acc: ', val_acc)\n\n\n    # Save the Model\n    torch.save(model.state_dict(), 'model.pt')\n    \n    # Use the garbage collector to save memory.\n    gc.collect()","13217e10":"for j, batch in enumerate(test_dataloader):\n        \n        inference_status = 'Batch ' + str(j+1) + ' of ' + str(len(test_dataloader))\n        \n        print(inference_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask)\n        \n        \n        # Get the preds\n        preds = outputs[0]\n\n\n        # Move preds to the CPU\n        preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n        \n        # Stack the predictions.\n\n        if j == 0:  # first batch\n            stacked_preds = preds\n\n        else:\n            stacked_preds = np.vstack((stacked_preds, preds))","b43dbe68":"stacked_preds","fd7b6bae":"# Take the argmax. This returns the column index of the max value in each row.\n\npreds = np.argmax(stacked_preds, axis=1)\n\npreds","35d93e3f":"# Load the sample submission.\n# The row order in the test set and the sample submission is the same.\n\npreds.shape","6cc5c2c6":"# Assign the preds to the prediction column\ndata['id'] = [i for i in range(data.shape[0])]\n\ndf_test = data.loc[ data['target'] == -1 ]\n\ndf_sub = pd.DataFrame()\ndf_sub['id'] = df_test['id']\ndf_sub['target'] = preds \n\ndf_sub.head()","abea02d0":"df_sub.to_csv(\"sub.csv\", index=False)","e84180b3":"| <a id='Helpful_Resources'><\/a>","fd435d7a":"# Section 3","58dae712":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","63f68eb9":"**Thank you for reading.**","4ec752da":"## A2 - GLUE Datasets\n\nGLUE (General Language Understanding Evaluation) is a performance bechmark that's used to compare the language understanding capability of machine learning models. A model's performance on 9 datasets is reduced to a single number. These are the datasets that are part of GLUE.\n\n1. MNLI -Multi-Genre Natural Language Inference\n2. QQP - Quora Question Pairs\n3. QNLI - Question Natural Langiage Inference\n4. SST-2 - Stanford Sentiment Treebank\n5. CoLA - Corpus of Linguistic Acceptability\n6. STS-B - Semantic Textual Similarity Benchmark\n7. MRPC - Microsoft Research Paraphrase Corpus\n8. RTE - Recognizing Textual Entailment\n9. WNLI - Winograd NLI\n\nMore Info:<br>\nGLUE Explained: Understanding BERT Through Benchmarks<br>\nhttps:\/\/mccormickml.com\/2019\/11\/05\/GLUE\/\n","2478deab":"In this section we will train a BERT Model on three folds and train an XLM-RoBERTa model on one fold. We will use PyTorch with a single TPU. For each model we will also make a prediction on the competition test set and create a submission csv file.\n\n### A few notes on using PyTorch with a TPU\n\n- Setting up PyTorch code to use a single xla device (TPU) is easier that setting it up to use all 8 TPU cores. Just a few lines of code need to be changed to switch from a GPU to a single TPU. The speed is not as fast as using all 8 TPU cores but the model does train faster than a GPU and there's more RAM available. \n\n- Pytorch XLA does not use memory as efficiently as Tensorflow. Therefore, my code tends to consistently crash when I try to use PyTorch with an 8 core TPU setup. \n\n- There is 4.9GB of disk space available in Kaggle notebooks. What I've found is that models trained on a TPU are larger than models trained on a GPU. For example, a Bert model trained on a GPU is 600MB. However, a BERT model trained on a TPU is approx. 1GB. Therefore, when running 5 fold cross validation, trying to save all 5 fold models (1GB each) will cause the Kaggle notebook to crash because the available disk space will be exceeded. For that reason here we will be training on three folds only.\n\n-  A TPU may take a few seconds to start running. Therefore, if you run your code and you see that nothing is happening, wait a little while. Don't cancel the run because you think that something is wrong.","3b32caa2":"## A3 - Datasets Separated by Task\n\na) Sentence Pair Classification Tasks<br>\nMNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG\n\nb) Single Sentence Classifications Tasks<br>\nSST-2, CoLA\n\nc) Question Answering Tasks<br>\nSQuAD (v1.1 and v2.0)\n\nd) Single Sentence Tagging Tasks<br>\nCoNLL-2003 NER","1d245208":"## A1 - Acronyms\n\n- NLP - Natural Language Processing\n- NLU - Natural Language Understanding\n- NLI - Natural Language Inference\n- NER - Named Entity Recognition\n- NSP - Next Sentence Prediction\n- MLM - Masked Language Model\n- PoS - Part of Speech\n- POST - Part of Speech Tagging\n- GLUE - The General Language Understanding Evaluation benchmark\n- SQuAD - Stanford Question Answering Dataset\n- SWAG - Situations With Adversarial Generations (Dataset)\n- XNLI - Cross Lingual Natural Language Inference (Dataset)\n- XLU - Cross-lingual Language Understanding\n\n","be7e414b":"| <a id='GLUE_Datasets'><\/a>","460927b5":"| <a id='Papers'><\/a>","23acd244":"## 3.2. Train an XLM-RoBERTa Model","fd127cba":"## Instantiate the Tokenizer","9c8a61fd":"| <a id='NLP_Applications'><\/a>","7111521a":"## Define the Optimizer","c12469d6":"## Process the Predictions","f9931ad7":"## Create a submission csv file","a1a9d36f":"## Define the device","63d9aae7":"## A4 - Papers\n\n- Attention is all you need<br>\nhttps:\/\/arxiv.org\/pdf\/1706.03762.pdf\n\n- BERT Paper<br>\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>\nhttps:\/\/arxiv.org\/pdf\/1810.04805.pdf\n\n- XLMRoberta Paper<br>\nUnsupervised Cross-lingual Representation Learning at Scale<br>\nhttps:\/\/arxiv.org\/pdf\/1911.02116.pdf\n\n- GLUE Paper<br>\nhttps:\/\/arxiv.org\/abs\/1804.07461<br>\nWebsite: https:\/\/gluebenchmark.com\/\n\n- MultiNLI Paper<br>\nA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference<br>\nhttps:\/\/cims.nyu.edu\/~sbowman\/multinli\/paper.pdf<br>\nWebsite: https:\/\/cims.nyu.edu\/~sbowman\/multinli\/\n\n- XNLI Paper<br>\nhttps:\/\/arxiv.org\/pdf\/1809.05053.pdf<br>\nWebsite: https:\/\/cims.nyu.edu\/~sbowman\/xnli\/\n\n- SentencePiece Paper<br>\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\nhttps:\/\/arxiv.org\/abs\/1808.06226","237235d4":"## Define the Model","b8b55c0a":"## A6 - Helpful Resources\n\n- GLUE Explained: Understanding BERT Through Benchmarks<br>\nhttps:\/\/mccormickml.com\/2019\/11\/05\/GLUE\/\n\n- Improving Language Understanding with Unsupervised Learning<br>\nhttps:\/\/openai.com\/blog\/language-unsupervised\/\n\n- Hugging Face Transformers Github<br>\nhttps:\/\/github.com\/huggingface\/transformers\n\n- Hugging Face Summary of Models<br>\nhttps:\/\/huggingface.co\/transformers\/model_summary.html\n\n- Hugging Face - Searchable model listing<br>\nhttps:\/\/huggingface.co\/models\n\n- Bert Video Series by ChrisMcCormickAI<br>\nPart 1<br>\nhttps:\/\/www.youtube.com\/watch?v=FKlPCK1uFrc<br>\nPart 2<br>\nhttps:\/\/www.youtube.com\/watch?v=zJW57aCBCTk<br>\nPart 3<br>\nhttps:\/\/www.youtube.com\/watch?v=x66kkDnbzi4<br>\nPart 4<br>\nhttps:\/\/www.youtube.com\/watch?v=Hnvb9b7a_Ps<br>\n\n- Data Processing For Question & Answering Systems: BERT vs. RoBERTa by Abhishek Thakur<br>\nhttps:\/\/www.youtube.com\/watch?v=6a6L_9USZxg\n\n- Sentencepiece Tokenizer With Offsets For T5, ALBERT, XLM-RoBERTa And Many More by Abhishek Thakur<br>\nhttps:\/\/youtu.be\/U51ranzJBpY\n\n- PyTorch on XLA Devices - docs<br>\nhttps:\/\/pytorch.org\/xla\/release\/1.5\/index.html\n","d4b9a715":"## Train the Model","4f70307f":"## A5 - What is NLP used for?\n\n- Text Classification\n- Translation\n- Named Entity Recognition\n- Part of Speech Tagging\n- Question Answering\n- Text Generation\n- Language Modeling\n- Text Summarization","cc0ec7fd":"### BasicsXLM-RoBERTa - PyTorch\n","3b7d66ba":"## Make a prediction on the test set","93618696":"## Create the Dataloader","80c5c39f":"| <a id='Datasets_Separated_by_Task'><\/a>"}}