{"cell_type":{"c369a893":"code","a683a694":"code","3c3c8878":"code","af9fda3b":"code","fbd9c6a1":"code","159d540b":"code","64706abf":"code","a2ad0bea":"code","d144bc35":"code","ef976195":"code","aed1f126":"code","fbade5de":"code","5a45defc":"code","e3db4a4e":"code","6de458f2":"code","72eb0ad2":"markdown","6e58e3b3":"markdown","46bd69ec":"markdown","1b9cffbe":"markdown","3c6b41b8":"markdown","6fc6cf7e":"markdown","27dbeaf0":"markdown","ee1c4d0a":"markdown","ceed7e37":"markdown"},"source":{"c369a893":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\ndf_angry = pd.read_csv('\/kaggle\/input\/emotion\/Emotion(angry).csv')\ndf_happy = pd.read_csv('\/kaggle\/input\/emotion\/Emotion(happy).csv')\ndf_sad = pd.read_csv('\/kaggle\/input\/emotion\/Emotion(sad).csv')\ndf_main = pd.concat([df_angry, df_happy, df_sad])\ndf_main = df_main.reset_index(drop=True)\n\n# Remove duplicates\ndf_main = df_main.drop_duplicates(subset=['content', 'sentiment'])\ndf_main = df_main.reset_index(drop=True)\n\n# Remove empty data\ndf_main['content'].replace('', np.nan, inplace=True)\ndf_main = df_main.dropna(subset = ['content'])\ndf_main = df_main.reset_index(drop=True)\n","a683a694":"print(df_main.head())\nprint('\\n')\nprint(df_main.info())","3c3c8878":"angry_text = df_main[df_main[\"sentiment\"] == 'angry'][\"content\"].values\nprint(angry_text[0])\nprint(\"\\n\")\nprint(angry_text[1])\nprint(\"\\n\")\nprint(angry_text[2])\nprint(\"\\n\")\nprint(angry_text[3])\nprint(\"\\n\")\nprint(angry_text[4])\nprint(\"\\n\")","af9fda3b":"happy_text = df_main[df_main[\"sentiment\"] == 'happy'][\"content\"].values\nprint(happy_text[0])\nprint(\"\\n\")\nprint(happy_text[1])\nprint(\"\\n\")\nprint(happy_text[2])\nprint(\"\\n\")\nprint(happy_text[3])\nprint(\"\\n\")\nprint(happy_text[4])\nprint(\"\\n\")","fbd9c6a1":"sad_text = df_main[df_main[\"sentiment\"] == 'sad'][\"content\"].values\nprint(sad_text[0])\nprint(\"\\n\")\nprint(sad_text[1])\nprint(\"\\n\")\nprint(sad_text[2])\nprint(\"\\n\")\nprint(sad_text[3])\nprint(\"\\n\")\nprint(sad_text[4])\nprint(\"\\n\")","159d540b":"tot = df_main.shape[0]\nvc = df_main['sentiment'].value_counts()\n\nnum_angry = vc['angry']\nnum_happy = vc['happy']\nnum_sad = vc['sad']\n\nslices = [num_angry, num_happy, num_sad]\nlabeling = ['Angry','Happy', 'Sad']\nexplode = [0.1, 0.1, 0.1]\nplt.pie(slices,explode=explode,shadow=True,autopct='%1.1f%%',labels=labeling,wedgeprops={'edgecolor':'black'})\nplt.title('Sentiment of Content')\nplt.tight_layout()\nplt.show()","64706abf":"import re\nimport string\nfrom textblob import TextBlob\nfrom tqdm.notebook import tqdm\n\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"thx\"   : \"thanks\"\n}\n\n\ndef clean(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower()\n    text = re.sub('\\[.*?\\]','',text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+','',text)\n    text = re.sub('<,*?>+\"','',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n    text = re.sub('\\n','',text)\n    text = re.sub('\\w*\\d\\w*','',text)\n    text = re.sub(\"xa0'\", '', text)\n    text = re.sub(u\"\\U00002019\", \"'\", text) # IMPORTANT: Their apostrophe character was not the usual one...\n    words = text.split()\n    for i in range(len(words)):\n        if words[i].lower() in contractions.keys():\n            words[i] = contractions[words[i].lower()]\n    text = \" \".join(words)\n    #text = TextBlob(text).correct()\n    return text\n\ndf_main['content'] = df_main['content'].apply(lambda x: clean(x))\n\n# Remove empty data\ndf_main['content'].replace('', np.nan, inplace=True)\ndf_main = df_main.dropna(subset = ['content'])\ndf_main = df_main.reset_index(drop=True)","a2ad0bea":"from wordcloud import WordCloud,STOPWORDS \n\nplt.style.use('fivethirtyeight')\nstopwords = set(STOPWORDS) \nstop_word= list(stopwords) + ['http','co','https','wa','amp','\u00fb','\u00db','HTTP','HTTPS']\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(df_main[df_main['sentiment']=='angry']['content']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Angry Content',fontsize=40)\n\nwordcloud2 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(df_main[df_main['sentiment']=='happy']['content']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Happy Content',fontsize=40)\n\nwordcloud3 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(df_main[df_main['sentiment']=='sad']['content']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Sad Content',fontsize=40)\nplt.show()","d144bc35":"plt.style.use('fivethirtyeight')\n\ndf_main['word_count'] = df_main['content'].apply(lambda x: len(x.split()))\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5))\n\ndf_angry = df_main[df_main['sentiment'] == 'angry']\nword = df_angry['word_count']\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red', kde=False)\nax1.set_title('Angry')\n\ndf_happy = df_main[df_main['sentiment'] == 'happy']\nword = df_happy['word_count']\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green', kde=False)\nax2.set_title('Happy')\n\ndf_sad = df_main[df_main['sentiment'] == 'sad']\nword = df_sad['word_count']\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='blue', kde=False)\nax3.set_title('Sad')\n\nfig.suptitle('Average word length by sentiment')\nplt.show()","ef976195":"import nltk\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))\n\n\ndef remove_stopwords(text):\n    text = text.split()\n    words = [w for w in text if w not in stopwords.words('english')]\n    return \" \".join(words)\n\ndf_main['content_no_sw'] = df_main['content'].apply(lambda x : remove_stopwords(x))\n","aed1f126":"from nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize,word_tokenize\n\n\nlemmatizer = WordNetLemmatizer()\nstatuses = df_main['content'].values.copy()\n\nfor i in range(len(statuses)):\n    a = statuses[i]\n    sentences = sent_tokenize(statuses[i])\n    word_list = []\n    for sent in sentences:\n        words = word_tokenize(sent)\n        for word in words:\n            if words not in word_list:\n                word_list.append(word)\n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    statuses[i] = ' '.join(w for w in word_list)\n    \nfrom nltk.stem import PorterStemmer\nporter = PorterStemmer()\n\nfor i in range(len(statuses)):\n    sentences = sent_tokenize(statuses[i])\n    word_list = []\n    for sent in sentences: \n        words = word_tokenize(sent)\n        for word in words: \n            if words not in word_list:\n                word_list.append(word)\n    word_list = [porter.stem(w) for w in word_list if w not in stop_words]\n    statuses[i] = ' '.join(w for w in word_list)\n\n    \ndf_main['content_lemm_stem_no_sw'] = statuses","fbade5de":"df_main.head()","5a45defc":"from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, plot_confusion_matrix\n\ndef show_cm(classifier, X_test, y_test):\n    plt.style.use('default')\n    class_names = clf.classes_\n    titles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\n    for title, normalize in titles_options:\n        disp = plot_confusion_matrix(classifier, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n        plt.title(title)\n        plt.show()","e3db4a4e":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import f1_score\n\ncount_vectorizer0 = CountVectorizer(ngram_range = (1,2))\ncount_vectorizer1 = CountVectorizer(ngram_range = (1,2))\ncount_vectorizer2 = CountVectorizer(ngram_range = (1,2))\n\nvecs0 = count_vectorizer0.fit_transform(df_main['content'])\nvecs1 = count_vectorizer1.fit_transform(df_main['content_no_sw'])\nvecs2 = count_vectorizer2.fit_transform(df_main['content_lemm_stem_no_sw'])\n\nclf0 = linear_model.RidgeClassifier().fit(vecs0, df_main[\"sentiment\"])\nprint(\"Content (Unigrams and Bigrams): No changes\")\nprint(\"Percent correctly labeled comments by Ridge Classifier :\")\nprint(clf0.score(vecs0, df_main[\"sentiment\"]))\nshow_cm(clf0, vecs0, df_main['sentiment'])\n\nclf1 = linear_model.RidgeClassifier().fit(vecs1, df_main[\"sentiment\"])\nprint(\"Content (Unigrams and Bigrams): No stop words\")\nprint(\"Percent correctly labeled comments by Ridge Classifier :\")\nprint(clf1.score(vecs1, df_main[\"sentiment\"]))\nshow_cm(clf1, vecs1, df_main['sentiment'])\n\nclf2 = linear_model.RidgeClassifier().fit(vecs2, df_main[\"sentiment\"])\nprint(\"Content (Unigrams and Bigrams): No stop words, lemmatized and stemmed\")\nprint(\"Percent correctly labeled comments by Ridge Classifier :\")\nprint(clf2.score(vecs2, df_main[\"sentiment\"]))\nshow_cm(clf2, vecs2, df_main['sentiment'])","6de458f2":"predict = clf1.predict(vecs1)\nerror_a_h = 0\nerror_a_s = 0\nerror_h_a = 0\nerror_h_s = 0\nerror_s_a = 0\nerror_s_h = 0\nfor i in range(len(predict)):\n    prediction = predict[i]\n    actual = df_main.loc[i, 'sentiment']\n    if actual == 'angry' and prediction == 'happy' and error_a_h == 0:\n        print(\"Angry status mislabeled as Happy:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_a_h += 1\n    elif actual == 'angry' and prediction == 'sad' and error_a_s == 0:\n        print(\"Angry status mislabeled as Sad:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_a_s += 1\n    elif actual == 'happy' and prediction == 'angry' and error_h_a == 0:\n        print(\"Happy status mislabeled as Angry:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_h_a += 1\n    elif actual == 'happy' and prediction == 'sad' and error_h_s == 0:\n        print(\"Happy status mislabeled as Sad:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_h_s += 1\n    elif actual == 'sad' and prediction == 'angry' and error_s_a == 0:\n        print(\"Sad status mislabeled as Angry:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_s_a += 1\n    elif actual == 'sad' and prediction == 'happy' and error_s_h == 0:\n        print(\"Sad status mislabeled as Happy:\")\n        print(df_main.loc[i, 'content'])\n        print('\\n')\n        error_s_h += 1\n        ","72eb0ad2":"# EDA","6e58e3b3":"Examples of happy content:","46bd69ec":"Looks like the super long statuses are all Angry (makes sense; I'd imagine an angry rant could drone on and on)","1b9cffbe":"Examples of sad content:","3c6b41b8":"Examples of angry content:","6fc6cf7e":"# Count Vectorizer + Tf-Idf","27dbeaf0":"To clean the data and make word clouds, I took some inspiration from the following kernel:\n\nhttps:\/\/www.kaggle.com\/moezabid\/disaster-tweets-nlp\n\nI highly reccommend you checking it out - they did some great work! ","ee1c4d0a":"The word clouds are about what I would expect them to be, but still interesting to visualize.","ceed7e37":"Let's lemmatize and stem the text, and also just remove stop words"}}