{"cell_type":{"a062971e":"code","3c46a4e5":"code","8d7fa2e3":"code","b1968e7c":"code","0f1504f5":"code","8e37fbd3":"code","120c7052":"code","8616ce6b":"code","9fbd8241":"code","c6c12465":"code","b12b5e4d":"code","37c73f65":"code","1514e670":"code","033aec5d":"code","b37ef5d5":"code","fb441330":"code","f0200663":"code","2516792e":"code","ae138451":"code","36377bb9":"code","7efb9970":"code","be02ddaf":"code","5891ef61":"code","8a3dbce2":"code","1702478b":"code","5213ea6c":"code","ec024625":"code","d7f582ba":"code","12a6f3be":"code","cd520705":"code","91615572":"code","350b6bf1":"code","ab4f1ac3":"code","64d806ae":"code","1638f33d":"code","ea32b64d":"code","9e3e4c16":"code","a6aad8fe":"code","5376a20b":"code","15cb5b0d":"code","7f399967":"code","84d08043":"code","70481d3e":"code","22cd2d6f":"code","131a7ed7":"code","fb824201":"code","b1b7c293":"code","39de1b3c":"code","b1a06a57":"code","537b1285":"code","6face0fb":"code","107c8799":"code","ac920512":"markdown","2eb355be":"markdown","140c2fda":"markdown","4615f24a":"markdown"},"source":{"a062971e":"import sklearn\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import learning_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import precision_recall_fscore_support\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","3c46a4e5":"data = pd.read_csv('\/kaggle\/input\/the-boston-houseprice-data\/boston.csv');","8d7fa2e3":"data","b1968e7c":"plt.figure(figsize=(10,8))\nheat_map_data = data.corr()\nsns.heatmap(heat_map_data, annot=True)","0f1504f5":"data.drop(columns=['TAX', 'DIS'], inplace=True)","8e37fbd3":"data","120c7052":"len(data.columns)","8616ce6b":"plt.figure(figsize=(10,8))\nn_features = len(data.columns)\nheat_map_data = (data.corrwith(data.MEDV).values * np.ones((n_features,n_features))) * np.identity(n_features)\ng = sns.heatmap(pd.DataFrame(heat_map_data, columns= data.columns, index=['MEDV' for _ in range(len(data.columns))]), annot=True, cmap='coolwarm') \ng.set_yticklabels(g.get_yticklabels(), rotation=45)\nplt.show()","9fbd8241":"data.drop(columns = ['CHAS'], inplace= True)","c6c12465":"data","b12b5e4d":"import warnings\nwarnings.filterwarnings('ignore')\nplt.figure(figsize=(20,15))\nfor i, column in enumerate(data.columns):\n    plt.subplot(5,3,i+1)\n    sns.distplot(data[column])","37c73f65":"# Removing extreme outliers and replacing the mild outliers with mean value\nfor column in data.columns:\n    q1 = data[column].quantile(.25)\n    q3 = data[column].quantile(.75)\n    iqr = q3 - q1\n    not_outliers = len(data[column][((q1- (1.5 *iqr)) < data[column]) & ((q3 + (1.5 * iqr)) > data[column])])\n    outliers = len(data[column][((q1- (1.5 *iqr)) >= data[column]) | ((q3 + (1.5 * iqr)) <= data[column])])\n    print(f\"{column} has {((outliers\/data[column].count()) * 100):.02f} % of outliers\")\n    x = data[column][((q1- (1.5 *iqr)) >= data[column]) | ((q3 + (1.5 * iqr)) <= data[column])]\n    if len(x) >= 1 and column != 'CHAS':\n        data.drop(x.sort_values(ascending=False).index[0], inplace=True)\n        data[column][x.sort_values(ascending=False).index[1:]] = data[column].mean()\n    print(\"After updating\/removing outliers : \")\n    outliers = len(data[column][((q1- (1.5 *iqr)) >= data[column]) | ((q3 + (1.5 * iqr)) <= data[column])])\n    print(f\"{column} has {((outliers\/data[column].count()) * 100):.02f} % of outliers\")\n    print(\"_\"*50)","1514e670":"fig, ax = plt.subplots(ncols = 7, nrows = 2, figsize = (20, 15))\nindex = 0\nax = ax.flatten()\n\nfor col, value in data.items():\n    sns.boxplot(y=col, data=data, ax=ax[index])\n    index += 1\nplt.tight_layout(pad = 0.5, w_pad=0.7, h_pad=5.0)","033aec5d":"data","b37ef5d5":"# sns.boxplot(x= 'CHAS', y='AGE', hue = 'CHAS', data=data, palette='Set3')\n# sns.boxplot(x= 'RM', data=data)","fb441330":"# sns.displot(data, x='RM', y='AGE', hue='CHAS')","f0200663":"data_X = data.iloc[:,:-1]\ndata_y = data.iloc[:,-1]","2516792e":"fig, ax = plt.subplots(ncols = 7, nrows = 2, figsize = (20, 15))\nindex = 0\nax = ax.flatten()\n\nfor col, value in data_X.items():\n    sns.boxplot(y=col, data=data_X, ax=ax[index])\n    index += 1\nplt.tight_layout(pad = 0.5, w_pad=0.7, h_pad=5.0)","ae138451":"# Ordinary Least square\nmodel_ols = LinearRegression().fit(data_X, data_y)","36377bb9":"model_ols.score(data_X, data_y)","7efb9970":"size, train_scores, valid_scores = learning_curve(model_ols, data_X, data_y)","be02ddaf":"plt.plot(size, train_scores)\nplt.xlabel('Training data size')\nplt.ylabel('Score')","5891ef61":"plt.plot(size, valid_scores)\nplt.xlabel('Validation data size')\nplt.ylabel('Score')","8a3dbce2":"plt.plot(size, train_scores[0], label='Training curve',color='red')\nplt.plot(size, valid_scores[0],label='Cross validation score', color='green')\nplt.legend()\nplt.xlabel('Dataset size')\nplt.ylabel('Score')","1702478b":"plt.plot(size, train_scores, label='Training curve')\nplt.plot(size, valid_scores,label='Cross validation score')\nplt.legend()","5213ea6c":"plt.figure(figsize=(20, 30))\n\nfor i,column in enumerate(data_X.columns):\n    plt.subplot(5,3, i+1)\n    plt.plot(data_X[column], data_y,'o', label=f'{column} actual data')\n    # scatter plot for predicted data\n    plt.plot(data_X.drop_duplicates(subset=[column])[column],\n             model_ols.predict(data_X.drop_duplicates(subset=[column])), 'o', label='predicted price')\n    # scatter plot for the given data\n    plt.plot(data_X.drop_duplicates(subset=[column])[column], \n             data.drop_duplicates(subset=[column])['MEDV'], 'o', label='actual price')\n    # straight line for the given data\n    plt.plot(np.unique(data_X[column]), np.poly1d(np.polyfit(data_X[column], data_y, 1))(np.unique(data_X[column])), label='actual price')\n    plt.xlabel(column)\n    plt.ylabel('Prices')\n    plt.legend()","ec024625":"linearly_varying_data = data_X\nshuffled_X, shuffled_y = shuffle(linearly_varying_data, data_y)\nX_train, X_test, y_train, y_test = train_test_split(shuffled_X, shuffled_y, test_size=0.2, random_state=10)\nmodel_ols = LinearRegression().fit(X_train, y_train)\nprint(f\"Training r2_score {model_ols.score(X_train, y_train)}\")","d7f582ba":"print(\"Testing data results : \")\nrsq_score = r2_score(model_ols.predict(X_test),y_test)\nmse = mean_squared_error(model_ols.predict(X_test),y_test)\nrmse = mean_squared_error(model_ols.predict(X_test),y_test, squared=False)\nmae = mean_absolute_error(model_ols.predict(X_test),y_test)\nprint(f\"R2_Score : {rsq_score} \\nMean Squared Error : {mse} \\nRoot Mean Squared Error : {rmse}\\nMean Absolute Error : {mae}\")","12a6f3be":"x_compare = pd.DataFrame({'Actual':y_train.head(10), 'Predicted':model_ols.predict(X_train.head(10))})\nx_compare.plot(kind='bar', title='Ordinary Least Square')","cd520705":"model_SVR = SVR()\nmodel_SVR.fit(X_train, y_train)","91615572":"model_SVR.score(data_X, data_y)","350b6bf1":"print(\"Testing data results : \")\nrsq_score = r2_score(model_SVR.predict(X_test),y_test)\nmse = mean_squared_error(model_SVR.predict(X_test),y_test)\nrmse = mean_squared_error(model_SVR.predict(X_test),y_test, squared=False)\nmae = mean_absolute_error(model_SVR.predict(X_test),y_test)\nprint(f\"R2_Score : {rsq_score} \\nMean Squared Error : {mse} \\nRoot Mean Squared Error : {rmse}\\nMean Absolute Error : {mae}\")","ab4f1ac3":"x_compare = pd.DataFrame({'Actual':y_train.head(10), 'Predicted':model_SVR.predict(X_train.head(10))})\nx_compare.plot(kind='bar', title='Support Vector Regressor')","64d806ae":"model_rfr = RandomForestRegressor(n_estimators=100)","1638f33d":"model_rfr.fit(X_train, y_train)","ea32b64d":"model_rfr.score(X_train, y_train)","9e3e4c16":"print(\"Testing data results : \")\nrsq_score = r2_score(model_rfr.predict(X_test),y_test)\nmse = mean_squared_error(model_rfr.predict(X_test),y_test)\nrmse = mean_squared_error(model_rfr.predict(X_test),y_test, squared=False)\nmae = mean_absolute_error(model_rfr.predict(X_test),y_test)\nprint(f\"R2_Score : {rsq_score} \\nMean Squared Error : {mse} \\nRoot Mean Squared Error : {rmse}\\nMean Absolute Error : {mae}\")","a6aad8fe":"model_rfr.score(X_test, y_test)","5376a20b":"size, train_scores, valid_scores = learning_curve(model_rfr, data_X, data_y)","15cb5b0d":"plt.plot(size, train_scores)\nplt.xlabel('Training data size')\nplt.ylabel('Score')","7f399967":"plt.plot(size, valid_scores)\nplt.xlabel('Validation data size')\nplt.ylabel('Score')","84d08043":"plt.plot(size, train_scores[0], label='Training curve',color='red')\nplt.plot(size, valid_scores[0],label='Cross validation score', color='green')\nplt.legend()\nplt.xlabel('Dataset size')\nplt.ylabel('Score')","70481d3e":"plt.plot(size, train_scores, label='Training curve')\nplt.plot(size, valid_scores,label='Cross validation score')\n# plt.legend()","22cd2d6f":"x_compare = pd.DataFrame({'Actual':y_train.head(10), 'Predicted':model_rfr.predict(X_train.head(10))})\nx_compare.plot(kind='bar', title='Random Forest Regressor')","131a7ed7":"model_knn = KNeighborsRegressor(n_neighbors=10)\nmodel_knn.fit(X_train, y_train)","fb824201":"model_knn.score(X_train, y_train)","b1b7c293":"print(\"Testing data results : \")\nrsq_score = r2_score(model_knn.predict(X_test),y_test)\nmse = mean_squared_error(model_knn.predict(X_test),y_test)\nrmse = mean_squared_error(model_knn.predict(X_test),y_test, squared=False)\nmae = mean_absolute_error(model_knn.predict(X_test),y_test)\nprint(f\"R2_Score : {rsq_score} \\nMean Squared Error : {mse} \\nRoot Mean Squared Error : {rmse}\\nMean Absolute Error : {mae}\")","39de1b3c":"x_compare = pd.DataFrame({'Actual':y_train.head(10), 'Predicted':model_knn.predict(X_train.head(10))})\nx_compare.plot(kind='bar', title='K-Nearest-Neighbour')","b1a06a57":"test_data = pd.read_csv('\/kaggle\/input\/test-data\/test.csv')","537b1285":"test_data","6face0fb":"test_data_modified = test_data.drop(columns=['ID', 'tax', 'dis', 'chas'])","107c8799":"pd.DataFrame({'id':test_data.ID, 'medv':model_rfr.predict(test_data_modified)}).to_csv('\/kaggle\/working\/output.csv', index=None)","ac920512":"> Here the target variable `MEDV` has the least correlation with `CHAS` so it can be removed","2eb355be":"> Here `RAD` and `TAX` has the max correlation of 0.91, so we can remove either one of them.","140c2fda":"> There are a lot of outliers in columns : `CRIM, ZN, RM, DIS, B, LSTAT`","4615f24a":"> We will now plot the normal distribution for every feature data."}}