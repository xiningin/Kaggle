{"cell_type":{"d00329bb":"code","e0d9cff2":"code","6897e85e":"code","654895f5":"code","f7066485":"code","b7d913ef":"code","69bcb153":"code","06e11e0a":"code","9c622d3c":"code","5a5125ac":"code","578900df":"code","5ef49fc0":"code","1be1e713":"code","10e7f6ba":"code","10ba049a":"code","8a436ffa":"code","b85dbdba":"code","42255d82":"code","e79dab52":"code","29751416":"code","eacf48dd":"code","0b84682b":"code","3f5b1043":"code","994fc268":"code","59977f76":"code","ac592a64":"code","a210e4cc":"code","d6bae67b":"code","3e6de6ba":"code","cb879305":"code","eef01515":"code","ca6fe1de":"code","95de0e86":"code","fbc460da":"code","e9bb83cf":"code","4bfc3665":"code","93597499":"code","0d83c0ab":"code","f677f6ed":"code","1ee931c0":"code","d823f36d":"code","77ec141e":"code","d1c77f60":"code","2bc8d7ec":"code","8907bc87":"code","26565de3":"code","34f1fa3d":"code","8e735bb6":"code","d43fd0f1":"code","513eb174":"code","0d0ceb3b":"code","934f140c":"code","e7638da7":"code","4cd7cf94":"code","bf0e970a":"code","46795344":"code","9f0946fe":"code","53d0df72":"code","2355b780":"code","df0a1030":"code","ab65042a":"code","657db41f":"code","9b7260c7":"code","620a9f00":"code","e9be2a70":"code","f36eb198":"code","ac01f4e8":"code","aede8256":"code","cd8ebf99":"code","6d9f79a5":"code","d000917e":"code","8132fcf8":"code","2cf67aee":"code","908028b9":"code","ffcf6345":"code","f8b894fe":"code","0ac08318":"code","42f2a8fc":"code","f39e32e5":"code","fec0f1bb":"code","77bfb93b":"code","21c7c90a":"code","25f8b505":"code","7ef2789f":"code","437065ae":"code","be22716c":"code","45c5d1fa":"code","5a7ea57a":"code","5bf678c1":"code","644c7469":"code","d03543d2":"code","eb8b7a64":"code","b182cebd":"code","0daeaee3":"code","fa2d6a8a":"code","79d77b56":"code","00293b8d":"code","a8645b18":"code","b1f15b60":"code","4ea24bde":"code","4d4ecd01":"code","1c6e6037":"code","a50c6583":"code","8baa7696":"code","32d73bc9":"code","9e2c5aa3":"code","361fc52b":"code","3efe5d34":"code","8bdc8c2b":"code","59f87572":"code","10c3bb78":"code","e7d09ec6":"code","de6838f0":"code","a587a3ac":"code","9b8054c9":"code","118f66c7":"code","add784de":"code","ab4f3ff2":"code","88de77d2":"code","38961446":"code","3d9fd1fe":"code","035aade3":"code","6c721f13":"code","03aba0f6":"code","7e4625db":"code","491e2e07":"code","ceea1ff4":"code","50da07ca":"code","15929e54":"code","08c2c0c2":"code","8313a529":"code","42119137":"code","8c8bce91":"code","ce6bddeb":"code","2b9f7c25":"code","7c5fab2f":"code","b474d748":"markdown","bbe73e9f":"markdown","573b9525":"markdown","6240deae":"markdown","52f2071f":"markdown","fd15541c":"markdown","35c73253":"markdown","ddacdcb6":"markdown","4a4e867f":"markdown","30250b38":"markdown","2514d907":"markdown","597aef0e":"markdown","1e80ec80":"markdown","a4f59bc8":"markdown","217b996d":"markdown","6abc3091":"markdown","20dd8be8":"markdown","f6909fb9":"markdown","6c57e03c":"markdown","890fc127":"markdown","a620d94f":"markdown","4a61503a":"markdown","3dcde888":"markdown","a18d2709":"markdown","5a97d49a":"markdown","94523bc9":"markdown","26ab7d85":"markdown","b9b27221":"markdown","e05639db":"markdown","52e5ca9a":"markdown","f3751847":"markdown","b0297998":"markdown","932b78d8":"markdown","64136864":"markdown","e6a4424d":"markdown","96746001":"markdown","191ba4c1":"markdown","ae9e0545":"markdown","95a2b092":"markdown","4334520f":"markdown","5eea8c02":"markdown","fade217a":"markdown","c3f4c24e":"markdown","fcfe9e77":"markdown","c54dc780":"markdown","ab3475b0":"markdown","6ebdb0c4":"markdown","f85a76de":"markdown","5f0f6ede":"markdown","a1e9c8b6":"markdown","ddefb3d5":"markdown","9de1b0d2":"markdown","9815c1ef":"markdown","865c7f4e":"markdown","b290fc93":"markdown","169368c4":"markdown","9be37381":"markdown"},"source":{"d00329bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0d9cff2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","6897e85e":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","654895f5":"train_len = len(train)","f7066485":"#percentage of nan values present in each feature\nfeatures_nan=[feature for feature in train.columns if train[feature].isnull().sum()>0]\n\nfor feature in features_nan:\n    print(feature, np.round(train[feature].isnull().mean()*100, 2),  ' % missing values')","b7d913ef":"num = train[['Age','SibSp','Parch','Fare']]\ncat = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]\n#num, #cat","69bcb153":"#distributions for all numeric variables \nfor i in num.columns:\n    plt.hist(num[i])\n    plt.title(i)\n    plt.show()","06e11e0a":"print(num.corr())","9c622d3c":"#compare survival rate across Age, SibSp, Parch, and Fare \n#pd.pivot_table(train, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","5a5125ac":"for i in cat.columns:\n    sns.barplot(cat[i].value_counts().index,cat[i].value_counts()).set_title(i)\n    plt.show()","578900df":"print(pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","5ef49fc0":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ntrain['Deck'] = train['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ntrain.Deck.unique()","1be1e713":"train_deck = train.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', \n                                                                        'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\ntrain_deck","10e7f6ba":"# Passenger in the T deck is changed to A\nidx = train[train['Deck'] == 'T'].index\ntrain.loc[idx, 'Deck'] = 'A'","10ba049a":"train_survived = train.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass',\n                                                                                   'Cabin', 'PassengerId', \n                                                                                   'Ticket']).rename(columns={'Name':'Count'}).transpose()\ntrain_survived","8a436ffa":"pd.pivot_table(train,index='Survived',columns='Deck', values = 'Name', aggfunc='count')","b85dbdba":"deck_mapping = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4,\"E\": 5,\"F\": 6,\"G\": 7,\"M\": 8 }\ntrain['Deck'] = train['Deck'].map(deck_mapping)","42255d82":"train.head()","e79dab52":"#percentage of nan values present in each feature\nfeatures_nan=[feature for feature in test.columns if train[feature].isnull().sum()>0]\n\nfor feature in features_nan:\n    print(feature, np.round(test[feature].isnull().mean()*100, 2),  ' % missing values')","29751416":"test['Deck'] = test['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ntest.Deck.unique()","eacf48dd":"test_deck = test.groupby(['Deck', 'Pclass']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', \n                                                                        'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\ntest_deck","0b84682b":"deck_mapping = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4,\"E\": 5,\"F\": 6,\"G\": 7,\"M\": 8 }\ntest['Deck'] = test['Deck'].map(deck_mapping)","3f5b1043":"test.head()","994fc268":"dataset = pd.concat([train, test], sort=True).reset_index(drop=True)\ndataset.head()","59977f76":"sns.distplot(dataset['Age'].dropna())","ac592a64":"figure=dataset.Age.hist(bins=50)\nfigure.set_title('Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('No of passenger')","a210e4cc":"figure=dataset.boxplot(column=\"Age\")","d6bae67b":"dataset['Age'].describe()","3e6de6ba":"uppper_boundary=dataset['Age'].mean() + 3* dataset['Age'].std()\nlower_boundary=dataset['Age'].mean() - 3* dataset['Age'].std()\nprint(lower_boundary), print(uppper_boundary),print(dataset['Age'].mean())","cb879305":"dataset.loc[dataset['Age']>=73,'Age']=73","eef01515":"figure=dataset.Fare.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","ca6fe1de":"dataset.boxplot(column=\"Fare\")","95de0e86":"dataset['Fare'].describe()","fbc460da":"#### Lets compute the Interquantile range to calculate the boundaries\nIQR=dataset.Fare.quantile(0.75)-dataset.Fare.quantile(0.25)","e9bb83cf":"lower_bridge=dataset['Fare'].quantile(0.25)-(IQR*3)\nupper_bridge=dataset['Fare'].quantile(0.75)+(IQR*3)\nprint(lower_bridge), print(upper_bridge)","4bfc3665":"dataset.loc[dataset['Fare']>=100,'Fare']=101","93597499":"figure=dataset.Age.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","0d83c0ab":"figure=dataset.Fare.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","f677f6ed":"dataset['num_Ticket'] = dataset.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ndataset['num_Ticket'].value_counts()","1ee931c0":"dataset['Ticket_letters'] = dataset.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').\n                                             replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\npd.set_option(\"max_rows\", None)\ndataset['Ticket_letters'].value_counts()","d823f36d":"dataset.head()","77ec141e":"#extract Title from Name \ndataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\npd.crosstab(dataset['Title'], dataset['Sex'])","d1c77f60":"title_mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs' }\ndataset.replace({'Title': title_mapping}, inplace=True)\n#titles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n#dataset['Title'] = dataset['Title'].map(title_mapping)","2bc8d7ec":"#encode title variables\ntitle_num_mapping = {\"Dr\": 1, \"Master\": 2, \"Miss\": 3, \"Mr\": 4,\"Mrs\": 5,\"Rev\": 6}\ndataset['Title'] = dataset['Title'].map(title_num_mapping)","8907bc87":"dataset.Title.unique()","26565de3":"#dataset['Title'] = dataset['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'Countess', 'Dona'], 'Miss\/Mrs\/Ms')\n#dataset['Title'] = dataset['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')","34f1fa3d":"# Extracting surnames from Name\ndataset['Last_Name'] = dataset['Name'].apply(lambda x: str.split(x, \",\")[0])","8e735bb6":"dataset.Last_Name.unique()","d43fd0f1":"#encode Embarked \nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","513eb174":"#encode Sex \nsex_mapping = {\"female\": 1, \"male\": 2}\ndataset['Sex'] = dataset['Sex'].map(sex_mapping)","0d0ceb3b":"dataset.head()","934f140c":"dataset[dataset['Age'].isnull()]\n#null_data = df[df.isnull().any(axis=1)]\n#df_all[df_all['Embarked'].isnull()]","e7638da7":"dataset['Age'].isnull().sum()","4cd7cf94":"age_corr=dataset.corr().abs().unstack().sort_values(ascending=False).reset_index()\n#dataframe index is : [1,5,6,10,11] and I would like to reset it to [0,1,2,3,4] using reset_index()\n#Reshaping the data using stack(), the column is stacked row wise\n#default sorting algorithm is 'quicksort'\nage_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\nage_corr[age_corr['Feature 1'] == 'Age']","bf0e970a":"age_by_pclass_sex = dataset.groupby(['Sex', 'Pclass','Title','Deck','SibSp','Parch']).median()['Age']\nage_by_pclass_sex","46795344":"#taking care of missing values in Age\ndataset['Age'] = dataset.groupby(['Pclass','SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))","9f0946fe":"dataset.Age.isnull().sum()","53d0df72":"#create an Ageband \ndataset['Ageband'] = pd.qcut(dataset['Age'], 10)\npd.pivot_table(dataset,index='Survived',columns='Ageband', values = 'Name', aggfunc='count')","2355b780":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndataset['Ageband'] = encoder.fit_transform(dataset['Ageband'])\n#dataset['AgeBin'] = pd.qcut(dataset['Age'], 4).astype(int)\n\n#label = LabelEncoder()\n#dataset['AgeBand'] = encoder.fit_transform(dataset['AgeBin'])\n\n#dataset.drop(['Age'], 1, inplace=True)","df0a1030":"#taking care of missing values in Embarked\ndataset['Embarked'] = dataset['Embarked'].fillna('1')#S:1","ab65042a":"fare_corr=dataset.corr().abs().unstack().sort_values(ascending=False).reset_index()\n#dataframe index is : [1,5,6,10,11] and I would like to reset it to [0,1,2,3,4] using reset_index()\n#Reshaping the data using stack(), the column is stacked row wise\n#default sorting algorithm is 'quicksort'\nfare_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\nfare_corr[fare_corr['Feature 1'] == 'Fare']","657db41f":"dataset[dataset['Fare'].isnull()]","9b7260c7":"med_fare=dataset.groupby(['SibSp', 'Parch', 'Pclass','Sex'])['Fare'].median()[0][0][3][2]\nmed_fare\ndataset['Fare']=dataset['Fare'].fillna(med_fare)","620a9f00":"med_fare","e9be2a70":"dataset.isnull().sum()","f36eb198":"#create Family_Size and IsAlone\ndataset['Family_Size'] = dataset['SibSp'] + dataset['Parch'] + 1\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndataset['Family_Size_Band'] = dataset['Family_Size'].map(family_map)\ndataset['IsAlone'] = dataset['Family_Size'].map(lambda s: 1\n                                                   if s == 1 else 0)","ac01f4e8":"DEFAULT_SURVIVAL_VALUE = 0.5\ndataset['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in dataset[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                dataset.loc[dataset['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                dataset.loc[dataset['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in dataset.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    dataset.loc[dataset['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    dataset.loc[dataset['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \n\nsns.catplot(x=\"Family_Size\",y=\"Survived\",data = dataset.iloc[:train_len],kind=\"bar\")","aede8256":"#create a Fareband\ndataset['Fareband'] = pd.qcut(dataset['Fare'], 13)\npd.pivot_table(dataset,index='Survived',columns='Fareband', values = 'Name', aggfunc='count')","cd8ebf99":"dataset.head()","6d9f79a5":"#title_mapping = {\"Mr\": 0, \"Miss\/Mrs\/Ms\": 1, \"Master\": 2, \"Dr\/Military\/Noble\/Clergy\": 3}\n#dataset['Title'] = dataset['Title'].map(title_mapping)","d000917e":"#sex_mapping = {\"female\": 0, \"male\": 1}\n#dataset['Sex'] = dataset['Sex'].map(sex_mapping)","8132fcf8":"#embarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\n#dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","2cf67aee":"dataset.Fareband=encoder.fit_transform(dataset.Fareband)\ndataset.Family_Size_Band=encoder.fit_transform(dataset.Family_Size_Band)","908028b9":"#remove unwanted columns part 1\ndataset.drop(labels=['Cabin', 'Fare', 'Age', 'Name', 'PassengerId', 'Ticket', ], axis=1, inplace=True)","ffcf6345":"dataset.drop(labels=['Parch','SibSp','Survived','Last_Name'], axis=1, inplace=True)","f8b894fe":"dataset.drop(labels=['Ticket_letters'], axis=1, inplace=True)","0ac08318":"dataset.head()","42f2a8fc":"dataset.shape","f39e32e5":"#created dummy variables from categories (also can use OneHotEncoder)\n#dummies = pd.get_dummies(dataset[['Deck','Pclass','Sex','Fareband',\n                                  #'Embarked','Ageband', 'Family_Size','Family_Size_Band','num_Ticket','Title',\n                                 #'IsAlone','Family_Survival']])","fec0f1bb":"#dummies.shape","77bfb93b":"#dummies.head()","21c7c90a":"train_df = dataset.loc[:890]\ntest_df = dataset.loc[891:]","25f8b505":"train_df.columns","7ef2789f":"test_df.shape","437065ae":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train=scaler.fit_transform(train_df)","be22716c":"X_test=scaler.fit_transform(test_df)","45c5d1fa":"y_train=train.Survived\ny_train.head()","5a7ea57a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","5bf678c1":"gnb = GaussianNB()\ncv = cross_val_score(gnb,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","644c7469":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","d03543d2":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","eb8b7a64":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","b182cebd":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","0daeaee3":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","fa2d6a8a":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","79d77b56":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),\n                                            ('rf',rf),('gnb',gnb),('svc',svc),\n                                            ('xgb',xgb),('dt',dt)], voting = 'soft') \ncv = cross_val_score(voting_clf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","00293b8d":"voting_clf.fit(X_train,y_train)\ny_hat_base_vc = voting_clf.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nbasic_submission_vc = {'PassengerId': testing.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission_vc = pd.DataFrame(data=basic_submission_vc)\nbase_submission_vc.to_csv('base_submission_votingclf.csv', index=False)","a8645b18":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","b1f15b60":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","4ea24bde":"from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, cross_val_score\nkfold = StratifiedKFold(n_splits=8)","4d4ecd01":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","1c6e6037":"'''Logistic Regression\nBest Score: 0.8328004343629344\nBest Parameters: {'C': 0.23357214690901212, 'max_iter': 2000, 'penalty': 'l1', 'solver': 'liblinear'}'''","a50c6583":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train,y_train)\nclf_performance(best_clf_knn,'KNN')","8baa7696":"'''KNN\nBest Score: 0.8350072186303434\nBest Parameters: {'algorithm': 'ball_tree', 'n_neighbors': 9, 'p': 1, 'weights': 'uniform'}'''\n#here, we got better score for cv=5 than cv=kfold!!!","32d73bc9":"y_hat_knn = clf_knn.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nbasic_submission_knn = {'PassengerId': testing.PassengerId, 'Survived': y_hat_knn}\nsubmission_knn = pd.DataFrame(data=basic_submission_knn)\nsubmission_knn.to_csv('submission_clf_knn.csv', index=False)","9e2c5aa3":"'''svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train,y_train)\nclf_performance(best_clf_svc,'SVC')\n\ny_hat_svc = clf_svc.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nsvc_submission = {'PassengerId': testing.PassengerId, 'Survived': y_hat_svc}\nsubmission_svc = pd.DataFrame(data=svc_submission)\nsubmission_svc.to_csv('submission_clf_svc.csv', index=False)'''\n\n'''SVC\nBest Score: 0.8428410018203504\nBest Parameters: {'C': 10, 'degree': 2, 'kernel': 'poly'}'''","361fc52b":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['poly'], 'degree' : [2], 'C': [10]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train,y_train)\nclf_performance(best_clf_svc,'SVC')","3efe5d34":"y_hat_svc = clf_svc.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nsvc_submission = {'PassengerId': testing.PassengerId, 'Survived': y_hat_svc}\nsubmission_svc = pd.DataFrame(data=svc_submission)\nsubmission_svc.to_csv('submission_clf_svc.csv', index=False)","8bdc8c2b":"'''rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,4,5,6,7,8,9,10,15,20,50,None],\n                                  'max_features': [3,'auto','sqrt','log2'],\n                                  'bootstrap': [False, True],\n                                  'criterion': ['gini', 'entropy'],\n                                  'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],\n                                  'min_samples_split': [2 ,3,4,5,6,7,8,9,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 200, \ncv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')'''\n\n'''y_hat_rf_rnd = clf_rf_rnd.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nrf_submission = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf_rnd}\nsubmission_rf = pd.DataFrame(data=rf_submission)\nsubmission_rf.to_csv('submission_clf_rf_rnd.csv', index=False)'''\n\n'''Random Forest\nBest Score: 0.8495762977841943\nBest Parameters: {'n_estimators': 100, 'min_samples_split': 3, \n'min_samples_leaf': 9, 'max_features': 'sqrt', 'max_depth': 4, 'criterion': 'gini', 'bootstrap': True}'''","59f87572":"'''#with stratified kfold\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,4,5,6,7,8,9,10,15,20,50,None],\n                                  'max_features': [3,'auto','sqrt','log2'],\n                                  'bootstrap': [False, True],\n                                  'criterion': ['gini', 'entropy'],\n                                  'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],\n                                  'min_samples_split': [2 ,3,4,5,6,7,8,9,10]}\n                                  \nclf_rf_rnd_2 = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 200, cv = kfold,\nverbose = True, n_jobs = -1)\nbest_clf_rf_rnd_2 = clf_rf_rnd_2.fit(X_train,y_train)\nclf_performance(best_clf_rf_rnd_2,'Random Forest')'''\n\n'''y_hat_rf_rnd_2 = clf_rf_rnd_2.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nrf_submission = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf_rnd_2}\nsubmission_rf = pd.DataFrame(data=rf_submission)\nsubmission_rf.to_csv('submission_clf_rf_rnd_2.csv', index=False)'''\n\n'''Random Forest\nBest Score: 0.8529701576576576\nBest Parameters: {'n_estimators': 400, 'min_samples_split': 2, 'min_samples_leaf': 6,\n'max_features': 3, 'max_depth': 4, 'criterion': 'gini', 'bootstrap': True}'''","10c3bb78":"#with stratified kfold\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400], \n                                  'bootstrap': [True],\n                                  'max_depth': [4],\n                                  'max_features': [3],\n                                  'bootstrap': [True],\n                                  'criterion': ['gini'],\n                                  'min_samples_leaf': [6],\n                                  'min_samples_split': [2]}\n                                  \nclf_rf_rnd_2 = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 200, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd_2 = clf_rf_rnd_2.fit(X_train,y_train)\nclf_performance(best_clf_rf_rnd_2,'Random Forest')","e7d09ec6":"'''Random Forest\nBest Score: 0.8529701576576576\nBest Parameters: {'n_estimators': 400, 'min_samples_split': 2,\n'min_samples_leaf': 6, 'max_features': 3, 'max_depth': 4, 'criterion': 'gini', 'bootstrap': True}'''","de6838f0":"y_hat_rf_rnd_2 = clf_rf_rnd_2.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission_rf_rnd = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf_rnd_2}\nsubmission_rf_rnd_2 = pd.DataFrame(data=submission_rf_rnd)\nsubmission_rf_rnd_2.to_csv('submission_clf_rf_rnd_2.csv', index=False)","a587a3ac":"'''rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,150,200,300,400,500],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [3,4,5,6],\n                                  'max_features': ['sqrt'],\n                                  'min_samples_leaf': [1,5,9],\n                                  'min_samples_split': [2,3,4,5]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,y_train)\nclf_performance(best_clf_rf,'Random Forest')'''\n\n'''Random Forest\nBest Score: 0.8529470843010483\nBest Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', \n'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}'''","9b8054c9":"'''rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,200,300,400,500],\n               'criterion':['gini'],\n                                  'bootstrap': [False, True],\n                                  'max_depth': [3,4,5,None],\n                                  'max_features': ['sqrt'],\n                                  'min_samples_leaf': [4,5,6,7],\n                                  'min_samples_split': [1,2,3,4]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,y_train)\nclf_performance(best_clf_rf,'Random Forest')'''\n\n'''Random Forest\nBest Score: 0.8541063384813385\nBest Parameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt',\n'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}'''","118f66c7":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100],\n               'criterion':['gini'],\n                                  'bootstrap': [True],\n                                  'max_depth': [4],\n                                  'max_features': ['sqrt'],\n                                  'min_samples_leaf': [4],\n                                  'min_samples_split': [2]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,y_train)\nclf_performance(best_clf_rf,'Random Forest')","add784de":"'''Random Forest\nBest Score: 0.8541063384813385\nBest Parameters: {'bootstrap': True, 'criterion': 'gini',\n'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}'''","ab4f3ff2":"y_hat_rf_grid = best_clf_rf.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nrf_submission_grid = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf_grid}\nsubmission_rf_grid = pd.DataFrame(data=rf_submission_grid)\nsubmission_rf_grid.to_csv('submission_clf_rf_grid.csv', index=False)","88de77d2":"best_rf = best_clf_rf.best_estimator_.fit(X_train,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=train_df.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","38961446":"clf_rf_3=RandomForestClassifier(criterion='gini', n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=1,\n                                           n_jobs=-1,\n                                           verbose=1)\nbest_clf_rf_3 = clf_rf_3.fit(X_train,y_train)\ny_hat_rf_3 = best_clf_rf_3.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nrf_submission_3 = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf_3}\nsubmission_rf_3 = pd.DataFrame(data=rf_submission_3)\nsubmission_rf_3.to_csv('submission_rf_3.csv', index=False)","3d9fd1fe":"clf_rf_4=RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=1,\n                                           n_jobs=-1,\n                                           verbose=1)\nbest_clf_rf_4 = clf_rf_4.fit(X_train,y_train)\ny_hat_rf_4 = best_clf_rf_4.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nrf_submission_4 = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf_4}\nsubmission_rf_4 = pd.DataFrame(data=rf_submission_4)\nsubmission_rf_4.to_csv('submission_rf_4.csv', index=False)","035aade3":"rf_param_grid_best = {\"max_depth\": [None],\n              \"max_features\": [3],\n              \"min_samples_split\": [4],\n              \"min_samples_leaf\": [5],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[200],\n              \"criterion\": [\"gini\"]}\n\ngs_rf = GridSearchCV(rf, param_grid = rf_param_grid_best, cv=kfold, n_jobs= -1, verbose = 1)\n\ngs_rf.fit(X_train, y_train)\nrf_best = gs_rf.best_estimator_\nprint(f'RandomForest GridSearch best params: {gs_rf.best_params_}')\nprint(f'RandomForest GridSearch best score: {gs_rf.best_score_}')","6c721f13":"y_hat_gs_rf = gs_rf.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nrf_submission_gs = {'PassengerId': testing.PassengerId, 'Survived': y_hat_gs_rf}\nsubmission_rf_gs = pd.DataFrame(data=rf_submission_gs)\nsubmission_rf_gs.to_csv('submission_gs_rf.csv', index=False)","03aba0f6":"'''xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250,300,400, 500,600,700,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7,0.75, 0.8,0.85, 0.9, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2,2.5,3,4],\n    'subsample': [0.5,0.55,0.6,0.65,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.05,0.1,0.2,0.3,0.5,0.6,0.7,0.9],\n    'gamma':[0,.01,.1,.5,1,10,100],\n    'min_child_weight':[0,.01,0.05,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 2000, \ncv = kfold, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')'''\n\n'''XGB\nBest Score: 0.8575048262548263\nBest Parameters: {'subsample': 0.7, 'sampling_method': 'uniform', \n'reg_lambda': 2, 'reg_alpha': 0.5, 'n_estimators': 700, 'min_child_weight': 0, 'max_depth': 10,\n'learning_rate': 0.01, 'gamma': 0.01, 'colsample_bytree': 0.75}'''","7e4625db":"'''xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [550,600,650],\n    'colsample_bytree': [0.9,0.95,1],\n    'max_depth': [12,15,20],\n    'reg_alpha': [0.5,1],\n    'reg_lambda': [0.5,1],\n    'subsample': [0.85,0.9,0.95],\n    'learning_rate':[0.85,0.9,0.95],\n    'gamma':[8,10,12],\n    'min_child_weight':[0],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,y_train)\nclf_performance(best_clf_xgb,'XGB')'''","491e2e07":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [700],\n    'colsample_bytree': [0.75],\n    'max_depth': [10],\n    'reg_alpha': [0.5],\n    'reg_lambda': [2],\n    'subsample': [0.7],\n    'learning_rate':[0.01],\n    'gamma':[.01],\n    'min_child_weight':[0],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 2000, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')","ceea1ff4":"'''XGB\nBest Score: 0.8575048262548263\nBest Parameters: {'subsample': 0.7, 'sampling_method': 'uniform', 'reg_lambda': 2, 'reg_alpha': 0.5, 'n_estimators': 700, 'min_child_weight': 0, 'max_depth': 10,\n'learning_rate': 0.01, 'gamma': 0.01, 'colsample_bytree': 0.75}'''","50da07ca":"y_hat_xgb_rnd = best_clf_xgb_rnd.best_estimator_.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\nxgb_submission_rnd = {'PassengerId': testing.PassengerId, 'Survived': y_hat_xgb_rnd}\nsubmission_xgb = pd.DataFrame(data=xgb_submission_rnd)\nsubmission_xgb.to_csv('submission_xgb_rnd.csv', index=False)","15929e54":"xgb_param_grid_best = {'learning_rate':[0.1], \n                  'reg_lambda':[0.3],\n                  'gamma': [1],\n                  'subsample': [0.8],\n                  'max_depth': [2],\n                  'n_estimators': [300]\n              }\n\ngs_xgb = GridSearchCV(xgb, param_grid = xgb_param_grid_best, cv=kfold, n_jobs= -1, verbose = 1)\n\ngs_xgb.fit(X_train,y_train)\n\nxgb_best = gs_xgb.best_estimator_\nprint(f'XGB GridSearch best params: {gs_xgb.best_params_}')\nprint(f'XGB GridSearch best score: {gs_xgb.best_score_}')","08c2c0c2":"y_hat_gs_xgb = gs_xgb.predict(X_test).astype(int)\ntesting=pd.read_csv('..\/input\/titanic\/test.csv')\ngs_xgb_submission = {'PassengerId': testing.PassengerId, 'Survived': y_hat_gs_xgb}\nsubmission_gs_xgb = pd.DataFrame(data=gs_xgb_submission)\nsubmission_gs_xgb.to_csv('submission_gs_xgb_new.csv', index=False)","8313a529":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf\nbest_xgb = best_clf_xgb_rnd.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc),\n                                                ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), \n                                                ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=kfold))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=kfold).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=kfold))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=kfold).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=kfold))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=kfold).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=kfold))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=kfold).mean())","42119137":"#in a soft voting classifier you can weight some models more than others. I used a grid search to explore different weightings\n#no new results here\nparams = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = kfold, verbose = True, n_jobs = -1)\nbest_clf_weight = vote_weight.fit(X_train,y_train)\nclf_performance(best_clf_weight,'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test)","8c8bce91":"#Make Predictions \nvoting_clf_hard.fit(X_train, y_train)\nvoting_clf_soft.fit(X_train, y_train)\nvoting_clf_all.fit(X_train, y_train)\nvoting_clf_xgb.fit(X_train, y_train)\n\nbest_rf.fit(X_train, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test).astype(int)\ny_hat_rf = best_rf.predict(X_test).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test).astype(int)","ce6bddeb":"#convert output to dataframe \nfinal_data = {'PassengerId': testing.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': testing.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': testing.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': testing.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': testing.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': testing.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","2b9f7c25":"#track differences between outputs \ncomparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)","7c5fab2f":"#prepare submission files \nsubmission.to_csv('submission_rf_aug.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard_aug.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft_aug.csv', index=False)\nsubmission_4.to_csv('submission_vc_all_aug.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb_aug.csv', index=False)","b474d748":"# Taking care of Outliers on numerical variables of combined dataset","bbe73e9f":"# 4. Random Forest","573b9525":"with randomized search cv to get a guess","6240deae":"# Encodings","52f2071f":"Importing the datasets","fd15541c":"# Comparing survival for low cardinality categorical variables ","35c73253":"# 2. KNN","ddacdcb6":"observe numeric and categorical values separately ","4a4e867f":"# 1. Logistic Regression","30250b38":"Since ,Cabin and Ticket have been used for creating new variables 'Deck', ['num_Ticket', & 'Ticket_letters']","2514d907":"# Check this out!!!\nthe model parameters were taken from another kernels. My best score was 0.81100. So, let's see how much we score in gs_rf, clf_rf_3, clf_rf_4. The main aim of these model parameters is to test how much better our model is comapred to some other kernels.   ","597aef0e":"# base_submission_votingclf.csv scored '0.77990'","1e80ec80":"Scaling the Dataset","a4f59bc8":"# This got me an accuracy of 0.80143","217b996d":"# The best score which I could come with was 0.81100.\n# As always, if we drop some columns which are a bit noisier (like Deck), and then if we perform a better feature tuning, we could come with a better accuracy score(results). ","6abc3091":"# 2.Fare","20dd8be8":"# This got me a score of 0.79425","f6909fb9":"# 5. XGB Classifier ","6c57e03c":"# This got me a score of 0.79665 ","890fc127":"# Create a new variable Family Survival","a620d94f":"# Taking care of Cabin Variable ","4a61503a":"# This got me a score of 0.78468","3dcde888":"Understanding Ticket Variable","a18d2709":"Now, the missing values have been taken care of","5a97d49a":"# Distributions of Variables in Training Datasets","94523bc9":"# Feature Engineering and taking care of missing values","26ab7d85":"Label Encodings","b9b27221":"Age is highly correlated with Pclass, Title, Deck, SibSp","e05639db":"Now, take care of some missing values","52e5ca9a":"# This got me a core of 0.79186","f3751847":"One Hot Encode other variables","b0297998":"# Let's consider Test Data","932b78d8":"# This got me a score of 0.80861","64136864":"# 1. Age","e6a4424d":"with gridsearch cv to get much better estimate","96746001":"# Importing the Libraries","191ba4c1":"# submission_vc_xgb_aug.csv scored 0.80861\n# submission_vc_all_aug.csv scored 0.81100\n# submission_vc_soft_aug.csv scored 0.79904\n# submission_vc_hard_aug.csv scored 0.80143\n# submission_rf_aug.csv scored 0.80382","ae9e0545":"# This got me a score of 0.80382","95a2b092":"# TITANIC ML DATASET(DM)\n\n# With the following code, I was able to get an accuracy score of 0.81100 (about 800-1000 rank). \nThe code is straightforward simple and easy to implement using ML Classifiers.\nVarious different classifiers were implemented, and RandomForestClassifier gave me the best score.\n\n# Feel free to upvote if you found this helpful.\n\nThanks","4334520f":"First, create Title and Surname(Last Name) from Name","5eea8c02":"On combined dataset, we will perform Feature Engineering","fade217a":"# Model Tuned Performance","c3f4c24e":"# Upvote if you found this kernel helpful!!!","fcfe9e77":"# Model Building","c54dc780":"# EXPLORING DATA USING VISUALIZATIONS","ab3475b0":"# Let's first consider Training Data\n\n\nNan values in the Training Set","6ebdb0c4":"# Now, we take care of the missing values","f85a76de":"with gs_rf a function copied from another kernel","5f0f6ede":"Again, the following model parameters were taken from another kernels.","a1e9c8b6":"Voting Classifier ","ddefb3d5":"This was taken from another kernel and increased the accuracy(by) manifold!!!","9de1b0d2":"Creating a Deck variable from Cabin Variable","9815c1ef":"# Encode categorical variables","865c7f4e":"# This got me a score of 0.81100","b290fc93":"# This got me a score of 0.81100","169368c4":"# 3. SVC","9be37381":"# Model Additional Ensemble Approaches"}}