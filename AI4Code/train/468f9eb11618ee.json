{"cell_type":{"4eff3626":"code","b85aae3e":"code","6099bf81":"code","b297b062":"code","a671ab40":"code","eb2a610f":"code","35dd8dd7":"code","f46e7b8a":"code","da2b6e3a":"code","d0415012":"code","2193a373":"code","3513ef9a":"code","3dd15539":"markdown","e0236f1c":"markdown","d2b0c46f":"markdown","691ab04f":"markdown","3651d13d":"markdown","7118a337":"markdown"},"source":{"4eff3626":"import os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport ast","b85aae3e":"df = pd.read_csv(\"\/kaggle\/input\/data\/data.csv\", index_col=0, \n                 converters={\"keyword_name\": ast.literal_eval, # these are list columns, need to use ast to actually get lists and not strings\n                            \"keyword_id\": ast.literal_eval,\n                            \"country_id\": ast.literal_eval,\n                            \"country_name\": ast.literal_eval,\n                            \"has_parts\": ast.literal_eval})\n\n# make merging later easier\ndf[\"creative_work_id\"] = df[\"creative_work_id\"].str.replace(\"\/\", \"\", n=1).str.replace(\"\/\", \"_\")\ndf.head(3)","6099bf81":"df.shape","b297b062":"df[df.type == \"http:\/\/schema.org\/NewsArticle\"].shape","a671ab40":"articles_text = [text_file for text_file in os.listdir(\"\/kaggle\/input\/data\/texts\") if text_file.endswith(\".txt\")]\n\ntexts = []\nfor path in articles_text:\n    with open(os.path.join(\"\/kaggle\/input\/data\/texts\", path), \"r\") as file:\n        texts.append(file.read())\n\ndata = pd.DataFrame().from_dict({\"path\": articles_text, \"text\": texts})\n\ndata[\"creative_work_id\"] = data[\"path\"].str.replace(\".txt\", \"\")\n\n# merge the texts with the rest of the data\ndata.merge(df, on=\"creative_work_id\", how=\"left\").head(3)","eb2a610f":"articles_html = [text_file for text_file in os.listdir(\"\/kaggle\/input\/data\/html\") if text_file.endswith(\".html\")]\n\nhtml = []\nfor path in articles_html:\n    with open(os.path.join(\"\/kaggle\/input\/data\/html\", path), \"r\") as file:\n        html.append(file.read())\n\nhtml[0][0:500]","35dd8dd7":"# !pip install newspaper3k ","f46e7b8a":"from newspaper import Article\nfrom newspaper.outputformatters import OutputFormatter\nfrom newspaper.configuration import Configuration\n\nconf = Configuration()\nconf.keep_article_html = True","da2b6e3a":"art = Article('', keep_article_html=True)\nart.set_html(html[0])\nart.parse()\n\nart.nlp()","d0415012":"art.keywords","2193a373":"art.summary","3513ef9a":"import re\nre.findall(r\"(?:<a .*?href=\\\")(.*?)(?:\\\")\", art.html)[0:12]","3dd15539":"\nWe can for example look for links on the web page:","e0236f1c":"## Reading in the data\n\nThere are basically three different data sets:\n- the meta data for each disinformation claim\n- the html page for each disinformation article\n- the raw text of a disinformation article as obtained by using the package `newspaper`\n\nThe meta data itself is already rich enough for analysis and it also contains a summary (in English) of each claim, few sentences why the claim is not true and a short abstract. \nOf course, the html pages contain the raw texts as well but they also contain much more information about for example links contained in the text and meta information of the web page. Note that many articles were not downloaded from the original web page but from the web archive. Only newsarticles were downloaded but since the csv data also contains disinformation claims of type media objects which were not downloaded. Unfortunately, not all web pages could be downloaded, in that case most likely both the original and the web archive link were not working anymore.","d2b0c46f":"### The csv meta data","691ab04f":"We can also load the html data instead:","3651d13d":"In total, there are 7369 news articles or media objects but only for the news articles the html\/txt was downloaded:","7118a337":"One can for example use the package `newspaper` to extract information from the html:"}}