{"cell_type":{"1876e9f6":"code","2d253586":"code","cb1c5108":"code","2832fc5a":"code","0be58a2c":"code","77a41a02":"code","6ab9798a":"code","0ef293d9":"code","cbfb11d2":"code","076223ec":"code","f081a776":"code","94799113":"code","ee7b948e":"code","1e4b2bd9":"code","1ef5eb50":"code","ef8162a4":"code","944647c2":"code","52f4ac3a":"code","e7c5331f":"code","d8deeb28":"code","23c7bf2a":"code","f3d1d1a9":"code","4ce1d5f7":"code","4fcbcc5a":"code","4c987111":"code","f29818a8":"code","7b951945":"code","7ce39aa5":"code","7c75f4ea":"code","e95ee25c":"code","c4f642c0":"code","90f11425":"code","e3fb86be":"code","3a4952e5":"code","9bf2cba2":"code","ee6ba276":"code","69290d80":"code","f06212d1":"code","7cfa5249":"code","e6d052ec":"code","ee98d180":"code","875e2bd0":"code","0c065137":"code","12a942e4":"code","bd8d05b2":"code","773ca8d5":"code","dfbb2dbd":"code","b38ef35c":"code","a8dfce53":"code","737c36bb":"code","b410c869":"code","4553eb27":"code","b8056ce0":"code","c92836c7":"code","69ef6f6a":"code","36617d7e":"code","16e8cb29":"code","ce6ba1b5":"code","4565fd0a":"code","73079cbd":"code","ed4e4486":"code","1b5c9af5":"code","495b4701":"markdown","bc59247e":"markdown","d96c6061":"markdown","ee597b47":"markdown","66548d6c":"markdown","c5045f08":"markdown","c73f79dc":"markdown","516c65fb":"markdown","917b545f":"markdown","9ac0fcf2":"markdown","d2f7bfff":"markdown","85356201":"markdown","9f631999":"markdown","115f3c2a":"markdown","529d47a1":"markdown","819b6789":"markdown","d4409b6f":"markdown","18bb64a3":"markdown","7b2f69e5":"markdown","da0f0e17":"markdown","0faf23e0":"markdown","d36554a3":"markdown","d92c5c2c":"markdown","1805e1ee":"markdown","b6385429":"markdown","fded9bdd":"markdown","e9492735":"markdown","bd0eaaf8":"markdown","ce49cd0d":"markdown"},"source":{"1876e9f6":"pip install uszipcode","2d253586":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom uszipcode import SearchEngine\nimport datetime\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport pylab\nfrom statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics, tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#set seaborn plot grid style\nsns.set_style(\"whitegrid\");","cb1c5108":"#import dataset\ndf = pd.read_csv('..\/input\/loan-modelling\/Loan_Modelling.csv')\ndat = df.copy()\ndat.head(100)","2832fc5a":"#Check for duplicated records - there are none, so we will continue\ndat.duplicated().sum()","0be58a2c":"#Get datatypes\ndat.info()","77a41a02":"#cast data\ndat['Education'] = dat['Education'].astype('category')\ndat['ZIPCode'] = dat['ZIPCode'].astype('str')","6ab9798a":"#Drop ID because it is useless\ndat.drop(columns = 'ID', inplace=True)","0ef293d9":"#Look at pairplots.\nsns.pairplot(dat, diag_kind='kde', corner=True)","cbfb11d2":"#convert dummy variables to counts to generate bar charts for univariate analysis\nploan = np.unique(dat['Personal_Loan'], return_counts=True)\nsec = np.unique(dat['Securities_Account'], return_counts=True)\ncd = np.unique(dat['CD_Account'], return_counts=True)\nonline = np.unique(dat['Online'], return_counts=True)\ncred = np.unique(dat['CreditCard'], return_counts=True)","076223ec":"fig = plt.figure(figsize = [15,15]);\nfig.subplots_adjust(hspace=0.6, wspace=0.2);\nplt.subplot(4,3,1);\nsns.distplot(dat['Age'], hist=False, kde_kws=dict(linewidth=3), color='limegreen');\nplt.title('Age', fontsize=14, fontweight='bold');\nplt.xlabel('(yrs)', fontsize=12, fontweight='bold');\nplt.yticks(np.arange(0,0.03,0.005),labels=['','','','','',''])\nplt.subplot(4,3,2);\nsns.distplot(dat['Experience'], hist=False, kde_kws=dict(linewidth=3), color='lightcoral');\nplt.title('Experience', fontsize=14, fontweight='bold');\nplt.xlabel('(yrs)', fontsize=12, fontweight='bold');\nplt.yticks(np.arange(0,0.03,0.005),labels=['','','','','',''])\nplt.subplot(4,3,3);\nsns.distplot(dat['Income'], hist=False, kde_kws=dict(linewidth=3), color='Cornflowerblue');\nplt.title('Income', fontsize=14, fontweight='bold');\nplt.xlabel('($1,000)', fontsize=12, fontweight='bold');\nplt.yticks(np.arange(0,0.012,0.002),labels=['','','','','',''])\nplt.subplot(4,3,4);\nsns.distplot(dat['Family'], hist=True, kde=False, color='mediumslateblue');\nplt.title('Family', fontsize=14, fontweight='bold');\nplt.xlabel('Size', fontsize=12, fontweight='bold');\nplt.subplot(4,3,5);\nsns.distplot(dat['CCAvg'], hist=False, kde_kws=dict(linewidth=3), color='orange');\nplt.title('CCAvg', fontsize=14, fontweight='bold');\nplt.xlabel('($1,000)', fontsize=12, fontweight='bold');\nplt.yticks(np.arange(0,0.4,0.1),labels=['','','',''])\nplt.subplot(4,3,6);\nsns.distplot(dat['Education'], hist=True, kde=False, color='darkkhaki');\nplt.title('Education', fontsize=14, fontweight='bold');\nplt.xlabel('')\nplt.xticks([1,2,3],['Undergrad','Grad','Prof.'], fontsize=12, fontweight='bold', rotation=35);\nplt.subplot(4,3,7);\nsns.barplot(x = ploan[0], y=ploan[1], color='slategrey');\nplt.title('Personal_Loan', fontsize=14, fontweight='bold');\nplt.xticks([0,1],['No','Yes'], fontsize=12, fontweight='bold');\nplt.subplot(4,3,8);\nsns.barplot(x = sec[0], y=sec[1], color='lightsteelblue');\nplt.title('Securities_Account', fontsize=14, fontweight='bold');\nplt.xticks([0,1],['No','Yes'], fontsize=12, fontweight='bold');\nplt.subplot(4,3,9);\nsns.barplot(x = cd[0], y=cd[1], color='firebrick');\nplt.title('CD_Account', fontsize=14, fontweight='bold');\nplt.xticks([0,1],['No','Yes'], fontsize=12, fontweight='bold');\nplt.subplot(4,3,10);\nsns.barplot(x = online[0], y=online[1], color='peru');\nplt.title('Online', fontsize=14, fontweight='bold');\nplt.xticks([0,1],['No','Yes'], fontsize=12, fontweight='bold');\nplt.subplot(4,3,11);\nsns.barplot(x = cred[0], y=cred[1], color='mediumspringgreen');\nplt.title('CreditCard', fontsize=14, fontweight='bold');\nplt.xticks([0,1],['No','Yes'], fontsize=12, fontweight='bold');","f081a776":"zips = dat['ZIPCode'].astype(str)\n\nsearch = SearchEngine(simple_zipcode=True)\ncity = []\ncounty = []\nstate = []\nmed_income = []\nmed_homev = []\nfor zc in zips:\n    blah = search.by_zipcode(zc).to_dict()\n    city.append(blah['major_city'])\n    county.append(blah['county'])\n    state.append(blah['state'])\n    med_income.append(blah['median_household_income'])\n    med_homev.append(blah['median_home_value'])\n\nlocation = pd.DataFrame()\nlocation['city'] = city\nlocation['county'] = county\nlocation['state'] = state\nlocation['med_income'] = med_income\nlocation['med_homev'] = med_homev","94799113":"location.head(20)","ee7b948e":"#All of the customers in the study are located in California. This Feature is not going to be helpful so will not be used.\nlocation['state'].value_counts()","1e4b2bd9":"# For this analysis, City, ZipCode Median Income, and ZipCode Median Homevalue will be employed. \ndat['city'] = location['city']\ndat['med_income'] = location['med_income']\ndat['med_homev'] = location['med_homev']","1ef5eb50":"#Drop zipcode as it is not useful anymore.\ndat.drop(columns='ZIPCode', inplace=True)","ef8162a4":"dat.head(20)","944647c2":"# work on copy of dataframe\ndat1=dat.copy()\n#encode cities into numerical categories\ndat1['city'] = LabelEncoder().fit_transform(dat1['city'])\n    \n#initialize imputer\nimputer = KNNImputer(n_neighbors=3)\n\n# Run imputer on dataframe\ndat1[:] = imputer.fit_transform(dat1)","52f4ac3a":"dat[['med_income','med_homev']].describe()","e7c5331f":"dat1","d8deeb28":"dat1[['med_income','med_homev']].describe()","23c7bf2a":"fields = ['Age','Experience','Income','Family','CCAvg','Education','Mortgage','Securities_Account','CD_Account','Online','CreditCard','med_income','med_homev']\nptype = ['violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot','violinplot']\nfig = plt.figure(figsize=[15,25]);\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfor i, field in enumerate(fields):\n    if i != 14:\n        exec(f\"plt.subplot(5,3,{i+1});\")\n        if ptype[i] == 'distplot':\n            exec(f\"sns.distplot(data=dat1, x='Personal_Loan', y='{field}', rug=True, hist=False);\")\n        else:\n            exec(f\"sns.{ptype[i]}(data=dat1, x='Personal_Loan', y='{field}');\")\n        exec(f\"plt.ylabel('{field}', fontsize=14, fontweight='bold');\")\n        exec(f\"plt.xlabel('Personal Loan', fontsize=14);\")\n        plt.xticks([0,1],labels=['No','Yes'],fontsize=12, fontweight='bold')","f3d1d1a9":"#create fontdict for axis labels\naxlab2 = {'family': 'serif',\n              'color': 'black',\n              'weight': 'bold',\n              'size': 16\n         }\n#create subplot layout\nfig = plt.figure(figsize=[10,10]);\ngrid = plt.GridSpec(8, 1, wspace=0.3, hspace=1.2);\nx = ['Age', 'Experience', 'Income', 'Family','CCAvg','Mortgage','med_income','med_homev'];\ncol = ['forestgreen','dodgerblue','goldenrod', 'coral', 'orange','goldenrod','red'];\n\n#loop to populate boxplots within subplots\nfor i in np.arange(0,7):\n    for j in np.arange(0,1): \n        exec(f'ax{i}{j} = plt.subplot(grid[i,j]);')\n        exec(f'sns.boxplot(x=dat1[x[{i}]], ax=ax{i}{j}, color=col[{i}]);')\n        exec(f'ax{i}{j}.set_title(x[{i}], fontdict=axlab2);')\n        exec(f'ax{i}{j}.set_xlabel(\"\", fontdict=axlab2);')\n        exec(f'a{i} = ax{i}{j}.axvline(dat1[x[{i}]].mean(),color= \"red\", linestyle=\"--\", label=\"mean\")')\n        exec(f'b{i} = ax{i}{j}.axvline(dat1[x[{i}]].mean()+ 3 * dat1[x[{i}]].std(),color= \"orange\", linestyle=\"--\", label=\"3sigma\")')\n        exec(f'ax{i}{j}.axvline(max([dat1[x[{i}]].mean()- 3 * dat1[x[{i}]].std(), 0]),color= \"orange\", linestyle=\"--\")')\n        exec(f'c{i} = ax{i}{j}.axvline(dat1[x[{i}]].mean()+ 2 * dat1[x[{i}]].std(),color= \"slategrey\", linestyle=\"--\", label=\"2sigma\")')\n        exec(f'ax{i}{j}.axvline(max([dat1[x[{i}]].mean()- 2 * dat1[x[{i}]].std(), 0]),color= \"slategrey\", linestyle=\"--\")')\n        plt.xticks(fontsize=14);\n\nplt.legend([a0, c0, b0], ['mean','2sigma','3sigma'], loc='upper center', bbox_to_anchor=(0.9, 16.7), fontsize=14)        \nfig.show();","4ce1d5f7":"dat2 = dat1.copy()\n#getting rid of outliers by reassigning outlier values to next highest value\ndat1.loc[dat1['med_income']==np.max(dat1['med_income']),'med_income'] = dat1.loc[dat1['med_income']!=np.max(dat1['med_income']),'med_income'].max()\ndat1.loc[dat1['med_income']==np.max(dat1['med_income']),'med_income'] = dat1.loc[dat1['med_income']!=np.max(dat1['med_income']),'med_income'].max()\ndat1.loc[dat1['Income']==np.max(dat1['Income']),'Income'] = dat1.loc[dat1['Income']!=np.max(dat1['Income']),'Income'].max()\ndat1.loc[dat1['Income']==np.max(dat1['Income']),'Income'] = dat1.loc[dat1['Income']!=np.max(dat1['Income']),'Income'].max()\ndat1.loc[dat1['CCAvg']==np.max(dat1['CCAvg']),'CCAvg'] = dat1.loc[dat1['CCAvg']!=np.max(dat1['CCAvg']),'CCAvg'].max()\ndat1.loc[dat1['CCAvg']==np.max(dat1['CCAvg']),'CCAvg'] = dat1.loc[dat1['CCAvg']!=np.max(dat1['CCAvg']),'CCAvg'].max()","4fcbcc5a":"#create fontdict for axis labels\naxlab2 = {'family': 'serif',\n              'color': 'black',\n              'weight': 'bold',\n              'size': 16\n         }\n#create subplot layout\nfig = plt.figure(figsize=[10,10]);\ngrid = plt.GridSpec(8, 1, wspace=0.3, hspace=1.2);\nx = ['Age', 'Experience', 'Income', 'Family','CCAvg','Mortgage','med_income','med_homev'];\ncol = ['forestgreen','dodgerblue','goldenrod', 'coral', 'orange','goldenrod','red'];\n\n#loop to populate boxplots within subplots\nfor i in np.arange(0,7):\n    for j in np.arange(0,1): \n        exec(f'ax{i}{j} = plt.subplot(grid[i,j]);')\n        exec(f'sns.boxplot(x=dat1[x[{i}]], ax=ax{i}{j}, color=col[{i}]);')\n        exec(f'ax{i}{j}.set_title(x[{i}], fontdict=axlab2);')\n        exec(f'ax{i}{j}.set_xlabel(\"\", fontdict=axlab2);')\n        exec(f'a{i} = ax{i}{j}.axvline(dat1[x[{i}]].mean(),color= \"red\", linestyle=\"--\", label=\"mean\")')\n        exec(f'b{i} = ax{i}{j}.axvline(dat1[x[{i}]].mean()+ 3 * dat1[x[{i}]].std(),color= \"orange\", linestyle=\"--\", label=\"3sigma\")')\n        exec(f'ax{i}{j}.axvline(max([dat1[x[{i}]].mean()- 3 * dat1[x[{i}]].std(), 0]),color= \"orange\", linestyle=\"--\")')\n        exec(f'c{i} = ax{i}{j}.axvline(dat1[x[{i}]].mean()+ 2 * dat1[x[{i}]].std(),color= \"slategrey\", linestyle=\"--\", label=\"2sigma\")')\n        exec(f'ax{i}{j}.axvline(max([dat1[x[{i}]].mean()- 2 * dat1[x[{i}]].std(), 0]),color= \"slategrey\", linestyle=\"--\")')\n        plt.xticks(fontsize=14);\n\nplt.legend([a0, c0, b0], ['mean','2sigma','3sigma'], loc='upper center', bbox_to_anchor=(0.9, 16.7), fontsize=14)        \nfig.show();","4c987111":"#plot correlation matrix heatmap\nfig, ax = plt.subplots(figsize=[15,10])\nsns.heatmap(dat1.corr(), ax=ax,  annot=True, linewidths=0.05, fmt= '.2f',cmap=\"RdBu\", vmin=-1, vmax=1)\nax.tick_params(axis='both', which='major', labelsize=14)\nax.set_title('Dataset Correlation Matrix', fontdict={'family': 'serif', 'color': 'black', 'size': 18, 'weight': 'bold'})\nfig.show();","f29818a8":"#Cast the data into types that will work with logistic regression.\ndat1['Family'] = dat1['Family'].astype('category')\ndat1['Education'] = dat1['Education'].astype('category')\ndat1['city'] = dat1['city'].astype('category')\ndat1['Age'] = dat1['Age'].astype(int)\ndat1['Experience'] = dat1['Experience'].astype(int)\ndat1['Personal_Loan'] = dat1['Personal_Loan'].astype(int)\ndat1['Securities_Account'] = dat1['Securities_Account'].astype(int)\ndat1['CD_Account'] = dat1['CD_Account'].astype(int)\ndat1['Online'] = dat1['Online'].astype(int)\ndat1['CreditCard'] = dat1['CreditCard'].astype(int)","7b951945":"dat1.info()","7ce39aa5":"# Define X and Y variables\nX = dat1.drop(['Personal_Loan'], axis=1)\nY = dat1[['Personal_Loan']]","7c75f4ea":"#compute VIF to deal with \nX_fs = X.copy()\nX_fs = add_constant(X_fs)\nvifser = pd.Series([variance_inflation_factor(X_fs.values,i) for i in range(X_fs.shape[1])],index=X_fs.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vifser))","e95ee25c":"#Drop multicollinear features.\nX_fs.drop(columns=['Experience'], inplace=True)\nvifser = pd.Series([variance_inflation_factor(X_fs.values,i) for i in range(X_fs.shape[1])],index=X_fs.columns)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vifser))","c4f642c0":"#split data into testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X_fs.drop(columns=['const']), Y, test_size=0.30, random_state=42)","90f11425":"#the below function displays performance metrics for each model as CVGridSearch performs cross validation\ndef evaluate_model(name, model, features, labels):\n    beg = datetime.datetime.now()\n    pred = model.predict(features)\n    end = datetime.datetime.now()\n    accuracy = round(accuracy_score(labels, pred), 3)\n    precision = round(precision_score(labels, pred), 3)\n    recall = round(recall_score(labels, pred), 3)\n    print('{} -- Accuracy: {} \/ Precision: {} \/ Recall: {} \/ Latency: {}ms'.format(name, accuracy, precision, recall, (end-beg).microseconds\/1000))\n    \n#the below function displays performance metrics of best model calculated through CVGridSearch\ndef print_results(results):\n    print('BEST PARAMETERS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+\/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))\n\n#Function to calculate recall score\ndef get_recall_score(model, train, test):\n    pred_train = model.predict(train)\n    pred_test = model.predict(test)\n    print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n    print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n    \n#the below function generated confusion matrix\ndef make_confusion_matrix(y_actual,y_predict,labels=[1, 0]):\n    cm=confusion_matrix( y_actual,y_predict, labels=[1, 0])\n    df_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in ['Purchase Loan','Not Purchase Loan']])\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    fig = plt.figure(figsize = (10,7))\n    hmp = sns.heatmap(df_cm, annot=labels,fmt='')\n    hmp.set_xticklabels(hmp.get_xmajorticklabels(), fontsize = 14, fontweight='bold')\n    hmp.set_yticklabels(hmp.get_ymajorticklabels(), fontsize = 14, fontweight='bold')\n    plt.ylabel('True label', fontsize = 14, fontweight='bold')\n    plt.xlabel('Predicted label',fontsize = 14, fontweight='bold')","e3fb86be":"#parameter grid for CVGridSearch to iterate through to tune hyperparameters\nparameters = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'penalty': ['l1', 'l2'],\n    'max_iter': list(range(100,800,100)),\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\n#initialize logistic regression solver\nlr = LogisticRegression(random_state=42)\n\n#initialize grid search by inputting model, paramter grid, and # of folds in cross validation\ncv = GridSearchCV(lr, parameters, cv=5)\n\n#Fit the model to the training data\ncv.fit(X_train, y_train)\n\nprint_results(cv)","3a4952e5":"lr_ev = evaluate_model('Logistic Regression', cv.best_estimator_, X_test, y_test)\nlr_ev","9bf2cba2":"lr_roc_auc = roc_auc_score(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\nfpr, tpr, thresholds = roc_curve(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\nfig = plt.figure(figsize=(14,8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lr_roc_auc)\nplt.plot([0, 1], [0, 1],'r--', linewidth=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 14, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize = 14, fontweight='bold')\nplt.title('Receiver Operating Characteristic', fontsize = 14, fontweight='bold')\nplt.legend(loc=\"lower right\", prop={\"size\":14})\nplt.show()","ee6ba276":"# The optimal cut off would be where tpr is high and fpr is low\nfpr, tpr, thresholds = roc_curve(y_test, cv.best_estimator_.predict_proba(X_test)[:,1])\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(optimal_threshold)","69290d80":"make_confusion_matrix(y_test, cv.best_estimator_.predict(X_test))\nplt.title('Confusion Matrix Prior to Applying Threshold', fontsize=16, fontweight='bold');","f06212d1":"y_pred_tr = (cv.best_estimator_.predict_proba(X_train)[:,1]>optimal_threshold).astype(int)\ny_pred_ts = (cv.best_estimator_.predict_proba(X_test)[:,1]>optimal_threshold).astype(int)\nmake_confusion_matrix(y_test,y_pred_ts)\nplt.title('Confusion Matrix After Applying Threshold', fontsize=16, fontweight='bold');","7cfa5249":"y_pred = cv.best_estimator_.predict(X_test)\nprint('Accuracy before applying threshold:',accuracy_score(y_test, y_pred))\nprint('Accuracy after applying threshold:',accuracy_score(y_test, y_pred_ts))","e6d052ec":"print(\"Recall before applying threshold:\",metrics.recall_score(y_test, y_pred))\nprint(\"Recall after applying threshold:\",metrics.recall_score(y_test, y_pred_ts))","ee98d180":"#extract misclassified records for analysis to gain insight into patterns\nlr_misclass_te = pd.DataFrame()\nlr_misclass_te['act'] = y_test['Personal_Loan']\nlr_misclass_te['pred'] = y_pred_ts\nlr_misclass_te_fea_fp = X_test[np.logical_and(lr_misclass_te['act'] == 0, lr_misclass_te['pred'] == 1)]\nlr_misclass_te_fea_fn = X_test[np.logical_and(lr_misclass_te['act'] == 1, lr_misclass_te['pred'] == 0)]","875e2bd0":"#plot false positive records in grid by feature\nfields = list(lr_misclass_te_fea_fp.columns)\nptype = 'distplot'\nfig = plt.figure(figsize=[15,25]);\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfig.suptitle('Logistic Regression - Test set False Positives', y = 0.9, fontsize = 16, fontweight='bold')\nfor i, field in enumerate(fields):\n    exec(f\"a{i}_ = plt.subplot(5,3,{i+1});\")\n    exec(f\"a{i} = sns.distplot(lr_misclass_te_fea_fp['{field}'], ax = a{i}_, hist=False, color='b',label='False Positive');\")\n    exec(f\"b{i} = sns.distplot(dat1.loc[dat1['Personal_Loan']==1,'{field}'], ax = a{i}_, hist=False, color='g', label='True Positive');\")\n    #exec(f\"a{i}_.get_legend().set_visible(False)\")\n    exec(f\"plt.xlabel('{field}', fontsize=14, fontweight='bold');\")\n#fig.legend(['False Positive','True Positive'], loc='upper center', bbox_to_anchor=(0.82, 0.92), fontsize=14);","0c065137":"dat1.loc[dat1['Personal_Loan']==1,:].describe().T","12a942e4":"lr_misclass_te_fea_fp.describe().T","bd8d05b2":"#plot false negative records in grid by feature\nfields = list(lr_misclass_te_fea_fp.columns)\nptype = 'distplot'\nfig = plt.figure(figsize=[15,25]);\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfig.suptitle('Logistic Regression - Test set False Negatives', y = 0.9, fontsize = 16, fontweight='bold')\nfor i, field in enumerate(fields):\n    exec(f\"a{i}_ = plt.subplot(5,3,{i+1});\")\n    exec(f\"a{i} = sns.distplot(lr_misclass_te_fea_fn['{field}'], ax = a{i}_, hist=False, color='b',label='False Negative');\")\n    exec(f\"b{i} = sns.distplot(dat1.loc[dat1['Personal_Loan']==0,'{field}'], ax = a{i}_, hist=False, color='g', label='True Negative');\")\n    #exec(f\"a{i}_.get_legend().set_visible(False)\")\n    exec(f\"plt.xlabel('{field}', fontsize=14, fontweight='bold');\")\n#fig.legend(['False Negative','True Negative'], loc='upper center', bbox_to_anchor=(0.82, 0.92), fontsize=14);","773ca8d5":"fig = plt.figure(figsize=[9,6]);\nsns.distplot(dat1.loc[dat1['Personal_Loan'] == 0,'Income'], hist=False, color='b',label='True Negative');\nsns.distplot(dat1.loc[dat1['Personal_Loan'] == 1,'Income'], hist=False, color='g',label='True Positive');\nplt.legend(fontsize=14);\nplt.yticks(np.arange(0,0.016, 0.002), labels=['','','','','','','',''])\nplt.xlabel('Income', fontsize=14, fontweight='bold')\nplt.title('Comparison of True Positive and True Negative Incomes', fontsize=16, fontweight='bold');","dfbb2dbd":"fig = plt.figure(figsize=[9,6]);\nsns.distplot(dat1.loc[dat1['Personal_Loan'] == 0,'CCAvg'], hist=False, color='b',label='True Negative');\nsns.distplot(dat1.loc[dat1['Personal_Loan'] == 1,'CCAvg'], hist=False, color='g',label='True Positive');\nplt.yticks(np.arange(0,0.5,0.1),labels=['','','','',''])\nplt.legend(fontsize=14);\nplt.xlabel('Month CC Spending ($ Thousands)', fontsize=14, fontweight='bold')\nplt.title('Comparison of True Positive and True Negative Monthly CC Spending', fontsize=16, fontweight='bold');","b38ef35c":"fig = plt.figure(figsize=[9,6]);\nsns.distplot(dat1.loc[dat1['Personal_Loan'] == 0,'Education'], hist=False, color='b',label='True Negative');\nsns.distplot(dat1.loc[dat1['Personal_Loan'] == 1,'Education'], hist=False, color='g',label='True Positive');\nplt.legend(fontsize=14);\nplt.xticks([1,2,3], labels=['Undergraduate','Graduate','Professional'], fontsize=14)\nplt.yticks(np.arange(0,1.1,0.2),labels=['','','','','',''])\nplt.xlabel('Education Level', fontsize=16, fontweight='bold')\nplt.title('Comparison of True Positive and True Negative Education Levels', fontsize=16, fontweight='bold');","a8dfce53":"#Calculate importance of features and generate barplot\nw = cv.best_estimator_.coef_[0]\nfeature_names = list(X_train.columns)\nfeature_importance = pd.DataFrame(feature_names, columns = [\"feature\"])\nfeature_importance[\"importance\"] = pow(np.e, w)\nfeature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n\nlr_imp_fig = plt.figure(figsize=[8,8]);\nlr_imp_fig.subplots_adjust(left=0.3);\nsns.barplot(x=feature_importance['importance']\/np.max(feature_importance['importance']), y=feature_importance['feature']);\nplt.xticks(fontsize=14);\nplt.yticks(fontsize=14, fontweight='bold');\nplt.ylabel('');\nplt.xlabel('Importance (Normalized)', fontsize=14, fontweight='bold');\nplt.title('Feature Importance - Logistic Regression', fontsize=14, fontweight='bold');\nplt.show();","737c36bb":"#get data that will be used for the decision tree \ndat2[['Experience','Income','Family','Education','Mortgage','Personal_Loan','Securities_Account','CD_Account','Online','CreditCard','city']] = dat2[['Experience','Income','Family','Education','Mortgage','Personal_Loan','Securities_Account','CD_Account','Online','CreditCard','city']].astype(int)\ndat2","b410c869":"# Define X and Y variables\nX = dat2.drop(['Personal_Loan'], axis=1)\nY = dat2[['Personal_Loan']]\n\n#Perform 70\/30 test train split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.30, random_state=42)","4553eb27":"#initialize DecisionTreeClassifier()\ndtree = DecisionTreeClassifier()\n\n#Defined Parameters for gridsearchcv\nparams = {\n    'criterion': ['gini','entropy'],\n    'max_depth': [3, 4, 5, 6], \n    'min_samples_leaf': [0.04, 0.06, 0.08], \n    'max_features': [0.2, 0.4, 0.6, 0.8],\n    'ccp_alpha': np.arange(0.0001,0.001,0.0001)\n}\n\n#initialize grid search\ncv = GridSearchCV(dtree, param_grid=params, cv=5)\n\n#Execute gridsearch and fit models\ncv.fit(X_train, y_train)\n\n#display results of gridsearch\nprint_results(cv)","b8056ce0":"dtree_ev = evaluate_model('Decision Tree', cv.best_estimator_, X_test, y_test)\ndtree_ev","c92836c7":"make_confusion_matrix(y_test, cv.best_estimator_.predict(X_test))","69ef6f6a":"print(\"Accuracy on training set : \",cv.best_estimator_.score(X_train, y_train))\nprint(\"Accuracy on test set : \",cv.best_estimator_.score(X_test, y_test))","36617d7e":"print(\"Recall on training set : \",metrics.recall_score(y_train,cv.best_estimator_.predict(X_train)))\nprint(\"Recall on test set : \",metrics.recall_score(y_test,cv.best_estimator_.predict(X_test)))","16e8cb29":"dt_misclass_te = pd.DataFrame()\ndt_misclass_te['act'] = y_test['Personal_Loan']\ndt_misclass_te['pred'] = cv.best_estimator_.predict(X_test)\ndt_misclass_te_fea_fp = X_test[np.logical_and(dt_misclass_te['act'] == 0, dt_misclass_te['pred'] == 1)]\ndt_misclass_te_fea_fn = X_test[np.logical_and(dt_misclass_te['act'] == 1, dt_misclass_te['pred'] == 0)]","ce6ba1b5":"fields = list(lr_misclass_te_fea_fn.columns)\nptype = 'distplot'\nfig = plt.figure(figsize=[15,25]);\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfig.suptitle('Decision Tree - Test set False Positives', y = 0.9, fontsize = 16, fontweight='bold')\nfor i, field in enumerate(fields):\n    exec(f\"a{i}_ = plt.subplot(5,3,{i+1});\")\n    exec(f\"a{i} = sns.distplot(dt_misclass_te_fea_fp['{field}'], ax = a{i}_, hist=False, color='b',label='False Positive');\")\n    exec(f\"b{i} = sns.distplot(dat2.loc[dat2['Personal_Loan']==1,'{field}'], ax = a{i}_, hist=False, color='g', label='True Positive');\")\n    #exec(f\"a{i}_.get_legend().set_visible(False)\")\n    exec(f\"plt.xlabel('{field}', fontsize=14, fontweight='bold');\")\n#fig.legend(['False Positive','True Positive'], loc='upper center', bbox_to_anchor=(0.82, 0.92), fontsize=14);","4565fd0a":"fields = list(lr_misclass_te_fea_fn.columns)\nptype = 'distplot'\nfig = plt.figure(figsize=[15,25]);\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfig.suptitle('Decision Tree - Test set False Negatives', y = 0.9, fontsize = 16, fontweight='bold')\nfor i, field in enumerate(fields):\n    exec(f\"a{i}_ = plt.subplot(5,3,{i+1});\")\n    exec(f\"a{i} = sns.distplot(dt_misclass_te_fea_fn['{field}'], ax = a{i}_, hist=False, color='b',label='False Negative');\")\n    exec(f\"b{i} = sns.distplot(dat2.loc[dat1['Personal_Loan']==0,'{field}'], ax = a{i}_, hist=False, color='g', label='True Negative');\")\n    #exec(f\"a{i}_.get_legend().set_visible(False)\")\n    exec(f\"plt.xlabel('{field}', fontsize=14, fontweight='bold');\")\n#fig.legend(['False Negative','True Negative'], loc='upper center', bbox_to_anchor=(0.82, 0.92), fontsize=14);","73079cbd":"feature_names = list(X.columns)\nfig = plt.figure(figsize=(10,20))\ntree.plot_tree(cv.best_estimator_,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nplt.show();","ed4e4486":"dt_imp = pd.DataFrame(cv.best_estimator_.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False)\nprint(dt_imp)","1b5c9af5":"dt_imp_fig = plt.figure(figsize=[8,8]);\ndt_imp_fig.subplots_adjust(left=0.3);\nsns.barplot(x=dt_imp['Imp']\/np.max(dt_imp['Imp']), y=dt_imp.index);\nplt.xticks(fontsize=14);\nplt.yticks(fontsize=14, fontweight='bold');\nplt.ylabel('');\nplt.xlabel('Importance', fontsize=14, fontweight='bold');\nplt.title('Feature Importance - Decision Tree Classifier');","495b4701":"### Observation\n\nCD Account holders contribute the most importance to the model correctly predicting whether a customer would obtain the personal loan, followed by education. Many of the features are contributing some to the model, but by far the largest contributors are those two.","bc59247e":"## Employing USZipCode to obtained detailed information from ZipCodes:","d96c6061":"### Observation\n\nAs a result of applying the threshold, accuracy decreased by ~4% but a substantial increase of ~23% was observed in recall. This ultimately means that false negatives were subtantially reduced, leading to more loan customers being predicted by the model overall.","ee597b47":"### Observation\n\nThe USZipCode API was able to extract median income and home value for most of the zipcodes but there are some null values. Below, KNNImputer() will be employed to impute the missing values. First, the city strings will be converted to labels because KNNImputer() requires numerical inputs. Then the data will be imputed with n_neighbors set to 3. Following the imputation operation, the statistical description generated above will be compared to the imputed data to ensure that the distribution wasn't drastically changed.","66548d6c":"### Observation \n\nGridSearchCV determined the best hyperparameters for the model as listed below and calculated that the Accuracy of the model with those hyperparameters was 0.951 +\/- 0.011. Among the spread of the hyperparameters input into GridDearchCV, the best C was found to be a in-the-middle value (neither complex or simple), max iteration 200 (less iterations to minimize log-loss), ridge regularization method (l2), and the newton-cq algorithm, which is one of the more simplistic algorithms for solving the loss function.\n\nBEST PARAMETERS: 0.951 (+\/-0.011) for {'C': 10, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n\nAn evaluation of the model on the test date indicates the following performance metrics:\n\nLogistic Regression -- Accuracy: 0.951 \/ Precision: 0.85 \/ Recall: 0.65 \/ Latency: 1.483ms\n\nThe means:\n\n1. Accuracy - 95.1% of the time, the model will correctly predict customers who would obtain the personal loan;\n2. Precision - 85.5% of all customers predicted to obtain the personal loan will actually obtain the personal loan;\n3. Recall - of all customers who obtain the personal loan, the model incorrectly predicted 35% of them; and \n4. The model will return a prediction for one customer in ~0.989 microseconds (calculated as 1.483ms\/len(X_test)*1000).\n","c5045f08":"# Goal for model and definition of metrics\n\nBecause the model that will be developed in this analysis will be predicting customers ripe for marketing a personal loan to, accuracy (the percent correctly predicted), recall (the percent of true actuals that were correctly predicted) and precision (the ratio of true positives to total predicted true) are important, though precision may or may not be as important, depending on form of marketing and the costs involved. Accuracy will indicate how many times the model will get things right, while recall will provide guidance into how many times the model missed a potential customer. In the case of precision, this may be important for reigning in the costs of marketing by preventing such costs being spent on customers who are unlikely to get the loan.  ","c73f79dc":"### Observation\n\nRight Shift of income. The model is clearly struggling with income level and clearly, there is some overlap of the max tail for the true negative incomes and the min tail for the true positive incomes.\n\nCCAvg is shifted to the opposite of the generalized cases and this is likely contributing to the misclassifications. \n\nEducation - The model is demonstrating that true negatives tend to be lower in education, but that is not the complete story for the contribution of education in the model and clearly, the difficulties the model is having dealing with education level is contributing to false negatives. As will be demonstrated in the below barplot, education is an important feature of the model.\n","516c65fb":"## Look at Outliers:","917b545f":"### Observation\nAge and Experience appear to be the only significantly multicollinear variables in the data. After dropping Experience, the looks to meet the assumptions for multicollinearity to proceed with logistic regression.","9ac0fcf2":"### Observation\n\nThe resulting model is very simple, with Education, Income, and CCAvg as important features of the model. ","d2f7bfff":"### Observation\nTreatment of outliers looks sufficient to address the problems in the data.","85356201":"### Observations\n\nFor True positives, the decision tree model looks to be providing excellent coverage for the features Income, Mortgage, CD_Account, and CreditCard. It's not clear where the model could improve from analysis of misclassified false positives. But for False Negative and very similar to what was observed when analyzing misclassification in the logistic regression model, Income and CCAvg appear to be left shifted. ","9f631999":"### Observation - False Positive Misclassifications\n\nLeft shift of income - True positive, mean of 144 and standard deviation of 31.5, False positive, mean of 132 and standard deviation of 44. Definitely a slight leftward shift in the min tail of the False positive income distribution but a rightward shift in the mean.\n\nSlight shift in CCAvg - Since Income and CCAvg are correlated to a certain extent, it is not surprising that there is also a left-shift in its mean, though the standard deviation does not appear to be affected.\n\nEducation - It looks like the model is not capturing the contribution of graduate degree education to preference toward the personal loan. There are definitely more undergrad representation in the false positives than in the true positive distribution.","115f3c2a":"# I'm going to include the city in this analysis. If this model was to be applied to customers in other states, city would need to be removed from the model. Also, an analysis would need to be performed to verify that median home values and incomes were consistent in other states and area with what was calculated here.","529d47a1":"### Observations:\n- Age and experience look to be highly positively correlated.\n- CCAvg and income positively correlated\n- Mortgage and income positively correlated\n- Relatively even spread across number in family from single to 4 member-household\n- Credit card average is left-skewed\n- Education - more undergrade than graduate or professional. Slightly more professional degree holders than graduate degree holders.\n- Extremely left-skewed data for mortgage.\n- Majority of customers have not obtained the personal loan (this is the dependent variable in this analysis).\n- Majority of customers do not have a securities account\n- Majority of customers do not have a CD Account\n- More online banking customers but not by much\n- Majority of customers dont have a credit card issued by Universal Bank but there is apriximately 30% who do.\n","819b6789":"## Analysis of Misclassification\n\nBelow, misclassified records are analyzed to determine whether there are patterns associated with false negative and false positive classifications. ","d4409b6f":"### Observation \n\nGridSearchCV determined the best hyperparameters for the model as listed below and calculated that the Accuracy of the model with those hyperparameters was 0.965 +\/- 0.012. Among the spread of the hyperparameters input into GridDearchCV, the best ccp_alpha was found to be a in-the-middle value (neither complex or simple), a very simple model with a max depth of 3, max features of 0.8, and minumum samples per lead of 0.06.\n\nBEST PARAMETERS: 0.965 (+\/-0.012) for {'ccp_alpha': 0.0005, 'criterion': 'gini', 'max_depth': 3, 'max_features': 0.8, 'min_samples_leaf': 0.06}\n\nAn evaluation of the model on the test date indicates the following performance metrics:\n\nDecision Tree -- Accuracy: 0.965 \/ Precision: 0.965 \/ Recall: 0.694 \/ Latency: 0.839ms\n\nThe means:\n\n1. Accuracy - 96.5% of the time, the model will correctly predict customers who would obtain the personal loan;\n2. Precision - 96.5% of all customers predicted to obtain the personal loan will actually obtain the personal loan;\n3. Recall - of all customers who obtain the personal loan, the model incorrectly predicted 31.6% of them; and \n4. The model will return a prediction for one customer in ~0.559 microseconds (calculated as 0.839ms\/len(X_test)*1000).\n\n","18bb64a3":"### Observation\n\nAs represented above, and in previous analysis, Personal_Loan appears to be significantly positively correlated with CCAvg and Income.","7b2f69e5":"##### Observation\n\nThe ROC curve demonstrates a highly dependable model with an AUC of 0.97 out of 1. The optimal threshold (false positive rate where true positive rate minus diagonal is maximized) was calculated at ~0.15. By employing this threshold in the model, it is expected that the optimal maximization of true positives and the optimal minimization of false positives will result. <b> Is this the correct threshold for the marketing campaign?<\/b> It may or may not be. If it is more appropriate to have a lower false positive rate, e.g. to control conversion costs, then a lower threshold should be selected. Conversely, if having a higher true positive rate is more important than controlling marketing costs, e.g. conversion costs are not directly proportional to the number of targets, then a higher threshold should be selected. Here, the optimal mix between true and false positives will be employed because there hasn't been any guidance otherwise on the subject.","da0f0e17":"### Observation\nThere doesn't appear to be many outliers in the data. For this analysis, Income, CCAvg, and med_income will have two outliers each addressed below.","0faf23e0":"## Perform hyperparameter tuning and 5-fold cross-validation using CVGridSearch\n\nThe following hyperparameters will be tuned to come to the best combination of parameters for logistic regression on the personal loan dataset.\n\n1. C - the penalty factor which is equal to 1\/$\\lambda$, where lambda is the regularization factor. As $\\lambda$ increases, the penalty for complexity in the model increases, therefore, because C is the inverse of $\\lambda\\$, as C decreases, the penalty for complexity increases.\n\n2. penalty - The regularization method, or method whereby cost and penalty are minimized -  either lasso regularization (L1) or ridge regularization (L2). Lasso regularization sums the absolute value of the magnitude of coefficients as penalty term to the log-loss function and ridge regression minimizes the cost and penalty by summing the squared magnitude of coefficients.\n\n3. max_iter - defines the maximum number of iterations the algorithm will undergo to minimize the log-loss function.\n\n4. Solver - the solvers are different algorithms available to minimize the log-loss function.","d36554a3":"# CART Decision Tree Classifier","d92c5c2c":"## Logistic Regression to predict loan customers","1805e1ee":"# Loan Marketing Optimization Employing Logistic Regression and CART Decision Tree Classifier.\n<br>\nAnalyst: Jordan Rich<br>\nKaggleID: JordanRich\n\n### Background and Context\n\nAllLife Bank has a growing customer base, a majority of which are liability customers (depositors) with varying balance levels. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more revenue from interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\n\nA campaign that the bank ran last year for its personal loans targeted to liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with a minimal budget.\n\n### Objective\n\n- Build a model to identify the potential customers who have a higher probability of obtaining the personal loan.\n- Identify which variables are most significant in the prediction model.\n- Identify which segments of customers should be targeted for the personal loan.\n \n\n### Data Features:\n* ID: Customer ID\n* Age: Customer\u2019s age in completed years\n* Experience: #years of professional experience\n* Income: Annual income of the customer (in thousand dollars)\n* ZIP Code: Home Address ZIP code.\n* Family: the Family size of the customer\n* CCAvg: Avg. spending on credit cards per month (in thousand dollars)\n* Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced\/Professional\n* Mortgage: Value of house mortgage if any. (in thousand dollars)\n* Personal_Loan: Did this customer accept the personal loan offered in the last campaign?\n* Securities_Account: Does the customer have securities account with the bank?\n* CD_Account: Does the customer have a certificate of deposit (CD) account with the bank?\n* Online: Do customers use internet banking facilities?\n* CreditCard: Does the customer use a credit card issued by Universal Bank?\n","b6385429":"# Conclusion and Recommendations for the Marketing Team\n\nThe foregoing analysis has demonstrated a model with high accuracy, precision, and recall for predicting customers who are highly likely to obtain Allife Bank's Personal Loan. It has been demonstrated that higher income individuals who have higher education levels and who spend more money monthly on credit cards are most likely to obtain the loan. This doesn't mean that such individuals are the best customers for the business, however. To improve upon the model, it would be good to investigate customer credit scores, debt-to-income ratio, and other features that would lead to finding customers who not only are highly likely to obtain the loan, but also highly likely to repay the loan in full.","fded9bdd":"## Comparison of Logistic Regression and Decision Tree Models\n\nLogistic Regression -- Accuracy: 0.912 \/ Precision: 0.55 \/ Recall: 0.879 \/ Latency: 1.483ms\n\nDecision Tree -- Accuracy: 0.965 \/ Precision: 0.965 \/ Recall: 0.694 \/ Latency: 0.839ms\n\n<b>Both models may be the right model for this application.<\/b> If marketing costs are not an important factor, then the logistic regression model is ideal because it has high accuracy but much better recall than the decision tree model. If minimizing marketing costs is a requirement, then the Decision Tree model is ideal because it has a very high precision score, meaning that each marketing attempt has a higher likelihood of success. For this application, latency is not going to be a big deal, and especially considering that both have very low values (for some applications, latency could be a very important deciding factor, e,g, when there is substantial real time requests). ","e9492735":"### Observation\n\nBy comparison of the before and after statistical descriptions, the KNNImputer operation appears to have been successful. There was not a significant shift in the distributions.","bd0eaaf8":"### Observation\n\nAll of the variables are in a numerical form, though Education and ZIPCode need to be converted to category and string data types, respectively, to permit further processing.","ce49cd0d":"### Observation\n\n- It appears that Customers who obtained the personal loan had higher incomes (mean appears to be ~$150k as opposed to a mean of ~60k for those that did not obtain the personal loan.\n- Customers who obtained the personal loan on average spent more than those that did not obtain the loan (~4k\/mo for loan customers and ~ 1.5k\/mo for non-loan customers). This makes sense as higher income allows for more spending.\n- Customers who obtained the personal loan appear to have more education."}}