{"cell_type":{"a83e4421":"code","560804ce":"code","83bfe3ed":"code","b41c3c38":"code","719fc50f":"code","a01fca95":"code","8b96dcf5":"code","72555708":"code","18f88f73":"code","4cea72be":"code","adf4493a":"code","4f073707":"code","23c6f518":"code","c478eeb1":"code","6e6d4b2e":"code","595ef476":"code","ab754429":"code","e05e1f7b":"code","6691138d":"code","119945b0":"code","647d2995":"code","e53babc6":"code","d0ea1f53":"code","e9dc6f92":"code","bf035071":"code","b8bb1847":"code","b1226a72":"code","e15ddc5d":"code","d9548cb7":"code","f7d3ae3a":"code","21a4a9cd":"code","e0365126":"code","bbefa061":"code","e62ebc45":"code","635c5c36":"code","67a68552":"code","cf3ceb5b":"code","0f62d0f5":"code","7dcd3019":"code","834f1643":"code","1408ee6b":"markdown","1e10273d":"markdown","eb0ccc61":"markdown","e280fed8":"markdown","650607f1":"markdown","4cf130d2":"markdown","e7dfa8bf":"markdown","5f7c6f79":"markdown","96ff5650":"markdown","6e5f27e7":"markdown","af624ef2":"markdown","6dcf43ff":"markdown","674cd394":"markdown","744bd727":"markdown","cfa9251a":"markdown","56ee8762":"markdown","2f463895":"markdown","905920a9":"markdown"},"source":{"a83e4421":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn import ensemble\n\nprint('complete')","560804ce":"df = pd.read_csv('..\/input\/carprice-assignment\/CarPrice_Assignment.csv')\ndf.head()","83bfe3ed":"# DATA HAS 26 COLUMNS AND 205 ROWS \ndf.shape","b41c3c38":"# PRINT A LIST OF THE COLUMNS IN THE DATA \nfor col in df.columns:\n    print(col)","719fc50f":"# MIXTURE OF INT, FLOAT AND OBJECT(STRING) COLUMNS \n# LOOKS LIKE SOME COLUMNS ARE OBJECT BUT ARE NUMBERS E.G. DOORNUMBER, ENGINESIZE \ndf.dtypes","a01fca95":"# NONE OF THE COLUMNS CONTAIN NULL VALUES \ndf.isna().sum()","8b96dcf5":"# ID AND SYMBOLING COLUMNS CAN BE DROPPED LATER ON. ID IS SELF EXPLANATORY BUT SYMBOLING DOES NOT HAVE A DEFINITION IN THE \n# DATA DICTIONARY IN KAGGLE \n\ndf1 = df.drop(columns=['car_ID','symboling'])\ndf1.shape","72555708":"# WHAT IS THE DISTRIBUTION OF THE CAR PRICE \n# THE MAJORITY OF THE CARS ARE BELOW 20,000\n\nsns.displot(df1['price'])","18f88f73":"# WE'VE GOT A RANGE OF DIFFERENT FEATURES TO DESCRIBE THE CAR BUT THIS MEANS THERE IS A LARGE RANGE OF VALUES E.G. THE MAX \n# VALUE FOR STROKE IS 4.17 COMPARED TO THE MAX VALUE IN PEAKRPM IS 6600. THIS WILL HAVE TO BE NORMALISED \n\n# NONE OF THE FEATURES HAVE NEGATIVE VALUES \n\ndf1.describe()","4cea72be":"# PLOT THE NUMERIC VALUES ON A BOX PLOT TO VIEW THE SPREAD OF THE DATA \n# VALUES ARE TOO WIDE TO VISUALISE EASILY \n\ndf2 = df1.drop(columns=['price'])\n\nsns.boxplot(data=df2, orient=\"h\", palette=\"Set2\")","adf4493a":"# DISPLAY ALL THE CATEGORICAL FEATURES \ncat = [col for col in df1 if df1[col].dtype == 'object']\ncat","4f073707":"# THE CAR NAME COLUMN CONTAINS 147 UNIQUE VALUES BECAUSE OF THE CAR MAKE AND MODEL \n# THIS COLUMN WILL PROBABLY BE DROPPED FROM THE MODELLING STAGE BECAUSE EACH MAKE WOULD NEED IT'S OWN COLUMN. RESULTING IN THE DATA BEING \n# VERY WIDE AND PROBABLY NOT VERY USEFUL COLUMNS\n\ndf1['CarName'].value_counts()","23c6f518":"# FUEL TYPE \n# MOST OF THE CARS IN THE DATA ARE GAS(PETROL)\n\nfig, ax = plt.subplots(figsize=(5,4))\n\nax = df1['fueltype'].value_counts().plot.bar()\n\nax = plt.title('Fuel type')","c478eeb1":"# ASPIRATION TYPE \n# MAJORITY OF CARS ARE NATURALLY ASPIRATED \n\nfig, ax = plt.subplots(figsize=(5,4))\n\nax = df1['aspiration'].value_counts().plot.pie()\n\nax = plt.title('Aspiration type')","6e6d4b2e":"# THERE ARE MORE CATEGORIES IN THE BODY TYPE COLUMN THAN THE PREVIOUS TWO\n# MOST CARS FALL INTO SEDAN OR HATCHBACK \n\nfig, ax = plt.subplots(figsize=(5,4))\n\nax = df1['carbody'].value_counts().plot.bar()\n\nax = plt.title('Car body type')","595ef476":"# HARDTOPS HAVE THE WIDEST PRICE DISTRIBUTION AND HIGHEST MEDIAN PRICE \n# SEDANS HAVE THE GREATEST NUMBER OF OUTLIERS \n\nsns.boxplot(x = 'price', y = 'carbody', data = df1, orient=\"h\")\nplt.title('Car body type and price')","ab754429":"# CYLINDERNUMBER IS CURRENTLY A STRING IT MIGHT NEED TO BE CONVERTED TO AN INT \n\nfig, ax = plt.subplots(figsize=(5,4))\n\nax = df1['cylindernumber'].value_counts().plot.bar()\n\nax = plt.title('Cylinders ')","e05e1f7b":"# LOOKS LIKE CARS WITH MORE HORSEPOWER HAVE A HIGHER PRICE \n\nsns.scatterplot(data=df1, x='horsepower', y='price')\n\nplt.title('Relationship between horsepower and price')","6691138d":"# CAR DIMENSIONS \n# CAR LENGTH AND WIDTH INCREASE WITH PRICE. HEIGHT DOES NOT HAVE A RELATIONSHIP WITH PRICE \n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(15, 5), sharey=True)\n\nsns.scatterplot(data=df1, x='carlength', y='price', ax=ax1)\nsns.scatterplot(data=df1, x='carwidth', y='price', ax=ax2)\nsns.scatterplot(data=df1, x='carheight', y='price',ax=ax3)\n\nax1.set_title('Car length')\nax2.set_title('Car width')\nax3.set_title('Car height')\n\nplt.show()","119945b0":"# CAR MPG \n# CARS WITH LOWER MPG ACROSS HIGHWAY AND CITY HAVE A HIGHER COST \n# THE LOWER MPG CARS HAVE A HIGHER AMOUNT OF CYLINDERS - ALMOST ALL THE HIGHER MPG CARS HAVE 4 CYLINDERS \n# CITY AND HIGHWAY MPG SHOW SIMILAR DATA, MIGHT BE WORTH DROPPING ONE OF THE COLUMNS \n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15, 6))\n\nsns.scatterplot(data=df1, x='citympg', y='price', hue='cylindernumber', ax=ax1)\nsns.scatterplot(data=df1, x='highwaympg', y='price', hue='cylindernumber', ax=ax2)\n\nax1.set_title('City MPG')\nax2.set_title('Highway MPG')\n\nplt.show()","647d2995":"# THE BELOW LOOP BUILDS 4 GRAPHS IN A 2 BY 2 GRID \n# THE GRAPHS ALL COVER THE FEATURES THAT RELATE TO THE ENGINE \n\n# ENGINE SIZE HAS A STRONG RELATIONSHIP WITH PRICE. BORE RATIO ALSO LOOKS TO BE CORRELATED WITH PRICE.\n# STROKE AND PEAK RPM DO NOT HAVE A CORRELATION\n\ncol = ('enginesize','boreratio','stroke', 'peakrpm')\n\nplt.figure(figsize=(15,10))\n\nfor i, c in enumerate(col):\n    plt.subplot(2,2, i+1)\n    sns.scatterplot(data=df1, x=c, y='price', color='green')\n","e53babc6":"# THERE ARE COLUMNS THAT COVER VERY SIMILAR DATA E.G. HIGHWAY AND CITY MPG\n# CAN ANY OF THESE COLUMNS BE REMOVED BECAUSE THEY ARE HIGHLY CORRELATED? \n# THE MPG COLUMNS ARE THE ONLY ONES THAT HAVE A CORRELATION >0.9 \n# OTHER COLUMNS TO DROP WILL HAVE TO BE JUDGED BY ANALYSIS \n\n# PLOT THEM ON A CORRELATION HEATMAP \nfig, ax = plt.subplots(figsize=(8,6))\nax = sns.heatmap(df2.corr().abs(), linewidths=0.5, cmap=\"YlGnBu\", annot=True, annot_kws={\"fontsize\":8})\n\n# plt.title('Correlation of the intelligence columns')\nplt.show()","d0ea1f53":"# PREPARING THE DATA FOR MODELLING \n# CREATE A COPY OF THE ORIGINAL DATAFRAME, DROP THE CAR NAME COLUMN BECAUSE IT CONTAINS TOO MANY UNIQUE VALUES \ndf3 = df1.copy()\ndf3 = df3.drop(columns=('CarName'))","e9dc6f92":"# SCALE THE NUMERIC COLUMNS BECAUSE THEY ARE ALL ON DIFFERNT SCALES E.G STROKE RANGES FROM 2.07-4.17 WHEREAS CAR WEIGHT RUNS TO OVER 4000\n\n# CREATE A LIST OF THE NUMERIC COLUMNS \nnf1 = [col for col in df3 if df3[col].dtype != 'object']\n\n# CREATE A COPY OF THE DATA CONTAINING ONLY THE NUMERIC COLUMNS \nnf2 = df3[nf1]\n\n# FIT STANDARD SCALER FROM SKLEARN TO THE NUMERIC ONLY DATAFRAME \nscaler = StandardScaler().fit(nf2.values)\n\n# TRANSFORM THE VALUES \nnf2 = scaler.transform(nf2.values)\n\n# OVER-WRITE THE PREVIOUS NUMERIC COLUMNS WITH THE NEW SCALED COLUMNS \ndf3[nf1] = nf2\n\n# PREVIEW THE RESULTS \ndf3.head()","bf035071":"# CONVERT THE CATEGORICAL COLUMNS (E.G. FUEL TYPE) FROM SINGLE COLUMNS TO MULTIPLE COLUMNS USING GET_DUMMIES. THIS CREATES NEW BINARY \n# COLUMNS FOR EACH UNIQUE VALUE IN THE COLUMN. IT ALSO REMOVES THE ORIGINAL COLUMN FROM THE DATAFRAME \n# LASTLY PRINT THE NEW COLUMNS \n\ndf3 = pd.get_dummies(df3)\n\nfor c in df3.columns:\n    print(c)\n\nprint('new dataframe shape {}'.format(df3.shape))","b8bb1847":"# SPLIT THE DATA INTO X (FEATURES) AND Y (TARGET)\n\nx = df3.drop(columns=['price'])\ny = df3['price']\n\nprint('x shape {}'.format(x.shape))\nprint('y shape {}'.format(y.shape))","b1226a72":"# SPLIT THE DATA INTO TEST AND TRAIN \n# 30% IS USED FOR TESTING AND THE REST FOR TRAINING \n# RANDOM_STATE SET TO 1 TO MAKE SURE RESULTS ARE REPEATABLE \nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state = 21)\n\n\nprint('x_train {}'.format(x_train.shape))\nprint('x_test {}'.format(x_test.shape))\nprint('y_train {}'.format(y_train.shape))\nprint('y_test {}'.format(y_test.shape))","e15ddc5d":"# INSTANTIATE THE MODEL\nregr = linear_model.LinearRegression()\n\n# FIT THE MODEL TO THE DATA \nregr.fit(x_train, y_train)\n\n# PREDICT THE Y VALUES FROM THE X TEST DATA \ny_pred = regr.predict(x_test)","d9548cb7":"# R2 \n# R2 IS AVALIABLE FROM THE LINEAR REGRESSION PACKAGE IN SKLEARN \nlr_r2 = regr.score(x_test, y_test)\nprint('linear regression r-squared {:.4f}'. format(lr_r2))\n\n# MAE \nlr_mae = mean_absolute_error(y_test, y_pred)\nprint('MAE\u00a0:\u00a0{:.4f}'.format(lr_mae))\n\n\n# RMSE\n# SQUARED = If True returns MSE value, if False returns RMSE value.\nlr_rmse = mean_squared_error(y_test, y_pred,  squared=False)\nprint('linear regression RMSE\u00a0:\u00a0{:.4f}'.format(lr_rmse))","f7d3ae3a":"# COEFF_ CONTAINS THE COEFFICIENTS FOUND FOR EACH INPUT VARIABLE. THESE COEFFICIENTS CAN PROVIDE THE BASIS FOR A CRUDE \n# FEATURE IMPORTANCE SCORE. HOWEVER, THIS ASSUMES THAT THE INPUT VARIABLES HAVE THE SAME SCALE OR HAVE BEEN SCALED PRIOR TO FITTING A MODEL.\n# WHICH WAS DONE AS PART OF THE DATA PREP STAGE \n\n# ASSIGN THE COLUMNS TO A VARIABLE \nx_cols = x.columns\n\n\n# COMPUTE AND STORE THE COEFFICIENTS \nregr_coef = regr.coef_\n\n\n# PLOT THE RESULTS ON THE GRAPH \nfig, ax = plt.subplots(figsize=(12,4))\n\nplt.plot(range(len(x_cols)), regr_coef)\nplt.xticks(range(len(x_cols)), x_cols, rotation=90)\nplt.show()","21a4a9cd":"x_cols2 = pd.DataFrame(x_cols, columns = ['feature'])\nregr_coef2 = pd.DataFrame(regr_coef, columns = ['coef'])\n\nframes = [x_cols2, regr_coef2]\n\nfeat_imp = pd.concat(frames, axis=1).sort_values(by=('coef'), ascending=False)\nfeat_imp.head(10)","e0365126":"# INSTANTIATE THE MODEL AND SELECT A SIZE OF ALPHA \nlasso = linear_model.Lasso(alpha=0.05)\n\n# FIT THE MODEL TO THE DATA \nlasso.fit(x_train, y_train)\n\n# PREDICT THE Y VALUES FROM THE X TEST DATA \ny_pred2 = lasso.predict(x_test)","bbefa061":"# R2 \nlasso_r2 = lasso.score(x_test, y_test)\nprint('lasso r-squared {:.4f}'. format(lasso_r2))\n\n# MAE\nlasso_mae = mean_absolute_error(y_test, y_pred2)\nprint('MAE\u00a0:\u00a0{:.4f}'.format(lasso_mae))\n\n# RMSE \nlasso_rmse = mean_squared_error(y_test, y_pred2,  squared=False)\nprint('lasso RMSE\u00a0:\u00a0{:.4f}'.format(lasso_rmse))","e62ebc45":"# LASSO REGRESSION HAS THE BENEFIT OF FORCING SOME COEFICIENTS TO BE EXACTLY EQUAL TO ZERO. IN EFFECT PERFORMING FEATURE SELECTION \n\n# COMPUTE AND STORE THE COEFFICIENTS \nlasso_coef = lasso.coef_\n\n\n# PLOT THE RESULTS ON THE GRAPH \nfig, ax = plt.subplots(figsize=(12,4))\n\nplt.plot(range(len(x_cols)), lasso_coef)\nplt.xticks(range(len(x_cols)), x_cols, rotation=90)\nplt.show()","635c5c36":"# BELOW IS A TABLE VERSION OF THE GRAPH ABOVE. IT ALLOWS US TO SEE THE MOST IMPORTANT FEATURES TO PREDICTING CAR PRICE ARE \n\nlasso_coef2 = pd.DataFrame(lasso_coef, columns = ['coef'])\n\nframes = [x_cols2, lasso_coef2]\n\nlasso_feat_imp = pd.concat(frames, axis=1).sort_values(by=('coef'), ascending=False)\nlasso_feat_imp.head(10)","67a68552":"# SET THE HYPERPARAMETERS FOR GRADIENT BOOSTING \nparams = {'loss' : 'ls', \n          'learning_rate' : 0.1, \n          'n_estimators' : 100, \n          'max_depth' : 8\n         }","cf3ceb5b":"gb_reg = ensemble.GradientBoostingRegressor(**params)\ngb_reg.fit(x_train, y_train)\n\ny_pred3 = gb_reg.predict(x_test)","0f62d0f5":"# R2 \ngb_r2 = gb_reg.score(x_test, y_test)\nprint('GB r-squared {:.4f}'. format(gb_r2))\n\n# MAE\ngb_mae = mean_absolute_error(y_test, y_pred3)\nprint('GB MAE\u00a0:\u00a0{:.4f}'.format(gb_mae))\n\n# RMSE \ngb_rmse = mean_squared_error(y_test, y_pred3,  squared=False)\nprint('GB RMSE\u00a0:\u00a0{:.4f}'.format(gb_rmse))","7dcd3019":"## BELOW IS TAKEN FROM THIS ARTICLE ON SKLEARN \n## https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html \n\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\nfor i, y_pred3 in enumerate(gb_reg.staged_predict(x_test)):\n    test_score[i] = gb_reg.loss_(y_test, y_pred3)\n\nfig = plt.figure(figsize=(6, 6))\nplt.subplot(1, 1, 1)\n\nplt.plot(np.arange(params['n_estimators']) + 1, gb_reg.train_score_, 'b-', label='Training Set Loss')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Loss')\n\n\nplt.title('Loss (least squares regression)')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Loss (least squares regression)')\n\nplt.legend(loc='upper right')\nplt.show()","834f1643":"print('linear regression r-squared {:.4f}'. format(lr_r2))\nprint('linear regression RMSE\u00a0:\u00a0{:.4f}'.format(lr_rmse))\nprint('linear regression MAE\u00a0:\u00a0{:.4f}'.format(lr_mae))\nprint('')\n\nprint('lasso r-squared {:.4f}'. format(lasso_r2))\nprint('lasso RMSE\u00a0:\u00a0{:.4f}'.format(lasso_rmse))\nprint('lasso MAE\u00a0:\u00a0{:.4f}'.format(lasso_mae))\nprint('')\n\nprint('GB r-squared {:.4f}'. format(gb_r2))\nprint('GB RMSE\u00a0:\u00a0{:.4f}'.format(gb_rmse))\nprint('GB MAE\u00a0:\u00a0{:.4f}'.format(gb_mae))","1408ee6b":"## Gradient Boosting\n\nGradient boosting (GB) is a machine learning technique for regression and classification.  It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. It produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees","1e10273d":"## Import packages ","eb0ccc61":"## Lasso Regression \n\nRidge regression, also known as L2 Regularization, is a regression technique that introduces a small amount of bias to reduce overfitting. It does this by minimizing the sum of squared residuals plus a penalty, where the penalty is equal to lambda times the slope squared. Lambda refers to the severity of the penalty and is selected by the user. \n\nLasso regression is similar but the penalty is calculated with the absolute value of the slope instead. This has the effect of forcing some of the coefficient estimates to be exactly equal to zero. \n\nAs with the selection of K in KNN (K-Nearest Neighbours) the selection of lambda(also referred to as Alpha) is importannt for lasso regression.  ","e280fed8":"### Linear Regression \n\nThe idea of linear regression is simply finding a line that best fits the data. The model will start by fitting a line to the data and measuring how well this line fits THE data by seeing how close it is to the data points. How well the line fits is measured by the sum of squared residuals. The residuals are the differences between the real data and the line. This process is repeated by the model to find the line that minimises the sum of squared residuals. ","650607f1":"SKLearn article on gradient boosting <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict","4cf130d2":"## Import the data from CSV ","e7dfa8bf":"## Model evaluation ","5f7c6f79":"## Modelling","96ff5650":"## Comparing model performance\n\nLasso regression performs the worst of the three models. Gradient boost out performs lasso and linear regression on MAE and RMSE","6e5f27e7":"### Note\n\nI could have used cross-validation to find an optium level for alpha. However, on this occasion I have selected 0.05","af624ef2":"The purpose of this project is to predict the price of a car based on the features in the data. Data taken from:<br>\nhttps:\/\/www.kaggle.com\/hellbuoy\/car-price-prediction","6dcf43ff":"## Model evaluation","674cd394":"## Model evaluation \n\nR2 <br>\nR2 represents the proportion of the variance in your data which is explained by your model; the closer to one, the better the fit.\n\nRMSE<br>\nThe Mean Squared Error (MSE) is a measure of how close a fitted line is to data points. The difference between MSE and Root Mean Squared Error (RMSE) is just the square root of the mean square error. That is probably the most easily interpreted statistic, since it has the same units as the quantity. The smaller the means squared error, the closer you are to finding the line of best fit.\n\nMAE <br>\nMean Absolute Error(MAE) calculates the absolute difference between actual and predicted values.","744bd727":"## Inspect the data ","cfa9251a":"## Data prepartion ","56ee8762":"## Data exploration","2f463895":"# Car Price Prediction","905920a9":"## Splitting the training data into test and train data sets "}}