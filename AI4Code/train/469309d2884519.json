{"cell_type":{"46dab29f":"code","de8713e0":"code","e7ef0423":"code","f0383ab8":"code","9d39883e":"code","8c8e43f6":"code","8a60a3d0":"code","4055174f":"code","1c7ac30e":"code","14864636":"code","00bea3a1":"code","ef5dda72":"code","2e9d217a":"code","f1dc65f3":"code","22aa0d75":"code","c10bfa52":"code","a4be587e":"code","0bcd2cad":"code","7bec7643":"code","326f8863":"code","b4dcdb0c":"code","c2f105e5":"code","c6a658bc":"code","7ccb194c":"code","7b659dd1":"code","3e69581a":"code","92db03c8":"code","2decffbf":"code","cd517c03":"code","3d55b62c":"code","0f609d08":"code","d8668faa":"code","979f1708":"code","b4ffa6e8":"code","971b1a7d":"code","483238b7":"code","5cd53376":"code","e9fcfeec":"code","eb8c2cf2":"code","2b289353":"code","86b7df8f":"code","5a52eb0f":"code","de3454b3":"code","286f530c":"code","28be9c9b":"code","8c3ec019":"code","ad8d9d7b":"code","2e18a49f":"markdown","6bba7d67":"markdown","2836e66c":"markdown","fb150ce6":"markdown","f6120fb4":"markdown","ebb81996":"markdown","554b2d56":"markdown","e7e76e38":"markdown","ed718f1c":"markdown","f4fe05dd":"markdown","aa4a0fac":"markdown","b8563c61":"markdown","38266aec":"markdown","b5705ad9":"markdown","11d9ee5a":"markdown","dce1a637":"markdown","d287dc4a":"markdown","fe9fe90b":"markdown","e9e6849b":"markdown","1c6a7953":"markdown","f1872d69":"markdown","a51202e7":"markdown","a3f1fb63":"markdown","d8561791":"markdown","93bb2c9f":"markdown","54353e85":"markdown","aa416c0f":"markdown","a3fea9da":"markdown","0bdf8aa2":"markdown","916f90e8":"markdown","489c22ad":"markdown","171dfd7d":"markdown","2c96c6c3":"markdown","12326001":"markdown","f51c94e9":"markdown","a7e7eef3":"markdown"},"source":{"46dab29f":"import numpy as np\n# Creating an Array\nA = np.array([\n        [ 3,  7],\n        [-4, -6],\n        [ 7,  8],\n        [ 1, -1],\n        [-4, -1],\n        [-3, -7]\n    ])\n\nm,n = A.shape # m-observations, n-features\n\nprint(\"Array:\")\nprint(A) # our array\n\nprint(\"---\")\nprint(\"Dimensions:\")\nprint(A.shape) # shape\n\nprint(\"---\")\nprint(\"Mean across Rows:\")\nprint(np.mean(A,axis=0))","de8713e0":"# Converting the array into a DataFrame ...\nimport pandas as pd\ndf = pd.DataFrame(A, columns = ['a0', 'a1'])\nprint(df)","e7ef0423":"# ... and a dataframe can as easily be converted to an array\ndf.values","f0383ab8":"import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# makes charts pretty\nimport seaborn as sns\nsns.set(color_codes=True)","9d39883e":"# plots\nplt.scatter(A[:,0],A[:,1])   # create a scatter plot \n\n# annotations\nfor i in range(m):\n    plt.annotate('('+str(A[i,0])+','+str(A[i,1])+')',(A[i,0]+0.2,A[i,1]+0.2))\n\n# axes\nplt.plot([-6,8],[0,0],'grey') # x-axis\nplt.plot([0,0],[-8,10],'grey') # y-axis\nplt.axis([-6, 8, -8, 10])\nplt.axes().set_aspect('equal')\n\n# labels\nplt.xlabel(\"$a_0$\")\nplt.ylabel(\"$a_1$\")\nplt.title(\"Dataset $A$\")","8c8e43f6":"# Calculate covariance of a0 and a1\na0 = A[:,0]\na1 = A[:,1]\nproduct = a0*a1 # element-wise product\nprint(\"Length of prod equals \" + str(len(product)))\nprint(\"---\")\nprint(\"Covariance:\")\nprint(np.sum(product)\/(m-1))","8a60a3d0":"# Get more stuff using NumPy's covariance method\nnp.cov(a0,a1)","4055174f":"# What is A.T?\nA.T # This is the transpose of matrix A","1c7ac30e":"# Matrix Multiplication, @ operator is used for calculating the dot product of two matrices\nA.T @ A # or np.dot(A.T,A)","14864636":"# As stated in the formula now we need to divide the  product by (m-1) to yield true Sample Covariance Matrix\n# Let's call it Sigma\nSigma = (A.T @ A)\/(m-1) # or np.cov(A.T)\nSigma","00bea3a1":"# obtaining the eigenvalues and eigen vectors for the matrix Sigma\nl, X = np.linalg.eig(Sigma)\nprint(\"Eigenvalues:\")\nprint(l)\nprint(\"---\")\nprint(\"Eigenvectors:\")\nprint(X)","ef5dda72":"# let's check the first Eigenvalue, Eigenvector combination\nprint(\"Sigma times eigenvector:\")\nprint(Sigma @ X[:,0]) # 2x2 times 2x1\nprint(\"Eigenvalue times eigenvector:\")\nprint(l[0] * X[:,0]) # scalar times 2x1","2e9d217a":"# ... and the product with the second eigenvalue\nprint(\"Sigma times eigenvector:\")\nprint(Sigma @ X[:,1]) # 2x2 times 2x1\nprint(\"Eigenvalue times eigenvector:\")\nprint(l[1] * X[:,1]) # scalar times 2x1, ANNOYING - MUST USE * vs. @","f1dc65f3":"print(\"The first principal component is eigenvector with largest evalue:\")\nprint(X[:,1])\nprint(\"---\")\nprint(\"Second principal component:\")\nprint(X[:,0])","22aa0d75":"# Are the two Principal components Orthogonal? If the dot product of two matrices is zero, then they are considered to be orthogonal\nX[:,1].T @ X[:,0]","c10bfa52":"# plotting the Eigen Vectors\nplt.scatter(A[:,0],A[:,1])\nscale = 3 # increase this scaling factor to highlight these vectors\nplt.plot([0,X[0,1]*scale],[0,X[1,1]*scale],'r') # First principal component\nplt.plot([0,X[0,0]*scale],[0,X[1,0]*scale],'g') # Second principal component\n\n# annotations\nfor i in range(m):\n    plt.annotate('('+str(A[i,0])+','+str(A[i,1])+')',(A[i,0]+0.2,A[i,1]+0.2))\n\n# axes\nplt.plot([-6,8],[0,0],'grey') # x-axis\nplt.plot([0,0],[-8,10],'grey') # y-axis\nplt.axis([-6, 8, -8, 10])\nplt.axes().set_aspect('equal')\n\n# labels\nplt.xlabel(\"$a_0$\")\nplt.ylabel(\"$a_1$\")\nplt.title(\"Eigenvectors of $\\Sigma$\")","a4be587e":"# change to matrix\nAmat = np.asmatrix(A)\nXmat = np.asmatrix(X)\nAmat","0bcd2cad":"# Choose eigenvector with highest eigenvalue as first principal component\npc1 = Xmat[:,1]\npc1","7bec7643":"Acomp = Amat @ pc1 # the dot product of a 6x2 and 2x1 matrix yields a 6x1 matrix\nprint(\"Compressed version of A:\")\nprint(Acomp)","326f8863":"Arec = Acomp @ pc1.T # the dot product of a  6x1 matrix and 1x2 matrix results into a 6x2 matrix\nprint(\"Reconstruction from 1D compression of A:\")\nprint(Arec)","b4dcdb0c":"plt.plot(Arec[:,0],Arec[:,1],'r', marker='o') # Arec in RED\n\n# axes\nplt.plot([-6,8],[0,0],'grey') # x-axis\nplt.plot([0,0],[-8,10],'grey') # y-axis\nplt.axis([-6, 8, -8, 10])\nplt.axes().set_aspect('equal')\n\n# labels\nplt.xlabel(\"$a_0$\")\nplt.ylabel(\"$a_1$\")\nplt.title(\"Reconstructing the 1D compression of $A$\")","c2f105e5":"print(np.linalg.matrix_rank(Amat)) # originally a Rank 2 matrix\nprint(np.linalg.matrix_rank(Arec)) # reconstructed matrix is Rank 1","c6a658bc":"# Add the Rank 1 matrix for the other vector to recover A completely\n# Here we are taking the dot product of matrix A with the principal components and the transpose of the principal components\nAmat @ Xmat[:,1] @ Xmat[:,1].T + Amat @ Xmat[:,0] @ Xmat[:,0].T","7ccb194c":"# Why does this work? Well, recall that the dot product of a matrix and its transpose (X @ X.T) is an identity matrix as X is orthonormal\n# Hence the entire expression becomes equivalent to multiplying a matrix with a unit matrix which returns the matrix itself.\nA @ Xmat @ Xmat.T","7b659dd1":"# plots\nplt.scatter(A[:,0], A[:,1]) # A in blue\nplt.plot(Arec[:,0],Arec[:,1],'r', marker='o') # Arec in RED\n\n# across observations\nfor i in range(m):\n    e = np.vstack((A[i],Arec[i]))\n    plt.plot(e[:,0],e[:,1],'b') # BLUE\n\n# axes\nplt.plot([-6,8],[0,0],'grey') # x-axis\nplt.plot([0,0],[-8,10],'grey') # y-axis\nplt.axis([-6, 8, -8, 10])\nplt.axes().set_aspect('equal')\n\n# labels\nplt.xlabel(\"$a_0$\")\nplt.ylabel(\"$a_1$\")\nplt.title(\"Back to $A$\")","3e69581a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","92db03c8":"# we are using the free glass datset.\n# The objective is to tell the type of glass based on amount of other elements present.\ndata = pd.read_csv('..\/input\/glass-data-for-pca\/glass.csv')","2decffbf":"data.head()","cd517c03":"data.isna().sum()","3d55b62c":"data=data.drop(labels=['index','Class'], axis=1)","0f609d08":"data.describe()","d8668faa":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaled_data=scaler.fit_transform(data)\n","979f1708":"df=pd.DataFrame(data=scaled_data, columns= data.columns)","b4ffa6e8":"df.describe()","971b1a7d":"from sklearn.decomposition import PCA\npca = PCA()\nprincipalComponents = pca.fit_transform(df)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Explained Variance')\nplt.show()","483238b7":"pca = PCA(n_components=4)\nnew_data = pca.fit_transform(df)\n# This will be the new data fed to the algorithm.\nprincipal_Df = pd.DataFrame(data = new_data\n             , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4'])","5cd53376":"principal_Df.head()","e9fcfeec":"np.random.seed(1)\nX = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\nplt.plot(X[:, 0], X[:, 1], 'o')\nplt.axis('equal');","eb8c2cf2":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X)\nprint(pca.explained_variance_)\nprint(pca.components_)","2b289353":"#To see what these numbers mean, let's view them as vectors plotted on top of the data:\n\nplt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 3 * np.sqrt(length)\n    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\nplt.axis('equal');","86b7df8f":"clf = PCA(0.95) # keep 95% of variance\nX_trans = clf.fit_transform(X)\nprint(X.shape)\nprint(X_trans.shape)","5a52eb0f":"X_new = clf.inverse_transform(X_trans)\nplt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\nplt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\nplt.axis('equal');","de3454b3":"from sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\npca = PCA(2)  # project from 64 to 2 dimensions\nXproj = pca.fit_transform(X)\nprint(X.shape)\nprint(Xproj.shape)","286f530c":"# Creating a scatter plot of the datapoints\nplt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar();","28be9c9b":"sns.set()\npca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","8c3ec019":"fig, axes = plt.subplots(8, 8, figsize=(8, 8))\nfig.subplots_adjust(hspace=0.1, wspace=0.1)\n\nfor i, ax in enumerate(axes.flat):\n    pca = PCA(i + 1).fit(X)\n    im = pca.inverse_transform(pca.transform(X[20:21]))\n\n    ax.imshow(im.reshape((8, 8)), cmap='binary')\n    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n            transform=ax.transAxes, color='green')\n    ax.set_xticks([])\n    ax.set_yticks([])","ad8d9d7b":"from IPython.html.widgets import interact\n\ndef plot_digits(n_components):\n    fig = plt.figure(figsize=(8, 8))\n    plt.subplot(1, 1, 1, frameon=False, xticks=[], yticks=[])\n    nside = 10\n    \n    pca = PCA(n_components).fit(X)\n    Xproj = pca.inverse_transform(pca.transform(X[:nside ** 2]))\n    Xproj = np.reshape(Xproj, (nside, nside, 8, 8))\n    total_var = pca.explained_variance_ratio_.sum()\n    \n    im = np.vstack([np.hstack([Xproj[i, j] for j in range(nside)])\n                    for i in range(nside)])\n    plt.imshow(im)\n    plt.grid(False)\n    plt.title(\"n = {0}, variance = {1:.2f}\".format(n_components, total_var),\n                 size=18)\n    plt.clim(0, 16)\n    \ninteract(plot_digits, n_components=range(1, 64), nside=range(1, 8)) # A in blue\n","2e18a49f":"**Pros of PCA:**\n\n- Correlated features are removed.\n- Model training time is reduced.\n- Overfitting is reduced.\n- Helps in better visualizations\n- Ability to handle noise\n\n**Cons of PCA**\n- The resultant principal components are less interpretable than the original data\n- Can lead to information loss if the explained variance threshold is not considered appropriately.\n","6bba7d67":"The lighter points are the original data, while the dark points are the projected version on the principal component axis.  We see that after truncating 5% of the variance of this dataset and then reprojecting it, the \"most important\" features of the data are maintained, and we've compressed the data by 50%!\n\nThis is the sense in which \"dimensionality reduction\" works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data.\n","2836e66c":"### Summary of Eigen-decomposition Approach\n1. Normalize columns of $A$ so that each feature has a mean of zero\n1. Compute sample covariance matrix $\\Sigma = {A^TA}\/{(m-1)}$\n1. Perform eigen-decomposition of $\\Sigma$ using `np.linalg.eig(Sigma)`\n1. Compress by ordering $k$ evectors according to largest e-values and compute $AX_k$\n1. Reconstruct from the compressed version by computing $A X_k X_k^T$\n","fb150ce6":"### Covariance\n\n_Variance_ is the measure of how a variable changes or varies and _co_ means together. Hence, _covariance_ is the measure of how two variables change together.\n<img src=\"https:\/\/th.bing.com\/th\/id\/R1e9b1efd31eca555a8b2075d4300d00d?rik=CFWK6g0C0Tgbgg&pid=ImgRaw\" width=\"500\">\n\nIf the covariance is high, it means that the variables are highly correlated and change in one results in a change in the other one too.\nGenerally, we avoid using highly correlated variables in building a machine learning model.","f6120fb4":"But the question is: if we talk about n dimensions, there are n-1 perpendicular lines possible on PC1. **How to select a line as PC2?**\n\nAnd the next question is: **what is the optimum number of Principal components needed?**\n","ebb81996":"## Mathematics Behind PCA\nWe are going to discuss PCA using a method called Singular Value Decomposition (SVD) which factorises the dataset matrix in such a way that it becomes a  product of the multiplication of three individual matrices:\n\nX(original Data)= $ U* \\Sigma* V^T$\n\nWhere V is the matrix that contains the principal components.\n","554b2d56":"All the above steps can be summarized with the following gif.\n[Wicked animated GIF which illustrates PCA](http:\/\/stats.stackexchange.com\/questions\/2691\/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n\nMagically, eigen-decomposition (or PCA) finds the line where\n1. the spread of values along the black line is **maximal**\n2. the projection error (sum of red lines) is **minimal**\n\n<img src=\"https:\/\/i.stack.imgur.com\/Q7HIP.gif\">","e7e76e38":"From the Scree plot, it can be seen that 20 components are required to explain 90% of the variance which is still better than computing using all the 64 features. The explained variance threshold can be choosen based on the doamin and business requirements.","ed718f1c":"### Application of PCA to the Digits Data\n\nThe dimensionality reduction might seem a bit abstract in two dimensions, but the projection and dimensionality reduction can be extremely useful when visualizing high-dimensional data.  Let's implement PCA to the digits data. This data consists of a collection of different points in the plane to represent a digit\n","f4fe05dd":"Let's take another look at this by using IPython's ``interact`` functionality to view the reconstruction of several images at once:","aa4a0fac":"The Linear Algebra way:\n$$\n\\Sigma = \\frac{A^TA}{(m-1)}\n$$","b8563c61":"In the diagram above, we are considering 3 orthogonal(_C3  is in the third dimension_) axes to show the distribution of data. If you notice the diagram on the right, the first two axes **C1** and **C2** successfully explain the maximum variation in the data whereas the axes **C3** only consists of a fewer number of points. Hence, while considering the principal components C1 and C2 will be our choices.","38266aec":"**Pre-requisite:** PCA assumes that the mean of all the individual columns is zero and the standard deviation is 1. So, before applying PCA, the data should be pre-processed appropriately.","b5705ad9":"### Conclusion\nFrom all the explanations above, we can conclude that PCA is a very powerful technique for reducing the dimensions of the data, projecting the data from a higher dimension to a lower dimension, helps in data visualization, helps in data compression and most of all increases the model training speed drastically by decreasing the number of variables involved in computation.\n","11d9ee5a":"## Scree Plots:\nScree plots are the graphs that convey how much variance is explained by corresponding Principal components. \n<img src=\"https:\/\/www.researchgate.net\/publication\/338762066\/figure\/fig2\/AS:850322390016002@1579744064672\/Scree-test-on-the-left-and-cumulative-variance-on-the-right-of-principal-component.jpg\" width=\"500\">\n\nAs shown in the given diagram, around 75 principal components explain approximately 90 % of the variance. Hence, 75 can be a good choice based on the scenario\n","dce1a637":"PCA seeks to find the **Principal Axes** in the data, and explain how vital those axes are in describing the data distribution","d287dc4a":"Recall from your Linear Algebra class that the following should hold:\n\n\\begin{eqnarray}\n\\Sigma x_0 &=& \\lambda_0 x_0 \\nonumber \\\\\n\\Sigma x_1 &=& \\lambda_1 x_1 \\nonumber \\\\\n\\end{eqnarray}","fe9fe90b":"#### Explained Variance Ratio\n\nAll of the above questions are answered using the *explained variance ratio*. It represents the amount of variance each principal component is able to explain.\n\nFor example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it\u2019s 5.\n\nEVR of PC1=$\\frac{Distance of PC1 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{50}{55}=0.91 $\n\nEVR of PC2=$\\frac{Distance of PC2 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{5}{55}=0.09 $\n\n\nThus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n\nIn a real-life scenario, this problem is solved using the **Scree Plots**\n","e9e6849b":"### Eigen-decomposition of $\\Sigma$\n\nAccording to [Wikipedia article on PCA](https:\/\/en.m.wikipedia.org\/wiki\/Principal_component_analysis), *\"PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix.\"* The second approach has already been discussed above. Let's discuss the first approach now.\n\n$\\Sigma$ is a real, symmetric matrix; thus, it has \n\n1) real eigenvalues, and\n\n2) orthogonal eigenvectors.\n\nDefinition:\n\nAn **eigenvector v** of a linear transformation **T** is a nonzero vector that, when **T** is applied to it, does not change direction. Applying __T__ to the eigenvector only scales the eigenvector by the scalar value \u03bb, called an **eigenvalue**. This condition can be written as the equation\n\n$$\n{\\displaystyle T(\\mathbf {v} )=\\lambda \\mathbf {v} ,} \n$$","1c6a7953":"## Principal Component Analysis: \nThe principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace This results into an entirely new coordinate system of the points where the first axis corresponds to the first principal component that explains the most variance in the data.\n\n**What are the principal components?**\nPrincipal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features.\n\nLet's take the following example where the data is distributed like the diagram on the left:\n<img src=\"https:\/\/cloudxlab.com\/blog\/wp-content\/uploads\/2018\/08\/PCA_hyperplane.png\" width=\"500\">\n\n\n","f1872d69":"By taking on the Rank-1 matrix related to the 2nd eigenvector you get back to the original data","a51202e7":"#### Choosing the Number of Components\n\nBut how much information have we thrown away?  We can figure this out by looking at the **explained variance** as a function of the components:\n","a3f1fb63":"#### Dimensionality Reduction: 2D to 1D","d8561791":"### Visualizing the Principal components\n\nAs humans can only perceive 3dimensions, we\u2019ll take a dataset with less than 4 dimensions. \n","93bb2c9f":"### Explaining the Maths involved through code","54353e85":"From the diagram above, it can be seen that 4 principal components explain almost 90% of the variance in data and 5 principal components explain around 95% of the variance in data.\n\nSo, instead of giving all the columns as input, we\u2019d only feed these 4 principal components of the data to the machine learning algorithm and we\u2019d obtain a similar result.\n","aa416c0f":"Sample covariance between $a_0$ and $a_1$:\n\n$$\ncov_{a_0,a_1} =\\frac{\\sum_{k=0}^{m-1}(a_0^k - \\bar{a_0})(a_1^k - \\bar{a_1})}{m-1}\n$$\n\nwhere $\\bar{a_0}$ is the mean of column $a_0$ and $\\bar{a_1}$ is the mean of column $a_1$","a3fea9da":"#### PCA for data compression\n\nAs mentioned, PCA can be used for a sort of data compression as well. Using a smaller value of ``n_components`` allows you to represent a higher dimensional point as a sum of just a few principal component vectors.\n\nHere's what a single digit looks like when you change the number of components:\n","0bdf8aa2":"Here, we see that earlier we had 9 columns in the data earlier. Now with the help of Scree plot and PCA, we have reduced the number of features to be used for model building to 4. This is the advantage of PCA. _It drastically reduces the number of features, thereby considerably reducing the training time for the model._","916f90e8":"By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let's see what the data look like after this compression:","489c22ad":"This gives us an idea of the relationship between the datapoints. Essentially, we have found the optimal stretch and rotation in 64-dimensional space and tried to fit it to a 2-Dimensional space that allows us to see the layout of the digits, **without reference** to the labels.\n","171dfd7d":"We\u2019ll go ahead and standardise this data as all the data is on a different scale.\n","2c96c6c3":"## Python Implementation","12326001":"Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more \"important\" than the other direction.\nThe explained variance quantifies this measure of \"importance\" in a direction.\n\nAnother way to think of it is that the second principal component could be **completely ignored** without much loss of information! Let's see what our data look like if we only keep 95% of the variance\n","f51c94e9":"#### Steps to Calculate PCA\n\n\n* Move the points so that the average point is on the origin. This is called a parallel translation. Although the coordinates of the points have changed, the corresponding distances among them remain the same.\n\n\n\n* Create the best fit line for the new data points. We first start with a random line(blue one), and then try to find the best fit line(the green one) so that the distance from individual data points is minimum and consequently the distance from origin is maximum. This best fit line is called Principal component1 or PC1.\n\n\n\n* PC2 is a line perpendicular to the PC1.\n* Then the axes PC1 and PC2 are rotated in a way that PC1 becomes the horizontal axis.\n<img src=\"https:\/\/miro.medium.com\/max\/550\/1*YyY7Lq1LcfHl_FePOlwcZQ.png\" width=\"300\">\n* Then based on the sample points the new points are projected using PC1 and PC2. Thus we get the derived features. \n<img src=\"https:\/\/www.andreaperlato.com\/img\/pc1pc2.png\" width=\"300\">","a7e7eef3":"In the diagram above, we can dynamically select the number of principal components and get to know the explained percentage of variance."}}