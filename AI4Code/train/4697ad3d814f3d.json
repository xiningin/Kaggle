{"cell_type":{"73b4b018":"code","bccb2645":"code","42d35c03":"code","663f2618":"code","8fc85186":"code","4fdfdcce":"code","a5b37ba2":"code","6b054dfc":"code","02a16cca":"code","ba7ad892":"code","91850170":"code","caa9fe4c":"code","8e827cae":"code","c38368b6":"code","3661f7b8":"code","91558794":"code","643b0f5e":"code","0ae6f725":"code","e936be55":"code","5ca179b5":"markdown","a9e65405":"markdown","e9c7b23e":"markdown","752ea0bf":"markdown","dd700029":"markdown","00e524b5":"markdown","4ebecf23":"markdown","02487c0f":"markdown","a68ba132":"markdown","0fa30cca":"markdown","c56f4ced":"markdown","f92512ad":"markdown","73eada1b":"markdown","b819f432":"markdown","489fa73b":"markdown","fad6e187":"markdown","287265a5":"markdown","3f8b84cd":"markdown"},"source":{"73b4b018":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.stem.snowball import SnowballStemmer \nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nimport gensim\nfrom gensim.models.word2vec import Word2Vec\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.plotting import plot_decision_regions\nimport gensim.downloader as api\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn.linear_model import LogisticRegression\nfrom statistics import mode\nfrom tqdm import tqdm\nfrom sklearn import metrics","bccb2645":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nX = train['text']\ny = train['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\nX_train, X_test, y_train, y_test = list(X_train), list(X_test), list(y_train), list(y_test)","42d35c03":"stop = stopwords.words('english')\ndef tokenizer(text):\n    tokenized = []\n    for string in text:\n        string = re.sub('[^a-z\\sA-Z]', '', string)\n        string = re.sub('http\\S+', '', string)\n        tokenized.append([w for w in string.split() if w not in stop])\n    return tokenized\n\nsnow_stemmer = SnowballStemmer(language='english') \ndef stemmer(text):\n    stem_string = []\n    for string in text: \n        stem_string.append([snow_stemmer.stem(word) for word in string])\n    return stem_string \n\nX_train = tokenizer(X_train)\nX_train = stemmer(X_train)\nX_test = tokenizer(X_test)\nX_test = stemmer(X_test)","663f2618":"corpus = X_train\nnlp = gensim.models.word2vec.Word2Vec(corpus, size=200,   \n            window=6, min_count=1, sg=1, iter=40)\nlen(nlp.wv.vocab) # number of words in a dictionary","8fc85186":"nlp.most_similar('fire', topn = 3)","4fdfdcce":"def get_features(model, text):\n    data = pd.DataFrame({'mean' : [], 'min' : [],'max' : []})\n    model_words = set(model.wv.vocab.keys()) # words known to model\n    for t, i in zip(text, range(len(text))):  \n        vec = np.zeros(model.vector_size, dtype=\"float32\")\n    \n        # Initialize a counter for number of words in a tweet\n        nwords = 0\n        # Loop over each word in the tweet and, if it is in the model's \n        #vocabulary, add its feature vector to the total\n        for word in t:\n            if word in  model_words: \n                vec = np.add(vec, model[word])\n                nwords += 1.\n\n        # get the average, min and max\n        if nwords > 0:\n            vec_mean = np.divide(np.sum(vec), nwords)\n            vec_min = np.min(vec)\n            vec_max = np.max(vec)\n            \n        data.loc[i] = list((vec_mean, vec_min, vec_max))\n    return data","a5b37ba2":"train_data = get_features(nlp, corpus)\ntest_data = get_features(nlp, X_test)\ntrain_data['target'] = y_train\ntrain_data.head()","6b054dfc":"# plot 3d\nfig = plt.figure(figsize = (10,8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(train_data[train_data[\"target\"]==0]['mean'], \n           train_data[train_data[\"target\"]==0]['min'], \n           train_data[train_data[\"target\"]==0]['max'], c=\"black\")\nax.scatter(train_data[train_data[\"target\"]==1]['mean'], \n           train_data[train_data[\"target\"]==1]['min'], \n           train_data[train_data[\"target\"]==1]['max'], c=\"red\")\nax.set(xlabel='mean', ylabel='min', zlabel='max')\nax.set_zlim(0,10)\nax.set_ylim(-8,0)\nax.set_xlim(-15,5)\n","02a16cca":"svc = SVC(random_state=1, C = 1, gamma = 10, kernel = 'rbf', probability = True)\nsvc.fit(train_data.drop(['target'],axis=1, inplace = False), train_data['target'])\n\ny_pred = svc.predict(test_data)\nprint('F1 = ', f1_score(y_true = y_test, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))","ba7ad892":"svc = SVC(random_state=1, C = 1, gamma = 5, kernel = 'rbf', probability = True)\nsvc.fit(train_data.drop(['target', 'max'],axis=1, inplace = False), train_data['target'])\n# plot decision regions\nfig= plt.figure(figsize=(14,4))\nplot_decision_regions(np.array(train_data.drop(['target', 'max'],axis=1, inplace = False)), \n                      np.array(train_data['target']), svc)\nplt.show()","91850170":"model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\nlen(model.wv.vocab) # the model has 3000000 words in a vocabulary","caa9fe4c":"model.most_similar('fire', topn = 3)","8e827cae":"X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.1, stratify=y)\nX_tr, X_ts, y_tr, y_ts = list(X_tr), list(X_ts), list(y_tr), list(y_ts)\ntrain_data = get_features(model, X_tr)\ntest_data = get_features(model, X_ts)\ntrain_data['target'] = y_tr\ntrain_data.head()","c38368b6":"svc = SVC(random_state=1, C = 1, gamma = 5, kernel = 'rbf', probability = True)\nsvc.fit(train_data.drop(['target'],axis=1, inplace = False), train_data['target'])\n\ny_pred = svc.predict(test_data)\nprint('F1 = ', f1_score(y_true =  y_ts, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))","3661f7b8":"documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train)]\nmodel_dbow = Doc2Vec(documents, vector_size=200, dm = 0, window=7, min_count=1, epochs = 40)\nlen(model_dbow.wv.vocab.keys()) # number of words in a vocabulary","91558794":"v = model_dbow.infer_vector(['fire in the forest'])\nmodel_dbow.docvecs.most_similar([v], topn=5)","643b0f5e":"test_vec  = []\ny_pred = []\ntrain_vec  = []\nfor i in X_train:\n    train_vec.append(model_dbow.infer_vector(i, steps = 20))\n\nfor i in X_test:\n    test_vec.append(model_dbow.infer_vector(i, steps = 20))\n\nfor v in tqdm(test_vec): \n    temp = []\n    for i in range(5):\n        id = model_dbow.docvecs.most_similar([v], topn=5)[i][0]\n        temp.append(y_train[id])\n    if (len(set(temp)) == 2):\n        y_pred.append(mode(temp))\n    else:\n        y_pred.append(temp[0])\n        ","0ae6f725":"# plot ROC curve, print accuracy and F1 score\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('F1 = ', f1_score(y_true = y_test, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))\n","e936be55":"svc = SVC(random_state=1, C = 5, gamma = 6, kernel = 'rbf', probability = True)\nsvc.fit(train_vec, y_train)\ny_pred = svc.predict(test_vec)\nprint('F1 = ', f1_score(y_true = y_test, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nplt.title('ROC curve')\nplt.plot(fpr, tpr, 'b')\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('F1 = ', f1_score(y_true = y_test, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))","5ca179b5":"<a id=\"section-six\"><\/a>\nNeither accuracy nor decision regions showed that model is able to understand and distinguish tweets. Maybe the problem is on the stage of transforming of words vectors into documents vectors. Or maybe we will get better prediction if we use well pre-trained word2vec model. On the next step we will download Google word2vec model trained on news.","a9e65405":"<a id=\"section-nine\"><\/a>\nKnowing this we can write an algorithm which will estimate the same way each document in our test data. This way we will get vector representation of each document in test data. Then we will use these vector embeddings to find 5 the most similar documents in our training dataset. After that we will find out which class majority of these 5 training documents belong to. It will be a predicted class for test document.  ","e9c7b23e":"<a id=\"section-6\"><\/a>\nThis way we got the same 3 features as when we trained model on tweets. There mean, min and max value of sum of vectorized words in each tweet. But this time we use words embeding from google model.","752ea0bf":"<a id=\"section-five\"><\/a>\nFrom the scatter plot we see that 'mean-min-max' method is not able to clearly seperate tweets. Nevertheless I will try to train SVC model on this data.","dd700029":"We just trained our model on a train data set. Using vectors embedding of the words we can find the most similar words to the given word.","00e524b5":"<a id=\"section-ten\"><\/a>\nNow lets train SVC model using vector representation of documents ","4ebecf23":"But the result looks even worse. So I can conclude that using Word2Vec algorithm with 'mean-min-max' approach is not a good way for classifying documents. For these purposes you should try another algorithm of transforming words vectors into document vectors. ","02487c0f":"<a id=\"section-three\"><\/a>\nNow we have vector for each word. For classification we need to somehow transform our words vectors into documents vectors. I am sure there might be thousands of ways to do it, but we will use the easiest and quickest way. We will just sum words vectors for each document and find 3 values: its mean, min value and max value. To some extent these values are supposed to reflect document meaning.","a68ba132":"<a id=\"section-four\"><\/a>\nThis way we got 3 features for each document. Using these 3 dimensions we are able to build 3D scatter plot.","0fa30cca":"But words in a model are not stemmed. Instead of stemming it we will just use our initial training data. ","c56f4ced":"<a id=\"section-one\"><\/a>\n## Load and process the data using our own tokenizer and SnowballStemmer","f92512ad":"<a id=\"section-eight\"><\/a>\nOn this stage we create 'TaggedDocument' using our X_train data. This object is supposed to be sent in Doc2Vec function. The Doc2Vec function will build vector embedings for each document in received data.","73eada1b":"<a id=\"section-seven\"><\/a>\n## Doc2Vec","b819f432":"<a id=\"section-two\"><\/a>\n## Word2Vec","489fa73b":"<a id=\"section-eleven\"><\/a>\nTo sum up, Doc2Vec works much better then Word2Vec model. But is is worth saying that for documents classification we need to somehow transform vectors of words made by Word2Vec to vectors of documents. The way I did it in this notebook is not the best. Likely that is why we got such bad result for Word2Vec model. Meanwhile, 'mean-min-max' approach is an easy way to transform word vectors and might work better on small and well distinguished datasets.","fad6e187":"<a id=\"section-0\"><\/a>","287265a5":"1. [Load and process the data](#section-one)\n2. [Word2Vec model](#section-two)\n  - [Transform Words vectors into doc vectors](#section-three)\n  - [3D visualization](#section-four)\n  - [SVC model for Word2Vec](#section-five)\n  - [Decision regions for SVC](#section-0)\n  - [Google pre-trained Word2Vec model](#section-six)\n  - [SVC on google model](#section-6)\n3. [Doc2Vec model](#section-seven)\n  - [Train Doc2Vec model](#section-eight)\n  - [Find the most similar targets for test dataset](#section-nine)\n  - [SVC model using Doc2Vec vectors](#section-ten) \n4. [Conclusion](#section-eleven)\n","3f8b84cd":"Using the next line we can get vector representation for given string according to our model weights. Then each such vector is used to find the most similar document among document used for training model. It returns id of 5 the most similar documents in training data."}}