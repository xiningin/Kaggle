{"cell_type":{"7839140a":"code","4bc50c0b":"code","07202787":"code","00539bec":"code","4f006b18":"code","553ce255":"code","41dc2dfa":"code","9743d722":"code","563eb1b9":"code","840518b5":"code","dfa9eab2":"code","01804f0c":"code","fddffdb2":"code","5266cb4a":"code","3887038b":"code","3a0246b6":"code","de480c8f":"markdown","41980019":"markdown","124ed6f4":"markdown","91787d1a":"markdown","fb254c6d":"markdown","eefe792f":"markdown","63d17d89":"markdown"},"source":{"7839140a":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom scipy.stats import boxcox\nfrom sklearn.preprocessing import RobustScaler, power_transform\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nwarnings.filterwarnings('ignore')","4bc50c0b":"# Loading dataset into memory\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\n# Shape of dataset\nprint(df.shape)\n\n# First 5 rows of dataset\ndf.head()","07202787":"# Basic statistical parameters of each column in dataset\ndf.describe()","00539bec":"# Select all column names except \"Class\"\ncols = df.drop('Class', axis = 1).columns\n\n# Histogram plots of selected columns\ndf[cols].hist(figsize = (20, 10), bins = 30)\nplt.tight_layout()\nplt.show()","4f006b18":"# Scaling Amount and Time columns\nscaler = RobustScaler()\ndf['Amount_scaled'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['Time_scaled'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n\ndf.drop(['Amount', 'Time'], axis = 1, inplace = True)","553ce255":"# Plot scaled Amount and Time features\ndf[['Amount_scaled', 'Time_scaled']].hist(figsize = (20, 5), bins = 50)\nplt.show()","41dc2dfa":"def skew_kurt(dataset):\n    '''\n    This function returns dataset, which contains skew and kurtosis for each of dataset columns except \"Class\"\n    '''\n    cols = dataset.drop('Class', axis = 1).columns\n\n    skew = dataset[cols].skew()\n    kurtosis = dataset[cols].kurtosis()\n\n    return pd.DataFrame({'Skew': skew.values, 'Kurtosis': kurtosis.values}, index = skew.index).T","9743d722":"# Skew and kurtosis of original dataset\norig_skew = skew_kurt(df)\norig_skew.head()","563eb1b9":"# Applying 'yeo-johnson' transformation to transformed dataset\ndf_transformed = df.copy()\ncols = df_transformed.drop('Class', axis = 1).columns\n\nfor col in cols:\n    if orig_skew.loc['Skew', col] < -1 or orig_skew.loc['Skew', col] > 1:        \n        df_transformed[col] = power_transform(df[col].values.reshape(-1, 1), method = 'yeo-johnson')","840518b5":"# Histograms of transformed features\ndf_transformed[cols].hist(figsize = (15, 8), bins = 30)\nplt.tight_layout()\nplt.show()","dfa9eab2":"# Comparison of skew and kurtosis of original and transformed dataset\ntransformed_skew = skew_kurt(df_transformed)\n\nplt.figure(figsize = (20, 6))\n\nplt.subplot(121)\nplt.plot(orig_skew.T[['Skew']], label = 'orig_skew')\nplt.plot(transformed_skew.T[['Skew']], label = 'trans_skew')\nplt.legend(); plt.grid(); plt.xticks(rotation = 90)\nplt.title('Skewness before and after transformation'); plt.xlabel('Column'); plt.ylabel('Skew')\n\nplt.subplot(122)\nplt.plot(orig_skew.T[['Kurtosis']], label = 'orig_kurt')\nplt.plot(transformed_skew.T[['Kurtosis']], label = 'trans_kurt')\nplt.legend(); plt.grid(); plt.xticks(rotation = 90)\nplt.title('Kurtosis before and after transformation'); plt.xlabel('Column'); plt.ylabel('Kurtosis')\nplt.show()","01804f0c":"# Defining training data and labels\ncols = df.drop('Class', axis = 1).columns\nX_orig = df[cols]\nX_trans = df_transformed[cols]\nY = df['Class']\n\n# Split training data using 33% of data as test dataset\ntrain_x_orig, test_x_orig, train_y_orig, test_y_orig = train_test_split(X_orig, Y, test_size = 0.33, stratify = Y, random_state = 1)\ntrain_x_trans, test_x_trans, train_y_trans, test_y_trans = train_test_split(X_trans, Y, test_size = 0.33, stratify = Y, random_state = 1)","fddffdb2":"# Training IsolationForest algorithm for both datasets\nclf_orig = IsolationForest(n_estimators = 200, max_samples = 1.0, n_jobs = -1, verbose = 1, random_state = 1)\nclf_orig.fit(train_x_orig)\npreds_orig = clf_orig.predict(test_x_orig)\n\nclf_trans = IsolationForest(n_estimators = 200, max_samples = 1.0, n_jobs = -1, verbose = 1, random_state = 1)\nclf_trans.fit(train_x_trans)\npreds_trans = clf_trans.predict(test_x_trans)","5266cb4a":"# Plot confusion matrix\nfig = plt.figure(figsize = (10, 10))\n\npreds_orig = np.where(preds_orig == -1, 1, 0)\npreds_trans = np.where(preds_trans == -1, 1, 0)\n\nplt.subplot(121)\nconfusion_orig = confusion_matrix(test_y_orig, preds_orig)\nsns.heatmap(confusion_orig, annot = True, fmt = 'd', square = True, xticklabels =  ['P_Non_fraud', 'P_Fraud'], \n            yticklabels = ['Non_fraud', 'Fraud'], cbar = False, cmap = 'Blues').set_title('Original dataset')\n\nplt.subplot(122)\nconfusion_trans = confusion_matrix(test_y_trans, preds_trans)\nsns.heatmap(confusion_trans, annot = True, fmt = 'd', square = True, xticklabels =  ['P_Non_fraud', 'P_Fraud'], \n            yticklabels = ['Non_fraud', 'Fraud'], cbar = False, cmap = 'Blues').set_title('Transformed dataset')\nplt.show()","3887038b":"# To test LOF I'll use only 10% of the data, because neighbouring classifiers can be eally slow and memory consuming with big amount of samples \ntrain_x_orig, test_x_orig, train_y_orig, test_y_orig = train_test_split(X_orig, Y, test_size = 0.1, stratify = Y, random_state = 1)\ntrain_x_trans, test_x_trans, train_y_trans, test_y_trans = train_test_split(X_trans, Y, test_size = 0.1, stratify = Y, random_state = 1)\n\nlof_orig = LocalOutlierFactor(n_jobs = -1)\nlof_preds_orig = lof_orig.fit_predict(test_x_orig)\n\nlof_trans = LocalOutlierFactor(n_jobs = -1)\nlof_preds_trans = lof_trans.fit_predict(test_x_trans)","3a0246b6":"fig = plt.figure(figsize = (10, 10))\n\nlof_preds_orig = np.where(lof_preds_orig == -1, 1, 0)\nlof_preds_trans = np.where(lof_preds_trans == -1, 1, 0)\n\nplt.subplot(121)\nconfusion_orig = confusion_matrix(test_y_orig, lof_preds_orig)\nsns.heatmap(confusion_orig, annot = True, fmt = 'd', square = True, xticklabels =  ['P_Non_fraud', 'P_Fraud'], \n            yticklabels = ['Non_fraud', 'Fraud'], cbar = False, cmap = 'Blues').set_title('Original dataset')\n\nplt.subplot(122)\nconfusion_trans = confusion_matrix(test_y_trans, lof_preds_trans)\nsns.heatmap(confusion_trans, annot = True, fmt = 'd', square = True, xticklabels =  ['P_Non_fraud', 'P_Fraud'], \n            yticklabels = ['Non_fraud', 'Fraud'], cbar = False, cmap = 'Blues').set_title('Transformed dataset')\nplt.show()","de480c8f":"We got almost same results for both datasets. Both classifiers managed to classify 90.74% fraud transactions correctly and about 90% of non_fraud transactions, but dataset with transformed features gave us slightly better results on predicting non_fraud transactions.","41980019":"Observations:\n* Time and Amount features must be scaled\n* Some features (like V1) skewed to the right or left and have high kurtosis, we can apply different transformations to reduse skewness and kurtosis to improve ML algorithm perfomance","124ed6f4":"According to skew plot - we managed to reduce skew and distribution of most features now are very close to normal distibution.","91787d1a":"In comparison with IsolationForest, LOF gave us poor results - only about 50% of fraud transactions were classified correctly on original dataset and even less on dataset with transformations.","fb254c6d":"In this kernel I want to experiment with 2 datasets:\n* Original dataset with scaled Amount and Time features\n* Original dataset with scaled Amount and Time features and transformation of features to reduse skewness and kurtosis","eefe792f":"The goal of this short kernel is to test 2 anomaly detection algorithms - IsolationForest and LocalOutlierFactor for fraud detection task.","63d17d89":"Observations:\n* V* features has mean that very close to 0 and standard deviation around 1, it means that we have deal with scaled data.\n* Time and Amount features are unscaled and we need to scale them before applying any prediction model.\n* Class mean is 0.001727, it means that we have deal with very imballanced dataset"}}