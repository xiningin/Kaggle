{"cell_type":{"76b2e3f0":"code","40112527":"code","932c3cec":"code","f03db669":"code","12337cf7":"code","d88ecc1d":"code","3f9f815d":"code","5972fc38":"code","dd1e08b6":"code","d5a1c6fa":"code","85019476":"code","1969b7c3":"code","ebaf1f26":"code","3fb2b212":"code","87c58f07":"code","b7de467f":"code","6cef8e76":"code","082d02da":"code","bf0e1ebe":"code","5ac48b66":"code","77f2d8b3":"code","30b5566f":"code","e88094b6":"code","8bb2c738":"code","b25d9ea7":"code","f8df73af":"code","fe1e87ca":"code","d3ed8d68":"code","f53d6eef":"code","0e493fe9":"code","cf6a5b23":"code","32c28e70":"code","bd8588ad":"code","ddcb92b7":"code","56f1e1ff":"code","ddb3b99f":"code","eb571dbf":"code","5e56ad11":"code","371552e2":"code","6aae3a09":"code","11e8847d":"code","9fd562a7":"markdown","64538531":"markdown","56dcc2c0":"markdown","56c62b95":"markdown","ac7ca248":"markdown","8a84c4f2":"markdown","eecde5ab":"markdown","70a8feba":"markdown","1fceba0b":"markdown","6c9930a3":"markdown","e4d35237":"markdown","0a09314e":"markdown","5ed35ad1":"markdown","022a34d1":"markdown","c9d69ace":"markdown","7aa48e82":"markdown","4d974596":"markdown","72aba7b9":"markdown","34132b12":"markdown","2acefee8":"markdown","61b09c88":"markdown","40ff7ac5":"markdown","1d115790":"markdown","ff340069":"markdown","22055e51":"markdown","8ab3fe0a":"markdown","6c7c379d":"markdown","afc806ee":"markdown","a3216bbd":"markdown","5a059d50":"markdown","99202978":"markdown","67c3fcd4":"markdown","2cff8fb5":"markdown","0d727261":"markdown","fb64f060":"markdown","f606c8ac":"markdown","aaa6db03":"markdown","3db089e8":"markdown","ab1c3cb0":"markdown","2fb32753":"markdown","455c94ac":"markdown","70492c74":"markdown","32f9acb9":"markdown","454775d4":"markdown","1d66fadc":"markdown","de4087d7":"markdown","35e2ed69":"markdown","a6c13c40":"markdown"},"source":{"76b2e3f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #data visualization\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n'''\nBunch of statements by Kaggle to get path to your data. \nDon't worry if you don't understand them they are not of much concern right now.\n'''\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","40112527":"# Loading Data in daaframe\n\ninfo_df = pd.read_csv('\/kaggle\/input\/predict-students-knowledge-level\/Data_User_Modeling_Dataset - Information.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/predict-students-knowledge-level\/Data_User_Modeling_Dataset - Test_Data.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/predict-students-knowledge-level\/Data_User_Modeling_Dataset - Training_Data.csv')","932c3cec":"# Information.csv\ninfo_df.head() #to print the first 5 rows of the dataframe","f03db669":"'''\nTo remove the column width restriction by pandas,\nAn esy solution is to see the column by converting it to a list\n'''\nlist(info_df['Attribute Information:']) ","12337cf7":"train_df.head()","d88ecc1d":"train_df.info()","3f9f815d":"train_df[' UNS']","5972fc38":"test_df.head()","dd1e08b6":"test_df.info()","d5a1c6fa":"# Let us start with training data\n\ntrain_df.columns","85019476":"'''\n* drop() function drops all the columns passed into it, \n* axis=1 is for telling drop() to delete values in the column not in the row, \n* inplace=True is for telling drop to save all the changes to oue dataframe.\n'''\ntrain_df.drop(['Unnamed: 6', 'Unnamed: 7','Attribute Information:'], axis=1, inplace=True)","1969b7c3":"# Now lets fix the name of 6th column.\n# Its a good practice to keep column names the same as provided in the data set but here for better understanding we will change UNS to Knowldege Level\ntrain_df.rename(columns = {' UNS':'Knowledge Level'}, inplace = True) ","ebaf1f26":"# And the magic has been done.\ntrain_df.head()","3fb2b212":"def fix_data(temp_df):\n    #remenber you can always optimize you work by defining a function or using a loop.\n    \n    temp_df.drop(['Unnamed: 6', 'Unnamed: 7','Attribute Information:'], axis=1, inplace=True)\n    temp_df.rename(columns = {' UNS':'Knowledge Level'}, inplace = True)\n    return temp_df","87c58f07":"test_df = fix_data(test_df)","b7de467f":"# And like this evrything is done.\ntest_df.head()","6cef8e76":"# Checking data for nan values\n'''\nAs above we need to do all operation twice for both of the data frames so we will design a function.\n'''\ndef clean_my_data(df):\n    if df.isnull().values.any(): # isnull().values.any() returns a boolean value depending upon the presence of null value anywhere in data\n        print(df.isnull()) # returns dataframe filled with boolean values for presence of null\n        df = df.dropna()\n    else: print('Your data do not contain any missing values.')\n    return df","082d02da":"train_df = clean_my_data(train_df)","bf0e1ebe":"test_df = clean_my_data(test_df)","5ac48b66":"import seaborn as sns\n'''\n* Here pairplot does is it takes all the features of the dataframe and plot them pair wise as can be seen below.\n* train_df.select_dtypes(include=[np.number]).columns this returns only those columns which contain numeric data.\n* passing the above columns in train_df we can easily return dataframe with only numeric data so that we can see the distribution of data \na swell as can observe the outliers presence.\n'''\nsns.pairplot(train_df[train_df.select_dtypes(include=[np.number]).columns])","77f2d8b3":"f,ax = plt.subplots(figsize=(10, 6))\nsns.boxplot(data = train_df, x='Knowledge Level', y='PEG', ax=ax)","30b5566f":"train_df.drop(train_df[train_df['PEG'] < 0 ].index, axis=1, inplace=True)","e88094b6":"#fig = plt.figure(figsize=)\n#axes = fig.subplots(2, 3)\ntrain_df.boxplot(layout=(2,3), by='Knowledge Level', figsize=[15,10])\n","8bb2c738":"f,ax = plt.subplots(figsize=(10, 4))\nsns.countplot(x = 'Knowledge Level', data = train_df, palette=\"Set2\", ax=ax)\nplt.grid(True)","b25d9ea7":"'''\nPandas come with the best method to start an EDA. You can directly use pandas to analyze and describe your data\n'''\ntrain_df.describe()","f8df73af":"f,ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(train_df.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)","fe1e87ca":"f,ax = plt.subplots(figsize=(10, 8))\nsns.scatterplot(train_df['STG'], train_df['PEG'], hue=train_df['Knowledge Level'], ax=ax, palette=\"Set1\")\nax.set_facecolor('#e0f2fc')\nplt.grid(True)","d3ed8d68":"f,ax = plt.subplots(figsize=(10, 8))\nsns.scatterplot(train_df['SCG'], train_df['PEG'], hue=train_df['Knowledge Level'], ax=ax, palette=\"Set1\")\nax.set_facecolor('#e0f2fc')\nplt.grid(True)","f53d6eef":"f,ax = plt.subplots(figsize=(10, 8))\nsns.scatterplot(train_df['PEG'], train_df['LPR'], hue=train_df['Knowledge Level'], ax=ax, palette=\"Set1\")\nax.set_facecolor('#e0f2fc')\nplt.grid(True)","0e493fe9":"train_df['Knowledge Level'].unique()","cf6a5b23":"Knwoledge_levels = train_df['Knowledge Level'].unique()","32c28e70":"df = train_df\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'species'. \ndf['Knowledge Level']= label_encoder.fit_transform(df['Knowledge Level']) \ntest_df['Knowledge Level'] = label_encoder.fit_transform(test_df['Knowledge Level'])\ndf\n","bd8588ad":"print('Encoding Approach:')\nfor i,j in zip(Knwoledge_levels, df['Knowledge Level'].unique()):\n    print('{}  ==>  {}'.format(i,j))","ddcb92b7":"df = df.append(test_df, ignore_index = True) #using append to add two datasets.","56f1e1ff":"X = df.drop(['Knowledge Level'],axis=1) # assigning X all the independent variable\ny = df['Knowledge Level'] #assigning y the target variable","ddb3b99f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #using train_test_split() to randomly split dataset to train and test.","eb571dbf":"# In actual we will be using Support Vector Classifier thats why we create an object of SVC.\nclf = SVC()\nclf.fit(X_train, y_train) #fitting data in the model.","5e56ad11":"clf.score(X_test, y_test)","371552e2":"clf_1 = SVC(C=50, gamma=1)","6aae3a09":"clf_1.fit(X_train, y_train)\nclf.score(X_test, y_test)","11e8847d":"clf_2 = SVC(C=100, gamma=0.1)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","9fd562a7":"# Observations: -\n- There is no linear correlation between any of the features.","64538531":"# Summary:\n\n- The data provided contain 5 independent features and 1 target variable.\n- Classification of Students on the basis of these 5 features is very likely to be possible.\n- Features do not hold any linear correlation with each other.\n- Most of the students are likely to be ranked at Middle level knowledge students.\n- The student count for students with highest knowledge level are second to last.","56dcc2c0":"# \u261d\ufe0f\nThe above information about our training data shows high inconsistency in it.\n- There are columns with no values in it.\n- There are columns which are named wrong way ('< space >UNS')\n- Column, 'Attribute Information' holds no relevance placed in this data frame.\n\n### Other important information.\n- In the training Data we have information about 258 students.\n- All the columns except UNS hold float value datatype data.\n- UNS holds either string or any other object data.","56c62b95":"------------------------------------------------\n# \ud83d\udd0d Understanding the DATA.\n----------------------------------------------\n\nWe will start by loading our data in pandas' dataframes and understanding them one by one.","ac7ca248":"# Cleaning and Fixing out Data\n","8a84c4f2":"## Limitation of label Encoding\n---\nLabel encoding convert the data in machine readable form, but it assigns a unique number(starting from 0) to each class of data. This may lead to the generation of priority issue in training of data sets. A label with high value may be considered to have high priority than a label having lower value.\n\nSo we also need to **One-Hot encoding**.\n\n*One-hot encoding* ensures that machine learning does not assume that higher numbers have a higher value. For example, the value '8' is bigger than the value '1', but that does not make '8' more important than '1'. The same is true for words. The value 'New York' is not more important than 'York'.\n\n![](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fi.imgur.com%2FmtimFxh.png&f=1&nofb=1)\n\nTo learn more about One-Hot Encoding [Click Here](https:\/\/www.quora.com\/What-is-one-hot-encoding-and-when-is-it-used-in-data-science?share=1)\n\nHere I will not be showing you the HO-Encoding otherwise it will be a lot to digest in one bite. Tough I am giving a code which you can use by yourself to understand what is One-Hot Encoding.\n```python\nhot_encoded=pd.get_dummies(df, columns=['Knowledge Level'])\nhot_encoded\n```","eecde5ab":"Now one part of our task has been done that is fixing now all we left with is Cleaning.\n\n---\n## Cleaning the Data\n![cleaning](https:\/\/i.pinimg.com\/originals\/41\/5b\/fd\/415bfd2c979b0f4c671a3a133e441785.gif)","70a8feba":"We will be using `Seaborn` for visualizing purpose as it is an extention to matplotlib with various awesome features.","1fceba0b":"# \u261d\ufe0f\nThe above plots shows no siginificance presence of outliers. Although we can move on with various other methods to detect and eliminate out liers. But for now I will not be covering those topics as purpose of this notebook is not to teach everything but to teach you the very basics of ML and DA.","6c9930a3":"![](https:\/\/media1.giphy.com\/media\/xT9KVmZwJl7fnigeAg\/giphy.gif)","e4d35237":"We have got an accuracy of about 94% for the first try which is a very good score for the first time run. Now to further incrase accuracy of the model we can tune the model using Hyperparameters. The list of Hyper parameters for SVM are:-\n1. Regularization parameter(C)\n2. kernel\n3. gamma\n4. coef0\n\nthese are few of the hyperparameters to change and tune the model. [Further Information](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","0a09314e":"so as conveyed in Information dataframe UNS contains level on which every student is ranked.","5ed35ad1":"Here we have two different files containing test and train data but most of the time we have a single data file containg the whole data and we need to crate two randomly separated datasets for training and testing purpose. \n\nSo I will merge the databasets and will show you how we do create random test and train datasets using `sklearn library`.","022a34d1":"# Label Encoding.\n\n\n### What the heck is Label Encoding?\nAs explained before our 'Knowledge Level' column is a string object and cannot be understood by the computer so we need to encode our string data to numeric form.\n\nEXAMPLE: \n<img src=\"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fpic.fizzy.cc%2Fimg%2Flabel_encoding.png&f=1&nofb=1\" >","c9d69ace":"### So far we have observed no linear distribution we have been able to see a glimpse of data distribution in separated groups.\n\n\nLets try to plot data in a way that we can observe groups if they exxist in distribution.","7aa48e82":"Now we need to repeat the 2 operations above for our testing dataset. Here there are just 2 operations and 2 dataframes what if I give you 10 datasets with 5 operation to be repeated you need to do it for 50 tiime. So our fundamentals of programming says that,\n\n#### We Programmers are Lazy as Hell.\n\nSo we will try to design a function that can accept a data frame and do all the work.","4d974596":"For now we will focusing only on the basics of cleaning and outlier detection.\n1. Checking Data for Null, NAN, or Invalid values.\n2. Scanning Data For outliers.\n3. Removing outliers on basis of visual observations.","72aba7b9":"Similarly we will be encoding vaues in Knowledge Level feature using Label Encoder from sklearn.preprocessing","34132b12":"![](https:\/\/media1.tenor.com\/images\/b6518d732a9cd1857f7c1dde76dc907a\/tenor.gif?itemid=11637169)","2acefee8":"# \u261d\ufe0f\nFrom the above tabel \n# Observations:-\n- The average performance degree of a student is 0.458\n- Degree of performance ranges from 0 to 0.93 in data set.\n- From Standard Deviation of all the features it is clear that distribution of data is not normal.","61b09c88":"We need to remove the last 3 columns and rename 6th column.","40ff7ac5":"# Exploratory Data Analysis\n\nWe will try to answer question and determine realtionships inside data.","1d115790":"# Tunning the model.\n\nby practise it is seen that most of the time increasing value of Regularization and decreasing value of gamma increases accuuracy.","ff340069":"I guess the definitions of acronyms are pretty self-undestandable. So, lets proceed now to our second and third dataset.\n\n-----------\n\n2. **Training_Data.csv**\n3. **Test_Data.csv**\n\nWe are being provided with two separate data files containg data for specific purpose of Training and Testing respectively.\nLets see what these contain inside them.","22055e51":"We have now understood what our data is and how it is provided to us. So, lets move to our next step.\n\n---\n# Discussion about what can we do and what should be done with our DATA.\n\n1. First thing first we need to do a bit of cleaning in our provided data to proceed with any kind of analysis or anything.\n2. We got a feature(Column) in our data ' UNS' whose name is to be fixed.\n3. UNS, is a [categorical Data](https:\/\/study.com\/academy\/lesson\/categorical-data-definition-analysis-examples.html) which cannot be directly used with our machine learning models.\n\n*Why it can not be used directly?*\n```\nI will be answering the question in a very simple way so that everyone can unerstand. UNS contains strings which are not defined in mathematics or in other words are not understandable by computers so we cannot feed our models these kind of data.\n```\n\n*What should be done to fix this?*\n\n```The computers understand only numbers and hence we will convert our categorical data to numeric data by using an approach called ``` [Label Encoding](https:\/\/www.geeksforgeeks.org\/ml-label-encoding-of-datasets-in-python\/).\n\n----\n\n# EDA and Modeling objectives: -\n\nAfter understanding basic tasks to carry over data we need to describe and understand big picture.\n1. Analyse Data\n    - Try finding out relation ship among various features of Data.\n    - Knowing what is your target variable in this case it is UNS (Level of Knowledge of a Studen).\n    - Try finding out relationship of target variable with other features.\n    - Note down very specific questions that can or should be answered by this type of data.\n        1. How UNS\/Level of Knowledge depends upon PEG (The exam performance of user for goal objects)\n        2. Is there any relationship between PEG (The exam performance of user for goal objects) and SCG (The degree of repetition number of user for goal object materails) and much more.\n        3. Id it reliable to predict UNS based upon the other features in the data.\n    - The most important part of EDA is to visualize your data to support you findings.\n    \n  ** The above are some of the basic questions that should be raised by your mind after understanding this data.\n     The main motive behind telling you all this is to make you understand the way you should start questioning your data.** \n\n2. Modeling Analysis:\n   \n       If you are a little bit familiar with types of Machine Learning problems then you must be having various models in your mind to use with this data but for beginners let me tell you about this topic in very simple words.\n       \n       When you see this data and understand that UNS\/Level of Knowldege is our target variable you know that the thing we need to do is to label a student or rank a student on the basis of him\/her performance. This labeling or ranking can be considered as classification right? So, this problem or data can only be solved using Machine Learning Classifiers. \n       \n       What I want to tell you is this that to label or classify something on the basis of its feature we use specific Machine learning models. If you go on Google and search for \"Types of problems Machine Learning solve.\"\n       We will discuss fiurther about it Later.\n       \n       ","8ab3fe0a":"# Importing Libraries:-\n\n1. **Pandas**:-  \n    \n    Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python     programming language [Pandas Documentation](https:\/\/pandas.pydata.org\/docs\/)\n***\n\n2. **Matplotlib**: - \n    \n    Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides     an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter.\n    [Matplotlib Documentation](https:\/\/matplotlib.org\/contents.html)\n","6c7c379d":"No change noticed.","afc806ee":"### If you liked and got benefited from this kernel make sure to upvote it.\n\n![Thankyou](https:\/\/1.bp.blogspot.com\/-h1ZVngco9Ho\/Xeo-8eXRpPI\/AAAAAAAAJhk\/GB1R49vn2Ukhp24tRlkv084E0oYWY4weACLcBGAsYHQ\/s400\/Thank-You-GIF%2B%252817%2529.gif)","a3216bbd":"## Box Plots\n****\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*2c21SkzJMf3frPXPAR_gZA.png\" height=300 width=600>\n\n\n*A boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile(Q1), median, third quartile (Q3), and \u201cmaximum\u201d). \nIt can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.*","5a059d50":"So now we have encoded our string values to numeric values.","99202978":"# #2 Notebook | Student Knowldege Level Prediction\n----\n## This notebook serves the purpose of a tutorial for beginners in EDA and Machine Learning Modeling.\n","67c3fcd4":"### Let's see how good is our model.","2cff8fb5":"![student picture](https:\/\/img.freepik.com\/free-vector\/boy-studying-with-book_113065-238.jpg?size=626&ext=jpg)\n\n## Assesment of Student's Knowledge Level\n\nYou can choose from a variety of methods to assess your students\u2019 prior knowledge and skills.  Some methods (e.g., portfolios, pre-tests, auditions) are direct measures of students\u2019 capabilities entering a course or program. Other methods (e.g., students\u2019 self-reports, inventories of prior courses or experiences) are indirect measures.\n\n*Here we have a data that shows Student's Knowldege based on **Perormance** ranked as:-*\n\n1. Very Low\n2. Low\n3. Middle\n4. High\n\n### My belief: -\n> It is very wrong to judge a student's knowledge level based on his\/her performance because knowledge can never be summarized on a sheet of paper. Many of the teachers, parents and gaurdians judge the students or kids on there quantitative performance in the school. Please Stop this!!\n----\n----\n\n# In this EDA and Modeling Notebook\n\n- Understanding our data and its features.\n- Making a note of Questions that can be answered using this data.\n- Cleaning our Data\n- Vizualizing our data to find answers in the data.\n- Using little statistics to explore and analyze our data.\n- Noting all the observations from the data.\n- Preparing Data for machine learning model (Pre-Processing)\n- Understanding which Machine Learning model will be suitable to use with our data to make predictions.\n- Setting up our models.\n- Testing our models.\n- Reconfiguring the model.\n- Testing again.\n- Making a few predictions.\n****\n***\n<img src=\"https:\/\/media1.tenor.com\/images\/a8794889e70df7b3c5b8149807c6340d\/tenor.gif?itemid=11880008\" height=450>","0d727261":"# Modeling\n\nSo lets see what we have and what we know.\n- During Analysis we noticed that there was an irregular data distrivution.\n- Distribution of data was nither completely linear nor was completely clustered.\n- Data is distributed in a way that it can divided into groups.\n\nThe model we can use can be either clustering or SVM. Well looking at the data we know making clusters will result in large errors so we will go with SVM(Support Vector Machine).\n\n# Support Vector Machine.\n\n<img src=\"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fi.stack.imgur.com%2F653iE.png&f=1&nofb=1\" height=450 width=450>\n\nIn machine learning, support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.","fb64f060":"### Also We can use Boxplots to visualize and identify Outliers.","f606c8ac":"Again No change is notice. \n### So we can conclude that our model can work at 92+% accuracy.\n\n> **NOTE** - You can also alter the accuracy of the model by shuffling and splitting test and train dataset.","aaa6db03":"### Outlier Detection.\n\nIn statistics, outliers are data points that don\u2019t belong to a certain population. It is an abnormal observation that lies far away from other values. An outlier is an observation that diverges from otherwise well-structured data.\n\n<img src=\"https:\/\/support.minitab.com\/en-us\/minitab\/18\/outlier_scatterplot.png\" height=250 width=250>\n\nThere are various mathematical and visual methods for eliminating outliers in data but we will be using some of them which are simple and basic.[Outlier Detection Article](https:\/\/towardsdatascience.com\/5-ways-to-detect-outliers-that-every-data-scientist-should-know-python-code-70a54335a623)","3db089e8":"> Next I will be showing you to create subplots using a loop and we will be plcing these boxplots in the grid of 6x6.","ab1c3cb0":"# \u261d\ufe0f\nIt is clear now that this dataframe or file contain information regarding the dataset and the acronyms in our dataset. Lets try to print these more effectively.","2fb32753":"### The first thing to notice from these three files is there need.\n\n\n1. Information.csv: - This file is provided to us for some kind of information regarding our data and will not contribute to any data related operation othe than providing knowledge about data in two other files.","455c94ac":"#### NextWe can do is, plot a Heat map of correlation.\n\nPlotting a correlation map will tell us about the linear relation ship between the features in the dataset.","70492c74":"# Observations \n- we can clearly see the distribution of data into groups but groups are not very distinguished with each other so lets start the modelling process but first we need to do an important stuff.","32f9acb9":"# \u261d\ufe0f\nThe above information about our testing data shows high inconsistency similar to our Training data.\n- There are columns with no values in it.\n- There are columns which are named wrong way ('< space >UNS')\n- Column, 'Attribute Information' holds no relevance placed in this data frame.\n\n### Other important Information\n- There are total of 145 students' information for aor testing basis.\n- The other information is same as our Training information that is good to work with.","454775d4":"# \u261d\ufe0f\nThe above output have three lines which are the paths to our dataset. There are 3 paths aas our data set contain three files.\n1. Test_Data.csv\n2. Information.csv\n3. Training_Data.csv","1d66fadc":"Well we can observe that ther exist PEG score less than 0 which can be an issue so we will eliminate lets see what happens to the dataset ","de4087d7":"# Observations: -\n- Here also we can see the presence of outliers in the feature PEG.\n- We will next try to identify the novely of these outlier i.e. are these outliers only due for PEG or we can see the same patterns in other column too.","35e2ed69":"# Observations: -\nThe frequency count for our Knowledge level shows:-\n- Maximum number of sutdents have Middle Level of Knowledge followed by Low and then High and at last are the very_low.\n- Count shows very less difference in the number if students having Middle and Low Level Knowledge.\n- While a huge difference can be seen between Low and very Low level and same for Middle and High level Knowledge students.\n","a6c13c40":"Now before using our data we need to do a few things\n- separate dependent and independent variables"}}