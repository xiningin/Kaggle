{"cell_type":{"796e2457":"code","a6efb49b":"code","624165a8":"code","1944878d":"code","f085de8e":"code","681b9aa3":"code","fbba27d1":"code","1cb1f7df":"code","32320a4c":"code","c8390ec6":"code","618fcecf":"code","e15a50cf":"code","036d1ced":"code","596aaa54":"code","ae601f70":"code","0cac64fd":"code","89c423a9":"code","b1128384":"code","9f3e9a5a":"code","14fd6a57":"code","564d0ea2":"code","085c3440":"code","20e1f53b":"code","540f111a":"code","b9c572bc":"code","d2c6fe98":"code","631bafd6":"code","6218003c":"code","ec35a0ea":"code","428edc8c":"code","7e36c23e":"code","07245298":"code","4aaf9c75":"code","973479b8":"code","b7168172":"code","7d661312":"code","92ed4c9b":"code","ce440a75":"code","814f20e9":"code","7e06c9d6":"code","4a89b62b":"code","45d5c801":"code","33a9bb9d":"code","eb97ddac":"code","a9316929":"code","96ab9058":"code","11248398":"code","f4ddd119":"code","a742e095":"code","d200b9a0":"code","ce497fbd":"code","9f066210":"code","bea15751":"code","9d358442":"code","70209259":"code","6c8b20e9":"code","266de230":"code","e9ba574b":"code","2a3d7b32":"code","2a4bcdbd":"code","79a53898":"code","428e2db6":"code","76b13470":"code","941c1b32":"code","850f03f2":"code","00d8d222":"code","af8abd81":"code","c8f954b3":"code","c1385355":"code","7abc93a3":"code","d5211908":"code","04426477":"code","652d0925":"code","05300866":"code","6d0cf75d":"code","496044ba":"code","df853451":"markdown","e47315bf":"markdown","976c1484":"markdown","86114a39":"markdown","67342d63":"markdown","85a13ac8":"markdown","9a9944aa":"markdown","6ab5daa7":"markdown","f5a875dd":"markdown","9bf5656d":"markdown","49fcad89":"markdown","405ed458":"markdown","c604e720":"markdown","de20c56b":"markdown","766e92ce":"markdown","6f9b22ec":"markdown","b97f1a43":"markdown","2f103569":"markdown","b43d49a3":"markdown","e7d06c21":"markdown","76c93fbb":"markdown","1bee4d1e":"markdown","446ee1b9":"markdown","ecb25c42":"markdown","cac6918e":"markdown","9e904ace":"markdown","92aa5ab4":"markdown","2bf1e6ed":"markdown","8cfd5027":"markdown","0f6a138c":"markdown","7901a310":"markdown","2f4ff45e":"markdown","aec85f4a":"markdown","ee1a81ec":"markdown","b5fd8050":"markdown","c30cdb12":"markdown","8b777cf0":"markdown","d4cc4ac9":"markdown","004544c4":"markdown","516348c3":"markdown","e1cb95e3":"markdown","00120242":"markdown","a2d5903a":"markdown","ee6784fd":"markdown","f57120a9":"markdown","809baed3":"markdown","620ce537":"markdown","bff07f52":"markdown","49b75f13":"markdown","ea03e2df":"markdown","d5cc5823":"markdown","072517bb":"markdown","e11c6e21":"markdown","f4770e76":"markdown","6231d015":"markdown","a9fa4176":"markdown"},"source":{"796e2457":"import warnings\nwarnings.filterwarnings('ignore')\nimport warnings\nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport datetime \nimport lightgbm as lgbm\n\npd.set_option('display.max_colwidth',None)","a6efb49b":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cat = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest_dataset = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","624165a8":"train.head()","1944878d":"train.shape","f085de8e":"train.isnull().sum()","681b9aa3":"train_dataset = train.copy()","fbba27d1":"train_dataset","1cb1f7df":"train_dataset[train_dataset['item_cnt_day'] == 2169.0]","32320a4c":"monthly_sales=train_dataset.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})","c8390ec6":"monthly_sales","618fcecf":"monthly_sales.columns","e15a50cf":"sales_by_month = train_dataset.groupby(['date_block_num'])['item_cnt_day'].sum()\nsales_by_month.plot()","036d1ced":"corr = train_dataset.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(train_dataset.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(train_dataset.columns)\nax.set_yticklabels(train_dataset.columns)\nplt.show()","596aaa54":"items.head()","ae601f70":"plt.rcParams['figure.figsize'] = (24, 9)\nsns.barplot(items['item_category_id'], items['item_id'], palette = 'colorblind')\nplt.title('Number of Item Sold Per Category', fontsize = 30)\nplt.xlabel('Item Categories', fontsize = 15)\nplt.ylabel('Items', fontsize = 15)\nplt.show()","0cac64fd":"plt.rcParams['figure.figsize'] = (24, 9)\nsns.countplot(train_dataset['date_block_num'], palette = 'colorblind')\nplt.title('Number of Item Sold Per Month Over 2013 - 2015', fontsize = 30)\nplt.xlabel('Month', fontsize = 15)\nplt.ylabel('Items Count', fontsize = 15)\nplt.show()","89c423a9":"# item_cat['item_category_name'].count()\nprint(item_cat['item_category_name'].nunique())\nprint(shops['shop_name'].nunique())","b1128384":"from wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'pink',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(shops['shop_name']))\n\n\nplt.title('Wordcloud for Shop Names', fontsize = 25)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","9f3e9a5a":"plt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'lightyellow',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(item_cat['item_category_name']))\n\n\nplt.title('Wordcloud for Item Category Names', fontsize = 24)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","14fd6a57":"train_dataset['date'] = pd.to_datetime(train_dataset['date'], errors='coerce')","564d0ea2":"days = []\nmonths = []\nyears = []\n\nfor day in train_dataset['date']:\n    days.append(day.day)\nfor month in train_dataset['date']:\n    months.append(month.month)    \nfor year in train_dataset['date']:\n    years.append(year.year)","085c3440":"plt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(days, palette= 'pastel')\nplt.title('The busiest days for the shops', fontsize = 24)\nplt.xlabel('Days', fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12)\n\nplt.show()","20e1f53b":"# busy month\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(months, palette= 'rocket')\nplt.title('The busiest months for the shops', fontsize = 24)\nplt.xlabel('Months', fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12)\n\nplt.show()\n\n# busy year\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(years, palette= 'cubehelix')\nplt.title('The busiest years for the shops', fontsize = 24)\nplt.xlabel('Years', fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12)\n\nplt.show()","540f111a":"train_dataset['day'] = days\ntrain_dataset['month'] = months\ntrain_dataset['year'] = years","b9c572bc":"train_dataset","d2c6fe98":"sns.countplot(train_dataset[(train_dataset.month == 2) & (train_dataset.year == 2013)]['shop_id'], palette='pastel')","631bafd6":"train_dataset.describe()","6218003c":"plt.figure(figsize=(10,4))\nplt.xlim(train_dataset.item_price.min(), train_dataset.item_price.max()*1.1)\nsns.boxplot(x=train_dataset.item_price)","ec35a0ea":"plt.figure(figsize=(10,4))\nplt.xlim(train_dataset.item_cnt_day.min(), train_dataset.item_cnt_day.max()*1.1)\nsns.boxplot(x=train_dataset.item_cnt_day)","428edc8c":"train_dataset = train_dataset[(train_dataset[\"item_price\"] > 0) & (train_dataset[\"item_price\"] < 50000)]\ntrain_dataset = train_dataset[(train_dataset[\"item_cnt_day\"] > 0) & (train_dataset[\"item_cnt_day\"] < 1000)]","7e36c23e":"train_dataset.shape","07245298":"train_dataset[train_dataset['item_price'] < 0]","4aaf9c75":"median = train_dataset[(train_dataset.shop_id==32)&(train_dataset.item_id==2973)&(train_dataset.date_block_num==4)&(train_dataset.item_price>0)].item_price.median()\nmedian","973479b8":"train_dataset[\"item_price\"] = train_dataset[\"item_price\"].map(lambda x: median if x<0 else x)","b7168172":"train_dataset[train_dataset['item_price'] < 0]","7d661312":"train_dataset[train_dataset['item_cnt_day'] < 0]","92ed4c9b":"train_dataset[\"item_cnt_day\"] = train_dataset[\"item_cnt_day\"].map(lambda x: 0 if x<0 else x)","ce440a75":"train_dataset[train_dataset['item_cnt_day'] < 0]","814f20e9":"train_dataset.head(2)","7e06c9d6":"print(\"total unique items: \", items['item_id'].nunique())\nprint(\"total unique items in train dataset: \", train_dataset['item_id'].nunique())\nprint(\"total unique items in test dataset: \", test_dataset['item_id'].nunique())\n\nprint(\"total unique shops: \", shops['shop_id'].nunique())\nprint(\"total unique shops in train dataset: \", train_dataset['shop_id'].nunique())\nprint(\"total unique shops in test dataset: \", test_dataset['shop_id'].nunique())","4a89b62b":"test_item_list = [x for x in (np.unique(test_dataset['item_id']))]\ntrain_item_list = [x for x in (np.unique(train_dataset['item_id']))]\n\nmissing_item_ids_ = [element for element in test_item_list if element not in train_item_list]\nlen(missing_item_ids_)","45d5c801":"shops","33a9bb9d":"# getting rid of \"!\" before shop_names\nshops['shop_name'] = shops['shop_name'].map(lambda x: x.split('!')[1] if x.startswith('!') else x)\nshops['shop_name'] = shops[\"shop_name\"].map(lambda x: '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"' if x == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"' else x)","eb97ddac":"shops['city'] = shops['shop_name'].map(lambda x: x.split(\" \")[0])\n# lets assign code to these city names too\nshops['city_code'] = shops['city'].factorize()[0]","a9316929":"shops.head(2)","96ab9058":"for shop_id in shops['shop_id'].unique():\n    shops.loc[shop_id, 'num_products'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_id'].nunique()\n    shops.loc[shop_id, 'min_price'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_price'].min()\n    shops.loc[shop_id, 'max_price'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_price'].max()\n    shops.loc[shop_id, 'mean_price'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_price'].mean()","11248398":"shops.head(2)","f4ddd119":"item_cat","a742e095":"cat_list = []\nfor name in item_cat['item_category_name']:\n    cat_list.append(name.split('-'))","d200b9a0":"item_cat['split'] = (cat_list)\nitem_cat['cat_type'] = item_cat['split'].map(lambda x: x[0])\nitem_cat['cat_type_code'] = item_cat['cat_type'].factorize()[0]\nitem_cat['sub_cat_type'] = item_cat['split'].map(lambda x: x[1] if len(x)>1 else x[0])\nitem_cat['sub_cat_type_code'] = item_cat['sub_cat_type'].factorize()[0]","ce497fbd":"item_cat.head(2)","9f066210":"item_cat.drop('split', axis = 1, inplace=True)\nitem_cat.head(2)","bea15751":"train_dataset = train_dataset[train_dataset[\"item_cnt_day\"]>0]\ntrain_dataset = train_dataset[[\"month\", \"date_block_num\", \"shop_id\", \"item_id\", \"item_price\", \"item_cnt_day\"]].groupby(\n    [\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n    {\"item_price\": \"mean\",\"item_cnt_day\": \"sum\", \"month\": \"min\"}).reset_index()\ntrain_dataset.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)\ntrain_dataset = pd.merge(train_dataset, items, on=\"item_id\", how=\"inner\")\ntrain_dataset = pd.merge(train_dataset, shops, on=\"shop_id\", how=\"inner\")\ntrain_dataset = pd.merge(train_dataset, item_cat, on=\"item_category_id\", how=\"inner\")","9d358442":"train_dataset.head(2)","70209259":"train_dataset.drop(['item_name', 'shop_name', 'city', 'item_category_name', 'cat_type', 'sub_cat_type'], axis = 1, inplace=True)","6c8b20e9":"train_dataset.head(1)","266de230":"test_dataset.head()","e9ba574b":"test_dataset.shape","2a3d7b32":"train_dataset.shape","2a4bcdbd":"train_dataset = train_dataset[train_dataset['shop_id'].isin(test_dataset['shop_id'].unique())]\ntrain_dataset = train_dataset[train_dataset['item_id'].isin(test_dataset['item_id'].unique())]","79a53898":"train_dataset.shape","428e2db6":"train_dataset.head(2)","76b13470":"final_train_dataset = train_dataset.copy()\nfinal_test_dataset = test_dataset.copy()","941c1b32":"def data_preprocess(sales_train, test=None):\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0),\n        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Adding new revenue column\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_month\"]\n    # Aggregate item_id \/ shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_month\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n    #print(sales_train_grouped)\n    # Merge the grouped data with the index\n    df = df.merge(\n        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df\n\ndataset_final = data_preprocess(final_train_dataset, final_test_dataset)","850f03f2":"dataset_final = pd.merge(dataset_final, items, on=\"item_id\", how=\"inner\")\ndataset_final = pd.merge(dataset_final, shops, on=\"shop_id\", how=\"inner\")\ndataset_final = pd.merge(dataset_final, item_cat, on=\"item_category_id\", how=\"inner\")\ndataset_final.head(3)","00d8d222":"dataset_final.drop(['item_name', 'shop_name', 'city', 'item_category_name', 'cat_type', 'sub_cat_type'], axis = 1, inplace=True)\ndataset_final.head(2)","af8abd81":"dataset_final.shape","c8f954b3":"def lag_feature(matrix, lag_feature, lags):\n    for lag in lags:\n        newname = lag_feature + f\"_lag_{lag}\"\n        print(f\"Adding feature {newname}\")\n        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n        targetseries[\"date_block_num\"] += lag\n        targetseries = targetseries.rename(columns={lag_feature: newname})\n        matrix = matrix.merge(\n            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n        )\n#     print(matrix)\n    return matrix\n\ndataset_final = lag_feature(dataset_final, 'item_cnt_month', lags=[1,2,3])\ndataset_final = lag_feature(dataset_final, 'item_revenue_month', lags=[1])\nprint(\"Lag features created..\")\nprint(dataset_final.columns)","c1385355":"dataset_final.fillna(0, inplace= True)\ndataset_final.head(2)","7abc93a3":"matrix = dataset_final[dataset_final.date_block_num>=12] \nmatrix.reset_index(drop=True, inplace=True)","d5211908":"matrix.head(2)","04426477":"matrix.columns","652d0925":"# # final_train_df = train_dataset[['date_block_num','item_id','shop_id','item_cnt_month']]\n# final_train_df = train_dataset.copy()\n# final_train_df = pd.concat([final_train_df, test_copy[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n# # final_train_df = final_train_df.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_month'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')\n# # final_train_df.reset_index(inplace = True)\n# final_train_df = final_train_df.pivot_table(index=['item_id','shop_id'], columns = 'date_block_num', values = 'item_cnt_month', fill_value = 0).reset_index()\n\n# # final_train_df = pd.merge(test_dataset,final_train_df,on = ['item_id','shop_id'],how = 'left')\n# # final_train_df.fillna(0,inplace = True)\n# final_train_df","05300866":"def fit_booster(\n    X_train,\n    y_train,\n    X_test=None,\n    y_test=None,\n    params=None,\n    test_run=False,\n    categoricals=[],\n    dropcols=[],\n    early_stopping=True,\n):\n    if params is None:\n        params = {\"learning_rate\": 0.1, \"subsample_for_bin\": 300000, \"n_estimators\": 50}\n\n    early_stopping_rounds = None\n    if early_stopping == True:\n        early_stopping_rounds = 50\n\n    if test_run:\n        eval_set = [(X_train, y_train)]\n    else:\n        eval_set = [(X_train, y_train), (X_test, y_test)]\n\n    booster = lgbm.LGBMRegressor(**params)\n\n    categoricals = [c for c in categoricals if c in X_train.columns]\n\n    booster.fit(\n        X_train,\n        y_train,\n        eval_set=eval_set,\n        eval_metric=[\"rmse\"],\n        verbose=100,\n        categorical_feature=categoricals,\n        early_stopping_rounds=early_stopping_rounds,\n    )\n\n    return booster\n\n\n\nkeep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 33\n# dropping this will reduce overfitting\ndropcols = [\n    \"shop_id\",\n    \"item_id\"\n] \nvalid = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain__ = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain__ = train__[train__.date_block_num >= keep_from_month]\nX_train = train__.drop(columns=\"item_cnt_month\")\ny_train = train__.item_cnt_month\nX_valid = valid.drop(columns=\"item_cnt_month\")\ny_valid = valid.item_cnt_month\n\n\n\nparams = {\n    \"num_leaves\": 966,\n    \"cat_smooth\": 45.01680827234465,\n    \"min_child_samples\": 27,\n    \"min_child_weight\": 0.021144950289224463,\n    \"max_bin\": 214,\n    \"learning_rate\": 0.01,\n    \"subsample_for_bin\": 300000,\n    \"min_data_in_bin\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.6,\n    \"subsample_freq\": 5,\n    \"n_estimators\": 8000,\n}\n\ncategoricals = [\n    \"item_category_id\",\n    \"month\",\n]  # These features will be set as categorical features by LightGBM and handled differently\n\nlgbooster = fit_booster(\n    X_train,\n    y_train,\n    X_valid,\n    y_valid,\n    params=params,\n    test_run=False,\n    categoricals=categoricals,\n)","6d0cf75d":"matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\nkeep_from_month = 2\ntest_month = 34\ntest__ = matrix.loc[matrix.date_block_num==test_month, :]\nX_test = test__.drop(columns=\"item_cnt_month\")\ny_test = test__.item_cnt_month\n\nX_test[\"item_cnt_month\"] = lgbooster.predict(X_test.drop(columns=dropcols)).clip(0, 20)","496044ba":"testing = test_dataset.merge(\n    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n    on=[\"shop_id\", \"item_id\"],\n    how=\"inner\",\n    copy=True,\n)\n# Verify that the indices of the submission match the original\nassert test_dataset.equals(testing[[\"ID\", \"shop_id\", \"item_id\"]])\ntesting[[\"ID\", \"item_cnt_month\"]].to_csv(\".\/submission.csv\", index=False)","df853451":"leaving behind the sale record of year 2013","e47315bf":"By judging from the above  outlier diagram above we see a price point further than the other points. So we can get rid of that point.\n\nAlso for `item_cnt_day` there is a point further than other point we will get rid of that point too.\n\nThe demonstration has been shown below:\n","976c1484":"checking the number of unique snop names and item category names","86114a39":"### Below we plot the outliers\n\n[this](https:\/\/www.kaggle.com\/homiarafarhana\/predict-future-sales#Exploratory-Data-Analysis) kernel has helped me understand this concept.","67342d63":"### <a name=\"soldpmonth\"><\/a>Checking how many items sold per per month i.e. (jan 2013 ~ Oct 2015)","85a13ac8":"we can see that item numbers in test and train sets are not equal. So making prediction for the missing items is going to be difficult.","9a9944aa":"Making a copy of the `train_dataset`, It is a good thing to so because we do not want to mess up the original content while exploring the data.","6ab5daa7":"plotting the monthly sales","f5a875dd":"# <a name=\"eda\"><\/a>EDA | Exploratory Data Analysis\n\n- ID - an Id that represents a (Shop, Item) tuple within the test set\n- shop_id - unique identifier of a shop\n- item_id - unique identifier of a product\n- item_category_id - unique identifier of item category\n- item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n- item_price - current price of an item\n- date - date in format dd\/mm\/yyyy\n- date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n- item_name - name of item\n- shop_name - name of shop\n- item_category_name - name of item category","9bf5656d":"from the description above, when we look at the max and min values we can see that there is an outlier for `item_price` and `item_cnt_day`","49fcad89":"# <a name=\"submission\"><\/a>Submission","405ed458":"extracting the city names","c604e720":"Adding the lag feature for column names `item_cnt_month` and `item_revenue_month`","de20c56b":"##### <a name=\"month\"><\/a>Monthly sales","766e92ce":"Lets find out which `item_ids` are in test_set but not in train_set","6f9b22ec":"# <a name=\"model\"><\/a>Model Creation\n\n[Reference kernel](https:\/\/github.com\/angliu-bu\/Kaggle-Predict-Future-Sales\/blob\/main\/predict-future-sales.ipynb)","b97f1a43":"# <a name=\"final\"><\/a>Preparing our final DF\n\nWe will also prepare our `train` and `test` datasets.\n\nNow we will split `train_dataset` into `train_set` and `validation_set`.\n","2f103569":"# <a name=\"imports\"><\/a>Imports\n","b43d49a3":"lets take a look at our `test_dataset`","e7d06c21":"Lets drop the following columns since it may not be necessary while building models","76c93fbb":"### <a name=\"correlation\"><\/a>checking for correlation\n\nthere are no noticeably strong pos\/neg correlation in sight ","1bee4d1e":"# <a name=\"datap\"><\/a>Data Preprocessing","446ee1b9":"\ud83e\udd17 \n\nAfter assigning the median value we can no longer find any record with negative pricing.","ecb25c42":"This is a second attempt. \n\nIn my [previous](https:\/\/www.kaggle.com\/esratmaria\/future-sales-prediction-eda#Model-Creation) kernel I predicted sale for **Jan 2013**.\nIn this kernel, My goal is to predict the sale for **Nov 2015**.\n\nEverything is pretty much same as the last one. I just tried to make a different version of the LGBM model and introduced few lag featured.\n\nI am documenting my process with a hope to get a better score which has been a bit difficult for me. Any suggestion would be appreciated.\n\n--------------------------------------------------------------------------------------------------","cac6918e":"In this competition, we are given sale information spanning over the year 2013 to 2015. Our goal is to predict the future sales.\n\nWe have data from Jan 2013 to Oct 2015, We will predict the sale for Nov 2015 by analysing the given data.\n\nThe kernels that helped me and I took inpirations from:\n\n- [kernel 1](https:\/\/www.kaggle.com\/sanjayar\/step-by-step-guide-for-sales-data-prediction-lstm)\n- [kernel 2](https:\/\/github.com\/sharmaroshan\/Predict-Future-Sales\/blob\/master\/Predicting_Future_Sales.ipynb)\n- [kernel 3](https:\/\/www.kaggle.com\/homiarafarhana\/predict-future-sales)\n- [kernel 4](https:\/\/www.kaggle.com\/stefanschulmeister87\/extensive-eda-and-data-preparation)\n\n\nContents:\n\n- [Imports](#imports)\n    - [data description](#data-desc)\n- [EDA](#eda)\n    - [checking for nulls](#nulls)\n    - [monthly sale](#month)\n    - [correlation](#correlation)\n    - [item sold per category](#soldpcat)\n    - [item sold per month](#soldpmonth)\n    - [wordcloud](#wordcloud)\n    - [busiest days\/months\/years for the shops](#busy)\n    - [dealing with outliers](#outlier)\n- [Data Processing](#data)\n    - [Checking for missing columns](#miss)\n    - [processing shop data](#shopp)\n    - [processing item category data](#catdatap)\n- [Preparing final DF for modeling](#final)\n- [Model Creation](#model)\n- [Prediction](#prediction)\n- [Submission](#submission)","9e904ace":"##### <a name=\"miss\"><\/a>checking to see if all `shop_id` and `item_id` from `test dataset` is also present in the `train dataset`","92aa5ab4":"#### <a name=\"wordcloud\"><\/a>WordCloud for shop name","2bf1e6ed":"no < 0 `item_cnt_day` value remaining","8cfd5027":"### <a name=\"outlier\"><\/a>Outliers","0f6a138c":"shape reduced","7901a310":"We can also see from the (2nd outlier)`item_cnt_day` diagram that there are some negative values. ","2f4ff45e":"we can see that in Feb of 2013 `shop_id` `31` has the highest number of sales","aec85f4a":"**2169** pieces of item ID **1173** were sold on **28\/10\/2015**","ee1a81ec":"##### <a name=\"data-desc\"><\/a>Provided data description\n\n- sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n- sample_submission.csv - a sample submission file in the correct format.\n- items.csv - supplemental information about the items\/products.\n- item_categories.csv  - supplemental information about the items categories.\n- shops.csv- supplemental information about the shops.","b5fd8050":"By looking at the plot above we can say that the sale is decreasing over months. However, some peaks are spotted during November.","c30cdb12":"Since we want to predict the sale for Nov, 2015 i.e. `date_block_num = 34` so we will have to add that to our `test_dataset` to be able to make sale prediciton.","8b777cf0":"creating a column`split` after `item_category_name` at '-'","d4cc4ac9":"Also, from the diagram we can see that there is a price point that is less than zero. We will fill that price with median value.\n","004544c4":"lets add few more features to our shop dataset like below:\n\n``` \n\"num_products\"\n\"min_price\"\n\"max_price\"\n\"mean_price\"\n```","516348c3":"converting the date into datetimelike format\n\ni.e. 01.02.2013    ==>    2013-02-01","e1cb95e3":"wordcloud for item categories","00120242":"-----------------------------------------------------------------------------------------------\n","a2d5903a":"### <a name=\"catdatap\"><\/a>Processing Item Category data\nThe Item category name is designed like below:\n\n\n- Item category name = type of the category + sub types\n\nfor example: an item category name **\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 - \u0411\u0438\u043b\u0435\u0442\u044b\t** is tranlated as **Service - Tickets**\n\nwhere the `type` of this category is **Service** and `subtype` is **Tickets** (what kind of service)..\n\nwe will now add these new features to our dataset","ee6784fd":"# <a name=\"prediction\"><\/a>Prediction","f57120a9":"with `city_code` we are assigning a unique label to each `city`","809baed3":"As shown above that not all the shop and item IDs from `test` are present in our `train_dataset` so lets only keep the IDs that are present in the `test_dataset`","620ce537":"`item_cnt_day` < 0 or -1 probably means that those items were returned. If the Items are returned then there are no sales involved as well. So, we can get rid of negative values and set it as 0.","bff07f52":"##### <a name=\"nulls\"><\/a>there are no nulls.","49b75f13":"#### <a name=\"busy\"><\/a>Busiest days for the shop","ea03e2df":"363 items are not found in `train_dataset` so predicting sales for these items is not easy since we do not have the prices for these items.","d5cc5823":"Busiest months and years for shops","072517bb":"### <a name=\"shopp\"><\/a>Processing shop data\n\nLets Look at all the Shops now. Every shop_name is designed like this:\nshop_name = city + kind of shop.\n\nWe attempt to extract the city feature out of the shop_name to add more diversity to our dataset.\n\nThe first two row shows that a city name is starting with '!', so we will get rid of the '!'.\n\nIndex 46 gives us a shop_name as **\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"** \n\nIDK Russian but analyzing different kernels it seem like the city name is actually **\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434** not **\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434**, there is an extra \" \". So we will get rid of that too.","e11c6e21":"### <a name=\"soldpcat\"><\/a>Checking how many items sold per category","f4770e76":"No `item_price` less than 0 remaining","6231d015":"Note that the list of shops and products slightly changes every month.","a9fa4176":"First lets create `test` and `train` dataset copies"}}