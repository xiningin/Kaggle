{"cell_type":{"24a9d809":"code","166ea205":"code","cb33560e":"code","e10c03d7":"code","5752e3a3":"code","cb984b88":"code","ce55cf10":"code","2776b56b":"code","eb662988":"markdown"},"source":{"24a9d809":"# Short version of forest cover type NN prediction\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom tensorflow.python.data import Dataset\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format","166ea205":"# data loading to dataframe\ndata = pd.read_csv(\"..\/input\/covtype.csv\", sep=\",\")\ndata.head()","cb33560e":"# splitting the data\nfrom sklearn.model_selection import train_test_split\nx=data[data.columns[:data.shape[1]-1]] # all columns except the last are x variables\ny=data[data.columns[data.shape[1]-1:]]-1 # the last column as y variable\nx_train, x_test, y_train, y_test = train_test_split(x, y , train_size = 0.5, random_state =  14)\n#this randomizes dataset and splits to train and test data","e10c03d7":"#batch_normalization\nfrom sklearn.preprocessing import StandardScaler\n\n# training\nnorm_tcolumns=x_train[x_train.columns[:10]] # only the first ten columns need normalization, the rest is binary\nscaler = StandardScaler().fit(norm_tcolumns.values)\nscaledf = scaler.transform(norm_tcolumns.values)\ntraining_examples = pd.DataFrame(scaledf, index=norm_tcolumns.index, columns=norm_tcolumns.columns) # scaledf is converted from array to dataframe\nx_train.update(training_examples)\n\n# validation\nnorm_vcolumns=x_test[x_test.columns[:10]]\nvscaled = scaler.transform(norm_vcolumns.values) # this scaler uses std and mean of training dataset\nvalidation_examples = pd.DataFrame(vscaled, index=norm_vcolumns.index, columns=norm_vcolumns.columns)\nx_test.update(validation_examples)","5752e3a3":"# model construction: forming the network layers using keras\n\nmodel = keras.Sequential([\n keras.layers.Dense(1024, activation=tf.nn.relu, input_shape=(x_train.shape[1],)), # neurons with relu activation, first layer with input \n keras.layers.Dropout(0.5), # dropout for reducing the overfitting problem\n keras.layers.Dense(512, activation=tf.nn.relu), # 2nd hidden layer\n keras.layers.Dropout(0.5),\n keras.layers.Dense(256, activation=tf.nn.relu), # 3rd hidden layer\n keras.layers.Dropout(0.5),\n keras.layers.Dense(7, activation=tf.nn.softmax)]) #  output layer with 7 categories\n\nmodel.compile(loss='sparse_categorical_crossentropy', #this loss method is useful for multiple categories, otherwise our model does not work\n optimizer=tf.train.AdamOptimizer(learning_rate=0.0043, beta1=0.9), metrics=['accuracy'])","cb984b88":"# train the model\nhistory1 = model.fit(x_train, y_train, epochs = 300, batch_size = 2048, verbose=0, validation_data = (x_test, y_test))","ce55cf10":"print('training acc.:',history1.history['acc'][-1],'\\n','test acc.:', (history1.history['val_acc'])[-1])","2776b56b":"# plot the accuracy history\nimport matplotlib.pyplot as plt\ndef plot_history(history1):\n plt.figure()\n plt.xlabel('Epoch')\n plt.ylabel('Accuracy %')\n plt.plot(history1.epoch, np.array(history1.history['acc']),\n label='Train Accuracy')\n plt.plot(history1.epoch, np.array(history1.history['val_acc']),\n label = 'Val Accuracy')\n plt.legend()\n plt.ylim([0.5, 1])\nplot_history(history1)","eb662988":"## Batch normalization has a great impact on the performance of the gradient descent. Even before trying out other optimization algorithms, one should always check if the dataset is properly prepared. After that, we can train our model. Adding dropouts is also useful in avoiding the overfitting problem. As a result, we see that the model has around 90% test accuracy, a huge improvement from our previous model."}}