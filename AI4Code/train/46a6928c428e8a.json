{"cell_type":{"dac0f672":"code","d7abf5df":"code","640af4a5":"code","3f9fe9a9":"code","a53ff01b":"code","94f96ad7":"code","824194d7":"code","07461632":"code","8b2b1848":"code","ce225ac2":"code","a8b8f9be":"code","89fc71e1":"code","5f55f34b":"code","310cc5cc":"code","601bde83":"code","964a23d6":"code","7f7a1f82":"code","a29f5fdc":"code","33d1d74c":"code","3cb543e6":"code","13f83c72":"code","6beb291a":"code","aa12a34c":"code","4712cf2e":"code","61da1b09":"code","b3e6e216":"markdown","b68230bf":"markdown","293c3632":"markdown","5a951c32":"markdown","6c85c524":"markdown","c24d297c":"markdown","5b4d0dfb":"markdown","487455dd":"markdown","d210acb7":"markdown","106acedb":"markdown","4efd523d":"markdown","a2fb1176":"markdown","541fee89":"markdown","2e0dce7d":"markdown","462f1c16":"markdown","88ea7edc":"markdown","af3f52c8":"markdown","449bfcc4":"markdown","ef4e1227":"markdown","08e07de6":"markdown"},"source":{"dac0f672":"import pandas as pd\nimport seaborn as sns\nimport re, nltk\nnltk.download('punkt')\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom matplotlib import pyplot\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport string\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.metrics import f1_score\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lime import lime_tabular\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nfrom tensorflow.keras.layers import Dense, Dropout\n\n","d7abf5df":"df =pd.read_csv(r'..\/input\/twitter-airline-sentiment\/Tweets.csv')\ndf.head()","640af4a5":"# Unique values of sentiment\ndf['airline_sentiment'].unique()","3f9fe9a9":"# Unique values of sentiment plot\n\nax = sns.countplot(x=\"airline_sentiment\", data=df)","a53ff01b":"# Unique values of airline\n\nplt.figure(figsize=(10,10))\nax = sns.countplot(x=\"airline\", data=df)","94f96ad7":"# I am tokenizing the tweet and also taking tokens from second index onwards as initital to gives airline name and '@' and lowering thm and later making it back a sentence\ndef clean_the_tweet(text):\n  tokens= nltk.word_tokenize(re.sub(\"[^a-zA-Z]\", \" \",text))\n  tokens = [token.lower() for token in tokens]\n  return ' '.join(tokens[2:])\n\n                 \n\ndef text_process(msg):\n  nopunc =[char for char in msg if char not in string.punctuation]\n  nopunc=''.join(nopunc)\n  return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n\n\n \ndef check_scores(clf,X_train, X_test, y_train, y_test):\n\n  model=clf.fit(X_train, y_train)\n  predicted_class=model.predict(X_test)\n  predicted_class_train=model.predict(X_train)\n  test_probs = model.predict_proba(X_test)\n  test_probs = test_probs[:, 1]\n  yhat = model.predict(X_test)\n  lr_precision, lr_recall, _ = precision_recall_curve(y_test, test_probs)\n  lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n\n\n  print('Train confusion matrix is: ',)\n  print(confusion_matrix(y_train, predicted_class_train))\n\n  print()\n  print('Test confusion matrix is: ')\n  print(confusion_matrix(y_test, predicted_class))\n  print()\n  print(classification_report(y_test,predicted_class)) \n  print() \n  train_accuracy = accuracy_score(y_train,predicted_class_train)\n  test_accuracy = accuracy_score(y_test,predicted_class)\n\n  print(\"Train accuracy score: \", train_accuracy)\n  print(\"Test accuracy score: \",test_accuracy )\n  print()\n  train_auc = roc_auc_score(y_train, clf.predict_proba(X_train)[:,1])\n  test_auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n\n  print(\"Train ROC-AUC score: \", train_auc)\n  print(\"Test ROC-AUC score: \", test_auc)\n  fig, (ax1, ax2) = plt.subplots(1, 2)\n\n  ax1.plot(lr_recall, lr_precision)\n  ax1.set(xlabel=\"Recall\", ylabel=\"Precision\")\n\n  plt.subplots_adjust(left=0.5,\n                    bottom=0.1, \n                    right=1.5, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\n  print()\n  print('Are under Precision-Recall curve:', lr_f1)\n  \n  fpr, tpr, _ = roc_curve(y_test, test_probs)\n\n\n  ax2.plot(fpr, tpr)\n  ax2.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n\n  print(\"Area under ROC-AUC:\", lr_auc)\n  return train_accuracy, test_accuracy, train_auc, test_auc\n\n\n\ndef grid_search(model, parameters, X_train, Y_train):\n  #Doing a grid\n  grid = GridSearchCV(estimator=model,\n                       param_grid = parameters,\n                       cv = 2, verbose=2, scoring='roc_auc')\n  #Fitting the grid \n  grid.fit(X_train,Y_train)\n  print()\n  print()\n  # Best model found using grid search\n  optimal_model = grid.best_estimator_\n  print('Best parameters are: ')\n  print( grid.best_params_)\n\n  return optimal_model\n  \n","824194d7":"# removing neutral tweets\n\ndf = df[df['airline_sentiment']!='neutral']\ndf['cleaned_tweet'] = df['text'].apply(clean_the_tweet)\n\ndf.head()\ndf['airline_sentiment'] = df['airline_sentiment'].apply(lambda x: 1 if x =='positive' else 0)\ndf.head()","07461632":"# Cleaning the tweets, removing punctuation marks\ndf['cleaned_tweet'] = df['cleaned_tweet'].apply(text_process)\ndf.reset_index(drop=True, inplace = True)\ndf.head()","8b2b1848":"df['airline_sentiment'].unique()","ce225ac2":"# Creating object of TF-IDF vectorizer\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\nX_tf_idf= vectorizer.fit_transform(df.cleaned_tweet)\nx_train, x_test, y_train, y_test = train_test_split(X_tf_idf, df['airline_sentiment'], random_state=42)\n","a8b8f9be":"\nSVM = svm.SVC( probability=True)\ns_train_accuracy, s_test_accuracy, s_train_auc, s_test_auc = check_scores(SVM,x_train, x_test, y_train, y_test)\n","89fc71e1":"# Tuning the hyperparameters\nparameters ={\n    \"C\":[0.1,1,10],\n    \"kernel\":['linear', 'rbf', 'sigmoid'],\n    \"gamma\":['scale', 'auto']\n}\n\n\n\nsvm_optimal = grid_search(svm.SVC(probability=True), parameters,x_train, y_train)","5f55f34b":"so_train_accuracy, so_test_accuracy, so_train_auc, so_test_auc = check_scores(svm_optimal,x_train, x_test, y_train, y_test)","310cc5cc":"m_train_accuracy, m_test_accuracy, m_train_auc, m_test_auc = check_scores(MultinomialNB(),x_train, x_test, y_train, y_test)\n","601bde83":"g_train_accuracy, g_test_accuracy, g_train_auc, g_test_auc=check_scores(GaussianNB(),x_train.toarray(), x_test.toarray(), y_train, y_test)\n","964a23d6":"a_train_accuracy, a_test_accuracy, a_train_auc, a_test_auc=check_scores(AdaBoostClassifier(),x_train,x_test, y_train, y_test)\n","7f7a1f82":"params = {'n_estimators': [10, 50, 100, 500],\n 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0],\n 'algorithm': ['SAMME', 'SAMME.R']}\n\nada_optimal_model = grid_search(AdaBoostClassifier(), params,x_train, y_train)","a29f5fdc":"ao_train_accuracy, ao_test_accuracy, ao_train_auc, ao_test_auc=check_scores(ada_optimal_model,x_train,x_test, y_train, y_test)\n","33d1d74c":"\n\nkfold = model_selection.KFold(n_splits = 3)\n  \n# bagging classifier\nmodel = BaggingClassifier(base_estimator = MultinomialNB(),\n                          n_estimators = 100)\n\nb_train_accuracy, b_test_accuracy, b_train_auc, b_test_auc= check_scores(model,x_train,x_test, y_train, y_test)\n","3cb543e6":"r_train_accuracy, r_test_accuracy, r_train_auc, r_test_auc= check_scores(RandomForestClassifier(random_state=0).fit(x_train, y_train), x_train,x_test,y_train,y_test)\n","13f83c72":"corpus = [df['cleaned_tweet'][i] for i in range( len(df))]\n\nvoc_size=5000\n\nonehot_=[one_hot(words,voc_size)for words in corpus] \n\nmax_sent_length=max([len(i) for i in corpus])\n\nembedded_docs=pad_sequences(onehot_,padding='pre',maxlen=max_sent_length)\n    \nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=max_sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nX_final=np.array(embedded_docs)\ny_final=np.array(df['airline_sentiment'])\nX_final.shape,y_final.shape\n","6beb291a":"X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\nmodel.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\n","aa12a34c":"y_test_pred=model.predict_classes(X_test)\ny_train_pred=model.predict_classes(X_train)\n","4712cf2e":"test_acc_lstm = accuracy_score(y_test,y_test_pred)\ntrain_acc_lstm = accuracy_score(y_train,y_train_pred)\ntest_roc_lstm = roc_auc_score(y_test,y_test_pred)\ntrain_roc_lstm = roc_auc_score(y_train,y_train_pred)\n","61da1b09":"data = [('Random Forest', r_train_accuracy, r_test_accuracy, r_train_auc, r_test_auc),\n ('MultinomialNB',m_train_accuracy, m_test_accuracy, m_train_auc, m_test_auc  ),\n('Bagged MultinomialNB',b_train_accuracy, b_test_accuracy, b_train_auc, b_test_auc ),\n ('AdaBoost',a_train_accuracy, a_test_accuracy, a_train_auc, a_test_auc ),\n('AdaBoost Optimized',ao_train_accuracy, ao_test_accuracy, ao_train_auc, ao_test_auc),\n('Gaussian Naive Bayes',g_train_accuracy, g_test_accuracy, g_train_auc, g_test_auc),\n('SVM', s_train_accuracy, s_test_accuracy, s_train_auc, s_test_auc),\n('SVM Optimized', so_train_accuracy, so_test_accuracy, so_train_auc, so_test_auc),\n('LSTM',train_acc_lstm, test_acc_lstm, train_roc_lstm, test_roc_lstm )]\n\n\nScores_ =pd.DataFrame(data = data, columns=['Model Name','Train Accuracy', 'Test Accuracy', 'Train ROC', 'Test ROC'])\nScores_.set_index('Model Name', inplace = True)\n\nScores_","b3e6e216":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Base SVM model with TF-IDF <\/h1>\n<\/div>\n","b68230bf":"> With increase in FPR, TPR also increases.\n\n> With increase in recall, precision decreases.","293c3632":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Using AdaBoost\n <\/h1>\n<\/div>\n","5a951c32":"> United has the most number of flights.\n\n> Virgin America has the least.","6c85c524":"> **base estimator here is: decision stump**","c24d297c":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Utility Functions <\/h1>\n<\/div>\n","5b4d0dfb":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Using Multinomial Naive Bayes <\/h1>\n<\/div>\n","487455dd":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Final Results\n <\/h1>\n<\/div>\n\n","d210acb7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Using Random Forest\n <\/h1>\n<\/div>\n\n","106acedb":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Using LSTM\n <\/h1>\n<\/div>\n\n","4efd523d":"> Positive and neutral tweets are almost equal.\n\n> Negative tweets are more than double of neutral or positive sentiments.","a2fb1176":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Importing Libraries <\/h1>\n<\/div>\n","541fee89":"> It is interesting to see in Naive Bayes, we are getting linear relationship.","2e0dce7d":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Text Preparation <\/h1>\n<\/div>\n","462f1c16":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Using Gaussian Naive Bayes\n <\/h1>\n<\/div>\n","88ea7edc":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Twitter Sentiment Analysis With SVM, Naive Bayes, Bagging, Random Forest, And LSTM  <\/h1>\n\n<\/div>\n<p><img src = \"https:\/\/www.vshsolutions.com\/wp-content\/uploads\/2019\/02\/airlines.jpeg\"  ><\/p>\n","af3f52c8":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> After optimizing the hyperparameters with TF-IDF <\/h1>\n<\/div>\n","449bfcc4":"\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Conclusion\n <\/h1>\n<\/div>\n\n**Most of the models are doing pretty well here.**","ef4e1227":"> With increase in recall, preciison decreases which makes sense also.\n\n> With increase in TPR, FPR inceases.","08e07de6":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:powderblue;\n           color:Black;\n           font-family:cursive\n           \">\n<h1 style=\"text-align:center;font-weight: bold\"> Bagging with MultinomialNB\n <\/h1>\n<\/div>\n"}}