{"cell_type":{"c262c52e":"code","fd7d8b88":"code","4b156d7c":"code","48276bea":"code","33cbe809":"code","6488acbd":"code","b1804e30":"code","35f13dff":"code","631326c5":"code","5c7c10fe":"code","90780d29":"code","de9d4d12":"code","47ef3854":"code","0fe39bef":"code","73c49fb5":"code","e7c0593b":"code","04244f2b":"code","e47c48b9":"code","0f2b0ac2":"code","697ef6f3":"code","591443e4":"code","2db7d9d7":"code","2cfdc5c3":"code","e435a968":"code","c8da7997":"code","d278983c":"code","c8e5d025":"code","d53a341e":"code","f998b85f":"code","54794247":"code","4f1bf40e":"code","f4fcfc8f":"code","3bb91079":"code","d47b0b1e":"code","6a7f8356":"code","7f0b3335":"code","22c6b5c1":"code","19087430":"markdown","828218ae":"markdown","8cacd044":"markdown","f33fe053":"markdown","1f315650":"markdown","8d3edafc":"markdown","b3bc8c2b":"markdown","aa64ecc0":"markdown","647154e2":"markdown","a3016b35":"markdown","d94cde9f":"markdown","903dc91f":"markdown"},"source":{"c262c52e":"# import libraries\n\n# basics libraries\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\n# vizualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# preprocessing libraries\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, LabelEncoder, StandardScaler, RobustScaler\n\n# model libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom xgboost.sklearn import XGBRegressor\n\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n# evaluation libraries\nfrom sklearn.metrics import accuracy_score","fd7d8b88":"# constants\nDROPOUT = 0.5\nEPOCHS = 10\nBATCH_SIZE = 50\nFOLDS = 10\n\nOPTIMIZER = tf.keras.optimizers.Adam()\nLOSS ='categorical_crossentropy'\nMETRICS = [\"accuracy\"]","4b156d7c":"# functions\ndef sales_duration(data):\n    data.date = pd.to_datetime(data.date)\n    number_of_days = data.date.max() - data.date.min()\n    number_of_years = number_of_days.days \/ 365\n    print(number_of_days.days, 'days')\n    print(number_of_years, 'years')\n    \ndef sales_per_day():\n    fig, ax = plt.subplots(figsize=(7,4))\n    plt.hist(train_df.num_sold, color='mediumblue')\n    \n    ax.set(xlabel = \"Sales Per day\",\n           ylabel = \"Count\",\n           title = \"Distribution of Sales Per Day\")\n\ndef daily_sales(data):\n    daily_data = data.copy()\n    daily_data.date = daily_data.date.apply(lambda x: str(x)[:-3])\n    daily_data = daily_data.groupby('date')['num_sold'].sum().reset_index()\n    daily_data.date = pd.to_datetime(daily_data.date)\n    return daily_data\n\ndef time_plot(data, x_col, y_col, title):\n    fig, ax = plt.subplots(figsize=(15,5))\n    sns.lineplot(x_col, y_col, data=data, ax=ax, color='mediumblue', label='Total Sales')\n    \n    second = data.groupby(data.date.dt.year)[y_col].mean().reset_index()\n    second.date = pd.to_datetime(second.date, format='%Y')\n    #sns.lineplot((second.date + datetime.timedelta(6*365\/12)), y_col, data=second, ax=ax, color='red', label='Mean Sales')   \n    \n    ax.set(xlabel = \"Date\",\n           ylabel = \"Sales\",\n           title = title)\n    \n    sns.despine()\n    \ndef get_diff(data):\n    data['sales_diff'] = data.num_sold.diff()\n    data = data.dropna()\n    \n    #data.to_csv('..\/data\/stationary_df.csv')\n    return data\n\ndef plot_results(results, original_df):\n\n    fig, ax = plt.subplots(figsize=(15,5))\n    sns.lineplot(original_df.date, original_df.num_sold, data=original_df, ax=ax, \n                 label='Original', color='mediumblue')\n    sns.lineplot(results.date, results.num_sold, data=results, ax=ax, \n                 label='Predicted', color='Red')\n    \n    ax.set(xlabel = \"Date\",\n           ylabel = \"Sales\")","48276bea":"# import dataset\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\")\n\ndataset_df = [train_df,test_df]\n\nsample_sub_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")","33cbe809":"# check for missing values\ntrain_df.isnull().sum()","6488acbd":"test_df.isnull().sum()","b1804e30":"#  \"row_id\" is not useful\ndel train_df[\"row_id\"]\ndel test_df[\"row_id\"]","35f13dff":"# do we have equal data for all stores ? for all countries ?\nprint(train_df[\"store\"].value_counts())\nsns.catplot(x=\"store\", kind=\"count\", palette=\"ch:.25\", data=train_df)","631326c5":"print(train_df[\"country\"].value_counts())\nprint(train_df[\"product\"].value_counts())","5c7c10fe":"# it seems like KaggleRama is selling more in all countries\nprint(train_df[['country','store','num_sold']].groupby(['country','store'], as_index=False).mean())","90780d29":"# the best seller is the Kaggle Hat\nprint(train_df[['country','product','num_sold']].groupby(['country','product'], as_index=False).mean())","de9d4d12":"# norway has the best results\nprint(train_df[['country','num_sold']].groupby(['country'], as_index=False).mean())","47ef3854":"# study of holidays -> https:\/\/www.kaggle.com\/drcapa\/holidays-finland-norway-sweden-20152019\nholi_df = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\")","0fe39bef":"holi_df.head()","73c49fb5":"holi_df[\"Date\"] = pd.to_datetime(holi_df[\"Date\"], format = '%Y-%m-%dT', errors = 'coerce')","e7c0593b":"# Duration of train_df -> from 2015 to 2018\nsales_duration(train_df)","04244f2b":"# Duration of test_df -> only 2019\nsales_duration(test_df)","e47c48b9":"# let's plot the sales for train_df\nsales_per_day()","0f2b0ac2":"train_daily_df = daily_sales(train_df)\ntrain_daily_df.head()","697ef6f3":"time_plot(train_daily_df, 'date', 'num_sold', 'Daily Sales')","591443e4":"\"\"\"\nThe 'date' feature needs to be modified\n\nMy first idea was erase the '-' in the 'date' feature and turn the object 'date' into an 'int' with the function below:\n\nfor dataset in dataset_df:\n    dataset[\"date\"] = dataset[\"date\"].str.replace(\"-\",\"\").astype(str).astype(int)\n    \nThen, I decided to use the 'to_datetime' function from pandas to transform 'date' from object to datetime type and then\nsplit it into several subfeatures to be able to chose what I want to put in my model (year, month, day ...)\n\"\"\"","2db7d9d7":"for dataset in dataset_df:\n    dataset[\"date\"] = pd.to_datetime(dataset[\"date\"], format = '%Y-%m-%dT', errors = 'coerce')","2cfdc5c3":"for dataset in dataset_df:\n    dataset[\"date_year\"] = dataset[\"date\"].dt.year\n    dataset[\"date_month\"] = dataset[\"date\"].dt.month\n    dataset[\"date_week\"] = dataset[\"date\"].dt.week\n    dataset[\"date_day\"] = dataset[\"date\"].dt.day\n    dataset[\"date_dayofweek\"] = dataset[\"date\"].dt.dayofweek\n    dataset[\"weekend\"] = 0","e435a968":"# fill the weekend column\nfor dataset in dataset_df:\n    i = 0\n    while i < len(dataset):\n        if dataset['date_dayofweek'][i] == 5 or dataset['date_dayofweek'][i] == 6 :\n            dataset['weekend'][i] = 1\n        i = i + 1","c8da7997":"train_df.head()","d278983c":"holi_df.head()","c8e5d025":"train_df.info()\ntrain_df.head()","d53a341e":"# mapping the categorical features ('country', 'store' and 'product')\nencoder = LabelEncoder()\n\nfor dataset in dataset_df:\n    dataset[\"country\"] = encoder.fit_transform(dataset[\"country\"])\n    dataset[\"store\"] = encoder.fit_transform(dataset[\"store\"])\n    dataset[\"product\"] = encoder.fit_transform(dataset[\"product\"])","f998b85f":"# we will not be using the 'date' feature on our model\n# same for 'date_year' feature because the model will train on values from 2015 tp 2018 and it will only\n# be tested on values from 2019\ndel train_df['date']\ndel train_df['date_year']\n\ncols = [\"country\", \"store\", \"product\", \"date_month\", \"date_week\", \"date_day\", \"date_dayofweek\", \"weekend\"]\n\n# Scaling\nscaler = StandardScaler()\n\nfor dataset in dataset_df:\n    dataset[cols] = scaler.fit_transform(dataset[cols])","54794247":"train_df.head()","4f1bf40e":"np.random.seed(713)","f4fcfc8f":"X_train = train_df.drop(\"num_sold\", axis=1).values\ny_train = train_df[\"num_sold\"].values","3bb91079":"X_test = test_df[cols].values","d47b0b1e":"model = RandomForestRegressor(n_estimators=100, max_depth=20)\n#model = XGBRegressor(n_estimators=100, learning_rate=0.2, objective='reg:squarederror')\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)","6a7f8356":"y_pred[:10]","7f0b3335":"# to submit\nresults = pd.Series(y_pred[:],name=\"num_sold\")\nsubmission = pd.concat([sample_sub_df[\"row_id\"], results],axis = 1)\n\nsubmission.to_csv(\"to_submit.csv\", index=False)","22c6b5c1":"# results observation\ntest_preds = pd.concat([test_df, results],axis = 1)\nsubmission_daily_df = daily_sales(test_preds)\n\nplot_results(submission_daily_df, train_daily_df)","19087430":"-------------------------------","828218ae":"# 2 - Data Observation","8cacd044":"# 0 - Introduction","f33fe053":"# 4 - Model and training","1f315650":"Hello all ! Thanks to the big kaggle community, I am learning a lot everyday, so if you have any comment on my work, do not hesitate !\nAlso, if you found this work interesting, please leave an upvote ! :)","8d3edafc":"About the data:\n\nThe train set has data from 2015 to 2018. \nThe test set has data only from 2019.\n\nThe main challenge for me was to find out how to deal with the 'date' feature. I find these two articles very useful for it:\n\n- https:\/\/towardsdatascience.com\/machine-learning-with-datetime-feature-engineering-predicting-healthcare-appointment-no-shows-5e4ca3a85f96\n- https:\/\/towardsdatascience.com\/5-machine-learning-techniques-for-sales-forecasting-598e4984b109\n\nDo not hesitate to check them out !","b3bc8c2b":"---------------------------------------","aa64ecc0":"# 3 - Data preprocessing","647154e2":"# 5 - Submission and result observation","a3016b35":"# 1 - Data analysis","d94cde9f":"It seems like our model is not that bad. Of course, there is still a lot of things to do but it is able to determine the peaks and the ups and downs.\n\nAreas of improvement:\n- train more models\n- more feature engineering on 'date' (e.g. holidays)\n- ...","903dc91f":"# 6 - Conclusion"}}