{"cell_type":{"fb62e501":"code","4baeee3a":"code","b9c3295e":"code","43b6720b":"code","d3491fa8":"code","439d22a0":"code","0b3cfbd3":"code","54a318bf":"code","a71f1ce1":"code","eaf9a887":"code","05ed83d4":"code","70e99faf":"code","df13c74c":"code","d0a2404c":"code","a3b81ab9":"code","a9b3cf78":"markdown","d67a22ed":"markdown","8540e71e":"markdown","4d195d86":"markdown","57968b39":"markdown","c1bc2e57":"markdown","8134601b":"markdown","508ba9ba":"markdown","c1ac68d9":"markdown","f70ea0f6":"markdown","a0aab7c9":"markdown","e7861ff4":"markdown","3c8193d9":"markdown","80a2ea8e":"markdown"},"source":{"fb62e501":"from tensorflow.contrib.layers import fully_connected\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,BatchNormalization,Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport cv2","4baeee3a":"def obtain_params(layer_no,csv):\n    activ ={'relu':tf.nn.relu,'tanh':tf.tanh,'sigm':tf.sigmoid,'None':None}\n    bool = {'TRUE':True,'FALSE':False,'None':None,'Auto':tf.AUTO_REUSE}\n    type,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout = csv.iloc[layer_no]\n    if filter!='None':filter=int(filter)\n    if kernel!='None':kernel=int(kernel)\n    if stride!='None':stride=int(stride)\n    if dropout!='None':dropout=float(dropout)\n    activation = activ[activation]\n    reuse=bool[reuse];is_train = bool[is_train]\n    return(type,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout)","b9c3295e":"def update_weights(parameters):\n    a=[variable.name for variable in tf.trainable_variables()]\n    a=[a[i][:len(a[i])-2] for i in range(len(a))]\n    for i in range(len(a)):\n        ind = a[i].find('\/')\n        with tf.variable_scope(a[i][:ind],reuse=True):\n            var = tf.get_variable(a[i][ind+1:])\n        forge = tf.assign(var,best_weights)\n    return forge.eval(feed_dict={best_weights:parameters[i]})\n\ndef check_accuracy(new_val,prev_val,counter,lr,lr_dec_limit,best_param):\n    if new_val>prev_val:\n        prev_val = new_val\n        best_param = sess.run(params)\n        counter = 0\n    else:\n        counter+=1\n    if counter>lr_dec_limit:\n        lr=lr\/10\n        counter=0\n    return (prev_val,best_param,counter,lr)\n\ndef create_fig(output):\n    \"\"\"Create the figures of convolved feature maps from the output array\"\"\"\n    shape = output.shape[3]\n    factors = np.array(range(1,shape+1))\n    factors = factors[shape%factors==0]\n    n = len(factors)\n    if n%2!=0:\n        up=factors[np.int(n\/2)]\n        lo=factors[np.int(n\/2)]\n    else:\n        up=factors[int(n\/2)]\n        lo=factors[int(n\/2)-1]\n    for i in range(1,shape+1):\n        plt.subplot(up,lo,i)\n        plt.imshow(output[0,:,:,i-1],cmap=\"Greys\")\n    plt.show()\n    return()","43b6720b":"df_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\").iloc[:]\ny_train = df_train.label\nX_train = df_train.drop([\"label\"],axis=1)\ndf_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\").iloc[:]\nclass fetch_mnist:\n    def __init__(self,X=X_train,y=y_train,Xt=df_test):\n        self.X_train = X\/255\n        self.y_train = y\n        self.X_test = Xt\/255\n    def fetch_train_batch(self,n,cnn=False):\n        rand_indx = np.random.permutation(len(self.X_train))\n        self.X_train = self.X_train.iloc[rand_indx]\n        self.y_train = self.y_train.iloc[rand_indx]\n        if cnn==False:\n            return(self.X_train[:n],self.y_train[:n])\n        if cnn==True:\n            shape = np.r_[n,28,28,1]\n            data=self.X_train[:n]\n            X_out = np.zeros(shape)\n            X_out[:,:,:,0] = [np.array(data.iloc[i]).reshape(28,28) for i in range(data.shape[0])]\n            return(X_out,self.y_train[:n])\n    def fetch_test_batch(self,n,cnn=False):\n        if cnn==False:\n            return(self.X_test[:n])\n        if cnn==True:\n            shape = np.r_[n,28,28,1]\n            data=self.X_test[:n]\n            X_out = np.zeros(shape)\n            X_out[:,:,:,0] = [np.array(data.iloc[i]).reshape(28,28) for i in range(data.shape[0])]\n            return(X_out)\n    def n_samples(self,tt):\n        if tt == \"train\":return(len(self.X_train))\n        if tt == \"test\":return(len(self.X_test))\n","d3491fa8":"batch_size = 200\nlr = 0.01\nn_iteration = 300\na=fetch_mnist()\nx,y = a.fetch_train_batch(batch_size,cnn=True)\nbatch_norm_momentum=0.75\ncsv = pd.read_csv(\"..\/input\/lenet5-parameters\/lenet5.csv\")","439d22a0":"tf.reset_default_graph()\nX=tf.placeholder(dtype=tf.float32,shape=(None,28,28,1),name=\"Features\")\nY = tf.placeholder(dtype=tf.int32,shape=(None),name=\"Labels\")\npadded_input = tf.pad(X, [[0, 0], [2, 2], [2, 2], [0, 0]], \"CONSTANT\")\nglobal_step = tf.Variable(0,dtype=tf.int32,trainable=False)\nlearning_rate = tf.train.exponential_decay(lr,global_step,100000,0.95)\ndata_gen = IDG(rotation_range=15)","0b3cfbd3":"################Convolution Layer 1\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(0,csv)\nconv1 = Conv2D(filters=filter,kernel_size=[kernel,kernel],strides=[stride,stride],padding=padding,activation=activation)\nout=conv1(padded_input)\n#################Pooling Layer 1\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(1,csv)\npool1 = MaxPool2D(pool_size=(kernel,kernel),strides=stride)\nout=pool1(out)\n#################Convolution Layer 2\nbatchnorm1=BatchNormalization(momentum=batch_norm_momentum)\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(2,csv)\nconv2 = Conv2D(filters=filter,kernel_size=[kernel,kernel],strides=[stride,stride],padding=padding,activation=activation)\nout=conv2(batchnorm1(out))\n#################Pooling Layer 1\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(3,csv)\npool2 = MaxPool2D(pool_size=(kernel,kernel),strides=stride)\nout=pool2(out)\n#################Convolution Layer 3\nbatchnorm2=BatchNormalization(momentum=batch_norm_momentum)\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(4,csv)\nconv3 = Conv2D(filters=filter,kernel_size=[kernel,kernel],strides=[stride,stride],padding=padding,activation=activation)\nout=conv3(batchnorm2(out))\n################Fully Connected Layer 1\nout = tf.reshape(out,(tf.shape(out)[0],out.get_shape()[3]))\nbatchnorm3=BatchNormalization(momentum=batch_norm_momentum)\ndrop1=Dropout(0.1)\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(5,csv)\nfc1 = Dense(units=kernel,activation=activation)\nout = fc1(drop1(batchnorm3(out)))\n################Fully Connected Layer 2\nbatchnorm4=BatchNormalization(momentum=batch_norm_momentum)\ndrop2=Dropout(0.2)\ntype,filter,kernel,stride,padding,activation,scope,reuse,is_train,dropout=obtain_params(6,csv)\nfc2 = Dense(units=kernel,activation=None)\nout = fc2(drop2(batchnorm4(out)))","54a318bf":"xentry=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,logits=out)\nloss = tf.reduce_sum(xentry,axis=0)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(loss,global_step=global_step)","a71f1ce1":"correct=tf.nn.in_top_k(out,Y,1)\nal_in = tf.reduce_mean(tf.cast(correct,tf.float32))\nparams = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='')\nsaver = tf.train.Saver()\ninit = tf.global_variables_initializer()","eaf9a887":"with tf.Session() as sess:\n    acc_val=np.empty((1))\n    sess.run(init)\n    X_data,y_data = a.fetch_train_batch(a.n_samples('train'),cnn=True)\n    for X_batch,y_batch in data_gen.flow(X_data,y_data,batch_size=batch_size):\n        sess.run(training_op,feed_dict={X:X_batch,Y:y_batch})\n        output=al_in.eval(feed_dict={X:X_batch,Y:y_batch})\n        acc_val=np.append(acc_val,output)\n        if global_step.eval()%50==0:print(\"Iteration:\",global_step.eval(),\"\\taccuracy\",output)\n        if global_step.eval()==2000:break\n# Obtaining prediction for all the samples\n    X_batch,y_val = a.fetch_train_batch(a.n_samples(\"train\"),cnn=True)\n    y_pred = np.argmax(out.eval(feed_dict={X:X_batch}),axis=1)\n    saver.save(sess,\"..\/working\/cnn_mnist_2.ckpt\")\n    sess.close()","05ed83d4":"acc_val = np.delete(acc_val,0)\nplt.figure(figsize=(20,10))\nplt.title(\"Accuracy v\/s Iterations\",fontdict={'fontsize':20})\nplt.grid(True)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Accuracy\")\nplt.plot(acc_val)\nplt.show()\n#Evaluating the confusion matrix\ncm = confusion_matrix(y_val, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = [str(i) for i in range(10)], \n                     columns = [str(i) for i in range(10)])\n\nplt.figure(figsize=(13,10))\nsns.heatmap(cm_df, annot=True)\nplt.title(\"Confusion Matrix\",fontdict={'fontsize':20})\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Actual Values\")\nplt.show()","70e99faf":"out_1 = conv1(padded_input)\nwith tf.Session() as sess:\n    saver.restore(sess,\"..\/working\/cnn_mnist_2.ckpt\")\n    X_test=a.fetch_test_batch(1,cnn=True)\n    output = out_1.eval(feed_dict={X:X_test})\n    sess.close()\nplt.imshow(X_test[0,:,:,0],cmap=\"Greys\")\nplt.title(\"Original Figure\",fontdict={'fontsize':20})","df13c74c":"fig = plt.figure(figsize=(10,10))\nplt.suptitle(\"Filter's effect from Convolution layer 1\",fontsize=20)\ncreate_fig(output)","d0a2404c":"out_2 = conv3(conv2(pool1(conv1(padded_input))))\nwith tf.Session() as sess:\n    saver.restore(sess,\"..\/working\/cnn_mnist_2.ckpt\")\n    X_test=a.fetch_test_batch(1,cnn=True)\n    output = out_2.eval(feed_dict={X:X_test})\n    sess.close()\nfig = plt.figure(figsize=(10,10))\nplt.suptitle(\"Filter's effect from Last Convolution layer\",fontsize=20)\ncreate_fig(output)","a3b81ab9":"with tf.Session() as sess:\n    saver.restore(sess,\"..\/working\/cnn_mnist_2.ckpt\")\n    X_test= a.fetch_test_batch(28000,cnn=True)\n    acc_test = out.eval(feed_dict={X:X_test})\n    out_put=np.argmax(acc_test,axis=1)\n    sess.close()\ndata = pd.DataFrame({'ImageId':range(1,len(X_test)+1),'Label':out_put})\ndata.to_csv(\"..\/working\/cnn_mnist_1.csv\",index=False)\nprint(data.head())","a9b3cf78":"#### Evaluation\nThe following contruct would use the list of size 10 obtained from the last **FC**(fully connected) layer to evaluate the accuracy. The highest value in the list would be passed on as the predicted value. The index corresponding to that value would be the predicted number. \nOften in Classification modules, we utilize the softmax function to restrict the output value between 0-1. Afterwards, cross-entroy cost functions follows, which gives out a high value if the prediction is wrong, (i.e. highest probability is assigned to the wrong number). The **tf.nn.sparse_softmax_cross_entropy_with_logits** functions applies softmax function, and later evaluated the cross-entropy of the output.","d67a22ed":"And Finally the output file","8540e71e":"# Lenet5 Architecture\nThis represents the simplest approach to implement the said topology. Most of the api are from keras. The parameters of each layer is stored in the imported .csv file. Other functions are used as well for creating figure, importing dataset and implementing methods e.g. early stop, cross-validation and learning rate scheduling.\n***\n#### EDIT #1 : Added exponential learning rate scheduling, batchnorm, dropout & confusion matrix!\n#### EDIT #2 : Added Data Augmentation using keras library (ImageDataGeneration)\nI am only adding a random rotation of (-20,20) degree range, since that is the onyl variation I expect in the generalization set. \n### Importing preliminary libraries","4d195d86":"#### Accuracy Measurements\nThe last important step in the construction phase is to specify how to mesure the model. We will simply use accuracy as our performance measure. First, for each instance, determine if the neural network\u2019s prediction is correct by checking whether or not the highest logit corresponds to the target class. For this you can use the **in_top_k()** function. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the network\u2019s overall accuracy.","57968b39":"### Declaring Constants","c1bc2e57":"Going Deeper, let us try the last Convolution Layer","8134601b":"The *update_params* function is used to update the weights obtained from the early stop process implementation. In future,I would work on implementing the **tf.keras.callbacks.EarlyStopping** to use the said method instead. \nAnother function implements learning rate scheduling based on the performace of the algorithm. Following function implements the creation of figure based on the size of output array obtained.","508ba9ba":"#### Creating Layers\nLayers, (i.e. convolution or pooling) are created below. The data is imported from the **lenet5.csv** file. Each of the layer's parameter are stored in rows. Use **csv.head()** command to briefly look at the file.\n#### EDIT #1:Batch normalization implemented also!!","c1ac68d9":"### Execution \nExecution of the above contructed graph.\n#### Edit #1: Inlcuding the operation to evaluate the confusion matrix. ","f70ea0f6":"Class fetch_mnist is used to obtain the data form the dataset","a0aab7c9":"The graph is observed to fluctuate a lot, but the ordinate dictates that the division (the band) is about **0.86**. This division is quite small, relative to the accruacy values obtained. But anyway, we would attempt to:-\n* Decrease the flucations (**using early stop**) \n* Increase the convergence rate (**using learning rate scheduling**)\n* Reduce the steady state error(**using cross-validation**)\n\nFurthermore techniques e.g. batch normalization and dropout would be experimented with, as I am quite doubfull about them influencing the above three factors, specially Data-augmentation. Most user suggest that it helps in CNN, but I dont know, lets see. At about 65th iteration, the values could use a little less learning rate, so we would like to implement one which takes a major effect near that value.\n#### Edit #1: Confusion matrix\nThe following are the values which the algorithm gets confused with,\n1. 9 predicted as 4\n2. 0 predicted as 6\n3. 7 predicted as 2\n\n(What a dumb computer!)\nNow that we know which samples are contributing to most of the error, **we can train the network which majorly constitutes of these values**, or we can first use these smaller and easier accuracy improving tools and observe the improvements. Let us do the latter.","e7861ff4":"I did not want to create each layer manually. Instead, The data is imported from the **lenet5.csv** file. Each of the layer's parameter are stored in rows of the stated file. Use **csv.head()** command to briefly look at the file. Since the file contained a mixed dtype, some of the values reuqired conversion to int, float and so on upon being called.","3c8193d9":"### Graph Construction\nAs stated earlier, the build would still be stuck with traditional tensorflow construction->execution style. In what follows, different variables and placeholders are declared. \n#### Edit#1: Implementing tf.train.exponential_decay as the learning rate schedule","80a2ea8e":"#### Recording \nWe would also contain the values of accuracy to plot and observe the rate of convergence. In future implementation with different techniques, it would enlight us, in what gives the best result.\n#### Edit #1: Let us also draw the confusion matrix."}}