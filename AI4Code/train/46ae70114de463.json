{"cell_type":{"a160f268":"code","4eeaf7fe":"code","feeb8e33":"code","bc153e87":"code","f82f9467":"code","fab8787d":"code","70047355":"code","e907c87c":"code","52734689":"code","9e3f7b2c":"code","c4fd9fbe":"code","ae9b5ab5":"code","7c8082a8":"code","fea0263a":"code","f8817a0a":"code","806dbc15":"code","6653234d":"code","c68ebaf7":"code","19a58ca8":"code","8bac79e5":"code","c0f3f64d":"code","20e45281":"code","0d7cd892":"markdown","3a02a939":"markdown"},"source":{"a160f268":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn import preprocessing\nfrom sklearn.neural_network import MLPClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4eeaf7fe":"#Reading Testing and Training files from csv to pd\nmy_training_data = pd.read_csv('..\/input\/brighton-a-memorable-city\/training.csv')\nmy_testing_data = pd.read_csv('..\/input\/brighton-a-memorable-city\/testing.csv')\nmy_testing_IDs = my_testing_data['ID']\n","feeb8e33":"#create array of all the predictions from the training data set\ntrainingPredictions = []\nfor i in my_training_data['prediction']:\n    trainingPredictions.append(i)\n","bc153e87":"#get percentage of \"memorable\" and \"not memorable\" images\nAll = my_training_data.shape[0]\nmemorable = my_training_data[my_training_data['prediction'] == 1]\nnotMemorable = my_training_data[my_training_data['prediction'] == 0]\n\nx = len(memorable)\/All\ny = len(notMemorable)\/All\n\nprint('memorable :',x*100,'%')\nprint('not memorable :',y*100,'%')","f82f9467":"#create an array with the names of all the features\ngist = [\"GIST\"]\nfor i in range(1,512):\n    gist.append(\"GIST.\" + str(i))\ngist\n\n\ncnns = [\"CNNs\"]\nfor j in range(1,4096):\n    cnns.append(\"CNNs.\" + str(j))\ncnns\ngist\nallFeatures = cnns+gist","fab8787d":"#function that standardises the input data\ndef standardise(data):\n    standard = data.loc[:,allFeatures].values\n    standard = StandardScaler().fit_transform(standard)\n    return standard","70047355":"#function that scales the input data sets\ndef scale(data_train,data_test):\n    min_max_scaler = preprocessing.MinMaxScaler()\n    \n    scale_train = data_train.loc[:,allFeatures].values\n    scale_train =min_max_scaler.fit_transform(scale_train)\n    \n    scale_test = data_test.loc[:,allFeatures].values\n    scale_test = min_max_scaler.transform(scale_test)\n    return scale_train, scale_test","e907c87c":"#removes unnecessary columns in the data sets\nmy_training_data = my_training_data.drop('ID',axis=1)\nmy_training_data = my_training_data.drop('prediction',axis=1)\nmy_testing_data = my_testing_data.drop('ID',axis=1)\n","52734689":"#splits the data into validation, training sets (20%\/80% split)\nX_train,X_test,y_train,y_test = train_test_split(my_training_data,trainingPredictions,test_size=0.2,random_state = 42)","9e3f7b2c":"#standardises and power transforms the data\nfrom sklearn.preprocessing import PowerTransformer\npt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True)\npt.fit(X_train)\nst_train = pt.transform(X_train)\nst_test = pt.transform(X_test)\n\n\n\n","c4fd9fbe":"from sklearn.neural_network import MLPClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\ns = ['sgd','adam','lbfgs']\nscore=[]\nfor i in s:\n    clf = MLPClassifier(solver=i,activation= 'relu',learning_rate='adaptive',\n                    hidden_layer_sizes=(5), random_state=1, max_iter=10000)\n    clf.fit(st_train,  y_train)\n    score.append(clf.score(st_test, y_test))\n\n\nplt.bar(s, score)\nplt.xlabel('solver used')\nplt.ylabel('score')\nplt.title('Testing effect of Different Solvers on Classifier')","ae9b5ab5":"from sklearn.neural_network import MLPClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\ns1 = [(5),(5,5),(5,5,5)]\ns = [1,2,4,8,16,32,64,128]\n\nscore=[]\nfor i in s:\n    clf = MLPClassifier(solver='sgd',activation= 'relu',learning_rate='adaptive',\n                    hidden_layer_sizes=i, random_state=1, max_iter=10000)\n    clf.fit(st_train,  y_train)\n    score.append(clf.score(st_test, y_test))\n    \nplt.plot(s, score)\nplt.xlabel('number of neurons')\nplt.ylabel('score')\nplt.title('Testing effect of Different neuron numbers on Classifier')","7c8082a8":"from sklearn.neural_network import MLPClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\ns1 = [(5),(5,5),(5,5,5),(5,5,5,5),(5,5,5,5,5)]\n\nscore=[]\nfor i in s1:\n    clf = MLPClassifier(solver='sgd',activation= 'relu',learning_rate='adaptive',\n                    hidden_layer_sizes=i, random_state=1, max_iter=10000)\n    clf.fit(st_train,  y_train)\n    score.append(clf.score(st_test, y_test))\n    \nscore\ns = [\"1 Layer\",\"2 Layers\",\"3 Layers\",\"4 Layers\",\"5 Layers\"]\nplt.bar(s, score)\nplt.xlabel('number of hidden layers')\nplt.ylabel('score')\nplt.title('Testing effect of number of hidden neuron layers on Classifier')","fea0263a":"from sklearn.neural_network import MLPClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\ns = ['identity','logistic','tanh','relu']\n\nscore=[]\nfor i in s:\n    clf = MLPClassifier(solver='sgd',activation= i,learning_rate='adaptive',\n                    hidden_layer_sizes=(5), random_state=1, max_iter=10000)\n    clf.fit(st_train,  y_train)\n    score.append(clf.score(st_test, y_test))\n    \n\nplt.bar(s, score)\nplt.xlabel('activation functions')\nplt.ylabel('score')\nplt.title('Testing effect different activation functions have on classifier')","f8817a0a":"from sklearn.neural_network import MLPClassifier\n#Preprocessing the data - Power Transforming\ntheTransformer = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True)\ntheTransformer.fit(my_training_data)\nt_train = theTransformer.transform(my_training_data)\nt_actual_test = theTransformer.transform(my_testing_data)","806dbc15":"#Create Classifier with final hyperparameters\ntheClassifier = MLPClassifier(solver='sgd',activation= 'tanh',learning_rate='adaptive',\n                    hidden_layer_sizes=(5), random_state=1, max_iter=1000)","6653234d":"#train the classifier with the training data \ntheClassifier.fit(t_train,  trainingPredictions)\n","c68ebaf7":"#Predict the classes of the London testing data and save the results to a csv file.\naqs=theClassifier.predict(t_actual_test)\n\ndfx = pd.DataFrame(data = aqs\n             , columns = ['prediction'], )\ndfx = pd.concat([my_testing_IDs,dfx], axis = 1)\nprint(dfx)\ndfx.to_csv('submission.csv', index = False)","19a58ca8":"sns.countplot(dfx['prediction'])","8bac79e5":"#Old code\npca = PCA(0.80)\nstandard = standardise(my_training_data)\nprincipalComponents= pca.fit_transform(standard)\ns = standardise(my_testing_data)\ns = pca.transform(s)\n\nprincipalDf = pd.DataFrame(data = principalComponents\n             )\n#print(principalDf)\n\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(solver='liblinear', class_weight = {1: 0.3848,0:0.6152})\nlogisticRegr.fit(principalComponents,trainingPredictions)\n\n\nx= logisticRegr.predict(s)\ndfx = pd.DataFrame(data = x\n             , columns = ['prediction'], )\ndfx = pd.concat([my_testing_IDs,dfx], axis = 1)\nprint(dfx)\ndfx.to_csv('submission.csv', index = False)","c0f3f64d":"#Old code\nprincipalComponents\ntrainingPredictions","20e45281":"#Old code\npca = PCA(n_components=1)\nprincipalComponentsCnns= pca.fit_transform(standard_cnns )\nprincipalComponentsGist= pca.fit_transform(standard_gist)\nprincipalDfCnns = pd.DataFrame(data = principalComponentsCnns\n             , columns = ['principal component 2'])\nprincipalDfGist = pd.DataFrame(data = principalComponentsGist\n             , columns = ['principal component 1'])\nprincipalDf = pd.concat([principalDfGist, principalDfCnns], axis = 1)\nprint(principalDf)","0d7cd892":"**END**","3a02a939":"**FINAL CODE**"}}