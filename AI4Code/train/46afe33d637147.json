{"cell_type":{"95995fe7":"code","14f8c1cd":"code","69158136":"code","4eb10396":"code","f6dfdb9b":"code","9479e916":"code","262cc8f0":"code","98c5bbf9":"code","cb4254da":"code","502c6474":"code","539d56da":"markdown","6563421e":"markdown","6e858c49":"markdown","149e5fd4":"markdown","c593cc17":"markdown","167aa6eb":"markdown"},"source":{"95995fe7":"import numpy as np\nnp.random.seed(123)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","14f8c1cd":"pixels = pd.read_csv('..\/input\/toysets\/6.overlap.csv',  names=['x1', 'x2', 'y'])\nys = pixels[['y']]\nXs = pixels.drop(['y'], axis=1)\n\nprint(Xs.shape)\nprint(ys.shape)","69158136":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier","4eb10396":"# train, cv, test\nXs, X_test, ys, y_test = train_test_split(Xs, ys, test_size=0.3)\nX_tr, X_cv, y_tr, y_cv = train_test_split(Xs, ys, test_size=0.3)","f6dfdb9b":"hist = {\n    'ks': [],\n    'acc_cv': [],\n    'acc_tr': []\n}\n\nfor k in range(1,50,2):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # fitting cv train\n    knn.fit(X_tr, y_tr)\n    # predict and eval  cv train\n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    acc_cv = accuracy_score(y_cv, pred_cv, normalize=True) * float(100)\n    acc_tr  = accuracy_score(y_tr, pred_tr, normalize=True) * float(100)\n    print(f\"k:{k}\\t val-acc: {acc_cv} \\ttrain-acc: {acc_tr}\")\n    \n    # log\n    hist['ks'].append(k)    \n    hist['acc_cv'].append(acc_cv)    \n    hist['acc_tr'].append(acc_tr)","9479e916":"plt.figure(figsize=(20, 7))\n\nplt.plot(hist['ks'], hist['acc_cv'], label='cv')\nplt.plot(hist['ks'], hist['acc_tr'], label='train')\n\nfor k, acc_cv in zip(hist['ks'], hist['acc_cv']):\n    plt.text(k, acc_cv, f'k={k}')\n\nplt.legend()\nplt.show()","262cc8f0":"k = 31\n\nknn = KNeighborsClassifier(n_neighbors=k)\n# fitting cv train\nknn.fit(X_tr, y_tr)\n# predict and eval  cv train\npred_cv = knn.predict(X_cv)\nacc_cv = accuracy_score(y_cv, pred_cv, normalize=True) * float(100)\nprint(f\"k:{k}\\t val-acc: {acc_cv}\")","98c5bbf9":"from sklearn.model_selection import cross_val_score","cb4254da":"hist = {\n    'ks': [],\n    'acc_cv': []\n}\n\nfor k in range(0, 60):\n    knn = KNeighborsClassifier(n_neighbors=k) \n    scores = cross_val_score(knn, Xs, ys, cv=10) # 10-fold\n    # 95% conf-interval: scores.mean() (+\/- 2*scores.std())\n    \n    # log\n    hist['ks'].append(k)\n    hist['acc_cv'].append(scores.mean())","502c6474":"plt.figure(figsize=(20, 7))\n\nplt.plot(hist['ks'], hist['acc_cv'], label='cv')\n\nfor k, acc_cv in zip(hist['ks'], hist['acc_cv']):\n    plt.text(k, acc_cv, f'k={k}')\n\nplt.legend()\nplt.show()","539d56da":"# Simple Cross Validation","6563421e":"- `k=31` is indeed best \n- Note that as dataset was small, k-fold's plot is smoother","6e858c49":"# K-Fold Cross Validation","149e5fd4":"`k=31` looks good","c593cc17":"# Prepare Data","167aa6eb":"# Best K\n\n- Small k leads to overfitting and big k leads to undefitting\n- `k=sqrt(n)` considered to be best just a rule. \n- We can find best `k` using corss-validation \/ k-fold crossvalidation. Chose k-fold cross validation if dataset is significantly small"}}