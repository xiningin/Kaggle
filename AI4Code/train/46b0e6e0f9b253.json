{"cell_type":{"1e9a31a6":"code","24e0b1d6":"code","98f07409":"code","ff4857da":"code","32a498a9":"code","a8f4b644":"code","9485aef0":"code","0c659948":"code","50916845":"code","0eb411f4":"code","77f05674":"code","ac7ba2e6":"code","a76ee977":"code","099493fc":"code","b9fe7081":"code","156318ad":"code","ff6bcc1a":"code","daf2b9c5":"code","781b413b":"code","be80d93c":"code","7592ebc7":"code","f77b07a4":"code","04944f9e":"code","6a4ca7c8":"code","3625b55c":"code","e59ab3bf":"code","a6e5890a":"code","83051cff":"code","77135857":"code","4c26910a":"code","99c710fc":"code","1736f7c7":"code","771f1b0c":"code","2235496f":"markdown"},"source":{"1e9a31a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24e0b1d6":"# visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats","98f07409":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","ff4857da":"df.head()","32a498a9":"df = df.set_index(\"PassengerId\")\ntest = test.set_index(\"PassengerId\")","a8f4b644":"test.head()","9485aef0":"df.describe()","0c659948":"df.dtypes","50916845":"df.isnull().sum()","0eb411f4":"print(\"Number of duplicate rows:\", df[df.duplicated()].shape[0])","77f05674":"# map names to just last name. we will use this to try and impute missing values for cabin.\n# the way we will impute it is that people with the same last name are part of 1 family.\n# if any family members are missing a value for cabin we will put in another family members cabin.\n\ndf[\"Name\"] = df[\"Name\"].apply(lambda x: x.split()[0].replace(\",\", \"\"))","ac7ba2e6":"def update_cabin(row):\n    '''Used to first map missing NaN to family cabins. Then we map \n       each cabin to just the first letter.'''\n    \n    if pd.isnull(row[\"Cabin\"]):\n        # we have found a null value.\n        family_members = df[df[\"Name\"] == row[\"Name\"]]\n\n        family_without_na = (family_members[\"Cabin\"] == np.nan)\n        if len(family_without_na) == 0:\n            cabin = np.nan\n        else:\n            cabin = df.loc[family_without_na.index[0], \"Cabin\"]\n        \n        row[\"Cabin\"] = cabin\n    \n    # we will also map cabins to just the first letter e.g. C28 -> C, D21 -> D. nan will stay as nan.\n    if not pd.isnull(row[\"Cabin\"]):\n        row[\"Cabin\"] = row[\"Cabin\"][0]\n    \n    return row\n    \n        \ndf = df.apply(update_cabin, axis=1)\ntest = test.apply(update_cabin, axis=1)","a76ee977":"# Now we will try to map the Tickets column so that we can retain information from there.\ndef update_tickets(row):\n    \n    if pd.isnull(row):\n        # If it is NaN we dont do anything\n        return row\n    \n    # we split the string. if length > 1 it means there were multiple strings.\n    # the prefix is removed if is multiple strings.\n    strings = row.split()\n    if len(strings) > 1:\n        ticket_num = strings[-1]\n    else:\n        ticket_num = strings[0]\n    try:\n        # try to round the ticket number to nearest 100.\n        return round(int(ticket_num), -2)\n    except ValueError:\n        # the string was for some reason not an integer.\n        return np.nan\n    \n\ndf[\"Ticket\"] = df[\"Ticket\"].apply(update_tickets)\ntest[\"Ticket\"] = test[\"Ticket\"].apply(update_tickets)","099493fc":"# remove outliers\ndf = df[(np.abs(stats.zscore(df[[\"Fare\", \"Age\", \"Ticket\", \"Pclass\", \"SibSp\", \"Parch\"]])) < 3)]","b9fe7081":"df[\"Survived\"].value_counts().plot(kind=\"bar\")","156318ad":"sns.boxplot(x=df[\"Age\"])","ff6bcc1a":"sns.boxplot(x=df[\"Fare\"])","daf2b9c5":"df[\"Embarked\"].value_counts().plot(kind=\"bar\")\nplt.title(\"# of passengers by embarking port\")\nplt.ylabel(\"# of passengers\")\nplt.xlabel(\"Port\")","781b413b":"df[\"Pclass\"].value_counts().plot(kind=\"bar\")\nplt.title(\"# of passengers by class\")\nplt.ylabel(\"# of passengers\")\nplt.xlabel(\"Class\")","be80d93c":"df[\"SibSp\"].value_counts().plot(kind=\"bar\")\nplt.title(\"# of passengers by # of siblings\/spouses\")\nplt.ylabel(\"# of passengers\")\nplt.xlabel(\"# of siblings\/spouses\")","7592ebc7":"df[\"Parch\"].value_counts().plot(kind=\"bar\")\nplt.title(\"# of passengers by # of parents\/children\")\nplt.ylabel(\"# of passengers\")\nplt.xlabel(\"# of parents\/children\")","f77b07a4":"sns.boxplot(x=df[\"Ticket\"])","04944f9e":"df[\"Cabin\"].value_counts().plot(kind=\"bar\")\nplt.title(\"# of passengers by cabin\")\nplt.ylabel(\"# of passengers\")\nplt.xlabel(\"Cabin\")","6a4ca7c8":"c= df.corr()\nsns.heatmap(c,annot=True)\nc","3625b55c":"# Useful function to plot learning curves of various models.\ndef plot_learning_curve(title, train_sizes, train_scores, test_scores):\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    train_error_mean = [(1 - x) for x in train_scores_mean]\n    test_error_mean = [(1 - x) for x in test_scores_mean]\n    \n    fig = plt.figure()\n    fig.suptitle(title)\n    ax = fig.add_subplot()\n    ax.grid()\n    ax.plot(train_sizes, train_error_mean, 'o-', color='r', label=\"Training error\")\n    ax.plot(train_sizes, test_error_mean, 'o-', color='g', label=\"Test error\")\n    ax.legend(loc=\"best\")\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Error\")","e59ab3bf":"# we will do some basic modelling using xgboost classifier to check out what data is ideal.\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# intercept made no difference in this case.\n#df[\"Intercept\"] = np.ones(df.shape[0])\n#test[\"Intercept\"] = np.ones(test.shape[0])\n\ny = df[\"Survived\"]\nX = df.drop(\"Survived\", axis=1)\nX, y = shuffle(X, y)\n\nnumeric_cols = [col for col in X.columns if df[col].dtype in [\"int64\", \"float64\"]]\nlow_cardinality_cols = [col for col in X.columns if df[col].dtype == \"object\" and df[col].nunique() < 10]\n\ncols_to_retain = numeric_cols + low_cardinality_cols\nX_full = X[cols_to_retain].copy()\nX_test = test[cols_to_retain].copy()","a6e5890a":"X_full.isnull().sum()","83051cff":"categorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n])","77135857":"def fit_and_score_xgboost(X, y, preprocessor):\n    \n    xgb_pipeline = Pipeline(steps=[\n            (\"preprocessor\", preprocessor),\n            (\"model\", XGBClassifier(n_estimators=40, learning_rate=0.2, use_label_encoder=False, eval_metric=\"logloss\"))\n        ])\n    train_sizes, train_scores, test_scores = learning_curve(xgb_pipeline,\n                                                       X, y, cv=5)\n    plot_learning_curve(\"XGBoost Classifier\", train_sizes, train_scores, test_scores)","4c26910a":"# Using the heatmap lets try out using the smallest set of columns that would allow us to model.\n'''\nX_min = X_full[[\"Age\", \"Parch\", \"Fare\", \"Sex\"]]\ntest_min = X_test[[\"Age\", \"Parch\", \"Fare\", \"Sex\"]]\n\npreprocessor_min = ColumnTransformer(transformers=[\n    (\"num\", numeric_transformer, [\"Age\", \"Fare\", \"Parch\"]),\n    (\"cat\", categorical_transformer, [\"Sex\"])\n])\n\nfit_and_score_xgboost(X_min, y, preprocessor_min)'''","99c710fc":"# Now we try using all columns in X_full.\n\nX_full_2 = X_full.copy()\nX_test_2 = X_test.copy()\n\npreprocessor_full = ColumnTransformer(transformers=[\n    (\"num\", numeric_transformer, numeric_cols),\n    (\"cat\", categorical_transformer, low_cardinality_cols)\n])\n\nfit_and_score_xgboost(X_full_2, y, preprocessor_full)","1736f7c7":"# We try to create some extra features now.\n","771f1b0c":" xgb_pipeline = Pipeline(steps=[\n            (\"preprocessor\", preprocessor_full),\n            (\"model\", XGBClassifier(n_estimators=40, learning_rate=0.1, use_label_encoder=False, eval_metric=\"logloss\"))\n        ])\nxgb_pipeline.fit(X_full, y)\npredictions = xgb_pipeline.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\" : test.index,\n                           \"Survived\" : predictions})\nsubmission.to_csv(\"\/kaggle\/working\/titanic.csv\", index=False)","2235496f":"<h3>Results<\/h3>\n\n<h5>Using smallest set<\/h5>\n<p> Fixed C=0.1, estimators=40. <\/p>\n<p> Using mean for numeric and mode for categoric scores: 0.756. <\/p>\n<p> Using mean for numeric and mode for categoric along with no intercept column scores: 0.756. <\/p>\n<p> Not using intercept, mean for numeric and constant for categoric scores: 0.756.<\/p>\n<p> Removing outliers with using mean and mode: 0.746 <\/p>\n\n<h5> Using all cols except name and passengerid <\/h5>\n\n<p> Going forward C=0.1, estimators=40, numeric_transformer strategy = mean, categorical transformer strategy = mode, and we remove all outliers: 0.779<\/p>\n<p> Outliers not removed score: 0.779<\/p>"}}