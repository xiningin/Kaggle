{"cell_type":{"1c4b0fff":"code","1dfbd259":"code","9b5791a4":"code","737e1f8c":"code","6529b1b2":"code","095f0d9d":"code","1de17b64":"code","8c9502aa":"code","a1f53517":"code","4ce40563":"code","90dce17d":"code","d5c1b6c0":"code","4fbc1543":"code","c3cd1f6b":"code","14de7960":"code","178fbde5":"code","98eb07ef":"code","794c39dc":"code","d8fca953":"code","34710fee":"code","32716b3f":"code","79443f59":"code","f6f09441":"code","7366e8f2":"code","60f5df0a":"code","a98ee1f4":"code","c4a77c43":"code","5698edc4":"code","22b6391f":"code","2865bdf9":"code","7f0cd305":"code","1c697c70":"code","dca6622a":"code","b0c62035":"code","6a457734":"code","a9fbbc56":"code","b49fc362":"code","dccad240":"code","ec1c2425":"code","b2913229":"code","79a4ca67":"code","aa28bafa":"code","f7866e2d":"code","3a023d68":"code","df1a1efd":"code","d8cae9e3":"code","961ae8a8":"code","faeb711a":"code","198e6e4c":"code","6cc787d0":"code","7572a78c":"code","3f6c23b5":"code","f28a6f0d":"code","46688f1c":"markdown","7f932e01":"markdown","de763edc":"markdown","587d3807":"markdown","c1b366a9":"markdown","5a8725ec":"markdown","96f3090f":"markdown","96408f33":"markdown","e074b8bc":"markdown","d74713a4":"markdown","9ec869cf":"markdown","ff075916":"markdown"},"source":{"1c4b0fff":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport numpy as np","1dfbd259":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","9b5791a4":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel(\"samples\")\nplt.title(\"distribution\")","737e1f8c":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 1][\"text\"].str.len())\nplt.title(\"Disaster tweets length\")","6529b1b2":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 0][\"text\"].str.len(), color= 'r')\nplt.title(\"No disaster tweets length\")","095f0d9d":"plt.grid()\n\nword1 = train[train[\"target\"] == 1][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)))\nplt.title(\"Disaster tweets length\")","1de17b64":"plt.grid()\n\nword1 = train[train[\"target\"] == 0][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)), color = 'r')\nplt.title(\"Disaster tweets length\")","8c9502aa":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","a1f53517":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))","4ce40563":"corpus = create_corpus(0)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","90dce17d":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"top words 0\")","d5c1b6c0":"corpus = create_corpus(1)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","4fbc1543":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"top words 1\")","c3cd1f6b":"corpus = create_corpus(1)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1\n        \n        \n","14de7960":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"Punctuation disaster 1\")","178fbde5":"corpus = create_corpus(0)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1","98eb07ef":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"Punctuation disaster 0\")","794c39dc":"from collections import Counter","d8fca953":"counter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\n\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","34710fee":"plt.title(\"most common words\")\nplt.grid()\nsns.barplot(x = y, y = x)","32716b3f":"df = pd.concat([train, test])\ndf.shape","79443f59":"df","f6f09441":"import re","7366e8f2":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)","60f5df0a":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))","a98ee1f4":"df","c4a77c43":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)","5698edc4":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_html(x))","22b6391f":"df","2865bdf9":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)","7f0cd305":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))","1c697c70":"df","dca6622a":"def remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","b0c62035":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))","6a457734":"df","a9fbbc56":"!pip install pyspellchecker","b49fc362":"from spellchecker import SpellChecker","dccad240":"spell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    \n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","ec1c2425":"#df['text']=df['text'].apply(lambda x : correct_spellings(x))","b2913229":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')","79a4ca67":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df[\"text\"]):\n        words = [word.lower() for word in word_tokenize(tweet) if \\\n        ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n        \n    return corpus","aa28bafa":"corpus = create_corpus(df)","f7866e2d":"embedding_dict = {}\n\nwith open('\/content\/drive\/MyDrive\/Projects\/Natural Disaster Tweets\/glove.6B.100d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","3a023d68":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","df1a1efd":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\n\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,\n                          maxlen = MAX_LEN, \n                         truncating = 'post', \n                         padding = 'post')","d8cae9e3":"word_index = tokenizer_obj.word_index\nprint('number of unique words: ', len(word_index))","961ae8a8":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words,100))\n\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","faeb711a":"from keras import regularizers\n\nmodel = Sequential()\n\nglove_embedding = Embedding(num_words, 100, embeddings_initializer = Constant(embedding_matrix), \n                     input_length = MAX_LEN, \n                     trainable = False)\n\nmodel.add(glove_embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(128, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\nmodel.add(Dense(256, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate=1e-5)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"accuracy\"])","198e6e4c":"model.summary()","6cc787d0":"train_data = tweet_pad[:train.shape[0]]\ntest_data = tweet_pad[train.shape[0]:]","7572a78c":"X_train, X_test, y_train, y_test = train_test_split(train_data, train[\"target\"].values, test_size = 0.20)","3f6c23b5":"hist = model.fit(X_train, y_train, batch_size = 64, epochs = 20, validation_data = (X_test, y_test))","f28a6f0d":"#Ploting Acuracy & Loss\r\nimport matplotlib.pyplot as plt\r\nplt.plot(hist.history['accuracy'])\r\nplt.plot(hist.history['val_accuracy'])\r\nplt.plot(hist.history['loss'])\r\nplt.plot(hist.history['val_loss'])\r\nplt.title(\"Model Accuracy\")\r\nplt.ylabel(\"Accuracy\")\r\nplt.xlabel(\"Epoch\")\r\nplt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\r\nplt.show()","46688f1c":"### Punctuation","7f932e01":"## Remove punctuation","de763edc":"## Remove emoji","587d3807":"# Glove vectorization (word2vec)","c1b366a9":"## Common words","5a8725ec":"## remove html tag","96f3090f":"## Spelling checker\n\nAdditional: spelling checker for indonesian dataset","96408f33":"# Data cleaning","e074b8bc":"\r\n\r\n```\r\n# This is formatted as code\r\n```\r\n\r\n# 1. Load Data","d74713a4":"## removing URLs","9ec869cf":"## Data Distribution","ff075916":"# 2. Create corpus"}}