{"cell_type":{"f9b68253":"code","944a6702":"code","37720ec4":"code","a051ae1e":"code","55c9d686":"code","6d2bab38":"code","58f51d8e":"code","944c6cb9":"code","a0775ef1":"code","843ce27a":"code","4dee702c":"code","7cc39067":"markdown","e8cd8489":"markdown","c0914475":"markdown","b5afa404":"markdown","4f484d6e":"markdown","fb940f8f":"markdown","c42e3749":"markdown","d5b31520":"markdown","dfc79d57":"markdown","865719ed":"markdown","fea2376b":"markdown","dfd9230c":"markdown","27d14f32":"markdown","67f80fe4":"markdown","40a67ebf":"markdown"},"source":{"f9b68253":"!pip install cmake 'gym[atari]' scipy","944a6702":"import gym\nenv = gym.make('Taxi-v3').env\nenv.render()","37720ec4":"env.reset()\nenv.render()\n\nprint('Action Space {}'.format(env.action_space))\nprint('State Space {}'.format(env.observation_space))","a051ae1e":"state = env.encode(3, 1, 2, 0)  # taxi row, taxi col, pas index, des index\nprint('State:', state)\n\nenv.s = state\nenv.render()","55c9d686":"env.P[328]","6d2bab38":"env.s = 328\n\nepochs = 0\npenalties, reward = 0, 0\n\nframes = []\n\ndone = False\n\nwhile not done:\n    action = env.action_space.sample()\n    state, reward, done, info = env.step(action)\n    \n    if reward == -10:\n        penalties += 1\n        \n    frames.append({\n        'frame': env.render(mode='ansi'),\n        'state': state,\n        'action': action,\n        'reward': reward\n    })\n    \n    epochs += 1\n    \nprint('Timesteps taken: {}'.format(epochs))\nprint('Penalties incurred {}'.format(penalties))","58f51d8e":"from IPython.display import clear_output\nfrom time import sleep\n\ndef print_frames(frames):\n    for i, frame in enumerate(frames):\n        clear_output(wait=True)\n        print(frame['frame'])\n        print(f\"Timestep: {i + 1}\")\n        print(f\"State: {frame['state']}\")\n        print(f\"Action: {frame['action']}\")\n        print(f\"Reward: {frame['reward']}\")\n        sleep(.1)\n        \nprint_frames(frames)","944c6cb9":"import numpy as np\nq_table = np.zeros([env.observation_space.n, env.action_space.n])","a0775ef1":"%%time\n\"\"\"Training the agent\"\"\"\n\nimport random\nfrom IPython.display import clear_output\n\n# Hyperparameters\nalpha = 0.1\ngamma = 0.6\nepsilon = 0.1\n\n# For plotting metrics\nall_epochs = []\nall_penalties = []\n\nfor i in range(1, 100001):\n    state = env.reset()\n\n    epochs, penalties, reward, = 0, 0, 0\n    done = False\n    \n    while not done:\n        if random.uniform(0, 1) < epsilon:\n            action = env.action_space.sample() # Explore action space\n        else:\n            action = np.argmax(q_table[state]) # Exploit learned values\n\n        next_state, reward, done, info = env.step(action) \n        \n        old_value = q_table[state, action]\n        next_max = np.max(q_table[next_state])\n        \n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table[state, action] = new_value\n\n        if reward == -10:\n            penalties += 1\n\n        state = next_state\n        epochs += 1\n        \n    if i % 100 == 0:\n        clear_output(wait=True)\n        print(f\"Episode: {i}\")\n\nprint(\"Training finished.\\n\")","843ce27a":"q_table[328]","4dee702c":"\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n\ntotal_epochs, total_penalties = 0, 0\nepisodes = 100\n\nfor _ in range(episodes):\n    state = env.reset()\n    epochs, penalties, reward = 0, 0, 0\n    \n    done = False\n    \n    while not done:\n        action = np.argmax(q_table[state])\n        state, reward, done, info = env.step(action)\n\n        if reward == -10:\n            penalties += 1\n\n        epochs += 1\n\n    total_penalties += penalties\n    total_epochs += epochs\n\nprint(f\"Results after {episodes} episodes:\")\nprint(f\"Average timesteps per episode: {total_epochs \/ episodes}\")\nprint(f\"Average penalties per episode: {total_penalties \/ episodes}\")","7cc39067":"## Gym's interface","e8cd8489":"* Equation:\n\n![image.png](attachment:image.png)","c0914475":"## The Reward Table","b5afa404":"## Enter Reinforcement Learning","4f484d6e":"### Intro to Q-Learning","fb940f8f":"# Implementing Q-learning in python","c42e3749":"## Back to our illustration","d5b31520":"* Q-table\n\n<img src=\"https:\/\/storage.googleapis.com\/lds-media\/images\/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png\" width=45%>","dfc79d57":"### Training the Agent","865719ed":"# Self-Driving Cab","fea2376b":"## Reminder of our problem","dfd9230c":"<img src=\"https:\/\/storage.googleapis.com\/lds-media\/images\/Reinforcement_Learning_Taxi_Env.width-1200.png\" width=35%>","27d14f32":"Source: https:\/\/www.learndatasci.com\/tutorials\/reinforcement-q-learning-scratch-python-openai-gym\/","67f80fe4":"### Evaluating the agent","40a67ebf":"# Solving the environment without Reinforcement Learning"}}