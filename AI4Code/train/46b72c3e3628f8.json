{"cell_type":{"1698a1a7":"code","e1cc6174":"code","1dd5d719":"code","5bf9d31d":"code","e0ac161d":"code","9f531ca9":"code","4cc76de9":"code","ba83ec1c":"code","f48d171f":"code","df917986":"code","36440600":"code","ac5bf2fd":"code","58109549":"code","4b1b220e":"code","5b83ba7c":"code","91c73ab2":"code","c2ef8f62":"code","3062a64d":"code","b8e27290":"code","b0031cf2":"code","daacab3c":"code","d6be3f2b":"code","7d33e99b":"code","b07948cf":"code","95ab6ee4":"code","255bbb60":"code","0a1889fa":"code","ab5bbdaa":"code","d23204a9":"code","5d7bc33e":"code","5aa2f09c":"markdown","e885f97f":"markdown","2b4bcf76":"markdown","d6acaf93":"markdown","30a096f6":"markdown","488fb678":"markdown"},"source":{"1698a1a7":"!pip install --user imblearn","e1cc6174":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, Activation, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import compute_class_weight\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom imblearn.over_sampling import SMOTE \nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","1dd5d719":"train_data = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/train.csv\", engine = 'python')","5bf9d31d":"train_data.head()","e0ac161d":"train_data.drop(columns=['image_id']).sum().plot.bar()\nplt.xlabel(\"Classes\")\nplt.ylabel('Counts')","9f531ca9":"height = 1365\nwidth = 2048\ncolor_channels = 3\nnew_height = 224\nnew_width = 224","4cc76de9":"images = np.ndarray(shape=(len(train_data), new_height, new_width, color_channels), dtype=np.float32)","ba83ec1c":"for i in range(len(train_data)):\n    print(\"Image: \" + str(i))\n    image = tf.keras.preprocessing.image.load_img(\"..\/input\/plant-pathology-2020-fgvc7\/images\/\"+train_data['image_id'].iloc[i]+'.jpg')\n    image = image.resize((new_width, new_height))\n    image = tf.keras.preprocessing.image.img_to_array(image)\n    print(image.shape)\n    images[i] = image","f48d171f":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.show()","df917986":"plotImages(images[:5] \/ 255)","36440600":"labels = train_data.drop(columns=['image_id'])","ac5bf2fd":"X_train, X_test, y_train, y_test = train_test_split(images, np.array(labels.values), test_size=0.20, random_state=42)","58109549":"sm = SMOTE(random_state=42)","4b1b220e":"X_train, y_train = sm.fit_resample(X_train.reshape((-1, new_height * new_width * 3)), y_train)","5b83ba7c":"X_train = X_train.reshape((-1, new_height, new_width, 3))","91c73ab2":"train_datagen = ImageDataGenerator(\n    rotation_range=45, width_shift_range=0.25,\n    height_shift_range=0.25, shear_range=0.5, \n    zoom_range=0.25,horizontal_flip=True, vertical_flip=True, brightness_range=[0.5, 1.5],\n    fill_mode=\"nearest\", rescale=1.\/255)\ntrain_datagen.fit(X_train)","c2ef8f62":"test_datagen = ImageDataGenerator(\n    rotation_range=45, width_shift_range=0.1,\n    height_shift_range=0.1, shear_range=0.2, \n    zoom_range=0.2,horizontal_flip=True, vertical_flip=True, brightness_range=[0.5, 1.5],\n    fill_mode=\"nearest\", rescale=1.\/255)\ntest_datagen.fit(X_test)","3062a64d":"model = Sequential()\nmodel.add(Conv2D(64, (3, 3), activation ='relu', padding = 'same', input_shape = images.shape[1:]))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((3, 3)))\nmodel.add(Conv2D(128, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(256, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(512, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, (3, 3), activation ='relu', padding = 'same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dense(4, activation = \"softmax\"))\nmodel.summary()","b8e27290":"model.compile(loss='categorical_crossentropy', \n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","b0031cf2":"LR_reduce=ReduceLROnPlateau(monitor='val_accuracy',\n                            patience=5,\n                            verbose=1)","daacab3c":"ES_monitor=EarlyStopping(monitor='val_loss',\n                          patience=10)","d6be3f2b":"history = model.fit(train_datagen.flow(X_train, y_train, batch_size=32), \n                    steps_per_epoch=X_train.shape[0] \/\/ 32,\n                    epochs=400, \n                    validation_data=test_datagen.flow(X_test, y_test, batch_size=32),\n                    validation_steps=X_test.shape[0] \/\/ 32, callbacks=[ES_monitor,LR_reduce])","7d33e99b":"plt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Convolutional Network Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'validation'])\nplt.show()","b07948cf":"model.save('plant_disease_model.h5')","95ab6ee4":"classifier = tf.keras.models.load_model('plant_disease_model.h5')","255bbb60":"test_data = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\")","0a1889fa":"test_data.head()","ab5bbdaa":"test_images = np.ndarray(shape=(len(train_data), new_height, new_width, color_channels), dtype=np.float32)","d23204a9":"for i in range(len(test_data)):\n    print(\"Image: \" + str(i))\n    image = tf.keras.preprocessing.image.load_img(\"..\/input\/plant-pathology-2020-fgvc7\/images\/\"+test_data['image_id'].iloc[i]+'.jpg')\n    image = image.resize((new_width, new_height))\n    image = tf.keras.preprocessing.image.img_to_array(image)\n    image = image\/255\n    print(image.shape)\n    test_images[i] = image","5d7bc33e":"pred = classifier.predict(test_images)\n\nres = pd.DataFrame()\nres['image_id'] = test_data['image_id']\nres['healthy'] = pred[:, 0]\nres['multiple_diseases'] = pred[:, 1]\nres['rust'] = pred[:, 2]\nres['scab'] = pred[:, 3]\nres.to_csv('submission.csv', index=False)\nres.head(10)","5aa2f09c":"# Plant Disease Classification Using Keras CNN","e885f97f":"### Image Data Generator Augments images by rotating, changing lighting, etc.","2b4bcf76":"### Define Model and Train","d6acaf93":"### Save Model and Classify Test Images","30a096f6":"### Use SMOTE to obtain a balanced dataset","488fb678":"## Basic EDA & Image Preprocessing"}}