{"cell_type":{"4736a778":"code","f2cb5629":"code","67f5ed85":"code","346448d6":"code","f30179d4":"code","8fe6b2ed":"code","534c5e80":"code","006bc192":"code","8d3d4b46":"code","1e87e475":"code","0d522622":"code","046deb16":"code","32738cb9":"code","011f2879":"code","c032d351":"code","7f27c525":"code","7af1e5ff":"code","02993f60":"code","a69827bd":"code","ccfbab95":"code","953a254b":"code","5b554180":"code","9e54f6c6":"code","a6d201ce":"code","da254fda":"code","7866329e":"code","aea19f79":"code","4a7035d3":"code","933044f4":"code","61573cf3":"code","6ccc83b3":"code","e80cc34f":"code","a3b1534e":"code","3d8683b9":"code","8125ad80":"code","6b96d86a":"code","19012d9d":"markdown","240fb343":"markdown","699fbc6d":"markdown","9d04658e":"markdown","e6603cc1":"markdown","c993726e":"markdown","7303a7ce":"markdown","2f7c21c5":"markdown"},"source":{"4736a778":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2cb5629":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","67f5ed85":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","346448d6":"train['train']  = 1\ntest['train']  = 0\ndf_train = pd.concat([train, test], axis=0,sort=False)  ","f30179d4":"df_train.info()","8fe6b2ed":"df_train.describe().transpose()","534c5e80":"df_train.shape","006bc192":"df_train.columns","8d3d4b46":"# Let's check our variable to predict\ndf_train['SalePrice'].describe()","1e87e475":"#histogram\nsns.distplot(df_train['SalePrice']);\n","0d522622":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","046deb16":"df_train","32738cb9":"df_train","011f2879":"#Highest saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = df_train.corr().nlargest(k, 'SalePrice').index\ncm = df_train[cols].corr().values\nsns.set(font_scale=1.2) # Size of the lettrers\nplt.subplots(figsize=(10, 8)) # Size of the heatmap\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","c032d351":"#Top 10 highest correlation dispersion map\ncols\nsns.pairplot(df_train[cols],\n             y_vars=['SalePrice'],\n             x_vars=cols)","7f27c525":"# let's fill with 0 our target variable\ndf_train['SalePrice'].fillna(0, inplace = True)","7af1e5ff":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.drop(missing_data[missing_data['Total'] == 0].index,inplace = True)\nmissing_data","02993f60":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","a69827bd":"#To handle missing data, we'll delete all the variables with missing data\ndf_train =df_train.drop((missing_data[missing_data['Total'] > 5]).index, axis=1)\nprint(df_train.isnull().sum().max())","ccfbab95":"#Missing numeric data\nmd = missing_data[missing_data['Total'] <= 5].index\nnum = df_train[md].dtypes[df_train[md].dtypes != 'object']\nfor col in num.index:\n    df_train[col].fillna(df_train[col].mode()[0], inplace=True)","953a254b":"#Missing categorical data\ncat = df_train[md].dtypes[df_train[md].dtypes == 'object']\nfor col in cat.index:\n    df_train[col].fillna(df_train[col].mode()[0], inplace=True)","5b554180":"#Checking if remain missing values.\ndf_train.isnull().sum().max()","9e54f6c6":"#Standardizing data\n#We need to establish a threshold that defines an observation as an outlier.To do so, we'll standardize the data.\n#In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.\n\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis])\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","a6d201ce":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","da254fda":"train.shape","7866329e":"#original data\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n#full data\ndf_train = df_train.reset_index(drop=True)\ndrop = df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000) & (df_train['train']==1)].index\ndf_train = df_train.drop(drop)","aea19f79":"sns.distplot(df_train['SalePrice'][df_train['SalePrice']>0] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'][df_train['SalePrice']>0], plot=plt)\nplt.show()","4a7035d3":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train['SalePrice'][df_train['SalePrice']>0] = np.log(df_train['SalePrice'][df_train['SalePrice']>0])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'][df_train['SalePrice']>0] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'][df_train['SalePrice']>0])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'][df_train['SalePrice']>0], plot=plt)\nplt.show()","933044f4":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)\nprint(df_train.shape)","61573cf3":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n#from lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math","6ccc83b3":"#Target, train and test data\ndf_final = df_train.drop(['Id',],axis=1)\n\ntarget = df_final['SalePrice'][df_final['train'] == 1]\n\ntrain_final = df_final[df_final['train'] == 1]\ntrain_final = train_final.drop(['train','SalePrice'],axis=1)\n\n\ntest_final = df_final[df_final['train'] == 0]\ntest_final = test_final.drop(['train','SalePrice'],axis=1)","e80cc34f":"x_train,x_test,y_train,y_test = train_test_split(train_final,target,test_size=0.20,random_state=0)","a3b1534e":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)","3d8683b9":"#Fitting all dataframe\nxgb.fit(x_train, y_train)\npredict = xgb.predict(x_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict1))))","8125ad80":"#Fitting train dataframe\nxgb.fit(train_final, target)\npredict_final = np.expm1(xgb.predict(test_final))\npredict_final","6b96d86a":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": predict_final\n    })\nsubmission.to_csv('submission.csv', index=False)","19012d9d":"# **MODELLING**","240fb343":"How 'SalePrice' looks with her new values:\n\nLow range values are similar and not too far from 0.\nHigh range values are far from 0 and the 7.something values are really out of range.\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.","699fbc6d":"# **OUTLIERS**","9d04658e":"* Normal distribution.\n* Positive skewness.\n* Show leptokurtic.","e6603cc1":"SalePrice is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\nIn case of positive skewness, log transformations usually works well. ","c993726e":"# **MISSING DATA**","7303a7ce":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","2f7c21c5":"# **JOINING THE DATAFRAMES**"}}