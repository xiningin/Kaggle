{"cell_type":{"c41a2bf5":"code","cfb121b6":"code","ba304477":"code","eb23f5e9":"code","94c5156d":"code","fe061262":"code","ce55fa56":"markdown"},"source":{"c41a2bf5":"import numpy as np\ndef bandit(A, q):\n    R = np.random.normal(q[A], 1)\n    \n    return R\ndef selectBandit(epsilon, excludeCurrentMax=False):\n    if(np.random.random()>epsilon):\n        return np.argmax(Q)\n    else:\n        return np.random.randint(0, len(Q))    ","cfb121b6":"num_exps = 2000\nnum_timesteps = 1000\nepsilon = 0.0\nrhist = np.zeros(num_timesteps)\nahist = np.zeros(num_timesteps)\n\nfor k in range(num_exps):\n    Q = np.zeros(10)\n    N = np.zeros(10)\n    q = np.array([np.random.standard_normal() for i in range(10)])\n    optimal_action=np.argmax(q)\n    reward_history = []\n    optimal_action_history = []\n    for i in range(num_timesteps):\n        # choose an action\n        A = selectBandit(epsilon)\n        R = bandit(A, q)\n        reward_history.append(R)\n        if A == optimal_action:\n            optimal_action_history.append(1)\n        else:\n            optimal_action_history.append(0)\n        N[A] += 1\n        Q[A] += 1\/N[A]*(R - Q[A])\n    \n    rhist += np.array(reward_history)\n    ahist += np.array(optimal_action_history)","ba304477":"rhist \/= np.float(num_exps)\nahist \/= np.float(num_exps)","eb23f5e9":"import matplotlib.pyplot as plt\n","94c5156d":"plt.plot(rhist, label='average reward, eps = ' + str(epsilon))\nplt.legend()\nplt.show()","fe061262":"plt.plot(ahist, label='average optimal action, eps = ' + str(epsilon))\nplt.legend()\nplt.show()","ce55fa56":"# Variances\n### Optimistic\n### UCB (Upper confidence boundary) => actually works better.\n# Convergence conditions\nTwo conditions to have convergence for this\n- Stationary\n- If non-stationary, should not change rapidly\n- E.g., Texas State Lottery (change the air + change the probability distribution of the balls) => make it non-stationary.\n\n\n"}}