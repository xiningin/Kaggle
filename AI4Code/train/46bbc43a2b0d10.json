{"cell_type":{"71d2cd16":"code","841ca699":"code","d3499881":"code","f2339407":"code","99f0cf37":"code","d6c8287b":"code","ea1d7e45":"code","dca19f50":"code","7292603e":"code","2be1bcd3":"code","26bf7b79":"code","62acf203":"code","2734ee12":"code","24535ea2":"code","0a3c81d8":"code","b4676abf":"code","817f8b29":"code","aa0ca5cd":"code","8799e5bd":"code","1e90a476":"code","4abaf653":"code","2558df43":"code","d9b45d82":"code","517e69cd":"code","bc5c4d82":"code","5ab1d8bf":"code","73ef2171":"code","4ac9008a":"code","74b62f65":"code","45da06c4":"code","f6768720":"code","d8afb071":"code","80d3f85a":"code","f91b9560":"code","5fa4c32b":"code","1654c97f":"code","70a842f1":"markdown","d4bc9f20":"markdown","7ed7cec2":"markdown","ead4b01f":"markdown","394094ed":"markdown","31b784af":"markdown","007c2861":"markdown","dd468374":"markdown","223e5b30":"markdown","52fc45a5":"markdown","5da8fcdd":"markdown","50ab04c6":"markdown","dd1de87f":"markdown","c93344a9":"markdown","39c8fa6f":"markdown","86ed78e2":"markdown","4881b9ad":"markdown","0e40c5b1":"markdown","ecdd3429":"markdown","b163cfe4":"markdown","dceba3e4":"markdown","ad0ea42e":"markdown"},"source":{"71d2cd16":"# Data Import on Kaggle\nimport os\nimport time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Importing libraries for the model\nimport xgboost as xgb \nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n# sklearn imports for analysis\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom scipy.stats import randint\nfrom sklearn.metrics import roc_curve, auc","841ca699":"data = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')","d3499881":"data = data.drop('id', axis=1)","f2339407":"memory_usage = data.memory_usage(deep=True) \/ 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","99f0cf37":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\ndata = reduce_memory_usage(data, verbose=True)\ntest_data = reduce_memory_usage(test_data, verbose=True)","d6c8287b":"data.describe()","ea1d7e45":"sample_df = data.sample(int(len(data) * 0.2))\nsample_df.shape","dca19f50":"# Let's confirm if the sampling is retaining the feature distributions\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.histplot(\n    data=data, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","7292603e":"sample_df","2be1bcd3":"# Check na values\nprint('Amount of existing NaN values', sample_df.isna().sum())\n\nprint('---------')\n# Target Class Distribution\ntarget_dist = sample_df.target.value_counts()\nprint('Distribution of Target Class \\n',target_dist)\nprint(target_dist[0]\/(target_dist[0] + target_dist[1]))","26bf7b79":"f, ax = plt.subplots(figsize=(8, 6))\ncorr = sample_df.iloc[:,:20].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","62acf203":"cat_variables = []\n\nfor column in sample_df.columns:\n    if len(sample_df[column].unique()) < 10:\n        cat_variables.append(column)\nprint(cat_variables)","2734ee12":"fig = plt.figure(figsize = (18, 100))\n\nfor i in range(len(sample_df.columns.tolist()[:241])):\n    if sample_df.columns.tolist()[:241][i] in ['f22', 'f43']: \n        continue\n    else:\n        plt.subplot(25,10,i+1)\n        sns.set_style(\"white\")\n        plt.title(sample_df.columns.tolist()[:241][i], size = 12, fontname = 'monospace')\n        a = sns.kdeplot(sample_df[sample_df.columns.tolist()[:241][i]], color = '#1a5d57', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n        plt.ylabel('')\n        plt.xlabel('')\n        plt.xticks(fontname = 'monospace')\n        plt.yticks([])\n        for j in ['right', 'left', 'top']:\n            a.spines[j].set_visible(False)\n            a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","24535ea2":"# Code from https:\/\/www.kaggle.com\/craigmthomas\/tps-oct-2021-eda\n\ncat_features = [\"f22\", \"f43\"]\ncat_features.extend([\"f{}\".format(x) for x in range(242, 285)])\n\nfig, axs = plt.subplots(11, 4, figsize=(4*4, 11*3), squeeze=False, sharey=True)\n\nptr = 0\nfor row in range(11):\n    for col in range(4):  \n        x = sample_df[[cat_features[ptr], \"target\"]].value_counts().sort_index().to_frame().rename({0: \"# of Samples\"}, axis=\"columns\").reset_index()\n        sns.barplot(x=cat_features[ptr], y=\"# of Samples\", hue=\"target\", data=x, ax=axs[row][col])\n        plt.xlabel(cat_features[ptr])\n        ptr += 1\n        del(x)\nplt.tight_layout()    \nplt.show()","0a3c81d8":"# We only want to scale the continuous features.\n\ncont_features = []\nfor x in range(0, 242):\n    if (x != 22) and (x!=43):\n        cont_features.append(\"f{}\".format(x))","b4676abf":"from sklearn.preprocessing import MinMaxScaler\n\nscale = MinMaxScaler()\nsample_df[cont_features]=scale.fit_transform(sample_df[cont_features])\nsample_df[cont_features]= scale.transform(sample_df[cont_features])  \n\nprint('Data scaled using : ', scale)","817f8b29":"X = sample_df.drop('target', axis=1)\ny = sample_df.target\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.7, random_state=42)\n\ndel sample_df # we do this to remove sample_df from memory","aa0ca5cd":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8799e5bd":"id_test_submission = test_data.id\nX_test_submission = test_data.drop('id', axis=1)\n\ndel test_data","1e90a476":"model_dict = {\n    'ADABoost': AdaBoostClassifier(),\n    'CatBoost': CatBoostClassifier(verbose=False),\n    'Light GBM': lgb.LGBMClassifier(random_state=0, verbose=-1),\n#     'XGB': xgb.XGBClassifier(random_state=0, n_estimators=10), \n#     'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=0, verbose=1),\n#     'Logistic Reg': LogisticRegression(random_state=0, max_iter=350, solver='lbfgs'),\n#     'Naive Bayes': GaussianNB(), \n#     'Support Vector Machine': SVC(random_state=0, verbose=1),\n#     'K Nearest Classifier': KNeighborsClassifier(),\n#     'Decison Tree': DecisionTreeClassifier(random_state=0),\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\n\nfor model, clf in model_dict.items():\n    start_time = time.time()\n\n    clf.fit(X_train, y_train)\n    \n    # test results\n    test_pred = clf.predict(X_test)\n    test_acc = roc_auc_score(y_test, test_pred)\n    \n    # train results\n    train_pred =  clf.predict(np.float32(X_train))\n    train_acc = roc_auc_score(y_train, train_pred)\n\n    print(model, 'Model')\n    print('Classification Report \\n',classification_report(y_test, test_pred))\n    print('Confusion Matrix \\n',confusion_matrix(y_test,test_pred))\n    print('Train Accuracy: ', train_acc)\n    print('Test Accuracy: ', test_acc)\n    print(\"\\n Ran in %s seconds\" % (time.time() - start_time))\n    print('--------------------------------')\n    \n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    test_acc_list.append(test_acc)   \n    \n\nresults = pd.DataFrame({\"model\": model_list, \"train_accuracy\": train_acc_list, \"test_acc\": test_acc_list})","4abaf653":"results","2558df43":"params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","d9b45d82":"%%time\n\n# Code from https:\/\/www.kaggle.com\/mohammadkashifunique\/tsp-single-xgboost-model\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_train, y_train)):\n    X_train_t, y_train_t = X.iloc[idx_train], y.iloc[idx_train]\n    X_test_t, y_test_t = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    params['learning_rate']=0.007\n    model1 = xgb.XGBClassifier(**params)\n    \n    model1.fit(X_train_t,y_train_t,\n              eval_set=[(X_train_t, y_train_t),(X_test_t,y_test_t)],\n              early_stopping_rounds=200,\n              verbose=False)\n    \n    params['learning_rate']=0.01\n    model2 = xgb.XGBClassifier(**params)\n    \n    model2.fit(X_train,y_train,\n              eval_set=[(X_train_t, y_train_t),(X_test_t,y_test_t)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model1)\n    \n    params['learning_rate']=0.05\n    model3 = xgb.XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train_t, y_train_t),(X_test_t,y_test_t)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model2)\n    \n    pred_valid = model3.predict_proba(X_test_t)[:,1]\n    fpr, tpr, _ = roc_curve(y_test_t, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model3.predict_proba(X_test_submission)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","517e69cd":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nnew_df = pd.DataFrame({'id': id_test_submission, 'target': predictions})\nnew_df.to_csv('.\/xgb_submission.csv', index=False)","bc5c4d82":"# params = {\n#     'num_leaves': [50, 28, 31, 50, 75],\n#     'learning_rate': [0.003, 0.009],\n#     'max_depth': [-1, 3, 5, 7],\n#     'n_estimators': [500, 1000, 2500],\n# }\n\n# lgb_estimator = lgb.LGBMClassifier(random_state=42)\n\n# grid = GridSearchCV(lgb_estimator, param_grid=params, scoring='roc_auc_ovr', cv=5, verbose=100)\n# lgb_model = grid.fit(X_train, y_train)\n\n# print(lgb_model.best_params_, lgb_model.best_score_)","5ab1d8bf":"lgb_model = lgb.LGBMClassifier(learning_rate=0.003, max_depth=-1, n_estimators=1000, num_leaves=50, random_state=42, verbose=100, device='gpu')\nlgb_model.fit(X_train, y_train)\npreds = lgb_model.predict(X_test)\nroc_auc = roc_auc_score(y_test, preds)\nprint('roc_auc of lgb:', roc_auc)","73ef2171":"submission_preds = lgb_model.predict(X_test_submission)\nnew_df = pd.DataFrame({'id': id_test_submission, 'target': submission_preds})\nnew_df.to_csv(\"lgb_submission.csv\",index=False)","4ac9008a":"# param_dist = { \"learning_rate\": np.linspace(0,0.2,5),\n#                \"max_depth\": randint(3, 10)}\n               \n# #Instantiate RandomSearchCV object\n# cat_model = CatBoostClassifier(random_state=42, verbose=500)\n# rscv = RandomizedSearchCV(cat_model , param_dist, scoring='roc_auc', cv=3)\n\n# #Fit the model\n# rscv.fit(X_train,y_train)\n\n# # Print the tuned parameters and score\n# print(rscv.best_params_)\n# print(rscv.best_score_)\n","74b62f65":"cat_model = CatBoostClassifier(learning_rate=0.003, max_depth=3, n_estimators=1000, random_state=42, verbose=100)\ncat_model.fit(X_train, y_train)\npreds = cat_model.predict(X_test)\nroc_auc = roc_auc_score(y_test, preds)\n# cat_score, time = train_model(cat_model)\nprint('roc_auc of cat:', roc_auc)","45da06c4":"submission_preds = cat_model.predict(X_test_submission)\nnew_df = pd.DataFrame({'id': id_test_submission, 'target': submission_preds})\nnew_df.to_csv(\"cat_submission.csv\",index=False)","f6768720":"ada_model = AdaBoostClassifier(random_state=42)\nada_model.fit(X_train, y_train)\npreds = ada_model.predict(X_test)\nroc_auc = roc_auc_score(y_test, preds)\n# ada_score, time = train_model(ada_model)\nprint('\\n roc_auc of ada:', roc_auc)","d8afb071":"# Code from https:\/\/www.kaggle.com\/ankitkalauni\/tps-21-oct-single-histgbm-0-85651\nhist_params = {'l2_regularization': 1.3244040135051264e-10,\n               'early_stopping': 'True',\n               'learning_rate': 0.0366777965884429, \n               'max_iter': 10000, \n               'max_depth': 3, \n               'max_bins': 129, \n               'min_samples_leaf': 13449, \n               'max_leaf_nodes': 68}\n\nkf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_train, y_train)):\n    hgbm_model = HistGradientBoostingClassifier(**hist_params)\n    hgbm_model.fit(X_train, y_train)\n    preds = hgbm_model.predict(X_test)\n#     pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_test, preds)\n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('--'*20)","80d3f85a":"# Splitting the entire dataset into train and test\n\nX = data.drop('target', axis=1)\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.85, random_state=42)","f91b9560":"params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","5fa4c32b":"%%time\n\n# Code from https:\/\/www.kaggle.com\/mohammadkashifunique\/tsp-single-xgboost-model\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_train, y_train)):\n    X_train_t, y_train_t = X.iloc[idx_train], y.iloc[idx_train]\n    X_test_t, y_test_t = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    params['learning_rate']=0.007\n    model1 = xgb.XGBClassifier(**params)\n    \n    model1.fit(X_train_t,y_train_t,\n              eval_set=[(X_train_t, y_train_t),(X_test_t,y_test_t)],\n              early_stopping_rounds=200,\n              verbose=False)\n    \n    params['learning_rate']=0.01\n    model2 = xgb.XGBClassifier(**params)\n    \n    model2.fit(X_train,y_train,\n              eval_set=[(X_train_t, y_train_t),(X_test_t,y_test_t)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model1)\n    \n    params['learning_rate']=0.05\n    model3 = xgb.XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train_t, y_train_t),(X_test_t,y_test_t)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model2)\n    \n    pred_valid = model3.predict_proba(X_test_t)[:,1]\n    fpr, tpr, _ = roc_curve(y_test_t, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model3.predict_proba(X_test_submission)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","1654c97f":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nnew_df = pd.DataFrame({'id': id_test_submission, 'target': predictions})\nnew_df.to_csv('.\/xgb_submission_final.csv', index=False)","70a842f1":"## Model Selection\n\nFinally, now that we're done with the preprocessing and EDA, we are going to take a look at how some basic models perform on a subset of the data (20%) without any parameter tuning. We can then retrain and evaluate the top performing models with a bigger dataset and tuned parameters.\n\nYou can take a look at the model_dict and add any models that you think might perform well. Or leave a comment and I'll add them asap!\n\n\n*P.S: Kaggle has a time-out error if the run time of a notebook exceeds a certain time limit. So, I'll comment some of these models out. However, I'll keep the top performing models uncommented.*","d4bc9f20":"\n## Hyperparameter Tuning\n\nWe'll start off by using GridSearchCV\/RandomizedSearchCV on these models with various parameters and selecting the best performing ones based on their roc_auc score.\n\nAgain, since the Grid Search and Randomized Search run for a long time, we run into a time-out error. So, I have commented those segments out but you can run them if you'd like! I have used the best chosen parameters and built the final model with those.","7ed7cec2":"Before we look at distributions, we need to split the data into continuous and categorical variables.","ead4b01f":"## What Next?\n\nI'll continue testing with other models to find better results. In the meantime, you could reach out to me through the comments if you have ideas on models I could add. \n \n#### If you liked this notebook, I'd appreciate it if you would leave an upvote!\n","394094ed":"# Tabular Playground Series - Oct 21\n\nThis month, our data consists of 284 feature variables and our target variable is binary classification. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. \n\n## Plan\n\nMoving forward this is the plan we are going to be following. Keep in mind, this is not a concrete plan and I might change it as we move through the notebook. This will show you my process on how I approach these datasets.\n\n- *Memory Reduction*\n- *Sampling to Reduce Training Time*\n- *EDA*\n- *Model Development*\n- *Hyperparameter Tuning*\n- *Feature Importance from top models*\n- *Selecting the best Model*","31b784af":"## Submission","007c2861":"## Histogram Gradient Boosting","dd468374":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","223e5b30":"\n## CatBoost\n\nTo show you more techniques, I have used RandomizedSearchCV for this model which is an alternative to GridSearchCV. Now that you have code samples of both, you can test them out and select which ones you like.","52fc45a5":"## Initial Model Selection\n\n\nNow that we've trained our first batch of models on default parameters, we can eliminate a few which didn't do well. The CatBoost, AdaBoost and the LightGBM models performed the best so we will keep those and perform hyperparamter tuning on them.","5da8fcdd":"\n## LightGBM\n\nThe parameters we set for grid search were:\n- learning_rate: 0.003, 0.009\n- max_depth: -1, 3, 5, 7\n- n_estimators: 500, 1000, 2500\n- num_leaves: 28, 31, 50, 75\n\nand the top performing parameters were\n- learning_rate: 0.003\n- max_depth: -1\n- n_estimators: 1000,\n- num_leaves: 50\n\nwith a score of (0.76)","50ab04c6":"## EDA\n\nLet's start looking at any correlations that might exist among the features.\nWe will also be looking at the densities of every feature.","dd1de87f":"## Memory Reduction\n\nIf you don't have any issues with memory, you can go ahead and skip this step. \nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. \n\nThere are several other methods to save RAM - you can refer to this article on [14 tips to save RAM memory](https:\/\/www.kaggle.com\/pavansanagapati\/14-simple-tips-to-save-ram-memory-for-1-gb-dataset). ","c93344a9":"## The Finale\n\nPhew! Now that we're done testing all these different models and tuning them, we have the results. About time, huh. You've been reading for ages (Thank you for that btw!) :)\n\nOur results showed the Histogram Gradient Boosting Classifier gave the best results. Let's train this (with the new parameters) on the entire dataset and see the results.","39c8fa6f":"## Sampling Data\n\nNow that we have reduced the memory usage by over 70%, let's sample the data. \n\nWhy are we doing this? Well, you don't have to. But if you're like me and own a Macbook Air that can't handle a dataset bigger than 100mb, this might be a good idea.\n\nWhen we are performing model selection and hyperparameter tuning later, we can't afford to let the notebook run for hours on end testing every model. Doing this, preserves the distributions of each feature while taking only 20% of the entire dataset and we can reduce the training time by using this sampled data.\n\nWe can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data. Once we decide on the model we want to use and improve its performance, we can train the final model on the entire dataset again.","86ed78e2":"## XGBoost\n","4881b9ad":"### Scaling \n\nWhile most of the models I plan to use in the 'model selection' section will not require any form of feature scaling (like, XGBoost, Random Forest, etc.), some of them (like, KNN and SVM) need it to work. \n\n##### Why\n\nIn general, algorithms that exploit distances or similarities (e.g. in the form of scalar product) between data samples, such as K-NN and Support Vector Machines, are sensitive to feature transformations.\n\nGraphical-model based classifiers, such as Fisher LDA or Naive Bayes, as well as Decision trees and Tree-based ensemble methods (Random Forests, XGBoost) are invariant to feature scaling, but still, it might be a good idea to rescale\/standardize your data.","0e40c5b1":"There doesn't seem to be any nan values in the data. Also, the target class is split evenly between the two groups","ecdd3429":"## Data Preparation\n\nIn this section, we will do some preprocessing. This part involves feature scaling and splitting the data into train and test sets.","b163cfe4":"### Train-Test Split\n\nLet's split our sampled data into train and test sets","dceba3e4":"\n## AdaBoost\n","ad0ea42e":"So, the categorical features in the dataset are f22, f43 and all the ones from f242-f284. Let's find the  distributions of the rest using kdeplot and the distributions of these using barplot (compared to target)."}}