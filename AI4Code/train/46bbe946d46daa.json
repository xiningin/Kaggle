{"cell_type":{"03d87794":"code","d2b68a6a":"code","b9f11772":"code","fc9c3a6f":"code","22298ddd":"code","e49e6c01":"code","8105074c":"code","e1c919ea":"code","ed28fd6a":"code","29271869":"code","cdb3822c":"code","f8e42217":"code","1c82367e":"code","f44a26c8":"code","8c7b4707":"code","a9e23305":"code","c8eb4ea9":"code","ccbc6238":"code","899454fc":"code","a179442f":"code","fd1f7b2d":"code","3dcf3e18":"code","c46d4520":"markdown","bdd06e8d":"markdown","d960d227":"markdown"},"source":{"03d87794":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2b68a6a":"%config Completer.use_jedi = False","b9f11772":"import category_encoders as ce\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer","fc9c3a6f":"cat_features = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nnumerical_features = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5','cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']","22298ddd":"df   = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv')\ny = df.target","e49e6c01":"oh = ce.OneHotEncoder()\nss = QuantileTransformer()\ndef pre_pipe(fr, train=True):\n    if train:\n        c = fr[cat_features]\n        c = oh.fit_transform(c)\n        n = fr[numerical_features]\n        n = pd.DataFrame(ss.fit_transform(n), columns=numerical_features)\n        f = c.merge(n, left_index=True, right_index=True)\n        return f\n    else: \n        c = fr[cat_features]\n        c = oh.transform(c)\n        n = fr[numerical_features]\n        n = pd.DataFrame(ss.transform(n), columns=numerical_features)\n        f = c.merge(n, left_index=True, right_index=True)\n        return f","8105074c":"len(pre_pipe(df).columns), len(pre_pipe(test, False).columns)","e1c919ea":"inp = 70\nhid = 440\nhid2 = 530\nhid3 = 378\nhid4 = 137\nhid5 = 52","ed28fd6a":"class NeuralNetwork(nn.Module):\n    def __init__(self, input_dim, num_hidden):\n        super().__init__()\n        self.linear1    = nn.Linear(input_dim, num_hidden)\n        self.linear2    = nn.Linear(num_hidden, hid2)\n        self.linear3    = nn.Linear(hid2,hid3)\n        self.linear4    = nn.Linear(hid3,hid4)\n        self.linear5    = nn.Linear(hid4,hid5)\n        self.batchNorm1 = nn.BatchNorm1d(num_hidden)\n        self.batchNorm2 = nn.BatchNorm1d(hid2)\n        self.batchNorm3 = nn.BatchNorm1d(hid3)\n        self.batchNorm4 = nn.BatchNorm1d(hid4)\n        self.batchNorm5 = nn.BatchNorm1d(hid5)\n        self.dropout    = nn.Dropout(p=0.48)\n        self.relu       = nn.ReLU()\n        self.sigmoid    = nn.Sigmoid()\n        self.out        = nn.Linear(hid5, 1)\n\n    def forward(self, x):\n        l1   = self.linear1(x)\n        n1   = self.batchNorm1(l1)\n        d1   = self.dropout(n1)\n        act1 = self.relu(d1)\n        l2   = self.linear2(act1)\n        n2   = self.batchNorm2(l2)\n        d2   = self.dropout(n2)\n        act2 = self.relu(d2)\n        l3   = self.linear3(act2)\n        n3   = self.batchNorm3(l3)\n        d3   = self.dropout(n3)\n        act3 = self.relu(d3)\n        l4   = self.linear4(act3)\n        n4   = self.batchNorm4(l4)\n        d4   = self.dropout(n4)\n        act4 = self.relu(d4)\n        l5   = self.linear5(act4)\n        n5   = self.batchNorm5(l5)\n        d5   = self.dropout(n5)\n        act5 = self.relu(d5)\n        l6   = self.out(act5)\n        output = l6 #self.sigmoid(l6)\n        return output\n    ","29271869":"class RMSELoss(torch.nn.Module):\n    def init(self):\n        super(RMSELoss,self).init()\n\n    def forward(self,x,y):\n        criterion = nn.MSELoss()\n        loss = torch.sqrt(criterion(x, y))\n        return loss","cdb3822c":"loss = RMSELoss()\nmodel = NeuralNetwork(inp,hid)","f8e42217":"def torch_fit(x, y, x_val, y_val,model, loss, lr, num_epochs):\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,nesterov=False)#torch.optim.Adam(model.parameters(), lr=lr) # #torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)#torch.optim.Adagrad(model.parameters(), lr=lr, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)#torch.optim.Adadelta(model.parameters(), rho=0.9, eps=1e-06, weight_decay=0)# # #torch.optim.Adam(model.parameters(), lr=lr)\n    count = -1\n    pvl = 0\n    for epoch in range(num_epochs):\n        optimizer.zero_grad()\n        y_pred_tensor = model(x)\n        loss_value = loss(y_pred_tensor, y)\n        val_loss   = loss(model(x_val),y_val)\n        print(f'Epoch {epoch}, loss {loss_value.item():.4f} val loss: {val_loss.item():.4f} count: {count}')\n        loss_value.backward()\n        optimizer.step()\n        if val_loss.item() > pvl:\n            count += 1\n        pvl = val_loss\n        if count > 3750: \n            break\n    return model","1c82367e":"from sklearn.model_selection import train_test_split","f44a26c8":"x = pre_pipe(df).values\nx_train, x_val, y_train, y_val = train_test_split(x,y)\nl = len(y_train)\nlv = len(y_val)\nl,lv","8c7b4707":"x_tensor = torch.tensor(x_train).float().cuda()\nx_val_tensor = torch.tensor(x_val).float().cuda()\ny_tensor = torch.tensor(y_train.values).float().cuda()\ny_val_tensor = torch.tensor(y_val.values).float().cuda()\ny_tensor = y_tensor.view(l,1).cuda()\ny_val_tensor = y_val_tensor.view(lv,1).cuda()","a9e23305":"x_tensor.size(), y_tensor.size()","c8eb4ea9":"model.cuda()\nnn_model = torch_fit(x_tensor, y_tensor, x_val_tensor, y_val_tensor, model=model, loss=loss, lr=0.00728, num_epochs=9550)","ccbc6238":"s_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\ns_df.head()\n#\/kaggle\/working\/subs.csv","899454fc":"tes_tensor = torch.tensor(pre_pipe(test, False).values).float().cuda()\ntes_tensor.size()","a179442f":"preds = model(tes_tensor)\npreds = preds.detach().cpu().numpy()","fd1f7b2d":"s_df.target = preds \ns_df.head()","3dcf3e18":"s_df.to_csv('\/kaggle\/working\/subs4.csv', index=False)","c46d4520":"### Import the data. ","bdd06e8d":"### Preprocessing \nIn the function below I have done some simple preprocessing, scaling the numerical data and one hot encoding the catergorical data.  ","d960d227":"### Tabular Playground with PyTorch \nI this notebook I have attempted to use PyTorch to get the best result, so far I have not managed to beat score obtained with Tree regressors, but it is a work in progress. \n"}}