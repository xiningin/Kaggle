{"cell_type":{"a37bb3dc":"code","5c4eec80":"code","a95f6af1":"code","2cc44ba9":"code","09d655c7":"code","69fa3760":"code","61f0f5d7":"code","1ae37ef2":"code","4de9b8d5":"code","4ba0764b":"code","de5fea79":"code","a6b03c84":"code","fdf119d3":"code","4a5d8cc8":"code","56e20785":"code","cc16bff7":"code","59bb0f5c":"code","e29e784f":"code","530d2916":"code","ae01196e":"code","870f7346":"code","101d68aa":"code","13036cc6":"code","da2d8014":"code","15470287":"code","26ed662a":"code","5a923844":"code","cc1ee40d":"code","bee8649b":"code","44ed0773":"code","ac553e33":"code","c897c9ce":"code","8e48a3da":"code","9536d257":"code","b0b8b192":"code","070f99f2":"code","cc4be3c2":"code","43c52f4d":"code","e092f723":"code","62459968":"code","b4979fbc":"code","01cfe8d7":"code","688f64f3":"code","a0288320":"code","7f72e16f":"code","e5cf75cd":"code","9fdfcb45":"code","c9132181":"code","6082065c":"markdown","6bb4b9e8":"markdown","7bcedbce":"markdown","ad522730":"markdown","e64a0bb4":"markdown","954b7efc":"markdown","bf079b99":"markdown","7e371ca7":"markdown","a9f33207":"markdown","563ffbea":"markdown","279f9af0":"markdown","48ffe024":"markdown","2122b86f":"markdown","a769ec60":"markdown","eb19fa14":"markdown","b86ec866":"markdown","a8f3c662":"markdown","5e4cc426":"markdown","25d4adb0":"markdown","3bf86625":"markdown","2d61edb0":"markdown","9dc482ff":"markdown"},"source":{"a37bb3dc":"#importing main libraries;\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#importing preprocessing features;\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#scaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\n\n#encoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n#imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n#importing algortihms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom catboost import CatBoostClassifier\n\n\n#pipeline\nfrom sklearn.pipeline import Pipeline\nfrom IPython.core.display import HTML\nfrom sklearn.utils import estimator_html_repr\n\n#outlier detection\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\n\n#metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc, make_scorer\n\n%matplotlib inline","5c4eec80":"import os\nprint(os.listdir(\"..\/input\"))","a95f6af1":"df = pd.read_csv('..\/input\/iba-ml2-mid-project\/train.csv', index_col='Id') #We don't really need the ID column to be found in the train dataset, therefore I setting it as index","2cc44ba9":"df_test = pd.read_csv('..\/input\/iba-ml2-mid-project\/test.csv', index_col='Id') #Same applies to the ID column of test dataset.","09d655c7":"import warnings\nwarnings.filterwarnings('ignore')  #just to ignore warnings related to graphs and other codes.","69fa3760":"df['credit_line_utilization'] = df['credit_line_utilization'].str.replace(',' , '.').astype(float) ","61f0f5d7":"df_test['credit_line_utilization'] = df_test['credit_line_utilization'].str.replace(',' , '.').astype(float)","1ae37ef2":"df.isna().sum()","4de9b8d5":"df.isna().sum().sum()","4ba0764b":"df_test['Debt Amount'] = df_test['monthly_income'] * df_test['ratio_debt_payment_to_income'] # as stated above, I am inserting\n# new column to test dataset as well.","de5fea79":"X = df.drop(columns=\"defaulted_on_loan\",axis=1)\ny = df[\"defaulted_on_loan\"]","a6b03c84":"X['Debt Amount'] = X['monthly_income'] * X['ratio_debt_payment_to_income'] #creating new column called \"Debt Amount\"","fdf119d3":"X['age_z_score'] = (X['age'] - X.age.mean()) \/ X.age.std()\nX['number_dependent_family_members_score'] = (X['number_dependent_family_members'] - X.number_dependent_family_members.mean()) \/ X.number_dependent_family_members.std()\nX['monthly_income_score'] = (X['monthly_income'] - X.monthly_income.mean()) \/ X.monthly_income.std()\nX['number_of_credit_lines_score'] = (X['number_of_credit_lines'] - X.number_of_credit_lines.mean()) \/ X.number_of_credit_lines.std()\nX['real_estate_loans_score'] = (X['real_estate_loans'] - X.real_estate_loans.mean()) \/ X.real_estate_loans.std()\nX['ratio_debt_payment_to_income_score'] = (X['ratio_debt_payment_to_income'] - X.ratio_debt_payment_to_income.mean()) \/ X.ratio_debt_payment_to_income.std()\nX['credit_line_utilization_score'] = (X['credit_line_utilization'] - X.credit_line_utilization.mean()) \/ X.credit_line_utilization.std()\nX['number_of_previous_late_payments_up_to_59_days_score'] = (X['number_of_previous_late_payments_up_to_59_days'] - X.number_of_previous_late_payments_up_to_59_days.mean()) \/ X.number_of_previous_late_payments_up_to_59_days.std()\nX['number_of_previous_late_payments_up_to_89_days_score'] = (X['number_of_previous_late_payments_up_to_89_days'] - X.number_of_previous_late_payments_up_to_89_days.mean()) \/ X.number_of_previous_late_payments_up_to_89_days.std()\nX['number_of_previous_late_payments_90_days_or_more_score'] = (X['number_of_previous_late_payments_90_days_or_more'] - X.number_of_previous_late_payments_90_days_or_more.mean()) \/ X.number_of_previous_late_payments_90_days_or_more.std()\nX['Debt Amount Score'] = (X['Debt Amount'] - X['Debt Amount'].mean()) \/ X['Debt Amount'].std() ","4a5d8cc8":"X.loc[X['age_z_score'] > 3, 'age'] = np.nan\nX.loc[X['number_dependent_family_members_score'] > 3, 'number_dependent_family_members'] = np.nan\nX.loc[X['monthly_income_score'] > 3, 'monthly_income'] = np.nan\nX.loc[X['number_of_credit_lines_score'] > 3, 'number_of_credit_lines'] = np.nan\nX.loc[X['real_estate_loans_score'] > 3, 'real_estate_loans'] = np.nan\nX.loc[X['ratio_debt_payment_to_income_score'] > 3, 'ratio_debt_payment_to_income'] = np.nan\nX.loc[X['credit_line_utilization_score'] > 3, 'credit_line_utilization'] = np.nan\nX.loc[X['number_of_previous_late_payments_up_to_59_days_score'] > 3, 'number_of_previous_late_payments_up_to_59_days'] = np.nan\nX.loc[X['number_of_previous_late_payments_up_to_89_days_score'] > 3, 'number_of_previous_late_payments_up_to_89_days'] = np.nan\nX.loc[X['number_of_previous_late_payments_90_days_or_more_score'] > 3, 'number_of_previous_late_payments_90_days_or_more'] = np.nan\nX.loc[X['Debt Amount Score'] > 3, 'Debt Amount'] = np.nan","56e20785":"X = X.drop(columns=['age_z_score', 'number_dependent_family_members_score','monthly_income_score','number_of_credit_lines_score','real_estate_loans_score','ratio_debt_payment_to_income_score','credit_line_utilization_score','number_of_previous_late_payments_up_to_59_days_score','number_of_previous_late_payments_up_to_89_days_score','number_of_previous_late_payments_90_days_or_more_score','Debt Amount Score'])","cc16bff7":"numeric = X.columns.to_list() #(All of our columns are numeric except target column)","59bb0f5c":"categorical = []  ","e29e784f":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=42)","530d2916":"# Imputer_num = SimpleImputer(strategy='median')\n# X_train[numeric] = Imputer_num.fit_transform(X_train[numeric])   #mean is the best","ae01196e":"# elliptic= EllipticEnvelope(contamination=0.01)\n# pred_outlier = elliptic.fit_predict(X_train)\n# mask = pred_outlier != -1\n\n# X_train_eli, y_train_eli = X_train.iloc[mask, :], y_train[mask]\n#  # Outliers are detected and filtered out.","870f7346":"# iso = IsolationForest(contamination=0.01)\n# outlier_pred = iso.fit_predict(X_train[numeric])\n# mask = outlier_pred != -1\n\n# X_train_iso, y_train_iso = X_train.iloc[mask, :], y_train[mask]\n#  # Outliers are detected and filtered out.","101d68aa":"categorical_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')), \n    ('one_hot_encoding', OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\nnumeric_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\ncolumn_transformer=ColumnTransformer(transformers=[\n    ('numeric', numeric_transformer, numeric),\n    ('categorical', categorical_transformer, categorical)\n])\n\nmodel_xgb=Pipeline(steps=[\n    ('preprocessing', column_transformer),\n    ('classifier', XGBClassifier(gamma=5, max_depth=3, min_child_weight=5, subsample=1.0, colsample_bytree=0.8))\n])                    \nmodel_xgb.fit(X_train, y_train)\nprint(classification_report(y_test, model_xgb.predict(X_test)))","13036cc6":"# param_space = {\n#         'classifier__min_child_weight': [1, 5, 10],\n#         'classifier__gamma': [0.5, 1, 1.5, 2, 5],\n#         'classifier__subsample': [0.6, 0.8, 1.0],\n#         'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n#         'classifier__max_depth': [3, 4, 5]\n# }","da2d8014":"# gridsearch=GridSearchCV(model_pipeline1,param_space,cv=StratifiedKFold()) #Utilized StratifiedKFold in gridsearch","15470287":"# gridsearch.fit(X_train, y_train)","26ed662a":"# final_model = gridsearch.best_estimator_\n# final_model.fit(X_train,y_train)","5a923844":"plt.style.use('fivethirtyeight')\nprediction = model_xgb.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, prediction)\nfig, ax = plt.subplots(figsize=(6,6))\nplt.plot(fpr, tpr, color='r')\n\nplt.plot(fpr, fpr, linestyle = 'dashed', color = 'b')\n\nplt.xlabel('False positive rate')\n\nplt.ylabel('True positive rate')\n\nAUROC = np.round(roc_auc_score(y_test, prediction), 6)\n\nplt.title(f'XGBoost Classifier ROC curve; AUROC: {AUROC}');\n\nplt.show()","cc1ee40d":"# y_pred=final_model1.predict_proba(test)[:,1]\n# pred=pd.DataFrame(y_pred)\n# test.reset_index(inplace=True)","bee8649b":"# dataset=pd.concat([test['Id'], pred], axis=1)\n# dataset.columns=['Id','Predicted']\n# dataset.to_csv('XGB5_SKF.csv',index=False)","44ed0773":"categorical_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')), \n    ('one_hot_encoding', OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\nnumeric_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\ncolumn_transformer=ColumnTransformer(transformers=[\n    ('numeric', numeric_transformer, numeric),\n    ('categorical', categorical_transformer, categorical)\n])\n\nmodel_rf=Pipeline(steps=[\n    ('preprocessing', column_transformer),\n    ('classifier', RandomForestClassifier(max_depth=2, random_state=0))\n])                    \nmodel_rf.fit(X_train, y_train)\nprint(classification_report(y_test, model_rf.predict(X_test)))","ac553e33":"plt.style.use('fivethirtyeight')\nprediction = model_rf.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, prediction)\nfig, ax = plt.subplots(figsize=(6,6))\nplt.plot(fpr, tpr, color='r')\n\nplt.plot(fpr, fpr, linestyle = 'dashed', color = 'b')\n\nplt.xlabel('False positive rate')\n\nplt.ylabel('True positive rate')\n\nAUROC = np.round(roc_auc_score(y_test, prediction), 6)\n\nplt.title(f'Random Forest Classifier ROC curve; AUROC: {AUROC}');\n\nplt.show()","c897c9ce":"categorical_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')), \n    ('one_hot_encoding', OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\nnumeric_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\ncolumn_transformer=ColumnTransformer(transformers=[\n    ('numeric', numeric_transformer, numeric),\n    ('categorical', categorical_transformer, categorical)\n])\n\nmodel_gb=Pipeline(steps=[\n    ('preprocessing', column_transformer),\n    ('classifier', GradientBoostingClassifier(n_estimators=300, learning_rate=.1, max_depth= 5))\n])                    \nmodel_gb.fit(X_train, y_train)\nprint(classification_report(y_test, model_gb.predict(X_test)))","8e48a3da":"plt.style.use('fivethirtyeight')\nprediction = model_gb.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, prediction)\nfig, ax = plt.subplots(figsize=(6,6))\nplt.plot(fpr, tpr, color='r')\n\nplt.plot(fpr, fpr, linestyle = 'dashed', color = 'b')\n\nplt.xlabel('False positive rate')\n\nplt.ylabel('True positive rate')\n\nAUROC = np.round(roc_auc_score(y_test, prediction), 6)\n\nplt.title(f'Gradient Boosting Classifier ROC curve; AUROC: {AUROC}');\n\nplt.show()","9536d257":"clf1 = RandomForestClassifier(max_depth=2, random_state=0)","b0b8b192":"clf2 = XGBClassifier(gamma=5, max_depth=3, min_child_weight=5, subsample=1.0, colsample_bytree=0.8)#using gridsearch best params","070f99f2":"clf3 = GradientBoostingClassifier(n_estimators=300, learning_rate=.1, max_depth= 5)","cc4be3c2":"categorical_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')), \n    ('one_hot_encoding', OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\nnumeric_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\ncolumn_transformer=ColumnTransformer(transformers=[\n    ('numeric', numeric_transformer, numeric),\n    ('categorical', categorical_transformer, categorical)\n])\n\nvoting_model=Pipeline(steps=[\n    ('preprocessing', column_transformer),\n    ('classifier', VotingClassifier(estimators=[('rf', clf1),('xgb', clf2),('gbc', clf3)], voting='soft', weights=[1,2,2]))\n])                    \nvoting_model.fit(X_train, y_train)\nprint(classification_report(y_test, voting_model.predict(X_test)))","43c52f4d":"plt.style.use('fivethirtyeight')\nprediction = voting_model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, prediction)\nfig, ax = plt.subplots(figsize=(6,6))\nplt.plot(fpr, tpr, color='r')\n\nplt.plot(fpr, fpr, linestyle = 'dashed', color = 'b')\n\nplt.xlabel('False positive rate')\n\nplt.ylabel('True positive rate')\n\nAUROC = np.round(roc_auc_score(y_test, prediction), 6)\n\nplt.title(f'Voting Classifier ROC curve; AUROC: {AUROC}');\n\nplt.show()","e092f723":"categorical_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')), \n    ('one_hot_encoding', OneHotEncoder(handle_unknown = \"ignore\"))\n])\n\nnumeric_transformer=Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\ncolumn_transformer=ColumnTransformer(transformers=[\n    ('numeric', numeric_transformer, numeric),\n    ('categorical', categorical_transformer, categorical)\n])\n\nmodel_cat=Pipeline(steps=[\n    ('preprocessing', column_transformer),\n    ('classifier', CatBoostClassifier(silent=True, iterations=1500, learning_rate=0.01,loss_function='Logloss', rsm=0.95, l2_leaf_reg=3.5, depth=5, eval_metric='AUC',random_seed=42))\n])  \n\nmodel_cat.fit(X_train, y_train)\nprint(classification_report(y_test, model_cat.predict(X_test)))","62459968":"plt.style.use('fivethirtyeight')\nprediction = model_cat.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, prediction)\nfig, ax = plt.subplots(figsize=(6,6))\nplt.plot(fpr, tpr, color='r')\n\nplt.plot(fpr, fpr, linestyle = 'dashed', color = 'b')\n\nplt.xlabel('False positive rate')\n\nplt.ylabel('True positive rate')\n\nAUROC = np.round(roc_auc_score(y_test, prediction), 6)\n\nplt.title(f'Voting Classifier ROC curve; AUROC: {AUROC}');\n\nplt.show()","b4979fbc":"# model_cat.get_params()","01cfe8d7":"# param_space = {\n#         'classifier__iterations': [1000, 1500, 2000],\n#         'classifier__depth': [3, 5, 7],\n#         'classifier__l2_leaf_reg': [2, 2.5, 3.5],\n#         'classifier__learning_rate': [0.01, 0.02, 0.1],\n#         'classifier__rsm': [0.9, 0.95]\n# }","688f64f3":"# gridsearch=GridSearchCV(model_cat,param_space,cv=StratifiedKFold()) ","a0288320":"# gridsearch.fit(X_train, y_train)","7f72e16f":"# final_model = gridsearch.best_estimator_\n# final_model.fit(X_train,y_train)","e5cf75cd":"test = df_test","9fdfcb45":"y_pred=model_cat.predict_proba(test)[:,1]\npred=pd.DataFrame(y_pred)\ntest.reset_index(inplace=True)","c9132181":"dataset=pd.concat([test['Id'], pred], axis=1)\ndataset.columns=['Id','Predicted']\ndataset.to_csv('CatBoostFinal_Model6.csv',index=False)","6082065c":"<i> <font size='4'> There were comma(',') instead of dot('.') in the credit_line_utilization column in both test and train datasets, therefore I cleaned these and changed the datatype to float.<\/font>","6bb4b9e8":"<font size='4' >Best, result was gained on <font color='red'>CatBoostClassifier() <\/font>algorithm, after doing hyperparameter tuning with gridsearch. ","7bcedbce":"### Conclusions","ad522730":"## Voting Classifier ","e64a0bb4":"### <font size='5'> Outlier Handling","954b7efc":"## RandomForestClassifier","bf079b99":"### Submission Prediction","7e371ca7":"## GradientBoostingClassifier","a9f33207":"<i><font size='4'> <font color='red'> To increase model accuracy <\/font>, I am adding <font color='red'> new column (Debt Payment) <\/font> which is created with Monthly_Income and Ratio_debt_payment_to_income columns. For exporting output from the test dataset, I have to add this new column to the test dataset as well, otherwise I will get an 'error'. By adding this column I increased my accuracy score.","563ffbea":"<i> <font size='4'>I tried 2 types of Outlier detection, but none of them gave better result than the manual outlier removal as implemented above.","279f9af0":"Inside, of the Pipeline, I used Simple Imputation with 'median' strategy and StandardScaler()","48ffe024":" <font color='purple'><h1><center>Model Building for [Step Project](https:\/\/www.kaggle.com\/c\/iba-ml2-mid-project)<\/center><\/h1>","2122b86f":"### <font size='5'> Model Fitting","a769ec60":"<i> <font size='4'> I did run GridSearch for hyperparameter tuning of XGBoost Classifier, It took a lot of computing power and time, therefore, I am directly using the best paramteres above and gridsearch part will be commented out.","eb19fa14":"First, I have tried several methods for Outlier Detection, none of them increased my accuracy as replacing outlier values with 'NaN' based on z-score method. Other than that, I introduced new column called Debt Payment, after thinking a bit, I realized given 'monthly_income' and 'Ratio_debt_payment_to_income' columns, we can calculate the total debt payment. Overall, introducing new column, increased my score.","b86ec866":"This gridsearch is taking a lot of computing power :) took me to run for more than 1 hour.","a8f3c662":"## XGBClassifier","5e4cc426":"I did run my model several algorithms:\n\n- XGBClassifier()\n- RandomForestClassifier()\n- GradientBoostingClassifier()\n- VotingClassifier()\n- CatBoostClassifier()","25d4adb0":"<i><font size='3.5'>For each column, I calculated z-score and changed value of the rows of column to 'NaN' whereas z-score is more than 3 which we can call Outliers. Later, inside of the pipeline, these NaN values will be imputed, which means we are not losing any rows with dropping outlier values.","3bf86625":"I used ROC AUC curve as the evaluation metric along with Classification Report which includes F1 ,Precision, Recall scores.","2d61edb0":"## CatBoost Classifier","9dc482ff":"<i>I commented out the gridsearch parts as they are taking a lot computer power and time to run. I just added the best paramters to the models inside pipeline."}}