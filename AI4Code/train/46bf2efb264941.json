{"cell_type":{"18c903c2":"code","8e23b3a0":"code","72e8f1f3":"code","b72c726b":"code","e6d7ab2b":"code","202b8238":"markdown","0ba8ac2c":"markdown"},"source":{"18c903c2":"from tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback\nimport json\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\nimport pickle\nimport keras\n\n\n#%% - load data - \ntrain_pickle_file = '..\/input\/pickling\/train.csv.pandas.pickle'\ntrain = pickle.load(open(train_pickle_file, 'rb'))\n\nfeatures = [c for c in train.columns if \"feature\" in c]\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","8e23b3a0":"#%% - build model - \n\ndef create_resnet(n_features, n_labels, learning_rate, label_smoothing, adam_decay):\n    inp = Input(shape=(n_features,), name='Input')\n    dropout_rate = 0.2\n    hidden_size = 256\n    \n    x = BatchNormalization()(inp)\n    x = Dropout(dropout_rate)(x)\n    \n    head_1 = tf.keras.Sequential([\n        Dense(hidden_size),\n        BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.01),\n        #tf.keras.layers.PReLU(),\n        Dropout(dropout_rate)],\n        name='Head1')\n    \n    x1 = head_1(x)\n    \n    x = Concatenate()([x,x1])\n    \n    head_2 = tf.keras.Sequential([\n        Dense(hidden_size),\n        BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.01),\n        #tf.keras.layers.PReLU(),\n        Dropout(dropout_rate),\n    ], name='Head2')\n    \n    x2 = head_2(x)\n    \n    x = Concatenate()([x1, x2])\n    \n    head_3 = tf.keras.Sequential([\n        Dense(hidden_size),\n        BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.01),\n        #tf.keras.layers.PReLU(),\n        Dropout(dropout_rate)\n    ], name='Head3')\n    \n    x3 = head_3(x)\n    x = Concatenate()([x2, x3])\n    \n    head_4 = tf.keras.Sequential([\n        Dense(hidden_size),\n        BatchNormalization(),\n        tf.keras.layers.LeakyReLU(0.01),\n        #tf.keras.layers.PReLU(),\n        Dropout(dropout_rate),\n    ], name='Head4')\n    x4 = head_4(x)\n    x = Concatenate()([x3, x4])\n    \n    output = Dense(n_labels)(x)\n    output = tf.keras.layers.Activation(\"sigmoid\")(output)\n    model = tf.keras.models.Model(inputs=inp, outputs=output)\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate, decay=adam_decay),\n                   loss=tf.keras.losses.BinaryCrossentropy(\n                       label_smoothing=label_smoothing),\n                   metrics=[tf.keras.metrics.AUC(name=\"auc\"), 'accuracy']\n                   )\n    #model.compile(optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n    #              loss=tf.keras.losses.BinaryCrossentropy(\n    #                  label_smoothing=label_smoothing),\n    #              metrics=[tf.keras.metrics.AUC(name=\"auc\"),'accuracy']\n    #              )\n    return model","72e8f1f3":"#%% - train test data split - \n\ntrain = train.query('date > 85').reset_index(drop=True)\n\n# without drop weight==0\ntrain.fillna(train.mean(), inplace=True)\n\n#f_mean = np.mean(train[features[1:]].values,axis=0)\nf_mean = train.mean()\n\ntrain['action'] = (train['resp'] > 0).astype('int')\ntrain['action_1'] = (train['resp_1'] > 0).astype('int')\ntrain['action_2'] = (train['resp_2'] > 0).astype('int')\ntrain['action_3'] = (train['resp_3'] > 0).astype('int')\ntrain['action_4'] = (train['resp_4'] > 0).astype('int')\n\nvalid = train.loc[(train.date >= 450) & (\n    train.date < 500)].reset_index(drop=True)\ntrain = train.loc[train.date < 450].reset_index(drop=True)\n\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\nfeat_cols = [f'feature_{i}' for i in range(130)]\nall_feat_cols = [col for col in feat_cols]\n\ntrain['cross_41_42_43'] = train['feature_41'] + \\\n    train['feature_42'] + train['feature_43']\ntrain['cross_1_2'] = train['feature_1'] \/ (train['feature_2'] + 1e-5)\nvalid['cross_41_42_43'] = valid['feature_41'] + \\\n    valid['feature_42'] + valid['feature_43']\nvalid['cross_1_2'] = valid['feature_1'] \/ (valid['feature_2'] + 1e-5)\n\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])","b72c726b":"#%% - define call back to evalute score on each epoch - \nclass TrainingEva(Callback):\n    def __init__(self, test, state_dir, fold):\n        super(TrainingEva, self).__init__()\n        # test = [X,resp,w,date]\n        self.test_X = test[0]\n        self.test_resp = test[1]\n        self.test_w = test[2]\n        self.test_date = test[3]\n        self.th = 0.5\n        self.log_dir = state_dir+f'\/logs_{fold}.json'\n        self.history = {'epoch': [], 'score': []}\n\n    def score(self, date, weight, resp, action):\n        count_i = len(np.unique(date))\n        Pi = np.bincount(date, weight * resp * action)\n        t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n        u = np.clip(t, 0, 6) * np.sum(Pi)\n        return u\n\n    def on_epoch_end(self, epoch, logs=None):\n        test_pred = self.model.predict(self.test_X)[:, 0]\n        test_action = np.where(test_pred >= self.th, 1, 0).astype(int)\n        test_u_score = self.score(\n            self.test_date, self.test_w, self.test_resp, test_action)\n\n        self.history['epoch'].append(epoch)\n        self.history['score'].append(test_u_score)\n\n        print(\"fold:{} epoch:{} val_score:{}\".format(fold, epoch, test_u_score))\n\n        with open(self.log_dir, 'w') as f:\n            json.dump(self.history, f)","e6d7ab2b":"#%% - train model - \nSEED = 42\nLABEL_SMOOTHING = 0.005\nBATCH_SIZE = 8192\nEPOCHS = 200\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLYSTOP_NUM = 3\nNFOLDS = 5\n\noutpath = '.\/'\nTRAINING = True\nif TRAINING:\n    for fold in range(NFOLDS):\n        #  - seed -\n        tf.random.set_seed(SEED+fold)\n        np.random.seed(SEED+fold)\n        # 1. prepare data\n        X_train = train[all_feat_cols].values\n        y_train = train[target_cols].values\n\n        X_valid = valid[all_feat_cols].values\n        y_valid = valid[target_cols].values\n        \n        resp_valid = valid['resp'].values\n        w_valid = valid['weight'].values\n        date_valid = valid['date'].values\n\n        # 2. create model\n        tf.keras.backend.clear_session()\n        clf = create_resnet(len(all_feat_cols), len(target_cols), LEARNING_RATE,\n                            LABEL_SMOOTHING, WEIGHT_DECAY)\n\n        # 3. create callback\n        esCallBack = EarlyStopping(\n            'val_auc', mode='max', patience=EARLYSTOP_NUM, restore_best_weights=(False))\n\n        filepath = '.\/'+outpath+'\/weights-improvement-{epoch:02d}-{val_auc:.2f}' + \\\n            f'-fold:{fold}.hdf5'\n        checkpoint = ModelCheckpoint(\n            filepath, verbose=0, save_best_only=(True), mode='max')\n\n        evaCallBack = TrainingEva([X_valid, resp_valid, w_valid, date_valid],\n                                  '.\/'+outpath, fold)\n\n        # 4. train model\n        history = clf.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n                          epochs=EPOCHS, batch_size=BATCH_SIZE,\n                          callbacks=[esCallBack, checkpoint, evaCallBack])\n        # 5. save log -\n        auc = history.history['auc']\n        val_auc = history.history['val_auc']\n        loss_score = history.history['loss']\n        val_loss_score = history.history['val_loss']\n        history_df = pd.DataFrame(\n            {'AUC': auc, 'val_AUC': val_auc, 'loss': loss_score, 'val_loss': val_loss_score})\n        history_df.to_csv('.\/'+outpath+f'\/history_df_{fold}.csv')\n\n        # - save model -\n        clf.save('.\/'+outpath+f'\/model_final_{fold}.h5')","202b8238":"# Intro\n\nHi guys, I am ML novice and I would like to first give many thanks to kagglers who share their awesome notebooks. They help me learn a lot!\n\nThis notebook is credit to a famous public pytoch resNet Notebook shared by Lin.   \nhttps:\/\/www.kaggle.com\/a763337092\/blending-tensorflow-and-pytorch  \nhttps:\/\/www.kaggle.com\/a763337092\/neural-network-starter-pytorch-version  \nhttps:\/\/www.kaggle.com\/a763337092\/pytorch-resnet-starter-training  \n\nI am more familiar with keras, so I replicated it in Keras to further tuning the model. I share the model with you guys for your own testing. \n\n\n# My questions\nFor me, I met these problems while tuning the model, maybe someone can help to advice? Thanks -\n\n(1) The performance of this keras model got similar performance as the pytorch model in the following link. But it is worse than the submitted model shared by Lin.   \nhttps:\/\/www.kaggle.com\/a763337092\/pytorch-resnet-starter-training  \n\n-> Can anyone advice how should I change parameters to train the model better? I changed the EARLYSTOP_NUM to 20, the performance improves around 20%, but still not as good as the public one.\n\n(2) In the public model, Lin blends 5 pytorch resNet model and one tf model. But when I submit the keras resNet model, 2 resNet models is the maximum even after I speed up other process. \nhttps:\/\/www.kaggle.com\/a763337092\/blending-tensorflow-and-pytorch  \n\n-> Is it because Keras model naturally runs slower than pytorch model? Is there any method to speed up the inference?\n\nThanks!\n","0ba8ac2c":"That's it!"}}