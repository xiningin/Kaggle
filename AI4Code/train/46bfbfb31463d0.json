{"cell_type":{"b1090372":"code","f78a2d85":"code","90ea9a27":"code","cb54d879":"code","a8aae491":"code","2f155aeb":"code","517f9faf":"code","ba8a336c":"code","95761bfe":"code","27895c8e":"code","84896e5d":"code","e8162d7a":"code","54012880":"code","9e2b8bd7":"code","f720306f":"code","c8660f75":"code","8d8e0431":"code","c21b7e05":"code","aff560c3":"code","e20a7efb":"code","18e0ec84":"code","17a4ab74":"code","de48f725":"code","501527fd":"code","7356f25a":"code","3979642a":"code","1b4ecbe2":"code","e934efc6":"code","4a54920a":"code","c2ae9924":"code","b2bf5a08":"code","53cc276f":"code","2e6702da":"code","33e8e081":"code","ea163684":"code","12492d9a":"code","25b432b7":"code","d472af3c":"code","40dec793":"code","49093203":"code","a088a96c":"code","82c5c7e0":"code","ca2162fd":"code","8748a30b":"code","485d9e75":"code","e26d3702":"code","536a8131":"code","0cdda084":"code","72066b51":"code","88c5d95d":"code","be530ffe":"code","902e2699":"code","3da6c088":"code","3798ad02":"code","4920ea0a":"code","aed8aab6":"code","8e91a824":"code","cfb47de5":"code","cd9d0ab9":"code","580231b1":"code","e0d55e9f":"code","d88789dd":"code","6240dd4a":"code","191fe87f":"code","4b805e09":"code","15000e6c":"code","45acfdd5":"code","bc090b0b":"code","c2f78d64":"code","c725b0a1":"code","165c0bd6":"code","9c5b1610":"code","d0116bd6":"code","b4472e5d":"code","d2bb0a8b":"code","19488873":"code","886557b6":"code","398895ce":"code","fab62e53":"code","2e716de7":"code","ec452c6b":"code","cdf24e8e":"code","de528ab1":"code","9a6c9d4d":"code","a3c8b169":"code","d1ad9588":"code","3c3cbdb0":"code","f5ff3552":"code","4a17758d":"code","94226a79":"code","ab71e741":"code","e6be1758":"code","d9c18744":"code","c8e81c15":"code","f2224b24":"code","4203163d":"code","10f75eec":"code","e76cb5a0":"code","526991c7":"code","dfec8743":"code","60326524":"code","98e28693":"code","614d1549":"code","d339df5c":"code","3747eb13":"code","90e27eea":"code","c0a4abaa":"code","2eb57032":"code","6dbf1f87":"code","df8a7577":"code","b9261273":"code","70cc5132":"code","c28fa43b":"code","ec50f412":"code","cafcd0c4":"code","8def5327":"code","268b985d":"code","6ec50a69":"code","795cbd4f":"code","eebfce24":"code","c30674ca":"code","fcfb25a9":"code","fce0e4ba":"code","d14f2dd6":"code","bae895de":"code","4d342d99":"code","6b848718":"code","69e57da7":"code","eeb100d4":"code","f8b3cd5c":"code","abd94c46":"code","955b656c":"code","95832b64":"code","2551fa19":"code","66f3dee2":"code","249253ef":"code","46104231":"code","e7e1dadc":"code","afeb7f39":"code","0798563a":"code","1cd58f6b":"code","7f29d88a":"code","efea3546":"code","f6bed8a5":"code","82783e56":"code","23aa2163":"code","09d7e0af":"code","9fa587b8":"code","6ef32e2f":"code","e12f19c1":"code","2be985ac":"code","d9a8bb89":"code","e5c4a20c":"code","0430979f":"code","f4f325f2":"code","33b0c7bd":"code","da689e78":"code","76aa0cf8":"code","5c084fe5":"code","5e92314a":"code","4d69db1b":"code","0c9dcc87":"code","eb5e8fb5":"code","d2b35973":"code","59b53979":"code","f5917de2":"code","80e79f73":"code","b8c75045":"code","a9253f44":"code","72838769":"code","adefb70e":"code","81a9e3d5":"code","50c72107":"code","21d245ce":"code","1ee3b829":"code","433c2926":"code","14aa8bfd":"code","d6581d04":"code","9026cd94":"code","bc41aadf":"code","570ed05a":"code","a510acba":"code","c6a5296f":"code","b6d7dd1e":"code","fda7547f":"code","d4702be2":"code","95bcbf17":"code","549f1291":"code","5b9c659d":"code","e720da7c":"code","1103e993":"code","ca7cde93":"code","4d6d1824":"code","35c03013":"code","312d44ce":"code","4f8ac7b8":"code","8af807b9":"code","a77bdf39":"code","e13bb5de":"code","2b648a57":"code","e4455aad":"code","e5910755":"code","712727e0":"code","450756c9":"code","8324ca24":"code","e7302a4a":"code","bac91fff":"code","3f369c57":"code","e4946e59":"code","26c14ea0":"code","94ef49d4":"code","ba38488e":"code","d894cb78":"code","9788aec1":"code","76f06995":"code","1724a65d":"code","5f3c7d7f":"code","f2607510":"code","36d66e45":"code","5988fe5b":"code","448fff6e":"code","f52390c7":"code","dec7f17f":"code","6779ce63":"code","492015cb":"code","105b8737":"code","a4e3ee74":"code","8bd808df":"code","ca60b5e1":"code","2e0c1a47":"code","9dd3fd49":"code","602e8165":"code","022c414f":"code","98502df1":"code","523cbdb8":"code","763186e6":"code","2dc7deb0":"code","821fc783":"code","af2a5b0f":"code","33ac1cee":"markdown","6bb43c04":"markdown","ee1afdb0":"markdown","aefffd74":"markdown","d0bcba9d":"markdown","339688a6":"markdown","0609ebb1":"markdown","afac344e":"markdown","46193f49":"markdown","062ece9e":"markdown","3d55e4a6":"markdown","e3506e6d":"markdown","43c49245":"markdown","1b9d2f41":"markdown","0571ecc1":"markdown","b4558e75":"markdown","85e03f6a":"markdown","6afdbee9":"markdown","93f15799":"markdown","f8ef9708":"markdown","772edfd2":"markdown","9c31a4b7":"markdown","3bed011c":"markdown","3c46ec82":"markdown","2ba1ee80":"markdown","4bd32d59":"markdown","f7fe1ec0":"markdown","7cbe4caf":"markdown","48258f6d":"markdown","860b68b1":"markdown","e3d53667":"markdown","c77b58d1":"markdown","d93f26b9":"markdown","4b0228cf":"markdown","f55f82d8":"markdown","ee9fadbc":"markdown","cecbbb17":"markdown","f60c29bf":"markdown","6192230a":"markdown","3f31f2ce":"markdown","3f0201aa":"markdown","5956e3b6":"markdown","3669a1b8":"markdown","728b7cfd":"markdown","14622a37":"markdown","186315f5":"markdown","01f60058":"markdown","4cf6da57":"markdown","00f5bdae":"markdown"},"source":{"b1090372":"#importing necessary packages and data ( The Data in csv format is downloaded )\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","f78a2d85":"#reading the data\ndf_telecom = pd.read_csv('..\/input\/telecom-churn-data')\ndf_telecom.head()","90ea9a27":"#alter the pandas for better visualization\npd.set_option('max_columns', 400)\npd.set_option('max_rows',400)","cb54d879":"#check the size of the data\ndf_telecom.shape","a8aae491":"#validating the data\ndf_telecom.describe()","2f155aeb":"#validating the column name and type\ndf_telecom.info(verbose=1)","517f9faf":"#Check the percentage of null values in the columns\ndf_telecom.isnull().sum() * 100 \/ len(df_telecom)","ba8a336c":"# missingno is used to identify missing values in the dataset \n\nimport missingno as msno\n\nmsno.matrix(df_telecom)","95761bfe":"#Before performing this operation let is check whether the data as null value or not\ndf_telecom[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].isnull().sum() * 100 \/ len(df_telecom[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']])","27895c8e":"# As the data is not null let us proceed to create the column and tag the user is churned or not\ndf_telecom['Churn'] =  ((df_telecom['total_ic_mou_9']==0.0) & (df_telecom['total_og_mou_9']==0.0) & (df_telecom['vol_2g_mb_9']==0.0) & (df_telecom['vol_3g_mb_9']==0.0)).astype(int)","84896e5d":"df_telecom.head()","e8162d7a":"# Count churn and non churn data in percentage\ndf_telecom.Churn.value_counts(1)*100","54012880":"#let us create column for total data recharged amount\n#total data recharge amount = total_rech_data *av_rech_amt_data\n#let us check if the data is null or not\ndf_telecom[['av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_8','total_rech_data_9']].isnull().sum() * 100 \/ len(df_telecom[['av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_8','total_rech_data_9']])","9e2b8bd7":"# There are null values in  recharge amount we can impute them with '0'\ndf_telecom[['av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_8','total_rech_data_9']]=df_telecom[['av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_8','total_rech_data_9']].fillna(0, axis=1)","f720306f":"#revalidation\ndf_telecom[['av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_8','total_rech_data_9']].isnull().sum() * 100 \/ len(df_telecom[['av_rech_amt_data_6','av_rech_amt_data_7','total_rech_data_6','total_rech_data_7','av_rech_amt_data_8','av_rech_amt_data_9','total_rech_data_8','total_rech_data_9']])","c8660f75":"#creating column for total data recharged amount\nfor month in [6,7,8,9]:\n    df_telecom['total_data_recharged_amt_'+str(month)] = df_telecom['total_rech_data_'+str(month)] * df_telecom['av_rech_amt_data_'+str(month)]","8d8e0431":"df_telecom.head()","c21b7e05":"df_telecom['ph_good_rech'] = (df_telecom['total_rech_amt_6'] + df_telecom['total_rech_amt_7']+df_telecom['total_data_recharged_amt_6']+df_telecom['total_data_recharged_amt_7'])\/2\ndf_telecom['ph_action_rech'] = df_telecom['total_rech_amt_8']+df_telecom['total_data_recharged_amt_8']\ndf_telecom['ph_churn_rech'] = df_telecom['total_rech_amt_9']+df_telecom['total_data_recharged_amt_9']","aff560c3":"#to get the high value customer\nprint('70th percentile average recharge amount :' ,df_telecom['ph_good_rech'].quantile(.70))","e20a7efb":"# lets filter out the high-value customers\ndf_high_value =  df_telecom.loc[df_telecom.ph_good_rech >=df_telecom['ph_good_rech'].quantile(.70)]","18e0ec84":"# we are left with  around 30k records which is almost the same as the value mentioned in the description (29.9k records)\ndf_high_value.shape\n","17a4ab74":"#we have already tagged the churners let us now drop the columns corresponding to churn phase\ndf_high_value.drop([col for col in df_high_value.columns if '_9' in col],axis=1,inplace=True)","de48f725":"#validate the columns\ndf_high_value.columns","501527fd":"#count churn and non churn data in %\ndf_high_value.Churn.value_counts(1)*100","7356f25a":"#check for unique values\ndf_high_value.nunique().sort_values()","3979642a":"#let us check the columns with unique values\nunique_dataset=pd.DataFrame(df_high_value.nunique())","1b4ecbe2":"# Validating the data with only constant value\ndf_high_value[unique_dataset[unique_dataset[0]==1].index].describe()","e934efc6":"#let us drop these columns\ndf_high_value.drop(columns=unique_dataset[unique_dataset[0]==1].index,inplace=True)","4a54920a":"# Validation\ndf_high_value.nunique().sort_values()","c2ae9924":"#validating the columns with only 2 unique values\ndf_high_value[unique_dataset[unique_dataset[0]==2].index].isnull().sum()","b2bf5a08":"# lets us find categorical columns\n# From data dictionary these columns seemed to be categorical.\n# Let us find out the unique values in them","53cc276f":"for col in unique_dataset[unique_dataset[0]==2].index:\n    print(df_high_value[col].value_counts())","2e6702da":"#let us populate the null data by -1 to indicate the nightpack and fb usgae of the user is unkown\ndf_high_value[unique_dataset[unique_dataset[0]==2].index]=df_high_value[unique_dataset[unique_dataset[0]==2].index].fillna(-1, axis=1)","33e8e081":"#validation \ndf_high_value.nunique().sort_values()","ea163684":"#let us drop the mobile_number column as all the value is unqiue\ndf_high_value.drop(['mobile_number'],axis=1,inplace=True)","12492d9a":"#Check the percentage of null values in the columns\nnull_dataset=pd.DataFrame(df_high_value.isnull().sum() * 100 \/ len(df_high_value))","25b432b7":"#let us check the columns with more than 40% null data\nnull_dataset[null_dataset[0]>40]","d472af3c":"df_high_value[null_dataset[null_dataset[0]>40].index].info()","40dec793":"# For the categorical data we can populate the mode\n# Let us take the list of categorical data\ndf_high_value[null_dataset[null_dataset[0]>0].index].select_dtypes(include='object').columns","49093203":"# Imputing missing values with mode\nfor col in df_high_value[null_dataset[null_dataset[0]>0].index].select_dtypes(include='object').columns:\n    print(col)\n    print(df_high_value[col].mode()[0])\n    df_high_value[col].fillna(df_high_value[col].mode()[0], inplace=True)","a088a96c":"# Let us convert the datatype for these columns\nfor col in df_high_value[null_dataset[null_dataset[0]>0].index].select_dtypes(include='object').columns:\n    df_high_value[col]=pd.to_datetime(df_high_value[col])","82c5c7e0":"#Now lets identify the number of days between recharge in months 6-7 and 7-8\ndf_high_value['ndays_bw_rech_6_7']=(df_high_value['date_of_last_rech_7']-df_high_value['date_of_last_rech_6']).apply(lambda x:int(x.days))\ndf_high_value['ndays_bw_rech_7_8']=(df_high_value['date_of_last_rech_8']-df_high_value['date_of_last_rech_7']).apply(lambda x:int(x.days))","ca2162fd":"# Verify the data set post column creation\ndf_high_value.head()","8748a30b":"#let us take the list of categorical data\ndf_high_value[null_dataset[null_dataset[0]>40].index].select_dtypes(include='float64').describe()","485d9e75":"#count_rech 2g\/3g is handeled in total_rech_data itself for each month, we can drop this data\n#validation:\ndf_high_value[['total_rech_data_6','total_rech_data_7','total_rech_data_8','count_rech_2g_6','count_rech_2g_7','count_rech_2g_8','count_rech_3g_6','count_rech_3g_7','count_rech_3g_8']]","e26d3702":"#let us drop these count_rech 2g\/3g column \ndf_high_value.drop(columns=['count_rech_2g_6','count_rech_2g_7','count_rech_2g_8','count_rech_3g_6','count_rech_3g_7','count_rech_3g_8'],inplace=True)","536a8131":"# argpu : Average revenue per user\n# let us populate 0 for all the null values\ndf_high_value[['arpu_3g_6','arpu_3g_7','arpu_3g_8','arpu_2g_6','arpu_2g_7','arpu_2g_8']]=df_high_value[['arpu_3g_6','arpu_3g_7','arpu_3g_8','arpu_2g_6','arpu_2g_7','arpu_2g_8']].fillna(0, axis=1)","0cdda084":"# let us check if the max_rech_data has any 0 value or not\ndf_high_value[(df_high_value['max_rech_data_6']==0) | (df_high_value['max_rech_data_7']==0) | (df_high_value['max_rech_data_8']==0)]","72066b51":"# imputing with 0 as minimum recharge amount is 1. So null values means no recharge or zero recharge\n\ndf_high_value[['max_rech_data_6','max_rech_data_7','max_rech_data_8']]=df_high_value[['max_rech_data_6','max_rech_data_7','max_rech_data_8']].fillna(0,axis=1)","88c5d95d":"#Check the percentage of null values in the columns\ndf_high_value.isnull().sum() * 100 \/ len(df_high_value)","be530ffe":"#all other data are less than 5%, let us drop the rows\n#before that let us take the count\ndf_high_value.shape","902e2699":"#dropping the rows with na values\ndf_high_value.dropna(inplace=True)","3da6c088":"df_high_value.shape","3798ad02":"28504\/30001\n#5% data is dropped","4920ea0a":"#Check the percentage of null values in the columns\ndf_high_value.isnull().sum() * 100 \/ len(df_high_value)","aed8aab6":"# Finished cleaning the data\n#let us check churn count\n#count churn and non churn data in %\ndf_high_value.Churn.value_counts(1)*100","8e91a824":"df_high_value.shape","cfb47de5":"#count churn and non churn data in number\ndf_high_value.Churn.value_counts()","cd9d0ab9":"# Categorical and Numerical \n\nnumerical_feats = df_high_value.dtypes[(df_high_value.dtypes =='float64') | (df_high_value.dtypes =='int64')].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_high_value.dtypes[df_high_value.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))\n#this is not helpful","580231b1":"#let us analyze the churn value\nsns.histplot(data=df_high_value, x=\"Churn\", hue=\"Churn\")","e0d55e9f":"# Let us define some functions that create plots\ndef uni(col):\n    \n    if col.dtype == np.int64 or col.dtype == np.float64:\n        sns.distplot(col)\n        \n    \n    elif col.dtype == 'category':\n        sns.countplot(col)","d88789dd":"uni(df_high_value.arpu_6)","6240dd4a":"uni(df_high_value.arpu_7)","191fe87f":"uni(df_high_value.arpu_8)","4b805e09":"uni(df_high_value.total_og_mou_6)","15000e6c":"uni(df_high_value.total_og_mou_7)","45acfdd5":"uni(df_high_value.total_og_mou_8)","bc090b0b":"uni(df_high_value.fb_user_6)","c2f78d64":"uni(df_high_value.fb_user_7)","c725b0a1":"uni(df_high_value.fb_user_8)","165c0bd6":"#let us analyze the churn data based on age on network\nsns.histplot(data=df_high_value, x=\"aon\", hue=\"Churn\")","9c5b1610":"#let us analayze the churn data based on recharge frequency\nGBR=[col for col in df_high_value.columns if 'ndays_bw_rech' in col]\nnr_rows = 2\nnr_cols = 2\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(GBR):\n            sns.boxplot(y=GBR[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","d0116bd6":"#let us analyze the anual return per customer in each month\narpu=['arpu_6','arpu_7','arpu_8']\nnr_rows = 2\nnr_cols = 2\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(arpu):\n            sns.boxplot(y=arpu[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","b4472e5d":"#analysis on -  On Network Minutes of Usage and Off Network Minutes of Usage :\nnet_mou=[col for col in df_high_value.columns if 'onnet_mou' in col]+[col for col in df_high_value.columns if 'offnet_mou' in col]\nnr_rows = 2\nnr_cols = 3\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(net_mou):\n            sns.boxplot(y=net_mou[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","d2bb0a8b":"#analysis on outgoing and incoming minutes of usage operator wise\n#T2T    \tOperator T to T, i.e. within same operator (mobile to mobile)\n#T2M    \tOperator T to other operator mobile\n#T2O    \tOperator T to other operator fixed line\n#T2F    \tOperator T to fixed lines of T\n#T2C    \tOperator T to it\u2019s own call center\n\noperator_wise=[col for col in df_high_value.columns if 'og_t' in col]+[col for col in df_high_value.columns if 'ic_t' in col]\nnr_rows = 13\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(operator_wise):\n            sns.boxplot(y=operator_wise[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","19488873":"#analysis on total incoming\/outgoing value:\ntotal=[col for col in df_high_value.columns if 'total_ic_mou' in col]+[col for col in df_high_value.columns if 'total_og_mou' in col]\nnr_rows = 2\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(total):\n            sns.boxplot(y=total[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","886557b6":"#analysis on total number of recharge value:\ntotal_rech=[col for col in df_high_value.columns if 'rech_num' in col]\nnr_rows = 2\nnr_cols = 2\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(total_rech):\n            sns.boxplot(y=total_rech[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","398895ce":"#analysis on amount value:\namount=[col for col in df_high_value.columns if '_amt' in col]\nnr_rows = 5\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(amount):\n            sns.boxplot(y=amount[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","fab62e53":"#the above data is not helpful\n#let us go for 2g\/3g related column:\ng_column=[col for col in df_high_value.columns if ('_2g' )  in col] + [col for col in df_high_value.columns if ('_3g' )  in col]\nnr_rows = 9\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(g_column):\n            sns.boxplot(y=g_column[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\",showfliers=False)\n    \nplt.tight_layout()    \nplt.show() ","2e716de7":"#Let us validate the monthly 2g\/3g package and sanchet package\npackage=[col for col in df_high_value.columns if ('monthly' )  in col] + [col for col in df_high_value.columns if ('sachet' )  in col]\nnr_rows = 3\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*7,nr_rows*5))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(package):\n            sns.barplot(y=package[i], x=df_high_value.Churn, data=df_high_value, ax = axs[r][c],hue=\"Churn\")\n    \nplt.tight_layout()    \nplt.show()","ec452c6b":"#creating dummy variables\n#check for unique values\ndf_high_value.nunique().sort_values()","cdf24e8e":"#the columns we can consider for dummy creation\nunique_dataset=pd.DataFrame(df_high_value.nunique())\ndummy=list(unique_dataset[unique_dataset[0]==3].index)\ndummy","de528ab1":"#creating dummy variables from categorical variable\nfor col in dummy:\n    dummies = pd.get_dummies(df_high_value[col])\n    dummies = dummies.add_prefix(f'{col}_')\n    df_high_value = pd.concat([df_high_value, dummies], axis = 1)\n    df_high_value.drop([col], axis = 1, inplace = True)\n    \n#we have already created dummy variable, let us now drop the columns we created -1\ndf_high_value.drop([col for col in df_high_value.columns if '-1' in col],axis=1,inplace=True)","9a6c9d4d":"#validaitng the dataframe\ndf_high_value.head()","a3c8b169":"#let us check for correlation factor \n#plotting the heat map\nplt.figure(figsize=[30,30])\nsns.heatmap(df_high_value.corr(), annot=True)","d1ad9588":"#check the size before proceeding\ndf_ml_high_value=df_high_value.copy()\ndf_ml_high_value.shape","3c3cbdb0":"#as timestamp is causing issue during data scale let us convert it to float \ndf_ml_high_value[df_ml_high_value.select_dtypes(include='datetime64').columns]=df_ml_high_value[df_ml_high_value.select_dtypes(include='datetime64').columns].values.astype('float64')","f5ff3552":"#let us split the data\ny = df_ml_high_value.pop('Churn')\nX = df_ml_high_value","4a17758d":"#let us split the data for train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.3,random_state = 1)","94226a79":"scaler = StandardScaler()\n#let us scale the data\nnp.random.seed(0)\nX_train[:]=scaler.fit_transform(X_train[:])\nX_test[:]=scaler.transform(X_test[:])","ab71e741":"print('size')\nprint(X_train.shape)\nprint(X_test.shape)","e6be1758":"#let use SMOTE as suggested in discussion call\nfrom imblearn.over_sampling import SMOTE\n\nupsampler = SMOTE() \nX_train,y_train = upsampler.fit_resample(X_train,y_train)","d9c18744":"#validation\nprint('size')\nprint(X_train.shape)\nprint(y_train.shape)\n#let us analyze the churn value\nsns.histplot(data=y_train)","c8e81c15":"#let us Reduce the number of variables using PCA as suggested \nfrom sklearn.decomposition import PCA\n\npca = PCA(random_state=42)\npca.fit(X_train)","f2224b24":"pca.components_","4203163d":"#explaoned variance ratio for each components\npca.explained_variance_ratio_","10f75eec":"var_cumu = np.cumsum(pca.explained_variance_ratio_)\nvar_cumu","e76cb5a0":"fig = plt.figure(figsize=[12,8])\n#plt.vlines(x=75, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.95, xmax=150, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.grid()\nplt.show()","526991c7":"from sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=75)\ndf_X_train_pca = pca_final.fit_transform(X_train)\n#let us check the size\ndf_X_train_pca.shape","dfec8743":"corrmat = np.corrcoef(df_X_train_pca.transpose())\ncorrmat.shape","60326524":"#plotting the heat map\nplt.figure(figsize=[30,30])\nsns.heatmap(corrmat, annot=True)","98e28693":"#let us change the same for test data \ndf_X_test_pca = pca_final.transform(X_test)\ndf_X_test_pca.shape","614d1549":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogistic_learner_pca = LogisticRegression(C=1e9)\nlogistic_model_pca=logistic_learner_pca.fit(df_X_train_pca, y_train)\nlogistic_model_pca.get_params()","d339df5c":"#let us make the prediction on test set\npred_probs_test = logistic_model_pca.predict_proba(df_X_test_pca)","3747eb13":"from sklearn import metrics\n\"{:2.3}\".format(metrics.roc_auc_score(y_test, pred_probs_test[:,1]))","90e27eea":"#to create confusion matrix, let us make the prediction\npred_probs_train = logistic_model_pca.predict_proba(df_X_train_pca)\npred_probs_train","c0a4abaa":"y_train.shape\npred_probs_train.shape","2eb57032":"y_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':pred_probs_train[:,1]})\ny_train_pred_final['ID'] = y_train.index\ny_train_pred_final.head()","6dbf1f87":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","df8a7577":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","b9261273":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","70cc5132":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","c28fa43b":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ec50f412":"# Let us calculate specificity\nTN \/ float(TN+FP)","cafcd0c4":"# Calculate false postive rate - predicting churn when customer does not have churned\nFP\/ float(TN+FP)","8def5327":"# positive predictive value \nTP \/ float(TP+FP)","268b985d":"# Negative predictive value\nTN \/ float(TN+ FN)","6ec50a69":"#let us create the ROC curve\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","795cbd4f":"draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","eebfce24":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","c30674ca":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","fcfb25a9":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.grid()\nplt.show()","fce0e4ba":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.56 else 0)\n\ny_train_pred_final.head()","d14f2dd6":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nprint(confusion)","bae895de":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted))","4d342d99":"#precision\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","6b848718":"#recall\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","69e57da7":"#precision recall tradeoff\nfrom sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","eeb100d4":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","f8b3cd5c":"#let us make the prediction on test set\ny_test_pred_final = logistic_model_pca.predict_proba(df_X_test_pca)\ny_test_pred_final","abd94c46":"#preparing data  for confusion matrix\ny_pred_final = pd.DataFrame({'Churn':y_test.values, 'Churn_Prob':y_test_pred_final[:,1]})\ny_pred_final['ID'] = y_test.index\ny_pred_final.head()","955b656c":"\n\ny_pred_final['predicted'] = y_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.56 else 0)\n\ny_pred_final.head()","95832b64":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.predicted )\nprint(confusion)","2551fa19":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_pred_final.Churn, y_pred_final.predicted))","66f3dee2":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","249253ef":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","46104231":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred_final.Churn, y_pred_final.predicted))","e7e1dadc":"from sklearn.model_selection import GridSearchCV\nlogistic_learner_pca_tune = LogisticRegression(penalty='l2')","afeb7f39":"log_params = {'C':[0.1, 0.2,0.3, 0.4, 0.5, 1, 2]}\nfolds=5\nlog_model_cv = GridSearchCV(estimator = logistic_learner_pca_tune, \n                        param_grid = log_params, \n                        scoring  = 'recall', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)  \nlog_model_cv.fit(df_X_train_pca,y_train)","0798563a":"log_model_cv.best_estimator_","1cd58f6b":"#let us increase the param range and check\nlog_params = {'C':[2,4,10,50,100,200,300]}\nfolds=5\nlog_model_cv = GridSearchCV(estimator = logistic_learner_pca_tune, \n                        param_grid = log_params, \n                        scoring  = 'recall', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)  \nlog_model_cv.fit(df_X_train_pca,y_train)","7f29d88a":"log_model_cv.best_estimator_","efea3546":"#let us increase the param range and check\nlog_params = {'C':[40,50,55,60]}\nfolds=5\nlog_model_cv = GridSearchCV(estimator = logistic_learner_pca_tune, \n                        param_grid = log_params, \n                        scoring  = 'recall', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)  \nlog_model_cv.fit(df_X_train_pca,y_train)","f6bed8a5":"log_model_cv.best_estimator_","82783e56":"#let us increase the param range and check\nlog_params = {'C':[x for x in range(45,55)]}\nfolds=5\nlog_model_cv = GridSearchCV(estimator = logistic_learner_pca_tune, \n                        param_grid = log_params, \n                        scoring  = 'recall', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)  \nlog_model_cv.fit(df_X_train_pca,y_train)","23aa2163":"log_model_cv.best_estimator_","09d7e0af":"#let us create a model with tuned parameters\nlogistic_learner_pca_tuned = LogisticRegression(penalty='l2',C=46)","9fa587b8":"logistic_model_pca_tuned=logistic_learner_pca_tuned.fit(df_X_train_pca, y_train)\nlogistic_model_pca_tuned.get_params()","6ef32e2f":"#let us make the prediction on test set\npred_probs_test = logistic_model_pca_tuned.predict_proba(df_X_test_pca)\n\"{:2.4}\".format(metrics.roc_auc_score(y_test, pred_probs_test[:,1]))","e12f19c1":"#to create confusion matrix, let us make the prediction\npred_probs_tuned_train = logistic_model_pca_tuned.predict_proba(df_X_train_pca)\n\ny_train_pred_tuned_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':pred_probs_tuned_train[:,1]})\ny_train_pred_tuned_final['ID'] = y_train.index\ny_train_pred_tuned_final.head()\n\ny_train_pred_tuned_final['predicted'] = y_train_pred_tuned_final.Churn_Prob.map(lambda x: 1 if x > 0.56 else 0)\n\n# Let's see the head\ny_train_pred_tuned_final.head()","2be985ac":"#to create confusion matrix, let us make the prediction\npred_probs_tuned_test = logistic_model_pca_tuned.predict_proba(df_X_test_pca)\n\ny_test_pred_tuned_final = pd.DataFrame({'Churn':y_test.values, 'Churn_Prob':pred_probs_tuned_test[:,1]})\ny_test_pred_tuned_final['ID'] = y_test.index\ny_test_pred_tuned_final.head()\n\ny_test_pred_tuned_final['predicted'] = y_test_pred_tuned_final.Churn_Prob.map(lambda x: 1 if x > 0.56 else 0)\n\n# Let's see the head\ny_test_pred_tuned_final.head()","d9a8bb89":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_tuned_final.Churn, y_train_pred_tuned_final.predicted )\nprint(confusion)","e5c4a20c":"# Confusion matrix \nconfusion_test = metrics.confusion_matrix(y_test_pred_tuned_final.Churn, y_test_pred_tuned_final.predicted )\nprint(confusion_test)","0430979f":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_tuned_final.Churn, y_train_pred_tuned_final.predicted))\n\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","f4f325f2":"# Let's check the overall accuracy in test data\nprint(metrics.accuracy_score(y_test_pred_tuned_final.Churn, y_test_pred_tuned_final.predicted))\n\nTP_test = confusion_test[1,1] # true positive \nTN_test = confusion_test[0,0] # true negatives\nFP_test = confusion_test[0,1] # false positives\nFN_test = confusion_test[1,0] # false negatives","33b0c7bd":"# Let's see the sensitivity of our logistic regression model\nsens= TP \/ float(TP+FN)\nTP \/ float(TP+FN)","da689e78":"# Let's see the sensitivity of our logistic regression model\nsens_test= TP_test \/ float(TP_test+FN_test)\nTP_test \/ float(TP_test+FN_test)","76aa0cf8":"# Let us calculate specificity\nTN \/ float(TN+FP)","5c084fe5":"draw_roc(y_train_pred_tuned_final.Churn, y_train_pred_tuned_final.predicted)","5e92314a":"draw_roc(y_test_pred_tuned_final.Churn, y_test_pred_tuned_final.predicted)","4d69db1b":"print(classification_report(y_train_pred_tuned_final.Churn, y_train_pred_tuned_final.predicted))","0c9dcc87":"#Logistic regression tuned model:\nprint('Accuracy :',metrics.accuracy_score(y_test_pred_tuned_final.Churn, y_test_pred_tuned_final.predicted))\nprint('Sensitivity :',sens_test)","eb5e8fb5":"from sklearn.ensemble import RandomForestClassifier\n\nRF=RandomForestClassifier()\nRFG=RF.fit(df_X_train_pca,y_train)","d2b35973":"y_train_pred=RFG.predict(df_X_train_pca)\ny_train_pred","59b53979":"#preparing data  for confusion matrix\ny_train_pred_df = pd.DataFrame({'Churn':y_train.values, 'Predicted':y_train_pred})\ny_train_pred_df['ID'] = y_train.index\ny_train_pred_df.head()","f5917de2":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_df.Churn, y_train_pred_df.Predicted )\nprint('Confusion Matrix:')\nprint(confusion)","80e79f73":"#ROC Score\n\"{:2.2}\".format(metrics.roc_auc_score(y_train_pred_df.Churn, y_train_pred_df.Predicted ))","b8c75045":"#let us test on train data\ny_test_pred=RFG.predict(df_X_test_pca)\ny_test_pred","a9253f44":"#preparing data  for confusion matrix\ny_test_pred_df = pd.DataFrame({'Churn':y_test.values, 'Predicted':y_test_pred})\ny_test_pred_df['ID'] = y_test.index\ny_test_pred_df.head()","72838769":"draw_roc(y_test_pred_df.Churn, y_test_pred_df.Predicted)","adefb70e":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_test_pred_df.Churn, y_test_pred_df.Predicted )\nprint('Confusion Matrix:')\nprint(confusion)","81a9e3d5":"\"{:2.2}\".format(metrics.roc_auc_score(y_test_pred_df.Churn, y_test_pred_df.Predicted ))","50c72107":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","21d245ce":"# Let's see the sensitivity of our RF model\nTP \/ float(TP+FN)","1ee3b829":"print(classification_report(y_test_pred_df.Churn, y_test_pred_df.Predicted))","433c2926":"#for parameter tuning let us start with depth\nrf = RandomForestClassifier()\nparameters = {'max_depth': range(1, 30,5)}","14aa8bfd":"rf_cv = GridSearchCV(rf, parameters, cv=5, scoring='recall',return_train_score=True)\nrf_cv.fit(df_X_train_pca,y_train)","d6581d04":"scores = rf_cv.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","9026cd94":"# we can take max depth between 10-15\n#let us check for estimators\nparameters = {'n_estimators': range(100, 250, 25)}\nrf_cv = GridSearchCV(rf, parameters, cv=5, scoring='recall',return_train_score=True)\nrf_cv.fit(df_X_train_pca,y_train)","bc41aadf":"scores = rf_cv.cv_results_\n# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","570ed05a":"#let us check for estimators by increasing the range\nparameters = {'n_estimators': range(250, 300, 25)}\nrf_cv = GridSearchCV(rf, parameters, cv=5, scoring='recall',return_train_score=True)\nrf_cv.fit(df_X_train_pca,y_train)","a510acba":"scores = rf_cv.cv_results_\n# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","c6a5296f":"#let us use n_estimators=200\nparameters = {'min_samples_leaf': range(1, 100, 10)}\nrf_cv = GridSearchCV(rf, parameters, cv=5, scoring='recall',return_train_score=True)\nrf_cv.fit(df_X_train_pca,y_train)","b6d7dd1e":"scores = rf_cv.cv_results_\n# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","fda7547f":"#let us consider the min_sample_leaf in range 10-20\n    \nparameters = {'min_samples_split': range(1, 50, 10)}\nrf_cv = GridSearchCV(rf, parameters, cv=5, scoring='recall',return_train_score=True)\nrf_cv.fit(df_X_train_pca,y_train)","d4702be2":"scores = rf_cv.cv_results_\n# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","95bcbf17":"#let us have min_samples_split in range 10 to 30 for better accuracy\n#from the random search result let us create a new model\n\nparameters = {\n    'max_depth': [8,10,14,16],\n    'n_estimators': [200],\n    'min_samples_leaf': [10],\n    'min_samples_split': [10,20,30]\n}\nrf_cv = GridSearchCV(rf, parameters, cv=5, scoring='recall',return_train_score=True)\nrf_cv.fit(df_X_train_pca,y_train)","549f1291":"rf_cv.best_params_","5b9c659d":"#let us create a new model with tuned parameters\nrf_tuned = RandomForestClassifier(max_depth= 16,min_samples_leaf= 10,min_samples_split= 10,n_estimators= 200)\nrf_tuned.fit(df_X_train_pca,y_train)","e720da7c":"#let us test on train data\ny_test_pred_rf_tuned=rf_tuned.predict(df_X_test_pca)\ny_test_pred_rf_tuned","1103e993":"#preparing data  for confusion matrix\ny_test_pred_rf_tuned_df = pd.DataFrame({'Churn':y_test.values, 'Predicted':y_test_pred_rf_tuned})\ny_test_pred_rf_tuned_df['ID'] = y_test.index\ny_test_pred_rf_tuned_df.head()","ca7cde93":"draw_roc(y_test_pred_rf_tuned_df.Churn, y_test_pred_rf_tuned_df.Predicted)","4d6d1824":"# Confusion matrix \nconfusion_rf_tuned = metrics.confusion_matrix(y_test_pred_rf_tuned_df.Churn, y_test_pred_rf_tuned_df.Predicted )\nprint('Confusion Matrix:')\nprint(confusion_rf_tuned)","35c03013":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_test_pred_rf_tuned_df.Churn, y_test_pred_rf_tuned_df.Predicted))\n\nTP = confusion_rf_tuned[1,1] # true positive \nTN = confusion_rf_tuned[0,0] # true negatives\nFP = confusion_rf_tuned[0,1] # false positives\nFN = confusion_rf_tuned[1,0] # false negatives","312d44ce":"# Let's see the sensitivity of our RF model\nTP \/ float(TP+FN)","4f8ac7b8":"print(classification_report(y_test_pred_rf_tuned_df.Churn, y_test_pred_rf_tuned_df.Predicted))","8af807b9":"import xgboost as xgb\n\nGBDT=xgb.XGBClassifier()\nGBDTG=GBDT.fit(df_X_train_pca,y_train)\n\ny_train_GBDTG_pred=GBDTG.predict(df_X_train_pca)","a77bdf39":"#preparing data  for confusion matrix\ny_train_GBDTG_pred_df = pd.DataFrame({'Churn':y_train.values, 'Predicted':y_train_GBDTG_pred})\ny_train_GBDTG_pred_df['ID'] = y_train.index\ny_train_GBDTG_pred_df.head()","e13bb5de":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_GBDTG_pred_df.Churn, y_train_GBDTG_pred_df.Predicted )\nprint('Confusion Matrix:')\nprint(confusion)","2b648a57":"#ROC Score\n\"{:2.2}\".format(metrics.roc_auc_score(y_train_GBDTG_pred_df.Churn, y_train_GBDTG_pred_df.Predicted ))","e4455aad":"#let us test on train data\ny_test_GBDTG_pred=GBDTG.predict(df_X_test_pca)\ny_test_GBDTG_pred","e5910755":"#preparing data  for confusion matrix\ny_test_GBDTG_pred_df = pd.DataFrame({'Churn':y_test.values, 'Predicted':y_test_GBDTG_pred})\ny_test_GBDTG_pred_df['ID'] = y_test.index\ny_test_GBDTG_pred_df.head()","712727e0":"draw_roc(y_test_GBDTG_pred_df.Churn, y_test_GBDTG_pred_df.Predicted)","450756c9":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_test_GBDTG_pred_df.Churn, y_test_GBDTG_pred_df.Predicted )\nprint('Confusion Matrix:')\nprint(confusion)","8324ca24":"\"{:2.2}\".format(metrics.roc_auc_score(y_test_GBDTG_pred_df.Churn, y_test_GBDTG_pred_df.Predicted))","e7302a4a":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our RF model\nTP \/ float(TP+FN)","bac91fff":"print(classification_report(y_test_GBDTG_pred_df.Churn, y_test_GBDTG_pred_df.Predicted))","3f369c57":"import xgboost as xgb\nparameters={'learning_rate':[0.01,0.02,0.03,0.04,0.05]}\n\nGBDT=xgb.XGBClassifier()\n\nGBDTG=GridSearchCV(GBDT,parameters,cv=3,scoring='recall',return_train_score=True,n_jobs=4,verbose=6)\nGBDTG.fit(df_X_train_pca,y_train)","e4946e59":"scores = GBDTG.cv_results_\n# plotting accuracies with learning_rate\nplt.figure()\nplt.plot(scores[\"param_learning_rate\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_learning_rate\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"learning_rate\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","26c14ea0":"parameters={'learning_rate':[0.05,0.1,0.5]}\n\nGBDT=xgb.XGBClassifier()\n\nGBDTG=GridSearchCV(GBDT,parameters,cv=3,scoring='recall',return_train_score=True,n_jobs=4,verbose=6)\nGBDTG.fit(df_X_train_pca,y_train)","94ef49d4":"scores = GBDTG.cv_results_\n# plotting accuracies with learning_rate\nplt.figure()\nplt.plot(scores[\"param_learning_rate\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_learning_rate\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"learning_rate\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","ba38488e":"parameters={'learning_rate':[0.1,0.13,0.15]}\n\nGBDT=xgb.XGBClassifier()\n\nGBDTG=GridSearchCV(GBDT,parameters,cv=3,scoring='recall',return_train_score=True,n_jobs=4,verbose=6)\nGBDTG.fit(df_X_train_pca,y_train)","d894cb78":"scores = GBDTG.cv_results_\n# plotting accuracies with learning_rate\nplt.figure()\nplt.plot(scores[\"param_learning_rate\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_learning_rate\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"learning_rate\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","9788aec1":"#let us select learning_rate= 0.13\nGBDT_tuned=xgb.XGBClassifier(learning_rate= 0.13)\nGBDT_tuned=GBDT_tuned.fit(df_X_train_pca,y_train)\n\ny_train_GBDTG_tuned_pred=GBDT_tuned.predict(df_X_train_pca)","76f06995":"y_test_GBDTG_tuned_pred=GBDT_tuned.predict(df_X_test_pca)\n#preparing data  for confusion matrix\ny_test_GBDTG_tuned_pred_df = pd.DataFrame({'Churn':y_test.values, 'Predicted':y_test_GBDTG_tuned_pred})\ny_test_GBDTG_tuned_pred_df['ID'] = y_test.index\ny_test_GBDTG_tuned_pred_df.head()","1724a65d":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_test_GBDTG_tuned_pred_df.Churn, y_test_GBDTG_tuned_pred_df.Predicted )\nprint('Confusion Matrix:')\nprint(confusion)","5f3c7d7f":"#ROC Score\n\"{:2.2}\".format(metrics.roc_auc_score(y_test_GBDTG_tuned_pred_df.Churn, y_test_GBDTG_tuned_pred_df.Predicted ))","f2607510":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our RF model\nTP \/ float(TP+FN)","36d66e45":"print(classification_report(y_test_GBDTG_tuned_pred_df.Churn, y_test_GBDTG_tuned_pred_df.Predicted))","5988fe5b":"# let us use logistic regression to identify the importatnt predictors\nimport statsmodels.api as sm\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","448fff6e":"X_train_rfe=X_train.copy()\nX_test_rfe=X_test.copy()","f52390c7":"# Recursive Feature Elimination (RFE) for Feature Selection in Python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X_train_rfe, y_train)","dec7f17f":"list(zip(X_train_rfe.columns,rfe.support_,rfe.ranking_))","6779ce63":"col = X_train_rfe.columns[rfe.support_]\ncol","492015cb":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]\nX_train_rfe.head()","105b8737":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train_rfe)), family = sm.families.Binomial())\nlogm1.fit().summary()","a4e3ee74":"# Calculate the VIFs for the model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8bd808df":"#dropping the variable for better analaysis\nX_train_rfe=X_train_rfe.drop(['total_og_mou_8'],axis=1)","ca60b5e1":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train_rfe)), family = sm.families.Binomial())\nlogm1.fit().summary()","2e0c1a47":"# Calculate the VIFs for the model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9dd3fd49":"#dropping the variable for better analaysis\nX_train_rfe=X_train_rfe.drop(['offnet_mou_8'],axis=1)","602e8165":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train_rfe)), family = sm.families.Binomial())\nlogm1.fit().summary()","022c414f":"# Calculate the VIFs for the model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","98502df1":"logm1.fit().params","523cbdb8":"imp_feat = pd.DataFrame({'Feature':logm1.fit().params.index, 'Coeff':abs(logm1.fit().params.values)})\nimp_feat.head(15)","763186e6":"#dropping the constant variable for better analaysis\nimp_feat=imp_feat.drop(0)","2dc7deb0":"#validating the data frame\nimp_feat.head(20)","821fc783":"# Plot the histogram of the error terms\nfig = plt.figure(figsize=[20,10])\nsns.barplot(data=imp_feat.sort_values(['Coeff']),y='Feature',x='Coeff')\nfig.suptitle('Important predictors', fontsize = 20)                  # Plot heading \nplt.xlabel('Coefficients', fontsize = 18)                         # X-label","af2a5b0f":"#the top key features for prediction are\nimp_feat.sort_values(['Coeff'],ascending=False).Feature","33ac1cee":"#### 8.13 % of high value customers have churned. There is a class imbalance. We need to treat this using SMOTE later when necessary.\n\n##### As we have marked our churn and non churn customers we can remove the columns belonging to churn phase as those are not available in real time.","6bb43c04":"#### We have treated all missing values from the data set. We are left with 28504 records and 161 columns ","ee1afdb0":"### From Logistic Regression, the most important features are :\n1. \"ph_churn_rech\" -> The average data and call usage of customer during the churn phase.\n2. total_ic_mou_8 -> Total incommign minutes of usage of customer during the active phase.\n3. arpu_8  -> Average revenue per customer during active phase.  \n4. roam_og_mou_8 ->  Outgoing roaming minues of use of customer.\n5. spl_ic_mou_8, and \n\n    arpu_7, loc_og_t2m_mou_8, sep_vbc_3g, onnet_mou_8, total_ic_mou_7,\n    max_rech_data_8, arpu_6, std_og_t2m_mou_8","aefffd74":"### Logistic regression model","d0bcba9d":"### Logistic regression model - hyperparameter tuning","339688a6":"### XGBoost","0609ebb1":"## Identifying important predictors","afac344e":"2. Filter high-value customers\n\n    As mentioned in document , we need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\n    \n    The first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.","46193f49":"*__Inference :__* \n\n    1.many columns have high correlation value","062ece9e":"### Inference : Average revenue per user is skewed to the right\n","3d55e4a6":"1. Derive new feature, let us create a feature to tag a customer is churned or not, as per the document Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\n    total_ic_mou_9\n\n    total_og_mou_9\n\n    vol_2g_mb_9\n\n    vol_3g_mb_9","e3506e6d":"##### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0","43c49245":"*__Inference :__* \n\n    1.The data is highly imabalanced.","1b9d2f41":"###  Data Understanding & Cleaning","0571ecc1":"*__Inference :__* \n\n    1.Total minutes of usage for incoming\/outcoming dropped in action phase for churned customers","b4558e75":"#### Finding Optimal Cutoff Point","85e03f6a":"*__Inference :__* \n\n    1.The churn rate is low if age on network is high.","6afdbee9":"### Introduction:\n\nTelecom companies focus more on retaining customers than on acquiring customers as it costs 5-10 times more to acquire a new customer than to retain an existing one.\n\nIt is observed that 80% of revenue comes from 20% of the customers. They are names as High Value Customers. Hence more emphasis is made on high value customers.\n\n### Data Provided:\n\nDataset contains customer usage data in the duration of 4 months.\nThese months are grouped as:\n1. Good phase - 6th and 7th month\n2. Action phase - 8th month\n3. Churn phase - 9th month\n\n### Business Objectives:\n\n1. Build a predictive model that can be used to know if customers are going to churn or not in the future.\n2. To identify those factors( variables ) that are helpful in predicting the churn and customer behavior.\n\n### Approach:\n\n1. Data Understanding & Cleaning\n2. EDA\n3. Data preparation for model building\n4. Handle Class Imbalance\n5. Dimensionality Reduction using PCA\n6. Classification models to predict Churn\n7. Model Evaluation\n8. Interpretable model creation to identify strong predictors of churn.\n9. Summary\n","93f15799":"## Data Preparation for model:","f8ef9708":"### XGBoost - HyperParameter tuning","772edfd2":"*__Inference :__* \n\n    1.ARPU drops in action phase in churned customers","9c31a4b7":"### Multivariate Analysis","3bed011c":"#### Precision and Recall","3c46ec82":"### Bivariate analysis","2ba1ee80":"### Inference : During 8th month the number of users who didn't use facebook has increased","4bd32d59":"*__Inference :__* \n\n    1.2g\/3g data usage drops in action phase for churned customers.\n    2.revenue generated by 2g\/3g usage also drops in action phase for churned customers.\n    3.usage and revenue dropped for sachet package in good phase for churned customers.","f7fe1ec0":"### Random Forest Classifier","7cbe4caf":"## Inference from EDA\n1. Churn rate is low if age on network is high.\n2. ARPU drops in action phase in churned customers\n3. On\/off network minutes usage drops in action phase for churned customers.\n4. Operator T to T's incoming\/outgoing calls - standard and local minutes of usage drops in action phase for churned customers\n5. Operator T to Other's incoming\/outgoing calls - standard and local minutes of usage drops in action phase for churned customers\n6. Operator T to Fixed line's incoming\/outgoing calls - local minutes of usage drops in action phase for churned customers\n7. Operator T to Own call center's outgoing calls - local minutes of usage drops in action phase for churned customers \n8. Total minutes of usage for incoming\/outcoming dropped in action phase for churned customers.\n9. Total number of recharges dropped in action phase for churned customers.\n10. Total call recharge amount drops in action phase in churned customers\n11. Last day recharge amount drops in action phase in churned customers\n12. Total data recharge amount drops in action phase in churned customers .\n13. 2g\/3g data usage drops in action phase for churned customers.\n14. Revenue generated by 2g\/3g usage also drops in action phase for churned customers.\n15. Usage and revenue dropped for sachet package in good phase for churned customers.\n16. Monthly package has dropped in action phase for churned customers, network availability\/ package cost might be an issue.","48258f6d":"### Inference: Total outgoing calls data distribution is skewed. Most of the values are lying around 1000","860b68b1":"## EDA\n\n### Univariate Analysis","e3d53667":"### Treating class imbalance","c77b58d1":"*__Inference :__* \n\n    1.monthly package has dropped in action phase for churned customers, network availability\/ package cost might be an issue","d93f26b9":"*__Inference :__* \n\n    1.Total number of recharges dropped in action phase for churned customers","4b0228cf":"# Results of various models\n| Model Name | Accuracy | Sensitivity |\n| --- | --- | --- |\n| Default\tLogistic Reg | 85% | 81% |\n| Default\tRandom forest | 96% | 34% |\n| Default\tXGBoost | 97% | 50% |\n| Tuned\tLogistic Reg | 85% | 81% |\n| Tuned\tRandom forest | 91% | 51% |\n| Tuned\tXGBoost | 76% | 57% |","f55f82d8":"### Model Summary:\nOut of Logistic Regression, Random forest and boosting algorithms, the Logistic regression identifies Churners better than Non churners with an accuracy of 85% and sensitivity of 81%","ee9fadbc":"# Business Insights:\n1. The above mentioned columns are very strong predictors of Churn.\n2. Most of the customers who churn have very low usage of calls and data during the churn phase. So we can give special offers for STD and ISD rates to those customers to retain them.\n3. The active phase is very critical to retain a customer, so during the active phase we can reduce roaming charges to retain customers.\n4. Customers who recharge with higher amounts are high value customers by definition, so giving them good deals on Higher data or call packages compared to the competitors would retain more customers.","cecbbb17":"### Reducing features -  PCA.","f60c29bf":"*__Inference :__* \n\n    1.On network minutes usage drops in action phase for churned customers.\n    2.Off network minutes usage drops in action phase for churned customers.","6192230a":"## Data Preparation","3f31f2ce":"3. Tag churners and remove attributes of the churn phase\n    After tagging churners, remove all the attributes corresponding to the churn phase (all attributes having \u2018 _9\u2019, etc. in their names)","3f0201aa":"#### From the curve above, 0.56 is the optimum point to take it as a cutoff probability.","5956e3b6":"*__Inference :__* \n\n    1.Total call recharge amount drops in action phase in churned customers\n    2.Maximum recharge amount drops in action phase in churned customers\n    3.Last day recharge amount drops in action phase in churned customers\n    4.Total data recharge amount drops in action phase in churned customers ","3669a1b8":"### Random Forest Classifier - Parameter Tuning","728b7cfd":"##### Creating a dataframe with the actual churn flag and the predicted probabilities","14622a37":"*sensitivity of regression model* : 0.82","186315f5":"*__Inference :__* \n\n    1.Operator T to T's incoming\/outgoing calls - standard and local minutes of usage drops in action phase for churned customers\n    2.Operator T to Other's incoming\/outgoing calls - standard and local minutes of usage drops in action phase for churned customers\n    3.Operator T to Fixed line's incoming\/outgoing calls - local minutes of usage drops in action phase for churned customers\n    4.Operator T to Own call center's outgoing calls - local minutes of usage drops in action phase for churned customers ","01f60058":"## Data Cleansing and missing value population","4cf6da57":"## Model Building - PCA Data","00f5bdae":"As per the assignment we are already provided with key data preapartion steps, let us follow that\n\n    1. Derive new features\n    2. Filter high-value customers\n    3. Tag churners and remove attributes of the churn phase"}}