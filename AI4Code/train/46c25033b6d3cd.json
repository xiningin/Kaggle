{"cell_type":{"f7077238":"code","1a6bcbc9":"code","ec98e317":"code","3c7a73df":"code","1d8d7c2c":"code","05891d48":"code","0689ae93":"code","5f2625d1":"code","dc0fbd29":"code","53c332ed":"code","ab192110":"code","afbd3ab3":"code","6f9c7d46":"code","2e43b68b":"code","b8513297":"code","4a70f3b1":"code","b30719a7":"code","1f5e3a04":"code","72ef341e":"code","194acad4":"code","9b6e94a5":"code","ac9a996b":"code","eb4baed4":"code","b52a2e6e":"code","9d17f994":"code","a279869a":"code","65d94796":"code","4f1331d4":"code","be8d2a8e":"markdown","2ea61109":"markdown","fb8e61a1":"markdown","076323b3":"markdown","a91f13f9":"markdown","5b68f203":"markdown","834bd1d5":"markdown","b52f3836":"markdown"},"source":{"f7077238":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a6bcbc9":"! pip install pyspark","ec98e317":"#Importing the nnecessary libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.sql.types import IntegerType, DoubleType\nimport os \nimport warnings\nwarnings.filterwarnings('ignore')","3c7a73df":"#Reading the data\npath = '..\/input\/bank-term-deposit'\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.csv(path, header=True, inferSchema=True, sep=';') \ndf.show()","1d8d7c2c":"#Mapping our target variable to value of 1 and 0\ntarget_variable_name = 'y'\ndf = df.withColumn('y', F.when(F.col('y') == 'yes',1).otherwise(0))","05891d48":"#Len of the dataset\ndf.count()\n\n#types of variables\nprint(df.dtypes)\nprint('n\/')\nprint(df.printSchema())","0689ae93":"#target count \ndf.groupBy(target_variable_name).count().show()","5f2625d1":"from pyspark.sql.functions import approxCountDistinct, countDistinct\n\"\"\"\nNote: approxCountDistinct and countDistinct can be used interchangeably.\nOnly difference is the computation time.\n\"approxCountDistinct\" is useful for large datasets\n\"countDistinct\" for small and medium datasets.\n\"\"\"\n\ndef cardinality_calculation(df, cut_off=1):\n    cardinality = df.select(*[approxCountDistinct(c).alias(c) for c in df.columns])\n    \n    #convert to pandas for efficient calculations\n    final_cardinality_df = cardinality.toPandas().transpose()\n    final_cardinality_df.reset_index(inplace=True)\n    final_cardinality_df.rename(columns={0:'Cardinality'}, inplace=True)\n    \n    #select variables with cardinality of 1\n    vars_selected = final_cardinality_df['index']\n    [final_cardinality_df['Cardinality'] <= cut_off]\n    \n    return final_cardinality_df, vars_selected\n\ncardinality_df, cardinality_vars_selected = cardinality_calculation(df)\n#Printing the unique values for each column\ncardinality_df","dc0fbd29":"from pyspark.sql.functions import count, when, isnan, col\n#missing values check\n\ndef missing_calculation(df, miss_percentage=0.80):\n    #checks for both NaN and null values\n    missing = df.select(*[count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n    length_df = df.count()\n    \n    ## convert to pandas for efficient calculations\n    final_missing_df = missing.toPandas().transpose()\n    final_missing_df.reset_index(inplace=True)\n    final_missing_df.rename(columns={0:'missing_count'}, inplace=True)\n    final_missing_df['missing_percentage'] = final_missing_df['missing_count']\/length_df\n    \n    #select variables with cardinality of 1\n    vars_selected = final_missing_df['index'][final_missing_df['missing_percentage'] >= miss_percentage]\n    return final_missing_df, vars_selected\n\nmissing_df, missing_vars_selected = missing_calculation(df)\nmissing_df","53c332ed":"#Before converting features into numerical representation, we need to separate variables on the basis of\n#their class.\n\ndef variable_type(df):\n    \n    vars_list = df.dtypes \n    char_vars = []\n    num_vars = []\n    for i in vars_list:\n        if i[1] in ('string'):\n            char_vars.append(i[0])\n        else:\n            num_vars.append(i[0])\n    \n    return char_vars, num_vars\n\nchar_vars, num_vars = variable_type(df)\nnum_vars.remove(target_variable_name)\n\n\ndef category_to_index(df, char_vars):\n    \n    char_df = df.select(char_vars)\n    indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\") for c in char_df.columns]\n    pipeline = Pipeline(stages=indexers)\n    char_labels = pipeline.fit(char_df)\n    df = char_labels.transform(df)\n    return df, char_labels\n\ndf, char_labels = category_to_index(df, char_vars)\ndf = df.select([c for c in df.columns if c not in char_vars])\n\n#A couple of thing is highligted in the code here:\n#1. StringIndexer takes an input (for example feature 'Education') and produces an ouput column(Education_Index)\n\n#2. One thing to note is that 'handleInvalid' option is set to keep here, so that when we encounter new \n#new data in the future is still works.\n\n#3. The next option is the Pipeline option, which is used to execute our steps sequentially.","ab192110":"df.show() #We can see that our features are converted into numbers","afbd3ab3":"#If you want to change the column name back to what it was, you can use rename is again using this\n\ndef rename_columns(df, char_vars):\n    mapping = dict(zip([i + '_index' for i in char_vars], char_vars))\n    df = df.select([F.col(c).alias(mapping.get(c, c)) for c in df.columns])\n    return df\ndf = rename_columns(df, char_vars)","6f9c7d46":"df.show() \n\n#NOICE its looking good now.","2e43b68b":"#assemble individual columns to one column - 'features'\ndef assemble_vectors(df, features_list, target_variable_name):\n    stages = []\n    #assemble vectors\n    assembler = VectorAssembler(inputCols=features_list, outputCol='features')\n    stages = [assembler]\n    #select all the columns + target + newly created 'features' column\n    selectedCols = [target_variable_name, 'features'] + features_list\n    #use pipeline to process sequentially\n    pipeline = Pipeline(stages=stages)\n    #assembler model\n    assembleModel = pipeline.fit(df)\n    #apply assembler model on data\n    df = assembleModel.transform(df).select(selectedCols)\n\n    return df, assembleModel, selectedCols\n\n#exclude target variable and select all other feature vectors\nfeatures_list = df.columns\n#features_list = char_vars #this option is used only for ChiSqselector\nfeatures_list.remove(target_variable_name)\n\n# apply the function on our dataframe\ndf, assembleModel, selectedCols = assemble_vectors(df, features_list, target_variable_name)","b8513297":"df.show()","4a70f3b1":"#You can use the writeen code to look at the schema of the combined vector features\n# df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]\n# import pandas as pd\n# for k, v in df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"].items():\n#     features_df = pd.DataFrame(v)","b30719a7":"#splitting the dataset \ntrain, test = df.randomSplit([0.7, 0.3], seed=12345)\ntrain.count(), test.count()","1f5e3a04":"clf = RandomForestClassifier(featuresCol='features', labelCol='y')\nclf_model = clf.fit(train)\nprint(clf_model.featureImportances)\nprint(clf_model.toDebugString)","72ef341e":"train_pred_result = clf_model.transform(train)\ntest_pred_result = clf_model.transform(test)","194acad4":"def evaluation_metrics(df, target_variable_name):\n    pred = df.select(\"prediction\", target_variable_name)\n    pred = pred.withColumn(target_variable_name, pred[target_variable_name].cast(DoubleType()))\n    pred = pred.withColumn(\"prediction\", pred[\"prediction\"].cast(DoubleType()))\n    metrics = MulticlassMetrics(pred.rdd.map(tuple))\n    # confusion matrix\n    cm = metrics.confusionMatrix().toArray()\n    acc = metrics.accuracy #accuracy\n    misclassification_rate = 1 - acc #misclassification rate\n    precision = metrics.precision(1.0) #precision\n    recall = metrics.recall(1.0) #recall\n    f1 = metrics.fMeasure(1.0) #f1-score\n    #roc value\n    evaluator_roc = BinaryClassificationEvaluator(labelCol=target_variable_name, rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n    roc = evaluator_roc.evaluate(df)\n    evaluator_pr = BinaryClassificationEvaluator(labelCol=target_variable_name, rawPredictionCol='rawPrediction', metricName='areaUnderPR')\n    pr = evaluator_pr.evaluate(df)\n    return cm, acc, misclassification_rate, precision, recall, f1, roc, pr","9b6e94a5":"train_cm, train_acc, train_miss_rate, train_precision, \\\n        train_recall, train_f1, train_roc, train_pr = evaluation_metrics(train_pred_result, target_variable_name)","ac9a996b":"test_cm, test_acc, test_miss_rate, test_precision, \\\n        test_recall, test_f1, test_roc, test_pr = evaluation_metrics(test_pred_result, target_variable_name)","eb4baed4":"print('Train accuracy - ', train_acc, ', Test accuracy - ', test_acc) \nprint('Train misclassification rate - ', train_miss_rate, ', Test misclassification rate - ', test_miss_rate) #lower is better\nprint('Train precision - ', train_precision, ', Test precision - ', test_precision)#higher is better\nprint('Train recall - ', train_recall, ', Test recall - ', test_recall)#higher is better, but in this case we have to maintain the trade off between recall and precision\nprint('Train f1 score - ', train_f1, ', Test f1 score - ', test_f1)\nprint('Train ROC - ', train_roc, ', Test ROC - ', test_roc)\nprint('Train PR - ', train_pr, ', Test PR - ', test_pr)","b52a2e6e":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef make_confusion_matrix_chart(cf_matrix_train, cf_matrix_test):\n    \n    list_values = ['0', '1']\n    \n    plt.figure(1, figsize=(10,5))\n    plt.subplot(121)\n    sns.heatmap(cf_matrix_train, annot=True, yticklabels=list_values, \n                                xticklabels=list_values, fmt='g')\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Pred\")\n    plt.ylim([0,len(list_values)])\n    plt.title('Train data predictions')\n    \n    plt.subplot(122)\n    sns.heatmap(cf_matrix_test, annot=True, yticklabels=list_values, \n                                xticklabels=list_values, fmt='g')\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Pred\")\n    plt.ylim([0,len(list_values)])\n    plt.title('Test data predictions')\n\n    plt.tight_layout()\n    return None","9d17f994":"make_confusion_matrix_chart(train_cm, test_cm)","a279869a":"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nclass CurveMetrics(BinaryClassificationMetrics):\n    def __init__(self, *args):\n        super(CurveMetrics, self).__init__(*args)\n\n    def _to_list(self, rdd):\n        points = []\n        results_collect = rdd.collect()\n        for row in results_collect:\n            points += [(float(row._1()), float(row._2()))]\n        return points\n\n    def get_curve(self, method):\n        rdd = getattr(self._java_model, method)().toJavaRDD()\n        return self._to_list(rdd)","65d94796":"#helper function to plot roc and PR curve\ndef plot_roc_pr(df, target_variable_name, plot_type, legend_value, title):\n    \n    preds = df.select(target_variable_name,'probability')\n    preds = preds.rdd.map(lambda row: (float(row['probability'][1]), float(row[target_variable_name])))\n    # Returns as a list (false positive rate, true positive rate)\n    points = CurveMetrics(preds).get_curve(plot_type)\n    plt.figure()\n    x_val = [x[0] for x in points]\n    y_val = [x[1] for x in points]\n    plt.title(title)\n    \n    if plot_type == 'roc':\n        plt.xlabel('False Positive Rate (1-Specificity)')\n        plt.ylabel('True Positive Rate (Sensitivity)')\n        plt.plot(x_val, y_val, label = 'AUC = %0.2f' % legend_value)\n        plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n    \n    if plot_type == 'pr':\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.plot(x_val, y_val, label = 'Average Precision = %0.2f' % legend_value)\n        plt.plot([0, 1], [0.5, 0.5], color='red', linestyle='--')\n    \n    plt.legend(loc = 'lower right')\n    return None","4f1331d4":"plot_roc_pr(train_pred_result, target_variable_name, 'roc', train_roc, 'Train ROC')\nplot_roc_pr(test_pred_result, target_variable_name, 'roc', test_roc, 'Test ROC')\nplot_roc_pr(train_pred_result, target_variable_name, 'pr', train_pr, 'Train Precision-Recall curve')\nplot_roc_pr(test_pred_result, target_variable_name, 'pr', test_pr, 'Test Precision-Recall curve')","be8d2a8e":"# Final Conclusion\n\n- Well,as the classes are imbalanced thats why the average-precision score is not precise.\n- We could use sampling techniques or boosting models or assigning class weights, in order to arrive a reduce our FP.\n\n\n#### Well that was all I wanted to showcase, If you have qny question regarding the notebook let me know !\n\nThank You !\nPS: Sorry for the poor markdown entries (Its my first notebook so still in a learning phase lol)","2ea61109":"- I am only going to use Random Forest Classifier for this problem, as the motive behind this notebook is to show the basic functionalities and structure wise implementation of PySpark.\n","fb8e61a1":"So the dataset is quite imbalanced. \n\nLets see how we can check the missing values and do cardinality check in PySpark.","076323b3":"# Assemble Features\n\n- The next step is to assemble the individual variables into a single feature vector.\n- This is useful because, instead of providing individual variables in the next steps, we can point to one variable. ","a91f13f9":"# __Missing Values Check__\n\nMissing values, as the name suggests, refers to a missing piece of information. We need\nto understand the reason behind the missing piece of information in order to build a\nbetter model. In general, data could be missing for the following three reasons:\n\n- Missing at random **(MAR)**\n- Missing completely at random **(MCAR)**\n- Missing not at random **(MNAR)**\n\nBy understanding the type of missing information, we can treat the data accordingly.","5b68f203":"- For the bank dataset,no columns are rejected based on these checks (no missing values).\n- However, in real-world datasets, you will see a handful of columns that can be eliminated after performing cardinality and missing value checks. \n- Well, that is all we need to learn for __EDA-based variable selection techniques.__\n- But before moving forward we need to take care of one last thing.\n    - We need to convert our categorical features(strings) into numerical numbers.\n    - There are multiple ways to do it in PySpark, I will be using __StringIndexer__ \n    - This function will assigns a number to each level. The most frequent value would get index 0, followed by the next most frequent and so on. \n    \n","834bd1d5":"# Modeling ","b52f3836":"# __Cardinality Check__\nCardinality is nothing but the number of unique values of a variable. In pandas we can achieve this task using simple function but in PySpark, it comes in quite handy. "}}