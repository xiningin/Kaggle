{"cell_type":{"38ff7022":"code","55213531":"code","36bc79e9":"code","66a7c3f4":"code","82065917":"code","e7f302f3":"code","c7532051":"code","8e2ca16c":"code","78b699fa":"code","4cd221e1":"code","084543d8":"code","e59d668b":"code","da5e44a4":"code","cb2a7309":"code","65a76d0d":"code","c2ac304c":"code","a83b5e6e":"code","80a1f1b1":"code","bd876df2":"code","416482c8":"code","8b6f77b4":"code","2452931c":"code","9e23ebc2":"code","cc976097":"code","6a70913a":"code","3401db90":"code","4332a86b":"code","7ef77f18":"code","1ebec447":"code","cde19dc8":"code","2b7d6e02":"code","8d114dae":"code","cf090656":"code","5bb636f0":"code","ed9dde2a":"code","d9d68282":"code","a6ef9190":"code","14fb868b":"code","8949e1ac":"code","3f375da0":"code","3fc5dafb":"code","7885281d":"code","6ddb5286":"code","af1c39da":"code","7bef94e1":"code","aaf129ba":"code","c0c59049":"code","59859154":"code","d7bafa58":"code","b0c5aa6e":"code","f0af1e93":"code","2144b874":"code","6ae98a54":"code","c36960d0":"code","893a51c3":"markdown","c7d8bd3a":"markdown","842c14a4":"markdown","20d1b52f":"markdown","4f15bfd1":"markdown","a1527863":"markdown","fa448159":"markdown","61f826b5":"markdown","7dedd6f6":"markdown","59cc6ece":"markdown"},"source":{"38ff7022":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.filterwarnings('ignore')","55213531":"df = pd.read_csv('\/kaggle\/input\/hr-analytics\/HR_comma_sep.csv')\ndf.head()","36bc79e9":"df.shape","66a7c3f4":"df.describe().T","82065917":"profile = ProfileReport(df)\nprofile","e7f302f3":"df.info()","c7532051":"df['Department'].unique()","8e2ca16c":"sns.countplot(df['left'],palette='Set3')","78b699fa":"sns.set_style('whitegrid')\nsns.set_palette('Set3')\ndf.hist(figsize=(18,10));","4cd221e1":"sns.countplot(df['salary'],hue=df['left'],palette='Set2')","084543d8":"sns.countplot(df['Department'],hue=df['left'],palette='Set2')","e59d668b":"sns.countplot(df['number_project'],hue=df['left'],palette='Set2')","da5e44a4":"pd.crosstab(df['satisfaction_level'],df['left']).plot(kind=\"bar\",figsize=(25,8),color=['green','brown' ])\nplt.title('Personnel satisfaction')\nplt.xlabel('satisfaction_level')\nplt.ylabel('Frequency')","cb2a7309":"pd.crosstab(df['last_evaluation'],df['left']).plot(kind=\"bar\",figsize=(25,8),color=['blue','red' ])\nplt.title('lastevaluation of left')\nplt.xlabel('last_evaluation')\nplt.ylabel('Frequency')","65a76d0d":"df['salary'].value_counts()","c2ac304c":"df['Department'].value_counts()","a83b5e6e":"table = df.pivot_table(\"satisfaction_level\", index=\"Department\", columns=\"salary\")\ntable","80a1f1b1":"sns.pairplot(df,hue= 'left',palette='Set2')","bd876df2":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True)","416482c8":"df.isnull().mean()","8b6f77b4":"## null count analysis\nimport missingno as msno\np=msno.bar(df)","2452931c":"df.info()","9e23ebc2":"def plot(df,col):\n    fig,(ax1,ax2)=plt.subplots(2,1)\n    sns.distplot(df[col],ax=ax1)\n    sns.boxplot(df[col],ax=ax2,color='skyblue')","cc976097":"plot(df,\"satisfaction_level\")","6a70913a":"plot(df,\"last_evaluation\")","3401db90":"plot(df,\"number_project\")","4332a86b":"plot(df,\"average_montly_hours\")","7ef77f18":"plot(df,\"time_spend_company\")","1ebec447":"print('Before drop duplicate:', df.shape)\n\n\n\ndf= df.drop_duplicates()\nprint('After drop duplicate:', df.shape)\n","cde19dc8":"from sklearn.preprocessing import LabelEncoder \n\n\nle=LabelEncoder()\ndf['salary']=le.fit_transform(df['salary']) \n","2b7d6e02":"df = pd.get_dummies(df,drop_first=True)\ndf.head()","8d114dae":"x= df.drop(\"left\", axis=1)\ny = df.left","cf090656":"from sklearn.model_selection import train_test_split\n\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,train_size=.80,random_state=22)\nprint(xtrain.shape)\nprint(xtest.shape)","5bb636f0":"from sklearn.preprocessing import MinMaxScaler\n\nscale = MinMaxScaler()\nxtrain = scale.fit_transform(xtrain, ytrain)\nxtest = scale.transform(xtest)","ed9dde2a":"from imblearn.over_sampling import SMOTE  \n \n\n\nsmt = SMOTE()\nxtrain, ytrain = smt.fit_resample(xtrain, ytrain)\nnp.bincount(ytrain)","d9d68282":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier \nimport xgboost as xgb \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection\n","a6ef9190":"# k ploting\nk_range = list(range(1,10))\nscore=[]\n\nfor k in k_range:\n    knn= KNeighborsClassifier(n_neighbors=k)\n    knn.fit(xtrain,ytrain)\n    ypred = knn.predict(xtest)\n    score.append(accuracy_score(ytest, ypred))\n    \n\nplt.plot(k_range,score)\n\n\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\n\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors') ","14fb868b":"sns.set_palette('Set2')\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,30):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(xtrain,ytrain)\n    \n    train_scores.append(knn.score(xtrain,ytrain))\n    test_scores.append(knn.score(xtest,ytest))\n\nplt.figure(figsize=(20,5))\np = sns.lineplot(range(1,30),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,30),test_scores,marker='o',label='Test Score')","8949e1ac":"#LogisticRegression\nlr_c=LogisticRegression(C=0.01, penalty= 'l2',random_state=22)\nlr_c.fit(xtrain,ytrain)\nlr_pred=lr_c.predict(xtest)\nlr_cm=confusion_matrix(ytest,lr_pred)\nlr_ac=accuracy_score(ytest, lr_pred)\n\n#MLP\nMLP = MLPClassifier(activation='relu', hidden_layer_sizes= (20, 30), learning_rate_init= 0.001, max_iter=200, solver ='adam',random_state=22)\nMLP.fit(xtrain,ytrain)\nMLP_pred=MLP.predict(xtest)\nMLP_cm=confusion_matrix(ytest,MLP_pred)\nMLP_ac=accuracy_score(ytest, MLP_pred)\n\n#Bayes\ngaussian=GaussianNB()\ngaussian.fit(xtrain,ytrain)\nbayes_pred=gaussian.predict(xtest)\nbayes_cm=confusion_matrix(ytest,bayes_pred)\nbayes_ac=accuracy_score(bayes_pred,ytest)\n\n#SVM  \nsvc_r=SVC(C= 100, kernel= 'poly',degree=2,random_state=22)\nsvc_r.fit(xtrain,ytrain)\nsvr_pred=svc_r.predict(xtest)\nsvr_cm=confusion_matrix(ytest,svr_pred)\nsvr_ac=accuracy_score(ytest, svr_pred)\n\n#RandomForest\nrdf_c=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=22)\nrdf_c.fit(xtrain,ytrain)\nrdf_pred=rdf_c.predict(xtest)\nrdf_cm=confusion_matrix(ytest,rdf_pred)\nrdf_ac=accuracy_score(rdf_pred,ytest)\n\n# DecisionTree Classifier\ndtree_c=DecisionTreeClassifier(criterion= 'gini', max_depth= 5, min_samples_leaf= 2, min_samples_split= 3, splitter= 'best',random_state=22)\ndtree_c.fit(xtrain,ytrain)\ndtree_pred=dtree_c.predict(xtest)\ndtree_cm=confusion_matrix(ytest,dtree_pred)\ndtree_ac=accuracy_score(dtree_pred,ytest)\n#KNN\nknn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(xtrain,ytrain)\nknn_pred=knn.predict(xtest)\nknn_cm=confusion_matrix(ytest,knn_pred)\nknn_ac=accuracy_score(knn_pred,ytest)","3f375da0":"plt.figure(figsize=(20,10))\n\nplt.subplot(2,4,1)\nplt.title(\"LogisticRegression_cm\")\nsns.heatmap(lr_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,2)\nplt.title(\"MLP\")\nsns.heatmap(MLP_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,3)\nplt.title(\"bayes_cm\")\nsns.heatmap(bayes_cm,annot=True,cmap=\"Oranges\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,4)\nplt.title(\"RandomForest\")\nsns.heatmap(rdf_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,5)\nplt.title(\"SVM\")\nsns.heatmap(svr_cm,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,6)\nplt.title(\"DecisionTree_cm\")\nsns.heatmap(dtree_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,4,7)\nplt.title(\"kNN_cm\")\nsns.heatmap(knn_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)","3fc5dafb":"print('LogisticRegression_accuracy:\\t',lr_ac)\nprint('MLP_accuracy:\\t\\t\\t',MLP_ac)\nprint('RandomForest_accuracy:\\t\\t',rdf_ac)\nprint('DecisionTree_accuracy:\\t\\t',dtree_ac)\nprint('KNN_accuracy:\\t\\t\\t',knn_ac)\nprint('SVM_accuracy:\\t\\t\\t',svr_ac)\nprint('Bayes_accuracy:\\t\\t\\t',bayes_ac)","7885281d":"models = pd.DataFrame({'Model': ['LogisticRegression','MLP','Bayes','SVM',\n                                      'RandomForest','DecisionTree_Classifier','KNN'],'Score': [lr_ac,MLP_ac,bayes_ac,svr_ac,rdf_ac,dtree_ac,knn_ac]})\n\nmodels.sort_values(by = 'Score', ascending = False).reset_index(drop=True)","6ddb5286":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"blue\",\"black\",\"red\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(20,8))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=models['Score'],y=models['Model'], palette=colors )\n","af1c39da":"DT2= DecisionTreeClassifier(criterion= 'gini', max_depth= 5, min_samples_leaf= 2, min_samples_split= 3, splitter= 'best',random_state=22)\n\nscores = cross_val_score(DT2, x , y , cv = 10, scoring = 'accuracy')\nprint(scores)\nprint(scores.mean())\n","7bef94e1":"lr_c2=LogisticRegression()\nscores_Lr = cross_val_score(lr_c2, x , y , cv = 10, scoring = 'accuracy')\nprint(scores_Lr)\nprint(scores_Lr.mean())","aaf129ba":"mlp2=MLPClassifier(activation='relu', hidden_layer_sizes= (20, 30), learning_rate_init= 0.001, max_iter=200, solver ='adam',random_state=22)\nscores_mlp = cross_val_score(mlp2, x , y , cv = 10, scoring = 'accuracy')\nprint(scores_mlp)\nprint(scores_mlp.mean())","c0c59049":"gb_2 = GaussianNB()\nscores_gb =cross_val_score(gb_2, x , y , cv = 10, scoring = 'accuracy')\nprint(scores_gb)\nprint(scores_gb.mean())","59859154":"svc_2 = SVC(C= 100, kernel= 'poly',degree=2,random_state=22)\nscores_svc = cross_val_score(svc_2,x,y,cv=10,scoring='accuracy')\n\nprint(scores_svc)\nprint(scores_svc.mean())","d7bafa58":"rdf_2 = RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=22)\nscores_rd = cross_val_score(rdf_2,x,y,cv=10,scoring='accuracy')\n\nprint(scores_rd)\nprint(scores_rd.mean())","b0c5aa6e":"knn_2 = KNeighborsClassifier(n_neighbors=2)\nscores_kn = cross_val_score(knn_2,x,y,cv = 10,scoring='accuracy')\n\nprint(scores_kn)\nprint(scores_kn.mean())","f0af1e93":"model = ['KNN','Randomforest','DesitionTree','NeuralNetwork','SVM','LogisticRegression','NaiveBayes']\ntreintest = [knn_ac,rdf_ac,dtree_ac,MLP_ac,svr_ac,lr_ac,bayes_ac]\ncross = [scores_kn.mean(),scores_rd.mean(),scores.mean(),scores_mlp.mean(),scores_svc.mean(),scores_Lr.mean(),scores_gb.mean()]","2144b874":"Comparison = pd.DataFrame({'Model':model,'TreinTest Accuracy':treintest,'Cross Val Accuracy':cross})\nComparison.sort_values(by='TreinTest Accuracy',ascending=False,inplace=True)","6ae98a54":"Comparison.reset_index(drop=True)","c36960d0":"kfold = model_selection.KFold(n_splits=10, random_state=22)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\nmodel4 = xgb.XGBClassifier()\nestimators.append(('xgboost', model4))\nmodel5 = KNeighborsClassifier()\nestimators.append(('Knn', model5))\nmodel6= RandomForestClassifier()\nestimators.append(('Random forest', model6))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, xtrain, ytrain, cv=kfold)\nprint(results.mean())","893a51c3":"# Duplicate","c7d8bd3a":"# Outlier detection","842c14a4":"# EDA & Data analysic","20d1b52f":"## balancing","4f15bfd1":"# missing value","a1527863":"## voting","fa448159":"## Data scailng","61f826b5":"# Train Test","7dedd6f6":"## the model","59cc6ece":"# Data encoding"}}