{"cell_type":{"341aa3e5":"code","0dd6f4dc":"code","aafa8b5f":"code","a6e85801":"code","b420c114":"code","7903f0aa":"code","274eb959":"code","719ffc30":"code","c84391e6":"code","ec70fe48":"code","9a51f435":"code","d157b5ab":"code","119cdd96":"code","6ca38b51":"code","5f4b98d8":"code","38ebb269":"code","efe2285e":"markdown","2a8edf90":"markdown","d70c0b4b":"markdown","0f581da3":"markdown","15d75264":"markdown","79de298b":"markdown","ccf5776c":"markdown","33b4e721":"markdown","5b5b6a81":"markdown"},"source":{"341aa3e5":"! pip install ftfy regex tqdm\n! pip install git+https:\/\/github.com\/openai\/CLIP.git","0dd6f4dc":"import numpy as np\nimport torch\n\nprint(\"Torch version:\", torch.__version__)\n\nassert torch.__version__.split(\".\") >= [\"1\", \"7\", \"1\"], \"PyTorch 1.7.1 or later is required\"","aafa8b5f":"import clip\n\nclip.available_models()","a6e85801":"model, preprocess = clip.load(\"ViT-B\/32\")\nmodel.cuda().eval()\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","b420c114":"preprocess","7903f0aa":"clip.tokenize(\"Hello World!\")","274eb959":"import os\nimport skimage\nimport IPython.display\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\nfrom collections import OrderedDict\nimport torch\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# images in skimage to use and their textual descriptions\ndescriptions = {\n    \"page\": \"a page of text about segmentation\",\n    \"chelsea\": \"a facial photo of a tabby cat\",\n    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n    \"rocket\": \"a rocket standing on a launchpad\",\n    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n    \"camera\": \"a person looking at a camera on a tripod\",\n    \"horse\": \"a black-and-white silhouette of a horse\", \n    \"coffee\": \"a cup of coffee on a saucer\"\n}","719ffc30":"original_images = []\nimages = []\ntexts = []\nplt.figure(figsize=(16, 5))\n\nfor filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n    name = os.path.splitext(filename)[0]\n    if name not in descriptions:\n        continue\n\n    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n  \n    plt.subplot(2, 4, len(images) + 1)\n    plt.imshow(image)\n    plt.title(f\"{filename}\\n{descriptions[name]}\")\n    plt.xticks([])\n    plt.yticks([])\n\n    original_images.append(image)\n    images.append(preprocess(image))\n    texts.append(descriptions[name])\n\nplt.tight_layout()\n","c84391e6":"image_input = torch.tensor(np.stack(images)).cuda()\ntext_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()","ec70fe48":"with torch.no_grad():\n    image_features = model.encode_image(image_input).float()\n    text_features = model.encode_text(text_tokens).float()","9a51f435":"image_features \/= image_features.norm(dim=-1, keepdim=True)\ntext_features \/= text_features.norm(dim=-1, keepdim=True)\nsimilarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T","d157b5ab":"count = len(descriptions)\n\nplt.figure(figsize=(20, 14))\nplt.imshow(similarity, vmin=0.1, vmax=0.3)\n# plt.colorbar()\nplt.yticks(range(count), texts, fontsize=18)\nplt.xticks([])\nfor i, image in enumerate(original_images):\n    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\nfor x in range(similarity.shape[1]):\n    for y in range(similarity.shape[0]):\n        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n\nfor side in [\"left\", \"top\", \"right\", \"bottom\"]:\n  plt.gca().spines[side].set_visible(False)\n\nplt.xlim([-0.5, count - 0.5])\nplt.ylim([count + 0.5, -2])\n\nplt.title(\"Cosine similarity between text and image features\", size=20)","119cdd96":"from torchvision.datasets import CIFAR100\n\ncifar100 = CIFAR100(os.path.expanduser(\"~\/.cache\"), transform=preprocess, download=True)","6ca38b51":"text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\ntext_tokens = clip.tokenize(text_descriptions).cuda()","5f4b98d8":"with torch.no_grad():\n    text_features = model.encode_text(text_tokens).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\ntext_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\ntop_probs, top_labels = text_probs.cpu().topk(5, dim=-1)","38ebb269":"plt.figure(figsize=(16, 16))\n\nfor i, image in enumerate(original_images):\n    plt.subplot(4, 4, 2 * i + 1)\n    plt.imshow(image)\n    plt.axis(\"off\")\n\n    plt.subplot(4, 4, 2 * i + 2)\n    y = np.arange(top_probs.shape[-1])\n    plt.grid()\n    plt.barh(y, top_probs[i])\n    plt.gca().invert_yaxis()\n    plt.gca().set_axisbelow(True)\n    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n    plt.xlabel(\"probability\")\n\nplt.subplots_adjust(wspace=0.5)\nplt.show()","efe2285e":"## Calculating cosine similarity\n\nWe normalize the features and calculate the dot product of each pair.","2a8edf90":"# Setting up input images and texts\n\nWe are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n\nThe tokenizer is case-insensitive, and we can freely give any suitable textual descriptions.","d70c0b4b":"# Text Preprocessing\n\nWe use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects.","0f581da3":"## Building features\n\nWe normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features.","15d75264":"# Loading the model\n\n`clip.available_models()` will list the names of available CLIP models.","79de298b":"# Preparation for Colab\n\nMake sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed.","ccf5776c":"# Zero-Shot Image Classification\n\nYou can classify images using the cosine similarity (times 100) as the logits to the softmax operation.","33b4e721":"# Interacting with CLIP\n\nThis is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications.","5b5b6a81":"# Image Preprocessing\n\nWe resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n\nThe second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n\n"}}