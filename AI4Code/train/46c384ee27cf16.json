{"cell_type":{"7f3dac3a":"code","ee87d1fa":"code","b782b408":"code","969dc120":"code","b2745f4a":"code","bae2a2d6":"code","9ecf5988":"code","8c1442ac":"code","0cd65631":"code","1ef0fc7b":"code","d17658f7":"code","0161025a":"code","255eefdd":"code","2b6b2bfe":"code","35dc66cc":"code","250878bf":"code","e7260558":"code","20972746":"code","edf8ab90":"code","ee354340":"code","86c81497":"code","999f7a7d":"code","300e223e":"code","cddb3265":"code","69e5912c":"code","e3933435":"code","8e13e534":"code","d9169e9d":"code","e89edb68":"code","899dd9cd":"code","8eb714bd":"code","9d243e22":"code","5f872cda":"code","2e5fa42e":"code","5fbd5108":"code","2704b353":"code","27e71d63":"code","d49b284e":"code","ed26683c":"code","f82a5977":"code","4c675d5f":"markdown","56ed1de5":"markdown","7da2a79a":"markdown","19b49132":"markdown","d869132b":"markdown","164e2481":"markdown","8a7a333c":"markdown","de59bb4d":"markdown","46f2f9f5":"markdown","e29bbc00":"markdown","1d635229":"markdown","f048d021":"markdown","2937069f":"markdown","b3013f33":"markdown","12e938a5":"markdown","1ff22d80":"markdown","c070c4ac":"markdown","d88d0024":"markdown","9c9bbd55":"markdown","c2fb6ddb":"markdown","7f68af04":"markdown","e50dc63c":"markdown","a30564a1":"markdown","b00fff1d":"markdown","77818155":"markdown","4e573366":"markdown","6c341cfd":"markdown","6d9c9eae":"markdown","34c99856":"markdown","0c55d462":"markdown","ed579b3a":"markdown","c88780f8":"markdown","404a9c3b":"markdown","83e080ea":"markdown","2029e678":"markdown","a4d3e6bc":"markdown"},"source":{"7f3dac3a":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn import svm\nimport mlxtend\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#Test Train Split\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n#Feature Scaling library\nfrom sklearn.preprocessing import StandardScaler\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n#ROC Curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","ee87d1fa":"df = pd.read_csv('..\/input\/dataparkinsons\/Data - Parkinsons')","b782b408":"df.shape # Check number of columns and rows in data frame","969dc120":"df.dtypes","b2745f4a":"df.head()","bae2a2d6":"df.describe().T","9ecf5988":"df.isnull().values.any() # If there are any null values in data set","8c1442ac":"#'name'  doesn't contribute in model building so it has to be removed from dataset\ndf = df.drop(['name'], axis=1)","0cd65631":"# studying the distribution of continuous attributes mean, median, mode defining the central tendency, \n# standard deviation refecting the spread and skewness reflecting the tail \ncols = list(df.columns)\ncols.remove('status')\nfor i in np.arange(len(cols)):\n    sns.distplot(df[cols[i]], color='blue')\n    #plt.xlabel('Experience')\n    plt.show()\n    print('Distribution of ',cols[i])\n    print('Mean is:',df[cols[i]].mean())\n    print('Median is:',df[cols[i]].median())\n    print('Mode is:',df[cols[i]].mode())\n    print('Standard deviation is:',df[cols[i]].std())\n    print('Skewness is:',df[cols[i]].skew())\n    print('Maximum is:',df[cols[i]].max())\n    print('Minimum is:',df[cols[i]].min())","1ef0fc7b":"df[\"status\"].value_counts().plot.bar(title='Freq dist of people with Parkinsons Disease')","d17658f7":"df_corr = df.corr()\nplt.subplots(figsize =(12, 7)) \nsns.heatmap(df_corr,annot=True)","0161025a":"# Dropping columns which are highly correlated to others since highly correlated data\n# gives inaccurate results with Logistic Regression\n#df = df.drop(['MDVP:Jitter(%)','MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ','Jitter:DDP','NHR','MDVP:Shimmer(dB)',\n#              'Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA','PPE'], axis=1)\n\ndf = df.drop(['MDVP:Jitter(Abs)','MDVP:RAP','MDVP:PPQ','Jitter:DDP','NHR','MDVP:Shimmer(dB)',\n              'Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ','Shimmer:DDA','spread1'], axis=1)","255eefdd":"df_corr = df.corr()\nplt.subplots(figsize =(12, 7)) \nsns.heatmap(df_corr,annot=True)","2b6b2bfe":"# Scatter plot\nl = len(df)\n#col = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', \n#       'MDVP:Shimmer','HNR', 'RPDE', 'DFA',\n#       'spread1', 'spread2', 'D2']\n\n\ncol = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)','MDVP:Jitter(%)',\n       'MDVP:Shimmer','HNR', 'RPDE', 'DFA',\n       'PPE', 'spread2', 'D2']\nfor i in np.arange(len(col)):\n#    df.plot(x='MDVP:Fo(Hz)', y='status', kind='scatter')\n    \n    sns.scatterplot(x='status',y=col[i], data=df)\n    plt.xlabel(col[i])\n    plt.ylabel('status')\n    plt.show()","35dc66cc":"#col = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)','MDVP:Shimmer','HNR','RPDE','DFA','spread1','spread2','D2']\nsns.pairplot(df[col])\nplt.show()\n","250878bf":"# Checking the presence of outliers\nl = len(df)\n#col = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)','MDVP:Shimmer','HNR', 'RPDE', 'DFA','spread1', 'spread2', 'D2']\nfor i in np.arange(len(col)):\n    sns.boxplot(x= df[col[i]], color='cyan')\n    plt.show()\n    print('Boxplot of ',col[i])\n    #calculating the outiers in attribute \n    Q1 = df[col[i]].quantile(0.25)\n    Q2 = df[col[i]].quantile(0.50)\n    Q3 = df[col[i]].quantile(0.75) \n    IQR = Q3 - Q1\n    L_W = (Q1 - 1.5 *IQR)\n    U_W = (Q3 + 1.5 *IQR)    \n    print('Q1 is : ',Q1)\n    print('Q2 is : ',Q2)\n    print('Q3 is : ',Q3)\n    print('IQR is:',IQR)\n    print('Lower Whisker, Upper Whisker : ',L_W,',',U_W)\n    bools = (df[col[i]] < (Q1 - 1.5 *IQR)) |(df[col[i]] > (Q3 + 1.5 * IQR))\n    print('Out of ',l,' rows in data, number of outliers are:',bools.sum())   #calculating the number of outliers","e7260558":"#Removing outliers by removing data below lower whisker and above upper whisker\nl = len(df)\n#col = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)','MDVP:Shimmer','HNR', 'RPDE', 'DFA','spread1','spread2', 'D2']\nfor i in np.arange(len(col)):\n    Q1 = df[col[i]].quantile(0.25)\n    Q3 = df[col[i]].quantile(0.75)\n    IQR = Q3 - Q1\n    df = df[(df[col[i]] > (Q1 - 1.5 *IQR)) & (df[col[i]] < (Q3 + 1.5 *IQR))]","20972746":"# verifying for  under represented class in 'status' column\ndf['status'].value_counts() ","edf8ab90":"# show cleaned-up dataset\ndf.head()","ee354340":"X_bank = df.drop(['status'], axis=1)\nX = df.drop(['status'], axis=1)\ny = df['status']","86c81497":"#Scaling data\nSC = StandardScaler()\nXSC= SC.fit_transform(X)","999f7a7d":"# 4. Split the data into training and test set in the ratio of 70:30 respectively (5 marks) \n\nX_train, X_test, y_train, y_test = train_test_split(XSC, y, train_size=0.7, test_size=0.3, random_state=56)","300e223e":"# verifying to ensure there are some records with 'status' = 1 for training and testing both.\nprint('y_train value counts are:\\n',y_train.value_counts())\nprint('y_test value counts are:\\n',y_test.value_counts())","cddb3265":"# Fit model on the Train-set\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nscores = model_selection.cross_val_score(logreg,XSC,y, cv=3,scoring= 'accuracy') \nprint('score', scores.mean())\n\n# Predict Test-set\n#y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n\n\n# Coefficient and intercept of model\ncoef = pd.DataFrame(logreg.coef_)\ncoef['intercept'] = logreg.intercept_\nprint('\\n\\nCoefficient :',coef)\n\n# Score of model\nmodel_score = logreg.score(X_test,y_test)\nprint('\\nScore of the model '+str(model_score))\n\n#Confusion metrics\nLGRcm_matrix = metrics.confusion_matrix(y_test,logreg.predict(X_test))\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test,logreg.predict(X_test)))","69e5912c":"#Confusion metrics :\n#  [[ 4  6]\n#   [ 2 34]]\n\n#true positives (TP): These are cases in which we predicted yes, and were actually yes.\nTP=34\n#true negatives (TN): We predicted no, and they were actually yes.\nTN=4\n#false positives (FP): We predicted yes, but they were actually no.(Also known as a \"Type I error.\")\nFP=6\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=2\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of logistic regression classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('Logistic regression Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('Logistic regression Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('Logistic regression Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('Logistic regression Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\n#----------------------------------------------------------------------------------------------------------------\n#AUC\nprint('Logistic regression Area Under the Curv: ',round(roc_auc_score(y_test,logreg.predict(X_test))*100))\n\nprint('\\nLogistic Regression Classification report:\\n',classification_report(y_test, logreg.predict(X_test)))\n# For better understanding of confusion matrix when 3 nearest Neighbours we plot it on heatmap\n\nLGRcm_matrix = metrics.confusion_matrix(y_test,logreg.predict(X_test))\n\nreport_LGR = classification_report(y_test,logreg.predict(X_test),labels=[1,0])\n\n#Area Under the ROC Curv of Logistic Regression\n\nprint('Logistic regression AUC: ',round(roc_auc_score(y_test,logreg.predict(X_test))*100))\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\n","e3933435":"# Call Nearest Neighbour algorithm, keeping number of neighbours as 3\nNNH = KNeighborsClassifier(n_neighbors= 3 , weights = 'uniform' )\nNNH.fit(X_train, y_train)\n\n# Score of the Model\nprint('NNH score when 3 nearest neighbours:', NNH.score(X_test, y_test))\n\n\n# For every test data point, predict it's label based on 3 nearest neighbours in this model. The majority class will \n# be assigned to the test data point\n\nprint('\\nConfusion metrics when 3 nearest neighbour:\\n', metrics.confusion_matrix(y_test,NNH.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, NNH.predict(X_test)*100))\n\n#-------------------------------------------------------------------------------------------------------------------\n#Iteration 1:\n# Call Nearest Neighbour algorithm, keeping number of neighbours as 5\nNNH1 = KNeighborsClassifier(n_neighbors= 5 , weights = 'uniform' )\nNNH1.fit(X_train, y_train)\n\n# Score of the Model\nprint('\\nNNH score when 5 nearest neighbours :', NNH1.score(X_test, y_test))\n\n# For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n# be assigned to the test data point\n\nprint('\\nConfusion metrics when 5 nearest neighbour:\\n',metrics.confusion_matrix(y_test,NNH1.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, NNH1.predict(X_test)*100))\n\n#-----------------------------------------------------------------------------------------------------------------\n\n#Iteration 2:\n\n# Call Nearest Neighbour algorithm, keeping number of neighbours as 9\nNNH2 = KNeighborsClassifier(n_neighbors= 9, weights = 'uniform' )\nNNH2.fit(X_train, y_train)\n\nprint('\\nNNH score when 9 nearest neighbours :', NNH2.score(X_test, y_test))\n\nprint('\\nConfusion metrics when 9 nearest neighbour:\\n',metrics.confusion_matrix(y_test,NNH2.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, NNH2.predict(X_test)*100))\n\n#----------------------------------------------------------------------------------------------\n#Classification reports\n\nprint('\\nClassification report for 3 nearest neighbour\\n',classification_report(y_test,NNH.predict(X_test)))\nprint('\\nClassification report for 5 nearest neighbour\\n',classification_report(y_test,NNH2.predict(X_test)))\nprint('\\nClassification report for 9 nearest neighbour\\n',classification_report(y_test,NNH2.predict(X_test)))\n\nreport_KNN = classification_report(y_test,NNH.predict(X_test))","8e13e534":"#Since AUC and NNH score is more for 3 nearest neighbour calculating metrices\n\n#Confusion metrics when 3 nearest neighbour:\n#     [[ 9  1]\n#      [ 3 33]]\n\n#true positives (TP): These are cases in which we predicted yes, and were actually yes.\nTP=33\n#true negatives (TN): We predicted no, and they were actually yes.\nTN=9\n#false positives (FP): We predicted yes, but they were actually no.(Also known as a \"Type I error.\")\nFP=1\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=3\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of KNN classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('KNN Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('KNN Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('KNN Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('KNN Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\n#----------------------------------------------------------------------------------------------------------------\n#AUC\nprint('KNN Area Under the Curv: ',roc_auc_score(y_test, NNH.predict(X_test)*100))\n\n# For better understanding of confusion matrix when 3 nearest Neighbours we plot it on heatmap\n\nKNNcm_matrix = metrics.confusion_matrix(y_test,NNH.predict(X_test))\n\nreport_KNN = classification_report(y_test,NNH.predict(X_test),labels=[1,0])\n\n#ROC Curve of KNN\n\nprint('KNN AUC: ',roc_auc_score(y_test, NNH.predict(X_test)*100))\nKNN_roc_auc = roc_auc_score(y_test, NNH.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, NNH.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='KNN (area = %0.2f)' % KNN_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()\n","d9169e9d":"#Iteration 1 - Fitting all variables, cleaned and normalized data\nGNB1 = GaussianNB()\nGNB1.fit(X_train, y_train)\n\n# Score of the Model\nprint('GNB score :', GNB1.score(X_test, y_test))\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test, GNB1.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, GNB1.predict(X_test)*100))","e89edb68":"#CALCULATING METRICES FOR CHECING MODEL Naive Bayes\n\n#Confusion metrics :\n#    [[ 6  4]\n#     [ 8 28]]\n\n#true positives (TP): These are cases in which we predicted yes, and were actually yes.\nTP=28\n#true negatives (TN): We predicted no, and they were actually yes.\nTN=6\n#false positives (FP): We predicted yes, but they were actually no.(Also known as a \"Type I error.\")\nFP=4\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=8\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of GNB classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('GNB Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('GNB Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('GNB Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('GNB Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\nprint('GNB AUC: ',roc_auc_score(y_test, GNB1.predict(X_test)*100))\n\n#-------------------------------------------------------------------------------------------------------------\n#AUC\nprint('GNB Area Under the Curv: ',roc_auc_score(y_test, GNB1.predict(X_test)*100))\n\nGNBcm_matrix = metrics.confusion_matrix(y_test, GNB1.predict(X_test))\n\nreport_GNB = classification_report(y_test,GNB1.predict(X_test),labels=[1,0])\n\n#ROC Curve of Naive Bayes\nprint('GNB AUC: ',roc_auc_score(y_test, GNB1.predict(X_test)*100))\nGNB_roc_auc = roc_auc_score(y_test, GNB1.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, GNB1.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='GNB (area = %0.2f)' % GNB_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","899dd9cd":"svm1 = svm.SVC(gamma=0.025, C=3)   \nsvm1.fit(X_train , y_train)\ny_pred = svm1.predict(X_test)\n# Score of the Model\nprint('SVM score :', svm1.score(X_test, y_test))\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test, svm1.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, svm1.predict(X_test)*100))","8eb714bd":"#CALCULATING METRICES FOR CHECING MODEL SVM\n\n#Confusion metrics :\n#    [[ 4  6]\n#     [ 0 36]]\n\n#true positives (TP): These are cases in which we predicted yes, and were actually yes.\nTP=36\n#true negatives (TN): We predicted no, and they were actually yes.\nTN=4\n#false positives (FP): We predicted yes, but they were actually no.(Also known as a \"Type I error.\")\nFP=6\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=0\n\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of SVM classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('SVM Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('SVM Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('SVM Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('SVM Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\n#-------------------------------------------------------------------------------------------------------------\n#AUC\nprint('SVM Area Under the Curv: ',roc_auc_score(y_test, svm1.predict(X_test)*100))\n\nSVMcm_matrix = metrics.confusion_matrix(y_test, svm1.predict(X_test))\n\nreport_svm = classification_report(y_test,svm1.predict(X_test),labels=[1,0])\n","9d243e22":"\nclf1 = KNeighborsClassifier(n_neighbors=3)\nclf2 = RandomForestClassifier(random_state=56)\nclf3 = GaussianNB()\nclf4 = svm.SVC(gamma=0.025, C=3) \nlr = LogisticRegression()\n\n\nsclf = StackingCVClassifier(classifiers=[clf1, clf2,clf3, clf4],\n                            meta_classifier=lr,\n                            random_state=56)\n\nsc = sclf.fit(X_train, y_train)\ny_predict = sc.predict(X_test)\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test, sc.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, sc.predict(X_test)*100))\n","5f872cda":"#CALCULATING METRICES FOR CHECING MODEL(Logestic Regression as meta classifier)\n\n#Confusion metrics :\n#     [[ 6  4]\n#      [ 1 35]]\n\n#true positives (TP): These are cases in which we predicted yes, and were actually yes.\nTP=35\n#true negatives (TN): We predicted no, and they were actually yes.\nTN=6\n#false positives (FP): We predicted yes, but they were actually no.(Also known as a \"Type I error.\")\nFP=4\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=1\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of Meta classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('Meta Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('Meta Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('Meta Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('Meta Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\nprint('Meta Area Under the Curv: ',roc_auc_score(y_test, sc.predict(X_test)*100))\n\n#-------------------------------------------------------------------------------------------------------------\n\nsclfcm_matrix = metrics.confusion_matrix(y_test, sc.predict(X_test))\n\nreport_sclf = classification_report(y_test,sc.predict(X_test),labels=[1,0])\n\n#ROC Curve of Meta Classifier\n\nprint('Meta AUC: ',roc_auc_score(y_test, sc.predict(X_test)*100))\nmeta_roc_auc = roc_auc_score(y_test, sc.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, sc.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Meta (area = %0.2f)' % meta_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.savefig('Log_ROC')\nplt.show()","2e5fa42e":"print('3-fold cross validation:\\n')\n\nfor clf, label in zip([clf1, clf2,clf3,clf4, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'SVM',\n                       'StackingClassifier']):\n\n    scores = model_selection.cross_val_score(clf, XSC, y, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n    scores1 = model_selection.cross_val_score(clf, XSC, y, \n                                              cv=3, scoring='recall')\n    print(\"Recall: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores1.mean(), scores1.std(), label))\n    scores2 = model_selection.cross_val_score(clf, XSC, y, \n                                              cv=3, scoring='precision')\n    print(\"Precision: %0.2f (+\/- %0.2f) [%s]\" \n          % (scores2.mean(), scores2.std(), label))\n    scores3 = model_selection.cross_val_score(clf,XSC,y, cv=3, scoring= 'f1')\n    print('F1 Score: %0.2f (+\/- %0.2f) [%s]' % (scores3.mean(),scores3.std(),label))\n    \n    print('\\n')","5fbd5108":"rfcl = RandomForestClassifier(random_state=56)\nrfcl1 = rfcl.fit(X_train, y_train)\ny_predict = rfcl1.predict(X_test)\n\nprint(rfcl1.score(X_test , y_test))\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test, rfcl1.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, rfcl1.predict(X_test)*100))\n","2704b353":"#CALCULATING METRICES FOR CHECING MODEL (Random Forest)\n\n#Confusion metrics :\n#    [[ 6  4]\n#     [ 1 35]]\n\n#true positives (TP): These are cases in which we predicted yes, and actually yes\nTP=35\n#true negatives (TN): We predicted no, and they were actually were no\nTN=6\n#false positives (FP): We predicted yes, but they were not.(Also known as a \"Type I error.\")\nFP=4\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=1\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of RFCL classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('RFCL Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('RFCL Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('RFCL Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('RFCL Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\n\n#AUC\nprint('RFCL Area Under the Curv: ',roc_auc_score(y_test, rfcl1.predict(X_test)*100))\n\n#-------------------------------------------------------------------------------------------------------------\n\n\nrfclcm_matrix = metrics.confusion_matrix(y_test, rfcl1.predict(X_test))\n\nreport_rfcl = classification_report(y_test,rfcl1.predict(X_test),labels=[1,0])\n\n#ROC Curve of Bagging Classifier\n\nprint('RFCL AUC: ',roc_auc_score(y_test, rfcl1.predict(X_test)*100))\nrfcl_roc_auc = roc_auc_score(y_test, rfcl1.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, rfcl1.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='RFCL (area = %0.2f)' % rfcl_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","27e71d63":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(base_estimator=rfcl, n_estimators=40,random_state=56)\n\nbgcl = bgcl.fit(X_train, y_train)\ny_predict = bgcl.predict(X_test)\n\nprint('bgcl Score: ',bgcl.score(X_test , y_test))\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test, bgcl.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, bgcl.predict(X_test)*100))\n","d49b284e":"#CALCULATING METRICES FOR CHECING MODEL (Bagging Classifier)\n\n#Confusion metrics :\n#    [[ 5  5]\n#     [ 1 35]]\n\n#true positives (TP): These are cases in which we predicted yes, and were actually yes.\nTP=35\n#true negatives (TN): We predicted no, and they were actually yes.\nTN=5\n#false positives (FP): We predicted yes, but they were actually no.(Also known as a \"Type I error.\")\nFP=5\n#false negatives (FN): We predicted no, but they were actually yes.(Also known as a \"Type II error.\")\nFN=1\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of BGCL classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('BGCL Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('BGCL Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('BGCL Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('BGCL Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\n\n#AUC\nprint('BGCL Area Under the Curv: ',roc_auc_score(y_test, bgcl.predict(X_test)*100))\n\n#-------------------------------------------------------------------------------------------------------------\n\n\nBGCLcm_matrix = metrics.confusion_matrix(y_test, bgcl.predict(X_test))\n\nreport_BGCL = classification_report(y_test,bgcl.predict(X_test),labels=[1,0])\n\n#ROC Curve of Bagging Classifier\n\nprint('BGCL AUC: ',roc_auc_score(y_test, bgcl.predict(X_test)*100))\nBGCL_roc_auc = roc_auc_score(y_test, bgcl.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, bgcl.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='GNB (area = %0.2f)' % BGCL_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","ed26683c":"# For better understanding of logistic regression confusion matrix we plot it on heatmap\n\nHM = pd.DataFrame(LGRcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(4,2))\nprint('Confusion matrix for logistic regression ')\nsns.heatmap(HM,annot=True, fmt='g')\nplt.show()\n\n# For better understanding of KNN confusion matrix we plot it on heatmap\nHM = pd.DataFrame(KNNcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(4,2))\nprint('KNN confusion matrix\\n')\nsns.heatmap(HM,annot=True, fmt='g') \nplt.show()\n\n# For better understanding of GNB confusion matrix we plot it on heatmap\nHM = pd.DataFrame(GNBcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(4,2))\nprint('GNB confusion matrix\\n')\nsns.heatmap(HM,annot=True, fmt='g') \nplt.show()\n\n\n# For better understanding of SVM confusion matrix we plot it on heatmap\nHM = pd.DataFrame(SVMcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(4,2))\nprint('SVM confusion matrix\\n')\nsns.heatmap(HM,annot=True, fmt='g') \nplt.show()\n\n\n\n# For better understanding of Meta Classifier Confusion matrix we plot it on heatmap\nHM = pd.DataFrame(sclfcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(4,2))\nprint('Meta Classifier confusion matrix\\n')\nsns.heatmap(HM,annot=True, fmt='g')\nplt.show()\n\n\n# For better understanding of 'RFCL' confusion matrix we plot it on heatmap\ndf_cm = pd.DataFrame(rfclcm_matrix, index = [i for i in [\"No\",\"Yes\"]],\n                  columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (4,2))\nprint('RFCL confusion matrix\\n')\nsns.heatmap(df_cm, annot=True ,fmt='g')\nplt.show()\n\n# For better understanding of 'BGCL' confusion matrix we plot it on heatmap\ndf_cm = pd.DataFrame(BGCLcm_matrix, index = [i for i in [\"No\",\"Yes\"]],\n                  columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (4,2))\nprint('BGCL confusion matrix\\n')\nsns.heatmap(df_cm, annot=True ,fmt='g')\nplt.show()","f82a5977":"print(\"\\nLogisitic Regression\\n\",report_LGR)\nprint(\"\\nKNN Classifier\\n\",report_KNN)\nprint(\"\\nNaive Bayes\\n\",report_GNB)\nprint(\"\\nSVM\\n\",report_svm)\nprint(\"\\nMeta Classifier\\n\",report_sclf)\nprint(\"\\nRandom Forest\",report_rfcl)\nprint(\"\\nBagging Classifier\\n\",report_BGCL)","4c675d5f":"# Confusion Matrix ","56ed1de5":"Print the confusion matrix for all the models ","7da2a79a":"#Bivariate Analysis using Correlation Coefficients and scatter plot\n","19b49132":"Standard deviation for most of the attributes is very low apart from the MVDP frequencies. It is highest for MDVP:Fhi(Hz) and lowest for MDVP:Jitter(Abs).\nSince, the mean and 50% value of spread1, spread2 are near by and the max and min values are almost equidistant fom the mean. \nLowest Frequency (Hz) is 65.476000(MDVP:Flo(Hz))is less than the and highest is 592.030000 (MDVP:Fo(Hz))","d869132b":"# SVM Model","164e2481":"# Case Study : Classification of patients with Parkinson\u2019s Disease using their voice recordings","8a7a333c":"Data Description & Context:\nParkinson\u2019s Disease (PD) is a degenerative neurological disorder marked by\ndecreased dopamine levels in the brain. It manifests itself through a deterioration\nof movement, including the presence of tremors and stiffness. There is commonly\na marked effect on speech, including dysarthria (difficulty articulating sounds),\nhypophonia (lowered volume), and monotone (reduced pitch range). Additionally,\ncognitive impairments and changes in mood can occur, and risk of dementia is\nincreased.\nTraditional diagnosis of Parkinson\u2019s Disease involves a clinician taking a\nneurological history of the patient and observing motor skills in various situations.\nSince there is no definitive laboratory test to diagnose PD, diagnosis is often\ndifficult, particularly in the early stages when motor effects are not yet severe.\nMonitoring progression of the disease over time requires repeated clinic visits by\nthe patient. An effective screening process, particularly one that doesn\u2019t require a\nclinic visit, would be beneficial. Since PD patients exhibit characteristic vocal\nfeatures, voice recordings are a useful and non-invasive tool for diagnosis. If\nmachine learning algorithms could be applied to a voice recording dataset to\naccurately diagnosis PD, this would be an effective screening step prior to an\nappointment with a clinician\nDomain:\nMedicine\nProprietary content. \u00a9Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited\nAttribute Information:\nname - ASCII subject name and recording number\nMDVP:Fo(Hz) - Average vocal fundamental frequency\nMDVP:Fhi(Hz) - Maximum vocal fundamental frequency\nMDVP:Flo(Hz) - Minimum vocal fundamental frequency\nMDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several\nmeasures of variation in fundamental frequency\nMDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,S\nhimmer:DDA - Several measures of variation in amplitude\nNHR,HNR - Two measures of ratio of noise to tonal components in the voice\nstatus - Health status of the subject (one) - Parkinson's, (zero) - healthy\nRPDE,D2 - Two nonlinear dynamical complexity measures\nDFA - Signal fractal scaling exponent\nspread1,spread2,PPE - Three nonlinear measures of fundamental frequency\nvariation 9. car name: string (unique for each instance)\n    \nObjective:\nGoal is to classify the patients into the respective labels using the attributes from\ntheir voice recordings","de59bb4d":"# Random Forest","46f2f9f5":"# KNN Model","e29bbc00":"8. Train at least one standard Ensemble model - Random forest, Bagging,\nBoosting etc, and note the accuracy ","1d635229":"# Model Implementation","f048d021":"3. Using univariate & bivariate analysis to check the individual attributes for\ntheir basic statistics such as central values, spread, tails, relationships\nbetween variables etc. mention your observations","2937069f":"DATA SETUP","b3013f33":"# 2. It is always a good practice to eye-ball raw data to get a feel of the data in\nterms of number of records, structure of the file, number of attributes,types of attributes and a general idea of likely challenges in the dataset. Mention a few comments in this regard (5 points)\n","12e938a5":"Looking at the central tendancy of the data, most of the medians are less than the mean , though a few attributes\nhave very close values of mean and median. \nStandard deviation is mostly high, apart from MDVP:Jitter(%),MDVP:RAP, MDVP:PPQ,Jitter:DDP,MDVP:Shimmer,Shimmer:APQ3, \nShimmer:APQ5, MDVP:APQ,Shimmer:DDA, NHR,PPE where it is near 0.\nMost of the attributes are right skewed with a long tail apart from HNR,RPDE and DFA which are left skewed.\nThe attributes 'spread1', 'spread2' and 'D2' have tails or outliers present on both the sides.\nThe data is mostly positivelyskewed apart from HNR which is negatively skewed.","1ff22d80":"# 1. Load the dataset","c070c4ac":"MDVP:Jitter(%) is highly correlated with MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP,NHR  (so drop MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP,NHR ) \n\nMDVP:Shimmer  is highly correlated with MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA( so drop MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA)\n\ndropping spread 1\n\n\nPPE is highly correlated to spread1,MDVP:Jitter(%),MDVP:Shimmer,RPDE,spread2 ( so drop PPE)\n\nMDVP:Jitter(%) is highly correlated to spread1 and MDVP:Shimmer (so drop MDVP:Jitter(%))","d88d0024":"There are no outliers in 'MDVP:Fo(Hz)','RPDE', 'DFA'.\nThere are 11,9,8,4,1 outliers on the side of upper whisker in MDVP:Fhi(Hz), MDVP:Flo(Hz), MDVP:Shimmer,spread1 and D2 respectively.\nThere are 3 outliers on the side of the lower whisker in HNR.\nspread2 has outliers on both sides of the the whiskers","9c9bbd55":"We can see few of the attributes are normally distributed like HNR column.\nA few of the attributes are positively correlated like the MDVP:Shimmer,spread1, spread2 columns etc.\nMDVP:Fo(Hz), MDVP:Fhi(Hz),MDVP:Flo(Hz) are positively correlated with each other.            \nMDVP:Shimmer has negative correlation with HNR","c2fb6ddb":"# Logistic Regression Model Fitting","7f68af04":"Scatter plot shows that most of the attributes have more data for status 1 than for status 0.","e50dc63c":"9. Compare all the models (minimum 5) and pick the best one among them","a30564a1":"# Scaling and Splitting dataset for KNN","b00fff1d":"Apart from Name which is an object and status which is and integer, all other attributes are floating point.","77818155":"Univariate Analysis","4e573366":"# Bagging Classifier","6c341cfd":"# Classification Reports","6d9c9eae":"# Applying Gaussian Naive Bayes","34c99856":"Cross Validation ","0c55d462":"#Model has improved with KNN and n_neighbours as 3, also AUC have improved","ed579b3a":"# Meta Classifier","c88780f8":"Looking at the Classification Report -\nSince, one of the 2 classes is under represented in 'status'. Deciding by accuaracy at model level can be misleading. So need to consider the accuracy at class level i.e; recall.\n\nRecall\/Sensitivity is best for SVM (Recall: 0.99 (+\/- 0.01) [SVM]). \n\nAccuracy,  precision and F1 score are best for Stacking classifier.\nSo Stacking classifier and SVM are best models out of all, since the recall for both is almost same, these can be used to \nclassify the patients into the respective labels.\n","404a9c3b":"There are 195 rows and 24 columns in data.","83e080ea":"6. Train at least 3 standard classification algorithms - Logistic Regression,\nNaive Bayes\u2019, SVM, k-NN etc, and note down their accuracies on the test\ndata (10 points)\n","2029e678":"Accuracy of logistic regression classifier on test set: 82.61%\nLogistic regression Misclassification Rate: It is often wrong: 17.39%\nLogistic regression Sensitivity: When its actually yes how often it predicts yes: 94.44%\nLogistic regression Specificity: When its actually no, how often does it predict no: 40.00%\nLogistic regression Precision: When it predicts yes, how often is it correct: 85.00%\nLogistic regression Area Under the Curv:  67.0\n\n#*************************************************************************************************************\nAccuracy of KNN classifier on test set: 91.30%\nKNN Misclassification Rate: It is often wrong: 8.70%\nKNN Sensitivity: When its actually yes how often it predicts yes: 91.67%\nKNN Specificity: When its actually no, how often does it predict no: 90.00%\nKNN Precision: When it predicts yes, how often is it correct: 97.06%\nKNN Area Under the Curv:  0.9083333333333332\nKNN AUC:  0.9083333333333332\n#*************************************************************************************************************\nAccuracy of GNB classifier on test set: 73.91%\nGNB Misclassification Rate: It is often wrong: 26.09%\nGNB Sensitivity: When its actually yes how often it predicts yes: 77.78%\nGNB Specificity: When its actually no, how often does it predict no: 60.00%\nGNB Precision: When it predicts yes, how often is it correct: 87.50%\nGNB AUC:  0.6888888888888889\nGNB Area Under the Curv:  0.6888888888888889\nGNB AUC:  0.6888888888888889\n#************************************************************************************************************\nAccuracy of SVM classifier on test set: 86.96%\nSVM Misclassification Rate: It is often wrong: 13.04%\nSVM Sensitivity: When its actually yes how often it predicts yes: 100.00%\nSVM Specificity: When its actually no, how often does it predict no: 40.00%\nSVM Precision: When it predicts yes, how often is it correct: 85.71%\nSVM Area Under the Curv:  0.7\n#************************************************************************************************************\n\nAccuracy of Meta classifier on test set: 89.13%\nMeta Misclassification Rate: It is often wrong: 10.87%\nMeta Sensitivity: When its actually yes how often it predicts yes: 97.22%\nMeta Specificity: When its actually no, how often does it predict no: 60.00%\nMeta Precision: When it predicts yes, how often is it correct: 89.74%\nMeta Area Under the Curv:  0.7861111111111111\nMeta AUC:  0.7861111111111111\n\n#*************************************************************************************************************\n\nAccuracy of RFCL classifier on test set: 89.13%\nRFCL Misclassification Rate: It is often wrong: 10.87%\nRFCL Sensitivity: When its actually yes how often it predicts yes: 97.22%\nRFCL Specificity: When its actually no, how often does it predict no: 60.00%\nRFCL Precision: When it predicts yes, how often is it correct: 89.74%\nRFCL Area Under the Curv:  0.7861111111111111\nRFCL AUC:  0.7861111111111111\n\n#*************************************************************************************************************\nAccuracy of BGCL classifier on test set: 86.96%\nBGCL Misclassification Rate: It is often wrong: 13.04%\nBGCL Sensitivity: When its actually yes how often it predicts yes: 97.22%\nBGCL Specificity: When its actually no, how often does it predict no: 50.00%\nBGCL Precision: When it predicts yes, how often is it correct: 87.50%\nBGCL Area Under the Curv:  0.7361111111111112\nBGCL AUC:  0.7361111111111112\n    \n#*************************************************************************************************************\n\n3-fold cross validation:  from the output in the above steps\n\n\nAccuracy: 0.80 (+\/- 0.03) [KNN]\nRecall: 0.87 (+\/- 0.00) [KNN]\nPrecision: 0.87 (+\/- 0.04) [KNN]\nF1 Score: 0.87 (+\/- 0.02) [KNN]\n\n\nAccuracy: 0.82 (+\/- 0.04) [Random Forest]\nRecall: 0.96 (+\/- 0.02) [Random Forest]\nPrecision: 0.84 (+\/- 0.05) [Random Forest]\nF1 Score: 0.89 (+\/- 0.02) [Random Forest]\n\n\nAccuracy: 0.73 (+\/- 0.03) [Naive Bayes]\nRecall: 0.77 (+\/- 0.08) [Naive Bayes]\nPrecision: 0.87 (+\/- 0.05) [Naive Bayes]\nF1 Score: 0.81 (+\/- 0.03) [Naive Bayes]\n\n\nAccuracy: 0.82 (+\/- 0.05) [SVM]\nRecall: 0.99 (+\/- 0.01) [SVM]\nPrecision: 0.81 (+\/- 0.04) [SVM]\nF1 Score: 0.89 (+\/- 0.03) [SVM]\n\n\nAccuracy: 0.84 (+\/- 0.05) [StackingClassifier]\nRecall: 0.98 (+\/- 0.01) [StackingClassifier]\nPrecision: 0.84 (+\/- 0.05) [StackingClassifier]\nF1 Score: 0.91 (+\/- 0.03) [StackingClassifier]\n","a4d3e6bc":"In the target column, there are 48 healthy people & 147 people with Parkinson's disease."}}