{"cell_type":{"669d512e":"code","0fe963e9":"code","4959d42a":"code","c2f2bf76":"code","fe84f3b6":"code","4f38fac0":"code","4a76c008":"code","6b9e8dc5":"code","80ae31f0":"markdown","92bb190c":"markdown","ac74cc37":"markdown","faae3526":"markdown","54f99f9d":"markdown","07c95351":"markdown","ad21c125":"markdown","d89ce870":"markdown","5ae4573d":"markdown","19415700":"markdown","1183a620":"markdown","a9502db3":"markdown"},"source":{"669d512e":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","0fe963e9":"#Load our MNIST dataset.\n(XTrain, YTrain), (XTest, YTest) = mnist.load_data()\n\nprint('XTrain class = ',type(XTrain))\nprint('YTrain class = ',type(YTrain))\n\n# shape of our dataset.\nprint('XTrain shape = ',XTrain.shape)\nprint('XTest shape = ',XTest.shape)\nprint('YTrain shape = ',YTrain.shape)\nprint('YTest shape = ',YTest.shape)\n# Number of distinct values of our MNIST target\nprint('YTrain values = ',np.unique(YTrain))\nprint('YTest values = ',np.unique(YTest))\n# Distribution of classes in our dataset.\nunique, counts = np.unique(YTrain, return_counts=True)\nprint('YTrain distribution = ',dict(zip(unique, counts)))\nunique, counts = np.unique(YTest, return_counts=True)\nprint('YTest distribution = ',dict(zip(unique, counts)))\n","4959d42a":"# we plot an histogram distribution of our test and train data.\nfig, axs = plt.subplots(1,2,figsize=(15,5)) \naxs[0].hist(YTrain, ec='black')\naxs[0].set_title('YTrain data')\naxs[0].set_xlabel('Classes') \naxs[0].set_ylabel('Number of occurrences')\naxs[1].hist(YTest, ec='black')\naxs[1].set_title('YTest data')\naxs[1].set_xlabel('Classes') \naxs[1].set_ylabel('Number of occurrences')\n# We want to show all ticks...\naxs[0].set_xticks(np.arange(10))\naxs[1].set_xticks(np.arange(10))\n\nplt.show()\n","c2f2bf76":"# Data normalization.\nXTrain = XTrain.astype('float32') \/ 255\nXTest = XTest.astype('float32') \/ 255\n# data reshapping.\nXTrain = XTrain.reshape((len(XTrain), np.prod(XTrain.shape[1:])))\nXTest = XTest.reshape((len(XTest), np.prod(XTest.shape[1:])))\n\nprint (XTrain.shape)\nprint (XTest.shape)\n","fe84f3b6":"InputModel = Input(shape=(784,))\nEncodedLayer = Dense(32, activation='relu')(InputModel)\nDecodedLayer = Dense(784, activation='sigmoid')(EncodedLayer)\n\nAutoencoderModel = Model(InputModel, DecodedLayer)\n# we can summarize our model.\nAutoencoderModel.summary()\n","4f38fac0":"# Let's train the model using adadelta optimizer\nAutoencoderModel.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nhistory = AutoencoderModel.fit(XTrain, XTrain,\n                    batch_size=256,\n                    epochs=100,\n                    shuffle=True,\n                    validation_data=(XTest, XTest))\n# Make prediction to decode the digits\nDecodedDigits = AutoencoderModel.predict(XTest)","4a76c008":"def plotmodelhistory(history): \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Autoencoder Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n# list all data in history\nprint(history.history.keys())\n# visualization of the loss minimization during the training process\nplotmodelhistory(history)","6b9e8dc5":"n=5\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    ax = plt.subplot(2, n, i + 1)\n    # input image\n    plt.imshow(XTest[i+10].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax = plt.subplot(2, n, i + 1 + n)\n    # Image decoded by our Auto-encoder\n    plt.imshow(DecodedDigits[i+10].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","80ae31f0":"Now let's visualize the distribution of our data by classes.","92bb190c":"### Visualization of the results.\nNow let's visualize for 5 images decoded by our model.","ac74cc37":"## Introduction.\nLet's start by understanding the operation of an auto-encoder.\n\n* Autoencoders are a class of neural network that attempt to recreate the input as its target using backpropagation. An autoencoder consists of two parts, an encoder and a decoder. The encoder will read the input and compress it to a compact representation, and the decoder will read the compact representation and recreate the input from it. In other words, the autoencoder tries to learn the identity function by minimizing the reconstruction error.\n\n* The number of hidden units in the autoencoder is typically less than the number of input (and output) units. This forces the encoder to learn a compressed representation of the input which the decoder reconstructs. If there is structure in the input data in the form of correlations between input features, then the autoencoder will discover some of these correlations, and end up learning a low dimensional representation of the data similar to that learned using principal component analysis (**PCA**).\n\n* The encoder and decoder components of an autoencoder can be implemented using either dense, convolutional, or recurrent networks, depending on the kind of data that is being modeled.\n\n* Autoencoders can also be stacked by successively stacking encoders that compress their input to smaller and smaller representations, and stacking decoders in the opposite sequence. Stacked autoencoders have greater expressive power and the successive layers of representations capture a hierarchical grouping of the input, similar to the convolution and pooling operations in convolutional neural networks.\n\n![auto-encoder.PNG](attachment:auto-encoder.PNG)\n\n* For example in the network shown above, we would first train layer **X** to reconstruct layer **X'** using the hidden layer **H1** (**ignoring H2**). We would then train the layer **H1** to reconstruct layer **H1'** using the hidden layer **H2**. Finally, we would stack all the layers together in the configuration shown and fine tune it to reconstruct **X'** from **X**. With better activation and regularization functions nowadays, however, it is quite common to train these networks in totality.","faae3526":"# Application of computer Vision: Understand Unsupervised learning auto-encoders with MNIST digit dataset.\n### Table of interest:\n1. Introduction\n2. Import all required Library\n3. Data Exploration and visualization.\n4. Data preprocessing.\n5. Building of our Auto-encoder.\n6. Model trainning and predict.\n7. Model evaluation.\n","54f99f9d":"## 6. Model trainning and predict.","07c95351":"***Please give an up-vote if you have found this kernel helpful. It will ke me motivate to do more.***\n\nThanks","ad21c125":"## 5. Building of our Auto-encoder.\nOur model has only one hidden(**encoded layer**) layer with 32 units neurons. The input and output layer(**decoded layer**) have each 784 units neurons.","d89ce870":"## 3. Data Exploration and visualization.","5ae4573d":"Now let's apply this principe of operation to our famous MNIST digit dataset.","19415700":"## 4. Data preprocessing.\nWe don't need more preprocessing here because we are in unsupervised learning. Just normalise and reshapping our data. We don't need the target data.","1183a620":"## 7. Model evaluation.\nThe `fit()` method on a Keras Model returns a `History` object. The `History.history` attribute is a dictionary recording training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\nFor our model we don't have metrics values because we didn't specify metrics when we compiled the model.","a9502db3":"## 2. Import all required Library"}}