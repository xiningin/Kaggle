{"cell_type":{"3592954a":"code","857f9ba0":"code","c451c08d":"code","da711d02":"code","4271debc":"code","db8d6dbb":"code","117afac8":"code","82c51f75":"code","8d8fc258":"code","868693e6":"code","4f61fdc2":"code","1434d2f1":"code","60361612":"code","e45852f6":"code","21f79807":"code","7cb43f18":"code","b9cd752d":"code","6c9318fe":"code","e69b27b9":"code","f84a6621":"code","139a6f27":"code","2741ceff":"code","059b348a":"code","38f0d382":"code","d1a00922":"code","74237f5e":"code","74125025":"code","21f7a99b":"markdown","540b33f2":"markdown","1b7cac6e":"markdown","ad7ba7c7":"markdown","46ab001f":"markdown","12066508":"markdown","8109d17d":"markdown","df731a3c":"markdown","c965fedc":"markdown","627d84fb":"markdown","57319f78":"markdown","deb8523e":"markdown","4c48d45a":"markdown","cb5b227f":"markdown"},"source":{"3592954a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","857f9ba0":"# To begin, I load in the csv-data into a pandas dataframe denoted as'df'\ndf = pd.read_csv('..\/input\/data.csv')\n# To take a peek into our data set\ndf.head()","c451c08d":"list(df.columns)","da711d02":"print(type(df['Age'][0]))\nprint(type(df['Nationality'][0]))\nprint(type(df['Overall'][0]))\nprint(type(df['Potential'][0]))\nprint(type(df['Value'][0]))\nprint(type(df['Wage'][0]))","4271debc":"def clean_d(string) :\n    last_char = string[-1]\n    if last_char == \"0\":\n        return 0\n    string = string[1:-1]\n    num = float(string)\n    if last_char == 'K':\n        num = num * 1000\n    elif last_char == 'M': \n        num = num * 1000000\n    return num","db8d6dbb":"df['Wage_Num'] = df.apply(lambda row: clean_d(row['Wage']), axis=1)\ndf.head()","117afac8":"df['Value_Num'] = df.apply(lambda row: clean_d(row['Value']), axis=1)\ndf.head()","82c51f75":"df.shape","8d8fc258":"df.isna().sum()","868693e6":"df = df.dropna(axis=0, subset=['Preferred Foot'])\ndf.isna().sum()","4f61fdc2":"import seaborn as sns\nsns.set()","1434d2f1":"# See the counts of right-footed players vs. left-footed players\nfoot_plots = df['Preferred Foot'].value_counts()\nfoot_plots.plot(kind='bar')","60361612":"sns.lineplot(x='Overall',y='Wage_Num',data=df)","e45852f6":"sns.lineplot(x='Overall',y='Value_Num',data=df)","21f79807":"#compare age with difference in potential to overall\ndf['Growth_Left'] = df['Potential'] - df['Overall']\nsns.lineplot(x='Age',y='Growth_Left',data=df)","7cb43f18":"sns.lineplot(x='Growth_Left',y='Wage_Num',data=df)","b9cd752d":"sns.lineplot(x='Age',y='Wage_Num',data=df)\n#Observation: line plot might not be best way to visualize this because of outliers","6c9318fe":"sns.lineplot(x='Age',y='Value_Num',data=df)","e69b27b9":"sns.lineplot(x='Growth_Left',y='Value_Num',data=df)","f84a6621":"sns.lineplot(x='Age',y='Overall',data=df)\n#Observation: outliers are significantly influencing overall trend","139a6f27":"top_100 = df[:100]\ntop_100.shape","2741ceff":"nationality_100_plots = top_100['Nationality'].value_counts()\nnationality_100_plots.plot(kind='bar')\n#Observation: European and South American nations dominate in the Top 100 players","059b348a":"age_100_plots = top_100['Age'].value_counts()\nage_100_plots.plot(kind='bar')\n#Observation: The late 20's are where the majority of Top-100 players are aged","38f0d382":"club_100_plots = top_100['Club'].value_counts()\nclub_100_plots.plot(kind='bar')\n#Observation: Almost ALL of the Top-100 players play in Europe, and if not, then in the generous salary-giving Chinese and Japanese nations","d1a00922":"#This seaborn function allows you to summarize all basic trends and correlations\nsns.pairplot(df, vars=['Age', 'Overall', 'Wage_Num', 'Value_Num', 'Potential', 'Growth_Left'])","74237f5e":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom learntools.core import *\n\n# Create target object and call it y\ny = df.Overall\n# Create X\nfeatures = ['Age', 'Value_Num', 'Wage_Num', 'Potential']\nX = df[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,train_size=0.8, test_size=0.2, random_state=1)\n\n# Specify Model\nml_model = DecisionTreeRegressor(random_state=1)\n# Fit Model\nml_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = ml_model.predict(val_X)\nval_mae = mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when using a Decision Tree: {:,.0f}\".format(val_mae))\nprint(train_X)\nprint(train_y)\nprint(val_X)\nprint(val_predictions)\nprint(val_y)","74125025":"# Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(val_y, rf_val_predictions)\n\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\nprint(train_X)\nprint(train_y)\nprint(val_X)\nprint(val_predictions)\nprint(val_y)","21f7a99b":"This concludes my first mini-project on my journey in Data Science. Please provide feedback on what to improve on and what to spend more focus in. How can I take this analysis to the next level?","540b33f2":"At this point, my data cleaning process is complete and I want to spend time conducting Exploratory Data Analysis, or EDA. For this I import the aesthetically-pleasing visualization library - seaborn.","1b7cac6e":"When I wrote my first versions of this data cleaning and transformation, I ran into significant problems that really confused me. All I wanted to do was iterate over the rows of the new column and change its value and data type. \n\nThe first time I tried it, I ran into a \"SettingWithCopy\" warning, which indicated that my code could be editing the copy of the value rather than the original value. \n\nWhen I changed my code and used the pandas loc function, the code worked without a warning but took an extremely long time. So I did some research and came upon this article: https:\/\/engineering.upside.com\/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2. Here, I saw that normal iteration (e.g. using a for loop as I was employing) was highly inefficient despite being highly logical due to the inner workings of pandas. \n\nFinally, I saw a better way to approach my data transformation would be to use the apply function, which was much more efficient and helped in speed. For using an apply function, I saw it would be cleaner to make a helper function to use to work with the data as I pleased, and this was my reason for writing clean_d above. ","ad7ba7c7":"Now that I have completed some data transformations to make my dataset easier to work with, I begin to wonder if all the rows have full entries or if there is data missing.","46ab001f":"For the final part of this data analysis, I want to apply some machine learning technique to take the features I identified as important to predict the most important one - 'Overall'. However, I am just starting in Machine Learning and the only ML experience I have is from the 'Introduction to Machine Learning' course on Kaggle, in which I was taught Decision Trees and Random Forests. So now I attempt to apply both to predict the overall rating for a player in the FIFA game. My performance metric will be Mean Absolute Error.","12066508":"When looking through this series of counts for missing values, the thing that stands out the most is the line of \"48\" counts. It is highly likely that these are all for the same players, and since 48 is such a small number in the grand scheme of the 18,207 players in this dataset, I think it is best to simply drop those rows entirely.","8109d17d":"When looking at these features, I see that some of them represent numerical values, but don't always represent them in a float or int format. For example, the Wage and Value features are written with the Euro, Thousands, and Millions symbols so instead of being written in a numerical form, they are strings. For making them easier to work with, I devise a function that will clean the data and transform it to a numerical form.","df731a3c":"Upon first glance, it can be seen that not all columns are shown, so I desire to see all the features the dataset makes available to us.","c965fedc":"In this kernel, I will complete data analysis upon the FIFA 19 player dataset. I am an avid soccer fan, so for my first-ever mini-data project, I was immediately pulled to this dataset. Please have a look through my work and thought processes and comment on improvements I can make as I start my personal data science journey.","627d84fb":"The main problem I ran into when creating and observing the patterns present in these line graphs were outliers. These outliers completely changed some parts of the graph to be misrepresentative of the overall trend. I know that it is best practice to drop outliers from data, but I hesitate to do so for this dataset because that would mean getting rid of some of the best, best players like Lionel Messi and Cristiano Ronaldo. In practical data science, outliers mean possible incorrect data collection or impossible entries, but players like LM and CR are just so legendary that they become outliers despite being truly that great. I don't have the heart to remove them from the data because the reality is they are the outliers of the sport, two great sportsmen of their time. In this case, what do I do for my data analysis?","57319f78":"Now our counts are much cleaner, and it is nice to see that the features I identified above as interesting to me all have 0 missing values. Another feature I was contemplating working with was 'Release Clause' but after observing that more than a thousand of its values are missing, it doesn't seem best to work with it. I could drop it, but I know that players even at the top of the game sometimes don't have a release clause, so I shouldn't go down that route. Imputation of the mean is not logical either, so I choose to not focus on this feature for my analysis.","deb8523e":"The general logic behind the function: First I look at what the last character is. Some strings represent 0 value, so there is no 'K' or 'M' that follows it. In this case, we return 0. If it doesn't end in '0', then we cut off the Euro value sign in the beginning and the last character from the end of the string to get the numerical value alone (which could have a decimal point in it), and then we transform it from a string data type to a float data type. Finally, we examine the last character and multiply by the appropriate amount, returning the number after.","4c48d45a":"Looking at the outputs, I am very alarmed to see that both of the machine learning techniques employed had almost perfect accuracy in predicting the Overall feature. I am fairly certain that my Mean Absolute Error should be more than 0. This is what I really need help with, so please comment below and let me know what is wrong and what I should fix.","cb5b227f":"It is immediately obvious that there are a multitude of characteristics each player is rated on, and I want my data analysis to be focused upon the ones that are seem important to a soccer fan like myself. Some of the key features I want to focus on are Age, Nationality, Overall, Potential, Value, and Wage."}}