{"cell_type":{"9cec4902":"code","c59322e0":"code","6292c4ec":"code","64e85f95":"code","574f4a3d":"code","eca39fd1":"code","d5f8f1f0":"code","1a31448e":"code","4c3417f9":"code","fcad6a35":"code","621dfffd":"code","7d6f629d":"code","12466998":"code","eb91b8f4":"code","5a5ee6f4":"markdown","56feebb4":"markdown","42ceb957":"markdown","4c33b238":"markdown","091c51b5":"markdown","530e0657":"markdown","3fe13cff":"markdown","a2854efb":"markdown","36a88381":"markdown","f20f602b":"markdown","2210dd50":"markdown","1efbb447":"markdown","431a89d8":"markdown","7a7b4a16":"markdown","cb20e6b1":"markdown","b531f39f":"markdown","c1958e43":"markdown","1baa89dd":"markdown","98f684cc":"markdown","5e087685":"markdown"},"source":{"9cec4902":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#Libraries for Data manipulation\nimport numpy as np\nimport pandas as pd\n\n#Libraries for Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Libraries for Natural Language Processing NLTK stands for Natural Language Toolkit\nimport nltk,re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\n\n#Libraries for Modelling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\n#Libraries for Classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,plot_confusion_matrix\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nnltk.download('punkt')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c59322e0":"data_sms=pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding = 'latin-1')\ndata_sms.head()","6292c4ec":"data_sms = data_sms.filter(['v1', 'v2'], axis = 1)\ndata_sms.columns = ['class', 'message']\ndata_sms.head()","64e85f95":"data_sms.drop_duplicates(inplace = True, ignore_index = True)","574f4a3d":"print('Number of null values:\\n')\ndata_sms.isnull().sum()","eca39fd1":"#Barplot to to show total number of ham and spam messages in the dataset\nsns.countplot(data=data_sms,x='class')","d5f8f1f0":"#Common words used in Spam Messages\ndata_words_spam_nl = ' '.join(list(data_sms[data_sms['class'] == 'spam']['message']))\ndata_wc_nl = WordCloud(width = 600, height = 512).generate(data_words_spam_nl)\nplt.figure(figsize = (13, 9))\nplt.imshow(data_wc_nl)\nplt.show()","1a31448e":"#Common words used in Ham Messages\ndata_words_ham_nl = ' '.join(list(data_sms[data_sms['class'] == 'ham']['message']))\ndata_wc_nl = WordCloud(width = 600, height = 512).generate(data_words_ham_nl)\nplt.figure(figsize = (13, 9))\nplt.imshow(data_wc_nl)\nplt.show()","4c3417f9":"full_data_sms = []\nlemmatizer = WordNetLemmatizer()\nfor i in range(data_sms.shape[0]):\n    mess_1 = data_sms.iloc[i, 1]\n    mess_1 = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', mess_1)\n    mess_1 = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', mess_1) \n    mess_1 = re.sub('\u00a3|\\$', 'moneysymb', mess_1) \n    mess_1 = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', mess_1) \n    mess_1 = re.sub('\\d+(\\.\\d+)?', 'numbr', mess_1) \n    mess_1 = re.sub('[^\\w\\d\\s]', ' ', mess_1) \n    mess_1 = re.sub('[^A-Za-z]', ' ', mess_1).lower() \n    token_messages = word_tokenize(mess_1)\n    mess = []\n    for word in token_messages:\n        if word not in set(stopwords.words('english')):\n            mess.append(lemmatizer.lemmatize(word))\n    txt_mess = \" \".join(mess)\n    full_data_sms.append(txt_mess)","fcad6a35":"add_df = CountVectorizer(max_features = 1500)\nX = add_df.fit_transform(full_data_sms).toarray()\ny = data_sms.iloc[:, 0]","621dfffd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\nprint('Number of rows in training set: ' + str(X_train.shape))\nprint('Number of rows in test set: ' + str(X_test.shape))","7d6f629d":"class_LR = LogisticRegression(random_state = 42, solver = 'liblinear').fit(X_train, y_train)\ny_pred_LR = class_LR.predict(X_test)\nprint('The first two predicted labels:', y_pred_LR[0], y_pred_LR[1], '\\n')\nclass_rep_LR = classification_report(y_test, y_pred_LR)\nprint('\\t\\t\\tClassification report:\\n\\n', class_rep_LR, '\\n')\nplot_confusion_matrix(class_LR, X_test, y_test) \nplt.show()","12466998":"class_SVC = SVC(probability = True, random_state = 42).fit(X_train, y_train)\ny_pred_SVC = class_SVC.predict(X_test)\nprint('The first two predicted labels:', y_pred_SVC[0], y_pred_SVC[1], '\\n')\nclass_rep_SVC = classification_report(y_test, y_pred_SVC)\nprint('\\t\\t\\tClassification report:\\n\\n', class_rep_SVC, '\\n')\nplot_confusion_matrix(class_SVC, X_test, y_test) \nplt.show()","eb91b8f4":"class_KNC = KNeighborsClassifier(n_neighbors = 3).fit(X_train, y_train)\ny_pred_KNC = class_KNC.predict(X_test)\nprint('The first two predicted labels:', y_pred_KNC[0], y_pred_KNC[1], '\\n')\nclass_rep_KNC = classification_report(y_test, y_pred_KNC)\nprint('\\t\\t\\tClassification report:\\n\\n', class_rep_KNC, '\\n')\nplot_confusion_matrix(class_KNC, X_test, y_test) \nplt.show()","5a5ee6f4":"Data needs to be converted into a numerical format where each word is represented by a matrix known as **Vectorization**.","56feebb4":"**Checking For Null Values**","42ceb957":"Then, the data will be split into X train, X test,y train and y test using train_test_split() method.","4c33b238":"Here the barplot will gives us an isights about the totla number of Ham and Spam messages in the dataset. ","091c51b5":"> # Explanatory Data Analysis","530e0657":"> # Table of Content\n\n* Introduction\n* Importing Libraries\n* Importing Data\n* Data Cleaning\n    * Dropping dublicate values\n    * Checking for Null Values\n* Explanatory Data Analysis\n* Preparing data for Modelling\n* Classification Models\n    * Logistic Regression\n    * Support Vector Classification\n    * K-Neigbour Classification\n*  Conclusion","3fe13cff":"**Support Vector Classification**","a2854efb":"Here, the visualization in wordcloud gives us an insight on what words are frequently used in Spam and Ham Messages.","36a88381":"> # Importing Libraries","f20f602b":"**Logistic Regression**","2210dd50":"> # Classification Models","1efbb447":"\nThen The following things are replaced:\n\n1.  email addresses with 'emailaddr';\n2.  URLs with 'httpaddr';\n3.  money symbols with 'moneysymb';\n4.  phone numbers with 'phonenumbr';\n5.  numbers with 'numbr';\n6.  remove all punctuations;\n7.  word to lower case.\n\nThen **Lemmatization**  is conducted. Lemmatization reduces a word form to its initial dictionary form (lemma). As a result of  word forms lemmatization, flexive endings are discarded and the main or dictionary form of the word is returned.\n\n**Tokenization** is process of breaking complex data into smaller units as tokens. It can be done by splitting paragraphs into sentences and sentences into words. \n\nThen with the help of NLTK library, stopwords will be removed. Stopwords are those words which occurs frequently and holds meaning in sentence structure but are irelevant in NLP  ","431a89d8":"> # Introduction \n\nHere, Using the data consisting of both Spam and Ham Message, A model will be built where the model will be able to identify whether the meaasge is spam or not. \n\nTo build this Classifer, three Classification models will be used\n1. Logistic Regression\n2. Support Vector Classification\n3. K-Neighbour Classification","7a7b4a16":"> # Conclusion\n\nSpam Classifer with 3 different classification is built. We have used Logistic regression, SVM and K-Neighbour Classification models to build and test our model.\n\nHope you like this notebook and upvote it.\n","cb20e6b1":"**Dropping dublicate values**","b531f39f":"> # Data Cleaning\n\nAs the class is named as v1 and the meassge is named as v2, the name of the data column will be changed and other column will not be included as they are irrelevant","c1958e43":"**K-Neighbour Classification**","1baa89dd":"> # Preparing data for Modelling","98f684cc":"# **SMS Spam Classifier**","5e087685":"> # Importing Data\n\nThe files contain one message per line. Each line consists of two columns: v1 contains the label (**ham** or **spam**) and v2 contains the **raw text**"}}