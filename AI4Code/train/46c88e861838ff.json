{"cell_type":{"1ee87c1e":"code","4cf476dd":"code","93b42ee0":"code","f844c3a4":"code","5070a94a":"code","17571c05":"code","efb2ded4":"code","253be839":"code","98c80aa0":"code","c9460057":"code","52a0b0ea":"code","7f921c46":"code","cb396125":"code","057d47ed":"code","7eec43d1":"code","aae496d3":"code","d1d304f1":"code","6d853d78":"code","6aada122":"code","11161e22":"code","2196d7d9":"markdown","3454493f":"markdown","698b7478":"markdown","23166c96":"markdown","3f9a90f0":"markdown","e54dd4f4":"markdown","0017787f":"markdown","ebb21b56":"markdown","73da4f82":"markdown","f1e618b9":"markdown","dfc78ff8":"markdown","1e98c74b":"markdown","0fcb589f":"markdown","cdc13b1b":"markdown","0ecc7948":"markdown","30c24d1f":"markdown","7e1d8028":"markdown","6a325780":"markdown","36a42561":"markdown"},"source":{"1ee87c1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4cf476dd":"# Read the data\nX_full = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n\nprint('Data Import Complete')","93b42ee0":"print(\"Full Training Set\")\nprint(\"Shape : \")\nprint(X_full.shape)\nprint(\"Columns\")\nprint(X_full.columns)\nprint(\"Summary\")\nprint(X_full.describe())","f844c3a4":"# type of data in each column\nprint(X_full.info())\nprint(X_full.head())","5070a94a":"# Read the data\nX = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n# extract target\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n\n# Option 2: Ordinal encoding\u00b6\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\nlabel_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\nlabel_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])\n\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","17571c05":"# Shape of data (num_rows, num_columns)\nprint(X_full.shape)\n\n# Number of missing values in each column of data\nmissing_val_count_by_column = (X_full.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","efb2ded4":"# plot some key features against price\nkeyfeatures=['LotFrontage','LotArea','TotalBsmtSF','GrLivArea','1stFlrSF','2ndFlrSF','GarageArea','MasVnrArea','YearBuilt']\nfor feature in keyfeatures:\n    sns.scatterplot(data=X_full, x=feature, y='SalePrice', hue='YearBuilt')\n    plt.show()","253be839":"# Colour code some features in living area against price graph\ncolourfeatures=['MSSubClass','MSZoning','HouseStyle','RoofStyle','BldgType','Heating','CentralAir','YearBuilt']\nfor feature in colourfeatures:\n    fig, ax =plt.subplots(1,2)\n    fig.set_size_inches(12, 6)\n    sns.scatterplot(data=X_full, x='YearBuilt', y='SalePrice', hue=feature,ax=ax[0])\n    sns.scatterplot(data=X_full, x='GrLivArea', y='SalePrice', hue=feature,ax=ax[1])\n    fig.show()","98c80aa0":"# plot key distributions\nfeatures = ['SalePrice','LotArea','LotFrontage','YearBuilt','1stFlrSF','2ndFlrSF','TotRmsAbvGrd','BedroomAbvGr','FullBath','OverallQual','Neighborhood']\nfor feature in features:\n    sns.histplot(data = X_full, x = feature)\n    plt.show()","c9460057":"# plot values against each other\ng = sns.pairplot(X_full[['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']], palette = 'seismic',height=4,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=50) )\ng.set(xticklabels=[])","52a0b0ea":"# Obtain target and predictors\ny = X_full.SalePrice\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = X_full[features].copy()\nX_test = X_test_full[features].copy()\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint('train and valid sets created')","7f921c46":"# Function for comparing different models\ndef score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n    model.fit(X_t, y_t)\n    preds = model.predict(X_v)\n    return mean_absolute_error(y_v, preds)\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\nprint('score_model and score_dataset functions created')","cb396125":"# Define the models\nmodel_1 = RandomForestRegressor(n_estimators=50, random_state=0)\nmodel_2 = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel_3 = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\nmodel_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\nmodel_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n\nmodels = [model_1, model_2, model_3, model_4, model_5]\n\nfor i in range(0, len(models)):\n    mae = score_model(models[i])\n    print(\"Model %d MAE: %d\" % (i+1, mae))\n    \n# Define a model\nmy_model = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n\n# Fit the model to the training data\nmy_model.fit(X, y)\n\n# Generate test predictions\npreds_test = my_model.predict(X_test)\n\n#view predictions\nprint((preds_test)[:5])","057d47ed":"# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('first_submission.csv', index=False)\n\nprint('first submission created')","7eec43d1":"# Read the data\nX_full = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll use only numerical predictors\nX = X_full.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint('Data imported')","aae496d3":"# Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n\n# Number of missing values in each column of training data\nprint(\"\\nMissing data: \")\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n\n# get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\n#print(reduced_X_train.shape)\n#print(reduced_X_valid.shape)\n\nprint(\"\\nMAE (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n\n# imputate missing values\n\n# imputation\nmy_imputer = SimpleImputer(strategy='most_frequent')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nprint(\"\\nMAE (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n\n# Preprocessed training and validation features\nfinal_imputer = SimpleImputer(strategy='median')\nfinal_X_train = pd.DataFrame(final_imputer.fit_transform(X_train))\nfinal_X_valid = pd.DataFrame(final_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nfinal_X_train.columns = X_train.columns\nfinal_X_valid.columns = X_valid.columns\n\n# Define and fit model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(final_X_train, y_train)\n\n# Get validation predictions and MAE\npreds_valid = model.predict(final_X_valid)\nprint(\"\\nMAE (Impute):\")\nprint(mean_absolute_error(y_valid, preds_valid))\n\n# preprocess test data\nfinal_X_test = pd.DataFrame(final_imputer.fit_transform(X_test))\n# get test predictions\npreds_test = model.predict(final_X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('2nd_submission.csv', index=False)\n\nprint('2nd submission created')","d1d304f1":"# Look at Categorical Data and test strategies for dealing with it\n\n# Read the data\nX = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n# extract target\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)\n\n# Option 1: Drop columns with categorical data\n    \ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n\nprint(\"Unique values in 'Condition2' column in training data:\", X_full['Condition2'].unique())\nprint(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())\n\n# Option 2: Ordinal encoding\u00b6\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\nlabel_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\nlabel_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])\n\nprint(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nprint(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","6d853d78":"# re build the model using all of the training data, proprocess the test data and make predictions\n\n# Read the data\nX = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n### Deal with missing Data ###\n\ndrop_X_train = X.select_dtypes(exclude=['object'])\n\n# get names of columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()]\n\n# drop columns in training and test data\nX = X.drop(cols_with_missing, axis=1)\nX_test = X_test.drop(cols_with_missing, axis=1)\n\n# imputate missing values\nmy_imputer = SimpleImputer(strategy='most_frequent')\nimputed_X = pd.DataFrame(my_imputer.fit_transform(X))\nimputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n\n# Imputation removed column names; put them back\nimputed_X.columns = X.columns\nimputed_X_test.columns = X_test.columns\n\n###\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nobject_cols = [col for col in X.columns if X[col].dtype == \"object\"]\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X[col].nunique() < 10]\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\n### Deal with Categories ###\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nX_test=X_test.fillna(method='ffill')\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X[low_cardinality_cols]))\nOH_cols_test= pd.DataFrame(OH_encoder.transform(X_test[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X.index\nOH_cols_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test= pd.concat([num_X_test, OH_cols_test], axis=1)\n\n### Build the model ###\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(OH_X_train, y)\npreds = model.predict(OH_X_test)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,'SalePrice': preds})\noutput.to_csv('3rdsubmission-final.csv', index=False)\nprint('prediction made')","6aada122":"# use pipelines to automate the process\n\n# Read the data\nX_full = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y,train_size=0.8, test_size=0.2,random_state=0)\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test[my_cols].copy()\n\nprint(X_train.head())\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))\n\n# Preprocessing of test data, fit model\npreds_test = clf.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('4th_submission.csv', index=False)\nprint('prediction made')","11161e22":"# use pipelines to automate the process\n\n# Read the data\nX_full = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/housing-prices-competition-for-kaggle-learn-users\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y,train_size=0.8, test_size=0.2,random_state=0)\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_full.columns if X_full[cname].nunique() < 10 and X_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_full = X_full[my_cols].copy()\nX_test = X_test[my_cols].copy()\n\nprint(X_train.head())\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\n# Preprocessing of test data, fit model\npreds_test = clf.predict(X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('4th_submission.csv', index=False)\nprint('prediction made')","2196d7d9":"## Handling categorical values\n\nThere are 3 approaches to handling categorical values; The first option is to simply remove or ignore all categorical values. The second approach is to fit an ordinal encoder to the training data, and use it to transform the data.The third option is cardinal encoding or one-hot encoding. This approach creates a new column for each unique entry in the categorical data, its only really suitable where the cardinality (number of unique values is low).","3454493f":"## Building a better model\n\nNow that we have established a baseline model we can start building better models. Lets start by dealing with some of the missing data.","698b7478":"## Check for missing values","23166c96":"## Exploring the data\n\nLets take a look at the data than we imported to get an idea of how much data we have, what sort of imformation this includes, how much variation there is in the data and how much data is missing.","3f9a90f0":"## Create first submission","e54dd4f4":"This very simple model achieved a score of 20998. which gives us a baseline against which we can evaluate the effectiveness of any feature engineering or future modelling we might perform.","0017787f":"## Import Data","ebb21b56":"This submissions achieves a score of 16625, an improvement of 4373 marks. The goal being to get the lowest score possible.","73da4f82":"## Build a simple model\n\nTo get started we can take a few of the features that don't have much missing data but that we think might have a high impact on price and build a simple model. This will give us baseline to work from. That way as we start adding more features, feature engineering and dealing with missing data we can see whether this makes our model better or worse. ","f1e618b9":"## Including categorical data in Model","dfc78ff8":"## About this project\n\nThis notebook is based on the Kaggle Intermediate Machine Learning Course. Much of the content is based on the free course which can be found at [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course).\n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)\n\nThe [Ames Housing Dataset](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) was compiled by Dean De Cock for use in data science education as an alternative to the Boston Housing dataset.\n\nThe Dataset consists of 1460 records with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. The competition challenges you to predict the final price of each home.\n\nThis notebook follows Intermediate Machine Learning Course submissions with additional data exploration, visualization and models created by myself (David Coxon).\n\nNote: Some of the course material on kaggle course relates to the [Melbourne Housing Dataset](https:\/\/www.kaggle.com\/dansbecker\/melbourne-housing-snapshot\/home)","1e98c74b":"After dealing with missing data and adding in some categorical data the score improved to 16162 an improvement of 497.","0fcb589f":"## Deal with missing values","cdc13b1b":"## Introducing pipelines\n\nAs the preprocessing stages required to deal with categorical data and both missing categorigal and numeric data for training and test data get more complex and as we go onto build more complex models, making pipelines to automate and simplify this process becomes more important.     \n","0ecc7948":"## Check data types","30c24d1f":"## Visualizing the Data\n\nNow that we know a little about the data statistically lets look at it visually to get a better idea about any patters there may be in it and what it might tell us about housing prices. ","7e1d8028":"## Look at categorical data\n\nIdendify features with categorical data and find number of unique values for each.","6a325780":"## Create a smaller feature set","36a42561":"## Create a XGboost Model"}}