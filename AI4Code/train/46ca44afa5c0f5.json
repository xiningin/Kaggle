{"cell_type":{"3ebc13f7":"code","d32ee462":"code","f56093c8":"code","35cbbae2":"code","cb880d90":"code","1d5e3e68":"code","ecb8cb2e":"code","7991f5e2":"code","cc570de2":"code","d3d088ce":"code","bbda3bb7":"code","4b88935d":"code","d99e35bc":"code","d029cea3":"code","24db0ce4":"code","4c00274d":"code","08db7634":"code","ef8431a7":"code","236c5565":"code","99894f9d":"code","b2c60355":"code","db983b13":"code","d1d08254":"code","fa7d8bed":"code","59438f37":"code","5103faac":"code","a7b9b44f":"code","4865c645":"markdown","e0f7dbf2":"markdown","f0116f81":"markdown","449d8c4c":"markdown","978aa894":"markdown","2b2979a2":"markdown","e30e5bbd":"markdown"},"source":{"3ebc13f7":"#Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","d32ee462":"#Reading csv file\ndf=pd.read_csv(\"..\/input\/creditcard.csv\")","f56093c8":"#Checking the variables\ndf.head()","35cbbae2":"#Check null values\ndf.isnull().sum()","cb880d90":"#Checking the available columns\ndf.columns","1d5e3e68":"#Dimensions of it\ndf.shape","ecb8cb2e":"#Counting all unique values of column \"sentiment\" from dataset \ndf[\"Class\"].value_counts()","7991f5e2":"# Check Class variables that has 0 value for Genuine transactions and 1 for Fraud\nprint(\"Class as pie chart:\")\nfig, ax = plt.subplots(1, 1)\nax.pie(df.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['Black','Blue'])\nplt.axis('equal')\nplt.ylabel('')","cc570de2":"import seaborn as sns\nsns.countplot('Class', data=df)\nprint('Frauds: ', round(df['Class'].value_counts()[1] \/ len(df) * 100, 2), '%')","d3d088ce":"#I dnt want time column for this prediction.So,i am dropping it\ndf = df.drop('Time', 1)","bbda3bb7":"#Checking dataset after droping \"time column\".\ndf.head()","4b88935d":"df.describe()","d99e35bc":"df.shape\n","d029cea3":"df.info()","24db0ce4":"#separating dependent and independent variable.\nX = df.iloc[:, :29].values\ny = df.iloc[:,-1].values\nprint(X)\nprint(y)","4c00274d":"X.shape","08db7634":"y.shape","ef8431a7":"\n#Splitiing dataset into training_set and testing_set.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2 ,random_state = 0)\n","236c5565":"#Scaling the features\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","99894f9d":"# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)","b2c60355":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","db983b13":"Total =56852+9+37+64","d1d08254":"#accuracy=(TP+TN)\/total\nAccuracy = (56852+64)\/Total\n\nprint(Accuracy)","fa7d8bed":"#Error_rate=1-accuracy\nError_rate = 1-Accuracy\nprint(Error_rate)","59438f37":"#Recall=TP\/FN+TP\nRecall = 64\/(64+37)\nprint(Recall)","5103faac":"#Precision=TP\/FP+TP\n\nPrecision = 64\/(9+64)\nprint(Precision)","a7b9b44f":"#Visualize the confusion matrix.....\nplt.figure(figsize=(20,10))\nplt.subplot(2,4,3)\nplt.title(\"LogisticRegression_cm\")\nsns.heatmap(cm,annot=True,cmap=\"Wistia\",fmt=\"d\",cbar=False)","4865c645":"# ||Credit Card Fraud Detection||\n\nProblem Statement:-\nThe Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be fraud. This model is then used to identify whether a new transaction is fraudulent or not. Our aim here is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications.","e0f7dbf2":"# ||Conclusion||\n\n-->**Logistic regression ** has given **accuracy of 99%**.\n\n-->I have used Logistic regression ,You can use any of classification algorithm.\n\n-->We can also improve on this accuracy by increasing the sample size or use deep learning algorithms however at the cost of computational expense.We can also use complex anomaly detection models to get better accuracy in determining more fraudulent cases.","f0116f81":"# || Observations ||\n \nThe data set is highly skewed, consisting of **492** frauds in a total of **284,807** observations. This resulted in only 0.172% fraud cases. This skewed set is justified by the low number of fraudulent transactions.\n\nThe dataset consists of numerical values from the 28 \u2018Principal Component Analysis (PCA)\u2019 transformed features, namely **V1 to V28**. Furthermore, there is no metadata about the original features provided, so pre-analysis or feature study could not be done.\n\nThe \u2018**Time**\u2019 and \u2018**Amount**\u2019 features are not transformed data.\n\nThere is no missing value in the dataset.","449d8c4c":"# ||Execution part||","978aa894":"# About\nThe dataset that is used for credit card fraud detection is derived from the following Kaggle URL :\n\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud","2b2979a2":"# PLEASE VOTE UP ,IF YOU LIKED IT!!","e30e5bbd":"![](https:\/\/media0.giphy.com\/media\/jbahvLZdAB17i\/giphy.gif)"}}