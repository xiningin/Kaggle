{"cell_type":{"49f43812":"code","724d37b5":"code","2a13a6be":"code","2e701ba7":"code","12cb44d5":"code","2d392087":"code","83389bff":"code","90a6a996":"code","a89de7e3":"code","e2a6a6fc":"code","0de75ae4":"code","006055f7":"code","e4015b0c":"code","9f088036":"code","e9d29cb0":"code","95065c89":"code","cf4108a3":"markdown","800a16b0":"markdown","80764c38":"markdown","c814c478":"markdown","5ae814e2":"markdown","5e5a8345":"markdown","67e58bb9":"markdown","59a7597a":"markdown"},"source":{"49f43812":"# Default Kaggle setup cell:\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n# Some more set-ups:\n# Import plotting modules\nimport seaborn as sns\nsns.set()\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\n%matplotlib inline\n\n# Tune the visual settings for figures in Seaborn lib\nsns.set_context(\n    \"notebook\", \n    font_scale=1.5,       \n    rc={ \n        \"figure.figsize\": (11, 8), \n        \"axes.titlesize\": 18 \n    }\n)\n\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 11, 8\n\n# splitting and preprocessing tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# models\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# evaluation\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score\n\npd.set_option('display.max_columns', None) # so we can see all the columns while working with DataFrames\n\n# Disable warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","724d37b5":"# Read the data\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\nprint('Train-valid data shape:', data.shape)\nprint('Test data shape:', X_test_full.shape)\ndata.head()","2a13a6be":"data.info(verbose=False)","2e701ba7":"# separate target from predictors:\ny = data.SalePrice\nX = data.drop(['SalePrice'], axis=1)\n\n# Select categorical columns with relatively low cardinality:\n# * Cardinality means the number of unique values in a column\ncat_cols = [col for col in X.columns if X[col].dtype == 'object']\n\n# OR we could do the same:\n# category_cols = X_train_full.select_dtypes(include='object').columns.tolist() \n\nlow_cardinality_cols = [col for col in X[cat_cols] \n                        if X[col].nunique() < 10]\n\nhigh_cardinality_cols = set(cat_cols) - set(low_cardinality_cols) \n# *** Can I encode them effectively? Drop it for now\n\n# Select numeric columns\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_cols = [col for col in X.columns if X[col].dtype in numeric_dtypes]\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, \n                                                                train_size=0.8, \n                                                                test_size=0.2,\n                                                                random_state=0)\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + num_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nprint(\"Dataframes' shapes before aligning:\", X_train.shape, X_valid.shape, X_test.shape)\n# * is it worth to do something about missing values before one-hot encoding?\n\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\nprint(\"Dataframes' shapes after aligning:\", X_train.shape, X_valid.shape, X_test.shape)\n\nX_train.head()\n","12cb44d5":"X_train.info(verbose=False)","2d392087":"missing_by_col = X_train.isnull().sum()\nmissing_by_col = missing_by_col[missing_by_col > 0].sort_values(ascending=False)\nmissing_by_col.plot.bar();","83389bff":"# imputate numerical missing values\nnum_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(num_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(num_imputer.transform(X_valid))\n\n# imputation removed column names, put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nmissing_by_col_imputed = imputed_X_train.isnull().any().sum()\nprint('Columns with missing after imputation:', missing_by_col_imputed)","90a6a996":"# take a look at target distribution\nsns.distplot(y);","a89de7e3":"# let's use base-10 logs, because it's possible to look at them \n# and see the magnitude of the original number: log(1)=0, log(10)=1, log(100)=2, etc.\nnormalized_target = np.log10(y)\n\nsns.distplot(normalized_target); # visualize the normalized data\n\n# Estimate the skewness directly using .skew():\nprint('Skewness before log transformation: {:.3f}'.format(data.SalePrice.skew()))\nprint('Skewness after log transformation: {:.3f}'.format(normalized_target.skew()))","e2a6a6fc":"skewed_features = []\nfor col in num_cols:\n    if -0.5 < X_train[col].skew() > 0.5:\n        skewed_features.append(col)\n    \nskewed_features","0de75ae4":"X_train_log = X_train.copy()\nX_valid_log = X_valid.copy()\nX_test_log = X_test.copy()\n\ny_train_log = np.log10(y_train)\ny_valid_log = np.log10(y_valid)\n\ndef reduce_skewness(data):\n    for col in skewed_features:\n        data[col] = np.log10(data[col])\n        \nreduce_skewness(X_train_log)\nreduce_skewness(X_valid_log)\nreduce_skewness(X_test_log)\n\nX_train_log.head()","006055f7":"temp_data = pd.concat([X_train, y_train], axis=1)\ncorr = temp_data.corr()\n# most correlatet features with SalePrice:\ncols = corr.nlargest(10, 'SalePrice')['SalePrice'].index\n# just properly shaped corr matrix (10x10):\ncorr10 = corr.nlargest(10, 'SalePrice')[cols] \n\nsns.heatmap(corr10, annot=True, fmt='.2f', linewidths=.5, center=0);","e4015b0c":"# Need to work with collinearity.","9f088036":"# peek the params from here:\n# https:\/\/www.kaggle.com\/davidrivasphd\/regression-with-stacking-light-gbm-xgboost\/comments\n# Generally, you need to use Grid Search for tunning the params\nxgboost = XGBRegressor(learning_rate=0.015,\n                       n_estimators=3460,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror', nthread=-1,\n                       scale_pos_weight=1, seed=27,\n                       reg_alpha=0.00006,\n                       random_state=0)\n\nxgboost.fit(X_train_log, y_train_log, \n              early_stopping_rounds=200,\n              eval_set=[(X_valid_log, y_valid_log)],\n              verbose=False)\n\n# inverse log transformation using 10 ** y\nxgboost_preds = 10 ** xgboost.predict(X_valid_log)","e9d29cb0":"xgboost_mae = mean_absolute_error(y_valid, xgboost_preds)\n# xgboost_rmse = np.sqrt(mean_squared_error(y_valid, xgboost_preds))\nxgboost_rmsle = np.sqrt(mean_squared_log_error(y_valid, xgboost_preds))\n\nprint(\"XGBoost MAE:\", xgboost_mae,'\\nXGBoost RMLSE:', xgboost_rmsle)","95065c89":"preds_test = 10 ** xgboost.predict(X_test_log)\noutput = pd.DataFrame({'Id': X_test_log.index,\n                      'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\n\noutput.head()","cf4108a3":"## Investigating correlation","800a16b0":"# Build model(-s)","80764c38":"We can see the target data (SalePrice) is skewed (right long tail means positive skewness). Regression models does not handle skewed data very well, they're underperforming in predicting such long tails (tree-based algorithms are OK with skewness though). So we'd better to remove the skewness. We can do that using log transformation:","c814c478":"## Dealing with the rest missing values","5ae814e2":"## Inspecting skewness","5e5a8345":"## Imputing","67e58bb9":"Now we need to do the same transformation with skewed features:","59a7597a":"# Setup"}}