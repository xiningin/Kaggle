{"cell_type":{"09d6bbb5":"code","b02e4f1d":"code","50d5c38e":"code","d952e8e5":"code","a7adefa3":"code","5bbb9f78":"code","a251368b":"code","6828c6de":"code","60f61b83":"code","5b59d39a":"code","abe3fd9d":"code","9915b8df":"code","e99cace6":"code","519dbe5c":"code","4a9a7bab":"code","d182142d":"code","e2bc215a":"code","558c27a5":"code","9d247eb5":"code","89e7cbe1":"code","e953f41f":"code","43dd26c8":"code","ac7eb5a0":"code","8857eb79":"code","73375d68":"code","7c2317f3":"code","8193476a":"code","538f09e8":"code","40d9e472":"code","3021fdc9":"code","7462aa4d":"code","fb40b834":"code","95206ba7":"code","83a4226d":"code","67622aaa":"code","1eff427d":"code","ecc10bcc":"code","9b26d981":"code","2e79c596":"code","45881abe":"code","656a92ab":"code","3a93ad05":"code","84a99151":"code","4fe9b2c3":"code","acb9d8fa":"code","0daa3240":"code","fad71887":"code","72f7036a":"code","34edfcff":"code","d2aa426b":"code","df20bf7a":"code","a20503de":"code","5f0698bb":"code","9b7f1b3c":"code","852c1876":"code","9a2b7bf2":"code","9765d652":"code","4daa93db":"code","dd44cb31":"code","2eade96f":"code","341a24b2":"code","79c01d75":"code","d0a6621a":"code","97c0e9e0":"code","908165e6":"code","908b1807":"code","185eb279":"code","d01f666e":"code","a0961e75":"code","1666ab89":"code","e28fb48b":"code","879c2c36":"code","8872a4d6":"code","d19411f5":"code","09651dff":"code","154b0895":"code","29ca86a3":"code","4c7b8ee1":"code","06930f60":"code","fcf56d49":"code","d7dd4028":"code","1e86af3f":"code","d9eb35df":"code","568313c0":"code","ec8d1bad":"code","220ea6cf":"code","ab1081e7":"code","7cae379c":"code","f207cb01":"code","e0bf9dd7":"code","b4e60821":"code","b82f156e":"code","e8ba4393":"code","1e072c01":"code","3e521ba7":"code","9cbf2812":"code","8b717f92":"code","4c691e2b":"code","8a43572a":"code","551b88c2":"code","1926e76b":"code","10666858":"code","eb0350e2":"code","f8333c5d":"code","76ce1c1f":"code","177e8adc":"code","cdd5025b":"code","6934e0b0":"code","23f20317":"code","8ba95e52":"code","e73727f8":"code","74500b08":"code","07bfde74":"code","2db0bf3a":"code","59677cdb":"code","4bff83b9":"code","92464519":"code","eb27df86":"code","5be89b24":"code","7b40be74":"code","a718509b":"code","3a6df4df":"code","49673439":"code","5baf5315":"code","ae2dc9ae":"code","9b919d5f":"code","5953855b":"code","1f35beb1":"code","fdeb84c9":"code","d3a92e2a":"code","2eb27301":"code","d028c2d7":"code","eca5590e":"code","8964d5eb":"code","6334e545":"code","bcf848ac":"code","ede74f8e":"code","3554f8da":"code","39b05ae7":"code","e7c12b71":"code","dc7516bd":"code","4ffb8f79":"code","f0f02c4d":"code","85dabe15":"code","a917275d":"code","2a8bf862":"code","ce695c6d":"code","9e6e412b":"markdown","860f8b2e":"markdown","7c089434":"markdown","21be2753":"markdown","5bbf287f":"markdown","ded1d177":"markdown","77bdf8b7":"markdown","5e41cc28":"markdown","b9d19516":"markdown","7c808265":"markdown","1f91973a":"markdown","65d18db0":"markdown","d44568ac":"markdown","60e2434c":"markdown","a2b26f10":"markdown","5d392dbc":"markdown","c2f5f38b":"markdown"},"source":{"09d6bbb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b02e4f1d":"import matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\nfrom scipy import stats\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(action='ignore', category = FutureWarning)\n\nsns.set_style(\"darkgrid\")\n\npd.set_option('display.max_rows', 250)\npd.set_option('display.max_columns', 250)\n\n#Not sure yet.\nfrom skopt.space import Real, Integer","50d5c38e":"#Here I will read the csvs, and print the dimensions of the datasets. Additionally, I will take a look at the training set. We also pull the ID column.\n\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_id = train.Id\ntest_id = test.Id\n\nprint(\"Train set: \", train.shape)\nprint(\"Test set: \", test.shape)\n\ntrain.info()","d952e8e5":"train.head()","a7adefa3":"#Some descriptive stats for continuous variables.\ntrain.describe().round(3)","5bbb9f78":"#Descriptive stats about categorical variables.\ntrain.describe(include = [\"O\"])","a251368b":"train[\"SalePrice\"].describe()","6828c6de":"sns.displot(train[\"SalePrice\"], kind = \"hist\", kde = \"True\")","60f61b83":"sns.displot(np.log(train[\"SalePrice\"]), kind = \"hist\", kde = \"True\")","5b59d39a":"df = pd.concat([train,test])\ndf","abe3fd9d":"#train[\"SalePrice\"] = np.log(train[\"SalePrice\"])\n#train = train.rename(columns = {\"SalePrice\": \"LogSalePrice\"})\n\n#This function will split our dataframe into numerical, categorical, and ordinal feats.\ndef feat_type(df):\n    num_feats = df.loc[:, df.dtypes != object].columns.tolist()\n    cat_feats = df.loc[:, df.dtypes == object].columns.tolist()\n    ord_feats =[]\n    for feat in num_feats:\n        if df[feat].value_counts().size <20:\n            ord_feats.append(feat)\n        \n    num_feats = [x for x in num_feats if x not in ord_feats + [\"Id\", \"SalePrice\", \"LogSalePrice\"]]\n    \n    return num_feats, cat_feats, ord_feats","9915b8df":"#This function will display some information for missing values. I would like information on number of missing, missing ratio, missing in train and missing in test sets.\n#df[df.SalePrice.notnull()] is the train set, as the test set does not have the sale price variable.\n\ndef missing(df):\n    miss = pd.DataFrame({\n        \"num_missing_values\": df.isnull().sum(),\n        \"missing_value_ratio\": (df.isnull().sum() \/ df.shape[0]).round(4),\n        \"missing_in_train\": df[df.SalePrice.notnull()].isnull().sum(),\n        \"missing_in_test\": df[df.SalePrice.isnull()].isnull().sum()\n    })\n    return miss[miss.num_missing_values > 0].sort_values(\"num_missing_values\", ascending = False)\n\nmissing(df)\n","e99cace6":"df[df.GarageFinish.isnull() & df.GarageType.notnull()]","519dbe5c":"#Fill in some of those missing values with the common responses.\n\ndf.loc[df.GarageFinish.isnull() & df.GarageType.notnull(), \"GarageFinish\"] = \"Fin\"\ndf.loc[df.GarageQual.isnull() & df.GarageType.notnull(), \"GarageQual\"] = \"TA\"\ndf.loc[df.GarageCars.isnull() & df.GarageType.notnull(), \"GarageCars\"] = 1\ndf.loc[df.GarageCond.isnull() & df.GarageType.notnull(), \"GarageCond\"] = \"TA\"","4a9a7bab":"#How many garages are built in the same year as their house?\n\ndf[df.GarageYrBlt == df.YearBuilt].shape[0]","d182142d":"#This seems like a decent way to fill in these missing values.\n\ndf.loc[df.GarageYrBlt.isnull() & df.GarageType.notnull(), \"GarageYrBlt\"] = df.loc[(df.GarageYrBlt.isnull()) & (df.GarageType.notnull())].YearBuilt","e2bc215a":"#Check median garage area for detached garages \n\n\ndf[(df.GarageType == \"Detchd\") & (df.YearBuilt < 1930) & (df.YearRemodAdd < 2005) & (df.YearRemodAdd > 1985) & (df.GarageCars == 1)].GarageArea.median()","558c27a5":"#Fill the missing garage area for the one detatched garage from above.\n\ndf.loc[(df.GarageArea.isnull()) & (df.GarageType.notnull()), \"GarageArea\"] = 234","9d247eb5":"num_feats, cat_feats, ord_feats = feat_type(df)","89e7cbe1":"#Looking at some of these categories with a large number of missing values, we see that the people who recorded the data coded missing data to none.\n#This is giving us \"false\" missing data in a way. Lets code this in.\n\nnone_cols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageCond\", \"GarageFinish\", \"GarageQual\", \n             \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType1\", \"MasVnrType\", \"BsmtFinType2\"]\n\nfor col in none_cols:\n    df[col].fillna(\"None\", inplace = True)\n    \nmissing(df)","e953f41f":"#Change masonry veneer area corresponding with the \"none\" responses in masonry veneer type\ndf.loc[df.MasVnrArea.isnull() & (df.MasVnrType == 'None'), \"MasVnrArea\"] = 0","43dd26c8":"#We replace the missing values here that are NA with 0.\n\nfor col in [\"BsmtFullBath\", \"BsmtHalfBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"GarageYrBlt\"]:\n    df[col].fillna(0, inplace = True)\n\n#If there were any uneccesary missing values, this should fix it. But the one missing value corresponds to no basement pretty much.\ndf[\"TotalBsmtSF\"].fillna(df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"] + df[\"BsmtUnfSF\"], inplace = True)","ac7eb5a0":"missing(df)","8857eb79":"#Lets look at MSZoning\nprint(df.MSZoning.value_counts())\n\ndf[df.MSZoning.isnull()]","73375d68":"#We have four values missing for MSZoning, but we know the neighborhoods for each of these. \n#We can replace each one with the proper MSZoning value for IDOTRR or Mitchel neighborhood value.\n\ndf.groupby(['Neighborhood']).MSZoning.value_counts()","7c2317f3":"df.loc[(df.MSZoning.isnull()) & (df.Neighborhood == \"IDOTRR\"), \"MSZoning\"] = \"RM\"\ndf.loc[(df.MSZoning.isnull()) & (df.Neighborhood == \"Mitchel\"), \"MSZoning\"] = \"RL\"","8193476a":"#We have one missing kitchen quality value, the two categories which probably impact it the most will be the overall quality and whether or not the kitchen\n#is above ground.\n\ndf.KitchenQual.value_counts()\n\n\nprint(df.groupby([\"OverallQual\", \"KitchenAbvGr\"]).KitchenQual.value_counts())\n\ndf[df.KitchenQual.isnull()]","538f09e8":"#Overall quality of the missing kitchenqual entry is 5, and kitchen above ground is 1, so we should fill it with 'TA' response \ndf.loc[(df.KitchenQual.isnull()) & (df.OverallQual == 5) & (df.KitchenAbvGr == 1), \"KitchenQual\"] = \"TA\"\n\nmissing(df)","40d9e472":"#Look at SaleType\nprint(df.SaleType.value_counts())\ndf[df.SaleType.isnull()]","3021fdc9":"#Lets look at the sale condition by neighborhood. We're looking for Sawyer neighborhood, and Normal Sale condition.\ndf.groupby(['Neighborhood', 'SaleCondition']).SaleType.value_counts()","7462aa4d":"#So we fill Sale Type with 'WD'\ndf.loc[(df.SaleType.isnull()) & (df.Neighborhood == 'Sawyer') & (df.SaleCondition == 'Normal'), \"SaleType\"] = \"WD\"\n\nmissing(df)","fb40b834":"#Take a look at electrical\nprint(df.Electrical.value_counts())\n\ndf[df.Electrical.isnull()]","95206ba7":"#We'll look at the year built and the neighborhood to compare to houses built roughly at the same time.\n\ndf[(df.YearBuilt > 2005) & (df.Neighborhood == \"Timber\")].Electrical.value_counts()\n\n","83a4226d":"#Fill in Electrical with SBrkr\ndf.loc[(df.Electrical.isnull()) & (df.Neighborhood == 'Timber') & (df.YearBuilt > 2005), \"Electrical\"] = \"SBrkr\"\n\nmissing(df)","67622aaa":"#Exterior1st Variable\nprint(df.Exterior1st.value_counts())\n\ndf[df.Exterior1st.isnull()]\n\n","1eff427d":"#I want to look at similar homes. We look at homes built within 10 years in the same neighborhood and same overall quality.\ndf[(df.Neighborhood == \"Edwards\") & (df.OverallQual == 5) & (df.YearBuilt < 1945) & (df.YearBuilt > 1935)]","ecc10bcc":"#Fill in Exterior 1st and 2nd with Wd Sdng. The observation is missing both features.\ndf.Exterior1st.fillna(\"Wd Sdng\", inplace = True)\ndf.Exterior2nd.fillna(\"Wd Sdng\", inplace = True)\n\nmissing(df)","9b26d981":"#Lets do Functional next.\n\nprint(df.Functional.value_counts())\ndf[df.Functional.isnull()]\n\n","2e79c596":"#Take a look at functional vs overall condition. \ndf.groupby([\"OverallCond\"]).Functional.value_counts()\n","45881abe":"#Fill them with their respective modes.\ndf.loc[(df.Functional.isnull()) & (df.OverallCond == 1), \"Functional\"] = \"Maj1\"\ndf.loc[(df.Functional.isnull()) & (df.OverallCond == 5), \"Functional\"] = \"Typ\"\n\nmissing(df)","656a92ab":"#Utilities\n\nprint(df.Utilities.value_counts())\ndf[df.Utilities.isnull()]","3a93ad05":"#We have every data point except one having allpub as the response. That's good enough for me.\ndf.Utilities.fillna(\"AllPub\", inplace = True)","84a99151":"#Last variable is LotFrontage\ndf[df.LotFrontage.isnull()]","4fe9b2c3":"df.groupby([\"Neighborhood\" , \"LotShape\", \"LotConfig\"]).LotFrontage.median()","acb9d8fa":"df.LotFrontage.fillna(df.groupby([\"Neighborhood\" , \"LotShape\", \"LotConfig\"]).LotFrontage.transform('median'), inplace = True)\ndf[df.LotFrontage.isnull()]","0daa3240":"df.LotFrontage.fillna(df.groupby([\"Neighborhood\" , \"LotShape\"]).LotFrontage.transform('median'), inplace = True)\nmissing(df)\ndf[df.LotFrontage.isnull()]","fad71887":"df.LotFrontage.fillna(df.groupby([\"Neighborhood\"]).LotFrontage.transform('median'), inplace = True)\nmissing(df)","72f7036a":"#MS Sub class is coded as integers. Lets change it or it will be interpreted incorrectly by the algorithm.\n\ndf[\"MSSubClass\"] = df[\"MSSubClass\"].astype(\"str\")\n\n","34edfcff":"#Fix mistake on garageyrblt, make the names of the groups for exterior2nd match exterior 1st.\n\ndf.loc[(df.GarageYrBlt == 2207), \"GarageYrBlt\"] = 2007\n\ndf.loc[(df.Exterior2nd == \"CmentBd\"), \"Exterior2nd\"] = \"CemntBd\"\ndf.loc[(df.Exterior2nd == \"Wd Shng\"), \"Exterior2nd\"] = \"WdShing\"\ndf.loc[(df.Exterior2nd == \"Brk Cmn\"), \"Exterior2nd\"] = \"BrkComm\"\n\n","d2aa426b":"def bar_box(df, col, target):\n    \n    sns.set_style(\"darkgrid\")\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex = True)\n    \n    order = sorted(df[col].unique())\n    \n    sns.countplot(data = df[df[target].notnull()], x = col, ax = axes[0], order = order)    \n    sns.countplot(data = df[df[target].isnull()], x = col, ax = axes[1], order = order)    \n    sns.boxplot(data = df, x = col, ax = axes[2], y = target, order = order)\n    \n    fig.suptitle(\"For Feature:  \" + col)\n    axes[0].set_title(\"in Training Set \")\n    axes[1].set_title(\"in Test Set \")\n    axes[2].set_title(col + \" --- \" + target)\n    \n    for ax in fig.axes:\n        plt.sca(ax)\n        plt.xticks(rotation=90)","df20bf7a":"def plot_scatter(df, col, target):\n    sns.set_style(\"darkgrid\")\n    \n    corr = df[[col, target]].corr()[col][1]    \n    c = [\"red\"] if corr >= 0.7 else ([\"brown\"] if corr >= 0.3 else\\\n                                    ([\"lightcoral\"] if corr >= 0 else\\\n                                    ([\"blue\"] if corr <= -0.7 else\\\n                                    ([\"royalblue\"] if corr <= -0.3 else [\"lightskyblue\"]))))    \n\n    fig, ax = plt.subplots(figsize = (5, 5))\n    \n    sns.scatterplot(x = col, y = target, data = df, c = c, ax = ax)        \n    ax.set_title(\"Correlation between \" + col + \" and \" + target + \" is: \" + str(corr.round(4)))","a20503de":"def feature_distribution(df, col, target, test = True):\n    sns.set_style(\"darkgrid\")\n    if test == True:\n        fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n\n        sns.kdeplot(data = df[df[target].notnull()], x = col, fill=True, label = \"Train\", ax = axes[0], color = \"orangered\")\n        sns.kdeplot(data = df[df[target].isnull()], x = col, fill=True, label = \"Test\", ax = axes[0], color = \"royalblue\")\n        axes[0].set_title(\"Distribution\")\n        axes[0].legend(loc = \"best\")\n        \n        sns.boxplot(data = df[df[target].notnull()], y = col, ax = axes[1], color = \"orangered\")\n        sns.boxplot(data = df[df[target].isnull()], y = col, ax = axes[2], color = \"royalblue\")\n        axes[2].set_ylim(axes[1].get_ylim())        \n        axes[1].set_title(\"Boxplot For Train Data\")\n        axes[2].set_title(\"Boxplot For Test Data\")\n        \n\n        stats.probplot(df[df[target].notnull()][col], plot = axes[3])\n        stats.probplot(df[df[target].isnull()][col], plot = axes[4])\n        axes[4].set_ylim(axes[3].get_ylim())        \n        axes[3].set_title(\"Probability Plot For Train data\")\n        axes[4].set_title(\"Probability Plot For Test data\")\n        \n        fig.suptitle(\"For Feature:  \" + col)\n    else:\n        fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n        \n        sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"orangered\")\n        sns.boxplot(data = df, y = col, ax = axes[1], color = \"orangered\")\n        stats.probplot(df[col], plot = axes[2])\n        \n        axes[0].set_title(\"Distribution\")\n        axes[1].set_title(\"Boxplot\")\n        axes[2].set_title(\"Probability Plot\")\n        fig.suptitle(\"For Feature:  \" + col)","5f0698bb":"df2 = df.copy()\ndf2[\"LogSalePrice\"] = np.log(df2[\"SalePrice\"])","9b7f1b3c":"num_feats, cat_feats, ord_feats = feat_type(df2)","852c1876":"for col in cat_feats:\n    bar_box(df2, col, target = \"LogSalePrice\")","9a2b7bf2":"for col in num_feats:\n    plot_scatter(df2, col, target = 'LogSalePrice')","9765d652":"for col in num_feats:\n    feature_distribution(df2, col, target = 'LogSalePrice')","4daa93db":"numeric_data = df2[df2.LogSalePrice.notnull()]\nnumeric_data2 = numeric_data[num_feats]\nnumeric_data2[\"LogSalePrice\"] = numeric_data[\"LogSalePrice\"]\n\n\ncorr = numeric_data2.corr()\n\nhighest_corr = corr.index[abs(corr[\"LogSalePrice\"] > 0.5)] \n\nplt.figure(figsize=(12,12))\nhm = sns.heatmap(numeric_data2[highest_corr].corr(),annot=True,cmap=\"RdYlGn\")\n","dd44cb31":"df2[cat_feats]","2eade96f":"df2[\"Older1945\"] = df2[\"MSSubClass\"].apply(lambda x: 1 if x in [\"30\", \"70\"] else 0)\n\ndf2[\"Newer1946\"] = df2[\"MSSubClass\"].apply(lambda x: 1 if x in [\"20\", \"60\", \"120\", \"160\"] else 0)\n\ndf2[\"AllStyles\"] = df2[\"MSSubClass\"].apply(lambda x: 1 if x in [\"20\", \"90\", \"190\"] else 0)\n\ndf2[\"AllAges\"] = df2[\"MSSubClass\"].apply(lambda x: 1 if x in [\"40\", \"45\", \"50\", \"75\", \"90\", \"150\", \"190\"] else 0)\n\ndf2[\"Pud\"] = df2[\"MSSubClass\"].apply(lambda x: 1 if x in [\"120\", \"150\", \"160\", \"180\"] else 0)\n\ndf2[\"Split\"] = df2[\"MSSubClass\"].apply(lambda x: 1 if x in [\"80\", \"85\"\"180\"] else 0)\n\ndf2[\"MSSubClass\"] = df2[\"MSSubClass\"].apply(lambda x: \"180\" if x == \"150\" else x)\n\n\n\ndf2[\"MSZoning\"] = df2[\"MSZoning\"].apply(lambda x: \"R\" if x.startswith(\"R\") else x)\n\n","341a24b2":"df2.loc[(df2[\"Condition1\"].isin([\"RRNn\", \"RRNe\"])) | (df2[\"Condition2\"].isin([\"RRNn\", \"RRNe\"])), \"RailroadDegree\"] = 1\ndf2.loc[(df2[\"Condition1\"].isin([\"RRAn\", \"RRAe\"])) | (df2[\"Condition2\"].isin([\"RRAn\", \"RRAe\"])), \"RailroadDegree\"] = 2\ndf2[\"RailroadDegree\"].fillna(0, inplace = True)\n\ndf2.loc[(df2[\"Condition1\"] == \"PosN\") | (df2[\"Condition2\"] == \"PosN\"), \"OffsiteFeature\"] = 1\ndf2.loc[(df2[\"Condition1\"] == \"PosA\") | (df2[\"Condition2\"] == \"PosA\"), \"OffsiteFeature\"] = 2\ndf2[\"OffsiteFeature\"].fillna(0, inplace = True)\n\ndf2[\"Norm1\"] = df2[\"Condition1\"].apply(lambda x: 1 if x == \"Norm\" else 0)\ndf2[\"Norm2\"] = df2[\"Condition2\"].apply(lambda x: 1 if x == \"Norm\" else 0)\ndf2[\"Norm\"] = df2[\"Norm1\"] + df2[\"Norm2\"]\ndf2.drop([\"Norm1\", \"Norm2\"], axis = 1, inplace = True)\n","79c01d75":"from sklearn.preprocessing import LabelEncoder\nord_le = LabelEncoder()","d0a6621a":"#Pull ordinal variables\ndf3 = df2.copy()","97c0e9e0":"cat_feats","908165e6":"#Many of these categorical variables are ordinal, lets map them. \n#Label encoder built-in to sklearn doesn't work very well as you cannot set the order.\nlotshape = {\"IR3\": 1, \"IR2\": 2, \"IR1\": 3, \"Reg\": 4}\nutilities = {\"ELO\": 1, \"NoSeWa\": 2, \"NoSewr\": 3, \"AllPub\": 4}\nlandslope = {\"Sev\": 1, \"Mod\": 2, \"Gtl\": 3}\n\ngeneral = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n\nbsmtexposure = {\"None\": 0, \"No\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3}\nbsmtfintype = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\nelectrical = {\"Mix\": 1, \"FuseP\": 2, \"FuseF\": 3, \"FuseA\": 4, \"SBrkr\": 5}\nfunctional = {\"Typ\": 1, \"Min1\": 2, \"Min2\": 3, \"Mod\": 4, \"Maj1\": 5, \"Maj2\": 6, \"Sev\": 7, \"Sal\": 8}\ngaragefinish = {\"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}\nfence = {\"None\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}\n\ndf3.replace({\"LotShape\": lotshape, \"Utilities\": utilities, \"LandSlope\": landslope, \n             \"BsmtExposure\": bsmtexposure, \"BsmtFinType1\": bsmtfintype, \"BsmtFinType2\":bsmtfintype, \"Electrical\": electrical, \n             \"Functional\": functional, \"GarageFinish\": garagefinish, \"Fence\": fence}, \n             inplace = True)\n\nfor col in [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"KitchenQual\", \n            \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"PoolQC\"]:\n    df3[col] = df3[col].replace(general)","908b1807":"num_feats3, cat_feats3, ord_feats3 = feat_type(df3)","185eb279":"#See what is left for nominal categorical variables.\ncat_feats3","d01f666e":"#Simplify some variables, convert to ordinal for others.\ndf3[\"BldgType\"] = df3[\"BldgType\"].apply(lambda x: \"2Fam\" if x in [\"2fmCon\", \"Duplex\"] else x)\ndf3[\"BldgType\"] = df3[\"BldgType\"].apply(lambda x: \"Townhouse\" if x.startswith(\"Town\") else x)\n\ndf3[\"HouseStyle\"] = df3[\"HouseStyle\"].apply(lambda x: \"1Story\" if x.startswith(\"1\") else x)\ndf3[\"HouseStyle\"] = df3[\"HouseStyle\"].apply(lambda x: \"2Story\" if x.startswith(\"2\") else x)\ndf3[\"HouseStyle\"] = df3[\"HouseStyle\"].apply(lambda x: \"Split\" if x.startswith(\"S\") else x)\n\ndf3[\"RoofStyle\"] = df3[\"RoofStyle\"].apply(lambda x: \"Other\" if x not in [\"Gable\"] else x)\ndf3[\"RoofMatl\"] = df3[\"RoofMatl\"].apply(lambda x: \"Other\" if x != \"CompShg\" else x)\ndf3[\"MasVnrType\"] = df3[\"MasVnrType\"].apply(lambda x: \"None_BrkCmn\" if x in [\"None\", \"BrkCmn\"] else x)\n\nfoundation = {\"Slab\": 0, \"BrkTil\": 1, \"Stone\": 1, \"Wood\": 2, \"CBlock\": 2, \"PConc\": 3}\n\ndf3.replace({\"Foundation\" : foundation}, inplace = True)\n\ndf3[\"Heating\"] = df3[\"Heating\"].apply(lambda x: \"Other\" if x != \"GasA\" else x)\n\ndf3[\"GarageType\"] = df3[\"GarageType\"].apply(lambda x: \"Carport_None\" if x in [\"CarPort\", \"None\"] else x)\ndf3[\"GarageType\"] = df3[\"GarageType\"].apply(lambda x: \"Basement_2Types\" if x in [\"Basment\", \"2Types\"] else x)\n\npaveddrive = {\"Y\": 2, \"P\": 1, \"N\" : 0}\ndf3.replace({\"PavedDrive\" : paveddrive}, inplace = True)\n\ndf3[\"SaleType\"] = df3[\"SaleType\"].apply(lambda x: \"WD\" if x.endswith(\"WD\") else x)\ndf3[\"SaleType\"] = df3[\"SaleType\"].apply(lambda x: \"Contract\" if x.startswith(\"Con\") else x)\ndf3[\"SaleType\"] = df3[\"SaleType\"].apply(lambda x: \"Oth\" if x == \"COD\" else x)\n","a0961e75":"num_feats3, cat_feats3, ord_feats3 = feat_type(df3)","1666ab89":"ord_feats3","e28fb48b":"#take a look at the list\nnum_feats3","879c2c36":"#Little bit of info.\ndf3[num_feats]","8872a4d6":"#New copy for manipulation\ndf4 = df3.copy()","d19411f5":"df4[\"FrontageRatio\"] = (df4[\"LotFrontage\"] \/ df4[\"LotArea\"])\ndf4[\"HQFloor\"] = df4[\"1stFlrSF\"] + df4[\"2ndFlrSF\"]\ndf4[\"FloorAreaRatio\"] = df4[\"GrLivArea\"] \/ df4[\"LotArea\"]\n\ndf4[\"TotalArea\"] = df4[\"TotalBsmtSF\"] + df4[\"GrLivArea\"]\ndf4[\"TotalPorch\"] = df4[\"WoodDeckSF\"] + df4[\"OpenPorchSF\"] + df4[\"EnclosedPorch\"] + df4[\"3SsnPorch\"] + df4[\"ScreenPorch\"]\n\ndf4[\"TotalFullBath\"] = df4[\"BsmtFullBath\"] + df4[\"FullBath\"]\ndf4[\"TotalHalfBath\"] = df4[\"BsmtHalfBath\"] + df4[\"HalfBath\"]\n\ndf4[\"TotalBsmtBath\"] = df4[\"BsmtFullBath\"] + 0.5 * df4[\"BsmtHalfBath\"]\ndf4[\"TotalBath\"] = df4[\"TotalFullBath\"] + 0.5 * (df4[\"BsmtHalfBath\"] + df4[\"HalfBath\"]) + df4[\"BsmtFullBath\"] + 0.5 * df4[\"BsmtHalfBath\"]\n\n\n#bunch of booleans for certain things.\ndf4[\"HasPool\"] = df4[\"PoolArea\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"Has2ndFlr\"] = df4[\"2ndFlrSF\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"HasBsmt\"] = df4[\"TotalBsmtSF\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"HasFireplace\"] = df4[\"Fireplaces\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"HasGarage\"] = df4[\"GarageArea\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"HasLowQual\"] = df4[\"LowQualFinSF\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"HasPorch\"] = df4[\"TotalPorch\"].apply(lambda x: 0 if x == 0 else 1)\ndf4[\"HasMiscVal\"] = df4[\"MiscVal\"].apply(lambda x: 0 if x == 0 else 1)\n\n\n","09651dff":"df4[\"RestorationAge\"] = df4[\"YearRemodAdd\"] - df4[\"YearBuilt\"]\ndf4[\"RestorationAge\"] = df4[\"RestorationAge\"].apply(lambda x: 0 if x < 0 else x)\ndf4[\"HasRestoration\"] = df4[\"RestorationAge\"].apply(lambda x: 0 if x == 0 else 1)\n\ndf4[\"YearAfterRestoration\"] = df4[\"YrSold\"] - df4[\"YearRemodAdd\"]\ndf4[\"YearAfterRestoration\"] = df4[\"YearAfterRestoration\"].apply(lambda x: 0 if x < 0 else x)\n\ndf4[\"BuildAge\"] = df4[\"YrSold\"] - df4[\"YearBuilt\"]\ndf4[\"BuildAge\"] = df4[\"BuildAge\"].apply(lambda x: 0 if x < 0 else x)\ndf4[\"IsNewHouse\"] = df4[\"BuildAge\"].apply(lambda x: 1 if x == 0 else 0)","154b0895":"#Now get the new feature list types, specifically for ordinal.\nnum_feats4, cat_feats4, ord_feats4 = feat_type(df4)","29ca86a3":"#Take another look at those scatter plots.\nfor col in num_feats4:\n    plot_scatter(df4, col, target = \"LogSalePrice\")\n","4c7b8ee1":"ord_feats4","06930f60":"for col in ord_feats4:\n    bar_box(df4, col, target = \"LogSalePrice\")\n\n","fcf56d49":"#manually bin groups with similar means on the box charts. Delete variables that give no info to price.\ndf4[\"LotShape\"] = df4[\"LotShape\"].apply(lambda x: 0 if x in [1, 2, 3] else 1)\ndf4[\"OverallCond\"] = df4[\"OverallCond\"].apply(lambda x: 1 if x in [1, 2] else (2 if x in [3,4] else 3))\ndf4[\"ExterCond\"] = df4[\"ExterCond\"].apply(lambda x: 1 if x in [1, 2] else 2)\ndf4[\"BsmtQual\"] = df4[\"BsmtQual\"].apply(lambda x: 1 if x in [0,1,2] else x-1)\ndf4[\"BsmtCond\"] = df4[\"BsmtCond\"].apply(lambda x: 1 if x in [0,1] else x)\ndf4[\"BsmtFinType1\"] = df4[\"BsmtFinType1\"].apply(lambda x : 1 if x in [1, 2, 3, 4, 5] else (2 if x == 6 else 0))\ndf4[\"BsmtFinType2\"] = df4[\"BsmtFinType2\"].apply(lambda x : 1 if x >= 1 else 0)\ndf4[\"HeatingQC\"] = df4[\"HeatingQC\"].apply(lambda x : 1 if x in [1,2] else (2 if x in [3,4] else 3))\ndf4[\"Electrical\"] = df4[\"Electrical\"].apply(lambda x : 1 if x in [1,2] else x-1)\ndf4[\"BsmtFullBath\"] = df4[\"BsmtFullBath\"].apply(lambda x : 1 if x > 0 else 0)\ndf4[\"FullBath\"] = df4[\"FullBath\"].apply(lambda x : 1 if x in [0, 1] else ( 3 if x >= 3 else 2))\ndf4[\"HalfBath\"] = df4[\"HalfBath\"].apply(lambda x : 1 if x >= 1 else x)\ndf4[\"TotRmsAbvGrd\"] = df4[\"TotRmsAbvGrd\"].apply(lambda x : 4 if x <= 4 else (10 if x>= 10 else x))\ndf4[\"Fireplaces\"] = df4[\"Fireplaces\"].apply(lambda x : 2 if x >= 2 else x)\ndf4[\"FireplaceQu\"] = df4[\"FireplaceQu\"].apply(lambda x : 1 if x <= 1 else x)\ndf4[\"GarageCars\"] = df4[\"GarageCars\"].apply(lambda x : 3 if x>= 3 else x)\ndf4[\"GarageQual\"] = df4[\"GarageQual\"].apply(lambda x : 1 if x in [0,1] else (4 if x >= 4 else x))\ndf4[\"GarageCond\"] = df4[\"GarageCond\"].apply(lambda x : 1 if x in [0,1,2] else 2)\ndf4[\"PoolQC\"] = df4[\"PoolQC\"].apply(lambda x : 1 if x == 5 else 0)\ndf4[\"TotalFullBath\"] = df4[\"TotalFullBath\"].apply(lambda x : 1 if x in [0,1] else (4 if x >= 4 else x))\ndf4[\"TotalBath\"] = df4[\"TotalBath\"].apply(lambda x : 4 if x >= 4 else x)\n","d7dd4028":"#test = df3.copy()\n#test[\"BsmtFinType1\"].value_counts()","1e86af3f":"#test[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].apply(lambda x : 1 if x in [1, 2, 3, 4, 5] else (2 if x == 6 else 0))\n#test[\"BsmtFinType1\"].value_counts()","d9eb35df":"#Variables to drop as the groups were the same.\ndrop_feats = [\"LandSlope\", \"BsmtHalfBath\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"Functional\", \"PoolArea\", \"Fence\",\n             \"MoSold\", \"YrSold\", \"AllStyles\", \"Pud\", \"Split\", \"RailroadDegree\", \"TotalHalfBath\", \"TotalBsmtBath\",\n             \"HasMiscVal\", \"HasRestoration\"]\n\ndf4.drop(drop_feats, axis = 1, inplace = True)","568313c0":"df5 = df4.copy()","ec8d1bad":"#Organize the features again.\nnum_feats5, cat_feats5, ord_feats5 = feat_type(df5)","220ea6cf":"for col in cat_feats5:\n    print(col, df5[col].value_counts().size)","ab1081e7":"target_encoding = [\"MSSubClass\", \"Neighborhood\", \"Exterior1st\", \"Exterior2nd\", \"Condition1\", \"Condition2\", \"HouseStyle\"]\n\nfor col in target_encoding:\n    feature_name = col + \"Rank\"\n    df5.loc[:, feature_name] = df5[col].map(df5.groupby(col).SalePrice.median())\n    df5.loc[:, feature_name] = df5.loc[:, feature_name].rank(method = \"dense\")","7cae379c":"df5[\"Exterior\"] = np.where((df5[\"Exterior1st\"] != df5[\"Exterior2nd\"]), \"Mixed\", df5[\"Exterior1st\"])\ndf5[\"No2ndExt\"] = df5[\"Exterior\"].apply(lambda x: 0 if x == \"Mixed\" else 1)","f207cb01":"for col in num_feats5:\n    corr = df5[[col, \"LogSalePrice\"]].corr()[col][1]\n    print(col, corr)","e0bf9dd7":"for col in num_feats5:\n    corr = df5[[col, \"LogSalePrice\"]].corr(method = 'spearman')[col][1]\n    print(col, corr)","b4e60821":"#Drop features with low correlation. We have checked both spearman and pearson correlations. Spearman can give us some insight\n#into non-linear relationships. We can try to model these with transformations.\n\ndrop_feats2 = [\"MSSubClass\", \"Neighborhood\", \"Exterior1st\", \"Exterior2nd\", \"Condition1\", \"Condition2\", \"HouseStyle\",\n              \"BsmtFinSF2\", \"BsmtUnfSF\", \"LowQualFinSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"MiscVal\",\n              \"FrontageRatio\", \"FloorAreaRatio\", \"RestorationAge\"]\n\ndf5.drop(drop_feats2, axis = 1, inplace = True)\n    ","b82f156e":"#Gather features again.\nnum_feats5, cat_feats5, ord_feats5 = feat_type(df5)","e8ba4393":"df5.drop(\"SalePrice\", axis = 1, inplace = True)","1e072c01":"numeric = df5.select_dtypes(exclude = 'object')\nnumeric_cols = numeric.columns\nnumeric_feats = [x for x in numeric_cols if x not in [\"Id\", \"LogSalePrice\"]]","3e521ba7":"for feat in numeric_feats:\n    if df5[feat].value_counts().size == 2:\n        cat_feats5.append(feat)\n        \nnumeric_feats = [x for x in numeric_feats if x not in cat_feats5 + [\"Id\", \"SalePrice\", \"LogSalePrice\"]]","9cbf2812":"from sklearn.preprocessing import RobustScaler\nrbsc = RobustScaler()\n\n\ndef prep_data(df, cat_cols, num_cols, target):\n     \n    numeric_std = pd.DataFrame(rbsc.fit_transform(df[num_cols]), columns = df[num_cols].columns)  \n    dummies = pd.get_dummies(df[cat_cols], drop_first = True)\n    \n    numeric_std = numeric_std.reset_index(drop = True)\n    \n    data = pd.concat([df, dummies], axis = 1).drop(cat_cols, axis = 1).drop(num_cols, axis = 1)\n    \n    data = data.reset_index(drop = True)\n    \n    data2 = pd.concat([data, numeric_std], axis = 1)\n    \n    \n    train = data2[data2[target].notnull()]\n    test = data2[data2[target].isnull()]\n    \n    return  train, test\n\n\ntrain, test = prep_data(df5, cat_feats5, numeric_feats, \"LogSalePrice\")\n\ntarget = \"LogSalePrice\"\npredictors = [x for x in train.columns if x not in [\"Id\", \"LogSalePrice\"]]","8b717f92":"#Lets look at the skew.\n\ntrain_skew = []\ntest_skew = []\n\nfor col in numeric_feats:\n    train_skew.append(train[col].skew())\n    test_skew.append(test[col].skew())\n    \nskew_df = pd.DataFrame({\"Feature\": numeric_feats, \"TrainSkewness\": train_skew, \"TestSkewness\": test_skew})\nskewed = skew_df[skew_df.TrainSkewness.abs() >= 0.5]\nskewed","4c691e2b":"train_skew_yeoj = []\ntest_skew_yeoj = []\n\nfor col in skewed.Feature.tolist():\n    train[col], fitted_lambda = stats.yeojohnson(train[col])\n    test[col] = stats.yeojohnson(test[col], fitted_lambda)\n    \n    train_skew_yeoj.append(train[col].skew())\n    test_skew_yeoj.append(test[col].skew())    \n    \nskewed[\"TrainSkewness_AfterYeoJohnson\"] = train_skew_yeoj\nskewed[\"TestSkewness_AfterYeoJohnson\"] = test_skew_yeoj\n\nskewed\n\n","8a43572a":"high_skew = skewed[skewed.TrainSkewness_AfterYeoJohnson.abs() > 1].Feature.tolist()\nprint(high_skew)","551b88c2":"#We can remove highly skewed variables from our non-decision tree based learning algorithms\ntest_dt = test.copy()\ntrain_dt = train.copy()\n\ntrain.drop(high_skew, axis = 1, inplace = True)\ntest.drop(high_skew, axis = 1, inplace = True)","1926e76b":"print(train.shape, test.shape)","10666858":"train.info()","eb0350e2":"from sklearn.model_selection import train_test_split\nX = train.iloc[:, 2:]\ny = train[\"LogSalePrice\"]","f8333c5d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","76ce1c1f":"from sklearn.linear_model import LinearRegression\nslr = LinearRegression()\nslr.fit(X_train, y_train)\ny_train_pred = slr.predict(X_train)\ny_test_pred = slr.predict(X_test)","177e8adc":"plt.scatter(y_train_pred, y_train_pred - y_train, c = 'steelblue', marker = \"o\",\n           edgecolor = 'white', label = 'Training Data')\nplt.scatter(y_test_pred, y_test_pred - y_test, c = 'limegreen', marker = 's',\n           edgecolor = 'white', label = 'Test Data')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y=0, color='black', lw = 2, xmin = 10, xmax = 14)\nplt.xlim([10, 14])","cdd5025b":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprint('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred),\n                                      mean_squared_error(y_test, y_test_pred)))\nprint(\"R-squared train: %.3f, test: %.3f\" % (r2_score(y_train,y_train_pred),\n                                            r2_score(y_test, y_test_pred)))","6934e0b0":"from sklearn.linear_model import RANSACRegressor\nransac = RANSACRegressor(LinearRegression(),\n                        max_trials = 500,\n                        min_samples = 50,\n                        loss = 'absolute_loss',\n                        residual_threshold = 2.5,\n                        random_state = 0)\nransac.fit(X_train,y_train)\ny_train_pred = ransac.predict(X_train)\ny_test_pred = ransac.predict(X_test)","23f20317":"plt.scatter(y_train_pred, y_train_pred - y_train, c = 'steelblue', marker = \"o\",\n           edgecolor = 'white', label = 'Training Data')\nplt.scatter(y_test_pred, y_test_pred - y_test, c = 'limegreen', marker = 's',\n           edgecolor = 'white', label = 'Test Data')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y=0, color='black', lw = 2, xmin = 10, xmax = 14)\nplt.xlim([10, 14])","8ba95e52":"#Not much difference here, seems like most outliers were delt with via robust scaling and\/or skew correction\/dropping\n\nprint('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred),\n                                      mean_squared_error(y_test, y_test_pred)))\nprint(\"R-squared train: %.3f, test: %.3f\" % (r2_score(y_train,y_train_pred),\n                                            r2_score(y_test, y_test_pred)))","e73727f8":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha = 8, max_iter = 5000)\n\nridge.fit(X_train, y_train)\ny_train_pred = ridge.predict(X_train)\ny_test_pred = ridge.predict(X_test)\n\nprint('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred),\n                                      mean_squared_error(y_test, y_test_pred)))\nprint(\"R-squared train: %.3f, test: %.3f\" % (r2_score(y_train,y_train_pred),\n                                            r2_score(y_test, y_test_pred)))","74500b08":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.0003, max_iter = 5000)\n\nlasso.fit(X_train, y_train)\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\n\nprint('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred),\n                                      mean_squared_error(y_test, y_test_pred)))\nprint(\"R-squared train: %.3f, test: %.3f\" % (r2_score(y_train,y_train_pred),\n                                            r2_score(y_test, y_test_pred)))","07bfde74":"from sklearn.linear_model import ElasticNet\nelanet = ElasticNet(alpha = 0.0003, l1_ratio = 0.95, max_iter = 5000)\n\nelanet.fit(X_train, y_train)\ny_train_pred = elanet.predict(X_train)\ny_test_pred = elanet.predict(X_test)\n\nprint('MSE train: %.3f, test: %.3f' % (mean_squared_error(y_train, y_train_pred),\n                                      mean_squared_error(y_test, y_test_pred)))\nprint(\"R-squared train: %.3f, test: %.3f\" % (r2_score(y_train,y_train_pred),\n                                            r2_score(y_test, y_test_pred)))","2db0bf3a":"elanet.score(X_train,y_train)","59677cdb":"from sklearn.model_selection import cross_val_score","4bff83b9":"models = [slr, ridge, lasso, elanet]\n\nfor model in models:\n    scores = cross_val_score(estimator = model,\n                            X = X_train,\n                            y = y_train,\n                            cv = 10,\n                            n_jobs = 1)\n    print()\n    print(\"%s CV accuracy scores: %s\" % (model, scores))\n    print(\"%s CV accuracy %.3f +\/- %.3f\" % (model, np.mean(scores), np.std(scores)))","92464519":"from sklearn.model_selection import learning_curve","eb27df86":"for model in models:\n    train_sizes, train_scores, test_scores = learning_curve(estimator = model,\n                                                           X = X_train,\n                                                           y = y_train,\n                                                           train_sizes = np.linspace(0.1, 1.0, 10),\n                                                            cv = 10,\n                                                            n_jobs = 1)\n    train_mean = np.mean(train_scores, axis = 1)\n    train_std = np.std(train_scores, axis = 1)\n    test_mean = np.mean(test_scores, axis = 1)\n    test_std = np.std(test_scores, axis = 1)\n    \n    plt.plot(train_sizes, train_mean, color = 'blue', marker = 'o',\n            markersize = 5, label = 'training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std,\n                    train_mean - train_std,\n                    alpha = 0.15, color = 'blue')\n    plt.plot(train_sizes, test_mean, color = 'green', linestyle ='--',\n            marker = 's', markersize =5, label = 'validation accuracy')\n    plt.fill_between(train_sizes, test_mean + test_std,\n                    test_mean - test_std, alpha = 0.15, color = 'green')\n    plt.grid()\n    plt.xlabel(\"number of training samples\")\n    plt.ylabel('Accuracy')\n    plt.legend(loc = 'lower right')\n    plt.ylim([0.7, 1.0])\n    plt.show()","5be89b24":"X_dt = train_dt.iloc[:, 2:]\ny_dt = train_dt[\"LogSalePrice\"]\n\nX_traindt, X_testdt, y_traindt, y_testdt = train_test_split(X_dt, y_dt, test_size = 0.3, random_state = 0)","7b40be74":"#Decision Tree Regressor did not work very well.\n#from sklearn.tree import DecisionTreeRegressor\n#tree = DecisionTreeRegressor(max_depth = 100)\n\nfrom sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(n_estimators = 100, criterion = 'mse', n_jobs = 1, max_depth = 50)\n\ndt_models = [tree, forest]\n\nfor model in dt_models:\n    scores = cross_val_score(estimator = model,\n                            X = X_traindt,\n                            y = y_traindt,\n                            cv = 10,\n                            n_jobs = 1)\n    print()\n    print(\"%s CV accuracy scores: %s\" % (dt_models, scores))\n    print(\"%s CV accuracy %.3f +\/- %.3f\" % (dt_models, np.mean(scores), np.std(scores)))","a718509b":"models =  [ridge, lasso, elanet]\nmodelsdt = [forest]","3a6df4df":"def graph_thing(models_lr, models_dt):\n    for model in models:\n        train_sizes, train_scores, test_scores = learning_curve(estimator = model,\n                                                               X = X_train,\n                                                               y = y_train,\n                                                               train_sizes = np.linspace(0.1, 1.0, 10),\n                                                                cv = 10,\n                                                                n_jobs = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        train_std = np.std(train_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n\n        plt.plot(train_sizes, train_mean, color = 'blue', marker = 'o',\n                markersize = 5, label = 'training accuracy')\n        plt.fill_between(train_sizes, train_mean + train_std,\n                        train_mean - train_std,\n                        alpha = 0.15, color = 'blue')\n        plt.plot(train_sizes, test_mean, color = 'green', linestyle ='--',\n                marker = 's', markersize =5, label = 'validation accuracy')\n        plt.fill_between(train_sizes, test_mean + test_std,\n                        test_mean - test_std, alpha = 0.15, color = 'green')\n        plt.grid()\n        plt.xlabel(\"number of training samples\")\n        plt.ylabel('Accuracy')\n        plt.legend(loc = 'lower right')\n        plt.ylim([0.7, 1.0])\n        plt.show()\n        \n    for model in models_dt:\n        train_sizes, train_scores, test_scores = learning_curve(estimator = model,\n                                                               X = X_traindt,\n                                                               y = y_traindt,\n                                                               train_sizes = np.linspace(0.1, 1.0, 10),\n                                                                cv = 10,\n                                                                n_jobs = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        train_std = np.std(train_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n\n        plt.plot(train_sizes, train_mean, color = 'blue', marker = 'o',\n                markersize = 5, label = 'training accuracy')\n        plt.fill_between(train_sizes, train_mean + train_std,\n                        train_mean - train_std,\n                        alpha = 0.15, color = 'blue')\n        plt.plot(train_sizes, test_mean, color = 'green', linestyle ='--',\n                marker = 's', markersize =5, label = 'validation accuracy')\n        plt.fill_between(train_sizes, test_mean + test_std,\n                        test_mean - test_std, alpha = 0.15, color = 'green')\n        plt.grid()\n        plt.xlabel(\"number of training samples\")\n        plt.ylabel('Accuracy')\n        plt.legend(loc = 'lower right')\n        plt.ylim([0.7, 1.0])\n        plt.show()","49673439":"graph_thing(models, modelsdt)","5baf5315":"from skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","ae2dc9ae":"kf = KFold(n_splits = 10, shuffle = True, random_state = 0)\n\n#def rmse_cv(model, X = X_train, y = y_train):    \n#   return np.sqrt(-cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv = kf)).mean()","9b919d5f":"#def on_step(optim_result):\n#    \"\"\"\n#    Callback meant to view scores after\n#    each iteration while performing Bayesian\n#    Optimization in Skopt\"\"\"\n#    score = opt.best_score_\n#    print(\"best score: %s\" % score)\n#    if score >= -0.11:\n#        print('Interrupting!')\n#        return True\n\n","5953855b":"import xgboost as xgb\nfrom datetime import datetime\n\n\n#xgbr =  xgb.XGBRegressor(objective = \"reg:squarederror\", n_jobs = -1, random_state = 1, learning_rate = 0.1, n_estimators = 200)\n\n#%%time\n#start = datetime.now()\n#print(start)\n\n#opt = GridSearchCV(xgbr, \n#                   {\n#                       \"max_depth\": np.arange(3, 13,1),\n#                       \"colsample_bytree\": np.arange(0.3, 1, 0.1),\n#                      \"reg_alpha\": np.arange(0, 1,0.1),\n#                       \"reg_lambda\": np.arange(0, 1,0.1),\n#                       \"gamma\": np.arange(0, 0.5,0.1)\n#                   }, \n#                    cv = kf,n_jobs = -1,\n#                    scoring = \"neg_root_mean_squared_error\",\n#                   )\n\n\n#opt.fit(X_train, y_train)\n\n\n#end = datetime.now()\n#print(end)\n\n#print(\"Best Score is: \", opt.best_score_, \"\\n\")\n\n#print(\"Best Parameters: \", opt.best_params_, \"\\n\")\n\n#xgbr2 = opt.best_estimator_\n#xgbr2\n\n","1f35beb1":"np.arange(0.25,1,0.05)","fdeb84c9":"#Found the parameters and put them above.\n#param_range = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0,  0.1, 0.3, 1, 3, 10, 30, 100] \n#param_grid = [{'alpha': param_range}]\n\n#lasso = Lasso(random_state = 1, max_iter = 5000)\n#gs = GridSearchCV(estimator = lasso, param_grid = param_grid, scoring = 'neg_root_mean_squared_error', cv = 10, n_jobs = -1)\n\n#gs.fit(X_train, y_train)\n#print(gs.best_score_)\n\n#print(gs.best_params_)","d3a92e2a":"#param_grid = [{'alpha': param_range}]\n\n#ridge = Ridge(random_state = 1, max_iter = 5000)\n#gs = GridSearchCV(estimator = ridge, param_grid = param_grid, scoring = 'neg_root_mean_squared_error', cv = 10, n_jobs = -1)\n\n#gs.fit(X_train, y_train)\n#print(gs.best_score_)\n\n#print(gs.best_params_)","2eb27301":"#param_grid = [{'alpha': param_range, 'l1_ratio': [0.00, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]}]\n\n#elanet = ElasticNet(max_iter = 5000)\n#gs = GridSearchCV(estimator = elanet, param_grid = param_grid, scoring = 'neg_root_mean_squared_error', cv = 10, n_jobs = -1)\n\n#gs.fit(X_train, y_train)\n#print(gs.best_score_)\n\n#print(gs.best_params_)","d028c2d7":"import lightgbm as lgb\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nxgb_model = xgb.XGBRegressor(colsample_bytree=0.25, gamma=0.0, learning_rate=0.01, max_depth=3,\n                             n_estimators=10000, n_jobs=-1, random_state=1, \n                             reg_alpha=0.24206673672530965, reg_lambda=0.40464485640717085, subsample=1.0)\n\nlgb_model = lgb.LGBMRegressor(colsample_bytree=0.25, learning_rate=0.01,\n                              max_depth=13, min_child_samples=7, n_estimators=10000,\n                              num_leaves=20, objective='regression', random_state=1,\n                              subsample=0.9330025956033094, subsample_freq=1)\n\ngbr_model = GradientBoostingRegressor(alpha=0.8979588317644014,\n                                      learning_rate=0.01, loss='huber',\n                                      max_depth=13, max_features=0.1, min_samples_split=109,\n                                      n_estimators=10000, n_iter_no_change=100, random_state=1)","eca5590e":"scores = cross_val_score(estimator = xgb_model,\n                        X = X_train,\n                        y = y_train,\n                        cv = 10,\n                        n_jobs = 1)\nprint()\nprint(\"%s CV accuracy scores: %s\" % (xgb_model, scores))\nprint(\"%s CV accuracy %.3f +\/- %.3f\" % (xgb_model, np.mean(scores), np.std(scores)))","8964d5eb":"models = {\n    \"LGBMRegressor\": lgb_model,\n    \"XGBRegressor\": xgb_model,\n    \"GradientBoostingRegressor\": gbr_model,\n    \"Lasso\": lasso,\n    \"Ridge\": ridge,\n    \"ElasticNet\": elanet,\n         }\n","6334e545":"X_test1 = test.copy()\nX_train1 = train.copy()\ny_train = train[\"LogSalePrice\"]\n","bcf848ac":"#X_train.drop(\"LogSalePrice\", axis = 1, inplace = True)\ny_train","ede74f8e":"X_test = X_test1.iloc[:,2:]\nX_train = X_train1.iloc[:,2:]","3554f8da":"lasso.fit(X_train,y_train)","39b05ae7":"X_test","e7c12b71":"X_train","dc7516bd":"oof_df = pd.DataFrame()\npredictions_df = pd.DataFrame()\n\n\nfor name, model in models.items():\n    \n    print(\"For model \", name, \"\\n\")\n    i = 1\n    oof = np.zeros(len(X_train))\n    predictions = np.zeros(len(X_test))\n    \n    for train_ix, test_ix in kf.split(X_train.values):\n        \n        print(\"Out of fold predictions generating for fold \", i)\n        \n        train_X, train_y = X_train.values[train_ix], y_train.iloc[train_ix]\n        test_X, test_y = X_train.values[test_ix], y_train.iloc[test_ix]\n        \n        if name == \"LGBMRegressor\":\n            model.fit(train_X, train_y,\n                      eval_set = [(test_X, test_y)],\n                      eval_metric = \"rmse\",\n                      early_stopping_rounds=200,\n                      verbose=0)\n            \n        elif name == \"XGBRegressor\":\n            model.fit(train_X, train_y,\n                      eval_set = [(test_X, test_y)],\n                      eval_metric = \"rmse\",\n                      early_stopping_rounds=250,\n                      verbose=0)\n        else:\n            model.fit(train_X, train_y)\n            \n        oof[test_ix] = oof[test_ix] + model.predict(X_train.values[test_ix])\n        predictions = predictions + model.predict(X_test.values)\n        \n        i = i + 1\n        \n        oof_df[name] = oof\n        predictions_df[name] = predictions \/ 10\n        \n        \n    print(\"\\nDone \\n\")\n\n","4ffb8f79":"predictions_df","f0f02c4d":"oof_df","85dabe15":"preds =  (2 * predictions_df[\"LGBMRegressor\"] +\n         2 * predictions_df[\"XGBRegressor\"] +\n         2 * predictions_df[\"GradientBoostingRegressor\"] +\n         predictions_df[\"Lasso\"] +\n         predictions_df[\"Ridge\"]+\n         predictions_df[\"ElasticNet\"]) \/ 9","a917275d":"preds","2a8bf862":"tid= test.Id\ntid","ce695c6d":"sub = pd.DataFrame()\nsub['Id'] = test.Id\nsub['SalePrice'] = np.expm1(preds).values\nsub.to_csv(\"blended.csv\", index = False)\n\nsub","9e6e412b":"## 5. Feature Engineering\n\nLets combine some of these variables and see if we can't get some more useful information out of them.\n\n### 5.1 Categorical Variables\n","860f8b2e":"### 2. Missing Value Treatment\n\nWe will take a look at missing values for the variables of interest.","7c089434":"### 1. Variable Identification\n\nHere we're just looking for the independent\/response and dependent\/explanatory variables. We also would like to classify what type of variables these are to begin our analysis. This is somewhat done for us already due to the nature of the data. Lets grab our data and take a look. ","21be2753":"From these descriptive statistics, we can see that the target variable is right-skewed. This is as expected, housing prices have a lower bound of (unrealistically) 0 dollars, and an unbounded upper value. We can get a closer look with a histogram. We will keep this in mind for later.","5bbf287f":"### 5.3 Ordinal Features\n\nWe should simplify these and combine bins where possible.","ded1d177":"A log transformation makes our target variable significantly less skewed and approach normally distributed. This isn't really a requirement for regression to work well, but it can help make things a bit more predictable. Additionally it can help when selecting features as there will be less values very far from the center. The prediction tends to be more accurate when the target variable is normal.\n\nLets combine the datasets, isolate our features by type, and do a bit of variable analysis.","77bdf8b7":"The response variable is SalePrice, looks like it is a numerical response and so we will use some sort of regression analysis. We have the types of all of the other explanatory variables which will help us with our analysis and model building.\n\nA closer look at the target variable.","5e41cc28":"### Hyperparameter tuning\nThe linear models are pretty easy, just iterate values until you get the \"best\" response on your CV set, but XGB has many hyperparameters. Two options for tuning are GridSearchCV and BayesSearchCV. BayesSearch seems to be a little more convenient because you can set intervals. \n\nI am going to use GridSearchCV\n","b9d19516":"### Assessing bias and variance\n\nAbove are various linear models with various results, I would like to take a look at the bias and variance to try to see if we're overfitting.","7c808265":"### 5.2 Numeric Features\n\nWe will try to combine some variables to make new variables, or see if some of them can be binned and converted to ordinal.","1f91973a":"Each of the linear models with some sort of regularization seem to be performing well and have decent bias\/variance tradeoff. Lets look at a decision tree model as well.\n","65d18db0":"### 6. Model Building\n\nLets take a look at some preliminary fits. I'm going to split my training data and look at a few linear models. I want to run a decision tree model as well and historically XGB model works quite well for this problem also.","d44568ac":"### 3\/4. Bivariate Analysis\n\nWe have three types of variables (numeric, categorical and ordinal), and the univariate and bivariate will be different for each. ","60e2434c":"Above are three functions to give various visualizations. ","a2b26f10":"That should be it for missing values and general \"fixes\" to the data set. We can now get into the bivariate analysis and eventually feature engineering.","5d392dbc":"### 5.4 Preprocessing\n\nWe've cleaned up all of our features, lets split our complete dataset into the train and test sections, and take a look at some preprocessing such as standardizing the numeric variables and converting catagorical variables into dummy variables.\n\nWe will also take a look at the skew.\n","c2f5f38b":"## Housing Prices Dataset\n\nLets take a look at the House Prices - Advanced Regression Techniques dataset. This is a somewhat large dataset with 79 explanatory variables so there is plenty of room for analysis. We will also be using the XGBOOST optimization method as it seems to be recommended and quite successful, as well as looking at a few other techniques.\n\n## Steps of Data Exploration\n1. Variable Identification\n2. Missing Value Treatment\n3. Univariate Analysis\n4. Bi-variate Analysis\n5. Feature Engineering\n    * Variable Transformation\n    * Variable Creation\n    * Feature Removal\n    * Preprocessing\n6. Model Building \n    * Bias\/Variance Analysis\n    * Hyperparameter tuning\n    \n    \nI will do my best to work through the given steps, occasionally I may be out of order but these are the ideas.\n\n"}}