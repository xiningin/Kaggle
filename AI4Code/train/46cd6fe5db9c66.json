{"cell_type":{"9b4cfdb5":"code","f1f22f37":"code","eb409768":"code","d5a9ed83":"code","79349e7e":"code","0fb9f3db":"code","353f4aaa":"code","fe9b5ea2":"code","dde90530":"code","b9761866":"code","c3130325":"code","2b7a9ada":"code","2696f8ac":"code","eda9e705":"code","ec218306":"code","6be7783c":"code","3bb8cf1a":"code","a82f3efe":"code","e3509267":"code","095686a2":"code","61ef81d3":"code","316775bd":"code","f5a55cd9":"code","d94b6b85":"code","f9188d1f":"code","108159b4":"code","552bfbc5":"code","95efb99c":"code","8b50ddae":"code","cbbda74e":"code","393aaeca":"code","d1a3aaf6":"code","e9f04b71":"code","dd89c971":"code","5865d2b4":"code","41a1e138":"code","9e8d5be8":"code","8ea40ed1":"code","4a058e9e":"code","8e26a532":"code","7924c120":"code","17587329":"code","10605b44":"markdown","8a388664":"markdown","eacf3a0e":"markdown","06a57145":"markdown","b1e326e0":"markdown","488aff67":"markdown","adca2687":"markdown","0081de1f":"markdown","da3505af":"markdown","36ed5478":"markdown","c1684e5b":"markdown","6ac785af":"markdown","a40b8200":"markdown","0ec4e1f5":"markdown","cc6775b9":"markdown","fb5617d5":"markdown","c0a8d8ad":"markdown","8d8a26d7":"markdown","86032f76":"markdown","4af804b8":"markdown","5210df9d":"markdown","fa6df126":"markdown","cf3ba2bb":"markdown","79c92b65":"markdown","1d9bf726":"markdown","29a48fdb":"markdown","f42081e7":"markdown","f588c0f0":"markdown","a5cca67b":"markdown","265ae3a1":"markdown","cf932a65":"markdown","05626f1e":"markdown","334d2d60":"markdown","f8d46f85":"markdown","6169a414":"markdown","880de88b":"markdown","9419bdad":"markdown","7f963a96":"markdown","6cf2503d":"markdown","ebce1a54":"markdown","fddf13b0":"markdown","d7a256dd":"markdown","11059b64":"markdown","c1f65b97":"markdown","6ae97395":"markdown","3b3a663a":"markdown","36eb8468":"markdown","4fe01b3a":"markdown","5d93651c":"markdown","a1001b5f":"markdown","2ffbff6d":"markdown","dcca6203":"markdown","1eeb2df1":"markdown"},"source":{"9b4cfdb5":"from prettytable import PrettyTable \n  \n# Specify the Column Names while initializing the Table \nmyTable = PrettyTable([\"Type\", \"Training Dataset\", \"Testing Dataset\"], \n                     title = \"Table 1 The composition of chest X-ray dataset\") \n  \n# Add rows \nmyTable.add_row([\"Normal\", \"1349 (25.7%)\", \"234 (37.5%)\"]) \nmyTable.add_row([\"Bacteria\", \"2538 (48.5%)\", \"242 (38.7%)\"]) \nmyTable.add_row([\"Virus\", \"1343 (25.7%)\", \"148 (23.7%)\"]) \nmyTable.add_row([\"Total\", \"5232 (100%)\", \"624 (100%)\"]) \n\nprint(myTable)","f1f22f37":"# Load the numpy and panda package for linear algebra and data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Imports packages to view data\nfrom glob import glob\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport os\nfrom skimage.io import imread\nimport cv2\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\n\n# Import keras packages\nfrom keras import applications\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.utils.data_utils import Sequence\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\n# Load the data \ndata_dir  = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\ntrain_dir = os.path.join(data_dir,'train\/')\nval_dir = os.path.join(data_dir,'val\/')\ntest_dir = os.path.join(data_dir,'test\/')\n\n# Setting Seeds for Reproducibility\n# seed = 231\n# np.random.seed(seed)\n# tf.random.set_seed(seed)\n","eb409768":"# Get the path to the normal and pneumonia sub-directories\nnormal_cases_dir = train_dir + 'NORMAL\/'\npneumonia_cases_dir = train_dir + 'PNEUMONIA\/'\n\n# Get the list of all the images\nnormal_cases = glob(normal_cases_dir+'\/*.jpeg')\npneumonia_cases = glob(pneumonia_cases_dir+'\/*.jpeg')\n\n# An empty list. We will insert the data into this list in (img_path, label) format\ntrain_data = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    train_data.append((img,0))\n\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    train_data.append((img, 1))\n\n# Get a pandas dataframe from the data we have in our list \ntrain_data = pd.DataFrame(train_data, columns=['image', 'label'],index=None)\n\n# Shuffle the data \ntrain_data = train_data.sample(frac=1.).reset_index(drop=True)\n\n# Get few samples for both the classes\npneumonia_samples = (train_data[train_data['label']==1]['image'].iloc[:4]).tolist()\nnormal_samples = (train_data[train_data['label']==0]['image'].iloc[:4]).tolist()\n\n# Concat the data in a single list and del the above two list\nsamples = pneumonia_samples + normal_samples\ndel pneumonia_samples, normal_samples\n\n# Plot the data \nf, ax = plt.subplots(2,4, figsize=(30,10))\nfor i in range(8):\n    img = imread(samples[i])\n    ax[i\/\/4, i%4].imshow(img, cmap='gray')\n    if i<4:\n        ax[i\/\/4, i%4].set_title(\"Pneumonia\")\n    else:\n        ax[i\/\/4, i%4].set_title(\"Normal\")\n    ax[i\/\/4, i%4].axis('off')\n    ax[i\/\/4, i%4].set_aspect('auto')\nplt.show()","d5a9ed83":"path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray'\ntrain_samplesize = pd.DataFrame.from_dict(\n    {'Normal': [len([os.path.join(path+'\/train\/NORMAL', filename) \n                     for filename in os.listdir(path+'\/train\/NORMAL')])], \n     'Pneumonia': [len([os.path.join(path+'\/train\/PNEUMONIA', filename) \n                        for filename in os.listdir(path+'\/train\/PNEUMONIA')])]})\n\n\nsns.barplot(data=train_samplesize).set_title('Training Set Data', fontsize=20)\nplt.show()","79349e7e":"training_images = tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/*\/*')\nvalidation_images = tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/*\/*')\n\n# Data before split\nprint(f'Before division of 80:20')\nprint(f'Total number of training images = {len(training_images)}')\nprint(f'Total number of validation images = {len(validation_images)}\\n')\n\n# Merge the training and validation, to split them afterwards\ntotal_files = training_images\ntotal_files.extend(validation_images)\nprint(f'Total number of images : training_images + validation_images = {len(total_files)}\\n')\n\n# Spliting 80:20\ntrain_images, val_images = train_test_split(total_files, test_size = 0.2)\nprint(f'After division of 80:20')\nprint(f'Total number of training images = {len(train_images)}')\nprint(f'Total number of validation images = {len(val_images)}')","0fb9f3db":"tf.io.gfile.makedirs('\/kaggle\/working\/val_dataset\/negative\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/val_dataset\/positive\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/train_dataset\/negative\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/train_dataset\/positive\/')","353f4aaa":"# Train images\nfor ele in train_images:\n    parts_of_path = ele.split('\/')\n\n    if 'PNEUMONIA' == parts_of_path[-2]:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/train_dataset\/positive\/' +  parts_of_path[-1])\n    else:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/train_dataset\/negative\/' +  parts_of_path[-1])\n\n# Validation images\nfor ele in val_images:\n    parts_of_path = ele.split('\/')\n\n    if 'PNEUMONIA' == parts_of_path[-2]:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/val_dataset\/positive\/' +  parts_of_path[-1])\n    else:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/val_dataset\/negative\/' +  parts_of_path[-1])","fe9b5ea2":"# Specify the Column Names while initializing the Table \nmytable = PrettyTable([\"Method\", \"Setting\"], \n                     title = \"Settings for the image augmentation.\") \n  \n# Add rows \nmytable.add_row([\"Rescale\", \"1\/255\"])\nmytable.add_row([\"Zoom Range\", \"0.3\"]) \nmytable.add_row([\"Vertical Flip\", \"True\"]) \n\nprint(mytable)","dde90530":"# Hyperparameters\nimage_size = 150\nepochs = 50\nbatch_size = 32","b9761866":"train_datagen = ImageDataGenerator(rescale = 1\/255,\n                                   zoom_range = 0.3,\n                                   # width_shift_range = 0.1,\n                                   # height_shift_range = 0.1, \n                                   # horizontal_flip = True,\n                                   # samplewise_center = True, \n                                   # rotation_range = 0.3,\n                                   # samplewise_std_normalization = False, \n                                   # featurewise_std_normalization = False,\n                                   vertical_flip=True)\n\nval_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_gen = train_datagen.flow_from_directory(\n    '\/kaggle\/working\/train_dataset\/',\n    target_size = (image_size, image_size),\n    batch_size = batch_size ,\n    class_mode = 'binary'\n)\n\nvalidation_gen = val_datagen.flow_from_directory(\n    '\/kaggle\/working\/val_dataset\/',\n    target_size = (image_size, image_size),\n    batch_size = batch_size ,\n    class_mode = 'binary'\n)\n\ntest_gen = test_datagen.flow_from_directory(\n     '..\/input\/chest-xray-pneumonia\/chest_xray\/test',\n    target_size = (image_size, image_size),\n    batch_size = batch_size , \n    class_mode = 'binary'\n)","c3130325":"count_normal = len([x for x in train_images if \"NORMAL\" in x])\ncount_pneumonia = len([x for x in train_images if \"PNEUMONIA\" in x])\n\nbias = np.log([count_pneumonia\/count_normal])\nbias\n\nweight_for_0 = (1 \/ count_normal)*(len(train_images))\/2.0 \nweight_for_1 = (1 \/ count_pneumonia)*(len(train_images))\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","2b7a9ada":"# Callbacks for Regularization\ncheckpoint = ModelCheckpoint(filepath='best_weights.CNN', \n                             save_best_only=True \n                             #save_weights_only=True\n                            )\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.3, \n                              patience=2, \n                              verbose=2, \n                              mode='max')","2696f8ac":"CNN = tf.keras.models.Sequential([\n    \n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu',padding='same', input_shape=(image_size, image_size, 3)),\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    \n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # The third convolution\n    tf.keras.layers.SeparableConv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.SeparableConv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # The fourth convolution\n    tf.keras.layers.SeparableConv2D(128, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.SeparableConv2D(128, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    # The fifth convolution\n    tf.keras.layers.SeparableConv2D(256, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.SeparableConv2D(256, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    \n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.7),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    \n    # Output layer, only 1 output neuron: it will contain a value from 0-1 \n    tf.keras.layers.Dense(1, activation='sigmoid'),\n    \n])","eda9e705":"# Model Summary\nCNN.summary()","ec218306":"# Compile our model\nCNN.compile(loss='binary_crossentropy',\n            optimizer='adam',\n            metrics=['accuracy','Recall', 'Precision'])","6be7783c":"# Fitting the model\nhistCNN = CNN.fit_generator(\n    train_gen, \n    steps_per_epoch = train_gen.samples \/\/ batch_size, \n    epochs = epochs, \n    validation_data = validation_gen, \n    validation_steps = validation_gen.samples \/\/ batch_size, \n    class_weight = class_weight, \n    callbacks = [checkpoint, lr_reduce])","3bb8cf1a":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(histCNN.history[met])\n    ax[i].plot(histCNN.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","a82f3efe":"test_result_CNN = CNN.evaluate_generator(test_gen, 624)\nprint('Loss rate at test data :', test_result_CNN[0])\nprint('Accuracy rate at test data :', test_result_CNN[1])\nprint('Recall rate at test data :', test_result_CNN[2])\nprint('Precision rate at test data :', test_result_CNN[3])","e3509267":"Inception_model = tf.keras.models.load_model('\/kaggle\/working\/best_weights.CNN')\n\nwrong_predicted_image = [[],[]]\ncorrect_predicted_image = [[],[]]\ni = 0\nwhile i< 5 and len(wrong_predicted_image[0]) < 6:\n    j = 0\n    while j < 32 and len(wrong_predicted_image[0]) < 6:\n        \n        image_array = (test_gen[i][0][j]).reshape(1,150,150,3)\n        \n        prediction = Inception_model.predict(image_array)\n        \n        if int(round(prediction[0][0])) != test_gen[i][1][j]:\n            wrong_predicted_image[0].append(image_array)\n            wrong_predicted_image[1].append(int(round(prediction[0][0])))\n            \n        elif len(correct_predicted_image[0]) < 6:\n            correct_predicted_image[0].append(image_array)\n            correct_predicted_image[1].append(int(round(prediction[0][0])))\n#        print(len(correct_predicted_image[0]),len(wrong_predicted_image[0]))\n        j += 1\n        \n    i += 1\n","095686a2":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in wrong_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if wrong_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'wrong_prediction_by_model --- {wrong_predicted_image[1]}')","61ef81d3":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in correct_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(150,150,3))\n    ax[i].imshow(image)\n    if correct_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'correct_prediction_by_model --- {correct_predicted_image[1]}')","316775bd":"# Hyperparameters\nimage_size = 224\nepochs = 50\nbatch_size = 64\n\ntrain_datagen = ImageDataGenerator(rescale = 1\/255,\n                                   zoom_range = 0.3,\n                                   # width_shift_range = 0.1,\n                                   # height_shift_range = 0.1, \n                                   # horizontal_flip = True,\n                                   # samplewise_center = True, \n                                   # rotation_range = 0.3,\n                                   # samplewise_std_normalization = False, \n                                   # featurewise_std_normalization = False,\n                                   vertical_flip=True)\n\nval_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_gen = train_datagen.flow_from_directory(\n    '\/kaggle\/working\/train_dataset\/',\n    target_size = (image_size, image_size),\n    batch_size = batch_size ,\n    class_mode = 'binary'\n)\n\nvalidation_gen = val_datagen.flow_from_directory(\n    '\/kaggle\/working\/val_dataset\/',\n    target_size = (image_size, image_size),\n    batch_size = batch_size ,\n    class_mode = 'binary'\n)\n\ntest_gen = test_datagen.flow_from_directory(\n     '..\/input\/chest-xray-pneumonia\/chest_xray\/test',\n    target_size = (image_size, image_size),\n    batch_size = batch_size , \n    class_mode = 'binary'\n)","f5a55cd9":"# Load the VGG-16 Model\nbase_VGG = tf.keras.applications.VGG16(input_shape=(image_size, image_size, 3),\n                                       include_top=False, \n                                       weights='imagenet')\nbase_VGG.trainable = False\n\n# Add some layers which we found to be \nVGG = tf.keras.Sequential([\n        base_VGG,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1,activation='sigmoid')\n        ])\n\n# Compile our model\nVGG.compile(loss='binary_crossentropy', \n            optimizer='adam', \n            metrics=['accuracy', 'Recall', 'Precision'])\nVGG.summary()","d94b6b85":"print(len(base_VGG.layers))","f9188d1f":"# Callbacks for Regularization\ncheckpoint = ModelCheckpoint(filepath='best_weights.VGG', \n                             save_best_only=True \n                             #save_weights_only=True\n                            )\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.3, \n                              patience=2, \n                              verbose=2, \n                              mode='max')","108159b4":"histVGG = VGG.fit_generator(\n    train_gen, \n    steps_per_epoch = train_gen.samples \/\/ batch_size, \n    epochs = epochs, \n    validation_data = validation_gen, \n    validation_steps = validation_gen.samples \/\/ batch_size,\n    class_weight = class_weight,\n    callbacks= [checkpoint, lr_reduce])","552bfbc5":"figure, axis = plt.subplots(1, 2, figsize=(18,5))\naxis = axis.ravel()\n\nfor i,element in enumerate(['accuracy', 'loss']):\n    axis[i].plot(histVGG.history[element])\n    axis[i].plot(histVGG.history['val_' + element])\n    axis[i].set_title('Model {}'.format(element))\n    axis[i].set_xlabel('epochs')\n    axis[i].set_ylabel(element)\n    axis[i].legend(['train', 'val'])","95efb99c":"test_result_VGG = VGG.evaluate_generator(test_gen, 624)\nprint('Loss rate at test data :', test_result_VGG[0])\nprint('Accuracy rate at test data :', test_result_VGG[1])\nprint('Recall rate at test data :', test_result_VGG[2])\nprint('Precision rate at test data :', test_result_CNN[3])","8b50ddae":"Inception_model = tf.keras.models.load_model('\/kaggle\/working\/best_weights.VGG')\n\nwrong_predicted_image = [[],[]]\ncorrect_predicted_image = [[],[]]\ni = 0\nwhile i< 5 and len(wrong_predicted_image[0]) < 6:\n    j = 0\n    while j < 32 and len(wrong_predicted_image[0]) < 6:\n        \n        image_array = (test_gen[i][0][j]).reshape(1,224,224,3)\n        \n        prediction = Inception_model.predict(image_array)\n        \n        if int(round(prediction[0][0])) != test_gen[i][1][j]:\n            wrong_predicted_image[0].append(image_array)\n            wrong_predicted_image[1].append(int(round(prediction[0][0])))\n            \n        elif len(correct_predicted_image[0]) < 6:\n            correct_predicted_image[0].append(image_array)\n            correct_predicted_image[1].append(int(round(prediction[0][0])))\n#        print(len(correct_predicted_image[0]),len(wrong_predicted_image[0]))\n        j += 1\n        \n    i += 1","cbbda74e":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in wrong_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(224,224,3))\n    ax[i].imshow(image)\n    if wrong_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'wrong_prediction_by_model --- {wrong_predicted_image[1]}')","393aaeca":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in correct_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(224,224,3))\n    ax[i].imshow(image)\n    if correct_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'correct_prediction_by_model --- {correct_predicted_image[1]}')","d1a3aaf6":"# Import ResNet-50\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n\nbase_resnet = ResNet50(input_shape=(image_size, image_size, 3),\n                                                     include_top=False, \n                                                     weights='imagenet')\n# base_resnet.trainable = False\n# Freazing the base trained layers\nfor layers in base_resnet.layers:\n    layers.trainable = False\n\n# Add some layers which we found to be \nResNet = tf.keras.Sequential([\n        base_resnet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Flatten(),\n     #  tf.keras.layers.Dense(2048, activation='relu'),\n     #  tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1,activation='sigmoid')\n        ])\n\n# Compile our model\nResNet.compile(loss='binary_crossentropy', \n               optimizer='adam', \n               metrics=['accuracy', 'Recall', 'Precision'])\n\n# ResNet.summary()","e9f04b71":"# Callbacks for Regularization\ncheckpoint = ModelCheckpoint(filepath='best_weights.ResNet', \n                             save_best_only=True \n                             # save_weights_only=True\n                            )\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.3, \n                              patience=2, \n                              verbose=2, \n                              mode='max')","dd89c971":"print(len(base_resnet.layers))","5865d2b4":"histResNet = ResNet.fit_generator(\n    train_gen, \n    steps_per_epoch = train_gen.samples \/\/ batch_size, \n    epochs = epochs, \n    validation_data = validation_gen, \n    validation_steps = validation_gen.samples \/\/ batch_size, \n    class_weight = class_weight,\n    callbacks= [checkpoint, lr_reduce])","41a1e138":"figure, axis = plt.subplots(1, 2, figsize=(18,5))\naxis = axis.ravel()\n\nfor i,element in enumerate(['accuracy', 'loss']):\n    axis[i].plot(histResNet.history[element])\n    axis[i].plot(histResNet.history['val_' + element])\n    axis[i].set_title('Model {}'.format(element))\n    axis[i].set_xlabel('epochs')\n    axis[i].set_ylabel(element)\n    axis[i].legend(['train', 'val'])","9e8d5be8":"test_result_ResNet = ResNet.evaluate_generator(test_gen, 624)\nprint('Loss rate at test data :', test_result_ResNet[0])\nprint('Accuracy rate at test data :', test_result_ResNet[1])\nprint('Recall rate at test data :', test_result_ResNet[2])\nprint('Precision rate at test data :', test_result_CNN[3])","8ea40ed1":"Inception_model = tf.keras.models.load_model('\/kaggle\/working\/best_weights.ResNet')\n\nwrong_predicted_image = [[],[]]\ncorrect_predicted_image = [[],[]]\ni = 0\nwhile i< 5 and len(wrong_predicted_image[0]) < 6:\n    j = 0\n    while j < 32 and len(wrong_predicted_image[0]) < 6:\n        \n        image_array = (test_gen[i][0][j]).reshape(1,224,224,3)\n        \n        prediction = Inception_model.predict(image_array)\n        \n        if int(round(prediction[0][0])) != test_gen[i][1][j]:\n            wrong_predicted_image[0].append(image_array)\n            wrong_predicted_image[1].append(int(round(prediction[0][0])))\n            \n        elif len(correct_predicted_image[0]) < 6:\n            correct_predicted_image[0].append(image_array)\n            correct_predicted_image[1].append(int(round(prediction[0][0])))\n#        print(len(correct_predicted_image[0]),len(wrong_predicted_image[0]))\n        j += 1\n        \n    i += 1","4a058e9e":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in wrong_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(224,224,3))\n    ax[i].imshow(image)\n    if wrong_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'wrong_prediction_by_model --- {wrong_predicted_image[1]}')","8e26a532":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 22 ,4\nfig, ax = plt.subplots(1,6)\n\ni = 0\nfor ele in correct_predicted_image[0]:\n    image = tf.keras.preprocessing.image.array_to_img(ele.reshape(224,224,3))\n    ax[i].imshow(image)\n    if correct_predicted_image[1][i] == 1:\n        ax[i].set_title(\"Classified as Pneumonia\")\n    else:\n        ax[i].set_title(\"Classified as Normal\")\n    i += 1\n\nprint(f'correct_prediction_by_model --- {correct_predicted_image[1]}')","7924c120":"#model.save('PNP.h5')","17587329":"# Deleting all the directories created in the below cell as it is only created to divide the dataset.\ntf.io.gfile.rmtree('\/kaggle\/working\/val_dataset\/')\ntf.io.gfile.rmtree('\/kaggle\/working\/train_dataset\/')","10605b44":"<a id=\"22\"><\/a>\n<font color=\"black\" size=+2.0><b>Visualise VGG-16 Model Performance  <\/b><\/font>","8a388664":"# <font color='darkblue'>  Convolutional Neural Network From Scratch <\/font>\n\n<a id=\"11\"><\/a>\n<font color=\"black\" size=+2.0><b>CNN Model: The Architecture<\/b><\/font>\n\nOur architecture for the CNN has been inspired by the article from Stephan and colleagues (2019), Yadav and Sjadav (2019), Ikechukwu and collegues (2021), and this [article](https:\/\/towardsdatascience.com\/deep-learning-for-detecting-pneumonia-from-x-ray-images-fc9a3d9fdba8). Their neural network architectures were specifically designed for pneumonia image classification tasks. The proposed architecture consists of the convolution, max-pooling, and classification layers combined together. We will now dive into each component and why we chose them.\n\n![https:\/\/www.mdpi.com\/applsci\/applsci-10-03233\/article_deploy\/html\/images\/applsci-10-03233-g001.png](https:\/\/www.mdpi.com\/applsci\/applsci-10-03233\/article_deploy\/html\/images\/applsci-10-03233-g001.png)\n\n\n\n\n#### Input Layer\nSince the architecture is like that of VGG Networks, the input image must be resized to the standard 224 \u00d7 224 pixels, this was needful because medical images like an X-ray, when taken from different devices come in various sizes. For efficient preprocessing, the images must be resized to 150 \u00d7 150 \u00d7 3 depicting the width, height, and channel numbers respectively. We chose 150 x 150 since that gave us the best performance of the model after fine tuning all the parameters. \n\n#### Convolutional Layer\nThis is the most important layer in our proposed CNN model, as it is where majority of the computations would be done. This layer's main job is to retrieve features from the image while keeping the spatial relationship between image pixels intact. This is accomplished by utilising a series of filters to learn the retrieved features. \nWe had 5 convolutional blocks with each Seperable convolutional layers. We used a filter size of 16, 32, 64, 128, 256 with a filter size of 3 * 3. In addition, the filters move along the input images, calculating the dot product function, also known as convolved features.\n\n#### Batch Normal Layer\nBatch normalisation is a technique for training very deep neural networks that standardises each mini-inputs batches to a layer. This stabilises the learning process and significantly reduces the number of training epochs needed to create deep networks. It is a transformation that keeps the mean output close to 0 and the standard deviation of the output close to 1. The idea is to alter the training data with small transformations to reproduce the variations, which makes learning more stable and quicker (Yadav & Jadhav, 2019).\n\n#### ReLU Layer\nThis layer is responsible for replacing all negative values to zero while allowing positive numbers to assume their respective values from the convolved features, thereby introduces non-linearity in the feature map.\n\n#### Fully Connected (FC) Layer\nAfter the convolution and pooling layers we add a couple of fully connected layers to wrap up the CNN architecture. All the activation functions from the preceding layer are related to the neurons in this layer. This layer's primary function in this study is to classify the returned convolved features from dataset images into their respective classes. \n\n#### Dropout Layer\nAfter the fully connected layers, we used dropout to prevent overfitting by temporarily \u201cdropping\u201d a neuron during training time at each iteration with probability p. So that all the inputs and outputs to this neuron will be disabled at the current iteration. The dropped-out neurons are resampled with probability p at every training step, so a dropped out neuron at one step can be active at the next one. The hyperparameter p is called the dropout-rate and we set it to 0.7 in the first dropout layer, 0.5 in the second, and 0.3 in the last corresponding to 70%, 50% and 30% of the neurons being dropped out which is proposed as the best option for X-ray image classification (yadav & Jadhav, 2019). It was found to be of high importance when it comes to classification of X-ray images (Stephan et al., 2019).\n\n#### Output Layer\nThe final layer, consisting of the two classes (Normal and Pneumonia) is presented at this layer.\n\n#### The pretrained models (VGG-16 and ResNet-50)\nVGG-16 and ResNet50: We conducted two experiments using pre-trained models as it is easier to fine-tune the parameters unlike a network trained from scratch.\n\n### Regularization \nIn order to prevent our model to overtrain we implement the following regularization measures: ReduceLROnPlateau and ModelCheckpoint which was found to be very effective for X-ray image classification (Singh, Kumar, Yadav, & Kaur, 2020).\n\n- We have adopted \"ReduceLROnPlateau\" as a Keras callback function to reduce the learning rate when the result stops improving. This function also helps the network to reduce the overfitting problem.\n- The checkpoint callback saves the best weights of the model, so next time we want to use the model, we do not have to spend time training it. \n\n### The Challenge\nBuild an algorithm to automatically identify whether a patient is suffering from pneumonia or not by looking at chest X-ray images and compare the performance with a pretrained VGG-16 and ResNet50 model. We want to achieve that the algorithms are extremely accurate because lives of people are at stake.\n\n\n** **\n- _Yadav, S. S., & Jadhav, S. M. (2019). Deep convolutional neural network based medical image classification for disease diagnosis. Journal of Big Data, 6(1). https:\/\/doi.org\/10.1186\/s40537-019-0276-2_\n- _Singh, D., Kumar, V., Yadav, V., & Kaur, M. (2020). Deep Neural Network-Based Screening Model for COVID-19-Infected Patients Using Chest X-Ray Images. International Journal of Pattern Recognition and Artificial Intelligence, 35(03), 2151004. https:\/\/doi.org\/10.1142\/s0218001421510046_\n- _Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949_\n- _Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027_\n","eacf3a0e":"### Plot images that were correctly predicted by our CNN Model\nHere we see 6 images that were correctly classified by our CNN model.","06a57145":"#### Training Data Distribution\nAgain you can see that there is a imbalance of the training data, which is very visible in this distribution. [This](https:\/\/www.kaggle.com\/georgiisirotenko\/pytorch-x-ray-transfer-learning-densenet) notebook is what we used for the distribution plot. \n\n","b1e326e0":"<a id=\"16\"><\/a>\n<font color=\"black\" size=+2.0><b>Visualise CNN Model Performance <\/b><\/font>\n\nWe used [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for vizualising all the Model performances","488aff67":"#### Plot images that were correctly predicted by The ResNet-50 model\nThese 6 images are an example of images that were correctly classified by the model","adca2687":"# <font color='darkblue'> Exploratory Data Analysis <\/font>\n\n<a id=\"7\"><\/a>\n<font color=\"black\" size=+2.0><b>Visualizing the Data<\/b><\/font>\n\nWe start with looking at some chest x-rays to get a better idea about the differences between lungs suffering from pneumonia and normal lungs. When interpreting the x-ray, the radiologist will look for white spots in the lungs (called infiltrates) that identify an infection. The edges of the lung especially close to the diaphragm won't be clearly visible when there is an infection present. The same goes for the edges around the hearth and the aorta. In a study where they used chest x-rays to determine Community acquired pneumonia, there was a diagnostic accuracy of 93.1% (BRON), this is a benchmark for our model.\n\nWe used [this](https:\/\/www.kaggle.com\/donpiano\/keras-resnet-50-for-pneumonia-x-ray-images) Notebook for the visualisation\n","0081de1f":"Below code block shows that the number of layers for base model (base_resnet) which is 175 and we chose to freeze all 175 layers while training. This means we will not be training those 175 freezed layers.\n","da3505af":"<a id=\"18\"><\/a>\n<font color=\"black\" size=+2.0><b>Prediction Plots  <\/b><\/font>\n\nCode below will be usefull for finding which predictions are either incorrect or correct by our CNN Model. ","36ed5478":"### Plot images that were correctly predicted by our CNN Model\nHere we can see 6 images that were correctly classified by the VGG16 model.","c1684e5b":"# <a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n\n<font color=\"darkblue\" size=+1><b>Introduction of the Problem<\/b><\/font>\n* [1. What is Peumonia? ](#1)\n* [2. The Importance of Diagnosing Pneumonia ](#2)\n* [3. How do doctors distinguish between healthy and unhealthy lungs? ](#3)\n* [4. Experts versus AI ](#4)    \n    \n<font color=\"darkblue\" size=+1><b>The Data<\/b><\/font>\n* [1. The X-Ray Images ](#5)\n* [2. Import Libraries and Data](#6)\n\n<font color=\"darkblue\" size=+1><b> Exploratory Data Analysis <\/b><\/font>\n* [1. Visualise the Data ](#7)\n* [2. Prepare the Data for Training ](#8)\n* [3. Create Directories ](#9)\n* [4. Custom Directory ](#10)\n    \n<font color=\"darkblue\" size=+1><b> Convolutional Neural Network From Scratch <\/b><\/font>\n* [1. CNN Model: The Architecture ](#11)\n* [2. Creating Data Generators ](#12)\n* [3. Correction for Data Imbalance ](#13)\n* [4. Finetune CNN Model ](#14)\n* [5. Train CNN Model ](#15)\n* [6. Visualise CNN Model Performance ](#16)\n* [7. Predict and Evaluate ](#17)\n* [8. Prediction Plots ](#18)\n\n<font color=\"darkblue\" size=+1><b> VGG-16 <\/b><\/font>\n* [1. VGG-16 Model: The Pretrained Model ](#19)\n* [2. Regularization ](#20)\n* [3. Train VGG-16 ](#21)\n* [4. Visualise VGG-16 Model Performance ](#22)\n* [5. Predict and Evaluate ](#23)\n* [6. Prediction Plots ](#24)\n\n<font color=\"darkblue\" size=+1><b> ResNet-50 <\/b><\/font>\n* [1. ResNet-50 Model: Pretrained Model ](#25)\n* [2. Regularization ](#26)\n* [3. Train VGG-16 ](#27)\n* [4. Visualise VGG-16 Model Performance ](#28)\n* [5. Predict and Evaluate ](#29)\n* [6. Prediction Plots ](#30)\n\n<font color=\"darkblue\" size=+1><b> Comparing the Models <\/b><\/font>\n* [1. Compare the results ](#31)\n* [2. Interpret the results ](#32)\n* [3. Conclusion ](#33)\n* [4. References ](#34)","6ac785af":"<a id=\"26\"><\/a>\n<font color=\"black\" size=+2.0><b>Regularization  <\/b><\/font>","a40b8200":"<a id=\"6\"><\/a>\n<font color=\"black\" size=+2.0><b>Import Libraries and Data<\/b><\/font>\n\nWe prepare the data which we use in our Convolutional Neural Network. The Chest X-ray data is given into three saperate folders: train, val, and test. Run following cell to set dataset path and other few variables which are used by ImageDataGenerator in next step.\n\n\n#### Our data is located in three folders\n   \n1. **Train:** the folder that contains the training images for training our model.\n2. **Val:** the folder that contains images which we will use to validate our model. A validation dataset is has its purpose to prevent our model from **Overfitting**. Overfitting is when the loss is not as low as it could be because the model learned too much noise. Therefore it can't handle data it hasn't see too well.\n3. **Test:** this folder contains the data that we use to test the model once it has learned the relationships between the images and their label (Pneumonia versus Not-Pneumonia)\n","0ec4e1f5":"<a id=\"28\"><\/a>\n<font color=\"black\" size=+2.0><b>Visualise ResNet-50 Model Performance  <\/b><\/font>","cc6775b9":"- **Accuracy:** The validation accuracy is above the training accuracy which indicates that the behaviour when training and testing are different, which could be because of the Dropout. When we trained our network, a percentage of the features are set to zero (20%) since we used a Dropout of 0.2. But then when we test the model, all the features that we used were rescaled appropriately. Thus the model at test time is more robust and can lead to higher testing accuracies. This could explain the why the validation accuracy is higher than the training accuracy.\n- **Loss:** The validation loss seems to overall follow the train loss quite well. Nevertheless, the validation loss is somewhat higher than the training loss so it seems as if the validation loss is slightly overfitting the training loss. Since the plot of training loss continues to decrease with epochs and the plot of validation loss decreases to a point that its under the training loss (in the beginnning around the first 5 epochs) and begins increasing again (around the 8th epoch) where is goes above the training loss, which indicates an overfit model. \n- We would thus add more data from other sources in order to prevent the overfitting and it failing to generalize to new data. In order to reduce overfitting we would maybe removing some features and making it simpler. But we could also use different data agumentations and more dropout layers. ","fb5617d5":"<a id=\"20\"><\/a>\n<font color=\"black\" size=+2.0><b>Regularization  <\/b><\/font>","c0a8d8ad":"<a id=\"9\"><\/a>\n<font color=\"black\" size=+2.0><b>Create Directories<\/b><\/font>\n\n- Creating directory in Kaggle\/working directory for training dataset and validation dataset after division of 80:20.\n- In both directory there is more two directory i.e. negative and positive\n- The directory structure is same as the structure in Input directory.","8d8a26d7":"### Plot images that were correctly predicted by The ResNet-50 model\nHere we can see 6 images that were incorrectly classified by the VGG16 model. A difference we can see with wrongfully classified images from our model is that these also contain images classified as pneumonia whilst being normal.","86032f76":"<a id=\"34\"><\/a>\n<font color=\"black\" size=+2.0><b>References  <\/b><\/font>\n\n- Anwar, A. (2019, June 17th).  Difference between AlexNet, VGGNet, ResNet, and Inception. Consulted on 24th of december 2021. van https:\/\/towardsdatascience.com\/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96\n- Afshar, P., Mohammadi, A., & Plataniotis, K. N. (2018). Brain Tumor Type Classification via Capsule Networks. 2018 25th IEEE International Conference on Image Processing (ICIP). https:\/\/doi.org\/10.1109\/icip.2018.8451379\n- Daniel P., Bewick, T., Welham, S., Mckeever, T. M., & Lim, W. S. (2017). Adults miscoded and misdiagnosed as having pneumonia: results from the British Thoracic Society pneumonia audit. Thorax, 72(4), 376\u2013379. https:\/\/doi.org\/10.1136\/thoraxjnl-2016-209405\n- Fourcade, A., & Khonsari, R. (2019). Deep learning in medical image analysis: A third eye for doctors. Journal of Stomatology, Oral and Maxillofacial Surgery, 120(4), 279\u2013288. https:\/\/doi.org\/10.1016\/j.jormas.2019.06.002\n- Fox, M. J. (2016). Subtyping Parkinson\u2019s Disease with Deep Learning Models (2016 PPMI Data Challenge Winner). The Michael J. Fox Foundation for Parkinson\u2019s Research | Parkinson\u2019s Disease. Geraadpleegd op 29 november 2021, van https:\/\/www.michaeljfox.org\/grant\/subtyping-parkinsons-disease-deep-learning-models-2016-ppmi-data-challenge-winner?grant_id=1518\n- He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https:\/\/doi.org\/10.1109\/cvpr.2016.90\n- Jyotiyana, M., & Kesswani, N. (2019). Deep Learning and the Future of Biomedical Image Analysis. Studies in Big Data, 329\u2013345. https:\/\/doi.org\/10.1007\/978-3-030-33966-1_15\n- Liu, T., Tsang, W., Huang, F., Lau, O. Y., Chen, Y., Sheng, J., Guo, Y., Akinwunmi, B., Zhang, C. J., & Ming, W. K. (2021). Patients\u2019 Preferences for Artificial Intelligence Applications Versus Clinicians in Disease Diagnosis During the SARS-CoV-2 Pandemic in China: Discrete Choice Experiment. Journal of Medical Internet Research, 23(2), e22841. https:\/\/doi.org\/10.2196\/22841\n- Liu, X., Faes, L., Kale, A. U., Wagner, S. K., Fu, D. J., Bruynseels, A., Mahendiran, T., Moraes, G., Shamdas, M., Kern, C., Ledsam, J. R., Schmid, M. K., Balaskas, K., Topol, E. J., Bachmann, L. M., Keane, P. A., & Denniston, A. K. (2019). A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. The Lancet Digital Health, 1(6), e271\u2013e297. https:\/\/doi.org\/10.1016\/s2589-7500(19)30123-2\n- Nagendran, M., Chen, Y., Lovejoy, C. A., Gordon, A. C., Komorowski, M., Harvey, H., Topol, E. J., Ioannidis, J. P. A., Collins, G. S., & Maruthappu, M. (2020a). Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies. BMJ, m689. https:\/\/doi.org\/10.1136\/bmj.m689\n- Miotto, R., Wang, F., Wang, S., Jiang, X., & Dudley, J. T. (2017). Deep learning for healthcare: review, opportunities and challenges. Briefings in Bioinformatics, 19(6), 1236\u20131246. https:\/\/doi.org\/10.1093\/bib\/bbx044\n- Pneumonia. (2021, 11 november). World Health Organisation. Geraadpleegd op 13 december 2021, van https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/pneumonia\n- Scott, J. A. G., Wonodi, C., Mo\u00efsi, J. C., Deloria-Knoll, M., DeLuca, A. N., Karron, R. A., Bhat, N., Murdoch, D. R., Crawley, J., Levine, O. S., O\u2019Brien, K. L., & Feikin, D. R. (2012). The Definition of Pneumonia, the Assessment of Severity, and Clinical Standardization in the Pneumonia Etiology Research for Child Health Study. Clinical Infectious Diseases, 54(suppl_2), S109\u2013S116. https:\/\/doi.org\/10.1093\/cid\/cir1065\n- Shen, J., Zhang, C. J. P., Jiang, B., Chen, J., Song, J., Liu, Z., He, Z., Wong, S. Y., Fang, P. H., & Ming, W. K. (2019). Artificial Intelligence Versus Clinicians in Disease Diagnosis: Systematic Review. JMIR Medical Informatics, 7(3), e10010. https:\/\/doi.org\/10.2196\/10010\n- Singh, D., Kumar, V., Yadav, V., & Kaur, M. (2020). Deep Neural Network-Based Screening Model for COVID-19-Infected Patients Using Chest X-Ray Images. International Journal of Pattern Recognition and Artificial Intelligence, 35(03), 2151004. https:\/\/doi.org\/10.1142\/s0218001421510046\n- Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949\n- Toraman, S., Alakus, T. B., & Turkoglu, I. (2020). Convolutional capsnet: A novel artificial neural network approach to detect COVID-19 disease from X-ray images using capsule networks. Chaos, Solitons & Fractals, 140, 110122. https:\/\/doi.org\/10.1016\/j.chaos.2020.110122\n- Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027\n- Yadav, S. S., & Jadhav, S. M. (2019). Deep convolutional neural network based medical image classification for disease diagnosis. Journal of Big Data, 6(1). https:\/\/doi.org\/10.1186\/s40537-019-0276-2\n\n#### NOTEBOOKS\n- *Vizualizing the Data:* [this notebook](https:\/\/www.kaggle.com\/donpiano\/keras-resnet-50-for-pneumonia-x-ray-images)\n- *Preparing the Data:* [this notebook](https:\/\/www.kaggle.com\/abhishekdhule\/pneumonia-detection-resnet-inception-tensorflow?scriptVersionId=40311825) and [this notebook](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia)\n- *CNN:* [this notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays) and [this](https:\/\/github.com\/abhinavsagar\/kaggle-notebooks\/blob\/master\/Chest_X-Ray.ipynb) github file\n- *Correction for Data Imbalance:* [this notebook](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays) and [this notebook](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia)\n- *Regularization:* [this notebook](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia)\n- *Vizualising the performance for all models*: [this notebook](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia)\n- *Prediction plots:* [this notebook](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia)\n- *VGG-16:* [this notebook](https:\/\/www.kaggle.com\/vnbhat\/pneumonia-detection-resnet50-vgg16) and [this notebook](https:\/\/www.kaggle.com\/abhishekdhule\/pneumonia-detection-resnet-inception-tensorflow?scriptVersionId=40311825)\n- *ResNet-50:* [this notebook](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) and [this notebook](https:\/\/www.kaggle.com\/vnbhat\/pneumonia-detection-resnet50-vgg16) ","4af804b8":"### Plot images that were incorrectly predicted by our CNN Model\nHere we can see 6 images that were incorrectly classified by the VGG16 model. A difference we can see with wrongfully classified images from our model is that these also contain images classified as pneumonia whilst being normal.","5210df9d":"<a id=\"10\"><\/a>\n<font color=\"black\" size=+2.0><b>Custom Directory<\/b><\/font>\n\nIn the cell below, we will copy all the files from input directory -> custom directory\n- From the train_images list, transferring all the files to train_dataset directory.\n- From the val_images list, transferring all the files to val_dataset directory","fa6df126":"# <font color='darkblue'> Introduction of the problem <\/font> \n\n<a id=\"1\"><\/a>\n<font color=\"black\" size=+2><b>What is Pneumonia?<\/b><\/font>\n\nPneumonia is a form of acute respiratory infection that affects the lungs. The lungs are made up of small sacs called alveoli, which fill with air when a healthy person breathes. When an individual has pneumonia, the alveoli are filled with pus and fluid, which makes breathing painful and limits oxygen intake.\n\nPneumonia is the single largest infectious cause of death in children worldwide. Pneumonia killed 740 180 children under the age of 5 in 2019, accounting for 14% of all deaths of children under five years old but 22% of all deaths in children aged 1 to 5. Pneumonia affects children and families everywhere, but deaths are highest in South Asia and sub-Saharan Africa (World Health Organisation, 2021).\n\n\n![https:\/\/www.svhlunghealth.com.au\/Images\/UserUploadedImages\/3447\/4_SVH_Lung_Health_Pneumonia_final_1080p.jpg](https:\/\/www.svhlunghealth.com.au\/Images\/UserUploadedImages\/3447\/4_SVH_Lung_Health_Pneumonia_final_1080p.jpg)\n\n\n<a id=\"2\"><\/a>\n<font color=\"black\" size=+2><b>The Importance of Diagnosing Pneumonia<\/b><\/font>\n\nThe risk of pneumonia is immense for many, especially in developing nations where billions face energy poverty and rely on polluting forms of energy. Over 150 million people get infected with pneumonia on an annual basis especially children under 5\u2009years old. In such regions, the problem can be further aggravated due to the dearth of medical resources and personnel. For example, in Africa\u2019s 57 nations, a gap of 2.3 million doctors and nurses exists. For these populations, accurate and fast diagnosis means everything. It can guarantee timely access to treatment and save much needed time and money for those already experiencing poverty (Stephen, Sain, Maduh, & Jeong, 2019).\n\n\n****\n- _World Health Organisation. (2021, 11th of November). Pneumonia. Consulted on 13th of December 2021, van https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/pneumonia_\n- _Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949_\n\n","cf3ba2bb":"- **Accuracy:** The validation accuracy pattern seems to overall follow the training accuracy line. In the beginning around the first five epochs there is a bit of a gap between the validation accuracy and training accuracy, where the validation accuracy is below the training accuracy. But since the validation accuracy then starts to follow the training accuracy around the 5th, which actually indicates a fairly good fit. Although the validation accuracy being a bit below that of the training accuracy (usually indicating overfitting), this is very minor so this is a good fit. \n- **Loss:** The validation loss pattern before the 5th epoch does not follow the training loss pattern and thus doesn't follow the data very well, but after the 5th epoch the validation loss pattern does seem to follow the data very well since it is very close to the training loss pattern. So the validation loss decreases to a certain point of stability, has a fairly small\/no gap with the training loss, and the training loss also decreases to a certain point of stability which both indicate a good fit. ","79c92b65":"# <font color='darkblue'>  Comparing the Models  <\/font>\n\n<a id=\"31\"><\/a>\n<font color=\"black\" size=+2.0><b>Compare the Results  <\/b><\/font>\n\n* Since this is a medical diagnosis case: The accuracy cannot be the only metric to evaluate the models.\n* In medical daignosis it is highly important to correctly predict the true values.\n* We cannot incorrectly diagnose a patient as having healthy lungs (normal x-ray) after the true report of diagnosis shows that patient has pneumonia (High sensitivity: True Positives).\n* So along with the highest accuracy we also need the highest recall.\n\n### Epoch trials\nWe tried different epoch sizes and saw that with 50 epochs the performance was best for the accuracy and loss, but especially the recall which is sensitivity and highly important in the medical field. Therefore we chose to go for 50 epochs and will report our results of it below.\n\n\n![Schermafbeelding 2021-12-26 om 11.34.18.png](attachment:5b760467-b713-47b4-aadb-6bedb593e556.png)\n","1d9bf726":"<a id=\"12\"><\/a>\n<font color=\"black\" size=+2.0><b>Creating Data Generators <\/b><\/font>\n\nThe dataset is highly imbalance with more of pneumonia cases versus normal cases, hence data augmentation was used to balance the dataset, thereby eliminated the possibility of overfitting the model. We used [this](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666285X21000558) research for our Data Augmentation","29a48fdb":"<a id=\"23\"><\/a>\n<font color=\"black\" size=+2.0><b>Predict and Evaluate  <\/b><\/font>\n\nWe predicted the performance of the model by prdicting on the evaluation data of the kaggle dataset. ","f42081e7":"#### Here we compile our model","f588c0f0":"<a id=\"3\"><\/a>\n<font color=\"black\" size=+2><b>How do doctors distinguish between healthy and unhealthy lungs?<\/b><\/font>\n\n__Figure 1.__ shows examples of chest X-rays from the dataset. The normal chest X-ray (left panel) depicts clear lungs without any areas of abnormal opacifcation in the image. Bacterial pneumonia (middle) typically exhibits a focal lobar consolidation, in the right upper lobe, whereas viral pneumonia (right) manifests with a more difuse interstitial pattern in both lungs (Kermany et al., 2018).\n\n\n\n![https:\/\/miro.medium.com\/max\/4800\/1*t-_EXQ3tlb8KOx6H7HN09A.jpeg](https:\/\/miro.medium.com\/max\/4800\/1*t-_EXQ3tlb8KOx6H7HN09A.jpeg)\n**Figure 1**\n\n\n\n** **\n_Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \u201cLabeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\u201d, Mendeley Data, V2, doi: 10.17632\/rscbjbr9sj.2_","a5cca67b":"<a id=\"30\"><\/a>\n<font color=\"black\" size=+2.0><b>Prediction Plots  <\/b><\/font>\n\n#### Code for plotting the incorrect and correct prediction images","265ae3a1":"<a id=\"4\"><\/a>\n<font color=\"black\" size=+2><b>Experts versus AI<\/b><\/font>\n\nDespite the fact that pneumonia is the most common cause of serious illness and death in young children worldwide, our ability, as clinicians, to infer an infectious pathological process in the lung from specific features of the history and examination is poor (Scott et al., 2012).\n\nMisdiagnosis, arbitrary charges, annoying queues, and clinic waiting times among others are long-standing phenomena in the medical industry across the world. These factors can contribute to patient anxiety about misdiagnosis by clinicians. However, with the increasing growth in use of big data in biomedical and health care communities, the performance of artificial intelligence (Al) techniques of diagnosis is improving and can help avoid medical practice errors (Daniel et al., 2017).\n\nSome research on medical image classification by CNN has achieved performances rivaling human experts. Deep learning can be used to leverage clinician activities in different domains and applications, such as disease risk prediction, personalized prescriptions, treatment recommendations, clinical trial recruitment as well as research and data analysis. As an example, Wang et al. recently won the Parkinson\u2019s Progression Marker\u2019s Initiative data challenge on subtyping Parkinson\u2019s disease using a temporal deep learning approach (Fox, 2016). Another example, CheXNet, a CNN with 121 layers trained on a dataset with more than 100,000 frontal-view chest X-rays (ChestX-ray 14), achieved a better performance than the average performance of four radiologists (Shen et al., 2019).\n\nMost of the experts got fairly high sensitivity but low specificity, while the CNN-based system got high values on both sensitivity and specificity (Nagendran et al., 20120). Moreover, on the average weight error measure, the CNN-based system exceeds two human experts (Liu et al., 2019).\n\nThe development of diverse AI techniques has contributed to early detections, disease diagnoses, and referral management. In addition, more than half of a randomized population group (55.8%: 428 out of 767) opted for AI diagnosis regardless of the description of the clinicians (Liu et al., 2021). \n\n** **\n- _Daniel, P., Bewick, T., Welham, S., Mckeever, T. M., & Lim, W. S. (2017). Adults miscoded and misdiagnosed as having pneumonia: results from the British Thoracic Society pneumonia audit. Thorax, 72(4), 376\u2013379. https:\/\/doi.org\/10.1136\/thoraxjnl-2016-209405_\n- _Jyotiyana, M., & Kesswani, N. (2019). Deep Learning and the Future of Biomedical Image Analysis. Studies in Big Data, 329\u2013345. https:\/\/doi.org\/10.1007\/978-3-030-33966-1_15_\n- _Liu, X., Faes, L., Kale, A. U., Wagner, S. K., Fu, D. J., Bruynseels, A., Mahendiran, T., Moraes, G., Shamdas, M., Kern, C., Ledsam, J. R., Schmid, M. K., Balaskas, K., Topol, E. J., Bachmann, L. M., Keane, P. A., & Denniston, A. K. (2019). A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. The Lancet Digital Health, 1(6), e271\u2013e297. https:\/\/doi.org\/10.1016\/s2589-7500(19)30123-2_\n- _Liu, T., Tsang, W., Huang, F., Lau, O. Y., Chen, Y., Sheng, J., Guo, Y., Akinwunmi, B., Zhang, C. J., & Ming, W. K. (2021). Patients\u2019 Preferences for Artificial Intelligence Applications Versus Clinicians in Disease Diagnosis During the SARS-CoV-2 Pandemic in China: Discrete Choice Experiment. Journal of Medical Internet Research, 23(2), e22841. https:\/\/doi.org\/10.2196\/22841_\n- _Nagendran, M., Chen, Y., Lovejoy, C. A., Gordon, A. C., Komorowski, M., Harvey, H., Topol, E. J., Ioannidis, J. P. A., Collins, G. S., & Maruthappu, M. (2020). Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies. BMJ, m689. https:\/\/doi.org\/10.1136\/bmj.m689_\n- _Scott, J. A. G., Wonodi, C., Mo\u00efsi, J. C., Deloria-Knoll, M., DeLuca, A. N., Karron, R. A., Bhat, N., Murdoch, D. R., Crawley, J., Levine, O. S., O\u2019Brien, K. L., & Feikin, D. R. (2012). The Definition of Pneumonia, the Assessment of Severity, and Clinical Standardization in the Pneumonia Etiology Research for Child Health Study. Clinical Infectious Diseases, 54(suppl_2), S109\u2013S116. https:\/\/doi.org\/10.1093\/cid\/cir1065_\n- _Shen, J., Zhang, C. J. P., Jiang, B., Chen, J., Song, J., Liu, Z., He, Z., Wong, S. Y., Fang, P. H., & Ming, W. K. (2019). Artificial Intelligence Versus Clinicians in Disease Diagnosis: Systematic Review. JMIR Medical Informatics, 7(3), e10010. https:\/\/doi.org\/10.2196\/10010_\n- _Fox, M. J. (2016). Subtyping Parkinson\u2019s Disease with Deep Learning Models (2016 PPMI Data Challenge Winner). The Michael J. Fox Foundation for Parkinson\u2019s Research | Parkinson\u2019s Disease. Geraadpleegd op 29 november 2021, van https:\/\/www.michaeljfox.org\/grant\/subtyping-parkinsons-disease-deep-learning-models-2016-ppmi-data-challenge-winner?grant_id=1518_","cf932a65":"# <font color='darkblue'> The Data <\/font>\n\n<a id=\"5\"><\/a>\n<font color=\"black\" size=+2><b>The X-Ray Images<\/b><\/font>\n\nA total of 5,856 X-ray images of anterior-posterior chests were carefully chosen from retrospective pediatric patients between 1 and 5\u2009years old. The dataset contains two kinds of chest X-ray Images: NORMAL and PNEUMONIA, which are stored in three folders. In the PNEUMONIA folder, two types of specifc PNEUMONIA can be recognized by the fle name: BACTERIA and VIRUS.\n\n__Table 1.__ describes the composition of the dataset. The training dataset contains 5232 X-ray images, while the testing dataset contains 624 images. In the training dataset, the image in the NORMAL class only occupies one-fourth of all data. In the testing dataset, the PNEUMONIA consists of 62.5% of all data, which means the accuracy of the testing data should be higher than 62.5%.\n\n\n** **\n- _Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \u201cLabeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\u201d, Mendeley Data, V2, doi: 10.17632\/rscbjbr9sj.2_","05626f1e":"The difference with the pictures that were incorrectly predicted is that when we take a look at the correctly predicted images; they look more alike as compared to the incorrectly predicted x-rays. Here the ribs all seem to be in the middle of the frame, no big differences in white shapes and black background. ","334d2d60":"### Plot images that were incorrectly predicted by our CNN Model\nHere we see 6 images that were incorrectly classified by our CNN model. We see that all examples were classified as pneumonia when they were actually normal x-rays. \n\nWe used [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for plotting the images","f8d46f85":"<a id=\"15\"><\/a>\n<font color=\"black\" size=+2.0><b>Train CNN Model <\/b><\/font>\n\nNow is the time to train out model!","6169a414":"<a id=\"24\"><\/a>\n<font color=\"black\" size=+2.0><b>Prediction Plots  <\/b><\/font>\n\nCode below will help with plotting the incorrect versus correct prediction of the VGG model.","880de88b":"<a id=\"21\"><\/a>\n<font color=\"black\" size=+2.0><b>Train VGG-16  <\/b><\/font>\n\nWe trained the layers we added by freezing the base 19 layers of the VGG-16. ","9419bdad":"# <font color='darkblue'>  VGG-16 Model  <\/font>\n\n<a id=\"19\"><\/a>\n<font color=\"black\" size=+2.0><b>VGG-16 Model: Pretrained Model  <\/b><\/font>\n\nVGG models are a type of CNN Architecture proposed by Karen Simonyan & Andrew Zisserman of Visual Geometry Group (VGG), Oxford University, which brought remarkable results for the ImageNet Challenge. It was proposed on 2014 by Simonyan and Zisserman VGG (Visual Geometry Group) is a convolution neural net (CNN) architecture and used to win ILSVR (ImageNet) competition in 2014 (K. Simonyan and A. Zisserman, 2014). VGG16 takes 224 x 244, since it was originally trained on 224\u00d7224 images. \n\n![https:\/\/www.researchgate.net\/profile\/Max-Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png](https:\/\/www.researchgate.net\/profile\/Max-Ferguson\/publication\/322512435\/figure\/fig3\/AS:697390994567179@1543282378794\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)\n\nAll parameters below we found by experimenting many times while fine-tuning the model. i.e for Dense layers, it was trained with different no. of neurons (4096, 1024, 512, 256) but the best result found by taking 512 neurons for Dense layers and using Dropout 0.2 gave us the best result as compared to 0.3, 0.4 or 0.5 during the fine tuning. As well as changing the hyperparameters to 224 x 224 with a batch size of 64 as compared to 150 x 150 . and a batch size of 32. This makes sense since the VGG-16 as well as the ResNet-50 are trained on 224 x 224 images from the  \n\nWe used inspiration from [this](https:\/\/www.kaggle.com\/vnbhat\/pneumonia-detection-resnet50-vgg16) Notebook.\n\n** **\n- _Simonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition. CoRR, abs\/1409.1556, 2014._","7f963a96":"<a id=\"33\"><\/a>\n<font color=\"black\" size=+2.0><b>Conclusion  <\/b><\/font>\n\nWe compared the performance of two pre-trained models (VGG-16 and ResNet-50) against a deep neural network we trained from scratch (CNN) by fine-tuning appropriate layers, regularizations and hyperparameters. Our examinations showed that our CNN model is able to generalize best to unseen data, a positive and big step in building a robust computer aided diagnostic tool.\n\n- Our proposed CNN approach works effectively on both the normal and pneumonia X-ray images utilized in our study. It is quite interesting to achieve 91.5% accuracy, 95.6% recall (sensitivity) and 91.1 % precision using our approach. \n\n- On the other hand, the pre-trained VGG-16 performs fairly good on X-ray images outperforming the ResNet-50, correctly identifying all pneumonia images with 92.6% accuracy and 96.1% recall of typical X-ray images, however the model fit was worse than the model fit of the CNN.\n\n- The pre-trained ResNet-50 performs less good on X-ray images, correctly identifying all pneumonia images with 80.4% accuracy and 91.1% recall of typical X-ray images, where the model fit was worse than the model fit of the CNN as well.\n\nWhen the performance of health-care professionals (4 radiologists) of predicting X-ray images as pneumonia or healthy was measured, the professionals scored a pooled sensitivity of 86\u00b74%. The performance of a convolutional neural network was also measured, and the CNN scored a specificity of 90\u00b75% which had beated the 4 radiologists when it comes to sensitivity (Liu et al., 2019). In our study the sensitivity was even higher: 95.6% for our CNN model. And a sensitivity of 96.1% for the VGG-16 and a sensitivity of 80.4% for the ResNet-50. That is a very interesting result. This suggests that these CNN models can very much still improve and are of high value to the medical field. \n\nDeep Learning (DL) technologies have proven to save time and money by tackling many tough tasks simultaneously (Jyotiyana,  & Kesswani, 2019). A radiologist in the Netherlands earns around 130.000 euro's a year, which goes up annually. We count 890 [radiologists](https:\/\/www.zorgkaartnederland.nl\/radioloog) in the Netherlands today, which amounts to 115.7 mln euro's each year. If CNN models can be integrated, and are accepted to make predictions, every radiologist that would focus less time on classifying pneumonia versus healthy lungs would save a lot of time and thus money which could be focused on something else. \n\nTherefore we think that deep learning can open the way toward the next generation of predictive health care systems. For this potential to be realized, statistical and medical tasks must be integrated at all levels, including study design, experiment planning, model building and refinement and data interpretation (Miotto et al., 2017). The greatest challenge to AI in these healthcare domains is not whether the technologies will be capable enough to be useful, but rather ensuring their adoption in daily clinical practice, which is very much needed (Lui et al., 2019).\u00a0\nMore than half of a randomized population group opted for AI diagnosis (Liu et al., 2021), so we conclude that CNN and other deep learning networks (like VGG-16 and the ResNet-50) could be of high importance in the medical field by saving a lot of money, time, resources and hopefully and most importantly save a lot of lives! \n\n** **\n- _Miotto, R., Wang, F., Wang, S., Jiang, X., & Dudley, J. T. (2017). Deep learning for healthcare: review, opportunities and challenges. Briefings in Bioinformatics, 19(6), 1236\u20131246. https:\/\/doi.org\/10.1093\/bib\/bbx044_","6cf2503d":"# <font color='darkblue'>  ResNet-50 Model  <\/font>\n\n<a id=\"25\"><\/a>\n<font color=\"black\" size=+2.0><b>ResNet-50 Model: Pretrained Model  <\/b><\/font>\n\nResnet50 is a deep residual network developed by (He at al., 2016) and is a subclass of convolutional neural networks used for image classification. It is the winner of ILSVRC 2015. The principal innovation is the introducing of the new architecture network-in-network using residual layers. The Renset50 consists of five steps each with a convolution and Identity block and each convolution block has 3 convolution layers and each identity block also has 3 convolution layers. Resnet50 has 50 residual networks, which is visible in the picture below with the black arrow's, and it accepts images of 224 \u00d7 224 pixels. We'll use a pre-trained model provided by keras and add some layers on the top.\n\n\n\n![https:\/\/miro.medium.com\/max\/1024\/1*BnoNVpj7uCNMOFOj1DQBQA.png](https:\/\/miro.medium.com\/max\/1024\/1*BnoNVpj7uCNMOFOj1DQBQA.png)\n\n\n\n\nWe again fine-tuned the ResNet-50, i.e., for Dense layers we again found 512 to be having the best result, and a Dropout of 0.2 also had the best result as compared to 0.5, 0.4, 0.3. \n\nWe used the article of (Victor, Murali, Deepu & Shivamurthy, 2021) as inspiration for the ResNet-50 model and [this](https:\/\/www.kaggle.com\/abhishekdhule\/pneumonia-detection-resnet-inception-tensorflow?scriptVersionId=40311825) notebook.\n  \n** **\n- _He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https:\/\/doi.org\/10.1109\/cvpr.2016.90_\n- _Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027_","ebce1a54":"Below code block shows that the number of layers for base model (base_VGG) which is 19 and out of 19 we chose to freeze all 19 layers while training. This means we will not be training those 19 freezed layers.","fddf13b0":"### CNN\nHere we create the structure of our model, we use the following layers: based on [this](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2666285X21000558) article and [this](https:\/\/github.com\/abhinavsagar\/kaggle-notebooks\/blob\/master\/Chest_X-Ray.ipynb) github file.","d7a256dd":"<a id=\"32\"><\/a>\n<font color=\"black\" size=+2.0><b>Interpret the Results  <\/b><\/font>\n\n\nNow we can reflect on the differences and similarities of our two pre-trained models. Theoretically the two models distinguish in the fact that ResNet-50 applies shortcut connections in order to try to solve the notorious problem of networks not being able to find simple mapping if it exists (vanishing gradient problem) (Anwar, 2019). VGG-16 arose of the idea that we needed to deal with the huge amount of parameters, this was done by fixing kernel sizes (Anwar, 2019). Both have proved to be status quo of the pretrained bases nowadays, and rightfully so. \n\nThe accuracy of the CNN, VGG-16 and ResNet-50 are respectively 91.5%, 92.6%, and 80.4%. The recall of the CNN, VGG-16 and ResNet-50 are respectively 95.6%, 96.1% and 91.1%. The loss of the CNN, VGG-16 and ResNet-50 are respectively 0.251, 0.218 and 0.512. We can see that the CNN and the VGG-16 have the highest accuracy, recall, and the lowest loss. Eventhough the VGG-16 has the highest accuracy and recall, and also the lowest loss, the vizualisation of these results show that the VGG-16 was very much overfitting whereas the CNN model had a good fit. \n\nSo therefore our CNN model outperforms the two pretrained models. This is not very surprising since we did a lot of finetuning on the CNN (for the layers, regularization, hyperparameters). Though when looking at the results between the two pretrained models we clearly see that VGG-16 outperforms ResNet-50. On accuracy, recall, and loss the VGG-16 performs outstanding whereas ResNet-50 slacks a little. Why is this the case? More hyperparameter tuning could solve this possibly, though it\u2019s no excuse considering that the used hyperparameters were also used in other literature such as from Ikechukwu and colleagues (2021). Possibly it could have to do with the fundamental differences of the pretrained basis, but this is something we could further explore in future projects.\n\n** **\n- _Anwar, A. (2019, June 17th).  Difference between AlexNet, VGGNet, ResNet, and Inception. Consulted on 24th of december 2021. van https:\/\/towardsdatascience.com\/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96_","11059b64":"We surely are no doctors or radiologists, but it does seem as if these pictures are more vaguely or that the person moved while the picture was taken. The first, third and fourth picture for instance have extra white shapes or waves at the top of the picture, which could indicate movement. In addition, these images are all somewhat different. Some pictures are more vague, some pictures only contain the torso zoomed in on the lungs, whereas other pictures also contain a bit of the arms and shoulders. So it could be very interesting if doctors and radiologists or other specialists would look into these wrongly predicted images and see if there are any typifications that are more reliably constructed than our assumptions with no medical knowledge.  ","c1f65b97":"#### Setting Hyperparameters","6ae97395":"#### This summary is a great way for us to see how our CNN is being set up","3b3a663a":"**_The dataset is highly imbalance with more of pneumonia cases versus normal cases, hence data augmentation was used to balance the dataset, thereby eliminated the possibility of overfitting the model._**","36eb8468":"<a id=\"27\"><\/a>\n<font color=\"black\" size=+2.0><b>Train VGG-16  <\/b><\/font>","4fe01b3a":"<a id=\"29\"><\/a>\n<font color=\"black\" size=+2.0><b>Predict and Evaluate  <\/b><\/font>\n\nWe predicted the performance of the model by prdicting on the evaluation data of the kaggle dataset. ","5d93651c":"<a id=\"17\"><\/a>\n<font color=\"black\" size=+2.0><b>Predict and Evaluate  <\/b><\/font>\n\nWe predicted the performance of the model by prdicting on the evaluation data of the kaggle dataset. ","a1001b5f":"<a id=\"13\"><\/a>\n<font color=\"black\" size=+2.0><b>Correction for Data Imbalance <\/b><\/font>\n\nWe used [this](https:\/\/www.kaggle.com\/amyjang\/tensorflow-pneumonia-classification-on-x-rays) and [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for the correction for data imbalance.","2ffbff6d":"<a id=\"14\"><\/a>\n<font color=\"black\" size=+2.0><b>Finetune CNN model <\/b><\/font>\n\nTo further finetune our CNN model, we used keras callbacks for further finetuning of our models. We used ModelCheckpoint to save the best weights of our CNN in order to plot the correctly and incorrectly predicted images which will be later in this Notebook, and so that we don't have to train the model again. This will save us time in the future.\n\nWe did not see a reason to use early stoppings. We did in the beginning, but this had a large influence on the accuracy since it stopped around 8 epochs. In the article from Stepehn and colleagues (2019) and Ikechukwu and colleagues (2021) they also did not use it, so therefore we decided to not use it in the end. \n\n#### Regularization\nIn order to prevent our model to overtrain we implement the following regularization measures. We used the ModelCheckpoint in order to plot the correctly and incorrectly predicted images later in this Notebook. We used [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for this.\n\n\n** **\n- _Stephen, O., Sain, M., Maduh, U. J., & Jeong, D. U. (2019). An Efficient Deep Learning Approach to Pneumonia Classification in Healthcare. Journal of Healthcare Engineering, 2019, 1\u20137. https:\/\/doi.org\/10.1155\/2019\/4180949_\n- _Victor Ikechukwu, A., Murali, S., Deepu, R., & Shivamurthy, R. (2021). ResNet-50 vs VGG-19 vs training from scratch: A comparative analysis of the segmentation and classification of Pneumonia from chest X-ray images. Global Transitions Proceedings, 2(2), 375\u2013381. https:\/\/doi.org\/10.1016\/j.gltp.2021.08.027_","dcca6203":"<a id=\"8\"><\/a>\n<font color=\"black\" size=+2.0><b>Preparing the Data for Training<\/b><\/font>\n\nThe given dataset has training, test and validation\/evaluation set, but test set only has 16 images whereas training set has 5216 images. To generate test samples we will do the following:  \n* So first we need to create a proper distribution set with 80% as training data and 20% as validation data.\n* So we'll merge training and validation set and then split them in the ratio of 80:20 repectively.\n\nWe used inspiration form [this](https:\/\/www.kaggle.com\/abhishekdhule\/pneumonia-detection-resnet-inception-tensorflow?scriptVersionId=40311825) and [this](https:\/\/www.kaggle.com\/csk03012\/vgg-91-inceptionnet-90-to-classify-pneumonia) notebook for the Preparing the data.\n","1eeb2df1":"- **Accuracy:** The validation accuracy pattern seems to highly overfit, which is indicated by the big gap between the validation accuracy and the training accuracy in such a way that the validation accuracy is way under the training accuracy and therefore seems to overfit. Especially when you look at the fact that aroung 5 epochs the validation accuracy goes down, but the training accuracy goes up: which really indicates overfitting. The networks' big architecture is also trained on much bigger datasets which could explain why it's overfitting and thus the architecture might as well be too complex for the task. Therefore we would want to get more data and use dropout layers sooner: in the convolutional layer for instance, but also adding dropout layers in general could prevent overfitting. Since you force the model to focus on the general patterns. \n- **Loss:** It is very clear that the validation loss does not follow the data very well. The validation loss is above the training loss all the way to the last epoch, and although the training loss as well as the validation loss seem to reach a certain point of stability, the validation loss has a large gap with the training loss. The validation loss indicates a higher learning rate but it will get stuck at worse values of loss than the training loss. There is too much \"energy\" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape. (Andrej Karpathy Stanford University at [this link](https:\/\/cs231n.github.io\/neural-networks-3\/)"}}