{"cell_type":{"0f20ea6d":"code","61f1f444":"code","1d5a589f":"code","aa5237a1":"code","c86c7d08":"code","0ef119ce":"code","b269d7ac":"code","632d4bfb":"code","8199283a":"code","0b92ed15":"code","c7b97382":"code","1f0bb21c":"markdown","e88b1533":"markdown","c505a0e1":"markdown","e74a16d9":"markdown","4fe6812a":"markdown","7c683b0c":"markdown","4efb73ca":"markdown","c4c666a7":"markdown"},"source":{"0f20ea6d":"#Import necessary Library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport time\n\n# Preprocessing\nfrom sklearn.model_selection import StratifiedKFold,KFold,train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import mode\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows',None)\npd.set_option('display.max_columns',None)\npd.set_option('float_format','{:f}'.format)","61f1f444":"#DataLoad, Shape, Describe\n\n#import the data and shape\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\nsample=pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")\nprint(train.shape,test.shape,sample.shape)\ntrain.describe().transpose()","1d5a589f":"train.columns","aa5237a1":"print(f'Total number of Train data: {train.shape[0]}')\nprint(f'\\033[92mTotal number of Test data: {test.shape[0]}')\nprint(f'\\033[96mNumber of Train_null identify: {sum(train.isna().sum())}')\nprint(f'\\033[96mNumber of Test_null identify: {sum(test.isna().sum())}')\n","c86c7d08":"#insert the kfold columns\ntrain['kfold'] = -1\n#distributing the data\nkfold=KFold(n_splits=5,random_state=42)\nfor fold, (tr_i,va_i) in enumerate(kfold.split(X=train)):\n    train.loc[va_i,'kfold'] = fold\n    \nprint(train.kfold.value_counts())\ntrain.to_csv(\"folds_5.csv\",index=False)\nprint(\"successfully folds\")","0ef119ce":"#feature separation\ndf= pd.read_csv(\".\/folds_5.csv\")\n#train.head()\n#features taken to train\nTARGET = 'target'\nfeatures = [f for f in df.columns if f not in(\"row_id\",\"kfold\",TARGET)]\ntest= test[features]\n","b269d7ac":"#preprocessing\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder,RobustScaler\nLE = LabelEncoder()\ndf[TARGET] = LE.fit_transform(train[TARGET])\n#see the target\ndf.head() ","632d4bfb":"df.target.unique()","8199283a":"%%time\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBRegressor,XGBClassifier\n#initialize the model\nprediction = []\nscore = []\n\nfor fold in range (5):\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n    \n        \n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {\n        'learning_rate': 0.001235,\n        'subsample': 0.95312,\n        'colsample_bytree': 0.1107,\n        'max_depth': 3,\n        'booster': 'gbtree', \n        'reg_lambda': 66.156,\n        'reg_alpha': 14.68267919457715,\n        'random_state':42,\n        'n_estimators':15000,\n        'objective': 'multi:softmax',\n        'eval_metric':'mlogloss'\n    }\n    \n    model= XGBClassifier(**xgb_params,\n                       gpu_id=0,\n                       tree_method='gpu_hist',\n                       predictor='gpu_predictor')\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=False)\n    \n    preds_valid = model.predict(xvalid)\n    \n    test_pre = model.predict(xtest)\n    prediction.append(test_pre)\n    \n    acc= accuracy_score(yvalid,preds_valid)\n    #Score \n    score.append(acc)\n    print(f\"fold|split:{fold},ACC:{acc}\")\n    \nprint(np.mean(score),np.std(score))\n","0b92ed15":"#reconfigure of split data\nfinal_predict = LE.inverse_transform(np.squeeze(mode(np.column_stack(prediction),axis=1)[0]).astype('int'))\nprint(final_predict)\nsample.target = final_predict\nsample.to_csv(\"submission.csv\",index=False)\nprint(\"Final achieve to send xgbclassifier output data\")\n","c7b97382":"sample.head()","1f0bb21c":"## **Output**","e88b1533":"# **TPS FEB2022**\n\n### Interesting tabular data\n\n### Day1: I am trying visualize what kind of data and built the base model\n\nSTEPS:\n\n1. Import necessary data\n2. Load the data, shape\n3. Stastical Analysis and Identify Null Data\n4. KFOLD\n5. Feature Separation\n6. Apply preprocessing only target data(Label encoder--> 10 class of data)\n7. Build the model(KNNClassifier|RobustScaler|Predict)\n","c505a0e1":"## **Preprocessing the Target**","e74a16d9":"## **Thankyou for Visiting!** \n\nSuggest Any new ways and tips","4fe6812a":"## **Load,Shape,Describe**","7c683b0c":"## **Feature separation**","4efb73ca":"## **Build the model**","c4c666a7":"## **Import necessary Library**"}}