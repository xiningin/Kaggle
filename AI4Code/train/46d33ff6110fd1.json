{"cell_type":{"8b74fc9c":"code","b54f1374":"code","6381f0b8":"code","fe97d948":"code","73a014db":"code","7e6c0e88":"code","5e47bc40":"code","ca5374be":"code","1f2c0a5f":"code","ca111de7":"code","add1c741":"code","e0d716e0":"code","e3d22bdc":"code","711949dc":"code","2a10d93b":"code","d2b6c6e6":"code","46e328ea":"code","34122cf9":"code","1f4f03d8":"code","4e5ccae8":"code","ee917b52":"code","193dcd9f":"markdown","5b7d055a":"markdown","ba7d7889":"markdown","b55709df":"markdown","08d117f8":"markdown","77b338c3":"markdown","44189cdc":"markdown","461c5446":"markdown","23379b14":"markdown","ffd3f9a4":"markdown","df5e9bde":"markdown","e640812a":"markdown","42ca69f3":"markdown","18ec5f5b":"markdown"},"source":{"8b74fc9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b54f1374":"train = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","6381f0b8":"train.dtypes, test.dtypes","fe97d948":"#splitting data into labels\ny_train = train['label']\ny_test = test['label']\nX_train = train.drop(labels = ['label'], axis=1)\nX_test = test.drop(labels = ['label'], axis = 1)","73a014db":"X_train = np.array(X_train, dtype = 'float32')\nX_test = np.array(X_test, dtype = 'float32')\ny_train = np.array(y_train, dtype = 'float32')\ny_test = np.array(y_test, dtype = 'float32')","7e6c0e88":"train.values.min(), train.values.max()","5e47bc40":"X_train, X_test = X_train \/ 255, X_test \/ 255","ca5374be":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","1f2c0a5f":"import matplotlib.pyplot as plt\nclass_names = ['T_shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nplt.imshow(X_train[0].reshape(28,28))\nlabel = int(y_train[0])\nplt.title(class_names[label])","ca111de7":"X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\nX_valid = X_valid.reshape(X_valid.shape[0], 28, 28, 1)\nX_train.shape","add1c741":"from keras.models import Sequential\nfrom keras.layers import Flatten, MaxPooling2D, Dropout, Dense, Conv2D\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [28,28, 1]))\n\nmodel.add(MaxPooling2D(pool_size = (2)))\nmodel.add(Dropout(0.25)) # reduces overfitting\nmodel.add(Flatten())\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(10, activation = 'softmax'))\n\nmodel.summary()","e0d716e0":"from keras.optimizers import Adam\nmodel.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001),metrics =['accuracy'])","e3d22bdc":"#if we want to implement early stopping to save training time we can modify below as model.fit(X_train, y_train, batch_size = 128, epochs = 50, verbose = 1,\n#validation_data = (X_valid, y_valid), callbacks = stop)\n\nfrom keras.callbacks import EarlyStopping\nstop = EarlyStopping(patience = 15) # patience stands for 15 epochs ","711949dc":"history = model.fit(X_train, y_train, batch_size = 400, epochs = 75, verbose = 1, validation_data = (X_valid, y_valid))","2a10d93b":"fig, ax = plt.subplots(1, 2, figsize = (15,5))\nax[0].plot(history.history['accuracy'], label = 'Accuracy')\nax[0].plot(history.history['val_accuracy'], label = 'Validation Accuracy')\nax[0].set_title('Accuracy')\nax[0].legend()\n\nax[1].plot(history.history['loss'], label = 'Loss')\nax[1].plot(history.history['val_loss'], label = 'Validation Loss')\nax[1].set_title('Loss')\nax[1].legend()","d2b6c6e6":"score = model.evaluate(X_test, y_test, verbose = 1)\nprint('Accuracy: ' + str(score[1]))\nprint('Loss: ' + str(score[0]))","46e328ea":"predicted_classes = model.predict_classes(X_test)\npredicted_classes = predicted_classes.reshape(-1,1)\ny_true = test['label']\n\npred_df = pd.DataFrame(predicted_classes)","34122cf9":"pred_df['Actual'] = y_true","1f4f03d8":"cols = ['Predictions', 'Actual']\npred_df.columns = [i for i in cols]\npred_df","4e5ccae8":"from sklearn.metrics import classification_report","ee917b52":"classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nprint(classification_report(y_true, predicted_classes, target_names = classes))","193dcd9f":"We can see above that the pixel range is between 0 & 255. We need to rescale our data to a 0-1 scale for our model to perform accurately. We can do this by dividing each value by 255.","5b7d055a":"## Compiling \n* Loss: We will use sparse_categorical_crossentropy as we have are solving a multiclass machine learning problem (10 classes). If we were doing one-hot we would use categorical crossentropy but that is not appropriate for this model. \n* Optimizer: Adam is widely used and a good start for beginners. (Descriptionon adam here later)\n* Metrics: in this model we will be using accuracy. This measures how many images are correcntly classified. One of the more simplistic metrics and good for beginners as generally we want to know how many images we classified correctly","ba7d7889":"We need to create class names as detailed in the data reference.","b55709df":"It seems that our model is very accurate but we do have moderate loss (~around .3).","08d117f8":"### References\n\n* Excellent notebook by Pavan: https:\/\/www.kaggle.com\/pavansanagapati\/a-simple-cnn-model-beginner-guide","77b338c3":"## Predictions","44189cdc":"## Examining the DataSet","461c5446":"### Early Stopping\n\nI am going to use early stopping to save training time in case we don't improve our model accuracy.","23379b14":"## Data","ffd3f9a4":"Keras and Tensorflow use numpy 32 arrays and so we must convert","df5e9bde":"# Classifying Fashion Using CNN: A Beginner's Tutorial","e640812a":"### Setting up a Model\n* We will use a Sequential model to classify the images.\n* A sequential model will take in a flatten layer which will convert our two dimensional array to a one dimensional array. (28 X 28 = 784; if you look at the shape of the data you can see that it is 28 X 28 if we want to create an image, shown above).\n* We will also instantiate Dense layers which are fully connected nueral layers. These layers have a lot of parameters along with weights and biases. If you call model.layers[1] you will get the first layer and can then call model.layers[1].weights to get the associated weights for that layer.\n* The last Dense layer will be however many classes there are. For this model we will be using 10 different classes. This layer returns an array of proabilities associated with how likely that the current image belongs to each class. Remember, 10 different classes, therefore 10 values in the array with sum(probabilities) = 1. The highest probability in the array will be the class assigned to the image. ","42ca69f3":"Before we feed the data to our model we need to shape the data so that the CNN can accurately depict each image.","18ec5f5b":"Our code now has 48,000 rows with 28 * 28 pixels each forming an image."}}