{"cell_type":{"0985854b":"code","8361c45d":"code","96c91fbe":"code","7b030d83":"code","6653088c":"code","554afa39":"code","5e69520e":"code","b0cd17d4":"code","23308693":"code","044a4449":"code","5b6a4b72":"code","a415f2ff":"code","38d03926":"code","ae2e4041":"code","3ca4c050":"code","35df8f33":"code","b4716e9d":"code","a4fab463":"code","e8abc2b2":"code","9daa5157":"markdown","a8341f0d":"markdown","8fecf3da":"markdown","e87fa9b4":"markdown","0fb97940":"markdown","c300047f":"markdown","92d0aabf":"markdown","61040493":"markdown","0b580fa9":"markdown","14f89b01":"markdown","75d29447":"markdown","9f7c28da":"markdown","426ee0c9":"markdown","ab8639df":"markdown","ab905e68":"markdown","9fda759e":"markdown","d128693c":"markdown","21ab8a09":"markdown","3e21ad27":"markdown","af711b75":"markdown","93941e3c":"markdown"},"source":{"0985854b":"import os\nfrom nltk.corpus import stopwords\nimport os, json\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nfrom torch.utils.data import DataLoader\nimport math\nfrom sentence_transformers import models, losses\nfrom sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nfrom sentence_transformers.readers import *\nimport logging\nfrom datetime import datetime\nimport scipy\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\nimport re\nimport gensim\nfrom nltk import pos_tag, word_tokenize\nimport scipy\nfrom scipy import spatial\nimport fasttext\nimport sent2vec\nimport os,json\nfrom pandas.io.json import json_normalize\nimport pandas as pd","8361c45d":"path_to_json = '\/home\/nlvm\/kaggle\/json_files\/'","96c91fbe":"model = sent2vec.Sent2vecModel()\nmodel.load_model('\/home\/nlvm\/Covid19_Model.bin')","7b030d83":"emptdf=pd.DataFrame()\njson_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n\nfields=['paper_id', 'metadata', 'abstract', 'body_text', 'back_matter']\nfields_v1=['cord_uid','title','doi','pmcid','pubmed_id','license','abstract','publish_time','authors','journal','has_full_text','full_text_file']\n\nprint('emptdf start')\n\nfor index, js in enumerate(json_files):\n    with open(os.path.join(path_to_json, js)) as json_file:\n        json_text = json.load(json_file)\n        #print(index)\n        #print(json_text)\n        if str(type(json_text))!=\"<class 'list'>\":\n\n            paper_id=json_text[\"paper_id\"]\n\n            try:\n                abstract=json_text[\"abstract\"]\n            except:\n                abstract=\"\"\n            body_text=json_text[\"body_text\"]\n            title=json_text[\"metadata\"][\"title\"]\n            authors=\"\"\n\n            try:\n                authors=json_text[\"metadata\"][\"authors\"]\n\n            except:\n                pass\n            xv=pd.DataFrame({'paper_id': [paper_id], 'abstract': [abstract], 'body_text':[body_text],'title':[title],'authors':[authors]})\n            #print(xv)\n\n            #xv=json_normalize(json_text)\n            #xv=xv[['paper_id', 'abstract', 'body_text', 'back_matter','metadata.title', 'metadata.authors']]\n            if not xv.empty:\n                emptdf=pd.concat([emptdf,xv])\n                ","6653088c":"# function to extract text\ndef process_text(row):\n    fullstr=\"\"\n    for i in row[\"body_text\"]:\n       \n        if fullstr:\n            fullstr=fullstr+\"\/n\"+i['text']\n        else:\n            fullstr=fullstr+i['text']\n          \n    return fullstr\n#function to extract abstract\ndef process_abstract(row):\n    fullstr=\"\"\n    for i in row[\"abstract\"]:\n       \n        if fullstr:\n            fullstr=fullstr+\" \"+i['text']\n        else:\n            fullstr=fullstr+i['text']\n          \n    return fullstr\nimport re\nemptdf[\"fullstr\"]=emptdf.apply(process_text,axis=1)\n\nemptdf[\"abstract_fullstr\"]=emptdf.apply(process_abstract,axis=1)","554afa39":"# function to flag covid related articles \ndef covid_flagfun(row):\n    lis=[\"covid\",\"2019-ncov\", \"2019 novel coronavirus\", \"coronavirus 2019\", \"coronavirus disease 19\", \"covid-19\", \"covid 19\", \"ncov-2019\", \"sars-cov-2\"]\n    fullstr=str(row[\"abstract_fullstr\"])\n    flag=0\n    for i in lis:\n        #print(fullstr)\n        #print(pattern)\n        if i==\"covid\":\n            pattern = re.compile(r\"{}\".format(i))\n        else :\n            pattern = re.compile(r\"{}*\".format(i))\n            \n        \n        if re.search(pattern,fullstr):\n            return 1\n            #print(1)\nemptdf[\"covid_flagv1\"]=emptdf.apply(covid_flagfun,axis=1)\nemptdf[\"covid_flagv1_abs\"]=emptdf.apply(covid_flagfun,axis=1)\n\nkeywords = [r\"2019[\\-\\s]?n[\\-\\s]?cov\", \"2019 novel coronavirus\", \"coronavirus 2019\", r\"coronavirus disease (?:20)?19\",\n            r\"covid(?:[\\-\\s]?19)?\", r\"n\\s?cov[\\-\\s]?2019\", r\"sars-cov-?2\", r\"wuhan (?:coronavirus|cov|pneumonia)\",\n            r\"rna (?:coronavirus|cov|pneumonia)\", r\"mers (?:coronavirus|cov|pneumonia)\", r\"influenza (?:coronavirus|cov|pneumonia)\",\n            r\"sars (?:coronavirus|cov|pneumonia)\", r\"sars\", r\"mers\", r\"pandemic\", r\"pandemics\"]\n\n    # Build regular expression for each keyword. Wrap term in word boundaries\nregex = \"|\".join([\"\\\\b%s\\\\b\" % keyword.lower() for keyword in keywords])\n\ndef tags(row):\n    if re.findall(regex, str(row[\"fullstr\"]).lower()):\n        tags = \"COVID-19\"\n    else:\n        tags=\"NON COVID\"\n    return tags\nemptdf[\"covid_flag\"]=emptdf.apply(tags,axis=1)\n\n# creating covid flag\n\ncovid_final_df=emptdf[(emptdf[\"covid_flag\"]==\"COVID-19\") |(emptdf[\"covid_flagv1\"]==1) | (emptdf[\"covid_flagv1_abs\"]==1) ]\n","5e69520e":"lis = list(covid_final_df.iloc[:,6])\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(400)\nimport nltk\nnltk.download('wordnet')\nstemmer = SnowballStemmer(\"english\")\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\n# Tokenize and lemmatize\ndef preprocess(text):\n    result=[]\n    for token in gensim.utils.simple_preprocess(text) :\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            # TODO: Apply lemmatize_stemming on the token, then add to the results list\n            result.append(lemmatize_stemming(token))\n    return result\nprocessed_docs = list(map(preprocess,lis))\ndictionary = gensim.corpora.Dictionary(processed_docs)\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\nfrom gensim import corpora, models\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n#Creating LDA Model\nlda_model = gensim.models.LdaMulticore(bow_corpus, \n                                       num_topics=5, \n                                       id2word = dictionary, \n                                       passes = 5,alpha=[0.01]*5,eta=[0.01]*len(dictionary.keys()) ,\n                                       workers=2)","b0cd17d4":"import pysolr\n\nsolr = pysolr.Solr('http:\/\/localhost:8983\/solr\/covid19_v4', always_commit=True)","23308693":"import inflect\np = inflect.engine()","044a4449":"import en_core_sci_lg\nnlp =  en_core_sci_lg.load()","5b6a4b72":"def process_query_v3(text):\n    punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    no_punct = \"\"\n    lis=['covid','novelcorona','coronavirus']\n    for char in text.lower():\n        if char not in punctuations:\n            no_punct = no_punct + char\n    no_punct = ''.join([i for i in no_punct if not i.isdigit()])\n    tok=\"\"\n    \n    doc = nlp(text.lower())\n    \n    taglis = []\n    result=[]\n    root_str = ''\n    compound_str = ''\n    num_nns = 0\n    num_nn = 0\n    noun_query = []\n    noun_query_lis = []\n    compound_lis = []\n    \n    for token in doc:\n        taglis.append([token.text,token.dep_,token.tag_,\n            token.head])\n        \n    #print(taglis)\n    \n    for tag_details in taglis:\n        if tag_details[1] == 'compound':\n            if tag_details[2] == 'NNS':\n                compound_str_nns = tag_details[0]\n                compound_str = tag_details[0]\n                query_ph='fullstr:\"' + tag_details[0] + '\"'\n                v=solr.search(query_ph)\n                num_nns = v.raw_response['response']['numFound']\n                \n                query_ph='fullstr:\"' + p.singular_noun(tag_details[0]) + '\"'\n                v=solr.search(query_ph)\n                num_nn = v.raw_response['response']['numFound']\n                if num_nns <= num_nn:\n                    compound_str = p.singular_noun(tag_details[0])\n                    idx = 0\n                    for i in taglis:\n                        for a, b in enumerate(i):\n                            if str(b) == compound_str_nns:\n                                taglis[idx][a] = compound_str\n                        idx += 1\n                    #taglis[idx] = root_str\n            else:\n                compound_str = tag_details[0]\n            compound_lis.append(compound_str) \n        \n    compound_lis = [i for n, i in enumerate(compound_lis) if i not in compound_lis[:n]]\n    \n    # Adding compound  to the main list (noun_query_lis) \n    for compound in range(len(compound_lis)):\n        query_ph='fullstr:\"' + ' '.join(x for x in compound_lis[0:len(compound_lis)-compound]) +'\"~1000'\n        v=solr.search(query_ph)\n        #print(query_ph)\n        if v.raw_response['response']['numFound'] > 0:\n            #print(query_ph)\n            for lis_ij in compound_lis[0:len(compound_lis)-compound]:\n                noun_query_lis.append(lis_ij)\n            break;\n    \n    #return noun_query_lis\n    \n            \n    noun_query_lis = [i for n, i in enumerate(noun_query_lis) if i not in noun_query_lis[:n]]\n\n    mod_lis=[]\n    #return noun_query_lis\n    for tag_details in taglis:\n        if tag_details[1] == 'nmod' or tag_details[1] == 'amod':\n            if tag_details[2] == 'NNS':\n                mod_str_nns = tag_details[0]\n                mod_str = tag_details[0]\n                query_ph='fullstr:\"' + tag_details[0] + '\"'\n                v=solr.search(query_ph)\n                num_nns = v.raw_response['response']['numFound']\n\n                query_ph='fullstr:\"' + p.singular_noun(tag_details[0]) + '\"'\n                v=solr.search(query_ph)\n                num_nn = v.raw_response['response']['numFound']\n                if num_nns <= num_nn:\n                    compound_str = p.singular_noun(tag_details[0])\n                    idx = 0\n                    for i in taglis:\n                        for a, b in enumerate(i):\n                            if str(b) == mod_str_nns:\n                                taglis[idx][a] = mod_str\n                        idx += 1\n                    #taglis[idx] = root_str\n            else:\n                mod_str = tag_details[0]\n            mod_lis.append(mod_str)\n\n    mod_lis = [i for n, i in enumerate(mod_lis) if i not in mod_lis[:n]]\n    #print(mod_lis)\n    \n    \n    # Adding nmod and amods  to the main list (noun_query_lis) \n    \n    for mod in range(len(mod_lis)):\n        query_ph='fullstr:\"' + ' '.join(x for x in mod_lis[0:len(mod_lis)-mod]) +'\"~1000'\n        v=solr.search(query_ph)\n        #print(query_ph)\n        if v.raw_response['response']['numFound'] > 0:\n            #print(query_ph)\n            for lis_ij in mod_lis[0:len(mod_lis)-mod]:\n                noun_query_lis.append(lis_ij)\n            break;\n    noun_query_lis = [i for n, i in enumerate(noun_query_lis) if i not in noun_query_lis[:n]]\n    #print(noun_query_lis)\n    #return noun_query_lis\n \n    head_lis = []\n    head_lis_final = []\n    noun_head_lis_final = []\n    vb_jj_head_lis_final = []\n    noun_vb_jj_lis_final = []\n    \n    \n    for tag_details in taglis:\n        head_lis.append(str(tag_details[3]))\n    \n    head_lis_final = [i for n, i in enumerate(head_lis) if i not in head_lis[:n]]\n    \n    #print('head_lis_final',head_lis_final)\n    \n    \n    # Adding verbs  to the main list (noun_query_lis) \n    \n    for head in head_lis_final:\n        noun_head_lis = []\n        vb_jj_head_lis = []\n        for tag_details in taglis:\n            if str(tag_details[1]) != 'ROOT' and 'NN' in str(tag_details[2]) and str(tag_details[3]) == str(head) and str(tag_details[0]) != str(head):\n                noun_head_lis.append(tag_details[0])\n            elif (str(tag_details[2]) == 'VB' or str(tag_details[2]) == 'JJ') and str(tag_details[3]) == str(head) and str(tag_details[0]) != str(head):\n                vb_jj_head_lis.append(tag_details[0])\n        if noun_head_lis:\n            noun_vb_jj_lis_final.append([head,noun_head_lis])\n        \n        if vb_jj_head_lis:\n            noun_vb_jj_lis_final.append([head,vb_jj_head_lis])\n    \n    for i in noun_vb_jj_lis_final:\n        if len(i[1]) == 0:\n            noun_vb_jj_lis_final.remove(i)\n    \n    #print('noun_vb_jj_lis_final', noun_vb_jj_lis_final)\n    for i in noun_vb_jj_lis_final:\n        for j in range(len(i[1])):\n            query_ph='fullstr:\"'+ ' '.join(x for x in i[1][0:len(i[1])-j]) + ' '  + str(i[0]) +'\"~1000'\n            v=solr.search(query_ph)\n            #print(query_ph)\n            if v.raw_response['response']['numFound'] > 0:\n                #print(query_ph)\n                \n                for lis_ij in i[1][0:len(i[1])-j]:\n                    noun_query_lis.append(lis_ij)\n                noun_query_lis.append(str(i[0]))\n                \n                break;\n    # Adding root  to the main list (noun_query_lis) \n    for tag_details in taglis:\n        if tag_details[1] == 'ROOT':\n            if tag_details[2] == 'NNS':\n                root_str_nns = tag_details[0]\n                root_str = tag_details[0]\n                query_ph='fullstr:\"' + tag_details[0] + '\"'\n                v=solr.search(query_ph)\n                num_nns = v.raw_response['response']['numFound']\n                \n                query_ph='fullstr:\"' + p.singular_noun(tag_details[0]) + '\"'\n                v=solr.search(query_ph)\n                num_nn = v.raw_response['response']['numFound']\n                if num_nns <= num_nn:\n                    root_str = p.singular_noun(tag_details[0])\n                    idx = 0\n                    for i in taglis:\n                        for a, b in enumerate(i):\n                            if str(b) == root_str_nns:\n                                taglis[idx][a] = root_str\n                        idx += 1\n                    #taglis[idx] = root_str\n            else:\n                root_str = tag_details[0]\n        noun_query.append(root_str) \n        \n        break;\n        \n    #print(taglis)\n    noun_query_lis.append(root_str) \n    \n    #return (noun_query_lis)\n    \n    for noun in range(len(noun_query_lis)):\n        noun_query_temp = noun_query_lis\n        query_ph='fullstr:\"' + ' '.join(x for x in noun_query_temp[0:len(noun_query_temp)-noun]) +'\"~1000'\n        v=solr.search(query_ph)\n        #print(query_ph)\n        if v.raw_response['response']['numFound'] > 0:\n            #print(query_ph)\n            break;\n            \n    noun_query_lis = noun_query_temp[0:len(noun_query_temp)-noun]\n    \n    noun_query_lis = [i for n, i in enumerate(noun_query_lis) if i not in noun_query_lis[:n]]\n    #print(stop_words)   \n    \n    noun_query_lis = [i for i in noun_query_lis if not i.lower() in gensim.parsing.preprocessing.STOPWORDS and len(i) > 2]\n    \n    # Returning keywords \n    return ' '.join(x for x in noun_query_lis)\n","a415f2ff":"def query_formation(query_str,dictionary):\n    query_str_orig = query_str\n    def set_tf_idf(row):\n        \n        try:\n            token_id=dictionary.token2id\n            idx=token_id[row[\"processed\"]]\n            for i in corpus_tfidf[0]:\n                if i[0]==idx:\n                    return i[1]\n        except:\n            return 0\n    global str    \n \n    def preprocess(text):\n        result=[]\n        wnl = WordNetLemmatizer()\n        for word, tag in pos_tag(word_tokenize(query_str)):\n            wntag = tag[0].lower()\n            wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n            if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 2:\n                if not wntag:\n                    result.append(word)\n                else:\n                    result.append(wnl.lemmatize(word, wntag))\n        #print(result)\n#         for token in gensim.utils.simple_preprocess(text) :\n#             if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n#                 # TODO: Apply lemmatize_stemming on the token, then add to the results list\n#                 result.append(wnl.lemmatize(token,)\n        return result\n    def lemmatize_stemming_df(row):\n        return stemmer.stem(WordNetLemmatizer().lemmatize(row[\"original\"], pos='v'))\n    query_lis=[]\n    query_lis_final=[]\n    #for i in nlp(query_str).ents:\n        #query_lis.append(str(i))\n    query_lis = preprocess(query_str)\n    query_str=\" \".join(query_lis)\n    #print (query_str)\n    query_lis_final.append(query_str)\n    df=pd.DataFrame( gensim.utils.simple_preprocess(query_lis_final[0]))\n    df.columns=[\"original\"]\n    df['processed'] = df.apply(lemmatize_stemming_df,axis=1) \n    a=[list(df[\"processed\"])]\n    bow_query = [dictionary.doc2bow(doc) for doc in a]\n    corpus_tfidf = tfidf[bow_query]\n    df['tf_idf'] = df.apply(set_tf_idf,axis=1)\n    df_filtered=df[df['tf_idf']>0]\n    df_filtered.sort_values(by=['tf_idf'], ascending=False,inplace=True)\n    df_filtered[\"boost\"]=df_filtered[\"tf_idf\"]\/df_filtered[\"tf_idf\"].sum()*1000\n    df_filtered.reset_index(drop=True)\n    phrase_query=\"\"\n    #print(df_filtered)\n    for i in range(len(df_filtered)):\n        phrase_query=phrase_query+\" \"+df_filtered.iloc[i,0]\n        if df_filtered.iloc[0:i,2].sum()>.95*df_filtered[\"tf_idf\"].sum():\n            break\n    \n    # Construction of Phrase Query\n    phrase_query=phrase_query.strip()\n    phrase_list=phrase_query.strip().split()\n    lis=[]\n    for i in range(len(phrase_query.strip().split())):\n        #print(i,len(phrase_query.strip().split()),\" \".join(phrase_list[0:-1-i]))\n        if i+2==len(phrase_query.strip().split()):\n            break\n        else:\n            lis.append(\" \".join(phrase_list[0:-1-i]) ) \n    lis.insert(0,process_query_v3(query_str_orig))    \n    \n    # Construction of term Query\n    term_lis=[]\n    term_lis=[str(x[0]) + \"*^\"  +str(int(x[1])) for x in zip(list(df_filtered[\"processed\"]),list(df_filtered[\"boost\"]))]\n    return lis,term_lis\n","38d03926":"def getSolr_results(query_string,dictionary):\n    lis=[\"covid\",\"2019-ncov\", \"2019 novel coronavirus\", \"coronavirus 2019\", \"coronavirus disease 19\", \"covid-19\", \"covid 19\", \"ncov-2019\", \"sars-cov-2\"]\n    result_list = []\n    idx = 0\n    query_string=query_string.replace(\"corona virus\", \"coronavirus\", 1)\n    query_string=query_string.replace(\"covid-19\", \" \", 1)\n    for i in lis:\n        query_string=query_string.replace(i, \"\", 1)\n\n    phrase,term = query_formation(query_string,dictionary)\n    #topic = query_topic(query_string,dictionary_title,model)\n    \n    #print(term)\n    for i in range(math.ceil(len(phrase)\/2)):\n        if i==0:\n            for k in lis:\n                phrase_query = 'fullstr:\"' + phrase[i] + \" \" + k+'\"~1000'\n                results = solr.search(phrase_query,rows=3)\n\n                for result in results:\n                    result_list.append([result['paper_id'][0],idx])\n                #print(phrase_query)\n                #print(len(results))\n                if(len(result_list))>=3:\n                    query_string_new=query_string.lower()+\" \"+ k\n                    #print(query_string_new)\n                    break\n\n\n\n\n        elif len(result_list) ==0 or i!=0:\n            phrase_query = 'fullstr:\"' + phrase[i] +'\"~1000'\n            results = solr.search(phrase_query,rows=3)\n            query_string_new=query_string.lower()\n\n\n        idx += 1\n        for result in results:\n            result_list.append([result['paper_id'][0],idx])\n            #print(result_list)\n\n    if len(result_list) < 50:\n        term_query = \"\"\n        for i in term:\n            if term_query:\n                term_query = term_query + \", \" + i\n            else:\n                term_query = i\n        term_query = 'fullstr:(' + term_query + ')'\n        #print(term_query)\n        results = solr.search(term_query,rows=3)\n        for result in results:\n            result_list.append([result['paper_id'][0],idx+1])\n            #print(result_list)\n    query_string_new=query_string_new.replace(\"covid\", \"covid-19\", 1)\n    articles_final=selec_articles(query_string_new,result_list) \n    para_final=selec_para(query_string_new,articles_final)\n    final_summary=final_return(query_string_new,articles_final,para_final)\n    return final_summary\n","ae2e4041":"def selec_articles(query_str,articles_lis):\n    def cosine_test(row):\n        query_vec=model.embed_sentence(query_str)\n        #print(type(row[\"para_vector\"]))\n        distances = scipy.spatial.distance.cdist(query_vec, row[\"para_vect_fullstr\"], \"cosine\")[0]\n        return distances[0]\n    rank1=[]\n    rank_others=[]\n    dist=[]\n    dist1=[]\n    query_vec=model.embed_sentence(query_str)\n    #print(query_vec.shape)\n    for i in articles_lis:\n        if i[1]==0:\n            rank1.append(i[0])\n        else :\n            rank_others.append(i[0])\n    rank1_artc=covid_final_df[covid_final_df.paper_id.isin(rank1)]\n\n    #rank1_array=np.array(rank1_artc.iloc[:,[0,10]])\n    #print(rank1_array[0][1])\n    #rank1_array[2]=scipy.spatial.distance.cdist(query_vec, rank1_array[1], \"cosine\")[0]\n    #print(rank1_array[1].shape)\n    #print(rank1_array)\n    if rank1_artc.shape[0]<3:\n        rankothers_artc=covid_final_df[covid_final_df.paper_id.isin(rank_others)]\n    #rank1_artc[\"test_distance\"]= rank1_artc.apply(cosine_test,axis=1)\n    #return rankothers_artc\n    for h in range(rank1_artc.shape[0]):\n        dist.append([scipy.spatial.distance.cdist(query_vec, rank1_artc.iloc[h,10], \"cosine\")[0][0],rank1_artc.iloc[h,0]])\n    if rank1_artc.shape[0]<3:\n        for k in range(rankothers_artc.shape[0]):\n            dist1.append([scipy.spatial.distance.cdist(query_vec, rankothers_artc.iloc[k,10], \"cosine\")[0][0],rankothers_artc.iloc[k,0]])\n    #rank1_artc=rank1_artc.sort_values(by=['test_distance'])\n    #a=rank1_artc.iloc[:,0]\n    dist=sorted(dist, key = lambda x: x[0])\n    if dist1:\n        sorted(dist1, key = lambda x: x[0])\n\n\n\n    return dist+dist1\n","3ca4c050":"for index, row in emptdf.iterrows():\n    x={}\n    for i in row[\"body_text\"]:\n        if i['text']:\n            x[\"paper_id\"]=row[\"paper_id\"]\n            x[\"paragraph\"]=i['text']\n            x_df=pd.DataFrame.from_dict(x,orient='index').T\n            consolidated_df=pd.concat([consolidated_df, x_df])\n\nconsolidated_df.to_json(\"\/home\/nlvm\/kaggle\/consolidated_df.json\",orient='records')\n    \nimport json\nwith open(\"\/home\/nlvm\/kaggle\/consolidated_df.json\") as json_file:\n    s=json.load(json_file)\npara_df=json_normalize(s)\npara_df.drop_duplicates(subset='paragraph', keep=\"last\",inplace=True)\n\n#Extracting Sentences from Paragraph\ndef sent_token(row):\n    sentences = nltk.sent_tokenize(row[\"paragraph\"])\n    return sentences\n\n#Creating Sentence Vector list\ndef sent_vector_lis(row):\n    lis=[]\n    for i in row[\"sent_tok\"]:\n        vec=model.embed_sentence(i)\n        lis.append(vec)\n    return lis\npara_df['sent_tok'] = para_df.apply(sent_token,axis=1)\npara_df[\"sent_vector_lis\"]= para_df.apply(sent_vector_lis,axis=1)\n\n#Aggregating sentence vector to paragraph by taking Mean of sentence vectors \ndef sent_vector(row):\n    lis=[]\n    for i in row[\"sent_tok\"]:\n        vec=model.embed_sentence(i)\n        lis.append(vec)\n    lis_arr=np.array(lis)\n    lis_arr_mean=lis_arr.mean(axis=0)\n    return lis_arr_mean\npara_df[\"sent_vect_mean\"]= para_df.apply(sent_vector,axis=1)\n\npara_df.reset_index(drop=True,inplace=True)\npara_df=para_df.reset_index()","35df8f33":"\n#Function returns the best paragrapgh using cosine distance\ndef selec_para(query_str,articles_shortlisted):\n\n\n    para_article=[]\n    dist_final=[]\n    query_vec=model.embed_sentence(query_str)\n    #print(query_vec.shape)\n    #print(articles_shortlisted)\n    for i in articles_shortlisted:\n\n        para_artc=para_df[para_df.paper_id==i[1]]\n\n        dist=[]\n        try:\n            for h in range(para_artc.shape[0]):\n                dist.append([scipy.spatial.distance.cdist(query_vec, para_artc.iloc[h,6], \"cosine\")[0][0],para_artc.iloc[h,0]])\n            dist=sorted(dist, key = lambda x: x[0])\n            dist_final.append(dist[0])\n        except:\n            pass\n    dist_final=sorted(dist_final, key = lambda x: x[0])    \n    return dist_final\n    #return para_artc.shape\n    #rank1_array=np.array(rank1_artc.iloc[:,[0,10]])\n    #print(rank1_array[0][1])\n    #rank1_array[2]=scipy.spatial.distance.cdist(query_vec, rank1_array[1], \"cosine\")[0]\n    #print(rank1_array[1].shape)\n    #print(rank1_array)\n    #rank1_artc[\"test_distance\"]= rank1_artc.apply(cosine_test,axis=1)\n \n    \n    #rank1_artc=rank1_artc.sort_values(by=['test_distance'])\n    #a=rank1_artc.iloc[:,0]\n\n\n    return dist_final\n","b4716e9d":"#Function returns the best sentences using Cosine distance\ndef best_sent(query_str,para_results):\n    query_vec=model.embed_sentence(query_str)\n\n    para_id=[]\n    for j in para_results:\n        para_id.append(j[1])\n    #print(para_id)\n    pararesults_df=para_df[para_df[\"index\"].isin(para_id)]\n    #print(pararesults_df)\n    idx=0\n    final_highlight=[]\n    try:\n        for index, row in pararesults_df.iterrows():\n            lis_para=[]\n            #print(\"parse:\",idx)\n            #print(\"sent: \",row[\"sent_tok\"])\n            #print(\"sent_vect\",len(row[\"sent_vector_lis\"]))\n            for i in range(len(row[\"sent_vector_lis\"])):\n\n                lis_para.append([scipy.spatial.distance.cdist(query_vec, row[\"sent_vector_lis\"][i], \"cosine\")[0][0],i])\n\n            lis_para=sorted(lis_para, key = lambda x: x[0])\n            #print(lis_para)\n            #print(len(row[\"sent_vector_lis\"]))\n            templis=[]\n            for l in range(math.ceil(len(lis_para)\/3)):\n                #print(\"ceil:\",len(lis_para))\n                #regex = re.compile('.*({}).*'.format(row[\"sent_tok\"][lis_para[l][1]]))\n                #print(row[\"sent_tok\"][lis_para[l][1]])\n                c=row[\"paragraph\"].find(row[\"sent_tok\"][lis_para[l][1]])\n                templis.append([c,len(row[\"sent_tok\"][lis_para[l][1]])])\n            templis=sorted(templis, key = lambda x: x[0])\n            #print(templis)\n            final_highlight.append([row[\"paper_id\"],[templis[0][0],templis[-1][0]+templis[-1][1]]])\n            idx+=1\n        return final_highlight\n    except:\n        return final_highlight","a4fab463":"with open(\"\/home\/nlvm\/kaggle\/covid_summarize_auth_insti_final.json\") as json_file:\n    s=json.load(json_file)\n\ncovid_summarize_auth_insti_final=json_normalize(s)\n\n#Function to fetch all the results converted to JSON and feed to API.\ndef final_return(query_str,articles_results,para_results):\n    articles_id=[]\n    para_id=[]\n    sumr_str=[]\n    for i in articles_results:\n        articles_id.append(i[1])\n    for j in para_results:\n        para_id.append(j[1]) \n    #print(para_id)\n    article_df=covid_final_df[covid_final_df.paper_id.isin(articles_id)].iloc[:,[0,3,6]] \n    pararesults_df=para_df[para_df[\"index\"].isin(para_id)].iloc[:,[1,2]]\n    auth_results = covid_summarize_auth_insti_final[covid_summarize_auth_insti_final['paper_id'].isin(articles_id)]\n    sent_highlights=best_sent(query_str,para_results)\n\n    if sent_highlights:\n        sent_high_df=pd.DataFrame(sent_highlights,columns=[\"paper_id\",\"pos\"])\n#     for index, row in article_df.iterrows():\n#         if row[\"abstract_fullstr\"]==\"\":\n#             sumr_str.append([row[\"fullstr\"],row[\"paper_id\"]])\n#         else:\n#             sumr_str.append([row[\"abstract_fullstr\"],row[\"paper_id\"]])\n            \n\n    final_summary1=pd.merge(article_df,pararesults_df,on=\"paper_id\",how=\"inner\")\n    final_summary2=pd.merge(final_summary1,auth_results,on=\"paper_id\",how=\"left\")\n    if sent_highlights:\n        final_summary=pd.merge(final_summary2,sent_high_df,on=\"paper_id\",how=\"left\")\n    else:\n        final_summary=final_summary2\n\n#     idx=0\n#     for sumr in sumr_str:\n#         #print(sumr[1])\n#         #summarize( sumr[0], word_count=300)\n#         print(sumr)\n    return final_summary.to_json(orient=\"records\")","e8abc2b2":"from flask import Flask, jsonify, request\nfrom flask_cors import CORS\nfrom flask_cors import CORS, cross_origin\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'the quick brown fox jumps over the lazy   dog'\napp.config['CORS_HEADERS'] = 'Content-Type'\n\ncors = CORS(app, resources={r\"\/foo\": {\"origins\": \"http:\/\/localhost:5000\"}})\n\n@app.route('\/covid',methods=['POST'])\n@cross_origin(origin='localhost',headers=['Content- Type','Authorization'])\ndef results():\n    data=request.get_json(force=True)\n    query=str(data[\"query\"])\n    covid_json = getSolr_results(query,dictionary)\n    #covid_final_df = covid_final(query,dictionary,dictionary_title,lda_model)\n    #covid_json = covid_final_df.to_json(orient=\"records\")\n    return covid_json\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0',port=5000)","9daa5157":"### 4. Loading JSON files to python dataframe\n#### Parsing through each individual json files and normalizing it and converting to python dataframe.\n","a8341f0d":"### 15. Extracting Paragraphs from the full body_text of the article\n#### Extracting sentences from the paragraph using Nltk tool kit\n#### Create sentence vectors and aggregate the paragraphs","8fecf3da":"### 1. Import Datatype and Python Packages","e87fa9b4":"### 6. Create Covid-19 flag","0fb97940":"### 13. Query the solr results and creates the final summary\n#### Phrase and Term queries will be passed to PySolr and  fetch matching Paper_Ids","c300047f":"### 20. API call","92d0aabf":"### 10. Import full ScispaCy pipeline for biomedical data with word vectors","61040493":"### 17. Identifying the best sentences from the paragraphs returned from the func: selec_para()","0b580fa9":"### 9. Import inflect library to form the Singular of Plural nouns","14f89b01":"### 8. Import pysolr library for extracting search results using Solr search Engine","75d29447":"### 18. Wrapper function to Jsonify the results","9f7c28da":"### 16. Identifying best paragraph from the articles returned from the func: selec_articles()","426ee0c9":"### 5. Extract Body text & Abstract from data\n#### Body text and Abstract columns are in the form of dictionary.Parsing through the dictionary and extracting value using the key \"text\" and appending it to get full body text and abstract","ab8639df":"### 3. Loading the custom trained sent2vec model\n#### Not able to upload the model as the size is 21GB","ab905e68":"# Covid-19 Research tool link: http:\/\/52.255.160.69\/","9fda759e":"### 11. Process query will send the process\/term queries to Solr Search engine & processes the results\n#### function to construct Phrase and term queries to be passed to the model \n#### Sequence =  compounds + amod + nmod + nouns + Verbs + root\n#### Named Entities,Dependency Tags ,verbs etc will be identified using Scispacy model ","d128693c":"### 14. Ranking the articles and returns the best 3 from the set of articles fetched by Solr\n#### Ranking will be based on Cosine distance of query string and full article vectors","21ab8a09":"### 12. Query formation creates the Process\/Term queries for Solr Search engine\n#### Using the keywords returned by Process_query function, query_formation will construct the phrase and term queries for Solr\n#### Term queries boosting value will be detrmined by the TF IDF value of the keyword in the dictionary created above .","3e21ad27":"### 2. Set path for Covid-19 JSON files","af711b75":"#### Data creation completed for Solr Search Engine","93941e3c":"### 7. Tokenize, Lemmatize and create LDA model\n#### Assigning topic to DataFrame"}}