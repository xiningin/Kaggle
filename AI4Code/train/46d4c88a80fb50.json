{"cell_type":{"d4012e9c":"code","83e40085":"code","a35513db":"code","ff9c7274":"code","e1b8d957":"code","d6238a4d":"code","c206d615":"code","6fc770f9":"code","7bd31ba7":"code","e57d8b33":"code","8171fcb5":"code","8b34fd6f":"code","23b564ce":"code","c181c1cb":"markdown","5c2de64f":"markdown","cbbe6e02":"markdown","29411a02":"markdown","305c9646":"markdown"},"source":{"d4012e9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83e40085":"import pandas as pd\nimport numpy as np\nimport emoji\nimport re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","a35513db":"lemmatizer = nltk.stem.WordNetLemmatizer()\nstops = set(stopwords.words('english'))","ff9c7274":"df = pd.read_csv('..\/input\/road-accidents-tweets-india\/road_accident_india_40k_tweets.csv').dropna(subset=['tweet']).reset_index(drop=True)\ndf.head()","e1b8d957":"df['time'] = pd.to_datetime(df['time'])","d6238a4d":"def preprocess_tweet(tweet: str, stop_words = []):\n    \n    # remove stock market ticker like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    \n    # remove hastags only remove the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    \n    # remove name tags\n    tweet = re.sub('@[^\\s]+', '', tweet)\n    \n    # tokenizer tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    \n    tweet_cleaned = []\n    \n    for token in tweet_tokens:\n        if token not in stop_words: # remove stop words\n            lemma_token = lemmatizer.lemmatize(token) # lemmatize the token\n            demojize = emoji.demojize(lemma_token) # demojize the token\n            tweet_cleaned.append(demojize)\n    \n    tweet_cleaned = \" \".join(tweet_cleaned)\n    tweet_cleaned = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', tweet_cleaned)  # replace all special character with spaces\n    tweet_cleaned = re.sub(r'\\s+', ' ', tweet_cleaned).strip() # clean any exra spaces in the text\n            \n    return tweet_cleaned","c206d615":"df['tweet_cleaned'] = df['tweet'].apply(lambda x: preprocess_tweet(x, stops))\ndf.head()","6fc770f9":"# create a vocabulary of words\n# ignore words that appear in 85 % of documents\ncv = CountVectorizer(max_df=0.85, max_features=200000)\nword_count_vector = cv.fit_transform(df['tweet_cleaned'])","7bd31ba7":"tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\ntfidf_transformer.fit(word_count_vector)","e57d8b33":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","8171fcb5":"# word to index dict\nfeature_names = cv.get_feature_names()\n\ndoc_keywords = []\n\nfor i, tweet in enumerate(df['tweet_cleaned'].to_list()):\n    \n    #generate tf-idf for the given document\n    tf_idf_vector = tfidf_transformer.transform(cv.transform([tweet])).tocoo()\n    \n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n\n    #extract only the top n; n here is 10\n    keywords = extract_topn_from_vector(feature_names,sorted_items,10)\n    \n    doc_keywords.append(list(keywords.keys()))\n","8b34fd6f":"df['keywords'] = doc_keywords","23b564ce":"df.head()","c181c1cb":"# CountVectorizer","5c2de64f":"# TFIDFTransformer","cbbe6e02":"# Clean Tweet","29411a02":"# Load Data","305c9646":"## Convert Date column from object type to DateTime type "}}