{"cell_type":{"d37f3b0c":"code","47c59e0e":"code","ca352605":"code","cc1478b0":"code","328e1ca7":"code","71a82854":"code","75837dd0":"code","bcee9bb0":"code","67d9f4a3":"code","ef3d8497":"code","74f3e993":"code","23a70915":"code","55df2f5c":"code","182f1fe0":"code","9562e2ec":"code","02dc8cd9":"code","965b459e":"code","4679be4c":"code","8d7e461f":"code","d73f1d6b":"code","5cded0a0":"code","55400db2":"code","0795d3c6":"code","e0288201":"code","a1df846f":"code","0fdf3ded":"code","de0aeed8":"code","1ea162aa":"code","b20e627a":"code","ea30ddd6":"code","78e00bee":"code","51b9455f":"code","95e082c9":"code","60a5eec0":"code","1970df5c":"code","a2cd88af":"code","d0446c23":"code","2decd96b":"code","9ee2908c":"code","465cd933":"code","dd6d0df3":"code","d1b8f578":"code","a1e188b6":"code","9ceede00":"code","9ed10492":"code","7ad22658":"code","7ce05985":"code","724659c7":"code","b51673f5":"code","c5cf8607":"code","9105bfac":"code","edb69de5":"code","a1b31297":"code","bb6f1286":"code","ec4f481e":"code","1f8b33a2":"code","9f6ab53a":"code","9074bf82":"code","93a8a3d6":"code","6e8b3da2":"markdown","7e499cce":"markdown","88b79850":"markdown","a434dca8":"markdown","903c6e3a":"markdown","bd7648dc":"markdown","dd00a711":"markdown","02ad1d3c":"markdown","03d7475d":"markdown","41a5fa04":"markdown","814dea72":"markdown","e57c6bdf":"markdown","ddeb72b5":"markdown","93b3eb1b":"markdown","19b1fb40":"markdown","a7c4c36e":"markdown","2029a0c2":"markdown","7d64f59f":"markdown","4e5dd29b":"markdown"},"source":{"d37f3b0c":"# Importing time\nimport time\n\n# Importing Markdown(styling) and display \nfrom IPython.display import display,Markdown\n\n# Importing libraries. \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# sklearn libraries\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.model_selection import cross_val_predict\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# warnings ignore\nimport warnings\nwarnings.filterwarnings(\"ignore\")","47c59e0e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca352605":"train_df=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# view First 5 rows with all cols of train data.\ntrain_df.head()","cc1478b0":"# view First 5 rows with all cols of test data.  \ntest_df.head()","328e1ca7":"# Check our data is balanced or imbalanced.\nsns.countplot(x=\"Survived\",data=train_df)\n# Count of each class\ntrain_df.Survived.value_counts()","71a82854":"# Create Target variable and pass the Survived col\ny=train_df[\"Survived\"].values\n# Drop Target variable in train data\ntrain_df.drop([\"Survived\",\"PassengerId\"],inplace=True,axis=1)\ntest_df.drop([\"PassengerId\"],inplace=True,axis=1)\ntrain_df.head()","75837dd0":"# create new variable train and pass \"1\" for train data and \"0\" for test data to retrive test data in future.\ntrain_df[\"train\"]=1\ntest_df[\"train\"]=0\n\n# It's better to combine both train and test data using pandas concat function.\ncombined_df=pd.concat([train_df,test_df])","bcee9bb0":"# view combined data.\ncombined_df.head()","67d9f4a3":"# Check shape of combined data.\ncombined_df.shape","ef3d8497":"from IPython.display import Image\nImage(\"\/kaggle\/input\/missing-values-mechanism\/Missingtheory.png\")","74f3e993":"# Function to get null values info.\ndef null_info(data,string):\n        data_cols=data.columns[data.isna().any()]\n        null_sum=data[data_cols].isna().sum().sort_values(ascending=False)\n        null_dtype=data[data_cols].dtypes\n        null_percent=round(data[data_cols].isna().mean()*100,2)\n        concat=pd.concat([null_sum,null_dtype,null_percent],axis=1,keys=[\"NaN count\",\"Dtypes\",\"%Ages\"],sort=True)\n        display(Markdown(\"### `{}`\".format(string)))\n        display(concat.head()) \n        \n        time.sleep(.5)\n\n        if any(concat[\"%Ages\"] > 50):\n            count=sum(concat[\"%Ages\"] > 50)\n            index_Count=concat[concat[\"%Ages\"]>50].index.tolist()\n            display(Markdown(\"Found `{}` columns **{}** which have more than 50% of missing values in `{}` \".format(count,index_Count,string)))\n            print(\"Removing columns which have more than 50% missing values\")\n            print(\" \")\n            concat.drop(index_Count,inplace=True,axis=0)\n            display(Markdown(\">**Removed** columns `{}` from data\".format(index_Count)))\n        display(concat)\n        return (concat.index,index_Count)","23a70915":"# Null_info function returns cols that are present and cols are deleted.\nnull_df,del_rows=null_info(combined_df,\"Null values on train Data\")","55df2f5c":"# view cols which are not removed by null_info function.\nnull_df","182f1fe0":"# create new dataframe and add those cols with values, because we don't want to spoil the original dataframes\n# by adding,removing, modifying values and doing mutiple operations.\nmodel_df= combined_df[null_df]","9562e2ec":"# view new dataframe\nmodel_df.head()","02dc8cd9":"# Get count of missing values\nmodel_df.isna().sum()","965b459e":"from IPython.display import Image\nImage(\"\/kaggle\/input\/missing-values-mechanism\/Missingtheory.png\")","4679be4c":"# function for imputation and display result.head of 2.\ndef imputation(data):\n    data_numeric=data.select_dtypes(include=np.number)\n    data_categorical=data.select_dtypes(exclude=np.number)\n    display(Markdown(\"## Before imputation\"))\n    display(data[data_numeric.isna().values].head(2))\n    display(data[data_categorical.isna().values].head(2))\n    data.fillna(data_numeric.median(),inplace=True) # imputing with median robust to outliers.\n    for i in data_categorical.columns:\n        data[i].fillna(data[i].value_counts().index[0], inplace=True) # imputing categorical with mode (most frequent).\n    display(Markdown(\"## After imputation\"))\n    display(data[data_numeric.isna().values].head(2))\n    display(data[data_categorical.isna().values].head(2))","8d7e461f":"imputation(model_df)","d73f1d6b":"def remove_exists(data1,data2,del_rows):\n    data1.drop(data2.columns,axis=1,inplace=True)\n    data1.drop(del_rows,axis=1,inplace=True)\n    display(data1.head())\nremove_exists(combined_df,model_df,del_rows)","5cded0a0":"# Get Numeric and Categorical Values into separate\ndef numr_cate(data):\n    data_numr=data.select_dtypes(include=np.number)\n    data_cate=data.select_dtypes(exclude=np.number)\n    display(Markdown(\"## Numeric Columns\"))\n    display(data_numr.head())\n    display(Markdown(\"## Categorical Columns\"))\n    display(data_cate.head())\n    return data_numr,data_cate\ndf_num,df_cate=numr_cate(combined_df)","55400db2":"df_num.head()","0795d3c6":"df_cate.head()","e0288201":"# Drop ticket column\ndf_cate.drop(\"Ticket\",axis=1,inplace=True)","a1df846f":"# combine categorical and numeric dataframe to model_df.\nmodel_df[df_cate.columns]=df_cate.copy()\nmodel_df[df_num.columns]=df_num.copy()","0fdf3ded":"model_df.head()","de0aeed8":"model_df.head()","1ea162aa":"model_df[\"Sex\"]=model_df.Sex.map({\"male\":0,\"female\":1})","b20e627a":"model_df[\"Embarked\"]=model_df.Embarked.map({\"S\":0,\"C\":1,\"Q\":2})","ea30ddd6":"# combine SibSp and Parch to one variable called family\nmodel_df[\"family\"]=model_df[\"SibSp\"] + model_df[\"Parch\"] + 1 ","78e00bee":"# Titles  \ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n# extract titles\nmodel_df['Title'] = model_df.Name.str.extract(' ([A-Za-z]+)\\.', expand= False)\n# replace titles with a more common title or as Rare\nmodel_df['Title'] = model_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr','Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\nmodel_df['Title'] = model_df['Title'].replace('Mlle', 'Miss')\nmodel_df['Title'] = model_df['Title'].replace('Ms', 'Miss')\nmodel_df['Title'] = model_df['Title'].replace('Mme', 'Mrs')\n# convert titles into numbers\nmodel_df['Title'] = model_df['Title'].map(titles)\n# filling NaN with 0, to get safe\nmodel_df['Title'] = model_df['Title'].fillna(0)","51b9455f":"# Binning age with certain values\nmodel_df['Age'] = model_df['Age'].astype(int)\nmodel_df.loc[ model_df['Age'] <= 11, 'Age'] = 0\nmodel_df.loc[(model_df['Age'] > 11) & (model_df['Age'] <= 18), 'Age'] = 1\nmodel_df.loc[(model_df['Age'] > 18) & (model_df['Age'] <= 22), 'Age'] = 2\nmodel_df.loc[(model_df['Age'] > 22) & (model_df['Age'] <= 27), 'Age'] = 3\nmodel_df.loc[(model_df['Age'] > 27) & (model_df['Age'] <= 33), 'Age'] = 4\nmodel_df.loc[(model_df['Age'] > 33) & (model_df['Age'] <= 40), 'Age'] = 5\nmodel_df.loc[(model_df['Age'] > 40) & (model_df['Age'] <= 66), 'Age'] = 6\nmodel_df.loc[ model_df['Age'] > 66, 'Age'] = 6","95e082c9":"model_df['Fare'] = model_df['Fare'].astype(int)\n# Binning Fare \nmodel_df.loc[ model_df['Fare'] <= 7.91, 'Fare'] = 0\nmodel_df.loc[(model_df['Fare'] > 7.91) & (model_df['Fare'] <= 14.454), 'Fare'] = 1\nmodel_df.loc[(model_df['Fare'] > 14.454) & (model_df['Fare'] <= 31), 'Fare'] = 2\nmodel_df.loc[(model_df['Fare'] > 31) & (model_df['Fare'] <= 99), 'Fare'] = 3\nmodel_df.loc[(model_df['Fare'] > 99) & (model_df['Fare'] <= 250), 'Fare'] = 4\nmodel_df.loc[ model_df['Fare'] > 250, 'Fare'] = 5","60a5eec0":"# dropping extra columns \nmodel_df.drop([\"SibSp\",\"Parch\",\"Name\"],axis=1,inplace=True)\nmodel_df.head()","1970df5c":"# Divide train and test data \nX_train=model_df[model_df.train==1]\nX_test=model_df[model_df.train==0]\nY_train=y # target variable\nX_train.drop(\"train\",axis=1,inplace=True)\nX_test.drop(\"train\",axis=1,inplace=True)","a2cd88af":"X_train.head()","d0446c23":"X_test.head()","2decd96b":"X_train.shape,X_test.shape","9ee2908c":"# SGD Classifier\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)","465cd933":"# Random tree classifier\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","dd6d0df3":"# Gradient boosting\nGBC = GradientBoostingClassifier()\nGBC.fit(X_train, Y_train)\nacc_GBC = round(GBC.score(X_train, Y_train) * 100, 2)","d1b8f578":"# Logistic regression.\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)","a1e188b6":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)","9ceede00":"# Guassian navie bayes.\ngaussian = GaussianNB() \ngaussian.fit(X_train, Y_train) \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","9ed10492":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","7ad22658":"# Linear support vector classifier.\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)","7ce05985":"# Decision Tree\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train)* 100, 2)","724659c7":"# Table for scores\nresults = pd.DataFrame({\n'Model': ['Support Vector Machines','GBC', 'KNN', 'Logistic Regression',\n'Random Forest', 'Naive Bayes', 'Perceptron',\n'Stochastic Gradient Decent',\n'Decision Tree'],\n'Score': [acc_linear_svc,acc_GBC, acc_knn, acc_log,\nacc_random_forest, acc_gaussian, acc_perceptron,\nacc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(10)","b51673f5":"# select random forest\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\n# Cross validation == n * (train_test_split) and scoring \nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","c5cf8607":"# Feature importances with random forest\nimportances =pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","9105bfac":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\",\n                                        min_samples_leaf = 1,\n                                        min_samples_split = 10,\n                                        n_estimators=100,\n                                        max_features='auto',\n                                        oob_score= True,\n                                        random_state=1,\n                                        n_jobs=-1)\nrandom_forest.fit(X_train, Y_train)\nprint(\"oob score:\", round(random_forest.score(X_train,Y_train), 4)*100, \"%\")","edb69de5":"# Evaluation of model using classification metrics\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n# cross validation prediction \npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=10)\nconfusion_matrix(Y_train, predictions)","a1b31297":"# Precision and recall score \nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","bb6f1286":"# Getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]","ec4f481e":"# Roc_Auc score \nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","1f8b33a2":"# make prediction and ready for submitting\npredictionss=random_forest.predict(X_test)","9f6ab53a":"# make submission file for kaggle\ntest_df=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission =pd.DataFrame({'PassengerId':test_df.PassengerId,'Survived':predictionss})\nsubmission.head()","9074bf82":"# check for shape\nsubmission.shape","93a8a3d6":"# save to csv file\nsubmission.to_csv(\"submission.csv\",index=False)","6e8b3da2":"- Removed col which have more than 50% of missing values ","7e499cce":"- **Select Algorithm with high score**","88b79850":"- Combined data with 1309 rows and 11 cols","a434dca8":"# 7. Building Model on different Algorithms.","903c6e3a":"# 1. Import Required Libraries.","bd7648dc":"# 4.1 Case Deletion.\n> - Case deletion is performed when the column have more than 50% of missing values in it.","dd00a711":"- **Observartion**: Test data doesn't have Survived(Target) column, we have to predict those values.","02ad1d3c":"# 10. Submission.","03d7475d":"# 4.2 Imputation.\n> - Filling Missing values (holes) in data.\n> - Many ways to deal with missing values.","41a5fa04":"# 6. Feature engineering","814dea72":"# find best parameters using gridsearchcv \n### 1. It's computationally expensive so run in cloud unless you don't have good GPU config.\n### 2. uncomment and run it.\n- param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25],\"min_samples_split\" : [2,4,8,10], \"n_estimators\": [100,150,200]}\n- from sklearn.model_selection import GridSearchCV, cross_val_score\n- rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True,random_state=1, n_jobs=-1)\n- clf = GridSearchCV(estimator=rf,cv=5, param_grid==param_grid, n_jobs=-1)\n- clf.fit(X_train, Y_train)\n- clf.best_params_","e57c6bdf":"# 2. Read data into pandas Dataframe","ddeb72b5":"# 5. Separate numeric and categorical variables for analysis.","93b3eb1b":"- Ticket col is mix of categorical and numerical and have more distinct values.","19b1fb40":"# 3. Combine both train and test data split target variable.","a7c4c36e":" # 8. Hyperparameter Optimization.","2029a0c2":"# 4. Null Values.\n> - Missing values == Information lost.\n> - Three types of Missing data Mechanism.\n        >**1. Missing Completely at Random (MCAR).**\n        >**2. Missing at Random (MAR).**\n        >**3. Missing Not at Random (MNAR).**\n> - [Detailed Explanation of Mechanisms](https:\/\/medium.com\/@danberdov\/dealing-with-missing-data-8b71cd819501)","7d64f59f":"- **Observation**: Survived col is removed from train data","4e5dd29b":"# 9. Evaluation."}}