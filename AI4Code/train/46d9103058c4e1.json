{"cell_type":{"a3c8a464":"code","5cdca3f6":"code","8df747ac":"code","b0f7b869":"code","8aadaf4d":"code","6959080b":"code","8d89e356":"code","f3370c37":"code","d1bf79f9":"code","72be61db":"code","9d97dd94":"code","4901ace1":"code","ba9a7b59":"code","b016be2d":"code","909c2bbb":"code","4068b9ad":"code","4f885b1c":"code","b621e3aa":"code","53900ebe":"code","54d1a8a5":"code","c687eb49":"code","55ef23bd":"code","c56193b7":"code","29adab0d":"code","990a3a16":"code","89d56a38":"code","ad487ef0":"code","9cd9f31b":"code","e107242c":"code","6502952a":"code","7271292f":"code","8112e651":"code","45b76580":"code","a677b0c2":"code","3ec40168":"code","6c8100eb":"code","8eaeaefd":"code","8098a6fb":"code","35399b6f":"code","347f3307":"code","f2c39085":"code","22bfcde3":"code","17b4971a":"code","86ae947e":"code","eaacface":"code","acbe8b7e":"code","6fbc02fb":"code","67cf0567":"code","4bd1b50d":"code","3817fc9a":"markdown","42cb8aaa":"markdown","020efb54":"markdown","b7806de5":"markdown","2d3a8189":"markdown","450916af":"markdown","d13cb272":"markdown","8b2d8519":"markdown","3190444c":"markdown","a07682e0":"markdown","a2bdc48a":"markdown","2078c0ad":"markdown","1519e7e7":"markdown","a619fb3e":"markdown","40da1d7f":"markdown","a128103b":"markdown"},"source":{"a3c8a464":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten,GlobalAveragePooling2D,BatchNormalization, Activation\nimport glob\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\n\n\nimport os\n\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\n","5cdca3f6":"SEED = 42\nDEBUG = False\n\nos.environ['PYTHONHASHSEED'] = str(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","8df747ac":"df = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/train.csv')\ndf.head()","b0f7b869":"print(len(df), df['StudyInstanceUID'].nunique())","8aadaf4d":"print('Number of Records:',len(df), 'Number of Patients:' ,df['PatientID'].nunique())","6959080b":"df['PatientID'].value_counts()","8d89e356":"df.columns","f3370c37":"df['path'] = '..\/input\/ranzcr-clip-catheter-line-classification\/train\/' + df['StudyInstanceUID']+'.jpg'\nlabels = ['ETT - Abnormal', 'ETT - Borderline',\n       'ETT - Normal', 'NGT - Abnormal', 'NGT - Borderline',\n       'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal',\n       'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']\n\nfor label in labels:\n    print(\"#\"*25)\n    print(label)\n    print(df[label].value_counts(normalize=True) * 100)\n","d1bf79f9":"if DEBUG:\n    _, df = train_test_split(df, test_size = 0.4, random_state=SEED, shuffle=True)\n","72be61db":"X_train, X_valid = train_test_split(df, test_size = 0.1, random_state=SEED, shuffle=True)","9d97dd94":"train_ds = tf.data.Dataset.from_tensor_slices((X_train.path.values, X_train[labels].values))\nvalid_ds = tf.data.Dataset.from_tensor_slices((X_valid.path.values, X_valid[labels].values))","4901ace1":"for path, label in train_ds.take(5):\n    print ('Path: {}, Label: {}'.format(path, label))","ba9a7b59":"for path, label in valid_ds.take(5):\n    print ('Path: {}, Label: {}'.format(path, label))","b016be2d":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntarget_size_dim = 300","909c2bbb":"def process_data_train(image_path, label):\n    # load the raw data from the file as a string\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.random_brightness(img, 0.3)\n    img = tf.image.random_flip_left_right(img)\n    img = tf.image.resize(img, [target_size_dim,target_size_dim])\n    return img, label\n\ndef process_data_valid(image_path, label):\n    # load the raw data from the file as a string\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [target_size_dim,target_size_dim])\n    return img, label\n","4068b9ad":"# Set `num_parallel_calls` so multiple images are loaded\/processed in parallel.\ntrain_ds = train_ds.map(process_data_train, num_parallel_calls=AUTOTUNE)\nvalid_ds = valid_ds.map(process_data_valid, num_parallel_calls=AUTOTUNE)","4f885b1c":"for image, label in train_ds.take(1):\n    plt.imshow(image.numpy().astype('uint8'))\n    plt.show()\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", labels[np.argmax(label.numpy())])","b621e3aa":"def configure_for_performance(ds, batch_size = 32):\n    ds = ds.cache('\/kaggle\/dump.tfcache') ## Due to ram and disk limitation of kaggle you cannot set cache to be true. \n                                    ## Setting this to true can increase performance by 2x\n    \n    ds = ds.repeat()\n    ds = ds.shuffle(buffer_size=1024)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\nbatch_size = 32\n\ntrain_ds_batch = configure_for_performance(train_ds)\nvalid_ds_batch = valid_ds.batch(batch_size*2)","53900ebe":"image_batch, label_batch = next(iter(train_ds_batch))","54d1a8a5":"\nplt.figure(figsize=(10, 10))\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n    label = labels[np.argmax(label_batch[i].numpy())]\n    plt.title(label)\n    plt.axis(\"off\")","c687eb49":"data_augmentation = keras.Sequential(\n    [\n        tf.keras.layers.experimental.preprocessing.RandomRotation(0.05, interpolation='nearest'),\n        #tf.keras.layers.experimental.preprocessing.RandomContrast((0.1))\n    ]\n)","55ef23bd":"\nplt.figure(figsize=(10, 10))\nfor i in range(16):\n    augmented_images = data_augmentation(image_batch)\n    ax = plt.subplot(4, 4, i + 1)\n    plt.imshow(augmented_images[i].numpy().astype(\"uint8\"))\n    label = labels[np.argmax(label_batch[i].numpy())]\n    plt.title(label)\n    plt.axis(\"off\")","c56193b7":"## Only available in tf2.3+\n\nfrom tensorflow.keras.applications import EfficientNetB3 \n","29adab0d":"def load_pretrained_model(weights_path, drop_connect, target_size_dim, layers_to_unfreeze=5):\n    model = EfficientNetB3(\n            weights=None, \n            include_top=False, \n            drop_connect_rate=0.4\n        )\n    \n    model.load_weights(weights_path)\n    \n    model.trainable = True\n\n    # for layer in model.layers[-layers_to_unfreeze:]:\n    #     if not isinstance(layer, tf.keras.layers.BatchNormalization): \n    #         layer.trainable = True\n\n    if DEBUG:\n        for layer in model.layers:\n            #print(layer.name, layer.trainable)\n            pass\n\n    return model\n\ndef build_my_model(base_model, optimizer, metrics, loss):\n    \n    inputs = tf.keras.layers.Input(shape=(target_size_dim, target_size_dim, 3))\n    x = data_augmentation(inputs)\n    outputs_eff = base_model(x)\n    global_avg_pooling = GlobalAveragePooling2D()(outputs_eff)\n    dense_1= Dense(256)(global_avg_pooling)\n    bn_1 = BatchNormalization()(dense_1)\n    activation = Activation('relu')(bn_1)\n    dropout = Dropout(0.3)(activation)\n    dense_2 = Dense(len(labels), activation='sigmoid')(dropout)\n\n    my_model = tf.keras.Model(inputs, dense_2)\n    \n    my_model.compile(\n        optimizer=optimizer,\n        loss=loss,\n        metrics=metrics\n    )\n    return my_model\n\n","990a3a16":"#!wget https:\/\/storage.googleapis.com\/keras-applications\/efficientnetb3_notop.h5\n## to get model weights","89d56a38":"model_weights_path = '..\/input\/noisystudent\/efficientnetb3_notop.h5'\nmodel_weights_path","ad487ef0":"drop_rate = 0.4 ## value of dropout to be used in loaded network\nbase_model = load_pretrained_model( model_weights_path, drop_rate, target_size_dim )\n\noptimizer = tf.keras.optimizers.Adam(lr = 1e-4)\nmetrics = tf.keras.metrics.AUC(multi_label=True)\nmy_model = build_my_model(base_model, optimizer, metrics = [metrics], loss='binary_crossentropy')\nmy_model.summary()","9cd9f31b":"weight_path_save = 'best_model.hdf5'\nlast_weight_path = 'last_model.hdf5'\n\ncheckpoint = ModelCheckpoint(weight_path_save, \n                             monitor= 'val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             mode= 'min', \n                             save_weights_only = False)\ncheckpoint_last = ModelCheckpoint(last_weight_path, \n                             monitor= 'val_loss', \n                             verbose=1, \n                             save_best_only=False, \n                             mode= 'min', \n                             save_weights_only = False)\n\n\nearly = EarlyStopping(monitor= 'val_loss', \n                      mode= 'min', \n                      patience=5)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.00001)\ncallbacks_list = [checkpoint, checkpoint_last, early, reduceLROnPlat]","e107242c":"if DEBUG:\n    epochs = 3\nelse:\n    epochs = 10\n    \nprint(f\"Model will train for {epochs} epochs\")","6502952a":"# from sklearn.utils import class_weight\n\n# classes_to_predict =[0, 1, 2, 3, 4]\n# class_weights = class_weight.compute_class_weight(\"balanced\", classes_to_predict, train_gen.labels)\n# class_weights_dict = {i : class_weights[i] for i,label in enumerate(classes_to_predict)}\n\n# print(class_weights_dict)\n","7271292f":"if DEBUG:\n    history = my_model.fit(train_ds_batch, \n                              validation_data = valid_ds_batch, \n                              epochs = epochs, \n                              callbacks = callbacks_list,\n                               steps_per_epoch = 1,\n                           validation_steps = 1\n                               #class_weight=class_weights_dict\n                              )\nelse:\n    steps_per_epoch = len(X_train) \/\/ batch_size\n    history = my_model.fit(train_ds_batch, \n                              validation_data = valid_ds_batch, \n                              epochs = epochs, \n                              callbacks = callbacks_list,\n                                steps_per_epoch = steps_per_epoch\n                               #class_weight=class_weights_dict\n                              )","8112e651":"\ndef plot_hist(hist):\n    plt.figure(figsize=(15,5))\n    local_epochs = len(hist.history[\"auc\"])\n    plt.plot(np.arange(local_epochs), hist.history[\"auc\"], '-o', label='Train AUC',color='#ff7f0e')\n    plt.plot(np.arange(local_epochs), hist.history[\"val_auc\"], '-o',label='Val AUC',color='#1f77b4')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Accuracy',size=14)\n    plt.legend(loc=2)\n    \n    plt2 = plt.gca().twinx()\n    plt2.plot(np.arange(local_epochs) ,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(np.arange(local_epochs) ,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    plt.legend(loc=3)\n    plt.ylabel('Loss',size=14)\n    plt.title(\"Model Accuracy and loss\")\n    \n    #plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    \n    plt.savefig('loss.png')\n    plt.show()","45b76580":"plot_hist(history)","a677b0c2":"my_model.load_weights(weight_path_save) ## load the best model or all your metrics would be on the last run not on the best one","3ec40168":"pred_valid_y = my_model.predict(valid_ds_batch,  verbose = True, workers=4)\npred_valid_y_labels = np.argmax(pred_valid_y, axis=-1)","6c8100eb":"valid_labels = np.concatenate([y.numpy() for x, y in valid_ds_batch], axis=0)","8eaeaefd":"valid_labels","8098a6fb":"import glob","35399b6f":"test_images = glob.glob('..\/input\/ranzcr-clip-catheter-line-classification\/test\/*.jpg')\n#test_images = test_images * 5\n#print(test_images)","347f3307":"df_test = pd.DataFrame(np.array(test_images), columns=['Path'])\ndf_test.head()","f2c39085":"test_ds = tf.data.Dataset.from_tensor_slices((df_test.Path.values))\n\n\ndef process_test(image_path):\n    # load the raw data from the file as a string\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [target_size_dim,target_size_dim])\n    return img\n    \ntest_ds = test_ds.map(process_test, num_parallel_calls=AUTOTUNE).batch(batch_size*2)","22bfcde3":"pred_y = my_model.predict(test_ds, workers=4, verbose=1)\n","17b4971a":"df_ss = pd.DataFrame(pred_y, columns = labels)","86ae947e":"df_test['image_id'] = df_test.Path.str.split('\/').str[-1].str[:-4]\ndf_ss['StudyInstanceUID'] = df_test['image_id']\ndf_ss.head()","eaacface":"df_ss.columns","acbe8b7e":"cols_reordered = ['StudyInstanceUID', 'ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal',\n       'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal',\n       'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n       'Swan Ganz Catheter Present']\n\ndf_order = df_ss[cols_reordered]","6fbc02fb":"df_order.head()","67cf0567":"df_order.to_csv('submission.csv', index=False)","4bd1b50d":"os.chdir(r'..\/working')\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","3817fc9a":"Adding Seed helps to reproduce results. Setting Debug Parameter will run the model on smaller number of epochs to validate the architecture.","42cb8aaa":"In this kernel I would use tf.data plus Keras to build a baseline, this type of baseline can be helpful to you in solving similar problems as well.","020efb54":"## Getting Predictions on Test Set","b7806de5":"## Evaluating Model on Validation Set","2d3a8189":"### Improving Performance","450916af":"## Data Loader using tf.Data","d13cb272":"## Work In Progress. This might not be the final solution\n\n### If you learnt something from this kernel kindly upvote :)**","8b2d8519":"## Creating Model","3190444c":"### Callbacks","a07682e0":"Things to take care of:\n\n1. Data leakage : Ideally I would not want the same patient in training as well as validation set, this indicates data leakage\n2. Will try to solve this problem using efficientdet (considering this as a detection problem) as train annotations are given\n3. Use OOF Cross Validation","a2bdc48a":"Class Distribution of dataset:","2078c0ad":"## Prepare Data","1519e7e7":"## Train Model","a619fb3e":"### Data Generator","40da1d7f":"### Spliting Dataset","a128103b":"## Data Augmentation"}}