{"cell_type":{"b7b96547":"code","b73b3de2":"code","834e17f9":"code","72967a78":"code","daf31ed3":"code","5ad99a46":"code","60e5326b":"code","c616e5b0":"code","89a579cb":"code","db2ae1be":"code","90a39598":"code","9a321dc6":"code","9e733bcc":"code","5bf98587":"code","21db2ad9":"code","1306d335":"code","36867624":"code","97b05c7c":"code","d92f0a89":"code","14d9fd55":"code","3c88268b":"code","5875d851":"code","df827c64":"code","52e5940a":"code","626fb5e9":"code","bd695baa":"code","8f3cc51a":"code","0d5714b7":"code","294afd16":"code","536f7e75":"code","7818a16c":"code","127d4340":"code","03e2cace":"markdown","9d609e4e":"markdown","b762dc67":"markdown","7240ea20":"markdown","3a367b0f":"markdown","205bb562":"markdown","3c8a74ce":"markdown","be3db94f":"markdown","fe8d793c":"markdown","7f22b475":"markdown","4b274057":"markdown","ac1e40da":"markdown","5754d761":"markdown","5b0be805":"markdown","c0b96cba":"markdown","388c97c2":"markdown","10eaa570":"markdown","baef94f7":"markdown","00f73854":"markdown","d4ec2ebb":"markdown","954b8f0f":"markdown","4d84e70f":"markdown"},"source":{"b7b96547":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # Splits data\nfrom xgboost import XGBRegressor # Our model\nfrom sklearn.metrics import mean_absolute_error # Evaluation metric\nfrom sklearn.compose import ColumnTransformer # Transforms variables\nfrom sklearn.pipeline import Pipeline # Creates Pipelines\nfrom sklearn.impute import SimpleImputer # Imputes missing values\nfrom sklearn.preprocessing import OneHotEncoder # Encodes variables\nfrom sklearn.model_selection import cross_val_score # Returns cross validation score\nimport seaborn as sns # For pretty visualisations\nimport matplotlib.pyplot as plt # For plotting\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","b73b3de2":"test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\n\n# The Sale price is right skewed\nplt.hist(train.SalePrice)\nplt.xlabel(\"$\")\nplt.ylabel(\"Count\")\nplt.title(\"Sale Price\")\nplt.show()\n    \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"]) # Normalising target variable (Model performs better)\ntrain = train[train['SalePrice'].astype(bool)] # Don't include houses that weren't sold\n\nplt.hist(train.SalePrice)\nplt.xlabel(\"log1($)\")\nplt.ylabel(\"Count\")\nplt.title(\"Sale Price Post-Transform\")\nplt.show()\n\ntest['train_test'] = 1\ntrain['train_test'] = 0\ntest['SalePrice'] = np.NaN\n\nall_data = pd.concat([test,train])\n# Combining the test data with training data isn't best practice, but it does help in analysing it.\n\n\n\nall_data.columns","834e17f9":"all_data.describe()","72967a78":"all_data.info()","daf31ed3":"all_data.head()","5ad99a46":"#ord_cols = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\n#all_data[ord_cols].head()","60e5326b":"'''\nordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\n\nfor cols in ord_cols:\n    all_data[cols] = all_data[cols].map(ordinal_map)\n\nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    all_data[col] = all_data[col].map(fintype_map)\n    \nall_data['BsmtExposure'] = all_data['BsmtExposure'].map(expose_map)\nall_data['Fence'] = all_data['Fence'].map(fence_map)\n\nord_vars = ord_cols + fin_col + ['BsmtExposure'] + ['Fence']\n'''","c616e5b0":"#ord_vars","89a579cb":"# Initially select text variables\ncat_vars = [x for x in all_data.columns if all_data[x].dtype == 'object']\nprint('Categorical Variables')\nprint(cat_vars)\n\n# Check if there's numerical categorical variables (e.g. quality out_of\/10)\nprint() \nprint('What\\'s left')\nleftover = [x for x in all_data.columns if x not in cat_vars]\nprint(leftover)","db2ae1be":"# Add in missing categorical variables\naddin = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']\nfor x in addin:\n    cat_vars.append(x)","90a39598":"#cat_vars = [x for x in cat_vars if x not in ord_vars]\ncat_vars","9a321dc6":"num_vars = [x for x in all_data.columns if x not in cat_vars]\n#num_vars = [x for x in num_vars if x not in ord_vars]\nnum_vars","9e733bcc":"remove = ['Id','train_test','SalePrice']\nfor x in remove:\n    num_vars.remove(x)\n\nnum_vars","5bf98587":"all_data[cat_vars].nunique()","21db2ad9":"# Iteratively plotting all categorical variables\nfig = plt.figure(figsize=(18,20))\nfor i in range(len(cat_vars)):\n    plt.subplot(12,5,i+1)\n    sns.barplot(all_data[cat_vars[i]].value_counts().index,all_data[cat_vars[i]].value_counts()).set_title(cat_vars[i])\nfig.tight_layout()","1306d335":"# Iteratively plotting all numerical variables\nfig = plt.figure(figsize=(18,20))\nfor i in range(len(num_vars)):\n    plt.subplot(9,5,i+1)\n    plt.hist(all_data[num_vars[i]])\n    plt.title(num_vars[i])\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Count\")\nfig.tight_layout()","36867624":"# Plotting living area to observe outliers\nplt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.title(\"Living Area\")\nplt.xlabel(\"Sq ft\")\nplt.ylabel(\"log1( Sale Price $ )\")","97b05c7c":"\"\"\"\nIt can also be a good idea to remove highly correlated variables to improve model performance.\nHighly correlated variables introduce multicollinearity.\n\"\"\"\nplt.figure(figsize=(14,12))\ncorr = all_data.drop(['Id'], axis = 1).corr()\nsns.heatmap(corr, mask = corr <0.8, cmap = 'Blues', linewidth = 0.5)","d92f0a89":"# Double check that we won't delete any important variables\ncorr[['SalePrice']].sort_values(['SalePrice'], ascending = False) # returns how correlated variables are with the sale price","14d9fd55":"# plotting relationship between variables and saleprice\ntemp_df_num = all_data.select_dtypes(include = [np.number])\nfig = plt.figure(figsize = (20,20))\nfor i in range(len(temp_df_num.columns)):\n    plt.subplot(12,5, i + 1)\n    sns.scatterplot(x = temp_df_num.columns[i], y = temp_df_num['SalePrice'], data = temp_df_num)\n    plt.title(temp_df_num.columns[i])\nfig.tight_layout(pad = 1.0)","3c88268b":"# Features to remove due to collinearity:\n\nredun = ['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars']\nall_data.drop(redun, axis = 1, inplace = True)\n# I can't remember which is in which so I'll delete it off both to be sure\nnum_vars = list(set(num_vars)-set(redun))\ncat_vars = list(set(cat_vars)-set(redun))","5875d851":"# useless Variables\n\nuseless = ['YrSold','MoSold']\nall_data.drop(useless, axis = 1, inplace = True)\nnum_vars = list(set(num_vars)-set(useless))\ncat_vars = list(set(cat_vars)-set(useless))","df827c64":"# Visualising the missing values in each column\nfig = plt.figure(figsize = (25,8))\nsns.heatmap(all_data.isnull(), cbar=False)\n\n'''\nPoolQC, MiscFeature & Alley are very sparse (lots of non-black), it should be good to remove them.\n'''\nempty = ['PoolQC','MiscFeature','Alley']\nall_data.drop(empty, axis = 1, inplace = True)\nnum_vars = list(set(num_vars)-set(empty))\ncat_vars = list(set(cat_vars)-set(empty))","52e5940a":"'''\n# Testing to see if performance improves\n\n# total columns\nall_data['TotalLot'] = all_data['LotFrontage'] + all_data['LotArea']\nall_data['TotalBsmtFin'] = all_data['BsmtFinSF1'] + all_data['BsmtFinSF2']\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['2ndFlrSF']\nall_data['TotalBath'] = all_data['FullBath'] + all_data['HalfBath']\nall_data['TotalPorch'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + all_data['ScreenPorch']\n\nfe = ['TotalLot','TotalBsmtFin','TotalSF','TotalBath','TotalPorch']\nnum_vars = num_vars + fe\nnum_vars\n\n# bins \"does this feature exist? 1(yes) or 0(no)\"\ncolum = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','2ndFlrSF','WoodDeckSF','TotalPorch']\n\nfor col in colum:\n    col_name = col+'_bin'\n    all_data[col_name] = all_data[col].apply(lambda x: 1 if x > 0 else 0)\n    num_vars.append(col_name)\n'''","626fb5e9":"# Preprocessing for numerical data\n# Replace missing numerical variables with 0\nnumerical_transformer = SimpleImputer(strategy='constant', fill_value = 0)\n\n# Preprocessing for categorical data\n# Replaces missing categorical variables with mode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_vars),\n        ('cat', categorical_transformer, cat_vars)\n    ])\n\n# Specifying input variables\ntrain_data = all_data[all_data.train_test == 0]\ntrain_data = train_data[train_data.GrLivArea < 4500] # Removing outliers\n#X = num_vars + cat_vars + ord_vars\nX = num_vars + cat_vars\nX_train = train_data[X]\n\n\n# Specifying target variable\ny_train = train_data.SalePrice\n\n# Specifying test input variables\ntest_data = all_data[all_data.train_test == 1]\nX_test = test_data[X]","bd695baa":"'''\n# Testing scaling Data\n\nfrom sklearn.preprocessing import RobustScaler\n\ncols = X_train.select_dtypes(np.number).columns\ntransformer = RobustScaler().fit(X_train[cols])\nX_train[cols] = transformer.transform(X_train[cols])\nX_test[cols] = transformer.transform(X_test[cols])\n'''","8f3cc51a":"# Our gradient boosted model\n\"\"\"\nn_estimators: The model creates decision trees, this parameter specifies the number of trees (default = 100). Higher = more granular, but greater chance of overfitting.\n\nmax_depth = The maximum tree depth (if confused, look up decision trees)\n\nlearning_rate: Used in gradient descent, controls how much the models weights change in response to errors. Too low = overfitting, too high = underfitting.\n\nsubsample: How much of the training data to randomly sample. 0.7 = 70% of training data is randomly sample each iteration.\n\nseed: A Random Seed, set to a value to be able to replicate the same random numbers each time. Useful for testing, can observe changes.\n\nearly_stopping_rounds: Each time the model iterates it either gets better or it doesn't. In our case if it iterates 5 times and doesn't improve, the model stops training. It helps prevent overfitting.\n\neval_set: Selects your evaluation data. The model runs on the training data and evaluates how accurately it makes predictions during training.\n\nVerbose: If you set it to True, it prints the evaluation metric at each boosting stage.\n\n\n\nA lot of parameter tuning is trial and error, these are the best settings that I happened to find. It could be possible to iteratively find the optimal parameters but it would take a considerable amount of time and you may end up overfitting your model, which would result in it performing poorly when used on other data outside of the training and tesing sets.\n\"\"\"\nmodel = XGBRegressor(n_estimators = 3460,\n                         max_depth = 3,\n                         learning_rate = 0.01,\n                         subsample = 0.7,\n                         seed=1,\n                         early_stopping_rounds=5,\n                         eval_set=[(X_train, y_train)],\n                         verbose=False)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_test)\n\n","0d5714b7":"# Validate model score\n# Do this to compare changes\nscores = cross_val_score(my_pipeline, X_train, y_train)\nprint(\"Model Cross-val Score: \",scores.mean())\n\n# This returns a cross-val score of ~91%","294afd16":"# Creates a dataframe with our results for submission\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': preds})\noutput[\"SalePrice\"] = np.expm1(output[\"SalePrice\"])\noutput.to_csv('submission.csv', index=False)","536f7e75":"# Number of predictions\nlen(output)","7818a16c":"# Combining column names and column scores\nzipped = zip(X_train.columns, my_pipeline['model'].feature_importances_)\ndf = pd.DataFrame(zipped, columns = [\"feature\", \"value\"])\n\n# Sort the features by the absolute value of their coefficient\ndf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\ndf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\ndf = df.sort_values(\"abs_value\", ascending=False)\n\nimport seaborn as sns\nfig, ax = plt.subplots(1, 1, figsize=(12, 7))\nsns.barplot(x=\"feature\",\n            y=\"value\",\n            data=df.head(20),\n           palette=df.head(20)[\"colors\"])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=20)\nax.set_title(\"Top 20 Features\", fontsize=25)\nax.set_ylabel(\"Coefficient\", fontsize=22)\nax.set_xlabel(\"Feature Name\", fontsize=22)\nplt.tight_layout()\nplt.savefig('features.png')","127d4340":"from xgboost import plot_tree\nfig, ax = plt.subplots(figsize=(30, 30))\nplot_tree(my_pipeline['model'], num_trees = my_pipeline['model'].get_booster().best_iteration, ax=ax, rankdir='LR')\nplt.savefig('model.png')","03e2cace":"#### Checking for collinearity","9d609e4e":"## Most Important Features","b762dc67":"#### Categorical variables","7240ea20":"It can also be a good idea to scale your variables, which can improve performance, however it did not in our case so it wasn't included. Below is an example of how that can be done:\n\nsource: [aqx](https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning)","3a367b0f":"#### Ordinal Variables\n\ne.g. var1: bad, ok, good, amazing, perfect <- there's a set order, so we can change them to 1,2,3,4,5 etc.\n\nNormally this is what is considered to be best practice, encoding ordinal variables like this leads to lower dimensionality (fewer variables) and typically better performance. In my case, leaving the ordinal variables unchanged as object variables and one-hot encoding them resulted in better model performance.","205bb562":"# Exploratory Data Analysis","3c8a74ce":"# Feature Engineering\n\nFeature Engineering = creating our own variables.\n\nThis is a method of extracting features from the data which we can use to better train our model. In our case this did not improve our model, so it won't be included. An example of feature engineering can be seen below:\n\nThe feature engineering code that I tested was taken from [aqx](https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning)","be3db94f":"# Predicting House Prices\n\nNote: Although it's not shown, it's important to do a more thorough investigation of your variables prior to building your model. If I included all of the data digging it would be a messy notebook.","fe8d793c":"# Read in data","7f22b475":"We'll need to clean the below up a little by removing outliers (>4500), our model is gradient boosted and can be sensitive to outliers in important variables.","4b274057":"# Evaluating Results","ac1e40da":" Year sold and month sold look to be useless.","5754d761":"## numerical variables","5b0be805":"#### Checking for redundant variables:","c0b96cba":"#### Numerical Variables","388c97c2":"below taken from [aqx](https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning)","10eaa570":"## Seperating Numerical, Categorical & Ordinal Variables","baef94f7":"## Visualising the model","00f73854":"# Import necessary packages","d4ec2ebb":"# Cleaning the data","954b8f0f":"# Modelling & Predicting","4d84e70f":"# Numerical exploration"}}