{"cell_type":{"3b1dcef9":"code","033b5c53":"code","f9a97f88":"code","2efbe2e3":"code","709fbb0f":"code","35bb9afe":"code","8d0df3b8":"code","671f6894":"code","31fb9f7b":"code","81b59c35":"code","4b0fdf12":"code","816e7e18":"code","3d593796":"code","48bfc316":"code","bbf8d069":"code","bc3fafbe":"code","0469c33e":"code","33b60ba2":"code","579e7a13":"code","c07c0e9e":"code","992694cc":"code","bc3ce329":"code","4bd06b62":"code","9c6f995f":"code","9ac6b9a9":"code","f396b036":"code","fbd39627":"code","35502d19":"code","cd66f81e":"code","4779aa6c":"code","0986c4d2":"code","ab9ddc5d":"code","1be71cdd":"code","a3dca2c6":"code","1ae1e769":"code","0c6dcd92":"code","f2ad0d19":"code","8a013030":"code","25a1de79":"code","482bf3f3":"code","ff19fb56":"code","b19b90f6":"code","1f7d5693":"code","dc126cad":"code","9c128d4d":"markdown","77b733c1":"markdown","bffa1cd8":"markdown","7010a8d2":"markdown","8e41132f":"markdown","810674f0":"markdown","b89f7575":"markdown","ec64d1b3":"markdown","85366274":"markdown","2de20c73":"markdown","afeba2fa":"markdown","05a6aeb8":"markdown","5400f4bc":"markdown","e9beee43":"markdown","74564208":"markdown","8581a301":"markdown","ba40b70c":"markdown","804ab028":"markdown","0860d177":"markdown","1ecf6712":"markdown","32bc3f11":"markdown","f1200e87":"markdown","d44744d0":"markdown","caee4a7d":"markdown","8ae27605":"markdown"},"source":{"3b1dcef9":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mlp\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler # OneHE is used for convert 'str' data to numerical\n#from sklearn.impute import SimpleImputer\n#from sklearn.compose import ColumnTransformer\n#from sklearn.pipeline import Pipeline  # multi preproseccing\n#from sklearn.base import BaseEstimator, TransformerMixin  #use to construct eg. AttributesAdder \nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","033b5c53":"Data = pd.read_csv(\"\/kaggle\/input\/cern-electron-collision-data\/dielectron.csv\")\nData.head(10)","f9a97f88":"Data.info()","2efbe2e3":"Data.describe()","709fbb0f":"Data.shape","35bb9afe":"Data.hist(bins=50, figsize=(20, 15))\nplt.show()","8d0df3b8":"corr_matrix = Data.corr()\nprint(corr_matrix['M'].sort_values(ascending=False))","671f6894":"Data['E_total'] = Data['E1'] + Data['E2']\n# Data['pt2_per_E2']=Data['pt2']\/Data['E2']","31fb9f7b":"Data.drop('Event', axis=1, inplace=True)\nData.drop('Run', axis=1, inplace=True)","81b59c35":"corr_matrix = Data.corr()\nprint(corr_matrix['M'].sort_values(ascending=False))","4b0fdf12":"plt.hist(Data['M'], bins=50)\nplt.show()","816e7e18":"plt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='YlGnBu')\nsns.\nplt.title('Correlation Matrix')\nplt.show()","3d593796":"plt.hist(Data['pt1'], bins=100)\nplt.show()","48bfc316":"Data['pt1_cat'] = pd.cut(Data['pt1'],\n                         bins=[0, 10, 20, 30, 40, 50, np.inf],\n                         labels=[1, 2, 3, 4, 5, 6])\nplt.hist(Data['pt1_cat'])\nplt.show()","bbf8d069":"\nst_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in st_split.split(Data, Data['pt1_cat']):\n    train_set = Data.loc[train_index]\n    test_set = Data.loc[test_index]\n    \ntrain_set","bc3fafbe":"for set_ in (train_set, test_set):\n    set_.drop('pt1_cat', axis=1, inplace=True)","0469c33e":"train_set.info()","33b60ba2":"train_set2 = train_set.dropna(subset=['M'], inplace=False)\ntrain_set2.info()","579e7a13":"from pandas.plotting import scatter_matrix\n\nattributes = ['pt1', 'pt2', 'E1', 'E2', 'E_total', 'M']\nscatter_matrix(train_set[attributes], figsize=(20, 15))\nplt.show()","c07c0e9e":"train_features = train_set2.drop('M', axis=1, inplace=False)\ntrain_label = train_set2['M'].copy()","992694cc":"scale = StandardScaler()\ncern_prepared = scale.fit_transform(train_features)","bc3ce329":"#from sklearn.preprocessing import PolynomialFeatures\n\n#cern_poly = PolynomialFeatures(2)\n#cern_prepared_2 = cern_poly.fit_transform(cern_prepared)","4bd06b62":"from sklearn.preprocessing import PolynomialFeatures\ncern_poly = PolynomialFeatures(2)\ndata_perpared = cern_poly.fit_transform(train_features)","9c6f995f":"lin_reg = LinearRegression()\nlin_reg.fit(cern_prepared, train_label)","9ac6b9a9":"scores = cross_val_score(lin_reg, cern_prepared, train_label,\n                         scoring='neg_mean_squared_error',\n                         cv=20)\nlin_rmse = np.sqrt(-scores)\nprint(lin_rmse)","f396b036":"def display_score(scores):\n    print('Scores:', scores),\n    print('Mean:', scores.mean()),\n    print('Std:', scores.std())\n\n\ndisplay_score(lin_rmse)","fbd39627":"tree_reg = DecisionTreeRegressor()\ntree_reg.fit(cern_prepared, train_label)","35502d19":"tree_scores = cross_val_score(tree_reg, cern_prepared, train_label,\n                              cv=10, scoring=\"neg_mean_squared_error\")\ntree_rmse = np.sqrt(-tree_scores)\nprint(tree_rmse)","cd66f81e":"display_score(tree_rmse)","4779aa6c":"forest_reg = RandomForestRegressor()\nforest_reg.fit(cern_prepared, train_label)","0986c4d2":"forest_score = cross_val_score(forest_reg, cern_prepared, train_label,\n                               cv=5, scoring='neg_mean_squared_error')\nforest_rmse = np.sqrt(-forest_score)\nprint(forest_rmse)","ab9ddc5d":"display_score(forest_rmse)","1be71cdd":"lin_reg = LinearRegression()\nlin_reg.fit(data_perpared, train_label)","a3dca2c6":"poly_scores = cross_val_score(lin_reg, data_perpared, train_label,\n                              scoring=\"neg_mean_squared_error\", cv=10)\npoly_scores_rmse = np.sqrt(-poly_scores)\nprint(poly_scores_rmse)","1ae1e769":"def display_score(scores):\n    print('Scores:', scores),\n    print('Mean:', scores.mean()),\n    print('Std:', scores.std())\n\n\ndisplay_score(poly_scores_rmse)","0c6dcd92":"tree_reg = DecisionTreeRegressor()\ntree_reg.fit(data_perpared, train_label)","f2ad0d19":"tree_poly_scores = cross_val_score(tree_reg, data_perpared, train_label,\n                        scoring='neg_mean_squared_error', cv=10)\ntree_poly_rmse = np.sqrt(-tree_poly_scores)\nprint(tree_poly_rmse)","8a013030":"display_score(tree_poly_rmse)","25a1de79":"forest_reg = RandomForestRegressor()\nforest_reg.fit(data_perpared, train_label)","482bf3f3":"forest_prediction = forest_reg.predict(data_perpared)\nforest_mes=mean_squared_error(forest_prediction, train_label)\nforest_rmse=np.sqrt(forest_mes)\nforest_rmse","ff19fb56":"forest_poly_scores = cross_val_score(forest_reg, data_perpared, train_label,\n                        scoring='neg_mean_squared_error', cv=5)\nforest_poly_rmse = np.sqrt(-forest_poly_scores)\nprint(forest_poly_rmse)","b19b90f6":"display_score(forest_poly_rmse)","1f7d5693":"test_data = test_set.dropna(subset=['M'])\ntest_X = test_data.drop(\"M\", axis=1)\ntest_y = test_data[\"M\"].copy()\ntest_X_poly = cern_poly.transform(test_X)","dc126cad":"final_predictions = forest_reg.predict(test_X_poly)\n#final_predictions = tree_reg.predict(test_X_poly)\nfinal_mse = mean_squared_error(final_predictions, test_y)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","9c128d4d":"*Looks very good*, It could be used for our main model due to testing with cross validation,\nLet's try **RandomForestRegressor**","77b733c1":"It is perfect result. We can use it as the best model and now we proceed in **Tunung Hyperparameters**.\\\nBefore it, let's try it for the **test_set**","bffa1cd8":"We could use `Pipeline` in order to prepare our training data with just one command which has been included both `SatandardScalar` and `PolynomialFeatures`. Since we want to show various models, here I did not use that. ","7010a8d2":"Splitting *Training set* and *Test set* based on categorized feature","8e41132f":"Getting some useful information about the data","810674f0":"It could be a good result, but let us examine these models with our *cern_prepared_2* which has been prepared by **PolynomialFeatures**.\\\nFirst for **LinearRegression**","b89f7575":"It seemsthat we have underfitting, that is not good model. Let's try another one. ","ec64d1b3":"Plotting the Histogram can be so useful in order to construct best training and test set.","85366274":"**Adding attribute**","2de20c73":"* **PolynomialFeatures**","afeba2fa":"Wow! It seems that we have second order model.\\\nLet's do it for **DecisionTreeRegressor**","05a6aeb8":"The essential liberaries","5400f4bc":" What about Tree?\\\n**DecisionTreeRegressor** as the second model","e9beee43":"* ****Training Model**** \\\nAs the first Model, We try the **LinearRegresion** model for linear equation (cern_prepared)","74564208":"Seems it containes 85 NaN or NA value in the label data *Mass*","8581a301":"_Do we have Overfitting ?_ Lest's test it with `Cross_val_score`","ba40b70c":"Drop some useless features","804ab028":"Visualizing data","0860d177":"*For getting a more precise regression model, we can tune hyperparameter as our next step...*\\\nWe would make it as soon as possible.","1ecf6712":"Looks scaling data is needed","32bc3f11":"Scaling the data is caused to have more precise model training","f1200e87":"Tha is not bad but it seems we still have underfitting\\\nBut let's try third one, **RandomForestRegressor**","d44744d0":"* # **Electron Mass Prediction**","caee4a7d":"> **PolynomialFeatures** are those features created by raising existing features to an exponent. The **degree** of the polynomial is used to control the number of features added, e.g. a degree of 3 will add two new variables for each input variable. Typically a small degree is used such as 2 or 3.","8ae27605":"Now, we have to separate the *label data* and the *predictors*"}}