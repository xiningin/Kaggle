{"cell_type":{"5884fd9c":"code","c71afcf9":"code","b6636201":"code","cfceeaae":"code","8e50d2e2":"code","ea9c8123":"code","3fcfaa93":"code","83c21f14":"code","42acb4b7":"code","e49c2a2e":"code","62181043":"code","fb327560":"code","e68d215e":"code","7f79849f":"code","9ddbebf8":"code","7285438a":"code","c2be30c2":"code","2e69025e":"code","4439db94":"code","6789f154":"code","e416f0bb":"code","722b4b13":"code","66b64699":"code","842ee497":"code","4cc810c3":"code","9cd8b522":"code","4605cf06":"code","b597a801":"code","627b760a":"code","1e5d694c":"code","13e2bdf6":"code","9c86dc81":"code","d7947fdb":"code","c8340aee":"code","8b6e83b9":"code","711eb6b1":"code","66e8f28b":"code","bb6e95f8":"code","79865154":"code","aecde12d":"code","d3642f84":"code","c71640dd":"code","5877bd73":"code","e696ba4c":"code","252e0e45":"code","3890ddc9":"code","3a41dc98":"code","458a9a25":"code","a3f21e2e":"code","09a8ad1c":"code","fb8d9f24":"code","42062d80":"code","7099a785":"code","8d59e0cd":"code","e5c25789":"code","d761ebc6":"code","ee1ae8ba":"code","d4eac90a":"code","9b7659da":"code","bc00217d":"code","26ca0507":"code","9d4e0422":"code","d2cbc63b":"code","4f009840":"code","d0a3c1c4":"code","e97f3cf5":"code","93bb5ea8":"code","caf8dc15":"code","c149f326":"code","ee958cda":"code","aac52bfc":"code","51782435":"code","85f2efe3":"code","63682c2d":"code","d6b30a2c":"code","52617c83":"code","63cff00a":"code","3b87cf69":"code","873e51c0":"code","c2aaa167":"code","7f54ea1c":"code","4bc17630":"code","f5b436ce":"code","00833668":"code","5e88e2e1":"code","46938b10":"markdown","3ee21404":"markdown","615641cb":"markdown","ee8cff44":"markdown","7cfd5835":"markdown","e9df9499":"markdown","0c84b657":"markdown","ef80d9a0":"markdown","929b024d":"markdown","7f8f6347":"markdown","e061b620":"markdown","d4fe4a49":"markdown","789688e9":"markdown","c80a6241":"markdown","134fdb57":"markdown","9de09948":"markdown","c7299202":"markdown","dfc6e0a2":"markdown","393c41b1":"markdown","12abc667":"markdown","9f0e73bb":"markdown","0d40b929":"markdown","e3cee712":"markdown","bcf42a87":"markdown","a97b3ad0":"markdown","04393aea":"markdown","85a9f9b5":"markdown","b3984abe":"markdown","092dea65":"markdown","62f13bbf":"markdown","b709cf18":"markdown","412fe99b":"markdown","4831b274":"markdown","9581111b":"markdown","c229d7d6":"markdown","07d58152":"markdown","294dceb5":"markdown","35a0ce33":"markdown","d829db80":"markdown","042adf06":"markdown","317fd921":"markdown","0d011aa1":"markdown","02895822":"markdown","5829fa71":"markdown","1399000d":"markdown","450f801a":"markdown","246837ed":"markdown","805664b6":"markdown","bb3536d8":"markdown","7ba4a106":"markdown","e9103acf":"markdown","e9e60337":"markdown","54ec45ce":"markdown","b6083c02":"markdown","0b06d8e4":"markdown","54eb1c0a":"markdown","cb682162":"markdown","05b8d2c9":"markdown","96eadb55":"markdown","ff5228bf":"markdown"},"source":{"5884fd9c":"!apt install apt-utils psmisc -y\n!killall -9 ngrok\n!rm -rf *ngrok*\n!wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n!mkdir -p ngrok-dir\n!mv ngrok ngrok-dir\/\n# \u03a3\u03c4\u03b7\u03bd \u03b5\u03c0\u03cc\u03bc\u03b5\u03bd\u03b7 \u03b3\u03c1\u03b1\u03bc\u03bc\u03ae \u03b2\u03ac\u03bb\u03c4\u03b5 \u03c4\u03bf \u03b4\u03b9\u03ba\u03cc \u03c3\u03b1\u03c2 authentication token \u03b1\u03c0\u03cc \u03c4\u03bf ngrok\n!ngrok-dir\/ngrok authtoken 1pQXAmawXFhcccxNaTqxBybnA8M_8ab6i8BfThM2qQcPHkUKv\n\nLOG_DIR = 'tb_log\/'\n!mkdir -p LOG_DIR\n\nget_ipython().system_raw(\n    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n    .format(LOG_DIR)\n)\nget_ipython().system_raw('ngrok-dir\/ngrok http 6006 &')\n! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\nprint(\"Wait 10-15 seconds and then click the URL above to open TensorBoard\")","c71afcf9":"!pip install --upgrade pip\n!pip install --upgrade stable_baselines3[extra]\n# we need a specific version of gym because of this issue: https:\/\/github.com\/DLR-RM\/stable-baselines3\/issues\/294\n!pip install gym==0.17.3","b6636201":"from stable_baselines3.common.env_util import make_atari_env\nfrom stable_baselines3.common.vec_env import VecFrameStack","cfceeaae":"!pip install sb3-contrib","8e50d2e2":"import datetime # For filenames while logging\nmax_steps=4000\nfrom stable_baselines3 import DQN, A2C, PPO\nfrom sb3_contrib import QRDQN","ea9c8123":"atari_env_name='PitfallDeterministic-v4'","3fcfaa93":"\n# \u039c\u03b5 \u03c4\u03b9\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03bf\u03cd\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03af\u03b4\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd Deepmind\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN \u03b4\u03b5\u03bd \u03b5\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 multi envs\nenv = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\n# \u03bc\u03b5 \u03ad\u03bd\u03b1 frame \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7 \u03b8\u03ad\u03c3\u03b7, \u03bc\u03b5 \u03b4\u03cd\u03bf \u03c4\u03b7\u03bd \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03bc\u03b5 \u03c4\u03c1\u03af\u03b1 \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b5 \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c4\u03bf\u03bd \u03c1\u03c5\u03b8\u03bc\u03cc \u03bc\u03b5\u03c4\u03b1\u03b2\u03bf\u03bb\u03ae\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7\u03c2 (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","83c21f14":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model_mlp = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn_mlp_pitfall\"+atari_env_name)","42acb4b7":"model_name='dqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model_cnn = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\ndqn_model_cnn.learn(total_timesteps=max_steps)\n\ndqn_model_cnn.save(\"dqn_cnn_pitfall\"+atari_env_name)","e49c2a2e":"model_name='a2c-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model_mlp = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\na2c_model_mlp.learn(total_timesteps=max_steps)\n\na2c_model_mlp.save(\"a2c_mlp_pitfall\"+atari_env_name)","62181043":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model_cnn =A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name)","fb327560":"model_name='ppo-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nppo_model_mlp = PPO('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\nppo_model_mlp.learn(total_timesteps=max_steps)\n\nppo_model_mlp.save(\"ppo_mlp_pitfall\"+atari_env_name)","e68d215e":"model_name='ppo-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nppo_model_cnn = PPO('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\nppo_model_cnn.learn(total_timesteps=max_steps)\n\nppo_model_cnn.save(\"ppo_cnn_pitfall\"+atari_env_name)","7f79849f":"model_name='qrdqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nqrdqn_model_mlp = QRDQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn_mlp_pitfall\"+atari_env_name)","9ddbebf8":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nqrdqn_model_cnn = QRDQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\nqrdqn_model_cnn.learn(total_timesteps=max_steps)\n\nqrdqn_model_cnn.save(\"qrdqn_cnn_pitfall\"+atari_env_name)","7285438a":"####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################","c2be30c2":"atari_env_name='Pitfall-v4'","2e69025e":"# \u039c\u03b5 \u03c4\u03b9\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03bf\u03cd\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03af\u03b4\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd Deepmind\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN \u03b4\u03b5\u03bd \u03b5\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 multi envs\nenv = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\n# \u03bc\u03b5 \u03ad\u03bd\u03b1 frame \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7 \u03b8\u03ad\u03c3\u03b7, \u03bc\u03b5 \u03b4\u03cd\u03bf \u03c4\u03b7\u03bd \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03bc\u03b5 \u03c4\u03c1\u03af\u03b1 \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b5 \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c4\u03bf\u03bd \u03c1\u03c5\u03b8\u03bc\u03cc \u03bc\u03b5\u03c4\u03b1\u03b2\u03bf\u03bb\u03ae\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7\u03c2 (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","4439db94":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model_mlp = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn_mlp_pitfall\"+atari_env_name)","6789f154":"model_name='dqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model_cnn = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\ndqn_model_cnn.learn(total_timesteps=max_steps)\n\ndqn_model_cnn.save(\"dqn_cnn_pitfall\"+atari_env_name)","e416f0bb":"model_name='a2c-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model_mlp = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\n\na2c_model_mlp.learn(total_timesteps=max_steps)\n\na2c_model_mlp.save(\"a2c_mlp_pitfall\"+atari_env_name)","722b4b13":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model_cnn =A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name)","66b64699":"model_name='ppo-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nppo_model_mlp = PPO('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\nppo_model_mlp.learn(total_timesteps=max_steps)\n\nppo_model_mlp.save(\"ppo_mlp_pitfall\"+atari_env_name)","842ee497":"model_name='ppo-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nppo_model_cnn = PPO('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\nppo_model_cnn.learn(total_timesteps=max_steps)\n\nppo_model_cnn.save(\"ppo_cnn_pitfall\"+atari_env_name)","4cc810c3":"model_name='qrdqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nqrdqn_model_mlp = QRDQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn_mlp_pitfall\"+atari_env_name)","9cd8b522":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nqrdqn_model_cnn = QRDQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\nqrdqn_model_cnn.learn(total_timesteps=max_steps)\n\nqrdqn_model_cnn.save(\"qrdqn_cnn_pitfall\"+atari_env_name)","4605cf06":"####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################\n####################################################################################","b597a801":"atari_env_name='PitfallNoFrameskip-v4'","627b760a":"# \u039c\u03b5 \u03c4\u03b9\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03bf\u03cd\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03af\u03b4\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd Deepmind\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN \u03b4\u03b5\u03bd \u03b5\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 multi envs\nenv = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\n# \u03bc\u03b5 \u03ad\u03bd\u03b1 frame \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7 \u03b8\u03ad\u03c3\u03b7, \u03bc\u03b5 \u03b4\u03cd\u03bf \u03c4\u03b7\u03bd \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03bc\u03b5 \u03c4\u03c1\u03af\u03b1 \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b5 \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c4\u03bf\u03bd \u03c1\u03c5\u03b8\u03bc\u03cc \u03bc\u03b5\u03c4\u03b1\u03b2\u03bf\u03bb\u03ae\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7\u03c2 (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","1e5d694c":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model_mlp = DQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn_mlp_pitfall\"+atari_env_name)","13e2bdf6":"model_name='dqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\ndqn_model_cnn = DQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\ndqn_model_cnn.learn(total_timesteps=max_steps)\n\ndqn_model_cnn.save(\"dqn_cnn_pitfall\"+atari_env_name)","9c86dc81":"model_name='a2c-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model_mlp = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\na2c_model_mlp.learn(total_timesteps=max_steps)\n\na2c_model_mlp.save(\"a2c_mlp_pitfall\"+atari_env_name)","d7947fdb":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\na2c_model_cnn =A2C('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name)","c8340aee":"model_name='ppo-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nppo_model_mlp = PPO('MlpPolicy', env, verbose=1, tensorboard_log=model_log)\n\n\nppo_model_mlp.learn(total_timesteps=max_steps)\n\nppo_model_mlp.save(\"ppo_mlp_pitfall\"+atari_env_name)","8b6e83b9":"model_name='ppo-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nppo_model_cnn = PPO('CnnPolicy', env, verbose=1, tensorboard_log=model_log)\n\nppo_model_cnn.learn(total_timesteps=max_steps)\nppo_model_cnn.save(\"ppo_cnn_pitfall\"+atari_env_name)","711eb6b1":"model_name='qrdqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nqrdqn_model_mlp = QRDQN('MlpPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn_mlp_pitfall\"+atari_env_name)","66e8f28b":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp\n\nqrdqn_model_cnn = QRDQN('CnnPolicy', env, verbose=1, tensorboard_log=model_log, buffer_size=100000)\n\n\nqrdqn_model_cnn.learn(total_timesteps=max_steps)\n\nqrdqn_model_cnn.save(\"qrdqn_cnn_pitfall\"+atari_env_name)","bb6e95f8":"from stable_baselines3.common.evaluation import evaluate_policy","79865154":"model = algoFN.load(f\".\/ppo_mlp_pitfallPitfallNoFrameskip-v4.zip\", verbose=1)\nevaluate_policy(model, test_env, n_eval_episodes=10)","aecde12d":"atari_env_name='PitfallDeterministic-v4'\n\n# \u039c\u03b5 \u03c4\u03b9\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03bf\u03cd\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03af\u03b4\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd Deepmind\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN \u03b4\u03b5\u03bd \u03b5\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 multi envs\nenv = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\n# \u03bc\u03b5 \u03ad\u03bd\u03b1 frame \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7 \u03b8\u03ad\u03c3\u03b7, \u03bc\u03b5 \u03b4\u03cd\u03bf \u03c4\u03b7\u03bd \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03bc\u03b5 \u03c4\u03c1\u03af\u03b1 \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b5 \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c4\u03bf\u03bd \u03c1\u03c5\u03b8\u03bc\u03cc \u03bc\u03b5\u03c4\u03b1\u03b2\u03bf\u03bb\u03ae\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7\u03c2 (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","d3642f84":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\n\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.0001\n                  ,n_steps = 20\n                  ,gamma = 0.9\n                  ,use_sde =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"1\")","c71640dd":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"2\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.9\n                  ,use_sde =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"2\")","5877bd73":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"3\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.9\n                  ,use_sde =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"3\")","e696ba4c":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"4\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 10\n                  ,gamma = 0.9\n                  ,use_sde =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"4\")","252e0e45":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"5\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.95\n                  ,use_sde =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"5\")","3890ddc9":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"6\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.95\n                  ,use_sde =True\n                  ,use_rms_prop =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"6\")","3a41dc98":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"6\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"6\")","458a9a25":"model_name='qrdqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.90\n                 ,learning_starts=1000\n                , tau=0.9\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-MlpPolicy\"+atari_env_name+\"1\")","a3f21e2e":"model_name='qrdqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.95\n                 ,learning_starts=1000\n                , tau=0.8\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-MlpPolicy\"+atari_env_name+\"1\")","09a8ad1c":"model_name='qrdqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.99\n                 ,learning_starts=1000\n                , tau=0.7\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-MlpPolicy\"+atari_env_name+\"1\")","fb8d9f24":"atari_env_name='Pitfall-v4'\n\n# \u039c\u03b5 \u03c4\u03b9\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03bf\u03cd\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03af\u03b4\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd Deepmind\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN \u03b4\u03b5\u03bd \u03b5\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 multi envs\nenv = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\n# \u03bc\u03b5 \u03ad\u03bd\u03b1 frame \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7 \u03b8\u03ad\u03c3\u03b7, \u03bc\u03b5 \u03b4\u03cd\u03bf \u03c4\u03b7\u03bd \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03bc\u03b5 \u03c4\u03c1\u03af\u03b1 \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b5 \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c4\u03bf\u03bd \u03c1\u03c5\u03b8\u03bc\u03cc \u03bc\u03b5\u03c4\u03b1\u03b2\u03bf\u03bb\u03ae\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7\u03c2 (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","42062d80":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.99\n                 ,learning_starts=1000\n                , tau=0.7\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"1\")","7099a785":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"2\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.8\n                 ,learning_starts=1000\n                , tau=0.7\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"2\")","8d59e0cd":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"3\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.99\n\n                , tau=0.9\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"3\")","e5c25789":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"4\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.99\n                 ,learning_starts=500\n                , tau=0.9\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"4\")","d761ebc6":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"5\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.99\n\n                , tau=0.3\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"5\")","ee1ae8ba":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"6\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"6\")","d4eac90a":"model_name='qrdqn-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"6\"\nmax_steps=int(10**5)\nqrdqn_model_mlp =QRDQN('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.8\n                 ,learning_starts=1000\n                , tau=0.7\n                    ,buffer_size=100000)\n\n\nqrdqn_model_mlp.learn(total_timesteps=max_steps)\n\nqrdqn_model_mlp.save(\"qrdqn-CnnPolicy\"+atari_env_name+\"6\")","9b7659da":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.8\n                 ,learning_starts=1000\n                , tau=0.7,\n                     train_freq=10\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"1\")","bc00217d":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"2\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.8\n                 ,learning_starts=1000\n                , tau=0.7\n                     ,train_freq=4\n                     ,gradient_steps=-1\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"2\")","26ca0507":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"3\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.8\n\n                , tau=0.7,\n                     train_freq=4,\n                     gradient_steps=-1\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"3\")","9d4e0422":"atari_env_name='PitfallNoFrameskip-v4'\n\n# \u039c\u03b5 \u03c4\u03b9\u03c2 \u03c3\u03c5\u03bd\u03b1\u03c1\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03bf\u03cd\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03af\u03b4\u03b9\u03b1 \u03c0\u03c1\u03bf\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1 \u03bc\u03b5 \u03c4\u03b7\u03bd Deepmind\n\n# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN \u03b4\u03b5\u03bd \u03b5\u03c0\u03b9\u03c4\u03c1\u03ad\u03c0\u03b5\u03b9 multi envs\nenv = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\n# \u03bc\u03b5 \u03ad\u03bd\u03b1 frame \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7 \u03b8\u03ad\u03c3\u03b7, \u03bc\u03b5 \u03b4\u03cd\u03bf \u03c4\u03b7\u03bd \u03c4\u03b1\u03c7\u03cd\u03c4\u03b7\u03c4\u03b1, \u03bc\u03b5 \u03c4\u03c1\u03af\u03b1 \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03bc\u03b5 \u03c4\u03ad\u03c3\u03c3\u03b5\u03c1\u03b1 \u03c4\u03bf\u03bd \u03c1\u03c5\u03b8\u03bc\u03cc \u03bc\u03b5\u03c4\u03b1\u03b2\u03bf\u03bb\u03ae\u03c2 \u03c4\u03b7\u03c2 \u03b5\u03c0\u03b9\u03c4\u03ac\u03c7\u03c5\u03bd\u03c3\u03b7\u03c2 (jerk)\nenv = VecFrameStack(env, n_stack=4)\n# Test environment must be unique\ntest_env = make_atari_env(atari_env_name, n_envs=1)\n# Frame-stacking with 4 frames\ntest_env = VecFrameStack(test_env, n_stack=4)","d2cbc63b":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.8\n\n                , tau=0.7,\n                     train_freq=4,\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"1\")","4f009840":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"2\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"2\")","d0a3c1c4":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"3\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 40\n                  ,gamma = 0.9\n                     train_freq=4,\n                     gradient_steps=-1\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"3\")","e97f3cf5":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"4\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.8\n                , tau=0.9,\n                     train_freq=10,\n\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"4\")","93bb5ea8":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"5\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 40\n                  ,gamma = 0.3\n                     train_freq=4,\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"5\")","caf8dc15":"model_name='dqn-MlpPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"6\"\nmax_steps=int(10**5)\ndqn_model_mlp =QRDQN('MlpPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                ,tau=0.7,\n                    ,buffer_size=100000)\n\n\ndqn_model_mlp.learn(total_timesteps=max_steps)\n\ndqn_model_mlp.save(\"dqn-MlpPolicy\"+atari_env_name+\"6\")","c149f326":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"1\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.001\n                  ,n_steps = 20\n                  ,gamma = 0.95\n                  ,use_sde =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"1\")","ee958cda":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"2\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log\n                  ,learning_rate =0.01\n                  ,n_steps = 20\n                  ,gamma = 0.95\n                  ,use_sde =True\n                  ,use_rms_prop =False)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"2\")","aac52bfc":"model_name='a2c-CnnPolicy'+atari_env_name\ntime_stamp=datetime.datetime.now().strftime(\"-%Y%m%d-%H%M%S\")\nmodel_log= LOG_DIR + model_name + time_stamp+\"3\"\nmax_steps=int(10**5)\na2c_model_cnn =A2C('CnnPolicy', env, verbose=0,device='cuda', tensorboard_log=model_log)\n\n\na2c_model_cnn.learn(total_timesteps=max_steps)\n\na2c_model_cnn.save(\"a2c_cnn_pitfall\"+atari_env_name+\"3\")","51782435":"!wget --no-check-certificate 'https:\/\/www.dropbox.com\/s\/opbqvg1bbqfrxet\/a2c_pong.zip?dl=1' -O a2c_pong.zip","85f2efe3":"from stable_baselines3 import A2C\na2c_model = A2C.load(\".\/a2c_cnn_pitfallPitfall-v4.zip\", verbose=1)","63682c2d":"# \u03a0\u03c1\u03bf\u03c3\u03bf\u03c7\u03ae, \u03c4\u03bf n_envs \u03c0\u03c1\u03ad\u03c0\u03b5\u03b9 \u03bd\u03b1 \u03b5\u03af\u03bd\u03b1\u03b9 \u03c0\u03ac\u03bd\u03c4\u03b1 \u03c4\u03bf \u03af\u03b4\u03b9\u03bf (\u03b1\u03c5\u03c4\u03cc \u03bc\u03b5 \u03c4\u03bf \u03bf\u03c0\u03bf\u03af\u03bf \u03ad\u03b3\u03b9\u03bd\u03b5 \u03b7 \u03b1\u03c1\u03c7\u03b9\u03ba\u03ae \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 \u03c4\u03bf\u03c5 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf\u03c5)\nnew_env = make_atari_env(atari_env_name, n_envs=4)\n# Frame-stacking with 4 frames\nnew_env = VecFrameStack(new_env, n_stack=4)\n# \u03a4\u03bf \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf \u03b4\u03b5\u03bd \u03ad\u03c7\u03b5\u03b9 \u03bc\u03b1\u03b6\u03af \u03c4\u03bf\u03c5 \u03c4\u03bf \u03c0\u03b5\u03c1\u03b9\u03b2\u03ac\u03bb\u03bb\u03bf\u03bd, \u03c0\u03c1\u03ad\u03c0\u03b5\u03b9 \u03bd\u03b1 \u03c4\u03bf\u03c5 \u03c4\u03bf \u03b1\u03bd\u03b1\u03b8\u03ad\u03c3\u03bf\u03c5\u03bc\u03b5 \u03be\u03b1\u03bd\u03ac.\na2c_model.set_env(new_env)","d6b30a2c":"max_steps=1000\na2c_model.learn(total_timesteps=max_steps)","52617c83":"mean_reward, std_reward = evaluate_policy(a2c_model, test_env, n_eval_episodes=10)\nprint(f\"Eval reward: {mean_reward} (+\/-{std_reward})\")","63cff00a":"a2c_model.save(\"a2c_pong\")","3b87cf69":"!apt-get install ffmpeg freeglut3-dev xvfb  -y # For visualization","873e51c0":"# Set up fake display; otherwise rendering will fail\nimport os\nos.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\nos.environ['DISPLAY'] = ':1'","c2aaa167":"video_folder = '\/kaggle\/working\/videos'","7f54ea1c":"import base64\nfrom pathlib import Path\n\nfrom IPython import display as ipythondisplay\n\ndef show_videos(video_path='', prefix=''):\n  \"\"\"\n  Taken from https:\/\/github.com\/eleurent\/highway-env\n\n  :param video_path: (str) Path to the folder containing videos\n  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n  \"\"\"\n  html = []\n  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n      video_b64 = base64.b64encode(mp4.read_bytes())\n      html.append('''<video alt=\"{}\" autoplay \n                    loop controls style=\"height: 400px;\">\n                    <source src=\"data:video\/mp4;base64,{}\" type=\"video\/mp4\" \/>\n                <\/video>'''.format(mp4, video_b64.decode('ascii')))\n  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))","4bc17630":"import numpy as np\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n\ndef record_video(eval_env, model, video_length=500, prefix='', video_folder=video_folder):\n  \"\"\"\n  :param env_id: (str)\n  :param model: (RL model)\n  :param video_length: (int)\n  :param prefix: (str)\n  :param video_folder: (str)\n  \"\"\"\n  # Start the video at step=0 and record video_length steps\n  env = VecVideoRecorder(eval_env, video_folder=video_folder,\n                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n                              name_prefix=prefix)\n\n  obs = env.reset()\n  for _ in range(video_length):\n    #action, _ = model.predict(obs)\n    #\u03b8\u03b1 \u03b5\u03c0\u03b9\u03bb\u03ad\u03be\u03bf\u03c5\u03bc\u03b5 \u03c4\u03b7\u03bd \u03b5\u03bd\u03ad\u03c1\u03b3\u03b5\u03b9\u03b1 \u03bc\u03b5 \u03c4\u03b9\u03c2 \u03bc\u03b5\u03b3\u03b1\u03bb\u03cd\u03c4\u03b5\u03c1\u03b5\u03c2 \u03c0\u03b9\u03b8\u03b1\u03bd\u03cc\u03c4\u03b7\u03c4\u03b5\u03c2\n    actions, _ = model.predict(obs, deterministic=True)\n    obs, _, _, _ = env.step(actions)\n\n  # Close the video recorder\n  env.close()","f5b436ce":"model = algoFN.load(f\".\/ppo_mlp_pitfallPitfallNoFrameskip-v4.zip\", verbose=1)","00833668":"record_video(test_env, a2c_model_cnn, video_length=5000, prefix='a2c_pitfall')","5e88e2e1":"show_videos(video_path = video_folder, prefix='a2c')","46938b10":"## A2C","3ee21404":"#### MLP","615641cb":"## \u0394\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03af\u03b1 \u03c0\u03b5\u03c1\u03b9\u03b2\u03ac\u03bb\u03bb\u03bf\u03bd\u03c4\u03bf\u03c2","ee8cff44":"#### dqn","7cfd5835":"## PPO","e9df9499":"## \u03a3\u03c5\u03bd\u03ad\u03c7\u03b9\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2","0c84b657":"#### CNN","ef80d9a0":"## A2C","929b024d":"# Deterministic-v4","7f8f6347":"#### MLP","e061b620":"#### MLP","d4fe4a49":"## \u039c\u03b1\u03ba\u03c1\u03bf\u03c7\u03c1\u03cc\u03bd\u03b9\u03b1 \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\n\n\u03a4\u03bf \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf \u03b8\u03b1 \u03b1\u03c0\u03bf\u03b8\u03b7\u03ba\u03b5\u03c5\u03b8\u03b5\u03af \u03c9\u03c2 zip \u03ba\u03b1\u03b9 \u03bc\u03c0\u03bf\u03c1\u03b5\u03af\u03c4\u03b5 \u03bd\u03b1 \u03c4\u03bf \u03ba\u03b1\u03c4\u03b5\u03b2\u03ac\u03c3\u03b5\u03c4\u03b5 \u03c4\u03bf\u03c0\u03b9\u03ba\u03ac \u03b1\u03c0\u03cc \u03c4\u03bf \u03b4\u03b5\u03be\u03af sidebar \u03c4\u03bf\u03c5 Kaggle \u03c3\u03c4\u03bf \"Data\" \u03ba\u03b1\u03b9 \u03bc\u03b5\u03c4\u03ac \u03c3\u03c4\u03bf ellipsis menu \u03c0\u03ac\u03bd\u03c9 \u03c3\u03c4\u03bf filename.\n\n\u039b\u03cc\u03b3\u03c9 \u03c4\u03c9\u03bd \u03c0\u03b5\u03c1\u03b9\u03bf\u03c1\u03b9\u03c3\u03bc\u03ce\u03bd \u03c4\u03c9\u03bd cloud notebooks, \u03c0\u03c1\u03bf\u03ba\u03b5\u03b9\u03bc\u03ad\u03bd\u03bf\u03c5 \u03bd\u03b1 \u03bc\u03c0\u03bf\u03c1\u03ad\u03c3\u03b5\u03c4\u03b5 \u03bd\u03b1 \u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03cd\u03c3\u03b5\u03c4\u03b5 \u03bc\u03b1\u03ba\u03c1\u03bf\u03c7\u03c1\u03cc\u03bd\u03b9\u03b1 \u03c4\u03b1 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 \u03c3\u03b1\u03c2, \u03b8\u03b1 \u03c0\u03c1\u03ad\u03c0\u03b5\u03b9 \u03bd\u03b1 \u03c4\u03b1 \u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03cd\u03b5\u03c4\u03b5, \u03bd\u03b1 \u03c3\u03ce\u03b6\u03b5\u03c4\u03b5 \u03c4\u03b1 zip files \u03ba\u03b1\u03b9 \u03c3\u03c4\u03b7 \u03c3\u03c5\u03bd\u03ad\u03c7\u03b5\u03b9\u03b1 \u03bd\u03b1 \u03c4\u03b1 \u03c6\u03bf\u03c1\u03c4\u03ce\u03bd\u03b5\u03c4\u03b5 \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03c3\u03c5\u03bd\u03b5\u03c7\u03af\u03b6\u03b5\u03c4\u03b5 \u03c4\u03b7\u03bd \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 \u03cc\u03c0\u03c9\u03c2 \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03c3\u03c4\u03bf \u03b5\u03c0\u03cc\u03bc\u03b5\u03bd\u03bf section.","789688e9":"## Tensorboard \u03ba\u03b1\u03b9 ngrok\n\n\u0391\u03bd\u03c4\u03b9\u03ba\u03b1\u03c4\u03b1\u03c3\u03c4\u03ae\u03c3\u03c4\u03b5 \u03c4\u03bf authentication token \u03c3\u03b1\u03c2 \u03c0\u03bf\u03c5 \u03c0\u03b1\u03af\u03c1\u03bd\u03b5\u03c4\u03b5 \u03b1\u03c0\u03cc \u03c4\u03bf [ngrok](https:\/\/ngrok.com\/) \u03c3\u03c4\u03b7 \u03b3\u03c1\u03b1\u03bc\u03bc\u03ae \u03c0\u03bf\u03c5 \u03c5\u03c0\u03bf\u03b4\u03b5\u03b9\u03ba\u03bd\u03cd\u03b5\u03c4\u03b1\u03b9. \u038c\u03c4\u03b1\u03bd \u03b5\u03ba\u03c4\u03b5\u03bb\u03b5\u03c3\u03c4\u03b5\u03af \u03c4\u03bf \u03ba\u03b5\u03bb\u03af \u03b8\u03b1 \u03c3\u03b1\u03c2 \u03b4\u03ce\u03c3\u03b5\u03b9 \u03c4\u03bf URL \u03cc\u03c0\u03bf\u03c5 \u03bc\u03c0\u03bf\u03c1\u03b5\u03af\u03c4\u03b5 \u03bd\u03b1 \u03b2\u03bb\u03ad\u03c0\u03b5\u03c4\u03b5 \u03c4\u03bf TensorBoard. \u03a3\u03b7\u03bc\u03b5\u03b9\u03ce\u03c3\u03c4\u03b5 \u03cc\u03c4\u03b9 \u03c3\u03b5 \u03c0\u03b5\u03c1\u03af\u03c0\u03c4\u03c9\u03c3\u03b7 \u03b5\u03c0\u03b1\u03bd\u03b5\u03ba\u03ba\u03af\u03bd\u03b7\u03c3\u03b7\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03c5\u03c1\u03ae\u03bd\u03b1 \u03b8\u03b1 \u03c0\u03c1\u03ad\u03c0\u03b5\u03b9 \u03bd\u03b1 \u03be\u03b1\u03bd\u03b1\u03c4\u03c1\u03ad\u03be\u03b5\u03c4\u03b5 \u03c4\u03bf \u03ba\u03b5\u03bb\u03af. \u0397 \u03b4\u03b9\u03b5\u03cd\u03b8\u03c5\u03bd\u03c3\u03b7 \u03b8\u03b1 \u03b5\u03af\u03bd\u03b1\u03b9 \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03b5\u03c4\u03b9\u03ba\u03ae, \u03b1\u03bb\u03bb\u03ac \u03c4\u03b1 \u03c0\u03c1\u03bf\u03b7\u03b3\u03bf\u03cd\u03bc\u03b5\u03bd\u03b1 \u03c3\u03c4\u03b1\u03c4\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac \u03c3\u03b1\u03c2 \u03b4\u03b5\u03bd \u03c7\u03ac\u03bd\u03bf\u03bd\u03c4\u03b1\u03b9 (\u03bc\u03ad\u03c7\u03c1\u03b9 \u03bd\u03b1 \u03b1\u03bd\u03b1\u03ba\u03c5\u03ba\u03bb\u03c9\u03b8\u03b5\u03af \u03bf \u03c0\u03c5\u03c1\u03ae\u03bd\u03b1\u03c2).","c80a6241":"#### MLP","134fdb57":"## DQN","9de09948":"## \u039f\u03c1\u03b9\u03c3\u03bc\u03cc\u03c2 \u03c0\u03b1\u03b9\u03c7\u03bd\u03b9\u03b4\u03b9\u03bf\u03cd\n\n\u0392\u03ac\u03bb\u03c4\u03b5 \u03b5\u03b4\u03ce \u03c4\u03bf string \u03c0\u03bf\u03c5 \u03b1\u03bd\u03c4\u03b9\u03c3\u03c4\u03bf\u03b9\u03c7\u03b5\u03af \u03c3\u03c4\u03bf \u03c0\u03b1\u03b9\u03c7\u03bd\u03af\u03b4\u03b9 \u03c3\u03b1\u03c2. \u0393\u03b9\u03b1 \u03c4\u03b7\u03bd \u03bf\u03bd\u03bf\u03bc\u03b1\u03c4\u03bf\u03bb\u03bf\u03b3\u03af\u03b1 \u03c4\u03c9\u03bd \u03c0\u03b5\u03c1\u03b9\u03b2\u03b1\u03bb\u03bb\u03cc\u03bd\u03c4\u03c9\u03bd:\n- v0 vs v4: v0 has repeat_action_probability of 0.25 (meaning 25% of the time the previous action will be used instead of the new action), while v4 has 0 (always follow your issued action)\n- Deterministic: a fixed frameskip of 4, while for the env without Deterministic, frameskip is sampled from [2,4]\n- There is also NoFrameskip-v4 with no frame skip and no action repeat stochasticity.\n\n\u0391\u03bd\u03b1\u03bb\u03c5\u03c4\u03b9\u03ba\u03cc\u03c4\u03b5\u03c1\u03b1 \u03c3\u03c4\u03b7\u03bd \u03b5\u03ba\u03c6\u03ce\u03bd\u03b7\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03ac\u03c3\u03ba\u03b7\u03c3\u03b7\u03c2.","c7299202":"# NoFrameskip-v4","dfc6e0a2":"#### qrdqn","393c41b1":"#### qrdqn","12abc667":"#### MLP","9f0e73bb":"#### MLP","0d40b929":"#### MLP","e3cee712":"## \u039a\u03b1\u03c4\u03b1\u03b3\u03c1\u03b1\u03c6\u03ae video \u03c4\u03bf\u03c5 actual gameplay \u03c4\u03bf\u03c5 \u03c0\u03c1\u03ac\u03ba\u03c4\u03bf\u03c1\u03b1\n\n\u03a9\u03c1\u03b1\u03af\u03b1 \u03c4\u03b1 \u03b4\u03b9\u03b1\u03b3\u03c1\u03ac\u03bc\u03bc\u03b1\u03c4\u03b1, \u03c9\u03c1\u03b1\u03af\u03b1\u03bf \u03c4\u03bf TensorBoard, \u03b1\u03bb\u03bb\u03ac \u03b8\u03ad\u03bb\u03bf\u03c5\u03bc\u03b5 \u03bf\u03c0\u03c9\u03c3\u03b4\u03ae\u03c0\u03bf\u03c4\u03b5 \u03bd\u03b1 \u03b4\u03bf\u03cd\u03bc\u03b5 \u03c4\u03bf\u03bd \u03c0\u03c1\u03ac\u03ba\u03c4\u03bf\u03c1\u03b1 \u03bd\u03b1 \u03c0\u03b1\u03af\u03b6\u03b5\u03b9 \u03c4\u03bf \u03c0\u03b1\u03b9\u03c7\u03bd\u03af\u03b4\u03b9!","bcf42a87":"# Plain-v4","a97b3ad0":"## \u03a6\u03cc\u03c1\u03c4\u03c9\u03c3\u03b7 \u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03c5\u03bc\u03ad\u03bd\u03bf\u03c5 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf\u03c5\n\n\u03a4\u03bf Kaggle \u03b4\u03b5\u03bd \u03bc\u03b1\u03c2 \u03b1\u03c6\u03ae\u03bd\u03b5\u03b9 \u03bd\u03b1 \u03b1\u03bd\u03b5\u03b2\u03ac\u03c3\u03bf\u03c5\u03bc\u03b5 \u03ac\u03bb\u03bb\u03bf\u03c5 \u03b5\u03af\u03b4\u03bf\u03c5\u03c2 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03b1 \u03b5\u03ba\u03c4\u03cc\u03c2 \u03b1\u03c0\u03cc datasets. \u0398\u03b1 \u03c7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03ae\u03c3\u03bf\u03c5\u03bc\u03b5 \u03ad\u03bd\u03b1\u03bd \u03bf\u03c0\u03bf\u03b9\u03bf\u03bd\u03b4\u03ae\u03c0\u03bf\u03c4\u03b5 \u03bb\u03bf\u03b3\u03b1\u03c1\u03b9\u03b1\u03c3\u03bc\u03cc Dropbox \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03b4\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03ae\u03c3\u03bf\u03c5\u03bc\u03b5 \u03ad\u03bd\u03b1 \u03b1\u03c0\u03b5\u03c5\u03b8\u03b5\u03af\u03b1\u03c2 link \u03b3\u03b9\u03b1 \u03c4\u03bf \u03b1\u03c1\u03c7\u03b5\u03af\u03bf zip \u03c4\u03bf\u03c5 \u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03c5\u03bc\u03ad\u03bd\u03bf\u03c5 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf\u03c5.\n1. \u03a4\u03bf\u03c0\u03bf\u03b8\u03b5\u03c4\u03ae\u03c3\u03c4\u03b5 \u03c4\u03bf zip \u03bc\u03ad\u03c3\u03b1 \u03c3\u03b5 \u03ad\u03bd\u03b1 \u03c6\u03ac\u03ba\u03b5\u03bb\u03bf Dropbox\n2. \u0391\u03bd \u03b5\u03af\u03c3\u03c4\u03b5 \u03c3\u03b5 Windows \u03ba\u03ac\u03bd\u03c4\u03b5 \u03b4\u03b5\u03be\u03af \u03ba\u03bb\u03b9\u03ba \u03c0\u03ac\u03bd\u03c9 \u03c3\u03c4\u03bf \u03b1\u03c1\u03c7\u03b5\u03af\u03bf \u03ba\u03b1\u03b9 \u03b5\u03c0\u03b9\u03bb\u03ad\u03be\u03c4\u03b5 \"Copy Dropbox Link'. \u0391\u03bd \u03b5\u03af\u03c3\u03c4\u03b5 \u03c3\u03b5 Linux \u03bc\u03c0\u03b5\u03af\u03c4\u03b5 \u03c3\u03c4\u03b7\u03bd web \u03b5\u03c6\u03b1\u03c1\u03bc\u03bf\u03b3\u03ae \u03ba\u03b1\u03b9 \u03ba\u03ac\u03bd\u03c4\u03b5 \u03c4\u03bf \u03af\u03b4\u03b9\u03bf (\u03bb\u03ad\u03b3\u03b5\u03c4\u03b1\u03b9 \"Copy link\"\n3. \u039a\u03ac\u03bd\u03c4\u03b5 \u03ba\u03ac\u03c0\u03bf\u03c5 paste \u03c4\u03bf URL https:\/\/www.dropbox.com\/s\/umw5k9wcuiy0l2u\/a2c_pong.zip?dl=0 \u03ba\u03b1\u03b9 \u03b1\u03bb\u03bb\u03ac\u03be\u03c4\u03b5 \u03c4\u03bf \u03c4\u03b5\u03bb\u03b5\u03c5\u03c4\u03b1\u03af\u03bf 0 \u03c3\u03b5 1 \u03cc\u03c0\u03c9\u03c2 \u03b5\u03b4\u03ce https:\/\/www.dropbox.com\/s\/umw5k9wcuiy0l2u\/a2c_pong.zip?dl=1\n4. \u0391\u03bd\u03c4\u03b9\u03b3\u03c1\u03ac\u03c8\u03c4\u03b5 \u03c4\u03bf \u03bd\u03ad\u03bf URL \u03c3\u03c4\u03bf clipboard.\n\n\u0395\u03b4\u03ce \u03c6\u03ad\u03c1\u03bd\u03bf\u03c5\u03bc\u03b5 \u03ad\u03bd\u03b1 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf \u03912C \u03c0\u03bf\u03c5 \u03ad\u03c7\u03bf\u03c5\u03bc\u03b5 \u03b5\u03ba\u03c0\u03b1\u03b9\u03b4\u03b5\u03cd\u03c3\u03b5\u03b9 \u03bd\u03c9\u03c1\u03af\u03c4\u03b5\u03c1\u03b1. \u0395\u03c3\u03b5\u03af\u03c2 \u03b1\u03c0\u03bb\u03ac \u03b1\u03bb\u03bb\u03ac\u03b6\u03b5\u03c4\u03b5 \u03c4\u03bf URL \u03c4\u03b7\u03c2 \u03c0\u03b1\u03c1\u03b1\u03ba\u03ac\u03c4\u03c9 \u03b5\u03bd\u03c4\u03bf\u03bb\u03ae\u03c2 \u03b3\u03b9\u03b1 \u03c4\u03b1 \u03b4\u03b9\u03ba\u03ac \u03c3\u03b1\u03c2 \u03b1\u03c1\u03c7\u03b5\u03af\u03b1. \u03a0\u03c1\u03bf\u03c3\u03ad\u03be\u03c4\u03b5 \u03bc\u03b5\u03c4\u03ac \u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03ac\u03bc\u03b5\u03c4\u03c1\u03bf -\u039f \u03bd\u03b1 \u03b2\u03ac\u03bb\u03b5\u03c4\u03b5 \u03c4\u03bf \u03c3\u03c9\u03c3\u03c4\u03cc \u03cc\u03bd\u03bf\u03bc\u03b1 \u03b1\u03c1\u03c7\u03b5\u03af\u03bf\u03c5, \u03bc\u03b5 \u03c0\u03bf\u03b9\u03bf  \u03cc\u03bd\u03bf\u03bc\u03b1 \u03b8\u03b1 \u03c3\u03c9\u03b8\u03b5\u03af \u03b4\u03b7\u03bb\u03b1\u03b4\u03ae.","04393aea":"## QR-DQN","85a9f9b5":"#### CNN","b3984abe":"## QR-DQN","092dea65":"## Deterministic-v4","62f13bbf":"#### CNN","b709cf18":"#### MLP","412fe99b":"## \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\n\n \u03a3\u03b7\u03bc\u03b5\u03b9\u03ce\u03c3\u03c4\u03b5 \u03cc\u03c4\u03b9 \u03c4\u03b1 timesteps \u03b5\u03af\u03bd\u03b1\u03b9 \u03c0\u03bf\u03bb\u03cd \u03bb\u03af\u03b3\u03b1 \u03ba\u03b1\u03b9 \u03cc\u03c4\u03b9 \u03b4\u03b5\u03bd \u03ba\u03ac\u03bd\u03bf\u03c5\u03bc\u03b5 \u03b4\u03b9\u03b5\u03c1\u03b5\u03cd\u03bd\u03b7\u03c3\u03b7 \u03c3\u03c4\u03b9\u03c2 \u03c0\u03b1\u03c1\u03b1\u03bc\u03ad\u03c4\u03c1\u03bf\u03c5\u03c2 \u03c4\u03bf\u03c5 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03bf\u03c5 \u03c0\u03bf\u03c5 \u03bc\u03c0\u03bf\u03c1\u03b5\u03af\u03c4\u03b5 \u03bd\u03b1 \u03b2\u03c1\u03b5\u03af\u03c4\u03b5 [\u03b5\u03b4\u03ce](https:\/\/stable-baselines3.readthedocs.io\/en\/master\/modules\/dqn.html#parameters).","4831b274":"#### CNN","9581111b":"#### a2c","c229d7d6":"#### MLP","07d58152":"#### MLP","294dceb5":"## A2C","35a0ce33":"#### MLP","d829db80":"#### dqn","042adf06":"# evaluate","317fd921":"## PPO","0d011aa1":"#### a2c","02895822":"#### MLP","5829fa71":"#### CNN","1399000d":"#### CNN","450f801a":"## DQN","246837ed":"## QR-DQN","805664b6":"## DQN","bb3536d8":"# \u03a0\u03b1\u03af\u03b6\u03bf\u03bd\u03c4\u03b1\u03c2 Atari \u03bc\u03b5 \u03b2\u03b1\u03b8\u03b9\u03ac \u03b5\u03bd\u03b9\u03c3\u03c7\u03c5\u03c4\u03b9\u03ba\u03ae \u03bc\u03ac\u03b8\u03b7\u03c3\u03b7\n\n\u03a0\u03c1\u03bf\u03c3\u03bf\u03c7\u03ae: \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03bc\u03c0\u03bf\u03c1\u03ad\u03c3\u03b5\u03b9 \u03bd\u03b1 \u03c4\u03c1\u03ad\u03be\u03b5\u03b9 \u03b1\u03c5\u03c4\u03cc \u03c4\u03bf notebook \u03c3\u03c4\u03bf Kaggle \u03c0\u03c1\u03ad\u03c0\u03b5\u03b9 \u03bd\u03b1 \u03b5\u03bd\u03b5\u03c1\u03b3\u03bf\u03c0\u03bf\u03b9\u03ae\u03c3\u03b5\u03c4\u03b5 \u03c4\u03bf Internet \u03ba\u03b1\u03b9 \u03c4\u03b7\u03bd GPU \u03b3\u03b9\u03b1 \u03c4\u03bf\u03bd \u03c0\u03c5\u03c1\u03ae\u03bd\u03b1 \u03c3\u03b1\u03c2. \u0391\u03c5\u03c4\u03cc \u03b1\u03c0\u03b1\u03b9\u03c4\u03b5\u03af \u03c4\u03b7\u03bd \u03b5\u03c0\u03b9\u03b2\u03b5\u03b2\u03b1\u03af\u03c9\u03c3\u03b7 \u03c4\u03bf\u03c5 \u03ba\u03b9\u03bd\u03b7\u03c4\u03bf\u03cd \u03c3\u03b1\u03c2 \u03bc\u03b5 SMS. \u03a0\u03b7\u03b3\u03b1\u03af\u03bd\u03c4\u03b5 \u03c3\u03c4\u03bf \u03b4\u03b5\u03be\u03af sidebar \u03c3\u03c4\u03b1 settings \u03ba\u03b1\u03b9 \u03b8\u03b1 \u03c4\u03bf \u03b4\u03b5\u03af\u03c4\u03b5. \n\n\u03a4\u03b5\u03bb\u03b9\u03ba\u03ac \u03c0\u03c1\u03ad\u03c0\u03b5\u03b9 \u03bd\u03b1 \u03ad\u03c7\u03b5\u03c4\u03b5 \u03b1\u03c5\u03c4\u03ae \u03c4\u03b7\u03bd \u03b5\u03b9\u03ba\u03cc\u03bd\u03b1:\n\n![](https:\/\/i.imgur.com\/Ek5hOIo.jpg)","7ba4a106":"## NoFrameskip-v4","e9103acf":"## \u0395\u03b3\u03ba\u03b1\u03c4\u03ac\u03c3\u03c4\u03b1\u03c3\u03b7 \u03b2\u03b9\u03b2\u03bb\u03b9\u03bf\u03b8\u03ae\u03ba\u03b7\u03c2 \u03ba\u03b1\u03b9 gym","e9e60337":"# Hyper-Parameters Tunning","54ec45ce":"## PPO","b6083c02":"## Plain-v4","0b06d8e4":"#### CNN","54eb1c0a":"#### CNN","cb682162":"#### CNN","05b8d2c9":"#### CNN","96eadb55":"#### CNN","ff5228bf":"#### CNN"}}