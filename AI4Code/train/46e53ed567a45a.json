{"cell_type":{"2ec25e35":"code","f168d6c3":"code","582abd9f":"code","d60ff553":"code","8125cd11":"code","a98557a4":"code","74ee243e":"code","7437946a":"code","c107bdd8":"code","89045bbd":"code","053ef7c1":"code","a9a2029d":"code","83a0750e":"code","159e8f63":"code","7d839af0":"code","5e78c3f0":"code","7a62b1cc":"code","ca51da87":"code","c24ab663":"code","1eb45db6":"code","4f966e8f":"code","057cebc6":"code","65a6ed6c":"code","c4c0d937":"code","40b57c84":"code","0ddafb60":"code","df02bdb5":"code","ccd52f91":"code","55e335e3":"code","f8bf6883":"code","0f3cd6d0":"code","1a0099ef":"code","6d568ccd":"code","03051131":"code","9b044be5":"code","0cace045":"code","840cc2d8":"code","53451be5":"code","20b3343e":"code","7eb52308":"code","310a42cf":"code","d5809f3c":"code","fe20384e":"code","df412c63":"code","7bbc6fa6":"code","22b99395":"markdown","16c16b59":"markdown","fed27ee8":"markdown","26b21ae3":"markdown","82ecaf97":"markdown","1287d545":"markdown","52021d86":"markdown","726c9b14":"markdown","f3d70948":"markdown","15caa7e1":"markdown","49a3ac0a":"markdown","a834411c":"markdown","ac55b198":"markdown","1e11e9a8":"markdown","0a251552":"markdown","a3835883":"markdown","464b7d68":"markdown","635b27fd":"markdown","38df5f66":"markdown","b210428f":"markdown","01d92abb":"markdown","1fee9055":"markdown","574287fd":"markdown","24808355":"markdown","702b2079":"markdown","2d0db260":"markdown","afff7507":"markdown","cb3269b0":"markdown","70d8ed13":"markdown"},"source":{"2ec25e35":"TRAINING_PATH='..\/input\/titanic\/train.csv'\nTRAINING_FOLDS_PATH='.\/'\n\nimport pandas as pd\n\ndf_train=pd.read_csv(TRAINING_PATH)\ndf_train.head()\n\ndf_train['Survived'].value_counts()\n\ndf_train['kfolds']=-1\ndf_train=df_train.sample(frac=1).reset_index(drop=True)\ndf_train.head()\n\nfrom sklearn import model_selection\n\nstrat_kf=model_selection.StratifiedKFold(n_splits=5)\n\nfor fold,(trn_,val_) in enumerate(strat_kf.split(X=df_train,y=df_train['Survived'])):\n  df_train.loc[val_,'kfolds']=fold\ndf_train.head()\n\ndf_train.to_csv(TRAINING_FOLDS_PATH+'train_folds.csv')\n","f168d6c3":"import joblib\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","582abd9f":"TRAINING_PATH='.\/train_folds.csv'\nTESTING_PATH='..\/input\/titanic\/test.csv'\nMODEL_PATH='.\/'\nSUBMISSION_FILES_PATH='.\/Submissions\/'","d60ff553":"df=pd.read_csv(TRAINING_PATH)\ndf.head()","8125cd11":"df.describe()","a98557a4":"# Count the number of null values in each column\ndf.isna().sum()","74ee243e":"# Total number of rows\nlen(df)","7437946a":"# Total number of unique values in each column\ndf.nunique()","c107bdd8":"# Check for class imbalance \ndf['Survived'].value_counts()","89045bbd":"# Unnamed:0 , Name and PassengerId have all different values so no value can determine the result\n# Ticket also has lots of different values\n# Cabin has lots of null values\ndf=df.drop(['Cabin','Name','PassengerId','Unnamed: 0','Ticket'],axis=1)\ndf.head()","053ef7c1":"# Use KNN Imputer to fill missing values\n\nfrom sklearn.impute import KNNImputer\nimputer=KNNImputer(n_neighbors=3)\n\ndf_knn_imputed=df\ndf_knn_imputed['Age']=imputer.fit_transform(df_knn_imputed['Age'].values.reshape(-1,1))\ndf_knn_imputed.head()","a9a2029d":"# Use Iterative Imputer to fill missing values\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimport numpy as np\nimputer=IterativeImputer(random_state=42)\n\ndf_iter_imputed=df\ndf_iter_imputed['Age']=imputer.fit_transform(df_iter_imputed['Age'].values.reshape(-1,1))\ndf_iter_imputed.head()","83a0750e":"# One hot encode the categorical columns - Sex and Embarked\n\ndf=pd.get_dummies(data=df_iter_imputed,columns=['Sex','Embarked'])\ndf.head()","159e8f63":"# Move the target and kfolds column to the last\n\ndf=df[[column for column in df if column not in['Survived','kfolds']]+['Survived','kfolds']]\ndf.head()","7d839af0":"# Scale the columns using MinMaxScaler except for the target and kfolds column\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ndf_2=pd.DataFrame(scaler.fit_transform(df),index=df.index,columns=df.columns)\ndf_2['kfolds']=df['kfolds']\ndf_2['Survived']=df['Survived']\ndf=df_2\ndf.head()","5e78c3f0":"def run(fold,df,models,target_name, save_model, print_details=False):\n  \n  # print(df.head())\n  # Training and validation sets\n  df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n  df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n  # x and y of training dataset\n  x_train=df_train.drop(target_name,axis=1).values\n  y_train=df_train[target_name].values\n\n  # x and y of validation dataset\n  x_valid=df_valid.drop(target_name,axis=1).values\n  y_valid=df_valid[target_name].values\n\n  # accuracy => will store accuracies of the models  (same for confusion_matrices)\n  accuracy=[]\n  confusion_matrices=[]\n  classification_report=[]\n\n  for model_name,model_constructor in list(models.items()):\n    clf=model_constructor\n    clf.fit(x_train,y_train)\n\n    # preds_train, preds_valid => predictions when training and validation x are fed into the trained model\n    preds_train=clf.predict(x_train)\n    preds_valid=clf.predict(x_valid)\n\n    acc_train=metrics.accuracy_score(y_train,preds_train)\n    acc_valid=metrics.accuracy_score(y_valid,preds_valid)\n\n    f1_train = metrics.f1_score(y_train,preds_train)\n    f1_valid = metrics.f1_score(y_valid,preds_valid)\n\n    conf_matrix=metrics.confusion_matrix(y_valid,preds_valid)\n    class_report=metrics.classification_report(y_valid,preds_valid)\n\n    accuracy.append(acc_valid)\n    confusion_matrices.append(conf_matrix)\n    classification_report.append(class_report)\n\n    if(print_details==True):\n      print(f'Model => {model_name} => Fold = {fold} => Training Accuracy = {acc_train} => Validation Accuracy = {acc_valid}')\n\n    if(save_model==True):\n      joblib.dump(clf, f\"{MODEL_PATH}{model_name}_F1_{f1_valid}_ACC_{acc_valid}_FOLD_{fold}.bin\")\n\n  if(print_details==True):\n    print('\\n--------------------------------------------------------------------------------------------\\n')\n    \n  return accuracy,confusion_matrices,classification_report","7a62b1cc":"def greedy_feature_selection(fold,df,models,target_name):\n\n  # target_index => stores the index of the target variable in the dataset\n  # kfolds_index => stores the index of kfolds column in the dataset\n\n  target_index=df.columns.get_loc(target_name)\n  kfolds_index=df.columns.get_loc('kfolds')\n\n  # good_features => stores the indices of all the optimal features\n  # best_scores => keeps track of the best scores \n  good_features=[]\n  best_scores=[]\n\n  # df has X and y and a kfolds column. \n  # no of features (no of columns in X) => total columns in df - 1 (there's 1 y) - 1 (there's 1 kfolds)\n  num_features=df.shape[1]-2\n\n  while True:\n\n    # this_feature => the feature added to the already selected features to measure the effect of the former on the model\n    # best_score => keeps track of the best score achieved while selecting features 1 at a time and checking its effect on the model\n    this_feature=None\n    best_score=0\n\n\n    for feature in range(num_features):\n\n      # if the feature is already in the good_features list, ignore and move ahead\n      if feature in good_features:\n        continue\n      \n      # add the currently selected feature to the already discovered good features\n      selected_features=good_features+[feature]\n\n      # all the selected features + target and kfolds column\n      df_train=df.iloc[:, selected_features + [target_index,kfolds_index]]\n\n      # fit the selected dataset to a model \n      accuracy,confusion_matrices,classification_report=run(fold,df_train,models,save_model= False, target_name=target_name)\n\n      # if any improvement is observed over the previous set of features\n      if(accuracy[0]>best_score):\n        this_feature=feature\n        best_score=accuracy[0]\n      \n    if(this_feature!=None):\n      good_features.append(this_feature)\n      best_scores.append(best_score)\n    \n    if(len(best_scores)>2):\n      if(best_scores[-1]<best_scores[-2]):\n        break\n    \n  return best_scores[:-1] , df.iloc[:, good_features[:-1] + [target_index,kfolds_index]]","ca51da87":"from sklearn.feature_selection import RFE\n\ndef recursive_feature_selection(df,models,n_features_to_select,target_name):\n  X=df.drop(labels=[target_name,'kfolds'],axis=1).values\n  y=df[target_name]\n  kfolds=df.kfolds.values\n\n  model_name,model_constructor=list(models.items())[0]\n\n  rfe=RFE(\n      estimator=model_constructor,\n      n_features_to_select=n_features_to_select\n  )\n\n  try:\n    rfe.fit(X,y)\n  except :\n    print(f\"{model_name} does not support feature importance... Returning original dataframe\\n\")\n    return df\n  else:\n    X_transformed = rfe.transform(X)\n    df_optimal=pd.DataFrame(data=[X,y,kfolds])\n    return df_optimal","c24ab663":"print('Greedy Feature Selection : ')\nprint('\\n')\nmodels={'XGB': XGBClassifier()}\nbest_scores,df_optimal_XGB=greedy_feature_selection(fold=4,df=df,models=models,target_name='Survived')\nprint(df_optimal_XGB.head())\n\nprint('\\n')\nprint(\"Recursive Feature Selection : \")\nprint('\\n')\ndf_recursive_optimal_XGB=recursive_feature_selection(df=df,models=models,n_features_to_select=5,target_name='Survived')\nprint(df_recursive_optimal_XGB.head())","1eb45db6":"models={'SVM' : SVC()}\nbest_scores,df_optimal_SVM=greedy_feature_selection(fold=4,df=df,models=models,target_name='Survived')\nprint(df_optimal_SVM.head())\n\nprint('\\n')\ndf_recursive_optimal_SVM=recursive_feature_selection(df=df,models=models,n_features_to_select=5,target_name='Survived')\nprint(df_recursive_optimal_SVM.head())","4f966e8f":"models={'RFC' : RandomForestClassifier()}\nbest_scores,df_optimal_RFC=greedy_feature_selection(fold=4,df=df,models=models,target_name='Survived')\nprint(df_optimal_RFC.head())\n\nprint('\\n')\ndf_recursive_optimal_RFC=recursive_feature_selection(df=df,models=models,n_features_to_select=5,target_name='Survived')\nprint(df_recursive_optimal_RFC.head())","057cebc6":"from sklearn import model_selection\nfrom sklearn import metrics\n\ndef hyperparameter_tune_and_run(df,num_folds,models,target_name,param_grid,evaluation_metric,print_details=False):\n  X=df.drop(labels=[target_name,'kfolds'],axis=1).values\n  y=df[target_name]\n\n  model_name,model_constructor=list(models.items())[0]\n\n  model = model_selection.GridSearchCV(\n      estimator = model_constructor,\n      param_grid = param_grid,\n      scoring = evaluation_metric,\n      verbose = 10,\n      cv = num_folds,\n      n_jobs=-1\n  )\n\n  model.fit(X,y)\n\n  if(print_details==True):\n    print(f\"Best score : {model.best_score_}\")\n\n    print(\"Best parameters : \")\n    best_parameters=model.best_estimator_.get_params()\n    for param_name in sorted(param_grid.keys()):\n      print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n  \n  return model","65a6ed6c":"# models={'XGB Classifier': XGBClassifier()}\n# param_grid = {\n#     \"learning_rate\":[0.01,0.015,0.025,0.05,0.1],\n#     \"gamma\":[0.05,0.1,0.3,0.5,0.7,0.9,1.0],\n#     \"max_depth\":[3,5,7,9,12,15,17,25],\n#     \"min_child_weight\":[1,3,5,7],\n#     \"subsample\":[0.6,0.7,0.8,0.9,1.0],\n#     \"colsample_bytree\":[0.6,0.7,0.8,0.9,1.0],\n#     # \"reg_lambda\":[0.01,0.03,0.05,0.07,0.09,0.1,1.0],\n#     # \"reg_alpha\":[0.01,0.03,0.05,0.07,0.09,0.1,1.0]\n# }\n# model = hyperparameter_tune_and_run(df=df_optimal_XGB,num_folds=5,models=models,target_name='Survived',param_grid=param_grid,evaluation_metric=\"accuracy\",print_details=True)","c4c0d937":"# models={'SVM Classifier': SVC()}\n# param_grid = {\n#     \"C\":[0.001,0.01,0.1,1,10,100,1000],\n#     \"gamma\":['auto'],\n#     \"class_weight\":['balanced']\n# }\n# SVM_model = hyperparameter_tune_and_run(df=df_optimal_SVM,num_folds=5,models=models,target_name='Survived',param_grid=param_grid,evaluation_metric=\"accuracy\",print_details=True)","40b57c84":"# models={'Random Forest': RandomForestClassifier()}\n# param_grid = {\n#     \"n_estimators\":[120,300,500,800,1200],\n#     \"max_depth\":[5,8,15,25,30,None],\n#     \"min_samples_split\":[1,2,5,10,15,100],\n#     \"min_samples_leaf\":[1,2,5,10],\n#     \"max_features\":[\"log2\",\"sqrt\",None]\n# }\n# Random_Forest_model = hyperparameter_tune_and_run(df=df_optimal_RFC,num_folds=5,models=models,target_name='Survived',param_grid=param_grid,evaluation_metric=\"accuracy\",print_details=True)","0ddafb60":"XGB_model=XGBClassifier(max_depth=4,learning_rate=0.1,colsample_bytree=0.8,gamma=0.3,min_child_weight=5,subsample=1.0)\nSVM_model=SVC(C=1000,class_weight='balanced',gamma='auto')\nRFC_model=RandomForestClassifier(max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=15,n_estimators=300)\n\nmodels={\n    'XGB Classifier' : XGB_model,\n    'SVM Classifier' : SVM_model,\n    'Random Forest Classifier' : RFC_model\n    }\n","df02bdb5":"accuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df_optimal_XGB,models=models,target_name='Survived', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","ccd52f91":"XGB_model=XGBClassifier(max_depth=4,learning_rate=0.1,colsample_bytree=0.8,gamma=0.3,min_child_weight=5,subsample=1.0)\nSVM_model=SVC(C=1000,class_weight='balanced',gamma='auto')\nRFC_model=RandomForestClassifier(max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=15,n_estimators=300)\n\nmodels={\n    'XGB Classifier' : XGB_model,\n    'SVM Classifier' : SVM_model,\n    'Random Forest Classifier' : RFC_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df_optimal_SVM,models=models,target_name='Survived', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","55e335e3":"XGB_model=XGBClassifier(max_depth=4,learning_rate=0.1,colsample_bytree=0.8,gamma=0.3,min_child_weight=5,subsample=1.0)\nSVM_model=SVC(C=1000,class_weight='balanced',gamma='auto')\nRFC_model=RandomForestClassifier(max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=15,n_estimators=300)\n\nmodels={\n    'XGB Classifier' : XGB_model,\n    'SVM Classifier' : SVM_model,\n    'Random Forest Classifier' : RFC_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df_optimal_RFC,models=models,target_name='Survived', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","f8bf6883":"df_test = pd.read_csv(TESTING_PATH)\ndf_test.head()","0f3cd6d0":"df_test.describe()","1a0099ef":"df_submit=pd.DataFrame()\ndf_submit['PassengerId']=df_test['PassengerId']\ndf_submit.head()","6d568ccd":"df_optimal_RFC.head()","03051131":"def get_preprocessed_test_data(df,df_test):\n    \n\n\n  df_test=pd.get_dummies(data=df_test,columns=['Sex','Embarked'])\n\n  optimal_data_cols = df.columns\n  optimal_data_cols = list(optimal_data_cols[:-2])\n  df_test=df_test.loc[:,optimal_data_cols]\n\n  null_columns = [k for k,v in dict(df_test.isna().sum()).items() if v!=0]\n\n  for null_column in null_columns:\n    df_test[null_column]=imputer.transform(df_test[null_column].values.reshape(-1,1))\n\n  df_test_2=pd.DataFrame(scaler.fit_transform(df_test),index=df_test.index,columns=df_test.columns)\n  df_test=df_test_2\n\n  return df_test","9b044be5":"df_test = get_preprocessed_test_data(df_optimal_RFC,df_test)\ndf_test.head()","0cace045":"df_test.isna().sum()","840cc2d8":"x_train=df_optimal_RFC.drop(['Survived','kfolds'],axis=1)\ny_train=df_optimal_RFC['Survived']","53451be5":"x_train.head()","20b3343e":"len(x_train)","7eb52308":"final_model = RandomForestClassifier(max_depth=30,max_features=None,min_samples_leaf=1,min_samples_split=15,n_estimators=300)\nfinal_model.fit(x_train,y_train)","310a42cf":"joblib.dump(final_model, f\"{MODEL_PATH}Final Random Forest Classifier Model.bin\")","d5809f3c":"df_submit['Survived']=final_model.predict(df_test)","fe20384e":"df_submit.head()","df412c63":"len(df_submit)","7bbc6fa6":"df_submit.to_csv('RFC_Submission_Allfeats_hyperparams_tuned_100.csv',index=False)","22b99395":"#### One hot encode categorical features","16c16b59":"# **Warning : These cells take hours to run! So Uncomment the lines if you want to run it. The output for the commented codes are given below**\n\n---\n\n\n# Hyperparameter tuning\n\nModels : \n\n1.   XGB Classifier\n2.   SVM Classifier\n3.   Random Forest Classifier\n\n\n","fed27ee8":"![image.png](attachment:80cd3c37-0cae-43ed-98cc-74df96b51579.png)\n\nBest score : 0.8271483271608812<br>\nBest parameters : <br>\n\tcolsample_bytree: 0.8 <br>\n\tgamma: 0.3 <br>\n\tlearning_rate: 0.1 <br>\n\tmax_depth: 5 <br>\n\tmin_child_weight: 5 <br>\n\tsubsample: 1.0 <br>","26b21ae3":"### Just re-train the RFC model once again!","82ecaf97":"#### Move the Survived and kfolds column to the end","1287d545":"### SVM (Only Greedy Feature Selection works!)","52021d86":"# Prediction","726c9b14":"#### MinMax Scaler","f3d70948":"### 2. Fitting the models on the dataset with features optimal to SVM ie. the dataframe we obtained after feature selection on SVM","15caa7e1":"# File Paths","49a3ac0a":"### XGB (Both Greedy and Recursive Feature Selection Techniques work!)","a834411c":"### Initialise XGB, SVC, Random Forest with their best hyperparameters","ac55b198":"#### Creating the dataframe to submit and inserting the Passenger ID column\n\n","1e11e9a8":"# Run the models","0a251552":"![image.png](attachment:5da2389a-a3b5-4702-b251-03234eb864d1.png)\n\nBest score : 0.8260435628648548 <br>\nBest parameters : <br>\n\tmax_depth: 30 <br>\n\tmax_features: None <br>\n\tmin_samples_leaf: 1 <br>\n\tmin_samples_split: 15 <br>\n\tn_estimators: 300 <br>","a3835883":"#### Drop Columns based on the number of unique values and number of null values (both should be sufficiently large to drop) ","464b7d68":"#### Imputer to fill in missing values\n\n1.   KNN Imputer\n2.   Iterative Imputer\n\n","635b27fd":"# Finding the optimal features for the different models\n\nModels :\n\n1.   XGB Classifier\n2.   SVM Classifier\n3.   Random Forest Classifier\n\n","38df5f66":"# Data Exploration\n\n1.   Null Values\n2.   Number of unique values\n\n","b210428f":"# Creating 5 Stratified K Fold cross validation sets","01d92abb":"### Random Forest (Both Greedy and Recursive Feature Selection work!)","1fee9055":"### Perform similar preprocessing on the test dataframe and use the same features in the test df as those in the train dataframe optimal for RFC","574287fd":"### 3. Fitting the models on the dataset with features optimal to Random Forest Classifier (RFC) ie. the dataframe we obtained after feature selection on RFC","24808355":"# Conclusion : Overall, XGB and Random Forest seem to have comparable accuracies but in 1-2 cases, Random Forest Classifier had a bit better accuracy among the two. So I decided to go ahead with Random Forest","702b2079":"### Hyperparameter Tune for XGB : \n","2d0db260":"# Importing Libraries","afff7507":"# Feature Selection\n\n1.   Greedy Feature Selection\n2.   Recursive Feature Selection\n\n","cb3269b0":"![image.png](attachment:bd270df1-640f-4f6f-a2a4-1fc394db20d0.png)\n\nBest score : 0.7957064842131694 <br>\nBest parameters : <br>\n\tC: 1000 <br>\n\tclass_weight: balanced <br>\n\tgamma: auto <br>","70d8ed13":"### 1. Fitting the models on the dataset with features optimal to XGB ie. the dataframe we obtained after feature selection on XGB"}}