{"cell_type":{"cda73384":"code","33772f57":"code","5e0c9e07":"code","2b9aa042":"code","dad9dfd2":"code","b3398b76":"code","3a0f2a14":"code","f8726a30":"code","535b5932":"code","ce3fadf2":"code","2fc7fc7a":"code","1a9b16c0":"code","63aa478e":"code","5877c6f1":"code","4028c531":"code","6c5ecb6d":"code","b3c9f3e2":"code","f53e4946":"code","4feaf040":"code","3c53f988":"code","ff123f45":"code","b2e0d6b0":"code","d9887c72":"code","0e60bed0":"code","f3d94e81":"code","ff472d5c":"code","16527b74":"code","78ced72e":"code","c5a3bf49":"code","e70efa9d":"code","2ebf96a2":"code","03256880":"code","f11f4f32":"code","044b74bc":"code","75c44f3e":"code","38345fc7":"code","613a1d49":"code","6ddc11b2":"code","0c4ac7ef":"markdown","24659874":"markdown","77adc73e":"markdown","10b22b88":"markdown","31a54242":"markdown","f42dc38f":"markdown","5ddb4864":"markdown","0fdb2681":"markdown","e8384380":"markdown","14c1669f":"markdown","db1fbdb5":"markdown","c640a0dc":"markdown","aa965268":"markdown","42877578":"markdown","0088557a":"markdown","cf85705a":"markdown","8be5c5f5":"markdown","54b56253":"markdown","1713dbbe":"markdown","1adefaf0":"markdown","d7ba6692":"markdown","fb40d3a7":"markdown","2f78788d":"markdown","d073b0f6":"markdown","5adf637e":"markdown","d1520dfc":"markdown","c7ddeda5":"markdown","d628436a":"markdown","bf750c8b":"markdown"},"source":{"cda73384":"# importing nesessary Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","33772f57":"# Reading Data\nproject_data = pd.read_csv('..\/input\/train.csv')\nresource_data = pd.read_csv('..\/input\/resources.csv')\ntest_data=pd.read_csv('..\/input\/test.csv')\n\n","5e0c9e07":"# Merging two dataframes \n\nprice_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)\nproject_data = pd.merge(project_data, price_data, on='id', how='left')\ntest_data=pd.merge(test_data,price_data,on='id',how='left')","2b9aa042":"# Data Overview\n\nprint(\"Number of data points in train data\", project_data.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", project_data.columns.values)","dad9dfd2":"\ncols = ['Date' if x=='project_submitted_datetime' else x for x in list(project_data.columns)]\nproject_data['Date'] = pd.to_datetime(project_data['project_submitted_datetime'])\nproject_data.drop('project_submitted_datetime', axis=1, inplace=True)\nproject_data.sort_values(by=['Date'], inplace=True)\nproject_data = project_data[cols]\n","b3398b76":"cols = ['Date' if x=='project_submitted_datetime' else x for x in list(test_data.columns)]\ntest_data['Date'] = pd.to_datetime(test_data['project_submitted_datetime'])\ntest_data.drop('project_submitted_datetime', axis=1, inplace=True)\ntest_data.sort_values(by=['Date'], inplace=True)\ntest_data = test_data[cols]","3a0f2a14":"\nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)\nproject_data.drop(['project_essay_1'], axis=1, inplace=True)\nproject_data.drop(['project_essay_2'], axis=1, inplace=True)\nproject_data.drop(['project_essay_3'], axis=1, inplace=True)\nproject_data.drop(['project_essay_4'], axis=1, inplace=True)\n\ntest_data[\"essay\"] = test_data[\"project_essay_1\"].map(str) +\\\n                        test_data[\"project_essay_2\"].map(str) + \\\n                        test_data[\"project_essay_3\"].map(str) + \\\n                        test_data[\"project_essay_4\"].map(str)\ntest_data.drop(['project_essay_1'], axis=1, inplace=True)\ntest_data.drop(['project_essay_2'], axis=1, inplace=True)\ntest_data.drop(['project_essay_3'], axis=1, inplace=True)\ntest_data.drop(['project_essay_4'], axis=1, inplace=True)","f8726a30":"project_data.isna().sum()","535b5932":"project_data.fillna(value='undefined',inplace=True)\ntest_data.fillna(value='undefined',inplace=True)\nproject_data.isna().sum()\n\n","ce3fadf2":"y = project_data['project_is_approved'].values\nproject_data.drop(['project_is_approved'], axis=1, inplace=True)\nproject_data.head(1)\nx=project_data","2fc7fc7a":"x_train,x_cv,y_train,y_cv=train_test_split(x,y,test_size=0.33,stratify=y)","1a9b16c0":"# Function to Pre Process project subject Categories\n\ndef clean_categories(df,col='project_subject_categories'):\n    catogories = list(df[col].values)\n    cat_list = []\n    for i in catogories:\n        temp = \"\"\n        for j in i.split(','): \n            if 'The' in j.split(): \n                j=j.replace('The','') \n            j = j.replace(' ','') \n            temp+=j.strip()+\" \" \n            temp = temp.replace('&','_')\n        cat_list.append(temp.strip())\n    \n    df['clean_categories'] = cat_list\n    df.drop([col], axis=1, inplace=True)\n\n    from collections import Counter\n    my_counter = Counter()\n    for word in df['clean_categories'].values:\n        my_counter.update(word.split())\n\n    cat_dict = dict(my_counter)\n    sorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\n    return sorted_cat_dict","63aa478e":"sorted_dict_key_x_train=clean_categories(x_train)\nsorted_dict_key_x_cv=clean_categories(x_cv)\nsorted_dict_key_test=clean_categories(test_data)","5877c6f1":"# Function to Pre Process project subject Sub Categories\n\ndef clean_subcategories(df,col='project_subject_subcategories'):\n    catogories = list(df[col].values)\n    sub_cat_list = []\n    for i in catogories:\n        temp = \"\"\n        for j in i.split(','): \n            if 'The' in j.split(): \n                j=j.replace('The','')\n            j = j.replace(' ','') \n            temp+=j.strip()+\" \" \n            temp = temp.replace('&','_') \n        sub_cat_list.append(temp.strip())\n    \n    df['clean_subcategories'] = sub_cat_list\n    df.drop([col], axis=1, inplace=True)\n\n    from collections import Counter\n    my_counter = Counter()\n    for word in df['clean_subcategories'].values:\n        my_counter.update(word.split())\n\n    sub_cat_dict = dict(my_counter)\n    sorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\n    return sorted_sub_cat_dict","4028c531":"sorted_sub_dict_key_x_train=clean_subcategories(x_train)\nsorted_sub_dict_key_x_cv=clean_subcategories(x_cv)\nsorted_sub_dict_key_test=clean_subcategories(test_data)\n","6c5ecb6d":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","b3c9f3e2":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","f53e4946":"# Preprocessing Essay Column\nfrom nltk.stem.snowball import SnowballStemmer\n#from tqdm import tqdm_notebook as tqdm\nstemmer=SnowballStemmer('english')\ndef preprocess_essay(data):\n    preprocessed_data=[]\n    for sentance in (data.values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent=' '.join(stemmer.stem(word) for word in sent.split() if word not in stopwords)\n        preprocessed_data.append(sent.lower().strip())\n    return preprocessed_data\n\npreprocessed_essays_x_train=preprocess_essay(x_train['essay'])\npreprocessed_essays_x_cv=preprocess_essay(x_cv['essay'])\npreprocessed_essays_test=preprocess_essay(test_data['essay'])","4feaf040":"# Preprocessing project title column\n\ndef preprocess_title(data):\n    preprocessed_data=[]\n    for sentance in (data.values):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent=' '.join(stemmer.stem(word) for word in sent.split() if word not in stopwords)\n        preprocessed_data.append(sent.lower().strip())\n    return preprocessed_data\n\npreprocessed_title_x_train=preprocess_title(x_train['project_title'])\npreprocessed_title_x_cv=preprocess_title(x_cv['project_title'])\npreprocessed_title_test=preprocess_title(test_data['project_title'])\n","3c53f988":"# data overview of project grade category\nproject_data['project_grade_category'].tail(3)","ff123f45":"# Preprocessing Grade\n\ndef preprocess_grade(data):\n    preprocessed_data=[]\n    for sentence in (data.values):\n        sentence=sentence.replace('Grades','')\n        sentence=sentence.replace('-','to')\n        preprocessed_data.append(sentence)\n    return preprocessed_data\n    \npreprocessed_grade_x_train=preprocess_grade(x_train['project_grade_category'])\npreprocessed_grade_x_cv=preprocess_grade(x_cv['project_grade_category'])\npreprocessed_grade_test=preprocess_grade(test_data['project_grade_category'])\n\n\n\n","b2e0d6b0":"x_train['clean_grade']=preprocessed_grade_x_train\nx_cv['clean_grade']=preprocessed_grade_x_cv\ntest_data['clean_grade']=preprocessed_grade_test","d9887c72":"test_data.columns","0e60bed0":"consolidated=pd.concat([x_train,x_cv,test_data],axis=0)","f3d94e81":"# Label Encoder\ncols = [\n    'teacher_prefix', \n    'school_state', \n    'clean_categories', \n    'clean_subcategories', \n    'clean_grade'\n]\n\nfor c in cols:\n    le = LabelEncoder()\n    le.fit(consolidated[c])\n    x_train[c] = le.transform(x_train[c].astype(str))\n    x_cv[c] = le.transform(x_cv[c].astype(str))\n    test_data[c] = le.transform(test_data[c].astype(str))","ff472d5c":"# Encoding categorical features of Train Data\ncat_encoded_x_train=x_train['clean_categories'].values.reshape(-1,1)\nsub_cat_encoded_x_train=x_train['clean_subcategories'].values.reshape(-1,1)\nstate_encoded_x_train=x_train['school_state'].values.reshape(-1,1)\nprefix_encoded_x_train=x_train['teacher_prefix'].values.reshape(-1,1)\ngrade_encoded_x_train=x_train['clean_grade'].values.reshape(-1,1)\nprint(cat_encoded_x_train.shape)","16527b74":"# Encoding categorical features of Cross Validation Data\ncat_encoded_x_cv=x_cv['clean_categories'].values.reshape(-1,1)\nsub_cat_encoded_x_cv=x_cv['clean_subcategories'].values.reshape(-1,1)\nstate_encoded_x_cv=x_cv['school_state'].values.reshape(-1,1)\nprefix_encoded_x_cv=x_cv['teacher_prefix'].values.reshape(-1,1)\ngrade_encoded_x_cv=x_cv['clean_grade'].values.reshape(-1,1)\nprint(cat_encoded_x_cv.shape)","78ced72e":"# Encoding categorical features of Test Data\ncat_encoded_test=test_data['clean_categories'].values.reshape(-1,1)\nsub_cat_encoded_test=test_data['clean_subcategories'].values.reshape(-1,1)\nstate_encoded_test=test_data['school_state'].values.reshape(-1,1)\nprefix_encoded_test=test_data['teacher_prefix'].values.reshape(-1,1)\ngrade_encoded_test=test_data['clean_grade'].values.reshape(-1,1)\nprint(cat_encoded_test.shape)","c5a3bf49":"# TFIDF Encoding of essay text\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer=TfidfVectorizer(min_df=10,max_features=5000)\nvectorizer.fit(preprocessed_essays_x_train)\nX_train_essay_tfidf = vectorizer.transform(preprocessed_essays_x_train)\nX_cv_essay_tfidf = vectorizer.transform(preprocessed_essays_x_cv)\ntest_essay_tfidf = vectorizer.transform(preprocessed_essays_test)\nprint(\"After vectorizations\")\nprint(X_train_essay_tfidf.shape, y_train.shape)\nprint(X_cv_essay_tfidf.shape, y_cv.shape)\nprint(test_essay_tfidf.shape)","e70efa9d":"# TFIDF encoding of project_title ,We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer=TfidfVectorizer(min_df=10)\nvectorizer.fit(preprocessed_title_x_train)\nX_train_title_tfidf = vectorizer.transform(preprocessed_title_x_train)\nX_cv_title_tfidf = vectorizer.transform(preprocessed_title_x_cv)\ntest_title_tfidf = vectorizer.transform(preprocessed_title_test)\nprint(\"After vectorizations\")\nprint(X_train_title_tfidf.shape, y_train.shape)\nprint(X_cv_title_tfidf.shape, y_cv.shape)\nprint(test_title_tfidf.shape)\n","2ebf96a2":"# Normalizing price\nfrom sklearn.preprocessing import Normalizer\nnormalizer = Normalizer()\nnormalizer.fit(x_train['price'].values.reshape(-1,1))\n\nX_train_price_norm = normalizer.transform(x_train['price'].values.reshape(-1,1))\nX_cv_price_norm = normalizer.transform(x_cv['price'].values.reshape(-1,1))\ntest_price_norm = normalizer.transform(test_data['price'].values.reshape(-1,1))\nprint(\"After vectorizations\")\nprint(X_train_price_norm.shape, y_train.shape)\nprint(X_cv_price_norm.shape, y_cv.shape)\n","03256880":"# Normalizing teacher_number_of_previously_posted_projects\nnormalizer = Normalizer()\nnormalizer.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_train_teacher_number_of_previously_posted_projects_norm = normalizer.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_cv_teacher_number_of_previously_posted_projects_norm = normalizer.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\ntest_teacher_number_of_previously_posted_projects_norm = normalizer.transform(test_data['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nprint(\"After vectorizations\")\nprint(X_train_teacher_number_of_previously_posted_projects_norm.shape, y_train.shape)\nprint(X_cv_teacher_number_of_previously_posted_projects_norm.shape, y_cv.shape)\n","f11f4f32":"from scipy.sparse import hstack\nx_train=hstack((cat_encoded_x_train,sub_cat_encoded_x_train,state_encoded_x_train,prefix_encoded_x_train,grade_encoded_x_train,X_train_title_tfidf,\nX_train_essay_tfidf,X_train_price_norm,X_train_teacher_number_of_previously_posted_projects_norm)).tocsr()\nx_cv=hstack((cat_encoded_x_cv,sub_cat_encoded_x_cv,state_encoded_x_cv,prefix_encoded_x_cv,grade_encoded_x_cv,X_cv_title_tfidf,\nX_cv_essay_tfidf,X_cv_price_norm,X_cv_teacher_number_of_previously_posted_projects_norm)).tocsr()\ntest_eval=hstack((cat_encoded_test,sub_cat_encoded_test,state_encoded_test,prefix_encoded_test,grade_encoded_test,test_title_tfidf,\ntest_essay_tfidf,test_price_norm,test_teacher_number_of_previously_posted_projects_norm)).tocsr()","044b74bc":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_cv.shape)\nprint(y_cv.shape)\nprint(test_eval.shape)","75c44f3e":"# Trainning XGBoost Model\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve\n\nclf = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,   \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=5, \n                      )\nclf.fit(x_train,y_train)\n\n\n","38345fc7":"# Predicting Train and AUC Scores using our model\ny_pred_train=clf.predict(x_train)\ntrain_fpr,train_tpr,train_threshold=roc_curve(y_pred_train,y_train)\ny_pred_cv=clf.predict(x_cv)\ncv_fpr,cv_tpr,cv_threshold=roc_curve(y_pred_cv,y_cv)\n","613a1d49":"# Plot results obtained from the model \nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(cv_fpr, cv_tpr, label=\"CV AUC =\"+str(auc(cv_fpr, cv_tpr)))\nplt.grid()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('FPR vs TPR')\nplt.legend()","6ddc11b2":"submission = pd.DataFrame()\ntest_pred=clf.predict_proba(test_eval)[:,1]\nsubmission['id'] = test_data['id']\nsubmission['project_is_approved'] = test_pred\nsubmission.to_csv('submission.csv', index=False)","0c4ac7ef":"#### TFIDF vectorizer","24659874":"## Text preprocessing","77adc73e":"# A Simple XGBoost Approach to solve DonorsChoose Application Screening","10b22b88":"## Submission","31a54242":"Here we will use XGBoost Classifier which is a Sckit Learn Wrapper for original XGBoost algorithm . For simplicity let's take 1000 estimators with learning rate as 0.01 .For better result we can use KFoldCrossValidation using GridSearch.","f42dc38f":" ### Merging all the above features","5ddb4864":"We will use TF-IDF Vectorizer to vectorize our text data","0fdb2681":"## Endnotes:","e8384380":"### Vectorizing Numerical features","14c1669f":"## Spliting Data","db1fbdb5":"Let's merge categorical,Text and Numerical Data to prepare final the evaluation data matrixes","c640a0dc":"we are getting 4 'Na' values in teacher prefix column. let's replace those with 'undefined'","aa965268":"let's split our data in to Train and CV to cross validate performance of our model before actual submission ","42877578":"## Data Preprocessing","0088557a":"## Project Grade Category","cf85705a":"As we are using Tree based Classifier we can not use One Hot Encoding to vectorize our text data ,instead we will use Label Encoding of Sckit Learn . It also comes with the limitation that it can not handle efficiently if a new category occurs at test data that was not seen in train data. To remove that probability we will consolidate the Train , Cross Validation and Test Data in order to get all categorical values.","8be5c5f5":"Take a look at the shapes of our final matrixes","54b56253":"## Reading Data","1713dbbe":"## Vectorizing Categorical data","1adefaf0":"The main intention of this kernel is to get familier with the XGBoost algorithm with simple featurization techniques .I am adding some interesting reads  on the topic :\n\n[1] https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/ \n\n[2] https:\/\/stats.stackexchange.com\/questions\/173390\/gradient-boosting-tree-vs-random-forest","d7ba6692":"## preprocessing of `project_subject_subcategories`","fb40d3a7":"## Vectorizing Text data","2f78788d":"we need to check for NaN values in our data , as it may lead to inconsistancy ","d073b0f6":"Now Let's take a first look of our data\n","5adf637e":"## preprocessing of `project_subject_categories`","d1520dfc":"# XGBoost Classifier","c7ddeda5":"merging two dataframers in order to prepare train data","d628436a":"As we have 'project_submitted_datetime available' with us we are sorting the data by date of project submission","bf750c8b":"we are merging essay columns in to one to preprocess easily"}}