{"cell_type":{"4a10f2e8":"code","95691935":"code","6aee8d25":"code","38bf27ab":"code","3509bce2":"code","42be8cd5":"code","69a745d0":"code","200a289f":"code","930fb8a3":"code","e6b235ba":"code","c4155925":"code","1a9fe09b":"code","5d2becb1":"code","ab9348ff":"code","76f750a7":"code","655426fe":"code","241414ba":"code","83308508":"code","668f2d56":"code","2de9b17c":"code","15a42776":"code","4b9681de":"code","21b77951":"code","d945cba8":"code","0b2d54ed":"code","cb9a750f":"code","21ac98d7":"markdown","cc574386":"markdown","72777d78":"markdown","5f858d5b":"markdown","1faa7e58":"markdown","440fbcd7":"markdown","a3692a47":"markdown","5c2157f4":"markdown","d5ae66f2":"markdown","ddb0e9c5":"markdown","a17d0e0f":"markdown","84b4676a":"markdown","b1ae3383":"markdown","8642023a":"markdown","359a7450":"markdown","c02dec9a":"markdown","75d0c7b7":"markdown","e35c5dc5":"markdown","e5042db1":"markdown","665ec286":"markdown","548a3a63":"markdown","f0f171e6":"markdown","3f2b8204":"markdown","1f6a2c7e":"markdown"},"source":{"4a10f2e8":"# importing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold,GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\n\n%matplotlib inline","95691935":"# filepath = 'data\/Human_Activity_Recognition_Using_Smartphones_Data.csv'\ntrain = pd.read_csv('..\/input\/human-activity-recognition-with-smartphones\/train.csv')\ntest = pd.read_csv('..\/input\/human-activity-recognition-with-smartphones\/test.csv')\ntrain.head()","6aee8d25":"train.dtypes.value_counts()","38bf27ab":"# see the min and max of data excluding the target\nprint('min = ',train.iloc[:, :-1].min().value_counts())\nprint('max = ',train.iloc[:, :-1].max().value_counts())","3509bce2":"# Examine the breakdown of activities-- to see if balanced or not\ntrain.Activity.value_counts(normalize=True)","42be8cd5":"train.isnull().sum().all() and test.isnull().sum().all()","69a745d0":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain['Activity'] = le.fit_transform(train.Activity)\ntest['Activity'] = le.fit_transform(test.Activity)\ntrain['Activity'].sample(5)\n## END SOLUTION","200a289f":"# Calculate the correlation values\nfeature_cols = train.columns[:-1]\ncorr_values = train[feature_cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values\n               .stack()\n               .to_frame()\n               .reset_index()\n               .rename(columns={'level_0':'feature1',\n                                'level_1':'feature2',\n                                0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()\ncorr_values","930fb8a3":"# The most highly correlated values\ncorr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')","e6b235ba":"X_train = train[feature_cols]\nX_test = test[feature_cols]\ny_train = train['Activity']\ny_test  = test['Activity']","c4155925":"y_train.value_counts(normalize=True)","1a9fe09b":"y_test.value_counts(normalize=True)","5d2becb1":"from sklearn.linear_model import LogisticRegression\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)","ab9348ff":"from sklearn.linear_model import LogisticRegressionCV\n# L1 regularized logistic regression\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)\n# L2 regularized logistic regression\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)","76f750a7":"# Predict the class and the probability for each\ny_pred = list()\ny_prob = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()","655426fe":"y_prob.head()","241414ba":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nmetrics = list()\ncm = dict()\n\nfor lab in coeff_labels:\n\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n              average='weighted')\n    \n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)","83308508":"metrics","668f2d56":"from sklearn.metrics import f1_score\n\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nf1_lr = f1_score(y_pred, y_test, average='weighted')\nf1_lr","2de9b17c":"## Display or plot the confusion matrix for each model.\n\nfig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\naxList[-1].axis('off')\n\nfor ax,lab in zip(axList[:-1], coeff_labels):\n    sns.heatmap(cm[lab], ax=ax, annot=True, fmt='d');\n    ax.set(title=lab);\n    \nplt.tight_layout()","15a42776":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\nknn = KNeighborsClassifier(n_neighbors=3, weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nf1_knn = f1_score(y_pred, y_test, average='weighted')\nf1_knn","4b9681de":"from sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntuning_parameters = {'max_depth':[2,4,6,8,10],\n                     'min_samples_leaf':[2,4,6,8,10], \n                     'min_samples_split':[2,4,6,8,10]}\n\nscorer = make_scorer(f1_score, average = 'micro')\nGR = GridSearchCV(DecisionTreeClassifier(random_state=42), tuning_parameters, scoring=scorer,)\nGR = GR.fit(X_train, y_train)\n\nprint('GR.best_estimator_: ', GR.best_estimator_)\nprint('GR.best_score_: ', GR.best_score_)\nprint('GR.best_params_: ', GR.best_params_)","21b77951":"GR.best_score_, GR.best_params_","d945cba8":"y_pred = GR.predict(X_test)\nf1_dt = f1_score(y_pred, y_test, average='weighted')\nf1_dt","0b2d54ed":"from sklearn.ensemble import GradientBoostingClassifier\nGBC = GradientBoostingClassifier(max_features=5, n_estimators=100, random_state=42)\nGBC.fit(X_train.values, y_train.values)\ny_pred = GBC.predict(X_test)\nf1_GBC = f1_score(y_pred, y_test, average='weighted')\nf1_GBC","cb9a750f":"pd.DataFrame({'Logistic Regression':f1_lr, 'KNN':f1_knn, \n                              'Decision Trees':f1_dt, 'Gradient Boosting':f1_GBC},index = ['F1_SCORE'])","21ac98d7":"logistic regression without regularization got us the highest F1_Score so we will choose it and Decision Trees took too long and got the worst score","cc574386":"Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either `LabelEncoder` needs to be used to convert the activity labels to integers, or if `DictVectorizer` is used, the resulting matrix must be converted to a non-sparse array.  \n\nwe will use `LabelEncoder` to fit_transform the \"Activity\" column, and look at 5 random values.","72777d78":"* Predict and store the class for each model.\n* Store the probability for the predicted class for each model. ","5f858d5b":"## `3.Decision Trees`","1faa7e58":"### `error metrics`\n\nFor each model, calculate the following error metrics: \n* Accuracy\n* Precision\n* Recall\n* F-score\n* Confusion Matrix","440fbcd7":"The data are all scaled from -1 (minimum) to 1.0 (maximum).","a3692a47":"# `Introduction`\n\nWe will be using the [Human Activity Recognition with Smartphones](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones) database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n\nFor each record in the dataset it is provided: \n\n- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration. \n- Triaxial Angular velocity from the gyroscope. \n- A 561-feature vector with time and frequency domain variables. \n- Its activity label. \n\nMore information about the features is available on the website above.","5c2157f4":"# `looking at correlation`\n\n* Calculate the correlations between the dependent variables.\n* Create a histogram of the correlation values\n* Identify those that are most correlated (either positively or negatively).","d5ae66f2":"## `1.Logistic Regression`","ddb0e9c5":"## Human Activity Recognition Using Smartphones Data Set","a17d0e0f":"# `Exploring and feature engineering`","84b4676a":"### so we will chooce `GradientBoostingClassifier` or `logistic regression`","b1ae3383":"# `key findings`","8642023a":"# `Applying classification models`\n\n- Logistic Regression\n- K-Nearest Neighbors (KNeighbors)\n- Decision Trees\n- Ensemble Methods (Gradient Boosting)","359a7450":"# `Plan for data exploration`\n1. Exploring data \n    * Examine the data types and value_counts\n2. feature engineering \n    * see the data distribution\n    * removing unimportant data if found \n    * dealing with missing (NaN) values if found\n    * feature scalling for continuous variables if needed\n3. encoding\n    * encoding for categorical variables if found as to Encode the activity label as an integer\n4. Spliting the Data\n5. Applying classification models\n    * Logistic Regression\n    * K-Nearest Neighbors (KNeighbors)\n    * Decision Trees\n    * Ensemble Methods (Gradient Boosting) \n6. Selecting the best model\n7. Next steps","c02dec9a":"# `Data split`\n\n* This can be done using any method, but consider using Scikit-learn's `StratifiedShuffleSplit` to maintain the same ratio of predictor classes.\n","75d0c7b7":"we can use another encoding method like `LabelBinarizer` and we can try another ensemble method like `Random forest`","e35c5dc5":"# `encoding`","e5042db1":"# `Next steps`","665ec286":"## `4.GradientBoosting`","548a3a63":"The data columns are all floats except for the activity label.","f0f171e6":"we maintained the distribution of the target class seccussfuly","3f2b8204":"## `2.K-Nearest Neighbors (KNeighbors)`","1f6a2c7e":"# `Selecting best model`"}}