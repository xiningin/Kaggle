{"cell_type":{"67292d6c":"code","957d72cc":"code","73b87cb5":"code","4f488c7b":"code","5145172e":"code","1d9e2e95":"code","d47ec061":"code","65db613b":"code","5a9c6965":"code","20223e17":"code","d6c81d56":"code","4bbb5dd0":"code","200aef27":"code","83200316":"code","a638e9af":"code","441602d4":"code","fd4ddf4f":"code","5adf7f2c":"code","d45ca4cd":"code","a1c6de54":"code","b70d38e5":"code","f44a821a":"code","c8b25908":"code","d9705a7e":"code","fd46b145":"code","2075449f":"code","77c55629":"code","43391c4e":"code","6b34205c":"markdown","6e607536":"markdown","7b49f598":"markdown","d6476fed":"markdown","fdf5ed44":"markdown","a075ea7f":"markdown","f6c7585b":"markdown","3d261b45":"markdown","55ff8131":"markdown","b235300b":"markdown","daee5641":"markdown","31875c1f":"markdown"},"source":{"67292d6c":"%matplotlib inline \nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport time\nimport copy\n\n\nfrom PIL import Image\n\nDATASET_SIZE = 8000\nBATCH_SIZE = 200\nW = H = 256\n\ntrain_path = '..\/input\/train\/'\ntest_path = '..\/input\/test\/'\n\nLABEL_MAP = {\n0: \"Nucleoplasm\" ,\n1: \"Nuclear membrane\"   ,\n2: \"Nucleoli\"   ,\n3: \"Nucleoli fibrillar center\",   \n4: \"Nuclear speckles\"   ,\n5: \"Nuclear bodies\"   ,\n6: \"Endoplasmic reticulum\"   ,\n7: \"Golgi apparatus\"  ,\n8: \"Peroxisomes\"   ,\n9:  \"Endosomes\"   ,\n10: \"Lysosomes\"   ,\n11: \"Intermediate filaments\"  , \n12: \"Actin filaments\"   ,\n13: \"Focal adhesion sites\"  ,\n14: \"Microtubules\"   ,\n15: \"Microtubule ends\"   ,\n16: \"Cytokinetic bridge\"   ,\n17: \"Mitotic spindle\"  ,\n18: \"Microtubule organizing center\",  \n19: \"Centrosome\",\n20: \"Lipid droplets\"   ,\n21: \"Plasma membrane\"  ,\n22: \"Cell junctions\"   ,\n23: \"Mitochondria\"   ,\n24: \"Aggresome\"   ,\n25: \"Cytosol\" ,\n26: \"Cytoplasmic bodies\",\n27: \"Rods & rings\"}\n\nLABELS = []\n\nfor label in LABEL_MAP.values():\n    LABELS.append(label)\n    \ntrain_csv_path = '..\/input\/train.csv'","957d72cc":"df = pd.read_csv(train_csv_path)\n\nTRAINING_SAMPLES = df.shape[0]\n\nprint(\"we have \" + str(TRAINING_SAMPLES) + \" different samples\")\nprint(\"And there are \"+  str(len(df.Target.unique())) + \" different combinations of labels in our dataset\")\n\nimport seaborn as sns\nsns.set(style=\"dark\")\n\nn = 20\n\nvalues = df['Target'].value_counts()[:n].keys().tolist()\ncounts = df['Target'].value_counts()[:n].tolist()\n\nplt.figure(figsize=(6,6))\npal = sns.cubehelix_palette(n, start=2, rot=0, dark=0, light=.75, reverse=True)\ng = sns.barplot(y=counts, x=values, palette=pal)\ng.set_title(str(n)+\" MOST COMMON LABEL COMBINATIONS\")\ng.set_xticklabels(g.get_xticklabels(),rotation=90);","73b87cb5":"from PIL import Image\n\ndef load_image(basepath, image_id):\n    images = np.zeros(shape=(256,256,4))\n    r = Image.open(basepath+image_id+\"_red.png\").resize((256,256))\n    g = Image.open(basepath+image_id+\"_green.png\").resize((256,256))\n    b = Image.open(basepath+image_id+\"_blue.png\").resize((256,256))\n    y = Image.open(basepath+image_id+\"_yellow.png\").resize((256,256))\n\n    images[:,:,0] = np.asarray(r)\n    images[:,:,1] = np.asarray(g)\n    images[:,:,2] = np.asarray(b)\n    images[:,:,3] = np.asarray(y)\n    \n    return images","4f488c7b":"targets = df['Target'].value_counts().keys()\ncounts = df['Target'].value_counts().values\n\nhow_many = counts\/TRAINING_SAMPLES*DATASET_SIZE\n\n# at least one example of each possible combination of labels..\nhow_many = how_many.astype('int')+1\n","5145172e":"import torch\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom skimage import io, transform\nfrom sklearn.preprocessing import MultiLabelBinarizer\nclasses = np.arange(0,28)\nmlb = MultiLabelBinarizer(classes)\nmlb.fit(classes)\n\nclass HumanProteinDataset(Dataset):\n\n    def __init__(self, csv_file,transform=None, test=False):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            test (Boolean): the csv no contains labels\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.test = test\n        self.complete_df = pd.read_csv(csv_file)\n        \n        if not test:\n            self.path = train_path\n            self.loadData()\n        else:\n            self.path = test_path\n            self.df = self.complete_df\n            \n        self.transform = transform\n        \n    def CreateDummyVariables(self):\n        self.complete_df['Targets'] = self.complete_df['Target'].map(lambda x: list(map(int, x.strip().split())))\n            \n    def loadData(self):\n        self.CreateDummyVariables()\n        self.df = pd.DataFrame(columns=['Id','Target'])\n        for i, target in enumerate(targets):\n            fdf = self.complete_df[self.complete_df['Target'] == target]\n            sample = fdf.sample(n=how_many[i], replace=False)\n            self.df = self.df.append(sample)\n        self.df = self.df.sample(frac=1).reset_index(drop=True)\n            \n    def __getitem__(self, idx):\n        \n        image = load_image(self.path, self.df['Id'].iloc[idx])\n        \n        sample = {'image': image}\n\n        if not self.test:\n            target = np.array(self.complete_df['Targets'].iloc[idx])\n            target = mlb.transform([target])\n            sample['target'] = target\n        \n        else:\n            sample['Id'] = self.df['Id'].iloc[idx]\n\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def shape(self):\n        return self.df.shape\n    \nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        \n        image = sample['image']\/255.0\n        \n        totensor = transforms.ToTensor()\n        \n        ret = {'image': totensor(image)}\n        \n        if \"target\" in sample.keys():\n            target = sample['target'][0]\n            ret['target'] = target\n        else:\n            ret['Id'] = sample['Id']\n                  \n        return ret","1d9e2e95":"dataset = HumanProteinDataset(train_csv_path, transform=ToTensor())","d47ec061":"def Show(sample):\n    f, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(25,25), sharey=True)\n\n    title = ''\n    \n    labels =sample['target']\n                \n    for i, label in enumerate(LABELS):\n        if labels[i] == 1:\n            if title == '':\n                title += label\n            else:\n                title += \" & \" + label\n            \n    ax1.imshow(sample['image'][0,:,:],cmap=\"hot\")\n    ax1.set_title('Red')\n    ax2.imshow(sample['image'][1,:,:],cmap=\"copper\")\n    ax2.set_title('Green')\n    ax3.imshow(sample['image'][2,:,:],cmap=\"bone\")\n    ax3.set_title('Blue')\n    ax4.imshow(sample['image'][3,:,:],cmap=\"afmhot\")\n    ax4.set_title('Yellow')\n    f.suptitle(title, fontsize=20, y=0.62)","65db613b":"import random\n\nidxs = random.sample(range(1, dataset.df.shape[0]), 3)\n\nfor idx in idxs:\n    Show(dataset[idx])","5a9c6965":"from torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nimport torch.nn as nn\nimport math\n\ndef prepare_loaders():\n    dataset.loadData()\n    num_train = len(dataset)\n    indices = list(range(num_train))\n    val_size = int(0.45 * num_train) \n\n    # Random, non-contiguous split\n    validation_idx = np.random.choice(indices, size=val_size, replace=False)\n    train_idx = list(set(indices) - set(validation_idx))\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    validation_sampler = SubsetRandomSampler(validation_idx)\n\n    dataset_sizes = {}\n\n    dataset_sizes['train'] = len(train_idx)\n    dataset_sizes['val'] = len(validation_idx)\n    \n    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE,num_workers=0, sampler=train_sampler)\n    validation_loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0,sampler=validation_sampler)\n\n    dataloaders = {}\n\n    dataloaders['train'] = train_loader\n    dataloaders['val'] = validation_loader\n    \n    return (dataloaders, dataset_sizes)\n","20223e17":"dataloaders, dataset_sizes = prepare_loaders()\n\ndataset.df.head()","d6c81d56":"# Wout = 1 + (Win - Kernel_size + 2Padding)\/Stride\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(      #input: 4xWxH\n            nn.Conv2d(4,8,5,1,2),        # input_channels, output_channels, kernel_size, stride, padding   \n            nn.ReLU(),                      \n            nn.MaxPool2d(kernel_size=2), #output: 8xW\/2xH\/2\n        )\n        self.conv2 = nn.Sequential(      #input: 4xWxH\n            nn.Conv2d(8,16,5,1,2),        # input_channels, output_channels, kernel_size, stride, padding   \n            nn.ReLU(),                      \n            nn.MaxPool2d(kernel_size=2), #output: 16xW\/4xH\/4\n        )\n        self.drop_out = nn.Dropout()\n        self.out1 = nn.Linear( int(16 * W\/4 * H\/4), 900)   # fully connected layer, output 28 classes\n        self.out2 = nn.Linear( 900, 28)   # fully connected layer, output 28 classes\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n        output = self.drop_out(x)\n        output = self.out1(x)\n        output = self.out2(output)\n        return output, x    # return x for visualization\n\ndef init_weights(m):\n        if type(m) == nn.Linear:\n            torch.nn.init.xavier_uniform(m.weight)\n            m.bias.data.fill_(0.01)","4bbb5dd0":"# create a new subdataset for training\ndataloaders, dataset_sizes = prepare_loaders()","200aef27":"losses = {}\naccuracys = {}\n\nlosses['train'] = []\nlosses['val'] = []\naccuracys['train'] = []\naccuracys['val'] = []","83200316":"def Train(model, epochs=10, criterion=nn.BCEWithLogitsLoss(reduction='sum'), optimizer= None):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n\n    if optimizer == None:\n        optimizer = optim.Adam(model.parameters(), lr=0.04, betas=(0.9, 0.99))\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"training with device: \" + str(device))\n    \n    model.to(device)\n    \n    for epoch in range(epochs):  # loop over the dataset multiple times\n        print('Epoch {}\/{}'.format(epoch+1, epochs))\n        print('-' * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n                \n            running_loss = 0.0    \n            running_corrects = 0.0\n    \n            for i, data in enumerate(dataloaders[phase], 0):            \n                # get the inputs\n                inputs, labels = data['image'], data['target']\n\n                inputs, labels = inputs.to(device,dtype=torch.float), labels.to(device,dtype=torch.float)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)[0]\n                    preds = outputs > 0\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                 # statistics\n                running_loss += loss.item() * inputs.size(0)\n                labels = labels.data.byte()\n                running_corrects += torch.sum((labels == preds).all(1))\n                                \n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.float() \/ dataset_sizes[phase]\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            losses[phase].append(epoch_loss)\n            accuracys[phase].append(epoch_acc)\n            \n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n                \n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\ndef run_model(model,batch):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    inputs = batch\n    inputs = inputs.to(device,dtype=torch.float)\n    out = model(inputs)\n    out = out[0].cpu()\n    return out","a638e9af":"# model creation and initialization\ncnn = CNN()\ncnn.apply(init_weights)\n\n","441602d4":"# training\ntorch.cuda.empty_cache()\ncnn = Train(cnn, epochs=10,  criterion=nn.BCEWithLogitsLoss(reduction='sum'), optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99)))","fd4ddf4f":"# training\ntorch.cuda.empty_cache()\ncnn = Train(cnn, epochs=10,  criterion=nn.BCEWithLogitsLoss(reduction='sum'), optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99)))\ndataloaders, dataset_sizes = prepare_loaders()\ntorch.cuda.empty_cache()\ncnn = Train(cnn, epochs=10,  criterion=nn.BCEWithLogitsLoss(reduction='sum'), optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99)))\n","5adf7f2c":"plt.plot(np.arange(len(losses['train'])), losses['train'],label=\"train\")\nplt.plot(np.arange(len(losses['val'])), losses['val'], label=\"val\")\nplt.legend()\nplt.title(\"loss by epoch\")\nplt.show()\n\nplt.plot(np.arange(len(accuracys['train'])), accuracys['train'], label=\"train\")\nplt.plot(np.arange(len(accuracys['val'])), accuracys['val'], label=\"val\")\nplt.title(\"accuracy by epoch\")\nplt.legend()\nplt.show()","d45ca4cd":"def save(model, full = True, name=\"model\"):\n    if not full:\n        torch.save(model.state_dict(), name+'_params.pkl')   # save only the parameters\n    else:\n        torch.save(model, name+'.pkl')  # save entire net\n\n        \ndef restore_net(name=\"model\"):\n    model = torch.load(name+'.pkl')\n    return model\n    ","a1c6de54":"save(cnn, name=\"cnn\")\n!ls","b70d38e5":"cnn = restore_net(name=\"cnn\")\n","f44a821a":"#!pip install dropbox\nimport dropbox\ndropbox_path='\/'\ndbx=dropbox.Dropbox('Your access token')\n\n# upload model to dropbox\n\nfile_name='cnn.pkl'\nwith open(file_name, 'rb') as f:\n    dbx.files_upload(f.read(),dropbox_path+file_name,mute=True, mode=dropbox.files.WriteMode.overwrite)\n\n","c8b25908":"# load model from dropbox\ndbx.files_download_to_file(file_name,dropbox_path+file_name)\n!ls","d9705a7e":"submit = pd.read_csv('..\/input\/sample_submission.csv')\n\ndataset_test = HumanProteinDataset(csv_file='..\/input\/sample_submission.csv', transform=transforms.Compose([\n    ToTensor()\n]), test=True)\n\ndataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE,\n                        shuffle=False, num_workers=0)\n","fd46b145":"ids = []\npredictions = []\n\ncnn = cnn.cuda()\n\nfor sample_batched in dataloader_test:\n        out = run_model(cnn,sample_batched['image'])\n        \n        preds = []\n        out = out.detach().numpy()\n        for sample in out:\n            p = \"\"\n            for i,label in enumerate(sample):\n                if label > 0:\n                    p += \" \" + str(i)\n                    print(p)\n            if p == \"\":\n                p = \"0\"\n            else:\n                p = p[1:]\n            preds.append(p)\n\n        ids += list(sample_batched['Id'])\n        predictions += preds\n\n    \n","2075449f":"print(predictions)","77c55629":"df = pd.DataFrame({'Id':ids,'Predicted':predictions})\ndf.to_csv('protein_classification.csv', header=True, index=False)\n\nprint(df)\n","43391c4e":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"protein_classification.csv\"):  \n    csv = df.to_csv( sep=',', encoding='utf-8', index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(df)","6b34205c":"# SAVE MODEL TO A FILE AND RESTORE FROM FILE","6e607536":"# TRAINING A MODEL WITH CNN\n# split training dataset into training+validation\n\nthe function prepare_loaders will call dataset.loadData, that means each time we \"prepare our loaders\" the dataset will contain different images for training and validation.","7b49f598":"Check if our images are being loaded right and visualice some of them:","d6476fed":"Throught this kernel I'm going to made a model for the  [Human Protein Atlas Image Classification](https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification) competition. Our models should be able to classify mixed patterns of proteins in microscope images. That means **this is a multilabel classification problem** so each sample could have more than one label.\n\nAs we have a really huge dataset, I will load \"batches\" of samples each time I train the model. The way I'll train the model will be:\n\n- Randomly load samples from the dataset based on the amount of samples of each type (for me, a type is a possible combination of labels) (proportionally).\n- Train the model with these samples (a few epochs)\n- Reload (randomly) another dataset and repeat  as long as it is required;\n\nMoreover, as I will train the model on a kaggle kernel, I will store my models in a dropbox remote directory and load them from it in order to stop training when needed and be able to continue training that model in other moment.\n\n![process](http:\/\/i67.tinypic.com\/33dza6u.png)\n\nThere are 28 possible label for a sample. Let's start with defining these possible labels and our data paths:","fdf5ed44":"# Visualice training curve","a075ea7f":"# submit\n","f6c7585b":"# SEND AND RECEIVE MODELS FROM\/TO DROPBOX TO STORE THEM\n\nWe need to create a [https:\/\/www.dropbox.com\/developers\/apps\/create](dropbox-app)  and generate an access token.","3d261b45":"# Training function\n\nsimple training for our model. Not use validation yet.","55ff8131":"As our dataset is huge, I will calculate the portion of it that correspond to each possible combination of labels and I will use this value to decide how many samples of each type (combination of labels) I will use for training.","b235300b":"We will use a pytorch type dataset defining that loading when create an instance. \n\nAs you can see, I will not load the images in the dataframe because of the obvious memory limitations. I'll load the image for each sample when __get_item__ is called. \n\nTe he function load_data use the array how_many calculated previously for fill an empty dataframe with samples of the complete_dataframe randomly. Also, afer fill that dataframe, we suffle it in order to get a random order of sample types while training.","daee5641":"# Model definition","31875c1f":"Now, how to load our images for a given id:"}}