{"cell_type":{"b8e89e07":"code","6d85c90c":"code","75facd11":"code","651bbc54":"code","f7b7c9a5":"code","ccd282cd":"code","2fe407ca":"code","da475fe4":"code","a551009b":"code","f0016b11":"code","61972325":"code","72d5f0db":"code","19027e5b":"code","235d704b":"code","d5a7ad70":"code","f32aa7d0":"code","7f883b44":"code","19d529df":"code","a448bf69":"code","03a9c813":"code","f9f46a4e":"code","a28035aa":"code","2174d935":"code","3054aaae":"code","dab418d1":"code","a9f29b0d":"code","fca902b1":"code","5cb171ff":"code","f23a058f":"code","e45643eb":"code","c1cc51bc":"code","f027f9d6":"code","c6c3115c":"code","41a68ce0":"code","f2f7e02e":"code","1c25e1de":"code","b8758407":"code","b947fc8c":"code","459aaa8f":"code","cb9f6601":"code","a3ac0003":"code","c7ea180e":"code","d78023f7":"code","47cdfe05":"code","a4984715":"code","a17e885d":"code","d6eb55ea":"code","32bcbedf":"code","f3c8fcc3":"markdown","df58159b":"markdown","2e3f8c80":"markdown","2cd7cde6":"markdown","64f59c57":"markdown","5cd33b2f":"markdown","ab429a07":"markdown","1d3fc0b9":"markdown","eb68ab0a":"markdown","ee06f9e6":"markdown","25c9edcf":"markdown","c1b0b289":"markdown","b6951b50":"markdown","d6d0098a":"markdown","9971ba80":"markdown","488a772a":"markdown","3c5871e5":"markdown","dbfecb9e":"markdown","5dcde3b1":"markdown","bd4fad8b":"markdown","81552917":"markdown","b244f2ab":"markdown","365814c9":"markdown"},"source":{"b8e89e07":"!rm .\/*.hdf5","6d85c90c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix\n\nimport cv2\nimport os\nimport glob","75facd11":"# Input data files are available in the \"..\/input\/\" directory.\nINPUT_PATH = \"..\/input\/pneumonia-detection\/chest_xray\"\n\n# List the files in the input directory.\nprint(os.listdir(INPUT_PATH))","651bbc54":"base_dir = INPUT_PATH\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'val')\ntest_dir = os.path.join(base_dir, 'test')\n\ntrain_0_dir = os.path.join(train_dir, 'Normal'.upper())\ntrain_1_dir = os.path.join(train_dir, 'Pneumonia'.upper())\n\nval_0_dir = os.path.join(val_dir, 'Normal'.upper())\nval_1_dir = os.path.join(val_dir, 'Pneumonia'.upper())\n\ntest_0_dir = os.path.join(test_dir, 'Normal'.upper())\ntest_1_dir = os.path.join(test_dir, 'Pneumonia'.upper())\n\ndef get_data_list():\n    train_0_list = [os.path.join(train_0_dir, fn) for fn in os.listdir(train_0_dir)]\n    train_1_list = [os.path.join(train_1_dir, fn) for fn in os.listdir(train_1_dir)]\n    val_0_list = [os.path.join(val_0_dir, fn) for fn in os.listdir(val_0_dir)]\n    val_1_list = [os.path.join(val_1_dir, fn) for fn in os.listdir(val_1_dir)]\n    test_0_list = [os.path.join(test_0_dir, fn) for fn in os.listdir(test_0_dir)]\n    test_1_list = [os.path.join(test_1_dir, fn) for fn in os.listdir(test_1_dir)]\n\n    # list dir numbers\n    print('total picture numbers in train_0_dir: ', len(train_0_list))\n    print('total picture numbers in train_1_dir: ', len(train_1_list))\n    print('total picture numbers in val_0_dir: ', len(val_0_list))\n    print('total picture numbers in val_1_dir: ', len(val_1_list))\n    print('total picture numbers in test_0_dir: ', len(test_0_list))\n    print('total picture numbers in test_1_dir: ', len(test_1_list))\n\n    return (train_0_list, train_1_list, val_0_list, val_1_list, test_0_list, test_1_list)","f7b7c9a5":"(train_0_list, train_1_list, val_0_list, val_1_list, test_0_list, test_1_list) = get_data_list()","ccd282cd":"import random \n(mv_cnt_0, mv_cnt_1) = (300, 300)\n\nif len(val_0_list) < mv_cnt_0:\n    mv_list_0 = random.sample(train_0_list, mv_cnt_0)\n    mv_list_1 = random.sample(train_1_list, mv_cnt_1)\n    train_0_list = [fn for fn in train_0_list if not fn in mv_list_0]\n    train_1_list = [fn for fn in train_1_list if not fn in mv_list_1]\n    val_0_list += mv_list_0\n    val_1_list += mv_list_1\n    \n    print('total picture numbers in train_0_dir: ', len(train_0_list))\n    print('total picture numbers in train_1_dir: ', len(train_1_list))\n    print('total picture numbers in val_0_dir: ', len(val_0_list))\n    print('total picture numbers in val_1_dir: ', len(val_1_list))\n    print('total picture numbers in test_0_dir: ', len(test_0_list))\n    print('total picture numbers in test_1_dir: ', len(test_1_list))","2fe407ca":"(left, top) = (15, 40)\n(y1, y2, x1, x2) = (top,top+200, left,left+200)\ndef image_resize(img_path):\n    # print(dataset.shape)\n    \n    im = cv2.imread(img_path)\n    im = cv2.resize(im, (224,224))\n    if im.shape[2] == 1:\n        # np.dstack(): Stack arrays in sequence depth-wise (along third axis).\n        # https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.dstack.html\n        im = np.dstack([im, im, im])\n        \n        # ----------------------------------------------------------------------------------------\n        # cv2.cvtColor(): The function converts an input image from one color space to another. \n        # [Ref.1]: \"cvtColor - OpenCV Documentation\"\n        #     - https:\/\/docs.opencv.org\/2.4\/modules\/imgproc\/doc\/miscellaneous_transformations.html\n        # [Ref.2]: \"Python\u8ba1\u7b97\u673a\u89c6\u89c9\u7f16\u7a0b- \u7b2c\u5341\u7ae0 OpenCV\" \n        #     - https:\/\/yongyuan.name\/pcvwithpython\/chapter10.html\n        # ----------------------------------------------------------------------------------------\n    x_image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    x_image = x_image[y1:y2, x1:x2]\n    x_image = cv2.resize(x_image, (150,150))\n    # Normalization\n    # x_image = x_image.astype(np.float32)\/255.\n    return x_image","da475fe4":"import matplotlib.pyplot as plt\nimport matplotlib.image as mimg\n%matplotlib inline\nimport cv2\nimport numpy as np","a551009b":"fn_list_0 = train_0_list[:4]\nfn_list_1 = train_1_list[:4]\n\nfig, ax = plt.subplots(2, 4, figsize=(20,10))\nfor i, axi in enumerate(ax.flat):\n    img_path = None\n    if i < 4:\n        img_path = fn_list_0[i]\n    else:\n        img_path = fn_list_1[i-4]\n    img = image_resize(img_path)#.astype(np.uint8)\n    axi.imshow(img, cmap='bone')\n    axi.set_title(img_path.split('\/')[-1])\n    axi.set(xticks=[], yticks=[])","f0016b11":"def create_dataset(img_path_list_0, img_path_list_1, return_fn = False):\n    # list of the paths of all the image files\n    normal = img_path_list_0\n    pneumonia = img_path_list_1\n\n    # --------------------------------------------------------------\n    # Data-paths' format in (img_path, label) \n    # labels : for [ Normal cases = 0 ] & [ Pneumonia cases = 1 ]\n    # --------------------------------------------------------------\n    normal_data = [(image, 0) for image in normal]\n    pneumonia_data = [(image, 1) for image in pneumonia]\n\n    image_data = normal_data + pneumonia_data\n\n    # Get a pandas dataframe for the data paths \n    image_data = pd.DataFrame(image_data, columns=['image', 'label'])\n#     print(image_data.head(5))\n    # Shuffle the data \n    image_data = image_data.sample(frac=1., random_state=100).reset_index(drop=True)\n    \n    # Importing both image & label datasets...\n    (x_images, y_labels) = ([image_resize(image_data.iloc[i][0]) for i in range(len(image_data))], \n                         [image_data.iloc[i][1] for i in range(len(image_data))])\n\n    # Convert the list into numpy arrays\n    x_images = np.array(x_images)\n    y_labels = np.array(y_labels)\n    \n    print(\"Total number of images: \", x_images.shape)\n    print(\"Total number of labels: \", y_labels.shape)\n    \n    if not return_fn:\n        return (x_images, y_labels)\n    else:\n        return (x_images, y_labels, image_data.image.values)","61972325":"# Import train dataset...\n(x_train, y_train) = create_dataset(train_0_list, train_1_list)\n\nprint(x_train.shape)\nprint(y_train.shape)","72d5f0db":"# Import val dataset...\n(x_val, y_val) = create_dataset(val_0_list, val_1_list)","19027e5b":"from keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\n# base_model = ResNet50(weights='..\/input\/keras-pretrained-models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', input_shape=(150, 150, 3), include_top = False, pooling = 'avg')\n# base_model = VGG16(weights='..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', input_shape=(150, 150, 3), include_top = False)\nbase_model = InceptionResNetV2(weights='..\/input\/keras-pretrained-models\/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5', \n                   input_shape=(150, 150, 3), include_top = False)\nbase_model.summary()","235d704b":"# create data generator (without data augment)\nfrom keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport keras.backend as K\n\n# rescale all image by 1\/255 \ndata_batch_size = 20\n\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","d5a7ad70":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef get_pred_score(y_true, y_pred):\n    mat = confusion_matrix(y_true, y_pred)\n    print(mat)\n\n    plt.figure(figsize=(8,6))\n    sns.heatmap(mat, square=False, annot=True, fmt ='d', cbar=True, annot_kws={\"size\": 16})\n    plt.title('0 : Normal   1 : Pneumonia', fontsize = 20)\n    plt.xticks(fontsize = 16)\n    plt.yticks(fontsize = 16)\n    plt.xlabel('predicted value', fontsize = 20)\n    plt.ylabel('true value', fontsize = 20)\n    plt.show()\n\n    tn, fp, fn, tp = mat.ravel()\n    print('\\ntn = {}, fp = {}, fn = {}, tp = {} '.format(tn, fp, fn, tp))\n\n    precision = tp\/(tp+fp)\n    recall = tp\/(tp+fn)\n    accuracy = (tp+tn)\/(tp+tn+fp+fn)\n    f1_score = 2. * precision * recall \/ (precision + recall)\n    f2_score = 5. * precision * recall \/ (4. * precision + recall)\n\n    print(\"Test Recall of the model \\t = {:.4f}\".format(recall))\n    print(\"Test Precision of the model \\t = {:.4f}\".format(precision))\n    print(\"Test Accuracy of the model \\t = {:.4f}\".format(accuracy))\n    print(\"Test F1 score of the model \\t = {:.4f}\".format(f1_score))\n    print(\"Test F2 score of the model \\t = {:.4f}\".format(f2_score))","f32aa7d0":"from keras import layers, models, Model\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","7f883b44":"# Import train dataset...\n(x_test, y_test, test_fns) = create_dataset(test_0_list, test_1_list, return_fn=True)","19d529df":"x = base_model.output\n# x = layers.GlobalAveragePooling2D()(x)\nx = layers.Flatten()(x)\nx = layers.Dense(512, activation = 'relu')(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(1, activation='sigmoid')(x)\nmodel = Model(base_model.input,x)","a448bf69":"# # Freezing a layer or set of layers means preventing their weights from being updated during training.\n# freezing_layer = None\n# model.trainable = False\nprint('This is the number of trainable weights: ', len(model.trainable_weights))\n# for layer in reversed(model.layers):\n#     # check to see if the layer has a 4D output\n#     if len(layer.output_shape) == 4:\n#         freezing_layer = layer.name\n#         break\n# print('Freezing layer = ', freezing_layer)\n# set_trainable = False\n# for layer in model.layers:\n#     if layer.name == freezing_layer:\n#         set_trainable = True\n#       # set trainable = True for layers after block5_conv1\n#     if set_trainable:\n#         layer.trainable = True\n#         print(layer.name)\n#     else:\n#         layer.trainable = False\n# print('This is the number of trainable weights after freezing the conv base:', len(model.trainable_weights))","03a9c813":"model.summary()","f9f46a4e":"# show trainable weights\n# [x.name for x in model.trainable_weights]","a28035aa":"# use ImageGenerator to generate more training data\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,  # Rescales all images by 1\/255\n    rotation_range = 10,\n    width_shift_range = 0.2, height_shift_range = 0.2,\n    fill_mode = 'nearest', shear_range = 0.2,\n    zoom_range = 0.2, horizontal_flip=False, \n)\ntrain_datagen.fit(x_train)\nval_datagen = ImageDataGenerator(rescale=1.\/255) #validation set no need to augment\nval_datagen.fit(x_val)\n\ntrain_generator = train_datagen.flow(x_train, y_train, batch_size=32) #increase batch size to 32\nval_generator = val_datagen.flow(x_val, y_val, batch_size=32) #increase batch size to 32","2174d935":"default_lr = 1e-4 \nadp_optimizer = optimizers.RMSprop(lr=default_lr, rho=0.9, epsilon=1e-08, decay=0.0)","3054aaae":"model.compile(optimizer=adp_optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\", get_f1])","dab418d1":"# Define a checkpoint callback for method2:\ncheckpoint_name = 'Weights-m2-{epoch:03d}--{val_loss:.5f}.hdf5'\ncheckpoint2 = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\nes = EarlyStopping(monitor='val_loss', patience=5)\ncallbacks_list2 = [checkpoint2]","a9f29b0d":"history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=30, validation_data=val_generator, validation_steps=20, callbacks=callbacks_list2)","fca902b1":"import matplotlib.pyplot as plt\n\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nepochs = range(1, len(acc)+1)\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nf1 = history.history['get_f1']\nval_f1 = history.history['val_get_f1']\n\nplt.plot(epochs, acc, 'bo', label='Train Acc')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc')\nplt.title('Accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, f1, 'bo', label='Train F1')\nplt.plot(epochs, val_f1, 'b', label='Validation F1')\nplt.title('F1 score')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Train Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Loss')\nplt.legend()\nplt.figure()\n\nplt.show()","5cb171ff":"test_data = []\ntest_labels = []\nfor (test_img, label) in zip(x_test, y_test):\n    test_data.append(test_img.astype(np.float32)\/255)\n    test_labels.append(label)\n\ntest_data = np.array(test_data)\ntest_labels = np.array(test_labels)\n\nprint(\"Total number of test examples: \", test_data.shape)\nprint(\"Total number of labels:\", test_labels.shape)","f23a058f":"def predict(model, test_data):\n    pred_prob = model.predict(test_data, batch_size=data_batch_size)\n    pred_res = np.asarray([1 if x > 0.5 else 0 for x in [x[0] for x in pred_prob]]) \n    return (pred_res, [x[0] for x in pred_prob])","e45643eb":"# Load best weight of model\nfrom pathlib import Path\nw_fnl = [str(fn) for fn in Path('.\/').glob('Weights-m2-*.hdf5')]\nw_fnl.sort(reverse=True)\nwights_file = w_fnl[0] # choose the best checkpoint \nmodel.load_weights(wights_file) # load it\nmodel.compile(optimizer=adp_optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\", get_f1])","c1cc51bc":"(y_pred, y_pred_prob) = predict(model, test_data)\nget_pred_score(test_labels, y_pred)","f027f9d6":"pred_result = pd.DataFrame({'imgPath': test_fns, 'label': test_labels, 'pred': y_pred, 'pred_prob': y_pred_prob})\npred_result['fn'] = pred_result.imgPath.apply(lambda ip: ip.split('\/')[-1])\npred_result.head()","c6c3115c":"false_result = pred_result[pred_result.pred != pred_result.label]\ntrue_0_fns = pred_result[(pred_result.pred == pred_result.label) & (pred_result.label == 0)].fn.values\ntrue_1_fns = pred_result[(pred_result.pred == pred_result.label) & (pred_result.label == 1)].fn.values\nfalse_0_fns = false_result[false_result.label == 0].fn.values\nfalse_1_fns = false_result[false_result.label == 1].fn.values\nprint(true_0_fns.shape, true_1_fns.shape, false_0_fns.shape, false_1_fns.shape)","41a68ce0":"# image_path = os.path.join(test_0_dir, false_0_fns[0])\nfn = false_0_fns[1]\nfolder = test_0_dir\nimage_path = os.path.join(folder, fn)\nprint(image_path)\n# img = image.load_img(image_path, target_size=(224, 224))\nimg = image_resize(image_path)\nimg = img.astype(np.float32)\/255\nimg_x = np.expand_dims(img, axis=0)\nimg_x.shape","f2f7e02e":"from keras.models import Model\nimport tensorflow as tf\nimport keras.backend as K\n\nclass GradCAM:\n    def __init__(self, model, classIdx=0, layerName=None):\n        # store the model, the class index used to measure the class\n        # activation map, and the layer to be used when visualizing\n        # the class activation map\n        self.model = model\n        self.classIdx = classIdx\n        self.layerName = layerName\n        self.sess = tf.compat.v1.Session()\n        # if the layer name is None, attempt to automatically find\n        # the target output layer\n        if self.layerName is None:\n            self.layerName = self.find_target_layer()\n\n    def find_target_layer(self):\n        # \u5c0b\u627e\u6700\u5f8c\u4e00\u5c64Conv layer\n        # attempt to find the final convolutional layer in the network\n        # by looping over the layers of the network in reverse order\n        for layer in reversed(self.model.layers):\n            # check to see if the layer has a 4D output\n            if len(layer.output_shape) == 4:\n                # model\u4e2d\u51fa\u73fe\u7684\u6700\u5f8c\u4e00\u500b\u8f38\u51fa\u7dad\u5ea6\u70ba4\u7684\u5c64\u5373\u5c0b\u627e\u76ee\u6a19\n                return layer.name\n        # otherwise, we could not find a 4D layer so the GradCAM\n        # algorithm cannot be applied\n        raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n\n    def compute_heatmap(self, image, eps=1e-8):\n        # construct our gradient model by supplying (1) the inputs\n        # to our pre-trained model, (2) the output of the (presumably)\n        # final 4D layer in the network, and (3) the output of the\n        # softmax activations from the model\n#         gradModel = Model(\n#             inputs=[self.model.inputs],\n#             outputs=[self.model.get_layer(self.layerName).output,\n#                 self.model.output])\n            # record operations for automatic differentiation\n        \n        pred = self.model.predict(image)\n        predictions = self.model.output\n        # model \u5c0d\u8f38\u5165\u4f5c\u5b8c\u9810\u6e2c\u5f8c\u53d6\u51fa\u8981\u7e6a\u5236heatmap\u7684\u985e\u5225\uff0c\u56e0\u6211\u5011\u7684model\u70ba\u4e8c\u5143\u5206\u985e\uff0c\u6545classIdx\u56fa\u5b9a\u70ba0\n        loss = predictions[:, self.classIdx]\n        convOutputs = self.model.get_layer(self.layerName).output\n\n        # \u7528 gradients \u51fd\u5f0f\u8a08\u7b97\u68af\u5ea6\u503c\u4f5c\u70ba\u5f8c\u9762\u756bheatmap\u7684\u6b0a\u91cd\n        # use automatic differentiation to compute the gradients\n        grads = K.gradients(loss, convOutputs)[0]\n\n        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n        iterate = K.function([self.model.input],[pooled_grads, convOutputs[0]])\n        (pooled_grads_value, conv_layer_output_value) = iterate([image])\n        for i in range(512):\n            # \u5c0dconv layer\u7684\u8f38\u51fa\u4e58\u4e0a\u6b0a\u91cd\u4ee5\u4f5c\u70ba\u7e6a\u5236heatmap\u7684raw data\n            conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n        \n        heatmap = np.mean(conv_layer_output_value, axis=-1)\n        # grab the spatial dimensions of the input image and resize\n        # the output class activation map to match the input image\n        # dimensions\n        (w, h) = (image.shape[2], image.shape[1])\n        # heatmap = cv2.resize(cam.numpy(), (w, h))\n        heatmap = cv2.resize(heatmap, (w, h))\n        # \u6b63\u898f\u5316 heatmap\u7684raw data\u4f7f\u503c\u843d\u57280~255\u4e4b\u9593(image data\u7684\u5408\u7406\u7bc4\u570d)\n        # normalize the heatmap such that all values lie in the range\n        # [0, 1], scale the resulting values to the range [0, 255],\n        # and then convert to an unsigned 8-bit integer\n        numer = heatmap - np.min(heatmap)\n        denom = (heatmap.max() - heatmap.min()) + eps\n        heatmap = numer \/ denom\n        heatmap = (heatmap * 255).astype(\"uint8\")\n        # return the resulting heatmap to the calling function\n        return (heatmap, pred)\n\n    def overlay_heatmap(self, heatmap, image, alpha=0.5,\n        colormap=cv2.COLORMAP_VIRIDIS):\n        # apply the supplied color map to the heatmap and then\n        # overlay the heatmap on the input image\n        heatmap = cv2.applyColorMap(heatmap, colormap)\n        output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n        # return a 2-tuple of the color mapped heatmap and the output,\n        # overlaid image\n        return (heatmap, output)","1c25e1de":"cam = GradCAM(model)\n(heatmap, _) = cam.compute_heatmap(img_x)","b8758407":"img = image_resize(image_path)\nprint(heatmap.shape, type(heatmap))\nprint(img[:, :, 0].shape, type(img[:, :, 0]))","b947fc8c":"(heatmap, output) = cam.overlay_heatmap(np.dstack([heatmap, heatmap, heatmap]), img, alpha=0.2)","459aaa8f":"plt.imshow(heatmap) # show heatmap","cb9f6601":"plt.imshow(output) # \u758a\u5408\u5716","a3ac0003":"# Create global CAM class\ncam = GradCAM(model)","c7ea180e":"def get_heatmap_with_pic(base_dir, fn, cam_model=cam):\n#     print(fn)\n    image_path = os.path.join(base_dir, fn)\n    title1 = fn\n    img = image_resize(image_path)\n    img_nz = img.astype(np.float32)\/255\n    img_x = np.expand_dims(img_nz, axis=0)\n    (heatmap, pred_prob) = cam_model.compute_heatmap(img_x)\n    (heatmap, output) = cam_model.overlay_heatmap(np.dstack([heatmap, heatmap, heatmap]), img, alpha=0.2)\n    title2 = 'pred_prob: {}'.format(pred_prob)\n    title3 = 'combined'\n    return (img, title1, heatmap, title2, output, title3)","d78023f7":"# \u4f7f\u7528 subplots \u5c07\u539f\u59cb\u5716\u3001heatmap\u53ca\u758a\u5408\u5716\u4e26\u6392\u5448\u73fe\ndef draw_heatmap_on_plt(base_dir, fn_list, cat='tp', cam_model=cam):\n    # Draw heatmap for true cases\n    pics_per_row = 2\n    cols = 3 * pics_per_row\n    rows = int(len(fn_list) \/ pics_per_row)\n    fig, ax = plt.subplots(rows, cols, figsize=(cols*5,rows*5))\n    for i, axi in enumerate(ax.flat):\n        title = ''\n        show_img = None\n        idx = i % 3\n        if idx == 0:\n            (img, title1, heatmap, title2, superimposed_img, title3) = get_heatmap_with_pic(base_dir, fn_list[int(i \/ 3)])\n            show_img = img\n            title = '{} ({})'.format(title1, cat)\n        elif idx == 1:\n            show_img = heatmap\n            title = title2\n        else:\n            show_img = superimposed_img\n            title = title3\n        axi.imshow(show_img, cmap='bone')\n        axi.set_title(title)\n        axi.set(xticks=[], yticks=[])","47cdfe05":"import datetime\ndisp_num = 15 #\u6bcf\u500b\u985e\u5225\u53d615\u5f35\u539f\u5716\npic_sec = 20 # \u756b\u4e00\u5f35 heatmap \u7d04\u898120\u79d2(\u4ee5kaggle\u7684notebook\u898f\u683c)","a4984715":"disp_cnt = min(disp_num, len(true_0_fns))\nprint('Will take around {} secs from {}'.format(disp_cnt*pic_sec, str(datetime.datetime.now())))\ndraw_heatmap_on_plt(test_0_dir, true_0_fns[:disp_cnt], 'tn')","a17e885d":"disp_cnt = min(disp_num, len(true_1_fns))\nprint('Will take around {} secs from {}'.format(disp_cnt*pic_sec, str(datetime.datetime.now())))\ndraw_heatmap_on_plt(test_1_dir, true_1_fns[:disp_cnt], 'tp')","d6eb55ea":"disp_cnt = min(disp_num, len(false_0_fns))\nprint('Will take around {} secs from {}'.format(disp_cnt*pic_sec, str(datetime.datetime.now())))\ndraw_heatmap_on_plt(test_0_dir, false_0_fns[:disp_cnt], 'fn')","32bcbedf":"disp_cnt = min(disp_num, len(false_1_fns))\nprint('Will take around {} secs from {}'.format(disp_cnt*pic_sec, str(datetime.datetime.now())))\ndraw_heatmap_on_plt(test_1_dir, false_1_fns[:disp_cnt], 'fp')","f3c8fcc3":"### \u8cc7\u6599\u524d\u8655\u7406","df58159b":"Preprocess images with the following operations and create dataset\n* resize to 224*224\n* Only capture (y1, y2, x1, x2) = (top,top+200, left,left+200) (left, top) = (15, 40) to focus on lung part only","2e3f8c80":"Make datasets for train, val and test ","2cd7cde6":"1. \u7531TN\u53caTP\u7684\u5716\u5f62\u53ef\u4ee5\u89c0\u5bdf\u5230\u7406\u60f3\u7684 heatmap \u4eae\u8655\u61c9\u7531\u5716\u7247\u4e2d\u9593\u5448\u5713\u5f62\u5c55\u958b\uff0c\u6216\u662f\u5728\u4e2d\u9593\u5f62\u6210\u67f1\u72c0\uff0c\u5f62\u6210\u4e2d\u9593\u4eae\u5169\u908a\u6697\u5716\u5f62\u3002\n1. \u4e8b\u5be6\u4e0a\u6a21\u578b\u5c0d\u90e8\u4efd TP case \u7684\u6c7a\u7b56\u5340\u57df\u4e5f\u4e0d\u662f\u5f88\u7406\u60f3(\u96d6\u4eae\u8655\u7531\u4e2d\u9593\u5c55\u958b\u4f46\u4ea6\u6709\u6db5\u84cb\u4e0d\u8db3\u60c5\u6cc1)\uff0c\u4f46\u56e0Test set \u4e2d positive\u6bd4\u4f8b\u4e5f\u8f03\u9ad8\uff0c\u4e5f\u4e0d\u6392\u9664\u6709\u6b6a\u6253\u6b63\u8457\u7684\u73fe\u8c61\u3002","64f59c57":"\u8a13\u7df4\u671f\u9593\u53ef\u770b\u5230\u6700\u4f73\u7684 val_loss \u964d\u70ba 0\uff0c\u6216\u8a31\u662f\u6709overfit\u7684\u73fe\u8c61\u767c\u751f... \u4e0d\u904e\u767c\u751f\u5728 validation set \u4e0a\u662f\u6bd4\u8f03\u5c11\u898b\uff1b\u8b80\u8005\u6709\u8208\u8da3\u53ef\u4ee5\u8abf\u5927 validation set \u7684\u7e3d\u91cf\u8dd1\u8dd1\u770b\u3002","5cd33b2f":"1. \u7531 FN \u53ca FP \u5716\u5f62\u7684 heatmap \u53ef\u4ee5\u770b\u51fa\u591a\u534a\u662f\u4eae\u8655\u504f\u5728\u89d2\u843d\u6216\u662f\u660e\u986f\u907a\u6f0f\u80ba\u90e8\u67d0\u5340\u57df\uff0c\u7576\u4f7f\u7528\u8005\u770b\u5230\u9019\u6a23\u7684heatmap\u6216\u8a31\u61c9\u8a72\u8003\u616e\u662f\u5426\u63a5\u53d7\u6a21\u578b\u5224\u65b7\u7d50\u679c\n1. \u6709\u4e9b heatmap \u6db5\u84cb\u826f\u597d\u4f46\u4ecd\u7136\u8aa4\u5224\u7684 case \u6216\u8a31\u61c9\u518d\u6aa2\u8996\u539f\u5716\u6a94\u770b\u662f\u5426\u6709\u9700\u8981\u8abf\u6574label(\u5224\u5b9a\u7d50\u679c)","ab429a07":"### Grad-CAM\n\n\u6b64 Grad-CAM \u7684\u5be6\u4f5c\u662f\u53c3\u8003 [\u9019\u7bc7\u6587\u7ae0](https:\/\/www.pyimagesearch.com\/2020\/03\/09\/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning\/) \u7684\uff1b\u4e3b\u8981\u5dee\u5225\u5728\u65bc\u8a72\u6587\u4ee5 TF 2.0 \u70ba backend\uff0c\u800c\u6211\u5011\u4f7f\u7528 TF 1.x\u3002\n\n\u770b\u500c\u5011\u82e5\u662f Google `Deeplearning CAM` \u6703\u770b\u5230\u6709\u8a31\u591a\u4e0d\u540c\u7684 CAM \u5be6\u4f5c\u65b9\u5f0f\uff0c\u4f46\u539f\u7406\u5927\u540c\u5c0f\u7570\uff0c`\u90fd\u662f\u8a08\u7b97\u67d0\u500b\u985e\u5225(\u901a\u5e38\u662f\u6a5f\u7387\u6700\u5927\u7684\u985e\u5225)\u7684\u6700\u7d42 output \u95dc\u65bc model \u4e2d\u6700\u5f8c\u4e00\u500b conv2d \u5c64\u4e4b\u68af\u5ea6\u503c\u4f5c\u70ba\u5716\u50cf\u5404\u90e8\u4f4d\u7684\u6b0a\u91cd` \u4f86\u7e6a\u5236heatmap\uff1b\u5dee\u5225\u5728\u65bc\u5982\u4f55\u5c0dheatmap\u4f5cnormalize\u4f7f\u5176\u66f4\u80fd\u7a81\u986f\u51fa\u91cd\u8981\u7684\u6c7a\u7b56\u90e8\u4f4d\u3002","1d3fc0b9":"==== Prepare test data for prediction ===","eb68ab0a":"Apply ResNet50 module of keras for training & prediction\n\n\u5728\u6b64\u6211\u5011 `\u4f7f\u7528 ResNet50 \u5728 Keras \u7684\u5be6\u4f5c (\u4f7f\u7528 TensorFlow 1.x \u7248\u672c\u70ba backend) \u4f86\u8a13\u7df4`\uff1b\u4e8b\u5be6\u4e0a ResNet50 \u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\u7684\u6578\u503c\u4e26\u4e0d\u662f\u5f88\u6b63\u5e38\uff0c\u4f46\u8dd1\u4f86\u7d50\u679c\u4e0d\u6bd4 VGG16 \u5dee\uff0c\u4e14\u672c\u7bc7\u91cd\u9ede\u5728 CAM\uff0c\u770b\u500c\u82e5\u6709\u8208\u8da3\u53ef\u4ee5 folk \u6b64 notebook \u904e\u53bb\u6539\u6210VGG16\u8a66\u8a66\u770b\u3002","ee06f9e6":"\u6211\u5011\u4f7f\u7528 VIRIDIS \u8272\u968e\u4f86\u7e6a\u5236 Heatmap\uff0c\u8d8a\u4eae\u8655(\u91d1\u9ec3\u8272)\u8868\u793a\u6b0a\u91cd\u503c\u8d8a\u9ad8\n\n![CV2\u4e2d\u7684\u5e38\u7528\u8272\u968e](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2020\/03\/keras_gradcam_colormap.png)\n\u5716\u7247\u51fa\u8655\uff1ahttps:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2020\/03\/keras_gradcam_colormap.png","25c9edcf":"\u7531\u758a\u5408\u5716\u53ef\u4ee5\u770b\u51fa\u91d1\u9ec3\u8272\u4e26\u672a\u5f88\u597d\u7684\u6db5\u84cb\u80ba\u90e8\u5340\u57df\uff0c\u5f88\u6709\u53ef\u80fd\u56e0\u6c92\u6709\u6db5\u84cb\u5230\u75c5\u7076\u800c\u8aa4\u5224","c1b0b289":"\u5c07 heatmap \u8207\u539f\u59cb\u5716\u578b\u758a\u5408\uff0calpha \u503c\u4f7f\u75280.2 (\u8d8a\u9ad8\u7684 alpha \u503c\u6703\u8b93\u539f\u5716\u8d8a\u6e05\u6670\uff0cheatmap\u8d8a\u6a21\u7cca)","b6951b50":"### Add dense layer on top of conv_base \n\n\u8207 [Pneumonia Detection by VGG16](https:\/\/www.kaggle.com\/whitelee\/pneumonia-detection-by-vgg16) \u7684\u5be6\u4f5c\u4e0d\u540c\uff0c\u5728\u6b64\u6211\u5011\u4f7f\u7528 `\u51fd\u6578\u5f0f\u7684\u5beb\u6cd5\u4f86\u5efa\u7acb Model` \uff0c\u800c\u975e\u5c07 base_model \u9472\u5728 Sequential \u4e2d\uff1b\u5982\u6b64\u4e00\u4f86\u5f8c\u7e8c\u5728 CAM \u5be6\u4f5c\u4e2d\u8f03\u5bb9\u6613\u627e\u5230 last_conv_layer \u3002","d6d0098a":"### \u7d50\u8a9e\n\n\u7531\u4ee5\u4e0a CASE \u7684 heatmap \u89c0\u5bdf\uff0c\u5927\u591a\u6578\u7684CASE\u7684\u4eae\u5340\u90fd\u9084\u5408\u7406(\u81ea\u8a8d\u70ba)\uff0c\u7576\u7136\u6db5\u84cb\u5ea6\u662f\u5426\u8db3\u5920\u53ef\u80fd\u8981\u52a0\u5165Domain\u5c08\u5bb6\u7684\u5224\u65b7\u624d\u6bd4\u8f03\u6e96\u78ba\u3002\u4f46\u6211\u5011\u76f8\u4fe1\u5728\u6574\u500b\u8cc7\u6599\u5206\u6790\u904e\u7a0b\u7684\u5404\u500b\u90e8\u4efd\u90fd\u53ef\u7531 CAM \u7372\u5f97\u5e6b\u52a9\uff1bHeatmap \u63d0\u4f9b\u4e86\u53ef\u8996\u5316\u7684\u6c7a\u7b56\u8cc7\u8a0a\u8b93\u958b\u767c\u4eba\u54e1\u53ef\u4ee5\u5c0d\u6a21\u578b\u7684\u6548\u80fd\u6709\u53e6\u4e00\u7a2e\u9762\u5411\u7684\u89c0\u5bdf(\u4f8b\u5982\u6a21\u578b\u662f\u771f\u7684\u5224\u65b7\u51fa\u4f86\u7684\u9084\u662f\u731c\u5c0d\u7684)\uff0c\u4f7f\u7528\u8005\u4e5f\u53ef\u7531\u4eae\u5340\u77e5\u9053\u6a21\u578b\u5224\u65b7\u4f9d\u64da\u662f\u5426\u5408\u7406\uff0c\u751a\u81f3\u53ef\u4ee5\u4f5c\u70ba\u56de\u982d\u6aa2\u8996\u8cc7\u6599(input)\u54c1\u8cea\u7684\u53c3\u8003\u3002\n\n\u4ee5\u4e0a\u662f\u5c0f\u5f1f\u9019\u9663\u5b50\u7684\u5b78\u7fd2\u5206\u4eab\uff0c\u5404\u4f4d\u770b\u500c\u82e5\u6709\u4efb\u4f55\u60f3\u6cd5\u6216\u5efa\u8b70\u4e5f\u6b61\u8fce\u63d0\u51fa\u8a0e\u8ad6\uff5e","9971ba80":"### List heatmaps of TP \/ TN \/ FP \/ FN cases \n\n\u5404 case \u53d6\u51fa 15 \u5f35\u539f\u59cb\u5716\u4f86\u7e6a\u5236 heatmap\uff0c\u89c0\u5bdf\u5224\u65b7\u932f\u8aa4\u4e4b case \u662f\u5426\u662f\u56e0 model \u7528\u4e86\u932f\u8aa4\u7684\u90e8\u4efd\u4f5c\u6c7a\u7b56\u3002","488a772a":"======== Show pictures after being resized=====","3c5871e5":"### Apply CAM to one FN case","dbfecb9e":"\u5728 [Pneumonia Detection by VGG16](https:\/\/www.kaggle.com\/whitelee\/pneumonia-detection-by-vgg16) \u4e2d\u6211\u5011\u900f\u904e Transfer Learning \u7684\u4f5c\u6cd5 fine-tune VGG16 model \u53d6\u5f97\u4e86\u4e0d\u932f\u7684\u9810\u6e2c\u7d50\u679c (F1 Score \u7d04 0.93 ~ 0.94)\uff0c\u5f8c\u7e8c\u4f5c\u4e86\u8a31\u591a\u5617\u8a66 (\u5c0d\u5f71\u50cf\u4f5cBlur, Shapern \u6216 Morphology \u8655\u7406\u3001\u63db model \u7b49) \u4e0d\u904e\u90fd\u7121\u6cd5\u53d6\u5f97\u7a81\u7834\u3002\n\n\u5728\u5617\u8a66\u904e\u7a0b\u4e2d\u63a5\u89f8\u5230 `CAM (Class Activation Mapping)` \u9019\u500b\u5be6\u4f5c\uff0cCAM \u53ef\u4ee5\u7528\u4f86 `\u5e6b\u52a9\u6211\u5011\u7406\u89e3\u5716\u7247\/\u5f71\u50cf\u4e2d\u90a3\u500b\u90e8\u4efd\u662f\u8b93 CNN \u4f5c\u51fa\u6700\u7d42\u6c7a\u7b56\u7684\u4f9d\u64da`\u3002\nCAM \u6700\u65e9\u7531 Bolei Zhou \u7b49\u4eba\u65bc2016\u5e74\u5728 [Learning Deep Features for Discriminative Localization](https:\/\/arxiv.org\/abs\/1512.04150) \u8ad6\u6587\u4e2d\u63d0\u51fa\uff0c2017\u5e74Selvaraju et al.\u7b49\u4eba\u5247\u63d0\u51fa\u6539\u826f\u7248\u672c [Grad-CAM](https:\/\/arxiv.org\/abs\/1610.02391) \u4ee5\u514b\u670d CAM \u5728 CNN \u7d50\u69cb\u4e0a\u7684\u9650\u5236\u3002\n\nGrad-CAM \u7684\u6982\u5ff5\u662f\u900f\u904e\u53cd\u5411\u50b3\u64ad (Back Propagation) \u63a8\u7b97\u5404\u985e\u5225\u5c0d\u5716\u7247\u4e2d\u5404\u90e8\u4f4d\u7684\u68af\u5ea6\u503c(Gradient) \u4f5c\u70ba\u6b0a\u91cd\uff0c\u4ee5\u7e6a\u88fd\u985e\u5225\u7684heatmap\u3002\n\u60f3\u5c0d Grad-CAM \u6709\u591a\u4e00\u9ede\u4e86\u89e3\u7684\u53ef\u53c3\u8003\u9019\u7bc7 [\u6587\u7ae0](https:\/\/medium.com\/%E6%89%8B%E5%AF%AB%E7%AD%86%E8%A8%98\/grad-cam-introduction-d0e48eb64adb)\u3002\n\n\u6211\u60f3 CAM \u53ef\u4ee5\u63d0\u4f9b\u5169\u500b\u5f88\u6709\u50f9\u503c\u7684\u7528\u9014:\n1. \u85c9 CAM \u4f5c\u70ba debug \u4f9d\u64da\uff0c\u627e\u51fa\u5c0d\u6a21\u578b\u6709\u5e6b\u52a9\u4e4b\u5716\u7247\u8655\u7406\u65b9\u5f0f (\u56e0\u70ba\u770b\u5b8cheatmap\u7d50\u679c\u5f8c\u4f9d\u7136\u6c92\u4ec0\u9ebc\u65b9\u5411\uff0c\u76ee\u524d\u6c92\u6709\u5617\u8a66\uff1b\u82e5\u770b\u5b8c\u672c\u6587\u6709\u4ec0\u9ebcidea\u6b61\u8fce\u5206\u4eab)\u3002\n2. \u5728\u5be6\u969b\u61c9\u7528\u4e0a\uff0c\u4ea6\u63d0\u4f9b heatmap \u544a\u77e5\u4f7f\u7528\u8005 model \u6c7a\u7b56\u7684\u90e8\u4f4d\uff0c\u4ee5\u5354\u52a9\u4f7f\u7528\u8005\u6c7a\u5b9a\u662f\u5426\u63a5\u53d7 model \u7684\u5224\u65b7\u3002\n\n\u4ee5\u4e0b\u5c55\u793a Fine-tune ResNet \u6a21\u578b\uff0c\u4e26\u5957\u7528 CAM \u7522\u751f TP \/ FP \/ TN \/ FN \u4e4b heatmap \u89c0\u5bdf\u6a21\u578b\u7684\u6c7a\u7b56\u4f9d\u64da\u3002","5dcde3b1":"###### Move images from train to val list to increase validation set\n\n\u6211\u5011\u89c0\u5bdf\u5230 `\u8abf\u6574 validation set \u4e2d 0 (mv_cnt_0) \u8207 1 (mv_cnt_1) \u7684\u6bd4\u4f8b\u5c0d\u6a21\u578b\u7684\u9810\u6e2c\u80fd\u529b\u6703\u7522\u751f\u5f71\u97ff` \uff0c\u5927\u81f4\u4e0a\u4f86\u8aaa (mv_cnt_0, mv_cnt_1) \u70ba (300, 300) \u6642 F1 score \u8868\u73fe\u8f03\u4f73\uff0c\u8b8a\u66f4\u70ba (200, 400) \u6642\u6703\u6709\u66f4\u597d\u7684 Recall\uff1b\u9019\u4e5f\u8aaa\u660e\u4e86 Validation set \u53ca Validation \u7b56\u7565\u8a2d\u8a08\u7684\u91cd\u8981\u6027\u3002","bd4fad8b":"### Extract false case for advanced analysis\n\n\u8a13\u7df4\u5b8c Model \u4ee5\u5f8c\uff0c\u53d6\u51fa TN, TP, FN, FP \u7684 case \u4f86\u5957\u7528 CAM \uff0c\u89c0\u5bdf\u5176 heatmap \u72c0\u6cc1\u3002","81552917":"\u82e5\u8a2d\u5b9a trainable \u70ba False\uff0c \u5247 Fine-tune \u53c3\u6578\u70ba 0 \u500b\uff0c\u9019\u662f\u6bd4\u8f03\u5947\u602a\u7684\u5730\u65b9\uff0c\u56e0\u6b64\u6211\u5011\u4e0d\u66f4\u6539trainable\u8a2d\u5b9a\uff0c\u7dad\u6301 492 \u500b\u53c3\u6578\uff1b\u4f46 492 \u500b\u53c3\u6578\u4f7f\u5f97\u6bcf\u500b epoch \u7684\u8a13\u7df4\u6642\u9593\u9577\u9054 42 \u79d2 (\u76f8\u8f03 VGG16 \u6bcf\u500b epoch \u70ba19\u79d2)\uff1b\u82e5\u4f7f\u7528 TF 2.0 \u70ba backend \u5247\u5728\u8a2d\u5b9a trainable \u5f8c Fine-tune \u53c3\u6578\u8b8a\u6210 4 \u500b\uff0c\u8a13\u7df4\u6642\u9593\u7e2e\u6e1b\uff0c\u4f46\u9810\u6e2c\u80fd\u529b\u4e0b\u964d\u8a31\u591a\u3002","b244f2ab":"\u8a2d\u5b9a call back \u4fdd\u7559\u6700\u4f73 val_loss \u4e4b\u6b0a\u91cd\u503c","365814c9":"\u63a5\u4e0b\u4f86\u5148\u5617\u8a66\u5c0d\u55ae\u5f35\u8f38\u5165\u7e6a\u5236heatmap\uff0c\u89c0\u5bdf\u7e6a\u5236\u53ca\u8207\u539f\u5716\u4e4b\u758a\u5408\u6548\u679c"}}