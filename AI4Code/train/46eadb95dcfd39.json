{"cell_type":{"1e46f18c":"code","eb7a8b49":"code","839a587c":"code","a58feb5d":"code","9170ee6b":"code","fa7fe702":"code","7d89b69a":"code","e60b22cb":"code","e8d67ad8":"code","61c8ba31":"code","91cac5f7":"code","a0d13965":"code","00021b10":"code","6ce4c2ff":"code","04e5bb24":"code","81846b6f":"code","911297fc":"code","d54c07f2":"code","832a243b":"code","7f70ebb0":"code","bec60504":"code","d90b7d5d":"code","4bfb7c20":"code","1cc013c7":"code","991b88d8":"code","2497fcfe":"code","93f0fc28":"code","68eace9d":"code","a6f6779e":"code","757c29f3":"code","a61cd188":"code","b9ac7eca":"code","d838f9d7":"code","8bca6499":"code","3d865ca9":"code","3c99890c":"code","b7a0be31":"code","e726d5ce":"code","9ddfbdc0":"code","1dfc0cf4":"code","df9f6d91":"code","8f7ff41e":"code","fa0afb28":"code","23e05072":"code","c32f8629":"code","fc4970ab":"code","718fe37e":"code","acb3d654":"code","f0cd3fb5":"code","1805e960":"code","3199ae88":"code","c795424d":"code","68efcb5c":"code","440f82bf":"code","54d2c458":"code","2662ed32":"code","00a85faa":"code","ca60bbaf":"code","1b141666":"code","3165b2c1":"code","2d73dec5":"code","c4128e65":"code","3ce379bc":"code","0a53f7ef":"code","364e94a1":"code","554c4277":"code","4866bad2":"code","59793806":"code","8e67e06c":"code","29d088dc":"code","72dc861f":"code","196d9e3f":"code","f9fd07af":"code","7ac25f6d":"code","291c3331":"code","16a3ccc6":"code","271cc85b":"code","2232461f":"code","1f977c10":"code","039d4e1c":"code","22d3abd7":"code","009a10c6":"code","1716b087":"code","9fa7805c":"code","4bac7cde":"code","7c97e5c9":"code","b4da4603":"code","d5965c3b":"code","40d70903":"code","9bb406d2":"code","bd82f43a":"code","95d317c9":"code","41e3d050":"code","f87f9184":"code","f2d8e693":"code","4627f345":"code","91423681":"code","409067b3":"code","89dd51a9":"code","d201c20b":"code","2cac5420":"code","926a8905":"code","e1101eaf":"code","2a7afb86":"code","502d2a69":"code","357934dd":"code","5a2dba57":"markdown","52e62ac3":"markdown","3315d21d":"markdown","83a9f250":"markdown","0481a519":"markdown","6771f53f":"markdown","5a11e714":"markdown","f848c2f7":"markdown","d4dabfa2":"markdown","c815e186":"markdown","898e38c5":"markdown","fde7785c":"markdown","0ed52af3":"markdown","9efb6005":"markdown","8b716afb":"markdown","bf6d45de":"markdown","c264bfb3":"markdown","136043f6":"markdown","af776d74":"markdown","17cedb17":"markdown","ada1fef7":"markdown"},"source":{"1e46f18c":"import pandas as pd # python dataframes\nimport numpy as np # python numerics\nimport matplotlib.pyplot as plt # python plotting\nimport seaborn as sns\n\n# Keras imports\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, GRU\nfrom keras.layers.embeddings import Embedding\n\n# LightGBM imports\nimport lightgbm as lgb\n\nfrom kaggle.competitions import twosigmanews # Needed to obtain training\/test data\n\nfrom tqdm import tqdm\nimport gc","eb7a8b49":"# Change DEBUG to False when you're ready, Corey.\nDEBUG = False\n\n# Change YEARMIN to change the cutoff point for your data\n# All data must be greater than YEARMIN\nYEARMIN = 2011","839a587c":"#random seeds for stochastic parts of neural network \nnp.random.seed(100)\nfrom tensorflow import set_random_seed\nset_random_seed(150)","a58feb5d":"# Load in market and news data\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()","9170ee6b":"# Require all data to be more recent than YEARMIN\nmarket_train = market_train.loc[market_train['time'].dt.year > YEARMIN]","fa7fe702":"market_train.columns","7d89b69a":"# # Require all TARGETS be in range (-1, 1)\n# market_train['returnsOpenNextMktres10'] = market_train['returnsOpenNextMktres10'].clip(-1,1)","e60b22cb":"# Are there any columns that have NA's?\n# Recall Neural Networks requires all values imputed\nprint('MARKET TRAIN:')\nfor col in market_train.columns:\n    print(col+' has '+str(market_train[col].isna().sum())+' NAs')\n    \nprint('\\n\\nNEWS TRAIN:')\n\n# Are there any columns that have NA's?\n# Recall Neural Networks requires all values imputed\nfor col in news_train.columns:\n    print(col+' has '+str(news_train[col].isna().sum())+' NAs')","e8d67ad8":"# Not using news_train, yet\ndel news_train\ngc.collect()","61c8ba31":"# If DEBUG, then don't read in all of the data.\nif DEBUG:\n    market_train = market_train.sample(50000, random_state=4)","91cac5f7":"# Attempt to impute by group by's median\nmarket_train['returnsClosePrevMktres1'] = market_train.groupby(['assetCode'])['returnsClosePrevMktres1'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsOpenPrevMktres1'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres1'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsClosePrevMktres10'] = market_train.groupby(['assetCode'])['returnsClosePrevMktres10'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsOpenPrevMktres10'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres10'].transform(lambda x: x.fillna(x.median()))\n\n# If the assetCode has no non-null values, then impute with column median\nmarket_train = market_train.fillna(market_train.median())","a0d13965":"market_train = market_train.sort_values(['assetCode','time']) # Sort it by time for use in LSTM later\nmarket_train.head()","00021b10":"market_train.columns","6ce4c2ff":"# from multiprocessing import Process, Manager\n\n# def dothing(L, assetCode):  # the managed list `L` passed explicitly.\n#     L.append(series_to_supervised(market_train.loc[market_train['assetCode']==assetCode, 'returnsOpenNextMktres10'].values.reshape(-1,1).flatten().tolist(),\n#                          market_train.loc[market_train['assetCode']==assetCode, ['assetCode','time','TARGET']],\n#                          n_in=7,\n#                          n_out=1,\n#                          dropnan=True,\n#                          pad=True))\n\n# with Manager() as manager:\n#     L = manager.list()  # <-- can be shared between processes.\n#     processes = []\n#     for assetCode in tqdm(market_train['assetCode'].unique()):\n#         p = Process(target=dothing, args=(L,assetCode))  # Passing the list\n#         p.start()\n#         processes.append(p)\n#     for p in processes:\n#         p.join()","04e5bb24":"# Define parameters for LSTM input creation\nn_in = 5\nn_out = 1","81846b6f":"# Adapted from: https:\/\/machinelearningmastery.com\/multivariate-time-series-forecasting-lstms-keras\/\n\ndef series_to_supervised(data, extraCols, n_in=1, n_out=1, dropnan=True, pad=False):\n    if pad: # If you do not have enough data to construct the n_in sequence...\n        data = np.asarray([data[0].tolist()]*n_in + data.tolist()) # Pad with earliest piece of data n_in amount of times\n        \n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    # Add extra columns\n    agg = pd.concat([agg.reset_index(drop=True), extraCols.reset_index(drop=True)], axis=1)\n    return agg","911297fc":"market_train['time'] = pd.to_datetime(market_train['time'].dt.date) # Change from datetime to date for less memory and easier merge with news","d54c07f2":"# Feature Engineering\nmarket_train['close_minus_open'] = market_train['close'] - market_train['open']\nmarket_train['margin1'] = market_train['open'] \/ market_train['close']","832a243b":"# Keep last 30 of each asset\ntotal_market_obs_df = [market_train.loc[(market_train['time'].dt.year >= 2016) & (market_train['time'].dt.month >= 9)].groupby('assetCode').tail(30).drop(['universe','returnsOpenNextMktres10'], axis=1)]","7f70ebb0":"ewma = pd.Series.ewm","bec60504":"# Copied from https:\/\/www.kaggle.com\/qqgeogor\/eda-script-67\nfrom multiprocessing import Pool\n\ndef create_lag(df_code,n_lag=[5,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        for window in n_lag:\n            #rolled = df_code[col].shift(shift_size).rolling(window=window)\n            rolled = df_code[col].rolling(window=window)\n            lag_mean = rolled.mean()\n            #lag_max = rolled.max()\n            #lag_min = rolled.min()\n            #lag_std = rolled.std()\n            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n            #df_code['%s_lag_%s_max'%(col,window)] = lag_max\n            #df_code['%s_lag_%s_min'%(col,window)] = lag_min\n            #df_code['%s_lag_%s_std'%(col,window)] = lag_std\n\n    return df_code#.fillna(-1)\n\ndef generate_lag_features(df,n_lag = [5]):\n#     features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n#        'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n#        'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n#        'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n#        'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n#        'returnsOpenNextMktres10', 'universe']\n    \n    assetCodes = df['assetCode'].unique()\n    #print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    #print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","d90b7d5d":"# Creates MACD\n\ndef create_lag2(df_code,n_lag=[5,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        df_code['%s_macd'%(col)] = ewma(df_code[col], span=12).mean() - ewma(df_code[col], span=26).mean()\n\n    return df_code#.fillna(-1)\n\ndef generate_lag_features2(df,n_lag = [5]):\n    \n    assetCodes = df['assetCode'].unique()\n    #print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    #print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag2, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","4bfb7c20":"# Creates Bollinger Bands\n\ndef create_lag3(df_code,n_lag=[5,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        df_code['%s_bollingerband_high'%(col)] = df_code[col].rolling(window=7).mean() + 2 * df_code[col].rolling(window=7).std()\n        df_code['%s_bollingerband_low'%(col)] = df_code[col].rolling(window=7).mean() - 2 * df_code[col].rolling(window=7).std()\n\n    return df_code#.fillna(-1)\n\ndef generate_lag_features3(df,n_lag = [5]):\n    \n    assetCodes = df['assetCode'].unique()\n    #print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    #print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag3, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","1cc013c7":"# Create EMA\n\ndef create_lag4(df_code,n_lag=[5,],shift_size=1):\n    code = df_code['assetCode'].unique()\n    \n    for col in return_features:\n        df_code['%s_ewma'%(col)] = ewma(df_code[col], span=9).mean()\n\n    return df_code#.fillna(-1)\n\ndef generate_lag_features4(df,n_lag = [5]):\n    \n    assetCodes = df['assetCode'].unique()\n    #print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n    #print('total %s df'%len(df_codes))\n    \n    pool = Pool(4)\n    all_df = pool.map(create_lag4, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(return_features,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","991b88d8":"market_train.columns","2497fcfe":"gc.collect()","93f0fc28":"# Get Mean Features\n\nreturn_features = ['volume','returnsClosePrevRaw1',\n       'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n       'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n       'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n       'returnsOpenPrevMktres10', \n        'open', 'close', \n        'margin1', 'close_minus_open']\nn_lag = [5]\nnew_df = generate_lag_features(market_train,n_lag=n_lag)\nmarket_train = pd.merge(market_train,new_df,how='left',on=['time','assetCode'])\n\ndel new_df\ngc.collect()","68eace9d":"# Get MACD Features\n\nreturn_features = ['open', 'close']\nn_lag = [5]\nnew_df = generate_lag_features2(market_train,n_lag=n_lag)\nmarket_train = pd.merge(market_train,new_df,how='left',on=['time','assetCode'])\n\ndel new_df\ngc.collect()","a6f6779e":"# Get Bollinger Bands Features\n\nreturn_features = ['open', 'close']\nn_lag = [5]\nnew_df = generate_lag_features3(market_train,n_lag=n_lag)\nmarket_train = pd.merge(market_train,new_df,how='left',on=['time','assetCode'])\n\ndel new_df\ngc.collect()","757c29f3":"# Get EMA Features\n\nreturn_features = ['open', 'close']\nn_lag = [5]\nnew_df = generate_lag_features4(market_train,n_lag=n_lag)\nmarket_train = pd.merge(market_train,new_df,how='left',on=['time','assetCode'])\n\ndel new_df\ngc.collect()","a61cd188":"market_train.head(3)","b9ac7eca":"market_train.tail(3)","d838f9d7":"market_train.columns","8bca6499":"market_train['volume_diff'] = market_train['volume'] - market_train['volume_lag_5_mean']\nmarket_train['returnsClosePrevRaw1_diff'] = market_train['returnsClosePrevRaw1'] - market_train['returnsClosePrevRaw1_lag_5_mean']\nmarket_train['returnsOpenPrevRaw1_diff'] = market_train['returnsOpenPrevRaw1'] - market_train['returnsOpenPrevRaw1_lag_5_mean']\nmarket_train['returnsClosePrevMktres1_diff'] = market_train['returnsClosePrevMktres1'] - market_train['returnsClosePrevMktres1_lag_5_mean']\nmarket_train['returnsOpenPrevMktres1_diff'] = market_train['returnsOpenPrevMktres1'] - market_train['returnsOpenPrevMktres1_lag_5_mean']\nmarket_train['returnsClosePrevRaw10_diff'] = market_train['returnsClosePrevRaw10'] - market_train['returnsClosePrevRaw10_lag_5_mean']\nmarket_train['returnsOpenPrevRaw10_diff'] = market_train['returnsOpenPrevRaw10'] - market_train['returnsOpenPrevRaw10_lag_5_mean']\nmarket_train['returnsClosePrevMktres10_diff'] = market_train['returnsClosePrevMktres10'] - market_train['returnsClosePrevMktres10_lag_5_mean']\nmarket_train['returnsOpenPrevMktres10_diff'] = market_train['returnsOpenPrevMktres10'] - market_train['returnsOpenPrevMktres10_lag_5_mean']\nmarket_train['open_diff'] = market_train['open'] - market_train['open_lag_5_mean']\nmarket_train['close_diff'] = market_train['close'] - market_train['close_lag_5_mean']\nmarket_train['margin1_diff'] = market_train['margin1'] - market_train['margin1_lag_5_mean']\nmarket_train['close_minus_open_diff'] = market_train['close_minus_open'] - market_train['close_minus_open_lag_5_mean']\n\nmarket_train['open_macd_diff'] = market_train['open_ewma'] - market_train['open_macd']\nmarket_train['close_macd_diff'] = market_train['close_ewma'] - market_train['close_macd']\n\nmarket_train['open_bb_high_diff'] = market_train['open'] - market_train['open_bollingerband_high']\nmarket_train['open_bb_low_diff'] = market_train['open'] - market_train['open_bollingerband_low']\n\nmarket_train['close_bb_high_diff'] = market_train['close'] - market_train['close_bollingerband_high']\nmarket_train['close_bb_low_diff'] = market_train['close'] - market_train['close_bollingerband_low']\n\nmarket_train['open_ewma_diff'] = market_train['open'] - market_train['open_ewma']\nmarket_train['close_ewma_diff'] = market_train['close'] - market_train['close_ewma']","3d865ca9":"date = market_train.time\nnum_target = market_train.returnsOpenNextMktres10.astype('float32')\nbin_target = (market_train.returnsOpenNextMktres10 >= 0).astype('int8')\nuniverse = market_train.universe.astype('int8')\n\n# Drop columns that are not features\n#market_train.drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], \n#        axis=1, inplace=True)\n#gc.collect()","3c99890c":"market_train.tail()","b7a0be31":"market_train.columns","e726d5ce":"from sklearn.model_selection import train_test_split","9ddfbdc0":"#x_train, x_test, y_train, y_test = train_test_split(df, target,test_size=0.1)  # Split df\ntrain_index = market_train.index[market_train['time'].dt.year < 2016].tolist()\ntest_index = market_train.index[(market_train['time'].dt.year >= 2016) & (market_train['universe']==1)].tolist() ","1dfc0cf4":"lgbm_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1',\n       'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n       'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n       'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n       'returnsOpenPrevMktres10', 'close_minus_open', 'margin1',\n       'volume_lag_5_mean', 'returnsClosePrevRaw1_lag_5_mean',\n       'returnsOpenPrevRaw1_lag_5_mean', 'returnsClosePrevMktres1_lag_5_mean',\n       'returnsOpenPrevMktres1_lag_5_mean', 'returnsClosePrevRaw10_lag_5_mean',\n       'returnsOpenPrevRaw10_lag_5_mean',\n       'returnsClosePrevMktres10_lag_5_mean',\n       'returnsOpenPrevMktres10_lag_5_mean', 'open_lag_5_mean',\n       'close_lag_5_mean', 'margin1_lag_5_mean', 'close_minus_open_lag_5_mean',\n       'open_macd', 'close_macd', 'open_bollingerband_high',\n       'open_bollingerband_low', 'close_bollingerband_high',\n       'close_bollingerband_low', 'open_ewma', 'close_ewma', 'volume_diff',\n       'returnsClosePrevRaw1_diff', 'returnsOpenPrevRaw1_diff',\n       'returnsClosePrevMktres1_diff', 'returnsOpenPrevMktres1_diff',\n       'returnsClosePrevRaw10_diff', 'returnsOpenPrevRaw10_diff',\n       'returnsClosePrevMktres10_diff', 'returnsOpenPrevMktres10_diff',\n       'open_diff', 'close_diff', 'margin1_diff', 'close_minus_open_diff',\n       'open_macd_diff', 'close_macd_diff', 'open_bb_high_diff',\n       'open_bb_low_diff', 'close_bb_high_diff', 'close_bb_low_diff',\n       'open_ewma_diff', 'close_ewma_diff']","df9f6d91":"d_train = lgb.Dataset(market_train.iloc[train_index].drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1),\n                      label = bin_target.iloc[train_index])\n\nd_valid = lgb.Dataset(market_train.iloc[test_index].drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1),\n                      label = bin_target.iloc[test_index])","8f7ff41e":"t_valid = date.iloc[test_index]\ny_valid = num_target.iloc[test_index]\nu_valid = universe.iloc[test_index]\n\n# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\nd_valid.params = {\n    'extra_time': t_valid.factorize()[0],\n    'mktres': y_valid.values,\n    'universe': u_valid.values\n}","fa0afb28":"d_valid.params","23e05072":"def sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time'] # array\n    df_mktres = valid_data.params['mktres'] # series\n    df_universe = valid_data.params['universe']\n\n    x_t = ((preds * 2) - 1) * pd.Series(df_mktres) * pd.Series(df_universe)\n    \n    # Here we take advantage of the fact that `df_mktres` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() \/ x_t_sum.std()\n\n    return 'sigma_score', score, True","c32f8629":"params = {}\n#params['max_bin'] = 220\nparams['learning_rate'] = 0.3\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'None'         \nparams['feature_fraction'] = 0.80      # feature_fraction \n#params['num_leaves'] = 2583\n#params['min_data'] = 213         # min_data_in_leaf\n#params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['max_depth'] = 7\nparams['lambda_l1'] = 3\nparams['lambda_l2'] = 3","fc4970ab":"corey_lgbm_model = lgb.train(params=params, \n                             train_set=d_train, \n                             num_boost_round=1000, \n                             valid_sets=d_valid,  \n                             early_stopping_rounds=50, \n                             verbose_eval=10,\n                             feval=sigma_score\n                            )","718fe37e":"import seaborn as sns\nimport matplotlib.pyplot as plt","acb3d654":"feat_importance = pd.DataFrame()\nfeat_importance[\"feature\"] = market_train.drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1).columns\nfeat_importance[\"gain\"] = corey_lgbm_model.feature_importance(importance_type='gain')\nfeat_importance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(15,20))\nax = sns.barplot(y=\"feature\", x=\"gain\", data=feat_importance)","f0cd3fb5":"# Make validation predictions\nyhat_lgbm = corey_lgbm_model.predict(market_train.iloc[test_index].drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1))\nyhat_lgbm = (yhat_lgbm * 2) - 1","1805e960":"params = {}\nparams['learning_rate'] = 0.3\nparams['boosting_type'] = 'goss'\nparams['objective'] = 'binary'\nparams['metric'] = 'None'          # or 'mse'\nparams['feature_fraction'] = 0.60     # feature_fraction \n#params['min_data'] = 200         # min_data_in_leaf\nparams['verbose'] = 0\nparams['max_depth'] = 4\n#params['lambda_l1'] = 1\nparams['lambda_l2'] = 1","3199ae88":"corey_goss_model = lgb.train(params=params, \n                           train_set=d_train, \n                           num_boost_round=3000, \n                           valid_sets=d_valid,  \n                           early_stopping_rounds=25, \n                           verbose_eval=10,\n                           feval=sigma_score\n                          )","c795424d":"feat_importance = pd.DataFrame()\nfeat_importance[\"feature\"] = market_train.drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1).columns\nfeat_importance[\"gain\"] = corey_goss_model.feature_importance(importance_type='gain')\nfeat_importance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(15,20))\nax = sns.barplot(y=\"feature\", x=\"gain\", data=feat_importance)","68efcb5c":"# Make validation predictions\nyhat_goss = corey_goss_model.predict(market_train.iloc[test_index].drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1))\nyhat_goss = (yhat_goss * 2) - 1","440f82bf":"params = {}\nparams['learning_rate'] = 0.3\nparams['boosting_type'] = 'dart'\nparams['objective'] = 'binary'\nparams['metric'] = 'None'          # or 'mse'\nparams['feature_fraction'] = 0.60     # feature_fraction \n#params['min_data'] = 200         # min_data_in_leaf\nparams['verbose'] = 0\nparams['max_depth'] = 10\n#params['lambda_l1'] = 1\nparams['lambda_l2'] = 0.4\nparams['drop_rate'] = 0.2\nparams['drop_seed'] = 3230","54d2c458":"corey_dart_model = lgb.train(params=params, \n                           train_set=d_train, \n                           num_boost_round=3000, \n                           valid_sets=d_valid,  \n                           early_stopping_rounds=50, \n                           verbose_eval=10,\n                           feval=sigma_score\n                          )","2662ed32":"feat_importance = pd.DataFrame()\nfeat_importance[\"feature\"] = market_train.drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1).columns\nfeat_importance[\"gain\"] = corey_dart_model.feature_importance(importance_type='gain')\nfeat_importance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(15,20))\nax = sns.barplot(y=\"feature\", x=\"gain\", data=feat_importance)","00a85faa":"# Make validation predictions\nyhat_dart = corey_dart_model.predict(market_train.iloc[test_index].drop(['returnsOpenNextMktres10', 'time', 'universe', 'assetCode', 'time', 'assetName'], axis=1))\nyhat_dart = (yhat_dart * 2) - 1","ca60bbaf":"del d_train, d_valid, params\ngc.collect()","1b141666":"market_train.columns","3165b2c1":"LSTM_COLUMNS_TO_USE = ['time', # Time variable is necessary\n                       'assetCode', # AssetCode is necessary to perform merges\/historical analysis\n                       'volume','returnsOpenPrevMktres10','returnsClosePrevMktres10','returnsOpenPrevRaw10','returnsClosePrevRaw10',\n                       #'open_macd_diff', # 9 day EMA minus the MACD\n                       #'open_ewma_diff', # Open minus 9 day EMA\n                       #'open_bb_low_diff', 'open_bb_high_diff', # Bollinger band stuff\n                       #'returnsOpenPrevMktres10_diff',\n                      'returnsOpenNextMktres10', # Target variable\n                      'universe', # binary variable indicating if entry will be used in metric\n                     ]","2d73dec5":"market_train.head()","c4128e65":"# Drop columns not in use\nmarket_train = market_train[LSTM_COLUMNS_TO_USE]","3ce379bc":"market_train.head()","0a53f7ef":"print('MARKET TRAIN:')\nfor col in market_train.columns:\n    print(col+' has '+str(market_train[col].isna().sum())+' NAs')","364e94a1":"# Attempt to impute by group by's median\n#market_train['open_bb_low_diff'] = market_train.groupby(['assetCode'])['open_bb_low_diff'].transform(lambda x: x.fillna(x.median()))\n#market_train['open_bb_high_diff'] = market_train.groupby(['assetCode'])['open_bb_high_diff'].transform(lambda x: x.fillna(x.median()))\n#market_train['returnsOpenPrevMktres10_diff'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres10_diff'].transform(lambda x: x.fillna(x.median()))\n\n# If the assetCode has no non-null values, then impute with column median\nmarket_train = market_train.fillna(market_train.median())","554c4277":"INFORMATION_COLS = ['assetCode','time','universe','returnsOpenNextMktres10'] # Needs to be in this order\nINPUT_COLS = [f for f in market_train.columns if f not in INFORMATION_COLS]","4866bad2":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nmarket_train[INPUT_COLS] = scaler.fit_transform(market_train[INPUT_COLS])","59793806":"market_train.head()","8e67e06c":"# Create LSTM input for each assetCode individually and store in a huge list\n# This takes 45-60 minutes to run, so be patient, Corey\nlstm_df_list = []\nfor assetCode in tqdm(market_train['assetCode'].unique()):\n    lstm_df_list.append(series_to_supervised(market_train.loc[market_train['assetCode']==assetCode, INPUT_COLS].values, # Input columns\n                                             market_train.loc[market_train['assetCode']==assetCode, INFORMATION_COLS],  # Information columns (e.g. TARGET, assetCode, ...)\n                                             n_in=n_in,\n                                             n_out=n_out,\n                                             dropnan=True,\n                                             pad=True))","29d088dc":"# Free up memory\ndel market_train\ngc.collect()","72dc861f":"# Rowbind your list of dataframes\nfull_lstm_df = pd.concat(lstm_df_list, axis=0).reset_index(drop=True)","196d9e3f":"# Free up memory\ndel lstm_df_list\ngc.collect()","f9fd07af":"full_lstm_df.head()","7ac25f6d":"# Convert to a TARGET 1 or 0. Did your stock increase, or not?\nfull_lstm_df['TARGET'] = (np.sign(full_lstm_df['returnsOpenNextMktres10']) + 1) \/ 2\n\n# Need to look into: If I don't convert and instead have a tanh activation at the end, does this improve score?","291c3331":"full_lstm_df.head()","16a3ccc6":"train = full_lstm_df.loc[full_lstm_df['time'].dt.year < 2016]\nvalid = full_lstm_df.loc[full_lstm_df['time'].dt.year >= 2016]\n\n# Only select those where you have universe\nvalid = valid.loc[valid['universe']==1]\n\n#train_index = full_lstm_df.index[(full_lstm_df['time'].dt.year < 2016)].tolist()\n#valid_index = full_lstm_df.index[(full_lstm_df['time'].dt.year >= 2016) & (full_lstm_df['universe']==1)].tolist()\n\n\n\n#from sklearn.model_selection import train_test_split\n\n#train, valid = train_test_split(full_lstm_df,test_size=0.20, random_state=21)","271cc85b":"# Save the history for recent\/active stocks\nactive_history = full_lstm_df.loc[(full_lstm_df['time'].dt.year >= 2016) & (full_lstm_df['time'].dt.month >= 11)].groupby('assetCode').tail(1) # change to tail(4) eventually?\n\n# Desperately need more memory\ndel full_lstm_df\ngc.collect()","2232461f":"train.head()","1f977c10":"valid_X, valid_y = valid.values[:, :-5], valid['TARGET'].values\nvalid_returns = valid.values[:, -2]\nvalid_universe = valid.values[:, -3]\nvalid_time = valid.values[:, -4]","039d4e1c":"del valid\ngc.collect()","22d3abd7":"%whos","009a10c6":"train_X, train_y = train.values[:, :-5], train['TARGET'].values","1716b087":"del train\ngc.collect()","9fa7805c":"# # split into input and outputs\n# # Assumption: The last column is your target column\n# # And the final 4 columns should not be used in training (here assetCode, time, returnsOpenNextMktres10, and TARGET)\n# train_X, train_y = train.values[:, :-5], train['TARGET'].values\n# valid_X, valid_y = valid.values[:, :-5], valid['TARGET'].values\n\nUSE_SIGMOID_ACTIVATION_LAYER = True\n\n# if not USE_SIGMOID_ACTIVATION_LAYER:\n#     train_y = train['returnsOpenNextMktres10'].values\n#     valid_y = valid['returnsOpenNextMktres10'].values\n    \n# del train, valid\n# gc.collect()","4bac7cde":"# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\nvalid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\nprint(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape)","7c97e5c9":"from keras import callbacks","b4da4603":"# https:\/\/medium.com\/@thongonary\/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n\nclass Metrics(callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = (pd.DataFrame(model.predict(X_val)) * 2) - 1 # Need to convert it back to [-1, 1] instead of [0, 1]\n        _sigmascore = sigma_scorelstm(y_val, y_predict)\n        print(\" \u2014 sigmascore: %f\" % (_sigmascore))\n\n        self._data.append({\n            'val_sigmascore': _sigmascore\n        })\n        return\n\n    def get_data(self):\n        return self._data\n\nmetrics = Metrics()","d5965c3b":"def sigma_scorelstm(y_true, y_pred):\n        x_t_i = y_pred * pd.DataFrame(valid_returns) * pd.DataFrame(valid_universe) # Multiply my confidence by return multiplied by universe\n        data = pd.concat([pd.DataFrame(valid_time), x_t_i], axis=1)\n        data.columns = ['day','x_t_i']\n        x_t = data.groupby('day').sum().values.flatten()\n        mean = np.mean(x_t)\n        std = np.std(x_t)\n        score_valid = mean \/ std\n        return score_valid","40d70903":"# model = Sequential()\n# model.add(LSTM(50, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n# model.add(Dropout(0.25))\n# model.add(LSTM(50, return_sequences=True))\n# model.add(Dropout(0.25))\n# model.add(LSTM(50))\n\n# https:\/\/arxiv.org\/ftp\/arxiv\/papers\/1801\/1801.01777.pdf\n# This paper reports more success using deeper NN architectures than shallow NN\nmodel = Sequential()\nmodel.add(GRU(50, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.50))\n#model.add(GRU(50, return_sequences=True))\n#model.add(Dropout(0.50))\nmodel.add(GRU(25, return_sequences=True))\nmodel.add(Dropout(0.50))\n#model.add(GRU(25, return_sequences=True))\n#model.add(Dropout(0.50))\n#model.add(GRU(10, return_sequences=True))\n#model.add(Dropout(0.50))\nmodel.add(GRU(10))\nmodel.add(Dropout(0.50))\n\nif USE_SIGMOID_ACTIVATION_LAYER:\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop') # RMS prop is supposed to be better for recurrent neural networks.\nelse:\n    model.add(Dense(1, activation='tanh'))\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n# fit network\n# 15 epochs yielded 0.49 sigma score\nhistory = model.fit(train_X, train_y, epochs=15, batch_size=1028, validation_data=(valid_X, valid_y), verbose=2, shuffle=True, callbacks=[metrics])","9bb406d2":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()","bd82f43a":"yhat_lstm = (pd.DataFrame(model.predict(valid_X)) * 2) - 1 # Need to convert it back to [-1, 1] instead of [0, 1]\n# yhat.to_csv('predictions.csv', index=False)","95d317c9":"x_t_i = yhat_lstm * pd.DataFrame(valid_returns) * pd.DataFrame(valid_universe) # Multiply my confidence by return multiplied by universe\ndata = pd.concat([pd.DataFrame(valid_time), x_t_i], axis=1)\ndata.columns = ['day','x_t_i']\nx_t = data.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","41e3d050":"# Desperately need memory. Let's delete more stuff.\ndel history, train_X, train_y, valid_X, valid_y\ngc.collect()","f87f9184":"x_t_i = pd.DataFrame((yhat_lgbm + yhat_goss + yhat_dart)\/3) * pd.DataFrame(valid_returns) * pd.DataFrame(valid_universe) # Multiply my confidence by return multiplied by universe\ndata = pd.concat([pd.DataFrame(valid_time), x_t_i], axis=1)\ndata.columns = ['day','x_t_i']\nx_t = data.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","f2d8e693":"x_t_i = (yhat_lstm + pd.DataFrame(yhat_lgbm + yhat_goss + yhat_dart))\/4 * pd.DataFrame(valid_returns) * pd.DataFrame(valid_universe) # Multiply my confidence by return multiplied by universe\ndata = pd.concat([pd.DataFrame(valid_time), x_t_i], axis=1)\ndata.columns = ['day','x_t_i']\nx_t = data.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","4627f345":"# Convert back to [0,1] for Logistic Regression\n# Also convert to DataFrame\nyhat_lgbm = pd.DataFrame((yhat_lgbm + 1) \/ 2)\nyhat_goss = pd.DataFrame((yhat_goss + 1) \/ 2)\nyhat_dart = pd.DataFrame((yhat_dart + 1) \/ 2)\nyhat_lstm = (yhat_lstm + 1) \/ 2","91423681":"truth = (np.sign(valid_returns) + 1) \/ 2","409067b3":"ensemble = pd.concat([yhat_lgbm, yhat_goss, yhat_dart, yhat_lstm], axis=1)\nensemble.columns = ['lgbm','goss','dart', 'lstm']\n\ntruth = pd.DataFrame(truth).astype('int')\ntruth.columns = ['y']","89dd51a9":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(ensemble, truth)\nlogreg.score(ensemble, truth) # R squared","d201c20b":"yhat_ens = (pd.DataFrame(logreg.predict_proba(ensemble)[:,1]) * 2) - 1 # Need to convert it back to [-1, 1] instead of [0, 1]\n\nx_t_i = yhat_ens * pd.DataFrame(valid_returns) * pd.DataFrame(valid_universe) # Multiply my confidence by return multiplied by universe\ndata = pd.concat([pd.DataFrame(valid_time), x_t_i], axis=1)\ndata.columns = ['day','x_t_i']\nx_t = data.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)","2cac5420":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()","926a8905":"# Delete universe, returnsOpenNextMktres10, TARGET. We wont have access to these in test.\ndel active_history['universe']\ndel active_history['returnsOpenNextMktres10']\ndel active_history['TARGET']","e1101eaf":"active_history.columns","2a7afb86":"# Correct LSTM columns to use\nLSTM_COLUMNS_TO_USE = [col for col in LSTM_COLUMNS_TO_USE if (col!='universe' and col!='returnsOpenNextMktres10')]","502d2a69":"for (market_obs_df, _, predictions_template_df) in days:\n    # Delete the earliest piece of data in anticipation of the new\n    \n    active_history.drop([col for col in active_history.columns if '(t-'+str(n_in)+')' in col], axis=1, inplace=True)\n\n    del active_history['time']\n    \n    \n    #######################\n    # LGBM and RF modeling:\n    \n    market_obs_df['time'] = pd.to_datetime(market_obs_df['time'].dt.date)\n        \n    # Feature Engineering\n    market_obs_df['close_minus_open'] = market_obs_df['close'] - market_obs_df['open']\n    market_obs_df['margin1'] = market_obs_df['open'] \/ market_obs_df['close']\n    \n    # Save to history df\n    total_market_obs_df.append(market_obs_df)\n    history_df = pd.concat(total_market_obs_df[-(np.max(30)+1):]) # Store last 30 for assetCodes\n    \n    # Get Mean Features\n\n    return_features = ['volume','returnsClosePrevRaw1',\n           'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n           'returnsOpenPrevMktres1', 'returnsClosePrevRaw10',\n           'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n           'returnsOpenPrevMktres10', \n            'open', 'close', \n            'margin1', 'close_minus_open']\n    n_lag = [5]\n    new_df = generate_lag_features(history_df,n_lag=n_lag)\n    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n\n    del new_df\n    gc.collect()\n\n    # Get MACD Features\n\n    return_features = ['open', 'close']\n    n_lag = [5]\n    new_df = generate_lag_features2(history_df,n_lag=n_lag)\n    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n\n    del new_df\n    gc.collect()\n\n    # Get Bollinger Bands Features\n\n    return_features = ['open', 'close']\n    n_lag = [5]\n    new_df = generate_lag_features3(history_df,n_lag=n_lag)\n    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n\n    del new_df\n    gc.collect()\n\n    # Get EMA Features\n\n    return_features = ['open', 'close']\n    n_lag = [5]\n    new_df = generate_lag_features4(history_df,n_lag=n_lag)\n    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n\n    del new_df\n    gc.collect()\n\n    market_obs_df['volume_diff'] = market_obs_df['volume'] - market_obs_df['volume_lag_5_mean']\n    market_obs_df['returnsClosePrevRaw1_diff'] = market_obs_df['returnsClosePrevRaw1'] - market_obs_df['returnsClosePrevRaw1_lag_5_mean']\n    market_obs_df['returnsOpenPrevRaw1_diff'] = market_obs_df['returnsOpenPrevRaw1'] - market_obs_df['returnsOpenPrevRaw1_lag_5_mean']\n    market_obs_df['returnsClosePrevMktres1_diff'] = market_obs_df['returnsClosePrevMktres1'] - market_obs_df['returnsClosePrevMktres1_lag_5_mean']\n    market_obs_df['returnsOpenPrevMktres1_diff'] = market_obs_df['returnsOpenPrevMktres1'] - market_obs_df['returnsOpenPrevMktres1_lag_5_mean']\n    market_obs_df['returnsClosePrevRaw10_diff'] = market_obs_df['returnsClosePrevRaw10'] - market_obs_df['returnsClosePrevRaw10_lag_5_mean']\n    market_obs_df['returnsOpenPrevRaw10_diff'] = market_obs_df['returnsOpenPrevRaw10'] - market_obs_df['returnsOpenPrevRaw10_lag_5_mean']\n    market_obs_df['returnsClosePrevMktres10_diff'] = market_obs_df['returnsClosePrevMktres10'] - market_obs_df['returnsClosePrevMktres10_lag_5_mean']\n    market_obs_df['returnsOpenPrevMktres10_diff'] = market_obs_df['returnsOpenPrevMktres10'] - market_obs_df['returnsOpenPrevMktres10_lag_5_mean']\n    market_obs_df['open_diff'] = market_obs_df['open'] - market_obs_df['open_lag_5_mean']\n    market_obs_df['close_diff'] = market_obs_df['close'] - market_obs_df['close_lag_5_mean']\n    market_obs_df['margin1_diff'] = market_obs_df['margin1'] - market_obs_df['margin1_lag_5_mean']\n    market_obs_df['close_minus_open_diff'] = market_obs_df['close_minus_open'] - market_obs_df['close_minus_open_lag_5_mean']\n\n    market_obs_df['open_macd_diff'] = market_obs_df['open_ewma'] - market_obs_df['open_macd']\n    market_obs_df['close_macd_diff'] = market_obs_df['close_ewma'] - market_obs_df['close_macd']\n\n    market_obs_df['open_bb_high_diff'] = market_obs_df['open'] - market_obs_df['open_bollingerband_high']\n    market_obs_df['open_bb_low_diff'] = market_obs_df['open'] - market_obs_df['open_bollingerband_low']\n\n    market_obs_df['close_bb_high_diff'] = market_obs_df['close'] - market_obs_df['close_bollingerband_high']\n    market_obs_df['close_bb_low_diff'] = market_obs_df['close'] - market_obs_df['close_bollingerband_low']\n\n    market_obs_df['open_ewma_diff'] = market_obs_df['open'] - market_obs_df['open_ewma']\n    market_obs_df['close_ewma_diff'] = market_obs_df['close'] - market_obs_df['close_ewma']\n    \n    # Make predictions with LGBM, GOSS, and DART.\n    \n    yhat_lgbm = pd.DataFrame( corey_lgbm_model.predict( market_obs_df[lgbm_cols]) )\n    \n    yhat_goss = pd.DataFrame( corey_goss_model.predict(market_obs_df[lgbm_cols]) )\n    \n    yhat_dart = pd.DataFrame( corey_dart_model.predict(market_obs_df[lgbm_cols]) )\n    \n    ##############################\n    #LSTM MODEL PART:\n    \n    # Reverse column order to prepare for column renaming later\n    active_history_cols = ['assetCode']\n    for step in range(0, n_in):\n        for varnum in range(1, len(INPUT_COLS)+1):\n            if step==0:\n                stepnum = ''\n            else:\n                stepnum = '-'+str(step)\n            active_history_cols.append('var'+str(varnum)+'(t'+stepnum+')')\n            \n    active_history = active_history[active_history_cols]\n    \n    # Drop columns not in use\n    market_obs_df = market_obs_df[LSTM_COLUMNS_TO_USE]\n    \n    # If there are any missing values for the new data, then impute with the median of that day.\n    market_obs_df = market_obs_df.fillna(market_obs_df.median())\n    \n    # StandardScale it\n    market_obs_df[INPUT_COLS] = scaler.transform(market_obs_df[INPUT_COLS])\n    \n    # If there was a column with all NA's, then fill it with 0's\n    market_obs_df = market_obs_df.fillna(0)\n    \n    # Update your active_history to include new data\n    active_history = market_obs_df.merge(right=active_history, how='outer', on='assetCode')\n    \n    # Rename your active_history columns as to what the NN model is expecting\n    active_history_cols = ['time','assetCode']\n    for step in range(0, n_in+1):\n        for varnum in range(1, len(INPUT_COLS)+1):\n            if step==0:\n                stepnum = ''\n            else:\n                stepnum = '-'+str(step)\n            active_history_cols.append('var'+str(varnum)+'(t'+stepnum+')')\n    active_history.columns = active_history_cols\n    \n    # Shift values to the left if you didn't receive an update from market_obs_df\n    # From https:\/\/stackoverflow.com\/questions\/37400246\/pandas-update-multiple-columns-at-once (you need .values at the end to dismiss column name indexing)\n    active_history_cols = []\n    for step in range(0, n_in):\n        for varnum in range(1, len(INPUT_COLS)+1):\n            if step==0:\n                stepnum = ''\n            else:\n                stepnum = '-'+str(step)\n            active_history_cols.append('var'+str(varnum)+'(t'+stepnum+')')\n            \n    active_history_cols2 = []\n    for step in range(1, n_in+1):\n        for varnum in range(1, len(INPUT_COLS)+1):\n            if step==0:\n                stepnum = ''\n            else:\n                stepnum = '-'+str(step)\n            active_history_cols2.append('var'+str(varnum)+'(t'+stepnum+')')\n\n    # var(t), var(t-1), ..., var(t - (n_in) + 1) = var(t-1), var(t-2), ..., var(t-(n_in))\n    active_history.loc[active_history['time'].isnull(), active_history_cols] = active_history.loc[active_history['time'].isnull(), active_history_cols2].values\n    \n    # Impute values with the same value over and over\n    LAST_VAL_TO_BE_IMPUTED = 'var1(t-'+str(n_in)+')'\n    for step in range(1, n_in+1):\n        active_history.loc[active_history[LAST_VAL_TO_BE_IMPUTED].isnull(), [col for col in active_history.columns if '(t-'+str(step)+')' in col]] = active_history.loc[active_history[LAST_VAL_TO_BE_IMPUTED].isnull(), [col for col in active_history.columns if '(t)' in col]].values\n    \n    # Predict on this\n    to_predict = active_history.loc[active_history['time'].notnull(), [col for col in active_history.columns if 'var' in col]]\n\n    to_predict = to_predict.values\n    to_predict = to_predict.reshape((to_predict.shape[0], 1, to_predict.shape[1]))\n    \n    # Predicting with the NN model\n    yhat_lstm = pd.DataFrame(model.predict(to_predict))\n    \n    # Predict on Ensemble now\n    ensemble = pd.concat([yhat_lgbm, yhat_goss, yhat_dart, yhat_lstm], axis=1)\n    ensemble.columns = ['lgbm','goss','dart', 'lstm']\n\n    preds = logreg.predict_proba(ensemble)[:,1]\n    preds = (preds * 2) - 1 # Convert from [0,1] to [-1,1]\n    \n    predictions_template_df['confidenceValue'] = preds\n    env.predict(predictions_template_df)","357934dd":"env.write_submission_file()","5a2dba57":"# Can i use Logistic Regression to improve my score? Let's try this instead of just averaging.","52e62ac3":"# Light preprocessing:","3315d21d":"## For some reason, shaping the series into LSTM format using parallel processes gives no improvement in speed than just doing it sequentially on one process. I'm just leaving the parallel process here to remind myself of how to code it, and who knows, I might come back to it.","83a9f250":"Since around September 2018 I have been able to code my own LSTM neural networks in Keras. LSTM makes perfect sense for this competition: we have data we receive at a daily granularity, and we're just predicting 1 or 0: did the stock go up, or down (of course, we must transform this by 2*pred - 1)","0481a519":"# Four columns have NA's. Let's impute them with the median of the group","6771f53f":"# Predictions:","5a11e714":"# Split into train and test. Yes, even though for time series your validation should be future data, it is extremely necessary to train on the most recent data to make future predictions. Therefore, I'm opting for just a regular old train_test_split.","f848c2f7":"# Impute NA's","d4dabfa2":"# Hypothesis: I think i dont have enough RAM to construct the list. So I am reducing amount of information being fed.","c815e186":"# Begin to train LSTM model","898e38c5":"# Ensemble score:","fde7785c":"# Train LGBM model","0ed52af3":"# LGBM, GOSS, DART Blend","9efb6005":"# Okay I'm not using LSTM I'm using GRU instead. but all my variable names are based on the word 'lstm' so deal with it =)","8b716afb":"# LGBM, GOSS, DART, LSTM Blend","bf6d45de":"# Created by Corey Levinson","c264bfb3":"# Train a GOSS Model","136043f6":"# Train a DART model","af776d74":"# Define the function to shape into the LSTM input","17cedb17":"A large difficulty of this problem comes from the fact that the data frame is not static. So we can't just do standard LSTM solution. We need to write the model to work as it receives a new row each day. That's a challenge I attempt to solve in this kernel.","ada1fef7":"# Normalize inputs for NN"}}