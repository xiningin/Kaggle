{"cell_type":{"2c7228e3":"code","c2cb3d71":"code","485a5876":"code","c6dffa0c":"code","001365c2":"markdown","ce60e522":"markdown","20f75abc":"markdown","0b57aaa8":"markdown"},"source":{"2c7228e3":"from cycler            import cycler\nfrom matplotlib.pyplot import figure,bar,xlabel,ylabel,legend,rc, plot,savefig, title, scatter\nfrom numpy             import argsort, exp, zeros, int32, sum\nfrom numpy.random      import rand\nfrom os                import walk\nfrom os.path           import join\nfrom pandas            import read_csv\nfrom random            import random, randrange, gauss\nfrom time              import time\n\n\nrc('lines', linewidth=2)\nrc('axes', \n   prop_cycle=(cycler(color=['c', 'm', 'y','b','c', 'm', 'y','b']) +\n                  cycler(linestyle=[ '--', '--','--','--',':', ':', ':',':'])))\n","c2cb3d71":"N_CONTESTS     = 20     # Minnimum number of contests for each contestant\nN_MAX          = 100    # Limit number of contestants - set to None for no limi\nMAX_ITERATIONS = 1000   # Used to limit iteations while computing BT\nFREQUENCY      = 150    # For work in progress plots\nPLOT_FILE      = 'bt-iterations'\nEPSILON        = 1e-8   # Contols iterations","485a5876":"train_data    = None\n\nfor dirname, _, filenames in walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_name = join(dirname, filename)\n        if filename.startswith('train'):\n            train_data = read_csv(path_name)\n                       ","c6dffa0c":"# create_wins_losses\n# # Compute a matrix w[i,k] -- the number of wins for i competing with k\ndef create_wins_losses(Lambdas):\n    w  = zeros((N_MAX,N_MAX))\n    for i in range(N_MAX):\n        for j in range(N_CONTESTS):\n            k = (i+randrange(1,N_MAX)) % N_MAX\n            if random() < Lambdas[i]\/(Lambdas[i] + Lambdas[k]):\n                w[i,k] += 1\n            else:\n                w[k,i] += 1\n    return w\n\n# update\n#\n# Update probablities\n\ndef update(p,w_symmetric,W,N):\n    p1 = zeros(N)\n    for i in range(N):\n        Divisor = 0\n        for j in range(N):\n            if i!=j and p[i]+p[j]>0:\n                Divisor += w_symmetric[i,j]\/(p[i]+p[j])\n        p1[i] = W[i]\/Divisor\n    return p1\/sum(p1)\n\n# normalize\n#\n# Make elements of a vector sum to 1 so it can be used as a probability\n\ndef normalize(p):\n    return p \/ sum(p)\n\nstart          = time()         \nBetas          = train_data.target.to_numpy()\nLambdas        = exp(Betas)\nN,_            = train_data.shape\nif N_MAX==None:\n    N_MAX = N\nelse:\n    Lambdas    = Lambdas[:N_MAX]\nw              = create_wins_losses( Lambdas)\nw_symmetric    = w + w.transpose()\nW              = sum(w,axis=1)  # Number won by i\np              = normalize(rand(N_MAX))\n\nfig            = figure(figsize=(10,10))\naxes           = fig.subplots(nrows=2,ncols=1)\naxes[0].plot(range(N_MAX),Lambdas\/sum(Lambdas),\n             label     = r'$\\lambda$',\n             linestyle = '-',\n             color     = 'k')\n\nfor k in range(MAX_ITERATIONS):\n    p1 = update(p,w_symmetric,W,N_MAX)\n    if (abs(p-p1)<EPSILON*p1).all():\n        p = p1\n        break\n    p  = p1\n    if k%FREQUENCY==0:\n        axes[0].plot(range(N_MAX),p,\n             label = f'iteration={k}')\n        \naxes[0].plot(range(N_MAX),p,\n    label     = f'Final',\n    linestyle = '-',\n    color     = 'r')\naxes[0].legend()\naxes[0].set_xlabel('index')\naxes[0].set_ylabel('p')\nelapsed = int(time() - start)\naxes[0].set_title(f'{N_MAX} Contestants, {N_CONTESTS} contests. Time = {elapsed} seconds, eps={EPSILON}, k={k}')\n\naxes[1].scatter(Lambdas\/sum(Lambdas),p,s=5,label=r'$\\lambda$ vs p',color='b')\naxes[1].plot(Lambdas\/sum(Lambdas),Lambdas\/sum(Lambdas),label='Ideal',color='k')\naxes[1].set_xlabel(r'$\\lambda$')\naxes[1].set_ylabel('p')\naxes[1].legend()\naxes[1].set_title(r'p versus $\\lambda$')\nfig.savefig(PLOT_FILE,bbox_inches='tight')\n\n","001365c2":"# Load Data\n\n## Data Dictionary\n\n|Train|Public Test|Hidden Test|Description|\n|--------------|--------------|----------|----------------------------------------------------|\n|id|id|id|Unique ID for excerpt|\n|url_legal|url_legal|- |URL of source (Omitted from some records in the test set--see [note](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/238670#1306025))|\n|license|license |-|License of source material (Omitted from some records in the test set--see [note](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/238670#1306025))|\n|excerpt|excerpt|excerpt|Text for predicting readability|\n|target|-|-|Readability|\n|standard_error|-|-|Measure of spread of scores among multiple raters for each excerpt|","ce60e522":"# Hyper parameters","20f75abc":"# Estimate Parameters\n\nThis is the algorithm from [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model#Estimating_the_parameters). We will plot progress to convergence, and\ncompare result with initial values.\n","0b57aaa8":"# Bradley-Terry\n## Purpose\nI created this notebook to gain a better understanding of the [Bradley-Terry Model](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model), which has been [used to allocate scores](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423) for the [Common Lit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize).\nIn particular I want to know:\n1. Can we reconstruct the target scores from the results of \"contests\" between documents?\n1. Is the reconstruction consistent? How much does it depend on the way the contestants have been sampled to build contests?\n1. Is the reconstruction efficient? Can we do better with something like Expecation Maximization?\n\n## Bradley-Terry\n\n[1] and [4] give two forms of the equation.\n$$\\begin{align}P(\\text{i beats j}) =& \\frac{e^{\\beta_i}}{e^{\\beta_i}+e^{\\beta_j}}\\\\\nlogit(P(\\text{i beats j})) =& \\lambda_i - \\lambda_j \\text{, where }\\\\\n\\lambda_i =& e^{\\beta_i} \\text{ and }\\\\\nlogit(p) =& \\log\\big(\\frac{p}{1-p}\\big)\\end{align}$$\n\n|Question|Outcome|\n|---------------------------------------|-----------------------------------------------------------|\n|Since some of the target scores in the context are negative, I conjecture that they correspond to the $\\beta_i$||\n|Target scores in contest are approximately Gaussian.What does this say about probabilities of contests?||\n\n## Methods\n\nThis code contained in this notebook sets up contests between pairs of texts in the training dataset. The opponents for each text are selected at random, and the outcome of each context are chosen at random, with the probability of extract $i$ being deemed easier to read than extract $j$ given by $\\frac{e^{\\beta_i}}{e^{\\beta_i}+e^{\\beta_j}}$. \n## Results\n\nI executed the code from this notebook offline, on my own laptop ($2\\times 2.2 GHz$, 8 G RAM)\n![bt-iterations-2834.png](attachment:0e50b950-e56c-4cd1-acd6-d7a0d7eeaea3.png). The execution time was in excess of 6 hours.\n\nI performed two more runs on a subset of the dataset to test consistency.\n![bt-iterations-1.png](attachment:314c6cc3-5297-4199-ab20-5d1be4cb5b63.png)\n![bt-iterations-2.png](attachment:edebaf9c-e9ce-406f-ad17-29e889822a60.png)\n\n## Conclusions\n\n1. Computing Bradley Terry was slow, using the Maximum Likelihood algorithm of [1].\n1. The scatter plots show that the scores are broadly consistent with the original scores (highly ranked texts are likely to remain high), but there is considerable variability.\n1. On balance, I no longer consider that the approach described in [Some thoughts on exploiting Bradley-Terry](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/242023) to be viable.\n\n## Further Work\n\n1. Can we reduce execution time significantly using a more efficient algoritm, such as Expectation Maximization?[6]\n1. Include RMS error in the scatter plots, as this would provide a lower bound on the competition score. *I have calculatesd RMS (not shown) for 500 texts, and got a value of 0.4, which is clearly too high.*\n\n## References\n||Title|Author|\n|---|-----------------------------------------------|-----------------------------|\n|[1]|[Bradley-Terry Model](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model)|Wikipedia Editors|\n|[2]|[Log5, the Logit Link and Bradley-Terry](https:\/\/www.kaggle.com\/jaredcross\/log5-logistic-regression-and-bradley-te\/comments)|Jared Cross|\n|[3]|[Target scores](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423)|Scott Crossley|\n|[4]|[Bradley-Terry Models in R](https:\/\/www.jstatsoft.org\/article\/view\/v012i01\/v12i01.pdf)|David Firth|\n|[5]|[Common Lit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize)|Scott Crossley|\n|[6]|[Efficient Bayesian Inference for Generalized Bradley-Terry Models](https:\/\/arxiv.org\/abs\/1011.1761)|Francois Caron and Arnaud Doucet|\n"}}