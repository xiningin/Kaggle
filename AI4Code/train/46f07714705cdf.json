{"cell_type":{"a280749a":"code","d183c2f4":"code","78da2fe9":"code","2d98cc73":"code","d47298d6":"code","59472dcb":"code","4bd290a8":"code","3e5ba692":"code","f63f4535":"code","b3a9ab53":"code","37fd4732":"code","d7f38336":"code","9c494aa1":"markdown","b4a7038d":"markdown","64a95164":"markdown","830dadc4":"markdown","695889c9":"markdown","60fd5169":"markdown","bc4b14cc":"markdown","92aa5e28":"markdown","495b34f3":"markdown","71c2d054":"markdown","ee6b963f":"markdown"},"source":{"a280749a":"# Data Wrangling and Data Analysis\n\nimport pandas as pd , numpy as np\nimport gc\n\n# Visualization\n\nfrom matplotlib import pyplot as plt, style\nimport seaborn as sns\n\n# Feature Engineering \/ Feature Selection\nimport optuna \nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\n\n\n# Model Building\n#import lightgbm as lgbm\n#from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n#from catboost import CatBoostClassifier, Pool\nfrom sklearn import metrics\n\n\n# Ignore Warings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d183c2f4":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","78da2fe9":"# Before working with the data, we reduce the use of memory, so we can improve performance\n\ndef import_data(file):\n    # Reading File\n    df = pd.read_csv(file)\n    \n    # Reducing Size by Optimizing Dtypes of columns\n    df = reduce_mem_usage(df)\n    \n    # Converting Bool cols into integer\n    bool_cols = []\n    for i, col in enumerate(df.columns):\n        if df[col].dtypes == bool:\n            bool_cols.append(i)\n    df.iloc[:, bool_cols] = df.iloc[:, bool_cols].astype(int)  \n    \n    return df\n\n# Train Data\ntrain = import_data('..\/input\/tpsoct2021-5-folds\/Train_tps_oct_2021_kfold.csv')\n\n#Test Data\ntest = import_data('..\/input\/tabular-playground-series-oct-2021\/test.csv')\n\n\n# Submission\n\nsubmission = import_data('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","2d98cc73":"# categorical Columns :- \n\nCat_cols = [col for col in train.columns if train[col].nunique() < 5]\nCat_cols.remove('target')\n#Cat_cols_indices = [train.columns.get_loc(col) for col in Cat_cols]\n\n# Continous Columns : -\n\nCon_cols = [col for col in train.columns if train[col].nunique() > 5]\n\n\n\n# Number of each respective column types\n\nprint(\"Number of Categorical Columns :\", len(Cat_cols))\nprint(\"Number of Continous Columns   :\", len(Con_cols))\n\ngc.collect()","d47298d6":"Con_cols_test = [cols for cols in Con_cols if cols not in [\"mean\", \"std\", \"min\", \"max\", \"abs_sum\", \"sem\"]]\ntest[\"mean\"] = test[Con_cols_test].mean(axis=1)\ntest[\"std\"] = test[Con_cols_test].std(axis=1)\ntest[\"min\"] = test[Con_cols_test].min(axis=1)\ntest[\"max\"] = test[Con_cols_test].max(axis=1)\ntest['abs_sum'] = test[Con_cols_test].abs().sum(axis=1)\ntest['sem'] = test[Con_cols_test].sem(axis=1)\ndel Con_cols_test\ngc.collect()","59472dcb":"XGB_params = {'learning_rate': 0.014679233453195013, 'reg_lamba':75.56651890088857, 'reg_alpha': 0.11766857055687065,\n              'gamma': 0.6407823221122686, 'subsample': 0.4640789338167099,\n              'subsample': 0.7,'colsample_bytree': 0.2,'colsample_bylevel': 0.6000000000000001,\n              'min_child_weight': 56.41980735551558,'max_depth': 6,\n              'verbosity' : 0, 'eval_metric' : 'auc','objective':\"binary:logistic\",\n              'use_label_encoder': False,'tree_method': 'gpu_hist',\n              \"seed\": 42, 'n_jobs': -1, 'n_estimators': 20000, 'predictor': 'gpu_predictor',\n              'learning_rate': 0.013474548048574765}\n","4bd290a8":"# Features and Target \n\ntrain.reset_index(inplace=True)\ntest.reset_index(inplace=True)\nFeatures = [c for c in train.columns if c not in ('id','index','target','kfold')]\ngc.collect()","3e5ba692":"# Kfold Loop\n\ndef XGB():\n    final_test_predictions = []\n    final_valid_predictions = {}\n    scores = []\n    for fold in range(5):\n    \n        x_train = train[train.kfold != fold].reset_index(drop=True)\n        x_valid = train[train.kfold == fold].reset_index(drop=True)\n        x_test = test[Features].copy()\n    \n        valid_ids = x_valid.index.values.tolist()\n   \n        y_train = x_train.target\n        y_valid = x_valid.target\n    \n        x_train = x_train[Features]\n        x_valid = x_valid[Features]\n    \n   \n    \n        gc.collect()\n\n        # Model Training\n        model = XGBClassifier(**XGB_params, n_threads=4)\n        model.fit(x_train, y_train,\n                  eval_set=[(x_valid,y_valid)],\n                  early_stopping_rounds=200,\n                  verbose = False)\n        gc.collect()\n    \n        # Predictions\n        valid_preds = model.predict_proba(x_valid)[:,1]\n        test_preds = model.predict_proba(x_test)[:,1]\n    \n        final_test_predictions.append(test_preds)\n        final_valid_predictions.update(dict(zip(valid_ids, valid_preds)))\n\n        # Score\n        auc = metrics.roc_auc_score(y_valid, valid_preds)\n        scores.append(auc)\n        print(f\"Fold: {fold + 1} Score: {auc}\")\n        print('||'*40)\n        gc.collect()\n    print(\"Overall Auc : - \", np.mean(scores))\n    return final_test_predictions, final_valid_predictions\n    \nfinal_test_predictions, final_valid_predictions = XGB()","f63f4535":"# final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient = \"index\")\n# final_valid_predictions.columns = ['id', 'pred_1']\n\n# final_valid_predictions.to_csv(\"train_pred_1.csv\", index = False)","b3a9ab53":"submission.target = np.mean(np.column_stack(final_test_predictions), axis = 1)\n\n#submission.columns = ['id', 'pred_1']\nsubmission.to_csv(\"submission.csv\", index = False)","37fd4732":"submission.head()","d7f38336":"# lgb_params = {\n#      'objective': 'binary',\n#      'n_estimators':N_ESTIMATORS,\n#      'importance_type': 'gain',\n#      'metric':'auc',\n#      'boosting_type': 'gbdt',\n#      'n_jobs' : -1, \n#     'learning_rate': 0.0038511441056118664, \n#     'subsample': 0.5827550088149794, \n#     'subsample_freq': 1, \n#     'colsample_bytree': 0.19599597755538956, \n#     'reg_lambda': 0.011685550612519125, \n#     'reg_alpha': 0.04502045156737212, \n#     'min_child_weight': 16.843316711276092, \n#     'min_child_samples': 412, \n#     'num_leaves': 546, \n#     'max_depth': 5, \n#     'cat_smooth': 36.40200359200525, \n#     'cat_l2': 12.979520035205597\n#     }","9c494aa1":"<font color = \"maroon\" size=5px><b>1.1 Reducing Data Size<\/b><\/font>","b4a7038d":"<font color = \"blue\" size=4px> <b>Interpretation : <\/b><\/font>\n\n* <font color = \"red\" size=4px>Most features Are Left Skewed, so We use Algorithms That Doesnt Expect Normality ,i.e any **tree based ensemble techniques** <\/font>\n\n<hr>","64a95164":"<font color = \"#191970\" size=6px><center><b>Data Collection<\/b><\/font>","830dadc4":"<font color = \"maroon\" size=5px><b>2.4 Feature Engineering<\/b><\/font>","695889c9":"<font color = \"#191970\" size=6px><center><b>LGBM HyperParameter Tuning<\/b><\/font>","60fd5169":"<font color = \"maroon\" size=4px><b>Since the dataset is vert large(1 million rows) . lest use below function to convert each column to the best fitting datatype for its range.<\/b><\/font>","bc4b14cc":"<font color = \"#191970\" size=6px><center><b>Import Required Libraries<\/b><\/font>","92aa5e28":" <font size = 1000px color = 'royalblue'><center><b>Blending<b><\/center><\/font>\n ","495b34f3":"<font color = \"#191970\" size=6px><center><b>XGB Model<\/b><\/font>","71c2d054":"<font color = \"maroon\" size=5px><b>3.1 Best Params From HyperParameter Tuning Notebook :- [HyperParameter Tuning TPS OCT-2021](https:\/\/www.kaggle.com\/jaysrivastav\/blending-tps-oct-21?scriptVersionId=77672359) <\/b><\/font>","ee6b963f":"<font color = \"maroon\" size=5px><b>2.3 Analysing Different Column Types<\/b><\/font>"}}