{"cell_type":{"8092b6b4":"code","8bd2e94c":"code","1abdd207":"code","9a615440":"code","4ffdf54c":"code","e4fcd512":"code","dc7238c9":"code","e07ef264":"code","5ea56a5b":"code","6cb2a769":"code","6680f8ce":"code","aa6ce477":"code","95dcabd6":"code","e6c0ce0c":"code","281a4090":"code","6b7e7817":"code","3441def9":"code","01abb3d2":"code","e641a39d":"code","7b57a774":"code","16b41478":"code","17ee76f2":"code","968dc411":"code","360749d3":"code","fd964204":"code","742d0bc5":"code","fb860028":"code","0ee48057":"code","9299dfd5":"code","ccd7bae8":"code","43a228cb":"code","42272bc0":"code","400a3659":"markdown","5dbf2f9d":"markdown","a16183ca":"markdown","0041a46d":"markdown"},"source":{"8092b6b4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import cohen_kappa_score as kappa_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom sklearn.preprocessing import StandardScaler\nkappa_scorer = make_scorer(kappa_score)\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\/\"))\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\/train\"))\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\/test\"))","8bd2e94c":"train_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/test.csv\")","1abdd207":"train_df.head()","9a615440":"cat_cols = ['Type','Age','Breed1','Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', \n          'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized','Health', 'Quantity','State','VideoAmt','PhotoAmt']","4ffdf54c":"num_cols = ['Fee']","e4fcd512":"text_cols = ['Description']","dc7238c9":"embed_sizes = [len(train_df[col].unique()) + 1 for col in cat_cols]","e07ef264":"print('scaling num_cols')\nfor col in num_cols:\n    print('scaling {}'.format(col))\n    col_mean = train_df[col].mean()\n    train_df[col].fillna(col_mean, inplace=True)\n    test_df[col].fillna(col_mean, inplace=True)\n    scaler = StandardScaler()\n    train_df[col] = scaler.fit_transform(train_df[col].values.reshape(-1, 1))\n    test_df[col] = scaler.transform(test_df[col].values.reshape(-1, 1))","5ea56a5b":"from tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","6cb2a769":"print('getting embeddings')\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open('..\/input\/fasttext-english-word-vectors-including-subwords\/wiki-news-300d-1M-subword.vec')))\n","6680f8ce":"num_words = 20000\nmaxlen = 80\nembed_size = 300","aa6ce477":"train_df['Description'] = train_df['Description'].astype(str).fillna('no text')\ntest_df['Description'] = test_df['Description'].astype(str).fillna('no text')","95dcabd6":"print(\"   Fitting tokenizer...\")\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(train_df['Description'].values.tolist())","e6c0ce0c":"train_df['Description'] = tokenizer.texts_to_sequences(train_df['Description'])\ntest_df['Description'] = tokenizer.texts_to_sequences(test_df['Description'])","281a4090":"word_index = tokenizer.word_index\nnb_words = min(num_words, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= num_words: continue\n    try:\n        embedding_vector = embeddings_index[word]\n    except KeyError:\n        embedding_vector = None\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n","6b7e7817":"def get_input_features(df):\n    X = {'description':pad_sequences(df['Description'], maxlen=maxlen)}\n    X['numerical'] = np.array(df[num_cols])\n    for cat in cat_cols:\n        X[cat] = np.array(df[cat])\n    return X","3441def9":"from keras.layers import Input, Embedding, Concatenate, Flatten, Dense, Dropout, BatchNormalization, CuDNNLSTM, SpatialDropout1D\nfrom keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(\n        Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\ncategorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Dense(256, activation = 'relu')(categorical_logits)\n\n\nnumerical_inputs = Input(shape=[len(num_cols)], name='numerical')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\nnumerical_logits = Dense(128, activation = 'relu')(numerical_logits)\n\ntext_inp = Input(shape=[maxlen], name='description')\ntext_embed = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(text_inp)\ntext_logits = SpatialDropout1D(0.2)(text_embed)\ntext_logits = Bidirectional(CuDNNLSTM(64, return_sequences=True))(text_logits)\navg_pool = GlobalAveragePooling1D()(text_logits)\nmax_pool = GlobalMaxPool1D()(text_logits)\ntext_logits = Concatenate()([avg_pool, max_pool])\n\nx = Concatenate()([categorical_logits, text_logits, numerical_logits])\nx = BatchNormalization()(x)\n\nx = Dense(128, activation = 'relu')(x)\nx = Dropout(0.3)(x)\nout = Dense(1, activation = 'sigmoid')(x)\n\nmodel = Model(inputs=[text_inp] + categorical_inputs + [numerical_inputs],outputs=out)\nmodel.compile(optimizer=Adam(lr = 0.0001), loss = 'mse')","01abb3d2":"from sklearn.model_selection import train_test_split\n\ntr_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 23)","e641a39d":"# from keras.utils.np_utils import to_categorical","7b57a774":"tr_df['AdoptionSpeed'].values.shape","16b41478":"y_train = tr_df['AdoptionSpeed'].values \/ 4\ny_valid = val_df['AdoptionSpeed'].values \/ 4","17ee76f2":"#for i, l in enumerate(tr_df['AdoptionSpeed'].values):\n#    y_train[i,l] = 1\n#for i, l in enumerate(val_df['AdoptionSpeed'].values):\n#    y_valid[i,l] = 1","968dc411":"X_train = get_input_features(tr_df)\nX_valid = get_input_features(val_df)\nX_test = get_input_features(test_df)","360749d3":"hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 256, epochs = 10, verbose = True)","fd964204":"y_pred = model.predict(X_test)[:,0]","742d0bc5":"y_pred.shape","fb860028":"y_pred = np.round(y_pred * 4).astype(int)","0ee48057":"y_pred[:10]","9299dfd5":"#y_pred2 = np.argmax(y_pred,axis = 1)\n#y_pred2.shape","ccd7bae8":"\nsubmission_df = pd.DataFrame(data={\"PetID\":test_df[\"PetID\"], \"AdoptionSpeed\":y_pred})\nsubmission_df.to_csv(\"submission.csv\", index=False)","43a228cb":"submission_df['AdoptionSpeed'].mean(), train_df['AdoptionSpeed'].mean()","42272bc0":"submission_df.head(20)","400a3659":"## Handling text columns","5dbf2f9d":"## Handling numerical columns","a16183ca":"## Define NN Model","0041a46d":"## Handling categorical columns"}}