{"cell_type":{"3dd3a94f":"code","07d424a6":"code","40430a19":"code","eb705749":"code","0521ac81":"code","f7547e79":"code","27c2da1c":"code","29f059c2":"code","9c590572":"code","c66fa4d3":"code","5b96a7a2":"code","2725831a":"code","5b66b957":"code","fb1e21f2":"code","0e411d33":"code","25f4fd78":"code","9277e68c":"code","d0c4fb1c":"code","16ef4f9a":"code","b8266083":"code","2b841b55":"code","d3dcdf57":"code","39908f89":"code","6addc669":"code","e044883d":"code","45345c9d":"code","9bbf8d50":"code","0ee9ed97":"code","52d7ea21":"code","dd0903e0":"code","a2eb297f":"code","d9747d5b":"code","3b8b93a0":"code","73bfd81d":"code","8a439b76":"code","7314ef28":"code","59d537d6":"code","07b77d13":"code","9d81fd70":"code","3e266745":"code","63395124":"code","5f2dd330":"code","6c1b9563":"code","ce2614de":"code","0b6555ef":"code","e8ddc4f1":"code","d254abbb":"code","86a8056b":"code","b76a29eb":"code","5f692fd0":"code","fbc4c8c5":"code","d203a1bf":"code","766b4350":"code","35c5f6df":"code","ab6d01a4":"code","e5fba3f8":"code","da347dae":"code","caa7c5ea":"code","f506d703":"code","d91807ca":"code","5ec968f4":"code","54e1c7f3":"code","ad4bfc36":"code","e9f53ba0":"markdown","bd413bdf":"markdown","c9e06786":"markdown","96ccfb5b":"markdown","d20e0a0f":"markdown","046ca6e7":"markdown"},"source":{"3dd3a94f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime","07d424a6":"import tensorflow as tf\nfrom tensorflow import keras\nfrom pylab import rcParams","40430a19":"class color:  # Testing to make the heading look a liitle more impressive\n   BOLD = '\\033[1m'","eb705749":"from sklearn.metrics import mean_squared_error , mean_absolute_error","0521ac81":"df = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/train.csv\")\ndf.head()","f7547e79":"df['date'] =  pd.to_datetime(df['date'])","27c2da1c":"df = df.set_index('date')","29f059c2":"df.head()","9c590572":"df_1_1 = df[(df.store==1) & (df.item==1)] ","c66fa4d3":"Deep1_all = df_1_1.resample('D')['sales'].sum()","5b96a7a2":"#Deep1_all = df.resample('D')['sales'].sum()  # this is of doing the forecast on the total dataset by day","2725831a":"Deep1_all.head()","5b66b957":"Deep1_all_With_index = Deep1_all.copy()","fb1e21f2":"Deep1_all_With_index =Deep1_all_With_index.reset_index()","0e411d33":"Deep1_all_With_index.head()","25f4fd78":"Deep1_all_With_index.head()","9277e68c":"Deep1 = Deep1_all_With_index.drop(['date'], axis = 1)","d0c4fb1c":"Deep1['sales'] = Deep1['sales'].astype('float32')","16ef4f9a":"Deep1.info()","b8266083":"values = Deep1.values","2b841b55":"print(values)","d3dcdf57":"values = values.astype('float32')","39908f89":"Deep1.shape","6addc669":"train_size = int(len(Deep1) -376) # This is 366 days of the year + 10 days of extra data beforehand\ntest_size = len(Deep1) - train_size\ntrain, test = Deep1.iloc[0:train_size], Deep1.iloc[train_size:len(Deep1)]\nprint(len(train), len(test))","e044883d":"print(test)","45345c9d":"print(train)","9bbf8d50":"train.head()","0ee9ed97":"# One of the most difficult parts of Deep Learning modelling is to get the dataset in the right format \n# This function completes that proces\n\ndef create_dataset(X, y, time_steps=1):\n    Xs, ys = [], []\n    for i in range(len(X) - time_steps):\n        v = X.iloc[i:(i + time_steps)].values\n        Xs.append(v)\n        ys.append(y.iloc[i + time_steps])\n        #print(Xs[-1], ys[-1])  \n    return np.array(Xs), np.array(ys)","52d7ea21":"test.shape","dd0903e0":"# These next few lines are about getting the data ready for modelling\ntime_steps = 10\n\n# reshape to [samples, time_steps, n_features]\nX_train, y_train = create_dataset(train, train.sales, time_steps)\n\n#X_train_c, y_train_c = create_dataset(X_train_c_a, y_train_c_a, time_steps)\nX_test, y_test = create_dataset(test, test.sales, time_steps)","a2eb297f":"print(X_train.shape, y_train.shape)\n# Note the 3 dimensional shape","d9747d5b":"len(X_test)\n","3b8b93a0":"\ndeep_model = keras.Sequential()\ndeep_model.add(keras.layers.LSTM(\n  units=128,\n  input_shape=(X_train.shape[1], X_train.shape[2])\n))\ndeep_model.add(keras.layers.Dense(units=2))\ndeep_model.add(keras.layers.Dense(units=1))\n\ndeep_model.compile(\n  loss='mse',\n  optimizer=keras.optimizers.Adam(0.001)) # was 0.001","73bfd81d":"history = deep_model.fit(\n    X_train, y_train,\n    epochs=30,\n    batch_size=16,\n    validation_split=0.1,\n    verbose=1,\n    shuffle=False\n)","8a439b76":"print(y_train)","7314ef28":"y_pred = deep_model.predict(X_test)","59d537d6":"print(y_pred)","07b77d13":"X_test.shape","9d81fd70":"print(X_test)","3e266745":"print(y_pred)","63395124":"print(X_test)","5f2dd330":"Results =[]","6c1b9563":"Results = pd.DataFrame( columns=['sales','pred'])","ce2614de":"Results['sales'] = test['sales']","0b6555ef":"Results.head()","e8ddc4f1":"y_pred_df = pd.DataFrame(y_pred, columns=['pred'])","d254abbb":"Results = Results[10:]  # As the Deep Learning process added the forst 10 dates I had to drop the first 10 rows, so the first result was 1 Jan 2017","86a8056b":"y_pred_df.head()","b76a29eb":"Results= Results.reset_index() ","5f692fd0":"Results.head(10)","fbc4c8c5":"Results ['pred'] = y_pred_df['pred']  ","d203a1bf":"Results = Results.set_index('index')","766b4350":"Results =Results.drop (['sales'],axis=1)","35c5f6df":"New_Results = pd.concat([Results, Deep1_all_With_index], axis=1)","ab6d01a4":"New_Results.head() ##### GOOD","e5fba3f8":"New_Results.tail() ##### GOOD","da347dae":"Results_with_date_2017 = New_Results[(New_Results.date>'2016-12-31')]","caa7c5ea":"Results_with_date_2017.head()","f506d703":"RMSE_Deep  = np.mean(np.sqrt((Results_with_date_2017['pred'] - Results_with_date_2017['sales']) ** 2)) \nprint(RMSE_Deep)","d91807ca":"# Note this compares to 4.009 from XG boost for the same data period","5ec968f4":"_ = Results_with_date_2017[['sales','pred']].plot(figsize=(15, 5))","54e1c7f3":"Results_with_date_Jan_2017 =Results_with_date_2017[(New_Results.date<'2017-02-01')]","ad4bfc36":"_ = Results_with_date_Jan_2017[['sales','pred']].plot(figsize=(15, 5))","e9f53ba0":"# Getting the data ready to compare and ensuring can see by actual date rather then an index","bd413bdf":"# Deep Learning for Time Series Store forecast \n* By Alex Dance https:\/\/www.linkedin.com\/in\/alex-dance\/\n* This notebook is one of several notebooks for a project to improve store and product forecasts\n1.\tEDA \u2013 Exploratory Data Analysis \u2013 includes working with annual forecasts\n2.\tMain Modelling\n3.\tXG Boost modelling by Month\n4.\tWeighted average\n5.\tARIMA \u2013 Month and Other Modelling\n6.\tDeep Learning\n","c9e06786":"# Getting Data Ready","96ccfb5b":"# The next few lines are to ensure can match the index to the date so can compare the predictions with an actual date","d20e0a0f":"# Plotting","046ca6e7":"# Now Looking at the data"}}