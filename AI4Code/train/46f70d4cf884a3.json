{"cell_type":{"448c3409":"code","1fe2a786":"code","2ed67b91":"code","fd556511":"code","f47ac838":"code","0b3662c3":"code","5c7e2e01":"code","7c0b18e7":"code","b9f4e9dc":"code","67c831eb":"code","0301d1a8":"code","81acda1c":"code","23427481":"code","30587c97":"code","4d840261":"code","b290bb54":"code","d0a2f742":"code","781be0da":"code","5becaa3a":"code","378df096":"code","1390a9de":"code","6b58d6a6":"code","9ab5424c":"code","ef01f432":"code","fced0afa":"markdown","4f64b82e":"markdown","db31660f":"markdown","48ecfe25":"markdown","0641d13d":"markdown","2538679e":"markdown","810fb7cc":"markdown","324f4067":"markdown"},"source":{"448c3409":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fe2a786":"# importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","2ed67b91":"# Importing data\n# As the data has no header, we set header to None\ndataframe = pd.read_csv(\"..\/input\/mines-vs-rocks\/sonar.all-data.csv\", header=None)\ndataframe.sample(5)","fd556511":"dataframe.shape\n# So we have 208 samples\n# a total of 60 featuers and 1 dependent variable","f47ac838":"# check for nan values ---> There are no null values\ndataframe.isna().sum().sum()","0b3662c3":"dataframe[60].value_counts() # label is not skewed\n# Mines -- 111 samples for mines\n# Rock -- 97 samples for mines","5c7e2e01":"X = dataframe.drop(60, axis=1)\ny = dataframe[60]","7c0b18e7":"y = pd.get_dummies(y, drop_first = True) # one column gets dropped (dummy variable trap)\ny.sample(5) # R --> 1 and M --> 0","b9f4e9dc":"# distributed of dependent categories\ny.value_counts()","67c831eb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y)","0301d1a8":"X_train.shape, X_test.shape","81acda1c":"%%time\n## Without Dropout\n# Defining the architecture of model\nmodel_without_dropout = keras.Sequential([\n    keras.layers.Dense(60, input_dim = 60, activation='relu'), # first dense layer\n    keras.layers.Dense(30, activation = 'relu'), # second dense layer\n    keras.layers.Dense(15, activation = 'relu'), # 3rd dense layer\n    keras.layers.Dense(1, activation = 'sigmoid') # output layer\n])\n\nmodel_without_dropout.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nmodel_without_dropout.fit(X_train, y_train, epochs = 100, batch_size = 8)","23427481":"# Here we can clearly see an accuracy of 1, which might be because of overfitting\nmodel_without_dropout.evaluate(X_train, y_train)","30587c97":"# Evaluating without dropout model on test set\nmodel_without_dropout.evaluate(X_test, y_test)","4d840261":"## prediction some samples\ny_pred = model_without_dropout.predict(X_test).reshape(-1)\nprint(y_pred[:10])\n\n# round the values to nearest integer i.e., 0 or 1 as the output Activation function was sigmoid (so output values are probabilities)\ny_pred = np.round(y_pred)\nprint(y_pred[:10])","b290bb54":"np.array(y_test[:10]).reshape(1,-1)","d0a2f742":"np.sum(np.array(y_test[:10]).reshape(1,-1) == y_pred[:10])\n# So out of 10, 5 correctly classified","781be0da":"# printing the metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, y_pred))","5becaa3a":"%%time\n## With Dropout\n# Defining the architecture of model\nmodel_with_dropout = keras.Sequential([\n    keras.layers.Dense(60, input_dim = 60, activation='relu'), # first dense layer\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(30, activation = 'relu'), # second dense layer\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(15, activation = 'relu'), # 3rd dense layer\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(1, activation = 'sigmoid') # output layer\n])\n\nmodel_with_dropout.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nmodel_with_dropout.fit(X_train, y_train, epochs = 100, batch_size = 8)","378df096":"# evaluating the model on train data\nmodel_with_dropout.evaluate(X_train, y_train)\n\n# the accuracy is not 1, thus less chances of overfitting","1390a9de":"# evaluating the model on test data\nmodel_with_dropout.evaluate(X_test, y_test)","6b58d6a6":"# predicting some samples\ny_pred = model_with_dropout.predict(X_test).reshape(-1)\nprint(y_pred[:10])\n\n# round the values to nearest integer ie 0 or 1\ny_pred = np.round(y_pred)\nprint(y_pred[:10])","9ab5424c":"np.sum(np.array(y_test[:10]).reshape(1,-1) == y_pred[:10])\n# So out of 10, 5 correctly classified","ef01f432":"# printing the metrics after dropout\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","fced0afa":"### Deep Learning (NN) without and with Dropout\n1. Without Dropout\n2. With Dropout","4f64b82e":"#### Segregating Dependent and Independent features","db31660f":"#### Target Varaible Encoding: Converting M,R to 1,0","48ecfe25":"##### Model with dropout","0641d13d":"#### Train-Test Split","2538679e":"#### Dependent Variable data distribution","810fb7cc":"### Deep Learning Solution\n* The aim behind this notebook is to implement and test the effect of Dropout on Neural Network.\n* Dropout in Neural Network is used to overcome the problem of overfitting in NN\n* It is a binary classification problem that requires a model to differentiate rocks from metal cylinders.","324f4067":"#### Checking for missing values"}}