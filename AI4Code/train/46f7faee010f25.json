{"cell_type":{"27b15c75":"code","a9a42db8":"code","8e698f83":"code","d5e8a875":"code","fa965a0d":"code","2b2d471c":"code","8ef05604":"code","ef40a656":"code","9b914376":"code","885d2a9c":"code","98eb5731":"code","81c0696a":"code","a626ff1a":"code","78289f00":"code","343a08f3":"code","77d3d56d":"code","07b6bbb5":"code","6f161a86":"code","2201a3ce":"code","dc9c320f":"code","b4c3fe47":"code","395fd0a6":"code","fd6745eb":"code","549510df":"code","c742f7c0":"code","d64643a8":"code","07a4fe7c":"code","32d6f36d":"code","5cbb2603":"code","2a0d60c8":"code","bc786ac1":"code","171088a0":"code","29c2c1b3":"code","5c77b10e":"code","4d478257":"code","404789da":"code","00443889":"code","099800a5":"code","3de8103a":"code","b4e3eb9e":"code","fa50cb5d":"code","b4a7d361":"code","34dfaaa9":"code","cbb8e49b":"code","6f833c91":"code","ed724e86":"code","e30a9868":"code","92325cb6":"code","3918d090":"code","ed039ec7":"code","152f39cf":"code","78294aa5":"code","0e3746cb":"code","2e2fa73d":"code","2a57eaca":"code","b3fd7df2":"code","89c5b531":"code","e1c051df":"code","e156bd0b":"code","e01d1141":"code","40d05659":"code","f0d7fa5d":"code","b963d6cc":"code","a862bb8b":"code","aaa38c4c":"code","04b06831":"code","7bfba9dc":"code","82126ac1":"code","80eafaaf":"code","31e83f23":"code","54950dea":"code","752e9627":"code","a8a23cc7":"code","517ffc98":"code","4f8759e2":"code","be4d8b0b":"code","7b73208a":"code","1b305413":"code","925e644f":"code","f9fa4b99":"code","c1f5d775":"code","7624050b":"code","927ade1b":"code","5fcd43ce":"code","7aa90e39":"code","417450cf":"code","8c77db44":"code","12461d0d":"code","1d16ebeb":"code","23131126":"markdown","300c339c":"markdown","25da520d":"markdown","c34a5083":"markdown","ae261ee8":"markdown","db5c7cc1":"markdown","e9788bd2":"markdown","1ab5b71c":"markdown","897bad30":"markdown","01457256":"markdown","f8049e4a":"markdown","40dac197":"markdown","c97caaa8":"markdown","1974e144":"markdown","ac090fb7":"markdown","38940614":"markdown","cde539af":"markdown","d5d4b577":"markdown","161bffed":"markdown","b60192c7":"markdown","242669a2":"markdown","d7e09076":"markdown","52ab3cc1":"markdown","816033b4":"markdown","bb07e131":"markdown","b9b0cbe5":"markdown","32224f37":"markdown","94483027":"markdown","486d7486":"markdown","f7953ccd":"markdown","bab5f1f2":"markdown","a8da7064":"markdown","dd513d9f":"markdown","28a37967":"markdown","a9526542":"markdown","5f561afe":"markdown","9343673b":"markdown","275a1117":"markdown","0c653fae":"markdown","8369dc46":"markdown","c90704bb":"markdown","370c64af":"markdown","4e786077":"markdown","493a10ee":"markdown"},"source":{"27b15c75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a9a42db8":"import matplotlib.pyplot as plt\nimport seaborn as sns","8e698f83":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf.head()","d5e8a875":"df = df.drop('Id', axis=1)","fa965a0d":"sns.displot(df['SalePrice'])","2b2d471c":"sns.boxplot(data=df, x='SalePrice')","8ef05604":"df['SalePrice'].describe()","ef40a656":"df.corr()['SalePrice'].sort_values()","9b914376":"sns.scatterplot(x='OverallQual',y='SalePrice',data=df)","885d2a9c":"df[(df['OverallQual']>8) & (df['SalePrice']<200000)]","98eb5731":"sns.scatterplot(x='GrLivArea',y='SalePrice',data=df)","81c0696a":"df[(df['GrLivArea']>4000) & (df['SalePrice']<400000)]","a626ff1a":"drop_indexes = df[(df['GrLivArea']>4000) & (df['SalePrice']<400000)].index","78289f00":"df = df.drop(drop_indexes, axis=0)","343a08f3":"sns.scatterplot(x='OverallQual',y='SalePrice',data=df)","77d3d56d":"sns.scatterplot(x='GrLivArea',y='SalePrice',data=df)","07b6bbb5":"df.info()","6f161a86":"df.isnull().sum()[df.isnull().sum()>0]","2201a3ce":"def percent_missing(df):\n    percent_null = 100* df.isnull().sum() \/ len(df)\n    percent_null = percent_null[percent_null>0].sort_values()\n    return percent_null","dc9c320f":"percent_null = percent_missing(df)\npercent_null","b4c3fe47":"sns.barplot(x=percent_null.index,y=percent_null)\nplt.xticks(rotation=90);","395fd0a6":"large_missing_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\ndf[large_missing_cols] = df[large_missing_cols].fillna('None')","fd6745eb":"percent_null = percent_missing(df)\npercent_null","549510df":"df[df['Electrical'].isnull()]","c742f7c0":"df = df.drop(1379,axis=0)","d64643a8":"df[df['MasVnrType'].isnull()]","07a4fe7c":"df[df['MasVnrArea'].isnull()]","32d6f36d":"df[df['MasVnrType'] == 'None']","5cbb2603":"df['MasVnrType'] = df['MasVnrType'].fillna(\"None\")\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(0)","2a0d60c8":"percent_null = percent_missing(df)\npercent_null","bc786ac1":"df[df['BsmtQual'].isnull()]","171088a0":"df[df['BsmtCond'].isnull()]","29c2c1b3":"df[df['BsmtFinType1'].isnull()]","5c77b10e":"df[df['BsmtExposure'].isnull()]","4d478257":"df[df['BsmtExposure'].isnull()][df['BsmtFinType1'].notna()]","404789da":"df[df['BsmtFinType2'].isnull()]","00443889":"df[df['BsmtFinType2'].isnull()][df['BsmtFinType1'].notna()]","099800a5":"df.loc[948, ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']]","3de8103a":"df.loc[332, ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']]","b4e3eb9e":"sns.displot(data=df, x='TotalBsmtSF', kind='hist')","fa50cb5d":"df = df.drop([332, 948], axis=0)","b4a7d361":"bsmt_cat_columns = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\ndf[bsmt_cat_columns] = df[bsmt_cat_columns].fillna(\"None\")","34dfaaa9":"percent_null = percent_missing(df)\npercent_null","cbb8e49b":"df[df['GarageType'].isna()].index == df[df['GarageYrBlt'].isna()].index","6f833c91":"df[df['GarageType'].isna()].index == df[df['GarageFinish'].isna()].index","ed724e86":"df[df['GarageType'].isna()].index == df[df['GarageQual'].isna()].index","e30a9868":"df[df['GarageType'].isna()].index == df[df['GarageCond'].isna()].index","92325cb6":"df[df['GarageArea'] == 0].index == df[df['GarageFinish'].isna()].index","3918d090":"garage_cat_features = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndf[garage_cat_features] = df[garage_cat_features].fillna(\"None\")","ed039ec7":"df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])\ndf['GarageYrBlt'] = df['GarageYrBlt'].astype('int64')","152f39cf":"def has_garage_label(row):\n    if row['GarageType'] == \"None\":\n        return \"no\"\n    else:\n        return \"yes\"\n    \ndf['HasGarage'] = df.apply(has_garage_label, axis = 1)\ndf['HasGarage'].value_counts()","78294aa5":"percent_null = percent_missing(df)\npercent_null","0e3746cb":"sns.displot(data=df, x='LotFrontage')","2e2fa73d":"df['LotFrontage'].describe()","2a57eaca":"df[['SalePrice', 'LotFrontage']].corr()","b3fd7df2":"X = df[['LotFrontage', 'LotArea', 'LotConfig', 'Neighborhood']].copy()\nX = X.dropna(axis=0)\ny = X.pop('LotFrontage')\nX['LotArea'] = X['LotArea'].astype('float64')\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\nfrontage_discrete_features = X.dtypes == int","89c5b531":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","e1c051df":"mi_scores = make_mi_scores(X, y, frontage_discrete_features)\nmi_scores","e156bd0b":"df[df['LotFrontage'].isna()]['LotArea'].describe()","e01d1141":"fig, axes = plt.subplots(figsize=(10,10))\n\nsns.histplot(ax=axes, data=df[df['LotFrontage'].isna()]['LotArea'])\nsns.histplot(ax=axes, data=df[df['LotFrontage'].notna()]['LotArea'], color='r', alpha=0.5)","40d05659":"fig, axes = plt.subplots(figsize=(10,10))\n\nsns.histplot(ax=axes, data=df[(df['LotFrontage'].isna()) & (df['LotArea'] > 20000)]['LotArea'])\nsns.histplot(ax=axes, data=df[(df['LotFrontage'].notna()) & (df['LotArea'] > 20000)]['LotArea'], color='r', alpha=0.5)","f0d7fa5d":"fig, axes = plt.subplots(2, 2, figsize=(10,10))\n\nsns.histplot(ax=axes[0][0], data=df[(df['LotFrontage'].isna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=500)\nsns.histplot(ax=axes[0][0], data=df[(df['LotFrontage'].notna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=500, color='r', alpha=0.5)\n\nsns.histplot(ax=axes[0][1], data=df[(df['LotFrontage'].isna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=1000)\nsns.histplot(ax=axes[0][1], data=df[(df['LotFrontage'].notna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=1000, color='r', alpha=0.5)\n\nsns.histplot(ax=axes[1][0], data=df[(df['LotFrontage'].isna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=2500)\nsns.histplot(ax=axes[1][0], data=df[(df['LotFrontage'].notna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=2500, color='r', alpha=0.5)\n\nsns.histplot(ax=axes[1][1], data=df[(df['LotFrontage'].isna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=5000)\nsns.histplot(ax=axes[1][1], data=df[(df['LotFrontage'].notna()) & (df['LotArea'] > 20000)]['LotArea'], binwidth=5000, color='r', alpha=0.5)\n\naxes[0][0].set_title('bin width = 500')\naxes[0][1].set_title('bin width = 1000')\naxes[1][0].set_title('bin width = 2500')\naxes[1][1].set_title('bin width = 5000')","b963d6cc":"fig, axes = plt.subplots(figsize=(10,10))\n\nsns.histplot(ax=axes, data=df[df['LotFrontage'].isna()]['LotArea'], binwidth=2500)\nsns.histplot(ax=axes, data=df[df['LotFrontage'].notna()]['LotArea'], binwidth=2500, color='r', alpha=0.5)","a862bb8b":"# Selecting the cluster near 50000 without a bin when bin width set at 2500\ndf[['LotArea', 'LotFrontage']][(df['LotArea'] > 50000) & (df['LotArea'] < 100000) & (df['LotFrontage'].isna())]","aaa38c4c":"# Selecting houses we have LotFrontage values for within 2 bin widths of 50000\ndf[['LotArea', 'LotFrontage']][(df['LotArea'] > 45000) & (df['LotArea'] < 55000) & (df['LotFrontage'].notna())]","04b06831":"# Calculating the mean LotFrontage of the houses we have LotFrontage values for within 2 bin widths of 50000\ndf[['LotArea', 'LotFrontage']][(df['LotArea'] > 45000) & (df['LotArea'] < 55000) & (df['LotFrontage'].notna())].mean()","7bfba9dc":"# Selecting cluster >100000 without a bin when bin width set to 2500\ndf[['LotArea', 'LotFrontage']][(df['LotArea'] > 100000) & (df['LotFrontage'].isna())]","82126ac1":"# Selecting the two largest houses we have LotFrontage values for\ndf[['LotArea', 'LotFrontage']][(df['LotArea'] > 60000) & (df['LotFrontage'].notna())]","80eafaaf":"# Calculating mean\/median LotFrontage of the two largest houses we have LotFrontage values for\ndf[['LotArea', 'LotFrontage']][(df['LotArea'] > 60000) & (df['LotFrontage'].notna())].mean()","31e83f23":"# Create bins up to 45000 at regular intervals of 2500\nbins = [i for i in range(0, 45001, 2500)]\n\n# Add on the 45000 to 60000 bin and the 55000 to 217500 bin\nbins += [60000, 217500]\ndf['LotAreaBins'] = pd.cut(df['LotArea'], bins)\n\n# Verify bin assignments\ndf[['LotArea', 'LotAreaBins']].head(20)","54950dea":"# Fills in missing LotFrontage values with respective bins' means\ndf['LotFrontage'] = df.groupby('LotAreaBins')['LotFrontage'].transform(lambda val: val.fillna(val.mean()))","752e9627":"df.loc[[384, 457, 1396, 249, 335, 706], 'LotFrontage']","a8a23cc7":"df[df['LotFrontage'].isna()]","517ffc98":"df.drop('LotAreaBins', axis = 1, inplace = True)","4f8759e2":"df.info()","be4d8b0b":"# LotShape\nlot_shape_order = [\"IR3\", \"IR2\", \"IR1\", \"Reg\"]\n\n# LandSlope\nland_slope_order = [\"Sev\", \"Mod\", \"Gtl\"]\n\n# ExterQual\nexter_qual_order = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# ExterCond\nexter_cond_order = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# BsmtQual \nbsmt_qual_order = [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# BsmtCond \nbsmt_cond_order = [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# BsmtExposure \nbsmt_exposure_order = [\"None\", \"No\", \"Mn\", \"Av\", \"Gd\"]\n\n# BsmtFinType1 \nbsmt_fin_type1_order = [\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"]\n\n# BsmtFinType2 \nbsmt_fin_type2_order = [\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"]\n\n# HeatingQC \nheating_qc_order = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# KitchenQual \nkitchen_qual_order = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# Functional \nfunctional_order = [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"]\n\n# FireplaceQu \nfireplace_qu_order = [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# GarageFinish \ngarage_finish_order = [\"None\", \"Unf\", \"RFn\", \"Fin\"]\n\n# GarageQual \ngarage_qual_order = [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# GarageCond \ngarage_cond_order = [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# PavedDrive \npaved_drive_order = [\"N\", \"P\", \"Y\"]\n\n# PoolQC \npool_qc_order = [\"None\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n\n# Fence\nfence_order = [\"None\", \"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"]","7b73208a":"col_names_ordinal_encode = [\"LotShape\", \"LandSlope\", \"ExterQual\", \"ExterCond\", \"BsmtQual\", \n                            \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \n                            \"HeatingQC\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \n                            \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PavedDrive\", \n                            \"PoolQC\", \"Fence\"]\n\nordinal_encode_order_lists = [lot_shape_order, land_slope_order, exter_qual_order, exter_cond_order, bsmt_qual_order,\n                             bsmt_cond_order, bsmt_exposure_order, bsmt_fin_type1_order, bsmt_fin_type2_order,\n                             heating_qc_order, kitchen_qual_order, functional_order, fireplace_qu_order,\n                             garage_finish_order, garage_qual_order, garage_cond_order, paved_drive_order,\n                             pool_qc_order, fence_order]\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfor i, colname in enumerate(col_names_ordinal_encode):\n    oe = OrdinalEncoder(categories = [ordinal_encode_order_lists[i]])\n    encoded_array = oe.fit_transform(df[[colname]])\n    df[[colname]] = encoded_array","1b305413":"df['MSSubClass'] = df['MSSubClass'].astype('str', copy = False)","925e644f":"from sklearn.model_selection import train_test_split\n\nX = df.drop('SalePrice', axis = 1)\ny = df['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10)","f9fa4b99":"# Preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Models\nfrom sklearn.linear_model import LinearRegression \nfrom xgboost import XGBRegressor \nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Evaluation\nfrom sklearn.metrics import mean_squared_error","c1f5d775":"numerical_columns = X.select_dtypes(exclude = ['object']).columns.to_list()\nobject_columns = X.select_dtypes(include = ['object']).columns.to_list()","7624050b":"numeric_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'NA')),\n    ('encoder', OneHotEncoder(handle_unknown = 'ignore'))\n])","927ade1b":"preprocessor = ColumnTransformer(transformers = [\n    ('num', numeric_transformer, numerical_columns),\n    ('cat', categorical_transformer, object_columns)\n])","5fcd43ce":"pipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])","7aa90e39":"param_grid = [\n    {\n        \"preprocessor__num__scaler\":[\n            StandardScaler(),\n            MinMaxScaler()\n        ],\n        \"regressor\": [\n            LinearRegression(),\n            KNeighborsRegressor(),\n            DecisionTreeRegressor(),\n            RandomForestRegressor(),\n            XGBRegressor()\n        ]\n    },\n#     {\n#         \"regressor\": [RandomForestRegressor(random_state = 42)],\n#         \"regressor__n_estimators\": [100, 300, 500, 1000],\n#         \"regressor__max_depth\": [3, 5, 8, 15],\n#         \"regressor__max_features\": [\"log2\", \"sqrt\", \"auto\"],\n#      },\n#      {\n#          \"regressor\": [XGBRegressor(random_state = 42)],\n#          \"regressor__max_depth\": [3, 5, 8, 15],\n#          \"regressor__learning_rate\": [0.1, 0.01, 0.05],\n#          \"regressor__gamma\": [0, 0.25, 1.0],\n#          \"regressor__lambda\": [0, 1.0, 10.0],\n#      },\n]","417450cf":"grid = GridSearchCV(pipe, param_grid = param_grid, scoring = \"neg_mean_squared_error\", cv = 5)\ngrid.fit(X_train, y_train)","8c77db44":"grid.best_params_","12461d0d":"grid.best_score_","1d16ebeb":"y_pred = grid.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(rmse)","23131126":"OverallQual and GrLivArea are strongly correlated with our target SalePrice","300c339c":"**A note on data leakage**\n\nA lot is written about the importance of creating holdout test sets to prevent data leakage. This project serves as a personal example of how easily it can be to forget to maintain best practice; I did not realize until I got to this point in the notebook that all the preprocessing that I have done would be invalid under a real-use case.\n\nI came to this realization after getting to the point where I was about to build the pipeline, noticed that a lot of examples include an imputer, and began to wonder whether under this project's current circumstance, I should include the step in my pipeline. I initially thought it unnecessary having so painstakingly addressed the problem of missing values, but the thought occurred to me: given how many null values came with this data set, if I was to get new data from the same source, I would undoubtedly have issues.\n\nThis thought then triggered questions about how to handle missing data in a test set assuming I had not done any preprocessing, which lead to realizing: \n\n* I relied on a careful examination of all the given data (especially for `LotFrontage`),\n* There was a chance that had I held out data at the outset, outliers could have been in the holdout set,\n* And had this been the case, the values I based my preprocessing on could have been very different (e.g., `LotFrontage` bins).\n\n**Beyond data leakage**\n\nThe mistake I made has enormous consequences for this project: in order to rectify it, I actually would have to return to the beginning and essentially start over and perform the following:\n\n* Before looking at any data, split the data into training and holdout sets\n* Identify the continous and categorical variables\n* Using only the training set:\n    * EDA\n    * Develop plans for outliers\/leverage points\n    * Develop plans for how to impute missing values\n    * Execute plan for outliers\/leverage points\n    * Execute imputation\n    * Perform ordinal and one-hot encoding\n\nIf I did that, I would be where I am now, but with properly partitioned data. Given the current state of affairs, I plan for the rest of this exercise to be about learning to build a complex pipeline that includes grid search cross-validation, then if I feel up to it, seeing if I can use the pipeline to rectify my mistakes.","25da520d":"Executing the grid search itself:","c34a5083":"As suspected, the LotArea has the highest MI score so imputing based off LotArea would probably yield the best results. Since LotArea is a continuous feature, it's very unlikely we would have an exact matching LotArea for every house missing a LotFrontage value, and even if we did, imputing purely on 1 or 2 values wouldn't generalize well. To remedy this, we will need to bin LotArea and use a measure of centrality (i.e., mean, median, or mode) of the appropriate bin as the imputed value.\n\nTo proceed, we need to pick a bin size, which I admittedly, do not have a solid methodology for. Briefly thinking through the process, bin sizes that are small suffer from high variance, the same issue as if there was just 1 corresponding LotArea for each missing LotFrontage value, while bin sizes that are large cause the imputed values to be closer to using measures of central tendancy of the entire data set and impart higher bias on the resulting model.\n\nWe can start by comparing distributions of the LotArea of the houses missing LotFrontage values with those that have them to get a sense of where we have LotArea overlap.","ae261ee8":"We'll plan to impute a LotFrontage value of 106.0 for indexes 249, 335, and 706.","db5c7cc1":"Creating separate pipelines for numerical and categorical transformations:","e9788bd2":"Since this data set consists of homes, there should be a LotFrontage value for every row. Examining the data also shows no rows with LotFrontage values of 0 suggesting other types of dwellings (e.g., apartments) in the data set. LotFrontage also demonstrates explanatory value of SalePrice. Given a significant loss of data if we simply dropped all the rows, but not so much missing data (usually more than 20-30% of the column) that we should drop the column entirely, we need to impute values.\n\nThere are several approaches we can take to doing this, but all of them ultimately require some subjective element. The simplest values to impute would be the mean or median of the whole data set, which happen to be fairly close. A slightly more informed way we could determine values is based on other houses with similar features.\n\nLooking through the other features we have available to us, the features that I think might tell us about a house's LotFrontage are LotArea, LotConfig, and Neighborhood. We can compare each feature's mutual information score with LotFrontage and determine which features tell us the most about the variable, then impute based on the results. More about mutual information as well as the 3 cells below can be found at: https:\/\/www.kaggle.com\/ryanholbrook\/mutual-information","1ab5b71c":"We drop `LotAreaBins` before continuing as the column was created for imputation purposes, not as a feature for prediction.","897bad30":"### Handling missing values","01457256":"Creating lists of numerical and categorical variables, which need to be separated due to different preprocessing steps:","f8049e4a":"We can account for these varying bin sizes at the upper extreme of our LotArea values in the list we give as a parameter to the pd.cut method.","40dac197":"We also notice some houses priced much higher than what we see in the rest of our distribution. Our model might struggle to accurately predict houses with very high prices.","c97caaa8":"We are now left with LotFrontage. Unfortunately, this variable is the most difficult to deal with in terms of missing values. If we just dropped all rows with missing values, we would lose 17.8% of our data. Let's explore the LotFrontage variable a bit more.","1974e144":"Comparing OverallQual and SalePrice, we see two houses that sold for much less than expected from their quality rating.","ac090fb7":"Combining the numeric and categorical transformer pipelines into a single `ColumnTransformer` object:","38940614":"Starting with ordinal encoding:\n\n`OverallQual` and `OverallCond` are currently type `int64` and are actually categorical, ordinal. These columns can be left alone as they came properly encoded per the data dictionary.\n\nHowever, the following variables are categorical, ordinal, but have not been properly encoded:\n`LotShape`\n`LandSlope`\n`ExterQual`\n`ExterCond`\n`BsmtQual`\n`BsmtCond`\n`BsmtExposure`\n`BsmtFinType1`\n`BsmtFinType2`\n`HeatingQC`\n`KitchenQual`\n`Functional`\n`FireplaceQu`\n`GarageFinish`\n`GarageQual`\n`GarageCond`\n`PavedDrive`\n`PoolQC`\n`Fence`\n\nWe need to create lists indidcating the ordinal ranking for each of these variables based on the descriptions provided with the data.","cde539af":"### Preprocessing","d5d4b577":"Double-checking our large LotArea houses with missing LotFrontage values, we imputed the values we expected and we now have a data set with no missing values.","161bffed":"One house has missing data for the Eletrical feature. We will drop this house from our training data.","b60192c7":"All remaining variables with `object` data type are categorical, nominal variables and should be one-hot encoded. This will be handled via a pipeline for sake of ease.","242669a2":"Continuing with one-hot encoding:\n\n`MSSubClass` is currently type `int64` but is actually categorical, nominal.\n\nHandling this variable requires one-hot encoding. We can convert this variable to a string type and let the encoder handle this with all the other categorical variables.","d7e09076":"Reviewing the houses where LotArea >20000 shows there is indeed a gap. Testing bin widths of 500, 1000, 2500, and even 5000 still leaves some houses missing LotFrontage values without a bin at the extreme, but 2500 looks to offer a relatively good compromise. When LotArea exceeds 50000, I plan to essentially create a non-uniform bin using the group clustered around 50000. We happen to have two houses with known LotFrontage values bracketing the 3 extremely large homes missing LotFrontage values. I think a reasonable approach would be to use impute these 3 homes with the mean\/median (they'll be equal) LotFrontage value of the two largest homes we have values for.","52ab3cc1":"We can see that all the Garage family of features are missing values at exactly the same indexes and the GarageArea is 0 at all those indexes.","816033b4":"It could be argued that the houses that sold for more than $700,000 might be worth removing from the training data as well, but they at least follow the trend dictated by GrLivArea and OverallQual.","bb07e131":"I imagine if I had a role in this industry, it would be possible to contact someone or find some way to actually get the missing data since it actually exists.\n\nDropping index 948 probably would have little effect on the model given its basement size is well represented. Dropping index 332 could actually help our model be more generalizable since its basement square footage is an outlier in our data set.\n\nDropping 2 more houses would leave us with a total of 5 dropped houses out of the original 1460 records, which is relatively insignificant. These 2 houses are examples of missing values that we could, in the real world, find some way to obtain.\n\nTo proceed, we will drop 332 and 948. We will then fill out the other Bsmt features with \"None\" as all are categorical features.","b9b0cbe5":"Creating the modeling pipeline:","32224f37":"The MasVnrType and MasVnrArea are missing the same amount of data. The data dictionary does indicate that houses without masonry veneer were supposed to be marked as \"None\" rather than NA so there is some ambiguity as to whether the missing data should be \"None\" and 0 respectively or is in fact some other value that was not properly recorded. Let's examine the houses.","94483027":"`YearBuilt` and `YearRemodAdd`, being time values, deserve some special consideration. They could be considered categorical  or numerical values depending on the analysis being done. In this case, both are acting as surrogates for age and therefore can be left as numerical values.","486d7486":"After creating the list of ordered values, we can pass these ordered lists and their corresponding column names to an OrdinalEncoder object, perform the transformation, and replace the column values with the resulting encoded array.","f7953ccd":"The Bsmt and Garage family of features appears to follow similar characteristics as the MasVnr family of features. There is some variation in the Bsmt family but if we look back at the counts, the slight increase in BsmtExposure and BsmtFinType2 missing data is due to 1 house each.","bab5f1f2":"Querying each column independently confirms that it's the same 8 houses missing both feature values. I have no one I can ask about whether the MasVnrArea number is auto-generated from the lack of a MasVnrType, but I would guess that the houses missing these values would be different if mistakes were made while recording. Many houses in this data set also do not have masonry veneer. I think the best course of action is to impute this data with \"None\" and 0 values.","a8da7064":"### Handling outliers","dd513d9f":"Based on the data description, it's likely the missing Eletrical data is actually missing. We will further investigate, but dropping the records without the Electrical won't cost us very much and will be the likely result.","28a37967":"We next need to identify the variables that require ordinal encoding and the variables that require one-hot encoding. ","a9526542":"We'll plan to impute LotFrontage value of 60.5 for indexes 384, 457, and 1396.","5f561afe":"Import block:","9343673b":"According to the data description, the null values for PoolQC, MiscFeature, Alley, Fence, and FireplaceQu were encoded as NA to indicate non-existence. We can easily remedy this by replacing null values with \"None\" as all the aforementioned features are qualitative.","275a1117":"There are 19 columns with missing values. How many missing values are we missing relative to the number of rows in our data set?","0c653fae":"For the bulk of the distributions there is significant overlap. Gaps start to appear somewhere around LotArea > 25000. We can look closer at this region.","8369dc46":"These two houses again appear to have sold for much less than expected from their GrLivArea. These two houses are outliers worth removing.","c90704bb":"Creating the parameter grid for grid searching:","370c64af":"### Building the Pipeline and Modeling","4e786077":"For 37 of the houses all the Bsmt family of features are missing. Per the data dictionary, these NA values indicate \"None\" values. However, there are 2 houses that look like they are truly missing values in the Bsmt family of features. Index 948 is missing BsmtExposure and 332 is missing BsmtFinType2. Let's further investigate.","493a10ee":"`GarageYrBlt` deserves some special consideration because of it being both a time value and because its missing values are dependent on the existence of a garage.\n\nThere are several approaches to handling this issue, but the goal is to account for the age of the garage and the feature of garage existence. The approach I am choosing to take is to set the missing `GarageYrBlt` values to the same year as the when the house was built, but also create another variable that categorizes whether the garage exists.\n\nThis solution avoids setting `GarageYrBlt` to 0 for non-existent garages and making them appear over 2000 years old. While this approach leads to imputation that will affect the resulting regression coefficient, it will effectively result in a different intercept for the houses without garages, offsetting the effect of the imputed `GarageYrBlt` values."}}