{"cell_type":{"577b8fa3":"code","0cdb6264":"code","081a4827":"code","34d1d33e":"code","d95a64c2":"code","46045ff3":"code","99dd28f9":"code","4f6804f2":"code","c847a659":"code","5ab4eae6":"code","08f27bb1":"code","370124a7":"code","c437bd39":"code","4619ac70":"code","6434a5f1":"code","0a982caf":"code","7285e635":"code","e8a68b82":"code","b2824b5a":"code","98429668":"code","f5366bdd":"code","09763bf0":"code","132dea78":"code","6e76201c":"code","24c0fc68":"code","fddbad67":"code","69ed897b":"code","88e3c199":"code","acab7f59":"code","11710a1e":"code","e7e20fdf":"code","e624263d":"code","a2f1c835":"markdown","c9812e10":"markdown","c4074b58":"markdown","96203d8b":"markdown","86ae0371":"markdown","3e21b955":"markdown","fb3c97c3":"markdown","7e2878e2":"markdown","7d14587f":"markdown","693229cb":"markdown","a5d2260f":"markdown","1ba6b86d":"markdown","3fbb7995":"markdown","d68e5b40":"markdown","eb3de16a":"markdown","0c37ebb7":"markdown","4915867d":"markdown","08857fcb":"markdown","cb47b665":"markdown","29b1bb59":"markdown","bede349f":"markdown"},"source":{"577b8fa3":"# import required Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import datasets                             # used to load embeded iris Dataset\nfrom sklearn.model_selection import train_test_split     # used to split data\n\nfrom sklearn.neighbors import KNeighborsClassifier       # used to get the classifier Algorithm\nfrom sklearn.preprocessing import Normalizer             # used to Normalize the data to make it easy to calssify the data\n\nfrom collections import Counter                          # Counter used in collections to get number of repeated target ","0cdb6264":"iris = datasets.load_iris()        # load iris dataset","081a4827":"iris.keys()                        # iris dataset ingriedents are shown in .keys()","34d1d33e":"iris.target_names  \n# [0] for 'setosa' , [1] for 'versicolor' , [2] for 'virginica'","d95a64c2":"iris.data ","46045ff3":"iris.target","99dd28f9":"# Concatenate the data('features') and the target and put them in DataFrame\ndf = pd.DataFrame(np.concatenate([iris['data'], iris['target'].reshape(-1,1)], axis=1),\n                 columns=iris['feature_names']+['target'])","4f6804f2":"df.head(2)","c847a659":"# Other Method to concatenate the data('features') and the target \ndf = pd.DataFrame(np.c_[iris['data'], iris['target']],\n                 columns=iris['feature_names']+['target'])","5ab4eae6":"df.head(2)","08f27bb1":"# Slice the DatFrame to get Features only \n# by dropping the target or by (iloc)\nX = df.drop('target', axis=1)\n#or\nX = df.iloc[:,:-1]\n\nX.head(2)","370124a7":"# Slice the DatFrame to get target or by (iloc)\ny = df['target']\n# or\ny = df.iloc[:,-1]\n\ny.head(2) ","c437bd39":"X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.2,\n                                                    random_state=0, \n                                                    shuffle=True)","4619ac70":"# transform the outs to arrays\nX_train = np.asarray(X_train)      \nX_test = np.asarray(X_test)       \n\ny_train = np.asarray(y_train)    \ny_test = np.asarray(y_test)      ","6434a5f1":"# Normalize the Dataset on the training and testing Features \n# only not for training or testing target\nmy_Normalizer = Normalizer().fit(X_train)\n\nnormalized_X_train = my_Normalizer.transform(X_train)\nnormalized_X_test = my_Normalizer.transform(X_test)","0a982caf":"normalized_X_train","7285e635":"df_normalized = pd.DataFrame(np.concatenate([normalized_X_train, y_train.reshape(-1,1)], axis=1),\n                       columns=iris['feature_names']+['target'])","e8a68b82":"df_normalized.head(3)","b2824b5a":"names = {0:'setosa', 1:'versicolor', 2:'virginica'} \ndf_normalized = df_normalized.replace({'target':names}) \ndf = df.replace({'target':names})\n\ndf_normalized.head()","98429668":"# Before Normalization\nbefore = sns.pairplot(df, hue='target')\nbefore.fig.suptitle('Before making Normalization', y=1.05)\nplt.show()","f5366bdd":"# After Normalization\nafter = sns.pairplot(data=df_normalized, hue='target')\nafter.fig.suptitle('After making Normalization', y=1.05)\nplt.show()","09763bf0":"def euclidean_distance(x_train, x_test_point):\n    '''\n    Args:\n    ----\n        x_train(array:DataFrame) : corresponding to the training Data\n        x_test_point(array:one Data Point as array) : corresponding the test Point (only one Point)\n        \n    Returns:\n    -------\n        return the Euclidean Distances between the only one tested Point and \n        all other Points in the training Data\n    '''\n    \n    distances = []\n    \n     # Loop over the rows of training Data \n    for row in range(len(x_train)):               \n       \n        distance_each = 0                      \n        for col in range(len(x_train[row])):             \n             # subtract each feature of training data with the tested point\n            distance_each += (x_train[row][col] - x_test_point[col])**2    \n            \n        distance_each = np.sqrt(distance_each)     \n        distances.append(distance_each)             \n        \n    distances_df = pd.DataFrame(distances, columns=['Distances'])\n        \n    return distances_df","132dea78":"def k_nearest_neighbors(distances_points, k):\n    '''\n    Args:\n    -----\n        distances_points(array:DataFrame) : corresponding to \n                                the Distances between tested Point and other Points\n        k (int)                 : corresponding to the number of required neighbours \n        \n    Returns:\n    -------\n        the nearest neighbours of required K , if K=3 return only the smallest 3 distances\n    '''\n    neighbours_df = distances_points.sort_values(by=['Distances'], axis=0)    \n    neighbours_df = neighbours_df[:k]                                            \n    return neighbours_df","6e76201c":"def most_counted(neighbours_df, y_train):\n    '''\n    Args:\n    ----\n        nearest_df(array:DataFrame) : corresponding to the nearest values in selected K\n        y_train                     : corresponding to the target of training data\n        \n    Returns:\n    --------\n        the major Vote (most common Prediction) [0 or 1 or 2]\n    '''\n    # count the number of repeated target in the sorted sliced DataFrame, \n    # Note: get its correspond to y_train\n    my_counter = Counter(y_train[neighbours_df.index])  \n    \n    most_counted_out = my_counter.most_common()[0][0]       \n    \n    return most_counted_out","24c0fc68":"def main_method(x_train, y_train, x_test, k):\n    '''\n    Args:\n    -----\n        x_train(array:Dataframe) : corresponding to the training features\n        y_train(array:Dataframe) : corresponding to the training target\n        x_test(array:Dataframe)  : corresponding to the testing features\n        k(int)                   : number of nearest neighbours\n        \n    Returns:\n    -------\n        the final output of the Predicted target [0 or 1 or 2] for the given (x_test) argument\n    '''\n    \n    result_TotalPoints = []\n    \n    # I have an array of testing data not only one point so , loop\n    for x_each_test_point  in x_test:\n        \n        # for each testing_point in the whole x_test , call the euclidean_distance function\n        distance_eachTestPoint = euclidean_distance(x_train, x_each_test_point)\n        \n        # the previous output is the input of the second function k_nearest_neighbors , call it to sort and slice\n        nearest_eachTestPoint = k_nearest_neighbors(distance_eachTestPoint, k)\n        \n        # the previous output is the input of the third function most_counted , call it and get the most repeated target\n        result_eachTestPoint = most_counted(nearest_eachTestPoint, y_train)\n        \n        # for each testing_point , now you have the predicted values , appened it to the list , and so on\n        result_TotalPoints.append(result_eachTestPoint)\n        \n    return result_TotalPoints\n        ","fddbad67":"# Call the main_method function and get the total results by KNN manually\nresult_manually = main_method(normalized_X_train, y_train, normalized_X_test, k=3)","69ed897b":"# this is the result of the testing_points by applying the KNN manually\nprint(result_manually)","88e3c199":"# Define the Model\nmodel = KNeighborsClassifier(n_neighbors=3)","acab7f59":"# Fit the Model \nmodel.fit(normalized_X_train, y_train)","11710a1e":"# Predict the model Results\nmodel_pred = model.predict(normalized_X_test)","e7e20fdf":"print(model_pred)","e624263d":"print(np.array_equal(result_manually, model_pred))","a2f1c835":"### Check this Link for (Normalization and Scaling)\nhttps:\/\/www.kaggle.com\/alexisbcook\/scaling-and-normalization","c9812e10":"### Check this Link for Counter in collections:","c4074b58":"![KNN-Algorithm-k3-edureka-437x300.png](attachment:KNN-Algorithm-k3-edureka-437x300.png)\n","96203d8b":"![KNN_final_a1mrv9.png](attachment:KNN_final_a1mrv9.png)","86ae0371":"* ## How to implement KNN Manually\n    * Get the distances between all the training Data\n    * Find the nearest K neighbours by sorting these distances\n    * Classify the point based on the majority vote","3e21b955":"![t.png](attachment:t.png)","fb3c97c3":"----------------------------------","7e2878e2":"![qu.png](attachment:qu.png)","7d14587f":"### Explore the Data","693229cb":"-------------------","a5d2260f":"![euc.png](attachment:euc.png)","1ba6b86d":"### \tSection Objectives:\n \n* Explain iris Dataset and make (Exploratory Data Analysis - EDA) on it.\n* Explain what are the benefits from apply Normalization on my Data.\n* What is KNN and why we use it.\n* Implement KNN from scratch using Manual Methods to reach the Solution.\n* Compare the Results with scikit-learn.\n* Discuss the choice of Distance (euclidean vs. manhattan)\n--------------------------------------------------------------------","3fbb7995":"### Implement KNN manually","d68e5b40":"### -------------------------------------------------- Questions ------------------------------------------------------------","eb3de16a":"--------------------------------------------------------------","0c37ebb7":"### Visualize the Data before and after Normalization","4915867d":"+ ## This Sheet provides some Notes on K-Nearest Neighbours (KNN)","08857fcb":"### Implement KNN by sklearn\n* Define the Model\n* Fit the Model\n* Predict the Model","cb47b665":"### What is KNN & How it is works","29b1bb59":"#### Check if both results is True","bede349f":"### EDA - Exploratory Data Analysis on iris Dataset"}}