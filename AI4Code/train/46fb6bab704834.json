{"cell_type":{"49888638":"code","f9abeb4d":"code","2b2e9133":"code","06a7b167":"code","8e399cf2":"code","4f80fb41":"code","f2352a96":"code","be470512":"code","04b55cb0":"code","bced960f":"code","a678bceb":"code","ddc116d4":"code","b6ef1433":"code","681d729a":"code","5334f3f8":"code","fb8d5ddc":"code","d5022cf1":"code","d7c9affa":"code","250a1be6":"code","750d1b73":"code","4bc172db":"code","cfd9fefb":"code","1155d161":"code","7a120123":"code","43739cce":"code","6c838c59":"code","cd010f21":"code","f07b0c2a":"code","c403af49":"code","9ee898dd":"code","bd710169":"code","f186ee00":"code","f2344e3a":"code","fd2fbc36":"code","15bfacae":"code","2c30a904":"code","b6c7abf5":"code","7783e438":"code","b0c45276":"code","1d691e65":"code","93823ffb":"code","3cb9f8ed":"code","233b0c61":"code","4f9bcceb":"code","5931016b":"code","77a4d93b":"code","c6a492f8":"code","9a46bdf4":"code","ceda74df":"code","e816e5c0":"code","4852239d":"code","182a6dfb":"code","a5e22e1c":"code","e3784eaf":"code","fb16ba6d":"code","46399bef":"code","7d372fd2":"code","0d803420":"code","ca91d448":"code","4c8847dc":"code","49273986":"code","6a6683fa":"code","c4fa77b9":"markdown","2ccb24ec":"markdown","0e9a02d1":"markdown","0f394b53":"markdown","34cd6283":"markdown","add34be4":"markdown","63d77859":"markdown","021c8041":"markdown","95df4f19":"markdown","86ba82d2":"markdown","0e4dd3b5":"markdown","359630c2":"markdown","5c30bc8c":"markdown","dcf56942":"markdown","e1d1e5d7":"markdown","beeef848":"markdown","0d3b7df5":"markdown","73d5e15b":"markdown","23d01e7f":"markdown","a914a652":"markdown","c37f7414":"markdown","f349e6ad":"markdown","3ea34461":"markdown","bfb08149":"markdown","82368f28":"markdown","40f54b14":"markdown","83ddece6":"markdown","ffdde02a":"markdown","41767074":"markdown","232237a0":"markdown","9c402876":"markdown","651731f0":"markdown","f1c38e8e":"markdown","7896d67c":"markdown","158b4ab0":"markdown","c4f74518":"markdown","c0ee007a":"markdown","131ddcd4":"markdown","20007b46":"markdown","c6dc4f22":"markdown","316c707b":"markdown","98648f17":"markdown"},"source":{"49888638":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom warnings import filterwarnings\nfrom mpl_toolkits.mplot3d import Axes3D\nimport statsmodels.api as sm\nimport missingno as msno\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom scipy.stats import levene\nfrom scipy.stats import shapiro\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve","f9abeb4d":"AlzheimerData = pd.read_csv(\"..\/input\/alzheimer-features\/alzheimer.csv\")\ndata = AlzheimerData.copy() # for VISUALIZATION\ndata[\"Group\"] = pd.Categorical(data[\"Group\"])\ndata[\"M\/F\"] = pd.Categorical(data[\"M\/F\"])\ndata[\"SES\"] = pd.Categorical(data[\"SES\"])\ndata[\"CDR\"] = pd.Categorical(data[\"CDR\"])\ndata[\"EDUC\"] = pd.Categorical(data[\"EDUC\"])\ndata[\"Age\"] = pd.Categorical(data[\"Age\"])\n\ndf = data.select_dtypes(include=[\"float64\",\"int64\",\"int32\"])","2b2e9133":"print(data.shape)\nprint(\"-.\"*40)\nprint(data.columns)\nprint(\"-.\"*40)\nprint(data.info())\nprint(\"-.\"*40)\nprint(data.describe().T)\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"SES\"])[\"MMSE\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"SES\"])[\"eTIV\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"SES\"])[\"nWBV\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"SES\"])[\"ASF\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"CDR\"])[\"MMSE\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"CDR\"])[\"eTIV\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"CDR\"])[\"nWBV\"].mean())\nprint(\"-.\"*40)\nprint(data.groupby([\"Group\",\"CDR\"])[\"ASF\"].mean())\nprint(\"-.\"*40)\nprint(data[\"Group\"].value_counts())\nprint(\"-.\"*40)\nprint(data[\"EDUC\"].value_counts())\nprint(\"-.\"*40)\nprint(data[\"M\/F\"].value_counts())\nprint(\"-.\"*40)\nprint(df.corr())\nprint(\"-.\"*40)\nprint(data.isnull().sum())\nprint(\"-.\"*40)","06a7b167":"msno.heatmap(data)\nmsno.matrix(data)\nplt.show()","8e399cf2":"data[\"Group\"].hist(figsize=(5,5))\nplt.show()","4f80fb41":"data[\"M\/F\"].hist(figsize=(5,5))\nplt.show()","f2352a96":"data[\"SES\"].hist(figsize=(5,5))\nplt.show()","be470512":"data[\"CDR\"].hist(figsize=(5,5))\nplt.show()","04b55cb0":"data[\"Age\"].hist(figsize=(5,5))\nplt.show()","bced960f":"fig = plt.figure()\nax = Axes3D(fig)\nax.scatter(data[\"MMSE\"], data[\"eTIV\"], data[\"nWBV\"], c=\"green\", s=20, alpha=0.5)\nplt.show()","a678bceb":"sns.scatterplot(x=\"SES\",y=\"MMSE\",hue=\"Group\",data=data)\nplt.show()","ddc116d4":"sns.scatterplot(x=\"SES\",y=\"eTIV\",hue=\"Group\",data=data)\nplt.show()","b6ef1433":"sns.scatterplot(x=\"SES\",y=\"nWBV\",hue=\"Group\",data=data)\nplt.show()","681d729a":"sns.scatterplot(x=\"SES\",y=\"ASF\",hue=\"Group\",data=data)\nplt.show()","5334f3f8":"sns.lineplot(x=\"SES\", y=\"MMSE\",hue=\"Group\", data=data)\nplt.show()","fb8d5ddc":"sns.lineplot(x=\"SES\", y=\"eTIV\",hue=\"Group\", data=data)\nplt.show()","d5022cf1":"sns.lineplot(x=\"SES\", y=\"nWBV\",hue=\"Group\", data=data)\nplt.show()","d7c9affa":"sns.lineplot(x=\"SES\", y=\"ASF\",hue=\"Group\", data=data)\nplt.show()","250a1be6":"Features = [\"MMSE\",\"eTIV\",\"nWBV\",\"ASF\"]","750d1b73":"corrPearson = data[Features].corr(method=\"pearson\")\ncorrSpearman = data[Features].corr(method=\"spearman\")","4bc172db":"fig = plt.figure(figsize=(10,8))\nsns.heatmap(corrPearson,annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)\n\nplt.title(\"Pearson Correlation\")\nplt.show()","cfd9fefb":"fig = plt.figure(figsize=(10,8))\nsns.heatmap(corrSpearman,annot=True,cmap='RdYlGn', vmin=-1, vmax=+1)\n\nplt.title(\"Spearman Correlation\")\nplt.show()","1155d161":"for i in Features:\n    print(i,\"-----------\")\n    print(shapiro(data[i]))","7a120123":"print(levene(data[\"MMSE\"],data[\"eTIV\"],data[\"nWBV\"],data[\"ASF\"]))","43739cce":"Columns = [\"Group\",\"M\/F\"]\nencode = LabelEncoder()\nfor i in Columns:\n    print(data[i].value_counts())\n    print(\"----\")\n    data[i] = encode.fit_transform(data[i])\n    print(data[i].value_counts())\n    print(\"----\"*30)","6c838c59":"data[\"SES\"] = AlzheimerData[\"SES\"]\ndata[\"CDR\"] = AlzheimerData[\"CDR\"]\ndata[\"EDUC\"] = AlzheimerData[\"EDUC\"]\ndata[\"Age\"] = AlzheimerData[\"Age\"]\n\nprint(data.info())","cd010f21":"DataForA = data.dropna()\nclf = LocalOutlierFactor()\nclf.fit_predict(DataForA)","f07b0c2a":"score = clf.negative_outlier_factor_\nscoreSort = np.sort(score)\nprint(scoreSort[0:50])","c403af49":"point = scoreSort[3]\nprint(DataForA[score == point])","9ee898dd":"againstvalues = DataForA < point\nprint(DataForA[againstvalues])","bd710169":"normalvalues = DataForA > point\nprint(data[normalvalues])","f186ee00":"data[\"SES\"].fillna(data[\"SES\"].mean(), inplace=True)\ndata[\"MMSE\"].fillna(data[\"MMSE\"].mean(), inplace=True)\nprint(data.isnull().sum())","f2344e3a":"x = data.drop(\"Group\",axis=1)\ny = data[\"Group\"]\n\nxTrain, xTest, yTrain, yTest = train_test_split(x,y,test_size=0.20,random_state=42)\n","fd2fbc36":"ols = sm.OLS(yTrain,xTrain).fit()\npredict = ols.predict(xTest)\nprint(ols.summary())\n# R2 -- 0.89","15bfacae":"lm = LinearRegression().fit(xTrain,yTrain)\npredict = lm.predict(xTest)\n\nR2CV = cross_val_score(lm,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.13\nerrorCV = -cross_val_score(lm,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","2c30a904":"pca = PCA()\nxRTrain = pca.fit_transform(scale(xTrain))\n\nlm = LinearRegression().fit(xRTrain,yTrain)\npredict = lm.predict(xTest)\n\nR2CV = cross_val_score(lm,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.13\nerrorCV = -cross_val_score(lm,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","b6c7abf5":"pls = PLSRegression().fit(xTrain,yTrain)\npredict = pls.predict(xTest)\n\nR2CV = cross_val_score(pls,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.13\nerrorCV = -cross_val_score(pls,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nfor i in range(1,20):\n    plstuned = PLSRegression(n_components=i).fit(xTrain,yTrain)\n    print(f\"{i}\",\"--\"*20)\n    predicttuned = plstuned.predict(xTest)\n    R2CVtuned = cross_val_score(plstuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\n    print(R2CVtuned)\n    # BEST IS 6 -- 0.15\n    errorCVtuned = -cross_val_score(plstuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\n    print(np.sqrt(errorCVtuned))\n    ","7783e438":"ridge = Ridge().fit(xTrain,yTrain)\npredict = ridge.predict(xTest)\n\nR2CV = cross_val_score(ridge,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(ridge,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nalpha = np.random.uniform(0.1,10,50)\n\ncv = RidgeCV(alphas=alpha,scoring=\"r2\",cv=10,normalize=True).fit(xTrain,yTrain)\nprint(cv.alpha_)\n\nridgetuned = Ridge(alpha=cv.alpha_).fit(xTrain,yTrain)\nR2CVtuned = cross_val_score(ridgetuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.15\nerrorCVtuned = -cross_val_score(ridgetuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","b0c45276":"lasso = Lasso().fit(xTrain,yTrain)\npredict = lasso.predict(xTest)\n\nR2CV = cross_val_score(lasso,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(lasso,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\ncv = LassoCV(alphas=None,max_iter=100000,normalize=True).fit(xTrain,yTrain)\nprint(cv.alpha_)\n\nlassotuned = Lasso(alpha=cv.alpha_,normalize=True).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(lassotuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.14\nerrorCVtuned = -cross_val_score(lassotuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","1d691e65":"elastic = ElasticNet().fit(xTrain,yTrain)\npredict = elastic.predict(xTest)\n\nR2CV = cross_val_score(elastic,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(elastic,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\ncv = ElasticNetCV(alphas=None,random_state=0).fit(xTrain,yTrain)\nprint(cv.alpha_)\n\nelastictuned = ElasticNet(alpha=cv.alpha_).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(elastictuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.15\nerrorCVtuned = -cross_val_score(elastictuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","93823ffb":"knn = KNeighborsRegressor().fit(xTrain,yTrain)\npredict = knn.predict(xTest)\n\nR2CV = cross_val_score(knn,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(knn,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nneighbor = {\"n_neighbors\":np.arange(1,10)}\n\ncv = GridSearchCV(knn,neighbor,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\nprint(cv.best_params_)\n# 7\n\nknntuned = KNeighborsRegressor(n_neighbors=7).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(knntuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# -0.15\nerrorCVtuned = -cross_val_score(knntuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","3cb9f8ed":"scaler = StandardScaler().fit(xTrain,yTrain)\nxRTrain = scaler.transform(xTrain)\n\nmlp = MLPRegressor().fit(xRTrain,yTrain)\npredict = mlp.predict(xTest)\n\nR2CV = cross_val_score(mlp,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(mlp,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\"alpha\":[0.0001,0.001,0.01,0.1,0.2],\n         \"hidden_layer_sizes\": [(20,20),(100,200,150),(300,200,250)],\n         \"activation\": [\"relu\",\"logistic\"]}\n\n# cv = GridSearchCV(mlp,params,cv=10,verbose=False,n_jobs=-1).fit(xRTrain,yTrain)\n# print(cv.best_params_)\n# {'activation': 'relu', 'alpha': 0.2, 'hidden_layer_sizes': (300, 200, 250)}\n\nmlptuned = MLPRegressor(activation=\"relu\",alpha=0.2,hidden_layer_sizes=(300,200,250))\n\nR2CVtuned = cross_val_score(mlptuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# -83\nerrorCVtuned = -cross_val_score(mlptuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","233b0c61":"cart = DecisionTreeRegressor().fit(xTrain,yTrain)\npredict = cart.predict(xTest)\n\nR2CV = cross_val_score(cart,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(cart,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\"min_samples_split\":range(2,100),\n         \"max_leaf_nodes\":range(2,10)}\n\n\n# cv = GridSearchCV(cart,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'max_leaf_nodes': 2, 'min_samples_split': 2}\n\ncarttuned = DecisionTreeRegressor(max_leaf_nodes=2,min_samples_split=2).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(carttuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.19\nerrorCVtuned = -cross_val_score(carttuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))\n","4f9bcceb":"bagg = BaggingRegressor(random_state=42,bootstrap_features=True).fit(xTrain,yTrain)\npredict = bagg.predict(xTest)\n\nR2CV = cross_val_score(bagg,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.14\nerrorCV = -cross_val_score(bagg,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nestimators = {\"n_estimators\": range(2,30)}\n\n# cv = GridSearchCV(bagg,estimators,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'n_estimators': 9}\n\nbaggtuned = BaggingRegressor(bootstrap_features=True,random_state=42, n_estimators=9).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(baggtuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.10\nerrorCVtuned = -cross_val_score(baggtuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","5931016b":"rf = RandomForestRegressor().fit(xTrain,yTrain)\npredict = rf.predict(xTest)\n\nR2CV = cross_val_score(rf,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.15\nerrorCV = -cross_val_score(rf,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\"max_depth\":range(1,20),\n         \"max_features\":[3,5,10,15, 20],\n         \"n_estimators\": [200,300,500,1000,2000]}\n\n# cv = GridSearchCV(rf,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'max_depth': 14, 'max_features': 3, 'n_estimators': 300}\n\nfrtuned = RandomForestRegressor(max_depth=14,max_features=3,n_estimators=300).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(frtuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.20\nerrorCVtuned = -cross_val_score(frtuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","77a4d93b":"gbm = GradientBoostingRegressor().fit(xTrain,yTrain)\npredict = gbm.predict(xTest)\n\nR2CV = cross_val_score(gbm,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.05\nerrorCV = -cross_val_score(gbm,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\"learning_rate\": [0.001, 0.01, 0.1, 0.2],\n          \"max_depth\": [3, 5, 8, 10],\n          \"n_estimators\": [200, 300, 500, 1000, 2000],\n          \"subsample\": [1, 0.5, 0.75]}\n\n# cv = GridSearchCV(gbm,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.5}\n\ngbmtuned = GradientBoostingRegressor(learning_rate=0.01,max_depth=3,\n                                     n_estimators=200,subsample=0.5).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(gbmtuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.23\nerrorCVtuned = -cross_val_score(gbmtuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","c6a492f8":"xgb = XGBRegressor().fit(xTrain,yTrain)\npredict = xgb.predict(xTest)\n\nR2CV = cross_val_score(xgb,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.15\nerrorCV = -cross_val_score(xgb,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\"colsample_bytree\": [0.4, 0.5, 0.6, 0.9, 1],\n          \"n_estimators\": [100, 200, 500, 1000],\n          \"max_depth\": [2, 3, 4, 5, 6],\n          \"learning_rate\": [0.1, 0.01, 0.5]}\n\n# cv = GridSearchCV(xgb,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100, 'colsample_bytree': 0.9}\n\nxgbtuned = XGBRegressor(colsample_bytree=0.9,\n                        n_estimators=100, learning_rate=0.1, max_depth=2).fit(xTrain, yTrain)\n\nR2CVtuned = cross_val_score(xgbtuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.14\nerrorCVtuned = -cross_val_score(xgbtuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))\n","9a46bdf4":"lgbm = LGBMRegressor().fit(xTrain,yTrain)\npredict = lgbm.predict(xTest)\n\nR2CV = cross_val_score(lgbm,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.16\nerrorCV = -cross_val_score(lgbm,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\n    \"n_estimators\": [100, 200, 500, 1000],\n    \"max_depth\": [2, 3, 4, 5, 6],\n    \"learning_rate\": [0.1, 0.01, 0.5]\n}\n\n\n# cv = GridSearchCV(lgbm,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 200}\n\nlgbmtuned = LGBMRegressor(learning_rate=0.01,max_depth=2,n_estimators=200).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(lgbmtuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.19\nerrorCVtuned = -cross_val_score(lgbmtuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","ceda74df":"catb = CatBoostRegressor(verbose=False).fit(xTrain,yTrain)\npredict = catb.predict(xTest)\n\nR2CV = cross_val_score(catb,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CV)\n# 0.06\nerrorCV = -cross_val_score(catb,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))\n\nparams = {\"depth\": [2,3,4,5,6,7],\n         \"learning_rate\": [0.1,0.01,0.001,0.5]}\n\n# cv = GridSearchCV(catb,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'depth': 4, 'learning_rate': 0.01}\n\ncatbtuned = CatBoostRegressor(verbose=False,depth=4,learning_rate=0.01).fit(xTrain,yTrain)\n\nR2CVtuned = cross_val_score(catbtuned,xTest,yTest,cv=10,scoring=\"r2\").mean()\nprint(R2CVtuned)\n# 0.20\nerrorCVtuned = -cross_val_score(catbtuned,xTest,yTest,cv=10,scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCVtuned))","e816e5c0":"models = [lm,pls,ridgetuned,elastictuned,knntuned,mlp,\n          carttuned,bagg,frtuned,gbmtuned,xgb,lgbmtuned,catbtuned]\n\n\nfor model in models:\n    name = model.__class__.__name__\n    predict = model.predict(xTest)\n    accuracy = r2_score(yTest, predict)\n    print(\"-\" * 28)\n    print(name + \": \")\n    print(f\"Accuracy: {accuracy}\")\n    ","4852239d":"models = [lm,pls,ridgetuned,elastictuned,knntuned,mlp,\n          carttuned,bagg,frtuned,gbmtuned,xgb,lgbmtuned,catbtuned]\n\nfor model in models:\n    name = model.__class__.__name__\n    predict = model.predict(xTrain)\n    accuracy = r2_score(yTrain, predict)\n    print(\"-\" * 28)\n    print(name + \": \")\n    print(f\"Accuracy: {accuracy}\")\n    ","182a6dfb":"lj = LogisticRegression(solver=\"liblinear\").fit(xTrain,yTrain)\npredict = lj.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(lj,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.82\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))","a5e22e1c":"gnb = GaussianNB().fit(xTrain,yTrain)\npredict = gnb.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(gnb,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.82\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))","e3784eaf":"kn = KNeighborsClassifier().fit(xTrain,yTrain)\npredict = kn.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(kn,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.43\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"n_neighbors\": np.arange(1,50)}\n\n# cv = GridSearchCV(kn,params,cv=10).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# print(cv.best_score_)\n# 'n_neighbors': 1\n\nkntuned = KNeighborsClassifier(n_neighbors=1).fit(xTrain,yTrain)\npredicttuned = kntuned.predict(xTest)\n\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(kntuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.65\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","fb16ba6d":"scaler = StandardScaler().fit(xTrain, yTrain)\nxRTrain = scaler.transform(xTrain)\nxRTest = scaler.transform(xTest)\n\nmlpc = MLPClassifier().fit(xRTrain,yTrain)\npredict = mlpc.predict(xRTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(mlpc,xRTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.79\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"alpha\": [0.01,0.02,0.005,0.001,0.0001],\n         \"hidden_layer_sizes\": [(3,5),(5,3),(10,10,10),(100,200,150),(100,100,100)],\n         \"solver\": [\"lbfgs\",\"adam\",\"sgd\"],\n         \"activation\": [\"relu\",\"logistic\"]}\n\n# cv = GridSearchCV(mlpc,params,cv=10,verbose=False,n_jobs=-1).fit(xRTrain,yTrain)\n# print(cv.best_params_)\n\nmlpctuned = MLPClassifier(alpha=0.1,solver=\"adam\",\n                          activation=\"relu\",hidden_layer_sizes=(100,100,100)).fit(xRTrain,yTrain)\n\npredicttuned = mlpctuned.predict(xRTest)\n\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(mlpctuned,xRTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.72\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))\n","46399bef":"cartc = DecisionTreeClassifier().fit(xTrain,yTrain)\npredict = cartc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(cartc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.74\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"max_depth\":range(1,20),\n         \"min_samples_split\":range(2,50)}\n\n# cv = GridSearchCV(cartc,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n# {'max_depth': 1, 'min_samples_split': 2}\n\ncartctuned = DecisionTreeClassifier(max_depth=1,min_samples_split=2).fit(xTrain,yTrain)\npredicttuned = cartctuned.predict(xTest)\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(cartctuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.83\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","7d372fd2":"rfc = RandomForestClassifier(random_state=42).fit(xTrain,yTrain)\npredict = rfc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(rfc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.81\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"max_depth\": [2, 5, 8, 10],\n          \"max_features\": [2, 5, 8],\n          \"n_estimators\": [10, 500, 1000],\n          \"min_samples_split\": [2, 5, 10]}\n\n# cv = GridSearchCV(rfc,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n\nrfctuned = RandomForestClassifier(max_depth=10, max_features=8,\n                                      min_samples_split=10, n_estimators=1000).fit(xTrain, yTrain)\n\npredicttuned = rfctuned.predict(xTest)\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(rfctuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.83\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","0d803420":"gbmc = GradientBoostingClassifier().fit(xTrain,yTrain)\npredict = gbmc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(gbmc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.82\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"learning_rate\": [0.001,0.01,0.1,0.05],\n         \"max_depth\": [3,5,10],\n         \"n_estimators\": [100,300,500,1000],\n         \"min_samples_split\":[2,5,10]}\n\n# cv = GridSearchCV(gbmc,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n\ngbmctuned = GradientBoostingClassifier(learning_rate=0.1, max_depth=3,\n                                           min_samples_split=10, n_estimators=1000).fit(xTrain, yTrain)\n\npredicttuned = gbmctuned.predict(xTest)\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(gbmctuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.81\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","ca91d448":"xgbc = XGBClassifier(verbose=False).fit(xTrain,yTrain)\npredict = xgbc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(xgbc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.84\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"n_estimators\": [100, 500, 1000, 2000],\n          \"subsample\": [0.6, 0.8, 1.0],\n          \"max_depth\": [3, 4, 5, 6],\n          \"learning_rate\": [0.1, 0.01, 0.02, 0.05],\n          \"min_samples_split\": [2, 5, 10]}\n\n# cv = GridSearchCV(xgbc,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n\nxgbctuned = XGBClassifier(learning_rate=0.01, max_depth=6, min_samples_split=2,\n                              n_estimators=100, subsample=0.8).fit(xTrain, yTrain)\n\npredicttuned = xgbctuned.predict(xTest)\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(xgbctuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.82\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","4c8847dc":"lgbmc = LGBMClassifier().fit(xTrain,yTrain)\npredict = lgbmc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(lgbmc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.77\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"n_estimators\": [100, 500, 1000, 2000],\n          \"subsample\": [0.6, 0.8, 1.0],\n          \"max-depth\": [3, 4, 5, 6],\n          \"learning_rate\": [0.1, 0.01, 0.02, 0.05],\n          \"min_child_samples\": [5, 10, 20]}\n\n# cv = GridSearchCV(lgbmc,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# print(cv.best_params_)\n\nlgbmctuned = LGBMClassifier(learning_rate=0.01, max_depth=3, min_child_samples=20,\n                           n_estimators=500, subsample=0.5).fit(xTrain, yTrain)\n\npredicttuned = lgbmctuned.predict(xTest)\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(lgbmctuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.81\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","49273986":"catbc = CatBoostClassifier(verbose=False).fit(xTrain,yTrain)\npredict = catbc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(catbc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n# 0.80\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nparams = {\"iterations\": [200, 500],\n          \"learning_rate\": [0.01, 0.05, 0.1],\n          \"depth\": [3, 5, 8]}\n\n# cv = GridSearchCV(catbc,params,cv=10,verbose=False,n_jobs=-1).fit(xTrain,yTrain)\n# rint(cv.best_params_)\n\ncatbctuned = CatBoostClassifier(depth=5, iterations=200, learning_rate=0.05,verbose=False).fit(xTrain, yTrain)\npredicttuned = catbctuned.predict(xTest)\nprint(accuracy_score(yTest,predicttuned))\nR2CVtuned = cross_val_score(catbctuned,xTest,yTest,cv=10).mean()\nprint(R2CVtuned)\n# 0.83\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))","6a6683fa":"models = [lj,gnb,kntuned,mlpc,cartctuned,rfctuned,gbmc,xgbc,lgbmctuned,catbctuned]\nr = pd.DataFrame(columns=[\"MODELS\",\"ACC\"])\n\nfor model in models:\n    name = model.__class__.__name__\n    predict = model.predict(xTest)\n    accuracy = accuracy_score(yTest, predict)\n    print(\"-\" * 28)\n    print(name + \": \")\n    print(f\"Accuracy: {accuracy}\")\n    result = pd.DataFrame([[name,accuracy*100]],columns=[\"MODELS\",\"ACC\"])\n    r = r.append(result)\n    \nsns.barplot(x=\"ACC\",y=\"MODELS\",data=r,color=\"r\")\nplt.xlabel(\"ACC\")\nplt.title(\"MODEL ACCURACY COMPARISON\")\nplt.show()","c4fa77b9":"# COMPARISON REGRESSOR","2ccb24ec":"# CONVERSION","0e9a02d1":"# GAUSSIAN NAIVE BAYES MODELS & ERROR & TUNING & PREDICT","0f394b53":"# BAGGING MODELS & ERROR & TUNING & PREDICT","34cd6283":"# RIDGE MODELS & ERROR & TUNING & PREDICT","add34be4":"# KNN MODELS & ERROR & TUNING & PREDICT","63d77859":"# COMPARISON CLASSIFIERS","021c8041":"# LIGHTGBM CLASSIFIER MODELS & ERROR & TUNING & PREDICT","95df4f19":"# LINEAR MODELS & ERROR & TUNING & PREDICT","86ba82d2":"# GBM CLASSIFIER MODELS & ERROR & TUNING & PREDICT","0e4dd3b5":"# RANDOM FORESTS (RF) MODELS & ERROR & TUNING & PREDICT","359630c2":"# LOGISTIC REGRESSION MODELS & ERROR & TUNING & PREDICT","5c30bc8c":"# NORMALITY","dcf56942":"# REGRESSION CLASSIFIER TREES(CART) MODELS & ERROR & TUNING & PREDICT","e1d1e5d7":"# KNN MODELS & ERROR & TUNING & PREDICT","beeef848":"# INFORMATIONS","0d3b7df5":"# PCR MODELS & ERROR & TUNING & PREDICT","73d5e15b":"# REGRESSION TREES(CART) MODELS & ERROR & TUNING & PREDICT","23d01e7f":"# EXTREME GRADIENT BOOSTING (XGBOOST) MODELS & ERROR & TUNING & PREDICT","a914a652":"# REGRESSOR MODELS FOR TRAIN","c37f7414":"# DATA","f349e6ad":"# OLS MODELS & ERROR & TUNING & PREDICT","3ea34461":"* Group --> Class\n* Age --> Age\n* EDUC --> Years of Education\n* SES --> Socioeconomic Status \/ 1-5\n* MMSE --> Mini Mental State Examination\n* CDR --> Clinical Dementia Rating\n* eTIV --> Estimated total intracranial volume\n* nWBV --> Normalize Whole Brain Volume\n* ASF --> Atlas Scaling Factor","bfb08149":"# XGBOOST CLASSIFIER MODELS & ERROR & TUNING & PREDICT","82368f28":"# PLS MODELS & ERROR & TUNING & PREDICT","40f54b14":"# NEURAL NETWORKS CLASSIFIER MODELS & ERROR & TUNING & PREDICT","83ddece6":"# HOMOGENEITY","ffdde02a":"# ENET MODELS & ERROR & TUNING & PREDICT","41767074":"# X & Y FOR MODELS","232237a0":"# CORRELATION VISUALIZATION","9c402876":"# LASSO MODELS & ERROR & TUNING & PREDICT","651731f0":"# MISSING VALUES PROCESS","f1c38e8e":"# CATEGORY BOOSTING (CATBOOST) MODELS & ERROR & TUNING & PREDICT","7896d67c":"# GRADIENT BOOSTING MACHINES (GBM) MODELS & ERROR & TUNING & PREDICT","158b4ab0":"# RANDOM FOREST CLASSIFIER (RF) MODELS & ERROR & TUNING & PREDICT","c4f74518":"# VISUALIZATION","c0ee007a":"#Group\n* Converted (0)\n* Demented (1)\n* Nondemented (2)\n\n#M\/F\n* F (0)\n* M (1)","131ddcd4":"# LIGHT GRADIENT BOOSTING (lIGHT GBM) MODELS & ERROR & TUNING & PREDICT","20007b46":"# AGAINIST VALUES","c6dc4f22":"# ARTIFICIAL NEURAL NETWORKS MODELS & ERROR & TUNING & PREDICT","316c707b":"# CATBOOST CLASSIFIER MODELS & ERROR & TUNING & PREDICT","98648f17":"# MISSING VALUES VISUALIZATION"}}