{"cell_type":{"81f12867":"code","fcb0e191":"code","1cf94a21":"code","f28e6631":"code","bdd2744b":"code","6457bd08":"code","be815737":"code","0c17661c":"code","460fb3f3":"code","8ebd5507":"code","57f70dc3":"code","bcb62bc2":"code","ffbb10c1":"code","e7e8d524":"code","463a0444":"code","49d3c16f":"code","de51c4b4":"code","0978e421":"code","d128e9c6":"code","9daba19c":"code","c4f3d2f8":"code","cab44f59":"code","57212929":"code","042f3c74":"code","c9288192":"code","4ccbe227":"code","2958efb2":"code","56225c47":"code","40b01593":"code","8dbcf7af":"code","124a583c":"code","90a7efbe":"code","3d2fc330":"code","9582341f":"code","b7abb63e":"code","da70af9a":"markdown"},"source":{"81f12867":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fcb0e191":"y_l=np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/Y.npy\")\nx_l=np.load(\"\/kaggle\/input\/sign-language-digits-dataset\/X.npy\")","1cf94a21":"liste1=[]\nliste2=[]\nfor i in range(y_l.shape[0]-1):\n    if list(y_l[i])==list(y_l[i+1]):\n        liste1.append(i)\n    else:\n        liste2.append(i)","f28e6631":"liste2","bdd2744b":"plt.imshow(x_l[203].reshape(64,64))","6457bd08":"plt.imshow(x_l[204].reshape(64,64))","be815737":"liste2","0c17661c":"plt.imshow(x_l[408].reshape(64,64))","460fb3f3":"liste2","8ebd5507":"plt.imshow(x_l[409].reshape(64,64))","57f70dc3":"plt.imshow(x_l[614].reshape(64,64))","bcb62bc2":"plt.imshow(x_l[821].reshape(64,64))","ffbb10c1":"plt.imshow(x_l[1027].reshape(64,64))","e7e8d524":"plt.imshow(x_l[1235].reshape(64,64))","463a0444":"plt.imshow(x_l[1442].reshape(64,64))","49d3c16f":"plt.imshow(x_l[1648].reshape(64,64))","de51c4b4":"plt.imshow(x_l[1854].reshape(64,64))","0978e421":"plt.imshow(x_l[2061].reshape(64,64))","d128e9c6":"y_l.shape","9daba19c":"liste2","c4f3d2f8":"img_size=64\nplt.subplot(1,2,1)\nplt.imshow(x_l[408].reshape(img_size,img_size))\nplt.axis(\"off\")\nplt.subplot(1,2,2)\nplt.imshow(x_l[900].reshape(img_size,img_size))\nplt.axis(\"off\")","cab44f59":"x=np.concatenate((x_l[204:409], x_l[822:1027]), axis=0)\nz=np.zeros(205)\no=np.ones(205)\ny=np.concatenate((z,o),axis=0).reshape(x.shape[0],1)\nprint(\"x shape:\", x.shape)\nprint(\"y shape:\", y.shape)","57212929":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.14, random_state=42)\nnumber_of_train=x_train.shape[0]\nnumber_of_test=x_test.shape[0]","042f3c74":"x_train_2d=x_train.reshape(number_of_train, x_train.shape[1]*x_train.shape[2])\nx_test_2d=x_test.reshape(number_of_test, x_test.shape[1]*x_test.shape[2])\nprint(\"x_train flatten:\", x_train_2d.shape)\nprint(\"y_train flatten:\", x_test_2d.shape)","c9288192":"x_train=x_train_2d.T\nx_test=x_test_2d.T\ny_train=y_train.T\ny_test=y_test.T","4ccbe227":"print(\"x_train\", x_train.shape)\nprint(\"x_test\", x_test.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"y_test\", y_test.shape)","2958efb2":"def initialize_weight_and_bias(dimension):\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","56225c47":"x_train.shape","40b01593":"def forward_backward_propogation(w,b,x_train,y_train):\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss=-y_train*(np.log(y_head))-((1-y_train)*np.log(1-y_head))\n    cost=np.sum(loss)\/x_train.shape[1]\n    \n    derivative_weight=np.dot(x_train,(y_head-y_train).T)\/x_train.shape[1]\n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients={\"derivative_weight\":derivative_weight, \"derivative_bias\":derivative_bias}\n    return cost,gradients","8dbcf7af":"def update(w,b,x_train,y_train,learning_rate,num_of_iter):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    for i in range(num_of_iter):\n        cost,gradients=forward_backward_propogation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w=w-(learning_rate*gradients[\"derivative_weight\"])\n        b=b-(learning_rate*gradients[\"derivative_bias\"])\n        \n        if i % 10==0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i %f\"%(i,cost))\n            \n    parameters={\"weight\":w, \"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index, rotation=\"vertical\")\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","124a583c":"x_test.shape","90a7efbe":"def predict(w,b,x_test):\n    y_prob=sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_prob.shape[1]):\n        if y_prob[0,i]>0.5:\n            y_prediction[0,i]=1\n        elif y_prob[0,i]<=0.5:\n            y_prediction[0,i]=0\n    return y_prediction","3d2fc330":"x_train.shape","9582341f":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_of_iter):\n    dimension=x_train.shape[0]\n    w,b=initialize_weight_and_bias(dimension)\n    \n    parameters, gradient, cost_list=update(w,b,x_train,y_train,learning_rate, num_of_iter)\n    \n    y_prediction=predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    \n    print(\"Test Accuracy:\", 100-np.mean(np.abs(y_prediction-y_test))*100, \"%\")","b7abb63e":"logistic_regression(x_train, y_train, x_test, y_test, 0.01, 190)","da70af9a":"* 0...203] --> 9\n* 204...408] --> 4\n* 409...614] --> 7\n* 615...821] --> 6\n* 822...1027] --> 1\n* 1028...1235] --> 8\n* 1236...1442] --> 4\n* 1443...1648] --> 3\n* 1649...1854] --> 2\n* 1855...2061] --> 5"}}