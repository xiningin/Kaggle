{"cell_type":{"5c491297":"code","07496316":"code","f4266738":"code","7e33b9d4":"code","f501df4b":"code","4b66bc8f":"code","2069869f":"code","821f7ec3":"code","5444d634":"code","dbf23446":"code","5a8b9e46":"code","321dcf31":"code","837e3fb7":"code","79baa469":"markdown","bfe312cd":"markdown","631045d2":"markdown","258e6b68":"markdown","d7b9dd60":"markdown","d5662401":"markdown","cf0686f4":"markdown","f5b13c3f":"markdown","274e253d":"markdown","5ea0e392":"markdown"},"source":{"5c491297":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics","07496316":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')  \nimport warnings\nwarnings.filterwarnings('ignore')  #this will ignore the warnings.it wont display warnings in notebook","f4266738":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import  accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder","7e33b9d4":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score\ndef printreport(exp, pred):\n    print(classification_report(exp, pred))\n    print(\"recall score\")\n    print(recall_score(exp,pred,average = 'macro'))","f501df4b":"gr = pd.read_csv('..\/input\/greedata\/featuren.csv')","4b66bc8f":"sns.set(style=\"whitegrid\")\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nax = sns.violinplot(x=\"label\", y=\"v2\", data=gr, inner=None)\nax = sns.swarmplot(x=\"label\", y=\"v2\", data=gr,color=\"c\", edgecolor=\"black\")","2069869f":"for column in gr.columns[0:-1]:\n    for spec in gr[\"label\"].unique():\n        selected_spec = gr[gr[\"label\"] == spec]\n        selected_column = selected_spec[column]\n        \n        std = selected_column.std()\n        avg = selected_column.mean()\n        \n        three_sigma_plus = avg + (3 * std)\n        three_sigma_minus =  avg - (3 * std)\n        \n        outliers = selected_column[((selected_spec[column] > three_sigma_plus) | (selected_spec[column] < three_sigma_minus))].index\n        gr.drop(outliers, inplace=True)\n        print(column, spec, outliers)","821f7ec3":"x = gr.iloc[:,0:3].values \ny = gr.label.values\nprint(x)\nprint(y)\n","5444d634":"from sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=0, stratify=y)","dbf23446":"# 7. XGboost Classification\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom xgboost import plot_importance\n\nxgb_cls = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=5)\nxgb_cls.fit(x_train, y_train)\ny_pred = xgb_cls.predict(x_test)\n\nprint('XGboost')\ns = accuracy_score(y_test, y_pred)\nprint('accury')\nprint(s)\ncm = confusion_matrix(y_test, y_pred)\nprintreport(y_test, y_pred)\n\n\n\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n\nplot_importance(xgb_cls)","5a8b9e46":"kfold = KFold(n_splits=5, shuffle=True)\nresults = cross_val_score(xgb_cls, x, y, cv=kfold)\nprint(results)\nprint(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","321dcf31":"# 4. Naive Bayes Classification\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\n\ny_pred = gnb.predict(x_test)\n\nprint('GNB')\ncm = confusion_matrix(y_test,y_pred)\ns = accuracy_score(y_test, y_pred)\nprint('accury')\nprint(s)\nprintreport(y_test, y_pred)\n\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","837e3fb7":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1, metric='minkowski')\nknn.fit(x_train,y_train)\n\ny_pred = knn.predict(x_test)\n\nprint('KNN')\ncm = confusion_matrix(y_test,y_pred)\ns = accuracy_score(y_test, y_pred)\nprint('accury')\nprint(s)\nprintreport(y_test, y_pred)\n\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","79baa469":"# K-Fold Validation","bfe312cd":"# Split Data","631045d2":"# K-Nearest Neighbour Classification","258e6b68":"sns.set(style=\"whitegrid\")\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nax = sns.violinplot(x=\"label\", y=\"v2\", data=gr, inner=None)\nax = sns.swarmplot(x=\"label\", y=\"v2\", data=gr,color=\"c\", edgecolor=\"black\")","d7b9dd60":"# Dropping outlier","d5662401":"# Comparing Classification Methods\n\n","cf0686f4":"# Quota Fuction","f5b13c3f":"from sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import SelectKBest\n\nselectkBest=SelectKBest(f_regression,k=3)\n \n## select features \nfeature=gr[['v1','v2','v3','v4','v5','v6','v7','v8','v9','v10','v11','v12']]\nbestFeature=selectkBest.fit_transform(feature,gr['label'])\n \n## result\nfeature.columns[selectkBest.get_support()]\n","274e253d":"# Select Features","5ea0e392":"# Gaussian Naive Bayes Classification"}}