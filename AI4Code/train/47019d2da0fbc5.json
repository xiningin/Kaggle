{"cell_type":{"8e5dee39":"code","6ea3b186":"code","4000be61":"code","1b01787d":"code","efb86f61":"code","5423bc6c":"code","e11509d8":"code","a89ac7f9":"code","85cf8f9b":"code","c07886c1":"code","acbe6ad1":"code","0674a3e8":"code","cca6d00e":"code","df46e5f4":"markdown"},"source":{"8e5dee39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Activation, Flatten, Dense, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6ea3b186":"train = pd.read_csv(\"..\/input\/train.csv\").values","4000be61":"def convert_to_onehot(a):\n    b = np.zeros((a.size, a.max() + 1))\n    b[np.arange(a.size), a] = 1\n    return b","1b01787d":"X = np.reshape(train[:, 1:], (-1, 28, 28, 1))\nY = train[:, 0]\n\nimg_num = 6001\nplt.imshow(X=X[img_num].reshape(28, 28), cmap=plt.get_cmap('gray'))\nplt.title(\"Y[\" + str(img_num) + \"] = \" + str(Y[img_num]))\n\nprint(\"Y[\" + str(img_num) + \"] = \" + str(Y[img_num]))\nY = convert_to_onehot(Y)\nprint(\"One hot encoded Y[\" + str(img_num) + \"] = \" + str(Y[img_num]))\nprint()\n\nX_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.1, random_state=42)\nprint(\"X_train shape: \" + str(X_train.shape))\nprint(\"Y_train shape: \" + str(Y_train.shape))\nprint(\"X_dev shape: \" + str(X_dev.shape))\nprint(\"Y_dev shape: \" + str(Y_dev.shape))","efb86f61":"def digitRecogModel(input_shape):\n    X_input = Input(input_shape)\n    \n    X = Conv2D(filters=16, kernel_size=3)(X_input)\n    X = Activation('relu')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Dropout(0.2)(X)\n    X = MaxPooling2D(pool_size=2)(X)\n    \n    X = Conv2D(filters=32, kernel_size=4)(X)\n    X = Activation('relu')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Dropout(0.2)(X)\n    X = MaxPooling2D(pool_size=2)(X)\n    \n    X = Conv2D(filters=64, kernel_size=4)(X)\n    X = Activation('relu')(X)\n    X = BatchNormalization(axis=3)(X)\n    X = Dropout(0.2)(X)\n    X = MaxPooling2D(pool_size=2)(X)\n    \n    X = Flatten()(X)\n    X = Dense(64, activation='relu')(X)\n    X = BatchNormalization(axis=1)(X)\n    X = Dropout(0.2)(X)\n    X = Dense(32, activation='relu')(X)\n    X = BatchNormalization(axis=1)(X)\n    X = Dropout(0.2)(X)\n    X = Dense(16, activation='relu')(X)\n    X = BatchNormalization(axis=1)(X)\n    X = Dropout(0.2)(X)\n    X = Dense(10, activation='sigmoid')(X)\n    \n    model = Model(inputs=X_input, outputs=X)\n    return model","5423bc6c":"model = digitRecogModel(X_train[0].shape)\nmodel.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])","e11509d8":"start = time.time()\nmodel.fit(x=X_train, y=Y_train, epochs=100, batch_size=20)\nprint(\"Training Time: \" + str(time.time() - start))","a89ac7f9":"preds = model.evaluate(x=X_dev, y=Y_dev)\n\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","85cf8f9b":"test = pd.read_csv('..\/input\/test.csv').values\nX_test = np.reshape(test, (-1, 28, 28, 1))","c07886c1":"results = model.predict(x=X_test)","acbe6ad1":"results = np.argmax(results, axis=1)","0674a3e8":"img_num = 3\nplt.imshow(X=X_test[img_num].reshape(28, 28), cmap=plt.get_cmap('gray'))\nplt.title('Prediction is ' + str(results[img_num]))","cca6d00e":"df = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('results.csv', header=True)","df46e5f4":"Learning Rate: 0.01 | Batch Size: 100<br \/>\nInput->Conv(16,3)->Activation(relu)->BatchNorm->Dropout(0.1)->MaxPool(2)->Conv(32,4)->Activation(relu)->BatchNorm->Dropout(0.1)->MaxPool(2)->Conv(64,4)->Activation(relu)->BatchNorm->Dropout(0.1)->MaxPool(2)<br \/>\n->FC(64,relu)->BatchNorm->Dropout(0.1)->FC(32,relu)->BatchNorm->Dropout(0.1)->FC(16,relu)->BatchNorm->Dropout(0.1)->FC(10,sigmoid)<br \/>\nEpoch 50\/50<br \/>\n37800\/37800 [==============================] - 42s 1ms\/step - loss: 7.9550e-04 - acc: 0.9960<br \/>\nTraining Time: 2050.899270057678<br \/>\nLoss = 0.0015409128824879624<br \/>\nTest Accuracy = 0.9911904761904762<br \/>\n\nLearning Rate: 0.01 | Batch Size: 100<br \/>\nInput->Conv(16,3)->Activation(relu)->BatchNorm->Dropout(0.1)->Conv(32,4)->Activation(relu)->BatchNorm->Dropout(0.1)->Conv(64,4)->Activation(relu)->BatchNorm->Dropout(0.1)<br \/>\n->FC(64,relu)->BatchNorm->Dropout(0.2)->FC(32,relu)->BatchNorm->Dropout(0.2)->FC(16,relu)->BatchNorm->Dropout(0.2)->FC(10,sigmoid)<br \/>\nEpoch 50\/50<br \/>\n37800\/37800 [==============================] - 41s 1ms\/step - loss: 0.0011 - acc: 0.9952<br \/>\nTraining Time: 2018.899007320404<br \/>\nLoss = 0.0015599045548667404<br \/>\nTest Accuracy = 0.9914285714285714<br \/>\n\nLearning Rate: 0.01 | Batch Size: 100<br \/>\nInput->Conv(32,3)->Activation(relu)->BatchNorm->Dropout(0.1)->Conv(64,4)->Activation(relu)->BatchNorm->Dropout(0.1)<br \/>\n->FC(64,relu)->BatchNorm->Dropout(0.2)->FC(32,relu)->BatchNorm->Dropout(0.2)->FC(16,relu)->BatchNorm->Dropout(0.2)->FC(10,sigmoid)<br \/>\nEpoch 50\/50<br \/>\n37800\/37800 [==============================] - 68s 2ms\/step - loss: 8.6022e-04 - acc: 0.9967<br \/>\nTraining Time: 3421.957444190979<br \/>\nLoss = 0.0017906383639997637<br \/>\nTest Accuracy = 0.9902380952380953<br \/>"}}