{"cell_type":{"54945bba":"code","4dda62cb":"code","116c5f9d":"code","ee966638":"code","1586de85":"code","d4a878f5":"code","23d991bc":"code","f032dd4f":"code","8b2622a0":"code","b7fe8106":"code","bd1f0f0d":"code","f63dbb18":"code","b07a4d7f":"code","eac04740":"code","c296b682":"markdown","66f455c0":"markdown","0815bc9b":"markdown","3b0301a6":"markdown","d6aaeda8":"markdown","c666f747":"markdown"},"source":{"54945bba":"!pip install kneed","4dda62cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.set_config_file(offline=True)\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom kneed import KneeLocator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","116c5f9d":"def MinMaxScaler(data):\n    return (data-np.min(data))\/(np.max(data)-np.min(data))\n\ndef Kmeans_clustering(df, clusterNum, max_iter, n_jobs):\n    scaler = StandardScaler()\n    scaler.fit(df)\n    df_std = pd.DataFrame(data=scaler.transform(df), columns=df.columns, index=df.index)\n    km_model = KMeans(n_clusters=clusterNum, max_iter=max_iter, n_jobs=n_jobs, random_state=666)\n    km_model = km_model.fit(df_std)\n    clusterdf= pd.DataFrame(data=km_model.labels_, columns=['ClusterNo'])\n    clusterdf.index = df.index\n    return clusterdf\n\ndef Kmeans_bestClusterNum(df, range_min, range_max, max_iter, n_jobs):\n    silhouette_avgs = []\n    sum_of_squared_distances = []\n    \n    ks = range(range_min,range_max+1)\n    for k in ks:\n        kmeans_fit = KMeans(n_clusters = k, n_jobs=n_jobs, max_iter=max_iter, random_state=666).fit(df)\n        cluster_labels = kmeans_fit.labels_\n        sum_of_squared_distances.append(kmeans_fit.inertia_)\n        \n    kn = KneeLocator(list(ks), sum_of_squared_distances, S=1.0, curve='convex', direction='decreasing')  \n    plt.xlabel('k')\n    plt.ylabel('sum_of_squared_distances')\n    plt.title('The Elbow Method showing the optimal k')\n    plt.plot(ks, sum_of_squared_distances, 'bx-')\n    plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n    print('Optimal clustering number:'+str(kn.knee))\n    print('----------------------------')    \n    \n    return kn.knee","ee966638":"%%time\nroot = Path('..\/input\/ashrae-feather-format-for-fast-loading')\n\ntrain_df = pd.read_feather(root\/'train.feather')\ntest_df = pd.read_feather(root\/'test.feather')\nweather_train_df = pd.read_feather(root\/'weather_train.feather')\nweather_test_df = pd.read_feather(root\/'weather_test.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')","1586de85":"#Let's just take site1 as example\ntrain_df = train_df.merge(building_meta_df, on='building_id')\ntrain_df = train_df[train_df['site_id']==1]\n\ntrain_df['merged_id'] = 'site'+train_df['site_id'].astype(str)+'_'\\\n                        +'bldg'+train_df['building_id'].astype(str)+'_'\\\n                        +'meter'+train_df['meter'].astype(str)\ntrain_df = train_df[['timestamp', 'site_id', 'building_id', 'meter' ,'meter_reading', 'merged_id']]\ntrain_df = train_df.set_index('timestamp')\n\ntrain_df = train_df.sort_values(['merged_id','timestamp'])\ntrain_df","d4a878f5":"train_df_pivot = train_df.pivot_table(values='meter_reading', index='timestamp', columns='merged_id')\ntrain_df_pivot","23d991bc":"df_PM_temp = train_df_pivot.copy()\ndf_PM_temp = (df_PM_temp-df_PM_temp.mean())\/df_PM_temp.std()\nSTL_decomp = seasonal_decompose(df_PM_temp.fillna(0), model='additive',freq=24*7,extrapolate_trend=1)\n\ndf_seasonal_temp=STL_decomp.seasonal.iloc[24*10:24*17,:]\ndf_seasonal_temp = df_seasonal_temp.T\n\ndf_seasonal_temp['ClusterNo'] = Kmeans_clustering(df=df_seasonal_temp, clusterNum=5, max_iter=100000, n_jobs=-1)\n\nfor ClusterNo in df_seasonal_temp['ClusterNo'].unique():\n    df_plot = df_seasonal_temp[df_seasonal_temp['ClusterNo']==ClusterNo].T.drop('ClusterNo')\n    print('ClusterNo: ' + str(ClusterNo))    \n    print('Amount of meters: ' + str(len(df_plot.T)))\n    df_plot.plot(figsize=(15,5),color='black',alpha=0.1,legend=False)\n    plt.show()\n    print('---------------------------------------------------------------------------------------------------')    ","f032dd4f":"df_PM_temp = train_df_pivot.copy()\ndf_PM_temp = (df_PM_temp-df_PM_temp.mean())\/df_PM_temp.std()\nSTL_decomp = seasonal_decompose(df_PM_temp.fillna(0), model='additive',freq=24*7,extrapolate_trend=1)\n\ndf_trend_temp = STL_decomp.trend\n#df_trend_temp = df_trend_temp.loc['2016']\ndf_trend_temp = df_trend_temp.T\n\ndf_trend_temp['ClusterNo'] = Kmeans_clustering(df=df_trend_temp, clusterNum=5, max_iter=100000, n_jobs=-1)\n\nfor ClusterNo in df_trend_temp['ClusterNo'].unique():\n    df_plot = df_trend_temp[df_trend_temp['ClusterNo']==ClusterNo].T.drop('ClusterNo')\n    print('ClusterNo: ' + str(ClusterNo))    \n    print('Amount of meters: ' + str(len(df_plot.T)))\n    df_plot.plot(figsize=(15,5),color='black',alpha=0.1,legend=False,ylim=(-3,3))\n    plt.show()\n    print('---------------------------------------------------------------------------------------------------')","8b2622a0":"#Graph of original data: \"site1_bldg105_meter0\"\ntrain_df_pivot['site1_bldg105_meter0'].iplot(kind='scatter')","b7fe8106":"#Let's clean these abnormal days by checking std_daily==0\n\nlist_powerMeter = list(train_df_pivot.columns)\n\nfor name_meter in list_powerMeter:\n    df_daily = train_df_pivot[name_meter].copy()\n    df_daily = df_daily.resample('D').std()\n\n    list_abnormalDate = list(df_daily[df_daily==0].index.strftime('%Y-%m-%d'))\n    train_df_pivot['Date'] = pd.to_datetime(train_df_pivot.index.date)\n\n    train_df_pivot.loc[train_df_pivot['Date'].isin(list_abnormalDate), name_meter] = np.nan\n    train_df_pivot = train_df_pivot.drop('Date', axis=1)  ","bd1f0f0d":"#Graph of cleand data: \"site1_bldg105_meter0\"\ntrain_df_pivot['site1_bldg105_meter0'].iplot(kind='scatter')","f63dbb18":"#Prepare data: \"site1_bldg105_meter0\"\nexample_df = train_df_pivot.copy()\nexample_df = example_df.dropna(axis=1) #Only take non-missing meter readings as inputs\n\nexample_df['elec_meas'] = train_df_pivot['site1_bldg105_meter0'].copy() #Take \"site1_bldg105_meter0\" as target\n\ntraindata = example_df[~example_df['elec_meas'].isna()]\ntestdata = example_df[example_df['elec_meas'].isna()]\n\ntrain_labels = traindata['elec_meas']\n\ntrain_features = traindata.drop('elec_meas', axis=1)\ntest_features = testdata.drop('elec_meas', axis=1)","b07a4d7f":"LGB_model = lgb.LGBMRegressor()\nLGB_model.fit(train_features, train_labels)\n\ntestdata['elec_pred'] = LGB_model.predict(test_features)\n\n# Use the forest's predict method on the train data\nexample_df['elec_pred'] = LGB_model.predict(example_df.drop('elec_meas', axis=1))","eac04740":"example_df[['elec_meas', 'elec_pred']].iplot(kind='scatter')","c296b682":"Boom! Here's the meter reading after filling missing\ud83d\ude0e","66f455c0":"Hey guys!\n\nAfter the competition closed, I want to share my idea of filling missing with you.\n(The original train data have too mcuh missing...)\n\nThe main idea is \"time-series similarity between meters in single site\".\n(for example, meter_A's pattern is very similar to meter_B in site1, and I do believe that some of you have noticed that)\nBy well utilizing such similarity, we can find out those missing data!","0815bc9b":"Successfully clean these abnormal days!","3b0301a6":"Hope you guys enjoy!","d6aaeda8":"Take a look of \"site1_bldg105_meter0\", we can observe abnormal constant in some days.","c666f747":"Now, let's take \"site1_bldg105_meter0\" as example for fillling missing"}}