{"cell_type":{"141f82b6":"code","05e8c567":"code","87f9061e":"code","a1daf78d":"code","19f36052":"code","58cb99d9":"code","71f78e38":"code","ed84a4d1":"code","96046e88":"code","68a39ebd":"code","4c216026":"code","7f382f6a":"markdown","7b7e1b84":"markdown","2cd6879b":"markdown","0732dbb8":"markdown","1e829b6d":"markdown"},"source":{"141f82b6":"# Global Variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_TREES = 1000\nEARLY_STOP = 25","05e8c567":"# Install TFDF library\n!pip3 install -q tensorflow_decision_forests --upgrade","87f9061e":"import numpy as np\nimport pandas as pd\nimport pyarrow\nimport warnings\nimport time\nimport gc\nimport os\n\n# Hide warnings\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n# Model and evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\n# Tensorflow\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\ntf.random.set_seed(RANDOM_SEED)","a1daf78d":"%%time\n# Load original training data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\n\n# List for tracking categorical features\ncategorical_features = list()\n\n# Downcast training data while keeping track of categorical variables\nfor col in train.columns:\n    if train[col].dtype == \"int64\":\n        train[col] = train[col].astype('int32')\n        # ignore target column\n        if col == \"target\": continue\n        categorical_features.append(\n            tfdf.keras.FeatureUsage(\n                name = col, \n                semantic = tfdf.keras.FeatureSemantic.CATEGORICAL\n            )\n        )\n    elif train[col].dtype == \"float64\":\n        train[col] = pd.to_numeric(train[col], downcast ='float')\n\n# Halve the data, we will get AUC estimates on a holdout set\ntrain, holdout = train_test_split(\n    train,\n    train_size = 500000,\n    stratify = train['target'],\n    shuffle = True\n)\n\n# Save pandas dataframe for quick retrieval later\ntrain.reset_index(drop = True, inplace = True)\nholdout.reset_index(drop = True, inplace = True)\ntrain.to_feather('train_500k.feather')\nholdout.to_feather('holdout_full.feather')\ndel train; gc.collect()\n\n# Create tensorflow data\nholdout_tf = tfdf.keras.pd_dataframe_to_tf_dataset(\n    holdout,\n    label = 'target',\n    in_place = True\n)\n\n# Save Train data\ntf.data.experimental.save(holdout_tf, \"holdout_tf\")\n\n# Garbage Collection (free up memory)\ndel holdout, holdout_tf; gc.collect()","19f36052":"def get_training_data(n_rows = 10000):\n    \n    assert 0 < n_rows < 500000\n    train = pd.read_feather('train_500k.feather')\n    \n    train, test = train_test_split(\n        train,\n        train_size = n_rows,\n        stratify = train['target'],\n        shuffle = True\n    )\n    \n    # Prepare Train Data\n    train_df = tfdf.keras.pd_dataframe_to_tf_dataset(\n        train,\n        label = 'target',\n        in_place = True\n    )\n    \n    return train_df","58cb99d9":"def train_model(n_rows = 10000, categorical = False):\n\n    train_df = get_training_data(n_rows)\n    gc.collect()\n\n    start = time.time()\n    # Define model, using explicitly defined categoricals\n    if categorical:\n        model = tfdf.keras.GradientBoostedTreesModel(\n            task = tfdf.keras.Task.CLASSIFICATION,\n            num_trees = NUM_TREES,\n            early_stopping_num_trees_look_ahead = EARLY_STOP,\n            features = categorical_features,\n            exclude_non_specified_features = False,\n            verbose = 0\n        )\n    else:\n        model = tfdf.keras.GradientBoostedTreesModel(\n            task = tfdf.keras.Task.CLASSIFICATION,\n            num_trees = NUM_TREES,\n            early_stopping_num_trees_look_ahead = EARLY_STOP,\n            verbose = 0\n        )\n\n    # Metric for validation \n    model.compile(\n        metrics=[tf.metrics.AUC()]\n    )\n\n    # Training\n    model.fit(train_df, verbose = 0)\n    end = time.time()\n\n    # Delete training data (free up memory)\n    del train_df\n    gc.collect()\n    \n    return model, round(end-start, 6)","71f78e38":"def get_holdout_preds(model):\n    \n    holdout_df = tf.data.experimental.load(\"holdout_tf\")\n\n    preds = model.predict(holdout_df)[:,0]\n\n    del holdout_df\n    gc.collect()\n    return preds","ed84a4d1":"def get_holdout_score(y_preds):\n    \n    holdout = pd.read_feather('holdout_full.feather')\n    y_true = holdout['target']\n    \n    return roc_auc_score(y_true, y_preds)","96046e88":"def get_benchmarks():\n    \n    data = defaultdict(list)\n    \n    for training_size in [10000, 20000, 30000, 40000, 50000, 75000]:\n\n        # Train model, no specified categorical features\n        model, training_time = train_model(n_rows = training_size)\n        preds = get_holdout_preds(model)\n        score = get_holdout_score(preds)\n        print('All Numerical Features')\n        print(f'Rows: {training_size}, Time: {round(training_time, 2)}')\n        print(f'Validation Score: {round(score, 6)}\\n')\n        \n        # save results\n        data['size'].append(training_size)\n        data['features'].append('numerical')\n        data['time'].append(training_time)\n        data['auc'].append(score)\n        \n        # free up memory\n        del model\n        gc.collect()\n        \n        # Train model, specify categorical features\n        model, training_time = train_model(\n            n_rows = training_size, \n            categorical = True\n        )\n        preds = get_holdout_preds(model)\n        score = get_holdout_score(preds)\n        print('Categorical Features')\n        print(f'Rows: {training_size}, Time: {round(training_time, 2)}')\n        print(f'Validation Score: {round(score, 6)}\\n')\n        \n        # save results\n        data['size'].append(training_size)\n        data['features'].append('categorical')\n        data['time'].append(training_time)\n        data['auc'].append(score)\n        \n        # free up memory\n        del model\n        gc.collect()\n        \n    return pd.DataFrame(data)","68a39ebd":"# Output has been hidden\ndata = get_benchmarks()","4c216026":"data","7f382f6a":"# Helper Functions\n\nWe create a several function to perform various steps of the training and evaluation process, mostly to avoid having too many things loaded in memory at once.","7b7e1b84":"# Benchmarks","2cd6879b":"# Tensorflow Decision Forests\n\nIn this notebook we get benchmarks for the gradient boosting model provided with the [Tensorflow Decision Forests](https:\/\/www.tensorflow.org\/decision_forests) library. For various input sizes (10k to 100k samples) we get AUC scores and training times for models with and without categorical features explicitly specified.\n\nPersonally, I don't think this library is ready to be used seriously for these competitions primarily because it runs so slowly (no GPU\/TPU optimizations yet) and requires a linux environment (which I can only easily access through these notebooks). You can get similar results much easier and faster using one of the established gradient boosting frameworks like XGBoost, LightGBM, and CatBoost.\n\n\n**Note:** This notebook will take several hours to run","0732dbb8":"We see that explicitly specifying the categorical features significantly increases the training times but does not result in notably better models so we will not bother specifying categorical features for subsequent models.","1e829b6d":"# Preparing Data\n\n1. Load original data\n2. Downcast datatypes wherever possible\n3. Split data into halves, use one half for estimation\n4. Save data in pandas and tensorflow formats"}}