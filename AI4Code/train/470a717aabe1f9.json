{"cell_type":{"d151fd05":"code","1cf6c023":"code","da2a573b":"code","78025619":"code","7fd23d86":"code","aedd8fa8":"code","3b4751b5":"code","8d49ce1b":"code","2cc770a2":"code","1b67a484":"code","4c77ae61":"code","bd6e9de2":"code","0440bdcc":"code","533adebb":"code","96af3cdc":"code","5f514612":"code","7a71c307":"code","8047f94f":"code","a88a5a49":"code","011bd84f":"code","d2910dcc":"code","1fce15aa":"code","6b74be25":"code","97b801fe":"code","2ce9865d":"code","6e11eefe":"code","4f2abd8c":"code","c782a9a7":"code","77bcc569":"code","bb348ff5":"code","8481aa6b":"code","ccc57511":"code","0607173b":"code","5be2be6c":"markdown","ccfed53b":"markdown","9305bfab":"markdown","37a426a6":"markdown","38083737":"markdown","54f7238e":"markdown","80c1a471":"markdown"},"source":{"d151fd05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch import autograd\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import Dataset,DataLoader,Subset\nfrom PIL import Image,ImageOps,ImageEnhance\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\n\nimport glob\nimport xml.etree.ElementTree as ET #for parsing XML\nimport shutil\nfrom tqdm import tqdm\nimport time\nimport random\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1cf6c023":"!unzip ..\/input\/Annotation.zip\n!unzip ..\/input\/all-dogs.zip","da2a573b":"TIME_LIMIT = 32400 - 60*10\nstart_time = time.time()\ndef elapsed_time(start_time):\n    return time.time() - start_time","78025619":"#random seeds\nseed = 2019\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\nBATCH_SIZE  = 32\nNUM_WORKERS = 4\nEMA = False\nLABEL_NOISE = False\nLABEL_NOISE_PROB = 0.1","7fd23d86":"PATH = 'all-dogs\/'\nimg_filenames = os.listdir(PATH)\nlen(img_filenames)","aedd8fa8":"PATH_ANNOTATION = 'Annotation\/'\nbreeds = glob.glob(PATH_ANNOTATION+'*')\nannotations = []\nfor breed in breeds:\n    annotations += glob.glob(breed+'\/*')\nlen(annotations)","3b4751b5":"breed_map = {}\nfor annotation in annotations:\n    breed = annotation.split('\/')[-2]\n    index = breed.split('-')[0]\n    breed_map.setdefault(index,breed)\nn_classes = len(breed_map)\nn_classes","8d49ce1b":"breed_map","2cc770a2":"#https:\/\/www.kaggle.com\/whizzkid\/crop-images-using-bounding-box\ndef bounding_box(img):\n    bpath = PATH_ANNOTATION + str(breed_map[img.split('_')[0]])+'\/'+str(img.split('.')[0])\n    tree  = ET.parse(bpath)\n    root  = tree.getroot()\n    objects = root.findall('object')\n    bbxs = []\n    for o in objects:\n        bndbox = o.find('bndbox') #reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        bbxs.append((xmin,ymin,xmax,ymax))\n    return bbxs","1b67a484":"def bounding_box_ratio(img):\n    bpath = PATH_ANNOTATION + str(breed_map[img.split('_')[0]])+'\/'+str(img.split('.')[0])\n    tree  = ET.parse(bpath)\n    root  = tree.getroot()\n    objects = root.findall('object')\n    bbx_ratios = []\n    for o in objects:\n        bndbox = o.find('bndbox') #reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        xlen = xmax - xmin\n        ylen = ymax - ymin\n        ratio = ylen \/ xlen\n        bbx_ratios.append((xlen,ylen,ratio))\n    return bbx_ratios","4c77ae61":"%%time\n#threshold for aspect ratio, at the same time idx for each bbx\nimg_filenames_th = []\nratios_th = []\nfor img in tqdm(img_filenames):\n    bbx_ratios = bounding_box_ratio(img)\n    for i,(xlen,ylen,ratio) in enumerate(bbx_ratios):\n        if ((ratio>0.2)&(ratio<4.0)):\n            img_filenames_th.append(img[:-4]+'_'+str(i)+'.jpg')\n            ratios_th.append(ratio)\nratios_th = np.array(ratios_th)\n\nprint('original : ', len(img_filenames))\nprint('after th : ', len(img_filenames_th))","bd6e9de2":"#from https:\/\/www.kaggle.com\/korovai\/dogs-images-intruders-extraction\nintruders = [\n    #n02088238-basset\n    'n02088238_10870_0.jpg',\n    \n    #n02088466-bloodhound\n    'n02088466_6901_1.jpg',\n    'n02088466_6963_0.jpg',\n    'n02088466_9167_0.jpg',\n    'n02088466_9167_1.jpg',\n    'n02088466_9167_2.jpg',\n    \n    #n02089867-Walker_hound\n    'n02089867_2221_0.jpg',\n    'n02089867_2227_1.jpg',\n    \n    #n02089973-English_foxhound # No details\n    'n02089973_1132_3.jpg',\n    'n02089973_1352_3.jpg',\n    'n02089973_1458_1.jpg',\n    'n02089973_1799_2.jpg',\n    'n02089973_2791_3.jpg',\n    'n02089973_4055_0.jpg',\n    'n02089973_4185_1.jpg',\n    'n02089973_4185_2.jpg',\n    \n    #n02090379-redbone\n    'n02090379_4673_1.jpg',\n    'n02090379_4875_1.jpg',\n    \n    #n02090622-borzoi # Confusing\n    'n02090622_7705_1.jpg',\n    'n02090622_9358_1.jpg',\n    'n02090622_9883_1.jpg',\n    \n    #n02090721-Irish_wolfhound # very small\n    'n02090721_209_1.jpg',\n    'n02090721_1222_1.jpg',\n    'n02090721_1534_1.jpg',\n    'n02090721_1835_1.jpg',\n    'n02090721_3999_1.jpg',\n    'n02090721_4089_1.jpg',\n    'n02090721_4276_2.jpg',\n    \n    #n02091032-Italian_greyhound\n    'n02091032_722_1.jpg',\n    'n02091032_745_1.jpg',\n    'n02091032_1773_0.jpg',\n    'n02091032_9592_0.jpg',\n    \n    #n02091134-whippet\n    'n02091134_2349_1.jpg',\n    'n02091134_14246_2.jpg',\n    \n    #n02091244-Ibizan_hound\n    'n02091244_583_1.jpg',\n    'n02091244_2407_0.jpg',\n    'n02091244_3438_1.jpg',\n    'n02091244_5639_1.jpg',\n    'n02091244_5639_2.jpg',\n    \n    #n02091467-Norwegian_elkhound\n    'n02091467_473_0.jpg',\n    'n02091467_4386_1.jpg',\n    'n02091467_4427_1.jpg',\n    'n02091467_4558_1.jpg',\n    'n02091467_4560_1.jpg',\n    \n    #n02091635-otterhound\n    'n02091635_1192_1.jpg',\n    'n02091635_4422_0.jpg',\n    \n    #n02091831-Saluki\n    'n02091831_1594_1.jpg',\n    'n02091831_2880_0.jpg',\n    'n02091831_7237_1.jpg',\n    \n    #n02092002-Scottish_deerhound\n    'n02092002_1551_1.jpg',\n    'n02092002_1937_1.jpg',\n    'n02092002_4218_0.jpg',\n    'n02092002_4596_0.jpg',\n    'n02092002_5246_1.jpg',\n    'n02092002_6518_0.jpg',\n    \n    #02093256-Staffordshire_bullterrier\n    'n02093256_1826_1.jpg',\n    'n02093256_4997_0.jpg',\n    'n02093256_14914_0.jpg',\n    \n    #n02093428-American_Staffordshire_terrier\n    'n02093428_5662_0.jpg',\n    'n02093428_6949_1.jpg'\n            ]\n\nlen(intruders)","0440bdcc":"def data_preprocessing(img_path,bbx_idx):\n    bbx = bounding_box(img_path)[bbx_idx]\n    img  = Image.open(os.path.join(PATH,img_path))#PILImage format\n    img_cropped  = img.crop(bbx)\n    return img_cropped","533adebb":"%%time\nbreed_map_2 = {}\nfor i,b in enumerate(breed_map.keys()):\n    breed_map_2[b] = i","96af3cdc":"class DogDataset(Dataset):\n    def __init__(self, path, img_list, transform1=None, transform2=None):\n        self.path      = path\n        self.img_list  = img_list\n        self.transform1 = transform1\n        self.transform2 = transform2\n        \n        self.imgs   = []\n        self.labels = []\n        for i,full_img_path in enumerate(self.img_list):\n            if full_img_path in intruders:\n                continue\n            #img\n            img_path = full_img_path[:-6]+'.jpg'\n            bbx_idx  = int(full_img_path[-5])\n            img = data_preprocessing(img_path,bbx_idx)\n            if self.transform1:\n                img = self.transform1(img) #output shape=(ch,h,w)\n            self.imgs.append(img)\n            #label\n            label = breed_map_2[img_path.split('_')[0]]\n            self.labels.append(label)\n            \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self,idx):\n        img = self.imgs[idx]\n        if self.transform2:\n            img = self.transform2(img)\n        label = self.labels[idx]\n        return {'img':img, 'label':label}","5f514612":"%%time\n# generate 64x64 images!\n#resize_size = 68\nimg_size    = 64\nbatch_size  = BATCH_SIZE\nMEAN1,MEAN2,MEAN3 = 0.5, 0.5, 0.5\nSTD1,STD2,STD3    = 0.5, 0.5, 0.5\n\ntransform1 = transforms.Compose([transforms.Resize(img_size)])\n\ntransform2 = transforms.Compose([transforms.RandomCrop(img_size),\n                                 #transforms.RandomAffine(degrees=5),\n                                 transforms.RandomHorizontalFlip(p=0.5),\n                                 #transforms.RandomApply(random_transforms, p=0.3),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize(mean=[MEAN1, MEAN2, MEAN3],\n                                                      std=[STD1, STD2, STD3]),\n                                ])\n\ntrain_set = DogDataset(path=PATH,\n                       img_list=img_filenames_th,\n                       transform1=transform1,\n                       transform2=transform2,\n                      )\n\ntrain_loader = DataLoader(train_set,\n                          shuffle=True, batch_size=batch_size,\n                          num_workers=NUM_WORKERS, pin_memory=True)","7a71c307":"len(train_set)","8047f94f":"img = data_preprocessing(img_filenames_th[1500][:-6]+'.jpg',0)\nimg = transform1(img)\nimg","a88a5a49":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","011bd84f":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","d2910dcc":"def conv3x3(in_channel, out_channel): #not change resolusion\n    return nn.Conv2d(in_channel,out_channel,\n                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n\ndef conv1x1(in_channel, out_channel): #not change resolution\n    return nn.Conv2d(in_channel,out_channel,\n                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n\ndef init_weight(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.orthogonal_(m.weight, gain=1)\n        if m.bias is not None:\n            m.bias.data.zero_()\n            \n    elif classname.find('Batch') != -1:\n        m.weight.data.normal_(1,0.02)\n        m.bias.data.zero_()\n    \n    elif classname.find('Linear') != -1:\n        nn.init.orthogonal_(m.weight, gain=1)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    \n    elif classname.find('Embedding') != -1:\n        nn.init.orthogonal_(m.weight, gain=1)\n        \n\n# class Attention(nn.Module):\n#     def __init__(self, c, h, w):\n#         super().__init__()\n#         self.attention_fc = nn.Linear(c,1, bias=False).apply(init_weight)\n#         self.bias         = nn.Parameter(torch.zeros((1,h,w,1), requires_grad=True))\n#         self.sigmoid      = nn.Sigmoid()\n        \n#     def forward(self,inputs):\n#         batch,c,h,w = inputs.size()\n#         x = torch.transpose(inputs, 1,2) #(*,c,h,w)->(*,h,c,w)\n#         x = torch.transpose(x, 2,3) #(*,h,c,w)->(*,h,w,c)\n#         x = self.attention_fc(x) + self.bias\n#         x = torch.transpose(x, 2,3) #(*,h,w,1)->(*,h,1,w)\n#         x = torch.transpose(x, 1,2) #(*,h,1,w)->(*,1,h,w)\n#         x = self.sigmoid(x)\n#         return inputs * x\n\nclass Attention(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.theta    = nn.utils.spectral_norm(conv1x1(channels, channels\/\/8)).apply(init_weight)\n        self.phi      = nn.utils.spectral_norm(conv1x1(channels, channels\/\/8)).apply(init_weight)\n        self.g        = nn.utils.spectral_norm(conv1x1(channels, channels\/\/2)).apply(init_weight)\n        self.o        = nn.utils.spectral_norm(conv1x1(channels\/\/2, channels)).apply(init_weight)\n        self.gamma    = nn.Parameter(torch.tensor(0.), requires_grad=True)\n        \n    def forward(self, inputs):\n        batch,c,h,w = inputs.size()\n        theta = self.theta(inputs) #->(*,c\/8,h,w)\n        phi   = F.max_pool2d(self.phi(inputs), [2,2]) #->(*,c\/8,h\/2,w\/2)\n        g     = F.max_pool2d(self.g(inputs), [2,2]) #->(*,c\/2,h\/2,w\/2)\n        \n        theta = theta.view(batch, self.channels\/\/8, -1) #->(*,c\/8,h*w)\n        phi   = phi.view(batch, self.channels\/\/8, -1) #->(*,c\/8,h*w\/4)\n        g     = g.view(batch, self.channels\/\/2, -1) #->(*,c\/2,h*w\/4)\n        \n        beta = F.softmax(torch.bmm(theta.transpose(1,2), phi), -1) #->(*,h*w,h*w\/4)\n        o    = self.o(torch.bmm(g, beta.transpose(1,2)).view(batch,self.channels\/\/2,h,w)) #->(*,c,h,w)\n        return self.gamma*o + inputs\n        \n    \nclass ConditionalNorm(nn.Module):\n    def __init__(self, in_channel, n_condition):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(in_channel, affine=False) #no learning parameters\n        self.embed = nn.Linear(n_condition, in_channel* 2)\n        \n        nn.init.orthogonal_(self.embed.weight.data[:, :in_channel], gain=1)\n        self.embed.weight.data[:, in_channel:].zero_()\n\n    def forward(self, inputs, label):\n        out = self.bn(inputs)\n        embed = self.embed(label.float())\n        gamma, beta = embed.chunk(2, dim=1)\n        gamma = gamma.unsqueeze(2).unsqueeze(3)\n        beta = beta.unsqueeze(2).unsqueeze(3)\n        out = gamma * out + beta\n        return out","1fce15aa":"# #BigGAN\n# class ResBlock_G(nn.Module):\n#     def __init__(self, in_channel, out_channel, condition_dim, upsample=True):\n#         super().__init__()\n#         self.cbn1 = ConditionalNorm(in_channel, condition_dim)\n#         self.upsample = nn.Sequential()\n#         if upsample:\n#             self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n#         self.conv3x3_1 = nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight)\n#         self.cbn2 = ConditionalNorm(out_channel, condition_dim)\n#         self.conv3x3_2 = nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight) \n#         self.conv1x1   = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n        \n#     def forward(self, inputs, condition):\n#         x  = F.relu(self.cbn1(inputs, condition))\n#         x  = self.upsample(x)\n#         x  = self.conv3x3_1(x)\n#         x  = self.conv3x3_2(F.relu(self.cbn2(x, condition)))\n#         x += self.conv1x1(self.upsample(inputs)) #shortcut\n#         return x\n\n# class Generator(nn.Module):\n#     def __init__(self, n_feat, codes_dim=24, n_classes=n_classes):\n#         super().__init__()\n#         self.fc   = nn.Sequential(\n#             nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight),\n#             #nn.BatchNorm1d(16*n_feat*4*4).apply(init_weight),\n#             #nn.ReLU(),\n#         )\n#         self.res1 = ResBlock_G(16*n_feat, 16*n_feat, codes_dim+n_classes, upsample=True)\n#         self.res2 = ResBlock_G(16*n_feat,  8*n_feat, codes_dim+n_classes, upsample=True)\n#         self.res3 = ResBlock_G( 8*n_feat,  4*n_feat, codes_dim+n_classes, upsample=True)\n#         self.attn = Attention(4*n_feat)\n#         self.res4 = ResBlock_G( 4*n_feat,  2*n_feat, codes_dim+n_classes, upsample=True)\n#         self.conv = nn.Sequential(\n#             #nn.BatchNorm2d(2*n_feat).apply(init_weight),\n#             nn.ReLU(),\n#             nn.utils.spectral_norm(conv3x3(2*n_feat, 3)).apply(init_weight),\n#         )\n        \n#     def forward(self, z, label_ohe, codes_dim=24):\n#         '''\n#         z.shape = (*,120)\n#         label_ohe.shape = (*,n_classes)\n#         '''\n#         batch = z.size(0)\n#         z = z.squeeze()\n#         label_ohe = label_ohe.squeeze()\n#         codes = torch.split(z, codes_dim, dim=1)\n        \n#         x = self.fc(codes[0]) #->(*,16ch*4*4)\n#         x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n        \n#         condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n#         x = self.res1(x, condition)#->(*,16ch,8,8)\n        \n#         condition = torch.cat([codes[2], label_ohe], dim=1)\n#         x = self.res2(x, condition) #->(*,8ch,16,16)\n        \n#         condition = torch.cat([codes[3], label_ohe], dim=1)\n#         x = self.res3(x, condition) #->(*,4ch,32,32)\n        \n#         x = self.attn(x) #not change shape\n        \n#         condition = torch.cat([codes[4], label_ohe], dim=1)\n#         x = self.res4(x, condition) #->(*,2ch,64,64)\n        \n#         x = self.conv(x) #->(*,3,64,64)\n#         x = torch.tanh(x)\n#         return x\n    \n\n# class ResBlock_D(nn.Module):\n#     def __init__(self, in_channel, out_channel, downsample=True):\n#         super().__init__()\n#         self.layer = nn.Sequential(\n#             nn.ReLU(),\n#             nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight),\n#             nn.ReLU(),\n#             nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight),\n#         )\n#         self.shortcut = nn.Sequential(\n#             nn.utils.spectral_norm(conv1x1(in_channel,out_channel)).apply(init_weight),\n#         )\n#         if downsample:\n#             self.layer.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n#             self.shortcut.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n        \n#     def forward(self, inputs):\n#         x  = self.layer(inputs)\n#         x += self.shortcut(inputs)\n#         return x\n    \n\n# class Discriminator(nn.Module):\n#     def __init__(self, n_feat, n_classes=n_classes):\n#         super().__init__()\n#         self.res1 = ResBlock_D(3, n_feat, downsample=True)\n#         self.attn = Attention(n_feat)\n#         self.res2 = ResBlock_D(  n_feat, 2*n_feat, downsample=True)\n#         self.res3 = ResBlock_D(2*n_feat, 4*n_feat, downsample=True)\n#         self.res4 = ResBlock_D(4*n_feat, 8*n_feat, downsample=True)\n#         self.res5 = ResBlock_D(8*n_feat,16*n_feat, downsample=False)\n#         self.fc   = nn.utils.spectral_norm(nn.Linear(16*n_feat,1)).apply(init_weight)\n#         self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat).apply(init_weight)\n        \n#     def forward(self, inputs, label):\n#         batch = inputs.size(0) #(*,3,64,64)\n#         h = self.res1(inputs) #->(*,ch,32,32)\n#         h = self.attn(h) #not change shape\n#         h = self.res2(h) #->(*,2ch,16,16)\n#         h = self.res3(h) #->(*,4ch,8,8)\n#         h = self.res4(h) #->(*,8ch,4,4)\n#         h = self.res5(h) #->(*,16ch,4,4)\n#         h = torch.sum((F.relu(h)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n#         outputs = self.fc(h) #->(*,1)\n        \n#         if label is not None:\n#             embed = self.embedding(label) #->(*,16ch)\n#             outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n        \n#         outputs = torch.sigmoid(outputs)\n#         return outputs","6b74be25":"#BigGAN + leaky_relu           \nclass ResBlock_G(nn.Module):\n    def __init__(self, in_channel, out_channel, condition_dim, upsample=True):\n        super().__init__()\n        self.cbn1 = ConditionalNorm(in_channel, condition_dim)\n        self.upsample = nn.Sequential()\n        if upsample:\n            self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n        self.conv3x3_1 = nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight)\n        self.cbn2 = ConditionalNorm(out_channel, condition_dim)\n        self.conv3x3_2 = nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight) \n        self.conv1x1   = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n        \n    def forward(self, inputs, condition):\n        x  = F.leaky_relu(self.cbn1(inputs, condition))\n        x  = self.upsample(x)\n        x  = self.conv3x3_1(x)\n        x  = self.conv3x3_2(F.leaky_relu(self.cbn2(x, condition)))\n        x += self.conv1x1(self.upsample(inputs)) #shortcut\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, n_feat, codes_dim=24, n_classes=n_classes):\n        super().__init__()\n        self.fc   = nn.Sequential(\n            nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight)\n        )\n        self.res1 = ResBlock_G(16*n_feat, 16*n_feat, codes_dim+n_classes, upsample=True)\n        self.res2 = ResBlock_G(16*n_feat,  8*n_feat, codes_dim+n_classes, upsample=True)\n        #self.attn2 = Attention(8*n_feat)\n        self.res3 = ResBlock_G( 8*n_feat,  4*n_feat, codes_dim+n_classes, upsample=True)\n        self.attn = Attention(4*n_feat)\n        self.res4 = ResBlock_G( 4*n_feat,  2*n_feat, codes_dim+n_classes, upsample=True)\n        self.conv = nn.Sequential(\n            #nn.BatchNorm2d(2*n_feat).apply(init_weight),\n            nn.LeakyReLU(),\n            nn.utils.spectral_norm(conv3x3(2*n_feat, 3)).apply(init_weight),\n        )\n        \n    def forward(self, z, label_ohe, codes_dim=24):\n        '''\n        z.shape = (*,120)\n        label_ohe.shape = (*,n_classes)\n        '''\n        batch = z.size(0)\n        z = z.squeeze()\n        label_ohe = label_ohe.squeeze()\n        codes = torch.split(z, codes_dim, dim=1)\n        \n        x = self.fc(codes[0]) #->(*,16ch*4*4)\n        x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n        \n        condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n        x = self.res1(x, condition)#->(*,16ch,8,8)\n        \n        condition = torch.cat([codes[2], label_ohe], dim=1)\n        x = self.res2(x, condition) #->(*,8ch,16,16)\n        #x = self.attn2(x) #not change shape\n        \n        condition = torch.cat([codes[3], label_ohe], dim=1)\n        x = self.res3(x, condition) #->(*,4ch,32,32)\n        x = self.attn(x) #not change shape\n        \n        condition = torch.cat([codes[4], label_ohe], dim=1)\n        x = self.res4(x, condition) #->(*,2ch,64,64)\n        \n        x = self.conv(x) #->(*,3,64,64)\n        x = torch.tanh(x)\n        return x\n    \n\nclass ResBlock_D(nn.Module):\n    def __init__(self, in_channel, out_channel, downsample=True):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.LeakyReLU(0.2),\n            nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight),\n            nn.LeakyReLU(0.2),\n            nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight),\n        )\n        self.shortcut = nn.Sequential(\n            nn.utils.spectral_norm(conv1x1(in_channel,out_channel)).apply(init_weight),\n        )\n        if downsample:\n            self.layer.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n            self.shortcut.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n        \n    def forward(self, inputs):\n        x  = self.layer(inputs)\n        x += self.shortcut(inputs)\n        return x\n    \n\nclass Discriminator(nn.Module):\n    def __init__(self, n_feat, n_classes=n_classes):\n        super().__init__()\n        self.res1 = ResBlock_D(3, n_feat, downsample=True)\n        self.attn = Attention(n_feat)\n        self.res2 = ResBlock_D(  n_feat, 2*n_feat, downsample=True)\n        #self.attn2 = Attention(2*n_feat)\n        self.res3 = ResBlock_D(2*n_feat, 4*n_feat, downsample=True)\n        self.res4 = ResBlock_D(4*n_feat, 8*n_feat, downsample=True)\n        self.res5 = ResBlock_D(8*n_feat,16*n_feat, downsample=False)\n        self.fc   = nn.utils.spectral_norm(nn.Linear(16*n_feat,1)).apply(init_weight)\n        self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat).apply(init_weight)\n        \n    def forward(self, inputs, label):\n        batch = inputs.size(0) #(*,3,64,64)\n        h = self.res1(inputs) #->(*,ch,32,32)\n        h = self.attn(h) #not change shape\n        h = self.res2(h) #->(*,2ch,16,16)\n        #h = self.attn2(h) #not change shape\n        h = self.res3(h) #->(*,4ch,8,8)\n        h = self.res4(h) #->(*,8ch,4,4)\n        h = self.res5(h) #->(*,16ch,4,4)\n        h = torch.sum((F.leaky_relu(h,0.2)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n        outputs = self.fc(h) #->(*,1)\n        \n        if label is not None:\n            embed = self.embedding(label) #->(*,16ch)\n            outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n        \n        outputs = torch.sigmoid(outputs)\n        return outputs","97b801fe":"# #BigGAN\n# print(count_parameters(model=Generator(n_feat=27, codes_dim=24, n_classes=n_classes))) #z.shape=(*,120)\n# print(count_parameters(model=Discriminator(n_feat=33, n_classes=n_classes)))","2ce9865d":"def generate_img(netG,fixed_noise,fixed_aux_labels=None):\n    if fixed_aux_labels is not None:\n        gen_image = netG(fixed_noise,fixed_aux_labels).to('cpu').clone().detach().squeeze(0)\n    else:\n        gen_image = netG(fixed_noise).to('cpu').clone().detach().squeeze(0)\n    #denormalize\n    gen_image = gen_image*0.5 + 0.5\n    gen_image_numpy = gen_image.numpy().transpose(0,2,3,1)\n    return gen_image_numpy\n\ndef show_generate_imgs(netG,fixed_noise,fixed_aux_labels=None):\n    gen_images_numpy = generate_img(netG,fixed_noise,fixed_aux_labels)\n\n    fig = plt.figure(figsize=(25, 16))\n    # display 10 images from each class\n    for i, img in enumerate(gen_images_numpy):\n        ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n        plt.imshow(img)\n    plt.show()\n    plt.close()","6e11eefe":"#https:\/\/www.kaggle.com\/osciiart\/resnet34-mel-ver3-log-multi-hardaug?scriptVersionId=13887036\ndef cycle(iterable):\n    \"\"\"\n    dataloader\u3092iterator\u306b\u5909\u63db\n    :param iterable:\n    :return:\n    \"\"\"\n    while True:\n        for x in iterable:\n            yield x","4f2abd8c":"#BigGAN\ndef run(lr_G=3e-4,lr_D=3e-4, beta1=0.0, beta2=0.999, nz=120, epochs=2, \n        n_ite_D=1, ema_decay_rate=0.999, show_epoch_list=None, output_freq=10):\n#     #G:4M, D:3M params\n#     netG = Generator(n_feat=22, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=26, n_classes=n_classes).to(device)\n#     #G:6M, D:5M params\n#     netG = Generator(n_feat=27, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=33, n_classes=n_classes).to(device)\n#     #G:8M, D:7M params\n#     netG = Generator(n_feat=32, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=38, n_classes=n_classes).to(device)\n    #G:10M, D:8M params\n    netG = Generator(n_feat=36, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n    netD = Discriminator(n_feat=42, n_classes=n_classes).to(device)\n#     #G:14M, D:12M params\n#     netG = Generator(n_feat=42, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=50, n_classes=n_classes).to(device)\n#     #G:25M, D:22M params\n#     netG = Generator(n_feat=56, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=68, n_classes=n_classes).to(device)\n#     #G:32M, D:31M params\n#     netG = Generator(n_feat=64, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=80, n_classes=n_classes).to(device)\n\n    if EMA:\n        #EMA of G for sampling\n        netG_EMA = Generator(n_feat=42, codes_dim=24, n_classes=n_classes).to(device)\n        netG_EMA.load_state_dict(netG.state_dict())\n        for p in netG_EMA.parameters():\n            p.requires_grad = False\n\n        \n    print(count_parameters(netG))\n    print(count_parameters(netD))\n    \n    real_label = 0.9\n    fake_label = 0\n    \n    D_loss_list = []\n    G_loss_list = []\n    \n    dis_criterion = nn.BCELoss().to(device)\n\n    optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(beta1, beta2))\n    optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, beta2))\n    \n    fixed_noise = torch.randn(32, nz, 1, 1, device=device)\n    #fixed_noise = fixed_noise \/ fixed_noise.norm(dim=1, keepdim=True)\n    fixed_aux_labels     = np.random.randint(0,n_classes, 32)\n    fixed_aux_labels_ohe = np.eye(n_classes)[fixed_aux_labels]\n    fixed_aux_labels_ohe = torch.from_numpy(fixed_aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n    fixed_aux_labels_ohe = fixed_aux_labels_ohe.float().to(device, non_blocking=True)\n\n    netG.train()\n    netD.train()\n\n    ### training here\n    for epoch in range(1,epochs+1):\n        if elapsed_time(start_time) > TIME_LIMIT:\n            print(f'elapsed_time go beyond {TIME_LIMIT} sec')\n            break\n        D_running_loss = 0\n        G_running_loss = 0\n        for ii, data in enumerate(train_loader):\n            ############################\n            # (1) Update D network\n            ###########################\n            for _ in range(n_ite_D):\n                \n                if LABEL_NOISE:\n                    real_label = 0.9\n                    fake_label = 0\n                    if np.random.random() < LABEL_NOISE_PROB:\n                        real_label = 0\n                        fake_label = 0.9\n                    \n                # train with real\n                netD.zero_grad()\n                real_images = data['img'].to(device, non_blocking=True) \n                batch_size  = real_images.size(0)\n                dis_labels  = torch.full((batch_size, 1), real_label, device=device) #shape=(*,)\n                aux_labels  = data['label'].long().to(device, non_blocking=True) #shape=(*,)\n                dis_output = netD(real_images, aux_labels) #dis shape=(*,1)\n                errD_real  = dis_criterion(dis_output, dis_labels)\n                errD_real.backward(retain_graph=True)\n\n                # train with fake\n                noise  = torch.randn(batch_size, nz, 1, 1, device=device)\n                #noise = noise \/ noise.norm(dim=1, keepdim=True)\n                aux_labels     = np.random.randint(0,n_classes, batch_size)\n                aux_labels_ohe = np.eye(n_classes)[aux_labels]\n                aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n                aux_labels_ohe = aux_labels_ohe.float().to(device, non_blocking=True)\n                aux_labels = torch.from_numpy(aux_labels).long().to(device, non_blocking=True)\n                \n                fake = netG(noise, aux_labels_ohe) #output shape=(*,3,64,64)\n                dis_labels.fill_(fake_label)\n                dis_output = netD(fake.detach(),aux_labels)\n                errD_fake  = dis_criterion(dis_output, dis_labels)\n                errD_fake.backward(retain_graph=True)\n                D_running_loss += (errD_real.item() + errD_fake.item())\/len(train_loader)\n                optimizerD.step()\n\n            ############################\n            # (2) Update G network\n            ###########################\n            netG.zero_grad()\n            dis_labels.fill_(real_label)  # fake labels are real for generator cost\n            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n            aux_labels     = np.random.randint(0,n_classes, batch_size)\n            aux_labels_ohe = np.eye(n_classes)[aux_labels]\n            aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n            aux_labels_ohe = aux_labels_ohe.float().to(device, non_blocking=True)\n            aux_labels = torch.from_numpy(aux_labels).long().to(device, non_blocking=True)\n            fake  = netG(noise, aux_labels_ohe)\n            \n            dis_output = netD(fake, aux_labels)\n            errG   = dis_criterion(dis_output, dis_labels)\n            errG.backward(retain_graph=True)\n            G_running_loss += errG.item()\/len(train_loader)\n            optimizerG.step()\n        \n        if EMA:\n            #update netG_EMA\n            param_itr = cycle(netG.parameters())\n            for i,p_EMA in enumerate(netG_EMA.parameters()):\n                p = next(param_itr)\n                p_EMA.data = (1-ema_decay_rate)*p_EMA.data + ema_decay_rate*p.data\n                p_EMA.requires_grad = False\n        \n        #log\n        D_loss_list.append(D_running_loss)\n        G_loss_list.append(G_running_loss)\n        \n        #output\n        if epoch % output_freq == 0:\n            print('[{:d}\/{:d}] D_loss = {:.3f}, G_loss = {:.3f}, elapsed_time = {:.1f} min'.format(epoch,epochs,D_running_loss,G_running_loss,elapsed_time(start_time)\/60))\n            \n        if epoch in show_epoch_list:\n            print('epoch = {}'.format(epoch))\n            if not EMA:\n                show_generate_imgs(netG,fixed_noise,fixed_aux_labels_ohe)\n            elif EMA:\n                show_generate_imgs(netG_EMA,fixed_noise,fixed_aux_labels_ohe)\n            \n        if epoch % 100 == 0:\n            if not EMA:\n                torch.save(netG.state_dict(), f'generator_epoch{epoch}.pth')\n            elif EMA:\n                torch.save(netG_EMA.state_dict(), f'generator_epoch{epoch}.pth')\n    \n    if not EMA:\n        torch.save(netG.state_dict(), 'generator.pth')\n    elif EMA:\n        torch.save(netG_EMA.state_dict(), 'generator.pth')\n    torch.save(netD.state_dict(), 'discriminator.pth')\n    \n    res = {'netG':netG,\n           'netD':netD,\n           'nz':nz,\n           'fixed_noise':fixed_noise,\n           'fixed_aux_labels_ohe':fixed_aux_labels_ohe,\n           'D_loss_list':D_loss_list,\n           'G_loss_list':G_loss_list,\n          }\n    if EMA:\n        res['netG_EMA'] = netG_EMA\n        \n    return res","c782a9a7":"%%time\n#show_epoch_list = np.arange(0,100,1)\nshow_epoch_list = np.arange(0,500+10,1)\n\nres = run(lr_G=3e-4,lr_D=3e-4, beta1=0.0, beta2=0.999, nz=120, epochs=500, \n          n_ite_D=1, ema_decay_rate=None, show_epoch_list=show_epoch_list, output_freq=5)\n# res = run(lr_G=3e-4,lr_D=3e-4, beta1=0.5, beta2=0.999, nz=120, epochs=500, \n#           n_ite_D=1, ema_decay_rate=None, show_epoch_list=show_epoch_list, output_freq=10)","77bcc569":"plt.plot(res['D_loss_list'], label='D_loss')\nplt.plot(res['G_loss_list'], label='G_loss')\nplt.grid()\nplt.legend()\nplt.title('loss history');","bb348ff5":"#truncation_trick\ndef submission_generate_images(res, truncated=None):\n    im_batch_size=50\n    n_images=10000\n    if not EMA:\n        netG = res['netG']\n    elif EMA:\n        netG = res['netG_EMA']\n    nz   = res['nz']\n    if not os.path.exists('..\/output_images'):\n        os.mkdir('..\/output_images')\n    for i_batch in range(0, n_images, im_batch_size):\n        if truncated is not None:\n            flag = True\n            while flag:\n                z = np.random.randn(100*im_batch_size*nz)\n                z = z[np.where(abs(z)<truncated)]\n                if len(z)>=im_batch_size*nz:\n                    flag=False\n            gen_z = torch.from_numpy(z[:im_batch_size*nz]).view(im_batch_size,nz,1,1)\n            gen_z = gen_z.float().to(device)\n        else:\n            gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n#         gen_z = gen_z \/ gen_z.norm(dim=1, keepdim=True)\n        aux_labels     = np.random.randint(0,n_classes, im_batch_size)\n        aux_labels_ohe = np.eye(n_classes)[aux_labels]\n        aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n        aux_labels_ohe = aux_labels_ohe.float().to(device)\n        \n        gen_images = netG(gen_z,aux_labels_ohe)\n        gen_images = gen_images.to(\"cpu\").clone().detach() #shape=(*,3,h,w), torch.Tensor\n        #denormalize\n        gen_images = gen_images*0.5 + 0.5\n        for i_image in range(gen_images.size(0)):\n            save_image(gen_images[i_image, :, :, :],\n                       os.path.join(f'..\/output_images', f'image_{i_batch+i_image:05d}.png'))\n    shutil.make_archive(f'images', 'zip', f'..\/output_images')","8481aa6b":"%%time\nsubmission_generate_images(res,truncated=0.8)","ccc57511":"if not EMA:\n    gen_image_numpy = generate_img(res['netG'],res['fixed_noise'],res['fixed_aux_labels_ohe'])\nelif EMA:\n    gen_image_numpy = generate_img(res['netG_EMA'],res['fixed_noise'],res['fixed_aux_labels_ohe'])\nfor img in gen_image_numpy:\n    plt.imshow(img)\n    plt.show()","0607173b":"elapsed_time(start_time)","5be2be6c":"# Pre-Processing","ccfed53b":"# Import Libraries","9305bfab":"# Submission","37a426a6":"  --- codes_dim=24, nz=120, trun(0.8), batch_size=32 ---  \n  \n* v6 :  epochs=120, 14M(G),12M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=32.91  \n* v7 :  epochs=70, 32M(G),31M(D), lr=2e-4(G),4e-4(D), beta1=0.0, beta2=0.999 : FID=45.10  \n* v8 :  epochs=90, 25M(G),22M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=47.23  \n* v9 :  epochs=120, EMA_G(decay_rate=0.9999), 14M(G),12M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=39.79  \n* v10:  epochs=140, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=33.95  \n* v11:  epochs=120, 14M(G),12M(D), lr=1e-4(G),4e-4(D), beta1=0.0, beta2=0.999 : FID=57.68   \n* v12:  epochs=120, 14M(G),12M(D), lr=3e-4(G),5e-4(D), beta1=0.0, beta2=0.999 : FID=38.79   \n* v13:  epochs=180, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=31.06  \n* v14:  epochs=180, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=33.95  \n* v16:  epochs=220, 4M(G),3M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=35.44  \n* v19:  epochs=170, same as v13 : FID=37.92  \n* v20:  epochs=180, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.5, beta2=0.999 : FID=33.57  \n* v21:  epochs=200, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=128 : FID=60.34  \n* v23:  epochs=130, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=16 : FID=33.85  \n  \n  --- remove bn ---  \n  \n* v24:  epochs=100, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=50.77    \n* v25:  epochs=80, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=52.29    \n  \n  --- corrected self_attention + leaky_relu ---  \n  \n* v26: epochs=90, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=49.14  \n* v27: epochs=70, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=58.00  \n* v29: epochs=MAX, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=16 : FID=  \n* v30: epochs=MAX, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=  \n* v32: epochs=MAX, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=  \n  \n  --- more attention layers ---  \n  \n* v31: epochs=MAX, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=  \n\n  \n  --- fc with bn and relu ---  \n  \n* v22:  epochs=170, fc(bn+relu), 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=35.34  \n  \n  --- leaky_relu ---  \n  \n* v17:  epochs=170, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=34.15  \n  \n  --- leaky_relu + label_noise prob 0.1 ---  \n  \n* v18:  epochs=180, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=33.93   ","38083737":"# Train","54f7238e":"# Model","80c1a471":"# Device"}}