{"cell_type":{"1f50203e":"code","7d13bfc2":"code","ce5368fd":"code","124dffa2":"code","def72636":"code","5d3cd41c":"code","72a7a3e0":"code","022142c4":"code","ac89a9ff":"code","75672b0c":"code","e1e17ddd":"code","9adfb4aa":"code","223c3d8a":"code","8f0ec4b4":"code","00fff9c6":"code","431c1cd0":"code","517fedea":"code","606180c6":"code","b68e5333":"code","0f72de37":"code","6f2fb686":"code","b14bf766":"code","294b32fa":"code","4aa66ee1":"code","021c063f":"code","47e0f726":"code","ca1a0570":"code","5487095e":"code","a1646a82":"code","753228b6":"code","cfffb75c":"code","8720a067":"code","cfd306f8":"code","654d94b1":"code","7880e59e":"code","29719112":"markdown"},"source":{"1f50203e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nplt.rcParams.update({'font.size': 18})\nsns.set(font_scale=2.5)\n\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\n\n#ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inlinethe current session","7d13bfc2":"df_train = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ndf_train.head()","ce5368fd":"df_test = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\ndf_test.head()","124dffa2":"for col in df_train.columns : \n    msg = 'columnn : {:>10}\\t count of NaN value : {:.0f}'.format(col, 100 * (df_train[col].isnull().sum() ))\n    print(msg)","def72636":"for col in df_test.columns : \n    msg = 'columnn : {:>10}\\t count of NaN value : {:.0f}'.format(col, 100 * (df_test[col].isnull().sum() ))\n    print(msg)","5d3cd41c":"df_train.info()","72a7a3e0":"df_train.shape","022142c4":"df_train = df_train.drop(['id'], axis=1)\ndf_test = df_test.drop(['id'], axis=1)","ac89a9ff":"int_column = df_train.dtypes[df_train.dtypes =='int64'].index |  df_train.dtypes[df_train.dtypes =='float64'].index","75672b0c":"for col in int_column : \n    plt.figure(figsize=(12,4))\n    \n    plt.subplot(1,2,1)\n    sns.distplot(df_train[col])\n    plt.xlabel(col)\n    plt.ylabel('Density')\n    \n    plt.subplot(1,2,2)\n    sns.boxplot(x='Response', y = col, data =df_train, showmeans = True)\n    plt.xlabel('Target')\n    plt.ylabel(col)\n    \n    plt.show()","e1e17ddd":"obj_column = df_train.dtypes[df_train.dtypes == 'object'].index\nfor i in range(0, len(obj_column)) :\n    print(obj_column[i])\n    print(df_train[obj_column[i]].unique())\n    print()","9adfb4aa":"for i in range(0, len(obj_column)) :\n    fig, ax = plt.subplots(figsize=(15,4))\n\n    sns.countplot(x = obj_column[i], data = df_train)\n    sns.set(font_scale=1)\n\n    ax.set_title('{} Count Distribution'.format(obj_column[i]))","223c3d8a":"df_train.loc[df_train['Gender'] == 'Male', 'Gender'] = 0\ndf_train.loc[df_train['Gender'] == 'Female', 'Gender'] = 1\ndf_test.loc[df_test['Gender'] == 'Male', 'Gender'] = 0\ndf_test.loc[df_test['Gender'] == 'Female', 'Gender'] = 1\n\ndf_train.loc[df_train['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ndf_train.loc[df_train['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ndf_train.loc[df_train['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\ndf_test.loc[df_test['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ndf_test.loc[df_test['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ndf_test.loc[df_test['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\n\ndf_train.loc[df_train['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ndf_train.loc[df_train['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0\ndf_test.loc[df_test['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ndf_test.loc[df_test['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0","8f0ec4b4":"pd.set_option('max_columns', None)\ndf_train.head()","00fff9c6":"pd.set_option('max_columns', None)\ndf_test.head()","431c1cd0":"heatmap_data = df_train\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(20,20))\nplt.title('Pearson Correlation of Features', y = 1.05, size=15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True, annot_kws={'size':16})\n\ndel heatmap_data\n","517fedea":"df_train.info()","606180c6":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier","b68e5333":"df_train","0f72de37":"df_test","6f2fb686":"X_train, X_test, y_train, y_test = train_test_split(df_train.drop('Response',axis=1),\n                                                    df_train['Response'],\n                                                    test_size=.3, random_state = 42,\n                                                    stratify= df_train['Response'])","b14bf766":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = pd.DataFrame(X_train, columns=df_train.drop('Response',axis=1).columns)\nX_test = pd.DataFrame(X_test, columns=df_train.drop('Response',axis=1).columns)","294b32fa":"X_train","4aa66ee1":"from sklearn.linear_model import LogisticRegression\nmodels = [LogisticRegression(),\n          DecisionTreeClassifier(),\n          RandomForestClassifier(),\n          XGBClassifier()]\n\nnames = [ 'LogisticRegression',\n         'DecisionTreeClassifier',\n          'RandomForestClassifier',\n          'XGBClassifier']\n\nfor model,name in zip(models,names):\n    m = model.fit(X_train,y_train)\n    print(name, 'report:')\n    print('Train score',model.score(X_train,y_train))\n    print('Test score',model.score(X_test,y_test))\n    print()\n    print(\"Train confusion matrix:\\n\",confusion_matrix(y_train, model.predict(X_train)),'\\n')\n    print(\"Test confusion matrix:\\n\",confusion_matrix(y_test, model.predict(X_test)))\n    print('*'*50)","021c063f":"model = DecisionTreeClassifier(max_depth=3)\nmodel.fit(X_train, y_train)\n\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(20,15))\nplot_tree(model,\n          feature_names= df_train.drop('Response', axis=1).columns,  \n          class_names= ['yes','no'],\n          filled=True)\nplt.show()","47e0f726":"from sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score\n\nm = LogisticRegression().fit(X_train,y_train)\npred_y = m.predict(X_test)\nprint('*'*50)\nprint('Report')\nprint('model : LogisticRegression')\nprint('Train score',m.score(X_train,y_train))\nprint('Test score',m.score(X_test,y_test))\nprint()\nprint(\"accuracy: %.2f\" %accuracy_score(y_test, pred_y))\nprint(\"Precision : %.3f\" % precision_score(y_test, pred_y))\nprint(\"Recall : %.3f\" % recall_score(y_test, pred_y))\nprint(\"F1 : %.3f\" % f1_score(y_test, pred_y))\nprint()\nprint(\"Train confusion matrix:\\n\",confusion_matrix(y_train, m.predict(X_train)),'\\n')\nprint(\"Test confusion matrix:\\n\",confusion_matrix(y_test, m.predict(X_test)))\nprint('*'*50)","ca1a0570":"lr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nprint('Train score',lr_clf.score(X_train,y_train))\nprint('Test score',lr_clf.score(X_test,y_test))\n\nlrCoef = LogisticRegression().fit(X_train,y_train).coef_\nprint(lrCoef)","5487095e":"print(\"Coefficient of Logistic Regression\")\nfor i in range(0, len(lrCoef[0])) :\n    print('{} : {}'.format(X_train.columns[i], lrCoef[0][i]))","a1646a82":"coefdf = pd.DataFrame(data=X_train.columns, index=range(0, len(lrCoef[0])), columns=['Feature'])\ncoefdf['Coef'] = lrCoef[0]\ncoefdf['Absuolute num of Coef'] = abs(lrCoef[0])\ncoefdf = coefdf.sort_values(by='Absuolute num of Coef', ascending=False).reset_index(drop=True)\ncoefdf","753228b6":"fig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(data=coefdf, y=coefdf['Feature'], x=coefdf['Coef'])\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.title('Coefficient of Logistic Regression\\n(score : 88%)', fontsize=20)\nplt.xlabel('Coefficient')\n\nplt.savefig('Coefficient of Logistic Regression.png')\nplt.show()","cfffb75c":"import shap","8720a067":"XGBmodel = XGBClassifier().fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(XGBmodel)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test, plot_size=(10,5), show=False)\nplt.title('SHAP of XGBoost Classifier\\n(score : 88%)')\nplt.figsize=(10,5)\nplt.show()","cfd306f8":"RFmodel = RandomForestClassifier().fit(X_train, y_train)","654d94b1":"from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(RFmodel, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nplt.figure(figsize=(10,5))\nplt.title('Permutation Importance of Random Forest Classifier\\n(score : 87%)')\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\nplt.boxplot(result.importances[sorted_idx].T,\n            vert=False, labels=X_test.columns[sorted_idx]);","7880e59e":"# RFmodel = RandomForestClassifier().fit(X_train, y_train)\n\n# explainer = shap.TreeExplainer(RFmodel)\n# shap_values = explainer.shap_values(X_test)\n\n# shap.summary_plot(shap_values, X_test, plot_size=(10,5), show=False)\n# plt.title('SHAP of Random Forest Classifier\\n(score : 87%)')\n# plt.show()","29719112":"Whether already **has insurance or not**(Previously_Insured) was the most important attribute to predict the customer's response.\n\nWhether someone's vehicle got damaged or not was also significant feature as much as 'Previously_Insured'.\n\nThe older the vehicle is, the more likely to be interested.\n\nOlder people tend to pay more attention about insurance than young people do.\n\nHow long the customer has been associated with the company('Vintage') or how did reach to the customer('Policy_Sales_Channel'), even gender or region were not that influential.\n\nIn conclusion, targeting on the older customer how got damaged old vehichle would be the most cost-efficient marketing strategy no matter how(e-mail, phone call, in person or wathever it is)."}}