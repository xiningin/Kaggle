{"cell_type":{"132d6d4d":"code","6e5065b1":"code","98663abb":"code","ded30c7d":"code","07a4511a":"code","a8e58680":"code","c0892571":"code","8f763df8":"code","ab74fba5":"code","b5063519":"code","d5b09a33":"code","fc0e2dd1":"code","e57cd63d":"code","a93b2e49":"code","b3b879c1":"code","24d01817":"code","b156438f":"code","d4d844d8":"code","1e38d375":"code","b60f60b4":"code","b056ad7d":"code","181c7fa5":"code","cd2e5c82":"code","0c20cbcc":"code","058554bd":"markdown","1f880241":"markdown","6f1a2079":"markdown","ecdb32ad":"markdown","bea9778b":"markdown","810f263b":"markdown","eacae2fd":"markdown","ead456b1":"markdown","2a8720d4":"markdown","1e8f5939":"markdown","6213ebf7":"markdown","f5b8efa7":"markdown"},"source":{"132d6d4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e5065b1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score","98663abb":"df = pd.read_csv('..\/input\/kindle-reviews\/kindle_reviews.csv')\ndf.head()","ded30c7d":"df['overall'].value_counts()","07a4511a":"df.describe","a8e58680":"# 22 missing reviews and needs to be dropped\ndf.isnull().sum()","c0892571":"#drop the rows where there are no reviews\ndf.dropna(subset = ['reviewText'], inplace = True)\n\n#changing the reviewTime column to be of datetime type\ndf.reviewTime = pd.to_datetime(df.reviewTime)\n\n#creating a column with just the year\ndf['Year'] = df.reviewTime.dt.year\ndf.head()","8f763df8":"df.Year.value_counts().sort_index().plot(kind = 'bar')\nplt.title('Number of Reviews per Year')\nplt.xlabel('Year')\nplt.ylabel('Number of Reviews')\nplt.show()","ab74fba5":"df.reviewerID.value_counts().head(10).plot(kind = 'bar')\nplt.xticks(rotation = 80)\nplt.xlabel('UserID')\nplt.ylabel('Number of Reviews')\nplt.show()","b5063519":"df.overall.value_counts().plot(kind = 'bar')\nplt.title('Number of Good Ratings vs Bad Ratings')\nplt.xlabel('Rating Scales')\nplt.xticks(rotation = 0)\nplt.ylabel('Total ratings')\nplt.show()","d5b09a33":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","fc0e2dd1":"reviews = df[['reviewText', 'overall']]\nreviews.head()","e57cd63d":"punc = str.maketrans('', '', string.punctuation)\n#apply the empty mapping table to each element of the series where x is the review for one document.\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(word.translate(punc) for word in x.split()))\n\n#removing words that is non alpha\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x: ' '.join(word for word in x.split() if word.isalpha()))\n\n#making all words to be lowercase\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x: ' '.join(word.lower() for word in x.split()))\n\n#list of stop words\nstop = stopwords.words('english')\n\n#removing the stop words\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(word for word in x.split() if word not in stop))\n\n#Lemmatize words to reduce them to their root form. Note: added the pos = 'v' to reduce the incoming word to verb root\nlem = WordNetLemmatizer()\nreviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(lem.lemmatize(word, pos = 'v') for word in x.split()))\n","a93b2e49":"reviews['overall'] = np.where(reviews['overall'] > 2, 1, 0)\nreviews.head()","b3b879c1":"reviews.overall.value_counts()","24d01817":"def make_xy(data, vec, n):\n    \"\"\"Takes in a dataframe with text and labels and returns a vocabulary of some sort\n        depending on the vectorizer used.\n        \n        Arguments:\n        \n        data - the input dataframe containing the text and the labels\n        \n        vec - the chosen vectorizer to use\n        \n        n - the number of samples per class\n        \"\"\"\n    temp = pd.DataFrame()\n    for rating in range(2):\n        temp = pd.concat([temp, data[data.overall == rating].sample(n, random_state = 42)], ignore_index = True)\n        \n    #vectorizing the vocabulary\n    X = vec.fit_transform(temp.reviewText)\n    y = temp.overall\n    return X, y","b156438f":"count = CountVectorizer()\nX, y = make_xy(reviews, count, 20000)","d4d844d8":"#using TfidfVectorizer\ntfidf = TfidfVectorizer()\nXt, yt = make_xy(reviews, tfidf, 20000)","1e38d375":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 42, stratify = y)\n\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\nprint('Training Accuracy: {:.2f}'.format(nb.score(X_train, y_train)))\nprint('Testing Accuracy: {:.2f}'.format(nb.score(X_test, y_test)))","b60f60b4":"#Tuning the min_df parameter for the vectorizer and the alpha in the multinomial Naive Bayes\nbest_alpha = 0\nbest_min_df = 0\nbest_score = 0\n\n#param_grid\nalphas = [.1, 1, 5, 10, 50]\nmin_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n\n#iterate throughout the param grid\nfor alpha in alphas:\n    for m_df in min_dfs:\n        tfidf = TfidfVectorizer(min_df = m_df)\n        X, y = make_xy(reviews, tfidf, 20000)\n        nb = MultinomialNB(alpha = alpha)\n        score = np.mean(cross_val_score(nb, X, y, scoring = 'accuracy', cv = 3))\n        if score > best_score:\n            best_score = score\n            best_alpha = alpha\n            best_min_df = m_df\n            \nprint('Best_score: {:.2f}'.format(best_score))\nprint('Best_alpha: {:.2f}'.format(best_alpha))\nprint('Best_min_df: {:.5f}'.format(best_min_df))","b056ad7d":"tfidf = TfidfVectorizer(min_df = .001)\nX, y = make_xy(reviews,tfidf, 20000)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42, stratify = y)\n\nnb = MultinomialNB(alpha = 5)\nnb.fit(X_train, y_train)\nnaive_pred = nb.predict(X_test)\nnaive_prob = nb.predict_proba(X_test)[:,1]\n\n\nprint('Training Accuracy with TfidfVectorizer: {:.2f}'.format(nb.score(X_train, y_train)))\nprint('Testing Accuracy with TfidfVectorizer: {:.2f}'.format(nb.score(X_test, y_test)))\nprint('\\n Confusion matrix:')\nprint(confusion_matrix(y_test, naive_pred))\nprint('\\n Classification Report: ')\nprint(classification_report(y_test, naive_pred))","181c7fa5":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg_pred = logreg.predict(X_test)\nlogreg_prob = logreg.predict_proba(X_test)[:,1]\n\nprint('Training Accuracy with TfidfVectorizer: {:.2f}'.format(logreg.score(X_train, y_train)))\nprint('Testing Accuracy with TfidfVectorizer: {:.2f}'.format(logreg.score(X_test, y_test)))\nprint('\\n Confusion matrix:')\nprint(confusion_matrix(y_test, logreg_pred))\nprint('\\n Classification Report: ')\nprint(classification_report(y_test, logreg_pred))","cd2e5c82":"c_space = np.logspace(-5,6, 8)\nbest_score = []\nscore = 0\nbest_c = 0\n\n#exhaustive search for the best parameters\nfor value in c_space:\n    svm = LinearSVC(C = value)\n    score = np.mean(cross_val_score(nb, X_train, y_train, scoring = 'accuracy', cv = 5))\n    if score > best_score:\n        best_score = score\n        best_c = value\n\n#train model using the best parameters determine from kfold\ntry:\n    svm = LinearSVC(best_c)\n    svm.fit(X_train, y_train)\n    svm_pred = svm.predict(X_test)\n    \nexcept:\n    svm = LinearSVC()\n    svm.fit(X_train, y_train)\n    svm_pred = svm.predict(X_test)\n\nprint('Training Accuracy with TfidfVectorizer: {:.2f}'.format(svm.score(X_train, y_train)))\nprint('Testing Accuracy with TfidfVectorizer: {:.2f}'.format(svm.score(X_test, y_test)))\nprint('\\n Confusion Matrix: ')\nprint(confusion_matrix(y_test, svm_pred))\nprint('\\n Classification Report')\nprint(classification_report(y_test, svm_pred))","0c20cbcc":"sns.set_style('white')\nfig, ax = plt.subplots()\nax.plot([0,1], [0,1], linestyle = '--', color = 'darkorange')\n\nprobs = [naive_prob, logreg_prob, svm_pred]\nlabels = ['Naive Bayes', 'Logistic Regression', 'SVC']\nfor idx in range(len(probs)):\n    fpr, tpr, thresholds = roc_curve(y_test, probs[idx])\n    ax.plot(fpr, tpr, label = (labels[idx] + ' AUC score = %.2f' % roc_auc_score(y_test, probs[idx])))\n    \nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves')\nax.legend(loc = 'lower right')\nax.margins(x = 0.01, y =0.02)\nplt.show()","058554bd":"Hyperparameter tuning in Naive Bayes","1f880241":"### Logistic Regression","6f1a2079":"## Text Preprocessing","ecdb32ad":"#### Coverting rating to positive and negative\n* less than 3 = negative\n* greater than 3 = positive","bea9778b":"Train Test Split","810f263b":"## Reading data","eacae2fd":"Balancing data by downsampling to minority class","ead456b1":"### Multinomial Naive Bayes","2a8720d4":"### ROC curve","1e8f5939":"Ratings count","6213ebf7":"### SVC","f5b8efa7":"Top 10 users based on reviews"}}