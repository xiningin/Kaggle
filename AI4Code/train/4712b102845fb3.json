{"cell_type":{"09e73c8d":"code","16e25737":"code","3f7ff809":"code","b3f3b5ea":"code","23045d35":"code","578090f8":"code","a16b82e7":"code","300e11da":"code","d706311c":"code","ec07583f":"code","d9a8f28a":"code","21343a5c":"code","666611c7":"code","17fa18ea":"code","601d318c":"code","b3fdc970":"code","dfb05b95":"code","fc157218":"code","d0395ba3":"code","17f88483":"code","2c4ce719":"code","64706cbe":"code","03a5b441":"code","9dd21380":"code","a036f80d":"code","cb8ed3db":"code","0a0ffc92":"code","ad39f2ce":"code","f7b3d1ea":"code","0ce67d10":"code","dc0e2418":"code","f9a7a550":"code","697fa76a":"code","f011f45b":"code","8a42c736":"code","b697947e":"code","6b95dc6c":"code","ef11c42b":"code","96aeb810":"code","263ebda4":"code","0687b90b":"code","684c8ebb":"code","11371131":"code","c896b58d":"code","75e75314":"code","0f434024":"code","24c9d32f":"code","f95de53c":"code","8de9d815":"code","588ecef2":"code","eb6db626":"code","592c7099":"code","810477ec":"code","d747d37b":"code","4afacb07":"code","5801b1dd":"code","befeac5e":"markdown","dd849456":"markdown","f3968174":"markdown","e80c6af4":"markdown","80846923":"markdown","bad4dc7b":"markdown","9126cae6":"markdown","ea3419d2":"markdown","aee7aa74":"markdown","255f8bb0":"markdown","9ec618be":"markdown","80639241":"markdown","68dd8608":"markdown","b2fff80b":"markdown","50b340f8":"markdown","c5120d89":"markdown","657c7a17":"markdown","bdcea297":"markdown","bd4ed636":"markdown","657e823f":"markdown","1981e889":"markdown","b27ef82d":"markdown","4d52c12f":"markdown","b2a662bf":"markdown","09aa10a6":"markdown"},"source":{"09e73c8d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv('..\/input\/encoded_transformed_data.csv')","16e25737":"df.drop('Unnamed: 0',axis = 1,inplace = True)\ndf.head()","3f7ff809":"df.shape","b3f3b5ea":"df.isnull().sum()","23045d35":"df.dropna(inplace = True)","578090f8":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split\nX = df.drop('price',axis = 1)\ny = pd.DataFrame(df['price'],columns=['price'])","a16b82e7":"y_par_mod = pd.DataFrame(np.log1p(y['price']).values,columns=['log_price'])","300e11da":"X_train, X_test, y_train, y_test = train_test_split(X,y_par_mod,test_size = 0.2,random_state = 0)","d706311c":"lr = LinearRegression()\nridge = Ridge(alpha = 0.1,normalize = True) # Scaling is mandatory for all distance based calculations\nlasso = Lasso(alpha = 0.0001,normalize = True)\nelasticnet = ElasticNet(l1_ratio=0.01,alpha = 0.0001,normalize = True)","ec07583f":"model = lr.fit(X_train,y_train)\nsns.barplot(x = X_train.columns,y = model.coef_[0])\nplt.title('LR coefficients')\nplt.xticks(rotation = 90)\nplt.yticks(np.arange(-1,1,0.1))","d9a8f28a":"lr.score(X_test,y_test)","21343a5c":"pd.DataFrame(model.coef_,columns = X_train.columns)","666611c7":"model = ridge.fit(X_train,y_train)\nsns.barplot(x = X_train.columns,y = model.coef_[0])\nplt.title('Ridge coefficients')\nplt.xticks(rotation = 90)\nplt.yticks(np.arange(-1,1,0.1))","17fa18ea":"model = lasso.fit(X_train,y_train)\nsns.barplot(x = X_train.columns,y = model.coef_)\nplt.title('Lasso coefficients')\nplt.xticks(rotation = 90)\nplt.yticks(np.arange(-1,1,0.1))","601d318c":"model = elasticnet.fit(X_train,y_train)\nsns.barplot(x = X_train.columns,y = model.coef_)\nplt.title('Elasticnet coefficients')\nplt.xticks(rotation = 90)\nplt.yticks(np.arange(-1,1,0.1))","b3fdc970":"from sklearn.model_selection import KFold\nfrom sklearn import metrics\nkf = KFold(n_splits=10,shuffle=True,random_state=0)\nfor model,name in zip([lr,ridge,lasso,elasticnet],['Linear_Regression','Ridge','LASSO','ElasticNet']):\n    rmse = []\n    for train_idx,test_idx in kf.split(X,y_par_mod):\n        X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n        y_train,y_test = y_par_mod.iloc[train_idx,:],y_par_mod.iloc[test_idx,:]\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        mse = metrics.mean_squared_error(y_test,y_pred)\n        rmse.append(np.sqrt(mse))\n    print('RMSE scores : %0.03f (+\/- %0.08f) [%s]'%(np.mean(rmse), np.std(rmse,ddof = 1), name))\n    print()","dfb05b95":"np.mean(y_par_mod['log_price'])","fc157218":"np.expm1(8.261835079048764 + 0.588)","d0395ba3":"np.expm1(8.261835079048764 - 0.588)","17f88483":"np.expm1(8.261835079048764 + 0.588 + 0.00621691)","2c4ce719":"np.expm1(8.261835079048764 + 0.588 - 0.00621691)","64706cbe":"np.expm1(8.261835079048764 - 0.588 - 0.00621691)","03a5b441":"np.expm1(8.261835079048764 - 0.588 + 0.00621691)","9dd21380":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_scaled = ss.fit_transform(X)","a036f80d":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsRegressor()\nparams = {\n    'n_neighbors' : [1,5,10,20,30,40,50,60,70,80,90,100],\n    'weights' : ['uniform','distance']\n}\ngscv = GridSearchCV(knn,params,scoring='neg_mean_squared_error',cv = 3)\ngscv.fit(X_scaled,y)\nprint(gscv.best_params_)","cb8ed3db":"from sklearn.model_selection import GridSearchCV","0a0ffc92":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(random_state=0)\nrf = RandomForestRegressor(random_state=0)\nparam_dt = {\n    'criterion' : ['mse','mae'],\n    'max_depth' : np.arange(1,11)\n}\nparam_rf = {\n    'n_estimators' : [1,3,5,7,10],\n    'max_depth' : np.arange(1,11)\n}\ngscv_dt = GridSearchCV(dt,param_dt,scoring='neg_mean_squared_error',cv = 3)\ngscv_rf = GridSearchCV(rf,param_rf,scoring='neg_mean_squared_error',cv = 3)\ngscv_dt.fit(X_scaled,y)\ngscv_rf.fit(X_scaled,y)\nprint(gscv_dt.best_params_)\nprint(gscv_rf.best_params_)","ad39f2ce":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","f7b3d1ea":"knn = KNeighborsRegressor(n_neighbors= 10, weights= 'distance')\ndt = DecisionTreeRegressor(max_depth = 10)\nrf = RandomForestRegressor(max_depth = 10, n_estimators = 10)","0ce67d10":"#Tuning n_estimators for Bagging\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.model_selection import KFold,cross_val_score\nmodels = []\nmodels.append(('KNN',knn))\nmodels.append(('Decision Tree',dt))\nfor name,model in models:\n    mse_var = []\n    for val in np.arange(1,11):\n        model_bag = BaggingRegressor(base_estimator = model,n_estimators = val,random_state = 0)\n        kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n        results = cross_val_score(model_bag,X_scaled,y,cv = kfold,scoring='neg_mean_squared_error')\n        mse_var.append(np.var(results,ddof = 1))\n    print(name,np.argmin(mse_var)+1)","dc0e2418":"#Tuning n_estimators for Boosting\nfrom sklearn.ensemble import AdaBoostRegressor\nmodels = []\nmodels.append(('RF',rf))\nmodels.append(('Decision Tree',dt))\nfor name,model in models:  \n    mse_mean = []\n    for val in np.arange(1,11):\n        model_boost = AdaBoostRegressor(base_estimator = model,n_estimators = val,random_state = 0)\n        kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n        results = cross_val_score(model_boost,X_scaled,y,cv = kfold,scoring='neg_mean_squared_error')\n        mse_mean.append(np.mean(results))\n    print(name,np.argmax(mse_mean)+1)","f9a7a550":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\n#Bagging Models\nknn_bag = BaggingRegressor(base_estimator = knn,n_estimators = 3,random_state = 0)\nDT_bag = BaggingRegressor(base_estimator = dt,n_estimators = 2,random_state = 0)\n#Boosting models\nrf_boost = AdaBoostRegressor(base_estimator = rf,n_estimators = 8,random_state = 0)\nDT_boost = AdaBoostRegressor(base_estimator = dt,n_estimators = 10,random_state = 0)","697fa76a":"#Tuning n_estimators for GradientBoosting\nfrom sklearn.ensemble import GradientBoostingRegressor\nmse_mean = []\nfor val in np.arange(1,500):\n    model_boost = GradientBoostingRegressor(n_estimators = val,random_state = 0)\n    kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n    results = cross_val_score(model_boost,X_scaled,y,cv = kfold,scoring='neg_mean_squared_error')\n    mse_mean.append(np.mean(results))\nprint(np.argmax(mse_mean)+1)","f011f45b":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import KFold,cross_val_score\nGBR = GradientBoostingRegressor(n_estimators = 235,random_state = 0)","8a42c736":"models = []\nmodels.append(('DTree',dt))\nmodels.append(('DTree Bagged',DT_bag))\nmodels.append(('DTree Boosted',DT_boost))\nmodels.append(('KNN',knn))\nmodels.append(('KNN Bagged',knn_bag))\nmodels.append(('RF',rf))\nmodels.append(('RF Boosted',rf_boost))\nmodels.append(('Gradient Boost',GBR))","b697947e":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n    cv_results = cross_val_score(model,X_scaled,y,cv = kfold,scoring='neg_mean_squared_error')\n    results.append(cv_results)\n    names.append(name)\n    print(name,' : ',np.mean(np.sqrt(np.abs(cv_results))),' -- ',np.std(np.sqrt(np.abs(cv_results)),ddof = 1))\n\nfig = plt.figure(figsize=(20,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","6b95dc6c":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n    cv_results = cross_val_score(model,X_scaled,y,cv = kfold,scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    print(name,' : ',np.mean(cv_results))\n\nfig = plt.figure(figsize=(20,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","ef11c42b":"np.mean(y['price'])","96aeb810":"6846.93 + 2728.87","263ebda4":"6846.93 - 2728.87","0687b90b":"6846.93 + 2728.87 + 32.40","684c8ebb":"6846.93 - 2728.87 - 32.40","11371131":"6846.93 + 2728.87 - 32.40","c896b58d":"6846.93 - 2728.87 + 32.40","75e75314":"np.mean(y['price'])","0f434024":"6846.93 + 2968.21","24c9d32f":"6846.93 - 2968.21","f95de53c":"6846.93 + 2968.21 + 12.08","8de9d815":"6846.93 - 2968.21 - 12.08","588ecef2":"6846.93 + 2968.21 - 12.08","eb6db626":"6846.93 - 2968.21 + 12.08","592c7099":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nss.fit(X)\nfrom sklearn.externals import joblib\njoblib.dump(ss, 'ss.pkl')","810477ec":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(n_estimators = 235,random_state = 0)\ngbr.fit(ss.fit_transform(X),y)\n\nfrom sklearn.externals import joblib\njoblib.dump(gbr, 'gbr_model.pkl')","d747d37b":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\nknn = KNeighborsRegressor(n_neighbors= 10, weights= 'distance')\nknn_bag = BaggingRegressor(base_estimator = knn,n_estimators = 3,random_state = 0)\nknn_bag.fit(ss.fit_transform(X),y)\n\nfrom sklearn.externals import joblib\njoblib.dump(knn_bag, 'knn_bagged_model.pkl')","4afacb07":"def pred(age,powerPS, No_of_days_online, kilometer, vehicleType, fuelType, model, brand, state, mfg, damage,gearbox):\n    map_dict = np.load('map_dict.npy',allow_pickle = 'TRUE').item()\n#     gbr_model = joblib.load('gbr_model.pkl')\n    knn_bagged_model = joblib.load('knn_bagged_model.pkl')\n    yn = {'Yes' : 1, 'No' : 0}\n    km = map_dict['kilometer'][kilometer]\n    vt = map_dict['vehicleType'][vehicleType]\n    ft = map_dict['fuelType'][fuelType]\n    mod = map_dict['model'][model]\n    br = map_dict['brand'][brand]\n    st = map_dict['state'][state]\n    com = map_dict['CountryOfManufacture'][mfg]\n    d = yn[damage]\n    gb = yn[gearbox]\n    arr = np.array([age,powerPS, No_of_days_online, km, vt, ft, mod, br, st, com, d,gb])\n    ss = joblib.load('ss.pkl')\n    arr_t = ss.transform(arr.reshape(1,-1))\n#     pred_val = gbr_model.predict(arr_t)\n    pred_val = knn_bagged_model.predict(arr_t)\n    return pred_val.tolist()\n    ","5801b1dd":"pred(age = 19, powerPS = 75, No_of_days_online = 0, kilometer = 150000, vehicleType = 'small car',fuelType = 'petrol', \\\n     model = 'golf', brand = 'volkswagen', state = 'Bayern', mfg = 'Germany',damage = 'No', gearbox = 'Yes')","befeac5e":"* DTree  :  3166.656666292124  --  20.688297393962202\n* DTree Bagged  :  3048.6105212780576  --  22.834231612754063\n* DTree Boosted  :  2920.243202345049  --  84.49880036135615\n* KNN  :  2902.0643393016458  --  21.264488151889246\n* KNN Bagged  :  2968.214229098139  --  12.085551507327446\n* RF  :  2874.4468242214257  --  44.693950897618045\n* RF Boosted  :  2810.063420003883  --  40.17073280040146\n* Gradient Boost  :  2728.8733868780087  --  32.40123373222216","dd849456":"# Non Parametric Models","f3968174":"#### Predicting Price for record with  Actual Price of 1500 euros","e80c6af4":"#### Rsquare\n* DTree  :  0.8449657928744093\n* DTree Bagged  :  0.8563115350229993\n* DTree Boosted  :  0.8681219220876164\n* KNN  :  0.8698168777240948\n* KNN Bagged  :  0.8638136745699165\n* RF  :  0.8721904277046238\n* RF Boosted  :  0.8778941513969005\n* Gradient Boost  :  0.8848697399665386","80846923":"### KNN Bagged with 3 estimators, 10 neighbors and unweighted\/distance based voting","bad4dc7b":"Worst case model range : 3866.64 euros to 9827.22 euros - 5960.58\n\nAverage case model range : 3878.72 euros to 9815.14 euros - 5936.42\n\nBest case model range :  3890.8 euros to 9803.06 euros - 5912.25","9126cae6":"# Regression Models","ea3419d2":"### Gradient boost with 235 estimators","aee7aa74":"# Parametric Models","255f8bb0":"### Gradient boost estimation\n* estimators - rsquare --- rmse\/std_dev\n* 100 - 0.86752 --- 2927.37\/40.5795\n* 150 - 0.87714 --- 2818.93\/38.7061\n* 200 - 0.88235 --- 2758.48\/34.2317\n* 220 - 0.88382 --- 2741.23\/32.6705\n* 225 - 0.88419 --- 2736.91\/32.4699\n* 235 - 0.88486 --- 2728.87\/32.4012\n* 245 - 0.88549 --- 2721.38\/34.0657\n* 300 - 0.88841 --- 2686.44\/35.589","9ec618be":"Worst case model range : 2136.98 euros to 7015.73 euros - 4878.75\n\nAverage case model range : 2150.32 euros to 6972.24 euros - 4821.92\n\nBest case model range : 2163.73 euros to 6929.02 euros - 4765.29","80639241":"{'criterion': 'mse', 'max_depth': 10}\n\n{'max_depth': 10, 'n_estimators': 10}","68dd8608":"## Hyperparameter tuning","b2fff80b":"### Generating deployment file","50b340f8":"Gradient boost model seems to give us the best results. But if we use knn bagged model, we tend to increase the bias error by about 8% but would yeild us an improvement in variance error by approximately 62%","c5120d89":"RF 8\n\nDecision Tree 10","657c7a17":"{'n_neighbors': 10, 'weights': 'distance'}","bdcea297":"### Ridge Model","bd4ed636":"RMSE scores : 0.586 (+\/- 0.00652309) [Linear_Regression]\n\nRMSE scores : 0.588 (+\/- 0.00621691) [Ridge]\n\nRMSE scores : 0.596 (+\/- 0.00613996) [LASSO]\n\nRMSE scores : 1.047 (+\/- 0.00700431) [ElasticNet]\n\nLinear model seems to give us the best results. But if we use Ridge model, we tend to increase the bias error by about 0.34% but would yeild us an improvement in variance error by approximately 4.69%. If we use Lasso model, we tend to increase the bias error by about 1.67% but would yeild us an improvement in variance error by approximately 5.87%.","657e823f":"1608.231753692143","1981e889":"Worst case model range : 4085.66 euros to 9608.19 euros - 5522.54\n\nAverage case model range : 4118.06 euros to 9575.8 euros - 5457.73\n\nBest case model range : 4150.46 euros to 9543.4 euros - 5392.94","b27ef82d":"KNN 3\n\nDecision Tree 2","4d52c12f":"Gradient Boost no. of estimators : 235","b2a662bf":"Finally, I would select the KNN Bagged Model as my final model for deployment considering the tradeoff between bias and variance error.","09aa10a6":"# Scaling Data"}}