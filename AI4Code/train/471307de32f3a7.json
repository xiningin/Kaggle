{"cell_type":{"5a1c71d6":"code","63acdad7":"code","5c981077":"code","28602008":"code","6a2bdee8":"code","4d178dc9":"code","11671492":"code","d982a81b":"code","05b074b1":"code","70979f33":"code","2d17e648":"code","712d757d":"markdown","b636e253":"markdown","04a347f8":"markdown","11c6234a":"markdown","d6ef45ff":"markdown","15137eb7":"markdown","d1ae0d16":"markdown","b2e6ff06":"markdown","c8db647d":"markdown"},"source":{"5a1c71d6":"import pandas as pd\nimport numpy as np","63acdad7":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\nx_train = train[train.columns[1:]]\nx_test = test\ny_train = train[train.columns[0]]\n\ntrain.head()","5c981077":"from sklearn.preprocessing import MinMaxScaler\ntransformer = MinMaxScaler().fit(x_train, y_train)\nx_train = transformer.transform(x_train)\nx_test = transformer.transform(x_test)","28602008":"def train_predict():\n    from sklearn.svm import SVC\n    clf = SVC(C=10, kernel=\"rbf\")\n    clf.fit(x_train, y_train)\n    return clf.predict(x_test)","6a2bdee8":"%%time\ny_pred_original = train_predict()","4d178dc9":"sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsub.Label = y_pred_original\nsub.to_csv('submission_original.csv',index=False)","11671492":"!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log","d982a81b":"from sklearnex import patch_sklearn\npatch_sklearn()","05b074b1":"%%time\ny_pred_oprimized = train_predict()","70979f33":"np.mean(y_pred_oprimized == y_pred_original)","2d17e648":"sub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsub.Label = y_pred_oprimized\nsub.to_csv('submission_sklearnex.csv',index=False)","712d757d":"![image.png](attachment:image.png)","b636e253":"This time, the train and predict took **a little over minute**, which saved us almost **7 minutes**! Let\u2019s make sure that the quality has not changed:","04a347f8":"To get optimizations, patch scikit-learn using scikit-learn-intelex:","11c6234a":"# Optimizing Kaggle kernels using Intel(R) Extension for Scikit-learn*\n\nFor classical machine learning algorithms, we often use the most popular Python library, [scikit-learn](https:\/\/scikit-learn.org\/stable\/). We use it to fit models and search for optimal parameters, but\u202fscikit-learn\u202fsometimes works for hours, if not days. Speeding up this process is something anyone who uses scikit-learn would be interested in.\n\nI want to show you how to get results faster without changing the code. To do this, we will use another Python library,\u202f**[scikit-learn-intelex](https:\/\/github.com\/intel\/scikit-learn-intelex)**. It accelerates scikit-learn and does not require you changing the code written for scikit-learn.\n\nI will use a Kaggle notebook in which the train and predict of the SVM model executed in over 7 minutes.","d6ef45ff":"Let\u2019s run the same code to train and predict the SVM model:","15137eb7":"Let's take the training and predict into a separate function:","d1ae0d16":"The train and predict of the SVM model took almost 7 minutes. Let's try to use scikit-learn-intelex. First, download it:","b2e6ff06":"![image.png](attachment:0c784ed7-8541-439e-b162-70fc8f0401b6.png)","c8db647d":"With scikit-learn-intelex patching you can:\n\n- Use your scikit-learn code for training and inference without modification.\n- Train and predict scikit-learn models up to **7 times faster**.\n- Get the same quality of predictions as other tested frameworks.\n\n*Please, upvote if you like.*"}}