{"cell_type":{"b6b9da4e":"code","1d5b1400":"code","721db4a9":"code","65d1930c":"code","2973641e":"code","23af2105":"code","1cff718e":"code","1844ff08":"code","5069be96":"code","93199570":"code","bd5c5a0d":"code","beced9d6":"code","83f9a90d":"code","3044fae5":"code","58785d81":"code","400deacc":"code","d372404b":"code","c1b556a0":"code","0569617a":"markdown","23e2740b":"markdown","5d4c147e":"markdown","819043af":"markdown","a58e715e":"markdown","4f956724":"markdown","8afca75e":"markdown","1b83c71d":"markdown","bbd1154b":"markdown","b8601cd4":"markdown","8f2a2dea":"markdown","90f9c96a":"markdown","5296c498":"markdown","8853d626":"markdown","ddea83f6":"markdown","04c155e3":"markdown","9250ce85":"markdown","ba6d1349":"markdown","85c7608b":"markdown","34c9e0f6":"markdown"},"source":{"b6b9da4e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom PIL import Image\n\nsns.set(style='white', context='notebook', palette='deep')","1d5b1400":"# Load the data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","721db4a9":"# Get labels\nY_train = train[\"label\"]\n\n# Drop 'label' column to get only the images\nX_train = train.drop(labels = [\"label\"],axis = 1).values\n\n# Get test subset values\nX_test = test.values\n\n# free some space\ndel train\ndel test","65d1930c":"print('X_train\\'s shape: ' + str(X_train.shape))\nprint('Y_train\\'s shape: ' + str(Y_train.shape))\nprint()\nprint('X_test\\'s shape: ' + str(X_test.shape))","2973641e":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\n\nprint('X_train\\'s shape: ' + str(X_train.shape))\nprint('X_test\\'s shape: ' + str(X_test.shape))","23af2105":"indices = np.random.randint(0, 42000, 16)\n\nplt.figure(figsize=(20,5))\ni = 0\nfor index in indices:\n    ax = plt.subplot(2, 8, i + 1)\n    ax.set_title('Label: ' + str(Y_train[index]))\n    ax.imshow(X_train[index][:,:,0])\n    i += 1","1cff718e":"print(Y_train.value_counts())\n\nsns.countplot(Y_train)","1844ff08":"# Normalizing the data\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train \/= 255\nX_test \/= 255","5069be96":"Y_train = to_categorical(Y_train, num_classes = 10)\n\nprint('Y_train\\'s shape: ' + str(Y_train.shape))","93199570":"# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=0)","bd5c5a0d":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation = \"softmax\"))","beced9d6":"# Compile the model\nmodel.compile(optimizer='RMSprop' , loss='categorical_crossentropy', metrics=['accuracy'])","83f9a90d":"# learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor='val_acc', \n    patience=3, \n    verbose=1, \n    factor=0.5, \n    min_lr=0.00001)","3044fae5":"# Data augmentation to prevent overfitting\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","58785d81":"epochs = 30\nbatch_size = 100\n\nmodel.fit_generator(\n    datagen.flow(X_train,Y_train, batch_size=batch_size),\n    epochs = epochs,\n    validation_data = (X_val,Y_val),\n    verbose = 1,\n    callbacks=[learning_rate_reduction])","400deacc":"(loss_train, accuracy_train) = model.evaluate(X_train, Y_train)\nprint('Performance on train data:')\nprint('Loss: ' + str(loss_train))\nprint('Accuracy: ' + str(accuracy_train*100) + '%')\n\n(loss_val, accuracy_val) = model.evaluate(X_val, Y_val)\nprint('Performance on validation data:')\nprint('Loss: ' + str(loss_val))\nprint('Accuracy: ' + str(accuracy_val*100) + '%')","d372404b":"# predict results\nresults = model.predict(X_test)\n\n# select the index with the maximum probability\nresults = np.argmax(results,axis = 1)\n# insert the predictions into a pandas series\nresults = pd.Series(results,name=\"Label\")","c1b556a0":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"mnist_predictions_cnn_datagen.csv\",index=False)","0569617a":"### Label Encoding\nOur Y_train is currently an array of digits from 0 to 9 corresponding to the labels of each of our X_train examples. This will be an issue since we'll be using the  [softmax](https:\/\/en.wikipedia.org\/wiki\/Softmax_function) function which outputs n probability vectors in which the i-th element in the j-th vector corresponds corresponds to the probability of the i-th training example belonging to the j-th class.\n\nWe can slove this by using the to_categorical function from [keras.utils.np_utils](https:\/\/keras.io\/utils\/) to turn our Y_train into 10 one hot vectors that are compaitable with the output of the softmax function.","23e2740b":"## Preprocessing\nBefore building our model we need to make sure our data is in the right format and apply any sort of processing that can help our model make the best use of our data.\n\n### Normalization\nNormalizing our data can lead to noticeable improvements in training our model. Since we're dealing with greyscale images it's sufficient to divide the pixel values with 255 to get a range from 0 to 1. This helps in reducing the effect of illumination differences.","5d4c147e":"# MNIST CNN Classifier","819043af":"Let's seperate the images from the labels in out training subset.[](http:\/\/)","a58e715e":"Let's add the ImageId column to comply with the output format and output our prediction to csv file.","4f956724":"# Loading The Dataset\nOur dataset is divided into 2 subsets in 2 csv files, the training data is in `train.csv`and the test data is in `test.csv`. We'll use pandas read_csv method to read the csv files into dataframes.","8afca75e":"## Fitting The Model\nNow that we've built the model we just need to choose 2 more hyperparameters for the fitting process, the number of epochs and the batch size.\n\nOne epoch is when every example in our dataset is passed forward and backward through the neural network once. Epochs are split into batches of training examples that get trained at the same time.\n\nWe chose 30 epochs as it seems to be the sweet spot between a bearable training time and acceptable performance from our trained model. Increasing the number of epochs could potentially improve the performance depending on the model, but usually it plateaus at some point.\n\nAs for the batch size, the smaller it is the less memory we need to use for each pass since we're using less samples, but that comes at the cost of a less accurate estimation of the gradient. Also a larger batch size results in a faster training process for our network as we need to do fewer passes and updates.\n\nWe chose 100 for our batch size as it's neither too small nor too big and it divides our dataset evenly, avoiding the issue of an incomplete last batch.","1b83c71d":"## Importing Packages\nWe'll start by importing the packages we need.","bbd1154b":"## Building The Model\nWe'll be using [keras](https:\/\/keras.io\/), a high-level neural networks API capable of running on top of TensorFlow, to create our CNN.\n\nArchitecture of the network:\n- 2 Conv layers with 32 5x5 filters with Same padding and a ReLU activation followed by a Max Pooling layer of size 2x2\n- 2 Conv layers with 64 3x3 filters with Same padding and a ReLU activation followed by a Max Pooling layer of size 2x2 and a stride of 2\n- A Fully Connected layer with 256 neurons and a ReLU activation with a dropout rate of 0.3\n- A Softmax output layer ouputting our 10 probability vectors","b8601cd4":"Let's randomly plot a few of the images and their corresponding labels to better visualize the nature of our dataset.","8f2a2dea":"A very useful step that helps prevent overfitting is data augmentation. By adding different types of noise randomly to our data we can improve the generalizability of our model making it better equipped to deal with unseen data with unexpected distortions.","90f9c96a":"We now have a general idea of the shape of our data, but let's take a look at the number of examples we have for each label to make sure we're not underrepresenting any of the digits.","5296c498":"### Validation Set\nTo evaluate our model during training and to avoid overfitting  we'll split our data into a 90% training subset and a 10% validation subset. The validation subset will be used to evaluate our model's performance on unseen data at the end of every epoch.","8853d626":"We'll use the **RMSprop** with the default values as it seems to work well with our model and gives better results than other optimizers like the **adam** optimizer.\n\nFor the loss function we'll be using the **categorical_crossentropy** function since our output categorical format.","ddea83f6":"## Introduction\nThis notebook shows the step by step development of a Convolutional Neural Network (CNN) digit classifier for the MNIST dataset using the Keras API with Tensorflow as a backend.","04c155e3":"We'll use an annealing learning rate to increase the speed at which our optimizer converges. Using the keras ReduceLROnPlateau callback we'll reduce our learning rate by half if our accuracy doesn't improve for 3 epochs.","9250ce85":"## Model Evaluation\nLet's check how well our model performes on both the training data and the validation data.","ba6d1349":"# Data Exploration\nTo get a better sense of our dataset, let's explore it a little.\n\nLet's start by printing the shapes of our X, Y and test data.","85c7608b":"As we can see we have 42,000 training examples each consisting of a 28x28px image flattened into a 784px vector and its corresponding label (i.e. the handwritten digit in the image).\nOur test data has 28,000 images of the same shape as our training images but without the labels, since we'll be generating the predictions to submit in the [Digit Recognizer](https:\/\/www.kaggle.com\/c\/digit-recognizer) competition.\n\nSince we're developing a CNN we need our input be a 3D image not a 1D array of pixels. Let's reshape our X_train and X_test to 3D matrices.","34c9e0f6":"## Predicting The Test Data\nNow that we've trained our model, we can use it to predict our test data."}}