{"cell_type":{"7d8d0c69":"code","ebbf5ac2":"code","5bdae255":"code","ca2efcf9":"code","bd36d163":"code","a7e21799":"code","55e86435":"code","d2928130":"code","828b5977":"code","5eb1d698":"code","4fef5415":"code","19f96c45":"code","1f7cec04":"code","df8c4f9c":"code","99bc693c":"code","ea0b9e16":"code","df05b942":"code","32f96fb4":"markdown","f0137dad":"markdown","dfdec1f3":"markdown","78b38272":"markdown","e3474756":"markdown","54fe2232":"markdown","378b1918":"markdown","85274083":"markdown","59135105":"markdown","d44a89ab":"markdown","52693942":"markdown","49536e1f":"markdown"},"source":{"7d8d0c69":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","ebbf5ac2":"np.random.seed(0)","5bdae255":"df = pd.read_csv('..\/input\/shakespeare-plays\/Shakespeare_data.csv')\ndf.head()","ca2efcf9":"data = ' '.join(df['PlayerLine'].astype(str))\ndata[:100]","bd36d163":"chars = sorted(list(set(data)))\ndata_size, vocab_size = len(data), len(chars)\nprint('data has %d characters, %d unique.' % (data_size, vocab_size))","a7e21799":"char_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }","55e86435":"def sigmoid(input, deriv=False):\n    if deriv:\n        return input*(1-input)\n    else:\n        return 1 \/ (1 + np.exp(-input))\n\ndef tanh(input, deriv=False):\n    if deriv:\n        return 1 - input ** 2\n    else:\n        return np.tanh(input)","d2928130":"def softmax(input):\n    out = np.exp(input - np.max(input))\n    return out \/ out.sum()","828b5977":"N, h_size, o_size = vocab_size, vocab_size, vocab_size\nseq_length = 25\nlearning_rate = 1e-1","5eb1d698":"Wz = np.random.rand(h_size, N) * 0.1 - 0.05\nUz = np.random.rand(h_size, h_size) * 0.1 - 0.05\nbz = np.zeros((h_size, 1))\n\nWr = np.random.rand(h_size, N) * 0.1 - 0.05\nUr = np.random.rand(h_size, h_size) * 0.1 - 0.05\nbr = np.zeros((h_size, 1))\n\nWh = np.random.rand(h_size, N) * 0.1 - 0.05\nUh = np.random.rand(h_size, h_size) * 0.1 - 0.05\nbh = np.zeros((h_size, 1))\n\nWy = np.random.rand(o_size, h_size) * 0.1 - 0.05\nby = np.zeros((o_size, 1))\n\nbeta = [Wy,  Wh,  Wr,  Wz,  Uh,  Ur,  Uz,  by,  bh,  br,  bz]","4fef5415":"n, p = 0, 0\nsmooth_loss = -np.log(1.0\/vocab_size)*seq_length\n\nmdWy, mdWh, mdWr, mdWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\nmdUh, mdUr, mdUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\nmdby, mdbh, mdbr, mdbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n\ngamma = [mdWy,mdWh,mdWr,mdWz,mdUh,mdUr,mdUz,mdby,mdbh,mdbr,mdbz]","19f96c45":"def sample(h, seed_ix, n):\n    # Initialize first word of sample ('seed') as one-hot encoded vector.\n    x = np.zeros((vocab_size, 1))\n    x[seed_ix] = 1\n    ixes = [seed_ix]\n    \n    for t in range(n):\n        # Calculate update and reset gates\n        z = sigmoid(np.dot(Wz, x) + np.dot(Uz, h) + bz)\n        r = sigmoid(np.dot(Wr, x) + np.dot(Ur, h) + br)\n        \n        # Calculate hidden units\n        h_hat = tanh(np.dot(Wh, x) + np.dot(Uh, np.multiply(r, h)) + bh)\n        h = np.multiply(z, h) + np.multiply((1 - z), h_hat)\n        \n        # Regular output unit\n        y = np.dot(Wy, h) + by\n        \n        # Probability distribution\n        p = softmax(y)\n\n        # Choose next char according to the distribution\n        ix = np.random.choice(range(vocab_size), p=p.ravel())\n        x = np.zeros((vocab_size, 1))\n        x[ix] = 1\n        ixes.append(ix)\n    \n    return ixes","1f7cec04":"def forward(inputs, alpha):\n    x, z, r, h_hat, h, y, p, loss = alpha\n    \n    for t in range(len(inputs)):\n        # Set up one-hot encoded input\n        x[t] = np.zeros((vocab_size, 1))\n        x[t][inputs[t]] = 1\n        \n        # Calculate update and reset gates\n        z[t] = sigmoid(np.dot(Wz, x[t]) + np.dot(Uz, h[t-1]) + bz)\n        r[t] = sigmoid(np.dot(Wr, x[t]) + np.dot(Ur, h[t-1]) + br)\n        \n        # Calculate hidden units\n        h_hat[t] = tanh(np.dot(Wh, x[t]) + np.dot(Uh, np.multiply(r[t], h[t-1])) + bh)\n        h[t] = np.multiply(z[t], h[t-1]) + np.multiply((1 - z[t]), h_hat[t])\n        \n        # Regular output unit\n        y[t] = np.dot(Wy, h[t]) + by\n        \n        # Probability distribution\n        p[t] = softmax(y[t])\n        \n        # Cross-entropy loss\n        loss = -np.sum(np.log(p[t][targets[t]]))\n        \n    return x, z, r, h_hat, h, y, p, loss","df8c4f9c":"def backward(inputs, alpha):\n    \n    x, z, r, h_hat, h, y, p, loss = alpha\n    \n    dWy, dWh, dWr, dWz = np.zeros_like(Wy), np.zeros_like(Wh), np.zeros_like(Wr), np.zeros_like(Wz)\n    dUh, dUr, dUz = np.zeros_like(Uh), np.zeros_like(Ur), np.zeros_like(Uz)\n    dby, dbh, dbr, dbz = np.zeros_like(by), np.zeros_like(bh), np.zeros_like(br), np.zeros_like(bz)\n    dhnext = np.zeros_like(h[0])\n    \n    # Backward prop\n    for t in reversed(range(len(inputs))):\n        # \u2202loss\/\u2202y\n        dy = np.copy(p[t])\n        dy[targets[t]] -= 1\n        \n        # \u2202loss\/\u2202Wy and \u2202loss\/\u2202by\n        dWy += np.dot(dy, h[t].T)\n        dby += dy\n        \n        # Intermediary derivatives\n        dh = np.dot(Wy.T, dy) + dhnext\n        dh_hat = np.multiply(dh, (1 - z[t]))\n        dh_hat_l = dh_hat * tanh(h_hat[t], deriv=True)\n        \n        # \u2202loss\/\u2202Wh, \u2202loss\/\u2202Uh and \u2202loss\/\u2202bh\n        dWh += np.dot(dh_hat_l, x[t].T)\n        dUh += np.dot(dh_hat_l, np.multiply(r[t], h[t-1]).T)\n        dbh += dh_hat_l\n        \n        # Intermediary derivatives\n        drhp = np.dot(Uh.T, dh_hat_l)\n        dr = np.multiply(drhp, h[t-1])\n        dr_l = dr * sigmoid(r[t], deriv=True)\n        \n        # \u2202loss\/\u2202Wr, \u2202loss\/\u2202Ur and \u2202loss\/\u2202br\n        dWr += np.dot(dr_l, x[t].T)\n        dUr += np.dot(dr_l, h[t-1].T)\n        dbr += dr_l\n        \n        # Intermediary derivatives\n        dz = np.multiply(dh, h[t-1] - h_hat[t])\n        dz_l = dz * sigmoid(z[t], deriv=True)\n        \n        # \u2202loss\/\u2202Wz, \u2202loss\/\u2202Uz and \u2202loss\/\u2202bz\n        dWz += np.dot(dz_l, x[t].T)\n        dUz += np.dot(dz_l, h[t-1].T)\n        dbz += dz_l\n        \n        # All influences of previous layer to loss\n        dh_fz_inner = np.dot(Uz.T, dz_l)\n        dh_fz = np.multiply(dh, z[t])\n        dh_fhh = np.multiply(drhp, r[t])\n        dh_fr = np.dot(Ur.T, dr_l)\n        \n        # \u2202loss\/\u2202h\ud835\udc61\u208b\u2081\n        dhnext = dh_fz_inner + dh_fz + dh_fhh + dh_fr\n        \n    return dWy, dWh, dWr, dWz, dUh, dUr, dUz, dby, dbh, dbr, dbz","99bc693c":"def train(inputs, targets, hprev):\n    # Initialize variables\n    # x, z, r, h_hat, h, y, p, loss\n    alpha = {}, {}, {}, {}, {-1: hprev}, {}, {}, 0\n    sequence_loss = 0\n\n    # Forward prop\n    alpha = forward(inputs, alpha)\n    sequence_loss += alpha[-1]\n\n    # Backward prop\n    grads = backward(inputs, alpha)\n\n    # alpha[4] - h\n    return sequence_loss, grads, alpha[4][len(inputs) - 1]","ea0b9e16":"def update(grads, beta):\n    for param, dparam, mem in zip(beta,grads,gamma):\n        # clip the array to be between -5 and 5\n        np.clip(dparam, -5, 5, out=dparam)\n        mem += dparam * dparam\n        param += -learning_rate * dparam \/ np.sqrt(mem + 1e-8)\n    \n    return beta","df05b942":"for r in range(100001):\n    # Reset memory if appropriate\n    if p + seq_length + 1 >= len(data) or n == 0:\n        hprev = np.zeros((h_size, 1))\n        p = 0\n    \n    # Get input and target sequence\n    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n\n    # Sample from model and print result\n    if n % 20000 == 0:\n        sample_ix = sample(hprev, inputs[0], 1000)\n        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n        print('----\\n%s\\n----' % (txt, ))\n\n    # Get gradients for current model based on input and target sequences\n    loss, grads, hprev = train(inputs, targets, hprev)\n    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n\n    # Print loss information\n    if n % 20000 == 0:\n        print('iter %d, loss: %f, smooth loss: %f' % (n, loss, smooth_loss))\n\n    # Update model with adagrad (stochastic) gradient descent\n    beta = update(grads, beta)\n\n    # Prepare for next iteration\n    p += seq_length\n    n += 1","32f96fb4":"<img align=left src=\"https:\/\/i.imgur.com\/JsgLLh6.jpg\" width=\"300px\">","f0137dad":"As you can see GRU is a really powerful algorithm, it allows to create new sentences.<br>\nThe reason why it's not creating cohesive phrases is because:<br>\n1. data is not cleaned (lower cases, wild card symbols, ponctuation)\n2. need of transfer learning with pre-trained models plus GRU so that the sentences belong to a context","dfdec1f3":"<img align=left width=600 src='https:\/\/i.imgur.com\/7DYpPBR.png'\/>","78b38272":"<a id='architecture'><\/a>\n# GRU Architecture","e3474756":"## Content\n\n1. [Data Preparation](#preparation)\n2. [Activation Functions and Derivatives](#activation)\n2. [GRU Architecture](#architecture)\n3. [Paramaters](#parameters)\n4. [Implementation](#implementation)","54fe2232":"<a id='implementation'><\/a>\n# Implementation","378b1918":"<a id='activation'><\/a>\n# Activation Functions and Derivatives","85274083":"<a id='conclusion'><\/a>\n# Conclusion","59135105":"<a id='parameters'><\/a>\n# Parameters","d44a89ab":"<a id='preparation'><\/a>\n# Data Preparation","52693942":"# Shakespear GRU from scratch","49536e1f":"## Notebook Goal\nThe goal of the notebook is:\n1. Show the GRU from scratch implementation\n2. Try to use GRU to create new Shakespear script"}}