{"cell_type":{"dea280c9":"code","7a93730d":"code","4b555045":"code","45eb881a":"code","e3fc4b1f":"code","d3291657":"code","40418cc3":"code","4a473c8b":"code","b96feb42":"code","dcf38985":"code","df0e76fd":"code","3b418945":"code","24072ca7":"code","7c631497":"code","3488b364":"code","40f81270":"code","4893854f":"code","e5b72328":"code","ed6e9704":"code","4fbaf73c":"code","997b3838":"code","85574763":"code","195936aa":"markdown","58bdb8e3":"markdown","26aa0ad9":"markdown","cb72da8e":"markdown","ebe389e1":"markdown"},"source":{"dea280c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a93730d":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","4b555045":"data = pd.read_csv(\"\/kaggle\/input\/bank-marketing-term-deposit\/bank_customer_survey.csv\")\ndata.shape","45eb881a":"data.head()","e3fc4b1f":"data.isnull().sum().sum()","d3291657":"data['y'].value_counts(normalize=True)","40418cc3":"df_cf0 = data[data['y'] == 0]\ndf_cf1 = data[data['y'] == 1]\nprint(df_cf0.shape, df_cf1.shape)","4a473c8b":"#take sample of df0 as per 1\ndf_cf0 = df_cf0.sample(df_cf1.shape[0], random_state=10)\nprint(df_cf0.shape, df_cf1.shape)","b96feb42":"df_new = pd.concat([df_cf0, df_cf1])\ndf_new.shape","dcf38985":"df_new.head()","df0e76fd":"#Encoding month with month numbers\n#df_new.month.unique()\nmonth_code = {'may':5, 'mar':3, 'jun':6, 'feb':2, 'jul':7, 'aug':8,\n              'apr':4, 'jan':1, 'nov':11,'dec':12, 'sep':9, 'oct':10}\n\ndf_new['month'] = df_new['month'].map(month_code)","3b418945":"# Encode education\nedc_code = {'unknown':0, 'primary':1, 'secondary':2, 'tertiary':3}\ndf_new['education'] = df_new['education'].map(edc_code)\n\n#Encode P_outcome\np_out_code = {'unknown':0, 'failure':1, 'other':2, 'success':3}\ndf_new['poutcome'] = df_new['poutcome'].map(p_out_code) ","24072ca7":"#onhot all remaining columns\n#cat_cols = ['job', 'marital', 'default', 'housing', 'loan', 'contact']\ndf_new = pd.get_dummies(df_new, drop_first=True)","7c631497":"#look at data\ndf_new.head()","3488b364":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense","40f81270":"X = df_new.drop('y', axis=1)\nY = df_new['y']\n\n#train-test split\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=10)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","4893854f":"#num of input features\nn_features = x_train.shape[1]","e5b72328":"#Define Model\nmodel = Sequential()\nmodel.add(Dense(10, activation=\"relu\", input_shape=(n_features,)))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","ed6e9704":"#compile model\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","4fbaf73c":"#train\nhistory = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0, shuffle=False)","997b3838":"# evaluate the model\nloss, acc = model.evaluate(x_test, y_test, verbose=0)\nprint('Test Accuracy: %.3f' % acc)","85574763":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'], label=\"train-loss\")\nplt.plot(history.history['val_loss'], label=\"val-loss\")\nplt.legend()\nplt.show()","195936aa":"**Thank you. If you like my Notebook, Please Upvote it, It gives immense motivation to move forward with Data Science Journey.**","58bdb8e3":"## Modelling","26aa0ad9":"**One-Hot Encoding**","cb72da8e":"## Sampling-SubSampling \nwe have to balance the dataset according to output classes","ebe389e1":"### Categorical Encoding\n**Label Encoding**"}}