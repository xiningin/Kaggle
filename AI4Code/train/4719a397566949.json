{"cell_type":{"fac54b5f":"code","a4d4d283":"code","3286b1b5":"code","dc14b2cc":"code","f7290fb9":"code","3f95d9ee":"code","7ab6689d":"code","8658fe77":"code","88883430":"code","8fe7dadb":"code","037fe41a":"code","d46d38a1":"code","058c7ec6":"code","42db142e":"code","5a962580":"code","96ddfbd0":"code","e6912aa7":"code","424f3939":"code","854731ae":"code","2fa03be4":"code","2f712653":"code","9ca5a1d9":"code","cd1e326b":"code","1ef353cf":"code","8530200c":"markdown","fdee0b62":"markdown","1ff9c13a":"markdown","4869b7cc":"markdown","c01394b6":"markdown","f853480c":"markdown","b9905b9b":"markdown","01db0de9":"markdown"},"source":{"fac54b5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4d4d283":"df = pd.read_csv('..\/input\/design-thinking-arxiv\/arxiv_papers.csv', delimiter=',', encoding = \"utf8\")\n\ndf.head()","3286b1b5":"!pip install -U spacy","dc14b2cc":"!python -m spacy download en_core_web_lg","f7290fb9":"!python -m spacy download en_core_web_sm","3f95d9ee":"import spacy\nnlp = spacy.load('en_core_web_sm')","7ab6689d":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\n# Create a nlp object\ndoc = nlp(\"Transformers have shown great potential in computer vision tasks. A common\\nbelief is their attention-based token mixer module contributes most to their\\ncompetence. However, recent works show the attention-based module in\\ntransformers can be replaced by spatial MLPs and the resulted models still\\nperform quite well.\")\n\nfor token in doc:\n    print(token.text, token.pos_, token.dep_)","8658fe77":"nlp.pipe_names","88883430":"nlp.disable_pipes('tagger', 'parser')","8fe7dadb":"nlp.pipe_names","037fe41a":"doc = nlp(\"To verify this, we deliberately\\nreplace the attention module in transformers with an embarrassingly simple\\nspatial pooling operator to conduct only the most basic token mixing.\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\")\nfor token in doc:\n    print(token.text)","d46d38a1":"nlp = spacy.load('en_core_web_sm')  #Must write that otherwise won't render correctly\n\n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))\n","058c7ec6":"from spacy import displacy\n\ndoc = nlp(\"\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\")\ndisplacy.render(doc, style=\"dep\" , jupyter=True)","42db142e":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.dep_)","5a962580":"spacy.explain(\"nsubj\"), spacy.explain(\"ROOT\"), spacy.explain(\"aux\"),spacy.explain('nmod'), spacy.explain(\"advcl\"), spacy.explain(\"dobj\")","96ddfbd0":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"We observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\")\n \n# Iterate over the tokens\nfor token in doc:\n    # Print the token and its part-of-speech tag\n    print(token.text, \"-->\", token.lemma_)","e6912aa7":"nlp = spacy.load('en_core_web_sm')\n\n# Create an nlp object\ndoc = nlp(\"Competitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\")\n \nsentences = list(doc.sents)\nlen(sentences)","424f3939":"for sentence in sentences:\n     print (sentence)","854731ae":"nlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Competitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\")\n#See the entity present\nprint(doc.ents)\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)","2fa03be4":"from spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc= nlp(\"\"\"Competitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines DeiT-B\/ResMLP-B24 by 0.3%\/1.1% accuracy\\nwith 35%\/52% f\"\"\")\n\nentities=[(i, i.label_, i.label) for i in doc.ents]\nentities","2f712653":"displacy.render(doc, style = \"ent\",jupyter = True)","9ca5a1d9":"nlp = spacy.load(\"en_core_web_lg\")\ntokens = nlp(\"Competitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines\")\n\nfor token in tokens:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)","cd1e326b":"nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\ntokens = nlp(\"Competitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer\/MLP-like baselines\")\n\nfor token1 in tokens:\n    for token2 in tokens:\n        print(token1.text, token2.text, token1.similarity(token2))","1ef353cf":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nnlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger model!\ntokens = nlp(\"And God said, Let there be light: and there was light. Jesus, The grace of the Lord Jesus be with the saints. So be it. I give praise to God at all times and make prayer for you. Naming the light, Day, and the dark, Night. And there was evening and there was morning, the first day. The grace of the Lord Jesus be with the saints. So be it.\")\n\nnewText =''\nfor word in tokens:\n if word.pos_ in ['ADJ', 'NOUN']:\n  newText = \" \".join((newText, word.text.lower()))\n\nwordcloud = WordCloud(stopwords=STOPWORDS).generate(newText)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8530200c":"#Part-Of-Speech (POS) Tagging","fdee0b62":"#Tokenization","1ff9c13a":"#Similarity","4869b7cc":"#NLP After disable pipes","c01394b6":"#That's all after checking that Spacy are not deprecated yet.","f853480c":"#Lemmatization","b9905b9b":"#Named Entity Recognition (NER)","01db0de9":"![](https:\/\/avatars.githubusercontent.com\/u\/45442578?s=280&v=4)github.com"}}