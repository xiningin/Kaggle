{"cell_type":{"e646a7da":"code","e060addb":"code","ddfe4090":"code","3bfe15ee":"code","fa999b80":"code","7b774c9b":"code","efb663b1":"code","8dfc3c59":"code","8c58a0a0":"code","9325cbbd":"code","e4a806fb":"code","cfcb555a":"code","33eb83a1":"code","e7b05f32":"code","fdbdafb8":"code","b48ac713":"code","435f4077":"code","42127e5f":"code","78659f39":"code","fefc5a2e":"code","3706b3d0":"code","9fa85206":"code","e1740970":"code","5f8b5601":"markdown","7d605053":"markdown","39b119f0":"markdown","8f6903bc":"markdown","05ad6cd4":"markdown","08c16ed5":"markdown","13b811c3":"markdown","d4df04c8":"markdown","2edfcd24":"markdown","63fc0927":"markdown","5e4474bd":"markdown","6c2542d8":"markdown","4410de33":"markdown","af13acec":"markdown"},"source":{"e646a7da":"import numpy as np\nimport pandas as pd\nimport os\nimport gc","e060addb":"x = pd.Series([1111.5, 1111.9, 30001, 30007]) \nprint(x)\nprint('nunique:', x.nunique(), '\\n')\n\ny = x.astype(np.float16)\nprint(y)\nprint('nunique:', y.nunique())","ddfe4090":"x = pd.Series(['a', 'b', 'c'])\ny = pd.Series(['a', 'c', 'a'])\nprint(x.astype('category').cat.codes)\nprint(y.astype('category').cat.codes)","3bfe15ee":"# safe downcast\ndef sd(col, max_loss_limit=0.001, avg_loss_limit=0.001, na_loss_limit=0, n_uniq_loss_limit=0, fillna=0):\n    \"\"\"\n    max_loss_limit - don't allow any float to lose precision more than this value. Any values are ok for GBT algorithms as long as you don't unique values.\n                     See https:\/\/en.wikipedia.org\/wiki\/Half-precision_floating-point_format#Precision_limitations_on_decimal_values_in_[0,_1]\n    avg_loss_limit - same but calculates avg throughout the series.\n    na_loss_limit - not really useful.\n    n_uniq_loss_limit - very important parameter. If you have a float field with very high cardinality you can set this value to something like n_records * 0.01 in order to allow some field relaxing.\n    \"\"\"\n    is_float = str(col.dtypes)[:5] == 'float'\n    na_count = col.isna().sum()\n    n_uniq = col.nunique(dropna=False)\n    try_types = ['float16', 'float32']\n\n    if na_count <= na_loss_limit:\n        try_types = ['int8', 'int16', 'float16', 'int32', 'float32']\n\n    for type in try_types:\n        col_tmp = col\n\n        # float to int conversion => try to round to minimize casting error\n        if is_float and (str(type)[:3] == 'int'):\n            col_tmp = col_tmp.copy().fillna(fillna).round()\n\n        col_tmp = col_tmp.astype(type)\n        max_loss = (col_tmp - col).abs().max()\n        avg_loss = (col_tmp - col).abs().mean()\n        na_loss = np.abs(na_count - col_tmp.isna().sum())\n        n_uniq_loss = np.abs(n_uniq - col_tmp.nunique(dropna=False))\n\n        if max_loss <= max_loss_limit and avg_loss <= avg_loss_limit and na_loss <= na_loss_limit and n_uniq_loss <= n_uniq_loss_limit:\n            return col_tmp\n\n    # field can't be converted\n    return col\n\n\ndef reduce_mem_usage_sd(df, deep=True, verbose=False, obj_to_cat=False):\n    numerics = ['int16', 'uint16', 'int32', 'uint32', 'int64', 'uint64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=deep).sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        # collect stats\n        na_count = df[col].isna().sum()\n        n_uniq = df[col].nunique(dropna=False)\n        \n        # numerics\n        if col_type in numerics:\n            df[col] = sd(df[col])\n\n        # strings\n        if (col_type == 'object') and obj_to_cat:\n            df[col] = df[col].astype('category')\n        \n        if verbose:\n            print(f'Column {col}: {col_type} -> {df[col].dtypes}, na_count={na_count}, n_uniq={n_uniq}')\n        new_na_count = df[col].isna().sum()\n        if (na_count != new_na_count):\n            print(f'Warning: column {col}, {col_type} -> {df[col].dtypes} lost na values. Before: {na_count}, after: {new_na_count}')\n        new_n_uniq = df[col].nunique(dropna=False)\n        if (n_uniq != new_n_uniq):\n            print(f'Warning: column {col}, {col_type} -> {df[col].dtypes} lost unique values. Before: {n_uniq}, after: {new_n_uniq}')\n\n    end_mem = df.memory_usage(deep=deep).sum() \/ 1024 ** 2\n    percent = 100 * (start_mem - end_mem) \/ start_mem\n    if verbose:\n        print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))\n    return df\n\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-data-minification\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024 ** 2 # just added \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage(deep=True).sum() \/ 1024 ** 2\n    percent = 100 * (start_mem - end_mem) \/ start_mem\n    print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))\n    return df","fa999b80":"def get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) \/ 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() \/ 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) \/ 1024 ** 2))","7b774c9b":"tmp = pd.read_pickle('\/kaggle\/input\/concat-dataframes\/concat.pkl')","efb663b1":"tmp.sample(20).sort_index()","8dfc3c59":"tmp.loc['test'].sample(10).sort_index()","8c58a0a0":"# cache a mini-dataset for examples\nexample = tmp[['card1', 'TransactionAmt', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C14']].copy()","9325cbbd":"stats = get_stats(tmp)\nstats","e4a806fb":"tmp1 = reduce_mem_usage_sd(tmp.copy(), verbose=True)","cfcb555a":"stats1 = get_stats(tmp1)","33eb83a1":"tmp1.to_pickle('safe_memreduced_2.5gb.pkl')","e7b05f32":"tmp1 = reduce_mem_usage_sd(tmp1, verbose=True, obj_to_cat=True)","fdbdafb8":"tmp1.to_pickle('safe_memreduced_1gb.pkl')","b48ac713":"# don't copy, as the original df is not needed anymore\ntmp2 = reduce_mem_usage(tmp, verbose=True)","435f4077":"overall_stats = pd.concat([stats.add_prefix('a_'), stats1.add_prefix('b_'), get_stats(tmp1).add_prefix('c_')], axis=1)","42127e5f":"overall_stats","78659f39":"new_feature = (example.groupby('card1')['TransactionAmt'].transform('mean'))\n\nprint_header()\nprint_values('mean_amt', 'original', new_feature)","fefc5a2e":"new_feature2 = sd(new_feature)\n\nprint_header()\nprint_values('mean_amt', 'default sd():', new_feature2)","3706b3d0":"new_feature3 = sd(new_feature, n_uniq_loss_limit=100)\n\nprint_header()\nprint_values('mean_amt', 'allow uniq loss:', new_feature3)","9fa85206":"new_feature = (example.groupby('card1')['TransactionAmt'].transform('nunique'))\nnew_feature2 = sd(new_feature)\n\nprint_header()\nprint_values('nunique', 'original', new_feature)\nprint_values('nunique', 'default sd():', new_feature2)","e1740970":"print_header()\nfor col in ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C14']:\n    f = example[col]\n    print_values(col, 'original', f)\n    \n    # here we try to use default setting (precise enough)\n    f = sd(f)\n    print_values(col, 'default sd():', f)\n\n    # here we allow to fill up to 3 na fields with -99\n    f = sd(f, na_loss_limit=3, fillna=-99)\n    print_values(col, 'limited na loss:', f)\n    print()\n","5f8b5601":"## Issue 2","7d605053":"## Load dataset\n\nThe dataset is prepared in https:\/\/www.kaggle.com\/alexeykupershtokh\/concat-dataframes","39b119f0":"## Minify","8f6903bc":"## Example 1: float conversion params","05ad6cd4":"## Functions","08c16ed5":"You can see that we lost `11957 - 11882 = 75` unique values but saved 50% of memory.","13b811c3":"Let's try to minify it with default settings","d4df04c8":"Oops, it didn't work. The reason is most likely in that the values are too dense (e.g. there could be values like 100.0001 and 100.0002). But as far as this feature is ordinal and we don't care about preserving all of the unique values, let's losen our minification rules.","2edfcd24":"## Example 2: automatic float to int conversion","63fc0927":"## Issue 1","5e4474bd":"let's try frequency encoding","6c2542d8":"Let's try to create a new feature Series","4410de33":"## Example 3: C1-C14 column compression (lossy for 3 rows)","af13acec":"## TL;DR:\n\n1. Classic variant:\n   * Mem. usage decreased from 4867.46 Mb to 2452.37 Mb (49.6% reduction)\n   * Number of unique values: 3285120 -> 3216884 **(2.0% lost)**\n2. My variant:\n   * Mem. usage decreased from 4847.46 Mb to 2515.03 Mb (48.1% reduction)\n   * Number of unique values: 3285120 -> 3285120 **(0.0% lost)**\n3. My variant with optional object -> category conversion (read the **Objects -> categories** section before using!):\n   * Mem. usage decreased from 4847.46 Mb to **1086.85 Mb (77.6% reduction)**\n   * Number of unique values: 3285120 -> 3285120 **(0.0% lost)**\n\n## Rationale\n\nIt seems that many competition teams use the same `reduce_mem_usage` function (with modifications) e.g. https:\/\/www.kaggle.com\/kyakovlev\/ieee-data-minification .\n\nThough I see a few major drawbacks in using it as is:\n1. Such functions either:\n    1. Don't use minimal possible types for the sake of (imaginary) safety, and therefore use more memory than actually needed.\n    2. Use float16 but don't guarantee that you don't lose precision or unique values much (see **Issue 1** below)\n2. None of them try to perform float to int conversion.\n3. It's done only once and don't allow you to easily minify newly created features.\n\nSo my functons address all of these problems.\nThey allow using really minimal amount of memory and guarantee not losing anything (precision, na values, unique values, etc.).\nAnd you can do minification on the fly for new columns: `df['a\/b'] = sd(df['a']\/df['b'])`.\n\nAlso my `sd` (stands for `safe downcast`) function is very flexible. If you consider you can allow to lose 0.1 precision when rounding but wanna save more memory, then no problem, just set `sd(col, max_loss_limit=0.1, avg_loss_limit=0.1)`.\n\n## Objects -> categories:\n\nMy functions can do object -> category conversion as well. But it's important to remember that if you do this for train and test separately they will have different internal representation (see **Issue 2** below) and may cause issues with ML algorithms if they mess with codes.\nIn my code I use a concatenated dataset with 2-level indexes so it's not a problem. See the **Load dataset** section below.\n\nIf you want them to be converted, use `obj_to_cat=True` arg.\nIn this case you'll get:\n* **Mem. usage decreased from 4847.46 Mb to 1086.85 Mb (77.6% reduction)**\n"}}