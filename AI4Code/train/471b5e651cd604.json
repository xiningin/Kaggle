{"cell_type":{"38e5c038":"code","7225c2ec":"code","be65b7b7":"code","3f7538aa":"code","3ccceb3f":"code","13fe9017":"code","1eda3983":"code","7220acdc":"code","9c2d9442":"code","6bd8db77":"code","65b5314b":"code","2a59af6c":"code","49560180":"markdown","cb90112a":"markdown"},"source":{"38e5c038":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7225c2ec":"#reading data (read csv)\ndataframe=pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')\nprint(dataframe.info())","be65b7b7":"#%% unnecessary  columns drop\ndataframe=dataframe.drop([\"Serial No.\"],axis=1) # or dataset.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)","3f7538aa":"#%% x , y axis for model\ny=dataframe.Research.values   # values  makes np array \nx_data=dataframe.drop([\"Research\"],axis=1) ","3ccceb3f":"# normalization   feature scaling\nx=(x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","13fe9017":"#%% train test splitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n#x_train(80%x), x_test(20%x) , y_train(80%y) , y_test(20%y) bu s\u0131ralama onu anlat\u0131yor.\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","1eda3983":"#sklearn with LR {test accuracy 0.73}\n\nfrom sklearn.linear_model import LogisticRegression\nlr= LogisticRegression()\nlr.fit(x_train.T,y_train.T)\n\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T))) \n","7220acdc":"# parameter initialize  and sigmoid function\n\n# dimensions 30 \ndef initialize_weights_and_bias(dimensions):\n    \n    w=np.full((dimensions,1),0.01)\n    b=0.0\n    return w,b\n   \ndef sigmoid(z):\n    \n    y_head=1\/(1+np.exp(-z))\n    return y_head\n","9c2d9442":"#forward and bacward propagation\n \n# Forward propagation steps:\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\n    \ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward\n    z=np.dot(w.T,x_train)+b # dot product array multiply and summation\n    y_head=sigmoid(z)\n    \n    #loss function\n    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    #cost function\n    cost=(np.sum(loss))\/x_train.shape[1]# x_train.shape[1]  is for scaling\n    \n    #backward\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n                                        # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n                                        # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n\n    \n    return cost,gradients","6bd8db77":"# Updating(learning) parameters\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = [] #for storage cost values\n    cost_list2 = [] \n    index = []\n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration): #\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    # plotting cost list regularly\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.01,number_of_iteration = 200)","65b5314b":"#%% prediction method\n # prediction\ndef predict(w,b,x_test):# prediction yapmak i\u00e7in w,b ile olu\u015fan modele x_test leri verece\u011fiz\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","2a59af6c":"#%% logistic regression total implement\n    \ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,number_of_iteration):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_of_iteration)\n\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.15, number_of_iteration = 250)\n#with this learning_rate = 0.15 and number_of_iteration = 500 : test accuracy is: 72.0 %)","49560180":"# 2nd way to LR\n![resim.png](attachment:resim.png)","cb90112a":"INTRODUCTION\n\nMy dataset is Graduate Admission and i am using logistic regression to find accuracy of the model\n\nI am using linear model from sklearn and manuelly defining method in python\n\nI hope you find this homework is usefull and please give feedback to improve my skills."}}