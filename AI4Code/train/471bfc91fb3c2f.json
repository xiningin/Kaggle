{"cell_type":{"f9e52a4d":"code","d206c83f":"code","7aeb8933":"code","e4c90b86":"code","d13eadc0":"code","87ad981c":"code","3e6f96ad":"code","78f224bb":"code","fc98a794":"code","f47b0abf":"code","152dacfe":"code","531c099a":"code","5d7e3dae":"code","11311987":"code","dc1296ed":"code","35929768":"code","5a2a00a4":"code","a1dacb0c":"code","80f44c6c":"code","51279b41":"code","7820666d":"code","2c4dd94d":"code","7ea9cd6e":"markdown","09cb900a":"markdown","7b519570":"markdown","14911e26":"markdown","a0a85ed8":"markdown","6baec5db":"markdown","805b3864":"markdown","95e548c2":"markdown","ed908f86":"markdown","edf5be0b":"markdown","b02de891":"markdown","c82efd54":"markdown"},"source":{"f9e52a4d":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nimport eli5\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom IPython.display import display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\n\nnp.random.seed(1234)\n\n# Any results you write to the current directory are saved as output.\nprint(os.listdir(\"..\/input\"))","d206c83f":"class PipelineWithCoef(Pipeline):\n    \"\"\"This class only adds the coef_ attribute to the original Pipeline\n    class. This allows us to use Pipelines in RFE.\n    \"\"\"\n    def __init__(self, steps, memory=None):\n        super(PipelineWithCoef, self).__init__(steps, memory)\n\n    def fit(self, X, y=None, **fit_params):\n        \"\"\"Calls last elements .coef_ method. Based on the sourcecode for decision_function(X).\n        Link: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/pipeline.py\n        \"\"\"\n        super(PipelineWithCoef, self).fit(X, y, **fit_params)\n        self.coef_ = self.steps[-1][1].coef_\n        return self","7aeb8933":"target_column = 'target'\nconverters = {target_column: lambda v: int(float(v))}  # the file contains string values 1.0 and 0.0, but int(1.0) and int(0.0) raise errors\ntrain_df = pd.read_csv('..\/input\/train.csv', index_col=0, converters=converters)\nfeature_columns = train_df.columns.drop(target_column)\nprint(train_df.shape)\ntrain_df.head()\n\ntrain_df.target.value_counts()","e4c90b86":"(train_df.isnull().sum() > 0).any()","d13eadc0":"fig, ax = plt.subplots(figsize=(19,14))\ncorr = train_df.corr()\nsns.heatmap(corr, ax=ax);","87ad981c":"target_correlations = corr[target_column].drop(target_column, axis=0)\ntop_target_correlations = target_correlations.abs().sort_values(ascending=False)[:50]\ntop_corr_feature_columns = list(top_target_correlations.index)\ntop_target_correlations[:20]","3e6f96ad":"#Scaling Numerical columns\nstd = StandardScaler()\nX_scaled = std.fit_transform(train_df[feature_columns])\nX_scaled = pd.DataFrame(X_scaled, columns=feature_columns)","78f224bb":"lr = LogisticRegression(solver='liblinear', random_state=42)\nparam_grid = {'class_weight' : ['balanced', None],\n              'penalty' : ['l2','l1'],\n              'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid = GridSearchCV(estimator=lr, param_grid=param_grid, iid=False, cv=3, scoring='roc_auc', n_jobs=-1)\ngrid.fit(X_scaled[feature_columns], train_df[target_column])\nprint(\"Grid best score was: {}\".format(grid.best_score_))\n\n# select the best parameters from the grid search above\nlr = LogisticRegression(solver='liblinear', **grid.best_params_)\nlr.fit(X_scaled[feature_columns], train_df[target_column])\n\nrfe_lr = RFE(lr, 25, step=1)\nrfe_lr.fit(X_scaled, train_df[target_column])","fc98a794":"test_df = pd.read_csv('..\/input\/test.csv', index_col=0)\nX_test_scaled = std.transform(test_df[feature_columns])\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns)\n\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = rfe_lr.predict_proba(X_test_scaled)\nsubmission.to_csv('rfe_lr_submission.csv', index=False)","f47b0abf":"def test_model(model, parameter_grid, train_feature_values, train_target_values, retrain_with_top_features=True, importance_threshold=0.0005):\n    \"\"\"Function for training and testing a model. This method accepts a parameter grid\n    with which you can do hyperparameter tuning. It is also possible to retrain the model\n    by automatically selecting the top features based on eli5.sklearn.PermutationImportance.\n\n    Args:\n        model (sklearn.pipeline.Pipeline): a scikit learn Pipeline\n        parameter_grid (dict): hyperparameter tuning specifications for the grid search\n        train_feature_values (pd.DataFrame): training features values\n        train_target_values (pd.Series): training target values\n        retrain_with_top_features (bool): if True, then eli5.sklearn.PermutationImportance\n            will be used to determine what features should be used to retrain the model.\n            Otherwise, the feature importances will still be calculated, but no new model\n            will be trained.\n        importance_threshold (float): importance threshold that must be reached from the\n            PermutationImportance methodology before a feature is included in the\n            retraining procedure.\n\n    Return:\n        sklearn.model_selection._search.GridSearchCV: a fitted model with the best\n            hyperparameters chosen\n    \"\"\"\n    metric = metrics.make_scorer(metrics.roc_auc_score, greater_is_better=True, needs_proba=True)\n\n    def cv_train_model(model, parameter_grid, train_feature_values, train_target_values):\n        \"\"\"Receives an unfitted model as a parameter and does a GridSearch with the\n        training data.\n        \n        Args:\n            model (sklearn.pipeline.Pipeline):\n            parameter_grid (dict):\n            train_feature_values (pd.DataFrame):\n            train_target_values (pd.Series):\n            \n        Returns:\n            sklearn.model_selection._search.GridSearchCV\n        \"\"\"\n        # cv: for integer\/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n        folds = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1234)  # repeats n_splits splits n_repeats times\n        folds_iterable = folds.split(train_feature_values, train_target_values)\n        grid_search = GridSearchCV(model, param_grid=parameter_grid, scoring=metric, iid=False, refit=True, cv=folds_iterable, return_train_score=True, n_jobs=-1)\n        grid_search_model = grid_search.fit(train_feature_values, train_target_values)\n\n        cv_results = pd.DataFrame.from_dict(grid_search_model.cv_results_)\n        cv_results.set_index('rank_test_score', drop=True, inplace=True)\n        cv_results.sort_index(inplace=True)\n        cv_result_columns = [column for column in cv_results.columns if column.startswith('param_')] + ['mean_test_score', 'std_test_score', 'mean_train_score', 'std_train_score']\n        display(cv_results[cv_result_columns].head(5))\n        return grid_search_model\n\n    print(\"Training first model...\")\n    grid_search_model = cv_train_model(model, parameter_grid, train_feature_values, train_target_values)\n\n    print(\"Checking feature importances...\")\n    perm = eli5.sklearn.PermutationImportance(grid_search_model, scoring=metric, n_iter=10, random_state=1234).fit(train_feature_values, train_target_values)\n    display(eli5.show_weights(perm, top=25))\n    features_importance_df = eli5.formatters.as_dataframe.explain_weights_df(perm)\n    top_feature_columns = [train_feature_values.columns[int(feature_column[1:])]\n                           for feature_column\n                           in features_importance_df[features_importance_df.weight > importance_threshold].feature]\n    if len(top_feature_columns) == 0:\n        print(\"No top feature columns found. Exiting function.\")\n        return grid_search_model, list(train_feature_values.columns)\n    top_feature_columns = top_feature_columns[:50]  # never take more than 50 top feature columns\n\n    if retrain_with_top_features and len(top_feature_columns) < len(train_feature_values.columns):\n        print(\"Retraining with top {} features.\".format(len(top_feature_columns)))\n        grid_search_model = cv_train_model(model, parameter_grid, train_feature_values[top_feature_columns], train_target_values)\n    elif retrain_with_top_features:\n        print(\"All features are important. Therefore not retraining.\")\n        top_feature_columns = list(train_feature_values.columns)\n    else:\n        print(\"Not retraining with top features.\")\n        top_feature_columns = list(train_feature_values.columns)\n\n    return grid_search_model, top_feature_columns","152dacfe":"# Create a dictionary to keep track of all scores\nall_scores = dict()","531c099a":"sc_lr = Pipeline(steps=[('sc', StandardScaler(with_mean=True, with_std=True)),\n                        ('logistic_regression', LogisticRegression(solver='liblinear', max_iter=100, random_state=42))])\nparam_grid = {'logistic_regression__class_weight': ['balanced', None],\n              'logistic_regression__penalty': ['l1', 'l2'],\n              'logistic_regression__C': [0.01, 0.1, 0.5, 1., 1.5, 10.]}\n\nsc_lr_model, sc_lr_feature_columns = test_model(sc_lr, param_grid, train_df[feature_columns], train_df[target_column])\nall_scores['sc_lr'] = sc_lr_model.best_score_","5d7e3dae":"sc_pca_lr = Pipeline(steps=[('sc', StandardScaler(with_mean=True, with_std=True)),\n                            ('pca', PCA()),\n                            ('logistic_regression', LogisticRegression(solver='liblinear', max_iter=1000))])\nparam_grid = {'pca__n_components': [5, 10, 15],\n              'logistic_regression__class_weight': ['balanced', None],\n              'logistic_regression__penalty': ['l1', 'l2'],\n              'logistic_regression__C': [0.1, 1., 10.]}\n\nsc_pca_lr_model, sc_pca_lr_feature_columns = test_model(sc_pca_lr, param_grid, train_df[feature_columns], train_df[target_column], importance_threshold=0.001)\nall_scores['sc_pca_lr'] = sc_pca_lr_model.best_score_","11311987":"sc_lda_lr = Pipeline(steps=[('sc', StandardScaler(with_mean=True, with_std=True)),\n                            ('lda', LinearDiscriminantAnalysis()),\n                            ('logistic_regression', LogisticRegression(solver='liblinear', max_iter=1000))])\nparam_grid = {'lda__n_components': [5, 10, 15],\n              'logistic_regression__class_weight': ['balanced', None],\n              'logistic_regression__penalty': ['l1', 'l2'],\n              'logistic_regression__C': [0.1, 1., 10.]}\n\nsc_lda_lr_model, sc_lda_lr_feature_columns = test_model(sc_lda_lr, param_grid, train_df[feature_columns], train_df[target_column])\nall_scores['sc_lda_lr'] = sc_lda_lr_model.best_score_","dc1296ed":"svc = Pipeline(steps=[('svc', SVC(kernel='linear', gamma='auto', probability=True))])\nparam_grid = {'svc__C': [0.01, 0.1, 0.5, 1., 1.5, 10., 100.],\n              'svc__class_weight': ['balanced', None]}\n\nsvc_model, svc_feature_columns = test_model(svc, param_grid, train_df[top_corr_feature_columns], train_df[target_column])\nall_scores['svc'] = svc_model.best_score_","35929768":"def test_model_with_rfe(best_model, train_feature_values, train_target_values, min_nr_features=10, max_nr_features=40):\n    \"\"\"Creates a model using recursive feature elimination (RFE) with the best parameters\n    found with GridSearchCV. This method tries out all variants with the number of features\n    to select between min_nr_features and max_nr_features (both inclusive) for RFE.\n    \n    Args:\n        best_model (sklearn.model_selection.GridSearchCV):\n        train_feature_values (pd.DataFrame):\n        train_target_values (pd.Series):\n        min_nr_features (int):\n        max_nr_features (int):\n        \n    Return:\n        RFE\n    \"\"\"\n    deepcopied_best_model = deepcopy(best_model.best_estimator_)\n    nr_feaures_to_select_range = list(range(min_nr_features, max_nr_features + 1))\n    all_scores = []\n    all_estimators = []\n    for n_features_to_select in tqdm(nr_feaures_to_select_range):\n        # recreate the original Pipeline\n        best_pipeline = PipelineWithCoef(steps=deepcopied_best_model.steps)\n        best_pipeline.fit(train_feature_values, train_target_values)\n        estimator = RFE(best_pipeline, n_features_to_select=n_features_to_select)\n        # train the model\n        estimator.fit(train_feature_values, train_target_values)\n        folds = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1234)  # repeats n_splits splits n_repeats times\n        folds_iterable = folds.split(train_feature_values, train_target_values)\n        scores = cross_val_score(estimator, train_feature_values, train_target_values, scoring='roc_auc', cv=folds_iterable)\n        all_scores.append(scores.mean())\n        all_estimators.append(estimator)\n        \n    def plot_roc_auc_scores(nr_features_list, scores_list):\n        \"\"\"Plots the ROC-AUC scores obtained from the number of features to select\n        for RFE.\n        \n        Args:\n            nr_features_list (list):\n            scores_list (list):\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(20, 4))\n        ax.plot(nr_features_list, scores_list)\n        for s in ax.spines:\n            ax.spines[s].set_visible(False)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n        ax.set_title('ROC-AUC Score', fontsize=16)\n        ax.set_xlabel('Nr Features Selected', fontsize=14);\n    \n    plot_roc_auc_scores(nr_feaures_to_select_range, all_scores)\n    \n    top_index = np.argmax(all_scores)\n    print(\"Number of features selected for best score: {}\".format(nr_feaures_to_select_range[top_index]))\n    best_estimator = all_estimators[top_index]\n    \n    return best_estimator","5a2a00a4":"rfe_sc_lr_model = test_model_with_rfe(sc_lr_model, train_df[feature_columns], train_df[target_column], max_nr_features=30)","a1dacb0c":"test_df = pd.read_csv('..\/input\/test.csv', index_col=0)\nprint(test_df.shape)\ntest_df.head()","80f44c6c":"def create_submission_file(model, test_data, prefix_output_file):\n    submission_df = pd.DataFrame(data={'id': test_data.index,\n                                       'target': model.predict_proba(test_data)[:,1]})\n    submission_df.to_csv(prefix_output_file + '_submission.csv', index=False)","51279b41":"all_scores","7820666d":"# select a few models from first modelling section to create outputs\ncreate_submission_file(sc_lr_model, test_df[sc_lr_feature_columns], 'sc_lr')\ncreate_submission_file(svc_model, test_df[svc_feature_columns], 'svc')","2c4dd94d":"# select the RFE model from recursive feature elimination to create outputs\ncreate_submission_file(rfe_sc_lr_model, test_df[feature_columns], 'rfe_sc_lr')","7ea9cd6e":"As you can see we have an inbalanced dataset. About 2\/3 of the data is of class 1.\n\nAre there an missing values?","09cb900a":"# Initial model\n\nLet's create a baseline model first and see later on whether we can improve it or create a different model that performs better.","7b519570":"# Exploratory Analysis","14911e26":"The models we created above actually all seem promising. According to the cross validation results, we don't even seem to overfit much. However, note though that it is a small dataset and that the results can be deceiving.","a0a85ed8":"# Recursive Feature Elimination\n\nLet's do some hyperparameter search on RFE; find out what number of features gives the model the best performance.","6baec5db":"# Conclusive\n\nAfter submitting the predictions, it is clear that the Pipeline models do not generalise well enough to the test set. So there's still overfitting going on here even though the cross validation results seem to think otherwise. Therefore it could be a good idea to apply stronger regularisation for example. Or create an ensemble model with different folds.\n\nI hope you enjoyed reading\/working with this kernel.","805b3864":"Let's check the correlations and create a heatmap.","95e548c2":"# Write a submission","ed908f86":"# Don't Overfit! 2\n\nWelcome to my kernel! This kernel will walk you through how you can iteratively model and select the best features. Along the way we'll also see how you can create one pipeline where all the preprocessing is merged into one model.\n\nTopics we'll cover:\n* Pipeline; combine multiple fit-transform steps into one model\n* GridSearch; how to find the best hyperparameters\n* eli5 (feature selection); what features shoulld be used for your model\n* visualizations\n\nSince I like to automate stuff, I will also show you how you could iteratively select features using eli5.","edf5be0b":"# Create and test model pipelines\n\nNow we come to the part where we create a model by selecting the best features with eli5. We will automate the process of learning, selecting features and learning again with the selected features.\n\nBelow we will define our main function. This function will allow us to train, select features and retrain.","b02de891":"The features aren't highly correlated with each other. Which is a good thing and quite surprising with this many variables.\n\nUnfortunately, nothing is highly correlated with the target variable.","c82efd54":"Let's create a class first that we can use later on with recursive feature elimination (RFE)."}}