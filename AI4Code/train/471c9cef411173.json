{"cell_type":{"35c67cb6":"code","ca190206":"code","a310fdec":"code","10736316":"code","8d863572":"code","2d3f021b":"code","f40a44b9":"code","2335af5f":"code","c79eae82":"code","923570f3":"code","7cc24248":"code","fabd450b":"code","ca0e25bd":"code","106401f5":"code","fdbe9b7b":"markdown","14e96cf2":"markdown","a88edd20":"markdown","8abfea9c":"markdown","da0cffe4":"markdown","1ff30b74":"markdown","9cff02cf":"markdown"},"source":{"35c67cb6":"import gensim\nprint(\"gensim version: \", gensim.__version__)\n\nimport pandas as pd\nprint(\"pandas version: \", pd.__version__)\n\nimport requests\nprint(\"requests version: \", requests.__version__)\n\nimport re\nprint(\"re version: \", re.__version__)\n\nimport argparse\nprint(\"argparse version: \", argparse.__version__)\n\nimport time\n\nimport sys\nimport os.path\nimport multiprocessing","ca190206":"DIR_DATA_A = \"..\/input\/ukara-test-phase\/\"\nDIR_DATA_B = \"..\/input\/ukara-test-phase\/\"\ndata_A_train = pd.read_csv(\"{}\/data_train_A.csv\".format(DIR_DATA_A))\ndata_A_dev = pd.read_csv(\"{}\/data_dev_A.csv\".format(DIR_DATA_A))\ndata_A_test = pd.read_csv(\"{}\/data_test_A.csv\".format(DIR_DATA_A))\n\ndata_B_train = pd.read_csv(\"{}\/data_train_B.csv\".format(DIR_DATA_B))\ndata_B_dev = pd.read_csv(\"{}\/data_dev_B.csv\".format(DIR_DATA_B))\ndata_B_test = pd.read_csv(\"{}\/data_test_B.csv\".format(DIR_DATA_B))","a310fdec":"def preprocess(text):\n    text = text.strip()\n    text = text.lower()\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n    text = re.sub(' +', ' ', text).strip()\n    return text\n\n\nstart_preprocess1 = time.time()\ndata_A_train['RESPONSE'] = data_A_train['RESPONSE'].apply(lambda x: preprocess(x))\ndata_A_dev['RESPONSE'] = data_A_dev['RESPONSE'].apply(lambda x: preprocess(x))\ndata_A_test['RESPONSE'] = data_A_test['RESPONSE'].apply(lambda x: preprocess(str(x)))\n\ndata_B_train['RESPONSE'] = data_B_train['RESPONSE'].apply(lambda x: preprocess(x))\ndata_B_dev['RESPONSE'] = data_B_dev['RESPONSE'].apply(lambda x: preprocess(x))\ndata_B_test['RESPONSE'] = data_B_test['RESPONSE'].apply(lambda x: preprocess(str(x)))\nend_preprocess1 = time.time()\n\n\nstimulus_a = [\"Pemanasan global terjadi karena peningkatan produksi karbon dioksida yang dihasilkan oleh pembakaran fosil dan konsumsi bahan bakar yang tinggi.\",\n\"Salah satu akibat adalah mencairnya es abadi di kutub utara dan selatan yang menimbulkan naiknya ketinggian air laut.\",\n\"kenaikan air laut akan terjadi terus menerus meskipun dalam hitungan centimeter akan mengakibatkan perubahan yang signifikan.\",\n\"Film \u201cWaterworld\u201d, adalah film fiksi ilmiah yang menunjukkan akibat adanya pemanasan global yang sangat besar sehingga menyebabkan bumi menjadi tertutup oleh lautan.\",\n\"Negara-negara dan daratan yang dulunya kering menjadi tengelamn karena terjadi kenaikan permukaan air laut.\",\n\"Penduduk yang dulunya bisa berkehidupan bebas menjadi terpaksa mengungsi ke daratan yang lebih tinggi atau tinggal diatas air.\",\n\"Apa yang akan menjadi tantangan bagi suatu penduduk ketika terjadi situasi daratan tidak dapat ditinggali kembali karena tengelam oleh naiknya air laut.\"]\n\nstimulus_b = [\"Sebuah toko baju berkonsep self-service menawarkan promosi dua buah baju bertema tahun baru seharga Rp50.000,00. sebelum baju bertema tahun baru dibagikan kepada pembeli, sebuah layar akan menampilkan tampilan gambar yang menampilkan kondisi kerja di dalam sebuah pabrik konveksi\/pembuatan baju. \",\n\"Kemudian pembeli diberi program pilihan untuk menyelesaikan pembeliannya atau menyumpangkan Rp50.000,00 untuk dijadikan donasi pembagian baju musim dingin di suatu daerah yang membutuhkan.\",\n\"Delapan dari sepuluh pembeli memilih untuk memberikan donasi.\",\n\"Menurut anda mengapa banyak dari pembeli yang memilih berdonasi?\"]\n\ndata_stimulus = []\n\nfor text in stimulus_a:\n    data_stimulus.append(preprocess(text))\n    \nfor text in stimulus_b:\n    data_stimulus.append(preprocess(text))\n    \ndata_stimulus.extend(data_A_train['RESPONSE'].values)\ndata_stimulus.extend(data_A_dev['RESPONSE'].values)\ndata_stimulus.extend(data_A_test['RESPONSE'].values)\ndata_stimulus.extend(data_B_train['RESPONSE'].values)\ndata_stimulus.extend(data_B_dev['RESPONSE'].values)\ndata_stimulus.extend(data_B_test['RESPONSE'].values)","10736316":"print(len(data_stimulus))\ndata_stimulus[0:3]","8d863572":"!wget http:\/\/opus.nlpl.eu\/download.php?f=OpenSubtitles\/v2018\/mono\/OpenSubtitles.raw.id.gz -O dataset.txt.gz\n!gzip -d dataset.txt.gz\n!tail dataset.txt","2d3f021b":"def download(link, file_name):\n    with open(file_name, \"wb\") as f:\n        print(\"Downloading %s\" % file_name)\n        response = requests.get(link, stream=True)\n        total_length = response.headers.get('content-length')\n\n        if total_length is None: # no content length header\n            f.write(response.content)\n        else:\n            dl = 0\n            total_length = int(total_length)\n            for data in response.iter_content(chunk_size=4096):\n                dl += len(data)\n                f.write(data)\n                done = int(50 * dl \/ total_length)\n                sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )\n                sys.stdout.flush()\n\ndef get_id_wiki(dump_path):\n    if not os.path.isfile(dump_path):\n        url = 'https:\/\/dumps.wikimedia.org\/idwiki\/latest\/idwiki-latest-pages-articles.xml.bz2'\n        download(url, dump_path)\n    return gensim.corpora.WikiCorpus(dump_path, lemmatize=False, dictionary={})","f40a44b9":"dump_path = 'idwiki-latest-pages-articles.xml.bz2'\nid_wiki = get_id_wiki(dump_path)","2335af5f":"dim = 100\nmodel_path = 'idwiki_word2vec_{}.model'.format(dim)\nextracted_path = 'idwiki.txt'","c79eae82":"print('Extracting text...')\nstart_preprocess2 = time.time()\nwith open(extracted_path, 'w') as f:\n    # ukara\n    i_ukara = 0\n    word_ukara = 0\n    for text in data_stimulus:\n        test_processed = text.strip()\n        f.write(test_processed + '\\n')\n        i_ukara += 1\n        word_ukara += len(test_processed.split())\n\n    # opensubs\n    i_opensubs = 0\n    word_opensubs = 0\n    with open('dataset.txt') as f_opensubs:\n        opensubs = f_opensubs.readlines()\n        for text in opensubs:\n            test_processed = preprocess(text).strip()\n            f.write(test_processed + '\\n')\n            i_opensubs += 1\n            word_opensubs += len(test_processed.split())\n\n    # wikipedia\n    i_wiki = 0\n    word_wiki = 0\n    for text in id_wiki.get_texts():\n        text = ' '.join(text)\n        f.write(text + '\\n')\n        i_wiki += 1\n        word_wiki += len(text.split())\n        \n    end_preprocess2 = time.time()\n            \n    print('total ukara text: ', str(i_ukara))\n    print('total ukara word:', str(word_ukara))\n    print('total opensubs text: ', str(i_opensubs))\n    print('total opensubs word:', str(word_opensubs)) \n    print('total wikipedia text: ', str(i_wiki))\n    print('total wikipedia word:', str(word_wiki)) ","923570f3":"def build_model(extracted_path, model_path, dim):\n    sentences = gensim.models.word2vec.LineSentence(extracted_path)\n    id_w2v = gensim.models.word2vec.Word2Vec(sentences, size=dim, workers=multiprocessing.cpu_count()-1)\n    id_w2v.save(model_path)\n    return id_w2v","7cc24248":"print('Building the model...')\nstart_training1 = time.time()\nmodel = build_model(extracted_path, model_path, dim)\nend_training1 = time.time()\nprint('Saved model:', model_path)","fabd450b":"print(\"Total word2vec vocabulary: \", len(model.wv.vocab))","ca0e25bd":"print(\"Time elapsed preprocessing data: {} second\".format((end_preprocess1-start_preprocess1)+(end_preprocess2-start_preprocess2)))","106401f5":"print(\"Time elapsed training wordembedding: {} second\".format((end_training1-start_training1)))","fdbe9b7b":"## Collecting Wikipedia Text","14e96cf2":"## Building Model","a88edd20":"## Initialization\nImporting libraries and setting contant variable","8abfea9c":"# UKARA: Building Word2Vec 100 Indonesia\n\nThis notebook produced the word2vec word embedding I use in my Ukara NLP Challenge submission. For more information, check the repository.  \n\n\nRepository: [https:\/\/github.com\/ilhamfp\/ukara-1.0-challenge](https:\/\/github.com\/ilhamfp\/ukara-1.0-challenge)","da0cffe4":"## Collecting Ukara Text","1ff30b74":"# Time Elapsed","9cff02cf":"## Collecting Open Subtitle Text"}}