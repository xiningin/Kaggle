{"cell_type":{"093494b4":"code","d1573380":"code","cb2f5a49":"code","1f3db016":"code","a8a2cbf7":"code","3ac43f08":"code","f4c24532":"code","e9c1932c":"code","7a0ddaf6":"code","59ea387c":"code","dd9e0fac":"code","f813fd24":"code","f1bfc80f":"code","ae21d9a8":"code","23b6535c":"code","65ba0e72":"code","582ae8e7":"code","057ce027":"code","57d89a57":"code","38d75c95":"code","990f9790":"code","427fccb0":"code","55088d34":"code","4e04463d":"code","77294288":"code","4be13fa9":"code","2543c771":"code","25cc9d26":"code","e87dd190":"code","754d3651":"code","722d2b79":"code","bf98c7f9":"code","e8f1ce29":"code","2e3256de":"code","53597d0d":"code","7757e52e":"code","a69761e7":"code","e2fb13a4":"code","d84df7d6":"code","83597f25":"code","51cd2096":"code","318c921a":"code","669421a2":"code","c3325810":"code","6b74aede":"code","9d9a1256":"code","eb0b931c":"code","fa14077c":"code","513538c0":"code","6d0f1acc":"code","c3d2163e":"code","c3e3f775":"code","591678a1":"code","baecd1d8":"code","d6d54c21":"code","69120e1e":"code","492c682b":"code","eb3120d9":"code","069e7ebe":"code","8d039177":"code","0c02d33b":"code","f6e9a5e7":"code","69184b06":"code","ec70d0d2":"code","415beab4":"code","054b986c":"code","1f6ca2f8":"code","73658a52":"code","09bca597":"code","d691bb7a":"code","8391582d":"code","9de5d2f6":"code","cb3bb7a7":"code","69fc157c":"code","0b14462a":"code","c60d8a9e":"code","c171582c":"code","557e3ea8":"code","d8210609":"code","e5f0cac6":"code","bb6e4162":"code","9490bcbd":"code","30aa1d54":"code","9252d040":"code","8316bc80":"code","72a30745":"code","7ff30784":"code","25c70e1f":"code","34e34ef9":"code","9df85348":"code","4d6d1012":"code","558f079f":"code","2976bcb9":"code","7d6d2770":"code","89586280":"code","0abcdb51":"code","21780eb0":"code","91473cbd":"code","021b0ffa":"code","8c9f9ccd":"code","7d2acc9f":"code","b8c33561":"code","f4158631":"code","e4e965b1":"code","65fe5c2e":"code","b83efb60":"code","b1ad867b":"code","17224aa6":"markdown","3cf072b1":"markdown","b8fa17de":"markdown","3f3325d5":"markdown","752dc99b":"markdown","177f1573":"markdown","15fbee3f":"markdown","de7c6efd":"markdown","61e9d91a":"markdown","07738f6f":"markdown","11450d42":"markdown","de98dda9":"markdown","3df1da1a":"markdown","2ba7fc10":"markdown","da00e75f":"markdown","51bc46c1":"markdown","a1e5555e":"markdown","fbcb7513":"markdown","d01c244c":"markdown","b3511bef":"markdown","c64af3b8":"markdown","2545724a":"markdown","290ade3c":"markdown","0eb96441":"markdown","8cc12ce6":"markdown","688b1851":"markdown","720e6dad":"markdown","2d1aa31b":"markdown","248ee2d3":"markdown","f1eb9652":"markdown","e6ad4ed4":"markdown","7afe5a0b":"markdown","e493a374":"markdown","16413956":"markdown","19e4ebcd":"markdown","dc1817f8":"markdown","72ea1660":"markdown","c8ebaf64":"markdown","1e80001b":"markdown","82162a89":"markdown","c910e533":"markdown","973cad90":"markdown","652a2523":"markdown","e329ff4f":"markdown","06f7790e":"markdown","06b02798":"markdown","c7cc3123":"markdown","d09e0389":"markdown","fb781533":"markdown","15ac55d2":"markdown","0b3a6f93":"markdown"},"source":{"093494b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\n\n# added libaries \n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('dark_background')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# loading the data \ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\",index_col='PassengerId')\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\",index_col='PassengerId')\nsub = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","d1573380":"plt.style.use('seaborn')\n\n\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score","cb2f5a49":"\ndef missing_value_status(df):\n    missing_values_count=df.isnull().sum()\n    print(missing_values_count)\n    total_missing=missing_values_count.sum()\n    total=np.product(df.shape)\n    total_rows=df.shape[0]\n    print(f'precentage missing of total data - {100*(total_missing\/total)} ' )\n    print(f'precentage missing of column data  data - {(100*(missing_values_count\/total_rows))} ' )\n    \n    ","1f3db016":"missing_value_status(train_data)\n\nmissing_value_status(test_data)\n","a8a2cbf7":"# in the test we  have also Fare missing, since it is very small amounts , it will be\n# replaced with mean. ","3ac43f08":"train_data.head()","f4c24532":"train_data.shape","e9c1932c":"train_data.head()\n","7a0ddaf6":"train_data.info()","59ea387c":"train_data.describe()","dd9e0fac":"test_data.info()","f813fd24":"# lets observe some basic correlation :\ndef titanic_corr(data):\n    correlation = data.corr()\n    sns.heatmap(correlation, annot=True, cbar=True, cmap=\"RdYlGn\")\n    \ntitanic_corr(train_data)","f1bfc80f":"sns.countplot(x='Survived', data=train_data);\n","ae21d9a8":"# we can have a base line model - which simply predicts that no one survive. ","23b6535c":"sns.catplot(x='Survived', col='Sex', kind='count', data=train_data);\n","65ba0e72":"# being a women correlates with survive. ","582ae8e7":"female=train_data[train_data['Sex']=='female']\nmale=train_data[train_data['Sex']=='male']\nprecent_female_survived=100*(female['Survived'].sum()\/female.shape[0])\nprecent_male_survived=100*(male['Survived'].sum()\/male.shape[0])\nprint(f'male survival rate- {precent_male_survived}, female survival rate- {precent_female_survived} ')","057ce027":"# another base model can be according to wther the sex is female or male. ","57d89a57":"sns.catplot(x='Survived', col='Pclass', kind='count', data=train_data);\n","38d75c95":"# it seems that being in the first calss increse the chances of survival. ","990f9790":"### Embarked","427fccb0":"sns.catplot(x='Survived', col='Embarked', kind='count', data=train_data);\n","55088d34":"train_data.groupby(['Cabin'])['Survived'].sum()","4e04463d":"train_data['Cabin'].unique","77294288":"temp_cabin_letter=train_data.copy()\ntemp_cabin_letter=temp_cabin_letter[temp_cabin_letter['Cabin'].notnull() ]\ntemp_cabin_letter['Cabin'] = temp_cabin_letter['Cabin'].apply(lambda x: x[0])\n\nprint('Chance of survival in %,  by Cabin letter: ')\n100*temp_cabin_letter.groupby(['Cabin'])['Survived'].sum()\/temp_cabin_letter.groupby(['Cabin'])['Survived'].count()\n\n\n","4be13fa9":"# We can see that being in cabin that start with either B,D,E \n# increse the survival rate. \n\n\n","2543c771":"temp_cabin_letter.groupby(['Cabin'])['Survived'].count()\n","25cc9d26":"# In T we had only 1 one. might be the capatain\ntrain_data_v1=train_data.copy()\ntrain_data_v1['Cabin']=train_data_v1['Cabin'].fillna('n')\ntrain_data_v1['Cabin'] = train_data_v1['Cabin'].apply(lambda x: x[0])\n100*(train_data_v1.groupby(['Cabin'])['Survived'].sum()\/train_data_v1.groupby(['Cabin'])['Survived'].count())\n\ntest_data_v1=test_data.copy()\n\ntest_data_v1['Cabin']=test_data_v1['Cabin'].fillna('n')\ntest_data_v1['Cabin'] = test_data_v1['Cabin'].apply(lambda x: x[0])","e87dd190":"100*train_data_v1.groupby('Cabin')['Survived'].sum()\/train_data_v1['Cabin'].value_counts()","754d3651":"sns.histplot(train_data.Fare, kde=False);\n","722d2b79":"# most pessengers paid less than 100. now lets see how it effects on the survival rate :\ntrain_data.groupby('Survived')['Fare'].hist(alpha=0.6);\n","bf98c7f9":"train_data_v1['Name']","e8f1ce29":"# it seems that what might be usfull the  pessenger title mr etc","2e3256de":"check=train_data_v1['Name'][1]\ncheck","53597d0d":"\n\n    \n\n\ntrain_data_v2=train_data_v1.copy()\ntrain_data_v2['Title']=train_data_v2['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\ntest_data_v2=test_data_v1.copy()\ntest_data_v2['Title']=test_data_v2['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n# after observation it seems wise to group some of the titles (i.e those which were not common enough )\n\ndic_titles={'Capt':'crew','Col':'crew', 'Don':'royal', \n            'Dona':'royal', 'Dr':'crew', 'Jonkheer':'royal','Lady':'royal',\n         'Major': 'crew','Master':'Master', 'Miss':'Miss','Mlle':'Miss',\n         'Mme':'Mrs', 'Mr':'Mr','Mrs':'Mrs','Ms': 'Mrs','Rev':'crew',\n            'Sir':'royal', 'the Countess':'royal'}\n\n\n\n\ntemp=train_data_v2.copy()\ntemp['Title']=temp['Title'].map(dic_titles)\ntrain_data_v2['Title']=temp['Title']\ntrain_data_v2['Title']\n\ntest_data_v2['Title']=test_data_v2['Title'].map(dic_titles)\ntest_data_v2['Title'].unique()","7757e52e":"train_data_v2['Title'].unique()","a69761e7":"train_data_v2['Title'].tail()","e2fb13a4":"# lets see the survival chances :\nprint('Survival chances by title')\n100*train_data_v2.groupby(['Title'])['Survived'].sum()\/train_data_v2['Title'].value_counts()","d84df7d6":"# we have few examples for  \n# Mme               1\n# Ms                1\n# Don               1\n# Jonkheer          1\n# Lady              1\n# the Countess      1\n# Sir               1\n\n# i might need to condider removing this item since they can cause instabilety. ","83597f25":"train_data_v2['Title'].unique()","51cd2096":"100*train_data_v2.groupby('SibSp')['Survived'].sum()\/\/train_data_v2['SibSp'].value_counts()","318c921a":"# it seems that if u have a childred (>2 is not including sppuse) the survival\n# rates are droping. yet this is not conclusive, if you have no sib\/suppose it\n# seems u are also less likely to survive","669421a2":"train_data_v2['SibSp'].value_counts()","c3325810":"100*train_data_v2.groupby('Parch')['Survived'].sum()\/\/train_data_v2['Parch'].value_counts()","6b74aede":"# lets look at the sum. \" family \" and on is alone \ntrain_data_v3=train_data_v2.copy()\ntrain_data_v3['Family_size']=train_data_v3['SibSp']+train_data_v3['Parch']\n\ntest_data_v3=test_data_v2.copy()\n\ntest_data_v3['Family_size']=test_data_v3['SibSp']+test_data_v3['Parch']\n\n100*train_data_v3.groupby('Family_size')['Survived'].sum()\/\/train_data_v3['Family_size'].value_counts()\n\n\ntrain_data_v3['is_alone']=train_data_v3['Family_size']==0\ntrain_data_v3['is_alone']=train_data_v3['is_alone'].map({True:0, False:1})\n\ntest_data_v3['is_alone']=test_data_v3['Family_size']==0\ntest_data_v3['is_alone']=test_data_v3['is_alone'].map({True:0, False:1})","9d9a1256":"train_data_v3['Family_size'].value_counts()\n","eb0b931c":"# it would be tested later on, if this feature helps the model. \n# lets look on alone","fa14077c":"\n\n# lambda x : True if (x > 10 and x < 20) else False","513538c0":"train_data_v3[['is_alone', 'Survived']].groupby(['is_alone'], as_index=False).mean()","6d0f1acc":"sns.histplot(train_data['Age'], kde=False);\n","c3d2163e":"train_data_v2.groupby('Survived')['Age'].hist(alpha=0.6);\n","c3e3f775":"test_data","591678a1":"train_data_v3['Ticket'].head(30)","baecd1d8":"# some question arise, maybe the first num of the ticket is enough ? how does it \n# corrolate with pclass and survival rate ? lets check it  out ! ","d6d54c21":"def split_ticket(x):\n    x=x.split(' ')\n    if len(x)>1:\n        \n        x=x[-1]\n        x=x[0]\n    else:\n        x=x[0]\n        if  x[0].isnumeric():\n            x=x[0]\n        else:\n            x='char'\n        \n    return x\n        \n    \ntrain_data_v4=train_data_v3.copy()    \ntest_data_v4=test_data_v3.copy()\n\ntrain_data_v4['first_num_tic']=train_data_v4['Ticket'].apply(lambda x: split_ticket(x) )\ntest_data_v4['first_num_tic']=test_data_v4['Ticket'].apply(lambda x: split_ticket(x) )\n\n\n# 100*train_data_v3.groupby(['first_num_tic'])['Survived'].sum()\/train_data_v3['first_num_tic'].value_counts()\ntrain_data_v4['first_num_tic'].unique()","69120e1e":"sns.catplot(x='Survived', col='first_num_tic', kind='count', data=train_data_v4);\n","492c682b":"\n\n# print(train_data_v7['Title'].value_counts())\n# # print(train_data_v7['Title'].unique())\n\n# train=set(train_data_v7['Title'].unique())\n# test=set(test_data_v7['Title'].unique())\n# all_titles=train.union(test)\n# all_titles\n\n\n# # lets prepare dictonery :\n# dic_titles={'Capt':'crew','Col':'crew', 'Don':'royal', \n#             'Dona':'royal', 'Dr':'crew', 'Jonkheer':'royal','Lady':'royal',\n#          'Major': 'crew','Master':'Master', 'Miss':'Miss','Mlle':'Miss',\n#          'Mme':'Mrs', 'Mr':'Mr','Mrs':'Mrs','Ms': 'Mrs','Rev':'crew',\n#             'Sir':'royal', 'the Countess':'royal'}\n# temp=train_data_v7.copy()\n# temp['Title']=temp['Title'].map(dic_titles)\n# temp.groupby(['Title'])['Age'].mean()\n\n# # we have a great association between title and age. lets use this to deal with the nan values","eb3120d9":"\n\nsns.histplot(data=train_data_v4, x=\"Age\");\n","069e7ebe":"# associate age with mean of the title group. \n\n# this is one option \n\n\n\n\ndic_titles={'Capt':'crew','Col':'crew', 'Don':'royal', \n            'Dona':'royal', 'Dr':'crew', 'Jonkheer':'royal','Lady':'royal',\n         'Major': 'crew','Master':'Master', 'Miss':'Miss','Mlle':'Miss',\n         'Mme':'Mrs', 'Mr':'Mr','Mrs':'Mrs','Ms': 'Mrs','Rev':'crew',\n            'Sir':'royal', 'the Countess':'royal'}\ntemp=train_data_v4.copy()\ntitle_age=temp.groupby(['Title'])['Age'].mean()\ntitle_age_dic=title_age.to_dict()\ntitle_age_dic\n","8d039177":"# # one way to go - impute age by mean of title\n# train_data_v5=train_data_v4.copy()\n# test_data_v5=test_data_v4.copy()\n\n# age_from_title=train_data_v5[train_data_v5['Age'].isnull()]['Title'].map(title_age_dic)\n# train_data_v5.loc[train_data_v5['Age'].isnull(),'Age']=age_from_title\n\n# age_from_title2=test_data_v5[test_data_v5['Age'].isnull()]['Title'].map(title_age_dic)\n# test_data_v5.loc[test_data_v5['Age'].isnull(),'Age']=age_from_title2\n                                             \n\n\n# train_data_v5.loc[train_data_v5['Age'].isnull(),['Age','Title']]\n\n\n\n# train_data_v5['Age'] = train_data_v5['Age'].astype('int')\n# test_data_v5['Age'] = test_data_v5['Age'].astype('int')","0c02d33b":"train_data_v5=train_data_v4.copy()\ntest_data_v5=test_data_v4.copy()\n\nage_avg= train_data_v5['Age'].mean()\nage_std= train_data_v5['Age'].std()\nage_null_count = train_data_v5['Age'].isnull().sum()\nage_null_count_test = test_data_v5['Age'].isnull().sum()\n\n\nage_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\nage_null_random_list_test = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count_test)\n\n\ntrain_data_v5.loc[train_data_v5['Age'].isnull(),'Age'] = age_null_random_list\ntest_data_v5.loc[test_data_v5['Age'].isnull(),'Age'] = age_null_random_list_test\n\ntrain_data_v5['Age'] = train_data_v5['Age'].astype('int')\ntest_data_v5['Age'] = test_data_v5['Age'].astype('int')\n    \n# train['CategoricalAge'] = pd.cut(train['Age'], 5)\n\n","f6e9a5e7":"test_data_v5['Age']","69184b06":"train_data_v5.info()","ec70d0d2":"sns.histplot(data=train_data_v5, x=\"Age\");\n","415beab4":"test_data_v5['Age']\n\n","054b986c":"\n\ndata = [train_data_v5, test_data_v5]\nfor dataset in data:\n    dataset['Age_bin'] = dataset['Age']\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 7\n    \n    dataset['Age'] = dataset['Age'].astype(str)\n    dataset.loc[ dataset['Age'] == '0', 'Age_bin'] = \"Children\"\n    dataset.loc[ dataset['Age'] == '1', 'Age_bin'] = \"Teens\"\n    dataset.loc[ dataset['Age'] == '2', 'Age_bin'] = \"Youngsters\"\n    dataset.loc[ dataset['Age'] == '3', 'Age_bin'] = \"Young Adults\"\n    dataset.loc[ dataset['Age'] == '4', 'Age_bin'] = \"Adults\"\n    dataset.loc[ dataset['Age'] == '5', 'Age_bin'] = \"Middle Age\"\n    dataset.loc[ dataset['Age'] == '6', 'Age_bin'] = \"Senior\"\n    dataset.loc[ dataset['Age'] == '7', 'Age_bin'] = \"Retired\"\n    \n    ","1f6ca2f8":" train_data_v5.info()","73658a52":"temp=train_data_v5[['Embarked']]\n\nimp = SimpleImputer(strategy=\"most_frequent\")\ntemp=imp.fit_transform(temp)\ntrain_data_v5['Embarked']=temp\n\ntemp2=test_data_v5[['Embarked']]\ntemp2=imp.transform(temp2)\ntest_data_v5[['Embarked']]=temp2\n\n\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain_data_v5['Fare'] = imp.fit_transform(train_data_v5[[\"Fare\"]]).ravel()\n\ndef fill_mean(imp,df,col_name):\n    df[col_name] = imp.transform(df[[col_name]]).ravel()\n    return df\n\ntest_data_v5=fill_mean(imp=imp,df=test_data_v5,col_name='Fare')","09bca597":"train_data_v5['Fare']=train_data_v5['Fare'].astype('int')\ntest_data_v5['Fare']=test_data_v5['Fare'].astype('int')\n\n\ntrain_data_v5['num_Fare']=train_data_v5['Fare']\ntest_data_v5['num_Fare']=test_data_v5['Fare']\n\n\ndata=[train_data_v5,test_data_v5]\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7) & (dataset['Fare'] <= 14), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    dataset['Fare'] = dataset['Fare'].astype(str)\n    dataset.loc[ dataset['Fare'] == '0', 'Fare_cat'] = \"Extremely Low\"\n    dataset.loc[ dataset['Fare'] == '1', 'Fare_cat'] = \"Very Low\"\n    dataset.loc[ dataset['Fare'] == '2', 'Fare_cat'] = \"Low\"\n    dataset.loc[ dataset['Fare'] == '3', 'Fare_cat'] = \"High\"\n    dataset.loc[ dataset['Fare'] == '4', 'Fare_cat'] = \"Very High\"\n    dataset.loc[ dataset['Fare'] == '5', 'Fare_cat'] = \"Extremely High\"","d691bb7a":"train_data_v5.info()","8391582d":"train_data_v5['Age']","9de5d2f6":"train_data_v5['Age'].describe()\n","cb3bb7a7":"train_data_v5['Age*Pclass']=train_data_v5['Age'].astype('int')*train_data_v5['Pclass'].astype('int')\ntest_data_v5['Age*Pclass']=test_data_v5['Age'].astype('int')*test_data_v5['Pclass'].astype('int')","69fc157c":"# scailing  all numeric categories\ntrain_data_v5.columns","0b14462a":"# rain_data_v8['Farm*Age']=train_data_v8['num_Fare']*train_data_v8['Age']\n\n\nnumerical_columns=[\n 'Age',\n 'SibSp',\n 'Parch',\n 'num_Fare',\n 'Family_size','Fare','Age*Pclass']\nfrom sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\ntest_data_v7=test_data_v5.copy()\ntemp_scaled=train_data_v5.copy()\n\ntemp_scaled[numerical_columns]= scale.fit_transform(temp_scaled[numerical_columns])\n\ntrain_data_v7=temp_scaled\n\ntest_data_v7[numerical_columns]=scale.transform(test_data_v7[numerical_columns])\n\n","c60d8a9e":"def scailing_min_max_column(df_train,df_test,col):\n    scale = MinMaxScaler()\n    temp_scaled[numerical_columns]= scale.fit_transform(temp_scaled[numerical_columns])\n\ntrain_data_v7=temp_scaled\n\ntest_data_v7[numerical_columns]=scale.transform(test_data_v7[numerical_columns])\n    ","c171582c":"categorical_columns=['Sex',  'Cabin',\n       'Embarked', 'Title', 'first_num_tic', 'Age_was_missing']","557e3ea8":"print(train_data_v7['Cabin'].unique())\nprint(train_data_v7['first_num_tic'].unique())\nprint(train_data_v7['Embarked'].unique())\nprint(train_data_v7['Title'].unique())\n","d8210609":"\n# label encoding - this option was drop for one hot encoded\ntempTr=train_data_v7.copy()\ntempTe=train_data_v7.copy()\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nlabel_encoding_columns=[ 'first_num_tic']\n\nfor col in label_encoding_columns:\n    print(f'col - {col}')\n    train_data_v7[col]=label_encoder.fit_transform(tempTr[col])\n    train_data_v7[col]=label_encoder.transform(tempTe[col])\n    \n\n\n","e5f0cac6":"train_data_v8=train_data_v7.copy()\ntest_data_v8=test_data_v7.copy()\n\n\nhot_columns=['Sex',\n       'Embarked', 'Title','Pclass','is_alone','Age_bin','Cabin','Fare_cat']\n\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_data_v8[hot_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test_data_v8[hot_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_data_v8.index\nOH_cols_test.index = test_data_v8.index\n\n\nOH_cols_train.columns = OH_encoder.get_feature_names(hot_columns)\nOH_cols_test.columns = OH_encoder.get_feature_names(hot_columns)\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_data_v8.drop(hot_columns, axis=1)\nnum_X_test = test_data_v8.drop(hot_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n\n# OH_X_train.columns = OH_encoder.get_feature_names(hot_columns)\n\n","bb6e4162":"train_data_v8=OH_X_train\ntest_data_v8=OH_X_test","9490bcbd":"train_data_v8.columns","30aa1d54":"# train_data_v8['Farm*Age']\n# rain_data_v8['Farm*Age']=train_data_v8['num_Fare']*train_data_v8['Age']","9252d040":"selected_features=[ 'Age', 'SibSp', 'Parch', 'Family_size',\n       'Sex_female', 'Sex_male', 'Embarked_C',\n       'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss', 'Title_Mr',\n       'Title_Mrs', 'Title_crew', 'Title_royal', 'Pclass_1', 'Pclass_2',\n       'Pclass_3', 'is_alone_0', 'is_alone_1', 'Age_bin_Adults',\n       'Age_bin_Children', 'Age_bin_Middle Age', 'Age_bin_Retired',\n       'Age_bin_Senior', 'Age_bin_Teens', 'Age_bin_Young Adults',\n       'Age_bin_Youngsters', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D',\n       'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_n',\n       'Fare_cat_Extremely High', 'Fare_cat_Extremely Low', 'Fare_cat_High', 'Fare_cat_Low',\n       'Fare_cat_Very High', 'Fare_cat_Very Low','Age*Pclass']\n\ny=train_data_v8['Survived']\ntrain=train_data_v8[selected_features]\ntest=test_data_v8[selected_features]","8316bc80":"train","72a30745":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_regression\n\n\n\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import GridSearchCV \nfrom xgboost import XGBClassifier","7ff30784":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\nscoring = {'accuracy': make_scorer(accuracy_score) ,  'prec': 'precision','recall':'recall',\n           'auc':'roc_auc'}","25c70e1f":"svc=SVC()\ncv = cross_val_score(svc,train,y,cv=5)\nprint(cv)\nprint(cv.mean())","34e34ef9":"lr=LogisticRegression()\ncv = cross_val_score(lr,train,y,cv=5)\nprint(cv)\nprint(cv.mean())","9df85348":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,train,y,cv=5)\nprint(cv)\nprint(cv.mean())\n\n","4d6d1012":"# Find optimum hyperparameters with RandomSearch\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distribs = {\n        'n_estimators': randint(low=50, high=150),\n        'max_features': randint(low=5, high=15),\n        'min_samples_split': randint(low=10, high=30),\n    }\n\nforest_clf = RandomForestClassifier(random_state=42)\nrnd_search_rf = RandomizedSearchCV(forest_clf, param_distributions=param_distribs, n_jobs=-1,\n                                n_iter=50, cv=5, scoring=\"accuracy\", random_state=42, return_train_score=True)\nrnd_search_rf.fit(train,y)\nprint(rnd_search_rf.best_params_)","558f079f":"# Find optimum hyperparameters with RandomSearch\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distribs = {\n        'n_estimators': randint(low=50, high=150),\n        'max_features': randint(low=5, high=15),\n        'min_samples_split': randint(low=10, high=30),\n    }\n\nforest_clf = RandomForestClassifier(random_state=42)\nrnd_search_rf = RandomizedSearchCV(forest_clf, param_distributions=param_distribs, n_jobs=-1,\n                                n_iter=50, cv=5, scoring=\"roc_auc\", random_state=42, return_train_score=True)\nrnd_search_rf.fit(train,y)\nprint(rnd_search_rf.best_params_)","2976bcb9":"X_train, X_test, Y_train, Y_test = train_test_split(train, y, train_size=0.80, test_size=0.15, stratify=y)\nprint('Train\/Test Sizes : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","7d6d2770":"\nparam_grid = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5,6,7,8], 'C': [.1, 1, 10, 100, 1000]},{'kernel': ['sigmoid'], 'C': [.1, 1, 10, 100, 1000]}]\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = GridSearchCV(SVC(),param_grid = param_grid\n                                \n                                 , scoring=\"accuracy\", cv=5)\ngrid.fit(train,y)\nprint('Best Parameters                               : ',grid.best_params_)\nprint('Best Score                                    : ',grid.best_score_)","89586280":"\n# param_grid = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n#                                   'C': [.1, 1, 10, 100, 1000]},\n#                                  {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n#                                  {'kernel': ['poly'], 'degree' : [2,3,4,5,6,,7,8,9], 'C': [.1, 1, 10, 100, 1000]}]\n\nX_train, X_test, Y_train, Y_test = train_test_split(train, y, train_size=0.80, test_size=0.15, stratify=y)\nprint('Train\/Test Sizes : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\nparam_grid = [{'kernel': ['poly'], 'degree' : [2,3,4,5,6,7,8,9], 'C': [0.01,.1, 1, 10, 100, 1000]}]\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = GridSearchCV(SVC(),param_grid = param_grid, scoring=\"roc_auc\", cv=5)\ngrid.fit(X_train,Y_train)\nprint('Best Parameters                               : ',grid.best_params_)\nprint('Best Score                                    : ',grid.best_score_)\n\ndecision_function = grid.best_estimator_.decision_function(X_test)\nfpr, tpr, thresholds = roc_curve(Y_test, decision_function)\n\nprint('True Positive Rates                           : ', tpr)\nprint('False Positive Rates                          : ', fpr)\nprint('Different Thresholds For Calculating TPR, FPR : ', thresholds)\nprint('Classification Report                         : ')\nprint(classification_report(Y_test, grid.best_estimator_.predict(X_test)))\n\nacc = grid.best_estimator_.score(X_test, Y_test)\nauc = roc_auc_score(Y_test, decision_function)\n\nwith plt.style.context(('ggplot', 'seaborn')):\n    plt.figure(figsize=(8,6))\n    plt.scatter(fpr, tpr, c='blue')\n    plt.plot(fpr, tpr, label=\"Accuracy:%.2f AUC:%.2f\" % (acc, auc), linewidth=2, c='red')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (recall)\")\n    plt.title('ROC Curve')\n    plt.legend(loc='best');","0abcdb51":"\n\n\n\n\nparam_grid = [{'penalty' : ['l1', 'l2'], 'C' : np.logspace(-4, 4, 20), 'solver' : ['liblinear']}]\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='roc_auc')\ngrid.fit(train,y)\n\n\nprint('Best Parameters                               : ',grid.best_params_)\nprint('Best Score                                    : ',grid.best_score_)","21780eb0":"param_grid = [{'penalty' : ['l1', 'l2'], 'C' : np.logspace(-4, 4, 20), 'solver' : ['liblinear']}]\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\ngrid.fit(train,y)\n\n\nprint('Best Parameters                               : ',grid.best_params_)\nprint('Best Score                                    : ',grid.best_score_)","91473cbd":"from sklearn.metrics import confusion_matrix\n\nconfusion=confusion_matrix(Y_test, grid.predict(X_test))\npd.DataFrame(data=confusion)","021b0ffa":"svc=SVC(C= 1, gamma= 'scale')\n\n# C': 1.0, 'gamma': 'scale'predict","8c9f9ccd":"        \nsvc_prec_recall=SVC(C= 10, gamma= 'auto')","7d2acc9f":"def train_final_and_subbmit(final_model,X,y,test,model_name):\n    final_model.fit(X, y)\n    pred = final_model.predict(test)\n    submission = pd.DataFrame({\n        \"PassengerId\": test.reset_index()[\"PassengerId\"],\n        \"Survived\": pred\n    })\n    \n    submission.to_csv('submission_'+model_name+'.csv', index=False)","b8c33561":"# the best so far . 0.776\n# 'C': 0.1, 'degree': 5, 'kernel': 'poly'\nsvc_opti_2=SVC(C= 0.1, degree=5, kernel='poly')\n\ntrain_final_and_subbmit(svc_opti_2,\ntrain,y,test,'SVC_OPTI_V2')\n\npred_df = pd.read_csv('submission_SVC_OPTI_V2.csv')\nsns.countplot(x='Survived', data=pred_df)","f4158631":"\n# 'C': 0.615848211066026, 'penalty': 'l1', 'solver': 'liblinear'\nlr=LogisticRegression(C=0.615848211066026, penalty= 'l1', solver = 'liblinear')\ntrain_final_and_subbmit(rf,\ntrain,y,test,'lr_opti_auc')\n\npred_df = pd.read_csv('submission_lr_opti_auc.csv')\nsns.countplot(x='Survived', data=pred_df)","e4e965b1":"\nlr=LogisticRegression(C=.6237, penalty= 'l1', solver = 'liblinear')\ntrain_final_and_subbmit(rf,\ntrain,y,test,'lr_opti_acc')\n\npred_df = pd.read_csv('submission_lr_opti_acc.csv')\nsns.countplot(x='Survived', data=pred_df)","65fe5c2e":"# train_final_and_subbmit(LR,\n# train,y,test,'LR_improve_features_v2')\n\n# pred_df = pd.read_csv('submission_LR_improve_features_v2.csv')\n# sns.countplot(x='Survived', data=pred_df)\n","b83efb60":"sns.countplot(x='Survived', data=train_data_v8)\n","b1ad867b":"sns.countplot(x='Survived', data=OH_X_train)","17224aa6":"### Name ","3cf072b1":"### Cabin exploration and feature engneering \n","b8fa17de":"### Fare ","3f3325d5":"Optimizing SVM  by accuracy","752dc99b":"# One-Hot encoding and label encoding ","177f1573":" RandomForess","15fbee3f":"adding null as a 'n' ","de7c6efd":"# The project : prediction titanic survivers\n\n\nIn this project, I focused mainly on optimizing the svm model for the prediction. This was decided after the svm reached better results \nthan the logistic regression and random forrest, both after hyperparametr optimization. \n","61e9d91a":"Being in class 3 means you are more likely to survive.","07738f6f":"# Selecting features","11450d42":"74 % of the women survived ! only 20 % of the men survived. We have here skewed data","de98dda9":"# Scailing","3df1da1a":"#### SibSp","2ba7fc10":"Optimizing random forrest - by accuracy ","da00e75f":"Feature engneering age into age groups ","51bc46c1":"### Exploring how the sex type relates to the survival rate.","a1e5555e":"> ","fbcb7513":"# Conclutions: \n\nAfter examining the three models, logisitc regression, RandomForrest and SVM, both in scoring and AUC, the best prediction (on the copetition) was SVM, where the hyperparametrs tuning was conducted via auc scoring. \n\nIt seems reasonable the assume that the reason the RF did poorly, is the fact that there is relativly little amount of data, which impairs the way RF algorithm optimize- while doing bootstaping , i.e sample randomly from the data for each tree, hence, if the size of the data is small, the diffrent trees will have more correlation between them, which inturn impair the algorithm. \n\nFor logisitc regression, the fact that it was not as good as the SVM might be due to the fact that the linear regression cant account for nonlinear relation, where in the svm, we can (for example by using poly kernel) ","d01c244c":"# Model building ! ","b3511bef":"Above we can see that are models tends to predict more false positive than false negative.","c64af3b8":"lets see wether the first word of the cabbin has correlation with survival ratr ","2545724a":"### Second approch to age missing values - using random selection by mean and std - this approach seem to give better accuaracy. \n\nThis approach seem to give better results. ","290ade3c":"general view on three models - SVM, Logisitic regression and RandomForesst","0eb96441":"### one way to go - impute age by mean of title","8cc12ce6":"Optimizing random forrest - by AUC  ","688b1851":"# Dealing with categorical data:","720e6dad":"# Embarked and Fare imputation ","2d1aa31b":"there not seem to be anything special about this two who dont hae the embarked data. \nI shall imputat it with the ","248ee2d3":"SVC","f1eb9652":"### Ticket","e6ad4ed4":"Missing data ","7afe5a0b":"Optimizing SVM  by AUC\n","e493a374":"### Age ","16413956":"Logisitc regression  AUC optimization ","19e4ebcd":"Logisitic regression","dc1817f8":"Creating new features ","72ea1660":"The final traing data : ","c8ebaf64":"Using metrics. \n\nAccuracy is not good enough metric for this data. this here we have some skewd data, for example : onlys predict not survive will\nhave accuracy of %60, only women survive will have accuracy of 75% ! \n","1e80001b":"### PARCH ","82162a89":"# Adding the ONE hot option ","c910e533":"before we continue what are the relevant colummns i want t use? \n\n\n'Survived', 'Pclass', , 'Sex', 'Age', 'SibSp', 'Parch', ',\n       'Fare', 'Cabin', 'Embarked', 'Title', 'Family_size', 'first_num_tic',\n       'Age_was_missing'\n\ndrop :  'Name' Ticket'","973cad90":"Lable encoding: ","652a2523":"Being in some decks means you are more likely to survive","e329ff4f":"# Feature engneering - Fare","06f7790e":"### Pessenger class ","06b02798":"Logisitc regression  accuracy optimization \n","c7cc3123":"Feature engneering name and creating new feature - title","d09e0389":"#  Data cleaning\n\n## handling missing values \n","fb781533":"### Age ","15ac55d2":"# Loading the data","0b3a6f93":"# EDA  & feature engneering "}}