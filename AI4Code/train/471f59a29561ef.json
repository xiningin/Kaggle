{"cell_type":{"bbaf955e":"code","f34833fc":"code","5fbe6568":"code","a0519441":"code","8c61b419":"code","f60ea48e":"code","01d1ed5b":"code","2b325b4b":"code","d49db3cf":"code","d7e92bed":"code","a2d3441f":"code","6c5fc398":"code","3370884c":"code","ee802300":"code","5584a945":"code","89cf2338":"code","4d3c3af4":"code","9356afbb":"markdown","9adf91bc":"markdown","eaf754b7":"markdown","1cfc03c3":"markdown","2da1ffc3":"markdown","dad3105d":"markdown","5df35d0b":"markdown","e2e3f5ba":"markdown","ec4f1249":"markdown","10eeb48b":"markdown","878d2949":"markdown"},"source":{"bbaf955e":"!pip -q install -U lightautoml","f34833fc":"# Standard python libraries\nimport os\nimport time\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.report.report_deco import ReportDeco","5fbe6568":"N_THREADS = 4\nN_FOLDS = 5\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nTIMEOUT = 7*3600*2\nTARGET_NAME = 'claim'","a0519441":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","8c61b419":"%%time\n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntrain_data.head()","f60ea48e":"test_data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\ntest_data.head()","01d1ed5b":"samp_sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsamp_sub.head()","2b325b4b":"features = test_data.columns","d49db3cf":"train_data['n_missing'] = train_data[features].isna().sum(axis=1)\ntest_data['n_missing'] = test_data[features].isna().sum(axis=1)\n\ntrain_data['mean'] = train_data[features].mean(axis=1)\ntest_data['mean'] = test_data[features].mean(axis=1)\n\ntrain_data['median'] = train_data[features].median(axis=1)\ntest_data['median'] = test_data[features].median(axis=1)\n\n#train_data['std'] = train_data[features].std(axis=1)\n#test_data['std'] = test_data[features].std(axis=1)\n\ntrain_data['var'] = train_data[features].var(axis=1)\ntest_data['var'] = test_data[features].var(axis=1)\n\ntrain_data['skew'] = train_data[features].skew(axis=1)\ntest_data['skew'] = test_data[features].skew(axis=1)\n\ntrain_data['kurtosis'] = train_data[features].kurtosis(axis=1)\ntest_data['kurtosis'] = test_data[features].kurtosis(axis=1)\n\ntrain_data['10%'] = train_data[features].quantile(q=0.1, axis=1)\ntest_data['10%'] = test_data[features].quantile(q=0.1, axis=1)\n\ntrain_data['25%'] = train_data[features].quantile(q=0.25, axis=1)\ntest_data['25%'] = test_data[features].quantile(q=0.25, axis=1)\n\ntrain_data['75%'] = train_data[features].quantile(q=0.75, axis=1)\ntest_data['75%'] = test_data[features].quantile(q=0.75, axis=1)\n\n#train_data['iqr']=train_data['75%'] - train_data['25%']\n#test_data['iqr'] = test_data['75%'] - test_data['25%']\n\ntrain_data['90%'] = train_data[features].quantile(q=0.9, axis=1)\ntest_data['90%'] = test_data[features].quantile(q=0.9, axis=1)","d7e92bed":"from autowoe import AutoWoE\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\n\ndef get_oof_and_test_pred(tr, real_te):\n    skf = StratifiedKFold(n_splits=3, random_state = RANDOM_STATE)\n\n    oof_preds_woe = np.zeros(len(tr))\n    real_test_preds_woe = np.zeros(len(real_te))\n\n    y = tr[TARGET_NAME].values\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(y, y)):\n\n        X_tr, X_val = tr.iloc[train_idx, :], tr.iloc[val_idx, :]\n\n        auto_woe = AutoWoE(monotonic=False,\n                         vif_th=20.,\n                         imp_th=0,\n                         th_const=32,\n                         force_single_split=True,\n                         min_bin_size = 0.005,\n                         oof_woe=True,\n                         n_folds=5,\n                         n_jobs=N_THREADS,\n                         regularized_refit=True,\n                         verbose=0)\n        _, X_tr = train_test_split(X_tr, test_size = 200000, random_state = RANDOM_STATE, stratify = X_tr[TARGET_NAME].values)\n        print(X_tr.shape)\n        auto_woe.fit(X_tr.drop('id', axis = 1), \n                     target_name=\"claim\")\n\n        val_pred = auto_woe.predict_proba(X_val)\n        print(\"FOLD {}, AUC_SCORE = {:.5f}\".format(fold, roc_auc_score(X_val['claim'], val_pred)))\n\n        oof_preds_woe[val_idx] = val_pred\n        real_test_preds_woe += auto_woe.predict_proba(real_te) \/ N_FOLDS\n\n    print(\"AUC_SCORE TRAIN = {:.5f}\".format(roc_auc_score(y, oof_preds_woe)))\n    \n    return oof_preds_woe, real_test_preds_woe","a2d3441f":"oof_preds_woe, real_test_preds_woe = get_oof_and_test_pred(train_data, test_data)\n\n\noof_preds_woe2, real_test_preds_woe2 = get_oof_and_test_pred(train_data, test_data)","6c5fc398":"train_data['oof_woe_1'] = oof_preds_woe\ntest_data['oof_woe_1'] = real_test_preds_woe\n\ntrain_data['oof_woe_2'] = oof_preds_woe2\ntest_data['oof_woe_2'] = real_test_preds_woe2\n\ntrain_data['oof_woe_12'] = 0.5 * oof_preds_woe + 0.5 * oof_preds_woe2\ntest_data['oof_woe_12'] = 0.5 * real_test_preds_woe + 0.5 * real_test_preds_woe2\n\ntrain_data['rank_oof_woe_1'] = rankdata(oof_preds_woe)\ntest_data['rank_oof_woe_1'] = rankdata(real_test_preds_woe)\n\ntrain_data['rank_oof_woe_2'] = rankdata(oof_preds_woe2)\ntest_data['rank_oof_woe_2'] = rankdata(real_test_preds_woe2)\n\ntrain_data['rank_oof_woe_12'] = 0.5 * rankdata(oof_preds_woe) + 0.5 * rankdata(oof_preds_woe2)\ntest_data['rank_oof_woe_12'] = 0.5 * rankdata(real_test_preds_woe) + 0.5 * rankdata(real_test_preds_woe2)","3370884c":"%%time\n\ntask = Task('binary', )","ee802300":"%%time\n\nroles = {'target': TARGET_NAME,\n         'drop': ['id']\n         }","5584a945":"%%time \n\nautoml = TabularAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                       tuning_params = {'max_tuning_time': 1900}, # more time for params tuning\n                       general_params = {'use_algos': [['lgb', 'lgb_tuned']]},\n                       selection_params = {'mode': 0} # no feature selection - everything is necessary :)\n                      )\n\noof_pred = automl.fit_predict(train_data, roles = roles)","89cf2338":"test_pred = automl.predict(test_data)\nprint('Prediction for test_data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))","4d3c3af4":"samp_sub[TARGET_NAME] = test_pred.data[:, 0]\nsamp_sub.to_csv('In_LightAutoML_we_trust.csv', index = False)","9356afbb":"# =========== LightAutoML model building ===========\n\n\n# Step 1. Task setup\n\nOn the cell below we create Task object - the class to setup what task LightAutoML model should solve with specific loss and metric if necessary (more info can be found [here](https:\/\/lightautoml.readthedocs.io\/en\/latest\/generated\/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task) in our documentation):","9adf91bc":"# Step 0.3. Imported models setup\n\nFor better reproducibility we fix numpy random seed with max number of threads for Torch (which usually try to use all the threads on server):","eaf754b7":"To solve the task, we need to setup columns roles. The **only role you must setup is target role**, everything else (drop, numeric, categorical, group, weights etc.) is up to user - LightAutoML models have automatic columns typization inside:","1cfc03c3":"# Additional materials\n\n- [Official LightAutoML github repo](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n- [LightAutoML documentation](https:\/\/lightautoml.readthedocs.io\/en\/latest)","2da1ffc3":"# Step 6. Retrain on the full dataset","dad3105d":"# Step 7. Create submission file","5df35d0b":"# Step 2. Feature roles setup","e2e3f5ba":"# Step 0.6. Add OOFs and Test predictions from AutoWoE models","ec4f1249":"# Step 0.1. Import libraries\n\nHere we will import the libraries we use in this kernel:\n- Standard python libraries for timing, working with OS etc.\n- Essential python DS libraries like numpy, pandas, scikit-learn and torch (the last we will use in the next cell)\n- LightAutoML modules: presets for AutoML, task and report generation module","10eeb48b":"# Step 0.4. Data loading\nLet's check the data we have:","878d2949":"# Step 0.2. Constants\n\nHere we setup the constants to use in the kernel:\n- `N_THREADS` - number of vCPUs for LightAutoML model creation\n- `N_FOLDS` - number of folds in LightAutoML inner CV\n- `RANDOM_STATE` - random seed for better reproducibility\n- `TEST_SIZE` - houldout data part size \n- `TIMEOUT` - limit in seconds for model to train\n- `TARGET_NAME` - target column name in dataset"}}