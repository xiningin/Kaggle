{"cell_type":{"a01e2016":"code","754747ab":"code","fa83248f":"code","e5e2445b":"code","3ed673dc":"code","6f5f843b":"code","dce8c03c":"code","6b9f3f48":"code","0471fb6a":"code","9690b7d1":"code","7864d73b":"code","081cc5d9":"code","437412a2":"code","9782c07b":"code","a7975b51":"code","b85f88a0":"code","9aed8abe":"code","1349370d":"code","5b43a70c":"code","9adc2ac5":"code","d327f39a":"code","94db55a6":"code","8686e7e2":"code","eca7b780":"code","eda28d05":"code","97c77ed7":"code","bc05d153":"code","ba87b6e3":"code","eb6cee9c":"code","f3a08dc3":"code","81fab7aa":"code","9dfbfe29":"code","ba8c4466":"code","2ea419cb":"code","8d0c3e44":"code","ac0e1936":"code","8974c0a0":"code","fd6a1502":"code","1ed1ff9f":"code","40940ade":"code","9d4f1aa8":"code","d5816020":"code","bb6f6bda":"code","541a21f4":"code","486ae91e":"code","a5efffc6":"code","05523d9a":"code","ce744804":"code","9a9b42b0":"code","71029175":"code","2b75ed6b":"code","c2a65055":"code","ada9a2ed":"code","880ba3ea":"code","d8466e4f":"code","4c48ac71":"code","02019958":"code","11733a24":"code","b12cbbc4":"code","eca5445e":"code","88fb089d":"code","4168a7a6":"code","b0608569":"code","1c9efb6e":"code","d7b57fea":"markdown","9a528223":"markdown","286ecfd7":"markdown","93486195":"markdown","1694e2fd":"markdown","a3e619cb":"markdown","07e4e5d1":"markdown","870fb7d9":"markdown","07e563e1":"markdown","ee90cbf4":"markdown","f95246ab":"markdown","a309e9a4":"markdown","f3505f7a":"markdown","9bf765af":"markdown","22b24a49":"markdown","b01a5052":"markdown","9d54f6c0":"markdown","a15abecd":"markdown","f88f907a":"markdown","cf446810":"markdown","a09a1d87":"markdown","bf8588bc":"markdown","2b5fc507":"markdown","1db3c381":"markdown","ba3dd18f":"markdown","4147b8a8":"markdown","c3ad26f2":"markdown","6126ba97":"markdown","8a6b82c7":"markdown","6a3bfd8a":"markdown","25b739f7":"markdown","e55c4690":"markdown","523a13e3":"markdown","b15b5ace":"markdown","134b2cdb":"markdown","ba23265b":"markdown","f7ed664e":"markdown","9693d7bb":"markdown","1e317529":"markdown"},"source":{"a01e2016":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nfrom scipy import stats\nimport plotly\nimport itertools\nimport warnings\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.tree import export_graphviz\nimport graphviz\nwarnings.filterwarnings('ignore')\n%matplotlib inline","754747ab":"data = pd.read_csv('..\/input\/data.csv')","fa83248f":"data.info()","e5e2445b":"list = ['id', 'Unnamed: 32']\ndata.drop(list, axis = 1, inplace = True)","3ed673dc":"data.head()","6f5f843b":"data.describe()","dce8c03c":"plt.figure(figsize=(10,10))\nsns.countplot(data['diagnosis'],  palette = \"husl\")","6b9f3f48":"fig = plt.figure(figsize = (20,15))\nplt.subplot(221)\nstats.probplot(data['radius_mean'], dist = 'norm', plot = plt)\nplt.title('QQPlot for radius mean')\nplt.subplot(222)\nstats.probplot(data['texture_mean'], dist = 'norm', plot = plt)\nplt.title('QQPlot for texture mean')\nplt.subplot(223)\nstats.probplot(data['perimeter_mean'], dist = 'norm', plot = plt)\nplt.title('QQPlot for perimeter mean')\nplt.subplot(224)\nstats.probplot(data['area_mean'], dist = 'norm', plot = plt)\nplt.title('QQPlot for area mean')\nfig.suptitle('Features distribution', fontsize = 20)","0471fb6a":"# To see qqplots for rest features delete #!\n\n#plt.figure(figsize=(15,8))\n#stats.probplot(data['smoothness_mean'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for smoothness mean')\n#plt.show()\n#stats.probplot(data['compactness_mean'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for compactness mean')\n#plt.show()\n#stats.probplot(data['concavity_mean'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concavity mean')\n#plt.show()\n#stats.probplot(data['concave points_mean'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave points mean')\n#plt.show()\n#stats.probplot(data['fractal_dimension_mean'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for fractal dimension mean')\n#plt.show()\n#stats.probplot(data['radius_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for radius se')\n#plt.show()\n#stats.probplot(data['texture_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for texture se')\n#plt.show()\n#stats.probplot(data['perimeter_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for perimeter se')\n#plt.show()\n#stats.probplot(data['area_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave area se')\n#plt.show()\n#stats.probplot(data['smoothness_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for smoothness se')\n#plt.show()\n#stats.probplot(data['compactness_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for compactness se')\n#plt.show()\n#stats.probplot(data['concavity_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concavity se')\n#plt.show()\n#stats.probplot(data['concave points_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave points se')\n#plt.show()\n#stats.probplot(data['symmetry_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave symmetry se')\n#plt.show()\n#stats.probplot(data['fractal_dimension_se'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for fractal dimension se')\n#plt.show()\n#stats.probplot(data['radius_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for radius worst')\n#plt.show()\n#stats.probplot(data['texture_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for texture worst')\n#plt.show()\n#stats.probplot(data['perimeter_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for perimeter worst')\n#plt.show()\n#stats.probplot(data['area_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave area worst')\n#plt.show()\n#stats.probplot(data['smoothness_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for smoothness worst')\n#plt.show()\n#stats.probplot(data['compactness_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for compactness worst')\n#plt.show()\n#stats.probplot(data['concavity_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concavity worst')\n#plt.show()\n#stats.probplot(data['concave points_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave points worst')\n#plt.show()\n#stats.probplot(data['symmetry_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for concave symmetry worst')\n#plt.show()\n#stats.probplot(data['fractal_dimension_worst'], dist = 'norm', plot = plt)\n#plt.title('QQPlot for fractal dimension worst')\n#plt.show()","9690b7d1":"fig = plt.figure(figsize = (20, 20))\nplt.subplot(321)\nsns.boxplot(x = data['diagnosis'], y = data['radius_mean'], palette = \"husl\")\nplt.title('Radius mean')\nplt.subplot(322)\nsns.boxplot(x = data['diagnosis'], y = data['texture_mean'], palette = \"husl\")\nplt.title('Texture mean')\nplt.subplot(323)\nsns.boxplot(x = data['diagnosis'], y = data['perimeter_mean'], palette = \"husl\")\nplt.title('Perimeter mean')\nplt.subplot(324)\nsns.boxplot(x = data['diagnosis'], y = data['area_mean'], palette = \"husl\")\nplt.title('Area mean')\nplt.subplot(325)\nsns.boxplot(x = data['diagnosis'], y = data['smoothness_mean'], palette = \"husl\")\nplt.title('Smoothness mean')\nplt.subplot(326)\nsns.boxplot(x = data['diagnosis'], y = data['compactness_mean'], palette = \"husl\")\nplt.title('Compactness mean')\nfig.suptitle('Features boxplots to compare malignant and benign', fontsize = 20)","7864d73b":"fig = plt.figure(figsize = (20, 20))\nplt.subplot(321)\nsns.boxplot(x = data['diagnosis'], y = data['fractal_dimension_mean'], palette = \"husl\")\nplt.title('Fractal dimension mean')\nplt.subplot(322)\nsns.boxplot(x = data['diagnosis'], y = data['texture_se'], palette = \"husl\")\nplt.title('Texture se')\nplt.subplot(323)\nsns.boxplot(x = data['diagnosis'], y = data['smoothness_se'], palette = \"husl\")\nplt.title('Smoothness se')\nplt.subplot(324)\nsns.boxplot(x = data['diagnosis'], y = data['symmetry_se'], palette = \"husl\")\nplt.title('Symmetry se')\nplt.subplot(325)\nsns.boxplot(x = data['diagnosis'], y = data['fractal_dimension_se'], palette = \"husl\")\nplt.title('Fractal dimension se')\nfig.suptitle('Features boxplots to compare malignant and benign', fontsize = 20)\n","081cc5d9":"df1 = data[data['diagnosis'] == 'M']\ndf2 = data[data['diagnosis'] == 'B']\ndf1.drop('diagnosis', axis = 1, inplace = True)\ndf2.drop('diagnosis', axis = 1, inplace = True)","437412a2":"feature = []\nt_value = []\np_value = []\nfor column in df1.columns:\n    ttest = stats.ttest_ind(df1[column], df2[column])\n    feature.append(column)\n    t_value.append(ttest[0])\n    p_value.append(ttest[1])\nttest_data = {'feature' : feature, 't_value' : t_value, 'p_value' : p_value}\nttest_df = pd.DataFrame(ttest_data)\nttest_df.loc[ttest_df['p_value'] > 0.05]","9782c07b":"fig = plt.figure(figsize = (20,20))\nplt.subplot(321)\nsns.pointplot(y = data['diagnosis'], x = data['fractal_dimension_mean'], join= False, capsize= 0.1, palette= 'husl')\nplt.title('Confidence interval for fractal dimencion mean')\nplt.subplot(322)\nsns.pointplot(y = data['diagnosis'], x = data['texture_se'], join= False, capsize= 0.1, palette= 'husl')\nplt.title('Confidence interval for texture se')\nplt.subplot(323)\nsns.pointplot(y = data['diagnosis'], x = data['smoothness_se'], join= False, capsize= 0.1, palette= 'husl')\nplt.title('Confidence interval for smoothness se')\nplt.subplot(324)\nsns.pointplot(y = data['diagnosis'], x = data['symmetry_se'], join= False, capsize= 0.1, palette= 'husl')\nplt.title('Confidence interval for symmetry se')\nplt.subplot(325)\nsns.pointplot(y = data['diagnosis'], x = data['fractal_dimension_se'], join= False, capsize= 0.1, palette= 'husl')\nplt.title('Confidence interval for fractal dimension se')\nfig.suptitle('Confidence intervals', fontsize = 20)","a7975b51":"list = ['fractal_dimension_mean', 'texture_se', 'smoothness_se', 'symmetry_se', 'fractal_dimension_se']\ndata.drop(list, axis = 1, inplace = True)","b85f88a0":"plt.figure(figsize=(25,20))\nplt.title('Correlation matrix')\nsns.heatmap(data.corr(), cmap = \"Blues_r\", annot = True)","9aed8abe":"fig = plt.figure(figsize = (20,15))\nplt.subplot(231)\nsns.scatterplot(x = data['perimeter_mean'], y = data['radius_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Perimeter mean vs radius mean')\nplt.subplot(232)\nsns.scatterplot(x = data['area_mean'], y = data['radius_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs radius mean')\nplt.subplot(233)\nsns.scatterplot(x = data['radius_mean'], y = data['radius_worst'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Radius mean vs radius worst')\nplt.subplot(234)\nsns.scatterplot(x = data['area_mean'], y = data['perimeter_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs perimeter mean')\nplt.subplot(235)\nsns.scatterplot(x = data['area_se'], y = data['perimeter_se'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area se vs perimeter se')\nplt.subplot(236)\nsns.scatterplot(x = data['perimeter_mean'], y = data['radius_worst'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Perimeter_mean vs radious worst')\nfig.suptitle('Correlation > 0.9', fontsize = 20)","1349370d":"fig = plt.figure(figsize = (20,15))\nplt.subplot(231)\nsns.scatterplot(x = data['concavity_mean'], y = data['concavity_worst'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Concavity mean vs concavity worst (correlation = 0.88)')\nplt.subplot(232)\nsns.scatterplot(x = data['concavity_mean'], y = data['concave points_worst'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Concavity mean vs concave points worst (correlation = 0.86)')\nplt.subplot(233)\nsns.scatterplot(x = data['area_mean'], y = data['concave points_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs concave points mean (correlation = 0.82)')\nplt.subplot(234)\nsns.scatterplot(x = data['area_mean'], y = data['radius_se'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs radius se (correaltion = 0.73)')\nplt.subplot(235)\nsns.scatterplot(x = data['compactness_mean'], y = data['symmetry_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Compactness mean vs symmetry mean (correlation = 0.6)')\nplt.subplot(236)\nsns.scatterplot(x = data['area_mean'], y = data['compactness_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs compactness mean (correlation = 0.5)')\nfig.suptitle('0.5 < correlation < 0.9', fontsize = 20)","5b43a70c":"fig = plt.figure(figsize = (20,15))\nplt.subplot(221)\nsns.scatterplot(x = data['area_mean'], y = data['texture_mean'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs texture mean (correlation = 0.32)')\nplt.subplot(222)\nsns.scatterplot(x = data['area_mean'], y = data['compactness_se'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Area mean vs compactness se (correlation = 0.21)')\nplt.subplot(223)\nsns.scatterplot(x = data['concavity_se'], y = data['texture_worst'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Concavity se vs texture worst (correlation = 0.1)')\nplt.subplot(224)\nsns.scatterplot(x = data['radius_mean'], y = data['fractal_dimension_worst'], hue = \"diagnosis\", data = data, palette = \"husl\")\nplt.title('Radius mean vs fractal dimension worst (correaltion = 0.0071)')\nfig.suptitle('correlation < 0.5', fontsize = 20)","9adc2ac5":"y = data['diagnosis'].map({'M' : 1, 'B' : 0})","d327f39a":"drop_list = ['diagnosis', 'radius_mean', 'perimeter_mean', 'concavity_mean', 'radius_se', 'perimeter_se', 'radius_worst', 'perimeter_worst', \n             'compactness_mean', 'concave points_mean', 'area_se', 'area_worst', 'smoothness_worst', 'compactness_worst', 'compactness_se', \n             'concavity_worst', 'concavity_se', 'fractal_dimension_worst', 'smoothness_mean']","94db55a6":"X = data.drop(drop_list, axis = 1)","8686e7e2":"y.shape, X.shape","eca7b780":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","eda28d05":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 17)","97c77ed7":"acc_score = []","bc05d153":"f_tree = DecisionTreeClassifier(random_state = 17)","ba87b6e3":"tree_params = {'max_depth' : np.arange(1, 11), 'max_features' : np.arange(1, 8)}","eb6cee9c":"tree_grid = GridSearchCV(f_tree, tree_params, cv = 20, n_jobs = -1)","f3a08dc3":"%%time\ntree_grid.fit(X_train, Y_train)","81fab7aa":"tree_grid.grid_scores_","9dfbfe29":"tree_grid.best_estimator_","ba8c4466":"dot_data = tree.export_graphviz(tree_grid.best_estimator_, out_file = None, feature_names = X_test.columns, class_names= ['B', 'M'], filled = True, leaves_parallel = True)\ngraph = graphviz.Source(dot_data)\ngraph","2ea419cb":"Y_predict = tree_grid.best_estimator_.predict(X_test)","8d0c3e44":"acc_score.append(accuracy_score(Y_test, Y_predict))\naccuracy_score(Y_test, Y_predict)","ac0e1936":"cm1 = confusion_matrix(Y_test, Y_predict)\nclasses_name = ['B', 'M']\nplt.figure(figsize = (10, 10))\nplot_confusion_matrix(cm1, classes = classes_name,  normalize= False, title = 'Confusion matrix for Decision Tree Classifier' )","8974c0a0":"log_reg_cv = LogisticRegressionCV(n_jobs= -1, random_state= 17, cv = 20, solver= 'lbfgs' )","fd6a1502":"%%time\nlog_reg_cv.fit(X_train, Y_train)","1ed1ff9f":"Y_predict = log_reg_cv.predict(X_test)","40940ade":"acc_score.append(accuracy_score(Y_test, Y_predict))\naccuracy_score(Y_test, Y_predict)","9d4f1aa8":"cm2 = confusion_matrix(Y_test, Y_predict)\nclasses_name = ['B', 'M']\nplt.figure(figsize = (10, 10))\nplot_confusion_matrix(cm2, classes = classes_name,  normalize= False, title = 'Confusion matrix for Logistic Regression CV Classifier')","d5816020":"log_reg = LogisticRegression()","bb6f6bda":"%%time\nlog_reg.fit(X_train, Y_train)","541a21f4":"Y_predict = log_reg.predict(X_test)","486ae91e":"acc_score.append(accuracy_score(Y_test, Y_predict))\naccuracy_score(Y_test, Y_predict)","a5efffc6":"cm3 = confusion_matrix(Y_test, Y_predict)\nclasses_name = ['B', 'M']\nplt.figure(figsize = (10, 10))\nplot_confusion_matrix(cm3, classes = classes_name,  normalize= False, title = 'Confusion matrix for Logidtic Regression Classifier')","05523d9a":"knn = KNeighborsClassifier(algorithm='ball_tree', weights = 'distance')\nknn_params = {'n_neighbors' : np.arange(1, 20)}","ce744804":"grid = GridSearchCV(knn, knn_params, cv = 20, n_jobs = -1)","9a9b42b0":"grid.fit(X_train, Y_train)","71029175":"grid.grid_scores_","2b75ed6b":"grid.best_estimator_","c2a65055":"Y_predict_knn = grid.best_estimator_.predict(X_test)","ada9a2ed":"acc_score.append(accuracy_score(Y_test,Y_predict_knn))\naccuracy_score(Y_test, Y_predict_knn)","880ba3ea":"cm4 = confusion_matrix(Y_test, Y_predict_knn)\nclasses_name = ['B', 'M']\nplt.figure(figsize = (10, 10))\nplot_confusion_matrix(cm4, classes = classes_name,  normalize= False, title = 'Confusion matrix for KNN (weight points by the inverse of their distance)')","d8466e4f":"knn = KNeighborsClassifier(algorithm='ball_tree')\nknn_params = {'n_neighbors' : np.arange(1, 20)}","4c48ac71":"grid = GridSearchCV(knn, knn_params, cv = 20, n_jobs = -1)","02019958":"grid.fit(X_train, Y_train)","11733a24":"grid.best_estimator_","b12cbbc4":"grid.grid_scores_","eca5445e":"Y_predict_knn = grid.best_estimator_.predict(X_test)","88fb089d":"acc_score.append(accuracy_score(Y_test, Y_predict_knn))\naccuracy_score(Y_test, Y_predict_knn)","4168a7a6":"cm5 = confusion_matrix(Y_test, Y_predict_knn)\nclasses_name = ['B', 'M']\nplt.figure(figsize = (10, 10))\nplot_confusion_matrix(cm5, classes = classes_name,  normalize= False, title = 'Confusion matrix for KNN')","b0608569":"fig = plt.figure(figsize = (30,20))\nplt.subplot(321)\nclasses_name = ['B', 'M']\nplot_confusion_matrix(cm1, classes = classes_name,  normalize= False, title = 'Confusion matrix for Decision Tree Classifier' )\nplt.subplot(322)\nclasses_name = ['B', 'M']\nplot_confusion_matrix(cm2, classes = classes_name,  normalize= False, title = 'Confusion matrix for Logistic Regression CV Classifier')\nplt.subplot(323)\nclasses_name = ['B', 'M']\nplot_confusion_matrix(cm3, classes = classes_name,  normalize= False, title = 'Confusion matrix for Logidtic Regression Classifier')\nplt.subplot(324)\nclasses_name = ['B', 'M']\nplot_confusion_matrix(cm4, classes = classes_name,  normalize= False, title = 'Confusion matrix for KNN (weight points by the inverse of their distance)')\nplt.subplot(325)\nclasses_name = ['B', 'M']\nplot_confusion_matrix(cm5, classes = classes_name,  normalize= False, title = 'Confusion matrix for KNN')","1c9efb6e":"model_data = {'model' : ['Decision tree', 'Logic Regression CV', 'Logic Regression', 'KNN (distance)', 'KNN'], 'accuracy' : acc_score}\nmodel_df = pd.DataFrame(model_data)\nmodel_df","d7b57fea":"### 2.2.1 Target distribution\n<br><\/br>","9a528223":"<br><\/br>\n## 3.4 Logistic regression ","286ecfd7":"<br><\/br>\n#### Let's take a look on our data.\n<br><\/br>","93486195":"<br><\/br>\n#### First five boxplots show as that there is strong difference between two classes (M = malignant, B = benign). Also we can see, that some of the features have outliers. But we can find some intresting features. Look at the boxplots of five features (fractal_dimension_mean, texture_se, smoothness_se, symmetry_se, fractal_dimension_se) shown below. The distributions of these features for malignant and benign are almost the same. Thise is new information for as. We can make a guess that these features have no influence on diagnosis. Let's confirme these statement using statistics.\n<br><\/br>","1694e2fd":"## 3.1 Function for visualization and train test splitting","a3e619cb":"## 2.3 Statistical analysis","07e4e5d1":"#### Will find out how look like relationship between features with correlation > 0.9","870fb7d9":"#### Let's split data set in two sepparate data sets for malignant and for benign and drop diagnosis from datasets.  \n<br><\/br>","07e563e1":"<br><\/br>\n## 3.6 KNN","ee90cbf4":"<br><\/br>\n## 2.4 Correlation","f95246ab":"#### To compare two classes we will use Student's t-test. For our convenience I'll show features with p-value > 0.05.","a309e9a4":"<br><\/br>\n#### Let's check the correlation between features \n<br><\/br>","f3505f7a":"<br><\/br>\n### 2.3.2 Statistical analysis","9bf765af":"<br><\/br>\n## 4. Conclusion","22b24a49":"## 1.1 Loading libraries\n<br><\/br>","b01a5052":"## 5. P.S.","9d54f6c0":"<br><\/br>\n#### Some of our fetatures, like 'area_mean', 'area_worst' etc., have lage max values. Are they outliers? Possible yes. We can check thise by building distribution plots of the features.  For example boxplots, qqplots, histograms plots. Distribution plots can show as, is the distribution of the feature normal and have it outliers.  \n<br><\/br>","a15abecd":"### 2.3.1 Prepare dataset","f88f907a":"#### In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.","cf446810":"## 2.1 Data head and describe","a09a1d87":"#### Will find out how look like relationship between features with  0.5 < correlation < 0.9","bf8588bc":"<br><\/br>\n## 3.2 Decision Tree","2b5fc507":"# 3. Predictive Modeling\n","1db3c381":"<br><\/br>\n## 3.3 Logistic regression CV = 20","ba3dd18f":"#### Correlation < 0.5","4147b8a8":"# 1. Loading libraries and reading data","c3ad26f2":"### 4.1 Results","6126ba97":"## 2.2 Features and target distribution ","8a6b82c7":"<br><\/br>\n#### As we can see feature mean of each class is within confidence interval of opposite class. Thats confirm our statistical conclusion. We can drop thise features from our data.\n<br><\/br>\n","6a3bfd8a":"<br><\/br>\n### 2.2.2 Features distribution\n<br><\/br>","25b739f7":"<br><\/br>\n#### As we can see, all features are totally completed, exept 'Unnamed: 32'. Let drop features 'id' and 'Unnamed: 32' from our data set.\n<br><\/br>","e55c4690":"<br><\/br>\n# 2. Exploratory Data Analysis (EDA)","523a13e3":"<br><\/br>\n#### As we can see, the real distribution of each feature is close to it theoretical distribution. But some features have strong deviation from theoretical distribution of the feature outside [-2, 2]. Next we will build boxplots to compare two grops ( M = malignant, B = benign), and will find something intresting.\n<br><\/br>","b15b5ace":"<br><\/br>\n## 1.3 What about missing values?\n<br><\/br>","134b2cdb":"### Thise is my start in data science. I am not a native speaker. If there are mistakes, please tell me in the comments below. Thanks for reading :) ","ba23265b":"#### What is the best way to see the distribution of the feature and find outliers? As discussed earlier, boxplots, qqplots and histograms can help us. As for me, i prefer boxplots and qqplot. First we will look at the distributions of features by plotting qqplots. Next, boxplots will help us to confirm information about distribution and find outliers. Then we will build features boxplots for two classes (M = malignant, B = benign), and compare them.   \n<br><\/br>","f7ed664e":"<br><\/br>\n## 1.2 Reading the data\n<br><\/br>","9693d7bb":"<br><\/br>\n#### Five features have p-value > 0.05. It means that these features have no influence on diagnosis. To confirm thise statement we can build confidence intervals. Let's do thise.","1e317529":"<br><\/br>\n## 3.5 KNN (weight points by the inverse of their distance)"}}