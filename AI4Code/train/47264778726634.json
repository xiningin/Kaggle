{"cell_type":{"6a750c88":"code","867e7715":"code","d0626603":"code","92da4446":"code","d1fb825e":"code","711337c1":"code","8661ffe7":"code","37eb4151":"code","18c01566":"code","07f4a3d0":"code","8840daf4":"code","507ef1e3":"code","a401414d":"code","6d454221":"code","363ee909":"code","cd641392":"code","347271b0":"code","fc7e93b5":"code","8ffe0bb7":"code","1e476c0c":"code","254cfe25":"code","8dd4f795":"code","7c26b0c2":"code","3ad4e14b":"code","83b920dd":"code","377ec850":"code","8cf2b5c5":"code","2b2a378a":"code","b1353831":"code","7dd99c44":"markdown","2b1e9929":"markdown","1005cbc2":"markdown","17d73cd1":"markdown","6df1d49a":"markdown","f4e5236f":"markdown","fcbba04f":"markdown","94681562":"markdown","4eb6ebdd":"markdown","3fd7b094":"markdown","bf995f12":"markdown","b2a1f284":"markdown","492ef7fa":"markdown","6c92c2b3":"markdown","50b4130d":"markdown","ee9774af":"markdown","6a51bd6e":"markdown","e1a46c54":"markdown","072d6c02":"markdown","1d8fd1eb":"markdown","2788eea5":"markdown","10fbdc16":"markdown","cf8b207c":"markdown"},"source":{"6a750c88":"!pip install fastcluster","867e7715":"import re\nimport gc\nimport fastcluster\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom itertools import chain\nfrom scipy.stats import entropy\nfrom scipy.spatial.distance import squareform\nfrom scipy.cluster.hierarchy import fcluster\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom spacy import displacy\n\nRANDOM_STATE = 3472\n\nnp.random.seed(RANDOM_STATE)\n\n%matplotlib inline","d0626603":"df = pd.read_csv('\/kaggle\/input\/cleaning-cord-19-metadata\/cord_metadata_cleaned.csv')\n\n# A simplification algorithm returns NaN for somes texts => no useful information\ndf = df.dropna(subset=['text_simplified']).reset_index(drop=True)","92da4446":"tokens = df['text_simplified'].str.split(' ').tolist()\ntokens = pd.Series(chain(*tokens))\ntokens_count = tokens.value_counts()\ntokens_count","d1fb825e":"ax = tokens_count.plot(figsize=(15, 5))\nax.set_xlabel(\"Token\")\nax.set_ylabel(\"Count\")\nax.grid(True)","711337c1":"ax = tokens_count[10000:].plot(figsize=(15, 5))\nax.set_xlabel(\"Token\")\nax.set_ylabel(\"Count\"),\nax.grid(True)","8661ffe7":"tfidf_vectorizer = TfidfVectorizer(\n    input='content',\n    lowercase=False,\n    preprocessor=lambda text: text,  \n    tokenizer=lambda text: text.split(' '),\n    token_pattern=None,\n    analyzer='word',\n    stop_words=None,\n    ngram_range=(1, 1),\n    max_features=10000,\n    binary=False,\n    norm='l2',\n    use_idf=True,\n    smooth_idf=True,\n    sublinear_tf=False,\n)\n\nfeatures = tfidf_vectorizer.fit_transform(df['text_simplified'])\nfeatures.shape","37eb4151":"features = features.astype('float32').toarray()","18c01566":"sample_size = 0.1\nsample_mask = np.random.choice(\n    a=[True, False], \n    size=len(features), \n    p=[sample_size, 1 - sample_size]\n)\n\nfeatures_sample = features[sample_mask]\nfeatures_sample.shape","07f4a3d0":"%%time\ndistance_matrix = pairwise_distances(features_sample, metric='cosine')","8840daf4":"%%time \ndistances = squareform(distance_matrix, force='tovector')\nZ = fastcluster.linkage(distances, method='complete', preserve_input=True)","507ef1e3":"sns.clustermap(\n    data=distance_matrix,\n    col_linkage=Z, \n    row_linkage=Z,\n    cmap=plt.get_cmap('RdBu'),\n)","a401414d":"dissimilarities = pd.Series(distance_matrix.flatten())","6d454221":"ax = dissimilarities.hist(bins=100, figsize=(15, 5))\nax.set_xlabel(\"Cosine dissimilarity\")\nax.set_ylabel(\"Count\")\nax.grid(True)","363ee909":"ax = dissimilarities[dissimilarities >= 0.8].hist(bins=100, figsize=(15, 5))\nax.set_xlabel(\"Cosine dissimilarity\")\nax.set_ylabel(\"Count\")\nax.grid(True)","cd641392":"ax = dissimilarities[dissimilarities >= 0.95].hist(bins=100, figsize=(15, 5))\nax.set_xlabel(\"Cosine dissimilarity\")\nax.set_ylabel(\"Count\")\nax.grid(True)","347271b0":"# Cluster features\nclusters = fcluster(Z, t=0.999, criterion='distance')\n\n# Plot clustermap\nclustermap = sns.clustermap(\n    data=distance_matrix,\n    col_linkage=Z, \n    row_linkage=Z,\n    cmap=plt.get_cmap('RdBu'),\n)\n\n# Draw clusters on the clustermap plot\ncluster_mapping = dict(zip(range(len(features_sample)), clusters))\nclustermap_clusters = pd.Series(\n    [cluster_mapping[id_] for id_ in list(clustermap.data2d.columns)]\n)\n\nfor cluster in set(clusters):\n    cluster_range = list(clustermap_clusters[clustermap_clusters == cluster].index)\n    clustermap.ax_heatmap.add_patch(\n        patches.Rectangle(\n            xy=(np.min(cluster_range), np.min(cluster_range)), \n            width=len(cluster_range), \n            height=len(cluster_range),\n            fill=False,\n            edgecolor='lightgreen',\n            lw=2\n        )\n    )\n    \nprint(f'There are {clustermap_clusters.nunique()} clusters.')","fc7e93b5":"del clustermap\ndel distance_matrix\ndel distances\ndel Z\n\ngc.collect()","8ffe0bb7":"model = KNeighborsClassifier(n_neighbors=5, metric='cosine', n_jobs=-1)\nmodel.fit(features_sample, clusters)","1e476c0c":"df['cluster'] = model.predict(features)","254cfe25":"cluster_count = df['cluster'].value_counts().sort_values()\n\nax = cluster_count.plot(kind='bar', figsize=(15, 5))\nax.set_xticks([])\nax.set_xlabel(\"Cluster id\")\nax.set_ylabel(\"Count\")\nax.grid(True)","8dd4f795":"noise_clusters = set(cluster_count[cluster_count <= 5].index)\nnoise_mask = df['cluster'].isin(noise_clusters)\n\ndf.loc[noise_mask, 'cluster'] = -1","7c26b0c2":"cluster_count = df['cluster'].value_counts().sort_values()\n\nax = cluster_count.plot(kind='bar', figsize=(15, 5))\nax.set_xticks([])\nax.set_xlabel(\"Cluster id\")\nax.set_ylabel(\"Count\")\nax.grid(True)","3ad4e14b":"columns = np.array(tfidf_vectorizer.get_feature_names())\ntop_k = 3\n\ndef describe(df: pd.DataFrame) -> pd.DataFrame:\n    order = features[df.index].mean(axis=0).argsort()[::-1][:top_k]\n    top_words = columns[order]\n    \n    cluster_id = df['cluster'].iloc[0]\n    for i, word in enumerate(top_words):\n        # For noisy clusters don't use keywords!\n        df[f'word_{i + 1}'] = word if cluster_id != -1 else ''\n        \n    return df\n\ndf = df.groupby('cluster').apply(describe)","83b920dd":"df.filter(regex='text_simplified|word_\\d+', axis=1)","377ec850":"cluster_id = 10\ndf_cluster = df.loc[df['cluster'] == cluster_id, :]\n\nkeywords = (df\n    .loc[df['cluster'] == 10, ['word_1', 'word_2', 'word_3']]\n    .drop_duplicates()\n    .values\n    .tolist()[0]\n) \n\nkeywords","8cf2b5c5":"elements = []\nfor _, text in df_cluster['text_simplified'].items():\n    ents = []\n    text = '\u2022 ' + text\n    for keyword in keywords:\n        matches = list(re.finditer(keyword, text))\n        for match in matches:\n            start, end = match.span()\n            ents.append({\n                \"start\": start,\n                \"end\": end, \n                \"label\": 'KEYWORD'\n            })\n            \n    elements.append({\n        'text': text,\n        \"ents\": ents,\n    })","2b2a378a":"displacy.render(elements, style=\"ent\", jupyter=True, manual=True)","b1353831":"(df\n    .drop(columns=['title_lang', 'abstract_lang', 'distance'])\n    .to_csv('\/kaggle\/working\/cord_metadata_keywords.csv', index=False)\n)","7dd99c44":"## Visualize data","2b1e9929":"## Save results\nA reviewer, who works with this file, should explore each cluster separately & make decisions whether a cluster is worth further exploration after `N` studies are screened.","1005cbc2":"## TF-IDF & hierarchical clustering","17d73cd1":"### Obtain features\n10000 top tokens in the dictionary seem enough.","6df1d49a":"## Install missing packages","f4e5236f":"### Find clustering threshold","fcbba04f":"On the first glance, there are no similar documents (almost every pair is blue i.e. distance is close to 1.0). The reason is that vector space is 10000 dimensional. For example if there are documents with 5 keywords in common, then comparing to 10000 \"possible agreements\", they result in cosine distance around `1.0`. The `clustermap` scale is wrong, because we have `0.0` distance on diagonal. The clustermap would look look reasonably if you change `TfidfVectorizer.max_features=100`, but then you discard some potential important keywords.","94681562":"### Create vocabulary","4eb6ebdd":"Each cluster has on average at least 10 elements, hence for safety we set up `n_neighbors=5` to minimize uncertainty.","3fd7b094":"### What does each cluster represent?\nWe saw some fancy kernels with PCA projection or T-SNE. First of all we don't think that 2D plot, where each datapoint is a study, brings any value to reviewers. Note that it's hard enough for Data Scientist to make any [conclusions from such plots](https:\/\/distill.pub\/2016\/misread-tsne\/). We decided to give reviewers a grasp of keywords that are associated with each cluster.","bf995f12":"The `complete` link merges outliers late, because they would increase maximum distances too much. Hence it is more robust to outliers (it prefers inliers first). Such property minimizes the chance of finiding relevant document in the \"irrelevant cluster\".","b2a1f284":"### Cluster the whole dataset\nWe couldn't perform calculation on the whole dataset due to memory issues. We decided to use KNN method to obtain clusters for the remaining 40k datapoints. Let's free some RAM first.","492ef7fa":"Not the scale we need, let's truncate top tokens.","6c92c2b3":"We decided not to truncate tokens from top of the list, because it contains some important tokens like `viru`,`infect`, `protein`. They could be used as keywords in the screening process.","50b4130d":"On averate each cluster should have at least 10 elements. Some elements will be appended to the current clusters if perform clustering on the whole dataset.","ee9774af":"Distance matrix & linkage calculation & plotting takes time and memory, therefore we decided to first work on 10% sample.","6a51bd6e":"## Read data","e1a46c54":"## Introduction\n\n### Background\nThe task is to find answers for the given medical question. Experts from the medical community are already familiar with such task producing many systematic reviews every year (e.g. [Cochrane](https:\/\/www.cochranelibrary.com\/) produces a great deal of them). A systematic review is a systematic medical knowledge synthesis of existing studies. Researchers work worldwide in distributed environments, working simultaneously on the same experiments. As a result, a lot of studies are published, often with contradictory or uncertain outcomes. The goal of a systematic review is to analyze existing studies and in turn make valid conclusions. \n\n#### How does the systematic review process look like?\n\n1. A relevant subset of studies is retrieved using a search strategy. The most common form of the search strategy is a Boolean query for a specific database. In the competition, we are already given a set of studies.\n\n2. The screening process takes place, which is performed in two stages. In the first stage reviewers investigate articles based on titles and abstracts & make decisions, which articles are relevant to the given question. Their goal is to minimize false negatives i.e. the chance that they miss the potentially relevant study. The reason behind this is that they don\u2019t have access to the full-texts, therefore they are uncertain about their decision. As a result, they decide to include the study for the next stage. The stage is also required to save money because in many cases the access to full-text is paid. In the second stage, the full-texts are obtained and screened. Take in mind that now reviewers have access to the full information. Reviewers need to know the rules of study relevancy level assessment. The rules are expressed as inclusion & exclusion criteria. Such rules are different for the first and second stages of the screening process. Although in the competition we are not given detailed criteria, the task description itself is a combination of question and inclusion criteria.\n\n3. Reviewers extract information from studies into the given extraction form. Extraction form is a set of fields that describe the object e.g. test article or control group. In some cases, the risk of bias is additionally performed to evaluate the certainty of the evidence (e.g. using [ROBINS](https:\/\/www.riskofbias.info\/) methodology).\n\n4. Conclusions are made & guidelines are published. Many systematic reviews include meta-analysis i.e. a quantitive way to make conclusions from data.\n\n### Objective  \nEach step exists to save reviewers' time in the succeeding steps. **<span style=\"color:green\">Our goal is to  save time during screening step using machine learning methods.<\/span>** \n\n### Methods\nThe dataset doesn't contain any expert knowledge, thus only pre-trained supervised models, unsupervised or semi-supervised models are applicable. Initially, we thought about using [K Nearest Neighbours](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm) method algorithm for the given task description. The main business issue with such an approach is that it isn't clear what K should be to minimize false negatives. Another slightly different issue is that the task description is from a different distribution than the studies' distribution.\n\n\n**<span style=\"color:green\">Our approach is to create clusters of studies & sample a subset of studies from each cluster. Each reviewer then goes through the sample & decide whether a cluster of studies is worth further exploration. It allows to quickly discard some clusters and in turn save time.<\/span> ** For example if there was only 1 relevant study in the set of 1000 screened documents (the cluster has 2000 total studies), then it wouldn't be worth to explore further the cluster. Eventually, the decision belongs to the reviewer, but it is the property of every decision support system. \n\n#### Text representation\n\n1. One of the most popular methods for text representation is TF-IDF. In theory, it is language-independent, but in practice, it requires a tokenization algorithm that is language-dependent (see [Japanese language](http:\/\/www.lrec-conf.org\/proceedings\/lrec2018\/pdf\/8884.pdf)). It works best if the least informative tokens and noise tokens are removed according to [Zipf's law](https:\/\/en.wikipedia.org\/wiki\/Zipf%27s_law). The main drawbacks of TF-IDF are that it doesn't understand language and it is very sensitive to morphological changes. We decided to use stemming & lemmatization (see specifics in the function `simplify_text` from previous notebook) to overcome those issues. But why do we use TF-IDF in the first place? Well, popularity is not a good reason, we have others. \n\n    * Discussions with our reviewers show that they mostly look for keywords.\n    * Our unpublished results for screening task show that more complicated deep learning methods don't provide a relevant advantage over simple methods (so far). We experimented with our custom datasets & [Cohen's datasets](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC1447545\/).\n    * A screening task can be expressed as a classification task. As shown in the paper [Rethinking Complex Neural Network Architectures for Document Classification (Table 2)](https:\/\/cs.uwaterloo.ca\/~jimmylin\/publications\/Adhikari_etal_NAACL2019.pdf), classical methods are still a good baseline.\n    * Current state of art BERT [doesn't work well in the unsupervised fahsion](https:\/\/arxiv.org\/abs\/1908.10084). Some variations like SBERT were explored, but up to my knowledge, there is no SBERT for the medical domain. You might find online models like [BioBERT](https:\/\/arxiv.org\/abs\/1901.08746) or [SciBERT](https:\/\/arxiv.org\/abs\/1903.10676), but they should suffer similar issues as BERT.\n\n\n2. Initially we wanted to experiment with medical sentence embeddings [called BioSentVec](https:\/\/github.com\/ncbi-nlp\/BioSentVec) used in [LitSense](https:\/\/www.ncbi.nlm.nih.gov\/research\/litsense\/), but Kaggle's limitation is 20GB for uploading the model. We will work with BioWordVec then, which is based on FastText.\n\n\n#### Clustering algorithms\nThere exist many clustering algorithms like [KMeans](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering), [GMM](https:\/\/en.wikipedia.org\/wiki\/Mixture_model#Gaussian_mixture_model), [DBSCAN](https:\/\/en.wikipedia.org\/wiki\/DBSCAN), [OPTICS](https:\/\/en.wikipedia.org\/wiki\/OPTICS_algorithm), [hierarchical clustering algorithms](https:\/\/en.wikipedia.org\/wiki\/Hierarchical_clustering) and many more. A nice comparison can be found [here](https:\/\/nbviewer.jupyter.org\/github\/scikit-learn-contrib\/hdbscan\/blob\/master\/notebooks\/Comparing%20Clustering%20Algorithms.ipynb) and [here](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#clustering-performance-evaluation). \n\n* DBSCAN and OPTICS return non-convex clusters & they don't require to decide on a number of clusters upfront. At the same time, they require different hyperparameters & they are very sensitive to the changes of those hyperparameters (shows my experience).\n* KMeans, GMM return convex clusters & require to decide on a number of clusters upfront. There is no easy way to make such a decision. There exist some heuristics like BIC & AIC criteria for methods that return likelihood (GMM), elbow methods (KMeans), threshold-based (hierarchical clustering algorithms) or more general ones (Silhouette Score).\n\nWe find GMM useful, because it is a probabilistic model meaning that we can optimize likelihood using variational learning. A concept of a likelihood allows us to use cross-validation. The clusters are convex and follow the Gaussian shape. It is found its biggest drawback, but in practice it is not so harmful. The biggest concern using GMM is Mahalanobis distance, which is not well-applicable to TF-IDF features, because TF factor is dependent on the text length. Using cosine distance with such features is much better. We decided to:\n\n1. Use hierarchical clustering algorithms for TF-IDF features with cosine distance. Distance threshold is chosen based on visualizations. The reason we chose such algorithm is that it shows the \"big picture\" using either distance matrix or dendrogram. In my opinion it is the most general view of clustering.\n2. Use GMM with hyperparameter search for BioWordVec features and Mahalonobis distance.\n\n### Remarks\nThe work here is the result of [Evidence Prime](https:\/\/evidenceprime.com\/) team. We work on the project called [LASER](https:\/\/evidenceprime.com\/laser\/), which aims to track scientific literature and synthesize the knowledge about the given topic. We share our experience & knowledge in the notebook, hoping that it will inspire the community to fight COVID-19. \n\n**<span style=\"color:red\">The notebook is based upon our previous work with the purpose to clean the dataset. They should be evaluated toghether!<\/span> ** [Link](https:\/\/www.kaggle.com\/quittend\/cleaning-cord-19-metadata\/notebook)","072d6c02":"Clusters with only one element are useless in our solution (they don't save reviewer's time), therefore we classify them all as noise. Although the number of clusters should be set up during clustering, we couldn't set up a higher distance threshold than `0.999`, because we would end up with one big cluster. We decided to correct for \"stupid clusters\". How many elements should be in one cluster? Well, at least 2 to save time, but we use the same number as a number of neighbors i.e. `5`. You can think of that as regularization (one shared hyperparameters).\n","1d8fd1eb":"## Import packages","2788eea5":"### How many elements do each cluster have?","10fbdc16":"Let's see an example of one cluster e.g. `10`.","cf8b207c":"**Notes:**\n* If you use parallel implementation (n_jobs == 1), then CPU usage is still 400% (a bug?)\n* If you use parallel implementation (n_jobs != 1), then diagonal is not zero (a bug?)"}}