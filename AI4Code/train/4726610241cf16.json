{"cell_type":{"c7fc5499":"code","e5d07a64":"code","c0617bde":"code","870a9ffc":"code","898d7ae8":"code","a8972cd2":"code","55b8e101":"code","1eb05f9f":"code","57c08f38":"code","f9373988":"code","00970726":"code","dcfeda16":"code","5ff197bc":"code","3b2f8e57":"markdown","250b6c2c":"markdown","e888d2c2":"markdown","e0637a60":"markdown","95981a1f":"markdown"},"source":{"c7fc5499":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport seaborn as sns \nfrom sklearn.datasets import load_boston\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","e5d07a64":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)","c0617bde":"train.head()","870a9ffc":"# Train data\nX=train.drop(columns = ['loss','id'])\ny=train['loss'].values\n# Test data\nX_test=test.drop(columns = ['id'])\nprint('Train set:', X.shape)\nprint('Test set:', X_test.shape)","898d7ae8":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_test_scaled = scaler.transform(X_test)","a8972cd2":"# import optuna\n# from catboost import CatBoostRegressor\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import KFold\n\n# def objective(trial, X=X_scaled, y=y):\n#     \"\"\"\n#     A function to train a model using different hyperparamerters combinations provided by Optuna. \n#     Log loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n#     \"\"\"\n#     oof_preds = np.zeros((X.shape[0],))\n#     for fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=10, shuffle=True, random_state=44).split(X_scaled, y)):\n#         X_train, y_train = X_scaled[train_idx], y[train_idx]\n#         X_valid, y_valid = X_scaled[valid_idx], y[valid_idx]\n#         # A set of hyperparameters to optimize by optuna\n#         cb_params = {\n#                  \"learning_rate\": trial.suggest_float('learning_rate', 0.001, 1.0),\n#                  \"l2_leaf_reg\": trial.suggest_float('l2_leaf_reg', 0.00001, 10),\n#                  \"bagging_temperature\": trial.suggest_float('bagging_temperature', 0.0, 10.0),\n#                  \"random_strength\": trial.suggest_float('random_strength', 1.0, 2.0),\n#                  \"depth\": trial.suggest_int('depth', 6, 15),\n#                  \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n#                  \"leaf_estimation_method\": trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),\n#             }\n\n#         model = CatBoostRegressor(random_state=42,\n#                                  thread_count=4,\n#                                  verbose=False,\n#                                  loss_function='RMSE',\n#                                  eval_metric='RMSE',\n#                                  od_type=\"Iter\",\n#                                  early_stopping_rounds=500,\n#                                  use_best_model=True,\n#                                  iterations=10000,\n#                                  task_type=\"GPU\",\n#                                  **cb_params)\n#         model.fit(X_train, y_train,\n#                 eval_set=[(X_valid, y_valid)], verbose=False)\n#         oof_preds[valid_idx] = model.predict(X_valid)\n    \n#     return mean_squared_error(y, oof_preds, squared=False)","55b8e101":"import optuna\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef objective(trial, X=X_scaled, y=y):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Log loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4)\n    # A set of hyperparameters to optimize by optuna\n    params = {\n        \"learning_rate\": trial.suggest_float('learning_rate', 0.001, 1.0),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n    }\n    \n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    model = CatBoostRegressor(random_state=42,\n                             verbose=False,\n                             eval_metric='RMSE',\n#                              use_best_model=True,\n                             task_type=\"GPU\",\n                             **params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)], verbose=False)\n    \n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)","1eb05f9f":"%%time\n# Creating Optuna object and defining its parameters\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials = 100)\n\n# Showing optimization results\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","57c08f38":"# params = {'learning_rate': 0.6966005931411917, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'subsample': 0.293719685175609}\nparams = study.best_trial.params","f9373988":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nn_splits = 10\ntest_preds = 0\nkf_rmse = []\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X_scaled, y)):\n    X_train, y_train = X_scaled[train_idx], y[train_idx]\n    X_valid, y_valid = X_scaled[valid_idx], y[valid_idx]\n    model = CatBoostRegressor(random_state=42,\n                             verbose=False,\n                             eval_metric='RMSE',\n#                              use_best_model=True,\n                             task_type=\"GPU\",\n                             **params)\n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)], verbose=False)\n       \n    valid_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_pred, squared=False)\n    print(f'Fold {fold+1}\/{n_splits} RMSE: {rmse:.4f}')\n    kf_rmse.append(rmse)\n    \n    test_preds += model.predict(X_test_scaled)\n\ntest_preds \/= n_splits\nprint(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')","00970726":"# model_fi = model.feature_importances_\n\n# x = np.arange(0, len(X.columns))\n# height = 0.3\n# fig, ax = plt.subplots(figsize=(10, 15))\n# bars1 = ax.barh(x-height, model_fi, height=height,\n#                 color=\"cornflowerblue\",\n#                 edgecolor=\"black\",\n#                 label='loss')\n# ax.set_title(\"Feature importances\", fontsize=20, pad=5)\n# ax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\n# ax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\n# ax.set_yticks(x)\n# ax.set_yticklabels(X.columns, fontsize=8)\n# ax.tick_params(axis=\"x\", labelsize=10)\n# ax.grid(axis=\"x\")\n# ax.legend(fontsize=13, loc=\"lower right\")\n# plt.margins(0.04, 0.01)\n# plt.gca().invert_yaxis()","dcfeda16":"preds = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\npreds.loss = test_preds\npreds.head()","5ff197bc":"preds.to_csv('submission.csv', index=False)","3b2f8e57":"## 2) Train Model\nvalue: 7.820554598329104","250b6c2c":"## Test Predict","e888d2c2":"## Feature importances","e0637a60":"## Optuna","95981a1f":"## 1) Load Data"}}