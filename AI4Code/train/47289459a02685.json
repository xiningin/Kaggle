{"cell_type":{"2eafb9ef":"code","c0a36eec":"code","c1c98f3b":"code","d49353f9":"code","10e74a97":"code","89d15968":"code","d2a42224":"code","94c39472":"code","9e41e9a0":"code","d0bfe31d":"code","0f4dc79a":"code","4a637d6c":"code","f6ed6044":"markdown","2987349d":"markdown","4bade419":"markdown","84c223c1":"markdown","937bf1aa":"markdown","e6837960":"markdown","04dfcbcb":"markdown","57b0c2e7":"markdown","b25333ef":"markdown","83db86fa":"markdown","aff84479":"markdown","5634d8d2":"markdown"},"source":{"2eafb9ef":"import pandas as pd\nfrom sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\n\n# Import dataset\nfile_path = '\/kaggle\/input\/company-bankruptcy-prediction\/data.csv'\ndf = pd.read_csv(file_path)\ndf.head(10)","c0a36eec":"# Check datatypes of dataframe columns\ndf.dtypes.value_counts()","c1c98f3b":"print('Total Null Values in Dataset: ',df.isna().sum().sum())","d49353f9":"# Investigate count of classes\ndf['Bankrupt?'].value_counts()","10e74a97":"def ModelPerformanceMetrics(Y_test,Y_pred):\n    cf = confusion_matrix(Y_test,Y_pred)\n    precision = precision_score(Y_test,Y_pred)\n    recall = recall_score(Y_test,Y_pred)\n    accuracy = accuracy_score(Y_test,Y_pred)\n    fscore = f1_score(Y_test,Y_pred)\n    print(cf)\n    print('Precision: {} \\nRecall: {} \\nAccuracy: {} \\nFScore: {}' \\\n          .format(round(precision,2),round(recall,2),round(accuracy,2),round(fscore,2)))","89d15968":"# Build a logistic regression model\nX = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","d2a42224":"training_set = pd.concat([X_train,Y_train], axis = 1)\ndf_bankrupt = training_set.loc[df['Bankrupt?'] == 1]\ndf_solvent = training_set.loc[df['Bankrupt?'] == 0]\nmultiplier = len(df_solvent)\/\/len(df_bankrupt)\ndf_bankrupt_boosted = pd.concat([df_bankrupt]*multiplier, ignore_index = True)\ndf_oversampled = pd.concat([df_bankrupt_boosted,df_solvent], ignore_index = True)\n\nX_train = df_oversampled.drop(['Bankrupt?'], axis = 1)\nY_train = df_oversampled['Bankrupt?']\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","94c39472":"df_bankrupt = df.loc[df['Bankrupt?'] == 1]\ndf_solvent = df.loc[df['Bankrupt?'] == 0].iloc[0:220,:]\ndf_undersampled = pd.concat([df_bankrupt,df_solvent], ignore_index = True)\n\nX = df_undersampled.drop(['Bankrupt?'], axis = 1)\nY = df_undersampled['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","9e41e9a0":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nClassifier = RandomForestClassifier(n_estimators = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","d0bfe31d":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\nClassifier = XGBClassifier(use_label_encoder=False)\nClassifier = Classifier.fit(X_train, Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","0f4dc79a":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nsm = SMOTE(sampling_strategy = 'auto', k_neighbors = 5, random_state = 0)\nX_smote, Y_smote = sm.fit_resample(X, Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, test_size = 0.2, random_state = 0)\n\nClassifier = LogisticRegression(max_iter = 1000)\nClassifier = Classifier.fit(X_train,Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","4a637d6c":"X = df.drop(['Bankrupt?'], axis = 1)\nY = df['Bankrupt?']\nsm = SMOTE(sampling_strategy = 'auto', k_neighbors = 5, random_state = 0)\nX_smote, Y_smote = sm.fit_resample(X, Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X_smote, Y_smote, test_size = 0.2, random_state = 0)\n\nClassifier = XGBClassifier(use_label_encoder=False)\nClassifier = Classifier.fit(X_train, Y_train)\nY_pred = Classifier.predict(X_test)\n\nModelPerformanceMetrics(Y_test,Y_pred)","f6ed6044":"SMOTE and Logistic Regression performed better than the undersampling technique, which was the best option so far. I'll combine SMOTE and XGBoost to see if that helps:","2987349d":"It's clear that Logistic Regression does not perform well on this dataset.","4bade419":"Classes are not even. This is something I'll look to handle as I decide on algorithms and sampling techniques to use.\n\nI'll define a function to output the metrics I want to see when comparing models:","84c223c1":"# Building a Model to Predict Company Bankruptcy\nUsing the bankruptcy data from the Taiwan Economic Journal for the years 1999\u20132009, I'll create a model to predict whether or not a company will go bankrupt.\n\n### Outline:\n1. Import libraries and data\n2. Check data for categorical data and null values\n3. Check if classes are even (ie. how many records do we have for each class?)\n4. Build and compare algorithms with and without the SMOTE sampling technique","937bf1aa":"Since there are no null values in the dataset, I can move on to check if the two classes (bankrupt, and not bankrupt) are even:","e6837960":"XGBoost didn't perform very well here. I'll try the SMOTE sampling technique and Logistic Regression next:","04dfcbcb":"That was better than logistic regression, but still not a useful model. I'll try undersampling, by removing a large number of records from the more prevalent class:","57b0c2e7":"This also illustrates why it is necessary to consider a variety of metrics when evaluating a model. Next, I'll try oversampling by increasing the less prevalent class (bankrupt companies) to be less than or equal to the more prevalent class (solvent companies):","b25333ef":"Random Forest isn't useful either.\n\nI'll try the XGBoost algorithm:","83db86fa":"The SMOTE technique used with the XGBoost algorithm performed exceptionally well, with precision of 98%, recall close to 100%, and an F-Score of 99%.","aff84479":"All features are numeric. Since there are 96 columns, I'll do a quick check to see if there are any null values in the entire dataset and then drill down by column if needed.","5634d8d2":"Undersampling improved performance, but still not enough to be useful. I'll see if random forest can handle the imbalanced classes:"}}