{"cell_type":{"9406e93d":"code","a0b314d3":"code","347a4d7b":"code","64480a9a":"code","4ec0511d":"code","f9030136":"code","fd27de18":"code","ed6265bb":"code","c2b1b3fd":"code","3c522261":"code","9c6ff835":"code","b8d5991f":"code","c8992d6c":"code","1657cd73":"code","b5877d71":"code","7e7a5bee":"code","942e7054":"code","e19400bb":"markdown","55fb45e3":"markdown","405d2ebc":"markdown","5496d9c7":"markdown","009b70eb":"markdown","086ed83b":"markdown","339e7ece":"markdown"},"source":{"9406e93d":"import math, os, re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nfrom keras.preprocessing.image import load_img \nfrom keras.preprocessing.image import img_to_array \nfrom keras.applications.vgg16 import preprocess_input \nfrom keras.applications.vgg16 import VGG16 \nfrom keras.models import Model\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\n\nprint(os.listdir('..\/input\/landmark-recognition-2021'))","a0b314d3":"path = '\/kaggle\/input\/landmark-recognition-2021'\nos.listdir(path)\ntrain_images = f'{path}\/train'\ntrain_df = pd.read_csv(f'{path}\/train.csv')\ntrain_df['path'] = train_df['id'].apply(lambda f: os.path.join('..\/input\/landmark-recognition-2021\/train',f[0], f[1], f[2], f + '.jpg'))\ntest_images = f'{path}\/test'\ntest_df = pd.read_csv(f'{path}\/sample_submission.csv')\ntest_df['path'] = test_df['id'].apply(lambda f: os.path.join('..\/input\/landmark-recognition-2021\/test',f[0], f[1], f[2], f + '.jpg'))\n\nnum_classes = train_df['landmark_id'].nunique()\nprint('number of target classes:', num_classes)\nprint('number of images in training set:', len(train_df))","347a4d7b":"counts = train_df['landmark_id'].value_counts()\ncounts.describe()","64480a9a":"# let's take the landmark ID with 5th highest image count\nsuperclass = train_df[train_df['landmark_id'] == counts.iloc[[5]].index[0]]\nimages = superclass.path.to_list()\nlen(images)","4ec0511d":"sample = superclass.sample(n=12, replace=False)\n\nplt.subplots(3, 4, figsize=(160, 160))\nfor i in range(len(sample)):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('Off')\n    image = img.imread(sample.iloc[i][2])\n    plt.imshow(image)\n    plt.title(f'landmark id:{sample.iloc[i][1]} ', fontsize=0)","f9030136":"model = VGG16()\nmodel = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n\ndef extract_features(file, model):\n    # VGG expects 224x224 arrays\n    img = load_img(file, target_size=(224,224))\n    img = np.array(img) \n    # reshape the data for the model reshape(batch size, height pixel, channels)\n    reshaped_img = img.reshape(1,224,224,3) \n    # prepare image for model\n    imgx = preprocess_input(reshaped_img)\n    # get the feature vector\n    features = model.predict(imgx, use_multiprocessing=True)\n    return features","fd27de18":"extract_features(images[0], model)","ed6265bb":"%%time\nfeatures = {}\n\n# loop through each image in the dataset\nfor image in images:\n    # extract features and update dictionary (filepath=key)\n    feature = extract_features(image, model)\n    features[image] = feature\n       \n# get a list of the filenames\nfilenames = np.array(list(features.keys()))\n\n# get a list of just the features\nfeat = np.array(list(features.values()))\nfeat.shape","c2b1b3fd":"# reshape to sample size count of 4096 vectors\nfeat = feat.reshape(-1,4096)\nfeat.shape","3c522261":"# PCA to select most important of 4,096 dimensions\nif len(images) >= 4096:\n    pca = PCA(n_components=len(images)\/\/100, random_state=888)\nelse:\n    pca = PCA(n_components=len(images)\/\/60, random_state=888)\n    \npca.fit(feat)\nx = pca.transform(feat)","9c6ff835":"# k-means clustering\n# ID good number of clusters using SSE distance to cluster center 'elbow method'\nnp.random.seed(888)\ns = np.zeros(50)\n\nfor k in range(0, 50):\n    est = KMeans(n_clusters=k+2)\n    est.fit(x)\n    s[k] = est.inertia_\n    \n\nplt.plot(range(0,50), s)\nplt.xlabel('cluster counts')\nplt.ylabel('distortion')\nplt.title('');","b8d5991f":"kmeans = KMeans(n_clusters=15, random_state=888).fit(x)","c8992d6c":"groups = {}\nfor file, cluster in zip(images,kmeans.labels_):\n    if cluster not in groups.keys():\n        groups[cluster] = []\n        groups[cluster].append(file)\n    else:\n        groups[cluster].append(file)","1657cd73":"def view_images_in_cluster(cluster):\n    plt.figure(figsize = (25,25));\n    files = groups[cluster]\n    # view <=50 images in cluster\n    if len(files) > 50:\n        print(f'showing 50 of {len(files)} images in cluster')\n        files = files[:49]\n    for index, file in enumerate(files):\n        plt.subplot(10,10,index+1);\n        img = load_img(file)\n        img = np.array(img)\n        plt.imshow(img)\n        plt.axis('off')","b5877d71":"view_images_in_cluster(2)","7e7a5bee":"view_images_in_cluster(4)","942e7054":"view_images_in_cluster(12)","e19400bb":"With such a high number of unique landmark IDs given the size of the training set, there is likely going to be imbalanced classsification issues. Viewing the distribution of class counts confirms imbalance.","55fb45e3":"Landmarks with a lot of images in the training set have a lot of different representations contained therein. Clustering can group images based on similar feature vectors.\nSteps:\n1. preprocess images into arrays with batch dimension \n2. extract features vector using VGG model prediction\n3. PCA to select most important components\n4. K-means clustering to group images (n clusters chosen with SSE elbow method)\n5. viewing images in clusters to compare","405d2ebc":"# Exploring Superclass Content with Clustering\n[Google Landmark Recognition 2021\n](https:\/\/www.kaggle.com\/c\/landmark-recognition-2021)\n\nThis notebook explores the target landmark ID class imbalance in this challenge and uses k-means clustering on images within a target superclass (i.e., target class with superhigh number of images in the training set) to determine how superclasses are organized.","5496d9c7":"### Image preprocessing and feature extraction","009b70eb":"Looking at a sample of images from a superclass shows a lot of diversity. The same landmark ID covers different buildings and landscapes.","086ed83b":"### PCA to identify important components\nMax n_components must be less than or equal to the number of images in the superclass as the square covariance matrix is sized according to the number of images","339e7ece":"There is no clear elbow in the cluster count plot, so I will choose 15 clusters to balance complexity in the model and the SSE cost function\n"}}