{"cell_type":{"e19ba5d0":"code","91eaafb2":"code","3f940b59":"code","6d0bfd59":"code","b2d7c1c0":"code","cd75e070":"code","16ad31bc":"code","450150a2":"code","a03e6569":"code","5f28adc1":"code","7cf1e33b":"code","041ff5b6":"code","828a36d3":"code","0dbe532c":"code","ae7f91e5":"code","09d47497":"code","7f651cce":"code","0fb3ed1f":"code","a12e2182":"code","5c52d2ab":"code","a14ec060":"code","619bd051":"code","f5614f18":"code","94925ea3":"code","b44a71dc":"code","38e134af":"code","b3d0aa00":"code","b57cc307":"code","6fbbb40d":"code","a9f2d12a":"code","58e0d8aa":"code","04cee879":"code","de2f0031":"code","0a856716":"code","a62635d8":"code","7f908d2e":"code","94d880c5":"markdown","55b28f0b":"markdown","bd318c19":"markdown","e2b3d873":"markdown","6fa9470c":"markdown","8e71d030":"markdown","a5b1ef0d":"markdown","30e7b834":"markdown","e2774b53":"markdown","6f0e4b2b":"markdown","b77cd103":"markdown","0d2f6d5e":"markdown","fb412c69":"markdown","25df43d8":"markdown","599f1695":"markdown","56d0785c":"markdown","fe23f393":"markdown","c5ebb2e3":"markdown","7a773a88":"markdown","85486988":"markdown","e13b1afd":"markdown"},"source":{"e19ba5d0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nsns.set_style('darkgrid')\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nplt.rc('figure',figsize=(18,9))\n%pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE","91eaafb2":"c_data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\nc_data = c_data[c_data.columns[:-2]]\nc_data.head()","3f940b59":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Customer_Age'],name='Age Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Customer_Age'],name='Age Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Customer Ages\")\nfig.show()","6d0bfd59":"ex.pie(c_data,names='Gender',title='Propotion Of Customer Genders')","b2d7c1c0":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Dependent_count'],name='Dependent count Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Dependent_count'],name='Dependent count Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Dependent counts (close family size)\")\nfig.show()","cd75e070":"ex.pie(c_data,names='Education_Level',title='Propotion Of Education Levels')","16ad31bc":"ex.pie(c_data,names='Marital_Status',title='Propotion Of Different Marriage Statuses')\n","450150a2":"ex.pie(c_data,names='Income_Category',title='Propotion Of Different Income Levels')","a03e6569":"ex.pie(c_data,names='Card_Category',title='Propotion Of Different Card Categories')","5f28adc1":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Months_on_book'],name='Months on book Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Months_on_book'],name='Months on book Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of months the customer is part of the bank\")\nfig.show()","7cf1e33b":"print('Kurtosis of Months on book features is : {}'.format(c_data['Months_on_book'].kurt()))","041ff5b6":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Total_Relationship_Count'],name='Total no. of products Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Total_Relationship_Count'],name='Total no. of products Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Total no. of products held by the customer\")\nfig.show()","828a36d3":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Months_Inactive_12_mon'],name='number of months inactive Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Months_Inactive_12_mon'],name='number of months inactive Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the number of months inactive in the last 12 months\")\nfig.show()","0dbe532c":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Credit_Limit'],name='Credit_Limit Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Credit_Limit'],name='Credit_Limit Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the Credit Limit\")\nfig.show()","ae7f91e5":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Total_Trans_Amt'],name='Total_Trans_Amt Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Total_Trans_Amt'],name='Total_Trans_Amt Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the Total Transaction Amount (Last 12 months)\")\nfig.show()","09d47497":"ex.pie(c_data,names='Attrition_Flag',title='Proportion of churn vs not churn customers')","7f651cce":"c_data.Attrition_Flag = c_data.Attrition_Flag.replace({'Attrited Customer':1,'Existing Customer':0})\nc_data.Gender = c_data.Gender.replace({'F':1,'M':0})\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Education_Level']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Income_Category']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Marital_Status']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Card_Category']).drop(columns=['Platinum'])],axis=1)\nc_data.drop(columns = ['Education_Level','Income_Category','Marital_Status','Card_Category','CLIENTNUM'],inplace=True)","0fb3ed1f":"sns.heatmap(c_data.corr('pearson'),annot=True)","a12e2182":"oversample = SMOTE()\nX, y = oversample.fit_resample(c_data[c_data.columns[1:]], c_data[c_data.columns[0]])\nusampled_df = X.assign(Churn = y)","5c52d2ab":"ohe_data = usampled_df[usampled_df.columns[15:-1]].copy()\n\nusampled_df = usampled_df.drop(columns=usampled_df.columns[15:-1])","a14ec060":"sns.heatmap(usampled_df.corr('pearson'),annot=True)\n","619bd051":"\nN_COMPONENTS = 10\n\npca_model = PCA(n_components = N_COMPONENTS )\n\npc_matrix = pca_model.fit_transform(ohe_data)\n\nevr = pca_model.explained_variance_ratio_\ncumsum_evr = np.cumsum(evr)\n\nax = sns.lineplot(x=np.arange(0,len(cumsum_evr)),y=cumsum_evr,label='Explained Variance Ratio')\nax.set_title('Explained Variance Ratio Using {} Components'.format(N_COMPONENTS))\nax = sns.lineplot(x=np.arange(0,len(cumsum_evr)),y=evr,label='Explained Variance Of Component X')\nax.set_xticks([i for i in range(0,len(cumsum_evr))])\nax.set_xlabel('Component number #')\nax.set_ylabel('Explained Variance')\nplt.show()","f5614f18":"usampled_df_with_pcs = pd.concat([usampled_df,pd.DataFrame(pc_matrix,columns=['PC-{}'.format(i) for i in range(0,N_COMPONENTS)])],axis=1)\nusampled_df_with_pcs","94925ea3":"sns.heatmap(usampled_df_with_pcs.corr('pearson'),annot=True)\n","b44a71dc":"X_features = ['Total_Trans_Ct','PC-3', 'PC-4' ,'PC-1','PC-0','PC-2','Total_Ct_Chng_Q4_Q1','Total_Relationship_Count']\n\nX = usampled_df_with_pcs[X_features]\ny = usampled_df_with_pcs['Churn']","38e134af":"train_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)","b3d0aa00":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.tree import DecisionTreeClassifier \n'''''from xgboost import XGBClassifier''\nfrom lightgbm import LGBMClassifier from catboost import CatBoostClassifier '''\nfrom sklearn.ensemble import GradientBoostingClassifier\n","b57cc307":"rf_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",RandomForestClassifier(random_state=42)) ])\nada_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",AdaBoostClassifier(random_state=42,learning_rate=0.7)) ])\nsvm_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",SVC(random_state=42,kernel='rbf')) ])\n\ngrd_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",GradientBoostingClassifier(random_state=42)) ])\nknn_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",KNeighborsClassifier()) ])\ndtr_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",DecisionTreeClassifier(random_state=42)) ])\ngnb_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",GaussianNB()) ])\n\n\nrf_f1_cross_val_scores = cross_val_score(rf_pipe,train_x,train_y,cv=5,scoring='f1')\nada_f1_cross_val_scores=cross_val_score(ada_pipe,train_x,train_y,cv=5,scoring='f1')\nsvm_f1_cross_val_scores=cross_val_score(svm_pipe,train_x,train_y,cv=5,scoring='f1')\n\ngrd_f1_cross_val_scores = cross_val_score(grd_pipe,train_x,train_y,cv=5,scoring='f1')\nknn_f1_cross_val_scores = cross_val_score(knn_pipe,train_x,train_y,cv=5,scoring='f1')\ndtr_cross_val_scores = cross_val_score(dtr_pipe,train_x,train_y,cv=5,scoring='f1')\ngnb_cross_val_scores = cross_val_score(gnb_pipe,train_x,train_y,cv=5,scoring='f1')\n","6fbbb40d":"classifiers_score = [\n    ('Random Forest', rf_f1_cross_val_scores),\n    ('Adaboost', ada_f1_cross_val_scores),\n    ('SVM', svm_f1_cross_val_scores),\n    ('Gradient Boosting', grd_f1_cross_val_scores),\n    ('KNeighbors', knn_f1_cross_val_scores),\n    ('DecisionTree', dtr_cross_val_scores),\n    ('GaussianNB', gnb_cross_val_scores)\n]\n","a9f2d12a":"\nfor i in range(len(classifiers_score)):\n    plt.subplot(len(classifiers_score),1,i+1)\n    ax = sns.lineplot(x=range(0,len(classifiers_score[i][1])),y=classifiers_score[i][1])\n    ax.set_title(classifiers_score[i][0])\n    ax.set_xticks([i for i in range(0,len(classifiers_score[i][1]))])\n    ax.set_xlabel('Fold Number')\n    ax.set_ylabel('F1 Score')\n    plt.show()\n    ","58e0d8aa":"rf_pipe.fit(train_x,train_y)\nrf_prediction = rf_pipe.predict(test_x)\n\nada_pipe.fit(train_x,train_y)\nada_prediction = ada_pipe.predict(test_x)\n\nsvm_pipe.fit(train_x,train_y)\nsvm_prediction = svm_pipe.predict(test_x)\n\nprint('F1 Score of Random Forest Model On Test Set - {}'.format(f1(rf_prediction,test_y)))\nprint('F1 Score of AdaBoost Model On Test Set - {}'.format(f1(ada_prediction,test_y)))\nprint('F1 Score of SVM Model On Test Set - {}'.format(f1(svm_prediction,test_y)))","04cee879":"ohe_data =c_data[c_data.columns[16:]].copy()\npc_matrix = pca_model.fit_transform(ohe_data)\noriginal_df_with_pcs = pd.concat([c_data,pd.DataFrame(pc_matrix,columns=['PC-{}'.format(i) for i in range(0,N_COMPONENTS)])],axis=1)\n\nunsampled_data_prediction_RF = rf_pipe.predict(original_df_with_pcs[X_features])\nunsampled_data_prediction_ADA = ada_pipe.predict(original_df_with_pcs[X_features])\nunsampled_data_prediction_SVM = svm_pipe.predict(original_df_with_pcs[X_features])","de2f0031":"print('F1 Score of Random Forest Model On Original Data (Before Upsampling) - {}'.format(f1(unsampled_data_prediction_RF,original_df_with_pcs['Attrition_Flag'])))\nprint('F1 Score of AdaBoost Model On Original Data (Before Upsampling) - {}'.format(f1(unsampled_data_prediction_ADA,original_df_with_pcs['Attrition_Flag'])))\nprint('F1 Score of SVM Model On Original Data (Before Upsampling) - {}'.format(f1(unsampled_data_prediction_SVM,original_df_with_pcs['Attrition_Flag'])))","0a856716":"ax = sns.heatmap(confusion_matrix(unsampled_data_prediction_RF,original_df_with_pcs['Attrition_Flag']),annot=True,cmap='coolwarm',fmt='d')\nax.set_title('Prediction On Original Data With Random Forest Model Confusion Matrix')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=18)\nax.set_yticklabels(['Predicted Not Churn','Predicted Churn'],fontsize=18)\n\nplt.show()","a62635d8":"unsampled_data_prediction_RF = rf_pipe.predict_proba(original_df_with_pcs[X_features])\nskplt.metrics.plot_precision_recall(original_df_with_pcs['Attrition_Flag'], unsampled_data_prediction_RF)","7f908d2e":"\n#model building with multiple models\nmodels = []\nmodels.append(('Naive Bayes', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('Decision Tree', DecisionTreeClassifier(random_state = 1)))\nmodels.append(('Random Forest', RandomForestClassifier(random_state = 1)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 1)))\n\n\n# train_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n        model.fit(train_x,train_y)\n        y_pred = model.predict(test_x)\n        f1_ = f1(test_y, y_pred)\n        print(\"{} : {}\".format(name,f1_))\n  ","94d880c5":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Almost half of the customers of the bank are married and interestingly enough almost the entire other half are customers which are single.only about 7% of the customers are divorced which is surprising considering the worldwide divorce rate statistics! (let me know where the bank is located and sign me up!)<\/span><\/p>","55b28f0b":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Cross Validation<\/h3>\n","bd318c19":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Libraries And Utilities<\/h3>","e2b3d873":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that the distribution of customer ages in our dataset follows a fairly normal distribution, thus further use of the age feature can be done with the normality assumption.<\/span><\/p>","6fa9470c":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Model Selection<\/h3>\n","8e71d030":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Data Upsampling Using SMOTE<\/h3>\n","a5b1ef0d":"<a id=\"1.1\"><\/a>\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Principal Component Analysis Of One Hot Encoded Data <\/h1>\n","30e7b834":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The distribution of Dependent counts is fairly normally distributed with a slight right skew.<\/span><\/p>","e2774b53":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Data Loading<\/h3>\n","6f0e4b2b":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>We have a low kurtosis value pointing to a very flat shaped distribution (as can be seen in the plots above as well) meaning we cannot assume normality of the feature.<\/span><\/p>","b77cd103":"<a id=\"1.1\"><\/a>\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Exploratory Data Analysis<\/h1>\n","0d2f6d5e":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The distribution of the total number of products held by the customer seems to be closer to a uniform distribution and may appear useless as a predictor for churn status.<\/span><\/p>","fb412c69":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Here we one hot encode all the categorical features describing different statuses of a customer.<\/span><\/p>","25df43d8":"<a id=\"1.1\"><\/a>\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Model Evaluation<\/h1>\n","599f1695":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>There are more samples of females in our dataset compared to males but the percentage of difference is not that significant so we can say that genders are uniformly distributed.<\/span><\/p>","56d0785c":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>If assuming that most of the customers with unknown education status lack any sort of education we can state that more than 70% of the customers have a formal education level of which about 35% have a higher level of education.<\/span><\/p>","fe23f393":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Data Preprocessing<\/h3>\n","c5ebb2e3":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>As we can see only 16% of the data samples represent churn customers, in the following steps I will use SMOTE to upsample the churn samples to match them with the regular customer sample size in order to give the later selected models a better chance of catching on small details which will almost definitely be missed out with such a size difference.<\/span><\/p>","7a773a88":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We will use principal component analysis to reduce the dimensionality of the one-hot encoded categorical variables losing some of the variances but at the same time using a couple of principal components instead of tens of one-hot encoded features will help me construct a better model.<\/span><\/p>","85486988":"<a id=\"1.1\"><\/a>\n<h1 style=\"background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center\">Model Evaluation On Original Data (Before Upsampling)<\/h1>\n","e13b1afd":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We see that the distribution of the total transactions (Last 12 months) displays a multimodal distribution, meaning we have some underlying groups in our data, it can be an interesting experiment to try and cluster the different groups and view the similarities between them and what describes best the different groups which create the different modes in our distribution.<\/span><\/p>"}}