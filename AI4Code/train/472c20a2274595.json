{"cell_type":{"3a7f8616":"code","68e8fc34":"code","fcb70298":"code","98407d4b":"code","fe3e7e84":"code","d598835c":"code","df5b363c":"code","796b564a":"code","5d11c5ff":"code","bf512a8f":"code","90d07773":"code","13282aaa":"code","833e342e":"code","65465b43":"code","50311721":"code","b5fc66bb":"code","1e1de7c6":"code","2ceddba1":"code","a6045bec":"code","e10189b6":"code","3ffffea0":"code","83bec390":"code","71c90640":"code","1338e6d2":"code","d94a5327":"code","8497fa25":"code","4d78446a":"code","a70f9863":"code","7cdb205e":"code","7708ddc5":"code","7a7a42ad":"code","703b389e":"code","1e4a76f7":"code","0899ab1f":"code","810ebb65":"code","9f46c37c":"code","6fa83f26":"code","4e324c66":"markdown","90d0191d":"markdown","9b7b93af":"markdown","b34471df":"markdown","ae9e6752":"markdown","eaca5d89":"markdown","3100fe02":"markdown","5fbe946c":"markdown","77a5cb6b":"markdown","9c1bab63":"markdown","ac56a6be":"markdown"},"source":{"3a7f8616":"# import data manipulation library\nimport numpy as np\nimport pandas as pd\n\n# import data visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import pystan model class\nimport pystan\n\n# import sklearn data preprocessing\nfrom sklearn.preprocessing import RobustScaler\n\n# import sklearn model class\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# import sklearn model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\n# import sklearn model evaluation classification metrics\nfrom sklearn.metrics import accuracy_score, auc, classification_report, confusion_matrix, f1_score, fbeta_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve","68e8fc34":"# acquiring training and testing data\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","fcb70298":"# visualize head of the training data\ndf_train.head(n=5)","98407d4b":"# visualize tail of the testing data\ndf_test.tail(n=5)","fe3e7e84":"# combine training and testing dataframe\ndf_train['datatype'], df_test['datatype'] = 'training', 'testing'\ndf_test.insert(1, 'target', np.nan)\ndf_data = pd.concat([df_train, df_test], ignore_index=True)","d598835c":"def swarmplot(categorical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a swarm plot applied for categorical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        categorical_x (list or str): The categorical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    categorical_x, numerical_y = [categorical_x] if type(categorical_x) == str else categorical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(categorical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.swarmplot(x=vj, y=vi, data=data, ax=axes[i*len(categorical_x) + j], rasterized=True) for i, vi in enumerate(numerical_y) for j, vj in enumerate(categorical_x)]\n    return fig","df5b363c":"# describe training and testing data\ndf_data.describe(include='all')","796b564a":"# convert dtypes numeric to object\ncol_convert = ['target']\ndf_data[col_convert] = df_data[col_convert].astype('object')","5d11c5ff":"# list all features type number\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\nprint('features type number:\\n items %s\\n length %d' %(col_number, len(col_number)))\n\n# list all features type object\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\nprint('features type object:\\n items %s\\n length %d' %(col_object, len(col_object)))","bf512a8f":"# feature exploration: histogram of all numeric features\n_ = df_data.hist(bins=20, figsize=(200, 150))","90d07773":"# feature exploration: target\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['id']).tolist()\n_ = swarmplot('target', col_number, df_data)","13282aaa":"# feature extraction: target\ndf_data['target'] = df_data['target'].fillna(-1)","833e342e":"# convert category codes for data dataframe\ndf_data = pd.get_dummies(df_data, columns=['datatype'], drop_first=True)","65465b43":"# convert dtypes object to numeric for data dataframe\ncol_convert = ['target']\ndf_data[col_convert] = df_data[col_convert].astype(int)","50311721":"# describe data dataframe\ndf_data.describe(include='all')","b5fc66bb":"# verify dtypes object\ndf_data.info()","1e1de7c6":"# select all features to evaluate the feature importances\nx = df_data[df_data['datatype_training'] == 1].drop(['id', 'target', 'datatype_training'], axis=1)\ny = df_data.loc[df_data['datatype_training'] == 1, 'target']","2ceddba1":"# set up lasso regression to find the feature importances\nlassoreg = Lasso(alpha=1e-5).fit(x, y)\nfeat = pd.DataFrame(data=lassoreg.coef_, index=x.columns, columns=['feature_importances']).sort_values(['feature_importances'], ascending=False)","a6045bec":"# plot the feature importances\nfeat[(feat['feature_importances'] < -1e-3) | (feat['feature_importances'] > 1e-3)].dropna().plot(y='feature_importances', figsize=(20, 5), kind='bar')\nplt.axhline(-0.05, color=\"grey\")\nplt.axhline(0.05, color=\"grey\")","e10189b6":"# list feature importances\nmodel_feat = feat[(feat['feature_importances'] < -0.05) | (feat['feature_importances'] > 0.05)].index","3ffffea0":"# select the important features\nx = df_data.loc[df_data['datatype_training'] == 1, model_feat]\ny = df_data.loc[df_data['datatype_training'] == 1, 'target']","83bec390":"# create scaler to the features\nscaler = RobustScaler()\nx = scaler.fit_transform(x)","71c90640":"# perform train-test (validate) split\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=0.25, random_state=58)","1338e6d2":"# linear regression model setup\nmodel_linreg = LinearRegression()\n\n# linear regression model fit\nmodel_linreg.fit(x_train, y_train)\n\n# linear regression model prediction\nmodel_linreg_ypredict = model_linreg.predict(x_validate)\n\n# linear regression model metrics\nmodel_linreg_rocaucscore = roc_auc_score(y_validate, model_linreg_ypredict)\nmodel_linreg_cvscores = cross_val_score(model_linreg, x, y, cv=20, scoring='roc_auc')\nprint('linear regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_linreg_rocaucscore, model_linreg_cvscores.mean(), 2 * model_linreg_cvscores.std()))","d94a5327":"# lasso regression model setup\nmodel_lassoreg = Lasso(alpha=0.01)\n\n# lasso regression model fit\nmodel_lassoreg.fit(x_train, y_train)\n\n# lasso regression model prediction\nmodel_lassoreg_ypredict = model_lassoreg.predict(x_validate)\n\n# lasso regression model metrics\nmodel_lassoreg_rocaucscore = roc_auc_score(y_validate, model_lassoreg_ypredict)\nmodel_lassoreg_cvscores = cross_val_score(model_lassoreg, x, y, cv=20, scoring='roc_auc')\nprint('lasso regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_lassoreg_rocaucscore, model_lassoreg_cvscores.mean(), 2 * model_lassoreg_cvscores.std()))","8497fa25":"# specify the hyperparameter space\nparams = {\n    'alpha': np.logspace(-4, -2, base=10, num=50),\n}\n\n# lasso regression grid search model setup\nmodel_lassoreg_cv = GridSearchCV(model_lassoreg, params, iid=False, cv=5)\n\n# lasso regression grid search model fit\nmodel_lassoreg_cv.fit(x_train, y_train)\n\n# lasso regression grid search model prediction\nmodel_lassoreg_cv_ypredict = model_lassoreg_cv.predict(x_validate)\n\n# lasso regression grid search model metrics\nmodel_lassoreg_cv_rocaucscore = roc_auc_score(y_validate, model_lassoreg_cv_ypredict)\nmodel_lassoreg_cv_cvscores = cross_val_score(model_lassoreg_cv, x, y, cv=20, scoring='roc_auc')\nprint('lasso regression grid search\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_lassoreg_cv_rocaucscore, model_lassoreg_cv_cvscores.mean(), 2 * model_lassoreg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_lassoreg_cv.best_params_)","4d78446a":"# ridge regression model setup\nmodel_ridgereg = Ridge(alpha=35)\n\n# ridge regression model fit\nmodel_ridgereg.fit(x_train, y_train)\n\n# ridge regression model prediction\nmodel_ridgereg_ypredict = model_ridgereg.predict(x_validate)\n\n# ridge regression model metrics\nmodel_ridgereg_rocaucscore = roc_auc_score(y_validate, model_ridgereg_ypredict)\nmodel_ridgereg_cvscores = cross_val_score(model_ridgereg, x, y, cv=20, scoring='roc_auc')\nprint('ridge regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_ridgereg_rocaucscore, model_ridgereg_cvscores.mean(), 2 * model_ridgereg_cvscores.std()))","a70f9863":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, 4, base=10, num=50)}\n\n# ridge regression grid search model setup\nmodel_ridgereg_cv = GridSearchCV(model_ridgereg, params, iid=False, cv=5)\n\n# ridge regression grid search model fit\nmodel_ridgereg_cv.fit(x_train, y_train)\n\n# ridge regression grid search model prediction\nmodel_ridgereg_cv_ypredict = model_ridgereg_cv.predict(x_validate)\n\n# ridge regression grid search model metrics\nmodel_ridgereg_cv_rocaucscore = roc_auc_score(y_validate, model_ridgereg_cv_ypredict)\nmodel_ridgereg_cv_cvscores = cross_val_score(model_ridgereg_cv, x, y, cv=20, scoring='roc_auc')\nprint('ridge regression grid search\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_ridgereg_cv_rocaucscore, model_ridgereg_cv_cvscores.mean(), 2 * model_ridgereg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_ridgereg_cv.best_params_)","7cdb205e":"# elastic net regression model setup\nmodel_elasticnetreg = ElasticNet(alpha=0.01, l1_ratio=0.9)\n\n# elastic net regression model fit\nmodel_elasticnetreg.fit(x_train, y_train)\n\n# elastic net regression model prediction\nmodel_elasticnetreg_ypredict = model_elasticnetreg.predict(x_validate)\n\n# elastic net regression model metrics\nmodel_elasticnetreg_rocaucscore = roc_auc_score(y_validate, model_elasticnetreg_ypredict)\nmodel_elasticnetreg_cvscores = cross_val_score(model_elasticnetreg, x, y, cv=20, scoring='roc_auc')\nprint('elastic net regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_elasticnetreg_rocaucscore, model_elasticnetreg_cvscores.mean(), 2 * model_elasticnetreg_cvscores.std()))","7708ddc5":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, -2, base=10, num=10),\n          'l1_ratio': np.linspace(0.1, 0.9, num=5),\n}\n\n# elastic net regression grid search model setup\nmodel_elasticnetreg_cv = GridSearchCV(model_elasticnetreg, params, iid=False, cv=5)\n\n# elastic net regression grid search model fit\nmodel_elasticnetreg_cv.fit(x_train, y_train)\n\n# elastic net regression grid search model prediction\nmodel_elasticnetreg_cv_ypredict = model_elasticnetreg_cv.predict(x_validate)\n\n# elastic net regression grid search model metrics\nmodel_elasticnetreg_cv_rocaucscore = roc_auc_score(y_validate, model_elasticnetreg_cv_ypredict)\nmodel_elasticnetreg_cv_cvscores = cross_val_score(model_elasticnetreg_cv, x, y, cv=20, scoring='roc_auc')\nprint('elastic net regression grid search\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_elasticnetreg_cv_rocaucscore, model_elasticnetreg_cv_cvscores.mean(), 2 * model_elasticnetreg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_elasticnetreg_cv.best_params_)","7a7a42ad":"# kernel ridge regression model setup\nmodel_kernelridgereg = KernelRidge(alpha=0.0001, kernel='polynomial', degree=4)\n\n# kernel ridge regression model fit\nmodel_kernelridgereg.fit(x_train, y_train)\n\n# kernel ridge regression model prediction\nmodel_kernelridgereg_ypredict = model_kernelridgereg.predict(x_validate)\n\n# kernel ridge regression model metrics\nmodel_kernelridgereg_rocaucscore = roc_auc_score(y_validate, model_kernelridgereg_ypredict)\nmodel_kernelridgereg_cvscores = cross_val_score(model_kernelridgereg, x, y, cv=20, scoring='roc_auc')\nprint('kernel ridge regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_kernelridgereg_rocaucscore, model_kernelridgereg_cvscores.mean(), 2 * model_kernelridgereg_cvscores.std()))","703b389e":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, -2, base=10, num=10),\n          'degree': [1, 2, 3, 4, 5],\n}\n\n# kernel ridge regression grid search model setup\nmodel_kernelridgereg_cv = GridSearchCV(model_kernelridgereg, params, iid=False, cv=5)\n\n# kernel ridge regression grid search model fit\nmodel_kernelridgereg_cv.fit(x_train, y_train)\n\n# kernel ridge regression grid search model prediction\nmodel_kernelridgereg_cv_ypredict = model_kernelridgereg_cv.predict(x_validate)\n\n# kernel ridge regression grid search model metrics\nmodel_kernelridgereg_cv_rocaucscore = roc_auc_score(y_validate, model_kernelridgereg_cv_ypredict)\nmodel_kernelridgereg_cv_cvscores = cross_val_score(model_kernelridgereg_cv, x, y, cv=20, scoring='roc_auc')\nprint('kernel ridge regression grid search\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_kernelridgereg_cv_rocaucscore, model_kernelridgereg_cv_cvscores.mean(), 2 * model_kernelridgereg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_kernelridgereg_cv.best_params_)","1e4a76f7":"# decision tree regression model setup\nmodel_treereg = DecisionTreeRegressor(splitter='best', min_samples_split=5)\n\n# decision tree regression model fit\nmodel_treereg.fit(x_train, y_train)\n\n# decision tree regression model prediction\nmodel_treereg_ypredict = model_treereg.predict(x_validate)\n\n# decision tree regression model metrics\nmodel_treereg_rocaucscore = roc_auc_score(y_validate, model_treereg_ypredict)\nmodel_treereg_cvscores = cross_val_score(model_treereg, x, y, cv=20, scoring='roc_auc')\nprint('decision tree regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_treereg_rocaucscore, model_treereg_cvscores.mean(), 2 * model_treereg_cvscores.std()))","0899ab1f":"# random forest regression model setup\nmodel_forestreg = RandomForestRegressor(n_estimators=100, min_samples_split=3, random_state=58)\n\n# random forest regression model fit\nmodel_forestreg.fit(x_train, y_train)\n\n# random forest regression model prediction\nmodel_forestreg_ypredict = model_forestreg.predict(x_validate)\n\n# random forest regression model metrics\nmodel_forestreg_rocaucscore = roc_auc_score(y_validate, model_forestreg_ypredict)\nmodel_forestreg_cvscores = cross_val_score(model_forestreg, x, y, cv=20, scoring='roc_auc')\nprint('random forest regression\\n  roc auc score: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_forestreg_rocaucscore, model_forestreg_cvscores.mean(), 2 * model_forestreg_cvscores.std()))","810ebb65":"# stan model setup\nmodel_code = \"\"\"\n    data {\n        int N; \/\/ the number of training data\n        int N2; \/\/ the number of testing data\n        int K; \/\/ the number of features\n        int y[N]; \/\/ the response variable\n        matrix[N,K] X; \/\/ the training matrix\n        matrix[N2,K] X_test; \/\/ the testing matrix\n    }\n    parameters {\n        vector[K] alpha;\n        real beta;\n    }\n    transformed parameters {\n        vector[N] y_linear;\n        y_linear = beta + X * alpha;\n    }\n    model {\n        alpha ~ cauchy(0, 10); \/\/ cauchy distribution\n        for (i in 1:K)\n            alpha[i] ~ student_t(1, 0, 0.03); \/\/ student t distribution\n        y ~ bernoulli_logit(y_linear); \/\/ bernoulli distribution, logit parameterization\n    }\n    generated quantities {\n        vector[N2] y_pred;\n        y_pred = beta + X_test * alpha;\n    }\n\"\"\"\n\nmodel_data = {\n    'N': 250,\n    'N2': 19750,\n    'K': 300,\n    'y': df_data.loc[df_data['datatype_training'] == 1, 'target'],\n    'X': df_data[df_data['datatype_training'] == 1].drop(['id', 'target', 'datatype_training'], axis=1),\n    'X_test': df_data[df_data['datatype_training'] == 0].drop(['id', 'target', 'datatype_training'], axis=1),\n}\n\nmodel_stan = pystan.StanModel(model_code=model_code)\n\n# stan model fit\nmodel_stan_fitted = model_stan.sampling(data=model_data, seed=58)","9f46c37c":"# prepare testing data and compute the observed value\nx_test = df_data[df_data['datatype_training'] == 0]\ny_test = pd.DataFrame(np.mean(model_stan_fitted.extract(permuted=True)['y_pred'], axis=0), columns=['target'], index=df_data.loc[df_data['datatype_training'] == 0, 'id'])","6fa83f26":"# submit the results\nout = pd.DataFrame({'id': y_test.index, 'target': y_test['target']})\nout.to_csv('submission.csv', index=False)","4e324c66":"With lasso regression submission, the LB score is 0.704. It's seem overfitting.","90d0191d":"> **Supply or submit the results**\n\nOur submission to the competition site Kaggle is ready. Any suggestions to improve our score are welcome.","9b7b93af":"> **Acquiring training and testing data**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames.","b34471df":"With linear regression submission, the LB score is 0.629. It's seem overfitting.","ae9e6752":"> **Problem overview**\n\nLong ago, in the distant, fragrant mists of time, there was a competition...\n\nIt was not just any competition. It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples... without overfitting. Data scientists \u2015 including Kaggle's very own Will Cukierski \u2015 competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse.\n\nSo... we're doing it again.\n\nThis is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend. In addition to bragging rights, the winner also gets swag. Enjoy!\n\nInteresting article:\n* https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/","eaca5d89":"> **Model, predict and solve the problem**\n\nNow, it is time to feed the features to Machine Learning models.","3100fe02":"With pystan bernoulli distribution, logit parameterization submission, the LB score is 0.859.","5fbe946c":"> **Feature exploration, engineering and cleansing**\n\nHere we generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution together with exploring some data.","77a5cb6b":"> **Analyze and identify patterns by visualizations**\n\nLet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilize the Seaborn plotting package which allows us to plot very conveniently as follows.\n\nThe Pearson Correlation plot can tell us the correlation between features with one another. If there is no strongly correlated between features, this means that there isn't much redundant or superfluous data in our training data. This plot is also useful to determine which features are correlated to the observed value.\n\nThe pairplots is also useful to observe the distribution of the training data from one feature to the other.\n\nThe pivot table is also another useful method to observe the impact between features.","9c1bab63":"With ridge regression submission, the LB score is 0.690. It's seem overfitting.","ac56a6be":"After extracting all features, it is required to convert category features to numerics features, a format suitable to feed into our Machine Learning models."}}