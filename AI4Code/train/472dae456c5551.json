{"cell_type":{"d15afe6e":"code","d1a3f739":"code","efd01b80":"code","3c149541":"code","bdb3fc2e":"code","ac40225a":"code","9d30b87a":"code","2ec6fade":"code","7cd4b0a9":"code","776253ac":"code","e415166c":"code","e2c4ba19":"code","547c108e":"code","0f06a856":"code","44d30dc9":"code","02766f19":"code","8da9797c":"code","1d50beda":"code","0f21517b":"code","6489a945":"code","06de9ca6":"code","adf8158b":"code","3458fdd2":"code","cc1a1e38":"code","c49debe7":"code","348ef8cf":"code","5c7e0ac3":"code","0343b5c0":"code","ffd1b46c":"code","770d8855":"code","83ab43b3":"code","b9a5e42a":"code","6a0c0d84":"code","88099962":"code","b99fb303":"code","2d83f509":"code","567d34c3":"code","1492aa61":"code","8ceaa7ab":"code","520d1102":"code","4682a3ac":"markdown","9fd0a15b":"markdown","d3e2664c":"markdown"},"source":{"d15afe6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1a3f739":"from lightgbm import LGBMClassifier as lgb\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom matplotlib import pyplot as plt\nnp.set_printoptions( threshold=100 )","efd01b80":"train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\n# features = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv')\n# example_sample_submission   = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/example_sample_submission.csv')\n","3c149541":"example_test   = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/example_test.csv')","bdb3fc2e":"train.shape","ac40225a":"train.head()","9d30b87a":"train.groupby('date')['ts_id'].count().plot(kind='bar')","2ec6fade":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","7cd4b0a9":"missing_values_table(train)","776253ac":"# Get the columns with > 50% missing\nmissing_df = missing_values_table(train);\nmissing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\nprint('We will remove %d columns.' % len(missing_columns))","e415166c":"#train_ = df.drop(list(missing_columns), axis=1)\n","e2c4ba19":"train = train.query('weight > 0').reset_index(drop = True)\n\ntrain['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\ndates = train.date\nX = train[features]\n# X = X.fillna(X.mean())\n# y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\ny = train['action'].values\n\nf_mean = np.mean(train[features[1:]].values,axis=0)","547c108e":"del(train)","0f06a856":"from sklearn.impute import SimpleImputer\n\nfill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\ncolumns = X.columns\nX = pd.DataFrame(fill_NaN.fit_transform(X))\nX.columns = columns\nX.index = X.index\n\n\n# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n# imp.fit(X)\nX.head()\n","44d30dc9":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns(X)","02766f19":"# import numpy as np\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n\n# imp = IterativeImputer(max_iter=10, random_state=0)\n# imp.fit(X)\n\n# # the model learns that the second feature is double the first\n# print(np.round(imp.transform(X)))\n","8da9797c":"## standartization'\nfrom sklearn.preprocessing import StandardScaler\n\n# create a scaler object\nscaler = StandardScaler()\ncolumns = X.columns\n# fit and transform the data\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX.columns = columns\nX","1d50beda":"## build feature \n# import sklearn.preprocessing as preproc\n# X = preproc.PolynomialFeatures(include_bias=False).fit_transform(X)","0f21517b":"# train.ts_id.nunique()","6489a945":"#lags\n# X['date'] = dates\n# X = X.set_index([\"date\"]).shift(1)\n# X = X.fillna(0)\n# X.head()\n\n# difs \n\nX= X.diff().fillna(0)\n","06de9ca6":"df_sample = X.copy()\ndf_sample['action'] = y.copy()\n\ndf_sample = df_sample.sample(frac=0.5, replace=False, random_state=1)","adf8158b":"# import seaborn as sns\n# sns.pairplot(df_sample)","3458fdd2":"# downsampled","cc1a1e38":"\"\"\"\nThis module provides a class to split time-series data for back-testing and evaluation.\nThe aim was to extend the current sklearn implementation and extend it's uses.\n\nMight be useful for some ;)\n\"\"\"\n\nimport logging\nfrom typing import Optional\n\nimport numpy as np\nfrom sklearn.model_selection._split import _BaseKFold\nfrom sklearn.utils import indexable\nfrom sklearn.utils.validation import _num_samples\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass TimeSeriesSplit(_BaseKFold):  # pylint: disable=abstract-method\n    \"\"\"Time Series cross-validator\n\n    Provides train\/test indices to split time series data samples that are observed at fixed time intervals,\n    in train\/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is\n    inappropriate.\n\n    This cross_validation object is a variation of :class:`TimeSeriesSplit` from the popular scikit-learn package.\n    It extends its base functionality to allow for expanding windows, and rolling windows with configurable train and\n    test sizes and delays between each. i.e. train on weeks 1-8, skip week 9, predict week 10-11.\n\n    In this implementation we specifically force the test size to be equal across all splits.\n\n    Expanding Window:\n\n            Idx \/ Time  0..............................................n\n            1           |  train  | delay |  test  |                   |\n            2           |       train     | delay  |  test  |          |\n            ...         |                                              |\n            last        |            train            | delay |  test  |\n\n    Rolling Windows:\n            Idx \/ Time  0..............................................n\n            1           | train   | delay |  test  |                   |\n            2           | step |  train  | delay |  test  |            |\n            ...         |                                              |\n            last        | step | ... | step |  train  | delay |  test  |\n\n    Parameters:\n        n_splits : int, default=5\n            Number of splits. Must be at least 4.\n\n        train_size : int, optional\n            Size for a single training set.\n\n        test_size : int, optional, must be positive\n            Size of a single testing set\n\n        delay : int, default=0, must be positive\n            Number of index shifts to make between train and test sets\n            e.g,\n            delay=0\n                TRAIN: [0 1 2 3] TEST: [4]\n            delay=1\n                TRAIN: [0 1 2 3] TEST: [5]\n            delay=2\n                TRAIN: [0 1 2 3] TEST: [6]\n\n        force_step_size : int, optional\n            Ignore split logic and force the training data to shift by the step size forward for n_splits\n            e.g\n            TRAIN: [ 0  1  2  3] TEST: [4]\n            TRAIN: [ 0  1  2  3  4] TEST: [5]\n            TRAIN: [ 0  1  2  3  4  5] TEST: [6]\n            TRAIN: [ 0  1  2  3  4  5  6] TEST: [7]\n\n    Examples\n    --------\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> tscv = TimeSeriesSplit(n_splits=5)\n    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE\n    TimeSeriesSplit(train_size=None, n_splits=5)\n    >>> for train_index, test_index in tscv.split(X):\n    ...    print('TRAIN:', train_index, 'TEST:', test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0] TEST: [1]\n    TRAIN: [0 1] TEST: [2]\n    TRAIN: [0 1 2] TEST: [3]\n    TRAIN: [0 1 2 3] TEST: [4]\n    TRAIN: [0 1 2 3 4] TEST: [5]\n    \"\"\"\n\n    def __init__(self,\n                 n_splits: Optional[int] = 5,\n                 train_size: Optional[int] = None,\n                 test_size: Optional[int] = None,\n                 delay: int = 0,\n                 force_step_size: Optional[int] = None):\n\n        if n_splits and n_splits < 5:\n            raise ValueError(f'Cannot have n_splits less than 5 (n_splits={n_splits})')\n        super().__init__(n_splits, shuffle=False, random_state=None)\n\n        self.train_size = train_size\n\n        if test_size and test_size < 0:\n            raise ValueError(f'Cannot have negative values of test_size (test_size={test_size})')\n        self.test_size = test_size\n\n        if delay < 0:\n            raise ValueError(f'Cannot have negative values of delay (delay={delay})')\n        self.delay = delay\n\n        if force_step_size and force_step_size < 1:\n            raise ValueError(f'Cannot have zero or negative values of force_step_size '\n                             f'(force_step_size={force_step_size}).')\n\n        self.force_step_size = force_step_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters:\n            X : array-like, shape (n_samples, n_features)\n                Training data, where n_samples is the number of samples  and n_features is the number of features.\n\n            y : array-like, shape (n_samples,)\n                Always ignored, exists for compatibility.\n\n            groups : array-like, with shape (n_samples,), optional\n                Always ignored, exists for compatibility.\n\n        Yields:\n            train : ndarray\n                The training set indices for that split.\n\n            test : ndarray\n                The testing set indices for that split.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)  # pylint: disable=unbalanced-tuple-unpacking\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        delay = self.delay\n\n        if n_folds > n_samples:\n            raise ValueError(f'Cannot have number of folds={n_folds} greater than the number of samples: {n_samples}.')\n\n        indices = np.arange(n_samples)\n        split_size = n_samples \/\/ n_folds\n\n        train_size = self.train_size or split_size * self.n_splits\n        test_size = self.test_size or n_samples \/\/ n_folds\n        full_test = test_size + delay\n\n        if full_test + n_splits > n_samples:\n            raise ValueError(f'test_size\\\\({test_size}\\\\) + delay\\\\({delay}\\\\) = {test_size + delay} + '\n                             f'n_splits={n_splits} \\n'\n                             f' greater than the number of samples: {n_samples}. Cannot create fold logic.')\n\n        # Generate logic for splits.\n        # Overwrite fold test_starts ranges if force_step_size is specified.\n        if self.force_step_size:\n            step_size = self.force_step_size\n            final_fold_start = n_samples - (train_size + full_test)\n            range_start = (final_fold_start % step_size) + train_size\n\n            test_starts = range(range_start, n_samples, step_size)\n\n        else:\n            if not self.train_size:\n                step_size = split_size\n                range_start = (split_size - full_test) + split_size + (n_samples % n_folds)\n            else:\n                step_size = (n_samples - (train_size + full_test)) \/\/ n_folds\n                final_fold_start = n_samples - (train_size + full_test)\n                range_start = (final_fold_start - (step_size * (n_splits - 1))) + train_size\n\n            test_starts = range(range_start, n_samples, step_size)\n\n        # Generate data splits.\n        for test_start in test_starts:\n            idx_start = test_start - train_size if self.train_size is not None else 0\n            # Ensure we always return a test set of the same size\n            if indices[test_start:test_start + full_test].size < full_test:\n                continue\n            yield (indices[idx_start:test_start],\n                   indices[test_start + delay:test_start + full_test])\n            \n# if __name__ == '__main__':\n#     X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n#     y = np.array([1, 2, 3, 4, 5, 6])\n#     tscv = TimeSeriesSplit(n_splits=5)\n#     print(tscv)  # doctest: +NORMALIZE_WHITESPACE\n#     for train_index, test_index in tscv.split(X):\n#         print('TRAIN:', train_index, 'TEST:', test_index)\n#         X_train, X_test = X[train_index], X[test_index]\n#         y_train, y_test = y[train_index], y[test_index]\n#     print(\"---------------------------------------------\")\n#     LARGE_IDX = np.arange(0, 30)\n#     rolling_window = TimeSeriesSplit(train_size=10, test_size=5, delay=3)\n#     print(rolling_window)\n#     for train_index, test_index in rolling_window.split(LARGE_IDX):\n#         print('TRAIN:', train_index, 'TEST:', test_index)\n#         X_train, X_test = LARGE_IDX[train_index], LARGE_IDX[test_index]\n","c49debe7":"class lightGBM(object):\n    def __init__(self, learning_rate=0.05, n_estimators=20, num_boost_round=10, num_leaves=30):\n        self.learning_rate = learning_rate\n        self.n_estimators = n_estimators\n        self.num_boost_round = num_boost_round\n        self.num_leaves = num_leaves\n        \n        \n    def fit(self, X, y):\n        try:\n            self.model = lgb(boosting_type='gbdt', num_leaves=self.num_leaves, learning_rate=self.learning_rate)\n            self.model.fit(X, y)\n        except Exception as err:\n            raise Exception('Nao foi possivel treinar o modelo ', err)\n        return self\n    \n    def predict(self,X):\n        self.predicted = self.model.predict(X)\n        self.predicted = pd.DataFrame(self.predicted, index= X.index, columns=['predicted'])\n        return self\n            ","348ef8cf":"# curva roc \n\ndef display_roc_curve(model, X_test, y_test):\n    plt.figure()\n    lw=2 \n    \n#     y_pred_proba = model.predict_proba(X_test)[::,1]\n    y_pred_proba = model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    auc_roc = roc_auc_score(y_test, y_pred_proba)\n    \n    plt.plot([0,1], [0,1], color='navy', lw=lw, linestyle= '--')\n    plt.plot(fpr, tpr, label= 'ROC curve(area = %0.2f)' % auc_roc, color = 'darkorange')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC')\n    plt.legend(loc='lower right')\n    \n    plt.show()\n    ","5c7e0ac3":"# lgbm = lightGBM()","0343b5c0":"# train['action'].value_counts(normalize=True)","ffd1b46c":"# X.shape","770d8855":"# Feature importance\n\n#lightGBM model fit\ngbm = lgb()\ngbm.fit(X, y)\ngbm.booster_.feature_importance()\n\n# importance of each attribute\nfea_imp_ = pd.DataFrame({'cols':X.columns, 'fea_imp':gbm.feature_importances_})\nfea_imp_ = fea_imp_.loc[fea_imp_.fea_imp > 0].sort_values(by=['fea_imp'], ascending = False)","83ab43b3":"fea_imp_.head(100)","b9a5e42a":"train_f = X[fea_imp_.head(30).cols]","6a0c0d84":"del(fea_imp_)","88099962":"#Recursive Feature Elimination(RFE)\n\n# from sklearn.feature_selection import RFE\n# # create the RFE model and select 10 attributes\n# rfe = RFE(gbm, 10)\n# rfe = rfe.fit(X_train, y_train)\n\n# # summarize the selection of the attributes\n# print(rfe.support_)\n\n# # summarize the ranking of the attributes\n# fea_rank_ = pd.DataFrame({'cols':X_train.columns, 'fea_rank':rfe.ranking_})\n# fea_rank_.loc[fea_rank_.fea_rank > 0].sort_values(by=['fea_rank'], ascending = True)","b99fb303":"# fea_rank_ = fea_rank_.loc[fea_rank_.fea_rank > 0].sort_values(by=['fea_rank'], ascending = True)","2d83f509":"# fea_rank_.head(10).cols","567d34c3":"rolling_window = TimeSeriesSplit(train_size=100000, test_size=5000, delay=0, n_splits=10)\n# rolling_window = TimeSeriesSplit(n_splits=5)\n\nfold = 0\n# X= train_f[fea_rank_.head(10).cols]\nX = train_f\nclf = lgb(boosting_type='gbdt', objective='binary')\n\n\nfor train_index, test_index in rolling_window.split(X):\n        fold +=1\n#         print('TRAIN:', train_index, 'TEST:', test_index)\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n    \n        clf = clf.fit(X_train, y_train)\n        pred = clf.predict(X_test)\n        \n        display_roc_curve(clf, X_test, y_test)\n        ","1492aa61":"# clf = lgb(boosting_type='gbdt', objective='binary')\n# clf.fit(X,y)\n# clf.predict(example_test)","8ceaa7ab":"# import numpy as np\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.svm import SVC\n\n# rolling_window = TimeSeriesSplit(train_size=10000, test_size=1000, delay=0)\n\n# # clf = make_pipeline(StandardScaler(), SVC(gamma='scale', C=0.5, kernel='rbf', degree=5, probability=True))\n# clf = SVC(gamma='scale', C=0.5, kernel='rbf', degree=5, probability=True)\n\n# X = X.fillna(X.mean())\n\n# for train_index, test_index in rolling_window.split(X):\n#         fold +=1\n#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n#         y_train, y_test = y[train_index], y[test_index]\n#         model = clf.fit(X_train, y_train)\n#         pred = model.predict(X_test)\n        \n#         display_roc_curve(model, X_test, y_test)\n        \n","520d1102":"import janestreet\n\njanestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\nfeatures = X.columns\nfor (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df[features]\n#     for i in features:\n#         x = X_test[i].mean()     \n#         X_test[i] = X_test[i].fillna(x)\n    X_test = X_test.fillna(0)\n    y_preds = clf.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","4682a3ac":"## Model","9fd0a15b":"## Data Prep","d3e2664c":"## Data Understanding"}}