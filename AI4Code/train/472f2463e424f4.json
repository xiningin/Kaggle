{"cell_type":{"a029df3c":"code","a94349a6":"code","3ae61605":"code","edb7d91d":"code","36a7190a":"code","c8202173":"code","125712e5":"code","d869663e":"code","62b56a64":"code","40091c10":"code","576c3afe":"code","ccdfcfe7":"code","b6adc5c3":"code","d64aced1":"code","95f88d48":"code","faad17d2":"code","df32c8df":"code","aad85264":"code","1bc95b6e":"code","8b31be6a":"code","d3eb5601":"code","38052038":"code","bd4054d4":"code","1f23edff":"code","19f53a4a":"code","b27be8ac":"code","7654aa82":"code","4536ae87":"code","945f5b61":"code","5d2612e7":"code","b5c181e3":"code","0ba2be42":"code","1e7f0690":"code","db6f1279":"code","8a6581c1":"code","df3cbb2f":"code","a7a9a6b4":"code","55bd7abc":"markdown","239fbe90":"markdown","e4a54145":"markdown","81add25d":"markdown","de0cd617":"markdown","dc3c3291":"markdown","fee3e1d2":"markdown","df2e4c06":"markdown","3cd24c9c":"markdown","ea35d402":"markdown","1b1924b3":"markdown","6b414b88":"markdown","49683b52":"markdown","e369b2d8":"markdown","a61a7b6c":"markdown","a05f0525":"markdown","33ccf684":"markdown","cb79a6c7":"markdown","3f02f984":"markdown","7e2dcce5":"markdown","a470d85d":"markdown","f6f2d720":"markdown","c108d153":"markdown","39a2d652":"markdown","d7999932":"markdown","b73e0973":"markdown","5b819e0f":"markdown"},"source":{"a029df3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a94349a6":"import matplotlib.pyplot as plt\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\nimport tensorflow as tf","3ae61605":"train_data = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntrain_data.head()","edb7d91d":"X_train = train_data.drop('label',axis=1)\nX_train.head()","36a7190a":"X_train=X_train.values\nprint(X_train.shape)","c8202173":"X_train=X_train.reshape(-1,28,28,1)\nprint(X_train.shape)","125712e5":"fig,axe=plt.subplots(2,2)\nidx = 0\nfor i in range(2):\n    for j in range(2):\n        axe[i,j].imshow(X_train[idx].reshape(28,28),cmap='gray')\n        idx+=1","d869663e":"X_train =  X_train.astype('float32')","62b56a64":"X_train = X_train\/255\nX_train = X_train*2 - 1.","40091c10":"print(X_train.max(),X_train.min())","576c3afe":"generator = Sequential()\ngenerator.add(Dense(512,input_shape=[100]))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Dense(256))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Dense(128))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization(momentum=0.8))\ngenerator.add(Dense(784))\ngenerator.add(Reshape([28,28,1]))","ccdfcfe7":"generator.summary()","b6adc5c3":"discriminator = Sequential()\ndiscriminator.add(Dense(1,input_shape=[28,28,1]))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dense(256))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(128))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(64))\ndiscriminator.add(LeakyReLU(alpha=0.2))\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(1,activation='sigmoid'))","d64aced1":"discriminator.summary()","95f88d48":"GAN =Sequential([generator,discriminator])\ndiscriminator.compile(optimizer='adam',loss='binary_crossentropy')\ndiscriminator.trainable = False","faad17d2":"GAN.compile(optimizer='adam',loss='binary_crossentropy')","df32c8df":"GAN.layers","aad85264":"GAN.summary()","1bc95b6e":"epochs = 30\nbatch_size = 100\nnoise_shape=100","8b31be6a":"with tf.device('\/gpu:0'):\n for epoch in range(epochs):\n    print(f\"Currently on Epoch {epoch+1}\")\n    \n    \n    for i in range(X_train.shape[0]\/\/batch_size):\n        \n        if (i+1)%50 == 0:\n            print(f\"\\tCurrently on batch number {i+1} of {X_train.shape[0]\/\/batch_size}\")\n            \n        noise=np.random.normal(size=[batch_size,noise_shape])\n       \n        gen_image = generator.predict_on_batch(noise)\n        \n        train_dataset = X_train[i*batch_size:(i+1)*batch_size]\n       \n        #training discriminator on real images\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = True\n        d_loss_real=discriminator.train_on_batch(train_dataset,train_label)\n        \n        #training discriminator on fake images\n        train_label=np.zeros(shape=(batch_size,1))\n        d_loss_fake=discriminator.train_on_batch(gen_image,train_label)\n        \n        \n        #training generator \n        noise=np.random.normal(size=[batch_size,noise_shape])\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = False\n        \n        d_g_loss_batch =GAN.train_on_batch(noise, train_label)\n        \n        \n        \n       \n    #plotting generated images at the start and then after every 10 epoch\n    if epoch % 10 == 0:\n        samples = 10\n        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, 100)))\n\n        for k in range(samples):\n            plt.subplot(2, 5, k+1)\n            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n            plt.xticks([])\n            plt.yticks([])\n\n        plt.tight_layout()\n        plt.show()\n\n        \n        \nprint('Training is complete')","d3eb5601":"noise=np.random.normal(size=[10,noise_shape])\n\ngen_image = generator.predict(noise)","38052038":"plt.imshow(noise)\nplt.title('How the noise looks')","bd4054d4":"fig,axe=plt.subplots(2,5)\nfig.suptitle('Generated Images from Noise using GANs')\nidx=0\nfor i in range(2):\n    for j in range(5):\n         axe[i,j].imshow(gen_image[idx].reshape(28,28),cmap='gray')\n         idx+=1","1f23edff":"generator = Sequential()\ngenerator.add(Dense(7 * 7 * 128, input_shape=[100]))\ngenerator.add(Reshape([7, 7, 128]))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\",\n                                 activation=\"relu\"))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"same\",\n                                 activation=\"tanh\"))","19f53a4a":"generator.summary()","b27be8ac":"discriminator = Sequential()\ndiscriminator.add(Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n                        activation=LeakyReLU(0.3),\n                        input_shape=[28, 28, 1]))\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n                        activation=LeakyReLU(0.3)))\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dense(1, activation=\"sigmoid\"))","7654aa82":"discriminator.summary()","4536ae87":"GAN =Sequential([generator,discriminator])\ndiscriminator.compile(optimizer='adam',loss='binary_crossentropy')\ndiscriminator.trainable = False","945f5b61":"GAN.compile(optimizer='adam',loss='binary_crossentropy')","5d2612e7":"GAN.layers","b5c181e3":"GAN.summary()","0ba2be42":"epochs = 150 #vary epoch size as per required to train the model\nbatch_size = 100","1e7f0690":"noise_shape=100","db6f1279":"\nwith tf.device('\/gpu:0'):\n for epoch in range(epochs):\n    print(f\"Currently on Epoch {epoch+1}\")\n    \n    \n    for i in range(X_train.shape[0]\/\/batch_size):\n        \n        if (i+1)%50 == 0:\n            print(f\"\\tCurrently on batch number {i+1} of {X_train.shape[0]\/\/batch_size}\")\n            \n        noise=np.random.normal(size=[batch_size,noise_shape])\n       \n        gen_image = generator.predict_on_batch(noise)\n        \n        train_dataset = X_train[i*batch_size:(i+1)*batch_size]\n       \n        #training discriminator on real images\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = True\n        d_loss_real=discriminator.train_on_batch(train_dataset,train_label)\n        \n        #training discriminator on fake images\n        train_label=np.zeros(shape=(batch_size,1))\n        d_loss_fake=discriminator.train_on_batch(gen_image,train_label)\n        \n        \n        #training generator \n        noise=np.random.normal(size=[batch_size,noise_shape])\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = False #while training the generator as combined model,discriminator training should be turned off\n        \n        d_g_loss_batch =GAN.train_on_batch(noise, train_label)\n        \n        \n        \n       \n    #plotting generated images at the start and then after every 10 epoch\n    if epoch % 10 == 0:\n        samples = 10\n        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, 100)))\n\n        for k in range(samples):\n            plt.subplot(2, 5, k+1)\n            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n            plt.xticks([])\n            plt.yticks([])\n\n        plt.tight_layout()\n        plt.show()\n\n        \n        \nprint('Training is complete')","8a6581c1":"noise=np.random.normal(loc=0, scale=1, size=(100,noise_shape))\n\ngen_image = generator.predict(noise)","df3cbb2f":"fig,axe=plt.subplots(2,5)\nfig.suptitle(\"Actual Images\")\nidx = 0\nfor i in range(2):\n    for j in range(5):\n        axe[i,j].imshow(X_train[idx].reshape(28,28),cmap='gray')\n        idx+=10","a7a9a6b4":"fig,axe=plt.subplots(2,5)\nfig.suptitle('Generated Images from Noise using DCGANs')\nidx=0\nfor i in range(2):\n     for j in range(5):\n         axe[i,j].imshow(gen_image[idx].reshape(28,28),cmap='gray')\n         idx+=3","55bd7abc":"# **Working Principle**\n\n**Generative** network is fed noise may be in the form a random distribution and generates fake data from the noise. The fake data from the generator is input to the discriminator. Once the training is complete, the generator should be able to generate real like data from the noises.\n\nThe interesting fact here is that the generator learns to produce meaningful images without even actually looking at the image.\n\n![1_CkMMefLPqcEKPuuPLZY2_Agg.png](attachment:1_CkMMefLPqcEKPuuPLZY2_Agg.png)\n\n**Discriminator** or the adversarial network works as the opponent to the generator. It is basically a  classifier or discriminator whose function is to distinguish between two different classes of data. Here the classes are real data (labelled as 1) and the fake data the generator produces (labelled as 0).\n\n","239fbe90":"1. Compiling the discriminator layer\n2. Compiling the GAN \n\nNOTE : the generator layer is not compiled seperately because it gets trained as part of the combined model but training the discriminator is necessary because it is trained before the combined model.","e4a54145":"# **Generative Adverserial Networks**\n\n\nGenerative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the \u201cadversarial\u201d) in order to generate new, synthetic instances of data that can pass for real data. They are used widely in image generation, video generation and voice generation.\n\n\n![Generative-Adversarial-Networks-Framework.png](attachment:Generative-Adversarial-Networks-Framework.png)\n\n\n\nWhile most deep generative models are trained by maximizing log likelihood or a lower bound on log likelihood, GANs take a radically different approach that does not require inference or explicit calculation of the data likelihood. Instead, two models are used to solve a minimax game: a generator which samples data, and a discriminator which classifies the data as real or generated.In theory these models are capable of modeling an arbitrarily complex probability distribution.\n\n\n\n\n\n\n","81add25d":"**Importing useful packages**","de0cd617":"# **References**\n\n1. https:\/\/arxiv.org\/abs\/1406.2661 - GANs paper in 2014 by Ian J. Goodfellow and others.\n2. https:\/\/arxiv.org\/abs\/1511.06434 - DCGANs in 2016\n3. https:\/\/arxiv.org\/pdf\/1611.02163.pdf - 2017 ICLR paper \n4. https:\/\/towardsdatascience.com\/understanding-generative-adversarial-networks-gans-cd6e4651a29","dc3c3291":"**NOTE**  after normalizing it is multiplied with 2 and substracted from 1 such that it ranges from (-1,1) because in DCGANs the last layer generative model activation is tanh which range is (-1,1) unlike sigmoid ranging (0,1) .","fee3e1d2":"dropping the labels from the dataset","df2e4c06":"# **Training the network**\n\nThe important thing about training a GAN is that the two components should never be trained together. Rather the network is trained in two different phases,the first phase is for training the discriminator and updating the weights appropriately and in the next step the generator is trained while the discriminator training is disabled.\n\n**Phase 1**\nDuring phase one of training the generator is fed random data(in the form of a distribution) as noise. The generator creates some random images which are given to the discriminator. The discriminator also takes input from dataset of real images. \nThe discriminator learns to distinguish the real data from the fake ones by learning or assessing features from it's inputs. The discriminator outputs some probability and difference between the predicted results and the actual results are backpropagated through the network and the weights of the discriminator is updated. \nRemember during this phase, the backpropagation stops at the end of the discriminator and the generator is not trained or updated.\n\n\n**Phase 2**\nIn this phase, the generator produced batch of images are directly given as input to the discriminator. The real images are not given this time to the discriminator. The generator learns by tricking the discriminator into it outputting false positives. The discriminator outputs probabilities which are assessed against the actual results and the weights of of the generator are updated through backpropagation.\nRemember here during backpropagation, the discriminator weights should not be updated and kept as they were before.\n\n![Train1_t82vgL9KcDVpT4JqCb9Q4Q.png](attachment:Train1_t82vgL9KcDVpT4JqCb9Q4Q.png)","3cd24c9c":"# **Simple GAN Model**","ea35d402":"**Generative part**","1b1924b3":"# **Introduction**\n\n\nGANs are made up of two components - \n\n1. **Generator - generates new data instances**\n\n2. **Discriminator - tries to distinguish the generated or fake data from the real dataset.**\n\nDiscriminative algorithms try to classify input data; that is, given the features of an instance of data, they predict a label or category to which that data belongs. So discriminative algorithms map features to labels. They are concerned solely with that correlation.One the other way,loosely speaking,generative algorithms do the opposite. Instead of predicting a label given certain features, they attempt to predict features given a certain label.\n\nWhile training they both start together from scratch and the generator learn to shape the random distribition through the training epochs.","6b414b88":"**Applications of GANs**\n\n* Generate Examples for Image Datasets\n* Generate Photographs of Human Faces\n* Generate Realistic Photographs\n* Image-to-Image Translation\n* Text-to-Image Translation\n* Semantic-Image-to-Photo Translation\n* Photos to Emojis\n* Face Aging\n* Super Resolution\n* 3D Object Generation\n\n\n![sr.jpg](attachment:sr.jpg)","49683b52":"In this model we construct the generator using transpose convolution layers instead of simple dense layers which helps better in feature capturing and prevents the issue as described earlier. Similarly while building the discriminator layer,instead of simple dense units it uses convolution layers to increase efficiency of it's classification.\n\n\n![GANsdeep.png](attachment:GANsdeep.png)\n\n\nThe paper that proposed this architecture https:\/\/arxiv.org\/abs\/1511.06434\nTo know more about Conv2D https:\/\/keras.io\/layers\/convolutional\/","e369b2d8":"# **Objective**\n\nThe objective of this notebook is to introduce beginners to the underlying concept of Generative Adversarial Networks (GANs) in the most intuitive way possible. The aim is to describe the working of such network without delving deep into the complex mathematics associated and in a more practical way such that the viewer can develop his own simple GAN. \n\nA simple GAN is first constructed here with only hidden dense layers and tries to output meaningful images. The output is analysed discovering one particular issue which is commonplace in simple GANs. The issue is resolved in the next part with one particular type of GAN called Deep Convolution Generative Adversarial Networks (DCGANs) .","a61a7b6c":"# **Data Visualization**","a05f0525":"**Generator producing images from noise**","33ccf684":"![](http:\/\/)","cb79a6c7":"# **Problems with simple GANs**\n\nIn practice, however, GANs suffer from many issues, particularly during training. One common\nfailure mode involves the generator collapsing to produce only a single sample or a small family of\nvery similar samples. In this case, the generator learns to trick the discriminator with a single image or a few images to believe as real image. Another involves the generator and discriminator oscillating during training,rather than converging to a fixed point. In addition, if one agent becomes much more powerful than the other, the learning signal to the other agent becomes useless, and the system does not learn.\nTo train GANs many tricks must be employed,one such method is using Deep Convolution Generative Adversarial Networks as discussed in a research paper published in 2016 (link in the references).\n\n\n","3f02f984":"**Inference from generated images**\n\nAs discussed before after iterating through few tens of epochs the generator learns to trick the discriminator with only one specific category of output image and hence stops learning beyond that point. The ouput after complete training is thus only single type of images almost similar in features ,i.e, shirts in this particular case and there is no variation in generated image features.\n\nThe above issue can be resolved using DCGANs as given below.","7e2dcce5":"# **Inference**\n\nIntroduction of deep convolution GAN helps in improving the variation in the output data and prevents the model from being stuck on single type of image. The interesting fact here seems that the generator being capable of creating meaningful images from noise inputs without even itself seeing the actual images and that's what makes GANS so interesting.","a470d85d":"# **GAN using Deep Convolutions**","f6f2d720":"The pixel data ranges from 0 to 255 hence dividing each pixel by 255,i.e,normalizing the data such that the range is within 0 to 1.","c108d153":"# **Loading and Preprocessing the data**","39a2d652":"**LOSS FUNCTION of simple GAN**\n\nIn the paper that introduced GANs(link below in the reference), the generator tries to minimize the following function while the discriminator tries to maximize it:\n\n![loss.png](attachment:loss.png)\nIn this function:\n\n* D(x) is the discriminator's estimate of the probability that real data instance x is real.\n* Ex is the expected value over all real data instances.\n* G(z) is the generator's output when given noise z.\n* D(G(z)) is the discriminator's estimate of the probability that a fake instance is real.\n* Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).\n\n\nThe generator can't directly affect the log(D(x)) term in the function, so, for the generator, minimizing the loss is equivalent to minimizing log(1 - D(G(z))).","d7999932":"# **Let's begin**","b73e0973":"noise generated using np.random.normal is given to as input to the generator. In the next step the generator produces batches of meaningful alike image from the random distribution.","5b819e0f":"**Discriminatory part**"}}