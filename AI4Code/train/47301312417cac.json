{"cell_type":{"03afe931":"code","792e61f9":"code","d6374bb0":"code","ce894843":"code","95fe1b6f":"code","50b490d4":"code","0e0c47ac":"code","9bcdf7d3":"code","45ba8a2c":"code","c6fa4a31":"code","6e2bef54":"code","f2bc1e9b":"code","f139b943":"code","5deb7896":"code","e3384e0e":"code","e00f0239":"code","0a886f49":"code","8dcd2ad6":"code","d3072fc4":"code","6e99e9fd":"code","e1111d41":"code","d7bff20f":"code","d09ca748":"code","0d1f9ed8":"code","d6c16fb1":"code","9b370900":"code","68e19184":"code","b7287246":"code","2a8f9db3":"code","3bb54065":"code","fbc86973":"code","5af7b8e2":"code","78ed88d2":"code","4e2690c5":"code","4e42263b":"markdown","c693f562":"markdown","8ee11966":"markdown","2dd2a792":"markdown","ef1f3811":"markdown","0e8be97e":"markdown","d7eb8669":"markdown","67ebc1de":"markdown","2f36129a":"markdown","4f26e429":"markdown","62dd5697":"markdown","2b7591cd":"markdown","7e13ffa1":"markdown","68c764e2":"markdown","d6751693":"markdown","3fbfd9a7":"markdown","868ef037":"markdown","5f299795":"markdown","9a55eb3b":"markdown","c1841421":"markdown","4e4534a1":"markdown","4dc6250d":"markdown","665852b2":"markdown","bd476a50":"markdown"},"source":{"03afe931":"import pandas as pd\nimport os\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nimport seaborn as sns\nrand_state = 42\nnp.random.seed(rand_state)\n\nwarnings.simplefilter('ignore')\n%matplotlib inline","792e61f9":"path = '..\/input\/sotu'\ndirs = os.listdir(path)\n\ndf = pd.DataFrame(columns=['year', 'president', 'party', 'text'])\n\nfor i in range(len(dirs)):\n    components = dirs[i].split('_')\n    name = components[0]\n    year = components[1].split('.')[0]\n    df.loc[i,'year'] = year\n    df.loc[i,'president'] = name   \n    \n    filename = os.path.join(path, dirs[i])\n    text_file = open(filename, \"r\")\n    \n    lines = text_file.read()\n    lines = lines.replace('\\n', ' ')\n    df.loc[i, 'text'] = lines.lower()\n    \ndf.year = df.year.astype(int) \ndf.president = df.president.astype(str)\ndf.text = df.text.astype(str)\nprint('Shape: ', df.shape)","d6374bb0":"# need to distinuish between Theodore Roosevelt and Franklin D. Roosevelt\nindices = df.query(\"president =='Roosevelt' & year <= 1909\").index\ndf.loc[indices,'president'] = 'Theodore Roosevelt'\n\nindices = df.query(\"president == 'Roosevelt'\").index\ndf.loc[indices,'president'] = 'Franklin D. Roosevelt'\n\nindices = df.query(\"president =='Bush' & year <= 1992\").index\ndf.loc[indices,'president'] = 'George H. W. Bush'\n\nindices = df.query(\"president == 'Bush'\").index\ndf.loc[indices,'president'] = 'George W. Bush'\n\nindices = df.query(\"president =='Johnson' & year <= 1869\").index\ndf.loc[indices,'president'] = 'Andrew Johnson'\n\nindices = df.query(\"president == 'Johnson'\").index\ndf.loc[indices,'president'] = 'Lyndon B. Johnson'\n\nindices = df.query(\"president =='Adams' & year <= 1801\").index\ndf.loc[indices,'president'] = 'John Adams'\n\nindices = df.query(\"president == 'Adams'\").index\ndf.loc[indices,'president'] = 'John Quincy Adams'\n\n\nindices = df.query(\"president =='Harrison' & year <= 1841\").index\ndf.loc[indices,'president'] = 'William Henry Harrison'\n\nindices = df.query(\"president == 'Harrison'\").index\ndf.loc[indices,'president'] = 'Benjamin Harrison'","ce894843":"def pres_to_party(name):\n    republican = ['Lincoln', 'Grant', 'Hayes', 'Garfield', 'Arthur', \n                  'Benjamin Harrison', 'McKinley', 'Theodore Roosevelt', \n                  'Taft', 'Harding', 'Coolidge', 'Hoover', 'Eisenhower', \n                  'Nixon', 'Ford', 'Reagan', 'George H. W. Bush', \n                  'George W. Bush', 'Trump']\n    if name in republican:\n        return 'Republican'\n    \n    democratic = ['Jackson', 'Buren', 'Polk', 'Pierce', \n                  'Buchanan', 'Cleveland', 'Wilson', 'Franklin D. Roosevelt', \n                  'Truman', 'Kennedy', 'Lyndon B. Johnson', 'Carter', 'Clinton', 'Obama']\n    if name in democratic:\n        return 'Democratic'\n    \n    whig = ['William Henry Harrison', 'Taylor', 'Fillmore']\n    if name in whig:\n        return 'Whig'\n    \n    national_union = ['Andrew Johnson']\n    if name in national_union:\n        return 'National Union'\n    \n    \n    unaffiliated = ['Washington', 'Tyler']\n    if name in unaffiliated:\n        return 'Unaffiliated'\n    \n    federalist = ['John Adams']\n    if name in federalist:\n        return 'Federalist'\n    \n    democratic_republican = ['Jefferson', 'Madison', 'Monroe', 'John Quincy Adams']\n    if name in democratic_republican:\n        return 'Democratic-Republican'\n    \ndf.party = df.president.apply(pres_to_party)","95fe1b6f":"df.set_index('year', inplace=True)\ndf.sort_index(inplace=True)\n\n# need to drop George Washington's 1790 address as the file is empty\ndf = df.iloc[1:,:]\ndf.head()","50b490d4":"df.groupby('party').size()","0e0c47ac":"df = df[df.party.isin(['Republican', 'Democratic'])]","9bcdf7d3":"from nltk import sent_tokenize\n\nsentences = [sent_tokenize(text) for text in df.text]\n\n# remove the first and last sentences (meaningless intro\/closing statements)\nfor i in range(len(sentences)):\n    del sentences[i][0]\n    del sentences[i][-1]\n    \n    \nsentence_lengths = [len(sent) for sent in sentences]\ndf['sentences'] = sentences\ndf['sentence_length'] = [len(sent) for sent in sentences]\n\n# now need to \"unstack\" the above list of lists of sentences\nsentences_all = []\nfor sentences in sentences:\n    for sent in sentences:\n        sentences_all.append(sent)","45ba8a2c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(10,6))\nsns.lineplot(x='year',y='sentence_length',hue='party',data=df.reset_index())\nplt.xlabel('')\nplt.ylabel('Number of Sentences')\nsns.despine()\nplt.show()","c6fa4a31":"keys = df.president.unique()\nvalues = np.arange(keys.shape[0])\npres_to_num = dict(zip(keys, values))\n\ndf['president_num'] = df.president.map(pres_to_num)\n# we will use Democratic as the positive class\ndf['party_num'] = (df.party == 'Democratic').astype(int)\n\ntarget = [] # np.zeros((len(sentences_all,)))\n\nfor i in range(df.shape[0]):\n    target.append(np.ones((df.iloc[i,4],)) * df.iloc[i,5])\n    \ntarget = np.concatenate(target, axis=0)","6e2bef54":"from keras.utils import to_categorical\n\ntarget = to_categorical(target)","f2bc1e9b":"from sklearn.model_selection import train_test_split\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences_all, target, test_size=0.2, random_state=42)","f139b943":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_all)\n\nx_train = tokenizer.texts_to_sequences(sentences_train)\nx_test = tokenizer.texts_to_sequences(sentences_test)","5deb7896":"sentences_words = [len(sequence) for sequence in x_train]\nprint(\"99% quantile: \", pd.Series(sentences_words).quantile(.99))\nsns.distplot(sentences_words)","e3384e0e":"vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint('Vocabulary size: ', vocab_size)\nprint(sentences_train[2])\nprint(x_train[2])","e00f0239":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 91\n\nx_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\nx_test = pad_sequences(x_test, padding='post', maxlen=maxlen)","0a886f49":"def plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","8dcd2ad6":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(target.shape[1], activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","d3072fc4":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nestop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\nmcp_save = ModelCheckpoint('embeddings_model.hdf5', save_best_only=True, monitor='val_acc', mode='max')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, verbose=1) #, epsilon=1e-4, mode='min')\n\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    verbose=True,\n                    validation_data=(x_test, y_test),\n                    callbacks=[estop, mcp_save, reduce_lr_loss],\n                    batch_size=128)\nloss, accuracy = model.evaluate(x_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(x_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","6e99e9fd":"import re\nfrom gensim import models, corpora\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nNUM_TOPICS = 20\nSTOPWORDS = stopwords.words('english')\n\nwnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n\ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]\n\ndef clean_text(text):\n    tokenized_text = word_tokenize(text.lower())\n    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n    return lemmatize_sent(' '.join(cleaned_text))","e1111d41":"df['tokens'] = df.text.apply(clean_text)\ndf.head()","d7bff20f":"#WordCloud\nfrom wordcloud import WordCloud, STOPWORDS\n\ncleaned_text = ' '.join(list(df.text))\n\nwordcloud = WordCloud(stopwords=STOPWORDS,max_words=100,\n                      background_color='black',min_font_size=6,\n                      width=3000,collocations=False,\n                      height=2500\n                     ).generate(cleaned_text)\n\nplt.figure(1,figsize=(20, 20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig('wordcloud_1.png')\nplt.show()","d09ca748":"token_string = ''\nfor tokens in df.tokens:\n    token_string += ' '.join(tokens) + ' '\n\nwordcloud_tokens = WordCloud(stopwords=STOPWORDS,max_words=100,\n                      background_color='black',min_font_size=6,\n                      width=3000,collocations=False,\n                      height=2500\n                     ).generate(token_string)\n\nplt.figure(1,figsize=(20, 20))\nplt.imshow(wordcloud_tokens)\nplt.axis('off')\nplt.savefig('wordcloud_2.png')\nplt.show()","0d1f9ed8":"# Build a Dictionary - association word to numeric id\ndictionary = corpora.Dictionary(df.tokens)\n\n'''\nWe can \"control\" the level in which we extract topics from:\n  * We can filter out tokens that show up in x% of all SOTU's, in effect\n    uncovering more hidden topics (only present in (1-x)% of the SOTUs).\n  * A similar strategy can be used to filter out very rare tokens by setting\n    no_below\n\nInitially we will not do this, but it follows that this strategy might be helpful\n\nin classification models. In this case we might also want to increase\n  the number of latent topics to discover: less frequenct topics could be \n  quite powerful in prediction\n'''\ndictionary.filter_extremes(no_below=3, no_above=.03)\n\n# Transform the collection of texts to a numerical form\ncorpus = [dictionary.doc2bow(text) for text in df.tokens]\n\n# Build the LDA model\nlda_model = models.LdaModel(corpus=corpus, \n                            random_state=rand_state, \n                            iterations=200,\n                            num_topics=20, \n                            id2word=dictionary)\n\nprint(\"LDA Model:\")\n \nfor idx in range(NUM_TOPICS):\n    # Print the first 10 most representative topics\n    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))","d6c16fb1":"import pyLDAvis.gensim as gensimvis\nimport pyLDAvis\n\n\nvis_data_lda = gensimvis.prepare(lda_model, corpus, dictionary)\n#pyLDAvis.save_html(vis_data_lda,'lda_all.html')\npyLDAvis.display(vis_data_lda)","9b370900":"lda_scores = [] #np.array((len(corpus), NUM_TOPICS))\n\nfor i in range(len(corpus)):\n    y = lda_model[corpus[i]]\n    #lda_scores.append([score[1] for score in y])\n    lda_scores.append({score[0]:score[1] for score in y})\n    \nlda_df = pd.DataFrame(lda_scores)\nlda_df.index = df.index\nlda_df.fillna(0.0, inplace=True)\nlda_df.head()","68e19184":"lda_df['party'] = df.party\nlda_df.groupby('party').mean()","b7287246":"from sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n\n\nlda_new = lda_df[lda_df.party.isin(['Republican', 'Democratic'])]\n\nX_train, X_test, y_train, y_test = train_test_split(lda_new.drop('party', axis=1), \n                                                    lda_new['party'], \n                                                    test_size=0.2, \n                                                    random_state=rand_state)\n\nmodel = RidgeClassifier(class_weight='balanced', random_state=rand_state)\nmodel.fit(X_train, y_train)\nprint(\"Train score: \", model.score(X_train, y_train))\nprint(\"Test score: \", model.score(X_test, y_test))","2a8f9db3":"path = '..\/input\/sotu'\ndirs = os.listdir(path)\n\ndf = pd.DataFrame(columns=['year', 'president', 'text'])\n\nfor i in range(len(dirs)):\n    components = dirs[i].split('_')\n    name = components[0]\n    year = components[1].split('.')[0]\n    df.loc[i,'year'] = year\n    df.loc[i,'president'] = name   \n    \n    filename = os.path.join(path, dirs[i])\n    text_file = open(filename, \"r\")\n    \n    lines = text_file.read()\n    df.loc[i, 'text'] = lines.replace('\\n', ' ')\n    \ndf.year = df.year.astype(int) \ndf.president = df.president.astype(str)\ndf.text = df.text.astype(str)\ndf.set_index('year', inplace=True)\ndf.sort_index(inplace=True)\n\n# need to drop George Washington's 1790 address as the file is empty\ndf = df.iloc[1:,:]","3bb54065":"def clean(text):\n    # remove \\\n    text = text.strip('\\\\')\n    # replace -- with space\n    text = text.replace('--',' ')\n    \n    return text","fbc86973":"from sumy.summarizers.lex_rank import LexRankSummarizer\nfrom sumy.summarizers.luhn import LuhnSummarizer\nfrom sumy.summarizers.lsa import LsaSummarizer\nfrom sumy.summarizers.text_rank import TextRankSummarizer\n\ndef summarize_text(text, should_print=True):\n    #Summarize the document with 2 sentences\n    summarizer = LexRankSummarizer()\n    summary = summarizer(parser.document, 2) \n\n    summarizer_1 = LuhnSummarizer()\n    summary_1 = summarizer_1(parser.document,2)\n\n    summarizer_2 = LsaSummarizer()\n    summary_2 =summarizer_2(parser.document,2)\n\n    summarizer_3 = TextRankSummarizer()\n    summary_3 =summarizer_3(parser.document,2)\n    \n    if should_print:\n        print('Lex Rank\\n===========')\n        for sentence in summary:\n            print('-', sentence)\n        \n        print('\\nLuhn\\n===========')\n        for sentence in summary_1:\n            print('-', sentence)\n            \n        print('\\nLSA\\n===========')\n        for sentence in summary_2:\n            print('-', sentence)\n            \n        print('\\nText Rank\\n===========')\n        for sentence in summary_3:\n            print('-', sentence)\n            \n    return summary + summary_1 + summary_2 + summary_3","5af7b8e2":"#Plain text parsers since we are parsing through text\nfrom sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\n\ntext = clean(df.loc[2017,'text'])\nparser = PlaintextParser.from_string(text, Tokenizer('english'))\n\nprint('-----------------------------------------')\nprint('Donald J. Trump, 2017')\nprint('-----------------------------------------')\nsummaries_2005 = summarize_text(text)","78ed88d2":"text = clean(df.loc[2012,'text'])\nparser = PlaintextParser.from_string(text, Tokenizer('english'))\n\nprint('-----------------------------------------')\nprint('Barack Obama, 2012')\nprint('-----------------------------------------')\nsummaries_2012 = summarize_text(text)","4e2690c5":"text = clean(df.loc[2005,'text'])\nparser = PlaintextParser.from_string(text, Tokenizer('english'))\n\nprint('-----------------------------------------')\nprint('George W. Bush, 2012')\nprint('-----------------------------------------')\nsummaries_2005 = summarize_text(text)","4e42263b":"Now that we have extracted the top 20 most likely topics, we can apply them to each address separately.","c693f562":"his is a multi-faceted analysis of State of the Union addresses (1790-2018), primarilly including:\n  1. Data Preprocessing\n    1. Read\n    2. Annotate\n    3. Tokenize\n    4. One-Hot Encoding\n  2. Classification\n    1. Split\n    2. Padding\n    3. Word Embeddings\n    4. CNN\n  3. Topic Modeling\n    1. Lemmatization\n    2. Word Cloud\n    3. Latent Dirichlet Allocation\n  4. Automatic Summarization\n    * Lex Rank  \n    * Luhn Summarizer  \n    * LSA  \n    * Text Rank\n  5. Conclusion\n  \n  \n# Data Preprocessing\n\nI will be using SOTU addresses from 1790 to 2018. Trump, 2019 SOTU can be found [here](https:\/\/www.whitehouse.gov\/briefings-statements\/remarks-president-trump-state-union-address-2\/) (official White House transcript). However, I will not be using this one for this analysis.\n\n## Read","8ee11966":"## Latent Dirichlet Allocation","2dd2a792":"Attach political party to lsi_scores to see if there is any major differences between the political parties.","ef1f3811":"## Word Embeddings \n\nText is considered a form of sequence data similar to time series data that you would have in weather data or financial data. There are various ways to vectorize text, such as:  \n  * Words represented by each word as a vector\n  * Characters represented by each character as a vector\n  * N-grams of words\/characters represented as a vector (N-grams are overlapping groups of multiple succeeding words\/characters in the text)  \n  \nIn this project, I will represent words as vectors which is the common way to use text in neural networks. Two possible ways to represent a word as a vector are one-hot encoding and word embeddings. We saw earlier how to convert Presidents to integers and then to one-hot encodings. For our text, however,we are going to represents words as dense word vectors (also called word embeddings) which are trained unlike the one-hot encoding which are hardcoded. This means that the word embeddings collect more information into fewer dimensions. This technique essentially maps semantic meaning into a geometric space, where basic vector arithmetic can be applied.  \n\nNow we are ready to build a convnet to perform the classification.  \n\n## CNN","0e8be97e":"# Summarization  \n\nWho wants to sit down and read a SOTU transcript? How about 177? Let's just summarize them :)  \n\nWe only summarize a few here, but the same idea can be extended to summarize anything. I am going to use 4 common summarization techniques: \n\n  1. Lex Rank  \n  2. Luhn Summarizer  \n  3. LSA  \n  4. Text Rank","d7eb8669":"## Annotate\n\nThere are a few presidents that have the same last name (Roosevelt, Bush, Johnson and Adams), so let's clean that up now.","67ebc1de":"# Classification  \n\nFor this project I will be building a model to accurately classify political party. I manually curated this information to match up with the order the files were read in (in dirs variable).","2f36129a":"Let's summarize one of the most recent SOTU's, 2017 by Donald Trump using a few different summarization techniques.","4f26e429":"## Word Cloud","62dd5697":"## One-Hot Encoding  \n\nWe have converted our President labels into integers, but to use this in a convnet we need to create one-hot-encodings for each so we have a (num_sentences, num_presidents) target matrix.****","2b7591cd":"It should not be suprising to find that both parties tend to talk about the same topics, so using just this (topic) information will not have much explanatory power. ","7e13ffa1":"Now let's attach the political party to the president (note some presidents changed parties throughout their political career, I am simply listing the party they belonged to while serving as President).","68c764e2":"So 99% of all SOTU sentences are under 91 words, so lets pad every sentence to this length.","d6751693":"Some topics do not apply to certain time periods, for example only present prior to 1900. Therefore it might be of more interest to isolate certain time periods, say a 4 year window.   \n\nThe next step will be to restrict our addresses to, say, the past 20 years and then either a) use the above methodology, paying attention to the filtering parameters (no_above and no_below) or b) use a semi-supervised learning technique such as [CorEx](https:\/\/github.com\/gregversteeg\/corex_topic). The latter requires that we provide the algorithm with \u201canchor words\u201d that represented potential topics we think the model should attempt to find. We can also specify how much weight it should give to each of these anchors; if we aren't very certain about a particular topic we can assign it a low weight and the model may override this suggestion. ","3fbfd9a7":"**George W. Bush, 2005**","868ef037":"## Padding  \n\nNote that when using bag of words or tf-idf all the resulting vectors are of the same size: the size of the vocabulary but when using this representation we are not guaranteed to have equal sized vectors. In fact most sentences are of varying lengths, so we should pad all the sentences so that they are the same length.**","5f299795":"Computers are excellent when dealing with numbers, so we need to convert our Presidents into integers. ","9a55eb3b":"Note that the political parties are not well balanced, the Democrats and Republicans accounnt for 177 out of 227 addresses, while the other three parties made up only 22%. Therefore, it might be more interesting to predict the probability that a given text belongs to a particular political party, eg. 62% Democrat and 38% Republican.","c1841421":"## Tokenize  \nNow we break each address into is constituent sentences. ****","4e4534a1":"**Barack Obama, 2012**","4dc6250d":"# Classification\n\nNow we use the build in Tokenizer class of Keras to convert each sentence into a list of integers (corresponding to their index in the vocabulary of SOTU text). We wil use this (as well as the target matrix we made above) in order to build a classification model.\n\n## Split  \nFirst let us split our data.","665852b2":"# Topic Modeling  \n\nFor the topic modeling, I will use a slightly different tokenization technique by adding pat-of-speech tagging first and then lemmatizing.  \n\n## Lemmatization","bd476a50":"Now let's make one using our **tokens** (lemmatized and stemmed)."}}