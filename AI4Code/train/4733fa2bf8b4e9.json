{"cell_type":{"1c434668":"code","90e31929":"code","a9b74e33":"code","541807a9":"code","66c32be7":"code","9408aa0c":"code","3ed997c8":"code","189b8eec":"code","c119687a":"code","b21fa70a":"code","3af6b1d0":"code","853332a5":"code","2385b605":"code","a9f470dc":"code","cdabf3f3":"code","961f1e1b":"code","213c0098":"code","bff6bd99":"code","0449cbcc":"code","8e9d3901":"code","b2c08961":"code","93a040de":"code","a7aecb78":"code","fa87afa0":"code","b3494872":"markdown","af901e68":"markdown","1fce3078":"markdown","be14e44b":"markdown","62ff4ae7":"markdown","a050f5ef":"markdown","5e2319bb":"markdown","bf1d72d2":"markdown","3640fd91":"markdown","50fcec25":"markdown","4401073f":"markdown","d562b2cd":"markdown","48b25d28":"markdown","3e59b6a5":"markdown","b85db545":"markdown","5ad7783d":"markdown","27a034a4":"markdown","0b4eed73":"markdown","7af91a33":"markdown","c815c1d0":"markdown","0c90c3bc":"markdown","3985a04f":"markdown","bed64849":"markdown","5478b441":"markdown","740c8b25":"markdown"},"source":{"1c434668":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom keras import backend as K\n\nfrom keras.datasets import mnist\nfrom keras.layers import (Activation, BatchNormalization, Concatenate, Dense,\n                          Dropout, Flatten, Input, Lambda, Reshape)\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical","90e31929":"'''The Dimensions of the traget images'''\nimg_rows = 28\nimg_cols = 28\nchannels = 1\n\nimg_shape = (img_rows , img_cols , channels)\n\nz_dim = 100 #random noise input for generator \n\nnum_classes = 10 #no. of classes to predict","a9b74e33":"class Dataset:\n    def __init__(self, num_labeled):\n\n        self.num_labeled = num_labeled                               \n        \n        (self.x_train, self.y_train), (self.x_test,self.y_test) = mnist.load_data()\n\n        def preprocess_imgs(x):\n            x = (x.astype(np.float32) - 127.5) \/ 127.5                   \n            x = np.expand_dims(x, axis=3)                                \n            return x\n\n        def preprocess_labels(y):\n            return y.reshape(-1, 1)\n\n        self.x_train = preprocess_imgs(self.x_train)                     \n        self.y_train = preprocess_labels(self.y_train)\n\n        self.x_test = preprocess_imgs(self.x_test)                       \n        self.y_test = preprocess_labels(self.y_test)\n\n    def batch_labeled(self, batch_size):\n        idx = np.random.randint(0, self.num_labeled, batch_size)         \n        imgs = self.x_train[idx]\n        labels = self.y_train[idx]\n        return imgs, labels\n\n    def batch_unlabeled(self, batch_size):\n        idx = np.random.randint(self.num_labeled, self.x_train.shape[0], batch_size)\n        imgs = self.x_train[idx]\n        return imgs\n\n    def training_set(self):\n        x_train = self.x_train[range(self.num_labeled)]\n        y_train = self.y_train[range(self.num_labeled)]\n        return x_train, y_train\n\n    def test_set(self):\n        return self.x_test, self.y_test","541807a9":"num_labeled = 100\n\ndataset = Dataset(num_labeled)","66c32be7":"def build_generator(z_dim):\n    model = Sequential()\n    model.add(Dense(256 * 7 * 7, input_dim=z_dim))                           \n    model.add(Reshape((7, 7, 256)))\n    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n    model.add(BatchNormalization())                                       \n    model.add(LeakyReLU(alpha=0.01))                                        \n    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same')) \n    model.add(BatchNormalization())                                          \n    model.add(LeakyReLU(alpha=0.01))                                         \n    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))  \n    model.add(Activation('tanh'))                                            \n    return model","9408aa0c":"def build_discriminator_net(img_shape):\n    model = Sequential()\n    \n    model.add(Conv2D(32,\n                     kernel_size = 3,\n                     strides=3,\n                    input_shape=img_shape,\n                    padding='same'))\n    \n    model.add(LeakyReLU(alpha=0.01))\n    \n    model.add(Conv2D(64,\n                     kernel_size = 3,\n                     strides=3,\n                    input_shape=img_shape,\n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.01))\n    \n    model.add(Conv2D(128,\n                     kernel_size = 3,\n                     strides=3,\n                    input_shape=img_shape,\n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.01))\n    model.add(Dropout(0.5))\n    \n    model.add(Flatten())\n    model.add(Dense(num_classes))    \n    return model    ","3ed997c8":"def build_discriminator_supervised(discriminator_net):\n    model = Sequential()\n    model.add(discriminator_net)\n    model.add(Activation('softmax'))    \n    return model","189b8eec":"def build_discriminator_unsupervised(discriminator_net):\n    model = Sequential()\n    model.add(discriminator_net)\n    \n    def predict(x):\n        # Transform distribution over real classes into a binary real-vs-fake probability\n        prediction = 1.0 - (1.0 \/ (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n        return prediction\n\n    model.add(Lambda(predict))    \n    return model","c119687a":"def build_gan(generator, discriminator):\n    model = Sequential()\n    model.add(generator)                                                    \n    model.add(discriminator)\n    return model                 ","b21fa70a":"# These layers are shared during supervised and unsupervised training\ndiscriminator_net = build_discriminator_net(img_shape)\n\n# Build & compile the Discriminator for supervised training\ndiscriminator_supervised = build_discriminator_supervised(discriminator_net)\ndiscriminator_supervised.compile(loss='categorical_crossentropy',\n                                 metrics=['accuracy'],\n                                 optimizer=Adam())\n\n# Build & compile the Discriminator for unsupervised training\ndiscriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\ndiscriminator_unsupervised.compile(loss='binary_crossentropy',\n                                   optimizer=Adam())","3af6b1d0":"generator = build_generator(z_dim)\n\n# Keep Discriminator\u2019s parameters constant for Generator training\ndiscriminator_unsupervised.trainable = False\n\n# Build and compile GAN model with fixed Discriminator to train the Generator\n# Note that we are using the Discriminator version with unsupervised output\ngan = build_gan(generator, discriminator_unsupervised)\ngan.compile(loss='binary_crossentropy', optimizer=Adam())","853332a5":"supervised_losses = []\niteration_checkpoints = []\nimage_grid_rows = 2\nimage_grid_columns = 2\n\ndef display_fake_images(gen_imgs):\n    gen_imgs_scale = 0.5 * gen_imgs + 0.5\n    fig, axs = plt.subplots(image_grid_rows,\n                        image_grid_columns,\n                        figsize=(7, 7),\n                        sharey=True,\n                        sharex=True)\n    fig.suptitle('Fake Images', fontsize=20)\n\n    cnt = 0\n    plt.figure(figsize=(9,9))\n    for i in range(image_grid_rows):\n        for j in range(image_grid_columns):\n            axs[i, j].imshow(gen_imgs_scale[cnt, :, :, 0], cmap='Blues_r')\n            axs[i, j].axis('off')\n            cnt += 1            \n    plt.show()","2385b605":"def display_label_images(imgs):\n    imgs_scale = 0.5 * imgs + 0.5\n    fig, axs = plt.subplots(image_grid_rows,\n                        image_grid_columns,\n                        figsize=(7, 7),\n                        sharey=True,\n                        sharex=True)\n\n    fig.suptitle('Labeled Images', fontsize=20)\n    cnt = 0\n    plt.figure(figsize=(9,9))\n    for i in range(image_grid_rows):\n        for j in range(image_grid_columns):\n            axs[i, j].imshow(imgs_scale[cnt, :, :, 0], cmap='Blues_r')\n            axs[i, j].axis('off')\n            cnt += 1\n    plt.show()    ","a9f470dc":"def train(iterations, batch_size, sample_interval):\n\n    # Labels for real images: all 1\n    real = np.ones((batch_size, 1))\n\n    # Labels for fake images: all 0\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n\n        # Take labeled samples.\n        imgs, labels = dataset.batch_labeled(batch_size)\n\n        # Encode One-hot encoder labels.\n        labels = to_categorical(labels, num_classes=num_classes)\n\n        # Take unlabeled samples.\n        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n\n        # Creates a batch of fake images.\n        z = np.random.normal(0, 1, (batch_size, z_dim))\n        gen_imgs = generator.predict(z)\n\n        # Train on labeled real samples.\n        d_loss_supervised, accuracy = discriminator_supervised.train_on_batch(imgs, labels)\n\n        # train on real, unlabeled samples.\n        d_loss_real = discriminator_unsupervised.train_on_batch(\n            imgs_unlabeled, real)\n\n        # Train on fake samples.\n        d_loss_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n\n        d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # Creates a batch of fake images.\n        z = np.random.normal(0, 1, (batch_size, z_dim))\n        gen_imgs = generator.predict(z)\n\n        # Train the creator.\n        g_loss = gan.train_on_batch(z, np.ones((batch_size, 1)))\n\n        if (iteration + 1) % sample_interval == 0:\n            display_fake_images(gen_imgs)\n            display_label_images(imgs)\n            # Record the supervised classification loss of the discriminator to plot the graph after training.\n            supervised_losses.append(d_loss_supervised)\n            iteration_checkpoints.append(iteration + 1)\n\n            # Print the training process.\n            print(\n                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f] [G loss: %f]\"\n                % (iteration + 1, d_loss_supervised, 100 * accuracy,\n                   d_loss_unsupervised, g_loss))","cdabf3f3":"iterations = 8000\nbatch_size = 32\nsample_interval = 800\n\n# Train the SGAN for the specified number of iterations\ntrain(iterations, batch_size, sample_interval)","961f1e1b":"losses = np.array(supervised_losses)\n\nplt.figure(figsize=(15, 5))\nplt.plot(iteration_checkpoints, losses, label=\"Discriminator loss\")\nplt.xticks(iteration_checkpoints, rotation=90)\nplt.title(\"Discriminator \u2013 Supervised Loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","213c0098":"x, y = dataset.training_set()\ny = to_categorical(y, num_classes=num_classes)\n\n_, accuracy = discriminator_supervised.evaluate(x, y)\nprint(\"Training Accuracy: %.2f%%\" % (100 * accuracy))","bff6bd99":"x, y = dataset.test_set()\ny = to_categorical(y, num_classes=num_classes)\n\n_, accuracy = discriminator_supervised.evaluate(x, y)\nprint(\"Test Accuracy: %.2f%%\" % (100 * accuracy))","0449cbcc":"# Supervised learning classifier with network structure like SGAN discriminator\nmnist_classifier = build_discriminator_supervised(build_discriminator_net(img_shape))\nmnist_classifier.compile(loss='categorical_crossentropy',\n                         metrics=['accuracy'],\n                         optimizer=Adam())","8e9d3901":"imgs, labels = dataset.training_set()\n\nlabels = to_categorical(labels, num_classes=num_classes)\n\ntraining = mnist_classifier.fit(x=imgs,\n                                y=labels,\n                                batch_size=32,\n                                epochs=30,\n                                verbose=1)\nlosses = training.history['loss']\naccuracies = training.history['accuracy']","b2c08961":"plt.figure(figsize=(10, 5))\nplt.plot(np.array(losses), label=\"Loss\")\nplt.title(\"Classification Loss\")\nplt.legend()\nplt.show()","93a040de":"plt.figure(figsize=(10, 5))\nplt.plot(np.array(accuracies), label=\"Accuracy\")\nplt.title(\"Classification Accuracy\")\nplt.legend()\nplt.show()","a7aecb78":"x, y = dataset.training_set()\ny = to_categorical(y, num_classes=num_classes)\n\n_, accuracy = mnist_classifier.evaluate(x, y)\nprint(\"Training Accuracy: %.2f%%\" % (100 * accuracy))","fa87afa0":"x, y = dataset.test_set()\ny = to_categorical(y, num_classes=num_classes)\n\n_, accuracy = mnist_classifier.evaluate(x, y)\nprint(\"Test Accuracy: %.2f%%\" % (100 * accuracy))","b3494872":"## Plotting Classification Accuracy.","af901e68":"## Calculating of classification accuracy on the test set","1fce3078":"## Supervised learning discriminator","be14e44b":"## Calculating of classification accuracy on the test set","62ff4ae7":"----------------------------------\n# Generator","a050f5ef":"<hr style=\"border: solid 3px blue;\">\n\n# Conclusion\n\nWe used labels for only 100 data in the MNIST dataset, and the rest did not use labels.\nWe can think of this as a situation where we collected 50000 data, but only 100 became labels.\n\nIn this situation, the method we can choose is the SGAN model to train the discriminator in a semi-supervised learning method.\n\nThe performance of the testset method is summarized as follows.\n* Semi-supervised learning method learning model accuracy: 69.95%\n* The model's accuracy of learning in the supervised learning method: 57.83%\n\nIn the semi-supervised learning method, the accuracy of the learning model is about 12.12% better.","5e2319bb":"## Calculating of classification accuracy on the training set","bf1d72d2":"-----------------------------------------------\n# Discriminator\n\n![](https:\/\/i.gifer.com\/origin\/a4\/a4f7d2ea5f8cdce902f9a510dbe1a141.gif)\n\nPicture Credit: https:\/\/i.gifer.com\n\nFor SGAN models, the discriminator is very busy. This is because supervised learning and unsupervised learning must be performed at the same time.\n\nHowever, during this process, the discriminator learns how to learn effectively when the labeled data is small.","3640fd91":"Wow! Our model completely memorized 100 labeled data.","50fcec25":"## Calculating of classification accuracy on the training set","4401073f":"The figures above shows labeled images and fake images that are input to a specific iteration. The discriminator has the ability to distinguish the real from the fake through the above inputs and the ability to classify the real ones by class.\nThrough this learning, training is effectively performed using a labeled dataset with a small number of SGAN models.","d562b2cd":"----------------------------------------------\n# Setting up","48b25d28":"GANs can be broadly divided into generators and discriminators. In general, GANs focus on the training of the generator to make the fakes look real, and then use the generators after training.\nHowever, in SGAN, learning proceeds in the direction of improving the performance of the discriminator, and the discriminator is used after training.\n\n**In this notebook, we would like to organize them in the following order.**\n1. After modeling and training for SGAN, check the result.\n2. After training the same CNN model through supervised learning, check the result.\n3. By comparing the performance of the above two models, it is confirmed that SGAN is effective in a dataset that does not have much labeling.","3e59b6a5":"As shown in the figure above, the discriminator uses three types of images.\n* Labeled images\n* Unlabeled images\n* Fake images\n\nIn SGAN, the discriminator is the most complex. The purpose of the discriminator has two goals:\n* Distinguish between real and fake. For this, the discriminator outputs the probability for binary classification using the sigmoid function.\n* If it is a real sample, it correctly classifies the label. For this, the discriminator uses the softmax function to output probabilities, one for each target class.","b85db545":"<hr style=\"border: solid 3px blue;\">\n\n# Comparing with Supervised learning classifier\n\nTo check the performance of the SGAN model, after training with the existing supervised learning model, check the results with the test set. Through this, we check which model is more effective in the case of a small dataset.","5ad7783d":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https:\/\/miro.medium.com\/max\/1313\/1*MIJZKbsCLYuNp5yV26OH8g.gif)\n\nPicture Credit: https:\/\/miro.medium.com\n\n**Semi-supervised learning**\n\n> Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision.\n> \n> Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Semi-supervised_learning\n\n\n**Labeled data is expensive!**\n In particular, collecting and labeling datasets is a very laborious and difficult task.\nHowever, in order for models to perform supervised learning well, a labeled big dataset is required.\n\nIn this notebook, when the labeled dataset is small, one of the ways to overcome this is the Semi-Supervised GAN (SGAN).","27a034a4":"![](https:\/\/drek4537l1klr.cloudfront.net\/langr\/Figures\/07fig03_alt.jpg)\n\nPicture Credit: https:\/\/drek4537l1klr.cloudfront.net","0b4eed73":"----------------------------------------------------------\n# Training\n\n![](https:\/\/drek4537l1klr.cloudfront.net\/langr\/Figures\/07fig02_alt.jpg)\n\nPicture Credit: https:\/\/drek4537l1klr.cloudfront.net","7af91a33":"--------------------------------------------------------------------\n## Building Generator Network","c815c1d0":"For SGAN, it is assumed that only 100 images are labeled in the MNIST traing dataset. \n\nAssume that the remaining 49900 labels are not set.","0c90c3bc":"## Unsupervised learning discriminator","3985a04f":"## Plotting Classification Loss","bed64849":"## Plotting the supervised learning loss of the discriminator.","5478b441":"----------------------------------------------------------------------------------------------\n## Building Discriminator Network","740c8b25":"The size of the random noise vector to be used as input for the generator is 100 dimensions, and the number of labels to predict is 10."}}