{"cell_type":{"a32a931b":"code","a41f6156":"code","0ca8555c":"code","7484de76":"code","3a70c8a9":"code","248f582a":"code","2601d951":"code","792b2f38":"code","1f5fe4e7":"code","8c035986":"code","82d16e9c":"code","1259ae69":"code","d8f73538":"code","85234275":"code","2e02075b":"code","d66b61fa":"code","2b1a5e14":"code","466db635":"code","ab6c9e0e":"code","cc1e49d9":"code","f8f5b75f":"code","bb215727":"code","ad928a10":"code","b05b2061":"code","bc6c599b":"code","7d5feb2d":"code","d5465e87":"code","75f7954a":"code","db980834":"code","f366b0ad":"code","24344ae0":"code","4941088b":"code","b35ed0bd":"code","8abdf712":"code","209f326b":"code","fc3bcf30":"code","8ef276d0":"code","3ec8a5ce":"code","5cab4680":"code","59e2dedf":"code","c9b4f202":"code","76055c98":"code","40605f84":"code","e24e3f19":"code","e0f68f15":"code","11cd2dbf":"code","d60f6e35":"code","3d541016":"code","0a5c5a4b":"code","f8b226ea":"code","5a33e77e":"code","885798fe":"code","1a5158c9":"markdown","bf495a84":"markdown","9e49194c":"markdown","c23b5d35":"markdown","a375421d":"markdown","130af6a2":"markdown","1c6cbef6":"markdown","c346d2a9":"markdown","24930023":"markdown","8b6d7598":"markdown","2feab294":"markdown","5f4a6e58":"markdown"},"source":{"a32a931b":"# import libraries\nimport numpy as np \nimport pandas as pd \nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('darkgrid')\n\nfrom fastai.tabular import *\npd.set_option('display.float_format', lambda x: '{:.2f}'.format(x)) #Limiting floats output to 2 decimal points","a41f6156":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","0ca8555c":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","7484de76":"sns.distplot(train['SalePrice']) ;\n\n# Get the fitted parameters \n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","3a70c8a9":"# log-transform the price\ntrain['SalePrice'] = train['SalePrice'].apply(np.log)\n\nsns.distplot(train['SalePrice'])\nplt.show();","248f582a":"# lets remove houses with living area <4000 as recommended by the author of the dataset\ntrain = train[train['GrLivArea']<4000]","2601d951":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values.copy()\nall_data = pd.concat((train, test), sort = 'True').reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","792b2f38":"missing_data = (all_data.isnull().sum() \/ len(all_data)).sort_values(ascending = False)*100\n\nmissing_data = pd.DataFrame(missing_data)\n\nplt.figure(figsize = (10,7))\nmissing_data.head(20).plot(kind = 'bar')\nplt.title('Percent of missing data');","1f5fe4e7":"# we could drop the first four columns as they miss too much data\ndroplist = list(missing_data.head(4).index)","8c035986":"all_data.drop(droplist, axis = 1, inplace = True)","82d16e9c":"col = train.corr().nlargest(10, 'SalePrice')['SalePrice'].index\ncorr_matrix = np.corrcoef(train[col].values.T)","1259ae69":"plt.figure(figsize = (10,8))\nsns.heatmap(corr_matrix, cmap = 'coolwarm', annot = True, xticklabels= col.values, yticklabels= col.values);","d8f73538":"#These are the candidates to drop if there are too little distinct values in the columns\ntrain.nunique().sort_values(ascending = False).tail(7)","85234275":"train.Utilities.value_counts()","2e02075b":"train.CentralAir.value_counts()","d66b61fa":"train.Street.value_counts()","2b1a5e14":"train.Alley.value_counts()","466db635":"#we can drop Street and Utilities as they contain no unique useful information\nall_data.drop(['Street', 'Utilities'], axis = 1, inplace = True)","ab6c9e0e":"#FireplaceQu : data description says NA means \"no fireplace\"\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","cc1e49d9":"\"\"\"\nLotFrontage : Since the area of each street connected to the house property most likely have a similar area\nto other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n\"\"\"\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","f8f5b75f":"# GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","bb215727":"# GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","ad928a10":"# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","b05b2061":"# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","bc6c599b":"# MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","7d5feb2d":"# MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","d5465e87":"# Functional : data description says NA means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","75f7954a":"# Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","db980834":"# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","f366b0ad":"# Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","24344ae0":"# SaleType : Fill in again with most frequent which is \"WD\"\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","4941088b":"# MSSubClass : Na most likely means No building class. We can replace missing values with None\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","b35ed0bd":"missing_data = (all_data.isnull().sum() \/ len(all_data)).sort_values(ascending = False)*100\n\nmissing_data = pd.DataFrame(missing_data)\nmissing_data.head(1)","8abdf712":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","209f326b":"all_data.head()","fc3bcf30":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","8ef276d0":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","3ec8a5ce":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","5cab4680":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","59e2dedf":"dep_var = 'SalePrice'","c9b4f202":"cat_names = list(train.select_dtypes(include = ['object', 'bool']).columns)","76055c98":"cont_names = list(train.select_dtypes(exclude = ['object', 'bool']).columns)","40605f84":"# Add back sale prices\ntrain['SalePrice'] = y_train","e24e3f19":"# defining steps to process the input data\nprocs = [FillMissing, Categorify, Normalize]\n\n# Test tabularlist\ntest = TabularList.from_df(test, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\n# Train data bunch - important to define label_cls = FloatList to ensure the model works with scalar values\ndata = (TabularList.from_df(train, path='.', cat_names=cat_names, cont_names=cont_names, procs=procs)\n                        .split_by_rand_pct(valid_pct = 0.2)\n                        .label_from_df(cols = dep_var, label_cls = FloatList, log = False )\n                        .add_test(test)\n                        .databunch( bs = 64))","e0f68f15":"data.show_batch(rows=5, ds_type=DatasetType.Valid)","11cd2dbf":"max_log_y = (np.max(train['SalePrice'])*1.2)\ny_range = torch.tensor([0, max_log_y], device=defaults.device)","d60f6e35":"# create the model\nlearn = tabular_learner(data, layers=[600,300], ps=[0.001,0.01], y_range = y_range, emb_drop=0.04, metrics=exp_rmspe)\n\nlearn.model\n\n# select the appropriate learning rate\nlearn.lr_find()\n\n# we typically find the point where the slope is steepest\nlearn.recorder.plot()","3d541016":"# Fit the model based on selected learning rate\nlearn.fit_one_cycle(50, max_lr = 1e-02)","0a5c5a4b":"#Plotting The losses for training and validation\nlearn.recorder.plot_losses(skip_start = 500)","f8b226ea":"learn.fit_one_cycle(5, 3e-4)","5a33e77e":"# get predictions\npreds, targets = learn.get_preds(DatasetType.Test)\nlabels = [np.exp(p[0].data.item()) for p in preds]\n\n# create submission file to submit in Kaggle competition\nsubmission = pd.DataFrame({'Id': test_ID, 'SalePrice': labels})\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.describe()","885798fe":"submission.head()","1a5158c9":"**House Prices with FASTAI**","bf495a84":"**Visualizing correlations in the dataset**","9e49194c":"**Checking the target**","c23b5d35":"#### Skewed features","a375421d":"2.6 Transforming numerical features into categorical","130af6a2":"Checking for missing data again","1c6cbef6":"2.4 Deadling with columns with little additional information","c346d2a9":"**Deadling with missing data**","24930023":"Correcting skew by Box Cox transformation","8b6d7598":"**FASTAI Modelling**","2feab294":"**Split of the tests**","5f4a6e58":"**Data analysis and feature engineering**"}}