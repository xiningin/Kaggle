{"cell_type":{"96464e42":"code","e9bb5ef6":"code","b6c049a8":"code","11eef168":"code","d3db74b2":"code","991a2c46":"code","0707db62":"code","7db018d1":"code","3c90c0d4":"markdown","b9df71f2":"markdown","ff3064f4":"markdown","1c15b491":"markdown","3ff37b6b":"markdown","39f07d8e":"markdown","ecb9bdaf":"markdown","5e608687":"markdown"},"source":{"96464e42":"import pandas as pd\nimport numpy as np\nimport optuna\n\nfrom sklearn import compose\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import pipeline\nfrom sklearn import preprocessing\n\nimport xgboost as xgb\nimport catboost as cat\n\n\n# This is nice handy constant to turn on and off the GPU. When `False`\n# the notebook will ignore the GPU even when present.\nGPU_ENABLED = True","e9bb5ef6":"train = pd.read_csv(\"train.csv\").sample(frac=0.10, random_state=42)\n\ncont_features = [f for f in train.columns.tolist() if f.startswith('cont')]\ncat_features = [f for f in train.columns.tolist() if f.startswith('cat')]\n\ny = train.target\nX = train","b6c049a8":"numerical_preprocessor = pipeline.Pipeline(steps=[\n    (\"imputer\", impute.SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", preprocessing.MinMaxScaler())\n])\n\ncategorical_preprocessor = pipeline.Pipeline(steps=[\n    (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n    (\"ordinal\", preprocessing.OrdinalEncoder())\n])\n\npreprocessor = compose.ColumnTransformer(\n    transformers=[\n        (\"numerical_preprocessor\", numerical_preprocessor, cont_features),\n        (\"categorical_preprocessor\", categorical_preprocessor, cat_features)\n    ]\n)","11eef168":"def train_model_for_study(X, y, model):\n    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n        X, \n        y, \n        test_size=0.20, \n        random_state=42\n    )\n\n    X_train = preprocessor.fit_transform(X_train, y_train)\n    X_valid = preprocessor.transform(X_valid)\n\n    model.fit(\n        X_train, \n        y_train,\n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)], \n        verbose=False\n    )\n\n    yhat = model.predict(X_valid)\n    return metrics.mean_squared_error(y_valid, yhat, squared=False)","d3db74b2":"def objective_xgb(trial):\n    \"\"\"\n    Objective function to tune an `XGBRegressor` model.\n    \"\"\"\n\n    params = {\n        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1.0, log=True),\n        'max_depth': trial.suggest_int(\"max_depth\", 2, 9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n    }\n\n    if GPU_ENABLED:\n        params[\"tree_method\"] = \"gpu_hist\"\n        params[\"predictor\"] = \"gpu_predictor\"\n\n    model = xgb.XGBRegressor(\n        booster=\"gbtree\",\n        objective=\"reg:squarederror\",\n        random_state=42,\n        **params\n    )\n\n    return train_model_for_study(X, y, model)","991a2c46":"def objective_cat(trial):\n    \"\"\"\n    Objective function to tune a `CatBoostRegressor` model.\n    \"\"\"\n\n    params = {\n        'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n        'od_wait':trial.suggest_int('od_wait', 500, 2300),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.01, 1),\n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n        'subsample': trial.suggest_uniform('subsample',0,1),\n        'random_strength': trial.suggest_uniform('random_strength',10,50),\n        'depth': trial.suggest_int('depth',1, 15),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n    }\n\n    if GPU_ENABLED:\n        params[\"task_type\"] = \"GPU\"\n        params[\"bootstrap_type\"] = \"Poisson\"\n\n    model = cat.CatBoostRegressor(\n        loss_function=\"RMSE\",\n        random_state=42,\n        **params,\n    )\n    \n    return train_model_for_study(X, y, model)","0707db62":"study_xgb = optuna.create_study(direction=\"minimize\")\nstudy_xgb.optimize(objective_xgb, n_trials=5)\nstudy_xgb.best_params","7db018d1":"study_cat = optuna.create_study(direction=\"minimize\")\nstudy_cat.optimize(objective_cat, n_trials=5)\nstudy_cat.best_params","3c90c0d4":"We can now run the study for the `XGBRegressor` model and display the best set of hyperparameters when it finishes. Here, I'm running 5 trials only, which is not enough to find a good set of hyperparameters. During the competition, I ended up running 300 trials.","b9df71f2":"This runs the study for the `CatBoostRegressor` model and displays the best set of hyperparameters when it finishes. Here, I'm running 5 trials only, which is not enough to find a good set of hyperparameters. During the competition, I ended up running 300 trials.","ff3064f4":"This is an utility function to avoid duplicating the code when exploring different models. I'll use this function from each one of the objective functions defined later in this notebook.","1c15b491":"This is an example on how to setup Optuna to tune hyperparameters for two different models. This example is part of [My First Kaggle Competition](https:\/\/www.kaggle.com\/santiagovaldarrama\/30-days-of-ml-stacked-ensembles).\n\n<img src='https:\/\/images.unsplash.com\/photo-1554696468-19f8c7a71ad5' alt='Hyperparameter Tuning'>","3ff37b6b":"This is the objective function to tune an `XGBRegressor` model. Notice how this function uses the `train_model_for_study()` function that we defined before.","39f07d8e":"And this is the objective function to tune a `CatBoostRegressor` model. Notice how this function also uses the `train_model_for_study()` function that we defined before.","ecb9bdaf":"There's a lot of data, so I'm going to load 5% only to illustrate how the notebook works without having to wait an eternity for it.","5e608687":"Let's define the preprocessing transformations that I will use with the original data.\n\nNotice that there are only two different transformations that I will be using:\n\n1. Scaling values using a Min-Max Scaler.\n2. Transforming categorical columns to ordinal values."}}