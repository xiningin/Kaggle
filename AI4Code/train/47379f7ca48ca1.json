{"cell_type":{"e8f16041":"code","5f639ec7":"code","6c818c16":"code","38850988":"code","7e0be018":"code","458f19bb":"code","224f609e":"code","cf615d5d":"markdown","5a048179":"markdown","afd38979":"markdown","2966f838":"markdown","f1951553":"markdown","fe35f3b3":"markdown","9f70aa09":"markdown"},"source":{"e8f16041":"import urllib.request as scrape\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport math\nimport os\nimport string\nimport logging\nimport numpy as np\nimport torch.tensor\nimport random\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport urllib.request as scrape\nimport torch\nimport torch.nn as nn\n\n\n#print(os.listdir('..\/input'))\nSvenska_namn = pd.read_csv('..\/input\/Svenska namn.csv') #Data is scraped from https:\/\/svenskanamn.alltforforaldrar.se\/populart\/\n","5f639ec7":"\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef getTableFromSite(url):\n    \"\"\"\n    :param url: url f\u00f6r det man vill plocka ut. Endast f\u00f6r 'https:\/\/svenskanamn.alltforforaldrar.se\/populart\/'\n    :return: dataframe of all names\n    \"\"\"\n\n    response = scrape.urlopen(url)\n    html_data = response.read()\n    response.close()\n    soup = BeautifulSoup(html_data)\n    table = soup.find_all(attrs={'class': 'table_td'})\n    names = []\n    values = []\n    for i in range(len(table)):\n        hh = table[i].get_text()\n        if i % 2 == 0:\n            names.append(hh.strip())\n        else:\n            values.append(hh.strip())\n\n    data = pd.DataFrame({'namn': names, 'antal': values})\n    return data\n\n\ndef train_validate_test_split(df, train_percent=0.6, validate_percent=0.2, seed=666):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df.index)\n    #print('We found %s lines of data' % m)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.iloc[perm[:train_end]]\n    validate = df.iloc[perm[train_end:validate_end]]\n    test = df.iloc[perm[validate_end:]]\n    return train, test, validate\n\n\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\n\ndef letter2index(letter):\n    return character_list.find(letter)\n\n\ndef indexesFromWord(word):\n    return [letter2index(letter) for letter in word]\n\n\ndef tensorFromWord(word):\n    indexes = indexesFromWord(word)\n    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n\n\ndef categoryFromOutput(output):\n    top_n, top_i = output.topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\n\n\n\n\n\n\n\ndef makeBatch_new(batch_size, maxchar, data = None, character_list = None):\n    \"\"\"\n    :param batch_size: nr of examples in bach\n    :param maxchar: size of longest name in vocabulary\n    :return: a padded tensor of size [hh*batch_size*L]\n    \"\"\"\n    inds = random.sample(list(data.index), batch_size)\n    seqs = list(data['namn'][inds])\n    vectorized_seqs = [[character_list.index(tok) for tok in seq] for seq in seqs]\n    seq_lengths = [len(x) for x in seqs]\n    seq_tensor = Variable(torch.zeros((len(vectorized_seqs), maxchar))).long()\n\n    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n\n    sorting = sorted([(e, i) for i, e in enumerate(seq_lengths)], reverse=True)\n    perm_idx = [x[1] for x in sorting]\n    seq_lengths = [x[0] for x in sorting]\n    seq_tensor = seq_tensor[perm_idx]\n    seq_tensor = seq_tensor.permute(1,0)  #batchFirst\n\n    labels = list(data['gender'][inds])\n    label_tensor = torch.LongTensor(batch_size)\n\n    for ii, gender in enumerate(labels):\n        if gender == 'flicknamn':\n            label_tensor[ii] = 1\n        else:\n            label_tensor[ii] = 0\n\n\n    label_tensor = label_tensor[perm_idx]\n    return seq_tensor, seq_lengths, label_tensor\n\n\n\n\n\n\n","6c818c16":"\nclass Params():\n    def __init__(self):\n        self.batch_size = 32\n        self.hidden_size = 12\n        self.embedding_size = 24\n        self.epochs = 50\n        self.nr_classes =2\n        self.gpu = False\n        self.learning_rate = 0.001\n        self.train_ratio = 0.82\n        self.val_ratio = 0.1\n        self.test_ratio = 1-self.val_ratio-self.train_ratio\n        self.n_layers = 2\n        self.dropout = 0.3\n\n\nArgs = Params()","38850988":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nclass LSTM_classifier(nn.Module):\n    def __init__(self, vocab_size, hidden_size, n_layers, dropout, embed_size):\n        super().__init__()\n\n        self.input_size = vocab_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.embed_size = embed_size\n        self.embedding = nn.Embedding(self.input_size, self.embed_size)\n        self.rnn = nn.LSTM(input_size=self.embed_size,\n                           hidden_size=hidden_size,\n                           dropout=dropout,\n                           num_layers=n_layers, bidirectional=True)\n        self.hidden2label = nn.Linear(2*hidden_size, 2)\n        #self.hidden = self.init_hidden()\n        self.softmax = nn.LogSoftmax(dim=1)\n        self.dropoutLayer = nn.Dropout()\n\n    def init_hidden(self, batch_size):\n        h0 = Variable(torch.zeros(self.n_layers*2, batch_size, self.hidden_size))\n        c0 = Variable(torch.zeros(self.n_layers*2, batch_size, self.hidden_size))\n        return h0, c0\n\n    def forward(self, inputs, input_lengths):  # inputs: antal i batch x max ord l\u00e4ngd\n        self.hidden = self.init_hidden(inputs.size(-1)) # -1 if batch_first=False\n        embedded = self.embedding(inputs)  # antal i batch * max ord l\u00e4ngd * Embedding_dim\n        packed = pack_padded_sequence(embedded, input_lengths, batch_first=False)  # packad\n        outputs, self.hidden = self.rnn(packed, self.hidden)  #(packed, self.hidden)  #\n        output, output_lengths = pad_packed_sequence(outputs, batch_first=False)\n        #print(output.size())\n        output = torch.transpose(output, 0, 1)\n        #print(output.size())\n        output = torch.transpose(output, 1, 2)\n        #print(output.size())\n        output = torch.tanh(output)\n        #print(output.size())\n        output, indices = F.max_pool1d(output,output.size(2), return_indices=True)\n        #print(output.size())\n        output = torch.tanh(output)\n        output = output.squeeze(2)\n        output = self.dropoutLayer(output)\n        #print(output.size())\n        output = self.hidden2label(output)\n        #print(output.size())\n        output = self.softmax(output)\n        return output, self.hidden\n\n\n\n\n","7e0be018":"\nletters = 'abcdefghijklmnopqrstxyz'\nmax_pages = 3\ntest_url = 'https:\/\/svenskanamn.alltforforaldrar.se\/populart\/flicknamn\/h\/page:2'\nbase_url = 'https:\/\/svenskanamn.alltforforaldrar.se\/populart\/'\ngenders = ['flicknamn', 'pojknamn']\npages = ['1', '2', '3']\n\nif 'Svenska namn.csv' in os.listdir('..\/input'):\n    Svenska_namn = pd.read_csv('..\/input\/Svenska namn.csv')\nelse:\n    Svenska_namn = pd.DataFrame()\n    for i, letter in enumerate(letters):\n        for j, gender in enumerate(genders):\n            for k, page in enumerate(pages):\n                url = base_url + gender + '\/' + letter + '\/page:' + page\n                df = getTableFromSite(url)\n                df['gender'] = gender\n                Svenska_namn = Svenska_namn.append(df, ignore_index=True)\n                print(url,'total_rows: ', Svenska_namn.shape[0],)\n                time.sleep(1)\n\n    Svenska_namn.to_csv('Svenska namn.csv')\n    \n\n#build a sorted list of possible characters\nSvenska_namn['namn'] = Svenska_namn['namn'].str.lower()\nlista = list(Svenska_namn['namn'])\ncharlist = \"\u00e5\u00e4\u00f6\u00c5\u00c4\u00d6 -\"\nfor ii in range(4140):\n    charlist = charlist + lista[ii]\n\nlista = ''.join(set(charlist))\nlista = ''.join(sorted(lista))\ncharacter_list = sorted(set(charlist))\ncategories = ['pojknamn', 'flicknamn']\nn_letters = len(character_list)\nn_categories = len(categories)\n\n#Split the data \ntrain_data, val_data, test_data = train_validate_test_split(Svenska_namn,seed=333)\n\nmodel = LSTM_classifier(vocab_size = n_letters, \n                        hidden_size = Args.hidden_size, \n                        n_layers = Args.n_layers, \n                        dropout = Args.dropout,\n                        embed_size = Args.embedding_size)\n\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\n\ndef train(input_tensor, input_sizes, target_tensor):\n    model.hidden = model.init_hidden(Args.batch_size) \n    model.zero_grad()\n    output, hidden = model.forward(input_tensor, input_sizes)\n    loss = criterion(output, target_tensor)\n    loss.backward()\n    optimizer.step()\n    return output, loss.item()\n\n\n","458f19bb":"\n#Training\nimport time\n\n\ncurrent_loss = 0\n\nall_losses = []\nval_acc = []\ntrain_acc = []\npredictions = []\nlabels = []\nprint_every = int(len(train_data)\/Args.batch_size) #every epoch\nn_iters  = int(len(train_data)\/Args.batch_size*Args.epochs)\nplot_every = print_every\nstart = time.time()\n\n\nfor iter in range(n_iters):\n    input_tensor, input_lengths, target_tensor = makeBatch_new(batch_size=Args.batch_size, \n                                                               maxchar= 14, \n                                                               data = train_data, \n                                                               character_list = character_list)\n    output, loss = train(input_tensor=input_tensor, \n                         input_sizes=input_lengths, \n                         target_tensor=target_tensor)\n    current_loss += loss\n    predictions.extend(list(torch.argmax(output, dim=1).numpy()))\n    labels.extend(list(target_tensor.numpy()))\n    # Print iter number, loss, name and guess\n    if (iter) % print_every == 0:\n        corrects = 0\n        corrects_train = 0\n        input_tensor, input_lengths, target_tensor = makeBatch_new(batch_size=len(val_data), \n                                                                   maxchar= 14, \n                                                                   data =val_data, \n                                                                   character_list = character_list)\n        \n        output, hidden = model(inputs = input_tensor, input_lengths = input_lengths)\n        preds = torch.argmax(output, dim=1)\n        for i in range(len(preds)):\n            if preds[i] == target_tensor[i]:\n                corrects += 1\n        \n        for i in range(len(labels)):\n            if labels[i] == predictions[i]:\n                corrects_train +=1\n        \n        \n        accuracy = corrects\/len(preds)\n        train_accuracy = corrects_train\/len(labels)\n        \n        val_acc.append(accuracy)\n        train_acc.append(train_accuracy)\n        labels = []\n        predictions =[]\n        print('Val acc after {} epochs is: {} - train acc: {} - time elapsed: {} - avg train loss: {}'.format(int(iter\/print_every+1), round(accuracy, 2),round(train_accuracy,2), timeSince(start), round(loss, 2)))\n\n        \n    if iter % plot_every == 0:\n        all_losses.append(current_loss \/ plot_every)\n        current_loss = 0 ","224f609e":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nplt.figure()\nplt.plot(all_losses[1:])\nplt.plot(val_acc[:])\nplt.plot(train_acc[:])\nplt.legend(['Average loss per epoch','validation accuracy','train accuracy'])","cf615d5d":"**And then, some arguments**\n","5a048179":"**The start of the code itself, along with initalization of the model**\n","afd38979":"**Common swedish name classification**","2966f838":"**Model**","f1951553":"This code is the implementation of a recurrent neural net in pytorch. The implementation is for classifying common swedish names into gender categories. It is a character level rnn, ant the net iteself is a bi-directional 2-layer lstm. It also has batch-feeding in the training part, whih makes it faster. Variable batch size training makes it versatile as well. ","fe35f3b3":"**Begin training **","9f70aa09":"**First, a bunch of functions**"}}