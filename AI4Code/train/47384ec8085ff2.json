{"cell_type":{"1dba0278":"code","ffef178e":"code","7f0c2a9f":"code","4ba70617":"code","39b97c56":"code","cc0ca41d":"code","5d4f5546":"code","c24b8a6c":"code","4c491113":"code","bd920711":"code","2547bafe":"code","11989218":"code","242629d7":"code","0bc63bde":"code","d63b4078":"code","127cd284":"code","fdbdd332":"code","2def59d4":"code","619c18b6":"code","c1ca6e81":"code","4412805e":"code","43069df6":"code","4b8dafbd":"code","00098eea":"code","bf0b2aa1":"code","3ab53fae":"code","0a4660d7":"code","478df8d8":"code","eb3469a8":"code","c5cf1634":"code","dc3456fb":"code","3f7dc780":"code","02823bb7":"markdown","214c707a":"markdown","951c4fbe":"markdown","341a8e83":"markdown","21622219":"markdown","384ef88a":"markdown","f2a38732":"markdown","dd79ba38":"markdown","17897209":"markdown","4fb529be":"markdown","0f6546cd":"markdown","b22036c4":"markdown","2bbc9b31":"markdown","d79b43e2":"markdown","4a830f36":"markdown","af0ec11d":"markdown","4f5823f6":"markdown","e5cd65e5":"markdown","ebce2ed1":"markdown","e8b55ba8":"markdown","e13b8301":"markdown","59bb17dd":"markdown","faf406c7":"markdown","bf355fa9":"markdown","5b88d2f3":"markdown","8dc7e9d8":"markdown","272e03b9":"markdown","85e14281":"markdown","fc70ba3d":"markdown","7ab2e7d1":"markdown","0bf1babf":"markdown","6f83e6de":"markdown","7a36656e":"markdown","e102ca25":"markdown","3c6b8d7a":"markdown"},"source":{"1dba0278":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score","ffef178e":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f0c2a9f":"sns.set_style('darkgrid')\ndf = pd.read_csv('\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndf.head()","4ba70617":"df.shape","39b97c56":"df.info()","cc0ca41d":"df.isnull().sum()","5d4f5546":"cat_features = ['gender', 'race\/ethnicity', 'parental level of education', 'lunch',\n       'test preparation course']\n\ncount = np.array([df[feature].unique().size for feature in cat_features])\n\nto_sort = np.argsort(count)[::-1]\ncat_features = np.array(cat_features)[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\ngraph = sns.barplot(cat_features,count)\nfor p in graph.patches:\n    graph.annotate(p.get_height(), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')\n\n\nplt.title(\"Number of unique values per each feature\")\nplt.xticks(rotation=45)\nplt.ylabel('Count')\nplt.xlabel('Feature')\nplt.show()","c24b8a6c":"cat_features = ['gender', 'race\/ethnicity', 'parental level of education', 'lunch',\n       'test preparation course']\nWIDTH = 16\nLENGTH = 30\n\nrows = math.ceil(len(cat_features)\/3)\nfig, ax = plt.subplots(5,1,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cat_features):\n    sns.countplot(df[feature],ax=ax[i])\n    ax[i].set_title(f'Distribution of a feature `{feature}`')","4c491113":"score = ['math score', 'reading score','writing score']\nround(df[score].describe(),2)","bd920711":"cont_features = ['math score', 'reading score','writing score']\nWIDTH = 15\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(rows,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    ax[i].hist(df[feature],alpha=0.6)\n    ax[i].set_title(f'Distribution of a feature `{feature}`')","2547bafe":"cont_features = ['math score', 'reading score','writing score']\ndf[cont_features].skew()","11989218":"cont_features = ['math score', 'reading score','writing score']\n\ndf1 = df[cont_features]\ncorr=df1.corr()\n\nplt.figure(figsize=(10,7))\nsns.heatmap(corr,\n            xticklabels=df1.columns,\n            yticklabels=df1.columns,\n            annot=True)\nplt.title('Correlation matrix of the continuous features')\nplt.show()","242629d7":"fig,ax = plt.subplots(1,3,figsize=(16,6))\nax = ax.flatten()\n\nex_scores = ['math score', 'reading score', 'writing score']\ni = 0\nfor feat1 in range(0,3):\n    for feat2 in range(feat1+1,3):\n        sns.regplot(x=ex_scores[feat1], y=ex_scores[feat2],data=df, ax=ax[i],scatter=False,color='r')\n        sns.scatterplot(x=ex_scores[feat1], y=ex_scores[feat2],data=df, ax=ax[i],alpha=0.4)\n        ax[i].set_xlim([0,100])\n        ax[i].set_ylim([0,100])\n        i+=1\n        \n","0bc63bde":"cont_features = ['math score', 'reading score','writing score']\ncat_variable = 'gender'\nWIDTH = 12\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i],palette = [\"#0101DF\", \"#DF0101\"])\n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","d63b4078":"df[df['math score'] == 0]","127cd284":"df[['math score','writing score','reading score']].min()","fdbdd332":"cont_features = ['math score', 'reading score','writing score']\ncat_variable = 'race\/ethnicity'\nWIDTH = 12\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i])\n    ax[i].set_xticklabels(ax[i].get_xticklabels(), Rotation= 45) \n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","2def59d4":"cont_features = ['math score', 'reading score','writing score']\ncat_variable = 'parental level of education'\nWIDTH = 12\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i])\n    ax[i].set_xticklabels(ax[i].get_xticklabels(), Rotation= 45) \n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","619c18b6":"cont_features = ['math score', 'reading score','writing score']\ncat_variable = 'lunch'\nWIDTH = 12\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i], palette=[\"#0101DF\", \"#DF0101\"])\n    ax[i].set_xticklabels(ax[i].get_xticklabels(), Rotation= 45) \n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","c1ca6e81":"cont_features = ['math score', 'reading score','writing score']\ncat_variable = 'test preparation course'\nWIDTH = 12\nLENGTH = 5\n\nrows = math.ceil(len(cont_features)\/3)\nfig, ax = plt.subplots(1,3,figsize=(WIDTH,LENGTH))\nax = ax.flatten()\nfor i,feature in enumerate(cont_features):\n    sns.boxplot(x=cat_variable, y=feature, data=df,ax=ax[i], palette=[\"#0101DF\", \"#DF0101\"])\n    ax[i].set_xticklabels(ax[i].get_xticklabels(), Rotation= 45) \n    ax[i].set_title(f'Cond. dist. of feature `{feature}`')","4412805e":"dfs = []\n\n\nfrom scipy.stats import f_oneway\n\ncont_features = ['math score', 'reading score','writing score']\n\nfor label in ['gender', \n              'race\/ethnicity', \n              'parental level of education', \n              'lunch','test preparation course']:\n \n    dic = {'Categorical': [],\n        'Numerical': [],\n        'p-value': [],\n        'p < 0.05': [],\n        'statistic': []}\n\n\n    for feature in cont_features:\n        values = []\n        for value in df[label].unique():\n            values.append(df[df[label] == value][feature].values)\n\n        statistic, pval = f_oneway(*values)\n\n        dic['Categorical'].append(label)\n        dic['Numerical'].append(feature)\n        dic['p-value'].append(pval)\n        dic['p < 0.05'].append(pval<0.05)\n        dic['statistic'].append(statistic)\n\n\n    dfs.append(pd.DataFrame(dic))","43069df6":"anova_df = pd.concat(dfs)\nmath = anova_df[anova_df['Numerical'] == 'math score']\nreading = anova_df[anova_df['Numerical'] == 'reading score']\nwriting = anova_df[anova_df['Numerical'] == 'writing score']","4b8dafbd":"math.sort_values(by='p-value',ascending=True)","00098eea":"reading.sort_values(by='p-value',ascending=True)","bf0b2aa1":"writing.sort_values(by='p-value',ascending=True)","3ab53fae":"from sklearn.preprocessing import OrdinalEncoder\n\ndf_new = df.copy()\n\n#Using ordinal encoder to encode features with only two unique values.\nfeatures_to_encode = ['gender','lunch','test preparation course']\nord_enc = OrdinalEncoder()\ndf_new[features_to_encode] = ord_enc.fit_transform(df_new[features_to_encode])\n\n\n#One hot encoding high cardinality features\ndf_new = pd.get_dummies(df_new)\n\n\ndf_new.head()","0a4660d7":"exam_scores = ['math score', 'reading score','writing score']\n\n#Feature matrix\nX = df_new.drop(exam_scores,axis=1)\n\n#Target variable\ny = (df_new['math score'] + df_new['reading score'] + df_new['writing score'])\/3\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=11)","478df8d8":"from sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.metrics import mean_squared_error\n\ntree_clf = DecisionTreeRegressor(max_depth=4,\n                                 min_samples_split=50,\n                                max_features='auto',\n                                criterion='mse').fit(X_train,y_train)\n\nMSE_test = mean_squared_error(y_pred=tree_clf.predict(X_test),y_true=y_test,squared=False)\nprint(f\"RMSE for test set: {MSE_test}\")","eb3469a8":"from sklearn.svm import SVR\n\n\nC_coeff = 16.42162462505648\ngamma_coeff = 0.010323600491562047\n\nsvr_clf = SVR(C=C_coeff, \n              gamma=gamma_coeff,\n              kernel='rbf').fit(X_train,y_train)\n\nMSE_test = mean_squared_error(y_pred=svr_clf.predict(X_test),y_true=y_test,squared=False)\nprint(f\"RMSE for test set: {MSE_test}\")\n","c5cf1634":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train,label=y_train)\nparam = {'max_depth': 1,\n        'eta': 1.7928804283381974,\n         'objective':'reg:squarederror',\n        'eval_metric':'rmse'}\nnum_round = 10\nbst = xgb.train(params=param,dtrain=dtrain, num_boost_round=30)\n\n\ndtest = xgb.DMatrix(X_test)\n\n\n\nMSE_test = mean_squared_error(y_pred=bst.predict(dtest),y_true=y_test,squared=False)\nprint(f\"RMSE for test set: {MSE_test}\")","dc3456fb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import backend as K\nimport random\n\n\nseed_value= 0\nos.environ['PYTHONHASHSEED']=str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value) # tensorflow 2.x\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\nann = keras.Sequential()\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\nann.add(tf.keras.layers.Dense(units = 7, activation = 'relu'))\nann.add(tf.keras.layers.Dense(units = 1, activation = 'linear'))\nann.compile(optimizer = 'adam', loss = root_mean_squared_error , metrics = ['MSE'])\nann.fit(X_train, y_train, batch_size = 32, epochs = 100,verbose=0)\n\nres = ann.predict(X_test).flatten()\nprint(f\"RMSE for test set: {mean_squared_error(y_pred=res,y_true=y_test,squared=False)}\")","3f7dc780":"rmse_score = [mean_squared_error(y_pred=tree_clf.predict(X_test),y_true=y_test,squared=False),\n          mean_squared_error(y_pred=svr_clf.predict(X_test),y_true=y_test,squared=False),\n          mean_squared_error(y_pred=bst.predict(xgb.DMatrix(X_test)),y_true=y_test,squared=False),\n          mean_squared_error(y_pred=ann.predict(X_test).flatten(),y_true=y_test,squared=False)]\nrmse_score = [round(x,2) for x in rmse_score]\nmodels = ['Decision Tree',  'SVM', 'Gradient Boost', 'ANN']\nscoredf = pd.DataFrame({'RMSE': rmse_score, 'Model': models}).sort_values(by='RMSE',ascending=False)\n\n\ncat_features = models\n\ncount = np.array(rmse_score)\n\nto_sort = np.argsort(count)\ncat_features = np.array(cat_features)[to_sort]\ncount = count[to_sort]\n\nplt.figure(figsize=(11,6))\ngraph = sns.barplot(cat_features,count)\nfor p in graph.patches:\n    graph.annotate(p.get_height(), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')\n\n\nplt.title(\"Performance of the regression models\")\nplt.xticks(rotation=45)\nplt.ylabel('RMSE score')\nplt.xlabel('Model')\nplt.show()","02823bb7":"Now let's consider the conditional distributions of each numeric variable (conditional on each discrete variable). We begin with `gender`","214c707a":"We see that the distributions are roughly normal (with a little tail to the left). Let's check the skewness.","951c4fbe":"These results are also within our expectations: kids with worse meals tend to perform worse (most likely this happens because kids who get worse meals are likely to be coming from poor families, and poor families imply worse conditions for studying, which as the result negatively influence the scores kids get on the exam).","341a8e83":"Importing relevant libraries","21622219":"ANOVA feature independence test: **writing** and categorical features","384ef88a":"# Regression","f2a38732":"# SVM","dd79ba38":"The results here are not surprising: the worse education the parent has received, the more likely it is that the child will perform worse on an exam. Although should be noted that the difference between `master's degree` and `bachelor's degree` is not significant as, for example, the difference between `high school` and `bachelor's degree`","17897209":"ANOVA feature independence test: **math** and categorical features","4fb529be":"One can observe that the group `A` (in general) has the worst performance across 3 exams, and group `E` tend to perform the best (although on writing exam, the superiority is not as significant as on math exam)","0f6546cd":"ANOVA feature independence test: **reading** and categorical features","b22036c4":"Interestingly (but perhaps not so surpisingly), the math turns out to be the **hardest** exam:\nThe average of math scores is lowest out of 3 subjects. Same goes for median, and Q3. But more striking difference appears when we consider minimum score achieved on the exams: the minimum score for the math is **0**, but for reading and writing, the scores are **17** and **10**, respectively.\nSo the upshot is: no matter what kind of metric we use, we see that (on average) the perfomance on the math exam is the worst.","2bbc9b31":"Now let's check (one more time) the min. values for the scores of all subjects","d79b43e2":"Let's first start with categorical features. Let's see the number of unique values per each variable.","4a830f36":"Shape of the dataset:","af0ec11d":"We see that all our categorical features and numerical features are dependent (we've set the threshold at $0.05$). Secondly, it seems that the features `test preparation course` and `lunch` are the most useful when predicting the exam score (this is signified by the fact that for all 3 numeric variables (i.e., scores), `test preparation course` and `lunch` are  in the top 3 features with the smallest $p$-value (the smaller $p$-value, the more unlikely it is that the features are independent))","4f5823f6":"Similarly, the result is intuitive: You've completed the preparation course $\\implies$ you are more prepared for exam $\\implies$ you are likely to get a better grade than those who didn't finished the course","e5cd65e5":"Let's use one way ANOVA to test independence between each categorical feature and each numeric feature.","ebce2ed1":"Now let's check the distributions of our numerical features (i.e., exam scores)","e8b55ba8":"Let's visualize the distributions.","e13b8301":"One interesting pattern can be observed: boys perform better on math exams, but girls perform better on reading\/writing exams. It also seems that there is one girl that performed very badly on *all* exams","59bb17dd":"Positive correlation for each pair of scores is not really surprising: **generally** speaking, if a student performs well on exam $A$, then it is quite likely that the student is conscientious (of course, unless he cheated or got a good grade thanks to other irregularities), implying the other grades will likely to be high too (which agrees with the fact that correlation between any two numeric features doesn't drop below $0.8$). Another unsurprising finding is that, `writing score` is more correlated with `reading score` than with `math score`. Similarly, `reading score` is more correlated with `writing score` than with `math score`. This makes sense: [To be a good writer, one needs to be a voracious reader](https:\/\/writing.stackexchange.com\/questions\/14189\/can-i-be-a-good-writer-without-reading-a-lot); yet to be good at math, one doesn't need to read a lot of a non-mathematical literature. Hence due to the inherent differences between math and the rest of two subjects, we observe a lower correlation.","faf406c7":"# Decision tree","bf355fa9":"Let's check the correlation between features.","5b88d2f3":"# ANN","8dc7e9d8":"As we see, each variable has negative skew, which agrees with conclusions we made while observing the graphs.","272e03b9":"We will try to predict the **average score**, i.e., \n\n$$\\text{average score} = \\frac{\\text{math score} + \\text{reading score} + \\text{writing score}}{3}$$\n\nSince all three variables are highly correlated, the predictions of the average score will be close to the predictions we would get when predicting the score for a single subject.","85e14281":"Now let's look at the univariate distributions.","fc70ba3d":"As we see, there are no nulls.","7ab2e7d1":"Feature preprocessing","0bf1babf":"Indeed, we see that she performed the worst across all exams.\n\nNow, let's have a look at influence of `race\\ethnicity` on exam scores.","6f83e6de":"Overview of the dataset","7a36656e":"# Gradient Boost","e102ca25":"# Introduction\n\nFirstly, we will use EDA to get a basic idea about the dataset, and then we will train regression models to try to predict the exam score.","3c6b8d7a":"Now let's look at the distirbutions conditional on `lunch`"}}