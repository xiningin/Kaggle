{"cell_type":{"cbafc418":"code","1a59f329":"code","e65f4af2":"code","10467be3":"code","cc505b93":"code","e7b1f8d3":"code","4c62b9e6":"code","1685f236":"code","97324c04":"code","f9fa4b6e":"code","4f926faf":"code","914b30bd":"code","b45ae394":"code","6cc6f15b":"code","1abf97d9":"code","6994d949":"code","e45a5ec5":"code","dd164cb8":"code","3b714258":"code","ab99e13e":"code","a69bbbea":"code","be1904e7":"code","1097d700":"code","c28a0e29":"code","b707872d":"code","c1d296b6":"code","dc9d4f01":"code","fc1447d6":"code","e83c2d8d":"code","c2cd37f6":"code","d632d71a":"code","45d3433f":"code","684dab14":"code","49bc45d4":"code","d736c2bb":"code","d1dbe57c":"code","edc06d28":"code","5f9b09a1":"code","75bf2dd2":"code","4e7635b2":"code","5fd1b4e8":"code","82f59c8e":"code","da3130d3":"code","8cee35a8":"code","5419009d":"code","82641fe9":"code","fd37a209":"code","45e60165":"code","2ee77603":"code","c757b188":"code","0826635f":"code","ba0afd69":"code","a3c65d35":"code","927347fe":"code","75141af4":"code","e0918dab":"code","198bedfb":"code","97decf08":"code","79a12a64":"code","b3a8fbd4":"code","97342928":"code","edd34ed5":"code","8dc935cf":"code","5a11c960":"code","ce8052d9":"code","c678961c":"code","96f63684":"code","cf362124":"code","f4d7d62b":"code","677f2e6f":"code","19c6a9c1":"code","11131278":"code","82a300d4":"code","92782f6d":"code","c701e433":"code","4955baa4":"code","c7c948fa":"code","8f61c46d":"code","47b05f08":"code","4144c9db":"code","06448b20":"code","78e0bba5":"code","a3f7efbf":"code","df9d363c":"code","dcc36677":"code","ac368578":"markdown","0d3948d6":"markdown","0aa70c3e":"markdown","5986bdb8":"markdown","6513306f":"markdown","1b2e0d91":"markdown","170f0121":"markdown","52ffb555":"markdown","db42be56":"markdown","65367cb2":"markdown","5328be7d":"markdown","6f8b48d9":"markdown","d77babd4":"markdown","22e35f86":"markdown","cbc304ae":"markdown","67e03851":"markdown","97905644":"markdown","cb5b5cc1":"markdown","283b9eb9":"markdown","72f57b53":"markdown","53883bba":"markdown","e01700c9":"markdown","f9607610":"markdown","2b25fa58":"markdown","9f3392b1":"markdown","7ad148ab":"markdown","9e1da9a2":"markdown","61783218":"markdown","dcee7202":"markdown","f79a58cb":"markdown","b71a0811":"markdown","3213b6f8":"markdown","4ec4e36d":"markdown","746c1382":"markdown","2c059c07":"markdown","d159ccb9":"markdown","ccbcf04b":"markdown","91742065":"markdown","7ea515d9":"markdown"},"source":{"cbafc418":"#pip install demoji","1a59f329":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport missingno as msno\nimport seaborn as sns\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WhitespaceTokenizer\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport pickle\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n# to Print very long string completely in pandas dataframe\npd.options.display.max_colwidth   \npd.options.display.max_colwidth = 300","e65f4af2":"# to remove emoje \nimport demoji\ndemoji.download_codes()","10467be3":"df1=pd.read_csv('..\/input\/rogue-content-and-arabic-spammers-on-twitter\/TweetsStreamingTotal.csv',encoding = \"utf-8-sig\")","cc505b93":"df1.head(1)","e7b1f8d3":"df1.shape","4c62b9e6":"# check the dataframe information\ndf1.info()","1685f236":"# Extract the data randomly,so if I run the code the data will change and this is an example for df sample becouse 1 saved the first data showed for me as df sample before.\ndf\u0640sample= df1.sample(frac =.03095)","97324c04":"#save the df test in csv file.\n# we save the file and store it in datasets file so we comments this code in order to not save the file in your laptop and the path will change\n\n#df\u0640sample.to_csv('.\/r_data.csv',encoding=\"utf-8-sig\")","f9fa4b6e":"# Read Data \ndf_full=pd.read_csv('.\/r_data.csv')","4f926faf":"df_full.info()","914b30bd":"df_full.shape","b45ae394":"df_full.head()","6cc6f15b":"missing = df_full.isnull().sum().sort_values(ascending = False)\npercentage = (df_full.isnull().sum()\/ df_full.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([missing, percentage], axis = 1, keys = ['Missing', '%'])\nmissing_data.head(24)","1abf97d9":"# visualize the location of missing values\nmsno.matrix(df_full,color = (0.90, 0.80, 0.65))","6994d949":"df_full.dropna(subset = [\"text\"], inplace=True)","e45a5ec5":"df_full['text'] = df_full['text'].apply(lambda x: demoji.replace(x))","dd164cb8":"df_full.text[156]","3b714258":"df_full['text'] = df_full['text'].apply(lambda x: re.sub('[!@#$:).;,?\u061f&%]', ' ', x.lower()))\ndf_full['text'] = df_full['text'].apply(lambda x: re.sub('[a-zA-Z]', ' ', x.lower()))\ndf_full['text'] = df_full['text'].apply(lambda x: re.sub(\"['''`\u00f7\u00d7\u061b<>_()*&^%][\u0640\u060c\/:\u061f.,'{}~\u00a6+|!\u2013\u0640''']\", ' ', x.lower()))\ndf_full['text'] = df_full['text'].apply(lambda x: re.sub('  ', ' ', x))\ndf_full['text'] = df_full['text'].apply(lambda x: re.sub('\u2026', ' ', x.lower()))\ndf_full['text'][156]","ab99e13e":"df_full['text'].replace(r'\u2026', ' ', inplace = True, regex = True)\ndf_full['text'].replace(r'_', ' ', inplace = True, regex = True)\ndf_full['text'].replace(r'\\\\\\)', ' ', inplace = True, regex = True)\ndf_full['text'].replace(r'\u060c', ' ', inplace = True, regex = True)\ndf_full['text'].replace(r'\"', ' ', inplace = True, regex = True)\ndf_full['text'].replace(r'-', ' ', inplace = True, regex = True)\ndf_full['text'].replace(r'\u061b', ' ', inplace = True, regex = True)\ndf_full['text'] = df_full['text'].str.replace('\\d+', '')\ndf_full['text'].replace(r'\\\/', ' ', inplace = True, regex = True)\ndf_full['text'] = df_full['text'].str.replace(' +', ' ')\ndf_full['text'] = df_full['text'].str.replace('\\n', ' ')\ndf_full['text'] = df_full['text'].str.replace('\u0623\u0646\u0627','\u0627\u0646\u0627')\ndf_full['text'] = df_full['text'].str.replace('\u263b ', '')\ndf_full['text'] = df_full['text'].str.replace('\u2022', '')           \ndf_full['text'] = df_full['text'].str.replace('\u201c', '')\ndf_full['text'] = df_full['text'].str.replace('\u201d', '')\ndf_full['text'] = df_full['text'].str.replace(' \ufd3e', '')\ndf_full['text'] = df_full['text'].str.replace('\u066a','')\ndf_full['text'] = df_full['text'].str.replace(')','')\ndf_full['text'] = df_full['text'].str.replace('(','')\ndf_full['text'] = df_full['text'].str.replace('*','')\ndf_full['text'] = df_full['text'].str.replace('+','')\ndf_full['text'] = df_full['text'].str.replace('=','')\ndf_full['text'] = df_full['text'].str.replace('[','')\ndf_full['text'] = df_full['text'].str.replace(']','')\ndf_full['text'] = df_full['text'].str.replace('~','')\ndf_full['text'] = df_full['text'].str.replace('\u300b','')\ndf_full['text'] = df_full['text'].str.replace('\u300a','')\ndf_full['text'] = df_full['text'].str.replace('\u2698','')\ndf_full['text'] = df_full['text'].str.replace('\u2661','')\ndf_full['text'] = df_full['text'].str.replace('\u2014','') \ndf_full['text'] = df_full['text'].str.replace('\\u2060','')\ndf_full['text'] = df_full['text'].str.replace('\uf074','')\ndf_full['text'] = df_full['text'].str.replace(' \ufe0f\ufe0f','')\ndf_full['text'] = df_full['text'].str.replace('\u00bb','')\ndf_full['text'] = df_full['text'].str.replace('\u00ab','')\ndf_full['text'] = df_full['text'].str.replace('^','')\ndf_full['text'] = df_full['text'].str.replace('>>','')\ndf_full['text'] = df_full['text'].str.replace('<<','')\ndf_full['text'] = df_full['text'].str.replace('}','')\ndf_full['text'] = df_full['text'].str.replace('{','')\ndf_full['text'] = df_full['text'].str.replace(' \u032e','')\ndf_full['text'] = df_full['text'].str.replace('>','')\ndf_full['text'] = df_full['text'].str.replace('\u25e1\u0308\u20dd','')\ndf_full['text'] = df_full['text'].str.replace('\u269a','')\ndf_full['text'] = df_full['text'].str.replace('\ufd3f','')\ndf_full['text'] = df_full['text'].str.replace('\\\\','')\ndf_full['text'] = df_full['text'].str.replace('xDxD','')\n\ndf_full['text'] = df_full['text'].str.replace('\u0651\u0651\u0651\u0651\u0651','')\ndf_full['text'] = df_full['text'].str.replace('\u21e2 \u21e0','')\ndf_full['text'] = df_full['text'].str.replace('\u21b7','')\ndf_full['text'] = df_full['text'].str.replace('o','')\ndf_full['text'] = df_full['text'].str.replace('\u21e2 \u21e0','')\ndf_full['text'] = df_full['text'].str.replace('re','')\ndf_full['text'] = df_full['text'].str.replace('\\r\\r \\r\\r ','')\ndf_full['text'] = df_full['text'].str.replace('\\r\\r','')","a69bbbea":"english_punctuations = string.punctuation\n\narabic_diacritics = re.compile(\"\"\"\n                             \u0651    | # Tashdid\n                             \u064e    | # Fatha\n                             \u064b    | # Tanwin Fath\n                             \u064f    | # Damma\n                             \u064c    | # Tanwin Damm\n                             \u0650    | # Kasra\n                             \u064d    | # Tanwin Kasr\n                             \u0652    | # Sukun\n                             \u0640     # Tatwil\/Kashida\n                         \"\"\", re.VERBOSE)\n\n\ndef normalize_arabic(text):\n    text = re.sub(\"[\u0625\u0623\u0622\u0627]\", \"\u0627\", text)\n    text = re.sub(\"\u0649\", \"\u064a\", text)\n    text = re.sub(\"\u0624\", \"\u0621\", text)\n    text = re.sub(\"\u0626\", \"\u0621\", text)\n    text = re.sub(\"\u06af\", \"\u0643\", text)\n    return text\n\n\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', text)\n    return text\n\n\ndef remove_punctuations(text):\n    translator  = re.compile('[%s]' % re.escape(english_punctuations))\n    translator.sub(' ', text)\n    text = re.sub(' +', ' ', text).strip()\n    return text\n\n\ndef remove_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)","be1904e7":"df_full['text'] = df_full['text'].apply(lambda x: normalize_arabic(x))\ndf_full['text'] = df_full['text'].apply(lambda x: remove_diacritics(x))\ndf_full['text'] = df_full['text'].apply(lambda x: remove_punctuations(x))\ndf_full['text'] = df_full['text'].apply(lambda x: remove_repeating_char(x))","1097d700":"df_full.head(1)","c28a0e29":"df_full.tail(5)","b707872d":"def is_blank_mask(df_full, text):\n    return df_full[text].astype(str).str.fullmatch(r\"\\s*\")","c1d296b6":"df_full['pass\/fail'] = np.where(is_blank_mask(df_full, 'text'),\n                            'fail',\n                            'pass')\n# check blank \n#tweet_1[tweet_1['text'].str.fullmatch(r\"\\s*\")]","dc9d4f01":"#new variable without blank tweet \"look to specific value\"\ndf_full=df_full.loc[df_full['pass\/fail']==\"pass\"]\ndf_full.shape","fc1447d6":"# Extract the train data randomly,so if I run the code the data will change and this is an example for df train becouse I saved the first data showed for me as df train before.\n#df\u0640sample_train= df_full.sample(frac =.07897)\n#df\u0640sample_train.head(1)","e83c2d8d":"#save date \n#df\u0640sample_train.to_csv('.\/df\u0640sample_for_tarin.csv',encoding=\"utf-8-sig\")","c2cd37f6":"df\u0640sample_train=pd.read_csv('.\/df\u0640sample_for_tarin.csv')\ndf\u0640sample_train.shape","d632d71a":"# select text_x in the data\ndf_train=df\u0640sample_train[['text']]","45d3433f":"# Extract the df test by index. that might be use it for future work.\nleft_merge=pd.merge(df_full, df\u0640sample_train, how='left', left_on=['Unnamed: 0'], right_on=['Unnamed: 0'])\nleft_merge.head(1)","684dab14":"left_merge.shape","49bc45d4":"left_merge['Unnamed: 0.1_y'] = left_merge['Unnamed: 0.1_y'].fillna(0)","d736c2bb":"df\u0640sample_test=left_merge.loc[left_merge['Unnamed: 0.1_y']==0]","d1dbe57c":"df\u0640sample_test.shape","edc06d28":"df\u0640sample_test.head(1)","5f9b09a1":"# select text_x in the data\ndf_test=df\u0640sample_test[['text_x']]","75bf2dd2":"df_test.rename({'text_x': 'text',}, axis=1, inplace=True)\ndf_test.head(10)","4e7635b2":"# Extract the tast data randomly\ndf_test__40k= df_test.sample(frac =.52897)\ndf_test__40k.shape\n\n#df_test__39k=df_test.loc[1:40000,['text']]\n#df_test__39k.tail(10)","5fd1b4e8":"#save test data\n#df_test__40k.to_csv('.\/df_test.csv',encoding=\"utf-8-sig\")","82f59c8e":"df_test__40k.head()","da3130d3":"df_full_text=df_full[['text']]","8cee35a8":"df_full_text['Tweet_tokenized'] = df_full_text.text.apply(lambda x: WhitespaceTokenizer().tokenize(x.lower()))","5419009d":"df_full_text.Tweet_tokenized[2]","82641fe9":"arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n","fd37a209":"def remove_stopwords(text):\n    text = [word for word in text if word not in arb_stopwords]\n    return text","45e60165":"df_full_text['tweet_nonstop'] = df_full_text['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))","2ee77603":"df_full_text.tail(5)","c757b188":"df_full_text.tweet_nonstop[9] #strip to remove spaces","0826635f":"#df_full_text.to_csv('.\/df_full_text_with_out_sto.csv',encoding=\"utf-8-sig\")","ba0afd69":"#Read data \ndf_train=pd.read_csv('..\/input\/df-train-label\/df_train_clean.csv')\n","a3c65d35":"df_train.tail(4)","927347fe":"#select text and label\ndf_train=df_train[['text','lable']]\n\ndf_train.head()","75141af4":"df_train.rename({'lable': 'sentiment_class'\n                       }, axis=1, inplace=True)\ndf_train.head(1)","e0918dab":"#converts our labels to integers\ndf_train['sentiment_class'] = df_train['sentiment_class'].astype('str').values\ndf_train['sentiment_class'] = df_train['sentiment_class'].astype('category').values\n\n#Then you can access the underlying integers usin the cat accessor method\ndf_train['sentiment_class'] = df_train['sentiment_class'].cat.codes \ndf_train['sentiment_class'].value_counts()","198bedfb":"df_train.tail()","97decf08":"#Making as copy\ndf = df_train.copy()","79a12a64":"ax = sns.countplot(x = 'sentiment_class', data = df, palette = 'Set2')\nplt.title ('Number of Tweets In Each Class')\nax.set_xticklabels(['non_spam', 'spam'])\nplt.show()","b3a8fbd4":"df['Tweet_tokenized'] = df.text.apply(lambda x: WhitespaceTokenizer().tokenize(x.lower()))\ndf.head()","97342928":"arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\ndef remove_stopwords(text):\n    text = [word for word in text if word not in arb_stopwords]\n    return text\n\ndf['tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\nmore_stopwords = {'\u060c', '\u0648', '\u0639\u0644\u0649',\"\u0641\u064a\",'\u062d\u062a\u0649','\u0643\u0644','\u0648\u0644\u0627','\u0627\u0644\u0627','\u0645\u0627','\u0647\u0630\u0627',\n                  '\u0623\u0648', '\u0630\u0627','\u062d\u064a\u0646','\u0644\u064a\u062a', '\u0634\u064a',\n                  '\u0627\u0644\u0627\u062a\u064a','\u0627\u0644\u0644\u0647','\u0623\u0648','\u0639\u0646\u0627','\u0648\u0627\u0644\u0644\u0647','\u0642\u0628\u0644','\u062e\u0644\u0627\u0644','\u0627\u0646','\u0627\u0644\u0644\u064a','\u0627\u0648'\n                  ,'\u0627\u0646\u0627','\u0628\u0640','\u0634\u064a\u0621','\u0641\u0642\u0637','\u064a\u0648\u0645','\u0639\u0644\u064a\u0646\u0627','\u0643\u0627\u0646','\u0627\u0630\u0627','\u0639\u0634\u0627\u0646','\u0648\u0634','\u0648\u0628\u0633\u0628\u0628'}\narb_stopwords = arb_stopwords.union(more_stopwords)","edd34ed5":"X = df['text']\ny = df['sentiment_class']","8dc935cf":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, stratify=df.sentiment_class,random_state = 55)","5a11c960":"df.shape, X_train.shape, X_test.shape","ce8052d9":"# vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = CountVectorizer()\n\nvectorizer.fit(X_train)\n\npickle.dump(vectorizer, open(\"modelVectorizer.pkl\", \"wb\"))\n\nX_train = vectorizer.transform(X_train)\nX_test  = vectorizer.transform(X_test)\nprint(\"{} Number of text in training has {} words\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"{} Number of text in testing {} words\".format(X_test.shape[0], X_test.shape[1]))","c678961c":"randomF = RandomForestClassifier(max_depth=350, n_estimators=9, max_features=11, random_state=14, min_samples_split=3)\nrandomF.fit(X_train, y_train)\nprint('Train score :',randomF.score(X_train, y_train))\nprint('Ttest score :',randomF.score(X_test, y_test))","96f63684":"logreg = LogisticRegression(max_iter=7)\nlogreg.fit(X_train, y_train)\nprint('train score' , logreg.score(X_train, y_train))\nprint('test score' , logreg.score(X_test, y_test))","cf362124":"cv=KFold(n_splits=5, shuffle=True, random_state=1)\ncross_val_score(logreg, X_test, y_test ,cv=cv).mean()","f4d7d62b":"ynew_2 = logreg.predict(X_test)\nprint(confusion_matrix(y_test,ynew_2))\nprint(classification_report(y_test,ynew_2))\nprint(accuracy_score(y_test, ynew_2))","677f2e6f":"filename = 'model.sav'","19c6a9c1":"# Save the model\npickle.dump(model, open(filename, 'wb'))","11131278":"# load the model\nloaded_model = pickle.load(open(filename, 'rb'))","82a300d4":"# read data\ndf_test=pd.read_csv('.\/df_test.csv')","92782f6d":"df_test.head()","c701e433":"df_test.shape","4955baa4":"df_test['Tweet_tokenized'] = df_test.text.apply(lambda x: WhitespaceTokenizer().tokenize(x.lower()))\ndf_test.head(1)","c7c948fa":"arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\ndef remove_stopwords(text):\n    text = [word for word in text if word not in arb_stopwords]\n    return text\n\ndf_test['tweet_nonstop'] = df_test['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n\nmore_stopwords = {'\u060c', '\u0648', '\u0639\u0644\u0649',\"\u0641\u064a\",'\u062d\u062a\u0649','\u0643\u0644','\u0648\u0644\u0627','\u0627\u0644\u0627','\u0645\u0627','\u0647\u0630\u0627',\n                  '\u0623\u0648', '\u0630\u0627','\u062d\u064a\u0646','\u0644\u064a\u062a', '\u0634\u064a',\n                  '\u0627\u0644\u0627\u062a\u064a','\u0627\u0644\u0644\u0647','\u0623\u0648','\u0639\u0646\u0627','\u0648\u0627\u0644\u0644\u0647','\u0642\u0628\u0644','\u062e\u0644\u0627\u0644','\u0627\u0646','\u0627\u0644\u0644\u064a','\u0627\u0648'\n                  ,'\u0627\u0646\u0627','\u0628\u0640','\u0634\u064a\u0621','\u0641\u0642\u0637','\u064a\u0648\u0645','\u0639\u0644\u064a\u0646\u0627','\u0643\u0627\u0646','\u0627\u0630\u0627','\u0639\u0634\u0627\u0646','\u0648\u0634','\u0648\u0628\u0633\u0628\u0628'}\narb_stopwords = arb_stopwords.union(more_stopwords)","8f61c46d":"ectorizer = CountVectorizer(max_features=4205)\nsample_features = vectorizer.fit_transform(df_test['text'])\nsample_features   ","47b05f08":"df_test['lable'] = loaded_model.predict(sample_features)","4144c9db":"df_test['sentiment_class'] = df_test['lable']\nchange_num_to_cat = {\"sentiment_class\": {0: \"not_spam\", 1: \"spam\"}}","06448b20":"df_test.replace(change_num_to_cat, inplace=True)\ndf_test.tail(100)","78e0bba5":"df_test['sentiment_class'].value_counts()","a3f7efbf":"## for figer \nnot_spam=df_test.loc[df_test['sentiment_class']== 'not_spam']","df9d363c":"## for figer \nspam=df_test.loc[df_test['sentiment_class']== 'spam']","dcc36677":"import plotly.graph_objects as go\nlabels_1 = ['not_spam', 'spam']\nvalues_1 = [len(not_spam), len(spam)]\n# Use `hole` to create a donut-like pie chart\nfig_1 = go.Figure(data = [go.Pie(labels = labels_1, values = values_1, hole = .3)])\nfig_1.update_layout(title_text = \"The Percentage of Tweets in class \"'\\n')\nfig_1.show()","ac368578":"## Clean Data","0d3948d6":"## Exploratory Data Analysis","0aa70c3e":"### Remove stop words","5986bdb8":"# Arabic tweets  ","6513306f":"## stop words","1b2e0d91":"#### The best score was with the Linear Support Vector Machine model which have achieved a 0.88 test score.","170f0121":"### Tokenization\nNow we will tokenize all the cleaned tweets in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","52ffb555":"------","db42be56":"### Preprocessing to Modeling","65367cb2":"###  Logistic Regression Model","5328be7d":"### Remove Stop Words","6f8b48d9":"### Save model ","d77babd4":"## Modeling ","22e35f86":"In this project, we seek to detect the adult content by applying sentiment analysis on users' Arabic tweets and classification them as spam.","cbc304ae":"## Tokenization","67e03851":"## Extract the train and test data randomly","97905644":"#### Extract the data randomly","cb5b5cc1":"## Introduction","283b9eb9":"### Part #1  Cleaning Data","72f57b53":"### Replace NAN with 0 in Unnamed: 0.1_y to select the test data that not included in the train data","53883bba":"### Test","e01700c9":"## Part # 2 Trining and Prediction ","f9607610":"### Tokenization\nNow we will tokenize all the cleaned tweets in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","2b25fa58":"### Remove the Characters ","9f3392b1":"### Check blank in text after cleaning then  remove it ","7ad148ab":"#### Train","9e1da9a2":"### Renam","61783218":"## Importing packages","dcee7202":"## Importing packages","f79a58cb":"## Prediction ","b71a0811":"### Check Missing Values","3213b6f8":"I take the rondom spmale from dataset and save it as cvs file to make the label manual (text classification: spam- non spam ) and I uesed excel to make lable","4ec4e36d":"### The Percentage of Tweets for Arabic tweets  ","746c1382":"###  Random Forest  Model","2c059c07":"- ### Text Classification ( labels )","d159ccb9":"### Rename","ccbcf04b":"### Remove Missing from text","91742065":"------------","7ea515d9":"### Handle Emoticons and Emojis\n"}}