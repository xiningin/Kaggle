{"cell_type":{"4b506743":"code","730a5a62":"code","8fb3369f":"code","61d660b5":"code","200bd005":"code","865fecc4":"code","0f5b3741":"code","03bdb179":"code","3a58d923":"code","e2d9a459":"code","ebca713e":"code","f4c5b1df":"code","342c6d5c":"code","dc7c4f2a":"code","caacebb1":"code","23deef7d":"code","e2304356":"code","e1f63795":"code","8059a2ce":"code","a9da1414":"code","79ca6c68":"code","b881fcb6":"code","12fe6574":"code","ae8f61ca":"code","64ed2190":"code","ab1c2dff":"code","505751d2":"code","303242b3":"code","19571684":"code","562ef54b":"markdown","17d7529b":"markdown","f84bd282":"markdown","f426ebe1":"markdown","401fa15a":"markdown","1eee9f11":"markdown","fe475914":"markdown","3e8ec5ea":"markdown","b42c8128":"markdown","87b26a32":"markdown","3c8e469e":"markdown","97f03641":"markdown","a09172e4":"markdown","6390b13f":"markdown","0c4c37c2":"markdown","e4fc7475":"markdown","a6202849":"markdown","3ba7b360":"markdown","118452b8":"markdown","6c1ec14a":"markdown","da2a1bb3":"markdown","ca0e8d35":"markdown","266452ef":"markdown","d557d5b6":"markdown","b7211964":"markdown","76b02e17":"markdown","d1c71b6d":"markdown","6481fc60":"markdown","70539df5":"markdown"},"source":{"4b506743":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/character-predictions.csv\")","730a5a62":"#--------------------- Data Cleansing -----------------------------------------\n\n#-------- culture induction -------\ncult = {\n    'Summer Islands': ['summer islands', 'summer islander', 'summer isles'],\n    'Ghiscari': ['ghiscari', 'ghiscaricari',  'ghis'],\n    'Asshai': [\"asshai'i\", 'asshai'],\n    'Lysene': ['lysene', 'lyseni'],\n    'Andal': ['andal', 'andals'],\n    'Braavosi': ['braavosi', 'braavos'],\n    'Dornish': ['dornishmen', 'dorne', 'dornish'],\n    'Myrish': ['myr', 'myrish', 'myrmen'],\n    'Westermen': ['westermen', 'westerman', 'westerlands'],\n    'Westerosi': ['westeros', 'westerosi'],\n    'Stormlander': ['stormlands', 'stormlander'],\n    'Norvoshi': ['norvos', 'norvoshi'],\n    'Northmen': ['the north', 'northmen'],\n    'Free Folk': ['wildling', 'first men', 'free folk'],\n    'Qartheen': ['qartheen', 'qarth'],\n    'Reach': ['the reach', 'reach', 'reachmen'],\n}\n\ndef get_cult(value):\n    value = value.lower()\n    v = [k for (k, v) in cult.items() if value in v]\n    return v[0] if len(v) > 0 else value.title()\n\ndata.loc[:, \"culture\"] = [get_cult(x) for x in data.culture.fillna(\"\")]\n\n#-------- culture induction -------\n\ndata.drop([\"name\", \"alive\", \"pred\", \"plod\", \"isAlive\", \"dateOfBirth\", \"DateoFdeath\"], 1, inplace = True)\n\ndata.loc[:, \"title\"] = pd.factorize(data.title)[0]\ndata.loc[:, \"culture\"] = pd.factorize(data.culture)[0]\ndata.loc[:, \"mother\"] = pd.factorize(data.mother)[0]\ndata.loc[:, \"father\"] = pd.factorize(data.father)[0]\ndata.loc[:, \"heir\"] = pd.factorize(data.heir)[0]\ndata.loc[:, \"house\"] = pd.factorize(data.house)[0]\ndata.loc[:, \"spouse\"] = pd.factorize(data.spouse)[0]\n\ndata.fillna(value = -1, inplace = True)\n''' $$ The code below usually works as a sample equilibrium. However in this case,\n this equilibirium actually decrease our accuracy, all because the original \nprediction data was released without any sample balancing. $$\n\ndata = data[data.actual == 0].sample(350, random_state = 62).append(data[data.actual == 1].sample(350, random_state = 62)).copy(deep = True).astype(np.float64)\n\n'''\nY = data.actual.values\n\nOdata = data.copy(deep=True)\n\ndata.drop([\"actual\"], 1, inplace = True)","8fb3369f":"#------------------ Feature Correlation ---------------------------------------\n\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(30,20)\nplt.show()","61d660b5":"#------------------ Predicting ------------------------------------------------\n\n#---------- 1 RandomForest -----------------\ndata.drop([\"S.No\"], 1, inplace = True)\n''' ATTENTION: This rf algorithm achieves 99%+ accuracy, this is because the \\\noriginal predictor-- the document releaser use exactly the same algorithm to predict!\n    '''\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(data, Y)\n\nprint('RandomForest Accuracy\uff1a(original)\\n',random_forest.score(data, Y))\n","200bd005":"#---------- 2 DecisionTree -----------------\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDT=DecisionTreeClassifier()\n\nDT.fit(data,Y)\n\nprint('DecisionTree Accuracy\uff1a(original)\\n',DT.score(data, Y))\n","865fecc4":"#---------- 3 SVC -----------------\n\nfrom sklearn.svm import SVC, LinearSVC\nsvc = SVC()\n\nsvc.fit(data, Y)\n\nprint('SVC Accuracy\uff1a\\n',svc.score(data, Y))\n","0f5b3741":"#---------- 4 LogisticRegression -----------------\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\nLR.fit(data, Y)\n\nprint('LogisticRegression Accuracy\uff1a\\n',LR.score(data, Y))","03bdb179":"#---------- 5 kNN -----------------\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(data, Y)\n\nprint('kNN Accuracy\uff1a\\n',knn.score(data, Y))","3a58d923":"#---------- 6 NaiveBayes Gaussian -----------------\n\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\n\ngaussian.fit(data, Y)\n\nprint('gaussian Accuracy\uff1a\\n',gaussian.score(data, Y))","e2d9a459":"#---------- 7 LogisticRegression with Cross Validation-----------------\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import cross_validation\npredictors=['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']\nalg=LogisticRegression(random_state=1)\nscores=cross_validation.cross_val_score(alg,Odata[predictors],Odata[\"actual\"],cv=3)\nprint('Logistic CrossValidation Accuracy\uff1a\\n',scores.mean())","ebca713e":"#---------- 8 LinearRegression-----------------\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_validation import KFold\npredictors=['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']\nalg=LinearRegression()\nkf=KFold(Odata.shape[0],n_folds=3,random_state=1)\npredictions=[]\nfor train,test in kf:\n    train_predictors=(Odata[predictors].iloc[train,:])\n    train_target=Odata[\"actual\"].iloc[train]\n    alg.fit(train_predictors,train_target)\n    test_predictions=alg.predict(Odata[predictors].iloc[test,:])\n    predictions.append(test_predictions)\n    \npredictions=np.concatenate(predictions,axis=0)\npredictions[predictions>.5]=1\npredictions[predictions<=.5]=0\naccuracy=sum(predictions==Odata[\"actual\"])\/len(predictions)\nprint('LinearRegression Accuracy\uff1a\\n',accuracy)","f4c5b1df":"#---------- 9 RandomForest with CrossValidation -----------------\n\nfrom sklearn import cross_validation\nfrom sklearn.ensemble import RandomForestClassifier\npredictors=['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']\nalg=RandomForestClassifier(random_state=1,n_estimators=150,min_samples_split=12,min_samples_leaf=1)\nkf=cross_validation.KFold(Odata.shape[0],n_folds=3,random_state=1)\nscores=cross_validation.cross_val_score(alg,Odata[predictors],Odata[\"actual\"],cv=kf)\nprint('RandomForest Accuracy\uff1a\\n',scores.mean())","342c6d5c":"#---------- 10 GradientBoosting -----------------\n\nfrom sklearn.cross_validation import KFold\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nalgorithms=[\n        [GradientBoostingClassifier(random_state=1,n_estimators=25,max_depth=3),['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']],\n        [LogisticRegression(random_state=1),['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']]]\n \n \nkf=KFold(Odata.shape[0],n_folds=3,random_state=1)\npredictions=[]\nfor train,test in kf:\n    train_target=Odata[\"actual\"].iloc[train]\n    full_test_predictions=[]\n    for alg,predictors in algorithms:\n        alg.fit(Odata[predictors].iloc[train,:],train_target)\n        test_predictions=alg.predict_proba(Odata[predictors].iloc[test,:].astype(float))[:,1]\n        full_test_predictions.append(test_predictions)\n    test_predictions=(full_test_predictions[0]+full_test_predictions[1])\/2\n    test_predictions[test_predictions<=.5]=0\n    test_predictions[test_predictions>.5]=1\n    predictions.append(test_predictions)\n\npredictions=np.concatenate(predictions,axis=0)\naccuracy=sum(predictions==Odata[\"actual\"])\/len(predictions)\nprint('GradientBoosting Accuracy: \\n',accuracy)","dc7c4f2a":"#---------- 11 TensorflowSoftmax -----------------\n\n\nimport tensorflow as tf\n\ndataset_X = data[['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']].as_matrix()\n\ndataset_Y = Odata[['actual']].as_matrix()\n\n\nX = tf.placeholder(tf.float32, shape=[None, 24])\ny = tf.placeholder(tf.float32, shape=[None, 1])\n \nweights = tf.Variable(tf.random_normal([24, 2]), name='weights')\nbias = tf.Variable(tf.zeros([2]), name='bias')\ny_pred = tf.nn.softmax(tf.matmul(X, weights) + bias)\n \ncross_entropy = - tf.reduce_sum(y * tf.log(y_pred + 1e-10), reduction_indices=1)\n\ncost = tf.reduce_mean(cross_entropy)\n \n\ntrain_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n \n \nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n \n    for epoch in range(50):\n        total_loss = 0.\n        for i in range(len(dataset_X)):\n            # prepare feed data and run\n            feed_dict = {X: [dataset_X[i]], y: [dataset_Y[i]]}\n            _, loss = sess.run([train_op, cost], feed_dict=feed_dict)\n            total_loss += loss\n        # display loss per epoch\n        print('Epoch: %04d, total loss=%.9f' % (epoch + 1, total_loss))\n    print(\"Train Complete\")\n","caacebb1":"# Bagging\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n# Bagged KNN\n\nfrom sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(data,Y)\nresult=cross_val_score(model,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","23deef7d":"# Bagged Decision Tree\n\nmodel=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(data,Y)\nresult=cross_val_score(model,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())\n","e2304356":"#------------------------ Boosting --------------------------------------------\n\nfrom sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","e1f63795":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","8059a2ce":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","a9da1414":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()\n","79ca6c68":"#-------------------------------- Battle Analysis -------------------------------------------------\n#----------------------------- Data Cleansing------------------------------------\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/battles.csv\")","b881fcb6":"data.iat[37,13]='win'\ndata.loc[data[\"name\"]==\"Siege of Winterfell\",\"battle_type\"]='siege'\ndata.loc[data[\"name\"]==\"Siege of Winterfell\",\"major_death\"]=0\ndata.loc[data[\"name\"]==\"Siege of Winterfell\",\"major_capture\"]=0\n#----------------------------------------#\ndata['attacker_king'] = data['attacker_king'].fillna('Without a king')   \ndata['defender_king'] = data['defender_king'].fillna('Without a king')\n  \ndata['defender_1'] = data['defender_1'].fillna('common people')\n\ndata['attacker_commander'] = data['attacker_commander'].fillna('Without a commander')   \ndata['defender_commander'] = data['defender_commander'].fillna('Without a commander')\n  \ndata['location'] = data['location'].fillna('Dont know where')\n","12fe6574":"#----- Only if we have to full the size of battle\n#attacker_size_mid = data['attacker_size'].median()  \n#defender_size_mid = data['defender_size'].median()\nfor i in range(1,len(data)):\n    if  np.isnan(data.iloc[i,17]) and np.isnan(data.iloc[i,18]):\n        continue\n    elif np.isnan(data.iloc[i,17]):\n        data.iat[i,17] = data.iat[i,18]\n    elif np.isnan(data.iloc[i,18]):\n        data.iat[i,18] = data.iat[i,17]\n        \ndata['battle_size'] = data['attacker_size'] + data['defender_size']\n","ae8f61ca":"#-------------------------------Data Analysis----------------------------------\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","64ed2190":"# 1 battle type-----------------\nplt.figure(figsize=[10,6])\ndata.battle_type.value_counts().plot(kind='bar')\nplt.title(\"battle type\") \nplt.ylabel(\"count\") \nplt.show()","ab1c2dff":"# 2 battle region---------------\n\nfig, axis1 = plt.subplots(1,1,figsize=(10,4))\naxis1.set_title(\"region distribution\")\nsns.countplot(x='region', data=data, ax=axis1)\nplt.show()","505751d2":"# 3 battle size-----------------\nfig, axis1 = plt.subplots(1,1,figsize=(30,8))\naxis1.set_title(\"battle size\")\nsns.barplot(x='name', y='battle_size', data=data, ax=axis1)\nplt.xticks(rotation=90)\nplt.show()","303242b3":"# 4 win&defeat board------------\n\nkingd = data['defender_king']\nkinga = data['attacker_king']\nking = kinga.append(kingd)\nking = king.drop_duplicates()\n\nl = [0]\nl = l * len(king)\nw = dict(zip(king,l))\nd = dict(zip(king,l))\n\nfor i in range(0,len(data)):\n    if  data.iloc[i,13]=='win':\n        w[data.iloc[i,3]]+=1\n        d[data.iloc[i,4]]+=1\n    else:\n        w[data.iloc[i,4]]+=1\n        d[data.iloc[i,3]]+=1\n        \nplt.figure(figsize=[15,6])\nplt.bar(range(len(w)), list(w.values()), align='center')\nplt.xticks(range(len(w)), list(w.keys()))\nplt.show()\nplt.figure(figsize=[15,6])\nplt.bar(range(len(d)), list(d.values()), align='center')\nplt.xticks(range(len(d)), list(d.keys()))\nplt.show()","19571684":"# 5 army size that house put\n\nhouse = dict(zip(king,l))\n\nfor i in range(0,len(data)):\n    if data.iloc[i,17] \/\/ 1 == data.iloc[i,17]:\n        house[data.iloc[i,3]] += data.iloc[i,17]\n    if data.iloc[i,18] \/\/ 1 == data.iloc[i,18]:\n        house[data.iloc[i,4]] += data.iloc[i,18]\n    print(i,house)\n\nplt.figure(figsize=[15,6])\nplt.bar(range(len(house)), list(house.values()), align='center')\nplt.xticks(range(len(house)), list(house.keys()))\n","562ef54b":"**8.LinearRegression with KFold**\n\n","17d7529b":"**This is my very first kernel ever,so any feedback of yours will be appreciated.**\n\nThe reason I chose this **A Song of Ice and Fire** dataset as my first step is  I am actually a big fan of the TV serial GoT, and I am so into the ASoIaF world.\n\n**\nIf you have any question,please leave a message below and I will check it.**","f84bd282":"**Bagged KNN**\n\n","f426ebe1":"**Feature Importance in different alg**\n\nLet us see importance of different features in RF and Boosts\n\n\nFrom all four fig we can see the consistency of them. \n\nAlthough the importance order are not exactly the same,**\"popularity\"** and** \"house\" **matters the most which is not a surprise to us. From the point of literary creating, main characters are put much more attention to than others. So it's easy to figure out what author--George.R.R.Martin are trying to express on the main characters he created.**\"House\" **is very importance to differ fate of people without doubt,people's fate are naturally bond with the rise or fall of a great house.\n\n**\"Title\",\"age\" ,\"culture\"**and**\"book5\" **also matters. It's believed noble,elder and appear later in chapters are more likely to survive,you can find related research in hottest kernels if you want.","401fa15a":"**7.LogisticRegression with Cross Validation**\n\n","1eee9f11":"This kernel is composed of 2 parts: \n\n**1. Character Death Prediction**       **2.Battle Analysis**\n\nIf you are looking for the Battle Analysis,you can skip the former part.\n\nLet's start the prediction now.","fe475914":"**Predicting**\n\nPredicting the death of characters by creating 10 different models of Machine Learning","3e8ec5ea":"**Fill NaN**\n\nOne battle lose many values,try to fill it with field knowledge : )\n\nA few battles are fight with no king or in nowhere, fill it up.","b42c8128":"**1.RandomForest**\n\n\n\n\n\n","87b26a32":"Now we have tried 10 different ML algorithms and 1 network unit.\n\nLet us do some **bagging** and **boosting**!","3c8e469e":"**6.NaiveBayes Gaussian**\n\n","97f03641":"**5.kNN**\n\n","a09172e4":"**10.GradientBoosting**\n\n","6390b13f":"**GradientBoost**\n\n","0c4c37c2":"**2.DecisionTree**\n","e4fc7475":"**Data Analysis**","a6202849":"**XGBoost**\n\nThe cross validated score for XGBoost is: 0.7449342115910598","3ba7b360":"Data cleansing\n\nThis part is from https:\/\/www.kaggle.com\/shaildeliwala\/exploratory-analysis-and-predictions\nby Shail Deliwala.\n\n**1)  Culture Induction**: Integrate similar culture into one single value.\n\n**2) Drop**: Drop data worthless for prediction like \"name\",\"dataOfBirth\".\n\n**3) Factorization**: Transform discrete data like \"title\",\"mother\",\"culture\" into numerical data.\n\n**4) Fill NaN**:Fill null value with -1.","118452b8":"**1. Character Death Prediction**\n\n**Some necessary libs & Read file**","6c1ec14a":"**Add Feature **\n\nCompute battle size for statistics.","da2a1bb3":"**AdaBoost**\n\n","ca0e8d35":"Here is a try of **softmax** unit on** tensorflow**.\nI am not familar with tf, so this is only an attempt.\n\n","266452ef":"**Feature Correlation**\n\nSee the correlation of features on heatmap\n","d557d5b6":"**Bagged Decision Tree**\n\n","b7211964":"Now we have go through the complete predicting\n\nComing next is the **battle analysis**.\n\n\n\n\n---------------------------------------------------------- **Battle Analysis** ------------------------------------------------------------\n\n\n\n\n**Some necessary lib & Read file**","76b02e17":"**9.RandomForest with CrossValidation**\n\n","d1c71b6d":"**4.LogisticRegression**\n\n","6481fc60":"**3.SVC**\n\n","70539df5":"That's all process of Battle Analysis.\n\n\nThanks if you read all the way through,leave a message if you have any question"}}