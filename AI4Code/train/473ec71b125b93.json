{"cell_type":{"da58b8b0":"code","26ca305a":"code","e4bf9ca9":"code","5b3cb592":"code","ac695a06":"code","cb2dd105":"code","da9d323b":"code","cc511a2d":"code","7e7d01d1":"code","0d9a8442":"code","221688ed":"code","b327faca":"code","3a4bfc6b":"code","7b9d29af":"code","3f43b4b1":"code","3b73637a":"code","148a7454":"code","7129367b":"code","e992e19d":"code","d77ecf3b":"code","562f20a4":"code","05c86891":"code","a5f21a29":"code","ec7b2ac0":"code","3a7219b8":"code","8a611cbb":"code","fbf554f2":"code","74dc8cb2":"code","d66939ec":"code","2a169c0e":"code","d97d25ff":"markdown","f46b073c":"markdown","1c59cb02":"markdown","36561230":"markdown","fb4ffa3d":"markdown","c072cb08":"markdown","821d6d31":"markdown","c1a477c1":"markdown","cd7a9afc":"markdown","90dbb099":"markdown","9f6edb63":"markdown","a6c4f805":"markdown","db8b148a":"markdown","929aa622":"markdown","f463a4d5":"markdown","c18c9eaa":"markdown","37765868":"markdown","e64f5fbd":"markdown","916da8c7":"markdown","ccec887b":"markdown","dfa74dec":"markdown","65beffa2":"markdown","d33ed2f3":"markdown","e087f145":"markdown","5d5090c4":"markdown","a56121db":"markdown","edb12e40":"markdown","0e8e71dd":"markdown","6e2a75e1":"markdown","af869949":"markdown","552d2339":"markdown","e1e87fcf":"markdown","9fc5c6e2":"markdown","8c6e0ef9":"markdown","a9ddcfc6":"markdown","63e8d76b":"markdown","110d9a30":"markdown","cc24dd0b":"markdown","b8a07d9e":"markdown","63c43ba5":"markdown","6fd099ca":"markdown","596daefc":"markdown","9d863736":"markdown","8df414e8":"markdown","73f09e08":"markdown","f0f25292":"markdown","bd312570":"markdown","2dcb308e":"markdown"},"source":{"da58b8b0":"%%capture \n# To get the latest version of W&B\n!pip install wandb --upgrade\n!pip install tensorflow-io","26ca305a":"import tensorflow as tf\nimport tensorflow_io as tfio\n\nimport os\nos.environ['WANDB_SILENT'] = \"true\"\nimport re\nimport gc\nimport glob\nimport wandb\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n%matplotlib inline\n\nimport IPython.display as ipd\n\n# Map libraries\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\nfrom kaggle_secrets import UserSecretsClient\n\n# Audio specific imports\nimport librosa as lb\nimport librosa.display\n\n# W&B login\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\nwandb.login(key=wandb_api)","e4bf9ca9":"METADATA_FILE_PATH = '..\/input\/birdclef-2021\/train_metadata.csv'\nmetadata_df = pd.read_csv(METADATA_FILE_PATH)\nmetadata_df.head(5)","5b3cb592":"# Load the file as W&B artifact. \nrun = wandb.init(project='birdclef', group='EDA')\nartifact = wandb.Artifact('train-metadata', type='dataset')\nartifact.add_file(METADATA_FILE_PATH)\nrun.log_artifact(artifact)\nrun.finish()","ac695a06":"# Reference: https:\/\/www.kaggle.com\/shahules\/bird-watch-complete-eda-fe\n# Unique eBird codes\nspecies = metadata_df['primary_label'].value_counts()\n\n# Make bar chart\nfig = go.Figure(data=[go.Bar(y=species.values, x=species.index)],\n                layout=go.Layout(margin=go.layout.Margin(l=0, r=0, b=10, t=50)))\n\n# Show chart\nfig.update_layout(title='Number of traning samples per species')\nfig.show()","cb2dd105":"plt.figure(figsize=(16, 6))\nax = sns.countplot(x = metadata_df['rating'], palette=\"hls\", order = metadata_df['rating'].value_counts().index)\n\nplt.title(\"Sound quality rating\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.xlabel(\"\");","da9d323b":"# Reference: https:\/\/www.kaggle.com\/andradaolteanu\/birdcall-recognition-eda-and-audio-fe\n# SHP file\nworld_map = gpd.read_file(\"..\/input\/world-shape-file\/99bfd9e7-bb42-4728-87b5-07f8c8ac631c2020328-1-1vef4ev.lu5nk.shp\")\n\n# Coordinate reference system\ncrs = {\"init\" : \"epsg:4326\"}\n\n# Lat and Long need to be of type float, not object\ndata = metadata_df[metadata_df[\"latitude\"] != \"Not specified\"]\ndata[\"latitude\"] = data[\"latitude\"].astype(float)\ndata[\"longitude\"] = data[\"longitude\"].astype(float)\n\n# Create geometry\ngeometry = [Point(xy) for xy in zip(data[\"longitude\"], data[\"latitude\"])]\n\n# Geo Dataframe\ngeo_df = gpd.GeoDataFrame(data, crs=crs, geometry=geometry)\n\n# Create ID for species\nspecies_id = geo_df[\"primary_label\"].value_counts().reset_index()\nspecies_id.insert(0, 'ID', range(0, 0 + len(species_id)))\n\nspecies_id.columns = [\"ID\", \"primary_label\", \"count\"]\n\n# Add ID to geo_df\ngeo_df = pd.merge(geo_df, species_id, how=\"left\", on=\"primary_label\")\n\n# === PLOT ===\nfig, ax = plt.subplots(figsize = (16, 10))\nworld_map.plot(ax=ax, alpha=0.4, color=\"grey\")\n\npalette = iter(sns.hls_palette(len(species_id)))\n\nfor i in range(264):\n    geo_df[geo_df[\"ID\"] == i].plot(ax=ax, markersize=20, color=next(palette), marker=\"o\", label = \"test\");","cc511a2d":"date_uploaded = metadata_df['date'].apply(lambda x: x.split('-')[0])\n\nplt.figure(figsize=(16, 6))\nax = sns.countplot(x=date_uploaded.values, palette=\"hls\")\n\nplt.title(\"Audio Files Registration per Year Made\", fontsize=16)\nplt.xticks(rotation=90, fontsize=13)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");","7e7d01d1":"TRAIN_SOUNDSCAPE = '..\/input\/birdclef-2021\/train_soundscape_labels.csv'\ntrain_soundscape_df = pd.read_csv(TRAIN_SOUNDSCAPE)\ntrain_soundscape_df.head()","0d9a8442":"train_soundscape_df['birds'].value_counts()","221688ed":"TEST = '..\/input\/birdclef-2021\/test.csv'\ntest_df = pd.read_csv(TEST)\ntest_df.head()","b327faca":"SHORT_TRAIN = '..\/input\/birdclef-2021\/train_short_audio\/'\nprint(f'Number of unique bird species: {len(os.listdir(SHORT_TRAIN))}')","3a4bfc6b":"recordings_per_label = {'species_id': [], \n                        'num_audio': []}\n\nfor label in os.listdir(SHORT_TRAIN):\n    num_recordings = len(os.listdir(SHORT_TRAIN+label))\n    recordings_per_label['species_id'].append(label)\n    recordings_per_label['num_audio'].append(num_recordings)\n        \nrecordings_per_label = pd.DataFrame.from_dict(recordings_per_label)\n\nrun = wandb.init(project='birdclef', group='EDA')\ndata = [[label, val] for (val, label) in sorted(zip(recordings_per_label.num_audio.values, recordings_per_label.species_id.values))[::-1]]\ntable = wandb.Table(data=data, columns = [\"species_id\", \"num_audio\"])\nwandb.log({\"recordings_per_label\" : wandb.plot.bar(table, \"species_id\", \"num_audio\",\n                               title=\"Number of recordings per label\")})\nrun.finish()\n\n# display W&B run page\nrun","7b9d29af":"num_data = np.sum(recordings_per_label['num_audio'])\nprint(f'Number of training data: {num_data}')","3f43b4b1":"NUM_AUDIO_PER_LABEL = 8 # \naudio_per_label = {}\n\nfor label in os.listdir(SHORT_TRAIN):\n    # add new key (label) to dict\n    audio_per_label[label] = []\n    # get NUM_AUDIO_PER_LABEL audio filenames per label\n    audio_filenames = random.sample(os.listdir(SHORT_TRAIN+label), NUM_AUDIO_PER_LABEL)\n    # append path to that file\n    for audio_filename in audio_filenames:\n        audio_per_label[label].append(SHORT_TRAIN+label+'\/'+audio_filename)\n        \n# We will use Weights and Biases to visualize audio waveforms and listen to bird sounds.\nrun = wandb.init(project='birdclef', group='EDA')\nfor label, audio_paths in audio_per_label.items():\n    audio_arr = []\n    sr_arr = []\n    for audio_path in audio_paths:\n        audio, sr = lb.load(audio_path)\n        audio_arr.append(audio)\n        sr_arr.append(sr)\n        \n    # log audio data for each label per step. \n    wandb.log({'audio-samples': [wandb.Audio(audio, caption=f'{label}', sample_rate=sr) \n                                             for audio, sr in zip(audio_arr, sr_arr)]})\n\nrun.finish()\n\n# display W&B run page\nrun","3b73637a":"# A sample audio. Change species_id to select different label\nsample_audio = audio_per_label['caltow'][3]\n# Load unnormalized audio\naudio, sr = lb.load(sample_audio)\n# Normalize audio\nnorm_audio = librosa.util.normalize(audio)\n\nprint(f'The shape of audio: {audio.shape}; sampling rate: {sr}; audio duration: {audio.shape[0]\/sr} s')\n\nrun = wandb.init(project='birdclef', group='EDA')\nwandb.log({'audio_sample': [wandb.Audio(audio, caption=f'Audio Sample', sample_rate=sr)]})\nwandb.log({'normalized_audio-sample': [wandb.Audio(norm_audio, caption=f'Normalized Audio Sample', sample_rate=sr)]})\nrun.finish()\n\nrun","148a7454":"position = tfio.experimental.audio.trim(audio, axis=0, epsilon=0.1)\ntrimmed_audio = audio[position[0]:position[1]]\n\nrun = wandb.init(project='birdclef', group='EDA')\nwandb.log({'Trimmed': [wandb.Audio(trimmed_audio, caption=f'Trimmed', sample_rate=sr)]})\nrun.finish()\n\nrun","7129367b":"audio_clips = []\naudio_time = len(audio)\/\/sr\nprint(f'The duration of audio is: {audio_time}')\n\nstart_sample = 0\nend_sample = sr*5 # sampling rate is number of samples per second. \n\nfor i in range(audio_time\/\/5):\n    audio_clips.append(audio[start_sample:end_sample])\n    start_sample = end_sample\n    end_sample+=sr*5\n    \nrun = wandb.init(project='birdclef', group='EDA')\nfor i, audio_clip in enumerate(audio_clips):\n    # Trim audio\n    position = tfio.experimental.audio.trim(audio_clip, axis=0, epsilon=0.1)\n    trimmed_audio = audio_clip[position[0]:position[1]]\n    \n    # Log clipped and trimmed audio\n    audio_arr = [audio_clip, trimmed_audio]\n    captions = ['Clipped', 'Trimmed']\n    \n    wandb.log({f'clipped_vs_trimmed': [wandb.Audio(aud, caption=f'{caption}', sample_rate=sr)\n                                              for aud, caption in zip(audio_arr, captions)]})\n\nrun.finish()\n\nrun","e992e19d":"fade = tfio.experimental.audio.fade(trimmed_audio, fade_in=100000, fade_out=200000, mode=\"exponential\")\n\nfig, ax = plt.subplots(2, figsize = (20, 8), dpi=120)\nfig.suptitle('Original Vs Trimmed', fontsize=16)\nlb.display.waveplot(trimmed_audio, sr=sr, ax=ax[0])\nlb.display.waveplot(fade.numpy(), sr=sr, ax=ax[1])\nplt.show()","d77ecf3b":"# compute fft\nfft = np.fft.fft(audio)\n# compute frequency \nfreq = np.fft.fftfreq(audio.size, 1\/sr)\nfreq = freq[:len(freq)\/\/2]\nprint(f'Max frequency in the audio: {freq[-1]}. Obviously it is going to be half of sampling rate')\n# get the magnitude\nmag_fft = abs(fft)\n# Remember 2nd half of the fft is repeated.\nmag_fft = mag_fft[:len(mag_fft)\/\/2]\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 8), dpi=120);\nfig.suptitle('Fast Fourier Transform')\nax[0].plot(audio)\nax[0].set_xlabel('Time', fontsize=16);\nax[0].set_ylabel('Amplitude', fontsize=16);\n\nax[1].plot(freq, mag_fft)\nax[1].set_xlabel('Freq(Hz)', fontsize=16);\nax[1].set_ylabel('Power', fontsize=16);","562f20a4":"# Parameters\nn_fft = 2048\nhop_length = 512","05c86891":"# Short-time Fourier transform (STFT)\nS = np.abs(lb.stft(audio, n_fft=n_fft, hop_length=hop_length))\n# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\nS_to_DB = librosa.amplitude_to_db(S, ref=np.max)\n\n# Plot spectrogram\nplt.figure(figsize=(16,4))\nlb.display.specshow(S_to_DB, y_axis='linear', sr=sr, hop_length=hop_length,\n                   x_axis='time') # Note linear y axis\nplt.title('Linear-Frequency Power Spectrogram');\nplt.colorbar();","a5f21a29":"# Short-time Fourier transform (STFT)\nS = np.abs(lb.stft(audio, n_fft=n_fft, hop_length=hop_length))\n# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\nS_to_DB = librosa.amplitude_to_db(S, ref=np.max)\n\n# Plot spectrogram \nplt.figure(figsize=(16,4))\nlb.display.specshow(S_to_DB, y_axis='log', sr=sr, hop_length=hop_length,\n                   x_axis='time') # Note logarithmic yaxis\nplt.title('Log-Frequency Power Spectrogram');\nplt.colorbar();","ec7b2ac0":"# Short-time Fourier transform (STFT)\nCQT_note = np.abs(lb.cqt(audio, sr=sr, hop_length=hop_length))\n# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\nCQT_note = librosa.amplitude_to_db(CQT_note, ref=np.max)\n\n# Plot spectrogram\nplt.figure(figsize=(16,4))\nlb.display.specshow(CQT_note, y_axis='cqt_note', sr=sr, hop_length=hop_length,\n                   x_axis='time') # Note cqt_note y axis\nplt.title('Constant-Q (Note) Power Spectrogram');\nplt.colorbar();","3a7219b8":"# Short-time Fourier transform (STFT)\nCQT_note = np.abs(lb.cqt(audio, sr=sr, hop_length=hop_length))\n# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\nCQT_note = librosa.amplitude_to_db(CQT_note, ref=np.max)\n\n# Plot spectrogram \nplt.figure(figsize=(16,4))\nlb.display.specshow(CQT_note, y_axis='cqt_hz', sr=sr, hop_length=hop_length,\n                   x_axis='time') # Note cqt_hz y axis\nplt.title('Constant-Q (Hz) Power Spectrogram');\nplt.colorbar();","8a611cbb":"Tgram = lb.feature.tempogram(y=audio, sr=sr)\n\n# Plot spectrogram\nplt.figure(figsize=(16,4))\nlb.display.specshow(Tgram, y_axis='tempo', sr=sr, hop_length=hop_length,\n                   x_axis='time') # Note cqt_hz y axis\nplt.title('Tempogram with BPM Markers');\nplt.colorbar();","fbf554f2":"C = librosa.feature.chroma_cqt(y=audio, sr=sr)\n\n# Plot spectrogram\nplt.figure(figsize=(16,4))\nlb.display.specshow(C, y_axis='chroma', sr=sr, hop_length=hop_length,\n                   x_axis='time') # Note cqt_hz y axis\nplt.title('Chromagram with Pitch Class');\nplt.colorbar();","74dc8cb2":"def get_path(label, filename):\n    return '..\/input\/birdclef-2021\/train_short_audio\/'+f'{label}\/{filename}'\n\n# Dataframe audio filenames with rating 4 or more.\nmetadata_tmp_df = metadata_df.loc[metadata_df['rating'] > 4]\nmetadata_tmp_df.loc[:, 'kaggle_path'] = metadata_tmp_df.apply(lambda row: get_path(row['primary_label'],\n                                                                            row['filename']), axis=1)\n\n# Get random samples for each label.\naudio_per_label = {}\n\nfor label in os.listdir(SHORT_TRAIN):\n    tmp = metadata_tmp_df.loc[metadata_tmp_df['primary_label'] == label]\n    tmp = tmp.sample(n=1, axis=0, replace=True)\n    audio_per_label[label] = tmp.kaggle_path.values[0]","d66939ec":"# We will use Weights and Biases to visualize different spectrograms.\nrun = wandb.init(project='birdclef', group='EDA')\nc = 0\nfor label, audio_path in audio_per_label.items():\n    # Image name\n    img_name = audio_path.split('\/')[-1].split('.')[0]\n\n    # Load audio\n    audio, sr = lb.load(audio_path)\n\n    # Compute spectrogram\n    # Short-time Fourier transform (STFT)\n    S = np.abs(lb.stft(audio, n_fft=n_fft, hop_length=hop_length))\n    # Convert an amplitude spectrogram to Decibels-scaled spectrogram.\n    S_to_DB = librosa.amplitude_to_db(S, ref=np.max)\n\n    # Save as spectrogram\n    fig = plt.figure(figsize=(16,4))\n    lb.display.specshow(S_to_DB, y_axis='linear', sr=sr, hop_length=hop_length,\n                       x_axis='time') # Note linear y axis\n    plt.xticks([]); plt.yticks([]); plt.xlabel(''); plt.ylabel('');\n    plt.savefig(f'{img_name}.png');\n    plt.close(fig)\n\n    # log audio data for each label per step. \n    wandb.log({\"linear-power-spectrogram\": [wandb.Image(f'{img_name}.png', caption=f'{label}')]})\n    \n    c+=1\n    if c==20:\n        break\n\nrun.finish()\n\n# display W&B run page\nrun","2a169c0e":"sample_audio = audio_per_label['caltow']\naudio, sr = lb.load(sample_audio)\n\nS = np.abs(lb.stft(audio, n_fft=n_fft, hop_length=hop_length))\nS_to_DB = librosa.amplitude_to_db(S, ref=np.max)\n\ncent = lb.feature.spectral_centroid(S=S)\ntimes = lb.times_like(cent)\n\nfig, ax = plt.subplots(figsize=(16,4))\nlibrosa.display.specshow(S_to_DB, y_axis='linear', x_axis='time', ax=ax)\nax.plot(times, cent.T, label='Spectral centroid', color='w')\nax.legend(loc='upper right');\nax.set(title='Log Power spectrogram');","d97d25ff":"> \ud83d\udccc Many segments are associated with `nocall`. `rucwar`, `bobfly1`, etc are commonly found. \n> \ud83d\udccc There are segments with two or more birds volcalization found. **We thus need to build a multi-label classifier**.","f46b073c":"> \ud83d\udccc Most of the recordings were uploaded in the year between 2012 and 2020. <br>\n> \ud83d\udccc Also note that some of the years are wrong (ex: 2104, 0199, etc).","1c59cb02":"> \ud83d\udccc Note how the initial and final silent waveform is removed. It might be useful preprocessing step after getting 5 second clips.","36561230":"### Constant Q (Note) Power Spectrogram","fb4ffa3d":"Reference: https:\/\/www.kaggle.com\/stefankahl\/birdclef2021-exploring-the-data\n> \ud83d\udccc `row_id`: Unique identifier of a 5-second segment of each soundscape file. <br>\n> \ud83d\udccc `site`: Recording site of the soundscape data. In this competition, 4 different sites (COL = Colombia, COR = Costa Rica, SNE = Sierra Nevada, SSW = Sapsucker Woods) are included. <br>\n> \ud83d\udccc `audio_id`: Identifier used to reference audio recordings. Filenames contain the file ID, recording site and recording date (yyyymmdd). <br>\n> \ud83d\udccc `seconds`: End time of the 5-second segment for which this entry states the label. <br>\n> \ud83d\udccc `birds`: primary label (i.e., eBird code) of the audible species of this segment. \u201cnocall\u201d references a segment without any bird vocalization. Segments can have more than one bird, in that case, eBird codes are separated by space. \u201cnocall\u201d can never appear together with other codes.\n","c072cb08":"#### What quality of audio recording available?","821d6d31":"# WORK IN PROGRESS :D","c1a477c1":"#### Let's see the effect of trimming on 5 second clips of the audio. ","cd7a9afc":">  \ud83d\udccc Click on the \u2699\ufe0f in the linear-power-spectrogram chart above to visualize the spectrogram per label. Note that I have only logged spectrogram for 20 labels.","90dbb099":"### Chromagram with Pitch Classes","9f6edb63":"## `train_metadata.csv`","a6c4f805":"## `test.csv`","db8b148a":"#### Spectral Centroid\n\nThe spectral centroid indicates at which frequency the energy of a spectrum is centered upon or in other words It indicates where the \u201d center of mass\u201d for a sound is located.","929aa622":"#### When was the audio uploaded?","f463a4d5":"### Tempogram with BPM markers","c18c9eaa":"#### What's the distribution of primary labels?","37765868":"(\u261d\ufe0f [W&B Run Page](https:\/\/wandb.ai\/ayush-thakur\/birdclef\/runs\/2713uqst?workspace=user-ayush-thakur))\n\n> \ud83d\udccc Note: The dashboard above have 8 samples of audio for each label. <br>\n> \ud83d\udccc Pro-tip 1: Click on the \u2699\ufe0f in the `audio-samples` chart above. There will be total of 397 steps, where each step represents an unique label. <br>\n> \ud83d\udccc Pro-tip 2: The audio will appear for the selected step (label). Each audio player's caption is the label name. \n\nYou can visualize the waveform of the audio beside listening to the music with just 3 lines of code. **I highly recommend spending time listening to the sample audio.******\n\n\ud83d\udc40 Some quick observations:\n* The audio duration varies from few seconds to few minutes.\n* The audio is noisy with the sound of rain, insects, humans talking, wind blowing, etc. \n* In some audio the sound of bird is coming from far off distance. ","e64f5fbd":"#### Number of training data","916da8c7":"#### Number of bird species","ccec887b":"> \ud83d\udccc Note that this constrained each signal between 0 and 1. Not sure if it would be a great idea to normalize the audio like this. Here's a short [reddit post](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/4ea0m7\/audio_normalizationpreprocessing_before\/). ","dfa74dec":"## `train_soundscape_labels.csv`","65beffa2":"**TL;DR for `train_short_audio`**\n\n**Usage**: To train fine-grained multi-class (label) audio classifier. <br>\n**Num species**: 397 <br>\n**Num training samples**: 62874 <br>\n**Class-imbalance**: Yes","d33ed2f3":"# \ud83d\udd08 2.  The Audio Files","e087f145":"#### Most commom birds vocalization found?","5d5090c4":"> \ud83d\udccc `n_fft=2048` samples, corresponds to a physical duration of 93 milliseconds at a sample rate of 22050 Hz.","a56121db":"# \ud83c\udfc6 3. Audio Features & Preprocessing","edb12e40":"> \ud83d\udccc [](http:\/\/)This Wikipedia page might be a good read. A recorded audio may be gradually reduced to silence at its end (fade-out), or may gradually increase from silence at the beginning (fade-in). Again I am not sure if it's going to be useful.","0e8e71dd":"#### Where are the birds located?","6e2a75e1":"### Listen to Audio\n\nWe will use Weights and Biases to visualize audio waveforms and listen to bird sounds.","af869949":"> \ud83d\udccc Note that while computing FFT the relevant information is in the first half of the array. The other half is simply repeated in inverse indices order. <br>\n> \ud83d\udccc There are some major regions of frequency concentration that too in high frequency region. <br>\n> \ud83d\udccc Note that there is numerical difference while computing FFT using TensorFlow and Numpy. For training a neura network it might not be an issue.","552d2339":"### About Weights and Biases\n\nThink of W&B like GitHub for machine learning models. With a few lines of code, save everything you need to debug, compare and reproduce your models \u2014 architecture, hyperparameters, model weights, GPU usage, and even datasets and predictions.\n\n* Create an account on https:\/\/wandb.ai.\n* Input your personal API token key to login (mine is added as [Kaggle Secrets](https:\/\/www.kaggle.com\/product-feedback\/114053))\n","e1e87fcf":"## Trim the noise","9fc5c6e2":"## Audio Normalization","8c6e0ef9":"## Spectrogram\n\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. ","a9ddcfc6":"> \ud83d\udccc Click on the \u2699\ufe0f in the clipped_vs_trimmed chart above. Each step is a unique pair of audio clip and it's trimmed counterpart. \n> \ud83d\udccc Note: By trimming we are losing on the background sound but there's no control over what is lost. Thus this might not be a good strategy to be applied on 5 second clips.","63e8d76b":"## Fast Fourier Transform","110d9a30":"#### Number of recordings per label","cc24dd0b":"> \ud83d\udccc Thankfully most of the audio is high quality. ","b8a07d9e":"# \ud83d\udc26 About the competition\n\n ### \ud83e\udd45 Goal\n \n **Indentify the species of the birds given raw audio in the wild.**\n \n ### \ud83e\udde0 Understanding the given data\n \n **Training data:**\n * `train_short_audio`: Directory of short recordings of individual bird calls generously uploaded by users of [xenocanto.org](https:\/\/www.xeno-canto.org\/). **This data will be used to train fine-grained audio classifier**. The audio recordings are downsampled to 32kHz and are in [ogg format](https:\/\/en.wikipedia.org\/wiki\/Ogg).\n * `train_soundscapes`: Directory of audio files that are comparable to the data that we will encounter in the test set. **This is the raw audio that's mentioned in the goal**. The recordings are are all roughly ten minutes long and in the ogg format. \n * `train_metadata.csv`: Wide range of metadata provided for the training data. \n * `train_soundscape_labels.csv`: This `csv` can be used to build the inference pipeline. \n \n**Testing data:**\n * `test_soundscapes`: Directory of recordings to be used for scoring. There are approximately 80 recordings **during submission** that will be will be roughly 10 minutes long and in ogg audio format.\n * `test.csv`: Same tabular information as `train_soundscape_labels.csv`.\n \n### Evaluation \n\nIn the `submission.csv`, for each `row_id\/time window`, you need to provide a space delimited list of the set of unique birds that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code `nocall`.\n\nThe submissions will be evaluated based on their row-wise micro averaged [F1 score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html).","63c43ba5":"### Let's look at the spectrogram for every class using W&B.\n\n**We will visualize linear-frequency power spectrogram for audio with rating 4 and more.**","6fd099ca":"## Fade In and Fade Out","596daefc":"> \ud83d\udccc Most of the audio recordings come from North and South America. A fair share of recordings are from Europe.","9d863736":"# \ud83d\udcc1 1. The CSV Files","8df414e8":"(\u261d\ufe0f [W&B Run Page](https:\/\/wandb.ai\/ayush-thakur\/birdclef\/runs\/1grwh85p?workspace=user-ayush-thakur))\n> \u26a0\ufe0f Disclaimer: Since there are too many labels, the `specied_id` (y-axis) looks clumsy. <br>\n> \ud83d\udccc Pro-tip 1: Scroll over the bar chart to look at the number of recordings per `species_id`. <br>\n> \ud83d\udccc Pro-tip 2: Click on the \u270f\ufe0f (Edit panel) icon in the chart to visualize the expanded version of the bar chart. <br>\n> \ud83d\udccc Pro-tip 3: In the `recordings_per_label_table` you can sort the columns in ascending or descending order by clicking on the column name. \n\nThere's significant class imbalance. Species `crfpar` and `stvhum2` got only 8 audio files. While 12 labels got 500 audio files each. Play with the W&B dashboard above to get more insight.","73f09e08":"### Linear Power Spectrogram","f0f25292":"> \ud83d\udccc `primary_label`: The primary bird sound that can be heard in the recording. <br>\n> \ud83d\udccc `secondary_label`: Extra birds sounds present in the recording. <br>\n> \ud83d\udccc `type`: The type of bird sound. <br>\n> \ud83d\udccc `latitude` and `longitude`: Location where the recording was done. <br>\n> \ud83d\udccc `scientific_name`: The scientific name of the bird species. <br>\n> \ud83d\udccc `common_name`: The bird known in common language. <br>\n> \ud83d\udccc `author`: Individual's name who recorded (possibly) and uploaded the audio. <br>\n> \ud83d\udccc `date`: The date the audio was recorded (possibly) and uploaded. <br>\n> \ud83d\udccc `filename`: The name of the audio file. <br>\n> \ud83d\udccc `license`: The license associated with that recording. <br>\n> \ud83d\udccc `rating`: The audio quality. <br>\n> \ud83d\udccc `time`: The time of the day the recording was uploaded. <br>\n> \ud83d\udccc `url`: The xenocanto.org url to d","bd312570":"### Constant Q (Hz) Power Spectrogram","2dcb308e":"### Log Power Spectrogram"}}