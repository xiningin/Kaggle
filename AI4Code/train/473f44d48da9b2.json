{"cell_type":{"4835d490":"code","d7745eb7":"code","78a28094":"code","634c442c":"code","56e6ca89":"code","7c98ec9f":"code","5372c380":"code","f45a22c6":"code","2cbabc80":"code","6ff55210":"code","9ea43e7b":"code","1f1fe840":"code","9decf2b5":"code","ffca33f6":"code","c5acf50a":"code","9c382db5":"code","2450accc":"code","786baf87":"code","df89e380":"code","cb2bee6f":"code","d8db7b7a":"code","ebb73456":"code","568b2ef2":"code","8c26bebc":"code","6e798c7c":"code","92595d91":"code","2af4a663":"code","36e9433c":"code","1df5e2e8":"code","1cd16b6a":"code","f3af63fa":"code","1a78ea0f":"code","7c4deb90":"code","1aaa1b87":"code","6344a80a":"code","2b0479b5":"code","b77023d5":"code","ea66640f":"code","17b25c03":"code","91ab5a77":"code","37d78599":"code","b7ced5ea":"code","f28c78a1":"code","7fc13779":"code","65561bf0":"code","1ac8068e":"code","da0656cc":"code","e142292f":"code","a215a449":"code","7dcbc17e":"markdown","e43912b4":"markdown","79b176a6":"markdown","67411cdb":"markdown","b97b39c7":"markdown","2fa79da4":"markdown","534b1419":"markdown","5a74e85e":"markdown","efdb72e0":"markdown"},"source":{"4835d490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7745eb7":"df = pd.read_csv('\/kaggle\/input\/ushealthinsurancedataset\/insurance.csv')\ndf.head()","78a28094":"from pandas_profiling import ProfileReport","634c442c":"ProfileReport(df)","56e6ca89":"df.info()","7c98ec9f":"df.sex = df.sex.astype('category')\ndf.smoker = df.smoker.astype('category')\ndf.region = df.region.astype('category')","5372c380":"df.info()","f45a22c6":"df = pd.get_dummies(df)\ndf.head()","2cbabc80":"X = df.drop('charges' , axis = 1)\nX.head()","6ff55210":"y = df['charges']\ny.head()","9ea43e7b":"from sklearn.model_selection import train_test_split","1f1fe840":"train_test_split?","9decf2b5":"# Splitting into training sets and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","ffca33f6":"!pip install lazypredict","c5acf50a":"from lazypredict.Supervised import LazyRegressor","9c382db5":"LazyRegressor?","2450accc":"lazy_model = LazyRegressor(verbose=0 , ignore_warnings=True , custom_metric=None)","786baf87":"all_models , all_predictions = lazy_model.fit(X_train , X_test , y_train , y_test)","df89e380":"all_models_dictionary = lazy_model.provide_models(X_train , X_test , y_train , y_test)","cb2bee6f":"pd.DataFrame(all_models).sort_values(by='RMSE',ascending=True)","d8db7b7a":"all_models_dictionary['GradientBoostingRegressor']","ebb73456":"all_models_dictionary['GradientBoostingRegressor']","568b2ef2":"from sklearn.ensemble import GradientBoostingRegressor","8c26bebc":"gbr_model = GradientBoostingRegressor(random_state=42)","6e798c7c":"gbr_model.fit(X_train , y_train)","92595d91":"y_pred = gbr_model.predict(X_test)","2af4a663":"# Checking Model metrics\nfrom sklearn.metrics import explained_variance_score","36e9433c":"explained_variance_score( y_test , y_pred )","1df5e2e8":"from joblib import dump\ndump(gbr_model , 'ClassicalMLInsuranceModel.joblib')","1cd16b6a":"from fastai.tabular.all import *","f3af63fa":"df.head()","1a78ea0f":"df.info()","7c4deb90":"list(df.columns)","1aaa1b87":"splits = RandomSplitter(valid_pct=0.2)(range_of(df))","6344a80a":"to = TabularPandas(\n    df , \n    procs = [ Categorify , FillMissing , Normalize ] , \n    cont_names = ['age',\n 'bmi',\n 'children',\n 'sex_female',\n 'sex_male',\n 'smoker_no',\n 'smoker_yes',\n 'region_northeast',\n 'region_northwest',\n 'region_southeast',\n 'region_southwest'] , \n    y_names = 'charges' , \n    splits =  splits\n)","2b0479b5":"dls = to.dataloaders(bs = 64)","b77023d5":"dls.show_batch()","ea66640f":"tabular_learner??","17b25c03":"learn = tabular_learner(\n    dls , \n    metrics = rmse\n)","91ab5a77":"learn","37d78599":"learn.lr_find()","b7ced5ea":"learn.fit_one_cycle(10 , 0.1 )","f28c78a1":"learn.show_results()","7fc13779":"X_test[:].iloc[1]","65561bf0":"learn.predict(X_test[:].iloc[1])","1ac8068e":"learn.save?","da0656cc":"!ls","e142292f":"learn.save('DeepLearningInsuranceModel')","a215a449":"!ls models","7dcbc17e":"# Data Preperation","e43912b4":"# ","79b176a6":"# As per EDA we can get general idea about data and it's correlations\n - age and charges have high correlation , That is understandable As age increases Insurance premium also increases.\n - A person being a smoker increases insurance premium charges , As smoking can cause health issues.\n - The data has no missing values , That's great\n - The data only has one duplicate row , we can take care of that in data preprocessing\n - Smoker feature is not equally distributed , 1064 Non Smokers are there with only 274 smokers , Being unbalanced data this has a possibility to bias the data , We can take care of that after seeing how our data model performs and do actions as needed.\n - Now we can prepare data for modeling","67411cdb":"# Classical Model has better RMSE Score than Deep Learning Model\n - GradientBoostedRegressor RMSE Score : 4521.28\n - Deep Learning Model RMSE Score : 5519.604004","b97b39c7":"# Now we need to try all Regresson models and check which one fits data better right ? There's a Quick and effective way to do this","2fa79da4":"# EDA Of Insurace Data","534b1419":"# GradientBoostingRegressor has the lowest RMSE score (it\u2019s able to fit the dataset the best out of all the potential models.)","5a74e85e":"# The best possible score is 1.0 for explained_variance_score , here we got 0.86 , Pretty good for a start","efdb72e0":"# Now Let's Dive Deep into Deep Learning"}}