{"cell_type":{"b43f5769":"code","039344b8":"code","01127111":"code","a4eccdd7":"code","0b5f2116":"code","54b40841":"code","845994ad":"code","d618c1f1":"code","2efba7d1":"code","c0c7acd8":"code","4cc63003":"code","67efd086":"code","79ec26ee":"code","39925bf7":"code","39bf6a1b":"code","10fdc60b":"markdown","55f8ba2c":"markdown","23feca5c":"markdown","4cb03699":"markdown","819aa70a":"markdown","b53e97ad":"markdown","974a2c6a":"markdown","e53649a4":"markdown","22caae37":"markdown","851a1253":"markdown","7071975d":"markdown","5c4134db":"markdown","47e6ca02":"markdown","bcbb907f":"markdown","13ec0f37":"markdown"},"source":{"b43f5769":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport os\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\n\n!pip install biosppy\n\n# Define the sampling rate\nfs=250\n\n# Define the Training and Test folders paths\ntraining_path = '\/kaggle\/input\/physiological-signals-processing-challenge-2122\/Training\/'\ntest_path = '\/kaggle\/input\/physiological-signals-processing-challenge-2122\/Test\/'\n\n# Load the Training CSV file with Pandas\ntraining_csv = pd.read_csv('\/kaggle\/input\/physiological-signals-processing-challenge-2122\/Training.csv')\n\n# Print some rows of the CSV\nprint(training_csv.head())\n\nprint(\"Number of episodes: \",len(training_csv))","039344b8":"# Let's load the data and store it in a list (this process can take a while). \n# We are going to use only a fraction in order to be this fast (101 recordings).\ntraining_signals = []\nfor i in training_csv['Id']:\n    training_signals.append(np.loadtxt(training_path+str(i)+'.txt',delimiter=',').T)\n    if i > 100:\n        break\n    \n# Check the number of records (should be 101)\nprint('The number of loaded records is '+str(len(training_signals)))\n\n# Now, we will convert the list into a 3-dimensional array, with shape = (number of records, number of signals, number of samples)\ntraining_signals = np.array(training_signals)\nprint('The final shape of the training signals matrix is '+str(np.shape(training_signals)))","01127111":"# Now, we will print the signals corresponding to the first record\nt = np.linspace(0,30,fs*30)\nplt.figure(figsize = (15,6))\nplt.subplot(121)\nplt.plot(t,training_signals[0,0,:])\nplt.xlim(0,5)\nplt.title('ECG signal')\nplt.ylabel('mV')\nplt.xlabel('Time (s)')\nplt.subplot(122)\nplt.plot(t,training_signals[0,1,:])\nplt.xlim(0,5)\nplt.title('Resp signal')\nplt.ylabel('l')\nplt.xlabel('Time (s)')\nplt.tight_layout()","a4eccdd7":"# Let's import some useful functions to get the QRS detection\nfrom biosppy.signals.ecg import hamilton_segmenter\nfrom biosppy.signals.ecg import ecg\n\n# QRS detection for the first ECG record\nr_peaks = hamilton_segmenter(training_signals[0,0,:],sampling_rate = fs)[0]\nplt.figure(figsize = (10,8))\nplt.plot(t,training_signals[0,0,:])\nplt.plot(r_peaks\/fs,training_signals[0,0,r_peaks],'r*')\nplt.xlim(0,10)\nplt.title('ECG signal')\nplt.ylabel('mV')\nplt.xlabel('Time (s)')\n\n# Now, we are going to get the mean and standard deviation of the R-R intervals (in ms) for this record\nrr_intervals = np.diff(r_peaks)\/250 * 1000 #ms\nmean_rr_interval_example = np.mean(rr_intervals)\nstd_rr_interval_example = np.std(rr_intervals)\n\nprint('The mean R-R interval for this record is '+str(mean_rr_interval_example)+' ms')\nprint('The standard deviation of the R-R intervals for this record is '+str(std_rr_interval_example)+' ms')","0b5f2116":"# We can obtain a complete analysis using ecg function from biosppy\nts, filtered, rpeaks, temp_ts, templates, hr_ts,hr = ecg(signal=training_signals[0,0,:], sampling_rate=fs, show=True)","54b40841":"# Now, let's compute the mean and standard deviation of R-R intervals \n# with the whole set of training signals\nrr_intervals_training_info = []\nfor i in range(0,training_signals.shape[0]):\n    rr_peaks = hamilton_segmenter(training_signals[i,0,:],sampling_rate = fs)[0]\n    rr_intervals = np.diff(rr_peaks)\/fs * 1000 \n    rr_intervals_training_info.append([np.mean(rr_intervals),np.std(rr_intervals)])\n\n# Let's convert the list into an array\nrr_intervals_training_info = np.array(rr_intervals_training_info)\n\n# Print the number of ECG recordings processed (it should be 101)\nprint('The number of processed recordings is '+str(len(rr_intervals_training_info)))","845994ad":"# Let's import a Gaussian NB detector from Scikit-Learn.\nfrom sklearn.naive_bayes import GaussianNB\n\n# Let's get the priors for each category\n\nP_h0 = np.mean(training_csv['Category'] == 0)\nP_h1 = np.mean(training_csv['Category'] == 1)\n\nprint('P_h0:',P_h0,'; P_h1:',P_h1)","d618c1f1":"# Let's build our Na\u00efve Bayes model\nnb_detector = GaussianNB(priors = [P_h0,P_h1])\n\n# Once we have our model, we should compute the matrices used as features and labels.\n# Features matrix (x_train) should have as shape (number of records,number of features).\n# In this Notebook, we only have two feature (mean and standard deviation of R-R intervals). \n# Therefore:\n\nx_train = np.copy(rr_intervals_training_info)\nprint(x_train.shape)\n\n# Labels vector (y_train) shape should be (number of records,).\n# It is very important to make sure that the order of labels in this vector\n# is the same as in the features matrix. For example, if the first record has\n# its corresponding features in the first row of the features matrix,\n# its label should be in the first position of the labels vector.\n\ny_train = training_csv['Category'].ravel()[0:len(x_train)]\nprint(y_train.shape)\n\n# Now, we can train our model and check its parameters.\nnb_detector.fit(x_train,y_train)\n\nprint(\"mean values n_classes, n_features\")\nprint(nb_detector.theta_)\n\nprint(\"variance values n_classes, n_features\")\nprint(nb_detector.sigma_)","2efba7d1":"# First, we have to list the filenames inside the Test folder. They will indicate the Id of each test record.\ntest_files = np.sort([int(os.path.splitext(filename)[0]) for filename in os.listdir(test_path)])\n\n# Then, we have to load each file. Take a look at the code used in the Training set.\n# note we are going to use only a fraction to speed up things (20 recordings).\ntest_signals = []\nfor i in range(len(test_files[:20])):\n    test_signals.append(np.loadtxt(test_path+str(test_files[i])+'.txt',delimiter=',').T)\n    \n# Check the number of records (it should be 20)\nprint('The number of loaded records is '+str(len(test_signals)))\n\n# Now, we will convert the list into a 3-dimensional array, with shape = (number of records, number of signals, number of samples)\ntest_signals = np.array(test_signals)\nprint('The final shape of the test signals matrix is '+str(np.shape(test_signals)))","c0c7acd8":"# Now, we will print the signals corresponding to the first record\nt = np.linspace(0,30,fs*30)\nplt.figure(figsize = (10,8))\nplt.subplot(121)\nplt.plot(t,test_signals[0,0,:])\nplt.xlim(0,5)\nplt.title('ECG signal')\nplt.ylabel('mV')\nplt.xlabel('Time (s)')\nplt.subplot(122)\nplt.plot(t,test_signals[0,1,:])\nplt.xlim(0,5)\nplt.title('Resp signal')\nplt.ylabel('l')\nplt.xlabel('Time (s)')\nplt.tight_layout()","4cc63003":"def get_rr_info(ECG,fs):\n    \"\"\" \n    Computes the mean and standard deviation of a single ECG record.\n    \n    Parameters\n    -----------\n    ECG: ECG recording (array)\n    fs: sampling rate (int).\n    \n    Return\n    ------\n    mean_RR: mean R-R interval (float). \n    std_RR: standard deviation of R-R intervals (float).\n    \"\"\"\n    rr_peaks = hamilton_segmenter(ECG,sampling_rate = fs)[0]\n    rr_intervals = np.diff(rr_peaks)\/250 * 1000\n    mean_rr = np.mean(rr_intervals)\n    std_rr = np.std(rr_intervals)\n    \n    return [mean_rr, std_rr]","67efd086":"# Now, let's compute the mean R-R intervals with the whole set of test signals\nrr_intervals_info_test = []\nfor i in range(0,test_signals.shape[0]):\n    rr_intervals_info_test.append(get_rr_info(test_signals[i,0,:],fs))\n\n# Let's convert the list into an array\nrr_intervals_info_test = np.array(rr_intervals_info_test)\n\n# Print the number of processed recordings (it should be 20)\nprint('The number of processed recordings is '+str(len(rr_intervals_info_test)))","79ec26ee":"# Features matrix (x_test) should have as shape (number of records,number of features).\n# In this Notebook example we have only 2 features stored in the rr_intervals_info_test, so...\nx_test = np.copy(rr_intervals_info_test)\n\n# Now, let's compute our predictions. At this point, please, make sure\n# that each row in y_test is correctly sorted (first row: first record;\n# second row: second record, etc.).\ny_hat_test = nb_detector.predict(x_test)","39925bf7":"from sklearn.metrics import f1_score\n# Let's compute the predictions using the training set feature matrix\ny_hat_train = nb_detector.predict(x_train)\n\n# Now, we can compute the training F1-score with the true labels and the predicted ones.\ntraining_f1_score = f1_score(y_train, y_hat_train,average='micro')\nprint('The obtained F1-Score for the training set is',training_f1_score)","39bf6a1b":"# To create solution we will first build a Pandas dataframe, where the\n# first column is the Id, and the second one, the predicted category.\n\n#note in order to be able to submit this solution file, you would need to use all the patients in test files. Please change the code accordingly.\ndf = pd.DataFrame({'Id': test_files[0:20], 'Category': y_hat_test})\ndf.to_csv('submission.csv',index = False)","10fdc60b":"In this Notebook example we will compute the mean and standard deviation of the R-R interval for each record and use them as features. Let's do it!","55f8ba2c":"As you can see... the performance of our model with the tested features in the training set is... quite bad. You will do it better ;)\n\n\nNow, we have almost arrived to the end of the process. Therefore... it is time to see the score we have obtained! We can do it directly from the Jupyter Notebook, by creating a CSV as an output file (the filename should be *submission.csv*). Here you can see an example:","23feca5c":"# How can we obtain the predictions for the test set?\nRemember that the Challenge is about doing predictions about unseen data, that is, the test set. Therefore, we have to perform the same procedure to:\n\n* Obtain the same features for the test set.\n* Predict to which of the categories each record belongs to.\n\nTherefore, we need to first load the test set recordings. Now, we have to iterate throughout the Test set folder:","4cb03699":"# Features Extraction: BioSPPy\n\nOne of the main aims of the Challenge is to obtain relevant features from the signals. To do that, we can use a module that can perform several biomedical signal processing. Among them, we can detect the QRS. You should check the module, for sure you can come up with new ideas to obtain some relevant features for the Challenge.\n\n[Biosppy Module](https:\/\/biosppy.readthedocs.io\/en\/stable\/)","819aa70a":"Once we have obtained the whole set of features (in our case, the mean and standard deviation of the ECG recordings), it is time to compute the predictions! What we will do is to create our feature matrix (y_test) and use the previously-trained Na\u00efve Bayes model to predict to which of the categories each record belongs to:\n\n$$P(H_1|\\boldsymbol{x}) \\mathop{\\gtrless}^{D_1}_{D_0}P(H_0|\\boldsymbol{x})$$","b53e97ad":"Once we have obtained the priors, we can build our Na\u00efve Bayes model.","974a2c6a":"# About this Challenge\nThe problem we want to address in this Challenge is the detection of apnea\/hypopnea events in Obstructive Sleep Apnea and Hypopnea Syndrome (OSA). This pathology is characterized by the partial or complete collapse of the upper airways during a certain period of time while sleeping. Depending on the degree of collapse and its duration, OSA events can be classified in:\n\n* Apnea: airflow drops a 90% (total collapse) for more than 10 seconds.\n* Hypopnea: airflow drops, at least, for a 30%, and it could include a drop of more than a 3% in the O2 saturation (SpO2).\n\nThe way to measure the severity of the pathology in patients who suffer OSA is based on the apnea-hypopnea index (AHI), which is a sum of all the events of both types suffered by the patient per hour of sleep.\n\nThe given data set was obtained from the MIT-BIH Polysomnographic Database, and it comprises labeled 30-second records. Although typical Polysomnographic records contains many types of physiological signals, in this case we will use just an ECG channel and a respiratory channel (obtained from a nasal thermistor). These records could belong to a normal respiration period, or an apnea\/hypopnea event.","e53649a4":"We don't know what could be the performance of our model with the Test set (we don't know the test labels, only Kaggle knows...), but one thing we could do is to see how our model fits to the Training set.\n\nTo do that, we will perform a prediction using the training set features (*x_train* matrix). Then, we will use the same performance metric as Kaggle, which is the F1-Score. This score can be computed using an implementation given by scikit-learn.","22caae37":"BioSPPy can also give us complete reports about the ECG... You can use that information to get more interesting features...","851a1253":"# How to build a Na\u00efve Bayes detector\n\nThis detector needs three elements:\n* *A priori* probabilities for each case (normal respiration vs apnea\/hypopnea).\n* Features to train the model.\n* Corresponding labels.\n\nFirst of all, let's compute the *a priori* probabilities of having normal respirations or apnea\/hypopnea events.","7071975d":"# PSF-PSP Challenge 2021-2022: how to deal with it\n\n## Grado en Ingener\u00eda Biom\u00e9dica - Biomedical Engineering Degree\n\n### Universidad Rey Juan Carlos\n\n\n### Authors\n\n#### \u00d3scar Barquero P\u00e9rez (<oscar.barquero@urjc.es>), Rebeca Goya Esteban (<rebeca.goyaesteban@urjc.es>), Miguel \u00c1ngel C\u00e1mara V\u00e1zquez (<miguelangel.camara@urjc.es>)\n\n<a rel=\"license\" href=\"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/\"><img alt=\"Licencia de Creative Commons\" style=\"border-width:0\" src=\"https:\/\/i.creativecommons.org\/l\/by-nc-sa\/4.0\/88x31.png\" \/><\/a><br \/>Este obra est\u00e1 bajo una <a rel=\"license\" href=\"http:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/\">licencia de Creative Commons Reconocimiento-NoComercial-CompartirIgual 4.0 Internacional<\/a>. ","5c4134db":"## Bayesian Decision Algorithm: Na\u00efve Bayes\n\nIn this notebook we are going to develop a very easy Na\u00efve Bayes detector to work on the Challenge. Na\u00efve Bayes is a MAP Bayes detector that assumes independence on the features (it is na\u00efve in that sense). We are going to use the implementation given by scikit-learn instead of our own implementation.\n\nA Bayesian MAP detector for binary classification has the following structure:\n\n$$P(H_1|\\boldsymbol{x}) \\mathop{\\gtrless}^{D_1}_{D_0}P(H_0|\\boldsymbol{x})$$\n\nThat is,\n\n$$P(\\boldsymbol{x}|H_1)P(H_1) \\mathop{\\gtrless}^{D_1}_{D_0}P(\\boldsymbol{x}|H_0) P(H_0)$$\n\nThe main difficulty of this detector is to be able to compute the likelihood, which is a conditional joint PDF:\n\n$$p(x_1,\\ldots,x_n|H_i)?????$$\n\nIn this point, we can use some assumptions to simplify our model. In this case, we will use a **na\u00efve** assumption about the relation between the features $x_i$. **What we will assume that the PDFs of the conditional features are independent**. Therefore:\n\n$$p(x_1,\\ldots,x_n|H_i)=p(x_1|H_i)p(x_2|H_i)\\cdots p(x_n|H_i)$$\n\nThe PDFs of each of the conditioned features are usually:\n* Binomial PDF, when the feature is binary (yes or no).\n* Multinomial PDF, when the feature has different levels (categorical variable).\n* Gaussian PDF, when the feature is numerical. Hey! Some transformations of the variable can be done in order to normalize it (log, etc.).\n\nFor example, let's suppose that the PDF of the j-th feature is a Gaussian PDF. Therefore, its equation will be:\n\n$$p(x_j|H_i)= \\frac{1}{\\sqrt{2\\pi\\sigma_j^2}}e ^{-1\/2\\frac{(x_j-mu_j)^2}{\\sigma_j^2}}$$\n\nIn this PDF we have two unknown parameters: $\\mu_j$, $\\sigma^2_{j}$.\n\nIn the training step, we will use the available data to estimate those parameters. To do that, we will assume that the input data are independent , so we can use the following maximum likelihood estimators:\n\n$$\\hat{\\mu}_j = \\frac{1}{N_{train}}\\sum^{N_{train}}_{k=1}x_{k,j}$$\n$$\\hat{\\sigma}^2_j = \\frac{1}{N_{train}}\\sum^{N_{train}}_{k=1}(x_{k,j}-\\hat{\\mu}_j)^2$$\n\nThe last parameter to be estimated are the *a priori* probabilities for each case. We can do it with the available training labels.","47e6ca02":"Now, it is time to get the same features, but for the test set. Maybe we can do it by creating a function...","bcbb907f":"If you go to the right-side panel, you will see a **Submit** option. Click there and... let's see what is your performance ;). You can also download the generated CSV file and do the submission manually.\n\nIf you use the whole test dataset, this version of the Notebook gives a score in Kaggle of 0.69. Using better features will dramatically improve the results, so... now it is your turn to try whatever you want :)\n\n**That's all folks!!** Good luck with your features obtention ;)","13ec0f37":"# First steps: let's load the training data\nLoading the data is, in this case, quite easy, since data files are just .txt organized in columns. We can use two different approaches to load the data:\n* First one: by iterating throughout the Training\/Test folders.\n* Second one: by using the CSV file, which contains the Training set filenames (and their corresponding labels).\n\nIn this part of the Notebook, we will use the second approach.\nLet's load some interesting packages and create function to extract the samples from a patient in the dataset!"}}