{"cell_type":{"7e95e691":"code","0d930c4f":"code","5dd07b5d":"code","6c9881ee":"code","cdc50dfe":"code","427259df":"code","508f8dac":"code","6ae2a4f8":"code","299cb86e":"code","c02b39a1":"code","46212259":"code","7979bb6c":"markdown","65956a0c":"markdown","94499cdf":"markdown","1351cecd":"markdown","e27090dd":"markdown","3efa480f":"markdown","6829fb9e":"markdown","cc16aa5e":"markdown","52a81a71":"markdown","770f3d2b":"markdown"},"source":{"7e95e691":"! conda install -y gdown","0d930c4f":"! gdown --id 1yypMeJQFpLrFsO9QuQJPYZ59oe5erO_w","5dd07b5d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.notebook import tqdm\nimport sklearn","6c9881ee":"mall_data = pd.DataFrame(pd.read_csv(\".\/Mall_Customers.csv\"))\n\nprint(\"Shape of mall data = \", mall_data.shape)","cdc50dfe":"mall_data.head()","427259df":"X = mall_data.iloc[:, [3, 4]].values\n\nprint(\"Shape of X = \", X.shape)\nprint(\"Type(X) = \", type(X))","508f8dac":"plt.figure(figsize  = (12, 8))\nplt.scatter(X[:, 0], X[:, 1], s = 100, c = 'black', label = 'Unclustered Data')\nplt.title(\"Data Points | Pre-Clustering\", fontsize = 18)\nplt.xlabel(\"x1 = Annual Income\", fontsize = 16)\nplt.ylabel(\"x2 = Spending Score\", fontsize = 16)\nplt.grid(True)\nplt.minorticks_on()\nplt.grid(which = \"major\", linestyle = \"-\", linewidth = '1.0', color = \"grey\")\nplt.grid(which = \"minor\", linestyle = \":\", linewidth = '0.5', color = \"black\")\nplt.legend()","6ae2a4f8":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = \"ward\")) # minimize variance\n\nplt.title(\"Dendrogram\")\nplt.xlabel(\"Customers\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","299cb86e":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = \"euclidean\", linkage = \"ward\")\ny_hc = hc.fit_predict(X)","c02b39a1":"print(y_hc)","46212259":"plt.figure(figsize = (12, 8))\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'green', label = 'cluster I')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'cluster II')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'brown', label = 'cluster III')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'black', label = 'cluster IV')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'violet', label = 'cluster V')\nplt.title(\"Hierarchical Clustering On Mall Data\", fontsize = 18)\nplt.xlabel(\"x1 = Annual Income\", fontsize = 16)\nplt.ylabel(\"x2 = Spending Income\", fontsize = 16)\nplt.grid(True)\n#plt.set_axisbelow(True)\n# Turn on the minor TICKS, which are required for the minor GRID\nplt.minorticks_on()\n# Customize the major grid\nplt.grid(which='major', linestyle='-', linewidth='1.0', color='grey')\n# Customize the minor grid\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.legend()","7979bb6c":"## Algorithm for Agglomerative Hierarchical Clustering : \n* Make each data point a single-point cluster -> We have N clusters.\n* Take two closest data points and make them one cluster -> Now, we have N-1 clustes.\n* Take the **two closest clusters** and merge them into one -> Now, we have N-2 clusters. Repeat this step until we are left with only one cluster.\n\n**The distance metric used for computing the distance is generaly Euclidean, however others may be used according to scenarios**. \n\nIllustration : \n\n![image.png](attachment:image.png) ","65956a0c":"In truth, Hierarchical Clustering(HC) is divided into two chapters : \n* Agglomerative Clustering\n* Divisive Clustering\n\nThe above figure will clearly aid you in understanding the primary difference.","94499cdf":"![image.png](attachment:image.png) ","1351cecd":"# Agglomerative Hierarchical Clustering : \n\n## Data Mining Track - Book I\n\n______________________________________________________________\nNext in the series : \n\nData Mining Track : Book II (Density Based Spatial Clustering with Noise [DBSCAN]) - **https:\/\/www.kaggle.com\/fireheart7\/dbscan**\n----------------------------------------\n\nAgglomerative Clustering is when we build clusters from bottom up. Think of many tiny bubbles floating on water and gradually attaching to each other, until they coalesce one giantic bubble. Similarly, at each iteration agglomerative clustering connects the nearest pair of clusters(starting with individual instances). \n\n*For those of you fellow readers familiar with the concept of Tree data structure, think of the previous paragraph as the case where we have a tree that branches for every pair of clusters, and continues to do so, until we get to leaves. These leaves are nothing but individual data points*. \n\nFor illustration:\n\n![image.png](attachment:image.png)","e27090dd":"## Distance Between Clusters\n\nSo imagine you have two clusters now. According to step III of the previous algorithm, we need to find the **two closest clusters**. But how to find the distance between them? Should we consider the centers of the two to evaluate the distance, or perhaps some point on the periphery? \n\nThis is a crucial question that needs to be answered. It's upto us what to consider, however many built in functions use centroids as far as I am aware.","3efa480f":"`What about Optmial Number of clusters?`\n\nIn general, to get the optimal number of clusters, we find the longest vertical line of the dendrogram, and take any threshold that crosses it.\n\n*A picture is worth a thousand words. This shall aid* : \n\n![image.png](attachment:image.png)","6829fb9e":"For demonstration purposes, we will clustering using only **Annual Income** and **Spending Score** feature. ","cc16aa5e":"**The way HC works is that it maintains a memory of how we went through this whole clustering process, starting with individual points to a massive single gigantic cluster in the end. This memory(which clustered merged when) is stored in something called a Dendrogram**.\n\nFor illustration : \n\n![image.png](attachment:image.png)\n\nSo, initially we merge A and B as one cluster. The question arises how we should signify that we just connected A and B. Well, to connect them we include a horizontal black line. But, wait! What about the height at which we should place this line connecting them. Turns out that the height is the **Eulidean distance**(or whatever distance metric is used). *It also represents computed dissimilarity between the points, that is proportion to how far these points are from each other*.\n\nSo, to wrap up : \n* Dissimilarity is measured by vertical lines. \n* Set a threshold for dissimilarity to obtain the different clusters(We will get to that ahead). \n* So, what is this threshold ? Suppose I choose a value on Y axis(the dissimilarity axis) as my threshold. Now, draw a line parallel to X at that Y, and the number of vertical Dendrogram line it crosses, gives us the number of clusters. \n_______________________________________","52a81a71":"Cluster number - 0 to 4 (as there are 5 clusters). ","770f3d2b":"**Mall data uploaded on google drive at : https:\/\/drive.google.com\/file\/d\/1yypMeJQFpLrFsO9QuQJPYZ59oe5erO_w\/view?usp=sharing** "}}