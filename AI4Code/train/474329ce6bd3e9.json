{"cell_type":{"3f4b543f":"code","ee0c65fb":"code","1d040089":"code","4479e9ce":"code","a83222f7":"code","f3489d21":"code","407628c4":"code","8cd0eba0":"code","0e7bd1f4":"code","b1507f57":"code","d8983606":"code","e1093bbb":"code","49747e55":"code","6a8163a1":"code","76763131":"code","b74da564":"code","a5247293":"code","492793da":"code","f7863c8d":"code","fa4fd3ca":"code","32d7fb48":"code","82b79fcf":"code","6f523c09":"code","abc8030a":"code","9599bca6":"code","094b8320":"code","f21785c8":"code","e6158de8":"code","0abefd37":"code","4fad8be1":"code","deeb5d00":"markdown","0adb51ff":"markdown","bb0d3eba":"markdown","2074a8c7":"markdown","3f50cea0":"markdown","c8d4de58":"markdown","038ce3e2":"markdown","9db961a8":"markdown","f6e7cf08":"markdown","8900ac97":"markdown","0a71c4af":"markdown","6c00302c":"markdown","a80cf966":"markdown"},"source":{"3f4b543f":"import numpy as np\nimport pandas as pd\nimport torch","ee0c65fb":"np.random.seed(2)\ntorch.manual_seed(2)","1d040089":"from torchvision import datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, ImageOps, ImageEnhance\nimport math\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline","4479e9ce":"class dataset(Dataset):\n    def __init__(self, file_path, transform=transforms.Compose([transforms.ToPILImage(), \n                                                                transforms.ToTensor(), \n                                                                transforms.Normalize(mean=(0.5,), \n                                                                                     std=(0.5,))])):\n        df = pd.read_csv(file_path)\n        if len(df.columns)==n_pixels:\n            self.X = df.values.reshape((-1, 28, 28)).astype(np.uint8)[:, :, :, None]\n            self.y = None\n        else:\n            self.X = df.iloc[:, 1:].values.reshape((-1, 28, 28)).astype(np.uint8)[:, :, :, None]\n            self.y = torch.from_numpy(df.iloc[:, 0].values)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        return self.transform(self.X[idx])","a83222f7":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","f3489d21":"n_pixels = len(test_df.columns)\nn_pixels","407628c4":"num_workers = 0\nbatch_size = 64\ntransform = transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation(degrees=20),\n                                transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\ntrain_dataset = dataset('..\/input\/train.csv', transform=transform)\ntest_dataset = dataset('..\/input\/test.csv')\ntrain_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","8cd0eba0":"dataiter = iter(train_dl)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(batch_size):\n    ax = fig.add_subplot(2, batch_size\/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(str(labels[idx].item()))","0e7bd1f4":"import torch.nn as nn","b1507f57":"class ConvLayer(nn.Module):\n    def __init__(self, in_channels=1, out_channels=256):\n        super(ConvLayer, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=1, padding=0)\n    def forward(self, x):\n        x = F.relu(self.conv(x))\n        return x","d8983606":"class PrimaryCaps(nn.Module):\n    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n        super(PrimaryCaps, self).__init__()\n        self.capsules = nn.ModuleList([\n            nn.Conv2d(in_channels, out_channels, kernel_size=9, stride=2, padding=0)\n            for _ in range(num_capsules)\n        ])\n    def forward(self, x):\n        batch_size = x.size(0)\n        u = [capsule(x).view(batch_size, 32*6*6, 1) for capsule in self.capsules]\n        u = torch.cat(u, dim=-1)\n        u_squashed = self.squash(u)\n        return u_squashed\n    \n    def squash(self, x):\n        squared_norm = (x**2).sum(dim=-1, keepdim=True)\n        scale = squared_norm\/(1+squared_norm)\n        output = scale * x\/torch.sqrt(squared_norm)\n        return output","e1093bbb":"def softmax(x, dim=1):\n    transposed_inp = x.transpose(dim, len(x.size())-1)\n    softmaxed = F.softmax(transposed_inp.contiguous().view(-1, transposed_inp.size(-1)), dim=-1)\n    return softmaxed.view(*transposed_inp.size()).transpose(dim, len(x.size())-1)","49747e55":"def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n    for iterations in range(routing_iterations):\n        c_ij = softmax(b_ij, dim=2)\n        s_j = (c_ij*u_hat).sum(dim=2, keepdim=True)\n        v_j = squash(s_j)\n        if iterations < routing_iterations-1:\n            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n            b_ij = b_ij + a_ij\n    return v_j","6a8163a1":"TRAIN_ON_GPU = torch.cuda.is_available()\nif TRAIN_ON_GPU: print('training on gpu')","76763131":"class DigitCaps(nn.Module):\n    def __init__(self, num_caps=10, previous_layer_nodes=32*6*6,\n                 in_channels=8, out_channels=16):\n        super(DigitCaps, self).__init__()\n        self.num_caps = num_caps\n        self.previous_layer_nodes = previous_layer_nodes\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.W = nn.Parameter(torch.randn(num_caps, previous_layer_nodes,\n                                          in_channels, out_channels))\n    \n    def forward(self, x):\n        x = x[None, :, :, None, :]\n        W = self.W[:, None, :, :, :]\n        x_hat = torch.matmul(x, W)\n        b_ij = torch.zeros(*x_hat.size())\n        if TRAIN_ON_GPU: b_ij = b_ij.cuda()\n        v_j = dynamic_routing(b_ij, x_hat, self.squash, routing_iterations=3)\n        return v_j\n    \n    def squash(self, x):\n        squared_norm = (x**2).sum(dim=-1, keepdim=True)\n        scale = squared_norm\/(1+squared_norm)\n        out = scale * x\/torch.sqrt(squared_norm)\n        return out","b74da564":"class Decoder(nn.Module):\n    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n        super(Decoder, self).__init__()\n        input_dim = input_vector_length*input_capsules\n        self.lin_layers = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, hidden_dim*2),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim*2, 28*28),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        classes = (x**2).sum(dim=-1)**0.5\n        classes = F.softmax(classes, dim=-1)\n        _, max_length_indices = classes.max(dim=1)\n        sparse_matrix = torch.eye(10)\n        if TRAIN_ON_GPU: sparse_matrix = sparse_matrix.cuda()\n        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n        x = x*y[:, :, None]\n        flattened_x = x.view(x.size(0), -1)\n        reconstructed = self.lin_layers(flattened_x)\n        return reconstructed, y","a5247293":"class CapsuleNetwork(nn.Module):\n    def __init__(self):\n        super(CapsuleNetwork, self).__init__()\n        self.conv_layer = ConvLayer()\n        self.primary_capsule = PrimaryCaps()\n        self.digit_capsule = DigitCaps()\n        self.decoder = Decoder()\n    def forward(self, x):\n        primary_caps_out = self.primary_capsule(self.conv_layer(x))\n        caps_out = self.digit_capsule(primary_caps_out).squeeze().transpose(0, 1)\n        reconstructed, y = self.decoder(caps_out)\n        return caps_out, reconstructed, y","492793da":"capsule_net = CapsuleNetwork()\n\nprint(capsule_net)\n\nif TRAIN_ON_GPU: capsule_net = capsule_net.cuda()","f7863c8d":"class CapsuleLoss(nn.Module):\n    def __init__(self):\n        super(CapsuleLoss, self).__init__()\n        self.reconstruction_loss = nn.MSELoss(size_average=False)\n    \n    def forward(self, x, labels, images, reconstructions):\n        batch_size = x.size(0)\n        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n        left = F.relu(0.9-v_c).view(batch_size, -1)\n        right = F.relu(v_c-0.1).view(batch_size, -1)\n        margin_loss = labels * left + 0.5 * (1.-labels) * right\n        margin_loss = margin_loss.sum()\n        images = images.view(reconstructions.size()[0], -1)\n        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n        return (margin_loss + 0.0005 * reconstruction_loss)\/images.size(0)","fa4fd3ca":"import torch.optim as optim\ncriterion = CapsuleLoss()\noptimizer = optim.Adam(capsule_net.parameters())","32d7fb48":"def train(capsule_net, criterion, optimizer, n_epochs, print_every=300):\n    losses = []\n    for epoch in range(1, n_epochs+1):\n        train_loss = 0.0\n        capsule_net.train() \n        for batch_i, (images, target) in enumerate(train_dl):\n            target = torch.eye(10).index_select(dim=0, index=target)\n            if TRAIN_ON_GPU: images, target = images.cuda(), target.cuda()\n            optimizer.zero_grad()\n            caps_output, reconstructions, y = capsule_net(images)\n            loss = criterion(caps_output, target, images, reconstructions)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            if batch_i != 0 and batch_i % print_every == 0:\n                avg_train_loss = train_loss\/print_every\n                losses.append(avg_train_loss)\n                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n                train_loss = 0 \n    return losses","82b79fcf":"n_epochs = 10","6f523c09":"losses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)","abc8030a":"plt.plot(losses)\nplt.title('Training Loss')\nplt.show()","9599bca6":"out = []","094b8320":"capsule_net.eval()\nfor image in test_dl:\n    if TRAIN_ON_GPU: image = image.cuda()\n    caps_out, reconstructed, y = capsule_net(image)\n    _, pred = torch.max(y.data.cpu(), 1)\n    out.extend(pred.numpy().tolist())","f21785c8":"len(out)","e6158de8":"sub = pd.read_csv('..\/input\/sample_submission.csv')","0abefd37":"sub['Label'] = out","4fad8be1":"sub.to_csv('capsule.csv', index=False)","deeb5d00":"For getting reconstructed images, we also create a decoder, which takes the 10 digit capsules and after a series of linear and relu operations, convert the capsules again into a 28x28 image","0adb51ff":"# Creating a Dataset to get images and labels from the csv to get images and labels with some transforms","bb0d3eba":"We apply all these layers to create capsule network","2074a8c7":"# A Capsule net has 3 main parts - \n1. A conv layer\n2. A primary capsule\n3. A digit capsule","3f50cea0":"For training, we use the data augmentations of random rotation. For testing, we just create normal tensors","c8d4de58":"This model gives 98.8% accuracy on public LB. This can further be improved by training more and adding more data augmentation","038ce3e2":"# Training the model and plotting the losses","9db961a8":"Plot one batch of images","f6e7cf08":"The digit capusle takes the 8 primary capsules and produces 10 capsules as output. These 10 capsules correspond to the 10 classes of MNIST. The digit capsules apply dynamic routing on the primary capsules to select the children which corresponds the maximum to each capsule.","8900ac97":"# Imports","0a71c4af":"For the loss function of our network, we use the CapsuleLoss module defined below. ","6c00302c":"The conv layer applies a normal convolution with kernel size of 9 and output channels of 256. The output thus produced is of size 20x20x256","a80cf966":"The primary capsule is just 8 stacked convolutions, whose output is then squashed."}}