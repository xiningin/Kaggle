{"cell_type":{"3409c966":"code","7b2c9c09":"code","85595317":"code","1ae8c605":"code","a8fc318c":"code","9c05d307":"code","f7502152":"code","23bde9ed":"code","48c2c9a5":"code","222e573c":"code","c32dd2a9":"code","b11921e0":"code","24898d6b":"code","2d21f682":"code","4336dc33":"code","b0974614":"code","aaa651d1":"code","8e6a691d":"code","56828622":"code","ec51ab9c":"code","cebb267f":"code","0d4ece7b":"markdown","ee4b18db":"markdown","46501409":"markdown","c66a34eb":"markdown","52b71138":"markdown","313f6294":"markdown","7a1507e0":"markdown","b37cb95c":"markdown","d2eeae66":"markdown","17cf44f3":"markdown"},"source":{"3409c966":"import operator as op\nimport random\nrandom.seed(123)\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.svm  import SVC\nimport sklearn.metrics as skm\n\nimport utils_data_prepping as udp\nimport utils_clf_models as clf\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,8)})\n\n\nfrom sklearn.exceptions import ConvergenceWarning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n                        module=\"sklearn\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7b2c9c09":"df = pd.read_csv('\/kaggle\/input\/drug-classification\/drug200.csv')\ndf.head()","85595317":"df.shape","1ae8c605":"df['Drug'].unique()","a8fc318c":"df.info()","9c05d307":"# Categorical Encoding \ncat_vars = ['Sex', 'BP', 'Cholesterol']\nfor i in cat_vars:\n    df[i+\"_cat\"] = df[i].astype('category').cat.codes\ndf.drop(cat_vars, axis=1, inplace=True)\ndf.head()","f7502152":"sns.histplot(data=df, x='Drug')\nplt.show()","23bde9ed":"# Masking to show only one side of the matrix\ncorr = np.corrcoef(df.corr())                        \nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Axtual Correlation matrix as a heatmap\nsns.heatmap(df.corr(), annot=True, mask=mask, cmap=\"YlGnBu\")\nplt.show","48c2c9a5":"sns.histplot(data=df, x='Na_to_K', hue='Drug')\nplt.show()","222e573c":"# Dictionary with model scores\nmodels = {}","c32dd2a9":"class knn():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.2, \n                                                            random_state = 2)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        errors = {}    \n        for k in range(1, 10):\n            model_k = KNeighborsClassifier(n_neighbors=k)\n            scores = cross_val_score(model_k, self.X_train, \n                                     self.Y_train, \n                                     cv=20, scoring='accuracy')\n            errors[k] = scores.mean()\n#         plt.plot(list(errors.keys()), list(errors.values()))\n#         plt.xlabel('Value of K for KNN')\n#         plt.ylabel('Cross-validated accuracy')\n        k_best = max(errors.items(), key=op.itemgetter(1))[0]\n        clf = KNeighborsClassifier(n_neighbors=k_best)\n        clf.fit(self.X_train, self.Y_train)\n        pred = clf.predict(self.X_test)\n        print(skm.classification_report(self.Y_test, pred))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n\nmodel = knn(df, 'Drug').pre_processing()\nmodels['knn'] = model.fit_pred_acc()","b11921e0":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'knn')\nclf1.preprocess_split(0.3, 62)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","24898d6b":"class nb():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.25, \n                                                            random_state = 9)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = GaussianNB()\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n\nmodel = nb(df, 'Drug')\nmodel = model.pre_processing()\nmodels['nb'] = model.fit_pred_acc() ","2d21f682":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'guass_nb')\nclf1.preprocess_split(0.3, 142)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","4336dc33":"class logreg():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.25, \n                                                            random_state = 16)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = LogisticRegression()\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\nmodel = logreg(df, 'Drug')\nmodel = model.pre_processing()\nmodels['logreg'] = model.fit_pred_acc()","b0974614":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'log_reg')\nclf1.preprocess_split(0.3, 62)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","aaa651d1":"class tree():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.3, \n                                                            random_state = 2)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = DecisionTreeClassifier()\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n    \nmodel = tree(df, 'Drug')\nmodel = model.pre_processing()\nmodels['tree'] = model.fit_pred_acc()","8e6a691d":"# Using utils\nX, y = udp.pre_processing(df, 'Drug')\nclf1 = clf.Classifier(X, y, 'tree')\nclf1.preprocess_split(0.3, 142)\nclf1.fit_predict()\nprint('For the training set:')\nclf1.metrics(printing=True)","56828622":"class svc():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n    \n    def pre_processing(self):\n        X = self.df.drop([self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.15, \n                                                            random_state = 64)\n        return self\n        \n    def fit_pred_acc(self):\n        # Elbow method to find best fit\n        clf = SVC(gamma='auto')\n        clf.fit(self.X_train, self.Y_train)  \n        pred = clf.predict(self.X_test)\n        drug_names = ['DrugY', 'drugA', 'drugB', 'drugC', 'drugX']\n        \n        print(skm.classification_report(self.Y_test, pred, target_names=drug_names))\n        return round(skm.accuracy_score(self.Y_test, pred), 2)\n    \nmodel = svc(df, 'Drug')\nmodel = model.pre_processing()\nmodels['svc'] = model.fit_pred_acc()","ec51ab9c":"# Using utils\n# X, y = udp.pre_processing(df, 'Drug')\n# clf1 = clf.Classifier(X, y, 'svc')\n# clf1.preprocess_split(0.3, 62)\n# clf1.fit_predict()\n# print('For the training set:')\n# clf1.metrics(printing=True)","cebb267f":"scores_df = pd.DataFrame.from_dict(models, orient='index', \n                              columns = ['accuracy'])\nscores_df","0d4ece7b":"## 2. Naive Bayes","ee4b18db":"## 3. Logistic Regression","46501409":"# Model Building & Evaluation","c66a34eb":"# Exploratory Data Analysis","52b71138":"## 1. K-Nearest Neighbors","313f6294":"## 2. Visualization of entire dataset","7a1507e0":"## 4. Decision Tree","b37cb95c":"## 3. Relationship between Drug Cat and Na to Potassium Ration","d2eeae66":"## 5. Support Vector Classifier","17cf44f3":"## 1. Distribution of target variable"}}