{"cell_type":{"59f6770a":"code","2e8df85a":"code","79cc5f5d":"code","cd8d5be1":"code","6ed72e70":"code","611ed652":"code","85843b0d":"code","731329a7":"code","6cb3b338":"code","b4364988":"code","b20afa91":"code","cd0546f6":"code","cc588f35":"code","2569af21":"code","8f688178":"code","5b89cd5e":"code","e91a96c0":"code","da7269da":"code","f36cb291":"code","cf4ec115":"code","67828604":"code","2913ea28":"code","280b51a9":"code","7f974c2f":"code","a79eda29":"code","6c9eec98":"code","c238947b":"code","333b3ca1":"markdown","057464b0":"markdown","7b3b243b":"markdown","2756269c":"markdown","516c2184":"markdown","7bdbd43d":"markdown","364f3e8e":"markdown"},"source":{"59f6770a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e8df85a":"import pandas as pd\nimport numpy as np\n","79cc5f5d":"train = pd.read_csv(\"\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/dataset\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/hackerearth-machine-learning-exhibit-art\/dataset\/test.csv\")","cd8d5be1":"train.head()","6ed72e70":"test.head()","611ed652":"train_Y = train['Cost'].abs()\ntrain_X = train.drop(['Cost'], axis=1)\ntest_X = test\ntrain_X","85843b0d":"import pandas_profiling ","731329a7":"train.profile_report()","6cb3b338":"train_X = train_X.drop(['Customer Id', 'Artist Name', 'Scheduled Date', 'Delivery Date', 'Customer Location', 'Transport'], axis = 1)\ntest_X = test.drop(['Customer Id', 'Artist Name', 'Scheduled Date', 'Delivery Date', 'Customer Location', 'Transport'], axis = 1)","b4364988":"# 1. Function to replace NAN values with mode value\ndef impute_nan_most_frequent_category(DataFrame,ColName):\n    # .mode()[0] - gives first category name\n     most_frequent_category=DataFrame[ColName].mode()[0]\n    \n    # replace nan values with most occured category\n     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n#2. Call function to impute most occured category\nfor Columns in ['Material', 'Remote Location']:\n    impute_nan_most_frequent_category(train_X,Columns)\n    impute_nan_most_frequent_category(test_X,Columns)\n    \n# Display imputed result\ntrain_X[['Material','Material_Imputed','Remote Location','Remote Location_Imputed']].head(10)\n#3. Drop actual columns\ntrain_X = train_X.drop(['Material', 'Remote Location'], axis = 1)\ntest_X = test_X.drop(['Material', 'Remote Location'], axis = 1)","b20afa91":"#test_X.profile_report()\ntrain_X","cd0546f6":"# Get list of categorical variables\ns = (train_X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","cc588f35":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = train_X.copy()\nlabel_X_test = test_X.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_X_train[col] = label_encoder.fit_transform(train_X[col])\n    label_X_test[col] = label_encoder.transform(test_X[col])","2569af21":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\nimputed_X_test = pd.DataFrame(my_imputer.transform(label_X_test))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = train_X.columns\nimputed_X_test.columns = test_X.columns","8f688178":"imputed_X_train","5b89cd5e":"import sklearn\nscaler = sklearn.preprocessing.StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(imputed_X_train), columns=imputed_X_train.columns)\nX_test = pd.DataFrame(scaler.transform(imputed_X_test), columns=imputed_X_test.columns)","e91a96c0":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, train_Y)","da7269da":"y_prediction_rf = rf.predict(X_test)\ny_prediction_rf","f36cb291":"#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","cf4ec115":"param_grid = [\n{'n_estimators': [50,100,250,500], \n 'max_depth': [10, 50, 100], 'bootstrap': [True, False]}\n]","67828604":"random_search_forest = RandomizedSearchCV(rf, param_grid, cv=10)\nrandom_search_forest.fit(X_train, train_Y)","2913ea28":"tuned_rf_best_random = random_search_forest.best_estimator_\ntuned_rf_best_random","280b51a9":"tuned_rf_best_random.fit(X_train, train_Y)","7f974c2f":"tuned_rf_random_pred = tuned_rf_best_random.predict(X_test)\ntuned_rf_random_pred","a79eda29":"df = pd.DataFrame()","6c9eec98":"df['Customer Id'] = test['Customer Id']\ndf['Cost'] = y_prediction_rf\ndf['Cost'] = df['Cost'].abs()\ndf","c238947b":"df.to_csv('prediction1.csv', index = False)","333b3ca1":"# Dealing with categorical columns having missing values","057464b0":"**Using Pandas Profiling for EDA**","7b3b243b":"#### Tuning RF","2756269c":"#### *Imputing missing values*","516c2184":"#### Label Encoding Categorical Data*Label Encoding Categorical Data*","7bdbd43d":"#### RandomForest Regressor","364f3e8e":"#### Data Normalisation"}}