{"cell_type":{"32c2afb8":"code","78312e4a":"code","9992ac35":"code","0c2c6c3a":"code","93d91f06":"code","56fd0371":"code","8650360c":"code","bd871526":"code","ea5321bc":"code","4fbf2613":"code","2615a027":"code","1e17fced":"code","3a31286b":"code","2b410e8e":"code","1daa2a9f":"code","7b680661":"code","ac0ea301":"code","471449b7":"code","684a4217":"code","b98fda46":"code","81a47120":"code","ae6f8aee":"code","93d543b7":"code","d844506d":"code","2301ba41":"code","921030b2":"code","e8b02370":"code","eb8969e2":"code","c583cceb":"code","67c53616":"code","0054f5d7":"markdown","2e62b71d":"markdown","edb21eea":"markdown","20c62341":"markdown","5f37b56d":"markdown","2340dc9d":"markdown","248e146c":"markdown","58267060":"markdown"},"source":{"32c2afb8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","78312e4a":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nimport seaborn as sns\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense,Flatten,Conv2D,MaxPool2D,Dropout,Activation,LeakyReLU\nfrom keras.optimizers import RMSprop\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import EarlyStopping \nfrom sklearn import metrics\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n","9992ac35":"train = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")\nDig_MNIST = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/Dig-MNIST.csv\")","0c2c6c3a":"sample_sub = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/sample_submission.csv\")","93d91f06":"print(\"Train set shape = \" +str(train.shape))\nprint(\"Test set shape = \" +str(test.shape))\nprint(\"Dif set shape = \" +str(Dig_MNIST.shape))","56fd0371":"train.head()","8650360c":"X=train.iloc[:,1:].values \nY=train.iloc[:,0].values \nY[:10]","bd871526":"X = X.reshape(X.shape[0], 28, 28,1) \nprint(X.shape)\n","ea5321bc":"Y = keras.utils.to_categorical(Y, 10) \nprint(Y.shape)","4fbf2613":"test.head()","2615a027":"x_test=test.drop('id', axis=1).iloc[:,:].values\nx_test = x_test.reshape(x_test.shape[0], 28, 28,1)\nx_test.shape","1e17fced":"Dig_MNIST.head()","3a31286b":"x_dig=Dig_MNIST.drop('label',axis=1).iloc[:,:].values\nprint(x_dig.shape)\nx_dig = x_dig.reshape(x_dig.shape[0], 28, 28,1)\nx_dig.shape","2b410e8e":"y_dig=Dig_MNIST.label\ny_dig.shape","1daa2a9f":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.10, random_state=42) ","7b680661":"train_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 10,\n                                   width_shift_range = 0.25,\n                                   height_shift_range = 0.25,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.25,\n                                   horizontal_flip = False)","ac0ea301":"valid_datagen = ImageDataGenerator(rescale=1.\/255) ","471449b7":"def lr_decay(epoch):#lrv\n    return initial_learningrate * 0.99 ** epoch","684a4217":"es = EarlyStopping(monitor='val_loss', verbose=1, patience=10)","b98fda46":"from sklearn import metrics\n","81a47120":"model = Sequential([\n    Conv2D(64, (3,3), padding='same', input_shape=(28, 28, 1)),\n    BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    LeakyReLU(alpha=0.1),\n    Conv2D(64,  (3,3), padding='same'),\n    BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    LeakyReLU(alpha=0.1),\n\n    layers.MaxPooling2D(2, 2),\n    Dropout(0.2),\n    \n    Conv2D(128, (3,3), padding='same'),\n    BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    LeakyReLU(alpha=0.1),\n    Conv2D(128, (3,3), padding='same'),\n    BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    LeakyReLU(alpha=0.1),\n    \n    layers.MaxPooling2D(2,2),\n    Dropout(0.2),    \n    \n    Conv2D(256, (3,3), padding='same'),\n    BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    LeakyReLU(alpha=0.1),\n    Conv2D(256, (3,3), padding='same'),\n    BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    LeakyReLU(alpha=0.1),\n\n   layers.MaxPooling2D(2,2),\n    Dropout(0.2),\n    \n    \n    Flatten(),\n    Dense(256),\n    LeakyReLU(alpha=0.1),\n \n    BatchNormalization(),\n    Dense(10, activation='softmax')\n])\nmodel.summary()","ae6f8aee":"initial_learningrate=2e-3\nbatch_size = 1024\nepochs = 50\ninput_shape = (28, 28, 1)","93d543b7":"model.compile(loss=\"categorical_crossentropy\",\n              optimizer=RMSprop(lr=initial_learningrate),\n              metrics=['accuracy'])","d844506d":"history = model.fit_generator(\n      train_datagen.flow(X,Y, batch_size=batch_size),\n      steps_per_epoch=100,\n      epochs=epochs,\n      callbacks=[LearningRateScheduler(lr_decay)           \n               ],\n      validation_data=valid_datagen.flow(X_valid,Y_valid),\n      validation_steps=50,  \n      verbose=2)","2301ba41":"preds_dig=model.predict_classes(x_dig\/255)\nmetrics.accuracy_score(preds_dig, y_dig)","921030b2":"predictions = model.predict_classes(x_test\/255.)","e8b02370":"submission = pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')","eb8969e2":"submission['label'] = predictions","c583cceb":"submission.head()","67c53616":"submission.to_csv(\"submission.csv\",index=False)","0054f5d7":"Now we convert the labels to categorical.","2e62b71d":"Let's load the data.","edb21eea":"Let's fit the model on the whole training set.","20c62341":"The next function reduces the learning rate as the training advances.","5f37b56d":"We split the data into training and validation set.","2340dc9d":"We use Keras ImageDataGenerator to artificially increase our training set.","248e146c":"Now we must reshape the date to make it Keras friendly.","58267060":"We slice the dataframes to define the features and the labels"}}