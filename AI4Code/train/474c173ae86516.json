{"cell_type":{"07999be2":"code","4d504738":"code","5d9a478a":"code","c4251285":"code","727eedcd":"code","5a7f4e81":"code","0289419a":"code","96212f25":"code","33a57a6f":"code","6d48f504":"code","a8e75ba6":"code","2915c321":"code","bcea369c":"code","22fc6b35":"code","9f8c3ca7":"code","febd53b7":"code","95429f91":"code","f5f61a7a":"code","7fad8008":"code","67c35fae":"code","c27dbd19":"code","f17f1287":"code","2214e81b":"code","2112272d":"markdown","12fc86ba":"markdown","8e5c3a5f":"markdown","0aa17c01":"markdown","e1d79b27":"markdown","854a097f":"markdown","b453402f":"markdown","e038a9ab":"markdown","8e95be85":"markdown"},"source":{"07999be2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d504738":"!pip install seqeval","5d9a478a":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport torch.nn as nn\n# from seqeval import metrics\nfrom sklearn import metrics\nfrom tqdm import tqdm\nfrom transformers import get_linear_schedule_with_warmup, AdamW\nfrom torch.utils.data import DataLoader","c4251285":"ner_data = pd.read_csv(\"\/kaggle\/input\/entity-annotated-corpus\/ner_dataset.csv\", encoding = \"ISO-8859-1\")\nner_data = ner_data.fillna(method='ffill')","727eedcd":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [w for w in s['Word'].values.tolist()]\n        self.text_sentencs = self.data.groupby('Sentence #').apply(agg_func)\n        agg_func = lambda s: [tag for tag in s['Tag'].values.tolist()]\n        self.labels = self.data.groupby('Sentence #').apply(agg_func)  \n\ngetter = SentenceGetter(ner_data)","5a7f4e81":"x_train = getter.text_sentencs\ny_train = getter.labels\n\ntrain_texts, val_texts, train_tags, val_tags = train_test_split(x_train.tolist(), y_train.tolist(), test_size=.2)\n\nunique_tags = set(tag for doc in y_train for tag in doc)\ntag2id = {tag: id for id, tag in enumerate(unique_tags)}\nid2tag = {id: tag for tag, id in tag2id.items()}","0289419a":"MAX_LEN = 128\nTRAIN_BATCH_SIZE = 16 \nVALID_BATCH_SIZE = 16\nMODEL_NAME = 'bert-base-cased'\nMODEL_STORING_PATH = 'best_model'\nMAX_PARTIAL_MODEL_EPOCHS = 5\nEARLY_STOPPING_PATIENCE_CLASSIFIER = 3\nMAX_WHOLE_MODEL_EPOCHS = 3\nEARLY_STOPPING_PATIENCE_WHOLE_MODEL=2","96212f25":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntrain_encodings = tokenizer(train_texts, is_split_into_words=True, truncation=True, padding='max_length', max_length=MAX_LEN)\nval_encodings = tokenizer(val_texts, is_split_into_words=True, truncation=True, padding='max_length', max_length=MAX_LEN)","33a57a6f":"import numpy as np\n\ndef encode_tags(tags, encodings):\n  labels = [[tag2id[tag] for tag in doc] for doc in tags]\n  encoded_labels = []\n  for i, doc_labels in enumerate(labels):\n      # create an empty array of -100\n    word_ids = encodings.word_ids(batch_index=i)\n    label_ids = []\n    for word_idx in word_ids:\n      if (word_idx==0 or word_idx) and word_idx != previous_word_idx:\n        label_ids.append(doc_labels[word_idx])\n      else: \n        label_ids.append(-100)\n      previous_word_idx = word_idx\n    encoded_labels.append(label_ids)\n  return encoded_labels\n\ntrain_labels = encode_tags(train_tags, train_encodings)\nval_labels = encode_tags(val_tags, val_encodings)","6d48f504":"class ExtractEntity:\n    def __init__(self, text_encodings, labels):\n        self.labels = labels\n        self.text_encodings = text_encodings\n\n    def __getitem__(self, idx):\n        labels = self.labels[idx]\n\n        return {\n            'input_ids' : torch.tensor(self.text_encodings.input_ids[idx], dtype=torch.long),\n            'attention_mask' : torch.tensor(self.text_encodings.attention_mask[idx], dtype=torch.long),\n            'token_type_id' : torch.tensor(self.text_encodings.token_type_ids[idx], dtype=torch.long),\n            'labels' : torch.tensor(labels, dtype=torch.float)\n        }\n        \n    def __len__(self):\n        return len(self.labels)","a8e75ba6":"train_dataset = ExtractEntity(train_encodings, train_labels)\nval_dataset = ExtractEntity(val_encodings, val_labels)\n\ntrain_loader = DataLoader(train_dataset, \n                          batch_size=4, \n                          shuffle=True)\neval_loader = DataLoader(val_dataset, \n                         batch_size=4)","2915c321":"class NERModel(nn.Module):\n    def __init__(self, num_labels):\n        super(NERModel, self).__init__()\n        self.num_labels = num_labels\n        self.bert_layer = AutoModel.from_pretrained(MODEL_NAME)\n        self.bert_drop = nn.Dropout(0.3)\n        self.l0 = nn.Linear(768, self.num_labels)\n\n    def forward(self, ids, mask, text_token_type):\n\n        bert_out_text = self.bert_layer(\n            input_ids = ids,\n            attention_mask = mask,\n            token_type_ids = text_token_type)\n\n        dropout_out = self.bert_drop(bert_out_text['last_hidden_state'])\n\n        output = self.l0(dropout_out)\n        return output","bcea369c":"def calculate_accuracy(outputs, labels, mask, type_acr = 'batch'):\n    if type_acr == 'batch':\n        mask = mask.cpu().detach().numpy()\n        labels = labels.cpu().detach().numpy()[mask == 1]\n        outputs = outputs.cpu().detach().numpy()[mask == 1]\n        predictions = np.argmax(outputs, axis=1)\n        return metrics.accuracy_score(labels.tolist(), predictions.tolist())\n    return metrics.accuracy_score(labels, outputs)\n    \n\ndef loss_fn(outputs, labels, num_labels):\n    lfn = nn.CrossEntropyLoss()\n    return lfn(outputs.view(-1, num_labels), labels.view(-1))\n","22fc6b35":"def train_fn(data_loader, model, optimizer, scheduler, device, accumulation_steps=2):\n    model.train()\n    \n    total_loss = 0\n    total_accuracy = 0\n    with tqdm(enumerate(data_loader), unit=\"batch\", total=len(data_loader)) as tepoch:\n        for batch_index, dataset in tepoch: \n            tepoch.set_description(f\"Epoch Started\")\n            \n            text_input_ids = dataset['input_ids']\n            text_attention_mask = dataset['attention_mask']\n            text_token_type_ids = dataset['token_type_id']\n            labels = dataset['labels']\n\n            text_input_ids = text_input_ids.to(device, dtype = torch.long)\n            text_attention_mask = text_attention_mask.to(device, dtype = torch.long)\n            text_token_type_ids = text_token_type_ids.to(device, dtype = torch.long)\n            labels = labels.to(device, dtype = torch.long)\n\n            optimizer.zero_grad()\n\n            outputs = model(\n                text_input_ids, \n                text_attention_mask,\n                text_token_type_ids\n            )\n\n            loss = loss_fn(outputs, labels, model.num_labels)\n            loss.backward()\n\n            train_accuracy = 100.0 * calculate_accuracy(outputs, labels, text_attention_mask)\n            tepoch.set_postfix(loss=loss.item(), accuracy=train_accuracy)\n            \n            if (batch_index+1) % accumulation_steps == 0 :\n                optimizer.step()\n                scheduler.step()\n        \n            total_loss += loss.item()\n            total_accuracy += train_accuracy\n            \n    return total_loss\/len(data_loader), total_accuracy\/len(data_loader)","9f8c3ca7":"def eval_fn(data_loader, model, device):\n    model.eval()\n    \n    final_labels = []\n    final_pred = []\n    \n    with torch.no_grad():\n        for _, dataset in tqdm(enumerate(data_loader), total=len(data_loader)):\n            text_input_ids = dataset['input_ids']\n            text_attention_mask = dataset['attention_mask']\n            text_token_type_ids = dataset['token_type_id']\n            labels = dataset['labels']\n\n            text_input_ids = text_input_ids.to(device, dtype = torch.long)\n            text_attention_mask = text_attention_mask.to(device, dtype = torch.long)\n            text_token_type_ids = text_token_type_ids.to(device, dtype = torch.long)\n            labels = labels.to(device, dtype = torch.long)\n\n            outputs = model(\n                text_input_ids, \n                text_attention_mask,\n                text_token_type_ids\n            )\n\n            mask = text_attention_mask.cpu().detach().numpy()\n            labels = labels.cpu().detach().numpy()[mask == 1]\n            outputs = outputs.cpu().detach().numpy()[mask == 1]\n            predictions = np.argmax(outputs, axis=1)\n\n            final_labels.extend(labels)\n            final_pred.extend(predictions)\n    return final_pred, final_labels","febd53b7":"device = torch.device(\"cuda\")","95429f91":"model = NERModel(len(unique_tags))\n\nfor param in model.bert_layer.parameters():\n    param.requires_grad = False\n\nmodel.to(device)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-4},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},\n]\n\nnum_training_steps = int(len(train_texts)\/TRAIN_BATCH_SIZE * MAX_PARTIAL_MODEL_EPOCHS)\n\noptimizer = AdamW(optimizer_parameters, lr=1e-3)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = num_training_steps\n)","f5f61a7a":"best_accuracy = 0\nearly_stopping_counter = 0\n\nfor epochs in range(MAX_PARTIAL_MODEL_EPOCHS):\n    print(\"Epoch :\", epochs)\n    loss, train_accuracy = train_fn(train_loader, model, optimizer, scheduler, device)\n    print(f\"Total Epoch Train Accuracy : {train_accuracy} with loss : {loss}\")\n    predicted, labels = eval_fn(eval_loader, model, device)\n    val_accuracy = calculate_accuracy(predicted, labels, _, 'epoch')\n    print(f\"Total Epoch Eval Accuracy : {100.0 *  val_accuracy}\")\n    if val_accuracy > best_accuracy:\n        early_stopping_counter = 0\n        best_accuracy = val_accuracy\n        torch.save(model.state_dict(), MODEL_STORING_PATH)\n    else:\n        early_stopping_counter += 1\n        if early_stopping_counter > EARLY_STOPPING_PATIENCE_CLASSIFIER:\n            break","7fad8008":"if val_accuracy > best_accuracy:\n    early_stopping_counter = 0\n    best_accuracy = val_accuracy\n    torch.save(model.state_dict(), MODEL_STORING_PATH)","67c35fae":"del model\nmodel = NERModel(len(unique_tags))\nmodel.load_state_dict(torch.load(MODEL_STORING_PATH))\nmodel.to(device)\n\nprint('New Model Loaded')","c27dbd19":"for param in model.bert_layer.parameters():\n    param.requires_grad = True","f17f1287":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-4},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},\n]\n\nnum_training_steps = int(len(train_texts)\/TRAIN_BATCH_SIZE * MAX_WHOLE_MODEL_EPOCHS)\n\noptimizer = AdamW(optimizer_parameters, lr=1e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = num_training_steps\n)","2214e81b":"early_stopping_counter = 0\n\nfor epochs in range(MAX_WHOLE_MODEL_EPOCHS):\n    print(\"Epoch :\", epochs)\n    loss, train_accuracy = train_fn(train_loader, model, optimizer, scheduler, device)\n    print(f\"Total Epoch Train Accuracy : {train_accuracy} with loss : {loss}\")\n    predicted, labels = eval_fn(eval_loader, model, device)\n    val_accuracy = calculate_accuracy(predicted, labels, _, 'epoch')\n    print(f\"Total Epoch Eval Accuracy : {val_accuracy}\")\n    if val_accuracy > best_accuracy:\n        early_stopping_counter = 0\n        best_accuracy = val_accuracy\n        torch.save(model.state_dict(), MODEL_STORING_PATH)\n    else:\n        early_stopping_counter += 1\n        if early_stopping_counter > EARLY_STOPPING_PATIENCE_WHOLE_MODEL:\n            break","2112272d":"# Finally Best Model Saved","12fc86ba":"# Create your data loader","8e5c3a5f":"# Lets Start Training","0aa17c01":"# Create your pytorch Model","e1d79b27":"# Define Train and Eval Function","854a097f":"# Basic Loading of data","b453402f":"# Choose your GPU Or CPU Or TPU","e038a9ab":"Lets Juice out the whole BERT model now. Fine tune the whole bert model.","8e95be85":"# Masked Accuracy Calculation and Masked loss"}}