{"cell_type":{"c09d3306":"code","ffccd251":"code","449f6666":"code","09c4bfa7":"code","783a08da":"code","93f3e7ab":"code","8409872b":"code","6a0cd0af":"code","3ca7aa05":"code","4afde92c":"code","6a49a359":"code","c1154c72":"code","0558976d":"code","57934870":"code","29f11181":"code","c06ac7af":"code","d593bd31":"code","3f2bffd8":"code","60b4f871":"code","028b1f66":"code","af9d3feb":"code","650e4b26":"code","07936348":"code","140b653a":"code","fc360ad6":"code","cf80e94f":"code","9ff70457":"code","5a3bb488":"code","912119ca":"code","616e900e":"code","ab5cff17":"code","c262c8cd":"code","23c6c981":"code","0353bc97":"code","8a599a84":"code","805d28f6":"code","0dc2592e":"code","36869e55":"code","56be6f3b":"code","aa354854":"code","f29f4348":"code","3921a669":"code","b1ed3f8a":"code","595e5206":"code","76909b3b":"code","3765e086":"code","63fa7244":"code","9d0e1eb5":"code","7f36c0ec":"code","c58b2c63":"code","77e7d072":"code","afc5e0fb":"code","123e6b9d":"code","4f18d03c":"code","4348151b":"code","bce54e15":"code","928c6202":"code","b6c5065c":"code","376c9391":"code","0dad26ff":"code","7ae63953":"code","6bfca67b":"code","7eea359d":"code","506a08c3":"code","aee897e8":"code","99836fab":"code","71d9e434":"code","6b00adaf":"markdown","cebbb972":"markdown","69a6a783":"markdown","ebccd45e":"markdown","8dd4e1ae":"markdown","929df013":"markdown","30bd732b":"markdown","9c4b38c1":"markdown","c9c773f3":"markdown"},"source":{"c09d3306":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\n%matplotlib inline\nfrom matplotlib import rcParams\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")","ffccd251":"rcParams['figure.figsize'] = (8.0, 5.0)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","449f6666":"file_1 = pd.read_csv('..\/input\/churn-prediction-of-bank-customers\/Churn_Modelling.csv')","09c4bfa7":"df_orig = pd.DataFrame(file_1)","783a08da":"df_orig.head()","93f3e7ab":"df = df_orig.copy()","8409872b":"# Dropping the id and name columns.\ndf.drop('CustomerId', axis=1, inplace=True)\ndf.drop('Surname', axis=1, inplace=True)\ndf.head()","6a0cd0af":"df.info()","3ca7aa05":"# Converting NumOfProducts column to categorical.\ndf['NumOfProducts'] = df['NumOfProducts'].astype(int)\ndf['NumOfProducts'] = df['NumOfProducts'].astype(object)","4afde92c":"# Creating seperate columns for categories\ndf = pd.get_dummies(df)\ndf.head()","6a49a359":"# Dropping excess columns\ndf.drop('Geography_Spain', axis=1, inplace=True)\ndf.drop('Gender_Male', axis=1, inplace=True)\ndf.drop('NumOfProducts_2', axis=1, inplace=True)\ndf.head()","c1154c72":"df.columns","0558976d":"df = df[['CreditScore', 'Age', 'Tenure', 'Balance', 'HasCrCard',\n       'IsActiveMember', 'EstimatedSalary', 'Geography_Germany',\n       'Geography_France', 'Gender_Female', 'NumOfProducts_1',\n       'NumOfProducts_4', 'NumOfProducts_3', 'Exited']] ","57934870":"# Correlation Matrix\ncorr = df.corr()\ncorr.style.background_gradient()","29f11181":"# Converting all Balances more than 0 to 1\ndf['Balance'] = df['Balance'].clip(upper=1)","c06ac7af":"# Dropping insignificant features as decided during previous excercises.\n# Age p-value = 0.0\n# Credit Score p-value = 0.0085\n# Balance p-value = 0.0\n# Estimated Salary p-value = 0.1222\ndf.drop('EstimatedSalary', axis=1, inplace=True)\ndf.drop('HasCrCard', axis=1, inplace=True)\n#df.drop('NumOfProducts', axis=1, inplace=True)\ndf.drop('Tenure', axis=1, inplace=True)\ndf.head()","d593bd31":"df = df.applymap(np.int64)","3f2bffd8":"df.loc[df.Balance == 0, 'Balance'] = -1\ndf.loc[df.IsActiveMember == 0, 'IsActiveMember'] = -1\ndf.loc[df.Geography_Germany == 0, 'Geography_Germany'] = -1\ndf.loc[df.Geography_France == 0, 'Geography_France'] = -1\ndf.loc[df.Gender_Female == 0, 'Gender_Female'] = -1\ndf.loc[df.NumOfProducts_1 == 0, 'NumOfProducts_1'] = -1\ndf.loc[df.NumOfProducts_3 == 0, 'NumOfProducts_3'] = -1\ndf.loc[df.NumOfProducts_4 == 0, 'NumOfProducts_4'] = -1\ndf.loc[df.Exited == 0, 'Exited'] = -1\ndf.head()","60b4f871":"# Scaling the data\nfrom sklearn.preprocessing import scale\n\ndf['CreditScore'] = scale(df['CreditScore'])\ndf['Age'] = scale(df['Age'])\n#df['Tenure'] = scale(df['Tenure'])\n#df['NumOfProducts'] = scale(df['NumOfProducts'])\n\ndf.head()","028b1f66":"df.columns","af9d3feb":"X = df[['CreditScore', 'Age', 'Balance', 'IsActiveMember', 'Geography_Germany',\n       'Geography_France', 'Gender_Female', 'NumOfProducts_1',\n       'NumOfProducts_4', 'NumOfProducts_3']]\ny = df['Exited']","650e4b26":"# Using Lasso to know features significance.\nfrom sklearn.linear_model import Lasso\nnames = df.columns\nlasso = Lasso(alpha=0.1)\nlasso_coef = lasso.fit(X, y).coef_\n_ = plt.plot(range(len(names)-1), lasso_coef)\nplt.xticks(range(len(names)-1), names, rotation=60)","07936348":"X = df[['CreditScore', 'Age', 'Balance', 'IsActiveMember', 'Geography_Germany',\n       'Geography_France', 'Gender_Female', 'NumOfProducts_1',\n       'NumOfProducts_4', 'NumOfProducts_3']]\ny = df['Exited']","140b653a":"# Splitting the data in test data and train data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=51)","fc360ad6":"# Fitting the data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\ny_pred = logreg.predict(X_test)\naccuracy_score(y_pred, y_test)","cf80e94f":"# Using GridSearch to find the best parameters\nfrom sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\nlogreg_cv=GridSearchCV(logreg,grid,cv=10, scoring='accuracy', refit=True, n_jobs=-1)\nlogreg_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)\nprint(logreg_cv.best_estimator_)","9ff70457":"accuracy_score(logreg_cv.predict(X_test), y_test)","5a3bb488":"# ROC Curve\nfrom sklearn.metrics import roc_curve\n\ny_pred_prob = logreg_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.legend()","912119ca":"# ROC AUC score. The area under ROC curve.\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, y_pred_prob)","616e900e":"from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, logreg_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","ab5cff17":"# Classification Report\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, logreg_cv.predict(X_test)))","c262c8cd":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(X_train,y_train)\naccuracy_score(knn.predict(X_test), y_test)","23c6c981":"knn = KNeighborsClassifier()\nk_grid={'n_neighbors':np.arange(1,20)}\nknn_cv=GridSearchCV(knn, k_grid, cv=10, refit=True, n_jobs=-1)\nknn_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\nprint(\"accuracy :\",knn_cv.best_score_)\nprint(knn_cv.best_estimator_)","0353bc97":"y_pred_prob = knn_cv.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='k Nearest Neighbors')\nplt.legend()","8a599a84":"roc_auc_score(y_test, y_pred_prob)","805d28f6":"print(confusion_matrix(y_test, knn_cv.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","0dc2592e":"print(classification_report(y_test, knn_cv.predict(X_test)))","36869e55":"from sklearn.svm import SVC\n\nCs = [0.1, 1, 10, 100]\ngammas = [0.001, .01, 0.1, 1, 10]\nparam_grid = {'C': Cs, 'gamma': gammas,'kernel': ['rbf'], 'probability':[True]}\n\nSVM_rbf_cv = GridSearchCV(SVC(), param_grid, cv=3, refit=True, n_jobs=-1)\nSVM_rbf_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",SVM_rbf_cv.best_params_)\nprint(\"accuracy :\",SVM_rbf_cv.best_score_)\nprint(SVM_rbf_cv.best_estimator_)","56be6f3b":"Cs = [1]\ngammas = [0.1]\nparam_grid = {'C': Cs, 'gamma': gammas,'kernel': ['rbf'], 'probability':[True]}\nSVM_rbf_cv_10 = GridSearchCV(SVC(), param_grid, cv=10, refit=True, n_jobs=-1)\nSVM_rbf_cv_10.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",SVM_rbf_cv_10.best_params_)\nprint(\"accuracy :\",SVM_rbf_cv_10.best_score_)","aa354854":"y_pred_prob = SVM_rbf_cv_10.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='SVM')\nplt.legend()","f29f4348":"roc_auc_score(y_test, y_pred_prob)","3921a669":"print(confusion_matrix(y_test, SVM_rbf_cv_10.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","b1ed3f8a":"print(classification_report(y_test, SVM_rbf_cv_10.predict(X_test)))","595e5206":"from sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV\n\nCs = [0.1, 1, 10, 100]\ngammas = [0.001, .01, 0.1, 0.5]\n\nparam_grid = {'C': Cs, 'gamma': gammas,'probability':[True],'kernel': ['poly'],'degree':[2,3] }\nSVM_poly_cv = RandomizedSearchCV(estimator = SVC(), param_distributions = param_grid, n_iter = 10, cv = 3, random_state=51, n_jobs = -1, refit=True)\nSVM_poly_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",SVM_poly_cv.best_params_)\nprint(\"accuracy :\",SVM_poly_cv.best_score_)\nprint(SVM_poly_cv.best_estimator_)","76909b3b":"from sklearn.model_selection import RandomizedSearchCV\nCs = [10]\ngammas = [0.5]\n\nparam_grid = {'C': Cs, 'gamma': gammas,'probability':[True],'kernel': ['poly'],'degree':[2] }\nSVM_poly_cv_10 = RandomizedSearchCV(estimator = SVC(), param_distributions = param_grid, n_iter = 10, cv = 10, random_state=51, n_jobs = -1, refit=True)\nSVM_poly_cv_10.fit(X_train,y_train)\n\nprint(\"accuracy :\",SVM_poly_cv_10.best_score_)","3765e086":"y_pred_prob = SVM_poly_cv_10.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='SVM')\nplt.legend()","63fa7244":"roc_auc_score(y_test, y_pred_prob)","9d0e1eb5":"print(confusion_matrix(y_test, SVM_poly_cv_10.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","7f36c0ec":"print(classification_report(y_test, SVM_poly_cv_10.predict(X_test)))","c58b2c63":"from sklearn.ensemble import RandomForestClassifier\n\nn_est = [int(x) for x in np.linspace(start = 50, stop = 1000, num = 10)]\nm_depth = [int(x) for x in np.linspace(5, 50, num = 5)]\nmin_samp = [3, 5, 6, 7, 10, 11]\nm_ftr = ['auto']\n\nparam_grid = {'max_depth': m_depth, 'max_features': m_ftr,'n_estimators': n_est,'min_samples_split': min_samp}\nRF_cv = RandomizedSearchCV(estimator = RandomForestClassifier(), n_iter=200, param_distributions =  param_grid, random_state=51, cv=3, n_jobs=-1, refit=True)\nRF_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",RF_cv.best_params_)\nprint(\"accuracy :\",RF_cv.best_score_)\nprint(RF_cv.best_estimator_)","77e7d072":"from sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'max_depth': [16], 'max_features': [\"auto\"],'n_estimators': [155],'min_samples_split': [11]}\nRF_cv_10 = RandomizedSearchCV(estimator = RandomForestClassifier(), n_iter=200, param_distributions =  param_grid, random_state=51, cv=10, n_jobs=-1, refit=True)\nRF_cv_10.fit(X_train,y_train)\n\nprint(\"accuracy :\",RF_cv_10.best_score_)","afc5e0fb":"y_pred_prob = RF_cv_10.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='Random Forest')\nplt.legend()","123e6b9d":"roc_auc_score(y_test, y_pred_prob)","4f18d03c":"print(confusion_matrix(y_test, RF_cv_10.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","4348151b":"print(classification_report(y_test, RF_cv_10.predict(X_test)))","bce54e15":"from sklearn.naive_bayes import GaussianNB\n\nnb_m = GaussianNB()\nnb_m.fit(X_train,y_train)\ny_pred = nb_m.predict(X_test)\naccuracy_score(y_pred, y_test)","928c6202":"y_pred_prob = nb_m.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='Naive Bayes')\nplt.legend()","b6c5065c":"roc_auc_score(y_test, y_pred_prob)","376c9391":"print(confusion_matrix(y_test, nb_m.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","0dad26ff":"print(classification_report(y_test, nb_m.predict(X_test)))","7ae63953":"from xgboost import XGBClassifier\n\nm_dep = [5,6,7,8]\ngammas = [0.01,0.001,0.001]\nmin_c_wt = [1,5,10]\nl_rate = [0.05,0.1, 0.2, 0.3]\nn_est = [5,10,20,100]\n\nparam_grid = {'n_estimators': n_est, 'gamma': gammas, 'max_depth': m_dep,\n              'min_child_weight': min_c_wt, 'learning_rate': l_rate}\n\nxgb_cv = RandomizedSearchCV(estimator = XGBClassifier(), n_iter=100, param_distributions =  param_grid, random_state=51, cv=3, n_jobs=-1, refit=True)\nxgb_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",xgb_cv.best_params_)\nprint(\"accuracy :\",xgb_cv.best_score_)\nprint(xgb_cv.best_estimator_)","6bfca67b":"from xgboost import XGBClassifier\n\nm_dep = [5]\ngammas = [0.001]\nmin_c_wt = [1]\nl_rate = [0.3]\nn_est = [20]\n\nparam_grid = {'n_estimators': n_est, 'gamma': gammas, 'max_depth': m_dep,\n              'min_child_weight': min_c_wt, 'learning_rate': l_rate}\n\nxgb_cv_10 = RandomizedSearchCV(estimator = XGBClassifier(), n_iter=300, param_distributions =  param_grid, random_state=51, cv=10, n_jobs=-1, refit=True)\nxgb_cv_10.fit(X_train,y_train)\n\nprint(\"accuracy :\",xgb_cv_10.best_score_)\n","7eea359d":"y_pred_prob = xgb_cv_10.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='xgb')\nplt.legend()","506a08c3":"roc_auc_score(y_test, y_pred_prob)","aee897e8":"print(confusion_matrix(y_test, xgb_cv_10.predict(X_test)))\n\n#Tp#Fp\n#Fn#Tn","99836fab":"print(classification_report(y_test, xgb_cv_10.predict(X_test)))","71d9e434":"algos = [logreg_cv, knn_cv, SVM_rbf_cv, SVM_poly_cv, RF_cv, nb_m, xgb_cv]\nlabels = ['Logistic Regression', 'knn', 'SVM rbf', 'SVM poly', 'Random Forest','Naive Bayes', 'XGB']\n\nplt.figure(figsize = (12,8))\nplt.plot([0,1], [0,1], 'k--')\n\nfor i in range(len(algos)):\n    y_pred_prob = algos[i].predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    plt.plot(fpr, tpr, label=labels[i])\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')","6b00adaf":"## SVM with 'poly' Kernal","cebbb972":"## Logistic Regression:","69a6a783":"# Applying Machine Learning Algorithms:","ebccd45e":"## Gaussian Naive Bayes:","8dd4e1ae":"# Applying:","929df013":"## Random Forest","30bd732b":"## Extreme Gradient boosting:","9c4b38c1":"## SVM with 'rbf' Kernal:","c9c773f3":"## kNN:"}}