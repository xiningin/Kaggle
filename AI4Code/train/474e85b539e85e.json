{"cell_type":{"0cba57a3":"code","6df71977":"code","56f355be":"code","740ec174":"code","2639a0a5":"code","956dc582":"code","eb039bb3":"code","16abd2d7":"code","3145aeac":"code","958ba6b4":"code","2149c92b":"code","b12b8f15":"code","6203e8db":"code","27fa38e7":"code","934a5bbc":"code","0b601eb5":"code","8b0f6886":"code","7d4571c6":"code","d04de792":"markdown","a658a4cb":"markdown","3a5dba40":"markdown","7cca9a37":"markdown","5ce145b4":"markdown","53fcc9fb":"markdown","6453ce2d":"markdown"},"source":{"0cba57a3":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim\nfrom nltk.corpus import brown\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\nfrom nltk.corpus import wordnet as wn\nimport tqdm\nfrom sklearn.model_selection import StratifiedKFold","6df71977":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")","56f355be":"sample_sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","740ec174":"sample_sub ","2639a0a5":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","956dc582":"train","eb039bb3":"def simple_prepro(s):\n    return [w for w in s.replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"]","16abd2d7":"def simple_prepro_tfidf(s):\n    return \" \".join([w for w in s.lower().replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"])","3145aeac":"qt_max = max([len(simple_prepro(l)) for l in list(train[\"question_title\"].values)])\nqb_max = max([len(simple_prepro(l))  for l in list(train[\"question_body\"].values)])\nan_max = max([len(simple_prepro(l))  for l in list(train[\"answer\"].values)])\nprint(\"max lenght of question_title is\",qt_max)\nprint(\"max lenght of question_body is\",qb_max)\nprint(\"max lenght of question_answer is\",an_max)","958ba6b4":"w2v_model = gensim.models.Word2Vec(brown.sents())","2149c92b":"def get_word_embeddings(text):\n    np.random.seed(abs(hash(text)) % (10 ** 8))\n    words = simple_prepro(text)\n    vectors = np.zeros((len(words),100))\n    if len(words)==0:\n        vectors = np.zeros((1,100))\n    for i,word in enumerate(simple_prepro(text)):\n        try:\n            vectors[i]=w2v_model[word]\n        except:\n            vectors[i]=np.random.uniform(-0.01, 0.01,100)\n    return np.concatenate([np.max(np.array(vectors), axis=0),\n                          np.array([min(len(text),5000)\/5000,\n                                    min(text.count(\" \"),5000)\/5000,\n                                    min(len(words),1000)\/1000,\n                                    min(text.count(\"\\n\"),100)\/100,\n                                   min(text.count(\"!\"),20)\/20,\n                                   min(text.count(\"?\"),20)\/20])])\n                           ","b12b8f15":"question_title = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_title\"].values)]\nquestion_title_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_title\"].values)]\n\nquestion_body = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_body\"].values)]\nquestion_body_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_body\"].values)]\n\nanswer = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"answer\"].values)]\nanswer_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"answer\"].values)]","6203e8db":"gc.collect()\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 60)\ntfidf_question_title = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_title\"].values)])\ntfidf_question_title_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_title\"].values)])\ntfidf_question_title = tsvd.fit_transform(tfidf_question_title)\ntfidf_question_title_test = tsvd.transform(tfidf_question_title_test)\n\ntfidf_question_body = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_body\"].values)])\ntfidf_question_body_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_body\"].values)])\ntfidf_question_body = tsvd.fit_transform(tfidf_question_body)\ntfidf_question_body_test = tsvd.transform(tfidf_question_body_test)\n\ntfidf_answer = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"answer\"].values)])\ntfidf_answer_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"answer\"].values)])\ntfidf_answer = tsvd.fit_transform(tfidf_answer)\ntfidf_answer_test = tsvd.transform(tfidf_answer_test)","27fa38e7":"type2int = {type:i for i,type in enumerate(list(set(train[\"category\"])))}\ncate = np.identity(5)[np.array(train[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)\ncate_test = np.identity(5)[np.array(test[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)","934a5bbc":"train_features = np.concatenate([question_title, question_body, answer,\n                                 tfidf_question_title, tfidf_question_body, tfidf_answer, \n                                 cate\n                                ], axis=1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test, \n                               tfidf_question_title_test, tfidf_question_body_test, tfidf_answer_test,\n                                cate_test\n                                ], axis=1)","0b601eb5":"num_folds = 10\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nvalid_preds = np.zeros((train_features.shape[0],30))\nfor train_index, val_index in kf.split(train_features):\n    gc.collect()\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(1024, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(512),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 300, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    valid_preds[val_index] = preds\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation\/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n    test_preds += model.predict(test_features)\/num_folds\nprint(fold_scores)","8b0f6886":"valid = 0\nfor col_index, col in enumerate(target_cols):\n    valid += spearmanr(valid_preds[:, col_index], train[col].values).correlation\/30\nprint(\"valid score is \",valid)","7d4571c6":"sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\nfor col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]\nsub.to_csv(\"submission.csv\", index = False)","d04de792":"Here we use a trained word2vec model that is easily available with nltk.<br>\nWe used SWEM with max pooling.<br>\nHere, add information about the length of the sentence and the number of line ,'&nbsp;&nbsp;' ,'?' and '!'.<br>\nConsecutive spaces can be useful information.","a658a4cb":"This is basic preprocessing. This time, symbols and words are attached, so they are separated here.","3a5dba40":"# Introduction\nI will add new information to TFIDF+NN model(https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark ).<br>\nTFIDF can create features based on actual vocabulary, but it can't handle well when there is another word of close meaning.<br>\nTherefore, I thought that adding SWEM(https:\/\/arxiv.org\/abs\/1805.09843) using learned word2vec as a feature value would increase the score.","7cca9a37":"# fearure engineering","5ce145b4":"From here on, I'm quite referring to https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark.","53fcc9fb":"The contribution of the score was not great, but if you use bert etc. instead of brown, I think the score will go up more.","6453ce2d":"The text is so long that it is difficult to apply RNN to all series."}}