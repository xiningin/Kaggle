{"cell_type":{"bee571ff":"code","df4f5f41":"code","e5f31382":"code","0c8ce4c7":"code","8be1047d":"code","0db3b98a":"code","d11a8f35":"code","f513a681":"code","31686679":"code","bfaf6a79":"code","f65d14ae":"code","e50838e9":"code","b00ab66f":"code","051ee25b":"code","ecb42baf":"code","0365f86b":"code","71ca5f75":"code","ee74d835":"code","d9c0a004":"code","44dd3868":"code","9eb2e45e":"code","63edd5e4":"code","a5496f48":"code","e51800de":"code","19648216":"code","8d195c1d":"code","ee6c4c52":"code","f4b302c5":"code","6da63914":"code","1d02a64b":"code","451704d1":"code","dc227608":"code","a145ed89":"code","f9409bb0":"code","9fb62060":"code","9bf0d32f":"code","1ff83b0d":"code","11b8295b":"code","2741bbac":"code","20912a0e":"code","78563d0f":"code","f25b4cef":"code","55aace96":"code","8de1f4ef":"code","8e9fd673":"code","a60f0651":"code","1cd86334":"code","16b72180":"code","43f9413e":"code","a8bd274a":"code","18ecd580":"code","415032a7":"code","4a17fe70":"code","80e257ae":"code","4d7e06c5":"code","e2229dc5":"code","46e8c1e4":"code","12481541":"code","385d6eea":"code","56ce4dde":"code","861b0ec4":"code","a1f88d6f":"code","9388dce9":"code","5b10995b":"code","824e7b12":"code","f377a669":"code","e9b62a98":"code","ddf694a9":"code","0bc4b1b0":"code","cb008b6e":"code","853c9ed2":"code","4ae4ead8":"code","1ff6333c":"code","14b4b838":"code","a358daf6":"code","1ddde77e":"code","9df7491f":"code","76f2eecd":"code","d4cfae7b":"code","7ffbe6f3":"code","b777e9bc":"code","02b75f7e":"code","00d85959":"code","363c60cd":"code","8ed51b83":"markdown","74263016":"markdown","249d465d":"markdown","993d3e34":"markdown","23a2d155":"markdown","c7e34e22":"markdown","854ca8f6":"markdown","92d5841c":"markdown","7afddad8":"markdown","59afae8d":"markdown","ba21fcc1":"markdown","8b1de2cd":"markdown","8784615a":"markdown","6cc72ca6":"markdown","3e31b34f":"markdown"},"source":{"bee571ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df4f5f41":"# Importing all the required libraries\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn import linear_model, metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_log_error, make_scorer, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\n\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","e5f31382":"!pip install openpyxl","0c8ce4c7":"books_train = pd.read_excel('\/kaggle\/input\/predict-book-prices\/train.xlsx')\nbooks_test =  pd.read_excel('\/kaggle\/input\/predict-book-prices\/test.xlsx')","8be1047d":"books_train.head()","0db3b98a":"books_test.head()","d11a8f35":"books_train.shape","f513a681":"books_train.info()","31686679":"#Lets check the missing values\nmissing_data = round(100*(books_train.isnull().sum()\/len(books_train.index)),2)\nmissing_data[missing_data>0]","bfaf6a79":"# we dont have any missing values,\n# now lets analyze data in individual columns\nbooks_train.Author.value_counts().head()","f65d14ae":"books_train.Edition.value_counts().head()","e50838e9":"# we can extract year of publicaction from the Edition column\nbooks_train['Year'] = books_train['Edition'].apply(lambda x : x.split()[-1])","b00ab66f":"books_train.Year.value_counts().head()","051ee25b":"books_train.head()","ecb42baf":"# lets check the Reviews column\nbooks_train.Reviews.value_counts().head()","0365f86b":"# we can remove the the repeated sentence 'out of 5 stars' and make it a float value\n\nbooks_train.Reviews = books_train.Reviews.apply(lambda x : x.split()[0]).astype(float)","71ca5f75":"books_train.Reviews.head()","ee74d835":"# Lets check the Ratings column\nbooks_train.Ratings.head()","d9c0a004":"# lets rremove the repeated words 'customer reviews' and cast the column to int\n# remove commas before casting to int\n\nbooks_train.Ratings = books_train.Ratings.apply(lambda x : x.split()[0].replace(',','')).astype(int)","44dd3868":"\nbooks_train.Ratings.head()","9eb2e45e":"# lets check Genre\n\nbooks_train.Genre.value_counts().head()","63edd5e4":"# many values have unnecessary (Books) appended to it lets clean it\nbooks_train.Genre = books_train.Genre.apply(lambda x : x.replace('(Books)','').strip())","a5496f48":"books_train.Genre.value_counts().head()","e51800de":"books_train.head()","19648216":"books_train.BookCategory.value_counts().head()","8d195c1d":"# lets check the Year column\nbooks_train.Year.value_counts().head()","ee6c4c52":"# lets remove the non-numeric values from Year column\nbooks_train.Year = books_train.Year.apply(lambda x : x if x.isdigit() else None)","f4b302c5":"books_train.Year.value_counts().head()","6da63914":"books_train.head()","1d02a64b":"books_train.info()","451704d1":"books_train.Edition.value_counts().head()","dc227608":"books_train['Bind'] = books_train.Edition.apply(lambda x : x.split(',')[1] if x.startswith('(') else x.split(',')[0])","a145ed89":"books_train.Bind.value_counts()","f9409bb0":"books_train.Bind = books_train.Bind.apply(lambda x : x if x in ['Paperback','Hardcover','Mass Market Paperback','Sheet music','Flexibound'] else 'Others')","9fb62060":"books_train.Bind.value_counts()","9bf0d32f":"books_train.Genre.nunique()","1ff83b0d":"books_train.BookCategory.nunique()","11b8295b":"books_train.BookCategory.value_counts()","2741bbac":"books_train.head()","20912a0e":"# lets check unique values of Year, since Year is a categorical data\nbooks_train.Year.value_counts().sort_index().head()","78563d0f":"books_train.Year = books_train.Year.fillna(books_train.Year.mode().iloc[0])","f25b4cef":"# lets create bins for Year as less < 1980, 1981-1990, 1991-2000, 2001-2010, 2010-2020\nbooks_train.Year =  books_train.Year.astype(int)\nYear_bins = [0,1980,1990,2000,2010,2020,9999]\nbooks_train['Year_bins'] = pd.cut(books_train.Year,Year_bins, labels=['<1980','1990','2000','2010','2020','>2020'])","55aace96":"books_train.Year_bins.value_counts()","8de1f4ef":"books_train.head()","8e9fd673":"books_train.info()","a60f0651":"books_train = books_train.drop(['Title','Author','Edition','Synopsis','Year','Genre'],axis = 1)","1cd86334":"cat_cols = ['BookCategory','Bind','Year_bins']","16b72180":"df_dummies_cat = pd.get_dummies(books_train[cat_cols],drop_first=True)\ndf_dummies_cat.head()","43f9413e":"# drop categorical variables \nbooks_train = books_train.drop(cat_cols, axis=1)","a8bd274a":"# concatenate the dummies with the main df\nbooks_train = pd.concat([books_train, df_dummies_cat], axis=1)","18ecd580":"books_train.shape","415032a7":"# lets do a test train split\nnp.random.seed(0)\n\ndf_train, df_test = train_test_split(books_train,train_size = 0.7, test_size = 0.3, random_state = 100)","4a17fe70":"df_train.head()","80e257ae":"df_train.describe()","4d7e06c5":"# Lets do the X and y split\n# For train\ny_train = df_train.pop('Price')\nX_train = df_train\n\n# For  test\ny_test = df_test.pop('Price')\nX_test = df_test","e2229dc5":"# lets scale the data using MinMaxScaler\n\nscaler = MinMaxScaler()\ncols = ['Reviews', 'Ratings']\n# lets fit_transform the train data\nX_train[cols] = scaler.fit_transform(X_train[cols])\n\n# Log transformation on y_train\n#y_train = np.log1p(y_train)\n\n# We only transform the test data\n\nX_test[cols] = scaler.transform(X_test[cols])\n#y_test = np.log1p(y_test)","46e8c1e4":"# Lets use LinearRegression to make a simple model\nlm = LinearRegression()\nlm.fit(X_train,y_train)","12481541":"# Lets write a utility method to collate various metrics of the model after prediction\n\ndef model_metrics(y_train,y_pred_train,y_test,y_pred_test):\n\n    # RMSE on train data\n    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n    r2_score_train = r2_score(y_train, y_pred_train)\n    print('RMSE for train:',rmse_train)\n    print('R2 score for train:',r2_score_train)\n    print('Validation score for train: ', 1 - np.sqrt(np.square(np.log10(y_pred_train +1) - np.log10(y_train +1)).mean()))\n\n    # RMSE on test data\n    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n    r2_score_test = r2_score(y_test, y_pred_test)\n    print('RMSE for test:',rmse_test)\n    print('R2 score for test:',r2_score_test)\n    print('Validation score for test: ', 1 - np.sqrt(np.square(np.log10(y_pred_test +1) - np.log10(y_test +1)).mean()))","385d6eea":"# Lets make predictions on the train and test data\n\n# predictions on train data\ny_pred_train = lm.predict(X_train)\n\n# predictions on test data\ny_pred_test = lm.predict(X_test)\n\nmodel_metrics(y_train,y_pred_train,y_test,y_pred_test)","56ce4dde":"# lets select top 15 of 30 features using RFE then do a RIDGE and  LASSO regularization\n\n# Lets select top 15 featues using RFE\n\nlm2 = LinearRegression()\nlm2.fit(X_train, y_train)\n\n# select top 15 features\nrfe = RFE(lm2, n_features_to_select=15)             \nrfe = rfe.fit(X_train, y_train)","861b0ec4":"# Top features selected by RFE\ntop_cols = X_train.columns[rfe.support_]\ntop_cols","a1f88d6f":"# Lets make predictions on the train and test data for RFE\n\n# predictions on train data\ny_pred_train = rfe.predict(X_train)\n\n# predictions on test data\ny_pred_test = rfe.predict(X_test)\n\n# Lets print different model metrics\nmodel_metrics(y_train,y_pred_train,y_test,y_pred_test)","9388dce9":"from sklearn.ensemble import RandomForestRegressor","5b10995b":"rf = RandomForestRegressor(random_state=100, n_jobs=-1)","824e7b12":"# hyper parameter tuning\nparams = {\n    'max_depth': [2,3,5,10,20],\n    'min_samples_leaf': [5,10,20,50,100,200],\n    'n_estimators': [10, 25, 50, 100]\n}","f377a669":"def get_score(y_val, y_pred):\n    return np.sqrt(mean_squared_log_error(y_pred, y_val))\n    \ncriteria = make_scorer(get_score, greater_is_better = False)","e9b62a98":"# Using GridSearchCV to get best score\ngrid_search = GridSearchCV(estimator=rf,\n                           param_grid=params,\n                           cv = 5,\n                           n_jobs=-1, verbose=0,scoring = 'r2')","ddf694a9":"grid_search.fit(X_train, y_train)","0bc4b1b0":"# Best score \ngrid_search.best_score_","cb008b6e":"# Getting the classifier with the best score\nrf_best = grid_search.best_estimator_\nrf_best","853c9ed2":"# Lets make predictions on the train and test data\n\n# predictions on train data\ny_pred_train = rf_best.predict(X_train)\n\n# predictions on test data\ny_pred_test = rf_best.predict(X_test)\n\n# Lets print different model metrics\nmodel_metrics(y_train,y_pred_train,y_test,y_pred_test)","4ae4ead8":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor(random_state=100)\n\n# Using GridSearchCV to get best score\n\nparams_grid = {\n    'n_estimators': [50, 100, 200, 300, 500]\n}\ngrid = GridSearchCV(estimator=gbr,\n                           param_grid=params,\n                           cv = 5,\n                           n_jobs=-1, verbose=0,scoring = 'r2')\n","1ff6333c":"grid.fit(X_train, y_train)\n\n# Best score \ngrid.best_score_","14b4b838":"# Getting the classifier with the best score\ngbr_best = grid.best_estimator_\ngbr_best","a358daf6":"# Lets make predictions on the train and test data\n\n# predictions on train data\ny_pred_train = gbr_best.predict(X_train)\n\n# predictions on test data\ny_pred_test = gbr_best.predict(X_test)\n\n# Lets print different model metrics\nmodel_metrics(y_train,y_pred_train,y_test,y_pred_test)","1ddde77e":"books_test.head()","9df7491f":"def data_preprocess(df):\n    \n    df.Reviews = df.Reviews.apply(lambda x : x.split()[0]).astype(float)\n    \n    df.Ratings = df.Ratings.apply(lambda x : x.split()[0].replace(',','')).astype(int)\n    \n    \n    df['Bind'] = df.Edition.apply(lambda x : x.split(',')[1] if x.startswith('(') else x.split(',')[0])\n    df.Bind = df.Bind.apply(lambda x : x if x in ['Paperback','Hardcover','Mass Market Paperback','Sheet music','Flexibound'] else 'Others')\n    \n    \n    df['Year'] = df['Edition'].apply(lambda x : x.split()[-1])\n    mode = df.Year.mode().iloc[0]\n    df.Year = df.Year.apply(lambda x : x if x.isdigit() else mode).astype(int)\n \n    Year_bins = [0,1980,1990,2000,2010,2020,9999]\n    df['Year_bins'] = pd.cut(df.Year,Year_bins, labels=['<1980','1990','2000','2010','2020','>2020'])\n    \n    df = df.drop(['Title','Author','Edition','Synopsis','Year','Genre'],axis = 1)\n    df_dummies_cat = pd.get_dummies(df[cat_cols],drop_first=True)\n    df_dummies_cat.head()\n    df = df.drop(cat_cols, axis=1)\n    df = pd.concat([df, df_dummies_cat], axis=1)\n    \n    return df\n    ","76f2eecd":"books_test = data_preprocess(books_test)","d4cfae7b":"books_test.head()","7ffbe6f3":"books_test[cols] = scaler.transform(books_test[cols])","b777e9bc":"# predictions on test data\nbooks_test_preds = rf_best.predict(books_test)\n\n# the predicted price values would be log values , hence need take a reverse transform of log1p on \n# predicted price to get the actual price values\n\n#predicted_prices = np.expm1(books_test_preds)\npredicted_prices = books_test_preds","02b75f7e":"predicted_prices","00d85959":"predicted_prices_df = pd.DataFrame({'Price':predicted_prices})\npredicted_prices_df.Price.head(10)","363c60cd":"predicted_prices_df.to_excel('submission.xlsx', index = False)","8ed51b83":"#### Let replace the null values in Year with the mode value","74263016":"### Data Preparation","249d465d":"X - y split","993d3e34":"#### Lets scale","23a2d155":"#### Lets drop the columns that we will not be using for model building \ndropping Title, Author, Edition, Synopsis, Year\nLets drop Genre also, seems similar to BookCategory","c7e34e22":"### Lets carry out all the data preprocessing for test data as we did for train data for predictions","854ca8f6":"#### Lets do some EDA on the train data","92d5841c":"Lets do Feature Selection using RFE","7afddad8":"1. Data Understanding and Exploration\nLet's look at the dataset and understand the size, attribute names etc.","59afae8d":"Train Test split","ba21fcc1":"Lets try Gradient Boosting","8b1de2cd":"#### Lets use Random Forest model","8784615a":"Scaling the data","6cc72ca6":"#### Lets create dummy variables for the categorical variables","3e31b34f":"#### Lets scale the X_train and X_test numeric columns"}}