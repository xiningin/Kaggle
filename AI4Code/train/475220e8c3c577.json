{"cell_type":{"419a9ff3":"code","6e347e42":"code","f10a2cba":"code","309fd6e9":"code","c75750fa":"code","67977dd9":"code","cb228a04":"code","76729f85":"code","af6afba4":"code","aaaf3f56":"code","237b16b6":"code","ce2cd16c":"code","15514f2c":"code","cc7a3cd2":"code","e9f6917f":"code","ff51cf16":"code","158bd638":"code","1140de1a":"code","7b38b40e":"code","d13f7145":"code","08bdbd8d":"code","f96a440d":"code","bc397f2e":"code","0e451b8e":"markdown","271221b1":"markdown","a4822cf3":"markdown","5707e379":"markdown","7224c40f":"markdown","c64612f2":"markdown","650d06bf":"markdown","8a8721da":"markdown","327f828d":"markdown","9ba9542c":"markdown","6d430769":"markdown","97181408":"markdown","e2b39a62":"markdown","5828af00":"markdown","7eea75fa":"markdown","1e3b61c2":"markdown","51d0d6e0":"markdown","a46afb53":"markdown","3b5a9874":"markdown"},"source":{"419a9ff3":"import requests\nimport urllib.request\nimport time\nimport pandas as pd\nimport numpy as np\n\nimport pandas as pd\nimport nltk\nimport re\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk import pos_tag, wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nimport string\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n","6e347e42":"training_df=pd.read_csv(\"..\/input\/training_set_df.csv\")","f10a2cba":"training_df['QnA'] = training_df['Question'] + str(' ') + training_df['Answer']\n\ntraining_df['QnA'] = training_df['QnA'].astype(str)\n\nspecial_char= \"[,.\u00ae'&$\u2019\\\"\\-()?]\"\ntraining_df['QnA'] = training_df['QnA'].apply(lambda x:x.lower())\ntraining_df['QnA'] = training_df['QnA'].str.replace('[^\\w\\s]','')\ntraining_df['QnA'] = training_df['QnA'].str.replace('nan','')\n\n#create new column for Tokenized words\ntraining_df['QnA_tokenized'] = training_df['QnA'].apply(nltk.word_tokenize)\n\ntraining_df['QnA_tokenized'] = training_df['QnA_tokenized'].apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n\npd.set_option('display.max_columns', None)\ntraining_df.head()\n\n#stemming QnA_tokenized column\ntraining_df['QnA_stemmed'] = training_df['QnA_tokenized'].apply(lambda x: [stemmer.stem(y) for y in x])\n\ntraining_df['QnA_stemmed']= training_df['QnA_stemmed'].astype(str)\n\ntraining_df.to_csv('training_finale.csv')\n\n\n\n\n","309fd6e9":"test_df=pd.read_csv('..\/input\/test_set_df.csv')\n\ntest_df['QnA'] = test_df['Question'] + str(' ') + test_df['Answer']\n\ntest_df['QnA'] = test_df['QnA'].astype(str)\n\nspecial_char= \"[,.\u00ae'&$\u2019\\\"\\-()?]\"\ntest_df['QnA'] = test_df['QnA'].apply(lambda x:x.lower())\ntest_df['QnA'] = test_df['QnA'].str.replace('[^\\w\\s]','')\ntest_df['QnA'] = test_df['QnA'].str.replace('nan','')\n\ntest_df['QnA'] = test_df['Question'] + str(' ') + test_df['Answer']\n\ntest_df['QnA'] = test_df['QnA'].astype(str)\n\nspecial_char= \"[,.\u00ae'&$\u2019\\\"\\-()?]\"\ntest_df['QnA'] = test_df['QnA'].apply(lambda x:x.lower())\ntest_df['QnA'] =test_df['QnA'].str.replace('[^\\w\\s]','')\ntest_df['QnA'] = test_df['QnA'].str.replace('nan','')\n\ntest_df['QnA_tokenized'] = test_df['QnA'].apply(nltk.word_tokenize)\n\ntest_df['QnA_tokenized'] = test_df['QnA_tokenized'].apply(lambda x: [item for item in x if item not in stopwords.words('english') ])\n\n#remove stopwords\ntest_df['QnA_tokenized'] = test_df['QnA_tokenized'].apply(lambda x: [item for item in x if item not in stopwords.words('english')])\n\n#stemming QnA_tokenized column\ntest_df['QnA_stemmed'] = test_df['QnA_tokenized'].apply(lambda x: [stemmer.stem(y) for y in x])\n\ntest_df['QnA_stemmed'] = test_df['QnA_stemmed'].astype(str)\n\ntest_df.to_csv('test_final.csv')\n\ntest_df.head()\n","c75750fa":"training_df = pd.read_csv('training_finale.csv')\ntest_df = pd.read_csv('test_final.csv')","67977dd9":"## Training and evaluation of Models with Training Set\n\nCorpus = training_df\n\nTrain_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['QnA_stemmed'],Corpus['subreddit'],test_size=0.2, random_state = 0)\n\nEncoder = LabelEncoder()\nTrain_Y = Encoder.fit_transform(Train_Y)\nTest_Y = Encoder.fit_transform(Test_Y)\n\nTfidf_vect = TfidfVectorizer(max_features=1000)\nTfidf_vect.fit(Corpus['QnA_stemmed'])\nTrain_X_Tfidf = Tfidf_vect.transform(Train_X)\nTest_X_Tfidf = Tfidf_vect.transform(Test_X)\n","cb228a04":"Naive = naive_bayes.MultinomialNB()\nNaive.fit(Train_X_Tfidf,Train_Y)\npredictions_NB = Naive.predict(Test_X_Tfidf)\nprint(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)","76729f85":"array = confusion_matrix(Test_Y, predictions_NB)\ncm = pd.DataFrame(array, index = ['clean', 'dirty','mean','dad'], columns = ['clean', 'dirty','mean','dad'])\nsn.set(font_scale=1.4)\nsn.heatmap(cm, annot=True,annot_kws={\"size\": 16})\n\n","af6afba4":"SVM = svm.SVC(C=0.85, kernel='linear', degree=3, gamma='auto')\nSVM.fit(Train_X_Tfidf,Train_Y)\npredictions_SVM = SVM.predict(Test_X_Tfidf)\nprint(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)","aaaf3f56":"array = confusion_matrix(Test_Y, predictions_SVM)\ncm = pd.DataFrame(array, index = ['clean', 'dirty','mean','dad'], columns = ['clean', 'dirty','mean','dad'])\nsn.set(font_scale=1.4)\nsn.heatmap(cm, annot=True,annot_kws={\"size\": 16})","237b16b6":"Corpus1 = test_df\n\nTfidf_vect = TfidfVectorizer(max_features=1000)\nTfidf_vect.fit(Corpus1['QnA_stemmed'])\nTest_X_Tfidf = Tfidf_vect.transform(Corpus1['QnA_stemmed'])\n\npredictions = SVM.predict(Test_X_Tfidf)","ce2cd16c":"prediction_df = pd.DataFrame(predictions, columns = ['prediction'])","15514f2c":"prediction_df.loc[prediction_df.prediction == 0, 'predicted_category'] = 'clean' \nprediction_df.loc[prediction_df.prediction == 1, 'predicted_category'] = 'dirty' \nprediction_df.loc[prediction_df.prediction == 2, 'predicted_category'] = 'mean' \nprediction_df.loc[prediction_df.prediction == 3, 'predicted_category'] = 'dad'","cc7a3cd2":"prediction_df.to_csv('predictions_finale.csv')\n\nprediction_df = pd.read_csv('predictions_finale.csv')\n\nprediction_df.drop(prediction_df.columns[prediction_df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n","e9f6917f":"\npredicted_categories_df= pd.concat([test_df,prediction_df], axis = 1)\npredicted_categories_df.drop(predicted_categories_df.columns[predicted_categories_df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\npredicted_categories_df.to_csv('predicted categories_final.csv')","ff51cf16":"cat_likes_agg_df = predicted_categories_df.copy()\ncat_likes_agg_df = cat_likes_agg_df.groupby('predicted_category').sum().reset_index()\n\ncat_count_agg_df = predicted_categories_df.groupby('predicted_category').size().reset_index(name ='Count')","158bd638":"fig = plt.figure(figsize=(30,20))\nax = fig.add_subplot(121)\nax1 = fig.add_subplot(122)\n# Set the title of the subplot\nax.set_title(\"Likes vs Type of Joke\")\n# Plot from the df\ncat_likes_agg_df.plot(x='predicted_category', y='Likes', kind = 'bar', ax =ax)\nax.set_ylabel('No. of Likes')\nax.set_title ('Likes vs Category')\nplt.xlabel(\"Joke type\", fontsize = 14)\nplt.ylabel(\"No. of Likes\", fontsize = 14)\n\nax1.set_ylabel('Count of Jokes')\nax1.set_title ('Count vs Category')\ncat_count_agg_df.plot(x='predicted_category', y='Count', kind = 'bar', ax =ax1)\n\nplt.show()","1140de1a":"agg_count_likes_df= pd.concat([cat_likes_agg_df,cat_count_agg_df], axis = 1)\n\nagg_count_likes_df['Avg likes per joke'] = agg_count_likes_df['Likes']\/agg_count_likes_df['Count']\n\nagg_count_likes_df = agg_count_likes_df.loc[:,~agg_count_likes_df.columns.duplicated()]\n","7b38b40e":"fig = plt.figure(figsize=(30,20))\nax = fig.add_subplot(111)\n\nax.set_title(\"Avg Likes vs Type of Joke\")\n\nagg_count_likes_df.plot(x='predicted_category', y='Avg likes per joke', kind = 'bar', ax =ax)\nax.set_ylabel('No. of Likes', fontsize = 20)\nax.set_title ('Averaged No. of Likes vs Category', fontsize = 20)\n\nplt.xlabel(\"Joke type\", fontsize = 14)\n\nplt.ylabel(\"No. of Likes\", fontsize = 14)\n\n\nplt.show()\n","d13f7145":"from wordcloud import WordCloud","08bdbd8d":"dirtyjokes_list =[]\ndirtyjokes_series = training_df.copy()\ndirtyjokes_series = training_df[training_df['subreddit'] == 'DirtyJokes']\ndirtyjokes_series = dirtyjokes_series['QnA_tokenized']\ndirtyjokes_list = dirtyjokes_series.tolist()\n\n\ndadjokes_list =[]\ndadjokes_series = training_df.copy()\ndadjokes_series = training_df[training_df['subreddit'] == 'MeanJokes']\ndadjokes_series = dadjokes_series['QnA_stemmed']\ndadjokes_list = dadjokes_series.tolist()\n","f96a440d":"plt.figure(figsize=(20,10))\n\n# create wordcloud here\nplt.figure(figsize=(15,8))\nwc = WordCloud(width=400, height=150, background_color=\"white\", max_words=20, relative_scaling=1.0)\ndesc_wordcloud = wc.generate(str(dirtyjokes_list))\n\n\nplt.imshow(desc_wordcloud)\nplt.axis(\"off\")\nplt.title(\"Wordcloud of dirty jokes\", fontsize=20)\nplt.show()\n","bc397f2e":"plt.figure(figsize=(20,10))\n\n# create wordcloud here\nplt.figure(figsize=(15,8))\nwc = WordCloud(width=400, height=150, background_color=\"white\", max_words=20, relative_scaling=1.0)\ndesc_wordcloud = wc.generate(str(dadjokes_list))\n\n\nplt.imshow(desc_wordcloud)\nplt.axis(\"off\")\nplt.title(\"Wordcloud of mean jokes\", fontsize=20)\nplt.show()","0e451b8e":"### Research Topic & Hypothesis\n\n#### Methodology \n\nWhile it may have been more straightfoward to simply aggregate and visualise the number of likes of each of the abovementioned subthreads(r\/CleanJokes, r\/DirtyJokes, r\/Meanjokes and r\/DadJokes) and juxtapose them against each other to determine the popularity of each joke category against another, a normalised standard of measurement (r\/Jokes) was pertinent to the impartiality of our objective. Possible causes for deviation from the normalised standard include i) number of subscribeers to each subthread and ii) type(age, gender, humor-preference) of subscribers of each subthread.\nHence, it was deemed more objective to use a generic joke thread with a substantial and varied enough membership rates and types. \n\nTherein lies the requirement for ML algorithms to categorise the jokes within r\/Jokes into their specific categories after training them with the abovementioned training dataset. \n\nAfter categorisation, visualisations of the aggregated categories and their corresponding likes were carried out to establish the most and least popular joke types, and a wordcloud was used to identify common words associated to the specific category of jokes. \n\n\n#### Datasets \n\nAll data sets were scraped from Reddit threads through the Reddit API (praw). A total of 5 subthreads were scraped; the data from subthreads r\/CleanJokes, r\/DirtyJokes, r\/Meanjokes and r\/DadJokes were combined and used as our training and validation dataset for our SVM and NB algorithms, and the subthread r\/Jokes was used as the actual test set for which the algorithm(s) were to be applied on. \n\n*for some reason, pip couldn't install the praw API on Kaggle, so the whole webscraping portion was omitted*","271221b1":"With a full dataframe of jokes, coupled with their respective number of likes and predicted categories, an aggregation of the jokes into their category is carried out to study the most likeable joke category, and their frequency of occurence within the r\/Joke subthread.","a4822cf3":"Here, we instantiate a dataframe and populate it according to the categories predicted by the SVM algorithm.\nWe then concatenate this dataframe of prediction results with the organice jokes themselves. Since the index hasn't been changed from the prediction step to the concatanation step, there was no need to merge on the ID. ","5707e379":"<center><h1>LMAO: the science <\/h1><\/center>\n\n<center><h2>A Natural Language Analysis through Text Categorisation with Machine Learning algorithms<\/h2><\/center>\n\n\n### Author(s)\n\n- Lim Yi Quan Robin\n- Nicholas Tee\n- Zhang Quan\n","7224c40f":"### Post text-classification analysis","c64612f2":"While it is clear that jokes that are classed as 'dad jokes' seem to have the most number of likes, there may still be a possibility that this was only caused by their abundance within the r\/jokes thread. Hence, a normalisation of the number of likes of each category to the number of jokes in each category was carried out, and plotted as below:","650d06bf":"## Executive Summary \nThis project aims to establish key central themes surrounding the most viral and likeable content on the Reddit subthread r\/Jokes to facilitate real world social interactions and applications. \n\nWe believe that dirty jokes would be the most popular ones as they can easily catch readers' attention and can be adapted for real life social interactions. Our findings are mostly consistent with our hypothesis, which revealed the proclivity of humans to take something that\u2019s usually regarded as taboo and crude, and turn it into a form of digestible humour. \n","8a8721da":"### Run SVM Model on Test set","327f828d":"### Wordcloud vocabulary\n\nWordclouds were formed based on the original, training subthreads, where the accuracy of their categorisation is more likely to be higher than a measly 48%. ","9ba9542c":"## Text Classification with ML Models","6d430769":"### Insights and Evaluation \n\n- Insight 1 \n\n     From the above wordclouds, it can be determined that jokes revolving around notions of man, woman and their concomitant actions and bodyparts tend to be more popular, while racial and religious themes of jokes tend to not sit well with general public humor. \n     \n\n- Insight 2\n\n    It is apparent that more context is required in the processing of languages, especially when we attempt to categorise humor. One possibility may be fine-tuning the SVM model with ngrams, or at least bigrams, allowing for the model to adjust to a larger degree of context. \n    \n\n- Insight 3 \n\n     An extension of this project may include its applications to the digital marketing sector, allowing teams to streamline their content creation processes by capitalising and focusing on lexicon that is highly likeable, and differentiating between acceptable dirty jokes and less tolerable mean jokes. ","97181408":"##  Training set text pre-processing","e2b39a62":"With this new normalised data visualisation, it can be observed that dirty jokes, once again, captivate the hearts and minds of the people, and being mean is clearly a no-no.","5828af00":"#### Model Selection\nWhile we can see that the SVM has a slightly higher accuracy than that of the NB algorithm, the justification for selecting the SVM model is buttressed by a better spread in its accurate predictions (i.e, the range of predictions, if correct, are not too far apart from each other in value)","7eea75fa":"## Test set text pre-processing","1e3b61c2":"#### Naive-Bayes Algorithm Training and Validation","51d0d6e0":"### Vectorization ","a46afb53":"### Conclusion\n\nFrom the above wordclouds, it can be determined that jokes revolving around notions of man, woman and their concomitant actions and bodyparts tend to be more popular, while racial and religious themes of jokes tend to not sit well with general public humor. \n\nIn spite of this, it is apparent that more context is required in the processing of languages, especially when we attempt to categorise humor. One possibility may be fine-tuning the SVM model with ngrams, or at least bigrams, allowing for the model to adjust to a larger degree of context. ","3b5a9874":"#### SVM Algorithm Training and Validation"}}