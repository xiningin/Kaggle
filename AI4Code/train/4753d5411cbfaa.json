{"cell_type":{"295b3cbf":"code","ff05b7c2":"code","37b40154":"code","149f598b":"code","8f4680fd":"code","42b62017":"code","f29f9f80":"code","cee79e57":"code","9f5db87c":"code","56e91b80":"code","f59b5391":"code","cbce1b8f":"markdown","096c6624":"markdown","f3195e6f":"markdown","ba1c78d4":"markdown","450506fe":"markdown","6cc93507":"markdown","c5fdd82a":"markdown","e9474309":"markdown","d319686a":"markdown","e540e788":"markdown","a7f0773e":"markdown"},"source":{"295b3cbf":"!nvidia-smi","ff05b7c2":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import Conv2d, Linear, CrossEntropyLoss\nfrom torchvision.models import densenet201, resnet152, vgg19_bn \nfrom torchvision.transforms import Compose, RandomAffine, RandomApply, ColorJitter, Normalize, ToTensor","37b40154":"folders = {\n    \"plots\": \"plots\",\n    \"models\": \"models\",\n    \"results\": \"results\"\n}\nfor key in folders.keys():\n    try:\n        os.makedirs(folders[key])\n    except FileExistsError:\n        # if file exists, pass\n        pass","149f598b":"class PCam(Dataset):\n    \"\"\"Patch Camelyon dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, train=True, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with labels.\n            root_dir (string): Root directory.\n            train (boolean): Whether loading training or testing data. \n                            This is required to have same number of examples in each \n                            classification to be able to train better.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        if train:\n            # make the number of example in each classification equal\n            dataframe = pd.read_csv(os.path.join(root_dir, csv_file))\n            min_value = dataframe['label'].value_counts().min()\n            frames = []\n            for label in dataframe['label'].unique():\n                frames.append(dataframe[dataframe['label'] == label].sample(min_value))\n                # .sample(frac=1) shuffles the data\n                # .reset_index(drop=True) do not add index while shuffling\n            self.labels = pd.DataFrame().append(frames).sample(frac=1).reset_index(drop=True)\n            self.data_folder = \"train\"\n        else:\n            self.labels = pd.read_csv(os.path.join(root_dir, csv_file))\n            self.data_folder = \"test\"\n        \n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image_name = os.path.join(self.root_dir,\n                                \"%s\/%s.tif\" % (self.data_folder, self.labels.iloc[idx, 0]))\n        image = Image.open(image_name)\n        # reduce image size to be able to train fast\n        image.thumbnail((40, 40), Image.ANTIALIAS)\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return self.labels.iloc[idx, 0], image, self.labels.iloc[idx, 1]","8f4680fd":"NUM_CLASSES = 2  # number of classes\nBATCH_SIZE = 32  # mini_batch size\nMAX_EPOCH = 10  # maximum epoch to train\nSTEP_SIZE = 2  # decrease in learning rate after epochs\nLEARNING_RATE = 0.00007  # learning rate\nGAMMA = 0.1  # used in decreasing the gamma","42b62017":"# other transformations are not included because they are included in these or those are not required in real life\ntrain_transform = Compose([\n    RandomAffine(45, translate=(0.15, 0.15), shear=45),\n    RandomApply([ColorJitter(saturation=0.5, hue=0.5)]),\n    ToTensor(),\n    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntest_transform = Compose(\n    [ToTensor(),\n     Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = PCam(csv_file='train_labels.csv', root_dir='..\/input', train=True, transform=train_transform)\ntrainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n\ntestset = PCam(csv_file='sample_submission.csv', root_dir='..\/input', train=False, transform=test_transform)\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","f29f9f80":"def eval_ensemble(nets, criterion, dataloader):\n    correct = 0\n    total = 0\n    total_loss = 0\n\n    for data in dataloader:\n        _, images, labels = data\n        #         images, labels = Variable(images), Variable(labels)\n        images, labels = Variable(images).cuda(), Variable(labels).cuda()\n\n        predictions = torch.zeros([images.size(0), NUM_CLASSES]).cuda()\n        for net in nets:\n            net.eval()\n            outputs = net(images)\n            predictions = predictions.add(outputs)\n\n        # predictions = predictions \/ len(nets)  # redundant division\n        _, predicted = torch.max(predictions.data, 1)\n        \n        total += labels.size(0)\n        correct += (predicted == labels.data).sum().item()\n        \n        loss = criterion(predictions, labels)\n        total_loss += loss.item()\n    return total_loss \/ total, correct \/ total","cee79e57":"def train_ensemble(nets, optimizers, schedulers, criterion, eval_criterion):\n    ensemble_name = 'ensemble'\n    train_loss_array = []\n    test_loss_array = []\n    train_accuracy_array = []\n    test_accuracy_array = []\n\n    print('Start training...')\n    for epoch in range(MAX_EPOCH):  # loop over the dataset multiple times\n        for scheduler in schedulers:\n            scheduler.step()\n        \n        running_loss = 0.0\n        for i, data in enumerate(trainloader):\n            _, images, labels = data\n            #             inputs, labels = Variable(inputs), Variable(labels)\n            images, labels = Variable(images).cuda(), Variable(labels).cuda()\n\n            predictions = torch.zeros([images.size(0), NUM_CLASSES]).cuda()\n            for net, optimizer in zip(nets, optimizers):\n                net.train()\n                optimizer.zero_grad()\n                outputs = net(images)\n                predictions = predictions.add(outputs)\n\n            # predictions = predictions \/ len(nets)  # redundant division\n            \n            # back prop\n            loss = criterion(predictions, labels)\n            loss.backward()\n            for optimizer in optimizers:\n                optimizer.step()\n            running_loss += loss.item()\n            \n            if i % 500 == 499:  # print every 2000 mini-batches\n                print('Step: %5d avg_batch_loss: %.5f' % (i + 1, running_loss \/ 500))\n                running_loss = 0.0\n                \n        print('Finish training this EPOCH, start evaluating...')\n        train_loss, train_acc = eval_ensemble(nets, eval_criterion, trainloader)\n        test_loss, test_acc = eval_ensemble(nets, eval_criterion, testloader)\n        print('EPOCH: %d train_loss: %.5f train_acc: %.5f test_loss: %.5f test_acc %.5f' %\n              (epoch + 1, train_loss, train_acc, test_loss, test_acc))\n\n        train_loss_array.append(train_loss)\n        test_loss_array.append(test_loss)\n\n        train_accuracy_array.append(train_acc)\n        test_accuracy_array.append(test_acc)\n    print('Finished Training')\n\n    # plot loss\n    plt.clf()\n    plt.plot(list(range(1, MAX_EPOCH + 1)), train_loss_array, label='Train')\n    plt.plot(list(range(1, MAX_EPOCH + 1)), test_loss_array, label='Test')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss vs Epochs [%s]' % net.name)\n    plt.savefig('.\/%s\/loss-%s.png' % (folders['plots'], ensemble_name))\n\n    # plot accuracy\n    plt.clf()\n    plt.plot(list(range(1, MAX_EPOCH + 1)), train_accuracy_array, label='Train')\n    plt.plot(list(range(1, MAX_EPOCH + 1)), test_accuracy_array, label='Test')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Accuracy vs Epochs [%s]' % net.name)\n    plt.savefig('.\/%s\/accuracy-%s.png' % (folders['plots'], ensemble_name))","9f5db87c":"def dump_ensemble_results(dataloader, nets):\n    ensemble_name = 'ensemble'\n    \n    alphas = []\n    for net in nets:\n        net.eval()\n        \n        true_positives = 0\n        for data in trainloader:\n            _, images, labels = data\n            images, labels = Variable(images).cuda(), Variable(labels).cuda()\n            outputs = net(images)\n            _, outputs = torch.max(outputs.data, 1)\n            index = (labels == 1)\n            true_positives += (outputs[index] == labels[index].data).sum().item()\n        alphas.append(true_positives)\n#     total_true_positives = sum(alphas)\n    \n    results = pd.DataFrame()\n    for data in dataloader:\n        image_names, images, labels = data\n#         images, labels = Variable(images), Variable(labels)\n        images, labels = Variable(images).cuda(), Variable(labels).cuda()\n    \n        predictions = torch.zeros([images.size(0), NUM_CLASSES]).cuda()\n        for index, net in enumerate(nets):\n            net.eval()\n            outputs = net(images) * alphas[index]  # \/ total_true_positives\n            predictions = predictions.add(outputs)\n\n#         predictions = predictions \/ total_true_positives\n        _, predictions = torch.max(predictions.data, 1)\n        results = results.append(pd.DataFrame({\"id\": image_names, \"label\": predictions.cpu().numpy()}))\n    results.to_csv(\"%s\/%s.csv\" % (folders['results'], ensemble_name), index=False)","56e91b80":"start = time.time()\nnet_list = []\noptimizer_list = []\nscheduler_list = []\n\n# DenseNet201\ndense_net = densenet201()\ndense_num_ftrs = dense_net.classifier.in_features\ndense_net.classifier = Linear(dense_num_ftrs, NUM_CLASSES)\ndense_net.name = \"DenseNet201\"\ndense_net = dense_net.cuda()\ndense_optimizer = Adam(dense_net.parameters(), lr=LEARNING_RATE)\ndense_exp_lr_scheduler = lr_scheduler.StepLR(dense_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\nnet_list.append(dense_net)\noptimizer_list.append(dense_optimizer)\nscheduler_list.append(dense_exp_lr_scheduler)\n\n# ResNet152\nres_net = resnet152()\nres_num_ftrs = res_net.fc.in_features\nres_net.fc = Linear(res_num_ftrs, NUM_CLASSES)\nres_net.name = \"ResNet152\"\nres_net = res_net.cuda()\nres_optimizer = Adam(res_net.parameters(), lr=LEARNING_RATE)\nres_exp_lr_scheduler = lr_scheduler.StepLR(res_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\nnet_list.append(res_net)\noptimizer_list.append(res_optimizer)\nscheduler_list.append(res_exp_lr_scheduler)\n\n# VGG19\nvgg_net = vgg19_bn()\nvgg_num_ftrs = vgg_net.classifier._modules['6'].in_features\nvgg_net.classifier._modules['6'] = Linear(vgg_num_ftrs, NUM_CLASSES)\nvgg_net.name = \"VGG11\"\nvgg_net = vgg_net.cuda()\nvgg_optimizer = Adam(vgg_net.parameters(), lr=LEARNING_RATE)\nvgg_exp_lr_scheduler = lr_scheduler.StepLR(vgg_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\nnet_list.append(vgg_net)\noptimizer_list.append(vgg_optimizer)\nscheduler_list.append(vgg_exp_lr_scheduler)\n\ntrain_criterion = CrossEntropyLoss()\nval_criterion = CrossEntropyLoss(reduction='sum')\ntrain_ensemble(net_list, optimizer_list, scheduler_list, train_criterion, val_criterion)\nprint(\"Time taken: %d secs\" % int(time.time() - start))","f59b5391":"dump_ensemble_results(testloader, net_list)","cbce1b8f":"## Classify Image\n\nGet predictions on the image using each of the model in ensemble. Prediction by a model impacts the overall prediction by a factor of its true positive count. This is because accuracy of a model is decided by sensitivity for data where false negative can cost a huge loss (here human life).","096c6624":"## Evaluation\n\nFunction to evaluate the model during training. A prediction is done based on the average value of the predictions using all the models.","f3195e6f":"## Loading Data\n\nAugment training data and not testing data and them in their respective data loaders.","ba1c78d4":"## Generate required folders\n\nGenerate the required folders to be able to\n* save the states\n* load from saved states\n* save plots\n* save results","450506fe":"## Train Model\n\nFunction to train the ensemble model. Loss is calculated based on the average prediction during training and gradients of each model in ensemble is calculated using this loss.","6cc93507":"## Classify Images\n\nUse the above trained ensemble to classify each of the testing image.","c5fdd82a":"## Hyperparameters\n\nSetting the hyperparameters.","e9474309":"## Import Libraries\n\nImport all the required libraries.","d319686a":"## Create Ensemble\n\nCreate a list of DenseNet201, ResNet152 and VGG19_BN. Each model has it's own criterion, optimizer and learning rate scheduler.","e540e788":"## PCam Dataset\n\nCustom dataset definition to be able to use PyTorch style of efficient data loading.\n\n### Challenges Faced\n\nA deep neural network tries to get the best performance and so having relatively more number of examples in one class is making the network to have a biased view of it's world. So, have to come up with a way to have same number of examples for each category.","a7f0773e":"## Check GPU Availability\n\nMake sure that GPU is available. If not turn the GPU state to on in Settings."}}