{"cell_type":{"3c47d338":"code","b0a8183f":"code","d4b99085":"code","21c47cd3":"code","f7c6bb1e":"code","3bf10abd":"code","3cae6dc2":"code","c38c78ee":"code","64259ac8":"code","a18e05bf":"code","87bf563a":"code","137604dd":"code","a0275c37":"code","e8b1f6f4":"code","71574a7f":"code","ec241bc8":"code","6f962664":"code","b1c2400f":"code","8d8dcc0c":"code","6131eda2":"code","3d0456f6":"code","eebc86a0":"code","a44a35bc":"code","da7107cf":"code","456ab66e":"code","2178138e":"code","dab0d0cc":"code","e038ef56":"code","6df7d82e":"code","02ba58ad":"code","eb8b52f1":"code","18a05074":"code","58ed11e0":"code","99f134d2":"code","541f8880":"code","13602c2a":"code","431ce39a":"code","99470b65":"code","818209f9":"code","5ffc8d98":"code","6174705a":"code","d4d3735c":"code","d0950aa2":"code","da724956":"code","8e5dbff3":"code","448fdbdd":"code","2bb5b7fa":"code","961e4fc9":"code","6d2194fa":"code","eb91e4b1":"code","50ae5bd1":"code","b8119b70":"code","b3a7d0ee":"code","c26d7db2":"code","a4f770b6":"code","f0cba396":"code","8c710531":"code","cca41479":"code","9add3bc9":"code","8cd40acf":"code","95ec8968":"code","af91b4bf":"code","8cd256a6":"code","773e8c90":"code","3c116c52":"code","19853d26":"code","bcea79d9":"code","7ddebd36":"code","4d219deb":"code","1665fa91":"code","10c5f185":"code","dfaa65c4":"code","47a2b965":"code","256f29fc":"code","2443c419":"code","413bba75":"code","be6493ea":"code","2acb23d5":"code","65e66a3d":"code","d1bce6ac":"code","7bd019f9":"code","b192ef16":"code","24d07e0f":"code","34b8fca8":"code","88ca294c":"code","401cef2e":"code","8b86b5a0":"markdown","af6e3b71":"markdown","e68deff0":"markdown","1400674c":"markdown","ae043847":"markdown","8918df18":"markdown","f9487545":"markdown","9e277160":"markdown","d8ec85ee":"markdown","82064857":"markdown","9e12e828":"markdown","a0fd7d4f":"markdown","c5160e3e":"markdown","d43a1a3c":"markdown","5a1eaa26":"markdown","ebaddcc3":"markdown","49ce89c2":"markdown","83a4aa42":"markdown","6e6a0df3":"markdown","da4fc6a4":"markdown","4354b7fc":"markdown","744c9d28":"markdown","b2a152e6":"markdown","16dda0a5":"markdown","5e5f774a":"markdown","88785b4d":"markdown","cb011f18":"markdown","e1d1bd3e":"markdown","55f9d160":"markdown","ce655089":"markdown","7dec90df":"markdown","0e404dd5":"markdown","9c80b350":"markdown","30ed45d2":"markdown","a73c30d9":"markdown","2aedadb9":"markdown","87521c8c":"markdown","d0bd73bc":"markdown","183d1e13":"markdown","6cfc9cec":"markdown","7cd8df70":"markdown","fa361c6c":"markdown","a8fbf8b4":"markdown","71fd79b3":"markdown","38b5bcde":"markdown","b5e331da":"markdown","eb43190e":"markdown","a0261755":"markdown","31e44af4":"markdown","90a0b0a1":"markdown","c0d1cc6f":"markdown","8daa8658":"markdown","76874725":"markdown","278ce649":"markdown","4d2118ad":"markdown","22b6c5a4":"markdown","315b7be9":"markdown","563fe6c9":"markdown","a4169396":"markdown","c5ffb80f":"markdown","fe1978a9":"markdown","25eacbad":"markdown","71011e71":"markdown","fae77439":"markdown","4a63a82f":"markdown","0ffa1a79":"markdown","69fdffbf":"markdown","2229d070":"markdown","27073857":"markdown","d4389b9f":"markdown","7f411e5a":"markdown","27a2bc14":"markdown","1f9c9b3e":"markdown","6857631a":"markdown","768330de":"markdown","fa4124a2":"markdown","f1233809":"markdown","316fb8f4":"markdown","37d1b68d":"markdown","f8d0aa4b":"markdown","76c7696e":"markdown","5010fddd":"markdown","50ba0200":"markdown","b3c32ec3":"markdown","9ca46309":"markdown","fca6c2e8":"markdown","ffba9da8":"markdown","02c82c70":"markdown","76c59f85":"markdown","4f0c3f74":"markdown"},"source":{"3c47d338":"%matplotlib inline\n\nimport copy\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as lines\nimport seaborn as sns\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b0a8183f":"import os\nos.chdir(\"\/kaggle\/input\/titanic\/\")","d4b99085":"#  Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n# Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Model selector\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n\n# Model evaluation\nfrom sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix","21c47cd3":"# import gender_submission, train dataset and predict datasets using pandas 'read_csv' function\ndf_train = pd.read_csv('train.csv')\ndf_predict = pd.read_csv('test.csv')","f7c6bb1e":"# for data exploration and analysis we will create a deep copy of out train dataset\ndf_eda = copy.deepcopy(df_train)","3bf10abd":"df_eda.head(2)","3cae6dc2":"data_definition = {'Variable':['survival', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n                   'Definition':['survival', 'Ticket class', 'Sex', 'Age in years', '# of siblings \/ spouses aboard the Titanic', \\\n                                 '# of parents \/ children aboard the Titanic', 'Ticket number', 'Passenger fare', 'Cabin number', 'Port of Embarkation'],\n                   'Key':['0 = No, 1 = Yes', '1 = 1st, 2 = 2nd, 3 = 3rd', 'Male or Female', \\\n                          'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'C = Cherbourg, Q = Queenstown, S = Southampton']}\n\npd.DataFrame.from_dict(data_definition)","c38c78ee":"# Consistent plots\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 10,5\nrcParams['xtick.labelsize'] = 9\nrcParams['ytick.labelsize'] = 9\nrcParams['axes.labelsize'] = 10\nrcParams['axes.grid.axis'] = 'y'\nrcParams['grid.linestyle'] = '--'\nrcParams['grid.alpha'] = '0.4'","64259ac8":"# set color mapping to sequential color palette\ncmap = 'PuBu'","a18e05bf":"# set color palette to discrete color palette \ncustomPalette = sns.color_palette('Pastel1')\nsns.set_palette(sns.color_palette(customPalette))","87bf563a":"customPalette","137604dd":"# sns.set_theme(style='whitegrid', font='serif')","a0275c37":"fig_title_prop = {'size': 18, 'color': '#4a4a4a', 'fontweight':'bold'}","e8b1f6f4":"sns.set_context('notebook', font_scale=1, rc={'lines.linewidth': 2.5})","71574a7f":"df_train.shape","ec241bc8":"datadict = pd.DataFrame()\ndatadict['dtype'] = df_eda.dtypes\ndatadict['count'] = df_eda.count()\ndatadict['missing_val'] = df_eda.isnull().sum()\ndatadict['missing_per'] = round(datadict['missing_val'] \/ datadict['count'], 2)\ndatadict['nunique'] = df_eda.nunique()","6f962664":"datadict.sort_values(by=['dtype', 'missing_per'], ascending=False)\ndatadict.style.background_gradient(cmap=cmap) ","b1c2400f":"# get discripte statistcs on \"object\" datatypes\ndf_train.describe(include=['object'])","8d8dcc0c":"# get discriptive statistcs on \"number\" datatypes\nsubset=['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ndf_train.describe(include=['number'])","6131eda2":"survive_per = df_eda['Survived'].value_counts(normalize=True)","3d0456f6":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n\nsurvive_per.plot(kind='pie', \n                 autopct='%.2f',\n                 labels=None,\n                 startangle = 180)\n\nax.set(ylabel='')\n\n# add horizontal line\nline = lines.Line2D([0, 1], [0.95, 0.95], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\n# add figure text\nfig.text(0, 0.98, 'Pieplot for Survival Rate', fontdict=fig_title_prop)\n\nplt.legend(labels=['Not Survived', 'Survived'], loc=(0,0), edgecolor='None', ncol=2, bbox_transform=True)\nplt.tight_layout()\nplt.show()","eebc86a0":"col_names = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nsub_count = 1\n\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(10,5))\nfig.subplots_adjust(hspace=0.5)\n# fig.suptitle('Countplot for Discrete Data', fontsize=15, fontweight=200)\n\nfor ax, feature in zip(axes.flatten(), df_eda[col_names].columns):\n    ax.set(title='Countplot #{}: {}'.format(sub_count, feature))\n    ax.xaxis.label.set_visible(False)\n    sns.countplot(df_eda[feature],ax=ax)\n    sub_count += 1\n\n# add horizontal line\nline = lines.Line2D([0, 1], [1.05, 1.05], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\nfig.text(0, 1.08, 'Countplot for Discrete Data', fontdict=fig_title_prop)\n\nplt.tight_layout()\nplt.show()","a44a35bc":"def triple_plot(data, title, c):\n    fig, ax = plt.subplots(3,1,figsize=(12,6),sharex=True)\n        \n    # add distribution plot\n    sns.distplot(data, ax=ax[0],color=c)\n    ax[0].set(xlabel=None)\n    ax[0].set_title('Histogram + KDE')\n    \n    # add box plot\n    sns.boxplot(data, ax=ax[1],color=c)\n    ax[1].set(xlabel=None)\n    ax[1].set_title('Boxplot')\n    \n    # add voilin plot\n    sns.violinplot(data, ax=ax[2],color=c)\n    ax[2].set(xlabel=None)\n    ax[2].set_title('Violin plot')\n    \n    # add figure title line\n    line = lines.Line2D([0, 1], [1, 1], transform=fig.transFigure, zorder=1000, figure=fig)\n    fig.lines.extend([line])\n    \n    # add title\n    fig.text(x=0, y=1.03, s=title, fontdict=fig_title_prop)\n     \n    plt.tight_layout(pad=3.0)\n    plt.show()","da7107cf":"triple_plot(df_eda['Age'], 'Distribution of Age', customPalette[1])","456ab66e":"triple_plot(df_eda['Fare'], 'Distribution of Fare', customPalette[0])","2178138e":"pd.value_counts(df_eda[df_eda['Age'] > 80]['Age'].values.flatten()).sum()","dab0d0cc":"pd.value_counts(df_eda[df_eda['Fare'] > 300]['Fare'].values.flatten()).sum()","e038ef56":"data = df_eda[['Age', 'Fare']]\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 5), sharex=False, sharey=False)\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor ax, feature, color in zip(ax.flatten(), data.columns, customPalette[:2]):\n    g = sns.distplot(data[feature], kde=True,ax=ax, color = color)\n    ax.xaxis.label.set_visible(False)\n    ax.yaxis.label.set_visible(False)\n    ax.set(title = '{}'.format(feature))\n    ax.text(x=0.97, y=0.90, transform=ax.transAxes, s=\"Skewness: %.2f\" % data[feature].skew(),\n            fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\n            backgroundcolor='white')\n    ax.text(x=0.97, y=0.84, transform=ax.transAxes, s=\"Kurtosis: %.2f\" % data[feature].kurt(),\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\n        backgroundcolor='white')\n\n# add horizontal line\nline = lines.Line2D([0, 1], [1, 1], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\n# add figure text as title\nfig.text(0, 1.03, 'Distribution Plot For Age and Fare', fontdict=fig_title_prop)\n\nplt.tight_layout()\nplt.show()","6df7d82e":"# create correlation matrix \ncorr = df_eda.corr()\ncorr.style.background_gradient(cmap=cmap)","02ba58ad":"# create figure\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n\n# define mask to hide the upper triangle of the heatmap \nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# # define cmap for color mapping\n# cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# rotate x axis ticks to 90 deg and y axis to 0 degree, set font size to 11 px\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=11)\nax.set_yticklabels(ax.get_xticklabels(), rotation = 0, fontsize=11)\n\n# create heatmap plot\ng = sns.heatmap(corr, \n                mask=mask,\n                vmin=-1, vmax=1,\n                linewidth=2.5,\n                annot=True,\n                cmap=cmap,\n                cbar=False,\n                ax=ax)\n\n# add horizontal line\nline = lines.Line2D([0, 1], [1, 1], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\n# add figure text as title\nfig.text(0, 1.03, 'Correlation Heatmap', fontdict=fig_title_prop)\n\nplt.tight_layout()\nplt.show()","eb8b52f1":"survival_rate = pd.DataFrame(df_eda.groupby('Sex').mean()['Survived'])\nmale_rate = survival_rate.loc['male']\nfemale_rate = survival_rate.loc['female']","18a05074":"\"\"\"\nnp.random.uniform() generates random samples from uniform distribution and returns the random samples as numpy array.\n\nParameters for this method are low=0.0, high=1.0, size=None\n\"\"\"\n\nmale_pos = np.random.uniform(0, male_rate, len(df_eda[(df_eda['Sex'] == 'male') & (df_eda['Survived'] == 1)]))\nmale_neg = np.random.uniform(male_rate, 1, len(df_eda[(df_eda['Sex'] == 'male') & (df_eda['Survived'] == 0)]))\nfemale_pos = np.random.uniform(0, female_rate, len(df_eda[(df_eda['Sex'] == 'male') & (df_eda['Survived'] == 1)]))\nfemale_neg = np.random.uniform(female_rate, 1, len(df_eda[(df_eda['Sex'] == 'male') & (df_eda['Survived'] == 0)]))","58ed11e0":"fig, ax = plt.subplots(1, 1, figsize=(5, 6))\n\nnp.random.seed(42)\n\n# Male Stripplot\nax.scatter(np.random.uniform(-0.3, 0.3, len(male_pos)), male_pos, color=customPalette[0], edgecolor='#d4dddd', label='Male(Survived=1)')\nax.scatter(np.random.uniform(-0.3, 0.3, len(male_neg)), male_neg, color=customPalette[0], edgecolor='#d4dddd', alpha=0.1, label='Male(Survived=0)')\n\n# Female Stripplot\nax.scatter(1+np.random.uniform(-0.3, 0.3, len(female_pos)), female_pos, color=customPalette[1], edgecolor='#d4dddd', label='Male(Survived=1)')\nax.scatter(1+np.random.uniform(-0.3, 0.3, len(female_neg)), female_neg, color=customPalette[1], edgecolor='#d4dddd', alpha=0.1, label='Male(Survived=0)')\n\n# Set Figure & Axes\nax.set_xlim(-0.5, 1.5)\nax.set_ylim(-0.03, 1.1)\n\n# Ticks\nax.set_xticks([0, 1])\nax.set_xticklabels(['Male', 'Female'], fontweight='bold', fontsize=12)\nax.set_yticks([], minor=False)\nax.set_ylabel('')\n\n# remove figure borders\nfor s in [\"top\", \"right\",\"left\", 'bottom']:\n    ax.spines[s].set_visible(False)\n\n# add horizontal line\nline = lines.Line2D([0, 1], [1.02, 1.02], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\n# add figure text as title\nfig.text(0, 1.05, 'Distribution of survivors by gender', fontdict=fig_title_prop)\nfig.text(0, 0.94, \n         'Only {:.2f}% people survived out of which\\n{:.2f}% were Male and {:.2f}% were Female.'.format(\n             survive_per[1]*100, \n             male_rate[0]*100, \n             female_rate[0]*100),\n         ha='left'\n        )\n\nax.legend(loc=(1, 0.4), edgecolor='None')\nplt.grid()\nplt.tight_layout()\nplt.show()","99f134d2":"fig, ax = plt.subplots(1, 1, figsize=(8, 6), sharey=True)\n\nsns.stripplot(x=\"Sex\", y=\"Age\", hue='Survived', data=df_eda, size=4, palette='YlOrBr', alpha=0.8)\n\nsns.violinplot(x='Sex', y='Age', data=df_eda, hue='Survived', split=True, palette=customPalette)\n\n\n# remove borders\nfor s in ['top', 'right', 'left', 'bottom']:\n    ax.spines[s].set_visible(False)\n\n# add horizontal line\nline = lines.Line2D([0, 1], [1.02, 1.02], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\n# add figure text as title\nfig.text(0, 1.05, 'Age vs Sex Vs Survived', fontdict=fig_title_prop)  \n\n# add span\nax.axhspan(xmin=0, xmax=1, \n           ymin=df_eda['Age'].mean() - 10, ymax=df_eda['Age'].mean() + 10 ,\n           facecolor=\"r\",\n           alpha=0.1, \n           zorder=1000)\n\nplt.legend(title='Survived', edgecolor='None', bbox_to_anchor=(1, 1))\nplt.tight_layout()\nplt.show()","541f8880":"# define funcction for converting age into age band\ndef age_band(num):\n    for i in range(1, 100):\n        if num < 10*i :  return f'{(i-1) * 10} ~ {i*10}'","13602c2a":"# add column in df_eda dataframe for age_band\ndf_eda['age_band'] = df_eda['Age'].apply(age_band)","431ce39a":"titanic_age = df_eda[['age_band', 'Survived']].groupby('age_band')['Survived'].value_counts().sort_index().unstack()\ntitanic_age.fillna(0, inplace=True)\ntitanic_age['Survival rate'] = round(titanic_age[1] \/ (titanic_age[0] + titanic_age[1]) * 100, 2)","99470b65":"fig, ax = plt.subplots(1, 1, figsize=(8, 5))\nfig.subplots_adjust(top=0.8)\n\ncolor_map = ['#aed9e0' for _ in range(9)]\ncolor_map[0] = color_map[8] = '#ffa69e' # color highlight\n\nax.bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], \n       color=color_map, width=0.55, \n#        edgecolor='black', \n       linewidth=0.7)\nax.set_ylim(0, 110)\n\nfor s in [\"top\", \"right\",\"left\"]:\n    ax.spines[s].set_visible(False)\n\n# Annotation Part\nfor i in titanic_age['Survival rate'].index:\n    ax.annotate(f\"{titanic_age['Survival rate'][i]:.02f}%\", \n                   xy=(i, titanic_age['Survival rate'][i] + 2.3),\n                   va = 'center', ha='center',fontweight='light', \n                   color='#4a4a4a')\n\n# mean line + annotation\nmean = df_eda['Survived'].mean() *100\nax.axhline(mean ,color='black', linewidth=0.4, linestyle='-')\nax.annotate(f\"mean : {mean :.4}%\", \n            xy=('70 ~ 80', mean + 5),\n            va = 'center', ha='center',\n            color='#4a4a4a',\n            bbox=dict(boxstyle='round', pad=0.4, facecolor='#d4dddd', linewidth=0, alpha=0.4))\n\n# add horizontal line\nline = lines.Line2D([0, 1], [1, 1], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\n# add figure text as title\nfig.text(0, 1.03, 'Age Band & Survival Rate', fontdict=fig_title_prop)\n\ngrid_y_ticks = np.arange(0, 101, 20)\nax.set_yticks(grid_y_ticks)\n# ax.grid(axis='y', linestyle='--', alpha=0.4)\n\nplt.tight_layout()\nplt.show()","818209f9":"# mean age for each class\npclass_age_mean = df_eda[['Age', 'Pclass']].groupby('Pclass').mean()\npclsas_survival_rate = df_eda[['Survived', 'Pclass']].groupby(['Pclass']).mean()\n\n# create figure\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", split=True, data=df_eda)\n    \nfor i in ['top', 'right', 'left', 'bottom']:\n    ax.spines[i].set_visible(False)\n\n# mean line + annotation\nfor i in range(len(pclass_age_mean)):\n    ax.annotate(\"Age(Mean): ~{:.0f}\".format(pclass_age_mean['Age'].iloc[i]), \n            xy=(i, 90),\n            va = 'center', ha='center',\n            color='#4a4a4a',\n            bbox=dict(boxstyle='round', pad=0.4, facecolor='#f8edeb', linewidth=0, alpha=0.4))\n    ax.annotate(\"Survival(%): {:.2f}%\".format(pclsas_survival_rate['Survived'].iloc[i]*100), \n            xy=(i, 80),\n            va = 'center', ha='center',\n            color='#4a4a4a',\n            bbox=dict(boxstyle='round', pad=0.4, facecolor='#d4dddd', linewidth=0, alpha=0.4))\n\nline = lines.Line2D([0,1], [1,1], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\nfig.text(0, 1.03, 'Age vs Pclass vs Survived', fontdict=fig_title_prop)\n\nax.legend(title='Survived', edgecolor='None', numpoints=1, markerscale=5, bbox_to_anchor=(1.25, 0.5)) \nax.grid(axis='y', linestyle='--', alpha=0.4)\n\nplt.tight_layout()\nplt.show()","5ffc8d98":"pclass_ulist = list(range(df_eda['Pclass'].nunique()))\nsex_ulist = list(df_eda['Sex'].unique())\n\nclass_gen = {'Pclass':[], 'Sex':[], 'Count':[], 'Count_per': [], 'Mean_Fare':[]}\n\nfor i in pclass_ulist:\n    for j in sex_ulist:\n        count = df_eda[(df_eda['Pclass'] == i+1) & (df_eda['Sex'] == j)]['Name'].count()\n        mean_fare = df_eda[(df_eda['Pclass'] == i+1) & (df_eda['Sex'] == j)]['Fare'].mean()\n        class_gen['Pclass'].append(i+1)\n        class_gen['Sex'].append(j)\n        class_gen['Count'].append(count)\n        class_gen['Count_per'].append(round(count \/ len(df_eda) * 100, 2))\n        class_gen['Mean_Fare'].append(round(mean_fare, 2))\n        \npd.DataFrame.from_dict(class_gen).style.background_gradient(cmap='PuBu')","6174705a":"fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\nsns.violinplot(y='Fare', x='Pclass', hue='Sex', data=df_eda, split=True)\n\nfor s in ['top', 'right', 'left', 'bottom']:\n    ax.spines[s].set_visible(False)\n\nline = lines.Line2D([0,1], [1,1], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\nfig.text(0, 1.03, 'Fare vs Pclass vs Sex', fontdict=fig_title_prop)\n\nax.grid(axis='y', linestyle='--', alpha=0.4)\nplt.legend(edgecolor='None')\nplt.tight_layout()","d4d3735c":"data = df_eda[['SibSp', 'Survived', 'Sex', 'PassengerId']].groupby(by=['Sex', 'Survived', 'SibSp']).count().reset_index()\ndata.rename(columns={'PassengerId':'Count'}, inplace=True)\nmale_sibsp = df_eda[df_eda['Sex']=='male'][['SibSp', 'Sex', 'Survived']]\nfemale_sibsp = df_eda[df_eda['Sex']=='female'][['SibSp', 'Sex', 'Survived']]","d0950aa2":"sibsp_sex = [male_sibsp, female_sibsp]","da724956":"fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=False, sharey=False)\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor ax, data in zip(ax, sibsp_sex):\n    sns.countplot(data=data, x='SibSp', hue='Survived', ax=ax)\n    ax.yaxis.label.set_visible(False)\n    ax.set(title='{}'.format(data['Sex'].value_counts().index[0]))\n    ax.get_legend().remove()\n    for s in ['top', 'right', 'left']:\n        ax.spines[s].set_visible(False)\n\nline = lines.Line2D([0,1], [1,1], transform=fig.transFigure, zorder=1000, figure=fig)\nfig.lines.extend([line])\n\nfig.text(0, 1.03, 'SibSp vs Sex vs Survived', fontdict=fig_title_prop)\n\nfig.legend(labels=['Not Survived', 'Survived'], edgecolor='None', bbox_to_anchor=(1.2, 0.5))\nplt.tight_layout()\nplt.show()","8e5dbff3":"train_copy = copy.deepcopy(df_train)\npredict_copy = copy.deepcopy(df_predict)","448fdbdd":"df_list = [train_copy, predict_copy]","2bb5b7fa":"msno.matrix(train_copy, color=(0.94, 0.54, 0.36), figsize=(10,5))\nplt.show()","961e4fc9":"for dataset in df_list:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","6d2194fa":"passengerid = predict_copy['PassengerId']","eb91e4b1":"drop_column = ['PassengerId','Cabin', 'Ticket']\npredict_copy.drop(drop_column, axis=1, inplace = True)","50ae5bd1":"print(f'Train Dataset has %.f columns: ' % len(train_copy.columns), list(df_train.columns))\nprint('-------'*20)\nprint(f'predict Dataset has %.f columns: ' % len(predict_copy.columns), list(df_predict.columns))","b8119b70":"print(\n    'Train dataset has {} missing values\\npredict dataset has {} missing values'.format(\n        np.sum(train_copy.isnull().sum()), \n        np.sum(predict_copy.isnull().sum())) \n)","b3a7d0ee":"for dataset in df_list:\n    #Discrete variables\n    dataset['familySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    \n    # retrun 1\/yes if familySize < 1 and 0\/no if familySize > 1\n    dataset['isAlone'] = dataset.apply(lambda x: 0 if (x['familySize'] > 1) else 1, axis=1)\n\n    # we will use regex to extract title from `Name` column\n    dataset['nameTitle'] = dataset['Name'].str.extract('([a-zA-Z]+\\.)', expand=True)[0].str.replace('.', '')\n    \n    # create fare and bin using qcut\n    dataset['fareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['ageBin'] = pd.cut(dataset['Age'].astype(int), 5)","c26d7db2":"stat_min = 10\n\nfor dataset in df_list:\n    title_names = (dataset['nameTitle'].value_counts() < stat_min) #this will create a true false series with title name as index\n    dataset['nameTitle'] = dataset['nameTitle'].apply(lambda x: 'Others' if title_names.loc[x] == True else x)\n    print(dataset['nameTitle'].value_counts())","a4f770b6":"print(f'Train Dataset has %.f columns: ' % len(train_copy.columns), list(train_copy.columns))\nprint('-------'*20)\nprint(f'Prediction Dataset has %.f columns: ' % len(predict_copy.columns), list(predict_copy.columns))","f0cba396":"# initialize LabelEncoder from Preprocessing module of Scikit-Learn\nlabel = LabelEncoder()\n\n# apply label encoding to `Sex`, `Embarked`, `nameTitle`, `ageBin`, `fareBin`\nfor dataset in df_list:    \n    dataset['sexCode'] = label.fit_transform(dataset['Sex'])\n    dataset['embarkedCode'] = label.fit_transform(dataset['Embarked'])\n    dataset['nameTitleCode'] = label.fit_transform(dataset['nameTitle'])\n    dataset['ageBinCode'] = label.fit_transform(dataset['ageBin'])\n    dataset['fareBinCode'] = label.fit_transform(dataset['fareBin'])","8c710531":"print(f'Train Dataset has %.f columns: ' % len(train_copy.columns), list(train_copy.columns))\nprint('-------'*20)\nprint(f'Prediction Dataset has %.f columns: ' % len(predict_copy.columns), list(predict_copy.columns))","cca41479":"# define y variable aka target\nTarget = ['Survived']","9add3bc9":"dummy_cols = ['Sex', 'nameTitle', 'Embarked']\ntrain_dummy = pd.get_dummies(train_copy[dummy_cols])\ntrain_dummy_cols = train_dummy.columns.to_list()","8cd40acf":"predict_dummy = pd.get_dummies(predict_copy[dummy_cols])\npredict_dummy_cols = train_dummy.columns.to_list()","95ec8968":"train_copy = pd.concat([train_copy, train_dummy], axis=1)","af91b4bf":"predict_copy = pd.concat([predict_copy, predict_dummy], axis=1)","8cd256a6":"# Target \ny = train_copy[Target]","773e8c90":"## Original selected feature + Label encoding features\n# column for label encoding data\ntrain1_cols = ['Pclass', 'SibSp', 'Parch', 'sexCode', 'embarkedCode', 'nameTitleCode', 'ageBinCode', 'fareBinCode']\n# dataframe for label encoding data\nX1 = train_copy[train1_cols]\n\n# columns for one-hot encoding data\ntrain2_cols = ['Pclass', 'SibSp', 'Parch', 'ageBinCode', 'fareBinCode'] + train_dummy_cols\n# dataframe for one-hot encoding data\nX2 = train_copy[train2_cols]","3c116c52":"predict_label = predict_copy[train1_cols]","19853d26":"predict_1hot = predict_copy[train2_cols]","bcea79d9":"# random_state -> seed or control random number generator\nX1_train, X1_test, y1_train, y1_test = model_selection.train_test_split(X1, y, test_size=0.2, random_state=42)\nX2_train, X2_test, y2_train, y2_test = model_selection.train_test_split(X2, y, test_size=0.2, random_state=42)","7ddebd36":"print('Train Dataset has {:.0f} examples, {:.2f}% of original.'.format(len(X1_train), round(len(X1_train) \/ len(X1) *100, 2)))\nprint('Test Datasets has {:.0f} examples, {:.2f}% of original.'.format(len(X1_test), round(len(X1_test) \/ len(X1) *100, 2)))","4d219deb":"MLA = [\n    #Ensemble Methods\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    \n    #Navies Bayes\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    \n    #XGBoost\n    XGBClassifier()    \n    ]","1665fa91":"#create table to compare MLA metrics for label encoding data\nMLA_cols_label = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare_label = pd.DataFrame(columns = MLA_cols_label)","10c5f185":"#create table to compare MLA metrics for one-hot encoding data\nMLA_cols_1hot = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare_1hot = pd.DataFrame(columns = MLA_cols_1hot)","dfaa65c4":"# index through MLA and save performance to table\nrow_index = 0\n\nfor alg in MLA:\n    # stratified K Fold Split\n    skf = StratifiedKFold(n_splits = 10, random_state=10)\n        \n    # cross validation\n    cv_results = model_selection.cross_validate(alg, X1_train, y1_train, return_train_score=True, cv=skf)\n    \n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare_label.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare_label.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    MLA_compare_label.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare_label.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare_label.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()  \n    MLA_compare_label.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    \n    row_index+=1","47a2b965":"MLA_compare_label.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare_label","256f29fc":"# index through MLA and save performance to table\nrow_index = 0\n\nfor alg in MLA:\n    # stratified K Fold Split\n    skf = StratifiedKFold(n_splits = 10, random_state=10)\n        \n    # cross validation\n    cv_results = model_selection.cross_validate(alg, X2_train, y2_train, return_train_score=True, cv=skf)\n    \n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare_1hot.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare_1hot.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    MLA_compare_1hot.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare_1hot.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare_1hot.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()  \n    MLA_compare_1hot.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    \n    row_index+=1","2443c419":"MLA_compare_1hot.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare_1hot","413bba75":"# # train the model on train set \n# model = svm.SVC()\n# model_1hot = model.fit(X2_train, y2_train) \n  \n# # print prediction results \n# predictions_1hot = model.predict(X2_test) \n# print(classification_report(y2_test, predictions_1hot)) ","be6493ea":"# defining parameters and values \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  ","2acb23d5":"grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = False) ","65e66a3d":"# fitting the model for grid search \ngrid.fit(X2_train, y2_train)","d1bce6ac":"print(grid.best_params_) ","7bd019f9":"print(grid.best_estimator_) ","b192ef16":"model_tuned = svm.SVC(C=100, gamma=0.01, kernel='rbf')\nmodel_tuned.fit(X2_train, y2_train)\npredict_tuned = model_tuned.predict(X2_test)\nprint(classification_report(y2_test, predict_tuned))","24d07e0f":"cm = confusion_matrix(y2_test, predict_tuned)\ncm","34b8fca8":"# Plot confusion matrix\n\ndef plot_cm(y_true, y_pred, figsize=(6,6)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(cm, cmap= cmap, annot=annot, fmt='', ax=ax)\n    \nplot_cm(y2_test, predict_tuned)","88ca294c":"test_prediction = model_tuned.predict(predict_1hot)\nsubmission = pd.DataFrame({\n    'PassengerId' : passengerid,\n    'Survived': test_prediction\n})\n\nsubmission['PassengerId'] = submission['PassengerId'].astype(int)\nsubmission['Survived'] = submission['Survived'].astype(int)","401cef2e":"# submission.to_csv(\"titanic_submission_20201129.csv\", index=False)","8b86b5a0":"<div class=\"alert alert-block alert-info\">\n    <h1><center><strong>\u2049\ufe0f Problem Introduction<\/strong><\/center><\/h1>\n    <p>The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n        \nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n    <\/p>\n    <hr>\n    <p>\nWe have been asked to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc)\n    <\/p>\n<\/div>","af6e3b71":"Calculate surviaval rate for each age groups. Since our `age_band` feature  and aggregated data has some NaN values, thus we will use fillna() and replace NaN with zeros.","e68deff0":"We will test different models and check for their accurary with both of our datasets.","1400674c":"#### 1.1 Peek into imported datasets","ae043847":"<div class=\"alert alert-warning\">  \n<h3><strong>2. Exploratory Data Analysis<\/strong><\/h3>\n<\/div>","8918df18":"#### SPLIT : Split the `train` dataset","f9487545":"Univariate analysis is the technique of comparing and analyzing the dependency of a single predictor and a response variable. The prefix \"uni\" means one, emphasizing the fact that the analysis only accounts for one variable's effect on a dependent variable. Univariate Analysis is thought to be one of the simplest forms of data analysis as it doesn't deal with causes or relationships, like a regression would. Primarily, Univariate Analysis simply takes data and provides a summary and associated patterns. ","9e277160":"We will create a table and populate it with the accuracy data of algorithms so that we can compare thier porformance. We will create separate tables for label encoding and one-hot encoding. ","d8ec85ee":"From the above chart it is clear that:\n1. Class 3 has most of Class 3 has the lowest mean age (~ 25 years) and lowest survival rate (~ 24%) of all three class\n2. Class 1 has highest mean age (~ 38 years) and survival rate (~ 63%) of all three classes ","82064857":"- Most of the data is numerical, 7 columns are numerical type and 5 are object \/ str type. \n- Some data is missing for columns `Age` and `Cabin`","9e12e828":"#### 2.2 Parameters Configuration","a0fd7d4f":"\ud83d\udca1 Tip: In case we want to create multiple `displot` we won't be able to do so with same for loop method as above, why?\n- [Difference between displot vs histplot](https:\/\/stackoverflow.com\/a\/63895570\/8390360)\n- [How to create multiple displot in same figure](https:\/\/stackoverflow.com\/a\/63974061\/8390360)","c5160e3e":"Form these 2 plots we can gather that :\n- For `Age` feature data is fairly normally distributed while `Fare` has high skewness which could be due to outlers. Next we will take a look at skewness and kurtosis for both `Age` and `Fare'\n- From boxplot it is clear `Fare` feature has some high value outliers, we will check what are these outliers and what should we do with them.\n- Also it is clear that most of `Fare` data is concentrated towards less values which indicates and also confirms that majority of people onboard purchased cheap tickets and thus there were more passengers with 3rd Class tickets, we can confirm this by checking correlation between `Fare` and `Pclass` features.","d43a1a3c":"#### CREATE: Feature Engineering for train and predict dataset","5a1eaa26":"Multivariate data analysis is a set of statistical models that examine patterns in multidimensional data by considering, at once, several data variables. It is an expansion of bivariate data analysis, which considers only two variables in its models.","ebaddcc3":"Let's graphically visualize the missing values in the train dataset","49ce89c2":"First lets define parameters grid which includes parameters and values we want to use to tune our model.","83a4aa42":"<br>","6e6a0df3":"**Font size and font family**","da4fc6a4":"#### 1.2 Definition of dataset","4354b7fc":"##### Model comparison for Label encoding data","744c9d28":"First we will create a function to convert passenger age into a range.","b2a152e6":"There is not much change in metrics even after tuning. Recall score for 'Survived' people is only 0.74 which means out of all survived classes in the truth sample our model is only able to classify 74% of them as survived while rest 26% as 'not survived'","16dda0a5":"Based on skewness and kurtosis values for `Age` column we can say that data is faily normally distributed while for `Fare` column we can say data is highly positvely skwed and also has presensce of outliers as kurtosis is very high.\n\nWe will check the characterstics of outlier's values in `Fare` feature based on which we will decide action in feature engineering section.","5e5f774a":"Using `get_dummy` method from Pandas to create one-hot encoding for categorical features 'Sex', 'nameTitle', 'Embarked'","88785b4d":"<div class=\"alert alert-warning\">  \n<h3><strong>Import libraries<\/strong><\/h3>\n<\/div>","cb011f18":"#### SELECTION: Select `Features` and `Target` variables for train and predict dataset","e1d1bd3e":"#### Cleaning The Data","55f9d160":"Survival rate vs Fare","ce655089":"Before delving into analysis let's make sure that our data is in tidy format. \n\nConditions to check if **data is tidy**:\n- Is every column a variable? \u2705\n- Is every row an observation? \u2705\n- Is every table a single observational unit? \u2705\n\nNow we can start exploring our dataset.","7dec90df":"Survival rate based on age of the passengers","0e404dd5":"* There is no passenger above 80\n* There are 3 passengers whose fare is greater than 300","9c80b350":"**Matplotlib Parameters**","30ed45d2":"From the analysis we can infer that:\n1. There is 100% survival rate for age band 80-90, but there was only 1 passenger in this range. We will try to eliminate this row as it can affect our model. Also does ticket class has any impact on his survival?\n2. There is no one in age range 70-80 who survived. Find out were they all in 3rd Class?\n3. Passengers with age range between 0-10 years has best survival rate, it also makes sense as protection of kids and women must have been priority.\n4. Old people had less chance of survival while for young passenger age range 20-30 is exception as their survival rate is lower than mean. Why?","a73c30d9":"- `Cabin` has missing data for around 77% of the rows, this could cause bais in the model so we will drop this column later.\n- `Age` column has 20% missing data, we will replace missing values with `mean` or 'median` depending upon the distribution of data\n- `Embarked` has missing data for 2 rows, we will drop these 2 rows","2aedadb9":"Let's look deeper into survival rate and its relationship with other feature.\n1. Find survival rate based on `sex`\n2. Find survival rate based on `age band` for this we will have to populate our dataframe with age ranges (bin size = 10) based on the age of the passengers.\n3. Check survival rate based on `class`.","87521c8c":"Resources:\n- https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n- https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- https:\/\/www.kaggle.com\/ruchi798\/break-the-ice\n- https:\/\/www.kaggle.com\/agungor2\/various-confusion-matrix-plots\n- https:\/\/trello.com\/c\/AUUdm4tq\/35-kaggle-awesome-visualization-with-titanic-dataset%F0%9F%93%8A-subinum","d0bd73bc":"Now we will populate out df_eda dataset with age ranges based on passenger age. ","183d1e13":"- `gender_submission` dataset only has 2 columns `PassengerId` and `Survived`. PassengerId kind of act as `key` which is connecting all three datasets.\n- `train` dataset has 12 columnns which contains details about passengers with observed value of 'Survived' label.\n- `predict` dataset all 11 columns as in `train` dataset except `survived` column, which also makes sense as we are required to predict `survived` label for given observations in the predict dataset.","6cfc9cec":"##### Model comparison for One-Hot encoding data","7cd8df70":"**Confusion Matrix** : It is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.","fa361c6c":"Now let's loop through our selected algorithms and record their accuracy in respective tables. We will use `Stratified K Fold` method for cross validation.\n\n- [What is Cross Validation?](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics))\n- [Different types of Cross Validations](https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/)","a8fbf8b4":"From the given `Problem Statement` and `train` dataset, we can identify our `Predictor` and `Target` variables:\n- Predictor Variables : Pclass, Gender, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked\n- Target Variable : Survived","71fd79b3":"\ud83d\udca1 Tip: `value_counts()` is a Series method as doesn't apply on DataFrame. If we have to apply it on DataFrame then we have to first flatten() the DataFrame and then use `pd.value_counts(flattened DataFrae)` method. ","38b5bcde":"Replace missing values in:\n- In `Age` and `Fare` column with median as data present is numerical and positively skewed\n- In `Embarked` column with mode as data present is categorical","b5e331da":"Let's check the values count of both `Age` greater than 100 and `Fare` greater than 300.","eb43190e":"Concatenate dummy data with train data (this will be unified source of processed data from which data can be extracted basis requirement)","a0261755":"<div class=\"alert alert-warning\">  \n<h3><strong>Model Selection<\/strong><\/h3>\n<\/div>","31e44af4":"#### 2.2 Dataset Exploration","90a0b0a1":"Create\/add new columns for `familySize`, `isAlone`, `nameTitle`, `fareBin`, `ageBin`","c0d1cc6f":"#### COMPLETING : Complete missing values in train and predict dataset","8daa8658":"Machine Learning Algorithm (MLA) Selection and Initialization","76874725":"But lets first evaluate our model without hyperparameter tuning.","278ce649":"> Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to xxx hypothesis and to check assumptions with the help of summary statistics and graphical representations.","4d2118ad":"Survival rate based on Sex","22b6c5a4":"##### Label Encoding","315b7be9":"We will create a list and pass both train and predict datasets, so that we can use for loop to apply feature engineering operations on both datasets at once.","563fe6c9":"##### One-Hot Encoding","a4169396":"Drop unhelpful columns from `predict` dataset: \n- Drop `Cabin` column as most of data in this column is missing\n- Drop `PassengerId` and `Ticket` columns as they does not contribute in analysis\n\nBut before dropping `PassengerId` from `test` column, we will save it as a list because we will need it to for submission.","c5ffb80f":"![image.png](attachment:b5b1169b-6e52-4819-981a-43164e269dcd.png)","fe1978a9":"<div class=\"alert alert-warning\">  \n<h3><strong>Model Tuning & Evaluation<\/strong><\/h3>\n<\/div>","25eacbad":"We will use `df_predict` dataset for submission so we cannot show it to the model therefore we will split our `train` dataset into 80\/20 ratio of `train` and `test` respectively.","71011e71":"From Correlation heatmap it is evident that: \n- `Fare` and `Survived` are positively correlated.\n- There is positive correlation between `Parch` and `SibSp`\n- There is negative correlation between `Fare` and `Pclass`, this again confirms that most of people had bought low fare tickets and thus more passengers had 3rd class tickets.\n- There is also negative correlation between `Pclass` and `Survive`, from this we can infer that lower that class (1 > 2 > 3) low were the chances of survival but we need to validate this.","fae77439":"**Size of the dataset**","4a63a82f":"#### Univariate Analysis","0ffa1a79":"There are various methods for converting categorical values in a column to numerical, some of important ones are mentioned below:\n\n- **Label Encoding:** It is simply converting each value in a column to a number. Label encoding has the advantage that it is straightforward but it has the disadvantage that the numeric values can be \u201cmisinterpreted\u201d by the algorithms.\n\n- **One-Hot Encoding:** In this strategy is to convert each category value into a new column and assigns a 1 or 0 (True\/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.\n\n- **Custom Binary Encoding:** Depending on the data set, you may be able to use some combination of label encoding and one hot encoding to create a binary column that meets your needs for further analysis.","69fdffbf":"Before starting out data exploration and analysis part, first we can define important configurations for Pandas, Seaborn, or Matplotlib we will be using.","2229d070":"Both `label` and `one-hot` encoding data suggests that SVC prrovides best accuracy score and almost identical for both scenarios. So we will use SVC and try to optimize with Hyperparameter tuning.","27073857":"Out of 891 passengers, only 38% survived, while majority 61% the passenger did not survive the disaster.","d4389b9f":"We have higher rate of False Negative.","7f411e5a":">The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.\n\u2014 Luca Massaron","27a2bc14":"<div class=\"alert alert-warning\">  \n<h3><strong>Submission<\/strong><\/h3>\n<\/div>","1f9c9b3e":"<div class=\"alert alert-warning\">  \n<h3><strong>Feature Engineering<\/strong><\/h3>\n<\/div>","6857631a":"- Given the dataset, we are able to predict whether or not a passenger will survive the titanic disaster with accuracy of `83.56%`.\n- We attempted to test and select best model highest accuracy for both `label` and `one-hot` encoding dataset.\n- In both cases, Support Vector Machine outperformed rest of the models in terms of accuracy. LDA and GPC are closer to SVC for `one-hot` encoded dataset.\n- Even after using `GridSearchCV` to tune SVC model, we were not able to improve the accuracy of our model.\n- From confusion matrix it is clear that we have higher rate of `false negatives`.\n\n[Class Imbalance Learning Method For SVM](http:\/\/www.cs.ox.ac.uk\/people\/vasile.palade\/papers\/Class-Imbalance-SVM.pdf)","768330de":"### Multivariate Analysis","fa4124a2":"#### Descriptive Statitics Summary","f1233809":"Convert title values in `nameTitle` column to `Others` if any title has less than 10 instances.","316fb8f4":"<div class=\"alert alert-warning\">  \n<h3><strong>1. Collect & Load Data<\/strong><\/h3>\n<\/div>","37d1b68d":"Lets expolore `train` dataset:","f8d0aa4b":"Prepare different versions of dataframe to test model - \n1. With label encoding data\n2. With one-hot encoding data","76c7696e":"- For male between the age range of 20-30 years, chances of survial are low\n- For female data is faily distributed throughout the y axis","5010fddd":"Feature engineering efforts mainly have two goals:\n1. Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n2. Improving the performance of machine learning models.","50ba0200":"#### CONVERT: Convert categorical values into numerical using Label Encoder for train and predict dataset","b3c32ec3":"- Survival rate is 38%\n- Mean age of passengers is 30 years, around 75% passengers are below38 years. Some data is missing for Age column.\n- Maximum fare is 512, whie 75% passengers's fare is around 31. Most of passengers must be travelling with lower class tickets. Also this suggests that there might be some outliers.","9ca46309":"**Color palette**","fca6c2e8":"we have 12 columns (11 features + 1 target) and 890 observations.","ffba9da8":"Groupby `Sex` feature and find percentage of `Survived`. Since Survived data is in 0 or 1 therefore aggregation as `mean()` is sufficient to find percentage.","02c82c70":"<div class=\"alert alert-warning\">  \n<h3><strong>Conclusion<\/strong><\/h3>\n<\/div>","76c59f85":"What does positive and negative correlation means?\n- **Positive Correlation:** This occurs when the value of one variable increases then the value of the other variable(s) also increases.\n- **Negative Correlation:** This occurs when the value of one variable increases and the value of other variable(s) decreases.","4f0c3f74":"Conclusions so far:\n- Only 38% passengers survived, Survival rate for female is way higher than male << Survival Rate {Male = 19%, Female = 72%} >>\n- Fare of 3rd Class was very cheap << Mean fares {1st Class = 13,  2nd Class = 20,  3rd Class = 84} >>\n- Majority of passengers were travelling through 3rd Class << Population Percentage {1st Class = 24%, 2nd Class = 20%, 3rd Class = 55%} >>\n- Majority of passengers travelling thorugh 3rd class were male (~38%)\n- Majority of passengers travelling were young (between 20 to 40 years = ~33%), 14% were travelling through 3rd Class"}}