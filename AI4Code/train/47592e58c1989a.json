{"cell_type":{"3b560e77":"code","29131ab3":"code","4f24f788":"code","32a92aac":"code","06328161":"code","6ff4be2d":"code","ddba20c1":"code","b9219443":"code","1f940bc7":"code","1449633f":"code","c9d8402b":"code","44de3e2d":"code","4ce2af8e":"code","839fcd3e":"code","f3649b97":"code","0fc4281a":"code","6ec6eb32":"code","8274d65b":"code","3b7ff636":"code","c33e64e0":"code","dcc4c480":"code","41366ce6":"code","c1e540c1":"code","1cfb2cac":"code","d1d1b790":"code","e99f84fd":"code","4827ffec":"code","22fcb9c6":"code","52c2e1f6":"code","f9a6d51e":"code","150a9075":"code","c0a974b5":"code","1278a216":"code","041ac7c2":"code","4ae37765":"code","d58dc0a7":"code","9b89086f":"code","0d0cb55a":"code","246b2e30":"code","98f8ba87":"code","0fbb85b5":"code","2109fb7d":"code","2c6d1e4c":"code","293a2da8":"code","c3c1a4bb":"code","f50f31e5":"code","e6a13438":"code","694e52d3":"code","fb271d40":"code","95a3cea3":"code","e00a19b8":"code","bc8d6e00":"code","807e87ae":"markdown","3a314ea6":"markdown","daba75e4":"markdown","e57d8b2b":"markdown","c24348b1":"markdown","777bf8d0":"markdown","1d43706d":"markdown","d0175fc6":"markdown","cdda5656":"markdown","c66aee52":"markdown"},"source":{"3b560e77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29131ab3":"filepath = '\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv'\ndata = pd.read_csv(filepath)\ndata.head()","4f24f788":"data = data[['Text', 'Score']]\ndata.head()","32a92aac":"data.shape","06328161":"import re\nfrom nltk.corpus import stopwords\n\ndef decontract(sentence):\n    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    return sentence\n\ndef cleanPunc(sentence): \n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\n\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent\n\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\"\", sentence)","6ff4be2d":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\n\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stopwords) + \")\\\\W\", re.I)","ddba20c1":"data['Text'] = data['Text'].str.lower()\ndata['Text'] = data['Text'].apply(decontract)\ndata['Text'] = data['Text'].apply(cleanPunc)\ndata['Text'] = data['Text'].apply(keepAlpha)\ndata['Text'] = data['Text'].apply(removeStopWords)","b9219443":"#removes characters repeated \ndata['Text'] = data['Text'].apply(lambda x: re.sub(r'(\\w)(\\1{2,})', r'\\1',x)) \ndata['token_size'] = data['Text'].apply(lambda x: len(x.split(' ')))","1f940bc7":"data.describe()","1449633f":"# Avoid too much padding by filtering too long texts\ndata = data.loc[data['token_size'] < 60]","c9d8402b":"data.head()","44de3e2d":"data = data.sample(n= 50000)","4ce2af8e":"# Construct a vocabulary\nclass ConstructVocab():\n    \n    def __init__(self, sentences):\n        self.sentences = sentences\n        self.word2idx = {}\n        self.idx2word = {}\n        self.vocab = set()\n        self.create_index()\n        \n    def create_index(self):\n        for sent in self.sentences:\n            self.vocab.update(sent.split(' '))\n        \n        #sort vacabulary\n        self.vocab = sorted(self.vocab)\n        \n        #add a padding token with index 0\n        self.word2idx['<pad>'] = 0\n        \n        #word to index mapping\n        for index, word in enumerate(self.vocab):\n            self.word2idx[word] = index + 1 # 0 is the pad\n            \n        #index to word mapping\n        for word, index in self.word2idx.items():\n            self.idx2word[index] = word","839fcd3e":"inputs = ConstructVocab(data['Text'].values.tolist())","f3649b97":"inputs.vocab[:10]","0fc4281a":"input_tensor = [[inputs.word2idx[s] for s in es.split(' ')] for es in data['Text']]","6ec6eb32":"input_tensor[:2]","8274d65b":"def max_length(tensor):\n    return max(len(t) for t in tensor)","3b7ff636":"max_length_input = max_length(input_tensor)\nmax_length_input","c33e64e0":"def pad_sequences(x, max_len):\n    padded = np.zeros((max_len), dtype=np.int64)\n    \n    if len(x) > max_len: padded[:] = x[:max_len]\n    else: padded[:len(x)] = x\n        \n    return padded","dcc4c480":"input_tensor = [pad_sequences(x, max_length_input) for x in input_tensor]","41366ce6":"import time\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport itertools\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\n\nrates = list(set(data.Score.unique()))\nnum_rates = len(rates)\n\nmlb = preprocessing.MultiLabelBinarizer()\ndata_labels = [set(rat) & set(rates) for rat in data[['Score']].values]\nbin_rates = mlb.fit_transform(data_labels)\ntarget_tensor = np.array(bin_rates.tolist())","c1e540c1":"target_tensor[:2]","1cfb2cac":"data[:2]","d1d1b790":"get_rating =  lambda x: np.argmax(x)+1","e99f84fd":"get_rating(target_tensor[0])","4827ffec":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(input_tensor, target_tensor, \n                                                    test_size=0.2, random_state=1000)\nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, \n                                                    test_size=0.5, random_state=1000)\n","22fcb9c6":"TRAIN_BUFFER_SIZE = len(X_train)\nVAL_BUFFER_SIZE = len(X_val)\nTEST_BUFFER_SIZE = len(X_test)\nBATCH_SIZE = 64\n\nTRAIN_N_BATCH = TRAIN_BUFFER_SIZE \/\/ BATCH_SIZE\nVAL_N_BATCH = VAL_BUFFER_SIZE \/\/ BATCH_SIZE\nTEST_N_BATCH = TEST_BUFFER_SIZE \/\/ BATCH_SIZE","52c2e1f6":"from torch.utils.data import Dataset, DataLoader","f9a6d51e":"# Use Dataset class to represent the dataset object\nclass MyData(Dataset):\n    def __init__(self, X, y):\n        self.data = X\n        self.target = y\n        self.length = [np.sum(1 - np.equal(x,0)) for x in X]\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        x_len = self.length[index]\n        \n        return x, y, x_len\n    \n    def __len__(self):\n        return len(self.data)","150a9075":"import torch\nfrom torch.autograd import Variable\n\ntrain_dataset = MyData(X_train, y_train)\nval_dataset = MyData(X_val, y_val)\ntest_dataset = MyData(X_test, y_test)\n\ntrain_dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE,\n                          drop_last=True, shuffle=True)\nval_dataset = DataLoader(val_dataset, batch_size = BATCH_SIZE,\n                          drop_last=True, shuffle=True)\ntest_dataset = DataLoader(test_dataset, batch_size = BATCH_SIZE,\n                          drop_last=True, shuffle=True)","c0a974b5":"train_dataset.dataset.target[:2]","1278a216":"embedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inputs.word2idx)\ntarget_size = len(target_tensor[0])","041ac7c2":"import tensorflow as tf\nimport keras.backend.tensorflow_backend as tfback\n\nprint(\"tf.__version__ is\", tf.__version__)\nprint(\"tf.keras.__version__ is:\", tf.keras.__version__)\n\ndef _get_available_gpus():\n    \"\"\"Get a list of available gpu devices (formatted as strings).\n\n    # Returns\n        A list of available GPU devices.\n    \"\"\"\n    #global _LOCAL_DEVICES\n    if tfback._LOCAL_DEVICES is None:\n        devices = tf.config.list_logical_devices()\n        tfback._LOCAL_DEVICES = [x.name for x in devices]\n    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n\ntfback._get_available_gpus = _get_available_gpus","4ae37765":"from keras import backend as K\nK.tensorflow_backend._get_available_gpus()","d58dc0a7":"def create_model(X_train):\n\n    model = Sequential()\n    model.add(Embedding(vocab_inp_size, embedding_dim, input_length=max_length_input))\n    model.add(Dropout(0.5))\n    model.add(GRU(units))\n    model.add(layers.Dense(5, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n    return model","9b89086f":"from keras.layers import Dense, Embedding, Dropout, GRU\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.utils.vis_utils import plot_model\n\nmodel = create_model(pd.DataFrame(X_train))\n\nplot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')","0d0cb55a":"class timecallback(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.times = []\n        # use this value as reference to calculate cummulative time taken\n        self.timetaken = time.process_time()\n    def on_epoch_end(self,epoch,logs = {}):\n        self.times.append((epoch,time.process_time() - self.timetaken))\n    def on_train_end(self,logs = {}):\n        plt.xlabel('Epoch')\n        plt.ylabel('Total time taken until an epoch in seconds')\n        plt.plot(*zip(*self.times))\n        plt.show()","246b2e30":"import matplotlib.pyplot as plt\n\ntimetaken = timecallback()\nhistory = model.fit(pd.DataFrame(X_train), y_train,\n                    epochs=10,\n                    verbose=True,\n                    validation_data=(pd.DataFrame(X_val), y_val),\n                    batch_size=64,\n                    callbacks = [timetaken])","98f8ba87":"time_epoch = []\ntime_epoch.append(timetaken.times[0][1])\nfor i in range(len(timetaken.times)-1):\n    time_epoch.append(timetaken.times[i+1][1] - timetaken.times[i][1])\ntime_epoch","0fbb85b5":"metrics = model.evaluate(pd.DataFrame(X_test), y_test)\nprint(\"Accuracy: {}\".format(metrics[1]))","2109fb7d":"from sklearn import metrics\nimport seaborn as sns\n\npredictions = model.predict_classes(pd.DataFrame(X_test)) \nlabels = y_test.argmax(axis=1)\nconf_matrix = metrics.confusion_matrix(predictions, labels)\nconf_matrix = conf_matrix.astype(float)\n\nfor i in range(len(conf_matrix)):\n    conf_matrix[i] = (conf_matrix[i]*100)\/conf_matrix[i].sum()\n\ndf_cm = pd.DataFrame(conf_matrix, index = [i for i in \"12345\"],\n                     columns = [i for i in \"12345\"])\nplt.figure(figsize = (10,7))\ncmap = sns.diverging_palette(10, 240, n=9, as_cmap=True)\nsns.heatmap(df_cm, annot=True, cmap=cmap)\nplt.title(\"Confusion matrix of categories (%)\", fontsize=14)\nplt.show()","2c6d1e4c":"#pip install pytorch-nlp\n#pip install torchviz","293a2da8":"import torch.nn as nn\n\nclass RateGRU(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_units, batch_sz, output_size):\n        super(RateGRU, self).__init__()\n        self.batch = batch_sz\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_units = hidden_units\n        self.output_size = output_size\n        \n        #layers\n        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n        self.dropout = nn.Dropout(p=0.5)\n        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n        self.fc = nn.Linear(self.hidden_units, self.output_size)\n        \n    def initialize_hidden_state(self, device):\n        return torch.zeros((1, self.batch, self.hidden_units)).to(device)\n    \n    def forward(self, x, lens, device):\n        x = self.embedding(x)\n        self.hidden = self.initialize_hidden_state(device)\n        output, self.hidden = self.gru(x, self.hidden)\n        out = output[-1, :, :]\n        out = self.dropout(out)\n        out = self.fc(out)\n        \n        return out, self.hidden","c3c1a4bb":"def sort_batch(X, y, lengths):\n    \"sort the batches by length\"\n    \n    lengths, indx = lengths.sort(dim=0, descending=True)\n    X = X[indx]\n    y = y[indx]\n    \n    return X.transpose(0, 1), y, lengths","f50f31e5":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = RateGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\nmodel.to(device)\n\nit = iter(train_dataset)\nx, y, x_len = next(it)\n\nxs, ys, lens = sort_batch(x, y, x_len)\n\nprint('Input size: ', xs.size())\n\noutput, _ = model(xs.to(device), lens, device)\nprint(output.size())","e6a13438":"use_cuda = True if torch.cuda.is_available() else False\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = RateGRU(vocab_inp_size, embedding_dim, units, BATCH_SIZE, target_size)\nmodel.to(device)\n\n#loss criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\ndef loss_function(y, prediction):\n    target = torch.max(y, 1)[1] \n    loss = criterion(prediction, target)\n    \n    return loss\n\ndef accuracy(target, logit):\n    target = torch.max(target, 1)[1]\n    corrects = (torch.max(logit, 1)[1].data == target).sum()\n    accuracy = 100. * corrects \/ len(logit)\n    \n    return accuracy","694e52d3":"from torchvision import models\nprint(model)","fb271d40":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    \n    start = time.time()\n    total_loss = 0\n    train_accuracy, val_accuracy = 0, 0\n    \n    for (batch, (inp, targ, lens)) in enumerate(train_dataset):\n        loss = 0\n        predictions, _ = model(inp.permute(1, 0).to(device), lens, device)\n        \n        loss += loss_function(targ.to(device), predictions)\n        batch_loss = (loss \/ int(targ.shape[1]))\n        total_loss += batch_loss\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        batch_accuracy = accuracy(targ.to(device), predictions)\n        train_accuracy += batch_accuracy\n        \n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Val Loss {:.4f}'.format(epoch + 1,\n                                                              batch, \n                                                              batch_loss.cpu().detach().numpy()))\n            \n    for (batch, (inp, targ, lens)) in enumerate(val_dataset):\n        \n        predictions, _ = model(inp.permute(1, 0).to(device), lens, device)\n        batch_accuracy = accuracy(targ.to(device), predictions)\n        val_accuracy += batch_accuracy\n        \n    print('Epoch {} Loss {:.4f} -- Train Acc. {:.4f} -- Val Acc. {:.4f}'.format(epoch + 1,\n                                                                                total_loss \/ TRAIN_N_BATCH,\n                                                                                train_accuracy \/ TRAIN_N_BATCH,\n                                                                                val_accuracy \/ TEST_N_BATCH))\n        \n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","95a3cea3":"x_raw = []\ny_raw = []\nall_predictions = []\ntest_accuracy = 0\n\nfor (batch, (inp, targ, lens)) in enumerate(test_dataset):\n        \n    predictions, _ = model(inp.permute(1, 0).to(device), lens, device)\n    batch_accuracy = accuracy(targ.to(device), predictions)\n    test_accuracy += batch_accuracy\n    \n    all_predictions = all_predictions + [i.item() for i in torch.max(predictions, 1)[1]]\n    y_raw = y_raw + [y.item() for y in torch.max(targ, 1)[1]]\n        \nprint('Test Accuracy {:.4f}'.format(test_accuracy.cpu().detach().numpy() \/ TEST_N_BATCH))","e00a19b8":"conf_matrix = metrics.confusion_matrix(all_predictions, y_raw)\nconf_matrix = conf_matrix.astype(float)\n\nfor i in range(len(conf_matrix)):\n    conf_matrix[i] = np.true_divide(conf_matrix[i],conf_matrix[i].sum())*100\n\ndf_cm = pd.DataFrame(conf_matrix, index = [i for i in \"12345\"],\n                     columns = [i for i in \"12345\"])\nplt.figure(figsize = (10,7))\ncmap = sns.diverging_palette(10, 240, n=9, as_cmap=True)\nsns.heatmap(df_cm, annot=True, cmap=cmap)\nplt.title(\"Confusion matrix of categories (%)\", fontsize=14)\nplt.show()","bc8d6e00":"283\/conf_matrix[i].sum()","807e87ae":"## Keras","3a314ea6":"### Pretesting","daba75e4":"### Binarizing the target","e57d8b2b":"### Padding data","c24348b1":"### Training","777bf8d0":"## PyTorch","1d43706d":"## Preprocessing","d0175fc6":"### Split data","cdda5656":"### Data Loader","c66aee52":"### Vectorizing input"}}