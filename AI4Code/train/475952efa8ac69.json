{"cell_type":{"65087847":"code","ecacd9ef":"code","9b61a58a":"code","93970768":"code","cacfde87":"code","cc935516":"code","05b93fa6":"code","48ae60ac":"code","1842ddd5":"code","994e8619":"code","b0f1936e":"code","4c821726":"code","24304219":"code","38c0efd6":"code","f81cafce":"code","bf69ed8d":"code","92d6f1b7":"code","350a06e1":"code","253334ef":"code","1e7fbd9b":"code","8ed013f7":"code","2806fa89":"code","f446a186":"code","1539bf90":"code","54d53b21":"code","5d4972bd":"code","16537aa3":"code","fb02cc84":"code","409fba7b":"code","2d1a336d":"code","7f68ccd1":"markdown","929e2a1b":"markdown","81e8f8e8":"markdown","cc24c7aa":"markdown","e24433d3":"markdown","97cefacf":"markdown","38b229d4":"markdown","5accbff0":"markdown","be9038c0":"markdown","dc0fb640":"markdown","9bf896f0":"markdown","b8eb9bf7":"markdown","e83bbcb8":"markdown","332b5794":"markdown","3b6f0955":"markdown","9b2da16e":"markdown","9cdc32dc":"markdown","bf77aa18":"markdown","424f2ec4":"markdown","92f61a51":"markdown","656439a4":"markdown","8b0aa5a8":"markdown","3728e50b":"markdown","130ee190":"markdown","80e2f945":"markdown","5a2a8ffe":"markdown","74f9db94":"markdown"},"source":{"65087847":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\nimport time\nimport os\nfrom pprint import pprint\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","ecacd9ef":"!pwd","9b61a58a":"#Change directory to where dataset is - not required on kaggle\n#os.chdir(\"X:\\\\Datasets\\creditcardfraud\")","93970768":"#Reading the dataset\nmaster_df=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","cacfde87":"#Keeping our original dataset safe :)\ntransaction_data = master_df.copy()","cc935516":"display(transaction_data.head())\nprint(\"Dimensions of the dataset are:\", transaction_data.shape)","05b93fa6":"#Data points for Fraudulent transactions\nfraud = len(transaction_data[transaction_data[\"Class\"]==1])\nprint(\"Total Fraud transactions are\", fraud, \", which is\") \nprint(round(fraud\/len(transaction_data) * 100, 2), \"% of the dataset\")","48ae60ac":"#Checking for null values (all values are numerical)\ntransaction_data.isnull().sum()\n\n# We have no null values","1842ddd5":"#Let's look at the summary of the data\ntransaction_data.describe()","994e8619":"#Let's convert the time variable into hours from seconds before moving forward\n\ntransaction_data['Time'] = transaction_data['Time']\/3600","b0f1936e":"#Distribution of Amount\nfigure, a = plt.subplots(1, 2, figsize=(15, 4))\namount = transaction_data['Amount'].values\n\nsns.boxplot(amount, ax = a[0])\na[0].set_title('Distribution of Amount')\n\nfraud_df = transaction_data[transaction_data['Class']==1]\namount_fraud = fraud_df['Amount'].values\n\nsns.boxplot(amount_fraud, ax = a[1])\na[1].set_title('Distribution of Amount w.r.t. Fraud Transactions')\n#Keeping a similar scale as previous graph for comparison\n#a[1].set_xlim([min(amount), max(amount)])\nplt.show()","4c821726":"#Checking distribution with time\n\nfigure, a = plt.subplots(1, 2, figsize=(15, 4))\ntime = transaction_data['Time'].values\n\nsns.distplot(time, ax = a[0])\na[0].set_title('Distribution of Transactions with Time')\na[0].set_xlim([min(time), max(time)])\n#fraud_df_time = transaction_data[transaction_data['Class']==1]\n\ntime_fraud = fraud_df['Time'].values\n\nsns.distplot(time_fraud, ax = a[1])\na[1].set_title('Distribution with Time for Fraud Transactions')\n#Keeping a similar scale as previous graph for comparison\na[1].set_xlim([min(time_fraud), max(time_fraud)])\nplt.show()","24304219":"#Scaling time and amount values, as all other predictors are already scaled through PCA\n#Using min-max scaler for higher efficiency \n\nscaler = MinMaxScaler()\ntransaction_data['Time'] = scaler.fit_transform(transaction_data[\"Time\"].values.reshape(-1,1))\ntransaction_data['Amount'] = scaler.fit_transform(transaction_data[\"Amount\"].values.reshape(-1,1))","38c0efd6":"display(transaction_data.head())","f81cafce":"#Splitting into values and class labels \ntarget = 'Class'\nlabels = transaction_data[target]\nvalues = transaction_data.copy()\nvalues = values.drop('Class', axis =1 )\ndisplay(values.head())\nprint(\"Dataset dimension :\", values.shape)\nprint(\"Labels dimension :\", labels.shape)","bf69ed8d":"# We can use Stratified Shuffle Split or StratifiedKFold\nstrat_split = StratifiedShuffleSplit(n_splits=10, random_state=42)\nfor train_index, test_index in strat_split.split(values, labels):\n    values_train, values_test = values.iloc[train_index], values.iloc[test_index]\n    labels_train, labels_test = labels.iloc[train_index], labels.iloc[test_index]\nprint(\"Percentage of Fraud Transactions in Training Set using Stratified Shuffle Split:\", \n      round(labels_train.value_counts()[1]\/len(labels_train)*100, 4), \"%\")\n\nkfold_split = StratifiedKFold(n_splits=10, random_state=47)\nfor train_index, test_index in kfold_split.split(values, labels):\n    values_train1, values_test1 = values.iloc[train_index], values.iloc[test_index]\n    labels_train1, labels_test1 = labels.iloc[train_index], labels.iloc[test_index]\nprint(\"Percentage of Fraud Transactions in Training Set using Stratified KFold:\", \n      round(labels_train1.value_counts()[1]\/len(labels_train1)*100, 4), \"%\")\n","92d6f1b7":"#Applying SMOTE on the training dataset\nsmote = SMOTE(sampling_strategy='minority', random_state=47)\nos_values, os_labels = smote.fit_sample(values_train, labels_train)\n\nos_values = pd.DataFrame(os_values)\nos_labels = pd.DataFrame(os_labels)\n\nplt.figure(figsize=(5, 5))\nsns.countplot(data = os_labels, x = 0 )\nplt.title(\"Distribution of Oversampled Training Set\")\nplt.xlabel(\"Fraud\")\n\n\nprint(\"Dimensions of Oversampled dataset is :\", os_values.shape)","350a06e1":"#SelectKBest on our oversampled training dataset\nos_values_skb = os_values.copy()\nskb = SelectKBest(k=15)\nos_values_skb = skb.fit_transform(os_values_skb, os_labels[0].ravel())\ndisplay(pd.DataFrame(os_values_skb).head())","253334ef":"# Getting feature scores from SelectKBest after fitting\nfeature_list = values.columns\nunsorted_list = zip(feature_list, skb.scores_)\n\nsorted_features = sorted(unsorted_list, key=lambda x: x[1], reverse=True)\nprint(len(sorted_features))\nprint(\"Feature Scores:\\n\")\npprint(sorted_features[:15])","1e7fbd9b":"selected_features = [i[0] for i in sorted_features[:15]]\nprint(selected_features)\n#Transforming test according to Feature Selection\n\n#values_test = values_test[selected_features]\nos_values_skb = os_values.copy()\ndisplay(values_test.head())","8ed013f7":"#Using this Oversampled data to fit a Logistic Regression model\nlogr = LogisticRegression().fit(os_values_skb, os_labels[0].ravel())\n\n#Predicting on original test set\npredictions = logr.predict(values_test)\n\nprint(\"Accuracy Score: \", round(accuracy_score(labels_test, predictions)*100, 2), '%')\nprint(classification_report(labels_test, predictions, target_names=[\"No Fraud\", \"Fraud\"]))\n","2806fa89":"#Computing major metrics\ny_score = logr.decision_function(values_test)\nprecision, recall, _ = precision_recall_curve(labels_test, y_score)\nfig = plt.figure(figsize=(10,5))\nsns.lineplot(recall, precision, drawstyle='steps-post')\nplt.fill_between(recall, precision, alpha=0.1, color='#78a2e2')\nplt.title('Precision-Recall Curve for Logistic Regression')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.1])\nplt.xlim([0.0, 1.1])\n","f446a186":"t0 = time.time()\n\nDT = DecisionTreeClassifier()\ny_pred_DT = DT.fit(os_values_skb, os_labels[0].ravel()).predict(values_test)\n\nt1 = time.time()\nprint(\"Fitting Decision Tree model took\", t1-t0, \"secs.\")\nprint(\".\"*10)\nada = AdaBoostClassifier()\ny_pred_ada = ada.fit(os_values_skb, os_labels[0].ravel()).predict(values_test)\n\nt2 = time.time()\nprint(\"Fitting Adaboost model took\", t2-t1, \"secs.\")\nprint(\".\"*10)\nxgb = GradientBoostingClassifier()\ny_pred_xgb = xgb.fit(os_values_skb, os_labels[0].ravel()).predict(values_test)\n\nt3 = time.time()\nprint(\"Fitting XGBoost model took\", t3-t2, \"secs.\")\nprint(\".\"*10)","1539bf90":"# Precision, Recall, and actual prediction numbers\nprint(\"Accuracy Score for DT: \", round(accuracy_score(labels_test, y_pred_DT)*100, 2), '%')\nprint(classification_report(labels_test, y_pred_DT))\nprint(\"-\" * 40)\nprint(\"Accuracy Score for AdaBoost: \", round(accuracy_score(labels_test, y_pred_ada)*100, 2), '%')\nprint(classification_report(labels_test, y_pred_ada))\nprint(\"-\" * 40)\nprint(\"Accuracy Score for XGB: \", round(accuracy_score(labels_test, y_pred_xgb)*100, 2), '%')\nprint(classification_report(labels_test, y_pred_xgb))","54d53b21":"#Trying below three models:\n# Logistic Regression Classifier\ndef tune_LogR():\n    print(\"------------------ Using Logistic Regression --------------------\")\n    clf = LogisticRegression()\n    param_grid = {\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C' : [1.0, 10.0, 25.0, 50.0, 100.0, 500.0, 1000.0],\n        'clf__solver' : ['liblinear']\n    }\n\n    return clf, param_grid","5d4972bd":"# Create pipeline\nclf, params = tune_LogR()\nestimators = [('clf', clf)]\npipe = Pipeline(estimators)\n\n# Create GridSearchCV Instance\ngrid = GridSearchCV(pipe, params)\ngrid.fit(os_values_skb, os_labels[0].ravel())\n\n# Final classifier\nclf = grid.best_estimator_\n\nprint('\\n=> Chosen parameters :')\nprint(grid.best_params_)\n\npredictions = clf.predict(values_test)\nprint(\"Accuracy Score: \", round(accuracy_score(labels_test, predictions)*100, 2), '%')\nprint(\"Classification Report:\\n\", classification_report(labels_test, predictions, target_names = ['Non-Fraud', 'Fraud']))","16537aa3":"#Plotting the P-R curve for this tuned model\ny_score = clf.decision_function(values_test)\nprecision, recall, _ = precision_recall_curve(labels_test, y_score)\nfig = plt.figure(figsize=(10,5))\nsns.lineplot(recall, precision, drawstyle='steps-post')\nplt.fill_between(recall, precision, alpha=0.1, color='#78a2e2')\nplt.title('Precision-Recall Curve for Logistic Regression - After Tuning')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.1])\nplt.xlim([0.0, 1.1])","fb02cc84":"#Plotting confusion matrix for this\ntn, fp, fn, tp = confusion_matrix(labels_test, predictions).ravel()\nprint(\"True Negatives:\", tn)\nprint(\"False Positives:\", fp)\nprint('False Negatives:', fn)\nprint('True Positives:', tp)","409fba7b":"splitter = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\nfor train_index, test_index in splitter.split(os_values, os_labels):\n    _, os_values_test = os_values.iloc[train_index], os_values.iloc[test_index]\n    _, os_labels_test = os_labels.iloc[train_index], os_labels.iloc[test_index]","2d1a336d":"predictions = clf.predict(os_values_test)\nprint(\"Accuracy Score: \", round(accuracy_score(os_labels_test, predictions)*100, 2), '%')\nprint(\"Classification Report:\\n\", classification_report(os_labels_test, predictions, target_names = ['Non-Fraud', 'Fraud']))","7f68ccd1":"#### Tuning Our Models\n\nWe will be tuning our basic Logistic Regression model, using various parameters, to obtain a better performance.","929e2a1b":"As we can see, our Logistic Regression model, is not proving to be as good as model which the accuracy is suggesting. The precision score is too low. The Precision-Recall curve though, depicts a different story. The documentation states the following:\n\n> <font color=\"#686a6d\">The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall)\"<\/font>","81e8f8e8":"Now, since our dataset is equally balanced, let's apply selectKbest for attaining the best features as per feature scores.<br> Note that we will be using SKB on the oversampled dataset, as that is the one which will be used to fit the model. Hence, transforming the complete dataset would be unneccesary.","cc24c7aa":"From this visualisation, we can see that there is not much difference in the trend with time as well. Though an interesting thing to note will be the higher number of fraud transactions near the 10th hour. The total transactions are higher around the 20th hour. But we can't say for sure if this assumption is believable as data points for fraud transactions are too less.","e24433d3":"It follows from this, that our analysis will be severely affected by the huge class imbalance that we have observed. Even during model building, our model might tend to overfit the data, and we might see very high accuracy scores. Hence, we will have to keep in mind the following things:\n\n- Dataset needs to be modified in a way, to provide some balance between both classes. We can generally do this through **Under Sampling or Over Sampling** the data.\n- Whichever model we use for prediction, we will have to use appropriate metrics to check our model performance, as simple scores such as Accuracy can be very misleading. Best approach would be to use Precision-Recall scores as well as **Area Under Precision-Recall Curve.**","97cefacf":"Unfortunately, after testing SelectKBest, our predictions were getting only worse, so we will not be using SelectKBest. For model building purpose, all features will be used, as is.","38b229d4":"### Summary","5accbff0":"The area under the P-R curve seems to depict that our model is working moderately well. We could expect a better performance though, since we have not even tuned this model as of now. We will have to tune our model with various parameters, for achieving a better performance. Before we do that, let's look at the performance with some other models.","be9038c0":"***","dc0fb640":"In conclusion, we have understood that this dataset has a huge class imbalance, that will definitely affect our analysis as well as model building approach. We have to keep in mind some important points, which are stated below:\n\n- The relationships between our target variable 'Class' and our predictor variables, is difficult to estimate if we just look at the original dataset. It is highly skewed.\n- To establish a clear understanding, we need to remove the class imbalance present in our dataset. We have used SMOTE as an oversampling technique for the minority class. We have not gone with undersampling as it may lead to information loss.\n- Since this is a binary classification problem, we have chosen Logistic Regression as our model. After oversampling and tuning our model, we have achieved an accuracy of **97.34%**. Our main metric, **Area Under the Precision Recall Curve**, seems to show that model performance is moderate. We have high recall scores, but lower precision scores.\n- Testing it on a balanced dataset, we have seen that both precision and recall prove to be very high.\n\n","9bf896f0":"We are getting similar results with both. Basically, our class imbalance percentage is maintained in our training sets, for both the cases. We will go ahead with StratifiedShuffleSplit.<br>","b8eb9bf7":"We are achieving similar results from this. Let's move on to tuning our parameters for our models, using GridSearchCV.\n***","e83bbcb8":"Also, I wish to use SelectKBest algorithm, to use the best features for classification. But since as of now, there is a huge class imbalance, the same will be represented through SelectKBest. So I will use it, after oversampling my dataset.","332b5794":"***\n#### Model Building\nLet's now test this oversampled dataset on a basic Logistic Regression model, as this is a binary classification problem","3b6f0955":"# Credit Card Fraud Detection\n#### A Case Study\n***\n","9b2da16e":"At first glance, we can easily see that features V1-28 have been transformed, as mentioned, using PCA. We can't really gather anything from them.<br>\nAlso, looks like 'Amount' might turn out to be a parameter to predict outliers(frauds), as there is a huge difference between Maximum amount (25691.16) and Mean amount (88.349619). Let's check.","9cdc32dc":"As we can see, the outliers in *fraud transactions* are not very significantly high, as compared to the whole dataset. So amount doesn't really look like a direct predictor. ","bf77aa18":"Our main concern, is to point out if a transaction can be safely classified as fraud or not. This model might not be highly accurate in predicting frauds, but at the same time the number of correctly predicted 'Non-Fraud' Transactions is a positive point. We don't want actual, genuine transactions to be classified as Fraud. The case presented in this study is definitely a real world scenario, where the number of fake transactions would be quite low, when compared to genuine transactions.","424f2ec4":"Before applying any sampling technique, it should be remembered that we are going to fit our model, using the *sampled training set*, but we will be predicting using only the *original test set*. So we will have to split the data into training and testing sets, before we can apply any sort of sampling.","92f61a51":"As is expected, we are getting amazing scores all over for this, as we have a balanced dataset, plus the model has been trained on it before.","656439a4":"***","8b0aa5a8":"From the source of this dataset, we have the information that there are over 2 lakh data points, out of which less than one percent is classified as fraud. It is a clear **Class Imbalance** scenario. As an initial thought, we will have to keep that in mind during our model building phase. Let's dive into it.","3728e50b":"Let's test out our model's performance on an balanced dataset, for validation purposes.\nWe will split the data with a ratio of 70:30, keeping the percentage equal, as in using the oversampled dataset.","130ee190":"The aim is to identify a fraudulent credit card transaction from a non-fraudulent one, and subsequently build a model to predict the same.\nThe dataset provided is for transactions made by credit cards in September 2013 by european cardholders, for a time period of two days.","80e2f945":"Lastly, as part of future additions to this study, I would like to go ahead with in depth analysis of the PCA scaled features provided to us, through Hypothesis testing. It might lead to some idea of how they are affecting the target class. Also, using unsupervised learning techniques such as K-Means Clustering, on the oversampled data, might prove to be beneficial, as SMOTE works on the principle of nearest neighbours as well.","5a2a8ffe":"Now, coming to the first problem of class imbalance, let's go forward with a technique called SMOTE, or Synthetic Minority Oversampling Technique. As the name suggests, it is an oversampling technique which will increase the minority class data points in proportion to the majority class.<br>\n\nIt should be noted that we are not going with UnderSampling, as it poses the risk of losing out on important information. With SMOTE (oversampling), more information is available.","74f9db94":"#### Let's go ahead with data manipulation."}}