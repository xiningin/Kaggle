{"cell_type":{"096f34e8":"code","bb7260f6":"code","e22ed3c5":"code","873b9411":"code","3ce22846":"code","e36177f9":"code","f9409ee7":"code","4a231bf0":"code","7299beca":"code","55e903fa":"code","95a665ea":"markdown","71eba6f5":"markdown","6a1cabc2":"markdown","62ba8102":"markdown","ce311e46":"markdown","a4cb3593":"markdown","5763c80c":"markdown","329414a2":"markdown","cdb54e1d":"markdown","b95ca7bf":"markdown"},"source":{"096f34e8":"!pip uninstall -y opencv-python\n\n!pip3 install opencv-contrib-python","bb7260f6":"!pip3 install imutils","e22ed3c5":"import numpy as np \nimport pandas as pd \nimport cv2, os, re\nimport matplotlib.pyplot as plt\nimport imutils\n\nfrom tqdm import tqdm\nfrom imutils.object_detection import non_max_suppression\n\nfrom tensorflow.keras.models import load_model","873b9411":"def find_regions(image, method):\n    \n    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n    ss.setBaseImage(image)\n    \n    if method == 'fast':\n        ss.switchToSelectiveSearchFast()\n    else:\n        ss.switchToSelectiveSearchQuality()\n    \n    rects = ss.process()\n    boxes = []\n    for (x,y,w,h) in rects:\n        \n        boxes.append([x,y,w,h])\n        pass\n    \n    return boxes\n    pass","3ce22846":"scene_path = '..\/input\/ships-in-satellite-imagery\/scenes\/scenes'\n\ndef load_scenes():\n    \n    scenes = []\n    dirfiles = os.listdir(scene_path)\n    dirfiles = sorted(dirfiles)\n    for file in dirfiles:\n        \n        print(os.path.join(scene_path, file))\n        scene = cv2.imread(os.path.join(scene_path, file))\n        scene = cv2.cvtColor(scene, cv2.COLOR_BGR2RGB)\n        scenes.append(scene)\n        pass\n    \n    return scenes\n    pass","e36177f9":"scenes = load_scenes()","f9409ee7":"%%time\nmethod = \"fast\"\n\nboxes = []\n\nfor scene in scenes:\n    \n    box_in_scene = find_regions(scene, method)\n    boxes.append(box_in_scene)\n    pass","4a231bf0":"model = load_model('..\/input\/ship-detection-using-faster-r-cnn\/ship-model.h5')\n\nmodel.summary()","7299beca":"%%time\n\nrois, locs = [], []\nimages = []\nfor i,scene in tqdm(enumerate(scenes)):\n    \n    (H, W) = scene.shape[:2]\n    region, loc = [],[]\n    for (x,y,w,h) in boxes[i]:\n\n        if w\/float(W) > 0.10 and h\/float(H) > 0.10:\n            continue\n\n        roi = scene[y:y+h,x:x+w]\n        roi = cv2.cvtColor(roi,cv2.COLOR_BGR2RGB)\n        roi = cv2.resize(roi, (48,48))\n\n        rois.append(roi)\n        locs.append((x,y,x+w,y+h))\n        pass\n    \n    preds = model.predict(np.array(rois,dtype=np.float32))\n    preds = np.argmax(preds, axis=1)\n    \n    img = scene.copy()\n    for (i,label) in enumerate(preds):\n\n        if label == 1:\n            (startX,startY,endX,endY) = locs[i]\n            cv2.rectangle(img,(startX,startY),(endX,endY),(0,255,0),2)\n        pass\n    \n    images.append(img)\n    del rois[:]\n    del locs[:]\n    pass","55e903fa":"for image in images:\n    \n    plt.imshow(image)\n    plt.show();","95a665ea":"Installing opencv-contrib as SelectiveSearch Algorithm is still not part of the official package","71eba6f5":"Installing *imutils* for non-max suppression libraries","6a1cabc2":"# Predicting the ROIs\n\nThe ROIs for each scene are taken and run through the model. ROIs with scaling factor greater than 0.10 are ignored as they are to be big for the and lead to uncessary ROIs being predicted. The scaling factor in an ideal situation should vary between 0.10 and 0.15, as they are the sweet spots, where a ship inside a ROI but not an entire city block.\n\nBounding boxes are drawn around each detected object if it is classified as a ship, otherwise not. The resulting images are stored in the *images* list for displaying.","62ba8102":"# Loading Saved Model\n\nThe model trained in [Ship Detection using Faster R-CNN: Part 1](https:\/\/www.kaggle.com\/apollo2506\/ship-detection-using-faster-r-cnn) on the dataset is loaded along with the weights obtained from the model. The current model is based upon the class weights. For the model based upon augmentation, Part 1 will have to be run with **AUGMENTATION** set to *True*.","ce311e46":"# Importing the libraries\n\nThe libraries required for implementing the *Ship detection* in a geo-spatial image are imported","a4cb3593":"To change the type of detection, change the *method* variable. Options:\n```\nfast\nquality\n```","5763c80c":"# Utitilty Functions\n\n1. find_regions: This method is used for finding Region of Interests(ROI) using the Selective Search Algorithm. There are 2 methods, *fast* and *quality*. The *fast* methods finds the ROI faster but with lesser quality, whie the *quality* finds more ROI at a cost of more time spent\n\n2. load_scenes: Load the geospatial images into a list for object detection.","329414a2":"# Finding ROIs\n\nThe Region of Interests(ROIs) are found for each loaded scene, depending on the method used, the time taken varies along with the detection. For countering this, it is better to create a feature map of the scene and then finding the ROIs.\n\n1. Fast -> 17min 28s\n\n2. Quality","cdb54e1d":"# Detection Using Selective Search\n\nThis notebook contains the steps required for detecting ships in the geo-spatial images using Selective Search. Selective Search works by over-segmenting an image using a superpixel algorithm.\n\nIt merges superpixels in a hierarchial fashion based on 5 key points:\n1. Color Similarity: Computing  a 25-bin histogram for each channel of an image. Similarity of any 2 regions is measured by the histogram intersection distance.\n\n2. Texture Similarity: For texture, Selective Search extracts the Gaussian derivatives at 8 orientation per channel(3\\*8=24). These are used to compute a 10-bin histogram. They are compared using histogram intersection distance.\n\n3. Size Similarity: Smaller regions are grouped earlier. Hierarchial Agglomearative Clustering are prone to clusters reaching a critical mass and then combining every overlapping location.\n\n4. Shape Similarity\/compatibility: Two regions are considered compatible if they fir into each other, thereby filling gaps in the regional proposal generation.\n\n5. Meta-similarity measure: Acts a linear combination of color, texture, size and shape similarity.\n\n[Selective Search for Object Recognition](http:\/\/www.huppelen.nl\/publications\/selectiveSearchDraft.pdf)","b95ca7bf":"# Displaying final Images\n\nThe final images containing the predicted bounding boxes are displayed in the next code block."}}