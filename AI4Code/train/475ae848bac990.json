{"cell_type":{"9260ff37":"code","1a4a4b7f":"code","340ad0c6":"code","511c5db6":"code","5ce63963":"code","2acd1c37":"code","fb6c32a0":"code","16f80934":"code","20a0075a":"code","1f9aa4a2":"code","30e17223":"code","3bebdf3a":"code","975bec57":"code","1eae68e7":"code","4f64f306":"code","ba51abfb":"code","7ab6bc18":"code","d2d6e126":"code","97e2aaad":"code","7a0deda3":"code","8790e22a":"code","db203e53":"code","60c60699":"code","818986dc":"code","f7a31ef1":"code","2c46557c":"code","73e58540":"markdown","7f50b412":"markdown","cdc5665a":"markdown","024c3866":"markdown","d53d0537":"markdown","5862d7b2":"markdown","8cf80f88":"markdown","36630078":"markdown","a10c8435":"markdown","d8098373":"markdown","05f7a3d0":"markdown","1ee6e359":"markdown","5d64093c":"markdown"},"source":{"9260ff37":"import gc\nimport os\nimport glob\nimport zipfile\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nimport cv2\nimport PIL\nfrom PIL import ImageOps, ImageFilter, ImageDraw","1a4a4b7f":"DATA_PATH = '..\/input\/'\nos.listdir(DATA_PATH)","340ad0c6":"# \uc774\ubbf8\uc9c0 \ud3f4\ub354 \uacbd\ub85c\nTRAIN_CROP_PATH = \".\/train_crop\"\nTEST_CROP_PATH = \".\/test_crop\"\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')\n\n# CSV \ud30c\uc77c \uacbd\ub85c\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))","511c5db6":"df_train.head()","5ce63963":"df_test.head()","2acd1c37":"def crop_resize_boxing_img(img_name, margin=16, size=(224, 224)) :\n    if img_name.split('_')[0] == \"train\" :\n        PATH = TRAIN_IMG_PATH\n        data = df_train\n    elif img_name.split('_')[0] == \"test\" :\n        PATH = TEST_IMG_PATH\n        data = df_test\n        \n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, \\\n                   ['bbox_x1','bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1,y1,x2,y2)).resize(size)","fb6c32a0":"!mkdir {TRAIN_CROP_PATH}","16f80934":"%%time\nfor i, row in df_train.iterrows():\n    cropped = crop_resize_boxing_img(row['img_file'])\n    cropped.save(f\"{TRAIN_CROP_PATH}\/{row['img_file']}\")","20a0075a":"!mkdir {TEST_CROP_PATH}","1f9aa4a2":"%%time\nfor i, row in df_test.iterrows():\n    cropped = crop_resize_boxing_img(row['img_file'])\n    cropped.save(f\"{TEST_CROP_PATH}\/{row['img_file']}\")","30e17223":"tmp_imgs = df_train['img_file'][100:105]\nplt.figure(figsize=(12,20))\n\nfor num, f_name in enumerate(tmp_imgs):\n    img = PIL.Image.open(os.path.join(TRAIN_IMG_PATH, f_name))\n    plt.subplot(5, 2, 2*num + 1)\n    plt.title(f_name)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    img_crop = PIL.Image.open(f\"train_crop\/{f_name}\")\n    plt.subplot(5, 2, 2*num + 2)\n    plt.title(f_name + ' cropped')\n    plt.imshow(img_crop)\n    plt.axis('off')","3bebdf3a":"from sklearn.model_selection import train_test_split\n\ndf_train[\"class\"] = df_train[\"class\"].astype('str')\n\ndf_train = df_train[['img_file', 'class']]\ndf_test = df_test[['img_file']]\n\nits = np.arange(df_train.shape[0])\ntrain_idx, val_idx = train_test_split(its, train_size = 0.8, random_state=42)\n\nX_train = df_train.iloc[train_idx, :]\nX_val = df_train.iloc[val_idx, :]\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(df_test.shape)","975bec57":"import tensorflow as tf\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","1eae68e7":"# Parameter\nimg_size = (224, 224)\nnb_train_samples = len(X_train)\nnb_validation_samples = len(X_val)\nnb_test_samples = len(df_test)\nepochs = 20\nbatch_size = 32\n\n# Define Generator config\ntrain_datagen = ImageDataGenerator(\n    horizontal_flip = True, \n    vertical_flip = False,\n    preprocessing_function=preprocess_input\n)\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# Make Generator\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=X_train, \n    directory=TRAIN_CROP_PATH,\n    x_col = 'img_file',\n    y_col = 'class',\n    target_size = img_size,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    seed=42\n)\n\nvalidation_generator = val_datagen.flow_from_dataframe(\n    dataframe=X_val, \n    directory=TRAIN_CROP_PATH,\n    x_col='img_file',\n    y_col='class',\n    target_size=img_size,\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=batch_size,\n    shuffle=False\n)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_CROP_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= img_size,\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)","4f64f306":"from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, GlobalAveragePooling2D","ba51abfb":"# for layer in resNet_model.layers:\n#     layer.trainable = False\n#     print(layer,layer.trainable)\n\nmobileNetModel = MobileNet(weights='imagenet', include_top=False)\n\nmodel = Sequential()\nmodel.add(mobileNetModel)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(196, activation='softmax', kernel_initializer='he_normal'))\nmodel.summary()","7ab6bc18":"from sklearn.metrics import f1_score\n\ndef micro_f1(y_true, y_pred):\n    return f1_score(y_true, y_pred, average='micro')\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","d2d6e126":"def get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0 :\n        return (num_samples \/\/ batch_size) + 1\n    else :\n        return num_samples \/\/ batch_size","97e2aaad":"%%time\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nfilepath = \"my_mobilenet_model_{val_acc:.2f}_{val_loss:.4f}.h5\"\n\nckpt = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True)\nes = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=1, mode='auto')\n\ncallbackList = [ckpt]\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch = get_steps(nb_train_samples, batch_size),\n    epochs=epochs,\n    validation_data = validation_generator,\n    validation_steps = get_steps(nb_validation_samples, batch_size),\n    callbacks = callbackList\n)\ngc.collect()","7a0deda3":"# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","8790e22a":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","db203e53":"model_list = sorted([i for i in os.listdir() if \"my_\" in i])\nmodel_list","60c60699":"model.load_weights(model_list[-1])","818986dc":"%%time\ntest_generator.reset()\nprediction = model.predict_generator(\n    generator = test_generator,\n    steps = get_steps(nb_test_samples, batch_size),\n    verbose=1\n)","f7a31ef1":"predicted_class_indices=np.argmax(prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","2c46557c":"!rm -rf *_crop","73e58540":"# Cropped Image Dataset","7f50b412":"CNN \ubaa8\ub378\uc744 \ucd08\uae30\ud654 \ud560\ub54c \ub2e4\uc74c\uacfc \uac19\uc774 `weights='imagenet'`\uc744 Keras\uc5d0\uc11c \ubaa8\ub378\uc744 \uc790\ub3d9\uc73c\ub85c \ub2e4\uc6b4\ubc1b\uc544 \ub85c\ub529\ud569\ub2c8\ub2e4.\n\npretrained \ubaa8\ub378\uc758 top layer\ub294 Fully connected layer\ub85c 1000\uac1c\ub97c \ubd84\ub958\ud558\ub294 \uc6a9\ub3c4\uc785\ub2c8\ub2e4.\n\n\uc774 \ubb38\uc81c\uc5d0\uc11c\ub294 196\uac00\uc9c0\uc758 \ubd84\ub958\ub97c \ud559\uc2b5\uc2dc\ucf1c\uc57c \ud558\uae30\ub54c\ubb38\uc5d0  \n`inclue_top=False` \uc635\uc158\uc73c\ub85c \ud574\ub2f9 \ub808\uc774\uc5b4\ub97c \ubd84\ub9ac\uc2dc\ud0a8 \ud6c4,\n`Dense(196)`\uc744 \ucd94\uac00\ud558\uc154\uc57c \ud569\ub2c8\ub2e4.\n\n\ud0dc\uc9c4\ub2d8\uc758 \uae30\ubcf8 \ucf54\ub4dc\uad6c\uc870\uac00 \uc774\ubbf8 \uadf8\ub807\uac8c \ub418\uc5b4\uc788\uae30 \ub54c\ubb38\uc5d0 \uc635\uc158\ub9cc \ucd94\uac00\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.","cdc5665a":"\uba3c\uc800 \uac00\uc7a5 \uc88b\uc740 \ubaa8\ub378\uc744 \ub85c\ub529\ud558\uae30 \uc704\ud574 \ub530\ub85c \ud30c\uc77c\uc744 \uc815\ub82c\ud574\uc8fc\uace0,  \n\ub9c8\uc9c0\ub9c9 \uc778\ub371\uc2a4 \ud30c\uc77c\uc744 \ubaa8\ub378\uc5d0 \ub85c\ub529\ud569\ub2c8\ub2e4.","024c3866":"Validation score\uac00 \uac00\uc7a5 \uc88b\uc740 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 `ModelCheckpoint`\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub97c \uc0ac\uc6a9\ud558\uba74 `monitor`\uc635\uc158\uc5d0 \uc9c0\uc815\ub41c \uc2a4\ucf54\uc5b4\uac00 \uac00\uc7a5 \uc88b\uc744 \ub54c \ubaa8\ub378 \ud30c\uc77c(weights)\uc744 \uc800\uc7a5\ud569\ub2c8\ub2e4.\n\n\ub2e4\uc74c\uacfc \uac19\uc774 \uc0ac\uc6a9\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","d53d0537":"## Submission","5862d7b2":"# Cropped Image Eye Checking","8cf80f88":"# Process Test Image Data Crop","36630078":"# Process Train Image Data Crop","a10c8435":"# Keras, How to use pretrained model?\n\npretrained \ubaa8\ub378\uc744 \ud558\ub294 \ubc95\uc744 \uc18c\uac1c\ud569\ub2c8\ub2e4.\n\n\ub300\ubd80\ubd84\uc758 \uc18c\uc2a4\ucf54\ub4dc\ub294 \ub2e4\ub978 \ucf54\ub4dc\ub97c \ucc38\uace0\ud558\uc5ec\uc11c \uc81c\uac00 \uc218\uc815\ud55c \ucf54\ub4dc\ub9cc \uc124\uba85\uc744 \ub4dc\ub9ac\ub3c4\ub85d\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ub9cc\uc57d \uce90\uae00\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \ucee4\ub110\uc744 \uc0ac\uc6a9\ud558\uc2e0\ub2e4\uba74,  \n\uaf2d \uc6b0\uce21 `Settings`\uc5d0\uc11c `GPU` \uc640 `Internet` \ud56d\ubaa9\uc744 `On`\uc73c\ub85c \ud65c\uc131\ud654 \ud574\uc8fc\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4.\n\n## References\n\n\ub300\ubd80\ubd84\uc758 \ucf54\ub4dc\ub294 \uae40\ud0dc\uc9c4\ub2d8 \ubca0\uc774\uc2a4\ub77c\uc778 \ucf54\ub4dc\uc640 \ud5c8\ud0dc\uba85\ub2d8\uc758 \uc774\ubbf8\uc9c0 Cropping \ucf54\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.  \n\uc88b\uc740 \uc790\ub8cc \uc81c\uacf5\ud574\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!\n\n* [Applications - Keras Documentation](https:\/\/keras.io\/applications\/)\n* [\uae40\ud0dc\uc9c4\ub2d8 \ucee4\ub110: [3rd ML Month] Car Model Classification Baseline](https:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline)\n* [\ud5c8\ud0dc\uba85\ub2d8 \ucee4\ub110: [3rd ML Month] Car Image Cropping](https:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping)\n\n## Introduction\n\nKeras Documentation\uc5d0\uc11c\ub294 Pretrained \ubaa8\ub378\uc5d0 \ub300\ud574\uc120 \ub2e4\uc74c\uacfc \uac19\uc774 \ub9d0\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n> The top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset.\n\n\uc774\ub294 ImageNet \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud574\uc11c \ud559\uc2b5\uc744 \uc2dc\ud0a8 weights\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n## What is a pretrained model?\n\n\ubbf8\ub9ac\uc798 \ud559\uc2b5\ub41c \ubaa8\ub378 \ud30c\uc77c\uc744 \ub2e4\uc6b4\ubc1b\uc544, \ud574\ub2f9 \ud30c\uc77c\ub85c Weight\ub97c \ucd08\uae30\ud654 \uc2dc\ud0a4\ub294 \uac83\uc744 Pretrained \ubaa8\ub378\uc774\ub77c \ud558\uace0,\n\n\uc774\ub7ec\ud55c \ud559\uc2b5\ubc95\uc744 **Transfer Learning(\uc804\uc774\ud559\uc2b5)** \uc774\ub77c \ud569\ub2c8\ub2e4.\n\n\ub2e4\uc74c\ubd80\ud130 \uc804\uc774 \ud559\uc2b5\uc5d0 \ud544\uc694\ud55c \ucf54\ub4dc \uc218\uc815 \ubd80\ubd84\ub9cc \uc124\uba85\ud574 \ub4dc\ub9ac\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n","d8098373":"\ud0dc\uba85\ub2d8\uc774 \uc791\uc131\ud558\uc2e0 \ud568\uc218\uc5d0\uc11c Resize \uae30\ub2a5\uc744 \ucd94\uac00\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n\uacf5\uc778\ub41c CNN \ub124\ud2b8\uc6cc\ud06c\uc758 Input shape\uc744 \ubcf4\uc2dc\uba74 `224 * 224`, `299 * 299` \uc640 \uac19\uc740 shape\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uae30 \ub54c\ubb38\uc5d0,  \n\ubbf8\ub9ac \ub370\uc774\ud130\ub97c \uc900\ube44\ud558\ub294 \ub2e8\uacc4\uc5d0\uc11c shape\uc744 \uc9c0\uc815\ud558\uace0 reshape(resize)\uc744 \ud558\uace0 \uc800\uc7a5\ud560 \uac83 \uc785\ub2c8\ub2e4.","05f7a3d0":"### \ub05d\uae4c\uc9c0 \ubd10\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!\n\n**\uc88b\uc740 \ub300\ud68c\uc640 \uc790\ub8cc\ub97c \uacf5\uc720\ud574 \uc8fc\uc2e0 \uce90\uae00 \ucf54\ub9ac\uc544 \ubd84\ub4e4\uaed8 \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4.**\n\n**\uc7ac\ubc0c\uac8c \uce90\uae00\ud574\uc694^^**","1ee6e359":"## Modeling","5d64093c":"\uc800 \uac19\uc740 \uacbd\uc6b0 \ubaa8\ub378\uc744 `MobileNet` \uc73c\ub85c \uc0ac\uc6a9\ud558\uc600\ub294\ub370,  \n\ubaa8\ub378\uc774 \uac00\ubccd\uace0 \ube60\ub974\uac8c \ud559\uc2b5\ub418\uc11c, \ud14c\uc2a4\ud2b8\uc5d0 \uc6a9\uc774\ud574 \ub9ce\uc774 \uc0ac\uc6a9\ud558\uace0\uc788\uc2b5\ub2c8\ub2e4.\n\n\ubaa8\ub378\uc744 \uad50\uccb4\ud558\uba74\uc11c \uac00\uc7a5 \uc88b\uc740 \uc544\ud0a4\ud14d\uccd0\ub97c \ucc3e\ub294 \uc791\uc5c5\ub3c4 \uc88b\uc740 \uc2a4\ucf54\uc5b4\ub97c \uc5bb\ub294\ub370 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4."}}