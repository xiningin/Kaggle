{"cell_type":{"875be9c0":"code","508b2d85":"code","814c7f1b":"code","6369b98b":"code","d9937f17":"code","a3b868f4":"code","0d9bb926":"code","35b684b9":"code","c6c6eb6e":"code","6dd649ff":"code","4b9dc4ef":"code","c86eec11":"code","7a7b5360":"code","6fb3ca85":"code","f80912f0":"code","597d472c":"code","f63f736a":"code","509c8809":"code","e2543c7e":"code","3f5af668":"code","c5ff69d1":"code","1e0b5b06":"code","95725fbd":"code","52fdb30e":"code","1b6250cd":"code","41e227a9":"code","99aaab0b":"code","e043371a":"code","e87a9796":"code","7e600dd2":"code","f441bb29":"code","c816bec5":"code","4095dde3":"markdown","73b8179a":"markdown","d342b749":"markdown","ece8d973":"markdown","e478853d":"markdown","62057456":"markdown","7f499726":"markdown","7ce19655":"markdown","00402cfa":"markdown","df2a2fae":"markdown","63755a43":"markdown"},"source":{"875be9c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","508b2d85":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline\n","814c7f1b":"train = pd.read_csv(\"\/kaggle\/input\/credit-card-buyers\/train data credit card.csv\")\ntest = pd.read_csv('\/kaggle\/input\/credit-card-buyers\/test data credit card.csv')\ntrain.head()","6369b98b":"plt.subplots(figsize=(20,10))\n\n\n\nplt.subplot(231)\nsns.barplot(x = 'Gender',y = 'Is_Lead',data = train)\n\nplt.subplot(232)\nsns.barplot(x = 'Gender',y = 'Age',data = train)\n\nplt.subplot(233)\nsns.barplot(x = 'Occupation',y = 'Avg_Account_Balance',data = train)\n\nplt.subplot(234)\nsns.barplot(x = 'Credit_Product',y = 'Is_Lead',data = train)\n\nplt.subplot(235)\nsns.barplot(x = 'Is_Active',y = 'Is_Lead',data = train)","d9937f17":"train.info()","a3b868f4":"# Check NAN Values\nfor i in train.columns:\n    print (i+\": \"+str(train[i].isna().sum()))","0d9bb926":"# as you see there is NAN values are present, particularly this is categorical feature in this case you can replace NAN values as missing values.\n# But as we just analysing, lets check if we drop NAN values then how it will perform.\ntrain['Credit_Product'] = train['Credit_Product'].dropna(axis = 0)\n","35b684b9":"train.Credit_Product.value_counts()","c6c6eb6e":"# Data is imbalanced, let used imbalance library from sklearn, and as you can see person who is buying credit card having less number of data \n# Either we can perform undersampling or oversampling, in this model we will try with undersampling lets check how it will perform.\nsns.countplot(x = 'Is_Lead',data = train)","6dd649ff":"train.Is_Lead.value_counts()","4b9dc4ef":"#There are some categorical features are present, accordingly we can use nominal encoding or else we can use ordinal encoding.\n# I am using ordinal encoding on occupation and on other feature im going to use nominal encoding.\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler\nlabel = LabelEncoder()\n\ntrain['Occupation'] = label.fit_transform(train['Occupation'])\n\ntrain['Gender'] = pd.get_dummies(train['Gender'],drop_first = True)\ntrain['Is_Active'] = pd.get_dummies(train['Is_Active'],drop_first = True)\ntrain['Credit_Product'] = pd.get_dummies(train['Credit_Product'],drop_first = True)\n","c86eec11":"x = train.drop(['Is_Lead','ID','Region_Code','Channel_Code'],axis = True)\ny = train.loc[:,['Is_Lead']]\nx","7a7b5360":"## Get the Fraud and the normal dataset \n\ninterested= train[train['Is_Lead']==1]\n\nnot_interested = train[train['Is_Lead']==0]","6fb3ca85":"print(interested.shape,not_interested.shape)","f80912f0":"from imblearn.under_sampling import NearMiss\n\n# Implementing Undersampling for Handling Imbalanced \nnm = NearMiss()\nX_res,y_res=nm.fit_resample(x,y)\n","597d472c":"X_res","f63f736a":"y_res.value_counts()","509c8809":"final_data = pd.merge(X_res,y_res,left_index = True,right_index = True)\nX_res.shape,y_res.shape","e2543c7e":"# most of the variable is normal, means it does not having outliers as checked one parameter is having most of the outliers. either you can remove outliers and perform separate algorithm for outliers.\nplt.figure(figsize = (20,8))\nsns.boxplot(y='Avg_Account_Balance',data = final_data)","3f5af668":"final_data","c5ff69d1":"Q1 = final_data['Avg_Account_Balance'].quantile(0.25)\nQ3 = final_data['Avg_Account_Balance'].quantile(0.75)\nIQR = Q3 - Q1\n\nfilter = (final_data['Avg_Account_Balance'] >= Q1 - 1.5 * IQR) & (final_data['Avg_Account_Balance']<= Q3 + 1.5 *IQR)\ntrain2 = final_data.loc[filter]  \nprint(\"data loss percentage {}%\".format(((len(final_data) - len(train2))\/len(final_data))*100))","1e0b5b06":"# Check data is correlated with each other, if features are correlated then you can drop that feature, but check wether that feature are using for same purpose.\n# from below graph,there is paramter which is showing correlation more than 0.5 that means these are correlated  but as you can see one feature is age and anpther one is vintage and both is used for differnet purpose.\ncorrelation = train2.corr()\nplt.figure(figsize = (20,8))\nsns.heatmap(correlation,annot = True,cmap = 'rocket')","95725fbd":"train2.head()","52fdb30e":"# Assigning dependent and independent values.\nx = train2.iloc[:,:-1]\n\ny = train2.iloc[:,-1]","1b6250cd":"from sklearn.preprocessing import StandardScaler\n\n\nstd = StandardScaler()\n\nx_std = std.fit_transform(x)","41e227a9":"from sklearn.model_selection import train_test_split\n\n#Split data into Train and test format\nx_train,x_test,y_train,y_test = train_test_split(x_std,y,test_size = 0.20,random_state =42)\n\nprint('Shape of Training Xs:{}'.format(x_train.shape))\nprint('shape of Test:{}'.format(x_test.shape))","99aaab0b":"#y_train = np.array(y_train).astype(int)\n#y_test= np.array(y_test).astype(int)","e043371a":"from sklearn import svm\nclf = svm.SVC()\nclf.fit(x_train,y_train)\ny_predicted = clf.predict(x_test)\nscore = clf.score(x_test,y_test)\nprint(score)","e87a9796":"from sklearn.metrics import confusion_matrix\n\n#Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","7e600dd2":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nada.fit(x_train,y_train)\ny_predicted = ada.predict(x_test)\nscore = ada.score(x_test,y_test)\nprint(score)","f441bb29":"#Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","c816bec5":"from sklearn.ensemble import GradientBoostingClassifier\ngradient = GradientBoostingClassifier()\ngradient.fit(x_train,y_train)\ny_predicted = gradient.predict(x_test)\nscore = gradient.score(x_test,y_test)\nprint(score)","4095dde3":"- Import Libraries\n- Import Data\n- Data Visualization\n- Data Preprocessing\n- Train-Test Data\n- Model Building\n- Conclusion","73b8179a":"<a id=\"2\"><\/a>\n<h1 style='background:#b28bb7; border:0; color:black'><center>LOADING DATA<\/center><\/h1>","d342b749":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#b28bb7; border:0; color:black'><center>INDEX<\/center><\/h1>\n\n","ece8d973":"\n- You can see gradient boosting model performing well, as compared to other model, also if you used hyperparameter then it will provide you greater accuracy.","e478853d":"**<span style=\"color:#65634a;\"> If you liked this Notebook, please do upvote.<\/span>**\n\n**<span style=\"color:#65634a;\"> If you have any suggestions or questions, I am all ears!<\/span>**\n\n**<span style=\"color:#65634a;\">Best Wishes!<\/span>**\n","62057456":"<a id=\"6\"><\/a>\n<h1 style='background:#b28bb7; border:0; color:black'><center>MODEL BUILDING<\/center><\/h1>","7f499726":"<a id=\"4\"><\/a>\n<h1 style='background:#b28bb7; border:0; color:black'><center>DATA PREPROCESSING<\/center><\/h1>","7ce19655":"- from above visualization,\n- from 1st barplot, males are more in count who are purchasing credit card as compared to females.\n- From 2nd barplot, in the data average age of males are more than females.\n- from 3rd barplot, According to occupation person who is entrepreneur that person is having more avg account balance.\n- from 4th barplot, person who is having previsouly any type of loan that person having chance of buying credit card.\n- final bar clearly metions, there no importance whether person active or not in both the cases person will buy credit card, but data showing credit card had been buying more if person is active.","00402cfa":"<a id=\"1\"><\/a>\n<h1 style='background:#b28bb7; border:0; color:black'><center>IMPORT LIBRARIES<\/center><\/h1>","df2a2fae":"<a id=\"3\"><\/a>\n<h1 style='background:#b28bb7; border:0; color:black'><center>DATA VISUALIZATION<\/center><\/h1>","63755a43":"<a id=\"5\"><\/a>\n<h1 style='background:#b28bb7; border:0; color:black'><center>TRAIN-TEST DATA<\/center><\/h1>"}}