{"cell_type":{"3afd1994":"code","fdc74e4f":"code","2ed16809":"code","a79dd256":"code","4eb11cb1":"code","ef57174b":"code","d00cfa8b":"code","017f9286":"code","a0ded91a":"code","ced17556":"code","6b4d9578":"code","632ac76c":"code","6a84a525":"code","1de50851":"code","88263014":"code","fb4c21a1":"code","7ac3ce89":"code","d9bc30d9":"code","503f1395":"code","de424d8a":"code","eda7b7d7":"code","4a42f959":"code","79897af1":"code","7d5ac76a":"code","bce51969":"code","e4ebabc5":"code","7cb4d4b0":"code","bcbecf28":"code","896b552b":"code","f29c0930":"code","866b41be":"code","8f72aa65":"code","2a433ffd":"code","bad7be22":"code","fe8529c8":"code","737530cf":"code","89f72965":"markdown","a3d95385":"markdown","08408c51":"markdown","87d85b23":"markdown","ba0e853b":"markdown","2afbc74e":"markdown","6bdfd739":"markdown","2338245b":"markdown","f4e0d29d":"markdown","68596bf9":"markdown","86ff7cc2":"markdown","bb00a58d":"markdown","3a9e5f1c":"markdown","745dd92a":"markdown","0fb6dfed":"markdown","aa43a12b":"markdown","6c3b201e":"markdown","9cb38b12":"markdown","15afb264":"markdown","a6e90005":"markdown","516d65aa":"markdown","abaa6b48":"markdown","126000cd":"markdown","1be986eb":"markdown","c7a214de":"markdown","5cf55740":"markdown","b256c454":"markdown","c0d173f8":"markdown","c2a9b63c":"markdown","4196a762":"markdown","538cd114":"markdown","63946c04":"markdown"},"source":{"3afd1994":"import pandas as pd\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fdc74e4f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import preprocessing","2ed16809":"teste = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\")\ntreino = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\")\nnteste = teste.dropna()\nntreino = treino.dropna()","a79dd256":"ntreino[\"income\"] = ntreino[\"income\"].map({\"<=50K\": 0, \">50K\":1})\nntreino[\"sex\"] = ntreino[\"sex\"].map({\"Male\": 0, \"Female\":1})","4eb11cb1":"ntreino.head()","ef57174b":"sns.heatmap(ntreino.corr(), annot=True, vmin=-1, vmax=1)","d00cfa8b":"sns.lineplot('education.num', 'income', data=ntreino)","017f9286":"sns.lineplot('hours.per.week', 'income', data=ntreino)","a0ded91a":"sns.pairplot(ntreino, hue='income')","ced17556":"names = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']\n# Get column names first\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n# Fit your data on the scaler object\nx = ntreino.loc[:, names].values\nx = scaler.fit_transform(x)\nscaled_df = pd.DataFrame(x, columns=names)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(scaled_df)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\ndata_treino_principal = pd.concat([principalDf, ntreino[['income']]], axis = 1)\ny = ntreino.income","6b4d9578":"y","632ac76c":"sns.pairplot(data_treino_principal, hue='income')","6a84a525":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.25)","1de50851":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","88263014":"log_pred = logmodel.predict(X_test)","fb4c21a1":"log_pred","7ac3ce89":"print(\"Logistic Regression Metrics\")\nprint(confusion_matrix(y_test, log_pred))\nprint(classification_report(y_test, log_pred))\nprint('Accuracy: ',accuracy_score(y_test, log_pred))","d9bc30d9":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","503f1395":"rfc_pred = rfc.predict(X_test)","de424d8a":"print(\"Random Forest Metrics\")\nprint(confusion_matrix(y_test, rfc_pred))\nprint(classification_report(y_test, rfc_pred))\nprint('Accuracy: ',accuracy_score(y_test, rfc_pred))","eda7b7d7":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)","4a42f959":"gnb_pred = gnb.predict(X_test)","79897af1":"print(\"Gaussian Naive Bayes Metrics\")\nprint(confusion_matrix(y_test, gnb_pred))\nprint(classification_report(y_test, gnb_pred))\nprint('Accuracy: ',accuracy_score(y_test, gnb_pred))","7d5ac76a":"ci = pd.read_csv(\"..\/input\/dataci\/train.csv\")","bce51969":"ci","e4ebabc5":"x_ci = ci.drop(columns=[\"Id\",\"median_house_value\"])\ny_ci = ci[\"median_house_value\"]","7cb4d4b0":"from sklearn.model_selection import train_test_split\nx_treino, x_teste, y_treino, y_teste = train_test_split(x_ci,y_ci,test_size=0.25)","bcbecf28":"treino_si = ci.drop(columns=\"Id\")\ntab = treino_si.corr(method=\"spearman\")\ndf = pd.DataFrame(tab)\n\ndef pinta(x):\n    if abs(x) == 1:\n        color = 'black'\n    elif abs(x) >= abs(df.quantile(q=0.75).mean()):\n        color = 'indianred' \n    elif abs(x) >= abs(df.mean().mean()):\n        color = 'aqua'\n    elif abs(x) <= abs(df.mean().mean())\/6:\n        color = 'white'\n    elif abs(x) <= abs(df.mean().mean())\/3:\n        color = 'paleturquoise' \n    elif abs(x) <= abs(df.mean().mean()):\n        color = 'dodgerblue'\n    else:\n        color = 'white'\n            \n    return 'background-color: %s' % color\n    \nprint(\"Salmon : correla\u00e7\u00e3o maior que a m\u00e9dia do 3\u00ba quartil das correla\u00e7\u00f5es \")\nprint(\"Azul mais vivo, aqua : correla\u00e7\u00e3o maior que a m\u00e9dia das correla\u00e7\u00f5es \")\nprint(\"Azul mais escuro, dodgerblue : correla\u00e7\u00e3o pequena \")\nprint(\"Azul mais clara, paleturquoise : correla\u00e7\u00e3o menor que um ter\u00e7o da m\u00e9dia\")\nprint(\"Branco : correla\u00e7\u00e3o menor que um sexto da m\u00e9dia \")\ndf.style.applymap(pinta)","896b552b":"fig, ax = plt.subplots()\nax.scatter(x = ci['median_income'], y = ci['median_house_value'] , s = 1)\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('median_income', fontsize=13)\nplt.title(\"Relacionamento entre renda e valor do im\u00f3vel\")\nplt.show()\n\nx_treino[\"median_age\"].hist()\nx_teste[\"median_age\"].hist()\nplt.ylabel('frequ\u00eancia', fontsize=13)\nplt.xlabel('median_age', fontsize=13)\nplt.title(\"Histograma de idades m\u00e9dias das regi\u00f5es; laranja teste e azul treino\")\n\n\nplt.subplots()\nplt.scatter(x = ci['median_age'], y = ci['median_house_value'], s=0.5)\nplt.ylabel('median_house_value', fontsize=13)\nplt.xlabel('median_age', fontsize=13)\nplt.title(\"Relacionamento entre idade m\u00e9dia e valor do im\u00f3vel\")\nplt.show()\n\nplt.subplots()\nplt.scatter(ci[\"longitude\"],ci[\"latitude\"], c= ci[\"median_age\"] , cmap = \"jet\" , s = 5)\nplt.title(\"Idade m\u00e9dia por posi\u00e7\u00e3o geogr\u00e1fica; quanto mais quente a cor, maior a idade\")\nplt.ylabel('latitude', fontsize=13)\nplt.xlabel('longitude', fontsize=13)\nplt.xlim(-130,-110)\nplt.show()","f29c0930":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n\n\nmodel = linear_model.Ridge(alpha = 300)\nmodel.fit(x_treino, y_treino)","866b41be":"predictionTestSet = model.predict(x_teste)","8f72aa65":"from sklearn.metrics import mean_squared_error\n\nerrorTestSet = mean_squared_error(y_teste, predictionTestSet)","2a433ffd":"erro_normalizado = errorTestSet\/10000000000","bad7be22":"erro_normalizado","fe8529c8":"las = linear_model.Lasso(alpha=0.6)\nlas = las.fit(x_treino, y_treino)\npred = las.predict(x_treino)\nerro_normalizado = mean_squared_error(y_treino, pred)\/10000000000\n\nprint(\"Cross-value-score:  \",las.score(x_treino,y_treino, sample_weight=None))\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,erro_normalizado)","737530cf":"from catboost import CatBoostRegressor\n\ntom = CatBoostRegressor(learning_rate=1, depth=4,num_trees= 20, loss_function='RMSE')\nfrajola = tom.fit(x_treino, y_treino)\ngarfield = tom.predict(x_treino)\n\ngato_de_botas = mean_squared_error(y_treino, garfield)\/10000000000\n\n\nprint(\"Erro quadr\u00e1tico m\u00e9dio:  \" ,gato_de_botas)","89f72965":"A primeira an\u00e1lise que realizaremos ser\u00e1 o \"Heat Map\", ele nos ajudar\u00e1 a entender a correla\u00e7\u00e3o entre features, e assim, entenderemos quais ser\u00e3o os fatores mais importantes ao definirmos nosso \"Target\"","a3d95385":"Aparentemente a Educa\u00e7\u00e3o (education.num) relaciona-se fortemente com o Sal\u00e1rio (income), vamos fazer um gr\u00e1fico para confirmar!","08408c51":"### 3.2 Lasso","87d85b23":"Como tivemos problemas utilizando a base fornecida(o teste n\u00e3o possui o target para validar nosso modelo), iremos utilizar a mesma base, por\u00e9m divideremos ela em teste e tem treino, tomaremos 25% do nosso dataframe para testes.","ba0e853b":"Dividimos por \"10000000000\" (10**5)**2 = 10**10 ","2afbc74e":"Iremos tranformar as features income e sex em boleanos, a fim de podermos utiliz\u00e1-las na cria\u00e7\u00e3o de gr\u00e1ficos e no aprendizado.","6bdfd739":"Feita as importa\u00e7\u00f5es, devemos importar nossos dados e prepar\u00e1-la, isto \u00e9, retirar as c\u00e9lulas de dados faltantes.","2338245b":"### 4. Conclus\u00f5es\nPodemos ver que nenhum dos m\u00e9todos aplicados foram extremamente efetivos, em compara\u00e7\u00e3o com a Base Adult, no entanto, o m\u00e9todo com menor Erro foi o Catboost, com o erro quadrado m\u00e9dio de 0.30 o com maior foi o m\u00e9todo Ridge, com 0.50 de erro.","f4e0d29d":"### 3.3 Catboost","68596bf9":"### 3. Regress\u00f5es","86ff7cc2":"### 3. Aplica\u00e7\u00e3o dos m\u00e9todos de aprendizado de m\u00e1quina\nUtilizaremos 3 m\u00e9todos, Regress\u00e3o Log\u00edstica, Random Forest e Naive Bayes.","bb00a58d":"### 3.1 Ridge","3a9e5f1c":"### 1. Prepara\u00e7\u00f5es\nAntes de entrarmos de cabe\u00e7a no c\u00f3digo, precisamos preparar nosso ambiente!\nRealizaremos as importa\u00e7\u00f5es das bibliotecas necess\u00e1rias.","745dd92a":"H\u00e1 uma rela\u00e7\u00e3o mas n\u00e3o t\u00e3o clara","0fb6dfed":"Agora que analisamos nossa base, vamos ao aprendizado!","aa43a12b":"### 3.3 Naive Bayes\nAqui utilizaremos uma vers\u00e3o do Naive Bayes denominada Gaussian Naive ","6c3b201e":"### 3.2 Random Forest","9cb38b12":"### 3.1 Regress\u00e3o Logistica","15afb264":"0.80, O Naive Bayes foi o menos acurado dos 3.","a6e90005":"Agora que nossa base foi preparada, podemos prosseguir para a pr\u00f3xima etapa!","516d65aa":"Hipotese confirmada, pode-se perceber que quanto maior a edua\u00e7\u00e3o, maior o sal\u00e1rio.\nVamos analisar Horas trabalhadas x Sal\u00e1rio tamb\u00e9m.","abaa6b48":"\nPara realizar a regress\u00e3o, vamos primeiro a mais uma parte de prepara\u00e7\u00e3o da base.","126000cd":"### 4. Conclus\u00e3o\nPodemos observar que todos os m\u00e9todos de aprendizagem de m\u00e1quina utilizados obtiveram acur\u00e1cia acima de 80%, o que representa um bom resultado no geral. Na an\u00e1lise mais individualizada para cada m\u00e9todo podemos destacar a regress\u00e3o log\u00edstica, a qual obteve a maior acur\u00e1cia (+1.1% em rela\u00e7\u00e3o \u00e0 random forest e +1.17% em rela\u00e7\u00e3o ao GNB) e tamb\u00e9m apresentou o menor tempo de processamento.\n\nCom rela\u00e7\u00e3o ao tempo de processamento, destaque negativo para o RF que obteve uma acur\u00e1cia intermedi\u00e1ria, mas apresentou o maior tempo de processamento, muito maior que os outros dois m\u00e9todos testados.","1be986eb":"## Parte extra: Explorando a base CaliforniaIncome","c7a214de":"Podemos ver que total_rooms est\u00e1 fortemente ligado com o total_bedrooms, assim como household est\u00e1 ligado ao total_bedrooms, e population est\u00e1 fortemente ligada com household. Podemos basear nossos modelos utilizando nessas features!","5cf55740":"### 2. Explorando a base","b256c454":"# PMR3508-2-43\n## Autor: Ilton Andrew\n## Explorando a base Adult com outros classificadores.","c0d173f8":"0.81 utilizando o m\u00e9todo de random forest! vamos ao pr\u00f3ximo m\u00e9todo!","c2a9b63c":"### 2. Entendendo nossa base\nAntes de come\u00e7armos a criar modelos, devemos entender nossa base para saber quais m\u00e9todos mais se adequar\u00e3o","4196a762":"Obtivemos uma acur\u00e1cia de 0.82! Vamos para o pr\u00f3ximo m\u00e9todo","538cd114":"Como agora temos nosso dataframe dividido em treino e teste, vamos descobrir o que h\u00e1 nessa base e o que podemos explorar e tentar descobrirmos possiveis rela\u00e7\u00f5es!","63946c04":"### 1. Prepara\u00e7\u00f5es"}}