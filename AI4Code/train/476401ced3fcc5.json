{"cell_type":{"caaf7b90":"code","72e51d30":"code","f677e1f7":"code","779a8503":"code","534bc84a":"code","85d0f8d0":"code","0d4e9ed2":"code","3a2a47be":"code","4116521a":"code","2c8762ed":"code","ee3f9a5b":"code","7109dcf6":"code","4ac5a1e3":"code","a9656b5b":"code","1b10db8e":"code","acdc85a9":"code","3f185ebf":"code","cc372605":"code","043244fd":"code","990049a8":"code","0368fcca":"code","9b53d582":"code","90e6a92a":"code","5fe7c1b9":"code","9f5e6337":"code","354839d4":"code","f6ad296f":"code","b34cb1a2":"code","832c87ad":"code","45506db6":"code","52b3f6a7":"code","3f106605":"code","b384dd21":"code","78b22880":"code","d296a214":"code","71e76b6c":"code","5067b19d":"markdown","5960784c":"markdown","1a67a2b4":"markdown","52649a3e":"markdown","e5c9df47":"markdown","10f54f62":"markdown","b65b94c1":"markdown","27d685bc":"markdown","1eb7e63f":"markdown","05e19bc6":"markdown","13a8a03e":"markdown","89ca7274":"markdown","a9f62081":"markdown","4763f8be":"markdown","30a1ab80":"markdown","33a089a5":"markdown","97a3c576":"markdown","9af28021":"markdown","a4ffed84":"markdown","7aa72a15":"markdown","3d654ae2":"markdown","f9d622ab":"markdown","ea35e673":"markdown","9c5ec022":"markdown","a0ca7a70":"markdown","5a7aa559":"markdown","cd7f7dc4":"markdown","b3a8c46a":"markdown","cdf14bd5":"markdown"},"source":{"caaf7b90":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modelling - sklearn\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Modeling - NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import ReduceLROnPlateau\n#import tensorflow as tf\n\n# Metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport warnings\nwarnings.simplefilter('ignore')","72e51d30":"# Download training data\ntrain = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/train.csv')","f677e1f7":"# Display the first 5 rows of the training dataframe.\ntrain.head()","779a8503":"# Information for training data\ntrain.info()","534bc84a":"# Download test data\ntest = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/test.csv')","85d0f8d0":"# Display the 7 last rows of the training dataframe\ntest.tail()","0d4e9ed2":"test.info()","3a2a47be":"# Select the stations with the most data in training dataset\ntrain = train.drop(['Id','3','4','5','6','7'], axis = 1)\ntrain = train.dropna().reset_index(drop=True)\ntrain.info()","4116521a":"# Display the statistics for training data\ntrain.describe()","2c8762ed":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('target')","ee3f9a5b":"# Select the stations with the most data in test dataset\ntest = test.drop(['Id','3','4','5','6','7'], axis = 1)\ntest = test.dropna().reset_index(drop=True)","7109dcf6":"# Display basic information about the test data\ntest.info()","4ac5a1e3":"# Training data splitting to new training (part of the all training) and validation data\ntrain_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.2, random_state=0)","a9656b5b":"train","1b10db8e":"# Display information about new training data\ntrain.info()","acdc85a9":"# Display information about validation data\nvalid.info()","3f185ebf":"def acc(y_true, y_pred):\n    # Calculation accuracy of prediction\n    return r2_score(y_true, y_pred)","cc372605":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['NN Regressor', 'NN Regressor with Dropout', 'MLP Regressor'], \n                       'train_score': 0, 'train_mse': 0, 'valid_score': 0, 'valid_mse': 0})\nresult","043244fd":"batch_size_num = 8\n#batch_size_num = int(len(train)\/8)\n#batch_size_num","990049a8":"%%time\ndef build_nn():\n\n    # Initializing the NN with 3 layers including 2 hidden layers\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=4, activation='tanh', input_shape=(len(train.columns),)))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=3, activation='relu'))    \n\n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_mse', \n                                    patience=2, \n                                    verbose=1, \n                                    factor=0.8, \n                                    min_lr=0.0001)\n    return model\n\nnn_model = build_nn()\nnn_model.fit(train, target_train, batch_size=batch_size_num, epochs=600, validation_data=(valid, target_valid), verbose=0)\n\n# Drawing metrics plot\nplt.plot(nn_model.history.history['mse'])\nplt.title('Metrics of NN model')\nplt.xlabel('Epochs')\nplt.ylabel('Metrics \"Mean Square Error\"') \nplt.show()\n\n# Prediction for training data\ny_train_nn = nn_model.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_nn), 2)\nprint(f'Accuracy of NN model model training is {acc_pred}')\n\n# Save to result DataFrame\nresult.loc[result['model'] == 'NN Regressor', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'NN Regressor', 'train_mse'] = mse(target_train, y_train_nn)","0368fcca":"# NN structure and parameters\nnn_model.summary()","9b53d582":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_nn = nn_model.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_nn),2)\nresult.loc[result['model'] == 'NN Regressor', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'NN Regressor', 'valid_mse'] = round(mse(target_valid, y_val_nn),2)\nprint(f'Accuracy of NN Regressor model prediction for valid dataset is {acc_pred_valid}')","90e6a92a":"%%time\ndef build_nn2():\n\n    # Initializing the NN with 3 layers including 2 hidden layers and Dropout\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=4, activation='tanh', input_shape=(len(train.columns),)))\n\n    model.add(Dense(units=2, activation='relu'))\n    \n    # Dropout\n    model.add(Dropout(0.2))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=4, activation='relu'))    \n\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n    \n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_mse', \n                                    patience=2, \n                                    verbose=1, \n                                    factor=0.05, \n                                    min_lr=0.0001)\n    \n    return model\n\nnn_model2 = build_nn2()\nnn_model2.fit(train, target_train, batch_size=batch_size_num, epochs=800, validation_data=(valid, target_valid), verbose=0)\n\n# Drawing metrics plot\nplt.plot(nn_model2.history.history['mse'])\nplt.title('Metrics of NN Regressor with Dropout')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Square Error') \nplt.show()\n\n# Prediction for training data\ny_train_nn2 = nn_model2.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_nn2), 2)\nprint(f'Accuracy of NN Regressor with Dropout training is {acc_pred}')\n\n# Save to result DataFrame\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'train_mse'] = round(mse(target_train, y_train_nn),2)","5fe7c1b9":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_nn2 = nn_model2.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_nn2),2)\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'NN Regressor with Dropout', 'valid_mse'] = mse(target_valid, y_val_nn2)\nprint(f'Accuracy of NN Regressor with Dropout prediction for valid dataset is {acc_pred_valid}')","9f5e6337":"%%time\n# MLPRegressor\nmlp = MLPRegressor()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,12)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [300, 500,]\n              }\n\n# Training model\nmlp_CV = GridSearchCV(mlp, param_grid=param_grid, cv=5, verbose=False)\nmlp_CV.fit(train, target_train)\nprint(mlp_CV.best_params_)\n\n# Prediction for training data\ny_train_mlp = mlp_CV.predict(train)\n\n# Accuracy of model\nacc_pred = round(acc(target_train, y_train_mlp), 2)\nprint(f'Accuracy of MLP Regressor model training is {acc_pred}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'MLP Regressor', 'train_score'] = acc_pred\nresult.loc[result['model'] == 'MLP Regressor', 'train_mse'] = mse(target_train, y_train_nn)","354839d4":"# Print rounded acc_pred to 2 decimal values after the text\ny_val_mlp = mlp_CV.predict(valid)\nacc_pred_valid = round(acc(target_valid, y_val_mlp),2)\nresult.loc[result['model'] == 'MLP Regressor', 'valid_score'] = acc_pred_valid\nresult.loc[result['model'] == 'MLP Regressor', 'valid_mse'] = round(mse(target_valid, y_val_mlp),2)\nprint(f'Accuracy of MLP Regressor model prediction for valid dataset is {acc_pred_valid}')","f6ad296f":"# Prediction of target for test data for all models\ny_test_nn = nn_model.predict(test)\ny_test_nn2 = nn_model2.predict(test)\ny_test_mlp = mlp_CV.predict(test)","b34cb1a2":"def plot_prediction(target, y_nn, y_nn2, y_mlp, data_name, MAV=0.5):\n    # Building plot with target, Maximum allowable value (MAV) and \n    # prediction for the data_name (training, validation or test) data by 3 models\n    \n    x = np.arange(len(y_nn))\n    plt.figure(figsize=(16,10))\n    if target is not None:\n        plt.scatter(x, target, label = \"Target data\", color = 'g')\n    plt.scatter(x, y_nn, label = \"NN prediction\", color = 'b')\n    plt.scatter(x, y_nn2, label = \"NN with Dropout\", color = 'm')\n    plt.scatter(x, y_mlp, label = \"MLP prediction\", color = 'y')\n    plt.plot(x, np.full(len(y_nn), MAV), label = \"Maximum allowable value\", color = 'r')\n    plt.title(f'Prediction for the {data_name} data')\n    plt.legend(loc='best')\n    plt.grid(True)","832c87ad":"plot_prediction(target_train, y_train_nn, y_train_nn2, y_train_mlp, 'training')","45506db6":"plot_prediction(target_valid, y_val_nn, y_val_nn2, y_val_mlp, 'validation')","52b3f6a7":"plot_prediction(None, y_test_nn, y_test_nn2, y_test_mlp, 'test')","3f106605":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","b384dd21":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 0.1]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","78b22880":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","d296a214":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']","71e76b6c":"print(f'The best model is \"{best_model_name}\"')","5067b19d":"**ADDITIONAL TASK:** Experiment with number of hidden layers and metrics.","5960784c":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","1a67a2b4":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE & Preprocessing data](#3)\n    - [Statistics & FE](#3.1)\n1. [Modeling](#4)\n    - [NN Regressor](#4.1)\n    - [NN Regressor with Dropout](#4.2)\n    - [MLP Regressor](#4.3)    \n1. [Test prediction](#5)\n1. [Results visualization](#6)\n1. [Select the best model](#7)","52649a3e":"![image.png](attachment:image.png)\n* 1 - the source of the river (see at the station first on the left), \n* ....\n* 8 (target) - the place of water intake in Vinnytsia (see at the station in the lower right corner)","e5c9df47":"## Dataset [Ammonium prediction in river water](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)","10f54f62":"## My upgrade:\n* improved model parameters batch_size_num, learning_rate_reduction, cross validation\n* changed nn layers amount, layer units quantity, activation function, epochs number","b65b94c1":"## 5. Test prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","27d685bc":"The analysis showed that many values are only available in stations 1 and 2, while others have much less data. I propose select only these two stations.","1eb7e63f":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Datasets for river water quality prediction](https:\/\/www.kaggle.com\/vbmokin\/datasets-for-river-water-quality-prediction)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n* [The system \"MONITORING AND ENVIRONMENTAL ASSESSMENT OF WATER RESOURCES OF UKRAINE\", State Agency of Water Resources of Ukraine](http:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index)","05e19bc6":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","13a8a03e":"### TASK: Experiment with:\n* batch size\n* units in layers \n* activation functions\n* epochs number\n* patience and factor in ReduceLROnPlateau","89ca7274":"### Map of the stations:\nhttp:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index\n\n![image.png](attachment:image.png)\n\nThe upper reaches of the Pivdennyi Bug river","a9f62081":"**ADDITIONAL TASK:** Experiment with number of hidden layers and metrics.","4763f8be":"### 4.3. MLP Regressor<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","30a1ab80":"## 7. Select the best model <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","33a089a5":"[Go to Top](#0)","97a3c576":"### Example of the NN structure:\n\n![image.png](attachment:image.png)","9af28021":"### TASK: Experiment with:\n* batch size\n* units in layers \n* activation functions\n* epochs number\n* patience and factor in ReduceLROnPlateau\n* Dropout parameter","a4ffed84":"### TASK: Experiment with:\n* hidden_layer_sizes (maximum value)\n* max_iter\n* cv (cross-validation)","7aa72a15":"### 4.2. NN Regressor with Dropout<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","3d654ae2":"<a class=\"anchor\" id=\"0\"><\/a>\n# [Thanks to](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l3at-nh4-nn-models)","f9d622ab":"### Possible Tasks:\n\n* Analysis of data dependences, including EDA.\n\n* Prediction the target data (water quaity in the target station) with the highest accuracy.\n\n* Analysis of impact on the prediction accuracy in target station from the different number of stations (1, 2, ... 7).","ea35e673":"## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","9c5ec022":"### 3.3. Training data splitting<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","a0ca7a70":"### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","5a7aa559":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","cd7f7dc4":"### 4.1. NN Regressor<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","b3a8c46a":"Dataset has data of the Ammonium ions concentration in river water (the maximum permissible value in Ukraine is 0.5 mg\/cub. dm).\n\nAmmonium ions (NH4) concentration is measured in mg\/cub. dm (ie milligrams in the cubic decimeter).\n\nDatasets has data of river water quality from 8 consecutive stations of the state water monitoring system for Pivdennyi Bug river (from the source of the river to the water intake of the city of Vinnytsia).\n\nTarget is a NH4 concentration in the river crossection with the water intake of the Vinnytsia city.\n\nData for the 1997-2019.","cdf14bd5":"## 6. Visualization<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}