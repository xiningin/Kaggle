{"cell_type":{"08661c01":"code","3e9f45f1":"code","857e70d6":"code","7b2c5cfd":"code","56b19aff":"code","8abe61e0":"code","ac6d7154":"code","852ce490":"code","bd3e4ed9":"code","f332c149":"code","4d4aa6a3":"code","db399584":"code","2dd8c34e":"code","7e587aa2":"code","2b384438":"code","700d5495":"code","795b77c2":"code","99b3e006":"code","cbe0d9da":"code","538badc9":"code","6cc72c17":"code","72416c1b":"code","d9b70b6a":"code","3c31b0da":"code","3ea4c3a1":"code","dc7ee373":"code","58841c18":"code","4f7c40d3":"code","48ef624e":"code","a5c08ad3":"code","64000fed":"code","51096806":"code","7085e65f":"code","b9885aa2":"code","aeea4aec":"code","93ed790f":"code","73c1beec":"code","64b738ac":"markdown","a6bd0b89":"markdown","56053b92":"markdown","0b793397":"markdown","79e4cbf4":"markdown","578deac4":"markdown","abc3b774":"markdown","c196f841":"markdown","de7fea34":"markdown","ca9b241c":"markdown","d5cdaf46":"markdown","0b6d7bcb":"markdown","29cb72a8":"markdown","ca0543cd":"markdown","eba4175b":"markdown","9c418f2d":"markdown","12320eec":"markdown"},"source":{"08661c01":"#importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import plot_confusion_matrix","3e9f45f1":"import warnings\nwarnings.filterwarnings(\"ignore\")","857e70d6":"iris_db = load_iris()\n\ndf = pd.DataFrame(data = iris_db.data, columns = iris_db.feature_names)\ndf['species'] = iris_db.target\ndf.head()","7b2c5cfd":"df.info()","56b19aff":"df.describe()","8abe61e0":"df_features = df[['species']].copy()\ndf_features['sepal_ratio'] = df['sepal length (cm)'] \/ df['sepal width (cm)']\ndf_features['petal_ratio'] = df['petal length (cm)'] \/ df['petal width (cm)']\ndf_features['length_ratio (p:s)'] = df['petal length (cm)'] \/ df['sepal length (cm)']\ndf_features['width_ratio (p:s)'] = df['petal width (cm)'] \/ df['sepal width (cm)']\ndf_features.head()","ac6d7154":"df_features.describe()","852ce490":"#heatmap on base data\nsns.heatmap(df.corr().abs(), cmap = 'Blues', annot = True, fmt = '.2g');","bd3e4ed9":"#Let's see if we have any outliers in the base data\nfig, axs = plt.subplots(ncols=3, nrows=2, figsize=(12, 6))\nindex = 0\naxs = axs.flatten()\nfor col_name, series_value in df.items():\n    sns.boxplot(y=col_name, data=df, ax=axs[index])\n    index += 1\nplt.tight_layout()","f332c149":"#counting the outliers\nfor col_name, series_value in df.items():\n    q1 = series_value.quantile(0.25)\n    q3 = series_value.quantile(0.75)\n    inter_quartile_range = q3 - q1\n    outlier_cut = series_value[(series_value <= q1 - 1.5 * inter_quartile_range) | (series_value >= q3 + 1.5 * inter_quartile_range)]\n    percent_value = np.shape(outlier_cut)[0] \/ np.shape(df)[0] * 100\n    print(\"Outliers in column %s = %.2f%%\" % (col_name, percent_value))","4d4aa6a3":"#heatmap on feature engineered data\nsns.heatmap(df_features.corr().abs(), cmap = 'Blues', annot = True, fmt = '.2g');","db399584":"#Let's see if we have any outliers in the feature engineered data\nfig, axs = plt.subplots(ncols=3, nrows=2, figsize=(12, 6))\nindex = 0\naxs = axs.flatten()\nfor col_name, series_value in df_features.items():\n    sns.boxplot(y=col_name, data=df_features, ax=axs[index])\n    index += 1\nplt.tight_layout()","2dd8c34e":"#counting the outliers\nfor col_name, series_value in df_features.items():\n    q1 = series_value.quantile(0.25)\n    q3 = series_value.quantile(0.75)\n    inter_quartile_range = q3 - q1\n    outlier_cut = series_value[(series_value <= q1 - 1.5 * inter_quartile_range) | (series_value >= q3 + 1.5 * inter_quartile_range)]\n    percent_value = np.shape(outlier_cut)[0] \/ np.shape(df_features)[0] * 100\n    print(\"Outliers in column %s = %.2f%%\" % (col_name, percent_value))","7e587aa2":"df_combined = pd.concat([df, df_features.drop(['species'], axis = 1)], axis = 1)\ndf_combined","2b384438":"#heatmap on combined data\nsns.heatmap(df_combined.corr().abs(), cmap = 'Blues', annot = True, fmt = '.2g');","700d5495":"#Let's see if we have any outliers in the combined data\nfig, axs = plt.subplots(ncols=5, nrows=2, figsize=(12, 6))\nindex = 0\naxs = axs.flatten()\nfor col_name, series_value in df_combined.items():\n    sns.boxplot(y=col_name, data=df_combined, ax=axs[index])\n    index += 1\nplt.tight_layout()","795b77c2":"#counting the outliers\nfor col_name, series_value in df_combined.items():\n    q1 = series_value.quantile(0.25)\n    q3 = series_value.quantile(0.75)\n    inter_quartile_range = q3 - q1\n    outlier_cut = series_value[(series_value <= q1 - 1.5 * inter_quartile_range) | (series_value >= q3 + 1.5 * inter_quartile_range)]\n    percent_value = np.shape(outlier_cut)[0] \/ np.shape(df_combined)[0] * 100\n    print(\"Outliers in column %s = %.2f%%\" % (col_name, percent_value))","99b3e006":"#setting up the test train split for the base data\nX = df.drop('species', axis = 1)\ny = df['species']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 37)","cbe0d9da":"print('X_train', 'X_test', 'y_train', 'y_test')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","538badc9":"#initiating the models\nreg = LogisticRegression()\nknn = KNeighborsClassifier()\ndtree = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsv = SVC()\ngb = GradientBoostingClassifier()\nxgb = XGBClassifier(use_label_encoder = False)\nvot = VotingClassifier([('reg', reg), ('knn', knn), ('dtree', dtree), ('rf', rf), ('sv', sv), ('gb', gb), ('xgb', xgb)])","6cc72c17":"#fitting the models on the base data\nfor model in reg, knn, dtree, rf, sv, gb, xgb, vot:\n    model.fit(X_train, y_train)","72416c1b":"#running predictions on the base data\nreg_pred = reg.predict(X_test)\nknn_pred = knn.predict(X_test)\ndtree_pred = dtree.predict(X_test)\nrf_pred = rf.predict(X_test)\nsv_pred = sv.predict(X_test)\ngb_pred = gb.predict(X_test)\nxgb_pred = xgb.predict(X_test)\nvot_pred = vot.predict(X_test)","d9b70b6a":"pred_df = pd.DataFrame({'Species': y_test,\n                        'Regression': reg_pred,\n                        'KNN': knn_pred,\n                        'Decision Tree': dtree_pred,\n                        'Random Forest': rf_pred,\n                        'Supprt Vector': sv_pred,\n                        'Gradient Boosting': gb_pred,\n                        'XG Boosting': xgb_pred,\n                        'Voting': vot_pred})\npred_df","3c31b0da":"fig, ax = plt.subplots(2,4, figsize = (15, 6))\n\nplot_confusion_matrix(reg, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][0])\nax[0][0].set_title(f'Logistic (R2={reg.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(knn, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][1])\nax[0][1].set_title(f'KNN (R2={knn.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(dtree, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][2])\nax[0][2].set_title(f'Decision Tree (R2={dtree.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(rf, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][3])\nax[0][3].set_title(f'Random Forest (R2={rf.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(sv, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][0])\nax[1][0].set_title(f'Support Vector (R2={sv.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(gb, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][1])\nax[1][1].set_title(f'GBoost (R2={gb.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(xgb, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][2])\nax[1][2].set_title(f'XGBoost (R2={xgb.score(X_test, y_test)*100: .2f}%)')\n\nplot_confusion_matrix(vot, X_test, y_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][3])\nax[1][3].set_title(f'Voting (R2={vot.score(X_test, y_test)*100: .2f}%)')\n\nplt.tight_layout()","3ea4c3a1":"#setting up the test train split for the feature engineered data\nX_features = df_features.drop('species', axis = 1)\ny_features = df_features['species']\n\nX_features_train, X_features_test, y_features_train, y_features_test = train_test_split(X_features, y_features, random_state = 37)","dc7ee373":"print('X_features_train', 'X_features_test', 'y_features_train', 'y_features_test')\nprint(X_features_train.shape, X_features_test.shape, y_features_train.shape, y_features_test.shape)","58841c18":"#fitting the models on the feature engineered data\nfor model in reg, knn, dtree, rf, sv, gb, xgb, vot:\n    model.fit(X_features_train, y_features_train)","4f7c40d3":"#running predictions on the feature engineered data\nreg_pred = reg.predict(X_features_test)\nknn_pred = knn.predict(X_features_test)\ndtree_pred = dtree.predict(X_features_test)\nrf_pred = rf.predict(X_features_test)\nsv_pred = sv.predict(X_features_test)\ngb_pred = gb.predict(X_features_test)\nxgb_pred = xgb.predict(X_features_test)\nvot_pred = vot.predict(X_features_test)","48ef624e":"pred_df_features = pd.DataFrame({'Species': y_features_test,\n                                 'Regression': reg_pred,\n                                 'KNN': knn_pred,\n                                 'Decision Tree': dtree_pred,\n                                 'Random Forest': rf_pred,\n                                 'Supprt Vector': sv_pred,\n                                 'Gradient Boosting': gb_pred,\n                                 'XG Boosting': xgb_pred,\n                                 'Voting': vot_pred})\npred_df_features","a5c08ad3":"fig, ax = plt.subplots(2,4, figsize = (15, 6))\n\nplot_confusion_matrix(reg, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][0])\nax[0][0].set_title(f'Logistic (R2={reg.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(knn, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][1])\nax[0][1].set_title(f'KNN (R2={knn.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(dtree, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][2])\nax[0][2].set_title(f'Decision Tree (R2={dtree.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(rf, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][3])\nax[0][3].set_title(f'Random Forest (R2={rf.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(sv, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][0])\nax[1][0].set_title(f'Support Vector (R2={sv.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(gb, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][1])\nax[1][1].set_title(f'GBoost (R2={gb.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(xgb, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][2])\nax[1][2].set_title(f'XGBoost (R2={xgb.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplot_confusion_matrix(vot, X_features_test, y_features_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][3])\nax[1][3].set_title(f'Voting (R2={vot.score(X_features_test, y_features_test)*100: .2f}%)')\n\nplt.tight_layout()","64000fed":"#setting up the test train split for the combined data\nX_combined = df_combined.drop('species', axis = 1)\ny_combined = df_combined['species']\n\nX_combined_train, X_combined_test, y_combined_train, y_combined_test = train_test_split(X_combined, y_combined, random_state = 37)","51096806":"print('X_combined_train', 'X_combined_test', 'y_combined_train', 'y_combined_test')\nprint(X_combined_train.shape, X_combined_test.shape, y_combined_train.shape, y_combined_test.shape)","7085e65f":"reg = LogisticRegression(max_iter = 1000)","b9885aa2":"#fitting the models on the combined data\nfor model in reg, knn, dtree, rf, sv, gb, xgb, vot:\n    model.fit(X_combined_train, y_combined_train)","aeea4aec":"#running predictions on the combined data\nreg_pred = reg.predict(X_combined_test)\nknn_pred = knn.predict(X_combined_test)\ndtree_pred = dtree.predict(X_combined_test)\nrf_pred = rf.predict(X_combined_test)\nsv_pred = sv.predict(X_combined_test)\ngb_pred = gb.predict(X_combined_test)\nxgb_pred = xgb.predict(X_combined_test)\nvot_pred = vot.predict(X_combined_test)","93ed790f":"pred_df_combined = pd.DataFrame({'Species': y_combined_test,\n                                 'Regression': reg_pred,\n                                 'KNN': knn_pred,\n                                 'Decision Tree': dtree_pred,\n                                 'Random Forest': rf_pred,\n                                 'Supprt Vector': sv_pred,\n                                 'Gradient Boosting': gb_pred,\n                                 'XG Boosting': xgb_pred,\n                                 'Voting': vot_pred})\npred_df_combined","73c1beec":"fig, ax = plt.subplots(2,4, figsize = (15, 6))\n\nplot_confusion_matrix(reg, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][0])\nax[0][0].set_title(f'Logistic (R2={reg.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(knn, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][1])\nax[0][1].set_title(f'KNN (R2={knn.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(dtree, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][2])\nax[0][2].set_title(f'Decision Tree (R2={dtree.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(rf, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[0][3])\nax[0][3].set_title(f'Random Forest (R2={rf.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(sv, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][0])\nax[1][0].set_title(f'Support Vector (R2={sv.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(gb, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][1])\nax[1][1].set_title(f'GBoost (R2={gb.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(xgb, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][2])\nax[1][2].set_title(f'XGBoost (R2={xgb.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplot_confusion_matrix(vot, X_combined_test, y_combined_test, values_format= 'd', display_labels=iris_db.target_names, ax = ax[1][3])\nax[1][3].set_title(f'Voting (R2={vot.score(X_combined_test, y_combined_test)*100: .2f}%)')\n\nplt.tight_layout()","64b738ac":"# Feature Engineering","a6bd0b89":"# Exploratory Data Analysis on Feature Engineered Data","56053b92":"# Importing Libraries","0b793397":"We have some outliers in the $petal$ $ratio$ column. Let's see if the outliers are significant:","79e4cbf4":"# DataFrame Setup ","578deac4":"# Modelling on Combined Data","abc3b774":"2.67% does not appear to be significant. We will avoid the use of the Robust Scaler when working with the base data.","c196f841":"We have some outliers in the $sepal$ $width$ $(cm)$ column. Let's see if the outliers are significant:","de7fea34":"# Modelling on Feature Engineered Data","ca9b241c":"# Exploratory Data Analysis on Combined Data","d5cdaf46":"# Initial Data Assessment","0b6d7bcb":"12.67% outliers. Let's keep our options open and experiment with the scalers.","29cb72a8":"Similar state of outliers as in the individual dataframes. Let's keep our options open when considering scalers.","ca0543cd":"We have some outliers in the $petal$ $ratio$ column. Let's see if the outliers are significant:","eba4175b":"# Exploratory Data Analysis on Base Data","9c418f2d":"# Modelling on Base Data","12320eec":"# Future Development\n- Introduce scalers.\n- Tune parameters."}}