{"cell_type":{"842096d9":"code","5407633a":"code","d5b04f5a":"code","98c3e39a":"code","450e5277":"code","6a35a694":"code","f0696fe5":"code","d9014351":"code","4a6b7d94":"code","5ee04a3f":"code","2774ace0":"code","f82c3dad":"code","8b711f35":"code","1865e090":"code","855ca8ef":"code","3571d2dc":"code","aeb69d15":"code","6c05c13b":"code","7776abc4":"code","1be65c1a":"code","687fcb91":"code","209a4369":"code","77c2d30a":"code","2e187918":"code","b9fb0029":"code","7981ef00":"code","6e83b0b3":"code","3787f89e":"code","6a8f4537":"code","f650db65":"code","805f00e5":"code","8c1e22ad":"code","e6e1a75f":"code","aec161d9":"code","38cce0ab":"code","6ddcdf91":"code","34342e8c":"code","d9d105f0":"code","0100ceb9":"code","eeea40e2":"code","635ca205":"code","9ad319d4":"code","d1669db1":"code","2e7e9fa6":"code","dd95cdcb":"markdown","f7e5751b":"markdown","4f4de99a":"markdown","2076fe07":"markdown","5821f31e":"markdown","0dbd13d0":"markdown","76f003f2":"markdown","e378c381":"markdown","7041922f":"markdown","5a09d658":"markdown","8d42bcad":"markdown","6cec54bf":"markdown","e1f2dc23":"markdown","12c35369":"markdown","60f54535":"markdown","05494040":"markdown","01c35528":"markdown","7f1666e8":"markdown"},"source":{"842096d9":"import numpy as np\nimport pandas as pd \nimport os\nfor dirnames, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirnames, filename))","5407633a":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","d5b04f5a":"train_data.head(5)","98c3e39a":"train_data.info()","450e5277":"train_data.describe()","6a35a694":"# lets see percentage of missing data\nmissing = (train_data.isnull().count() - train_data.count()) \/ train_data.isnull().count() * 100\nmissing = pd.DataFrame(data = { 'original': train_data.isnull().count() , 'missing' : train_data.isnull().count() - train_data.count(), '%' : missing})\nmissing.sort_values(by= '%', ascending = False).head(3)","f0696fe5":"# Lets see Age distribution once\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain_data['Age'].hist(bins= 40, figsize= (5, 5))\nplt.show()","d9014351":"# Lets try to explore the data wrt to Age and Sex a little bit\n# but before that lets prepare some data for plotting\n\n# finding the Age distribution of passengers who survived in Male and Female\nwomen = train_data[train_data['Sex'] == 'female']\nmen = train_data[train_data['Sex'] == 'male']\n\nSurvived_women_Age_dist = women[women['Survived'] == 1].Age.dropna()\nSurvived_men_Age_dist = men[men['Survived'] == 1].Age.dropna()\n\n# finding the Age distribution of passengers who died in Male and Female\nDied_women_Age_dist = women[women['Survived'] == 0].Age.dropna()\nDied_men_Age_dist = men[men['Survived'] == 0].Age.dropna()","4a6b7d94":"# Plot the data\n\nimport seaborn as sns\nfig, axes = plt.subplots(nrows=1, ncols= 2, figsize = (10, 4))\n\nax = sns.distplot(Survived_men_Age_dist, bins = 40, label= 'Survived', ax = axes[0], kde= False)\nax = sns.distplot(Died_men_Age_dist, bins = 40, label = 'Not Survived', ax= axes[0], kde = False)\nax.legend()\nax.set_title('Male')\n\nax = sns.distplot(Survived_women_Age_dist, bins = 30, label= 'Survived', ax= axes[1], kde= False)\nax = sns.distplot(Died_women_Age_dist, bins= 30, label = 'Not Survived', ax= axes[1], kde= False)\nax.legend()\nax.set_title('Female')","5ee04a3f":"# inferences:\n# 1. men tends to have higher survival rate when they are 20-35 years of age\n# 2. females have higher survival rate alltogether than men\n# 3. females among themselves have high survival rate 14-40(approx)\n# 4. infants in both cases have higher probability of survival\n# 5. men have low survival when they are above 45 or between 5-17(approx)\n# IT LOOKS A GOOD IDEA TO CREATE A SEPERATE AGE GROUP FEATURE THUS SCALE THE DATA EVENLY.","2774ace0":"# Lets see how having number of relatives affects the survival rates, by adding it as a feature in a photocopy dataset.\ntrain_copy = train_data.copy()\ntrain_copy['relatives'] = train_data['SibSp'] + train_data['Parch']\naxes = sns.factorplot('relatives','Survived', \n                      data=train_copy,kind= 'point' , aspect = 2 )","f82c3dad":"#Lets see how many were people with 0 relatives and also check the same for 6 to see if they are significant in number and lucky as well.\ntrain_copy['relatives'].value_counts()","8b711f35":"corr =  train_copy.corr()\ncorr['Survived'].sort_values(ascending = False)","1865e090":"plot = train_data['Fare'].hist(bins = 20, label= 'Fare')\nplot.legend()","855ca8ef":"sns.factorplot('Pclass', 'Survived', data= train_data, kind=  'bar', aspect= 1.4)","3571d2dc":"sns.factorplot('Embarked', 'Survived', data= train_data, kind=  'bar', aspect= 1.4)","aeb69d15":"# Before doing any preprocessing to the data let's just keep an original copy of it separate.\ntrain_original = train_data.copy()\ntrain_data = train_data.drop(['Survived'], axis = 1)","6c05c13b":"#lets have a look at the features before proceeding.\ntrain_data.info()","7776abc4":"# Since we want to get the exact changes in our test set as well, so we will get ourself a custom transformer of our \n# own that we can call on test set as well later on. For now let's work on Training data.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CustomTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, DoTransform= True):\n        self.DoTransform = DoTransform\n    def fit(self, X, y= None):\n        return self\n    def transform(self, X, y= None):\n            if self.DoTransform == True:\n                X['AgeBucket'] = (X['Age'] \/\/ 15 * 15).astype('Int64')\n                X['relatives'] = X['SibSp'] + X['Parch']\n                X['Not_Alone'] = X['relatives'].where(X['relatives'] == 0, 1)\n                X.drop(['Age'], axis = 1, inplace= True)\n                X['Fare'] = pd.qcut(X['Fare'], 6, labels = False)\n                return X\n            else:\n                return X\n\n# CustTran = CustomTransformer(DoTransform = True) 'Name', 'Ticket', 'Cabin',\n# train_data_tr = CustTran.transform(train_data)\n# train_data_tr\n\n# Above Statements are just to check if our transformer is working fine , we will call its object in through our Pipeline.","1be65c1a":"# Lets seperately list out our numerical and categorical attributes first which we need and remove what we don't need.\ntrain_num = train_data.drop(['Sex', 'Embarked', 'Name', 'Ticket', 'Cabin'], axis = 1)\nnum_attribs = list(train_num)\ntrain_cat = train_data.drop(num_attribs, axis = 1)\ntrain_cat = train_cat.drop(['Name', 'Ticket', 'Cabin'], axis = 1)\ncat_attribs = list(train_cat)\nprint(cat_attribs)\nprint(num_attribs)\ntrain_cat","687fcb91":"# Now lets create a pipeline that will run our transformers - both for numerical and categorical attributes\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_pipeline = Pipeline([\n    ('custom_tr', CustomTransformer(DoTransform = True)),\n    ('impute', SimpleImputer(strategy= 'median')),\n    ])\n\ntrain_num_tr = num_pipeline.fit_transform(train_num)\ntrain_num_tr[0:5, :]\n\n# Above 2 lines of code are just to run the current cell to see if it's working fine. We will collectively run the whole \n# pipeline later","209a4369":"cat_pipeline = Pipeline([\n    ('Embarked', SimpleImputer(strategy= 'most_frequent')),\n    ('cat_encode', OneHotEncoder(sparse = False)),\n])\ntrain_cat_tr =  cat_pipeline.fit_transform(train_cat)\ntrain_cat_tr","77c2d30a":"# # We wil finally run both pipelines here using ColumnTransformer by passing numerical and categorical part of the data\n# whereever it's required\nfrom sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    ('num_transform', num_pipeline, num_attribs),\n    ('cat_transform', cat_pipeline, cat_attribs),\n    \n])\nfinal_train_data = full_pipeline.fit_transform(train_data)\nfinal_train_data.shape","2e187918":"# Now let's delete passengerID from the numerical data refering to its index using numpy.delete(arr, index, axis)\n# We didn't delete it in our custom transformer because we will run the same for test data as well, and we need\n# passengerID in the test set for submission to kaggle\n\nX_train = np.delete(final_train_data, 0, 1) \nprint(X_train.shape)\nX_train[:5, :]\n","b9fb0029":"# Get the labels for y_train\ny_train = train_original['Survived']","7981ef00":"# I will be fitting the data to various machine learning models to see which performs good.\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","6e83b0b3":"rfc = RandomForestClassifier(n_estimators=100, oob_score=True, random_state = 42)\nrfc.fit(X_train, y_train)\nrfc.score(X_train, y_train)","3787f89e":"dtc = DecisionTreeClassifier(random_state = 42)\ndtc.fit(X_train, y_train)\ndtc.score(X_train, y_train)","6a8f4537":"svc = LinearSVC()\nsvc.fit(X_train, y_train)\nsvc.score(X_train, y_train)","f650db65":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_train, y_train)","805f00e5":"logr = LogisticRegression()\nlogr.fit(X_train, y_train)\nlogr.score(X_train, y_train)","8c1e22ad":"from sklearn.model_selection import cross_val_score as cvs\nscores = cvs(rfc, X_train, y_train, cv = 4, scoring = 'accuracy')\nprint(\"Scores: \", scores)\nprint('Mean Score: ', scores.mean()) \nprint(\"Std Dev: \", scores.std())","e6e1a75f":"# Lets look at the feature importance now that how each feature affect our model\nfeatureImportance = rfc.feature_importances_\nfeatureImportance = pd.DataFrame({'Features' : ['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeBucket', 'Relatives', \n                                               'Not_Alone', 'Female', 'Male', 'C', 'Q', 'S'],\n                                 'Importance' : featureImportance}).sort_values(by = 'Importance', ascending = False)\nfeatureImportance","aec161d9":"featureImportance.plot.bar()","38cce0ab":"# we see that 'C' , 'Q' and 'S' does not show to have any importance to the classifier. Also seems that Not Alone feature\n# is quite useless as well. I may want to drop those features from my training set to see if predictions imporves.","6ddcdf91":"X_train = np.delete(X_train, [6, 9 ,10, 11], axis = 1)\nX_train.shape\n","34342e8c":"#testing it on our classifier again\nscores = cvs(rfc, X_train, y_train, cv = 10, scoring = 'accuracy')\nprint(\"Scores: \", scores)\nprint('Mean Score: ', scores.mean()) \nprint(\"Std Dev: \", scores.std())","d9d105f0":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV\ngs_clf = GridSearchCV(estimator=rfc, param_grid=param_grid, n_jobs=-1)\ngs_clf.fit(X_train, y_train)\ngs_clf.best_params_","0100ceb9":"final_model = gs_clf.best_estimator_","eeea40e2":"final_model.fit(X_train, y_train)\nfinal_model.score(X_train, y_train)","635ca205":"from sklearn.externals import joblib\njoblib.dump(final_model, 'final_model')","9ad319d4":"Y_test = test_data.copy()\nY_test = full_pipeline.transform(test_data)\nY_test = np.delete(Y_test, [0, 6, 9 ,10, 11], axis = 1)\nY_test.shape","d1669db1":"final_predictions = final_model.predict(Y_test)\nfinal_predictions = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': final_predictions})\njoblib.dump(final_predictions, 'final_predictions')\nfinal_submission = final_predictions.to_csv(r'final_submission.csv', index = False)\njoblib.dump(final_submission, 'final_submission')\n","2e7e9fa6":"from IPython.display import FileLink\nFileLink(r'final_submission.csv')","dd95cdcb":"*84% accuracy. Well, that's not too bad of a score to submit onto kaggle. However it is just the training data, and you can expect, if you are a biginner like me to have a little bit of data leakage into our model that could result in not the same performance on test set as we have in training set, that is also called over fitting. *\n\n**It all builts up with practice, the most time consuming and crucial part of working on Machine learning projects is EDA and Feature Engineering. And we could definately built something lot better than what we have done above by having more extensive visual understanding and analysis of our data using interesting tools and intuitive ideas, like we have alot of them on kaggle itself, contributed by various experienced kagglers. **","f7e5751b":"Let's check the score for our classifier again, this time with best parameters tweeked in.","4f4de99a":"Fare has a strong correlation with Survival rate, but it also has very uneven distribution\nI want to create Fare groups of decent size but that would just distribute most of the Fares into few of the groups and rest groups would get very small number of Fares. We would look at a way to create an even distribution(buckets\/groups) using great sklearn qcut() function after a little while. \nPassenger ID doesn't seems to be have much affect on survival.\nParch and Sibsp have both positive and negetive correlation respectively. Better did we combine them above already to get more closer picture.\nWe have already handles Age\nNow let's look at Pclass which is also giving inverse correlation with Survival rate","2076fe07":"The submission according to compititon rules should be in csv format with exactly two cols: PassengerID of test set and our corresponding predictions. So I have just concatenated the PassengerID to my predictions and exported it as a new csv file for final submission.","5821f31e":"****Predicting the Survivals of 1912 Titanic Disaster","0dbd13d0":"Now let's proceed to the final steps: Saving our model using sklearn's joblib. \nAnd then finally fitting and generating predictions for the test set.","76f003f2":"we see here passengers with 1-3 relatives had higher chances of surival but it droped down for people with more than 3(except 6)\npassengers with 0 relative had only 30% of them survived.  ","e378c381":"That's all Folks :)","7041922f":"You can generate a downloadable link of your saved file using FileLink.","5a09d658":"we tried to pack everything in our transformer that cannot be using inbuilt classes. Now lets perform some transformations that can be handled by inbuilt classes such as handling the missing data and converting the categorical ones into numerical.","8d42bcad":"Well, that looks like an improvement! Now our model has over 82% accuracy with standard deviation of 3%.","6cec54bf":"we see class 1 has a higher chances of survival and a passenger with class 3 has fairly lower survival rate. That's an already clear distinction to be left the same way. So let's move forward.","e1f2dc23":"Hyperparameter Tuning: using GridSearchCV","12c35369":"![image](https:\/\/famousoceanliners.files.wordpress.com\/2015\/07\/wpid-titanic_sinking_stu_w1.jpg)","60f54535":"Seems only handfull of people were there with 6 relatives, but a lot of people were traveling alone( 0 relatives) and out of which we see only around 30% of them survived. May be we could create a seperate feature of people traveling alone which decently affected their chances of survival. Let's keep this thing in mind for later and look at the correlation of each feature with the Survival rate using .corr() function.","05494040":"DATA PREPROCESSING: Formation of custom transformers along with a data Pipeline that would perform our custom transformation along with some inbuilt preprocessing steps. ","01c35528":"This looks pretty decent if not best, we have our best performing model a score of around over 81% with a standard deviation of 1%.","7f1666e8":"Now, We will run our pipeline on the test data: Remember, apart from the transformations done inside the pipeline we also did some transformations outside of the pipeline like deleting few Non Important features(based on their indices).\neverything shall be repeated for the test data as well in order to successfully run it through our pipeline.\nThe no. of columns in our test data before running it through our saved pipeline should be exactly same as no. of columns there were in training data while it passed through the pipeline."}}