{"cell_type":{"437b507d":"code","49d5aa82":"code","a9fa697a":"code","17286455":"code","57ce34c8":"code","581f2d83":"code","b45e9898":"markdown","074ab35c":"markdown","532ff4b3":"markdown","d7e47d49":"markdown","cf08fb1a":"markdown","3c543042":"markdown"},"source":{"437b507d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn.utils import weight_norm as WN\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader as DL\nimport torch.nn.functional as F\n\nimport os\nimport gc\nfrom time import time\n\nseed = 0\n\n\ndef breaker(num=50, char=\"*\"):\n    print(\"\\n\" + num*char + \"\\n\")\n\n\ndef head(x, no_of_ele=5):\n    print(x[:no_of_ele])\n\n    \nsc_ = StandardScaler()\nle_ = LabelEncoder()","49d5aa82":"def clean_and_build_loaders(batch_size):\n    breaker()\n    print(\"Loading Data ...\")\n    class DS(Dataset):\n        def __init__(self, X=None, y=None, mode=\"train\"):\n            self.mode = mode\n            self.X = X\n            if self.mode == \"train\" or self.mode == \"valid\":\n                self.y = y\n        \n        def __len__(self):\n            return self.X.shape[0]\n    \n        def __getitem__(self, idx):\n            if self.mode == \"train\" or self.mode == \"valid\":\n                return torch.FloatTensor(self.X[idx]), torch.LongTensor(self.y[idx])\n            else:\n                \n                return torch.FloatTensor(self.X[idx])\n    \n    tr_data = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\", engine=\"python\")\n    ts_data = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\", engine=\"python\")\n    \n    tr_data, ts_data = tr_data.drop(labels=\"id\", axis=1), ts_data.drop(labels=\"id\", axis=1)\n    \n    tr_features, labels = tr_data.iloc[:, :-1].copy().values, tr_data.iloc[:, -1].copy().values\n    ts_features = ts_data.copy().values\n    \n    tr_features = sc_.fit_transform(tr_features)\n    ts_features = sc_.transform(ts_features)\n    labels = le_.fit_transform(labels)\n    \n    IL = ts_features.shape[1]\n    OL = len(set(labels))\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(tr_features, labels, test_size=0.2, \n                                                          shuffle=True, random_state=seed, stratify=labels)\n    \n    tr_data_setup = DS(X=X_train, y=y_train.reshape(-1, 1), mode=\"train\")\n    va_data_setup = DS(X=X_valid, y=y_valid.reshape(-1, 1), mode=\"valid\")\n    ts_data_setup = DS(X=ts_features, y=None, mode=\"test\")\n    \n    dataloaders = {\n        \"train\" : DL(tr_data_setup, batch_size=batch_size, shuffle=True, generator=torch.manual_seed(seed)),\n        \"valid\" : DL(va_data_setup, batch_size=batch_size, shuffle=False),\n        \"test\"  : DL(ts_data_setup, batch_size=batch_size, shuffle=False) \n    }\n    \n    return dataloaders, IL, OL","a9fa697a":"def build_model(IL, HL, OL, use_DP=False, DP=0.5):\n    breaker()\n    print(\"Building Model ...\")\n    class Classifier(nn.Module):\n        def __init__(self, IL=None, HL=None, OL=None, use_DP=False, DP=0.5):\n\n            super(Classifier, self).__init__()\n\n            self.use_DP = use_DP\n            if self.use_DP:\n                self.DP_ = nn.Dropout(p=DP)\n\n            self.HL = HL\n\n            if len(self.HL) == 1:\n                self.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n                self.FC1 = WN(nn.Linear(in_features=IL, out_features=HL[0]))\n\n                self.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n                self.FC2 = WN(nn.Linear(in_features=HL[0], out_features=OL))\n\n            elif len(self.HL) == 2:\n                self.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n                self.FC1 = WN(nn.Linear(in_features=IL, out_features=HL[0]))\n\n                self.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n                self.FC2 = WN(nn.Linear(in_features=HL[0], out_features=HL[1]))\n\n                self.BN3 = nn.BatchNorm1d(num_features=HL[1], eps=1e-5)\n                self.FC3 = WN(nn.Linear(in_features=HL[1], out_features=OL))\n\n            elif len(self.HL) == 3:\n                self.BN1 = nn.BatchNorm1d(num_features=IL, eps=1e-5)\n                self.FC1 = WN(nn.Linear(in_features=IL, out_features=HL[0]))\n\n                self.BN2 = nn.BatchNorm1d(num_features=HL[0], eps=1e-5)\n                self.FC2 = WN(nn.Linear(in_features=HL[0], out_features=HL[1]))\n\n                self.BN3 = nn.BatchNorm1d(num_features=HL[1], eps=1e-5)\n                self.FC3 = WN(nn.Linear(in_features=HL[1], out_features=HL[2]))\n\n                self.BN4 = nn.BatchNorm1d(num_features=HL[2], eps=1e-5)\n                self.FC4 = WN(nn.Linear(in_features=HL[2], out_features=OL))\n\n        def getOptimizer(self, lr=1e-3, wd=0):\n            return optim.Adam(self.parameters(), lr=lr, weight_decay=wd)\n\n        def getPlateauLR(self, optimizer=None, patience=5, eps=1e-6):\n            return optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=patience, eps=eps, verbose=True)\n\n        def forward(self, x):\n            if not self.use_DP:\n                if len(self.HL) == 1:\n                    x = F.relu(self.FC1(self.BN1(x)))\n                    x = F.log_softmax(self.FC2(self.BN2(x)), dim=1)\n\n                    return x\n\n                elif len(self.HL) == 2:\n                    x = F.relu(self.FC1(self.BN1(x)))\n                    x = F.relu(self.FC2(self.BN2(x)))\n                    x = F.log_softmax(self.FC3(self.BN3(x)), dim=1)\n\n                    return x\n\n                elif len(self.HL) == 3:\n                    x = F.relu(self.FC1(self.BN1(x)))\n                    x = F.relu(self.FC2(self.BN2(x)))\n                    x = F.relu(self.FC3(self.BN3(x)))\n                    x = F.log_softmax(self.FC4(self.BN4(x)), dim=1)\n\n                    return x\n            else:\n                if len(self.HL) == 1:\n                    x = F.relu(self.DP_(self.FC1(self.BN1(x))))\n                    x = F.log_softmax(self.FC2(self.BN2(x)), dim=1)\n\n                    return x\n\n                elif len(self.HL) == 2:\n                    x = F.relu(self.DP_(self.FC1(self.BN1(x))))\n                    x = F.relu(self.DP_(self.FC2(self.BN2(x))))\n                    x = F.log_softmax(self.FC3(self.BN3(x)), dim=1)\n\n                    return x\n\n                elif len(self.HL) == 3:\n                    x = F.relu(self.DP_(self.FC1(self.BN1(x))))\n                    x = F.relu(self.DP_(self.FC2(self.BN2(x))))\n                    x = F.relu(self.DP_(self.FC3(self.BN3(x))))\n                    x = F.log_softmax(self.FC4(self.BN4(x)), dim=1)\n\n                    return x\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    torch.manual_seed(seed)\n    model = Classifier(IL=IL, HL=HL, OL=OL, use_DP=use_DP, DP=DP)\n    \n    return device, model","17286455":"def train(model, tr_data, va_data, epochs, lr, wd, device):\n    def fit_(model=None, optimizer=None, scheduler=None, epochs=None,\n             trainloader=None, validloader=None, criterion=nn.NLLLoss(), \n             device=None, path=None, verbose=None):\n        breaker()\n        print(\"Training ...\")\n        breaker()\n        \n        Losses, Accuracies = [], []\n        \n        bestLoss, bestAccs = {\"train\": np.inf, \"valid\" : np.inf}, {\"train\" : 0.0, \"valid\" : 0.0}\n        DLS = {\"train\" : trainloader, \"valid\" : validloader}\n        \n        model.to(device)\n        start_time = time()\n        for e in range(epochs):\n            e_st = time()\n            epochLoss = {\"train\" : 0.0, \"valid\" : 0.0}\n            epochAccs = {\"train\" : 0.0, \"valid\" : 0.0}\n            \n            for phase in [\"train\", \"valid\"]:\n                if phase == \"train\":\n                    model.train()\n                else:\n                    model.eval()\n                \n                lossPerPass, accsPerPass = [], []\n                \n                for X, y in DLS[phase]:\n                    X, y = X.to(device), y.to(device).view(-1)\n                    \n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(phase == \"train\"):\n                        output = model(X)\n                        loss = criterion(output, y)\n                        if phase == \"train\":\n                            loss.backward()\n                            optimizer.step()\n                    lossPerPass.append(loss.item())\n                    accsPerPass.append(getAccuracy(output, y))\n                epochLoss[phase] = np.mean(np.array(lossPerPass))\n                epochAccs[phase] = np.mean(np.array(accsPerPass))\n            Losses.append(epochLoss)\n            Accuracies.append(epochAccs)\n            \n            torch.save(model.state_dict(), os.path.join(path, \"Epoch_{}.pt\".format(e+1)))\n            \n            if epochLoss[\"valid\"] < bestLoss[\"valid\"]:\n                bestLoss = epochLoss\n                BLE = e + 1\n            \n            if epochAccs[\"valid\"] > bestAccs[\"valid\"]:\n                bestLoss = epochAccs\n                BAE = e + 1\n        \n            if scheduler:\n                scheduler.step(epochLoss[\"valid\"])\n            \n            if verbose:\n                print(\"Epoch: {} | Train Loss: {:.5f} | Valid Loss: {:.5f} | Train Accs: {:.5f} | Valid Accs: {:.5f} | Time: {:.2f} seconds\".format(e+1, epochLoss[\"train\"], epochLoss[\"valid\"], epochAccs[\"train\"], epochAccs[\"valid\"], time()-e_st))\n        \n        breaker()\n        print(\"Best Validation Loss at Epoch {}\".format(BLE))\n        breaker()\n        print(\"Ime Taken [{} Epochs] : {:.2f} minutes\".format(epochs, (time()-start_time)\/60))\n        breaker()\n        print(\"Training Complete\")\n        breaker()\n        \n        return Losses, Accuracies, BLE, BAE\n    \n    def getAccuracy(y_pred, y_true):\n        y_pred, y_true = torch.argmax(y_pred, dim=1).detach(), y_true.detach()\n        \n        return torch.count_nonzero(y_true == y_pred).item() \/ len(y_pred)\n    \n    \n    optimizer = model.getOptimizer(lr=lr, wd=wd)\n    path = \".\/checkpoints\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    \n    L, A, BLE, BAE = fit_(model=model, optimizer=optimizer, epochs=epochs, \n                          trainloader=tr_data, validloader=va_data, device=device,\n                          path=path, verbose=True)\n    \n    TL, VL, TA, VA = [], [], [], []\n\n    for i in range(len(L)):\n        TL.append(L[i][\"train\"])\n        VL.append(L[i][\"valid\"])\n        TA.append(A[i][\"train\"])\n        VA.append(A[i][\"valid\"])\n\n    x_Axis = np.arange(1, len(L)+1)\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(x_Axis, TL, \"r\", label=\"Training Loss\")\n    plt.plot(x_Axis, VL, \"b--\", label=\"validation Loss\")\n    plt.legend()\n    plt.grid()\n    plt.subplot(1, 2, 2)\n    plt.plot(x_Axis, TA, \"r\", label=\"Training Accuracy\")\n    plt.plot(x_Axis, VA, \"b--\", label=\"validation Accuracy\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n    return L, A, BLE, BAE","57ce34c8":"def main():\n    # Config\n    batch_size = 256\n    epochs = 5\n    lr, wd = 1e-3, 0\n    \n    dataloaders, IL, OL = clean_and_build_loaders(batch_size)\n    device, model = build_model(IL, [512], OL)\n    _, _, BLE, _ = train(model, dataloaders[\"train\"], dataloaders[\"valid\"], epochs, lr, wd, device)\n    \n    return model, device, OL, dataloaders[\"test\"], BLE\n\nmodel, device, num_classes, ts_data, BLE = main()","581f2d83":"def make_submission(model, device, dataloader, num_classes, BLE):\n    def predict_(model=None, dataloader=None, num_classes=None, device=None, path=None):\n        if path:\n            model.load_state_dict(torch.load(os.path.join(path)))\n        \n        model.to(device)\n        model.eval()\n        \n        y_pred = torch.zeros(1, num_classes).to(device)\n        \n        for X in dataloader:\n            X = X.to(device)\n            with torch.no_grad():\n                output = torch.exp(model(X))\n            y_pred = torch.cat((y_pred, output), dim=0)\n        \n        return y_pred[1:].detach().cpu().numpy()\n            \n                \n    y_pred = predict_(model, ts_data, num_classes, device, os.path.join(\".\/checkpoints\", \"Epoch_{}.pt\".format(BLE)))\n    ss = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\", engine=\"python\")\n    ss.iloc[:, 1:] = y_pred\n    ss.to_csv(\".\/submission.csv\", index=False)\n    \nmake_submission(model, device, ts_data, num_classes, BLE)","b45e9898":"# Data Processing","074ab35c":"# Model","532ff4b3":"# Submission","d7e47d49":"# Helpers","cf08fb1a":"# Library Imports","3c543042":"# Main"}}