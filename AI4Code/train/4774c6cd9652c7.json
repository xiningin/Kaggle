{"cell_type":{"6776f2e6":"code","9d892bb5":"code","3eb197bc":"code","7956ff95":"code","9e5ecb96":"code","208a8dc6":"code","272d3d3c":"code","1501777c":"code","b3245e8b":"code","5a6b38c3":"code","39cecdf5":"code","ead3213a":"code","6fcf515d":"code","cbd93821":"code","3e2845f6":"code","7c4c6eb4":"code","f0b88439":"code","add7d149":"code","9f99889f":"code","c76c15d6":"code","4a580ec2":"code","6905dfd0":"code","294096d3":"code","ca2cf1f5":"code","7bdfd7a0":"code","6bce622b":"code","30a38809":"code","fbe4d4d2":"code","de11be42":"code","c7b759e1":"code","fc3b38d1":"code","4f86e743":"code","928731e4":"code","b7f8a927":"code","ab3cf4bd":"code","49d7f2c3":"code","6e698ae5":"code","3070189e":"code","8b2866a6":"code","c1e3ad10":"code","ec32748f":"code","644e0245":"code","c40c8265":"code","2ac2d30c":"code","15e7bc7a":"code","18a482ee":"code","626c20c5":"code","677ad00d":"code","fd021491":"code","962e470f":"code","d5035de1":"code","22ac2248":"code","88d4962a":"code","baf5b49a":"code","eb047fa9":"code","3e9d74b3":"code","0d120825":"code","b1090709":"markdown","abf8b468":"markdown","5ba34321":"markdown","3b6b0f95":"markdown","f9ace321":"markdown","b0ed695c":"markdown","246e3e77":"markdown","3a8788de":"markdown","2737f959":"markdown","b876cc1a":"markdown","9319a1e9":"markdown","e132fe3c":"markdown","8d91403c":"markdown","d52520a9":"markdown","047b77bf":"markdown","1d079b1c":"markdown","3727ae3c":"markdown","6b8fc177":"markdown","2cf54203":"markdown","7c4806b7":"markdown","dd270eac":"markdown","e4c382b1":"markdown","c6783375":"markdown","d6070e16":"markdown","4621970d":"markdown","dbb5f9ca":"markdown","66179e97":"markdown","4b6d34d9":"markdown","526c3ca7":"markdown","a45ab809":"markdown","e75ce7f0":"markdown","12c874e3":"markdown","c6cc1be7":"markdown","75295f48":"markdown","b318f967":"markdown","b745121b":"markdown","83414fa5":"markdown","45cf6107":"markdown"},"source":{"6776f2e6":"# Vari\u00e1vel para indicar para qual modelo se deseja realizar grid search.\n# Por limita\u00e7\u00f5es do kernel do kaggle, fazer o grid search apenas de um modelo por vez.\n#    \"RFR\"      -> Random Forest Regressor grid search\n#    \"XGB\"      -> XGBoost Regressor grid search\n#    \"TPOT\"     -> TPOT Regressor\n#    \"MLPR\"     -> MLP Regressor grid search\n#    \"Default\"  -> Executar os modelos com os par\u00e2metros padr\u00e3o (sem grid search)\n#    \"Best\"     -> Executar os melhores modelos de acordo com grid search anteriores\ngrid_search = \"Best\"","9d892bb5":"import os\nimport warnings\nimport gc\nimport xgboost\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport datetime as dt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ShuffleSplit, train_test_split\nfrom sklearn.metrics import make_scorer, mean_absolute_error, r2_score\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.svm import LinearSVR\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","3eb197bc":"properties2016 = pd.read_csv('..\/input\/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('..\/input\/properties_2017.csv', low_memory = False)","7956ff95":"def changeToFloat32(properties):\n    for c, dtype in zip(properties.columns, properties.dtypes):\n        if dtype == np.float64:\n            properties[c] = properties[c].astype(np.float32)","9e5ecb96":"changeToFloat32(properties2016)\nchangeToFloat32(properties2017)","208a8dc6":"train2016 = pd.read_csv('..\/input\/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\ntrain2017 = pd.read_csv('..\/input\/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)","272d3d3c":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv', low_memory = False)","1501777c":"def add_date_features(df):\n    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n    df.drop([\"transactiondate\"], inplace=True, axis=1)\n    return df","b3245e8b":"dropcols = ['finishedsquarefeet12', 'finishedsquarefeet13'\\\n            ,'finishedsquarefeet15', 'finishedsquarefeet6'\\\n            ,'finishedsquarefeet50', 'fullbathcnt', 'calculatedbathnbr']\n\nproperties2016.drop(dropcols, axis=1, inplace=True)\nproperties2017.drop(dropcols, axis=1, inplace=True)","5a6b38c3":"train2016 = add_date_features(train2016)\ntrain2017 = add_date_features(train2017)","39cecdf5":"train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\ntrain2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')","ead3213a":"train2016.head()","6fcf515d":"train2017.head()","cbd93821":"# Necess\u00e1rio anular os valores das colunas de 'tax' de acordo com as regras da competi\u00e7\u00e3o\ntrain2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan","3e2845f6":"train_df = pd.concat([train2016, train2017], axis = 0)\ntest_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')","7c4c6eb4":"del properties2016, properties2017, train2016, train2017\ngc.collect();","f0b88439":"train_df.shape","add7d149":"test_df.shape","9f99889f":"def add_new_features(df):\n    df['AREA_UTIL_IMOVEL'] = df['calculatedfinishedsquarefeet'] \/ df['lotsizesquarefeet']\n    df['AREA_QUINTAL'] = df['lotsizesquarefeet'] - df['calculatedfinishedsquarefeet']\n    df['TOTAL_COMODOS'] = df['bathroomcnt'] + df['bedroomcnt']\n    \n    import datetime\n    now = datetime.datetime.now()\n    df['IDADE_IMOVEL'] = now.year - df['yearbuilt']\n    \n    df['TOTAL_IMPOSTOS'] = df['taxvaluedollarcnt'] + df['taxamount']\n    \n    #Taxa de impostos total por taxas de uma pesquisa\n    df['TAXA_IMPOSTOS'] = df['taxvaluedollarcnt'] \/ df['taxamount']\n       \n    #Quantidade de imoveis por municipio\n    county_count = df['regionidcounty'].value_counts().to_dict()\n    df['IMOVEIS_MUNICIPIO'] = df['regionidcounty'].map(county_count)\n\n    #Quantidade de imoveis por cidade\n    city_count = df['regionidcity'].value_counts().to_dict()\n    df['IMOVEIS_CIDADE'] = df['regionidcity'].map(city_count)\n    \n    df['latitude'] = df['latitude'] \/ 1e7\n    df['longitude'] = df['longitude'] \/ 1e7","c76c15d6":"add_new_features(train_df)\nadd_new_features(test_df)","4a580ec2":"def cat_to_code(df):\n    object_type = df.select_dtypes(include=['object']).columns.values\n    df[object_type] = df[object_type].astype('category')\n    for column in object_type:\n        df[column] = df[column].cat.codes","6905dfd0":"cat_to_code(train_df)\ncat_to_code(test_df)","294096d3":"test_df['transactiondate'] = pd.Timestamp('2016-12-01') \ntest_df = add_date_features(test_df)\ntest_df.fillna(-999, inplace=True)","ca2cf1f5":"train_df.dropna(thresh=0.70*len(train_df), axis=1, inplace=True)\ntrain_df.shape","7bdfd7a0":"train_df.loc[:, (train_df != train_df.iloc[0]).any()].shape","6bce622b":"train_df.dropna(inplace=True)","30a38809":"train_df.isnull().values.any()","fbe4d4d2":"train_df.shape","de11be42":"#x_train, x_test, y_train, y_test = train_test_split(train_df.drop(['logerror', 'parcelid'], axis=1), train_df.logerror, test_size=1\/3, random_state=42)#","c7b759e1":"#print(\"x_train: {}, x_test: {}, y_train: {}, y_test: {}\".format(x_train.shape, x_test.shape, y_train.shape, y_test.shape))","fc3b38d1":"def linearRegressor(X_train, Y_train, X_test, Y_test):\n  regressor = LinearRegression(fit_intercept=True)\n\n  model = regressor.fit(X_train, Y_train)\n\n  pred_LR = regressor.predict(X_test)\n  \n  resultados(Y_test, pred_LR, \"Linear Regressor\")\n\n  # Returns the trained model\n  return model","4f86e743":"def randomForestRegressor(X_train, Y_train, X_test, Y_test):\n  # Gerar conjuntos de valida\u00e7\u00e3o-cruzada para o treinamento de dados\n  cv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n\n  rfr =  RandomForestRegressor(random_state=42)\n\n  # Numero de arvores no random forest\n  n_estimators = [int(x) for x in np.linspace(start = 180, stop = 220, num = 11)]\n  # Numero de features para considerar a cada separacao\n  max_features = ['auto', 'sqrt']\n  # Profundidade maxima da arvore\n  max_depth = [int(x) for x in np.linspace(20, 30, num = 6)]\n  # Quantidade minima de amostras para se separar um no\n  min_samples_split = list(range(4,8))\n  # Quantidade minima de amostras requeridas em cada no folha\n  min_samples_leaf = list(range(1,4))\n\n  # Grid search anterior\n  #n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n  #max_features = ['auto', 'sqrt']\n  #max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n  #min_samples_split = [2, 5, 10, 15, 100]\n  #min_samples_leaf = [1, 2, 5, 10]\n\n  # Create the random grid\n  params = {'n_estimators': n_estimators,\n                 'max_features': max_features,\n                 'max_depth': max_depth,\n                 'min_samples_split': min_samples_split,\n                 'min_samples_leaf': min_samples_leaf}\n\n  #Transformar 'performance_metric' em uma fun\u00e7\u00e3o de pontua\u00e7\u00e3o utilizando 'make_scorer' \n  scoring_fnc = make_scorer(mean_absolute_error)\n\n  # Gerar o objeto de busca em matriz\n  grid = RandomizedSearchCV(estimator = rfr, param_distributions = params, scoring=scoring_fnc, \n                            cv=cv_sets, n_iter=100, verbose=2)\n\n  # Ajustar o objeto de busca em matriz com os dados para calcular o modelo \u00f3timo\n  grid = grid.fit(X_train, Y_train)\n\n  # Escolher o melhor estimador para predizer os dados de teste\n  best_rfr = grid.best_estimator_\n\n  print(\"Best Estimator: \\n{}\\n\".format(grid.best_estimator_))\n  print(\"Best Parameters: \\n{}\\n\".format(grid.best_params_))\n\n  pred_RFR = best_rfr.predict(X_test)\n  \n  resultados(Y_test, pred_RFR, \"Random Forest Regressor\")\n   \n  # Returns the best trained model\n  return best_rfr","928731e4":"def xgb(X_train, Y_train, X_test, Y_test):\n  \n  cv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n  \n  xgb = XGBRegressor()\n\n  # xgboost params\n  params = {\n              'objective':['reg:linear'],\n              'learning_rate': [.03, .033, .035, .037],\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [2, 4, 10],\n              'silent': [1],\n              'subsample': [0.6, 0.7, 0.8],\n              'colsample_bytree': [0.6, 0.7, 0.8],\n              'n_estimators': [500],\n              'base_score': [np.mean(Y_train), 0.5],\n              'reg_lambda': [0.3, 0.5, 0.8],\n              'eval_metric': ['mae', 'logloss']\n              \n  }\n\n  # Gerar o objeto de busca em matriz\n  grid = RandomizedSearchCV(estimator = xgb, param_distributions = params, scoring='neg_mean_squared_error', \n                            cv=cv_sets, n_iter=100, verbose=2)\n\n  grid = grid.fit(X_train, Y_train)\n\n  # Escolher o melhor estimador para predizer os dados de teste\n  best_xgb = grid.best_estimator_\n\n  print(\"Best Estimator: \\n{}\\n\".format(grid.best_estimator_))\n  print(\"Best Parameters: \\n{}\\n\".format(grid.best_params_))\n\n  pred_xgb = best_xgb.predict(X_test)\n  \n  resultados(Y_test, pred_xgb, \"XGBoost Regressor\")\n\n  # Returns the trained model\n  return best_xgb","b7f8a927":"def tpotRegressor(X_train, Y_train, X_test, Y_test):\n    from tpot import TPOTRegressor\n\n    tpot = TPOTRegressor(generations=10, population_size=100, scoring='neg_mean_absolute_error', verbosity=2)\n    tpot.fit(X_train, Y_train)\n    print(tpot.score(X_test, Y_test))\n    tpot.export('tpot_zillow_pipeline.py')","ab3cf4bd":"def resultados(Y_test, predictions, model_name):\n\n    #calculando o erro de uma \u00e1rvore de decis\u00e3o para regress\u00e3o:\n    mae_RFR = mean_absolute_error(predictions, Y_test)\n    print (\"Erro m\u00e9dio absoluto: {}\".format(mae_RFR))\n\n    #Acur\u00e1cia do modelo\n    #r2_RFR = r2_score(predictions, Y_test)\n    #print (\"\u00cdndice R\u00b2 (score): {}\".format(r2_RFR))\n    \n    sns.set(style=\"whitegrid\")\n    fig, axs = plt.subplots(ncols=2, sharey=False, figsize=(15,5))\n    sns.residplot(predictions, Y_test, color=\"g\", ax=axs[0]).set_title(\"Residuals plot of \" + model_name)\n    sns.scatterplot(x=Y_test, y=predictions, ax=axs[1]).set_title(\"Model Error\")\n    axs[1].set(xlabel='True Values', ylabel='Predicted Values')","49d7f2c3":"def submitPredictions(predictions, model_name):\n    y_pred=[]\n\n    for i,predict in enumerate(predictions):\n        y_pred.append(str(round(predict,4)))\n    y_pred=np.array(y_pred)\n\n    output = pd.DataFrame({'ParcelId': test_df['ParcelId'].astype(np.int32),\n            '201610': y_pred, '201611': y_pred, '201612': y_pred,\n            '201710': y_pred, '201711': y_pred, '201712': y_pred})\n\n    cols = output.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    output = output[cols]\n\n    from datetime import datetime\n\n    print( \"\\nWriting results to disk ...\" )\n    output.to_csv('sub{}_{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S'), model_name), index=False)","6e698ae5":"def submitBestModelRFR(X_train, Y_train, X_test, Y_test):\n\n    print(\"Using Best Random Forest Regressor Model...\")\n    \n    # Best model according to previous grid search\n    best_rfr = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n               max_features='auto', max_leaf_nodes=None,\n               min_impurity_decrease=0.0, min_impurity_split=None,\n               min_samples_leaf=1, min_samples_split=5,\n               min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n               oob_score=False, random_state=42, verbose=0, warm_start=False)\n    \n    # Visualizing model performance on training dataset\n    best_rfr.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_best_rfr = best_rfr.predict(X_test)\n    resultados(Y_test, predictions_best_rfr, \"Best RFR\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(best_rfr.predict(test_df[X_train.columns.values]), \"best_RFR\")","3070189e":"def submitBestModelXGB(X_train, Y_train, X_test, Y_test):\n    \n    print(\"Using Best XGBoost Regressor Model...\")\n    \n    # Best model according to previous grid search\n    best_xgb = XGBRegressor(base_score=0.014415016651286966, booster='gbtree',\n       colsample_bylevel=1, colsample_bytree=0.6, eval_metric='logloss',\n       gamma=0, importance_type='gain', learning_rate=0.03,\n       max_delta_step=0, max_depth=5, min_child_weight=10, missing=None,\n       n_estimators=500, n_jobs=1, nthread=None, objective='reg:linear',\n       random_state=0, reg_alpha=0, reg_lambda=0.3, scale_pos_weight=1,\n       seed=None, silent=1, subsample=0.8)\n\n    #Previous XGB Parameters\n#     XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#            colsample_bytree=0.7, gamma=0, importance_type='gain',\n#            learning_rate=0.03, max_delta_step=0, max_depth=5,\n#            min_child_weight=4, missing=None, n_estimators=500, n_jobs=1,\n#            nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n#            reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,\n#            subsample=0.7)\n\n    # Visualizing model performance on training dataset\n    best_xgb.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_best_xgb = best_xgb.predict(X_test)\n    resultados(Y_test, predictions_best_xgb, \"Best XGB\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(best_xgb.predict(test_df[X_train.columns.values]), \"best_XGB\")","8b2866a6":"def submitBestModelTPOT(X_train, Y_train, X_test, Y_test):\n    \n    print(\"Using Best TPOT Model (Linear SVR)...\")\n    \n    best_svr = LinearSVR(C=0.1, dual=True, epsilon=0.001, loss=\"squared_epsilon_insensitive\", tol=0.0001)\n    \n    # Visualizing model performance on training dataset\n    best_svr.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_best_svr = best_svr.predict(X_test)\n    resultados(Y_test, predictions_best_svr, \"Best SVR\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(best_svr.predict(test_df[X_train.columns.values]), \"best_SVR\")","c1e3ad10":"def defaultXGB(X_train, Y_train, X_test, Y_test):\n    print (\"Using default XGB Regressor...\")\n    \n    default_xgb = XGBRegressor()\n    \n    # Visualizing model performance on training dataset\n    default_xgb.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_default_xgb = default_xgb.predict(X_test)\n    resultados(Y_test, predictions_default_xgb, \"Default XGB\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(default_xgb.predict(test_df[X_train.columns.values]), \"default_XGB\")","ec32748f":"def defaultRFR(X_train, Y_train, X_test, Y_test):\n    print (\"Using default RFR Regressor...\")\n    \n    default_rfr = RandomForestRegressor()\n        \n    # Visualizing model performance on training dataset\n    default_rfr.fit(X_train, Y_train)\n    # Results for the training dataset\n    predictions_default_rfr = default_rfr.predict(X_test)\n    resultados(Y_test, predictions_default_rfr, \"Default RFR\")\n\n    # Testing the model on test dataframe and submiting results\n    submitPredictions(default_rfr.predict(test_df[X_train.columns.values]), \"default_RFR\")","644e0245":"train_county = train_df[train_df.propertycountylandusecode == train_df.propertycountylandusecode.value_counts().argmax()]","c40c8265":"train_county.shape","2ac2d30c":"X_train, X_test, Y_train, Y_test = train_test_split(train_county.drop(['logerror', 'parcelid'], axis=1), train_county.logerror, test_size=1\/3, random_state=42)","15e7bc7a":"print(\"x_train: {}, x_test: {}, y_train: {}, y_test: {}\".format(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape))","18a482ee":"print(\"Running Linear Regressor...\")\nmodel_lr = linearRegressor(X_train, Y_train, X_test, Y_test)\nsubmitPredictions(model_lr.predict(test_df[X_train.columns.values]), \"LR\")","626c20c5":"if (grid_search == \"Default\"):\n    defaultXGB(X_train, Y_train, X_test, Y_test) \n    defaultRFR(X_train, Y_train, X_test, Y_test)","677ad00d":"if (grid_search == \"RFR\"):\n    print(\"Running Random Forest Regressor...\")\n    model_rfr = randomForestRegressor(X_train, Y_train, X_test, Y_test)\n    submitPredictions(model_rfr.predict(test_df[X_train.columns.values]), \"RFR\")","fd021491":"if (grid_search == \"XGB\"):\n    print(\"Running XGBoost...\")\n    model_xgb = xgb(X_train, Y_train, X_test, Y_test)\n    submitPredictions(model_xgb.predict(test_df[X_train.columns.values]), \"XGB\")","962e470f":"if (grid_search == \"TPOT\"):\n    tpotRegressor(X_train, Y_train, X_test, Y_test)","d5035de1":"if (grid_search == \"Best\"):\n    submitBestModelRFR(X_train, Y_train, X_test, Y_test)\n    submitBestModelXGB(X_train, Y_train, X_test, Y_test)\n    submitBestModelTPOT(X_train, Y_train, X_test, Y_test)","22ac2248":"#Retorno do TPOT\n'''\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVR\n\n# NOTE: Make sure that the class is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH\/TO\/DATA\/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1).values\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'].values, random_state=None)\n\n# Average CV score on the training set was:-0.0678352369864653\nexported_pipeline = LinearSVR(C=0.1, dual=True, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.0001)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n'''","88d4962a":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler","baf5b49a":"def submitNeuralNetwork(X_train, X_test, Y_train, Y_test):\n    \n    print(\"Using Neural Network ...\")\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n    \n    rn_regressor = MLPRegressor(hidden_layer_sizes = (350,200,120,),early_stopping = True)\n    rn_regressor.fit(X_train_sc, Y_train)\n    \n    predictions_rn = rn_regressor.predict(X_test_sc)\n    \n    # Visualizing model performance on training dataset\n    # Results for the training dataset\n    resultados(Y_test, predictions_rn, \"Neural Network\")\n\n    # Testing the model on test dataframe and submiting results\n    #submitPredictions(rn_regressor.predict(test_df[X_train.columns.values]), \"Neural_Network\")","eb047fa9":"submitNeuralNetwork(X_train, X_test, Y_train, Y_test)","3e9d74b3":"def NeuralNetwork_gs(X_train, X_test, Y_train, Y_test):\n    \n    cv_sets = ShuffleSplit(n_splits=5, test_size = 0.25)\n    \n    print(\"Using Neural Network with Grid Search...\")\n    scaler = StandardScaler()\n    X_train_sc = scaler.fit_transform(X_train)\n    X_test_sc = scaler.transform(X_test)\n\n    rn_regressor = MLPRegressor()\n\n    # neural network params\n    params = {\n        'hidden_layer_sizes':[(150,100,),(200,150,100,),(300,200,100)],\n        'activation':['relu'],\n        'solver':['sgd','adam'],\n        'learning_rate': ['constant','adaptive'],\n        'learning_rate_init':[.001],\n        'early_stopping':[True]\n    }\n\n    # Gerar o objeto de busca em matriz\n    grid = GridSearchCV(estimator = rn_regressor, param_grid = params, scoring='neg_mean_squared_error',cv=cv_sets, verbose=2,n_jobs = -1)\n\n    grid = grid.fit(X_train_sc, Y_train)\n\n    # Escolher o melhor estimador para predizer os dados de teste\n    best_model = grid.best_estimator_\n\n    print(\"Best Estimator: \\n{}\\n\".format(grid.best_estimator_))\n    print(\"Best Parameters: \\n{}\\n\".format(grid.best_params_))\n\n    pred_rn = best_model.predict(X_test_sc)\n  \n    resultados(Y_test, pred_rn, \"MLP Regressor\")\n\n    # Returns the trained model\n    return best_model\n    ","0d120825":"if (grid_search == \"MLPR\"):\n    rn_model = NeuralNetwork_gs(X_train, X_test, Y_train, Y_test)","b1090709":"### Executar um dos grid search definidos acima de acordo com o valor definido da vari\u00e1vel","abf8b468":"# Encontrando os melhores modelos e submetendo suas predi\u00e7\u00f5es no conjunto de teste","5ba34321":"# Caso n\u00e3o se deseje executar grid search, submeter os resultados dos melhores modelos","3b6b0f95":"## Fun\u00e7\u00e3o para exibir resultados no conjunto de treinamento","f9ace321":"## Regress\u00e3o Linear foi escolhida como baseline para os demais modelos.","b0ed695c":"Os modelos foram avaliados de acordo com o score retornado pela submiss\u00e3o ao kaggle do conjunto de testes.\nApesar de o desempenho do modelo de Regress\u00e3o Linear ser bom no conjunto de treinamento, quando utilizado o conjunto de testes ele apresentou o pior desempenho. \n","246e3e77":"### Dataframe resultante","3a8788de":"## Grid Search com Redes Neurais","2737f959":"## Definindo XGBOOST","b876cc1a":"## Definindo Regressor Linear que servir\u00e1 como baseline para os demais modelos","9319a1e9":"## Compila\u00e7\u00e3o de resultados das predi\u00e7\u00f5es submetidas\n\n### XGBoost: \nBaseline:<br\/>\n    Private Score: 0.12437<br\/>\n    Public Score: 0.11853<br\/><br\/>\nGridsearch:\n1. Version 1\n    * Private Score: 0.11582\n    * Public Score: 0.10853\n2. Version 2\n    * Private Score: 0.09201\n    * Public Score: 0.08213\n\n### Random Forest:\nBaseline:<br\/>\n    Private Score: 0.15999<br\/>\n    Public Score: 0.15315<br\/><br\/>\nGridsearch:\n1. Version 1\n    * Private Score: 0.14568\n    * Public Score: 0.14060\n\n### Linear Regressor:\nBaseline:<br\/>\nPrivate Score: 1.36184<br\/>\nPublic Score: 1.11183","e132fe3c":"## Definindo TPOTRegressor\n[API refference](https:\/\/epistasislab.github.io\/tpot\/api\/), [Using TPOT](https:\/\/epistasislab.github.io\/tpot\/using\/)","8d91403c":"## Remo\u00e7\u00e3o de dados ausentes","d52520a9":"### Convertendo colunas categ\u00f3ricas para num\u00e9ricas","047b77bf":"# Carregando Datasets","1d079b1c":"## Separando train_df em conjuntos de treinamento e holdout para visualiza\u00e7\u00e3o de resultados","3727ae3c":"## Melhor XGBRegressor de acordo com o grid search","6b8fc177":"## Definindo Random Forest Regressor","2cf54203":"## Regressor usando Redes Neurais","7c4806b7":"## Removendo colunas que n\u00e3o agregam informa\u00e7\u00f5es novas","dd270eac":"# Configurando dataframes de treinamento e teste","e4c382b1":"### Fazendo o merge dos dados das propriedades com os dados de treinamento","c6783375":"## Fun\u00e7\u00e3o para submeter (para o kaggle) as predi\u00e7\u00f5es de um modelo","d6070e16":"# Definindo fun\u00e7\u00f5es para execu\u00e7\u00e3o dos modelos ","4621970d":"## Melhor TPOT Regressor de acordo com execu\u00e7\u00e3o anterior","dbb5f9ca":"# Ajustando os datasets para a execu\u00e7\u00e3o dos modelos de aprendizagem","66179e97":"### Concatenando propriedades de 2016 e 2017","4b6d34d9":"### Criando features nos conjuntos de treinamento e teste","526c3ca7":"### Remover colunas com mais de 70% de dados ausentes","a45ab809":"# An\u00e1lise de dados","e75ce7f0":"### Removendo linhas com valores NaN","12c874e3":"## Fun\u00e7\u00e3o para adicionar features envolvendo datas","c6cc1be7":"*Devido as limita\u00e7\u00f5es do kaggle (tempo de execu\u00e7\u00e3o limitado a 9h), para treinar os modelos usamos somente dados do munic\u00edpio com mais im\u00f3veis*","75295f48":"# Tratamento das bases de treinamento e teste","b318f967":"### Remover colunas com valores \u00fanicos","b745121b":"## Definindo modelos XGB e RFR com seus par\u00e2metros padr\u00e3o","83414fa5":"### Tratamento do conjunto de testes","45cf6107":"## Melhor RFR de acordo com o Gridsearch"}}