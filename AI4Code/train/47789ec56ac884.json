{"cell_type":{"55af709f":"code","00f8cacf":"code","ca748884":"code","1251a27c":"code","914f336f":"code","09eb5ccd":"code","1574f35a":"code","c3f20042":"code","b4ef38d1":"code","7a7a0051":"code","ca42b717":"code","31864aea":"markdown","4de4a199":"markdown","4810b226":"markdown","681665d6":"markdown","ccfee388":"markdown","7abef1b5":"markdown","12f37f49":"markdown","d41cebc5":"markdown","cbea5f7b":"markdown","f69e866c":"markdown","b1585daa":"markdown","47dec5d9":"markdown","8cc85525":"markdown","23f2841a":"markdown","60e5396e":"markdown","4eb3a617":"markdown","363fd22d":"markdown","b60bc397":"markdown","7d1c198d":"markdown","2436295a":"markdown","41cede6e":"markdown","2f7d003c":"markdown","f8741f17":"markdown","7cb3752a":"markdown","f25218ad":"markdown","331a2f68":"markdown","81c1e616":"markdown"},"source":{"55af709f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n# Transformation\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import power_transform\n# Models\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\n# Metrics\nfrom easymetrics import diebold_mariano_test\nfrom easymetrics import r2_all\nfrom easymetrics import evs_all\nfrom easymetrics import mae_all\nfrom easymetrics import rmse_all\nfrom easymetrics import rmsle_all","00f8cacf":"def plot_r2_mae_evs_rmse_rmsle(data):\n    X = np.arange(5)\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_axes([0,0,1,1])\n    ax.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n    ax.bar(X + 0.25, data[1], color = 'r', width = 0.25)\n    ax.bar(X + 0.50, data[2], color = 'g', width = 0.25)\n    ax.set_ylabel(\"Values\")\n    ax.set_title(\"Metrics\")\n    ax.set_xticks(X + 0.20 \/ 2)\n    ax.set_xticklabels(('R2', 'MAE', 'EVS', 'RMSE', 'RMSLE'))\n    ax.legend(labels=['Train', 'Test','Valid'])\n    return fig, ax","ca748884":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf_power = pd.read_csv('..\/input\/hydropower-generation\/Hydropower_Consumption.csv', sep = ',')\ncat_columns = df_power.select_dtypes('object').columns\ndf_power = df_power.drop(columns = cat_columns)\n\nscaler = MinMaxScaler()\n\ndf_power = power_transform(df_power, method='yeo-johnson')\ndf_power = scaler.fit_transform(df_power)\n\ndf_power = pd.DataFrame(scaler.fit_transform(df_power), \n                        columns=['2000','2001','2002','2003','2004','2005',\n                                 '2006','2007','2008','2009','2010','2011',\n                                 '2012','2013','2014','2015','2016','2017',\n                                 '2018','2019'])\n\nsns.distplot(df_power.iloc[:, 0:19])","1251a27c":"df_power.describe()","914f336f":"X = df_power.iloc[:,0:19]\ny = df_power.iloc[:,19]\n\n# (optional) plot train & test\nfig, ax=plt.subplots(1,2,figsize=(30, 6))\nsns.distplot(X, ax=ax[0])\nsns.distplot(y, ax=ax[1])","09eb5ccd":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)","1574f35a":"xgbm = XGBRegressor()\nparam_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:squarederror'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\nxgbm_gs = GridSearchCV(xgbm,\n                        param_grid,\n                        cv = 5,\n                        n_jobs = -1,\n                        verbose = 2)\n\nxgbm_gs.fit(X_train, y_train)\n\ny_pred_xgbm = xgbm_gs.predict(X_test)\n\nxgbm_best = xgbm_gs.best_estimator_\nprint(xgbm_gs.best_params_)","c3f20042":"r2_train, r2_test, r2_valid = r2_all(xgbm_best, X_train, y_train, X_test, y_test, y_pred_xgbm)\nevs_train, evs_test, evs_valid = evs_all(xgbm_best, X_train, y_train, X_test, y_test, y_pred_xgbm)\nmae_train, mae_test, mae_valid = mae_all(xgbm_best, X_train, y_train, X_test, y_test, y_pred_xgbm)\nrmse_train, rmse_test, rmse_valid = rmse_all(xgbm_best, X_train, y_train, X_test, y_test, y_pred_xgbm)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(xgbm_best, X_train, y_train, X_test, y_test, y_pred_xgbm)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","b4ef38d1":"lgbm = lgb.LGBMRegressor()\nparam_grid = {'task': ['train'],\n              'boosting_type': ['gbdt'],\n              'objective': ['regression'],\n              'metric': ['l2', 'auc'],\n              'learning_rate': [0.005],\n              'feature_fraction': [0.9],\n              'bagging_fraction': [0.7],\n              'bagging_freq': [10],\n              'verbose': [0],\n              'max_depth': [8],\n              'num_leaves': [128],  \n              'max_bin': [512],\n              #'num_iterations': [100000],\n              'n_estimators': [1000]}\n\nlgbm_gs = GridSearchCV(lgbm,\n                        param_grid,\n                        cv = 5,\n                        n_jobs = -1,\n                        verbose = 2)\n\nlgbm_gs.fit(X_train, y_train)\n\ny_pred_lgbm = lgbm_gs.predict(X_test)\n\nlgbm_best = lgbm_gs.best_estimator_\nprint(lgbm_gs.best_params_)","7a7a0051":"r2_train, r2_test, r2_valid = r2_all(lgbm_best, X_train, y_train, X_test, y_test, y_pred_lgbm)\nevs_train, evs_test, evs_valid = evs_all(lgbm_best, X_train, y_train, X_test, y_test, y_pred_lgbm)\nmae_train, mae_test, mae_valid = mae_all(lgbm_best, X_train, y_train, X_test, y_test, y_pred_lgbm)\nrmse_train, rmse_test, rmse_valid = rmse_all(lgbm_best, X_train, y_train, X_test, y_test, y_pred_lgbm)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(lgbm_best, X_train, y_train, X_test, y_test, y_pred_lgbm)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","ca42b717":"rt = diebold_mariano_test(y_test,y_pred_xgbm,y_pred_lgbm,h = 1, crit=\"MAD\")\nprint(rt)\nrt = diebold_mariano_test(y_test,y_pred_xgbm,y_pred_lgbm,h = 1, crit=\"MSE\")\nprint(rt)\nrt = diebold_mariano_test(y_test,y_pred_xgbm,y_pred_lgbm,h = 1, crit=\"poly\", power=4)\nprint(rt)","31864aea":"![](https:\/\/www.revistacobertura.com.br\/site-2017\/wp-content\/uploads\/2018\/12\/Machine-Learning.jpeg)","4de4a199":"First, the basic libraries are imported: **pandas**, **matplotlib** and **numpy** to Dataframes, Graphs and numeric operations; **MinMaxScaler** to normalize our data between 0 and 1, **train_test_split** to help split the dataset (usually 70% training\/ 30% testing), **GridSearchCV** for hyperparameter tuning, **xgboost** and **lightgbm** for our machine learning models, and the new created **easymetrics** library for performance tests (this library is avaiable here at Kaggle, in \"Utility Scripts\" Section","4810b226":"# 6. Diebold-Mariano Test","681665d6":"The **XGBoost** is an open-source software library which provides a gradient boosting framework for C++, Java, Python,R, Julia,Perl, and Scala. It works on Linux, Windows, and macOS. From the project description, it aims to provide a \"Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library\". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, and Apache Flink. It has gained much popularity and attention recently as the algorithm of choice for many winning teams of machine learning competitions.","ccfee388":"# 3. Support Functions","7abef1b5":"## 5.1. XGboost Hyperparametrization","12f37f49":"The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set. After this, metrics are measured through our library, to define model's performance.","d41cebc5":"Now, my goal was compare two models that would predict power generation for 2019, based on the previous 18 years (2000 - 2018). For this, the data set was separated into X and y, X being my prediction data, and y what I intended to predict.  The data is divided into training (70%) and testing (30%).","cbea5f7b":"# 5. Models and Hyperparametrization","f69e866c":"Functions created to help reduce the code. In this case, this one show a graph of our metrics.","b1585daa":"The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set. After this, metrics are measured through our library, to define model's performance.","47dec5d9":"Once we have the model ready and the prediction made, we can apply the metrics and analyze the results.","8cc85525":"### Table of Contents","23f2841a":"# 2. Base Libraries","60e5396e":"To measure the efficiency of our generated models, we use a bunch of metrics (maybe more than we need):\n\n* **mean_absolute_error** (MAE): a measure of errors between paired observations expressing the same phenomenon;\n* **mean_squared_error**  (root - RMSE): the standard deviation of the residuals (prediction errors);\n* **mean_squared_log_error** (root - RMSLE): that measures the ratio between actual and predicted;\n* **r2_score** (R2): coefficient of determination, the proportion of the variance in the dependent variable that is predictable from the independent variable(s); and\n* **explained_variance_score** (EVS): measures the discrepancy between a model and actual data\n* **diebold_mariano_test** (DM): compares the forecast accuracy of two forecast methods.","4eb3a617":"Suppose that the difference between the first list of prediction and the actual values is e1 and the second list of prediction and the actual value is e2. The length of time-series is T.\nThen d can be defined based on different criterion (crit).\n\n* MSE : d = (e1)^2 - (e2)^2\n* MAD : d = abs(e1) - abs(e2)\n* MAPE: d = abs((e1 - actual)\/(actual))\n* Poly: d = (e1)^power - (e2)^power\n\nThe null hypothesis is E[d] = 0.\nThe test statistics follow the student-T distribution with degree of freedom (T - 1).","363fd22d":"# 7. Conclusions","b60bc397":"The results of the Diebold-Mariano Test shows that the difference between this two forecasts is not too great, which leads us to believe that both frameworks have a similar capacity in terms of machine learning for regression.","7d1c198d":"# 8. References","2436295a":"Trying to find an implementation of the Diebold-Mariano test, I ended up finding just one code made available on GitHub (Thanks to John Tsang for that - https:\/\/github.com\/johntwk\/Diebold-Mariano-Test\/). So, I thought it could help the Kaggle community to develop a kernel that contained this metric, for anyone who needed to use it. \nHowever, to help me, I ended up creating a simple script (also available here for the community) called **easymetrics**. It contains not only the Diebold-Mariano test, but also easier ways (for me at least) to achieve the results of training, testing and validating 5 metrics: R2, Explained Variance Score, RMSE, RMSLE and MAE.\n\nIn this kernel, **XGboost** and **LightGBM** frameworks are hyperparametrized and compared using Diebold-Mariano Test. The dataset used is a energy generation compilation of several countries, measured in THh between 2000 and 2019. Its contents were extracted from World in Data.\n","41cede6e":"# 1. Introduction","2f7d003c":"## 5.2. LightGBM Hyperparametrization","f8741f17":"Initially, the data is imported from the dataset, and the categorical columns are excluded (in this case, only \"Country\"). To transform the dataset and still keep it as a Dataframe, the *scaler* library is used inside the *Dataframe* library, normalizing the data between 0 and 1, and keeping the dataframe properties. After this, the *describe* function is called, to show some important data about the dataframe, like mean, max, min, std and others.","7cb3752a":"[1] Brownlee, Jason (March 31, 2020). \"Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost\".\n\n[2] Chen, Tianqi; Guestrin, Carlos (2016). \"XGBoost: A Scalable Tree Boosting System\". In Krishnapuram, Balaji; Shah, Mohak; Smola, Alexander J.; Aggarwal, Charu C.; Shen, Dou; Rastogi, Rajeev (eds.). Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016. ACM. pp. 785\u2013794. arXiv:1603.02754. doi:10.1145\/2939672.2939785.\n\n[3] Diebold, F. X. and Mariano, R. S. (1995), Comparing predictive accuracy, Journal of business & economic statistics 13(3), 253-264.\n\n[4] Harvey, D., Leybourne, S., & Newbold, P. (1997). Testing the equality of prediction mean squared errors. International Journal of forecasting, 13(2), 281-291.\n \n[5] Kopitar, Leon; Kocbek, Primoz; Cilar, Leona; Sheikh, Aziz; Stiglic, Gregor (July 20, 2020). \"Early detection of type 2 diabetes mellitus using machine learning-based prediction models\". Scientific Reports. 10 (1): 11981. Bibcode:2020NatSR..1011981K. doi:10.1038\/s41598-020-68771-z. PMC 7371679. PMID 32686721 \u2013 via www.nature.com.\n\n","f25218ad":"**LightGBM**, short for Light Gradient Boosting Machine, is a free and open source distributed gradient boosting framework for machine learning originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.","331a2f68":"1. [Introduction](#1.-Introduction)\n\n2. [Base Libraries](#2.-Base-Libraries)\n\n3. [Support Functions](#3.-Support-Functions)\n\n4. [Data Preprocessing](#4.-Data-Preprocessing)\n\n5. [Models and Hyperparametrization](#5.-Models-and-Hyperparametrization)\n\n6. [Diebold-Mariano Test](#6.-Diebold-Mariano-Test)\n\n7. [Conclusions](#7.-Conclusions)\n\n8. [References](#8.-References)","81c1e616":"# 4. Data Preprocessing"}}