{"cell_type":{"ca77f4fb":"code","36db0bb0":"code","e25a4368":"code","cb8ecf5a":"code","e6b0034c":"code","d8679220":"code","8d73f028":"code","01c3a0f9":"code","bf67c92c":"code","4407bdf5":"code","fba27e2a":"code","8a805e18":"code","1da32054":"code","faaa83bc":"code","ae809939":"code","b7de8884":"code","af5827a4":"code","4ccab6fc":"code","7fcc4efb":"code","ebf856f8":"code","474e33db":"code","68b6f2c7":"code","3cf152f7":"code","488dc3d3":"code","a6a019b8":"code","a5a9c686":"code","401b79a5":"code","6415efb7":"code","3530858a":"code","ee1c4698":"code","ba3c91f6":"code","38cd5030":"code","ed5b1f88":"code","7f712545":"code","2d56ae59":"code","fdd9eb22":"code","c87697ca":"code","e3566762":"code","fd912299":"code","ebc0a71d":"code","6419bb45":"code","f6ea47d5":"code","8e60133c":"code","be0723e5":"code","931fa830":"code","23b1427d":"code","8272a10d":"code","c8525544":"code","c58b45df":"code","a57b051d":"code","bc7c1451":"code","0d510191":"code","25fcfdca":"code","e6c4c825":"code","0b4bd7d9":"code","27a0d6fc":"code","7fbcceb6":"code","3ec4526d":"code","578fed73":"code","e2a9ff2b":"code","d9901ba7":"code","8697fea0":"code","1bfc2b03":"code","6eaa5a44":"code","a2a4d3a7":"code","1f4f87f5":"code","ed9a2d21":"markdown","ceef534d":"markdown","3a304148":"markdown","8353da38":"markdown","e4da5551":"markdown","b735bbf9":"markdown","e483fbdb":"markdown","8f6891c3":"markdown","c5be8fba":"markdown","1dd363b5":"markdown","cd7570eb":"markdown","b96c5baf":"markdown","6cb46280":"markdown","1fa4fc34":"markdown","34a268be":"markdown","80af91cf":"markdown","adf9c557":"markdown","83c3a12c":"markdown","dc819d3e":"markdown","421b3fc1":"markdown","26d43870":"markdown","7b1f96e6":"markdown","399267f8":"markdown","1e5fe897":"markdown","1e280f3b":"markdown","38df624f":"markdown","7c4249c2":"markdown","4b5b75e1":"markdown","d2918553":"markdown","6af7514c":"markdown","671e229b":"markdown","277773e9":"markdown","b3c08f45":"markdown","57e5c892":"markdown","4a569b16":"markdown","1cd1176e":"markdown","9e2c15a3":"markdown","c3dbde56":"markdown","346b0638":"markdown","02720f32":"markdown","dfa69ad7":"markdown","cab6661e":"markdown","93df4121":"markdown","5a60e154":"markdown","68a27c6c":"markdown","71792d5d":"markdown","dbcbfa7a":"markdown"},"source":{"ca77f4fb":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","36db0bb0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e25a4368":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","cb8ecf5a":"df.head()","e6b0034c":"df.info()","d8679220":"df.isnull().sum()","8d73f028":"df = df.drop('Unnamed: 32', axis=1)","01c3a0f9":"df = df.drop('id', axis = 1)","bf67c92c":"df['diagnosis'] = df['diagnosis'].replace(['M', 'B'], [1,0])","4407bdf5":"sns.countplot('diagnosis', data = df)","fba27e2a":"df.corr()['diagnosis'].sort_values()","8a805e18":"df.corr()['diagnosis'][:-1].sort_values().plot(kind='bar')","1da32054":"plt.figure(figsize=(30,30))\nsns.heatmap(df.corr(), annot = True, cmap= \"coolwarm\")","faaa83bc":"X = df.drop('diagnosis', axis = 1).values\nY = df['diagnosis'].values","ae809939":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.25,random_state=101)","b7de8884":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","af5827a4":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","4ccab6fc":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Flatten, Conv2D, MaxPooling2D","7fcc4efb":"X_train.shape","ebf856f8":"model = Sequential()\n\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dense(units=20,activation='relu'))\nmodel.add(Dense(units=10,activation='relu'))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# For a binary classification problem\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","474e33db":"from tensorflow.keras.callbacks import EarlyStopping\ncb = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)","68b6f2c7":"model.fit(x=X_train_scaled,y=Y_train, validation_data=(X_test_scaled, Y_test), batch_size=450, epochs=600, callbacks=[cb])","3cf152f7":"losses = pd.DataFrame(model.history.history)","488dc3d3":"losses.plot()","a6a019b8":"predictions = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")","a5a9c686":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score","401b79a5":"print(confusion_matrix(Y_test,predictions))","6415efb7":"print(classification_report(Y_test,predictions))","3530858a":"print(accuracy_score(Y_test,predictions))","ee1c4698":"from sklearn.preprocessing import StandardScaler","ba3c91f6":"scaler_kkn = StandardScaler()","38cd5030":"scaled_features = scaler_kkn.fit_transform(X)","ed5b1f88":"X_train_kn, X_test_kn, Y_train_kn, Y_test_kn = train_test_split(scaled_features,Y, test_size=0.25)","7f712545":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\ndistortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\n\nfor k in range(1,30):\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n \n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                        'euclidean'), axis=1)) \/ X.shape[0])\n    inertias.append(kmeanModel.inertia_)\n \n    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n                                   'euclidean'), axis=1)) \/ X.shape[0]\n    mapping2[k] = kmeanModel.inertia_\n","2d56ae59":"plt.plot(range(1,30), distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method using Distortion')\nplt.show()","fdd9eb22":"from sklearn.neighbors import KNeighborsClassifier","c87697ca":"knn = KNeighborsClassifier(n_neighbors=8)","e3566762":"knn.fit(X_train_kn,Y_train_kn)","fd912299":"predictions = knn.predict(X_test_kn)","ebc0a71d":"print(confusion_matrix(Y_test_kn,predictions))","6419bb45":"print(classification_report(Y_test_kn,predictions))","f6ea47d5":"print(accuracy_score(Y_test_kn,predictions))","8e60133c":"X_train_dt, X_test_dt, Y_train_dt, Y_test_dt = train_test_split(X,Y, test_size=0.25)","be0723e5":"from sklearn.tree import DecisionTreeClassifier","931fa830":"decision_tree_model = DecisionTreeClassifier()","23b1427d":"decision_tree_model.fit(X_train_dt,Y_train_dt)","8272a10d":"predictions = decision_tree_model.predict(X_test_dt)","c8525544":"print(confusion_matrix(Y_test_dt,predictions))","c58b45df":"print(classification_report(Y_test_dt,predictions))","a57b051d":"print(accuracy_score(Y_test_dt,predictions))","bc7c1451":"from sklearn.ensemble import RandomForestClassifier","0d510191":"rf = RandomForestClassifier(n_estimators=100)","25fcfdca":"rf.fit(X_train_dt, Y_train_dt)","e6c4c825":"predictions = rf.predict(X_test_dt)","0b4bd7d9":"print(confusion_matrix(Y_test_dt,predictions))","27a0d6fc":"print(classification_report(Y_test_dt,predictions))","7fbcceb6":"print(accuracy_score(Y_test_dt,predictions))","3ec4526d":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","578fed73":"parameters = [{'kernel': ['rbf'], 'gamma': [ 1e-3, 1e-2, 1e-2, 1e0, 1e1, 1e2],\n                     'C': [0.001,0.01, 0.1, 1, 10, 100, 1000, 10000,100000]}]","e2a9ff2b":"scores = ['precision', 'recall']\nfor score in scores:\n \n    model_svm = GridSearchCV(SVC(),parameters, cv= 3, scoring='%s_macro' % score)\n    model_svm.fit(X_train, Y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print('Gamma:',model_svm.best_estimator_.gamma)\n    print('C:',model_svm.best_estimator_.C)","d9901ba7":"model_svm_best = SVC(max_iter = 1000000, kernel = 'rbf', gamma =model_svm.best_estimator_.gamma, C =model_svm.best_estimator_.C)\n","8697fea0":"model_svm_best.fit(X_train, Y_train)","1bfc2b03":"predictions = model_svm_best.predict(X_test)","6eaa5a44":"print(confusion_matrix(Y_test,predictions))","a2a4d3a7":"print(classification_report(Y_test,predictions))","1f4f87f5":"print(accuracy_score(Y_test,predictions))","ed9a2d21":"### Scaling Data","ceef534d":"Let's now divide our dataset in two parts. The X with the features and the Y with the lable 'diagnosis'.","3a304148":"### Creating The Model","8353da38":"To avoid overfiting create a earlystop criteria!","e4da5551":"##### Correlation Between The Features","b735bbf9":"Now we preform a cicle to train and evaluate our model evaluating the precision and recall with each combination of C and Gamma.","e483fbdb":"------> Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 \/ area - 1.0)\n\ng) concavity (severity of concave portions of the contour)\n\nh) concave points (number of concave portions of the contour)\n\ni) symmetry\n\nj) fractal dimension (\"coastline approximation\" - 1)\n\n------>The mean, standard error and \"worst\" or largest (mean of the three\n        largest values) of these features were computed for each image,\n        resulting in 30 features. For instance, field 3 is Mean Radius, field\n        13 is Radius SE, field 23 is Worst Radius.\n\n------>All feature values are recoded with four significant digits.\n\n------>Missing attribute values: none\n\n------>Class distribution: 357 benign, 212 malignant","8f6891c3":"### Train and Predict","c5be8fba":"# <font color='green'><b> KNeighbors Classifier <\/b><\/font>","1dd363b5":"### Tunning the best parameters for C and Gamma","cd7570eb":"### Train and Predicting","b96c5baf":"Any variables that are on a large scale will have a much larger effect on the distance between the observations than variables that are on a small scale. So we have to standarize the variables!","6cb46280":"Knowing the id of the patient will be irrelevant for our model and may cause confusion, so we drop it.","1fa4fc34":"##### 3) 'diagnosis' feature.","34a268be":"### Creating The Model","80af91cf":"### Creating The Model","adf9c557":"We start to create a dictionary with the most commun values:","83c3a12c":"### Evaluating The Model","dc819d3e":"### Creating the Model","421b3fc1":"Lets scaling the data to avoid problem in training our Neural Network. Reminder: fit_transform -> X_train ; tranform -> X_test","26d43870":"### Evaluating The Model","7b1f96e6":"In oposition to the Logistic Regression, we need to split the data into training data and test data, using the standadize data.\n\nNote: To take a better conclusion from the analyse of the models we will keep the test_size constant","399267f8":"### Evaluating The Model","1e5fe897":"# <font color='green'><b> Random Forest <\/b><\/font>","1e280f3b":"### Train Test Split","38df624f":"# <font color='green'><b> NEURAL NETWORKS <\/b> (Multi-Layer Perceptron)<\/font>","7c4249c2":"### Engineering Data Analysis","4b5b75e1":"As we can see only the 'Unnamed: 32' feature has all values filled with null. So this columns can be droped!","d2918553":"### Train Test Split","6af7514c":"##### 2) 'id' feature.","671e229b":"After we train our model analyze the loss and validation loss in order to find out if any change need to be done in the call back to make the model better.","277773e9":"# Breast Cancer Wisconsin (Diagnostic) DataSet","b3c08f45":"### Standardize the Features","57e5c892":"From this model we can conclude that the best K value is around 8, where we see the elbow. So lets train our model with that number of clusters.","4a569b16":"### Train Test Split","1cd1176e":"### Train and Predicting","9e2c15a3":"### Import the DataSet and Vizualize the Information","c3dbde56":"### Train and Predicting","346b0638":"### Creating The Model","02720f32":"### Model Evaluation","dfa69ad7":"# <font color='green'><b> Support Vector Machine <\/b><\/font>","cab6661e":"### Training The Model","93df4121":"### Choosing a K Value","5a60e154":"Note: this code is called the \"Elbow Method\" used to choose the best K value.\n\nTaken from: https:\/\/www.geeksforgeeks.org\/elbow-method-for-optimal-value-of-k-in-kmeans\/","68a27c6c":"##### 1) 'Unnamed: 32' feature.","71792d5d":"As we can see this feature is a String Object so we need to convert it to a binary classification. By default M will be assign to 1 and B will be assign to 0.","dbcbfa7a":"# <font color='green'><b> Decision Trees <\/b><\/font>"}}