{"cell_type":{"b97fc634":"code","8df5b3e5":"code","2ac6f09e":"code","a1cd3ef0":"code","0bb5c85d":"code","cc0325a7":"code","c8f8f250":"code","467ccd18":"code","01172af4":"code","60e569c1":"code","abd2f41b":"code","4a8f4a3e":"code","99cc6e49":"code","489e61c0":"code","49c177c1":"code","c8469617":"code","fd5b57bd":"code","eec0e741":"code","5bc1bd96":"code","f087b9f7":"code","8311491b":"code","0d7556dd":"code","3fc19eb0":"code","9f3be063":"code","2b4c8f56":"code","556725c9":"code","3e73c7b4":"code","faa32713":"code","c3d6eb50":"code","ae99e986":"code","cd132d10":"code","180c9e2f":"code","35422bfb":"code","477bfab1":"code","7cb81cc4":"code","43cca3f0":"code","79497057":"code","8810c57d":"code","7bfbee50":"code","6da0f4eb":"code","4ee87995":"code","1c5ff0df":"code","b765d8c3":"code","64586d55":"code","62f1e413":"code","c9bc5ba5":"code","1b95f1bc":"code","4cd40bbc":"code","17631aa0":"code","1ae579e7":"code","4f94e48b":"code","ce3f4913":"code","44efbd12":"code","364de0e7":"code","0860e335":"code","30e3fdf2":"code","c1acf3f7":"code","fae73fab":"code","8f6c1436":"code","5a1875dc":"code","5e68b53d":"code","17411d13":"code","c51fdb65":"code","7ceb9569":"code","9912ebb6":"code","630c8e53":"code","e89cac06":"code","a45ddfd1":"code","b265a8a9":"code","e4db0d7e":"code","b6493e51":"code","30176bca":"code","05203373":"code","b8571b52":"code","caae529a":"code","5f645e8c":"code","0ea4621d":"code","01bc7d43":"code","b387ef16":"code","d4d5e6a8":"code","23f524aa":"code","78503d81":"code","9386900f":"code","1e77af06":"code","d68009c5":"code","2495b9e4":"code","d3df82d2":"code","739d8880":"code","ff003bd1":"code","1e133080":"code","d0c7b193":"code","3cf9aa5d":"code","278083c3":"code","3d544a32":"code","7ee27574":"code","37aadbcd":"code","910d2753":"code","65123375":"code","38f4990b":"code","1e317c3a":"code","c44af3bd":"code","63faf37a":"code","63ff7a16":"code","eaff84c2":"code","f0cad8b6":"code","eae45b1e":"code","8c0492dd":"code","d969e1ba":"code","66b120f0":"code","81ba5437":"code","9f43e460":"code","c46ce9a6":"code","d340294d":"code","af1f4280":"code","f5e0bb04":"code","8747b58c":"code","c56e2211":"code","82c0e8f0":"code","16be5e39":"code","a0e1a4b0":"code","078e3e1b":"code","813101f6":"code","d0906c15":"code","924b438f":"code","94dfb657":"code","aadefae4":"code","d00f264b":"code","0aa36781":"code","1b1f9557":"code","24f9606b":"code","8a0fd84a":"code","09c43601":"code","ca5f5036":"markdown","b168beb7":"markdown","18471b2d":"markdown","20a48d76":"markdown","8a6ec568":"markdown","c13403e9":"markdown","30c6f1f3":"markdown","d66d5b86":"markdown","4b39c48c":"markdown","9da31366":"markdown","69cbfde9":"markdown","89bed463":"markdown","9ac5fd10":"markdown","4aefeee2":"markdown","ebc422e5":"markdown","1e81bdc1":"markdown","08a821af":"markdown","f06e677a":"markdown","8b8eed2a":"markdown","80607270":"markdown","edcce18c":"markdown","4dc75a9a":"markdown","b1f1577a":"markdown","fcca0034":"markdown","e5ffe33e":"markdown","ba2340d0":"markdown","3bb538d6":"markdown","a48f8a82":"markdown","9733fe2b":"markdown","a4818219":"markdown","aba7bbe0":"markdown","f3f48e3b":"markdown","73441f83":"markdown","58920ebb":"markdown","4f77d1de":"markdown","99588bbe":"markdown","1e81ea11":"markdown","2a5ee29e":"markdown","b719e71f":"markdown","95e9e9ef":"markdown","3eef92c1":"markdown","254b832a":"markdown","d558f121":"markdown","952747fe":"markdown","8e4d87e4":"markdown"},"source":{"b97fc634":"#loading in necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","8df5b3e5":"#loading data sets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2ac6f09e":"#printing dimensions of data sets\nprint (\"Dimensions of training data:\", train.shape)\nprint (\"Dimensions of testing data:\", test.shape)","a1cd3ef0":"#preview of training set\ntrain.head()","0bb5c85d":"#printing all integer type variables in the training set\nprint(train.dtypes[train.dtypes=='int64'])","cc0325a7":"#printing all float type variables in the training set\nprint(train.dtypes[train.dtypes=='float64'])","c8f8f250":"#printing all string types variables in training set\nprint(train.dtypes[train.dtypes=='object'])","467ccd18":"#printing preview of testing set\ntest.head()","01172af4":"#printing all integer type variables in the testing set\nprint(test.dtypes[test.dtypes=='int64'])","60e569c1":"#printing all float type variables in the testing set\nprint(test.dtypes[test.dtypes=='float64'])","abd2f41b":"#printing all string type variables in the testing set\nprint(test.dtypes[test.dtypes=='object'])","4a8f4a3e":"#Checking to see if Null values present in training set\n\n#Displaying ratio of missing variables to total number of rows in training data\ntraining_data_missing_values_ratio = np.round(train.isnull().sum().loc[train.isnull().sum()>0,]\/(len(train)) * 100.0,1)\ntraining_data_missing_values_ratio = training_data_missing_values_ratio.reset_index()\ntraining_data_missing_values_ratio.columns = ['column', 'ratio']\ntraining_data_missing_values_ratio.sort_values(by=['ratio'], ascending=False)","99cc6e49":"#Checking to see if Null values present in testing set\n\n#Displaying ratio of missing variables to total number of rows in testing data\ntesting_data_missing_values_ratio = np.round(test.isnull().sum().loc[test.isnull().sum()>0,]\/(len(test)) * 100.0,1)\ntesting_data_missing_values_ratio = testing_data_missing_values_ratio.reset_index()\ntesting_data_missing_values_ratio.columns = ['column', 'ratio']\ntesting_data_missing_values_ratio.sort_values(by=['ratio'], ascending=False)","489e61c0":"#Displaying a few plots with Sale Price\nf, ax = plt.subplots(figsize=(8, 4))\nsns.regplot(x=train.GrLivArea, y=train.SalePrice)\nplt.title(\"GrLivArea vs SalePrice\")","49c177c1":"#Noticed a few outliers, so I will be dropping them from our data set\ntrain = train.drop(train.loc[(train.GrLivArea>4000) & (train.SalePrice < 200000),].index)\n\nf, ax = plt.subplots(figsize=(8, 4))\nsns.regplot(x=train.GrLivArea, y=train.SalePrice)\nplt.title(\"GrLivArea vs SalePrice\")","c8469617":"#PLotting distribution of Sale Price\nf, ax = plt.subplots(figsize=(10, 6))\nsns.distplot(train.SalePrice)","fd5b57bd":"#Heavily right skewed\nfrom scipy import stats\nf, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(train.SalePrice, plot=plt)","eec0e741":"#kurtosis validates that the distribution is not normal\nfrom scipy.stats import kurtosis\nkurtosis(train.SalePrice)","5bc1bd96":"#Take the logarithm of Sale Price to get a more normal distribution\nf, ax = plt.subplots(figsize=(10, 6))\nsns.distplot(np.log(train.SalePrice))","f087b9f7":"#QQ-Plot appears more normal\nf, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(np.log(train.SalePrice), plot=plt)","8311491b":"#kurtosis is closer to 0, it appears more normal\nkurtosis(np.log(train.SalePrice))","0d7556dd":"#Getting all variables that have numerical values\nnumeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.columns","3fc19eb0":"#Getting all variables that have string values\ncategorical_features = train.select_dtypes(include=[np.object])\ncategorical_features.columns","9f3be063":"#Examining the correlation between all numerical values with Sale Price\nnumeric_features.corr()['SalePrice'].sort_values(ascending = False)","2b4c8f56":"#Correlation Matrix\ncorr = train.iloc[:,1:].corr()\nf, ax = plt.subplots(figsize=(20, 8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n        cmap = \"YlGnBu\")\nplt.title(\"Correlation Matrix\")","556725c9":"#Looking at highly correlated variables \nhigh_corr_cols = numeric_features.corr()['SalePrice'].sort_values(ascending = False)[numeric_features.corr()[\n    'SalePrice'].sort_values(ascending = False)>0.5].index.tolist()\nhigh_corr_cols","3e73c7b4":"#Heat map of highly correlated variables with Sale Price\ncorr = train[high_corr_cols].corr()\nf, ax = plt.subplots(figsize=(20, 8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            annot=True,\n        cmap = \"YlGnBu\")\nplt.title(\"Correlation Matrix\")","faa32713":"#Imputing missing values in categorical variables for plotting purposes\nfor cols in categorical_features:\n    train[cols] = train[cols].astype('category')\n    if train[cols].isnull().any():\n        train[cols] = train[cols].cat.add_categories(['MISSING'])\n        train[cols] = train[cols].fillna('MISSING')\n\n#Mass plotting of categorical variables to see if relationships with Sale Price exist\ndef mass_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(mass_boxplot, \"value\", \"SalePrice\")","c3d6eb50":"train2 = train\n#some numerical values that are actually categorical values\ncols_to_conv_to_categorical = [\"MSSubClass\", \"OverallQual\", \"OverallCond\", \"MoSold\", \"YrSold\"]\nfor cols in cols_to_conv_to_categorical:\n    train2[cols] = train2[cols].apply(str)\n\n#plotting many boxplots to see relationship with Sale Price\ndef mass_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\nf = pd.melt(train2, id_vars=['SalePrice'], value_vars=cols_to_conv_to_categorical)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(mass_boxplot, \"value\", \"SalePrice\")","ae99e986":"#plotting more categorical variables with Sale Price\nfor cols in [\"YearRemodAdd\", \"YearBuilt\", \"GarageYrBlt\"]: \n    data = pd.concat([train2['SalePrice'], train2[cols]], axis=1)\n    f, ax = plt.subplots(figsize=(40, 20))\n    fig = sns.boxplot(x=cols, y=\"SalePrice\", data=data)\n    plt.xticks(rotation=90, fontsize = 20)\n    title_name = \"SalePrice Across \" + cols\n    plt.title(str(title_name), fontsize = 25)","cd132d10":"#Taking a closer look at the distribution of Sale Price across OverAllQual\ndata = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\ndata[\"OverallQual\"] = data[\"OverallQual\"].apply(int)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.title(\"SalePrice vs. OverAllQual\")","180c9e2f":"#getting the order of neighborhood based by median sale price\nneighborhood_order = train[['Neighborhood','SalePrice']].groupby([\n    'Neighborhood']).describe()['SalePrice']['50%'].sort_values(ascending=True).index","35422bfb":"#Plotting Sale Price across Neighborhoods\ndata = pd.concat([train['SalePrice'], train['Neighborhood']], axis=1)\nf, ax = plt.subplots(figsize=(20, 12))\nfig = sns.boxplot(x='Neighborhood', y=\"SalePrice\", data=data, order = neighborhood_order)\nfig.axis(ymin=0, ymax=800000);\nplt.title(\"SalePrice Across Neighborhoods\")","477bfab1":"train_feature_check = pd.read_csv('..\/input\/train.csv')\ntrain_feature_check = train_feature_check.drop(train_feature_check.loc[(train_feature_check.GrLivArea>4000) & (train_feature_check.SalePrice < 200000),].index)\nprint(\"Original dimension:\", train_feature_check.shape)","7cb81cc4":"def conv_to_numeric(df, column_list, mapper):\n    for cols in column_list:\n        df[str(\"o\")+cols] = df[cols].map(mapper)\n        df[str(\"o\")+cols].fillna(0, inplace = True)         ","43cca3f0":"#remapping categorical variables with numerical values\nconvert_col_1 = [\"ExterQual\", \"ExterCond\",\"BsmtQual\", \"BsmtCond\", \"HeatingQC\",\"KitchenQual\",\"FireplaceQu\",\n                  \"GarageQual\",\"GarageCond\",\"PoolQC\"]\nmapper = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None':0}\n\nconv_to_numeric(train_feature_check, convert_col_1, mapper)\n\nconvert_col_2 = [\"BsmtExposure\"]\nmapper = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None':0}\nconv_to_numeric(train_feature_check, convert_col_2, mapper)\n\nconvert_col_3 = [\"BsmtFinType1\", \"BsmtFinType2\"]\nmapper = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1 ,'None':0}\nconv_to_numeric(train_feature_check, convert_col_3, mapper)\n\nconvert_col_4 = [\"GarageFinish\"]\nmapper = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'None':0}\nconv_to_numeric(train_feature_check, convert_col_4, mapper)\n\nconvert_col_5 = [\"Fence\"]\nmapper = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'None':0}\nconv_to_numeric(train_feature_check, convert_col_5, mapper)","79497057":"convert_col = convert_col_1 + convert_col_2 + convert_col_3 + convert_col_4 + convert_col_5\nnew_features_col=[]\n\nfor cols in convert_col:\n    cols = str(\"o\")+cols\n    new_features_col.append(cols)","8810c57d":"#plotting variables\nfor cols in new_features_col:\n    data = pd.concat([train_feature_check['SalePrice'], train_feature_check[cols]], axis=1)\n    f, ax = plt.subplots(figsize=(12, 8))\n    fig = sns.boxplot(x=cols, y=\"SalePrice\", data=data)\n    fig.axis(ymin=0, ymax=800000)","7bfbee50":"#created new variable TotalSqFt \ntrain_feature_check[\"TotalSqFt\"] = train_feature_check[\"TotalBsmtSF\"] + train_feature_check[\"1stFlrSF\"] + train_feature_check[\"2ndFlrSF\"]\nnew_features_col.append(\"TotalSqFt\")\nnew_features_col.append(\"SalePrice\")","6da0f4eb":"print(\"Training dimension:\", train.shape)\nprint(\"Training with new features dimension:\", train_feature_check.shape)","4ee87995":"#correlation matrix with new features\ncorr = train_feature_check[new_features_col].corr()\nf, ax = plt.subplots(figsize=(20, 8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n        annot=True,\n        cmap = \"YlGnBu\")\nplt.title(\"Correlation Matrix for New Features\")","1c5ff0df":"#combining training and testing set to address missing values\ntrain_row = train.shape[0]\ntest_row = test.shape[0]\ny = train[\"SalePrice\"]\nall_data = pd.concat([train,test]).reset_index(drop=True)\nall_data = all_data.drop(['SalePrice'], axis = 1)\nprint(\"Size of concatenated train and test datasets:\", all_data.shape)","b765d8c3":"train.shape","64586d55":"test.shape","62f1e413":"data_missing_values_ratio = np.round(\n    all_data.isnull().sum().loc[all_data.isnull().sum()>0,]\/(len(all_data)) * 100.0,3\n)\ndata_missing_values_counts = all_data.isnull().sum().loc[all_data.isnull().sum()>0,].reset_index()\ndata_missing_values_counts.columns = ['column', 'counts']\n\ndata_missing_values_ratio = data_missing_values_ratio.reset_index()\ndata_missing_values_ratio.columns = ['column', 'ratio']\n\ndata_missing_values = pd.merge(left = data_missing_values_ratio , right = data_missing_values_counts, on = 'column', how='inner')\ndata_missing_values = data_missing_values.sort_values(by=['ratio'], ascending=False).reset_index(drop=True)\ndata_missing_values","c9bc5ba5":"ind = np.arange(data_missing_values.shape[0])\nwidth = 0.1\nfig, ax = plt.subplots(figsize=(20,8))\nrects = ax.barh(ind, data_missing_values.counts.values[::-1], color='b')\nax.set_yticks(ind)\nax.set_yticklabels(data_missing_values.column.values[::-1], rotation='horizontal')\nax.set_xlabel(\"Missing Observations Count\")\nax.set_title(\"Missing Observations Counts\")\nplt.show()","1b95f1bc":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","4cd40bbc":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","17631aa0":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","1ae579e7":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","4f94e48b":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","ce3f4913":"all_data[\"LotFrontage\"].describe()","44efbd12":"all_data[\"LotFrontage\"] = all_data[\"LotFrontage\"].fillna(all_data[\"LotFrontage\"].describe()['50%'])","364de0e7":"all_data[\"GarageCond\"] = all_data[\"GarageCond\"].fillna(\"None\")","0860e335":"all_data[\"GarageQual\"] = all_data[\"GarageQual\"].fillna(\"None\")","30e3fdf2":"np.sort(all_data[\"GarageYrBlt\"].unique().tolist())","c1acf3f7":"all_data.loc[all_data[\"GarageYrBlt\"] > 2016,][\"GarageYrBlt\"] ","fae73fab":"all_data.loc[(all_data[\"GarageYrBlt\"] > 2016),][\"YearBuilt\"]","8f6c1436":"all_data.loc[all_data[\"GarageYrBlt\"] > 2016,\"GarageYrBlt\"]  =  all_data.loc[(all_data[\"GarageYrBlt\"] > 2016),\"YearBuilt\"]","5a1875dc":"all_data[\"GarageYrBlt\"] = all_data[\"GarageYrBlt\"].fillna(0)","5e68b53d":"all_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].fillna(\"None\")","17411d13":"all_data[\"GarageType\"] = all_data[\"GarageType\"].fillna(\"None\")","c51fdb65":"all_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].fillna(\"None\")","7ceb9569":"all_data[\"BsmtCond\"] = all_data[\"BsmtCond\"].fillna(\"None\")","9912ebb6":"all_data[\"BsmtQual\"] = all_data[\"BsmtQual\"].fillna(\"None\")","630c8e53":"all_data[\"BsmtFinType2\"] = all_data[\"BsmtFinType2\"].fillna(\"None\")","e89cac06":"all_data[\"BsmtFinType1\"] = all_data[\"BsmtFinType1\"].fillna(\"None\")","a45ddfd1":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")","b265a8a9":"all_data[\"MasVnrArea\"].describe()","e4db0d7e":"all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(all_data[\"MasVnrArea\"].describe()['50%'])","b6493e51":"all_data[\"MSZoning\"].value_counts()","30176bca":"all_data[\"MSZoning\"] = all_data[\"MSZoning\"].fillna(all_data[\"MSZoning\"].value_counts()[0])","05203373":"all_data[\"Utilities\"].value_counts()","b8571b52":"all_data = all_data.drop([\"Utilities\"], axis = 1)","caae529a":"all_data[\"Functional\"].value_counts()","5f645e8c":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(all_data[\"Functional\"].value_counts()[0])","0ea4621d":"all_data[\"BsmtHalfBath\"] = all_data[\"BsmtHalfBath\"].fillna(0)","01bc7d43":"all_data[\"BsmtFullBath\"] = all_data[\"BsmtFullBath\"].fillna(0)","b387ef16":"all_data[\"GarageCars\"] = all_data[\"GarageCars\"].fillna(0)","d4d5e6a8":"all_data[\"Exterior2nd\"].value_counts()","23f524aa":"all_data[\"Exterior2nd\"] = all_data[\"Exterior2nd\"].fillna(all_data[\"Exterior2nd\"].value_counts()[0])","78503d81":"all_data[\"Exterior1st\"].value_counts()","9386900f":"all_data[\"Exterior1st\"] = all_data[\"Exterior1st\"].fillna(all_data[\"Exterior1st\"].value_counts()[0])","1e77af06":"all_data[\"KitchenQual\"].value_counts()","d68009c5":"all_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].fillna(all_data[\"KitchenQual\"].value_counts()[0])","2495b9e4":"all_data[\"Electrical\"].value_counts()","d3df82d2":"all_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(all_data[\"Electrical\"].value_counts()[0])","739d8880":"all_data[\"BsmtUnfSF\"] = all_data[\"BsmtUnfSF\"].fillna(0)","ff003bd1":"all_data[\"BsmtFinSF2\"] = all_data[\"BsmtFinSF2\"].fillna(0)","1e133080":"all_data[\"BsmtFinSF1\"] = all_data[\"BsmtFinSF1\"].fillna(0)","d0c7b193":"all_data[\"SaleType\"].value_counts()","3cf9aa5d":"all_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].value_counts()[0])","278083c3":"all_data[\"TotalBsmtSF\"] = all_data[\"TotalBsmtSF\"].fillna(0)","3d544a32":"all_data[\"GarageArea\"] = all_data[\"GarageArea\"].fillna(0)","7ee27574":"#Check to see if any other missing values\nall_data.isnull().sum().loc[all_data.isnull().sum()>0,]","37aadbcd":"print(\"All Data Dimensions:\", all_data.shape)","910d2753":"all_data[\"TotalSqFt\"] = all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"]","65123375":"#remapping categorical variables for training and testing set\nconvert_col_1 = [\"ExterQual\", \"ExterCond\",\"BsmtQual\", \"BsmtCond\", \"HeatingQC\",\"KitchenQual\",\"FireplaceQu\",\n                  \"GarageQual\",\"GarageCond\",\"PoolQC\"]\nmapper = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None':0}\n\nconv_to_numeric(all_data, convert_col_1, mapper)\n\nconvert_col_2 = [\"BsmtExposure\"]\nmapper = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None':0}\nconv_to_numeric(all_data, convert_col_2, mapper)\n\nconvert_col_3 = [\"BsmtFinType1\", \"BsmtFinType2\"]\nmapper = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1 ,'None':0}\nconv_to_numeric(all_data, convert_col_3, mapper)\n\nconvert_col_4 = [\"GarageFinish\"]\nmapper = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'None':0}\nconv_to_numeric(all_data, convert_col_4, mapper)\n\nconvert_col_5 = [\"Fence\"]\nmapper = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'None':0}\nconv_to_numeric(all_data, convert_col_5, mapper)","38f4990b":"print(\"All Data New Dimensions:\", all_data.shape)","1e317c3a":"cols_to_conv_to_categorical = cols_to_conv_to_categorical + [\"YearRemodAdd\", \"YearBuilt\", \"GarageYrBlt\"]\n\nfor cols in cols_to_conv_to_categorical:\n    all_data[cols] = all_data[cols].astype(str)","c44af3bd":"from scipy.stats import skew\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","63faf37a":"#Displaying an example of skewness in data\nsns.distplot(all_data[\"LotArea\"])","63ff7a16":"from scipy.stats import skew\nskew(all_data[\"LotArea\"])","eaff84c2":"#After applying the Box Cox Transformation, we eliminate majority of the skewness and normalize the variable.\nfrom scipy.special import boxcox1p\nall_data_LotArea_boxcox_transform = boxcox1p(all_data[\"LotArea\"], 0.15)\nskew(all_data_LotArea_boxcox_transform)","f0cad8b6":"sns.distplot(all_data_LotArea_boxcox_transform)","eae45b1e":"#Apply this to all features that exhibit skewness of over \nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","8c0492dd":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","d969e1ba":"ntrain = all_data[:train_row]\nntest = all_data[train_row:]","66b120f0":"ntrain = ntrain.drop(['Id'], axis = 1)\nntest = ntest.drop(['Id'], axis = 1)","81ba5437":"#import required packages\nfrom sklearn.linear_model import ElasticNet, Lasso,Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor, plot_importance\nimport time\nfrom mlxtend.regressor import StackingCVRegressor\n\nRANDOM_SEED = 1","9f43e460":"#defining number of folds\nn_folds = 5\n\ndef cross_val_rmse(model):\n    \"\"\"This function will be used to perform cross validation and gather the average RMSE across five-folds for a model\"\"\"\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(ntrain.values)\n    rmse= np.sqrt(-cross_val_score(model, ntrain.values, np.log(y).values, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","c46ce9a6":"models = [\n          Ridge(alpha=0.5, random_state=RANDOM_SEED),\n          ElasticNet(alpha=0.005, random_state=RANDOM_SEED),\n          Lasso(alpha = 0.005, random_state=RANDOM_SEED),\n          XGBRegressor(random_state=RANDOM_SEED)\n         ]\nmodel_name = [\"Ridge\",\"ElasticNet\",\"Lasso\",\"XGBoost\"]\n\nfor name, model in zip(model_name, models):\n    model_test = make_pipeline(RobustScaler(), model)\n    score = cross_val_rmse(model_test)\n    print(name, \": {:.4f} ({:.4f})\".format(score.mean(), score.std()))","d340294d":"def grid_search_function(func_X_train, func_X_test, func_y_train, func_y_test, parameters, model):\n    grid_search = GridSearchCV(model, parameters,  scoring='neg_mean_squared_error')\n    regressor = grid_search.fit(func_X_train,func_y_train)\n    return regressor","af1f4280":"def train_test_split_function(X,y, test_size_percent):\n    \"\"\"Fucntion to perform train_test_split\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_percent, random_state=RANDOM_SEED)\n    return X_train, X_test, y_train, y_test","f5e0bb04":"starttime = time.monotonic()\nparameters = {'ridge__alpha':[1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20], 'ridge__random_state':[RANDOM_SEED],\n             'ridge__max_iter':[100000000]}\npipe = Pipeline(steps=[('rscale',RobustScaler()), ('ridge',Ridge())])\nX_train, X_test, y_train, y_test = train_test_split(ntrain, \n                                                             np.log(y), \n                                                             test_size = 0.20,\n                                                             random_state=RANDOM_SEED\n                                                            )\n\nridge_regressor = grid_search_function(X_train, X_test, y_train, y_test, \n                                     parameters, \n                                     model = pipe)\n\nprint(\"That took \", (time.monotonic()-starttime)\/60, \" minutes\")\n\nprint(\"\\nBest Params:\",ridge_regressor.best_estimator_)\n\nprint(\"\\nBest Score:\",np.sqrt(-ridge_regressor.best_score_))","8747b58c":"ridge_regressor.best_estimator_.steps","c56e2211":"starttime = time.monotonic()\nparameters = {}\npipe = Pipeline(steps=[('ridge',Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=100000000, normalize=False, random_state=1, solver='auto', tol=0.001))])\n\nX_train, X_test, y_train, y_test = train_test_split(ntrain, \n                                                             np.log(y), \n                                                             test_size = 0.20,\n                                                             random_state=RANDOM_SEED\n                                                            )\nridge_model = grid_search_function(X_train, X_test, y_train, y_test, \n                                     parameters, \n                                     model = pipe)\n\nprint(\"That took \", (time.monotonic()-starttime)\/60, \" minutes\")\n\nprint(\"\\nBest Params:\",ridge_model.best_estimator_)\n\nprint(\"\\nBest Score:\",np.sqrt(-ridge_model.best_score_))","82c0e8f0":"starttime = time.monotonic()\nparameters = {}\npipe = Pipeline(steps=[('enet',ElasticNet(alpha=0.0005, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n      max_iter=100000000, normalize=False, positive=False,\n      precompute=False, random_state=1, selection='cyclic', tol=0.0001,\n      warm_start=False))])\nX_train, X_test, y_train, y_test = train_test_split(ntrain, \n                                                             np.log(y), \n                                                             test_size = 0.20,\n                                                             random_state=RANDOM_SEED\n                                                            )\n\nenet_model = grid_search_function(X_train, X_test, y_train, y_test, \n                                     parameters, \n                                     model = pipe)\n\nprint(\"That took \", (time.monotonic()-starttime)\/60, \" minutes\")\n\nprint(\"\\nBest Params:\",enet_model.best_estimator_)\n\nprint(\"\\nBest Score:\",np.sqrt(-enet_model.best_score_))","16be5e39":"starttime = time.monotonic()\nparameters = {}\npipe = Pipeline(steps=[('lasso',Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=100000000,\n   normalize=False, positive=False, precompute=False, random_state=1,\n   selection='cyclic', tol=0.0001, warm_start=False))])\n\nX_train, X_test, y_train, y_test = train_test_split(ntrain, \n                                                             np.log(y), \n                                                             test_size = 0.20,\n                                                             random_state=RANDOM_SEED\n                                                            )\n\nlasso_model = grid_search_function(X_train, X_test, y_train, y_test, \n                                     parameters, \n                                     model = pipe)\n\nprint(\"That took \", (time.monotonic()-starttime)\/60, \" minutes\")\n\nprint(\"\\nBest Params:\",lasso_model.best_estimator_)\n\nprint(\"\\nBest Score:\",np.sqrt(-lasso_model.best_score_))","a0e1a4b0":"##UNCOMMENT to run\n\n#starttime = time.monotonic()\n#parameters = {'xgb__random_state':[RANDOM_SEED],\n#             'xgb__gamma':[0,0.1], \n#              'xgb__learning_rate':[0.01,0.05,0.1],\n#             'xgb__n_jobs':[-1], \n#              'xgb__n_estimators':[500,1000,2000],\n#             'xgb__reg_lambda':[0,0.5,1],\n#              'xgb__reg_alpha':[0,0.5,1]\n#              }\n#\n#pipe = Pipeline(steps=[('xgb',XGBRegressor())])\n#X_train, X_test, y_train, y_test = train_test_split(ntrain, \n#                                                             np.log(y), \n#                                                             test_size = 0.20,\n# random_state=RANDOM_SEED)\n#\n#xgb_regressor = grid_search_function(X_train, X_test, y_train, y_test, \n#                                     parameters, \n#                                     model = pipe)\n#\n#print(\"That took \", (time.monotonic()-starttime)\/60, \" minutes\")\n#\n#print(\"\\nBest Params:\",xgb_regressor.best_estimator_)\n#\n#print(\"\\nBest Score:\",np.sqrt(-xgb_regressor.best_score_))\n\n#####Output###\n####That took  66.38542943511857  minutes\n####\n####Best Params: Pipeline(memory=None,\n####     steps=[('xgb', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n####       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n####       max_depth=3, min_child_...\n####       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n####       silent=True, subsample=1))])\n####\n####Best Score: 0.12259807102070201\n##\n###xgb_regressor.best_estimator_.steps\n####[\n#### ('xgb', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n####         colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n####         max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n####         n_jobs=-1, nthread=None, objective='reg:linear', random_state=1,\n####         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n####         silent=True, subsample=1))]","078e3e1b":"starttime = time.monotonic()\nparameters = {}\npipe = Pipeline(steps=[\n ('xgb', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n         colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n         max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n         n_jobs=-1, nthread=None, objective='reg:linear', random_state=1,\n         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n         silent=True, subsample=1))])\n\nX_train, X_test, y_train, y_test = train_test_split(ntrain, \n                                                             np.log(y), \n                                                             test_size = 0.20,\n                                                             random_state=RANDOM_SEED\n                                                            )\n\nxgb_regressor = grid_search_function(X_train, X_test, y_train, y_test, \n                                     parameters, \n                                     model = pipe)\n\nprint(\"That took \", (time.monotonic()-starttime)\/60, \" minutes\")\n\nprint(\"\\nBest Params:\",xgb_regressor.best_estimator_)\n\nprint(\"\\nBest Score:\",np.sqrt(-xgb_regressor.best_score_))","813101f6":"Ridge_model = Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=100000000,\n   normalize=False, random_state=1, solver='auto', tol=0.001)\n\nEnet_model = ElasticNet(alpha=0.0005, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n      max_iter=100000000, normalize=False, positive=False,\n      precompute=False, random_state=1, selection='cyclic', tol=0.0001,\n      warm_start=False)\n\nlasso_model = Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=100000000,\n   normalize=False, positive=False, precompute=False, random_state=1,\n   selection='cyclic', tol=0.0001, warm_start=False)\n\nxgb_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n         colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n         max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n         n_jobs=-1, nthread=None, objective='reg:linear', random_state=1,\n         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n         silent=True, subsample=1)","d0906c15":"#running a stacking model\nregression_stacker = StackingCVRegressor(regressors = [\n    Enet_model, Ridge_model, xgb_model],\n                                         meta_regressor = lasso_model,\n                                         cv=3)\n\nscore = cross_val_rmse(regression_stacker)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","924b438f":"Ridge_model.fit(ntrain.values, np.log(y).values)\ny_pred = Ridge_model.predict(ntest.values)\ny_train_pred =  Ridge_model.predict(ntrain.values)\nexp_y_pred = np.exp(y_pred)\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.SalePrice = exp_y_pred\nsubmission.to_csv(\"ridge_submission.csv\",index = False)\nprint(\"Training Score:\", np.sqrt(mean_squared_error(np.log(y).values, y_train_pred)))","94dfb657":"#Examining magnitudes of features for Ridge regression\npredictors = ntrain.columns\n\ncoef = pd.Series(Ridge_model.coef_,predictors).sort_values()\ncoef2 = coef[coef!=0]\nf, ax = plt.subplots(figsize=(60, 20))\ncoef2.plot(kind='bar', title='Model Coefficients')\n","aadefae4":"Enet_model.fit(ntrain.values, np.log(y).values)\ny_pred = Enet_model.predict(ntest.values)\ny_train_pred =  Enet_model.predict(ntrain.values)\nexp_y_pred = np.exp(y_pred)\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.SalePrice = exp_y_pred\nsubmission.to_csv(\"elastic_net_submission.csv\",index = False)\nprint(\"Training Score:\", np.sqrt(mean_squared_error(np.log(y).values, y_train_pred)))","d00f264b":"#Examining magnitudes of features for Elastic Net regression\npredictors = ntrain.columns\n\ncoef = pd.Series(Enet_model.coef_,predictors).sort_values()\ncoef2 = coef[coef!=0]\nf, ax = plt.subplots(figsize=(30, 12))\ncoef2.plot(kind='bar', title='Model Coefficients')\n","0aa36781":"lasso_model.fit(ntrain.values, np.log(y).values)\ny_pred = lasso_model.predict(ntest.values)\ny_train_pred =  lasso_model.predict(ntrain.values)\nexp_y_pred = np.exp(y_pred)\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.SalePrice = exp_y_pred\nsubmission.to_csv(\"lasso_submission.csv\",index = False)\nprint(\"Training Score:\", np.sqrt(mean_squared_error(np.log(y).values, y_train_pred)))","1b1f9557":"#Examining magnitudes of features for Lasso regression\npredictors = ntrain.columns\n\ncoef = pd.Series(lasso_model.coef_,predictors).sort_values()\ncoef2 = coef[coef!=0]\nf, ax = plt.subplots(figsize=(30, 12))\ncoef2.plot(kind='bar', title='Model Coefficients')\n","24f9606b":"xgb_model.fit(ntrain.values, np.log(y).values)\ny_pred = xgb_model.predict(ntest.values)\ny_train_pred =  xgb_model.predict(ntrain.values)\nexp_y_pred = np.exp(y_pred)\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.SalePrice = exp_y_pred\nsubmission.to_csv(\"xgboost_submission.csv\",index = False)\nprint(\"Training Score:\", np.sqrt(mean_squared_error(np.log(y).values, y_train_pred)))","8a0fd84a":"#Examining feature importance (most important features) of XGBoost\nxgb_feature_importance_df = pd.DataFrame({\"column_names\":ntrain.columns, \"feature_importance\": xgb_model.feature_importances_})\nxgb_feature_importance_filtered_df = xgb_feature_importance_df.loc[xgb_feature_importance_df.feature_importance>0.003].sort_values('feature_importance',ascending=False)\nf, ax = plt.subplots(figsize=(20, 12))\nsns.barplot(x=\"feature_importance\", y=\"column_names\", data=xgb_feature_importance_filtered_df)","09c43601":"regression_stacker.fit(ntrain.values, np.log(y).values)\ny_pred = regression_stacker.predict(ntest.values)\ny_train_pred =  regression_stacker.predict(ntrain.values)\nexp_y_pred = np.expm1(y_pred)\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.SalePrice = exp_y_pred\nsubmission.to_csv(\"stacking_exp1m_submission.csv\",index = False)\nprint(\"Training Score:\", np.sqrt(mean_squared_error(np.log(y).values, y_train_pred)))","ca5f5036":"__MasVnrType:__ Masonry veneer type. The data description mentions NA which means \"No Masonry veneer\". We will replace these NA values with \"None\".","b168beb7":"__KitchenQual:__ Kitchen quality. We will impute the missing kitchen quality with 'TA' as it the most frequently occuring kitchen quality.\n","18471b2d":"__GarageFinish__: Interior finish of the garage. The data description mentions NA which means \"No Garage\". We will replace these NA values with \"None\".\n\n","20a48d76":"__Exterior2nd:__ Exterior covering on house (if more than one material). We will impute the missing exterior convering type with 'VinylSd' as it the most frequently occuring exterior covering on house (if more than one material).\n","8a6ec568":"Modeling","c13403e9":"__MiscFeature:__ Miscellaneous feature not covered in other categories. The data description mentions NA which means \"None\". We will replace these NA values with \"None\".","30c6f1f3":"__BsmtQual:__ Evaluates the height of the basement. The data description mentions NA which means \"No Basement\". We will replace these NA values with \"None\".","d66d5b86":"__EDA__","4b39c48c":"Best Models","9da31366":"__GarageCars:__ Size of garage in car capacity. We will assume that these houses have missing values because there are  no garages so we will replace these NA values with zero.","69cbfde9":"__Alley:__ Type of alley access to property. The data description mentions NA which means \"None\". We will replace these NA values with \"None\".","89bed463":"__GarageCond__: Garage condition. The data description mentions NA which means \"No Garage\". We will replace these NA values with \"None\".","9ac5fd10":"__LotFrontage__: Linear feet of street connected to property. We will replace the NA values with the median value.","4aefeee2":"# Data Processing","ebc422e5":"__MSZoning:__ Identifies the general zoning classification of the sale. We will impute the missing zones with 'RL' as it the most frequently occuring zoning classification.","1e81bdc1":"__BsmtExposure:__ Refers to walkout or garden level walls. The data description mentions NA which means \"No Basement\". We will replace these NA values with \"None\".","08a821af":"__BsmtFullBath:__ Basement full bathrooms. We will assume that these houses have missing values because there are no basements so we will replace these NA values with zero.","f06e677a":"__BsmtFinType1:__ Rating of basement finished area. The data description mentions NA which means \"No Basement\". We will replace these NA values with \"None\".","8b8eed2a":"__BsmtCond:__ Evaluates the general condition of the basement. The data description mentions NA which means \"No Basement\". We will replace these NA values with \"None\".","80607270":"__Electrical:__ Electrical system.  We will impute the missing kitchen quality with 'SBrkr' as it the most frequently occuring electrical system.\n","edcce18c":"Majority of the imputation will be based on the data dictionary. Some will require some assumptions and intuition.","4dc75a9a":"__GarageQual__: Garage quality. The data description mentions NA which means \"No Garage\". We will replace these NA values with \"None\".","b1f1577a":"__Functional:__ Home functionality (Assume typical unless deductions are warranted). We will impute the missing functional with 'Typ' as it is the most frequently occuring type of Home functionality.","fcca0034":"__SaleType:__ Type of sale. We will impute the missing type of sale with 'WD' as it the most frequently occuring type of sale.","e5ffe33e":"__MasVnrArea:__ Masonry veneer area in square feet. We will replace the NA values with the median value.","ba2340d0":"__Scoring Models__","3bb538d6":"__Utilities:__ Type of utilities available. As we can see the distribution of type of utilities do not differentiate except for one \"NoSeWa\" and two NA values. This variable does not provide meaningful information in predicting SalePrice, so I will be dropping this variable.","a48f8a82":"Some variables are not labeled as categorical variables, even when they are intended to be so I will be fixing those variables.","9733fe2b":"__Fence__: Describes quality of fence. The data description mentions NA which means \"No Fence\". We will replace these NA values with \"None\".\n    ","a4818219":"After the boxplots, I saw many any relationships in the ordering of the categorical variables. So I converted the following variables into ordinal and examined how their relationship with SalePrice will perform in modeling.","aba7bbe0":"__TotalBsmtSF:__ Total square feet of basement area. We will assume that these houses have missing values because there are no basements so we will replace these NA values with zero.","f3f48e3b":"__PoolQC:__ Pool Quality. The data description mentions NA which means \"No Pool\". We will replace these NA values with \"None\".","73441f83":"__GarageType:__ Garage location. The data description mentions NA which means \"No Garage\". We will replace these NA values with \"None\".","58920ebb":"There definitely appears to be an increasing relationship between the overall quality of the house and its sale price.","4f77d1de":"Getting a benchmark for each model","99588bbe":"__BsmtHalfBath:__ Basement half bathrooms. We will assume that these houses have missing values because there are no basements so we will replace these NA values with zero.","1e81ea11":"__BsmtFinSF2:__ Type 2 finished square feet. We will assume that these houses have missing values because there are no basements so we will replace these NA values with zero.\n","2a5ee29e":"__BsmtFinType2:__ Rating of basement finished area (if multiple types). The data description mentions NA which means \"No Basement\". We will replace these NA values with \"None\".","b719e71f":"__FireplaceQu__: Describes quality of the Fireplace. The data description mentions NA which means \"No Fireplace\". We will replace these NA values with \"None\".\n    ","95e9e9ef":"__GarageYrBlt:__ Year garage was built. We will assume that these houses do not have a garage. We will replace these NA values with \"None\". I also noticed an outlier in the year built for a garage. The year 2207 is an invalid date. I replaced this year with the year that the house was built in.","3eef92c1":"__Exterior1st:__ Exterior covering on house. We will impute the missing exterior convering type with 'VinylSd' as it the most frequently occuring exterior covering on house.\n","254b832a":"__GarageArea:__ Size of garage in square feet. We will assume that these houses have missing values because there are no garages so we will replace these NA values with zero.","d558f121":"__BsmtUnfSF:__ Unfinished square feet of basement area. We will assume that these houses have missing values because there are no basements so we will replace these NA values with zero.\n","952747fe":"__Outlier Detection__","8e4d87e4":"__BsmtFinSF1:__ Type 1 finished square feet. We will assume that these houses have missing values because there are no basements so we will replace these NA values with zero.\n"}}