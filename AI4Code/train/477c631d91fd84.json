{"cell_type":{"767f422c":"code","63fe4fa4":"code","f1cfac45":"code","f3911d52":"code","2e225815":"code","dbd6e6d3":"code","c39f58b7":"code","bee53a81":"code","0bdefaec":"code","52c04a39":"code","75af2786":"markdown","a2f371d5":"markdown","e421a76c":"markdown","21696337":"markdown","86c8c34c":"markdown","7c418f81":"markdown","2f0e6863":"markdown","3e72a0a9":"markdown","0eaaa7a8":"markdown"},"source":{"767f422c":"from IPython.display import Image\nimport os\n!ls ..\/input\/","63fe4fa4":"Image(\"..\/input\/pictures\/perceptron.png\")","f1cfac45":"Image(\"..\/input\/pictures\/mathformula.png\")","f3911d52":"import numpy as np\n\n#Randomly initialize 2 weights and one weight for bias. We're only taking first two decimal places.\nweights = np.around(np.random.uniform(size=2), decimals=2)\nbias = np.around(np.random.uniform(size=1),decimals=2)","2e225815":"print(weights)\nprint(bias)","dbd6e6d3":"x_1 = 0.7 # input 1\nx_2 = 0.86 # input 2\n\nprint('x1 is {} and x2 is {}'.format(x_1, x_2))","c39f58b7":"z_1 = x_1 * weights[0] + x_2 * weights[1] + bias[0]\n\nprint('The linear combination of inputs and their weights is {}'.format(z_1))","bee53a81":"Image(\"..\/input\/function\/sigmoid.png\")","0bdefaec":"a_1 = 1.0 \/ (1.0 + np.exp(-z_1))","52c04a39":"print('The output of the network for x1 = 0.7 and x2 = 0.87 is {}'.format(np.around(a_1, decimals=4)))","75af2786":"# Structure of Perceptron\n\nLet's revise on how the structure of Perceptron looks like.","a2f371d5":"Let's check the weights which got randomly assigned just to be sure.","e421a76c":"Let's assign values to input 1 and input 2.","21696337":"All the inputs and it's corresponding weights are multiplied and then added.\nThis sum will be an input to the activation function which finally yields our output **y**.\n\nNow, let's start with the coding part. I will quickly run you through all the code, if you've any questions, please comment.","86c8c34c":"Import numpy for mathematical calculations.","7c418f81":"Let's calculate the linear combination of inputs and their weights.","2f0e6863":"Let's feed this sum into an activation function, here it is Sigmoid Function, to get the output.","3e72a0a9":"There are **two inputs and one bias**. Every input has it's own weight which is randomly intialized. The input of bias is always equal to 1. Let's see the mathematical form of this structure.","0eaaa7a8":"# Code for Perceptron (Artificial Neuron)"}}