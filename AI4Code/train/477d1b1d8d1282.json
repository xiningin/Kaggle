{"cell_type":{"6a70f95d":"code","9ff3ea38":"code","2101ae93":"code","abaa3e54":"code","cbc71376":"code","be53cb9b":"code","31e85b97":"code","0b3c233c":"code","cbae1cca":"code","e6841b92":"code","1ed5d0dc":"code","e8939593":"code","3774d807":"code","33514e4e":"code","ddd27503":"code","e3163bdf":"code","e9057a39":"code","65a6df6f":"code","4e887a56":"code","f38fbf8b":"code","c5b26229":"code","523aa1c1":"code","f332a88a":"code","fab704a1":"code","04eee980":"code","f3ba31f5":"code","1fdc2e18":"code","11d10535":"code","eab89f13":"code","fec5966d":"code","8c75c131":"code","e0b12461":"code","1682ac86":"code","17b79ef7":"code","59ace6d9":"code","14aa3380":"code","dd73e84f":"code","73a21a57":"code","9291ee2e":"code","7bfaf0b4":"code","accae0e5":"code","f7450791":"code","bde7fa21":"code","31f87baa":"code","ff047f8d":"code","83366204":"code","8cb8d06c":"code","b2516a6b":"code","57d30899":"code","36900eae":"code","91aa64ea":"code","11ac8238":"code","bd0bd067":"code","c203cdca":"code","6b88e2b9":"code","c821e8c9":"code","df7ed08f":"code","16187d78":"code","5b7314aa":"code","92bed8b1":"markdown","9ba3b463":"markdown","6991ff4c":"markdown","092d4b88":"markdown","b774826f":"markdown","5db39942":"markdown","3c7f0fe7":"markdown","be2fda43":"markdown","98b5767f":"markdown"},"source":{"6a70f95d":"import sklearn\nprint(sklearn.__version__) # 0.17.1\n","9ff3ea38":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)|","2101ae93":"loading = pd.read_csv('\/kaggle\/input\/trends-assessment-prediction\/loading.csv') # \u30bf\u30f3\u30d1\u30af\u8cea\u306e\u6fc3\u5ea6\u30de\u30c3\u30d7\nfnc = pd.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/fnc.csv\") # functional network connectivity (\u8133\u306e\u63a5\u7d9a\u6027\u306e\u6307\u6a19\u3089\u3057\u3044)\nscore = pd.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/train_scores.csv\")","abaa3e54":"score","cbc71376":"fnc_features, loading_features = list(fnc.columns[1:]), list(loading.columns[1:])# \u30b3\u30e9\u30e0\u3092\u62bd\u51fa (id\u3092\u9664\u5916)","be53cb9b":"df = fnc.merge(loading, on=\"Id\")\nscore[\"is_train\"] = True\ndf = df.merge(score, on=\"Id\", how=\"left\")\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()","31e85b97":"FNC_SCALE = 1\/400\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","0b3c233c":"features = loading_features + fnc_features","cbae1cca":"from sklearn.model_selection import KFold\n\ndef metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)\/np.sum(y_true, axis=0))","e6841b92":"import lightgbm\nfrom lightgbm import LGBMRegressor","1ed5d0dc":"lgb_params = {\n        'max_depth': -1,\n        'num_leaves': 3,\n        'min_data_in_leaf': 13,\n    }","e8939593":"NUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind] # train, val split\n        train_df = train_df[train_df[target].notnull()] # null\u6392\u9664\n\n        model = LGBMRegressor(**lgb_params) # create lightGBM model\n        model.fit(train_df[features], train_df[target]) #df\u304b\u3089feature, target\u3092\u6307\u5b9a\u3057\u3066model\u3092train\n\n        y_oof[val_ind] = model.predict(val_df[features]) #Out of fold, \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306e\u5916\u306e\u30c7\u30fc\u30bf\u3092\u96c6\u3081\u308b\n        y_test[:, f] = model.predict(test_df[features])\n                \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","3774d807":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","33514e4e":"t_df.head(1)\nt_df.to_csv(\"training_lightgbm.csv\", index=False)","ddd27503":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","e3163bdf":"sub_df.to_csv(\"submission_lightgbm.csv\", index=False)","e9057a39":"import xgboost\nfrom xgboost import XGBRegressor","65a6df6f":"xgb_params = {\n        'objective': 'reg:squarederror',\n        \"learning_rate\": 0.1,\n        'max_depth': 3,\n        'n_estimators': 200,\n        'subsample': 0.5,\n        'colsample_bytree': 0.5\n    }","4e887a56":"%%time\nNUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = XGBRegressor(**xgb_params)\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","f38fbf8b":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","c5b26229":"t_df.to_csv(\"training_xgboost.csv\", index=False)","523aa1c1":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","f332a88a":"sub_df.to_csv(\"submission_xgboost.csv\", index=False)","fab704a1":"from sklearn.linear_model import Lasso","04eee980":"%%time\nNUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = Lasso(alpha=0.1)\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","f3ba31f5":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","1fdc2e18":"t_df.to_csv(\"training_lasso.csv\", index=False)","11d10535":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","eab89f13":"sub_df.to_csv(\"submission_lasso.csv\", index=False)","fec5966d":"from sklearn.linear_model import BayesianRidge","8c75c131":"%%time\nNUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = BayesianRidge()\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","e0b12461":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","1682ac86":"t_df.to_csv(\"training_ridge.csv\", index=False)","17b79ef7":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","59ace6d9":"sub_df.to_csv(\"submission_ridge.csv\", index=False)","14aa3380":"from sklearn.linear_model import ElasticNet","dd73e84f":"%%time\nNUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = ElasticNet()\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","73a21a57":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","9291ee2e":"t_df.to_csv(\"training_elastic.csv\", index=False)","7bfaf0b4":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","accae0e5":"sub_df.to_csv(\"submission_elastic.csv\", index=False)","f7450791":"from sklearn.linear_model import TweedieRegressor","bde7fa21":"%%time\nNUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = TweedieRegressor()\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","31f87baa":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","ff047f8d":"t_df.to_csv(\"training_tweedie.csv\", index=False)","83366204":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","8cb8d06c":"sub_df.to_csv(\"submission_tweedie.csv\", index=False)","b2516a6b":"from sklearn.linear_model import LinearRegression","57d30899":"%%time\nNUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = LinearRegression()\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","36900eae":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","91aa64ea":"t_df.to_csv(\"training_linear.csv\", index=False)","11ac8238":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","bd0bd067":"sub_df.to_csv(\"submission_linear.csv\", index=False)","c203cdca":"from sklearn.svm import SVR","6b88e2b9":"NUM_FOLDS = 4\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\noveral_score = 0\nfor target, c, w in [(\"age\", 100, 0.3), (\"domain1_var1\", 10, 0.175), (\"domain1_var2\", 10, 0.175), (\"domain2_var1\", 10, 0.175), (\"domain2_var2\", 10, 0.175)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model = SVR(C=c, cache_size=3000.0)\n        model.fit(train_df[features], train_df[target])\n\n        y_oof[val_ind] = model.predict(val_df[features])\n        y_test[:, f] = model.predict(test_df[features])\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    \n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    overal_score += w*score\n    print(target, np.round(score, 4))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 4))","c821e8c9":"t_df = df[[\"Id\", \"pred_age\", \"pred_domain1_var1\", \"pred_domain1_var2\", \"pred_domain2_var1\", \"pred_domain2_var2\"]]\nt_df = t_df.sort_values(\"Id\")","df7ed08f":"t_df.to_csv(\"training_svr.csv\", index=False)","16187d78":"sub_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]\nsub_df = sub_df.sort_values(\"Id\")","5b7314aa":"sub_df.to_csv(\"submission_svr.csv\", index=False)","92bed8b1":"# TweedieRegressor","9ba3b463":"# Lasso","6991ff4c":"# Linear Regression","092d4b88":"# Ridge","b774826f":"# LightGBM","5db39942":"# XGBoost","3c7f0fe7":"# ElasticNet","be2fda43":"# SVM","98b5767f":"## This is preprocess for what stacking model based on 8 model about Gradient Boost and Linear Regression\n\n### You can use the ouput files submission_{name}.csv or training_{name}.csv\n### submission_{name}.csv \u30fb\u30fb\u30fb A submission file predicted using the {name} algorithm.\n### training_{name}.csv \u30fb\u30fb\u30fb A training file predicted using the {name} algorithm.\n\nStacking code is https:\/\/www.kaggle.com\/ngo1013\/svr-stacking\/"}}