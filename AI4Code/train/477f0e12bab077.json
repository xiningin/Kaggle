{"cell_type":{"0fc35c9f":"code","c4177c82":"code","d32103ff":"code","01a2a749":"code","627b3707":"code","844ef493":"code","e5a1b14e":"code","0fbb7a11":"code","55763edf":"code","e21a74f0":"code","e8f80d43":"code","fac38021":"code","15bb820c":"code","48f74129":"code","1963c0ee":"code","5b22d1d4":"code","f779a18a":"code","26b63fcb":"code","640fe8dd":"code","5e486c4a":"code","18b3cdb0":"code","7b88866d":"code","c3c4b86c":"code","23036947":"code","ea892581":"code","00b0f769":"code","4c206c7e":"code","31591d96":"code","48830a1a":"code","915c019e":"code","65e29212":"code","bb1acd55":"code","24c62481":"code","badc28a0":"code","655be428":"code","4eb5f9db":"code","1312551f":"code","9b57664b":"code","c94bc233":"code","597e7d57":"code","45c1d4da":"code","40483fe4":"code","38405047":"code","93aceca8":"code","0e42085a":"code","0e33ba51":"code","fa960e41":"code","4e4d27d6":"code","d3e3bbd2":"code","bb27de19":"code","29fdfaf4":"code","c25dee52":"code","9ab552ee":"code","bcafa426":"code","cc485855":"code","a6070e74":"code","b1deba18":"code","7c8a7f07":"code","a9b2bfe0":"code","286ad355":"code","177061ff":"code","3a7d17b7":"code","bf33c2e2":"markdown","f6e9ff67":"markdown","20df9e1e":"markdown","6f07c174":"markdown","d0967c67":"markdown","8740528d":"markdown","b7ffb11c":"markdown","32610aed":"markdown","e826aa7b":"markdown","ec4e52bd":"markdown","84f62cae":"markdown","7c5ed03d":"markdown","70564cf9":"markdown","529f9e1e":"markdown","53f1b5b7":"markdown","995e7940":"markdown","d8a72319":"markdown","af2cc0a8":"markdown","0dc2e82b":"markdown","11c62fb8":"markdown","10a6dfd6":"markdown","0ae85611":"markdown","8ac22fd4":"markdown","f9c86511":"markdown","b7b7aec0":"markdown","9a678c81":"markdown","5a5dfc68":"markdown","a169db43":"markdown","a8ece851":"markdown","11b794cb":"markdown","078dfde2":"markdown","92b2ec41":"markdown","bc41ef80":"markdown","eefe339b":"markdown","027ff7cb":"markdown","652a12ed":"markdown","0295d6d1":"markdown","4c298b76":"markdown","931c9b9c":"markdown","a6b779c7":"markdown","98057e4e":"markdown","02284ae1":"markdown","aadb10b8":"markdown"},"source":{"0fc35c9f":"import numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Models\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nimport tensorflow as tf\nimport os\nimport tensorflow_datasets as tfds\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom matplotlib.ticker import MaxNLocator\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000","c4177c82":"tf.config.experimental.list_physical_devices('GPU')","d32103ff":"df=pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","01a2a749":"df.info()","627b3707":"df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')\ndf.isnull().sum()","844ef493":"calib=df[:4000]\ncalib.TotalCharges.isnull().sum()","e5a1b14e":"df.dropna(inplace=True)","0fbb7a11":"df.drop('customerID',1,inplace=True)","55763edf":"df.nunique()","e21a74f0":"uniq=df.nunique()\nord_cols=uniq[uniq==3].index\nord_cols","e8f80d43":"for col in ord_cols:\n    print(df[col].unique())","fac38021":"df['Churn'].replace(to_replace='Yes', value=1, inplace=True)\ndf['Churn'].replace(to_replace='No',  value=0, inplace=True)\ndf2=df\ndf2['SeniorCitizen'] = df2['SeniorCitizen'].apply(lambda x: 'Yes' if x == 1 else 'No')\ndf1=pd.get_dummies(df)\ndf1.head()","15bb820c":"corre=df1.corr()\nf, ax = plt.subplots(figsize=(14,16))\nsns.heatmap(corre,mask=corre<0.75,linewidth=0.25,cmap=\"Blues\",linecolor='Black', square=True)","48f74129":"plt.figure(figsize=(15,8))\ndf1.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')","1963c0ee":"df['gender'].value_counts()","5b22d1d4":"uniq=df.nunique()\ncat_cols=uniq[uniq<5].index\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in df.columns:\n    if df[i].dtype in numeric_dtypes:\n        numeric.append(i)\nnumeric,cat_cols","f779a18a":"numeric=['tenure', 'MonthlyCharges', 'TotalCharges']\ncat_feats=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n       'Contract', 'PaperlessBilling', 'PaymentMethod']","26b63fcb":"def srt_dist(df=df,cols=cat_feats):\n    fig, axes = plt.subplots(8, 2,squeeze=True)\n    axes = axes.flatten()\n\n    for i, j in zip(cols, axes):\n\n        (df[i].value_counts()*100.0 \/len(df)).plot.pie(autopct='%.1f%%',figsize =(10,37), fontsize =15,ax=j )                                                                      \n        j.yaxis.label.set_size(15)","640fe8dd":"srt_dist()\n","5e486c4a":"import matplotlib.ticker as mtick\ncolors = ['#4D3425','#E4512B']\npartner_dependents = df.groupby(['Partner','Dependents']).size().unstack()\n\nax = (partner_dependents.T*100.0 \/ partner_dependents.T.sum()).T.plot(kind='bar',\n                                                                width = 0.2,\n                                                                stacked = True,\n                                                                rot = 0, \n                                                                figsize = (6,4),\n                                                                color = colors)\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.legend(loc='center',prop={'size':10},title = 'Dependents',fontsize =10)\nax.set_ylabel('% Customers',size = 10)\nax.set_title('% Customers with\/without dependents based on whether they have a partner',size = 10)\nax.xaxis.label.set_size(10)\n\n# Code to add the data labels on the stacked bar chart\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0f}%'.format(height), (p.get_x()+.25*width, p.get_y()+.4*height),\n                color = 'white',\n               weight = 'bold',\n               size = 10)","18b3cdb0":"ax = sns.distplot(df['tenure'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('Tenure (months)')\nax.set_title('# of Customers by their tenure')","7b88866d":"fig, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3, sharey = True, figsize = (20,6))\n\nax = sns.distplot(df[df['Contract']=='Month-to-month']['tenure'],\n                   hist=True, kde=False,\n                   bins=int(180\/5), color = 'turquoise',\n                   hist_kws={'edgecolor':'black'},\n                   kde_kws={'linewidth': 4},\n                 ax=ax1)\nax.set_ylabel('# of Customers')\nax.set_xlabel('Tenure (months)')\nax.set_title('Month to Month Contract')\n\nax = sns.distplot(df[df['Contract']=='One year']['tenure'],\n                   hist=True, kde=False,\n                   bins=int(180\/5), color = 'steelblue',\n                   hist_kws={'edgecolor':'black'},\n                   kde_kws={'linewidth': 4},\n                 ax=ax2)\nax.set_xlabel('Tenure (months)',size = 14)\nax.set_title('One Year Contract',size = 14)\n\nax = sns.distplot(df[df['Contract']=='Two year']['tenure'],\n                   hist=True, kde=False,\n                   bins=int(180\/5), color = 'darkblue',\n                   hist_kws={'edgecolor':'black'},\n                   kde_kws={'linewidth': 4},\n                 ax=ax3)\n\nax.set_xlabel('Tenure (months)')\nax.set_title('Two Year Contract')","c3c4b86c":"services = ['PhoneService','MultipleLines','InternetService','OnlineSecurity',\n           'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nsrt_dist(df,services)","23036947":"df[['MonthlyCharges', 'TotalCharges']].plot.scatter(x = 'MonthlyCharges',y='TotalCharges')","ea892581":"(df['Churn'].value_counts()*100.0 \/len(df)).plot.pie(autopct='%.1f%%', fontsize =15)","00b0f769":"sns.pairplot(df,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\")","4c206c7e":"cat_feats=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n       'Contract', 'PaperlessBilling', 'PaymentMethod']\nfig,axes = plt.subplots(16)\naxes = axes.flatten()\n\nfor i, j in zip(cat_feats, axes):\n\n    sortd = df.groupby([i])['Churn'].median().sort_values(ascending=False)\n    j=sns.catplot(x=i,\n                y='Churn',\n                data=df,\n                kind='bar')\n    j.set_ylabels(\"Churn Probability\")","31591d96":"# Split and normalization\ny=df1.Churn.values\nX=df1.drop('Churn',1)\nfrom sklearn.preprocessing import MinMaxScaler\nfeatures = X.columns.values\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features\ny=df1['Churn']","48830a1a":"X_train=X.iloc[:3993,:]\nX_test=X.iloc[3993:,:]\ny_train=df1.iloc[:3993,:]['Churn']\ny_test=df1.iloc[3993:,:]['Churn']","915c019e":"def stratified_cv(X, y, clf_class, shuffle=True,  **kwargs):\n    stratified_k_fold = StratifiedKFold().split(X,y)\n    y_pred = y.copy()\n    for ii, jj in stratified_k_fold: \n        Xtrain, Xtest = X.iloc[ii], X.iloc[jj]\n        ytrain = y.iloc[ii]\n        clf = clf_class(**kwargs)\n        clf.fit(X_train,y_train)\n        y_pred.iloc[jj] = clf.predict(Xtest)\n    return y_pred","65e29212":"print('Gradient Boosting Classifier:  {:.2f}'.format(\n    metrics.accuracy_score(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\n\nprint('Support vector machine(SVM):   {:.2f}'.format(\n    metrics.accuracy_score(y, stratified_cv(X, y, svm.SVC))))\n\nprint('Random Forest Classifier:      {:.2f}'.format(\n    metrics.accuracy_score(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\n\nprint('K Nearest Neighbor Classifier: {:.2f}'.format(\n    metrics.accuracy_score(y, stratified_cv(X, y, neighbors.KNeighborsClassifier))))\n\nprint('Logistic Regression:           {:.2f}'.format(\n    metrics.accuracy_score(y, stratified_cv(X, y, linear_model.LogisticRegression))))\nprint('XGBoost Classifier:           {:.2f}'.format(\n    metrics.accuracy_score(y, stratified_cv(X, y, XGBClassifier))))","bb1acd55":"print('Gradient Boosting Classifier:\\n {}\\n'.format(\n    metrics.classification_report(y, stratified_cv(X, y, ensemble.GradientBoostingClassifier))))\n\nprint('Support vector machine(SVM):\\n {}\\n'.format(\n    metrics.classification_report(y, stratified_cv(X, y, svm.SVC))))\n\nprint('Random Forest Classifier:\\n {}\\n'.format(\n    metrics.classification_report(y, stratified_cv(X, y, ensemble.RandomForestClassifier))))\n\nprint('K Nearest Neighbor Classifier:\\n {}\\n'.format(\n    metrics.classification_report(y, stratified_cv(X, y, neighbors.KNeighborsClassifier,n_neighbors=11))))\n\nprint('Logistic Regression:\\n {}\\n'.format(\n    metrics.classification_report(y, stratified_cv(X, y, linear_model.LogisticRegression))))\n      \nprint('XGBoost Classifier:\\n {}\\n'.format(\n    metrics.classification_report(y, stratified_cv(X, y, XGBClassifier))))","24c62481":"# Tuning Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Create param grid.   \nparam_rf=[{'n_estimators' : list(range(10,150,15)),\n            'max_features' : list(range(6,32,5))}]\n\n# Create grid search object\n\nclf = RandomizedSearchCV(RandomForestClassifier(), param_distributions = param_rf, n_iter=50, cv = 5, refit=True,verbose=1, n_jobs=-1,)\n\n# Fit on data\nbest_clf = clf.fit(X, y)\n\nprint(best_clf.best_params_)\nbest_clf.best_score_\n","badc28a0":"# Tuning Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nparam_grid = [\n    {'penalty' : ['l1', 'l2'],\n    'C' : np.logspace(-5, 5, 20),\n    'solver' : ['liblinear'] }]\nclf = RandomizedSearchCV(LogisticRegression(), param_distributions = param_grid, n_iter=20, cv = 5, refit=True,verbose=1, n_jobs=-1,)\n\n# Fit on data\n\nbest_clf = clf.fit(X, y)\nprint(best_clf.best_params_)\nbest_clf.best_score_","655be428":"# Pre-made estimators inputs\n\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nXnn = df2.drop(['Churn'], axis=1)\nynn = df2['Churn']\n\nfrom sklearn.model_selection import train_test_split as tts\n\nX_tr,X_tes,Y_tr,Y_tes = tts(Xnn,ynn, test_size=0.2,random_state=42)\n\n# feature extraction\n\ntenure = tf.feature_column.numeric_column('tenure')\nmonthly_charges = tf.feature_column.numeric_column('MonthlyCharges')\ntotal_charges = tf.feature_column.numeric_column('TotalCharges')\ncol_unique_val_counts = []\ncat_columns = []\nfor col in Xnn.columns:\n    if Xnn[col].dtype.name != 'object':\n        continue\n    unique_vals = Xnn[col].unique()\n    col_unique_val_counts.append(len(unique_vals))\n    cat_columns.append(col)\n    print(col, \"->\",unique_vals)\ncat_cols = [tf.feature_column.categorical_column_with_hash_bucket(col, hash_bucket_size=size) \n            for col, size in zip(cat_columns, col_unique_val_counts)]\nnum_cols = [tenure, monthly_charges, total_charges]\nfeature_columns = num_cols + cat_cols","4eb5f9db":"\nn_classes = 2 # churn Yes or No\nbatch_size = 128\nimport tensorflow_datasets as tfds\n\nchurn_file = tf.data.Dataset.from_tensor_slices((X,y))\ndef make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n    def input_function():\n        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n        if shuffle:\n            ds = ds.shuffle(1000)\n        ds = ds.batch(batch_size).repeat(num_epochs)\n        return ds\n    return input_function\ntrain_input_fn = make_input_fn(X_tr, Y_tr)\neval_input_fn = make_input_fn(X_tes,Y_tes, num_epochs=1, shuffle=False)\n\nimport tempfile\nmodel_dir = tempfile.mkdtemp()\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n  0.001,\n  decay_steps=(X_tr.shape[0]\/128)*50,\n  decay_rate=1,\n  staircase=False)\n\ndef get_optimizer():\n    return tf.keras.optimizers.Adam(lr_schedule)\ndef get_callbacks(name):\n    return [\n    tfdocs.modeling.EpochDots(),\n    tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n    tf.keras.callbacks.TensorBoard(logdir\/name),\n  ]\nlinear_model= tf.estimator.LinearClassifier(model_dir=model_dir,feature_columns=feature_columns, n_classes=n_classes,  \n                                            optimizer=lambda:get_optimizer())\nlinear_model.train(input_fn=train_input_fn)\nresult = linear_model.evaluate(eval_input_fn)\n\n\nprint(result)\n","1312551f":"# DNN Classifier (Deep Neural Network)\nfeature_cols1=[]\nfor col in Xnn.columns:\n    if Xnn[col].dtype.name != 'object':\n        continue\n    unique_vals = Xnn[col].unique()\n    col_unique_val_counts.append(len(unique_vals))\n    cat_columns.append(col)\n    cat_cols1 = tf.feature_column.categorical_column_with_vocabulary_list(col,unique_vals,num_oov_buckets=2)         \n    catcols1_embedding = tf.feature_column.embedding_column(cat_cols1, dimension=4)                     \n    feature_cols1.append(catcols1_embedding)\nfeature_cols=feature_cols1 + num_cols\nfeature_layer = tf.keras.layers.DenseFeatures(feature_cols1)\n\ndnnmodel=tf.estimator.DNNClassifier(\n    hidden_units=[128,54,8], feature_columns=feature_cols , n_classes=2,\n    optimizer=lambda:get_optimizer(), activation_fn=tf.nn.relu,\n    dropout=0.3 )\ndnnmodel.train(input_fn=train_input_fn)\nresult = dnnmodel.evaluate(eval_input_fn)\n\n\nprint(result)\n","9b57664b":"from keras.models import Sequential \nfrom keras.layers import InputLayer \nfrom keras.layers import Dense \nfrom keras.layers import Dropout \nfrom keras.constraints import maxnorm\n\nfrom  IPython import display","c94bc233":"# Model 1\nnn_model = Sequential()\nnn_model.add(Dense(64,kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                input_dim=46, activation='relu' ))\nnn_model.add(Dropout(rate=0.2))\nnn_model.add(Dense(8,kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                    activation='relu'))\nnn_model.add(Dropout(rate=0.1))\nnn_model.add(Dense(1, activation='sigmoid'))\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n      0.001,\n      decay_steps=(X_train.shape[0]\/32)*50,\n      decay_rate=1,\n      staircase=False)\n\ndef get_optimizer():\n    return tf.keras.optimizers.Adam(lr_schedule)\ndef get_callbacks():\n    return [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=70,restore_best_weights=True)]\nnn_model.compile(loss = \"binary_crossentropy\", \n                  optimizer = get_optimizer(), \n                  metrics=['accuracy'])\n    \n\nhistory = nn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32,\n                    callbacks= get_callbacks(),verbose=0)\n\nplt.plot(history.history['accuracy']) \nplt.plot(history.history['val_accuracy']) \nplt.title('model accuracy') \nplt.ylabel('accuracy')\nplt.xlabel('epoch') \nplt.legend(['train', 'test'], loc='upper left') \nplt.show()","597e7d57":"yprednn=nn_model.predict(X_test)\nyprednn=yprednn.round()\nprint('Neural Network:\\n {}\\n'.format(\n    metrics.classification_report(yprednn, y_test)))\nnn_conf_matrix=metrics.confusion_matrix(yprednn,y_test)\nconf_mat_nn = pd.DataFrame(\n    nn_conf_matrix, \n    columns=[\"Predicted NO\", \"Predicted YES\"], \n    index=[\"Actual NO\", \"Actual YES\"]\n)\nprint(conf_mat_nn)","45c1d4da":"test_loss, test_acc = nn_model.evaluate(X_test,  y_test, verbose=2)","40483fe4":"# Model 2\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\nimport IPython\nimport kerastuner as kt\n\ndef build_model(hp):\n    inputs = tf.keras.Input(46,)\n    x = inputs\n    for i in range(hp.Int('num_layers', 1,3)):\n        x =  tf.keras.layers.Dense(units=hp.Int('units_' + str(i),32,256, step=32, default=64),\n             kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n    \n    x = tf.keras.layers.Dense(\n      hp.Int('hidden_size', 4,64, step=4, default=8),\n             kernel_regularizer=tf.keras.regularizers.l2(0.001),\n             activation='relu')(x)\n    x = tf.keras.layers.Dropout(\n      hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(\n      hp.Float('learning_rate', 1e-3,1e-1, sampling='log')),\n    loss=\"binary_crossentropy\", \n    metrics=['accuracy'])\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=10,\n    executions_per_trial=1,\n    \n    )\nbatch_size=32\ntuner.search(X_train, y_train,\n                     epochs=100,batch_size=batch_size,\n                     validation_data=(X_test,y_test),\n                     callbacks= [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                                    patience=40,restore_best_weights=True)],verbose=False)","38405047":"best_hp = tuner.get_best_hyperparameters()[0] \nbest_hp.values","93aceca8":"# according to documentation the model has to be refit\nmodel2 = tuner.hypermodel.build(best_hp)\nhistory2 = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32,\n                        callbacks= tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                                patience=40,restore_best_weights=True),verbose=0)\nplt.plot(history2.history['accuracy']) \nplt.plot(history2.history['val_accuracy']) \nplt.title('model accuracy') \nplt.ylabel('accuracy')\nplt.xlabel('epoch') \nplt.legend(['train', 'test'], loc='upper left') \nplt.show()\n","0e42085a":"yprednn1=model2.predict(X_test)\nyprednn1=yprednn1.round()\nprint('Neural Network:\\n {}\\n'.format(\n    metrics.classification_report(yprednn1, y_test)))\nnn_conf_matrix1=metrics.confusion_matrix(yprednn1,y_test)\nconf_mat_nn1 = pd.DataFrame(\n    nn_conf_matrix1, \n    columns=[\"Predicted NO\", \"Predicted YES\"], \n    index=[\"Actual NO\", \"Actual YES\"]\n)\nprint(conf_mat_nn1)","0e33ba51":"# Model 3 \n\n\nmodel = Sequential()\nmodel.add(Dense(64,kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                input_dim=46, activation='relu' ))\nmodel.add(Dropout(rate=0.35))\nmodel.add(Dense(8,kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                    activation='relu'))\nmodel.add(Dropout(rate=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n      0.001,\n      decay_steps=(X_train.shape[0]\/1024)*100,\n      decay_rate=1,\n      staircase=False)\n\ndef get_optimizer():\n    return tf.keras.optimizers.Adam(lr_schedule)\ndef get_callbacks():\n    return [tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=150,\n        restore_best_weights=True)]\nmodel.compile(loss = \"binary_crossentropy\", \n                  optimizer = get_optimizer(), \n                  metrics=['accuracy'])\n    \n\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024,\n                    callbacks= get_callbacks(),verbose=0)\n\nplt.plot(history.history['accuracy']) \nplt.plot(history.history['val_accuracy']) \nplt.title('model accuracy') \nplt.ylabel('accuracy')\nplt.xlabel('epoch') \nplt.legend(['train', 'test'], loc='upper left') \nplt.show()","fa960e41":"yprednn=model.predict(X_test)\nyprednn=yprednn.round()\nprint('Neural Network:\\n {}\\n'.format(\n    metrics.classification_report(yprednn, y_test)))\nprint('Neural Network:\\n {}\\n'.format(\n    metrics.confusion_matrix(yprednn, y_test)))","4e4d27d6":"from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc","d3e3bbd2":"rf_conf_matrix  = metrics.confusion_matrix(y, stratified_cv(X, y, ensemble.RandomForestClassifier,n_estimators=113))\nconf_mat_rf = pd.DataFrame(\n    rf_conf_matrix, \n    columns=[\"Predicted NO\", \"Predicted YES\"], \n    index=[\"Actual NO\", \"Actual YES\"]\n)\nprint((conf_mat_rf\/7032)*100)\ncv=StratifiedKFold(n_splits=6)\nclassifier=RandomForestClassifier(n_estimators=113)\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold\ntprs=[]\naucs=[]\nmean_fpr=np.linspace(0,1,100)\nfig,ax=plt.subplots()\nfor i,(train,test) in enumerate(cv.split(X,y)):\n    classifier.fit(X.iloc[train],y.iloc[train])\n    viz=plot_roc_curve(classifier,X.iloc[test],y.iloc[test],name='ROC fold {}'.format(i),alpha=0.3,lw=1,ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n        label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(mean_fpr, mean_tpr, color='b',\n        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                label=r'$\\pm$ 1 std. dev.')\n\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=\"Receiver operating characteristic example\")\nax.legend(loc=\"lower right\")\nplt.show()\n","bb27de19":"rfmodel=RandomForestClassifier(n_estimators= 130, max_features= 6,n_jobs=-1)\nrfmodel.fit(X_train,y_train)\nlg_probs = rfmodel.predict_proba(X_test)\nlg_probs=lg_probs[:,1]\nyhat = rfmodel.predict(X_test)\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, lg_probs)\nlr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n# summarize scores\nprint('RF: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\npyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\npyplot.plot(lr_recall, lr_precision, marker='.', label='RF')\n# axis labels\npyplot.xlabel('Recall')\npyplot.ylabel('Precision')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","29fdfaf4":"rf_conf_matrix  = metrics.confusion_matrix(y, stratified_cv(X, y, XGBClassifier))\nconf_mat_XG = pd.DataFrame(\n    rf_conf_matrix, \n    columns=[\"Predicted NO\", \"Predicted YES\"], \n    index=[\"Actual NO\", \"Actual YES\"]\n)\nprint((conf_mat_XG\/7032)*100)\ncv=StratifiedKFold(n_splits=6)\nclassifier=XGBClassifier()\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold\ntprs=[]\naucs=[]\nmean_fpr=np.linspace(0,1,100)\nfig,ax=plt.subplots()\nfor i,(train,test) in enumerate(cv.split(X,y)):\n    classifier.fit(X.iloc[train],y.iloc[train])\n    viz=plot_roc_curve(classifier,X.iloc[test],y.iloc[test],name='ROC fold {}'.format(i),alpha=0.3,lw=1,ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n        label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(mean_fpr, mean_tpr, color='b',\n        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                label=r'$\\pm$ 1 std. dev.')\n\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=\"Receiver operating characteristic example\")\nax.legend(loc=\"lower right\")\nplt.show()\n","c25dee52":"rfmodel=XGBClassifier(n_jobs=-1)\nrfmodel.fit(X_train,y_train)\nlg_probs = rfmodel.predict_proba(X_test)\nlg_probs=lg_probs[:,1]\nyhat1 = rfmodel.predict(X_test)\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, lg_probs)\nlr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n# summarize scores\nprint('XGB: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\npyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\npyplot.plot(lr_recall, lr_precision, marker='.', label='XGB')\n# axis labels\npyplot.xlabel('Recall')\npyplot.ylabel('Precision')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","9ab552ee":"from sklearn.linear_model import LogisticRegression\nrf_conf_matrix  = metrics.confusion_matrix(y, stratified_cv(X, y, LogisticRegression))\nconf_mat_logis = pd.DataFrame(\n    rf_conf_matrix, \n    columns=[\"Predicted NO\", \"Predicted YES\"], \n    index=[\"Actual NO\", \"Actual YES\"]\n)\nprint((conf_mat_logis\/7032) *100)\ncv=StratifiedKFold(n_splits=6)\nclassifier=LogisticRegression()\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold\ntprs=[]\naucs=[]\nmean_fpr=np.linspace(0,1,100)\nfig,ax=plt.subplots()\nfor i,(train,test) in enumerate(cv.split(X,y)):\n    classifier.fit(X.iloc[train],y.iloc[train])\n    viz=plot_roc_curve(classifier,X.iloc[test],y.iloc[test],name='ROC fold {}'.format(i),alpha=0.3,lw=1,ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n        label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(mean_fpr, mean_tpr, color='b',\n        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                label=r'$\\pm$ 1 std. dev.')\n\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=\"Receiver operating characteristic example\")\nax.legend(loc=\"lower right\")\nplt.show()\n","bcafa426":"lgmodel=LogisticRegression(C= 784.75,penalty='l2',solver='liblinear',n_jobs=-1)\nlgmodel.fit(X_train,y_train)\nlg_probs = lgmodel.predict_proba(X_test)\nlg_probs=lg_probs[:,1]\nyhat1 = lgmodel.predict(X_test)\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, lg_probs)\nlr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n# summarize scores\nprint('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\npyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\npyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n# axis labels\npyplot.xlabel('Recall')\npyplot.ylabel('Precision')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","cc485855":"\nprint((conf_mat_nn\/3039)*100) #As computed in Neural Networks with Keras section above,here as a percentage\n\n# generate a no skill prediction (majority class)\nns_probs = [0 for _ in range(len(y_test))]\n\n# predict probabilities\nlr_probs = nn_model.predict(X_test)\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nlr_auc = roc_auc_score(y_test, lr_probs)\n# summarize scores\nprint('No Skill: ROC AUC=%.3f' % (ns_auc))\nprint('Neural Network: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\npyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='Neural')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","a6070e74":"\nprint((conf_mat_nn\/3039)*100) #As computed in Neural Networks with Keras section above,here as a percentage\n\n# generate a no skill prediction (majority class)\nns_probs = [0 for _ in range(len(y_test))]\n\n# predict probabilities\nlr_probs = nn_model.predict(X_test)\n\nyhat = nn_model.predict(X_test)\nyhat=yhat.round()\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\nlr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n# summarize scores\nprint('Neural Network: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n# plot the precision-recall curves\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\npyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\npyplot.plot(lr_recall, lr_precision, marker='.', label='Neural Network')\n# axis labels\npyplot.xlabel('Recall')\npyplot.ylabel('Precision')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","b1deba18":"weights = pd.Series(lgmodel.coef_[0],\n                 index=X.columns.values)\nprint (weights.sort_values(ascending = False)[:20].plot(kind='bar'))","7c8a7f07":"print (weights.sort_values(ascending = True)[:20].plot(kind='bar'))","a9b2bfe0":"rf =  ensemble.RandomForestClassifier(n_estimators=130,max_features=6, n_jobs=-1)\nrf.fit(X, y)\nfeature_importance = rf.feature_importances_\nfeat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nfeat_importances = feat_importances.nlargest(19)\nfeat_importances.plot(kind='barh' , figsize=(10,10)) ","286ad355":"from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\ndef base_model():\n    nn_model = Sequential()\n    nn_model.add(Dense(64,kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                input_dim=46, activation='relu' ))\n    nn_model.add(Dropout(rate=0.2))\n    nn_model.add(Dense(8,kernel_regularizer=tf.keras.regularizers.l2(0.001),\n                    activation='relu'))\n    nn_model.add(Dropout(rate=0.1))\n    nn_model.add(Dense(1, activation='sigmoid'))\n    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n                  0.001,\n                  decay_steps=(X_train.shape[0]\/32)*50,\n                  decay_rate=1,\n                  staircase=False)\n\n    def get_optimizer():\n        return tf.keras.optimizers.Adam(lr_schedule)\n    def get_callbacks():\n        return [\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=70,restore_best_weights=True)]\n    nn_model.compile(loss = \"binary_crossentropy\", \n                  optimizer = get_optimizer(), \n                  metrics=['accuracy'])\n    return nn_model\n\n\nmy_model = KerasRegressor(build_fn=base_model)    \nmy_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=32,\n                    callbacks= get_callbacks(),verbose=0)\n\nperm = PermutationImportance(my_model, random_state=1).fit(X[:500].values,y[:500].values,verbose=False)\neli5.show_weights(perm, feature_names = X.columns.tolist())","177061ff":"eli5.show_weights(perm, feature_names = X.columns.tolist())","3a7d17b7":"import shap\nfrom tensorflow.keras import Sequential\n# load your data here, e.g. X and y\n# create and fit your model here\n\n# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.DeepExplainer(nn_model,data=X[:500].values)\nshap_values = explainer.shap_values(X.values)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n#shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\n\nshap.summary_plot(shap_values, X, plot_type=\"bar\")","bf33c2e2":"# 3. Performance of the models\n### Confusion Matrix, ROC Curve and Precision Recall Curves\n\nThe confusion matrix and ROC curve give the sense of true positive negative accuracy,however it is precision-recall curve that gives sense of accuracy in imbalanced dataset. In this dataset there are more negative instances than positive and thus Precision-Recall curve shows real performance. \n\nThe ROC can be overly optimistic as it will be more if model predicts negative instances properly but fails on positive instances.","f6e9ff67":"It is important to check the distribution of these features provided in our data to check for biases if the feature values are impartially distributed.\n\n","20df9e1e":"Some cols are binary and some cols have 3 unique values","6f07c174":"Thus, ROC Curve shows that Logistic Regression generalizes better for this particular problem. ","d0967c67":"Assuming there is no rule stating validation data values cannot be dropped, the 4 missing values are dropped.","8740528d":"# 2. Modelling","b7ffb11c":"The effect of greater batch size can be observed from this learning curves.","32610aed":"### 2) Feature importance according to Random Forest","e826aa7b":"Thus, hypertuning does not result in better performance. In fact it gives a much complex model. Hypertuning may not be useful for small datasets to the extent.\nHowever, in the Keras documentation under section of overfit and underfit, it is concluded that accuracy improves as capacity of the neural network is 'small'. Thus, as this is a similarly sized dataset, we can also use a network of 3 hidden layers with 64 to 128 hidden units at most.","ec4e52bd":"Thus, the random forest perfoms best owing to highest F1 scores. F1 scores is a function of precision and recall. XG Boost and GBM may also perform equally if hypertuned.\n\nHigher the F1 score, lower the False Positive and False Negative rates.","84f62cae":"As Keras does not provide a feature importance feature in the documentation i have demonstrated two ways of doing it. Reference is a Stack Over Flow answer.","7c5ed03d":"TotalCharges is an object type? Thus converting it into numeric.","70564cf9":"# Conclusion\n1. It can be seen Total charges is the most important feature and rightfully so. The number one reason customers will 'churn' if they find the service to be expensive or unaffordable. \n2. Tenure is also important, the customers who have been using the sevice for a long time or have long time contracts, which are cheaper in general, are less likely to churn. It is interesting to observe that Tenure is listed as more important for neural network model. \n3. As observed in the EDA, most customers who are on month to month contracts are more likely to churn. Itcan be hypothesized that the reason is to be attributed to personal reasons of customers to have reservations about long term contracts or higher costs per unit time resulting from monthly contracts.\n4. As seen in EDA, other important features are online security, electronic payment method, fiber optic internet service, tech support.\n5. The features which are not important are gender, dependents, partner, streaming TV, backup and device protection.\n\nOffers and improving churn rate:\n1. Discounts: As the most important feature is total charges, followed by monthly charges, potential churners identified through the modelling should be offered huge discount on next month or months contract. This covers 80 % of the reasons identified for churning. For this modelling, the False Negative Rate should be minimised or Recall should be maximised so that the discounts are sent to maximum of the potential churners.\n2. New contract: A six month or four month contract should be implemented. This will encourage the reserved customers who want shorter contracts and will increase their tenure on the service thus making them less likely to churn.\n3. Online Security: This service should be promoted more and offered complimentary\/free for trial periods depending on cost to company. The customers who do not have online security are more likely to churn and thus this offer could be combined with the first one mentioned and discount could only be offered on this.\n4. Fiber optic: The fiber optic internet is costly and thus should either be promoted to appropriate target audience or better technology can be implemented to cut cost on this service. Ultimately the market research team has to decide the break even point for this service, whether it is profiting as much as the loss of customers it is causing.\n5. Select services discounting: In addition to offering disounts on next month contract, discounts can be offered on important services to potential churners. These services include discount on online security, tech support, DSL internet, electronic checks and potentially streaming movies. \n\nAfter these actions are implemented the modelling using newer data needs to performed and analyze the improvements. Another method to quantify the offers to be made is using manually generated features and their effect on the model.","529f9e1e":"### Observation\nThus, we can see that Random Forest and XGBoost are most accurate models, the Logistic Regression generalizes best and predicts both classes, churn and no churn, equally accurately. \n\nThus it is Logistic Regression that will predict better if more positive instances, churn labels, are present in unseen data.","53f1b5b7":"### Hyperparameter Tuning \n","995e7940":"### 1)Feature importance according to Logistic Regression.","d8a72319":"### 1. Random Forest performance","af2cc0a8":"### 2. XGBoost performance","0dc2e82b":"# 4. Feature importance and Conclusion","11c62fb8":"### 2.1 Machine Learning Models","10a6dfd6":"Thus, it can be noted that most customers are young people, only 30% have dependents, and only 10% have no phone service.Thus, the correlations drawn from these variables can be doubted.","0ae85611":"Customer ID is useless for modelling and prediction as it is just arbritary identification.","8ac22fd4":"Linear Classifier model of tensorflow","f9c86511":"# 2.2. Neural Network","b7b7aec0":"1. Linear model Premade Estimator\n2. DNNClassifier Premade Estimator\n3. Keras Sequential model\n4. Hypertuning with Keras","9a678c81":"It is observed probability of churn is high for highly correlated features such as No partner or dependent, no tech support, month to month contract,etc.","5a5dfc68":"Naturally, month to month contract customers tenure lower than customer with two year contracts.","a169db43":"### 3. Logistic Regression Performance","a8ece851":"# References\n1. Random Forest vs Neural Networks for Predicting Customer Churn, Abhinav Sagar, Medium,https:\/\/towardsdatascience.com\/random-forest-vs-neural-networks-for-predicting-customer-churn-691666c7431e\n\n2. Using basic neural networks to predict churn, Laurier Mantel, https:\/\/www.kaggle.com\/lauriermantel\/using-basic-neural-networks-to-predict-churn\n\n3. How to Use ROC Curves and Precision-Recall Curves for Classification in Python, Jason Brownlee, https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n4. Tensorflow, Keras,sklearn documentation\n\n5. Feature importance of Neural Network,StackOverFlow, https:\/\/stackoverflow.com\/questions\/45361559\/feature-importance-chart-in-neural-network-using-keras-in-python#:~:text=It%20most%20easily%20works%20with,using%20it%20is%20very%20straightforward.&text=At%20the%20moment%20Keras%20doesn,to%20extract%20the%20feature%20importance.","11b794cb":"# 1. Exploratory Data Analysis","078dfde2":"### 3) Neural Network Feature Importance","92b2ec41":"Testing different batch sizes : It results in slightly lower performances with both lesser and greater sizes than 32. \n\nThus this code block may be skipped.","bc41ef80":"Thus, there is equal distribution of genders in the dataset and neither is correlated to churn","eefe339b":"Multi-Collinearity:\n1. Monthly charges and Fiber optic: This indicates fiber optic internet is a costly service, hence a contributor to Churn.\n2. Total Charges and Tenure: Naturally, the more years a customer has been subscribed to the service, the more the total charges.\n3. No internet services and no phone services are correlated across different varibles.This can be used to form a seperate feature. However in the distributions it is observed that data points are few and thus it could make the matrix very sparse. If no internet\/phone service in all features is denoted as 'No' it is a loss of data indicating the customer chose to opt out of the service despite having internet service.","027ff7cb":"We can use stratified CV which splits train and test data according to classes, it is useful for imbalanced categorical variables such as this dataset","652a12ed":"### 4. Neural Networks performance","0295d6d1":"\n\n# Introduction\nCustomer attrition or customer churn occurs when customers or subscribers stop doing business with a company or service. Customer churn is a critical metric because it is much more cost effective to retain existing customers than it is to acquire new customers as it saves cost of sales and marketing. Customer retention is more cost-effective as you\u2019ve already earned the trust and loyalty of existing customers.\n\nThere are various ways to calculate this metric as churn rate may represent the total number of customers lost, the percentage of customers lost compared to the company\u2019s total customer count, the value of recurring business lost, or the percent of recurring value lost. However in this dataset, it is defined as a binary variable for each customer and calculating the rate is not the objective. The concept of the churn rate indicates there are factors which influence it and thus the objective here is to identify and quantify those factors.\n\nI had come across this data analysis as part of an assignment for a online recruitment drive and it had to completed in 12 hours. Thus, it is a fairly easy and beginner level project with fewer variables. Since then I have decided to use a nueral network for prediction too.\n\n## Contents\n\n### 1. Exploratory Data Analysis: \nOnly missing values found were in total charges column. I decided to drop those observations as they were very minute compared to the total observations. No features could be dropped owing to multi-collinearity as explained below the heatmap of collinearity. Only customer ID was dropped.\n### 2. Modelling\n#### 2.1 Machine Learning models\nSeveral models were tested and tuning was done for Logistic Regression and Random Forest.\n#### 2.2 Neural Networks\nA basic neural network model is formed using pre-made linear and DNN classifiers and then using Keras Sequential models. Hyperparameter tuning for learning rate and number of layers is done. The performance is constant at 0.79 to 0.82 F1 score.\n### 3. Performance of models\nThe performance of the model is demonstrated using confusion matrix and AUC ROC plots section. Random Forest gives a F1 score of 0.90 using stratified cross validation  Logistic Regression gives ROC score of 0.84 compared to 0.81 of Random Forest. Stratified cross validation is used as it gives more accurate and generalized score of the model and will predict better on unseen data than one set validation especially for imbalanced dataset.\nThe neural networks perform similarly to Logistic Regression. As seen in literature, neural networks are only useful for larger datasets.\n### 4. Feature importance and Conclusion\nThe key factors or the important features are discussed in the Conclusion section. The key factors were Total Charges, tenure, monthly charges, and customer services such as tech support. The potential offers and implementation is discussed in detail in Conclusion section. 5 methods are discussed involving discounts on selected services.\n\n\n","4c298b76":"Only Contract and Internet service have different unique values. The No interenet variable could be assigned to No as in some notebooks on Kaggle. However, dummy variables seems better encoding option instead as in former case there will be loss of data in which customer has chosen not to opt for a service despite having internet service. In case the number of features was larger, label encoding or mapping would be considered.","931c9b9c":"For neural networks, both premade estimators and Keras Sequential models are used. I could not find documentation or references for hypertuning premade estimators, additionally most references I came across are on Convolutional Neural Networks and Image Classification.\nThe Keras models are hypertuned for learning rate and number of layers. The hyper parameter tuned model shows similar performance as the dataset is smaller than usual neural network applications.","a6b779c7":"Thus, it coincides with intuition that more customers who do not have a partner also do not have dependents ","98057e4e":"People having lower tenure and higher monthly charges are tend to churn more. Also as you can see below; having month-to-month contract and fiber obtic internet have a really huge effect on churn probability.","02284ae1":"## Keras model\nA 64-8-1 dense layered model with a decaying learning rate of batch size 32 is used.","aadb10b8":"### Hypertuning Keras\nThe documentation on Keras tuner explains this very well. \nHere the number of hidden units, number of neurons in the hidden layers, learning rate and drop out rates are hypertuned. \n\nAccording to Andrew Ng's course, learning rate is by far most important followed by momentum beta, mini batch size and number of hidden units."}}