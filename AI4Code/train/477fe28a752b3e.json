{"cell_type":{"8f8aba34":"code","ea80585e":"code","f15a6ca7":"code","399daf92":"code","e43c6850":"code","fb54f725":"code","aeb768d5":"code","c84c6681":"code","aceb48ad":"code","430d1734":"code","cdf286d0":"code","85b1eca4":"code","8f448631":"code","c5eadc0c":"code","fdff92ff":"code","1b4b4fc2":"code","ac9596ca":"code","41df4e8f":"code","7f1df3b2":"code","83f58b4b":"code","3826f626":"code","e55dcf52":"code","0498f5c8":"code","e1649777":"code","cb3927ac":"code","e078959a":"code","78659441":"code","78bea0d3":"code","39e0ef97":"code","c0f70eb2":"code","da83a8ea":"code","c1c52c2b":"markdown","da598dd1":"markdown","9d349967":"markdown"},"source":{"8f8aba34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n#submission = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/submission.csv\")\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/test.csv\", parse_dates=[\"Date\"])\ntrain = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-2\/train.csv\", parse_dates=[\"Date\"])\nglobal_data = pd.read_csv(\"..\/input\/externalcountrydata\/Global_Data_by_Country_2019.csv\")\ncountry_info=pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\n\n","ea80585e":"train.loc[train[\"Province_State\"].isnull(), \"Province_State\"]=train.loc[train[\"Province_State\"].isnull(), \"Country_Region\"]\n\ntrain.rename(columns = {'Country_Region':'Country', 'Province_State':'Province'}, inplace = True) \n\n\n","f15a6ca7":"\ntrain=train.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\n","399daf92":"country_info.info()","e43c6850":"#train=train.merge(country_info[[\"country\", \"medianage\"]], how='left', left_on='Country', right_on='country' )\n\n#medianage, smoker, hospibed, lung","fb54f725":"train.info()","aeb768d5":"test.rename(columns = {'Country_Region':'Country', 'Province_State':'Province'}, inplace = True) \n#test=test.merge(country_info[[\"country\", \"medianage\"]], how='left', left_on='Country', right_on='country' )\ntest.loc[test[\"Province\"].isnull(), \"Province\"]=test.loc[test[\"Province\"].isnull(), \"Country\"]\n\nX_test=test.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\nX_test=X_test.drop(\"Population\", axis=1)\nX_test=X_test.drop(\"CountryName\", axis=1)\nX_test=X_test.rename(columns={\"ExtraColumn\": \"Population\"})\n","c84c6681":"del global_data","aceb48ad":"train=train.drop(\"Population\", axis=1)\ntrain=train.drop(\"CountryName\", axis=1)\n\ntrain=train.rename(columns={\"ExtraColumn\": \"Population\"})\n","430d1734":"train.head()","cdf286d0":"mindates = train[train[\"ConfirmedCases\"]>0].groupby(['Province'])[\"Date\"].min()\nmindates.reset_index()\nmindatesDF = mindates.to_frame()\nmindatesDF.rename(columns={\"Date\":\"MinDate\"}, inplace=True)\ntrain=train.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\ntrain[\"DaysFrom1stCase\"]=(train[\"Date\"]-train[\"MinDate\"]).dt.days\ntrain.loc [train[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\nfirst_day=train[train[\"ConfirmedCases\"]>0].Date.min()\ntrain[\"DaysFromStart\"]=(train[\"Date\"]-first_day).dt.days \n## after version 2 ###\n#train=pd.get_dummies(train, prefix='prov', prefix_sep='_', dummy_na=True, columns=\"Province\", sparse=False, drop_first=False, dtype=None)\n\n","85b1eca4":"train.head()","8f448631":"from sklearn.preprocessing import LabelEncoder \n\nlabelencoder = LabelEncoder()\n","c5eadc0c":"province=labelencoder.fit_transform(train[\"Province\"])\ntrain=pd.concat([train, pd.DataFrame(province)], axis=1)","fdff92ff":"\ntrain=train.drop([\"Country\",\"MinDate\", \"Province\"], axis=1)\n#train=train.drop([\"Country\",\"MinDate\"], axis=1)","1b4b4fc2":"train.rename(columns={0:\"Province\"})","ac9596ca":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\n\ny_train_CC=train.loc[:,\"ConfirmedCases\"]\ny_train_F=train.loc[:, \"Fatalities\"]\nX_train=train.drop([\"ConfirmedCases\", \"Fatalities\", \"Id\", \"Date\"], axis=1)","41df4e8f":"for col in X_train.columns:\n    X_train[col]=X_train[col].fillna(X_train[col].median())","7f1df3b2":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n#pca = PCA(n_components=7)\n#pca.fit(X_train)\n#X_train= pd.DataFrame(pca.transform(X_train))\n\n\n\nX_train_real_CC, X_test_val_CC, y_train_real_CC, y_test_val_CC = train_test_split(\n        X_train, y_train_CC, test_size=0.3, random_state=0)\n\n\n\nlgb_train_CC = lgb.Dataset(X_train_real_CC, y_train_real_CC)\nlgb_eval_CC = lgb.Dataset(X_test_val_CC, y_test_val_CC, reference=lgb_train_CC)\n\n\nX_train_real_F, X_test_val_F, y_train_real_F, y_test_val_F = train_test_split(\n        X_train, y_train_F, test_size=0.3, random_state=0)\n\nlgb_train_F = lgb.Dataset(X_train_real, y_train_real_F)\nlgb_eval_F = lgb.Dataset(X_test_val, y_test_val_F, reference=lgb_train_F)\n\n\n#X_train.head()","83f58b4b":"params_CC = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.3,\n        'num_leaves': 30,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\n\nparams_F = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.28,\n        'num_leaves': 35,\n        'min_data_in_leaf': 2,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\n\ngbm_CC = lgb.LGBMRegressor(n_jobs=-1)\ngbm_F = lgb.LGBMRegressor(n_jobs=-1)\n\nparam_grid = {\n    \"num_leaves\" : np.linspace(10, 200, 4, dtype=np.int32),\n    'learning_rate': np.linspace(0.1, 1, 5),\n    'n_estimators': np.linspace(10, 1000, 5, dtype=np.int32),\n    'early_stopping_rounds' : [20],\n}\n\ngbm_CC = GridSearchCV(gbm_CC, param_grid, cv=3, scoring=\"neg_mean_squared_error\", verbose=100, n_jobs=-1)\ngbm_CC.fit(X_train_real_CC, y_train_real_CC, eval_set=[(X_test_val_CC, y_test_val_CC)], eval_metric=\"rmse\")\nprint('Best parameters:', gbm_CC.best_params_)\n\ngbm_F = GridSearchCV(gbm_F, param_grid, cv=3, scoring=\"neg_mean_squared_error\", verbose=100, n_jobs=-1)\ngbm_F.fit(X_train_real_F, y_train_real_F, eval_set=[(X_test_val_F, y_test_val_F)], eval_metric=\"rmse\")\nprint('Best parameters:', gbm_F.best_params_)\n","3826f626":"\ntest.head()","e55dcf52":"\n## add days since first case column ##\nX_test=X_test.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\nX_test[\"DaysFrom1stCase\"]=(X_test[\"Date\"]-X_test[\"MinDate\"]).dt.days\nX_test.loc [X_test[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\nX_test[\"DaysFromStart\"]=(X_test[\"Date\"]-first_day).dt.days \n\n#X_test=pd.get_dummies(X_test, prefix='prov', prefix_sep='_', dummy_na=True, columns=\"Province\", sparse=False, drop_first=False, dtype=None)\n\nX_test=X_test.drop([\"Country\", \"MinDate\"], axis=1)\n\nX_test=X_test.drop([\"ForecastId\", \"Date\"], axis=1)\n\n\n\n\n","0498f5c8":"province=labelencoder.transform(X_test[\"Province\"])\nX_test=pd.concat([X_test,pd.DataFrame(province) ], axis=1)\nX_test=X_test.drop([\"Province\"], axis=1)\nfor col in X_test.columns:\n    X_test[col]=X_test[col].fillna(X_test[col].median())","e1649777":"X_test.head()","cb3927ac":"#X_test= pca.transform(X_test)\n\n#y_pred=regressor.predict(X_test)\n#y_pred_CC = gbm_CC.predict(X_test, num_iteration=gbm_CC.best_iteration)\n#y_pred_F = gbm_F.predict(X_test, num_iteration=gbm_F.best_iteration)\n\n\ny_pred_CC = gbm_CC.predict(X_test)\ny_pred_F = gbm_F.predict(X_test)\n","e078959a":"forecastId=test.ForecastId.to_numpy()\nsubmission_CC=pd.DataFrame(y_pred_CC)\nsubmission_CC=submission_CC.rename(columns={0:\"ConfirmedCases\"})\nsubmission_F=pd.DataFrame(y_pred_F)\nsubmission_F=submission_F.rename(columns={0:\"Fatalities\"})\nforecastIdDF=pd.DataFrame(forecastId)\nforecastIdDF=forecastIdDF.rename(columns={0:\"ForecastId\"})\nsubmission=pd.concat([forecastIdDF, submission_CC, submission_F ], axis=1)","78659441":"submission.head()","78bea0d3":"submission.to_csv(\"submission.csv\", index=False)","39e0ef97":"submission.info()","c0f70eb2":"test=pd.concat([test, submission ], axis=1)","da83a8ea":"test[test.Province==\"Algeria\"]","c1c52c2b":"params_CC = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.3,\n        'num_leaves': 30,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\n\nparams_F = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.28,\n        'num_leaves': 35,\n        'min_data_in_leaf': 2,\n        'num_iteration': 100,\n        'verbose': 20\n}\ngbm_CC = lgb.train(params_CC,\n            lgb_train_CC,\n            num_boost_round=100,\n            valid_sets=lgb_eval_CC,\n            early_stopping_rounds=10)\n\ngbm_F = lgb.train(params_F,\n            lgb_train_F,\n            num_boost_round=100,\n            valid_sets=lgb_eval_F,\n            early_stopping_rounds=10)","da598dd1":"# Introduction\nI'm focussing on having the most meaningful external data added to the original data set, use simple off the shelf tools to make my predictions. Most of the data I've got are from https:\/\/data.worldbank.org\/ from http:\/\/www.stats.gov.cn\/english\/Statisticaldata\/AnnualData\/ and https:\/\/catalog.data.gov\/dataset\/age-adjusted-death-rates-and-life-expectancy-at-birth-all-races-both-sexes-united-sta-1900\n\nJoined and cleaned the data on Excel before uploading","9d349967":"**Transform the Test Dataset before prediction**"}}