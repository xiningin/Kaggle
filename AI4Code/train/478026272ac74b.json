{"cell_type":{"a4d51185":"code","3aa68f8f":"code","527a3e9c":"code","28b72ee5":"code","232ed868":"code","253303ec":"code","b65b35a0":"code","a3151d26":"code","680046af":"code","6ca0f15d":"code","9dc43ffd":"code","c7892489":"code","28b80256":"code","20773d2c":"code","aac933e8":"code","9a2d346b":"code","a03a154e":"code","9b527618":"code","a145493b":"code","a6ac97cd":"code","cc31ebc4":"code","55f85ec0":"code","aaa1e53c":"code","311599da":"code","9901607f":"code","2adb7aa1":"code","d1391d96":"code","d73c1c05":"code","966ff012":"code","11d34841":"code","eab39bda":"code","5799d210":"code","f97c81bb":"code","9a4de05d":"code","ef2fa490":"code","2ea0cfdf":"code","69771f6c":"code","9a4c2d2c":"code","735770de":"code","523060b4":"code","c179f4d4":"code","81b34b00":"code","894fce85":"code","b42fbaa4":"code","a9a546f9":"code","e65e501e":"code","acdc4c54":"code","b65d43d0":"code","0e614d18":"code","a9307c12":"code","6f0c42f2":"code","43b4be19":"code","ea6ca33e":"code","e570e15b":"code","15ae88d5":"code","4c1e4e9b":"code","067e09ed":"code","9ee68016":"code","e6c0128d":"code","36583869":"code","8d900e59":"code","55005d99":"code","c0cadf81":"code","5f1ae8b3":"markdown","76a99de3":"markdown","86e2b138":"markdown","fbee54b5":"markdown","cbee7817":"markdown","ef3bc148":"markdown","34f07c8f":"markdown","f628ba41":"markdown","d796fd4f":"markdown","19bc366f":"markdown","6ca8faa4":"markdown","01935952":"markdown","88899e7b":"markdown","f2324abf":"markdown","dc650825":"markdown"},"source":{"a4d51185":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3aa68f8f":"#from IPython.display import FileLink, FileLinks\n\n#p_df.to_csv('\/path\/to\/data.csv', index=False)\n#p_df.to_excel('\/path\/to\/data.xlsx', index=False)\n\n#FileLinks('\/path\/to\/')\n","527a3e9c":"import re\nimport gensim\nimport nltk\nimport gensim\nimport pickle\nimport pyLDAvis\nimport numpy as np\nimport pandas as pd\nimport pyLDAvis.gensim\nfrom pprint import pprint\nfrom nltk.corpus import stopwords\nfrom gensim import corpora\nfrom gensim.utils import simple_preprocess\nfrom sklearn.naive_bayes import BernoulliNB\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom gensim.utils import simple_preprocess\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom sklearn.metrics import classification_report, roc_curve\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\n#nltk.download('stopwords')\n#The previous is already up-to-date\n#add Word_Vectorizer\n#np.randomseed(42)\n#how do I assign a random seed value?","28b72ee5":"cd \/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/","232ed868":"headlines = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)\nheadlines_v2 = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)","253303ec":"headlines['article_link']","b65b35a0":"stemmer = PorterStemmer()\n#probably small enough that I can use a Lemmatizer","a3151d26":"#nltk.download('wordnet')\n#haven't seen an update phrased like this before...","680046af":"'or something' ' liar'  ' cocky ' ' naaah'   'Oooops' ' \"scare quotes\"'\n#normally, stop words are sarcastic words, for this project the reverse is true\n\n#elsewise stopwords may be sarcastic and so, informative here","6ca0f15d":"headlines['website_o'] = headlines['article_link'].str.extract(r'(theonion)',)","9dc43ffd":"headlines['website_hp'] = headlines['article_link'].str.extract(r'(huffingtonpost)',)\nheadlines['website'] = headlines.website_o.str.cat(headlines.website_hp, na_rep=' ')\nheadlines.drop(columns=['article_link','website_o', 'website_hp'], axis=1, inplace=True)","c7892489":"pd.set_option('max_colwidth', None)\n","28b80256":"print(headlines['headline'])\n#how do I view all of it?","20773d2c":"headlines.head()","aac933e8":"#Scare quotes and 'or something' in the title are dead give aways.  I do not yet know how to code for them though","9a2d346b":"data = headlines[['is_sarcastic', 'headline']]","a03a154e":"sentences = headlines['headline']","9b527618":"#sentences","a145493b":"#data_words = np.array(headlines['headline']).reshape(-1,1)\n#data_words = headlines['headline'].astype(str)\nword_list = headlines['headline'].tolist()","a6ac97cd":"#word_list","cc31ebc4":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'the', 'it', 'in', 'to'])\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        # deacc=True removes punctuations\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if word not in stop_words] for doc in texts]\ndata_list = headlines['headline'].tolist()\ndata_words = list(sent_to_words(data_list))\n# remove stop words\ndata_words = remove_stopwords(data_words)\nprint(data_words[:1][0][:30])","55f85ec0":"\"\"\"lda_model...                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)\"\"\"\n\"\"\"top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n\n\"\"\"\n\"\"\"lda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)\n\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])\n\"\"\"","aaa1e53c":"sns.countplot(df.is_sarcastic ,data=df);\n\n#It stripped my data frome","311599da":"data.head()","9901607f":"tokens = [word_tokenize(sen) for sen in data['headline']]\n","2adb7aa1":"words_tokenized = [simple_preprocess(word) for word in word_list]","d1391d96":"train_X = data.loc[:2600,['headline']]\ntrain_y = data.loc[:2600,'is_sarcastic']\n\ntest_X = data.loc[2600:,['headline', ]]\ntest_y = data.loc[2600:,'is_sarcastic']","d73c1c05":"X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.20, stratify = train_y, random_state=42)\nprint(f'Training set size = {len(X_train)}')\nprint(f'Validation set size = {len(X_val)}')","966ff012":"stopwords = set(STOPWORDS)\n\nprint(STOPWORDS)","11d34841":"num_words = 10000\nmaxlen=100\npadding='post'\ntruncating='post'\noov_tok = \"<OOV>\"\nembedding_dim = 16\ndropout_rate = 0.30\n\n#tokenizer will automatically help in choosing most frequent words, to handle sentences that are not in the training set we use out of vocabulary(\"<OOV>\")\n#create the Tokenizer and define an out of vocabulary token\ntokenizer = Tokenizer(num_words = num_words, oov_token=oov_tok)\n\n#fitting the sentences to using created tokenizer object\ntokenizer.fit_on_texts(X_train.headline)\n\n#full list of words is available as the tokenizer's word index\nword_index = tokenizer.word_index\n\n#creates sequence of tokens representing each sentence\nX_train_sequences = tokenizer.texts_to_sequences(X_train.headline)\n\n#padding them with zeros and\/or truncating them. If don't provide the max length, then the sequences are padded to match the length of the longest sentence.\nX_train_padded = pad_sequences(X_train_sequences, maxlen=100, dtype='float32', padding=padding, truncating=truncating, value=0.0)\n\nX_val_sequences = tokenizer.texts_to_sequences(X_val.headline)\nX_val_padded = pad_sequences(X_val_sequences, maxlen=100, dtype='float32', padding=padding, truncating=truncating, value=0.0)\n\ny_train = np.asarray(np.array(y_train)).astype(np.float32)\ny_val = np.asarray(np.array(y_val)).astype(np.float32)\n","eab39bda":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","5799d210":"num_epochs = 30\ninitial_lr = 0.001\n\n#Early stop the training if there's no improvement in model performance for 20 epochs\nearly = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: initial_lr * 10**(epoch\/20))\n\n# #Reduce model learning rate if validation loss reach plateau\n# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                               patience=20, min_lr=0.001)\n\n\nhistory = model.fit(X_train_padded, y_train, epochs = num_epochs, validation_data=(X_val_padded, y_val), callbacks= [early,lr_schedule])","f97c81bb":"def plot_graphs(history, string):\n    \n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","9a4de05d":"#Is that unusual spike in accuracy from a small sample size?","ef2fa490":"X_test_sequences = tokenizer.texts_to_sequences(test_X.headline)\n\n#padding them with zeros and\/or truncating them. If don't provide the max length, then the sequences are padded to match the length of the longest sentence.\nX_test_padded = pad_sequences(X_test_sequences, maxlen=100, dtype='float32', padding=padding, truncating=truncating, value=0.0)\ny_test = np.asarray(np.array(test_y)).astype(np.float32)","2ea0cfdf":"y_predict = model.predict(X_test_padded)\n\npredict = np.round(y_predict,0).flatten()\n\nresult = pd.DataFrame()\nresult['predict'] = predict\nresult['actual'] = y_test\nresult","69771f6c":"for i in range(10):\n    print(test_X.headline.to_list()[i])\n    print(y_predict[i],'\\n')","9a4c2d2c":"matrix = pd.crosstab(result.predict, result.actual, rownames=['Predicted'], colnames=['Actial'])\nsns.heatmap(matrix, annot=True, fmt='g')\nplt.title(f'Original Dataset')\nplt.show()","735770de":"print(classification_report(y_test, predict))\n","523060b4":"fpr, tpr, _ = roc_curve(y_test, y_predict)\n\nplt.title('ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr, label='Embeddings with Global Average Pooling layer')\nplt.legend(loc='lower right')\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()","c179f4d4":"dictionary = corpora.Dictionary()","81b34b00":"BoW_corpus = [dictionary.doc2bow(word, allow_update=True) for word in words_tokenized]\n","894fce85":"#word_tokenize(data_words)\n#BoW_corpus","b42fbaa4":"#id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n#print(id_words)\n#It is now, human readable\n#It will *not* work in the operator; however, because it is *not* a Dictionary","a9a546f9":"id2word = corpora.Dictionary(data_words)\n","e65e501e":"#Lemmatize the Document\n#lemmatizer = WordNetLemmatizer()\n#Documents are typically lemmatized or tokenized...but not both","acdc4c54":"# Import the wordcloud library\nfrom wordcloud import WordCloud\n# Join the different processed titles together.\nlong_string = ','.join(list(headlines['headline'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","b65d43d0":"#This looks like its creating vectors for the words\nimport gensim.corpora as corpora\n# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n# Create Corpus","0e614d18":"#texts = str.strip(a) for a in sub_texts","a9307c12":"texts.info()\n#print(texts)","6f0c42f2":"id2word.filter_extremes(no_below=20, no_above=0.9, keep_n=100000)\n#id2word.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\n#This prevents somewords from getting lost in the dictionary","43b4be19":"# Term Document Frequencytexts = [str.split(a) for a in data_words]\n\n#corpus = [id2word.doc2bow(text) for text in texts]\ncorpus = [id2word.doc2bow(text) for text in data_words]\n# View\nprint(corpus[:1][0][:30])\n#Vectorizing words is a method for, in this case ,finding opposite words, in an attempt  to hopefully find a cleaner separation between the data\n#It is not showing me the original words here - did I type something badly?\n#There is a problem because corpus is empty...","ea6ca33e":"\"\"\"#Define a Helper Function that will allow us to refine the model\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])\"\"\"","e570e15b":"id_words.info","15ae88d5":"\"\"\"#Visualize the top ten words *without the wordcloud* to refine the model\n#also, stolen\ncount_vectorizer= Count_Vectorizer...\nwords, word_values = get_top_n_words(n_top_words=15,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data)\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()\"\"\"","4c1e4e9b":"#This is an alternite parsing-then counting code. \n#t-SNE is more advanced than what I am used to.\n\"\"\"top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lda_categories, lda_counts);\nax.set_xticks(lda_categories);\nax.set_xticklabels(labels);\nax.set_title('LDA topic counts');\nax.set_ylabel('Number of headlines');\"\"\"\n\"\"\"top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)\"\"\"\n\"\"\"words = np.asanyarray(word_list)\"\"\"","067e09ed":"#X is the feature set of potentiall sarcastic words","9ee68016":"#X = headlines[['headline']]\n#y = headlines['is_sarcastic']\n#The training and test sets should actually come form the LDApyVis machine","e6c0128d":"from sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X, y)\n\n","36583869":"#Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n# a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n#These are measures of the goodness of fit","8d900e59":"#good perplexity\n#not very good coherence score","55005d99":"X = headlines[['headline']]\ny = headlines['is_sarcastic']","c0cadf81":"#Since its a binary classification, I assume that it is best to use Kullback-Liebler Divergence","5f1ae8b3":"**Clean Data, Creat Dictionary, FitLDA, Create Document Term Matrix, Instantiate LDA, Fit LDA**","76a99de3":"**Data Preparation**","86e2b138":"Versions 1 through 99 I was trying to apply an LDA model to the data.  The multinomial distribution can treat data as vectors but the binomial cannot - this is one of the main ways that I came to realize I should have been using Naive Bayes","fbee54b5":"**Notebook Notes**","cbee7817":"\"\"\"Wordclouds are so messy that many data scientists refuse to use them.  I couldn't find a suitable substitute here.  This confirms the common sense notion that most given words ar enot sarcastic\"\"\"","ef3bc148":"\"This word cloud seems totally uninformative, not a single Sarcastic word\"","34f07c8f":"**Priors on the Table**","f628ba41":"**Fit the Appropriate Model**","d796fd4f":"    **Redo as a CNN Model**","19bc366f":"**Visualization**","6ca8faa4":"Check the Goodness of Fit","01935952":"**Remove Stop Words**","88899e7b":"Make Predictions","f2324abf":"**Vectorize the words**","dc650825":"**Tokenize the Document**"}}