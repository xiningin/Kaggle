{"cell_type":{"c8199c7f":"code","4f11a8db":"code","12ac9bd1":"code","f331f256":"code","8f6c8cae":"code","48849222":"code","041df6aa":"code","2b5ebe11":"code","c35327a7":"code","824e7517":"code","c86aff98":"code","c6813f28":"code","788ba767":"code","1955f3e6":"code","065da123":"code","df29a248":"code","3d78226b":"code","9bcda14d":"code","bd8cf0e7":"code","a6416e50":"code","68fe4319":"code","ad6b952d":"code","c79583f8":"code","c48759c7":"code","53814b19":"code","2a2a7f1f":"code","ef38dcb5":"code","a9ce3872":"code","fa3c1330":"code","1dd41f24":"code","02d4cb2d":"code","e3fb5a73":"code","cb440534":"code","18e2775a":"code","22d59e45":"code","84c27aa9":"markdown","61f45d24":"markdown","a172cf30":"markdown","8ac59220":"markdown","d60ca3e2":"markdown","aa31ff12":"markdown","d95c9249":"markdown","6776cb6f":"markdown","5c7900cd":"markdown","90ce9612":"markdown","bc4eb530":"markdown","04dea429":"markdown","16966270":"markdown","09f5c6a3":"markdown","1088381a":"markdown","36c8d65c":"markdown","ade8120d":"markdown","4bc5c4ec":"markdown","c738fbc0":"markdown","dc2519ed":"markdown","531ca10a":"markdown","0dcedd92":"markdown","12b9b567":"markdown","c4ba37c3":"markdown","313075ac":"markdown","320392de":"markdown"},"source":{"c8199c7f":"import pandas as pd\nimport numpy as np\n\n# What do we say to python warnings? NOT TODAY\nimport warnings\nwarnings.filterwarnings('ignore')","4f11a8db":"def select_features(df, target, th):\n    \"\"\"\n    Select features.\n    \"\"\"\n    # Select rows with our target value\n    proc_df = df[df[target].isna() == False]\n    \n    # Remove useless columns\n    to_drop = [col for col in proc_df.columns if ((\"(F)\" in col) or (\"(fpm)\" in col) or (\"Thermal\" in col) or (\"Air movement\" in col)) and (col != target)] + [\"Database\", \"Publication (Citation)\", \"Data contributor\"]\n    proc_df = proc_df.drop(to_drop, axis=1)\n    \n    # Remove columns with a lot of missing values\n    # Get columns with less than 30% of data missing\n    s = (proc_df.isna().sum() \/ len(proc_df) * 100 < th)\n    to_keep = list(s[s].index)\n    proc_df = proc_df[to_keep]\n    \n    return proc_df","12ac9bd1":"def group_koppen_categories(category):\n    if \"A\" in category:\n        return \"A\"\n    elif \"B\" in category:\n        return \"B\"\n    elif \"C\" in category:\n        return \"C\"\n    elif \"D\" in category:\n        return \"D\"\n    elif \"E\" in category:\n        return \"E\"\n    else:\n        return None\n    \ndef handle_categorical_features(df):\n\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import OneHotEncoder\n    \n    # Drop climate\n    df.drop(\"Climate\", axis=1, inplace=True)\n    # Group koppen climate\n    df[\"Koppen climate classification\"] = [group_koppen_categories(category) for category in df[\"Koppen climate classification\"]]\n    # Fill missing cooling strategy in city Harbin\n    df.loc[df.City == \"Harbin\", \"Cooling startegy_building level\"] = \"Naturally Ventilated\"\n    # Fill missing city in Malaysia\n    df.loc[df.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n    \n    # Input mode by city in Season and Cooling strategy\n    # Get list of categorical variables with missing values\n    cols = [\"Season\", \"Cooling startegy_building level\"]\n    # Input mode for each city\n    for city in df.City.unique():\n        # Filter data of selected city\n        temp = df.loc[df.City == city, cols]\n        # Create imputer\n        imputer = SimpleImputer(strategy='most_frequent')\n        # Input missing values with mode\n        imputed = pd.DataFrame(imputer.fit_transform(temp))\n        # Rename columns and index\n        imputed.columns = temp.columns\n        imputed.index = temp.index\n        # Replace in dataframe\n        df.loc[df.City == city, cols] = imputed\n        \n    # Encode\n    # Get list of categorical variables\n    s = (df.dtypes == 'object')\n    cols = list(s[s].index)\n    # One Hot Encoder\n    for col in cols:\n        # Create encoder\n        OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        # Transform\n        OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[[col]]))\n        # Get categories names and rename\n        names = OH_encoder.categories_\n        OH_cols.columns = names\n        OH_cols.index = df.index\n        # Add encoded columns\n        df[list(names[0])] = OH_cols\n\n    # Drop un-encoded column\n    df.drop(cols, axis=1, inplace=True)\n    \n    return df\n    \ndef get_balance_dataset_index(df, target):\n    \"\"\"\n    df: the dataset to balance\n    target: the name of the target column\n    \"\"\"\n    \n    # Get count for each category\n    value_counts = df.value_counts(target).to_dict()\n    \n    # List comprehension to find the key with the minimum value (count)\n    min_category = [k for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    min_count = [v for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    \n    # For each category in your target\n    dfs = []\n    for key in value_counts:\n        if key == min_category:\n            df1 = df[df[target] == min_category]\n        else:\n            df1 = df[df[target] == key].sample(min_count, random_state=55)\n        dfs.append(df1)\n    \n    dfb = pd.concat(dfs)\n    print(f\"Your balance dataset: {dfb.value_counts(target).to_dict()}\")\n    \n    return dfb.index","f331f256":"def handle_numerical_features(df, df_raw):\n    \"\"\"\n    Input mean by city.\n    \"\"\"\n    \n    cols = ['Clo',\n             'Met',\n             'Air temperature (C)',\n             'Relative humidity (%)',\n             'Air velocity (m\/s)',\n             'Outdoor monthly air temperature (C)',\n             'City',\n             'Country']\n\n    df1 = df_raw[cols]\n\n    # Input mode in missing Malaysia city\n    df1.loc[df1.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n\n    cols = ['Clo',\n             'Met',\n             'Air temperature (C)',\n             'Relative humidity (%)',\n             'Air velocity (m\/s)',\n             'Outdoor monthly air temperature (C)']\n\n    # Input mean by city in numerical features\n    # Input mode for each city\n    for city in df1.City.unique():\n        # Filter data of selected city\n        temp = df1.loc[df1.City == city, cols]\n        # Serie with the mean per column\n        means = temp.mean()\n        # Fill the missing values with the mean\n        temp = temp.fillna(means)\n        # Replace in dataframe\n        df1.loc[df1.City == city, cols] = temp\n\n    # there are cities with all null\n    df1 = df1.fillna(df1.mean())\n\n    # Add to dataset\n    df = df.drop(cols, axis=1)\n    df[cols] = df1.drop([\"City\",\"Country\"], axis=1) # we don't need the column City anymore, we have it encoded\n    \n    return df","8f6c8cae":"def simple_fe(df, df_raw):\n    \"\"\"\n    Add simple features to data frame.\n    \"\"\"\n    # New categorical features\n    # Load countries data\n    countries = pd.read_csv(\"..\/input\/latitude-and-longitude-for-every-country-and-state\/world_country_and_usa_states_latitude_and_longitude_values.csv\",\n                           usecols=[\"country\",\"latitude\"])\n    # Rename column\n    countries = countries.rename(columns={\"country\":\"Country\"})\n    # Merge\n    df_country = pd.merge(df_raw[[\"Country\"]], countries, how=\"left\", on=\"Country\")\n    # Add feature\n    df_country[\"is_northern\"] = [1 if lat > 0 else 0 for lat in df_country.latitude]\n    # Fill missings\n    df_country.loc[df_country[\"Country\"].isin([\"USA\", \"UKA\"]), [\"latitude\"]] = 1\n    # Join\n    df = df.join(df_country[[\"is_northern\"]])\n    \n    # New numerical feature\n    df[\"clo_met_ratio\"] = df.Clo \/ df.Met\n    \n    # Group features\n    mean_air_temp = pd.DataFrame(df_raw.groupby([\"City\",\"Season\"])[\"Air temperature (C)\"].mean()).reset_index().rename(columns={\"Air temperature (C)\":\"mean_air_temp\"})\n    mean_rel_humidity = pd.DataFrame(df_raw.groupby([\"City\",\"Season\"])[\"Relative humidity (%)\"].mean()).reset_index().rename(columns={\"Relative humidity (%)\":\"mean_rel_humidity\"})\n    means_df = pd.merge(df_raw, mean_air_temp, how=\"left\", on=[\"City\",\"Season\"]).merge(mean_rel_humidity, how=\"left\", on=[\"City\",\"Season\"])[[\"mean_air_temp\", \"mean_rel_humidity\"]]\n    means_df.fillna(means_df.mean(), inplace=True)\n    df = df.join(means_df)\n    \n    return df","48849222":"target = \"Thermal sensation acceptability\"\n\n# Load original data\ndf_raw = pd.read_csv(\"\/kaggle\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\", low_memory=False)\n# Select features\ndf = select_features(df_raw, target, 25)\n# Handle categorical features\ndf = handle_categorical_features(df)\n#get indexes from balanced dataset, this is to use always the same rows to predict\nidx = get_balance_dataset_index(df, target)\n# Handle numerical features\ndf = handle_numerical_features(df, df_raw)\n# Add simple features\ndf = simple_fe(df, df_raw)","041df6aa":"df.head()","2b5ebe11":"df.info()","c35327a7":"from sklearn.preprocessing import MinMaxScaler","824e7517":"cols = [\"Clo\",               \n        \"Met\",               \n        \"Air temperature (C)\",\n        \"Relative humidity (%)\",\n        \"Air velocity (m\/s)\",\n        \"Outdoor monthly air temperature (C)\",\n        \"clo_met_ratio\",\n        \"mean_air_temp\",\n        \"mean_rel_humidity\"]","c86aff98":"# Select the columns to scale\nX = df[cols].copy()\nX.head()","c6813f28":"# Create the scaler\nscaler = MinMaxScaler()\n\n# Fit your data (returns an array, we are creating a dataframe here)\nX_scaled = pd.DataFrame(scaler.fit_transform(X))\n\n# Add the columns names (where removed in previous step)\nX_scaled.columns = X.columns\n\n# Check it out\nX_scaled.head()","788ba767":"from sklearn.decomposition import PCA","1955f3e6":"# Create principal components object\npca = PCA(random_state=55)\n\n# Fit your data\nX_pca = pca.fit_transform(X_scaled)\n\n# Create PCA dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca,columns=component_names)\nX_pca.index = df.index\n\n# Check it out\nX_pca.head()","065da123":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings","df29a248":"import matplotlib.pyplot as plt","3d78226b":"# This a function to plot\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","9bcda14d":"# explained variance\nplot_variance(pca)","bd8cf0e7":"# Add them to the data set\ndf[[f\"PC{i+1}\" for i in range(X_pca.shape[1])]] = X_pca\ndf.head()","a6416e50":"# Drop\ndf.drop(cols, axis=1, inplace=True)","68fe4319":"from sklearn.cluster import KMeans","ad6b952d":"inertia = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(X_scaled)\n    inertia.append(kmeanModel.inertia_)","c79583f8":"plt.figure(figsize=(6,4))\nplt.plot(K, inertia, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","c48759c7":"# Create cluster objet\nkmeans = KMeans(n_clusters=2, random_state=55)\n\n# Fit your data\nX_scaled[\"Cluster\"] = kmeans.fit_predict(X_scaled)\nX_scaled.index = df.index\n\n# Add column with the clusters found\ndf[\"Cluster\"] = X_scaled[\"Cluster\"].astype(\"int\")\n\n#Check it out\ndf.head()","53814b19":"df.info()","2a2a7f1f":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score, classification_report","ef38dcb5":"dfb = df.loc[idx]","a9ce3872":"dfb.head()","fa3c1330":"X = dfb.drop(\"Thermal sensation acceptability\", axis=1)\ny = dfb[\"Thermal sensation acceptability\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=55)","1dd41f24":"# Define the model\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, \n                         objective=\"binary:logistic\", eval_metric=\"auc\", use_label_encoder=False)\n\n# Perform cross validation with 5 folds\nprint(\"Training...\")\nscores = cross_val_score(my_model, \n                          X_train, y_train,\n                          cv=5,\n                          scoring='roc_auc')\nprint(\"...done.\")","02d4cb2d":"print(f\"Scores: {scores}\")\nprint(f\"Mean scores: {np.mean(scores)}\")","e3fb5a73":"# This is the model we are going to train (a simple one)\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, objective=\"binary:logistic\")\n\n# Train\nprint(\"Training...\")\nmy_model.fit(X_train, y_train, verbose=False)\nprint(\"...done\")","cb440534":"# And we predict\nprediction = pd.DataFrame({\"y_pred\": my_model.predict(X_test), \"y_real\": y_test})\nroc_auc_score(prediction.y_real, prediction.y_pred)","18e2775a":"print(classification_report(prediction.y_real, prediction.y_pred))","22d59e45":"cols = [\"Clo\",               \n        \"Met\",               \n        \"Air temperature (C)\",\n        \"Relative humidity (%)\",\n        \"Air velocity (m\/s)\",\n        \"Outdoor monthly air temperature (C)\",\n        \"clo_met_ratio\",\n        \"mean_air_temp\",\n        \"mean_rel_humidity\"]\n\ndef pca_and_clustering(df, cols):\n    \"\"\"\n    Applies PCA and clustering to add features to data set.\n    \n    df: dataset\n    cols: list of continuous columns\n    \"\"\"\n    # Import packages\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.decomposition import PCA\n    from sklearn.cluster import KMeans\n    \n    # Scale data\n    # Select the columns to scale\n    X = df[cols].copy()\n    # Create the scaler\n    scaler = MinMaxScaler()\n    # Fit your data (returns an array, we are creating a dataframe here)\n    X_scaled = pd.DataFrame(scaler.fit_transform(X))\n    # Add the columns names (where removed in previous step)\n    X_scaled.columns = X.columns\n\n    #\u00a0PCA\n    # Create principal components object\n    pca = PCA(random_state=55)\n    # Fit your data\n    X_pca = pca.fit_transform(X_scaled)\n    # Create PCA dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca,columns=component_names)\n    X_pca.index = df.index\n    # Add them to the data set\n    df[[f\"PC{i+1}\" for i in range(X_pca.shape[1])]] = X_pca\n    # Drop\n    df.drop(cols, axis=1, inplace=True)\n\n    #\u00a0Clustering\n    # Create cluster objet\n    kmeans = KMeans(n_clusters=2, random_state=55)\n    # Fit your data\n    X_scaled[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    X_scaled.index = df.index\n    # Add column with the clusters found\n    df[\"Cluster\"] = X_scaled[\"Cluster\"].astype(\"int\")\n    \n    return df","84c27aa9":"# PCA\n\nOnce the data is scaled we can apply PCA. Make sure to [check out the docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html) to learn more about this.\n\nPCA is a widely used technique with two main applications: descriptive and feature engineering. Here we will be focusing in the second one: we want to create new features from PCA components. One of the advantages of this method is that each component is a linear combination of the features in your data; the result are decorrelated features that can replace the original ones.","61f45d24":"An important value to know is the variance explained per component. In general, this value represent how much information the component is carrying. Let's plot it so is easy to see:","a172cf30":"And now we have to drop the original columns cause we replaced that information with our principal component.","8ac59220":"Ok, \"K-means\", meaning K clusters. But.. how do I know how many clusters to choose? A widely common technique is [the elbow method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)). This process is based on [the inertia (or within-cluster sum-of-squares)](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#k-means), which is the same metric that the K-means algortihm tries to minimize when \"finding\" the clusters. The lower the intertia, the better.\n\nThe idea is to run the K-means algorithm for several values of K, calculate the intertia and plot inertia vs. K. The most appropriate K is when the innertia curve breaks (or make a kind of \"elbow\"); this is not the number of clusters that minimize the inertia: with each cluster you add you are reducing the inertia, the value that minimize it completely is the number of points in your data. The purpose is to select the number of K that minimize the inertia AND minimize the number of clusters.\n\nSouns messy? let's try it out, it's way easier than it sounds.","d60ca3e2":"Check out the code here, each step is commented:","aa31ff12":"# Notebook goal\nThe goal of this notebook is to create new features with more complex techniques:\n\n- Principal components analysis (PCA) \n- Clustering with K-means\n\nAt the end, we will train a predictive model. Here will be shown only one of the many possibilities in machine learning, feel free to experiment and play around with the data on your own. We will be focusing on the techniques and not on the model performance.","d95c9249":"## Balance dataset\nWe are going to use the function we defined before! it's at the beginning of this notebook.","6776cb6f":"As mentioned before, these techniques are for continuous features, let's select them. We have only a few:","5c7900cd":"And transform them:","90ce9612":"# (Optional) One function to do it all\nThis part is optional. We are going to write functions that follows all the steps we performed here. These functions will be used in the following notebooks.","bc4eb530":"Pretty clear with a plot, right? Here the optimal value would be 2 (or maybe 3); adding more clusters does no reduce inertia that much. Let's use `k=2`:","04dea429":"We won't get deep into this, but you can try to explain the components exploring the *loadings*: these are the weights for each original variable in the linear combination that we obtained with PCA. Here is the code to create a dataframe with them:","16966270":"| Notebook           | Categorical features | Missing values in categorical | Missing values in numerical | Feature engineering                    | ROC-AUC score |\n|--------------------|----------------------|-------------------------------|-----------------------------|----------------------------------------|---------------|\n| Preprocessing pt.2 | One Hot Encoding     | Mode input                    | 0 input                     | -                                      |0.6591         |\n| Preprocessing pt.3 | One Hot Encoding     | Mode Input                    | Mean input                  | -                                      |0.6466         |\n| FE pt.1            | One Hot Encoding     | Mode Input                    | Mean input                  | Simple                                 |0.6507         |\n| FE pt.2            | One Hot Encoding     | Mode Input                    | Mean input                  | Simple + PCA & Clustering              |0.6481         |","09f5c6a3":"# Model\nNow is time to train the model We are going the follow the same process as before:\n1. Balance data set\n2. Split in train and test\n3. 5-folds cross validation training\n4. Training with whole training data set\n5. Prediction of test data set","1088381a":"## Train and predict\nLet's see what happens when we train with the whole training data set.","36c8d65c":"# Clustering\nThe sencond technique we are going to try out is [K-means clustering](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html). There are a lot of different clustering techniques, this is one of the most common.\n\nClustering is an unsupervised algorithm, which means that does not need a target feature to be trained. It will learn from the data itself and the relationship between features and will create groups (clusters) containing similar points. This similarity is based on the spatial distance within them. All features must be in the same scale and be continuous, so make sure of that before aplying a clustering technique.","ade8120d":"# Introduction\n\nThis is a serie of notebooks thar should be visited in order, they are all linked in the table of content. In this notebook we are going to create some simple features and run a classification model.\n\n### Content table\n- [Preprocessing pt. 1: data transformation & EDA](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-1\/)\n- [Preprocessing pt. 2: encoding categorical variables](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-2\/)\n- [Preprocessing pt. 3: handling missing values](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-3) \n- [Feature engineering pt. 1: simple features](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-1)\n- **Feature engineering pt. 2: clustering & PCA** (you are here)\n    - [Load data](#Load-data)\n    - [Scale data](#Scale-data)\n    - [PCA](#PCA)\n    - [Clustering](#Clustering)\n    - [Model](#Model)\n        - [Balance dataset](#Balance-dataset)\n        - [Split and cross validation](#Split-and-cross-validation)\n        - [Train and predict](#Train-and-predict)\n- [Feature engineering pt. 3: target encoding](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-3)","4bc5c4ec":"I can't believe that we are working so hard on these features and the performance keeps being the same.","c738fbc0":"Remember to make a copy of your data to scale it (for the model we are going to use the original features)","dc2519ed":"# Load data\nThe functions used here were defined in previous notebooks, they replicate the process followed there to use always the same dataset, you can ignore them.","531ca10a":"# Scale data\nFirst steps before applying PCA and clustering are:\n\n1. Select continous featureas\n2. Scale the data\n\n[ScikitLearn offers several techniques for scaling and normalization](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html), we will be using here [*MinMaxScaler*](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html#), which scales each column to the range [0,1] based on the minimum and max value.","0dcedd92":"## Split and cross validation\nLet's split our data. Don't forget to set a `random_state` if you want to replicate the process and obtain the same results.","12b9b567":"Looks like our first try was the best one :( This could happens, remember that here we are foccusing on the techniques and not in the performance. In the following notebook we will use one more feature enginnering technique and discuss about this.","c4ba37c3":"The first 2 components explains more than 80% of the variance. We could try to add only those two to our data set or use all of them. Notice that this is not an strict rule: you can add all of them, you can select some of them, you can ignore them. There is not recipe for this, all depends on the objective of your model and the performance you want to achieve. There is a lot of \"change stuff and seeing what happens\" in machine learning, so don't be afraid to play around with different techniques and get familiar with your data.","313075ac":"And we cross-validate with our training data:","320392de":"And we get a new categorical feature, `cluster`. Whe only choose 2 clusters so it's already binary, but remember that if you choose more clusters you will have to encode it."}}