{"cell_type":{"1b23c944":"code","7b5744d3":"code","ea0057fc":"code","e2d0c056":"code","705af651":"code","c70341a9":"code","a5225a9d":"code","200d3c09":"code","66792e64":"code","8b6d99ad":"code","a4a2f581":"code","dabcbe9f":"code","050ad0cd":"code","7f2da162":"code","94bc89f2":"code","b4273e1d":"code","090a8f55":"code","83562f9f":"code","202aecc1":"code","3c8a3b61":"code","74cadb85":"code","2fe3faeb":"code","112b783b":"code","d147945b":"code","529a09b9":"code","3f8854eb":"code","2cc3aeff":"code","bddb41f0":"code","481220ed":"code","d27840bf":"code","5fbae018":"code","ea295508":"markdown","2a52d93b":"markdown","479927ec":"markdown"},"source":{"1b23c944":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7b5744d3":"#!\/usr\/bin\/env python\n# coding: utf-8\n\"\"\"\n@author: nakayama.s\n\"\"\"\nimport os\nimport warnings\nimport gc\nimport time\nfrom tqdm import tqdm\nimport functools","ea0057fc":"def graph_insight(data):\n    print(set(data.dtypes.tolist()))\n    df_num = data.select_dtypes(include = ['float64', 'int64'])\n    df_num.hist(figsize=(16, 16), bins=50, xlabelsize=8, ylabelsize=8);\n\ndef eda(data):\n    # print(data)\n    print(\"----------Top-5- Record----------\")\n    print(data.head(5))\n    print(\"-----------Information-----------\")\n    print(data.info())\n    print(\"-----------Data Types------------\")\n    print(data.dtypes)\n    print(\"----------Missing value----------\")\n    print(data.isnull().sum())\n    print(\"----------Null value-------------\")\n    print(data.isna().sum())\n    print(\"----------Shape of Data----------\")\n    print(data.shape)\n    print(\"----------describe---------------\")\n    print(data.describe())\n    print(\"----------tail-------------------\")\n    print(data.tail())\n    \ndef read_csv(path):\n  # logger.debug('enter')\n  df = pd.read_csv(path)\n  # logger.debug('exit')\n  return df\n\ndef load_train_data():\n  # logger.debug('enter')\n  df = read_csv(SALES_TRAIN_V2)\n  # logger.debug('exit')\n  return df\n\ndef load_test_data():\n  # logger.debug('enter')\n  df = read_csv(TEST_DATA)\n  # logger.debug('exit')\n  return df\n\ndef graph_insight(data):\n    print(set(data.dtypes.tolist()))\n    df_num = data.select_dtypes(include = ['float64', 'int64'])\n    df_num.hist(figsize=(16, 16), bins=50, xlabelsize=8, ylabelsize=8);\n\ndef drop_duplicate(data, subset):\n    print('Before drop shape:', data.shape)\n    before = data.shape[0]\n    data.drop_duplicates(subset,keep='first', inplace=True) #subset is list where you have to put all column for duplicate check\n    data.reset_index(drop=True, inplace=True)\n    print('After drop shape:', data.shape)\n    after = data.shape[0]\n    print('Total Duplicate:', before-after)\n\ndef unresanable_data(data):\n    print(\"Min Value:\",data.min())\n    print(\"Max Value:\",data.max())\n    print(\"Average Value:\",data.mean())\n    print(\"Center Point of Data:\",data.median())\n\nSAMPLE_SUBMISSION    = '..\/input\/sample_submission.csv'\nTRAIN_DATA           = '..\/input\/train.csv'\nTEST_DATA            = '..\/input\/test.csv'\n\nsample          = read_csv(SAMPLE_SUBMISSION)\ntrain           = read_csv(TRAIN_DATA)\ntest            = read_csv(TEST_DATA)","e2d0c056":"eda(train)","705af651":"import optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,confusion_matrix,f1_score,classification_report","c70341a9":"train.shape","a5225a9d":"test.shape","200d3c09":"X_train_all = train.drop(['target','id'], axis=1)","66792e64":"X_train_all = X_train_all.reset_index()","8b6d99ad":"Y_train_all = train['target']","a4a2f581":"X_train_all.head()","dabcbe9f":"Y_train_all.head()","050ad0cd":"import lightgbm as lgb","7f2da162":"def lb_opt(X_train_all,Y_train_all,trial):\n    (X_train,X_test,y_train,y_test) = train_test_split(X_train_all,Y_train_all,test_size=0.2,random_state=0)\n    #paramter_tuning using optuna\n    bagging_freq =  trial.suggest_int('bagging_freq',1,10),\n    min_data_in_leaf =  trial.suggest_int('min_data_in_leaf',2,100),\n    max_depth = trial.suggest_int('max_depth',1,20),\n    learning_rate = trial.suggest_loguniform('learning_rate',0.001,0.1),\n    num_leaves = trial.suggest_int('num_leaves',2,70),\n    num_threads = trial.suggest_int('num_threads',1,10),\n    min_sum_hessian_in_leaf = trial.suggest_int('min_sum_hessian_in_leaf',1,10),\n    \n    lightgbm_tuna = lgb.LGBMClassifier(\n        random_state = 0,\n        verbosity = 1,\n        bagging_seed = 0,\n        boost_from_average = 'true',\n        boost = 'gbdt',\n        metric = 'auc',\n        bagging_freq = bagging_freq ,\n        min_data_in_leaf = min_data_in_leaf,\n        max_depth = max_depth,\n        learning_rate = learning_rate,\n        num_leaves = num_leaves,\n        num_threads = num_threads,\n        min_sum_hessian_in_leaf = min_sum_hessian_in_leaf\n    )\n    \n    lightgbm_tuna.fit(X_train,y_train)\n    lb_predict_test = lightgbm_tuna.predict(X_test)\n    #print('accuracy_score is {} '.format(accuracy_score(y_test,lb_predict_test)))\n    \n    return (1 - (accuracy_score(y_test,lb_predict_test)) )","94bc89f2":"X_test_all = test.drop(['id'], axis=1)","b4273e1d":"X_test_all = X_test_all.reset_index()","090a8f55":"optuna.logging.disable_default_handler()","83562f9f":"X_test_all.shape","202aecc1":"lb_best_para_list =[]\npred_list = []\npred_prob_list = []\nindex_list = []\nfor i in tqdm(range(512)): \n    x = X_train_all[X_train_all['wheezy-copper-turtle-magic']==i]\n    y = Y_train_all[X_train_all['wheezy-copper-turtle-magic']==i]\n    x_test = X_test_all[X_test_all['wheezy-copper-turtle-magic']==i]\n    lb_study = optuna.create_study()\n    lb_study.optimize(functools.partial(lb_opt,x,y),n_trials = 50)\n    lb_best_para_list.append(lb_study.best_params)\n    lgbm =  lgb.LGBMClassifier(**lb_study.best_params)\n    lgbm.fit(x,y)\n    index_list.append(x_test['index'].values)\n    pred_list.append(lgbm.predict(x_test))\n    pred_prob_list.append(lgbm.predict_proba(x_test)[:,1])","3c8a3b61":"lb_best_para_list[:5]","74cadb85":"pred =[]\nindex = []","2fe3faeb":"for i in range(512):\n    for j in range(len(pred_list[i])):\n        pred.append(pred_list[i][j])","112b783b":"for i in range(512):\n    for j in range(len(index_list[i])):\n        index.append(index_list[i][j])","d147945b":"pred_prpbb =[]\nfor i in range(512):\n    for j in range(len(pred_prob_list[i])):\n        pred_prpbb.append(pred_prob_list[i][j])","529a09b9":"len(pred_prpbb)","3f8854eb":"submission_nosort = pd.DataFrame({\n    'id':index,\n    'target':pred_prpbb\n})","2cc3aeff":"submission_nosort.head()","bddb41f0":"submission_nosort = submission_nosort.sort_values(by=['id'], ascending=True)","481220ed":"sample['target'] = submission_nosort['target']","d27840bf":"sample.head()","5fbae018":"sample.to_csv('submission.csv',index=False)","ea295508":"# Summary\n## 1. [eda&easy_data_confirmation](#jump1)\n## 2. [make models as first_model using optuna](#jump2)","2a52d93b":"<a name=\"jump2\"><\/a>\n## make Lightgbm as first_model using optuna\n* [Lightgbm](https:\/\/lightgbm.readthedocs.io\/en\/latest\/_modules\/lightgbm\/sklearn.html)\n\n## prameter_tuning using [Oputuna](https:\/\/optuna.readthedocs.io\/en\/stable\/)","479927ec":"<a name=\"jump1\"><\/a>\n## eda & easy_data_confirmation\n"}}