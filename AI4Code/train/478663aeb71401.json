{"cell_type":{"2ff10548":"code","a4ab27c3":"code","83c90d02":"code","dcdad09f":"code","03a8e104":"code","9bb4cf5c":"code","920a3de9":"code","6455fad3":"code","41394959":"code","5030c3e9":"code","0b0db038":"code","844b1493":"code","bd13dc80":"code","c2b91bbe":"code","08ba0844":"code","8e00759e":"code","749042f4":"code","afdfaa06":"code","6b0e808f":"code","7f69649d":"code","a678ceb8":"code","ff83d043":"code","d609aedb":"code","376ccf59":"code","990b0004":"code","1a073e15":"code","407b9864":"code","5b054b37":"markdown","63cb2810":"markdown","dfe94c28":"markdown","fc80e892":"markdown","158e9998":"markdown","474fadbe":"markdown"},"source":{"2ff10548":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4ab27c3":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\n\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","83c90d02":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","dcdad09f":"df['date'] = df.date.apply(lambda l: pd.Timestamp(l).value)","03a8e104":"def weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue'] * 100)), x['measurable_impressions']) * 1000 , axis=1)","9bb4cf5c":"train = df[df.date < pd.Timestamp('06-21-2019').value]\ntest = df[df.date >= pd.Timestamp('06-21-2019').value]\n\nY_train = df[(df.date < pd.Timestamp('06-21-2019').value)].CPM\nY_test = df[(df.date >= pd.Timestamp('06-21-2019').value)].CPM","920a3de9":"Y_test = Y_test[test['CPM'].between(-1e-10, test['CPM'].quantile(.95))]\ntest = test[test['CPM'].between(-1e-10, test['CPM'].quantile(.95))]","6455fad3":"train.drop(labels=['total_revenue', 'measurable_impressions', 'order_id' , 'date'], axis=1, inplace=True)\ntest.drop(labels=['total_revenue', 'measurable_impressions', 'order_id' , 'date'], axis=1, inplace=True)","41394959":"plt.figure(figsize=(15, 10))\nplt.hist(train.CPM, bins=200);","5030c3e9":"Y_train = Y_train[train['CPM'].between(-1e-10, test['CPM'].quantile(.97))]\ntrain = train[train['CPM'].between(-1e-10, test['CPM'].quantile(.97))]","0b0db038":"train['is_non_zero'] = train.CPM != 0","844b1493":"train.is_non_zero.value_counts(normalize=True)","bd13dc80":"train.drop(labels=['CPM'], axis=1, inplace=True)\ntest.drop(labels=['CPM'], axis=1, inplace=True)","c2b91bbe":"train.head()","08ba0844":"cat_cols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]","8e00759e":"X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(train, train.is_non_zero, test_size=0.1, random_state=42)\nX_train_clf.drop(labels=['is_non_zero'], axis=1, inplace=True)\nX_val_clf.drop(labels=['is_non_zero'], axis=1, inplace=True)","749042f4":"xgb_clf = CatBoostClassifier(n_estimators=2000, learning_rate=0.03, eval_metric='Accuracy', metric_period=50)\nxgb_clf.fit(X_train_clf, y_train_clf, eval_set=(X_val_clf, y_val_clf), cat_features = cat_cols, early_stopping_rounds=100)","afdfaa06":"def optimize_f1(clf, X, y):\n    scores = [(g, f1_score(y, clf.predict_proba(X)[:,1:] > g)) for g in np.linspace(0, 1, 20)]\n    return sorted(scores, key=lambda l: -l[1])[0][0]","6b0e808f":"board = optimize_f1(xgb_clf, X_val_clf, y_val_clf)","7f69649d":"X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(train[train.is_non_zero], Y_train[train.is_non_zero], test_size=0.1, random_state=42)\nX_train_reg.drop(labels=['is_non_zero'], axis=1, inplace=True)\nX_val_reg.drop(labels=['is_non_zero'], axis=1, inplace=True)","a678ceb8":"xgb = CatBoostRegressor(eval_metric='RMSE', loss_function='RMSE', metric_period=50)\nxgb.fit(X_train_reg, y_train_reg, eval_set=(X_val_reg, y_val_reg), cat_features = cat_cols, early_stopping_rounds=100)","ff83d043":"clf_preds = np.squeeze(xgb_clf.predict_proba(test)[:,1:] > board)","d609aedb":"submit = np.zeros(test.shape[0])","376ccf59":"submit[clf_preds] = xgb.predict(test[clf_preds])","990b0004":"predicted_mse = mean_squared_error(submit, Y_test)\npredicted_mse","1a073e15":"expected_mse = 4850","407b9864":"if predicted_mse < expected_mse:\n    print(\"OK\")\nelse:\n    print(\"Akshay WIN!\")","5b054b37":"Almost balansed target for classification task!","63cb2810":"# Inference","dfe94c28":"# Fit models","fc80e892":"In plotted histogram we can see too much zero values.\n\nThen let's build next pipeline:\n\n-fit classificator to detect zero CPM\n\n-fit regressor to predict non-zero CPM","158e9998":"# Some EDA","474fadbe":"97 percentile gives more data and removes too big CPM"}}