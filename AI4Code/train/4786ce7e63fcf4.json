{"cell_type":{"5e1bcb06":"code","d6a173da":"code","7f04681d":"code","303c7bc6":"code","a8132a26":"code","855085c3":"code","0a6408be":"code","7edd582d":"code","d3aca255":"code","1452274f":"code","8c00772e":"code","0f046b62":"markdown","5173745e":"markdown","f895f105":"markdown","00eddd58":"markdown","0566d146":"markdown","24046711":"markdown","0bc1df77":"markdown"},"source":{"5e1bcb06":"!pip install -q csv_diff","d6a173da":"import time\nimport os\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport gc\nimport joblib\nfrom csv_diff import load_csv, compare","7f04681d":"# Work in PC, Kaggle, Colab\nIN_PC = ('VSCODE_CWD' in os.environ) and (not 'AK_KAGGLE_ENV' in os.environ)\nIN_DOCKER = ('AK_KAGGLE_ENV' in os.environ)\nIN_COLAB = ('COLAB_GPU' in os.environ)\n# DATA_PATH = \"..\/..\/data\" if os.path.exists(\"..\/..\/data\") and IN_PC else \"..\/..\/..\/data\" if os.path.exists(\"..\/..\/..\/data\") and IN_PC else \"\/kaggle\"\n\nif IN_PC:\n    DATA_PATH = os.path.abspath(\"..\/..\/data\") if os.path.isdir(\"..\/..\/data\") else os.path.abspath(\"..\/..\/..\/data\")\nelif IN_COLAB:\n    DATA_PATH = \"\/content\/drive\/MyDrive\/Colab_Kaggle\/data\"\nelse:\n    DATA_PATH = \"\/kaggle\"","303c7bc6":"target_col='Cover_Type' # name of target column\nid_col='Id'\nn_splits=5\ncompetition='tps1221'\nfullname_competition ='tabular-playground-series-dec-2021'\n\n\n# CSV_PATH = DATA_PATH + '\/working\/'+ competition + '\/' + config['csv_name']\nWORKING_PATH = DATA_PATH + '\/working\/'\nJOBLIB_PATH = DATA_PATH + '\/input\/tps1221data\/'\nORGIN_CSV_PATH = DATA_PATH + '\/input\/'+ fullname_competition +'\/'\n# DB_PATH = DATA_PATH + '\/working\/'+ competition + '\/' + config['db_name']\n\nint_colums = [\n    'Id','Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways',\n     'Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points','Wilderness_Area1','Wilderness_Area2','Wilderness_Area3',\n     'Wilderness_Area4','Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type7','Soil_Type8','Soil_Type9',\n     'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19',\n     'Soil_Type20','Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type26','Soil_Type27','Soil_Type28','Soil_Type29','Soil_Type30',\n     'Soil_Type31','Soil_Type32','Soil_Type33','Soil_Type34','Soil_Type35','Soil_Type36','Soil_Type37','Soil_Type38','Soil_Type39','Soil_Type40'\n            ]","a8132a26":"%%time\ntime_s = time.time()\n\ntrain = pd.read_csv(ORGIN_CSV_PATH + 'train.csv',dtype='float32')\ntest = pd.read_csv(ORGIN_CSV_PATH + 'test.csv',dtype='float32')\nsubmission = pd.read_csv(ORGIN_CSV_PATH + 'sample_submission.csv')\n\nfor col in int_colums:\n    train[col] = train[col].astype(np.int32)\n    test[col] = test[col].astype(np.int32)\n\ntrain[target_col] = train[target_col].astype(np.int8)\n\ntime_e = time.time()\n\ntime__ = '{:.1f}'.format(time_e-time_s)\nmemory_ = (train.memory_usage(index = True).sum() + test.memory_usage(index = True).sum() + submission.memory_usage(index = True).sum() )\/ 1e9\nmemory_str = '{:.2f}'.format(memory_)\ntypes_ = (train.dtypes).value_counts()","855085c3":"print(f'{3*\"=\"} traditional method with reduce memory {18*\"=\"}\\nMemory:\\t{memory_str} gb\\nTime:\\t{time__} sec\\n\\n{types_}')","0a6408be":"%%time\ntime_s_j = time.time()\n\ntrain_job = joblib.load(JOBLIB_PATH + 'train-32.pkl')\ntest_job = joblib.load(JOBLIB_PATH + 'test-32.pkl').drop(columns=[id_col])\nsubmission_job = joblib.load(JOBLIB_PATH + 'submission-32.pkl')\n\ntime_e_j = time.time()\n\ntime__j = '{:.1f}'.format(time_e_j-time_s_j)\nmemory_j = (train_job.memory_usage(index = True).sum() + test_job.memory_usage(index = True).sum() + submission_job.memory_usage(index = True).sum() )\/ 1e9\nmemory_j_str = '{:.2f}'.format(memory_j)\ntypes_j = (train_job.dtypes).value_counts()","7edd582d":"print(f'{3*\"=\"} joblib method (all in one){18*\"=\"}\\nMemory:\\t{memory_j_str} gb\\nTime:\\t{time__j} sec\\n\\n{types_j}')\nprint()\nprint(25*'===')\nprint('With JOBLIB method,     Data load {:.2f}% faster and {:.2f}% less memory'.format(100 * (time_e-time_s-time_e_j+time_s_j) \/ (time_e-time_s), 100 * (memory_ - memory_j) \/ memory_))\nprint(25*'===')\n","d3aca255":"# I have to save CSV\nif not os.path.exists(WORKING_PATH + 'train-32.csv'):\n    train_tmp = joblib.load(JOBLIB_PATH + 'train-32.pkl')\n    train_tmp.to_csv(WORKING_PATH + 'train-32.csv', index=False, float_format='%g')\n    \nif not os.path.exists(WORKING_PATH + 'test-32.csv'):\n    test_tmp = joblib.load(JOBLIB_PATH + 'test-32.pkl')\n    test_tmp.to_csv(WORKING_PATH + 'test-32.csv', index=False, float_format='%g')\n\nif not os.path.exists(WORKING_PATH + 'submission-32.csv'):\n    submission_tmp = joblib.load(JOBLIB_PATH + 'submission-32.pkl')\n    submission_tmp.to_csv(WORKING_PATH + 'submission-32.csv', index=False, float_format='%g')","1452274f":"del train\ndel train_job\ndel train_tmp\ndel test\ndel test_tmp\ndel test_job\ndel submission\ndel submission_job\ndel submission_tmp\ngc.collect();","8c00772e":"diff_test = compare(\n    load_csv(open(ORGIN_CSV_PATH + 'test.csv'), key=id_col),\n    load_csv(open(WORKING_PATH + 'test-32.csv'), key=id_col)\n)\ndiff_test","0f046b62":"# Load data with joblib (all in one)","5173745e":"# Compare traditional method vs joblib","f895f105":"# DATA_PATH","00eddd58":"![kaggle-memory-limit](https:\/\/raw.githubusercontent.com\/akmeghdad\/data-science-note\/master\/src\/images\/kaggle-memory-limit.png)","0566d146":"# Variables","24046711":"# Imports","0bc1df77":"# Load data in traditional method with reduce memory"}}