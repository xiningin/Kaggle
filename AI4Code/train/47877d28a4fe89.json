{"cell_type":{"d55e4d9f":"code","e6505305":"code","291c5ba6":"code","f30980e2":"code","ac67591b":"code","3ca2ea01":"code","42fbe9e9":"code","61b9dc39":"code","d21272d9":"code","4ef9c908":"code","141d0595":"code","ebba597d":"code","cbfe4862":"code","4bb7fd90":"code","bb2fa24a":"code","5187db22":"code","97550be7":"code","7e75137c":"code","8a022397":"code","12d79f4c":"code","3cd06a23":"code","9735538c":"code","ad23522a":"code","d0c2c7a8":"code","77db1e31":"code","095c360f":"code","3502b197":"code","73c729d6":"code","08907286":"code","4817e0e6":"code","a8dd1bd1":"code","f5db8d2a":"code","095a3e62":"code","3ac903c4":"code","3f81b6f7":"code","46debdfe":"code","8fbc4ec2":"code","cab0d0fa":"code","500b29ae":"code","4b9d97ca":"code","8add190c":"code","2b4e983b":"code","89f5871f":"code","402d8c1d":"code","97ae7a86":"code","2e7306ed":"code","27ee3c12":"code","2aabbb28":"code","0c73a628":"code","ea3e8f99":"code","8e8b74b5":"code","141bec07":"code","69d14ebe":"code","a0604ec4":"code","19aa1869":"code","5d49eb42":"code","e34cd432":"code","91a90b86":"code","79f1ea03":"code","e1888440":"code","3fea70d8":"code","e6fb707b":"code","826e0f53":"code","b0b939eb":"code","949f4ced":"code","e8e9d60c":"code","b4dfb1bc":"code","ce6e8d83":"code","54c11f22":"code","e50d9995":"code","eabdcfd2":"code","f1df0e7b":"code","7c25a100":"code","06b375a8":"code","a05d1bf5":"code","ae59a2b9":"code","81e79693":"code","c61a7d73":"code","a705e56b":"code","d00bce4c":"markdown"},"source":{"d55e4d9f":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np,gc # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e6505305":"# Read train and test data with pd.read_csv():\ntrain_id= pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntest_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ntrain_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")","291c5ba6":"train_id.head()","f30980e2":"test_id.head()","ac67591b":"train_tr.head()","3ca2ea01":"test_tr.head()","42fbe9e9":"train_id.info()","61b9dc39":"test_id.info()","d21272d9":"train_tr.info()","4ef9c908":"train_tr.info()","141d0595":"test_tr.info()","ebba597d":"train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntrain.head()","cbfe4862":"test=pd.merge(test_tr, test_id, on = \"TransactionID\",how=\"left\",left_index=True, right_index=True)\ntest.head()","4bb7fd90":"del train_id, train_tr, test_id, test_tr","bb2fa24a":"test.columns=train.columns.drop(\"isFraud\")","5187db22":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","97550be7":"#train=reduce_mem_usage2(train)\n#test=reduce_mem_usage2(test)","7e75137c":"train.isna().sum()\n\n","8a022397":"def make_corr(Vs):\n    cols = Vs.columns\n    plt.figure(figsize=(15,15))\n    sns.heatmap(train[cols].corr(), cmap='RdBu_r', annot=True, center=0.0)\n    #plt.title(Vs[0]+' - '+Vs[-1],fontsize=14)\n    plt.show()\n","12d79f4c":"msno.matrix(train.loc[:,\"V1\":\"V11\"]);","3cd06a23":"make_corr(train.loc[:,\"V1\":\"V11\"])","9735538c":"msno.matrix(train.loc[:,\"V12\":\"V34\"]);","ad23522a":"make_corr(train.loc[:,\"V12\":\"V34\"])","d0c2c7a8":"msno.matrix(train.loc[:,\"V35\":\"V52\"]);","77db1e31":"make_corr(train.loc[:,\"V35\":\"V52\"])","095c360f":"msno.matrix(train.loc[:,\"V53\":\"V74\"]);","3502b197":"make_corr(train.loc[:,\"V53\":\"V74\"])","73c729d6":"msno.matrix(train.loc[:,\"V75\":\"V94\"]);","08907286":"make_corr(train.loc[:,\"V75\":\"V94\"])","4817e0e6":"msno.matrix(train.loc[:,\"V95\":\"V137\"]);","a8dd1bd1":"make_corr(train.loc[:,\"V95\":\"V137\"])","f5db8d2a":"msno.matrix(train.loc[:,\"V138\":\"V166\"]);","095a3e62":"make_corr(train.loc[:,\"V138\":\"V166\"])","3ac903c4":"msno.matrix(train.loc[:,\"V167\":\"V216\"]);","3f81b6f7":"make_corr(train.loc[:,\"V167\":\"V216\"])","46debdfe":"msno.matrix(train.loc[:,\"V217\":\"V234\"]);","8fbc4ec2":"make_corr(train.loc[:,\"V217\":\"V234\"])","cab0d0fa":"drop_col=[]\ntrain_colmns=train.loc[:,\"V1\":\"V339\"].columns\ntest_columns=test.loc[:,\"V2\":\"V339\"].columns\nfor col1,col2 in zip(train_colmns,test_columns):\n            \n        if ((train.loc[:,col1:col2].corr().loc[col2].sum()-1)>0.75) & (train[col1].isna().sum()== train[col2].isna().sum()):\n            print(\"'\"+col2+\"'\",', ',end='')\n            drop_col.append(col2)\n                ","500b29ae":"train=train.drop(drop_col,axis=1)\ntest=test.drop(drop_col,axis=1)\ndel drop_col","4b9d97ca":"for col in train.columns: \n       if sum(train[col].isnull())\/float(len(train.index)) > 0.90:\n            print(\"'\"+col+\"'\",', ',end='')\n            train=train.drop(col,axis=1)\n            test=test.drop(col,axis=1)","8add190c":"train.info()","2b4e983b":"    emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',     \n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n         train[c + '_bin'] = train[c].map(emails)\n         test[c + '_bin'] = test[c].map(emails)\n    \n        ","89f5871f":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=train,test=test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=train, test_df=test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=train,df2=test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=train, test_df=test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","402d8c1d":"encode_CB('card1','addr1')","97ae7a86":"train['day'] = train.TransactionDT \/ (24*60*60)\ntrain['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\ntest['day'] = test.TransactionDT \/ (24*60*60)\ntest['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)","2e7306ed":"train[(train[\"card1\"]==15775)&(train[\"addr1\"]==251.0)][[\"TransactionID\",\"isFraud\",\"TransactionDT\",\"TransactionAmt\",\"card1\",\"card2\",\"card3\",\"card4\",\"addr1\",\"uid\"]]","27ee3c12":"na_low=[]\nfor col in train.loc[:,'TransactionAmt':].columns: \n       if sum(train[col].isnull())\/float(len(train.index)) < 0.30:\n            na_low.append(col)\n            print(\"'\"+col+\"'\",', ',end='')\n      ","2aabbb28":"na_low=['TransactionAmt' , 'ProductCD' , 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'addr1' ,\n        'addr2' , 'P_emaildomain' , 'C1' , 'C2' , 'C3' , 'C4' , 'C5' , 'C6' , 'C7' , 'C8' , 'C9' , 'C10' , \n        'C11' , 'C12' , 'C13' , 'C14'  , 'M6' ]","0c73a628":"numeric=train[na_low]._get_numeric_data().columns\nnumeric","ea3e8f99":"encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])","8e8b74b5":"encode_CB('card1','addr1')","141bec07":"train['day'] = train.TransactionDT \/ (24*60*60)\ntrain['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\ntest['day'] = test.TransactionDT \/ (24*60*60)\ntest['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)","69d14ebe":"encode_FE(train,test,['uid'])\nencode_AG(numeric, ['uid'],['mean',\"std\"], train, test, fillna=True, usena=False)\n","a0604ec4":"categorical_columns=test.columns.drop(test._get_numeric_data().columns)\ncategorical_columns=categorical_columns.drop('uid')\ncategorical_columns","19aa1869":"encode_AG2(categorical_columns, ['uid'], train, test)","5d49eb42":"# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","e34cd432":"del train['uid'], test['uid']","91a90b86":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","79f1ea03":"train=reduce_mem_usage2(train)\ntest=reduce_mem_usage2(test)","e1888440":"train.shape, test.shape","3fea70d8":"categorical_columns=test.columns.drop(test._get_numeric_data().columns)\ncategorical_columns","e6fb707b":"from sklearn import preprocessing\nfor i in categorical_columns: \n    lbe=preprocessing.LabelEncoder()\n    train[i]=lbe.fit_transform(train[i].astype(str))","826e0f53":"for i in categorical_columns:    \n    test[i]=lbe.fit_transform(test[i].astype(str))","b0b939eb":"train_columns=train.columns\ntrain_columns=train_columns.drop(\"isFraud\")\ntest.columns=train_columns\ntest.columns","949f4ced":"train.shape, test.shape","e8e9d60c":"for i in categorical_columns:\n    if (test[i].max()== train[i].max())&(train[i].max()<8):\n            test = pd.get_dummies(test, columns = [i])\n            train=pd.get_dummies(train, columns = [i])\n\n    ","b4dfb1bc":"train.shape, test.shape","ce6e8d83":"train.shape, test.shape","54c11f22":"train_TransactionID= train[\"TransactionID\"]\ntest_TransactionID=test[\"TransactionID\"]\nTransactionDT=train[\"TransactionDT\"]\nX= train.drop([ 'TransactionDT', 'TransactionID'], axis=1)\ny = train['isFraud']\ntest = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\ndel train","e50d9995":"#X=X.drop(feature_drop,axis=1)\n#test=test.drop(feature_drop,axis=1)","eabdcfd2":"X=X.drop(\"isFraud\", axis=1)","f1df0e7b":"X.head()","7c25a100":"X.shape, test.shape","06b375a8":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n          \n         }","a05d1bf5":"folds = TimeSeriesSplit(n_splits=5)\n\naucs = list()\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = X.columns\n\ntraining_start_time = time()\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    \n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n    aucs.append(clf.best_score['valid_1']['auc'])\n    \n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\nprint('-' * 30)\nprint('Training has finished.')\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\nprint('Mean AUC:', np.mean(aucs))\nprint('-' * 30)","ae59a2b9":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances_TimeFold.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","81e79693":"#clf right now is the last model, trained with 80% of data and validated with 20%\nbest_iter = clf.best_iteration","c61a7d73":"clf = lgb.LGBMClassifier(**params, num_boost_round=best_iter)\nclf.fit(X, y)","a705e56b":"#set the output as a dataframe and convert to csv file named submission.csv\npredictions = clf.predict_proba(test)[:, 1]\noutput = pd.DataFrame({ \"TransactionID\" : test_TransactionID, \"isFraud\": predictions })\noutput.to_csv('submission_lgbm.csv', index=False)\n\n","d00bce4c":"feature_drop=[\"V6\", \"id_34\", \"V84\", \"V141\", \"id_38_0\", \"V111_uid_std\", \"V279\", \"V242\", \"V15\", \"V135\", \"V181\", \"V99\", \"V186\", \"V335\",\n              \"M9_0\", \"C3\", \"V281\", \"M8_1\", \"M5_2\", \"V92\", \"M7_2\", \"V226\", \"V132\", \"id_04\", \"M2_1\", \"V118_uid_mean\", \"V117_uid_std\", \n              \"V172\", \"V2\", \"V337\", \"V21\", \"V50\", \"addr2_uid_mean\", \"id_12_1\", \"V293\", \"V194\", \"V123\", \"id_15_1\", \"V326\", \"V175\", \"V174\",\n              \"id_16_1\", \"V122_uid_mean\", \"M8_2\", \"V31\", \"M7_1\", \"id_38_1\", \"M8_0\", \"V319\", \"id_15_2\", \"DeviceType_1\", \"V118_uid_std\",\n              \"id_28_0\", \"V286\", \"V122_uid_std\", \"V95\", \"C3_uid_mean\", \"id_35_0\", \"DeviceType_0\", \"V173\", \"V196\", \"V121_uid_mean\", \"V287\",\n              \"V252\", \"id_28_1\", \"V120_uid_mean\", \"id_29_1\", \"V328\", \"V121_uid_std\", \"V108\", \"V247\", \"uid_M1_ct\", \"M9_2\", \"V8\", \"id_37_1\",\n              \"addr1_uid_std\", \"id_12_2\", \"V290\", \"V109\", \"id_29_0\", \"ProductCD_3\", \"V120_uid_std\", \"V117_uid_mean\", \"id_36_0\", \"id_15_0\",\n              \"V98\", \"V300\", \"id_16_2\", \"addr2_uid_std\", \"V334\", \"V101\", \"C3_uid_std\", \"id_37_0\", \"V284\", \"V288\", \"V115\", \"V138\", \"V104\",\n              \"V302\", \"V14_uid_std\", \"id_16_0\", \"card1_uid_std\", \"M6_2\", \"V65\", \"id_35_1\", \"V297\", \"V110\", \"card3_uid_std\", \"V14_uid_mean\", \n              \"id_36_1\", \"id_15_3\", \"M1_1\", \"ProductCD_0\", \"V111\", \"V41\", \"id_12_0\", \"M1_2\", \"id_28_2\", \"V325\", \"V107_uid_mean\", \"V14\", \"V68\", \n              \"V107_uid_std\", \"V116\", \"M3_2\", \"V1\", \"id_35_2\", \"id_29_2\", \"V114\", \"V65_uid_std\", \"M2_2\", \"V27_uid_std\", \"V27\", \"ProductCD_4\",\n              \"V240\", \"V65_uid_mean\", \"DeviceType_2\", \"V27_uid_mean\", \"V68_uid_std\", \"V89\", \"V88\", \"id_37_2\", \"id_36_2\", \"id_38_2\", \"V68_uid_mean\", \n              \"M1_0\", \"card4_3\", \"uid_card6_ct\", \"uid_card4_ct\", \"V305_uid_std\", \"V305_uid_mean\", \"V305\", \"V241\", \"V122\", \"V121\", \"V120\", \"V118\", \"V117\", \"V107\", ]"}}