{"cell_type":{"a7fea06e":"code","9f490a8b":"code","52a85b98":"code","c8555970":"code","11bc087e":"code","0338c909":"code","63c3edad":"code","de27196a":"code","b3ecd8c3":"code","513be52d":"code","88791eae":"code","5e3abb38":"code","97305b6b":"code","0f54ba7c":"code","9476693a":"code","ec4cd448":"code","7a40b758":"code","341497f7":"code","70063405":"code","25db37f5":"code","cd6615de":"code","a7cbba74":"code","fbffac02":"code","ce6354fb":"markdown","d51b3c99":"markdown","57d568c3":"markdown","e7294055":"markdown","05b05ce9":"markdown","af473aa4":"markdown","46220139":"markdown","19fa65f5":"markdown","272a3465":"markdown","8a92dcf3":"markdown","9d5466c0":"markdown","7738d7ce":"markdown","449f10be":"markdown","8593ec74":"markdown","be005d44":"markdown"},"source":{"a7fea06e":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 20\n    batch_size = 64\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    num_heads = 2\n    epochs = 10 # Number of Epochs to train\nconfig = Config()","9f490a8b":"!pip install -q tensorflow==2.6.0","52a85b98":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\nimport pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport sklearn\nfrom sklearn.model_selection import train_test_split","c8555970":"data = pd.read_csv(\"\/kaggle\/input\/englishspanish-translation-dataset\/data.csv\")\ndata.head()","11bc087e":"data[\"spanish\"] = data[\"spanish\"].apply(lambda item: \"[start] \" + item + \" [end]\")","0338c909":"data.head()","63c3edad":"strip_chars = string.punctuation + \"\u00bf\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\nprint(strip_chars)\ndef spanish_standardize(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\"%re.escape(strip_chars), \"\")\nenglish_vectorization = TextVectorization(\n    max_tokens=config.vocab_size, \n    output_mode=\"int\", \n    output_sequence_length=config.sequence_length,\n)\nspanish_vectorization = TextVectorization(\n    max_tokens=config.vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=config.sequence_length + 1,\n    standardize=spanish_standardize,\n)\nenglish_vectorization.adapt(list(data[\"english\"]))\nspanish_vectorization.adapt(list(data[\"spanish\"]))","de27196a":"def preprocess(english, spanish):\n    english = english_vectorization(english)\n    spanish = spanish_vectorization(spanish)\n    return ({\"encoder_inputs\": english, \"decoder_inputs\": spanish[:, :-1]}, spanish[:, 1:])\ndef make_dataset(df, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((list(df[\"english\"]), list(df[\"spanish\"])))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.prefetch(16).cache()\n    return dataset","b3ecd8c3":"train, valid = train_test_split(data, test_size=config.validation_split)\ntrain.shape, valid.shape","513be52d":"train_ds = make_dataset(train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(valid, batch_size=config.batch_size, mode=\"valid\")","88791eae":"for batch in train_ds.take(1):\n    print(batch)","5e3abb38":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super(TransformerEncoder, self).__init__(*kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, \n            key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential([\n            layers.Dense(dense_dim, activation=\"relu\"),\n            layers.Dense(embed_dim)\n        ])\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs,\n            value=inputs,\n            key=inputs,\n            attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n    ","97305b6b":"class TokenAndPositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(TokenAndPositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","0f54ba7c":"class TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential([\n            layers.Dense(latent_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n        \n    def call(self, inputs, encoder_outputs, mask = None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask != None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n        \n        attention_output_1 = self.attention_1(\n            query=inputs, \n            value=inputs, \n            key=inputs, \n            attention_mask=causal_mask\n        )\n        \n        out_1 = self.layernorm_1(inputs + attention_output_1)\n        \n        attention_output_2 = self.attention_2(\n            query=out_1, \n            value=encoder_outputs, \n            key=encoder_outputs, \n            attention_mask=padding_mask\n        )\n        \n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n        \n        proj_output = self.dense_proj(out_2)\n        \n        return self.layernorm_3(out_2 + proj_output)\n    \n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0\n        )\n        return tf.tile(mask, mult)","9476693a":"def get_transformer(config):\n    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n    x = TokenAndPositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(encoder_inputs)\n    encoder_outputs = TransformerEncoder(config.embed_dim, config.latent_dim, config.num_heads)(x)\n    encoder = keras.Model(encoder_inputs, encoder_outputs)\n\n    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n    encoded_seq_inputs = keras.Input(shape=(None, config.embed_dim), name=\"decoder_state_inputs\")\n    x = TokenAndPositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(decoder_inputs)\n    x = TransformerDecoder(config.embed_dim, config.latent_dim, config.num_heads)(x, encoded_seq_inputs)\n    x = layers.Dropout(0.5)(x)\n    decoder_outputs = layers.Dense(config.vocab_size, activation=\"softmax\")(x)\n    decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\n    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n    transformer = keras.Model(\n        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n    )\n    return transformer","ec4cd448":"transformer = get_transformer(config)","7a40b758":"transformer.summary()","341497f7":"keras.utils.plot_model(transformer, show_shapes=True)","70063405":"transformer.compile(\n    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)","25db37f5":"transformer.fit(train_ds, epochs=config.epochs, validation_data=valid_ds)","cd6615de":"spanish_vocab = spanish_vectorization.get_vocabulary()\nspanish_index_lookup = dict(zip(range(len(spanish_vocab)), spanish_vocab))\ndef decode_sequence(transformer, input_sentence):\n    tokenized_input_sentence = english_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(config.sequence_length):\n        tokenized_target_sentence = spanish_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = spanish_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence","a7cbba74":"for i in np.random.choice(len(data), 10):\n    item = data.iloc[i]\n    translated = decode_sequence(transformer, item[\"english\"])\n    print(\"English:\", item[\"english\"])\n    print(\"Spanish:\", item[\"spanish\"])\n    print(\"Translated:\", translated)","fbffac02":"transformer.save_weights(\"model.h5\")","ce6354fb":"Let's choose some random texts, translate them to English using this Model and compare with actual result.","d51b3c99":"### Token and Position Embedding","57d568c3":"### The TransformerEncoder","e7294055":"### The Transformer Model","05b05ce9":"## Model Training\n","af473aa4":"## Model Saving","46220139":"## Overview\nIn this Notebook, I will develop a English-Spanish Translation Model using Transformer Encoder-Decoder Architecture.","19fa65f5":"Install latest version of TensorFlow. ","272a3465":"## Translation","8a92dcf3":"### Transformer Decoder","9d5466c0":"## Setup","7738d7ce":"## Model Development","449f10be":"## Configuration","8593ec74":"## Import datasets","be005d44":"# English-Spanish Translation: Transormer\n## Table of Contents\n* Overview\n* Configuration\n* Setup\n* Import datasets\n* Model Development"}}