{"cell_type":{"04344aeb":"code","16ea3a76":"code","33f196ef":"code","c1839b25":"code","ca094484":"code","a4af1a1d":"code","a40f56f8":"code","c6f1239c":"code","a21832b8":"code","7b3334ca":"code","b4bcee56":"code","d497c21b":"code","1de0d6f2":"code","4d3f96c2":"code","53e06bfa":"code","36bcc33d":"code","0467c298":"code","839769f1":"code","655d7e44":"code","c7e100e7":"code","97ee10c3":"code","5b188b91":"code","ece6dfae":"code","92e5c045":"code","64dbcad5":"code","27e96dea":"code","27b833a9":"code","2e124da7":"code","02dbc87a":"code","917c4fd3":"code","ad50475c":"code","24f000fd":"code","3f53c1b3":"code","5220916b":"code","11914af2":"code","dc97873f":"code","2589d76a":"code","a3517c57":"code","597eee15":"code","7c375124":"code","f68023fe":"code","5152fabb":"code","439cd2ab":"markdown","d5db3e4e":"markdown","b716617b":"markdown","b775e53b":"markdown","e7a8a3d0":"markdown"},"source":{"04344aeb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16ea3a76":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport collections\n\n#3d\nfrom mpl_toolkits import mplot3d\n\n%matplotlib inline\n\n#model preparation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n#simple linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR\n#classtering\nfrom sklearn.cluster import KMeans\n\n#metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error","33f196ef":"data=pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\ndata.head()","c1839b25":"data.info()\nprint('\\nCheck \"null\" and \"NA\"\\nCheck \"null\"')\nprint(data.isnull().sum())\nprint('\\nCheck \"NA\"')\nprint(data.isna().sum())","ca094484":"data.describe(include='all')","a4af1a1d":"col_wo_price=data.columns.drop('price')\n\nplot=sns.pairplot(data=data,\n    x_vars=col_wo_price, y_vars=['price'],\n                  height=4)\n\nfor ax in plot.axes.flat:\n    ax.tick_params(axis='x', labelrotation=45)","a40f56f8":"num_col_wo_price=data.select_dtypes(include=np.number).columns.drop('price')\n\nplot=sns.pairplot(data=data,\n    x_vars=num_col_wo_price, y_vars=['price'],\n                  #kind='kde',\n                  height=5)\n\nfor ax in plot.axes.flat:\n    ax.tick_params(axis='x', labelrotation=45)","c6f1239c":"cat_col_wo_price=data.select_dtypes(exclude=np.number).columns\n\nplot=sns.pairplot(data=data,\n    x_vars=num_col_wo_price, y_vars=['price'],\n                  #kind='kde',\n                  height=6)\n\nfor ax in plot.axes.flat:\n    ax.tick_params(axis='x', labelrotation=45)","a21832b8":"fig, axes = plt.subplots(1, len(cat_col_wo_price), figsize=(24,6))\n\nfor i, c in enumerate(cat_col_wo_price):\n    print(i, c)\n    \n    sns.boxplot(data=data, x=c, y='price',\n               ax=axes[i])\n\nplt.show()","7b3334ca":"data_wo_model=pd.get_dummies(data.drop('model', axis='columns'))\n#MB_data_wo_model.columns=MB_data_wo_model.columns.str.replace('.*_', 'bla_').str.strip('bla_')\n#MB_data_wo_model.head()\nprice_corr=data_wo_model.corr()['price'].sort_values()[:-1]\nprice_corr=pd.DataFrame(price_corr)\nprice_corr.columns=['corr']\n\n#set colors\ncond=[\n    abs(price_corr['corr']).gt(0).le(0.2),\n    abs(price_corr['corr']).gt(0.2).le(0.5),\n    abs(price_corr['corr']).gt(0.5).le(0.7),\n    abs(price_corr['corr']).gt(0.7).le(0.9),\n    abs(price_corr['corr']).gt(0.9)\n    ]\nvalue=['Very Low','Low','Everage','High','Very High']\ncolor=['whitesmoke','tan','darkorange','limegreen','darkgreen',]\n\nprice_corr['corr_type']=np.select(cond, value)\nprice_corr['color']=np.select(cond, color)\n\n#price_corr\n\nbar=price_corr['corr'].plot(kind='bar', \n                figsize=(24,10),\n                color=price_corr['color'])\nbar.set_title('Price Correlation\\n', fontsize=16, color='tab:blue')\nbar.set_ylabel('Price Correation')\nbar.set_xlabel('Feutures')\n\nplt.xticks(rotation=45)\nplt.show()","b4bcee56":"all_corr=pd.get_dummies(data).corr()['price'].sort_values()\nall_corr=pd.DataFrame(all_corr)\nall_corr['abs']=abs(all_corr['price'])\nall_corr.sort_values(by='abs', ascending=False, inplace=True)\nall_corr=all_corr.iloc[1:]\nall_corr['price']","d497c21b":"all_corr['price'].iloc[:5]","1de0d6f2":"top_model=data.describe(include='all').loc['top','model']\ntop_model","4d3f96c2":"#Only one model and only one featur\nb_mat=data['model']==top_model\nmodel=data[b_mat]\nmodel=model[['mileage','price']]\nprint(model.shape)\nmodel.head()","53e06bfa":"sns.pairplot(data=model,\n             y_vars=['price'],\n             height=4.5)\nplt.show()","36bcc33d":"model_table=pd.DataFrame(columns=['Name','MAE','RMSE','R2_score'])\nmodel_table\n\nX=np.array(model['mileage']).reshape((-1,1))\ny=np.array(model['price'])\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nLR=LinearRegression()\nLR.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training score: {}'.format(LR.score(X_train, y_train)))\nprint('Test score: {}'.format(LR.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=LR.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \n    \n\nstat={'Name':'Simple Linear Regression',\n         'MAE': '{0:.0f}'.format(mean_absolute_error(y_test, y_hat)),\n         'RMSE': '{0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat))),\n         'R2_score': '{0:.2f}'.format(r2_score(y_test, y_hat))}\n\nmodel_table=model_table.append(stat, ignore_index=True)\n\nplt.plot(X_test, y_test, 'o', color='tab:blue')\nplt.plot(X_test, y_hat, '-', color='tab:orange')\nplt.show()\n\nmodel_table=model_table.drop_duplicates()\nmodel_table","0467c298":"X=np.array(model['mileage']).reshape((-1,1))\ny=np.array(model['price'])\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=2\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', LinearRegression())\n]\n\npipeline=Pipeline(steps)\npipeline.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training score: {}'.format(pipeline.score(X_train, y_train)))\nprint('Test score: {}'.format(pipeline.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=pipeline.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \n    \n\nstat={'Name':'Polynomial Regresion',\n         'MAE': '{0:.0f}'.format(mean_absolute_error(y_test, y_hat)),\n         'RMSE': '{0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat))),\n         'R2_score': '{0:.2f}'.format(r2_score(y_test, y_hat))}\n\nmodel_table=model_table.append(stat, ignore_index=True)\n\nplt.figure(figsize=(6,6))\nplt.plot(X_test, y_test, 'o', color='tab:blue')\n\n#polyline\nnew_X=np.linspace(min(X_test),max(X_test),len(X_test)*1000)\nnew_y=pipeline.predict(new_X)\nplt.plot(new_X, new_y, '-', color='tab:orange')\n\nplt.show()\n\nmodel_table=model_table.drop_duplicates()\nmodel_table","839769f1":"X=np.array(model['mileage']).reshape((-1,1))\ny=np.array(model['price'])\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=np.arange(3,7)\nd\n\nfig, axes = plt.subplots(1,len(d), figsize=(24,6))\n\nfor i, d in enumerate(d):\n    \n    steps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', LinearRegression())\n    ]\n    pipeline=Pipeline(steps)\n    pipeline.fit(X_train, y_train)\n\n    #print('\\n')\n    #print('Training score: {}'.format(pipeline.score(X_train, y_train)))\n    #print('Test score: {}'.format(pipeline.score(X_test, y_test)))\n    #print('\\n')\n\n    y_hat=pipeline.predict(X_test)\n\n    #for i in range(0,11):\n        #print(i)\n        #print('Actual value: ', y_test[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n\n\n    order=str(d)\n    stat={'Name':'Polynomial Regresion Order: '+str(d),\n             'MAE': '{0:.0f}'.format(mean_absolute_error(y_test, y_hat)),\n             'RMSE': '{0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat))),\n             'R2_score': '{0:.2f}'.format(r2_score(y_test, y_hat))}\n\n    model_table=model_table.append(stat, ignore_index=True)\n\n    #plt.figure(figsize=(6,6))\n    axes[i].plot(X_test, y_test, 'o', color='tab:blue')\n\n    new_X=np.linspace(min(X_test),max(X_test),len(X_test)*1000)\n    new_y=pipeline.predict(new_X)\n    axes[i].plot(new_X, new_y, '-', color='tab:orange')\n\n    model_table=model_table.drop_duplicates()\n    \nplt.show()\nmodel_table","655d7e44":"#So... \n#The simple lineare regression looks underfiting and the polynomial regressions from 5th to 6th order look overfiting.\n#We can notice that the 2nd and 4rd order polynomial regression work well and show not so bad result in MAE, RMSE and R2 score.\n#But the 3rd and 4rd order polynomial regression line are little confused with very large mileage, It can be improved\n#Lets try to improve all of them and look the result","c7e100e7":"#We can use Ridge\nX=np.array(model['mileage']).reshape((-1,1))\ny=np.array(model['price'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train: ', X_train.shape)\nprint('y train: ', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=np.arange(2,5)\nd\n\nfig, axes = plt.subplots(1,len(d), figsize=(24,6))\n\nfor i, d in enumerate(d):\n\n    steps=[\n        ('scalar', StandardScaler()),\n        ('poly', PolynomialFeatures(degree=d)),\n        ('model', Ridge(alpha=10, fit_intercept=True))\n    ]\n\n    pipeline=Pipeline(steps)\n\n    pipeline.fit(X_train, y_train)\n\n    y_hat=pipeline.predict(X_test) \n    \n    order=str(d)\n    stat={'Name':' Ridge Polynomial Regresion Order: '+str(d),\n             'MAE': '{0:.0f}'.format(mean_absolute_error(y_test, y_hat)),\n             'RMSE': '{0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat))),\n             'R2_score': '{0:.2f}'.format(r2_score(y_test, y_hat))}\n\n    mode_table=model_table.append(stat, ignore_index=True)\n\n    #plt.figure(figsize=(6,6))\n    axes[i].plot(X_test, y_test, 'o', color='tab:blue')\n\n    new_X=np.linspace(min(X_test),max(X_test),len(X_test)*1000)\n    new_y=pipeline.predict(new_X)\n    axes[i].plot(new_X, new_y, '-', color='tab:orange')\n\n    model_table=model_table.drop_duplicates()\n    \nplt.show()\nmodel_table","97ee10c3":"#Lets try to use Lasso\nX=np.array(model['mileage']).reshape((-1,1))\ny=np.array(model['price'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train: ', X_train.shape)\nprint('y train: ', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=np.arange(2,5)\nd\n\nfig, axes = plt.subplots(1,len(d), figsize=(24,6))\n\nfor i, d in enumerate(d):\n\n    steps=[\n        ('scalar', StandardScaler()),\n        ('poly', PolynomialFeatures(degree=d)),\n        ('model', Lasso(alpha=0.3, fit_intercept=True))\n    ]\n\n    pipeline=Pipeline(steps)\n\n    pipeline.fit(X_train, y_train)\n\n    y_hat=pipeline.predict(X_test) \n    \n    order=str(d)\n    stat={'Name':' Lasso Polynomial Regresion Order: '+str(d),\n             'MAE': '{0:.0f}'.format(mean_absolute_error(y_test, y_hat)),\n             'RMSE': '{0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat))),\n             'R2_score': '{0:.2f}'.format(r2_score(y_test, y_hat))}\n\n    model_table=model_table.append(stat, ignore_index=True)\n\n    #plt.figure(figsize=(6,6))\n    axes[i].plot(X_test, y_test, 'o', color='tab:blue')\n\n    new_X=np.linspace(min(X_test),max(X_test),len(X_test)*1000)\n    new_y=pipeline.predict(new_X)\n    axes[i].plot(new_X, new_y, '-', color='tab:orange')\n\n    model_table=model_table.drop_duplicates()\n    \nplt.show()\nmodel_table","5b188b91":"#So it's not that I want to see =(\n#But the most efficient way is to use Ridge\n#Lest make some improvisation","ece6dfae":"## Lets transform X to sqrt of X and use polynomial regression\nX=np.array(model['mileage']).reshape((-1,1))\ny=np.array(model['price'])\n\nget_root=FunctionTransformer(np.sqrt)\nX=get_root.transform(X)\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\nprint('\\n')\n\nd=np.arange(2,5)\nd\n\nfig, axes = plt.subplots(1,len(d), figsize=(24,6))\n\nfor i, d in enumerate(d):\n\n    steps=[\n        ('scalar', StandardScaler()),\n        ('poly', PolynomialFeatures(degree=d)),\n        ('model', Ridge(alpha=10, fit_intercept=True))\n    ]\n\n    pipeline=Pipeline(steps)\n\n    pipeline.fit(X_train, y_train)\n\n    y_hat=pipeline.predict(X_test) \n    \n    order=str(d)\n    stat={'Name':' Ridge Polynomial Regresion Sqrt Order: '+str(d),\n             'MAE': '{0:.0f}'.format(mean_absolute_error(y_test, y_hat)),\n             'RMSE': '{0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat))),\n             'R2_score': '{0:.2f}'.format(r2_score(y_test, y_hat))}\n\n    model_table=model_table.append(stat, ignore_index=True)\n\n    #plt.figure(figsize=(6,6))\n    axes[i].plot(np.power(X_test,2), y_test, 'o', color='tab:blue')\n\n    new_X=np.linspace(min(X_test),max(X_test),len(X_test)*1000)\n    new_y=pipeline.predict(new_X)\n    axes[i].plot(np.power(new_X,2), new_y, '-', color='tab:orange')\n\n    model_table=model_table.drop_duplicates()\n    \nplt.show()\nmodel_table","92e5c045":"#I suppose that the best model is Sqrt polynomial regression with ridge regularization =)","64dbcad5":"#Only one model and only two featurs\nb_mat=data['model']==top_model\nmodel=data[b_mat]\nmodel=model[['mileage','year','price']]\nprint(model.shape)\nmodel.head()","27e96dea":"model_table2=pd.DataFrame(columns=['Name','MAE','RMSE','R2_score'])\nmodel_table2\n\nX=model[['mileage','year']]\n#X.head()\ny=model['price']\n#y.head()\n\nget_root=FunctionTransformer(np.sqrt)\nX=get_root.transform(X)\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=2\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', Ridge(alpha=10, fit_intercept=True))\n]\n\npipeline=Pipeline(steps)\n\npipeline.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training Score: {}'.format(pipeline.score(X_train, y_train)))\nprint('Test Score: {}'.format(pipeline.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=pipeline.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test.iloc[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \nprint('MAE: {0:.0f}'.format(mean_absolute_error(y_test, y_hat))),\nprint('RMSE: {0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat)))),\nprint('R2_score: {0:.2f}'.format(r2_score(y_test, y_hat)))","27b833a9":"#Lets plot it in 3D\nx1=np.power(X_test.iloc[:,0],2)\n#print(x[:5])\nx2=np.power(X_test.iloc[:,1],2)\n#print(y[:5])\ny=y_test\n#print(z[:5])\n\nfig=plt.figure(figsize=(24,8))\n\naxes1=fig.add_subplot(131, projection='3d')\naxes2=fig.add_subplot(132)\naxes3=fig.add_subplot(133)\n\n\naxes1.scatter(x1, x2 , y,\n          marker='o')\naxes1.set_xlabel('mileage')\naxes1.set_ylabel('year')\naxes1.set_zlabel('price')\n\naxes2.scatter(x1, y, marker='o')\n\naxes2.set_xlabel('mileage')\naxes2.set_ylabel('price')\n\naxes3.scatter(x2, y, marker='o')\n\naxes3.set_xlabel('year')\naxes3.set_ylabel('price')\n\nfig.show()","2e124da7":"features=pd.get_dummies(data)\n\ncol=all_corr.index\ny=features['price']\nX=features[col]\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=2\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', Ridge(alpha=10, fit_intercept=True))\n]\n\npipeline=Pipeline(steps)\n\npipeline.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training Score: {}'.format(pipeline.score(X_train, y_train)))\nprint('Test Score: {}'.format(pipeline.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=pipeline.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test.iloc[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \nprint('MAE: {0:.0f}'.format(mean_absolute_error(y_test, y_hat))),\nprint('RMSE: {0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat)))),\nprint('R2_score: {0:.2f}'.format(r2_score(y_test, y_hat)))","02dbc87a":"features=pd.get_dummies(data)\n\ncol=all_corr.index\ny=features['price']\nX=features[col]\n\nX['mileage']=np.sqrt(X['mileage'])\nX['year']=np.sqrt(X['year'])\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=2\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', Ridge(alpha=10, fit_intercept=True))\n]\n\npipeline=Pipeline(steps)\n\npipeline.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training Score: {}'.format(pipeline.score(X_train, y_train)))\nprint('Test Score: {}'.format(pipeline.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=pipeline.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test.iloc[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \nprint('MAE: {0:.0f}'.format(mean_absolute_error(y_test, y_hat))),\nprint('RMSE: {0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat)))),\nprint('R2_score: {0:.2f}'.format(r2_score(y_test, y_hat)))","917c4fd3":"#Some Calstering may improve the model\n#data.head()\nclas=data.drop(['mileage','year'], axis='columns')\nclas=clas.select_dtypes(include=np.number)\nclas.head()\n\ncol_wo_price=clas.columns.drop('price')\n\nplot=sns.pairplot(data=clas,\n    x_vars=col_wo_price, y_vars=['price'],\n                  height=5)\n\nfor ax in plot.axes.flat:\n    ax.tick_params(axis='x', labelrotation=45)\n    \nplt.show()","ad50475c":"fig, axes = plt.subplots(1,3, figsize=(24,8))\n\nfor i, c in enumerate(col_wo_price):\n    print(c)\n    wcss=[]\n    #print(clas[['price', c]].head())\n    for n in range(1,11):\n        kmeans = KMeans(n_clusters= n, init='k-means++', random_state=0)\n        kmeans.fit(clas[['price', c]])\n        wcss.append(kmeans.inertia_)\n\n    axes[i].plot(range(1,11), wcss)\n    axes[i].set_title(c)\n    #axes[i].set_xlabel('no of clusters')\n    #axes[i].set_ylabel('wcss')\n    #print(wcss)\n        \nplt.show()","24f000fd":"clas=data.drop(['mileage','year'], axis='columns')\nclas=clas.select_dtypes(include=np.number)\n#clas.head()\ncol_wo_price=clas.columns.drop('price')\n\nfor i, c in enumerate(col_wo_price):\n\n    X=np.array(clas[c]).reshape((-1,1))\n    y=np.array(clas['price'])\n\n    steps=[\n        ('scalar', StandardScaler()),\n        ('kmeans', KMeans(n_clusters=6, init='k-means++', random_state=0))\n    ]\n\n    pipeline=Pipeline(steps)\n\n    y_kmeans= pipeline.fit_predict(X, y)\n\n    new_clas=clas\n    col=c+'_class'\n    new_clas[col]=y_kmeans\n    \nnew_clas.head()\n\nfig=plt.figure(figsize=(24,24))\n\nax=[None for _ in range(9)]\n\nax[0] = plt.subplot2grid((5,3), (0,0), colspan=1)\nax[1] = plt.subplot2grid((5,3), (0,1), colspan=1)\nax[2] = plt.subplot2grid((5,3), (0,2), colspan=1)\n\nax[3] = plt.subplot2grid((5,3), (1,0), colspan=1)\nax[4] = plt.subplot2grid((5,3), (1,1), colspan=1)\nax[5] = plt.subplot2grid((5,3), (1,2), colspan=1)\n\n#ax[3] = plt.subplot2grid((5,3), (1,0), colspan=3, rowspan=2)\n\nax[6] = plt.subplot2grid((5,3), (2,0), colspan=3, rowspan=2)\n\ncoln=(len(new_clas.columns)-1)\/2+1\n#print(type(coln))\nnew_col=new_clas.columns[int(coln):]\n\nfor i, c in enumerate(new_col):\n    print(i,c)\n    print(c.rstrip('_class'))\n    sns.scatterplot(data=new_clas, x=c.rstrip('_class'), y='price', hue=c, palette='tab10',\n                    ax=ax[i])\n    \nfor i, c in enumerate(new_col):\n    print(i,c)\n    print(c.rstrip('_class'))\n    sns.boxplot(data=new_clas, x=c, y='price',palette='tab10',\n                    ax=ax[i+3])\n\ncorr_clas=new_clas#[['price','class']]\nfor c in new_col:\n    corr_clas=corr_clas.astype({c:'category'})\n\n#corr_clas=corr_clas.astype({'class': 'category'})\n#corr_clas.dtypes\ncorr_clas=pd.get_dummies(corr_clas)\ncorr_clas=corr_clas.corr()\ncorr_clas\n\n#mask = np.zeros_like(corr_clas)\n#mask[np.triu_indices_from(mask)] = True\n#sns.heatmap(corr_clas, annot=True, mask=mask, vmax=.3, square=True)\n\ncond=[\n    abs(corr_clas['price']).gt(0).le(0.2),\n    abs(corr_clas['price']).gt(0.2).le(0.5),\n    abs(corr_clas['price']).gt(0.5).le(0.7),\n    abs(corr_clas['price']).gt(0.7).le(0.9),\n    abs(corr_clas['price']).gt(0.9)\n    ]\nvalue=['Very Low','Low','Everage','High','Very High']\ncolor=['whitesmoke','tan','darkorange','limegreen','darkgreen',]\n\ncorr_clas['corr_type']=np.select(cond, value)\ncorr_clas['color']=np.select(cond, color)\n\n\n\nax[4]=corr_clas['price'].drop('price').plot(kind='bar', color=corr_clas['color'].drop('price'))\n\nnew_clas.head()","3f53c1b3":"new_data=data.drop(col_wo_price, axis=1)\nnew_data.head()\nnew_data=pd.concat([new_data, clas.drop('price', axis=1)], axis=1)\nnew_data=new_data.drop(col_wo_price, axis=1)\n\nfor c in new_col:\n    new_data=new_data.astype({c:'category'})\n\nnew_data.head()","5220916b":"features=pd.get_dummies(new_data)\nfeatures.head()\n\ndata_wo_model=pd.get_dummies(new_data.drop('model', axis='columns'))\n#MB_data_wo_model.columns=MB_data_wo_model.columns.str.replace('.*_', 'bla_').str.strip('bla_')\n#MB_data_wo_model.head()\nprice_corr=data_wo_model.corr()['price'].sort_values()[:-1]\nprice_corr=pd.DataFrame(price_corr)\nprice_corr.columns=['corr']\n\n#set colors\ncond=[\n    abs(price_corr['corr']).gt(0).le(0.2),\n    abs(price_corr['corr']).gt(0.2).le(0.5),\n    abs(price_corr['corr']).gt(0.5).le(0.7),\n    abs(price_corr['corr']).gt(0.7).le(0.9),\n    abs(price_corr['corr']).gt(0.9)\n    ]\nvalue=['Very Low','Low','Everage','High','Very High']\ncolor=['whitesmoke','tan','darkorange','limegreen','darkgreen',]\n\nprice_corr['corr_type']=np.select(cond, value)\nprice_corr['color']=np.select(cond, color)\n\n#price_corr\n\nbar=price_corr['corr'].plot(kind='bar', \n                figsize=(24,10),\n                color=price_corr['color'])\nbar.set_title('Price Correlation\\n', fontsize=16, color='tab:blue')\nbar.set_ylabel('Price Correation')\nbar.set_xlabel('Feutures')\n\nplt.xticks(rotation=45)\nplt.show()","11914af2":"all_corr=pd.get_dummies(new_data).corr()['price'].sort_values()\nall_corr=pd.DataFrame(all_corr)\nall_corr['abs']=abs(all_corr['price'])\nall_corr.sort_values(by='abs', ascending=False, inplace=True)\nall_corr=all_corr.iloc[1:]\nall_corr['price']","dc97873f":"features=pd.get_dummies(new_data)\n\ncol=all_corr.index\ny=features['price']\nX=features[col]\n\nX['mileage']=np.sqrt(X['mileage'])\nX['year']=np.sqrt(X['year'])\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=2\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', Ridge(alpha=10, fit_intercept=True))\n]\n\npipeline=Pipeline(steps)\n\npipeline.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training Score: {}'.format(pipeline.score(X_train, y_train)))\nprint('Test Score: {}'.format(pipeline.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=pipeline.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test.iloc[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \nprint('MAE: {0:.0f}'.format(mean_absolute_error(y_test, y_hat))),\nprint('RMSE: {0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat)))),\nprint('R2_score: {0:.2f}'.format(r2_score(y_test, y_hat)))","2589d76a":"wcss=[]\n\nfor i in range(1,21):\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(clas)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(1,21), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('no of clusters')\nplt.ylabel('wcss')\nplt.show()\n\n#print(wcss)\n#print(wcss.index(min(wcss)))","a3517c57":"clas=data.drop(['mileage','year'], axis='columns')\nclas=clas.select_dtypes(include=np.number)\n#clas.head()\n\ncol_wo_price=clas.columns.drop('price')\nX=clas[col_wo_price]\ny=clas['price']\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('kmeans', KMeans(n_clusters=13, init='k-means++', random_state=0))\n]\n\npipeline=Pipeline(steps)\n\ny_kmeans= pipeline.fit_predict(X, y)\n\nnew_clas=clas\nnew_clas['class']=y_kmeans\nnew_clas\n\nfig=plt.figure(figsize=(24,24))\n\nax=[None for _ in range(9)]\n\nax[0] = plt.subplot2grid((5,3), (0,0), colspan=1)\nax[1] = plt.subplot2grid((5,3), (0,1), colspan=1)\nax[2] = plt.subplot2grid((5,3), (0,2), colspan=1)\n\nax[3] = plt.subplot2grid((5,3), (1,0), colspan=3, rowspan=2)\n\nax[4] = plt.subplot2grid((5,3), (3,0), colspan=3, rowspan=2)\n\ncol_wo_price=clas.columns.drop(['price', 'class'])\n\nfor i, c in enumerate(col_wo_price):\n    #print(i,c)\n    sns.scatterplot(data=new_clas, x=c, y='price', hue='class', palette='tab10',\n                    ax=ax[i])\n    \nsns.boxplot(data=new_clas, x='class', y='price', palette='tab10',\n            ax=ax[3])\n\n#new_clas=new_clas[new_clas['class']==3]\n#plot=sns.pairplot(data=new_clas,\n    #x_vars=col_wo_price, y_vars=['price'], hue='class',\n                  #height=5)\n        \ncorr_clas=new_clas[['price','class']]\ncorr_clas=corr_clas.astype({'class': 'category'})\n#corr_clas.dtypes\ncorr_clas=pd.get_dummies(corr_clas)\ncorr_clas=corr_clas.corr()\ncorr_clas\n\n#mask = np.zeros_like(corr_clas)\n#mask[np.triu_indices_from(mask)] = True\n#sns.heatmap(corr_clas, annot=True, mask=mask, vmax=.3, square=True)\n\ncond=[\n    abs(corr_clas['price']).gt(0).le(0.2),\n    abs(corr_clas['price']).gt(0.2).le(0.5),\n    abs(corr_clas['price']).gt(0.5).le(0.7),\n    abs(corr_clas['price']).gt(0.7).le(0.9),\n    abs(corr_clas['price']).gt(0.9)\n    ]\nvalue=['Very Low','Low','Everage','High','Very High']\ncolor=['whitesmoke','tan','darkorange','limegreen','darkgreen',]\n\ncorr_clas['corr_type']=np.select(cond, value)\ncorr_clas['color']=np.select(cond, color)\n\n\n\nax[4]=corr_clas['price'].drop('price').plot(kind='bar', color=corr_clas['color'].drop('price'))\n        \nfig.tight_layout()\nplt.show()","597eee15":"data['class']=y_kmeans\ndata=data.astype({'class': 'category'})\ndata=data.drop(col_wo_price, axis=1)\ndata.head()","7c375124":"features=pd.get_dummies(data)\nfeatures.head()\n\ndata_wo_model=pd.get_dummies(data.drop('model', axis='columns'))\n#MB_data_wo_model.columns=MB_data_wo_model.columns.str.replace('.*_', 'bla_').str.strip('bla_')\n#MB_data_wo_model.head()\nprice_corr=data_wo_model.corr()['price'].sort_values()[:-1]\nprice_corr=pd.DataFrame(price_corr)\nprice_corr.columns=['corr']\n\n#set colors\ncond=[\n    abs(price_corr['corr']).gt(0).le(0.2),\n    abs(price_corr['corr']).gt(0.2).le(0.5),\n    abs(price_corr['corr']).gt(0.5).le(0.7),\n    abs(price_corr['corr']).gt(0.7).le(0.9),\n    abs(price_corr['corr']).gt(0.9)\n    ]\nvalue=['Very Low','Low','Everage','High','Very High']\ncolor=['whitesmoke','tan','darkorange','limegreen','darkgreen',]\n\nprice_corr['corr_type']=np.select(cond, value)\nprice_corr['color']=np.select(cond, color)\n\n#price_corr\n\nbar=price_corr['corr'].plot(kind='bar', \n                figsize=(24,10),\n                color=price_corr['color'])\nbar.set_title('Price Correlation\\n', fontsize=16, color='tab:blue')\nbar.set_ylabel('Price Correation')\nbar.set_xlabel('Feutures')\n\nplt.xticks(rotation=45)\nplt.show()","f68023fe":"all_corr=pd.get_dummies(data).corr()['price'].sort_values()\nall_corr=pd.DataFrame(all_corr)\nall_corr['abs']=abs(all_corr['price'])\nall_corr.sort_values(by='abs', ascending=False, inplace=True)\nall_corr=all_corr.iloc[1:]\nall_corr['price']","5152fabb":"features=pd.get_dummies(data)\n\ncol=all_corr.index\ny=features['price']\nX=features[col]\n\nX['mileage']=np.sqrt(X['mileage'])\nX['year']=np.sqrt(X['year'])\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('X train:', X_train.shape)\nprint('y train:', y_train.shape)\nprint('X test: ', X_test.shape)\nprint('y test: ', y_test.shape)\n\nd=2\n\nsteps=[\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=d)),\n    ('model', Ridge(alpha=10, fit_intercept=True))\n]\n\npipeline=Pipeline(steps)\n\npipeline.fit(X_train, y_train)\n\nprint('\\n')\nprint('Training Score: {}'.format(pipeline.score(X_train, y_train)))\nprint('Test Score: {}'.format(pipeline.score(X_test, y_test)))\nprint('\\n')\n\ny_hat=pipeline.predict(X_test)\n\nfor i in range(0,11):\n    #print(i)\n    print('Actual value: ', y_test.iloc[i], 'prediction: ', '{0:.0f}'.format(y_hat[i]), '\\n')\n    \nprint('MAE: {0:.0f}'.format(mean_absolute_error(y_test, y_hat))),\nprint('RMSE: {0:.0f}'.format(np.sqrt(mean_squared_error(y_test, y_hat)))),\nprint('R2_score: {0:.2f}'.format(r2_score(y_test, y_hat)))","439cd2ab":"2 features LINEAR REGRESSION","d5db3e4e":"POLYNOMIAL REGRESSION with One Model","b716617b":"POLYNOMIAL REGRESSION with One Model, degree from 3 to 6","b775e53b":"SIMPLE LINEAR REGRESSION with One Model","e7a8a3d0":"MULTI-LINEAR REGRESSION"}}