{"cell_type":{"07bf43f3":"code","7e34b359":"code","821a167e":"code","17687903":"code","140a5a6f":"code","54bbc75d":"code","d9071a3a":"code","abbb64d6":"code","cc4dd120":"code","58a7de75":"code","25d139a7":"code","b819c2fd":"code","cb018c59":"code","c9f5eda2":"code","b16a1e99":"code","fa41f74f":"code","5b3cd163":"code","1f4121b9":"code","bdf5a8bb":"code","e4da56bd":"code","325f814d":"code","1f96d0d7":"code","15627be5":"code","ddde7ddd":"code","21ca6736":"code","0cd4e6eb":"code","0056c21d":"code","fdac73d8":"code","5aba5765":"code","551acf10":"code","69ac34b8":"code","f3718634":"code","2f1cea5f":"code","8b8ba438":"code","b712c435":"code","f1e28f52":"code","eda72b98":"code","347d4e6e":"code","371f122b":"code","8955352b":"code","dda59f09":"code","7b534e15":"code","e3c6da45":"code","4e0554bf":"code","b9ddd9b3":"code","a152a74b":"code","8a379500":"code","73c57d3f":"code","409a839d":"code","344101ef":"code","6205a6d2":"code","e66ca113":"code","16f81cbf":"code","486a0058":"code","1fb0f494":"code","87cae566":"code","2ea93612":"code","7598f8ad":"code","2f5bb267":"code","7ad2060a":"code","d78b2436":"code","ff0cb959":"code","5766f567":"code","4876d14f":"markdown","0f042eb6":"markdown","9565e698":"markdown","d4ca29b3":"markdown","cb008074":"markdown","e53d86d7":"markdown","622a3b90":"markdown","828533d2":"markdown","a93fa7d3":"markdown"},"source":{"07bf43f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e34b359":"import scipy.io as sio\ntest = sio.loadmat('\/kaggle\/input\/alcohol.mat')\ntest","821a167e":"test['alcohol'].shape","17687903":"df1=pd.read_csv('\/kaggle\/input\/normal.csv',index_col=0)\ndf2=pd.read_csv('\/kaggle\/input\/alcohol.csv',index_col=0)\ndf1=df1.append(df2,ignore_index=True)","140a5a6f":"df1.info()","54bbc75d":"df1.head(1)","d9071a3a":"df=df1\nimport seaborn as sns\nsns.countplot(x='type', data=df)","abbb64d6":"df.isnull().sum()","cc4dd120":"df=df.sample(frac=1).reset_index(drop=True)","58a7de75":"df.head()","25d139a7":"x=df.drop([\"type\"]  ,axis=1)\nx.shape","b819c2fd":"y = df.loc[:,'type'].values\ny.shape","cb018c59":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nx = scaler.transform(x)\nfrom keras.utils import to_categorical\ny = to_categorical(y)\ny","c9f5eda2":"x","b16a1e99":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=4)\n# x = pca.fit_transform(x)","fa41f74f":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)","5b3cd163":"x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))","1f4121b9":"x_train.shape","bdf5a8bb":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\n\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\ntf.keras.backend.clear_session()\n\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=(1,2048),activation=\"relu\",return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,activation=\"sigmoid\"))\nmodel.add(Dropout(0.2))\n#model.add(LSTM(100,return_sequences=True))\n#model.add(Dropout(0.2))\n#model.add(LSTM(50))\n#model.add(Dropout(0.2))\nmodel.add(Dense(2, activation='sigmoid'))\nfrom keras.optimizers import SGD\nmodel.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\nmodel.summary()","e4da56bd":"history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\nscore, acc = model.evaluate(x_test, y_test)","325f814d":"from sklearn.metrics import accuracy_score\npred = model.predict(x_test)\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y_test,axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","1f96d0d7":"x_complete=x\nx_complete = np.reshape(x_complete, (x_complete.shape[0],1,x_complete.shape[1]))\nfrom sklearn.metrics import accuracy_score\npred = model.predict(x_complete)\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y,axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","15627be5":"x=df.drop([\"type\"] ,axis=1).values\nx.shape\ny = df.loc[:,'type'].values\ny.shape\ny=to_categorical(y)","ddde7ddd":"x.shape","21ca6736":"type(x)","0cd4e6eb":"y","0056c21d":"a = np.zeros(shape=(240,64,32))","fdac73d8":"for i in range(len(x)):\n    a[i]=np.reshape(x[i],(-1,32))\na.shape","5aba5765":"x=a","551acf10":"from sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)\n\nx_train = x_train.reshape(x_train.shape[0], 64, 32, 1)\nx_test = x_test.reshape(x_test.shape[0], 64, 32, 1)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\nfrom keras.utils import np_utils\n# building a linear stack of layers with the sequential model\nmodel = Sequential()\n# convolutional layer\nmodel.add(Conv2D(64, kernel_size=(5,5), strides=(1,1), padding='valid', activation='relu', input_shape=(64,32,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\n# model.add(Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n# model.add(MaxPool2D(pool_size=(2,2)))\n\n# model.add(Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n# model.add(MaxPool2D(pool_size=(2,2)))\n\n# model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n# model.add(MaxPool2D(pool_size=(2,2)))\n\n# flatten output of conv\nmodel.add(Flatten())\n# hidden layer\nmodel.add(Dense(1000, activation='relu'))\n# # hidden layer\n# model.add(Dense(512, activation='relu'))\n# # hidden layer\n# model.add(Dense(256, activation='relu'))\n# # hidden layer\n# model.add(Dense(64, activation='relu'))\n# # hidden layer\n# model.add(Dense(16, activation='relu'))\n# output layer\nmodel.add(Dense(2, activation='softmax'))\n\n# compiling the sequential model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n# mcp_save = ModelCheckpoint('2dcnn_mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n\n# training the model for 10 epochs\nmodel.fit(x_train, y_train, batch_size=1, epochs=50, validation_data=(x_test, y_test))#callbacks=[earlyStopping, mcp_save, reduce_lr_loss]","69ac34b8":"x_complete=x\nx_complete = x_complete.reshape(x_complete.shape[0], 64, 32, 1)\nfrom sklearn.metrics import accuracy_score\npred = model.predict(x_complete)\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y,axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","f3718634":"x_complete.shape","2f1cea5f":"pred = model.predict(x_complete[:48])\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y[:48],axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","8b8ba438":"from sklearn.metrics import accuracy_score\npred = model.predict(x_complete[48:96])\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y[48:96],axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","b712c435":"from sklearn.metrics import accuracy_score\npred = model.predict(x_complete[96:144])\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y[96:144],axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","f1e28f52":"from sklearn.metrics import accuracy_score\npred = model.predict(x_complete[144:192])\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y[144:192],axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","eda72b98":"from sklearn.metrics import accuracy_score\npred = model.predict(x_complete[192:])\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y[192:],axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","347d4e6e":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\ntype(X_train)","371f122b":"# X = np.random.randn(4000,270)\n# y = np.ones((4000,1))\n# y[0:999] = 2\n# y[1000:1999] = 3\n# y[2000:2999] = 0\nx=df.drop([\"type\"]  ,axis=1).values\ny = df.loc[:,'type'].values\ny=to_categorical(y)\nfrom keras.layers import Dense, InputLayer, Dropout, Flatten, BatchNormalization, Conv1D, MaxPooling1D\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nnum_classes = 2\n\nX_train = X_train.reshape(X_train.shape[0], 2048,1).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 2048, 1).astype('float32') \n\nmodel = Sequential()\nmodel.add(Conv1D(filters=32, kernel_size=5, input_shape=(2048, 1)))\nmodel.add(MaxPooling1D(pool_size=5 ))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n\n# compiling the sequential model\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n# training the model for 10 epochs\nmodel.fit(X_train, y_train, batch_size=1, epochs=30, validation_data=(X_test, y_test))","8955352b":"x_complete=x\nx_complete = x_complete.reshape(x_complete.shape[0], 2048,1).astype('float32')\nfrom sklearn.metrics import accuracy_score\npred = model.predict(x_complete)\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y,axis=1)\nprint(expected_classes.shape)\nprint(predict_classes.shape)\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Testing Accuracy: {correct}\")","dda59f09":"import scipy.io as sio\ntemp1 = sio.loadmat('\/kaggle\/input\/alcohol.mat')\nalcohol=temp1['alcohol']\nalcohol.shape","7b534e15":"import scipy.io as sio\ntemp1 = sio.loadmat('\/kaggle\/input\/normal.mat')\nnormal=temp1['normal']\nnormal.shape","e3c6da45":"y=[1 for i in range(120)]\ny.extend([0 for i in range(120)])\ny=np.array(y)\ny.shape","4e0554bf":"x=np.concatenate([alcohol,normal])\nx.shape","b9ddd9b3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)","a152a74b":"y_train","8a379500":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import cross_val_score\nimport itertools\nfrom itertools import chain\nimport time\nstart = time.time()\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\nprediction = clf_rf.predict(X_test)\ny_score = clf_rf.predict_proba(X_test)[:,1]\nscores = cross_val_score(clf_rf, x, y, cv=5)\nend = time.time()\n\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nfrom sklearn.metrics import roc_auc_score\nprint('auc= ',roc_auc_score(y_test,y_score))\n\nstart = time.time()\n\nclf_et = ExtraTreesClassifier()\nclf_et.fit(X_train, y_train)\nprediction = clf_et.predict(X_test)\nscores = cross_val_score(clf_et, x, y, cv=5)\n\nend = time.time()\n\n\nprint(\"Extra Trees Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nstart = time.time()\n\nclf_dt = DecisionTreeClassifier()\nclf_dt.fit(X_train, y_train)\nprediction = clf_dt.predict(X_test)\nscores = cross_val_score(clf_dt, x, y, cv=5)\nprint(confusion_matrix( y_test,prediction))\nend = time.time()\n\nprint(\"Dedicion Tree Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))","73c57d3f":"start = time.time()\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\nprediction = clf_rf.predict(x)\ny_score = clf_rf.predict_proba(x)[:,1]\nscores = cross_val_score(clf_rf, x, y, cv=5)\nend = time.time()\n\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y,prediction))\nfrom sklearn.metrics import roc_auc_score\nprint('auc= ',roc_auc_score(y,y_score))","409a839d":"X=x\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\n\nstart = time.time()\n\nclf = SVC(kernel='linear',C=1,gamma=1)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\n\nend = time.time()\nlabels = np.unique(y_test)\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC LINEAR Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))\n\nprint(confusion_matrix(y_test, prediction,labels=labels))\n\nstart = time.time()\n\nclf_svmpoly = SVC(kernel='poly')\nclf_svmpoly.fit(X_train, y_train)\nprediction = clf_svmpoly.predict(X_test)\nscores = cross_val_score(clf_svmpoly, X, y, cv=5)\n\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC POLY Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))\n\nstart = time.time()\n\nclf_svmsigmoid = SVC(kernel='sigmoid',probability=True)\nclf_svmsigmoid.fit(X_train, y_train)\nprediction = clf_svmsigmoid.predict(X_test)\nscores = cross_val_score(clf_svmsigmoid, X, y, cv=5)\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC SIGMOID Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))\n\n\nstart = time.time()\n\nclf_svmrbf = SVC(kernel='rbf',probability=True)\nclf_svmrbf.fit(X_train, y_train)\nprediction = clf_svmrbf.predict(X_test)\nscores = cross_val_score(clf_svmrbf, X, y, cv=5)\n\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC RBF Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nstart = time.time()\n\nclf_svmnu = NuSVC()\nclf_svmnu.fit(X_train, y_train)\nprediciton = clf_svmnu.predict(X_test)\nscores = cross_val_score(clf_svmnu, X, y, cv=5)\nend = time.time()\n\n\nprint(\"NuSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))\nstart = time.time()\n\nclf_svmlinear = LinearSVC()\nclf_svmlinear.fit(X_train, y_train)\nprediction = clf_svmlinear.predict(X_test)\nscores = cross_val_score(clf_svmlinear, X, y, cv=5)\n\n\nprint(\"LinearSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))","344101ef":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","6205a6d2":"from sklearn.linear_model import RidgeClassifier\n# define models and parameters\nmodel = RidgeClassifier()\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n# define grid search\ngrid = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","e66ca113":"from sklearn.neighbors import KNeighborsClassifier\n# define models and parameters\nmodel = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","16f81cbf":"# define model and parameters\nmodel = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","486a0058":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier()\nn_estimators = [1,2,3,4,5,8,10, 100, 1000]\n# define grid search\ngrid = dict(n_estimators=n_estimators)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","1fb0f494":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nn_estimators = [10, 100, 1000]\nmax_features = ['sqrt', 'log2']\n# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","87cae566":"# from sklearn.ensemble import GradientBoostingClassifier\n# # define models and parameters\n# model = GradientBoostingClassifier()\n# n_estimators = [10, 100, 1000]\n# learning_rate = [0.001, 0.01, 0.1]\n# subsample = [0.5, 0.7, 1.0]\n# max_depth = [3, 7, 9]\n# # define grid search\n# grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n# grid_result = grid_search.fit(X, y)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","2ea93612":"svc=SVC(kernel='poly')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(accuracy_score(y_test,y_pred))","7598f8ad":"svc=SVC(kernel='poly')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') #cv is cross validation\nprint(scores)","2f5bb267":"from sklearn.naive_bayes import GaussianNB\n\nstart = time.time()\n\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_train, y_train)\nprediction = clf_gnb.predict(X_test)\nscores = cross_val_score(clf_gnb, X, y, cv=5)\nend = time.time()\n\nprint(\"Gaussian NB Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report( y_test,prediction))","7ad2060a":"start = time.time()\n\nclf_svmpoly = SVC(kernel='poly')\nclf_svmpoly.fit(X_train[:,0].reshape(-1, 1), y_train)\nprediction = clf_svmpoly.predict(X_test[:,0].reshape(-1, 1))\nscores = cross_val_score(clf_svmpoly, X[:,0].reshape(-1, 1), y, cv=5)\n\n\nend = time.time()\n\n# accuracy_all.append(accuracy_score(prediction, y_test))\n# cvs_all.append(np.mean(scores))\n\nprint(\"SVC POLY Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+\/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\nprint(classification_report(y_test, prediction))","d78b2436":"scores","ff0cb959":"from sklearn.feature_selection import SelectKBest, f_classif\nselector = SelectKBest( k=2).fit(X,y)\nx_new = selector.transform(X) # not needed to get the score\nscores = selector.scores_\nscores","5766f567":"X_train[:,0]","4876d14f":"**Bad accuracy with 1D CNN**","0f042eb6":"# Testing on smaller subset of total dataset at a time","9565e698":"1D CNN","d4ca29b3":"EEG Classifictaion using LSTM\nThis used used LSTM model to classify electroencephalogram (EEG) brain signal and to predict the human emotions .The notebook classifies data into 3 classes negative,nuteral and positive.\n\nThe dataset used for this notebook is freely avialable in the following linkhttps:\/\/www.kaggle.com\/birdy654\/eeg-brainwave-dataset-feeling-emotion\n\nload & read the dataset","cb008074":"**Below one achieves Highest**","e53d86d7":"2d  CNN","622a3b90":"**2D CNN ENDS**","828533d2":"# CHECK USING **ML ALGOS**","a93fa7d3":"testing on complete dataset instead of only test dataset"}}