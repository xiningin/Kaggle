{"cell_type":{"49cff707":"code","6bc84bd6":"code","4cf9687d":"code","502fe3f5":"code","a171330d":"code","b2efd227":"code","c9b8efe3":"code","6b01f4ad":"code","55559724":"code","fdf366d0":"code","11092595":"code","954d0073":"code","bf62a8c0":"code","1ade40ad":"code","7b6f0515":"code","f549083b":"code","efd0d051":"code","c47b0036":"code","05b406b3":"code","a4710807":"code","74c55b99":"code","f559713f":"code","1549841e":"code","4d65a624":"code","734cbbba":"code","6f9af794":"code","c15dcbe8":"code","d84ae292":"code","44de8c5a":"code","3ad7f9e8":"code","98b24403":"code","ecd48f09":"code","2b80608c":"code","771d5bb0":"code","537521f9":"code","94dc73ad":"code","7fe04e6f":"code","2ce5ec6d":"code","1cc8516a":"code","5527b807":"code","54182530":"code","4b54af9f":"code","f5f9329b":"code","7069dd27":"code","6a1a1d08":"code","9fe516a9":"code","4346c4ff":"code","05680a63":"code","e28abb4b":"code","1b9a7f42":"code","4797c3b0":"markdown","1b5d6331":"markdown","032bb7a6":"markdown","139e72cf":"markdown","67517093":"markdown","9741f19b":"markdown","0f1ea671":"markdown","aadd70fa":"markdown","207852f4":"markdown","094d6aa8":"markdown","672ea28d":"markdown","53277e78":"markdown","e1b33fff":"markdown","bfbb4e67":"markdown","75de107b":"markdown","bc4e309d":"markdown","b75a7d3c":"markdown","0e09f74d":"markdown","4319b698":"markdown","a3bc9f9d":"markdown","6bbdddd7":"markdown","e693383a":"markdown","f7e01cb7":"markdown","ac89feda":"markdown","d3931269":"markdown","3e4b4697":"markdown","9a261542":"markdown","d3129177":"markdown","0934b81e":"markdown","a37911cc":"markdown","65b05ccf":"markdown","52340df5":"markdown","8d1fd02e":"markdown","4917bc34":"markdown","90caa1af":"markdown","f3170dc8":"markdown","96e76d17":"markdown","75242442":"markdown","124be644":"markdown","cdeb2ff3":"markdown"},"source":{"49cff707":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bc84bd6":"import matplotlib.pyplot as plt\nimport seaborn as sns","4cf9687d":"df_credit = pd.read_csv(os.path.join(dirname, filename))","502fe3f5":"df_credit.head()","a171330d":"df_credit = df_credit.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis = 1)","b2efd227":"def basic_infos(data):\n    print(\"Dataset shape is: \", data.shape,\"\\n\")\n    print(\"Dataset columns are: \",data.columns,\"\\n\")\n    print(\"Dataset dimensions are:\",data.ndim,\"\\n\")\n    print(\"Dataset information is:\\n\",data.info(),\"\\n\")\n    categorical, numerical = [], []\n    for i in data.columns:\n        if data[i].dtype==object:\n            categorical.append(i)\n        else:\n            numerical.append(i)\n    print(\"Categorical datatype columns are: \", [i for i in categorical],\"\\n\")\n    print(\"Numercial datatype columns are: \", [i for i in numerical],\"\\n\")\n    return categorical, numerical","c9b8efe3":"cat, num = basic_infos(df_credit)","6b01f4ad":"print(df_credit.isnull().sum())","55559724":"labels = df_credit['Attrition_Flag'].value_counts().index.to_list()\nsizes = df_credit['Attrition_Flag'].value_counts()\nexplode=(0,0.2)\nfig, ax=plt.subplots()\nax.pie(sizes, labels = labels, explode = explode, autopct=\"%1.1f%%\", shadow=True, startangle=90)\nax.axis('equal')\nplt.show()","fdf366d0":"plt.figure(figsize=(25,7))\nplt.hist(df_credit['Customer_Age'], edgecolor = 'red')\nplt.show()","11092595":"labels = df_credit['Gender'].value_counts().index.to_list()\nsizes = df_credit['Gender'].value_counts()\nexplode=(0,0.1)\nfig, ax=plt.subplots()\np, tx, autotexts = ax.pie(sizes, labels = labels, explode = explode, autopct=\"\", shadow=True, startangle=90)\nfor i, a in enumerate(autotexts):\n    a.set_text(\"{}\".format(sizes[i]))\nax.axis('equal')\nplt.show()","954d0073":"df_credit['Dependent_count'].value_counts()","bf62a8c0":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Dependent_count'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","1ade40ad":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Education_Level'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","7b6f0515":"labels = df_credit['Marital_Status'].value_counts().index.to_list()\nsizes = df_credit['Marital_Status'].value_counts()\nfig, ax=plt.subplots()\nax.pie(sizes, labels = labels, autopct=\"%1.1f%%\", shadow=True, startangle=90)\nax.axis('equal')\nplt.show()","f549083b":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Income_Category'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","efd0d051":"import squarify #for making treemap, we need squarify\nplt.figure(figsize=(20,8))\nlabels=[i for i in zip(df_credit['Card_Category'].value_counts().index.to_list(), list(df_credit['Card_Category'].value_counts()))]\ncolors = [plt.cm.Spectral(i\/float(len(labels))) for i in range(len(labels))]\nsquarify.plot(sizes=df_credit['Card_Category'].value_counts(),color=colors, label=labels, alpha=.8)","c47b0036":"plt.figure(figsize=(20,8))\nplt.hist(df_credit['Months_on_book'], edgecolor = 'pink')","05b406b3":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Total_Relationship_Count'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","a4710807":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Months_Inactive_12_mon'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","74c55b99":"fig, ax = plt.subplots(figsize = (20,8))\ngraph = sns.countplot(df_credit['Contacts_Count_12_mon'])\nfor p in graph.patches:\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","f559713f":" cat","1549841e":"df_credit['Attrition_Flag'] = df_credit['Attrition_Flag'].astype('category')\ndf_credit['Attrition_Flag'] = df_credit['Attrition_Flag'].cat.codes","4d65a624":"df_credit.head()","734cbbba":"def making_new_df(data, columnlist):\n    for i in columnlist:\n        dummy = pd.get_dummies(data[i])\n        #print(dummy)\n        del dummy[dummy.columns[-1]]\n        data = pd.concat([data, dummy], axis = 1)\n    return data","6f9af794":"df_credit_new = making_new_df(df_credit, cat[1:])","c15dcbe8":"df_credit_new","d84ae292":"corr = df_credit_new.corr(method='pearson')","44de8c5a":"plt.figure(figsize=(35,35))\nax= sns.heatmap(corr, vmin = -1, vmax = 1, square = True, annot=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation = 45, horizontalalignment = 'right', fontsize=10)\nplt.show()","3ad7f9e8":"df_credit_new = df_credit_new.drop(columns= cat[1:], axis = 1)","98b24403":"df_credit_new","ecd48f09":"plt.figure(figsize = (20,6))\nsns.set_style('darkgrid')\n\nx = df_credit_new.loc[:, df_credit_new.columns !='Attrition_Flag']\ny = df_credit_new.loc[:, df_credit_new.columns == 'Attrition_Flag']\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index = x.columns)\nimg = feat_importances.nlargest(10).plot(kind = 'barh')\nlabels1 = feat_importances.nlargest(10).plot(kind = 'barh').get_yticklabels()\nplt.show()","2b80608c":"required_labels = list()\nfor i in labels1:\n    j = str(i)\n    required_labels.append(j[12:len(j)-2])","771d5bb0":"from sklearn.model_selection import train_test_split","537521f9":"X = df_credit_new.loc[:, required_labels]\nY = df_credit_new.iloc[:, 1]","94dc73ad":"train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.25, random_state=42)","7fe04e6f":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nmodel_lr = LR.fit(train_x, train_y)\ny_lr_predict = model_lr.predict(test_x)","2ce5ec6d":"LR_df = pd.DataFrame(data = {\"Actual\": test_y.to_numpy(), \"Predicted\": y_lr_predict})","1cc8516a":"LR_df","5527b807":"model_lr.score(test_x, test_y)","54182530":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nmodel_rfr = rfc.fit(train_x, train_y)\ny_rfr_predict = model_rfr.predict(test_x)\nRFR_df = pd.DataFrame(data = {\"Actual\": test_y, \"Predicted\": y_rfr_predict})","4b54af9f":"RFR_df","f5f9329b":"model_rfr.score(test_x, test_y)","7069dd27":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nmodel_gnb = gnb.fit(train_x, train_y)\ny_gnb_predict = model_gnb.predict(test_x)\nGNB_df = pd.DataFrame(data = {\"Actual\":test_y, \"Predicted\": y_gnb_predict})","6a1a1d08":"GNB_df","9fe516a9":"model_gnb.score(test_x, test_y)","4346c4ff":"from sklearn.svm import SVC\nsvc = SVC()\nmodel_svc = svc.fit(train_x, train_y)\ny_svc_predict = model_svc.predict(test_x)\nsvc_df = pd.DataFrame(data = {\"Actual\":test_y, \"Predicted\": y_svc_predict})","05680a63":"svc_df","e28abb4b":"model_svc.score(test_x, test_y)","1b9a7f42":"x_axis = ['LR', 'RFC', 'NB', 'SVC']\ny_axis = [model_lr.score(test_x, test_y), model_rfr.score(test_x, test_y), model_gnb.score(test_x, test_y), model_svc.score(test_x, test_y)]\nplt.figure(figsize = (10,7))\nplt.plot(x_axis, y_axis, \"*\")\nplt.xticks(rotation = -45)","4797c3b0":"# Feature Selection","1b5d6331":"Lets see what is the age composition of all credit card users","032bb7a6":"## Education_Level","139e72cf":"## Total_Relationship_Count","67517093":"# General information retrival","9741f19b":"## Months_Inactive_12_mon","0f1ea671":"Lets see our holders highest education level","aadd70fa":"## Contacts_Count_12_mon","207852f4":"#### Even though being informative, its clumsy, so now we will try different method so as to get those upmost independent variables upon which Attrition_Flag depends upon","094d6aa8":"### As already told in the main page that to remove bith the Naive_Bayes columns, lets do that and then we will retirive info","672ea28d":"We got the info that 16.1% customers are attrited customers","53277e78":"## Marital_Status","e1b33fff":"## Random Forest Classififer","bfbb4e67":"# Model Making","75de107b":"#### Converting rest cat into dummies","bc4e309d":"We can roughly estimate that most of the credit card holders are in the range of age 40 to 55","b75a7d3c":"# General Exploration","0e09f74d":"## Gender\n","4319b698":"We can see the number of females and males acquiring credit cards","a3bc9f9d":"# Feature Engineering","6bbdddd7":"Lets know the marital status of the individuals","e693383a":"## Naive Bayes","f7e01cb7":"Attrition_Flag tells us whether the customer is existing or attrited","ac89feda":"So with this we come to an end of this notebook, do upvote if you think my work is good. If you find anything wrong, or have any doubt, do ask. I will try to answer them with the knowledge I have.","d3931269":"## Months_on_book","3e4b4697":"## Attrition_Flag","9a261542":"## We got the highest accuracy by Random Forest Classifier algorithm.","d3129177":"## Dependent_count","0934b81e":"## Logistic Regression","a37911cc":"## Support Vector Classifier","65b05ccf":"#### Attrition_Flag","52340df5":"## Income_Category","8d1fd02e":"Removing all those categorical columns","4917bc34":"### Lets split our model into training and testing set","90caa1af":"## Customer_Age","f3170dc8":"1 - Existing Customer 0 - Attrited Customer","96e76d17":"Lets get how many people of different genders are the credit card holders","75242442":"#### lets make a heatmap so as to get, whats the correlation b\/w every other columns. But as we have categorical columns as well, so we will use integer encoding so make them numerical and then make heatmap","124be644":"## Card_Category\nLets know what are the types of cards, and whats their composition\n\nTo get to know about card category, we are gonna use treemap","cdeb2ff3":"#### These are the categorical variables, we will now either do encoding or make dummy so as to make them all numerical so that we can plot out heatmap and proceed further"}}