{"cell_type":{"b3feab71":"code","d372797a":"code","7de6fc21":"code","a8302e40":"code","b36a011e":"code","20d3b2f2":"code","55210b57":"code","828ab15a":"code","0173a839":"code","41cd4cee":"code","fa8e340e":"code","1f24f903":"code","61bd15bb":"markdown","fe00abc5":"markdown","c6cffba0":"markdown","147a09c7":"markdown","052e6a4f":"markdown","29e520de":"markdown","7d1671b6":"markdown","7ffb33a4":"markdown","cd7ad955":"markdown","c51fc996":"markdown"},"source":{"b3feab71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d372797a":"data = pd.read_csv('\/kaggle\/input\/fake-news-dataset\/news.csv')","7de6fc21":"data.head()","a8302e40":"data=data.drop([\"Unnamed: 0\"],axis=1)","b36a011e":"data.head(10)","20d3b2f2":"import numpy as np\nimport random\nimport pprint\nimport pandas as pd\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.python.framework import ops\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\ntf.disable_eager_execution()","55210b57":"import json\nimport tensorflow as tf\nimport csv\nimport random\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\n\n\nembedding_dim = 50\nmax_length = 54\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size=3000\ntest_portion=.1","828ab15a":"le = preprocessing.LabelEncoder()\nle.fit(data['label'])\ndata['label']=le.transform(data['label'])","0173a839":"title=[]\ntext = []\nlabels=[]\n#random.shuffle(data)\nfor x in range(training_size):\n    title.append(data['title'][x])\n    text.append(data['text'][x])\n    labels.append(data['label'][x])\n\n\ntokenizer1 = Tokenizer()\ntokenizer1.fit_on_texts(title)\n#tokenizer2 = Tokenizer()\n#tokenizer2.fit_on_texts(text)\nword_index1 = tokenizer1.word_index\nvocab_size1=len(word_index1)\n#word_index2 = tokenizer2.word_index\n#vocab_size2=len(word_index2)\n\nsequences1 = tokenizer1.texts_to_sequences(title)\npadded1 = pad_sequences(sequences1,  padding=padding_type, truncating=trunc_type)\n#sequences2 = tokenizer2.texts_to_sequences(text)\n#padded2 = pad_sequences(sequences2,  padding=padding_type, truncating=trunc_type)\n\nsplit = int(test_portion * training_size)\n\ntest_sequences1 = padded1[0:split]\ntraining_sequences1 = padded1[split:training_size]\n#test_sequences2 = padded2[0:split]\n#training_sequences2 = padded2[split:training_size]\ntest_labels = labels[0:split]\ntraining_labels = labels[split:training_size]","41cd4cee":"# Note this is the 100 dimension version of GloVe from Stanford\n# I unzipped and hosted it on my site to make this notebook easier\nembeddings_index = {};\nwith open('\/kaggle\/input\/glove50d\/glove.6B.50d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size1+1, embedding_dim));\nfor word, i in word_index1.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","fa8e340e":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size1+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\n\ntraining_padded = np.array(training_sequences1)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(test_sequences1)\ntesting_labels = np.array(test_labels)\n\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)\n\nprint(\"Training Complete\")\n","1f24f903":"X=\"Karry to go to France in gesture of sympathy \"\n\nsequences = tokenizer1.texts_to_sequences([X])[0]\nsequences = pad_sequences([sequences],maxlen=54 , padding=padding_type, truncating=trunc_type )\nif(model.predict(sequences,verbose=0)[0][0] >= 0.5 ):\n    print(\"This news is True\")\nelse:\n    print(\"This news is false\")\n","61bd15bb":"# *Importing important libraries*","fe00abc5":"**Encoding labels**","c6cffba0":"# Loading the fake news dataset.","147a09c7":"**Making some variables**","052e6a4f":"# Tokenizing","29e520de":"<h1> Creating Model <\/h1>","7d1671b6":"using columns separately for temporal basis as a pipeline just for good accuracy.","7ffb33a4":"**Taking a look in our dataset**","cd7ad955":"# Cleansing the dataset","c51fc996":"<h1> word Embedding <\/h1>"}}