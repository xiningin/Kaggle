{"cell_type":{"701f699e":"code","f891cb5f":"code","a9268222":"code","c73453eb":"code","0a0f390a":"code","eecb8c3b":"code","0c6059e3":"code","8956bbcd":"code","c15e94a4":"code","5512fb73":"code","6ee317ee":"code","703b7e3b":"code","53b3abc6":"code","09c97eca":"code","fd468228":"code","f05dd4c1":"code","24ba0e49":"code","651b13e5":"code","3235e471":"code","ee4a2897":"code","2c1771d9":"code","2d1909a9":"code","35c940f9":"code","868cfad9":"code","e23b0e02":"code","57ac8705":"code","d9753021":"code","39603c65":"code","d32a57ff":"code","562cf469":"code","cf900d28":"code","1665d67b":"code","02103552":"code","f995c387":"code","488210b1":"code","2668e96d":"code","6d79bff9":"code","d21850fe":"code","ae46d030":"code","e842f188":"code","6d70bc10":"code","50b8523f":"code","e17c7d2a":"code","28e84f8b":"code","7dc4afdc":"code","05ad4014":"code","21b1673b":"code","e59d1be0":"code","481d12d8":"code","a21c40be":"code","9ff47f54":"code","a0c79d39":"code","917b7fb5":"code","d3eaa165":"code","c923980c":"code","3f7f10f1":"code","9a6e50ba":"code","9de58132":"code","856324e3":"code","6bc501e6":"code","bcc26b56":"code","127c3da2":"code","f6226c85":"code","85b23c48":"code","4d36382e":"markdown","2a7ddf0e":"markdown","696abba3":"markdown","ae83111e":"markdown","28cbff2c":"markdown"},"source":{"701f699e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    \n    \n    \n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f891cb5f":"!pip install ..\/input\/pyphen\/Pyphen-0.10.0-py3-none-any.whl","a9268222":"!pip install ..\/input\/textstat\/textstat-0.7.1-py3-none-any.whl","c73453eb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nimport textstat\nfrom textstat.textstat import textstatistics\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0a0f390a":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import TruncatedSVD\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV","eecb8c3b":"train_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","0c6059e3":"train_df.head()","8956bbcd":"train_df.shape","c15e94a4":"train_df.info()","5512fb73":"train_df.isnull().sum()","6ee317ee":"test_df.head()","703b7e3b":"test_df.shape","53b3abc6":"test_df.info()","09c97eca":"test_df.isnull().sum()","fd468228":"train_df = train_df.drop_duplicates()\ntrain_df.shape","f05dd4c1":"test_df = test_df.drop_duplicates()\ntest_df.shape","24ba0e49":"fig = plt.figure(figsize=(10,15))\nplt.subplot(2,1,1)\nsns.distplot(train_df['target'])\nplt.subplot(2,1,2)\nsns.distplot(train_df['standard_error'])","651b13e5":"#dropping columns url_legal and license\ntrain_df = train_df.drop(columns=['url_legal','license'],axis=1)","3235e471":"summary_df = train_df","ee4a2897":"summary_df.head()","2c1771d9":"no_sentence = []\ntotal_word = 0\naverage_word = 0\nno_words = []\naverage_word_sen = []\nfor passage in summary_df['excerpt']:\n    count_sen = len(re.findall(r'\\.', passage))\n    no_sentence.append(count_sen)\n    for sen in passage:\n        count_word = len(re.findall(r'\\w+', sen))\n        total_word += count_word\n    average_word = total_word\/count_sen\n    no_words.append(total_word)\n    average_word_sen.append(average_word)\nsummary_df['no_sen'] = no_sentence\nsummary_df['no_word'] = no_words\nsummary_df['average_word_sen'] = average_word_sen","2d1909a9":"count_letter = 0\ntotal_letter = []\nall_word = []\ncount_unique_word = []\nfor passage in summary_df['excerpt']:\n    for word in re.findall(r'\\w+', passage):\n        all_word.append(word)\n        count_letter += len(word)\n    all_word = list(set(all_word))\n    count_unique_word.append(len(all_word))\n    total_letter.append(count_letter)\nsummary_df['letter_count'] = count_letter\nsummary_df['count_unique_word'] = count_unique_word","35c940f9":"count = 0\nunique_words = []\ncount_non_stop = []\nstop_words = list(set(stopwords.words('english')))\nfor passage in summary_df['excerpt']:\n    words = word_tokenize(passage)\n    for w in words:\n        if w not in stop_words and w not in unique_words:\n            count += 1\n            unique_words.append(w)\n    count_non_stop.append(count)\nsummary_df['count_non_stop'] = count_non_stop","868cfad9":"passage_length = []\nmax_pass_len = []\nmin_pass_len = []\nsent_passage = []\nfor passage in summary_df['excerpt']:\n    passage_length.append(len(passage))\n    sent_length = sent_tokenize(passage)\n    sent_passage.append(sent_length)\n    max_pass_len.append(len(max(sent_length)))\n    min_pass_len.append(len(min(sent_length)))\nsummary_df['passage_length'] = passage_length\nsummary_df['max_pass_len'] = max_pass_len\nsummary_df['min_pass_len'] = min_pass_len","e23b0e02":"summary_df.info()","57ac8705":"summary_df.head()","d9753021":"dale_chall_list = []\nfor text in summary_df['excerpt']:\n    dale_chall = textstat.dale_chall_readability_score(text)\n    dale_chall_list.append(dale_chall)\nprint(len(dale_chall_list))","39603c65":"coleman_liau_list = []\nfor text in summary_df['excerpt']:\n    coleman_liau = textstat.coleman_liau_index(text)\n    coleman_liau_list.append(coleman_liau)\nprint(len(coleman_liau_list))","d32a57ff":"smog_index_list = []\nfor text in summary_df['excerpt']:\n    smog_index = textstat.smog_index(text)\n    smog_index_list.append(smog_index)\nprint(len(smog_index_list))","562cf469":"automated_readability_index_list = []\nfor text in summary_df['excerpt']:\n    automated_readability_index = textstat.automated_readability_index(text)\n    automated_readability_index_list.append(automated_readability_index)\nprint(len(automated_readability_index_list))","cf900d28":"linsear_write_formula_list = []\nfor text in summary_df['excerpt']:\n    linsear_write_formula = textstat.linsear_write_formula(text)\n    linsear_write_formula_list.append(linsear_write_formula)\nprint(len(linsear_write_formula_list))","1665d67b":"gunning_fog_list = []\nfor text in summary_df['excerpt']:\n    gunning_fog = textstat.gunning_fog(text)\n    gunning_fog_list.append(gunning_fog)\nprint(len(gunning_fog_list))","02103552":"flesch_reading_ease_list = []\nfor text in summary_df['excerpt']:\n    flesch_reading_ease = textstat.flesch_reading_ease(text)\n    flesch_reading_ease_list.append(flesch_reading_ease)\nprint(len(flesch_reading_ease_list))","f995c387":"flesch_kincaid_grade_list = []\nfor text in summary_df['excerpt']:\n    flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n    flesch_kincaid_grade_list.append(flesch_kincaid_grade)\nprint(len(flesch_kincaid_grade_list))","488210b1":"difficult_words_list = []\nfor text in summary_df['excerpt']:\n    difficult_words = textstat.difficult_words(text)\n    difficult_words_list.append(difficult_words)\nprint(len(difficult_words_list))","2668e96d":"summary_df['dale_chall_list'] = dale_chall_list\nsummary_df['coleman_liau_list'] = coleman_liau_list\nsummary_df['smog_index_list'] = smog_index_list\nsummary_df['automated_readability_index_list'] = automated_readability_index_list\nsummary_df['linsear_write_formula_list'] = linsear_write_formula_list\nsummary_df ['gunning_fog_list'] = gunning_fog_list\nsummary_df['flesch_reading_ease_list'] = flesch_reading_ease_list\nsummary_df['flesch_kincaid_grade_list'] = flesch_kincaid_grade_list\nsummary_df['flesch_kincaid_grade_list'] = flesch_kincaid_grade_list","6d79bff9":"summary_df.info()","d21850fe":"fig = plt.figure(figsize=(20,20))\nsns.heatmap(summary_df.corr(), annot = True)","ae46d030":"summary_df = summary_df.drop(columns=['no_sen','no_word', 'dale_chall_list', 'smog_index_list', 'count_non_stop', 'letter_count', 'flesch_reading_ease_list', 'automated_readability_index_list', 'gunning_fog_list', 'flesch_kincaid_grade_list'])","e842f188":"fig = plt.figure(figsize=(20,20))\nsns.heatmap(summary_df.corr(), annot = True)","6d70bc10":"def tokenize_def(text):\n    text = re.sub(r'[^\\w\\s]','',text)\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n\n    clean_tokens = []\n    for tok in tokens:\n        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n        clean_tokens.append(clean_tok)\n\n    return clean_tokens","50b8523f":"#kf = KFold(n_splits=10, random_state= 42, shuffle= True)\nvect = CountVectorizer(tokenizer=tokenize_def)\ntfidf = TfidfTransformer()\n#clf = RandomForestRegressor()\nlgb = LGBMRegressor(max_depth = 2, num_leaves = 50, learning_rate = 0.05)\n#xgb = XGBRegressor()\nsvd = TruncatedSVD(n_components=10, n_iter=10, random_state=42)\n#lasso = Lasso()\n#ridge = Ridge()","e17c7d2a":"summary_df_vect = vect.fit_transform(summary_df['excerpt'])","28e84f8b":"summary_df_tfidf = tfidf.fit_transform(summary_df_vect)","7dc4afdc":"summary_df_svd = svd.fit_transform(summary_df_tfidf)","05ad4014":"summary_df_svd = pd.DataFrame(summary_df_svd)","21b1673b":"summary_df_svd.head()","e59d1be0":"sample = pd.merge(summary_df, summary_df_svd, left_index=True, right_index=True)","481d12d8":"sample.head(3)","a21c40be":"X = sample.drop(columns=['id', 'excerpt', 'target'], axis=1)\ny = sample['target']","9ff47f54":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","a0c79d39":"params = {'max_depth':[2],'num_leaves':[50],'learning_rate':[0.05]}\nmodel = GridSearchCV(lgb, params)","917b7fb5":"model.fit(X_train, y_train)","d3eaa165":"model.score(X_train, y_train)","c923980c":"model.score(X_test, y_test)","3f7f10f1":"pred = model.predict(X_test)","9a6e50ba":"mean_squared_error(y_test, pred, squared=False)","9de58132":"test_df = test_df.drop(columns=['url_legal','license'],axis=1)\n\nsummary_df = test_df\n\nno_sentence = []\ntotal_word = 0\naverage_word = 0\nno_words = []\naverage_word_sen = []\ncount_letter = 0\ntotal_letter = []\nall_word = []\ncount_unique_word = []\ncount = 0\nunique_words = []\ncount_non_stop = []\npassage_length = []\nmax_pass_len = []\nmin_pass_len = []\nsent_passage = []\n\nfor passage in summary_df['excerpt']:\n    count_sen = len(re.findall(r'\\.', passage))\n    no_sentence.append(count_sen)\n    for sen in passage:\n        count_word = len(re.findall(r'\\w+', sen))\n        total_word += count_word\n    average_word = total_word\/count_sen\n    no_words.append(total_word)\n    average_word_sen.append(average_word)\nsummary_df['no_sen'] = no_sentence\nsummary_df['no_word'] = no_words\nsummary_df['average_word_sen'] = average_word_sen\n\n\nfor passage in summary_df['excerpt']:\n    for word in re.findall(r'\\w+', passage):\n        all_word.append(word)\n        count_letter += len(word)\n    all_word = list(set(all_word))\n    count_unique_word.append(len(all_word))\n    total_letter.append(count_letter)\nsummary_df['letter_count'] = count_letter\nsummary_df['count_unique_word'] = count_unique_word\n\nstop_words = list(set(stopwords.words('english')))\nfor passage in summary_df['excerpt']:\n    words = word_tokenize(passage)\n    for w in words:\n        if w not in stop_words and w not in unique_words:\n            count += 1\n            unique_words.append(w)\n    count_non_stop.append(count)\nsummary_df['count_non_stop'] = count_non_stop\n\n\nfor passage in summary_df['excerpt']:\n    passage_length.append(len(passage))\n    sent_length = sent_tokenize(passage)\n    sent_passage.append(sent_length)\n    max_pass_len.append(len(max(sent_length)))\n    min_pass_len.append(len(min(sent_length)))\nsummary_df['passage_length'] = passage_length\nsummary_df['max_pass_len'] = max_pass_len\nsummary_df['min_pass_len'] = min_pass_len\n\ncoleman_liau_list = []\nfor text in summary_df['excerpt']:\n    coleman_liau = textstat.coleman_liau_index(text)\n    coleman_liau_list.append(coleman_liau)\n    \nlinsear_write_formula_list = []\nfor text in summary_df['excerpt']:\n    linsear_write_formula = textstat.linsear_write_formula(text)\n    linsear_write_formula_list.append(linsear_write_formula)\n    \nsummary_df['coleman_liau_list'] = coleman_liau_list\nsummary_df['linsear_write_formula_list'] = linsear_write_formula_list\n\nsummary_df_vect = vect.fit_transform(summary_df['excerpt'])\nsummary_df_tfidf = tfidf.fit_transform(summary_df_vect)\nsummary_df_svd = svd.fit_transform(summary_df_tfidf)\nsummary_df_svd = pd.DataFrame(summary_df_svd)\n\ntest_df = pd.merge(summary_df, summary_df_svd, left_index=True, right_index=True)\n\ndf_test = test_df.drop(columns=['id', 'excerpt'], axis=1)","856324e3":"pred = model.predict(df_test)","6bc501e6":"predictions = pd.DataFrame(pred, columns=['target'])\npredictions","bcc26b56":"submission = test_df[['id']]\nsubmission","127c3da2":"submission = pd.merge(submission,predictions, right_index=True, left_index=True)","f6226c85":"submission","85b23c48":"submission.to_csv(\".\/submission.csv\",index=False, float_format='%.1f')","4d36382e":"Adding few new columns into the train_df displaying summary count of words and sentence in the passage mentioned in excerpt.","2a7ddf0e":"The graph representing the target values seems normally distributed representing that rating done for the article where most of them are medium complex and other are either too easy or too hard to be classified correctly.","696abba3":"Graph representing standard_error in the dataset looks to be at 0.5 error value which means that most of the passage which the user is not to understand are misjudge to be of medium complexibility by them.","ae83111e":"# Dale Chall Readability Score","28cbff2c":"# Model Building"}}