{"cell_type":{"8cba5d6c":"code","5b29d326":"code","cb7ce1c3":"code","de145600":"code","7a62a717":"code","a96a6bfa":"code","1568608c":"code","dc649b4d":"code","499e43ea":"code","377e8be0":"code","6f208015":"code","7cf97768":"code","8b168595":"code","2b47fed6":"code","3ffa4c90":"code","443bf709":"code","c20c5fc2":"code","ad72b2ee":"code","dabf62bd":"code","9bdfc222":"code","4504c3b6":"code","8278b31d":"code","2414c574":"code","1e75528c":"code","6f97de0b":"code","9916e54f":"code","56a336fe":"code","eb94559d":"code","df8d94a9":"code","b73ee6f3":"code","320f8e24":"code","52c59e06":"code","ac93f395":"code","c6ec62ce":"code","19ad6f5f":"code","e437edba":"code","abe6030d":"code","70469a29":"code","7188b55f":"code","59ce5ad3":"code","4e91f139":"code","6549a90b":"code","f4c97df7":"markdown","a26265b4":"markdown","97f68167":"markdown","e420e632":"markdown","e847f345":"markdown","934bf4a6":"markdown","961bfa42":"markdown","c1c29662":"markdown","9e40e464":"markdown","90ec58ac":"markdown","f9d731e3":"markdown","ab926521":"markdown","251f6a42":"markdown","52623cd5":"markdown","a33a25f5":"markdown","152d6226":"markdown","4ad0c00f":"markdown","be9ece51":"markdown","55dd2292":"markdown","7f1b9278":"markdown","568b0df4":"markdown","191ec273":"markdown","0f7067ef":"markdown","8c318bbc":"markdown","c93ff8e2":"markdown","ead650ec":"markdown","d8ba4327":"markdown","8c382f2f":"markdown","d3689546":"markdown","76dedd2f":"markdown","856ed09e":"markdown","2e08e2cd":"markdown","681d6aa3":"markdown","394422b3":"markdown","442a83cd":"markdown","5e8c96ab":"markdown","902ea091":"markdown","5970baad":"markdown","04db44a1":"markdown","32af8e3a":"markdown","b72f7786":"markdown","fb1cac8a":"markdown","850db7e2":"markdown","20378005":"markdown"},"source":{"8cba5d6c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom scipy.stats import norm\nimport scipy.stats as st\n\n!pip install sklearn-contrib-py-earth\nfrom pyearth import Earth\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5b29d326":"train_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ndisplay(train_df.head())\ntrain_df.describe()\nprint(train_df.columns)","cb7ce1c3":"cont_FEATURES = ['cont%d' % (i) for i in range(0, 14)]\ncat_FEATURES = ['cat%d' % (i) for i in range(0, 10)]","de145600":"train_df.info()","7a62a717":"def plot_outliers(df, feature, threshold=3):\n    mean, std = np.mean(df), np.std(df)\n    z_score = np.abs((df-mean) \/ std)\n    good = z_score < threshold\n\n    print(f\"Rejection {(~good).sum()} points\")\n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good\n    \ndef plot_lof_outliers(df, feature):\n    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.001, p=1)\n    good = lof.fit_predict(df) > 0.5 # change this value to set the threshold for outliers\n    print(f\"Rejection {(~good).sum()} points\")\n    \n    visual_scatter = np.random.normal(size=df.size)\n    plt.scatter(df[good], visual_scatter[good], s=2, label=\"Good\", color=\"#4CAF50\")\n    plt.scatter(df[~good], visual_scatter[~good], s=8, label=\"Bad\", color=\"#F44336\")\n    plt.legend(loc='upper right')\n    plt.title(feature)\n    plt.show();\n    \n    return good","a96a6bfa":"good = plot_outliers(train_df['target'], 'target', threshold=4)","1568608c":"train_df = train_df[good]\nprint('Now train_df has %d rows.' % (train_df.shape[0]))","dc649b4d":"good = plot_lof_outliers(train_df['target'].values.reshape(train_df['target'].shape[0], -1), 'target')","499e43ea":"train_df = train_df[good]\nprint('Now train_df has %d rows.' % (train_df.shape[0]))","377e8be0":"for feature in cont_FEATURES:\n    plot_outliers(train_df[feature], feature)","6f208015":"for feature in cont_FEATURES:\n    # There some reshaping done here for syntax sake\n    plot_lof_outliers(train_df[feature].values.reshape(train_df[feature].shape[0], -1), feature)","7cf97768":"len(set(list(train_df['id'].values)))","8b168595":"for feature in cont_FEATURES:\n    sns.violinplot(x=train_df[feature], inner='quartile', bw=0.1)\n    plt.title(feature)\n    plt.show();","2b47fed6":"for cat in cat_FEATURES:\n    values = train_df.groupby(cat)['id'].count().reset_index()\n    sns.barplot(x=cat, y='id', data=values)\n    plt.title(cat)\n    plt.show();","3ffa4c90":"for feature in cat_FEATURES:\n    sns.violinplot(x=feature, y='target', data=train_df, inner='quartile');\n    plt.title(feature)\n    plt.show()","443bf709":"number_of_rows = train_df.shape[0]\nfor feature in cat_FEATURES:\n    percentage_common_category = train_df.groupby(feature)['id'].count().reset_index()\n    print(feature)\n    print(percentage_common_category['id'].max() \/ number_of_rows)","c20c5fc2":"def plot_cdf(df, feature):\n    ps = 100 * st.norm.cdf(np.linspace(-4, 4, 10)) # The last number in this tuple is the number of percentiles\n    x_p = np.percentile(df, ps)\n\n    xs = np.sort(df)\n    ys = np.linspace(0, 1, len(df))\n\n    plt.plot(xs, ys * 100, label=\"ECDF\")\n    plt.plot(x_p, ps, label=\"Percentiles\", marker=\".\", ms=10)\n    plt.legend()\n    plt.ylabel(\"Percentile\")\n    plt.title(feature)\n    plt.show();\n\nfor feature in cont_FEATURES:\n    plot_cdf(train_df[feature], feature)","ad72b2ee":"# This plots a 16x16 matrix of correlations between all the features and the target\n# Note: I sometimes comment this out because it takes a few minutes to run and doesn't show any useful information.\n\n# pd.plotting.scatter_matrix(train_df, figsize=(10, 10));","dabf62bd":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(train_df.drop(columns=['id']).corr(), annot=True, cmap='viridis', fmt='0.2f', ax=ax)","9bdfc222":"sns.violinplot(x=train_df['target'], inner='quartile', bw=0.1)\nplt.title('target')\nplt.show();","4504c3b6":"for feature in cont_FEATURES:\n    #sns.kdeplot(x=train_df['target'], y=train_df[feature], bins=20, cmap='magma', shade=True) \n    plt.hist2d(x=train_df['target'], y=train_df[feature], bins=20)\n    plt.xlabel(feature)\n    plt.ylabel('target')\n    plt.title(feature)\n    plt.show()","8278b31d":"# Remove cat4 and cat6 since over 95% of instances have the same value\ntrain_df = train_df.drop(columns=['cat4', 'cat6'])\n\ncat_FEATURES.remove('cat4')\ncat_FEATURES.remove('cat6')\n\nprint(cat_FEATURES)","2414c574":"# Converting to Ordinal Variables\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\n\nordinal_cat_FEATURES = ordinal_encoder.fit_transform(train_df[cat_FEATURES])\nordinal_cat_FEATURES","1e75528c":"kf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = ordinal_cat_FEATURES[train_index, :]\n    test_X = ordinal_cat_FEATURES[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Mean Score: \", np.mean(scores))","6f97de0b":"# Converting to One-Hot Encoded Variables\nfrom sklearn.preprocessing import OneHotEncoder\nonehot_encoder = OneHotEncoder(handle_unknown='ignore') # Ignore categories that we don't see in training\nonehot_cat_FEATURES = onehot_encoder.fit_transform(train_df[cat_FEATURES])\nonehot_cat_FEATURES\nprint(onehot_encoder.get_feature_names())","9916e54f":"kf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = onehot_cat_FEATURES[train_index, :]\n    test_X = onehot_cat_FEATURES[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Mean Score: \", np.mean(scores))","56a336fe":"from sklearn.decomposition import PCA\npca_FEATURES = ['cont1', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12']\nnon_pca_FEATURES = [feature for feature in cont_FEATURES if feature not in pca_FEATURES]\n# For sake of argument we will reduce the number of variables by 50%\npca = PCA(n_components=5)\n\npca.fit(train_df[pca_FEATURES])","eb94559d":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = train_df[pca_FEATURES].iloc[train_index, :]\n    test_X = train_df[pca_FEATURES].iloc[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Mean Score: \", np.mean(scores))","df8d94a9":"# Test the features after applying PCA\nkf = KFold(n_splits=5)\n\nscores = []\npca_X = pca.transform(train_df[pca_FEATURES])\nfor train_index, test_index in kf.split(pca_X):\n    \n    train_X = pca_X[train_index, :]\n    test_X = pca_X[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Mean Score: \", np.mean(scores))","b73ee6f3":"for feature in non_pca_FEATURES:\n    plt.scatter(train_df[feature], train_df['target'], s=2)\n    plt.title(feature)\n    plt.show()","320f8e24":"# Create the square and cube of the features\nsq_features = []\ncb_features = []\nlog_features = []\nexp_features = []\n\nfor feature in non_pca_FEATURES:\n    sq_feature = feature + '_2'\n    cb_feature = feature + '_3'\n    log_feature = feature + '_log'\n    exp_feature = feature + '_exp'\n    \n    sq_features = sq_features + [sq_feature]\n    cb_features = cb_features + [cb_feature]\n    log_features = log_features + [log_feature]\n    exp_features = exp_features + [exp_feature]\n    \n    train_df[sq_feature] = train_df[feature]**2\n    train_df[cb_feature] = train_df[feature]**3\n    train_df[log_feature] = np.log10(train_df[feature])\n    train_df[exp_feature] = np.exp(train_df[feature])\n","52c59e06":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = train_df[non_pca_FEATURES].iloc[train_index, :]\n    test_X = train_df[non_pca_FEATURES].iloc[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Continuous Variables\")\nprint(\"Mean Score: \", np.mean(scores))","ac93f395":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = train_df[sq_features].iloc[train_index, :]\n    test_X = train_df[sq_features].iloc[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Squared Variables\")\nprint(\"Mean Score: \", np.mean(scores))","c6ec62ce":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = train_df[cb_features].iloc[train_index, :]\n    test_X = train_df[cb_features].iloc[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Cubed Variables\")\nprint(\"Mean Score: \", np.mean(scores))","19ad6f5f":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = train_df[log_features].iloc[train_index, :]\n    test_X = train_df[log_features].iloc[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Log Variables\")\nprint(\"Mean Score: \", np.mean(scores))","e437edba":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = train_df[exp_features].iloc[train_index, :]\n    test_X = train_df[exp_features].iloc[test_index, :]\n    \n    train_target = train_df['target'].iloc[train_index]\n    test_target = train_df['target'].iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"Exponential Variables\")\nprint(\"Mean Score: \", np.mean(scores))","abe6030d":"# Construct our features \n\nfeatures =  sq_features # + cb_features +  log_features + exp_features + non_pca_FEATURES\nX = train_df[features] \n\npca_features = ['pca_' + str(i) for i in range(0, pca_X.shape[1])]\nX[pca_features] = pd.DataFrame(pca_X, index=X.index)\n\nordinal_features = ['ordinal_' + str(i) for i in range(0, ordinal_cat_FEATURES.shape[1])]\nX[ordinal_features] = pd.DataFrame(ordinal_cat_FEATURES, index=X.index)\n\ny = train_df['target']","70469a29":"X['target'] = y\nX.to_csv('preprocessed_train.csv', index=False)\n","7188b55f":"# Preprocess the Test Set\nsq_features = []\nfor feature in non_pca_FEATURES:\n    sq_feature = feature + '_2'\n    sq_features = sq_features + [sq_feature]\n\n    test_df[sq_feature] = test_df[feature]**2\n\ntest_pca_X = pca.transform(test_df[pca_FEATURES])\ntest_ordinal_cats = ordinal_encoder.transform(test_df[cat_FEATURES])\n\nfeatures =  sq_features # + cb_features +  log_features + exp_features + non_pca_FEATURES\ntest_X = test_df[features] \n\npca_features = ['pca_' + str(i) for i in range(0, test_pca_X.shape[1])]\ntest_X[pca_features] = pd.DataFrame(test_pca_X, index=test_X.index)\n\nordinal_features = ['ordinal_' + str(i) for i in range(0, ordinal_cat_FEATURES.shape[1])]\ntest_X[ordinal_features] = pd.DataFrame(test_ordinal_cats, index=test_X.index)\n\ntest_X.to_csv('preprocessed_test.csv', index=False)","59ce5ad3":"# Test the non-PCA features\nkf = KFold(n_splits=5)\n\nscores = []\nfor train_index, test_index in kf.split(train_df):\n    \n    train_X = X.iloc[train_index, :]\n    test_X = X.iloc[test_index, :]\n    \n    train_target = y.iloc[train_index]\n    test_target = y.iloc[test_index]\n    \n    \n    model = LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n    model.fit(train_X, train_target, eval_set=[(test_X, test_target)], verbose=False)\n    preds = model.predict(test_X)\n    score = mean_squared_error(test_target, preds)\n    \n    scores.append(score)\n    print(\"Score:\", score)\n\nprint(\"LGBM Regressor\")\nprint(\"Mean Score: \", np.mean(scores))","4e91f139":"model = Earth(allow_missing=True)\nmodel.fit(X, y)","6549a90b":"preds = model.predict(X)\nprint(\"Mean Square Error: \", mean_squared_error(y, preds))","f4c97df7":"### Target Outliers","a26265b4":"So after the above results we will run our baselines with:\n* PCA to replace some of our continuous variables\n* The remaining continious variables will be add with all the above transformations\n* The categorical variables will be added with ordinal encoding","97f68167":"# Analysing Distributions\n\nHere we will look at correlations between the features, distributions of the features.\n\nFirst let's check that each row has it's own unique id.","e420e632":"We will now run these two models against our XGBoostRegressor to get a quick sense of it's performance.","e847f345":"We can see that we have 14 continuous variables and 10 categorical variables.","934bf4a6":"# Analyse the Target","961bfa42":"We will now look at the **LOF (Local Outlier Factor)** outliers.","c1c29662":"We can see that the above graph is far too busy to show us any useful information. However, at least we know that there isn't any clear correlations between a particular variable and the target.","9e40e464":"### Categorical Variables\n\nFirst let's look at what values the categorical variables can take.","90ec58ac":"# Empirical CDFs\n\nThe below graphs show us where the 10th\/20th\/....\/90th percentiles lie for each of the features.","f9d731e3":"The above is harder to read as it has picked some points inside grouping. However, since there are only 300 points and I trust the LOF measurement, I am going to remove these points from dataset as well.","ab926521":"####\u00a0Ordinal Variables","251f6a42":"Above we can see a cluster of features (cont1, cont5-cont12) that appear to be quite highly correlated together. This suggests that dimensionality reduction techniques could be used to reduce these features to a smaller set.","52623cd5":"## Categorical Variables ","a33a25f5":"**Note**: Here I would also like to apply winsorising, bucketting for one or two variables where required. This will be in a future version of the notebook so watch this space. ","152d6226":"#### LGMB Regressor","4ad0c00f":"Next we will look at the LOF outliers.","be9ece51":"#### Splines Mars\n\nThis model is great for finding polynomial patterns in continuous features.","55dd2292":"# Takeaways and Future Work\n\nTakeaways:\n* None of the features are \"silver bullets\" for making accurate predictions\n* Outliers exist in the dataset but aren't common\n* Dimensionality reduction performs well for a subset of the features\n* Transformations of the continuous variables has little affect on the score\n\nFuture Work:\n* I'm going to create a notebook where I dig deeper into these models and do some hyperparameter tuning to improve the performance. \n","7f1b9278":"#### Transformations","568b0df4":"Above we can see that these points are very reasonable outliers. There is a clear grouping for the target values however these points marked in red fall outside this grouping. I will therefore remove these 24 rejected points.","191ec273":"# Cleaning the Dataset\n\nFollowing the steps of the Machine Learning Checklist we will start by cleaning out invalid values and outliers from the dataset.","0f7067ef":"So we can see by introducing PCA our models performed significantly better.","8c318bbc":"## Continuous Variables","c93ff8e2":"From the above I think we should remove some categories from the dataset since they are so small they serve no purpose. \n\nFirst let's look at the percentage of the rows that are the most common category.","ead650ec":"# Baselines","d8ba4327":"# Baselines with EDA and Feature Engineering\n\nBelow is my exploratory analysis, feature engineering and them some baseline models. \n\nPlease let me know what you think in the comments and **upvote** if you find anything useful.\n\nThanks and enjoy!","8c382f2f":"# Load the Data\n\nHere we will load the data into a pandas dataframe.","d3689546":"# Correlation\n\nHere we can look at the correlation between the features and each other (and the target)","76dedd2f":"### Invalid Values","856ed09e":"This is perhaps the most revealing visualisations. It shows us that our features (especially '*cont1*' and '*cont5*') have unusual distributions. '*cont1*' appears to turn into an categorical variable when greater than 0.4 and '*cont4*' is a linear distribution once above 0.3. \n\nThis could suggest that these variables need to split into additional features or have functions applied to their values to create a bigger distinction between very similar values.","2e08e2cd":"Here we can see that there are no *non-null* values so there is nothing to remove here.\n\n### Outliers\n\n#### **Removing outliers is less of a science and more of an art form. So I will leave the choice up to you, but show you how to visualise these points.**\n\nFirst we will look at outliers for the *target*.\n\nWe will add noise to the one dimensional features in order to \"explode\" the points out, helping us see the distributions and potential outliers.\n\nWe will use two methods for finding outliers:\n* The first will consider a point to be an outlier if it is N standard deviations from the mean. N is defined as the threshold.\n* A more complex form of outlier detection is LOF (Local Outlier Factor) which uses a points 20 nearest neighbours to determine if it is in a low density region (and therefore potentially and outlier).","681d6aa3":"So above we can see that the majority of the features do not contain outliers, however features *cont5* and *cont12* do contain some points that are could be considered as outliers.","394422b3":"The takeaway from this is that the categorical variables are not a silver bullet for determining the target. The models will need to receive a combination of these variables in order to make accurate predictions.","442a83cd":"#### Categorical Variables","5e8c96ab":"### Continuous Variables","902ea091":"We can see from above that cat4 and cat6 are over 95% one class. And so they will be of minimal use to use and can be removed from the dataset.","5970baad":"We can see from the above that there are a small number of reasonable outliers selected here. I am therefore not going to remove any of these points as outliers.","04db44a1":"Finally we will look at the 2D histogram plots for each features vs. the target, this can be a clue of unusual correlations between the target and features. \n\n**Note:** There is also code for a KDE plot but these take a long time to run.","32af8e3a":"This doesn't show us much that is interesting other than the target is grouped around it's mean of 7.5, with some long tails out to either side.","b72f7786":"#### PCA","fb1cac8a":"The above shows us that each feature has a unique distribution which could likely be used to help our models make predictions.\n\nWe can also see that contain features contain points that are most likely outliers (looking at the tails\/heads), namely *cont3*, *cont4*, *cont5*, *cont6* and *cont12*.","850db7e2":"### Feature Outliers\n\nFirst we will look at the threshold outliers.","20378005":"# Feature Engineering"}}