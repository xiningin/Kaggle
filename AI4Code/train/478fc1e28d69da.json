{"cell_type":{"92dc4f43":"code","2cb353e0":"code","b29e6b79":"code","6aec733b":"code","d7c3e783":"code","62c6fb98":"code","0c574790":"code","5a402e24":"code","1a0368f9":"code","2fceb951":"code","a2a43165":"code","58c70eae":"code","23ff7372":"code","d25d66a9":"code","272006d9":"code","2c1906d4":"code","fca5ef66":"code","463d8403":"code","f10011cf":"code","a6e1ef74":"code","9d17b433":"code","2c449da3":"code","fab14b2f":"code","ea372762":"code","6f02bbd9":"code","08663c3e":"code","e99f9ffa":"code","68789f64":"code","3555dc37":"markdown"},"source":{"92dc4f43":"PATH = '..\/input\/lish-moa\/'","2cb353e0":"#!pip install tensorflow-addons","b29e6b79":"import gc\nimport random\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n#ptimizer=tfa.optimizers.AdamW(lr = 2e-3, weight_decay = 1e-5, clipvalue = 700)\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.optimizers import SGD, Adam, RMSprop\n\nimport tensorflow_addons as tfa\n\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.merge import concatenate\n\n\nfrom sklearn.metrics import log_loss","6aec733b":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","d7c3e783":"def seed_everything(seed=999):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(49)","62c6fb98":"train_features = pd.read_csv(PATH+'train_features.csv')\ntest_features =pd.read_csv(PATH+'test_features.csv')\ntrain_targets_nonscored =pd.read_csv(PATH+'train_targets_nonscored.csv')\ntrain_targets_scored =pd.read_csv(PATH+'train_targets_scored.csv')\nsample_submission =pd.read_csv(PATH+'sample_submission.csv')","0c574790":"###################Select Feature#######################\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nID = ['sig_id']\nCP = ['cp_type','cp_time','cp_dose']\nG = [x for x in train_features.columns if 'g-' in x]\nC = [x for x in train_features.columns if 'c-' in x]\nX = train_features[C+G].values\n\nTargets = train_targets_scored.columns[1:].tolist()\nY = train_targets_scored[Targets].values[:,1:]\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X, Y)\nprint(model.feature_importances_)\n","5a402e24":"sel_feat= model.feature_importances_.argsort()[-200:][::-1]\nZ=train_features[C+G]\nname_sel_feat= Z.iloc[:,sel_feat].columns\n###################Select Feature#######################","1a0368f9":"#Let us check data types of train_features\ntrain_features.select_dtypes(include=['object','category','int','float']).dtypes","2fceb951":"#Which are the object datatypes\ntrain_features.select_dtypes(include=['object']).dtypes","a2a43165":"#This means we have have cp_type and cp_dose as categorical variables, also cp_time which has only three values 24, 48 and 72\ntr_features= [col for col in train_features.columns if col!='sig_id']\nprint(\"Length of train features list without 'sig_id' is:\", len(tr_features))","58c70eae":"# Now use label encoder to convert train_features and test_features df together\nfor train_feat in ['cp_type', 'cp_dose','cp_time']:\n    le = LabelEncoder()\n    le.fit(list(train_features[train_feat].astype(str).values) + list(test_features[train_feat].astype(str).values))\n    train_features[train_feat] = le.transform(list(train_features[train_feat].astype(str).values))\n    test_features[train_feat] = le.transform(list(test_features[train_feat].astype(str).values))","23ff7372":"#Let us check whether label encoding done\nprint(train_features.head(5))\n#Let us check whether label encoding done\nprint(test_features.head(5))","d25d66a9":"#Let us keep on train_feat columns for NN\n#\ntr_features = name_sel_feat\ntrain = train_features[tr_features]\ntest= test_features[tr_features]\ntrain_target = train_targets_scored.drop(['sig_id'], axis=1)\ntrain_target_aux = train_targets_nonscored.drop(['sig_id'], axis=1)","272006d9":"#So label encoding is done, let us move towards building NN\n#We are using StandardScalar to transform all features and feed a balanced input to the neural net\n\nsc = StandardScaler()\n#train = sc.fit_transform(train)\n#test = sc.transform(test)","2c1906d4":"len_X =int(train.shape[1])\nprint(len_X)","fca5ef66":"target_feat_len= train_target.shape[1]\ntarget_aux_len= train_target_aux.shape[1]\n#print(target_feat_len,train_target_aux)","463d8403":"## You can play around with network architecture and check what works\n\n#Using functional API, we will create a model with two outputs. First for scored columns and second for nonscored columns.\n#Nonscored output is forcing model to consider this targets and I \"hope\" that it will produce more better accuracy\n#Using batch normalization and high dropouts so that we prevent overfitting\n\n#9\/11 added tfa.layers.WeightNormalization\n\ndef getModel2(input_dim,target_feature_length,target_auxiliary_length):\n    visible = Input(shape=(input_dim,))\n    hidden1 = tfa.layers.WeightNormalization(Dense(875, activation='relu'))(visible)\n    batchnorm1= BatchNormalization()(hidden1)\n    dropout1= Dropout(0.5)(batchnorm1)\n    hidden2 = tfa.layers.WeightNormalization(Dense(1750, activation='relu'))(dropout1)\n    batchnorm2= BatchNormalization()(hidden2)\n    dropout2= Dropout(0.5)(batchnorm2)\n    hidden3 = tfa.layers.WeightNormalization(Dense(100, activation='relu'))(dropout2)\n    batchnorm3= BatchNormalization(name=\"batchnorm3\")(hidden3)\n    dropout3= Dropout(0.5,name=\"dropout3\")(batchnorm3)    \n    output_2 = tfa.layers.WeightNormalization(Dense(target_auxiliary_length, activation='sigmoid',name=\"ouput_2\"))(dropout3)\n    \n    hidden4 = tfa.layers.WeightNormalization(Dense(206, activation='relu',name=\"hidden4\"))(output_2)\n    batchnorm4= BatchNormalization(name=\"batchnorm4\")(hidden4)\n    dropout4= Dropout(0.5,name=\"dropout4\")(batchnorm4)\n    \n    concat1 = tf.keras.layers.Concatenate(name=\"concate_nonscored_feat\")([output_2, dropout4])\n    \n    output1 = tfa.layers.WeightNormalization(Dense(target_feature_length, activation='sigmoid', name=\"outputscore\"))(concat1)\n    output2 = tfa.layers.WeightNormalization(Dense(target_auxiliary_length, activation='sigmoid',name=\"outputnonscore\"))(concat1)\n    model = Model(inputs=visible, outputs=[output1,output2])\n    return model","f10011cf":"#Create a sample model and check summay\nmodelNN= getModel2(len_X,target_feat_len,target_aux_len)\nmodelNN.summary()\n\n","a6e1ef74":"def metric(y_true, y_pred):\n    #print('y_true:', y_true.head(3))\n    #print('y_pred:',y_pred.head(3))\n    metrics = []\n    for _target in train_target.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","9d17b433":"#Plot model and delete from memory\nplot_model(modelNN, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\ndel modelNN","2c449da3":"\"\"\"\nfrom sklearn.model_selection import KFold\nNFOLD = 10\nkf = KFold(n_splits=NFOLD)\n\nBATCH_SIZE=32\nEPOCHS=50\n\npout = np.zeros((test_features.shape[0], target_feat_len))\npaux = np.zeros((test_features.shape[0], target_aux_len))\n#Train is already a numpy as it went through StandardScalar\ntrain_features = train\n#train_targets needs to be converted to numpy\ntrain_targets = train_target.values\ntrain_target_aux_np=train_target_aux.values\n\npred = np.zeros((train_features.shape[0], target_feat_len))\n\ncnt=0\nfor tr_idx, val_idx in kf.split(train_features):\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=0.0001, mode='auto')\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    #net = getModel(len_X,target_feat_len)\n    net= getModel2(len_X,target_feat_len,target_aux_len)\n    #net.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    net.compile(optimizer = tfa.optimizers.AdamW(lr = 2e-3, weight_decay = 1e-5, clipvalue = 700), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    net.fit(train_features[tr_idx], [train_targets[tr_idx],train_target_aux_np[tr_idx]], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(train_features[val_idx], [train_targets[val_idx],train_target_aux_np[val_idx]]), verbose=0, callbacks=[early_stopping,reduce_lr_loss])\n    print(\"Training for this fold\", net.evaluate(train_features[tr_idx], [train_targets[tr_idx],train_target_aux_np[tr_idx]], verbose=0, batch_size=BATCH_SIZE))\n    print(\"Validation for this fold\", net.evaluate(train_features[val_idx], [train_targets[val_idx],train_target_aux_np[val_idx]], verbose=0, batch_size=BATCH_SIZE))\n    print(\"Predict on validation data for this fold\")\n    pred[val_idx] = net.predict(train_features[val_idx], batch_size=BATCH_SIZE, verbose=0)[0]\n    print(f'OOF Metric log_loss for this FOLD {cnt} : {metric(pd.DataFrame(train_targets[val_idx]), pd.DataFrame(pred[val_idx], columns=train_target.columns))}')\n    #print(f'OOF Metric log_loss for this FOLD {cnt} : {metric(train_targets[val_idx], pred[val_idx])}')\n    print(\"Predict test for nonscored targets\")\n    paux += net.predict(test, batch_size=BATCH_SIZE, verbose=0)[1] \/ NFOLD\n    print(\"Predict test with scored targets\")\n    pout += net.predict(test, batch_size=BATCH_SIZE, verbose=0)[0] \/ NFOLD\n    \n\"\"\"","fab14b2f":"from sklearn.model_selection import KFold\n#from sklearn.model_selection import MultilabelStratifiedKFold\nNFOLD = 10\n\nBATCH_SIZE=16\nEPOCHS=100\n\npout = np.zeros((test_features.shape[0], target_feat_len))\npaux = np.zeros((test_features.shape[0], target_aux_len))\nfoldmetric=0\n#Train is already a numpy as it went through StandardScalar\ntrain_features = train\n#train_targets needs to be converted to numpy\ntrain_targets = train_target\ntrain_target_aux_np=train_target_aux\n\npred = np.zeros((train_features.shape[0], target_feat_len))\n\ncnt=0\nkf = KFold(n_splits=NFOLD)\n#kf = MultilabelStratifiedKFold(n_splits=NFOLD,shuffle=True, random_state=49)\nfor tr_idx, val_idx in kf.split(train_features):\n#for tr_idx, val_idx in enumerate(kf.split(train_targets,train_targets)):\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=0.0001, mode='auto')\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    #net = getModel(len_X,target_feat_len)\n    net= getModel2(len_X,target_feat_len,target_aux_len)\n    #net.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    net.compile(optimizer = tfa.optimizers.AdamW(lr = 2e-3, weight_decay = 1e-5, clipvalue = 700), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    net.fit(train_features.iloc[tr_idx], [train_targets.iloc[tr_idx],train_target_aux_np.iloc[tr_idx]], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(train_features.iloc[val_idx], [train_targets.iloc[val_idx],train_target_aux_np.iloc[val_idx]]), verbose=0, callbacks=[early_stopping,reduce_lr_loss])\n    print(\"Training for this fold\", net.evaluate(train_features.iloc[tr_idx], [train_targets.iloc[tr_idx],train_target_aux_np.iloc[tr_idx]], verbose=0, batch_size=BATCH_SIZE))\n    print(\"Validation for this fold\", net.evaluate(train_features.iloc[val_idx], [train_targets.iloc[val_idx],train_target_aux_np.iloc[val_idx]], verbose=0, batch_size=BATCH_SIZE))\n    print(\"Predict on validation data for this fold\")\n    pred[val_idx] = net.predict(train_features.iloc[val_idx], batch_size=BATCH_SIZE, verbose=0)[0]\n    fold_metric = metric(train_targets.iloc[val_idx], pd.DataFrame(pred[val_idx],columns=train_target.columns))\n    print(f'OOF Metric log_loss for this FOLD {cnt} : {fold_metric}')\n    foldmetric += fold_metric\/ NFOLD\n    print(\"Average metric is:\",foldmetric)\n    #print(f'OOF Metric log_loss for this FOLD {cnt} : {metric(train_targets[val_idx], pred[val_idx])}')\n    print(\"Predict test for nonscored targets\")\n    paux += net.predict(test, batch_size=BATCH_SIZE, verbose=0)[1] \/ NFOLD\n    print(\"Predict test with scored targets\")\n    pout += net.predict(test, batch_size=BATCH_SIZE, verbose=0)[0] \/ NFOLD","ea372762":"submission = pd.DataFrame(data=pout, columns=train_target.columns)\nsubpaux = pd.DataFrame(data=paux, columns=train_target_aux.columns)","6f02bbd9":"submission.insert(0, column = 'sig_id', value=sample_submission['sig_id'])\nsubpaux.insert(0, column = 'sig_id', value=sample_submission['sig_id'])","08663c3e":"submission.loc[test_features['cp_type'] == 1, train_target.columns] = 0","e99f9ffa":"submission.to_csv('submission.csv', index=False)\nsubpaux.to_csv('subaux.csv', index=False)","68789f64":"print(submission.shape)\nprint(subpaux.shape)","3555dc37":"> Attribution: \nGreat kernel by https:\/\/www.kaggle.com\/stanleyjzheng\/baseline-nn-with-k-folds and other excellent kernels in this and previous competitions \n\nWhat has changed:\n9\/13\n\nsubmission.loc[test_features['cp_type'] == 1, train_target.columns] = 0\n______________________________________________________________________\n9\/12\n* Added WeightNormalization\n* Concat layer to feed nonscored output back, just before scored output. This should act as  feature extraction. We are going one step ahead from just adding the output for nonscored features.\n* AdamW optimizer \n* sklearn log_loss metric\n\n______________________________________________________________________\n9\/10\n\n* Using function API to build the model\n* Added an output for nonscored features, with belief that by doing this network will exhibit better prediction even with  scored output \n* StandardScalar \n"}}