{"cell_type":{"d04ce890":"code","6c52c6a9":"code","4bdcce83":"code","6f2f3e28":"code","d43c2965":"code","4f7fde05":"code","7abec8bc":"code","e6290119":"code","ed627bba":"code","b9b040e7":"code","bb63d47d":"code","dced47e6":"code","e2581ad4":"code","1615b09e":"code","95288be8":"code","6f158a68":"code","8737aa74":"code","ffe3712f":"code","174c2048":"code","65159ec1":"code","f94e7611":"code","6f4ab81a":"code","e85f7c96":"code","600da054":"code","df218e13":"code","51b20caa":"code","7122b43d":"code","d30d50f3":"code","05b1609d":"code","2d85c4f5":"code","7c330079":"code","01102841":"code","394193cc":"code","ba14e44b":"code","eb66d929":"code","c6432a46":"code","3cce0b8d":"code","df49fcd4":"code","b5f87513":"code","96435349":"code","9517cfcc":"code","20b57047":"code","1850a078":"code","5ffa9e38":"markdown","9b15ea5f":"markdown","b72e92a3":"markdown","474f8174":"markdown","40e99d14":"markdown","db17c9fc":"markdown","f6ccc8e0":"markdown","04156d87":"markdown","397086a7":"markdown","161891ec":"markdown","074aa098":"markdown","bb1341ad":"markdown","fb72a4e4":"markdown","ea9a33b9":"markdown","d7a215eb":"markdown","e0aaf55c":"markdown","60baf6da":"markdown","ce8fa6f8":"markdown","4373c0aa":"markdown","825ebc62":"markdown","2b7025e3":"markdown","9a9ce8b5":"markdown","0c7ae4c8":"markdown","a5631cda":"markdown","198eb7a6":"markdown","889205b3":"markdown","0cc2c435":"markdown","660fc96b":"markdown","739dc0a0":"markdown","38e3e1a5":"markdown","b1498020":"markdown","a0ef190d":"markdown","6a27eb93":"markdown","7eeff0f3":"markdown"},"source":{"d04ce890":"# Main Libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew, kurtosis\nimport math\nimport os\n\n# import operator for dictionary sorting operations\nimport operator\n\n# preprocessing imports\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\n\n# train-test split\nfrom sklearn.model_selection import train_test_split\n\n# linear regression models\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\n# cross val, scored, scaler\nfrom sklearn.model_selection import GridSearchCV,  cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import RobustScaler\n\n# boxcox normalisation\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# stacking\nfrom sklearn.base import BaseEstimator, RegressorMixin, clone\n\n# ignore warnings\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\nwarnings.filterwarnings(\"ignore\", category=Warning)\nprint('Warnings will be ignored!')\n\n# suppress scientific notation\npd.set_option('display.float_format', lambda x: '%.5f' % x)","6c52c6a9":"# create dataframes for exploration\ntrain_original = pd.read_csv('..\/input\/train.csv')\ntest_original = pd.read_csv('..\/input\/test.csv')","4bdcce83":"# create train and test datasets\ntrain = train_original.copy()\ntest = test_original.copy()","6f2f3e28":"# drop outliers suggested by dataset author\ntrain = train.drop(train[train[\"GrLivArea\"] > 4000].index, inplace=False)","d43c2965":"# combine dataframes for simplicity\nfull = pd.concat([train,test], ignore_index=True)","4f7fde05":"full.head().transpose()","7abec8bc":"full.describe().transpose()","e6290119":"full.info()","ed627bba":"print('Train Shape: ' + str(np.shape(train)) + '\\n' + 'Test Shape: ' + str(np.shape(test)) + '\\n' + 'Full Shape: ' + str(np.shape(full)))\n","b9b040e7":"categories = train.select_dtypes(include=['object']).columns\nnumericals = train.select_dtypes(include=['float64', 'int64']).columns\nprint('Num of Categories:  ' + str(len(categories)) + '\\n' + 'Num of Values:  ', str(len(numericals)))","bb63d47d":"# Function provides details of missing columns\ndef missing_data_report(dataset, train_or_test = \"Train\"):\n    missing_data = dataset.isnull().sum().sort_values(ascending=False)\n    missing_data_percent = ((dataset.isnull().sum() \/ dataset.isnull().count()) * 100).sort_values(ascending=False)\n    \n    missing_report = pd.concat([missing_data, missing_data_percent], axis=1, keys=['Total', 'Percentage'])\n    missing_report.rename_axis(train_or_test, inplace=True)\n    \n    return missing_report[missing_report > 0]\n\n# Function to customise table display\ndef multi_table(table_list):\n    from IPython.core.display import HTML\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' +\n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )","dced47e6":"# Dataframe of missing data\ntrain_missing, test_missing = missing_data_report(train, \"Train\"), missing_data_report(train, \"Test\")\n\n# Top Ten Missing data report\nmulti_table([train_missing.head(10), test_missing.head(10)])","e2581ad4":"# Check for NAs\n\ndef checknulls(df):\n    nullcols = df.isnull().sum().sort_values(ascending=False)\n    return nullcols[nullcols>0]\n\nchecknulls(full)","1615b09e":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat,vmax=.8, square=True)","95288be8":"sig_numerical = [\"SalePrice\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"1stFlrSF\", \"2ndFlrSF\", \"BedroomAbvGr\"]\ndis = train[sig_numerical].hist(bins=15, figsize=(15,6), layout=(2,4))\n","6f158a68":"sig_categorical = [\"MSZoning\", \"LotShape\", \"Neighborhood\", \"CentralAir\", \"SaleCondition\", \"MoSold\", \"YrSold\"]\nfig, ax = plt.subplots(2,4, figsize=(20,10))\n\n# Loop over every categorical variable to create countplot\nfor var, subplot in zip(sig_categorical, ax.flatten()):\n    sns.countplot(train[var], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)","8737aa74":"k = 10 #number of variables for heatmap\ncormat=train.corr()\ncols = cormat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","ffe3712f":"chartA = sns.pairplot(data=train,x_vars=['OverallQual', 'GrLivArea', 'ExterQual', 'KitchenQual',\n       'GarageCars'],y_vars=['SalePrice'])\n\nchartB = sns.pairplot(data=train,x_vars=['GarageArea', 'TotalBsmtSF', '1stFlrSF', 'BsmtQual'],y_vars=['SalePrice'])","174c2048":"sig_categorical = [\"MSZoning\", \"LotShape\", \"Neighborhood\", \"CentralAir\", \"SaleCondition\", \"MoSold\", \"YrSold\"]\nfig, ax = plt.subplots(2,4, figsize=(20,10))\n\n# Loop over every categorical variable to create countplot\nfor var, subplot in zip(sig_categorical, ax.flatten()):\n    sns.countplot(train[var], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)","65159ec1":"train['SalePrice'].describe()","f94e7611":"sns.distplot(train['SalePrice'])\nprint('Kurt: ' + str(kurtosis(train['SalePrice'])) + '\\n' + 'Skew: ' +str(skew(train['SalePrice'])))","6f4ab81a":"plt.figure(1); plt.title('Normal')\nsns.distplot(train['SalePrice'], kde=False, fit=stats.norm)\n\nplt.figure(2); plt.title('Log Norm')\nsns.distplot(train['SalePrice'], kde=False, fit=stats.lognorm)","e85f7c96":"# fix a few incorrect values; these values either had typos or had 'garage built' after house was sold\nfull['GarageYrBlt'][2588] = 2007\nfull['GarageYrBlt'][2545] = 2007\nfull['YearBuilt'][2545] = 2007","600da054":"def feature_transformations(df):\n\n    # drop nulls\n    df['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0)\n    df['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0)\n    df['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0)\n    df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(0)\n    df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\n    df['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)\n    df['GarageCars'] = df['GarageCars'].fillna(0)\n    df['GarageArea'] = df['GarageArea'].fillna(0) \n\n    # feature transformations\n    df['TotalLivingSF'] = df['GrLivArea'] + df['TotalBsmtSF'] - df['LowQualFinSF']\n    df['TotalMainBath'] = df['FullBath'] + (df['HalfBath'] * 0.5)\n    df['TotalBath'] = df['TotalMainBath'] + df['BsmtFullBath'] + (df['BsmtHalfBath'] * 0.5)\n    df['AgeSold'] = df['YrSold'] - df['YearBuilt']\n    df['TotalPorchSF'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF']\n    df['TotalSF'] = df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GarageArea']\n\n    # garage year built nulls and transformation\n    df['GarageYrBlt'] = df['GarageYrBlt'].replace(np.nan, 1900)\n    df['GarageAgeSold'] = df['YrSold'] - df['GarageYrBlt']\n\n    # other age\n    df['LastRemodelYrs'] = df['YrSold'] - df['YearRemodAdd']\n    df['LastRemodelYrs'] = df['LastRemodelYrs'].replace(-1, 0)\n    df['LastRemodelYrs'] = df['LastRemodelYrs'].replace(-2, 0)\n\n\n    return df\n  \nfull = feature_transformations(full)\ntrain = feature_transformations(train)","df218e13":"def mean_price_map(feature):\n  \n    le = LabelEncoder()    \n    feature = le.fit_transform(feature)  \n    mean_prices = train.groupby(feature, as_index=True)['SalePrice'].mean()\n    mean_price_length = len(mean_prices)\n    numbers = np.linspace(0, mean_price_length, (mean_price_length+1))\n    mean_price_dict = dict(zip(numbers, mean_prices))\n    return mean_price_dict\n    \ndef median_price_map(feature):\n\n    le = LabelEncoder()    \n    feature = le.fit_transform(feature)  \n    med_prices = train.groupby(feature, as_index=True)['SalePrice'].median()\n    med_price_length = len(med_prices)\n    numbers = np.linspace(0, med_price_length, (med_price_length+1))\n    med_price_dict = dict(zip(numbers, med_prices))\n    return med_price_dict\n\n\ndef mean_square_footage(feature):\n  \n    le = LabelEncoder()    \n    feature = le.fit_transform(feature)  \n    mean_sqft = train.groupby(feature, as_index=True)['TotalLivingSF'].mean()\n    mean_sqft_length = len(mean_sqft)\n    numbers = np.linspace(0, mean_sqft_length, (mean_sqft_length+1))\n    mean_sqft_dict = dict(zip(numbers, mean_sqft))\n    return mean_sqft_dict","51b20caa":"def df_transform(df):\n  \n    # neighborhood\n    le = LabelEncoder()\n    df['Neighborhood'] = le.fit_transform(df['Neighborhood'])\n    df['NhoodMedianPrice'] = df['Neighborhood'].map(median_price_map(train['Neighborhood']))\n    df['NhoodMeanPrice'] = df['Neighborhood'].map(mean_price_map(train['Neighborhood']))\n    df['NhoodMeanSF'] = df['Neighborhood'].map(mean_square_footage(train['Neighborhood']))\n    df['NhoodMeanPricePerSF'] = df['NhoodMeanPrice'] \/ df['NhoodMeanSF']\n    df['ProxyPrice'] = df['NhoodMeanPricePerSF'] * df['TotalSF']\n\n    # fixer upper score\n    df['FxUp_SaleCond'] = df.SaleCondition.map({'Partial': 0, 'Normal': 0, 'Alloca': 0, 'Family': 0, \n                                              'Abnorml': 3, 'AdjLand': 0, np.nan: 0}).astype(int)\n    df['FxUp_Foundation'] = df.Foundation.map({'PConc':0, 'Wood':0, 'Stone':0, 'CBlock':1, 'BrkTil': 0, 'Slab': 2, np.nan: 0}).astype(int)\n    df['FxUp_HeatingQC'] = df.HeatingQC.map({'Ex':0, 'Gd':0, 'TA':0, 'Fa':2, 'Po': 5, np.nan: 0}).astype(int)\n    df['FxUp_Heating'] = df.Heating.map({'GasA':0, 'GasW':0, 'OthW':2, 'Wall':3, 'Grav': 4, 'Floor': 4, np.nan: 0}).astype(int)\n    df['FxUp_CentralAir'] = df.CentralAir.map({'Y':0, 'N':6, np.nan: 0}).astype(int)\n    df['FxUp_GarageQual'] = df.GarageQual.map({'Ex':0, 'Gd':0, 'TA':0, 'Fa':1, 'Po': 3, np.nan: 0}).astype(int)\n    df['FxUp_PavedDrive'] = df['PavedDrive'].map({'Y':0, 'P':0, 'N':2, np.nan: 0}).astype(int)\n    df['FxUp_Electrical'] = df.Electrical.map({'SBrkr':0, 'FuseA':2, 'FuseF':2, 'FuseP':2, 'Mix': 4, np.nan: 0}).astype(int)\n    df['FxUp_MSZoning'] = df.MSZoning.map({'FV':0, 'RL':0, 'RM':0, 'RH':0, 'C (all)':3 , np.nan: 0}).astype(int)\n    df['FxUp_Street'] = df.Street.map({'Pave':0, 'Grvl':3, np.nan: 0}).astype(int)\n    df['FxUp_OverallQual'] = df.OverallQual.map({1: 5, 2: 5, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0})\n    df['FxUp_KitchenQual']= df.KitchenQual.map({'Ex':0, 'Gd':0, 'TA':0, 'Fa':1, 'Po': 4, np.nan: 0}).astype(int)\n\n    df['FixerUpperScore'] = (45 - df['FxUp_SaleCond'] - df['FxUp_Foundation'] - df['FxUp_HeatingQC'] \n                           - df['FxUp_Heating'] - df['FxUp_CentralAir'] - df['FxUp_GarageQual'] - df['FxUp_PavedDrive'] -\n                           df['FxUp_Electrical'] - df['FxUp_MSZoning'] - df['FxUp_Street'] - df['FxUp_OverallQual'] - df['FxUp_KitchenQual'])\n\n\n    # map MSSubClass\n    df['MSSubClass'] = df['MSSubClass'].astype(str)\n    df['MSSubClass'] = df.MSSubClass.map({'180':1, '30':2, '45':2, '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n\n    # LotAreaCut\n    df[\"LotAreaCut\"] = pd.qcut(df.LotArea,10)\n    df[\"LotAreaCut\"] = df.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n\n\n\n    # drop cfeatures  \n    df = df.drop(['RoofMatl', 'Exterior2nd'], axis=1)\n\n\n    # assign labels for alley\n    df['Alley'] = df['Alley'].replace('Pave', 1)\n    df['Alley'] = df['Alley'].replace('Grvl', 0)\n    df['Alley'] = df['Alley'].replace(np.nan, 2)\n    df['Alley'] = df['Alley'].astype(int)\n\n    # assign labels for MasVnrType\n    df['MasVnrType'] = df['MasVnrType'].replace('Stone', 4)\n    df['MasVnrType'] = df['MasVnrType'].replace('BrkFace', 3)\n    df['MasVnrType'] = df['MasVnrType'].replace('BrkCmn', 2)\n    df['MasVnrType'] = df['MasVnrType'].replace('CBlock', 1)\n    df['MasVnrType'] = df['MasVnrType'].replace('None', 0)\n    df['MasVnrType'] = df['MasVnrType'].replace(np.nan, 0)\n    df['MasVnrType'] = df['MasVnrType'].astype(int)\n\n    # masonry veneer area\n    df['MasVnrArea'] = df['MasVnrArea'].replace(np.nan, 0)\n\n    # assign value labels for basement features\n    df['BsmtQual'] = df.BsmtQual.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n    df['BsmtQual'] = df['BsmtQual'].astype(int)\n\n    df['BsmtCond'] = df.BsmtCond.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n    df['BsmtCond'] = df['BsmtCond'].astype(int)\n\n    df['BsmtExposure'] = df.BsmtExposure.map({'Gd':4, 'Av':3, 'Mn':2, 'No': 1, np.nan: 0})\n    df['BsmtExposure'] = df['BsmtExposure'].astype(int)\n\n    df['BsmtFinType1'] = df.BsmtFinType1.map({'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ': 2, 'Unf': 1, np.nan: 0})\n    df['BsmtFinType1'] = df['BsmtFinType1'].astype(int)\n\n    df['BsmtFinType2'] = df.BsmtFinType2.map({'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ': 2, 'Unf': 1, np.nan: 0})\n    df['BsmtFinType2'] = df['BsmtFinType2'].astype(int)  \n\n    # electrical mapping; replace nulls with \"3\" for standard breaker (mode)\n    df['Electrical'] = df.Electrical.map({'SBrkr':3, 'FuseA':2, 'FuseF':1, 'FuseP':0, 'Mix': 1, np.nan: 3})\n    df['Electrical'] = df['Electrical'].astype(int)\n\n    # fireplace mapping\n    df['FireplaceQu'] = df.FireplaceQu.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n    df['FireplaceQu'] = df['FireplaceQu'].astype(int)\n\n    # garage features\n    df['GarageType'] = df.GarageType.map({'2Types':5, 'Attchd':4, 'Basment':3, 'BuiltIn':4, 'CarPort': 1, 'Detchd': 2,np.nan: 0})\n    df['GarageType'] = df['GarageType'].astype(int)\n\n    df['GarageFinish'] = df.GarageFinish.map({'Fin':3, 'RFn':2, 'Unf':1, np.nan: 0})\n    df['GarageFinish'] = df['GarageFinish'].astype(int)\n\n    df['GarageQual'] = df.GarageQual.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n    df['GarageQual'] = df['GarageQual'].astype(int)\n\n    df['GarageCond'] = df.GarageCond.map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, np.nan: 0})\n    df['GarageCond'] = df['GarageCond'].astype(int)\n\n    # miscellenous feature mapping\n    df['PoolQC'] = df.PoolQC.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, np.nan: 0})\n    df['PoolQC'] = df['PoolQC'].astype(int)\n\n    df['Fence'] = df.Fence.map({'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, np.nan: 0})\n    df['Fence'] = df['Fence'].astype(int)\n\n    df['MiscFeature'] = df.MiscFeature.map({'Shed':1, 'Elev':0, 'Gar2':0, 'Othr':0, 'TenC':0, np.nan: 0})\n    df['MiscFeature'] = df['MiscFeature'].astype(int)\n    df['Shed'] = df['MiscFeature']\n    df = df.drop(['MiscFeature'], axis=1)\n\n    # fill in remaining nulls\n    df['LotFrontage'] = df['LotFrontage'].replace(np.nan, 0)\n\n    # deal with categorial variables\n    df['MSZoning'] = df.MSZoning.map({'FV':4, 'RL':3, 'RM':2, 'RH':2, 'C (all)':1 , np.nan: 3})\n    df['MSZoning'] = df['MSZoning'].astype(int)\n\n    df['Street'] = df.Street.map({'Pave':1, 'Grvl':0, np.nan: 1}) \n    df['Street'] = df['Street'].astype(int)\n\n    # assign value of 0 to regular lots; 1 for all categories of irregular\n    df['LotShape'] = df.LotShape.map({'Reg':0, 'IR1':1, 'IR2':1, 'IR3':1, np.nan: 1}) \n    df['LotShape'] = df['LotShape'].astype(int)\n\n    # assign value of '3' to hillside, '2' to level, low, and nulls, '1' to banked\n    df['LandContour'] = df.LandContour.map({'HLS':3, 'Bnk':1, 'Lvl':2, 'Low':2, np.nan: 2}) \n    df['LandContour'] = df['LandContour'].astype(int)\n\n    # only 1 entry w\/ public utilities\n    df['Utilities'] = df.Utilities.map({'AllPub':1, 'NoSeWa':0, np.nan: 2}) \n    df['Utilities'] = df['Utilities'].astype(int)\n\n    # mode = inside\n    df['LotConfig'] = df.LotConfig.map({'CulDSac':2, 'FR3':1, 'FR2':1, 'Corner':1, 'Inside':0, np.nan: 0}) \n    df['LotConfig'] = df['LotConfig'].astype(int)\n\n    # land slope, mode = Gtl\n    df['LandSlope'] = df.LandSlope.map({'Sev':2, 'Mod':1, 'Gtl':0, np.nan: 0}) \n    df['LandSlope'] = df['LandSlope'].astype(int)\n\n    # proxmmity to conditions\n    df['Condition1'] = df.Condition1.map({'PosA':5, 'PosN':4, 'RRNe':3, 'RRNn':3, \n                                        'Norm':2, 'Feedr':0, 'Artery':0, 'RRAn':1, 'RRAe':0, np.nan: 2}) \n    df['Condition1'] = df['Condition1'].astype(int)\n\n    df['Condition2'] = df.Condition1.map({'PosA':5, 'PosN':4, 'RRNe':3, 'RRNn':3, \n                                        'Norm':2, 'Feedr':0, 'Artery':0, 'RRAn':1, 'RRAe':0, np.nan: 2}) \n    df['Condition2'] = df['Condition1'].astype(int)\n\n    # \n    df['BldgType'] = df.BldgType.map({'1Fam':4, 'TwnhsE':3, 'Twnhs':2, 'Duplex':1, '2fmCon':0, np.nan: 4}) \n    df['BldgType'] = df['BldgType'].astype(int)\n\n    df['HouseStyle'] = df.HouseStyle.map({'2.5Fin':7, '2Story':6, '1Story':5, 'SLvl':4, \n                                        '2.5Unf':3, '1.5Fin':2, 'SFoyer':1, '1.5Unf':0, np.nan: 5}) \n    df['HouseStyle'] = df['HouseStyle'].astype(int)\n\n    # gabel and hip most common roof styles by far; guess on value of others\n    df['RoofStyle'] = df.RoofStyle.map({'Hip':2, 'Shed':2, 'Gable':1, 'Mansard':1, 'Flat':1, 'Gambrel':0, np.nan: 1}) \n    df['RoofStyle'] = df['RoofStyle'].astype(int)\n\n    df['Exterior1st'] = df.Exterior1st.map({'Stone': 8, 'CemntBd': 7, 'VinylSd': 6, 'BrkFace': 5, \n                                        'Plywood': 4, 'HdBoard': 3, 'Stucco': 2, 'ImStucc': 2, \n                                        'WdShing': 1, 'Wd Sdng': 1, 'MetalSd': 1, 'BrkComm': 0, \n                                        'CBlock': 0, 'AsphShn': 0, 'AsbShng': 0, np.nan: 3}) \n    df['Exterior1st'] = df['Exterior1st'].astype(int)\n\n    df['ExterQual'] = df.ExterQual.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, np.nan: 1})\n    df['ExterQual'] = df['ExterQual'].astype(int)\n\n    df['ExterCond'] = df.ExterCond.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po': 0, np.nan: 1})\n    df['ExterCond'] = df['ExterCond'].astype(int)\n\n    df['Foundation'] = df.Foundation.map({'PConc':3, 'Wood':2, 'Stone':2, 'CBlock':2, 'BrkTil': 1, 'Slab': 0, np.nan: 2})\n    df['Foundation'] = df['Foundation'].astype(int)\n\n    df['Heating'] = df.Heating.map({'GasA':2, 'GasW':1, 'OthW':0, 'Wall':0, 'Grav': 0, 'Floor': 0, np.nan: 2})\n    df['Heating'] = df['Heating'].astype(int)\n\n    df['HeatingQC'] = df.HeatingQC.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po': 0, np.nan: 1})\n    df['HeatingQC'] = df['HeatingQC'].astype(int)\n\n    df['CentralAir'] = df.CentralAir.map({'Y':1, 'N':0, np.nan: 1})\n    df['CentralAir'] = df['CentralAir'].astype(int)\n\n    df['KitchenQual'] = df.KitchenQual.map({'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po': 0, np.nan: 2})\n    df['KitchenQual'] = df['KitchenQual'].astype(int)\n\n    df['Functional'] = df.Functional.map({'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, \n                                        'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0, np.nan: 7}) \n    df['Functional'] = df['Functional'].astype(int)\n\n    df['PavedDrive'] = df['PavedDrive'].map({'Y':2, 'P':1, 'N':0, np.nan: 2})\n    df['PavedDrive'] = df['PavedDrive'].astype(int)\n\n    df['SaleType'] = df.SaleType.map({'New': 2, 'WD': 1, 'CWD': 1, 'Con': 1, 'ConLI': 1, \n                                        'ConLD':1, 'ConLw':1, 'COD': 0, 'Oth': 0, np.nan: 1}) \n    df['SaleType'] = df['SaleType'].astype(int)\n\n    df['SaleCondition'] = df.SaleCondition.map({'Partial': 5, 'Normal': 4, 'Alloca': 4, \n                                              'Family': 2, 'Abnorml': 1, 'AdjLand': 0, np.nan: 4})\n    df['SaleCondition'] = df['SaleCondition'].astype(int)\n\n    df['SeasonSold'] = df.MoSold.map({1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3, 12:0})\n    df['SeasonSold'] = df['SeasonSold'].astype(int)\n\n    df['TotalHouseOverallQual'] = df['TotalSF'] * df['OverallQual']\n    df['Functional_OverallQual'] = df['Functional'] * df['OverallQual']\n    df['TotalSF_LotArea'] = df['TotalSF'] * df['LotArea']\n    df['TotalSF_Condition'] = df['TotalSF'] * df['Condition1']\n\n    return df","7122b43d":"full = df_transform(full)","d30d50f3":"n_train=train.shape[0]\ndf = full[:n_train]","05b1609d":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in full.drop(['FxUp_SaleCond'], axis=1):\n    if full[i].dtype in numeric_dtypes: \n        numerics2.append(i)\n\nskew_features = full[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'skew':skew_features})\nskews = skews.drop(['SalePrice'], axis=0)","2d85c4f5":"high_skew = skew_features[skew_features > 0.75]\nskew_index = high_skew.index\n   \nfor i in high_skew.index:\n    full[i]= boxcox1p(full[i], boxcox_normmax(full[i]+1))\n        \nskew_features2 = full[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews2 = pd.DataFrame({'skew':skew_features2})\nprint(skews2.to_string())","7c330079":"allset = full[['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'Alley', 'BedroomAbvGr',\n       'BldgType', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath',\n       'BsmtQual', 'BsmtUnfSF', 'CentralAir', 'Condition1', 'Condition2',\n       'Electrical', 'EnclosedPorch', 'ExterCond', 'ExterQual', 'Exterior1st',\n       'Fence', 'FireplaceQu', 'Fireplaces', 'Foundation', 'FullBath',\n       'Functional', 'GarageArea', 'GarageCars', 'GarageCond', 'GarageFinish',\n       'GarageQual', 'GarageType', 'GarageYrBlt', 'GrLivArea', 'HalfBath',\n       'Heating', 'HeatingQC', 'HouseStyle', 'KitchenAbvGr',\n       'KitchenQual', 'LandContour', 'LandSlope', 'LotArea', 'LotConfig',\n       'LotFrontage', 'LotShape', 'LowQualFinSF', 'MSSubClass', 'MSZoning', 'MasVnrArea',\n       'MasVnrType', 'MiscVal', 'MoSold', 'Neighborhood', 'OpenPorchSF',\n       'OverallCond', 'OverallQual', 'PavedDrive', 'PoolArea', 'PoolQC',\n       'RoofStyle', 'SaleCondition', 'SalePrice', 'SaleType', 'ScreenPorch',\n       'Street', 'TotRmsAbvGrd', 'TotalBsmtSF', 'Utilities', 'WoodDeckSF',\n       'TotalLivingSF', 'TotalMainBath',\n       'TotalBath', 'AgeSold', 'TotalPorchSF', 'TotalSF', 'TotalArea',\n       'GarageAgeSold', 'LastRemodelYrs', 'NhoodMedianPrice', 'NhoodMeanPrice',\n       'NhoodMeanSF', 'NhoodMeanPricePerSF', 'ProxyPrice',  \n       'FixerUpperScore', 'LotAreaCut', 'Shed', 'SeasonSold',\n       'TotalHouseOverallQual', 'Functional_OverallQual', 'TotalSF_LotArea',\n       'TotalSF_Condition']]\n\ntrain = allset[:n_train]\ntest = allset[n_train:]\n\nscaler = RobustScaler()\n\nX_train = train.drop(['SalePrice'], axis=1)\nX_test = test.drop(['SalePrice'], axis=1)\ny_train = train['SalePrice']\n\nX_train_scaled = scaler.fit(X_train).transform(X_train)\ny_train_logged = np.log(train['SalePrice'])\nX_test_scaled= scaler.transform(X_test)\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled)\ny_logged_df = pd.DataFrame(y_train_logged)\nX_test_scaled_df = pd.DataFrame(X_test_scaled)","01102841":"lasso_fi=Lasso(alpha=0.001)\nlasso_fi.fit(X_train_scaled,y_train_logged)\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso_fi.coef_}, index=X_train.columns)\nFI_sorted = FI_lasso.sort_values(\"Feature Importance\",ascending=False)\nprint(FI_sorted.to_string())","394193cc":"# Root mean squared error (RMSE)\ndef rmse(y_pred, y_test):\n    return np.sqrt(mean_squared_error(y_test, y_pred))\n\n\nclass CvScore(object):\n    def __init__(self, list, name_list, X, y, folds=5, score='neg_mean_squared_error', seed=66, split=0.33):\n        self.X = X\n        self.y = y\n        self.folds = folds\n        self.score = score\n        self.seed = seed\n        self.split = split\n        self.model = list[0]\n        self.list = list\n        self.name = name_list[0]\n        self.name_list = name_list\n    \n    def cv(self):\n        cv_score = cross_val_score(self.model, self.X, self.y, cv=self.folds, scoring=self.score)\n        score_array = np.sqrt(-cv_score)\n        mean_rmse = np.mean(score_array)\n        print(\"Mean RMSE: \", mean_rmse)\n    \n    def cv_list(self):\n        for name, model in zip(self.name_list, self.list):\n            cv_score = cross_val_score(model, self.X, self.y, cv=self.folds, scoring=self.score)\n            score_array = np.sqrt(-cv_score)\n            mean_rmse = np.mean(score_array)\n            std_rmse = np.std(score_array)\n            print(\"{}: {:.5f}, {:.4f}\".format(name, mean_rmse, std_rmse))","ba14e44b":"lr = LinearRegression()\n\n# Best parameters found:  {'alpha': 40} 0.1120323653109581\nridge = Ridge(alpha=40)\n\n# Best parameters found:  {'alpha': 0.0001} 0.11267514926665378\nlasso = Lasso(alpha=0.0001)\n\n# Best parameters found:  {'max_depth': 80, 'n_estimators': 800} 0.12673567602317132\nrfr = RandomForestRegressor(max_depth=80, n_estimators=800)\n\nregression_list = [lr, ridge, lasso, rfr]\nname_list = [\"Linear\", \"Ridge\", \"Lasso\", \"Random Forest\"]","eb66d929":"scores = CvScore(regression_list, name_list, X_train_scaled,y_train_logged)\nscores.cv_list()","c6432a46":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(\"Best parameters found: \", grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","3cce0b8d":"param_grid={'alpha':[0.00001, 0.0001, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 2, 4, 10, 20, 25, 30, 35, 40, 60, 100, 250, 500]}\ngrid(Ridge()).grid_get(X_train_scaled,y_train_logged,param_grid)","df49fcd4":"param_grid={'alpha':[0.00001, 0.0001, 0.01, 0.1, 0.25, 0.5, 0.75, 1]}\ngrid(Lasso()).grid_get(X_train_scaled,y_train_logged,param_grid)","b5f87513":"# Random Forest Regression\nrfr_param_grid = {\n     'n_estimators':[100, 200, 400, 600, 800],\n     'max_depth': [80, 120, 160, 200]\n}\n\ngrid(RandomForestRegressor()).grid_get(X_train_scaled,y_train_logged,rfr_param_grid)","96435349":"# define cross validation strategy\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse\n\n# class object group for ensembling the model predictions together\nclass AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","9517cfcc":"# even weighted model\nvariables = 4\n\nw1 = 1\/variables\nw2 = 1\/variables\nw3 = 1\/variables\nw4 = 1\/variables\n\nweight_avg = AverageWeight(mod = [lr, lasso, ridge, rfr],weight=[w1,w2,w3,w4])\n\nscore = rmse_cv(weight_avg,X_train_scaled,y_train_logged)\nprint(score.mean())","20b57047":"a = Imputer().fit_transform(X_train_scaled)\nb = Imputer().fit_transform(y_train_logged.values.reshape(-1,1)).ravel()\n\nensemble_model = AverageWeight(mod = [lr, lasso, ridge, rfr],weight=[w1,w2,w3,w4])\nensemble_model = ensemble_model.fit(a,b)","1850a078":"prediction = np.exp(ensemble_model.predict(X_test_scaled))\nprint(prediction)\n\nid_list = test_original['Id']\nsubmission =pd.DataFrame({'Id': id_list, 'SalePrice': prediction})\nsubmission#.to_csv(\"submission.csv\",index=False)","5ffa9e38":"### Test Data","9b15ea5f":"# Normalisation","b72e92a3":"### Feature Importance\n\nL1 penalties (lasso) will exclude variables that are not too well correlated. ","474f8174":"### Transformations","40e99d14":"### Scaling","db17c9fc":"### Hyperparameter Tuning\n\nUsing a grid search cross-validation method, the models could be fine tuned with the optimal paramaters. \n\nGridSearch looks at the best mean score for each combination of parameters in each model.\n\n* Ridge: alpha\n* Lasso: alpha\n* Random Forest Regressors : number of estimators and maximum depth","f6ccc8e0":"### Load Data","04156d87":"SalePrice is quite varied from the maximum and the minimum. ","397086a7":"We created a list of numerics to transform\n\n__BoxCox Transformation__:\n\nA boxcox transformation standardised our data; standardising the data reduced the range and effectively the variablity. \n\nStandardization (also called z-score normalization) transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1.\n\n__Scaling__:\n\nImplementing the the RobustScaler help to scale the data, similarly as the boxcox transformation. \n\nIn scaling (also called min-max scaling), you transform the data such that the features are within a specific range e.g. [0, 1].\n\nDefinitions sourced from:\nhttps:\/\/kharshit.github.io\/blog\/2018\/03\/23\/scaling-vs-normalization\n","161891ec":"* Fix a few incorrect values; these values either had typos or had 'garage built' after house was sold\n* Impute 0 values for variables where nulls likely mean absence of the attribute\n* Calculate several aggregate variables on square footage and baths\n* Change 'YearBuilt' to an 'AgeSold' variable which is slightly more accurate\n* Add a 'TotalArea' feature that includes the garage\n* Change a few values for the year of remodeling as they were listed as remodeled after house sold","074aa098":"### Imports","bb1341ad":"Logging it will definitely fit the data better","fb72a4e4":"### Visualising of significant categorical variables","ea9a33b9":"## Looking at the distribution of \"SalePrice\"","d7a215eb":"# House Prices - House Linnister\nKaggle Competition - Ames Dataset\n\n![image](https:\/\/u.realgeeks.media\/amesre\/_rgg\/landscape_images\/BeautifulHomeExterior.jpg)\n\n\n__Introduction__:\n\nWe're a small team of Data Science interns from Explore Data Science Academy. This is our first exposure to Kaggle and we got to build our first few model have a look.\n\nKaggle describes this competition as follows:\n\nAsk a home buyer to describe their dream house, and they probably won\u2019t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\u2019s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\n__Competition Description__:\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this [competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) from Kaggle challenges you to predict the final price of each home.\n\nFor a detailed description of the data set, click [here](..\/input\/data_description.txt).","e0aaf55c":"Having a look at the categories and numerical values using the train set (test will have the same number of categorical variables but not numerical variables as 'SalePrice' is not included in test; this is what we want to predict)","60baf6da":"### Linear Regression \n\n$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$\n\n\n### Lasso\n\n$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^p|b_j|$$\n\n### Ridge \n\n$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^pb_j^2$$\n\n### Random Forest Regressor\n\n\n\n\n\n","ce8fa6f8":"### Root Mean Squared Error (RMSE)\n\n$$\\sqrt{\\frac{1}{N}\\sum_{i=1}^Ne^2}$$\n\n\n\n### Cross Validation\n\n\n$$CV(f) = \\frac{1}{N}\\sum_{i=1}^NL(\\hat{y_i}, \\hat{f}^{-k(i)}(x_i))$$","4373c0aa":"### Skewed Features","825ebc62":"### Visual Representation of highly correlated features","2b7025e3":"# Where to for the future?\n","9a9ce8b5":"### Correlation matrix\nThis is done to have a clear picture on the correlation between columns in the dataset.**","0c7ae4c8":"# Exploratory Data Analysis (EDA)\n\n### Take a First Look of our Data:\nThe first step was to have a look at what the data had to show us","a5631cda":"> ### A look at the missing data","198eb7a6":"### BoxCox Transformation","889205b3":"# Feature engineering\n\n__Favouring ordinal__:\n\nOrdinal Is favoured in this case as it creates ranking for the model to recognise. Some of the categories are simply \"present\" or \"not present\" but require a scale.\n\n__Grouping variabes__:\n\nGrouping qualities and surface area were done as there was significant correlations between the the different all the qualities and surface areas\n\nThanks to [juliencs](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset)\n","0cc2c435":"### Distribution of significant numerical variables","660fc96b":"![me-machine-learning-mathematics-damn-maths-41419748.png]","739dc0a0":"### Ensemble","38e3e1a5":"# Loading and Preparing","b1498020":"### Finding the best variables","a0ef190d":"# Model\n\nAssessing Models:\nWe utilised the Root Mean Squared Error (RMSE) to identify the performance of the train and test, ultimately, to indicate any overfitting\/underfitting in the models.\nCross Validation assisted us with running permutations of the different parameters to produce the most effective model with the least RMSE\n\n__Models Performed:__\n* Linear Regression\n* Ridge\n* Lasso\n* Random Forest Regressor\n","6a27eb93":"SalePrice is definitely skewed and not normal. We will log it when splitting into the test set","7eeff0f3":"![image](attachment:329jaz.jpg)"}}