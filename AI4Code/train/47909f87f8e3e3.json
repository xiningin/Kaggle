{"cell_type":{"92339cdb":"code","4337e0fe":"code","10969c66":"code","ffc9b9d2":"code","b163829b":"code","61a7d5e7":"code","2b0e35f7":"code","806cfd11":"code","099f438c":"code","37f5329a":"code","bd2e0be5":"code","868fc7fd":"code","5c79aaba":"code","68756708":"code","816c1479":"code","93ba73d5":"code","795bf5a4":"code","ea9b81b8":"code","d84b8230":"code","49aca9e8":"code","5aca18b9":"code","3d1bfefa":"code","82ce26b4":"code","451760c5":"code","a38d6243":"code","8d8d063f":"code","b5af8335":"code","eece38e2":"code","37baf684":"code","1bc7839c":"code","8c2c514e":"code","fe46c7f8":"markdown","59cb864b":"markdown"},"source":{"92339cdb":"# Basic Function\nimport numpy as np \nimport pandas as pd \nimport os\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\nfrom tqdm import tqdm_notebook\n\n# Keras for text preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence ","4337e0fe":"train = pd.read_csv('..\/input\/quora-question-pairs\/train.csv')#.fillna('something')\ntest = pd.read_csv('..\/input\/quora-question-pairs\/test.csv')#.fillna('something')","10969c66":"train.head()","ffc9b9d2":"test.head()","b163829b":"# note that spacy_tok takes a while run it just once\ndef encode_sentence(path, vocab2index, N=400, padding_start=True):\n    x = spacy_tok(path.read_text())\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc, l","61a7d5e7":"re_br = re.compile(r'<\\s*br\\s*\/?>', re.IGNORECASE)\ndef sub_br(x): return re_br.sub(\"\\n\", x)\n\nmy_tok = spacy.load('en')\ndef spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]","2b0e35f7":"# counts = Counter()\n# for text in tqdm_notebook(train.question1):\n#     counts.update(spacy_tok(text))\n# for text in tqdm_notebook(train.question2):\n#     counts.update(spacy_tok(text))\n# for text in tqdm_notebook(test.question1):\n#     counts.update(spacy_tok(text))\n# for text in tqdm_notebook(teat.question2):\n#     counts.update(spacy_tok(text))","806cfd11":"MAX_LEN=60\nWORD_NUM = 180000","099f438c":"train_q1 = train[\"question1\"].fillna(\"something\").values\ntrain_q2 = train[\"question2\"].fillna(\"something\").values\n\ntest_q1 = test[\"question1\"].fillna(\"something\").values\ntest_q2 = test[\"question2\"].fillna(\"something\").values\n\ntokenizer = Tokenizer(num_words=MAX_LEN, filters='!\"#$%&()*+,-.\/:;<=>@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(list(train_q1)+list(train_q2)+list(test_q1)+list(test_q2))","37f5329a":"# train_q1_seq = tokenizer.texts_to_sequences(train_q1)\n# train_q2_seq = tokenizer.texts_to_sequences(train_q2)\n# test_q1_seq = tokenizer.texts_to_sequences(test_q1)\n# test_q2_seq = tokenizer.texts_to_sequences(test_q2)\n# train_q1_seq = pad_sequences(train_q1_seq, maxlen=MAX_LEN)\n# train_q2_seq = pad_sequences(train_q2_seq, maxlen=MAX_LEN)\n# test_q1_seq = pad_sequences(test_q1_seq, maxlen=MAX_LEN)\n# test_q2_seq = pad_sequences(test_q2_seq, maxlen=MAX_LEN)","bd2e0be5":"word_index = tokenizer.word_index","868fc7fd":"def load_glove(word_index):\n    EMBEDDING_FILE = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n    \n#     all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n#     embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(WORD_NUM, len(word_index))\n#     embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, 300))\n    for word, i in tqdm_notebook(word_index.items()):\n        if i >= WORD_NUM: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix ","5c79aaba":"glove_embeddings = load_glove(word_index)","68756708":"class QuoraDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=60, train=True):\n        self.train = train\n        self.q1 = df['question1'].fillna('something').values\n        self.q2 = df['question2'].fillna('something').values\n        \n        self.q1 = tokenizer.texts_to_sequences(self.q1)\n        self.q2 = tokenizer.texts_to_sequences(self.q2)\n        \n        self.q1 = pad_sequences(self.q1, maxlen=max_len)\n        self.q2 = pad_sequences(self.q2, maxlen=max_len)\n            \n        if train:\n            self.y = df['is_duplicate'].values\n        \n    def __len__(self):\n        return len(self.q1)\n    \n    def __getitem__(self, idx):\n        if self.train:\n            return self.q1[idx], self.q2[idx], self.y[idx]\n        else:\n            return self.q1[idx], self.q2[idx]","816c1479":"from sklearn.model_selection import train_test_split","93ba73d5":"train_df, valid_df = train_test_split(train, test_size=0.2)","795bf5a4":"train_ds = QuoraDataset(train_df, tokenizer, train=True)\nvalid_ds = QuoraDataset(valid_df, tokenizer, train=True)\ntest_ds = QuoraDataset(test, tokenizer, train=False)","ea9b81b8":"TRAIN_BS = 128\nTEST_BS = 2048","d84b8230":"train_dl = DataLoader(train_ds, batch_size=TRAIN_BS, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=TEST_BS, shuffle=False)\ntest_dl = DataLoader(test_ds, batch_size=TEST_BS, shuffle=False)","49aca9e8":"class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, hidden_size, max_features=18000, embed_size=300):\n        super(NeuralNet, self).__init__()\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n\n        self.lstm1 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.linear = nn.Linear(4*hidden_size, 2*16)\n        self.relu = nn.ELU()\n        self.dropout = nn.Dropout(0.1)\n        \n        self.out = nn.Linear(2*16, 1)\n        \n    def forward(self, q1, q2):\n        q1_embedding = self.embedding(q1)\n        q2_embedding = self.embedding(q2)\n        \n        q1_lstm, (h1, _) = self.lstm1(q1_embedding)\n        q2_lstm, (h2, _) = self.lstm2(q2_embedding)\n        \n#         print(q1_lstm.shape)\n#         print(q2_lstm.shape)\n#         print(h1.shape)\n#         print(h2.shape)\n#         print(torch.mean(q1_lstm,dim=1).shape)\n    \n        \n#         avg_pool = torch.mean(h1, 1)\n#         max_pool, _ = torch.max(h2, 1)\n        \n        q1 = torch.cat((h1[0], h1[1]), 1)\n        q2 = torch.cat((h2[0], h2[1]), 1)\n        \n#         q1 = self.linear(q1)\n#         q2 = self.linear(q2)\n        \n        \n        conc = self.relu(self.linear(torch.cat([q1,q2],dim=1)))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        \n        return out","5aca18b9":"def val_metrics(model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for q1, q2, y in valid_dl:\n        q1 = q1.long().cuda()\n        q2 = q2.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(q1, q2)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss\/total, correct\/total","3d1bfefa":"def train_epocs(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for q1,q2, y in tqdm_notebook(train_dl):\n            q1 = q1.long().cuda()\n            q2 = q2.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(q1, q2)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics(model, valid_dl)\n#         if i % 1 == 1:\n        print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss\/total, val_loss, val_acc))","82ce26b4":"model = NeuralNet(glove_embeddings,hidden_size=128).cuda()","451760c5":"train_epocs(model, epochs=10, lr=0.01)","a38d6243":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","8d8d063f":"test.head()","b5af8335":"next(iter(test_dl))","eece38e2":"test_preds = np.zeros(len(test_ds))\nfor i, (q1, q2) in enumerate(test_dl):\n    q1 = q1.long().cuda()\n    q2 = q2.long().cuda()\n    y_pred = model(q1, q2).detach()\n    test_preds[i * TEST_BS:(i+1) * TEST_BS] = sigmoid(y_pred.cpu().numpy())[:, 0]","37baf684":"submission = pd.read_csv('..\/input\/quora-question-pairs\/sample_submission.csv')","1bc7839c":"submission['is_duplicate'] = test_preds","8c2c514e":"submission.to_csv('submission.csv', index=False)","fe46c7f8":"## Dataset","59cb864b":"## Model"}}