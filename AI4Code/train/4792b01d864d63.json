{"cell_type":{"d9220ad2":"code","f897c601":"code","c8d93a5f":"code","bb998eb5":"code","6fa8a813":"code","7b1f8ac1":"code","ad4b98b7":"code","52cc1b6d":"code","73db7d0b":"code","807a2072":"code","79ffa3f9":"code","9b529884":"code","413840cb":"code","a4903696":"code","5388b9d7":"code","b30bb59a":"code","5572bc7b":"code","eb99aa40":"code","729bdce9":"code","b313677c":"code","0dc126a3":"code","ead24ee4":"markdown","fbbce0c3":"markdown","68525248":"markdown","264400b4":"markdown","862ac39f":"markdown","f5cb1f09":"markdown","e959f26d":"markdown","b1a937b4":"markdown","478fe6b0":"markdown","cd15367a":"markdown","dee9eb24":"markdown","c36e414e":"markdown"},"source":{"d9220ad2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importing alll the necessary packages to use the various classification algorithms\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.cross_validation import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f897c601":"# Importing Data\ndata = pd.read_csv(\"..\/input\/Iris.csv\")","c8d93a5f":"# Showing first five columns\ndata.head()","bb998eb5":"# Showing last five columns\ndata.tail()","6fa8a813":"# Checking Null Values\ndata.isnull().sum()","7b1f8ac1":"# We need to drop useless columns\ndata.drop([\"Id\"], axis = 1, inplace = True)","ad4b98b7":"# Statistics Features\ndata.describe()","52cc1b6d":"sns.jointplot(data.loc[:,'SepalLengthCm'], data.loc[:,'PetalLengthCm'], kind=\"regg\", color=\"#ce1414\")","73db7d0b":"sns.set(style=\"white\")\ndf = data.loc[:,['SepalLengthCm','SepalWidthCm','PetalLengthCm', 'PetalWidthCm']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=4)","807a2072":"# Histogram\n# bins = number of bar in figure\ndata.SepalLengthCm.plot(kind = 'hist', bins = 50, figsize = (15,15))\ndata.SepalWidthCm.plot(kind = 'hist', bins = 50, figsize = (15,15))\ndata.PetalLengthCm.plot(kind = 'hist', bins = 50, figsize = (15,15))\ndata.PetalWidthCm.plot(kind = 'hist', bins = 50, figsize = (15,15))\nplt.show()","79ffa3f9":"# Correlation map\nf,ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","9b529884":"# Create a trace\ntrace = go.Scatter(\n    x = data.SepalLengthCm,\n    y = data.SepalWidthCm,\n    mode = 'markers'\n)\n\ndata_2 = [trace]\nfig = dict(data = data_2)\niplot(fig)","413840cb":"# Trace 2\ntrace1 = go.Scatter(\n    x  = data.PetalLengthCm,\n    y  = data.PetalWidthCm,\n    mode = 'markers',\n    marker = dict(\n        size = 16,\n        colorscale = 'Viridis',\n        showscale = True\n    )\n)\n\ndata_1 = [trace1]\nfig = dict(data = data_1)\niplot(fig)","a4903696":"trace2 = go.Box(\n    y = data.PetalLengthCm\n)\n\ntrace3 = go.Box(\n    y = data.PetalWidthCm\n)\n\ndata_2 = [trace2, trace3]\nfig = dict(data = data_2)\niplot(fig)","5388b9d7":"# Split into train and test\n# The attribute test_size = 0.2 splits the data into 80% and 20% ratio. train = 80% and test = 20%\ntrain, test = train_test_split(data, test_size = 0.3)","b30bb59a":"# Four parameter going to help predict our main subject\ntrain_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# Output our training data\ntrain_x = train.Species\ntest_Y = test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# Output our test data\ntest_y = test.Species","5572bc7b":"# Implement our regression\nmodel = LogisticRegression()\n# Fitting the model\nmodel.fit(train_X,train_x)\n# Predict the fitting data\nprediction = model.predict(test_Y)\nprint('The accuracy is', metrics.accuracy_score(prediction,test_y))","eb99aa40":"# Implementing SVM\nmodel = svm.SVC()\n# Fitting the model\nmodel.fit(train_X,train_x)\n# Predict the fitting data\nprediction = model.predict(test_Y) \nprint('The accuracy is:',metrics.accuracy_score(prediction,test_y))","729bdce9":"# Implementing Decision Tree Classifier\nmodel = DecisionTreeClassifier()\n# Fitting the model\nmodel.fit(train_X,train_x)\nprediction = model.predict(test_Y)\nprint('The accuracy is',metrics.accuracy_score(prediction,test_y))","b313677c":"# This examines 3 neighbours\nmodel = KNeighborsClassifier(n_neighbors=3) \n# Fitting the model\nmodel.fit(train_X,train_x)\nprediction = model.predict(test_Y)\nprint('The accuracy is',metrics.accuracy_score(prediction,test_y))","0dc126a3":"# Find Best K Value\nscore_list = []\nfor each in range(1,50):\n    knn_2 = KNeighborsClassifier(n_neighbors = each)\n    knn_2.fit(train_X, train_x)\n    score_list.append(knn_2.score(test_Y,test_y))\n\nplt.plot(range(1,50), score_list)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","ead24ee4":"### Decision Tree","fbbce0c3":"<a id=\"p2\"><\/a>\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics.","68525248":"# Contents\n1. [Importing Libraries and Packages](#p1)\n2. [Loading and Viewing Data Set](#p2)\n4. [Visualization](#p3)\n5. [Initializing, Optimizing, and Predicting](#p4)","264400b4":"### Support Vector Machines (SVM)","862ac39f":"<a id=\"p3\"><\/a>\n# 3. Visualization\n\nIn order to visualizate the data, we are goingo to use matplotlib and seaborn.","f5cb1f09":"#### As you can see our data clean.","e959f26d":"<a id=\"p4\"><\/a>\n# 4. Initializing, Optimizing, and Predicting\nNow that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. ","b1a937b4":"**If you liked the kernel, please upvote or make a comment. They motivate me :)**","478fe6b0":"### Splitting The Data into Training And Testing Dataset","cd15367a":"### Logistic Regression","dee9eb24":"### K-Nearest Neighbours","c36e414e":"<a id=\"p1\"><\/a>\n# 1. Importing Libraries and Packages\nWe will use these packages to help us manipulate the data and visualize the features\/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data."}}