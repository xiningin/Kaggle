{"cell_type":{"9b571722":"code","b9ac6585":"code","3075cd92":"code","9c2f0c36":"code","437951e7":"code","6564e331":"code","1f5a29fb":"code","af8507de":"code","c5716b10":"code","f9d5d293":"code","f64eef09":"code","c992c727":"code","333d10e6":"code","123e7f3d":"code","738cbc79":"code","a7dc366f":"code","0c690336":"code","b513c5d9":"code","8124ee98":"code","cfdcb033":"code","2f63193d":"code","2a2ddb9a":"code","4aee2b3d":"code","2994e0c9":"code","703d54a2":"code","334dc1e1":"code","c7417b9d":"code","f1514d1a":"code","5a24f7a0":"code","594731e6":"code","d5fb8498":"code","d21fba74":"code","8f3734fd":"code","abdf54fd":"code","585814f4":"code","9fd44220":"code","94ece9fd":"code","0429893d":"code","c38516b5":"code","07acd772":"code","e1a5424a":"code","1e32b49f":"code","fc0ce2f1":"code","111eb76a":"code","5a5d6c5e":"code","8e210877":"code","aa8b2415":"code","e792b5ad":"code","fcf883a3":"code","76abf702":"code","8d5fcd3c":"code","fc9e51b7":"code","928bb67f":"code","e8f9a4f8":"code","f7bcd61a":"code","35eb9d76":"code","17319ca1":"code","3cf473f2":"code","6ce505d4":"code","b85be474":"code","61bcfc85":"code","eba99bf6":"code","510b1896":"code","69a541ca":"code","00e639f7":"code","dcd8e7ee":"code","d0f6cb5b":"code","fedeee4c":"code","5dc4a171":"code","2a274255":"code","ab9dd680":"code","cfe9c70b":"code","b83b354a":"code","9bdd0494":"code","90e9c397":"code","15c32e82":"code","f1b10450":"code","30c4c590":"code","03dc1b74":"code","9740584f":"code","cd1a9595":"markdown","872d795f":"markdown","8b053926":"markdown","b9caabaa":"markdown","6fb5bf56":"markdown","f57ef047":"markdown","e12ed50b":"markdown","1363556f":"markdown","19aea903":"markdown","5248a9f9":"markdown","acf25006":"markdown","7f84f3f1":"markdown","e270f6fb":"markdown","171b52e4":"markdown","1c9feeab":"markdown","0a7e464c":"markdown","9f3cbb73":"markdown","925515c0":"markdown","9802f46f":"markdown","fb051063":"markdown","5378cc2a":"markdown","3c52b2e2":"markdown","65041a8d":"markdown","ddf282bd":"markdown","4c74e713":"markdown","919ec676":"markdown","cf4bc016":"markdown","9ff0344b":"markdown","fb205c2a":"markdown","6fc2fea1":"markdown","046e64c9":"markdown","79611c92":"markdown","0c6fb6d8":"markdown","879eb064":"markdown","319140b6":"markdown","3e8c68df":"markdown","cbcb93c1":"markdown","c64c9a7f":"markdown","3a34655c":"markdown","5762dd70":"markdown","c2232bae":"markdown","2fca96ee":"markdown","539ee2aa":"markdown","cfec73af":"markdown","f7ac528c":"markdown","30fe7938":"markdown","bfbbadc0":"markdown","0812f814":"markdown","2c614a3c":"markdown","fcf1d8c0":"markdown","9be0cee9":"markdown","2f20a228":"markdown","d3932753":"markdown","566636c4":"markdown","61784e65":"markdown","ae23b0f8":"markdown","bba45439":"markdown","58b4b8f1":"markdown","edd45c15":"markdown","7295cb78":"markdown","056edf42":"markdown","87dfba77":"markdown","f2849f6b":"markdown","607a097c":"markdown","b5274cf9":"markdown","a55d44cd":"markdown"},"source":{"9b571722":"!python -m spacy download en_vectors_web_lg\n!python -m spacy link en_vectors_web_lg en_vectors_web_lg","b9ac6585":"from __future__ import unicode_literals\nimport spacy\nnlp = spacy.load('en_vectors_web_lg')","3075cd92":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9c2f0c36":"import math\ndef distance2d(x1, y1, x2, y2):\n    return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)","437951e7":"distance2d(70, 30, 75, 40) # panda and capybara","6564e331":"distance2d(8, 3, 65, 90) # tarantula and elephant","1f5a29fb":"import json","af8507de":"color_data = json.loads(open(\"\/kaggle\/input\/color-data-xkcd\/xkcd.json\").read())","c5716b10":"def hex_to_int(s):\n    s = s.lstrip(\"#\")\n    return int(s[:2], 16), int(s[2:4], 16), int(s[4:6], 16)","f9d5d293":"colors = dict()\nfor item in color_data['colors']:\n    colors[item[\"color\"]] = hex_to_int(item[\"hex\"])","f64eef09":"colors['olive']","c992c727":"colors['red']","333d10e6":"colors['black']","123e7f3d":"import math\ndef distance(coord1, coord2):\n    # note, this is VERY SLOW, don't use for actual code\n    return math.sqrt(sum([(i - j)**2 for i, j in zip(coord1, coord2)]))\ndistance([10, 1], [5, 2])","738cbc79":"def subtractv(coord1, coord2):\n    return [c1 - c2 for c1, c2 in zip(coord1, coord2)]\nsubtractv([10, 1], [5, 2])","a7dc366f":"def addv(coord1, coord2):\n    return [c1 + c2 for c1, c2 in zip(coord1, coord2)]\naddv([10, 1], [5, 2])","0c690336":"def meanv(coords):\n    # assumes every item in coords has same length as item 0\n    sumv = [0] * len(coords[0])\n    for item in coords:\n        for i in range(len(item)):\n            sumv[i] += item[i]\n    mean = [0] * len(sumv)\n    for i in range(len(sumv)):\n        mean[i] = float(sumv[i]) \/ len(coords)\n    return mean\nmeanv([[0, 1], [2, 2], [4, 3]])","b513c5d9":"distance(colors['red'], colors['green']) > distance(colors['red'], colors['pink'])","8124ee98":"def closest(space, coord, n=10):\n    closest = []\n    for key in sorted(space.keys(),\n                        key=lambda x: distance(coord, space[x]))[:n]:\n        closest.append(key)\n    return closest","cfdcb033":"closest(colors, colors['red'])","2f63193d":"closest(colors, [150, 60, 150])","2a2ddb9a":"closest(colors, subtractv(colors['purple'], colors['red']))","4aee2b3d":"closest(colors, addv(colors['blue'], colors['green']))","2994e0c9":"# the average of black and white: medium grey\nclosest(colors, meanv([colors['black'], colors['white']]))","703d54a2":"# an analogy: pink is to red as X is to blue\npink_to_red = subtractv(colors['pink'], colors['red'])\nclosest(colors, addv(pink_to_red, colors['blue']))","334dc1e1":"# another example: \nnavy_to_blue = subtractv(colors['navy'], colors['blue'])\nclosest(colors, addv(navy_to_blue, colors['green']))","c7417b9d":"import random\nred = colors['red']\nblue = colors['blue']\nfor i in range(14):\n    rednames = closest(colors, red)\n    bluenames = closest(colors, blue)\n    print (\"Roses are \" + rednames[0] + \", violets are \" + bluenames[0])\n    red = colors[random.choice(rednames[1:])]\n    blue = colors[random.choice(bluenames[1:])]","f1514d1a":"doc = nlp(open(\"\/kaggle\/input\/dracula-story-pg345\/pg345.txt\").read())\n# use word.lower_ to normalize case\ndrac_colors = [colors[word.lower_] for word in doc if word.lower_ in colors]\navg_color = meanv(drac_colors)\nprint (avg_color)","5a24f7a0":"closest(colors, avg_color)","594731e6":"for cname in closest(colors, colors['mauve']):\n    print (cname + \" trousers\")","d5fb8498":"# let's define documents with 1 or 2 sentences each.\n\ndocuments =[\n            \"I was hungry ,so i ate a cake. Now I like cakes more.\",\n            \"I ate chicken but I'm still hungry so I bought a tasty cake and gave it to my hungry sister\",\n            \"My sister was hungry so she ate the tasty cake.\"\n]","d21fba74":"import re\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')","8f3734fd":"# normalization\nprocessed_documents = []\nfor document in documents:\n    processed_text = document.lower()\n    processed_text = re.sub('[^a-zA-Z]', ' ', processed_text )\n    processed_documents.append(processed_text)\n    \nprocessed_documents","abdf54fd":"# tokenization\n# note that we didn't use setence tokenization because we are considering the document wise\nall_words = [nltk.word_tokenize(sent) for sent in processed_documents]\nprint(all_words)","585814f4":"# stopwords removal\n\nfrom nltk.corpus import stopwords\nfor i in range(len(all_words)):\n    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\nall_words","9fd44220":"from nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer() \nfinal_docs = []\nfor words in all_words:\n    doc = [lemmatizer.lemmatize(word) for word in words]\n    final_docs.append(doc)\nfinal_docs","94ece9fd":"# constructing vocab\nvocab = []\nfor words in final_docs:\n    for word in words:\n        if not (word in vocab):\n            vocab.append(word)","0429893d":"vocab","c38516b5":"# Document Matrix\ndm=[]\ndef initTemparray(length_vocab):\n    a=[]\n    for i in range(length_vocab):\n        a.append(0)\n      #print(a)\n    return a\n\nfor doc in final_docs:\n    temparray = initTemparray(len(vocab))\n    for word in doc:\n        #word = removeChars(word)\n        if word in vocab:\n            temparray[vocab.index(word)]= temparray[vocab.index(word)] + 1\n    dm.append(temparray)\n","07acd772":"dm \n# dm means document matrix and dv means document vector\n# Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. ","e1a5424a":"# Cosidering only the first 3 words in vocabulary for easy representation\n\nvocab_wise = list(zip(*dm[::-1]))","1e32b49f":"vocab_wise","fc0ce2f1":"from matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\n\n# Since we can plot only 3 dimentions at a time we could take only the first 3 rows representing hungry, ate, cake.\nx_vals, y_vals, z_vals = list(vocab_wise[0]), list(vocab_wise[1]), list(vocab_wise[2])\n","111eb76a":"import plotly.express as px\nfig = px.scatter_3d(x=x_vals, y=y_vals, z=z_vals, text=vocab[0:3])\n\nfig.update_layout(scene = dict(\n                    xaxis_title='hungry',\n                    yaxis_title='ate',\n                    zaxis_title='cake'),\n                    width=1000,\n                    margin=dict(r=20, b=5, l=5, t=5))\n\nfig.show()","5a5d6c5e":"print(dm[0])\nprint(final_docs[0])\nprint(vocab)","8e210877":"print(dm[1])\nprint(final_docs[1])\nprint(vocab)","aa8b2415":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d","e792b5ad":"# Co-occurance matrix\n# We build the map by taking into account each word, say hungry and finding the documents that has hungry (usage) and adding them \ncmatrix=[]\nfor word in vocab:\n    current_index =vocab.index(word)\n    temparray = initTemparray(len(vocab))\n    print(f'Constructing for word: {word} = 1')\n    for idx, dv in enumerate(dm):\n        print('-'*45)\n        print(f'{idx+1} row in dm')\n        print(dv)\n        if dv[current_index]==1:\n            for i in range(len(vocab)):\n                temparray[i]=temparray[i]+ dv[i]\n                \n    cmatrix.append(temparray)\n    print(f'Final corpus map for {word}: {temparray}')\n","fcf883a3":"cmatrix","76abf702":"print(vocab)","8d5fcd3c":"final_docs","fc9e51b7":"import copy\n# normalized corpusmap\nnormCorpusMap = copy.deepcopy(cmatrix)\nfor j in range(len(vocab)):\n    for i in range(len(vocab)):\n        normCorpusMap[j][i] = cmatrix[j][i] - max(cmatrix[j])+1","928bb67f":"normCorpusMap","e8f9a4f8":"from matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\n\n# Since we can plot only 3 dimentions at a time we could take only the first 3 rows representing hungry, ate, cake.\nx_vals, y_vals, z_vals = normCorpusMap[0], normCorpusMap[1], normCorpusMap[2]\n","f7bcd61a":"import plotly.express as px\nfig = px.scatter_3d(x=x_vals, y=y_vals, z=z_vals, text=vocab)\n\nfig.update_layout(scene = dict(\n                    xaxis_title='hungry',\n                    yaxis_title='ate',\n                    zaxis_title='cake'),\n                    width=1000,\n                    margin=dict(r=20, b=10, l=10, t=10))\n\nfig.show()","35eb9d76":"#sample test\ntokens = nlp(\"dog cat banana afskfsd\")\n\nfor token in tokens:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)","17319ca1":"doc = nlp(open(\"\/kaggle\/input\/dracula-story-pg345\/pg345.txt\").read())\ntype(doc)","3cf473f2":"# all of the words in the text file\ntokens = list(set([w.text for w in doc if w.is_alpha]))","6ce505d4":"len(tokens)","b85be474":"print(tokens[:10])","61bcfc85":"nlp.vocab","eba99bf6":"nlp.vocab['cheese'].vector","510b1896":"def vec(s):\n    return nlp.vocab[s].vector","69a541ca":"from numpy import dot\nfrom numpy.linalg import norm\n\n# cosine similarity\ndef cosine(v1, v2):\n    if norm(v1) > 0 and norm(v2) > 0:\n        return dot(v1, v2) \/ (norm(v1) * norm(v2))\n    else:\n        return 0.0","00e639f7":"cosine(vec('dog'), vec('puppy')) > cosine(vec('trousers'), vec('octopus'))","dcd8e7ee":"def spacy_closest(token_list, vec_to_check, n=10):\n    return sorted(token_list,\n                  key=lambda x: cosine(vec_to_check, vec(x)),\n                  reverse=True)[:n]","d0f6cb5b":"# what's the closest equivalent of basketball?\nspacy_closest(tokens, vec(\"basketball\"))","fedeee4c":"# halfway between day and night\nspacy_closest(tokens, meanv([vec(\"day\"), vec(\"night\")]))","5dc4a171":"spacy_closest(tokens, vec(\"wine\"))","2a274255":"spacy_closest(tokens, subtractv(vec(\"wine\"), vec(\"alcohol\")))","ab9dd680":"spacy_closest(tokens, vec(\"water\"))","cfe9c70b":"spacy_closest(tokens, addv(vec(\"water\"), vec(\"frozen\")))","b83b354a":"# analogy: blue is to sky as X is to grass\nblue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\nspacy_closest(tokens, addv(blue_to_sky, vec(\"grass\")))","9bdd0494":"def meanv(coords):\n    # assumes every item in coords has same length as item 0\n    sumv = [0] * len(coords[0])\n    for item in coords:\n        for i in range(len(item)):\n            sumv[i] += item[i]\n    mean = [0] * len(sumv)\n    for i in range(len(sumv)):\n        mean[i] = float(sumv[i]) \/ len(coords)\n    return mean\nmeanv([[0, 1], [2, 2], [4, 3]])","90e9c397":"def sentvec(s):\n    sent = nlp(s)\n    return meanv([nlp.vocab[word].vector for word in sent])","15c32e82":"nlp.vocab[\"my\"].vector","f1b10450":"from __future__ import unicode_literals, print_function\nfrom spacy.lang.en import English # updated\n\n# creating a custom pipeline for getting sentenses alone\n\nnlp = English()\nnlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\ndoc = nlp(open(\"\/kaggle\/input\/dracula-story-pg345\/pg345.txt\").read())\nsentences = [sent.string.strip() for sent in doc.sents]","30c4c590":"len(sentences)","03dc1b74":"def spacy_closest_sent(space, input_str, n=10):\n    input_vec = sentvec(input_str)\n    return sorted(space,\n                  key=lambda x: cosine(np.mean([w.vector for w in x], axis=0), input_vec),\n                  reverse=True)[:n]","9740584f":"#for sent in spacy_closest_sent(sentences, \"My favorite food is strawberry ice cream.\"):\n#    print (sent.text)\n#    print (\"---\")","cd1a9595":"1. The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`, thereby demonstrating that the vectors are working how we expect them to:","872d795f":"The closest words to \"water\":","8b053926":"The following function takes a list of sentences from a spaCy parse and compares them to an input sentence, sorting them by cosine similarity.","b9caabaa":"That's all well and good for color words, which intuitively seem to exist in a multidimensional continuum of perception, and for our animal space, where we've written out the vectors ahead of time. But what about... arbitrary words? Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property?\n\nTo answer that, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https:\/\/en.wikipedia.org\/wiki\/Distributional_semantics), which states that:\n\n    Linguistic items with similar distributions have similar meanings.\n    \nWhat's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n\n    It was really cold yesterday.\n    It will be really warm today, though.\n    It'll be really hot tomorrow!\n    Will it be really cool Tuesday?\n    \nAccording to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n\nIn other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts.","6fb5bf56":"#### GloVe vectors\n\nBut you don't have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https:\/\/nlp.stanford.edu\/projects\/glove\/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll be using for the rest of this tutorial.","f57ef047":"But if you add \"frozen\" to \"water,\" you get \"ice\":","e12ed50b":"## Distributional semantics\n\nIn the previous section, the examples are interesting because of a simple fact: colors that we think of as similar are \"closer\" to each other in RGB vector space. In our color vector space, or in our animal cuteness\/size space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing. They're also, for many purposes, *functionally identical*. Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it's probably also okay to show them results for, say,","1363556f":"### Word vectors in spaCy\n\nOkay, let's have some fun with real word vectors. We're going to use the GloVe vectors that come with spaCy to creatively analyze and manipulate the text of Bram Stoker's *Dracula*. First, make sure you've got `spacy` imported, which we already have. ","19aea903":"### Count Vectorization\nConsider a Corpus C of D documents {d1,d2\u2026..dD} and N unique tokens (Vocabulary) extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).[](http:\/\/)","5248a9f9":"This is a direct port of https:\/\/gist.github.com\/aparrish\/2f562e3737544cf29aaf1af30362f469","acf25006":"### Finding the closest item\n\nJust as we wanted to find the animal that most closely matched an arbitrary point in cuteness\/size space, we'll want to find the closest color name to an arbitrary point in RGB space. The easiest way to find the closest item to an arbitrary vector is simply to find the distance between the target vector and each item in the space, in turn, then sort the list from closest to farthest. The `closest()` function below does just that. By default, it returns a list of the ten closest items to the given vector.\n\n> Note: Calculating \"closest neighbors\" like this is fine for the examples in this notebook, but unmanageably slow for vector spaces of any appreciable size. As your vector space grows, you'll want to move to a faster solution, like SciPy's [kdtree](https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.spatial.KDTree.html) or [Annoy](https:\/\/pypi.python.org\/pypi\/annoy).","7f84f3f1":"To get the vector for a sentence, we simply average its component vectors, like so:","e270f6fb":"That's right, it's something like turquoise or cyan! What if we find the average of black and white? Predictably, we get gray:","171b52e4":"## Different types of Word Embeddings\n\nThe different types of word embeddings can be broadly classified into two categories-\n\nFrequency based Embedding\nPrediction based Embedding","1c9feeab":"For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:","0a7e464c":"... for, like, actual poets. By [Allison Parrish](http:\/\/www.decontextualize.com\/)\n\nIn this tutorial, I'm going to show you how word vectors work. This tutorial assumes a good amount of Python knowledge, but even if you're not a Python expert, you should be able to follow along and make small changes to the examples without too much trouble.\n\nThis is a \"[Jupyter Notebook](https:\/\/jupyter-notebook-beginner-guide.readthedocs.io\/en\/latest\/),\" which consists of text and \"cells\" of code. After you've loaded the notebook, you can execute the code in a cell by highlighting it and hitting Ctrl+Enter. In general, you need to execute the cells from top to bottom, but you can usually run a cell more than once without messing anything up. Experiment!\n\nIf things start acting strange, you can interrupt the Python process by selecting \"Kernel > Interrupt\"\u2014this tells Python to stop doing whatever it was doing. Select \"Kernel > Restart\" to clear all of your variables and start from scratch.\n\n## Why word vectors?\n\nPoetry is, at its core, the art of identifying and manipulating linguistic similarity. I have discovered a truly marvelous proof of this, which this notebook is too narrow to contain. (By which I mean: I will elaborate on this some other time)","9f3cbb73":"### Co-Occurrence Matrix \n\nLet\u2019s say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are-\n\n- A co-occurrence matrix of size V X V. Now, for even a decent corpus V gets very large and difficult to handle. So generally, this architecture is never preferred in practice.\n- A co-occurrence matrix of size V X N where N is a subset of V and can be obtained by removing irrelevant words like stopwords etc. for example. This is still very large and presents computational difficulties.\nBut, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.\n\nWe will be creating a co-occurance matrix of size V X V","925515c0":"Another example of color analogies: Navy is to blue as true green\/dark grass green is to green:","9802f46f":"Now there may be quite a few variations while preparing the above matrix M. The variations will be generally in-\n\n- The way dictionary is prepared.\nWhy? Because in real world applications we might have a corpus which contains millions of documents. And with millions of document, we can extract hundreds of millions of unique words. So basically, the matrix that will be prepared like above will be a very sparse one and inefficient for any computation. So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary.\n- The way count is taken for each word.\nWe may either take the frequency (number of times a word has appeared in the document) or the presence(has the word appeared in the document?) to be the entry in the count matrix M. But generally, frequency method is preferred over the latter.","fb051063":"The following cell defines a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector.","5378cc2a":"### Cosine similarity and finding closest neighbors\n\nThe cell below defines a function `cosine()`, which returns the [cosine similarity](https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity) of two vectors. Cosine similarity is another way of determining how similar two vectors are, which is more suited to high-dimensional spaces. [See the Encyclopedia of Distances for more information and even more ways of determining vector similarity.](http:\/\/www.uco.es\/users\/ma1fegan\/Comunes\/asignaturas\/vision\/Encyclopedia-of-distances-2009.pdf)\n\n(You'll need to install `numpy` to get this to work. If you haven't already: `pip install numpy`. Use `sudo` if you need to and make sure you've upgraded to the most recent version of `pip` with `sudo pip install --upgrade pip`.)","3c52b2e2":"Let's define the documents and the common cleanup and preprocessing first.","65041a8d":"Testing it out, we can find the ten colors closest to \"red\":","ddf282bd":"To calculate the average color, we'll follow these steps:\n\n1. Parse the text into words\n2. Check every word to see if it names a color in our vector space. If it does, add it to a list of vectors.\n3. Find the average of that list of vectors.\n4. Find the color(s) closest to that average vector.\n\nThe following cell performs steps 1-3:","4c74e713":"... or the ten colors closest to (150, 60, 150):","919ec676":"The examples above are fairly simple from a mathematical perspective but nevertheless *feel* magical: they're demonstrating that it's possible to use math to reason about how people use language.","cf4bc016":"# Understanding word vectors","9ff0344b":"#### Constructing Vocabulary or Tokens\n\nFor every algorithm this is a common pre-processing stage. Consists of 4 steps\n\n1. Normalize\n1. Tokenize\n1. Stopwords removal\n1. Stem\/Lemmatize\n\nWe do all this using some library like nltk","fb205c2a":"Just as a test, the following cell shows that the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\":","6fc2fea1":"Exercise for the reader: Use the vector arithmetic functions to rewrite a text, making it...\n\n* more blue (i.e., add `colors['blue']` to each occurrence of a color word); or\n* more light (i.e., add `colors['white']` to each occurrence of a color word); or\n* darker (i.e., attenuate each color. You might need to write a vector multiplication function to do this one right.)","046e64c9":"## Text Processing\n\nIn the world of Natural Language Processing, we construct Word Embeddings using different Word vector methodologies.\n\nNLP => Converting text into structured data.\nNLU => After getting structured data, understanding what the user input means.\nNLG => Converting structured data into text with some insights from NLU.\n\nNLU and NLG are subsets within NLP.","79611c92":"### Vector math\n\nBefore we keep going, we'll need some functions for performing basic vector \"arithmetic.\" These functions will work with vectors in spaces of any number of dimensions.\n\nThe first function returns the Euclidean distance between two points:","0c6fb6d8":"### Manual verification of Document Matrix","879eb064":"And the following cell creates a dictionary and populates it with mappings from color names to RGB vectors for each color in the data:","319140b6":"Let's find the sentence in our text file that is closest in \"meaning\" to an arbitrary input sentence. First, we'll get the list of sentences:","3e8c68df":"And the cell below creates a list of unique words (or tokens) in the text, as a list of strings.","cbcb93c1":"## Further resources\n\n* [Word2vec](https:\/\/en.wikipedia.org\/wiki\/Word2vec) is another procedure for producing word vectors which uses a predictive approach rather than a context-counting approach. [This paper](http:\/\/clic.cimec.unitn.it\/marco\/publications\/acl2014\/baroni-etal-countpredict-acl2014.pdf) compares and contrasts the two approaches. (Spoiler: it's kind of a wash.)\n* If you want to train your own word vectors on a particular corpus, the popular Python library [gensim](https:\/\/radimrehurek.com\/gensim\/) has an implementation of Word2Vec that is relatively easy to use. [There's a good tutorial here.](https:\/\/rare-technologies.com\/word2vec-tutorial\/)\n* When you're working with vector spaces with high dimensionality and millions of vectors, iterating through your entire space calculating cosine similarities can be a drag. I use [Annoy](https:\/\/pypi.python.org\/pypi\/annoy) to make these calculations faster, and you should consider using it too.","c64c9a7f":"### Fun with spaCy, Dracula, and vector arithmetic\n\nNow we can start doing vector arithmetic and finding the closest words to the resulting vectors. For example, what word is closest to the halfway point between day and night?","3a34655c":"The following function converts colors from hex format (`#1a2b3c`) to a tuple of integers:","5762dd70":"Testing it out:","c2232bae":"... is less than the distance between \"tarantula\" and \"elephant\":","2fca96ee":"Now, we'll pass the averaged color vector to the `closest()` function, yielding... well, it's just a brown mush, which is kinda what you'd expect from adding a bunch of colors together willy-nilly.","539ee2aa":"Modeling animals in this way has a few other interesting properties. For example, you can pick an arbitrary point in \"animal space\" and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken.\n\nReasoning visually, you can also answer questions like: what's halfway between a chicken and an elephant? Simply draw a line from \"elephant\" to \"chicken,\" mark off the midpoint and find the closest animal. (According to our chart, halfway between an elephant and a chicken is a horse.)\n\nYou can also ask: what's the *difference* between a hamster and a tarantula? According to our plot, it's about seventy five units of cute (and a few units of size).\n\nThe relationship of \"difference\" is an interesting one, because it allows us to reason about *analogous* relationships. In the chart below, I've drawn an arrow from \"tarantula\" to \"hamster\" (in blue):\n\n![Animal analogy](http:\/\/static.decontextualize.com\/snaps\/animal-space-analogy.png)\n\nYou can understand this arrow as being the *relationship* between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I've also transposed this same arrow (this time in red) so that its origin point is \"chicken.\" The arrow ends closest to \"kitten.\" What we've discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy:\n\n    Tarantulas are to hamsters as chickens are to kittens.\n    \nA sequence of numbers used to identify a point is called a *vector*, and the kind of math we've been doing so far is called *linear algebra.* (Linear algebra is surprisingly useful across many domains: It's the same kind of math you might do to, e.g., simulate the velocity and acceleration of a sprite in a video game.)\n\nA set of vectors that are all part of the same data set is often called a *vector space*. The vector space of animals in this section has two *dimensions*, by which I mean that each vector in the space has two numbers associated with it (i.e., two columns in the spreadsheet). The fact that this space has two dimensions just happens to make it easy to *visualize* the space by drawing a 2D plot. But most vector spaces you'll work with will have more than two dimensions\u2014sometimes many hundreds. In those cases, it's more difficult to visualize the \"space,\" but the math works pretty much the same.","cfec73af":"(The `**` operator raises the value on its left to the power on its right.)\n\nSo, the distance between \"capybara\" (70, 30) and \"panda\" (74, 40):","f7ac528c":"## Sentence similarity","30fe7938":"### Color magic\n\nThe magical part of representing words as vectors is that the vector operations we defined earlier appear to operate on language the same way they operate on numbers. For example, if we find the word closest to the vector resulting from subtracting \"red\" from \"purple,\" we get a series of \"blue\" colors:","bfbbadc0":"### TF-IDF\n\nA common issue we have with text analysis is that some words are much more frequent than others and aren\u2019t useful for classification. For example, words like \u201cthe\u201d, \u201cis\u201d, and \u201ca\u201d are common English words that don\u2019t convey much meaning. These words will differ from task to task depending on the domain of the text documents. If we are working with movie reviews, the word \u201cmovie\u201d will be frequent but not useful. If we were working with email data, on the other hand, the word \u201cmovie\u201d may not be frequent and would be useful.\nThe simplest way to account for these overrepresented words is to divide word count by the proportion of text documents each word appeared in. For example, the document:\n\n\u201cI loved this movie! It was great, great, great.\u201d\n\n\u2026contains the word \u201cloved\u201d and \u201cmovie\u201d once each. Now, let\u2019s suppose that we look at all the other documents and find that, in total, \u201cloved\u201d appears in 1% of text documents and \u201cmovie\u201d appears in 33%. We could now weight our scores as\n\n`\u201cloved\u201d = times it appears in text \/ proportion of texts it appears in = 1 \/ 1%`\n`\u201cmovie\u201d = times it appears in text \/ proportion of texts it appears in = 1 \/ 33%`\n\nBefore applying weights, both \u201cloved\u201d and \u201cmovie\u201d had a score of 1 (since each word appeared in the sentence once). After we apply weights, \u201cloved\u201d has a score of 100 and \u201cmovie\u201d has a score of 3. The score for \u201cloved\u201d is much higher relative to \u201cmovie\u201d, indicating that we care about the word \u201cloved\u201d much more than \u201cmovie\u201d.\nIn fact, our score for \u201cloved\u201d is now 33 times larger than our score for \u201cmovie\u201d. While we suspect that \u201cmovie\u201d should be less important than \u201cloved\u201d for predicting whether a review is positive or negative, this relative difference might be too big. Very rare words \u2014 perhaps, misspelled words \u2014 will receive too much relative weight in our current weighting scheme.\n\nWe need to strike a balance between downweighting very frequent words without overweighting rare words. This is what term frequency\u2013inverse document frequency (tf-idf) weighting does for us. In the simple weighting scheme, we used the formula:\n\n**times a word appears in text * (1 \/ proportion of texts it appears in)**\n\ntf-idf weighting alters this formula slightly by taking the log of the second term:\n\n**times a word appears in text * log(1 \/ proportion of texts it appears in)**\n\nBy taking the log, we ensure that our weight changes slowly in relation to how frequently a word appears in all our documents. This means that while common words are downweighted, they aren\u2019t downweighted too much. (There\u2019s also a connection to information theory, too).","0812f814":"This matches our intuition about RGB colors, which is that purple is a combination of red and blue. Take away the red, and blue is all you have left.\n\nYou can do something similar with addition. What's blue plus green?","2c614a3c":"### Doing bad digital humanities with color vectors\n\nWith the tools above in hand, we can start using our vectorized knowledge of language toward academic ends. In the following example, I'm going to calculate the average color of Bram Stoker's *Dracula*.\n\n(Before you proceed, make sure to [download the text file from Project Gutenberg](http:\/\/www.gutenberg.org\/cache\/epub\/345\/pg345.txt) and place it in the same directory as this notebook.)\n\nFirst, we'll load [spaCy](https:\/\/spacy.io\/):","fcf1d8c0":"For Python code and detailed example:\nhttps:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089","9be0cee9":"Here are the sentences in *Dracula* closest in meaning to \"My favorite food is strawberry ice cream.\" (Extra linebreaks are present because we didn't strip them out when we originally read in the source text.)","2f20a228":"Variations of `night` and `day` are still closest, but after that we get words like `evening` and `morning`, which are indeed halfway between day and night!","d3932753":"If you subtract \"alcohol\" from \"wine\" and find the closest words to the resulting vector, you're left with simply a lovely dinner:","566636c4":"## Animal similarity and simple linear algebra\n\nWe'll begin by considering a small subset of English: words for animals. Our task is to be able to write computer programs to find similarities among these words and the creatures they designate. To do this, we might start by making a spreadsheet of some animals and their characteristics. For example:\n\n![Animal spreadsheet](http:\/\/static.decontextualize.com\/snaps\/animal-spreadsheet.png)\n\nThis spreadsheet associates a handful of animals with two numbers: their cuteness and their size, both in a range from zero to one hundred. (The values themselves are simply based on my own judgment. Your taste in cuteness and evaluation of size may differ significantly from mine. As with all data, these data are simply a mirror reflection of the person who collected them.)\n\nThese values give us everything we need to make determinations about which animals are similar (at least, similar in the properties that we've included in the data). Try to answer the following question: Which animal is most similar to a capybara? You could go through the values one by one and do the math to make that evaluation, but visualizing the data as points in 2-dimensional space makes finding the answer very intuitive:\n\n![Animal space](http:\/\/static.decontextualize.com\/snaps\/animal-space.png)\n\nThe plot shows us that the closest animal to the capybara is the panda bear (again, in terms of its subjective size and cuteness). One way of calculating how \"far apart\" two points are is to find their *Euclidean distance*. (This is simply the length of the line that connects the two points.) For points in two dimensions, Euclidean distance can be calculated with the following Python function:","61784e65":"### 2.1 Frequency based Embedding\n\nThere are generally three types of vectors that we encounter under this category.\n\n1. Count Vector\n1. TF-IDF Vector\n1. 1. Co-Occurrence Vector","ae23b0f8":"The `addv` vector adds two vectors together:","bba45439":"And the `meanv` function takes a list of vectors and finds their mean or average:","58b4b8f1":"Here are the closest words in _Dracula_ to \"wine\":","edd45c15":"The `subtractv` function subtracts one vector from another:","7295cb78":"## Language with vectors: colors\n\nSo far, so good. We have a system in place\u2014albeit highly subjective\u2014for talking about animals and the words used to name them. I want to talk about another vector space that has to do with language: the vector space of colors.\n\nColors are often represented in computers as vectors with three dimensions: red, green, and blue. Just as with the animals in the previous section, we can use these vectors to answer questions like: which colors are similar? What's the most likely color name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colors, what's the name of those colors' \"average\"?\n\nWe'll be working with this [color data](https:\/\/github.com\/dariusk\/corpora\/blob\/master\/data\/colors\/xkcd.json) from the [xkcd color survey](https:\/\/blog.xkcd.com\/2010\/05\/03\/color-survey-results\/). The data relates a color name to the RGB value associated with that color. [Here's a page that shows what the colors look like](https:\/\/xkcd.com\/color\/rgb\/). Download the color data and put it in the same directory as this notebook.\n\nA few notes before we proceed:\n\n* The linear algebra functions implemented below (`addv`, `meanv`, etc.) are slow, potentially inaccurate, and shouldn't be used for \"real\" code\u2014I wrote them so beginner programmers can understand how these kinds of functions work behind the scenes. Use [numpy](http:\/\/www.numpy.org\/) for fast and accurate math in Python.\n* If you're interested in perceptually accurate color math in Python, consider using the [colormath library](http:\/\/python-colormath.readthedocs.io\/en\/latest\/).\n\nNow, import the `json` library and load the color data:","056edf42":"Using this function, we can get a list of synonyms, or words closest in meaning (or distribution, depending on how you look at it), to any arbitrary word in spaCy's vocabulary. In the following example, we're finding the words in *Dracula* closest to \"basketball\":","87dfba77":"If you take the difference of \"blue\" and \"sky\" and add it to grass, you get the analogous word (\"green\"):","f2849f6b":"Just as with the tarantula\/hamster example from the previous section, we can use color vectors to reason about relationships between colors. In the cell below, finding the difference between \"pink\" and \"red\" then adding it to \"blue\" seems to give us a list of colors that are to blue what pink is to red (i.e., a slightly lighter, less saturated shade):","607a097c":"You can see the vector of any word in spaCy's vocabulary using the `vocab` attribute, like so:","b5274cf9":"### 2.2 Prediction based Embedding","a55d44cd":"### Interlude: A Love Poem That Loses Its Way"}}