{"cell_type":{"8c1153e4":"code","0328cf18":"code","9b04cfd8":"code","4f94b150":"code","545e0692":"code","3935320d":"code","8bb2e0f7":"code","8c63ca3e":"code","befa5b5c":"code","738a365d":"code","d68d06e0":"code","96741be3":"code","0cd8e3a4":"code","8fbc5763":"code","4cdd7303":"code","f899fd7a":"code","8deadf18":"code","15022085":"code","6d928726":"code","ee0f4a68":"code","dec9e54e":"code","b70c5ea6":"code","fd58479a":"code","74922b99":"code","bb51cd4c":"code","fe0256da":"markdown","8d84d93f":"markdown","9bb948f6":"markdown"},"source":{"8c1153e4":"import os\nimport numpy as np\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nSOURCE_DIR = '\/kaggle\/input\/shopee-product-matching'\nTRAIN_IMAGE = 'train_images'\nOPENCV_EAST = '\/kaggle\/input\/opencv-east\/frozen_east_text_detection.pb'\nOUTPUT_DIR = '\/kaggle\/working'\n\nIMG_DIR = ''\nfor dirname, _, filenames in os.walk(os.path.join(SOURCE_DIR, TRAIN_IMAGE)):\n    for filename in filenames:\n        IMG_DIR = dirname\n        break","0328cf18":"train_df = pd.read_csv(os.path.join(SOURCE_DIR, 'train.csv'))\ntrain_df.head(5)","9b04cfd8":"train_df.info(memory_usage='deep')","4f94b150":"#Lets take a sample dataframe before execution\nfrom PIL import Image\n#PIL Image.open is lazy function and does not load image into memory\ndef get_ht_wd(filename, file_path=IMG_DIR):\n    H, W = None, None\n    try:\n        img = Image.open(os.path.join(file_path, filename))\n        W, H = img.size\n    except Exception as e:\n        print('unable to read file {}'.format(file_path))\n    return W,H\n\ntrain_df['dimension'] = train_df['image'].apply(lambda x: get_ht_wd(x))\ntrain_df[['width', 'height']] = pd.DataFrame(train_df['dimension'].tolist(), index=train_df.index)\ntrain_df.head(2)","545e0692":"fig, ax = plt.subplots(figsize=(10,10))\nax.scatter(x='width', y='height', data=train_df)\nax.set_xlabel('width')\nax.set_ylabel('height')\nplt.show()","3935320d":"grouped_df = train_df.groupby('label_group', as_index = False).agg({'image': ','.join, 'title': '|'.join})\ngrouped_df['duplicate_count'] = grouped_df['image'].str.count(',') + 1\ngrouped_df.head(5)","8bb2e0f7":"fig, ax = plt.subplots(figsize=(10,10))\nax.hist(grouped_df['duplicate_count'])\nplt.show()\n#below plot analysis reveal most of the images are repeated 5 times in given database","8c63ca3e":"pip install google_trans_new","befa5b5c":"#Sample Analysis\nfrom google_trans_new import google_translator\nimport traceback\n#translate title indonesian to english\ntranslator = google_translator()\ndef translate_to_english(txt):\n    eng_txt = ''\n    try:\n        eng_txt = translator.translate(txt)\n        eng_txt = '|'.join([_.strip() for _ in eng_txt.split('|')])  # trim spaces\n    except Exception as e:\n        print(traceback.print_exc())\n        #print(txt)\n    finally:\n        return eng_txt\nsample_grp_labels = grouped_df.iloc[0:5]['label_group'].values\nsample_df = train_df[train_df['label_group'].isin(sample_grp_labels)].copy()\nsample_df.reset_index(inplace=True)\nsample_df['title_eng'] = sample_df['title'].apply(translate_to_english)\nsample_df['title_eng']","738a365d":"#covert images to 320 X 320 for processing\nimport shutil\nimport cv2\nMAX_HEIGHT = 320\nMAX_WIDTH = 320\n#shutil.rmtree(os.path.join(OUTPUT_DIR, TRAIN_IMAGE))\nos.mkdir(os.path.join(OUTPUT_DIR, TRAIN_IMAGE))\ndef resize_img(file_name,\n               file_path=os.path.join(SOURCE_DIR, TRAIN_IMAGE),\n               output_path=os.path.join(OUTPUT_DIR, TRAIN_IMAGE)):\n\n    im = cv2.imread(os.path.join(file_path, file_name))\n    ht, w, channels = im.shape\n    if ht < MAX_HEIGHT & w < MAX_WIDTH:\n        im = cv2.resize(im, (round(MAX_WIDTH \/ w), round(MAX_HEIGHT \/ ht)), interpolation=cv2.INTER_AREA)\n    else:\n        im = cv2.resize(im, (MAX_WIDTH, MAX_HEIGHT), interpolation=cv2.INTER_CUBIC)\n    cv2.imwrite(os.path.join(output_path, file_name), im)\n\nsample_df['image'].apply(resize_img)\n","d68d06e0":"from matplotlib.colors import rgb_to_hsv\nclass ImageAnalysis():\n    def __init__(self, file_path):\n        self.display_cols = 3\n        self.file_path = file_path\n        \n    def get_RGB_HSV_GRAY(self, img):\n        image = cv2.imread(os.path.join(self.file_path, img))\n        orig = image.copy()\n        #opencv read image in BGR and matplotlib reads in RGB format\n        morig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n        # Convert BGR to HSV\n        mhsv = rgb_to_hsv(morig)\n        # Convert BGR to black\/white\n        gray = cv2.cvtColor(orig, cv2.COLOR_BGR2GRAY)\n        return morig, mhsv, gray\n\n    def plot_color_change(self, img, txt):\n        orig, hsv, gray = self.get_RGB_HSV_GRAY(img)\n\n        fig = plt.figure(figsize=(15, 15))\n        print(txt)\n        fig.add_subplot(1, self.display_cols, 1)\n        plt.imshow(orig)\n        plt.axis('off')\n        plt.title('origin')\n\n        fig.add_subplot(1, self.display_cols, 2)\n        plt.imshow(hsv)\n        plt.axis('off')\n        plt.title('hsv')\n\n        fig.add_subplot(1, self.display_cols, 3)\n        plt.imshow(gray, cmap='gray')\n        plt.axis('off')\n        plt.title('gray')\n        plt.show()\n\n            \nRESIZED_DIR = os.path.join(OUTPUT_DIR, TRAIN_IMAGE)\n\nia = ImageAnalysis(RESIZED_DIR)\nmask = (sample_df['label_group'] == sample_grp_labels[0])\nsample_df[mask].apply(\n    lambda x: ia.plot_color_change(x['image'], x['title']), axis=1)\n","96741be3":"# Approach\n# There are three steps  :\n# Find similarity in title text\n# Find similarity in Image\n# Combine similarity of title text and image and predict\n\n# 1) Find Similarity in title text\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tokenize import word_tokenize\nfrom gensim import corpora\nimport gensim\nimport re\nfrom gensim.parsing.preprocessing import remove_stopwords\n\nclass TextActions:\n    def __init__(self):\n        pass\n    @staticmethod\n    def lemmatize(words):\n        lemmas = set()\n        for word in words:\n            lemma = wn.morphy(word)\n            if lemma is None:\n                lemmas.add(word)\n            else:\n                lemmas.add(lemma)\n        return list(lemmas)\n\n    @staticmethod\n    def clean_sentence(sentence):\n        # make all text lowercase\n        sentence = sentence.lower()\n        # removing everything except alphabets`\n        sentence = re.sub('[^a-zA-Z]+', ' ', sentence)\n        sentence = remove_stopwords(sentence)\n\n        # removing short words\n        sentence = ' '.join([w for w in sentence.split() if len(w)>3])\n        return sentence\n\n    @staticmethod\n    def tokenize(sentence):\n        return word_tokenize(sentence)\n    \n    @staticmethod\n    def remove_duplicate(lst):\n        return list(set(lst))\n    \n    @staticmethod\n    def spell_checker(corpus, words):\n        pass #TODO:symspell\n\n    @staticmethod\n    def fetch_req_words(sentence):\n        sentence = TextActions.clean_sentence(sentence)\n        words = TextActions.tokenize(sentence)\n        words = TextActions.lemmatize(words)\n        return words\n\nta = TextActions()\n\nsample_df['word_tokens'] = sample_df['title_eng'].apply(ta.fetch_req_words)\n\nsample_df['word_tokens'].head(5)","0cd8e3a4":"import itertools\nclass TitleAnalysis:\n    def __init__(self):\n        self.dictionary = None\n    def create_dictionary(self, tokens):\n        self.dictionary = list(set(itertools.chain.from_iterable(tokens)))\n        #print(self.dictionary)\n    def plot_titlewise_words(self, tokens, titles):\n        fig, ax = plt.subplots(figsize=(15,10))\n        for title_no, words in enumerate(tokens, 1):\n            #consider no words match dictonary\n            x , y = words, [title_no] * len(words)\n            #print(x, y)\n            ax.scatter(x, y, marker='*')\n        ax.set_xticks(self.dictionary)\n        ax.set_xticklabels(self.dictionary, rotation =70, fontsize=16)\n        ax.set_xlabel('token', fontsize=16)\n        ax.set_yticks([1,2,3])\n        ax.set_yticklabels(titles, rotation =45, fontsize=16)\n        ax.set_ylabel('title', fontsize=16)\n        #plt.grid(True)\n        plt.show()\n        \n\nta = TitleAnalysis()\n#print(sample_df[mask]['word_tokens'].to_list())\nta.create_dictionary(sample_df[mask]['word_tokens'].to_list())\nta.plot_titlewise_words(sample_df[mask]['word_tokens'].to_list(),sample_df[mask]['title_eng'].to_list())","8fbc5763":"from gensim.models import Word2Vec\nMODEL_DIR = 'model'\nw2v_model = 'product_word2vec.model'\n#shutil.rmtree(os.path.join(OUTPUT_DIR, MODEL_DIR))\nos.mkdir(os.path.join(OUTPUT_DIR, MODEL_DIR))\n\ncommon_texts = sample_df['word_tokens'].to_list()\nmodel = Word2Vec(sentences=common_texts, size=100,\n                 window=5, min_count=1, workers=4, min_alpha=0.0007)\n\nmodel.save(os.path.join(*[OUTPUT_DIR, MODEL_DIR, w2v_model]))\n","4cdd7303":"from gensim.models import KeyedVectors\nmodel = KeyedVectors.load(os.path.join(*[OUTPUT_DIR, MODEL_DIR, w2v_model]))\n'|'.join(model.wv.vocab)","f899fd7a":"from sklearn.decomposition import PCA\nX = model[model.wv.vocab]\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\nplt.scatter(result[:, 0], result[:, 1])\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0], result[i, 1]))","8deadf18":"#using word movers distance to calc similarity between title\n#ON same products\nfig, ax = plt.subplots(figsize=(16, 16))\n\nsample_df = sample_df.sort_values(by='label_group')\nticks = (sample_df['label_group'].astype(str) + ',' + sample_df['title_eng'].str.slice(0, 10) + '..').to_list()\nfor record_idx_1 in sample_df.index:\n    for record_idx_2 in sample_df.index:\n        sent1 = sample_df.loc[record_idx_1, 'title_eng']\n        sent2 = sample_df.loc[record_idx_2, 'title_eng']\n        token1 = sample_df.loc[record_idx_1, 'word_tokens']\n        token2 = sample_df.loc[record_idx_2, 'word_tokens']\n        #print(model.wv.wmdistance(token1, token2))\n        mv_distance = round(model.wv.wmdistance(token1, token2), 3)\n        x = record_idx_1\n        y = record_idx_2\n        ax.plot(x, y)\n        c = 'green' # is similar\n        if mv_distance < 0.75 and mv_distance > 0.50:\n            c = 'blue' # can be similar\n        elif mv_distance >= 0.75:\n            c = 'white' # no similarity\n        ax.annotate(mv_distance, (x,y), bbox=dict(facecolor=c, pad=5))\nax.set_xticks(range(len(ticks)))\nax.set_xticklabels(ticks, rotation =90, fontsize=14)\nax.set_yticks(range(len(ticks)))\nax.set_yticklabels(ticks, fontsize=14)\n#plt.grid(True)\nplt.show()\n#smaller the distance means sentences are similar","15022085":"#using cosine similarity to calc similarity between title\nfig, ax = plt.subplots(figsize=(16, 16))\n\nsample_df = sample_df.sort_values(by='label_group')\nticks = (sample_df['label_group'].astype(str) + ',' + sample_df['title_eng'].str.slice(0, 10) + '..').to_list()\nfor record_idx_1 in sample_df.index:\n    for record_idx_2 in sample_df.index:\n        sent1 = sample_df.loc[record_idx_1, 'title_eng']\n        sent2 = sample_df.loc[record_idx_2, 'title_eng']\n        token1 = sample_df.loc[record_idx_1, 'word_tokens']\n        token2 = sample_df.loc[record_idx_2, 'word_tokens']\n        #print(model.wv.wmdistance(token1, token2))\n        cosine_similarity = round(model.wv.n_similarity(token1, token2), 3)\n        x = record_idx_1\n        y = record_idx_2\n        ax.plot(x, y)\n        c = 'white'\n        if cosine_similarity < 0.75 and mv_distance > 0.50:\n            c = 'blue'\n        elif cosine_similarity >= 0.75:\n            c = 'green'\n        ax.annotate(cosine_similarity, (x,y), bbox=dict(facecolor=c, pad=5))\nax.set_xticks(range(len(ticks)))\nax.set_xticklabels(ticks, rotation =90, fontsize=14)\nax.set_yticks(range(len(ticks)))\nax.set_yticklabels(ticks, fontsize=14)\n#plt.grid(True)\nplt.show()","6d928726":"#\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nd2v_model = 'product_doc2vec.model'\n#shutil.rmtree(os.path.join(OUTPUT_DIR, MODEL_DIR))\n#os.mkdir(os.path.join(OUTPUT_DIR, MODEL_DIR))\n\ncommon_texts = sample_df['word_tokens'].to_list()\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\nmodel = Doc2Vec(documents=documents, vector_size=100,\n                 window=5, min_count=1, workers=4, min_alpha=0.0007)\n\nmodel.save(os.path.join(*[OUTPUT_DIR, MODEL_DIR, d2v_model]))\nmodel = Doc2Vec.load(os.path.join(*[OUTPUT_DIR, MODEL_DIR, d2v_model]))","ee0f4a68":"test = sample_df.loc[0,'word_tokens']\n\nnew_vector = model.infer_vector(test)\nsimilar = model.docvecs.most_similar([new_vector])\nprint(similar)\n\n","dec9e54e":"#using Levenshtein distance\n#Levenshtein distance between two words is the minimum number of single-character edits\n#(insertions, deletions or substitutions) required to change one word into the other\nfrom fuzzywuzzy import fuzz\n        \ndef compare_title(title1, title2):\n    c1 = fuzz.ratio(title1, title2)\n    c2 = fuzz.partial_ratio(title1, title2)\n    c3 = fuzz.token_set_ratio(title1, title2)\n    return c1, c2, c3\n\nimport matplotlib.colors as mcolors\nbounds=[0, 50, 75, 101]\ncols = ['white', 'blue', 'green']\ncmap, norm = mcolors.from_levels_and_colors(bounds, cols)\n\nfig, ax = plt.subplots(figsize=(16, 16))\n\nsample_df = sample_df.sort_values(by='label_group')\nticks = (sample_df['label_group'].astype(str) + ',' + sample_df['title_eng'].str.slice(0, 10) + '..').to_list()\n\ntitle_match = []\nfor record_idx_1 in sample_df.index:\n    match = []\n    for record_idx_2 in sample_df.index:\n        title1 = sample_df.loc[record_idx_1, 'title_eng']\n        title2 = sample_df.loc[record_idx_2, 'title_eng']\n        c1, c2, c3 = compare_title(title1, title2)\n        match.append(c2)\n        x = record_idx_1\n        y = record_idx_2\n        c = 'white'\n        if c2 < 75 and c2 > 50:\n            c = 'blue'\n        elif c2 >= 75:\n            c = 'green'\n        ax.plot(x, y)\n        ax.annotate(c2, (x,y), bbox=dict(facecolor=c, pad=5))\n    title_match.append(match)\n\nax.set_xticks(range(len(ticks)))\nax.set_xticklabels(ticks, rotation =90, fontsize=14)\nax.set_yticks(range(len(ticks)))\nax.set_yticklabels(ticks, fontsize=14)\nax.set_title('Levenshtein distance between titles')\nplt.show()","b70c5ea6":"#Lets topic model to see if we can segregate titles into groups\ndictionary = corpora.Dictionary(sample_df.loc[:,'word_tokens'])\ndictionary","fd58479a":"corpus = [dictionary.doc2bow(text) for text in sample_df.loc[:,'word_tokens']]\nNUM_TOPICS = 10\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","74922b99":"import pyLDAvis.gensim\npd.options.display.max_colwidth = 5000\nlda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary,  mds='tsne')\npyLDAvis.enable_notebook()\npyLDAvis.display(lda_display)","bb51cd4c":"text = sample_df['word_tokens'][17]\nprint(text)\nnew_doc_bow = [dictionary.doc2bow(text)]\nprint(ldamodel.get_document_topics(new_doc_bow[0]))\n#topic 4\ntopics[4]","fe0256da":"**#conclusion wmd , cosine similarity on wordvec or docvec dont work **","8d84d93f":"**ANALYSIS of given Image height and width as big images use high memory**","9bb948f6":"**From above scatter plot we can conclude that images are square shaped i.e height=width**"}}