{"cell_type":{"09c58312":"code","60f70d04":"code","7439902d":"code","848e72ce":"code","4db84fba":"code","aa8eae13":"code","6030dc25":"code","459be839":"code","d7c5addf":"code","d6ba9a56":"code","6f103d34":"code","04839189":"code","dafdeb87":"code","5a49b399":"code","dfd779f3":"code","d51442dc":"markdown","e82809e7":"markdown","6ace202d":"markdown","24db253d":"markdown","f4416d03":"markdown","d9d6fd44":"markdown","27b04cf8":"markdown","c2355a69":"markdown","e9ba58c1":"markdown","1cf86772":"markdown","f21e6cfe":"markdown","2398e46a":"markdown"},"source":{"09c58312":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import np_utils\n\nimport os\nprint(os.listdir(\"..\/input\"))","60f70d04":"iris=pd.read_csv('..\/input\/Iris.csv')\niris.head()","7439902d":"sns.pairplot(iris.iloc[:,1:6],hue='Species')","848e72ce":"sns.countplot('Species',data=iris)","4db84fba":"# Create feature and target arrays\nX = iris.iloc[:,1:5]\ny = iris.iloc[:,-1]\n# Label encode Class (Species)\nencoder = preprocessing.LabelEncoder()\nencoder.fit(y)\nencoded_y = encoder.transform(y)\n# One Hot Encode\ny_dummy = np_utils.to_categorical(encoded_y)\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y_dummy, test_size = 0.2, random_state=123, stratify=y)\nprint('X_train:', X_train.shape)\nprint('X_test:', X_test.shape)","aa8eae13":"# Imports\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import SGD\n\n# Building the model\nmodel = Sequential()\nmodel.add(Dense(8, input_dim=4, activation='sigmoid'))\nmodel.add(Dense(6, activation='sigmoid'))\nmodel.add(Dense(3, activation='sigmoid'))\n\n# Compiling the model\nmodel.compile(loss = 'categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()","6030dc25":"# training the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=30, verbose=0)","459be839":"# Evaluating the model on the training and testing set\nscore = model.evaluate(X_train, y_train)\nprint(\"\\n Training Accuracy:\", score[1])","d7c5addf":"# use scikit-learn to grid search the batch size and epochs\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier","d6ba9a56":"# function to create model\ndef create_model():\n    model = Sequential()\n    model.add(Dense(8, input_dim=4, activation='sigmoid'))\n    model.add(Dense(6, activation='sigmoid'))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n\n# define the parameters to search in grid search \nbatch_size = [10, 50, 100]\nepochs = [10, 50, 100]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","6f103d34":"# function to create model\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(8, input_dim=4, activation='sigmoid'))\n    model.add(Dense(6, activation='sigmoid'))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n\n# define the parameters to search in grid search \noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","04839189":"# function to create model\ndef create_model(learn_rate=0.01, momentum=0):\n    model = Sequential()\n    model.add(Dense(8, input_dim=4, activation='sigmoid'))\n    model.add(Dense(6, activation='sigmoid'))\n    model.add(Dense(3, activation='sigmoid'))\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\n    model.compile(loss = 'categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n\n# define the parameters to search in grid search \nlearn_rate = [0.01, 0.1, 0.2]\nmomentum = [0.2, 0.6, 0.9]\nparam_grid = dict(learn_rate=learn_rate, momentum=momentum)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","dafdeb87":"# function to create model\ndef create_model(activation='relu'):\n    model = Sequential()\n    model.add(Dense(8, input_dim=4, activation=activation))\n    model.add(Dense(6, activation=activation))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n\n# define the parameters to search in grid search \nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\nparam_grid = dict(activation=activation)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","5a49b399":"from keras.layers import Dropout\nfrom keras.constraints import maxnorm\n\n# function to create model\ndef create_model(dropout_rate=0.0, weight_constraint=0):\n    model = Sequential()\n    model.add(Dense(8, input_dim=4, activation='tanh', kernel_constraint=maxnorm(weight_constraint)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(6, activation='tanh'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n\n# define the parameters to search in grid search \nweight_constraint = [0, 1, 2]\ndropout_rate = [0.0, 0.1, 0.2]\nparam_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","dfd779f3":"# function to create model\ndef create_model(neurons=1):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=4, activation='tanh', kernel_constraint=maxnorm(1)))\n    model.add(Dropout(0.1))\n    model.add(Dense(6, activation='tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(3, activation='sigmoid'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n\n# define the parameters to search in grid search \nneurons = [1, 5, 10, 15, 20, 25, 30]\nparam_grid = dict(neurons=neurons)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\ngrid_result = grid.fit(X_train, y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","d51442dc":"## Modeling\nThe idea is to train a DNN with different hyper-parameters in order to choose which one is better for the dataset. ","e82809e7":"### Neurons in the Hidden Layer","6ace202d":"> It looks like the **DNN** work much better now from **.33 to .98.**","24db253d":"## Exploratory Data Analysis\n\n[Basic Exploratory Data Analysis](https:\/\/www.kaggle.com\/camiloemartinez\/lucky-charms-lovers) to understand a little bit about the dataset. ","f4416d03":"### Learning Rate and Momentum","d9d6fd44":"### Training Optimization Algorithm","27b04cf8":"## Objective\n\nUsing **deep learning [keras]**  the idea is to understand how to tune a neural network hyper-parameters in order to achive a better model.","c2355a69":"### Batch Size and Number of Epochs","e9ba58c1":"**0.33** is the initial **Accuracy** measure, lets see how to increase that figure.","1cf86772":"## Hyperparameters for Deep Learning\n> Plan: Find the most promising model little by little and continue performing some tuning in order to find the best model possible.","f21e6cfe":"### Neuron Activation Function","2398e46a":"### Dropout Regularization"}}