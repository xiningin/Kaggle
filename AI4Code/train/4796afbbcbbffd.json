{"cell_type":{"9c355244":"code","3ed407ac":"code","256ba0b6":"code","ec02605f":"code","e702c1b9":"code","771dfc9f":"code","9b2c4b37":"code","5e3db40a":"code","bcbf0a78":"markdown","1f8f86b5":"markdown","f3ed4a00":"markdown","28f4f621":"markdown","0d774e03":"markdown","ceabb42a":"markdown","059b82ca":"markdown","3b245e5b":"markdown","c524424b":"markdown"},"source":{"9c355244":"import numpy as np\nfrom random import random\nfrom numpy import cumsum\nfrom numpy import array_equal\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Bidirectional","3ed407ac":"# create a cumulative sum sequence\ndef get_sequence(n_timesteps):\n    # create a sequence of random numbers in [0,1]\n    X = np.array([random() for _ in range(n_timesteps)])\n    # calculate cut-off value to change class values\n    limit = n_timesteps\/4.0\n    # determine the class outcome for each item in cumulative sequence\n    y = np.array([0 if x < limit else 1 for x in cumsum(X)])\n    return X, y\n\n\n# create multiple samples of cumulative sum sequences\ndef get_sequences(n_sequences, n_timesteps):\n    seqX, seqY = list(), list()\n    # create and store sequences\n    for _ in range(n_sequences):\n        X, y = get_sequence(n_timesteps)\n        seqX.append(X)\n        seqY.append(y)\n    # reshape input and output for lstm\n    seqX = np.array(seqX).reshape(n_sequences, n_timesteps, 1)\n    seqY = np.array(seqY).reshape(n_sequences, n_timesteps, 1)\n    return seqX, seqY\n\nX, y = get_sequence(10)\nprint(X)\nprint(y)","256ba0b6":"n_timesteps = 10\n\n\n# define LSTM\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(50, return_sequences = True), input_shape=(n_timesteps, 1)))\nmodel.add(TimeDistributed(Dense(1, activation='sigmoid')))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","ec02605f":"# train LSTM\nX, y = get_sequences(50000, n_timesteps)\nX_test, y_test = get_sequences(5000, n_timesteps)\nmodel.fit(X, y, epochs=1, batch_size=10, validation_data=(X_test, y_test))","e702c1b9":"# make predictions\nfor _ in range(10):\n    X1, y1 = get_sequences(1, n_timesteps)\n    yhat = model.predict_classes(X1, verbose=0)\n    exp, pred = y1.reshape(n_timesteps), yhat.reshape(n_timesteps) \n    print('y=%s, yhat=%s, correct=%s' % (exp, pred, array_equal(exp,pred)))","771dfc9f":"# define Forward LSTM\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences = True, input_shape=(n_timesteps, 1)))\nmodel.add(TimeDistributed(Dense(1, activation='sigmoid')))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(X, y, epochs=1, batch_size=10, validation_data=(X_test, y_test))","9b2c4b37":"# make predictions\nfor _ in range(10):\n    X1, y1 = get_sequences(1, n_timesteps)\n    yhat = model.predict_classes(X1, verbose=0)\n    exp, pred = y1.reshape(n_timesteps), yhat.reshape(n_timesteps) \n    print('y=%s, yhat=%s, correct=%s' % (exp, pred, array_equal(exp,pred)))","5e3db40a":"# define Backward LSTM\nmodel = Sequential()\nmodel.add(LSTM(50, return_sequences = True, input_shape=(n_timesteps, 1), go_backwards=True))\nmodel.add(TimeDistributed(Dense(1, activation='sigmoid')))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(X, y, epochs=1, batch_size=10, validation_data=(X_test, y_test))","bcbf0a78":"<h3>The Bidirectional LSTM<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.9px;\">\nBidirectional LSTMs focus on the problem of getting the most out of the input sequence by stepping through input time steps in both the forward and backward directions.<br> In practice, this architecture involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second. <br>This approach was developed some time ago as a general approach for improving the performance of Recurrent Neural Networks (RNNs).\n<\/div>\n\n![image.png](attachment:image.png)\n\n<div style=\"font-family:verdana; word-spacing:1.9px;\">\nBidirectional LSTMs are a small step on top of this capability.<br> Specifically, Bidirectional LSTMs are supported in Keras via the Bidirectional layer wrapper that essentially merges the output from two parallel LSTMs, one with input processed forward and one with output processed backwards.<br><br>\n    The Bidirectional wrapper layer also allows you to specify the merge mode; that is how the forward and backward outputs should be combined before being passed on to the next layer. The options are:\n    <ul>\n        <li>\u2018sum\u2019: The outputs are added together.\n            <li>\u2018mul\u2019: The outputs are multiplied together.\n            <li>\u2018concat\u2019: The outputs are concatenated together (the default), providing double the\nnumber of outputs to the next layer.\n            <li>\u2018ave\u2019: The average of the outputs is taken.\n    <\/ul>\n    \n The default mode is to concatenate, and this is the method often used in studies of bidirectional LSTMs. <br>In general, it might be a good idea to test each of the merge modes on your problem to see if you can improve upon the concatenate default option.\n<\/div>","1f8f86b5":"<center><h3>Modelling<\/h3>","f3ed4a00":"<center><h2>Cumulative Sum Prediction Problem<\/h2><\/center>\n<br>\n<div style=\"font-family:verdana; word-spacing:1.9px;\">\nThe problem is defined as a sequence of random values between 0 and 1. This sequence is taken as input for the problem with each number provided once per time step. <br>A binary label (0 or 1) is associated with each input. The output values are all 0. Once the cumulative sum of the input values in the sequence exceeds a threshold, then the output value flips from 0 to 1.<br><br>\nA threshold of one quarter (1\/4) the sequence length is used. For example, below is a sequence\nof 10 input time steps (X):<br><br>\n    <blockquote>0.63144003 0.29414551 0.91587952 0.95189228 0.32195638 0.60742236 0.83895793 0.18023048\n    0.84762691 0.29165514<\/blockquote><br>\n    The corresponding classification output (y) would be:<br><br>\n    <blockquote>0 0 0 1 1 1 1 1 1 1<\/blockquote><br>\n<\/div>\n\n![image.png](attachment:image.png)","28f4f621":"<center><h3>Sequence Generation","0d774e03":"<h3>Backward LSTM","ceabb42a":"<center><h3>Importing Libraries","059b82ca":"<h3>Bidirectional","3b245e5b":"<h3>Forward LSTM","c524424b":"<center><h3>Predicting sequences"}}