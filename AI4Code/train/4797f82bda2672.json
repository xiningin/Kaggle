{"cell_type":{"dfac6324":"code","1940256f":"code","dec02964":"code","30d7ee16":"code","ab9a1516":"code","4656acae":"code","e8d09ca9":"code","4fddbc96":"code","89bbce81":"code","2f0befac":"code","0a530f9c":"code","95112092":"code","c9dd70f2":"code","131df220":"code","afac31e7":"code","220288cd":"code","4bb9880b":"code","eb3f1f06":"code","83560666":"markdown","d40519a4":"markdown","e830438d":"markdown","c4f41174":"markdown","6b533d5d":"markdown","ad91fc7f":"markdown","d8235540":"markdown","235c7bc7":"markdown","824b6aa1":"markdown","7986af5e":"markdown","58acafb8":"markdown","e5bceb65":"markdown","bc551b64":"markdown","434b68f5":"markdown","74169773":"markdown","57d54f3d":"markdown","e27c2b45":"markdown","3e8f718b":"markdown","7c810bbe":"markdown","9bcad839":"markdown","8a854f60":"markdown","d72e2bd9":"markdown","4ae67124":"markdown","38c3a479":"markdown","5f0c6a47":"markdown","f75f2bfc":"markdown","6ef043f2":"markdown","de49dc2e":"markdown","90bedfe3":"markdown"},"source":{"dfac6324":"#Lets Code","1940256f":"# Import Library\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image","dec02964":"# Setting model as Vgg19\nmodel = models.vgg19(pretrained = True).features\n","30d7ee16":"#print parameter of models\nprint(model)","ab9a1516":"class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        # The first number x in convx_y gets added by 1 after it has gone\n        # through a maxpool, and the second y if we have several conv layers\n        # in between a max pool. These strings (0, 5, 10, ..) then correspond\n        # to conv1_1, conv2_1, conv3_1, conv4_1, conv5_1 mentioned in NST paper\n        self.chosen_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n\n        # We don't need to run anything further than conv5_1 (the 28th module in vgg)\n        # Since remember, we dont actually care about the output of VGG: the only thing\n        # that is modified is the generated image (i.e, the input).\n        self.model = models.vgg19(pretrained=True).features[:29]\n\n    def forward(self, x):\n        # Store relevant features\n        features = []\n\n        # Go through each layer in model, if the layer is in the chosen_features,\n        # store it in features. At the end we'll just return all the activations\n        # for the specific layers we have in chosen_features\n        for layer_num, layer in enumerate(self.model):\n            x = layer(x)\n\n            if str(layer_num) in self.chosen_features:\n                features.append(x)\n\n        return features","4656acae":"def load_image(image_name):\n    image = Image.open(image_name)\n    image = loader(image).unsqueeze(0)\n    return image.to(device)\n","e8d09ca9":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimsize = 356","4fddbc96":"# remember we need to keep the size of all images the same.","89bbce81":"# Here we may want to use the Normalization constants used in the original\n# VGG network (to get similar values net was originally trained on), but\n# I found it didn't matter too much so I didn't end of using it. If you\n# use it make sure to normalize back so the images don't look weird.","2f0befac":"loader = transforms.Compose(\n    [\n        transforms.Resize((imsize, imsize)),\n        transforms.ToTensor(),\n        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)","0a530f9c":"!ls ..\/input\/\n","95112092":"original_img = load_image('..\/input\/originals\/labrador.jpg')\nstyle_img = load_image('..\/input\/originals\/painting.jpg')","c9dd70f2":"#generated = torch.randn(original_img.shape,device = device , requires_grad = True)\n\ngenerated = original_img.clone().requires_grad_(True)\nmodel = VGG().to(device).eval()\n","131df220":"# Hyerparameters\ntotal_steps = 6000\nlearning_rate = 0.001\n","afac31e7":"alpha = 1\nbeta = 0.01","220288cd":"optimizer = optim.Adam([generated],lr = learning_rate)","4bb9880b":"for step in range(total_steps):\n    # Obtain the convolution features in specifically chosen layers\n    generated_features = model(generated)\n    original_img_features = model(original_img)\n    style_features = model(style_img)\n\n    # Loss is 0 initially\n    style_loss = original_loss = 0\n\n    # iterate through all the features for the chosen layers\n    for gen_feature, orig_feature, style_feature in zip(\n        generated_features, original_img_features, style_features\n    ):\n\n        # batch_size will just be 1\n        batch_size, channel, height, width = gen_feature.shape\n        original_loss += torch.mean((gen_feature - orig_feature) ** 2)\n        # Compute Gram Matrix of generated\n        G = gen_feature.view(channel, height * width).mm(\n            gen_feature.view(channel, height * width).t()\n        )\n        # Compute Gram Matrix of Style\n        A = style_feature.view(channel, height * width).mm(\n            style_feature.view(channel, height * width).t()\n        )\n        style_loss += torch.mean((G - A) ** 2)\n\n    total_loss = alpha * original_loss + beta * style_loss\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    if step % 200 == 0:\n        print(total_loss)\n        save_image(generated, \"generated.png\")","eb3f1f06":"from IPython.display import display\nfrom PIL import Image\n\n\npath=\"..\/input\/finalimage\/generated.png\"\ndisplay(Image.open(path))","83560666":"\nIn other words, we take\n\n**Yellow Labrador + Beautiful Painting = Beautiful Labrador**","d40519a4":"![image.png](attachment:image.png)","e830438d":"**What Convolutional Neural Network Captures ?**\n","c4f41174":"**How Convolutional Neural Networks are used to capture Content and Style of images?**","6b533d5d":"Let\u2019s understand it with the help of an example:\n\nLet the first two channel in the above image be Red and Yellow.Suppose, the red channel captures some simple feature (say, vertical lines) and if these two channels were correlated then whenever in the image there is a vertical lines that is detected by Red channel then there will be a Yellow-ish effect of the second channel.\n\nNow,let\u2019s look at how to calculate these correlations (mathematically).\n\nIn-order to calculate a correlation between different filters or channels we calculate the dot-product between the vectors of the activations of the two filters.The matrix thus obtained is called Gram Matrix.","ad91fc7f":"![image.png](attachment:image.png)","d8235540":"![image.png](attachment:image.png)\nGram Matrix for style Image","235c7bc7":"We are gonna go through all concepts and code this thing from scratch.\n\nLet's start","824b6aa1":"![image.png](attachment:image.png)","7986af5e":"Now,we are in the position to define Style loss:\n\nCost function between Style and Generated Image is the square of difference between the Gram Matrix of the style Image with the Gram Matrix of generated Image.","58acafb8":"VGG19 network is used for Neural Style transfer. VGG-19 is a convolutional neural network that is trained on more than a million images from the ImageNet database. The network is 19 layers deep and trained on millions of images. Because of which it is able to detect high-level features in an image.\n\nNow, this \u2018encoding nature\u2019 of CNN\u2019s is the key in Neural Style Transfer. Firstly, we initialize a noisy image, which is going to be our output image(G). We then calculate how similar is this image to the content and style image at a particular layer in the network(VGG network). Since we want that our output image(G) should have the content of the content image(C) and style of style image(S) we calculate the loss of generated image(G) w.r.t to the respective content(C) and style(S) image.","e5bceb65":"![image.png](attachment:image.png)\nGramMatrix for generated Image","bc551b64":"[0,5,10,19,28]","434b68f5":"Now, at Layer 1 using 32 filters the network may capture simple patterns, say a straight line or a horizontal line which may not make sense to us but is of immense importance to the network, and slowly as we move down to Layer 2 which has 64 filters, the network starts to capture more and more complex features it might be a face of a dog or wheel of a car. This capturing of different simple and complex features is called feature representation.\n\nImportant thing to not here is that CNNs does not know what the image is, but they learn to encode what a particular image represents. This encoding nature of Convolutional Neural Networks can help us in Neural Style Transfer. Let\u2019s dive a bit more deeper.","74169773":"Before calculating style loss, let\u2019s see what is the meaning of \u201cstyle of a image\u201d or how we capture style of an image.\n\nHow we capture style of an image ?\n\n![image.png](attachment:image.png)\n\nThis image shows different channels or feature maps or filters at a particular chosen layer l.Now, in order to capture the style of an image we would calculate how \u201ccorrelated\u201d these filters are to each other meaning how similar are these feature maps.But what is meant by correlation ?","57d54f3d":"## What does that even mean ?\n\n### *Neural style transfer is an optimization technique used to take two images\u2014a content image and a style reference image (such as an artwork by a famous painter)\u2014and blend them together so the output image looks like the content image, but \u201cpainted\u201d in the style of the style reference image.*\n\n### *This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network.*","e27c2b45":"You may have noticed Alpha and beta in the above equation.They are used for weighing Content and Style cost respectively.In general,they define the weightage of each cost in the Generated output image.\n\nOnce the loss is calculated,then this loss can be minimized using backpropagation which in turn will optimize our randomly generated image into a meaningful piece of art.","3e8f718b":"**But how do we know whether they are correlated or not ?**\n\nIf the dot-product across the activation of two filters is large then two channels are said to be correlated and if it is small then the images are un-correlated.Putting it mathematically :\n\nGram Matrix of Style Image(S):\n\nHere k and k\u2019 represents different filters or channels of the layer L. Let\u2019s call this Gkk\u2019[l][S].","7c810bbe":"Link to original Paper  : https:\/\/arxiv.org\/pdf\/1508.06576.pdf\n ","9bcad839":"## Style Loss","8a854f60":"Gram Matrix for Generated Image(G):\n\nHere k and k\u2019 represents different filters or channels of the layer L.Let\u2019s call this Gkk\u2019[l][G].","d72e2bd9":"Link to andrew Ng videos \n\nhttps:\/\/youtu.be\/R39tWYYKNcI (Part 1)\n\nhttps:\/\/youtu.be\/ChoV5h7tw5A (Part 2)\n\nhttps:\/\/youtu.be\/xY-DMAJpIP4 (Part 3)\n\nhttps:\/\/youtu.be\/b1I5X3UfEYI (Part 4)","4ae67124":"![image.png](attachment:image.png)","38c3a479":"You did't got that. It's okay. I ain't going anywhere :)","5f0c6a47":"## Content Loss\nCalculating content loss means how similar is the randomly generated noisy image(G) to the content image(C).In order to calculate content loss :\n\nAssume that we choose a hidden layer (L) in a pre-trained network(VGG network) to compute the loss.Therefore, let P and F be the original image and the image that is generated.And, F[l] and P[l] be feature representation of the respective images in layer L.Now,the content loss is defined as follows:","f75f2bfc":"![image.png](attachment:image.png)","6ef043f2":"## Total Loss Function :\n\n\nThe total loss function is the sum of the cost of the content and the style image.Mathematically,it can be expressed as :","de49dc2e":"Having the above intuition, let\u2019s define our Content Loss and Style loss to randomly generated noisy image.","90bedfe3":"# Introduction"}}