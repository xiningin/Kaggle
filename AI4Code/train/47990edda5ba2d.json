{"cell_type":{"03d558d6":"code","fa06ff6d":"code","72eea9d2":"code","84d718a7":"code","4c5b8856":"code","cb592dd7":"code","2a168ca1":"code","7cf18f29":"code","3b9e3c55":"code","594a5d42":"code","ad57963e":"code","8c9d17bf":"code","e5456d8e":"code","4695eb6c":"code","7004f6ad":"code","93f9782b":"code","a02f958b":"code","897d3b6a":"code","9a7f080f":"markdown","53c13e0c":"markdown","d98e538f":"markdown","2017da76":"markdown","904842fe":"markdown","bac44ae1":"markdown","3dac0707":"markdown"},"source":{"03d558d6":"import numpy as np \nimport pandas as pd \nimport os\nfrom pathlib import Path \nfrom tqdm import tqdm\nimport re\nimport gc\nfrom pandas.api.types import is_integer_dtype\n# import lightgbm as lgb\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport seaborn as sns","fa06ff6d":"BASE_DIR = '..\/input\/optiver-realized-volatility-prediction'\nbase_path = Path(BASE_DIR)","72eea9d2":"# OPTIMIZE MEMORY from Chris (https:\/\/www.kaggle.com\/cdeotte\/time-split-validation-malware-0-68)\ndef reduce_memory(df,col):\n    mx = df[col].max()\n    if mx<256:\n            df[col] = df[col].astype('uint8')\n    elif mx<65536:\n        df[col] = df[col].astype('uint16')\n    else:\n        df[col] = df[col].astype('uint32')","84d718a7":"def make_parquet_df(path_list, train_df=None):\n    df_list = []\n    for path in tqdm(path_list):\n        df = pd.read_parquet(path)\n        stock_id = stock_key(path)\n        df['stock_id'] = stock_id\n        df['stock_id'] = df['stock_id'].astype(np.uint8)\n        ## Currently adding Targets ends in out of memory\n        if train_df is not None:\n            df = df.merge(train_df, on=['stock_id', 'time_id'])\n        df_list.append(df)\n    return pd.concat(df_list, ignore_index=True)","4c5b8856":"train_df = pd.read_csv(f\"{BASE_DIR}\/train.csv\")\nreduce_memory(train_df, 'stock_id')\nreduce_memory(train_df, 'time_id')\ntrain_df.info()\ngc.collect()\ntrain_df.head()","cb592dd7":"target_dtype = 'float32'\nprint(f\"Max error when converting target to {target_dtype}:\", (train_df['target'] - train_df['target'].astype(target_dtype)).max())\ntarget_dtype = 'float16'\nprint(f\"Max error when converting target to {target_dtype}:\", (train_df['target'] - train_df['target'].astype(target_dtype)).max())\n# train_df['target'] = train_df['target'].astype(target_dtype)","2a168ca1":"train_books_paths = list(Path(base_path \/ 'book_train.parquet').rglob('*.parquet'))\ntrain_trades_paths = list(Path(base_path \/ 'trade_train.parquet').rglob('*.parquet'))","7cf18f29":"stock_key = lambda path: int(re.findall(\"stock_id=(\\d*)\",str(path))[0])\ntrain_books_paths.sort(key = stock_key)\ntrain_trades_paths.sort(key = stock_key)","3b9e3c55":"train_books_df = make_parquet_df(train_books_paths[:10], train_df=None)\n# train_trades_df = make_parquet_df(train_trades_paths)","594a5d42":"for col in train_books_df.columns:\n    if is_integer_dtype(train_books_df[col].dtype):\n        reduce_memory(train_books_df,col)\ngc.collect()\ntrain_books_df.info()\ntrain_books_df.head()","ad57963e":"train_books_df['ask_price_diff'] = train_books_df['ask_price2'] - train_books_df['ask_price1']\ntrain_books_df['ask_size_diff'] = train_books_df['ask_size1'] - train_books_df['ask_size2']\ntrain_books_df['bid_price_diff'] = train_books_df['bid_price1'] - train_books_df['bid_price2']\ntrain_books_df['bid_size_diff'] = train_books_df['bid_size1'] - train_books_df['bid_size2']","8c9d17bf":"col = 'ask_price_diff'\nprint(f\"{col}: Maximum {train_books_df[col].max()} and Minimum {train_books_df[col].min()}\")\ncol = 'bid_price_diff'\nprint(f\"{col}: Maximum {train_books_df[col].max()} and Minimum {train_books_df[col].min()}\")\ncol = 'ask_size_diff'\nprint(f\"{col}: Maximum {train_books_df[col].max()} and Minimum {train_books_df[col].min()}\")\ncol = 'bid_size_diff'\nprint(f\"{col}: Maximum {train_books_df[col].max()} and Minimum {train_books_df[col].min()}\")\n","e5456d8e":"unique_time_ids = train_df['time_id'].nunique()\ntime_ids_per_stock = train_df.groupby('stock_id')['time_id'].nunique()\nprint(f\"There are {unique_time_ids} unique time ids in the training sample, the following stocks are missing some time_ids\")\ntime_ids_per_stock[time_ids_per_stock < unique_time_ids]","4695eb6c":"max_stock_id = train_df['stock_id'].max()\nmissing_stock_ids = [idx for idx in range(max_stock_id) if idx not in train_df['stock_id'].unique()]\nmissing_stock_ids","7004f6ad":"stock_corr_df = pd.pivot(train_df, index='time_id', columns='stock_id', values='target').corr()\nstock_corr_df.describe()","93f9782b":"train_df['rolled_target'] = train_df.groupby('stock_id')['target'].transform(lambda x: x.rolling(window=10, min_periods=5).mean())\nsubset_df = train_df[train_df['stock_id'].isin(range(0,15))]\nsubset_df = subset_df[subset_df['time_id'].isin(range(0,1000))]\n","a02f958b":"fig = px.line(subset_df, x='time_id', y='rolled_target', color='stock_id')\nfig.show()","897d3b6a":"sns.lineplot(data=subset_df, x='time_id', y='rolled_target', hue='stock_id')","9a7f080f":"# Stock Correlation","53c13e0c":"# Functions","d98e538f":"### Warning: Time_ids are not sequential, they have been randomly shuffled\nMore Info: https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/249564\nThis high level of correlation does seem to imply that time_id are the same for all stocks as asked in the discussion","2017da76":"The Fact that some stocks are missing some of the time_ids, might be a sign that these would be part of the test set","904842fe":"# Load Data","bac44ae1":"It seems many of the stocks volatility are highly correlated","3dac0707":"# Data Validation"}}