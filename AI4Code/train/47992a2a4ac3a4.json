{"cell_type":{"020ce4bd":"code","78116f94":"code","47aee03c":"code","aa12e2fd":"code","609d0bb4":"code","2137d64c":"code","d2f227ed":"code","a30eb830":"code","4c0a4192":"code","8a1b5104":"code","609ecd78":"code","9d792803":"code","8dfb732c":"code","8cf9f61b":"code","1685bee7":"code","d36ef105":"code","05f8b205":"code","02b214a0":"code","ccc4379a":"code","96905cfc":"code","9e909df6":"code","373cf7e9":"code","f196ec8f":"code","86f21508":"code","17157503":"markdown","b1446cdc":"markdown","a6af9e7f":"markdown","d1792351":"markdown","9b7152cd":"markdown","056c1e28":"markdown","4451f964":"markdown","d4b3af49":"markdown","eb79bc77":"markdown","7bd3cd04":"markdown","d2502a36":"markdown","0a60f568":"markdown"},"source":{"020ce4bd":"import sys, os, gc\nimport zipfile\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nimport gensim.models.keyedvectors as word2vec","78116f94":"# unzip file to specified path\ndef import_zipped_data(file, output_path):\n    with zipfile.ZipFile(\"..\/input\/\/jigsaw-toxic-comment-classification-challenge\/\"+file+\".zip\",\"r\") as z:\n        z.extractall(\"\/kaggle\/working\")\n        \ndatasets = ['train.csv', 'test.csv', 'test_labels.csv', 'sample_submission.csv']\n\nkaggle_home = '\/kaggle\/working'\nfor dataset in datasets:\n    import_zipped_data(dataset, output_path = kaggle_home)","47aee03c":"test_df = pd.read_csv('\/kaggle\/working\/test.csv')\ntrain_df = pd.read_csv('\/kaggle\/working\/train.csv')\nsample_input = pd.read_csv('\/kaggle\/working\/sample_submission.csv')\ntest_labels = pd.read_csv('\/kaggle\/working\/test_labels.csv')","aa12e2fd":"train_df.head()","609d0bb4":"TEXT = 'comment_text'\nlabels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n# add label to mark non-toxic comments\ntrain_df['non-toxic'] = 1 - train_df[labels].max(axis=1)\n# replace na values with placeholder\ntrain_df[TEXT].fillna(\"unknown\", inplace=True)\ntest_df[TEXT].fillna(\"unknown\", inplace=True)","2137d64c":"# isolate classification labels and input text\ny = train_df[labels]\nlist_sentences_train = train_df[TEXT]\nlist_sentences_test = test_df[TEXT]","d2f227ed":"max_features, maxlen = 20000, 200\n#\u00a0tokenize training and test data\ntk = Tokenizer(num_words=max_features)\ntk.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tk.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tk.texts_to_sequences(list_sentences_test)\n# pad sequences for homogeneous length\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","a30eb830":"def get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef get_embedding_size(embedding):\n    embedding_size = {'glove': 25, 'word2vec': 300, 'fasttext': 300}\n    if embedding not in embedding_size.keys():\n        print(f'Embedding type {embedding} is not supported')\n        raise ValueError\n    return embedding_size.get(embedding, None)\n    \ndef build_matrix(embedding, max_features):\n    # define sources and embedding size for each supported word embedding\n    if(embedding==\"glove\"):\n        embedding_idx = dict(get_coefs(*o.strip().split(\" \")) for o in open('..\/input\/glove-twitter\/glove.twitter.27B.25d.txt'))\n        embed_size = 25\n    elif(embedding==\"word2vec\"):\n        word2vec_dict = word2vec.KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)\n        embed_size = 300\n        embedding_idx = {}\n        for word in word2vec_dict.vocab:\n            embedding_idx[word] = word2vec_dict.word_vec(word)\n        # clean-up embedding dict\n        del word2vec_dict\n        gc.collect()\n    elif(embedding==\"fasttext\"):\n        embedding_idx = dict(get_coefs(*o.strip().split(\" \")) for o in open('..\/input\/fasttext\/wiki.simple.vec'))\n        embed_size = 300\n    else:\n        print(f'Embedding type {embedding} is not supported')\n        raise ValueError\n\n    # limit vocabulary size\n    nb_words = min(max_features, len(tk.word_index))\n    # create embedding matrix template filled with zeroes\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    # save all word embeddings common to training data and pre-trained corpus\n    for word, i in tk.word_index.items():\n        if i >= max_features: break\n        # try to obtain embedding\n        tmp = embedding_idx.get(word)\n        # if word exists in pre-trained embeddings, add embedding to feature matrix\n        if tmp is not None:\n            embedding_matrix[i] = tmp\n    return embedding_matrix","4c0a4192":"def get_cnn_model(num_filters, filter_sizes, embed_size, embedding_matrix):    \n    in_layer = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(in_layer)\n    x = SpatialDropout1D(0.4)(x)\n    x = Reshape((maxlen, embed_size, 1))(x)\n    \n    conv, maxpool = [], []\n    for i in range(len(filter_sizes)):\n        conv.append(Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size), kernel_initializer='normal', activation='elu')(x))\n        maxpool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv[i]))\n     \n    z = Concatenate(axis=1)(maxpool)   \n    z = Flatten()(z)\n    z = Dropout(0.1)(z)\n    out_layer = Dense(6, activation=\"sigmoid\")(z)\n    \n    model = Model(inputs=in_layer, outputs=out_layer)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model","8a1b5104":"embedding_matrix = build_matrix('word2vec', max_features = 20000)\nembedding_matrix.shape","609ecd78":"max_features, maxlen = embedding_matrix.shape\n\n#\u00a0tokenize training and test data\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n\n# pad sequences for homogeneous length\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","9d792803":"batch_size = 256\nepochs = 3\nmodel = get_cnn_model(num_filters = 32, filter_sizes = [3,5], embed_size = get_embedding_size('word2vec'), embedding_matrix = embedding_matrix)\nX_tra, X_val, y_tra, y_val = train_test_split(X_train, y[labels], train_size=0.95, random_state=233)\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))","8dfb732c":"y_pred = model.predict(X_val, batch_size=1024)\nresults = model.evaluate(X_val, y_val)\nprint('Test loss, Test acc:', results)","8cf9f61b":"# save the model\nmodel.save('cnn_word2vec.h5')\n# reload_model = keras.models.load_model('cnn_word2vec.h5')","1685bee7":"embedding_matrix = build_matrix('glove', max_features = 20000)\nembedding_matrix.shape","d36ef105":"max_features, maxlen = embedding_matrix.shape\n\n#\u00a0tokenize training and test data\ntk = Tokenizer(num_words=max_features)\ntk.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tk.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tk.texts_to_sequences(list_sentences_test)\n\n# pad sequences for homogeneous length\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","05f8b205":"batch_size = 256\nepochs = 3\nmodel = get_cnn_model(num_filters = 32, filter_sizes = [3,5], embed_size = get_embedding_size('glove'), embedding_matrix = embedding_matrix)\nX_tra, X_val, y_tra, y_val = train_test_split(X_train, y[labels], train_size=0.95, random_state=233)\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val)) # verbose = 2","02b214a0":"y_pred = model.predict(X_val, batch_size=1024)\nresults = model.evaluate(X_val, y_val)\nprint('Test loss, Test acc:', results)","ccc4379a":"# save the model\nmodel.save('cnn_glove.h5')\n# reload_model = keras.models.load_model('cnn_glove.h5')","96905cfc":"embedding_matrix = build_matrix('fasttext', max_features = 20000)\nembedding_matrix.shape","9e909df6":"max_features, maxlen = embedding_matrix.shape\n\n#\u00a0tokenize training and test data\ntk = Tokenizer(num_words=max_features)\ntk.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tk.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tk.texts_to_sequences(list_sentences_test)\n\n# pad sequences for homogeneous length\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","373cf7e9":"batch_size = 256\nepochs = 3\nmodel = get_cnn_model(num_filters = 32, filter_sizes = [3,5], embed_size = get_embedding_size('fasttext'), embedding_matrix = embedding_matrix)\nX_tra, X_val, y_tra, y_val = train_test_split(X_train, y[labels], train_size=0.95, random_state=233)\nhist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val)) # verbose = 2","f196ec8f":"y_pred = model.predict(X_val, batch_size=1024)\nresults = model.evaluate(X_val, y_val)\nprint('Test loss, Test acc:', results)","86f21508":"# save the model\nmodel.save('cnn_fasttext.h5')\n# reload_model = keras.models.load_model('cnn_fasttext.h5')","17157503":"## 5. Conclusions\n- Selecting a sensitive number of `max_features` reduced training time from around 20 min per epoch to less than a minute. Accuracy wasn't affected.\n- All three pre-embeddings provide good results, which suggests that the three original documents they where trained on contain enough information to characterize and effectively differenciate different types of toxic behaviour.","b1446cdc":"### 5.1 Word2Vec Embeddings\nLet's start loading the pre-trained `Word2Vec` embeddings:","a6af9e7f":"## 5. Convolutional Neural Network Model\nFirst, we'll define a simple, parametrized model. It can be defined by specifying:\n- Number of convolutional filters\n- Size of each convolutional filter\n- Embedding size\n- Embedding matrix","d1792351":"### 5.2 GloVe Embeddings","9b7152cd":"The following function returns an embedding matrix with loaded weights from the selected pre-trained word embedding:","056c1e28":"We can repeat the same exercise using a different embedding. Let's take `GloVe`:","4451f964":"## 1. Introduction\n\nThis kernel shows multiple Deep Learning models for Text Classification. Words are vectorized using multiple Word Embeddings (Word2Vec, GloVe and FastTest) both out-of-the-box and with additional training on the task's specific data. Models used are based on different architectures involving Convolutional and Recurrent Neural Networks. \n\nThe notebook follows these steps:\n\n - Load train and test datasets\n - Feature extraction\/vectorization of corpus with Word Embeddings\n - Training of multi-class categorization DL models for toxicity levels and type\n - Models hyperparameter tuning\n - Performance metrics and DL models comparison","d4b3af49":"### 5.3 FastText Embeddings","eb79bc77":"## 3. Training Data Pre-processing","7bd3cd04":"# Table of Contents\n1. [Introduction](#1)\n2. [Import Libraries and Datasets](#2)\n3. [Training Data Pre-Processing](#3)\n4. [Pre-trained Word Embeddings](#3)\n5. [Convolutional Neural Network Model](#4)\n - 5.1 [Word2Vec Embeddings](#5)\n - 5.2 [GloVe Embeddings](#6)\n - 5.2 [FastText Embeddings](#7)\n6. [Conclusions](#8)","d2502a36":"## 2. Import Libraries and Datasets","0a60f568":"## 4. Pre-Trained Word Embeddings\nEmbeddings are numerical representations of tokens (e.g. words, n-grams) that encode information about their meaning and context. Intuitively, words that usually appear in similar contexts, will be assigned similar encodings.\n\nIn this notebook we use the following word embeddings:\n\n### **1. Word2Vec**\n\n**Word2Vec** embeddings trained on Google Negative News data - as negative words may be more informative to text toxicity classification.\n\nWord2Vec trains a model on the context on each word, so similar words will have similar numerical representations. Each token is fed to the NN through an embedding layer initialized with random weights. The algorithm minimizes the loss of predicting the target words given the context words.\n\n\n### **2. GloVe**\n\n**GloVe** embeddings trained on Twitter data - trained on social media short messages, may be semantically similar to Wikipedia Comments data GloVe is similar to Word2Vec. \n\nGloVe learns by constructing a frequency co-occurrence matrix of size words times context. Since it's a very high-dimensional gigantic matrix, this matrix is factorized to achieve a lower-dimension representation.\n\n### **3. FastText**\n\nFastText is quite different from the above 2 embeddings. While Word2Vec and GLOVE treat each word as the smallest unit to train on, FastText uses n-gram characters as the smallest unit. This implies it can generate better word embeddings for rare or even new, unseen words."}}