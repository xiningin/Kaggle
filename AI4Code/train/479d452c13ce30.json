{"cell_type":{"b4a3d3ec":"code","cc3d86e4":"code","877932b8":"code","73ddb5e9":"code","22b03403":"code","874f1cb1":"code","f551e548":"code","e332ab63":"code","2deb4a77":"code","b493f0db":"code","8491c669":"code","079a5178":"code","47e1998a":"code","6ce50baf":"code","4887b546":"code","597f5537":"code","88130c71":"code","57c2aa6d":"code","a74f0304":"code","3a698f49":"code","4c486310":"code","0fd9c5a8":"code","b7efd16f":"code","6ee374b6":"code","7dc5889c":"code","3ac9cd86":"code","49dcfbf1":"code","b0e3eef6":"code","e6222e51":"code","113926d6":"code","514d69fc":"code","283c35aa":"code","e8b014a3":"code","ded99537":"code","b63ad63a":"code","90dde708":"code","597b5ec7":"code","f830884f":"code","3b9500dd":"code","9177eca0":"code","b26e168a":"code","beeeda6a":"code","845a67e0":"code","1fef0e0b":"code","3485632c":"code","221dd581":"code","38dd390d":"code","cffcb59f":"code","589a97f8":"code","ae8274db":"code","ba060ac3":"code","7414d600":"markdown","d8425b93":"markdown","93695ffe":"markdown","42ca4a97":"markdown","eeb8344f":"markdown","5394475e":"markdown","95b97b47":"markdown","70dc688f":"markdown","413db9c3":"markdown","bb25acc0":"markdown","33d56967":"markdown","6e3eee70":"markdown","f68fbc03":"markdown","17204c29":"markdown","897c1e69":"markdown","29311434":"markdown","0cf20fbe":"markdown","034ee2d5":"markdown","08ecfb4f":"markdown","1b6ce146":"markdown","f88dc40d":"markdown","3f797f24":"markdown","e619eda0":"markdown","2bb3afb4":"markdown","8c23b70d":"markdown","623f9d03":"markdown","d50bd61b":"markdown","50a48eeb":"markdown","e782fd12":"markdown"},"source":{"b4a3d3ec":"# Importing the libraries \nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","cc3d86e4":"# Importing the Boston Housing dataset\nfrom sklearn.datasets import load_boston\nboston = load_boston()","877932b8":"# Initializing the dataframe\ndata = pd.DataFrame(boston.data)","73ddb5e9":"# See head of the dataset\ndata.head()","22b03403":"#Adding the feature names to the dataframe\ndata.columns = boston.feature_names\ndata.head()","874f1cb1":"#Adding target variable to dataframe\ndata['PRICE'] = boston.target \n# Median value of owner-occupied homes in $1000s","f551e548":"#Check the shape of dataframe\ndata.shape","e332ab63":"data.columns","2deb4a77":"data.dtypes","b493f0db":"# Identifying the unique number of values in the dataset\ndata.nunique()","8491c669":"# Check for missing values\ndata.isnull().sum()","079a5178":"# See rows with missing values\ndata[data.isnull().any(axis=1)]","47e1998a":"# Viewing the data statistics\ndata.describe()","6ce50baf":"# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape","4887b546":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')","597f5537":"# Spliting target variable and independent variables\nX = data.drop(['PRICE'], axis = 1)\ny = data['PRICE']","88130c71":"# Splitting to training and testing data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)","57c2aa6d":"# Import library for Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear regressor\nlm = LinearRegression()\n\n# Train the model using the training sets \nlm.fit(X_train, y_train)","a74f0304":"# Value of y intercept\nlm.intercept_","3a698f49":"#Converting the coefficient values to a dataframe\ncoeffcients = pd.DataFrame([X_train.columns,lm.coef_]).T\ncoeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})\ncoeffcients","4c486310":"# Model prediction on train data\ny_pred = lm.predict(X_train)","0fd9c5a8":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","b7efd16f":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","6ee374b6":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","7dc5889c":"# Checking Normality of errors\nsns.distplot(y_train-y_pred)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","3ac9cd86":"# Predicting Test data with the model\ny_test_pred = lm.predict(X_test)","49dcfbf1":"# Model Evaluation\nacc_linreg = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_linreg)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","b0e3eef6":"# Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest Regressor\nreg = RandomForestRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","e6222e51":"# Model prediction on train data\ny_pred = reg.predict(X_train)","113926d6":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","514d69fc":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","283c35aa":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","e8b014a3":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","ded99537":"# Model Evaluation\nacc_rf = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_rf)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","b63ad63a":"# Import XGBoost Regressor\nfrom xgboost import XGBRegressor\n\n#Create a XGBoost Regressor\nreg = XGBRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","90dde708":"# Model prediction on train data\ny_pred = reg.predict(X_train)","597b5ec7":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","f830884f":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","3b9500dd":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","9177eca0":"#Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","b26e168a":"# Model Evaluation\nacc_xgb = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_xgb)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","beeeda6a":"# Creating scaled set to be used in model to improve our results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","845a67e0":"# Import SVM Regressor\nfrom sklearn import svm\n\n# Create a SVM Regressor\nreg = svm.SVR()","1fef0e0b":"# Train the model using the training sets \nreg.fit(X_train, y_train)","3485632c":"# Model prediction on train data\ny_pred = reg.predict(X_train)","221dd581":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","38dd390d":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","cffcb59f":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","589a97f8":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","ae8274db":"# Model Evaluation\nacc_svm = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_svm)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","ba060ac3":"models = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Support Vector Machines'],\n    'R-squared Score': [acc_linreg*100, acc_rf*100, acc_xgb*100, acc_svm*100]})\nmodels.sort_values(by='R-squared Score', ascending=False)","7414d600":"## Hence XGBoost Regression works the best for this dataset.****","d8425b93":"#### Train the model","93695ffe":"#### For test data","42ca4a97":"# XGBoost Regressor","eeb8344f":"Here the model evaluations scores are almost matching with that of train data. So the model is not overfitting.","5394475e":"### Model Evaluation","95b97b47":"There is no pattern visible in this plot and values are distributed equally around zero. So Linearity assumption is satisfied","70dc688f":"#### Model Evaluation","413db9c3":"#### For test data","bb25acc0":"# SVM Regressor","33d56967":"\ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\nAdjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\nMAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y.\u00a0\n\nMSE: The\u00a0mean square error\u00a0(MSE) is just like the MAE, but\u00a0squares\u00a0the difference before summing them all instead of using the absolute value.\u00a0\n\nRMSE: The\u00a0mean square error\u00a0(MSE) is just like the MAE, but\u00a0squares\u00a0the difference before summing them all instead of using the absolute value.\u00a0\n\n\n\n\n","6e3eee70":"### Model Evaluation","f68fbc03":"The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price. To train our machine learning model with boston housing data, we will be using scikit-learn\u2019s boston dataset.\n\nIn this dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column (price).\nhttps:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/housing.names","17204c29":"#### Training the model","897c1e69":"Here the residuals are normally distributed. So normality assumption is satisfied","29311434":"# Random Forest Regressor ","0cf20fbe":"# Evaluation and comparision of all the models","034ee2d5":"#### For test data","08ecfb4f":"CRIM per capita crime rate by town <br>\nZN proportion of residential land zoned for lots over 25,000 sq.ft. <br>\nINDUS proportion of non-retail business acres per town <br>\nCHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) <br>\nNOX nitric oxides concentration (parts per 10 million) <br>\nRM average number of rooms per dwelling <br>\nAGE proportion of owner-occupied units built prior to 1940 <br>\nDIS weighted distances to five Boston employment centres <br>\nRAD index of accessibility to radial highways <br>\nTAX full-value property-tax rate per 10,000usd <br>\nPTRATIO pupil-teacher ratio by town <br>\nB 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town <br>\nLSTAT % lower status of the population <br>","1b6ce146":"max_depth (int) \u2013 Maximum tree depth for base learners.\n\nlearning_rate (float) \u2013 Boosting learning rate (xgb\u2019s \u201ceta\u201d)\n\nn_estimators (int) \u2013 Number of boosted trees to fit.\n\ngamma (float) \u2013 Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\nmin_child_weight (int) \u2013 Minimum sum of instance weight(hessian) needed in a child.\n\nsubsample (float) \u2013 Subsample ratio of the training instance.\n\ncolsample_bytree (float) \u2013 Subsample ratio of columns when constructing each tree.\n\nobjective (string or callable) \u2013 Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n\nnthread (int) \u2013 Number of parallel threads used to run xgboost. (Deprecated, please use n_jobs)\n\nscale_pos_weight (float) \u2013 Balancing of positive and negative weights.\n","f88dc40d":"#### Train the model","3f797f24":"### Please upvote if you found this kernel useful! :) <br>\n### Feedback is greatly appreciated!","e619eda0":"#### Model Evaluation","2bb3afb4":"#### For test data","8c23b70d":"#### Training the model","623f9d03":"# Linear regression","d50bd61b":"# Boston house price prediction","50a48eeb":"C : float, optional (default=1.0): The penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n\nkernel : string, optional (default='rbf\u2019): kernel parameters selects the type of hyperplane used to separate the data. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed\u2019 or a callable.\n\ndegree : int, optional (default=3): Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n\ngamma : float, optional (default='auto\u2019): It is for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set. Current default is 'auto' which uses 1 \/ n_features.\n\ncoef0 : float, optional (default=0.0): Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : boolean, optional (default=True): Whether to use the shrinking heuristic.","e782fd12":"Each record in the database describes a Boston suburb or town."}}