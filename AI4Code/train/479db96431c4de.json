{"cell_type":{"b9646548":"code","7c3ed630":"code","bbed61e7":"code","be6de44c":"code","98e1d8ce":"code","64ab481d":"code","1fddf6d7":"code","5a880f10":"code","19bb185d":"code","22739104":"code","ddfd833d":"code","2e90c1a8":"code","1adbdf02":"code","3d2fe31f":"code","a276c89f":"code","64d0c344":"code","d8d51c38":"code","981ecbd9":"code","b045e09b":"code","b74a1169":"code","a5a7ee56":"code","7c80a7e1":"code","880ced77":"code","8fa465ae":"code","fae8c5d3":"code","1d97a961":"code","350f37f2":"code","d8517b97":"code","32afef36":"code","dca6fbd6":"code","0acd10d2":"code","600a7d04":"code","e2340bd8":"code","5dbd1cc1":"code","663533e6":"code","20e02601":"code","b656ca3c":"code","a27981c9":"code","743a30c1":"code","c9a29f06":"code","8a4a94dc":"code","65f1af2d":"markdown","0c98aac0":"markdown","82318884":"markdown","c5b82d45":"markdown","61e366dd":"markdown","2f6f568e":"markdown","3e61028e":"markdown","2468b9be":"markdown","7db75565":"markdown","42d7ca65":"markdown","1020d508":"markdown","1dd67ec8":"markdown","4ca2bfcc":"markdown","33705257":"markdown","53370f4e":"markdown","9274141e":"markdown","5660c491":"markdown","95e2fe39":"markdown"},"source":{"b9646548":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\npd.set_option('max_colwidth', None)","7c3ed630":"question_names = ['first_year','last_year','is_mandatory','timing','strain','has_revaccinations','revaccination_timing','location', 'manufacturer', 'supplier', 'groups']","bbed61e7":"def read_text(row):\n    code = row['alpha_2_code']\n    filename=row['filename'].replace('.txt', '')\n    filename = f'\/kaggle\/input\/hackathon\/task_1-google_search_txt_files_v2\/{code}\/{filename}.txt'\n    \n    with open(filename, 'r') as file:\n        data = file.read()#.replace('\\n', ' ')\n    return data\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef get_snippets(text):\n    '''\n        Returns sentences in the text which contain more than 5 tokens and at least one verb.\n    '''\n    return [sent.text.strip() for sent in nlp(text).sents \n                 if len(sent.text.strip()) < 350 and len(sent.text.strip().split()) > 5 and any([token.pos_ == 'VERB' for token in sent])]","be6de44c":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport tqdm\n\ndef get_negative_examples(df, q):\n    df = df[df[q].notna()][['alpha_2_code','country','url', 'filename',q]]\n    negative_examples = []\n\n    for _, row in df.iterrows():\n        text = read_text(row)\n        snippets = get_snippets(text)\n\n        tfidf_vectorizer = TfidfVectorizer()\n        tfidf_matrix = tfidf_vectorizer.fit_transform(snippets)\n\n        sim = cosine_similarity(tfidf_vectorizer.transform([row[q]]),tfidf_matrix)\n        res = pd.DataFrame()\n        res['snippet'] = snippets\n        res['sim'] = sim[0]\n        low_sim = res[res['sim']<0.1]['snippet'].values\n        negative_examples.extend(low_sim)\n    return negative_examples","98e1d8ce":"path = '\/kaggle\/input\/bcg-manually-reviewed-cleaned'\nfile = f'{path}\/manually_reviewed_cleaned.csv'\ndf_man = pd.read_csv(file, encoding = \"ISO-8859-1\")\n\ndf_man.columns = ['alpha_2_code', 'country', 'url', 'filename', 'is_pdf','Comments',\n              'Snippet'] + question_names + ['snippet_len', 'text_len']","64ab481d":"datasets = []\nfor q in question_names:\n    print(q)\n    file = f'{path}\/{q}_labeled.csv'\n    df_labeled = pd.read_csv(file, encoding = \"ISO-8859-1\")\n    neg = get_negative_examples(df_man, q)\n    \n    df_data = pd.DataFrame({'snippet': df_labeled['sentence'], 'label': df_labeled['label']})\n    df_data = df_data.append(pd.DataFrame({'snippet': neg, 'label': 0}), ignore_index=True)\n    \n    print(df_data.shape)\n    display(df_data['label'].value_counts(normalize=True))\n    print()\n    \n    datasets.append(df_data)","1fddf6d7":"from sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\n\n\ndef eval_model(X, y):\n    clfs = [('Dummy', DummyClassifier(strategy='prior')),\n            ('LogReg', LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')),]\n    \n    for name, clf in clfs:\n        pipeline = Pipeline([\n        ('tfidf', TfidfVectorizer()),\n        ('clf', clf),\n        ])\n    \n        scores = cross_validate(pipeline, X, y, cv=4, scoring=('accuracy', 'f1', 'roc_auc'), return_train_score=True)\n\n        print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'ACC', np.mean(scores['train_accuracy']), np.mean(scores['test_accuracy'])))\n\n        print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'F1', np.mean(scores['train_f1']), np.mean(scores['test_f1'])))\n\n        print(\"{:10s} {:5s} | Train: {:.3f}, Test: {:.3f}\".format(name, 'AUC', np.mean(scores['train_roc_auc']), np.mean(scores['test_roc_auc'])))\n    \n    \ndef get_model(X, y):\n    pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('clf', LogisticRegression(random_state=0, solver='lbfgs', class_weight='balanced')),\n    ])\n    \n    return pipeline.fit(X, y)","5a880f10":"for q, dataset in zip(question_names, datasets):\n    print(q)\n    eval_model(dataset['snippet'], dataset['label'])","19bb185d":"models = [get_model(d['snippet'], d['label']) for d in datasets]","22739104":"path = '\/kaggle\/input\/hackathon'\nfiles = [f'{path}\/task_1-google_search_english_original_metadata.csv',\n         f'{path}\/task_1-google_search_translated_to_english_metadata.csv']\ndfs = []\nfor file in files:\n    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n    dfs.append(df)\ndf = pd.concat(dfs, ignore_index=True)","ddfd833d":"df.drop(['Is Processed', 'Comments', 'language', 'query'], axis=1, inplace=True)\ndf.drop(df[df['is_downloaded']==False].index, inplace=True)\ndf['char_number'] = pd.to_numeric(df['char_number'], errors='coerce')\ndf.drop(df[df['char_number']==0].index, inplace=True)\ndf.drop_duplicates('url', keep=False, inplace=True)\ndf.drop(df[df['url'].str.contains('researchgate.net')].index, inplace=True)\ndf.drop(df[df['is_pdf']].index, inplace=True)\nassert all(df[df['alpha_2_code'].isna()]['country']=='Namibia')\ndf['alpha_2_code'].fillna('NA', inplace=True)","2e90c1a8":"from urllib.parse import urlparse\ndf['url_domain'] = df['url'].apply(lambda x: urlparse(x).netloc)","1adbdf02":"df_filtered = df[(df['url'].str.contains('vaccin')) |\n                (df['url'].str.contains('bcg')) |\n                 (df['url_domain']=='www.sciencedirect.com') |\n                 (df['url_domain']=='www.ncbi.nlm.nih.gov')]","3d2fe31f":"f\"Working with {df_filtered.shape[0]} sources\"","a276c89f":"! pip install pandarallel","64d0c344":"import bs4 as bs\nimport urllib.request\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\ndef get_url_title(url):\n    try:\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla\/5.0'})\n        source = urllib.request.urlopen(req).read()\n        soup = bs.BeautifulSoup(source,'lxml')\n        if not soup.title:\n            print('No title')\n            print(url)\n            return \"\"\n        return soup.title.text\n    except Exception as e:\n        print(e)\n        print(url)\n        return \"\"","d8d51c38":"df_filtered['url_title'] = df_filtered['url'].parallel_apply(get_url_title)","981ecbd9":"pd.options.mode.chained_assignment = None","b045e09b":"df_filtered['title_has_country'] = df_filtered.apply(lambda row: row['country'] in row['url_title'], axis=1)\ndf_filtered.drop(df_filtered[df_filtered['title_has_country'] == False].index, inplace=True)","b74a1169":"df_filtered.drop_duplicates('url_title', inplace=True)","a5a7ee56":"f\"Working with {df_filtered.shape[0]} sources\"","7c80a7e1":"import tqdm\n\ndfs = []\nfor _, row in tqdm.tqdm(df_filtered.iterrows()):\n    data = read_text(row)\n    \n    snippets = get_snippets(data)\n    \n    result = pd.DataFrame()\n    result['sentence'] = snippets\n    result['len'] = result['sentence'].apply(len)\n    result['country'] = row['country']\n    result['url'] = row['url']\n    result['filename'] = row['filename']\n    result['alpha_2_code'] = row['alpha_2_code']\n    \n    for q, model in zip(question_names, models):\n        result[q] = model.predict_proba(snippets)[:,1]\n    \n    result = result.replace(0, np.nan)\n    result = result.dropna(how='all', axis=0, subset=question_names)\n    \n    dfs.append(result)","880ced77":"result = pd.concat(dfs, ignore_index=True)","8fa465ae":"f\"Considering {result.shape[0]} snippets\"","fae8c5d3":"result.groupby('country')['url'].unique().apply(len).value_counts()","1d97a961":"def get_source_score(row, min_score=0.65):\n    res = {'score': sum([row[q].max() for q in question_names if row[q].max() > min_score])}\n    for q in question_names:\n        if row[q].max() > min_score:\n            res[q] = row.loc[row[q].idxmax()]['sentence']\n            res[f'{q}_score'] = row[q].max()\n        else:\n            res[q] = np.nan\n            res[f'{q}_score'] = np.nan\n    return res","350f37f2":"final_result = result.groupby(['country', 'alpha_2_code', 'url', 'filename']).apply(lambda x: pd.Series(get_source_score(x))).sort_values('score', ascending=False).groupby(['country']).head(2)","d8517b97":"final_result = final_result.replace(0, np.nan)\nfinal_result.dropna(how='all', inplace=True)\nfinal_result.reset_index(inplace=True)","32afef36":"f\"Final result has {final_result.shape[0]} different sources\"","dca6fbd6":"final_result.head()","0acd10d2":"final_result['country'].value_counts()","600a7d04":"total_n = 0\nfor q in question_names:\n    n = final_result[final_result[q].notna()].shape[0]\n    total_n += n\n    print(f'{q}: {n}')","e2340bd8":"f\"Total number of extracted answers: {total_n}\"","5dbd1cc1":"path = '\/kaggle\/input\/hackathon'\nfile = f'{path}\/BCG_world_atlas_data-2020.csv'\ndf_atlas = pd.read_csv(file)","663533e6":"final_result.insert(3, 'atlas', 'no')\nfinal_result['comments'] = ''","20e02601":"cols = [c for c in final_result.columns if 'score' not in c]\nassert len(cols) == len(df_atlas.columns)\nres_to_append = final_result[cols]\nres_to_append.columns = df_atlas.columns","b656ca3c":"df_atlas_ext = df_atlas.append(res_to_append.fillna(''), ignore_index=True)","a27981c9":"df_atlas_ext.sort_values('Contry Name (Mandatory field)', inplace=True)","743a30c1":"f\"Original dataset has {df_atlas.shape[0]} entries\"","c9a29f06":"f\"Extended dataset has {df_atlas_ext.shape[0]} entries\"","8a4a94dc":"df_atlas_ext.to_csv(f'\/kaggle\/working\/df_atlas_extended.csv', index=False)","65f1af2d":"We apply one additional filtering at article level - the title has to contain the name of the respective country.","0c98aac0":"Finally, we decided to filter sources so that they include two specific websites, which we manually inspected and contain credible information - www.ncbi.nlm.nih.gov and www.sciencedirect.com.\nThe rest of the sources we're filtering based on the url itself - it has to contain at least one of the substrings *'vaccin'* or *'bcg'*.","82318884":"Here we see that for most countries - 73, there is only a single source of data we consider. For 48 countries there are 2 sources. For the rest of the countries with more than 2 sources we will select only 2 based on a source score calculated below.","c5b82d45":"## Filter sources","61e366dd":"# Train 11 models","2f6f568e":"# Supervised datasets","3e61028e":"The evaluation shows that on 6 out of the 11 questions (**last_year**, **is_mandatory**, **strain**, **revaccination_timing**, **location** and **manufacturer**) the model beats the baseline on all 3 metrics","2468b9be":"We do a cross validation using the whole training sets and compare the results to a 'Dummy' majority class baseline.","7db75565":"We can see that for some questions the resulting dataset is bigger (i.e first_year, timing with over 1K examples), but for others like revaccination_timing we have only 30 examples.\n\nThe class imbalance is pretty much the same between the 11 datasets - around 2% positive examples.","42d7ca65":"# Extend BCG atlas data","1020d508":"# Predict with models","1dd67ec8":"## Load data","4ca2bfcc":"# Evaluate models","33705257":"Here we combine our positively labeled data with the negative examples we extract from the manually reviewed dataset.","53370f4e":"Extracted answers per question:","9274141e":"# Motivation\nSo far using the [Task 1: Label extraction for all 11 questions](https:\/\/www.kaggle.com\/didizlatkova\/task-1-label-extraction-for-all-11-questions) notebook we have sets of snippets that answer each of the 11 questions. In this final notebook we'll expand the BCG atlas data by:\n* Creating 11 supervised datasets by extracting negative examples from manually reviewed data (similarly to how we did it in [Task 1: Supervised dataset from manually reviewed](https:\/\/www.kaggle.com\/didizlatkova\/task-1-supervised-dataset-from-manually-reviewed))\n* Training 11 Logistic Regression models to predict whether a snippet answers each of the 11 questions in the task\n* Filtering the huge number of data sources available\n* Splitting the texts from the filtered data sources into snippets\n* Predicting the probability that each snippet answers each question using the trained models\n* Selecting only the top 2 sources per country that answer the most questions with biggest confidence","5660c491":"In the end, we've managed to extend the BCG Atlas dataset with 168 answers from 93 different sources for 75 countries.","95e2fe39":"We ignore all answers with probability from the model < 0.65. Using the rest, the score for a souce is the sum of the probabilities of the most confident answers for all 11 questions."}}