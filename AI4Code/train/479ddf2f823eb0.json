{"cell_type":{"bbe2491f":"code","79b13379":"code","bd6e5f7d":"code","7fad967d":"code","23724862":"code","87eda340":"code","5e74f016":"code","d5889464":"code","f04ccdeb":"code","d1b945c6":"code","a131757a":"code","940a5ccf":"code","c2da4062":"code","4fb773da":"code","dcd91f46":"code","de377fac":"code","5556712e":"code","512b82e2":"code","f9ae2c34":"code","c74eaabf":"code","cbe38c17":"code","3dc9d493":"code","8f607d31":"code","80be2486":"code","2bc32b6c":"code","957b1788":"code","0924d423":"code","15e3b07d":"code","1a9be872":"code","df25a149":"code","1915030e":"code","e1875b6b":"code","d6604cd4":"code","850e4b78":"markdown","e1c9d0a6":"markdown","3e02f518":"markdown","d6a1643c":"markdown","220f9bf6":"markdown","ddf5b33b":"markdown","aa6d1f16":"markdown","17ffff72":"markdown","05dcd37e":"markdown","3073950d":"markdown","0c5baa2b":"markdown","7b88e950":"markdown","6446c3b0":"markdown","6c69fea2":"markdown","12c26a7b":"markdown","90baa06f":"markdown","ad6c568e":"markdown","65d47c75":"markdown","fa0005cb":"markdown","3b514409":"markdown","ddf1b60e":"markdown"},"source":{"bbe2491f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","79b13379":"# Read files\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n#print(train)","bd6e5f7d":"################\n# prepare data #\n################\n\n#Save ID\ntrain_id = train[\"Id\"]\ntest_id = test[\"Id\"]\n\n#Drop ID columns (no need for prediction)\ntrain.drop(\"Id\", axis=1, inplace= True)\ntest.drop(\"Id\", axis=1, inplace= True)\n","7fad967d":"#Describe Data columns\n#print (train.columns)\n#print(test.columns)\n#print(train.shape,test.shape)\n#for i in train.columns:\n#    if i not in test.columns:\n#        print(\"Missing colums : |%s|\" % (i,))\n#train.describe()\ntrain['SalePrice'].describe()\nsns.distplot(train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())\n\n","23724862":"sns.distplot(np.log1p(train.SalePrice ));","87eda340":"# Apply log transformation on sale price to \n# get Normal distribution\ntrain_salePrice = train.SalePrice \ntrain.SalePrice = np.log1p(train.SalePrice )\n# New prediction\ny_train = train.SalePrice.values\ny_train_orig = train.SalePrice","5e74f016":"data_features = pd.concat((train, test), sort=True).reset_index(drop=True)\nprint(data_features.shape)\n\n# Missing data in train\ndata_features_na = data_features.isnull().sum()\ndata_features_na = data_features_na[data_features_na>0]\ndata_features_na.sort_values(ascending=False)","d5889464":"str_vars = ['MSSubClass','YrSold','MoSold']\nfor var in str_vars:\n    data_features[var] = data_features[var].apply(str)\n    \n#__________________________________________________________________________________    \n#Version 1 (Most popular value!)\n#common_vars = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\n#for var in common_vars:\n#    data_features[var] = data_features[var].fillna(data_features[var].mode()[0])   \n#__________________________________________________________________________________\n#Version 2 (empty data ~> worst case)\ncommon_vars = ['Exterior1st','Exterior2nd']\nfor var in common_vars:\n    data_features[var] = data_features[var].fillna('Other')   \n\ncommon_vars = ['SaleType']\nfor var in common_vars:\n    data_features[var] = data_features[var].fillna('Oth')   \n\ncommon_vars = ['Electrical']\nfor var in common_vars:\n    data_features[var] = data_features[var].fillna('FuseP')   \n\ncommon_vars = ['KitchenQual']\nfor var in common_vars:\n    data_features[var] = data_features[var].fillna('Po')   \n#__________________________________________________________________________________\n\n#Version 0\n#data_features['MSZoning'] = data_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))    \n#Version 1\n#data_features['MSZoning'] = data_features['MSZoning'].fillna(data_features['MSZoning'].mode()[0])   \n#Version 2\ndata_features['MSZoning'] = data_features['MSZoning'].fillna('I')   \n\n# Replacing missing data with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual',\n            'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\"PoolQC\"\n           ,'Alley','Fence','MiscFeature','FireplaceQu','MasVnrType','Utilities']:\n    data_features[col] = data_features[col].fillna('None')\n    \n# Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','MasVnrArea','BsmtFinSF1','BsmtFinSF2'\n           ,'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BsmtUnfSF','TotalBsmtSF'):\n    data_features[col] = data_features[col].fillna(0)\n\n# group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n#Version 1\n#data_features['LotFrontage'] = data_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n#Version 2\ndata_features['LotFrontage'] = data_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.min()))\n\nprint('Features size:', data_features.shape)\n\n# data description says NA means typical\ndata_features['Functional'] = data_features['Functional'].fillna('Typ')","f04ccdeb":"#data_features[var]\n\n#missing data check\n#total = data_features.isnull().sum().sort_values(ascending=False)\n#percent = (data_features.isnull().sum()\/data_features.isnull().count()).sort_values(ascending=False)\n#missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n#missing_data.head(10)","d1b945c6":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = data_features.select_dtypes(include=['object']).columns\nprint(categorical_features)\nnumerical_features = data_features.select_dtypes(exclude = [\"object\"]).columns\nprint(numerical_features)\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\nfeat_num = data_features[numerical_features]\nfeat_cat = data_features[categorical_features]","a131757a":"# Plot skew value for each numerical value\nfrom scipy.stats import skew \nskewness = feat_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","940a5ccf":"skewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nprint(\"Mean skewnees: {}\".format(np.mean(skewness)))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    feat_num[feat] = boxcox1p(feat_num[feat], boxcox_normmax(feat_num[feat] + 1))\n    data_features[feat] = boxcox1p(data_features[feat], boxcox_normmax(data_features[feat] + 1))\n    \n    \nfrom scipy.stats import skew \nskewness.sort_values(ascending=False)","c2da4062":"skewness = feat_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\n\nprint(\"There are {} skewed numerical features after Box Cox transform\".format(skewness.shape[0]))\nprint(\"Mean skewnees: {}\".format(np.mean(skewness)))\nskewness.sort_values(ascending=False)","4fb773da":"# Calculating totals before droping less significant columns\n\n#  Adding total sqfootage feature \ndata_features['TotalSF']=data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']\n#  Adding total bathrooms feature\ndata_features['Total_Bathrooms'] = (data_features['FullBath'] + (0.5 * data_features['HalfBath']) +\n                               data_features['BsmtFullBath'] + (0.5 * data_features['BsmtHalfBath']))\n#  Adding total porch sqfootage feature\ndata_features['Total_porch_sf'] = (data_features['OpenPorchSF'] + data_features['3SsnPorch'] +\n                              data_features['EnclosedPorch'] + data_features['ScreenPorch'] +\n                              data_features['WoodDeckSF'])\n\n\ndata_features['haspool'] = data_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata_features['hasgarage'] = data_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata_features['hasbsmt'] = data_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata_features['hasfireplace'] = data_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n# data_features['Super_quality'] = OverallQual * \n# vars = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'FullBath']","dcd91f46":"# Not normaly distributed can not be normalised and has no central tendecy\ndata_features = data_features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF'], axis=1)\n# data_features = data_features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF',\n#                          'PoolArea','3SsnPorch','LowQualFinSF','MiscVal','BsmtHalfBath','ScreenPorch',\n#                          'ScreenPorch','KitchenAbvGr','BsmtFinSF2','EnclosedPorch','LotFrontage'\n#                          ,'BsmtUnfSF','GarageYrBlt'], axis=1)\n\nprint('data_features size:', data_features.shape)\n","de377fac":"train = data_features.iloc[:len(y_train), :]\ntest = data_features.iloc[len(y_train):, :]\nprint(['Train data shpe: ',train.shape,'Prediction on (Sales price) shape: ', y_train.shape,'Test shape: ', test.shape])","5556712e":"vars = data_features.columns\n# vars = numerical_features\nfigures_per_time = 4\ncount = 0 \ny = y_train\nfor var in vars:\n    x = train[var]\n#     print(y.shape,x.shape)\n    plt.figure(count\/\/figures_per_time,figsize=(25,5))\n    plt.subplot(1,figures_per_time,np.mod(count,4)+1)\n    plt.scatter(x, y);\n    plt.title('f model: T= {}'.format(var))\n    count+=1","512b82e2":"# Removes outliers \n# outliers = [30, 88, 462, 631, 1322]\n# train = train.drop(train.index[outliers])\ny_train = train['SalePrice']","f9ae2c34":"# vars_box = ['OverallQual','YearBuilt','BedroomAbvGr']\nvars_box = feat_cat\nfor var in vars_box:\n    data = pd.concat([train['SalePrice'], train[var]], axis=1)\n    f, ax = plt.subplots(figsize=(8, 6))\n    fig = sns.boxplot(x=var, y=\"SalePrice\", data=data)","c74eaabf":"# Complete numerical correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=1, square=True);","cbe38c17":"# saleprice correlation matrix\ncorr_num = 15 #number of variables for heatmap\ncols_corr = corrmat.nlargest(corr_num, 'SalePrice')['SalePrice'].index\ncorr_mat_sales = np.corrcoef(train[cols_corr].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 9))\nhm = sns.heatmap(corr_mat_sales, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 7}, yticklabels=cols_corr.values, xticklabels=cols_corr.values)\nplt.show()","3dc9d493":"# pair plots for variables with largest correlation\nvar_num = 8\nvars = cols_corr[0:var_num]\n\nsns.set()\nsns.pairplot(train[vars], size = 2.5)\nplt.show();","8f607d31":"data_features = data_features.drop(\"SalePrice\", axis = 1)\nfinal_features = pd.get_dummies(data_features)\n\nprint(final_features.shape)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(y):, :]\nX.shape, y_train.shape, X_test.shape\n\n\nprint(X.shape,y_train.shape,X_test.shape)","80be2486":"# Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.95:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_test = X_test.drop(overfit, axis=1).copy()\n\nprint(X.shape,y_train.shape,X_test.shape)","2bc32b6c":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n# from sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","957b1788":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# model scoring and validation function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","0924d423":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4, #was 3\n                                       learning_rate=0.01, \n                                       n_estimators=8000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2, # 'was 0.2'\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n\n# xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n#                                      max_depth=3, min_child_weight=0,\n#                                      gamma=0, subsample=0.7,\n#                                      colsample_bytree=0.7,\n#                                      objective='reg:linear', nthread=-1,\n#                                      scale_pos_weight=1, seed=27,\n#                                      reg_alpha=0.00006)\n\n\n\n# setup models hyperparameters using a pipline\n# The purpose of the pipeline is to assemble several steps that can be cross-validated together, while setting different parameters.\n# This is a range of values that the model considers each time in runs a CV\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n\n\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=alphas2,random_state=42, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                         alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, lightgbm),\n                                meta_regressor=elasticnet,\n                                use_features_in_secondary=True)\n\n# store models, scores and prediction values \nmodels = {'Ridge': ridge,\n          'Lasso': lasso, \n          'ElasticNet': elasticnet,\n          'lightgbm': lightgbm}\n#           'xgboost': xgboost}\npredictions = {}\nscores = {}","15e3b07d":"for name, model in models.items():\n    \n    model.fit(X, y)\n    predictions[name] = np.expm1(model.predict(X))\n    \n    score = cv_rmse(model, X=X)\n    scores[name] = (score.mean(), score.std())","1a9be872":"# get the performance of each model on training data(validation set)\nscoreList = []\nprint('---- Score with CV_RMSE-----')\nscore = cv_rmse(ridge)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscoreList.append(1 - score.mean())\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscoreList.append(1 - score.mean())\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscoreList.append(1 - score.mean())\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscoreList.append(1 - score.mean())\n\nscore = cv_rmse(stack_gen)\nprint(\"stack_gen score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscoreList.append(1 - score.mean())\n\n# score = cv_rmse(xgboost)\n# print(\"xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n\n#Fit the training data X, y\nprint('----START Fit----',datetime.now())\nprint('Elasticnet')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso')\nlasso_model = lasso.fit(X, y)\nprint('Ridge')\nridge_model = ridge.fit(X, y)\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n# print('xgboost')\n# xgb_model_full_data = xgboost.fit(X, y)","df25a149":"def blend_models_predict(X):\n    return ((0.25  * elastic_model.predict(X)) + \\\n            (0.25 * lasso_model.predict(X)) + \\\n            (0.2 * ridge_model.predict(X)) + \\\n            (0.10 * lgb_model_full_data.predict(X)) + \\\n#             (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.2 * stack_gen_model.predict(np.array(X))))\n\ndef blend_models_predict_ABL(X, _scoreList):\n    \n    print(\"scoreList~>|%s|\" % (_scoreList,))\n    #ScoreSum = 0\n    #for i in range(len(_scoreList)):\n    #    ScoreSum += _scoreList[i]\n    ScoreSum = sum(_scoreList)\n    print(\"ScoreSum~>|%s|\" % (ScoreSum,))    \n    return ((scoreList[2]\/ScoreSum  * elastic_model.predict(X)) + \\\n            (scoreList[1]\/ScoreSum * lasso_model.predict(X)) + \\\n            (scoreList[0]\/ScoreSum * ridge_model.predict(X)) + \\\n            (scoreList[3]\/ScoreSum * lgb_model_full_data.predict(X)) + \\\n#             (0.1 * xgb_model_full_data.predict(X)) + \\\n            (scoreList[4]\/ScoreSum* stack_gen_model.predict(np.array(X))))","1915030e":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))\n\nprint('RMSLE score on train data: (blend_models_predict2)')\nprint(rmsle(y, blend_models_predict_ABL(X, scoreList)))","e1875b6b":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.iloc[:,1] = (np.expm1(blend_models_predict_ABL(X_test, scoreList)))","d6604cd4":"#### q1 = submission['SalePrice'].quantile(0.0042)\n# q2 = submission['SalePrice'].quantile(0.99)\n# # Quantiles helping us get some extreme values for extremely low or high values \n# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n# submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission.csv\", index=False)","850e4b78":"Preparing the data\n\nDropping Sale price, Creating dummy variable for the categorial variables and matching dimentions between train and test","e1c9d0a6":" ~> transformation {Y = log (1+x)}","3e02f518":"Optional: Box plot\nBox plot is heavy, one can manualy choose the intresting parameters","d6a1643c":"3. Prepare data\n    * drop id columns\n    * normalise sale price\n    * fill Nan value","220f9bf6":"Delete features","ddf5b33b":"Blend model prediction","aa6d1f16":"**1. Import libraries and load data**","17ffff72":"Pairplot for the most intresting parameters","05dcd37e":"2. The predicted variable - Sales price Skew & kurtosis analysis ","3073950d":"Training the model","0c5baa2b":"Removing overfit","7b88e950":"apply Box Cox transformation (~normalization)","6446c3b0":"Plotting the data for analysis","6c69fea2":"4. Adding features Data (sum of separeted data)","12c26a7b":"Separate data into numerical and categorical data","90baa06f":"submission","ad6c568e":"Spliting data back to train and test","65d47c75":"Defining models","fa0005cb":"*Comparing data to sale price through correlation matrix*\n\nNumerical values correlation matrix, to locate dependencies between different variables.","3b514409":"Defining folds and score functions","ddf1b60e":"Creating the model"}}