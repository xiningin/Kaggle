{"cell_type":{"9dcd0a68":"code","27f2a4fd":"code","c735afa9":"code","a79a2f35":"code","1b8977ea":"code","f61796fe":"code","c8eea73f":"code","d28f2b19":"code","c876849f":"code","9207f38e":"code","6c769a4c":"code","f5aaf052":"code","2d82bfe2":"markdown","ef627fe5":"markdown","5b889a9b":"markdown","6eba362c":"markdown","ff00c696":"markdown","a63a3379":"markdown","c6fa5437":"markdown","9e6d6aa6":"markdown","046c9b66":"markdown","5be96a7c":"markdown","1f8407b6":"markdown","dceecfdc":"markdown","c8e48088":"markdown","ec27ee74":"markdown","7f1f6783":"markdown","61d0b63e":"markdown","9e356bff":"markdown"},"source":{"9dcd0a68":"import os\nimport random\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport torch\nfrom PIL import Image\nimport torchvision\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom torchvision import datasets, models, transforms","27f2a4fd":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","c735afa9":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)","a79a2f35":"# Random iamge file\nrandom_file = random.choice(os.listdir('\/kaggle\/input\/few-shot-object-detection\/train\/images'))\n\n# Read the iamge file\nimg = cv2.imread(os.path.join('\/kaggle\/input\/few-shot-object-detection\/train\/images', random_file))\nplt.imshow(img)\n ","1b8977ea":"box = open(os.path.join('\/kaggle\/input\/few-shot-object-detection\/train\/labels', random_file.replace(\"jpg\", \"txt\")))\nfor line in box:\n    x1, y1, x2, y2 = line.split()\n    cv2.rectangle(img, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), 4)  \nplt.imshow(img)","f61796fe":"\ndata_path = \"\/kaggle\/input\/few-shot-object-detection\/train\/images\"\ntarget_path = \"\/kaggle\/input\/few-shot-object-detection\/train\/labels\"\n\ndef _prepare_food_dataset(data_path, target_path):\n    \"\"\"\n    Args: \n    data_path[str] --> Path to train images folder\n    target_path[str] --> Path to target labels folder\"\"\"\n    \n    # Ensure the image ids and boxes are in order; \n\n    img_files = os.listdir(data_path)\n    box_files = os.listdir(target_path)\n    \n    # Placeholders for collecting the images and target box coordinates\n    imgs = []\n    boxes = []\n    for i in img_files[:10]:\n        # Read images\n        img_ = Image.open(os.path.join(data_path, i))\n        # Read boxes\n        box = open(os.path.join(target_path, i.replace(\"jpg\", \"txt\")))\n        for line in box:\n            x1, y1, x2, y2 = line.split()\n        imgs.append(img_)\n        boxes.append([int(x1), int(y1), int(x2), int(y2)])\n        \n    return imgs, boxes\n\nimgs,boxes = _prepare_food_dataset(data_path, target_path)\n","c8eea73f":"import matplotlib.patches as patches\n\nfig,ax = plt.subplots(1,4, figsize=(25,25))\n\nfor idx, (img,box) in enumerate(zip(imgs[:4],boxes[:4])):\n    img =  np.array(img, dtype=np.uint8)\n    \n    # Display the image\n    ax[idx].imshow(img)\n    # Create a Rectangle patch\n    rect = patches.Rectangle((box[0],box[1]),box[2],box[3],linewidth=1,edgecolor='r',facecolor='none')\n    # Add the patch to the Axes\n    ax[idx].add_patch(rect)\n    \n    ","d28f2b19":"class FoodDataset(torch.utils.data.Dataset):\n    def __init__(self, data,targets, train = True):\n        self.data = data\n        self.targets = targets\n        self.train = train\n    def __getitem__(self, idx):\n\n        # Load images and labels\n        img = self.data[idx]\n        box = torch.as_tensor(self.targets[idx], dtype=torch.float32)\n        \n        # Since we are dealing with PIL image; Apply transformation\n        data_transforms = transforms.Compose([transforms.ToTensor(), \n                                              transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        if self.train:\n            img = data_transforms(img)\n    \n        return img, box\n        \n    def __len__(self):\n        return len(self.data)","c876849f":"def to_2d_tensor(inp):\n    \"\"\"Flatten the 3D tensor to 2D\"\"\"\n    inp = torch.Tensor(inp)\n    if len(inp.size()) < 2:\n        inp = inp.unsqueeze(0)\n    return inp\n\ndef xywh_to_x1y1x2y2(boxes):\n    \"\"\"Anchor cooradinate transformation\"\"\"\n    boxes = to_2d_tensor(boxes)\n    boxes[:, 2] += boxes[:, 0] - 1\n    boxes[:, 3] += boxes[:, 1] - 1\n    return boxes\n\ndef x1y1x2y2_to_xywh(boxes):\n    \"\"\"Anchor cooradinate retransformation\"\"\"\n    boxes = to_2d_tensor(boxes)\n    boxes[:, 2] -= boxes[:, 0] - 1\n    boxes[:, 3] -= boxes[:, 1] - 1\n    return boxes\n\ndef compute_acc(predicted, target, theta=0.75):\n    \"\"\"Compute accuracy, using intersection over union loss\"\"\"\n    IoU = compute_IoU(predicted, target)\n    corr = (IoU >= theta).sum()\n    return corr \/\/ predicted.size(0)\n\ndef compute_IoU(predicted, boxes2):\n    \"\"\"Intersection over Union loss function\n    Note that, our objective is to maximize the intersection area between the predicted and target boxes\"\"\"\n    \n    # Apply necessary transformations on the predictions and target to ease numerical operations \n    \n    predicted = to_2d_tensor(predicted)\n    predicted = xywh_to_x1y1x2y2(predicted)\n    \n    target = to_2d_tensor(target)\n    target = xywh_to_x1y1x2y2(target)\n    \n    # Compute intersecting points of predicted and target boxes\n    intersec = predicted.clone()\n    intersec[:, 0] = torch.max(predicted[:, 0], target[:, 0])\n    intersec[:, 1] = torch.max(predicted[:, 1], target[:, 1])\n    intersec[:, 2] = torch.min(predicted[:, 2], target[:, 2])\n    intersec[:, 3] = torch.min(predicted[:, 3], target[:, 3])\n    \n    def compute_area(boxes):\n        \"\"\"Helper method to compute area of bounding boxes\"\"\"\n        # in (x1, y1, x2, y2) format\n        dx = boxes[:, 2] - boxes[:, 0]\n        dx[dx < 0] = 0\n        dy = boxes[:, 3] - boxes[:, 1]\n        dy[dy < 0] = 0\n        return dx * dy\n    \n    # Compute Loss\n    \n    # Area of predicted boxes\n    predicted_area = compute_area(predicted)\n    \n    # Area of target boxes\n    target_area = compute_area(target)\n    \n    # Intersection between predicted and target\n    intersection_area = compute_area(intersec)\n    \n    assert((predicted_area + target_area - intersection_area <= 0).sum() == 0)\n    \n    return intersection_area \/ (predicted_area + target_area - intersection_area) ","9207f38e":"# Load pretrained model as feature extractor\nmodel = models.resnet18(pretrained=True)\n\n# in_features from the pretrained model\nfeature_input_size = model.fc.in_features\n\n# Add a box prediction layer using the in_feature size\nmodel.fc = torch.nn.Linear(feature_input_size, 4) # 4--> our target (x1,y1,x2,y2)\n\n# Move my model to target device\nmodel = model.to(device)","6c769a4c":"# Train dataset\nimgs,boxes = _prepare_food_dataset(data_path,target_path)\n\n# Pytorch Dataset instance\ndataset = FoodDataset(imgs,boxes,train=True )\n\n# Pytorch dataloader\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=4)\n\n# Loss \ncriterion = torch.nn.SmoothL1Loss().to(device)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n# Learning rate scheduler for the optimizer\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Save best model\/ #TODO: Explore Bias Variance tradeoff\nbest_model_state = model.state_dict()\n\n# My best epoch by default says \nbest_epoch = -1 # Just last epoch, we assume\nbest_acc = 0.0 # Target accuracy \n\nepoch_loss = {'train': [], 'test': []}\nepoch_acc = {'train': [], 'test': []}\n\nepochs = 10 # Number of iterations over my dataset\nfor epoch in range(epochs):\n    accs = []\n    losses = []\n    for phase in ('train', 'test'):\n        if phase == 'train':\n            scheduler.step()\n            model.train(True)\n        else: # Evaluation mode\n            model.train(False)\n            \n        for ims, boxes in data_loader: # Loop through each batch of Train data\/ Test data\n            # Move my data to target device\n            inputs = Variable(ims.to(device)) # Ofcoure Variable instance is not neccessary\n            targets = Variable(boxes.to(device))\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            # Compute loss\n            loss = criterion(outputs, targets)\n            # For the sake of model training performance, perform model accuracy\/testing at CPU by default\n            acc = compute_acc(outputs.data.cpu(), targets.data.cpu())\n            \n            # Number of samples in the batch\n            nsample = inputs.size(0)\n            \n            accs.append(acc\/\/nsample)\n            losses.append(loss.item()\/\/nsample)\n            if phase == 'train':\n                loss.backward()\n                optimizer.step()\n        \n        if phase == 'test' and np.mean(accs) > best_acc:\n            best_acc = np.mean(accs)\n            best_epoch = epoch\n            best_model_state = model.state_dict()\n            \n        print('[{}]\\tEpoch: {}\/{}\\tLoss: {:.4f}\\tAcc: {:.2%}'.format(phase, epoch+1, \n                                                                                   epochs, np.mean(losses), \n                                                                                   np.mean(accs)))\n        epoch_loss[phase].append(np.mean(losses))\n        epoch_acc[phase].append(np.mean(accs))\n        \n    print('[Info] best test acc: {:.2%} at {}th epoch'.format(best_acc, best_epoch))\n    torch.save(best_model_state, 'best_model_state.path.tar')\n","f5aaf052":"# df = pd.DataFrame(columns=[\"Id\", \"Expected\"])\n\n# predictions = get_predictions_for_image(img)\n\n# list_item = []\n# for i in range(len(predictions)):\n#     x1, y1, x2, y2, confidence = predictions[i]\n#     list_item.append([x1, y1, x2, y2, confidence])\n# df.loc[len(df)] = [filename, sorted(list_item)]","2d82bfe2":"### Write your custom dataset","ef627fe5":"### Criteria\/Loss function helpers","5b889a9b":"### Confirm _prepare_food_dataset return valid data\/label ","6eba362c":"Snippet to generate csv file for submission","ff00c696":"# Object Localization ","a63a3379":"### Train-test split\nTrain and test data are already provided\n\nAccess the test data and the steps are analogous as we did for training data","c6fa5437":"#### Corresponding target (Coordinates of bounding boxes)","9e6d6aa6":"### Your Model\n\n#### Feauture extractor\n","046c9b66":"### Set default device ","5be96a7c":"\n### Training Loop","1f8407b6":"**1. Data preparation**\n\nProvided training data preparation steps\n\n**2. Prepare train and test dataset using FoodDataset Helper;** \n\nNOTE: This notebook provided model training setup only on Train data\n\n**3. Prepare pytorch dataloader:**\n\nbatch_size is the hyperparameter to be explored\n\n**4. Model: Our model has two network: Feature extractor(pretrained ResNet) and a Linear prediction layer**\n\nFeel free to\n\ni) Develop your own feature extractor network or any pretrained model you prefer. \nii) Deep prediction layer\n\n**5.Loss function:**\n\nWe applied simple IoU loss function\n\nFeel free to explore other loss functions, Example\n> https:\/\/medium.com\/nodeflux\/distance-iou-loss-an-improvement-of-iou-based-loss-for-object-detection-bounding-box-regression-4cbdd23d8660\n\n**Overall**\n\nExplore by different, \n\n**models**(Custom or pretrained feature extractors, deeper prediction layer), **Optimizers**(adam,RMSProp, etc), **initial learning rate**, **learning rate schedulers**, **batch_size**, **loss functions** \n","dceecfdc":"## Imports","c8e48088":"This notebook guide you to develop custom object localization for the food dataset","ec27ee74":"#### Suggestion\nExplore image transformation\/augmentation techniques that improves model robustness   \n","7f1f6783":"### Dataset","61d0b63e":"### Pytorch Custom Dataset ","9e356bff":"#### Sample train data"}}