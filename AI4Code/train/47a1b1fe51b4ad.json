{"cell_type":{"f94407bc":"code","3c106541":"code","bb3421f4":"code","887260fc":"code","eb56edc0":"code","4923b99c":"code","415e2df7":"code","8045ce31":"code","5adf924b":"code","f62176e8":"code","5bb618a8":"code","146f1928":"code","d8dd6220":"code","edeefee9":"code","dbae3104":"code","10cff0a2":"code","680de2f4":"code","76ea6658":"code","78cea0d4":"code","6a9a3d12":"code","6149ed22":"code","960b92b1":"code","723e8d08":"code","e2c54c4e":"code","5a923615":"code","6af92009":"code","096ad57b":"code","49bc2055":"code","cb1ce2e3":"code","3a255513":"markdown","991aa4bd":"markdown","214e6870":"markdown","2d51625f":"markdown","694b7c29":"markdown","f01723c8":"markdown"},"source":{"f94407bc":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3c106541":"train, test = pd.read_csv('..\/input\/titanic\/train.csv'), \\\n              pd.read_csv('..\/input\/titanic\/test.csv')","bb3421f4":"train.info()","887260fc":"test.info()","eb56edc0":"train.head()","4923b99c":"train.isna().sum()","415e2df7":"test.isna().sum()","8045ce31":"sns.heatmap(train.isnull(), cmap='YlGnBu')","5adf924b":"datasets = [train, test]\nfor d in datasets:\n    for i in list(d.columns):\n        if (d[i].notnull().sum() \/ d.shape[0] <= .5) \\\n            or i in ['Name', 'Ticket']:\n            d.drop(i, axis=1, inplace=True)\n        elif d[i].dtype == float or train[i].dtype == int:\n            d[i] = d[i].fillna(d[i].mean())\n        else:\n            d[i] = d[i].fillna(d[i].mode()[0])","f62176e8":"sns.heatmap(train.isnull(), cmap='YlGnBu')","5bb618a8":"y_train = train['Survived']\nx_train, x_test = train.drop('Survived', axis=1), test\nx_train.drop('PassengerId', axis=1, inplace=True)","146f1928":"datasets = [x_train, x_test]\nfor d in datasets:\n    d['Embarked'] = d['Embarked'].fillna('S')\n    d['Embarked'] = d['Embarked'].map({\n        'S': 0,\n        'C': 1,\n        'Q': 2\n    })\n    d['Sex'] = d['Sex'].map({\n        'male': 0,\n        'female': 1\n    })\n    d['FamilySize'] = d['SibSp'] + d['Parch'] + 1","d8dd6220":"sns.heatmap(x_train.corr(), cmap='YlGnBu')","edeefee9":"x_train.head()","dbae3104":"x_test.head()","10cff0a2":"x_train.describe(include='all')","680de2f4":"x_test.describe(include='all')","76ea6658":"scaler = StandardScaler()\nfor d in datasets:\n    for c in ['Age', 'Fare', 'Parch', \\\n              'Pclass', 'SibSp', 'FamilySize']:\n        d[c] = d[c].astype(float)\n        d[c] = scaler.fit_transform(d[c].values.reshape(-1, 1))","78cea0d4":"for d in datasets:\n    sns.distplot(d['Age'], label=c)","6a9a3d12":"for d in datasets:\n    sns.distplot(d['Fare'], label=c)","6149ed22":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(32, input_dim=x_train.shape[1], activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    loss='binary_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy']\n)","960b92b1":"num_epochs = 42\nhistory = model.fit(x_train, y_train, epochs=num_epochs, \\\n                    batch_size=50, validation_split = 0.2)","723e8d08":"model.summary()","e2c54c4e":"scores = model.evaluate(x_train, y_train, batch_size=32)","5a923615":"print(f'Loss: {scores[0]} Accuracy: {scores[1]}')","6af92009":"loss_train = history.history['loss']\nloss_validation = history.history['val_loss']\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, loss_train, 'g', label='Training')\nplt.plot(epochs, loss_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","096ad57b":"acc_train = history.history['accuracy']\nacc_validation = history.history['val_accuracy']\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, acc_train, 'g', label='Training')\nplt.plot(epochs, acc_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()","49bc2055":"x_test_without_ids = x_test.drop('PassengerId', axis=1)\ny_predicted = model.predict(x_test_without_ids)\ny_test = (y_predicted > 0.5).astype(int).reshape(x_test_without_ids.shape[0])","cb1ce2e3":"results = pd.DataFrame()\nresults['PassengerId'] = x_test['PassengerId']\nresults['Survived'] = y_test\nresults.to_csv('submission.csv', index=False)","3a255513":"### Data Pre-processing\n\n  - **Data Cleaning**\n  - **Feature Engineering**\n  - **Feature Normalization**","991aa4bd":"## Titanic: Machine Learning from Disaster\n\n**Objective**: Predict if a passenger survived the sinking of the Titanic or not. Predict a 0 or 1 value for the variable.\n\n**Metric**: Percentage of passengers correctly predicted (Accuracy).","214e6870":"### Model Evaluation\n\nAnalysis Loss and Accuracy with Evolution","2d51625f":"### Dependencies\n\n  - **NumPy**\n  - **Pandas**\n  - **SeaBorn**\n  - **SciKit-Learn**\n  - **TensorFlow**  ","694b7c29":"### Submission Dataset\n\nPredicting Survival and Data Preparation","f01723c8":"### Model Building\n\nNeral Network with two 32-node hidden layers."}}