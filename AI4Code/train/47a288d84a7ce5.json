{"cell_type":{"b0795d7b":"code","790e083a":"code","4dd89564":"code","d86a181b":"code","67524d0b":"code","5800f60f":"code","c4411d32":"code","06b3a1dc":"code","b1f56798":"code","1435eda8":"code","2df2512e":"code","4c385ab7":"code","1987ab6b":"code","d92524f6":"code","58103fc0":"code","afaf5d70":"code","707b3d5c":"code","e9d20de7":"code","d5207e2a":"code","18703c8d":"code","c4a92aa1":"code","8c27276c":"code","7c8bdb10":"code","938d879f":"code","b04ca236":"code","0fca062f":"code","ac637fa3":"code","598ec9e5":"code","0de4492b":"code","4d0290ea":"code","65c86192":"code","2daae596":"code","4a190779":"code","54e7a2b8":"code","f6cf382d":"code","2f32f81e":"code","b0247c53":"code","111c15cb":"code","b0d04fbc":"code","47b6d7ad":"code","a4f33add":"code","76684b01":"code","ed30aeb8":"code","30d2e8d6":"code","1ac1a2c9":"code","8eb42dd3":"code","f5d029fe":"code","a54a7b96":"code","5b9a7d05":"code","f5b487b0":"code","39920549":"code","2d67cf6d":"code","7c971cdd":"code","22bdbe2c":"code","e54fadb1":"code","97b86326":"code","3b7b2463":"code","6ea2dcd3":"code","040b9a6d":"code","e2b0b1e4":"code","34d51edf":"code","8e0b9c06":"code","5e3f49a2":"code","7b956621":"code","fd4d49c4":"code","494cc8e4":"code","18fb74a4":"code","f10457e2":"code","c54fa7b2":"code","51368625":"code","454ee342":"code","0ae241f5":"code","6f56fd37":"code","e4e2280b":"code","e4ec0786":"code","362600b3":"code","a68a5f36":"code","0ab16e2f":"code","35b4f829":"code","cac511af":"code","121a54b3":"code","7c480ab1":"code","b9a8e421":"code","b3d8c501":"code","43090c11":"code","8d18ec2a":"code","35664f7e":"code","4e7dd468":"code","9cbb53b3":"code","bc1f128d":"code","dd15bbe1":"code","d7ed2532":"code","f9d61b91":"code","0d7598a5":"code","f9f7e6b0":"code","2822bf30":"code","4b45c20b":"code","8fdbb831":"code","d3c86553":"code","e9ce641f":"code","6794db96":"code","45d5e130":"code","dbf3a323":"code","53c7322a":"code","37cd76c0":"code","ebe6e490":"code","9911da7c":"code","37cecd11":"code","efa3acd0":"code","21cad536":"code","a06a7a3f":"code","48449504":"code","046efcfb":"code","acf49dd7":"code","6418bb45":"code","74313302":"code","bb352be7":"code","c3c09f46":"code","93d7352f":"code","ab274840":"code","a0102515":"code","80ed106d":"code","9c9a0db7":"code","a3f74dde":"code","3ef595cf":"code","afe8e7ef":"code","755193df":"code","0968b9d1":"code","42328da4":"code","3eaa5725":"code","72b95212":"code","d8109a0a":"code","47aaf50a":"code","afff1660":"code","6b10a756":"markdown","1286dc90":"markdown","a9f20b81":"markdown","1442657f":"markdown","09127da4":"markdown","ffded30c":"markdown","2ccb4704":"markdown","54a5cc61":"markdown","5727363f":"markdown","a53fb7ca":"markdown","80974a4a":"markdown","14772222":"markdown","595753af":"markdown","ba638857":"markdown","88a293af":"markdown","78e869f9":"markdown","2ff3d7f3":"markdown","a474d01e":"markdown","0df7820a":"markdown","4f83e60e":"markdown","841d676f":"markdown","62d75520":"markdown","b20b1a39":"markdown","45a2c9bb":"markdown","a2b7f618":"markdown","31c67fb6":"markdown","33fc3a07":"markdown","32375889":"markdown","97f576ba":"markdown","0cd99727":"markdown","6fb8ba94":"markdown","83d5d15d":"markdown","3a2d985a":"markdown","a6432593":"markdown","045cb80c":"markdown","56f118e9":"markdown","b05049af":"markdown","f973e6fa":"markdown","a2117612":"markdown","cf04743c":"markdown","98491d0f":"markdown","b1d80a47":"markdown","79290778":"markdown","b09c054c":"markdown","56968237":"markdown","d13304ec":"markdown","6686447d":"markdown","1b20d700":"markdown","523607c8":"markdown","76642f80":"markdown","f1abcd74":"markdown","91795da4":"markdown","627d805f":"markdown","0d238696":"markdown","ea1caedf":"markdown","fec8cf44":"markdown","88ed80ef":"markdown","01abb3c5":"markdown","fab241c1":"markdown","2bb66ebc":"markdown","cb8733c3":"markdown","3328b712":"markdown","350157b8":"markdown","949a586e":"markdown","877af762":"markdown","dd0c56f4":"markdown","e99f3fd8":"markdown","9763282e":"markdown","083b2d4a":"markdown","fce198d2":"markdown","3796947f":"markdown","83f7ebed":"markdown","f65a018f":"markdown","7060879e":"markdown","4dda3da9":"markdown","2991ec20":"markdown","3153e5c0":"markdown","2a1d4bf2":"markdown","4196aa2e":"markdown","fc61352b":"markdown","8fb8c517":"markdown","bd2ae112":"markdown","7779f97b":"markdown","7e4cade5":"markdown","3472e14d":"markdown","a8a2f33c":"markdown","8911d178":"markdown","e7ea337a":"markdown","59206d97":"markdown","88852ad8":"markdown","eaf9c217":"markdown","24e3c69e":"markdown","bf1d299b":"markdown","9df91815":"markdown","d70f2e06":"markdown"},"source":{"b0795d7b":"# Importing useful libraries\n\nimport math\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import KNNImputer\nfrom scipy import stats\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import SelectKBest,f_regression,mutual_info_regression\nfrom sklearn.model_selection import train_test_split\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeRegressor \n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error,explained_variance_score\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import RidgeCV\nfrom numpy import arange\n\n\nfrom matplotlib import pyplot\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","790e083a":"# Importing raw data using pandas\ndf = pd.read_csv('..\/input\/us-census-demographic-data\/acs2017_census_tract_data.csv')\n#sample Preview of raw data\ndf.head()","4dd89564":"# Finding the input dataset size and shape\ndf.shape","d86a181b":"# Printing metadata of the dataset\ndf.info()","67524d0b":"# Printing the descriptive Statistics for the Numerical fields in dataframe\ndf.describe(percentiles=[.25,.5,.75,.90,.95,.99])","5800f60f":"# Visualizing the  missing values as a matrix using missingno library\nmsno.matrix(df)\nplt.show()","c4411d32":"# Visualizing the correlation between the number of  missing values in different columns  using missingno library\nmsno.heatmap(df)\nplt.show()","06b3a1dc":"# Encoding the Nominal value columns ('State' and 'County') to convert them to numerical values using OrdinalEncoder\n\nord_enc = OrdinalEncoder()\ndf[\"State_Code\"] = ord_enc.fit_transform(df[[\"State\"]])\n\ndf[\"County_Code\"] = ord_enc.fit_transform(df[[\"County\"]])\n\ndf.head()","b1f56798":"#Top and bottom five tracts based on Income  \nprint (\"Here are the top and bottom five tracts based on Income: \")\ndf[[\"State\",\"County\",\"TractId\",\"Income\"]].sort_values(by = \"Income\", ascending = False)","1435eda8":"# Count of Tracts, by State and County, where the Income value is missing\nprint(\"Total number of Tracts where the Income value is missing: \", df[df[\"Income\"].isnull()].TractId.count())\nprint(\"\\n\\nThe list of top 10 State and County, by number of Tracts, where the Income value is missing: \")\ndf[df[\"Income\"].isnull()].groupby([\"State\",\"County\"]).TractId.count().sort_values(ascending = False).head(10)","2df2512e":"# Exploring the columns where the values are in totals.\ndf[[\"TotalPop\",\"Men\",\"Women\",\"VotingAgeCitizen\",\"Employed\"]].describe()","4c385ab7":"# Women vs income \nplt.scatter(df['Women'], df['Income'], label=\"Women\")\nplt.title(\"Women vs. Income\")\nplt.xlabel(\"Count Women\")\nplt.ylabel(\"Income\")\nplt.legend(loc='upper right')\nplt.show()","1987ab6b":"#Check the rows where the Total Pop =0\ndf.loc[df['TotalPop'] == 0]","d92524f6":"#Exploring the tracts with 0 population data\ndf.loc[(df['TotalPop'] == 0) & (df['Men'] == 0) & (df['Women'] == 0) & (df['VotingAgeCitizen'] == 0)  ]","58103fc0":"print(\"Number of Tracts where the 'Total Population' is not same as the total of Men and Women Population = \",\n      (df['TotalPop'] - (df['Men'] + df['Women'])).sum())","afaf5d70":"# Converting those columns to % of Total Population\nto_percent = ['Men','Women','VotingAgeCitizen','Employed']\ndf[to_percent] = df[to_percent].div(df[\"TotalPop\"], axis=\"index\")*100","707b3d5c":"df[[\"Employed\",\"PrivateWork\",\"PublicWork\",\"SelfEmployed\",\"FamilyWork\",\"Unemployment\"]].describe()","e9d20de7":"# Employment related columns in the dataset are percentages of the 'Employed' column\n# Converting these columns to a percentage of the Total Poluation itself\nto_perc_emp = ['PrivateWork','PublicWork','SelfEmployed','FamilyWork']\ndf[to_perc_emp] = df[to_perc_emp].multiply(df[\"Employed\"], axis=\"index\")\/100","d5207e2a":"#Unemployment vs Income \n\nplt.figure()\nplt.scatter(df['Unemployment'], df['Income'])\nplt.title(\"Unemployment vs. Income\")\nplt.xlabel(\"Unemployment\")\nplt.ylabel(\"Income\")\nplt.show()","18703c8d":"df[[\"Poverty\",\"ChildPoverty\"]].describe()","c4a92aa1":"df[[\"Professional\",\"Service\",\"Office\",\"Construction\",\"Production\"]].describe()","8c27276c":"plt.figure()\nplt.scatter(df['Professional'], df['Income'], label=\"Professional\")\nplt.scatter(df['Service'], df['Income'], label=\"Service\")\nplt.scatter(df['Office'], df['Income'], label=\"Office\")\nplt.scatter(df['Construction'], df['Income'], label=\"Construction\")\nplt.scatter(df['Production'], df['Income'], label=\"Production\")\nplt.title(\"JobType vs. Income\")\nplt.xlabel(\"County Citizens%\")\nplt.ylabel(\"Income\")\nplt.legend(loc='upper right')\nplt.show()","7c8bdb10":"df[[\"Drive\",\"Carpool\",\"Transit\",\"Walk\",\"OtherTransp\",\"WorkAtHome\",\"MeanCommute\"]].describe()","938d879f":"#Finding the Duplicates rows in dataframe\ndf.duplicated().sum()","b04ca236":"# Check if there are any duplicate rows across the independent varibales set\nprint(\"Total number of duplicate rows with TractId are:\", len(df[df.duplicated(['TractId'])]))","0fca062f":"# Filter out data where TotalPopulation is > 0 in the dataset\ndf = df.loc[df['TotalPop'] > 0]","ac637fa3":"# Total Null values in Raw dataset\ndf.isnull().sum().sum()","598ec9e5":"# Finding missing values count on column level along with the percentage of missing.\n\nMissing_total   = df.isnull().sum().sort_values(ascending=False) \nMissing_percent = df.isnull().sum() \/ len(df) * 100\nmissing_data_info = pd.concat([Missing_total, Missing_percent], axis = 1, keys=['Missing_Total','Missing_Percent'])\nmissing_data_info.sort_values(by = 'Missing_Percent',ascending=False)","0de4492b":"# As a bar plot\nmissing_data_info['Attribute_Name'] = missing_data_info.index\n\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.barplot(x = 'Attribute_Name', y = 'Missing_Total', data = missing_data_info)\nplt.xticks(rotation = 90)\nplt.show()","4d0290ea":"# Visualizing the  missing values as a matrix using missingno library\nmsno.matrix(df)\nplt.show()","65c86192":"# Visualizing the correlation between the number of  missing values in different columns  using missingno library\nmsno.heatmap(df)\nplt.show()","2daae596":"#Drop the redundant columns and the TractId columns, as it add no value anymore. \n#Also dropping the IncomeErr, IncomePerCap and IncomePerCapErr columns too as they are not target variables\ndf = df.drop(['TractId','State','County','TotalPop','IncomeErr','IncomePerCap','IncomePerCapErr'], axis=1)\n","4a190779":"#import sklearn StandardScaler library for z-score normalization  \nscaler = StandardScaler()\n\n#create a new dataframe with normalized values\nnormdf = pd.DataFrame(scaler.fit_transform(df),columns = df.columns) \n\nnormdf.head()\n","54e7a2b8":"#confirm the number of missing values after the normalization step\nnormdf.isnull().sum().sum()","f6cf382d":"#Imputing KNN with 5 neighbours\nimputer = KNNImputer(n_neighbors = 5, weights = 'uniform', metric ='nan_euclidean')\n\ncolumnlist = normdf.columns\n\n#Fitting & transforming the imputer on the data set\nnormdf[columnlist] = pd.DataFrame(imputer.fit_transform(normdf[columnlist]),columns = [columnlist])","2f32f81e":"# Checking the NULL values in dataset after the Using KNN imputation mehtod \nnormdf.isnull().sum().sum()","b0247c53":"normdf.info()","111c15cb":"# Visualizing the missing values as a matrix POST imputation\nmsno.matrix(normdf)\nplt.show()","b0d04fbc":"normdf.shape","47b6d7ad":"# Histograms of various Input variables alongside Box Plots and visualization of Mean and Median values \n\nfor col in normdf.columns:\n    \n    print(col)\n    Q1 = normdf[col].quantile(0.25)\n    Q3 = normdf[col].quantile(0.75)\n    IQR = Q3-Q1\n\n    print('Number of outliers = ',len(normdf[((normdf < (Q1 - 1.5 * IQR)) |(normdf > (Q3 + 1.5 * IQR))).any(axis=1)]))\n\n    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.4, 1)})\n\n    mean = normdf[col].mean()\n    median = normdf[col].median()\n\n    sns.boxplot(normdf[col], ax = ax_box)\n    ax_box.axvline(mean, color = 'r', linestyle='--')\n    ax_box.axvline(median, color = 'g', linestyle='-')\n\n    sns.distplot(normdf[col])\n    ax_hist.axvline(mean, color = 'r', linestyle='--')\n    ax_hist.axvline(median, color = 'g', linestyle='-')\n\n    plt.legend({'Mean':mean,'Median':median})\n\n    ax_box.set(xlabel = '')\n    plt.show()\n    print('===================================================')","a4f33add":"#Find the outliers records which fall with +\/- 5% boundaries \n\nQ1 = normdf.quantile(0.05)\nQ3 = normdf.quantile(0.95)\nIQR = Q3 - Q1\n\nprint(\"Number of outliers for entire dataset = \", len(normdf[((normdf < (Q1 - 1.5 * IQR)) |(normdf > (Q3 + 1.5 * IQR))).any(axis=1)]))","76684b01":"normalised_df = normdf.copy()","ed30aeb8":"# Finding the Outliers and Removing them from data using \"Z-score\" statistic\nz = np.abs(stats.zscore(normalised_df))\nprint(z)","30d2e8d6":"Cleaned_Data = (z < 3).all(axis=1) \n#Acquiring the cleaned data for Analysis \nZscore_data = normalised_df[Cleaned_Data]","1ac1a2c9":"print(\"dataset shape before outlier removal\", normalised_df.shape)\nprint(\"dataset shape after outlier removal\", Zscore_data.shape)","8eb42dd3":"# Combining the data for Asia and Pacific races as Pacific race has very sparse data and can be merged with another race data\nnormdf['APAC'] = normdf['Asian'] + normdf['Pacific']","f5d029fe":"normdf = normdf.drop(['Asian', 'Pacific'], axis=1)","a54a7b96":"#import sklearn Quantile Transformer library for non-Linear normalization  \n# This method transforms the data with many ouliers to have a Uniform\/Normal distribution \nscaler = QuantileTransformer(random_state=0,output_distribution = 'uniform')\n\n#create a new dataframe with normalized values\ntransformed_df = pd.DataFrame(scaler.fit_transform(normdf),columns = normdf.columns) \n\ntransformed_df.head()","5b9a7d05":"# Correlation heatmap of the various varioables to check the correlation across dependant and independent variables \n# Spearman Correlation coefficient\nplt.figure(figsize=(25, 15))\ncorr = transformed_df.corr('spearman')\nsns.heatmap(corr, annot=True)\n\nplt.show()","f5b487b0":"#Pairplots for all features\nsns.pairplot(transformed_df)","39920549":"X = transformed_df.loc[ : , transformed_df.columns != 'Income']\nY = transformed_df['Income']    ","2d67cf6d":"# Fit the data in the SelectKBest method with Mutual information for a continuous target.\n# Based on the results of hyper-parameter tuning the best k value is 15\n\nfs = SelectKBest(score_func = mutual_info_regression, k = 15) \nfeatures = fs.fit(X,Y)\n\n#Get the feature names\nfeature_name =  fs.get_support(1)\nX_new = X[X.columns[feature_name]] \nkbest_feature = X_new.columns\n\nprint(\"Best 15 features using K-best algorithm - \",kbest_feature)","7c971cdd":"# plot the scores\npyplot.figure(figsize=(25, 10))\npyplot.bar([col for col in X.columns], fs.scores_)\npyplot.xticks(rotation=45)\npyplot.show()","22bdbe2c":"scoresSFS = []\n\n#iterate SFS transformation for every combination of columns to plot the scores for every feature combination\nfor i in range(0, len(X.columns)):\n    # call SFS Forward method - on Linear regression\n    sfs = SFS(LinearRegression(),\n              k_features=i+1,\n              forward=True, # True for Sequential Forward selection\n              floating=False,\n              scoring = 'r2',\n              cv = 0)\n    \n    sfs.fit(X, Y)\n\n    scoresSFS.append(sfs.k_score_)","e54fadb1":"# Plot scores for Sequential Forward Selection(SFS)\nplt.figure(figsize=(25, 10))\nplt.plot([i for i in range(0, len(X.columns))], scoresSFS)\nplt.title('SFS Scores')\nplt.xlabel('Number of Attributes')\nplt.ylabel('Score')\npyplot.xticks(rotation=45)\nplt.show()","97b86326":"# Based on the above we select about 15 features set from the entire set\nsfs = SFS(LinearRegression(),\n            k_features=15,\n            forward=True,\n            floating=False,\n            scoring = 'r2',\n            cv = 0)\n    \nsfs.fit(X, Y)\nprint(\"Best 15 features using forward selection algorithm - \", sfs.k_feature_names_)","3b7b2463":"# Feature selection using Sequential Backward Selection(SBS)\nscoresSBS = []\n\n#iterate SFS transformation for every combination of columns to plot the scores for every feature combination\nfor i in range(0, len(X.columns)):\n    \n    sbs = SFS(LinearRegression(),\n              k_features=i+1,\n              forward=False, # False for Sequential Backward selection\n              floating=False,\n              scoring = 'r2',\n              cv = 0)\n    \n    sbs.fit(X, Y)\n    scoresSBS.append(sbs.k_score_)","6ea2dcd3":"# Plot scores for Sequential Forward Selection(SFS)\nplt.figure(figsize=(25, 10))\nplt.plot([i for i in range(0, len(X.columns))], scoresSBS)\nplt.title('SBS Scores')\nplt.xlabel('Number of Attributes')\nplt.ylabel('Score')\npyplot.xticks(rotation=45)\nplt.show()\n","040b9a6d":"# Sequential Backward Selection(SBS)\nsbs = SFS(LinearRegression(),\n          k_features=15,\n          forward=False,\n          floating=False,\n          scoring = 'r2',\n          cv = 0)\n\nsbs.fit(X, Y)\n\nprint(\"Best 15 features using backward selection algorithm - \", sbs.k_feature_names_)","e2b0b1e4":"# Using DecisionTreeRegressor as the estimator method\nestimator = DecisionTreeRegressor() # default MSE criterion\n\n# initialize RFE method with 15 featires  \nrfe_selector = RFE(estimator, n_features_to_select=15, step=1)\nrfe_selector.fit(X,Y)\n\n#get the list of 15 features  \nfeature_name =  rfe_selector.get_support(1)\nX_new = X[X.columns[feature_name]] \nrfe_features = X_new.columns\nprint(\"Best 15 features using RFE algorithm - \",rfe_features)","34d51edf":"#Get the best featire subset by each method\nkbest_feature_set = set(kbest_feature)\nsfs_feature_set = set(sfs.k_feature_names_)\nsbs_feature_set = set(sbs.k_feature_names_)\nrfe_feature_set = set(rfe_features)\n\n#get the intersection feature list from all methods \ncommon_feature_set = list(kbest_feature_set & sfs_feature_set & sbs_feature_set & rfe_feature_set)\nprint(\"Common features are - \", common_feature_set)","8e0b9c06":"# This method can be used to pass a set of selected feature list and get a new DataFrame as an output\n# with only selected features. Refer below for new DFs created from various feature selection methods.\ndef newDF_SelectedFeatures(df, feature_List):\n    df_Custom = df.iloc[:, feature_List]\n    return df_Custom","5e3f49a2":"def filtered_df(independent_var_df, features):\n    local_df = pd.DataFrame(independent_var_df, columns = features)\n    local_df.info()\n    return local_df","7b956621":"kbest_df = filtered_df(X, kbest_feature_set)","fd4d49c4":"sfs_df = filtered_df(X, sfs_feature_set)","494cc8e4":"sbs_df = filtered_df(X, sbs_feature_set)","18fb74a4":"rfe_df = filtered_df(X, rfe_feature_set)","f10457e2":"common_features_df = filtered_df(X, common_feature_set)","c54fa7b2":"# function to plot the variance ratio of the each of the features based on eigen value analysis \ndef pca_features (actual_df):\n    \n    cov_matrix = np.cov(actual_df.T)\n    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n    \n    # Step 3 (continued): Sort eigenvalues in descending order\n\n    # Make a set of (eigenvalue, eigenvector) pairs\n    eig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n\n    # Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\n    eig_pairs.sort()\n\n    eig_pairs.reverse()\n    \n    # Extract the descending ordered eigenvalues and eigenvectors\n    eigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n    eigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n\n    # Let's confirm our sorting worked, print out eigenvalues\n    \n    #Total sum of eigen values\n    tot = sum(eigenvalues)\n    var_explained = [(i \/ tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n    # eigen vector... there will be 8 entries as there are 8 eigen vectors)\n    cum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 8 entries with 8 th entry \n    # cumulative reaching almost 100%\n\n    plt.figure(figsize=(25, 10))\n    plt.bar(np.arange(len(var_explained)), var_explained, alpha=1, align='center', label='individual explained variance')\n    plt.step(np.arange(len(var_explained)),cum_var_exp, where= 'mid', label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc = 'best')\n    plt.show()","51368625":"# function to run PCA transformation for input dataframes and number of components\ndef pca_transform(actual_df, components_count):\n\n    pca = PCA(n_components=components_count)\n\n    #fit the data to the PCA model\n    principalComponents = pca.fit_transform(actual_df)\n    principalDf = pd.DataFrame(data = principalComponents)\n\n    return principalDf","454ee342":"# Run PCA analysis on standard scaled data frame with all features\npca_features(normdf)","0ae241f5":"#Get PCA transformed data for the 15 PCs from above\npca_norm_df = pca_transform(normdf, 15)","6f56fd37":"# Run PCA analysis on Quantile transformed data frame with all features\npca_features(transformed_df)","e4e2280b":"#Get PCA transformed data for the 15 PCs from above\npca_quantile_df = pca_transform(transformed_df, 15)","e4ec0786":"pca_features(kbest_df)","362600b3":"#Get PCA transformed data for the 11 PCs from above\npca_kbest_df = pca_transform(kbest_df, 11)","a68a5f36":"pca_features(sfs_df)","0ab16e2f":"#Get PCA transformed data for the 12 PCs from above\npca_sfs_df = pca_transform(sfs_df, 12)","35b4f829":"pca_features(sbs_df)","cac511af":"#Get PCA transformed data for the 12 PCs from above\npca_sbs_df = pca_transform(sbs_df, 12)","121a54b3":"pca_features(rfe_df)","7c480ab1":"#Get PCA transformed data for the 12 PCs from above\npca_rfe_df = pca_transform(rfe_df, 12)","b9a8e421":"pca_features(common_features_df)","b3d8c501":"#Get PCA transformed data for the 5 PCs from above\npca_common_df = pca_transform(common_features_df, 5)","43090c11":"# function to run a linear regresion model for the input datasets\n# 1. splits the input datasets into train & test sets\n# 2. fits a linear regression model\n# 3. predicts the target based on test set\n# 4. prints the model evaluation metrics\n\ndef linearRegressionTest(input_df, target_df, feature_selection_method):\n    \n    params_count = len(input_df.columns)\n    \n    X_train,X_test,Y_train,Y_test = train_test_split(input_df,target_df,test_size = 0.2)\n    \n    model = LinearRegression()\n\n    # Train the model, using training data set\n    model.fit(X_train, Y_train)\n\n    # Use trained model to predict on test dataset\n    y_pred = model.predict(X_test)\n\n    print(\"\\n--------------------------------------------------\\n\")\n    print(\"Score for model having \\'\",feature_selection_method, \"\\' using \\\"\", params_count,\"\\\" features\\n\" )\n\n    model_score = model.score(input_df, target_df) \n    mse = mean_squared_error(Y_test, y_pred)\n    r2 = r2_score(Y_test, y_pred)\n    rmse = math.sqrt(mse)\n    mae = mean_absolute_error(Y_test, y_pred)\n    evs = explained_variance_score(Y_test, y_pred)\n    \n    print(\"Model score is          = %.4f\" % model_score)\n    print(\"R2 score                = %.4f\" % r2)\n    print(\"Explained Variance score= %.4f\" % evs)\n    print(\"Mean absolute error     = %.4f\" % mae)\n    print(\"Mean squared error      = %.4f\" % mse)\n    print(\"Root Mean squared error = %.4f\" % rmse)\n    print(\"\\n--------------------------------------------------\\n\")\n    \n    return model_score, r2, rmse, mae","8d18ec2a":"normdf_X = normdf.loc[ : , normdf.columns != 'Income']\nnormdf_y = normdf['Income']\nmodel_score_1, r2_1, rmse_1, mae_1 = linearRegressionTest(normdf_X, normdf_y, \n                                                                 \"no data transformation and feature selection\" )","35664f7e":"transformedf_X = transformed_df.loc[ : , transformed_df.columns != 'Income']\ntransformedf_y = transformed_df['Income']\nmodel_score_2, r2_2, rmse_2, mae_2 = linearRegressionTest(transformedf_X, transformedf_y, \n                                                                 \"Quantile transformation and no feature selection\" )","4e7dd468":"model_score_3, r2_3, rmse_3, mae_3 = linearRegressionTest(kbest_df, transformedf_y, \"Kbest algorithm\")","9cbb53b3":"model_score_4, r2_4, rmse_4, mae_4 = linearRegressionTest(sfs_df, transformedf_y, \"SFS algorithm\")","bc1f128d":"model_score_5, r2_5, rmse_5, mae_5 = linearRegressionTest(sbs_df, transformedf_y, \"SBS algorithm\")","dd15bbe1":"model_score_6, r2_6, rmse_6, mae_6 = linearRegressionTest(rfe_df, transformedf_y, \"RFE algorithm\")","d7ed2532":"model_score_7, r2_7, rmse_7, mae_7 = linearRegressionTest(common_features_df, transformedf_y, \"common features\")","f9d61b91":"model_score_8, r2_8, rmse_8, mae_8 = linearRegressionTest(pca_norm_df, normdf_y, \"PCA on untransformed data\" )","0d7598a5":"model_score_9, r2_9, rmse_9, mae_9 = linearRegressionTest(pca_quantile_df, transformedf_y, \n                                                                 \"PCA on qunatile transformed data\" )","f9f7e6b0":"model_score_10, r2_10, rmse_10, mae_10 = linearRegressionTest(pca_kbest_df, transformedf_y, \n                                                                      \"PCA on Kbest algorithm\" )","2822bf30":"model_score_11, r2_11, rmse_11, mae_11 = linearRegressionTest(pca_sfs_df, transformedf_y, \"PCA on SFS algorithm\" )","4b45c20b":"model_score_12, r2_12, rmse_12, mae_12 = linearRegressionTest(pca_sbs_df, transformedf_y, \"PCA on SBS algorithm\" )","8fdbb831":"model_score_13, r2_13, rmse_13, mae_13 = linearRegressionTest(pca_rfe_df, transformedf_y, \"PCA on RFE algorithm\" )","d3c86553":"model_score_14, r2_14, rmse_14, mae_14 = linearRegressionTest(pca_common_df, transformedf_y, \n                                                                      \"PCA on common attributes\" )","e9ce641f":"#Build the random forest regression model with input datasets based on the number of trees \ndef randomForestRegressionModel(X, y,num_of_trees, feature_selection_method):\n    params_count = len(X.columns)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    #Build the random forest regressor model\n    RFModel = RandomForestRegressor(n_estimators = num_of_trees, random_state = 42, verbose = 1)\n    #Train the random forest regression model to dataset\n    RFModel.fit(X_train,y_train)\n    #Predict on test features\n    y_pred = RFModel.predict(X_test)\n    \n    print(\"\\n--------------------------------------------------\\n\")\n    print(\"Score for model having \\'\",feature_selection_method, \"\\' using \\\"\", params_count,\n          \"\\\" features with number of trees - \", num_of_trees)\n \n    # evaluate model metrics   \n    model_score = RFModel.score(X, y) \n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    rmse = math.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n    evs = explained_variance_score(y_test, y_pred)\n    \n    print(\"Model score is          = %.4f\" % model_score)\n    print(\"R2 score                = %.4f\" % r2)\n    print(\"Explained Variance score= %.4f\" % evs)\n    print(\"Mean absolute error     = %.4f\" % mae)\n    print(\"Mean squared error      = %.4f\" % mse)\n    print(\"Root Mean squared error = %.4f\" % rmse)\n    print(\"\\n--------------------------------------------------\\n\")\n    \n    return model_score, r2, rmse, mae","6794db96":"model_score_15, r2_15, rmse_15, mae_15 = randomForestRegressionModel(normdf_X, normdf_y, 50, \n                                                                             \"no data transformation and feature selection\")","45d5e130":"model_score_16, r2_16, rmse_16, mae_16 = randomForestRegressionModel(normdf_X, normdf_y, 100, \n                                                                             \"no data transformation and feature selection\")","dbf3a323":"model_score_17, r2_17, rmse_17, mae_17 = randomForestRegressionModel(transformedf_X, transformedf_y, 50,\n                                                                             \"Quantile transformation and no feature selection\")","53c7322a":"model_score_18, r2_18, rmse_18, mae_18 = randomForestRegressionModel(transformedf_X, transformedf_y, 100, \"Quantile transformation and no feature selection\")","37cd76c0":"model_score_19, r2_19, rmse_19, mae_19 = randomForestRegressionModel(kbest_df, transformedf_y,50,\"Kbest algorithm\")","ebe6e490":"model_score_20, r2_20, rmse_20, mae_20 = randomForestRegressionModel(kbest_df, transformedf_y,100,\"Kbest algorithm\")","9911da7c":"model_score_21, r2_21, rmse_21, mae_21 = randomForestRegressionModel(sfs_df, transformedf_y,50,\"SFS algorithm\")","37cecd11":"model_score_22, r2_22, rmse_22, mae_22 = randomForestRegressionModel(sfs_df, transformedf_y,100,\"SFS algorithm\")","efa3acd0":"model_score_23, r2_23, rmse_23, mae_23 = randomForestRegressionModel(sbs_df, transformedf_y,50,\"SBS algorithm\")","21cad536":"model_score_24, r2_24, rmse_24, mae_24 = randomForestRegressionModel(sbs_df, transformedf_y,100,\"SBS algorithm\")","a06a7a3f":"model_score_25, r2_25, rmse_25, mae_25 = randomForestRegressionModel(rfe_df, transformedf_y,50,\"RFE algorithm\")","48449504":"model_score_26, r2_26, rmse_26, mae_26 = randomForestRegressionModel(rfe_df, transformedf_y,100,\"RFE algorithm\")","046efcfb":"model_score_27, r2_27, rmse_27, mae_27 = randomForestRegressionModel(common_features_df, transformedf_y,50,\n                                                                             \"common features\")","acf49dd7":"model_score_28, r2_28, rmse_28, mae_28 = randomForestRegressionModel(common_features_df, transformedf_y,100,\n                                                                             \"common features\")","6418bb45":"model_score_29, r2_29, rmse_29, mae_29 = randomForestRegressionModel(pca_norm_df, normdf_y,50,\n                            \"PCA on untransformed data\")","74313302":"model_score_30, r2_30, rmse_30, mae_30 = randomForestRegressionModel(pca_norm_df,transformedf_y,100,\n                                                                             \"PCA on untransformed data\")","bb352be7":"model_score_31, r2_31, rmse_31, mae_31 = randomForestRegressionModel(pca_quantile_df, transformedf_y,50,\n                                                                             \"PCA on Quantile transformed data\")","c3c09f46":"model_score_32, r2_32, rmse_32, mae_32 = randomForestRegressionModel(pca_quantile_df, transformedf_y,100,\n                                                                             \"PCA on Quantile transformed data\")","93d7352f":"model_score_33, r2_33, rmse_33, mae_33 = randomForestRegressionModel(pca_kbest_df, transformedf_y,50,\n                                                                             \"PCA on Kbest algorithm\")","ab274840":"model_score_34, r2_34, rmse_34, mae_34 = randomForestRegressionModel(pca_kbest_df, transformedf_y,100,\n                                                                             \"PCA on Kbest algorithm\")","a0102515":"model_score_35, r2_35, rmse_35, mae_35 = randomForestRegressionModel(pca_sfs_df, transformedf_y,50,\n                                                                             \"PCA on SFS algorithm\")","80ed106d":"model_score_36, r2_36, rmse_36, mae_36 = randomForestRegressionModel(pca_sfs_df, transformedf_y,100,\n                                                                             \"PCA on SFS algorithm\")","9c9a0db7":"model_score_37, r2_37, rmse_37, mae_37 = randomForestRegressionModel(pca_sbs_df, transformedf_y,50,\n                                                                             \"PCA on SBS algorithm\")","a3f74dde":"model_score_38, r2_38, rmse_38, mae_38 = randomForestRegressionModel(pca_sbs_df, transformedf_y,100,\n                                                                             \"PCA on SBS algorithm\")","3ef595cf":"model_score_39, r2_39, rmse_39, mae_39 = randomForestRegressionModel(pca_rfe_df, transformedf_y,50,\n                                                                             \"PCA on RFE algorithm\")","afe8e7ef":"model_score_40, r2_40, rmse_40, mae_40 = randomForestRegressionModel(pca_rfe_df, transformedf_y,100,\n                                                                             \"PCA on RFE algorithm\")","755193df":"model_score_41, r2_41, rmse_41, mae_41 = randomForestRegressionModel(pca_common_df, transformedf_y,50,\n                                                                             \"PCA on common attributes\")","0968b9d1":"model_score_42, r2_42, rmse_42, mae_42 = randomForestRegressionModel(pca_common_df, transformedf_y,100,\n                                                                             \"PCA on common attributes\")","42328da4":"\nRegmodel = RidgeCV()\n\nX_train,X_test,y_train,y_test = train_test_split(transformedf_X,transformedf_y,test_size = 0.2, random_state=0)\n\n# Train the model, using training data set\nRegmodel.fit(X_train, y_train)\n\n# Use trained model to predict on test dataset\ny_pred = Regmodel.predict(X_test)\n\n#Print the Model evaluation results\nprint('Model score                = %.4f' % Regmodel.score(transformedf_X,transformedf_y))\nprint('Variance score or R2 score = %.4f' % r2_score(y_test, y_pred))\nprint(\"Mean squared error         = %.4f\" % mean_squared_error(y_test, y_pred))\nprint(\"Root Mean squared error    = %.4f\" % math.sqrt(mean_squared_error(y_test, y_pred)))\nprint('alpha: %f' % Regmodel.alpha_)\n\nmodel_score_43 = Regmodel.score(transformedf_X,transformedf_y)\nr2_43 = r2_score(y_test, y_pred)\nrmse_43 = math.sqrt(mean_squared_error(y_test, y_pred))\nmae_43 = mean_absolute_error(y_test, y_pred)","3eaa5725":"# Polynomial transformation with degree 2 \npolynomial = PolynomialFeatures(degree=2)\nfeat_poly = polynomial.fit_transform(transformedf_X)\n\nX_train,X_test,y_train,y_test = train_test_split(feat_poly,transformedf_y,test_size = 0.2, random_state=0)\n\npol_reg = LinearRegression()\npol_reg.fit(X_train, y_train)\n\n# Use trained model to predict on test dataset\ny_pred = pol_reg.predict(X_test)\n\n#Print the Model evaluation results\nprint('Model score                = %.4f' % pol_reg.score(feat_poly,transformedf_y))\nprint('Variance score or R2 score = %.4f' % r2_score(y_test, y_pred))\nprint(\"Mean squared error         = %.4f\" % mean_squared_error(y_test, y_pred))\nprint(\"Root Mean squared error    = %.2f\" % math.sqrt(mean_squared_error(y_test, y_pred)))\n\nmodel_score_44 = pol_reg.score(feat_poly,transformedf_y)\nr2_44 = r2_score(y_test, y_pred)\nrmse_44 = math.sqrt(mean_squared_error(y_test, y_pred))\nmae_44 = mean_absolute_error(y_test, y_pred)","72b95212":"\n# Polynomial transformation with degree 3\npolynomial = PolynomialFeatures(degree=3)\nfeat_poly = polynomial.fit_transform(transformedf_X)\n\nX_train,X_test,y_train,y_test = train_test_split(feat_poly,transformedf_y,test_size = 0.2, random_state=0)\n\npol_reg = LinearRegression()\npol_reg.fit(X_train, y_train)\n\n# Use trained model to predict on test dataset\ny_pred = pol_reg.predict(X_test)\n\n#Print the Model evaluation results\nprint('Model score                = %.4f' % pol_reg.score(feat_poly,transformedf_y))\nprint('Variance score or R2 score = %.4f' % r2_score(y_test, y_pred))\nprint(\"Mean squared error         = %.4f\" % mean_squared_error(y_test, y_pred))\nprint(\"Root Mean squared error    = %.2f\" % math.sqrt(mean_squared_error(y_test, y_pred)))\n","d8109a0a":"# Stats comparison for various Linear Regression models created above with varying input attributes\nmodel_score = [model_score_1, model_score_2, model_score_3, model_score_4, model_score_5, model_score_6, model_score_7,\n               model_score_8, model_score_9, model_score_10, model_score_11, model_score_12, model_score_13, \n               model_score_14]\n\nr2_Score = [r2_1, r2_2, r2_3, r2_4, r2_5, r2_6, r2_7, r2_8, r2_9, r2_10, r2_11, r2_12, r2_13, r2_14]\n\nMAE = [mae_1, mae_2, mae_3, mae_4, mae_5, mae_6, mae_7, mae_8, mae_9, mae_10, mae_11, mae_12, mae_13, mae_14]\n\nRMSE = [rmse_1, rmse_2, rmse_3, rmse_4, rmse_5, rmse_6, rmse_7, rmse_8, rmse_9, rmse_10, rmse_11, rmse_12, rmse_13, rmse_14]\n\nindex = ['All_30', 'Quantile_30', 'KBest_15','SFS_15', 'SBS_15', 'RFE_15', 'Common_12', 'PCA_15', 'PCA_Quantile_15',\n        'PCA_KBest_11', 'PCA_SFS_12', 'PCA_SBS_12', 'PCA_RFE_12',  'PCA_Common_5']\n\ndf = pd.DataFrame({'Model Score':model_score, 'R2 Score':r2_Score, 'MAE':MAE, 'RMSE':RMSE}, \n                  index = index)\n\nax = df.plot.bar(figsize=(18, 5))\nax.set_ylim([0, 1])\nplt.xticks(rotation=45, horizontalalignment=\"center\")","47aaf50a":"# Stats comparison for various Random Forest Regression models created above \n# with varying input attributes and 50 Trees\nmodel_score = [model_score_15, model_score_17, model_score_19, model_score_21, model_score_23, model_score_25, \n               model_score_27,\n               model_score_29, model_score_31, model_score_33, model_score_35, model_score_37, model_score_39, \n               model_score_41]\n\nr2_Score = [r2_15, r2_17, r2_19, r2_21, r2_23, r2_25, r2_27, r2_29, r2_31, r2_33, r2_35, r2_37, r2_39, r2_41]\n\nMAE = [mae_15, mae_17, mae_19, mae_21, mae_23, mae_25, mae_27, mae_29, mae_31, mae_33, mae_35, mae_37, mae_39, \n       mae_41]\n\nRMSE = [rmse_15, rmse_17, rmse_19, rmse_21, rmse_23, rmse_25, rmse_27, rmse_29, rmse_31, rmse_33, rmse_35, \n        rmse_37, rmse_39, rmse_41]\n\nindex = ['All_30', 'Quantile_30', 'KBest_15','SFS_15', 'SBS_15', 'RFE_15', 'Common_12', 'PCA_15', 'PCA_Quantile_15',\n        'PCA_KBest_11', 'PCA_SFS_12', 'PCA_SBS_12', 'PCA_RFE_12',  'PCA_Common_5']\n\ndf = pd.DataFrame({'Model Score':model_score, 'R2 Score':r2_Score, 'MAE':MAE, 'RMSE':RMSE}, \n                  index = index)\n\nax = df.plot.bar(figsize=(18, 5))\nax.set_ylim([0, 1])\nplt.xticks(rotation=45, horizontalalignment=\"center\")","afff1660":"# Stats comparison for various Random Forest Regression models created above \n# with varying input attributes and 100 Trees\nmodel_score = [model_score_16, model_score_18, model_score_20, model_score_22, model_score_24, model_score_26, \n               model_score_28,\n               model_score_30, model_score_32, model_score_34, model_score_36, model_score_38, model_score_40, \n               model_score_42]\n\nr2_Score = [r2_16, r2_18, r2_20, r2_22, r2_24, r2_26, r2_28, r2_30, r2_32, r2_34, r2_36, r2_38, r2_40, r2_42]\n\nMAE = [mae_16, mae_18, mae_20, mae_22, mae_24, mae_26, mae_28, mae_30, mae_32, mae_34, mae_36, mae_38, mae_40, \n       mae_42]\n\nRMSE = [rmse_16, rmse_18, rmse_20, rmse_22, rmse_24, rmse_26, rmse_28, rmse_30, rmse_32, rmse_34, rmse_36, \n        rmse_38, rmse_40, rmse_42]\n\nindex = ['All_30', 'Quantile_30', 'KBest_15','SFS_15', 'SBS_15', 'RFE_15', 'Common_12', 'PCA_15', 'PCA_Quantile_15',\n        'PCA_KBest_11', 'PCA_SFS_12', 'PCA_SBS_12', 'PCA_RFE_12',  'PCA_Common_5']\n\ndf = pd.DataFrame({'Model Score':model_score, 'R2 Score':r2_Score, 'MAE':MAE, 'RMSE':RMSE}, \n                  index = index)\n\nax = df.plot.bar(figsize=(18, 5))\nax.set_ylim([0, 1])\nplt.xticks(rotation=45, horizontalalignment=\"center\")","6b10a756":"#### 3.2 Observations:\n5. Looks like there are 696 Tracts where the Total population is captured as zero. Also from the above sample it looks like the other fields are also either  0 value or NaN. That doesnt sound realistic. ","1286dc90":"### 3.2 Data Analysis   \n\nGiven the nature of data analyzing the data in groups of features. ","a9f20b81":"## <span style='color:blue'> 1. Import required libraries <span>","1442657f":"Now Encoding the State and County as they are the only Categorical features in the dataset.","09127da4":"##### Observation:\nFrom the above chart the variance (step height) is larger with less than 15 Principal components. And after 15 Components the variance almost remains constant and gradually flattens.","ffded30c":"##### Observation:\nFrom the above chart the variance is optimal at around 5 Principal Components.","2ccb4704":"#### Observation\nBased on the above we select about 15 features from the entire set as the accuracy of the Linear regression model remains almost constant after 15 features  ","54a5cc61":"#### Using Select K Best method \n\nSelect features according to the k highest scores.","5727363f":"### 7.5 Comparison of all Regression models above","a53fb7ca":"#### Using Common features dataset (i.e. using features selected by: Select K-best, SFS, SBS & RFE methods)","80974a4a":"##### Run the above Random Forest Regression function for all 14 datasets, with 50 and 100 number of trees  ","14772222":"An outlier is a data point in a data set that is distant from all other observation. There are multiple ways to deal with outliers in dataset:\n1. IQR\n2. Z-score\n3. Clustering using DBSCAN\n\nIt completely depends on the data to use any of the approaches, see the below graph for outlier visualiation.","595753af":"##### Observation:\nFrom the above chart the variance is optimal at around 12 Principal Components.","ba638857":"## <span style='color:blue'>2. Load the 2017 Census Tract dataset <span>\n\nA census tract is a geographic region defined for the purpose of taking a census. Several tracts commonly exist within a county. \nCensus tracts represent the smallest territorial entity for which population data are available in many countries. In the United States, census tracts are subdivided into block groups and census blocks. ","88a293af":"#### Observation:\nFrom the above chart the variance is optimal at around 12 Principal Components.","78e869f9":"Now checking for null values in the dataset ","2ff3d7f3":"## <span style='color:blue'>5. Outlier Analysis <\/span>","a474d01e":"### 5.2 Using IQR method","0df7820a":"Based on all the above model evaluation results that we have created with different attribute sets, we recommend \u201cRandom Forest based Regression model\u201d with input dataset that has undergone Quantile transformation and then PCA (dimensionality reduction method) applied to it. \n\n* The number of independent features is reduced from 37 to 15.\n* Obtained model accuracy score of about 96% and Variance score (R2) of ~ 87%. The loss function RMSE is as low as 0.102.\n* The above recommendation is also based on a below factors:\n    * The relationship between the Target feature (Household Income) and independent features is non-linear. \n    * The Target feature is Household Income which can have large variability, the model extrapolation is not necessary.\n    * Model interpretability is low priority compared to the Model accuracy.\n    * We had a large amount of data to come up with a good model with Random Forest regressor\n","4f83e60e":"Applying Quantile transformation, didn't chnage the correlation between the variables.","841d676f":"#### Using PCA\n\nPrincipal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.","62d75520":"A Random Forest is an ensemble technique capable of performing both regression and classification tasks, with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. \n\n* The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.\n* Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. This part is called Bootstrap.\n* Every decision tree has high variance, but when we combine all of them together in parallel then the resultant variance is low as each decision tree gets perfectly trained on that particular sample data and hence the output doesn\u2019t depend on one decision tree but multiple decision trees. \n  * In the case of a classification problem, the final output is taken by using the majority voting classifier. \n  * In the case of a regression problem, the final output is the mean of all the outputs. This part is Aggregation.","b20b1a39":"### 6.3 Feature subset selection","45a2c9bb":"Now analyzing the Employment Type fields - Professional, Service, Office, Construction and Production","a2b7f618":"#### Random Forest Regression model","31c67fb6":" Before Imputing missing values let's remove redundant columns and then normalize the data. ","33fc3a07":"From the above chart, considering the 15 features for which the k-best score is greater than 0.1.","32375889":"Now analyzing the Commute Type fields - Drive, Carpool, Transit, Walk, OtherTransp, WorkAtHome, MeanCommute","97f576ba":"##### 5. PCA on features selected based on SFS-Backward method","0cd99727":"#### Using features selected with Sequential Forward Selection (SFS) method","6fb8ba94":"#### Using Sequential Feature Selection (SFS) - Forward method\n\nTransformer that performs Sequential Feature Selection.\n\nThis Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator.","83d5d15d":"## <span style='color:blue'> 6. Feature Engineering <\/span>","3a2d985a":"Now lets check the pairplot and correlation matrix...","a6432593":"### 7.4 Polynomial regression ","045cb80c":"#### Observations\n2. The above IQR method about 8% (5,719) of the data is marked as outlier. Removing them would cause significant reduction in training data.\n\nNow lets try Z-Score Outlier analysis method... ","56f118e9":"## <span style='color:blue'> 3. Data Exploration and Analysis <span>","b05049af":"Now for the remaining Tracts data, checking if the Total population is always total of Men + Women population.   ","f973e6fa":"### 6.1  Feature correlation analysis","a2117612":"##### 7. PCA on common features selected from k-best, SFS-Forward, SFS-Backward and RFE methods","cf04743c":"Let's visualize the missing values using various techniques.","98491d0f":"### 7.1 Linear Regression Modeling","b1d80a47":"### 7.2 Randon Forest Regression Modeling","79290778":"## <span style='color:blue'>4. Missing value Analysis and Imputation <span>","b09c054c":"### 7.3 Ridge regression ","56968237":"#### Observation\nFrom the above chart and to remain consistent with SFS method - selecting the top 15 features from the all feature set. ","d13304ec":"Now lets convert the totals to percentages based on the Total population value. That way all the fields are represented in Percentage values consistently.  ","6686447d":"## Problem Statement - \nEvery 10 years, the US government conducts a survey of the entire nation to understand the current distribution of the population. The Census Bureau updates the estimates approximately every year. \nThis information is useful for local authorities to take various economic and development decisions like location of schools, hospitals, stores, etc. \n","1b20d700":"### 4.1 Remove erroneous records","523607c8":"Based on offline data analysis, the below fields are expressed as percentage of \"Employed\" population - PrivateWork, PublicWork, SelfEmployed and FamilyWork. \nThus it makes sense to convert the values of the above columns as the percentage of Total population. ","76642f80":"##### Observation:\nFrom the above chart the variance is optimal at around 11 Principal Components.","f1abcd74":"<a id=\"step1\"><\/a>","91795da4":"Now check for Income related features","627d805f":"From observations made earlier, we can safely remove the  above 696 Tract data, where the data captured erroneously.","0d238696":"### Conclusion of Outlier analysis:\nGiven the nature of the dataset, it is expected to have many outliers. Now removing outliers would make the Model less reliable. Thus the decision to keep the outliers in the dataset and explore feature transformations to ensure good performance of the Regression models.      ","ea1caedf":"### 5.1 Visualize Data distribution","fec8cf44":"### Conclusion Feature Engineering:\nBased on all the above steps, we have obtained below listed datasets with unique feature subset, that can be used to measure performance of various Regression models.\n1. Standard scaled dataset with all features\n2. Quantiled transformed dataset with all features\n3. k-Best features dataset\n4. SFS-Forward features dataset\n5. SFS-Backward features dataset\n6. RFE features dataset\n7. Common features dataset (from above #3 to #6)\n\nPCA transformed datasets of all above datasets:\n8. PCA transformed Standard scaled dataset with all features\n9. PCA transformed Quantiled transformed dataset with all features\n10. PCA transformed k-Best features dataset\n11. PCA transformed SFS-Forward features dataset\n12. PCA transformed SFS-Backward features dataset\n13. PCA transformed RFE features dataset\n14. PCA transformed Common features dataset (from above #3 to #6)","88ed80ef":"#### Using features selected with Sequential Backward Selection (SBS) method","01abb3c5":"Now build the dataframes for each of the five feature sets obtained so far - \n1. k-best\n2. SFS-Forward\n3. SFS-Backward\n4. RFE\n5. intersection of all above","fab241c1":"#### 5. Observations\n3. With Z-Score outlier method, about 27% of the data is marked as outlier. Removing them would cause significant reduction in training data.\n\nNow lets try other DBScan analysis methods... ","2bb66ebc":"#### Using Sequential Backward Selection (SBS) method","cb8733c3":"##### 1. PCA on Standard scaled data frame with all features","3328b712":"#### 5. Observations\n1. Based on the above plots, there are many outlier values in almost all features. The data distribution is not normally distributed for most of the features. \n\nPerforming IQR outlier analysis across all columns...","350157b8":"## <span style='color:blue'> 7. Modeling <\/span>","949a586e":"##### 4. PCA on features selected based on SFS-Forward method","877af762":"Now analyzing population features - TotalPop, Men, Women, VotingAgeCitizen and Employed. ","dd0c56f4":"#### Using PCA","e99f3fd8":"#### 4.2 Observations \n1. By using KNN imputation we were able to impute all the 3,009 missing values, with no rows dropped. \n\nlet's see the same in visualization","9763282e":"Now analyzing population features - TotalPop, Men, Women, VotingAgeCitizen and Employed. \n\nInterestingly these features contain total values, where as the remaining population metrics are in % of population.   ","083b2d4a":"#### Using features selected with Select K-Best method","fce198d2":"### 4.2 Using KNN Imputation method","3796947f":"#### Observations:\n1. So the Census tract dataset contains 74001 rows and 37 columns.\n\n2. Apart from State and County the rest of the 35 fields are numeric.\n\n3. The count for all the fields dont match so some of the fields have <b>missing values<\/b>. \n\nLets quickly visualize the missing values. ","83f7ebed":"#### 4. Observations\n1. There are total of 3527 missing values in our total dataset. With the Income having the highest number of missing values.","f65a018f":"#### Using RFE method\n\nFeature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.","7060879e":"Check the scatterplots more deeply for the correlated features ","4dda3da9":"#### Selecting the best subset of Features\n\nNow lets identify the list of common features as recommended by all the above feature selection methods.","2991ec20":"## Table of contents\n1. Import required libraries\n2. Load the 2017 Census Tract dataset \n3. Data Exploration and Analysis\n4. Missing value Analysis and Imputation\n5. Outlier Analysis\n6. Feature Engineering\n7. Modeling\n8. Conclusion","3153e5c0":"##### Observation:\nFrom the above chart the variance is optimal at around 12 Principal Components.","2a1d4bf2":"##### 6. PCA on features selected based on RFE method","4196aa2e":"#### 3.3 Observations\n1. No duplicates records with and without TractId.","fc61352b":"Starting with State, County and Tract features","8fb8c517":"# Household Income Prediction based on US Demographic data\n## BITS Pilani AI ML Course Capstone Project (Group-33)","bd2ae112":"##### 3. PCA on features selected based on k-best method","7779f97b":"### 5.3 Using Z-Score method","7e4cade5":"### 3.1 Basic exploration","3472e14d":"#### 3.2 Observations:\n4. There are missing values in the Target feature - Income. \nWe will address this later...","a8a2f33c":"#### Random Forest Regression without feature selection techniques","8911d178":"### 3.3 Check for Duplicate records","e7ea337a":"##### Run the above function on all 14 datasets obtained above","59206d97":"#### Using features selected with RFE method","88852ad8":"#### 3.2 Observations:\n6. TotalPop always is the sum of Men and Women population.\n\nWe can probably remove the TotalPop feature later... ","eaf9c217":"Now analyzing the Poverty related features - Poverty, ChildPoverty ","24e3c69e":"## 8. Conclusion","bf1d299b":"##### Observation:\nFrom the above chart the variance (step height) is larger with less than 15 Principal components. And after 15 Components the variance almost remains constant and gradually flattens.","9df91815":"### 6.2 Quantile transformation\n\nTransform features using quantiles information. \nThis method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values.\nThe cumulative distribution function of a feature is used to project the original values. \n\nObserved better model performance with \"uniform\" distribution method. ","d70f2e06":"##### 2. PCA on Quantiled transformed data frame with all features"}}