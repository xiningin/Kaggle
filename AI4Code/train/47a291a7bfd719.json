{"cell_type":{"003841a4":"code","4fc775a0":"code","65aed164":"code","fadfb100":"code","a46c51c2":"code","cace2dde":"code","2c28dbb7":"code","9ed9cc8b":"code","8d7cb665":"code","c72c5f38":"code","871ba34d":"code","73dd66bc":"code","926da821":"code","72c506ad":"code","5cdad492":"code","825a808b":"code","a914d19e":"code","3a2b0baf":"code","8d7d4b42":"code","03596da4":"code","416f4995":"code","9978a6a7":"code","964236de":"code","9b6733d5":"code","eba4d8b2":"code","b4475803":"code","decb4d52":"code","23e94ebd":"code","0403dfdc":"code","5b74c9e8":"code","1460fa90":"code","427ae472":"code","67be1e1b":"code","bcbe43cb":"code","6cf16533":"code","5b75379e":"code","acef2e94":"code","4fc1f252":"code","04ca2f81":"code","c1aa3c66":"code","000e4b4d":"code","ad35f9e2":"code","f0db8e12":"code","5a8bec1f":"code","85425a92":"code","4d3910da":"code","ba8b7129":"code","b6cc22a4":"code","f757c9aa":"code","0e70ec89":"code","5973fd55":"code","76dde0f5":"code","120e219a":"code","59c65c69":"code","206dce9e":"code","2c6e478d":"code","834291a4":"code","5bf74d6e":"code","0b9fcaeb":"code","549edae6":"code","d4944534":"code","1f9bf711":"code","d6b84636":"code","bf6d911b":"code","efeac826":"code","34846b86":"code","1ba4b19e":"code","73790fa0":"code","3941990e":"code","bc3579d1":"code","5712a768":"code","b47d37d3":"code","609fc78e":"code","a18c7a66":"code","799fdb95":"markdown","a88295e8":"markdown","a4260c53":"markdown","b94fd8f4":"markdown","9cda0683":"markdown","2105fafc":"markdown","b2cc0836":"markdown","4142f971":"markdown","13ed1dc1":"markdown","59e8f4f7":"markdown","6df219dd":"markdown","5a705a65":"markdown","63909928":"markdown","6234c263":"markdown","a3a82dee":"markdown","7590e850":"markdown","7026ecb0":"markdown","97387d63":"markdown"},"source":{"003841a4":"# Library\n\n# Firstly used libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Warnings\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# Data Preprocessing\n\nfrom sklearn.neighbors import LocalOutlierFactor \nfrom sklearn import preprocessing\n\n# Modeling\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\n!pip install lightgbm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Model Tuning\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","4fc775a0":"# read the data\nhitters=pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\nhitters.head()","65aed164":"# There are 322 observations and int-float-object types of features in this data set.\n\ndf=hitters.copy()\nprint(df.shape)\ndf.info()","fadfb100":"#There are 59 null values in Hitters data set\ndf.isnull().sum().sum()","a46c51c2":"# All these NA values comes from \"Salary\" feature\ndf.isnull().sum()","cace2dde":"# There are high correlated  features within themselves. However,being high correlated is not a problem in machine learning algorithms.\n# In addition, there isn't high correlation between independent features and target feature(Salary).\nplt.figure(figsize=(14,12))\nsns.heatmap(df.corr(), annot=True, cmap=\"BuPu\");","2c28dbb7":"# If the missing values don't come from Salary(target feature), i would have thought to assign mean according to these results.\n# Because, there seems to be a relation between categoric variables and Salary values for example there is an important differences between being E Division and W Devision.\n\nprint(\"New League= A\" ,df[df[\"NewLeague\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"New League= N\" ,df[df[\"NewLeague\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= A\" ,df[df[\"League\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= N\" ,df[df[\"League\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= E\" ,df[df[\"Division\"]==\"E\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= W\" ,df[df[\"Division\"]==\"W\"].agg({\"Salary\":\"mean\"}))","9ed9cc8b":"#drop NA values\n\ndf1=df.dropna()\ndf1.shape","8d7cb665":"# understanding skewness of the features ( It is acceptable if the skewness is btween -1 and 1)\n# When the value of the skewness is negative, the tail of the distribution is longer towards the left hand side of the curve.\n# When the value of the skewness is positive, the tail of the distribution is longer towards the right hand side of the curve.\ndf1.skew(axis = 0, skipna = True) ","c72c5f38":"df1.skew(axis = 0, skipna = True)[(df1.skew(axis = 0, skipna = True) >1) | (df1.skew(axis = 0, skipna = True)< -1)]","871ba34d":"# Applying log transformation for right skewed features and applying exponential for left skewed features\nsns.distplot(df1[\"CAtBat\"], hist=False);","73dd66bc":"df1[\"CAtBat\"]= np.log(df1[\"CAtBat\"])","926da821":"sns.distplot(df1[\"CHits\"], hist=False);","72c506ad":"df1[\"CHits\"]= np.log(df1[\"CHits\"])","5cdad492":"sns.distplot(df1[\"CHmRun\"], hist=False);","825a808b":"df1[\"CHmRun\"]=np.log(df1[\"CHmRun\"])","a914d19e":"sns.distplot(df1[\"CRuns\"], hist=False);","3a2b0baf":"df1[\"CRuns\"]= np.log(df1[\"CRuns\"])","8d7d4b42":"sns.distplot(df1[\"CRBI\"], hist=False);","03596da4":"df1[\"CRBI\"]= np.log(df1[\"CRBI\"])","416f4995":"sns.distplot(df1[\"CWalks\"], hist=False);","9978a6a7":"df1[\"CWalks\"]= np.log(df1[\"CWalks\"])","964236de":"sns.distplot(df1[\"PutOuts\"], hist=False);","9b6733d5":"df1[\"PutOuts\"]= np.log(df1[\"PutOuts\"])","eba4d8b2":"sns.distplot(df1[\"Assists\"], hist=False);","b4475803":"df1[\"Assists\"]= np.log(df1[\"Assists\"])","decb4d52":"df1.head()","23e94ebd":"# get dummies\n\ndf1 =pd.get_dummies(df1,columns= [\"League\",\"Division\",\"NewLeague\"], drop_first=True)\ndf1.head(2)","0403dfdc":"numeric_df1=df1.loc[:, \"AtBat\":\"Errors\"]\ncat_df1=df1.loc[:, \"League_N\":\"NewLeague_N\"]\ny_df1= df1[\"Salary\"]","5b74c9e8":"y_df1","1460fa90":"numeric_df1.isin(['-inf']).any()==True","427ae472":"numeric_df1[numeric_df1[\"CHmRun\"].astype(\"str\").str.get(1)==\"i\"].index","67be1e1b":"numeric_df1[numeric_df1[\"CHmRun\"].astype(\"str\").str.get(1)==\"i\"]","bcbe43cb":"numeric_df1[\"CHmRun\"].describe()","6cf16533":"# assign median to infinite values in CHmRun\n\nnumeric_df1.loc[[7, 188, 239],\"CHmRun\"]=3.688879\nnumeric_df1[\"CHmRun\"].describe()","5b75379e":"numeric_df1[numeric_df1[\"PutOuts\"].astype(\"str\").str.get(1)==\"i\"].index","acef2e94":"numeric_df1[\"PutOuts\"].describe()","4fc1f252":"# assign median to infinite values in PutOuts\n\nnumeric_df1.loc[[9, 65, 132, 149, 186, 196, 198, 207, 249, 251, 267],\"PutOuts\"]=5.411646\nnumeric_df1[\"PutOuts\"].describe()","04ca2f81":"numeric_df1[numeric_df1[\"Assists\"].astype(\"str\").str.get(1)==\"i\"].index","c1aa3c66":"numeric_df1[\"Assists\"].describe()","000e4b4d":"# assign median to infinite values in Assists\n\nnumeric_df1.loc[[9, 65, 132, 149, 176, 186, 196, 198, 207, 249, 251, 255, 267, 304],\"Assists\"]=3.806662\nnumeric_df1[\"Assists\"].describe()","ad35f9e2":"df2=numeric_df1.copy()\n\n# LOF  Outlier Detection\n\nclf= LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf.fit_predict(df2)\n\ndf2_scores=clf.negative_outlier_factor_\nnp.sort(df2_scores)[0:10]","f0db8e12":"sns.boxplot(df2_scores);","5a8bec1f":"outlier_indexes=df2.loc[df2_scores< -1.73878565]\noutlier_indexes","85425a92":"# Throw away outliers from Salary feature also according to these indexes .","4d3910da":"y_df1=pd.DataFrame(y_df1).drop(index=[217,295])\ny_df1=y_df1.reset_index(drop=True)\nprint(y_df1.shape )\ny_df1.head(2)","ba8b7129":"df2= df2.loc[df2_scores> -1.73878565]\ndf2=df2.reset_index(drop=True)\nprint(df2.shape)\ndf2.head(2)","b6cc22a4":"# Throw away outliers from dummies also according to these indexes .","f757c9aa":"cat_df1=pd.DataFrame(cat_df1).drop(index=[217,295])\ncat_df1=cat_df1.reset_index(drop=True)\nprint(cat_df1.shape)\ncat_df1.head(2)","0e70ec89":"y_df1","5973fd55":"df3= pd.concat([df2,y_df1,cat_df1], axis=1)\ndf3","76dde0f5":"df2.head(2)","120e219a":"df2_columns=df2.columns\nstandardized_df2=preprocessing.scale(df2)\nstandardized_df2=pd.DataFrame(standardized_df2, columns=df2_columns)\nstandardized_df2.head(2)","59c65c69":"y_df10=hitters.dropna()[\"Salary\"]\ny_df10=pd.DataFrame(y_df10).drop(index=[217,295])\ny_df10=y_df10.reset_index(drop=True)\ny_df10.shape","206dce9e":"cat_df10=cat_df1.reset_index(drop=True)","2c6e478d":"df4= pd.concat([standardized_df2,y_df10,cat_df10], axis=1)\ndf4","834291a4":"df5=df4.copy()","5bf74d6e":"plt.figure(figsize=(12,10))\nsns.heatmap(df5.corr(), annot=True, cmap=plt.cm.Reds);","0b9fcaeb":"df5[\"walks\/cwalks\"]= df5[\"Walks\"]\/df5[\"CWalks\"]\ndf5[\"CAtBat\/Years\"]= df5[\"CAtBat\"]\/df5[\"Years\"]\ndf5[\"CHits\/Years\"]= df5[\"CHits\"]\/df5[\"Years\"]\ndf5[\"CHmRun\/Years\"]= df5[\"CHmRun\"]\/df5[\"Years\"]\ndf5[\"Hits\/CHits\"]= df5[\"Hits\"]\/df5[\"CHits\"]\ndf5[\"Assists\/Errors\"]= df5[\"Assists\"]\/df5[\"Errors\"]\ndf5[\"CHits\/CRBI\"]= df5[\"CHits\"]\/df5[\"CRBI\"]\ndf5[\"HmRun\/CHmRun\"]= df5[\"HmRun\"]\/df5[\"CHmRun\"]\ndf5[\"CRBI\/RBI\"]= df5[\"CRBI\"]\/df5[\"RBI\"]\ndf5[\"CRuns\/CHits\"]= df5[\"CRuns\"]\/df5[\"CHits\"]\ndf5[\"AtBat\/PutOuts\"]= df5[\"AtBat\"]\/df5[\"PutOuts\"]\ndf5[\"Walks\/Years\"]=df5[\"Walks\"]\/df5[\"Years\"]\n\nplt.figure(figsize=(14,12))\nsns.heatmap(df5.corr(), annot=True, cmap=plt.cm.Blues);","549edae6":"df6=df3.copy()\n\ndf6[\"walks\/cwalks\"]= df6[\"Walks\"]\/df6[\"CWalks\"]\ndf6[\"CAtBat\/Years\"]= df6[\"CAtBat\"]\/df6[\"Years\"]\ndf6[\"CHits\/Years\"]= df6[\"CHits\"]\/df6[\"Years\"]\ndf6[\"CHmRun\/Years\"]= df6[\"CHmRun\"]\/df6[\"Years\"]\ndf6[\"Hits\/CHits\"]= df6[\"Hits\"]\/df5[\"CHits\"]\ndf6[\"CHits\/CRBI\"]= df6[\"CHits\"]\/df6[\"CRBI\"]\ndf6[\"CRBI\/RBI\"]= df6[\"CRBI\"]\/df6[\"RBI\"]\ndf6[\"CRuns\/CHits\"]= df6[\"CRuns\"]\/df6[\"CHits\"]\ndf6[\"AtBat\/PutOuts\"]= df6[\"AtBat\"]\/df6[\"PutOuts\"]\ndf6[\"Walks\/Years\"]=df6[\"Walks\"]\/df6[\"Years\"]","d4944534":"y=df3[\"Salary\"]\nX=df3.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","1f9bf711":"y=df4[\"Salary\"]\nX=df4.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","d6b84636":"y=df5[\"Salary\"]\nX=df5.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","bf6d911b":"y=df6[\"Salary\"]\nX=df6.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","efeac826":"# LGB Feature Importance according to df6\n\n\ny=df6[\"Salary\"]\nX=df6.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nlgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n\nImportance = pd.DataFrame({'Importance':lgb_model.feature_importances_*100}, \n                          index = X_train.columns)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', figsize=(14,12))\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","34846b86":"# df7\n\n# Feature Selection( throw away the features which are not so important for LGBM)\n\ndf7=df6.copy()\ndf7= df7.drop(\"Division_W\", axis=1)\ndf7= df7.drop(\"League_N\", axis=1)","1ba4b19e":"y=df7[\"Salary\"]\nX=df7.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nlgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","73790fa0":"?LGBMRegressor","3941990e":"lgbm_params= { \"boosting_type\" : [\"dart\"],\n              \"learning_rate\": [0.09, 0.1,0.11, 0.2],\n              \"n_estimators\": [90,100,110,150],\n              \"num_leaves\" :[30,31,32],\n              \"max_depth\": [7,10],\n              \"colsample_bytree\": [1,0.8,0.5,0.4]}","bc3579d1":"lgbm_cv_model = GridSearchCV(lgb_model, \n                             lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose =2).fit(X_train, y_train)","5712a768":"lgbm_cv_model.best_params_","b47d37d3":"tuned_lgbm= LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred= tuned_lgbm.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))\n","609fc78e":"#Check train error to control overfitting\n\ntuned_lgbm2= LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred2= tuned_lgbm.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred2))","a18c7a66":"# LGB Feature Importance according to final df and final model\n\n\ny=df7[\"Salary\"]\nX=df7.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nlgb_model = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n\nImportance = pd.DataFrame({'Importance':lgb_model.feature_importances_*100}, \n                          index = X_train.columns)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', figsize=(14,12))\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","799fdb95":"\n## 1. Data Understanding\n> 1. K\u00fct\u00fcphaneler import edildi.Hitters veri setindeki g\u00f6zlem say\u0131s\u0131, de\u011fi\u015fken t\u00fcrleri, eksik de\u011ferler ve de\u011fi\u015fkenler aras\u0131 korelasyonlar incelendi.\n\n> 2. Salary de\u011fi\u015fkenine atama yap\u0131labilecek bir ili\u015fki var m\u0131 diye ara\u015ft\u0131r\u0131ld\u0131, Division kategorilerinin maa\u015flar\u0131nda anlaml\u0131 farkl\u0131l\u0131k oldu\u011fu g\u00f6zlemlense de, hedef de\u011fi\u015fken olmas\u0131 sebebiyle atama yap\u0131lmad\u0131.\n\n\n## 2. Data Preprocessing\n> 1. Veri seti,birbirinden farkl\u0131 4 \u00f6n i\u015fleme s\u00fcrecinden ge\u00e7ti. Nihai olarak 4 data frame olu\u015fturuldu. (df3-df4-df5-df6)\n\n> 2. df3 -> NA de\u011ferleri at\u0131ld\u0131.Skewness de\u011feri -1 ile 1 aras\u0131nda olmayanlar\u0131n sa\u011fa \u00e7arp\u0131k olduklar\u0131 distplot ile g\u00f6zlemlendi.\n\n> 3. df3 -> CAtBat - CHits - CHmRun - CRuns - CRBI - CWalks - PutOuts - Assists de\u011fi\u015fkenlerine log transformation yap\u0131ld\u0131. -\u221e (-inf) gelen de\u011ferlere median atamas\u0131 yap\u0131ld\u0131.\u00c7arp\u0131kl\u0131klar\u0131 giderildi.\n\n> 4. df3 -> df1 veri seti, kategorik, numerik ve hedef de\u011fi\u015fkene g\u00f6re split edildi. Kategorik de\u011fi\u015fkenlere dummy d\u00f6n\u00fc\u015f\u00fcm\u00fc yap\u0131ld\u0131.\n\n> 5. df3 -> LOC ile outlierlar tespit edildi. Boxplot ile incelendi, di\u011fer skorlardan \u00e7ok uzakta kalan 2 g\u00f6zlemin indexi belirlendikten sonra kategorik, numerik ve hedef de\u011fi\u015fken i\u00e7eren dflerden ayr\u0131 ayr\u0131 at\u0131ld\u0131.\n\n> 6. df3 -> concat ile kategorik, numerik ve hedef de\u011fi\u015fken i\u00e7eren df ler birle\u015ftirildi. 261 g\u00f6zlem i\u00e7eren nihai df olu\u015fturuldu ve df3 ismi verildi.\n\n> 7. df4 -> Yukar\u0131da olu\u015fturulan df3 standardize edildi. (mean 0, std=1)\n\n> 8. df5 -> Yukar\u0131da olu\u015fturulan df4 \u00fczerine yeni de\u011fi\u015fkenler eklendi. Bunlar: walks\/cwalks, CAtBat\/Years, CHits\/Years , CHmRun\/Years, Hits\/CHits, Assists\/Errors, CHits\/CRBI, HmRun\/CHmRun, CRBI\/RBI, CRuns\/CHits, AtBat\/PutOuts, Walks\/Years\n\n> 9. df6 -> Yukar\u0131da olu\u015fturulan df3 \u00fczerine yeni de\u011fi\u015fkenler eklendi. Bunlar: walks\/cwalks, CAtBat\/Years, CHits\/Years , CHmRun\/Years, Hits\/CHits, CHits\/CRBI, CRBI\/RBI, CRuns\/CHits, AtBat\/PutOuts, Walks\/Years   \n\n\n## 3. Modeling\n\n> 1. Yukar\u0131da belirtilen 4 data frame modellere fit ettirildi. ( Regression-Ridge- Lasso-Elastic Net- KNN - CART - RF- SVR - GBM - XGBoost - LightGBM - CatBoost)\n\n> 2. **df3 -> en iyi RMSE -> LightGBM 191.9**\n\n> 3. **df4 -> en iyi RMSE -> LightGBM 194.7**\n\n> 4. **df5 -> en iyi RMSE -> CatBoost 198.8**\n\n> 5. **df6 -> en iyi RMSE -> LightGBM 189.4**\n\n> 6. 4 data setinde genel olarak Light GBM ile daha d\u00fc\u015f\u00fck hata elde edildi. En iyi sonu\u00e7 ise 189 ile df6ya fit edilen Light GBM ile elde edildi. \n\n\n## 4. Tuning\n\n> 1. Light GBM feature selection ile \u00f6nemi \u00e7ok az olan 2 de\u011fi\u015fkeni att\u0131m -> **df7-> LightGBM 186.8**\n\n> 2. Hiperparametrelerden, boosting_type, learning_rate, n_estimators, num_leaves, max_depth, colsample_bytree de\u011ferleri denenerek en iyi model bulunmaya \u00e7al\u0131\u015f\u0131ld\u0131 -> **LightGBM 183.9**\n\n\n# Sonu\u00e7:\n> **Nihai RMSE 183.9 olarak hesapland\u0131.**\n\n> **Train verisi \u00fczerinde overfitting kontrol\u00fc yap\u0131ld\u0131 ve olmad\u0131\u011f\u0131 g\u00f6zlemlendi.**\n\n> **Nihai modele g\u00f6re en \u00f6nemli de\u011fi\u015fkenler saptand\u0131.**\n","a88295e8":"# DATA PREPROCESSING","a4260c53":"## df4 modeling","b94fd8f4":"## df5 modeling","9cda0683":"# TUNING","2105fafc":"# MODELING","b2cc0836":"## 1st Trial : df3\n\n* **df1-->df2-->df3**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* size= 322-->261","4142f971":"## df3 modeling","13ed1dc1":"## 3rd Trial : df5\n\n* **df4-->df5**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* standardize df2\n* generating variables\n* size= 322-->261","59e8f4f7":"## 4th Trial : df6\n\n* **df3-->df6**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* generating variables\n* size= 322-->261","6df219dd":"# REPORTING","5a705a65":"# Light GBM modeling for Hitters Data Set( RMSE: 183.9 )","63909928":"## Feature Importance","6234c263":"# Default Parameters\n\nboosting_type='gbdt',\n    num_leaves=31,\n    max_depth=-1,\n    learning_rate=0.1,\n    n_estimators=100,\n    subsample_for_bin=200000,\n    objective=None,\n    class_weight=None,\n    min_split_gain=0.0,\n    min_child_weight=0.001,\n    min_child_samples=20,\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n    random_state=None,\n    n_jobs=-1,\n    silent=True,\n    importance_type='split',\n    kwargs,","a3a82dee":"# DATA UNDERSTANDING","7590e850":"### LGB Feature Selection and LGBM : df7","7026ecb0":"## 2nd Trial : df4\n\n* **df2-->df10-->df4**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* standardize df2\n* size= 322-->261","97387d63":"## df6 modeling"}}