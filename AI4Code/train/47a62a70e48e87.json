{"cell_type":{"e3400719":"code","352b6fb4":"code","64d96076":"code","baafac0d":"code","80e4bfb3":"code","debe4d91":"code","a5258906":"code","28447b6b":"code","37eebfe6":"code","18e0afb5":"code","df2885b4":"code","d68e82ed":"code","22032684":"code","e1a70a8c":"code","e05eca3e":"code","79d5d085":"code","612dbc2d":"code","7db6754a":"code","bd365ef1":"code","3e0c7ecc":"code","a0b33312":"code","88372eff":"code","e5b59e1a":"code","f0ef8afd":"code","724cdf6e":"code","013f7f97":"code","e2b156f6":"code","a4919be4":"code","48e84eee":"code","ff6f7442":"code","7483f1ef":"code","4115dea5":"code","77330a04":"code","0cbf0374":"code","58f9d556":"code","0b94ec31":"code","e8cc6da5":"code","227309eb":"code","b01056dc":"code","dae2fe2a":"code","2b238746":"code","8cb45302":"code","6e9bd920":"code","0257f7d9":"code","30e02009":"code","fc3ec2be":"code","120539f1":"code","2cd11ae7":"code","aeb6b83f":"code","b46211a4":"code","79a4cc49":"code","0e25d437":"code","bef0fe71":"code","7b8eebf9":"code","8dbf182f":"code","51eb88e2":"code","f5a2ff61":"markdown","eed51c23":"markdown","796ae1c4":"markdown","2b1af70a":"markdown","e0943aa2":"markdown","04553575":"markdown","09ac9505":"markdown","28fbdf0e":"markdown","1e5f3350":"markdown","6e7307b6":"markdown"},"source":{"e3400719":"import pandas as pd","352b6fb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip install contractions\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport re\nimport logging\nimport itertools\nimport unicodedata\nimport string\nfrom nltk.corpus import stopwords\nimport contractions\n\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64d96076":"!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip\n!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\n!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\n!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip","baafac0d":"train_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\nsample_submission_df = pd.read_csv('sample_submission.csv')\ntest_labels_df = pd.read_csv('test_labels.csv')","80e4bfb3":"train_df.head()","debe4d91":"for i in ['toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate']:\n    print(\"{} :\".format(i.upper()))\n    print(train_df.loc[train_df[i]==1, 'comment_text'].sample().values, \"\\n\")","a5258906":"train_df.head()","28447b6b":"train_df.describe()","37eebfe6":"## Text cleaning\n\nimport re\n\nclass TextCleaningUtils:\n    '''\n        This class contains implementations of various text cleaning operations (Static Methods)\n    '''\n\n    cleaning_regex_map = {\n        'web_links': r'(?i)(?:(?:http(?:s)?:)|(?:www\\.))\\S+',\n#         'special_chars': r'[^a-zA-Z0-9\\s\\.,!?;:]+',\n        'special_chars': r'[^a-zA-Z\\s\\.,!?;:]+',  ## removing nums\n        'redundant_spaces': r'\\s\\s+',\n        'redundant_newlines': r'[\\r|\\n|\\r\\n]+',\n        'twitter_handles': r'[#@]\\S+',\n        'punctuations': r'[\\.,!?;:]+'\n    }\n    \n    def expand_abbreviations(text):\n        text = re.sub(r\"he's\", \"he is\", text)\n        text = re.sub(r\"there's\", \"there is\", text)\n        text = re.sub(r\"We're\", \"We are\", text)\n        text = re.sub(r\"That's\", \"That is\", text)\n        text = re.sub(r\"won't\", \"will not\", text)\n        text = re.sub(r\"they're\", \"they are\", text)\n        text = re.sub(r\"Can't\", \"Cannot\", text)\n        text = re.sub(r\"wasn't\", \"was not\", text)\n        text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n        text= re.sub(r\"aren't\", \"are not\", text)\n        text = re.sub(r\"isn't\", \"is not\", text)\n        text = re.sub(r\"What's\", \"What is\", text)\n        text = re.sub(r\"haven't\", \"have not\", text)\n        text = re.sub(r\"hasn't\", \"has not\", text)\n        text = re.sub(r\"There's\", \"There is\", text)\n        text = re.sub(r\"He's\", \"He is\", text)\n        text = re.sub(r\"It's\", \"It is\", text)\n        text = re.sub(r\"You're\", \"You are\", text)\n        text = re.sub(r\"I'M\", \"I am\", text)\n        text = re.sub(r\"shouldn't\", \"should not\", text)\n        text = re.sub(r\"wouldn't\", \"would not\", text)\n        text = re.sub(r\"couldn't\", \"could not\", text)\n        text = re.sub(r\"i'm\", \"I am\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", text)\n        text = re.sub(r\"I'm\", \"I am\", text)\n        text = re.sub(r\"Isn't\", \"is not\", text)\n        text = re.sub(r\"Here's\", \"Here is\", text)\n        text = re.sub(r\"you've\", \"you have\", text)\n        text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n        text = re.sub(r\"we're\", \"we are\", text)\n        text = re.sub(r\"what's\", \"what is\", text)\n        text = re.sub(r\"couldn't\", \"could not\", text)\n        text = re.sub(r\"we've\", \"we have\", text)\n        text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n        text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n        text = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", text)\n        text = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", text)\n        text = re.sub(r\"who's\", \"who is\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", text)\n        text = re.sub(r\"y'all\", \"you all\", text)\n        text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n        text = re.sub(r\"would've\", \"would have\", text)\n        text = re.sub(r\"it'll\", \"it will\", text)\n        text = re.sub(r\"we'll\", \"we will\", text)\n        text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n        text = re.sub(r\"We've\", \"We have\", text)\n        text = re.sub(r\"he'll\", \"he will\", text)\n        text = re.sub(r\"Y'all\", \"You all\", text)\n        text = re.sub(r\"Weren't\", \"Were not\", text)\n        text = re.sub(r\"Didn't\", \"Did not\", text)\n        text = re.sub(r\"they'll\", \"they will\", text)\n        text = re.sub(r\"DON'T\", \"DO NOT\", text)\n        text = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", text)\n        text = re.sub(r\"they've\", \"they have\", text)\n        text = re.sub(r\"they'd\", \"they would\", text)\n        text = re.sub(r\"i'd\", \"I would\", text)\n        text = re.sub(r\"should've\", \"should have\", text)\n        text = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", text)\n        text = re.sub(r\"where's\", \"where is\", text)\n        text = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", text)\n        text = re.sub(r\"i'll\", \"I will\", text)\n        text = re.sub(r\"weren't\", \"were not\", text)\n        text = re.sub(r\"They're\", \"They are\", text)\n        text = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", text)\n        text = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", text)\n        text = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", text)\n        text = re.sub(r\"let's\", \"let us\", text)\n        text = re.sub(r\"it's\", \"it is\", text)\n        text = re.sub(r\"can't\", \"cannot\", text)\n        text = re.sub(r\"don't\", \"do not\", text)\n        text = re.sub(r\"you're\", \"you are\", text)\n        text = re.sub(r\"i've\", \"I have\", text)\n        text = re.sub(r\"that's\", \"that is\", text)\n        text = re.sub(r\"i'll\", \"I will\", text)\n        text = re.sub(r\"doesn't\", \"does not\",text)\n        text = re.sub(r\"i'd\", \"I would\", text)\n        text = re.sub(r\"didn't\", \"did not\", text)\n        text = re.sub(r\"ain't\", \"am not\", text)\n        text = re.sub(r\"you'll\", \"you will\", text)\n        text = re.sub(r\"I've\", \"I have\", text)\n        text = re.sub(r\"Don't\", \"do not\", text)\n        text = re.sub(r\"I'll\", \"I will\", text)\n        text = re.sub(r\"I'd\", \"I would\", text)\n        text = re.sub(r\"Let's\", \"Let us\", text)\n        text = re.sub(r\"you'd\", \"You would\", text)\n        text = re.sub(r\"It's\", \"It is\", text)\n        text = re.sub(r\"Ain't\", \"am not\", text)\n        text = re.sub(r\"Haven't\", \"Have not\", text)\n        text = re.sub(r\"Hadn't\", \"Had not\", text)\n        text = re.sub(r\"Could've\", \"Could have\", text)\n        text = re.sub(r\"youve\", \"you have\", text)  \n        text = re.sub(r\"don\u00e5\u00abt\", \"do not\", text)  \n\n        return text\n    \n    def remove_emojis(text):\n        emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        text=emoji_clean.sub(r'',text)\n        url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n        text=url_clean.sub(r'',text)\n        return text\n    \n#     def remove_punctuations(text):\n#         punct_tag=re.compile(r'[^\\w\\s]')\n#         text=punct_tag.sub(r'',text)\n#         return text\n\n    @staticmethod\n    def clean_text_from_regex(text, text_clean_regex):\n        '''\n            Follow a particular cleaning expression, provided\n            as an input by an user to clean the text.\n        '''\n\n        text = text_clean_regex.sub(' ', text).strip()\n        return text\n\n    @staticmethod\n    def strip_html(text):\n        soup = BeautifulSoup(text, \"html.parser\")\n        return soup.get_text()\n\n    @staticmethod\n    def remove_special_chars(text):\n        '''\n            Replace any special character provided as default,\n            which is present in the text with space\n        '''\n\n        special_chars_regex = re.compile(TextCleaningUtils.cleaning_regex_map['special_chars'])\n        text = TextCleaningUtils.clean_text_from_regex(text, special_chars_regex)\n        return text\n\n    @staticmethod\n    def remove_redundant_spaces(text):\n        '''\n            Remove any redundant space provided as default,\n            that is present in the text.\n        '''\n\n        redundant_spaces_regex = re.compile(\n            TextCleaningUtils.cleaning_regex_map['redundant_spaces'])\n        text = TextCleaningUtils.clean_text_from_regex(text, redundant_spaces_regex)\n        return text\n\n    @staticmethod\n    def remove_web_links(text):\n        '''\n            Removes any web link that follows a particular default expression,\n            present in the text.\n        '''\n\n        web_links_regex = re.compile(TextCleaningUtils.cleaning_regex_map['web_links'])\n        text = TextCleaningUtils.clean_text_from_regex(text, web_links_regex)\n        return text\n\n    @staticmethod\n    def remove_twitter_handles(text):\n        '''\n            Removes any twitter handle present in the text.\n        '''\n\n        twitter_handles_regex = re.compile(TextCleaningUtils.cleaning_regex_map['twitter_handles'])\n        text = TextCleaningUtils.clean_text_from_regex(text, twitter_handles_regex)\n        return text\n\n    @staticmethod\n    def remove_redundant_newlines(text):\n        '''\n            Removes any redundant new line present in the text.\n        '''\n\n        redundant_newlines_regex = re.compile(\n            TextCleaningUtils.cleaning_regex_map['redundant_newlines'])\n        text = TextCleaningUtils.clean_text_from_regex(text, redundant_newlines_regex)\n        return text\n\n    @staticmethod\n    def remove_punctuations(text):\n        '''\n            Removes any punctuation that follows the default expression, in the text.\n        '''\n\n        remove_punctuations_regex = re.compile(TextCleaningUtils.cleaning_regex_map['punctuations'])\n        text = TextCleaningUtils.clean_text_from_regex(text, remove_punctuations_regex)\n        return text\n\n    @staticmethod\n    def remove_exaggerated_words(text):\n        '''\n            Removes any exaggerated word present in the text.\n        '''\n\n        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n\n    @staticmethod\n    def replace_multiple_chars(text):\n        '''\n            Replaces multiple characters present in the text.\n        '''\n\n        char_list = ['.', '?', '!', '#', '$', '\/', '@', '*', '(', ')', '+']\n        final_text = ''\n        for i in char_list:\n            if i in text:\n                pattern = \"\\\\\" + i + '{2,}'\n                repl_str = i.replace(\"\\\\\", \"\")\n                text = re.sub(pattern, repl_str, text)\n                final_text = ' '.join(text.split())\n        return final_text\n\n    @staticmethod\n    def replace_sign(text):\n        '''\n            Replaces any sign with words like & with 'and', in the text.\n        '''\n        sign_list = {'&': ' and ', '\/': ' or ', '\\xa0': ' '}\n        final_text = ''\n        for i in sign_list:\n            if i in text:\n                text = re.sub(i, sign_list[i], text)\n                final_text = ' '.join(text.split())\n        return final_text\n\n    @staticmethod\n    def remove_accented_char(text):\n        text = unicodedata.normalize('NFD', text) \\\n            .encode('ascii', 'ignore') \\\n            .decode(\"utf-8\")\n        return str(text)\n\n    @staticmethod\n    def replace_characters(text, replace_map):\n        '''\n            Replaces any character custom provided by an user.\n        '''\n\n        for char, replace_val in replace_map.items():\n            text = text.replace(char, replace_val)\n        return text\n    \ndef clean_data(df,col_to_clean):\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.expand_abbreviations)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_emojis)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_special_chars)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_spaces)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_punctuations)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_exaggerated_words)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_redundant_newlines)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_twitter_handles)\n    df[col_to_clean] = df[col_to_clean].apply(TextCleaningUtils.remove_web_links)\n    df[col_to_clean] = df[col_to_clean].astype(str)\n    df[col_to_clean] = df[col_to_clean].str.lower()\n    \n    return df","18e0afb5":"%%time\ntrain_df = clean_data(train_df, 'comment_text')","df2885b4":"train_df['clean'] = np.where((train_df['toxic']==0) & (train_df['severe_toxic']==0) & (train_df['obscene']==0) & (train_df['threat']==0) & (train_df['insult']==0) & (train_df['identity_hate']==0), 1,0)","d68e82ed":"train_df.head(10)","22032684":"from nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize","e1a70a8c":"train_df['sentence']=train_df['comment_text'].apply(lambda x: sent_tokenize(x))\n\ntrain_df['tokens']=train_df['comment_text'].apply(lambda x: word_tokenize(x))","e05eca3e":"from nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\n\ndef get_clean_token_list(words):\n    clean_words=[]\n    for word in words:\n        flag=True\n        for letter in word:\n            if letter.isdigit() or letter in string.punctuation:\n                flag=False\n        if flag:\n            clean_words.append(word)\n    return clean_words\n\nprint(get_clean_token_list([\"hewy\",\"ho!w\",\"are\",\"yo6u\",\"?dd\",\"111\",\"qwerty\"]))\n\ndef remove_stopwords(words):\n    \"\"\"\n    pass series get series\n    \"\"\"\n    filtered_sent=[]\n    for word in words:\n        if word not in stop_words:\n            filtered_sent.append(word)\n    return filtered_sent\n\n\nprint(remove_stopwords([\"hey\",\"how\",\"are\",\"you\",\"?\"]))","79d5d085":"train_df['tokens']=train_df['tokens'].apply(remove_stopwords)\n\ntrain_df['tokens']=train_df['tokens'].apply(get_clean_token_list)\n\ntrain_df['tokens'].head()","612dbc2d":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer","7db6754a":"stem = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef stem_sentence(sentence):\n    stem_sentence=[]\n    for word in sentence:\n        stem_sentence.append(stem.stem(word))\n    return stem_sentence\n\ndef lemmatize_sentence(sentence):\n    lem_sentence=[]\n    for word in sentence:\n        lem_sentence.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n    return lem_sentence","bd365ef1":"train_df['stem_sentence']=train_df['tokens'].apply(lambda x: stem_sentence(x))\n\n\n\ntrain_df['lemm_sentence']=train_df['tokens'].apply(lambda x: lemmatize_sentence(x))\n\n","3e0c7ecc":"train_df[['tokens','stem_sentence']].head(10)","a0b33312":"train_df[['tokens','lemm_sentence']].head(10)","88372eff":"from nltk.probability import FreqDist","e5b59e1a":"reviews=[word for review in train_df['lemm_sentence'] for word in review]\n\nfdist_reviews = FreqDist(reviews)\n\nfdist_reviews.most_common(20)","f0ef8afd":"toxic_reviews=train_df[train_df['toxic']==1]['lemm_sentence']\n\ntoxic_tokens=[word for review in toxic_reviews for word in review]\n\nfdist_toxic_tokens = FreqDist(toxic_tokens)\n\nfdist_toxic_tokens.most_common(20)","724cdf6e":"def get_most_common_tokens(df,label):\n    label_reviews=df[df['toxic']==1]['lemm_sentence']\n    label_tokens=[word for review in label_reviews for word in review]\n    fdist_label_tokens = FreqDist(label_tokens)\n    print(fdist_label_tokens.most_common(20))\n    return fdist_label_tokens","013f7f97":"train_df.columns","e2b156f6":"severe_toxic_reviews=train_df[train_df['severe_toxic']==1]['lemm_sentence']\n\nsevere_toxic_tokens=[word for review in severe_toxic_reviews for word in review]\n\nfdist_severe_toxic_tokens = FreqDist(severe_toxic_tokens)\n\nfdist_severe_toxic_tokens.most_common(20)","a4919be4":"import nltk","48e84eee":"train_df.head()","ff6f7442":"train_df['pos_tag']=train_df['lemm_sentence'].apply(lambda x: nltk.pos_tag(x))\n\ntrain_df['pos_tag'].head()","7483f1ef":"keep_tags=[\"JJ\",\"JJR\",\"JJS\",\"RB\",\"RBR\",\"RBS\",\"UH\",\"NN\",\"NNS\",\"NNP\",\"NNPS\"]\n\ndef filter_tag(pos_list):\n    pos_clean_list=[]\n    for t in pos_list:\n        if t[1] in keep_tags:\n            pos_clean_list.append(t[0])\n    return pos_clean_list\n\nfilter_tag([('asked', 'RB'), ('review', 'NN'), ('scale', 'RBS')])","4115dea5":"train_df['filter_pos_tag']=train_df['pos_tag'].apply(filter_tag)\n\ntrain_df[['lemm_sentence','filter_pos_tag']].head(10)","77330a04":"train_df['clean_sentence']=train_df['filter_pos_tag'].apply(lambda x: ' '.join(x))","0cbf0374":"train_df.head()","58f9d556":"clean_reviews=train_df[(train_df['toxic']==0)&(train_df['severe_toxic']==0)&(train_df['obscene']==0)&(train_df['threat']==0)&(train_df['insult']==0)&(train_df['identity_hate']==0)]['filter_pos_tag']\n\nclean_tokens=[word for review in clean_reviews for word in review]\n\nfdist_clean_tokens = FreqDist(clean_tokens)\n\nfdist_clean_tokens.most_common(20)","0b94ec31":"def get_most_common_tokens(df,label):\n    label_reviews=df[df[label]==1]['lemm_sentence']\n    label_tokens=[word for review in label_reviews for word in review]\n    fdist_label_tokens = FreqDist(label_tokens)\n#     print(fdist_label_tokens.most_common(20))\n    return fdist_label_tokens","e8cc6da5":"train_df.columns","227309eb":"label_list = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']","b01056dc":"for label in label_list:\n    print(label.upper())\n    get_most_common_tokens(train_df,label)","dae2fe2a":"train_df.head(20)","2b238746":"train_df[train_df['clean_sentence'].str.contains(\"hahahahahahahahahahahahahahahahahahahahahahahahahahahahahahaha\")]","8cb45302":"def clean_regex(word):\n    lol_word = re.sub(\n                r\"\"\"(?ix)\\b    # assert position at a word boundary\n                (?=lol)        # assert that \"lol\" can be matched here\n                \\S*            # match any number of characters except whitespace\n                (\\S+)          # match at least one character (to be repeated later)\n                (?<=\\blol)     # until we have reached exactly the position after the 1st \"lol\"\n                \\1*            # then repeat the preceding character(s) any number of times\n                \\b             # and ensure that we end up at another word boundary\"\"\", \n                \"lol\", word)\n    if lol_word != word:\n        return lol_word\n                    \n    fuck_word = re.sub(\n        r\"\"\"(?ix)\\b    # assert position at a word boundary\n        (?=fuck)        # assert that \"lol\" can be matched here\n        \\S*            # match any number of characters except whitespace\n        (\\S+)          # match at least one character (to be repeated later)\n        (?<=\\bfuck)     # until we have reached exactly the position after the 1st \"lol\"\n        \\1*            # then repeat the preceding character(s) any number of times\n        \\b             # and ensure that we end up at another word boundary\"\"\", \n        \"fuck\", word)\n    if fuck_word != word:\n        return fuck_word\n    \n    haha_word = re.sub(\n        r\"\"\"(?ix)\\b    # assert position at a word boundary\n        (?=haha)        # assert that \"lol\" can be matched here\n        \\S*            # match any number of characters except whitespace\n        (\\S+)          # match at least one character (to be repeated later)\n        (?<=\\bhaha)     # until we have reached exactly the position after the 1st \"lol\"\n        \\1*            # then repeat the preceding character(s) any number of times\n        \\b             # and ensure that we end up at another word boundary\"\"\", \n        \"haha\", word)\n    if haha_word != word:\n        return haha_word\n    \n    cunt_word = re.sub(\n        r\"\"\"(?ix)\\b    # assert position at a word boundary\n        (?=cunt)        # assert that \"lol\" can be matched here\n        \\S*            # match any number of characters except whitespace\n        (\\S+)          # match at least one character (to be repeated later)\n        (?<=\\bcunt)     # until we have reached exactly the position after the 1st \"lol\"\n        \\1*            # then repeat the preceding character(s) any number of times\n        \\b             # and ensure that we end up at another word boundary\"\"\", \n        \"cunt\", word)\n    if cunt_word != word:\n        return cunt_word\n    \n    ty_word = re.sub(\n        r\"\"\"(?ix)\\b    # assert position at a word boundary\n        (?=ty)        # assert that \"lol\" can be matched here\n        \\S*            # match any number of characters except whitespace\n        (\\S+)          # match at least one character (to be repeated later)\n        (?<=\\bty)     # until we have reached exactly the position after the 1st \"lol\"\n        \\1*            # then repeat the preceding character(s) any number of times\n        \\b             # and ensure that we end up at another word boundary\"\"\", \n        \"ty\", word)\n    if ty_word != word:\n        return ty_word\n    return ''","6e9bd920":"def remove_repititative_char(sentence):\n    clean_sentence = []\n    for word in sentence.split():\n        if len(word)>23:\n            new_word = clean_regex(word)\n#             new_word = re.sub(r'([a-z])\\1+', r'\\1', word)\n#             print(word)\n#             print(new_word)\n            clean_sentence.append(new_word)\n        else:\n            clean_sentence.append(word)\n    return ' '.join(clean_sentence)\n    \nremove_repititative_char(\"fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck\")","0257f7d9":"train_df['clean_sentence'] = train_df['clean_sentence'].apply(lambda x: remove_repititative_char(x))","30e02009":"category_text = train_df[train_df['clean']!=1]['clean_sentence'].values\ncategory_text = \" \".join(category_text)\nlong_len_words = [word for word in category_text.split() if len(word)>23]\nprint(\"num of long length words in clean text : {}\".format(len(long_len_words)))\nlong_len_words","fc3ec2be":"# Word Cloud","120539f1":"from PIL import Image\ndef display_cloud(label):\n    plt.subplots(figsize=(10,10))\n    text = train_df[train_df[label]==1][\"clean_sentence\"].tolist()\n    wc = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=1800,\n                          height=800,\n                         )\n    wc.generate(' '.join(text))\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.title(label,fontsize=36)\n    plt.show()","2cd11ae7":"for label in label_list:\n    display_cloud(label)","aeb6b83f":"def gram_analysis(data,gram):\n    stop_words_set = set(stopwords.words('english'))\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stop_words_set]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens","b46211a4":"def gram_freq(df, gram, categ_col, text_col):\n    category_text = \" \".join(df[df[categ_col]==1][text_col].sample(200).values)\n    toks = gram_analysis(category_text, gram)\n    tok_freq = pd.DataFrame(data=[toks, np.ones(len(toks))]).T.groupby(0).sum().reset_index()\n    tok_freq.columns = ['token','frequency']\n    tok_freq = tok_freq.sort_values(by='frequency',ascending=False)\n    \n    plt.figure(figsize=(15,8))\n    plt.title(\"{} most common tokens\".format(categ_col))\n    sns.barplot(x='token', y='frequency', data=tok_freq.iloc[:30])\n    plt.xticks(rotation=90)\n    plt.show()\n    \n    return ","79a4cc49":"for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'clean']:\n    gram_freq(train_df,2, i, 'comment_text')","0e25d437":"for i in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'clean']:\n    gram_freq(train_df,3, i, 'comment_text')","bef0fe71":"train_df.head()","7b8eebf9":"%%time\ntfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\ntrain_tfidf=tfidf_vect.fit_transform(train_df['clean_sentence'].values.tolist())\ntrain_tfidf.shape","8dbf182f":"## Outputs from the TF-IDF transformed data\nprint(train_tfidf)","51eb88e2":"#Count Vectorization\nimport matplotlib\nimport matplotlib.pyplot as plt\ndef vectorize(data):\n    cv=CountVectorizer()\n    fit_data_cv=cv.fit_transform(data)\n    return fit_data_cv,cv\n\n#Tfidf vectorization from sklearn\ndef tfidf(data):\n    tfidfv=TfidfVectorizer()\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_cv,tfidfv\n\ndef dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        \n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label        )\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Negative Review')\n        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Negative Review')\n        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Negtive Review')\n        color_orange=mpatches.Patch(color='orange',label='Positive Review')\n        plt.legend(handles=[color_orange,color_red])\n        plt.title(\"TSNE\")\n        plt.show()\ntrain_data=train_df       \ndata_vect=train_data['clean_sentence'].values\ntrain_data_cv,cv= vectorize(data_vect)\n\ndata_vect_good=count_good['review'].values\ntarget_vect=train_data['Binary'].values\ntarget_data_vect_good=train_df[train_df['sentiment']=='positive']['Binary'].values\ndata_vect_bad=count_bad['review'].values\ntarget_data_vect_bad=train_df[train_df['sentiment']=='positive']['Binary'].values\n\nreal_review_train_data_cv,cv=vectorize(data_vect_good)\n\nprint(train_data.head())\ndimen_reduc_plot(train_data_cv,target_vect,1)\ndimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,1)\ndimen_reduc_plot(real_review_train_data_cv,target_data_vect_bad,1)\n# dimen_reduc_plot(train_data_cv,target_vect,3)\n# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,3)\n# dimen_reduc_plot(train_data_cv,target_vect,2)\n# dimen_reduc_plot(real_review_train_data_cv,target_data_vect_good,2)\n","f5a2ff61":"## Word Cloud","eed51c23":"## Bi-gram analysis\n- Most common bigrams in each category","796ae1c4":"### Closer look at long words ","2b1af70a":"# Text cleaning\n\nCleaning to remove the following nature of elements from text\n\n- HTML codes\n- URLs\n- Emojis\n- Stopwords\n- Punctuations\n- Expanding Abbreviations","e0943aa2":"## Vectorization and Embeddings","04553575":"### Sample comments corresponding to each category","09ac9505":"We will try with TFIDF and Count vectorization strategies and will be going in further sections.\n\n- [TF-IDF Vectorization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html): This works by applying a logarithmic term to inverse document frequency (IDF) part other than determining the \"TF\" or term freqency part. The formulation can be shown as follows:\n\n<img src=\"https:\/\/plumbr.io\/app\/uploads\/2016\/06\/tf-idf.png\">\n\n- [Count Vectorization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html): This is a simpler vectorization technique which relies on frequency of occurence of a particular term in a document or corpus.\n\nA pictorial representation about the way in which vectorization occurs is provided:\n\n<img src=\"https:\/\/www.oreilly.com\/library\/view\/applied-text-analysis\/9781491963036\/assets\/atap_0408.png\">\n\nLet us now go ahead and vectorize the corpus and test its dimensionality.","28fbdf0e":"# Problem:\n\nComments belong to the following categories: \n- Toxic\n- Severe_toxic\n- Obscene\n- Threat\n- Insult\n- Identity_hate \n\nEach record could have multiple labels on. ","1e5f3350":"## Visualizing the  Vector Space\n\nAs words and sentences are vectorized, the dimensions of the vector space becomes significantly large to be accomodated in a model. For any computation system it is recommended to keep the dimensions of a tensor (matrix) as small as possible and maintain its regularity.  For tensors with larger dimensions and irregular shapes, it is difficult for the system to perform any operation (matrix \/tensor multiplication etc.). Complex operations like tensor differentiation (Jacobian) or numerical approximation is another difficult thing to do for large matrices. The rank plays an important aspect for these operations.\n\nNow, we have to reduce the dimensions ,else the kernel will run out of memory. For this  we wmploy 3 different decomposition techniques:\n\n- [PCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n- [SVD](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html)\n- [TSNE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html)\n\nThese algorithms rely on Eigen vector decomposition and Eigen matrices for creating smaller matrices. These reduced matrices are well-fitted to perform any numerical approximation tasks from differentiation to higher order non linear dynamics. [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) in general is a well known method and forms the base of all decomposition techniques.  Pictorially it operates as follows with the help of orthogonal Eigen vectors:\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_ica_vs_pca_thumb.png\">\n\n\nTSNE is a more sophisticated [method](https:\/\/lvdmaaten.github.io\/publications\/papers\/AISTATS_2009.pdf)  which uses a non convex optimization along with gradient descent. This is different than Eigen Vector (convex optimization) method of PCA and hence different results may be obtained in different iterations. It is a memory intensive method and is often powerful at the expense of longer execution time.\n\n<img src=\"https:\/\/miro.medium.com\/max\/685\/1*njEd7PiqBW-zW38E23Ho9w.png\">\n","6e7307b6":"## Tri-gram analysis\n- Most common trigrams in each category"}}