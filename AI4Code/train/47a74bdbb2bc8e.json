{"cell_type":{"9e0cde7b":"code","6a958744":"code","ed41a168":"code","1a920783":"code","c06cd974":"code","45307eba":"code","8d91364f":"code","28ec3b45":"code","cc5f7f4a":"code","0288a351":"code","6959ea20":"code","c30ba12e":"code","f2b11c42":"code","be35de70":"markdown","20bdf769":"markdown","e6b65713":"markdown","1d9b3a98":"markdown","5050695a":"markdown","da1e0371":"markdown","4f7e9fc9":"markdown","bd537490":"markdown","27342656":"markdown","a2bc07c1":"markdown","242f7d70":"markdown","4d007fb1":"markdown","094b1b69":"markdown"},"source":{"9e0cde7b":"import pandas as pd\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n","6a958744":"directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/annotations\"\nimage_directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\"\ndf = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/submission.csv\")","ed41a168":"cvNet = cv2.dnn.readNetFromCaffe('..\/input\/caffe-face-detector-opencv-pretrained-model\/architecture.txt',\n                                 '..\/input\/caffe-face-detector-opencv-pretrained-model\/weights.caffemodel')","1a920783":"def getJSON(filePathandName):\n    with open(filePathandName,'r') as f:\n        return json.load(f)\ndef adjust_gamma(image, gamma=1.0):\n    invGamma = 1.0 \/ gamma\n    table = np.array([((i \/ 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))","c06cd974":"jsonfiles= []\nfor i in os.listdir(directory):\n    jsonfiles.append(getJSON(os.path.join(directory,i)))\njsonfiles[0]","45307eba":"df.head()","8d91364f":"data = []\nimg_size = 124\nmask = ['face_with_mask']\nnon_mask = [\"face_no_mask\"]\nlabels={'mask':0,'without mask':1}\nfor i in df[\"name\"].unique():\n    f = i+\".json\"\n    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n        if j[\"classname\"] in mask:\n            x,y,w,h = j[\"BoundingBox\"]\n            img = cv2.imread(os.path.join(image_directory,i),1)\n            img = img[y:h,x:w]\n            img = cv2.resize(img,(img_size,img_size))\n            data.append([img,labels[\"mask\"]])\n        if j[\"classname\"] in non_mask:\n            x,y,w,h = j[\"BoundingBox\"]\n            img = cv2.imread(os.path.join(image_directory,i),1)\n            img = img[y:h,x:w]\n            img = cv2.resize(img,(img_size,img_size))    \n            data.append([img,labels[\"without mask\"]])\nrandom.shuffle(data)\n\np = []\nfor face in data:\n    if(face[1] == 0):\n        p.append(\"Mask\")\n    else:\n        p.append(\"No Mask\")\nsns.countplot(p)","28ec3b45":"X = []\nY = []\nfor features,label in data:\n    X.append(features)\n    Y.append(label)\n\nX = np.array(X)\/255.0\nX = X.reshape(-1,124,124,3)\nY = np.array(Y)","cc5f7f4a":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\nxtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,    \n        rotation_range=15,    \n        width_shift_range=0.1,\n        height_shift_range=0.1,  \n        horizontal_flip=True,  \n        vertical_flip=False)\n\ndatagen.fit(xtrain)\n\nhistory = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n                    epochs=20,\n                    verbose=1,\n                    validation_data=(xval, yval))","0288a351":"plt.plot(history.history['accuracy'],'g')\nplt.plot(history.history['val_accuracy'],'b')\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","6959ea20":"plt.plot(history.history['loss'],'g')\nplt.plot(history.history['val_loss'],'b')\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","c30ba12e":"model.save(\"Mask_Model.h5\")","f2b11c42":"test_images = ['2756.png','5342.jpg', '4591.png','3939.png','3939.png','3911.png']\n\ngamma = 2.0\nfig = plt.figure(figsize = (14,14))\nrows = 3\ncols = 2\naxes = []\nassign = {'0':'Mask','1':\"No Mask\"}\nfor j,im in enumerate(test_images):\n    image =  cv2.imread(os.path.join(image_directory,im),1)\n    image =  adjust_gamma(image, gamma=gamma)\n    (h, w) = image.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n    cvNet.setInput(blob)\n    detections = cvNet.forward()\n    for i in range(0, detections.shape[2]):\n        try:\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n            frame = image[startY:endY, startX:endX]\n            confidence = detections[0, 0, i, 2]\n            if confidence > 0.2:\n                im = cv2.resize(frame,(img_size,img_size))\n                im = np.array(im)\/255.0\n                im = im.reshape(1,124,124,3)\n                result = model.predict(im)\n                if result>0.5:\n                    label_Y = 1\n                else:\n                    label_Y = 0\n                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n        \n        except:pass\n    axes.append(fig.add_subplot(rows, cols, j+1))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()","be35de70":"Using the mask and the non_mask labels, the bounding box data of the json files is extracted. The faces of a particular image are extracted and stored in the data list with its tag for the learning process.","20bdf769":"# **Data Processing**\n\nThe next step is now to explore the JSON data provided for the training.\nThe Annotations field contains the data of all the faces present in a particular image.\nThere are different class names, but the real class names are face_with_mask and face_no_mask.","e6b65713":"# **Training Model**","1d9b3a98":"# **Approach**\n\n* Extract face data for training.\n* Train the classifier to classify faces in mask or labels without a mask.\n* Detect faces while testing data using SSD face detector.\n* Using the trained classifier, classify the detected faces.\n\nIn the third step of the above process, you have to think about what is the SSD face detector? Well, the **SSD is a Single Shot Multibox Detector**. This is a technique used to detect objects in images using a single deep neural network.\n\nIt is used for the detection of objects in an image. Using a basic architecture of the VGG-16 architecture, the SSD can outperform other object detectors such as YOLO and Faster R-CNN in terms of speed and accuracy.\n\n![](https:\/\/www.researchgate.net\/profile\/Adam_Nowosielski\/publication\/332948824\/figure\/fig5\/AS:767146284036100@1559913335810\/The-model-of-Single-Shot-MultiBox-Detector-SSD-25.ppm)","5050695a":"# **Testing The Model**","da1e0371":"# **Load Data**\n\nNow, let\u2019s get started with the task of Face Mask Detection with Machine Learning by using the Python programming language. I will start this task by importing the necessary Python libraries that we need for this task","4f7e9fc9":"1. The getJSON function retrieves the json file containing the bounding box data in the training dataset.\n\n\n2. The adjust_gamma function is a non-linear operation used to encode and decode luminance or tristimulus values in video or still image systems. Simply put, it is used to instil a little bit of light into the image. If gamma <1, the image will shift to the darker end of the spectrum and when gamma> 1, there will be more light in the image.\n* Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.\n* Enhancing an image provides better contrast and a more detailed image as compare to non enhanced image. Image enhancement has very applications. It is used to enhance medical images, images captured in remote sensing, images from satellite e.t.c\n","bd537490":"# **Introduction**\n\n\nFace mask detection has a range of applications from capturing the movement of the face to facial recognition which at first requires the face to be detected with very good precision. Face detection is more relevant today as it is not only used on images, but also in video applications like real-time surveillance and face detection in videos.\n\nHigh precision image classification is now possible with advances in convolutional networks. Pixel level information is often needed after face detection, which most face detection methods do not provide.\n\nObtaining pixel-level detail has been a difficult part of semantic segmentation. Semantic segmentation is the process of assigning a label to each pixel in the image.\n\n![](https:\/\/www.logmask.com\/images\/mask-detection-sample.jpg)","27342656":"**Training and Validation Visualizations**","a2bc07c1":"The visualization above tells us that the number of mask images> Number of images without a mask, so this is an unbalanced dataset. But since we\u2019re using a pre-trained SSD model, which is trained to detect unmasked faces, this imbalance wouldn\u2019t matter much.\n\nlet\u2019s reshape the data before training a neural network","242f7d70":"Now the next step is to train a Neural Network for the task of Face Mask Detection with Machine Learning","4d007fb1":"**Creating Helper Functions**","094b1b69":"By analyzing the output above, we can observe that the whole system works well for faces that have spatial dominance. But fails in the case of images where the faces are small and take up less space in the overall image.\n\nFor best results, different image preprocessing techniques can be used, or the confidence threshold can be kept lower, or one can try different blob sizes.\n\nHope you liked this article on face mask detection with machine learning using the Python programming language. Please feel free to ask your valuable questions in the comments section below."}}