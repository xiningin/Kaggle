{"cell_type":{"a5074179":"code","9c57bd6d":"code","77fa00e4":"code","973f8a85":"code","b22e614e":"code","0ee69439":"code","a2552233":"code","ab4bd6be":"code","eee1dd08":"code","a8b39461":"code","d2e4f97f":"code","615521af":"code","c31bffb6":"code","8f797699":"code","dbcf72bb":"code","7eac3d5f":"code","796cb27a":"code","e651e150":"code","429ccd3c":"markdown","1e7a588a":"markdown","ccf106a9":"markdown","151ed1e8":"markdown","5bad6d54":"markdown","75f890d4":"markdown"},"source":{"a5074179":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c57bd6d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport theano\nimport tensorflow\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom sklearn.experimental import enable_halving_search_cv  \nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.model_selection import HalvingRandomSearchCV","77fa00e4":"train=pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\npseudo_label = pd.read_csv('..\/input\/pseudosubforann\/predforpseudo.csv', index_col=0)","973f8a85":"print(train.shape)\nprint(test.shape)\nprint(pseudo_label.shape)","b22e614e":"test['Survived'] = [x for x in pseudo_label.Survived]\ntrain['Survived'].value_counts()\ntrain.isnull().sum()\ntest.isnull().sum()\n#train.head()\ntrain['Ticket'].nunique()\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\ntrain_copy = train.copy()\ntest_copy = test.copy()\ntrain = train.drop(['PassengerId', 'Ticket'], axis = 1)\ntest = test.drop(['PassengerId', 'Ticket',], axis = 1)\ncombine = [train, test]\n\ntrain['Cabin'].fillna('U', inplace=True)\ntrain['Cabin'] = train['Cabin'].apply(lambda x: x[0])\n\ntest['Cabin'].fillna('U', inplace=True)\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0])\n\ntrain['Cabin'].unique()\nfor dataset in combine:\n  dataset['Cabin'] = dataset['Cabin'].fillna('U')\n  dataset['Cabin'] = dataset['Cabin'].apply(lambda x: x[0])\n  \npd.crosstab(train['Cabin'], train['Survived'])\ntrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index = False).mean().sort_values(by = 'Survived', ascending = True)\ncabin_mapping = {\"T\": 0, \"U\": 1, \"A\": 2, \"G\": 3, \"C\": 4, \"F\": 5, \"B\": 6, \"E\": 7, \"D\": 8}\nfor dataset in combine:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)\n    dataset['Cabin'] = dataset['Cabin'].fillna(0)\n\n#train.head()\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].map(lambda x: x.split(',')[1].split('.')[0].strip())\n\npd.crosstab(train['Title'], train['Sex'])\n\n\n# for dataset in combine:\n#     dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n\n#     dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n#     dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n#     dataset['Title'] = dataset['Title'].replace('Sir', 'Mr')\n#     dataset['Title'] = dataset['Title'].replace('Dr', 'Mr')\n    \n# train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n# title_mapping = {\"Mrs\": 4, \"Miss\": 3, \"Mr\": 1, \"Master\": 2, \"Rare\": 0}\n# for dataset in combine:\n#     dataset['Title'] = dataset['Title'].map(title_mapping)\n#     dataset['Title'] = dataset['Title'].fillna(0)\n\n#train.head()\n\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n#train.head()\n# Imputing Missing Values\n\ntrain['Age'].fillna(train['Age'].dropna().median(), inplace=True)\ntest['Age'].fillna(train['Age'].mean(), inplace = True)\ntest['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\ntrain['Embarked'].fillna('C', inplace = True)\ntest['Embarked'].fillna('C', inplace = True)\ntrain['Fare'].fillna(train['Fare'].dropna().median(), inplace = True)\ntrain.drop('Title',axis=1,inplace=True)\ntest.drop('Title',axis=1,inplace=True)\n\ntrain.isnull().sum()\ntest.isnull().sum()\n\ntrain['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n\n\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 0\n    \n#train.head()\n\n\n\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n'''\nfor dataset in combine:    \n    dataset.loc[ dataset['FamilySize'] > 4, 'FamilySize'] = 0  \n    dataset.loc[ dataset['FamilySize'] <= 4, 'FamilySize'] = 1\n    \ntrain.head()\n\n'''\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n#train.head()\n\ntrain['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n\n\n\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain = train.drop(['FareBand'], axis=1)\n#combine = [train, test]\ndata = pd.concat([train, test], axis=0)\n\n#train.head(5)\n\n# train = train.drop(['AgeBand', 'Name', 'SibSp', 'Parch' ], axis = 1)\n# test = test.drop(['Name', 'SibSp', 'Parch'], axis = 1)\n# # splitting the dataset into x(independent variables) and y(dependent variables)\n\n# x_train = train.drop('Survived', axis = 1)\n# y_train = train.Survived\n\n# # print(x_train.shape)\n# # print(y_train.shape)\n\n# x_test = test\n\n# # print(x_test.shape)\n# horizontal_stack = pd.concat([x_train, y_train], axis=1)\n# horizontal_stack.index+=1\n# train_df=horizontal_stack\n# # train_df\n# x_test.index+=100000\n# test_df=x_test\n# #testcopy=test_df.copy()\n","0ee69439":"data = pd.concat([train, test], axis=0)\ntrain = data.iloc[:train.shape[0]]\ntest = data.iloc[train.shape[0]:].drop(columns=['Survived'])","a2552233":"train.columns","ab4bd6be":"test.columns","eee1dd08":"lab_cols = ['Pclass','Age', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ntarget = 'Survived'\n\nfeatures_selected = ['Pclass', 'Sex', 'Age','Embarked','Parch','SibSp','Fare','Cabin']\n\nX = data.drop(target, axis=1)\nX = X[features_selected]\ny = data[target]\n\ntest = test[features_selected]","a8b39461":"X.shape","d2e4f97f":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=101, test_size=0.25)","615521af":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=8))\n    classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=64,kernel_initializer='uniform',activation='relu'))\n    classifier.add(Dropout(rate = 0.2))\n#     classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n#     classifier.add(Dropout(rate = 0.2))\n    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n    return classifier","c31bffb6":"%%time\nclassifier = KerasClassifier(build_fn = build_classifier)\nparam_grid = dict(optimizer = ['Adam'],\n                  epochs=[50,100,200,250],\n                  batch_size=[16,32,64,128])\ngrid = HalvingRandomSearchCV(classifier, param_grid, scoring='accuracy',cv=10,n_jobs=-1,random_state=101)\ngrid_result = grid.fit(X_train, y_train)\nbest_parameters = grid.best_params_\nbest_accuracy = grid.best_score_","8f797699":"print(grid.best_params_)\nprint(grid.best_score_)","dbcf72bb":"test_df=test.copy()\ntest_df.index+=100000","7eac3d5f":"test_preds = grid.predict_proba(test_df)[:,1]\ntest_preds","796cb27a":"threshold = pd.Series(test_preds).sort_values(ascending = False).head(34911).values[-1]\nprint(f\"Current threshold is: {threshold}\")","e651e150":"# yfinal = (test_preds > threshold).astype(int).reshape(test_df.shape[0])\n# output = pd.DataFrame({'PassengerId': test_df.index, 'Survived': yfinal})\n# output.to_csv('thresholdpseudoann.csv', index=False)","429ccd3c":"# Building Model","1e7a588a":"# TPS April'21 (ANN + Pseudolabel) ","ccf106a9":"# Feature Engineering","151ed1e8":"**Submission File**","5bad6d54":"### Do upvote if you find this Notebook helpful.  ","75f890d4":"**********************************************************************\n## Current Score : 81.101 %\n**********************************************************************\n### Previous score : 80.782 %\n\nhttps:\/\/www.kaggle.com\/pranjalverma08\/tps-april-21-the-ann-approach-score-80-782"}}