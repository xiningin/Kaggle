{"cell_type":{"95917f99":"code","b4197398":"code","3294b4bc":"code","e2760c73":"code","29291ea9":"code","675c24e4":"code","0ffcba71":"code","84835da2":"code","92cb3644":"code","5be3df25":"code","9584ab6e":"code","6d1eab32":"code","353289d8":"code","08a6bab9":"code","25757178":"code","5c634e7a":"code","72089107":"code","230bb443":"code","71e27c3f":"code","45b2cb90":"code","756bbd2b":"code","eb057d2a":"code","2754a594":"code","183ff216":"code","9cfd97b7":"code","9de23b8f":"code","f249ae2b":"code","c1ef16f9":"code","92a313f6":"code","bc06645e":"code","079ce332":"code","8e444619":"code","f34bc2f3":"markdown","1fbd1b29":"markdown","9b4e9d49":"markdown","3f066918":"markdown","35c5dc16":"markdown","76df311b":"markdown","bedd4198":"markdown","0aecc9bb":"markdown","03a4d97c":"markdown","c4335078":"markdown","4a488ebb":"markdown","bd761b1b":"markdown","29e67a35":"markdown","c8859e4c":"markdown","19c2103d":"markdown","f675a73b":"markdown","4a53e5d5":"markdown","43fd623b":"markdown"},"source":{"95917f99":"import tensorflow as tf\ntf.__version__","b4197398":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split # split train in train + validation\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Lambda, Dense, Flatten, Dropout, BatchNormalization, Convolution2D , MaxPooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# notebook parameters\ntrain_path = '\/kaggle\/input\/digit-recognizer\/train.csv'\ntest_path = '\/kaggle\/input\/digit-recognizer\/test.csv'","3294b4bc":"train = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nprint('Dim of train: {}'.format(train.shape))\nprint('Dim of test: {}'.format(test.shape))","e2760c73":"X_train = train.iloc[:, 1:].values.astype('float32')  # all pixel values\nY_train = train.iloc[:, 0].values.astype('int32')  # only labels i.e targets digits\nX_test = test.values.astype('float32')","29291ea9":"X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\nprint('Dim of train: {}'.format(X_train.shape))\nprint('Dim od test: {}'.format(X_test.shape))","675c24e4":"j = 0\nfor i in range(6, 15):\n    j += 1\n    plt.subplot(330 + j)\n    plt.imshow(X_train[i,:,:,:].reshape((28,28)), cmap=plt.get_cmap('gray'))\n    plt.title(Y_train[i])","0ffcba71":"# ... feature standarization (later use)\nmean_px = X_train.mean().astype(np.float32)\nstd_px = X_train.std().astype(np.float32)\n\ndef standardize(x):\n    return (x-mean_px)\/std_px","84835da2":"# ... one-hot-encoding of labels\nY_train = to_categorical(Y_train, num_classes=10)\nnum_classes = Y_train.shape[1]\nprint('Number of classes: {}'.format(num_classes))","92cb3644":"# ... split the train data to train + validation (to monitor performance while training)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, \n                                                  test_size=0.10, random_state=777)","5be3df25":"# ... define NN architecture\nmodel = Sequential() # Initialize a Sequential object to define a NN layer by layer (sequentially)\nmodel.add(Lambda(standardize, input_shape=(28,28,1))) # add a first layer which standardizes the input (grey-scale image of shape (28,28,1))\nmodel.add(Flatten(data_format='channels_last')) # add a layer which transform the input of shape (28, 28, 1) to shape (n_x,)\nmodel.add(Dense(10, activation='softmax')) # add a layer of 10 neurons that connect each one with all neurones in the previous layer\nprint(\"input shape \",model.input_shape)\nprint(\"output shape \",model.output_shape)","9584ab6e":"# ... define optimizer, loss function (and therefore a cost function) and metrics to monitor while training\nmodel.compile(\n    optimizer=RMSprop(lr=0.001, # learning rate (alpha)\n                      rho=0.9, # momentum of order 2 (rho*dW + (1-rho)*dW**2)\n                      momentum=0.0, # momentum of order 1 (rho*dW + (1-rho)*dW**2)\n                      epsilon=1e-07, # term to avoid dividing by 0\n                      centered=False, # if True standardize the gradients (high computational cost)\n                      name='RMSprop'),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'])","6d1eab32":"# ... fit NN (mini-batch approach of size 64 for 7 epochs with RMSprop optimizer)\nhistory = model.fit(x=X_train,\n                    y=Y_train,\n                    batch_size=64,\n                    epochs=20,\n                    verbose=2,\n                    validation_data=(X_val, Y_val))","353289d8":"# ... Visualize performance\nhistory_dict = history.history\nepochs = history.epoch\ntrain_accuracy = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\n\nplt.figure()\nplt.plot(epochs, train_accuracy, c='b')\nplt.plot(epochs, val_accuracy, c='r')\nplt.title('learning curves')\nplt.ylim(0.9,1)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.show()","08a6bab9":"# ... get real validation labels\nreal_val_class = []\nfor row in range(Y_val.shape[0]):\n    prod = Y_val[row,:]*np.array(range(10))\n    real_class = int(max(prod))\n    real_val_class.append(int(real_class))","25757178":"# ... validation error analysis\nerrors = pd.DataFrame({\n    'real': real_val_class,\n    'predict': model.predict_classes(X_val)\n})\n\nerror_model_index = errors.loc[errors.real != errors.predict,:].index\nerror_model_index","5c634e7a":"i_check = error_model_index[0]\nprint('Real value: {}'.format(errors.loc[i_check,'real']))\nprint('Predict value: {}'.format(errors.loc[i_check,'predict']))\n\nplt.figure(1)\nplt.imshow(X_val[i_check,:,:,:].reshape((28,28)), cmap=plt.get_cmap('gray'))\nplt.title(Y_val[i_check])","72089107":"i_check = error_model_index[1]\nprint('Real value: {}'.format(errors.loc[i_check,'real']))\nprint('Predict value: {}'.format(errors.loc[i_check,'predict']))\n\nplt.figure(1)\nplt.imshow(X_val[i_check,:,:,:].reshape((28,28)), cmap=plt.get_cmap('gray'))\nplt.title(Y_val[i_check])","230bb443":"# ... define NN architecture\nmodel_complex = Sequential() # Initialize a Sequential object to define a NN layer by layer (sequentially)\nmodel_complex.add(Lambda(standardize,input_shape=(28,28,1))) # add a first layer which standardizes the input (grey-scale image of shape (28,28,1))\nmodel_complex.add(Flatten(data_format='channels_last')) # add a layer which transform the input of shape (28, 28, 1) to shape (n_x,)\nmodel_complex.add(Dense(512, activation='relu')) # add a layer of 512 neurons that connect each one with all neurones in the previous layer\nmodel_complex.add(Dense(256, activation='relu')) # add a layer of 512 neurons that connect each one with all neurones in the previous layer\nmodel_complex.add(Dense(128, activation='relu')) # add a layer of 512 neurons that connect each one with all neurones in the previous layer\nmodel_complex.add(Dense(64, activation='relu')) # add a layer of 512 neurons that connect each one with all neurones in the previous layer\nmodel_complex.add(Dense(10, activation='softmax')) # add a layer of 10 neurons that connect each one with all neurones in the previous layer","71e27c3f":"model_complex.compile(optimizer='RMSprop', \n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])","45b2cb90":"history_complex = model_complex.fit(x=X_train,\n                                    y=Y_train,\n                                    batch_size=64,\n                                    epochs=20,\n                                    verbose=2,\n                                    validation_data=(X_val, Y_val))","756bbd2b":"# ... Visualize performance\nhistory_dict = history_complex.history\nepochs = history_complex.epoch\ntrain_accuracy = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\n\nplt.figure()\nplt.plot(epochs, train_accuracy, c='b')\nplt.plot(epochs, val_accuracy, c='r')\nplt.title('learning curves')\nplt.ylim(0.9,1)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.show()","eb057d2a":"# ... predictions\npredictions = model_complex.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                          \"Label\": predictions})\nsubmissions.to_csv(\"model_complex.csv\", index=False, header=True)","2754a594":"# ... define architecture\ncnn_model = Sequential()\ncnn_model.add(Lambda(standardize, input_shape=(28,28,1)))\ncnn_model.add(Convolution2D(64,(3,3), activation='relu'))\ncnn_model.add(BatchNormalization())\ncnn_model.add(MaxPooling2D())\ncnn_model.add(Flatten())\ncnn_model.add(Dense(124, activation='relu'))\ncnn_model.add(BatchNormalization())\ncnn_model.add(Dropout(0.9))\ncnn_model.add(Dense(10, activation='softmax'))","183ff216":"# ... define optimizer, loss function and metrics to monitor\ncnn_model.compile(optimizer=Adam(learning_rate=0.0001), \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])","9cfd97b7":"history_cnn = cnn_model.fit(x=X_train,\n                            y=Y_train,\n                            batch_size=256,\n                            epochs=200,\n                            verbose=2,\n                            validation_data=(X_val, Y_val))","9de23b8f":"# ... Visualize performance\nhistory_dict = history_cnn.history\nepochs = history_cnn.epoch\ntrain_accuracy = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\n\nplt.figure()\nplt.plot(epochs, train_accuracy, c='b')\nplt.plot(epochs, val_accuracy, c='r')\nplt.title('learning curves')\nplt.ylim(0.9,1)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.show()","f249ae2b":"predictions = cnn_model.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                          \"Label\": predictions})\nsubmissions.to_csv(\"cnn_simple.csv\", index=False, header=True)","c1ef16f9":"# ... define architecture\ncnn_adam_model = Sequential()\ncnn_adam_model.add(Lambda(standardize, input_shape=(28,28,1)))\ncnn_adam_model.add(Convolution2D(1024,(3,3), activation='relu'))\ncnn_adam_model.add(BatchNormalization())\ncnn_adam_model.add(MaxPooling2D())\ncnn_adam_model.add(Convolution2D(512,(3,3), activation='relu'))\ncnn_adam_model.add(BatchNormalization())\ncnn_adam_model.add(MaxPooling2D())\ncnn_adam_model.add(Convolution2D(256,(3,3), activation='relu'))\ncnn_adam_model.add(BatchNormalization())\ncnn_adam_model.add(MaxPooling2D())\ncnn_adam_model.add(Flatten())\ncnn_adam_model.add(Dense(512, activation='relu'))\ncnn_adam_model.add(BatchNormalization())\ncnn_adam_model.add(Dropout(0.9))\ncnn_adam_model.add(Dense(124, activation='relu'))\ncnn_adam_model.add(BatchNormalization())\ncnn_adam_model.add(Dropout(0.9))\ncnn_adam_model.add(Dense(10, activation='softmax'))","92a313f6":"# ... define optimizer, loss function and metrics to monitor\ncnn_adam_model.compile(optimizer=Adam(), \n                       loss='categorical_crossentropy',\n                       metrics=['accuracy', 'mse'])","bc06645e":"history_cnn_adam = cnn_adam_model.fit(x=X_train,\n                                      y=Y_train,\n                                      batch_size=124,\n                                      epochs=50,\n                                      verbose=2,\n                                      validation_data=(X_val, Y_val))","079ce332":"# ... Visualize performance\nhistory_dict = history_cnn_adam.history\nepochs = history_cnn_adam.epoch\ntrain_accuracy = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\n\nplt.figure()\nplt.plot(epochs, train_accuracy, c='b')\nplt.plot(epochs, val_accuracy, c='r')\nplt.title('learning curves')\nplt.ylim(0.9,1)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.show()","8e444619":"predictions = cnn_adam_model.predict_classes(X_test, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                          \"Label\": predictions})\nsubmissions.to_csv(\"cnn_complex.csv\", index=False, header=True)","f34bc2f3":"Before making network ready for training we have to make sure to add below things to compile forward and back-propagation:\n\n1. A loss function: to measure how good the network is at each step\n2. An optimizer: to update network as it sees more data and reduce loss value\n3. Metrics: to monitor performance of network over train and validation datasets","1fbd1b29":"### 9. Complex Convolutional Neural Networks - 3 CNN layers","9b4e9d49":"Now let's try a convolutional neural networks (CNN for short) which is Neural Network architectures which performe better on images by extracting usefull features in the first layers (convolutional layers).","3f066918":"### NN Digit recognizer","35c5dc16":"### 7. Complex NN - 5 layers","76df311b":"### 4. Visualization examples","bedd4198":"### 2. Convert pandasDF to arrays","0aecc9bb":"### 6. Simple NN - 1 layer","03a4d97c":"### 5. Preprocessing images","c4335078":"### 0. Initial Configuration","4a488ebb":"As we saw in the learning rates, there is not a big difference between Train performance and Validation performance (it means there is a little or no **variance error**). On the othe hand, the accuracy obtained in Validation is ~0.92 which is a bit far from the best possible score (~0.999 accuracy for other players) so there is a bit **bias error** that we can try to reduce:\n\n* one way to achieve this is by increment the complexity of our model (try a bigger NN)\n* adding more data to train our model (look for more data or performe Data Augmentation)\n\nIn this section we will use a more complex architecture following the patters of classic neural networks: decrease the number of neurons per layer. We will perform Data Augmentation in the last section.","bd761b1b":"In this notebook I updated the excellent work of [Poonam Ligade](http:\/\/www.kaggle.com\/poonaml\/deep-neural-network-keras-way) as a first view of neural nets in the Keras way version 2.1.0. I tried to simplify and avoid complexity to show just a few examples of Neural Network architectures on the MNIST dataset. \n\nFor those who do not know it, as Tensorflow is a higher-level framework than python to build Neural Networks, Keras is a higher-level framework (API) than Tensorflow. Keras allow a higher level of abstraction to implement complex neural networks easy and quick.\n\nI hope this will be usefull to anyone like me who wants to start using Neural Networks.\n\nAny comment will be welcome!","29e67a35":"### 1. Read datasets","c8859e4c":"Lets first create a simple model from Keras Sequential layer:\n\n1. Lambda layer performs simple arithmetic operations like sum, average, exponentiation etc.\n2. In 1st layer of the model we have to define input dimensions of our data in (rows,columns,colour channel) format. (In theano colour channel comes first)\n3. Flatten will transform input into 1D array.\n4. Dense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer. In the last layer we have to specify output dimensions\/classes of the model. Here it's 10, since we have to output 10 different digit labels.","19c2103d":"### 8. Simple Convolutional Neural Networks - 1 CNN layer","f675a73b":"### 3. Expand the array dim for channels","4a53e5d5":"### Structure:\n0. Initial configuration\n1. Read data\n2. Convert pandasDF to array\n3. Example Visualization\n4. Expand array dim for channels\n5. Preproccessing images+\n6. Simple NN - 1 layer\n7. Complex NN - 5 layers\n8. Simple Convolutional Neural Networks - 1 CNN layer\n9. Complex Convolutional Neural Networks - 3 CNN layer\n10. Data Augmentation + Complex Convolutional Neural Networks - X CNN layer","43fd623b":"There is an improvement in the performance."}}