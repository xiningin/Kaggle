{"cell_type":{"192fd5ef":"code","83bcf526":"code","c877f403":"code","ccd2e24e":"code","20ca4f82":"code","762b205d":"code","f0af2cb0":"code","8bdb3417":"code","2c0d0cd1":"code","2a5df834":"code","38b4c0ca":"code","fd5a0393":"code","c623396c":"code","0ea9c3c2":"code","d2eb107a":"code","9867b0bc":"code","70c29981":"code","86b12ab2":"code","8a3734f0":"code","ebc1de03":"code","b524dd69":"code","a2ef9942":"code","de7ff455":"code","133b59bf":"code","bb417c03":"code","f140cc13":"code","0dad1da4":"code","918e880d":"code","fb6303c2":"code","bfdfaad8":"code","0d160b30":"code","58054a52":"code","233f08ea":"code","2b73f316":"code","c1f1159a":"code","2ada9c50":"code","50d4e2c4":"code","41c55aba":"code","8c0859f9":"code","e7d3612b":"code","3ce72773":"code","79d3512e":"code","3f048d39":"code","436dd8f5":"code","e390bc36":"code","4685f43f":"code","de66f6e2":"code","88961059":"code","c3cf9c55":"code","dcee817d":"code","0a99f405":"code","1b065559":"code","c3c73046":"code","dd1fa304":"code","97905504":"code","e496a033":"code","cc3a7c29":"code","fb8e579f":"code","920631c7":"code","39299c79":"code","06fc2e58":"code","0c96a4d5":"code","5cca2075":"code","8131eb41":"code","79bcf5a8":"code","24286127":"code","ae2f9f0f":"code","61940935":"code","e5d41b24":"code","450e9f82":"code","cd31d678":"code","2e160a85":"code","cc4c101a":"code","560a7eb4":"code","4ab8087c":"code","3e4dd086":"code","b605fd5a":"code","b837d752":"code","45a6747a":"code","75b61cd6":"code","44b176c7":"code","179b0d2f":"code","04e05dc6":"code","7cf1182d":"code","4bd8d927":"code","7b8a0540":"code","9cc5af91":"code","812a7002":"code","44f5dd41":"code","0caff877":"code","4ec0f3d0":"code","4c2b9ed7":"code","36e99a02":"code","8bb6b870":"code","f49d88ad":"code","291ee2e3":"code","b629e526":"code","43c294ab":"code","91533ee9":"code","3ec3feeb":"code","d5b8e2e5":"code","c3091785":"code","c135cadd":"code","3c0d6a7a":"code","8b55ebb4":"code","b9cc5258":"code","b0c381b7":"code","81a40095":"code","0970288d":"code","4d90a3f1":"code","00ac9798":"code","e024a445":"code","455c1a4b":"code","00a39dbd":"code","d54d3d6d":"code","24913fdd":"code","62fdfc8d":"code","34342fcb":"code","d5a3dbd8":"code","0d1d5fca":"code","dce53a5f":"code","c4b20e4e":"code","9e36cd23":"code","d1b61743":"code","79bf023a":"code","1a9f31d8":"code","d7476108":"code","d40650a8":"code","a7d2d3bf":"code","7d81bc4e":"code","f1bea6a5":"code","13e138ff":"code","fbc1fcdf":"code","e0b7d229":"code","c3489b8a":"code","c10a9cf9":"code","6d8c6523":"code","61b2d2fc":"code","3c6f79ee":"code","2ff4a2c9":"code","0801cbec":"code","d69ebce1":"code","7cb3c5cb":"code","832625cb":"code","452e71e7":"code","36a65a23":"code","310c9f30":"code","55438643":"code","2017b7d7":"code","2e041cd7":"code","cd17c3a2":"code","054eacba":"code","5eb45eaa":"code","9a5cad0d":"code","95bc2e90":"code","b459943f":"code","1fbba3ed":"code","19fb4ada":"code","167c66c8":"code","d0fc3727":"code","808b5d72":"code","2ccc7095":"code","847ebe30":"code","753a726e":"code","5c6e7fe7":"code","d5fd98e5":"code","9ad2665b":"code","0fc3a948":"code","c4cebc2e":"code","49f849d1":"code","1d67c841":"code","4c078597":"code","b85d5dc6":"code","ce5a316d":"code","59b4dafd":"code","934b72f2":"code","f63e591e":"code","c174f046":"code","92f21fac":"code","fb3581db":"code","b88a255b":"code","92497a09":"code","5e90a8b7":"code","8869b689":"code","97bbab7f":"code","54b836ae":"code","52f62e85":"code","0fc81717":"code","8852952b":"code","97e48649":"code","fcd3df54":"code","2c463a7c":"code","e0a2c4b2":"code","2cf0d62e":"code","e07c00d2":"markdown","48128516":"markdown","6973bffd":"markdown","eff2bf26":"markdown","cce10c0b":"markdown","50ceafbe":"markdown","0ce6121c":"markdown","3273b72a":"markdown","d76baecd":"markdown","3607b651":"markdown","af07c968":"markdown","ea6c2e89":"markdown","69a0686f":"markdown","5a75739f":"markdown","66e2135d":"markdown","bb5ca2b5":"markdown","792805d7":"markdown","7de917da":"markdown","c8882f79":"markdown","909b077e":"markdown","4b4ae9bf":"markdown","8a0d33c2":"markdown","42d775b3":"markdown","5807d0a3":"markdown","b880a1b6":"markdown","4c067bfc":"markdown","5f0d587d":"markdown","fdac8603":"markdown","3e830437":"markdown","e2126a60":"markdown","734dff72":"markdown","ed6b536e":"markdown","04727c30":"markdown","7b4a9a0a":"markdown","67adc6a3":"markdown","44458b5e":"markdown","2eab419d":"markdown","bddfd8eb":"markdown","ea44f4d8":"markdown","4b6ca65f":"markdown","45155f9b":"markdown","bd72915b":"markdown","53e901d5":"markdown","4b54d4d4":"markdown","2f2fa7c9":"markdown","483c6b5b":"markdown","47f1abc9":"markdown","39c75baa":"markdown","13faab59":"markdown","d17d0030":"markdown","d521c1e5":"markdown","ad7dea47":"markdown","4d42fed5":"markdown","083c1134":"markdown","d4b1cb9e":"markdown","3e4d311b":"markdown","3cfd8237":"markdown","54adfd77":"markdown","c52eb843":"markdown","1145c774":"markdown","576764d3":"markdown","b28acebf":"markdown","ca8cd6e3":"markdown","41963ad2":"markdown","aac6ee18":"markdown","a1d4cc2e":"markdown","22675322":"markdown","abcdaaf3":"markdown","93df5285":"markdown","d8f91ddb":"markdown","76b70b38":"markdown","eca446fc":"markdown","d58e289c":"markdown","375bf76b":"markdown","e448a21d":"markdown","c993e242":"markdown","8a197115":"markdown","f27842fa":"markdown","b4e92cf9":"markdown","476c42da":"markdown","b18327ff":"markdown","462f9733":"markdown","bd7a9643":"markdown","7564141e":"markdown","0c6cc393":"markdown","17e74c86":"markdown","de56a34e":"markdown","e854fe84":"markdown","bf850f47":"markdown","9c2efc32":"markdown","fd8f19ad":"markdown","50a73bb2":"markdown","0f87c2a4":"markdown","bc342cff":"markdown","6211f985":"markdown"},"source":{"192fd5ef":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport networkx as nx\nimport nltk, string\nimport matplotlib.pyplot as plt\nimport re,glob\nimport time\nimport os\nimport random\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(['gives'])\n\nrandom_state = 21\n\n'''remove punctuation, lowercase, stem'''\npunct = '-'\nremove_punctuation_map = dict((ord(char), ' ') for char in punct)    \ndef normalize(text):\n    return nltk.word_tokenize(text.lower().translate(remove_punctuation_map))\n\ndef clean_text(text):\n    text = text.lower().translate(remove_punctuation_map)\n    \n    return ' '.join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text))\n\nnumwords={}\nnumbers = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\"nine\", \"ten\",\"eleven\",\"twelve\",\"thirteen\",\"fourteen\",\"fifteen\",\"sixteen\",\"seventeen\",\"eighteen\"]\nfor idx, word in enumerate(numbers):    numwords[word] = (idx)\n\n\n\n#column heads equals 25 plus any added\nlen_ch = 25\n\n\n\n\n'''remove punctuation, lowercase, stem'''\n\n\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]\n\n","83bcf526":"path = '..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Additional data\/'\nfilename = 'kaggle_data_dictionary.csv'\n\ndf_kddict = pd.read_csv(path + filename)\ndf_kddict.set_index('Field Name', inplace=True)\ndf_kddict.head(40)","c877f403":"df_kddict_T = df_kddict.T\n# df_kddict_T.head()","ccd2e24e":"df_kddict_T.insert(loc = 25, column = 'NOTES', value = '')\ndf_kddict_T.insert(loc = 26, column = 'SELECTION_PROCESS', value = '')\ndf_kddict_T.insert(loc = 18, column = 'VOCATIONAL_QUAL', value = '')\ndf_kddict_T.insert(loc = 5, column = 'AND_OR', value = '')\ndf_kddict_T.insert(loc = 7, column = 'DEGREE_REQ', value = '')\nlen_ch += 5\ndf_new_kddict = df_kddict_T.T\n# df_new_kddict.head(40)","20ca4f82":"#REQUIREMENT_SET_ID is set to string. Although it is a number, math ops are not required\ndf_new_kddict.at['REQUIREMENT_SET_ID','Data Type'] = 'string'\ndf_new_kddict.at['REQUIREMENT_SET_ID','Allowable Values'] = '0-9'\ndf_new_kddict.at['REQUIREMENT_SET_ID','Accepts Null Values?'] = 'Yes'\ndf_new_kddict.at['REQUIREMENT_SET_ID','Additional Notes'] = 'Many requirements do not have an assigned REQUIREMENT_SET_ID '\n\n#REQUIREMENT_SUBSET_ID is set to string. Although it is a number, math ops are not required\ndf_new_kddict.at['REQUIREMENT_SUBSET_ID','Accepts Null Values?'] = 'Yes'\ndf_new_kddict.at['REQUIREMENT_SUBSET_ID','Additional Notes'] = 'Many sub- requirements do not have an assigned REQUIREMENT_SUBSET_ID '\n\n#AND_OR is set to string. Although it is a number, math ops are not required\ndf_new_kddict.at['AND_OR','Data Type'] = 'string'\ndf_new_kddict.at['AND_OR','Description'] = 'Requirement conjunctions: The And or Or that separates requirement sets, suggesting options for entry into the job class or multiple requirements to satisfyE1. Overall requirement conjunction, separating overall requirement setsE2. Sub-requirement conjunction, separating sub-requirement sets'\ndf_new_kddict.at['AND_OR','Annotation Letter'] = 'E1\/E2'\ndf_new_kddict.at['AND_OR','Accepts Null Values?'] = 'Yes'\n\n#DEGREE_REQ will be useful when reviewing balance in the EDA\ndf_new_kddict.at['DEGREE_REQ','Data Type'] = 'string'\ndf_new_kddict.at['DEGREE_REQ','Description'] = 'Is a degree required for a requirement option or was it required for a previous role'\ndf_new_kddict.at['DEGREE_REQ','Accepts Null Values?'] = 'No'\ndf_new_kddict.at['DEGREE_REQ','Additional Notes'] = 'DEGREE_REQ will be useful when reviewing balance in the EDA'\n\n\n#There are nine School_Types \n\ndf_new_kddict.at['SCHOOL_TYPE','Allowable Values'] =  'AMERICAN BAR ASSOCIATION ACCREDITED LAW SCHOOL, APPRENTICESHIP, \\\nCOLLEGE, COLLEGE OR TRADE SCHOOL, COLLEGE OR UNIVERSITY, \\\nCOLLEGE OR UNIVERSITY OR TRADE SCHOOL, HIGH SCHOOL OR G.E.D. EQUIVALENT \\\nHIGH SCHOOL, UNIVERSITY, COLLEGE, TRADE OR TECHNICAL SCHOOL,UNIVERSITY' \n\n#As SCHOOL_TYPE include certification, the details should appear here\ntypes = df_new_kddict.loc['EDUCATION_MAJOR','Additional Notes']\ndf_new_kddict.at['EDUCATION_MAJOR','Additional Notes'] = types +', Certificate details'\n\n\n#As well as part\/full time,experience can be measure in hours \n\ntypes = df_new_kddict.loc['FULL_TIME_PART_TIME','Allowable Values']\ndf_new_kddict.at['FULL_TIME_PART_TIME','Allowable Values'] = types +', xxx HOURS'\ndf_new_kddict.at['FULL_TIME_PART_TIME','Additional Notes'] = 'Some experience requirements are measured in hours'\n\n\n#As well as *S*Q,courses can be measure in hours \n\ntypes = df_new_kddict.loc['COURSE_LENGTH','Allowable Values']\ndf_new_kddict.at['COURSE_LENGTH','Allowable Values'] = types +', xxx HOURS'\n\n#Full list of options\ndf_new_kddict.at['DRIV_LIC_TYPE','Allowable Values'] = 'A|A OR B|A AND\/OR B|A,B OR C|A OR I|B|B OR C|B AND\/OR C|C'\ndf_new_kddict.at['DRIV_LIC_TYPE','Additional Notes'] = 'Where there are both requirements and possibilities only the requirements are recorded'\n\n\n#There is a fifth Exam_Types \ntypes = df_new_kddict.loc['EXAM_TYPE','Allowable Values']\ndf_new_kddict.at['EXAM_TYPE','Allowable Values'] = types +', EXEMPT_EMPLOYEES'\n\n#Not all bulletins have gen salaries\ndf_new_kddict.at['ENTRY_SALARY_GEN','Accepts Null Values?'] = 'Yes'\n\n#Not all bulletins have open dates recorded\ndf_new_kddict.at['OPEN_DATE','Accepts Null Values?'] = 'Yes'\n\n\n#df_new_kddict.reset_index()\ndf_new_kddict.reset_index(level=0, inplace=True)\ndf_new_kddict.head(40)","762b205d":"column_heads = df_new_kddict['Field Name'].tolist()\nprint (column_heads)","f0af2cb0":"c_h = df_new_kddict['Field Name'].to_dict()\ncol_heads = dict([(value, key) for key, value in c_h.items()]) \n\nprint (col_heads)","8bdb3417":"path = '..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Additional data\/'\nfilename ='job_titles.csv'\nwith open(path + \"\/\" + filename, 'r', errors='ignore') as f:\n        j_t = f.readlines()\nlen_j_t = len(j_t)\nfor i in range(len_j_t):\n        j_t[i] = j_t[i].replace(\"\\n\",\"\")\n        if j_t[i]  ==  'Vocational Worker  DEPARTMENT OF PUBLIC':\n            j_t[i] = 'VOCATIONAL WORKER'\n\nj_t.append('SEASONAL POOL MANAGER')\nj_t.append('OPEN WATER LIFEGUARD')\nj_t.append('ELECTRICAL ENGINEER')\nj_t.append('WASTEWATER TREATMENT MECHANIC')\nj_t.append('TELECOMMUNICATIONS PLANNER')\nj_t.append('CONSTRUCTION EQUIPMENT SERVICE SUPERVISOR')\nj_t.append('COMPUTER OPERATOR')\nj_t.append('SPECIAL PROGRAM ASSISTANT')\nj_t.append('FIRE PROTECTION ENGINEER')\nj_t.append('PRINT SHOP TRAINEE')\nj_t.append('IMPROVEMENT ASSESSOR')\nj_t.append('ENGINEERING ASSOCIATE')\nj_t.append('SAFETY ENGINEER PRESSURE VESSELS')\nj_t.append('SENIOR ROOFER')\nj_t.append('SENIOR CLERK TYPIST')\nj_t.append('SENIOR STREET SERVICES INVESTIGATOR')\nj_t.append('LIBRARY CLERICAL ASSISTANT')\nj_t.append('WASTEWATER TREATMENT ELECTRICIAN')\nj_t.append('PERFORMING ARTS PROGRAM COORDINATOR')\nj_t.append('PROCUREMENT AIDE')\nj_t.append('LOAD DISPATCHER')\nj_t.append('ASSOCIATE ZOO CURATOR')\nj_t.append('AIRPORT POLICE SERGEANT')\nj_t.append('FIRE SPRINKLER FITTER')\n\n\n\n\n#Executive, Senior, Coordinating or Web Content Producer.?\n\n\n#sort the list so that longest by number of words is first\n#this allows us to remove a match and so duplicates are avoided when searching for subordinate roles\nj_t.sort(key=lambda x: len(x.split()), reverse=True)\n\n#print (j_t)","2c0d0cd1":"# path = '..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Additional data\/'\n# sample_job_class = pd.read_csv(path + 'sample job class export template.csv')\n# df_job_class = sample_job_class.copy()\n# sample_job_class.head()","2a5df834":"def find_experience(line,job):\n    #j_t contains a list of the job class names\n    len_j_t = len(j_t)\n    max_pos = 0\n    exp = ''\n    last_exp = ''\n    line = line.upper()\n    full_line = line\n    job = job.upper()\n\n    #only focus on part of line that is relevant\n    assist_pattern =  '(.*)'+ 'ASSISTING' + '(.*)'\n    if re.search(assist_pattern, line):\n        line = re.search(assist_pattern, line).group(1)\n    assist_pattern =  '(.*)'+ 'SUBSTITUT' + '(.*)'\n    if re.search(assist_pattern, line):\n        line = re.search(assist_pattern, line).group(1)\n    assist_pattern =  '(.*)'+ 'YEARS OF WHICH' + '(.*)'\n    if re.search(assist_pattern, line):\n        line = re.search(assist_pattern, line).group(1)\n    assist_pattern =  '(.*)'+ 'INCLUDING'+ ('.*?')+ ('YEARS') + '(.*)'\n    if re.search(assist_pattern, line):\n        line = re.search(assist_pattern, line).group(1)\n\n    #search the line for each job class\n    #print ('line',line)\n    for i in range(len_j_t):\n        pattern =  j_t[i]+ '( I{0,3} )'\n        pattern =  j_t[i]+ '( I{0,3}[ |\\.|;|,])'\n        pattern2 =  j_t[i] + '( |\\.|;|,)'\n        \n        special = 'FIREFIGHTER'\n        special_ex = 'ENDORSEMENT'\n        special2 = 'ARCHITECT'\n        special2_ex = 'LICENSE'\n        \n        if job == j_t[i] and re.search(pattern2, line):\n            #remove instances of the job title in the requirements section\n            #print ('job removed', job)\n            src = re.search(pattern2, line)\n            line =line.replace(src.group(0),'')\n                           \n        #we are looking for subordinates, so dont want assistants or the job itself\n        if re.search(pattern2, line) and job!= j_t[i] and job != ('ASSISTANT '+ j_t[i]):\n            #exclude references where job title is used for other information\n            if not(j_t[i] == special and  re.search(special + \" \" +special_ex, line)):\n                 if not(j_t[i] == special2 and  re.search(special2 + \" \" +special2_ex, line)):\n                    #include all the subsets I, II, III etc\n                    src = re.search(pattern2, line)\n                    matches = re.findall(pattern, line) \n                    if matches:\n                        for match in matches:\n                            exp = exp + j_t[i]  + match +', '\n                    else:\n                        exp = exp + j_t[i] +', '\n                    #remove the job so we dont include shorter versions: eg \"senior job\" and \"job\"\n                    line =line.replace(src.group(0),'')\n                    #but need to find the last job in line to mark beginning of other text\n                    src = re.search(j_t[i], full_line)\n                    if src.start(0) > max_pos:\n                        max_pos = src.start(0)\n                        last_exp = j_t[i]\n    exp = exp.rstrip(', ')\n    return exp, last_exp\n\n# TEST CODE\n# line = \"1. Two years of full-time paid experience as a Safety Engineer Pressure Vessels with the City of Los Angeles; and\"\n# job = 'Senior Safety Engineer Pressure Vessels'\n\n# find_experience(line,job)","38b4c0ca":"def examination_type(line, level):\n#The exam type information is sometimes spread over two lines and so we have two levels, one for each line.\n#level = 0 for identifying we are in exam type\n#level = 1 for the second line\n\n    if (level == 0):\n        exam_type_pattern = '(THIS EXAM)(.*?)( IS TO BE GIVEN)(.*)'   \n        exam_type_pattern2 = 'FOR EXEMPT EMPLOYEES SEEKING TO BECOME(.*)'   \n        if re.search(exam_type_pattern,line):\n            #print (re.search(exam_type_pattern,line).group(4))\n            row[col_heads['EXAM_TYPE']]  = re.search(exam_type_pattern,line).group(4)\n            level = 1\n        if  re.search(exam_type_pattern2,line):\n            #print (line)\n            row[col_heads['EXAM_TYPE']]  = line\n            level = 1            \n    else:\n        #print ('line, level',line, level)\n#         #print (line)\n#         row[col_heads['EXAM_TYPE']]  = row[col_heads['EXAM_TYPE']] + ' ' + line\n        level = 0\n#Sometimes the second line has other info, or there isn't a second line\n        exam_type_pattern3 = 'The City of'   \n        \n        if not re.search(exam_type_pattern3,line):\n            #normal case\n            exam_type = row[col_heads['EXAM_TYPE']] + ' ' + line  \n            exam_type = exam_type.replace ('AN AN','AN')\n            exam_type = exam_type.replace ('AND OPEN','AND AN OPEN')\n            exam_type = exam_type.replace ('AND OPEN','AND AN INTERDEPARTMENTAL')\n            exam_type = exam_type.replace ('BOTH ON','ON')\n            exam_type = exam_type.replace ('ON BOTH','ON')\n            exam_type = exam_type.replace ('ON A ','ON AN ') \n            exam_type = exam_type.replace ('AND ON AN','AND AN')  \n            exam_type = exam_type.replace ('AND ON AN','AND AN')  \n            exam_type = exam_type.replace ('TO ON','ON')  \n            exam_type = exam_type.replace ('ONLY ON A','ON A')  \n            exam_type = exam_type.replace ('BASIS ONLY','BASIS')  \n       \n            exam_type = exam_type.replace ('INTERDEPARMENTAL','INTERDEPARTMENTAL')  \n            exam_type = exam_type.replace ('BASIS   NVVC','BASIS')  \n   \n        \n            exam_type = exam_type.replace ('AND INTERDEPARTMENTAL','AND AN INTERDEPARTMENTAL')  \n       \n            exam_type = exam_type.replace ('COMPETITVE','COMPETITIVE')\n            exam_type = exam_type.replace ('ON AN OPEN COMPETITIVE AND AN INTERDEPARTMENTAL PROMOTIONAL BASIS','ON AN INTERDEPARTMENTAL PROMOTIONAL AND AN OPEN COMPETITIVE BASIS')\n            \n            row[col_heads['EXAM_TYPE']]  = exam_type\n        else:\n            row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']].replace ('ONLY ON A','ON A') \n#Dictionary requires an abbreviated set of options\n        row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']].replace ('ON AN OPEN COMPETITIVE BASIS', 'OPEN')\n        row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']].replace ('FOR EXEMPT EMPLOYEES SEEKING TO BECOME CIVIL SERVICE EMPLOYEES', 'EXEMPT_EMPLOYEES')\n        row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']].replace('ON AN INTERDEPARTMENTAL PROMOTIONAL BASIS','INT_DEPT_PROM')\n        row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']].replace('ON AN DEPARTMENTAL PROMOTIONAL BASIS','DEPT_PROM')\n        row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']].replace('ON AN INTERDEPARTMENTAL PROMOTIONAL AND AN OPEN COMPETITIVE BASIS','OPEN_INT_PROM')\n\n           \n            \n            \n            \n            \n   \n    return level","fd5a0393":"def degree_req(line):\n# cases to consider:\n# require possession of a degree \n# degree require \n\n    line = line.lower()\n            \n    degree_req_pattern = '(degree)(.*)(required)'  \n    degree_req_pattern2 = '(require)(.*?)(degree)'   \n        \n    if re.search(degree_req_pattern,line) or  re.search(degree_req_pattern2,line):\n        if not re.search('desired',line):\n            #print ('deg', line)\n            row[col_heads['DEGREE_REQ']]  = 'YES'\n            #for senior jobs a college education is implicit because of required previous experience\n            if pd.isna(row[col_heads['SCHOOL_TYPE']]) :\n                 row[col_heads['SCHOOL_TYPE']] = 'COLLEGE OR UNIVERSITY'\n    \n    return    ","c623396c":"def clean_licence_line(lclass):\n    lclass = lclass.rstrip(\" \").lstrip(\" \")\n    lclass = lclass.rstrip(\"\\'\").lstrip(\"\\'\").upper()\n    lclass = lclass.replace( '\"', '')\n    lclass = lclass.replace( 'CLASS', '')\n    lclass = lclass.replace( '(', '')\n    lclass = lclass.replace( ')', '')\n    lclass = lclass.replace( '  ', ' ')\n    lclass = lclass.replace( 'I OR A', 'A OR I')\n    lclass = lclass.replace( 'B OR A', 'A OR B')\n    lclass = lclass.replace( 'C OR B', 'B OR C')\n    lclass = lclass.replace( 'C AND\/OR B', 'B AND\/OR C')\n    lclass = lclass.replace( '1\/A OR 2\/B', 'A OR B')\n    return lclass\n\n\ndef driver_licence(line):\n# cases to consider:\n# a valid california driver's license is required.\n# a valid california driver's license may be required.\n# for positions requiring a valid class b driver's license\n# a valid unrestricted california commercial class a or class b driver's license and valid medical certificate approved\n# a valid california commercial class b driver's license \n#Some positions may require a valid California Class C and\/or Class B driver's license.\n# \n\n    line = line.lower()\n#     if ('driver'  in line):\n#         if ('some positions may require' not in line):\n#             if (\"a valid california driver's license is required\" not in line):\n#                 print (line)\n            \n    driver_lic_pattern = '(driver)(.*)(required)'  \n    driver_lic_may_pattern = '(driver)(.*)(may)(.*)(required|approved)'   \n    driver_lic_may_pattern2 = '(positions)(.*?)(may)(.*?)(require)(.*?)(driver)'   \n    \n    #driver_lic_class_pattern = '(.*)(class)(.*)(driver)(.*)(required|approved)'   \n#    driver_lic_class_pattern = '(.*?)(class)(?!\\..*?)(driver)(.*?)(;|\\.|:|,|\\r|\\n)'   \n    driver_lic_class_pattern = '(.*?)(class)(.*?)(commercial|california|driver)(.*?)(required)(.*?)(;|\\.|:|,|\\r|\\n)'   \n    #driver_lic_may_class_pattern = 'positions(.*?)may(.*?)(requir)(.*?)(class)(.*)(driver)'   \n    driver_lic_may_class_pattern = 'positions(.*?)may(.*?)(requir)(.*?)(class)(.*?)(driver|license)'   \n    driver_lic_may_class_pattern2 = '(.*?)(class)(.*?)(commercial|california|driver)(.*?)(may)(.*?)(;|\\.|:|,|\\r|\\n)' \n   \n    driver_lic_endorse_pattern = '(license with )(.*?)(endorsement)'   \n    \n    if re.search(driver_lic_may_pattern,line) or  re.search(driver_lic_may_pattern2,line):\n            #print ('dlp', ' P', line)\n            row[col_heads['DRIVERS_LICENSE_REQ']]  = 'P'\n    elif re.search(driver_lic_pattern,line):\n#            print ('dlp', ' R')\n            row[col_heads['DRIVERS_LICENSE_REQ']]  = 'R'\n    if re.search(driver_lic_class_pattern,line):\n            may = re.search(driver_lic_class_pattern,line).group(1)\n            #print ('may', may)\n            #print (\"re.search('may',may)\",re.search('may',may))\n            if pd.isna(re.search('may',may)) and not re.search('(incumbents|employees| of this class)|(positions in the class)|(class of commercial)',line):\n                lclass = re.search(driver_lic_class_pattern,line).group(3)\n                #print ('dlpc', lclass)\n                lclass = clean_licence_line(lclass)\n                #print ('dlpcs', lclass)\n                if row[col_heads['DRIV_LIC_TYPE']] == '':\n                    row[col_heads['DRIV_LIC_TYPE']]  =  lclass\n#                 if row[col_heads['DRIV_LIC_TYPE']] == '':\n#                     row[col_heads['DRIV_LIC_TYPE']]  =  lclass.upper() + \" \"\n#                 else:\n#                     row[col_heads['DRIV_LIC_TYPE']]  = row[col_heads['DRIV_LIC_TYPE']] + lclass + \" \"\n\n                if row[col_heads['DRIVERS_LICENSE_REQ']] != 'R':\n                    row[col_heads['DRIVERS_LICENSE_REQ']]  = 'R'\n#                 print ('R')\n\n    if re.search(driver_lic_may_class_pattern,line):\n  #              print(\"row[col_heads['DRIVERS_LICENSE_REQ']]\",row[col_heads['DRIVERS_LICENSE_REQ']])\n                lclass = re.search(driver_lic_may_class_pattern,line).group(6)\n                lclass = clean_licence_line(lclass)\n                \n                #print ('dlpcsm', lclass)\n                #print (lclass)\n                #print (line)\n                if row[col_heads['DRIVERS_LICENSE_REQ']] != 'R' or row[col_heads['DRIVERS_LICENSE_REQ']] != 'P':\n                    row[col_heads['DRIV_LIC_TYPE']]  = lclass\n                    row[col_heads['DRIVERS_LICENSE_REQ']]  = 'P'\n                    #print ('dlmpc', 'P',lclass.upper())\n                    #print ('dlmpcline', line)\n\n    if re.search(driver_lic_may_class_pattern2,line) and not re.search('(incumbents|employees| of this class)|(positions in the class)|(class of commercial)',line):\n        lclass = re.search(driver_lic_may_class_pattern2,line).group(3)\n        lclass = clean_licence_line(lclass)\n        if row[col_heads['DRIVERS_LICENSE_REQ']] != 'R' or row[col_heads['DRIVERS_LICENSE_REQ']] != 'P':\n            row[col_heads['DRIV_LIC_TYPE']]  = lclass\n            row[col_heads['DRIVERS_LICENSE_REQ']]  = 'P'\n               \n    if re.search(driver_lic_endorse_pattern,line):\n            lclass = re.search(driver_lic_endorse_pattern,line).group(2) + re.search(driver_lic_endorse_pattern,line).group(3)\n            #print ('endorse', lclass)\n            row[col_heads['ADDTL_LIC']]  = lclass.upper()\n    \n    return    ","0ea9c3c2":"\ndef entry_salaries (body,row):\n    \n    line = body.replace (',','')\n    line = body.replace ('*','')\n    line = line.lower()\n    line = line.replace(' to ','-')\n    \n    #print (\"sal line\", line)\n    salary_flat_pattern = '(\\$|\\$ )(\\d{4,8})(.*)(flat)(.*?)(rated)'\n    salary_range_pattern = '(\\$|\\$ )(\\d{4,8})(-)(\\$|'')(\\d{4,8})'\n    \n\n    salary_pattern = '(.*)(department of water and power)(.*)'  \n    if re.search(salary_pattern,line):\n        part1 = re.search(salary_pattern,line).group(1)\n        part2 = re.search(salary_pattern,line).group(3)\n    else:\n        part1 = line\n        part2 = ''\n\n    part1s = ''\n    part2s = ''\n\n    #salary_flat_pattern = '(\\$)(.*)(flat-rated\\))'\n    if re.search(salary_flat_pattern,part1):\n        part1s = re.search(salary_flat_pattern,part1).group(1) + re.search(salary_flat_pattern,part1).group(2)  +'(flat-rated)' \n    if re.search(salary_flat_pattern,part2):\n        part2s = re.search(salary_flat_pattern,part2).group(1) + re.search(salary_flat_pattern,part2).group(2)  +'(flat-rated)'\n        \n    if re.search(salary_range_pattern,part1):\n        part1s = re.search(salary_range_pattern,part1).group(0) \n    if re.search(salary_range_pattern,part2):\n        part2s = re.search(salary_range_pattern,part2).group(0) \n    if (part1s != ''):\n        row[col_heads['ENTRY_SALARY_GEN']]  = \"\\\\\" + part1s\n    if (part2s != ''):\n        row[col_heads['ENTRY_SALARY_DWP']]  = \"\\\\\" + part2s\n    if re.search('scale pending',part1):\n         row[col_heads['ENTRY_SALARY_GEN']]  = part1\n    if re.search('scale pending',part2):\n         row[col_heads['ENTRY_SALARY_GEN']]  = part2\n       \n    \n    #print (line)\n    #print (part1s)\n    #print (part2s)\n\n    return row\n","d2eb107a":"def req_clr (row):\n#When iterating through the requirements, the previous requirements \n#need to be cleared but other details are retained\n    row[col_heads['EDUCATION_YEARS']] = ''\n    row[col_heads['SCHOOL_TYPE']] = ''\n    row[col_heads['EDUCATION_MAJOR']] = ''\n    row[col_heads['EXPERIENCE_LENGTH']] = ''\n    row[col_heads['FULL_TIME_PART_TIME']] = ''\n    row[col_heads['COURSE_LENGTH']] = ''\n    row[col_heads['COURSE_COUNT']] = ''\n    row[col_heads['COURSE_SUBJECT']] = ''\n    row[col_heads['MISC_COURSE_DETAILS']] = ''\n    row[col_heads['EXP_JOB_CLASS_FUNCTION']] = ''\n    row[col_heads['EXP_JOB_CLASS_ALT_RESP']] = ''\n    row[col_heads['EXP_JOB_CLASS_TITLE']] = ''\n    row[col_heads['REQUIREMENT_SET_ID']] = ''\n    row[col_heads['REQUIREMENT_SUBSET_ID']] = ''\n    row[col_heads['AND_OR']] = ''\n\n    return (row)","9867b0bc":"def fill_row(title,content):\n    #print ('content', content)\n    #print ('row[col_heads[title]] ', row[col_heads[title]] )\n    row[col_heads[title]] = content\n    return True\n\ndef clr_and_fill_row(title,content):\n    row[col_heads[title]] = content\n    return \n ","70c29981":"\n\ndef clean_txt (line):\n    #specific non-standard usage with modifications to some text to make searches work\n    #hard coded: too few examples to generalise\n    line = line.replace ('health and safety',' health & safety')\n    line = line.replace ('Sr.','Senior')\n    line = line.replace ('Pre-','Pre')\n    line = line.replace ('Administratve','Administrative')\n    \n    line = line.replace ('Construction Maintenance Superintendent','Construction and Maintenance Superintendent')\n    return line\n\ndef  convert_words_to_number (line):\n# convert word numbers to numbers as strings\n            \n    oldline = line.replace ('-',' ')\n    line =\"\"\n    for i, word in enumerate(oldline.split()):\n        #print (word)\n        word_l = word.lower()\n        if word_l in numwords:\n            #print ('numwords[word_l]',numwords[word_l])\n            word = str (numwords[word_l])\n        line = line + \" \" + word\n    #print ('oldline',oldline)\n    #print ('line',line)\n    return line\n\nnum_index_pattern = '^(.|'')(\\d)(\\.)(.*)'\nchar_index_pattern = '(?i)^(.|''|.\\()([a-z])(\\.|\\))(.*)'\nedpattern = '(apprenticeship|high school, university, college, trade or technical school|trade school or college|college or trade school|college or university or trade school|college or university|college|university|American Bar Association accredited law school)'\n\n\ndef requirement_and_sub(line, row):\n    #print ('set line', line)\n    if  re.search(num_index_pattern,line):\n        curr_req = re.search(num_index_pattern,line).group(2)\n        #row[col_heads['REQUIREMENT_SET_ID']] = int(curr_req)\n        row[col_heads['REQUIREMENT_SET_ID']] = curr_req\n    elif  re.search(char_index_pattern,line):\n        curr_reqsub = re.search(char_index_pattern,line).group(2)\n        row[col_heads['REQUIREMENT_SUBSET_ID']] = curr_reqsub.upper()\n    else:   \n        row[col_heads['REQUIREMENT_SET_ID']] = ''\n        row[col_heads['REQUIREMENT_SUBSET_ID']] = ''\n    if line.endswith('or'):\n         row[col_heads['AND_OR']] = 'OR'\n    if line.endswith('and'):\n         row[col_heads['AND_OR']] = 'AND'\n       \n        \n    return row\n\ndef education(line, row):\n    semester_pattern ='(\\d{1,3})(.)(semester)'\n    coursework_pattern ='(\\d{1,3} hours)( of course work)(.*?)(;|\\.)'\n    qtr_pattern ='(\\d{1,3})(.)(quarter units)(.*?)( from|in|with|of|at)(.*)(;|\\.)'\n    courses_pattern = '(?i)(completion of )(\\d{1,3})(.)(course)'\n    pattern2 = 'college|university'\n    major_pattern = '(?i)(major in|degree*.?in|degree from|college in)(.*?)(;|including|in |and |from|\\.)'\n    grad_pattern = '(?i)(graduation from)'\n    pattern3 = 'high school or G.E.D. equivalent'\n    course_len = ''\n    exp_found = False\n    if (re.search(major_pattern, line) and not re.search('may be substituted', line)):\n        exp_found = fill_row('EDUCATION_MAJOR',(re.search(major_pattern, line).group(2).replace(\", \", \"|\").replace(\" or \", \"|\")))\n        row[col_heads['DEGREE_REQ']]  = 'YES'\n    if (re.search(grad_pattern, line) and not re.search('may be substituted', line)):\n        row[col_heads['DEGREE_REQ']]  = 'YES'\n\n    if (re.search(edpattern, line)):\n        clr_and_fill_row('SCHOOL_TYPE', re.search(edpattern, line).group(0).upper())\n        course_len_pattern ='(\\d{1,3})(.)(year)(.{1,5})(college or university or trade school|college or university|college|university)'\n        if (re.search(course_len_pattern, line)):\n            #exp_found = fill_row('EDUCATION_YEARS',int(re.search(course_len_pattern, line).group(1)))\n            exp_found = fill_row('EDUCATION_YEARS',(re.search(course_len_pattern, line).group(1)))\n\n    if (re.search(semester_pattern, line)):\n        course_len = re.search(semester_pattern, line).group(1)+ 'S'\n    if (re.search(qtr_pattern, line)):\n        course_len = course_len + re.search(qtr_pattern, line).group(1)+ 'Q'\n        exp_found = fill_row('COURSE_SUBJECT',re.search(qtr_pattern, line).group(6))\n        exp_found = fill_row('COURSE_LENGTH', course_len)\n    if (re.search(courses_pattern, line)):\n        exp_found = fill_row('COURSE_COUNT',re.search(courses_pattern, line).group(2))\n\n    if (re.search(coursework_pattern, line)):\n        exp_found = fill_row('COURSE_SUBJECT',re.search(coursework_pattern, line).group(2) + re.search(coursework_pattern, line).group(3))\n        exp_found = fill_row('COURSE_LENGTH',re.search(coursework_pattern, line).group(1))\n\n        \n#school                    \n    if (re.search(pattern3, line)):\n        exp_found = fill_row('SCHOOL_TYPE', pattern3.upper())\n    return row,exp_found\n\ndef record_apprenticeship (line, row):\n    app_pattern ='(?i)(completion of)(.*?)(apprenticeship)(.*?)(;|\\.|and)'\n    if (re.search(app_pattern, line)):\n        row[col_heads['EDUCATION_MAJOR']] = row[col_heads['EDUCATION_MAJOR']]  + \" \" + re.search(app_pattern, line).group(0)\n    \n    return row\n\ndef apply_certificate(certificate):\n    exp_found = False\n    if not (re.search(\"at the time\", certificate)) and not (re.search(\"medical\", certificate)) \\\n    and not (re.search(\"conducting disciplinary\", certificate)) and not (re.search(\"certificates of occupancy,\", certificate))\\\n    and not (re.search(\"business tax certificates\", certificate)) and not (re.search(\"submit verification\", certificate)) \\\n    and not (re.search(\"issuance\", certificate)):\n\n        exp_found = True\n        row[col_heads['VOCATIONAL_QUAL']] = row[col_heads['VOCATIONAL_QUAL']]  + \" \" + certificate \n    #    if row[col_heads['EDUCATION_MAJOR']] =='':\n        row[col_heads['EDUCATION_MAJOR']] = row[col_heads['EDUCATION_MAJOR']]  + \" \" + row[col_heads['VOCATIONAL_QUAL']] \n        row[col_heads['SCHOOL_TYPE']] = 'CERTIFICATION' \n    return exp_found\n\ndef cert_and_completion(line, row):\n#certification\n    exp_found = False\n#(?s:.*) forces regex to find last match\n\n    cert_pattern ='(?i)(possession of)(.*?)(certificate)(.*?)(;|\\.|and)'\n    cert_pattern1 ='(?i)(certified)(.*?)(certification council)'\n    cert_pattern2 ='(?i)(and|or)(.*?)(certification)(.*?)(;|\\.)'\n    cert_pattern3 ='(?i)(?s:.*)(\\.|;|and |or )(.*?)(certificate)(.*?)(;|\\.|and)'\n\n    cert_pattern ='(?i)(possession|completion of)(.*?)(certificate)(.*?)(;|\\.|and)'\n\n    if (re.search(cert_pattern, line)):\n        exp_found = apply_certificate(re.search(cert_pattern, line).group(1) +\\\n                                                    re.search(cert_pattern, line).group(2) + \\\n                                                    re.search(cert_pattern, line).group(3) + re.search(cert_pattern, line).group(4))\n    elif (re.search(cert_pattern1, line)):\n        exp_found = apply_certificate(re.search(cert_pattern1, line).group(0))\n\n    elif (re.search(cert_pattern2, line)):\n        exp_found = apply_certificate(re.search(cert_pattern2, line).group(2)+re.search(cert_pattern2, line).group(3)\\\n                                      +re.search(cert_pattern2, line).group(4))\n\n    elif (re.search(cert_pattern3, line)):\n        exp_found = apply_certificate( re.search(cert_pattern3, line).group(2) +\\\n                                                    re.search(cert_pattern3, line).group(3) + \\\n                                                    re.search(cert_pattern3, line).group(4))\n\n\n#completion of miscellaneous course and certification requirements that cannot readily be converted to school type and EDUCATION_MAJOR\n    comp_pattern ='(?i)(completion|attainment)(.*?)(in |of )(.*?)(;|\\.)'\n    if (re.search(comp_pattern, line))  and not re.search(edpattern, line): \n        exp_found = fill_row('VOCATIONAL_QUAL', (re.search(comp_pattern, line)).group(0))\n        #print ('line misc 1',line)\n    if (re.search(comp_pattern, line))  and  re.search('high school, university, college, trade or technical school|community college or trade school', line):\n        exp_found = fill_row('VOCATIONAL_QUAL', (re.search(comp_pattern, line)).group(0))\n        #print ('line misc 2',line)\n  \n    return row,exp_found\n\ndef experience_len(line, row):\n    exp_found = False\n\n    pattern ='(full.time|part.time|years as a|\\d{0,1}.\\d{2,6} hours)(.*)(|;|\\.)'\n    if (re.search(pattern, line)):\n        if re.search(pattern, line).group(1) != 'years as a':\n            if  pd.isna(re.search(\"course work\", re.search(pattern, line).group(2))):\n                row[col_heads['FULL_TIME_PART_TIME']] = re.search(pattern, line).group(1).upper()\n                #print ('in ftpt',line)\n        month_pattern = '(\\d{1,3})(.)(month)'\n        year_pattern = '(\\d{1,3})(.)(year)(.{1,5})(full.time|part.time|as a)'\n        if (re.search(month_pattern, line)):\n            fract_yr = (int(re.search(month_pattern, line).group(1)))\/12\n            fract_yr = str(fract_yr)\n            exp_found = fill_row('EXPERIENCE_LENGTH', fract_yr)\n            exp_found = fill_row('EXP_JOB_CLASS_FUNCTION', re.search(pattern, line).group(2))\n        if (re.search(year_pattern, line)):\n            #yr = (int(re.search(year_pattern, line).group(1)))\n            yr = str(re.search(year_pattern, line).group(1))\n            #print ('yr exp A',yr,re.search(pattern, line).group(1))\n            exp_found = fill_row('EXPERIENCE_LENGTH', yr)\n            exp_found = fill_row('EXP_JOB_CLASS_FUNCTION',  re.search(pattern, line).group(2))\n    year_pattern2 ='(\\d{1,3})(.)(years of experience)(.*)(|;|\\.|and|,)'\n    if (re.search(year_pattern2, line)):\n        #yr = (int(re.search(year_pattern2, line).group(1)))\n        yr = str(re.search(year_pattern2, line).group(1))\n        #print ('yr exp B',yr,re.search(year_pattern2, line).group(1))\n        #print ('yr exp lne',line)\n        exp_found = fill_row('EXPERIENCE_LENGTH', yr)\n        exp_found = fill_row('EXP_JOB_CLASS_FUNCTION',  re.search(year_pattern2, line).group(3) + re.search(year_pattern2, line).group(4)  )\n\n    return row,exp_found\n\ndef catch_all (line,row):\n#strip the req id and then print without mod\n    if  re.search(num_index_pattern,line):\n            line = re.search(num_index_pattern,line).group(4)\n    if  re.search(char_index_pattern,line):\n            line = re.search(char_index_pattern,line).group(4)\n    exp_found = fill_row('EXP_JOB_CLASS_FUNCTION', line)\n    return row\n\ndef experience(line, row,exp,last_exp):              \n    row[col_heads['EXP_JOB_CLASS_TITLE']] = exp\n    job_pattern =last_exp.lower()+'(.*)'+'(;|\\.|:|,|\\r|\\n)'\n    #print('exp',line)\n    if (re.search(job_pattern, line.lower())):\n        alt = re.search(job_pattern, line.lower()).group(1)\n        or_pattern = '(.{1,2})(or.)'\n        if (re.match(or_pattern, alt)):\n            #print('exp2',line)\n            exp_found = fill_row('EXP_JOB_CLASS_ALT_RESP', re.search(job_pattern, line.lower()).group(1))\n            exp_found = clr_and_fill_row('EXP_JOB_CLASS_FUNCTION', '')\n        else:\n            class_pattern = '(.*)(in a class|at the level|performing the duties|paid experience as)(.*)(;|\\.|:|,|\\r|\\n)'\n            if re.search(class_pattern,line.lower()):\n                #print('exp3',line)\n\n                exp_found = clr_and_fill_row('EXP_JOB_CLASS_FUNCTION', '')\n                alt_class = \"or \" + re.search(class_pattern, line.lower()).group(2) +re.search(class_pattern, line.lower()).group(3)+ re.search(class_pattern, line.lower()).group(4)\n                exp_found = fill_row('EXP_JOB_CLASS_ALT_RESP', alt_class)\n            else:\n                job_pattern2 ='(experience)(.*)' +last_exp.lower()+'(.*)'+'(;|\\.|:|,|\\r|\\n)'\n                if re.search(job_pattern2, line.lower()):\n                    exp_found = fill_row('EXP_JOB_CLASS_FUNCTION', re.search(job_pattern2, line.lower()).group(0))\n                else:\n                    exp_found = fill_row('EXP_JOB_CLASS_FUNCTION', re.search(job_pattern, line.lower()).group(1))\n    return row\n\n","86b12ab2":"def process_state (state,body,line,row,job,data_list):\n    #print ('state',state)\n    global eda_row\n    if state == 'duties':\n        body += line\n        row[col_heads['JOB_DUTIES']] = body.replace('\\n','')\n    if state == 'annualsalary':\n        line = line.replace (',','').replace(' to ','-')\n        body += line\n        entry_salaries (body,row)\n    if state == 'notes':\n        row[col_heads['NOTES']] += line\n    if state == 'selection':\n        row[col_heads['SELECTION_PROCESS']] += line        \n    if state == 'requirements':\n        exp_found = False\n        line = clean_txt (line)       \n        if (line !=''):\n            sub_pattern = 'substitut'\n            #if substitute is found in a requirement, it refers to substituting requirements\n            #and is not a new requirement. Hence the information is recorded but not processed\n            if (re.search(sub_pattern, line)):\n                    exp_found = fill_row('EXP_JOB_CLASS_FUNCTION', line)\n            else:\n                line = convert_words_to_number (line)\n                row = requirement_and_sub(line, row)\n                row,exp_found =  education(line, row)\n                row,exp_found = cert_and_completion(line, row)\n                row = record_apprenticeship (line, row)\n                row,exp_found = experience_len(line, row)\n                exp,last_exp = find_experience (line,job)\n                if exp == '' and not exp_found:\n                    catch_all (line,row)\n                else:\n                    if (exp):\n                        row = experience(line, row,exp,last_exp)\n                    #Specials where job requirement is defined in terms of the job itself\n                    if re.search('Zoo Registrar',line):\n                        exp_found = fill_row('EXP_JOB_CLASS_ALT_RESP','performing the duties of a Zoo Registrar')\n                        exp_found = clr_and_fill_row('EXP_JOB_CLASS_FUNCTION', '')\n                    if re.search('Administratve Hearing Examiner',line):\n                        exp_found = fill_row('EXP_JOB_CLASS_ALT_RESP','within the past two years from the date of filing as an exempt or contract Administratve Hearing Examiner')\n                        exp_found = clr_and_fill_row('EXP_JOB_CLASS_FUNCTION', '')\n   \n            #one row is allocated for each requirement            \n            save_row = row.copy()\n            data_list.append(save_row)\n            #however for the EDA it is convenient to have one row per job\n            len_row = len(row)\n            for i in range(len_row):\n                if eda_row[i] == '':\n                     eda_row[i] = row[i]\n                elif  row[i] not in eda_row[i]:\n                    eda_row[i] = eda_row[i] + ' ' + row[i]\n            row = req_clr (row)\n    return row, body,data_list","8a3734f0":"#each sub requirement generates a new row\n#to fulfill the requirements of the sample file,when a file is finished, the last information found \n#must be \"backfilled\" into the earlier rows\n\ndef backfill (data_list, start_job_index, row):\n    \n    last_job_index = len(data_list)\n    for i in range ( start_job_index,last_job_index):\n        data_list[i][col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']]\n        data_list[i][col_heads['DRIVERS_LICENSE_REQ']] = row[col_heads['DRIVERS_LICENSE_REQ']]\n        data_list[i][col_heads['DRIV_LIC_TYPE']] = row[col_heads['DRIV_LIC_TYPE']]\n        data_list[i][col_heads['ADDTL_LIC']] = row[col_heads['ADDTL_LIC']]\n        data_list[i][col_heads['DEGREE_REQ']] = row[col_heads['DEGREE_REQ']]\n\n    data_list[start_job_index][col_heads['NOTES']] = row[col_heads['NOTES']]\n    data_list[start_job_index][col_heads['SELECTION_PROCESS']] = row[col_heads['SELECTION_PROCESS']] \n\n    eda_row[col_heads['EXAM_TYPE']] = row[col_heads['EXAM_TYPE']]\n    eda_row[col_heads['DRIVERS_LICENSE_REQ']] = row[col_heads['DRIVERS_LICENSE_REQ']]\n    eda_row[col_heads['DRIV_LIC_TYPE']]= row[col_heads['DRIV_LIC_TYPE']]\n    eda_row[col_heads['ADDTL_LIC']] = row[col_heads['ADDTL_LIC']]\n    eda_row[col_heads['DEGREE_REQ']] = row[col_heads['DEGREE_REQ']]\n   #print ('eda_row',eda_row)\n    save_row = eda_row.copy()\n    eda_data_list.append(save_row)\n\n    return last_job_index","ebc1de03":" def line_interp (line, title_line, row, state, body,level,job,data_list):\n    class_pattern ='(Code:.*)(\\d{4})(.*)'\n    sub_pattern = 'substitut'\n\n#save the job title and report if inconsistent    \n    if line != '' and title_line:\n        j = line.replace('\\tREVISED','')\n        job= j.replace('\\n','').lower().title()\n        #in some cases the first line of the file is not the job title\n        if job.upper() != 'CAMPUS INTERVIEWS ONLY':\n            row[col_heads['JOB_CLASS_TITLE']] = job\n            title_line = False\n        else:\n            print ()\n            print ('CAMPUS INTERVIEWS ONLY at file top',row[col_heads['FILE_NAME']] )\n            print ()\n            \n#save the job class\n    if (re.search(class_pattern, line)):\n        if row[col_heads['JOB_CLASS_NO']]  =='':\n            \n            \n            job_class = re.search(class_pattern, line).group(2)\n            if re.search (job_class,row[col_heads['FILE_NAME']]):            \n                #only save the first instance because\n                #SENIOR ELECTRIC SERVICE REPRESENTATIVE has wrong code at btm of file\n                row[col_heads['JOB_CLASS_NO']] = re.search(class_pattern, line).group(2)\n            else:\n                print ()\n                print (row[col_heads['FILE_NAME']] ,'JOB_CLASS mismatch')\n                print ()\n                \n\n    elif \"Open Date:\" in line:\n        row[col_heads['OPEN_DATE']]  = line.split(\"Open Date:\")[1].split(\"(\")[0].strip().replace ('-','\/')\n\n#use the upper case headings to define the current state\n#process the state on reaching the next heading\n    elif (line.isupper()and not \"$\" in line):\n        state = ''\n        body =''\n    if state != '':\n        row, body,data_list = process_state (state,body,line,row,job,data_list)\n    elif re.search('DUTIES',line):\n        state = 'duties'\n    elif re.search('REQUIREMENT',line):\n        state = 'requirements'\n    elif re.search('(ANNUAL SALARY)|(ANNUALSALARY)',line):\n        state = 'annualsalary'\n    elif re.search('NOTE',line):\n        state = 'notes'\n    elif re.search('SELECTION',line):\n        state = 'selection'\n\n#look for licence and degree type anywhere in the file as the information is dispersed        \n    driver_licence(line)\n    degree_req(line)\n    level = examination_type(line,level)\n    return line, title_line, row, state, body,level, job,data_list","b524dd69":"def chk_file_valid(filename):\n    #files are marked manually as invalid where appropriate\n    file_valid = True\n    if filename == 'ANIMAL CARE TECHNICIAN SUPERVISOR 4313 122118.txt':\n        #excluded because bulliten text is wrong\n        file_valid = False\n    if filename == 'WASTEWATER COLLECTION SUPERVISOR 4113 121616.txt':\n        #excluded because bulliten text is wrong\n        file_valid = False\n    if filename == 'SENIOR EXAMINER OF QUESTIONED DOCUMENTS 3231 072216 REVISED 072716.txt':\n        #excluded because bulliten text is wrong\n        file_valid = False\n    if filename == 'SENIOR UTILITY SERVICES SPECIALIST 3753 121815 (1).txt':\n        #excluded because a newer bulliten exists\n        file_valid = False\n    if filename == 'CHIEF CLERK POLICE 1219 061215.txt':\n        #excluded because a newer bulliten exists\n        file_valid = False\n       \n    if file_valid == False:\n        print ()\n        print ('invalid', filename)\n        print()\n    return file_valid\n    ","a2ef9942":"bulletin_dir = \"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\"\ndata_list = []\neda_data_list = []\nbody = ''\nstate = ''\njob = ''\nlevel = 0\ncnt = 0\nstart_job_index = 0\nglobal eda_row\n\nfor filename in os.listdir(bulletin_dir):\n     if cnt >-1 and cnt <700:   #use this line to control number of files reviewed during testing\n        row = [''] * len_ch\n        eda_row= [''] * len_ch\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            row[col_heads['FILE_NAME']] = filename\n            print ('. ',end=\"\")\n            title_line = True\n            file_valid = chk_file_valid(filename)\n            if (file_valid):\n                for index,line in enumerate(f.readlines()):\n                    line = line.rstrip().lstrip()\n                    if (line !='' and line != \"OR\" ):\n                        pattern = '(.*?)(; or)(.*)' \n                        #sometimes a significant alternative is included within a requirement\n                        if re.search(pattern, line) and(len(re.search(pattern, line).group(3))) > 40 and  state == 'requirements':\n                            print () \n                            print ('double requirement found', filename)\n                            print()   \n\n                            _, title_line, row, state, body,level,job,data_list = \\\n                                    line_interp (re.search(pattern, line).group(1) + re.search(pattern, line).group(2), title_line, row, state, body,level,job,data_list)\n                            _, title_line, row, state, body,level,job,data_list = \\\n                                    line_interp (re.search(pattern, line).group(3), title_line, row, state, body,level,job,data_list)\n                        else:\n                            line, title_line, row, state, body,level,job,data_list = \\\n                                line_interp (line, title_line, row, state, body,level,job,data_list)             \n                start_job_index = backfill(data_list,start_job_index,row)\n        cnt += 1\n\ndf_job_class = pd.DataFrame(data_list)\ndf_job_class.columns = column_heads\ndf_eda_exam = pd.DataFrame(eda_data_list)\ndf_eda_exam.columns = column_heads\n","de7ff455":"#pd.options.display.max_colwidth = 200 ;-1\npd.set_option('max_colwidth', 700)\nwith pd.option_context(\"display.max_rows\", 700): display (df_job_class)","133b59bf":"#cannot sort job class\n# df_job_class.sort_values('FILE_NAME', inplace = True)\n# df_job_class = df_job_class.reset_index(drop=True)\n# df_eda_exam.sort_values('FILE_NAME', inplace = True)\n# df_eda_exam = df_eda_exam.reset_index(drop=True)\n\n\n# df_job_class.head(640)","bb417c03":"workingpath = ('..\/working\/')\n\ndf_job_class.to_csv(workingpath + 'job_class.csv')\ndf_eda_exam.to_csv(workingpath + 'eda_exam.csv')","f140cc13":"workingpath = ('..\/working\/')\ndf_job_class = pd.read_csv(workingpath + 'job_class.csv', \\\n                                 converters={'JOB_CLASS_NO': str, 'REQUIREMENT_SET_ID': str, \\\n                                             'EXP_JOB_CLASS_TITLE': str},index_col=0)\ndf_eda_exam = pd.read_csv(workingpath + 'eda_exam.csv', \\\n                                 converters={'JOB_CLASS_NO': str, 'REQUIREMENT_SET_ID': str, 'EXP_JOB_CLASS_TITLE': str},index_col=0)\ndf_job_class = df_job_class.replace('',np.NaN)\ndf_eda_exam = df_eda_exam.replace('',np.NaN)\n\ndf_job_class.head(640)\n","0dad1da4":"include =['object', 'float', 'int'] \ndf_eda_exam.describe( include = include)","918e880d":"print ('FILE_NAME:       ',df_eda_exam['FILE_NAME'].apply(type).eq(str).all())\nprint ('JOB_CLASS_TITLE: ',df_eda_exam['JOB_CLASS_TITLE'].apply(type).eq(str).all())\nprint ('JOB_CLASS_NO:    ',df_eda_exam['JOB_CLASS_NO'].apply(type).eq(str).all())\nprint ('JOB_DUTIES:      ',df_eda_exam['JOB_DUTIES'].apply(type).eq(str).all())\nprint ('DEGREE_REQ):     ',df_eda_exam['DEGREE_REQ'].apply(type).eq(str).all())\nprint ('EXAM_TYPE:       ',df_eda_exam['EXAM_TYPE'].apply(type).eq(str).all())\n\n","fb6303c2":"print ('EDUCATION_YEARS:   ',df_eda_exam['EDUCATION_YEARS'].apply(type).eq(float).any())\nprint ('EXPERIENCE_LENGTH: ',df_eda_exam['EXPERIENCE_LENGTH'].apply(type).eq(float).any())\n\nprint ('OPEN_DATE: ',df_eda_exam['OPEN_DATE'].dtype)\n\n","bfdfaad8":"#The Describe function output above indicates that 1 job bulletin has no Job_Class\n#This cell showed it was:\n#Vocational Worker DEPARTMENT OF PUBLIC WORKS.txt\n# df_eda_exam['JOB_CLASS_NO']= np.where(pd.isna(df_eda_exam['JOB_CLASS_NO']), \n#                             '    ', \n#                             df_eda_exam['JOB_CLASS_NO'])\n# df_eda_exam.sort_values('JOB_CLASS_NO',  inplace = True)\n# df_eda_exam.head(10)","0d160b30":"df_job_class['JOB_CLASS_NO']= np.where(pd.isna(df_job_class['JOB_CLASS_NO']), \n                            '0000', \n                            df_job_class['JOB_CLASS_NO'])\ndf_eda_exam['JOB_CLASS_NO']= np.where(pd.isna(df_eda_exam['JOB_CLASS_NO']), \n                            '0000', \n                            df_eda_exam['JOB_CLASS_NO'])\ndf_eda_exam.sort_values('JOB_CLASS_NO',  inplace = True)\n\ndf_eda_exam.head(2)\n","58054a52":"df_eda_exam['JOB_CLASS_NO']= np.where(pd.isna(df_eda_exam['JOB_CLASS_NO']), \n                            '0000', \n                            df_eda_exam['JOB_CLASS_NO'])\ndf_eda_exam.sort_values('JOB_CLASS_NO',  inplace = True)\n\ndf_eda_exam.head(2)","233f08ea":"#The Describe function output above shows that 7 job bulletins have no duty\n#This cell finds the 7:\n#ENGINEER OF FIRE DEPARTMENT\n#Vocational Worker DEPARTMENT OF PUBLIC WORKS\n#FIRE ASSISTANT CHIEF \n#FIRE BATTALION CHIEF\n#FIRE HELICOPTER PILOT \n#FIRE INSPECTOR \n#APPARATUS OPERATOR \n\n# df_eda_exam['JOB_DUTIES']= np.where(pd.isna(df_eda_exam['JOB_DUTIES']), \n#                             '    ', \n#                             df_eda_exam['JOB_DUTIES'])\n# df_eda_exam.sort_values('JOB_DUTIES',  inplace = True)\n# df_eda_exam.head(10)","2b73f316":"# Replace missing duties NOT PROVIDED\ndf_job_class['JOB_DUTIES']= np.where(pd.isna(df_job_class['JOB_DUTIES']), \n                            'NOT PROVIDED', \n                            df_job_class['JOB_DUTIES'])\n\n#df_job_class.head(1)\n","c1f1159a":"\ndf_eda_exam['JOB_DUTIES']= np.where(pd.isna(df_eda_exam['JOB_DUTIES']), \n                            'NOT PROVIDED', \n                            df_eda_exam['JOB_DUTIES'])\n\ndf_eda_exam.head(2)\n","2ada9c50":"df_job_class['DRIV_LIC_TYPE']= np.where(pd.isna(df_job_class['DRIV_LIC_TYPE']), \n                            'NONE', \n                            df_job_class['DRIV_LIC_TYPE'])\n#print (df_job_class['DEGREE_REQ'].nunique())\ndf_job_class.head(20)\ndf_eda_exam['DRIV_LIC_TYPE']= np.where(pd.isna(df_eda_exam['DRIV_LIC_TYPE']), \n                            'NONE', \n                            df_eda_exam['DRIV_LIC_TYPE'])\n#print (df_eda_exam['DEGREE_REQ'].nunique())\ndf_eda_exam.head(20)","50d4e2c4":"df_job_class['SCHOOL_TYPE']= np.where(pd.isna(df_job_class['SCHOOL_TYPE']), \n                            'NONE', \n                            df_job_class['SCHOOL_TYPE'])\n#print (df_job_class['DEGREE_REQ'].nunique())\ndf_job_class.head(20)\ndf_eda_exam['SCHOOL_TYPE']= np.where(pd.isna(df_eda_exam['SCHOOL_TYPE']), \n                            'NONE', \n                            df_eda_exam['SCHOOL_TYPE'])\n#print (df_eda_exam['DEGREE_REQ'].nunique())\ndf_eda_exam.head(20)","41c55aba":"df_job_class['DEGREE_REQ']= np.where(pd.isna(df_job_class['DEGREE_REQ']), \n                            'NO', \n                            df_job_class['DEGREE_REQ'])\n#print (df_job_class['DEGREE_REQ'].nunique())\ndf_job_class.head(20)\ndf_eda_exam['DEGREE_REQ']= np.where(pd.isna(df_eda_exam['DEGREE_REQ']), \n                            'NO', \n                            df_eda_exam['DEGREE_REQ'])\n#print (df_eda_exam['DEGREE_REQ'].nunique())\ndf_eda_exam.head(20)","8c0859f9":"#types not right yet\ndf_job_class[\"OPEN_DATE\"] = df_job_class[\"OPEN_DATE\"].astype('datetime64[ns]')\ndf_eda_exam[\"OPEN_DATE\"] = df_eda_exam[\"OPEN_DATE\"].astype('datetime64[ns]')\n","e7d3612b":"print ('FILE_NAME:       ',df_eda_exam['FILE_NAME'].apply(type).eq(str).all())\nprint ('JOB_CLASS_TITLE: ',df_eda_exam['JOB_CLASS_TITLE'].apply(type).eq(str).all())\nprint ('JOB_CLASS_NO:    ',df_eda_exam['JOB_CLASS_NO'].apply(type).eq(str).all())\nprint ('JOB_DUTIES:      ',df_eda_exam['JOB_DUTIES'].apply(type).eq(str).all())\nprint ('DEGREE_REQ):     ',df_eda_exam['DEGREE_REQ'].apply(type).eq(str).all())\nprint ('EXAM_TYPE:       ',df_eda_exam['EXAM_TYPE'].apply(type).eq(str).all())\n\nprint ('EDUCATION_YEARS:   ',df_eda_exam['EDUCATION_YEARS'].apply(type).eq(float).any())\nprint ('EXPERIENCE_LENGTH: ',df_eda_exam['EXPERIENCE_LENGTH'].apply(type).eq(float).any())\n\nprint ('OPEN_DATE: ',df_eda_exam['OPEN_DATE'].dtype)\n\n\n\n","3ce72773":"#to print out the full file\npd.options.display.max_colwidth = 200 ;-1\nwith pd.option_context(\"display.max_rows\", 2000): display (df_job_class)","79d3512e":"include =['object', 'float', 'int'] \ndf_eda_exam.describe( include = include)","3f048d39":"pd.options.display.max_colwidth = 200 ;-1\nwith pd.option_context(\"display.max_rows\", 2000): display (df_eda_exam)","436dd8f5":"df_exam_group = df_job_class.groupby('REQUIREMENT_SET_ID').count()\ndf_exam_group.head(20)","e390bc36":"df_exam_group = df_job_class.groupby('REQUIREMENT_SUBSET_ID').count()\ndf_exam_group.head(20)","4685f43f":"df_exam_group = df_job_class.groupby('AND_OR').count()\ndf_exam_group.head(20)","de66f6e2":"df_job_class_group = df_job_class.groupby('SCHOOL_TYPE').count()\n\ndf_job_class_group.head(30)","88961059":"df_exam_group = df_eda_exam.groupby('SCHOOL_TYPE').count()\n\ndf_exam_group.head(30)","c3cf9c55":"df_job_class_f =df_job_class['SCHOOL_TYPE'].value_counts().reset_index()\ndf_job_class_f['index']=df_job_class_f['index'].apply(lambda x : x.title())\ndf_job_class_f=df_job_class_f.groupby('index',as_index=False).agg('sum')\nlabels=df_job_class_f['index']\nsizes=df_job_class_f['SCHOOL_TYPE']\nplt.figure(figsize=(5,7))\nplt.pie(sizes,labels=labels)\nplt.gca().axis('equal')\nplt.title('SCHOOL_TYPE including apprenticeship and certification' )\nplt.show()","dcee817d":"df_exam_group = df_job_class.groupby('EDUCATION_MAJOR').count()\n\nwith pd.option_context(\"display.max_rows\", 2000): display (df_exam_group)","0a99f405":"df_exam_group = df_job_class.groupby('FULL_TIME_PART_TIME').count()\n\ndf_exam_group.head(20)","1b065559":"df_eda_exam_group = df_eda_exam.groupby('COURSE_COUNT').count()\ndf_eda_exam_group.head(30)","c3c73046":"df_eda_exam_group = df_eda_exam.groupby('COURSE_LENGTH').count()\ndf_eda_exam_group.head(30)","dd1fa304":"df_eda_exam_group = df_eda_exam.groupby('COURSE_SUBJECT').count()\nwith pd.option_context(\"display.max_rows\", 2000): display (df_exam_group)","97905504":"df_eda_exam_group = df_eda_exam.groupby('DRIVERS_LICENSE_REQ').count()\ndf_eda_exam_group.head(30)","e496a033":"df_eda_exam_group = df_eda_exam.groupby('DRIV_LIC_TYPE').count()\ndf_eda_exam_group.head(30)","cc3a7c29":"df_eda_exam_group = df_eda_exam.groupby('ADDTL_LIC').count()\ndf_eda_exam_group.head(20)","fb8e579f":"df_eda_exam_f =df_eda_exam['EXAM_TYPE'].value_counts().reset_index()\ndf_eda_exam_f['index']=df_eda_exam_f['index'].apply(lambda x : x.title())\ndf_eda_exam_f=df_eda_exam_f.groupby('index',as_index=False).agg('sum')\nlabels=df_eda_exam_f['index']\nsizes=df_eda_exam_f['EXAM_TYPE']\nplt.figure(figsize=(5,7))\nplt.pie(sizes,explode=(0.1, 0.1,0.1,0.1,0.1),labels=labels)\nplt.gca().axis('equal')\nplt.title('Exam Type for all Job Bulletins' )\nplt.show()","920631c7":"df_eda_exam_group = df_eda_exam.groupby('EXAM_TYPE').count()\ndf_eda_exam_group.head(30)","39299c79":"df_exam_group = df_eda_exam.groupby('VOCATIONAL_QUAL').count()\n\nwith pd.option_context(\"display.max_rows\", 2000): display (df_exam_group)","06fc2e58":"df_exam_group = df_eda_exam.groupby('DEGREE_REQ').count()\n\ndf_exam_group.head(30)","0c96a4d5":"\ndf_eda_exam = df_eda_exam.sort_values('ENTRY_SALARY_GEN')\n\ndf_eda_exam['entry_salary'] = 99\ndf_eda_exam['final_salary'] = 99\ndf_eda_exam['pc_range'] = 99\n\n#with pd.option_context(\"display.max_rows\", 2000): display (df_eda_exam)\n","5cca2075":"salary_flat_pattern = '(\\d{4,6})(.*)(flat-rated)'\nsalary_range_pattern = '(\\d{4,6})(.*?)(\\d{4,6})'\n\n#df_salary_eda['entry_salary'] = df_salary_eda['ENTRY_SALARY_GEN']\n\nfor i, row in df_eda_exam.iterrows():\n    salary_range =  ''\n    if not pd.isna(row['ENTRY_SALARY_GEN']):\n        salary_range = row['ENTRY_SALARY_GEN']\n    elif not pd.isna(row['ENTRY_SALARY_DWP']):\n        salary_range = row['ENTRY_SALARY_DWP']\n   # print ('FILE_NAME', row['FILE_NAME'])\n    #print ('salary_range',salary_range)\n    \n    #print ('ENTRY_SALARY_GEN',row['ENTRY_SALARY_GEN'])\n    #print ('ENTRY_SALARY_DWP',row['ENTRY_SALARY_DWP'])\n      \n    entry_salary = -1\n    final_salary = -1\n\n    if re.search(salary_flat_pattern,salary_range):\n        #print ('re.search(salary_flat_pattern,salary_range).group(1)',re.search(salary_flat_pattern,salary_range).group(1))\n        entry_salary = int(re.search(salary_flat_pattern,salary_range).group(1))\n        final_salary = 0\n    if re.search(salary_range_pattern,salary_range):\n        #print ('re.search(salary_range_pattern,salary_range).group(1)',re.search(salary_range_pattern,salary_range).group(1))\n        entry_salary = int(re.search(salary_range_pattern,salary_range).group(1))\n        final_salary = int(re.search(salary_range_pattern,salary_range).group(3))\n    #print ('entry_salary',entry_salary)\n    #print ('final_salary',final_salary)\n    if final_salary != 0:\n        pc_range = 100* (final_salary - entry_salary)\/entry_salary\n    else:\n        pc_range = 0\n    df_eda_exam.loc[i,'entry_salary'] = entry_salary\n    df_eda_exam.loc[i,'final_salary'] = final_salary\n    df_eda_exam.loc[i,'pc_range'] = pc_range\ndf_eda_exam.describe()                                  ","8131eb41":"bins = pd.cut(df_eda_exam['entry_salary'], [0, 50000, 75000, 100000, 150000, 200000, 250000])\n\ndf_eda_exam.groupby(bins)['entry_salary'].agg(['count'])\n\n","79bcf5a8":"df_eda_exam = df_eda_exam.sort_values('entry_salary')\ndf_eda_exam.head(3)\n","24286127":"import seaborn as sns\n\nplt.rcParams['figure.figsize']=(12,6)\nsns.distplot(df_eda_exam.entry_salary.fillna(axis=0, method='ffill'), bins =30, color = 'red')\n","ae2f9f0f":"labels=['Hispanic or Latino','White','Asian','Black or African American','Two or More Races','American Indian and Alaska Native','Native Hawaiian and Other Pacific Islander']\nsizes=[48.7,28.4,11.7,8.9,3.5,0.7,.2]\nplt.figure(figsize=(5,7))\nplt.pie(sizes,explode=(0.1, 0.1,0.1,0.1,0.1,0.1,0.1),labels=labels)\nplt.gca().axis('equal')\nplt.title('LA Ethnicity' )\nplt.show()","61940935":"#df_ge = pd.read_csv(\"..\/input\/la-applicants-gender-and-ethnicity\/Job_Applicants_by_Gender_and_Ethnicity.csv\")\ndf_ge = pd.read_csv(\"..\/input\/la-applicants-gender-and-ethnicity\/rows.csv\")\n\nHispanic = 48.7\nWhite =  28.4\nAsian =  11.7\nBlack_or_African_American =  8.9\nTwo_or_more_races = 3.5\nAmerican_Indian_and_Alaska_Native = 0.7\nNative_Hawaiian_and_Pacific_Islander = 0.2\n\ndf_g_and_e = df_ge.copy()\n#df_g_and_e = df_g_and_e.rename(columns={'Job Description': 'JOB_DESCRIPTION', 'Job Number': 'JOB_CLASS_NO'})\ndf_g_and_e = df_g_and_e.rename(columns={'Job Description': 'JOB_DESCRIPTION'})\n\n#df_g_and_e['JOB_CLASS_NO'] = df_g_and_e['JOB_CLASS_NO'].map(lambda x: str(x)[:4])\ndf_g_and_e['JOB_CLASS_NO'] = df_g_and_e['JOB_DESCRIPTION'].map(lambda x: re.search ('(\\d{4})',x).group(1)  if re.search ('(\\d{4})',x)  else None)\n\ndf_g_and_e.fillna('NONE', inplace = True)\n\nclass_pattern = '(\\d{4})'\nfor index, row in df_g_and_e.iterrows():\n    #print('before',df_g_and_e.iloc[index]['JOB_DESCRIPTION'])\n    #print('before',df_g_and_e.iloc[index]['Job Number'])\n    if re.search ('(^\\d{4})',df_g_and_e.iloc[index]['Job Number']):\n        #print ('in')\n        df_g_and_e.at[index,'JOB_CLASS_NO'] = df_g_and_e.iloc[index]['Job Number'][:4]\n    else:\n        #print ('out')\n        df_g_and_e.at[index,'JOB_CLASS_NO'] = re.search (class_pattern,df_g_and_e.iloc[index]['JOB_DESCRIPTION']).group(1)\n    #print('after',df_g_and_e.iloc[index]['JOB_CLASS_NO'])\n        \n\n                                                           \ndf_g_and_e.insert(loc = 3, column = 'HISPANIC_REP', value = 100 * df_g_and_e['Hispanic']\/ \n                  (Hispanic * (df_g_and_e['Apps Received']- df_g_and_e['Unknown_Ethnicity'])))\ndf_g_and_e.insert(loc = 4, column = 'CAUCASIAN_REP', value = 100 * df_g_and_e['Caucasian']\/ \n                  (White * (df_g_and_e['Apps Received']- df_g_and_e['Unknown_Ethnicity'])))\ndf_g_and_e.insert(loc = 5, column = 'ASIAN_REP', value = 100 * df_g_and_e['Asian']\/ \n                  (Asian * (df_g_and_e['Apps Received']- df_g_and_e['Unknown_Ethnicity'])))\ndf_g_and_e.insert(loc = 6, column = 'BLACK_REP', value = 100 * df_g_and_e['Black']\/ \n                  (Black_or_African_American * (df_g_and_e['Apps Received']- df_g_and_e['Unknown_Ethnicity'])))\ndf_g_and_e.insert(loc = 7, column = 'AI_OR_AN_REP', value = 100 * df_g_and_e['American Indian\/ Alaskan Native']\/ \n                  (American_Indian_and_Alaska_Native * (df_g_and_e['Apps Received']- df_g_and_e['Unknown_Ethnicity'])))\ndf_g_and_e.insert(loc = 8, column = 'FILIPINO_REP', value = 100 * df_g_and_e['Filipino']\/ \n                  (Native_Hawaiian_and_Pacific_Islander * (df_g_and_e['Apps Received']- df_g_and_e['Unknown_Ethnicity'])))\n\ndf_g_and_e =df_g_and_e.sort_values('Apps Received', ascending  = False)\ndf_g_and_e.describe()","e5d41b24":"with pd.option_context(\"display.max_rows\", 2000): display (df_g_and_e)\n","450e9f82":"df_eda_ge_prep = df_eda_exam.copy()\ndf_eda_ge_prep = df_eda_ge_prep.drop (['REQUIREMENT_SET_ID','REQUIREMENT_SUBSET_ID','AND_OR','JOB_DUTIES','EXP_JOB_CLASS_ALT_RESP', \\\n                                    'EXP_JOB_CLASS_FUNCTION','OPEN_DATE','NOTES','SELECTION_PROCESS'], axis = 1)\ndf_eda_ge = df_eda_ge_prep.merge(right=df_g_and_e, how = 'right',\n                                            left_on ='JOB_CLASS_NO',\n                                            right_on ='JOB_CLASS_NO')\n\ndf_eda_ge.describe()","cd31d678":"with pd.option_context(\"display.max_rows\", 2000): display (df_eda_ge)","2e160a85":"def plot_full_sample_cmp_sub(var_f, var_s, index, column):\n    labels1=var_f[index]\n    sizes1=var_f[column]\n    labels2=var_s[index]\n    sizes2=var_s[column]\n    colors = ['yellowgreen','red','gold','lightskyblue','lightcoral','blue','pink', 'darkgreen','yellow','grey','violet','magenta','cyan']\n    \n    fig = plt.figure()\n    ax1 = fig.add_axes([0, 0, .5, .5], aspect=1)\n    ax1.pie(sizes1, labels=labels1, colors = colors,radius = 1.2)\n    ax1.set_title ('ALL JOB BULLETINS\\n')\n\n    ax2 = fig.add_axes([.5, .0, .5, .5], aspect=1)\n    ax2.pie(sizes2, labels=labels2,colors = colors, radius = 1.2)\n    ax2.set_title ('SAMPLE WITH APPLICATIONS DATA\\n')\n    plt.show()\n    return\n\ndef plot_full_sample_cmp(column):\n    var_f=df_eda_exam[column].value_counts().reset_index()\n    var_f['index']=var_f['index'].apply(lambda x : x.title())\n    var_f=var_f.groupby('index',as_index=False).agg('sum')\n\n    var_s=df_eda_ge[column].value_counts().reset_index()\n    var_s['index']=var_s['index'].apply(lambda x : x.title())\n    var_s=var_s.groupby('index',as_index=False).agg('sum')\n\n    plot_full_sample_cmp_sub(var_f, var_s, 'index', column)\n    return\n\n\nplot_full_sample_cmp ('EXAM_TYPE')","cc4c101a":"plot_full_sample_cmp ('SCHOOL_TYPE')","560a7eb4":"bins = pd.cut(df_eda_exam['entry_salary'], [-1, 50000, 75000, 100000, 150000, 200000, 250000])\nvar_f = df_eda_exam.groupby(bins)['entry_salary'].agg(['count']).reset_index()\nlabels1=var_f['entry_salary']\nsizes1=var_f['count']\n\nbins = pd.cut(df_eda_ge['entry_salary'], [-1, 50000, 75000, 100000, 150000, 200000, 250000])\nvar_s = df_eda_ge.groupby(bins)['entry_salary'].agg(['count']).reset_index()\n\nplot_full_sample_cmp_sub(var_f, var_s, 'entry_salary', 'count')\nplt.show()","4ab8087c":"plot_full_sample_cmp ('DEGREE_REQ')","3e4dd086":"plot_full_sample_cmp ('FULL_TIME_PART_TIME')","b605fd5a":"plot_full_sample_cmp ('DRIVERS_LICENSE_REQ')","b837d752":"df_ge_summ = df_ge.copy()\ndf_ge_summ.drop(df_ge_summ.columns[[0, 1, 2,3,7,8,9,10,11,12,13]], axis=1, inplace=True)\ndf_ge_summ = df_ge_summ.sum(axis = 0, skipna = True)\n\ndf_ge_summ.head()","45a6747a":"labels=df_ge_summ.index\nsizes=df_ge_summ\nplt.figure(figsize=(5,7))\nplt.pie(sizes,explode=(0.1, 0.1,0.1),labels=labels)\nplt.gca().axis('equal')\nplt.title('Gender of all applicatants' )\nplt.show()","75b61cd6":"df_ge_summ = df_ge.copy()\ndf_ge_summ.drop(df_ge_summ.columns[[0, 1, 2,3,4,5,6,13]], axis=1, inplace=True)\ndf_ge_summ = df_ge_summ.sum(axis = 0, skipna = True)\n#df_ge_summ.head(10)","44b176c7":"#LA population data\n#using the sample application data titles and removing 2 or races as we don't have application data for this category\ntotal = 48.7 + 28.4 + 11.7 + 8.9 +0.7 + .2\nBlack = 8.9\/total\nHispanic = 48.7\/total\nAsian = 11.7\/total\nCaucasian = 28.4\/total\nAmerican_IndianAlaskan_Native = .7\/total\nFilipino = .2\/total\n","179b0d2f":"def cmp_apps_with_pop (labels2,sizes2, title):\n\n    labels1=['Black','Hispanic','Asian','Caucasian','American Indian\/Alaska Native','Filipino']\n    sizes1=[Black,Hispanic,Asian,Caucasian,American_IndianAlaskan_Native,Filipino]\n    fig = plt.figure()\n    ax1 = fig.add_axes([0, 0, .5, .5], aspect=1)\n    ax1.pie(sizes1, explode=(0.1, 0.1, 0.1,0.1,0.1,0.1), labels=labels1, radius = 1.2)\n    ax1.set_title ('LA ETHNICITY\\n')\n\n    ax2 = fig.add_axes([.5, .0, .5, .5], aspect=1)\n    ax2.pie(sizes2, labels=labels2, explode=(0.1, 0.1, 0.1,0.1, 0.1,0.1),radius = 1.2)\n    ax2.set_title (title)\n    plt.show()\n    return\n\ncmp_apps_with_pop (df_ge_summ.index,df_ge_summ, 'APPLICATIONS FOR ALL JOBS\\n')\n","04e05dc6":"df_eda_ge_group = df_eda_ge.groupby('DEGREE_REQ').sum()\ndf_eda_ge_group.head()","7cf1182d":"df_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Female':'Male']\ndf_eda_ge_group_T.head(30)","4bd8d927":"def cmp_apps_with_pop_g (labels,sizes1, title1, sizes2, title2):\n\n    fig = plt.figure()\n    ax1 = fig.add_axes([0, .0, .5, .5], aspect=1)\n    ax1.pie(sizes1, labels=labels, explode=(0.1, 0.1),radius = 1.2)\n    ax1.set_title (title1)\n\n    ax2 = fig.add_axes([.5, .0, .5, .5], aspect=1)\n    ax2.pie(sizes2, labels=labels, explode=(0.1, 0.1),radius = 1.2)\n    ax2.set_title (title2)\n    plt.show()\n    return\n\ncmp_apps_with_pop_g (df_eda_ge_group_T.index,df_eda_ge_group_T['NO'], 'APPLICATIONS FOR NON-DEGREE LEVEL JOBS\\n',df_eda_ge_group_T['YES'], 'APPLICATIONS FOR DEGREE LEVEL JOBS\\n')\n","7b8a0540":"df_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Black':'Filipino']\n\ndf_eda_ge_group_T.head(30)","9cc5af91":"# #LA population data\n# #using the sample application data titles and removing 2 or races aswe don't have application data for this category\n# total = 48.7 + 28.4 + 11.7 + 8.9 +0.7 + .2\n# Black = 8.9\/total\n# Hispanic = 48.7\/total\n# Asian = 11.7\/total\n# Caucasian = 28.4\/total\n# American_IndianAlaskan_Native = .7\/total\n# Filipino = .2\/total\n\n# labels=['Black','Hispanic','Asian','Caucasian','American Indian\/Alaska Native','Filipino']\n# sizes=[Black,Hispanic,Asian,Caucasian,American_IndianAlaskan_Native,Filipino]\n# plt.figure(figsize=(5,7))\n# plt.pie(sizes,explode=(0.1, 0.1,0.1,0.1,0.1,0.1),labels=labels)\n# plt.gca().axis('equal')\n# plt.title('LA ETHNICITY' )\n# plt.show()\n","812a7002":"def cmp_apps_with_pop (labels2,sizes2, title):\n\n    labels1=['Black','Hispanic','Asian','Caucasian','American Indian\/Alaska Native','Filipino']\n    sizes1=[Black,Hispanic,Asian,Caucasian,American_IndianAlaskan_Native,Filipino]\n    fig = plt.figure()\n    ax1 = fig.add_axes([0, 0, .5, .5], aspect=1)\n    ax1.pie(sizes1, explode=(0.1, 0.1, 0.1,0.1,0.1,0.1), labels=labels1, radius = 1.2)\n    ax1.set_title ('LA ETHNICITY\\n')\n\n    ax2 = fig.add_axes([.5, .0, .5, .5], aspect=1)\n    ax2.pie(sizes2, labels=labels2, explode=(0.1, 0.1, 0.1,0.1, 0.1,0.1),radius = 1.2)\n    ax2.set_title (title)\n    plt.show()\n    return\n\ncmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['YES'], 'APPLICATIONS FOR DEGREE LEVEL JOBS\\n')\n","44f5dd41":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['NO'], 'APPLICATIONS FOR NON DEGREE LEVEL JOBS\\n')\n","0caff877":"df_eda_ge_group = df_eda_ge.groupby('SCHOOL_TYPE').sum()\ndf_eda_ge_group.head(20)","4ec0f3d0":"df_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Female':'Male']\ndf_eda_ge_group_T.head(30)","4c2b9ed7":"cmp_apps_with_pop_g (df_eda_ge_group_T.index,df_eda_ge_group_T['APPRENTICESHIP'], 'APPLICATIONS FOR APPRENTICESHIP LEVEL JOBS\\n',df_eda_ge_group_T['COLLEGE OR UNIVERSITY'], 'APPLICATIONS FOR COLLEGE OR UNIVERSITY LEVEL JOBS\\n')\n","36e99a02":"df_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Black':'Filipino']\ndf_eda_ge_group_T.head(30)","8bb6b870":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['APPRENTICESHIP'], 'APPLICATIONS FOR APPRENTICESHIP LEVEL JOBS\\n')\n","f49d88ad":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['COLLEGE OR UNIVERSITY'], 'APPLICATIONS FOR COLLEGE OR UNIVERSITY LEVEL JOBS\\n')\n","291ee2e3":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['CERTIFICATION'], 'APPLICATIONS FOR CERTIFICATION LEVEL JOBS\\n')\n","b629e526":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['NONE'], 'APPLICATIONS FOR CERTIFICATION LEVEL JOBS\\n')\n","43c294ab":"df_eda_ge_group = df_eda_ge.groupby('DRIV_LIC_TYPE').sum()\ndf_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Female':'Male']\ndf_eda_ge_group_T.head(30)","91533ee9":"labels=['Female','Male']\nsizes =[2+1+337+96+33,461+84+4370+3437+250]\nplt.figure(figsize=(5,7))\nplt.pie(sizes,explode=(0.1, 0.1),labels=labels)\nplt.gca().axis('equal')\nplt.title('Special Driver Licence Required' )\nplt.show()","3ec3feeb":"bins = pd.cut(df_eda_ge['entry_salary'], [-1, 50000, 75000, 100000, 150000, 200000, 250000])\ndf_eda_ge_group = df_eda_ge.groupby(bins).sum()\ndf_eda_ge_group.head()","d5b8e2e5":"df_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Female':'Male']\ndf_eda_ge_group_T.head(30)","c3091785":"df_eda_ge_group_T = df_eda_ge_group_T.rename(columns={ df_eda_ge_group_T.columns[0]: \"Up to $50k\",df_eda_ge_group_T.columns[1]: \"50k-$75k\",df_eda_ge_group_T.columns[4]: \"Over $150k\"})\ndf_eda_ge_group_T.head()","c135cadd":"cmp_apps_with_pop_g (df_eda_ge_group_T.index,df_eda_ge_group_T['Up to $50k'], 'APPLICATIONS FOR UP TO $50k JOBS\\n',df_eda_ge_group_T['50k-$75k'],'APPLICATIONS FOR $50k-$75k+ LEVEL JOBS\\n')\n","3c0d6a7a":"cmp_apps_with_pop_g (df_eda_ge_group_T.index,df_eda_ge_group_T['Up to $50k'], 'APPLICATIONS FOR UP TO $50k JOBS\\n',df_eda_ge_group_T['Over $150k'],'APPLICATIONS FOR $150k+ LEVEL JOBS\\n')\n","8b55ebb4":"df_eda_ge_group_T = df_eda_ge_group.T\ndf_eda_ge_group_T = df_eda_ge_group_T['Black':'Filipino']\ndf_eda_ge_group_T = df_eda_ge_group_T.rename(columns={ df_eda_ge_group_T.columns[0]: \"Up to $50k\",\\\n                                                      df_eda_ge_group_T.columns[1]: \"50k-$75k\",\\\n                                                      df_eda_ge_group_T.columns[2]: \"75k-$100k\",\\\n                                                      df_eda_ge_group_T.columns[3]: \"100k-$150k\",\\\n                                                      df_eda_ge_group_T.columns[4]: \"Over $150k\"})\n\ndf_eda_ge_group_T.head(30)","b9cc5258":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['Up to $50k'], 'APPLICATIONS FOR UP TO $50k JOBS\\n')\n","b0c381b7":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['50k-$75k'], 'APPLICATIONS FOR $50-75k JOBS\\n')\n","81a40095":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['75k-$100k'], 'APPLICATIONS FOR $75-100k JOBS\\n')\n","0970288d":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['100k-$150k'], 'APPLICATIONS FOR $100-150k JOBS\\n')\n","4d90a3f1":"cmp_apps_with_pop (df_eda_ge_group_T.index,df_eda_ge_group_T['Over $150k'], 'APPLICATIONS FOR UP TO $150+k JOBS\\n')\n","00ac9798":"df_job_class.head()","e024a445":"df_explicit = pd.DataFrame()\ndata_list = []\n\ndf_job_class_len = len(df_job_class)\nindex = 0\nwhile index < df_job_class_len:\n    job = df_job_class.iloc[index]['JOB_CLASS_TITLE']\n    reqs = df_job_class.iloc[index]['EXP_JOB_CLASS_TITLE']\n    exp_len = df_job_class.iloc[index]['EXPERIENCE_LENGTH']\n    license = df_job_class.iloc[index]['DRIVERS_LICENSE_REQ']\n    license_type = df_job_class.iloc[index]['DRIV_LIC_TYPE']\n    ed_yr = df_job_class.iloc[index]['EDUCATION_YEARS']\n    school_type = df_job_class.iloc[index]['SCHOOL_TYPE']\n    course_length = df_job_class.iloc[index]['COURSE_LENGTH']\n    #print ('job',job) \n    \n    #print ('req',reqs)\n    if not pd.isna(reqs):\n        for i, word in enumerate(reqs.split(',')): \n            word = word.rstrip(' ').lstrip(' ')\n            #print ('req words',word)\n            data_list_len = len(data_list)\n            copy_found  = False\n            j = 0\n            while copy_found == False and j < data_list_len:\n                list_job = data_list[j][0].upper()\n                list_word = data_list[j][1].upper()\n                if list_job == job and list_word == word:\n                    print ('list_job',list_job)\n                    copy_found = True\n                j += 1\n            if word != '' and copy_found == False:\n                data_list.append([job.upper(), word,exp_len,license,license_type,ed_yr,school_type,course_length])\n    index += 1\ndf_explicit = pd.DataFrame(data_list)\ndf_explicit.columns = [\"JOB\", \"REQUIREMENT\", \"EXPERIENCE_LENGTH\",\"DRIVERS_LICENSE_REQ\",\"DRIV_LIC_TYPE\",\"EDUCATION_YEARS\",\"SCHOOL_TYPE\",\"COURSE_LENGTH\"]\ndf_explicit.head()\n","455c1a4b":"include =['object', 'float', 'int'] \ndf_eda_exam.describe( include = include)","00a39dbd":"with pd.option_context(\"display.max_rows\", 2000): display (df_explicit)\n","d54d3d6d":"include =['object', 'float', 'int'] \ndf_explicit.describe( include = include)","24913fdd":"df_explicit_g = df_explicit.groupby('JOB').count()\n\ndf_explicit_s_g = df_explicit_g.sort_values('REQUIREMENT',ascending = False)\ndf_explicit_s_g.head()","62fdfc8d":"\nG = nx.Graph()\n\njob = 'WATER UTILITY SUPERINTENDENT'\n#G.add_node(job)\ndf_explicit_len = len(df_explicit)\nindex = 0\nedges={}\n\nwhile index < df_explicit_len:\n    if df_explicit.iloc[index]['JOB'] == job:\n        #print (job, \":   \",df_explicit.iloc[index]['REQUIREMENT'] )\n        G.add_edge(job,df_explicit.iloc[index]['REQUIREMENT'])\n        edges[job,df_explicit.iloc[index]['REQUIREMENT']] = '2yr'\n        \n    index +=1\nplt.figure(figsize=(15, 15)) \nplt.axis('off')\npos = nx.circular_layout(G)\n\nnx.draw_networkx_edge_labels(G,pos,edge_labels=edges\n,font_color='red')\n\nnx.draw_networkx(G, pos,with_labels=True, node_color='red', font_size=12, node_size=20000, arrows = True, width = 2)\nplt.show()\n#print (edges)\n","34342fcb":"\ndef findsubsrecurse (job, edges, depth ):\n    G.add_node(job)\n    df_explicit_len = len(df_explicit)\n    index = 0\n    #print ('depth', depth)\n    \n    while index < df_explicit_len:\n        #print ('index',index)\n        \n        if  (df_explicit.iloc[index]['JOB'] == job or\\\n             df_explicit.iloc[index]['JOB'] == job +' I' or\\\n             df_explicit.iloc[index]['JOB'] == job + \" II\" or\\\n             df_explicit.iloc[index]['JOB'] == job + \" III\") and depth<10:\n            #print ('link found')\n            #print ('job,req', job,df_explicit.iloc[index]['REQUIREMENT'] )\n            G.add_edge(df_explicit.iloc[index]['REQUIREMENT'],job)\n            edges[df_explicit.iloc[index]['REQUIREMENT'],job] = \\\n            str(df_explicit.iloc[index]['EXPERIENCE_LENGTH'])+'yr'\n            depth += 1\n            #print ('depth in loop1',depth)\n            edges, depth  = findsubsrecurse (df_explicit.iloc[index]['REQUIREMENT'], edges, depth )\n            \n        index +=1\n    return edges, depth\n\ndef findsubs (job):\n    edges={}\n    job= job.upper()\n    depth = 0\n    edges, depth  = findsubsrecurse (job,edges, depth )\n    plt.figure(figsize=(15, 15)) \n    plt.axis('off')\n    pos = nx.circular_layout(G)\n    #pos = nx.spectral_layout(G)\n    pos[job] = np.array([0, 0])\n    nx.draw_networkx_edge_labels(G,pos,edge_labels=edges,font_color='red')\n\n    nx.draw_networkx(G, pos,with_labels=True, node_color='red', \n                     font_size=12, node_size=2000, arrows = True, width = 2)\n    plt.show()\n\n    \n    #print (pos)\n    return","d5a3dbd8":"G = nx.DiGraph()\nfindsubs ('SENIOR SYSTEMS ANALYST')","0d1d5fca":"G = nx.DiGraph()\nfindsubs ('WATER UTILITY SUPERINTENDENT')","dce53a5f":"G = nx.DiGraph()\nfindsubs ('CHIEF INSPECTOR')","c4b20e4e":"G = nx.DiGraph()\nfindsubs ('ELECTRICAL SERVICES MANAGER')\n","9e36cd23":"G = nx.DiGraph()\nfindsubs ('UTILITY SERVICES MANAGER')","d1b61743":"G = nx.DiGraph()\n# no subs findsubs ('Applications Programmer') education plus paid experience performing systems or programming tasks in a professional IT environment\n# no subs findsubs ('Accountant') graduation required  not work experience required\n# no subs findsubs ('Accounting Clerk') but paid clerical acconting work is required\n# no subs findsubs ('Assistant Street Lighting Electrician') experience working in the construction, maintenance, and repair of street lighting circuitry\n# findsubs ('Building Mechanical Inspector')  #ASSISTANT INSPECTOR\n#findsubs ('Detention Officer') #PARK RANGER\n# no subs findsubs (' Equipment Mechanic')\n# no subs findsubs (' Field Engineering Aide')\n# no subs findsubs ('Housing Inspector')#ASSISTANT INSPECTOR\n# no subs findsubs ('Housing Investigator')\n# no subs findsubs ('Librarian')\n#findsubs ('Security Officer')  #PARK RANGER\n# no subs findsubs ('Senior Administrative Clerk')\n# no subs findsubs ('Senior Custodian')\nfindsubs ('Senior Equipment Mechanic') #HEAVY DUTY EQUIPMENT MECHANIC Auto Electrician\n#findsubs ('Tree Surgeon') # Tree Surgeon assistant\n\n","79bf023a":"\ndef findsuprecurse (experience, edges):\n    G.add_node(experience)\n    df_explicit_len = len(df_explicit)\n    index = 0\n    while index < df_explicit_len:\n        if  (df_explicit.iloc[index]['REQUIREMENT'] == experience or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == experience +' I' or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == experience + \" II\" or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == experience + \" III\"):\n             \n            G.add_edge(experience,df_explicit.iloc[index]['JOB'])\n            edges[experience, df_explicit.iloc[index]['JOB'] ]= \\\n                 str(df_explicit.iloc[index]['EXPERIENCE_LENGTH'])+'yr'\n            findsuprecurse (df_explicit.iloc[index]['JOB'], edges)\n        index +=1\n    return edges\ndef findsup (job):\n    edges={}\n   \n    edges = findsuprecurse (job, edges)\n    plt.figure(figsize=(15, 15)) \n    plt.axis('off')\n    pos = nx.circular_layout(G)\n    pos[job] = np.array([0, 0])\n    \n    nx.draw_networkx_edge_labels(G,pos,edge_labels=edges,font_color='red')\n\n    nx.draw_networkx(G, pos,with_labels=True, node_color='red', \n                     font_size=12, node_size=2000, arrows = True, width = 2)\n    plt.show()\n    #print (edges)\n    return\n","1a9f31d8":"G = nx.DiGraph()\nfindsup ('SYSTEMS ANALYST')","d7476108":"G = nx.DiGraph()\nfindsup('PUBLIC RELATIONS SPECIALIST')","d40650a8":"G = nx.DiGraph()\nfindsup('WELDER')","a7d2d3bf":"G = nx.DiGraph()\nfindsup('ELECTRICAL CRAFT HELPER')","7d81bc4e":"\nG = nx.DiGraph()\nfindsup('ACCOUNTANT')","f1bea6a5":"G = nx.DiGraph()\nfindsup('AUTOMOTIVE SUPERVISOR')","13e138ff":"# dot.edge_attr.update(arrowhead='vee', arrowsize='2', dir ='back')\n\n# for index, row in df_explicit.iterrows():\n#     dot.edge(str(row[\"JOB\"]), \"AND\"+str(index))\n#     dot.edge(\"AND\"+str(index), str(row[\"REQUIREMENT\"]), label=str (row['EXPERIENCE_LENGTH']+'yr'))\n#     if row['DRIVERS_LICENSE_REQ'] != '':\n#             dot.edge( \"AND\"+str(index), \"DRIVER LICENCE\", label = str(row[\"DRIVERS_LICENSE_REQ\"]))\n\n    \n# dot","fbc1fcdf":"from graphviz import Digraph\ndot = Digraph(name='Promotion Options')\n\ndef find_promotions(job):\n    dot.edge_attr.update(arrowhead='vee', arrowsize='2', dir ='back')\n\n    df_explicit_len = len(df_explicit)\n    index = 0\n    andstr = ''\n    licstr = ''\n    while index < df_explicit_len:\n        if  (df_explicit.iloc[index]['REQUIREMENT'] == job or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == job +' I' or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == job + \" II\" or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == job + \" III\"):\n\n            if not pd.isna(df_explicit.iloc[index]['SCHOOL_TYPE']):\n                \n                if not pd.isna(df_explicit.iloc[index][\"EDUCATION_YEARS\"]):\n                    #print ('df_explicit.iloc[index][\"EDUCATION_YEARS\"]',df_explicit.iloc[index][\"EDUCATION_YEARS\"])\n                    dot.edge( \"AND\"+andstr, df_explicit.iloc[index][\"SCHOOL_TYPE\"].title(), label = str(df_explicit.iloc[index][\"EDUCATION_YEARS\"])+ 'yr')\n                elif not pd.isna(df_explicit.iloc[index][\"COURSE_LENGTH\"]):\n                    #print ('df_explicit.iloc[index][\"COURSE_LENGTH\"]',df_explicit.iloc[index][\"COURSE_LENGTH\"])\n                    dot.edge( \"AND\"+andstr, df_explicit.iloc[index][\"SCHOOL_TYPE\"].title(), label = df_explicit.iloc[index][\"COURSE_LENGTH\"].title())\n            \n            dot.edge(df_explicit.iloc[index][\"JOB\"].title(), \"AND\"+andstr)\n            dot.edge(\"AND\"+andstr, df_explicit.iloc[index][\"REQUIREMENT\"].title())   #, label=str (df_explicit.iloc[index]['EXPERIENCE_LENGTH']+'yr'))\n            if not pd.isna(df_explicit.iloc[index]['DRIVERS_LICENSE_REQ']):\n                if not pd.isna(df_explicit.iloc[index][\"DRIV_LIC_TYPE\"]):\n                    dot.edge( \"AND\"+andstr, \"Driver License\", label = df_explicit.iloc[index][\"DRIVERS_LICENSE_REQ\"].title() )\n                else:\n                    dot.edge( \"AND\"+andstr, \"Driver License\", label = df_explicit.iloc[index][\"DRIVERS_LICENSE_REQ\"].title() + \", \" +df_explicit.iloc[index][\"DRIV_LIC_TYPE\"].title())\n            andstr= andstr + \" \"\n            licstr= licstr + \" \"\n        index +=1\n    \n    return","e0b7d229":"dot = Digraph()\nfind_promotions('ASSISTANT SIGNAL SYSTEMS ELECTRICIAN')\ndot","c3489b8a":"dot = Digraph()\nfind_promotions('ENVIRONMENTAL SUPERVISOR')\ndot","c10a9cf9":"dot = Digraph()\nfind_promotions('AUDITOR')\ndot","6d8c6523":"dot = Digraph()\nfind_promotions('WELDER')\ndot","61b2d2fc":"dot = Digraph()\nfind_promotions('ACCOUNTANT')\ndot","3c6f79ee":"dot = Digraph()\nfind_promotions('AUTOMOTIVE SUPERVISOR')\ndot","2ff4a2c9":"from graphviz import Digraph\n#dot = Digraph(name='Promotion Options')\n\ndef find_promotions_min(job):\n    dot.edge_attr.update(arrowhead='vee', arrowsize='2', dir ='back')\n\n    df_explicit_len = len(df_explicit)\n    index = 0\n    andstr = ''\n    licstr = ''\n    while index < df_explicit_len:\n        if  (df_explicit.iloc[index]['REQUIREMENT'] == job or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == job +' I' or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == job + \" II\" or\\\n             df_explicit.iloc[index]['REQUIREMENT'] == job + \" III\"):\n            label=str (df_explicit.iloc[index]['EXPERIENCE_LENGTH'])+'yr'\n#             print ('index, label',index,label)\n#             print (\"df_explicit.iloc[index]['REQUIREMENT']\",df_explicit.iloc[index]['REQUIREMENT'])\n#             print (\"df_explicit.iloc[index]['JOB']\",df_explicit.iloc[index]['JOB'])\n            dot.edge(df_explicit.iloc[index][\"JOB\"].title(), df_explicit.iloc[index][\"REQUIREMENT\"].title(), \\\n                     label=str (df_explicit.iloc[index]['EXPERIENCE_LENGTH'])+'yr')\n        index +=1\n    return","0801cbec":"dot = Digraph()\n \nindex = 0\ndf_eda_exam\nwhile index < len(df_eda_exam):\n#    print ('main index',index)\n    find_promotions_min(df_eda_exam.iloc[index]['JOB_CLASS_TITLE'].upper())\n    \n    print ('.',end=\"\")\n    index +=1\ndot","d69ebce1":"#The outout looks much more interesting on my Kernel where the x axis is unlimited.\n#Here is the dot file...\n\nprint (dot)","7cb3c5cb":"from graphviz import Digraph\ndot = Digraph(name='Promotion Options')\ndot.edge_attr.update(arrowhead='vee', arrowsize='2', dir ='back')\n    \ndef find_promotions_minrecurse(job, depth):    \n    dot.edge_attr.update(arrowhead='vee', arrowsize='2', dir ='back')\n    df_explicit_len = len(df_explicit)\n    #print (df_explicit_len)\n    index = 0\n    while index < df_explicit_len:\n        #print ('job',job)\n        if  (df_explicit.iloc[index]['JOB'] == job or\\\n             df_explicit.iloc[index]['JOB'] == job +' I' or\\\n             df_explicit.iloc[index]['JOB'] == job + \" II\" or\\\n             df_explicit.iloc[index]['JOB'] == job + \" III\") and depth<40:           \n            req_pattern ='(.*)( I)'\n            next_job = df_explicit.iloc[index]['REQUIREMENT']\n            #print ('next_job',next_job)\n            if re.search (req_pattern,next_job ):\n                next_job = re.search (req_pattern,next_job ).group(1)\n\n            dot.edge(df_explicit.iloc[index][\"JOB\"].title(), \\\n                     next_job.title(), \\\n                     label=str (df_explicit.iloc[index]['EXPERIENCE_LENGTH'])+'yr')\n            depth += 1\n            #print ('depth in loop1',depth)\n           \n            depth  = find_promotions_minrecurse (next_job,  depth )\n    \n        index +=1\n    return depth\n\n\n\ndef findsubords (job):\n    \n    depth = 0\n    depth  = find_promotions_minrecurse (job,depth )\n    return","832625cb":"dot = Digraph(strict = True)\n\n\nfindsubords ('ENGINEER OF SURVEYS')\ndot","452e71e7":"dot = Digraph(strict = True)\n\nfindsubords ('DIRECTOR OF AIRPORTS ADMINISTRATION')\ndot","36a65a23":"dot = Digraph()\n\nfindsubords ('ELECTRICAL SERVICES MANAGER')\ndot","310c9f30":"dot = Digraph(strict = True)\n\nfindsubords ('UTILITY SERVICES MANAGER')\ndot","55438643":"from textblob import TextBlob\ncnt = 0\nsent =  []\n\nfor filename in os.listdir(bulletin_dir):\n#     if cnt >-1 and cnt <20:\n        neg_sent = 0\n        pos_sent = 0\n        neut_sent = 0\n        cmp_sent = 0\n        total = 0\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            row[col_heads['FILE_NAME']] = filename\n            #print (filename)\n            for line in f.read().split('\\n'):\n               \n                analysis = TextBlob(line)\n                if filename == 'HEAVY DUTY EQUIPMENT MECHANIC 3743 021717.txt' and analysis.sentiment.polarity < -0.15:\n                    print ('analysis',analysis.sentiment.polarity)\n                    print ('line', line)\n\n                cmp_sent += analysis.sentiment.polarity\n#Initial tests shows the subjectivity measure is not useful for this application\n#                 if analysis.sentiment.subjectivity >0:\n#                     print(\"subjectivity\",analysis.sentiment.subjectivity)\n#                     print (line)\n                if analysis.sentiment.polarity < 0:\n                    neg_sent+= 1\n                    total +=1\n                    #print ('analysis.sentiment: polarity, subjectivity',analysis.sentiment.polarity,analysis.sentiment.subjectivity)\n                    #print ('analysis.sentiment_assessments',analysis.sentiment_assessments)\n\n                    #print ('neg line', line)\n                if analysis.sentiment.polarity > 0:\n                    pos_sent+= 1\n                    total +=1\n                    #print ('analysis.sentiment: polarity, subjectivity',analysis.sentiment.polarity,analysis.sentiment.subjectivity)\n                    #print ('analysis.sentiment_assessments',analysis.sentiment_assessments)\n                if analysis.sentiment.polarity == 0:\n                    neut_sent+= 1\n                   \n                #         print ('neg_sent', neg_sent)\n#         print ('pos_sent', pos_sent)\n#         print ('neut_sent', neut_sent)\n#         print ('cmp_sent', cmp_sent\/total)\n        sent.append([filename,cmp_sent\/total,neg_sent,pos_sent])\n        cnt +=1\n#         print ('sent list',sent)\n#print ('sent list',sent)\n        \ndf_sent = pd.DataFrame(sent)\n\n\n","2017b7d7":"\ndf_sent.columns = [\"FILENAME\", \"SENTIMENT\", \"NEG_SENTIMENT\", \"POS_SENTIMENT\"]\ndf_sent = df_sent.sort_values('SENTIMENT')\ndf_sent.head()","2e041cd7":"df_sent = df_sent.sort_values('SENTIMENT', ascending  = False)\ndf_sent.head()","cd17c3a2":"df_sent.describe()","054eacba":"feminine_coded_words = [\"agree\", \"affectionate\", \"child\", \"cheer\", \"collab\", \"commit\", \"communal\",   \"compassion\", \"connect\", \"considerate\", \"cooperat\", \"co-operat\", \"depend\",   \"emotiona\", \"empath\", \"feel\", \"flatterable\", \"gentle\", \"honest\", \"interpersonal\",   \"interdependen\", \"interpersona\", \"inter-personal\", \"inter-dependen\", \"interpersona\",   \"kind\", \"kinship\", \"loyal\", \"modesty\", \"nag\", \"nurtur\", \"pleasant\", \"polite\",   \"quiet\", \"respon\", \"sensitiv\", \"submissive\", \"support\", \"sympath\", \"tender\",   \"together\", \"trust\", \"understand\", \"warm\", \"whin\", \"enthusias\", \"inclusive\",   \"yield\", \"shar\"]\n\nmasculine_coded_words = [ \"active\", \"adventurous\", \"aggress\", \"ambitio\", \"analy\",   \"assert\", \"athlet\", \"autonom\", \"battle\", \"boast\", \"challeng\", \"champion\",   \"compet\", \"confident\", \"courag\", \"decid\", \"decision\", \"decisive\", \"defend\",   \"determin\", \"domina\", \"dominant\", \"driven\", \"fearless\", \"fight\", \"force\",   \"greedy\", \"head-strong\", \"headstrong\", \"hierarch\", \"hostil\", \"implusive\",   \"independen\", \"individual\", \"intellect\", \"lead\", \"logic\", \"objective\", \"opinion\",   \"outspoken\", \"persist\", \"principle\", \"reckless\", \"self-confiden\", \"self-relian\", \"selfsufficien\", \"selfconfiden\", \"selfrelian\", \"selfsufficien\", \"stubborn\", \"superior\",\"unreasonab\"]\n","5eb45eaa":"def assess(ad_text):\n    #print (ad_text)\n    ad_text = ''.join([i if ord(i) < 128 else ' ' for i in ad_text])\n    ad_text = re.sub(\"[\\\\s]\", \" \", ad_text, 0, 0)\n    ad_text = re.sub(\"[\\.\\t\\,\\:;\\(\\)\\.]\", \"\", ad_text, 0, 0).split(\" \")\n    ad_text = [ad for ad in ad_text if ad != \"\"]\n    \n    feminine_coded_words_fnd = ''\n    feminine_coded_words_cnt = 0\n    for adword in ad_text:\n        for word in feminine_coded_words:\n            if adword.startswith(word):\n                feminine_coded_words_cnt +=1\n                if word not in feminine_coded_words_fnd:\n                    feminine_coded_words_fnd += \" \" + word\n\n    masculine_coded_words_fnd = ''\n    masculine_coded_words_cnt = 0\n    for adword in ad_text:\n        for word in masculine_coded_words:\n            if adword.startswith(word):\n                masculine_coded_words_cnt += 1\n                if word not in masculine_coded_words_fnd:\n                     masculine_coded_words_fnd += \" \" + word\n                \n    #print ('feminine_coded_words_fnd', feminine_coded_words_fnd)\n    #print ('masculine_coded_words_fnd', masculine_coded_words_fnd)\n\n    return feminine_coded_words_fnd, masculine_coded_words_fnd,feminine_coded_words_cnt,masculine_coded_words_cnt","9a5cad0d":"cnt = 0\nword_code =  []\n\nfor filename in os.listdir(bulletin_dir):\n#    if cnt >-1 and cnt <20:\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            text = f.read()\n            f,m, f_cnt, m_cnt = assess (text)\n            g_c = 'not det'\n            if f_cnt and not m_cnt:\n                g_c = \"strongly feminine\"\n            elif m_cnt and not f_cnt:\n                g_c = \"strongly masculine\"\n            elif (not m_cnt and not f_cnt) or (m_cnt == f_cnt):\n                g_c = \"neutral\"\n            elif f_cnt > m_cnt:\n                g_c = \"feminine\"\n            elif m_cnt > f_cnt:\n                g_c = \"masculine\"\n            word_code.append([filename,f,m, f_cnt, m_cnt, g_c])\n            cnt +=  1\n#print  (word_code)            \ndf_word_code = pd.DataFrame(word_code)\ndf_word_code.columns = [\"FILENAME\", \"F_WORDS\", \"M_WORDS\", \"F_CNT\", \"M_CNT\",\"GENDER_CODE\"]\ndf_word_code = df_word_code.sort_values('GENDER_CODE')\n\n\n","95bc2e90":"df_word_code.head(10)","b459943f":"df_word_code = df_word_code.sort_values('GENDER_CODE', ascending  = False)\ndf_word_code.head(10)\n","1fbba3ed":"df_word_code.describe()","19fb4ada":"df_word_code_group = df_word_code.groupby('GENDER_CODE').count()\ndf_word_code_group.head()","167c66c8":"# from nltk.tokenize import sent_tokenize, word_tokenize\n# not_punctuation = lambda w: not (len(w)==1 and (not w.isalpha()))\n# get_word_count = lambda text: len(list(filter(not_punctuation, word_tokenize(text))))\n# get_sent_count = lambda text: len(sent_tokenize(text))\n\n# from nltk.corpus import cmudict\n# prondict = cmudict.dict()\n# prondict[\"apple\"]\n\n","d0fc3727":"\n# numsyllables_pronlist = lambda w: len(list(filter(lambda s: s[-1].isdigit(), w)))\n# def numsyllables(word):\n#     try:\n#         return list(set(map(numsyllables_pronlist, prondict[word.lower()])))\n#     except KeyError:\n#         return [0]\n\n","808b5d72":"# def numsyllables(word):\n#     try:\n#         x = prondict[word.lower()]\n#         syll = 0\n#         for y in x[0]:\n#             if y[-1].isdigit():\n#                 syll += 1\n#         return  syll\n#     except KeyError:\n#         return 0\n    ","2ccc7095":"# def text_statistics(text):\n#     word_count = get_word_count(text)\n#     sent_count = get_sent_count(text)\n#     print('word_count',word_count)\n#     #w is argument to the function\n#     syllable_count = sum(map(lambda w: max(numsyllables(w)), word_tokenize(text)))\n#     print ('syllable_count',syllable_count)\n#     return word_count, sent_count, syllable_count\n","847ebe30":"# flesch_formula = lambda word_count, sent_count, syllable_count : 206.835 - 1.015*word_count\/sent_count - 84.6*syllable_count\/word_count\n# def flesch(text):\n#     word_count, sent_count, syllable_count,not_found = text_statistics(text)\n# #     print ('word_count ',word_count, 'sent_count ', sent_count, 'syllable_count ', syllable_count)\n# #     print ('words_not_found',not_found)\n#     return flesch_formula(word_count, sent_count, syllable_count)\n \n# fk_formula = lambda word_count, sent_count, syllable_count : 0.39 * word_count \/ sent_count + 11.8 * syllable_count \/ word_count - 15.59\n# def flesch_kincaid(text):\n#     word_count, sent_count, syllable_count,not_found = text_statistics(text)\n#     return fk_formula(word_count, sent_count, syllable_count)\n","753a726e":"# cnt = 0\n# for filename in os.listdir(bulletin_dir):\n#     if cnt >-1 and cnt <2:\n#         with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n#             text = f.read()\n#             text = text.replace('. . ', '' )\n#             print (filename)\n#             print ('flesch_reading_ease',flesch(text))\n#             print ('flesch_reading_grade',flesch_kincaid(text))\n#             #print (text)\n#             print ()\n#         cnt +=1\n ","5c6e7fe7":"# from nltk.corpus import cmudict\n# word = 'president'\n# d = cmudict.dict()\n# for x in d[word.lower()]:\n#     for y in x:\n#         if y[-1].isdigit():\n#             print (y)\n#     print (x)","d5fd98e5":"# from nltk.corpus import cmudict\n# d = cmudict.dict()\n# def nsyl(word):\n#     a = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]] \n# #     b = list(y for y in x if y[-1].isdigit()) for x in d[word.lower()]\n# #     a = [len b]\n#     return a\n# print (nsyl('president'))","9ad2665b":"youandme = []\ncnt = 0\nfor filename in os.listdir(bulletin_dir):\n    \n    if cnt >203 and cnt <205:\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            text = f.read()\n            text = text.lower()\n            print (filename)\n            print ('You instances')\n            you_list = re.findall ('( |^)(you )(.*?)(,|\\.|;)' , text)\n            print (len(you_list))\n            print (you_list)\n\n            print ('We instances')\n            we_list = re.findall ('( |^)(we )(.*?)(,|\\.|;)' , text)\n            print (len(we_list))\n            print (we_list)\n\n            \n            print ()\n\n    cnt +=1\n","0fc3a948":"youandme = []\ncnt = 0\nfor filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            text = f.read()\n            text = text.lower()\n            you_list = re.findall ('( |^)(you )(.*?)(,|\\.|;)' , text)\n            we_list = re.findall ('( |^)(we )(.*?)(,|\\.|;)' , text)\n#            we_list = re.findall ('(we )(.*?)(,|\\.|;)' , text)\n            youandme.append([filename,len(you_list),len(we_list)])\n\n#print (youandme)\ndf_youandme = pd.DataFrame(youandme)\ndf_youandme.columns = [\"FILENAME\", \"YOU_INSTANCE\", \"WE_INSTANCE\"]\ndf_youandme = df_youandme.sort_values('YOU_INSTANCE',ascending  =False)\ndf_youandme.head()","c4cebc2e":"df_youandme = df_youandme.sort_values('WE_INSTANCE',ascending  =False)\ndf_youandme.head()","49f849d1":"!pip install PyPDF2\n!pip install textstat\nfrom textstat.textstat import textstatistics, easy_word_set, legacy_round\n\n","1d67c841":"#commented out as commiting was crashing\n# !git clone https:\/\/github.com\/shivam5992\/textstat.git\n# !cd textstat\n# !pip install textstat","4c078597":"from textstat.textstat import textstatistics, easy_word_set, legacy_round\n\nimport textstat\n\ncnt = 0\nfor filename in os.listdir(bulletin_dir):\n    if cnt >-1 and cnt <3:\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            text = f.read()\n            print (filename)\n            print ('flesch_reading_ease',textstat.flesch_reading_ease(text))\n            print ('flesch_kincaid_grade',textstat.flesch_kincaid_grade(text))\n            print ('smog_index',textstat.smog_index(text))\n            print ('coleman_liau_index',textstat.coleman_liau_index(text))\n            print ('automated_readability_index',textstat.automated_readability_index(text))\n            print ('dale_chall_readability_score',textstat.dale_chall_readability_score(text))\n            print ('difficult_words',textstat.difficult_words(text))\n            print ('linsear_write_formula',textstat.linsear_write_formula(text))\n            print ('gunning_fog',textstat.gunning_fog(text))\n            print ('text_standard',textstat.text_standard(text))\n\n            #print (text)\n\n            print ()\n        cnt +=1\n ","b85d5dc6":"from nltk.tokenize import sent_tokenize, word_tokenize\nnot_punctuation = lambda w: not (len(w)==1 and (not w.isalpha()))\nget_word_count = lambda text: len(list(filter(not_punctuation, word_tokenize(text))))\nget_sent_count = lambda text: len(sent_tokenize(text))\n\nfrom nltk.corpus import cmudict\nprondict = cmudict.dict()\n\nnumsyllables_pronlist = lambda w: len(list(filter(lambda s: s[-1].isdigit(), w)))\ndef numsyllables(word):\n    try:\n        return list(set(map(numsyllables_pronlist, prondict[word.lower()])))\n    except KeyError:\n        return [0]\n\ndef text_statistics(text):\n    word_count = get_word_count(text)\n    sent_count = get_sent_count(text)\n    \n    syllable_count = 0\n    not_found = 0\n    for word in word_tokenize(text):\n        if word.isalpha():\n            syllable_count += numsyllables(word)[0]\n            if numsyllables(word)[0] == 0:\n                not_found += 1\n    syllable_count += not_found * syllable_count \/ word_count\n    return word_count, sent_count, syllable_count,not_found\n\nflesch_formula = lambda word_count, sent_count, syllable_count : 206.835 - 1.015*word_count\/sent_count - 84.6*syllable_count\/word_count\ndef flesch(text):\n    word_count, sent_count, syllable_count,not_found = text_statistics(text)\n#     print ('word_count ',word_count, 'sent_count ', sent_count, 'syllable_count ', syllable_count)\n#     print ('words_not_found',not_found)\n    return flesch_formula(word_count, sent_count, syllable_count)\n \nfk_formula = lambda word_count, sent_count, syllable_count : 0.39 * word_count \/ sent_count + 11.8 * syllable_count \/ word_count - 15.59\ndef flesch_kincaid(text):\n    word_count, sent_count, syllable_count,not_found = text_statistics(text)\n    return fk_formula(word_count, sent_count, syllable_count)\n","ce5a316d":"cnt = 0\nfor filename in os.listdir(bulletin_dir):\n    if cnt >-1 and cnt <2:\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            text = f.read()\n            text = text.replace('. . ', '' )\n            print (filename)\n            print ('flesch_reading_ease',flesch(text))\n            print ('flesch_reading_grade',flesch_kincaid(text))\n            #print (text)\n            print ()\n        cnt +=1\n ","59b4dafd":"readability = []\ncnt = 0\nfor filename in os.listdir(bulletin_dir):\n#    if cnt >-1 and cnt <2:\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            text = f.read()\n            text = text.replace('. . ', '' )\n#             print (filename)\n#             print ('flesch_reading_ease',flesch(text))\n#             print ('flesch_reading_grade',flesch_kincaid(text))\n#             #print (text)\n#             print ()\n            readability.append([filename,flesch(text),flesch_kincaid(text)])\n\n#        cnt +=1\n\n\n  \n\ndf_readability = pd.DataFrame(readability)\ndf_readability.columns = [\"FILENAME\", \" FLESCH_READING_EASE\", \"FLESCH_READING_GRADE\"]\ndf_readability = df_readability.sort_values('FLESCH_READING_GRADE')\n","934b72f2":"df_readability.head(10)","f63e591e":"df_readability = df_readability.sort_values('FLESCH_READING_GRADE',ascending = False )\ndf_readability.head(10)","c174f046":"cnt = 0\nfor filename in os.listdir(bulletin_dir):\n    #change this line to exam a different file\n    if cnt >-1 and cnt <2:\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            for index,line in enumerate(f.readlines()):\n                text = line.rstrip().lstrip()\n                if (text !=''):\n                    print (text)\n                    print ('flesch_reading_ease',flesch(text))\n                    print ('flesch_reading_grade',flesch_kincaid(text))\n\n                    print ()\n        cnt +=1\n \n\n","92f21fac":"df_duties = df_eda_exam.copy()\ndf_duties = df_duties.sort_values('JOB_DUTIES')\ndf_duties['JOB_DUTIES']= np.where(pd.isna(df_duties['JOB_DUTIES']), \n                            '.', \n                            df_duties['JOB_DUTIES'])\n\ndf_duties.head()","fb3581db":"#just trying req text instead\n\n\n\ndf_duties = df_eda_exam.copy()\n\ndf_duties['EXP_JOB_CLASS_TITLE']= np.where(pd.isna(df_duties['EXP_JOB_CLASS_TITLE']), \n                            '.', \n                            df_duties['EXP_JOB_CLASS_TITLE'])\ndf_duties['EXP_JOB_CLASS_ALT_RESP']= np.where(pd.isna(df_duties['EXP_JOB_CLASS_ALT_RESP']), \n                            '.', \n                            df_duties['EXP_JOB_CLASS_ALT_RESP'])\ndf_duties['EXP_JOB_CLASS_FUNCTION']= np.where(pd.isna(df_duties['EXP_JOB_CLASS_FUNCTION']), \n                            '.', \n                            df_duties['EXP_JOB_CLASS_FUNCTION'])\n\n#over write duties for quick test\ndf_duties['JOB_DUTIES'] = df_duties['EXP_JOB_CLASS_TITLE'] + \" \" + df_duties['EXP_JOB_CLASS_ALT_RESP'] + \" \"+  df_duties['EXP_JOB_CLASS_FUNCTION'] \n\ndf_duties.head(10)","b88a255b":"df_duties = df_duties[df_duties.JOB_DUTIES != '']\ndf_duties.describe()","92497a09":"start = time.time()\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ntotal_duties = [\"\".join(x) for x in (df_duties['JOB_DUTIES'])]\nvectorizer = TfidfVectorizer(tokenizer=normalize)\ntfidf = vectorizer.fit_transform(total_duties)\n\ncachedStopWords = stopwords.words(\"english\")\n#print(total_duties)\n\ntotal_duties_l  = [x.lower() for x in total_duties]\n#print(total_duties_l)\n\n# punct = '-'\n# remove_punctuation_map = dict((ord(char), ' ') for char in punct)    \nremove_punctuation_map = dict((ord(char), ' ') for char in string.punctuation)    \n\nall_words = [nltk.word_tokenize(x.translate(remove_punctuation_map)) for x in total_duties_l]\n\nfor i in range(len(all_words)):  \n    all_words[i] = [w for w in all_words[i] if w not in cachedStopWords]\n\n#print(all_words)\n\nend = time.time()\nprint('run time',end - start)\nprint (len(df_eda_exam))\nprint(len(total_duties_l))\n","5e90a8b7":"from gensim.models import FastText\nstart = time.time()\nembed_size = 300\n\"\"\"all_words is a list of all the duties with the words separated and cleaned\"\"\"\nft_model = FastText(all_words, size=embed_size, window=5, min_count=2, workers=1\n                    ,sg=1)\nprint('Time to build FastText model: {} mins'.format(round((time.time() - start) \/ 60, 2)))\n\n\nft_model.wv.most_similar(positive=[\"water\"])","8869b689":"\"\"\"Uncomment this to see how tdidfs are stored\"\"\"\n#print (tfidf)\n","97bbab7f":"\nstart = time.time()\n#tfidf is calculated above\n#There a tfidf value for every word in all_words\nrows, cols = tfidf.nonzero()\nprint (rows)\nprint (cols)\nrows_l = len(rows)\n\ns_embed = []\ns_embeds = []\ndividend = []\natStart = True\noldr = -1\nw_cnt = 0\n\"\"\"using vectorization calculated in the Word2Vec section\"\"\"\nvocab = vectorizer.get_feature_names()\n#print (vocab)\n#this method of calculating the embeddings is a bit ugly but takes advantage of how tfidfs are stored\n#for every question\nfor i in range (rows_l):\n    r = rows[i]\n    c = cols[i]\n    if (oldr != r):\n        #new Duty and so store last embeddings\n        if (atStart == False):\n            #calc embedding for last Duty\n            s_embed = np.divide(dividend, divisor)\n            s_embeds.append(s_embed.flatten())\n            \n        else: \n            atStart = False\n        oldr = r\n        w_cnt = 0\n        dividend = np.zeros((1, embed_size))\n        divisor = 0\n\n       \n    #find the next word\n    word = vocab[c]\n    if word in ft_model.wv.vocab:\n        #word is in the vocab and so calculate its contribution to the question vector\n        wt = tfidf[r,c]\n        #print (wt, word)\n        w_embed = ft_model.wv[word]\n        #print(w_embed)\n        #print(w_embed * wt)\n        dividend = np.add(dividend, w_embed * wt)\n        divisor += wt\n        w_cnt +=1\n#    else:\n#        print (word, \" not in vocab\")\ns_embed = np.divide(dividend, divisor)\ns_embeds.append(s_embed.flatten())\n#print (s_embeds)\nend = time.time()\nprint('Sentence embedding run time',end - start)\nstart = time.time()\n\nq_embed_array = cosine_similarity(s_embeds, s_embeds)\nprint('q_embed_array',q_embed_array)\nprint ('q_embed_arrayshape', len(q_embed_array))\nend = time.time()\nprint('cosine sim time',end - start)","54b836ae":"\"\"\"function to produce dataframe of results of similarity tests\"\"\"\ndef get_sim_duties (column_head,index,sim_array,duties,duty):\n    h_threshold =0.94\n    l_threshold =0.9\n\n    col_h = column_head + str(index)\n    \n    df_sim_d = pd.DataFrame({'Cosine':sim_array[:,index], col_h:duties['JOB_DUTIES']})\n\n    df_sim_d_sorted = df_sim_d.sort_values('Cosine',ascending = False )\n    if df_sim_d_sorted.iloc[0]['Cosine'] > .9999:\n        df_sim_d_sorted = df_sim_d_sorted.drop(df_sim_d_sorted.index[0])\n\n    h_num = 0\n    l_num = 0\n    worst_h_num = -1\n    i = 0\n    duties_len = len(duties)\n    #while i< duties_len and df_sim_d_sorted.iloc[i]['Cosine'] > l_threshold:\n        #print ('i, df_sim_d_sorted.iloc[i]['Cosine']')\n#         if df_sim_d_sorted.iloc[i]['Cosine'] > l_threshold:\n#             l_num += 1\n#             worst_match_to_profs= i\n#         if df_sim_q_sorted.iloc[i]['Cosine'] > h_threshold:\n#             worst_h_num = i\n#             h_num += 1\n#         i += 1\n    \n    df_sim_d_sample = df_sim_d_sorted[:10]\n        \n    best_cos_0 = df_sim_d_sample.iloc[0]['Cosine']\n    best_cos_9 = df_sim_d_sample.iloc[9]['Cosine']\n    \n    df_sim_d_sample = df_sim_d_sample.drop ('Cosine', axis=1).reset_index()\n    df_sim_d_sample = df_sim_d_sample.drop ( 'index', axis=1)\n\n    df_sim_d_sample_T = df_sim_d_sample.T\n    df_sim_d_sample_T.insert(loc=0, column='Job', value=[duty.iloc[index]['JOB_CLASS_TITLE']] )\n    df_sim_d_sample_T.insert(loc=1, column='Duty', value=[duty.iloc[index]['JOB_DUTIES']]  )\n    df_sim_d_sample_T.insert(loc=2, column='best_cos', value=best_cos_0)\n    df_sim_d_sample_T.insert(loc=3, column='10th_best_cos', value=best_cos_9)\n#     df_sim_q_sample_T.insert(loc=4, column='similar Q to students', value= h_num)\n#     df_sim_q_sample_T.insert(loc=5, column='Qs to profs', value=l_num)\n    df_sim_d_sample_T.insert(loc=4, column='best matches', value=' ')\n\n#     if worst_h_num > -1:\n#         df_sim_q_sample_T.insert(loc=17, column='worst match to students', value=df_sim_q_sorted.iloc[worst_h_num][col_h])\n#     else:\n#         df_sim_q_sample_T.insert(loc=17, column='worst match to students', value='not available')\n    \n#     if l_num > 0:\n#         df_sim_q_sample_T.insert(loc=18, column='worst match to profs', value=df_sim_q_sorted.iloc[worst_match_to_profs][col_h])\n#     else:\n#         df_sim_q_sample_T.insert(loc=18, column='worst match to profs', value='not available')\n    \n    \n    \n    return ( df_sim_d_sample_T)","52f62e85":"\"\"\"Compare  Duty with Duty using sentence embedding\"\"\"\nstart = time.time()\n\nresults_T = get_sim_duties ('Duty',0,q_embed_array,df_duties,df_duties)\n# for i in range(1,sample_len):\nfor i in range(1,20):\n\n    next_result = get_sim_duties ('Duty',i,q_embed_array,df_duties,df_duties) \n    results_T = pd.concat([results_T,next_result])\nresults_FastText = results_T.T\npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\nend = time.time()\nprint('df time',end - start)\n\ndisplay (results_FastText) ","0fc81717":"df_job_class.to_csv(\"lacitystructureddatafile.csv\")\ndf_job_class.describe()","8852952b":"df_new_kddict.to_csv(\"reviseddatadictonary.csv\")\ndf_new_kddict.describe()","97e48649":"df_explicit.to_csv(\"explicitlinkdata.csv\")\ndf_explicit.describe()","fcd3df54":"# #Code to find files in available datasets\n# import os\n# inputFolder = '..\/input\/'\n# for root, directories, filenames in os.walk(inputFolder):\n#    for filename in filenames:\n#        print(os.path.join(root,filename))\n","2c463a7c":"#Used for cleaning data\n#find duplicates class titles\n#no duplicates now\n\n# df_eda_exam_len = len(df_eda_exam)\n# print (df_eda_exam_len)\n# index = 0\n# while index < df_eda_exam_len:\n#     index2 = 0\n#     #print (index)\n#     var = df_eda_exam.iloc[index]['JOB_CLASS_TITLE']\n#     while index2 < df_eda_exam_len:\n#         if df_eda_exam.iloc[index]['FILE_NAME'] != df_eda_exam.iloc[index2]['FILE_NAME']:\n     \n#             if var == df_eda_exam.iloc[index2]['JOB_CLASS_TITLE']:\n#                 print (df_eda_exam.iloc[index]['FILE_NAME'], df_eda_exam.iloc[index2]['FILE_NAME'])\n#         index2 += 1\n#     index += 1\n","e0a2c4b2":"# #Used for cleaning data\n# #find duplicates class codes\n\n# df_eda_exam_len = len(df_eda_exam)\n# print (df_eda_exam_len)\n# index = 0\n# while index < df_eda_exam_len:\n#     index2 = 0\n#     #print (index)\n#     var = df_eda_exam.iloc[index]['JOB_CLASS_NO']\n#     while index2 < df_eda_exam_len:\n#         if df_eda_exam.iloc[index]['FILE_NAME'] != df_eda_exam.iloc[index2]['FILE_NAME']:\n     \n#             if var == df_eda_exam.iloc[index2]['JOB_CLASS_NO']:\n#                 print (df_eda_exam.iloc[index]['FILE_NAME'], df_eda_exam.iloc[index2]['FILE_NAME'])\n#         index2 += 1\n#     index += 1\n","2cf0d62e":"# #find number of open jobs where internal explicit candidates has been identified\n\n# df_job_class_len = len(df_job_class)\n# print (df_job_class_len)\n# index = 0\n# jobs_found = 0\n# jobs_not_found = 0\n# job_no_sub = 0\n# job_chk = 0\n# while index < df_job_class_len:\n#     if df_job_class.iloc[index]['EXP_JOB_CLASS_TITLE'] != '':\n#         job_chk +=1\n#         print (df_job_class.iloc[index]['EXAM_TYPE'])\n#         if re.search('ON AN INTERDEPARTMENTAL PROMOTIONAL AND AN OPEN COMPETITIVE BASIS',df_job_class.iloc[index]['EXAM_TYPE'] ) or re.search('ON AN OPEN COMPETITIVE BASIS',df_job_class.iloc[index]['EXAM_TYPE']) :\n#             print (df_job_class.iloc[index]['FILE_NAME'], df_job_class.iloc[index]['EXAM_TYPE'])\n#             jobs_found += 1\n            \n            \n#             print (\"exp\", df_job_class.iloc[index]['FILE_NAME'],df_job_class.iloc[index]['EXP_JOB_CLASS_TITLE'])\n#         elif re.search('ON AN DEPARTMENTAL PROMOTIONAL BASIS',df_job_class.iloc[index]['EXAM_TYPE']) \\\n#                 or re.search('ON AN INTERDEPARTMENTAL PROMOTIONAL BASIS',df_job_class.iloc[index]['EXAM_TYPE']):\n#             jobs_not_found += 1\n#     else:\n#         job_no_sub += 1\n#     index += 1\n# print ('jobs_found',jobs_found)\n# print ('jobs_not_found',jobs_not_found)\n# print ('jos_no_sub',job_no_sub)\n\n# print ('job_chk',job_chk)\n","e07c00d2":"Reviewing the above charts by eye indicates that the sample is a reasonable reflection of the full set of bulletins.\n","48128516":"### For later EDA it will be useful to know which jobs require no DRIV_LIC_TYPE\n\nThis is a more specialists job related license linked with traditionally male orientated jobs","6973bffd":"### Reviewing application balance for various groupings of jobs","eff2bf26":"### The structured data file","cce10c0b":"### Reloading","50ceafbe":"#### Course Information","0ce6121c":"# Data Science for Good: City of Los Angeles\n\n\n## Summary\n\nThis Kernel is an entry into the City of LA Kaggle competition which requires the production of a single structured job bulletins CSV file and analysis designed to identify improvements.\n\nThe dataframe which is the source of the CVS file can be found [here](#sdf). A second dataframe called df_eda_exam has also been produced [here](#eda_exam) for the Exploratory Data Analysis (EDA).\n\nThe [applications](#gedata) for a sample of the job bulletins has been show to be a reasonable representation of the full set of job bulletins. Analysis of the CSV file and the applications shows:\n\n* [significant gender imbalance](#gender) for applications to some job class type and salary levels\n \n* an [under-representation of the hispanic community](#ethnicity) in all levels which is most apparent at the top salary range\n\nThe need for welcoming, inclusive and more readable language has been identified by [qualitative](#jblanganal) and [quantitive](#langanal) means. [Recommendations](#bullrec) are made to improve the job bulletin language to fulfill the requirements of the problem stated. The recommendations could be implemented in a staged process.\n\nGraphical representation of the [explicit links](#el) between jobs showing promotion routes have been provided. The data used for these graphial representations can be interogated to provide employee career progression suggestions.\n\nA series of [Next Steps](#nextsteps) are suggested to improve the structured data file and the analysis.\n\n","3273b72a":"## Language Analysis<a id='langanal'><\/a>\n\nThe  job bulletins have been reviewed quantitatively for the following properties:\n\n\n### [Sentiment Analysis](#sa)\n\nThe results show that the bulletins usually deliver a negative sentiment which is largely due to the legal process statements.\n\n### [Gender Coding](#gc)\n\nThe results show that the bulletins are heavily biased to masculine wording.\n\n### [You and Me](#yandme)\n\nTextio, a platform that predicts the type of response job offers will get based on their wording, report that ads using \u201cyou\u201d and \u201cwe\u201d are filled faster. Unfortunately when we do find  instances of the words in the job bulletins, they are related to formal instructions and so the opportunity is missed. \n\n### [Readability](#readr)\n\nMany of the job bulletins have a reading ease suitable for university work. Texts need to be much easier to read.\n","d76baecd":"### Application Data<a id='gedata'><\/a>\n\nWe have [access](https:\/\/catalog.data.gov\/dataset\/applicant-information-from-7-1-2014-to-9-30-2014-7835b) to the application data of 187 of the 672 job bulletins supplied","3607b651":"#### Gender<a id='gendertotal'><\/a>","af07c968":"#### Ethnicity: salary<a id='ethnicitysalary'><\/a>","ea6c2e89":"\n## The problem\n\nThis Kernel is an entry into the City of LA Kaggle competition with the following problem statement:\n\nThe content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor\u2019s Office wants to reimagine the city\u2019s job bulletins by using text analysis to identify needed improvements.\n\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: \n\n(1) identify language that can negatively bias the pool of applicants; \n\n(2) improve the diversity and quality of the applicant pool; and\/or \n\n(3) make it easier to determine which promotions are available to employees in each job class.\n\nDuring the competition, the discussion moved from a focus on diversity to the idea of  balance. Does the LA City recruitment process encourage the development of a work force where the make-up  of the city\u2019s population is represented well in all departments and level of seniority? This would be a balanced workforce.\n\n\n## The Single Structure CVS file\n\nThe dataframe which is the source of the CVS file can be found [here](#sdf). As the requirements can be complex there  is often more that one row per job bulletin\n\nA second dataframe called df_eda_exam has also been produced [here](#eda_exam) for the Exploratory Data Analysis (EDA). This dataframe consists of one row per job bulletin which makes analysis more straightforward. \n\n## CVS file production\nThe CSV file has been produced in an iterative manner with a cycle of code refactoring as familiarilty with the data grew. Anomolies have been identified by output to the terminal. Some of these outputs have been retained in the final version.\n\nThe CSV file has been tested in the EDA  as well as the data cleaning and validation sections by comparing the outputs with the revised data dictionary. Much of this comparison has been done visually given the power of pandas to focus using things like .describe, .groupby and sort.\n\nTo increase readability and the potential for re-use by others, the coding strategy has been to:\n\nisolate the different data types processing into separate functions as far as possible.\n\nuse regex as the main technique for data extraction, using the less esoteric methods available. \n\nReadability has been prioritised over processing speed with the exception of coding for just one read per job bulletin.\n\nIt is likely that the data cleaning routines will not used extensively and so time has not been spent in optimising or generalising. As functions will not be reused, parameter passing is not strictly necessary. However as Python global usage is not straight forward, parameter passing has been retained.\n\nThis approach would not have been appropriate if the task was to provide a generalised method for many different organisations. The current method is focussed on simplifying the process by taking advantage of the particular format used. Given enough data, a machine learning approach would be possible. Alternatively a pre-processing stage could have been introduced for first stage cleaning that could be bespoke.\n\n## Exploratory Data Analysis\nThe main purpose of the EDA is to extract insights to develop recommendations concerning the improvement of the job bulletins as required in the problem statement. .\n\nAs a useful secondary effect, the analysis further tests the validity of the CSV file.\n\nA range of common graphics packages have been used includding matlib, seaborn, networkx and graphviz.\n\nAs well as the usual python libraries I have used Textblob, Textstat for the language analysis.\n\n### Applications Data\n\nThere is a file providing [gender and ethnicity data](#gedata)  for 187 of the 678 job bulletins provided.\n\n[Here](#sample) I show that the sample is a reasonable representation of the ful set of bulletins by comparing ENTRY_SALARY, SCHOOL_TYPE, EXAM_TYPE, DEGREE_REQ, FULL_TIME? and driver license required.\n\n\nThe following insights have been drawn from the data:\n\n[There is an gender inbalance](#gendertotal) in the total applications with 40% coming from women\n\n[The reflection of ethnicity](#ethnicitytotal) in the total applications compared to the [city population](#laethnicity) is more nuanced. Using the terms in the gender and ethnicity data:\n\n    applications from the black and filipino community are much higher than would be predicted from population data\n    \n    applications from the hispanic community are lower than would be predicted from population data\n    \n    applications from the white and asian community are much lower than would be predicted from population data\n \n Digging deeper into the data, it is possible to generate further actionable insights:\n\n#### Gender<a id='gender'><\/a>\nFrom a [gender point of view](#genderdegree), the total number of applications for non-degree level jobs is reasonably balanced. However there is a huge gender imbalance at the job level. For instance only a small fraction of the applications for [apprenticeship type jobs](#genderapprenticeship) and jobs requiring [special driver licenses](#genderdl) are from women.\n\nThe gender imbalance in the total number of applications  is almost entirely due to the larger percentage of males applying for [degree level jobs](#genderdegree). This grouping includes jobs where previously experience has needed a degree level qualification. When we look at applications for jobs that require a [college\/university education](#genderapprenticeship) for the current role, women are far better represented. However they are still in the minority.\n\nWomen are less likely to apply for senior roles that are implied both a [higher salary](#gendersalary) and a college\/university qualification.\n\n #### Ethnicity<a id='ethnicity'><\/a>\n The applications for [appenticeship roles](#ethnicityapprenticeship) most accurately reflects the LA population. This is is useful to know in light of the fact that there is such a wide gender disparity for these roles.\n \n The set of [salary charts](#ethnicitysalary) show the  following trends:\n \n*      the hispanic  community is under-represented at all level and it is most apparent at the top salary range\n     \n*      the caucasion community dominates the top salary ranges\n     \n*      the asian community is  better represented for roles with higher salaries and academic requirements.      \n*      the the black and filipino community are well represented in all charts\n     \n ### EDA Conclusion\n**When identifying language that can negatively bias the pool of applicants and finding ways of improve the diversity and quality of the applicant pool, we should put a priority on the gender and hispanic imbalances identified.**","69a0686f":"#### Salary\n\nIn two cases, an entry salary is not available in either the GEN or DWP column. In these case -1 is used.","5a75739f":"### Finding promotion routes<a id='promotional'><\/a>","66e2135d":"### You and Me<a id='yandme'><\/a>\n\nTextio report that taking a personal approach rather than a formal one produces more applications. This is an opportunity to talk to the applicant as a person and let the culture shine through. \n\nIn the following cells, we count the usage of the words you and we in the  job bulletins. Unfortunately when we do find  instances of the words, they are related to formal instructions and so the opportunity is missed. ","bb5ca2b5":"## Next Steps<a id='nextsteps'><\/a>\n\nThe structured data file may not be in the ideal format and so this kernel could be rerun with the necessary code edits. The code has been written with readability and ease of revision as a top priority to allow for this.\n\nThe provided data is not complete and there are some inconsistencies. If the kernel is rerun with new data, comment out the chk_file_valid(filename) function [here](#topcell). Remove the names of new files from the [missing list](#missingfiles). The kernal will then print out details of anomolies it finds.\n\nIn this kernal the structured datafile has been used with application data to draw insights about where balance could be a problem. By replace the application data with successful candidate and current team make-up data further insight are possible without any code changes.\n\nThere are a number of suggestions concerning the re=presentation of the job bulletins. Again, when revisions are produced the kernal can be re-run to show what improvements have been made. \n\nWhen editting a single file, it can be checked using either Microsoft Word or [this resource](http:\/\/www.readabilityformulas.com\/freetests\/six-readability-formulas.php) for readability. For [gender coding](#gc), refer to these words. Also you can do a you\/we\/us count using your word processing package. The [output](#sa) from the Sentiment Analysis shows the kind of words to avoid. Words do matter, and with these pointers, I think we all hope that more inclusive job bulletins will naturally follow. Remember to keep it short and relegate the small print!\n\nBy adding further data, the explicit link data and diagrams will be automatically updated. It is interesting to compare the diagrams here with the Job Path data provided. Differences are due to both missing job bulletins and new links found by the software.\n\nImplicit links could be an interesting follow up project. We would need revised job bulletins and any further data including fuller job descriptions.\n","792805d7":"## [Explicit links](#explicitlinks)\n\nExtract<a id='el'><\/a> from the Description of promotions in job bulletins.docx file:\n\n*All such promotions could thus be represented in a single directed graph, representing all the City\u2019s job classes and the allowable promotions between them. You are invited to explore ways to visualize this directed graph.*\n\nTwo methods of representing these links are explored:\n\n    circular representation with the job in focus at the centre\n\n    hierarchical representation with the job in focus at the appropriate level\n\nThe single graph requested is presented [here](#hierarchyall).\n\nPerhaps a more useful presentation is at the department level and some examples are presented [here](#hierarchydepartment). It is usefuk to note that these examples often differ from the City JOb Path diagrams because the raw data used is different. The output from the digraph format used here is in text format and could be used as the input for other methods of presentation.\n\nA hierarchical method has also been used to [present graphcially](#hierarchyreq) a fuller set of requirements including school type and licence requirement.\n\nThe circular diagrams are useful when focissing on one job class. [Here](#el) are some examples.\n\nThe first set of plots show the [subordinate](#el) positions to a role, ie who could be promoted. The second set of plots show the subordinate positions to one of the [difficult](#difficult) to fill roles report by the hosts. The third set of plots show what [promotions](#promotional) are available to a role. There are some amazing routes...\n","7de917da":"### Gender Coding<a id='gc'><\/a>\n\nThis tool uses the original list of gender-coded words from the research paper written by [Danielle Gaucher, Justin Friesen, and Aaron C. Kay: Evidence That Gendered Wording in Job Advertisements Exists and Sustains Gender Inequality (Journal of Personality and Social Psychology, July 2011, Vol 101(1), p109-28).](http:\/\/gender-decoder.katmatfield.com\/static\/documents\/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf)\n\nTheir results support the proposition that some words have feminine connotations and other have masculine connotations. This gender coding results in maintaining traditional gender division in work as people identify the wording with their understanding of whether they will \"belong\".\n\nThe words identified in the paper are reproduced below and in the following cells the job bulletins are [\"gender scored\".](#gcresults)\n\n\n\n","c8882f79":"#### Readability Results<a id='readr'><\/a>\n   \nMany of the job bulletins have a reading ease suitable for university work. Texts need to be much easier to read.","909b077e":"#### Example of difficult to understand text\n\nOn the http:\/\/www.readabilityformulas.com site, the following text scored:\n\n**Flesch Reading Ease score: -99.4 (text scale)\nFlesch Reading Ease scored your text: impossible to comprehend.\n\n**Flesch-Kincaid Grade Level: 57.4\nGrade level: College Graduate and above.****\n\nThe examination score will be based entirely on the interview.  In the interview, emphasis may be placed on the candidate's experience, training and professional development as they have provided the knowledge of: grant administration processes including collecting and reviewing applications, managing contracts, maintaining the department database of records and archives, presenting grants to the Cultural Affairs commission, and communicating information to grantees; various art disciplines including dance, literary arts, media arts, music, theater, urban and design arts, visual arts, public sculptures, monuments, murals, and  interdisciplinary or multidisciplinary art experiences where performances and other activities explore non-traditional formats and processes fusing or transcending distinct art disciplines; and the ability to logically and effectively organize priorities sufficient to plan and coordinate community art programs; seek advice regarding possible problems in order to determine how unexpected changes will affect other aspects of a project or program; conduct online and traditional research to gather data, fact check information, and prepare memoranda, letters, news releases, or reports; make recommendations regarding art programs or departmental changes based on staff experience and customer feedback, data, and qualitative histories or outcomes; persuasively communicate art program information including evaluations, department opinions, and recommended courses of action to diverse audiences through oral presentations; facilitate discussions in community meetings and grant review sessions; interact with artists, developers, contractors, agencies, other City employees, management, elected officials, the public, and others in a courteous, tactful, persuasive, and effective manner; and other necessary skills, knowledge, and abilities.\n\n\n\nThe following rewording scored:\n\nFlesch Reading Ease score: 31.4 (text scale)\nFlesch Reading Ease scored your text: difficult to read.\n\nFlesch-Kincaid Grade Level: 12.8\nGrade level: College.\n\n\nThe examination score will be based entirely on the interview.  In the interview, emphasis may be placed on the candidate's experience, training and professional development.  \n\nWe will be looking for experience in a grant administration processes. This should include collecting and reviewing applications, managing contracts and communicating information to grantees. If you are successful you will also be involved in maintaining the department database of records and archives and presenting grants to the Cultural Affairs commission.\n\nDo you have experience in a variety of art disciplines? For instance  dance, literary arts, media arts, music, theater, urban and design arts, visual arts, public sculptures, monuments, murals.\n\nHave you developed any art experiences where performances and other activities explore non-traditional formats and processes? For instance, we are interested in interdisciplinary or multidisciplinary art experiences which fuse or transcend distinct art disciplines. \n\nYou should be able to show you can logically and effectively plan and coordinate community art programs. How have you dealt with unexpected changes or other problems?\n\nWhat experience do you have in gathering data from various sources including online, staff experience and customer feedback? Have you been involved in fact checking, preparing reports and news releases? Have you been involved in make written or oral recommendations regarding art programs or departmental changes based on the data gathered? \n\nHave you facilitated discussions in community meetings and grant review sessions?\n\nWe would also like to hear about your experience of interacting with artists, developers, contractors, agencies, other City employees, management, elected officials and the public. As you would expect we insist on a courteous, tactful, persuasive, and effective approach.","4b4ae9bf":"## Job Bulletin Language Analysis<a id='jblanganal'><\/a>\n\n\nThe concept of Belongingness should be useful in this analysis. [Belongingness](http:\/\/www.fortefoundation.org\/site\/DocServer\/gendered_wording_JPSP.pdf?docID=16121) is defined as a feeling that one fits in with others in a particular domain- it affects people's willingness to engage. Belongingness cues can be picked up from the environment.\n\nPeople will pick up these cues in job descriptions and draw conclusions beyond the set of requirements.\n\nThe bulletins should therefore [use inclusive language](http:\/\/breezy.hr\/blog\/3-simple-rules-for-using-inclusive-language-in-your-job-ads), i.e. make an intentional choice to use words that do not marginalize groups of people who may be knowingly or unknowingly discriminated against because of their culture, race, ethnicity, gender, sexual orientation, age, disability, socioeconomic status, appearance\u200a\u2014\u200aor any other factor that simply shouldn\u2019t play a role.\n\n[This BBVA article](http:\/\/www.bbva.com\/en\/an-inclusive-workplace-begins-with-the-wording-of-job-ads\/) contains a 10 point checklist that can act as a starting point in analysing current job bulletins and  data driven evidence to support recomended changes.\n","8a0d33c2":"#### EDUCATION_MAJOR","42d775b3":"## Displaying the Sample Template","5807d0a3":"### A diagram showing all promotional pathways<a id='hierarchyall'><\/a>","b880a1b6":"#### Salary","4c067bfc":"## Implicit Links using Fasttext with sentence embedding \n\nThere is a Kaggle tutorial on Word2Vec here: https:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial\n\nHere is an article which explains of the idea behind Word2Vec: http:\/\/cgi.cs.mcgill.ca\/~enewel3\/posts\/implementing-word2vec\/\n\n\"One of these assumptions is the distributional hypothesis, which is the idea that the meaning of a word can be understood from the words that tend to be near it. For example \u201cbread\u201d might tend to show up near \u201ceat\u201d, \u201cbake\u201d, \u201cbutter\u201d, \u201ctoast\u201d, etc., and this entourage gives a signal of what \u201cbread\u201d means.\"\n\nHere is the original paper: https:\/\/arxiv.org\/pdf\/1301.3781.pdf\n\nThis Kaggle blog alerted to me that FaxtText may produce better results than Word2Vec: https:\/\/www.kaggle.com\/antonsruberts\/sentence-embeddings-centorid-method-vs-doc2vec\n\nThe author Antons Rubert states:\n\n\"The main difference of FastText from Word2Vec is that it uses sub-word information (i.e character n-grams). While it brings additional utility to the embeddings, it also considerably slows down the process.\"\n\nThe method used here is identical to the one I have used previously for the Word2Vec model except that the vectors are calculated by FastText rather than Word2Vec.\n\nBy using FastText here, we retain all the advantages of the Word2Vec process and improve the accuracy.\n\nFirst build the FastText model and show that the vectors can produce word similarities in the question bow..\n\n\n","5f0d587d":"#### DRIV_LIC_TYPE","fdac8603":"#### Textstat results\n\nThis method is not used as the results don't agree well with other online sources.\n","3e830437":"#### Gender: SCHOOL_TYPE Applications<a id='genderapprenticeship'><\/a>","e2126a60":"#### Gender coding results<a id='gcresults'><\/a>","734dff72":"## [Implicit Linking](#implicitlinks)\n\nA test to develop implicit linking using Fasttext and word embedding comparison of the duty or requirements has been tried.\n\nThe inital results are not useful for a number of reasons and [recommendation](#implicitlinks) for future work are provided.","ed6b536e":"\n### Duty embedding\n\nIn the following code the FastText word vectors are combined to produce vectors for each Duty. This is done by finding the average of all the embeddings improved by taking into account the tfidf scores as described in this paper: http:\/\/www2.aueb.gr\/users\/ion\/docs\/BioNLP_2016.pdf.\n\nThese Duty vectors are then used to find similarity using cosine similarity.\n","04727c30":"# Implicit linking using Fasttext and word embedding.<a id='implicitlinks'><\/a>\n\nIn the dataframe above, the first rows are for the job being considered. The rows below show the duties of jobs that are considered to be similar.\n\nThe similarity is measured by the cosine value and the closer it is to 1, the better the match. The table shows that it is difficult to distinguish between duties with them all having similar cosine values. This is due mainly to the fact that the job specific words are overpowered by the process words which are common. Matching based on the level of the job is also unhelpful (manager of x does not impy suitabilty to manager of b). Overall, there is too little text to make a useful match.\n\nAs the provision of implicit links is not part of the competition, this line of enquiry has been terminated.\n\nRecommendations for future work:\nWhen the requirements have been re-worked with less precision and less process wording the approach could be retried. Tuning is possible by removing common unhelpful words. Better still, is there a source that has a more extensive description of the job that could be used?\n\n","7b4a9a0a":"##\u00a0EDA Code\n\nThe df_eda_exam dataframe has one row for each job class and is useful for analysis.<a id='eda_exam'><\/a>\n","67adc6a3":"## Gender and Ethnicity Analysis","44458b5e":"### Readability<a id='read'><\/a>\n\nThe Flesch Formula or Flesch Reading Ease Formula tells us how easy or difficult a text is to read. The higher the Reading Ease number is then the easier it is to read. The grade score corresponds to a notional US school grade.\n\nAn ease score of 60-70 is a good target for thr internet. 0-30 is suited for university work\n\nThe results below show a wide variation in the automatic counting methods found.\nI have therefore coded an alternative version that agrees reasonably well with Microsoft and  the readabilityformula website\n\nARTS ASSOCIATE\n\n21.8   Flesch-Kincaid  Reading Ease score\n\n17.1   Flesch-Kincaid Grade Level\n\nSENIOR AUTOMOTIVE SUPERVISOR\n\n24.2   Flesch-Kincaid  Reading Ease score\n\n14.7   Flesch-Kincaid Grade Level\n\n**The following results were obtained from the free test found here**:\n\nhttp:\/\/www.readabilityformulas.com\/freetests\/six-readability-formulas.php\n                              \nARTS ASSOCIATE\n\n23.5   Flesch-Kincaid  Reading Ease score\n\n15.5   Flesch-Kincaid Grade Level\n\nSENIOR AUTOMOTIVE SUPERVISOR\n\n33.5   Flesch-Kincaid  Reading Ease score\n\n12.2   Flesch-Kincaid Grade Level\n\n**Microsoft Word produced the following results:**\n\nARTS ASSOCIATE\n\n19.3   Flesch-Kincaid  Reading Ease score\n\n16.1   Flesch-Kincaid Grade Level\n\nwith 1890 words and 52 sentences\n\nSENIOR AUTOMOTIVE SUPERVISOR\n\n24.7   Flesch-Kincaid  Reading Ease score\n\n14.2   Flesch-Kincaid Grade Level\n\nwith 890 words and 35 sentences\n\n**Textstat produced the following results:**\n\nARTS ASSOCIATE 2454 072117 REV 072817.txt\n\n -38.47 flesch_reading_ease \n\n33.1 flesch_kincaid_grade \n\nSENIOR AUTOMOTIVE SUPERVISOR\n\n-15.39 Flesch-Kincaid  Reading Ease score   \n\n26.3  Flesch-Kincaid Grade Level   \n","2eab419d":"### Comparing the sum of all applications","bddfd8eb":"Output which lists all the promotion links","ea44f4d8":"#### Ethnicity<a id='ethnicitytotal'><\/a>","4b6ca65f":"## Job Bulletins: conclusions and recommendations\n\nThe current job bulletins do not communicate to the potential applicant why they should be interested in the job and the organisation. The bulletins are process driven and read like offical public notices. The small print has taken over. There is a place for small print but it needs to be demoted. Afterall who reads the t&cs?\n\nIdeally the format should change to respond to general best practice and the specific LA City requirements of encouraging applications to address the  gender and the Hispanic imbalances . Here is an ideal proposed structure:\n\nJob Title indicating the level of the job\n\nSalary\n\nWhy they should be interested in terms of: \n\n\tThe job challenges and opportunities\n    \n\tThe team and its culture\n    \n\tThe opportunity to progress\n    \n\tThe work environment\n    \n\tThe benefits\n    \nWritten to help them see themselves in the role\n\nClear and short application notes\n\nThis would be a big change and so a first stage bulletins could be to refactored as follows:\n\nJob Title indicating the level of the job\n\nSalary\n\nDuties\n\nRequirements\n\nApplication Process\n\nSmall print\n\nThe text should be written:\n\n* Improve readability with shorter words and sentences. \n* Replace insider\/jargon words like \u201cThis examination is based on a validation study\u201d\n* Reduce the negative sentiment typical of the process driven approach that talks about examinations and disqualification.\n* Express ideas in feminine co-operative style rather than masculine\n* Let the organisation culture shine through the words\n* Show the diversity statement is more than a set of necessary legal words.\n \nTry to loosen up on the strict requirements. Studies show that women are much less likely to apply for a job than men if they fall short of all the stated requirements. Here is an [intersting  paragraph](https:\/\/hbr.org\/2014\/08\/why-women-dont-apply-for-jobs-unless-theyre-100-qualified) about job requirements:\n\n\"There was a sizable gender difference in the responses for one other reason: 15% of women indicated the top reason they didn\u2019t apply was because \u201cI was following the guidelines about who should apply.\u201d Only 8% of men indicated this as their top answer. Unsurprisingly, given how much girls are socialized to follow the rules, a habit of \u201cfollowing the guidelines\u201d was a more significant barrier to applying for women than men.\"","45155f9b":"#### EXAM_TYPE\n\nThis means the type of opportunity.\n\nJust over a half are open to non-employees\n","bd72915b":"## Data Notes\n\n#### 683 bulletins were provided. Of these 2 were duplicate job classes and only the latest bulletin has been included:\n\nCHIEF CLERK POLICE 1249 083118.txt\n\nSENIOR UTILITY SERVICES SPECIALIST 3573 113018.txt\n\n#### In 3 cases the body text does not match the file name and so these bulletins have also been excluded:\n\nANIMAL CARE TECHNICIAN SUPERVISOR 4313 122118.txt\n\nWASTEWATER COLLECTION SUPERVISOR 4113 121616.txt\n\nSENIOR EXAMINER OF QUESTIONED DOCUMENTS 3231 072216 REVISED 072716.txt\n\n#### In one case a class code is not provided:\n\nVocational Worker DEPARTMENT OF PUBLIC WORKS.txt\n\nThe structured data file therefore contains 678 jobs with 677 class codes.\n\n","53e901d5":"## This is the Structured Data File<a id='sdf'><\/a>\n\nThis is the structured data file following production, cleaning and validation.\n","4b54d4d4":"#### ADDTL_LIC","2f2fa7c9":"### Most of the difficult to fill roles are open to everyone not just LA City employees.<a id='difficult'><\/a>\n\n\n\nThe following 17 job classes can be challenging to fill with qualified candidates:\n\n    Accountant\n    Accounting Clerk\n    Applications Programmer\n    Assistant Street Lighting Electrician\n    Building Mechanical Inspector\n    Detention Officer\n    Electrical Mechanic\n    Equipment Mechanic\n    Field Engineering Aide\n    Housing Inspector\n    Housing Investigator\n    Librarian\n    Security Officer\n    Senior Administrative Clerk\n    Senior Custodian\n    Senior Equipment Mechanic\n    Tree Surgeon\n\nIn the future, our Personnel Department expects to find it challenging to fill the following classes:\n\n    IT-related classes (e.g., Applications Programmer)\n    Wastewater classes\n    Inspector classes\n    Journey-level classes\n","483c6b5b":"## Loading in the Job Classes and adding missing ones<a id='missingfiles'><\/a>","47f1abc9":"## Saving and reloading the raw data\n### Saving","39c75baa":"## Finding Explicit Links<a id='explicitlinks'><\/a>","13faab59":"### JOB_CLASS_NO","d17d0030":"### Gender: Driver License<a id='genderdl'><\/a>","d521c1e5":"#### VOCATIONAL_QUAL\n\nThis is ofetn a repeat of contents of the Major column but is also a catch all for miscellaneous training requirements","ad7dea47":"#### FULL_TIME_PART_TIME\n\nThe majority of the experience required by the job requirements is for FULL_TIME.\n\nSome experience is measured in hours, eg:\n\nADMINISTRATIVE HEARING EXAMINER\n520 hours of paid experience with the City of Los Angeles within the past two years from the date of filing as an exempt or contract Administratve Hearing Examiner.\n","4d42fed5":"### For later EDA it will be useful to know which jobs require no SCHOOL_TYPE","083c1134":"### Network diagrams to show explicit links between job classes<a id='el'><\/a>","d4b1cb9e":"### For later EDA it will be useful to know which jobs require a degree or previous experience that required a degree","3e4d311b":"### Looking for subordinates, ie who could apply","3cfd8237":"#### School Type\n","54adfd77":"JOB_CLASS_NO, JOB_DUTIES and DEGREE_REQ fail currently","c52eb843":"## Output for competition","1145c774":"### The following cells test for those columns where all rows must contain the appropriate type or NaN.","576764d3":"### Reformatting salary information to allow graphical presentation","b28acebf":"## Data Clean and Validation\n\n","ca8cd6e3":"### LA Ethnicity<a id='laethnicity'><\/a>\n\n[Los Angeles Demographics](http:\/\/www.census.gov\/quickfacts\/fact\/table\/losangelescitycalifornia\/PST045218#PST045218)\n\nAccording to the US Census bureau and using their terms, the composition of Los Angeles was estimated in 2018 as:\n\n        Hispanic or Latino:48.7%\n        White: 28.4%\n        Asian: 11.7%\n        Black or African American: 8.9%\n        Two or More Races: 3.5%\n        American Indian and Alaska Native: 0.7%\n        Native Hawaiian and Other Pacific Islander: 0.2%","41963ad2":"### Checking how representative the application data is of the 678 job bulletins<a id='sample'><\/a>\n\nThe gender and ethnicity data of the job applicants looks like useful information. But we do need to be sure that it is a reasonable representation of the job bulletin set provided.\n\nTo do this we need to merge the g&e data witht the structured file. We can then compare the full set with the sample for a number of different measures. \n ","aac6ee18":"## Functions called by the cell that produces the structured data file","a1d4cc2e":"### Job Adverts: best practice\n\nAs well as considering diversity and bias, we need to improve the quality of applications. [This blog](https:\/\/www.talentlyft.com\/en\/blog\/article\/167\/job-advertisement-best-practices) gives some interesting insights and the recommendations are consistent with the BBVA article above.\n\nEssentially the quality candidate is in charge. You need to sell the job and tell them why they should consider it. ","22675322":"#### Line by line Examination\nThis cell can be used to examine a job bulletin, line by line to see where the readability is compromised","abcdaaf3":"## Cell that produces the structured data file<a id='topcell><\/a>\n\nA '.' is printed for each job bulletin reviewed\n\nJob bulletin inconsistencies are reported including:\n\n**double requirement found** which means that the terminator ; or was found **within** a requriement line. An attempt is made to handle this but it would be better if the file followed the template guidelines.\n\n**invalid files** because they ar either duplicates or have contents inconsistent with the title\n\n**CAMPUS INTERVIEWS ONLY at file top** where the job title is not the first line\n\n**mismatch between class number in the file and in the filename**. Sometimes the content is ok. When it is not, it is marked maually as invalid in chk_file_valid\n\n\n\n\n","93df5285":"### Of the 10 point, we can measure the LA City job bulletins for the following aspect:\n\n**Avoid extreme language:** this can include jargon, imprecise exclusive words like \"expert\", negative sentiment and difficult to read sentences. For instance, is the level of english required to understand the bulletin higher than that required to perform the job? All these examples exclude people that could be good candidates. \n\n**Avoid words that may convey stereotypes:** words like \"lead\" and \"determine\" reflect masculinity and may deter women from applying. Words that convey individuality over group endevour deter some under represented groups ([see page 13 of this article)](http:\/\/gender-decoder.katmatfield.com\/static\/documents\/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf). \n\n**Avoid using masculine nouns and pronouns.** Using the second-person singular allows to avoid using masculine nouns and adjectives. However, when a direct reference is unavoidable, it is advisable to use gender-neutral nouns, such as \u201cthe person\u201d or \u201cthe candidates\u201d.\n\n**Use \u201cyou\u201d and \u201cus\u201d.** According to Textio, a platform that predicts the type of response job offers will get based on their wording, offers that use \u201cyou\u201d and \u201cwe\u201d are filled faster. Expressions like \u201cyou love finding the best solution to a problem\u201d to address candidates are much better than impersonal ones like \u201cthe ideal candidate\u201d.\n\n**Write as concisely as possible.** Job offers should be brief. Ads written concisely are usually filled faster and usually draw in more applications.\n\n### The remaining aspects are  more qualitative but recommendations about the following can still be made:\n\n**Avoid unclear or unnecessary requirements**: these cannot be measured but they can be found. For instance: \"a driver licence *may be* required\" will deter both high quality candidates who don't want to waste their time as well as less confident candidates. The EDA shows that some job requirements many complicated requirement options.\n\n**Convey a growth mindset.** Companies that are committed to the development of their talent are more likely to attract candidates from underrepresented groups. Expressions that reflect fixed qualities such as \u201cnatural-born analytical thinker\u201d, \u201cextremely intelligent\u201d or \u201cconstantly outperforming\u201d discourage aspiring candidates who may have high growth potential. The opposite happens with expressions such as \u201cpassionate learner\u201d or \u201cmotivated to take on challenges\u201d.\n\n**State the company\u2019s purpose and values.** Emphasizing the company\u2019s values and mission is a good practice that should be taken into account when drawing up the offer, as it can help the candidates determine if it is a place where they would like to work.\n\n**Demonstrate commitment to diversity and inclusion.** It is very advisable to devote some space to describing the company\u2019s commitment to looking for all kinds of talent to build a diverse workforce in which all social groups are represented.\n\n    \nHiring better starts by writing better. Good writing is, in many cases, the key to promoting inclusion. According to Textio, openings advertised using inclusive language get filled 17% faster and attract 23% more female candidates.\n    \n\n    \n\n    \n\n    \n\n    \n","d8f91ddb":"### Hierarchical Diagram at Department Level<a id='hierarchydepartment'><\/a>","76b70b38":"#### DRIVERS_LICENSE_REQ\n","eca446fc":"#### SCHOOL_TYPE","d58e289c":"### Missing Duties replaced with'NOT PROVIDED'","375bf76b":"### Making suggestions about possible promotions\n\nThe df_explicit  dataframe can be used to make promotion suggestions. Given an employees job class, education and driver licence details, the dataframe can be interrogated to supply possible promotion routes or provide information about extra experience\/qualification that would be needed to open up a career route.\n","e448a21d":"### Missing Duties replaced with'0000'","c993e242":"### The following cells test for those columns where all rows must contain the appropriate type.\n","8a197115":"### Reviewing the data ","f27842fa":"## Data Dictionary\n\n### Load in Kaggle Dictionary","b4e92cf9":"### Modify the Dictionary","476c42da":"In the dataframe below a 100% match of applications to the population would give a value of 1.","b18327ff":"#### sentence embedding for duties using FastText","462f9733":"#### Ethnicity: SCHOOL_TYPE Applications<a id='ethnicityapprenticeship'><\/a>","bd7a9643":"#### Requirements\n\nRequirement lists are complicated with one job bulletin providing 8 main options and another providing 10 sub options.","7564141e":"## Job Bulletins: conclusions and recommendations<a id='bullrec'><\/a>\n\nThe current job bulletins do not communicate to the potential applicant why they should be interested in the job and the organisation. The bulletins are process driven and read like offical public notices. The small print has taken over. There is a place for small print but it needs to be demoted. Afterall who reads the t&cs?\n\nIdeally the format should change to respond to general best practice and the specific LA City requirements of encouraging applications to address the  gender and the Hispanic imbalances . Here is an ideal proposed structure:\n\nJob Title indicating the level of the job\n\nSalary\n\nWhy they should be interested in terms of: \n\n\tThe job challenges and opportunities\n    \n\tThe team and its culture\n    \n\tThe opportunity to progress\n    \n\tThe work environment\n    \n\tThe benefits\n    \nWritten to help them see themselves in the role\n\nClear and short application notes\n\nThis would be a big change and so a first stage bulletins could be to refactored as follows:\n\nJob Title indicating the level of the job\n\nSalary\n\nDuties\n\nRequirements\n\nApplication Process\n\nSmall print\n\nThe text should be written to:\n\n* Improve readability with shorter words and sentences. \n* Replace insider\/jargon words like \u201cThis examination is based on a validation study\u201d\n* Reduce the negative sentiment typical of the process driven approach that talks about examinations and disqualification.\n* Express ideas in feminine co-operative style rather than masculine\n* Let the organisation culture shine through the words\n* Show the diversity statement is more than a set of necessary legal words.\n\n*** Are job bulletins available in Spanish? If not, could it happen, it would definitely send a mesage of inclusivity. If they are available, could they be made more accessible**\n\n \nTry to loosen up on the strict requirements. Studies show that women are much less likely to apply for a job than men if they fall short of all the stated requirements. Here is an [intersting  paragraph](https:\/\/hbr.org\/2014\/08\/why-women-dont-apply-for-jobs-unless-theyre-100-qualified) about job requirements:\n\n\"There was a sizable gender difference in the responses for one other reason: 15% of women indicated the top reason they didn\u2019t apply was because \u201cI was following the guidelines about who should apply.\u201d Only 8% of men indicated this as their top answer. Unsurprisingly, given how much girls are socialized to follow the rules, a habit of \u201cfollowing the guidelines\u201d was a more significant barrier to applying for women than men.\"","0c6cc393":"# Contents\n\n## The problem\n## The Single Structure CVS file\n## CVS file production\n## Exploratory Data Analysis\n        Applications Data\n            Gender\n            Ethnicity\n        EDA Conclusion\n## Language Analysis       \n    ### Sentiment Analysis\n    ### Gender Coding\n    ### You and Me\n    ### Readability\n## Job Bulletins: conclusions and recommendations\n## Explicit links\n## Implicit links\n## Data Notes\n## Data Dictionary\n    ### Load in Kaggle Dictionary\n    ### Modify the Dictionary\n## Loading in the Job Classes and adding missing ones\n## Displaying the Sample Template\n## Functions called by the cell that produces the structured data file\n## Cell that produces the structured data file\n## Saving and reloading the raw data\n## Data Clean and Validation\n## This is the Structured Data File\n## EDA Code\n## Gender and Ethnicity Analysis\n    ### LA Ethnicity\n    ### Application Data\n    ### Checking how representative the application data is of the 678 job bulletins\n    ### Comparing the sum of all applications\n    ### Reviewing application balance for various groupings of jobs\n## Finding Explicit Links\n    ### Making suggestions about possible promotions\n    ### A diagram showing all promotional pathways\n## Job Bulletin Language Analysis\n## Job Bulletins: conclusions and recommendations\n## Implicit Links using Fasttext with sentence embedding\n## Output for competition\n","17e74c86":"### Job classes with the most first level subordinates","de56a34e":"#### Degree required?\n","e854fe84":"### Sentiment Analysis: polarity<a id='sa'><\/a>\n\nThe Textblob library provides a quick method of measuring the sentiment and subjectivity of a piece of text. For the job bulletins, subjectivity is not an interesting concept but the polarity data is useful. \n\nNegative sentiments is unwelcoming and exclusive.\n\nThe cell below can be modified to output the particular lines that are judged to have negative sentiment. They are largely related to process information about what happens if candidates fail to do stuff. A better way of making these legal process points should be sought.\n","bf850f47":"### Explicit link data","9c2efc32":"#### Gender: salary<a id='gendersalary'><\/a>","fd8f19ad":"Over specification of requirements is off putting for some underrepresented groups and a less formal, more people focussed approach  should be helpful. For instance, the Real Estate Associate requirement is currently:\n\n*1. Graduation from an accredited four-year college or university and successful completion of at least:\n\ta)\tsix semester or eight quarter units of college level courses in real estate from an accredited college or university; or\n\tb)  \t60 hours of course work from a recognized professional real estate\/right-of-way association; or\n2. Two years of full-time paid experience as a Real Estate Trainee for the City of Los Angeles and successful completion of at least:*\n*a) \tsix semester or eight quarter units of college level courses in real estate from an accredited college or university; or\nb) 48 hours of course work from a recognized professional real estate\/right-of-way association.\n\nOne year of full-time paid experience in performing right-of-way work; appraising the market value of real property; managing commercial or industrial real property; or negotiating on behalf of a large organization or governmental agency for the acquisition, sale, or lease of real property rights may be substituted for up to two years of college education (i.e., 30 semester\/45 quarter units = 1 year of college education) lacking on a year-for-year basis, but may not be substituted for the required courses in real estate.*\n\nThis could be loosened to  say:\n\n1. Graduation from an accredited four-year college\/university and successful completion of some real estate course work; or\n2. Two years of full-time paid experience working in real estate and completion of some real estate course work.\n\n\n\n","50a73bb2":"Now find the total career paths all the way back to an entry job...","0f87c2a4":"#### Gender: degree required?<a id='genderdegree'><\/a>","bc342cff":"### Graphical representation of requirement<a id='hierarchyreq'><\/a>","6211f985":"### The revised data dictionary"}}