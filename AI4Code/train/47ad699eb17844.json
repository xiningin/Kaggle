{"cell_type":{"e34b9810":"code","190ad13a":"code","79e98f28":"code","9be98dea":"code","ff39513f":"code","3a860d7c":"code","3f229cbe":"code","0de0a925":"code","95cb8a9e":"code","1642a147":"code","64f4a3e3":"code","b5736de7":"code","61467f48":"code","5bcc0d82":"markdown","c18f33a2":"markdown","0d447c24":"markdown","aca93b28":"markdown","3a218b85":"markdown","eea45267":"markdown","8e137918":"markdown","7fe947b9":"markdown"},"source":{"e34b9810":"from keras import layers, models\nimport numpy as np # to use reshape()\nfrom keras import datasets  # we can bring our data from here\nfrom keras.utils import np_utils  # to_categorical\nimport matplotlib.pyplot as plt\n","190ad13a":"# bring our data\n\n(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()","79e98f28":"# check \n\nprint(X_train.shape)\nprint(X_test.shape)\n","9be98dea":"# check\n\nX_train[0]","ff39513f":"# check\n\nplt.imshow(X_train[0])\nplt.show()","3a860d7c":"# we change from output(0~9) to output((0 or 1) * 10) vector\n\nY_train = np_utils.to_categorical(y_train)\nY_test = np_utils.to_categorical(y_test)","3f229cbe":"# we need to change 3 dimention data to 2 dimention for training\n\nL, W, H, C = X_train.shape # L = data count, W = width, H = heigh, C = RGB\n\n# DNN handle 2-dimention vector data type, so we need to change to (first line = data count, second = W*H*C)\nX_train = X_train.reshape(-1,W*H*C) # -1 means auto\nX_test = X_test.reshape(-1,W*H*C)","0de0a925":"X_train.shape","95cb8a9e":"# normalization\n\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","1642a147":"def Data_func():\n    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n\n    Y_train = np_utils.to_categorical(y_train)\n    Y_test = np_utils.to_categorical(y_test)\n\n    L, W, H, C = X_train.shape \n    \n    X_train = X_train.reshape(-1, W * H * C)\n    X_test = X_test.reshape(-1, W * H * C)\n\n    # normalization\n    X_train = X_train \/ 255.0\n    X_test = X_test \/ 255.0\n\n    return (X_train, Y_train), (X_test, Y_test)","64f4a3e3":"class DNN(models.Sequential):\n    def __init__(self, Nin, Nh_l, Pd_l, Nout):\n        super().__init__()\n        \n        # first hidden layer\n        self.add(layers.Dense(Nh_l[0], activation='relu',\n                              input_shape=(Nin,), name='Hidden-1'))\n        self.add(layers.Dropout(Pd_l[0]))      \n        \n        # second hidden layer (we dont have to set input_shape, keras do it)\n        self.add(layers.Dense(Nh_l[1], activation='relu',\n                              name='Hidden-2'))\n        self.add(layers.Dropout(Pd_l[1]))\n\n        \n        # output layer\n        self.add(layers.Dense(Nout, activation='softmax'))\n\n        self.compile(loss='categorical_crossentropy',\n                     optimizer='adam',\n                     metrics=['accuracy'])","b5736de7":"# check for accuracy\ndef plot_acc(history, title=None):\n    # summarize history for accuracy\n    if not isinstance(history, dict):\n        history = history.history\n\n    plt.plot(history['accuracy'])\n    plt.plot(history['val_accuracy'])\n    if title is not None:\n        plt.title(title)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Training', 'Verification'], loc=0)\n    # plt.show()\n    \n# check for loss\ndef plot_loss(history, title=None):\n    # summarize history for loss\n    if not isinstance(history, dict):\n        history = history.history\n\n    plt.plot(history['loss'])     # training loss\n    plt.plot(history['val_loss']) # validation loss\n    if title is not None:\n        plt.title(title)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Training', 'Verification'], loc=0)\n    # plt.show()","61467f48":"def main(Pd_l=[0.0, 0.0]):\n    Nh_l = [100, 50]\n    number_of_class = 10\n    Nout = number_of_class\n\n    (X_train, Y_train), (X_test, Y_test) = Data_func()\n    model = DNN(X_train.shape[1], Nh_l, Pd_l, Nout)\n    history = model.fit(X_train, Y_train, epochs=100, batch_size=100, validation_split=0.2)\n\n    performace_test = model.evaluate(X_test, Y_test, batch_size=100)\n    print('Test Loss and Accuracy ->', performace_test)\n\n    plot_acc(history, '(a) Changes in accuracy')\n    plt.show()\n    plot_loss(history, '(b) Changes in loss')\n    plt.show()\n\n\nif __name__ == '__main__':\n    main(Pd_l=[0.0, 0.0])","5bcc0d82":"* We use relu activation function for preventing overfitting\n* Dropout(P) means Send a signal with P probability","c18f33a2":"# Simple DNN\n\nDNN (Deep Neural Network)\n![13131.png](attachment:13131.png)\n* **DNN (Deep Neural Network):** A **deep neural network (DNN)** is an artificial neural network (ANN) with multiple layers between the input and output layers\n* **DNN (Deep Neural Network):** has multiple hidden layers   (multiple = deep)\n* If there are many layers, it cause **vanishing gradient problem**, So it is key point to prevent **overfitting**\n* Our **DNN** is focusing on classification which ficture is, when **DNN** gets ficture of 10 classes (CIFAR-10)\n* We are using Keras module\n\n<hr>\n\nHow to use this notebook :\n\nThere is only minimum explanation\n\nThis notebook could be helpful for who want to see how code works right away\n\nPlease upvote if it was helpful !\n\n<hr>\n\n## Content\n1. [Import Libraries](#one)\n2. [Prepare Data](#two)\n3. [Modeling](#three)\n4. [Training & Evaluation](#four)\n\n<hr>","0d447c24":"* 60000 data ( 50000 for training, 10000 for test)\n* 32 * 32 * 3 (RGB image),(each pixel can have 0~255 value)\n* 0 ~ 9 (10 classes)\n* https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\n![12111.png](attachment:12111.png)","aca93b28":"<a id=\"three\"><\/a>\n# 3. Modeliing","3a218b85":"## Reference\n* Coding chef 3 minute deep learning  -(https:\/\/github.com\/jskDr\/keraspp\/blob\/master\/ex3_2_dnn_cifar10_cl.py)\n* Wikipedia - DNN  -(https:\/\/en.wikipedia.org\/wiki\/DNN)","eea45267":"<a id=\"four\"><\/a>\n# 4. Training & Evaluation","8e137918":"<a id=\"two\"><\/a>\n# 2. Prepare Data","7fe947b9":"<a id=\"one\"><\/a>\n# 1. Import Libraries"}}