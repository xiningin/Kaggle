{"cell_type":{"da47427c":"code","3717bd9e":"code","815210ac":"code","d43cb5a1":"code","bb6fb5fe":"code","65040143":"code","3b87d033":"code","dd635c29":"code","e05cf936":"code","dc2c014e":"code","242e1524":"code","1809e56b":"code","68c9b711":"code","6ca36715":"code","52ff8f6b":"code","f6d401cc":"code","31d7d69e":"code","46723574":"code","f24dc073":"markdown","4dd25479":"markdown","03eb5c5e":"markdown","99328d29":"markdown","e50540d1":"markdown","23f17aab":"markdown","e52d4cbb":"markdown","6fe27791":"markdown","7f28f294":"markdown","819919f4":"markdown","cc9eb162":"markdown","a2d7f5bb":"markdown","d1f3aec3":"markdown","1bae5b1f":"markdown","fd830a59":"markdown","6c7d2506":"markdown","ed7aa772":"markdown","9e90ad4a":"markdown","1923cf21":"markdown","89b40202":"markdown","f06b90ca":"markdown","ec2e987b":"markdown","bd04f3c4":"markdown","1aa7d7c1":"markdown","9c07cfea":"markdown","cb93d081":"markdown","6341f0c5":"markdown","0754a269":"markdown","7b96e02b":"markdown","a8a6c5f9":"markdown","0f6f38e3":"markdown","aa12e6de":"markdown","937d08ba":"markdown","08f0361b":"markdown","c916b79d":"markdown","eecb6240":"markdown","0d0335d5":"markdown","6a7cbf3e":"markdown","550fd208":"markdown","80cf1b7f":"markdown"},"source":{"da47427c":"# Lets plot a scatter chart, which shall help us in guessing the no. of clusters we may need\n\nimport matplotlib.pyplot as plt\nx = [6.3, 5.8, 7.1, 5.1, 4.9, 4.7, 4.6, 7, 6.4, 6.9, 5.5]\ny = [3.3, 2.7, 3, 3.5, 3, 3.2, 3.1, 3.2, 3.2, 3.1, 2.3]\nplt.scatter(x,y)\nplt.show()","3717bd9e":"import matplotlib.pyplot as plt\nx = [6.3, 5.8, 7.1, 5.1, 4.9, 4.7, 4.6, 7, 6.4, 6.9, 5.5]\ny = [3.3, 2.7, 3, 3.5, 3, 3.2, 3.1, 3.2, 3.2, 3.1, 2.3]\n\ncx = [6.3, 5.1, 7]\ncy = [3.3, 3.5, 3.2]\n\nplt.scatter(x,y)\nplt.scatter(cx,cy,label='Centroids',color='red',marker='1')\n\nplt.legend()\nplt.show()","815210ac":"import matplotlib.pyplot as plt\nx = [6.3, 5.8, 7.1, 5.1, 4.9, 4.7, 4.6, 7, 6.4, 6.9, 5.5]\ny = [3.3, 2.7, 3, 3.5, 3, 3.2, 3.1, 3.2, 3.2, 3.1, 2.3]\n\ncx = [6.35, 5.1, 7]\ncy = [3.25, 2.97, 3.1]\n\nplt.scatter(x,y)\nplt.scatter(cx,cy,label='New Centroids',color='red',marker='1')\n\nplt.legend()\nplt.show()","d43cb5a1":"x1 = [6.3, 6.4]\ny1 = [3.3, 3.2]\nx2 = [5.8, 5.1, 4.9, 4.7, 4.6, 5.5]\ny2 = [2.7, 3.5, 3, 3.2, 3.1, 2.3]\nx3 = [7.1, 7, 6.9]\ny3 = [3, 3.2, 3.1]\nplt.scatter(x1,y1,color='r')\nplt.scatter(x2,y2,color='b')\nplt.scatter(x3,y3,color='g')\nplt.scatter(cx,cy,label='Centroids',color='red',marker='1')\nplt.legend()\nplt.show()","bb6fb5fe":"x1 = [6.3, 5.8, 7.1]\ny1 = [3.3, 2.7, 3]\nx2 = [5.1, 4.9, 4.7, 4.6]\ny2 = [3.5, 3, 3.2, 3.1]\nx3 = [7, 6.4, 6.9, 5.5]\ny3 = [3.2, 3.2, 3.1, 2.3]\nplt.scatter(x1,y1,color='r')\nplt.scatter(x2,y2,color='b')\nplt.scatter(x3,y3,color='g')\nplt.show()","65040143":"#to read and format the Iris file\nimport pandas as pd \n\n#package to perform Kmeans algorithm\nfrom sklearn.cluster import KMeans \n\n#For numerical functions\nimport numpy as np\n\n#for graphs\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d","3b87d033":"#Read the Iris dataset\n\ndata = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","dd635c29":"#add headers to the file\nattributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndata.columns = attributes\ndata.head()","e05cf936":"# converting class names (string) to numbers which will aid us in plotting\ndata['class-num'] = data['class'].map( {'Iris-setosa': 0,'Iris-versicolor': 1,'Iris-virginica': 2} )","dc2c014e":"data.head()","242e1524":"#Define the features to be used for the algo i.e. remove column 'class' which is not required for the algo\nX = data.drop(columns=[\"class\",\"class-num\"])\n\n#Let's see the first five records in the file\nX.head()","1809e56b":"#Let's see the data characteristic\ndata.describe()","68c9b711":"wcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 10), wcss)\nplt.title('Elbow-Method using WCSS')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Within-Cluster-Sum-of-Squares')\nplt.show()","6ca36715":"kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\nkmeans.fit(X)","52ff8f6b":"inertia = kmeans.predict(X)","f6d401cc":"X[\"Clusters\"] = inertia\nX.head()","31d7d69e":"fig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.scatter3D(X['sepal_length'],X['sepal_width'],X['petal_length'],c=X['Clusters'],cmap='hsv')\nplt.show()","46723574":"fig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.scatter3D(data['sepal_length'],data['sepal_width'],data['petal_length'],c=data['class-num'],cmap='hsv')\nplt.show()","f24dc073":"As you can see, there are some discrepencies. The two main reason for this discrepency is:<br>\n**1)Inadequate Features:** We have considered only Sepal Length and Width and have not taken into consideration Petal Length & Width <br>\n**2)Low Sample:** We have considered very few Data Points <br>\n\nAbove pointers are a good learing lessons and stresses on the point  how critical is to identify right number of Features and Samples","4dd25479":"# Python Exercise:","03eb5c5e":"----","99328d29":"### Let's compare the above plot with Original data (class)","e50540d1":"|Data #\t|sepal length in cm\t|sepal width in cm\t|Dist. from C1\t|Dist. from C2\t|Dist. from C3\t|Cluster tagged|\n|---|-------|-------|-------|-------|------|---|\n|1\t|6.3\t|3.3\t|0.00\t|1.00\t|0.60\t|C1|\n|2\t|5.8\t|2.7\t|1.10\t|0.10\t|1.70\t|C2|\n|3\t|7.1\t|3.0\t|0.50\t|1.50\t|0.10\t|C3|\n|4\t|5.1\t|3.5\t|1.00\t|0.00\t|1.60\t|C2|\n|5\t|4.9\t|3.0\t|1.70\t|0.70\t|2.30\t|C2|\n|6\t|4.7\t|3.2\t|1.70\t|0.70\t|2.30\t|C2|\n|7\t|4.6\t|3.1\t|1.90\t|0.90\t|2.50\t|C2|\n|8\t|7.0\t|3.2\t|0.60\t|1.60\t|0.00\t|C3|\n|9\t|6.4\t|3.2\t|0.00\t|1.00\t|0.60\t|C1|\n|10\t|6.9\t|3.1\t|0.40\t|1.40\t|0.20\t|C3|\n|11\t|5.5\t|2.3\t|1.80\t|0.80\t|2.40\t|C2|","23f17aab":"#### Clustering falls under the Unserpervised Machine Leaning Family. \n\nMore on the Machine Learning Classification:\n\n![](https:\/\/raw.githubusercontent.com\/nadarsubash\/articles\/master\/ML_Classification.jpeg)\n*Disclaimer: this may not be an exhaustive list, but important ones for someone starting into Machine Learning World*\n\n## K-Means is a Heuristic Algorithm which groups together relatively closer objects into K Clusters \nK-Means is one of the most widely used Clustering algorithm ","e52d4cbb":"##### Let's compare above plot with the Pictorial representation of the Original Distribution","6fe27791":"Let's take Data # 1, 4 & 8 as the centroid for the first iteration <br>\n$C_{1}$ = (6.3, 3.3) <br>\n$C_{2}$ = (5.1, 3.5) <br>\n$C_{3}$ = (7.0, 3.2) <br>","7f28f294":"#### Let's take a look at the newly created clusters vis-a-vis Actual Class as per the original data","819919f4":"##### Steps involved in K-Means algorithm","cc9eb162":"## Let's use KMeans package from Scikit learn to perform this exercise on entire Iris dataset","a2d7f5bb":"### KMeans Cluster has been able to largely group the points accurately...well almost. This is a good begining!\n#### ....and the end of this Tutorial with Hands-on Exercise!","d1f3aec3":"### Step 2) Decide no. of clusters *(randomly based on above plot)* ","1bae5b1f":"We are taking very few samples (records) of each species for this Tutorial <br>\n*Complete dataset is freely available [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Iris)*","fd830a59":"### Let's plot the data based on cluster created using KMeans algo","6c7d2506":"### Step 4) Measure the distance between the Centroids and each of the data points","ed7aa772":"### Step 6) Iterate Step 4 and Step 5 till there is no change to the Centroid","9e90ad4a":"Hmm, our KMeans exercise seems to have fit the objects relatively accurate :)\n\nThis concludes the Tutorial for K-Means clustering","1923cf21":"Manhattan Distance between data point **X**(x1, y1) and centroid **C**(x2, y2) is: <br>\nDistance **dist(X, C)** = |x1 \u2013 x2| + |y1 \u2013 y2|\n\n**ITERATION 1**","89b40202":"# K-Means Clustering - Detailed Tutorial with Hands-on","f06b90ca":"### Step 5) Now calculate new centroid location based on the 'Cluster tagged'","ec2e987b":"This distance shall help us decide which point is closeset to which Centroid, which in turn shall help us decide to form clusters (groups)","bd04f3c4":"Here is the visualization of the Iris data on SandDance *(pretty cool image)*\n![](https:\/\/raw.githubusercontent.com\/nadarsubash\/articles\/master\/SandDance-Iris.jpg)","1aa7d7c1":"##### New Centroids are:\n$C_{1}$ = (6.35, 3.25) <br>\n$C_{2}$ = (5.1, 2.97) <br>\n$C_{3}$ = (7.0, 3.1) <br>","9c07cfea":"##### Let's see pictorial representation of the above formed clusters","cb93d081":"WCSS is computed as:<br>\n$$WCSS=\\sum_{i=1}^k(\\sum_{j=1}^n (X_{j}-C_{i})^2)$$  <br>\n*where:* <br>\n*$C_{k}$ is centroid for observation $X_{i}$* <br>\n*k is number of centroids* <br>\n*n is number of objects within respective centroid* <br>","6341f0c5":"### Some of the important parameters used in KMeans Functions:\n\nWe can define predefined number of clusters\/centroids using the **n_clusters** parameter. Initial selection of the cluster centers (coordinates) can be done using various techniques using **init** parameter. Options are:<br>\n*\u2018k-means++\u2019* : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence<br>\n*\u2018random\u2019*: choose k observations (rows) at random from data for the initial centroids.<br><br>\nWe can also decide the number of time the k-means algorithm will be run with *different centroid seeds* using **n_init** parameter. The final results will be the best output of n_init consecutive runs in terms of inertia.<br><br>\n**max_iter** - Maximum number of iterations of the k-means algorithm for a single run.<br>\n**random_state** - Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.","0754a269":"|\tCluster tagged\t|\tActual Class\t|\n|---|---|\n|\tC1\t|\tIris-virginica\t|\n|\tC1\t|\tIris-versicolor\t|\n|\tC2\t|\tIris-virginica\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-versicolor\t|\n|\tC3\t|\tIris-virginica\t|\n|\tC3\t|\tIris-versicolor\t|\n|\tC3\t|\tIris-versicolor\t|","7b96e02b":"|Data #\t|sepal length in cm\t|sepal width in cm\t|Dist. from C1\t|Dist. from C2\t|Dist. from C3\t|Cluster tagged\t|\n|---|---|---|---|---|---|---|\n|\t1\t|\t6.3\t|\t3.3\t|\t0.0\t|\t1.5\t|\t0.5\t|\tC1\t|\n|\t9\t|\t6.4\t|\t3.2\t|\t0.0\t|\t1.5\t|\t0.5\t|\tC1\t|\n|\t2\t|\t5.8\t|\t2.7\t|\t1.1\t|\t0.4\t|\t1.6\t|\tC2\t|\n|\t4\t|\t5.1\t|\t3.5\t|\t1.0\t|\t0.5\t|\t1.5\t|\tC2\t|\n|\t5\t|\t4.9\t|\t3\t|\t1.7\t|\t0.2\t|\t2.2\t|\tC2\t|\n|\t6\t|\t4.7\t|\t3.2\t|\t1.7\t|\t0.2\t|\t2.2\t|\tC2\t|\n|\t7\t|\t4.6\t|\t3.1\t|\t1.9\t|\t0.4\t|\t2.4\t|\tC2\t|\n|\t11\t|\t5.5\t|\t2.3\t|\t1.8\t|\t0.3\t|\t2.3\t|\tC2\t|\n|\t3\t|\t7.1\t|\t3\t|\t0.5\t|\t2.0\t|\t0.0\t|\tC3\t|\n|\t8\t|\t7\t|\t3.2\t|\t0.6\t|\t2.1\t|\t0.1\t|\tC3\t|\n|\t10\t|\t6.9\t|\t3.1\t|\t0.4\t|\t1.9\t|\t0.1\t|\tC3\t|","a8a6c5f9":"Formula for calculating new centroids:\n$$C_{x}=(\\frac{1}{n})\\sum_{i=1}^{n}x_{i}$$    $$C_{y}=(\\frac{1}{n})\\sum_{i=1}^{n}y_{i}$$","0f6f38e3":"### Next Step is to identify optimum number of clusters (k)\nHere we use **Elbow Method** *(clear elbow formed at the optimum cluster)* \n\nWe plot the graph of Within-Cluster-Sum-of-Squares **(WCSS)** against the Total Number of clusters to view formation of elbow to arrive at the optimum number of cluster **k** \n\n**Idea is** - as the number of Clusters increases, sum of distance between the objects and it's respective clusters reduces *(objects get closer to their respective Cluster)*. But hey, we don't want to overfit either...correct. Hence the quest for optimum **k**","aa12e6de":"|Data #\t|sepal length in cm\t|sepal width in cm\t|class|\n|-----|-----|-----|-----|\n|1\t|6.3\t|3.3\t|Iris-virginica|\n|2\t|5.8\t|2.7\t|Iris-virginica|\n|3\t|7.1\t|3\t|Iris-virginica|\n|4\t|5.1\t|3.5\t|Iris-setosa|\n|5\t|4.9\t|3\t|Iris-setosa|\n|6\t|4.7\t|3.2\t|Iris-setosa|\n|7\t|4.6\t|3.1\t|Iris-setosa|\n|8\t|7\t|3.2\t|Iris-versicolor|\n|9\t|6.4\t|3.2\t|Iris-versicolor|\n|10\t|6.9\t|3.1\t|Iris-versicolor|\n|11\t|5.5\t|2.3\t|Iris-versicolor|","937d08ba":"### Step 3) Identify the Centroids *$C_{i}$*","08f0361b":"Let's use **Manhattan Distance** for this calculation <br>\n*You may as well check other distance calculation methods like Euclidean, Minkowski distance etc*","c916b79d":"for simplicity sake, we shall only consider two dimentional data i.e. only sepal length and width","eecb6240":"### Step 1) Let's first take some sample data *(what better than iris data)*","0d0335d5":"Again for simplicity sake, we go with **k=3**, as our aim is to understand what happens in the background of the K-Means algorithm.\n\n*N.B.* We can use Elbow method using Within-Cluster-Sum-of-Squares (WCSS) in Python to decide optimized number of *k*. This is explained in detail in the exercise **below**  *(KMeans clustering using package from SciKitLearn)*","6a7cbf3e":"In this example, there is no change to the Centroid in the 2nd Iteration, hence we can assume this to be the most optimum Centroid\n\n**ITERATION 2**","550fd208":"We can see from above plot that prominent elbows are formed between at 2, 3 and 4<br>\nNoticeably, WCSS reduces marginally below 3 and hence we select 3 as optimum **k**","80cf1b7f":"-----"}}