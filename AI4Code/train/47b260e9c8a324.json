{"cell_type":{"b25559b2":"code","94ff06c2":"code","524b458c":"code","650cdaed":"code","bb8312d2":"code","627fcd89":"code","6c28ae74":"code","ec36424a":"code","62d175fb":"code","c193e2b5":"code","f6a46dab":"code","3f349cc9":"code","4e1e424b":"code","d0c6ba68":"code","f34558a7":"code","b8b0c304":"code","291ba927":"code","3e42805b":"code","b3311b68":"code","ba744f3a":"code","8e704889":"code","fe1da62f":"code","8fab2d51":"code","39fdf8dc":"code","41ca49a5":"code","6757cf2e":"code","10b8b94c":"code","7a92b060":"code","56902d82":"code","c9e5c9bd":"code","d7509201":"code","67b71d33":"code","7d2908ba":"code","a70f7fe3":"code","91ada213":"code","e270e717":"code","52712338":"markdown","4c82ba17":"markdown","17040746":"markdown","628e03ca":"markdown","5786a4bb":"markdown","745b0a68":"markdown","22e3d9b0":"markdown","933e69ed":"markdown","1d97e91e":"markdown","b878d280":"markdown","99c62f80":"markdown","f6e39a99":"markdown","b6c5a65c":"markdown","c4e4f5f2":"markdown","555fdac7":"markdown","9afcae12":"markdown","7124e386":"markdown","2c0d0c24":"markdown","a40f14db":"markdown","16cfc5e5":"markdown","f86daa36":"markdown","cc72cee3":"markdown","0f40380f":"markdown","346549ed":"markdown","29d4ec9e":"markdown","528438a1":"markdown","821eb438":"markdown","21497d40":"markdown","92ea841d":"markdown","49cfe7c4":"markdown","c7748a22":"markdown","a79c43c8":"markdown","15763200":"markdown","13505813":"markdown","d8415910":"markdown","b12950ff":"markdown","3a15ec00":"markdown","1d92eb1c":"markdown","257de50f":"markdown","afa8f5e7":"markdown"},"source":{"b25559b2":"import matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom sklearn.svm import OneClassSVM\nfrom numpy.random import seed\nfrom keras.layers import Input, Dropout\nfrom keras.layers.core import Dense \nfrom keras.models import Model, Sequential, load_model\nfrom keras import regularizers\nfrom keras.models import model_from_json\nfrom scipy.special import softmax","94ff06c2":"main_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\nmain_df.describe()","524b458c":"#heatmap e correlaciones de -1 a 1\nsns.heatmap(main_df.corr(), vmin= -1, vmax = 1)","650cdaed":"main_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\n#Eliminando columnas sin sentido para esta propuesta                           (axis=1) = columns\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n#Volteando los valores de la columna\nmain_df['pCut::Motor_Torque'] = main_df['pCut::Motor_Torque'] *-1\n#Heatmap\nsns.heatmap(main_df.corr(), vmin= -1, vmax = 1)\n","bb8312d2":"def handle_non_numeric(df):\n    # Tomamos los valores de las columnas de nuestro df\n    columns = df.columns.values\n    \n    for column in columns:\n        \n        # Diccionario con cada valor numerico con cada texto\n        text_digit_vals = {}\n        \n        # Receibimos texto para convertilo a valor numerico\n        def convert_to_int (val):\n            \n\n            return text_digit_vals[val]\n        \n        # Verificamos que los valores no sean flotantes o enteros\n        if df[column].dtype !=np.int64 and df[column].dtype != np.float64:\n            \n            # Obtenemos los valores de la columna actual\n            column_contents = df[column].values.tolist()\n            \n            # Obtenemos los valores unicos de la columna\n            unique_elements = set(column_contents)\n            x=0\n            \n            for unique in unique_elements:\n                \n                if unique not in text_digit_vals:\n                    text_digit_vals[unique] = x\n                    x+=1\n            \n            df[column] = list(map(convert_to_int, df[column]))\n    \n    return df","627fcd89":"#Tomando todo el conjunto de datos\nmain_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\n#Eliminando columnas con informaci\u00f3n no deseada \/ irrelevante para el algoritmo\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n#Transformando modos en datos clasificados\nmain_df = handle_non_numeric(main_df)\n\nX = main_df\n\n#Definiendo el procesamiento para los datos\nscaler = preprocessing.MinMaxScaler()\n#Preprocesamiento\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\n\n#Escalando\nX = preprocessing.scale(X)\n#Obtenemos las primeras 200.000 filas.\nX_train = X[:200000]\n\n\n#Creaci\u00f3n de una SVM OneClass adecuada\nocsvm = OneClassSVM(nu=0.25, gamma=0.05)\nocsvm.fit(X_train)","6c28ae74":"df=main_df.copy()\ndf['anomaly'] = pd.Series(ocsvm.predict(X))","ec36424a":"#Saving Dataframe.\ndf.to_csv('Labled_df.csv')","62d175fb":"#Reading into dataframe\ndf = pd.read_csv('..\/input\/created\/Labled_df.csv', index_col=0)\ndf.head()","c193e2b5":"#Obteniendo grupos etiquetados\nscat_1 = df.groupby('anomaly').get_group(1)\nscat_0 = df.groupby('anomaly').get_group(-1)\n\n# Plot size\nplt.subplots(figsize=(15,7))\n\n# Plot group 1 -labeled, color green, point size 1\nplt.plot(scat_1.index,scat_1['pCut::Motor_Torque'], 'g.', markersize=1)\n\n# Plot group -1 -labeled, color red, point size 1\nplt.plot(scat_0.index, scat_0['pCut::Motor_Torque'],'r.', markersize=1)\n","f6a46dab":"#Crear un marco de datos para la puntuaci\u00f3n de cada muestra de datos\nscore = pd.DataFrame()\n#Devolviendo puntajes al df\nscore['score'] = ocsvm.score_samples(X)\n\n#Plot size\nplt.subplots(figsize=(15,7))\n#Plotting\nscore['score'].plot()\n#Guardando puntajes dataframe\nscore.to_csv('SVM_Score.csv')","3f349cc9":"fig, ax = plt.subplots(figsize=(15,7))\n\n\n((score['score'].rolling(20000).mean())*-1).plot(ax=ax)","4e1e424b":"plt.subplots(figsize=(15,7))\nplt.plot(score.index, score['score'],'r.', markersize=1)","d0c6ba68":"#------ Preparando funciones para entrenamiento y predicci\u00f3n futura -----\nmain_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\nmain_df = handle_non_numeric(main_df)\nX = main_df\n\nscaler = preprocessing.MinMaxScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\nX = preprocessing.scale(X)\n#-------------------------------------------------------------------\n\n\n#Porcentaje de los datos que se considerar\u00e1n condici\u00f3n saludable\ntrain_percentage = 0.15\n#Valor entero para el segmento que se considerar\u00e1 en buen estado\ntrain_size = int(len(main_df.index)*train_percentage)\n#Tomando parte de los datos de entrenamiento\nX_train = X[:train_size]\n\n\nkmeans = KMeans(n_clusters=1)\n\nkmeans.fit(X_train)\n\nk_anomaly = main_df.copy()\n\nk_anomaly = pd.DataFrame(kmeans.transform(X))\n\nk_anomaly.to_csv('KM_Distance.csv')\n\nplt.subplots(figsize=(15,7))\n\nplt.plot(k_anomaly.index, k_anomaly[0], 'g', markersize=1)","f34558a7":"#------------------------- Preparando informacion para el entrenamiento --------------------------- \nmain_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\nmain_df = handle_non_numeric(main_df)\nX = main_df\n\nscaler = preprocessing.MinMaxScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\n\n\n\nX = preprocessing.scale(X)\n\n\ntrain_percentage = 0.15\ntrain_size = int(len(main_df.index)*train_percentage)\n\nX_train = X[:train_size]\n#----------------------------------------------------------------------------------\n\n\n\n#Semilla para validaci\u00f3n y entrenamiento de lotes aleatorios\nseed(10)\nact_func = 'elu'\nmodel=Sequential()\n\n# Primera capa oculta, conectada al vector de entrada X.\nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform',\n                kernel_regularizer=regularizers.l2(0.0),\n                input_shape=(X_train.shape[1],)\n               )\n         )\n# Segunda capa oculta\nmodel.add(Dense(10,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n# Tercera capa oculta\nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\n# Capa de entrada\nmodel.add(Dense(X_train.shape[1],\n                kernel_initializer='glorot_uniform'))\n\n# Funci\u00f3n de p\u00e9rdida y elecci\u00f3n del optimizador\nmodel.compile(loss='mse',optimizer='adam')\n\n# Modelo de tren para 50 \u00e9pocas, tama\u00f1o de lote de 200\nNUM_EPOCHS=50\nBATCH_SIZE=200\n\n#Aprovechando la validaci\u00f3n y la p\u00e9rdida de entrenamiento durante \u00e9pocas\nhistory=model.fit(np.array(X_train),np.array(X_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=NUM_EPOCHS,\n                  validation_split=0.1,\n                  verbose = 1)\n","b8b0c304":"plt.subplots(figsize=(15,7))\n\nplt.plot(history.history['loss'],'b',label='Training loss')\nplt.plot(history.history['val_loss'],'r',label='Validation loss')\nplt.legend(loc='upper right')\nplt.xlabel('Epochs')\nplt.ylabel('Loss, [mse]')\n\nplt.show()","291ba927":"#Reconstrucci\u00f3n de datos de entrenamiento\nX_pred = model.predict(np.array(X_train))\nX_pred = pd.DataFrame(X_pred,columns=main_df.columns)\nX_pred.index = pd.DataFrame(X_train).index\n\nscored = pd.DataFrame(index=pd.DataFrame(X_train).index)\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n\nplt.subplots(figsize=(15,7))\nsns.distplot(scored['Loss_mae'],\n             bins = 15, \n             kde= True,\n            color = 'blue');\n\n\n\n","3e42805b":"\n#Reconstruyendo toda la informacion\nX_pred = model.predict(np.array(X))\nX_pred = pd.DataFrame(X_pred,columns=main_df.columns)\nX_pred.index = pd.DataFrame(X).index\n\n#Devolviendo la media de las p\u00e9rdidas para cada columna y poni\u00e9ndola en un marco de datos\nscored = pd.DataFrame(index=pd.DataFrame(X).index)\nscored['Loss_mae'] = np.mean(np.abs(X_pred-X), axis = 1)\n\nplt.subplots(figsize=(15,7))\n\n\n#Guardando el dataframe\nscored.to_csv('AutoEncoder_loss.csv')\n\nplt.plot(scored['Loss_mae'],'b',label='Prediction Loss')\n\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.ylabel('Loss, [mse]')","b3311b68":"plt.subplots(figsize=(15,7))\n\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\nplt.plot(enc_loss.index,enc_loss['Loss_mae'], 'g.', markersize=1,label=\"AutoEncoder Loss\")\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.show()\n\nplt.subplots(figsize=(15,7))\nk_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nplt.plot(k_anomaly.index,k_anomaly['0'], 'g.', markersize=1,label=\"KM cluster Distance\")\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.show()\n\nplt.subplots(figsize=(15,7))\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nplt.plot(score.index,score['score'], 'g.', markersize=1,label=\"OCSVM score\")\nplt.legend(loc='upper right')\nplt.xlabel('Sample')\nplt.show()\n","ba744f3a":"#Plot size\nplt.subplots(figsize=(15,7))\n\n#Leyendo cada csv\nk_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\n#Escalando datos para la visualizaci\u00f3n\nk_distance = k_anomaly\/k_anomaly.max()\nsvm_score = (score\/score.max())*-1\n\nplt.plot(enc_loss.index,enc_loss['Loss_mae'], label=\"AutoEncoder Loss\")\nplt.plot(svm_score.index, svm_score['score'],label=\"OCSVM score\")\nplt.plot(k_distance.index,k_distance['0'], label=\"Kmeans Euclidean Dist\")\n\n\n\nplt.gca().legend(('AutoEncoder Loss','OCSVM score * -1','Kmeans Euclidean Dist'))\n","8e704889":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\ncorr.corr()","fe1da62f":"#---- Leyendo datos y pasarlos al marco de datos nuevamente ----- \n\nk_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n#---------------------------------------------------------\n\n\n\nplt.subplots(figsize=(15,7))\n\n#Scatter plot de SVM\nplt.plot(corr.index, corr['SVM_score'], 'g.', markersize=1, label = 'OCSVM_score')\n#Trazado de la media m\u00f3vil de 1000\nplt.plot(corr.index, corr['SVM_score'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.legend(loc='upper right')\nplt.show()\n\n\nplt.subplots(figsize=(15,7))\nplt.plot(corr.index, corr['KM_cluster_distance'], 'g.', markersize=1, label = 'KM_cluster_distance')\nplt.plot(corr.index, corr['KM_cluster_distance'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.legend(loc='upper right')\nplt.show()\n\n\nplt.subplots(figsize=(15,7))\nplt.plot(corr.index, corr['AutoEnc_loss'], 'g.', markersize=1, label = 'AutoEnc_loss')\nplt.plot(corr.index, corr['AutoEnc_loss'].rolling(1000).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.legend(loc='upper right')\nplt.show()\n\n","8fab2d51":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['SVM_score'].head(160000), bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['KM_cluster_distance'].head(160000),bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['AutoEnc_loss'].head(160000),bins=15)\nplt.show()","39fdf8dc":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['SVM_score'], bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['KM_cluster_distance'],bins=15)\nplt.show()\n\nplt.subplots(figsize=(10,7))\nsns.distplot(corr['AutoEnc_loss'],bins=15)\nplt.show()","41ca49a5":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\n\n\n#Creando una matriz para que los umbrales se tracen en todo el conjunto de datos\nlower_threshold = np.full((corr['SVM_score'].size, 1), 0)\nupper_threshold = np.full((corr['SVM_score'].size, 1), 18000)\nhigh_density_threshold = np.full((corr['SVM_score'].size, 1), 13250)\n\nplt.subplots(figsize=(15,7))\nplt.plot(corr.index, corr['SVM_score'], 'k', markersize=1, label = 'OCSVM_score')\nplt.plot(corr.index, corr['SVM_score'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.plot(corr.index, lower_threshold, label='Lower Threshold')\nplt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\nplt.plot(corr.index, high_density_threshold, label = 'Highest Density')\nplt.legend(loc='upper right')\nplt.show()\n\n\nlower_threshold = np.full((corr['KM_cluster_distance'].size, 1), 1.2)\nupper_threshold = np.full((corr['KM_cluster_distance'].size, 1), 17.5)\nhigh_density_threshold = np.full((corr['KM_cluster_distance'].size, 1), 2.5)\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr.index, corr['KM_cluster_distance'], 'k', markersize=1, label = 'KM_cluster_distance')\nplt.plot(corr.index, corr['KM_cluster_distance'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.plot(corr.index, lower_threshold, label='Lower Threshold')\nplt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\nplt.plot(corr.index, high_density_threshold, label = 'Highest Density')\nplt.legend(loc='upper right')\nplt.show()\n\nlower_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0)\nupper_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.1)\nhigh_density_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.05)\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr.index, corr['AutoEnc_loss'], 'k', markersize=1, label = 'AutoEnc_loss')\nplt.plot(corr.index, corr['AutoEnc_loss'].rolling(100).mean(), 'r', markersize=1, label = 'Moving Mean')\nplt.plot(corr.index, lower_threshold, label='Lower Threshold')\nplt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\nplt.plot(corr.index, high_density_threshold, label = 'Highest Density')\nplt.legend(loc='upper right')\nplt.show()\n","6757cf2e":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr['KM_cluster_distance'],corr['SVM_score'],'b.',markersize=1 )\nplt.xlabel('KM')\nplt.ylabel('SVM')\nplt.show()\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr['AutoEnc_loss'],corr['SVM_score'],'b.' ,markersize=1 )\nplt.xlabel('Encoder')\nplt.ylabel('SVM')\nplt.show()\n\nplt.subplots(figsize=(15,7))\n  \nplt.plot(corr['AutoEnc_loss'],corr['KM_cluster_distance'],'b.' ,markersize=1 )\nplt.xlabel('Encoder')\nplt.ylabel('KM')\nplt.show()","10b8b94c":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nmain_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\n\n#Pasar la p\u00e9rdida del codificador al marco de datos principal, para que sea m\u00e1s f\u00e1cil separar por mes\nmain_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n\n#Obteniendo lista de los meses\nmonths = main_df['month'].dropna().unique()\n\n#Recorriendo cada mes\nfor month in months:\n    #Tomando la porci\u00f3n del marco de datos para cada mes\n    month_df = main_df.groupby('month').get_group(month)\n    \n    upper_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.1)\n    high_density_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.05)\n    \n    plt.subplots(figsize=(15,7))\n    plt.plot(month_df.index, month_df['AutoEnc_loss'], label=f'AutoEnc_loss month_{month}')\n    plt.plot(month_df.index, upper_threshold, label = 'Upper Threshold')\n    plt.plot(month_df.index, high_density_threshold, label = 'Highest Density')\n    plt.legend(loc='upper right')\n    plt.ylim(0,1.3)\n    \n    plt.show()\n    \n    ","7a92b060":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nmain_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\n\nmain_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n\nmonths = main_df['month'].dropna().unique()\n\nfor month in months:\n    month_df = main_df.groupby('month').get_group(month)    \n    \n    plt.subplots(figsize=(15,7))\n    sns.distplot((month_df['AutoEnc_loss']), bins=15).set_title(f'Month {month} Loss Distribution')\n    #X axis limits\n    plt.xlim([-1.2,1.2])\n    plt.show()","56902d82":"k_anomaly = pd.read_csv('..\/input\/created\/KM_Distance.csv')\nscore = pd.read_csv('..\/input\/created\/SVM_Score.csv')\nenc_loss = pd.read_csv('..\/input\/created\/AutoEncoder_loss.csv')\n\ncorr = pd.DataFrame()\ncorr['SVM_score'] = score['score']\ncorr['KM_cluster_distance'] = k_anomaly['0']\ncorr['AutoEnc_loss'] = enc_loss['Loss_mae']\n\nmain_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\n\nmain_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n\nmonths = main_df['month'].dropna().unique()\n\nfor month in months:\n    month_df = main_df.groupby('month').get_group(month)\n    kurt = (month_df['AutoEnc_loss']).kurtosis()\n    print(f'Month {month} kurtosis = {kurt}')","c9e5c9bd":"main_df = pd.read_csv('..\/input\/datasetsone-year-compiledcsv\/One_year_compiled.csv')\nmain_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\nmain_df = handle_non_numeric(main_df)\nX = main_df\n\nscaler = preprocessing.MinMaxScaler()\n\nX = pd.DataFrame(scaler.fit_transform(X), \n                              columns=X.columns, \n                              index=X.index)\nX = preprocessing.scale(X)\n\ntrain_percentage = 0.15\ntrain_size = int(len(main_df.index)*train_percentage)\n\nX_train = X[:train_size]\n\nseed(10)\n\nact_func = 'elu'\n\n# Input layer:\nmodel=Sequential()\n# First hidden layer, connected to input vector X. \nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform',\n                kernel_regularizer=regularizers.l2(0.0),\n                input_shape=(X_train.shape[1],)\n               )\n         )\n\nmodel.add(Dense(10,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\nmodel.add(Dense(50,activation=act_func,\n                kernel_initializer='glorot_uniform'))\n\nmodel.add(Dense(X_train.shape[1],\n                kernel_initializer='glorot_uniform'))\n\nmodel.compile(loss='mse',optimizer='adam')\n\n# Train model for 100 epochs, batch size of 10: \nNUM_EPOCHS=50\nBATCH_SIZE=200\n\nhistory=model.fit(np.array(X_train),np.array(X_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=NUM_EPOCHS,\n                  validation_split=0.1,\n                  verbose = 1)","d7509201":"#Prediciendo y pasando la predicci\u00f3n al dataframe\nX_pred = model.predict(np.array(X))\nX_pred = pd.DataFrame(X_pred,columns=main_df.columns)\nX_pred.index = pd.DataFrame(main_df).index\n\n#Pasando X de un array a un dataframe\nX = pd.DataFrame(X,columns=main_df.columns)\nX.index = pd.DataFrame(main_df).index\n\n#Dataframe donde ir\u00e1 toda la p\u00e9rdida por columnas\nloss_df = pd.DataFrame()\n\nmain_df.drop('mode',axis=1, inplace=True)\n\nfor column in main_df.columns:\n    loss_df[f'{column}'] = (X_pred[f'{column}'] - X[f'{column}']).abs()\n     \n    plt.subplots(figsize=(15,7))\n    plt.plot(loss_df.index, loss_df[f'{column}'], label=f'{column} loss')\n    plt.legend(loc='upper right')\n    \n    plt.show()\n\nloss_df.to_csv('AutoEncoder_loss_p_column.csv')","67b71d33":"sftmax_df = pd.read_csv('..\/input\/created\/AutoEncoder_loss_p_column.csv', index_col=0)\nsftmax_df = softmax(sftmax_df, axis=1)\nsftmax_df.describe()","7d2908ba":"for column in sftmax_df.columns:\n    \n\n    plt.subplots(figsize=(15,7))\n    plt.plot(sftmax_df.index, sftmax_df[f'{column}'], label=f'{column} loss')\n    plt.legend(loc='upper right')\n    \n    plt.show()","a70f7fe3":"plt.subplots(figsize=(15,7))\n\n#Labels for stackbar plot\ndf_label = ['Torque', 'Cut lag','Cut speed','Cut position','Film position','Film speed','Film lag','VAX']\n\n#Stackbar plot\nplt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n              sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pSpintor::VAX_speed'],\n             labels = df_label)\n\nplt.legend(loc='upper center', ncol=8)\n\nplt.ylim(0,1)","91ada213":"plt.subplots(figsize=(15,7))\n\ndf_label = ['Torque', 'Cut lag','Cut speed','Cut position','Film position','Film speed','Film lag','VAX']\n\nsftmax_df = sftmax_df[400000:600000]\n\nplt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n              sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n             sftmax_df['pSpintor::VAX_speed'],\n             labels = df_label)\n\nplt.legend(loc='upper center', ncol=8)\n\nplt.ylim(0,1)","e270e717":"for column in sftmax_df.columns:\n    \n\n    plt.subplots(figsize=(15,7))\n    \n    sns.distplot(( sftmax_df[f'{column}']), bins=15).set_title(f'Contribution Distribution')\n    plt.xlim(0,1)\n    \n    plt.show()","52712338":"# Resultados del analysis","4c82ba17":"# Contexto\n##### Este dataset contiene la informacion de un compomente en degradacion durante 12 meses y se registraron sus cambios. Y se inici\u00f3 en el proyecto europeo de investigaci\u00f3n e innovaci\u00f3n IMPROVE.","17040746":"# Approach\n\n**Se probar\u00e1n algunos algoritmos para ver si podemos obtener informaci\u00f3n sobre la m\u00e1quina. Usaremos OneClass SVM y KMeans con 1 cl\u00faster para probar la agrupaci\u00f3n. Despu\u00e9s de eso, probaremos un Autoencoder para intentar reproducir datos basados en el estado de salud de las m\u00e1quinas.**\n\n**En los 3 casos, tomaremos un segmento de las primeras filas y lo consideraremos como el estado correcto de la m\u00e1quina, luego lo enviaremos a los algoritmos. Despu\u00e9s de eso, le daremos a los algoritmos la totalidad del conjunto de datos y veremos c\u00f3mo funcionan en el resto de los datos. Las desviaciones, puntuaciones bajas y p\u00e9rdidas elevadas se considerar\u00e1n anomal\u00edas a estudiar.**","628e03ca":"**Distribuci\u00f3n de p\u00e9rdidas sobre los datos de entrenamiento**","5786a4bb":"# OneClass SVM approach\n\n**OneClass SVM se utiliza para la detecci\u00f3n de valores at\u00edpicos, intenta encontrar 2 clases en los datos, la clase \"normal\" y los valores at\u00edpicos. Usaremos la SVM para intentar encontrar valores at\u00edpicos y anomal\u00edas.**","745b0a68":"# one-year-industrial-component-degradation\n\nDataset:  https:\/\/www.kaggle.com\/inIT-OWL\/one-year-industrial-component-degradation","22e3d9b0":"**Trazando el porcentaje de la contribuci\u00f3n de cada columna a la p\u00e9rdida total**","933e69ed":"**Ahora para hacer lo mismo pero con todos nuestros datos para ver la p\u00e9rdida a lo largo del tiempo, esto nos dar\u00e1 datos interesantes.**","1d97e91e":"# KMeans approach\n\n**El enfoque de Kmeans har\u00e1 lo mismo que el OC-SVM**","b878d280":"# Detecci\u00f3n de sensor\n\n**Ahora que sabemos d\u00f3nde tiene un problema la m\u00e1quina, intentaremos encontrar qu\u00e9 componente \/ sensor est\u00e1 causando esta perturbaci\u00f3n en el codificador autom\u00e1tico. Para eso lo entrenaremos nuevamente, y obtendremos sus predicciones y p\u00e9rdidas para cada columna para ver cu\u00e1l tiene la mayor contribuci\u00f3n a la p\u00e9rdida total.**","99c62f80":"**Ahora aplicaremos la funci\u00f3n Softmax a cada fila para que podamos obtener el porcentaje que cada columna contribuye a la p\u00e9rdida total. En cuanto a la suma de cada fila nos dar\u00e1 1.**","f6e39a99":"**Visualizando anomalias**","b6c5a65c":"**Comparando los tres metodos de deteccion de anomalia por medio de Scatter plots con media m\u00f3vil**","c4e4f5f2":"**Visualizando puntuaciones para todo el conjunto de datos**","555fdac7":"### Diagrama de dispersi\u00f3n para la puntuaci\u00f3n de cada algoritmo, para ver a trav\u00e9s del ruido.","9afcae12":"**Guardando el Dataframe**","7124e386":"**Eliminemos algunas columnas y volteemos nuestros valores de columna de motor torque multiplic\u00e1ndo por -1. Todo esto para una mayor comprensi\u00f3n visual de la grafica**","2c0d0c24":"**Importamos las librerias necesarias**","a40f14db":"**Una forma interesante de ver la anomal\u00eda es la cola de la distribuci\u00f3n. Como pudimos ver en la gr\u00e1fica por mes, el cuarto mes tiene el punto de anomal\u00eda m\u00e1s alto. Tambi\u00e9n es evidente la influencia de eso en la distribuci\u00f3n de la p\u00e9rdida. Quiz\u00e1s al observar la curtosis de cada mes podamos obtener m\u00e1s informaci\u00f3n.**\n\n**La curtosis nos informar\u00e1 sobre la forma de la distribuci\u00f3n. Una curtosis alta significa que muchos puntos de datos tienen el mismo valor y que las colas o la desviaci\u00f3n est\u00e1ndar son realmente peque\u00f1as o inexistentes (en nuestro caso, muchos puntos de datos cercanos a 0 significan una buena condici\u00f3n de la m\u00e1quina). Curtosis baja significa que tenemos muchos puntos de datos dispersos, lo que le da a la distribuci\u00f3n colas anchas y gruesas, casi del tama\u00f1o de su pico.** ","16cfc5e5":"**Ahora podemos trazar un diagrama de pila para visualizar mejor la contribuci\u00f3n de cada columna a la p\u00e9rdida total. Como ver\u00e1, la posici\u00f3n de Blades contribuye mucho a la p\u00e9rdida total en ese pico que vimos. Miraremos m\u00e1s de cerca a esa porci\u00f3n.**","f86daa36":"# Continuando con Autoencoder\n**Aunque existe cierta simularidad de anomal\u00edas visuales entre los algoritmos, los algoritmos de agrupamiento nos dan mucho ruido y poco en lo que trabajar. Por otro lado, el codificador autom\u00e1tico tiene una carrera casi segura hasta el punto de falla. No podemos concluir con absoluta certeza que los componentes hayan cambiado despu\u00e9s del pico de p\u00e9rdida m\u00e1s alto, pero es muy posible.**\n\n**Adem\u00e1s, ahora analizaremos la p\u00e9rdida del codificador por mes, con los umbrales.**  ","cc72cee3":"**Parece que el error de retraso de la hoja tambi\u00e9n proporciona una gran parte de la p\u00e9rdida total. La posible explicaci\u00f3n aqu\u00ed es que la cuchilla est\u00e1 desgastada y, por eso, est\u00e1 comenzando a desviarse del camino que la m\u00e1quina intenta trazar para la cuchilla al cortar la pel\u00edcula.**\n\n**Ahora veremos la distribuci\u00f3n de la contribuci\u00f3n de cada columna a la p\u00e9rdida total, solo para comprender mejor qu\u00e9 sensores est\u00e1n dando mayor p\u00e9rdida.**","0f40380f":"**Scatter plot de los puntajes del algoritmo vs los otros algoritmos**","346549ed":"**Leyendo el dataframe**","29d4ec9e":"**Scatplot para ver la partitura a trav\u00e9s del ruido**","528438a1":"**Buscando correlaci\u00f3n entre los algoritmos**","821eb438":"**Predecir y clasificar el conjunto de datos en anomal\u00edas y no anomal\u00edas, luego pasarlo a un marco de datos.**","21497d40":"# AutoEncoder approach\n\n**AutoEncoders son redes neuronales que expanden y comprimen datos en dimensiones m\u00e1s altas y m\u00e1s bajas, luego intenta recrear los datos. La idea es que el autocodificador comprender\u00e1 la relaci\u00f3n entre las caracter\u00edsticas y, a partir de eso, recrear\u00e1 los datos exactos que se le dieron.**\n\n**Entrenaremos el algoritmo con el estado saludable de la m\u00e1quina. A medida que intenta reconstruir el resto de los datos como el estado correcto, la p\u00e9rdida de reconstrucci\u00f3n, la diferencia entre los datos de la m\u00e1quina pronosticados y los datos de la m\u00e1quina real, se considerar\u00e1 un estado \"incorrecto\"**","92ea841d":"**Ahora busquemos la correlaci\u00f3n entre los datos. Como se puede ver, lo \u00fanico que parece correlacionase es el motor torque, la velocidad de la hoja y el error de retraso de la hoja, tambi\u00e9n la velocidad VAX y la velocidad de la envolvedora. Excluyendo, por supuesto, el mes y el n\u00famero de muestra, y todas las autocorrelaciones en la diagonal.**","49cfe7c4":"**Ahora usaremos la informaci\u00f3n sobre la distribuci\u00f3n de la p\u00e9rdida de entrenamiento para determinar algunos umbrales para los gr\u00e1ficos. Tambi\u00e9n se trazar\u00e1n los medios m\u00f3viles.**","c7748a22":"**Ahora veremos la distribuci\u00f3n de p\u00e9rdidas por mes.**","a79c43c8":"**Graficar la p\u00e9rdida de validaci\u00f3n y la p\u00e9rdida de entrenamiento a lo largo de los epochs**","15763200":"**As\u00ed que los meses con baja curtosis son los meses con m\u00e1s anomal\u00edas, lo que nos puede decir un poco sobre el estado de la m\u00e1quina.**","13505813":"# Conclusion \n\n**Con toda la informaci\u00f3n recopilada, podr\u00edamos decir d\u00f3nde y cu\u00e1ndo la m\u00e1quina sufri\u00f3 una degradaci\u00f3n masiva. Tambi\u00e9n pudimos decir cu\u00e1l de las medidas contribuye m\u00e1s a la p\u00e9rdida en todo el a\u00f1o de la m\u00e1quina, lo que nos dice que estos componentes fabricados necesitan m\u00e1s atenci\u00f3n. Se pueden utilizar umbrales de percentiles, an\u00e1lisis de anomal\u00edas de distribuci\u00f3n, SVM y muchos otros m\u00e9todos para la detecci\u00f3n del desgaste de los componentes a lo largo del tiempo con la informaci\u00f3n que se proporciona aqu\u00ed.**\n\n**Un problema con este m\u00e9todo es la necesidad de preprocesar y escalar los datos por completo para luego entregarlos al algoritmo. Una forma de superar este problema es utilizar este conjunto de datos o una porci\u00f3n de \u00e9l y combinarlo con una nueva porci\u00f3n de datos para los an\u00e1lisis del estado del sistema. Y para cada nuevo segmento de datos que necesite analizar, eliminamos el segmento agregado anterior y mantenemos este conjunto de datos intacto.** ","d8415910":"**Graficando cada algoritmo, con OCSVM invertido sobre 0.**","b12950ff":"## Tratamiento de datos no num\u00e9ricos\n\n**En nuestro conjunto de datos, los modos de la m\u00e1quina pueden influir en los patrones de datos, por lo que necesitamos transformar esos datos de cadenas a clases num\u00e9ricas. Esta funci\u00f3n har\u00e1 precisamente eso.** ","3a15ec00":"**Primero pasamos el df a un variable llamada main_df el cual contienes todos los archivos de los 12 meses guardados en un solo csv para manejar mas facilmente los datos**","1d92eb1c":"**Ahora alimentaremos el algoritmo con los mismos datos de entrenamiento y haremos que intente reconstruir los datos. Luego veremos la distribuci\u00f3n de la p\u00e9rdida sobre los datos de entrenamiento, m\u00e1s adelante usaremos esta distribuci\u00f3n para determinar algunos umbrales.**","257de50f":"**Distribuci\u00f3n de p\u00e9rdidas en todo el conjunto de datos**","afa8f5e7":"**Media m\u00f3vil de puntuaci\u00f3n invertida**"}}