{"cell_type":{"fba8a049":"code","222703e6":"code","f10ed0c1":"code","b4c0a81d":"code","65d818a9":"code","09c5ead9":"code","e88a4743":"code","8ee5eb4f":"code","6066fe5a":"code","b42510f3":"code","e71b40b6":"code","888f1d34":"code","9a5e1ab2":"code","a7dd83b0":"code","6440f04d":"code","91520a03":"code","61cbc1b9":"code","8c0627bd":"code","9836e0e8":"code","0f4beaeb":"code","8a27a769":"code","a98239b8":"code","0865287e":"code","403995c6":"code","96cdbae3":"code","1b7bcf84":"code","419e21b8":"code","00cfbe84":"code","acc8474f":"code","a02e5e07":"code","00ad59dc":"code","41ba1a82":"code","2aac2d51":"code","0ace569a":"code","49a59862":"code","490c6309":"code","a9bd1664":"code","f89e8f5e":"code","f243aaa2":"code","1fb4b94c":"code","9e770122":"code","c3e828d0":"code","717d7997":"code","8ab9be45":"code","12698483":"code","c3a199ad":"code","20acaa96":"code","5daecea1":"code","bb483fe3":"code","89376fca":"code","a75c35d5":"code","c432a3e0":"code","bea335f8":"code","c114a45b":"code","0d4a6c1e":"code","a77c5003":"code","a5f47982":"code","d1dd07bc":"code","2976a769":"code","3fa167b9":"code","5792fb80":"code","2ebb74b1":"code","7c543981":"code","d7d54347":"code","f3018d99":"markdown","4ce4cc67":"markdown","68780821":"markdown","fba78038":"markdown","728c63d9":"markdown","b2472e40":"markdown","e49dc536":"markdown","5e0a431d":"markdown","1b134817":"markdown","016acf9a":"markdown"},"source":{"fba8a049":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","222703e6":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n","f10ed0c1":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b4c0a81d":"train.head()\n","65d818a9":"test.head()\n\n","09c5ead9":"train.columns","e88a4743":"test.columns","8ee5eb4f":"train['SalePrice'].describe()\n","6066fe5a":"sns.distplot(train['SalePrice'])\n","b42510f3":"train[\"SalePrice\"].isna().sum()","e71b40b6":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","888f1d34":"total = pd.concat([train,test])","9a5e1ab2":"one_hot_encoded_training_predictors = pd.get_dummies(train)\none_hot_encoded_test_predictors = pd.get_dummies(test)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","a7dd83b0":"final_train[\"SalePrice\"]","6440f04d":"total_fin = pd.concat([final_train, final_test])","91520a03":"y = total_fin[\"SalePrice\"]\nX = total_fin.drop([\"SalePrice\"],axis=1)","61cbc1b9":"nulls = total_fin.isnull().sum()\nnulls[nulls > 0]","8c0627bd":"total_final = total_fin.drop(columns=['Utilities_NoSeWa', 'Utilities_NoSeWa','Condition2_RRAe','Condition2_RRAn','Condition2_RRNn','HouseStyle_2.5Fin','HouseStyle_2.5Fin','RoofMatl_ClyTile','RoofMatl_Membran','RoofMatl_Metal','RoofMatl_Roll','Exterior1st_ImStucc','Exterior1st_Stone','Exterior2nd_Other','Heating_Floor','Heating_OthW','Electrical_Mix','GarageQual_Ex','PoolQC_Fa','MiscFeature_TenC'])\n","9836e0e8":"total_final_s=total_final.apply(lambda x: x.fillna(x.mean()),axis=0)","0f4beaeb":"nulls = total_final_s.isnull().sum()\nnulls[nulls > 0]","8a27a769":"total_final_s","a98239b8":"import matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\nXX = total_final_s.drop('SalePrice', 1)\ny = total_final_s['SalePrice']\n\n#log transform the target:\nyy = np.log1p(y)\n\n#log transform skewed numeric features:\nnumeric_feats = XX.dtypes[XX.dtypes != \"object\"].index\n\nskewed_feats = XX[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nXX[skewed_feats] = np.log1p(XX[skewed_feats])\n\nfrom sklearn.model_selection import train_test_split\n\ntraina, testa = train_test_split(total_final_s , test_size=0.2,random_state=27)\nX_train, X_test, y_train, y_test = train_test_split(XX, yy, test_size=0.33)","0865287e":"X_train","403995c6":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n#for this post we will use MinMaxScaler\nscaler=MinMaxScaler()","96cdbae3":"X_train_total=pd.DataFrame(scaler.fit_transform(X_train.T).T,columns=X_train.columns)\nX_test_total=pd.DataFrame(scaler.fit_transform(X_test.T).T,columns=X_test.columns)","1b7bcf84":"# decision tree for feature importance on a regression problem\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\n\n# define the model\nmodel = DecisionTreeRegressor()\n# fit the model\nmodel.fit(X_train_total, y_train)\n# get importance\nimportance = model.feature_importances_\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))","419e21b8":"n_top_features = 50\ntop_features = importance.argsort()[-n_top_features:]\nprint(top_features)  # [ 0  4  7 12  5]","00cfbe84":"X_train_final = X_train_total.iloc[:, top_features]\nX_test_final = X_test_total.iloc[:, top_features]\n","acc8474f":"X_train_final","a02e5e07":"X_test_final","00ad59dc":"##Linear Regression\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression().fit(X_train_final, y_train)\nreg.score(X_train_final, y_train)\n\n","41ba1a82":"reg.coef_\n","2aac2d51":"reg.intercept_","0ace569a":"y_train_pred = reg.predict(X_train_final)","49a59862":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_pred)))","490c6309":"y_pred = reg.predict(X_test_final)","a9bd1664":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_pred)))","f89e8f5e":"##Ridge Regression\n\nfrom sklearn.linear_model import Ridge\nimport numpy as np\nfrom pandas import read_csv\n# evaluate an ridge regression model on the dataset\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import absolute\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Ridge\n\n\nridge = Ridge(alpha=0.1, normalize=True)\nridge.fit(X_train_final, y_train)\ny_ridge_pred = ridge.predict(X_test_final)\n","f243aaa2":"y_train_ridge_pred = ridge.predict(X_train_final)","1fb4b94c":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_ridge_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_ridge_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_ridge_pred)))","9e770122":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_ridge_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_ridge_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_ridge_pred)))","c3e828d0":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n##Lasso Regression\nmodel_lasso = Lasso(alpha=0.01)\nmodel_lasso.fit(X_train_final, y_train) \ny_pred_lasso= model_lasso.predict(X_test_final)\n","717d7997":"y_train_lasso_pred = model_lasso.predict(X_train_final)","8ab9be45":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_lasso_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_lasso_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_lasso_pred)))","12698483":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_pred_lasso))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_pred_lasso))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_pred_lasso)))","c3a199ad":"##KNN\nmodel_knn = KNeighborsRegressor(n_neighbors=8)\nprint(model)\nKNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n          metric_params=None, n_jobs=1, n_neighbors=8, p=2,\n          weights='uniform') \n\nmodel_knn.fit(X_train_final, y_train)\n","20acaa96":"y_train_knn_pred = model_knn.predict(X_train_final)","5daecea1":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_knn_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_knn_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_knn_pred)))","bb483fe3":"y_pred_knn = model_knn.predict(X_test_final)","89376fca":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_pred_knn))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_pred_knn))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_pred_knn)))","a75c35d5":"##Decision Tree Algorithm\nrt = DecisionTreeRegressor(max_depth=5)\nmodel_r = rt.fit(X_train_final, y_train)\ny_pred_dta = model_r.predict(X_test_final)\n","c432a3e0":"y_train_dta_pred = model_r.predict(X_train_final)","bea335f8":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_dta_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_dta_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_dta_pred)))","c114a45b":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_pred_dta))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_pred_dta))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_pred_dta)))","0d4a6c1e":"#Random Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nX, y = make_regression(n_features=4, n_informative=2,\n                       random_state=0, shuffle=False)\nregrr = RandomForestRegressor(max_depth=2, random_state=0)\nregrr.fit(X_train_final, y_train)\ny_pred_rfr = regrr.predict(X_test_final)\n","a77c5003":"y_train_rr_pred = regrr.predict(X_train_final)","a5f47982":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_rr_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_rr_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_rr_pred)))","d1dd07bc":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_pred_rfr))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_pred_rfr))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_pred_rfr)))","2976a769":"##SVM\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nn_samples, n_features = 10, 5\nregrsvm = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\nregrsvm.fit(X_train_final, y_train)\n\ny_pred_svm = regrsvm.predict(X_test_final)\n","3fa167b9":"y_train_svm_pred = regrsvm.predict(X_train_final)","5792fb80":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_train,y_train_svm_pred))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_train,y_train_svm_pred))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_train,y_train_svm_pred)))","2ebb74b1":"from sklearn import metrics\n##MAE\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,y_pred_svm))\n##MSE\nprint(\"MSE:\",metrics.mean_squared_error(y_test,y_pred_svm))\n##RMSE\nprint(\"RMSE:\",np.sqrt(metrics.mean_absolute_error(y_test,y_pred_svm)))","7c543981":"submission = pd.DataFrame({\n        \"Id\": X_test_final[\"Id\"],\n        \"SalePrice\": y_pred_dta\n    })\nsubmission.to_csv('submission.csv', index=False)","d7d54347":"submission.head()","f3018d99":"** Data Viz**","4ce4cc67":"**Data Preprocessing:**\n\nData Cleaning: Identifying and correcting mistakes or errors in the data.\n\nFeature Selection: Identifying those input variables that are most relevant to the task.\n\nData Transforms: Changing the scale or distribution of variables.\n\nFeature Engineering: Deriving new variables from available data.\n\nDimensionality Reduction: Creating compact projections of the data.","68780821":"**Using Decision Tree will be the best apporach given the lowest RMSE and its not generating Overfitting\/Underfitting Problems.**","fba78038":"**One hot encoder**","728c63d9":"Target Varaible = \"SalePrice\" ","b2472e40":"**Min-Max Scaler**","e49dc536":"General Rule\n\nUnderfitting models: In general High Train RMSE, High Test RMSE.\n\nOverfitting models: In general Low Train RMSE, High Test RMSE.","5e0a431d":"**Model Building**\n\nLinear Regression\n\nRidge Regression\n\nLasso Regression\n\nKNN\n\nDecision Tree Algorithm\n\nRandom Regression\n\nSVM","1b134817":"Statistical imputation transform for the horse colic dataset\n\nfrom numpy import isnan\nfrom pandas import read_csv\nfrom sklearn.impute import SimpleImputer","016acf9a":"**Merging Data For Furhter Analysis**"}}