{"cell_type":{"3a26c330":"code","68024eeb":"code","9b078319":"code","7610ff88":"code","4ae4687d":"code","e7642d74":"markdown","e85de42d":"markdown"},"source":{"3a26c330":"import os, json\nimport pandas as pd, numpy as np\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport pytorch_lightning as pl\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom datasets import Dataset, DatasetDict\n\nfrom tqdm import tqdm\ntqdm.pandas()","68024eeb":"class DiscourseDataModule(pl.LightningDataModule):\n    def __init__(self, mname, train_fl, dev_fl, test_fl):\n        super().__init__()\n        self.mname = mname\n        self.tokenizer = AutoTokenizer.from_pretrained(mname, add_prefix_space=True)\n        self.train_fl, self.validation_fl, self.test_fl = train_fl, dev_fl, test_fl\n        self.train_batch_size = 8\n        self.eval_batch_size = 32\n        \n    def load_dataset(self, fl):\n        all_edus = []\n        with open(fl) as f:\n            dat = json.load(f)\n            for k in range(len(dat)):\n                all_edus.append(dat[str(k)])\n        return Dataset.from_dict({\"words\": all_edus})\n        \n    def setup(self, stage: str):\n        self.dataset = DatasetDict({\"train\": self.load_dataset(self.train_fl), \\\n                                    \"validation\": self.load_dataset(self.validation_fl), \\\n                                    \"test\": self.load_dataset(self.test_fl)\n                                   })\n\n        for split in self.dataset.keys():\n            self.dataset[split] = self.dataset[split].map(\n                self.convert_to_features,\n                batched=True,\n                remove_columns=['words']\n            )\n            self.dataset[split].set_format(type=\"torch\")\n        self.eval_splits = [x for x in self.dataset.keys() if \"validation\" in x]\n\n    def prepare_data(self):\n        AutoTokenizer.from_pretrained(self.mname, add_prefix_space=True)\n\n    def train_dataloader(self):\n        return DataLoader(self.dataset[\"train\"], batch_size=self.train_batch_size)\n\n    def val_dataloader(self):\n        if len(self.eval_splits) == 1:\n            return DataLoader(self.dataset[\"validation\"], batch_size=self.eval_batch_size)\n        elif len(self.eval_splits) > 1:\n            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n\n    def test_dataloader(self):\n        if len(self.eval_splits) == 1:\n            return DataLoader(self.dataset[\"test\"], batch_size=self.eval_batch_size)\n        elif len(self.eval_splits) > 1:\n            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n\n    def convert_to_features(self, example_batch):\n        features = self.tokenizer(example_batch[\"words\"], is_split_into_words=True, \\\n                                  padding=True, truncation=False, add_special_tokens=False)\n        tot_feats, tot_width = len(features[\"attention_mask\"]), len(features[\"attention_mask\"][0])\n        labels = [[0 for _ in range(tot_width)] for _ in range(tot_feats)]\n        \n        for ft_ix in range(tot_feats):\n            word_ids = features.word_ids(batch_index=ft_ix)\n            #Last tokens of each discourse\n            word_ids = word_ids[::-1]\n            tot_words = len(example_batch[\"words\"][ft_ix])\n            \n            for w_ix in range(tot_words-1): #Ignore last word. We have no business segmenting it.\n                ix = len(word_ids)-word_ids.index(w_ix)-1\n                labels[ft_ix][ix] = 1\n        features[\"labels\"] = labels\n        return features","9b078319":"BATCH_SIZE, N_EPOCHS = 32, 20\nWEIGHT_DECAY, LR, ADAM_EPS = 0., 1e-5, 1e-8\npred_labels = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nloss_weights = torch.Tensor([0.15, 0.85]).to(device)\n\nclass ShallowSegmentor(pl.LightningModule):\n    def __init__(self, mname, num_layers=6, is_bart=False):\n        super().__init__()\n        model = AutoModel.from_pretrained(mname)\n        if is_bart: #Bart-base ENCODER ONLY FRAMEWORK\n            model.encoder.layers = model.encoder.layers[:num_layers]\n            self.model = model.encoder\n        else: #Roberta-base\n            model.encoder.layer = torch.nn.ModuleList([layer for layer in model.encoder.layer[:num_layers]])\n            self.model = model\n        self.linear = nn.Linear(self.model.config.hidden_size, 1)\n        self.dropout = nn.Dropout(0.2)\n        self.sigmoid = nn.Sigmoid()\n        self.epoch_tracker = 0\n\n    def forward(self, ip, mask_src):\n        #Why is this frozen?!\n        #with torch.no_grad():\n        top_vec = self.model(ip, attention_mask=mask_src)\n        top_vec = top_vec[0]\n        vec = self.dropout(top_vec)\n        vec = self.linear(vec).squeeze()\n        return self.sigmoid(vec) * mask_src\n\n    def training_step(self, batch, batch_idx):\n        src, label, mask_src = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n        output = self.forward(src, mask_src)\n        \n        batch_weights = torch.gather(loss_weights.unsqueeze(0).repeat_interleave(len(label), dim=0), -1, label)\n        loss_fn = torch.nn.BCELoss(weight=batch_weights, reduction='sum')\n        loss = loss_fn(output, label.float())\n        self.log(\"train\/loss\", loss.item())\n        return loss\n    \n    def _transform_predictions(self, tensor, mask_src):\n        results = []\n        array = tensor.data.cpu().numpy()\n        m = mask_src.sum(dim=-1).data.cpu().numpy()\n        for idx in range(len(array)):\n            cur_arr = array[idx][:m[idx]]\n            now=0\n            result=[]\n            for idy in range(len(cur_arr)):\n                if cur_arr[idy]==1:\n                    result.append((now, idy))\n                    now=idy\n                if idy == len(cur_arr)-1 and now!=idy:\n                    result.append((now, idy))\n            results.append(result)\n        return results\n        \n    def validation_step(self, batch, batch_idx):\n        src, label, mask_src = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n        output = self.forward(src, mask_src)\n\n        batch_weights = torch.gather(loss_weights.unsqueeze(0).repeat_interleave(len(label), dim=0), -1, label)\n        loss_fn = torch.nn.BCELoss(weight=batch_weights, reduction='sum')\n        loss = loss_fn(output, label.float())\n        self.log(\"val\/loss\", loss.item())\n        \n        output = (output > 0.5) + 0\n        predictions = self._transform_predictions(output, mask_src)\n        answers = self._transform_predictions(label, mask_src)\n        f1s = []\n        for prediction, answer in zip(predictions, answers):\n            count = 0\n            for item in prediction:\n                if item in answer:\n                    count+=1\n            #F1 is either overlap fraction\n            #or 1 if both pred and answers are empty lists\n            #or 0 if answer is  empty, but preds is not.\n            f1 = float(2.0*count\/(len(prediction)+len(answer))) if  bool(prediction or answer) else float(not bool(prediction))\n            f1s.append(f1)\n        return answers, predictions, f1s\n\n    def validation_epoch_end(self, validation_step_outputs):\n        global pred_labels\n        mean_f1s = 0.\n        tot_f1s = 0\n        for _, _, f1s in validation_step_outputs:\n            mean_f1s += sum(f1s)\n            tot_f1s += len(f1s)\n        mean_f1 = mean_f1s\/tot_f1s\n        self.log(\"val\/f1\", mean_f1)\n        \n        #Show first 10 elements of first batch only\n        \n        answers, preds, _ = validation_step_outputs[0]\n        answers, preds = answers[:10], preds[:10]\n        pred_labels.extend([(self.epoch_tracker, str(gt), str(pr)) for gt, pr in zip(answers, preds)])\n        self.epoch_tracker += 1\n    \n    def configure_optimizers(self):\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        t_total = N_EPOCHS * len(gdm_data.dataset[\"train\"]) \/\/ BATCH_SIZE\n        warmup_steps = int(0.1 * t_total)\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": WEIGHT_DECAY,\n            },\n            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=ADAM_EPS)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n        )\n        return [optimizer], [scheduler]","7610ff88":"from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nwandbkey = user_secrets.get_secret(\"wandbkey\")\nwandb.login(key=wandbkey)\n\nmodel_names = {\"bartb\": \"facebook\/bart-base\", \"robb\": \"roberta-base\"}\nAVAIL_GPUS = min(1, torch.cuda.device_count())","4ae4687d":"#TODO: Save every model Done\n#TODO: Grad clip norm Done\n#TODO: Wandb runs per train Done\n#TODO: Eval scores Done (Sorta?!)\nfor mkey, mname in model_names.items():\n    for num_layers in range(1, 7):\n        local_name = f\"{mkey}_{num_layers}layers\"\n        wandb_logger = WandbLogger(name=local_name, project=\"kaggle_feedback\", log_model=True, \\\n                                   notes=\"jdoesv\/edu-segmentation-models-on-gum; V11\", \n                                  tags=[\"gum\", \"disc_segment\", \"smaller_layers\"])\n        \n        checkpoint_callback = ModelCheckpoint(monitor='val\/loss', dirpath=local_name, \\\n                                              filename='epoch{epoch:02d}-val_loss{val\/loss:.2f}',\n                                              auto_insert_metric_name=False,\n                                              #every_n_epochs=1,\n                                              #save_top_k=N_EPOCHS\n                                             )\n        early_stop_callback = EarlyStopping(monitor=\"val\/f1\", patience=5, verbose=False, mode=\"max\")\n\n        pl.seed_everything(208)\n        \n        gdm_data = DiscourseDataModule(mname, \\\n                          \"..\/input\/edu-disrpt-2021-datasets-gum\/gum_train_edus.json\", \\\n                         \"..\/input\/edu-disrpt-2021-datasets-gum\/gum_dev_edus.json\", \\\n                         \"..\/input\/edu-disrpt-2021-datasets-gum\/gum_test_edus.json\")\n        gdm_data.prepare_data()\n        gdm_data.setup(\"fit\")\n        \n        \n        segmentor = ShallowSegmentor(mname, num_layers=num_layers, is_bart=\"bart\" in mkey) \n        wandb_logger.watch(segmentor)\n        trainer = pl.Trainer(logger=wandb_logger, \\\n                             max_epochs=N_EPOCHS, \\\n                             gpus=AVAIL_GPUS, \\\n                             callbacks=[checkpoint_callback, early_stop_callback], \\\n                             gradient_clip_val=1.)\n        trainer.fit(segmentor, gdm_data)\n        \n        columns = [\"epoch_num\", \"gt\", \"pred\"]\n        wandb_logger.log_text(key=\"val\/samples\", columns=columns, data=pred_labels)\n        pred_labels = []\n        wandb.finish()    \n","e7642d74":"# Utils\nHyperparams from [code](https:\/\/github.com\/fajri91\/discourse_probing\/tree\/3f8c89c18a4eb217820667116ec17f6cec9b7e12\/segment). Code mostly incorrect. Fetch hyperparams and params from [paper](https:\/\/aclanthology.org\/2021.naacl-main.301.pdf) \n\nGUM only first","e85de42d":"```python\nimport json\nfrom tqdm import tqdm\nwith open(\"..\/input\/edu-disrpt-2021-datasets-gum\/gum_train_edus.json\") as f:\n    dat = json.load(f)\ntot_edus = 0\ntot_words = 0\nfor key in tqdm(range(len(dat))):\n    tot_edus += len(dat[str(key)])\n    tot_words += sum([True for edu in dat[str(key)] for word in edu.split()])\ntot_edus\/tot_words\n```\n\n0 wt: 0.14986802621410866\n1 wt: 1 - 0 wt"}}