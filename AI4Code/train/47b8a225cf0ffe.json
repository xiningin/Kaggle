{"cell_type":{"ba2a9cae":"code","cbc372d8":"code","e2e1ed71":"code","f60073bb":"code","da716bdc":"code","63a88105":"code","d38743a3":"code","20e0041f":"code","1c3af13d":"code","b93ec163":"code","302eda2b":"code","f8568a94":"code","0ebf0d28":"code","23a6b2bd":"code","fb5b0c32":"code","688ddd2b":"code","9b39e375":"markdown","3dc8bcd6":"markdown","766a485c":"markdown","35e509c6":"markdown","dd448877":"markdown"},"source":{"ba2a9cae":"!pip install ..\/input\/kerasapplications\/keras-team-keras-applications-3b180cb -f .\/ --no-index >\/dev\/null","cbc372d8":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf \nimport matplotlib.pyplot as plt \nfrom tqdm.notebook import tqdm \nfrom tensorflow.keras.layers import (\n    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, Add, Conv2D, AveragePooling2D, \n    LeakyReLU, Concatenate \n)\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import Sequence\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.applications as tfka\n\nfrom sklearn.model_selection import train_test_split, KFold\nimport seaborn as sns\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.optimizers import AdamW","e2e1ed71":"# Future TPU support\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    GPU=False\nexcept ValueError:\n    print(\"Not connected to a TPU runtime. Using CPU\/GPU strategy\")\n    strategy = tf.distribute.MirroredStrategy()\n    GPU=True","f60073bb":"EPOCHS = 10 # number of epochs to train for in each fold\n\nBATCH_SIZE = 4 * strategy.num_replicas_in_sync # batch size of images during training\n\nprint('Batch size = '+str(BATCH_SIZE))\n\nIM_SIZE = 512 #If you are training efficientnet please read https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n\n\n\nBASE_MODEL='EfficientNetB1' # supported models are listed here https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\n\n# args to pass to base model e.g. for EfficientNet {drop_connect_rate=0.4} default is 0.2\nBASE_MODEL_KWARGS = dict(drop_connect_rate=0.2,input_shape=(IM_SIZE,IM_SIZE,1),include_top=False,weights=None)\n\nNFOLD = 5 # number of folds in K-fold cross-validation (CV)\n\n\n\nSTART_LR = 1e-2 * strategy.num_replicas_in_sync # the initial learning rate \nFIRST_DECAY_STEPS = 1e1\n\nLR_STRATEGY='CosineDecayRestarts' # check for options in the cell below\nLR_KWARGS=dict(initial_learning_rate=START_LR, first_decay_steps=FIRST_DECAY_STEPS)\n\nWD = 1e-2 #Weight decay\nOPTIMIZER='AdamW' # check for options in the cell below\nOPTIMIZER_KWARGS=dict(weight_decay=WD)\n\nSAVE_BEST = True # default is True to save best weights on validation loss\n\nMIXED_PRECISION = True\n\n\nEALRY_STOPPING_PATIENCE = 40\n\nTRAIN_STEPS = int(( 8 \/ BATCH_SIZE) * 32) # number of steps per epoch in training\n\nVAL_STEPS = int((8 \/ BATCH_SIZE  ) * 16) # number of steps per epoch in validation\n\n\n\nDATA_DIR=[\"..\/input\/osic-pulmonary-fibrosis-progression\",\"..\/input\/osic-pulmonary-fibrosis-progression-lungs-mask\"]\n\n\n\nimport datetime\n\ndef get_now_string():\n  x = str(datetime.datetime.now())\n  date=x[:10].split('-')\n  time=x[11:].split(':')\n  time[2]=time[2].split('.')[0]\n  dates=\"\".join(date)+\"_\"+\"\".join(time)\n  return dates\n\nOUTPUT_DIR=get_now_string()+\"\/\"\nos.mkdir(OUTPUT_DIR)\nprint(\"saving models to :\",OUTPUT_DIR)","da716bdc":"# Supported optimizers\n\noptimizers_mapper={\n    \"AdamW\":tfa.optimizers,\n    \"ConditionalGradient\":tfa.optimizers,\n    \"LAMB\":tfa.optimizers,\n    \"LazyAdam\":tfa.optimizers,\n    \"NovoGrad\":tfa.optimizers,\n    \"RectifiedAdam\":tfa.optimizers,\n    \"SGDW\":tfa.optimizers,\n    \"SWA\":tfa.optimizers,\n    \"Yogi\":tfa.optimizers,\n    \"Adadelta\":tf.keras.optimizers,\n    \"Adagrad\":tf.keras.optimizers,\n    \"Adam\":tf.keras.optimizers,\n    \"Adamax\":tf.keras.optimizers,\n    \"Ftrl\":tf.keras.optimizers,\n    \"Nadam\":tf.keras.optimizers,\n    \"RMSprop\":tf.keras.optimizers,\n    \"SGD\":tf.keras.optimizers\n       }\n\n# Supported Learning rates schedules\nschedules_mapper={\n    \"CyclicalLearningRate\":tfa.optimizers,\n    \"ExponentialCyclicalLearningRate\":tfa.optimizers,\n    \"Triangular2CyclicalLearningRate\":tfa.optimizers,\n    \"TriangularCyclicalLearningRate\":tfa.optimizers,\n    \"ExponentialDecay\":tf.keras.optimizers.schedules,\n    \"InverseTimeDecay\":tf.keras.optimizers.schedules,\n    \"PiecewiseConstantDecay\":tf.keras.optimizers.schedules,\n    \"PolynomialDecay\":tf.keras.optimizers.schedules,\n    \"CosineDecay\":tf.keras.experimental,\n    \"CosineDecayRestarts\":tf.keras.experimental,\n    \"LinearCosineDecay\":tf.keras.experimental,\n    \"NoisyLinearCosineDecay\":tf.keras.experimental,\n    \n       }","63a88105":"train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv') ","d38743a3":"train.head()","20e0041f":"train.SmokingStatus.unique()","1c3af13d":"def get_tab(df):\n    vector = [(df.Age.values[0] - 30) \/ 30] \n    \n    if df.Sex.values[0] == 'male':\n       vector.append(0)\n    else:\n       vector.append(1)\n    \n    if df.SmokingStatus.values[0] == 'Never smoked':\n        vector.extend([0,0])\n    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n        vector.extend([1,1])\n    elif df.SmokingStatus.values[0] == 'Currently smokes':\n        vector.extend([0,1])\n    else:\n        vector.extend([1,0])\n    return np.array(vector) ","b93ec163":"A = {} \nTAB = {} \nP = [] \nfor i, p in tqdm(enumerate(train.Patient.unique())):\n    sub = train.loc[train.Patient == p, :] \n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    c = np.vstack([weeks, np.ones(len(weeks))]).T\n    a, b = np.linalg.lstsq(c, fvc)[0]\n    \n    A[p] = a\n    TAB[p] = get_tab(sub)\n    P.append(p)","302eda2b":"def get_img(path):\n    d = pydicom.dcmread(path)\n    return cv2.resize((d.pixel_array - d.RescaleIntercept) \/ (d.RescaleSlope * 1000), (IM_SIZE, IM_SIZE))","f8568a94":"x, y = [], []\nfor p in tqdm(train.Patient.unique()):\n    try:\n        ldir = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression-lungs-mask\/mask_noise\/mask_noise\/{p}\/')\n        numb = [float(i[:-4]) for i in ldir]\n        for i in ldir:\n            x.append(cv2.imread(f'..\/input\/osic-pulmonary-fibrosis-progression-lungs-mask\/mask_noise\/mask_noise\/{p}\/{i}', 0).mean())\n            y.append(float(i[:-4]) \/ max(numb))\n    except:\n        pass","0ebf0d28":"class IGenerator(Sequence):\n    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n    def __init__(self, keys, a, tab, batch_size=BATCH_SIZE):\n        self.keys = [k for k in keys if k not in self.BAD_ID]\n        self.a = a\n        self.tab = tab\n        self.batch_size = batch_size\n        \n        self.train_data = {}\n        for p in train.Patient.values:\n            self.train_data[p] = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/')\n    \n    def __len__(self):\n        return 1000\n    \n    def __getitem__(self, idx):\n        x = []\n        a, tab = [], [] \n        keys = np.random.choice(self.keys, size = self.batch_size)\n        for k in keys:\n            try:\n                i = np.random.choice(self.train_data[k], size=1)[0]\n                img = get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{k}\/{i}')\n                x.append(img)\n                a.append(self.a[k])\n                tab.append(self.tab[k])\n            except:\n                print(k, i)\n       \n        x,a,tab = np.array(x), np.array(a), np.array(tab)\n        x = np.expand_dims(x, axis=-1)\n        return [x, tab] , a","23a6b2bd":"def build_model(base_model=BASE_MODEL,**kwargs):\n\n    inp = Input(shape=kwargs['input_shape'])\n    del kwargs['input_shape']\n    base =  getattr(tfka, base_model)(**kwargs) \n    x = base(inp)\n    x = GlobalAveragePooling2D()(x)\n    inp2 = Input(shape=(4,))\n    x2 = tf.keras.layers.GaussianNoise(0.2)(inp2)\n    x = Concatenate()([x, x2]) \n    x = Dropout(0.5)(x)\n    \n    # Explicit f32 because of mixed precision \n    x = Dense(1,dtype='float32')(x)\n    \n    model = Model([inp, inp2] , x)\n    return model","fb5b0c32":"\nif GPU:\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    session = tf.compat.v1.Session(config=config)\n    if MIXED_PRECISION:\n      policy = mixed_precision.Policy('mixed_float16')\n      \n      mixed_precision.set_policy(policy)\n\n        \nkf = KFold(n_splits=NFOLD, random_state=42,shuffle=False)\nP = np.array(P)\nsubs = []\nfolds_history = []\nwith strategy.scope():\n\n    for fold, (tr_idx, val_idx) in enumerate(kf.split(P)):\n        print('#####################')\n        print('####### Fold %i ######'%fold)\n        print('#####################')\n        \n        print('Training...')\n\n        er = tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_loss\",\n            min_delta=1e-3,\n            patience=EALRY_STOPPING_PATIENCE,\n            verbose=1,\n            mode=\"auto\",\n            baseline=None,\n            restore_best_weights=True,\n        )\n\n        cpt = tf.keras.callbacks.ModelCheckpoint(\n            filepath=f'{OUTPUT_DIR}fold-%i.h5'%fold,\n            monitor='val_loss', \n            verbose=1, \n            save_best_only=SAVE_BEST,\n            mode='auto'\n        )\n        \n        lrs=getattr(schedules_mapper[LR_STRATEGY], LR_STRATEGY)(**LR_KWARGS)\n        lrs_call_back=tf.keras.callbacks.LearningRateScheduler(lrs)\n        callbacks =[lrs_call_back,cpt,er]\n\n        optimizer=getattr(optimizers_mapper[OPTIMIZER], OPTIMIZER)(**OPTIMIZER_KWARGS)\n        #Loss scaling for GPU\n        if MIXED_PRECISION:\n            optimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')\n\n        model = build_model(**BASE_MODEL_KWARGS)\n        model.compile(optimizer=optimizer, loss=\"mae\") \n        history = model.fit(IGenerator(keys=P[tr_idx], \n                                       a = A, \n                                       tab = TAB), \n                            steps_per_epoch = TRAIN_STEPS,\n                            validation_data=IGenerator(keys=P[val_idx], \n                                       a = A, \n                                       tab = TAB),\n                            validation_steps = VAL_STEPS, \n                            callbacks = callbacks, \n                            epochs=EPOCHS)\n        folds_history.append(history.history)\n        print('Training done!')","688ddd2b":"if SAVE_BEST:\n    mean_val_loss = np.mean([np.min(h['val_loss']) for h in folds_history])\nelse:\n    mean_val_loss = np.mean([h['val_loss'][-1] for h in folds_history])\nprint('Our mean CV MAE is: ' + str(mean_val_loss))","9b39e375":"# Training Parameters\n\n","3dc8bcd6":"# CV Evaluation","766a485c":"# Training","35e509c6":"\n# Future Work\n\nPlease suggest in the comments\n\n# References\n\n[1] Michael Kazachok's Linear Decay (based on ResNet CNN)\n     Model that uses images can be found at: https:\/\/www.kaggle.com\/miklgr500\/linear-decay-based-on-resnet-cnn\n     \n[2] Ulrich GOUE's Osic-Multiple-Quantile-Regression-Starter\n     Model that uses tabular data can be found at: https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter\n\n[3] Wei Hao Khoong's K-Fold TF-EfficientNet Models (Training) https:\/\/www.kaggle.com\/khoongweihao\/k-fold-tf-efficientnet-models-training\n\n[4] Jeremy Howard's AMAZING Fast.ai course https:\/\/docs.fast.ai\/\n\n[5] Yixing Fu's awesome guide \"Image classification via fine-tuning with EfficientNet\" https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n\n","dd448877":"# Overview\n\nBased on Wei Hao's awesome training pipeline [3]\n\n# Features\n \n- GPU with optional Mixed precision support\n- TPU support *IN PROGRESS*\n- Wide range of supported backend models via Keras Applications models\n- Wide range of supported optimizers in Tensorflow optimizers and Tensorflow Addons\n- Wide range of supported learning rate scheduling strategies via Tensorflow, Tensorflow experimental and Tensorflow Addons\n- RandAug augmentation support with UNetAug added as a custom aug strategy *IN PROGRESS*\n- Overall cleanup and parameterization of the pipeline\n\n\n\nBelow you can train efnB7 relatively fast ~18 seconds per epoch with 600 image size which makes this notebook able to train large models for more epochs in the 9 hour limit.\n\nDon't forget to turn on the GPU or TPU :)\n"}}