{"cell_type":{"b2af7f8f":"code","8f4ea732":"code","078d3e89":"code","b46d3622":"code","c76f7f46":"code","09560290":"code","7d8e0724":"code","0ee90cb8":"code","aee94ced":"code","0525855b":"code","de8f8a84":"code","d0ef256e":"code","db46722d":"code","31dfccce":"code","680a75eb":"code","4c6d7f7e":"code","a3fc1934":"code","a34873b5":"code","37c64a8b":"code","0c452e37":"code","c04bec31":"code","bd7b7c1b":"code","4c7ee217":"code","b3793792":"code","7bf2fb2c":"code","94eecd2a":"code","712afa4d":"code","7e536570":"code","3f4fead2":"code","5626235d":"code","e64bb09c":"markdown","67850cb1":"markdown","30a2bf33":"markdown","e25a7435":"markdown","f9c57e1a":"markdown","1956523f":"markdown","7998aa78":"markdown","544a8dff":"markdown","5886571d":"markdown","f2dc9c67":"markdown","9df5d325":"markdown","4630805d":"markdown","a68bd55a":"markdown","0c68d2c8":"markdown","3491b7bf":"markdown","6663ffd4":"markdown","aad862f1":"markdown","b2a5b13f":"markdown","fd228dab":"markdown","4ac6eb64":"markdown","74079c26":"markdown","8c35a7c5":"markdown","86c93a62":"markdown","fb267efd":"markdown","44f49daa":"markdown","83f392f5":"markdown","9ef48f4b":"markdown","b0f2a96c":"markdown","2a0e4caf":"markdown"},"source":{"b2af7f8f":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab","8f4ea732":"diabetes_df=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndiabetes_df.head(10)","078d3e89":"diabetes_df.describe()","b46d3622":"#Checking for Null Values in each Feature\ndiabetes_df.isnull().sum()","c76f7f46":"#Drawing a histogram of each feature\ndef draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='deepskyblue')\n        ax.set_title(feature+\" Distribution\",color = 'black')\n        \n    fig.tight_layout()\n    plt.show()\n\ndraw_histograms(diabetes_df, diabetes_df.columns,4,3)","09560290":"#Checking the outcome counts\ndiabetes_df.Outcome.value_counts()","7d8e0724":"sn.countplot(x='Outcome', data=diabetes_df)","0ee90cb8":"#Looking for correlation between the different features\nsn.pairplot(data=diabetes_df)","aee94ced":"plt.figure(figsize=(12,10))\nsn.heatmap(diabetes_df.corr(), annot=True,cmap ='coolwarm', vmax=.6)","0525855b":"import statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Get variables for which to compute VIF and add intercept term\nX = diabetes_df[['Pregnancies', 'Glucose', 'BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']]\nX['Intercept'] = 1\n\n# Compute and view VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n# View results using print\nprint(vif[0:-1])","de8f8a84":"#adding a constant value to the dataframe. This will not influence the accuracy of our model.\nfrom statsmodels.tools import add_constant as add_constant\ndiabetes_df_constant = add_constant(diabetes_df)","d0ef256e":"#running a logistic regression model in order to look at the p values of each feature\nimport scipy.stats as st\nst.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\ncols=diabetes_df_constant.columns[:-1]\nmodel = sm.Logit(diabetes_df.Outcome,diabetes_df_constant[cols])\nresult = model.fit()\nresult.summary()","db46722d":"#dropping the SkinThickness feature\ndiabetes_df_drop = diabetes_df_constant.drop(['SkinThickness'], axis=1)","31dfccce":"#Replacing the 0 values of the Glucose, Insulin, BMI, and BloodPressure features with their median values\nmedian_glucose = diabetes_df_drop['Glucose'].median(skipna=True)\nmedian_Insulin = diabetes_df_drop['Insulin'].median(skipna=True)\nmedian_BMI = diabetes_df_drop['BMI'].median(skipna=True)\nmedian_bp = diabetes_df_drop['BloodPressure'].median(skipna=True)\ndiabetes_df_drop['Glucose']=diabetes_df_drop.Glucose.mask(diabetes_df_drop.Glucose == 0,median_glucose)\ndiabetes_df_drop['Insulin']=diabetes_df_drop.Insulin.mask(diabetes_df_drop.Insulin == 0,median_Insulin)\ndiabetes_df_drop['BMI']=diabetes_df_drop.BMI.mask(diabetes_df_drop.BMI == 0,median_BMI)\ndiabetes_df_drop['BloodPressure']=diabetes_df_drop.BloodPressure.mask(diabetes_df_drop.BloodPressure == 0,median_bp)","680a75eb":"#Feature histograms after replacing missing data\ndraw_histograms(diabetes_df_drop, diabetes_df_drop.columns,4,3)","4c6d7f7e":"st.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\ncols=diabetes_df_drop.columns[:-1]\nmodel = sm.Logit(diabetes_df.Outcome,diabetes_df_drop[cols])\nresult = model.fit()\nresult.summary()","a3fc1934":"def back_feature_elem (data_frame,dep_var,col_list):\n    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n    P-value above alpha one at a time and returns the regression summary with all p-values below alpha\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)","a34873b5":"#using feature elimination function given above\nresult=back_feature_elem(diabetes_df_drop,diabetes_df_drop.Outcome,cols)","37c64a8b":"result.summary()","0c452e37":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))","c04bec31":"import sklearn\nnew_features = diabetes_df[['Pregnancies','Glucose','BMI','DiabetesPedigreeFunction','Outcome']]","bd7b7c1b":"#Creating the training and test sets\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)","4c7ee217":"#Running our machine learning model\nfrom sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","b3793792":"#Our Accuracy score\nsklearn.metrics.accuracy_score(y_test,y_pred)","7bf2fb2c":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nhm = sn.heatmap(conf_matrix, annot=True,fmt='d',cmap='coolwarm',vmax=50, cbar=False)\nhm.set_title(\"Model Confusion Matrix\")\nhm","94eecd2a":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","712afa4d":"print('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) = ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy = ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) = ',TP\/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) = ',TN\/float(TN+FP),'\\n',\n\n'Positive predictive value = TP\/(TP+FP) = ',TP\/float(TP+FP),'\\n',\n\n'Negative predictive Value = TN\/(TN+FN) = ',TN\/float(TN+FN),'\\n',\n\n'Positive likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity\/(1-specificity),'\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity)\/specificity)","7e536570":"from sklearn.preprocessing import binarize\nfor i in range(1,5):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')","3f4fead2":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Diabetes classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","5626235d":"sklearn.metrics.roc_auc_score(y_test,y_pred_prob_yes[:,1])","e64bb09c":"# 1. Introduction","67850cb1":"# Pima Indians: Logistic regression, handling missing data, feature reduction, and interpretations and insights from the model.","30a2bf33":"After cleaning the data, some of our P values are still high. We will now use recursive feature elimination. With this technique, we will test the importance of each feature, remove the feature with the lowest performance, then run the test again untill all of our features p values fall below our alpha of 0.05.","e25a7435":"After running recursive feature elimination, we removed the Insulin, BloodPressure, and Age features. We are left with the Pregnancies, Glucose, BMI, and DiabetesPedigreeFunction features.","f9c57e1a":"## The Features","1956523f":"A big thanks to my partner on this project Jordan King for all of his help with concepts of logistic regression and interpretations of the data. \n\nWe took a lot of inspiration from the kaggle user \"Nisha\" and her approach to interpreting logistic regression models on her \"Heart Disease and Prediction using Logistic Regression\" project found here: https:\/\/www.kaggle.com\/neisha\/heart-disease-prediction-using-logistic-regression.","7998aa78":"# 2. Data Exploration and Cleaning","544a8dff":"## Interpreting the values of model's coefficients","5886571d":"## Credits","f2dc9c67":"# Conclusion","9df5d325":"There are 500 without diabetes and 268 with diabetes.","4630805d":"In logistic regression, colinear features can reduce the accuracy of the estimate coefficients for the features in the model. There appears to be some correlation between the different features. Several features will likely be removed. A good way to check for colinearity is to look at the VIF (variance inflation factor) values of each feature. If a feater has a VIF value greater than 5, it is safe to remove it.","a68bd55a":"## Exploring the data","0c68d2c8":"So we have no null values.","3491b7bf":"In this notebook we will create a machine learning model with logistic regression, use a feature reduction technique to eleminate less useful features, interpret the coefficients of each feature, and interpret the outcomes of our machine learning model. ","6663ffd4":"## The Data Set","aad862f1":"Time to clean the data. We will assume that a value of 0 means that the value is actually missing, and was defaulted to 0. We need to drop the SkinThickness Feature because there is too much missing data, and we need to replace the 0(missing) values for the glucose, insulin, BMI, and blood pressure features with the median value.","b2a5b13f":"Our model shows that, holding all other reatures constant, the odds of getting diagnosed with diabetes for females 21+ years old:\n\n1. For every pregnancy the woman has, her odds of being diagnosed with diabetes increases by <b>15.4%<\/b>.\n2. For every 1 mg\/dL increase in Glucose, her odds of being diagnosed with diabetes increased by <b>3.76%<\/b>.\n3. For every unit increase in BMI, her odds of being diagnosed with diabetes increases by <b>9.28%<\/b>.\n4. For every unit increase in the Diabetes Pedigree Function, her odds of being diagnosed with diabetes increases by <b>241.55%<\/b>\n","fd228dab":"Our logistic regression model predicted correctly 81% of the time, and our AUC score was 87%. We are pretty happy with those results. We were also able to get some interesting information about the likelyhood of our different features affecting a diabetes diagnosis.","4ac6eb64":"## Training our machine learning model","74079c26":"## Cleaning the Data","8c35a7c5":"Our model predicted correctly 81% of the time.","86c93a62":"None of our VIF values are large enough to drop a feature. Instead, we will clean up the missing values in our data, and then use a technique called Recursive feature elimination to decide which features to train our model on.","fb267efd":"# 3. Building the Model","44f49daa":"True Positives: 35\nTrue Negatives: 90\nFalse Positives: 10\nFalse Negatives: 19","83f392f5":"This is a data set from the National Institute of Diabetes and Digestive and Kidney Diseases. All patients here are females at least 21 years old of pima indian heritage. ","9ef48f4b":"<b>Pregnancies<\/b>: Number of times pregnant\n\n<b>Glucose<\/b>: Plasma glucose concentration after 2 hours in an oral glucose tolerance test\n\n<b>BloodPressure<\/b>: Diastolic blood pressure (mm Hg)\n\n<b>SkinThickness<\/b>: Triceps skin fold thickness (mm)\n\n<b>Insulin<\/b>: 2-Hour serum insulin (mu U\/ml)\n\n<b>BMI<\/b>: Body mass index (weight in kg\/(height in m)^2)\n\n<b>DiabetesPedigreeFunction<\/b>: Diabetes pedigree function\n\n<b>Age<\/b>: Age (years)\n\n<b>Outcome<\/b>: Class variable (0 or 1) 268 of 768 are 1, the others are 0","b0f2a96c":"They seem to have replaced missing data with the number 0 in several categores: Glucose, BloodPressure, SkinThickness, Insulin, BMI.","2a0e4caf":"<b>Sensitivity<\/b> is the true positive rate.\n\n<b>Specificity<\/b> is the true negative rate.\n\n<b>Positive predictive value<\/b> is the probability that somebody predicted to be positive with diabetes actually has diabetes.\n\n<b>Negative predictive value<\/b> is the probability that somebody predicted to be negative with diabetes, doesn't actually have diabetes.\n\n<b>Positive likelihood ratio<\/b> gives the change in odds of having a diagnosis in women with a positive test. Our positive likelihood ratio indicates a 6.9 fold increase in the odds of having diabetes in a woman with a positive test result.\n\n<b>Negative likelihood ratio<\/b> gives the change in the odds of having a diagnosis in women with a negative test. Our negative likelihood ratio indicates a 2.6 fold decrease in the odds of having diabetes in a woman with a negative test result.\n\ncitation: https:\/\/www.uws.edu\/wp-content\/uploads\/2013\/10\/Likelihood_Ratios.pdf\n"}}