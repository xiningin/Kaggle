{"cell_type":{"8d2c805b":"code","2613ff1c":"code","feae4a33":"code","0f6e517e":"code","25c9d5b1":"code","17df1249":"code","52c25bad":"code","244bfc8e":"code","e5ee8332":"code","0498c1eb":"code","b9d7ae36":"code","072f955f":"markdown","d80871b2":"markdown","e9393240":"markdown","3e8e6512":"markdown","16a4aece":"markdown","fc002491":"markdown","67fb710e":"markdown","35276543":"markdown","464517bb":"markdown","af6e9329":"markdown","5bda04bc":"markdown","7be90d76":"markdown","eab50531":"markdown","e8742c01":"markdown"},"source":{"8d2c805b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n!pip install seqeval\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.keras import backend as K\nfrom seqeval.metrics import accuracy_score\nfrom seqeval.metrics import f1_score\nfrom seqeval.metrics import precision_score\nfrom seqeval.metrics import recall_score\nfrom seqeval.metrics import classification_report as seqeval_cs\nfrom sklearn.metrics import classification_report as sklearn_cs\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2613ff1c":"\ndef _large_compatible_negative(tensor_type):\n  \"\"\"Large negative number as Tensor.\n  This function is necessary because the standard value for epsilon\n  in this module (-1e9) cannot be represented using tf.float16\n  Args:\n    tensor_type: a dtype to determine the type.\n  Returns:\n    a large negative number.\n  \"\"\"\n  if tensor_type == dtypes.float16:\n    return dtypes.float16.min\n  return -1e9\n\nclass Custom_Softmax(tf.keras.layers.Layer):  \n# Softmax from TF 2.4.0, if you are already using TF 2.4.0, no need for this\n    def __init__(self, axis=-1, **kwargs):\n        super(Custom_Softmax, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -1e.9 for masked positions.\n            adder = (1.0 - math_ops.cast(mask, inputs.dtype)) * (\n                _large_compatible_negative(inputs.dtype))\n\n        # Since we are adding it to the raw scores before the softmax, this is\n            # effectively the same as removing these entirely.\n            inputs += adder\n        if isinstance(self.axis, (tuple, list)):\n            if len(self.axis) > 1:\n                return math_ops.exp(inputs - math_ops.reduce_logsumexp(\n                    inputs, axis=self.axis, keepdims=True))\n            else:\n                return K.softmax(inputs, axis=self.axis[0])\n        return K.softmax(inputs, axis=self.axis)\n\n    def get_config(self):\n        config = {'axis': self.axis}\n        base_config = super(Softmax, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @tf_utils.shape_type_conversion\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\ndef masked_softmax_cross_entropy_loss(y_true,y_pred): \n    masked_pred = tf.boolean_mask(y_pred,y_pred._keras_mask)\n    masked_true = tf.boolean_mask(y_true,y_pred._keras_mask)\n\n    loss = tf.math.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(masked_true,masked_pred))\n    return(loss)\n\n    \ndef build_word_vocab(corpus,pad_index=0,pad_token=\"<PAD>\",unk_index=1,unk_token=\"<UNK>\"):  \n# Builds word level vocabulary dictionary from given corpus    \n\n  word_to_index = {}\n  index_to_word = {}\n  word_to_index[pad_token] = pad_index  \n  word_to_index[unk_token] = unk_index\n  index_to_word[pad_index] = pad_token\n  index_to_word[unk_index] = unk_token   \n  index = 0\n  if index == pad_index:\n    index += 1\n    if index == unk_index:\n      index += 1 \n  if index == unk_index:\n    index += 1\n    if index == pad_index:\n      index += 1 \n  for string in corpus:\n    # tokens = word_tokenize(str(string).lower())\n    tokens = string\n    for token in tokens:\n      if token not in word_to_index:      \n        word_to_index[token] = index\n        index_to_word[index] = token\n        index += 1\n        if index == pad_index:\n          index += 1\n          if index == unk_index:\n            index += 1 \n        if index == unk_index:\n          index += 1\n          if index == pad_index:\n            index += 1 \n  return word_to_index,index_to_word\n\ndef build_char_vocab(corpus,pad_index=0,pad_token=\"<PAD>\",unk_index=1,unk_token=\"<UNK>\"):  \n# Builds character level vocabulary dictionary from given corpus      \n    \n    char_to_index = {}\n    index_to_char = {}\n    char_to_index[pad_token] = pad_index\n    char_to_index[unk_token] = unk_index    \n    index = 0\n    if index == pad_index:\n        index += 1\n        if index == unk_index:\n            index += 1 \n    if index == unk_index:\n        index += 1\n        if index == pad_index:\n            index += 1 \n    for string in corpus:\n        tokens = string\n        for token in tokens:\n              for char in token:\n                if char not in char_to_index:      \n                    char_to_index[char] = index\n                    index_to_char[index] = char\n                    index += 1\n                    if index == pad_index:\n                        index += 1\n                        if index == unk_index:\n                            index += 1 \n                    if index == unk_index:\n                        index += 1\n                        if index == pad_index:\n                            index += 1 \n    return char_to_index,index_to_char\n\n\n\n\ndef to_padded_list_word(sentences,word_to_index,labels=[],pos=[],pad_cat=0,pad_token='<PAD>',\n                        unk_token='<UNK>',pos_pad=0,max_seq_len=1000):\n# Creates a 2d word level padded sequence from a given list of tokenized sentences. This also creates\n# masks to be propagated through the neural net. Masks are important to avoid processing the pads in \n# a padded sequence, and helps in stopping backprop loss from pads. This code is NOT optimized for \n# performance.\n    \n    seq_len = 0\n    for tokens in sentences:\n        if len(tokens) > seq_len:\n            seq_len = len(tokens)\n    # print(seq_len)\n    if max_seq_len < seq_len:\n        seq_len = max_seq_len\n\n    pad_index = word_to_index[pad_token]\n    unk_index = word_to_index[unk_token]\n    sents = []\n    labels_sents = []\n    pos_sents = []\n    mask_sents = []\n    p = 0\n    for tokens in sentences:\n        if len(tokens) < 0:\n            continue    \n        sent = []\n        labels_words = []\n        pos_words = []\n        mask_words = []\n        q = 0\n        for word in tokens:\n            try:    \n                sent.append(word_to_index[word])\n            except KeyError:\n                sent.append(unk_index)\n            if len(labels) > 0:\n                labels_words.append(labels[p][q])\n            if len(pos) > 0:\n                pos_words.append(pos[p][q])          \n            mask_words.append(True)\n            if len(sent) == seq_len:\n                sents.append(sent)\n                labels_sents.append(labels_words)\n                pos_sents.append(pos_words)\n                mask_sents.append(mask_words)\n                sent = []\n                labels_words = []\n                pos_words = []\n            q += 1\n        if len(sent) > 0:\n            pad_len = seq_len-len(sent)\n            sent.extend(pad_len*[pad_index])\n            labels_words.extend(pad_len*[pad_cat])\n            mask_words.extend(pad_len*[False])\n            pos_words.extend(pad_len*[pos_pad])\n            sents.append(sent)\n            labels_sents.append(labels_words)\n            mask_sents.append(mask_words)\n            pos_sents.append(pos_words)\n        p += 1  \n\n    return(sents,labels_sents,mask_sents,pos_sents)\n\n\n\ndef to_padded_list_char(sentences,char_to_index,labels=[],pos=[],pad_cat=0,pad_token='<PAD>',\n                        unk_token='<UNK>',pos_pad=0,max_seq_len=1000,max_word_len=15): \n# Creates a 3d character level padded sequence from a given list of tokenized sentences. This also creates\n# masks to be propagated through the neural net. Masks are important to avoid processing the pads in \n# a padded sequence, and helps in stopping backprop loss from pads. This code is NOT optimized for \n# performance.\n        \n    seq_len = 0\n    for string in sentences:\n        if len(string) > seq_len:\n            seq_len = len(string)\n    if max_seq_len < seq_len:\n        seq_len = max_seq_len\n    max_len = []\n    pad_index = char_to_index[pad_token]\n    unk_index = char_to_index[unk_token]\n    sents = []\n    pos_sents = []\n    labels_sents = []\n    mask_sents = []\n    p = 0\n    for string in sentences:\n        sent_len = seq_len    \n        tokens = string\n        sent = []\n        pos_words = []\n        labels_words = []\n        mask_words = []\n        q = 0\n        for word in tokens:                      \n            word_char = []\n            for char in word:        \n                try:                      \n                    word_char.append(char_to_index[char])\n                except KeyError:\n                    word_char.append(unk_index) \n                if len(word_char) == max_word_len:\n                    sent.append(word_char)\n                    if len(labels) > 0:\n                        labels_words.append(labels[p][q])\n                    if len(pos) > 0:\n                        pos_words.append(pos[p][q])         \n                    mask_words.append(True)\n                    word_char = []\n            if len(sent) == seq_len:\n                sents.append(sent)\n                labels_sents.append(labels_words)\n                pos_sents.append(pos_words)\n                mask_sents.append(mask_words)\n                sent = []\n                labels_words = []\n                pos_words = []\n                mask_words = []\n            if len(word_char) > 0:\n                for i in range(max_word_len-len(word_char)):                      \n                    word_char.append(pad_index)\n                sent.append(word_char)\n                if len(labels) > 0:\n                    labels_words.append(labels[p][q])\n                if len(pos) > 0:\n                    pos_words.append(pos[p][q])        \n                mask_words.append(True)\n            q += 1\n\n        if len(sent) > 0:\n            sent_len = len(sent)\n            for i in range(seq_len-sent_len):\n                word_char = []\n                for j in range(max_word_len):\n                    word_char.append(pad_index)\n                sent.append(word_char)\n                labels_words.append(pad_cat)\n                pos_words.append(pos_pad)\n                mask_words.append(False)\n            sents.append(sent)\n            labels_sents.append(labels_words)            \n            pos_sents.append(pos_words)\n            mask_sents.append(mask_words)\n        p += 1  \n  \n    return(sents,labels_sents,mask_sents,pos_sents)\n\ndef datagen_char(corpus,corpus_labels,corpus_pos,vocab,pad_cat,batch_size):\n# Creates a character based generator to fed to the neural network\n\n    df = pd.DataFrame({'sent':corpus})\n    df['labels'] = corpus_labels\n    df['pos'] = corpus_pos\n    while(1):\n        data = df.sample(frac = 1)\n        prev_index = 0\n        for index in range(batch_size,len(data),batch_size):\n            x = data.iloc[prev_index:index]['sent'] \n            y = data.iloc[prev_index:index]['labels'].to_list()\n            pos = data.iloc[prev_index:index]['pos'].to_list()\n            x,y,mask,pos = to_padded_list_char(x,vocab,y,pos,pad_cat)\n            yield ({'input_ids': np.array(x,dtype=np.float64),\n                    'attention_masks': np.array(mask),\n                    'pos_tags': np.array(pos,dtype=np.float64)},\n                   np.array(y,dtype=np.int32))\n            prev_index = index\n        if prev_index < len(data):\n            x = data.iloc[prev_index:len(data)]['sent']\n            y = data.iloc[prev_index:len(data)]['labels'].to_list()\n            pos = data.iloc[prev_index:len(data)]['pos'].to_list()\n            x,y,mask,pos = to_padded_list_char(x,vocab,y,pos,pad_cat)\n            yield ({'input_ids': np.array(x,dtype=np.float64),\n                    'attention_masks': np.array(mask),\n                    'pos_tags': np.array(pos,dtype=np.float64)},\n                   np.array(y,dtype=np.int32))\n\n\ndef datagen_word(corpus,corpus_labels,corpus_pos,vocab,pad_cat,batch_size):\n# Creates a word based generator to fed to the neural network    \n\n    df = pd.DataFrame({'sent':corpus})\n    df['labels'] = corpus_labels\n    df['pos'] = corpus_pos\n    while(1):\n        data = df.sample(frac = 1)\n        prev_index = 0\n        for index in range(batch_size,len(data),batch_size):\n            x = data.iloc[prev_index:index]['sent'] \n            y = data.iloc[prev_index:index]['labels'].to_list()\n            pos = data.iloc[prev_index:index]['pos'].to_list()\n            x,y,mask,pos = to_padded_list_word(x,vocab,y,pos,pad_cat)\n            yield ({'input_ids': np.array(x,dtype=np.float64),\n                    'attention_masks': np.array(mask),\n                    'pos_tags': np.array(pos,dtype=np.float64)},\n                   np.array(y,dtype=np.int32))\n            prev_index = index\n        if prev_index < len(data):\n            x = data.iloc[prev_index:len(data)]['sent']\n            y = data.iloc[prev_index:len(data)]['labels'].to_list()\n            pos = data.iloc[prev_index:len(data)]['pos'].to_list()\n            x,y,mask,pos = to_padded_list_word(x,vocab,y,pos,pad_cat)\n            yield ({'input_ids': np.array(x,dtype=np.float64),\n                    'attention_masks': np.array(mask),\n                    'pos_tags': np.array(pos,dtype=np.float64)},\n                   np.array(y,dtype=np.int32))\n\n            \ndef process_data(data_path,force_lowercase=False):\n# Data Processing borrowed from - \n# \"https:\/\/www.kaggle.com\/abhishek\/entity-extraction-model-using-bert-pytorch\"\n\n    df = pd.read_csv(data_path, encoding=\"latin-1\")\n    if force_lowercase:\n        df['Word'] = df['Word'].str.lower()\n    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n\n    enc_pos = preprocessing.LabelEncoder()\n    enc_tag = preprocessing.LabelEncoder()\n\n    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n\n\n    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n    return sentences, pos, tag, enc_pos, enc_tag\n\n","feae4a33":"\n#MODEL\n            \nclass label_init(tf.keras.initializers.Initializer):\n    def __call__(self,shape,dtype):\n        return( tf.eye(shape[0],shape[1],dtype=dtype))\n\nclass sent_char_encoder(tf.keras.layers.Layer):\n    def __init__(self,vocab_dim,char_embed_size,word_embed_size):\n        super().__init__(name='sent_char_encoder')\n        self.embed = tf.keras.layers.Embedding(vocab_dim,char_embed_size,name='char_embeds')\n        self.conv = tf.keras.layers.Conv1D(word_embed_size,3,activation='relu',use_bias=False)\n        self.dense = tf.keras.layers.Dense(word_embed_size)\n    def call(self,x):\n        x = self.embed(x)\n        x = self.conv(x)\n        x = tf.math.reduce_max(x,axis=2)\n        return(self.dense(x))\n\n\nclass BiLstm_attnv2(tf.keras.Model):\n    def __init__(self,vocab_dim,char_embed_size,word_embed_size,use_char_level_embeddings=True,\n                 lstm_hidden=8,num_classes=10,dropout=0.3,use_pos_features=False,num_pos=10):\n        super().__init__(name='BiLstm_attnv2')\n        self.num_classes = num_classes\n        if use_char_level_embeddings:\n            self.embed = sent_char_encoder(vocab_dim,char_embed_size,word_embed_size)\n        else:\n            self.embed = tf.keras.layers.Embedding(vocab_dim,word_embed_size,name='word_embeds')\n        if use_pos_features:\n            self.use_pos = True\n            self.pos_embed = tf.keras.layers.Embedding(num_pos,num_pos,embeddings_initializer=label_init,trainable=False,dtype=tf.float32,name='pos_embeddings')\n        self.lab_embed = tf.keras.layers.Embedding(num_classes,num_classes,embeddings_initializer=label_init,trainable=False,dtype=tf.float32,name='label_embeddings')\n        self.encoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_hidden,return_sequences=True,return_state=True),name='lstm_enc')\n        self.postencoder = tf.keras.layers.Dense(2*lstm_hidden,activation='relu')\n        self.drop_postencoder = tf.keras.layers.Dropout(dropout)\n        self.attn_proj =  tf.keras.layers.Dense(2*lstm_hidden,use_bias=False,activation='relu')\n        self.context_proj = tf.keras.layers.Dense(lstm_hidden,use_bias=False,activation='tanh')\n        self.drop_context_proj = tf.keras.layers.Dropout(dropout)\n        self.out =  tf.keras.layers.Dense(num_classes)   #Use this for tensorflow < 2.4.0\n        \n#         self.softmax = tf.keras.layers.Softmax(axis=1) #Use this if already on tensorflow >= 2.4.0\n        self.softmax = Custom_Softmax(axis=1)\n    def call(self,x,training=False,mask_out=True):\n        x_enc = self.embed(x['input_ids'])\n        if self.use_pos:\n            x_pos = self.pos_embed(x['pos_tags'])\n            x_enc = tf.concat([x_enc,x_pos],axis=-1)\n        mask = x['attention_masks']\n        softmax_mask = tf.tile(mask[...,tf.newaxis],[1,1,tf.shape(x['attention_masks'])[1]])  \n        encoded = self.encoder(x_enc,mask=mask)\n        attn_proj = self.attn_proj(encoded[0]) \n        decoded = self.drop_postencoder(self.postencoder(x_enc))\n        e = tf.matmul(attn_proj,tf.transpose(decoded,perm=[0,2,1]))\n        alpha = tf.matmul(tf.transpose(encoded[0],perm=[0,2,1]),self.softmax(e,mask=softmax_mask))\n        context = tf.concat([tf.transpose(alpha,perm=[0,2,1]),decoded],axis=2)\n        o = self.drop_context_proj(self.context_proj(context))\n        out = self.out(o)  \n        if mask_out:\n            out._keras_mask = mask\n        return(out)\n\n\n","0f6e517e":"TRAINING_FILE = \"..\/input\/entity-annotated-corpus\/ner_dataset.csv\"\nsentences, pos, tag, enc_pos, enc_tag = process_data(TRAINING_FILE)\nlabel_array = enc_tag.classes_\npos_array = enc_pos.classes_","25c9d5b1":"train_test_split = 0.85\ntrain_val_split = 0.9\n\ntrain_sents = sentences[:int(train_test_split*len(sentences))]\ntrain_tags = tag[:int(train_test_split*len(tag))]\ntrain_pos = pos[:int(train_test_split*len(pos))]\n\ntest_sents = sentences[int(train_test_split*len(sentences)):]\ntest_tags = tag[int(train_test_split*len(tag)):]\ntest_pos = pos[int(train_test_split*len(pos)):]\n\nval_sents = train_sents[int(train_val_split*len(train_sents)):]\nval_tags = train_tags[int(train_val_split*len(train_tags)):]\nval_pos = train_pos[int(train_val_split*len(train_pos)):]\n\ntrain_sents = train_sents[:int(train_val_split*len(train_sents))]\ntrain_tags = train_tags[:int(train_val_split*len(train_tags))]\ntrain_pos = train_pos[:int(train_val_split*len(train_pos))]\n\ncti,itc = build_char_vocab(train_sents,pad_index=0,pad_token=\"<PAD>\",unk_index=1,unk_token=\"<UNK>\")\nwti,itw = build_word_vocab(train_sents,pad_index=0,pad_token=\"<PAD>\",unk_index=1,unk_token=\"<UNK>\")\nprint(len(train_sents),len(val_sents),len(test_sents))","17df1249":"use_char_level_model = True\nuse_pos_features = True\n\nif use_char_level_model:\n    datagen = datagen_char\n    vocab = cti\nelse:\n    datagen = datagen_word\n    vocab = wti\n    \nvocab_dim = len(vocab)\nbatch_size = 64\ntrain_steps = int(len(train_sents)\/batch_size)\ntest_steps = int(len(test_sents)\/batch_size)\nval_steps = int(len(val_sents)\/batch_size)\ngen_train = datagen(train_sents,train_tags,train_pos,vocab,16,batch_size)\ngen_test = datagen(test_sents,test_tags,test_pos,vocab,16,batch_size)\ngen_val = datagen(val_sents,val_tags,val_pos,vocab,16,batch_size)\n\n\n\nepochs = 50\ncharacter_embedding_size = 256\nword_embedding_size = 256\nlstm_hidden_size = 256\nloss_func = masked_softmax_cross_entropy_loss\noptimizer = tf.keras.optimizers.Adam()\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=6,restore_best_weights=True)\n\nnet =  BiLstm_attnv2(vocab_dim,\n                     character_embedding_size,\n                     word_embedding_size,\n                     lstm_hidden=lstm_hidden_size,\n                     num_classes=len(label_array),\n                     use_char_level_embeddings=use_char_level_model,\n                     dropout=0.3,\n                     use_pos_features=use_pos_features,\n                     num_pos=len(pos_array))\n\nnet.compile(optimizer,loss_func,metrics=['accuracy'])\n\nprint('Training.....')\ntrain_history = net.fit(gen_train,batch_size=batch_size,epochs=50,steps_per_epoch=train_steps,\n        validation_data=gen_val,validation_steps=val_steps,callbacks=[callback],verbose=False)\nprint('Finished Training !')","52c25bad":"from matplotlib import pyplot as plt\nhistory = train_history\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","244bfc8e":"#Test Loop\ni=0\ntrue = []\npred = []\nfor data in gen_test:            \n    x = data[0]\n    y = data[1]               \n    y_pred = net(x, training = False)  # Forward pass  \n    masked_pred = tf.boolean_mask(y_pred,y_pred._keras_mask)\n    masked_y = tf.boolean_mask(y,y_pred._keras_mask)\n    label_true = label_array[masked_y.numpy().astype(int)]\n    label_pred = label_array[np.argmax(masked_pred.numpy(),axis=-1).astype(int)]\n    true += label_true.tolist()\n    pred += label_pred.tolist()\n    i+=1\n    if i > test_steps:\n        break \n","e5ee8332":"print(sklearn_cs(true,pred))","0498c1eb":"print(seqeval_cs([true],[pred]))","b9d7ae36":"# Randomly selects a sentence from the test data and displays the predictions\nindex = np.random.randint(len(test_sents))\nstatements = [test_sents[index]]\npos = [test_pos[index]]\nx,y,mask,pos = to_padded_list_char(statements,cti,[],pos,0)\nx = np.array(x,dtype=np.float64)\nmask = np.array(mask)\npos = np.array(pos)\ninput_seq = {'input_ids': np.array(x,dtype=np.float64),\n             'attention_masks': np.array(mask),\n             'pos_tags': np.array(pos,dtype=np.float64)}\nlabels = net(input_seq)\n\nfor i in range(len(statements)):\n    print(statements[i])\n    print(label_array[np.argmax(labels[i],axis=-1)])\n    ","072f955f":"# Task Description\n**NER or Named Entity Recognition is a task of predicting the entitiy type represented by a word, given a certain set a features. In its base form, its simply modelling a conditional distribution over a set of labels given some form of features.**","d80871b2":"# Random predictions\n**Lets see some randomly chosen predictions from the test data**","e9393240":"**Split the data into different sets and building vocabulary to be used for encoding**","3e8e6512":"# Model Definition\n**Defining a Bidirectional LSTM Model with Attention**","16a4aece":"**For this notebook, I will be using a Bidirectional LSTM model with attention. Input to this model supports character level encoding. I believe character level model may perform better than the word based for this particular task. This mode also supports POS(part of speech) tags as input features. However, one can quickly see that all the Named entities are Nouns. So, I doubt POS tag of the subject word is going to be of much help. But, LSTM can extract sequence information, and POS tags of neighboring words may help. Lets see how this model performs\nThis model is inspired from Neural Machine Translator described here - http:\/\/web.stanford.edu\/class\/cs224n\/assignments\/a4.pdf**","fc002491":"**Fortunately, 'O' tags can be ignored, as they are predicted with almost perfect accuracy and we can remove them from the metrics. This helps us focus on the entities of interest and how our model performs for them. I will be using Seqeval library to calculate this based on Conlleval script**","67fb710e":"# Training the Model\n**Defining training parameters and training the model using early stopping**","35276543":"**Accuracy and Loss plots**","464517bb":"# Loading and splitting the data\n**Loading the data**","af6e9329":"**Above scores have humbled our expectations and shows the true prediction power of our model. True may be a very strong term as we have literally removed the majority class from the metrics.**","5bda04bc":"**How do we get such features ?\n- We can either manually design them(like - if the subject word is capitalized ? previous words, part of speech tags, etc.)\n- Or we can have a model like neural network extract such features on its own. Often, these features extracted by neural net are incomprehensible for us, but they do well for the net**","7be90d76":"**Some utility functions and classes to help processing the data**","eab50531":"**Below is the test metrics when keeping all labels in consideration. This shows a really high f1 score, which is heavily biased by the predictions for the 'O' tags. It is important to predict 'O' tags correctly as we dont want entities to appear where they should not. However, the sheer numbers of the 'O' tags overwhelm the metric.**","e8742c01":"# Testing the Model\n**Running the model on test data and collecting all the true labels and predicted labels for the test data to be used for calculating metrics like accuracy, precision, recall and f1 score**"}}