{"cell_type":{"b7ff2128":"code","e252ca58":"code","62fa26d8":"code","86da5918":"code","9e106d91":"code","dc36c47a":"code","fd4c5cdd":"code","196c1120":"code","b5a4b6a0":"code","837fb0dc":"code","25d45f6e":"code","13e62981":"code","446f4c82":"code","81cdcb7f":"code","f307ee6c":"code","67763bce":"code","8ab1032e":"code","f7afb672":"code","0f3fc6d5":"code","75fe0b7e":"code","1fe7d0d3":"code","4fe5c6d9":"code","bbc0922b":"code","e1a50372":"code","63ff75b2":"code","f43ce217":"code","24585484":"code","c0f2ba95":"code","76668e46":"code","d587a013":"code","433f2603":"code","a8b3805e":"code","523f3463":"code","c96b928e":"code","e9fc46fd":"code","8a210ccd":"code","25b9ad49":"code","61000490":"code","182a8159":"code","2e8998ee":"code","bd18eede":"code","859fc8d7":"code","04c55eb5":"code","f23038a4":"code","848df95b":"code","ffe40a66":"code","190b6db5":"code","b49cc383":"code","eb4084ff":"code","4cfea6ee":"code","40820c41":"code","5d6a81b4":"code","30818226":"code","21a8cbec":"code","efec63e2":"code","53477fd1":"code","66c4a4e4":"code","ffb6fb6e":"code","0a5254c2":"code","f24e739b":"code","ccd0fa66":"code","a006f8ea":"code","12ba2deb":"code","a59b8526":"code","23733ca6":"code","c2ab4dd4":"code","789ed769":"code","dac2f157":"code","53897628":"code","a74d2388":"code","423b8494":"code","4c8b5db9":"code","7a9d8188":"code","22967859":"code","adff708c":"code","8deca9d6":"code","b4100310":"code","a77bb756":"code","169c5daa":"code","d04f8613":"code","15f7aa68":"code","d33ff149":"code","5638a1d8":"code","81461386":"code","c0fe2b81":"code","f941de01":"code","217ed85e":"code","d923887b":"code","9b8164b7":"code","a499a3e1":"code","62099dcf":"code","be5751ff":"code","cb623ac6":"code","5417117f":"code","86c93a23":"code","8b844e88":"code","b1569690":"code","4e0522f3":"code","2c38e798":"code","4f49d210":"code","c4cae7e6":"code","f4363279":"code","0ee76c09":"code","aac8da90":"code","2b9ffd3a":"code","653b08c1":"code","a5179ca2":"code","5e1dc2ce":"code","e4ea69f4":"code","4a09e210":"code","4e096356":"code","fd5472aa":"code","057a3cd9":"code","f0fcbafc":"code","ea3064c0":"code","4d58640b":"code","2bf028d0":"code","4fdd3fca":"code","ebfe66bf":"code","89f5caf2":"code","0c847e8f":"code","013cc058":"code","08cfa7b2":"code","e90974b1":"code","d2629845":"code","3da1dfa9":"code","abc78f9e":"code","c8edf4fc":"code","e57668cc":"code","f199e020":"code","2bf89dd9":"code","3c76d522":"code","b9ad6b95":"code","edfc8d89":"code","c640b50f":"code","b1d7accf":"code","b2080302":"code","ec069fe2":"code","8f9bd509":"code","da0a61c5":"code","49becfda":"code","ab128943":"code","93eacf85":"code","4016f72b":"code","2917e40c":"code","cacc66d4":"code","c478dbe4":"code","98d45d23":"code","72102c20":"code","7392e7af":"code","df56f475":"code","2e0bf5c7":"code","79496aba":"code","99dcde2c":"code","d0c83b02":"code","2691d52f":"code","c79553c6":"code","6c4fbf83":"code","1be98002":"code","f1f7383a":"code","33e20858":"code","ea0922ab":"code","d0c245d0":"code","8ec2b0a7":"code","e7088e94":"code","16a8da2e":"code","6928d995":"code","19f749a5":"code","0e7d476c":"code","8a580a93":"code","bfb87bfa":"code","8e14b7db":"code","337949a9":"code","223d2f00":"code","b2529989":"code","5024d0fb":"code","39cbf3a0":"code","27dec90b":"code","a6652664":"code","75055063":"code","2c17cfde":"code","cd2af08d":"code","ac476ea3":"code","fcef62cf":"code","24f896a4":"code","f80146ab":"code","8a37172d":"code","0eb57c02":"code","421c6458":"code","0485ab5b":"code","46cef700":"code","bc1a207e":"code","47552a07":"markdown","5af39958":"markdown","566e326b":"markdown","1cccb337":"markdown","56741261":"markdown","184ccbc1":"markdown","8ab37e45":"markdown","926debe1":"markdown","f9f274e3":"markdown","efb110dd":"markdown","03743047":"markdown","22a581de":"markdown","84e5f90e":"markdown","91831eef":"markdown","1d52b98f":"markdown","ea4d59cc":"markdown","d3240162":"markdown","9f528320":"markdown","5b2741b4":"markdown","afb384cd":"markdown","b0e050a2":"markdown","aea4d02b":"markdown","708f228a":"markdown","24503c2a":"markdown","af73aa42":"markdown","7dfa5be5":"markdown","03e707c8":"markdown","39697e5d":"markdown","fc300e61":"markdown","6b167e87":"markdown","94826d66":"markdown","135dd966":"markdown","58b016b3":"markdown","e96be80b":"markdown","639dedef":"markdown","fbb0b487":"markdown","2533eda8":"markdown","71db83b5":"markdown","506246ac":"markdown","ddea86e8":"markdown","17b28715":"markdown","fd11df4e":"markdown","23da83cb":"markdown","ef51090a":"markdown","7e3f5450":"markdown","ac23faa6":"markdown","d4c660ff":"markdown","ce0b0c71":"markdown","001a91ba":"markdown","7ed5e8cf":"markdown","4c9c9ed0":"markdown","ba2f8ac2":"markdown","a7934ec0":"markdown","60067858":"markdown","d1b61a08":"markdown","beeb1be1":"markdown","c2581720":"markdown","89902c1a":"markdown","4a1e5793":"markdown","cd7b3d67":"markdown","9d72cb01":"markdown","cc48181a":"markdown","086299e7":"markdown","2e670d76":"markdown","124daa50":"markdown","d2432adf":"markdown","92b493c3":"markdown","15ea34f7":"markdown"},"source":{"b7ff2128":"# for basic operations\nimport numpy as np \nimport pandas as pd \n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n# plt.style.use('fivethirtyeight')\n\n# for modeling \nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\n# from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, Lasso\nfrom sklearn.svm import OneClassSVM\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")","e252ca58":"data = pd.read_csv(\"..\/input\/uci-semcom\/uci-secom.csv\")\ndata","62fa26d8":"data = data.rename({\"Pass\/Fail\":'Pass_Fail'},axis=1)","86da5918":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str\n\ndisplay(summary(data).sort_values(by='Nulls', ascending=False))","9e106d91":"def null_values(df):\n    \"\"\"a function to show null values with percentage\"\"\"\n    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)","dc36c47a":"df_na = null_values(data)\ndf_na","fd4c5cdd":"df_na = df_na[df_na[\"Percentage (%)\"] > 50]\ndf_na","196c1120":"df_na.shape","b5a4b6a0":"df_na.index","837fb0dc":"data = data.drop(axis=1, columns=df_na.index)\ndata.shape","25d45f6e":"data.head()","13e62981":"#strategy 1: latest information available\ndata.fillna(method='ffill', inplace=True)\ndata.fillna(method='bfill', inplace=True)\ndata","446f4c82":"null_values(data)","81cdcb7f":"data.shape","f307ee6c":"data[\"5\"].nunique()","67763bce":"def unique_columns(df):\n    \"\"\"a function to show unique columns with column name\"\"\"\n    uni_col_list = []\n    for column in df.columns:\n        if df[column].nunique() == 1:\n            uni_col_list.append(column)\n    return uni_col_list","8ab1032e":"len(unique_columns(data))","f7afb672":"data[\"179\"].nunique()","0f3fc6d5":"data.shape","75fe0b7e":"data = data.drop(axis=1, columns=unique_columns(data))\ndata.shape","1fe7d0d3":"# We will not use the \"Time\" column in the following steps, so this column can be completely deleted.\ndata.drop(columns=\"Time\", axis=1, inplace=True)\ndata.shape","4fe5c6d9":"data.info()","bbc0922b":"# pie chart\n# We have highly imbalanced class with only 6.6% failures and 93.4% pass\n\nplt.rcParams['figure.figsize'] = (5,5)\nlabels = ['Pass', 'Fail']\nsize = data['Pass_Fail'].value_counts()\ncolors = ['blue', 'green']\nexplode = [0, 0.1]\n\nplt.style.use('seaborn-deep')\nplt.pie(size, labels =labels, colors = colors, explode = explode, autopct = \"%.2f%%\", shadow = True)\nplt.axis('off')\nplt.title('Target: Pass or Fail', fontsize = 20)\nplt.legend()\nplt.show()\ndata[\"Pass_Fail\"].value_counts()","e1a50372":"data.corr()[\"Pass_Fail\"].sort_values()","63ff75b2":"# Remove the highly collinear features from data\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        x: features dataframe\n        threshold: features with correlations greater than this value are removed\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n\n    return x","f43ce217":"#Remove columns having more than 70% correlation\n#Both positive and negative correlations are considered here\ndata = remove_collinear_features(data,0.70)","24585484":"data.shape","c0f2ba95":"# from sklearn.model_selection import train_test_split","76668e46":"# separating the dependent and independent data\nX = data.drop('Pass_Fail', axis=1)\ny = data['Pass_Fail']\n\n# getting the shapes of new data sets x and y\nprint(\"shape of X:\", X.shape)\nprint(\"shape of y:\", y.shape)","d587a013":"# the function train_test_split creates random data samples (default: 75-25%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)  # seed for comparable results","433f2603":"# gettiing the shapes\nprint(\"shape of X_train: \", X_train.shape)\nprint(\"shape of X_test: \", X_test.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of y_test: \", y_test.shape)","a8b3805e":"# Let's check if the pass \/ fail ratio is preserved in y_test data.\nsize = y_test.value_counts()\nplt.pie(size, labels =labels, colors = colors, explode = explode, autopct = \"%.2f%%\", shadow = True)\nplt.axis('off')\nplt.title('y_test: Pass or Fail', fontsize = 20)\nplt.legend()\nplt.show()","523f3463":"# standardization\n# from sklearn.preprocessing import StandardScaler\n\n# creating a standard scaler\nsc = StandardScaler()\n\n# fitting independent data to the model\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","c96b928e":"# import lazypredict\n# from lazypredict.Supervised import LazyClassifier\n# from sklearn.utils.testing import ignore_warnings\n\n# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n# models, predictions = clf.fit (X_train, X_test, y_train, y_test)\n# models","e9fc46fd":"cv_acc = {}\ncv_TPR = {}\ncv_FPR = {}","8a210ccd":"def plot_result(model, name:str):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc[name] = round(scores.mean(), 4)*100  # balanced accuracy\n    cv_TPR[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (Max)\n    cv_FPR[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (Min)\n    print('Average Balanced Accuracy (CV=10):', scores.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","25b9ad49":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix","61000490":"dtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\nplot_result(dtc, \"dtc\")","182a8159":"# plot tree\nplt.figure(figsize=(16,6))\nplot_tree(dtc, filled = True, class_names=[\"-1\", \"1\"], feature_names=X.columns, fontsize=11);","2e8998ee":"cv_acc","bd18eede":"cv_FPR","859fc8d7":"cv_TPR","04c55eb5":"from sklearn.linear_model import LogisticRegression","f23038a4":"lr = LogisticRegression(max_iter=150)\nplot_result(lr, \"lr\")","848df95b":"from sklearn.svm import SVC","ffe40a66":"svc = SVC(C=1.0, kernel='rbf', gamma='scale')  # default values\nplot_result(svc, \"svc\")","190b6db5":"from sklearn.ensemble import RandomForestClassifier","b49cc383":"rfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None)\nplot_result(rfc, \"rfc\")","eb4084ff":"def plot_feature_importances(model):\n    feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)[:10]\n\n    sns.barplot(x=feature_imp, y=feature_imp.index)\n    plt.title(\"Feature Importance\")\n    plt.show()\n\n    print(f\"Top 10 Feature Importance for {str(model).split('(')[0]}\\n\\n\",feature_imp[:10],sep='')","4cfea6ee":"plot_feature_importances(rfc)","40820c41":"from sklearn.ensemble import GradientBoostingClassifier","5d6a81b4":"gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=3, random_state=42)\nplot_result(gbc, \"gbc\")","30818226":"plot_feature_importances(gbc)","21a8cbec":"from sklearn.naive_bayes import GaussianNB","efec63e2":"nb = GaussianNB()\nplot_result(nb, \"nb\")","53477fd1":"from sklearn.neighbors import KNeighborsClassifier","66c4a4e4":"knn = KNeighborsClassifier(n_neighbors=3)\nplot_result(knn, \"knn\")","ffb6fb6e":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score","0a5254c2":"xgb = XGBClassifier(eval_metric = \"logloss\")\nplot_result(xgb, \"xgb\")","f24e739b":"plot_feature_importances(xgb)","ccd0fa66":"from xgboost import plot_importance\nplot_importance(xgb,max_num_features=10)\nplt.xlabel('The F-Score for each features')\nplt.ylabel('Importances')\nplt.show()","a006f8ea":"cv_acc","12ba2deb":"cv_TPR","a59b8526":"cv_FPR","23733ca6":"cv_FPR.values()","c2ab4dd4":"df_eval = pd.DataFrame(data={'model': list(cv_acc.keys()), 'bal_acc': list(cv_acc.values()), 'recall': list(cv_TPR.values()), 'fallout':list(cv_FPR.values())})\ndf_eval","789ed769":"fig, ax = plt.subplots(1,3, figsize=(14, 4))\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0])\nax[0].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[1])\nax[1].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[2])\nax[2].set_title(\"Unbalanced Test FPR\")\nplt.show()","dac2f157":"y_test.value_counts(normalize=True)","53897628":"y_train.value_counts(normalize=True)","a74d2388":"# pip install imblearn\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE","423b8494":"oversmote = SMOTE()\nX_train_os, y_train_os= oversmote.fit_resample(X_train, y_train)","4c8b5db9":"y_train_os.value_counts().plot.bar(color=[\"blue\", \"red\"])\nplt.show()","7a9d8188":"X_train_os.shape","22967859":"cv_acc_balance = {}\ncv_TPR_balance = {}\ncv_FPR_balance = {}","adff708c":"def plot_result_smote(model, name:str):\n    model.fit(X_train_os, y_train_os)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance[name] = round(scores.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (maximieren)\n    cv_FPR_balance[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (minimieren)\n    print('Average Balanced Accuracy (CV=10):', scores.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","8deca9d6":"# Decision tree\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n\nplot_result_smote(dtc, \"dtc\")","b4100310":"# Logistic Regression\nlr = LogisticRegression(max_iter=200)\nplot_result_smote(lr, \"lr\")","a77bb756":"# SVC\nsvc = SVC(C=1.0, kernel='rbf', gamma='scale')  # default Werte\nplot_result_smote(svc, \"svc\")","169c5daa":"# Random Forest\nrfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None)\nplot_result_smote(rfc, \"rfc\")","d04f8613":"# Gradient Boost\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=3, random_state=42)\nplot_result_smote(gbc, \"gbc\")","15f7aa68":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smote(nb, \"nb\")","d33ff149":"# kNN\nknn = KNeighborsClassifier(n_neighbors=3)\nplot_result_smote(knn, \"knn\")","5638a1d8":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",n_estimators=25, learning_rate=0.01,  max_depth=3, random_state=42)\nplot_result_smote(xgb, \"xgb\")","81461386":"cv_acc","c0fe2b81":"df_eval_smote = pd.DataFrame(data={'model': list(cv_acc_balance.keys()), 'bal_acc': list(cv_acc_balance.values()), 'recall': list(cv_TPR_balance.values()), 'fallout':list(cv_FPR_balance.values())})\ndf_eval_smote","f941de01":"fig, ax = plt.subplots(2,3, figsize=(16, 8))\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test FPR\")\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test FPR\")\nplt.tight_layout()\nplt.show()","217ed85e":"from scipy.stats import pearsonr","d923887b":"df_filtered = pd.DataFrame(X_train_os).apply(lambda x: pd.Series(pearsonr(x, y_train_os), index=[0, 1]), axis =0).T.applymap(lambda x: np.round(x,3)).sort_values(by=1, ascending=False)\ndf_filtered","9b8164b7":"df_filtered = df_filtered[df_filtered[1] <= 0.05]\ndf_filtered","a499a3e1":"df_filtered.index","62099dcf":"X_train_os_selected_filter = pd.DataFrame(X_train_os)[df_filtered.index]\nX_test_fe = pd.DataFrame(X_test)[df_filtered.index]\nX_train_os_selected_filter","be5751ff":"cv_acc_balance_fe = {}\ncv_TPR_balance_fe = {}\ncv_FPR_balance_fe = {}","cb623ac6":"def plot_result_smoted_fe(model, name:str):\n    model.fit(X_train_os_selected_filter, y_train_os)\n    y_pred = model.predict(X_test_fe)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores = cross_val_score(model, X_test_fe, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_fe[name] = round(scores.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_fe[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (maximize)\n    cv_FPR_balance_fe[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (minimize)\n    print('Average Balanced Accuracy (CV=10):', scores.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plt.figure(figsize=(3,3))\n    plot_confusion_matrix(model, X_test_fe, y_test)\n    plt.show()","5417117f":"# Decision tree\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n\nplot_result_smoted_fe(dtc, \"dtc\")","86c93a23":"# Logistic Regression\nlr = LogisticRegression(max_iter=200)\nplot_result_smoted_fe(lr, \"lr\")","8b844e88":"# SVC\nsvc = SVC(C=1.0, kernel='rbf', gamma='scale')  # default Werte\nplot_result_smoted_fe(svc, \"svc\")","b1569690":"# Random Forest\nrfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None)\nplot_result_smoted_fe(rfc, \"rfc\")","4e0522f3":"# Gradient Boost\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=3, random_state=42)\nplot_result_smoted_fe(gbc, \"gbc\")","2c38e798":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smoted_fe(nb, \"nb\")","4f49d210":"# kNN\nknn = KNeighborsClassifier(n_neighbors=3)\nplot_result_smoted_fe(knn, \"knn\")","c4cae7e6":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",n_estimators=25, learning_rate=0.01,  max_depth=3, random_state=42)\nplot_result_smoted_fe(xgb, \"xgb\");","f4363279":"df_eval_smote_fe = pd.DataFrame(data={'model': list(cv_acc_balance.keys()), 'bal_acc': list(cv_acc_balance.values()), 'recall': list(cv_TPR_balance.values()), 'fallout':list(cv_FPR_balance.values())})\ndf_eval_smote_fe","0ee76c09":"fig, ax = plt.subplots(3,3, figsize=(16, 8))\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"Smoted_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"Smoted_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"Smoted_Featured Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","aac8da90":"from sklearn.decomposition import PCA","2b9ffd3a":"X_train_os","653b08c1":"pca = PCA().fit(X_train_os)","a5179ca2":"fig, ax = plt.subplots(figsize=(20,8))\nxi = np.arange(0, 160, step=1)\ny = np.cumsum(pca.explained_variance_ratio_[0:160:1])\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='.', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 160, step=2), rotation=90) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","5e1dc2ce":"pca = PCA(n_components=145)\npca.fit(X_train_os)\nper_var = np.round(pca.explained_variance_ratio_ * 100, 1)\nlabels = ['PC' + str(x) for x in range(1,len(per_var)+1)]\n\nplt.figure(figsize=(20,6))\nplt.bar(x=range(len(per_var)), height=per_var, tick_label=labels)\nplt.title('Total explained variance {}'.format(np.round(sum(per_var),2)))\nplt.ylabel('Explained variance in percent')\nplt.xticks(rotation=90)\nplt.show()","e4ea69f4":"X_train_os_pca = pca.transform(X_train_os)\npd.DataFrame(X_train_os_pca)","4a09e210":"# Top 20 columns that have the greatest impact\nloading_scores = pd.Series(pca.components_[0], index=X.columns)\nloading_scores.abs().sort_values(ascending=False)[:20]","4e096356":"X_test_pca = pca.transform(X_test)","fd5472aa":"cv_acc_balance_pca = {}\ncv_TPR_balance_pca = {}\ncv_FPR_balance_pca = {}","057a3cd9":"def plot_result_smoted_pca(model, name:str):\n    model.fit(X_train_os_pca, y_train_os)\n    y_pred = model.predict(X_test_pca)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores = cross_val_score(model, X_test_pca, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_pca[name] = round(scores.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_pca[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance_pca[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    print('Average Balanced Accuracy (CV=10):', scores.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(3,3))\n    plot_confusion_matrix(model, X_test_pca, y_test)\n    plt.show()","f0fcbafc":"# Decision tree\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n\nplot_result_smoted_pca(dtc, \"dtc\")","ea3064c0":"# Logistic Regression\nlr = LogisticRegression(max_iter=200)\nplot_result_smoted_pca(lr, \"lr\")","4d58640b":"# SVC\nsvc = SVC(C=1.0, kernel='rbf', gamma='scale')  # default Werte\nplot_result_smoted_pca(svc, \"svc\")","2bf028d0":"# Random Forest\nrfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None)\nplot_result_smoted_pca(rfc, \"rfc\")","4fdd3fca":"# Gradient Boost\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=3, random_state=42)\nplot_result_smoted_pca(gbc, \"gbc\")","ebfe66bf":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smoted_pca(nb, \"nb\")","89f5caf2":"# kNN\nknn = KNeighborsClassifier(n_neighbors=3)\nplot_result_smoted_pca(knn, \"knn\")","0c847e8f":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",n_estimators=25, learning_rate=0.01,  max_depth=3, random_state=42)\nplot_result_smoted_pca(xgb, \"xgb\");","013cc058":"df_eval_smote_pca = pd.DataFrame(data={'model': list(cv_acc_balance.keys()), 'bal_acc': list(cv_acc_balance.values()), 'recall': list(cv_TPR_balance.values()), 'fallout':list(cv_FPR_balance.values())})\ndf_eval_smote_pca","08cfa7b2":"fig, ax = plt.subplots(4,3, figsize=(16, 10))\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"Smoted_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"Smoted_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"Smoted_Featured Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,0])\nax[3,0].set_title(\"Smoted_PCA Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,1])\nax[3,1].set_title(\"Smoted_PCA Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,2])\nax[3,2].set_title(\"Smoted_PCA Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","e90974b1":"df_eval[\"type\"] = \"Unbalanced\"\ndf_eval_smote[\"type\"] = \"Smote\"\ndf_eval_smote_fe[\"type\"] = \"Smote_FE\"\ndf_eval_smote_pca[\"type\"] = \"Smote_PCA\"","d2629845":"frames = [df_eval, df_eval_smote, df_eval_smote_fe, df_eval_smote_pca]\ndf_result = pd.concat(frames, ignore_index=True)\ndf_result['model'] = df_result['model'].str.upper()\ndf_result[[\"recall\", \"fallout\", \"bal_acc\"]] = df_result[[\"recall\", \"fallout\", \"bal_acc\"]].apply(lambda x: np.round(x, 2))","3da1dfa9":"sns.relplot(x=\"recall\", y=\"fallout\", hue=\"model\", size=\"bal_acc\", sizes=(40, 400), col=\"type\", alpha=1, palette=\"bright\", height=4, legend='full', data=df_result)","abc78f9e":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline","c8edf4fc":"X_train_os_selected_filter","e57668cc":"X_test_fe","f199e020":"# grid_searcher = GridSearchCV(svc, params, cv = 10, n_jobs=-1)\n# grid_searcher.fit(X,y)","2bf89dd9":"cv_acc_balance_fe_tuned = {}\ncv_TPR_balance_fe_tuned = {}\ncv_FPR_balance_fe_tuned = {}","3c76d522":"def plot_result_smoted_fe_tuned(model, name:str):\n    model.fit(X_train_os_selected_filter, y_train_os)\n    y_pred = model.predict(X_test_fe)\n\n    # Evaluation based on a 10-fold cross-validation\n    scores = cross_val_score(model, X_test_fe, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_fe_tuned[name] = round(scores.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_fe_tuned[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance_fe_tuned[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    print('Average Balanced Accuracy (CV=10):', scores.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(3,3))\n    plot_confusion_matrix(model, X_test_fe, y_test)\n    plt.show()","b9ad6b95":"# Decision Tree\nparams = {'criterion' : [\"gini\", \"entropy\"],\n          'max_depth':[2, 3, 4, 5, 10], \n          'min_samples_leaf':[100, 50, 10, 1],\n          'min_samples_split':[2, 4, 8, 16]}\n\ngrid_searcher = GridSearchCV(DecisionTreeClassifier(), params)\ngrid_searcher.fit(X_train_os_selected_filter, y_train_os)\n#plot_result_smoted_fe_tuned(grid_searcher, \"dtc\")","edfc8d89":"grid_searcher.best_params_","c640b50f":"grid_searcher.best_estimator_","b1d7accf":"plot_result_smoted_fe_tuned(grid_searcher.best_estimator_, \"dtc\")","b2080302":"# Logistic Regression\nparams = {'penalty' : ['l1', 'l2'],\n          'C' : np.logspace(-4, 4, 20),\n          'max_iter':[200, 300],\n          'solver' : ['liblinear']}\n\ngrid_searcher = GridSearchCV(LogisticRegression(), params)\ngrid_searcher.fit(X_train_os_selected_filter, y_train_os)\n\nprint(grid_searcher.best_estimator_)\nplot_result_smoted_fe_tuned(grid_searcher.best_estimator_, \"lr\")","ec069fe2":"# SVC\nparams = {'kernel':(['linear','rbf']), \n          'C':[0.001, 0.01, 0.1, 1, 10, 100], \n          'gamma':[0.001, 0.01, 0.1, 1]}\n\ngrid_searcher = GridSearchCV(SVC(), params)\ngrid_searcher.fit(X_train_os_selected_filter, y_train_os)\n\nprint(grid_searcher.best_estimator_)\nplot_result_smoted_fe_tuned(grid_searcher.best_estimator_, \"svc\")","8f9bd509":"# Random Forest\nparams={'criterion': ['entropy', 'gini'],\n        'n_estimators' : list(range(10,101,10)),\n        'max_features' : list(range(6,32,5))}\n\ngrid_searcher = GridSearchCV(RandomForestClassifier(), params)\ngrid_searcher.fit(X_train_os_selected_filter, y_train_os)\n\nprint(grid_searcher.best_estimator_)\nplot_result_smoted_fe_tuned(grid_searcher.best_estimator_, \"rfc\")","da0a61c5":"# These parameters could not be optimized with GridSearch because it took longer than intended. \n# Then new Parameter manual was tried again\n\n# params={\n#        \"learning_rate\": [1],\n#       \"min_samples_split\": [50, 10, 2],\n#        \"min_samples_leaf\": [1, 5, 10],\n#        \"max_depth\":[3,4,5],\n#        \"subsample\":[0.5, 1.0],\n#        \"n_estimators\":[10, 50, 100],\n#        \"random_state\":[42]}\n\ngbc = GradientBoostingClassifier(learning_rate=1, max_depth=6, n_estimators=40, random_state=42)  # manually selected parameters\n# grid_searcher = GridSearchCV(GradientBoostingClassifier(), params)\n# grid_searcher.fit(X_train_os_selected_filter, y_train_os)\n\n# print(grid_searcher.best_estimator_)\nplot_result_smoted_fe_tuned(gbc, \"gbc\")","49becfda":"# Naive Bayes has no tuning\nnb = GaussianNB()\nplot_result_smoted_fe_tuned(nb, \"nb\")","ab128943":"# kNN\nparams = {'n_neighbors':[2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n         'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_searcher = GridSearchCV(KNeighborsClassifier(), params)\ngrid_searcher.fit(X_train_os_selected_filter, y_train_os)\n\nprint(grid_searcher.best_estimator_)\nplot_result_smoted_fe_tuned(grid_searcher.best_estimator_, \"knn\")","93eacf85":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",n_estimators=25, learning_rate=0.01,  max_depth=3, random_state=42)\nplot_result_smoted_fe_tuned(xgb, \"xgb\");","4016f72b":"df_eval_smote_fe_tuned = pd.DataFrame(data={'model': list(cv_acc_balance.keys()), 'bal_acc': list(cv_acc_balance.values()), 'recall': list(cv_TPR_balance.values()), 'fallout':list(cv_FPR_balance.values())})\ndf_eval_smote_fe_tuned","2917e40c":"fig, ax = plt.subplots(5,3, figsize=(16, 10))\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"Smoted_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"Smoted_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_fe.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"Smoted_Featured Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,0])\nax[3,0].set_title(\"Smoted_PCA Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,1])\nax[3,1].set_title(\"Smoted_PCA Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,2])\nax[3,2].set_title(\"Smoted_PCA Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_fe_tuned.sort_values(by=\"recall\"), ax=ax[4,0])\nax[4,0].set_title(\"Smoted_FE_Tuned Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_fe_tuned.sort_values(by=\"recall\"), ax=ax[4,1])\nax[4,1].set_title(\"Smoted_FE_Tuned Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_fe_tuned.sort_values(by=\"recall\"), ax=ax[4,2])\nax[4,2].set_title(\"Smoted_FE_Tuned Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","cacc66d4":"df_eval_smote_fe_tuned[\"type\"] = \"Smote_FE_Tuned\"","c478dbe4":"frames = [df_eval, df_eval_smote, df_eval_smote_fe, df_eval_smote_pca, df_eval_smote_fe_tuned]\ndf_result = pd.concat(frames, ignore_index=True)\ndf_result['model'] = df_result['model'].str.upper()\ndf_result[[\"recall\", \"fallout\", \"bal_acc\"]] = df_result[[\"recall\", \"fallout\", \"bal_acc\"]].apply(lambda x: np.round(x, 2))\ndf_result","98d45d23":"sns.relplot(x=\"recall\", y=\"fallout\", hue=\"model\", size=\"bal_acc\", sizes=(40, 400), col=\"type\", alpha=1, palette=\"bright\", height=4, legend='full', data=df_result)","72102c20":"data","7392e7af":"sns.boxplot(x=data['0'], whis=2);","df56f475":"sns.boxplot(x=data['Pass_Fail'], whis=2);","2e0bf5c7":"from scipy import stats\n\nz = np.abs(stats.zscore(data))\nprint(z)","79496aba":"z[0][1]","99dcde2c":"threshold = 3\nprint(np.where(z > 3))","d0c83b02":"print(z[0][82])  # So, the data point \u2014 0th record on column 82 is an outlier.","2691d52f":"data_o = data[(z < 3).all(axis=1)]","c79553c6":"data_o.shape","6c4fbf83":"sns.boxplot(x=data_o['0'], whis=2);","1be98002":"sns.countplot(data_o.Pass_Fail);","f1f7383a":"X = data.drop('Pass_Fail', axis=1)\ny = data['Pass_Fail']","33e20858":"from scipy import stats\n\nz = np.abs(stats.zscore(X))\nprint(z)","ea0922ab":"X_o = X[(z < 3).all(axis=1)]\nX_o.shape","d0c245d0":"X_o.index","8ec2b0a7":"y_o = y.iloc[X_o.index]","e7088e94":"sns.countplot(y_o);","16a8da2e":"Q1 = X.quantile(0.25)\nQ3 = X.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","6928d995":"((X < (Q1 - 2 * IQR)) | (X > (Q3 + 2 * IQR)))","19f749a5":"X_out = X[~((X < (Q1 - 2.5 * IQR)) | (X > (Q3 + 2.5 * IQR))).any(axis=1)]\nX_out.shape","0e7d476c":"X_out","8a580a93":"# the function train_test_split creates random data samples (default: 75-25%)\nX_train_o, X_test_o, y_train_o, y_test_o = train_test_split(X_o, y_o, stratify=y_o, random_state=42)  # seed for comparable results","bfb87bfa":"X_train_o.shape","8e14b7db":"y_test_o.value_counts().plot.bar();","337949a9":"y_train_o.value_counts().plot.bar();","223d2f00":"from sklearn.preprocessing import StandardScaler","b2529989":"std_scale = StandardScaler().fit(X_train_o)\nX_train_o_scaled = std_scale.transform(X_train_o)\npd.DataFrame(X_train_o_scaled).head()","5024d0fb":"X_test_o_scaled = std_scale.transform(X_test_o)\npd.DataFrame(X_test_o_scaled).head()","39cbf3a0":"from imblearn.over_sampling import SMOTE","27dec90b":"oversmote = SMOTE()\nX_train_os, y_train_os= oversmote.fit_resample(X_train_o_scaled, y_train_o)","a6652664":"y_train_os.value_counts().plot.bar(color=[\"blue\", \"red\"])\nplt.show()","75055063":"X_train_os.shape","2c17cfde":"cv_acc_balance_o = {}\ncv_TPR_balance_o = {}\ncv_FPR_balance_o = {}","cd2af08d":"def plot_result_smote_o(model, name:str):\n    model.fit(X_train_os, y_train_os)\n    y_pred = model.predict(X_test_o_scaled)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores = cross_val_score(model, X_test_o_scaled, y_test_o, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_o[name] = round(scores.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_o[name] = (confusion_matrix(y_test_o, y_pred)[1][1]\/confusion_matrix(y_test_o, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance_o[name] = (confusion_matrix(y_test_o, y_pred)[0][1]\/confusion_matrix(y_test_o, y_pred)[0].sum())*100  # fallout (min)\n    print('Aerage Balanced Accuracy (CV=10):', scores.mean())\n\n    # print classification report\n    print(classification_report(y_test_o, y_pred, zero_division=0))\n\n    # Plot Confusion matrix\n    plot_confusion_matrix(model, X_test_o_scaled, y_test_o)\n    plt.show()","ac476ea3":"# Decision Tree\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n\nplot_result_smote_o(dtc, \"dtc\")","fcef62cf":"# Logistic Regression\nlr = LogisticRegression(max_iter=200)\nplot_result_smote_o(lr, \"lr\")","24f896a4":"# SVC\nsvc = SVC(C=1.0, kernel='rbf', gamma='scale')  # default Werte\nplot_result_smote_o(svc, \"svc\")","f80146ab":"# Random Forest\nrfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None)\nplot_result_smote_o(rfc, \"rfc\")","8a37172d":"# Gradient Boost\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=3, random_state=42)\nplot_result_smote_o(gbc, \"gbc\")","0eb57c02":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smote_o(nb, \"nb\")","421c6458":"# kNN\nknn = KNeighborsClassifier(n_neighbors=3)\nplot_result_smote_o(knn, \"knn\")","0485ab5b":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",n_estimators=25, learning_rate=0.01,  max_depth=3, random_state=42)\nplot_result_smote_o(xgb, \"xgb\");","46cef700":"df_eval_smote_o = pd.DataFrame(data={'model': list(cv_acc_balance.keys()), 'bal_acc': list(cv_acc_balance.values()), 'recall': list(cv_TPR_balance.values()), 'fallout':list(cv_FPR_balance.values())})\ndf_eval_smote_o","bc1a207e":"fig, ax = plt.subplots(3,3, figsize=(16, 8))\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc\", y=\"model\", data=df_eval_smote_o.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_o.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_o.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"Smote Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","47552a07":"Smote_FE looks better than Smote_PCA. Hyperparameter is optimized by Smote_FE.","5af39958":"**8-XGBOOST**","566e326b":"### Use algorithms","1cccb337":"## Extra section: Handling outliers","56741261":"## Machine learning completion of the project: The SECOM data set","184ccbc1":"Our record rose to 267 lines","8ab37e45":"##### 4-Random Forest","926debe1":"#### Use Algorithm","f9f274e3":"In this plot it looks like GradientBoosting in Smote has a better result","efb110dd":"According to Smote and Feature Selection with Pearson Correlation, nb gave the better result for recall and fallout.","03743047":"## Iteration 2: (with SMOTE)","22a581de":"### Scaling Data","84e5f90e":"#### Use Algorithm","91831eef":"#### SMOTE","1d52b98f":"##### 5-Gradient Boosting","ea4d59cc":"##### 3-SVM","d3240162":"we know that the principal components explain a part of the variance. From the Scikit-learn implementation, we can get the information about the explained variance and plot the cumulative variance. The PCA algorithm is going to standardize the input data frame, calculate the covariance matrix of the features. Thanks to [Mikulski](https:\/\/www.mikulskibartosz.name\/pca-how-to-choose-the-number-of-components\/)","9f528320":"According to Smote and PCA, none of the models really gave relatively good results.","5b2741b4":"### Many Thanks.","afb384cd":"### EDA (Exploratory Data Analysis)","b0e050a2":"* The SECOM data set contains 1567 examples that originate from a production line for wafer manufacture. \n* Each example is a vector of 590 sensor measurements plus an identification of the pass \/ fail test. \n* Among the 1567 examples, there are only 104 failed cases marked positive (coded as 1), while a much larger number of examples pass the test and are marked negative (coded as -1). \n* The imbalance between passed and failed examples, as well as the large amount of measurement data obtained from hundreds of sensors, make it difficult to analyze this data set precisely.","aea4d02b":"### Unique values in columns","708f228a":"![image.png](attachment:image.png)","24503c2a":"GradientBoosting yielded the optimized result as better FPR and relative mean strong recall scores. The balance accuracy is also relatively good.","af73aa42":"**Neither column has a great correlation with Pass_Fail**","7dfa5be5":"##### IQR Score","03e707c8":"### Split data","39697e5d":"## Iteration 5: (with SMOTE_FE and hyperparameter optimization)","fc300e61":"#### z-score\nThe Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.","6b167e87":"![Preventive_Medicine-e1576294312614.png](attachment:Preventive_Medicine-e1576294312614.png)","94826d66":"##### 7-kNN","135dd966":"## Iteration 4: (with SMOTE and PCA)","58b016b3":"#### We now have cleaner records than before. All values are metric and not NA values.\n- We have 1567 rows and 447 columns before modeling","e96be80b":"##### 1-Decision tree","639dedef":"#### 28 columns with more than 50% NAs were removed","fbb0b487":"![image.png](attachment:e035d519-a7a6-40aa-9ded-d6c2564657df.png)","2533eda8":"##### 2-Logistic Regression","71db83b5":"## THE END. THANK YOU FOR YOUR ATTENTION.","506246ac":"The loads (loading scores) indicate \"how high a variable X loads on a factor Y\". \n\n(The i-th principal components can be selected via i in pca.components_ [0].)","ddea86e8":"NaiveBayes gave high BalanceAccuracy and TPR_Score (Recall), but it gave the poor FPR_Score (Fallout) in this unbalanced data set.","17b28715":"#### GridSearch","fd11df4e":"## Iteration 1: (Unbalanced data)","23da83cb":"## Data Cleaning\/Preprocessing","ef51090a":"Now we have a cleaner data set. No NaN values.","7e3f5450":"### Missing Values","ac23faa6":"All 1 values have been deleted, so you have to work with X again","d4c660ff":"It looks like n_components = 145 is suitable for% 95 total explained variance,","ce0b0c71":"#### There are 28 columns that contain more than 50% NAs. These must be removed.","001a91ba":"#### Use Algorithm","7ed5e8cf":"## Data Cleaning","4c9c9ed0":"### Use Algorithm","ba2f8ac2":"### Import and Load","a7934ec0":"### Balancing data","60067858":"**Lazy Predict**","d1b61a08":"### Evaluation (iteration 1)","beeb1be1":"SVC looks good, less FPR and large TPR with relatively good balance accuracy","c2581720":"#### There are 116 columns that contain unique values. These columns must be removed.  ","89902c1a":"### Use algorithms","4a1e5793":"## Iteration 3: (with SMOTE and Feature Selection Filter method)","cd7b3d67":"You could get 137 lines with just 2.5 whiskers. This number of records will not be enough. ","9d72cb01":"**There is unbalanced record. The Failing Values are only 104. Data balancing must be applied**","cc48181a":"#### Then data scaled with z_score are processed further.","086299e7":"#### Summary: \nIn this project the SECOM dataset was analyzed by machine learning methods with 5 iterations as a classification. For each iteration one tried little by little to achieve a better model result. Therefore 7 different algorithms (DecisionTree, Logistic Regression, Support Vector Machine, Random Forest, Gradient Boost, NaiveBayes and kNearestNeigbour) were used. After the data cleaning and EDA process, the data set was scaled with StandartScaler because there were many large and small features. After that, something special (oversampling, FeatureSelection, FeatureExtraction, HyperParameter optimization) was applied in each iteration. In the end, GradientBoost with only oversampled and scaled data set gave better results. The outliers can be checked for the next iteration and for each column one should decide whether 1.5xquarter is the correct limit for the real outliers.  ","2e670d76":"##### The data is split using the train_test_split function.","124daa50":"#### Missing values must be fill","d2432adf":"The first array contains the list of row numbers and second array respective column numbers, which means a Z-score higher than 3.","92b493c3":"### Data Scaling","15ea34f7":"##### 6-Naive Bayes"}}