{"cell_type":{"8a36ac73":"code","c12cbba4":"code","44cd3a19":"code","9382246d":"code","14d25ccf":"code","f971d494":"code","b15bd7c0":"code","22175fb2":"code","c40e366b":"code","499aeb91":"code","778738a2":"code","e2abe6d7":"code","d57aba35":"code","22f27f24":"code","23f129ed":"code","d4dda553":"code","75b53240":"code","925ea959":"code","c484eccd":"code","504b14dc":"code","ec7852e8":"code","cdebc389":"code","c4675d48":"code","254cb54f":"code","dab60fc5":"code","fec66967":"code","9bfc990a":"code","8441e18e":"code","ee15c476":"code","c585bed7":"code","8817c4e0":"code","68a6df7e":"code","f2da9180":"code","27ef95fa":"code","74264d18":"code","ff5f9cc2":"code","81a82aa4":"code","3da5c12d":"code","8a8473b7":"code","87e3e906":"code","c9a61d6c":"code","85461493":"code","4f48d033":"code","9a9528ac":"code","95a68228":"code","9ff71769":"code","24b9deea":"code","27065e65":"code","992d72dd":"code","39b312f2":"code","26e9a0de":"code","eb6d057c":"code","ac534743":"code","9408623d":"code","a5e61dc8":"code","6fa03658":"code","b441e77a":"markdown","5873b2fa":"markdown","f53e9f09":"markdown","67d3649d":"markdown","4f1e6691":"markdown","54d77a41":"markdown","121bd9e8":"markdown","33689c00":"markdown","a563b894":"markdown","f2c005c9":"markdown","ced1305c":"markdown","b0a0634e":"markdown","d498cfe2":"markdown","3f6a2dc9":"markdown","e1eab260":"markdown","cf2bccb6":"markdown","36fa88c7":"markdown","90e1ee16":"markdown","c7113c62":"markdown","8eb4041e":"markdown","cc82526a":"markdown","6ba6d15e":"markdown","4d72daa2":"markdown","0743a67e":"markdown","fbf8e8a4":"markdown","158468d4":"markdown","71737142":"markdown","e9088454":"markdown","babba98d":"markdown","5ce0960b":"markdown","15d8939a":"markdown","f2981b54":"markdown","aeec77c0":"markdown","3cae5e5a":"markdown","e00426b8":"markdown","c02ebbaa":"markdown","274b0288":"markdown","37d85619":"markdown"},"source":{"8a36ac73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c12cbba4":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","44cd3a19":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","9382246d":"data.head()\n","14d25ccf":"data.isnull().sum()","f971d494":"f, ax = plt.subplots(1,2, figsize=(10,5))\ndata['Survived'].value_counts().plot.pie( autopct='%1.1f%%')\nax[1].set_title('Survived')\nax[1].set_ylabel('')\nsns.countplot('Survived', data=data, ax=ax[0])","b15bd7c0":"data.groupby(['Sex', 'Survived'])['Survived'].count()","22175fb2":"f, ax = plt.subplots(1,2, figsize=(10,5))\ndata.groupby(['Sex'])['Survived'].mean().plot.bar(ax=ax[0])\nax[0].set_title = 'Survived vs Sex'\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])","c40e366b":"pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r')","499aeb91":"sns.countplot('Pclass', hue='Survived', data=data)","778738a2":"pd.crosstab(data.Pclass, [data.Sex, data.Survived], margins=True).style.background_gradient(cmap='summer_r')","e2abe6d7":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=data)","d57aba35":"print('Oldest Passenger was', data.Age.max())\nprint('Youngest Passenger was', data.Age.min())\nprint('Average Age of passengers was', data.Age.mean())","22f27f24":"f, ax = plt.subplots(1, 2, figsize=(10,5))\nsns.violinplot('Pclass', 'Age', hue='Survived', data=data, split=True, ax=ax[0])\nsns.violinplot('Sex', 'Age', hue='Survived', data=data, split=True, ax=ax[1])","23f129ed":"data['Initials'] = 0\nfor i in data:\n    data['Initials'] = data.Name.str.extract('([A-Za-z]+)\\.')","d4dda553":"pd.crosstab(data.Sex, data.Initials).style.background_gradient('summer_r')","75b53240":"data['Initials'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                         ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],\n                         inplace=True)","925ea959":"data.groupby('Initials')['Age'].mean()","c484eccd":"# assign age values to null accrding to initials\ndata.loc[(data.Age.isnull())&(data.Initials=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initials=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initials=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initials=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initials=='Other'),'Age']=46","504b14dc":"data.Age.isnull().any() #So no null values left finally ","ec7852e8":"f, ax = plt.subplots(1,2, figsize=(12,7))\nax[0].set_title('Survived = 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\nax[1].set_xticks(x1)\ndata[data.Survived==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black')\nax[1].set_title('Survived = 1')\ndata[data.Survived==1].Age.plot.hist(ax=ax[1], bins=20, edgecolor='black')","cdebc389":"pd.crosstab([data.Embarked, data.Pclass], [data.Sex, data.Survived], margins=True).style.background_gradient(cmap='summer_r')","c4675d48":"sns.factorplot('Embarked','Survived',data=data)","254cb54f":"# due to maximum passenger board in embarked S we fill Nan values with S\ndata['Embarked'].fillna('S',inplace=True)\ndata.Embarked.isnull().any()# Finally No NaN values","dab60fc5":"pd.crosstab(data.SibSp, data.Survived).style.background_gradient(cmap='summer_r')","fec66967":"sns.barplot(x='SibSp', y='Survived', data=data)","9bfc990a":"print('Highest Fare is ', max(data.Fare))\nprint('Lowest Fare is ', min(data.Fare))\nprint('Average Fare is ', data.Fare.mean())","8441e18e":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","ee15c476":"sns.heatmap(data=data.corr(), cmap='RdYlGn', annot=True, linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","c585bed7":"data['Age_band']= 0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head()","8817c4e0":"data['Age_band'].value_counts().to_frame()","68a6df7e":"sns.barplot('Age_band', 'Survived', data=data)","f2da9180":"data['Family_size'] = 0\ndata['Family_size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_size==0,'Alone']=1#Alone","27ef95fa":"f, ax = plt.subplots(1, 2, figsize=(12,5))\nsns.barplot('Family_size', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Family_size vs Survived')\nsns.barplot('Alone', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Alone vs Survived')","74264d18":"data['Fare_range'] = pd.qcut(data['Fare'], 4)\ndata.head()","ff5f9cc2":"# we convert fare range into bins as we did with Age_band\ndata['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3","81a82aa4":"sns.barplot('Fare_cat', 'Survived', data=data)","3da5c12d":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Initials'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","8a8473b7":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(15,8)\nplt.show()","87e3e906":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","c9a61d6c":"train, test = train_test_split(data, test_size=0.3, random_state=0, stratify=data.Survived)","85461493":"train_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[0]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[0]]\nX=data[data.columns[1:]]\nY=data['Survived']","4f48d033":"model = svm.SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nSVMPrediction = model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(SVMPrediction,test_Y))","9a9528ac":"model = svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nSVMLinearPrediction = model.predict(test_X)\nprint('Accuracy for linear SVM', metrics.accuracy_score(SVMLinearPrediction, test_Y))","95a68228":"model = LogisticRegression()\nmodel.fit(train_X, train_Y)\nLogisticPrediction = model.predict(test_X)\nprint('Accuracy for Logistic Regression', metrics.accuracy_score(LogisticPrediction, test_Y))","9ff71769":"model = DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nDecisionTreePrediction = model.predict(test_X)\nprint('Accuracy for Decsion Tree', metrics.accuracy_score(DecisionTreePrediction, test_Y))","24b9deea":"model = KNeighborsClassifier()\nmodel.fit(train_X, train_Y)\nKNNPrediction = model.predict(test_X)\nprint('Accuracy for KNN', metrics.accuracy_score(KNNPrediction, test_Y))","27065e65":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","992d72dd":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nNBPrediction=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(NBPrediction,test_Y))","39b312f2":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nRandomForestPrediction=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(RandomForestPrediction,test_Y))","26e9a0de":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction","eb6d057c":"kfold = KFold(n_splits=10, random_state=22)\nmodelMean = []\nmodelAccuracy = []\nModelStd = []\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor model in models:\n    cv_result = cross_val_score(model, X, Y, cv = kfold, scoring='accuracy')\n    modelMean.append(cv_result.mean())\n    ModelStd.append(cv_result.std())\n    modelAccuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':modelMean,'Std':ModelStd},index=classifiers)       \nnew_models_dataframe2","ac534743":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()\n","9408623d":"from sklearn.model_selection import GridSearchCV","a5e61dc8":"C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper = {'kernel': kernel, 'C': C, 'gamma': gamma}\nSVMGridSearchModel = GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\nSVMGridSearchModel.fit(X, Y)\nprint(SVMGridSearchModel.best_score_)\nprint(SVMGridSearchModel.best_estimator_)","6fa03658":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\nRFGridSearchModel=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\nRFGridSearchModel.fit(X,Y)\nprint(RFGridSearchModel.best_score_)\nprint(RFGridSearchModel.best_estimator_)","b441e77a":"# Decision Tree","5873b2fa":"# Feature Engineering And Data Cleaning\nNow what is Feature Engineering?\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n\nAn example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling.","f53e9f09":"# Logistic Regression","67d3649d":"# Thanks a lot for having a look at this notebook. If you found this notebook useful, Do Upvote.","4f1e6691":"# SibSip-->Discrete Feature","54d77a41":"Now change and check the different values of neighbours. The default is 5","121bd9e8":"# **Sex -> Categorical Feature**","33689c00":"# Observations for all features","a563b894":"# Radical Support Vector Machines","f2c005c9":"# Cross Validation","ced1305c":"The best score for **Rbf-Svm is 82.82%** with **C=0.4 and gamma=0.3**. For **RandomForest**, score is abt **81.9% with n_estimators=300**.","b0a0634e":"# Age_Band\nBecause Age is a continous varaible it is hard to group it. So to tackle this problem we can take two options. Either go for Normalization or Binning. Here use Binning.\n\nSo maximum Age is 80 and if we divide 80 into 5 bins. 80\/5 = 16. Bins Size = 16","d498cfe2":"# Linear Support Vector Machine","3f6a2dc9":"# Converting String Values into Numeric","e1eab260":"The looks interesting as male were more than females on board. But the ratio of survivied of females are higher than male.","cf2bccb6":"# Correlation Between Features","36fa88c7":"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is **No**, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as **model variance**.\n\nTo overcome this and get a generalized model,we use **Cross Validation**.","90e1ee16":"#  Hyper Parameter Tuning","c7113c62":"# **Analysing The Features**","8eb4041e":"# K-Nearest Neighbours (KNN)","cc82526a":"**Observations:**\n1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n\n2)The oldest Passenger was saved(80 years).\n\n3)Maximum number of deaths were in the age group of 30-40.","6ba6d15e":"# Family_Size And Alone\nWe create two new Features Family_size and Alone. Family_size tells us the size of the family and Alone is Boolean tells either a person is alone or not","4d72daa2":"We have null values in Age and we can't fill null values with mean, because we it is not good to assign 4 year old boy assign age of 24. To tackle this we extract initials from name and assign age accordingly","0743a67e":"# Fare_Range\nFare is also a continous feature. so we convert it into bins of 4","fbf8e8a4":"# # Predictive Modeling\nWe use 7 models to predict and analyze\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression","158468d4":"# Dropping UnNeeded Features","71737142":"# SVM","e9088454":"**Observations:**\n1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.\n\n3)For males, the survival chances decreases with an increase in age.","babba98d":"# PClass -> Ordinal Feature","5ce0960b":"# Random Forest","15d8939a":"# Random Forest","f2981b54":"Accoring to above graph. **Pclass 1** have more survival rate than **Pclass 3**. Which means money and status matters for survival rate. As we can see Pclass 3 have more passengers than Pclass 1 but survival rate is low than compare to others.\n\nLets check relationship between Sex and Pclass.","aeec77c0":"# Fare--> Continous Feature","3cae5e5a":"The highest coorelated feature is SibSp and Parch which is 0.41. So we don't need to exclude any feature and continue with our current features. ","e00426b8":"# Gaussian Naive Bayes","c02ebbaa":"* **SEX:** Women has high chance of survival as compare to men\n* **Pclass**: Pclass 1 has more survival chance as compare to Pclass3. And womens in Pclass 1 has alomost rate of survial 1.\n* **Age:** Children of age between 5-10 has a high chance of survival. Passengers with age group 15-35 dies alot\n* **Embarked:**  This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S. Passengers at Q were all from Pclass33\n* **Parch + SibSp:** Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.","274b0288":"# Embarked -> Categorical Data","37d85619":"# Age -> Continous Data"}}