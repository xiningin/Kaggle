{"cell_type":{"defb81cd":"code","3cad49e3":"code","13d997b1":"code","6a01499e":"code","0b6e8244":"code","2807493c":"code","9f77ce42":"code","db1552a7":"code","e13bff5c":"code","4cc37eed":"code","0002c14f":"code","24035b84":"code","f9564100":"code","2cc14909":"code","eacccbeb":"code","ce50df6b":"code","4bc1198f":"code","37944f0c":"code","153c6b66":"code","24837f0a":"code","634c32dd":"code","789fb9c3":"code","e8fd18cf":"code","47289044":"code","b33f9f95":"code","814b5c5b":"code","eceff357":"code","f586980f":"code","b58e70d1":"code","47c5d91c":"code","a5306608":"code","353ea783":"code","49e728a9":"code","bc09e9ac":"code","bdfd9986":"code","3bbbd4fa":"code","847b23fa":"code","eba72fbb":"code","1ca77e62":"code","e348dbe6":"code","573f95a6":"code","6cd9e191":"code","2890dd1b":"code","67c1169a":"code","b9181c26":"code","e979eb0a":"code","60c861a3":"code","8ace9171":"code","32509b83":"code","913e1777":"code","e7d54061":"code","058268b9":"code","023cc903":"code","bf759c93":"code","356da8d5":"code","ef395bd6":"code","bccb892a":"code","99d443be":"code","3c2afeeb":"code","66f77bc8":"code","1163775b":"code","00872a04":"code","46c5d9cb":"code","48c000be":"code","82f65da8":"code","fab5db37":"code","4b16a881":"code","d28d1166":"code","134d744c":"markdown","9239911f":"markdown","bd18499a":"markdown","3e514c23":"markdown","341827f0":"markdown","23b9d46e":"markdown","a12e216f":"markdown","367d1364":"markdown","a5d79dcb":"markdown"},"source":{"defb81cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # regular expression libary.\nimport nltk # Natural Language toolkit\nnltk.download(\"stopwords\")  #downloading stopwords\nnltk.download('punkt')\nfrom nltk import word_tokenize,sent_tokenize\nnltk.download('wordnet')\nimport nltk as nlp\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom wordcloud import WordCloud \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,precision_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3cad49e3":"df=pd.read_csv(\"\/kaggle\/input\/ttc4900\/7allV03.csv\")","13d997b1":"df_copy=df.copy()","6a01499e":"df_copy.head()","0b6e8244":"import seaborn as sns \n\nsns.countplot(\"category\",data=df_copy)","2807493c":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nlabels=le.fit_transform(df_copy.category)","9f77ce42":"labels","db1552a7":"\n\"\"\"\n4==Siyaset\n0==Dunya\n1==ekonomi\n2==kultur\n3==saglik\n5==spor\n6==teknoloji\n\n\"\"\"","e13bff5c":"from nltk.corpus import stopwords\n\nnltk.download(\"stopwords\")","4cc37eed":"text_list=[]\n\nfor text in df_copy.text:\n    text = text.lower()  #B\u00fcy\u00fck harften -K\u00fc\u00e7\u00fck harfe \u00e7evirme\n    text = re.sub(\"[^abc\u00e7defg\u011fh\u0131ijklmno\u00f6prs\u015ftu\u00fcvyz]\",\" \",text)\n    text=nltk.word_tokenize(text) # splits the words that are in the sentence from each other.\n    text =[word for word in text if not word in set(stopwords.words(\"turkish\"))]\n    lemma=nlp.WordNetLemmatizer()\n    text=[lemma.lemmatize(word) for word in text] # this code finds the root of the word for a word in the sentence and change them to their root form.\n    text=\" \".join(text)\n    text_list.append(text) # store sentences in list","0002c14f":"len(text_list)","24035b84":"from sklearn.feature_extraction.text import CountVectorizer #Bag of Words\n\nmax_features=500 # \"number\" most common(used) words in reviews\n\ncount_vectorizer=CountVectorizer(max_features=max_features) \n\nsparce_matrix=count_vectorizer.fit_transform(text_list).toarray()","f9564100":"sparce_matrix.shape #4900 sentences 500 most used words","2cc14909":"sparce_matrix[0:10,0:20]","eacccbeb":"print(\"Top {} the most used word by reviewers: {}\".format(max_features,count_vectorizer.get_feature_names()))","ce50df6b":"data=pd.DataFrame(count_vectorizer.get_feature_names(),columns=[\"Words\"])","4bc1198f":"data.head()","37944f0c":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(12,12))\nwordcloud=WordCloud(background_color=\"black\",width=1024,height=768).generate(\" \".join(data.Words[5:]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","153c6b66":"X=sparce_matrix\ny=labels","24837f0a":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nprint(\"x_train\",X_train.shape)\nprint(\"x_test\",X_test.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","634c32dd":"lgbm_model=LGBMClassifier()\n\nlgbm_model.fit(X_train,y_train)","789fb9c3":"y_pred=lgbm_model.predict(X_test)","e8fd18cf":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))","47289044":"from sklearn.model_selection import RandomizedSearchCV","b33f9f95":"lgbm_params={\"n_estimators\":[100,500,100,2000],\n            \"subsample\":[0.6,0.8,1.0],\n            \"learning_rate\":[0.1,0.01,0.02,0.05],\n            \"min_child_samples\":[5,20,10],\n            \"max_depth\":[3,4,5,6]}\n\nlgbm=LGBMClassifier()\nlgbm_cv=RandomizedSearchCV(lgbm,lgbm_params,cv=10,n_jobs=-1,verbose=2)\nlgbm_cv_model=lgbm_cv.fit(X_train,y_train)","814b5c5b":"lgbm_cv_model.best_params_","eceff357":"lgbm=LGBMClassifier(learning_rate= 0.01,max_depth= 3,min_child_samples= 10,n_estimators= 2000,subsample= 0.6)\nlgbm_tuned_model=lgbm.fit(X_train,y_train)","f586980f":"y_pred=lgbm_tuned_model.predict(X_test)","b58e70d1":"cm=confusion_matrix(y_test,y_pred)","47c5d91c":"plt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') #835 true predictions","a5306608":"print(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))","353ea783":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model=RandomForestClassifier(random_state=42)\nrf_model.fit(X_train,y_train)","49e728a9":"y_pred=rf_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))\n\ncm=confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') ","bc09e9ac":"rf_params={\"max_depth\":[2,5,8,10],\n           \"max_features\":[2,5,8],\n           \"n_estimators\":[10,500,1000],\n          \"min_samples_split\":[2,5,10]}","bdfd9986":"rf=RandomForestClassifier(random_state=42)\nrf_cv=RandomizedSearchCV(rf,rf_params,cv=10,n_jobs=-1,verbose=2)\nrf_cv_model=rf_cv.fit(X_train,y_train)","3bbbd4fa":"rf_cv_model.best_params_","847b23fa":"rf_tuned_model=RandomForestClassifier(random_state=42,max_depth=10,max_features= 2,min_samples_split= 5,n_estimators= 1000)","eba72fbb":"rf_tuned_model.fit(X_train,y_train)\n\ny_pred=rf_tuned_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))\n\ncm=confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') ","1ca77e62":"xgb=XGBClassifier()\n\nxgb_model=xgb.fit(X_train,y_train)\n\ny_pred=xgb_model.predict(X_test)\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\nprint(\"Precision:\",precision_score(y_test, y_pred,average=\"micro\"))\n\ncm=confusion_matrix(y_test,y_pred)\n\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,fmt='d') ","e348dbe6":"print(classification_report(y_test, y_pred)) \n# classification report is good function for seeing how well our model predict labels in each class  ","573f95a6":"X_test[10] # I use this text for prediction","6cd9e191":"y_test[10] # true label of X_test[10] 3== health","2890dd1b":"xgb_model.predict(X_test[10].reshape(-1,500)) # predicted label of X_test[10] \"Predicted Correctly\"","67c1169a":"import collections \n\nfor index,liste in enumerate(sparce_matrix):\n    if collections.Counter(liste) == collections.Counter(X_test[10]): # searching X_test[10] in space matrix and Turn its index\n        print(index)\n        \n        \n    ","b9181c26":"count_vectorizer.inverse_transform(sparce_matrix[2827]) \n# these are words from X_test[10](tokenized and cleared) and as you see there are related word with health(3) like ila\u00e7==medicine,bilim==science","e979eb0a":"df_copy.text[2827] # original text ","60c861a3":"X=df_copy.text.copy()\n","8ace9171":"X.head()","32509b83":"X_list=[] # store text in this list\n\nfor text in X:\n    text = text.lower()\n    text = re.sub(\"[^abc\u00e7defg\u011fh\u0131ijklmno\u00f6prs\u015ftu\u00fcvyz]\",\" \",text)\n    text = text.split()\n    text =[word for word in text if not word in set(stopwords.words(\"turkish\"))]\n    text=\" \".join(text)\n    X_list.append(text) # store sentences in list","913e1777":"y_label=[]\n\nfor i in labels:\n    y_label.append(i)","e7d54061":"y_label[0:10]","058268b9":"X_train, X_test, y_train, y_test = train_test_split(X_list, y_label, test_size=0.2, random_state=42,shuffle=True)","023cc903":"print(len(X_train)) # 3920 sentences\nprint(len(y_train)) #3920 Labels\nprint(len(X_test))  # 980 sentences\nprint(len(y_test)) # 980 labels","bf759c93":"max_lenght=100\n\ntokenizer = Tokenizer() \ntokenizer.fit_on_texts(X_train)\n\n\nword_index = tokenizer.word_index # creating word dict for words in training\n\nsequences = tokenizer.texts_to_sequences(X_train)  # replacing words with the number corresponding to them in the dictionary(word_index)\n\nX_train_padded = pad_sequences(sequences, padding='post',maxlen=max_lenght) # padding words\n\nprint(len(word_index)) # I have 94836 words in my dictionary","356da8d5":"print(\"Original Version:\",X_train[0])\nprint(\"---------------------------------\")\nprint(\"Padded version\",X_train_padded[0]) \nprint(\"---------------------------------\")\nprint(\"Tokenized version:\",sequences[0])  # change words with number that corresponding to word word_index\nprint(\"---------------------------------\")\nprint(\"Shape after the padding:\",X_train_padded.shape) # make our input same size","ef395bd6":"X_test_sequences = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_sequences,padding=\"post\",maxlen=max_lenght)","bccb892a":"print(\"Original Version:\",X_test[0])\nprint(\"---------------------------------\")\nprint(\"Padded version\",X_test_padded[0])   # make inputs same size\nprint(\"---------------------------------\")\nprint(\"Tokenized version:\",X_test_sequences[0]) # change words with number that corresponding to word word_index\nprint(\"---------------------------------\")\nprint(\"Shape after the padding:\",X_test_padded.shape) # make inputs same size","99d443be":"import tensorflow as tf\n\nvocab_size = len(tokenizer.word_index)+1\nembedding_dim=16\n\nmodel = tf.keras.Sequential([\n    \n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=100),\n    \n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.BatchNormalization(),\n    \n    tf.keras.layers.Flatten(),\n    \n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(7, activation='softmax')\n])\n\nadam= tf.keras.optimizers.Adam(lr=0.01) \n\n\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\nmodel.summary()","3c2afeeb":"y_train_label=np.asarray(y_train).reshape(-1,1)\ny_test_label=np.asarray(y_test).reshape(-1,1)","66f77bc8":"num_epochs = 10\nBATCH_SIZE=64\nhistory=model.fit(X_train_padded,y_train_label,batch_size=BATCH_SIZE ,epochs=num_epochs, validation_data=(X_test_padded,y_test_label))","1163775b":"import matplotlib.pyplot as plt\n\n\nplt.plot(history.history[\"accuracy\"],color=\"green\")\nplt.plot(history.history[\"loss\"],color=\"red\")\nplt.title(\"Train accuracy and Train loss\")\nplt.legend([\"Accuracy\",\"Loss\"])\nplt.grid()","00872a04":"plt.plot(history.history[\"val_accuracy\"],color=\"blue\")\nplt.plot(history.history[\"val_loss\"],color=\"orange\")\nplt.title(\"Test accuracy and Test loss\")\nplt.legend([\"Val_accuracy\",\"Val_loss\"])\nplt.grid()","46c5d9cb":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train_padded,y_train_label)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test_padded,y_test_label)[1]*100 , \"%\")","48c000be":"pred = model.predict_classes(X_test_padded)","82f65da8":"pred[10:20]","fab5db37":"y_test_label[10:20]","4b16a881":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,pred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,annot=True,linecolor=\"white\",fmt='')\n","d28d1166":"print(classification_report(y_test_label,pred)) # model is better when classify 3 and 5 classes","134d744c":"# LightGBM ","9239911f":"## Turkish Text Classification with Deep Learning","bd18499a":"## XGBoost Classifier","3e514c23":"I didt tune my Xgboost model because it took too much time.","341827f0":"So,This is end of my notebook. Thanks for you to took a look at  my notebook.I hope you like it :)\n\nI changed my LSTM model more than 15 times but I didnt prevent overfitting. \nI think machine learning algorithm is much better than LSTM model for this dataset but You may achieve higher score then me.\n\nif you have a solution please feel free to write your thoughts on comment section","23b9d46e":"## Random Forest Classifier","a12e216f":"## Model Tunning","367d1364":"LSTM model is overfitted on training data ","a5d79dcb":"## Model Tunning"}}