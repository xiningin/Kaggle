{"cell_type":{"08f94a21":"code","49ad7aec":"code","75379d70":"code","6f43cce8":"code","9f2342aa":"code","31163d5f":"code","4278e0d8":"code","5ba2a23e":"code","75c54aaa":"code","d623308d":"code","974e0857":"code","c84a10dc":"code","76e7e0a0":"code","5458f3b0":"code","fe04cbba":"code","86988593":"code","550936c8":"code","6d1ea31b":"code","620b7ed9":"code","de5f4ecd":"code","e566d290":"code","375f53ff":"code","b116d089":"code","162f946d":"code","9cbe1222":"code","b19aed7c":"code","bec7e639":"code","83597a6c":"code","1320cb41":"code","f99ca7d3":"code","212c2fde":"code","4c7b349f":"code","eb743624":"code","77a36427":"code","7b7a88a0":"code","f01615f3":"code","87e873fe":"code","ac95343b":"code","b6f4d7e6":"code","ab861e59":"code","8366d447":"code","7f912558":"code","09b01145":"code","76b6299e":"code","dd6bb198":"code","4514c96f":"code","3bc0a6cd":"code","0acef41c":"code","9316b78e":"code","9cc4afc8":"code","ec2ecece":"code","3e4a0bc0":"code","53fcb156":"markdown","4ead4fe5":"markdown","470b8f6a":"markdown","bf2d2708":"markdown","2ab30913":"markdown","7ba18eff":"markdown","2dd26c87":"markdown","f429592e":"markdown","37cd3d9d":"markdown","2976a628":"markdown","4ef7b8d4":"markdown","fc599b6d":"markdown","95b38012":"markdown","1220736f":"markdown","44dd67be":"markdown","480344f5":"markdown","0670dcfd":"markdown","9f81b5cb":"markdown","0cbc001c":"markdown","306b34c6":"markdown","3a0db727":"markdown","f5c5cfc2":"markdown","4dd0a8b9":"markdown","a350d46d":"markdown","2b0a1fb5":"markdown"},"source":{"08f94a21":"!pip uninstall keras --yes \n!pip install keras==2.6.0\n!pip uninstall tensorflow --yes\n!pip install tensorflow==2.6.0 \nprint(\"pip installs complete\")","49ad7aec":"from IPython.core.display import HTML\nHTML(\"<script>Jupyter.notebook.kernel.restart()<\/script>\")","75379d70":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport plotly\nimport sklearn\nfrom sklearn import preprocessing\n\nfrom tensorflow.keras.layers import IntegerLookup\nfrom tensorflow.keras.layers import Normalization\nfrom tensorflow.keras.layers import StringLookup","6f43cce8":"print(tf.__version__)\nprint(keras.__version__)\n\n# Should be versions 2.6.0","9f2342aa":"raw_data = pd.read_csv(\"..\/input\/advertising-missingrecords\/advertising_missingdata.csv\")\nraw_data.head()","31163d5f":"raw_data.describe().transpose()","4278e0d8":"df = raw_data.copy()","5ba2a23e":"df.head()","75c54aaa":"df.info()","d623308d":"# histogram of Daily Time Spent on Site by gender\nimport plotly.express as px\nfig = px.histogram(df, x=\"Daily Time Spent on Site\", color=\"Male\", opacity=0.65, width=1200)\nfig.show()","974e0857":"fig2 = px.histogram(df, x=\"Daily Internet Usage\", color=\"Male\", opacity=0.65, width=1200)\nfig2.show()","c84a10dc":"fig3 = px.histogram(df, x=\"Area Income\", color=\"Male\", opacity=0.70, width=1200)\nfig3.show()","76e7e0a0":"import plotly.express as px\ndf[\"Clicked on Ad\"] = df[\"Clicked on Ad\"].astype(str)\ndf[\"Male\"] = df[\"Male\"].astype(str)\nfig4 = px.scatter(df, x=\"Daily Internet Usage\", y=\"Daily Time Spent on Site\", color=\"Clicked on Ad\", facet_col=\"Male\",opacity=0.75)\nfig4.show()","5458f3b0":"sns.set_theme(style=\"ticks\")\nsns.pairplot(df, hue=\"Clicked on Ad\")","fe04cbba":"fig = px.histogram(df, x=\"Clicked on Ad\", opacity = 0.65, width=600)\nfig.update_layout(bargap=0.2)\nfig.show()","86988593":"df['Age_bracket'] = pd.qcut(df['Age'],5).astype(str)\npd.qcut(df['Age'],5)","550936c8":"df.groupby(['Age_bracket']).mean()['Area Income']","6d1ea31b":"df['Age_bracket'] = df['Age_bracket'].replace('(18.999, 28.0]', 1).astype(str)\ndf['Age_bracket'] = df['Age_bracket'].replace('(28.0, 32.0]', 2).astype(str)\ndf['Age_bracket'] = df['Age_bracket'].replace('(32.0, 37.0]', 3).astype(str)\ndf['Age_bracket'] = df['Age_bracket'].replace('(37.0, 44.0]', 4).astype(str)\ndf['Age_bracket'] = df['Age_bracket'].replace('(44.0, 61.0]', 5).astype(str)\ndf.head()","620b7ed9":"Age_bracket = df.groupby('Age_bracket').median()['Area Income']\n\nfor i in range(0,5):\n  print('Median Income of age group {}s: {}'.format(i, Age_bracket[i]))\nprint('Median age of all passengers: {}'.format(df['Area Income'].median()))\n\n# Filling the missing values in Area Income with the medians of Age groups\ndf['Area Income'] = df.groupby('Age_bracket')['Area Income'].apply(lambda x: x.fillna(x.mean()))","de5f4ecd":"df.head()","e566d290":"# Missing values have been imputed\ndf[df['Area Income'].isnull()]","375f53ff":"df['Time of Day'] = pd.to_datetime(df['Timestamp']).dt.hour\ndf['Day of Month'] = pd.to_datetime(df['Timestamp']).dt.day\ndf['Day Name'] = pd.to_datetime(df['Timestamp']).dt.day_name()\ndf['Month of Year'] = pd.to_datetime(df['Timestamp']).dt.month_name()\ndf.head()","b116d089":"df['Clicked on Ad'] = df['Clicked on Ad'].astype(int)","162f946d":"# Load packages to handle the Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","9cbe1222":"# List of all features \nfeatures = ['Daily Time Spent on Site', 'Age', 'Area Income',\n       'Daily Internet Usage', 'Ad Topic Line', 'City', 'Male', 'Country',\n       'Timestamp', 'Age_bracket', 'Time of Day',\n       'Day of Month', 'Day Name', 'Month of Year', 'Clicked on Ad']\ndf =df.reindex(columns=features)\ndata = df.copy()","b19aed7c":"# Dropping columns I will not be using \ndf.drop(columns=['Age_bracket', 'Timestamp', 'Ad Topic Line', 'City'], inplace=True)\ntarget = df['Clicked on Ad']\ndf.drop(columns='Clicked on Ad', inplace=True)\n\n# Changing data types back to string\ndf['Time of Day'] = df['Time of Day'].astype(str)\ndf['Day of Month'] = df['Day of Month'].astype(str)","bec7e639":"# Splitting data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(df, target, test_size = 0.2, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)","83597a6c":"print(\"Training set shape: \", X_train.shape[0])\nprint(\"Validation set shape: \", X_val.shape[0])\nprint(\"Testing set shape: \", X_test.shape[0])","1320cb41":"# List of all numeric features\nnumeric_features=  ['Daily Time Spent on Site', 'Age','Area Income', 'Daily Internet Usage']\n\n# Numeric feature pipeline\nnumeric_transformer = Pipeline(steps=[\n    # We have already dealt with missing values early (imputer is not necessary here)\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# List of all categorical features\ncategorical_features = ['Male', 'Country', 'Time of Day', 'Day of Month', 'Day Name', 'Month of Year']\n# Categorical feature transformer\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Combine each transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n                  ('num', numeric_transformer, numeric_features),\n                  ('cat', categorical_transformer, categorical_features)])\n\n# Add classifier to preprocessing pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\n# Train model\nclf.fit(X_train, y_train)\nprint(\"Model Score: %.3f\" % clf.score(X_val, y_val))","f99ca7d3":"numeric_features=  ['Daily Time Spent on Site', 'Age','Area Income', 'Daily Internet Usage']\n\n# Numeric feature pipeline\nnumeric_transformer = Pipeline(steps=[\n    # We have already dealt with missing values early (imputer is not necessary here)\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\n# List of all categorical features\ncategorical_features = ['Male', 'Country', 'Time of Day', 'Day of Month', 'Day Name', 'Month of Year']\n# Categorical feature transformer\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Combine each transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n                  ('num', numeric_transformer, numeric_features),\n                  ('cat', categorical_transformer, categorical_features)])","212c2fde":"from sklearn.ensemble import RandomForestClassifier\n\n# Add classifier to preprocessing pipeline\nclf_rf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', RandomForestClassifier())])\n\n# Train model\nclf_rf.fit(X_train, y_train)\nprint(\"Model Score: %.3f\" % clf_rf.score(X_val, y_val))","4c7b349f":"from sklearn.compose import make_column_selector as selector\nfrom sklearn.linear_model import LogisticRegressionCV \n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, selector(dtype_exclude=\"object\")),\n    ('cat', categorical_transformer, selector(dtype_include=\"category\"))])\n\nclf2 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\n\nclf2.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf2.score(X_val, y_val))","eb743624":"from sklearn.metrics import classification_report\ny_pred = clf.predict(X_test)\ntarget_names = ['0', '1']\nprint(classification_report(y_test, y_pred, target_names=target_names))","77a36427":"from sklearn.metrics import plot_confusion_matrix\nnp.set_printoptions(precision=2)\n\nclass_names = clf.classes_\n\nplot_confusion_matrix(clf, X_test, y_test,\n                        display_labels=class_names,\n                        cmap=plt.cm.Blues)","7b7a88a0":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, clf_rf.predict_proba(X_test)[:,1])\nlr_auc = roc_auc_score(y_test, clf.predict(X_test))\nrf_roc_auc = roc_auc_score(y_test, clf_rf.predict(X_test))\n\n\nplt.figure(figsize=(9,7))\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lr_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier (area = %0.2f)' % rf_roc_auc)\n\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base',color = \"black\", linestyle='dashed')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","f01615f3":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"_______________________________________________________________________\")\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n\n        print(\"_______________________________________________________________________\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")\n\n\nprint_score(clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(clf, X_train, y_train, X_test, y_test, train=False)\nprint_score(clf_rf, X_train, y_train, X_test, y_test, train=True)\nprint_score(clf_rf, X_train, y_train, X_test, y_test, train=False)","87e873fe":"from sklearn.metrics import average_precision_score\ny_score = clf.decision_function(X_test)\naverage_precision = average_precision_score(y_test, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","ac95343b":"data.drop(columns=['Ad Topic Line', 'City', 'Timestamp', 'Age_bracket'], inplace=True)","b6f4d7e6":"# Create training and validation set\n\nval_dataframe = data.sample(frac=0.2, random_state=10)\ntrain_dataframe = data.drop(val_dataframe.index)\n\nprint(\n    \"Using %d samples for training and %d for validation\"\n    % (len(train_dataframe), len(val_dataframe))\n)","ab861e59":"# Function to extract labels, convert dataframe in Tensorflow Dataset and shuffle\ndef dataframe_to_dataset(dataframe):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop(\"Clicked on Ad\")\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    ds = ds.shuffle(buffer_size=len(dataframe))\n    return ds\n\n\ntrain_ds = dataframe_to_dataset(train_dataframe)\nval_ds = dataframe_to_dataset(val_dataframe)","8366d447":"for x, y in train_ds.take(1):\n    print(\"Input:\", x)\n    print(\"Target:\", y)","7f912558":"# Batch the data for performance during training\n\ntrain_ds = train_ds.batch(50)\nval_ds = val_ds.batch(50)","09b01145":"from tensorflow.keras.layers import IntegerLookup\nfrom tensorflow.keras.layers import Normalization\nfrom tensorflow.keras.layers import StringLookup\n\n\ndef encode_numerical_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature\n\ndef encode_categorical_feature(feature, name, dataset, is_string):\n    lookup_class = StringLookup if is_string else IntegerLookup\n    # Create a lookup layer which will turn strings into integer indices\n    lookup = lookup_class(output_mode=\"binary\")\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the set of possible string values and assign them a fixed integer index\n    lookup.adapt(feature_ds)\n\n    # Turn the string input into integer indices\n    encoded_feature = lookup(feature)\n    return encoded_feature","76b6299e":"# Convert columns into form we need\ndata['Male'] = data['Male'].astype('string')\ndata['Country'] = data['Country'].astype('string')\ndata['Day Name'] = data['Day Name'].astype('string')\ndata['Month of Year'] = data['Month of Year'].astype('string')","dd6bb198":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom functools import partial","4514c96f":"tf.keras.backend.clear_session() # Resets all keras states\n\ntf.random.set_seed(45)\n\n# Categorical features encoded as integers\ntime = keras.Input(shape=(1,), name=\"Time of Day\", dtype=\"int64\")\nday_month = keras.Input(shape=(1,), name=\"Day of Month\", dtype=\"int64\")\n\n\n# Categorical feature encoded as string\nmale = keras.Input(shape=(1,), name=\"Male\", dtype=\"string\")\ncountry = keras.Input(shape=(1,), name=\"Country\", dtype=\"string\")\nday_name = keras.Input(shape=(1,), name=\"Day Name\", dtype=\"string\")\nmonth = keras.Input(shape=(1,), name=\"Month of Year\", dtype=\"string\")\n\n# Numerical features\ninternet_usage = keras.Input(shape=(1,), name=\"Daily Internet Usage\")\nincome = keras.Input(shape=(1,), name=\"Area Income\")\ntime_on_site = keras.Input(shape=(1,), name=\"Daily Time Spent on Site\")\nage = keras.Input(shape=(1,), name=\"Age\")\n\n\nall_inputs = [\n    time,\n    day_month,\n    male,\n    country,\n    day_name,\n    month,\n    internet_usage,\n    income,\n    time_on_site,\n    age\n]\n\n# Integer categorical features\ntime_encoded = encode_categorical_feature(time, \"Time of Day\", train_ds, False)\nday_month_encoded = encode_categorical_feature(day_month, \"Day of Month\", train_ds, False)\n\n\n# String categorical features\nmale_encoded = encode_categorical_feature(male, \"Male\", train_ds, True)\ncountry_encoded = encode_categorical_feature(country, \"Country\", train_ds, True)\nday_name_encoded = encode_categorical_feature(day_name, \"Day Name\", train_ds, True)\nmonth_encoded = encode_categorical_feature(month, \"Month of Year\", train_ds, True)\n\n# Numerical features\ninternet_usage_encoded = encode_numerical_feature(internet_usage, \"Daily Internet Usage\", train_ds)\nincome_encoded = encode_numerical_feature(income, \"Area Income\", train_ds)\ntime_on_site_encoded = encode_numerical_feature(time_on_site, \"Daily Time Spent on Site\", train_ds)\nage_encoded = encode_numerical_feature(age, \"Age\", train_ds)\n\n\nall_features = layers.concatenate(\n    [\n        time_encoded,\n        day_month_encoded,\n        male_encoded,\n        country_encoded,\n        day_name_encoded,\n        month_encoded,\n        internet_usage_encoded,\n        income_encoded,\n        time_on_site_encoded,\n        age_encoded\n    ]\n)\n\n\n# Create thin wrapper for Dense layer\nRegularizedDense = partial(keras.layers.Dense,\n                           activation = \"relu\",\n                           # Add regularization to improve generalization\n                           kernel_regularizer = keras.regularizers.l2(0.03))\n\n# Model Structure\nx = RegularizedDense(64)(all_features)\nx = RegularizedDense(32)(x)\nx = RegularizedDense(16)(x)\nx = layers.Dropout(0.50)(x) # Help prevent overfitting\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\n\n# Compile Model\nmodel = keras.Model(all_inputs, output)\nmodel.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])","3bc0a6cd":"keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")","0acef41c":"# Early Stopping Callback\nes_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=4,\n                                            restore_best_weights=True)\n\n# Callback to control learning rate\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.20,\n                              patience=3, min_lr=0.001)","9316b78e":"history = model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[reduce_lr, es_callback])","9cc4afc8":"model.evaluate(x = val_ds)","ec2ecece":"def visualize_accuracy(history, title):\n    accuracy = history.history[\"accuracy\"]\n    val_accuracy = history.history[\"val_accuracy\"]\n    epochs = range(len(accuracy))\n    plt.figure(figsize=(9,7))\n    plt.plot(epochs, accuracy, \"b\", label=\"Training accuracy\")\n    plt.plot(epochs, val_accuracy, \"r\", label=\"Validation accuracy\")\n    plt.title(title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()\n\n\nvisualize_accuracy(history, \"Training and Validation Accuracy\")","3e4a0bc0":"def visualize_loss(history, title):\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    epochs = range(len(loss))\n    plt.figure(figsize=(9,7))\n    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n    plt.title(title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\nvisualize_loss(history, \"Training and Validation Loss\")","53fcb156":"**Lets explore the target distribution to see which model evaluation measures will be best:**\n\n- The target variable, `Clicked on Ad` classes are balanced.\n  - Accuracy will be a suitable performance metric for our model.","4ead4fe5":"## **Split data into training and validation sets**","470b8f6a":"- **Another way to construct Pipeline by specifying data types to handle**","bf2d2708":"**Many variables appear to have clear linear seperation**\n- As aforementioned, this task should not require a sophisticated model to perform well.","2ab30913":"## **Evaluate**","7ba18eff":"**In this data, the Income distributions by gender are similar.  It does appear that replace missing `Area Income` values with group center statistics is much more representative to the sample** \n- We will explore other options after further exploration.","2dd26c87":"# **Neural Network**\n- I will train a neural network to predict Ad clicks based on the Advertising Dataset","f429592e":"## **Timestamp Data**\n- I will extract some additional features from the Timestamp feature to make it more useful for our model\n- From the Timestamp I will create:\n  - Hour of Day\n  - Month of Year\n  - Day of Week\n  - Day of Month","37cd3d9d":"- Missing 250 values for Area Income Feature.\n- lets explore the data and see what may be the best method for imputation","2976a628":"# **Preprocessing & Model Pipeline**","4ef7b8d4":"**The goal of this project is to use the advertisement dataset to build a classification model to predict whether someone click on the ad or not**\n\n- First we will do some quick exploration of the data","fc599b6d":"## **Random Forest Classifier**","95b38012":"## ROC Curve","1220736f":"# **Import Data**","44dd67be":"**Based on the Plot below, it appears `Daily time spent on the site` and `Daily internet` usage are important in determining whether a person Click on Ad or not.**\n- There is clear linear seperation between Clicking on an Ad or not\n- A simple model will likely be suitable for the classification task","480344f5":"## **Plot Model Architecture**","0670dcfd":"## **Compile Model**","9f81b5cb":"## Evaluation Metrics","0cbc001c":"**Found this nice classification function by [Fares Sayah](https:\/\/www.kaggle.com\/faressayah)**","306b34c6":"## **Logistic Regression - Model Pipeline**","3a0db727":"## **Train Model**","f5c5cfc2":"# Import Packages","4dd0a8b9":"# **Data Exploration**","a350d46d":"# **Data Cleaning**","2b0a1fb5":"## **Missing Values: Area Income**\n\n- I will impute missing values in `Area Income` with means from Age Group"}}