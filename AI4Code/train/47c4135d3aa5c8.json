{"cell_type":{"882ef2ba":"code","4568b713":"code","c2764230":"code","db55b668":"code","5a3b551d":"code","b97173ce":"code","e4582912":"code","9c03d749":"code","4d44204c":"code","172f5489":"code","3f82ee71":"code","929c5506":"code","c12ad8da":"code","e120613f":"code","21221d13":"code","ea1c2e58":"code","392c6480":"code","4ae91577":"code","5a7b900f":"code","9d407788":"code","65e0c2a1":"code","0ff75d92":"code","20c0ed77":"code","920c6f5f":"code","8ae8480f":"code","93208259":"code","f6c68168":"code","3eb581e1":"code","ddb7d05f":"code","8a65212d":"code","33c88be9":"code","fd42c111":"code","ba6078a0":"code","ce8d1112":"code","d0f3591f":"code","b9be7949":"code","609b5515":"markdown","0c55fdc3":"markdown","445fd3ac":"markdown","51371b37":"markdown","9cc73d66":"markdown","c59db86c":"markdown","6497248e":"markdown","5d46fed5":"markdown","19759faa":"markdown","6e3ba31e":"markdown","2c64c9a1":"markdown","3b51b788":"markdown","94746a1c":"markdown","e6d3c12b":"markdown","0726f170":"markdown"},"source":{"882ef2ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4568b713":"# Loading necessary libraries\nimport pandas as pd\nimport numpy as np\n\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, download_plotlyjs\ninit_notebook_mode(connected = True)","c2764230":"directory = '..\/input\/airline-passenger-satisfaction\/'\nfiles = ['train.csv', 'test.csv']\n\nprint(f'Loading training data from csv file...')\nraw_data_train = pd.read_csv(directory + files[0])\nprint('Training dataset loaded.')\n\nprint(f'Loading test data from csv file...')\nraw_data_test = pd.read_csv(directory + files[1])\nprint('Test dataset loaded.')","db55b668":"# Function defined to check medata of a dataframe\ndef master_dataframe(dataframe):\n    df_metadata = pd.DataFrame({'Datatype': dataframe.dtypes,\n                                \"Null Values\": dataframe.isna().sum(),  \n                                \"Null %\": round(dataframe.isna().sum()\/len(dataframe)*100, 2),\n                                \"No: Of Unique Values\": dataframe.nunique()})\n    \n    df_describe = dataframe.describe(include='all').T\n    \n    df_metadata = df_metadata.join(df_describe)  \n\n    return df_metadata","5a3b551d":"# Checking metadata of the training dataset\nmaster_dataframe(raw_data_train)","b97173ce":"# Let's replace the Null values in the field \"Arrival Delay in Minutes\" with zero assuming that they were not delayed.\n# You may also choose to drop the rows completely as it would not make much of a difference as, we will be removing only 0.3% of the rows.\n\ndata_cleaned = raw_data_train.copy()\ndata_cleaned = data_cleaned.fillna(0)\ndata_cleaned['Arrival Delay in Minutes'].isna().sum()","e4582912":"# Let's drop the fields \"Unnamed: 0\" and \"id\".\ndata_cleaned.drop(columns = ['Unnamed: 0', 'id'], axis = 1, inplace = True)\ndata_cleaned.columns","9c03d749":"# Let's divide our data into Categorical and Continous dataset.\n\ncat_data_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Inflight wifi service',\n       'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'satisfaction']\ncont_data_cols = ['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']","4d44204c":"# Function to show the frequency distribution table and bar chart of the distribution of the variables.\ndef frequency_distribution(feature):\n    if feature in cat_data_cols:\n        freq_dist = data_cleaned[feature].value_counts().reset_index()\n        freq_dist.rename(columns = {feature: 'Frequency', 'index': feature}, inplace = True)\n        freq_dist['% Of Distribution'] = round(freq_dist['Frequency']\/freq_dist['Frequency'].sum() * 100, 2)\n        freq_dist[feature] = freq_dist[feature].astype(str)\n        freq_dist.sort_values(by = ['% Of Distribution'], ascending = True, inplace = True)\n        \n    else:\n        return 'Enter a valid Categorical feature from the data set'\n        exit()\n\n    data = [go.Bar(x = freq_dist[feature], \n                   y = freq_dist['% Of Distribution'], \n                   text = freq_dist['% Of Distribution'], \n                   textposition = 'outside', \n                   textfont = {'color': 'white'},\n                   marker = dict(color = freq_dist['% Of Distribution'], \n                                 line = {'color': 'white', 'width': 1.5}))]\n    \n    layout = go.Layout(title = dict(text = 'Frequency Distribution : '+ feature , \n                                    x = 0.5, \n                                    y = 0.88), \n                       xaxis = dict(title = feature, \n                                    titlefont = {'size': 16}), \n                       yaxis = dict(title = '% Of Distribution', \n                                    titlefont = {'size': 16}), \n                       height = 500,\n                       width = 550, \n                       template = 'plotly_dark')\n    \n    fig = go.Figure(data = data, \n                    layout = layout)\n    \n    \n    \n    return iplot(fig)","172f5489":"# Pass any categorical feature to the function to check the frequency distribution.\nfrequency_distribution('Ease of Online booking')","3f82ee71":"# Checking the frequency distribution for all the categorical variables using a for loop and the function.\nfor col in cat_data_cols:\n    frequency_distribution(col)","929c5506":"# Function to show the dispersion of the data in continous features.\ndef dispersion(feature):\n    if feature in cont_data_cols:\n        data = [go.Box(x = data_cleaned[feature], \n                       name = str(feature),\n                       marker = dict(line = {'color': 'white', 'width': 1.5}, \n                                     color = 'indianred'))]\n        \n        layout = go.Layout(title = dict(text = 'Dispersion : '+ feature , \n                                    x = 0.5, \n                                    y = 0.88), \n                           xaxis = dict(title = 'Values', \n                                    titlefont = {'size': 16}), \n                           height = 300,\n                           width = 500,\n                           template = 'plotly_dark')\n        \n        fig = go.Figure(data = data, \n                    layout = layout)\n        \n    else:\n        return 'Enter a valid continous feature from the data set'\n        exit()\n    \n    \n    return iplot(fig)","c12ad8da":"dispersion('Age')","e120613f":"# Checking the dispersion for all the continous variables using a for loop and the function.\nfor col in cont_data_cols:\n    dispersion(col)","21221d13":"def satisfaction_by_feature(feature):\n    if feature in cat_data_cols:\n        grp_by_feature = raw_data_train.groupby(by = [feature, 'satisfaction'])['id'].count().reset_index()\n        grp_by_total = raw_data_train.groupby(by = [feature])['id'].count().reset_index()\n        grp_by = pd.merge(left = grp_by_feature, right = grp_by_total, on = feature, how = 'inner')\n        grp_by['% Satisfied\\Dissatisfied'] = round(grp_by['id_x']\/grp_by['id_y'] * 100, 2)\n        grp_by.drop(columns = ['id_x', 'id_y'], inplace = True)\n    else:\n        return 'Enter a valid Categorical feature from the data set'\n        exit()\n        \n    data = [go.Bar(x = grp_by[grp_by['satisfaction'] == 'satisfied'][feature], \n                   y = grp_by[grp_by['satisfaction'] == 'satisfied']['% Satisfied\\Dissatisfied'], \n                   name = 'Satisfied', \n                   text = grp_by[grp_by['satisfaction'] == 'satisfied']['% Satisfied\\Dissatisfied'], \n                   textposition = 'inside', \n                   textfont = {'color': 'white'}, \n                   marker = dict(line = {'color': 'black', 'width': 1.5})), \n       \n            go.Bar(x = grp_by[grp_by['satisfaction'] == 'neutral or dissatisfied'][feature], \n                   y = grp_by[grp_by['satisfaction'] == 'neutral or dissatisfied']['% Satisfied\\Dissatisfied'], \n                   name = 'Dissatisfied', \n                   text = grp_by[grp_by['satisfaction'] == 'neutral or dissatisfied']['% Satisfied\\Dissatisfied'], \n                   textposition = 'inside', \n                   textfont = {'color': 'white'}, \n                   marker = dict(line = {'color': 'black', 'width': 1.5}))]\n\n    layout = go.Layout(title = dict(text = 'Satisfaction Rate By : ' + feature, \n                                    font = {'size': 20}, \n                                    x = 0.5, \n                                    y = 0.88),\n                       xaxis = dict(title = feature, \n                                    titlefont = {'size': 16}),\n                       yaxis = dict(title = 'Satisfaction Rate', \n                                    titlefont = {'size': 16}),\n                       barmode = 'stack', \n                       hovermode = 'closest',\n                       height = 500, \n                       width = 650)\n\n    fig = go.Figure(data = data, \n                    layout = layout)\n    \n    return iplot(fig)\n      ","ea1c2e58":"# Checking the satisfaction rate relationship for all the categorical variables using a for loop and the function.\nfor col in cat_data_cols[:-1]:\n    satisfaction_by_feature(col)","392c6480":"raw_data_train['Arrival Delay in Minutes'].mean()","4ae91577":"raw_data_train[(raw_data_train['Arrival Delay in Minutes'] < 28 )].groupby('satisfaction')['id'].count().reset_index()","5a7b900f":"raw_data_train[(raw_data_train['Departure Delay in Minutes'] == 0 ) & (raw_data_train['Arrival Delay in Minutes'] == 0 )].groupby('satisfaction')['id'].count().reset_index()","9d407788":"raw_data_train[(raw_data_train['Departure Delay in Minutes'] == 0 ) & (raw_data_train['Arrival Delay in Minutes'] == 0 )].count()","65e0c2a1":"# Let's start with LabelEncoder.\nencoded_data = data_cleaned.copy()\ncols_to_encode = ['Gender', 'Customer Type', 'Type of Travel', 'satisfaction']\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nencoded_data[cols_to_encode] = data_cleaned[cols_to_encode].apply(le.fit_transform)","0ff75d92":"# Let's use get dummies to encode the Class feature\nencoded_data = pd.get_dummies(data = encoded_data, columns = ['Class'], drop_first = True)","20c0ed77":"master_dataframe(encoded_data)","920c6f5f":"# Let's rearrange the columns in the dataset so that the dependent variable in the last column.\ncols = [col for col in encoded_data if col != 'satisfaction'] + ['satisfaction']\nencoded_data = encoded_data[cols]","8ae8480f":"encoded_data = encoded_data[['Gender', 'Customer Type', 'Type of Travel', 'Age', 'Flight Distance',\n           'Inflight wifi service', 'Departure\/Arrival time convenient',\n           'Ease of Online booking', 'Gate location', 'Food and drink',\n           'Online boarding', 'Seat comfort', 'Inflight entertainment',\n           'On-board service', 'Leg room service', 'Baggage handling',\n           'Checkin service', 'Inflight service', 'Cleanliness',\n           'Departure Delay in Minutes', 'Arrival Delay in Minutes', 'Class_Eco',\n           'Class_Eco Plus', 'satisfaction']]","93208259":"# Let's separate the dependent and independent variables.\nX = encoded_data.iloc[:, :-1].values\ny = encoded_data.iloc[:, -1].values\nprint('Shape of X: ', X.shape)\nprint('Shape of y: ', y.shape)","f6c68168":"# Let's split the data set into train and test set.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of X_test: ', X_test.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of y_test: ', y_test.shape)","3eb581e1":"# Let's standardise the columns.\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[:, 3:-1] = sc.fit_transform(X_train[:, 3:-1])\nX_test[:, 3:-1] = sc.transform(X_test[:, 3:-1])","ddb7d05f":"X_train[:, 4]","8a65212d":"# Creating the LogisticRegression model\nfrom sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression()\nlr_classifier.fit(X_train, y_train)\n\n# Predicting the output for test dataset\ny_pred = lr_classifier.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix and accruacy score.\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_lr = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_lr)","33c88be9":"from sklearn.svm import SVC\nsvc_classifier = SVC(kernel = 'linear')\nsvc_classifier.fit(X_train, y_train)\n\n# Predicting the output for test dataset\ny_pred = svc_classifier.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix and accruacy score.\ncm = confusion_matrix(y_test, y_pred)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_SVC = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_SVC)","fd42c111":"svm_classifier = SVC(kernel = 'rbf')\nsvm_classifier.fit(X_train, y_train)\n\n# Predicting the output for test dataset\ny_pred = svm_classifier.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix and accruacy score.\ncm = confusion_matrix(y_test, y_pred)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_SVM = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_SVM)","ba6078a0":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifer = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_classifer.fit(X_train, y_train)\n\n# Predicting the output for test dataset\ny_pred = knn_classifer.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix and accruacy score.\ncm = confusion_matrix(y_test, y_pred)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_knn = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_knn)","ce8d1112":"from sklearn.naive_bayes import GaussianNB\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\ny_pred_nb = nb_classifier.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_nb)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_nb = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_nb)","d0f3591f":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion = 'entropy')\ndt_classifier.fit(X_train, y_train)\n\ny_pred_dt = dt_classifier.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_dt)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_dt = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_dt)","b9be7949":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nrf_classifier.fit(X_train, y_train)\n\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Lets measure the accuracy of the model using the confusion matrix.\ncm = confusion_matrix(y_test, y_pred_rf)\n\ncorr_pred = cm[0, 0] + cm[1, 1]\ntotal = cm.sum()\ncorr_pred_per_rf = round(corr_pred\/total*100, 2)\nprint('Percentage of correct predictions: ', corr_pred_per_rf)","609b5515":"# Univariant Analysis","0c55fdc3":"Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. ... It is the analysis of the relationship between the two variables.","445fd3ac":"We can see how various facilities provided by the airline companies i.e. the categorical features affect the rate of satisfaction.","51371b37":"**From the above metadata table, we have the below information :**\n\n1. None of the columns have Null values except \"Arrival Delay in Minutes\". And the metric says percentage of NUll values is      just 0.3%, that means we can completely drop the null values. But we will decide on handling them going forward.\n\n   Replcing Null values with mean of \"Arrival Delay in Minutes\" might be a bad idea as the field seem to have outliers which    would give incorrect meaning to the data. \n\n2. Almost all the features are categorical in nature except some continous features. This clearly explains there\n   is difference in scale between them. We might have to standardize the data to a common scale before building the model to    give equal weightage to each feature.\n   \n \n 3. Fields \"Unnamed: 0\" and \"id\" are unique identifiers to the data, hence we can exclude them from our\n    set of independent features.\n    \n 4. There are some features which are categorical in nature and are of data type object. That means, we have \n    to properly encode them before including them to train the model.\n    \n  With this said let's begin our journey in deeply understanding the data available.  ","9cc73d66":"# Logisitc Regression","c59db86c":"# Bivariant Analysis","6497248e":"# K Nearest Neighbour","5d46fed5":"Let's encode the categorical features like Gender, Customer Type, Class, Type Of Travel & Satisfaction so that we can use them to train our model.\n\nThe features Gender, Customer Type, Type Of Travel & Satisfaction are nominal categorical features.i.e. they are not ordered and hence can be encoded using LabelEncoder.\n\nThe feature Class is an ordinal categorical feature, hence we will create dummy columns for this feature.","19759faa":"# Data Transformation","6e3ba31e":"# SVC","2c64c9a1":"# Naive Bayes Classifier","3b51b788":"# SVM - Kernel","94746a1c":"# Random Forect Classifier","e6d3c12b":"Univariant analysis is the basic type of analysis that is performed by an analyst which involves only single variable. Hence, there is no other variable to compare or define a relationship with. Univariant analysis involves checking the frequency distribution, range, dispersion and other characteristcs of a variable using pivot tables, bar chart, pie chart, histogram, box plots etc.","0726f170":"# Decision Tree Classifier"}}