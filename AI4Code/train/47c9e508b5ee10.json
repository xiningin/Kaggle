{"cell_type":{"3269182f":"code","a9586eae":"code","04f468ff":"code","13fb7002":"code","d80943c9":"code","62e5d990":"code","72acdf35":"code","6c4ba237":"code","f1f77ba8":"code","c447b1bf":"code","83baef45":"code","b250e412":"code","b41aae86":"code","3f69c25a":"code","cc8757ed":"code","67b973c4":"code","2538f1a6":"code","2001c4ec":"code","4cf9346c":"code","889092b6":"code","6e32ff5c":"code","79732ab3":"code","392926ca":"code","bc2f42e9":"code","eda499f4":"code","04216df1":"code","8570e04d":"code","dd803842":"code","07d614f7":"code","39f8a070":"code","da9c558b":"code","306a67a2":"code","0e8f3242":"code","052c0cf1":"code","052d1df1":"code","32658580":"code","a72fc879":"code","7ee7c451":"code","f3b1efbf":"code","1b3b5f82":"code","7d06644e":"code","97b3eddc":"code","c0075955":"code","3902f7f9":"code","dd1d3829":"code","a93d4ba9":"code","560aa3cc":"code","e8db1603":"code","397ec190":"code","99a1aaa3":"code","6e4bc352":"code","409711b9":"code","a3a3e147":"markdown","812e8a8d":"markdown","403791bd":"markdown","9b0b4fb9":"markdown","605d5a4b":"markdown","d5dd1afa":"markdown","dbc32ac5":"markdown","8ffe5455":"markdown","7106cb6f":"markdown","3fab5728":"markdown","116d122b":"markdown","5c91e3a2":"markdown","8d4a50c4":"markdown","1dd4be90":"markdown","8f2ed0e1":"markdown"},"source":{"3269182f":"#------------------------------------------Libraries---------------------------------------------------------------#\n####################################################################################################################\n#-------------------------------------Boiler Plate Imports---------------------------------------------------------#\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n#---------------------------------------Text Processing------------------------------------------------------------#\nimport regex\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\n#####################################################################################################################","a9586eae":"names = []\nbase = '\/kaggle\/input\/topic-modelling-on-emails\/Data\/'\nwith os.scandir(base) as entries:\n    for entry in entries:\n        if(entry.is_file() == False):\n            names.append(entry.name)\nnames","04f468ff":"names.sort()","13fb7002":"files = {}\nunique = []\nfor name in names:\n    path = base + name+'\/'\n    x = []\n    with os.scandir(path) as entries:\n        for entry in entries:\n            if(entry.is_file()):\n                x.append(entry.name)\n    files[name] = x\n    files[name].sort()","d80943c9":"for k, v in files.items():\n    print(k, len(v))","62e5d990":"names","72acdf35":"for i in range(len(names)):\n    x = files[names[i]]\n    for j in x:\n        for k in range(i+1, len(names)):\n            key = names[k]\n            if j in files[key]:\n                files[key].remove(j)","6c4ba237":"for k, v in files.items():\n    print(k, len(v))","f1f77ba8":"data = {}\ni = 0\n\nfor genre in files.keys() :\n    texts = files[genre]\n    for text in texts:\n        if text in files[genre]:\n            path = base + genre + '\/' + text\n            with open(path, \"r\", encoding = \"latin1\") as file:\n                data[i] = file.readlines()\n                i = i+1\n            data[i-1] = [\" \".join(data[i-1]), genre] \n\ndata = pd.DataFrame(data).T\nprint(data.shape)\ndata.columns = ['Text', 'Class']\ndata.head()","c447b1bf":"data.info()","83baef45":"data.isna().sum()","b250e412":"unique = list(data.Text.unique())\nlen(unique)","b41aae86":"dic = dict(data)","3f69c25a":"uni = {}\ni = 0\nfor k in range(len(list(dic['Text']))):\n    if dic['Text'][k] in unique:\n        uni[i] = [dic['Text'][k], dic['Class'][k]]\n        unique.remove(dic['Text'][k])\n        i += 1","cc8757ed":"data = pd.DataFrame(uni).T\nprint(data.shape)\ndata.columns = ['Text', 'Class']\ndata.head()","67b973c4":"plt.figure(figsize=(10,5))\nax = sns.countplot(data.Class, palette = sns.color_palette(\"mako\"))","2538f1a6":"def make_wordcloud(words,title):\n    cloud = WordCloud(width=1920, height=1080,max_font_size=200, max_words=300, background_color=\"white\").generate(words)\n    plt.figure(figsize=(20,20))\n    plt.imshow(cloud, interpolation=\"gaussian\")\n    plt.axis(\"off\") \n    plt.title(title, fontsize=60)\n    plt.show()","2001c4ec":"wordnet_lemmatizer = WordNetLemmatizer()\n\nstop = stopwords.words('english')\n\nfor punct in punctuation:\n    stop.append(punct)\n\ndef filter_text(text, stop_words):\n    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n    filtered_text = [regex.sub(u'\\p{^Latin}', u'', w) for w in word_tokens if w.isalpha() and len(w) > 3]\n    filtered_text = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in filtered_text if not w in stop_words] \n    return \" \".join(filtered_text)","4cf9346c":"data[\"filtered_text\"] = data.Text.apply(lambda x : filter_text(x, stop)) \ndata.head()","889092b6":"all_text = \" \".join(data[data.Class == \"Crime\"].filtered_text) \nmake_wordcloud(all_text, \"Crime\")","6e32ff5c":"count = pd.DataFrame(all_text.split(), columns = ['words'])\ntop_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\nplt.figure(figsize=(10,5))\nsns.barplot(x = top_10.words.value_counts().index,\n            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))","79732ab3":"all_text = \" \".join(data[data.Class == \"Politics\"].filtered_text) \nmake_wordcloud(all_text, \"Politics\")","392926ca":"count = pd.DataFrame(all_text.split(), columns = ['words'])\ntop_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\nplt.figure(figsize=(10,5))\nsns.barplot(x = top_10.words.value_counts().index,\n            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))","bc2f42e9":"all_text = \" \".join(data[data.Class == \"Science\"].filtered_text) \nmake_wordcloud(all_text, \"Science\")","eda499f4":"count = pd.DataFrame(all_text.split(), columns = ['words'])\ntop_10 = count[count['words'].isin(list(count.words.value_counts()[:10].index[:10]))]\nplt.figure(figsize=(10,5))\nsns.barplot(x = top_10.words.value_counts().index,\n            y = top_10.words.value_counts(), palette = sns.color_palette(\"mako\"))","04216df1":"data['Class'].value_counts()","8570e04d":"data=data.groupby('Class',as_index = False,group_keys=False).apply(lambda s: s.sample(1095,replace=True))","dd803842":"plt.figure(figsize=(10,5))\nax = sns.countplot(data.Class, palette = sns.color_palette(\"mako\"))","07d614f7":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport transformers\n\nimport nltk\nimport re\n\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nplt.style.use('seaborn')","39f8a070":"print(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))","da9c558b":"from transformers import TFXLNetModel, XLNetTokenizer","306a67a2":"xlnet_model = 'xlnet-large-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","0e8f3242":"number_of_classes = 1 #len(names)\nnumber_of_classes","052c0cf1":"def create_xlnet(mname):\n    \"\"\" Creates the model. It is composed of the XLNet main block and then\n    a classification head its added\n    \"\"\"\n    # Define token ids as inputs\n    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n\n    # Call XLNet model\n    xlnet = TFXLNetModel.from_pretrained(mname)\n    xlnet_encodings = xlnet(word_inputs)[0]\n\n    # CLASSIFICATION HEAD \n    # Collect last step from last hidden state (CLS)\n    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n    # Apply dropout for regularization\n    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n    # Final output \n    outputs = tf.keras.layers.Dense(number_of_classes, activation='sigmoid', name='outputs')(doc_encoding)\n\n    # Compile model\n    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n\n    return model","052d1df1":"xlnet = create_xlnet(xlnet_model)","32658580":"xlnet.summary()","a72fc879":"from sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\ny = le.fit_transform(data['Class'])","7ee7c451":"text = data['filtered_text']\nlabels = data['Class']\n\n\nX_train, X_test, y_train, y_test = train_test_split(text, y, test_size=0.15, random_state=196)","f3b1efbf":"def get_inputs(text, tokenizer, max_len=512):\n    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in text]\n    inp_tok = np.array([a['input_ids'] for a in inps])\n    ids = np.array([a['attention_mask'] for a in inps])\n    segments = np.array([a['token_type_ids'] for a in inps])\n    return inp_tok, ids, segments","1b3b5f82":"def warmup(epoch, lr):\n    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n    However, as we are finetuning for few epoch it's not crucial.\n    \"\"\"\n    return max(lr +1e-6, 2e-5)\n\ndef plot_metrics(pred, true_labels):\n    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n    auc = roc_auc_score(true_labels, pred)\n\n    fig, ax = plt.subplots(1, figsize=(8,8))\n    ax.plot(fpr, tpr, color='red')\n    ax.plot([0,1], [0,1], color='black', linestyle='--')\n    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n    return fig","7d06644e":"inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)","97b3eddc":"callbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n    tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n]","c0075955":"hist = xlnet.fit(x=inp_tok, y=y_train, epochs=4, batch_size=2, validation_split=.15, callbacks=callbacks)","3902f7f9":"inp_tok, ids, segments = get_inputs(X_test, xlnet_tokenizer)","dd1d3829":"preds = xlnet.predict(inp_tok, verbose=True)","a93d4ba9":"#plot_metrics(preds, y_test);","560aa3cc":"pred_analysis_df = pd.DataFrame({'tweet':X_test.values, 'pred':preds.flatten(), 'real':y_test})\npred_analysis_df['rounded'] = np.array(pred_analysis_df['pred'] > 0.5, dtype='int')\ndiff = pred_analysis_df[pred_analysis_df['real'] != pred_analysis_df['rounded']]","e8db1603":"#change to see other examples\nidx = 44\n\ntweet, real, pred = diff.iloc[idx, [0,2,3]]\nprint(tweet)\nprint(\"PRED: \" + str(pred))\nprint(\"REAL: \" + str(real))","397ec190":"# tweets = dataf_test['clean']\n\n# inp_tok, ids, segments = get_inputs(tweets, xlnet_tokenizer)","99a1aaa3":"preds = xlnet.predict(inp_tok, verbose=True)","6e4bc352":"# dataf_test['target'] = preds\n# dataf_test['target'] = np.array(dataf_test['target'] >= 0.5, dtype='int')\n# dataf_test[['id', 'target']].to_csv('submission.csv', index=False)","409711b9":"xlnet.save_weights(\"xlnet.h5\")","a3a3e147":"* From the above result it is clearly implied that the class 'Entertainment' had no data unique to its class. \n* By not considering it, we are also eliminating any variance that can be caused by the duplicate data.","812e8a8d":"### Science","403791bd":"### Top 10 words in the Politics Category","9b0b4fb9":"## Read Data from the '.txt' files","605d5a4b":"# Introduction\n## Data\n* The input data comprises of files within the folders with class labels as their name.\n* The textual data are emails written from one journalist to another or to their source, regarding a story.\n* Emails can be seen labelled under multiple classes which would mislead the model.\n\n# Methodology\n* To clean our data and obtain some meaningful insights, we first need to make sure our data is properly labelled and stored\n* To do so:\n    1. Read the text from each file and while doing so, make sure we are not reading duplicate data.\n    2. Load the textual data into DataFrame for easier analysis.\n    3. Clean the raw text by:\n        * Removing special characters, punctuations, pronouns, stopwords.\n        * Tokeizing each data point, i.e segmenting text into sentences and further into words.\n        * Normalize the text by converting it into lower case.\n        * Extract the lemma for each word. Ex: Lemma(swimming) -> swim.","d5dd1afa":"* We now know how many files are labelled under each class. Our job now is to remove the data points that are labelled under other classes. Ex: 14147.txt is labelled under 'Crime', 'Entertainment' and 'Science', so we will be removing the entry from 'Entertainment' and 'Science'.\n* The risk here is that we might have removed the entry from a correctly labelled class. Ex: 14147.txt may be labelled as 'Science' initially and was repeated in other classes, by removing it from 'Science' we are mislabelling the data as 'Crime'.\n* But 'Science' already contains the most no. of entries, making it easier for us to train the model for that particular class. Hence, our approach doesn't affect the analysis.","dbc32ac5":"### Top 10 words in the Crime Category","8ffe5455":"# Oversampling The Data","7106cb6f":"### Politics","3fab5728":"# Testing","116d122b":"Now that we managed to load our data frame, we can move to the next step, i.e cleaning the data.","5c91e3a2":"There still exists few duplicate texts which might have been the result of poor data management or sending the same mail multiple times.","8d4a50c4":"We can now find some useful insights into the data set by constructing wordclouds and find term frequencies in each class.\n\n### Crime","1dd4be90":"# XLNET","8f2ed0e1":"### Top 10 words in the Science Category"}}