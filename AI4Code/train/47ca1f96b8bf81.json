{"cell_type":{"69f0c20a":"code","76585b92":"code","09077cf8":"code","86b7aad9":"code","512ab6c5":"code","2bb9b6f2":"code","99fb47d6":"code","cc316e8c":"code","a876a08b":"code","02bff445":"code","c0aa35f0":"code","44208035":"code","a8e26d53":"code","9c4c16d5":"code","b62519bb":"code","45d92c3b":"code","9a586eba":"code","51b49d5d":"code","3e1b1720":"code","29d6afed":"code","06a358f3":"code","afaf64b1":"code","6f04a50c":"code","800f895c":"code","e99e40a6":"code","11debdcd":"code","c8bab2e1":"code","e53aa767":"code","79a59e14":"markdown","dadb2eed":"markdown","72f6e03b":"markdown","95e56ac6":"markdown","7404d0d0":"markdown","7897645f":"markdown","6e3e8296":"markdown","5b33a263":"markdown","29c7767c":"markdown","d6061c23":"markdown","01330e24":"markdown"},"source":{"69f0c20a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","76585b92":"dataframe = pd.read_csv(\"..\/input\/train-balanced-sarcasm.csv\")","09077cf8":"# allows us to take a look at some of the data\ndataframe.head()","86b7aad9":"# tells you how many values in each column\ndataframe.info()","512ab6c5":"# simply shows the number of rows and columns in your data set\ndataframe.shape","2bb9b6f2":"# displays the total number of rows for each value of label (in this case 0 and 1)\ndataframe['label'].value_counts()","99fb47d6":"# now we split these out into two sets, sarc and notsarc\nnotsarc = dataframe[dataframe['label'] == 0]\nsarc = dataframe[dataframe['label'] == 1]","cc316e8c":"sarc['comment']","a876a08b":"sarc['subreddit'].value_counts().head()","02bff445":"notsarc['subreddit'].value_counts().head()","c0aa35f0":"dataframe['subreddit'].value_counts().head()","44208035":"sub_df_subreddit = dataframe.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df_subreddit[sub_df_subreddit['sum'] > 1000].sort_values(by='mean', ascending=False).head(10)","a8e26d53":"sub_df_author = dataframe.groupby('author')['label'].agg([np.size, np.mean, np.sum])\nsub_df_author[sub_df_author['sum'] > 5].sort_values(by='mean', ascending=False).head(10)","9c4c16d5":"print(len(dataframe[(dataframe['author'] == 'Biffingston') & (dataframe['label'] == 0)]))\nprint(len(dataframe[(dataframe['author'] == 'Biffingston') & (dataframe['label'] == 1)]))","b62519bb":"print(\"score zero \" , dataframe[dataframe['score'] == 0]['label'].describe()['mean'])\nprint(\"positive score \" , dataframe[dataframe['score'] > 0]['label'].describe()['mean'])\nprint(\"negative score \" , dataframe[dataframe['score'] < 0]['label'].describe()['mean'])\n","45d92c3b":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","9a586eba":"# there are some comments with null, these need to be dropped as it can cause issues\ndataframe.dropna(subset=['comment'], inplace=True)","51b49d5d":"# lets split out the data into training and validation sets\n# train_texts is what we train, y_train is the column ref and the label\n# valid_texts is what we validate or test on, y_valid is the column ref and the label\n\n# the default split is 75%training set, 25% testing set \n\n\n\ntrain_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(dataframe['comment'], dataframe['label'], random_state=17)","3e1b1720":"# build bigrams, put a limit on maximal number of features\n# and minimal word frequency\ntf_idf = TfidfVectorizer(ngram_range=(1, 4), max_features=20000, min_df=4)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=1)\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","29d6afed":"%%time\ntfidf_logit_pipeline.fit(train_texts, y_train)","06a358f3":"%%time\n# this produces a a prediction an array the same size as valid texts, where 0 and 1 refer to the sarcasm predicton\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)","afaf64b1":"valid_pred","6f04a50c":"accuracy_score(y_valid, valid_pred)","800f895c":"cm  = confusion_matrix(y_valid,valid_pred)\ncm","e99e40a6":"tp_accuracy = cm[0][0]\/(cm[0][0] + cm[0][1])\ntn_accuracy = cm[1][1]\/(cm[1][0]+cm[1][1])\nprint(\"true positive accuracy: \" + str(tp_accuracy))\nprint(\"true negative accuracy: \" + str(tn_accuracy))","11debdcd":"def amISerious(model, text):\n    result = model.predict([text])[0]\n    if (result == 1):\n        return \"sarcastic\"\n    else:\n        return \"not sarcastic\"\n    ","c8bab2e1":"amISerious(tfidf_logit_pipeline,\"whatever i didnt enjoy that film\")","e53aa767":"amISerious(tfidf_logit_pipeline,\"yeah whatever, i didnt enjoy that film\")","79a59e14":"Ok so looking at the data for a frequent author we can see they have balanced it across author.  So author can be disregarded as feature to train sarcasm on ","dadb2eed":"First we use pandas to read in the balanced dataset into a panda dataframe\nSecondly we split that dataframe into two sets where one is sarcastic and one is not sarcastic","72f6e03b":"> Its balanced as we can see we have 505413 results assigned to both the sarcastic and non-sarcastic labels","95e56ac6":"Now we split our data into a training and validation set.\n\nThe model gets trained on the training set\n\nThe accuracy of the model then gets its accuracy measured by using the validation set","7404d0d0":"The above table is intersting.  I disregarded any subreddits with less than 1000 entries (the problem if sorting by mean sarcyness subjects with very low entries will come to the top naturally)\n\nSo it does seem that certain topics are more likely to be sarcastic.\n\nLets do the same for author","7897645f":"   **Are you being serious!! (no really are you..?)**\n   \n\nIn this notebook I am going to look into the different techiques to see if we can predict whether or not a comment is being sarcastic.\n\nFor this I will be using the Sarcasm On Reddit data set.  The dataset I will be using will be approximately 1.3 million comments.  Approximately half of these are labeld as sarcastic (the user specified the \\s flag in reddit to indicate they were being actually sarcastic) and the other hald is not sarcastic.\n\nAside from the intial comment, we also have the following features which we can use\n* subject being discussed\n* author of the original comment\n* follow up reply comment from another (unspecified) reddit user\n\n1. We will start by doing some initial statistical analysis to see if certain authors or subjects lean to being more sarcastic.\nWe will then do some basic ngram analysis and wordBubbles to see if there are certain phrases that are very prevalent in sarcastic and non sarcastic comments.\n\n1. We will then train a model just using the comment to see how well we can predict if another comment is sarcastic enought.\n\n1. We will then see if we can improve that prediction by factoring in the author and the subject being discussed (which really are just weightings)\n\n1. Finally we will look into how well the model (trasined just on the comments) detetcts sarcasm in a non reddit context (assuming I can get some reliable data for this)\n\n\n","6e3e8296":"**Part one - reading in data and some basic statistical analysis**","5b33a263":"But what about ups, downs and score, do people like sarcasm, does that get voted up more.   We'll classify into 3 bins, those with a score of zero, those with a positve score, those with a negative score","29c7767c":"Ok, just looking straight at the comments in this way doesn't take into account the volume of comments.  We can't conclude for example that AskReddit is the most sartcastic subject because it has the most comments.  We need to produce a table of percentage of comments for each subject that are sarcastic (we can then do the same for author as well)\n","d6061c23":"**Now we move on to the machine learning part of this all.**\n\n","01330e24":"Hmmm, I'm having a hard time excepting the above.  Something is wrong, how can so many authors have exactly the same number of sarcastic and unsarcatic comments.  Unless it is in the way the balanced set was created.  Maybe they took an even number of both types of comments for each author.  In which case, author is useless.  I guess it is important to know and understand how the balanced data set was created."}}