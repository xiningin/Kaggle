{"cell_type":{"fb3b202c":"code","9aff3ced":"code","fbec4f44":"code","b8f53656":"code","69c7f2ea":"code","143c1478":"code","ee214f44":"code","de4fdaf7":"code","fc4fd8f3":"code","7bacb93c":"code","2b9db802":"code","fe0a51ac":"code","fa6fe22b":"code","7b858a71":"code","2ce44af0":"code","b4628727":"code","3452611b":"code","dd47d0d9":"code","977be2d5":"markdown","2d05de45":"markdown","6dd32bab":"markdown","0484601a":"markdown","bc809308":"markdown","34681c3e":"markdown","8cd6e5d3":"markdown","c4e7a45b":"markdown","2afc2263":"markdown","21efbd8c":"markdown","e773eef0":"markdown","affa8515":"markdown"},"source":{"fb3b202c":"# For math and visualization\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats.contingency import association\n\n# For ordinal encoding categorical variables, splitting data, cross-validation\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold, KFold, cross_val_score\n\n# For training model\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# For parameters optimization\nimport optuna\nfrom optuna import create_study\nfrom optuna.visualization import plot_param_importances","9aff3ced":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fbec4f44":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv') # train-set\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv') # test-set\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","b8f53656":"train.info()","69c7f2ea":"test.info()","143c1478":"categorical_features = ['cat' + str(i) for i in range(10)]\nnumerical_features = ['cont' + str(j) for j in range(14)]\n\nfor col in categorical_features:\n     print(set(train[col].value_counts().index) == set(test[col].value_counts().index))","ee214f44":"fig, ax = plt.subplots(figsize=(12, 6))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"lightsteelblue\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show()","de4fdaf7":"print(f\"{(train['target'] < 5).sum() \/ len(train) * 100:.3f}% of the target values are less than 5\")","fc4fd8f3":"# Create dataframe to plot\ndf = train[numerical_features]\n\n# Calculating correlation values\ndf = df.corr().round(2)\n\n# Making a heatmap\nplt.figure(figsize=(12,12))\nax = sns.heatmap(df, annot=True, center = 0, cmap=sns.color_palette(\"vlag\", as_cmap=True), linewidths = 1, linecolor = 'black', annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show()","7bacb93c":"df_cat = train[categorical_features]\nfactors_paired = [(i,j) for i in df_cat.columns.values for j in df_cat.columns.values]\n\ncat_as = []\n\nfor f in factors_paired:\n    if f[0] != f[1]:\n        associations = association(pd.crosstab(df_cat[f[0]], df_cat[f[1]]), method=\"cramer\")   \n        cat_as.append(associations)\n        \n    else:      \n        cat_as.append(0)\n        \n\ncat_as = np.array(cat_as).reshape(10, 10)\ncat_as = pd.DataFrame(cat_as, index=df_cat.columns.values, columns=df_cat.columns.values)\n\n\nplt.figure(figsize=(12,12))\nax = sns.heatmap(cat_as, annot=True, center = 0, cmap=sns.color_palette(\"vlag\", as_cmap=True), linewidths = 1, linecolor = 'black', annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Categorical feature association heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show()","2b9db802":"for col in categorical_features:\n    encoder = OrdinalEncoder()\n    train[col] = encoder.fit_transform(np.asarray(train[col]).reshape(-1, 1))\n    test[col] = encoder.transform(np.asarray(test[col]).reshape(-1, 1))\n    \ntest.head()","fe0a51ac":"X = train.drop([\"id\", \"target\"], axis=1)\nX_test = test.drop([\"id\"], axis=1)\ny = train[\"target\"]","fa6fe22b":"def objective(trial, data=X, target=y):\n    \n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=58)\n    \n    params = {\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"objective\": \"reg:squarederror\",\n        \"n_estimators\": 10000,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.05, 0.55),\n        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.4, 0.9),\n        \"subsample\": trial.suggest_loguniform(\"subsample\", 0.5, 0.9),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 1.0, 40.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1.0, 40.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 10, 1000),\n        \"seed\": 72,\n        \"n_jobs\": 4,\n    }\n    \n    # Create and train a model with overfitting monitoring\n    model = XGBRegressor(**params)      \n    model.fit(X_tr, y_tr, eval_set=[(X_val,y_val)], early_stopping_rounds=100, verbose=False)\n    preds = model.predict(X_val)\n    \n    #Watching the final RMSE metric\n    rmse = mean_squared_error(y_val, preds, squared=False)\n    \n    return rmse","7b858a71":"%%time\n# Calculating optimization time\n\n# Create a task for the framework\nstudy = optuna.create_study(direction='minimize')  # Minimizing the error\nstudy.optimize(objective, n_trials=5)              # Number of iterations: 10\n\n# Watching the final metrics\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best score:', study.best_trial.value)","2ce44af0":"plot_param_importances(study)","b4628727":"study.best_params","3452611b":"xgb_params = study.best_params\n\nxgb_params['metric'] = 'rmse'\nxgb_params['random_state'] = 72\nxgb_params['n_estimators'] = 10000\nxgb_params[\"verbosity\"] = 0\nxgb_params[\"objective\"] = \"reg:squarederror\"\nxgb_params['n_jobs'] = 4\nxgb_params['booster'] = 'gbtree'\n\nxgb_params","dd47d0d9":"%%time\n# Setting up fold parameters\nsplits = 10\nskf = KFold(n_splits=splits, shuffle=True, random_state=72)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\nout_of_preds = np.zeros((X.shape[0],))\npreds = 0\ntotal_mean_rmse = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              )\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    preds += model.predict(X_test) \/ splits\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.\n    out_of_preds[valid_idx] = model.predict(X_valid)\n    \n    # Getting score for a fold model\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, out_of_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_rmse += fold_rmse \/ splits\n    \nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")","977be2d5":"Loading train and test datasets:","2d05de45":"Thank You for watching!","6dd32bab":"Yes, it's OK there.\n\nLet's check target distribution","0484601a":"Checking for missing values in all columns:","bc809308":"Looking at the importance of hyperparameters when tuning...","34681c3e":"Now let's look at feature correlation heatmap to see if there is a correlation between the target value and other features, or a significant correlation between the features.","8cd6e5d3":"...and the best hyperparameter values","c4e7a45b":"And finally training the model with the obtained values of hyperparameters using folds:","2afc2263":"We see that there were no missing values in either the train or test set.\n\nNow let's check if all the values of categorical features are present in both datasets:","21efbd8c":"As we can see, the target value does not have a strong correlation with other features.\n\nWe are now ready to prepare the data. Let's encode categorical features with OrdinalEncoder:","e773eef0":"Optimization framework Optuna is used for fast and efficient optimization of model hyperparameters.","affa8515":"Separating the target value:"}}