{"cell_type":{"7b416a24":"code","e34c80c5":"code","0d65ceca":"code","ed3542ed":"code","5d3cb95b":"code","ce184cb5":"code","e39a0d53":"code","e398301b":"code","2fbf9504":"code","0fbd6ecb":"code","523ebf73":"code","9fca8900":"code","bd5ab291":"code","42458e2d":"code","ae679222":"code","bedd1e54":"code","0cceeaa3":"code","364c60cf":"code","89b08e38":"code","da4fcd03":"code","33a567c1":"markdown","818540d7":"markdown","f8f76e11":"markdown","6fd52c72":"markdown","36a451c3":"markdown","34ea4b85":"markdown","91c5b6f0":"markdown","fb0b964f":"markdown","6d40e4bf":"markdown","5c4100e6":"markdown","1f4c54af":"markdown","29e16f28":"markdown","4c38571d":"markdown"},"source":{"7b416a24":"import pdb\nimport pickle\nimport string\n\nimport time\n\nimport gensim\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport scipy\nimport sklearn\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords, twitter_samples\nfrom nltk.tokenize import TweetTokenizer\nimport pandas as pd\nfrom os import getcwd","e34c80c5":"# return the data stored as a dict of word: embedding pairs \n\nen_embeddings_subset = pickle.load(open(\"..\/input\/trans-data\/trans\/en_embeddings.p\", \"rb\"))\nfr_embeddings_subset = pickle.load(open(\"..\/input\/trans-data\/trans\/fr_embeddings.p\", \"rb\"))","0d65ceca":"def get_dict(file_name):\n    \"\"\"\n    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n    Check out the files this function takes in your workspace.\n    \"\"\"\n    my_file = pd.read_csv(file_name, delimiter=' ')\n    etof = {}  # the english to french dictionary to be returned\n    for i in range(len(my_file)):\n        # indexing into the rows.\n        en = my_file.loc[i][0]\n        fr = my_file.loc[i][1]\n        etof[en] = fr\n\n    return etof","ed3542ed":"# loading the english to french dictionaries\nen_fr_train = get_dict('..\/input\/trans-data\/trans\/eng_fren_train.txt')\nprint('The length of the English to French training dictionary is', len(en_fr_train))\nen_fr_test = get_dict('..\/input\/trans-data\/trans\/eng_fren_test.txt')\nprint('The length of the English to French test dictionary is', len(en_fr_train))","5d3cb95b":"def get_matrices(en_fr, french_vecs, english_vecs):\n    \"\"\"\n    Input:\n        en_fr: English to French dictionary\n        french_vecs: French words to their corresponding word embeddings.\n        english_vecs: English words to their corresponding word embeddings.\n    Output: \n        X: a matrix where the columns are the English embeddings.\n        Y: a matrix where the columns correspong to the French embeddings.\n    \"\"\"\n    \n    # list to append the english word \n    X_w = list()\n    # list to append the French word \n    Y_w = list()\n    \n    \n    # list to append the english word embeddings\n    X_l = list()\n    # list to append the French word embeddings\n    Y_l = list()\n    \n    # get the english words from the dict\n    english_words = list(en_fr_train.keys())\n    # get the French words from the dict\n    french_words = list(en_fr_train.values())\n    \n    # for each (english_word, its french_translation):\n    for word in zip(english_words,french_words):\n        \n        # if the english word in our subset of english embeddings \n        # and if the french word in our subset of french embeddings \n        if word[0] in english_vecs.keys() and word[1] in french_vecs.keys():\n            \n            # append the embedding for the english word[i] in X_l\n            X_l.append(english_vecs[word[0]])\n            # append the embedding for the french word[i] in Y_l\n            Y_l.append(french_vecs[word[1]])\n            \n            # list of found english words\n            X_w.append(word[0])\n            # list of found french words\n            Y_w.append(word[1])\n            \n            \n    # transform the list of english word embeddings to a numpy array\n    X = np.vstack(X_l)\n    # transform the list of french word embeddings to a numpy array\n    Y = np.vstack(Y_l)\n    \n    return X_w, Y_w, X, Y","ce184cb5":"# get training dataset\nf_eng_w, f_fr_w, X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)","e39a0d53":"f_eng_w_val, f_fr_w_val, X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)","e398301b":"def compute_loss(X, Y, R):\n    \n    # in this notbook X in in the shape of: (4932, 300)\n    # in this notbook Y in in the shape of: (4932, 300)\n    # in this notbook R in in the shape of: (300, 300)\n    \n    # m is the number of training examples\n    m = X.shape[0]\n    \n    # X(4932, 300).R (300, 300) -> (4932, 300)\n    # XR (4932, 300) - Y (4932, 300) -> (4932, 300)\n    \n    # diff between XR and Y\n    diff = np.dot(X, R) - Y\n    # elementwise squared\n    squared_diff = diff ** 2\n    # sum of squared diffs\n    sum_squared_diff = np.sum(squared_diff)\n    \n    loss = sum_squared_diff \/ m\n    \n    return loss","2fbf9504":"def compute_gradient(X, Y, R):\n    \n    # m is the number of training examples\n    m = X.shape[0]\n    \n    # shape(X) -> (4932, 300)\n    # shape(XTranspose) -> (300, 4932)\n    # shape(X.R-Y) -> (4932, 300)\n    # XT (300, 4932) . (X.R-Y) (4932, 300) -> (300, 300)\n\n    # gradient is X^T(XR - Y) * 2\/m\n    gradient = np.dot(X.transpose(),np.dot(X,R)-Y)*(2\/m)\n    \n    return gradient","0fbd6ecb":"def train(X, Y, train_steps=100, learning_rate=0.0001, show_loss_after_n_iteration=25):\n    \n    # Shape(R) -> (300, 300)\n    R = np.random.rand(X.shape[1], X.shape[1])\n    \n    # for each iteration:\n    # compute the gradient and modify R based on that gradient and the learning_rate\n    for iteration in range(train_steps):\n        \n        # compute the gradient\n        gradient = compute_gradient(X, Y, R)\n        \n        # modify R\n        R = R - (learning_rate * gradient)\n        \n        # compute the loss and modifying R\n        loss = compute_loss(X_train, Y_train, R)\n        \n        # show loss after each 100 iteration\n        if iteration % show_loss_after_n_iteration == 0:\n            print(\"Loss After {} is: {}\".format(iteration, loss))\n        \n    return R","523ebf73":"R_train = train(X_train, Y_train, train_steps=1500, learning_rate=0.5, show_loss_after_n_iteration=100)","9fca8900":"def cosine_similarity(A, B):\n    '''\n    Input:\n        A: a numpy array which corresponds to a word vector\n        B: A numpy array which corresponds to a word vector\n    Output:\n        cos: numerical number representing the cosine similarity between A and B.\n    '''\n    # you have to set this variable to the true label.\n    cos = -10\n    dot = np.dot(A, B)\n    norma = np.linalg.norm(A)\n    normb = np.linalg.norm(B)\n    cos = dot \/ (norma * normb)\n\n    return cos","bd5ab291":"def nearest_neighbor(v, candidates, k=1):\n    \n    similarity_l = []\n\n    # for each candidate vector...\n    for row in candidates:\n        # get the cosine similarity\n        cos_similarity = cosine_similarity(v,row)\n\n        # append the similarity to the list\n        similarity_l.append(cos_similarity)\n        \n    # sort the similarity list and get the indices of the sorted list\n    sorted_ids = np.argsort(similarity_l)\n\n    # get the indices of the k most similar candidate vectors\n    k_idx = sorted_ids[-k:]\n\n    \n    return k_idx","42458e2d":"def test_vocabulary(X, Y, R):\n    '''\n    Input:\n        X: a matrix where the columns are the English embeddings.\n        Y: a matrix where the columns correspong to the French embeddings.\n        R: the transform matrix which translates word embeddings from\n        English to French word vector space.\n    Output:\n        accuracy: for the English to French capitals\n    '''\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    # The prediction is X times R\n    pred = np.dot(X,R)\n\n    # initialize the number correct to zero\n    num_correct = 0\n\n    # loop through each row in pred (each transformed embedding)\n    for i in range(len(pred)):\n        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n        pred_idx = nearest_neighbor(pred[i],Y)\n\n        # if the index of the nearest neighbor equals the row of i... \\\n        if pred_idx == i:\n            # increment the number correct by 1.\n            num_correct += 1\n\n    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n    accuracy = num_correct \/ len(pred)\n\n    ### END CODE HERE ###\n\n    return accuracy","ae679222":"def predict (R, X): \n    \n    X = en_embeddings_subset[X]\n    \n    X.reshape((1, 300))\n    \n    prediction = np.dot(X, R)\n    \n    pred_idx = nearest_neighbor(prediction,Y_train)\n    \n    return f_fr_w[int(pred_idx)]","bedd1e54":"predict (R_train, 'people')","0cceeaa3":"predict (R_train, 'can')","364c60cf":"predict (R_train, 'city')","89b08e38":"predict (R_train, 'used')","da4fcd03":"acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\nprint(f\"accuracy on test set is {acc:.3f}\")","33a567c1":"# 2- Data Preperation ","818540d7":"The function compute_gradient will compute the gradient of the loss with respect to transform matrix R","f8f76e11":"### Finding the optimal R with gradient descent algorithm","6fd52c72":"In the txt file that contain the training and testing datasets, data is stored in two columns the first one is the english word and the secon\u064a one the its French translation.\n\nThe funtion **get_dict** will take the file and return a dictionary of English: French pairs","36a451c3":"### Train the model by finding R that will give the minimun Loss","34ea4b85":"# Machine Translation English to French","91c5b6f0":"### Model Evaluation on test set ","fb0b964f":"The function **get_matrices** will give us two matrices X will prepresent the training examples (a matrix of english word embeddings) and Y will prepresent the true target (a matrix of French word embeddings)","6d40e4bf":"# 5- Thank you","5c4100e6":"# 1- Importing ","1f4c54af":"Thank you for reading, I hope you enjoyed and benefited from it.\n\nIf you have any questions or notes please leave it in the comment section.\n\nIf you like this notebook please press upvote and thanks again.","29e16f28":"The function  **compute_loss** will compute the loss between **XR** and **Y** by computing the **(Frobenius norm squared) \/ m** of XR-Y","4c38571d":"# 3- Implement the Translation as a Linear Tranformation"}}