{"cell_type":{"2398c6f5":"code","774f21dd":"code","0dc5ec3a":"code","be8e3f36":"code","d31a1bd7":"code","c6ea7384":"code","0721da78":"code","4487b5bc":"code","45f36dbc":"code","5bc788f3":"code","7b822870":"code","f584e562":"markdown","395be345":"markdown","6e704d77":"markdown","91fa0ee6":"markdown","2c456557":"markdown","1283f1a9":"markdown","38869061":"markdown","f947e70c":"markdown","af2181e6":"markdown","a960be8b":"markdown","03e63c44":"markdown"},"source":{"2398c6f5":"import numpy as np\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport time\nimport shutil\nimport PIL\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,BatchNormalization, Input\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nfrom IPython.core.display import display, HTML","774f21dd":"def tr_plot(tr_data, start_epoch):\n    #Plot the training and validation data\n    tacc=tr_data.history['accuracy']\n    tloss=tr_data.history['loss']\n    vacc=tr_data.history['val_accuracy']\n    vloss=tr_data.history['val_loss']\n    Epoch_count=len(tacc)+ start_epoch\n    Epochs=[]\n    for i in range (start_epoch ,Epoch_count):\n        Epochs.append(i+1)   \n    index_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\n    val_lowest=vloss[index_loss]\n    index_acc=np.argmax(vacc)\n    acc_highest=vacc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n","0dc5ec3a":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","be8e3f36":"class TRC(keras.callbacks.Callback):\n    def __init__(self, model, base_model, epochs,  ask_epoch):\n        super(TRC, self).__init__()\n        self.model=model               \n        self.ask_epoch=ask_epoch\n        self.epochs=epochs\n        self.ask=True\n        self.base_model=base_model        \n        \n    def on_train_begin(self, logs=None):        \n        if self.ask_epoch == 0:\n            self.ask=False\n            self.ask_epoch=1\n        if self.ask_epoch >= self.epochs:\n            print('ask_epoch >= epochs, will train for ', epochs, ' epochs' )           \n            self.ask=False\n        if self.epochs == 1:\n            self.ask=False\n        else:\n            if self.base_model !=None and self.base_model.trainable == False:                \n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt, continue \\n'\n                msg=msg + 'or make the base model trainable then asked to enter number of epochs to run then ask again\\n'\n                print (msg, flush=True)\n            else:\n                msg=f'Training will proceed until epoch {ask_epoch}, then you will be asked to either halt \\n'\n                msg=msg + 'or enter the number of epochs to train on then you will be asked again\\n'\n                print (msg, flush=True)\n            \n        self.start_time= time.time()\n        \n    def on_train_end(self, logs=None):        \n        tr_duration=time.time() - self.start_time            \n        hours = tr_duration \/\/ 3600\n        minutes = (tr_duration - (hours * 3600)) \/\/ 60\n        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n        print (msg, flush=True) \n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        if self.ask:\n            if epoch + 1 ==self.ask_epoch:                \n                if self.base_model !=None and self.base_model.trainable ==  False:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                    msg=msg + 'or T to train base model\\n'                    \n                else:\n                    msg='\\n Enter H to end training,an integer for the number of additional epochs to run then ask again\\n'\n                ans=input(msg )\n                if ans == 'T' or ans== 't':\n                    self.base_model.trainable=True\n                    print ('Base Model trainable is now set to ', self.base_model.trainable)\n                    msg='\\n enter an integer for the number of additional epochs to run then ask again\\n'\n                    ans=(input(msg))\n                    \n                if ans == 'H' or ans =='h' or ans == '0':\n                    print ('\\nTraining halted on epoch ', epoch+1, ' due to user input\\n', flush=True)\n                    self.model.stop_training = True\n                else:\n                    self.ask_epoch += int(ans)\n                    if self.ask_epoch > self.epochs:\n                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush=True)\n                    else:\n                        print ('\\n Training will continue to epoch ', self.ask_epoch, flush=True)\n                        \n            \n            ","d31a1bd7":"train_dir=r'..\/input\/100-bird-species\/train'\ntest_dir=r'..\/input\/100-bird-species\/test'\nvalid_dir=r'..\/input\/100-bird-species\/valid'","c6ea7384":"img_shape=(128,128,3) # use 128 X128 versus 224 X224 to reduce training time\nimg_size=(img_shape[0], img_shape[1])\nmsg='For training set'\nprint_in_color(msg, (0,255,255), (55,65,80))\ntrain_ds=tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir, image_size=img_size, seed=123, batch_size=30, shuffle=True)\nmsg=' For validation set'\nprint_in_color(msg, (0,255,255), (55,65,80))\nvalid_ds=tf.keras.preprocessing.image_dataset_from_directory(directory=valid_dir, image_size=img_size, seed=123, batch_size=30, shuffle=True)\nmsg='For the test set'\nprint_in_color(msg, (0,255,255), (55,65,80))\ntest_ds=tf.keras.preprocessing.image_dataset_from_directory(\n            directory=test_dir, image_size=img_size, shuffle=False, batch_size=30) # set shuffle=False to keep file order","0721da78":"class_names=train_ds.class_names\nclass_count=len(class_names)\nplt.figure(figsize=(20,20))\nfor images, labels in train_ds.take(1):\n    for i in range (25):\n        plt.subplot(5,5,i +1)\n        img=images[i]\/255         \n        plt.title(class_names[labels[i]], color='blue', fontsize=12)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.show()\n","4487b5bc":"base_model=tf.keras.applications.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \nx=base_model.output\nbase_model.trainable=False # Set base model as NOT trainable initially\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\nx=Dropout(rate=.45, seed=123)(x)        \noutput=Dense(class_count, activation='softmax')(x)\nmodel=Model(inputs=base_model.input, outputs=output)\nmodel.compile(Adamax(lr=.001), loss='sparse_categorical_crossentropy', metrics=['accuracy']) ","45f36dbc":"epochs=10\nask_epoch=2\nrlronp=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5,  patience=1, verbose=1)\ncallbacks=[rlronp,TRC(model=model, base_model=base_model,epochs=epochs,  ask_epoch=ask_epoch)]\nhistory=model.fit( train_ds, validation_data=valid_ds, epochs=epochs, verbose=1, callbacks=callbacks)\n  ","5bc788f3":"tr_plot(history, 0)","7b822870":"ytrue=[]\nfor images, label in test_ds:   \n    for e in label:\n        ytrue.append(class_names[e]) # list of class names associated with each image file in test dataset \nypred=[]\nerrors=0\ncount=0\npreds=model.predict(test_ds, verbose=1) # predict on the test data\nfor i, p in enumerate(preds):\n    count +=1\n    index=np.argmax(p) # get index of prediction with highest probability\n    klass=class_names[index] \n    ypred.append(klass)  \n    if klass != ytrue[i]:\n        errors +=1\nacc= (count-errors)* 100\/count\nmsg=f'there were {count-errors} correct predictions in {count} tests for an accuracy of {acc:6.2f} % '\nprint_in_color(msg, (0,255,255), (55,65,80)) \nypred=np.array(ypred)\nytrue=np.array(ytrue)\nclr = classification_report(ytrue, ypred, target_names=class_names)\nprint(\"Classification Report:\\n----------------------\\n\", clr)    ","f584e562":"### plot the training data","395be345":"### make predictions on test set, compute accuracy and create classification report","6e704d77":"### create the model","91fa0ee6":"### define a sub class of keras callbacks\nThe TRC custom callback is useful when you are using transfer learning training your model.  Initially in transfer learning you set the base model as base_model.trainable= False where base model is the transfer learning model you seleted, You then run for a number of epochs at which point you now want to fine tune your model by making the base model trainable. The TRC callback makes that convenient to do. \n\ncallbacks=[ASK(model, base_model, epochs,  ask_epoch)] where:\n\n - model is the variable name for your compiled model \n - base_model is the name of your base model (transfer learning model)\n - epochs is an integer representing the number of epochs the model is to be trained for\n - ask_epoch is  an integer that specifies how many epoch to train up to until the user is quired.  You can enter an \n   integer N to train for N more epochs with the base model not trainable then you you be queried again, or you can enter    H to halt training, or you can enter T to make the base model trainable. If T is entered you will then be asked to\n   enter an integer N for how many more epochs to train then you will be queried again\n   \n","2c456557":"### create reduce learning rate on plateau callback and instantiate TRC callback\n### then train the model","1283f1a9":"### show some  images- ","38869061":"### create a function to plot training data from model.fit","f947e70c":"### define function to print text in RGB foreground and background colors","af2181e6":"### define the directories ","a960be8b":"### create the Datasets","03e63c44":"# My experience is you get better results in less epochs when using transfer learning if you leave the base_model (transfer learning model) as trainable. The docs say not to do that but I have run hundreds of models that way. As proof look at the results in this notebook at epoch 10. Then look at the results in notebook TF datasets F1 score=98% in this same data set. At epoch 10 the results are much better. It uses exactly the same data and the same model "}}