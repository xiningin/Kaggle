{"cell_type":{"a6caaf25":"code","61764b2a":"code","1a5a1839":"code","af62debd":"code","74582dde":"code","66e27553":"code","493a7a8a":"code","aa9a11f7":"code","62295ea9":"code","fe7babce":"code","e768bf3f":"code","47470d2d":"code","994149b5":"code","8f355d2c":"code","c14491cf":"code","ae805baf":"code","34aaf55d":"code","9797f7a0":"code","673b87f6":"code","1f88bc92":"code","43332d94":"markdown"},"source":{"a6caaf25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","61764b2a":"train=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nprint(train.shape)\ntrain.head()","1a5a1839":"test=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nprint(test.shape)\ntest.head()","af62debd":"Y_train=train[\"label\"]\nX_train=train.drop(labels=[\"label\"],axis=1)","74582dde":"plt.figure(figsize=(15,7))\ng=sns.countplot(Y_train,palette=\"icefire\")\nplt.title(\"Number of digit classes\")\nY_train.value_counts()","66e27553":"img=X_train.iloc[5].as_matrix()\nimg=img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train.iloc[0,0])\nplt.axis(\"off\")\nplt.show()","493a7a8a":"img = X_train.iloc[13].as_matrix()\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train.iloc[3,0])\nplt.axis(\"off\")\nplt.show()","aa9a11f7":"#Normalization\nX_train=X_train\/255.0\ntest=test\/255.0\nprint(\"x_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","62295ea9":"X_train=X_train.values.reshape(-1,28,28,1)\ntest=test.values.reshape(-1,28,28,1)\nprint(\"x_train shape: \",X_train.shape)\nprint(\"test shape: \",test.shape)","fe7babce":"#Label Encoding\nfrom keras.utils.np_utils import to_categorical\nY_train=to_categorical(Y_train,num_classes=10)","e768bf3f":"from sklearn.model_selection import train_test_split\nX_train,X_val,Y_train,Y_val=train_test_split(X_train,Y_train,test_size=0.1,random_state=29)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_test shape\",Y_val.shape)","47470d2d":"plt.imshow(X_train[13][:,:,0],cmap='gray')\nplt.show()","994149b5":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D,MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel=Sequential()\n\nmodel.add(Conv2D(filters=16,kernel_size=(5,5),padding='Same',\n                activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n#\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),padding='Same',\n                activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Dropout(0.2))\n\n#Fully Connected\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,activation='softmax'))","8f355d2c":"optimizer=Adam(lr=0.001,beta_1=0.9,beta_2=0.999)","c14491cf":"model.compile(optimizer,loss=\"categorical_crossentropy\",metrics=['accuracy'])","ae805baf":"epochs=30\nbatch_size=300","34aaf55d":"datagen=ImageDataGenerator(\nfeaturewise_center=False,\nsamplewise_center=False,\nfeaturewise_std_normalization=False,\nsamplewise_std_normalization=False,\nzca_whitening=False,\nrotation_range=0.5,\nzoom_range=0.5,\nwidth_shift_range=0.5,\nheight_shift_range=0.5,\nhorizontal_flip=0.5,\nvertical_flip=0.5)\ndatagen.fit(X_train)","9797f7a0":"history=model.fit_generator(datagen.flow(X_train,Y_train,batch_size=batch_size),\n                           epochs=epochs,validation_data=(X_val,Y_val),steps_per_epoch=X_train.shape[0]\/\/batch_size)","673b87f6":"plt.plot(history.history['val_loss'],color='b',label='validation loss')\nplt.title(\"Test Loss\")\nplt.xlabel(\"Number of epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n","1f88bc92":"import seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","43332d94":"**Train Test Split**"}}