{"cell_type":{"14ba13aa":"code","2b03f495":"code","6f27f6fc":"code","f30ca604":"code","80bb0160":"code","abbd21d3":"code","b4faf502":"code","908a54fb":"code","fda6a3d0":"code","6d43ed43":"code","34f4ac4f":"code","f7434406":"code","b7d4869d":"code","e6a06ef6":"code","b1a8d6fc":"code","6e535480":"code","3be66df8":"code","39767125":"code","5e6c9b9b":"code","a1ec8a6f":"code","cc401cbe":"code","2462387b":"markdown","600cc051":"markdown","e071b9d1":"markdown","4df8660c":"markdown","d697d491":"markdown","27ae3fbd":"markdown","5bd7fc03":"markdown","e9251fd5":"markdown","6897431e":"markdown","60ca9635":"markdown","972992f5":"markdown"},"source":{"14ba13aa":"import datetime\nimport pandas as pd\nfrom time import time\n\nimport numpy as np\nfrom scipy.optimize import minimize, fsolve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import log_loss","2b03f495":"def log_loss_numpy(y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    for i in range(y_pred.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss \/ y_pred.shape[1]\n\ndef calc_auc(y_pred):\n    auc = 0\n    for task_id in range(y_pred.shape[1]):\n        auc -= roc_auc_score(y_true=y_true[:, task_id], y_score=y_pred[:, task_id])\n    return auc \/ y_pred.shape[1]\n\ndef func_numpy_metric2(weights):\n    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n    score = calc_auc(oof_blend)\n    return score\n\ndef func_numpy_metric(weights):\n    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n    score = log_loss_numpy(oof_blend)\n    return score\n\ndef grad_func(weights):\n    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n    gradients = np.zeros(oof.shape[0])\n    for i in range(oof.shape[0]):\n        a, b, c = y_true, oof_clip[i], 0\n        for j in range(oof.shape[0]):\n            if j != i:\n                c += weights[j] * oof_clip[j]\n        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)\/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n    return gradients","6f27f6fc":"train_targets_scored = pd.read_csv('..\/input\/oof-weight-optimizer-public\/model1.csv')\ntarget_columns = [c for c in train_targets_scored.columns if c not in ['id']]\n\ny_true = train_targets_scored['target'].values\ny_true = y_true.reshape(-1,1)\noof_dict = {\n    'model1':\"..\/input\/oof-weight-optimizer-public\/model1.csv\",\n    'model2':\"..\/input\/oof-weight-optimizer-public\/model2.csv\",\n    'model3':\"..\/input\/oof-weight-optimizer-public\/model3.csv\",   \n    'model4':\"..\/input\/oof-weight-optimizer-public\/model4.csv\",   \n    'model5':\"..\/input\/oof-weight-optimizer-public\/model5.csv\",   \n    'model6':\"..\/input\/oof-weight-optimizer-public\/model6.csv\",   \n}\n\ntarget_columns  = ['preds']\noof = np.zeros((len(oof_dict), y_true.shape[0], len(target_columns) ))\n\nfor i in range(oof.shape[0]):\n    valid = pd.read_csv( list(oof_dict.values())[i] )\n    valid = train_targets_scored.drop(columns=target_columns).merge(valid[['id']+target_columns], on='id', how='left').fillna(0)\n    oof[i] = valid[target_columns].values ","f30ca604":"%%time\n\nlog_loss_scores = {}\nfor n, key in enumerate(oof_dict.keys()):\n    score_oof = calc_auc(oof[n])\n    log_loss_scores[key] = score_oof\n    print(f'{key:40s} CV:', score_oof)\n    \nprint('-' * 60)","80bb0160":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\n\nsubmit = pd.read_csv(\"..\/input\/g2net-gravitational-wave-detection\/training_labels.csv\")\nsubs = np.zeros((len(oof_dict), y_true.shape[0], len(target_columns) ))\n\nfor i, p in enumerate(oof_dict.keys()):\n    print(i,p)\n    tmp = pd.read_csv(oof_dict[p])\n    valid = train_targets_scored.drop(columns=target_columns).merge(tmp[['id']+target_columns], on='id', how='left').fillna(0)\n    subs[i,:,:] = valid[target_columns].values \n\ncorr = np.corrcoef(subs.reshape(len(oof_dict), -1))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, annot=True, fmt=\"g\",\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\nax.set_ylim(corr.shape[0], 0)\nplt.yticks(rotation=0)","abbd21d3":"tol = 1e-10\ninit_guess = [1 \/ oof.shape[0]] * oof.shape[0]\nbnds = [(0, 1) for _ in range(oof.shape[0])]\ncons = {'type': 'eq', \n        'fun': lambda x: np.sum(x) - 1, \n        'jac': lambda x: [1] * len(x)}\n\nprint('Inital Blend OOF:', func_numpy_metric2(init_guess))\nstart_time = time()\n\nres_scipy = minimize(fun = func_numpy_metric2, \n                     x0 = init_guess, \n                     method = 'Nelder-Mead', \n                     #method='SLSQP',\n                     jac = grad_func, \n                     bounds = bnds, \n                     constraints = cons, \n                     tol = tol)\n\nprint(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Optimised Blend OOF:', res_scipy.fun)\nprint('Optimised Weights:', res_scipy.x)\nprint('-' * 70)\n\nfor n, key in enumerate(oof_dict.keys()):\n    print(f'{key:40s} Optimised Weights:', res_scipy.x[n])","b4faf502":"ws = [ res_scipy.x[i] for i in range(len(oof_dict.keys()))]\nws\/np.sum(ws)","908a54fb":"oof = np.zeros((len(oof_dict), y_true.shape[0], len(target_columns) ))\n\nfor i in range(oof.shape[0]):\n    valid = pd.read_csv( list(oof_dict.values())[i] )\n    valid = train_targets_scored.drop(columns=target_columns).merge(valid[['id']+target_columns], on='id', how='left').fillna(0)\n    oof[i] = valid[target_columns].values","fda6a3d0":"from scipy.stats import rankdata\nfor i in range(oof.shape[0]):\n    for j in range(len(target_columns)):\n        oof[i,:,j] = rankdata(oof[i,:,j],method='average')","6d43ed43":"overall_oof = np.mean(oof, axis=0)\/np.max(oof)\nprint(calc_auc(overall_oof))","34f4ac4f":"predictions = []\ntarget_columns = ['target']\nlabel_cols = target_columns","f7434406":"preds_dict = {\n    'model1':\"..\/input\/oof-weight-optimizer-public\/submission1.csv\",\n    'model2':\"..\/input\/oof-weight-optimizer-public\/submission2.csv\",\n    'model3':\"..\/input\/oof-weight-optimizer-public\/submission3.csv\",\n    'model4':'..\/input\/oof-weight-optimizer-public\/submission4.csv',\n    'model5':'..\/input\/oof-weight-optimizer-public\/submission5.csv',\n    'model6':'..\/input\/oof-weight-optimizer-public\/submission6.csv',\n}","b7d4869d":"for i in range(len(preds_dict)):\n    sub = pd.read_csv(list(preds_dict.values())[i])\n    predictions.append(sub)","e6a06ef6":"y_pred = predictions[0]['target'].values\ntrain_targets_scored = predictions[0]\npreds = np.zeros((len(preds_dict), y_pred.shape[0], len(target_columns) ))","b1a8d6fc":"weights = ws\nweights = weights\/np.sum(weights)","6e535480":"weighted_y_pred = pd.DataFrame()\nweighted_y_pred['id'] = predictions[0]['id']\nfor column in label_cols:\n    column_data = []\n    for i in range(len(preds_dict)):\n        column_data.append(predictions[i][column] * weights[i])\n    weighted_y_pred[column] = np.sum(column_data, axis=0)","3be66df8":"submission = weighted_y_pred\nsubmission.to_csv('submission_optimized.csv', index=False)\nsubmission.head()","39767125":"weighted_y_pred = pd.DataFrame()\nweighted_y_pred['id'] = predictions[0]['id']\nfor column in label_cols:\n    column_data = []\n    for i in range(len(preds_dict)):\n        column_data.append(predictions[i][column] \/ len(preds_dict))\n    weighted_y_pred[column] = np.sum(column_data, axis=0)","5e6c9b9b":"submission = weighted_y_pred\nsubmission.to_csv('submission_mean.csv', index=False)\nsubmission.head()","a1ec8a6f":"preds = np.zeros((len(preds_dict), y_pred.shape[0], len(target_columns) ))\n#print(preds.shape)\nweighted_y_pred = pd.DataFrame()\nweighted_y_pred['id'] = predictions[0]['id']\nfor i in range(preds.shape[0]):\n    valid = pd.read_csv( list(preds_dict.values())[i] )\n    valid = train_targets_scored.drop(columns=target_columns).merge(valid[['id']+target_columns], on='id', how='left').fillna(0)\n    preds[i] = valid[target_columns].values\n    \nfrom scipy.stats import rankdata\n\nfor i in range(preds.shape[0]):\n    for j in range(len(target_columns)):\n        preds[i,:,j] = rankdata(preds[i,:,j],method='average')\n        \nweighted_y_pred[label_cols] = np.mean(preds,axis=0)","cc401cbe":"submission = weighted_y_pred\nsubmission.to_csv('submission_rank.csv', index=False)\nsubmission.head()","2462387b":"### Simple averaged","600cc051":"### Rank averaging","e071b9d1":"### Define major metrics","4df8660c":"# Making submission","d697d491":"### open oof csv files","27ae3fbd":"### Observe correlation","5bd7fc03":"### Using optimized weight","e9251fd5":"## This notebook forked from [Optimise Blending Weights with Bonus :0](https:\/\/www.kaggle.com\/gogo827jz\/optimise-blending-weights-with-bonus-0) by [Yirun Zhang](https:\/\/www.kaggle.com\/gogo827jz).","6897431e":"### Rank Averaging from [tips: rank averaging](https:\/\/www.kaggle.com\/c\/ranzcr-clip-catheter-line-classification\/discussion\/205564) by [Tawara](https:\/\/www.kaggle.com\/ttahara).","60ca9635":"### Blending Weights Optimize","972992f5":"The nelder-mead method requires normalization because the sum of the weights does not equal 1."}}