{"cell_type":{"2824883a":"code","da0adaa7":"code","06a9ee8d":"code","8d4708c9":"code","8fbd764e":"code","ebad664e":"code","7b8c831c":"code","9e68d9c1":"code","8ff5d4da":"code","b5e5fc4a":"code","fd4afec7":"code","ecc89ba1":"code","3f23b6fd":"code","d3d53658":"code","02c177bb":"code","e34f5b9e":"code","13f8a864":"code","91ec278d":"code","d784ee21":"code","ffd3b4d8":"code","dcbc13c7":"code","3ee37938":"code","de91b5c3":"code","c3b18687":"code","fac05d0c":"code","e4d65a41":"code","2964c3f1":"code","d1b0a1a8":"code","0d520269":"code","f32c7bb4":"code","2f9847cb":"code","a5509ebd":"code","4e266288":"code","d655edd5":"code","519e97b0":"code","b42bbb95":"code","adaa377a":"code","b671f959":"code","60480e74":"code","7a82d4db":"code","ff49b298":"code","97c06a56":"code","0e93d4bf":"code","7ab0c902":"code","348a2116":"code","2518551d":"code","75fe889d":"code","fedd8641":"markdown","a1214b86":"markdown","a7572b7c":"markdown","18bc9c9f":"markdown","fcf2a9d5":"markdown"},"source":{"2824883a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da0adaa7":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","06a9ee8d":"from sklearn.preprocessing import LabelEncoder","8d4708c9":"feature0_encoder=LabelEncoder()\nfeature1_encoder=LabelEncoder()\nfeature2_encoder=LabelEncoder()\nfeature3_encoder=LabelEncoder()\nfeature4_encoder=LabelEncoder()\nfeature5_encoder=LabelEncoder()\nfeature6_encoder=LabelEncoder()\nfeature7_encoder=LabelEncoder()\nfeature8_encoder=LabelEncoder()\nfeature9_encoder=LabelEncoder()","8fbd764e":"train['cat0']=feature0_encoder.fit_transform(train['cat0'])\ntrain['cat1']=feature1_encoder.fit_transform(train['cat1'])\ntrain['cat2']=feature2_encoder.fit_transform(train['cat2'])\ntrain['cat3']=feature3_encoder.fit_transform(train['cat3'])\ntrain['cat4']=feature4_encoder.fit_transform(train['cat4'])\ntrain['cat5']=feature5_encoder.fit_transform(train['cat5'])\ntrain['cat6']=feature6_encoder.fit_transform(train['cat6'])\ntrain['cat7']=feature7_encoder.fit_transform(train['cat7'])\ntrain['cat8']=feature8_encoder.fit_transform(train['cat8'])\ntrain['cat9']=feature9_encoder.fit_transform(train['cat9'])","ebad664e":"test['cat0']=feature0_encoder.transform(test['cat0'])\ntest['cat1']=feature1_encoder.transform(test['cat1'])\ntest['cat2']=feature2_encoder.transform(test['cat2'])\ntest['cat3']=feature3_encoder.transform(test['cat3'])\ntest['cat4']=feature4_encoder.transform(test['cat4'])\ntest['cat5']=feature5_encoder.transform(test['cat5'])\ntest['cat6']=feature6_encoder.transform(test['cat6'])\ntest['cat7']=feature7_encoder.transform(test['cat7'])\ntest['cat8']=feature8_encoder.transform(test['cat8'])\ntest['cat9']=feature9_encoder.transform(test['cat9'])","7b8c831c":"x_train = train.drop(['id','target'],axis=1)\ny_train = train['target']\ntest_id = test['id']\nx_test = test.drop(['id'],axis=1)","9e68d9c1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom warnings import filterwarnings\n\nfilterwarnings(\"ignore\", category=DeprecationWarning) \nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","8ff5d4da":"X_train,X_val,Y_train,Y_val = train_test_split(x_train,y_train,random_state=42)","b5e5fc4a":"def check_rmse(model,x_val,y_val):\n    pred = model.predict(x_val)\n    return np.sqrt(mean_squared_error(y_val,pred))","fd4afec7":"# n_jobs = -1 means to use all the CPU available\nknn_reg = KNeighborsRegressor(n_jobs=-1)\nmlp_reg = MLPRegressor()\nrf_reg = RandomForestRegressor(n_jobs=-1)\ngb_reg = GradientBoostingRegressor()\nxgb_reg = XGBRegressor(n_jobs=-1)\nlgbm_reg = LGBMRegressor(n_jobs=-1)\ncat_reg =  CatBoostRegressor()\n","ecc89ba1":"models = [knn_reg,mlp_reg,rf_reg,gb_reg,xgb_reg,lgbm_reg,cat_reg]\nmodels_name = ['knn_reg','mlp_reg','rf_reg','gb_reg','xgb_reg','lgbm_reg','cat_reg']","3f23b6fd":"rmse_error = []\nfor i,model in enumerate(models):\n    model.fit(X_train,Y_train)\n    rmse = check_rmse(model,X_val,Y_val)\n    rmse_error.append(rmse)\n    print(f\"Model : {models_name[i]}   rmse = {rmse}\")","d3d53658":"import matplotlib.pyplot as plt\n%matplotlib inline","02c177bb":"plt.barh(models_name,rmse_error)\nplt.ylabel(\"Models\")\nplt.xlabel(\"RMSE\") \nplt.show()\n# Lower the rmse the better","e34f5b9e":"def submission(model,filename):\n    pred = model.predict(x_test)\n    pred = pd.DataFrame(pred,columns=['target'])\n    sub = pd.concat([test_id,pred],axis=1)\n    sub.set_index('id',inplace=True)\n    sub.to_csv(f\"Submission_file_{filename}.csv\")","13f8a864":"submission(cat_reg,\"Catboost\")","91ec278d":"import skopt\nimport skopt.plots\nfrom datetime import datetime","d784ee21":"class Params_Evaluate():\n    def __init__(self, X_train, X_val, y_train, y_val):\n        self.X_train = X_train\n        self.X_val =  X_val\n        self.y_train = y_train\n        self.y_val = y_val\n        self.n=0\n        \n    def select_model(self, model):\n        self.model = model\n\n        \n    def evaluate_params(self,params):\n        model =  self.model.set_params(**params)\n        model.fit(self.X_train, self.y_train)\n        \n        rmse_train = check_rmse(model, self.X_train, self.y_train)\n        rmse_val = check_rmse(model, self.X_val, self.y_val)\n        \n        print(\"Iteration {} with RMSE = {}\/{} (val\/train) at {}\".format(self.n, rmse_val, rmse_train, str(datetime.now().time())[:8]))\n        self.n+=1\n        \n        return(rmse_val)","ffd3b4d8":"search_space = [\n         skopt.space.Integer(4, 12, name='max_depth'),\n         skopt.space.Integer(50, 200, name='n_estimators'),\n         skopt.space.Integer(17, 24, name='max_features'),\n         skopt.space.Real(0.0, 1.0, name='min_impurity_decrease'),\n         skopt.space.Categorical(categories = [True, False],name=\"bootstrap\")\n         ]","dcbc13c7":"HPO_params = {\n              'n_calls':100,\n              'n_random_starts':20,\n              'base_estimator':'ET',\n              'acq_func':'EI',\n              'n_jobs' : -1\n             }","3ee37938":"evaluator = Params_Evaluate(X_train, X_val, Y_train, Y_val)","de91b5c3":"evaluator.select_model(rf_reg)","c3b18687":"@skopt.utils.use_named_args(search_space)\ndef objective(**params):\n    return  evaluator.evaluate_params(params)","fac05d0c":"%%time\nresults = skopt.forest_minimize(objective, search_space,**HPO_params)","e4d65a41":"skopt.plots.plot_convergence(results)","2964c3f1":"def to_named_params(results, search_space):\n    params = results.x\n    param_dict = {}\n    params_list  =[(dimension.name, param) for dimension, param in zip(search_space, params)]\n    for item in params_list:\n        param_dict[item[0]] = item[1]\n    \n    return(param_dict)","d1b0a1a8":"best_params = to_named_params(results, search_space)\nbest_params['n_estimators'] = int(best_params['n_estimators'])\nbest_params['max_features'] = int(best_params['max_features'])\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params","0d520269":"best_rf_reg = rf_reg.set_params(**best_params )","f32c7bb4":"best_rf_reg.fit(X_train,Y_train)","2f9847cb":"check_rmse(best_rf_reg,X_val,Y_val)","a5509ebd":"submission(best_rf_reg,\"best_rf\")","4e266288":"search_space_xgb= [\n         skopt.space.Integer(1000, 10000, name='n_estimators'),\n         skopt.space.Integer(2, 12, name='max_depth'),\n         skopt.space.Real(0.0, 1.0, name='learning_rate'),\n         skopt.space.Real(0.0, 1.0, name='eta'),\n         skopt.space.Real(0.0, 1.0, name='colsample_bytree'),\n         skopt.space.Real(0.0, 1.0, name='subsample'),\n         skopt.space.Integer(1, 30, name='reg_lambda'),\n         skopt.space.Integer(0, 30, name='reg_alpha'),\n         skopt.space.Categorical(categories = [\"gbtree\", \"dart\"],name=\"booster\")\n         ]","d655edd5":"@skopt.utils.use_named_args(search_space_xgb)\ndef objective(**params):\n    return  evaluator.evaluate_params(params)","519e97b0":"evaluator = Params_Evaluate(X_train, X_val, Y_train, Y_val)\nevaluator.select_model(xgb_reg)","b42bbb95":"%%time\n# uncomment the below lines to train for xgboost. It was taking a lot of time when whole notebook ran altogether\n# results_xgb = skopt.forest_minimize(objective, search_space_xgb,**HPO_params)\n","adaa377a":"# best_params_gxb = to_named_params(results_xgb, search_space_xgb)\n# best_params_gxb['n_estimators'] = int(best_params_gxb['n_estimators'])\n# best_params_gxb['max_depth'] = int(best_params_gxb['max_depth'])\n\n# best_xgb_reg = xgb_reg.set_params(**best_params_gxb )\n# best_xgb_reg\n","b671f959":"# best_xgb_reg.fit(x_train,y_train)\n# submission(best_xgb_reg,\"best_xgb\")","60480e74":"from sklearn.model_selection import KFold\n","7a82d4db":"k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\npreds = 0\ntotal_mean_rmse = 0\nfor num, ( valid_idx,train_idx) in enumerate(k_fold.split(x_train)):\n    X_train, X_valid = x_train.loc[train_idx], x_train.loc[valid_idx]\n    Y_train, Y_valid = y_train.loc[train_idx], y_train.loc[valid_idx]\n    try:\n        model = XGBRegressor(**xgb_params,n_jobs=-1)\n    except:\n        #this runs if above xgboost hyperparameter tuning is not done\n        model = XGBRegressor(n_jobs=-1)\n    model.fit(X_train, Y_train,\n              verbose=False,\n              eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              )\n\n    preds += model.predict(x_test) \/ 10\n    fold_rmse = check_rmse(model,X_valid,Y_valid)\n    print(f\"In Fold {num} RMSE: {fold_rmse}\")\n    total_mean_rmse += fold_rmse \/ 10\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")\npreds = pd.DataFrame(preds,columns=['target'])\nsub = pd.concat([test_id,preds],axis=1)\nsub.set_index('id',inplace=True)\nsub.to_csv(\"Submission_file_Kfold_.csv\")","ff49b298":"import lightgbm as lgb\nimport xgboost as xgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import r2_score","97c06a56":"def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=8, random_seed=6,n_estimators=10000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    \n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'regression', 'metric':'mae'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=False, verbose_eval =200, metrics=['mae'])\n        return max(cv_result['l1-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n                                            'num_leaves': (24, 80),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (5, 30),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 80),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=200)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    models=[]\n    for model in range(len( lgbBO.res)):\n        models.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(models).idxmax()]['target'],lgbBO.res[pd.Series(models).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(x_train, y_train, init_round=5, opt_round=10, n_folds=3,random_seed=6,n_estimators=10000)","0e93d4bf":"opt_params ","7ab0c902":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='regression'\nopt_params[1]['metric']='mae'\nopt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","348a2116":"d_train=lgb.Dataset(x_train, label=y_train)\nmodel2=lgb.train(opt_params,d_train,2000)","2518551d":"check_rmse(model2,X_valid,Y_valid)\n#Much better but almost overfit","75fe889d":"submission(model2,\"lgbBO\")","fedd8641":"## Base models with default params","a1214b86":"### Random Forest","a7572b7c":"### XG Boost","18bc9c9f":"### KFold","fcf2a9d5":"### BayesianOptimization"}}