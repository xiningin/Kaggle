{"cell_type":{"69043856":"code","30d56113":"code","7517e946":"code","9a464c2b":"code","7187c430":"code","61000005":"code","c9d55ed1":"code","1fe7b16a":"code","88797764":"code","72bc02e2":"code","94a8dfcb":"code","3d0ffc84":"code","2595c91a":"code","8c50b9e8":"markdown","90768fcc":"markdown","c87f1cac":"markdown","d4a15980":"markdown","ac78f47c":"markdown","018a17d9":"markdown","27628813":"markdown","b0168434":"markdown","b79542f9":"markdown","3179a2dd":"markdown","f5e91f48":"markdown","e334ea0f":"markdown"},"source":{"69043856":"import numpy as np # For arithmetics and arrays\nimport math # For inbuilt math functions\nimport pandas as pd # For handling data frames\nimport collections # used for dictionaries and counters\nfrom itertools import permutations # used to find permutations\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function to easily split data into training and testing samples\nfrom sklearn.decomposition import PCA # Principal component analysis used to reduce the number of features in a model\nfrom sklearn.preprocessing import StandardScaler # used to scale data to be used in the model\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\n\nimport pickle # To save the trained model and then read it\n\nimport seaborn as sns # Create plots\nsns.set(style=\"ticks\")\nimport matplotlib.pyplot as plt","30d56113":"import requests\nurl = 'https:\/\/raw.githubusercontent.com\/amankharwal\/Website-data\/master\/lang_data.csv'\nres = requests.get(url, allow_redirects=True)\nwith open('lang_data.csv','wb') as file:\n    file.write(res.content)\ndf = pd.read_csv('lang_data.csv')","7517e946":"df","9a464c2b":" # Read raw data\ndf = df.dropna() # remove null values for the \"text\" column\ndf['text'] = df['text'].astype(str) # Convert the column \"text\" from object to a string in order to operate on it\ndf['language'] = df['language'].astype(str)","7187c430":"# Define a list of commonly found punctuations\npunc = ('!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\")\nvowels=['a','e','i','o','u']\n# Define a list of double consecutive vowels which are typically found in Dutch and Afrikaans languages\nsame_consecutive_vowels = ['aa','ee', 'ii', 'oo', 'uu'] \nconsecutive_vowels = [''.join(p) for p in permutations(vowels,2)]\ndutch_combos = ['ij']\n\n# Create a pre-defined set of features based on the \"text\" column in order to allow us to characterize the string\ndf['word_count'] = df['text'].apply(lambda x : len(x.split()))\ndf['character_count'] = df['text'].apply(lambda x : len(x.replace(\" \",\"\")))\ndf['word_density'] = df['word_count'] \/ (df['character_count'] + 1)\ndf['punc_count'] = df['text'].apply(lambda x : len([a for a in x if a in punc]))\ndf['v_char_count'] = df['text'].apply(lambda x : len([a for a in x if a.casefold() == 'v']))\ndf['w_char_count'] = df['text'].apply(lambda x : len([a for a in x if a.casefold() == 'w']))\ndf['ij_char_count'] = df['text'].apply(lambda x : sum([any(d_c in a for d_c in dutch_combos) for a in x.split()]))\ndf['num_double_consec_vowels'] = df['text'].apply(lambda x : sum([any(c_v in a for c_v in same_consecutive_vowels) for a in x.split()]))\ndf['num_consec_vowels'] = df['text'].apply(lambda x : sum([any(c_v in a for c_v in consecutive_vowels) for a in x.split()]))\ndf['num_vowels'] = df['text'].apply(lambda x : sum([any(v in a for v in vowels) for a in x.split()]))\ndf['vowel_density'] = df['num_vowels']\/df['word_count']\ndf['capitals'] = df['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\ndf['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])\/float(row['character_count']),axis=1)\ndf['num_exclamation_marks'] =df['text'].apply(lambda x: x.count('!'))\ndf['num_question_marks'] = df['text'].apply(lambda x: x.count('?'))\ndf['num_punctuation'] = df['text'].apply(lambda x: sum(x.count(w) for w in punc))\ndf['num_unique_words'] = df['text'].apply(lambda x: len(set(w for w in x.split())))\ndf['num_repeated_words'] = df['text'].apply(lambda x: len([w for w in collections.Counter(x.split()).values() if w > 1]))\ndf['words_vs_unique'] = df['num_unique_words'] \/ df['word_count']\ndf['encode_ascii'] = np.nan\nfor i in range(len(df)):\n    try:\n        df['text'].iloc[i].encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n        df['encode_ascii'].iloc[i] = 0\n    else:\n        df['encode_ascii'].iloc[i] = 1","61000005":"df.groupby('language').mean().T","c9d55ed1":"df.corr(method ='pearson')","1fe7b16a":"sns.pairplot(df)","88797764":"#split dataset into features and target variable\nfeature_cols = list(df.columns)[2:]\nX = df[feature_cols] # Features\ny = df[['language']] # Target variable\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% train and 20% test","72bc02e2":"# Standardize the data\nscaler = StandardScaler()\n# Fit on training set only.\nscaler.fit(X_train)\n# Transform both the training set and the test set.\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Make an instance of the model to retain 95% of the variance within the old features.\npca = PCA(.95)\npca.fit(X_train)\n\nprint('Number of Principal Components = '+str(pca.n_components_))\n# Number of Principal Components = 13\n\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)","94a8dfcb":"dt_clf = DecisionTreeClassifier() # Create Decision Tree classifer object\ndt_clf = dt_clf.fit(X_train,y_train) # Fit\/Train Decision Tree Classifer on training set\n\n# Save model to file in the current working directory so that it can be imported and used.\n# I use the pickle library to save the parameters of the trained model\npkl_file = \"decision_tree_model.pkl\"\nwith open(pkl_file, 'wb') as file:\n    pickle.dump(dt_clf, file)\n\n# Load previously trained model from pickle file\nwith open(pkl_file, 'rb') as file:\n    dt_clf = pickle.load(file)\n\ndt_clf # parameters of the Decision Tree model are shown below and can be further optimized to improve model performance\n\ny_pred = dt_clf.predict(X_test) #Predict the response for test dataset","3d0ffc84":"accuracy_score_dt = accuracy_score(y_test, y_pred)","2595c91a":"labels = ['English', 'Afrikaans', 'Nederlands']\n# Confusion Matrix\ncm_Model_dt = confusion_matrix(y_test, y_pred, labels)\nfig = plt.figure(figsize=(9,9))\nax = fig.add_subplot(111)\nsns.heatmap(cm_Model_dt, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\ntitle = 'Decision Tree Model Accuracy Score = '+ str(round(accuracy_score_dt*100,2)) +\"%\"\nplt.title(title, size = 15)","8c50b9e8":"### The graph above shows how many texts were categorized correctly in each of the languages, with the y-axis representing actual or actual output and the x-axis representing expected output. This tells us that the model does well at predicting English texts, in addition to Afrikaans texts.","90768fcc":"We can also visualize the pairwise correlation matrix using the following command:","c87f1cac":"## Language Classification: Splitting The Data\nBefore going any further in building our linguistic classification model, we need to divide the dataset into training and test sets:\n\nI divided the dataset into 80% training and 20% testing. Other percentage splits can be used, but these values are generally used.\nThe training set will be used to fit the models and store the parameters, the robustness of which will be tested on the test set.","d4a15980":"The decision tree algorithm gave an accuracy of almost 90%. Now let\u2019s have a look at the confusion matrix to visualize the classified languages with their accuracy:","ac78f47c":"## Using Decision Tree Algorithm\nA decision tree model learns by dividing the training set into subsets based on an attribute value test, and this process is repeated over recursive partitions until the subset at a node has the same value as the target parameter, or when additional splitting does not improve. the predictive capacity of the model.\n\nI will adapt the decision tree classifier to the training set and save the model parameters to a pickle file, which can be imported for future use. We then use the model to predict or rank the texts in the languages using the test set.","018a17d9":"## Language Classification: Summarizing Features\nAfter building the above feature set, we can calculate averages of these features by language to check if there are any obvious significant differences. To do this, simply run the command below","27628813":"## Reducing Correlation\nWe should aim to use only the most unique characteristics in our classification models, as the correlated variables do not add much to the predictive power of the models.\n\nOne method used in machine learning to reduce the correlation between features is called principal component analysis or PCA:","b0168434":"## Language Classification: Correlation\nNext, we need to look at the degree of correlation between the characteristics we have created. The idea behind correlation with the context of our task it that if two or more characteristics are strongly correlated with each other, then it is likely that they will have very similar explanatory power when classifying languages.\n\nAs such, we can only keep one of these features and get the same predictive power from our model. To calculate the correlation matrix, we can run the following command:","b79542f9":"We can notice how several of the variables are strongly positively correlated. For example, word_count and character_count have a correlation of around 96%, which means they tell us roughly the same thing in terms of the length of a text for each language considered.","3179a2dd":"## Language Classification: Feature Creation\nI will now create a new set of features that will be used in the language classification model to classify text into three languages: English, Afrikaans, and Dutch. As mentioned, different features may be more effective in classifying other languages:","f5e91f48":"#### Looking at the first feature, for example, word_count, we can notice that Afrikaans sentences are likely to be made up of more words than English and Dutch.","e334ea0f":"## Language Classification with Machine Learning Using Python\nFirst I will need to import some of the common Python packages and modules used to manage data, metrics and machine learning models needed to build and evaluate our predictive models, as well as modules to visualize our data.\n\nSo let\u2019s start with the task of language classification with Machine Learning using the python programming language by importing all the modules and packages needed for this task:"}}