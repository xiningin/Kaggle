{"cell_type":{"cdf16723":"code","471bcce1":"code","874e70e7":"code","633be207":"code","d8b45e43":"code","9761a9c5":"code","aced2a33":"code","7f44f9f8":"code","c2c0d8f2":"code","f391e202":"code","110fde4a":"code","a04eb2b7":"code","7d61e253":"code","a5be3729":"code","002aa35c":"code","358b31ae":"code","b9364820":"code","19130914":"code","3cd98a15":"code","21c0f81b":"code","4d4ae352":"markdown","6509a0d0":"markdown","da9e480b":"markdown","28ea918f":"markdown","647147b1":"markdown","5b837b16":"markdown","11caa85b":"markdown","82c510a5":"markdown","4653c41e":"markdown","fc8ed0e1":"markdown","f8e4a097":"markdown","ea6ffe50":"markdown","0a9cea1d":"markdown","926c9236":"markdown","715a91d1":"markdown","93750b84":"markdown","ca5f3d21":"markdown","0e6cb548":"markdown","2aabd304":"markdown"},"source":{"cdf16723":"# Programming\nimport pandas as pd\nimport numpy as np\n\n# Machine Learning | sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Other\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")","471bcce1":"df = pd.read_csv('\/kaggle\/input\/nasa-asteroids-classification\/nasa.csv',\n                 parse_dates=['Close Approach Date', 'Orbit Determination Date', 'Epoch Date Close Approach'])","874e70e7":"# Fixing seed for reproducibility\nseed = 1234","633be207":"print(df.shape)\ndf.head()","d8b45e43":"df.info()","9761a9c5":"df.describe()","aced2a33":"# Check for missing values\nprint(df.isnull().sum())\n\n# Visually inspect missing values\nmsno.matrix(df)","7f44f9f8":"# Checking visually for feature correlation\nsns.set(rc={'figure.figsize':(30,20)})\nsns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"Spectral\", annot=True)\nplt.show()\nplt.close()","c2c0d8f2":"# Dropping completely correlated features and datetime features\ndf = df.drop(['Est Dia in M(min)', 'Est Dia in M(max)', 'Est Dia in Miles(min)', 'Est Dia in Miles(max)', 'Est Dia in Feet(min)', 'Est Dia in Feet(max)', 'Est Dia in KM(max)',\n              'Relative Velocity km per hr', 'Miles per hour',\n              'Miss Dist.(Astronomical)', 'Miss Dist.(lunar)', 'Miss Dist.(miles)',\n              'Semi Major Axis',\n              'Neo Reference ID', 'Name',\n              'Close Approach Date', 'Epoch Date Close Approach', 'Orbit Determination Date'],axis=1)\n\n# Plotting feature correlation with reduced dataset\nsns.set(rc={'figure.figsize':(30,20)})\nsns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"Spectral\", annot=True)\nplt.show()\nplt.close()","f391e202":"# Encoding the target variable\nl_enc = LabelEncoder()\ndf['hazardous'] = l_enc.fit_transform(df.Hazardous) \nprint('Hazardous == True -> 1')\nprint('Hazardous == False -> 0\\n')\n\n# Checking if the other categorical features need to be encoded\nprint(df['Orbiting Body'].unique())\nprint(df['Equinox'].unique())\nprint('\\n')\n# Removing them since there is only a single value that is identical across all observations\ndf = df.drop(['Orbiting Body', 'Equinox', 'Hazardous'], axis=1)\n\n# Check after all the changes\nprint(df.info())\ndf.head()","110fde4a":"# Creating the Features\/Label split as numpy arrays\nfeatures = df.drop('hazardous', axis=1).values\nlabel = df.hazardous.values\n\n# Creating the test\/train split\ntraining_features, test_features, training_label, test_label = train_test_split(features, label,\n                                                                                test_size=0.8,\n                                                                                stratify=label,\n                                                                                random_state=seed)\n\n# Getting feature labels for future plotting\ndf_graph = df.copy()\nfeature_names = df_graph.drop('hazardous', axis=1).columns.tolist()\ndel df_graph","a04eb2b7":"# Creating the pipeline\nlogreg_pipe = Pipeline([('Scaling', StandardScaler()),\n                        ('LogReg', LogisticRegression())])\n\n# Creating hyperparameter options\nlogreg_params = {'LogReg__C': np.arange(0, 10, 0.1),\n                 'LogReg__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                 'LogReg__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n                 'LogReg__random_state': [seed]}\n\n# GrideSearcCV\nlogreg_grid = GridSearchCV(estimator=logreg_pipe, param_grid=logreg_params,\n                           scoring='accuracy', cv=5)\nlogreg_grid.fit(training_features, training_label)\nlogreg_opt_param = logreg_grid.best_params_\nlogreg_best_score = (logreg_grid.best_score_*100).round(2)\nlogreg_best_est = logreg_grid.best_estimator_\n\n# Score on holdout data\nlogreg_holdout_score = (logreg_grid.score(test_features, test_label)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(logreg_opt_param)\nprint('Optimal Estimator:')\nprint(logreg_best_est)\nprint('\\n')\nprint('Training Accuracy {}'.format(logreg_best_score))\nprint('Testing Accuracy {}'.format(logreg_holdout_score))","7d61e253":"# Creating hyperparameter options\ndectree_params = {'max_depth': np.arange(0, 20, 1),\n                  'criterion': ['gini', 'entropy'],\n                  'min_samples_leaf': np.arange(0, 1, 0.05),\n                  'random_state': [seed]}\n\n# GrideSearcCV\ndectree_grid = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=dectree_params,\n                            scoring='accuracy', cv=5)\ndectree_grid.fit(training_features, training_label)\ndectree_opt_param = dectree_grid.best_params_\ndectree_best_score = (dectree_grid.best_score_*100).round(2)\ndectree_best_est = dectree_grid.best_estimator_\ndectree_feat_imp = dectree_best_est.feature_importances_\n\n# Score on holdout data\ndectree_holdout_score = (dectree_grid.score(test_features, test_label)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(dectree_opt_param)\nprint('Optimal Estimator:')\nprint(dectree_best_est)\nprint('\\n')\nprint('Training Accuracy {}'.format(dectree_best_score))\nprint('Testing Accuracy {}'.format(dectree_holdout_score))","a5be3729":"sns.set(rc={'figure.figsize':(20,10)})\n\n# Plotting the optimal tree\nplt.subplot(1, 2, 1)\nplot_tree(dectree_best_est,\n          feature_names=feature_names,  \n          class_names=['Non-Hazardous [0]', 'Hazardous [1]'],\n          filled=True)\n\n# Plotting feature importnace\nplt.subplot(1, 2, 2)\nplt.barh(feature_names, dectree_feat_imp)\n\nplt.show()\nplt.close","002aa35c":"# Creating the pipeline\nsvm_pipe = Pipeline([('Scaling', StandardScaler()),\n                     ('SVM', SVC())])\n\n# Creating hyperparameter options\nsvm_params = {'SVM__C': np.arange(0, 20, 0.1),\n              'SVM__gamma': [0.001, 0.01, 0.1, 1, 2, 5],        \n              'SVM__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n              'SVM__random_state': [seed]}\n\n# GrideSearcCV\nsvm_grid = GridSearchCV(estimator=svm_pipe, param_grid=svm_params,\n                        scoring='accuracy', cv=5)\nsvm_grid.fit(training_features, training_label)\nsvm_opt_param = svm_grid.best_params_\nsvm_best_score = (svm_grid.best_score_*100).round(2)\nsvm_best_est = svm_grid.best_estimator_\n\n# Score on holdout data\nsvm_holdout_score = (svm_grid.score(test_features, test_label)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(svm_opt_param)\nprint('Optimal Estimator:')\nprint(svm_best_est)\nprint('\\n')\nprint('Training Accuracy {}'.format(svm_best_score))\nprint('Testing Accuracy {}'.format(svm_holdout_score))","358b31ae":"# Creating hyperparameter options\nrf_params = {'max_depth': np.arange(0, 20, 1),\n             'criterion': ['gini', 'entropy'],\n             'min_samples_leaf': np.arange(0, 1, 0.05),\n             'random_state': [seed],\n             'n_estimators': np.arange(0, 10, 1)}\n\n# GrideSearcCV\nrf_grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=rf_params,\n                       scoring='accuracy', cv=5)\nrf_grid.fit(training_features, training_label)\nrf_opt_param = rf_grid.best_params_\nrf_best_score = (rf_grid.best_score_*100).round(2)\nrf_best_est = rf_grid.best_estimator_\nrf_feat_imp = rf_best_est.feature_importances_\n\n# Score on holdout data\nrf_holdout_score = (rf_grid.score(test_features, test_label)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(rf_opt_param)\nprint('Optimal Estimator:')\nprint(rf_best_est)\nprint('\\n')\nprint('Training Accuracy {}'.format(rf_best_score))\nprint('Testing Accuracy {}'.format(rf_holdout_score))","b9364820":"# Plotting feature importnace\nplt.barh(feature_names, rf_feat_imp)\nplt.show()\nplt.close","19130914":"# Creating hyperparameter options\nxgb_params = {'max_depth': np.arange(0, 5, 1),\n              'objective': ['binary:logistic'],\n              'random_state': [seed],\n              'alpha': [0, 0.01, 0.1, 1],\n              'lambda': [0, 0.01, 0.1, 1],\n              'subsample': [0.25, 0.5, 0.75],\n              'colsample_bytree': [0.25, 0.5, 0.75],\n              'eval_metric': ['logloss']}\n\n# GrideSearcCV\nxgb_grid = GridSearchCV(estimator=XGBClassifier(), param_grid=xgb_params,\n                            scoring='accuracy', cv=5)\nxgb_grid.fit(training_features, training_label)\nxgb_opt_param = xgb_grid.best_params_\nxgb_best_score = (xgb_grid.best_score_*100).round(2)\nxgb_best_est = xgb_grid.best_estimator_\nxgb_feat_imp = xgb_best_est.feature_importances_\n\n# Score on holdout data\nxgb_holdout_score = (xgb_grid.score(test_features, test_label)*100).round(2)\n\nprint('Optimal Hyperparameters:')\nprint(xgb_opt_param)\nprint('Optimal Estimator:')\nprint(xgb_best_est)\nprint('\\n')\nprint('Training Accuracy {}'.format(xgb_best_score))\nprint('Testing Accuracy {}'.format(xgb_holdout_score))","3cd98a15":"# Plotting feature importance\nplt.barh(feature_names, xgb_feat_imp)","21c0f81b":"# Alternative plotting with XGBoost library built-in feature importance plot function\nplot_importance(xgb_best_est)","4d4ae352":"# Step 3: Imputing","6509a0d0":"# Libraries & Data Import","da9e480b":"# Logistic Regression","28ea918f":"# Step 4: Dimensionality Reduction","647147b1":"# Decision Tree","5b837b16":"# Step 5: Categorical Feature Encoding","11caa85b":"# Step 6: Train\/Test Split","82c510a5":"# XGBoosting","4653c41e":"# CONCLUSIONS","fc8ed0e1":"### Model Performance on Unseen Data\n Model | Accuracy (%)\n - | -\nLogistic Regression | 95.47\nDecision Tree | 99.44\nSVM | 94.99\nRandom Forest | 99.47\nXGBoosting | **99.49**\n\nAs we can see above, **XGBoosting** has provided the best performance on unseen data, and thus is the best model for this classifcation problem out of those tested. Nonetheless the difference amongst the three tree based models (Decision Tree \/ Random Forest \/ XGBoosting) is almost negligible. \n\nIn an scenario were the model was going to be deployed into production, it would be interesting to look into the relation between the different accuracy performances and resource consumption. Given the small performance difference, an argument can be made that the best model, out of the three tree based models, is the one that consumes less resources.\n\n### Feature Importance\nLooking into each tree based model we can see a pattern on feature importance of the best estimator found for each model.\n\nFound on 100% of the best models:\n- Minimum Orbit Intersection\n- Absolute Magnitude\n\nFound on 33% of the best models:\n- Orbit ID\n- Perihelion Distance\n- Est dia in KM(min). This feature is the one I selected to capture size. Given the perfect correlation between size features (same measurement, different units) any of them would yield the same result, as long as only one is selected as part of the training dataset.\n\n\n### Overfitting\n- All models present some degree of overfitting (train accuracy > test accuracy). Nonetheless the differences are relatively small, less than 1 percentage point (except for SVM). Given this I would not consider that the selected model presents a significant overfitting issue.\n","f8e4a097":"This step is not necessary for this specific dataset, as there are no missing values.","ea6ffe50":"# MODELS","0a9cea1d":"# INTRODUCTION","926c9236":"# PRE-PROCESSING","715a91d1":"# SVM\n","93750b84":"# Step 2: Check Missing Values","ca5f3d21":"### Data\nThe data is about Asteroids - NeoWs.\nNeoWs (Near Earth Object Web Service) is a RESTful web service for near earth Asteroid information. With NeoWs a user can: search for Asteroids based on their closest approach date to Earth, lookup a specific Asteroid with its NASA JPL small body id, as well as browse the overall data-set.\n\nAcknowledgements\nData-set: All the data is from the (http:\/\/neo.jpl.nasa.gov\/). This API is maintained by SpaceRocks Team: David Greenfield, Arezu Sarvestani, Jason English and Peter Baunach.\n \n### Tasks\nBased on the information within the dataset we want to performs two tasks:\n- Develop a **model that predicts if an asteroid is going to be hazardous or not**\n- Identify which **features are more relevant towards the classfication** on point 1\n\nTo tackle this two tasks I'll use the methods listed below, and extract the results from the method with best performance on unseen data\n- Logistic Regression\n- Decision Tree\n- Random Forest\n- SVM\n- XGBossting\n ","0e6cb548":"# Step 1: Data Inspection","2aabd304":"# Random Forest"}}