{"cell_type":{"b119f510":"code","585313de":"code","0c3e20cd":"code","5117b4af":"code","5b94a344":"code","20babe80":"code","b0ad37e9":"code","9ae149e0":"code","1a94a472":"code","20909229":"code","83e1e8ad":"code","dc9ff39c":"code","3cf71bd3":"code","a100731c":"code","778bf333":"code","f9df5edb":"code","5fa48a75":"code","5dd22c2d":"code","0879674c":"code","bd1daca5":"code","3e6ff8c4":"code","033a213a":"code","c53f8c38":"code","03cbe3f8":"code","cc19b85c":"code","d003d3dc":"code","fed26661":"code","547b1328":"code","8fc86ac1":"code","d89524de":"code","0d7311c7":"code","f3b6081e":"code","a68ab764":"code","cac6c88a":"code","473d0162":"code","2f9f05e2":"code","2e0a08a4":"code","9755e37f":"code","d66e4366":"code","2538611c":"code","6dfda2fe":"code","0f6ad523":"code","7431e09f":"code","ae90b943":"code","444a81fd":"markdown","3684418d":"markdown","9c780ade":"markdown","1ea6fe76":"markdown","ba2f4e59":"markdown","b18f00bc":"markdown","99b112c6":"markdown","aaddcbec":"markdown","eccf3166":"markdown","7f0fdb0b":"markdown","6b926bfd":"markdown","a3c8b8f4":"markdown","b865402d":"markdown","b3df21c6":"markdown","a7ba5794":"markdown","9305fd5f":"markdown","8368c1c1":"markdown"},"source":{"b119f510":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","585313de":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')","0c3e20cd":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5117b4af":"train_df.head()","5b94a344":"test_df.head()","20babe80":"train_df.info()","b0ad37e9":"train_df = train_df.drop('Cabin', axis=1)","9ae149e0":"train_df.head()","1a94a472":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass', y='Age', data=train_df, palette='rainbow')","20909229":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 28\n        else:\n            return 24\n    else:\n        return Age","83e1e8ad":"train_df['Age'] = train_df[['Age', 'Pclass']].apply(impute_age, axis=1)","dc9ff39c":"train_df.dropna(inplace=True)","3cf71bd3":"train_df.head()","a100731c":"sex = pd.get_dummies(train_df['Sex'], drop_first=True)\nembarked = pd.get_dummies(train_df['Embarked'], drop_first=True)","778bf333":"train_df.drop(train_df[['Name', 'Sex', 'Ticket', 'Embarked']], axis=1, inplace=True)","f9df5edb":"train_df.head()","5fa48a75":"train_df = pd.concat([train_df, sex, embarked], axis=1)","5dd22c2d":"train_df.info()","0879674c":"test_df.head()","bd1daca5":"test_df.info()","3e6ff8c4":"test_df['Age'] = test_df[['Age', 'Pclass']].apply(impute_age, axis=1)","033a213a":"test_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)","c53f8c38":"test_df.head()","03cbe3f8":"sex_test = pd.get_dummies(test_df['Sex'], drop_first=True)\nembarked_test = pd.get_dummies(test_df['Embarked'], drop_first=True)","cc19b85c":"test_df.drop(['Name', 'Sex', 'Embarked', 'Ticket', 'Cabin'], axis=1, inplace=True)","d003d3dc":"test_df = pd.concat([test_df, sex_test, embarked_test],  axis=1)","fed26661":"test_df.head()","547b1328":"train_df.head()","8fc86ac1":"X_train = train_df.drop(['Survived', 'PassengerId'], axis=1)\ny_train = train_df['Survived']\nX_test = test_df.drop('PassengerId', axis=1)","d89524de":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\nlog_pred = log_reg.predict(X_test)\nlog_score = round(log_reg.score(X_train, y_train) *100, 2)","0d7311c7":"log_score","f3b6081e":"from sklearn.tree import DecisionTreeClassifier\nd_tree = DecisionTreeClassifier()\nd_tree.fit(X_train, y_train)\ndt_pred = d_tree.predict(X_test)\ndt_score = round(d_tree.score(X_train, y_train) * 100, 2)","a68ab764":"dt_score","cac6c88a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nknn_score = round(knn.score(X_train, y_train) * 100, 2)","473d0162":"knn_score","2f9f05e2":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nrf_score = round(rf.score(X_train, y_train) * 100,2)","2e0a08a4":"rf_score","9755e37f":"from sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train, y_train)\nsvm_predict = svm.predict(X_test)\nsvm_score = round(svm.score(X_train, y_train)*100, 2)","d66e4366":"svm_score","2538611c":"from sklearn.naive_bayes import GaussianNB\nguassian = GaussianNB()\nguassian.fit(X_train, y_train)\ngua_pred = guassian.predict(X_test)\ngua_score = round(guassian.score(X_train, y_train)*100, 2)","6dfda2fe":"gua_score","0f6ad523":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Decision Tree'],\n    'Score': [svm_score, knn_score, log_score, \n              rf_score, gua_score, dt_score]})\nmodels.sort_values(by='Score', ascending=False)","7431e09f":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": dt_pred\n    })","ae90b943":"submission.to_csv('submission.csv', index=False)","444a81fd":"# <h1>Predictions : Finding the best model<\/h1>","3684418d":"## 1) Logistic Regression","9c780ade":"## We will now check all the scores by comparing against each other.","1ea6fe76":"> We can see that the Decision Tree Classifier gives the best Accuracy Score of 98.09.","ba2f4e59":"# <h1>Test Dataset Cleaning<\/h>","b18f00bc":"> we will create a train test split of the Train_df dataset in order to find the best Classification Model.\n> we will then select the best model based on our findings and then apply the same on Test Dataset","99b112c6":"## 6) Naive Bayes","aaddcbec":"## 2) Decision Trees","eccf3166":"We can see from the plot above that the average age of any given person depends on the class of cabin they are in. So we can infer as below.\n* >Class 1 = 37\n* >Class 2 = 28\n* >Class 3 = 24","7f0fdb0b":"## 3) KNN","6b926bfd":"We will now submit this data to Kaggle","a3c8b8f4":"> we will do the same cleaning as we did in train dataset","b865402d":"There are some missing entries in the 'Age', 'Cabin', 'Embarked', column. \n>1. We will remove the 'Cabin' column completely.\n>2. We will use the dropna method to delete the row in the 'Embarked' column which will not impact the Data Frame as only 2 entries are null,\n>3. For the 'Embarked' column we will create a function to impute the age based on the mean age with the 'Pclass' column.","b3df21c6":">we will remove all the non numerical columns and then use pd.get_dummies to get the dummy values of the Pclass & Sex column","a7ba5794":"## 4) Random Forest","9305fd5f":"## 5) SVM","8368c1c1":"![](http:\/\/)> <h1>**Data Cleaning: Train Dataset**<\/h1>"}}