{"cell_type":{"4616d216":"code","352f51fe":"code","9593e562":"code","deed6147":"code","e168a0de":"code","0fb32ce0":"code","6500942c":"code","b4d9a41a":"code","3926d21f":"code","d209671a":"code","6818450e":"code","c085cbd1":"code","a64fa613":"code","9605c8cb":"code","74d40dc8":"code","3e4eed23":"code","394b6c2f":"markdown","ca81f40a":"markdown","ddcb4d3c":"markdown","f0bc53c4":"markdown","bcfb892a":"markdown","78ea4f0e":"markdown","3202f32e":"markdown","0d0f673d":"markdown","5447225e":"markdown","30af797b":"markdown","6cc3e30c":"markdown","25dbf427":"markdown","c2bc5dae":"markdown","5dc6b36b":"markdown"},"source":{"4616d216":"def get_one_file_data(file_path_list, stop_words_list, data_type_list, only_words=True):\n    '''\n    \u8bfb\u53d6json\u6587\u4ef6\uff0c\u83b7\u53d6\u6587\u672c\u6570\u636e\n    :param file_path_list:\n    :param stop_words_list:\n    :param data_type_list: list\u7ed3\u6784 title,authors\u3001 abstracts\u3001body\u56db\u79cd\n    :return:\n    '''\n    re_data = []\n\n    not_words = '[^\\u4e00-\\u9fa5^a-z^A-Z^0-9^ ]'\n    cop = re.compile(not_words)  # \u5339\u914d\u4e0d\u662f\u4e2d\u6587\u3001\u5927\u5c0f\u5199\u3001\u6570\u5b57\u7684\u5176\u4ed6\u5b57\u7b26\n    for file_path in file_path_list:\n        re_data_tmp = []\n        with open(file_path, 'r') as f:\n            json_data = json.load(f)\n            # \u7531\u4e8e\u540e\u7eed\u9700\u8981\u6839\u636epaper_id\u8bfb\u53d6\u5bf9\u5e94\u7684json\u6570\u636e\uff0c\u8fd9\u91cc\u7684paper_id\u52a0\u4e0a\u4e86\u5bf9\u5e94\u8def\u5f84\u7684\u524d\u7f00\n            paper_id = file_path.split(covid_util.basic_data_path)[1].replace(os.sep, \"-\")[1:-5]\n            # paper_id = paper_id_pre + \"_\" + json_data['paper_id']\n            re_data_tmp.append(paper_id)\n\n            for data_type in data_type_list:\n                if data_type == 'title':\n                    title = json_data['metadata']['title'].replace(\"\\\"\", \"\")\n                    if only_words:\n                        title = cop.sub('', title)\n                    re_data_tmp.append(title)\n\n                if data_type == 'authors':\n                    authors = \"\"\n                    for author in json_data['metadata']['authors']:\n                        authors += author['first'] + \"\".join(author['middle']) + author['last'] + \" \"\n                    authors = authors.replace(\"\\\"\", \"\")\n                    if only_words:\n                        authors = cop.sub('', authors)\n\n                    re_data_tmp.append(authors)\n\n                if data_type == 'abstracts':\n                    abstracts = \"\"\n                    for abstract in json_data['abstract'].replace(\"\\\"\", \"\"):\n                        abstract_text = \"\"\n                        if only_words:\n                            for word in abstract['text'].split(\" \"):\n                                if word not in stop_words_list:\n                                    abstract_text += word + \" \"\n                            abstracts += abstract_text\n                            abstracts = cop.sub('', abstracts)\n                        else:\n                            abstracts += encode_sentence( abstract['text'])\n                    re_data_tmp.append(abstracts)\n\n                if data_type == 'body':\n                    body_texts = \"\"\n                    for body_text in json_data['body_text']:\n                        body_text_tmp = \"\"\n                        if only_words:\n                            for word in body_text['text'].split(\" \"):\n                                if word not in stop_words_list:\n                                    body_text_tmp += word + \" \"\n                            body_texts += body_text_tmp\n                            body_texts = cop.sub('', body_texts)\n                        else:\n                            body_texts += encode_sentence(body_text['text'])\n\n                    body_texts = body_texts.replace(\"\\\"\", \"\")\n                    re_data_tmp.append(body_texts)\n\n            re_data.append(re_data_tmp)\n\n    return re_data","352f51fe":"def get_multi_file_data(all_file_list, data_type_list, limit=-1, only_words=True):\n    '''\n    \u5e76\u884c\u8bfb\u53d6json\u6587\u4ef6\n    limit\u8868\u793a\u4e00\u5171\u5ea6\u591a\u5c11\u4e2a\uff0c-1\u8868\u793a\u8bfb\u53d6\u5168\u90e8\n    all_file_list\u4f1a\u6839\u636egroup_size \u786e\u5b9a\u4e00\u5171\u4f7f\u7528\u51e0\u4e2aCPU\u6838\u8dd1\uff0c\u4e00\u4e2aCPU\u6838\u8d1f\u8d23\u4e00\u4e2agroup_size\u6570\u636e\u7684\u8bfb\u53d6\n    :param all_file_list:\n    :param data_type_list:\u8bfb\u53d6json\u6587\u4ef6\u7684\u7c7b\u578b\n    :param limit:\n    :return:\n    '''\n    read_file_list = all_file_list\n    stop_words_list = load_stop_words(covid_util.stop_words_path)\n    group_size = int(len(read_file_list) \/ covid_util.thread_num)\n    if limit > 0:\n        read_file_list = read_file_list[:limit]\n        group_size = int(limit \/ covid_util.thread_num)\n    if group_size == 0:\n        group_size = 1\n    file_list = []\n    for i in range(0, len(read_file_list), group_size):\n        file_list.append(read_file_list[i:i + group_size])\n\n    # \u4f7f\u7528\u8ba1\u7b97\u673a\u5168\u90e8\u7684cpu\u6838\u6570\u591a\u8fdb\u7a0b\u5e76\u884c\n    executor = Parallel(n_jobs=-1, backend='multiprocessing')\n    tasks = (delayed(get_one_file_data)(small_file_list, stop_words_list, data_type_list, only_words) for\n             small_file_list in\n             file_list)\n    tasks_data_list = executor(tasks)\n    re_data = []\n    for data_list in tasks_data_list:\n        for data in data_list:\n            re_data.append(data)\n    columns = ['paper_id']\n    for data_type in data_type_list:\n        columns.append(data_type)\n    df = pd.DataFrame(data=re_data, columns=columns)\n    # df[['paper_id']].to_csv(covid_util.get_basic_data_index_save_path, index=False)\n    df = df.set_index(['paper_id'], drop=True)\n    print(df.shape)\n    data_path = os.path.join(covid_util.project_data_path, '_'.join(data_type_list) + \".csv\")\n    df.to_csv(data_path, index=True, encoding='utf8')","9593e562":"def train_word2vector(texts, window_size, min_count, embedding_size, model_path):\n    '''\n    \u8bad\u7ec3Word2vector\n    :param texts:\n    :param window_size:\n    :param min_count:\n    :param model_path:\n    :return:\n    '''\n    texts = [line.split(\" \") for line in texts]\n    model = word2vec.Word2Vec(sentences=texts, size=embedding_size, window=window_size, min_count=min_count)\n    model.wv.save_word2vec_format(model_path, binary=True)\n\n\ndef convert_text_vector(texts, text_nums_words, num_words, w2v_model_path, tokenizer_save_path, embedding_size,\n                        embedding_matrix_save_path, text_vector_path):\n    '''\n    \u51fd\u6570\u6709\u4e24\u4e2a\u529f\u80fd\uff1a\n        1.\u83b7\u53d6\u8bcd\u7684Embedding\u77e9\u9635\n        2.\u5c06\u6587\u672c\u8f6c\u6210\u8bcd\u5411\u91cf\uff0c\u7ed3\u679c\u662f\u4e00\u4e2a\u4e09\u7ef4\u6570\u7ec4[\u6587\u672c\u7684\u4e2a\u6570,\u6bcf\u4e2a\u6587\u672c\u7684\u8bcd\u4e2a\u6570,\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u957f\u5ea6]\n    :param texts:\u539f\u59cb\u5206\u597d\u8bcd\u7684\u6587\u672c\uff0c\u957f\u77ed\u4e0d\u4e00\uff0c\u540e\u7eed\u9700\u8981\u8fdb\u884c\u89c4\u6574\u5316\n    :param text_nums_words:\u6bcf\u6bb5\u6587\u672c\u6700\u540e\u4fdd\u7559\u7684word\u4e2a\u6570\n    :param num_words:\u603b\u7684\u8bcd\u5e93\u5927\u5c0f\n    :param w2v_model_path:\u4e0a\u4e00\u6b65\u8bad\u7ec3\u597d\u7684w2v\u6a21\u578b\u7684\u5b58\u50a8\u5730\u5740\n    :param tokenizer_save_path:\u8bcd\u8ba1\u6570\u5668\u7684\u5b58\u50a8\u5730\u5740\n    :param embedding_size:\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cfsize\n    :param embedding_matrix_save_path:Embedding\u77e9\u9635\u7684\u5b58\u50a8\u5730\u5740\n    :param text_vector_path:\u6587\u672c\u5411\u91cf\u5316\u4e4b\u540e\u7684\u5b58\u50a8\u5730\u5740\n    :return:\n    '''\n    model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=True)\n\n    tokenizer = Tokenizer(\n        num_words=num_words,\n        split=' ',\n        lower=False,\n        char_level=False,\n        filters=''\n    )\n    # \u5c06\u6bcf\u4e00\u884c\u6587\u672c\u4e2d\u7684\u8bcd\u62c6\u51fa\u6765\u5f62\u6210\u5355\u4e2a\u7684\u8bcd\uff0c\u7528\u4e8e\u540e\u9762\u7684Embedding\n    words_matrix = [text.strip().split(\" \") for text in texts]\n    tokenizer.fit_on_texts(words_matrix)\n    covid_util.save(tokenizer, tokenizer_save_path)\n\n    # \u6bcf\u4e2a\u5355\u8bcd\u7684\u7d22\u5f15\n    word_index_dict = tokenizer.word_index\n\n    # \u83b7\u53d6\u6bcf\u4e2a\u8bcd\u7684\u5411\u91cf\u8868\u793a\uff0c\u8fd9\u91cc\u91cd\u65b0\u6784\u5efa\u4e00\u4e2aEmbedding\u77e9\u9635\u7684\u4f5c\u7528\u662f\u540e\u7eed\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65f6\u53ef\u4ee5\u5c06\u5176\u4f5c\u4e3aEmbedding\u7684\u521d\u59cb\u6743\u91cd\n    # \u5982\u679c\u7b80\u5355\u7684\u4f7f\u7528word2Vec\u5c06\u8bcd\u5411\u91cf\u5316\uff0c\u5219\u8fd9\u4e2a\u90e8\u5206\u6ca1\u6709\u5b9e\u9645\u7684\u7528\u5904\n    embedding_matrix = np.zeros((len(word_index_dict) + 1, embedding_size), dtype=np.float)\n    embedding_matrix[0] = np.zeros(embedding_size, dtype=np.float)\n    texts_seq = tokenizer.texts_to_sequences(texts)\n    texts_seq = sequence.pad_sequences(texts_seq, maxlen=text_nums_words)\n\n    # \u5c06\u6bcf\u884c\u6587\u672c\u8f6c\u6210\u5bf9\u5e94\u7684\u5411\u91cf\uff0c\u662f\u4e00\u4e2a\u4e09\u7ef4\u7684\u6570\u7ec4\n    text_vector = np.zeros((len(texts_seq), text_nums_words, embedding_size), dtype=np.float)\n\n    for word, index in word_index_dict.items():\n        if word in model.vocab:\n            embedding_matrix[index] = model.word_vec(word)\n    text_index = 0\n    for word_index_list in texts_seq:\n        text_arr = np.zeros((text_nums_words, embedding_size), dtype=np.float)\n        word_count = 0\n        for word_index in word_index_list:\n            text_arr[word_count] = embedding_matrix[word_index]\n            word_count += 1\n        text_vector[text_index] = text_arr\n        text_index += 1\n    covid_util.save(embedding_matrix, embedding_matrix_save_path)\n    covid_util.save(text_vector, text_vector_path)\n\n\ndef run(texts, text_nums_words, window_size, min_count, embedding_size, w2v_model_path, num_words,\n        tokenizer_save_path, embedding_matrix_save_path, text_vector_path):\n    # \u8bad\u7ec3Word2Vec\n    train_word2vector(texts, window_size, min_count, embedding_size, w2v_model_path)\n\n    # \u5c06\u6587\u672c\u8f6c\u6210\u5411\u91cf\n    convert_text_vector(texts, text_nums_words, num_words, w2v_model_path, tokenizer_save_path, embedding_size,\n                        embedding_matrix_save_path, text_vector_path)\n","deed6147":"def autoencoder(dims, activation='relu'):\n    '''\n    \u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0encoder\u548cdecoder\n    :param dims: \u9700\u8981\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u6bcf\u4e00\u5c42\u7684\u7ef4\u5ea6\uff0c\u5176\u4e2ddims[0]\u662f\u539f\u59cb\u6570\u636e\u7684\u7ef4\u5ea6\uff0cdims[-1]\u8868\u793a\u7684\u662f\u6700\u540e\u9700\u8981\u964d\u7ef4\u7684\u7ef4\u5ea6\n    :param activation:\u6fc0\u6d3b\u51fd\u6570\n    :return:encoder model\u548cdecoder model\n    '''\n\n    # \u603b\u7684\u5c42\u6570\u5305\u62ec\u8f93\u5165\u5c42\u548c\u8f93\u51fa\u5c42\uff0c\u6240\u4ee5\u4e2d\u95f4\u7684\u5168\u8fde\u63a5\u5c42\u6570\u4e3a\u603b\u6570-2\n    n_layers = len(dims)\n\n    # \u5b9a\u4e49\u8f93\u5165\u5c42\n    input_data = Input(shape=(dims[0],), name='input')\n    x = input_data\n\n    # \u5b9a\u4e49encoder\u7684\u4e2d\u95f4\u5404\u4e2a\u5168\u8fde\u63a5\u5c42\n    for i in range(n_layers - 2):\n        x = Dense(dims[i + 1], activation=activation, name=\"encoder_{}\".format(i))(x)\n\n    # \u5b9a\u4e49\u8f93\u51fa\u5c42\n    encoder = Dense(dims[-1], name=\"encoder_out\")(x)\n\n    # \u5b9a\u4e49decoder\u7684\u4e2d\u95f4\u5404\u4e2a\u5168\u8fde\u63a5\u5c42,\u8ddfencoder\u7684\u5404\u5c42\u987a\u5e8f\u53cd\u8fc7\u6765\n    x = encoder\n    for i in range(n_layers - 2, 1, -1):\n        x = Dense(dims[i], activation=activation, name=\"decoder_{}\".format(i))(x)\n    decoder = Dense(dims[0], name=\"decoder_out\")(x)\n\n    return Model(inputs=input_data, outputs=decoder, name=\"encoder_decoder_model\")","e168a0de":"def run(limit=-1):\n    # \u4ece\u539f\u59cb\u6570\u636e\u96c6\u8bfb\u53d6\u6570\u636e\u5e76\u5b58\u50a8\u78c1\u76d8\n    data_type_list = ['title', 'authors', 'abstracts']\n    get_basic_data.run(data_type_list, limit)\n    data_path = os.path.join(covid_util.project_data_path, '_'.join(data_type_list) + \".csv\")\n    basic_data = pd.read_csv(data_path, encoding='utf8')\n    for column in basic_data.columns.values:\n        basic_data[column] = basic_data[column].astype(str)\n\n    # author\u5411\u91cf\u5316\n    author_texts = basic_data['authors'].tolist()\n    convert_word_vector.run(author_texts, covid_util.author_text_num_words,\n                            covid_util.author_window_size, covid_util.author_min_count,\n                            covid_util.author_embedding_size,\n                            covid_util.author_w2v_model_path, covid_util.author_total_num_words\n                            , covid_util.author_tokenizer_path, covid_util.author_embedding_vector_path,\n                            covid_util.author_text_vector_path)\n    # abstract\u5411\u91cf\u5316\n    abstract_texts = basic_data['abstracts'].tolist()\n    convert_word_vector.run(abstract_texts, covid_util.abstract_text_num_words,\n                            covid_util.abstract_window_size, covid_util.abstract_min_count,\n                            covid_util.abstract_embedding_size,\n                            covid_util.abstract_w2v_model_path, covid_util.abstract_total_num_words\n                            , covid_util.abstract_tokenizer_path, covid_util.abstract_embedding_vector_path,\n                            covid_util.abstract_text_vector_path)\n\n\ndef encoder_vecotr():\n    author_x = covid_util.load(covid_util.author_text_vector_path)\n    samples, n, m = author_x.shape\n    author_x_new = author_x.reshape(samples, n * m)\n\n    abstract_x = covid_util.load(covid_util.abstract_text_vector_path)\n    samples, n, m = abstract_x.shape\n    abstract_x_new = abstract_x.reshape(samples, n * m)\n    X = np.hstack((author_x_new, abstract_x_new))\n    print(X.shape)\n\n    dims = [X.shape[-1], 500, 500, 2000, 100]\n    encoder_decoder_model = autoencoder(dims)\n    plot_model(encoder_decoder_model, to_file=os.path.join(covid_util.images_data_path, 'encoder_decoder_model.png'),\n               show_shapes=True)\n\n    batch_size = 256\n    epochs = 50\n\n    encoder_decoder_model.compile(optimizer='adam', loss='mse')\n    encoder_decoder_model.fit(X, X, batch_size=batch_size, epochs=epochs)\n    encoder_decoder_model.save(os.path.join(covid_util.project_data_path, 'encoder_model.h5'))\n\n    # encoder\uff0cdecoer\u6574\u4e2a\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\u53d6\u51fa\u5176\u4e2d\u7684encoder\u5c42\u8fdb\u884c\u539f\u59cb\u6570\u636e\u7684\u7f16\u7801\n    encoder_model = Model(inputs=encoder_decoder_model.input,\n                          outputs=encoder_decoder_model.get_layer(\"encoder_out\").output)\n    encoder_x = encoder_model.predict(X)\n    print(encoder_x)\n    covid_util.save(encoder_x, os.path.join(covid_util.project_data_path, 'encoder_x_100.bigram'))\n\n\ndef train_best_cluster():\n    encoder_X = covid_util.load(os.path.join(covid_util.project_data_path, 'encoder_x_100.bigram'))\n    for cluster_num in range(4, 10, 2):\n        kmeans = MiniBatchKMeans(n_clusters=cluster_num, random_state=0, batch_size=1000).fit(encoder_X)\n        cluster_center = kmeans.cluster_centers_\n        cluster_labels = kmeans.labels_\n        with open(os.path.join(covid_util.log_data_path, 'MiniBatchKMeans_re.log'), 'a') as f:\n            y_pred = kmeans.predict(encoder_X)\n            index_score = metrics.calinski_harabaz_score(encoder_X, y_pred)\n            DBI, DI = covid_util.cluster_DBI_DI(cluster_center, encoder_X, cluster_labels, cluster_num)\n            s = \"cluster_num:{},DBI:{},DI:{},index_score:{}\\n\".format(cluster_num, DBI, DI, index_score)\n            print(s)\n            f.write(s)\n    # \u6700\u4f73\u7ed3\u679c\u662f4\u7c7b\n\n\ndef run_all():\n    run(-1)\n    encoder_vecotr()\n    cluster_num = 4\n    autoencoder_x = covid_util.load(os.path.join(covid_util.project_data_path, 'encoder_x_100.bigram'))\n    kmeans = MiniBatchKMeans(n_clusters=cluster_num, random_state=0, batch_size=1000).fit(autoencoder_x)\n    predict = kmeans.predict(autoencoder_x)\n    paper_id = np.array(pd.read_csv(covid_util.get_basic_data_index_save_path).values)\n    predict = predict.reshape(paper_id.shape[0], 1)\n    y_predict = pd.DataFrame(np.hstack((paper_id, predict)), columns=['paper_id', 'class_label'])\n    y_predict.to_csv(os.path.join(covid_util.project_data_path, 'kmeans_predict.csv'), index=False)","0fb32ce0":"def get_content_key_textrank(paper_id_list, sentence_save_path, words_save_path, key_sentence_num=10):\n    data_type_list = ['body']\n    paper_id_path = []\n    for paper_id in paper_id_list:\n        file_path = os.sep.join(paper_id.split(\"-\"))\n        paper_id_path.append(os.path.join(covid_util.basic_data_path, file_path + \".json\"))\n    get_basic_data.get_multi_file_data(paper_id_path, data_type_list, -1, only_words=False)\n    data_path = os.path.join(covid_util.project_data_path, '_'.join(data_type_list) + \".csv\")\n    basic_data = pd.read_csv(data_path, encoding='utf8', index_col=0)\n    for column in basic_data.columns.values:\n        basic_data[column] = basic_data[column].astype(str)\n    data_sentence = []\n    data_sentence_weights = []\n    data_words = []\n    data_words_weights = []\n    sentence_delimiters = ['?', '!', ';', '\uff1f', '\uff01', '\u3002', '\uff1b', '\u2026\u2026', '\u2026', '\\n', '.', '...']\n\n    for index, row in basic_data.iterrows():\n        tmp_data_sentence = []\n        tmp_data_sentence_weights = []\n        tr4s = TextRank4Sentence(stop_words_file=covid_util.stop_words_no_path, delimiters=sentence_delimiters)\n        try:\n            tr4s.analyze(text=row['body'], lower=True, source='no_filter')\n        except Exception as e:\n            with open(os.path.join(covid_util.log_data_path, 'textrank_error.log'), 'a') as f:\n                f.write(\"sentence-\" + str(index) + \"\\n\")\n        for item in tr4s.get_key_sentences(key_sentence_num):\n            sentence = decode_sentence(item['sentence'])\n            tmp_data_sentence.append(sentence)\n            tmp_data_sentence_weights.append(item['weight'])\n\n        tmp_data_words = []\n        tmp_data_words_weights = []\n        tr4w = TextRank4Keyword(stop_words_file=covid_util.stop_words_path)\n        try:\n            tr4w.analyze(text=row['body'], lower=True)\n        except Exception as e:\n            with open(os.path.join(covid_util.log_data_path, 'textrank_error.log'), 'a') as f:\n                f.write(\"words-\" + str(index) + \"\\n\")\n        for item in tr4w.get_keywords(key_sentence_num, word_min_len=1):\n            tmp_data_words.append(item['word'])\n            tmp_data_words_weights.append(item['weight'])\n\n        data_sentence.append(tmp_data_sentence)\n        data_sentence_weights.append(tmp_data_sentence_weights)\n        data_words.append(tmp_data_words)\n        data_words_weights.append(tmp_data_words_weights)\n\n    columns = ['top_{}'.format(i + 1) for i in range(key_sentence_num)]\n    key_sentence_df = pd.DataFrame(data=data_sentence, columns=columns)\n    key_sentence_weights_df = pd.DataFrame(data=data_sentence_weights, columns=columns)\n    key_words_df = pd.DataFrame(data=data_words, columns=columns)\n    key_words_weights_df = pd.DataFrame(data=data_words_weights, columns=columns)\n\n    key_sentence_df.index = basic_data.index\n    key_sentence_weights_df.index = basic_data.index\n    key_words_df.index = basic_data.index\n    key_words_weights_df.index = basic_data.index\n\n    key_sentence_df.to_csv(sentence_save_path, sep=\"$\")\n    key_sentence_weights_df.to_csv(sentence_save_path.replace(\"sentence\", \"sentence_weights\"), sep=\"$\")\n    key_words_df.to_csv(words_save_path, sep=\"$\")\n    key_words_weights_df.to_csv(words_save_path.replace(\"words\", \"words_weights\"), sep=\"$\")\n\n    del key_sentence_df\n    del key_sentence_weights_df\n    del key_words_df\n    del key_words_weights_df\n    gc.collect()\n    \ndef run(type='textrank', limit=-1):\n    texts_cluster = pd.read_csv(os.path.join(covid_util.project_data_path, 'kmeans_predict.csv'))\n    if limit > 0:\n        texts_cluster = texts_cluster.head(limit)\n    for i in range(covid_util.cluster_num):\n        paper_id_list = texts_cluster[texts_cluster['class_label'] == i]['paper_id'].values.tolist()\n        total_paper_num = len(paper_id_list)\n        index = 1\n        if total_paper_num > covid_util.content_max_paper_num:\n            for j in range(0, total_paper_num, covid_util.content_max_paper_num):\n                paper_id_list_tmp = paper_id_list[j:j + covid_util.content_max_paper_num]\n\n                sentence_save_path = os.path.join(covid_util.project_data_path,\n                                                  \"content_key_sentence_{}_{}_{}.csv\".format(type, i, index))\n                words_save_path = os.path.join(covid_util.project_data_path,\n                                               \"content_key_words_{}_{}_{}.csv\".format(type, i, index))\n                if type == 'textrank':\n                    get_content_key_textrank(paper_id_list_tmp, sentence_save_path, words_save_path,\n                                             key_sentence_num=covid_util.key_sentence_num)\n                if type == 'tfidf':\n                    get_content_key_tfidf(paper_id_list_tmp, words_save_path)\n                print(\"\u8fdb\u5ea6:,\u7b2c{}\u7c7b\uff0c{}\/{}\".format(i, index, int(total_paper_num \/ covid_util.content_max_paper_num) + 1))\n                index += 1\n        else:\n            sentence_save_path = os.path.join(covid_util.project_data_path,\n                                              \"content_key_sentence_{}_{}.csv\".format(type, i))\n            words_save_path = os.path.join(covid_util.project_data_path,\n                                           \"content_key_words_{}_{}.csv\".format(type, i))\n            if type == 'textrank':\n                get_content_key_textrank(paper_id_list, sentence_save_path, words_save_path,\n                                         key_sentence_num=covid_util.key_sentence_num)\n            if type == 'tfidf':\n                get_content_key_tfidf(paper_id_list, words_save_path)","6500942c":"import pandas as pd\nimport glob\nimport os\nfrom  wordcloud import WordCloud\nimport matplotlib.pyplot as plt","b4d9a41a":"def get_class_info(class_label):\n    columns=[]\n    for i in range(1,11):\n        columns.append(\"top_{}\".format(i))\n    sentence_df=pd.DataFrame(columns=columns)\n    words_df=pd.DataFrame(columns=columns)\n    project_data_path='.\/data\/project_data\/'\n    for r in glob.glob(project_data_path+'content_key_sentence_textrank_{}*.csv'.format(class_label)):\n        sentence_df=sentence_df.append(pd.read_csv(r,index_col=0,sep='$'))\n\n    for r in glob.glob(project_data_path+'content_key_words_textrank_{}*.csv'.format(class_label)):\n        words_df=words_df.append(pd.read_csv(r,index_col=0,sep='$'))\n    print(sentence_df.shape)\n    print(words_df.shape)\n    \n    sentence_df_count={}\n    for index,row in sentence_df.iterrows():\n        for col in columns:\n            if not pd.isnull(row[col]):\n                if row[col] in sentence_df_count:\n                    sentence_df_count[row[col]]+=1\n                else:\n                    sentence_df_count[row[col]]=1\n\n    words_text=[]\n    words_df=words_df.fillna('nan')\n    for col in columns:\n        words_text.extend(words_df[col])\n    wordcloud = WordCloud(background_color='white',\n                       width=4000,\n                       height=2000,\n                       margin=2,\n                       max_words=100, \n                       random_state=10)  \n    w = wordcloud.generate(\" \".join(words_text))  \n    plt.figure(figsize=(10,10))\n    plt.imshow(w)\n    plt.axis('off')  \n    plt.savefig('.\/data\/images\/wordcloud_{}.jpg'.format(class_label))\n    df=pd.DataFrame({'sentence':list(sentence_df_count.keys()),'count':list(sentence_df_count.values())})\n    df=df.sort_values(['count'],ascending=False)\n    return df","3926d21f":"sentence_df_count_0=get_class_info(0)","d209671a":"sentence_df_count_1=get_class_info(1)","6818450e":"sentence_df_count_2=get_class_info(2)","c085cbd1":"sentence_df_count_3=get_class_info(3)","a64fa613":"sentence_df_count_0.head(10)","9605c8cb":"sentence_df_count_1.head(10)","74d40dc8":"sentence_df_count_2.head(10)","3e4eed23":"sentence_df_count_3.head(10)","394b6c2f":"# \u6587\u672c\u805a\u7c7b\u4e0e\u5173\u952e\u8bcd\u3001\u5173\u952e\u53e5\u7684\u62bd\u53d6","ca81f40a":"## 4.AutoEncoder\u964d\u7ef4","ddcb4d3c":"## 5.KMeans\u805a\u7c7b","f0bc53c4":"## 6.TextRank\u83b7\u53d6\u5173\u952e\u8bcd\u548c\u5173\u952e\u53e5","bcfb892a":"\u8fd9\u91cc\u4e3b\u8981\u4f7f\u7528author\u548cabstract\u5411\u91cf\u8fdb\u884c\u805a\u7c7b\u64cd\u4f5c\uff0c\u6ca1\u6709\u9009\u62e9body_text\u7684\u539f\u56e0\u662f\u592a\u5927\uff0c\u5355\u673a\u7684\u5185\u5b58\u4e0d\u591f","78ea4f0e":"\u4f7f\u7528Parallel\u5e76\u884c\u8bfb\u53d6json\u6587\u4ef6\u5e76\u5b58\u50a8\u5230\u5bf9\u5e94\u7684csv\u6587\u4ef6\u4e2d:","3202f32e":"\u7ecf\u8fc7\u4e0a\u9762\u7684\u4e00\u4e9b\u5de5\u4f5c\uff0c\u867d\u7136\u5b8c\u6210\u4e86\u6574\u4e2aNLP\u7684\u8fc7\u7a0b\uff0c\u4f46\u662f\u7ed3\u679c\u5374\u4e0d\u662f\u5f88\u8ba9\u4eba\u6ee1\u610f\u3002\n\n\u5728\u8bcd\u4e91\u7684\u8868\u73b0\u4e0a\uff0c\u5404\u4e2a\u7c7b\u522b\u4e4b\u95f4\u7684\u533a\u5206\u4e0d\u662f\u5f88\u660e\u663e:\n\n\u7b2c1\u7c7b\u4e2d\u7684\u51e0\u4e2a\u6838\u5fc3\u8bcd\u662fcell\uff0cprotein\u3002\u8fd9\u4e2a\u7c7b\u522b\u7684\u8bba\u6587\u53ef\u80fd\u662f\u7814\u7a76\u75c5\u6bd2\u7684\u5185\u90e8\u6784\u9020\n\n\u7b2c2\u7c7b\u4e2d\u7684\u51e0\u4e2a\u6838\u5fc3\u8bcd\u662fprotein\uff0cinfection\u3002\u8fd9\u4e2a\u7c7b\u522b\u7684\u8bba\u6587\u53ef\u80fd\u662f\u7814\u7a76\u75c5\u6bd2\u7684\u4f20\u64ad\u8fc7\u7a0b\n\n\u7b2c3\u7c7b\u4e2d\u7684\u51e0\u4e2a\u6838\u5fc3\u8bcd\u662fstudy\uff0cvirus\u3002\u8fd9\u4e2a\u7c7b\u522b\u7684\u8bba\u6587\u53ef\u80fd\u662f\u7814\u7a76\u75c5\u6bd2\u7684\u53d8\u5f02\n\n\u7b2c4\u7c7b\u4e2d\u7684\u51e0\u4e2a\u6838\u5fc3\u8bcd\u662fexpression\uff0cinfection\u3002\u8fd9\u4e2a\u7c7b\u522b\u7684\u8bba\u6587\u53ef\u80fd\u662f\u7814\u7a76\u75c5\u6bd2\u7684\u75c7\u72b6\u4ee5\u53ca\u4f20\u64ad\u7279\u5f81\n\n\n\u5728\u5173\u952e\u53e5\u5b50\u7684\u8868\u73b0\u4e0a\u5374\u4e0d\u662f\u5f88\u597d\u7684\u62bd\u53d6\u5230\u5173\u952e\u4fe1\u606f\uff0c\u76f8\u540c\u7c7b\u522b\u4e2d\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u53e5\u5b50\u5f80\u5f80\u90fd\u662f\u5f88\u7248\u6743\u6216\u8005\u8bba\u6587\u53c2\u8003\u7f51\u7ad9\u7b49\u8fd9\u4e9b\u57fa\u7840\u4fe1\u606f\uff0c\u66f4\u52a0\u6709\u7528\u7684\u4fe1\u606f\u4e0d\u662f\u5f88\u7a81\u51fa\u3002\n","0d0f673d":"## 2.\u57fa\u672c\u6570\u636e\u7684\u62c6\u89e3","5447225e":"\u6838\u5fc3\u5b9e\u73b0\u65b9\u6cd5\u5728`convert_word_vector.py`\u6587\u4ef6\u4e2d\uff1a","30af797b":"## 1.\u9879\u76ee\u57fa\u7840\u4fe1\u606f\u8bf4\u660e\n\n\u672c\u6b21\u9879\u76ee\u6e90\u81ea[Kaggle\u7ade\u8d5b](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/)\u3002\u7531\u4e8e\u7f51\u7edc\u4e0e\u7ebf\u4e0a\u8d44\u6e90\u7684\u539f\u56e0\uff0c\u65e0\u6cd5\u9ad8\u6548\u7684\u5728Kaggle\u7684\u5b98\u65b9\u7f51\u7ad9\u4e0a\u5efa\u7acb\u81ea\u5df1\u7684NoteBook\uff0c\u800c\u4e14\u4e5f\u65e0\u6cd5\u4e0a\u4f20\u6570\u636e\u96c6\u3002\u6240\u4ee5\u91c7\u53d6\u7684\u65b9\u5f0f\u662f\u4e0b\u8f7d\u6570\u636e\u5230\u672c\u5730\uff0c\u5b8c\u6210\u4e4b\u540e\u4e0a\u4f20\u5230Github\u3002\u9879\u76ee\u5b8c\u6574\u4ee3\u7801\u7684GitHub\u5730\u5740\uff1ahttps:\/\/github.com\/yuguosen\/covid-2019\n\n\u9879\u76ee\u7684\u5173\u952e\u73af\u5883\u914d\u7f6e\u5982\u4e0b\uff1a\n\n|`Software`|`Version`|\n|---|---|\n|python|3.7.3|\n|Anaconda|4.8.2|\n|Keras|2.2.4|\n|scikit-learn|0.20.3|\n|numpy|1.16.2|\n|pandas|0.24.2|\n\n\u672c\u6b21\u9879\u76ee\u5b9e\u9a8c\u6ca1\u6709\u4f7f\u7528GPU\uff0c\u5185\u5b58\u4e5f\u53ea\u67098G\uff0c\u8fd9\u65b9\u9762\u786e\u5b9e\u9650\u5236\u4e86\u5f88\u591a\u7684\u64cd\u4f5c\uff0c\u4f8b\u5982\u6211\u672c\u6765\u60f3\u4f7f\u7528BERT\u6765\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u5c31\u50cf[\u8fd9\u4e2a](https:\/\/www.kaggle.com\/dirktheeng\/anserini-bert-squad-for-semantic-corpus-search\/comments)\uff0c\u4f46\u662f\u8fdb\u884c\u7b80\u5355\u7684Demo\u6d4b\u8bd5\u540e\u53d1\u73b0\u786c\u4ef6\u8d44\u6e90\u8ddf\u4e0d\u4e0a\n\u9042\u8f6c\u5316\u601d\u8def\u5229\u7528AutoEncoder\u8fdb\u884c\u964d\u7ef4\uff0c\u7136\u540e\u4f7f\u7528KMeans\u8fdb\u884c\u805a\u7c7b\u64cd\u4f5c\u3002\u4e0d\u8fc7\u8fd9\u6837\u7684\u597d\u5904\u5c31\u662f\u90a3\u4e9b\u8d44\u6e90\u4e0d\u662f\u5f88\u591f\u7684\u540c\u5b66\u90fd\u80fd\u591f\u4e0a\u624b\u8bd5\u4e00\u4e0b\uff0c\u54c8\u54c8\u54c8\u3002\n\n\n\n\n\u9879\u76ee\u7684\u6574\u4f53\u601d\u8def\u4e3b\u8981\u6709\u4ee5\u4e0b\u51e0\u4e2a\u6b65\u9aa4\uff1a\n\n1.\u62bd\u53d6\u539f\u59cbjson\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u5305\u62ectitle\uff0cauthors\uff0cabstracts\u548cbody_text\n\n2.\u5c06\u5bf9\u5e94\u7684\u6587\u672c\u6570\u636e\u5206\u8bcd\u3001\u53bb\u9664\u505c\u7528\u8bcd\u540e\u5411\u91cf\u5316\u5e76\u8fdb\u884cEmbedding\n\n3.\u5c06Embedding\u4e4b\u540e\u7684\u7279\u5f81\u4f7f\u7528AutoEncoder\u8fdb\u884c\u964d\u7ef4\n\n4.\u4f7f\u7528Kmeans\u8fdb\u884c\u805a\u7c7b\n\n5.\u805a\u7c7b\u4e4b\u540e\u4f7f\u7528TextRank\u62bd\u53d6\u6bcf\u4e2a\u7c7b\u522b\u7684\u5173\u952e\u5b57\u3001\u5173\u952e\u53e5","6cc3e30c":"## 3.\u6587\u672c\u5411\u91cf\u5316","25dbf427":"## 7.\u5404\u4e2a\u7c7b\u522b\u5173\u952e\u8bcd\u548c\u5173\u952e\u53e5\u5b50\u7684\u7edf\u8ba1","c2bc5dae":"\u8bfb\u53d6\u5355\u4e2a\u6587\u4ef6\u7684json\u6570\u636e\uff1a","5dc6b36b":"\u5728\u4f7f\u7528KMeans\u805a\u7c7b\u5b8c\u6210\u4e4b\u540e,\u6bcf\u7c7b\u4f7f\u7528TextRank\u62bd\u53d6\u5173\u952e\u8bcd\u548c\u5173\u952e\u53e5\uff0cTextRank\u7684\u601d\u60f3\u6bd4\u8f83\u7b80\u5355\uff0c\u901a\u8fc7\u8bcd\u4e4b\u95f4\u7684\u76f8\u90bb\u5173\u7cfb\u6784\u5efa\u56fe\uff0c\u7136\u540e\u4f7f\u7528PageRank\u7b97\u6cd5\u4e0d\u65ad\u7684\u8fed\u4ee3\u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u7684rank\u503c\uff0c\u7136\u540e\u6839\u636erank\u503c\u7684\u5927\u5c0f\u786e\u5b9a\u5173\u952e\u8bcd\u3002\n\n\u7531\u4e8eTextRank\u4e0d\u9700\u8981\u904d\u5386\u548c\u5b66\u4e60\u6574\u4e2a\u6587\u6863\u5e93\uff0c\u6240\u4ee5\u80fd\u591f\u6bd4\u8f83\u597d\u7684\u89e3\u51b3\u5185\u5b58\u4e0d\u591f\u95ee\u9898"}}