{"cell_type":{"d67e3c04":"code","6a765de8":"code","a3faec20":"code","38a98e36":"code","e989e9ae":"code","7b38b5bb":"code","56af4122":"code","bd896e25":"code","6c7503d6":"code","7e89ca52":"code","d9105065":"code","ff6c5c4d":"code","fdbc547b":"code","7e471606":"code","cde881c7":"code","d93a6e4c":"code","dcf78713":"code","b1bbc4fe":"code","6e43fd7c":"code","a71180e8":"code","6ada64de":"code","9046d1a2":"code","c968ee58":"code","28ebe5b0":"code","d088c3ec":"code","1017e558":"code","4fd42197":"code","9563a4c2":"code","5e404aa3":"code","3044f5f1":"code","59a7a76e":"code","3ab9aa99":"code","dff00be9":"code","4c9d2d0d":"code","a885ce69":"code","6f4c5337":"code","2e8738a3":"code","a448dea7":"code","6ffb4a13":"code","986a8c43":"code","de1899b3":"code","9d8f62a5":"code","8cca09d6":"code","10dc7612":"code","f071639b":"code","d8d380b7":"code","e59d5c89":"code","24dcbe48":"code","d9a69eaa":"code","0b863520":"code","5c80f096":"code","7fe70214":"code","81403c93":"code","8ec636ed":"code","47a689d2":"code","64dfea69":"code","d304f51e":"code","2cac2e82":"code","663add5d":"code","0a113cd6":"code","ae04f27e":"code","aaf6ce40":"code","01d76245":"code","d7802c8d":"markdown","cbd580dd":"markdown","b0eb1b57":"markdown","6eaf823d":"markdown","ca0b6b6f":"markdown","0720ecf3":"markdown","184356b5":"markdown","e8db6472":"markdown","8515bea3":"markdown","060c80a5":"markdown","37fc587d":"markdown","84178af8":"markdown","499ab404":"markdown","d8f954d7":"markdown","3f715188":"markdown","69ba9b95":"markdown","f910c8d0":"markdown","b30fdbb3":"markdown","ddd1fca2":"markdown","4ad27e4c":"markdown","fd689f1e":"markdown","4a83c5b6":"markdown","0d639ec6":"markdown","1c31c47c":"markdown","24433295":"markdown","2219d566":"markdown","c8757489":"markdown","e2b3a1e0":"markdown","ef24bfa1":"markdown","c8ff06ce":"markdown","b9164bd7":"markdown","bb2e55d7":"markdown","759b62c2":"markdown","a5cf01bc":"markdown","87d791f2":"markdown","a3aa6802":"markdown","1e32f759":"markdown","200f368c":"markdown","11a4388c":"markdown","26bc9936":"markdown","590ba281":"markdown","6ebd1725":"markdown","665763a9":"markdown"},"source":{"d67e3c04":"%config Completer.use_jedi = False\n\n!pip install -Uqqq plotnine\n\n!pip install -Uqqq pyicu\n!pip install -Uqqq pycld2\n!pip install -Uqqq morfessor\n!pip install -Uqqq polyglot\n\n!pip install -Uqqq transformers","6a765de8":"import matplotlib.pyplot as plt\nfrom plotnine import *\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\ncomments_to_score = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\nvalidation_data = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')","a3faec20":"unique_comments = pd.Series(validation_data.melt('worker')['value'].unique())\nlen(unique_comments)","38a98e36":"uc1 = unique_comments.str.strip().unique()\nuc2 = unique_comments.str.strip().str.lower().unique()\nuc3 = unique_comments.str.replace(\"\\n\", \" \").str.strip().str.lower().unique()\nprint(f'''\n    Number of unique comments: {len(unique_comments)}\n    Removing tailing spaces: {len(uc1)}\n    And lowering the case: {len(uc2)}\n    And removing scapes: {len(uc3)}\n    '''\n)","e989e9ae":"def sanitize(texts): return pd.Series(texts).str.replace('\\n', ' ').str.strip().str.lower()\n\nunique_comments = sanitize(unique_comments)\n\nid2txt = {k:v for k, v in enumerate(unique_comments)}\ntxt2id = {k:v for v, k in enumerate(unique_comments)}\n\n_validation_data = validation_data.copy()\n_validation_data['less_toxic_id'] = sanitize(_validation_data['less_toxic']).apply(lambda x: txt2id[x])\n_validation_data['more_toxic_id'] = sanitize(_validation_data['more_toxic']).apply(lambda x: txt2id[x])\n_validation_data['hashed_pair'] = (_validation_data[['less_toxic_id', 'more_toxic_id']]\n                                   .astype(str).apply(lambda x: '-'.join(sorted(x)), axis=1))\n_validation_data['hashed_pair_un'] = (_validation_data[['less_toxic_id', 'more_toxic_id']]\n                                   .astype(str).apply(lambda x: '-'.join(x), axis=1))","7b38b5bb":"df = _validation_data['hashed_pair'].value_counts().to_frame('n')\n\n(ggplot(df, aes('n'))\n + geom_bar(fill = 'orange', color = 'black')\n + geom_text(\n     aes(label=after_stat('prop*100'), group=1),\n     stat='count',\n     va='bottom',\n     format_string='{:.1f}%'\n )\n + ggtitle('Number of times each pair of comments is evaluated')\n + xlab('Number of comparisons')\n + ylab('Frequency')\n)","56af4122":"df['n'].value_counts()","bd896e25":"idx = df.index[df['n'] < 3]\nsingle_pairs = _validation_data.set_index('hashed_pair').loc[idx]\nsingle_comments = single_pairs[['less_toxic', 'more_toxic']].values.flatten()\n\nfrom difflib import SequenceMatcher\nsimilarity = []\nfor i, commentA in enumerate(tqdm(single_comments)):\n    for j, commentB in enumerate(single_comments):\n        if i < j:\n            similarity.append(SequenceMatcher(None, commentA, commentB).ratio())\n        else: \n            similarity.append(0)\n        \nsimilarity = np.array(similarity).reshape((len(single_comments), -1))\n\nplt.imshow(similarity);","6c7503d6":"for j in [-1, -2, -3, -4]:\n    id_max = np.unravel_index(similarity.flatten().argsort()[j], similarity.shape)\n    for i in [0, 1]:\n        print(f'Comment {i + 1}:\\n{\"-\"*50}')\n        print(single_comments[id_max[i]])\n        print(\"-\"*50, sep = '')\n    print('\\n', '='*50, '\\n', sep = '')","7e89ca52":"idx = df.index[df['n'] == 3]\n_validation_data = _validation_data.set_index('hashed_pair').loc[idx].reset_index()\n_validation_data.shape","d9105065":"_validation_data.to_csv('validation_data_clean.csv', index = False)","ff6c5c4d":"_validation_data","fdbc547b":"df = (_validation_data[['less_toxic_id', 'more_toxic_id']]\n      .melt(value_name = 'n')['n']\n      .value_counts()\n      .to_frame()\n     )\n(ggplot(df, aes('factor(n)'))\n + geom_bar(fill = 'orange', color = 'black')\n + geom_text(\n     aes(label=after_stat('prop*100'), group=1),\n     stat='count',\n     ha = 'left',\n     nudge_y = 200,\n     format_string='{:.1f}%'\n )\n + coord_flip()\n + ggtitle('TODO')\n + xlab('Number of comparisons per comment')\n + ylab('Frequency')\n + ylim(0, 10500)\n)","7e471606":"(ggplot(df, aes('factor(n)'))\n + geom_bar(fill = 'orange', color = 'black')\n + scale_y_log10()\n + coord_flip()\n + ggtitle('TODO')\n + xlab('Number of comparisons per comment')\n + ylab('Frequency (log10)')\n)","cde881c7":"_validation_data['worker'].nunique()","d93a6e4c":"df = _validation_data['worker'].value_counts().to_frame('n')\ndf.T","dcf78713":"(ggplot(df, aes('n'))\n + geom_histogram(bins = 20, fill = 'orange', color = 'black')\n + ggtitle('Histogram of Workload')\n + xlab('Number of comparisons per worker')\n + ylab('Absolute Frequency (number of workers at the bin)')\n)","b1bbc4fe":"unnanimous_pairs = (_validation_data['hashed_pair_un'].value_counts() == 3).to_dict()\n_validation_data['is_unanimous'] = _validation_data['hashed_pair_un'].apply(lambda x: unnanimous_pairs[x])","6e43fd7c":"df = _validation_data.groupby('hashed_pair').head(1)\n\n(ggplot(df, aes('is_unanimous'))\n + geom_bar(fill = 'orange', color = 'black')\n + geom_text(\n     aes(label=after_stat('prop*100'), group=1),\n     stat='count',\n     va='bottom',\n     format_string='{:.1f}%'\n )\n + ggtitle('Quantity of Unanimous decisions')\n + xlab('Is unanimous?')\n + ylab('Absolute Frequency')\n)","a71180e8":"_validation_data.groupby('worker')['is_unanimous'].agg(['mean', 'sum']).T","6ada64de":"df = (_validation_data\n      .groupby('worker')['is_unanimous']\n      .agg(['mean', 'sum', 'count'])\n      .reset_index()\n     )\ndf['bins_mean'] = pd.cut(df['mean'], 10)\n\n(ggplot(df.query('count > 5'), aes(x = 'bins_mean', y = after_stat('count')))\n + geom_bar(fill = 'orange', color = 'black')\n + ggtitle('Frequency of Unannimity\\n(for workers with more than 5 labels)')\n + coord_flip()\n + xlab('Relative frequency of unannimity per worker')\n + ylab('Number of workers at that frequency of unannimity')\n)","9046d1a2":"_df = df.query('count > 5').sort_values('mean', ascending = False).head(20)\n(ggplot(_df, aes(x = 'factor(worker)', y = 'mean'))\n + geom_bar(stat = 'identity', fill = 'orange', color = 'black')\n + ggtitle('Unanimous frequency by worker\\n(top 20 of workers with over 5 labels)')\n + coord_flip()\n + xlab('Worker id')\n + ylab('Unanimity rate')\n + scale_x_discrete(limits = _df['worker'][::-1])\n)","c968ee58":"_df = df.query('count > 5').sort_values('mean', ascending = False).tail(20)\n(ggplot(_df, aes(x = 'factor(worker)', y = 'mean'))\n + geom_bar(stat = 'identity', fill = 'orange', color = 'black')\n + ggtitle('Unanimous frequency by worker\\n(worst 20 of workers with over 5 labels)')\n + coord_flip()\n + xlab('Worker id')\n + ylab('Unanimity rate')\n + scale_x_discrete(limits = _df['worker'][::-1])\n)","28ebe5b0":"(ggplot(df, aes(x = 'count', y = 'mean'))\n + geom_hline(yintercept = 0.5, color = 'red')\n + geom_point(color = 'orange')\n + geom_smooth(method = 'lm', color = 'darkorange')\n)","d088c3ec":"df = (_validation_data\n      .query('is_unanimous')\n      .groupby('hashed_pair')\n      .head(1)[['less_toxic', 'more_toxic']]\n      .apply(lambda x: x.apply(lambda x: len(x)), axis = 1)\n      .melt(var_name = 'toxicity', value_name = 'length')\n     )\ndf_mean = df.groupby('toxicity').median().reset_index()","1017e558":"(ggplot(df, aes(x = 'length', fill = 'toxicity'))\n + geom_density(color = 'black', alpha = 0.5)\n + geom_vline(df_mean, aes(xintercept = 'length', color = 'toxicity'))\n + xlab('Length of the comment')\n)","4fd42197":"(ggplot(df.query('length < 500'), aes(x = 'length', fill = 'toxicity'))\n + geom_density(color = 'black', alpha = 0.5)\n + geom_vline(df_mean, aes(xintercept = 'length', color = 'toxicity'))\n + xlab('Length of the comment (clipped at 500)')\n)","9563a4c2":"(_validation_data['less_toxic'].str.len() > _validation_data['more_toxic'].str.len()).mean()","5e404aa3":"from polyglot.detect import Detector","3044f5f1":"def get_language(text):\n    return Detector(\"\".join(x for x in text if x.isprintable()), quiet=True).languages[0].name","59a7a76e":"%%capture\nlangs = [get_language(comment) for comment in unique_comments]","3ab9aa99":"df = pd.DataFrame({\n    'text': unique_comments,\n    'lang': langs\n})\ndf.tail()","dff00be9":"df['lang'].value_counts().to_frame().T","4c9d2d0d":"df[df['lang'] == 'un']","a885ce69":"df[df['lang'] == 'German']","6f4c5337":"from transformers import pipeline\nfrom tqdm.notebook import tqdm","2e8738a3":"def predict(text):\n    try:\n        p = classifier(text)[0]\n        df = pd.DataFrame(p).set_index('label').T\n        df['text'] = text\n        return df\n    except:\n        return None","a448dea7":"classifier = pipeline(\"text-classification\", model='distilbert-base-uncased-finetuned-sst-2-english', return_all_scores=True)","6ffb4a13":"unique_comments[1]","986a8c43":"predict(unique_comments[1])","de1899b3":"predictions = [predict(text) for text in tqdm(unique_comments[:250])]\npredictions = pd.concat(predictions).reset_index(drop = True)\npredictions.tail(1)","9d8f62a5":"(ggplot(predictions.melt('text'), aes(x = 'label', y = 'value'))\n+ geom_boxplot(fill = 'orange', color = 'black')\n)","8cca09d6":"(predictions['NEGATIVE'] > 0.5).mean()","10dc7612":"predictions.sort_values('NEGATIVE').reset_index().loc[1, 'text']","f071639b":"classifier = pipeline(\"text-classification\", model='bhadresh-savani\/distilbert-base-uncased-emotion', return_all_scores=True, function_to_apply = 'sigmoid')","d8d380b7":"predictions = [predict(text) for text in tqdm(unique_comments[:250])]\npredictions = pd.concat(predictions).reset_index(drop = True)\npredictions.tail(1)","e59d5c89":"(ggplot(predictions.melt('text'), aes(x = 'label', y = 'value'))\n+ geom_boxplot(fill = 'orange', color = 'black')\n)","24dcbe48":"predictions.sort_values('love', ascending = False).reset_index().loc[1, 'text']","d9a69eaa":"classifier = pipeline(\"text-classification\", model='unitary\/toxic-bert', return_all_scores=True, function_to_apply = 'sigmoid')","0b863520":"predictions = [predict(text) for text in tqdm(unique_comments[:250])]\npredictions = pd.concat(predictions).reset_index(drop = True)\npredictions.tail(1)","5c80f096":"(ggplot(predictions.melt('text'), aes(x = 'label', y = 'value'))\n+ geom_boxplot(fill = 'orange', color = 'black')\n)","7fe70214":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","81403c93":"from tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nimport torch","8ec636ed":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","47a689d2":"MODEL_NAME = 'unitary\/toxic-bert'","64dfea69":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","d304f51e":"model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)","2cac2e82":"# remove the slicing of [:36] to run the whole set\ncomments_to_score = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')[:36] \n# comments_to_score_texts = comments_to_score['text'].values","663add5d":"BS = 8\ndef get_comments_to_score():\n    txts = comments_to_score['text'].values\n    for i in range(0, len(txts), BS):\n        yield txts[i : i + BS].tolist()","0a113cd6":"outputs = []\nfor sequences in tqdm(get_comments_to_score(), total = len(comments_to_score) \/\/ BS): \n    tokens = tokenizer(sequences, \n                       padding=True, \n                       truncation=True, \n                       add_special_tokens=True,\n                       return_tensors=\"pt\").to(device)\n    output = model(**tokens)\n    outputs.append(output['logits'].cpu().detach().numpy())","ae04f27e":"predictions = np.concatenate(outputs)[:,0]","aaf6ce40":"comments_to_score['score'] = predictions\ncomments_to_score = comments_to_score.drop('text', axis = 1)","01d76245":"comments_to_score.to_csv('submission.csv', index = False)\ncomments_to_score","d7802c8d":"The same is also true for most of the other 'non-english' labels. So we can assume that at least the vast majority is written in (sort of) English","cbd580dd":"Visualizing the same data using a log scale shows a clear linear relationship, which is quite neat.","b0eb1b57":"# 9. Are the length of toxic comments any different from non-toxic?","6eaf823d":"<h1><center>Comprehensive EDA for the Jigsaw comp + Sentiment analysis using Huggingface Pipeline + Benchmark Baseline<\/center><\/h1>\n                           \n                           \n<center><img src = \"https:\/\/i.imgur.com\/iRX7hwu.png\" width = \"1000\" height = \"400\"\/><\/center>                                                                            ","ca0b6b6f":"Right out of the bat, we can see that some workers have zero unanimous decisions, and some have 70, which will have a widespread.","0720ecf3":"# 8. Are some workers that are notably better? (i.e., participated more on unanimities)","184356b5":"Indeed, the less toxic comments are a bit longer than the non-toxic comments, let's see if this could be used as a predictor for this dataset.","e8db6472":"From this we have over 14k comments in english and over 50 of unknown language. Let's check the ones unknown","8515bea3":"<h3 style='background:orange; color:black'><center>Thank you =)<\/center><\/h3>\n","060c80a5":"From the table we can see that it ranges from 1 pair to 248 pairs.","37fc587d":"# 4. What is the frequency of that each comment is ranked?","84178af8":"Ok, apparently, the remaining comments are a fluke on the labeling process and not just the same comment saved with artifacts.\n\nFrom now on **I FILTERED THOSE OUT FROM THE REST OF THE EDA**","499ab404":"## b. Sadness, joy, love, anger, fear and surprise","d8f954d7":"# 11. What are the main sentiment on this dataset?","3f715188":"The pairs with only one comparison are probably a bug or could be sanitization problems, so let's further investigate this.","69ba9b95":"<h3 style='background:orange; color:black'><center>Consider upvoting this notebook if you found it helpful.<\/center><\/h3>","f910c8d0":"# 1. How many unique comments are in the dataset?","b30fdbb3":"Ok, that is interesting. The new dataset has EXACTLY 30k comparisons (i.e., 10k unique pairs) \n\nLet me save this dataset for future usage","ddd1fca2":"We define a unanimous decision when all the three workers agree.","4ad27e4c":"# 5. How many workers?","fd689f1e":"As expected the more labels a worker grade the closest it is to the mean. In my opinion we should not exploit this information as it is most likely noise.","4a83c5b6":"So in this dataset, we apparently have 14251 unique comments. Let's check if we can sanitize any of them","0d639ec6":"# 6. How many pairs did each worker labeled?","1c31c47c":"From the image, we can see that are indeed some quite similar comments, let's check a few of them.","24433295":"## a. Positive vs. Negative","2219d566":"# Baseline using unitary\/toxic-bert model","c8757489":"753 people worked as labelers, that is quite a lot. Let's check how many labels each of them did.","e2b3a1e0":"# 3. Are all comments compared three times?","ef24bfa1":"## c. Toxic, sever toxic, obscene, threat, insult, identity hate","c8ff06ce":"# 10. Are all comments written in English?","b9164bd7":"We dropped the number of unique comments by removing leading\/trailing spaces and lowering the cases by 13. There might be more stuff to clean, but I will leave it like that.\n\nFor the remainder of my analysis, it might be handy to have unique IDs for each comment, so let's do that now.","bb2e55d7":"But the majority of workers labeled less than 5 pairs. Only a handfull labeled more than 200.","759b62c2":"# 2. What types of 'noise' are in the text? (spaces, and so on)","a5cf01bc":"From this, we can see that most comments (over 8k) are compared only against a single comment (i.e., three times). But a few others are compared against multiple other comments.","87d791f2":"Let's investigate it a bit further and see how it coorelates with the total number of unanimity per worker.","a3aa6802":"Close but no cigar. This naive predictor would give us 54% accuracy, which is not *that* terble, but I am sure we can do much better using deeplearning.","1e32f759":"Not surprisingly, the majority of workers will reach unanimity at half the time (just like the global average). But there are a few that are significantly better and a few worse.","200f368c":"Remarkably, almost half of the decisions were unanimous. Keep in mind that random chance would be 25% only.","11a4388c":"Ok, they are clearly too poorly written to be detected as English... lmao","26bc9936":"This EDA attempts to answer the following questions I had:\n1. How many unique comments are in the dataset?\n1. What types of 'noise' are in the text? (spaces, and so on)\n1. Are all comments compared three times?\n1. What is the frequency of that each comment is ranked?\n1. How many workers?\n1. How many pairs did each worker labeled?\n1. What is the frequency of unanimity?\n1. Are some workers that are notably better? (i.e., participated more on unanimities)\n1. Are the length of toxic comments any different from non-toxic?\n1. Are all comments written in English?\n1. What are the main sentiment on this dataset considering the following models:\n    1. Positive vs. Negative\n    1. Sadness, joy, love, anger, fear and surprise\n    1. Toxic, sever toxic, obscene, threat, insult, identity hate","590ba281":"**For the offilne version check:**\nhttps:\/\/www.kaggle.com\/coldfir3\/simple-inference-notebook-on-pre-trained-model","6ebd1725":"## Investigating if there are any more sanity problems...","665763a9":"# 7. What is the frequency of unanimity?"}}