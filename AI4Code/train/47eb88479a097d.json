{"cell_type":{"8a50fc71":"code","31fbf988":"code","d0a596e6":"code","190dd7e7":"code","3f0be9bd":"code","b69f41b0":"code","6dcafae4":"code","6615dee9":"code","a75eaa18":"code","7cfc3093":"code","8cd512b8":"code","4c2e7a6c":"markdown","b2e86523":"markdown","24e38288":"markdown","79bc875c":"markdown","16fb3c5d":"markdown","406294ef":"markdown","281e6324":"markdown","fec0ab4d":"markdown","1fada7e6":"markdown","ad3b22d1":"markdown"},"source":{"8a50fc71":"!pip install --upgrade pip\n!pip uninstall -y allennlp\n!pip install transformers==4.1.1 typer\n!pip install -U pytorch-lightning\n!pip install https:\/\/github.com\/veritable-tech\/pytorch-lightning-spells\/archive\/master.zip\n!pip install -U sentencepiece","31fbf988":"!mkdir -p \/src\/finetuning-t5\n!git clone https:\/\/github.com\/ceshine\/finetuning-t5.git -b master \/src\/finetuning-t5","d0a596e6":"%cd \/src\/finetuning-t5\/mnli\n%git checkout 13b9351","190dd7e7":"!mkdir -p data\/multinli_1.0\n!cp -r \/kaggle\/input\/multinli-nyu\/multinli_1.0\/* data\/multinli_1.0\/\n!ls data\/multinli_1.0\/","3f0be9bd":"!mkdir -p data\/kaggle\n!cp -r \/kaggle\/input\/contradictory-my-dear-watson\/* data\/kaggle\/\n!ls data\/kaggle\/","b69f41b0":"!python preprocess\/preprocess_kaggle.py\n!python preprocess\/preprocess_mnli.py\n!python utils\/reduce_sentencepiece_vocab.py google\/mt5-base","6dcafae4":"!mkdir cache\/multinli\n!python preprocess\/tokenize_dataset.py multinli --tokenizer-name cache\/mt5-base","6615dee9":"!SEED=3313 python train.py --batch-size 64 --grad-accu 1 --max-len 128 --epochs 2 --t5-model cache\/mt5-base \\\n    --lr 2e-3 --dataset multinli --disable-progress-bar --valid-frequency 0.5 --freeze-embeddings","a75eaa18":"!python evaluate.py cache\/mt5-base_best --corpus multinli --split-name test_matched --batch-size 32","7cfc3093":"!python evaluate.py cache\/mt5-base_best --corpus multinli --split-name test_mismatched --batch-size 32","8cd512b8":"!mv cache\/tb_logs \/kaggle\/working\n!mv cache\/mt5-base_best \/kaggle\/working","4c2e7a6c":"## Get the code","b2e86523":"## Export the Tensorboard log files and the trained model","24e38288":"## Calculate accuracy of the \"mismatched\" dev set","79bc875c":"## Only keep tokens that we need","16fb3c5d":"## Prepare the environment","406294ef":"## Tokenize","281e6324":"## Fine-tune the model","fec0ab4d":"## Preprocess the dataset","1fada7e6":"## Calculate accuracy of the \"matched\" dev set","ad3b22d1":"[The mT5 models were trained with the multitask objective, so it'll be harder to get good results with simple finetuning procedure.](https:\/\/github.com\/google-research\/text-to-text-transfer-transformer\/blob\/master\/released_checkpoints.md#t511) This notebook conducts an experiment to find the level of accuracy we can get with a naive finetuning scheme.\n\nIn my experiment, AdaFactor with a custom learning rate schedule has been shown to perform better than AdamW and AdaFactor with the built-in schedule."}}