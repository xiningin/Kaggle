{"cell_type":{"6932588b":"code","0b424896":"code","bac84f7e":"code","43e2231b":"code","99c060b5":"code","468ec10c":"code","214bc4ec":"code","fd39802a":"code","a5dadb02":"code","46e35707":"code","4b47d3f4":"code","12557e10":"code","2eefbb92":"code","6df1dadc":"code","d61a853b":"code","5786470b":"code","f58ea2bd":"code","85776987":"code","9e18beae":"code","caab1611":"code","ee12c17b":"markdown","ab9d6008":"markdown","e83f5378":"markdown","a8bd804d":"markdown","33e5f337":"markdown","34feffab":"markdown","ec509b9a":"markdown","4a1d70e0":"markdown","e12e7cf6":"markdown"},"source":{"6932588b":"## Importando meu script de inicializa\u00e7\u00e3o padr\u00e3o\nfrom imports_and_styles import *\n\n## Importando demais bibliotecas necess\u00e1rias\n!pip install attention --quiet\n\nimport tensorflow as tf\nimport keras\nfrom keras import metrics, losses, models\nfrom keras import backend as K\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.layers import Dense, LSTM, Dropout, Conv1D, Bidirectional\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\n\nfrom attention import Attention\n\nimport plotly.express as px\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\nmetrics = root_mean_squared_error\n\n# !pip3 install --upgrade pandas --quiet\n# import pandas\n\n!python --version\nprint('pandas version: ' +  pd.__version__)\nprint(\"tensorflow version: \" + tf.__version__)\n## Aqui abaixo v\u00e3o aparecer um monte de outras bibliotecas importadas pelo script de inicializa\u00e7\u00e3o, assim como a descri\u00e7\u00e3o da fun\u00e7\u00e3o 'reduce_memory_usage' que ser\u00e1 usada no notebook","0b424896":"# Carrega os arquivos csv\ndf      = pd.read_csv('\/kaggle\/input\/desafio-falconi-i\/train.csv', parse_dates=['date'])\ndf_test = pd.read_csv('\/kaggle\/input\/desafio-falconi-i\/test.csv', parse_dates=['date'])\n\n# Cria vari\u00e1vel 'Pandemia', com valor 1 quando a loja est\u00e1 fechada \n# ideia vem de uma an\u00e1lise pr\u00e9via dos dados\n\nfrom_ts = '2020-03-18'\nto_ts   = '2020-10-22'\ndf['Pandemia'] = np.where(df.date < from_ts, 0, np.where(df.date > to_ts, 0, 1))\n# df['Post_pandemia'] = np.where(df.date > to_ts, 1, 0)\n\n# Deletamos da base de dados os dias em meio a pandemia\n# df = df[df['Pandemia']==0]\n\n# Copiando os dataframes como bom costume somente\ntrain_df    = df.copy()\ndf_testLoop = df_test.copy()\n\n# Reduzindo a quantidade de mem\u00f3ria utilizada pelas bases, para agilizar o processamento\ntrain_df    = reduce_memory_usage(train_df, verbose=True)\ndf_testLoop = reduce_memory_usage(df_testLoop, verbose=True)\n\n# Ordenando o dataframe para termos cada dia de venda de cada produto de cada loja, em ordem\ntrain_df.sort_values(by=['cod_store', 'cod_product', 'date'], axis=0, inplace=True)","bac84f7e":"## Limpeza da base:\n### 1- Cria\u00e7\u00e3o de coluna de identifica\u00e7\u00e3o de cada produto em cada loja\n### 2- Transforma\u00e7\u00e3o do id acima em formato categoria\n### 3- Deletando colunas que n\u00e3o ser\u00e3o utilizadas\n\n# drop_cols = ['id', 'cod_product', 'precipitation', 'temperature_min', 'temperature_max', 'week_avg_price', 'Pandemia', 'Post_pandemia']\n\n# train= (train_df\n#         .assign (cods = lambda x: x.cod_store.astype(str) +'|'+ x.cod_product.astype(str) +'|')\n#         .assign (cods = lambda x: x.cods.astype('category'))\n#         .drop        (columns = drop_cols)\n# #         .groupby     (['cod_store','date']).sum()\n#         .reset_index (drop=False)\n#         )\n\n# train\n\n\ndrop_cols = ['id', 'cod_product','precipitation', 'temperature_min', 'temperature_max', 'week_avg_price', 'Pandemia']\n\ntrain= (train_df\n#         .assign (cods = lambda x: x.cod_store.astype(str) +'|'+ x.cod_product.astype(str) +'|')\n        .assign (cod_store = lambda x: x.cod_store.astype('category'))\n#         .assign (cod_product = lambda x: x.cod_product.astype('category'))\n        .drop   (columns = drop_cols)\n        )\n\ntrain","43e2231b":"## Transformando a base de dados: \n### Cada coluna ser\u00e1 a quantidade de vendas de cada produto, de cada loja\n### Cada linha corresponde a 1 dia\n\ntrain1= (pd\n         .pivot_table (train,index='date',columns='cod_store',values='sale_quantity',aggfunc='sum')\n#          .sort_index  ()\n#          .reset_index (drop=False)\n         .copy()\n         )\n\ntrain1","99c060b5":"## Transformando as quantidades de venda para o range de 0 a 1, adequando a base ao modelo de redes neurais\n\n# sc = MinMaxScaler(feature_range = (0, 1))\ntrain2 = train1.to_numpy()\ntrain2","468ec10c":"train_2 = pd.DataFrame(train2, columns = list(train1.columns), index = train1.index)\n# train_2 = train_2.reset_index(drop=False)\nfrom_ts = '2020-03-18'\nto_ts   = '2020-10-22'\n\n# train1.reset_index (drop=False)\n# train1['data'] = train1.index\ntrain_2['Pandemia'] = np.where(train_2.index < from_ts, 0, np.where(train_2.index > to_ts, 0, 1))\ntrain_2['Post_pandemia'] = np.where(train_2.index > to_ts, 1, 0)\nprint(train_2.head())\n\nsc = MinMaxScaler(feature_range = (0, 1))\ntrain_3 = sc.fit_transform(train_2)\ntrain_3","214bc4ec":"## EXEMPLO DA FUN\u00c7\u00c3O TimeseriesGenerator\n### A Tabela abaixo ser\u00e1 usada como exemplo. Ela cont\u00e9m 6 linhas e 3 colunas\n\nTabela = [[ 1, 1, 1],\n          [ 2, 2, 2],\n          [ 3, 3, 3],\n          [ 4, 4, 4],\n          [ 5, 5, 5],\n          [ 6, 6, 6]]\n\n### n_lenght \u00e9 o Rolling Window a ser usado - qtd de linhas lidas para prever a seguinte\n#### Nesse exemplo, lemos 2 linhas para prever a 3\u00aa, ou seja, rolling window = 2\n\nn_length = 2\n\n### generator conter\u00e1 todos os dados no formato correto para o modelo\ngenerator = TimeseriesGenerator(data    = Tabela,   ## dados para aprendizado\n                                targets = Tabela,   ## dados que queremos prever (os mesmos)\n                                length  = n_length, ## length de nossa rolling window\n                                batch_size = 1 ## batches antes de re-calcularmos os parametros\n                                )\n\n# Imprime em tela o n\u00famero de amostras (linhas) lidas => linha de previs\u00e3o:\nfor i in range(len(generator)):\n    x, y = generator[i]\n    print('%s => %s\\n' % (x, y))","fd39802a":"## Usando a ferramenta do Keras para gerar TimeSeries\ndays_to_predict = 14                        ## Qtd de dias que queremos prever\nn_input         = 28                        ## Defini\u00e7\u00e3o de 14 dias como rolling window\n\nn_features      = len(list(train1.columns)) ## Contagem de N\u00daMERO DE COLUNAS no dataframe\nn_index         = train1.count().max()      ## Contagem de N\u00daMERO DE LINHAS no dataframe\ntrain_rows      = round((n_index \/ 100 * 90))      ## SEPARA 20% FINAIS dos dados, para valida\u00e7\u00e3o\n\n## Gerando a base de treino\ntrain_gen = TimeseriesGenerator(data       = train2,  ## dados para aprendizado\n                                targets    = train2.loc[:,],  ## dados que queremos prever (o mesmo)\n                                length     = n_input, ## length de nossa rolling window\n                                batch_size = 1, ## batches antes de re-calcularmos os parametros\n                                end_index  = train_rows ## train_gen = 80% do total de dados\n                                )\n\n# n\u00famero de amostras para treino:\nprint('Training samples: %d' % len(train_gen))\n\n## AS LINHAS ABAIXO SERVEM PARA CRIAR O DATASET DE VALIDA\u00c7\u00c3O\n\n# Gerando a base de teste, que ser\u00e1 utilizada como valida\u00e7\u00e3o para o treinamento\ntest_gen = TimeseriesGenerator(data        = train2,\n                               targets     = train2,\n                               length      = n_input,\n                               batch_size  = 1,\n                               start_index = train_rows + 1  ## test_gen = 20% dos dados\n                               )\n\n# n\u00famero de amostras para valida\u00e7\u00e3o:\nprint('Validation samples: %d' % len(test_gen))","a5dadb02":"## Usando a ferramenta do Keras para gerar TimeSeries\n# def Gen_timeseries(df1, n_features, n_index, n_input, train_rows):\n\nn_input         = 28                        ## Defini\u00e7\u00e3o de 14 dias como rolling window\nn_features      = len(list(train_2.columns)) ## Contagem de N\u00daMERO DE COLUNAS no dataframe\nn_index         = train_2.count().max()      ## Contagem de N\u00daMERO DE LINHAS no dataframe\ntrain_rows      = round((n_index \/ 100 * 90))\n\n\ndf = train_3.copy()\ndays_to_predict = 14\n\nX_train = []\ny_train = []\nfor i in range(n_input, train_rows):  #(train_rows - days_to_predict)):\n    X_train.append(df[i - n_input : i])\n    y_train.append(df[i][0 : 3])\n\nX_train = np.array(X_train, dtype = 'float16')\ny_train = np.array(y_train, dtype = 'float16')\nprint(X_train.shape,y_train.shape)\n\nX_val = []\ny_val = []\nfor i in range((train_rows + n_input), n_index):\n    X_val.append(df[i - n_input : i])\n    y_val.append(df[i][0 : 3])\n\nX_val = np.array(X_val, dtype = 'float16')\ny_val = np.array(y_val, dtype = 'float16')\nprint(X_val.shape,y_val.shape)\n\n#     return(X_train, y_train, X_val, y_val)","46e35707":"## Defini\u00e7\u00e3o do modelo de redes neurais, usando uma camada Convolucional e uma LSTM\n\n## Inicia a rede neural - 'sequencial' permite adicionar camadas sem dizer seus inputs e outputs\nmodel = tf.keras.Sequential()  \n\nmodel.add(Conv1D(filters     = 32,          # *      ## quantidade de filtros para gerar    \n                 kernel_size = 3,           # *      ## tamanho da janela do filtro         \n                 activation  = 'relu',      # *      ## modo de ativa\u00e7\u00e3o da sa\u00edda da camada\n                 padding     = 'same',                \n                 input_shape = (n_input, n_features) ## formato dos dados,\n                ))                                   ## n_input = dias de rolling window\n\n\n## Usando uma camada bi-direcional, que faz o fluxo dos dados em ambos os sentidos temporais\n## Camada LSTM guarda informa\u00e7\u00f5es dos batch lido, atuando como um modelo Auto Regressivo\n## LSTM Bi-direcional usa informa\u00e7\u00f5es passadas e futuras para fazer melhores predi\u00e7\u00f5es!\nmodel.add(keras.layers.Bidirectional(LSTM(100, return_sequences=True)))\n\n## Aleat\u00f3riamente (p = 0.2) faz alguns outputs=0, evitando overfitting (bom costume com LSTM)\nmodel.add(Dropout(0.2))      # *\n\n## Camada Attention faz o modelo reconhecer quais dados s\u00e3o relevantes e 'ignorar' os demais\nmodel.add(Attention())                     \n\n## Camada comum de rede neural 'densely-connected' \n## (cada neur\u00f4nio de uma camada se conecta a todos os neur\u00f4nios da camada seguinte)\nmodel.add(Dense(150, activation='relu'))   # * \nmodel.add(Dense(150, activation='relu'))   # * (alterar qtd de neuronios pode mudar outputs)\nmodel.add(Dense(150, activation='relu'))   # * \n\n## \u00daltima camada do modelo. Gera as sa\u00eddas com 'n_features' = total de produtos\/lojas\nmodel.add(Dense(3))            \n\nmodel.compile(loss      = tf.losses.MeanSquaredError(), # Famoso 'mse'\n              optimizer = Adam(),\n              metrics   = [tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n              )\n\nmodel.summary()   ## descomente essa linha para gerar a descri\u00e7\u00e3o detalhada do modelo","4b47d3f4":"## Abaixo vemos nossa Rede Neural\n### A camada Attention est\u00e1 representada por todas suas componentes: \n####  Lambda, Dense, Dot, Activation, Dot, Concatenate e Dense.\n\nplot_model(model,\n#            show_shapes=True,\n#            show_dtype=True,\n           show_layer_names=False,\n           rankdir=\"LR\",\n           dpi=128,\n           )","12557e10":"### Early Stopping diz ao nosso modelo quando ele deve parar o treinamento, se baseando nos resultados obtidos para a a base de valida\u00e7\u00e3o. Usado para evitar overfitting!\n### OBS 1: Epochs (\u00e9pocas) \u00e9 a qtd de vezes que treinaremos a base inteira *\n\n### OBS2: SE FOR USAR VALIDATION, TROQUE monitor ='loss' PARA: ='val_rmse')\n\nearly_stop = EarlyStopping(monitor   = 'val_rmse', # M\u00e9trica para acompanhar.\n                           patience  = 10,     # 3 \u00e9pocas sem melhorias = p\u00e1ra o treinamento *\n                           verbose   = 1,     # ver dados de early stopping\n                           mode      = 'min', # m\u00e9trica \u00e9 MELHOR o quanto MENOR for\n                           restore_best_weights = True  # retorna rede com melhores resultados\n                           )","2eefbb92":"## Rodamos a modelagem com o c\u00f3digo abaixo\n### OBS 1: Epochs (\u00e9pocas) \u00e9 a qtd de vezes que treinaremos a base inteira *\n### OBS 2: CASO USE A BASE DE VALIDA\u00c7\u00c3O, DESCOMENTE A LINHA 5\n\nmodel.fit(X_train, y_train,                     # base de treinamento gerada com o rolling window\n          validation_data = (X_val,y_val),    # base de valida\u00e7\u00e3o rolling window\n          epochs     = 100,               # qtd m\u00e1xima de \u00e9pocas para o treinamento *\n          verbose    = 1,                # ver os resultados parciais\n          callbacks  = [early_stop],     # chama o early stopping\n          batch_size = 10,\n          workers    = os.cpu_count(),   # qtd de cpus usadas no treinamento = total de cpus\n          use_multiprocessing = True)    # para usar mais de uma cpu (usado com 'workers')","6df1dadc":"for i in range(5):\n    print(model.train_on_batch(X_val, y_val))","d61a853b":"## C\u00f3digo que computa as predi\u00e7\u00f5es usando o modelo treinado anteriormente\n\n## copiando o df original pra evitar dor de cabe\u00e7a com erros.....\ntrain3 = train_3.copy()                         \n\nfor n in range(days_to_predict):        ## lembrando que 'days_to_predict' = 14\n    inputs = train3[-n_input :]         ## 'input' recebe os \u00faltimos 14 dias ([-14:]) de dados   \n    X_test = []\n    X_test.append(inputs[0 : n_input])  ## 'X_test' recebe os 14 dias de 'input'\n    X_test = np.array(X_test)\n    \n    predictions = model.predict(X_test)  ## predi\u00e7\u00e3o acontece aqui!\n    b = np.array([[0, 1]])\n    predictions_concat = np.concatenate((predictions, b), axis=1)\n    ## concatena as predi\u00e7\u00f5es logo ao final da base original para que quando rode novamente,\n    ## a linha de previs\u00e3o gerada esteja dentre as \u00faltimas 14!\n    train3 = np.vstack((train3 , predictions_concat))","5786470b":"train3  = train3[-days_to_predict:]     ## \u00faltimas 14 linhas desse df s\u00e3o as previs\u00f5es geradas\ntrain4  = sc.inverse_transform(train3)  ## converte de volta para a escala original dos dados\n\n### gerando um df com as predi\u00e7\u00f5es calculadas\nresults = pd.DataFrame(data    = train4,              ## predi\u00e7\u00f5es calculadas                       \n                       columns = list(train_2.columns) ## nomes originais das colunas\n                       )  ","f58ea2bd":"results = results.drop(columns=['Pandemia', 'Post_pandemia'])\nresults","85776987":"## adicionando informa\u00e7\u00e3o de dia (data) ao df com as previs\u00f5es\n\nresults.index = pd.date_range(start='2020-12-09', periods = days_to_predict)\n# results1 = results.melt(ignore_index=False)\n# results1 = results1.reset_index(drop=False)\n\n# ## retornando a coluna 'id'\n# results1['id'] = results1['variable'].astype(str) + results1['index'].astype(str)\n\n# results2 = (results1\n#             .drop     (columns = ['index', 'variable'])\n#             .rename   (columns = {'value' : 'sale_quantity1'})\n#             )\n                        \n# LSTM_results = results2[['id', 'sale_quantity1']]\n# LSTM_results\nresults","9e18beae":"## PEGADINHA DO DESAFIO: existem produtos novos na base 'sample_submission', e consideraremos suas vendas = 0\n### para isso, precisamos carregar o arquivo de submiss\u00e3o, dar um merge com nossos resultados, usando fillna(0) para preencher os dados faltantes com 0.\ndf_sample    = pd.read_csv('..\/input\/desafio-falconi-i\/sample_submission.csv')\nLSTM_results = LSTM_results.merge(df_sample, on='id', how='outer')\nLSTM_results_final = LSTM_results[['id', 'sale_quantity1']].fillna(0)\n\n## retornando nomes originais - desorganiza\u00e7\u00e3o minha\nLSTM_results_final = LSTM_results_final.rename(columns = {'sale_quantity1': 'sale_quantity'})\n\n## nunca uma venda ser\u00e1 negativa, melhor fazer todos os negativos = 0 hehehe\nLSTM_results_final['sale_quantity'] = (LSTM_results_final['sale_quantity']\n                                            .apply(lambda x : x if x > 0 else 0)\n                                            )","caab1611":"LSTM_results_final.to_csv('fingers_crossed.csv', index=False)\nLSTM_results_final.max()","ee12c17b":"# 1\u00ba Desafio Falconi de AI\n## Prever a quantidade de vendas de cada produto em cada loja de uma rede de Fast-Food, para os pr\u00f3ximos 14 dias\n#### Nesse notebook, usaremos redes neurais para criar um modelo de previs\u00e3o de vendas.\n\n### Interessante notar que **_n\u00e3o h\u00e1 nenhuma nova feature_** nessa modelagem!","ab9d6008":"## 4- Cria\u00e7\u00e3o do dataset para o modelo de redes neurais\nO c\u00f3digo de exemplo abaixo mostra o funcionamento do TimeseriesGenerator, uma biblioteca com ferramentas para cria\u00e7\u00e3o de redes neurais.\n\nA ideia aqui \u00e9 que nosso modelo de Machine Learning processe alguns dias de dados (linhas) de nosso Dataframe, e em seguida tente prever quais ser\u00e3o os valores no pr\u00f3ximo dia.<br><br>\nExemplo:<br>\n1 Passo: Modelo l\u00ea os dados do 1\u00ba dia at\u00e9 o 4\u00ba dia, e em seguida prev\u00ea o 5\u00ba dia<br>\n2 Passo: Modelo l\u00ea os dados do 2\u00ba dia at\u00e9 o 5\u00ba dia, e em seguida prev\u00ea o 6\u00ba dia<br>\n3 Passo: Modelo l\u00ea os dados do 3\u00ba dia at\u00e9 o 6\u00ba dia, e em seguida prev\u00ea o 7\u00ba dia<br>\n\n![Sliding Window](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1316515\/2192645\/Sliding_window.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210504%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210504T144911Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=5b82c353397afa98f5e39ab7389aa3f600dc4c542cee4600476b47f926a4ee8e0da4146b75e1ab99d944c585304b233cf390dab30885d3e1bbf5b3e1c3b130d68d41358e9be60ee05a25de06253a54ca971f6a575899911ee11d0088298374c1ae71168afa2bcd034dbcbd8a29a95024d313d389bd6fed5255a552a6101df9d152847dcf099892c8de12f5527a2abee9c84f8509ee576a4d9492116e97907055bb9aed2e4085d1de67d4eec507dc997dea65047e7c74247c0e5b10b063f4f5f5f1858f9d32e8b808189e1a8460c2ca1153aafe76a17b39cb901caef60313caf8c6f8912f30972e1dd1743bc156abd441cbb83dc04bfc8887daf603eb81a1fffa)\n\nCada passo na verdade recebe o nome de _**'batch'**_, portanto podemos descrever de forma mais simples:\n\nbatch 1: &emsp; linhas [1 a 4] => linha [5]<br>\nbatch 2: &emsp; linhas [2 a 5] => linha [6]<br>\nbatch 3: &emsp; linhas [3 a 6] => linha [7]<br>\n\nE a quantidade de linhas utilizadas para prever a seguinte (ex: [1 a 4] ) \u00e9 chamada de _**'Rolling Window'**_, e na fun\u00e7\u00e3o utilizada abaixo o Rolling Window ser\u00e1 chamado de _**'length'**_.\n\nLink para a documenta\u00e7\u00e3o do TimeseriesGenerator - Keras: <br>\n[TimeseriesGenerator Docs link](https:\/\/keras.io\/api\/preprocessing\/timeseries\/#timeseriesgenerator-class)","e83f5378":"### PEGADINHA DO DESAFIO:\nNo arquivo de submiss\u00e3o existem alguns produtos novos, que n\u00e3o foram vistos anteriormente pelo modelo.<br>\nA solu\u00e7\u00e3o usada nesse exemplo \u00e9 de simplesmente _**considerar as vendas desses novos produtos como 0**_\n\nEste \u00e9 um problema de se usar um modelo de Machine Learning no formato proposto aqui, onde n\u00e3o existe espa\u00e7o para adicionarmos uma nova feature....<br>\n\u00c9 um problema dentre outros diversos, que n\u00e3o se encaixam no escopo desse notebook.\n\n -- Aos interessados, sugiro ler sobre como utilizar outras features na previs\u00e3o, como as de clima e feriados, e como modelar a Pandemia de uma forma menos agressiva que s\u00f3 deletar os seus dias rs rs \n","a8bd804d":"## 6- Gera\u00e7\u00e3o das previs\u00f5es para os pr\u00f3ximos 14 dias\n#### obs: h\u00e1 uma 'pegadinha' no desafio, continue lendo","33e5f337":"## 3- Feature Engineering","34feffab":"## 5- Cria\u00e7\u00e3o e treino do Modelo de redes neurais \n### Usando uma camada Convolucional e uma camada LSTM (Long Short Term Memory)\n\n#### No c\u00f3digo abaixo, explico algumas fun\u00e7\u00f5es das camadas da rede, sendo que um * indica que aquele par\u00e2metro pode ser ajustado para mehorar os resultados.\n\nLinks das documenta\u00e7\u00f5es das camadas:\n\n1. Conv1D: [link](https:\/\/keras.io\/api\/layers\/convolution_layers\/convolution1d\/)\n2. Bidirectional: [link](https:\/\/keras.io\/api\/layers\/recurrent_layers\/bidirectional\/)\n3. LSTM: [link](https:\/\/keras.io\/api\/layers\/recurrent_layers\/lstm\/)\n4. Dropout: [link](https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/)\n5. Attention: [link](https:\/\/github.com\/philipperemy\/keras-attention-mechanism) _(esse \u00e9 diferente e vale a pena ler!)_\n6. Dense: [link](https:\/\/keras.io\/api\/layers\/core_layers\/dense\/)","ec509b9a":"### Abaixo criamos o dataset para a rede neural. \n### N\u00e3o utilizei uma base para valida\u00e7\u00e3o, pois obtive melhores resultados sem ela. Caso queira testar a valida\u00e7\u00e3o, fa\u00e7a o seguinte:<br>\n##### 1. Na linha 7, 'train_rows' = n_index _**\/\/ 100 * XX**_, onde XX indica o percentual da base usada para treino ;\n##### 2. Descomentar as linhas do 'test_gen'","4a1d70e0":"## 2- Carregando os arquivos .csv e algumas modifica\u00e7\u00f5es iniciais\n\n -- Deletaremos os dias de pandemia em que a loja 3 (ou era a loja 2?) ficou fechada, mais simples e serve para um notebook de exemplo!","e12e7cf6":"## 1- Importando bibliotecas de fun\u00e7\u00f5es"}}