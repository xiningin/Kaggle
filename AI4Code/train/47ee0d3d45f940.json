{"cell_type":{"35e5b07c":"code","2d01a46e":"code","6c0a1ab5":"code","c9a810fb":"code","113096fa":"code","d388fea3":"code","41c03154":"code","f1555a78":"code","19b1855c":"code","5dd5306b":"code","2bad524f":"code","a03d8117":"code","b83bb3ed":"code","7479ab8a":"code","b756950a":"code","59440850":"code","ab6efd72":"code","404f6019":"code","912c7f26":"code","c0e21e9d":"code","f6ee7f13":"code","490d1591":"code","0856c8df":"code","3758dbad":"code","17e2d6bd":"code","1bc80c6d":"code","474337c1":"code","0b176510":"code","d4a906f1":"code","16309536":"code","5d741aee":"code","4f8b2413":"code","5d26c35e":"code","546e400d":"code","ca46855d":"code","7b1bcca2":"code","380ebc03":"code","7e0ca10a":"code","1e14e826":"code","5c0cdb4d":"code","8a2ad599":"code","519bd12b":"code","291261ba":"code","416d5983":"code","884322ad":"code","ed542a95":"code","51bb9491":"code","aad2a8a1":"code","94608c99":"code","00901f4a":"code","eec8f8dc":"code","10074e19":"code","b856ac27":"code","f098c6c6":"code","4758bc8c":"code","2112f064":"code","35853026":"code","bdfb6dad":"code","0d5b723e":"code","451788e0":"code","050a6a84":"code","07b120b2":"code","30f47200":"code","3e699452":"markdown","17889f7f":"markdown","02b42b4b":"markdown","77d33e5f":"markdown","ad18c64e":"markdown","4d07ac92":"markdown","3ca987e7":"markdown","c0b65b94":"markdown","cdfee919":"markdown","01952510":"markdown","9da9c04a":"markdown","b6959a12":"markdown","469ae83c":"markdown","090f1d2a":"markdown","c6120847":"markdown","a7467128":"markdown","4c81cfe9":"markdown","f98d7f81":"markdown","6b618a3a":"markdown","f058a9e2":"markdown","c6937d42":"markdown","9533dc38":"markdown","a02364ab":"markdown","91c4d821":"markdown","dfe7e4f7":"markdown","30baa68e":"markdown","42b1f8ff":"markdown","fd5be130":"markdown","fb92da11":"markdown","8b5c496c":"markdown","f9ca6529":"markdown","b3467e3c":"markdown","0eae22ca":"markdown","e627397b":"markdown","c58ac56f":"markdown","0e1db9af":"markdown","f9347734":"markdown"},"source":{"35e5b07c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d01a46e":"import nltk","6c0a1ab5":"#Here we get ou data and create a list for messages \nmessages=[line.rstrip() for line in open(\"..\/input\/smsspamcollection\/SMSSpamCollection\")]\n","c9a810fb":"len(messages)","113096fa":"#We can reach any of the messages \nmessages[0]","d388fea3":"#we can also reach any nunch of messages we want\nfor message_no, message_con in enumerate(messages[:15]):\n    print(message_no, message_con )","41c03154":"#There is a tab separation between labes of the message and its content\nmessages[1000]\n","f1555a78":"messages=pd.read_csv(\"..\/input\/smsspamcollection\/SMSSpamCollection\", sep=\"\\t\", names=[\"Label\",\"Message\"])\nmessages.head()\n#Here we transform our data into a dataframe that we can use pandas tools and functions","19b1855c":"#We get overall statistical information about our data\nmessages.describe()","5dd5306b":"# we can also get statistical information label by label\nmessages.groupby(\"Label\").describe()","2bad524f":"messages[\"Length\"]=messages[\"Message\"].apply(len)","a03d8117":"messages.head()","b83bb3ed":"import seaborn as sns","7479ab8a":"sns.set_style(\"whitegrid\")","b756950a":"sns.distplot(messages[\"Length\"])","59440850":"import cufflinks as cf","ab6efd72":"cf.go_offline()","404f6019":"messages[\"Length\"].iplot(kind=\"histogram\")\n#When we move over the data we can interactively see the exact data at that point","912c7f26":"# we see from the histogram above that there is a message longer than 900\nmessages[\"Length\"].max()","c0e21e9d":"#Here we get the longest message\nmessages[messages[\"Length\"] == 910][\"Message\"].iloc[0]","f6ee7f13":"messages.hist(column=\"Length\",by=\"Label\",figsize=(8,6))\n","490d1591":"import string","0856c8df":"sample=\" This is just for trial before using the real data. Therefore, do not take into account that. Is it ok?\"\nsample","3758dbad":"no_punc=[word for word in sample if word not in string.punctuation]\nno_punc\n#Here I can get rid of punctuations via string.punctuation function","17e2d6bd":"#Here we get the original string without punctuation \nno_punc=\"\".join(no_punc)\nno_punc\n","1bc80c6d":"from nltk.corpus import stopwords","474337c1":"new_no_stopw=[word for word in no_punc.split() if word.lower() not in stopwords.words(\"english\")]","0b176510":"#Now I have a cleaner data without stopwords(very common words)\nnew_no_stopw","d4a906f1":"def pretext_processor(text):\n    no_punc=[word for word in text if word not in string.punctuation]\n    no_punc=\"\".join(no_punc)\n    no_stop=[word for word in no_punc.split() if word.lower() not in stopwords.words(\"english\")]\n    return no_stop","16309536":"pretext_processor(sample)\n#Our function works as we expected without any problem","5d741aee":"#Here we check our function with the first 5 rows\nmessages[\"Message\"].head().apply(pretext_processor)","4f8b2413":"from sklearn.feature_extraction.text import CountVectorizer","5d26c35e":"vectorized_messages=CountVectorizer(analyzer=pretext_processor).fit(messages[\"Message\"])","546e400d":"print(len(vectorized_messages.vocabulary_))\n#we have 11425 word in our vocabulary of the messages","ca46855d":"message9=messages[\"Message\"][8]\nmessage9","7b1bcca2":"vectorized_message9=vectorized_messages.transform([message9])\nprint(vectorized_message9)\n#This the numeric or vectoral representation of the single message ","380ebc03":"vectorized_messages.get_feature_names()[5275]\n# we can also recheck which number represent which word\n#For example 5275 in the message 8 represenst the word \"call\"","7e0ca10a":"vectorized_message9.shape","1e14e826":"transformed_messages=vectorized_messages.transform(messages[\"Message\"])","5c0cdb4d":"transformed_messages.shape\n#Our vectorized messages has 5572 rows and 11425 columns","8a2ad599":"transformed_messages.nnz\n# this return non zero occurances as 50548","519bd12b":"from sklearn.feature_extraction.text import TfidfTransformer","291261ba":"tfidf=TfidfTransformer().fit(transformed_messages)\n#Here we fit the data into the TfidfTransformer","416d5983":"tfidf8=tfidf.transform(vectorized_message9)\n#Here we transform the 8th message to see how it works","884322ad":"print(tfidf8)# Here we can see the importance of the words word by word inside entire message","ed542a95":"vectorized_messages.get_feature_names()[9253] \n# Here we get the name of the most important word in message 8 according to the idf table we have before","51bb9491":"tfidf.idf_[vectorized_messages.vocabulary_[\"increase\"]]","aad2a8a1":"messages_tfidf=tfidf.transform(transformed_messages)","94608c99":"from sklearn.naive_bayes import MultinomialNB","00901f4a":"spam_detect_model= MultinomialNB().fit(messages_tfidf, messages[\"Label\"])","eec8f8dc":"spam_detect_model.predict(tfidf8)","10074e19":"messages[\"Label\"][8]\n#Here we check our model's prediction with the original data, it seems that it predicts true as spam","b856ac27":"#Now we get all the predictions of our model and assign it into a variable called all_predictions\nall_predictions=spam_detect_model.predict(messages_tfidf)","f098c6c6":"all_predictions","4758bc8c":"from sklearn.model_selection import train_test_split","2112f064":"X=messages[\"Message\"]\ny=messages[\"Label\"]","35853026":"X_train,X_test,y_train, y_test= train_test_split(X,y)","bdfb6dad":"from sklearn.pipeline import Pipeline","0d5b723e":"pipeline=Pipeline([(\"bow\",CountVectorizer(analyzer=pretext_processor)),(\"tfidf\",TfidfTransformer()),(\"classifier\", MultinomialNB())])","451788e0":"pipeline.fit(X_train,y_train)","050a6a84":"predictions=pipeline.predict(X_test)\npredictions","07b120b2":"from sklearn.metrics import classification_report, confusion_matrix","30f47200":"print(classification_report(y_test,predictions))\nprint(\"*************************\")\nprint(confusion_matrix(y_test,predictions))","3e699452":"Term frequency-inverse document frequency is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.","17889f7f":"Example:\n\nConsider a document containing 100 words wherein the word \"house\" appears 10 times.\n\nThe term frequency (i.e., tf) for house is then (10 \/ 100) = 0.1. Now, assume we have 10 million documents and the word house appears in one hundre of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 \/ 1,00) = 5. Thus, the Tf-idf weight is the product of these quantities: 0.1 * 5 = 0.5.\n\n","02b42b4b":"We can check whether our function works properly or not","77d33e5f":"# 4. Vectorization of the Data","ad18c64e":"We can also see the importance of a word as follows:\n\nHere we see the tfidf score of the word \"increase\"","4d07ac92":"# 3. Text Preprocessing:\nwe have to convert the raw messages (sequence of characters) into vectors (sequences of numbers) before implemeting the NLP algorithm","3ca987e7":"we can also check the shape of the our messages after vectorization as follows:","c0b65b94":"We will just insert a list of we have done into step parameter in Pipeline","cdfee919":"It seems that our model predicts very good, there are only 53 messages that our model predicted fail as False Negative","01952510":"> Now we have gotten our model's predictions and compare result with actual labels with classification report","9da9c04a":"Pipeline(steps, *, memory=None, verbose=False)\n |  \n |  Pipeline of transforms with a final estimator.\n |  \n |  Sequentially apply a list of transforms and a final estimator.\n |  Intermediate steps of the pipeline must be 'transforms', that is, they\n |  must implement fit and transform methods.\n |  The final estimator only needs to implement fit.","b6959a12":"4.2. *Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n","469ae83c":"Here we splitted data into train and test sets and instead of doing all of these transformations we have done until now, we will use sklearn pipeline to spend more time","090f1d2a":"In order to understand how good our model predict, we need to split our data as train set and test set","c6120847":"4.1.Count how many times does a word occur in each message (Known as term frequency)\n\n\n","a7467128":"Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n\nIDF(t) = log_e(Total number of documents \/ Number of documents with term t in it).","4c81cfe9":"This represents the number of times a word appears in a document, divided by the total number of words in that document\n\nTerm Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n\nTF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document).","f98d7f81":"# 5. Training A Model:","6b618a3a":"# 1. Importing Libraries and Data","f058a9e2":"In order to get rid of the tab and create a clean dataset we will use pandas","c6937d42":"Now we will transform the entire vectorized words into TF-IDF as follows:","9533dc38":"Now we can test how the vectorized messages look in a single message","a02364ab":"Above we see that the messages include a label and their content","91c4d821":"Now our data has fitted to the ml algorithm and we will check how it predicts based on a single message","dfe7e4f7":"As we can see above this message includes 18 unique word all of which appear only one time in entire message","30baa68e":"4.3. *Normalize the vectors to unit length, to abstract from the original text length (L2 norm)","42b1f8ff":"Now we will create a function that do all of these operations for our real data","fd5be130":"Now we will tranform our data into vectors so that we can apply our NLP algorithm in three steps using the bag-of-words model:\n\n","fb92da11":"We will create new columns in order to get a better understanding of the data:","8b5c496c":"# 2. Exploratory Data Analysis and Feature Engineering:","f9ca6529":"we can get better and interactive visualizations of our data via Cufflinks ","b3467e3c":"Now we can make some visualization to get deeper understanding:","0eae22ca":"From the figure above we understand that spam messages tends to be longer than ham messages and spam messages are centered aroun 150 characters, and this is a good sign to detect SPAM messages versus HAM messages","e627397b":"Above we can see the distribution of the length of the messages","c58ac56f":"Now we will get rid of stop words:","0e1db9af":"MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)\n |  \n |  Naive Bayes classifier for multinomial models\n |  \n |  The multinomial Naive Bayes classifier is suitable for classification with\n |  discrete features (e.g., word counts for text classification). The\n |  multinomial distribution normally requires integer feature counts. However,\n |  in practice, fractional counts such as tf-idf may also work.","f9347734":"we will get rid of punctiation by using a sample before using it in the actual data"}}