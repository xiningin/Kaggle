{"cell_type":{"d4438c6a":"code","2585f805":"code","bb909b96":"code","e07e3ec1":"code","8b78506e":"code","ae3bdf61":"code","a68f7227":"code","1d5f8fe1":"code","37981602":"code","b9371e23":"code","997525b0":"markdown","5da14c4f":"markdown","77a4e120":"markdown","4d44cb0d":"markdown"},"source":{"d4438c6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xml.etree.ElementTree as ET\nimport matplotlib.pyplot as plt, zipfile\nfrom PIL import Image\nimport time\nfrom numpy import expand_dims\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2585f805":"ComputeLB = True\nDogsOnly = True\n\nimport numpy as np, pandas as pd, os\nimport xml.etree.ElementTree as ET \nimport matplotlib.pyplot as plt, zipfile \nfrom PIL import Image \n\nROOT = '..\/input\/stanford-dogs-dataset\/'\nif not ComputeLB: ROOT = '..\/input\/'\nIMAGES = os.listdir(ROOT + 'images\/Images\/')\nbreeds = os.listdir(ROOT + 'annotations\/Annotation\/') \n\nidxIn = 0; namesIn = []\nimagesIn = np.zeros((25000,64,64,3))\n\n# CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n# https:\/\/www.kaggle.com\/paulorzp\/show-annotations-and-breeds\nif DogsOnly:\n    for breed in breeds:\n        #print(breed)\n        for dog in os.listdir(ROOT+'annotations\/Annotation\/'+breed):\n            try: img = Image.open(ROOT+'images\/Images\/'+ breed + '\/' +dog+'.jpg')\n            except: continue           \n            tree = ET.parse(ROOT+'annotations\/Annotation\/'+breed+'\/'+dog)\n            root = tree.getroot()\n            objects = root.findall('object')\n            for o in objects:\n                bndbox = o.find('bndbox') \n                xmin = int(bndbox.find('xmin').text)\n                ymin = int(bndbox.find('ymin').text)\n                xmax = int(bndbox.find('xmax').text)\n                ymax = int(bndbox.find('ymax').text)\n                w = np.min((xmax - xmin, ymax - ymin))\n                img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n                img2 = img2.resize((64,64), Image.ANTIALIAS)\n                #hay una foto que tiene shape (64,64,4)\n                #para chequear si se tiene shape indicado, hay una foto que tiene shape 64,64,4\n                if(np.asarray(img2).shape == (64,64,3)):\n                    imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                    imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                    namesIn.append(breed)\n                    idxIn += 1\n                else:\n                    #foto no tiene shape 64,64,3\n                    None\n                #if idxIn%1000==0: print(idxIn)\n    idx = np.arange(idxIn)\n    np.random.shuffle(idx)\n    imagesIn = imagesIn[idx,:,:,:]\n    namesIn = np.array(namesIn)[idx]\n    \n# RANDOMLY CROP FULL IMAGES\nelse:\n    x = np.random.choice(np.arange(25000),10000)\n    for k in range(len(x)):\n        img = Image.open(ROOT + 'images\/Images\/' + IMAGES[x[k]])\n        w = img.size[0]\n        h = img.size[1]\n        sz = np.min((w,h))\n        a=0; b=0\n        if w<h: b = (h-sz)\/\/2\n        else: a = (w-sz)\/\/2\n        img = img.crop((0+a, 0+b, sz+a, sz+b))  \n        img = img.resize((64,64), Image.ANTIALIAS)\n        imagesIn[idxIn,:,:,:] = np.asarray(img)\n        namesIn.append(IMAGES[x[k]])\n        if idxIn%1000==0: print(idxIn)\n        idxIn += 1\n    \n# DISPLAY CROPPED IMAGES\nprint('value of idxIn: ',idxIn)\nx = np.random.randint(0,idxIn,25)\nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n        plt.axis('off')\n        if not DogsOnly: plt.title(namesIn[x[k*5+j]],fontsize=11)\n        else: plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n        plt.imshow(img)\n    plt.show()","bb909b96":"daImages = imagesIn[10000:12000,:,:,:]\ndatagen = ImageDataGenerator(horizontal_flip=True)\nit = datagen.flow(daImages, batch_size=1, shuffle=False)\n\ndaArray = np.ones((1,64,64,3))\n\n#agregar a imagesIn las nuevas imagenes\nfor k in range(2000):\n    print('k: ',k)\n    batch = it.next()\n    image = batch[0]\n    image = np.expand_dims(image,axis=0)\n    imagesIn = np.append(imagesIn, image, axis=0)\n\nprint(imagesIn.shape)","e07e3ec1":"from keras.models import Model, Sequential\nfrom keras.layers import Input,Dropout,Activation, Conv2DTranspose,Dense, Conv2D, Reshape, Flatten, concatenate, UpSampling2D, BatchNormalization, LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.optimizers import SGD, Adam\nfrom keras.initializers import RandomNormal","8b78506e":"#Generator\ndog = Input((100,))\nx = Dense(2048,activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.02, seed=None))(dog)\nx = Reshape((4,4,128))(x)\n#0.02 standard deviation\nx = Conv2DTranspose(512, kernel_size=5, use_bias=False,padding='same', kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None), strides=(1,1))(x)\n#add dropout\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Conv2DTranspose(256, kernel_size=5, use_bias=False,padding='same', kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None), strides= (2,2))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Conv2DTranspose(128, kernel_size=5, use_bias=False,padding='same', kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None), strides=(2,2))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Conv2DTranspose(64, kernel_size=5, use_bias=False,padding='same', kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None), strides=(2,2))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Conv2DTranspose(32, kernel_size=5, use_bias=False,padding='same',kernel_initializer=RandomNormal(mean=0.0, stddev=0.02, seed=None), strides=(2,2))(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Conv2DTranspose(3, kernel_size=5, activation='tanh', padding='same', kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None), strides=(1,1))(x)\n\ngenerator = Model(dog, x)\ngenerator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')\ngenerator.summary()","ae3bdf61":"#Discriminator\ninp = Input((12288,))\nx = Reshape((64,64,3))(inp)\n\n#x = Conv2D(128, (2,2), use_bias=False, activation='relu')(inp)\nx = Conv2D(64, strides=(1,1),kernel_size=5, padding='same',use_bias=False, kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None))(x)\nx = LeakyReLU(alpha=0.2)(x)\nx = Conv2D(64, kernel_size=5, strides=(2,2), padding='same', use_bias=False, kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None))(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.2)(x)\nx = Conv2D(128, kernel_size=4, strides=(2,2), padding='same', use_bias=False, kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None))(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.2)(x)\nx = Conv2D(256, kernel_size=4, strides=(2,2), padding='same', use_bias=False, kernel_initializer= RandomNormal(mean=0.0, stddev=0.02, seed=None))(x)\nx = BatchNormalization()(x)\nx = LeakyReLU(alpha=0.2)(x)\nx = Flatten()(x)\nx = Dense(1, activation='sigmoid')(x)\n\ndiscriminator = Model(inp, x)\n\ndiscriminator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')\ndiscriminator.summary()","a68f7227":"#Gan recibe input, codigo de 100 valores\ninp = Input(shape=(100,))\n# se emplea el generador para generar una foto\nx = generator(inp)\n#la foto se pone en forma de 12288 para pasarla por discriminador\nx = Reshape((12288,))(x)\n#el discriminador determina si la foto es perro o no (discriminador tiene entrenamiento)\n#discriminator.trainable = False\ndiscriminator.trainable=False\ngan_output= discriminator(x)\n\n#modelo recibe codigo y devuelve los resultados de si ese codigo genera foto de perro o no\ngan = Model(inp, gan_output)\ngan.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')\ngan.summary()","1d5f8fe1":"num_epochs = 60","37981602":"#entrenar 50 veces con toda la data\n#50 epochs. \n\n\nindexOfData= 128\nstart = time.time()\nfor j in range(num_epochs):\n    print(' ############################### GAN Epoch ############################### :',j+1)\n    \n    #188 steps to complete an epoch\n    for d in range(188):\n        print('step: ',d+1)\n        print('Training Discriminator')\n        #128 imagenes\n        x_real = ((imagesIn[indexOfData-128:indexOfData,:,:,:]-127.5)\/127.5).reshape(-1,12288)\n        #aplicando label smoothing a labels\n        y_real = np.random.uniform(low=0.7, high=1, size=(128,1))\n        #y_real = np.ones((22125,1))\n    \n        if d==187:\n            indexOfData=128\n        else:\n            indexOfData=indexOfData+128\n        \n        #128 codigos random de 100\n        noises = np.random.rand(128,100)\n        x_fake = generator.predict(x=noises).reshape(-1,12288)\n        #y_fake = np.zeros((128,1))\n        #aplicando label smoothing a labels\n        y_fake = np.random.uniform(low=0, high=0.3, size=(128,1))\n\n\n        #training the with the real data\n        #h1 = discriminator.fit(x=x_real, y=y_real, batch_size=128, epochs=1)\n        h1 = discriminator.train_on_batch(x=x_real, y=y_real)\n        #training with fake data from the generator\n        print('loss on real images: ',h1)\n        #h2 = discriminator.fit(x=x_fake, y=y_fake, batch_size=128, epochs=1)\n        h2 = discriminator.train_on_batch(x=x_fake, y=y_fake)\n        print('loss on fake images: ',h2)\n\n        code2 = np.random.rand(128,100)\n        #y_trick = np.ones((7000,1))\n        #label smoothing\n        y_trick = np.random.uniform(low=0.7, high=1, size=(128,1))\n\n        print('Training GAN (Generator)')\n        #h3 = gan.fit(x=code2, y=y_trick, batch_size=128, epochs=1)\n        h3 = gan.train_on_batch(x=code2, y=y_trick)\n        print('loss on generator: ',h3)\n    \n\nend = time.time()\n\ntotalTime = end-start\ntotalTime = totalTime\/60\n\nprint('total elapsed time: ' + str(totalTime) +' minutes')","b9371e23":"result = generator.predict_on_batch(x=np.random.rand(25,100))\n\nprint(result.shape)\n\n#denormalizando desde [-1,1] a [0,255]\nresult = (result*127.5)+(127.5)\nkk=0\nfor k in range(5):\n  plt.figure(figsize=(15,3))\n  for j in range(5):\n    plt.subplot(1,5,j+1)\n    img = Image.fromarray(result[kk,:,:,:].astype('uint8'))\n    plt.axis('off')\n    plt.imshow(img)\n    kk=kk+1\nplt.show()","997525b0":"Extraer fotos y guardarlas en imagenIn de shape (22125,64,64,3)","5da14c4f":"En namesOfDogs guardar los labels, los nombres de las razas","77a4e120":"GAN","4d44cb0d":"Crear Generador"}}