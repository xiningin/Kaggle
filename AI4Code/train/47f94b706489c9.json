{"cell_type":{"76a7aa32":"code","ef7d7b1e":"code","9991048f":"code","5587dc7e":"code","91f12600":"code","596cb7af":"code","b2aedf8f":"code","be266fcb":"code","c93baf12":"code","5bb9dff6":"code","25b3cf35":"code","4044d6c9":"code","c65dfb2b":"code","b82ff017":"code","77c453a6":"code","debe889d":"code","4feb4bae":"code","733b50fa":"code","6e53e2c7":"code","be4ddcb7":"code","0abb9ab4":"code","17108b76":"code","3166a581":"markdown","b060ddf0":"markdown","9c87bdbb":"markdown","00e6f8d8":"markdown","c1e20128":"markdown","cd0ea9a4":"markdown","ae6e8b7f":"markdown","64a1b925":"markdown","66f4154c":"markdown","f50b9fb4":"markdown","68f5efb7":"markdown","d54313e3":"markdown"},"source":{"76a7aa32":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ef7d7b1e":"data = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")\ndata.head()","9991048f":"data.info()","5587dc7e":"categorical = data.select_dtypes('object').columns\nnumerical = data.drop(['HeartDisease'], axis=1).select_dtypes('number').columns","91f12600":"sns.countplot(x='HeartDisease', data = data)","596cb7af":"data[numerical].describe()","b2aedf8f":"fig = plt.figure(figsize=(25,30)) #figure size\na = 4  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\n\n\nfor column in numerical:\n    plt.subplot(a, b, c)\n    sns.histplot(x=column, data=data, color='darkred')\n    c+=1\n    \nplt.tight_layout()\nplt.show()","be266fcb":"skews = data[numerical].skew()\nskews","c93baf12":"data.corr()['HeartDisease'].sort_values(ascending=False)[1:]","5bb9dff6":"data[categorical].nunique()","25b3cf35":"fig = plt.figure(figsize=(15,10)) #figure size\na = 2  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\n\n#ploting categorical features\nfor column in categorical:\n    plt.subplot(a, b, c)\n    sns.barplot(x=column, y=data['HeartDisease'], data=data, palette='rocket')\n    c+=1\n    \nplt.tight_layout()\nplt.show()","4044d6c9":"#catboost\nfrom catboost import CatBoostClassifier\n\n#hyperparameter tuning\nimport optuna\n#optuna.logging.set_verbosity(0)\n\n#scoring tools\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report","c65dfb2b":"#spliting data_train \nX = data.drop(['HeartDisease'], axis=1)\ny = data['HeartDisease']\n\n#spliting data into 5 fold \ncv = KFold(n_splits=5, random_state=22, shuffle=True)","b82ff017":"#catboost with default parameters\ncat = CatBoostClassifier(cat_features=['FastingBS', 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina','ST_Slope'], verbose=0)  \nscores = cross_val_score(cat, X, y, cv=cv, scoring=\"accuracy\")\nprint(f'Accuracy with default parameters: {round(scores.mean(), 4)}')","77c453a6":"#optuna optimization\ndef objective(trial):\n\n    #parameter range\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n    }\n    \n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n        \n    model = CatBoostClassifier(**param, cat_features=['FastingBS', 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina','ST_Slope'], verbose=0)  \n    \n    scores = cross_val_score(\n        model, X, y, cv=cv,\n        scoring=\"accuracy\"\n    )\n    \n    return scores.mean()\n\n#optimazing to maximize accuracy in 100 trials\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)","debe889d":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n\n#redefining Catboost with the best trial parameters\ncat = CatBoostClassifier(**study.best_trial.params, cat_features=['FastingBS', 'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina','ST_Slope'], verbose=0)","4feb4bae":"optuna.visualization.plot_optimization_history(study)","733b50fa":"print(f'Accuracy after optimization: {round(study.best_value, 4)}')","6e53e2c7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ncat.fit(X_train, y_train)","be4ddcb7":"pred = cat.predict(X_test)\nprint(f'Accuracy: {round(accuracy_score(y_test, pred),4)}\\n')\nprint(classification_report(y_test, pred))","0abb9ab4":"#creating a feature importances data frame\nfeature_importance = np.array(cat.get_feature_importance())\nfeatures = np.array(X_train.columns)\ndf_importances = pd.DataFrame({'Features':features,'Feature importance':feature_importance})\n\n#sorting values\ndf_importances.sort_values(by=['Feature importance'], ascending=False, inplace=True)\n\n#barplot\nfig = sns.barplot(x='Feature importance', y='Features', data = df_importances, palette='rocket')\nplt.show()","17108b76":"df_importances.set_index('Features', drop=True)","3166a581":"We have no null values. There are 5 categorical and 6 numerical features excluding the target","b060ddf0":"CatBoost is a very powerfull algorithm for gradient boosting on decision trees. Compared to other similar popular algorithms like XGBoost, Random Forest, LightGBM etc. it can handle categorical features directly without encoding and it has simpler hyperparameter tuning process. In this kernel we will test Catboost and optimize it with Optuna. \n\nCatboost model gave us very good accuracy results compared to the other models. It made modeling much faster since it does not require converting categotical features, scaling or nomalization.\n\nIn this kernel we focused only on modeling. To improve accuracy score we would need to remove outliers and do more data analysis. This will be done in another kernel.","9c87bdbb":"Data is balanced","00e6f8d8":"#### Features description\n- Age: Age of the patient [years]\n- Sex: Sex of the patient [M: Male, F: Female]\n- ChestPainType: [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n- RestingBP: Resting blood pressure [mm Hg]\n- Cholesterol: Serum cholesterol [mm\/dl]\n- FastingBS: Fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n- RestingECG: Resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n- MaxHR: Maximum heart rate achieved [Numeric value between 60 and 202]\n- ExerciseAngina: Exercise-induced angina [Y: Yes, N: No]\n- Oldpeak: ST [Numeric value measured in depression] \n- ST_Slope: The slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n- HeartDisease: Output class [1: heart disease, 0: Normal]","c1e20128":"## Categorical Features","cd0ea9a4":"## Numerical Features","ae6e8b7f":"# Exploratory Data Analysis","64a1b925":"# Introduction","66f4154c":"## Target Feature","f50b9fb4":"# Heart Failure Prediction with CatBoost","68f5efb7":"# CatBoost","d54313e3":"Small skew values meaning normal like distribution. Normalization is not needed. Anyway, CatBoost does not require data normalization."}}