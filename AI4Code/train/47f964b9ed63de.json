{"cell_type":{"e01d9151":"code","ddcbc785":"code","bf732e0a":"code","d9ad59a3":"code","e9a7a26f":"code","d7b1b330":"code","60dba907":"code","5e63472a":"code","4506b169":"code","e73d3f8b":"code","d46466e1":"code","35ebf6dc":"code","48e48d4b":"code","6bbe52e7":"code","e18433ca":"code","bfb8855d":"code","e0a9912b":"code","edcbc0e0":"code","b387e035":"code","48c17a65":"code","20a50c3f":"code","8ea27ab2":"code","90ea615b":"code","e0fb705c":"code","6608f39a":"code","4a9cd556":"code","5b6fb964":"code","04c0cffa":"code","baca8487":"code","70ba5fd3":"code","916752be":"code","2d8cabd7":"code","e8b19ff6":"code","259f39fd":"code","ac790b02":"code","2b2ab4ed":"code","565d44e1":"code","e5b0b1af":"code","b8c99d01":"code","97f775b7":"code","8b1748cd":"code","47752dd3":"code","83630174":"code","b7e50953":"code","e58d69e2":"code","5d909c95":"code","3bcbe708":"code","24e81838":"code","411b1cc3":"code","1a1fbc66":"code","cc53311a":"code","ccde6ec1":"code","3d7ad89d":"code","e047911f":"code","48285ed9":"code","d6b4662b":"code","0df4c2a0":"code","0b3808bf":"code","e748cc9d":"code","fb2efc97":"code","ba17cb11":"code","bd513421":"code","743e935a":"code","bc8187bd":"code","2fc1ea59":"code","2d4372e3":"code","b70c2fcf":"code","c9d0e329":"code","f1b2e8b5":"code","278459a5":"code","c78fe806":"code","0a7be3a3":"code","baf7b3e4":"code","b20adc2b":"code","f1c71fe7":"code","97f54494":"code","ad3b8eed":"code","03834b0d":"code","5b708ac9":"code","fb248898":"code","49d0ed72":"code","6c34f141":"code","95e30fc6":"code","9f61e220":"code","cf3868fb":"code","e406ad94":"code","95402a4d":"markdown","18ff1a16":"markdown","252843f1":"markdown","cd4e26d0":"markdown","3ee72753":"markdown","662f1845":"markdown","2e1a8445":"markdown","6994deb3":"markdown","944a3a33":"markdown","fd43d4c7":"markdown","c262a702":"markdown","738f0fa3":"markdown","7f9eda81":"markdown","b92d8eee":"markdown","10526ce5":"markdown","75b76d39":"markdown","7d13e29f":"markdown","eafa0485":"markdown","51d431d6":"markdown","fb5ec49e":"markdown","3a9a4874":"markdown","6766792a":"markdown","4dcd2d75":"markdown","69b52847":"markdown","78b20fb7":"markdown","6ac7a323":"markdown","1ce09c1f":"markdown","d4c45705":"markdown","a62fdee2":"markdown","23f58791":"markdown","74ee57d3":"markdown"},"source":{"e01d9151":"base_path = \"~\/.kaggle\/competitions\/avito-demand-prediction\/\"\ntrain_image_base_path = base_path + \"data\/competition_files\/train_jpg\/\"\ntest_image_base_path = base_path + \"data\/competition_files\/test_jpg\/\"","ddcbc785":"import time\n\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.cross_validation import KFold\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom nltk.corpus import stopwords \n\nimport matplotlib.pyplot as plt\nimport re\nimport string","bf732e0a":"NFOLDS = 5\nSEED = 42\n\nclass TargetEncoder:\n    # Adapted from https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\n    def __repr__(self):\n        return 'TargetEncoder'\n\n    def __init__(self, cols, smoothing=1, min_samples_leaf=1, noise_level=0, keep_original=False):\n        self.cols = cols\n        self.smoothing = smoothing\n        self.min_samples_leaf = min_samples_leaf\n        self.noise_level = noise_level\n        self.keep_original = keep_original\n\n    @staticmethod\n    def add_noise(series, noise_level):\n        return series * (1 + noise_level * np.random.randn(len(series)))\n\n    def encode(self, train, test, target):\n        for col in self.cols:\n            if self.keep_original:\n                train[col + '_te'], test[col + '_te'] = self.encode_column(train[col], test[col], target)\n            else:\n                train[col], test[col] = self.encode_column(train[col], test[col], target)\n        return train, test\n\n    def encode_column(self, trn_series, tst_series, target):\n        temp = pd.concat([trn_series, target], axis=1)\n        # Compute target mean\n        averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n        # Compute smoothing\n        smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - self.min_samples_leaf) \/ self.smoothing))\n        # Apply average function to all target data\n        prior = target.mean()\n        # The bigger the count the less full_avg is taken into account\n        averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n        averages.drop(['mean', 'count'], axis=1, inplace=True)\n        # Apply averages to trn and tst series\n        ft_trn_series = pd.merge(\n            trn_series.to_frame(trn_series.name),\n            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n            on=trn_series.name,\n            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n        # pd.merge does not keep the index so restore it\n        ft_trn_series.index = trn_series.index\n        ft_tst_series = pd.merge(\n            tst_series.to_frame(tst_series.name),\n            averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n            on=tst_series.name,\n            how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n        # pd.merge does not keep the index so restore it\n        ft_tst_series.index = tst_series.index\n        return self.add_noise(ft_trn_series, self.noise_level), self.add_noise(ft_tst_series, self.noise_level)\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n        if(seed_bool == True):\n            params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n        \ndef get_oof(clf, x_train, y, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        print('\\nFold {}'.format(i))\n        x_tr = x_train[train_index]\n        y_tr = y[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","d9ad59a3":"training = pd.read_csv(base_path + 'train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\ntesting = pd.read_csv(base_path + 'test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n\ntesting_users = testing[\"user_id\"]\ntraining_users = training[\"user_id\"]\n\n\ntraindex = training.index\ntestdex = testing.index\n\nntrain = training.shape[0]\nntest = testing.shape[0]\n\ny = training.deal_probability.copy()\ntraining.drop(\"deal_probability\",axis=1, inplace=True)\n\ndf = pd.concat([training, testing],axis=0)\ndel training, testing\ngc.collect()","e9a7a26f":"df[\"price\"] = np.log(df[\"price\"] + 0.001)\ndf[\"price_empty\"] = df[\"price\"].isnull().astype(np.float)\ndf[\"price\"].fillna(df[\"price\"].mean(skipna=True), inplace=True)","d7b1b330":"df[\"Weekday\"] = df['activation_date'].dt.weekday","60dba907":"df[\"image_top_1\"].fillna(-999,inplace=True)\ndf[\"image_top_1\"] = df[\"image_top_1\"].apply(lambda x: int(round(x)))","5e63472a":"def make_col_from_dict(x, y, default = -1.0):\n    l = np.zeros((len(x)))\n    for r, i in enumerate(x.values):\n        l[r] = y.get(tuple(i), default)\n    return pd.Series(l, index=x.index)\n\ndef make_col_from_dict_untupled(x, y, default = -1.0):\n    l = np.zeros((len(x)))\n    for r, i in enumerate(x.values):\n        l[r] = y.get(i, default)\n    return pd.Series(l, index=x.index)","4506b169":"# Raw price features \ncat_params = [\"parent_category_name\", \"category_name\", \"image_top_1\"]\nfor extra_columns in [[], [\"region\"], [\"city\"]]:\n    for col in cat_params:\n        name_prefix = \"_\".join([col] + extra_columns)\n        \n        print(name_prefix)\n        means = df.groupby([col] + extra_columns)[\"price\"].mean().to_dict()\n        medians = df.groupby([col] + extra_columns)[\"price\"].median().to_dict()\n        counts = df.groupby([col] + extra_columns)[\"price\"].count().to_dict()\n        var = df.groupby([col] + extra_columns)[\"price\"].var().to_dict()\n\n        df[name_prefix + \"_mean_price\"] = make_col_from_dict(df[[col] + extra_columns], means)\n        df[name_prefix + \"_median_price\"] = make_col_from_dict(df[[col] + extra_columns], medians)\n        df[name_prefix + \"_counts\"] = make_col_from_dict(df[[col] + extra_columns], counts)\n        df[name_prefix + \"_var_price\"] = make_col_from_dict(df[[col] + extra_columns], var).fillna(0.0) # Var may have zero variance if there is single row","e73d3f8b":"def get_user_feature_map():\n    # Loading data\n    used_cols = [\"item_id\", \"user_id\", \"parent_category_name\", \"category_name\", \"price\"]\n\n    train = pd.read_csv(base_path + 'train.csv', usecols=used_cols)\n    train_active = pd.read_csv(base_path + 'train_active.csv', usecols=used_cols)\n    test = pd.read_csv(base_path + 'test.csv', usecols=used_cols)\n    test_active = pd.read_csv(base_path + 'test_active.csv', usecols=used_cols)\n\n    all_samples = pd.concat([\n        train,\n        train_active,\n        test,\n        test_active\n    ]).reset_index(drop=True)\n    all_samples.drop_duplicates(['item_id'], inplace=True)\n\n    del train_active, test_active, train, test\n    gc.collect()\n\n    train_periods = pd.read_csv(base_path + 'periods_train.csv', parse_dates=['date_from', 'date_to'])\n    test_periods = pd.read_csv(base_path + 'periods_test.csv', parse_dates=['date_from', 'date_to'])\n\n    all_periods = pd.concat([\n        train_periods,\n        test_periods\n    ])\n\n    del train_periods\n    del test_periods\n    gc.collect()\n\n    # Calculating how many times \/ for how long each item_id was put on site\n    all_periods['days_up'] = all_periods['date_to'].dt.dayofyear - all_periods['date_from'].dt.dayofyear\n    gp = all_periods.groupby(['item_id'])[['days_up']]\n    item_up_features = pd.DataFrame()\n    item_up_features['days_up_sum'] = gp.sum()['days_up']\n    item_up_features['times_put_up'] = gp.count()['days_up']\n    item_up_features.reset_index(inplace=True)\n    item_up_features.rename(index=str, columns={'index': 'item_id'})\n\n    all_periods.drop_duplicates(['item_id'], inplace=True)\n    all_periods = all_periods.merge(item_up_features, on='item_id', how='left')\n    all_periods = all_periods.merge(all_samples, on='item_id', how='left')\n\n    gp = all_periods.groupby(['user_id'])[['days_up_sum', 'times_put_up']].mean().reset_index() \\\n    .rename(index=str, columns={\n        'days_up_sum': 'avg_days_up_user',\n        'times_put_up': 'avg_times_up_user'\n    })\n\n    n_user_items = all_samples.groupby(['user_id'])[['item_id']].count().reset_index() \\\n    .rename(index=str, columns={\n        'item_id': 'n_user_items'\n    })\n    gp = gp.merge(n_user_items, on='user_id', how='outer')\n\n    user_features = gp.groupby(\"user_id\").mean().to_dict()\n    return user_features","d46466e1":"if os.path.isfile(\"user_features.pickle\"):\n    print(\"Loading cached user features\")\n    with open(\"user_features.pickle\", \"rb\") as f:\n        user_features = pickle.load(f)\nelse:\n    print(\"Calculating and caching user features\")\n    user_features = get_user_feature_map()\n    with open(\"user_features.pickle\", \"wb\") as f:\n        pickle.dump(user_features, f)","35ebf6dc":"for x in [\"avg_days_up_user\", \"avg_times_up_user\", \"n_user_items\"]:\n    df[x] = make_col_from_dict_untupled(df['user_id'], user_features[x])\n    df[x].fillna(-1.0, inplace=True)","48e48d4b":"del user_features\ngc.collect()","6bbe52e7":"df_active = pd.read_csv(base_path + \"train_active.csv\", parse_dates=[\"activation_date\"], \n                        usecols=[\"region\", \"city\", \"category_name\", \"item_id\", \"activation_date\"])\ndf_active.set_index(\"item_id\", inplace=True)","e18433ca":"df_active_test = pd.read_csv(base_path + \"test_active.csv\", parse_dates=[\"activation_date\"], \n                             usecols=[\"region\", \"city\", \"category_name\", \"item_id\", \"activation_date\"])\ndf_active_test.set_index(\"item_id\", inplace=True)","bfb8855d":"df_active = pd.concat([df_active, df_active_test])\ndel df_active_test","e0a9912b":"df_periods = pd.read_csv(base_path + \"periods_train.csv\", parse_dates = [\"date_from\", \"date_to\"],\n                        usecols = [\"item_id\", \"date_from\", \"date_to\"])\ndf_periods.set_index(\"item_id\", inplace=True)","edcbc0e0":"df_periods_test = pd.read_csv(base_path + \"periods_test.csv\", parse_dates = [\"date_from\", \"date_to\"],\n                        usecols = [\"item_id\", \"date_from\", \"date_to\"])\ndf_periods_test.set_index(\"item_id\", inplace=True)","b387e035":"df_periods = pd.concat([df_periods, df_periods_test])\ndel df_periods_test","48c17a65":"joined = df_active.join(df_periods)\ndel df_periods, df_active\ngc.collect()","20a50c3f":"joined.reset_index(inplace=True)","8ea27ab2":"joined[\"date_diff\"] = joined[\"date_to\"].dt.dayofyear - joined[\"date_from\"].dt.dayofyear","90ea615b":"repeats = joined.groupby(\"item_id\")[\"region\"].count().to_dict()\njoined[\"repeats\"] = joined[\"item_id\"].apply(lambda x: repeats.get(x))","e0fb705c":"from scipy.stats import describe\njoined[\"date_diff\"].fillna(-1.0, inplace=True)","6608f39a":"with open(\"date_features.pickle\", \"wb\") as f:\n    pickle.dump(joined, f)","4a9cd556":"with open(\"date_features.pickle\", \"rb\") as f:\n    joined = pickle.load(f)","5b6fb964":"print(\"Region\")\nreg_cat = joined.groupby([\"region\", \"category_name\"])\nreg_cat_diff = reg_cat[\"date_diff\"].mean().to_dict()\nreg_cat_count = reg_cat[\"date_diff\"].count().to_dict()\nreg_cat_repeats = reg_cat[\"repeats\"].mean().to_dict()\n\nprint(\"City\")\ncity_cat = joined.groupby([\"city\", \"category_name\"])\ncity_cat_diff = city_cat[\"date_diff\"].mean().to_dict()\ncity_cat_count = city_cat[\"date_diff\"].count().to_dict()\ncity_cat_repeats = city_cat[\"repeats\"].mean().to_dict()\n\nprint(\"Just category\")\njust_cat = joined.groupby([\"category_name\"])\njust_cat_diff = just_cat[\"date_diff\"].mean().to_dict()\njust_cat_count = just_cat[\"date_diff\"].count().to_dict()\njust_cat_repeats = just_cat[\"repeats\"].mean().to_dict()","04c0cffa":"df[\"category_date_diff\"] = df[\"category_name\"].apply(lambda x: just_cat_diff.get(x, -1.0))\ndf[\"category_count\"] = df[\"category_name\"].apply(lambda x: just_cat_count.get(x, -1.0))\ndf[\"category_repeats\"] = df[\"category_name\"].apply(lambda x: just_cat_repeats.get(x, -1.0))\n\ndf[\"region_category_date_diff\"] = make_col_from_dict(df[[\"region\", \"category_name\"]], reg_cat_diff)\ndf[\"region_category_repeats\"] = make_col_from_dict(df[[\"region\", \"category_name\"]], reg_cat_repeats)\ndf[\"region_category_counts\"] = make_col_from_dict(df[[\"region\", \"category_name\"]], reg_cat_count)\n\ndf[\"city_category_date_diff\"] = make_col_from_dict(df[[\"city\", \"category_name\"]], city_cat_diff)\ndf[\"city_category_repeats\"] = make_col_from_dict(df[[\"city\", \"category_name\"]], city_cat_repeats)\ndf[\"city_category_counts\"] = make_col_from_dict(df[[\"city\", \"category_name\"]], city_cat_count)","baca8487":"df.drop([\"activation_date\"], axis=1, inplace=True)","70ba5fd3":"del joined","916752be":"def load_pairs(filenames):\n    d = {}\n    for f in filenames:\n        with open(f, \"rb\") as f:\n            while True:\n                try:\n                    pair = pickle.load(f)\n                    d[pair[0]] = np.array(pair[1])\n                except EOFError:\n                    break\n    return d, len(pair[1])\n\ndef append_feature_pairs(df, image_col, pair_dict, pair_len, feature_names):\n    zero_obj = -np.ones((pair_len))\n    packed_images_vectors = image_col.apply(lambda x: pair_dict.get(str(x), zero_obj))\n    df[feature_names] = pd.DataFrame(packed_images_vectors.values.tolist(), index=df.index)","2d8cabd7":"image_vectors_dense, p_len = load_pairs([\"vectors_compacted1300k.pickle\"])\ndense_img_vec_names = [str(x + 1) + \"_dense_img_vector\" for x in range(p_len)]\nappend_feature_pairs(df, df[\"image\"] + \".jpg\", image_vectors_dense, p_len, dense_img_vec_names)\ndel image_vectors_dense","e8b19ff6":"feature_dict, p_len = load_pairs([str(x) + \"_img.pickle\" for x in range(8)])\nsimple_img_features = [\n    \"blurrness\", \n    \"size\", \"dims\",\n    \"w\", \"h\", \n    \"avg_color_r\", \"avg_color_g\", \"avg_color_b\",\n    \"pixel_w\",\n    \"light\", \"darkness\", \"img_class\", \"img_proba\"]\n\nappend_feature_pairs(df, df[\"image\"], feature_dict, p_len, simple_img_features)\ndel feature_dict","259f39fd":"# :)\ndf[\"img_class\"] = df[\"img_class\"].apply(lambda x: int(round(x)))","ac790b02":"image_pal_dict, p_len = load_pairs([str(x) + \"_pal.pickle\" for x in range(16)])\npal_feature_names = [\n    \"cful\",\n    \"dom1y\", \"dom1u\", \"dom1v\",\n    \"dom2y\", \"dom2u\", \"dom2v\",\n    \"dom1portion\", \"dom2portion\",\n    \"ymean\", \"umean\", \"vmean\",\n    \"yvar\", \"uvar\", \"vvar\",\n    \"hmean\", \"lmean\", \"smean\",\n    \"hvar\", \"lvar\", \"svar\",\n]\n\nappend_feature_pairs(df, df[\"image\"] + \".jpg\", image_pal_dict, p_len, pal_feature_names)\ndel image_pal_dict","2b2ab4ed":"image_seg_dict, p_len = load_pairs([\"segment_features.pickle\"])\nseg_feature_names = ['kp5',\n 'kp10',\n 'kp20',\n 'mask_size_64',\n 'contour_size_64',\n 'largest_contour_size_64',\n 'contours_64_larger_5',\n 'contours_64_larger_100',\n 'contours_64_larger_500',\n 'mask_size_128',\n 'contour_size_128',\n 'largest_contour_size_128',\n 'contours_128_larger_5',\n 'contours_128_larger_100',\n 'contours_128_larger_500',\n 'mask_size_192',\n 'contour_size_192',\n 'largest_contour_size_192',\n 'contours_192_larger_5',\n 'contours_192_larger_100',\n 'contours_192_larger_500']\n\nappend_feature_pairs(df, df[\"image\"] + \".jpg\", image_seg_dict, p_len, seg_feature_names)\ndel image_seg_dict","565d44e1":"for col in simple_img_features + pal_feature_names + seg_feature_names:\n    df.loc[~np.isfinite(df[col]), col] = -1.0\n# df[[\"image\"] + pal_feature_names + simple_img_features + dense_img_vec_names].to_csv(\"img_export.csv\")\ndf.drop([\"image\"], axis=1, inplace=True)","e5b0b1af":"city_pop = pd.read_csv(\"city_population_wiki_v3.csv\")\ncity_pop = city_pop.groupby(\"city\")[\"population\"].mean().to_dict()","b8c99d01":"df[\"city_pop\"] = df[\"city\"].apply(lambda x: city_pop.get(x, -1.0))","97f775b7":"region_stats = pd.read_csv(\"zarplata2.csv\")\ndef preproc_region(x):\n    return x.lower().replace(\"\u043e\u0431\u043b.\", \"\").replace(\"\u043e\u0431\u043b\u0430\u0441\u0442\u044c\", \"\").replace(\" \", \"\")\nregion_stats[\"region\"] = region_stats[\"region\"].apply(preproc_region)\nregion_stats = region_stats.groupby(\"region\").mean().to_dict()","8b1748cd":"for x in [\"zarplata\",\"woman\",\"mduh\",\"mtrud\",\"mpens\",\"mdet\"]:\n    df[x] = df[\"region\"].apply(lambda y: region_stats[x].get(preproc_region(y), -1.0))","47752dd3":"del region_stats, city_pop","83630174":"def count_consecutive_upper(x):\n    tops = 0\n    current = 0\n    last = False\n    for l in x:\n        if l.isupper():\n            current += 1\n            last = True\n        else:\n            if last:\n                last = False\n                tops = max(current, tops)\n                current = 0\n    return max(current, tops)","b7e50953":"import re\nimport string\nnonascii_pattern = re.compile('[\\x00-\\x7f]')\n\nfor col in [\"description\", \"title\"]:\n    df[col] = df[col].fillna(\"\")\n    print(col + \"...\")\n    df[col + \"_tokens\"] = df[col].apply(lambda x: len(x.split()))\n    df[col + \"_consecutive_\"] = df[col].apply(lambda x: count_consecutive_upper(x))\n    df[col + \"_len\"] = df[col].apply(lambda x: len(x))\n    df[col + \"_upper\"] = df[col].apply(lambda x: sum([y.isupper() for y in x]))\n    df[col + \"_upper_frac\"] = df[col + \"_upper\"] \/ (df[col + \"_len\"] + 0.001)\n    df[col + \"_punct\"] = df[col].apply(lambda x: sum([y in string.punctuation for y in x]))\n    df[col + \"_punct_frac\"] = df[col + \"_punct\"] \/ (df[col + \"_len\"] + 0.001)\n    df[col + \"_special\"] = df[col].apply(lambda x: len(re.sub(nonascii_pattern, '', x)))\n    df[col + \"_special_frac\"] = df[col + \"_special\"] \/ (df[col + \"_len\"] + 0.001)\n    if col == \"description\":\n        df.drop(\"description_upper\", axis = 1, inplace = True)\n        df.drop(\"description_punct\", axis = 1, inplace = True)\n    if col == \"title\":\n        df.drop(\"title_upper\", axis = 1, inplace = True)\n        df.drop(\"title_punct\", axis = 1, inplace = True)","e58d69e2":"import pickle\n\nwith open(\"stemmed.pickle\", \"rb\") as f:\n    title_stemmed, description_stemmed, params_stemmed = pickle.load(f)\n    \ndf[\"title_stemmed\"] = pd.Series(title_stemmed, df.index)\ndf[\"description_stemmed\"] = pd.Series(description_stemmed, df.index)\ndf[\"params_stemmed\"] = pd.Series(params_stemmed, df.index)\n\ndel title_stemmed, description_stemmed, params_stemmed","5d909c95":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\ndef make_svd_features(df, column, features_prefix, n_comp = 15, ngram_range=(1,1), max_features=25000):\n    print(features_prefix)\n    tfidf_vec = TfidfVectorizer(ngram_range = ngram_range, max_features=max_features)\n    full_tfidf = tfidf_vec.fit_transform(df[col].fillna(\"\").values.tolist())\n\n    svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n    svd_obj.fit(full_tfidf)\n    df_svd = pd.DataFrame(svd_obj.transform(full_tfidf))\n    df_svd.columns = [features_prefix + \"_\" + str(i+1) for i in range(n_comp)]\n    df_svd.set_index(df.index, inplace=True)\n    df[df_svd.columns] = df_svd","3bcbe708":"make_svd_features(df, df[\"description_stemmed\"], \"svd_description_stemmed\")\nmake_svd_features(df, df[\"title_stemmed\"], \"svd_title_stemmed\")\nmake_svd_features(df, df[\"params_stemmed\"], \"svd_params_stemmed\")","24e81838":"print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\nrussian_stop = set(stopwords.words('russian')) | set(stopwords.words('english'))\n\ntfidf_para = {\n    \"stop_words\": russian_stop,\n    \"analyzer\": 'word',\n    \"token_pattern\": r'\\w{1,}',\n    \"sublinear_tf\": True,\n    \"dtype\": np.float32,\n    \"norm\": 'l2',\n    #\"min_df\":5,\n    #\"max_df\":.9,\n    \"smooth_idf\":False\n}\n\n\ndef get_col(col_name): return lambda x: x[col_name]\n##I added to the max_features of the description. It did not change my score much but it may be worth investigating\nvectorizer = FeatureUnion([\n    ('description_stemmed', TfidfVectorizer(\n        ngram_range=(1, 2),\n        max_features=25000,\n        **tfidf_para,\n        preprocessor=get_col('description_stemmed'))),\n    ('title_stemmed', TfidfVectorizer(\n        ngram_range=(1, 2),\n        stop_words = russian_stop,\n        preprocessor=get_col('title_stemmed'))),\n    ('params_stemmed', CountVectorizer(\n        ngram_range=(1, 2),\n        stop_words = russian_stop,\n        preprocessor=get_col('params_stemmed')))\n])\n\nstart_vect=time.time()\n\n#Fit my vectorizer on the entire dataset instead of the training rows\n#Score improved by .0001\nvectorizer.fit(df.to_dict('records'))\n\nready_df = vectorizer.transform(df.to_dict('records'))\nsparse_vocab = vectorizer.get_feature_names()\nprint(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)\/60))","411b1cc3":"from sklearn.cross_validation import KFold\nkf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmses = {}\nbest_oof = None\n\nfor alpha in np.arange(10.0, 40.0, 2.0):\n    ridge_params = {'alpha': alpha, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n                    'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n\n    #Ridge oof method from Faron's kernel\n    #I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n    #It doesn't really add much to the score, but it does help lightgbm converge faster\n    ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n    \n    ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])\n    \n    rms = sqrt(mean_squared_error(y, ridge_oof_train))\n    rmses[alpha] = rms\n    if rms == min(rmses.values()):\n        best_oof = (ridge_oof_train, ridge_oof_test)\n    print('Alpha: ' + str(alpha) + ' ridge OOF RMSE: {}'.format(rms))","1a1fbc66":"with open(\"tf-idf.pickle\", \"wb\") as f:\n    pickle.dump(ready_df, f)\n    pickle.dump(best_oof, f)\n    pickle.dump(sparse_vocab, f)","cc53311a":"with open(\"tf-idf.pickle\", \"rb\") as f:\n    ready_df = pickle.load(f)\n    best_oof = pickle.load(f)\n    sparse_vocab = pickle.load(f)","ccde6ec1":"ridge_oof_train, ridge_oof_test = best_oof","3d7ad89d":"ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\ndf['ridge_preds'] = ridge_preds","e047911f":"df.drop([\"title_stemmed\", \"description_stemmed\", \"params_stemmed\"], axis=1,inplace=True)\ndf.drop([\"description\", \"title\"], axis=1,inplace=True)","48285ed9":"tfidf_nearest_features = pd.read_csv(\"tf_idf_nearest_features.csv.zip\", compression=\"zip\")\ntfidf_nearest_features.set_index(\"item_id\", inplace=True)","d6b4662b":"img_nearest_features = pd.read_csv(\"img_nearest_features.csv.zip\", compression=\"zip\")\nimg_nearest_features.set_index(\"item_id\", inplace=True)","0df4c2a0":"cat_img_nearest_features = pd.read_csv(\"cat_nearest_features.csv.zip\", compression=\"zip\")\ncat_img_nearest_features.set_index(\"item_id\", inplace=True)","0b3808bf":"df[tfidf_nearest_features.columns] = tfidf_nearest_features\ndf[img_nearest_features.columns] = img_nearest_features\ndf[cat_img_nearest_features.columns] = cat_img_nearest_features","e748cc9d":"for col in list(tfidf_nearest_features.columns) + list(img_nearest_features.columns) + list(cat_img_nearest_features.columns):\n    df[col].fillna(-999.0, inplace=True)","fb2efc97":"to_drop = []\nfor col in df.columns:\n    if \"target\" in col:\n        to_drop.append(col)\nif len(to_drop) > 0:\n    df.drop(to_drop, axis=1, inplace=True)","ba17cb11":"del img_nearest_features, tfidf_nearest_features, cat_img_nearest_features\ngc.collect()","bd513421":"def make_similarity_feature(group_col, variables_to_agg):\n    variables_to_agg = (variables_to_agg - variables_to_agg.mean()) \/ (variables_to_agg.max() - variables_to_agg.min())\n    sims = np.zeros((len(group_col)))\n    for x in np.unique(group_col):\n        inds = np.where(group_col == x)[0]\n        centroid = np.median(variables_to_agg.iloc[inds], axis=0)\n        sims[inds] = np.dot(variables_to_agg.iloc[inds], centroid)\n    return sims","743e935a":"def make_similarity_feature_sparse(group_col, variables_to_agg):\n    variables_to_agg = (variables_to_agg - variables_to_agg.mean()) \/ (variables_to_agg.max() - variables_to_agg.min())\n    sims = np.zeros((len(group_col)))\n    for x in np.unique(group_col):\n        inds = np.where(group_col == x)[0]\n        centroid = np.median(variables_to_agg.iloc[inds], axis=0)\n        sims[inds] = np.dot(variables_to_agg.iloc[inds], centroid)\n    return sims","bc8187bd":"df[\"region_cat\"] = df[\"region\"].astype(str) + \" \" + df[\"category_name\"].astype(str)\nsvd_description_columns = list(filter(lambda x: \"svd_description\" in x, list(df.columns)))\nsvd_title_columns = list(filter(lambda x: \"svd_title\" in x, list(df.columns)))","2fc1ea59":"group_columns = [\"category_name\", \"parent_category_name\", \n                 \"param_1\", \"param_2\",\n                 \"image_top_1\", \"img_class\",\n                 \"region_cat\"]\nagg_columns = [\n    (\"title_desc_svd\", svd_description_columns + svd_title_columns),\n    (\"desc_vec\", svd_description_columns),\n    (\"title_vec\", svd_title_columns),\n    (\"image_vec\", dense_img_vec_names),\n    (\"price\", [\"price\"])\n]\n\nfor gr in group_columns:\n    for agg_n, agg_c in agg_columns:\n        print(\"sims_\" + gr + \"_\" + agg_n)\n        sim_text = make_similarity_feature(df[gr].fillna(\"na\"), df[agg_c])\n        df[\"sims_\" + gr + \"_\" + agg_n] = pd.Series(sim_text, index = df.index)","2d4372e3":"df.drop([\"region_cat\"], axis=1, inplace=True)","b70c2fcf":"len(set(df[\"user_id\"]) & set(testing_users)), len(set(df[\"user_id\"]) - set(testing_users))","c9d0e329":"# Droppping unneccessary levels\n# df[\"user_id\"][~df[\"user_id\"].isin(set(testing_users))] = \"Unknown\"","f1b2e8b5":"categorical = [\"img_class\", \"user_id\", \n               \"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\n               \"image_top_1\",\"param_1\",\"param_2\",\"param_3\"]\n\nlbl = LabelEncoder()\nfor col in categorical:\n    print(col)\n    df[col] = lbl.fit_transform(df[col].fillna('Unknown').astype(str))","278459a5":"def describe_columns(cols = df.columns):\n    from scipy.stats import describe\n    for c in cols:\n        print(\"\\n\\n\", c, df[c][0].__class__)\n        print(c, \"train:\")\n        print(\"-1: \", sum(df.loc[traindex][c] == -1))\n        print(describe(df.loc[traindex][c][df.loc[traindex][c] != -1]))\n        print(c, \"test:\")\n        print(\"-1: \", sum(df.loc[testdex][c] == -1))\n        print(describe(df.loc[testdex][c][df.loc[testdex][c] != -1]))\n","c78fe806":"X = hstack([csr_matrix(df.loc[traindex,:].values), ready_df[0:traindex.shape[0]]])\ntesting = hstack([csr_matrix(df.loc[testdex,:].values), ready_df[traindex.shape[0]:]])\ntfvocab = df.columns.tolist() + sparse_vocab\n\nfor shape in [X,testing]:\n    print(\"{} Rows and {} Cols\".format(*shape.shape))\n\ngc.collect()","0a7be3a3":"# X = np.array(df.loc[traindex,:].values)\n# testing = np.array(df.loc[testdex,:].values)\n# tfvocab = df.columns.tolist() # + tfvocab\n# \n# for shape in [X,testing]:\n#     print(\"{} Rows and {} Cols\".format(*shape.shape))\n# gc.collect()","baf7b3e4":"print(time.time())\n\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 430,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 2,\n    'learning_rate': 0.01,\n    'verbose': 0,\n    \"num_threads\": 8,\n}\n\nVALID = True\n\nif VALID:\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=23)\n    \n    # LGBM Dataset Formatting \n    lgtrain = lgb.Dataset(X_train, y_train,\n                    feature_name=tfvocab,\n                    categorical_feature = categorical)\n    lgvalid = lgb.Dataset(X_valid, y_valid,\n                    feature_name=tfvocab,\n                    categorical_feature = categorical)\n    \n    lgb_clf = lgb.train(\n        lgbm_params,\n        lgtrain,\n        num_boost_round=20000,\n        valid_sets=[lgtrain, lgvalid],\n        valid_names=['train','valid'],\n        early_stopping_rounds=50,\n        verbose_eval=25\n    )\n\nelse:\n    # LGBM Dataset Formatting \n    lgtrain = lgb.Dataset(X, y,\n                          feature_name = tfvocab,\n                          categorical_feature = categorical)\n    # del X; gc.collect()\n    # Go Go Go\n    lgbm_params[\"silent\"] = False\n    lgbm_params[\"verbose\"] = 1\n    lgb_clf = lgb.train(\n        lgbm_params,\n        lgtrain,\n        num_boost_round=1800,\n    )\nprint(time.time())","b20adc2b":"print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))","f1c71fe7":"# Feature Importance Plot\nf, ax = plt.subplots(figsize=[7,10])\nlgb.plot_importance(lgb_clf, max_num_features=50, ax=ax)\nplt.title(\"Light GBM Feature Importance\")\nplt.savefig('feature_import.png')","97f54494":"best_tree_count = lgb_clf.best_iteration\nbest_tree_count","ad3b8eed":"lgsub = pd.DataFrame(lgb_clf.predict(testing), columns=[\"deal_probability\"],index=testdex)\nlgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\nlgsub.to_csv(\"lgsub_with_features.csv\",index=True,header=True)","03834b0d":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 7, shuffle=True, random_state=42)\n\nmodels = []\nfold_pred = []\nfold_val = []\nw = np.zeros((testing.shape[0]))\n\nX = csr_matrix(X)","5b708ac9":"for train_index, test_index in kf.split(X, y):\n    print(\"TRAIN: \", len(train_index), \"TEST:\", len(test_index))\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    lgtrain = lgb.Dataset(X_train, y_train,\n                          feature_name = tfvocab,\n                          categorical_feature = categorical)\n\n    lgb_clf = lgb.train(\n        lgbm_params,\n        lgtrain,\n        num_boost_round=best_tree_count, # 2454\n    )\n\n    pred = lgb_clf.predict(X_test)\n    w += lgb_clf.predict(testing)\n    \n    models.append(lgb_clf)\n    fold_pred.append(pred)\n    fold_val.append(test_index)\n    \n    print(mean_squared_error(y_test, pred))\nw \/= 7","fb248898":"with open(\"folds.pickle\", \"wb\") as f:\n    pickle.dump(fold_pred, f)\n    pickle.dump(fold_val, f)\n    pickle.dump(models, f)","49d0ed72":"import pickle\nwith open(\"folds.pickle\", \"rb\") as f:\n    fold_pred = pickle.load(f)\n    fold_val = pickle.load(f)\n    models = pickle.load(f)","6c34f141":"oofs = np.empty((len(traindex), len(fold_val)))\noof_predictions = np.zeros((len(testdex), len(fold_val)))\noofs.fill(np.nan)\n\nfor i in range(len(fold_val)):\n    oofs[fold_val[i], i] = fold_pred[i]\n    oof_predictions[:, i] = models[i].predict(testing)","95e30fc6":"pd.DataFrame({\"item_id\": np.array(traindex),\n              \"train_off_predictions\": np.nanmean(oofs, axis=1)}).to_csv(\"train_oof_predictions2.csv\")\npd.DataFrame({\"item_id\": np.array(testdex),\n              \"test_predictions\": np.nanmean(oof_predictions, axis=1)}).to_csv(\"test_predictions_folds2.csv\")","9f61e220":"lgsub = pd.DataFrame(w, columns=[\"deal_probability\"], index=testdex)\nlgsub['deal_probability'].clip(0.0, 1.0, inplace=True)\nlgsub.to_csv(\"lgsub_with_features.csv\",index=True,header=True)","cf3868fb":"gc.collect()","e406ad94":"# !kaggle competitions submit -c avito-demand-prediction -f lgsub_with_features.csv -m LGBBaselineWithFeatures","95402a4d":"### BOW + prediction\nI should rewrite this mess","18ff1a16":"# Data loading","252843f1":"Merging, making features","cd4e26d0":"# Text features","3ee72753":"### Applying features","662f1845":"### Price features in categories","2e1a8445":"### Image basic parameters (class, size, average colors etc)","6994deb3":"### Clean-up\nSome features have NaNs or Infs in them. Let's fill these","944a3a33":"### Dense vectors","fd43d4c7":"# Building train-test sets","c262a702":"Unfortunately, target features induce leaks. In future, I should use folds to properly calculate them. For now, let's just drop them.","738f0fa3":"# Similarity features\nCore idea of these features is similar to the nearest neighbout-based features. But instead of calculating features from nearest neighbours, let's estimate how much particular item sticks out from his category\/class according to this distance metric.","7f9eda81":"# Avito demand prediction challenge\n\nThis notebook contains code that makes single-model prediction that scores ~0.2195 on public leaderboard. [Repo](https:\/\/github.com\/rampeer\/kaggle-avito) .\n\nNotebook contains data processing, some simple features, cross-validation and submission generation. Most of the features take a while to calculate, so I calculate them in [separate notebooks](https:\/\/github.com\/rampeer\/kaggle-avito), cache them in .pickle files, and just load here. Running all notebooks from scratch would take ~4 days on my machine (64 Gb RAM, 8 cores, GPU).\n\nI'm not great at explaining things; comments and suggestions are appreciated.\n\nThis is my first time seriously (=not just submitting baseline) participating in Kaggle competition. I stood on shoulders of titans:\n - BoW examples by [tunguz](https:\/\/www.kaggle.com\/tunguz\/bow-meta-text-and-dense-features-lb-0-2242\/code) and [Nick Brook](https:\/\/www.kaggle.com\/nicapotato\/bow-meta-text-and-dense-features-lgbm), oof by [mmueller](https:\/\/www.kaggle.com\/mmueller\/stacking-starter?scriptVersionId=390867)\n - TargetEncoder by [samratp](https:\/\/www.kaggle.com\/samratp\/wordbatch-ridge-fm-frtl-target-encoding-lgbm) and [ogrellier](https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features)\n - ODS community (many ideas and suggestions)\n\n\n#### Contents:\n\n1. [Settings, imports](#Settings-and-imports)\n2. [Data loading](#Data-loading)\n3. Features\n  - [Date and price features](#Date-and-price-features)\n  - [User features](#User-features)\n  - [Image features](#Image-features)\n  - [City\/region-level external data](#City-and-region-extra-data)\n  - [Nearest-neighbour - based features](#KNN-features)\n  - [Text features](#Text-features)\n  - [Similarity features](#Similarity-features)\n4. [Encoding categorial variables](#Encoding-categorial-variables)\n5. [Making train\/test sets](#Building-train-test-sets)\n6. [Running single model and estimating tree count](#Running-single-model)\n7. [Using folds to make final prediction](#Using-folds)","b92d8eee":"# User features","10526ce5":"# Date and price features\nApplying log-transform to the price, so its distribution would appear normal-ish.\n\nNaNs are filled with mean (filling it with mean by category reduced my score for some reason).","75b76d39":"# Image features\nImage features are the most computational-intensive features in this notebook. They are calculated in separate (img_features.ipynb)[notebook].","7d13e29f":"# Encoding categorial variables","eafa0485":"### Image segmentation features","51d431d6":"### Simple character\/word-level feautres\nHow many words\/specific character there are in description\/title","fb5ec49e":"# Using folds","3a9a4874":"### SVD\/BOW features\nStemming slightly improves score, but takes a while to calculate, so I stem words and cache results in [separate notebook](stemming.ipynb).","6766792a":"# Settings and imports\nThe script expects `base_path` to be a directory that contains unpacked csv files, `train_image_base_path` and `test_image_base_path` to be directories that contain unpacked train and test images respectively.","4dcd2d75":"### SVD\nIdea is rather simple: create TF-IDF matrices and compact them into dense representations using truncated SVD. Such dense representations capture most of the variance of the original TF-IDF matrix, so we can say that it extracts \"patterns\" from it, and estimates how close each title\/description to each pattern.\n\nI run title\/description\/param SVDs separately because otherwise words from lengthy descriptions will overwhelm words from title and params.\n\n","69b52847":"# City and region extra data\nAdding extra features provided by my teammates. These numbers were crawled from Wikipedia, and add (subtract :) few extra scoring points","78b20fb7":"### Stats to ensure that everything is OK\nYou might want to run this function to carefully check whether distribution of feautures in train\/test are consistent.","6ac7a323":"### Palette features","1ce09c1f":"# Running single model","d4c45705":"### Using features","a62fdee2":"# Date\/region\/city features\nI'll clean this mess","23f58791":"Periods","74ee57d3":"# KNN features\nIdea of these features: let's take some features and distance measure, and find nearest objects according to it. After that, let's extract features from those objects (price mean\/variance, for example).\n\nIt takes ~30 hours to build these features from scratch. So, they are calculated in separate notebook and cached."}}