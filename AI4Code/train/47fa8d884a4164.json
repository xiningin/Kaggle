{"cell_type":{"0928e523":"code","ca23172b":"code","fca543eb":"code","95abbde7":"code","87db6a8d":"code","363e5b18":"code","80d8385b":"code","0fcf8f02":"code","9db4ce0d":"code","0815af64":"code","a6c175d4":"code","7ffbe357":"code","1372832a":"code","a3468974":"code","b55d3308":"code","1f4491e8":"code","3bb05ef3":"code","80c6b6be":"code","d7c6770f":"code","36eeaa65":"code","7924b968":"code","c5a8c49d":"code","69c4c8f2":"code","28759b3f":"code","3ddb6857":"code","03f5eda7":"code","e6ecfe84":"code","3b2e6d29":"code","1b578b43":"code","ffd9f44a":"code","fa37d57d":"code","ee884b6d":"code","98a53ed8":"code","9aa998e5":"markdown","ad47e85f":"markdown","aa33710c":"markdown","9d48fe05":"markdown","abfe9bdd":"markdown","293ad8c4":"markdown","b2e5281c":"markdown","cd553bf7":"markdown"},"source":{"0928e523":"!pip install -U --no-build-isolation --no-deps ..\/input\/transformers-master\/ -qq","ca23172b":"import sys\nsys.path.append(\"..\/input\/tez-lib\/\")\nimport collections\nimport numpy as np\nimport transformers\nimport pandas as pd\nfrom datasets import Dataset\nfrom functools import partial\nfrom tqdm import tqdm\nimport json\nimport torch\n\nfrom sklearn import metrics\nimport transformers\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport tez\nfrom string import punctuation","fca543eb":"class ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        start_logits = transformer_out.start_logits\n        end_logits = transformer_out.end_logits\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}","95abbde7":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","87db6a8d":"special_tokens = {\n    \"muril\": 2,\n    \"xlmr\": 3,\n    \"rembert\": 2,\n}\n\ndef postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n    all_answers = []\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": float(start_logits[start_index] + end_logits[end_index]),\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n        valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n        all_answers.append({\"id\": example[\"id\"], \"predictions\": valid_answers})\n    return all_answers, predictions","363e5b18":"def convert_to_one_array(logits, max_length, stride, offset):\n    num_chunks = len(logits)\n    ctx_size = max_length-offset-1\n    max_ctx_size = num_chunks*ctx_size-(num_chunks-1)*stride\n    final_size = offset+max_ctx_size\n\n    full = np.zeros((final_size))\n    full[0:max_length-1] = logits[0][:-1]\n\n    left_idx = max_length-1-stride\n    for idx in range(1, num_chunks):\n        right_idx = left_idx+ctx_size\n\n        full[left_idx:right_idx] += logits[idx][offset:-1]\n        full[left_idx:left_idx+stride]\/=2\n        \n        left_idx = right_idx-stride  \n    return full\n\ndef token_level_to_char_level(text, offsets, preds):\n    probas_char = np.ones(len(text))*-100\n    for i, offset in enumerate(offsets):\n        if offset[0] or offset[1]: # remove padding and sentiment\n            probas_char[offset[0]:offset[1]] = preds[i]\n    \n    return probas_char\n\n\ndef new_postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, max_seq_length, doc_stride, num_special_tokens, file_name, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    all_start_logits, all_end_logits = raw_predictions\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n        \n    new_start_features = collections.OrderedDict()\n    new_end_features = collections.OrderedDict()\n    \n    char_level_starts = {}\n    char_level_ends = {}\n    \n    predictions = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n    all_answers = []\n\n    for example_index, example in enumerate(tqdm(examples)):\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        num_q_tokens = len(tokenizer(example[\"question\"], add_special_tokens=False)[\"input_ids\"])\n        flat_input_ids = tokenizer(example[\"question\"], example[\"context\"], return_offsets_mapping=True)\n        \n        left_offset = num_q_tokens+num_special_tokens\n        \n        id_ = example[\"id\"]\n        \n        new_start_features[id_] = convert_to_one_array(all_start_logits[feature_indices], max_seq_length, doc_stride, left_offset)\n        new_end_features[id_] = convert_to_one_array(all_end_logits[feature_indices], max_seq_length, doc_stride, left_offset)        \n        \n        start_indexes = np.argsort(new_start_features[id_])[-1 : -n_best_size - 1 : -1].tolist()\n        end_indexes = np.argsort( new_end_features[id_])[-1 : -n_best_size - 1 : -1].tolist()\n        \n        offset_mapping = flat_input_ids[\"offset_mapping\"]\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                if (\n                    start_index >= len(offset_mapping)\n                    or end_index >= len(offset_mapping)\n                    or offset_mapping[start_index] is None\n                    or offset_mapping[end_index] is None\n                    or start_index < left_offset\n                ):\n                    continue\n                if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                    continue\n\n                start_char = offset_mapping[start_index][0]\n                end_char = offset_mapping[end_index][1]\n                valid_answers.append(\n                    {\n                        \"score\": float(new_start_features[id_][start_index] + new_end_features[id_][end_index]),\n                        \"text\": example[\"context\"][start_char:end_char],\n                        \"start\": start_index,\n                        \"end\": end_index\n                    }\n                )\n\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n        valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n        all_answers.append({\"id\": example[\"id\"], \"predictions\": valid_answers})\n\n    \n        char_level_starts[id_] = token_level_to_char_level(example[\"context\"], offset_mapping, new_start_features[id_])\n        char_level_ends[id_] = token_level_to_char_level(example[\"context\"], offset_mapping, new_end_features[id_])\n\n\n    with open(f\"char-level-start-logits-{file_name}\", \"wb\") as fp:\n        pickle.dump(char_level_starts, fp)\n\n    with open(f\"char-level-end-logits-{file_name}\", \"wb\") as fp:\n        pickle.dump(char_level_ends, fp)\n        \n        \n    return all_answers, predictions","80d8385b":"test_data = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\ntest_data[\"len\"] = [len(x) for x in test_data[\"context\"]]\n\ndo_inference = len(test_data) != 5\n\nchar_threshold = 15_000\n\n# short_data = test_data.copy()\nshort_data = test_data[test_data[\"len\"]<char_threshold].reset_index(drop=True)\nlong_data = test_data[test_data[\"len\"]>=char_threshold].reset_index(drop=True)","0fcf8f02":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/convert-to-pytorch-muril-large\")\n    \n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 384\n    doc_stride = 128\n\n    test_dataset = Dataset.from_pandas(short_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n    chunk1 = [\"..\/input\/convert-to-pytorch-muril-large\/nbroad\/flax-muril-large-chaii-f0\",]*3\n    chunk2 = [\"..\/input\/convert-to-pytorch-muril-large-f567\/nbroad\/flax-muril-large-chaii-f5\"]*3\n    models = chunk1+chunk2\n\n    data_loader = torch.utils.data.DataLoader(\n        test_feats_small.with_format(\"torch\"), \n        batch_size=64,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n\n    for model_name, fold in tqdm(zip(models, [0, 8, 9, 5, 6, 7])):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"{model_name[:-1]}{fold}\/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-muril-large-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","9db4ce0d":"import pickle\n\nif do_inference:\n    fin_start_logits \/= len(models)\n    fin_end_logits \/= len(models)\n    \n    # This is for voting\n#     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n#     with open('muril-large-preds.json', \"w\") as fp:\n#         json.dump(to_save, fp)\n\n    all_answers, predictions = new_postprocess_qa_predictions(\n    test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"muril-large\", n_best_size=20, max_answer_length=30, squad_v2=False\n)\n\n#     short_data[\"PredictionString\"] = long_data[\"id\"].map(fin_preds)","0815af64":"# if do_inference:\n#     tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/bb-base-chaii\")\n#     pad_on_right = tokenizer.padding_side == \"right\"\n#     max_length = 4096\n#     doc_stride = 2048\n\n#     test_dataset = Dataset.from_pandas(test_data)\n#     test_features = test_dataset.map(\n#         partial(\n#             prepare_validation_features, \n#             tokenizer=tokenizer,\n#             pad_on_right=pad_on_right, \n#             max_length=max_length,\n#             doc_stride=doc_stride\n#         ),\n#         batched=True,\n#         remove_columns=test_dataset.column_names\n#     )\n#     test_feats_small = test_features.map(\n#         lambda example: example, remove_columns=['example_id', 'offset_mapping']\n#     )\n\n#     fin_start_logits = None\n#     fin_end_logits = None\n\n#     models = [\n#         \"..\/input\/bb-base-chaii\",\n#         \"..\/input\/nbroad-flax-muril-bb-base-chaii-f2\",\n#         \"..\/input\/nbroad-flax-muril-bb-base-chaii-f3\",\n#         \"..\/input\/nbroad-flax-muril-bb-base-chaii-f4\",\n#         \"..\/input\/nbroad-flax-bb-base-chaii-f5\",\n#         \"..\/input\/nbroad-flax-muril-bb-base-chaii-f6\",\n#         \"..\/input\/nbroad-flax-muril-bb-base-chaii-f7\", \n#     ]\n\n#     data_loader = torch.utils.data.DataLoader(\n#         test_feats_small.with_format(\"torch\"), \n#         batch_size=16,\n#         num_workers=4,\n#         pin_memory=True,\n#         shuffle=False\n#     )\n\n\n#     for fold, model_name in tqdm(enumerate(models)):\n#         model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n#         model.transformer.load_state_dict(torch.load(f\"{model_name}\/pytorch_model.bin\"))\n#         model.to(\"cuda\")\n#         model.eval()\n\n#         start_logits = []\n#         end_logits = []\n\n#         for b_idx, data in enumerate(data_loader):\n#             with torch.no_grad():\n#                 for key, value in data.items():\n#                     data[key] = value.to(\"cuda\")\n#                 output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n#                 start = output[0].detach().cpu().numpy()\n#                 end = output[1].detach().cpu().numpy()\n#                 start_logits.append(start)\n#                 end_logits.append(end)\n\n#         start_logits = np.vstack(start_logits)\n#         end_logits = np.vstack(end_logits)\n\n#         if fin_start_logits is None:\n#             fin_start_logits = start_logits\n#             fin_end_logits = end_logits\n#         else:\n#             fin_start_logits += start_logits\n#             fin_end_logits += end_logits\n            \n# #         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n# #         with open(f\"top-preds-bb-f{fold}.json\", \"w\") as fp:\n# #             json.dump(to_save, fp)\n\n#         del model\n#         torch.cuda.empty_cache()","a6c175d4":"# # if do_inference:\n# #     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n# if do_inference:\n#     fin_start_logits \/= len(models)\n#     fin_end_logits \/= len(models)\n    \n# #     to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits))\n# #     with open('muril-large-preds.json', \"w\") as fp:\n# #         json.dump(to_save, fp)\n\n#     all_answers, predictions = new_postprocess_qa_predictions(\n#     test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"bb-4k\", n_best_size=20, max_answer_length=30, squad_v2=False\n# )","7ffbe357":"# if do_inference:\n#     long_data[\"PredictionString\"] = long_data[\"id\"].map(fin_preds)","1372832a":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/murilbasecased\")\n    \n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 1024\n    doc_stride = 512\n\n    test_dataset = Dataset.from_pandas(test_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n    data_loader = torch.utils.data.DataLoader(\n        test_feats_small.with_format(\"torch\"), \n        batch_size=16,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n    \n    model_name = \"..\/input\/muril-large-bigbird-1k-6f\/nbroad\/1k-shuf-squad-chaii-6f0\"\n    for fold in tqdm(range(6)):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"..\/input\/muril-large-bigbird-1k-6f\/nbroad\/1k-shuf-squad-chaii-6f{fold}\/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-muril-large-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","a3468974":"if do_inference:\n    \n    fin_start_logits \/= len(models)\n    fin_end_logits \/= len(models)\n    \n    all_answers, predictions = new_postprocess_qa_predictions(\n    test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"bb-1k\", n_best_size=20, max_answer_length=30, squad_v2=False\n)","b55d3308":"if do_inference:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/xlmrob\")\n    pad_on_right = tokenizer.padding_side == \"right\"\n    max_length = 384\n    doc_stride = 128\n\n    test_dataset = Dataset.from_pandas(short_data)\n    test_features = test_dataset.map(\n        partial(\n            prepare_validation_features, \n            tokenizer=tokenizer,\n            pad_on_right=pad_on_right, \n            max_length=max_length,\n            doc_stride=doc_stride\n        ),\n        batched=True,\n        remove_columns=test_dataset.column_names\n    )\n    test_feats_small = test_features.map(\n        lambda example: example, remove_columns=['example_id', 'offset_mapping']\n    )\n\n    fin_start_logits = None\n    fin_end_logits = None\n\n\n    data_loader = torch.utils.data.DataLoader(\n        test_feats_small.with_format(\"torch\"), \n        batch_size=16,\n        num_workers=4,\n        pin_memory=True,\n        shuffle=False\n    )\n\n    model_name = \"..\/input\/convert-to-pytorch-xlmr-large-chaii-6f\/nbroad\/xlmr-large-chaii-6f0\"\n    for fold in tqdm(range(6)):\n        model = ChaiiModel(model_name=model_name, num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n        model.transformer.load_state_dict(torch.load(f\"..\/input\/convert-to-pytorch-xlmr-large-chaii-6f\/nbroad\/xlmr-large-chaii-6f{fold}\/pytorch_model.bin\"))\n        model.to(\"cuda\")\n        model.eval()\n\n        start_logits = []\n        end_logits = []\n\n        for b_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                for key, value in data.items():\n                    data[key] = value.to(\"cuda\")\n                output, _, _ = model(ids=data[\"input_ids\"], mask=data[\"attention_mask\"])\n                start = output[0].detach().cpu().numpy()\n                end = output[1].detach().cpu().numpy()\n                start_logits.append(start)\n                end_logits.append(end)\n\n        start_logits = np.vstack(start_logits)\n        end_logits = np.vstack(end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n            \n#         to_save, fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))      \n#         with open(f\"top-preds-bb-f{fold}.json\", \"w\") as fp:\n#             json.dump(to_save, fp)\n\n        del model\n        torch.cuda.empty_cache()","1f4491e8":"if do_inference:\n    fin_start_logits \/= 6\n    fin_end_logits \/= 6\n    \n    all_answers, predictions = new_postprocess_qa_predictions(\n    test_dataset, tokenizer, test_features, (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=3, file_name=\"xlmr\", n_best_size=20, max_answer_length=30, squad_v2=False\n)","3bb05ef3":"if do_inference:\n    import tensorflow as tf\n    import tensorflow.keras.backend as K\n\n    rembert_model_path = \"..\/input\/rembert-tf\"\n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(rembert_model_path)\n\n    strategy = tf.distribute.get_strategy()\n    AUTO     = tf.data.experimental.AUTOTUNE\n\n    max_length = 384\n    doc_stride = 128\n\n    pad_on_right = tokenizer.padding_side == \"right\"","80c6b6be":"if do_inference:\n    def build_model():\n        roberta = transformers.TFAutoModel.from_pretrained(rembert_model_path)\n\n        input_ids = tf.keras.layers.Input(shape = (max_length, ), name = 'input_ids', dtype = tf.int32)\n        attention_mask = tf.keras.layers.Input(shape = (max_length, ), name = 'attention_mask', dtype = tf.int32)\n\n        embeddings = roberta(input_ids=input_ids, attention_mask=attention_mask)[0]\n\n        x1 = tf.keras.layers.Dropout(0.1)(embeddings)\n        x1 = tf.keras.layers.Dense(1, dtype=tf.float32)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax', name='start_positions', dtype=tf.float32)(x1)\n\n        x2 = tf.keras.layers.Dropout(0.1)(embeddings)\n        x2 = tf.keras.layers.Dense(1, dtype=tf.float32)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax', name='end_positions', dtype=tf.float32)(x2)\n\n        model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = [x1, x2])\n\n        model.compile()\n\n        return model","d7c6770f":"if do_inference:\n    \n    fin_start_logits = None\n    fin_end_logits = None\n\n    batch_size = 128\n    \n    strategy = tf.distribute.get_strategy()\n    \n\n    test_dataset = Dataset.from_pandas(short_data)\n\n    test_features = test_dataset.map(\n            partial(\n                prepare_validation_features, \n                tokenizer=tokenizer,\n                pad_on_right=pad_on_right, \n                max_length=max_length,\n                doc_stride=doc_stride\n            ),\n            batched=True,\n            remove_columns=test_dataset.column_names\n        )\n\n    test_features = test_features.with_format('tensorflow')\n\n    test_x = {x: test_features[x] for x in ['input_ids', 'attention_mask']}\n    test_slices = tf.data.Dataset.from_tensor_slices((test_x)).batch(batch_size)\n\n    models = [\n            \"..\/input\/rembert-0-3hi-v2\/rembert-fit-chaii\/fold0\/tf_model.h5\",\n            \"..\/input\/rembert-0-3hi-v2\/rembert-fit-chaii\/fold1\/tf_model.h5\",\n            \"..\/input\/rembert-0-3hi-v2\/rembert-fit-chaii\/fold2\/tf_model.h5\",\n            \"..\/input\/rembert-4-6-ta\/rembert-fit-chaii\/fold4\/tf_model.h5\", \n            \"..\/input\/rembert-4-6-ta\/rembert-fit-chaii\/fold5\/tf_model.h5\",\n    ]\n\n    for model_name in models:\n        K.clear_session()\n        strategy = tf.distribute.get_strategy()\n\n        with strategy.scope():\n            model = build_model()\n            model.load_weights(model_name)\n\n\n        temp_start_logits, temp_end_logits = model.predict(test_slices, batch_size=batch_size, verbose=1)\n\n\n        start_logits = np.vstack(temp_start_logits)\n        end_logits = np.vstack(temp_end_logits)\n\n        if fin_start_logits is None:\n            fin_start_logits = start_logits\n            fin_end_logits = end_logits\n        else:\n            fin_start_logits += start_logits\n            fin_end_logits += end_logits\n\n\n    fin_start_logits \/= 5\n    fin_end_logits \/= 5\n\n    all_answers, predictions = new_postprocess_qa_predictions(\n        test_dataset, tokenizer, test_features.with_format(\"numpy\"), (fin_start_logits, fin_end_logits), max_length, doc_stride, num_special_tokens=2, file_name=\"remb\", n_best_size=20, max_answer_length=30, squad_v2=False\n    )","36eeaa65":"if do_inference:\n    def move_to_boundary(start, end, context):\n        initial_start = start\n        start_offset = 0\n        end_offset = 0\n        if not context[start].isspace():\n            start_offset += 1\n            while start-start_offset > 0 and not context[start-start_offset].isspace():\n                start_offset+=1\n        if not context[end].isspace():\n            end_offset += 1\n            while end+end_offset < len(context) and not context[end+end_offset].isspace():\n                end_offset += 1\n        return context[start-start_offset:end+end_offset]","7924b968":"if do_inference:\n    from scipy.special import softmax   \n    \n    def fix_duplicate_scores(scores, starts=True):\n        # if starts=True, will keep the value with the lower index\n        # [1,2,2,3,4] -> [1,2,-10, 3, 4]\n        # otherwise, keep the value with higher index\n        # [1,2,2,3,4] -> [1,-10, 2, 3, 4]   \n\n        if not starts:\n            scores = scores[::-1]\n\n        prev = scores[0]\n        new_scores = [prev]\n        for val in scores[1:]:\n            if abs(val-prev) < 1e-5:\n                new_scores.append(-100)\n            else:\n                new_scores.append(val)\n            prev = val\n\n        return np.array(new_scores) if starts else np.array(new_scores[::-1])\n    \n    def process_char_level_logits(filenames, data, multipliers, n_best_size=20, max_answer_length=35):\n        all_starts = {}\n        for filename in filenames:\n            with open(f\"char-level-start-logits-{filename}\", \"rb\") as fp:\n                all_starts[filename] = pickle.load(fp)\n\n        all_ends = {}\n        for filename in filenames:\n            with open(f\"char-level-end-logits-{filename}\", \"rb\") as fp:\n                all_ends[filename] = pickle.load(fp)\n\n        predictions = collections.OrderedDict()\n        all_answers = []\n        for id_, context in data[[\"id\", \"context\"]].values:\n            start_logits = sum([softmax(logits[id_])*multipliers[filename] for filename, logits in all_starts.items()])\n            end_logits = sum([softmax(logits[id_])*multipliers[filename] for filename, logits in all_ends.items()])\n            \n            start_logits = fix_duplicate_scores(start_logits, starts=True)\n            end_logits = fix_duplicate_scores(end_logits, starts=False)\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n\n            valid_answers = []\n            for start_index in start_indexes:\n                if start_logits[start_index] == -100: continue\n                for end_index in end_indexes:\n                    if end_logits[end_index] == -100: continue\n                    if end_index < start_index or end_index - start_index + 1 > 30:\n                        continue\n\n                    text = context[start_index:end_index]\n                    valid_answers.append(\n                        {\n                            \"score\": float(start_logits[start_index] + end_logits[end_index]),\n                            \"text\": text,\n                            \"start\": start_index,\n                            \"end\": end_index,\n                            \"moved\": move_to_boundary(start_index, end_index, context)\n                        }\n                    )\n        \n            sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)\n\n            if len(valid_answers) > 0:\n                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n            else:\n                best_answer = {\"text\": \"\", \"score\": 0.0}\n\n            answer = best_answer[\"moved\"]\n            predictions[id_] = answer\n\n            valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n            all_answers.append({\"id\": id_, \"predictions\": valid_answers})\n\n        return all_answers, predictions\n\n    multipliers = {\n        \"bb-4k\": 0.5,\n        \"muril-large\": 0.8,\n        \"bb-1k\": 1.3,\n        \"xlmr\": 1.1,\n        \"remb\": 1.2\n    }\n    filenames = [\n        'muril-large', \n        #\"bb-4k\",\n        \"bb-1k\", \n        \"xlmr\", \n        \"remb\"\n    ]\n    output_short = process_char_level_logits(filenames, short_data, multipliers)\n    \n    filenames = [\n#         \"bb-4k\", \n        \"bb-1k\"\n    ]\n    output_long = process_char_level_logits(filenames, long_data, multipliers)","c5a8c49d":"if do_inference:\n    short_data[\"PredictionString\"] = short_data[\"id\"].map(output_short[1])\n    long_data[\"PredictionString\"] = long_data[\"id\"].map(output_long[1])","69c4c8f2":"# sub2 = pd.DataFrame(submission, columns=[\"id\", \"PredictionString\"])\n\n# final = pd.concat([sub1, sub2], axis=0, ignore_index=True)\n# final = final.merge(test_data[[\"context\", \"question\", \"id\"]], on=\"id\")","28759b3f":"# # folds have been averaged and have only 1 prediction file \n# if do_inference:\n#     model_fold_preds = {}\n#     with open('xlmr-large-preds.json') as fp:\n#         model_fold_preds[\"xlmr\"] = json.load(fp)\n#     with open('muril-large-preds.json') as fp:\n#         model_fold_preds[\"muril\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     top_k = 4\n#     for i in range(len(short_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(top_k)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(voted_preds)","3ddb6857":"# if do_inference:\n#     model_fold_preds = {}\n#     model = \"xlmr\"\n#     for fold_ in range(10):\n#         with open(f\"top-preds-{model}-f{fold_}.json\") as fp:\n#             model_fold_preds[f\"{model}-{fold_}\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     top_k = 100\n#     for i in range(len(short_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(10)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     short_data[\"PredictionString\"] = short_data[\"id\"].map(voted_preds)","03f5eda7":"# if do_inference:\n#     model_fold_preds = {}\n    \n#     model = \"bb\"\n#     for fold_ in range(7):\n#         with open(f\"top-preds-{model}-f{fold_}.json\") as fp:\n#             model_fold_preds[f\"{model}-{fold_}\"] = json.load(fp)\n\n#     from collections import Counter\n\n#     voted_preds = {}\n#     for i in range(len(long_data)):\n#         cnt = Counter()\n#         for fold_, preds in enumerate(model_fold_preds.values()):\n#             cnt.update([text[\"text\"] for text in preds[i][\"predictions\"]][:top_k])\n            \n#         most_common = cnt.most_common(10)\n#         voted_preds[preds[i][\"id\"]] = most_common[0][0]\n        \n#     long_data[\"PredictionString\"] = long_data[\"id\"].map(voted_preds)","e6ecfe84":"if do_inference:\n    test_data = pd.concat([short_data, long_data], axis=0, ignore_index=True)","3b2e6d29":"# if do_inference:\n#     test_data = short_data.copy()","1b578b43":"test_data","ffd9f44a":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\", \":\"]\nbad_endings = [\"-\", \"(\", \")\", \"\u2013\", \",\", \";\", \":\"]\n\ntamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\ntamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\ntamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\nhindi_ad = \"\u0908\"\nhindi_bc = \"\u0908.\u092a\u0942\"\n\ncleaned_preds = []\nif do_inference:\n    for pred, context in test_data[[\"PredictionString\", \"context\"]].values:\n        pred = pred.strip()\n        if pred == \"\":\n            cleaned_preds.append(pred)\n            continue\n\n        # I haven't check sure if this makes a difference, but there is one answer in the training set that ends like this and I think it is an annotator mistake\n        # see my notebook here for details https:\/\/www.kaggle.com\/nbroad\/chaii-qa-character-token-languages-eda \n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n\n        pred = pred.lstrip(\"\".join(bad_starts))\n        pred = pred.rstrip(\"\".join(bad_endings))\n\n        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n            pred = pred+\".\"\n\n\n        cleaned_preds.append(pred)\n\n    test_data[\"PredictionString\"] = cleaned_preds","fa37d57d":"if do_inference:\n    test_data[\"pred_len\"] = [len(x) for x in test_data[\"PredictionString\"]]\n# do something if the prediction is too short","ee884b6d":"if do_inference:\n    test_data[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)\nelse:\n    test_data[\"PredictionString\"] = \"lol\"\n    test_data[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)","98a53ed8":"if do_inference:\n    print(test_data[[\"id\", \"PredictionString\"]])","9aa998e5":"# Please excuse how messy it is \ud83d\ude31","ad47e85f":"# MuRIL BigBird Base  (I ended up not using this)\nAll data  \n4k max length","aa33710c":"# MuRIL Large","9d48fe05":"## Attempts at voting that didn't work","abfe9bdd":"# RemBERT\n384_128  \nShort Data","293ad8c4":"# XLMR-Large 6 folds\n\n384_128  \nShort data","b2e5281c":"# MuRIL BigBird Large\n1k max length  \nALL data","cd553bf7":"## Char-level averaging that never got finished"}}