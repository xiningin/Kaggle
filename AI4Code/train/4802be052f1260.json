{"cell_type":{"0192feb1":"code","98f8d882":"code","3b60f135":"code","4a3ec16e":"code","cc281a3e":"code","0f5f8ca5":"code","72ff77b9":"code","7ba2ff62":"code","40bca4cf":"code","a8c2365b":"code","fe25e804":"code","6e9d1ff2":"code","7a870682":"code","f92a844b":"code","b8d101e8":"code","7f47b2e5":"code","12e129dc":"code","6abf6864":"code","b4534cda":"code","35ca3373":"code","8bf9115d":"code","f048e595":"code","a08686de":"code","5faad480":"code","c548b5c4":"markdown","a9e6f9ad":"markdown","68e13dd1":"markdown","981c9cd2":"markdown","15632180":"markdown","6ee8ad10":"markdown","85ad1905":"markdown","321fcd6a":"markdown"},"source":{"0192feb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98f8d882":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv').drop(['id','Unnamed: 32'],axis = 1)\nprint('Data Shape',df.shape)\ndf['diagnosis'] = df['diagnosis'].replace({'M':1,'B':0})","3b60f135":"df.info()","4a3ec16e":"df.head()","cc281a3e":"#Change Diagnosis to be last column\ndf['target'] = df['diagnosis'].copy()\ndf.drop(['diagnosis'],axis = 1,inplace = True)\ndf.head()","0f5f8ca5":"corr = df.corr()","72ff77b9":"plt.figure(figsize =(20,32))\nn = 0\nfor i in list(df.iloc[:,:-1].columns):\n        n += 1\n        plt.subplot(10,3,n)\n        plt.subplots_adjust(hspace = 0.5,wspace = 0.2)\n        sns.boxplot(x=df[i])\nplt.show()","7ba2ff62":"corr_target = (df[df.columns[0:]].corr()['target'][:-1]).to_frame()\nplt.figure(1,figsize =(20,12))\nsns.barplot(y = corr_target.index,x = corr_target['target'],data = corr_target, orient = \"h\")\nplt.title('Correlation of Target to other features in mean')\nplt.ylabel('Correlation with Target')\nplt.xlabel('Features')\nplt.show()","40bca4cf":"plt.figure(figsize = (12,12))\nthresh = 0.7\nmask = np.abs(corr[\"target\"]) > thresh\nheavy_features = corr.columns[mask].tolist()\nsns.heatmap(df[heavy_features].corr(),annot=True,cmap = 'coolwarm');\nplt.title(\"Correlation Between Features w 0.75 Threshold\");\nplt.show();","a8c2365b":"print('Features that heavily influence target are: ', heavy_features)","fe25e804":"g = sns.pairplot(df.loc[:,heavy_features],hue = 'target',diag_kind=\"kde\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","6e9d1ff2":"sns.relplot(data=df.loc[:,heavy_features],x=\"concave points_worst\", y=\"area_worst\",\n    hue=\"target\", size=\"radius_worst\",\n    palette='seismic', sizes=(10,200))\nsns.relplot(data=df.loc[:,heavy_features],x=\"concave points_mean\", y=\"area_mean\",\n    hue=\"target\", size=\"radius_mean\",\n    palette='seismic', sizes=(10,200))\nplt.show()","7a870682":"cols = list(df.iloc[:,:-1].columns)\ntarget = ['target']","f92a844b":"# Scaling the features since, PCA is sensitive to scale\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Separating out the features\nX = df.loc[:, cols].values\n# Separating out the target\ny = df.loc[:,target].values\n# Standardizing the features\nX = scaler.fit_transform(X)\n\n# Pricipal Component Analysis\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 6)\nprincipalComponents = pca.fit_transform(X)\npc_df = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2','pc3','pc4','pc5','pc6'])\npc_df = pd.concat([pc_df, df[target]], axis = 1)","b8d101e8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV","7f47b2e5":"def grid_search(est,param_grid):\n    fin_model = GridSearchCV(estimator=est,param_grid=param_grid,cv = 10,n_jobs = -1)\n    return fin_model","12e129dc":"def neigh(X_train,y_train,X_test,y_test):\n    pipe = Pipeline([('neigh', KNeighborsClassifier())])\n    param_grid = {'neigh__n_neighbors':[i for i in range(0,36)],\n                 'neigh__weights':['uniform', 'distance']}\n    pipe = grid_search(pipe,param_grid)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    print('KNN_CLASSIFIER: ')\n    print(classification_report(y_test, y_pred,zero_division = 1))\n    print('PIPE SCORE: ', pipe.score(X_test, y_test))\n    print('ACCURACY SCORE: ',accuracy_score(y_test, y_pred))\n    print(pipe.best_params_)","6abf6864":"def forest(X_train,y_train,X_test,y_test):\n    #Parameter Grid\n    pipe = Pipeline([('forest', RandomForestClassifier(random_state = 0))])\n    param_grid = {'forest__n_estimators':[10,20,50,100,150,200],\n                 'forest__max_depth' : [1,2,3,4,5,6],\n                 'forest__max_features':['sqrt', 'log2']}\n    pipe = grid_search(pipe,param_grid)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    #Evaluation Phase\n    print('RANDOM FOREST: ')\n    print(classification_report(y_test, y_pred,zero_division = 1))\n    print('PIPE SCORE: ', pipe.score(X_test, y_test))\n    print('ACCURACY SCORE: ',accuracy_score(y_test, y_pred))\n    print(pipe.best_params_)","b4534cda":"def svm(X_train,y_train,X_test,y_test):\n    pipe = Pipeline([('svm', SVC(gamma = 'auto',random_state = 0))])\n    param_grid = {'svm__kernel':['linear','poly', 'rbf'],\n                 'svm__degree':[1,2,3,4,5],\n                 'svm__C' : [1,2,3,4,5]}\n    pipe = grid_search(pipe,param_grid)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    print('SVM: ')\n    print(classification_report(y_test, y_pred,zero_division = 1))\n    print('PIPE SCORE: ', pipe.score(X_test, y_test))\n    print('ACCURACY SCORE: ',accuracy_score(y_test, y_pred))\n    print(pipe.best_params_)","35ca3373":"X = pc_df.iloc[:,:-1]\ny = pc_df.iloc[:,-1]\nfrom sklearn.model_selection import train_test_split as tts\nX_train,X_test,y_train,y_test = tts(X,y,test_size = 0.2,random_state = 7)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","8bf9115d":"neigh(X_train,y_train,X_test,y_test)","f048e595":"forest(X_train,y_train,X_test,y_test)","a08686de":"svm(X_train,y_train,X_test,y_test)","5faad480":"from xgboost import XGBClassifier\npipe = Pipeline([('xgb', XGBClassifier(random_state = 0))])\nparam_grid = {'xgb__learning_rate':[0.001,0.003,0.005,0.05],\n                 'xgb__base_score': [1,0.5,0.3,0.1],\n                 'xgb__importance_type': ['gain','entropy']}\npipe = grid_search(pipe,param_grid)\npipe.fit(X_train,y_train)\n#make predictions for test data\ny_pred = pipe.predict(X_test)\nprint(classification_report(y_test, y_pred,zero_division = 1))\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nprint(pipe.best_params_)","c548b5c4":"### **Principal Component Analysis can be performed on the above data, since the points ares heavily correlated.**","a9e6f9ad":"Tumors can be benign (noncancerous) or malignant (cancerous). \nBenign tumors tend to grow slowly and do not spread. \nMalignant tumors can grow rapidly, invade and destroy nearby normal tissues, and spread throughout the body.","68e13dd1":"# Model Building","981c9cd2":"# DATA","15632180":"## PCA","6ee8ad10":" ### A suitable threshold will be 0.7","85ad1905":"# Exploratory Data Analysis","321fcd6a":"Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29()\n\nAttribute Information:\n\n* 1) Diagnosis (1 = malignant, 0 = benign)\n\nTen real-valued features are computed for each cell nucleus:**(2-31)**\n\n\n* a) radius (mean of distances from center to points on the perimeter)\n* b) texture (standard deviation of gray-scale values)\n* c) perimeter\n* d) area\n* e) smoothness (local variation in radius lengths)\n* f) compactness (perimeter^2 \/ area - 1.0)\n* g) concavity (severity of concave portions of the contour)\n* h) concave points (number of concave portions of the contour)\n* i) symmetry\n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 2 is Mean Radius, field\n12 is Radius SE, field 22 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant"}}