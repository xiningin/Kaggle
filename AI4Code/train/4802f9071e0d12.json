{"cell_type":{"758fed07":"code","98ea07ec":"code","a8775ef0":"code","ca853ed9":"code","c47e6248":"code","976c35f7":"code","5f025fc4":"code","d52f847f":"code","f4f22d72":"code","09571676":"code","3362740e":"code","4a90bd4f":"code","5cbd950d":"code","2f5e5d53":"code","c2b155e7":"code","76a952dd":"markdown","69fdf2c3":"markdown","ef7a3c40":"markdown","9da603c7":"markdown","382069cd":"markdown","881db09a":"markdown"},"source":{"758fed07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport sys\n#import matplotlib as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nimport sklearn\n\n\nprint('Python: {}'.format(sys.version))\nprint('Numpy: {}'.format(np.__version__))\nprint('Pandas: {}'.format(pd.__version__))\n#print('MatplotLib: {}'.format(plt.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\n# Any results you write to the current directory are saved as output.","98ea07ec":"data=pd.read_csv(\"..\/input\/creditcard.csv\")\n","a8775ef0":"data.columns","ca853ed9":"data.shape","c47e6248":"data.describe()","976c35f7":"data= data.sample(frac=0.1,random_state=1)\ndata.shape","5f025fc4":"data.hist(figsize = (40,30))\nplt.show","d52f847f":"fraud =data[data[\"Class\"]==1]\nvalid =data[data[\"Class\"]==0]\n\noutlier_fraction = len(fraud) \/ float(len(valid))\nprint(outlier_fraction)\n\nprint(\"Fraud Case: {}\".format(len(fraud)))\nprint(\"Valid Case: {}\".format(len(valid)))","f4f22d72":"#Correlation Matrix\ncorrmat=data.corr().abs()\nplt.figure(figsize=(12,9))\nsns.heatmap(corrmat,vmax=0.8,square=True)\nplt.show()","09571676":"data.corr()[\"Class\"].sort_values()","3362740e":"#get all the columns from the dataframe\ncolumns = data.columns.tolist()\n\n#filter the column to remove data dnt want\ncolumns =[c for c in columns if c not in [\"Class\"]]\n\n#Store the variable we will be predicting \ntarget =\"Class\"\n\nX=data[columns]\nY=data[target]\n\nX.shape","4a90bd4f":"Y.shape","5cbd950d":"from sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","2f5e5d53":"# define a random state\nstate = 1\n\n# define the outlier detection methods\nclassifiers = {\n        \"Isolation Forest\": IsolationForest(max_samples=len(X),\n                                                   contamination= outlier_fraction,\n                                                   random_state = state),\n        \"Local Outlier Factor \":LocalOutlierFactor(n_neighbors =20,novelty=True,\n                                                          contamination=outlier_fraction)\n    }","c2b155e7":"#fit the model\nn_outlier=len(fraud)\n\nfor i ,(clf_name,clf ) in enumerate(classifiers.items()):\n    #fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\" :\n        y_pred = clf.fit_predict(X)\n        scores_pred =clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred=clf.predict(X)\n    #Reshape the prediction values to 0 for valid , 1 for fraud\n    y_pred[y_pred == 1] =0\n    y_pred[y_pred == -1] = 1\n    \n    n_errors = (y_pred != Y).sum()\n    \n    #Run Classification Mertices\n    print('{}: {}'.format(clf_name,n_errors))\n    print('accuracy_score: {}'.format(accuracy_score(Y,y_pred)))\n    print('Classification Report: {}'.format(classification_report(Y,y_pred)))","76a952dd":"    Count for all the columns are equal so we are not missing any values here . \n        ","69fdf2c3":"LOF - unsupervised - calculate the anamoly detection method - anamoly score of each sample - caluclate the \ndeviation of density how isolated the object with surrounded neighbors - sameway as KNN - \ntwo common anamoly detection method - \n\nIsolation Forest - return the anamoly score of each sample - isolate the observation by randomly select the features - tree structures from root node to leaf","ef7a3c40":"            # Load the dataset creditcard.csv","9da603c7":"As wer have 284807 large datasets so here to reduce the computation time i am sample only a fracaton of this database.","382069cd":"        Class is highly correlated with v11,v4,v14,v17","881db09a":"Very few 1 classes found - so we find the number of Fraud Cases in our dataset"}}