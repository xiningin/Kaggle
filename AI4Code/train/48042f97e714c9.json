{"cell_type":{"fe55bfd3":"code","703d9c0b":"code","a69e8f9b":"code","7994c921":"code","691f16d0":"code","caae094d":"code","480ae0fa":"code","3fa62db3":"code","b4b629f0":"code","9a43b2bb":"code","168bfdab":"code","f3054d43":"code","b30b7182":"code","496881be":"code","7a280820":"code","8280460f":"code","d70bbeee":"code","4703bf6d":"code","f57b10ac":"code","fbafb4d6":"code","f8f88359":"code","617b388a":"code","352fad8a":"code","3418543b":"code","50f75a2b":"code","a0d95ce2":"code","a8b61606":"code","863d8a8e":"code","47b956c2":"code","35ea72fe":"code","0d059950":"code","ffda0bcf":"code","b649f299":"code","2f9fc8ac":"markdown","e9c54dfc":"markdown","53aa638a":"markdown","ca95d526":"markdown","744e8bd8":"markdown","16e1de35":"markdown","447de22e":"markdown","123ea399":"markdown","7f795d09":"markdown","5d778906":"markdown","49661d69":"markdown","8cb8921f":"markdown","42c44796":"markdown","43ad8e6f":"markdown","1e5e15c7":"markdown","04dd4531":"markdown","85b8895f":"markdown","b4301f06":"markdown","3c5f3339":"markdown","a4eacbf6":"markdown","bac0289d":"markdown","cef17b1e":"markdown","abad1d7c":"markdown","ad0347c2":"markdown","4ed2a546":"markdown","5fa254aa":"markdown","9e8064ff":"markdown","9c912965":"markdown","815259f0":"markdown","ca74d19f":"markdown","09eb7fc0":"markdown","f58f0945":"markdown","32cae27e":"markdown","3c2c8bd6":"markdown","a213ef2e":"markdown","46e8ff9c":"markdown","6abd6f04":"markdown","d1b365d0":"markdown","fd51a103":"markdown","c29d5404":"markdown","1cb1e63e":"markdown","b3699171":"markdown","531384fe":"markdown","478cad39":"markdown","cbbdfa60":"markdown","15c67e31":"markdown"},"source":{"fe55bfd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom datetime import datetime, timedelta, date\nimport statsmodels.formula.api as smf \nimport scipy\nfrom scipy import stats\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","703d9c0b":"mkt = pd.read_csv(\"\/kaggle\/input\/marketing-data\/marketing_data.csv\")","a69e8f9b":"mkt.isna().sum()","7994c921":"# Income\nmkt = mkt.rename(columns={' Income ':'Income'})\nmkt['Income'] = mkt['Income'].replace({'\\$': '', ',': ''}, regex=True)\nmkt['Income'] = pd.to_numeric(mkt['Income'])\n\n# Dt_Customer\ntoday = datetime.now()\nmkt['Dt_Customer']= pd.to_datetime(mkt['Dt_Customer'])\nmkt['Dt_Customer'] = (today - mkt['Dt_Customer']).dt.days","691f16d0":"f, axs = plt.subplots(7,4,figsize=(15,30))\nmkt['Year_Birth'].plot(kind='box', ax=axs[0,0])\naxs[0,0].title.set_text('Year_Birth')\nmkt['Education'].value_counts().plot(kind='bar',ax=axs[0,1])\naxs[0,1].title.set_text('Education')\nmkt['Marital_Status'].value_counts().plot(kind='bar',ax=axs[0,2])\naxs[0,2].title.set_text('Marital_Status')\nmkt['Income'].plot(kind='box', ax=axs[0,3])\naxs[0,3].title.set_text('Income')\nmkt['Kidhome'].value_counts().plot(kind='bar',ax=axs[1,0])\naxs[1,0].title.set_text('Kidhome')\nmkt['Teenhome'].value_counts().plot(kind='bar',ax=axs[1,1])\naxs[1,1].title.set_text('Teenhome')\nmkt['Dt_Customer'].plot(kind='box', ax=axs[1,2])\naxs[1,2].title.set_text('Dt_Customer')\nmkt['Recency'].plot(kind='box', ax=axs[1,3])\naxs[1,3].title.set_text('Recency')\nmkt['MntWines'].plot(kind='box', ax=axs[2,0])\naxs[2,0].title.set_text('MntWines')\nmkt['MntFruits'].plot(kind='box', ax=axs[2,1])\naxs[2,1].title.set_text('MntFruits')\nmkt['MntMeatProducts'].plot(kind='box', ax=axs[2,2])\naxs[2,2].title.set_text('MntMeatProducts')\nmkt['MntFishProducts'].plot(kind='box', ax=axs[2,3])\naxs[2,3].title.set_text('MntFishProducts')\nmkt['MntSweetProducts'].plot(kind='box', ax=axs[3,0])\naxs[3,0].title.set_text('MntSweetProducts')\nmkt['MntGoldProds'].plot(kind='box', ax=axs[3,1])\naxs[3,1].title.set_text('MntGoldProds')\nmkt['NumDealsPurchases'].plot(kind='box', ax=axs[3,2])\naxs[3,2].title.set_text('NumDealsPurchases')\nmkt['NumWebPurchases'].plot(kind='box', ax=axs[3,3])\naxs[3,3].title.set_text('NumWebPurchases')\nmkt['NumCatalogPurchases'].plot(kind='box', ax=axs[4,0])\naxs[4,0].title.set_text('NumCatalogPurchases')\nmkt['NumStorePurchases'].plot(kind='box', ax=axs[4,1])\naxs[4,1].title.set_text('NumStorePurchases')\nmkt['NumWebVisitsMonth'].plot(kind='box', ax=axs[4,2])\naxs[4,2].title.set_text('NumWebVisitsMonth')\nmkt['AcceptedCmp3'].value_counts().plot(kind='bar',ax=axs[4,3])\naxs[4,3].title.set_text('AcceptedCmp3')\nmkt['AcceptedCmp4'].value_counts().plot(kind='bar',ax=axs[5,0])\naxs[5,0].title.set_text('AcceptedCmp4')\nmkt['AcceptedCmp5'].value_counts().plot(kind='bar',ax=axs[5,1])\naxs[5,1].title.set_text('AcceptedCmp5')\nmkt['AcceptedCmp1'].value_counts().plot(kind='bar',ax=axs[5,2])\naxs[5,2].title.set_text('AcceptedCmp1')\nmkt['AcceptedCmp2'].value_counts().plot(kind='bar',ax=axs[5,3])\naxs[5,3].title.set_text('AcceptedCmp2')\nmkt['Response'].value_counts().plot(kind='bar',ax=axs[6,0])\naxs[6,0].title.set_text('Response')\nmkt['Complain'].value_counts().plot(kind='bar',ax=axs[6,1])\naxs[6,1].title.set_text('Complain')\nmkt['Country'].value_counts().plot(kind='bar',ax=axs[6,2])\naxs[6,2].title.set_text('Country')\n\nf.delaxes(axs[6][3])\nf.tight_layout()\nplt.show()","caae094d":"mkt.loc[mkt.Income.isna(),'Income'] = mkt.Income.median()\nmkt.loc[mkt.Income > 600000,'Income'] = mkt.Income.median()","480ae0fa":"# Marital_Status\nmkt = mkt[~mkt.Marital_Status.isin(['Alone','YOLO','Absurd'])]\n\n# Year_Birth\nmkt.loc[mkt.Year_Birth < 1920,'Year_Birth'] = round(mkt[mkt.Year_Birth > 1920].Year_Birth.mean())\n\n# Kidhome\nmkt['HasKid'] = np.where(mkt['Kidhome'] > 0, 1, 0)\n\n# Teenhome\nmkt['HasTeen'] = np.where(mkt['Teenhome'] > 0, 1, 0)","3fa62db3":"f, axs = plt.subplots(8,4,figsize=(15,30))\nmkt['Year_Birth'].plot(kind='box', ax=axs[0,0])\naxs[0,0].title.set_text('Year_Birth')\nmkt['Education'].value_counts().plot(kind='bar',ax=axs[0,1])\naxs[0,1].title.set_text('Education')\nmkt['Marital_Status'].value_counts().plot(kind='bar',ax=axs[0,2])\naxs[0,2].title.set_text('Marital_Status')\nmkt['Income'].plot(kind='box', ax=axs[0,3])\naxs[0,3].title.set_text('Income')\nmkt['Kidhome'].value_counts().plot(kind='bar',ax=axs[1,0])\naxs[1,0].title.set_text('Kidhome')\nmkt['Teenhome'].value_counts().plot(kind='bar',ax=axs[1,1])\naxs[1,1].title.set_text('Teenhome')\nmkt['Dt_Customer'].plot(kind='box', ax=axs[1,2])\naxs[1,2].title.set_text('Dt_Customer')\nmkt['Recency'].plot(kind='box', ax=axs[1,3])\naxs[1,3].title.set_text('Recency')\nmkt['MntWines'].plot(kind='box', ax=axs[2,0])\naxs[2,0].title.set_text('MntWines')\nmkt['MntFruits'].plot(kind='box', ax=axs[2,1])\naxs[2,1].title.set_text('MntFruits')\nmkt['MntMeatProducts'].plot(kind='box', ax=axs[2,2])\naxs[2,2].title.set_text('MntMeatProducts')\nmkt['MntFishProducts'].plot(kind='box', ax=axs[2,3])\naxs[2,3].title.set_text('MntFishProducts')\nmkt['MntSweetProducts'].plot(kind='box', ax=axs[3,0])\naxs[3,0].title.set_text('MntSweetProducts')\nmkt['MntGoldProds'].plot(kind='box', ax=axs[3,1])\naxs[3,1].title.set_text('MntGoldProds')\nmkt['NumDealsPurchases'].plot(kind='box', ax=axs[3,2])\naxs[3,2].title.set_text('NumDealsPurchases')\nmkt['NumWebPurchases'].plot(kind='box', ax=axs[3,3])\naxs[3,3].title.set_text('NumWebPurchases')\nmkt['NumCatalogPurchases'].plot(kind='box', ax=axs[4,0])\naxs[4,0].title.set_text('NumCatalogPurchases')\nmkt['NumStorePurchases'].plot(kind='box', ax=axs[4,1])\naxs[4,1].title.set_text('NumStorePurchases')\nmkt['NumWebVisitsMonth'].plot(kind='box', ax=axs[4,2])\naxs[4,2].title.set_text('NumWebVisitsMonth')\nmkt['AcceptedCmp3'].value_counts().plot(kind='bar',ax=axs[4,3])\naxs[4,3].title.set_text('AcceptedCmp3')\nmkt['AcceptedCmp4'].value_counts().plot(kind='bar',ax=axs[5,0])\naxs[5,0].title.set_text('AcceptedCmp4')\nmkt['AcceptedCmp5'].value_counts().plot(kind='bar',ax=axs[5,1])\naxs[5,1].title.set_text('AcceptedCmp5')\nmkt['AcceptedCmp1'].value_counts().plot(kind='bar',ax=axs[5,2])\naxs[5,2].title.set_text('AcceptedCmp1')\nmkt['AcceptedCmp2'].value_counts().plot(kind='bar',ax=axs[5,3])\naxs[5,3].title.set_text('AcceptedCmp2')\nmkt['Response'].value_counts().plot(kind='bar',ax=axs[6,0])\naxs[6,0].title.set_text('Response')\nmkt['Complain'].value_counts().plot(kind='bar',ax=axs[6,1])\naxs[6,1].title.set_text('Complain')\nmkt['Country'].value_counts().plot(kind='bar',ax=axs[6,2])\naxs[6,2].title.set_text('Country')\nmkt['HasKid'].value_counts().plot(kind='bar',ax=axs[6,3])\naxs[6,3].title.set_text('HasKid')\nmkt['HasTeen'].value_counts().plot(kind='bar',ax=axs[7,0])\naxs[7,0].title.set_text('HasTeen')\n\nf.delaxes(axs[7][1])\nf.delaxes(axs[7][2])\nf.delaxes(axs[7][3])\nf.tight_layout()\nplt.show()","b4b629f0":"mkt_dummy = pd.get_dummies(mkt, columns=['Education', 'Marital_Status', 'Kidhome', 'Teenhome', \n                'Country'], drop_first=True, prefix=['Education', 'Marital_Status', \n                'Kidhome', 'Teenhome', 'Country'], prefix_sep='_')","9a43b2bb":"def norm_func(i):\n    x = (i-i.min())\t\/ (i.max()-i.min())\n    return (x)\n\nmkt_scale = norm_func(mkt_dummy.iloc[:,:]) ","168bfdab":"ml1 = smf.ols('NumStorePurchases ~ Year_Birth+Income+Dt_Customer+Recency+MntWines+MntFruits+MntMeatProducts+\\\n              MntFishProducts+MntSweetProducts+MntGoldProds+NumDealsPurchases+NumWebPurchases+NumCatalogPurchases\\\n              +NumWebVisitsMonth+AcceptedCmp3+AcceptedCmp4+AcceptedCmp5+AcceptedCmp1+AcceptedCmp2+Response+\\\n              Complain+Education_Basic+Education_Graduation+Education_Master+Education_PhD+\\\n              Marital_Status_Married+Marital_Status_Single+Marital_Status_Together+Marital_Status_Widow+\\\n              Kidhome_1+Kidhome_2+Teenhome_1+Teenhome_2+Country_CA+Country_GER+Country_IND+Country_ME+Country_SA\\\n              +Country_SP+Country_US', data=mkt_scale).fit()\n\nml1.summary()","f3054d43":"params = ml1.params.sort_values(ascending=False)\nplt.figure(figsize=(15, 4)) \nplt.bar(params.index, params)\nplt.xticks(rotation=90)\nplt.title('Variable Scores')\nplt.xlabel('Variable')\nplt.ylabel('Coefficient')\nplt.show()","b30b7182":"param_narrow = params[params>0.1].append(params[params<-0.1])\nparam_narrow=param_narrow[~(param_narrow.index=='Intercept')]\nplt.figure(figsize=(8, 4)) \nplt.bar(param_narrow.index, param_narrow)\nplt.xticks(rotation=90)\nplt.title('Variable Scores')\nplt.xlabel('Variable')\nplt.ylabel('Coefficient')\nplt.show()","496881be":"total_purchase = mkt.loc[:,['NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','Country']]\ntotal_purchase = total_purchase.groupby('Country').sum().sum(axis=1)\n\ntotal_purchase = total_purchase.sort_values(ascending=False)\nplt.figure(figsize=(8, 4)) \nplt.bar(total_purchase.index, total_purchase)\nplt.bar('US',total_purchase['US'])\nplt.xticks(rotation=90)\nplt.title('Total Purchases Across Countries')\nplt.xlabel('Country')\nplt.ylabel('Total Purchases')\nplt.show()","7a280820":"number_cust = mkt.loc[:,['Country']]\nnumber_cust = number_cust.value_counts()\n\nave_purchase = (total_purchase \/ number_cust.to_numpy()).sort_values(ascending=False)\n\nplt.figure(figsize=(8, 4)) \nplt.bar(ave_purchase.index, ave_purchase)\nplt.bar('US',ave_purchase['US'])\nplt.xticks(rotation=90)\nplt.title('Average Purchases Across Countries')\nplt.xlabel('Country')\nplt.ylabel('Average Purchases')\nplt.show()\n","8280460f":"total_value = mkt.loc[:,['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','Country']]\ntotal_value = total_value.groupby('Country').sum().sum(axis=1)\ntotal_value = total_value.sort_values(ascending=False)\n\nplt.figure(figsize=(8, 4)) \nplt.bar(total_value.index, total_value)\nplt.bar('US',total_value['US'])\nplt.xticks(rotation=90)\nplt.title('Total Purchase Value Across Countries')\nplt.xlabel('Country')\nplt.ylabel('Total Purchase Value')\nplt.show()","d70bbeee":"ave_value = (total_value \/ number_cust.to_numpy()).sort_values(ascending=False)\n\nplt.figure(figsize=(8, 4)) \nplt.bar(ave_value.index, ave_value)\nplt.bar('US',ave_value['US'])\nplt.xticks(rotation=90)\nplt.title('Average Purchase Value Across Countries')\nplt.xlabel('Country')\nplt.ylabel('Average Purchase Value')\nplt.show()","4703bf6d":"gold_purchase = mkt.loc[:,['MntGoldProds','NumStorePurchases']]\ngold_purchase['AboveAvg'] = np.where(gold_purchase['MntGoldProds'] > gold_purchase['MntGoldProds'].mean(),'Above','Below')\n\n# Preparing data for gold buyers & non gold buyers\ngold_buyers = gold_purchase[gold_purchase.AboveAvg=='Above'].loc[:,'NumStorePurchases']\nnon_buyers = gold_purchase[gold_purchase.AboveAvg=='Below'].loc[:,'NumStorePurchases']\n\nprint(scipy.stats.levene(gold_buyers,non_buyers))\n","f57b10ac":"from scipy.stats import ttest_ind  \n    \ndef t_test(x,y,alternative='equal'):\n        _, double_p = ttest_ind(x,y,equal_var = False)\n        if alternative == 'equal':\n            pval = double_p\n        elif alternative == 'greater':\n            if np.mean(x) > np.mean(y):\n                pval = double_p\/2.\n            else:\n                pval = 1.0 - double_p\/2.\n        elif alternative == 'less':\n            if np.mean(x) < np.mean(y):\n                pval = double_p\/2.\n            else:\n                pval = 1.0 - double_p\/2.\n        return pval\n\nprint(\"At 0.05 significance level, \\n\")    \n    \n# Two tailed T-Test\np = t_test(gold_buyers,non_buyers,alternative='equal')\nif p > 0.05:\n    print(\"For 2-tailed test:\\nWith P-Val of\", p, \"we fail to reject the null hypothesis. Population means are equal\\n\")\nelse:\n    print(\"For 2-tailed test:\\nWith P-Val of\", p, \"null hypothesis is rejected. Population means are not equal\\n\")\n\n# One tailed T-Test\np2 = t_test(gold_buyers,non_buyers,alternative='greater')\nif p2 > 0.05:\n    print(\"For 1-tailed test:\\nWith P-Val of\", p2, \"we fail to reject the null hypothesis. \\nStore purchases of people who spend more on gold < Store purchases of people who spend less\")\nelse:\n    print(\"For 1-tailed test:\\nWith P-Val of\", p2, \"null hypothesis is rejected. \\nStore purchases of people who spend more on gold > Store purchases of people who spend less\")\n","fbafb4d6":"ml2 = smf.ols('MntFishProducts ~ Year_Birth+Income+Dt_Customer+Recency+MntWines+MntFruits+MntMeatProducts+\\\n              MntSweetProducts+MntGoldProds+NumDealsPurchases+NumWebPurchases+NumCatalogPurchases+\\\n              NumWebVisitsMonth+NumStorePurchases+AcceptedCmp3+AcceptedCmp4+AcceptedCmp5+AcceptedCmp1+\\\n              AcceptedCmp2+Response+Complain+Education_Basic+Education_Graduation+Education_Master+Education_PhD\\\n              +Marital_Status_Married+Marital_Status_Single+Marital_Status_Together+Marital_Status_Widow+\\\n              HasKid+HasTeen+Country_CA+Country_GER+Country_IND+Country_ME+Country_SA\\\n              +Country_SP+Country_US+Marital_Status_Married*Education_PhD', data=mkt_scale).fit()\n\nml2.summary()","f8f88359":"params = ml2.params.sort_values(ascending=False)\nplt.figure(figsize=(15, 4)) \nplt.bar(params.index, params)\nplt.xticks(rotation=90)\nplt.title('Variable Scores')\nplt.xlabel('Variable')\nplt.ylabel('Coefficient')\nplt.show()","617b388a":"param_narrow = params[params>0.1].append(params[params<-0.1])\nparam_narrow=param_narrow[~(param_narrow.index=='Intercept')]\nparam_narrow.append(params[params.index=='Marital_Status_Married:Education_PhD'])\nplt.figure(figsize=(8, 4)) \nplt.bar(param_narrow.index, param_narrow)\nplt.bar('Marital_Status_Married:Education_PhD',params[params.index=='Marital_Status_Married:Education_PhD'])\nplt.xticks(rotation=90)\nplt.title('Variable Scores')\nplt.xlabel('Variable')\nplt.ylabel('Coefficient')\nplt.show()","352fad8a":"# Target variable \"Response\", which is also the latest campaign ran\n\nX = mkt_scale.drop(['ID','HasKid','HasTeen','Response'],axis=1)\ny = mkt_scale.Response\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nplt.figure(figsize=(15, 4)) \nplt.bar(X.columns[:-7], importance[:-7])\nplt.bar(X.columns[-7:], importance[-7:])\nplt.xticks(rotation=90)\nplt.title('Feature Importance for Latest Campaign \"Response\"')\nplt.xlabel('Feature')\nplt.ylabel('Coefficient')\nplt.show()","3418543b":"# Target variable \"AcceptedCmp5\", which is the fifth campaign ran\n\nX = mkt_scale.drop(['ID','HasKid','HasTeen','AcceptedCmp5'],axis=1)\ny = mkt_scale.AcceptedCmp5\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nplt.figure(figsize=(15, 4)) \nplt.bar(X.columns[:-7], importance[:-7])\nplt.bar(X.columns[-7:], importance[-7:])\nplt.xticks(rotation=90)\nplt.title('Feature Importance for Campaign 5')\nplt.xlabel('Feature')\nplt.ylabel('Coefficient')\nplt.show()","50f75a2b":"# Target variable \"AcceptedCmp4\", which is the fourth campaign ran\n\nX = mkt_scale.drop(['ID','HasKid','HasTeen','AcceptedCmp4'],axis=1)\ny = mkt_scale.AcceptedCmp4\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nplt.figure(figsize=(15, 4)) \nplt.bar(X.columns[:-7], importance[:-7])\nplt.bar(X.columns[-7:], importance[-7:])\nplt.xticks(rotation=90)\nplt.title('Feature Importance for Campaign 4')\nplt.xlabel('Feature')\nplt.ylabel('Coefficient')\nplt.show()","a0d95ce2":"# Target variable \"AcceptedCmp3\", which is the third campaign ran\n\nX = mkt_scale.drop(['ID','HasKid','HasTeen','AcceptedCmp3'],axis=1)\ny = mkt_scale.AcceptedCmp3\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nplt.figure(figsize=(15, 4)) \nplt.bar(X.columns[:-7], importance[:-7])\nplt.bar(X.columns[-7:], importance[-7:])\nplt.xticks(rotation=90)\nplt.title('Feature Importance for Campaign 3')\nplt.xlabel('Feature')\nplt.ylabel('Coefficient')\nplt.show()","a8b61606":"# Target variable \"AcceptedCmp2\", which is the second campaign ran\n\nX = mkt_scale.drop(['ID','HasKid','HasTeen','AcceptedCmp2'],axis=1)\ny = mkt_scale.AcceptedCmp2\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nplt.figure(figsize=(15, 4)) \nplt.bar(X.columns[:-7], importance[:-7])\nplt.bar(X.columns[-7:], importance[-7:])\nplt.xticks(rotation=90)\nplt.title('Feature Importance for Campaign 2')\nplt.xlabel('Feature')\nplt.ylabel('Coefficient')\nplt.show()","863d8a8e":"# Target variable \"AcceptedCmp1\", which is the first campaign ran\n\nX = mkt_scale.drop(['ID','HasKid','HasTeen','AcceptedCmp1'],axis=1)\ny = mkt_scale.AcceptedCmp1\n# define the model\nmodel = RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nplt.figure(figsize=(15, 4)) \nplt.bar(X.columns[:-7], importance[:-7])\nplt.bar(X.columns[-7:], importance[-7:])\nplt.xticks(rotation=90)\nplt.title('Feature Importance for Campaign 1')\nplt.xlabel('Feature')\nplt.ylabel('Coefficient')\nplt.show()","47b956c2":"campaign = mkt.loc[:,['Response','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','Country']]\ncampaign = campaign.groupby('Country').mean()\n\nf, axs = plt.subplots(3,2,figsize=(15,8))\ncampaign['Response'].plot(kind='bar', ax=axs[0,0])\naxs[0,0].title.set_text('Acceptance % of Latest Campaign')\ncampaign['AcceptedCmp5'].plot(kind='bar', ax=axs[0,1])\naxs[0,1].title.set_text('Acceptance % of Campaign 5')\ncampaign['AcceptedCmp4'].plot(kind='bar', ax=axs[1,0])\naxs[1,0].title.set_text('Acceptance % of Campaign 4')\ncampaign['AcceptedCmp3'].plot(kind='bar', ax=axs[1,1])\naxs[1,1].title.set_text('Acceptance % of Campaign 3')\ncampaign['AcceptedCmp2'].plot(kind='bar', ax=axs[2,0])\naxs[2,0].title.set_text('Acceptance % of Campaign 2')\ncampaign['AcceptedCmp1'].plot(kind='bar', ax=axs[2,1])\naxs[2,1].title.set_text('Acceptance % of Campaign 1')\n\nf.tight_layout()\nplt.show()","35ea72fe":"campaign_takeup = mkt.loc[:,['Response','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']]\n\ncampaign_takeup = campaign_takeup.melt()\ncampaign_takeup = pd.crosstab(campaign_takeup[\"variable\"], campaign_takeup[\"value\"]).sort_values(0)\n\ncols = list(campaign_takeup.columns)\na, b = cols.index(0), cols.index(1)\ncols[b], cols[a] = cols[a], cols[b]\ncampaign_takeup = campaign_takeup[cols]\n\ncampaign_takeup.columns = \"Yes\",\"No\"\ncampaign_takeup.plot.bar(stacked=True)\nplt.title('Acceptance of Marketing Campaigns')\nplt.xlabel('Campaign')\nplt.ylabel('Acceptance')\nplt.legend(title='Response',loc='upper right')\nplt.show()","0d059950":"average_cust = mkt.drop('ID',axis=1).mean()\naverage_cust = pd.DataFrame(average_cust)\naverage_cust.loc['Dt_Customer',:] = str(datetime.today() - timedelta(days = average_cust.loc['Dt_Customer',0]))\nmodal = mkt.drop('ID',axis=1).mode().transpose().loc[['Country','Education','Marital_Status']]\naverage_cust = average_cust.append(modal)\nprint(average_cust)","ffda0bcf":"products = mkt.loc[:,['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds']]\nproducts = products.sum().sort_values(ascending=False)\n\nplt.bar(products.index,products)\nplt.xticks(rotation=90)\nplt.title('Product Performance in last 2 years')\nplt.xlabel('Product Category')\nplt.ylabel('Number Sold')\nplt.show()","b649f299":"channels = mkt.loc[:,['NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases']]\nchannels = channels.sum().sort_values(ascending=False)\n\nplt.bar(channels.index,channels)\nplt.xticks(rotation=90)\nplt.title('Channel Performance in last 2 years')\nplt.xlabel('Channel')\nplt.ylabel('Number of Products Sold')\nplt.show()","2f9fc8ac":"**5. Is there a significant relationship between geographical regional and success of a campaign?**\n\nFor this, we will fit a random forest classifier to the data, to predict response to the various campaigns (6 different models will be fit). The algorithm will rate each feature importance with a coefficient, based on the reduction effect on entropy. \n\nWe will plot the feature importance scores, and then the feature importance scores for the country variables will be studied to determine if there is a significant relationship to campaign success.\n\nNote that logistic regression was tried, but the model did not converge for many of the campaigns, thus we are taking this approach instead.","e9c54dfc":"Based on the chart, we can see that wine performed the best, with the highest number of items sold, followed by meat products. On the other hand, Gold, Fish, Sweet and Fruits products are les popular, with similar number of items sold","53aa638a":"Based on the results, P-value is very small, thus reject the null hypothesis that the variances are equal. \n\nThus, we will perform 2 sample T-Test for Unequal Variances","ca95d526":"We will plot the parameter coefficients to visualize the effect of each parameter on the variable 'MntFishProducts'","744e8bd8":"**1. What factors are significantly related to the number of store purchases?**\n\nFor this, we will build a multiple linear regression model to predict the number of store purchases. Based on the coefficients of the model, we will be able to plot & determine the most significant factors (positive and negative) related to the number of store purchases.","16e1de35":"Based on the hypothesis test, we can conclude that store purchases of people who spend more on gold is greater than store purchases of people who spend less on gold. Thus, the supervisor's claim is justified.","447de22e":"Again, we see that the US does not fare better than the rest of the world, being significantly behind Mexico in terms of average purchase value per customer, and very close to all the other countries\n\nThus, we can conclude with certainty that US does not fare better than the rest of the world in terms of total purchases.","123ea399":"Next, we will handle the outliers discussed in the previous section by either rectifying, removing or retaining them. We will handle the following outliers:\n\n* Year_Birth: Significant outliers on the lower spectrum to be imputed with mean (calculated excluding these values)\n* Marital_Status: Customer data in the 3 categories 'Alone', 'YOLO' and 'Absurd' to be removed\n* Kidhome & Teenhome: New features to be created, converting these variables into binary variables\n","7f795d09":"**2. Does US fare significantly better than the Rest of the World in terms of total purchases?**\n\nFor this, we will tabulate the total purchases of each country. Then, we will visualize this information for US as compared to the rest of the world.\n\nIf necessary, we will perform hypothesis testing to determine if it is statistically significant that US fares better than the rest of the world in terms of total purchases","5d778906":"**1. Which marketing campaign is most successful?**\n\nFor this, we will plot the takeup rate for all campaigns","49661d69":"# Section 04: CMO Recommendations\n\nBased on the findings in the sections above, we provide the following data driven recommendations:\n\n1. Regional Market Recommendations:\n    * Mexico presents as an interested market with willingness to pay, as they have the highest average purchase quantities & purchase values, despite the company having low market penetration in the country.\n\n2. Channel Recommendations:\n    * CMO should relook at website strategy. As shown in the exploratory data analytics, there is large interest in website as shown by high number of visits in the last month. However, this is not being converted into sales as the average number of website purchases in the past 2 years is lower than the average visits in a single month.\n\n3. Product Recommendations:\n    * There is a large appetite for wine & meat products from the company, thus the company can try to diversify and bring in more premium products in these categories to further drive sales. Additionally, there are some customers with very high income values ( > 100k ) who may be interested in these products.\n\n4. Campaign Recommendations:\n    * Future campaigns should try to emulate and repeat features in latest campaign ran by the company as it performed very well, with highest takeup rate of 14% compared to the average campaign takeup rate of 7%. \n    * Since there is no strong relationship between geographical regional and success of a campaign, future campaigns can be piloted at a smaller scale on 1\/2 countries to find out what works and what doesn't, before rolling out to other countries. This will help to save cost on failed campaigns as well as increase agility in campaign rollouts\n    \n5. Customer Loyalty Recommendations:\n    * CMO should consider some loyalty program or rewards for members to increase stickiness & purchase frequency. This is because engagement within members is rather low, as many have been members for over a year, but median recency (days since last purchase) is approximately 50 days. Considering the store has a large variety of fresh products (Fruits, Fish & Meat), they would prefer weekly \/ biweekly visits of customers\n","8cb8921f":"# Section 01: Exploratory Data Analysis\n\nIn this section, we will answer the following questions:\n1. Are there any null values or outliers? How will you wrangle\/handle them?\n2. Are there any variables that warrant transformations?\n3. Are there any useful variables that you can engineer with the given data?\n4. Do you notice any patterns or anomalies in the data? Can you plot them?","42c44796":"**4. Which channels are underperforming?**\n\nWe will plot a bar plot for each of the products, based on the interactions in the last 2 years. Number of web visits will not be included as data is only available for the past month ","43ad8e6f":"**4. Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish? What other factors are significantly related to amount spent on fish? (Hint: use your knowledge of interaction variables\/effects)**\n\nFor this, we will build a multiple linear regression model to predict the amount spent on fish products. We will be using the standardized variables, so each variable is scaled from 0 to 1. Based on the coefficients of the model, we will be able to plot & determine the most significant factors (positive and negative) related to the amount spent on fish products, and identify if married PhD candidates is a strong factor.","1e5e15c7":"On average, a customer looks like this, divided into several categories:\n\nDemographic:\n* Born between 1968 and 1969\n* Income: $51,959\n* 0.44 kids and 0.51 teens at home, for an average of 1 dependent at home\n* Married\n* Graduated (likely high school)\n* From Spain\n\nLoyalty:\n* Became a customer on 10-07-2013\n* Last made a purchase 49 days ago\n\nExpenditure in last 2 years:\n* Wine: 304.03\n* Fruits: 26.30\n* Meat: 167.11\n* Fish: 37.45\n* Sweet: 27.11\n* Gold: 43.90\n\nChannels:\n* Deals Purchases: 2.32\n* Web Purchases: 4.08\n* Catalog Purchases: 2.66\n* Store Purchases: 5.79\n* Number of Web Visits in past month: 5.32\n\nInteractions:\n* Complains: 0.01\n* Accepted latest campaign: 0.15\n* Accepted Campaign 1: 0.064\n* Accepted Campaign 2: 0.013\n* Accepted Campaign 3: 0.073\n* Accepted Campaign 4: 0.075\n* Accepted Campaign 5: 0.073","04dd4531":"**4. Do you notice any patterns or anomalies in the data? Can you plot them?**\n\nThere are some anomalies in the data that have been pointed out in while handling outliers, including:\n* Year_Birth: Some significant outliers are present on the lower spectrum. These datapoints are likely false as these people would be well over 100 years old\n* Marital_Status: There are 3 categories 'Alone', 'YOLO' and 'Absurd' which are not valid marital statuses, with very low number of customers in each\n* Country: There are only 3 entries for customers from Mexico, and this may skew results related to this variable\n\nThe plots have already been included in the outlier analysis section, and will not be presented again here\n","85b8895f":"Based on the model, the factors that are significantly related to the amount spent on fish are: 'MntSweetProducts', 'MntFruits', 'MntMeatProducts', 'NumCatalogPurchases', 'MntGoldProds' and 'Country_ME'.\n\nThis means that customers who tend to spend on other products will also spend on fish products. Interestingly, Customers in Mexico also tend to buy more fish\n\nIn comparison, the interaction term of Married PhD customers have a very low coefficient. Thus, it does not have a significant relationship with the amount spent on fish.","b4301f06":"There are 24 customers that are missing 'Income' data, and none of the other columns contain any missing data.\n\nWe will plot all the variables for visualization before performing any data wrangling on missing data & outliers, to ensure a hollistic approach.","3c5f3339":"From this, we can clearly see that the US is second to last in terms of total purchases, only ahead of Mexico. Thus. US definitely does not fare better than the rest of the world in terms of total purchases.\n\nLet's take a look if the US fares better in terms of average purchases per customer","a4eacbf6":"From this, we can also see that the US does not fare better than the rest of the world, being significantly behind Mexico in terms of average purchases per customer, and very close to all the other countries\n\nLet's take a look if the US fares better in terms of total purchases amounts. For this, we will total the values in the 6 columns 'MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts and MntGoldProds","bac0289d":"We first set up the hypothesis test:\n\nHo: Store purchases of people who spend more on gold <= Store purchases of people who spend less on gold \n\nHa: Store purchases of people who spend more on gold > Store purchases of people who spend less on gold\n\nNext, we check if the 2 samples have equal variances","cef17b1e":"With this information, we can handle the missing data discovered above.\n\nSince there is also a far outlier for 'Income' of > 600,000, we will impute the missing data as well as this outlier with the median value of 'Income', which is not affected by outliers in the dataset","abad1d7c":"**3. Which products are performing best?**\n\nWe will plot a bar plot for each of the products, based on the amount sold in the last 2 years","ad0347c2":"From the variable plots, we identify the following regarding the variables:\n* Year_Birth: Some significant outliers are present on the lower spectrum. These datapoints are likely false as these people would be well over 100 years old\n* Marital_Status: There are 3 categories 'Alone', 'YOLO' and 'Absurd'\u00a0which are not valid marital statuses, with very low number of customers in each\n* Income: There are some significant outliers on the upper bound, with one outlier > 600,000\n* Kidhome & Teenhome: The number of customers with 2 kid \/ teenager at home is very low, causing the classes to be very imbalanced. It may make more sense to convert the variables into binary variables indicating presence or absence of kid \/ teen\n* Amount Spent on Categories: The outliers for these categories are all acceptable & valid values of purchases, and shall be kept as is\n* Number of Touchpoint Visits: The outliers for these categories are all acceptable & valid values, and shall be kept as is\n* Campaign Responses: It can be observed that campaigns 1, 3, 4 and 5 performed similarly, whereas campaign 2 performed poorly. The latest campaign (under variable 'Response') had the best performance\n* Complain: Very few customers have filed any complaints\n* Country: Most of the customers are from Spain, and there is a very small group of customers from Mexico","4ed2a546":"# Introduction:\n\nThis notebook is a response to the dataset task 'Business Analysis with EDA & Statistics'. The task details is as follows:\n\n> You're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions.\n\nThis notebook will contain the following sections:\n* Section 01: Exploratory Data Analysis\n\n* Section 02: Statistical Analysis\n\n* Section 03: Data Visualization\n\n* Section 04: CMO Recommendations\n","5fa254aa":"From this, it can be seen that we have successfully removed extreme outliers, and some sparse classes mentioned above","9e8064ff":"# Section 03: Data Visualization\nIn this section, we will present data visualizations to answer the following questions:\n\n1. Which marketing campaign is most successful?\n2. What does the average customer look like for this company?\n3. Which products are performing best?\n4. Which channels are underperforming?","9c912965":"The graph above displays the acceptance rates of the various campaigns, in descending order\n\nBased on the graph, we can conclude that the most recent campaign is the most successful one.","815259f0":"Then, we will plot each variable to visualize the distribution of the data and identify any outliers or imbalanced classes.\n\nWe will plot boxplots for quantitative variables and barplots (of value counts) for qualitative, categorical and binary (yes\/no) variables","ca74d19f":"**3. Are there any useful variables that you can engineer with the given data?**\n\nIn the previous section, we have already engineered new features 'HasKid' and 'HasTeen'\n\nAdditionally , we will perform the following feature engineering steps to prepare the data for statistical analysis:\n1. Creating dummy variables for each qualitative variables so that they can be used in performing statistical modelling.\n2. Perform min-max scaling on all quantitative variables to standardize them\n\nThe standardized variables will be stored in a separate variable, so that the initial variable values are still easily accessible","09eb7fc0":"**3. Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test**\n\nFor this, we will first tabulate the data (number of in store purchases) for 2 populations, those with above average spend on gold, and those with average or lower spending on gold.\n\nThen, we will perform a T test for population means to determine if there is statistical evidence to support the claim that people who spend above average amount on gold also has more in store purchases\n\n","f58f0945":"We will arbitrarily use -0.1 and 0.1 as the cutoff for coefficients, in order to narrow down the important factors. We will also plot the coefficient of the interaction term 'Marital_Status_Married:Education_PhD' for comparison","32cae27e":"**2. What does the average customer look like for this company?**\n\nWe will average across each qualitative variables, and take the modal category of all categorical variables to obtain the average customer for this company","3c2c8bd6":"Based on the chart, we can see that most customers preferred purchasing in physical stores, as it has the most number of items sold. This is followed by online website, catalog, and deals.\u00a0Deals is the most underperforming channel","a213ef2e":"**2. Are there any variables that warrant transformations?**\n\nTransformations have already been done above for columns 'Dt_Customer' and 'Income' to ensure they are useable for statistical analysis. The following transformations will be done:\n\n* Income: Column renamed to remove leading & trailing spaces. We also reformatted the data values to remove dollar sign and commas\n* Dt_Customer: Values converted to represent 'days since joining' by subtracting the date joined from today's date.","46e8ff9c":"From the plot above, we can see that the acceptance rate (%) of each campaign across the various countries tend to be quite low and rather uniform. Thus, it makes sense and further supports our conclusion that \"Country\" is not a significant feature to predict campaign success.\n\nNote that the dataset only contains 3 customer datapoints for Mexico, thus the acceptance rate appears to be high (i.e. If 1 customer accepts the campaign, success rate would already be at 33%)","6abd6f04":"Based on the feature importance plots for each of the campaigns, it can be observed that the \"Country\" variables (in orange) tend to have very small coefficients, meaning that they are less important in predicting campaign success as compared to other features\n\nWe can further verify this by plotting the campaign acceptance rate across different countries.","d1b365d0":"Similar to total purchases, US is also second to last in terms of total purchase value, only ahead of Mexico. Thus. US definitely does not fare better than the rest of the world in terms of total purchase value.\n\nLet's take a look if the US fares better in terms of average purchase value per customer","fd51a103":"From the chart, we conclude that the 7 factors above are most significantly related to number of store purchases. Among the 7 factors, \n* Positive Effect: 'MntWines', 'NumWebPurchases', 'NumDealsPurchases', 'Income' and 'MntFruits' have a positive effect on the number of store purchases in decreasing order. This means that as the values of these variables increase, the number of store purchases also increases, with 'MntWines' having the largest effect on number of store purchases\n* Negative Effect: On the other hand, 'NumCatalogPurchases' and 'NumWebVisitsMonth' have a negative effect on the number of store purchases in decreasing order. This means that as the values of these variables increase, the number of store purchases decreases, with 'NumWebVisitsMonth' having the largest negative effect on number of store purchases\n\nOverall, it can be inferred that the selection of wines and fruits, as well as the attractive deals attract customers to make store purchases. Besides that, customers with higher income also tend to make more store purchases.\n\nOn the other hand, customers who tend to make catalog purchases are less likely to make purchases in the store.\n\nHowever, there are 2 contrasting factors, 'NumWebPurchases' and 'NumWebVisitsMonth' which are both website related but have opposing effects on the number of store purchases. This is a factor that should be further studied by the CMO and marketing team, and it is worth noting that 'NumWebVisitsMonth' only captures the past month's data, whereas 'NumWebPurchases' and 'NumStorePurchases' both seemingly capture the lifetime data of the customer. \n\nThus, it can be hypothesized that the store has been promoting their website on advertisements \/ social media lately, driving customers to switch from regular store purchases to website purchase, whereas customers who historically make many purchases on the website are loyal customers of the store, and tend to make in store purchases as well.","c29d5404":"# Section 02: Statistical Analysis\n\nIn this section, we perform statistical analysis to answer the following questions. For each question, we will provide a brief explanation of the findings, suitable for non-technical users:\n\n1. What factors are significantly related to the number of store purchases?\n2. Does US fare significantly better than the Rest of the World in terms of total purchases?\n3. Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n4. Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish? What other factors are significantly related to amount spent on fish? (Hint: use your knowledge of interaction variables\/effects)\n5. Is there a significant relationship between geographical regional and success of a campaign?","1cb1e63e":"Before that, we will have to perform some transformations on the variables \"Income\" and \"Dt_Customer\". The main reason being that the syntax of the data contained are not suitable for statistical analysis (currency formatting & date formatting). The following transformations will be done:\n1. Income: We rename the column to remove leading & trailing spaces. We will also reformat the data values to remove dollar sign and commas\n2. Dt_Customer: We convert the values to represent 'days since joining' by subtracting the date joined from today's date.","b3699171":"We then perform scaling of variables, and storing the values in a new dataframe 'mkt_scale'","531384fe":"We will plot the parameter coefficients to visualize the effect of each parameter on the variable 'NumStorePurchases'","478cad39":"We will arbitrarily use -0.1 and 0.1 as the cutoff for coefficients, in order to narrow down the important factors","cbbdfa60":"We will plot the variables against to visualize the changes that have been made. We will also plot the 2 new variables 'HasKid' and 'HasTeen'","15c67e31":"**1. Are there any null values or outliers? How will you wrangle\/handle them?**\n\nWe start by checking for missing data in the dataset"}}