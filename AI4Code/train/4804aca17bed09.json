{"cell_type":{"8b7129bd":"code","e1b48801":"code","de008090":"code","6f9cbc13":"code","182b90f5":"code","134302e2":"code","161eac9c":"code","1d771378":"code","f2466114":"code","8b33a71d":"code","f4674ec0":"code","a5e864e3":"code","cc7ceb71":"code","93ea7ab5":"code","9e3c9f65":"code","9478c48a":"code","0dc62e40":"code","2a9d01fc":"code","e70dc336":"code","8b92188b":"code","7a49d85b":"code","08356547":"code","6a1f4d04":"code","85ea6435":"code","fc30c1d0":"code","3cdee2f2":"code","60d9d883":"code","f8b0b683":"code","8737667f":"code","950779a4":"code","fc2f5997":"code","cca06a65":"code","75f6cc20":"code","ff70656d":"markdown","0e19865c":"markdown","128f033b":"markdown","cf57da67":"markdown","e47e249f":"markdown","07c54e6f":"markdown","72b74104":"markdown","94baa730":"markdown","9eb07ffa":"markdown","dc472338":"markdown","0ef80179":"markdown","0610f2f0":"markdown","d0af7f3a":"markdown","28ad6ff7":"markdown","c1824848":"markdown","462d7cb8":"markdown","5fd100ea":"markdown","6fe37439":"markdown","1f150f91":"markdown"},"source":{"8b7129bd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n# sample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=0)\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=0)","e1b48801":"train.shape","de008090":"train.dtypes.value_counts()","6f9cbc13":"grouped = train.columns.to_series().groupby(train.dtypes).groups\nprint(grouped)","182b90f5":"train.head()","134302e2":"for col in train.columns:\n    try:\n        empty = train[col].isna().value_counts()[1]\n    except KeyError as e:\n        empty = 0\n    ratio = empty \/ train.shape[0]\n    if ratio > 0.25:\n        print(\"{}: {:0.2f}\".format(col, ratio))","161eac9c":"train2 = train.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'])\n\ntrain.head()","1d771378":"print(train2.shape)\nprint(train2.dtypes.value_counts())\n\ngrouped = train2.columns.to_series().groupby(train.dtypes).groups","f2466114":"objs = list(grouped.values())[2]\ndense = []\n\nfor col in objs:\n    categs = train[col].nunique()\n    if categs > 2:\n        dense.append(col)\n\nprint(train[dense].shape)\ntrain[dense].describe()","8b33a71d":"train[objs].nunique().median()","f4674ec0":"very_dense = []\n\nfor col in dense:\n    categs = train[col].nunique()\n    if categs > 5:\n        very_dense.append(col)\n\nprint(train[very_dense].shape)\ntrain[very_dense].describe()","a5e864e3":"from scipy.stats import pearsonr\n\ntarget = train['SalePrice']\n\nPearson = {}\n\nfor categ in very_dense:\n    OHdf = pd.get_dummies(train[categ])\n    pearson = np.empty(OHdf.shape[1])\n    pval = np.empty(OHdf.shape[1])\n    for i, col in enumerate(OHdf.columns):\n        pearson[i], pval[i] = pearsonr(OHdf[col], target)\n    Pearson[categ] = {'r': np.mean(pearson), 'p': np.mean(pval)}\n    print(\"{}: r: {:0.2f}, p: {:0.2f}\".format(categ, Pearson[categ]['r'], Pearson[categ]['p']))","cc7ceb71":"dense_relevant = []\n\nfor col, pearson in Pearson.items():\n    if pearson['p'] < 0.05:\n        dense_relevant.append(col)\n\nprint(dense_relevant)","93ea7ab5":"train['BsmtFinType1'].unique()","9e3c9f65":"ind = train2.index\n\nDenseDf = pd.DataFrame(index=ind)\n\nfor col in [x for x in dense if x not in very_dense]:\n    OHdf = pd.get_dummies(train[col], prefix=col)\n    DenseDf = pd.concat([DenseDf, OHdf], axis=1)\n\nDenseDf = pd.concat([DenseDf, pd.get_dummies(train['BsmtFinType1'], prefix='Bsmt')], axis=1)\n\nDenseDf.head()","9478c48a":"binaries = [x for x in objs if x not in dense]\n\nprint(binaries)","0dc62e40":"train['Street'].unique() # Pave va a ser 1","2a9d01fc":"train['Utilities'].unique() # AllPub va a ser 1","e70dc336":"train['CentralAir'].unique() # Y va a ser 1","8b92188b":"BNdf = train[binaries]\nBNen = pd.DataFrame(index=ind)\n\nBNyes = {'Street':'Pave','Alley':'Pave','Utilities':'AllPub','CentralAir':'Y'}\n\nfor col in BNdf.columns:\n    yes = pd.Series( (train[col] == BNyes[col]).astype(int), index=ind, name=col )\n    BNen = pd.concat([BNen, yes], axis=1)\n\nBNen.head()","7a49d85b":"CategDf = pd.concat([BNen, DenseDf], axis=1)\nCategDf.head()","08356547":"good = [x for x in train2.columns if x not in objs]\ngood.remove('SalePrice')\n\n# Engineered DataFrames\ndfX = pd.concat([train2[good], CategDf], axis=1)\ndfY = train2['SalePrice'].astype(float)\n\ndfX.shape","6a1f4d04":"fig, axes = plt.subplots(6, 6, figsize=(20, 20))\n\nfor i, col in enumerate(dfX[good]):\n    sns.boxplot(x=col, data=dfX, ax=axes.flat[i])","85ea6435":"# importar modelos necesarios de sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","fc30c1d0":"X_train, X_validate, y_train, y_validate = train_test_split(dfX, dfY, random_state=42)\n\nmodel = Pipeline([(\"imputer\", SimpleImputer(strategy='mean')),\n                  (\"scaler\", RobustScaler()),\n                  (\"regression\", Lasso())])\nmodel.fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(model.score(X_train, y_train)))\nprint(\"Training set mse: {:.2f}\".format(mean_squared_error(y_train, model.predict(X_train))))\nprint(\"Training set mae: {:.2f}\\n\".format(mean_absolute_error(y_train, model.predict(X_train))))\n\nprint(\"Validation set score: {:.2f}\".format(model.score(X_validate, y_validate)))\nprint(\"Validation set mse: {:.2f}\".format(mean_squared_error(y_validate, model.predict(X_validate))))\nprint(\"Validation set mae: {:.2f}\".format(mean_absolute_error(y_validate, model.predict(X_validate))))","3cdee2f2":"from sklearn.ensemble import RandomForestRegressor\n\nforestM = Pipeline([(\"imputer\", SimpleImputer(strategy=\"mean\")),\n                   (\"regressor\", RandomForestRegressor(random_state=42))])\n\ntrees = np.arange(1, 100)\nparam_grid = {'regressor__n_estimators':trees}\n\nforest = GridSearchCV(forestM, param_grid=param_grid, cv=5, n_jobs=-1)\nforest.fit(X_train, y_train)\n\nprint(\"Best params: {}\\n\".format(forest.best_params_))\n\nprint(\"Training set score: {:.2f}\".format(forest.score(X_train, y_train)))\nprint(\"Training set mse: {:.2f}\".format(mean_squared_error(y_train, forest.predict(X_train))))\nprint(\"Training set mae: {:.2f}\\n\".format(mean_absolute_error(y_train, forest.predict(X_train))))\n\nprint(\"Validation set score: {:.2f}\".format(forest.score(X_validate, y_validate)))\nprint(\"Validation set mse: {:.2f}\".format(mean_squared_error(y_validate, forest.predict(X_validate))))\nprint(\"Validation set mae: {:.2f}\".format(mean_absolute_error(y_validate, forest.predict(X_validate))))","60d9d883":"X_test = test.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'])\n\nt_ind = X_test.index\n\nt_numeric = [x for x in X_test.columns if x not in objs]\n\nt_BNdf = test[binaries]\nt_BNen = pd.DataFrame(index=t_ind)\n\nfor col in t_BNdf.columns:\n    yes = pd.Series( (test[col] == BNyes[col]).astype(int), index=t_ind, name=col )\n    t_BNen = pd.concat([t_BNen, yes], axis=1)\n\nt_BNen.head()","f8b0b683":"t_DenseDf = pd.DataFrame(index=t_ind)\n\nfor col in [x for x in dense if x not in very_dense]:\n    OHdf = pd.get_dummies(X_test[col], prefix=col)\n    t_DenseDf = pd.concat([t_DenseDf, OHdf], axis=1)\n\nt_DenseDf = pd.concat([t_DenseDf, pd.get_dummies(X_test['BsmtFinType1'], prefix='Bsmt')], axis=1)\n\nprint(\"Train dense shape: {}\".format(DenseDf.shape))\nprint(\"Test dense shape: {}\".format(t_DenseDf.shape))\n\nt_DenseDf.head()","8737667f":"print([x for x in DenseDf.columns if x not in t_DenseDf.columns])","950779a4":"for col in ['Electrical_Mix', 'GarageQual_Ex']:\n    t_DenseDf[col] = np.zeros(shape=t_ind.shape, dtype=int)\n    \nprint(\"Train dense shape: {}\".format(DenseDf.shape))\nprint(\"Test dense shape: {}\".format(t_DenseDf.shape))","fc2f5997":"X_test = pd.concat([X_test[t_numeric], t_BNen, t_DenseDf], axis=1)\n\nprint(\"Train shape: {}\".format(dfX.shape))\nprint(\"Test shape: {}\".format(X_test.shape))","cca06a65":"y_pred = forest.predict(X_test)\n\ny_pred.shape","75f6cc20":"submission = pd.Series(data=y_pred, index=t_ind, name='SalePrice')\n\nsubmission.head()","ff70656d":"Como podemos ver, tenemos 16 categor\u00edas que clasifican como \"muy densas\", con este tama\u00f1o podemos analizar cada una para decidir si las dejamos en nuestro DataFrame basado en su relevancia. Haremos la codificaci\u00f3n de cada una y obtendremos un *coeficiente de correlaci\u00f3n de Pearson* para determinar su importancia estad\u00edstica como categor\u00eda.\n\nPara determinar su importancia usaremos la funci\u00f3n `pearsonr` de la librer\u00eda `scipy` que tambi\u00e9n devuelve un *valor-p* y usaremos un intervalo de confianza del 95%.","0e19865c":"# Feature Engineering","128f033b":"## UwU\n\nAhora juntamos `BNen` con `DenseDf` y podemos preparar nuestro DataFrame 100% num\u00e9rico.\n\n> La categor\u00eda `MSSubClass` es categ\u00f3rica a pesar de constar de int, pero me di\u00f3 flojera arreglarla.","cf57da67":"# Predicciones","e47e249f":"Nuestro DataFrame `test` no presenta los valores *Mix* ni *Ex* en la categor\u00eda **Electrical** y **GarageQual** respectivamente, por lo que podemos crear series vac\u00edas con este nombre para rellenar el DataFrame","07c54e6f":"El dataset tiene 81 categor\u00edas. Primero, hay que determinar si podemos eliminar algunas por falta de informaci\u00f3n basado en un n\u00famero arbitrario. Por ejemplo, podemos eliminar las columnas con m\u00e1s del 25% de los datos faltantes","72b74104":"Las categor\u00edas que filtr\u00f3 el algoritmo son buenas candidatas para eliminar. Y podemos ver que todas excepto por **FieplaceQu** tienen menos del 20% de datos, por lo que ser\u00eda in\u00fatil usarlas.","94baa730":"Hay que volver a contar las variables","9eb07ffa":"## Codificar variables categ\u00f3ricas\n\nComo pudimos ver al contar las columnas, tenemos 38 variables categ\u00f3ricas. Aquellas que tienen 2 posibles estados se pueden codificar como 0 \u00f3 1, pero aquellas con m\u00e1s de 2 estados deben de separarse en \"one-hot\" encoding, donde cada posible estado se separa en su propia columna y se le asigna el valor de 0 \u00f3 1.","dc472338":"*`RobustScaler` it is!*","0ef80179":"## Predicci\u00f3n\n\nEl momento de la verdad","0610f2f0":"## Primer modelo, regresi\u00f3n lineal\n\nPara un primer modelo siempre es bueno intentar una regresi\u00f3n lineal sencilla. Voy a armar un Pipeline sencillo que haga lo siguiente:\n\n1. `SimpleImputer`\n1. Normalizar los datos input\n1. Pasar los datos por un modelo linear\n\nPara el modelo linear usar\u00e9 *Lasso*, ya que hace autom\u00e1ticamente la selecci\u00f3n de variables estad\u00edsticamente relevantes, similar a como lo hice manualmente en las variables categ\u00f3ricas. Para saber qu\u00e9 *scaler* voy a utilizar, es \u00fatil un diagrama de cajas. si hay valores at\u00edpicos una buena selecci\u00f3n ser\u00eda el `RobustScaler` de sklearn, de otra manera eligir\u00e9 el `StandardScaler`.","d0af7f3a":"*Voil\u00e1!* De todas las categor\u00edas densas la \u00fanica que es estad\u00edsticamente significativa es **BsmtFinType1** que seg\u00fan la documentaci\u00f3n es el estado del s\u00f3tano. Ok boomer.","28ad6ff7":"Buscamos las variables que denominar\u00e9 \"densas\", es decir, que tienen m\u00e1s de dos posibles estados \u00fanicos. Pero a simple vista se pueden ver algunas con valores demasiado elevados como **Neighborhood** o **HouseStyle**.\n\nPara evitar saturar nuestro DataFrame y evitar sobre-adaptar haremos un an\u00e1lisis de las categor\u00edas \"muy densas\", en este caso las definir\u00e9 como aquellas que pasen la media de n\u00famero de estados \u00fanicos.","c1824848":"## Random Forest","462d7cb8":"## Test encoding","5fd100ea":"## Codificaci\u00f3n binaria\n\nComo dije antes, las variables categ\u00f3ricas con dos posibles estados se pueden codificar como 1 o 0. Aunque debemos verlas para decidir cu\u00e1l de sus dos estados es equivale a \"1\".","6fe37439":"# EDA headers","1f150f91":"Ya que tenemos el *valor-p* promedio por categoria podemos ver cu\u00e1les son estad\u00edsticamente relevantes"}}