{"cell_type":{"dedcf6ee":"code","3e43b794":"code","88e097ea":"code","9d56683e":"code","02341cc6":"code","1d9b45dc":"code","e12bc458":"code","1fd86048":"code","ef60ef87":"code","43bc95da":"code","183744c4":"code","5b6c15f2":"code","bf3035e8":"code","41486540":"code","61c677ea":"code","57e01be3":"code","526cf61a":"code","c6c1d6a3":"code","094e296e":"code","550fb809":"code","1fb47948":"code","502e0227":"code","8616f598":"code","4b4e4114":"code","0d2f0917":"code","4255a3f9":"code","61e444e3":"code","8dcfb3b7":"code","e6f7e408":"code","fc03fdbc":"code","cea284a1":"code","87201344":"code","a282fad8":"code","3e41a711":"code","f5bddd1c":"code","6fc6b782":"code","17db8ba4":"code","fc133ffe":"code","572cd7c1":"code","e3a476c0":"code","4032ca7d":"code","3dba1330":"code","4027ef56":"code","abe971f2":"code","c580bbcd":"code","232864ce":"code","d3d67858":"code","ea4153b8":"code","ec612dbb":"markdown","d160fdb1":"markdown","58743bac":"markdown","d5957711":"markdown","e41cebf6":"markdown","a93f78ca":"markdown","bb36ba39":"markdown","e1cb74a7":"markdown","f6c7c407":"markdown","bde01289":"markdown","2556756d":"markdown","fde995b7":"markdown","8a7570a4":"markdown"},"source":{"dedcf6ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nfrom scipy import stats\nfrom scipy.special import boxcox1p\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e43b794":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_data.head()","88e097ea":"train_data.shape","9d56683e":"train_data.info()","02341cc6":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_data.head()","1d9b45dc":"test_data.shape","e12bc458":"test_data.info()","1fd86048":"train_data.isna().sum()","ef60ef87":"train_data.drop('Id', inplace = True,axis = 1)\ntest_id = test_data['Id']\ntest_data.drop('Id', inplace = True,axis = 1)\n","43bc95da":"print(f'After dropping ID feature , train data : {train_data.shape} and test data = {test_data.shape}')","183744c4":"min_percentile= 0.001\nmax_percentile= 0.999\n# Use numeric features\nfeatures = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\ntarget= 'SalePrice'\nnrows= int(np.ceil(len(features)\/2))\nncols= 2 \n\ndef detect_and_remove_outliers(inline_delete= True):\n    global train_data\n    fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (24, nrows * 6))\n    outliers = []\n    cnt = 0\n    for row in range (0, nrows):\n        for col in range (0, ncols):\n            # df_outliers = outlier_detection_using_percentile(features[cnt])\n            # Outlier detection using percentile\n            min_thresold, max_thresold = train_data[features[cnt]].quantile([min_percentile, max_percentile])\n            # print(f'outlier_detection_using_percentile()> min_thresold: {min_thresold}, max_thresold: {max_thresold}')\n            # print (f'No of outliers below min_thresold: {len(train[train[feature] < min_thresold])}')\n            # print (f'No of outliers above max_thresold: {len(train[train[feature] > max_thresold])}')\n            df_outliers = train_data[(train_data[features[cnt]] < min_thresold) | (train_data[features[cnt]] > max_thresold)]\n\n            # Updaing list of outliers\n            outliers = outliers + df_outliers.index.tolist()\n\n            # Plot feature vs target using scatter plot\n            ax[row][col].scatter(x = train_data[features[cnt]], y= train_data[target])\n     \n            # Mark outlier records in same scatter plot\n            ax[row][col].scatter(x= df_outliers[features[cnt]],  y=df_outliers[target], marker =\"o\", edgecolor =\"red\", s = 100)\n            ax[row][col].set_xlabel(features[cnt])\n            ax[row][col].set_ylabel(target)\n            ax[row][col].set_title('Outlier detection for feature ' + features[cnt])\n\n            if inline_delete: \n                # Drop the outliers inline\n                # drop_outliers(df_outliers.index.tolist())\n                # print(f'Shape of train data= {train.shape}')\n                train_data = train_data.drop(df_outliers.index.tolist())\n                train_data.reset_index(drop = True, inplace = True)\n                # print(f'Shape of train data= {train.shape}')\n\n            cnt = cnt + 1\n            if cnt >= len(features):\n                break\n    plt.show()\n\n    print(f'outliers: {outliers}')\n    unique_outliers= list(set(outliers))\n    print(f'unique_outliers: {unique_outliers}')\n    \n    if inline_delete == False: \n        # Drop the unique outliers from final list\n        print(f'Shape of train data= {train_data.shape}')\n        train_data = train_data.drop(unique_outliers)\n        train_data.reset_index(drop = True, inplace = True)\n        print(f'Shape of train data= {train_data.shape}')\n        \n        \ndetect_and_remove_outliers(inline_delete= False)","5b6c15f2":"y_train = train_data['SalePrice']\ntrain_data.drop('SalePrice', inplace = True, axis = 1)\nprint('Shape of train data = {} and test data = {}'.format(train_data.shape,test_data.shape))\ntrain_data.head()","bf3035e8":"train_data['Utilities'].value_counts(dropna = False)","41486540":"train_data['Street'].value_counts(dropna = False)","61c677ea":"train_data['PoolQC'].value_counts(dropna = False)","57e01be3":"train_data = train_data.drop(['Utilities','Street','PoolQC'],axis = 1)\ntest_data = test_data.drop(['Utilities','Street','PoolQC'],axis = 1)\nprint('shape of train_data = {} and test_data = {}'.format(train_data.shape,test_data.shape))","526cf61a":"for col in ('MSSubClass', 'YrSold', 'MoSold'):\n    train_data[col] = train_data[col].astype(str)","c6c1d6a3":"# Get the list of column that have null values\ncol_na = train_data.columns[train_data.isnull().any()]\n\n# Get missing value count in each column\ntrain_data_na_cnt = train_data[col_na].isnull().sum()\n\n# Get missing values percentage for each column\ntrain_data_na = (train_data[col_na].isnull().sum()\/len(train_data)) * 100\n#train_data_na = train_data_na.sort_values(ascending = False)\ntrain_data_na = pd.DataFrame({'Total Null Val': train_data_na_cnt, 'Null Value %': train_data_na})\ntrain_data_na = train_data_na.sort_values(by='Null Value %', ascending=False)\ntrain_data_na","094e296e":"for col in ('MSSubClass', 'YrSold', 'MoSold'):\n    test_data[col] = test_data[col].astype(str)","550fb809":"# Get the list of column that have null values\ncol_na = test_data.columns[test_data.isnull().any()]\n\n# Get missing value count in each column\ntest_data_na_cnt = test_data[col_na].isnull().sum()\n\n# Get missing values percentage for each column\ntest_data_na = (test_data[col_na].isnull().sum()\/len(test_data)) * 100\n#test_data_na = test_data_na.sort_values(ascending = False)\ntest_data_na = pd.DataFrame({'Total Null Val': test_data_na_cnt, 'Null Value %': test_data_na})\ntest_data_na = test_data_na.sort_values(by='Null Value %', ascending=False)\ntest_data_na","1fb47948":"for col in ('MiscFeature', 'Alley','FireplaceQu', 'GarageFinish', 'GarageQual', 'Fence', 'GarageType', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    train_data[col] = train_data[col].fillna('None')\n    print(f'Feature: {col}, Null Count: {train_data[col].isnull().sum()}, Unique Values: {train_data[col].unique()}')\n\n\nfor col in ('MiscFeature', 'Alley','FireplaceQu', 'GarageFinish', 'GarageQual', 'Fence', 'GarageType', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    test_data[col] = test_data[col].fillna('None')\n    print(f'Feature: {col}, Null Count: {test_data[col].isnull().sum()}, Unique Values: {test_data[col].unique()}')","502e0227":"train_data['LotFrontage'] = train_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntest_data['LotFrontage'] = test_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","8616f598":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    train_data[col] = train_data[col].fillna(0)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    test_data[col] = test_data[col].fillna(0)","4b4e4114":"# As per the data description, assume 'Typ' home functionality unless deductions are warranted.\ntrain_data['Functional'] = train_data['Functional'].fillna('Typ')\ntest_data['Functional'] = test_data['Functional'].fillna('Typ')","0d2f0917":"for col in ('MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd', 'SaleType'):\n    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n    print(f'Feature: {col}, Null Count: {train_data[col].isnull().sum()}, Unique Values: {train_data[col].unique()}')","4255a3f9":"for col in ('MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd', 'SaleType'):\n    test_data[col] = test_data[col].fillna(test_data[col].mode()[0])\n    print(f'Feature: {col}, Null Count: {test_data[col].isnull().sum()}, Unique Values: {test_data[col].unique()}')","61e444e3":"print(f'Shape of data: {train_data.shape}')\nprint(f'Count of null values: {train_data.isnull().sum().sum()}')","8dcfb3b7":"print(f'Shape of data: {test_data.shape}')\nprint(f'Count of null values: {test_data.isnull().sum().sum()}')","e6f7e408":"train_data.columns","fc03fdbc":"train_data = train_data.drop(['LowQualFinSF'], axis= 1) # Default drop axis is 0 i.e. rows \ntrain_data.reset_index(drop = True, inplace = True)\n\ntest_data = test_data.drop(['LowQualFinSF'], axis= 1) # Default drop axis is 0 i.e. rows \ntest_data.reset_index(drop = True, inplace = True)","cea284a1":"#Combining both test and train data for ease of feature remoing and adding new features !!\nall_data = pd.concat((train_data,test_data)).reset_index(drop = True) #  Drops the current index of the DataFrame and replaces it with an index of increasing integers\n#Lets check the count of numerical and categorical features\ncat_feats = all_data.dtypes[all_data.dtypes == \"object\"].index\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nprint(f\"Number of categorical features: {len(cat_feats)}, Numerical features: {len(numeric_feats)}\")\n\nskew_features = all_data[numeric_feats].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skew_features})\n\nprint(f'Skew in numerical features. Shape of skewness: {skewness.shape}')\nskewness.head(10)","87201344":"# We will use threshold '0.5' to apply Box Cox transformation.\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\n# Use box-cox transformation to transform numeric values with high skew into normal distribution. \n# Here we are using 'boxcox_normmax()' function to compute optimal Box-Cox transform parameter(lmbda) for input data.\n# We are using the default method(pearsonr) to determine the optimal transform parameter (lmbda) for boxcox1p\n# boxcox1p compute the Box-Cox transformation of 1 + x. (log 0 is undefined)\nfor i in skew_index:\n    all_data[i] = boxcox1p(all_data[i], stats.boxcox_normmax(all_data[i] + 1))","a282fad8":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['TotalSF1'] = all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['YrBltAndRemod']= all_data['YearBuilt'] + all_data['YearRemodAdd']\n\nall_data['TotalBathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                               all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n\nall_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n                              all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n                              all_data['WoodDeckSF'])\n\nprint(f'Shape all_data: {all_data.shape}')","3e41a711":"all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint(f'Shape all_data: {all_data.shape}')","f5bddd1c":"#Lets check the count of numerical and categorical features\ncat_feats = all_data.dtypes[all_data.dtypes == \"object\"].index\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nprint(f\"Number of categorical features: {len(cat_feats)}, Numerical features: {len(numeric_feats)}\")\n\nprint(f\"\\nList of cetagorical features: { cat_feats.to_list() }\\n\\nList of numerical features: { numeric_feats.to_list() }\")","6fc6b782":"cat_feats_nominal = ['MSSubClass', 'MSZoning', 'Neighborhood', 'Condition1', 'Condition2', 'HouseStyle', 'CentralAir', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Electrical', 'MasVnrType', 'Exterior1st', 'Exterior2nd', 'Heating', 'Foundation']\ncat_feats_ordinal = ['Alley', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'BldgType', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','PavedDrive', 'Fence']\n\nnumeric_feats_cont= ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'TotalSF', 'TotalSF1', 'YrBltAndRemod', 'TotalBathrooms', 'TotalPorchSF']\nnumeric_feats_ordinal= ['OverallQual', 'OverallCond']\nnumeric_feats_descrete= ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars','haspool', 'has2ndfloor', 'hasgarage', 'hasbsmt', 'hasfireplace']\n\nprint(f\"Number of cat_feats_nominal: {len(cat_feats_nominal)}, cat_feats_ordinal: {len(cat_feats_ordinal)}, numeric_feats_cont: {len(numeric_feats_cont)}, numeric_feats_ordinal: {len(numeric_feats_ordinal)}, numeric_feats_descrete: {len(numeric_feats_descrete)} \")","17db8ba4":"# List of categorical ordinal feature\nprint(f'List of categorical ordinal features: {cat_feats_ordinal}')\n\nall_data['Alley'].replace(to_replace = ['None', 'Grvl', 'Pave'], value = [0, 1, 2], inplace = True)\nall_data['LotShape'].replace(to_replace = ['Reg', 'IR1', 'IR2', 'IR3'], value = [3, 2, 1,0], inplace = True)\nall_data['LandContour'].replace(to_replace = ['Lvl', 'Bnk', 'Low', 'HLS'], value = [3, 2, 1,0], inplace = True)\nall_data['LotConfig'].replace(to_replace = ['Inside', 'FR2', 'Corner', 'CulDSac', 'FR3'], value = [0, 3, 1, 2, 4], inplace = True)\nall_data['LandSlope'].replace(to_replace = ['Gtl', 'Mod', 'Sev'], value = [2, 1, 0], inplace = True)\nall_data['BldgType'].replace(to_replace = ['1Fam', '2fmCon', 'Duplex', 'TwnhsE', 'Twnhs'], value = [4, 3, 2, 1, 0], inplace = True)\nall_data['RoofStyle'].replace(to_replace = ['Gable', 'Hip', 'Gambrel', 'Mansard', 'Flat', 'Shed'], value = [4, 2, 3, 1, 5, 0], inplace = True)\nall_data['RoofMatl'].replace(to_replace = ['ClyTile', 'CompShg', 'Membran', 'Metal', 'Roll', 'Tar&Grv', 'WdShake', 'WdShngl'], value = [7, 6, 5, 4, 3, 2, 1, 0], inplace = True)\nall_data['ExterQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa'], value = [3, 2, 1, 0], inplace = True)\nall_data['ExterCond'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po'], value = [4, 3, 2, 1, 0], inplace = True)\nall_data['BsmtQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\nall_data['BsmtCond'].replace(to_replace = ['Gd', 'TA', 'Fa', 'Po', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\nall_data['BsmtExposure'].replace(to_replace = ['Gd', 'Av', 'Mn', 'No', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\nall_data['BsmtFinType1'].replace(to_replace = ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\nall_data['BsmtFinType2'].replace(to_replace = ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\nall_data['HeatingQC'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po'], value = [4, 3, 2, 1, 0], inplace = True)\nall_data['KitchenQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa'], value = [3, 2, 1, 0], inplace = True)\nall_data['Functional'].replace(to_replace = ['Typ', 'Min1', 'Min2', 'Mod',  'Maj1', 'Maj2', 'Sev'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\nall_data['FireplaceQu'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], value = [5, 4, 3, 2, 1, 0], inplace = True)\nall_data['GarageType'].replace(to_replace = ['2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd', 'None'], value = [6, 5, 4, 3, 2, 1, 0], inplace = True)\nall_data['GarageFinish'].replace(to_replace = ['Fin', 'RFn', 'Unf', 'None'], value = [3, 2, 1, 0], inplace = True)\nall_data['GarageQual'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], value = [5, 4, 3, 2, 1, 0], inplace = True)\nall_data['GarageCond'].replace(to_replace = ['Ex', 'Gd', 'TA', 'Fa',  'Po', 'None'], value = [5, 4, 3, 2, 1, 0], inplace = True)\nall_data['PavedDrive'].replace(to_replace = ['Y', 'P', 'N'], value = [2, 1, 0], inplace = True)\nall_data['Fence'].replace(to_replace = ['GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'None'], value = [4, 3, 2, 1, 0], inplace = True)\n\nprint(f'\\nShape of all_data: {all_data.shape}')\nall_data.head()","fc133ffe":"print(f'List of categorical nominal features: {cat_feats_nominal}')","572cd7c1":"# Get k-1 dummies to avoid multicollinearity\ncat_feats_nominal_one_hot = pd.get_dummies(all_data[cat_feats_nominal], drop_first= True).reset_index(drop=True)\n\nprint(f'Shape of cat_feats_nominal_one_hot: {cat_feats_nominal_one_hot.shape}')\ncat_feats_nominal_one_hot.head()","e3a476c0":"# First we need to drop the catgorical nominal columns from all_data\nall_data = all_data.drop(cat_feats_nominal, axis= 'columns')\n\nall_data = pd.concat([all_data, cat_feats_nominal_one_hot], axis='columns')\nprint(f'Shape of all_data: {all_data.shape}')\nall_data.head()","4032ca7d":"train_data = all_data[:len(y_train)]\ntest_data = all_data[len(y_train):]\nprint(f'Shape of train: {train_data.shape}, test:{test_data.shape}')","3dba1330":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_tr,y_te=train_test_split(train_data,y_train,test_size=0.01,random_state=80)","4027ef56":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_tr)\nlr.score(X_test,y_te)","abe971f2":"pred =np.round(lr.predict(test_data),2)","c580bbcd":"pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv').head()","232864ce":"Submission = pd.DataFrame(data = pred,columns = ['SalePrice'])\nSubmission['Id'] = test_id\nSubmission = Submission[['Id','SalePrice']]\nSubmission.head()","d3d67858":"Submission.set_index('Id', inplace = True)","ea4153b8":"Submission.to_csv('Submission.csv')","ec612dbb":"### Numeric Feature Scaling\u00b6\nIn order to give every feature same importance we perform feature scaling. There are many technioques like Min-Max Scaler, Robust Scaler etc. to do feature scaling.\nBefore we can finalize any scaling technique lets check the skewness of our numeric features. Skewness is the measure of degree of asymmetry of a distribution.\n* skewness = 0 : normally distributed.\n* skewness > 0 : more weight in the left tail of the distribution.\n* skewness < 0 : more weight in the right tail of the distribution.","d160fdb1":"#### Replace with Most Frequent Value\nFor low percentage of null values, we will use most frequent value to replace the categorical missing value.","58743bac":"# OUTLIERS CHECK \n\nTrying to check for outliers and removing them using inline deleting technique !\nUsing only numerical features for this process !","d5957711":"We cannot delete the categorical features directly as they play an important role in the estimation of our Saleprice though some features like Alley , LotShape , LandContour etc , dont seem to play a direct role ,nonetheless they are important when considering a house_price and so are not removed !","e41cebf6":"# Deleting unuseful Features\n* Before we start with missing values , first we need to remove the unuseful features !\n* 'Utilities' feature contains almost all the values of one type of utility only. Since it wont be usefull in modeling we can drop this feature.\n* 'Street' feature also contains the unbalance data of type of road access to property. We can drop it.\n* 'PoolQC' most of the data is missing for this feature, we can drop it.","a93f78ca":"#### Replace numerical missing values with 0","bb36ba39":"Adding New Features\u00b6\n* Since area related features are very important to determine the house price, we will create new feature by name 'TotalSF' by adding 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'.\n* Similarly we will create one more new feature by name 'TotalSF1' by adding 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'. Here 'BsmtFinSF1' and 'BsmtFinSF2' represent finished square feet of all area, thats why we are creating separate feature using it.\n* Create new feature 'YrBltAndRemod' by adding 'YearBuilt' and 'YearRemodAdd'\n* Create new feature 'TotalBathrooms' by adding all the bathrooms in the house.\n* Create new feature 'TotalPorchSF' by adding all porch area.","e1cb74a7":"Since Id column is not useful in the analysis of our data , we will actually remove it from both the test and training data .","f6c7c407":"### Filling NULL VALUES\n#### Replace with None\nWe will replace the categorical ordinal as well as nominal features missing values with 'None'\nIn case of nominal feature it will become one of the category and in case of ordinal feature it will be trated as least order value","bde01289":"### Encoding Categorical Variables\nWe will use *Manual Label Encoding* for ordinal categorical variables and *One Hot Encoding* for nominal categorical variables.\nReason for doing manual label encoding for known order is, if we use sklearn's label encoder, it will randomly encode these ordinal variables and therefore ordinality would be lost.\nRemember that for missing values we have added 'None' category, which we will encode with '0'.","2556756d":"Now lets add new features based on the availability of the swimming pool, second floor, garage, basement and firepalce.","fde995b7":"#### Replace with Median\nLotFrontage: Linear feet of street connected to property. Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood, we can fill in missing values by the median LotFrontage of the neighborhood.\nSince 'LotFrontage containes continuous data we are taking 'median' value.","8a7570a4":"# Data Preprocessing \n\nNow that we have some insights , it will help us now to perform some operations and remove unuseful features , identify the null values, fix the datatypes of some features and also group the data based on the features , they contain !.\n"}}