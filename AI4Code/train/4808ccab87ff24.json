{"cell_type":{"c03f6cd0":"code","a72e61b9":"code","89adad32":"code","a0ffec03":"code","3c2517e4":"code","5adfe7a0":"code","07cf60eb":"code","6938541b":"code","ab7fcfc8":"code","858f19f7":"code","a248b5c8":"code","b9a2666e":"code","a0163506":"code","5422e745":"code","ba2e21e6":"code","1d18cdfb":"code","6d7a2993":"code","5b44d017":"markdown","074ccdd7":"markdown","415ebc1f":"markdown","c6563cf8":"markdown","10d94c2c":"markdown","60a76852":"markdown","7c62c7f8":"markdown","f548a9b2":"markdown","d29996ef":"markdown","ac431964":"markdown","9f29709d":"markdown","afd803ef":"markdown"},"source":{"c03f6cd0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a72e61b9":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\n","89adad32":"train.head()\n","a0ffec03":"print('The shape of the training data frame is:',train.shape)\nprint('The shape of the test data frame is:',test.shape)","3c2517e4":"def convert_train_to_torch(train):\n    train_input = torch.tensor(train.values) #We convert everything in a torch\n    train_target = train_input[:,0] #We take separately the information of labels\n    train_input = train_input[:,1:]\n    return train_input, train_target\n\ndef convert_test_to_torch(test):\n    test_input = torch.tensor(test.values)\n    return test_input\n\n    ","5adfe7a0":"train_input, train_target = convert_train_to_torch(train)\ntest_input = convert_test_to_torch(test)","07cf60eb":"plt.imshow(train_input[1].view(28,28), cmap=\"gray\")\nplt.show()","6938541b":"def transform_shape_tensor(data):\n    data = torch.reshape(data, (-1,28,28))\n    data = data.unsqueeze(1)\n    return data","ab7fcfc8":"train_input = transform_shape_tensor(train_input)\ntest_input = transform_shape_tensor(test_input)","858f19f7":"print('After data transformation the shape of the training input is', train_input.shape)\nprint('After data transformation the shape of the test input is', test_input.shape)","a248b5c8":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        \n        #Define all the layers of the CNN\n        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3)\n        self.bn3 = nn.BatchNorm2d(64)\n        \n        \n        self.conv4 = nn.Conv2d(64, 128, kernel_size = 3)\n        self.bn4 = nn.BatchNorm2d(128)\n        \n        self.fc1 = nn.Linear(128, 100)\n        self.out = nn.Linear(100, 10)\n        self.dropout = nn.Dropout(p=0.3)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.bn1(x)\n        \n        x = F.max_pool2d(self.conv2(x), 2, 2)\n        x = F.relu(x)\n        x = self.bn2(x)\n        \n        \n        x = F.max_pool2d(self.conv3(x), 2, 2)\n        x = F.relu(x)\n        x = self.bn3(x)\n        \n        x = F.max_pool2d(self.conv4(x), 3, 3)\n        x = F.relu(x)\n        x = self.bn4(x)\n        \n        x = self.dropout(F.relu(self.fc1(x.view(-1,128))))\n        x = F.relu(self.out(x))\n        \n        return x\n    \n  ","b9a2666e":"\ndef train_model(model, train_input, train_target, mini_batch_size):\n    criterion = nn.CrossEntropyLoss()\n    \n    eta = 1e-1\n    optimizer = torch.optim.SGD(model.parameters(), lr = eta)\n    sum_loss = 0\n    \n    for b in range(0, train_input.size(0), mini_batch_size):\n        output = model(train_input.narrow(0, b, mini_batch_size))\n        loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n        #loss = F.cross_entropy(input = train_target.narrow(0, b, mini_batch_size), target = output)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        sum_loss += loss.item()\n    return sum_loss\n\n\n\ndef compute_nb_errors(model, input, target, mini_batch_size):\n    nb_errors = 0\n\n    for b in range(0, input.size(0), mini_batch_size):\n        output = model(input.narrow(0, b, mini_batch_size))\n        _, predicted_classes = output.max(1)\n        for k in range(mini_batch_size):\n            if target[b + k] != predicted_classes[k]:\n                nb_errors = nb_errors + 1\n                \n    return nb_errors","a0163506":"\n\n\nmini_batch_size= 100\n\nmodel = CNN()\nprint(model)\n\nfor k in range(30):\n    model.train\n    train_model(model, train_input.float(), train_target, mini_batch_size)\n    model.eval\n    print(compute_nb_errors(model,train_input.float(), train_target, mini_batch_size)) #, compute_nb_errors(model,test_input,new_test_target, mini_batch_size))\n    ","5422e745":"output = model(test_input.float())","ba2e21e6":"_,Predictions = output.max(1)\nPredictions.detach().numpy()","1d18cdfb":"#Predictions_pd = pd.Dataframe()\n\ndata = {'ImageId':np.arange(1,28001), 'label':Predictions.detach().numpy()}\noutput = pd.DataFrame(data)","6d7a2993":"output.to_csv('submission.csv', index =False)","5b44d017":"Now that our model is trained we have to pass the test set by our model to have the results. ","074ccdd7":"# Submission of the test target","415ebc1f":"#### Change the shape of the tensors\n\nAs we explain before each image is encoded by a tensor of shape 1x784. However we are going to reshape this, in order that eac image will be encoded by a tensor of shape 28x28. Indeed, this new shape is much more intuitive since it will represent the images by the resolution (images are 28x28 pixels). It is also better for the CNN models on images to work with 2D or 3D tensors. The tensors will have the shape of the image above.","c6563cf8":"![](http:\/\/cdn-images-1.medium.com\/max\/1000\/0*At0wJRULTXvyA3EK.png)","10d94c2c":"The following functions transform the pandas data frame into tensors. We have 3 tensors\n* First train_input is a tensor of dimension 42000x784 wich means that we have 42000 samples\/images and each one is encoded by a tensor of shape 1x784\n* train_target is a tensor of dimension 42000x1 and contains the digit that the image represents\n* test_target is of dimension 28000x784 and is as the train_input but with less samples ","60a76852":"# Digit Recognizer using PyTorch from scratch","7c62c7f8":"# Summary\nIn this notebook we will built a model that given an image representing a digit classifies the image with the corresponding digit. The images that we will work with are 28x28 pixels. Our model will be a Convolutional Neural Network and we will use the pytorch framework to design the NN.","f548a9b2":"# Training the model\n\nOn the following we will train our models. The idea is that our model learns from the data and update its parameter in order to learn to distinguish digits.\n\n* We define the function train_model. The idea of this function is to pass an image by the model and look at the output if the output is not the corresponding label (ex: the image is a 3 but our model thinks it is an 8) then we are going to correct our parameters. This correction is done using the gradient, the idea is that we have a loss function that represent the error of our model and we compute the gradient wrt the parameters in order to minimize this error.\n\n* We also define the function compute_nb_errors that computes how many labels are correct. This function is usefull to have an idea of how well we are doing. \n\n","d29996ef":"## Import Libraries\n","ac431964":"#  Create the Model: a CNN\n\nHere we define our model $f(x)$. We can see the network as a function f that given an image x it will give an output between 0,..,9\n\n$$f: R^{724} \\Rightarrow 0,...,9$$\n\nThe question then is:\n\n## How do we code the model?\n\nOur first model will be a CNN and to code it we will create a subclass of the already define in the torch package class Module. Our class has to have at least 2 important methods:\n\n* The constructor __init__ where we specify the attributes of our class. For example in our case we will have 2 convolutional layers followed each one by 2 Batch Normalizations; and 2 linear layers. If you want to understand in deep what are the convolutional NN, linear layers, Batch Normalization, dropout, Relu you can write it on comments. A good series of video that explain this in a practical and easy way is available on the Deep Lizard youtube channel: https:\/\/www.youtube.com\/watch?v=v5cngxo4mIg&list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG. \n\n* The forward method. This method corresponds to passing the data set through our model to applying our function f to our data set. \n\n","9f29709d":"# Pytorch Framework\n\n Pytorch is a python package that is an open source ML library (http:\/\/pytorch.org) that provides two high-level features:\n \n* Tensor computation with stong GPU acceleration\n* Deep Neural Networks built on a tape-based autograd system\n\n####  What is a tensor?\n\nA tensor is a generalized matrix.\n\n* A 0d tensor is a scalar\n* A 1d tensor is a vector\n* A 2d tensor is a Matrix\n* A 3d tensor can be seen as a vector of matrices(e.g. A multichannel image)\n\n##### Pytorch advantage is:\n\n* Efficient tensor operations on CPU\/GPU\n* Automatic differentiation on tensors (necessary to train NN, since we use gradient descent)\n* Optimizers (Algorithms that optimize the training of a NN for ex: SGD instead of a usual GD)\n\nWe will explain later what is a GD or a SGD.\n\nIn the recent years Pythorch has been used more and more and today is the most used ML framework in academia:\n\n![](https:\/\/blog.exxactcorp.com\/wp-content\/uploads\/2020\/01\/pasted-image-0-5-.png)","afd803ef":"### Let's transform our images into tensors.\n\nBy the moment our images are pandas data frame since on the first lines we used the method pd.readcsv().\n\nLet's observe how each image is represented now and how we will transform it into a tensor. We observe below that the training set has a supplementary column because of the label. The label is the digit that the image is supposed to represent\n\n"}}