{"cell_type":{"c7e71c64":"code","198143e0":"code","617075f2":"code","29ccf408":"code","a91b611a":"code","eaa8d4f3":"code","a451f191":"code","57916c88":"code","3651cbde":"code","7b35e693":"code","368f4f82":"code","aac5a4d8":"code","15234379":"code","79174b90":"code","3fca3790":"code","24bec349":"code","2d39498b":"code","d9f6e505":"code","47c93653":"code","e153a3df":"code","368255f7":"code","2d3d7994":"code","ae5f96d0":"code","fa440d3d":"code","8536148f":"markdown","3b14202a":"markdown","6ed14091":"markdown","4c224a59":"markdown","5713a523":"markdown","402e905d":"markdown","912e9dab":"markdown","cf8b6293":"markdown","fbb578aa":"markdown","7f0c5835":"markdown","86a3e45a":"markdown","58a64bbe":"markdown","063eca95":"markdown","13f28f18":"markdown","96ef7279":"markdown","ec12b763":"markdown","ac8d67af":"markdown","9617ef09":"markdown","bfa6df80":"markdown","7711b231":"markdown"},"source":{"c7e71c64":"from pandas import read_csv\n\nsample_submission = read_csv('..\/input\/sample_submission.csv')\n\nonly0s = sample_submission.assign(item_cnt_month = 0)\nonly1s = sample_submission.assign(item_cnt_month = 1)\n\nonly0s.head()","198143e0":"only1s.head()","617075f2":"del only0s, only1s\n\ntrain = read_csv('..\/input\/sales_train.csv')\ntrain_by_month = train.groupby(['date_block_num', 'shop_id', 'item_id'])[['item_cnt_day']].sum().clip(0, 20)\ntrain_by_month.columns = ['item_cnt_month']\ntrain_by_month = train_by_month.reset_index()\ndel train\n\ntrain_by_month.head()","29ccf408":"train_by_month.groupby('date_block_num')['item_cnt_month'].mean().tail()","a91b611a":"print('About %.2f%% of train values are 0s' % (train_by_month[train_by_month['item_cnt_month'] == 0].shape[0] * 100 \/ train_by_month.shape[0]))","eaa8d4f3":"test = read_csv('..\/input\/test.csv')\nlen(test.shop_id.unique()), len(test.item_id.unique()), len(test)","a451f191":"from itertools import product\nfrom pandas import DataFrame\n\npairs = DataFrame(list(product(list(range(34)), test.shop_id.unique(), test.item_id.unique())), columns = ['date_block_num', 'shop_id', 'item_id'])\npairs.head()","57916c88":"pairs.shape","3651cbde":"def displayWithSize(df):\n    print('Shape : %i x %i' % df.shape)\n    m = df.memory_usage().sum()\n    if m >= 1000000000:\n        print('Total memory usage : %.2f Go' % (m \/ 1000000000))\n    else:\n        print('Total memory usage : %.2f Mo' % (m \/ 1000000))\n    return df.head()","7b35e693":"from numpy import uint8, uint16, float16\n\npairs_red = pairs.assign(date_block_num = pairs['date_block_num'].astype(uint8))\npairs_red = pairs_red.assign(shop_id = pairs['shop_id'].astype(uint8))\npairs_red = pairs_red.assign(item_id = pairs['item_id'].astype(uint16))\ndel pairs\n\ninflated_train = pairs_red.merge(train_by_month, on=['date_block_num', 'shop_id', 'item_id'], how='left')\ninflated_train.fillna(0.0, inplace=True)\ndel pairs_red\n\ndisplayWithSize(inflated_train)","368f4f82":"inflated_train.dtypes","aac5a4d8":"from numpy import finfo\n\nfinfo(float16)","15234379":"from numpy import float64\n\ninflated_train[inflated_train['date_block_num'] == 33]['item_cnt_month'].astype(float64).clip(0, 20).mean()","79174b90":"from matplotlib.pyplot import plot\n%matplotlib inline\n\nsales_by_month = inflated_train.groupby('date_block_num')['item_cnt_month'].sum().tolist()\nplot(sales_by_month)","3fca3790":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom matplotlib.pyplot import figure\n\ndecomposition = seasonal_decompose(sales_by_month, freq=12, model='multiplicative')\nfig = figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(15, 8)","24bec349":"def rolling_mean(data, timespan):\n    n = len(data)\n    output = []\n    for i in range(n):\n        maxWindow = min(i - max(0, i-timespan\/\/2), min(n, i+timespan\/\/2) - i)\n        output.append(data[i-maxWindow:i+maxWindow+1])\n    return list(map(lambda x: sum(x) \/ len(x), output))\n\nrmean = rolling_mean(sales_by_month, 12)\nplot(rmean)","2d39498b":"from numpy import mean\n\nincr_step = mean(list(map(lambda x: x[0] - x[1], zip(rmean[22:28], rmean[21:27]))))\nincr_step","d9f6e505":"rmean_corrected = rmean[:28]\ncurrent = rmean_corrected[-1]\n\nfor _ in range(6):\n    current += incr_step\n    rmean_corrected.append(current)\n\nplot(rmean_corrected)","47c93653":"forecast = list(map(lambda x: x[0] * x[1], zip(rmean_corrected, decomposition.seasonal)))\n\nplot(sales_by_month)\nplot(forecast)","e153a3df":"forecast.append((rmean_corrected[-1] + incr_step) * decomposition.seasonal[-12])\n\nplot(sales_by_month)\nplot(forecast)","368255f7":"month_n = 32\nmonth_n_plus_1 = 33\n\nitem_cnt_month_n = inflated_train[inflated_train['date_block_num'] == month_n]['item_cnt_month']\n\nC0 = sum(item_cnt_month_n == 0)\nC = len(item_cnt_month_n)\nmt = inflated_train[inflated_train['date_block_num'] == month_n_plus_1]['item_cnt_month'].mean()\n\nitem_cnt_month_n_plus_1 = item_cnt_month_n \/ sales_by_month[month_n]\nitem_cnt_month_n_plus_1 = (item_cnt_month_n_plus_1 * forecast[month_n_plus_1]).clip(0, 20)\nitem_cnt_month_n_plus_1[item_cnt_month_n_plus_1 == 0] = (C \/ C0) * (mt - item_cnt_month_n_plus_1.mean())\n\nitem_cnt_month_n_plus_1.mean(), mt","2d3d7994":"from sklearn.metrics import mean_squared_error\nfrom numpy import sqrt\n\ndef rmse(y, y_pred):\n    return sqrt(mean_squared_error(y, y_pred))\n\nrmse(item_cnt_month_n_plus_1, inflated_train[inflated_train['date_block_num'] == month_n_plus_1]['item_cnt_month'])","ae5f96d0":"month_n = 33\nmonth_n_plus_1 = 34\n\nitem_cnt_month_n = inflated_train[inflated_train['date_block_num'] == month_n]['item_cnt_month']\n\nC0 = sum(item_cnt_month_n == 0)\nC = len(item_cnt_month_n)\nmt = 0.284\n\nitem_cnt_month_n_plus_1 = item_cnt_month_n \/ sales_by_month[month_n]\nitem_cnt_month_n_plus_1 = (item_cnt_month_n_plus_1 * forecast[month_n_plus_1]).clip(0, 20)\nitem_cnt_month_n_plus_1[item_cnt_month_n_plus_1 == 0] = max(0, min(20, (C \/ C0) * (mt - item_cnt_month_n_plus_1.mean())))\n\nitem_cnt_month_n_plus_1.mean(), mt","fa440d3d":"DataFrame(list(zip(range(len(item_cnt_month_n_plus_1)), item_cnt_month_n_plus_1)), columns=['ID', 'item_cnt_month']).to_csv('submission.csv', index=False)","8536148f":"Not quite what we expected ! This surely means that there are a **lot more 0s in test set than in train set**. Are there actually any 0s in train set ?","3b14202a":"Seems good. Now we can compare the mean.","6ed14091":"What is the RMSE ?","4c224a59":"The submission with only 0s gives us a score of $1.25011$, and the one with only 1s yields $1.4124$ Using the following formula, those two values allow us to compute the exact mean of true values in public leaderboard.\n\n$$MSE(1) - MSE(0) = \\sum_{i=0}^N \\frac{(y_i - 1)^2}{N} - \\sum_{i=0}^N \\frac{y_i^2}{N}$$\n\n$$ = \\sum_{i=0}^N \\frac{y_i^2 - 2y_i + 1}{N} - \\frac{y_i^2}{N}$$\n\n$$ = \\sum_{i=0}^N \\frac{1 - 2y_i}{N}$$\n\n$$ = 1 - 2\\sum_{i=0}^N \\frac{y_i}{N}$$\n\n$$ = 1 - 2\\overline{y}$$\n\nGiven that $MSE(0) = 1,5627750121$ and $MSE(1) = 1.9949$, we can therefore establish that the mean value of true values is $0.283936502$ (We'll consider $0.284$ for convenience).\n\nIt is interesting to compare with the mean of train set to see **if public\/private split can be identified**.\n\nTo do this, we first need to **aggregate train sales monthly** to be on the same page as test. We must not forgt to clamp values between 0 and 20.","5713a523":"All right ! We have our forecasted values for both month 33 and 34, let's do some predictions !","402e905d":"Ok ! Now we can compare mean values of last month in train and month in test !","912e9dab":"Now join with train set. We also downcast values to reduce memory usage.","cf8b6293":"Looks rather nice ! We can forecast for month 34.","fbb578aa":"Very interesting ! The residual part **oscillate between 0.9 and 1.1** which is **only 10%** of original data. This is low enough. We can hence build a **multiplicative factor based on trend * seasonal**, that will, combined with latest months, help us build a no-ML based model.\n\nThe strategy for this seasonal decompose is simple : **seasonal part is repetitive** so we will just **use it as is**. **Trend part is not** : we will use a rolling mean scheme to **forecast trend** on month 33 (validation) and 34 (kaggle prediction). We then multiply both, to get a factor. Then, to forecast month n+1, we divide month n sales by the factor for month n, and multiply by factor for month n+1.","7f0c5835":"**Way less** than test set. There has to be something in the way the organizers built the test set. Let's have a look at it.","86a3e45a":"### ** WARNING **\n\n**Downcasting** is a good strategy to help **reduce memory error chances**, but it has its ***limit*** : each data type has its own min and max values, for example **float16 goes from -65504 to +65504**, hence when calculating things like mean or sum, during the calculus **the value can exceed the max and result in a NaN or inf value**. So be careful :)\n\nThat's why we did not downcast **item_cnt_month**.","58a64bbe":"Not quite convinced we can call this a good result. But still, it is way better than the constant 0s prediction so...\n\nThe interest lied in the approach anyway :)","063eca95":"Amazing ! This clearly means that public\/private split in the leaderboard is **completely random** !!\n\nHow does the sales trend look like with our new train set ?","13f28f18":"Let's see if product of both trend and seasonal is close to reality","96ef7279":"A nice trend draws itself ! It is **going up**, because we are only looking at a **small subset of items\/shops**, that is on of the **latest as it is from test set**. Can we use a seasonal decomposition on this one ?","ec12b763":"## ** LB probing, public\/private split and simple seasonal decomposition **\n> \nHello fellow Kagglers :)\n\nThe kernel I'm building here aims at trying to use **leaderboard probing** to obtain a reasonable score on the public LB. The main idea is to **not use any ML model** to predict sales for the month of November 2015.","ac8d67af":"## ** Part I - LB probing and forecasting **\n\nWe'll first use sample submissions, one with only 0s and one with only 1s to calculate the mean of true LB values.","9617ef09":"## ** Part II - No-ML predictions **\n\nThe process is simple. We know data about month N, and we want to predict for month N+1.\n\nWe have real **mean sales for month N** and forecasted **mean sales for month N+1**.\nWe know exactly the **mean values of month N+1** (or at least an estimation).\n\nWe proceed as follow :\n - We take all **item_cnt_month** values from month N\n - We divide values by **mean sales of month N**\n - We multiply values by **mean sales of month N+1**\n - We fill 0 values with a constant value such that the total mean of all values becomes the **mean values of month N+1** (this one goes from the fact that to minimize RMSE with a constant, you need to take the mean of true values)\n \nThe last step is a little bit tricky. Let us note :\n - $C_0$ the number of 0s in values\n - $C$ the number of values\n - $m_c$ the current mean\n - $m_t$ the target mean\n - $y_i$ the value $i$\n \nWe then have :\n\n$$m_c = \\sum_{i=0}^C \\frac{y_i}{C} \\iff m_c = \\sum_{i=0, y_i\\ne0}^C \\frac{y_i}{C}$$\n\nAnd :\n\n$$m_t = m_c + (m_t - m_c) = \\sum_{i=0, y_i\\ne0}^C \\frac{y_i}{C} + \\frac{C_0}{C_0}(m_t - m_c) = \\sum_{i=0, y_i\\ne0}^C \\frac{y_i}{C} + \\sum_{i=0, y_i=0}^C \\frac{C}{C_0}\\frac{m_t - m_c}{C}$$\n\n$$ = \\frac{1}{C}(\\sum_{i=0, y_i\\ne0}^C y_i + \\sum_{i=0, y_i=0}^C \\frac{C}{C_0}(m_t - m_c))$$","bfa6df80":"**42 * 5100 is 214200** so yes, basically the test set is built using **all combination of those 42 shops and 5100 items**. No wonder there are so many 0s in true values.\n\nWe need to make the **train set equivalent** to this scheme. To do that, we'll simply use those 42 shops and 5100 items, make all possible pairs with **date_block_num** from 0 to 33 and join with train set. Let's see how it goes.**","7711b231":"We see a clear divergence at the end, that's because we don't have data about the future. Therefore we need to smoothen it using a mean of last 6 months trend."}}