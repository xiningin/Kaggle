{"cell_type":{"d47befb8":"code","d4fbb03e":"code","f8d18883":"code","f1cd881d":"code","5a908919":"code","b014ba8d":"code","dbd7f426":"code","dd1440b5":"code","1fa930d4":"code","ac61c68d":"code","bf62111e":"code","f4f6b55b":"code","f600f814":"code","ab999ee4":"code","3e0f4052":"code","88d76920":"code","c657f46d":"code","5c3776d6":"code","e43ea080":"code","88f45d7a":"code","718ee634":"code","fefc1694":"code","9e76a64b":"markdown","5c533160":"markdown","749a63e2":"markdown","2425024e":"markdown"},"source":{"d47befb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d4fbb03e":"# Numerical libraries\nimport numpy as np   \n\n# to handle data in form of rows and columns \nimport pandas as pd    \n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns\n\n# Import Logistic Regression machine learning library\nfrom sklearn.linear_model import LogisticRegression\n\n#Sklearn package's data splitting function which is based on random function\nfrom sklearn.model_selection import train_test_split\n\n\n# calculate accuracy measures and confusion matrix\nfrom sklearn import metrics\n\n# To scale the dimensions we need scale function which is part of sckikit preprocessing libraries\nfrom sklearn import preprocessing\n\n# To enable plotting graphs in Jupyter notebook\n%matplotlib inline ","f8d18883":"# reading the CSV file into pandas dataframe\ndiab_df = pd.read_csv('\/kaggle\/input\/diabetes-dataset\/diabetes2.csv')","f1cd881d":"# Let's eyeball the data\ndiab_df.head()","5a908919":"diab_df.info()","b014ba8d":"diab_df.shape\n# There are 768 rows and 9 columns","dbd7f426":"diab_df.describe().T\n# Insulin and DiabetesPedigreeFunction has mean and median has big diff","dd1440b5":"# check missing values count\nmissing_values=diab_df.columns[diab_df.isnull().any()]\ndiab_df[missing_values].isnull().sum()","1fa930d4":"# as part of eyeballing data we found there are 0 values in many columns, \n# let's find out how many 0 values are there in all columns\n\n(diab_df == 0).sum(axis=0)","ac61c68d":"#We will use 'median' to replace 0 for all columns except for 'Insulin' as diff between mean and median was big\ndiab_df['Pregnancies'].replace(0,diab_df['Pregnancies'].median(),inplace=True)\ndiab_df['Glucose'].replace(0,diab_df['Glucose'].median(),inplace=True)\ndiab_df['BloodPressure'].replace(0,diab_df['BloodPressure'].median(),inplace=True)\ndiab_df['SkinThickness'].replace(0,diab_df['SkinThickness'].median(),inplace=True)\ndiab_df['BMI'].replace(0,diab_df['BMI'].median(),inplace=True)\n\ndiab_df['Insulin'].replace(0,diab_df['Insulin'].mean(),inplace=True)","bf62111e":"# let's check if all 0 values are replaced now\n\n(diab_df == 0).sum(axis=0)","f4f6b55b":"# Let us look at the target column which is 'Outcome' to understand how the data is distributed amongst the various values\ndiab_df.groupby(['Outcome']).count()\n\n# The ratio is almost 1:2 in favor of class 0.  The model's ability to predict class 0 will \n# be better than predicting class 1. ","f600f814":"# Pairplot using sns\n\nsns.pairplot(diab_df)","ab999ee4":"# data for Insulin and DiabetesPedigreeFunction looks skewed\n\n# the mean for Insulin is 80(rounded) while the median is 30.5 which clearly indicates an extreme long tail on the right\n# the mean for DiabetesPedigreeFunction is 0.47 while the median is 0.37 which clearly indicates a long tail on the right\n","3e0f4052":"diab_df.corr()\n# there is no strong correlation between any columns\n","88d76920":"# Data for BMI, Glucose and BloodPressure has normal distribution\nsns.distplot(diab_df['Glucose'],kde=True)\n","c657f46d":"sns.distplot(diab_df['BloodPressure'],kde=True)\n","5c3776d6":"sns.distplot(diab_df['BMI'],kde=True)","e43ea080":"# most of the data columns has some outliers, We will see Insulin separetly due to max value \nplt.subplots(figsize=(20,15))\nsns.boxplot(data=diab_df.drop(['Insulin','Outcome'],axis=1))","88f45d7a":"# Insulin has high number of outliers compared to other columns\nplt.subplots(figsize=(20,15))\nsns.boxplot(data=diab_df['Insulin'])","718ee634":"x=diab_df.drop('Outcome',axis=1)\ny=diab_df['Outcome']\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25, random_state=1)\ntype(x_train)\n\nmodel=LogisticRegression()\nmodel.fit(x_train,y_train)\ny_predict=model.predict(x_test)\nmodel_score=model.score(x_test,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","fefc1694":"# To scale the dimensions we need scale function which is part of sckikit preprocessing libraries\nx_train_scaled = preprocessing.scale(x_train)\nx_test_scaled = preprocessing.scale(x_test)\n\nmodel=LogisticRegression()\nmodel.fit(x_train_scaled,y_train)\ny_predict=model.predict(x_test_scaled)\nmodel_score=model.score(x_test_scaled,y_test)\nprint('Accuracy = ',model_score)\nprint(metrics.confusion_matrix(y_test,y_predict))\n","9e76a64b":"## Observation\n<b>Model without scaling gave better accuracy hence we will review results of first model<\/b>\n\n### Logistic Regression with Train and Test ratio of 75:25\n\n#### Analyzing the confusion matrix\n \n<b>True Positives (TP)<\/b>: we correctly predicted that 42 patients Diabetes.\n\n<b>True Negatives (TN):<\/b> we correctly predicted that 110 patients DO NOT have Diabetes.\n\n<b>False Positives (FP):<\/b> we incorrectly predicted that 13 patients have Diabetes (a \"Type I error\") \nFalsely predict positive Type I error\n\n\n<b><font color='Red'>False Negatives (FN):<\/font><\/b> we incorrectly predicted that 27 patients DO NOT have Diabetes (a \"Type II error\") \nFalsely predict negative Type II error\n\n<b>Accuracy Score of the Model is 79.16%<\/b>\n\n","5c533160":"## Diabetes dataset from Kaggle\nhttps:\/\/www.kaggle.com\/kandij\/diabetes-dataset\n\n\nThe data was collected and made available by \u201cNational Institute of Diabetes and Digestive and Kidney Diseases\u201d as part of the Pima Indians Diabetes Database. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here belong to the Pima Indian heritage (subgroup of Native Americans), and are females of ages 21 and above.\n\n#### Dataset have following column\n- Pregnancies\n- Glucose\n- BloodPressure\n- SkinThickness\n- Insulin\n- BMI\n- DiabetesPedigreeFunction\n- Age\n- Outcome\n","749a63e2":"### Let's see if we can improve the model -----Iteration 2 -------","2425024e":"## Model Prediction"}}