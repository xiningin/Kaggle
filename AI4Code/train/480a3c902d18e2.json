{"cell_type":{"880b0c5a":"code","c7c32616":"code","33c4055e":"code","9c09d7ff":"code","7db4ea76":"code","9ffde959":"code","4441187d":"code","ef573041":"code","ff5a44b7":"code","2bebff76":"code","e3a4a8ff":"code","ad242808":"code","771cc142":"code","723b52a5":"code","e6e81af7":"markdown","59bf887c":"markdown","27062f75":"markdown","5080e017":"markdown","50cbaae4":"markdown","626aedff":"markdown","e2288691":"markdown","6c437b98":"markdown","b5dd6d44":"markdown","bddb07aa":"markdown","02d3acf0":"markdown","b9586722":"markdown","88d60cc0":"markdown","47c945cd":"markdown"},"source":{"880b0c5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7c32616":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats","33c4055e":"data_dir = '\/kaggle\/input\/tabular-playground-series-may-2021'\nrandom_seed = 80808","9c09d7ff":"train_data = pd.read_csv(os.path.join(data_dir, 'train.csv'))\ntr_X = train_data.iloc[:, 1:51] # Feature columns\ntr_y = train_data['target']\n\nprint(train_data.shape)\ntrain_data.head()","7db4ea76":"print(\"Data Types by Columns\")\nprint(train_data.dtypes)\n\nnull_count = np.sum(np.isnan(tr_X.to_numpy()))\n\nprint(f'\\nNumber of missing values in features: {null_count}.')","9ffde959":"fig, ax = plt.subplots()\n\nax.set_title(\"Count of Samples by Target Class\")\nsns.countplot(x='target', data=train_data, order=['Class_1', 'Class_2', 'Class_3', 'Class_4'])","4441187d":"row_count = train_data.shape[0]\n\nagg_df = tr_X.agg(['min', 'max', 'mean']).transpose()\n\n# agg_df\n\nagg2_ls = [\n    ((f1 > 0).sum(),\n     (f1 < 0).sum()\n    )\n    for f1 in [tr_X[col] for col in tr_X]\n]\n\nagg2_df = pd.DataFrame(agg2_ls, columns=['PosCnt', 'NegCnt'], index=agg_df.index)\nagg2_df['PosProp'] = agg2_df['PosCnt'].div(row_count)\nagg2_df['NegProp'] = agg2_df['NegCnt'].div(row_count)\n\nfeat_ch_df = pd.concat([agg_df, agg2_df.iloc[:, [0, 2, 1, 3]]], axis=1)\n\nfeat_ch_df","ef573041":"print(f'Maximum positive proportion: {feat_ch_df[\"PosProp\"].max()}')\nprint(f'Mean positive proportion: {feat_ch_df[\"PosProp\"].mean()}')\nprint(f'Maximum negative proportion: {feat_ch_df[\"NegProp\"].max()}')\nprint(f'Mean negative proportion: {feat_ch_df[\"NegProp\"].mean()}')\n","ff5a44b7":"feat_arr = tr_X.to_numpy()\n\npos_arr = (feat_arr > 0).astype('int32')\nrow_cnts = np.sum(pos_arr, axis=1)\n\nrc_gb = pd.DataFrame({'PosCnt': row_cnts, 'target': tr_y}).groupby(by='target')\ncl_cnt = rc_gb.aggregate(['min', 'max', 'mean'])\n\nprint(\"Positive Values in Rows by Classes\")\ncl_cnt","2bebff76":"# Create data for factors\n\n# Use previous structure to get positive counts\n\ncl_pos_cnt = rc_gb.sum().iloc[:, 0]\npos_cnt = int(cl_pos_cnt.sum())\n\nrow_list = []\n\nfor col in tr_X:\n    pos_cl_counts = train_data.loc[train_data[col] > 0, 'target'].value_counts().sort_index()\n    cl_act_prop = pos_cl_counts.div(cl_pos_cnt)                 # Actual proportion of positive values by class for this feature\n    f_mean_prop = cl_act_prop.mean()                             # Mean proportion across classes\n    cl_factor = cl_act_prop.div(f_mean_prop)                    # Scaled factor for each class\n\n    row_list.append([col] + list(cl_factor))","e3a4a8ff":"exp_df = pd.DataFrame.from_records(row_list, columns=['Feature'] + list(cl_pos_cnt.index), index=['Feature'])\n\nfig, ax = plt.subplots()\nfig.set_figheight(15)\n\nsns.heatmap(exp_df, center=1.0, annot=True, fmt='0.2f', ax=ax)","ad242808":"corr_df = train_data.iloc[:, 1:51].corr()\n\nprint(\"Sample of feature correlations\")\n\ncorr_df.iloc[:10, :10]","771cc142":"corr_arr = corr_df.to_numpy()\nc_max = np.max(corr_arr[corr_arr < 1])\nc_min = np.min(corr_arr)\n\nprint(\"Strongest positive correlation between features: \", c_max)\nprint(\"Strongest negative correlation between features: \", c_min)","723b52a5":"# Correlation Map\n\nfig, ax = plt.subplots(figsize=(20, 12))\nax.set_title(\"Correlation Heatmap with Enhanced Color\", fontsize=14)\n\nsns.heatmap(corr_df, vmax=c_max*1.1, center=0.0, annot=False)","e6e81af7":"# Load Data and Top-Level Checks","59bf887c":"# Synopsis\n\nThis is an examination of feature and class characteristics.  The approach is influenced by the large number of 0 values in the data.","27062f75":"# Heatmap of Feature Positive Proportions by Class\n\nIn order to highlight the comparison between classes for each row, we will scale the proportions of each cell.  If we used raw counts, the large number of samples from Class 2 would overwhelm other variation.  If we used the proportions without scaling, then the differences between features (rows) might hide the patterns--the whole row would be lighter for the features with large numbers of positive values.  We will also scale for the differences in positive value rates between classes; this keeps Class 4 from consistently having a lower rating.","5080e017":"# Characteristics of Rows by Target Classes\n\nThis table shows the minimum, maximum and mean counts of positive values by rows within each target class.","50cbaae4":"## Data types and missing values","626aedff":"# Setup","e2288691":"Note that the range of these values is narrow compared to other data sets.  No class has more than 35% more or 20% less positive values than we would expect for any feature.  So the presence of a postive value in a sample for any one feature will have limited effect on the probabilities that we would calculate.","6c437b98":"# Correlations\n\nAs we look for correlations between features, we find that they are very small.  If we did a heatmap without significant color enhancement, everything except the diagonal would be the same, essentially a correlation of 0.  The color enhancement is done by setting vmax which controls the upper limit of the color range.","b5dd6d44":"# Basic Characteristics by Feature","bddb07aa":"There is only one feature with more than 10 negative values--feature_31.  At 199 negative values, these are negligible, representing about 0.2% of all rows.\n\nPositive features are much less common than 0's.  Only about 20% of all values are positive.","02d3acf0":"We can see that rows in class 4 have about 10% less positive values than the rows in the other classes.  Note that both class 2 and class 4 have at least one sample with no positive values.","b9586722":"We see that all our features are integers and no values are missing.","88d60cc0":"# Distribution of Target Values","47c945cd":"This table provides the minimum, maximum and mean values for each feature.  The other rows are the counts and proportions of positive and negative values."}}