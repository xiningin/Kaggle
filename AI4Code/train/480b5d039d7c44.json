{"cell_type":{"65c2883a":"code","6f5a550f":"code","4a841c16":"code","b9cfdbe0":"code","ab42f01a":"code","e5845878":"code","09995575":"code","03c665a9":"code","0d441e81":"code","90fb9a6b":"code","3137fee3":"code","5a0da6ac":"code","57e35328":"code","dfc65b02":"code","4bbfd970":"code","e408b353":"code","a11657a8":"code","a8e01d09":"code","4f85a533":"code","24dcd9f8":"code","13b7e862":"code","85fec859":"code","1b8e3ac6":"code","513ab9d8":"code","e251d396":"code","92c5337f":"code","2bb018f2":"code","c922ae7a":"code","c2f8f146":"code","cf0979b6":"code","cb6062ae":"code","b7247bea":"code","8bd2e187":"code","3f4d9bc2":"code","d7b10a5d":"code","2bb302fc":"code","2ebb4fcc":"code","3458c14b":"code","2549ad9c":"code","e421d896":"code","f00924d4":"code","3d8b5b5c":"code","d1fdb9aa":"code","24402f6c":"code","51a9b877":"code","f6d6de69":"code","cb21f1bd":"code","a976f8dc":"code","14bb713c":"code","0193469d":"code","3273c066":"code","c1327924":"markdown","d70039a9":"markdown","c97290d6":"markdown","26394a8c":"markdown","b2d9184c":"markdown","8936c1b5":"markdown","8b10e5a8":"markdown","ec4c9e98":"markdown","d8a7c070":"markdown","57bafc3b":"markdown","9d1971cf":"markdown","3a008e86":"markdown","cfeaa837":"markdown","36144023":"markdown","fc261215":"markdown","357fdcee":"markdown","99f11e9d":"markdown","686ef8d6":"markdown","ae81b4f5":"markdown","1a4d5610":"markdown","4eff413c":"markdown","a8dcfc08":"markdown","8ec49626":"markdown","a0e0dff1":"markdown"},"source":{"65c2883a":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, sys, seaborn as sns","6f5a550f":"data = pd.read_pickle(r'..\/input\/knn-model\/churndata.pkl')  ","4a841c16":"churndata=pd.DataFrame(data)","b9cfdbe0":"churndata.head()","ab42f01a":"df = churndata.drop(columns=['id', 'phone', 'total_revenue', 'cltv', 'churn_score'])","e5845878":"df.info()","09995575":"df['churn_value'].value_counts()","03c665a9":"round(df.describe(),2)    # Will only contain numeric values","0d441e81":"df.describe(include='all')   # We have to specify include= object or all, in order to contain more type of columns..","90fb9a6b":"df.describe(include='object')","3137fee3":"for i in df.columns:\n    print (i, len(df[i].unique())) #Print the column and number of unique values it contains","5a0da6ac":"pd.DataFrame([[i,len(df[i].unique())] for i in df.columns], columns=['Variable','Unique values'])","57e35328":"df_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\ndf_uniques","dfc65b02":"binary_variables=list(df_uniques[df_uniques['Unique Values']==2].index)\nbinary_variables","4bbfd970":"categorical_variables=list(df_uniques[(df_uniques['Unique Values']>2) & (df_uniques['Unique Values']<=6)].index)\ncategorical_variables","e408b353":"for i in categorical_variables:\n    print (i,df[i].unique())","a11657a8":"ordinal_variables = ['contract', 'satisfaction']","a8e01d09":"rest = list(set(df.columns) - set(ordinal_variables) - set(categorical_variables) - set(binary_variables))\nrest","4f85a533":"df['months'].unique()","24dcd9f8":"df['months'] = pd.cut(df['months'], bins=5)","13b7e862":"df.months","85fec859":"ordinal_variables.append('months')","1b8e3ac6":"ordinal_variables","513ab9d8":"numeric_variables = list(set(df.columns) - set(ordinal_variables) - set(categorical_variables) - set(binary_variables))","e251d396":"numeric_variables","92c5337f":"df[numeric_variables].hist(figsize=(12, 6))","2bb018f2":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder","c922ae7a":"lb, le = LabelBinarizer(), LabelEncoder()","c2f8f146":"for column in ordinal_variables:\n    df[column] = le.fit_transform(df[column])                #Ordinal categorial variables","cf0979b6":"df[ordinal_variables].astype('category').describe()","cb6062ae":"for column in binary_variables:\n    df[column] = lb.fit_transform(df[column])            #Binary variables","b7247bea":"nominal_variables = list(set(categorical_variables) - set(ordinal_variables))","8bd2e187":"nominal_variables","3f4d9bc2":"df = pd.get_dummies(df, columns = nominal_variables, drop_first=True)         #Nominal categorical variables","d7b10a5d":"df.describe().T    ","2bb302fc":"from sklearn.preprocessing import MinMaxScaler\nmm = MinMaxScaler()","2ebb4fcc":"for column in [ordinal_variables + numeric_variables]: \n    df[column] = mm.fit_transform(df[column])","3458c14b":"round(df.describe().T, 3)","2549ad9c":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score","e421d896":"y, X = df['churn_value'], df.drop(columns='churn_value')\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)","f00924d4":"# Estimate KNN model and report outcomes\nknn = KNeighborsClassifier(n_neighbors=3)\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n# Precision, recall, f-score from the multi-class support function\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))","3d8b5b5c":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","d1fdb9aa":"cm=confusion_matrix(y_test,y_pred, labels=knn.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\ndisp.plot(cmap='Blues')","24402f6c":"y_test.value_counts()  #These values confirm that our confusion matrix is well labeled","51a9b877":"knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n# Precision, recall, f-score from the multi-class support function\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))","f6d6de69":"cm=confusion_matrix(y_test,y_pred, labels=knn.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\ndisp.plot(cmap='Blues')","cb21f1bd":"max_k = 40\nf1_scores = list()\nerror_rates = list() # 1-accuracy\n\nfor k in range(1, max_k):\n    \n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n    knn = knn.fit(X_train, y_train)\n    \n    y_pred = knn.predict(X_test)\n    f1 = f1_score(y_pred, y_test)\n    f1_scores.append((k, round(f1_score(y_test, y_pred), 4)))\n    error = 1-round(accuracy_score(y_test, y_pred), 4)\n    error_rates.append((k, error))\n    \nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nerror_results = pd.DataFrame(error_rates, columns=['K', 'Error Rate'])","a976f8dc":"sns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = f1_results.set_index('K').plot(figsize=(12, 12), linewidth=6)\nax.set(xlabel='K', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.title('KNN F1 Score')","14bb713c":"sns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = error_results.set_index('K').plot(figsize=(12, 12), linewidth=6)\nax.set(xlabel='K', ylabel='Error Rate')\nax.set_xticks(range(1, max_k, 2))\nplt.title('KNN Elbow Curve')","0193469d":"knn21 = KNeighborsClassifier(n_neighbors=21, weights='distance')\nknn21 = knn21.fit(X_train, y_train)\ny_pred = knn21.predict(X_test)\n# Preciision, recall, f-score from the multi-class support function\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))","3273c066":"cm=confusion_matrix(y_test,y_pred, labels=knn.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\ndisp.plot(cmap='Blues')","c1327924":"### Let's find out which variables we didn't identify yet:","d70039a9":"### Identifying numerical variables:","c97290d6":"### Reading our data with read_pickle function","26394a8c":"#### We can encode months column using binning encoding, let's create 5 bins:","b2d9184c":"# Bulding our first K-Nearest Neighbour model:","8936c1b5":"Above we can identify that contract and satisfaction correspond to ordinal categorical variables, therefore the rest corresponds to nominal.","8b10e5a8":"### Identifying categorical variables:","ec4c9e98":"## To determine the right value for K, let's examine results for values of K from 1 to 40.\nThis time, we will focus on two measures, the F-1 Score, and the Error Rate (1-Accuracy):","d8a7c070":"### Identifying binary variables:","57bafc3b":"Above look at min and max columns for each feature, it would be nice if they were max-min scaled i.e. [0,1] ","9d1971cf":"#### Splitting data into training and testing datasets: ","3a008e86":"Notice that our data contains a unique ID, an indicator for phone customer status, total lifetime value, total revenue, and a bank-estimated churn score, we will not be using these features, so they can be dropped from the data.","cfeaa837":"### Let's explore our data and see the type of each column:","36144023":"**As we can see from both plots above the elbow point is in k=21, so lets build a model with this characteristics:**","fc261215":"# Encoding variables:","357fdcee":"Drop first is used to avoid multicollinearity ","99f11e9d":"#### Plot F1 results:","686ef8d6":"# Classifying type of variables:","ae81b4f5":"### This time, let's use k=5 and weight the results by distance:","1a4d5610":"As our binary and one hot encoded variables have min and max values 0 and 1 respectively, we only have to scale our ordinal and numerical variables which contains values out of this range.","4eff413c":"#### Let's begin with a model with k=3 and evaluate its error metrics: ","a8dcfc08":"Above we can see how many unique values contain each object column, which will help us to identify type of categorical variables. ","8ec49626":"#### Hi, welcome to my project, today we will build K-Nearest Neighbours algorithms to predict customer churning.\nWe will use a customer churn dataset from a hypotetical telecommunications company which includes customer data, usage of long-distance, data usage, monthly revenue, type of offerings, and other services purchased by customers. We are using the subset of customers who have phone accounts. Since the data include a mix of numeric, categorical, and ordinal variables, we will load this data and do some preprocessing, then use KNN models to predict customer churn rates.","a0e0dff1":"### Let's identify which variables are binary, categorical nominal, categorical ordinal, and numerical. As we know the non-numeric features must be encoded."}}