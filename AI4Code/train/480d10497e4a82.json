{"cell_type":{"7b7c34d1":"code","eecd8142":"code","069bbd4b":"code","235d7215":"code","1779571a":"code","9f44049f":"code","53559fa4":"code","7f2db2a6":"code","5be65d57":"code","942536d5":"code","1a4575d3":"code","ae03d868":"code","cbed2df8":"code","7d16ec3a":"code","e193f694":"code","9a12b727":"code","433e8ab9":"code","3aa03289":"code","ccb20d3f":"code","73b8cf8d":"code","14576dbe":"code","0fe2a7aa":"code","65155a9d":"code","4dacc024":"code","1bc0e7bb":"code","6c6d398c":"code","00f0380c":"code","4347d788":"code","88742291":"code","27f095f9":"code","a12771e5":"code","15344c75":"code","0c45d1b6":"markdown","57c2c1a7":"markdown","34818048":"markdown","3deb1c2e":"markdown","4117d177":"markdown","95f0eae2":"markdown","54e6efc2":"markdown","e731ab30":"markdown","8da16bf2":"markdown","ff793045":"markdown","22d63036":"markdown"},"source":{"7b7c34d1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme()\nsns.set_palette(\"rainbow\")\n%matplotlib inline","eecd8142":"train_df = pd.read_csv(r\"..\/input\/titanic\/train.csv\")\n\ntrain = train_df.copy()\n\ntrain.sample(5)","069bbd4b":"def RowCol(data):\n    print(f\"Total Row is {data.shape[0]} and total Column is {data.shape[1]}\")     ","235d7215":"RowCol(train)","1779571a":"train.head()","9f44049f":"countfeature = [\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\ncountlist = list(enumerate(countfeature))","53559fa4":"plt.figure(figsize = (15,15))\nplt.suptitle(\"Countplot of Categorical Features\", fontsize=18)\nfor i in countlist: \n    plt.subplot(2,3,i[0]+1)\n    sns.countplot(data = train, x = i[1], hue = \"Survived\", palette=\"rainbow\")\n    plt.ylabel(\"\")\n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 10})\nplt.tight_layout()\nplt.show()","7f2db2a6":"numfeature = [\"Age\", \"Fare\"]\nenumfeat = list(enumerate(numfeature))","5be65d57":"plt.figure(figsize=(20,9))\nplt.suptitle(\"Distribution and Outliers of Numerical Data\", fontsize=20)\nfor i in enumfeat:\n    plt.subplot(1,4,i[0]+1)\n    sns.boxplot(data = train[i[1]], palette=\"rainbow\")\n    plt.xlabel(str(i[1]))\nfor i in enumfeat:\n    plt.subplot(1,4,i[0]+3)\n    sns.histplot(data = train[i[1]], palette=\"rainbow\", bins=15)\n    plt.xlabel(str(i[1]))\nplt.tight_layout()\nplt.show()","942536d5":"plt.figure(figsize=(15,12))\nplt.suptitle(\"Distribution & Kernel Density Estimation of Numerical Features\", fontsize=20)\nfor i in enumfeat:\n    plt.subplot(2,1,i[0]+1)\n    sns.histplot(x = train[i[1]], kde=True, bins=30, color=(0.50,0.20,0.70))\nplt.tight_layout()\nplt.show()","1a4575d3":"plt.figure(figsize=(6,8))\nplt.title(\"Correlation of Survival column with Independent Features\", fontsize=15)\ncorr = train.corr()[\"Survived\"].sort_values(ascending=False)[1:]\nsns.barplot(x=corr.index, y=corr, color=(0.90,0.30,0.50))\nplt.tight_layout()\nplt.show()","ae03d868":"plt.figure(figsize=(13,5))\nplt.title(\"Scatterplot of Age and Fare according to how many Survived\", fontsize=15)\nsns.scatterplot(data = train, x = \"Age\", y=\"Fare\", hue='Survived')\nplt.tight_layout()\nplt.show()","cbed2df8":"plt.figure(figsize=(15,5))\nplt.suptitle(\"Probability Distribution of numerical columns according to number of Survived\", fontsize = 20)\nfor i in enumfeat:\n    plt.subplot(1,2,i[0]+1)\n    sns.kdeplot(data=train, x=i[1], hue=\"Survived\")\nplt.tight_layout()\nplt.show()","7d16ec3a":"plt.figure(figsize=(13,11))\n\nplt.subplot(1,2,1)\nplt.title(\"Age according to Pclass and how many Survived\", fontsize=13)\nsns.boxplot(data = train, y =\"Age\", x =\"Pclass\", hue=\"Survived\", palette=\"cool\")\n\n#Two extreme outliers were ruining the boxplot on the right. Showfliers parameter has been used to remove the outliers of Fare. \nplt.subplot(1,2,2)\nplt.title(\"Embarked from which port according to Fare and how many Survived\", fontsize=13)\nsns.boxplot(data = train, y =\"Fare\", x =\"Embarked\", hue=\"Survived\", palette=\"cool\", showfliers=False)\n\nplt.tight_layout()\nplt.show()","e193f694":"def dropcol(data):\n    data.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\"], inplace=True)\ndropcol(data=train)","9a12b727":"train.head()","433e8ab9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns=[\"Survived\"]), train[\"Survived\"], \n                                                    test_size=0.2, random_state=42)","3aa03289":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest,chi2","ccb20d3f":"import missingno as msno\nmsno.matrix(train, color=(0.50,0.30,0.80))\nplt.show()\nx = train.isnull().sum()\nfor a, b in x.items():\n    if b > 0:\n        print(f\"There are {b} missing values in column: {a}\")","73b8cf8d":"X_train.head()","14576dbe":"def cabinnull(data):\n    data[\"Cabin\"].fillna(\"M\", inplace=True)\ncabinnull(X_train)\ncabinnull(X_test)","0fe2a7aa":"X_train.head()","65155a9d":"#handling missing values\ntf1 = ColumnTransformer(transformers=[\n    (\"AgeImputer\", SimpleImputer(), [2]),\n    (\"EmbarkedImputer\", SimpleImputer(strategy=\"most_frequent\"), [-1])\n], remainder=\"passthrough\")","4dacc024":"#encoding categorical features\ntf2 = ColumnTransformer(transformers=[\n    (\"SexEncoder\", OrdinalEncoder(), [3]),\n    (\"EmbarkedOneHot\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"), [1,7])\n], remainder=\"passthrough\")","1bc0e7bb":"# Scaling\ntf3 = ColumnTransformer([\n    ('scale',StandardScaler(),slice(0,-1))\n])","6c6d398c":"# Model\ntf4 = LogisticRegression()","00f0380c":"pipe = make_pipeline(tf1,tf2,tf3,tf4)","4347d788":"# Display Pipeline\n\nfrom sklearn import set_config\nset_config(display='diagram')","88742291":"pipe.fit(X_train, y_train)","27f095f9":"y_pred = pipe.predict(X_test)","a12771e5":"from sklearn.metrics import accuracy_score\naccuracy = round(accuracy_score(y_test, y_pred),3)*100\nprint(f\"The accuracy of the model is: {accuracy}%\")","15344c75":"accuracy_score(y_test, y_pred)","0c45d1b6":"## Univariate Analysis\nFirst, we will only analyze the independent features. We will try to plot the categorical and the numerical distributions and will try to learn something from them. Once we are done with this, bivariate and multivariate will be easier as we will have more intuition of the data.","57c2c1a7":"### **1.1 Overview**\n* `PassengerId` is the unique id of the row and it doesn't have any effect on target\n* `Survived` is the target variable we are trying to predict (**0** or **1**):\n    - **1 = Survived**\n    - **0 = Not Survived**\n* `Pclass` (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has **3** unique values (**1**,  **2 **or **3**):\n    - **1 = Upper Class**\n    - **2 = Middle Class**\n    - **3 = Lower Class**\n* `Name`, `Sex` and `Age` are self-explanatory\n* `SibSp` is the total number of the passengers' siblings and spouse\n* `Parch` is the total number of the passengers' parents and children\n* `Ticket` is the ticket number of the passenger\n* `Fare` is the passenger fare\n* `Cabin` is the cabin number of the passenger\n* `Embarked` is port of embarkation and it is a categorical feature which has **3** unique values (**C**, **Q** or **S**):\n    - **C = Cherbourg**\n    - **Q = Queenstown**\n    - **S = Southampton**","34818048":"The name Titanic derives from the Titans of Greek mythology. Built in Belfast, Ireland, in the United Kingdom of Great Britain and Ireland, the RMS Titanic was the second of the three Olympic-class ocean liners\u2014the first was the RMS Olympic and the third was the HMHS Britannic. Britannic was originally to be called Gigantic and was to be over 1,000 feet (300 m) long. They were by far the largest vessels of the British shipping company White Star Line's fleet, which comprised 29 steamers and tenders in 1912. The three ships had their genesis in a discussion in mid-1907 between the White Star Line's chairman, J. Bruce Ismay, and the American financier J. P. Morgan, who controlled the White Star Line's parent corporation, the International Mercantile Marine Co.","3deb1c2e":"# Importing Necessary Libraries","4117d177":"# Feature Engineering and Model Building using Pipeline","95f0eae2":"# Train test split","54e6efc2":"# Exploratory Data Analysis","e731ab30":"First, we will need to handle the missing values. We will be using our Machine Learning pipeline.","8da16bf2":"# Prediction and Accuracy Score","ff793045":"# Importing Dataset","22d63036":"## Bivariate and Multivariate Analysis\nNow, as we have a intuition of the independent variable, we will try to learn something new from analysing the relationship between these individual variables and how they are correlated. This will help us in feature engineering and further in selecting an appropriate model for the inference of test data."}}