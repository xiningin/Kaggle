{"cell_type":{"7ecf38cc":"code","da963ceb":"code","9ac60728":"code","ac95c017":"code","07cf3c2e":"code","41c0b72a":"code","18066382":"code","e79d9575":"code","5a26ed42":"code","be85e7fe":"code","bbb7f601":"code","d27490d9":"code","9f2efaeb":"code","a8e1d0f0":"code","4d8dc444":"code","af3b5eb7":"code","87ff6777":"code","91e25d69":"code","cb674c63":"code","0b855674":"code","2044a578":"code","a092b1ef":"code","0a0286db":"code","34e7821c":"code","3f2ad8ee":"code","1f275457":"code","493debf9":"code","6d72be7c":"code","58218203":"code","18b83f74":"code","0e527abb":"code","7bbce42a":"code","697ef760":"code","fefb4b89":"code","c5b7c4c4":"code","2255c905":"code","1fd3bc7f":"code","835ae8da":"code","3a50832e":"code","176727fe":"code","36253214":"code","eebe924a":"code","e101a5ae":"code","62bd145f":"code","23c59f3b":"code","521b1fdb":"code","31bed984":"code","6c1bd98e":"code","a68ab8b6":"code","b40969fe":"code","22ace529":"code","f0e9e11d":"code","7d9576a8":"code","84e83ce7":"code","76bc68cf":"code","2e82cf74":"code","8bf872a6":"code","d51e8bb7":"code","93763b33":"code","e7117405":"code","b93c399a":"code","19908a64":"code","d7dd07da":"code","d7e78b01":"code","28cf1e3c":"code","5d55fdd8":"markdown","5938ab28":"markdown","f36a175c":"markdown","24be39e5":"markdown","d8ccb895":"markdown","8f235c65":"markdown","04a1b781":"markdown","fadcf39e":"markdown","b2a93f47":"markdown","9815683f":"markdown","26ade08e":"markdown","54a03503":"markdown","b4e7c533":"markdown","f12491a1":"markdown","5beeeb66":"markdown","6d9cf713":"markdown","43af3fd6":"markdown","ffc947f2":"markdown","2a7614c2":"markdown","71b7d283":"markdown","4bd9e678":"markdown","cfc52d77":"markdown","49282a18":"markdown","ad9d0375":"markdown","a67f379a":"markdown","dc2796c3":"markdown","45626a7d":"markdown","e8e4d38b":"markdown","55a589ae":"markdown","b48016f0":"markdown","85b6162f":"markdown","64fe16f2":"markdown","d98d9bda":"markdown","27de69d8":"markdown","a77d9547":"markdown"},"source":{"7ecf38cc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style('darkgrid')\nimport os","da963ceb":"df_holi = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv')\ndf_oil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv')\ndf_stores = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ndf_test = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv')\ndf_train = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv')\ndf_transactions = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv')","9ac60728":"# function related basic eda \ndef eda_basic(df):\n    print(\"\\n >> Data <<\\n\\n\")\n    print(df.head())\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> Shape <<\")\n    print(df.shape)\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> Info <<\")\n    print(df.info())\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> Columns <<\")\n    print(df.columns)\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> Description <<\")\n    print(df.describe())\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> Dataypes <<\")\n    print(df.dtypes)\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> Null values <<\")\n    print(df.isnull().sum())\n    print(\"\\n======================================\\n\")\n    print(\"\\n >> N\/A values <<\")\n    print(df.isna().sum())\n    print(\"\\n======================================\\n\")\n#     print(df.value_counts())","ac95c017":"print(\"Basi EDA of holidays_event dataset\\n\")\neda_basic(df_holi)","07cf3c2e":"print(\"Basi EDA of oil dataset\\n\")\neda_basic(df_oil)","41c0b72a":"print(\"Basi EDA of Stores dataset\\n\")\neda_basic(df_stores)","18066382":"print(\"Basi EDA of train dataset\\n\")\neda_basic(df_train)","e79d9575":"print(\"Basi EDA of test dataset\\n\")\neda_basic(df_test)","5a26ed42":"print(\"Basi EDA of transactions dataset\\n\")\neda_basic(df_transactions)","be85e7fe":"def date_form(df):\n    df['date'] = pd.to_datetime(df['date'], format = \"%Y-%m-%d\")\n#     df.head()\n    ","bbb7f601":"# Applying data_from function to dataset\ndate_form(df_holi)\ndate_form(df_oil)\ndate_form(df_train)\ndate_form(df_test)\ndate_form(df_transactions)","d27490d9":"# df_holi.head()\n# df_oil.head()\n# df_train.head()\n# df_test.head()\n# df_transactions.head()","9f2efaeb":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\ndf_oil.plot.line(x=\"date\", y=\"dcoilwtico\", color=\"b\", ax=axes, rot=0)\nplt.title(\"Dependency of the oil from the data\")\nplt.show()","a8e1d0f0":"def grouped(df,key,freq,col):\n    df_grouped = df.groupby([pd.Grouper(key=key, freq=freq)]).agg(mean = (col, 'mean'))\n    df_grouped = df_grouped.reset_index()\n    return df_grouped","4d8dc444":"df_grouped_trans_w = grouped(df_transactions, 'date', 'w', 'transactions')\ndf_grouped_trans_w","af3b5eb7":"def add_time(df, key, freq, col):\n    df_grouped = grouped(df, key,freq, col)\n    df_grouped['time'] = np.arange(len(df_grouped.index))\n    column_time = df_grouped.pop('time')\n    df_grouped.insert(1, 'time', column_time)\n    return df_grouped","87ff6777":"df_grouped_train_w = add_time(df_train, 'date', 'W', 'sales')\ndf_grouped_train_m = add_time(df_train, 'date', 'M', 'sales')","91e25d69":"df_grouped_train_w.head()","cb674c63":"df_grouped_train_m.head()","0b855674":"fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(30,20))\n\n# Transactions(weekly)\naxes[0].plot('date', 'mean', data=df_grouped_train_w, color='grey', marker='o')\naxes[0].set_title(\"Transactions (grouped by week)\", fontsize=20)\n\n# Sales (weekly)\naxes[1].plot('time', 'mean', data=df_grouped_train_w, color='0.75')\naxes[1].set_title('Sales (grouped by week)', fontsize=20)\n\n# Linear regression\naxes[1] = sns.regplot(x='time',\n                     y='mean',\n                     data = df_grouped_train_w,\n                     scatter_kws = dict(color='0.75'),\n                     ax = axes[1])\n\n# Sales (Monthly)\naxes[2].plot('time', 'mean', data=df_grouped_train_m, color='0.75')\naxes[2].set_title('Sales [grouped by Month]', fontsize=20)\n\n# Linear Regression\naxes[2] = sns.regplot(x='time',\n                     y = 'mean',\n                     data = df_grouped_train_m,\n                     scatter_kws = dict(color='0.75'),\n                     line_kws={\"color\": \"red\"},\n                     ax = axes[2])\n\nplt.show()","2044a578":"def add_lag(df, key, freq, col, lag):\n    df_grouped = grouped(df, key, freq, col)\n    name = 'Lag_' + str(lag)\n    df_grouped['Lag'] = df_grouped['mean'].shift(lag)\n    return df_grouped","a092b1ef":"df_grouped_train_w_lag1 = add_lag(df_train, 'date', 'W', 'sales',1)\ndf_grouped_train_m_lag1= add_lag(df_train, 'date', 'W', 'sales',1)\n\ndf_grouped_train_w_lag1.head()","0a0286db":"fig,axes = plt.subplots(nrows = 2, ncols=1, figsize=(30,20))\naxes[0].plot('Lag', 'mean', data=df_grouped_train_w_lag1,color=\"0.75\",linestyle=(0,(1,10)))\naxes[0].set_title('Sales (grouped by week)', fontsize=20)\naxes[0] = sns.regplot(x='Lag',\n                     y='mean',\n                     data = df_grouped_train_w_lag1,\n                     scatter_kws= dict(color='0.75'),\n                     ax = axes[0])\n\naxes[1].plot('Lag', 'mean', data=df_grouped_train_m_lag1, color=\"0.75\",linestyle=(0,(1,10)))\naxes[1].set_title(\"Sales (groupes by month)\", fontsize=20)\naxes[1] = sns.regplot(x='Lag',\n                     y='mean',\n                     data = df_grouped_train_m_lag1,\n                     scatter_kws = dict(color='0.75'),\n                     line_kws={'color':'red'},\n                     ax = axes[1])\n\nplt.show()","34e7821c":"def plot_stats(df, column, ax,color,angle):\n    count_classes = df[column].value_counts()\n    ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)\n    ax.set_title(column.upper(), fontsize=20)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(angle)","3f2ad8ee":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\nfig.autofmt_xdate()\nfig.suptitle(\"Stats of df_holidays\".upper())\nplot_stats(df_holi, \"type\", axes[0], \"pastel\", 45)\nplot_stats(df_holi, \"locale\", axes[1], \"rocket\", 45)\nplt.show()","1f275457":"fig, axes = plt.subplots(nrows = 4, ncols=1, figsize=(20,40))\nplot_stats(df_stores, \"city\", axes[0], \"mako_r\", 45)\nplot_stats(df_stores, \"state\", axes[1], \"rocket_r\", 45)\nplot_stats(df_stores, \"type\", axes[2], \"magma\", 0)\nplot_stats(df_stores, \"cluster\", axes[3], \"viridis\", 0)","493debf9":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\ncount_classes = df_train['family'].value_counts()\nplt.title(\"Stats of df_train\".upper())\ncolors = ['#ff9999','#66b3ff','#99ff99',\n          '#ffcc99', '#ffccf9', '#ff99f8', \n          '#ff99af', '#ffe299', '#a8ff99',\n          '#cc99ff', '#9e99ff', '#99c9ff',\n          '#99f5ff', '#99ffe4', '#99ffaf']\n\nplt.pie(count_classes, \n        labels = count_classes.index, \n        autopct='%1.1f%%',\n        shadow=True, \n        startangle=90, \n        colors=colors)\n\nplt.show()","6d72be7c":"df_train[\"family\"].nunique(dropna=True)","58218203":"df_test.head()","18b83f74":"# dropping the onpromotion coz it won't be used\n\ntrain_data = df_train.copy().drop(['onpromotion'], axis=1)\ntest_data = df_test.copy().drop(['onpromotion'], axis=1)","0e527abb":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler","7bbce42a":"ordinal_encoder = OrdinalEncoder(dtype=int)\ntrain_data[['family']] = ordinal_encoder.fit_transform(train_data[['family']])\ntest_data[['family']] = ordinal_encoder.transform(test_data[['family']])","697ef760":"train_data","fefb4b89":"#counting number of days\nn_o_days_train=train_data[\"date\"].nunique(dropna = False) \nprint('number of day train:',n_o_days_train)\n\n# number of store\nn_o_stores_train=train_data[\"store_nbr\"].nunique(dropna = False) \nprint('number of stores train:',n_o_stores_train)\n\n# number of family\nn_o_families_train=train_data[\"family\"].nunique(dropna = False) \nprint('number of family\/type of prod train:',n_o_families_train)","c5b7c4c4":"##counting the number of days\nn_o_days_test=test_data[\"date\"].nunique(dropna = False) \nprint('number of day test:',n_o_days_test)\n\n# number of store\nn_o_stores_test=test_data[\"store_nbr\"].nunique(dropna = False) \nprint('number of stores test:',n_o_stores_test)\n\n# number of family\nn_o_families_test=test_data[\"family\"].nunique(dropna = False) \nprint('number of family\/type of prod test:',n_o_families_test)","2255c905":"pivoted_train = train_data.pivot(index=['date'], columns=['store_nbr', 'family'], values='sales')\npivoted_train.head()","1fd3bc7f":"pivoted_train[1][0]","835ae8da":"train_samples = int(n_o_days_train*0.95)\ntrain_samples","3a50832e":"train_samples_df = pivoted_train[:train_samples]\ntrain_samples_df","176727fe":"valid_samples_df = pivoted_train[train_samples:]\nvalid_samples_df","36253214":"minmax = MinMaxScaler()\nminmax.fit(train_samples_df)\n\nscaled_train_samples = minmax.transform(train_samples_df)\nscaled_val_samples = minmax.transform(valid_samples_df)","eebe924a":"scaled_train_samples[10:]","e101a5ae":"scaled_val_samples[10:]\n","62bd145f":"# n_past --> no. of past observations\n# n_future --> no.of past observations\n\ndef split_series(series, n_past, n_future):\n    X, y = list(), list()\n    for window_start in range(len(series)):\n        past_end = window_start + n_past\n        future_end = past_end + n_future\n        if future_end > len(series):\n            break\n            \n        # slicing past and future\n        past, future = series[window_start:past_end,:], series[past_end:future_end,:]\n        X.append(past)\n        y.append(future)\n    \n    return np.array(X), np.array(y)\n\nn_past =16\nn_future = 16\nn_features = n_o_stores_train * n_o_families_train # num of features","23c59f3b":"X_train, y_train = split_series(scaled_train_samples, n_past, n_future)\nX_val, y_val = split_series(scaled_val_samples, n_past, n_future)","521b1fdb":"print('X_train.shape',X_train.shape)\nprint('y_train.shape',y_train.shape)\nprint('X_val.shape',X_val.shape)\nprint('y_val.shape',y_val.shape)","31bed984":"from tensorflow.keras.layers import LSTM, Dense, Embedding\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, TimeDistributed\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","6c1bd98e":"model = Sequential()\n\nmodel.add(LSTM(units=256, return_sequences=True,input_shape=[n_past, n_features]))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=128, return_sequences=True))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n#TimeDistributed layer\nmodel.add(TimeDistributed(Dense(n_features)))\n\nmodel.compile(loss=\"mae\", optimizer=Adam(learning_rate=0.001), metrics=['mae'])","a68ab8b6":"model.summary()","b40969fe":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_mae', \n                           min_delta=0.0001,\n                           patience=100,\n                           restore_best_weights=True)\n\nepochs= 1000\n\nmodel_history = model.fit(X_train, y_train, \n                          validation_data=(X_val, y_val),\n                          epochs = epochs,\n                          callbacks = [early_stop],\n                          batch_size=512,\n                          shuffle=True)","22ace529":"plt.plot(model.history.history['loss'])\nplt.plot(model.history.history['val_mae'])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(['Train', 'Validation'])\nplt.show()","f0e9e11d":"X_test_pred = scaled_val_samples[-n_past:,:].reshape((1, n_past, n_features))\nprint(X_test_pred.shape)\nscaled_test_predict = model.predict(X_test_pred)","7d9576a8":"scaled_test_predict.shape","84e83ce7":"X_train_pred = scaled_train_samples[-n_past:,:].reshape((1, n_past, n_features))\nprint(X_train_pred.shape)\nscaled_train_predict = model.predict(X_test_pred)","76bc68cf":"scaled_train_predict.shape","2e82cf74":"# Inverse transform from the previous min max scaler\ny_predict = pd.DataFrame(minmax.inverse_transform(scaled_test_predict.reshape((n_future, n_features))),columns=valid_samples_df.columns)","8bf872a6":"y_predict","d51e8bb7":"pivoted_test = test_data.pivot(index=['date'], columns=['store_nbr', 'family'], values=None)\npivoted_test","93763b33":"pivoted_test.values","e7117405":"pivoted_train.values","b93c399a":"submission = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/sample_submission.csv')","19908a64":"# submission","d7dd07da":"## mapping ypredict to pivoted test data\nfor day_ith, day_ith_pred in y_predict.iterrows():\n    #day_ith iteration, 16 days in totals\n    #day_ith_pred, predicted data of 9 stores, 33 classes of good for each day\n    #Iterate over DataFrame rows as (index, Series) pairs.\n#     print(n_samples_per_day)\n    # n_samples_per_day number of \n    for n_samples_per_day in range(len(day_ith_pred)): ## iterating the number of sample, from 0 to 1781, for 16 days\n#         print(pivoted_test.iloc[[day_ith], [n_samples_per_day]])\n        sample_id = pivoted_test.iloc[[day_ith], [n_samples_per_day]].values[0][0] #total number of samples\n        values= max(0,day_ith_pred.values[n_samples_per_day]) #price that is negative will be set to 0\n        submission.at[sample_id, 'sales'] = values","d7e78b01":"submission","28cf1e3c":"submission.to_csv('submission.csv')","5d55fdd8":"sliding window for converting series to sample to be used with supervised learning algorithm","5938ab28":"So, now we can check the results of grouping on the example of **df_train (grouped by weeks on sales, after that, mean was counted).**","f36a175c":"## Splitting the data into train and validation","24be39e5":"# Importing Essential Libraries","d8ccb895":"There are different types of files in this task, so instead of playing with each dataset let's create a function which will tell us all required characteristics of the particular data(file).\n\nCharacteristcs are\n- Data (first 5 records)\n- Shape of the data\n- Essential information \n- Columns in the data\n- Desciption (Statistical)\n- Datatypes of columns\n- Presence of null values\n- N\/A values form the dataset","8f235c65":"The data need to be re-organized as discrete-time data (days)\n date as timestamp\/time-series input, store number and family as columns and sales is the numerical data of interest for RNN","04a1b781":"So lag features let us fit curves to lag plots where each observation in a series is plotted against the previous observation. Let's build same plots, but with 'lag' feature:","fadcf39e":"Exploring and visualizing the data int statistical aspect","b2a93f47":"### This notebook includes Data cleaning, exploratory data analysis,data visualization among different factors of the data.\n### Main technique I used is Recurrent Neural Networks which is LSTM.","9815683f":"## Thank You!","26ade08e":"### Scaling the data","54a03503":"# Submitting resulting csv file for Kaggle competition","b4e7c533":"# Store Sales - Time series Forecasting\ud83d\udcc8\ud83d\udcc9","f12491a1":"# Data Preprocessing","5beeeb66":"# Exploratory Data Analysis (Analyzing and cleaning)","6d9cf713":"**Date** is very important factor in the dataset","43af3fd6":"Let'focus on the **family** factor","ffc947f2":"Grouped data on transactions dataset","2a7614c2":"Plots based on **Linear Regression**","71b7d283":"Except **Stores** dataset each data cotains **Date** column.","4bd9e678":"### Applying this function to all datas which contains a **Date** column","cfc52d77":"Now converting the data via split_series function","49282a18":"# Traning the model - LSTM","ad9d0375":"count values of some columns of df_stores","a67f379a":"As we have so much rows in out dataset, it will be easier to group data, as example, by week or month. The aggregation will be made by **mean**","dc2796c3":"Let's check store number 1 and product number 0","45626a7d":"## Lag feature\n\nLag features are values at prior timesteps that are considered useful because they are created on the assumption that what happened in the past can influence or contain a sort of intrinsic information about the future. For example, it can be beneficial to generate features for sales that happened in previous days at 4:00 p.m. if you want to predict similar sales at 4:00 p.m. the next day.","e8e4d38b":"# Load the dataset","55a589ae":"Here we can look through some variables and see some dependencies. Firstly, let's check the **dependency of the oil from the date**","b48016f0":"#### Encoding the family feature","85b6162f":"# Visualization ","64fe16f2":"# Forecasting the model","d98d9bda":"And, for better forecasting we'll add **time** column to our dataframe.","27de69d8":"Let's **plot pie** chart for **'family'** of **df_train**","a77d9547":"From above graph we can say that model trained well!"}}