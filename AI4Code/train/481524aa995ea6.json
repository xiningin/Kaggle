{"cell_type":{"8b93bb50":"code","c99b06b7":"code","18269cf0":"code","b900f47f":"code","d8b95ca3":"code","2dda4c06":"code","63e2215e":"code","58f8fb82":"code","b4bd1788":"code","f548cdde":"code","3d34a214":"code","a02902a9":"code","397e4449":"code","595e03c2":"code","95ed6047":"code","c22f9aa4":"code","156eaa60":"code","0ed1755c":"code","2b1df84f":"code","e83ce1c3":"code","368d8eb4":"markdown","4b5f61a8":"markdown","002f5b93":"markdown","939f69fb":"markdown","c7e374e7":"markdown","c579f6cd":"markdown","eabc42ed":"markdown"},"source":{"8b93bb50":"import numpy as np\nimport re","c99b06b7":"def tokenize(text):\n    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n    return pattern.findall(text.lower())","18269cf0":"tokens = tokenize(\"Hello from the other side\")\nprint(tokens)","b900f47f":"id_to_word = {i:x for (i, x) in enumerate(tokens)}\nword_to_id = {x:i for (i, x) in enumerate(tokens)}","d8b95ca3":"print(word_to_id)\nprint(id_to_word)","2dda4c06":"def generate_training_data(tokens, word_to_id, window_size):\n    X, Y = [], []\n\n    for i in range(len(tokens)):\n        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n                   list(range(i + 1, min(len(tokens), i + window_size + 1)))\n        for j in nbr_inds:\n            X.append(word_to_id[tokens[i]])\n            Y.append(word_to_id[tokens[j]])\n            \n    return np.array(X), np.array(Y)","63e2215e":"def expand_dims(x, y):\n    x = np.expand_dims(x, axis=0)\n    y = np.expand_dims(y, axis=0)\n    return x, y","58f8fb82":"x, y = generate_training_data(tokens, word_to_id, 3)\nx, y = expand_dims(x, y)","b4bd1788":"# generated training data\nx, y","f548cdde":"def init_parameters(vocab_size, emb_size):\n    wrd_emb = np.random.randn(vocab_size, emb_size) * 0.01\n    w = np.random.randn(vocab_size, emb_size) * 0.01\n    \n    return wrd_emb, w","3d34a214":"def softmax(z):\n    return np.divide(np.exp(z), np.sum(np.exp(z), axis=0, keepdims=True) + 0.001)","a02902a9":"def forward(inds, params):\n    wrd_emb, w = params\n    word_vec = wrd_emb[inds.flatten(), :].T\n    z = np.dot(w, word_vec)\n    out = softmax(z)\n    \n    cache = inds, word_vec, w, z\n    \n    return out, cache","397e4449":"def cross_entropy(y, y_hat):\n    m = y.shape[1]\n    cost = -(1 \/ m) * np.sum(np.sum(y_hat * np.log(y + 0.001), axis=0, keepdims=True), axis=1)\n    return cost","595e03c2":"def dsoftmax(y, out):\n    dl_dz = out - y\n    \n    return dl_dz","95ed6047":"def backward(y, out, cache):\n    inds, word_vec, w, z = cache\n    wrd_emb, w = params\n    \n    dl_dz = dsoftmax(y, out)\n    # deviding by the word_vec length to find the average\n    dl_dw = (1\/word_vec.shape[1]) * np.dot(dl_dz, word_vec.T)\n    dl_dword_vec = np.dot(w.T, dl_dz)\n    \n    return dl_dz, dl_dw, dl_dword_vec","c22f9aa4":"def update(params, cache, grads, lr=0.03):\n    inds, word_vec, w, z = cache\n    wrd_emb, w = params\n    dl_dz, dl_dw, dl_dword_vec = grads\n    \n    wrd_emb[inds.flatten(), :] -= dl_dword_vec.T * lr\n    w -= dl_dw * lr\n    \n    return wrd_emb, w","156eaa60":"vocab_size = len(id_to_word)\n\nm = y.shape[1]\ny_one_hot = np.zeros((vocab_size, m))\ny_one_hot[y.flatten(), np.arange(m)] = 1\n\ny = y_one_hot","0ed1755c":"batch_size=256\nembed_size = 50\n\nparams = init_parameters(vocab_size, 50)\n\ncosts = []\n\nfor epoch in range(5000):\n    epoch_cost = 0\n    \n    batch_inds = list(range(0, x.shape[1], batch_size))\n    np.random.shuffle(batch_inds)\n    \n    for i in batch_inds:\n        x_batch = x[:, i:i+batch_size]\n        y_batch = y[:, i:i+batch_size]\n        \n        pred, cache = forward(x_batch, params)\n        grads = backward(y_batch, pred, cache)\n        params = update(params, cache, grads, 0.03)\n        cost = cross_entropy(pred, y_batch)\n        \n        epoch_cost += np.squeeze(cost)\n        \n    costs.append(epoch_cost)\n    \n    if(epoch % 250 == 0):\n        print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))","2b1df84f":"x_test = np.arange(vocab_size)\nx_test = np.expand_dims(x_test, axis=0)\nsoftmax_test, _ = forward(x_test, params)\ntop_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]","e83ce1c3":"for input_ind in range(vocab_size):\n    input_word = id_to_word[input_ind]\n    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n    print(\"{}'s skip-grams: {}\".format(input_word, output_words))","368d8eb4":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Generate Data<\/center><\/h3>","4b5f61a8":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Forward Propagation<\/center><\/h3>","002f5b93":"![skip.png](attachment:skip.png)","939f69fb":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Backward Propagation<\/center><\/h3>","c7e374e7":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Training<\/center><\/h3>","c579f6cd":"### Cost Function","eabc42ed":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Test<\/center><\/h3>"}}