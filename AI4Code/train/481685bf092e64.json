{"cell_type":{"1f36ca16":"code","9eedfe84":"code","f449e1cc":"code","4a338002":"code","c8f2f358":"code","2f43a953":"code","b634d151":"code","d053e4be":"code","242d9df0":"code","dddec6f3":"code","02d80912":"code","fa95a08b":"code","033c3bf8":"code","b34df84b":"code","46bd0e2e":"code","9a47adc5":"code","6818975f":"code","6232f8d5":"code","0fe62ab1":"code","ef2982be":"code","ca3c3eba":"code","54cf10d1":"code","b787bda3":"code","3b8a023a":"code","e82ca861":"code","e4cab5fd":"code","bb1a9a54":"code","aa0753e4":"code","26144b43":"code","3e8305c6":"code","4e5a4755":"code","de01e564":"code","a13c487d":"code","de5594eb":"code","79e261f0":"code","392be2ae":"code","4168985d":"code","0008c524":"code","4013d999":"code","3cc475e4":"code","302225da":"code","528c4fae":"code","ebfeee98":"code","6c967fcb":"code","6ed09805":"code","8df15e99":"code","ac20b2c3":"code","98d8c7b2":"code","79026da6":"code","39117ce3":"code","5911f9d1":"code","f4f6edf1":"code","3a1cce0e":"code","c1dbafbd":"code","06cb81ea":"markdown","27ba8859":"markdown","a995bd8d":"markdown","ae32c4c1":"markdown","d9d74287":"markdown","3d2d97ab":"markdown","c3117bdd":"markdown","a07efcc7":"markdown","ba1d3bd8":"markdown","8d07587d":"markdown","8b4b70f2":"markdown","7b9cf4c7":"markdown","31a88aa9":"markdown"},"source":{"1f36ca16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9eedfe84":"import pandas as pd\nimport numpy as np\nfrom time import time\n\nimport matplotlib.pyplot as plt","f449e1cc":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","4a338002":"train = train.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\ntrain.head()\ntrain.shape","c8f2f358":"for i in train.columns:\n    if 'SalePrice' not in i:\n        if 'object' in str(train[str(i)].dtype):\n            train[str(i)] = train[str(i)].fillna(train[str(i)].mode().index[0])\n            \nfor i in train.columns:\n    if 'SalePrice' not in i:\n        if 'object' not in str(train[str(i)].dtype):\n            train[str(i)] = train[str(i)].fillna(train[str(i)].mean())\n            \ntrain.head()","2f43a953":"# multiple column label encoding\nfor i in train.columns:\n    if 'SalePrice' not in i:\n        if 'object' in str(train[str(i)].dtype):\n            train[str(i)]=train[str(i)].astype('category').cat.codes\n\ntrain.head()","b634d151":"out=[179,441,692,770,804,899,1047,1170,1183]\nfor i in out:\n    train.drop(train.index[[i]],inplace=True)","d053e4be":"train['SalePrice'] = np.log1p(train['SalePrice'])","242d9df0":"train.info()","dddec6f3":"x = train.drop(\"SalePrice\", axis = 1)\ny = train[\"SalePrice\"]","02d80912":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.8 , random_state = 27)","fa95a08b":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators= 500,\n min_samples_split= 2,\n min_samples_leaf= 1,\n max_features = 'sqrt',\n max_depth = 15\n)","033c3bf8":"rf.fit(x_train, y_train)\npred_rf = rf.predict(x_test)","b34df84b":"import seaborn as sns\nsns.distplot(y_test-pred_rf)","46bd0e2e":"import xgboost\nxgb = xgboost.XGBRegressor(subsample = 0.6, n_estimators = 1100, min_child_weight = 3, max_depth = 5, learning_rate =  0.005)\nxgb.fit(x_train, y_train)\npred_xgb = xgb.predict(x_test)","9a47adc5":"sns.distplot(y_test-pred_xgb)","6818975f":"import lightgbm\nlgbm = lightgbm.LGBMRegressor(bagging_fraction=0.7, bagging_freq=10, boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,\n       importance_type='split', learning_rate=0.005, max_bin=512,\n       max_depth=8, metric=['l2', 'auc'], min_child_samples=20,\n       min_child_weight=0.001, min_split_gain=0.0, n_estimators=1000,\n       n_jobs=-1, num_iterations=100000, num_leaves=128,\n       objective='regression', random_state=None, reg_alpha=0.0,\n       reg_lambda=0.0, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=0, task='train', verbose=0)\nlgbm.fit(x_train, y_train)\npred_lgbm = lgbm.predict(x_test)","6232f8d5":"sns.distplot(y_test-pred_lgbm)","0fe62ab1":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nscore = cross_val_score(rf, x,y ,  cv = 10)","ef2982be":"print(score)","ca3c3eba":"score.mean()","54cf10d1":"#from sklearn.model_selection import KFold, cross_val_score, train_test_split\nscore1 = cross_val_score(xgb, x,y ,  cv = 10)","b787bda3":"print(score1)","3b8a023a":"score1.mean()","e82ca861":"lgbm = lightgbm.LGBMRegressor(bagging_fraction=0.7, bagging_freq=10, boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,\n       importance_type='split', learning_rate=0.005, max_bin=512,\n       max_depth=8, metric=['l2', 'auc'], min_child_samples=20,\n       min_child_weight=0.001, min_split_gain=0.0, n_estimators=1000,\n       n_jobs=-1, num_iterations=100000, num_leaves=128,\n       objective='regression', random_state=None, reg_alpha=0.0,\n       reg_lambda=0.0, silent=True, subsample=1.0,\n       subsample_for_bin=200000, subsample_freq=0, task='train', verbose=0)\nlgbm.fit(x_train, y_train)","e4cab5fd":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nscore2 = cross_val_score(lgbm, x,y ,  cv = 10)","bb1a9a54":"print(score2)","aa0753e4":"score2.mean()","26144b43":"#from sklearn.model_selection import RandomizedSearchCV\n #Randomized Search CV\n\n# Number of trees in random forest\n#n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Various learning rate parameters\n#learning_rate = ['0.005','0.01', '0.05','0.1','0.2','0.3', '0.4', '0.5']\n#num_leaves = [int(x) for x in np.linspace(start = 10, stop = 100, num = 2)]\n# Maximum umber of levels in tree\n#max_depth = [int(x) for x in np.linspace(20, 60, num = 6)]\n# max_depth.append(None)\n#Subssample parameter values\n#subsample=[0.7,0.6,0.8,1,1.2,1.3,1.4,1.5]\n# Minimum child weight parameters\n#min_child_weight=[int(x) for x in np.linspace(0.00001, 10, num = 10)]\n","3e8305c6":"#random_grid = {'n_estimators': n_estimators,\n#               'learning_rate': learning_rate,\n#               #'max_depth': max_depth,\n#               'num_leaves': num_leaves,\n#               'subsample': subsample,\n#               'min_child_weight': min_child_weight}\n#\n#print(random_grid)","4e5a4755":"#lgbm_random = RandomizedSearchCV(estimator = lgbm, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = 1)","de01e564":"#lgbm_random.fit(x_train, y_train)","a13c487d":"#lgbm_random.best_params_","de5594eb":"#lgbm_random.best_score_","79e261f0":"#pred_lgbm_random = lgbm_random.predict(x_test)","392be2ae":"#score4 = cross_val_score(lgbm_random, x,y ,  cv = 10)","4168985d":"target = pd.DataFrame(columns=['actual', 'rf_pred', 'xgb_pred', 'lgb_pred'])","0008c524":"target['actual']=y_test\ntarget['rf_pred']=pred_rf\ntarget['xgb_pred']=pred_xgb\ntarget['lgb_pred']=pred_lgbm","4013d999":"target","3cc475e4":"target['ensemble'] = 0.7*target.iloc[:,3] + 0.15*(target.iloc[:,1]+target.iloc[:,2])\ntarget","302225da":"from sklearn import metrics\nfor i in target.columns[1:]:\n    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, target[i])))","528c4fae":"target = np.expm1(target.iloc[:,:])\ntarget.head()","ebfeee98":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","6c967fcb":"test = test.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\ntest.head()","6ed09805":"for i in test.columns:\n    if 'object' in str(test[str(i)].dtype):\n        test[str(i)] = test[str(i)].fillna(test[str(i)].mode().index[0])\n            \nfor i in test.columns:\n    if 'object' not in str(test[str(i)].dtype):\n        test[str(i)] = test[str(i)].fillna(test[str(i)].mean())\n            \ntest.head()","8df15e99":"# multiple column label encoding\nfor i in test.columns:\n    if 'object' in str(test[str(i)].dtype):\n        test[str(i)]=test[str(i)].astype('category').cat.codes\n\ntest.head()","ac20b2c3":"pred1 = rf.predict(test)\npred2 = xgb.predict(test)\npred3 = lgbm.predict(test)","98d8c7b2":"final = 0.7*pred3 + 0.15*(pred1+pred2)","79026da6":"final","39117ce3":"final = np.expm1(final)","5911f9d1":"final","f4f6edf1":"sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsub.head()","3a1cce0e":"sub['SalePrice'] = final\nsub","c1dbafbd":"sub.to_csv('submission.csv', index=False)","06cb81ea":"# Ensemble all models (Bagging)","27ba8859":"# LGBM Model","a995bd8d":"## XGB CV","ae32c4c1":"#Split","d9d74287":"## LGBM CV","3d2d97ab":"## LGBM HYPER CV","c3117bdd":"# Test Data","a07efcc7":"# CV with K fold","ba1d3bd8":"# Submission","8d07587d":"# XGB Model","8b4b70f2":"# RF Model","7b9cf4c7":"## LGBM Hyperparameter","31a88aa9":"## RF CV"}}