{"cell_type":{"24521fe3":"code","6d028304":"code","a347870a":"code","7dff3319":"code","5ae27940":"code","ee929386":"code","62c0752f":"code","d91002b2":"code","785a2bd4":"code","e25409ec":"code","dbd1053d":"code","890b2fa4":"code","e7bfb06b":"code","0af08da7":"code","8c77b44e":"code","0cc52d54":"markdown","33df21a7":"markdown","4026ccc6":"markdown","90271ac9":"markdown","c275be9b":"markdown","7c5d689d":"markdown","e98829af":"markdown"},"source":{"24521fe3":"import os\nimport time\nimport math\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoConfig\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import DistilBertTokenizer","6d028304":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","a347870a":"BASE_DATA_PATH = Path(\"..\/input\/commonlitreadabilityprize\/\")\n\n!ls {BASE_DATA_PATH}","7dff3319":"df_test = pd.read_csv(BASE_DATA_PATH \/ \"test.csv\")","5ae27940":"df_test.head(3)","ee929386":"class CommonLitDataset(Dataset):\n    \n    def __init__(self, df, tokenizer, max_length):\n    \n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        tokenized_input = self.tokenizer(row.excerpt, return_tensors=\"pt\", \n                                         max_length=self.max_length, \n                                         padding=\"max_length\", truncation=True)\n        return {\n            \"ids\": tokenized_input[\"input_ids\"][0],\n            \"masks\": tokenized_input[\"attention_mask\"][0],\n        }","62c0752f":"class TextRegressionModel(nn.Module):\n    \n    def __init__(self, model_name, dropout_p=0.1):\n        super(TextRegressionModel, self).__init__()\n        \n        self.model = AutoModel.from_pretrained(CFG.model_path) # AutoModel.from_config(AutoConfig.from_pretrained(\"config.json\"))\n        self.features = nn.Linear(768, 768)\n        self.dropout = nn.Dropout(dropout_p)\n        self.out = nn.Linear(768, 1)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        output = F.relu(self.features(output.last_hidden_state[:, 0]))\n        output = self.dropout(output)\n        output = self.out(output)\n        return output","d91002b2":"class CFG:\n    model_name = \"distilbert-base-cased\"\n    # Default model (from HuggingFace) path\n    model_path = \"..\/input\/pt-distilbert-base-cased\/distilbert-base-cased\/\"\n    max_length = 256\n    dropout_p = 0.3\n    batch_size = 16\n    n_epochs = 10\n    weight_decay = 1e-6\n    lr = 3e-4\n    min_lr = 1e-6\n    scheduler = \"CosineAnnealingLR\"\n    T_max = 10\n    seed = 42\n    n_folds = 5    \n    print_freq = 500\n    # num_workers = 4\n","785a2bd4":"def inference(model, states, data_loader, device=device):\n\n    results = []\n    with torch.no_grad():\n        for state in states:\n            outputs = []\n            model.load_state_dict(state)\n            model.to(device)\n            model.eval()\n            for step, batch in enumerate(data_loader):\n                input_ids = batch[\"ids\"].to(device)\n                attention_masks = batch[\"masks\"].to(device)\n                output = model(input_ids, attention_masks)\n                outputs.append(output.detach().cpu().numpy())\n        \n            results.append(np.stack(outputs)[0])\n        \n    return np.array(results)","e25409ec":"tokenizer = DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbasecased\/distilbert-base-cased_tokenizer\/\")\ndataset = CommonLitDataset(df_test, tokenizer, CFG.max_length)\ndata_loader = DataLoader(dataset, batch_size=CFG.batch_size, \n                         shuffle=False)","dbd1053d":"states = [torch.load(f\"..\/input\/distilbertbasecased\/distilbert-base-cased_fold_{fold_idx}_best.pth\")[\"model\"] for fold_idx in range(5)]","890b2fa4":"model = TextRegressionModel(CFG.model_name, CFG.dropout_p)\n# model.load_state_dict()\n# model.to(device)","e7bfb06b":"outputs = inference(model, states, data_loader, device)","0af08da7":"df_sub = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ndf_sub[\"target\"] = outputs.mean(0)\ndf_sub.to_csv(\"submission.csv\",index=False)","8c77b44e":"df_sub","0cc52d54":"# Config","33df21a7":"# Dataset","4026ccc6":"# About this notebook\n\n* Pytorch DistilBert inference code.\n* Training notebook is [here](https:\/\/www.kaggle.com\/snnclsr\/commonlit-readability-training\/).\n\nIf this notebook is helpful, feel free to upvote :)\n\n**Some of the parts of this notebook taken from [Y.Nakama](https:\/\/www.kaggle.com\/yasufuminakama)'s notebooks. Please also check his notebooks as well from [here](https:\/\/www.kaggle.com\/yasufuminakama\/code)**","90271ac9":"# Model","c275be9b":"# Data Loading","7c5d689d":"# Inference","e98829af":"# Imports"}}