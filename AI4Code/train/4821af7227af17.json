{"cell_type":{"e5c07050":"code","806e0483":"code","135ea275":"code","dd3831ce":"code","aa9f3f32":"code","1288f113":"code","455d5695":"code","436640da":"code","636312c0":"code","a69342e8":"code","bce78e3c":"code","d22f4fb7":"code","dab1e9ef":"code","ef8ff16f":"code","f34d79d9":"code","53bd34fe":"code","f209a52e":"code","95788f3c":"code","1ba6144e":"code","9ac8063f":"code","66d4dd6f":"code","0c24ffaf":"code","484e26b8":"code","65210ee7":"code","9a6befd8":"code","49141a37":"code","f6b93748":"code","6ea16068":"code","8f865caa":"code","47278217":"code","9bf0891b":"code","edf5705b":"code","a4adf767":"markdown","a354976c":"markdown","1d0bc296":"markdown","4c8da944":"markdown","0d48cdd0":"markdown","291e6a18":"markdown","8f851973":"markdown","f2d35794":"markdown","51112c85":"markdown","5d8f90b3":"markdown","7e57687f":"markdown","a6c20e1a":"markdown","d52d1817":"markdown","1db89db5":"markdown","f7c08083":"markdown","7facd715":"markdown","4584562d":"markdown","98f3e762":"markdown","bea465de":"markdown","2ba4fde5":"markdown","f71670c3":"markdown","d2252751":"markdown","42fb3a88":"markdown","a68a0181":"markdown","128def0b":"markdown","20ff8cd8":"markdown","e9d0498d":"markdown","06fd39db":"markdown","8ecb9613":"markdown","fa03fa01":"markdown","94f3ddbf":"markdown","985d43c3":"markdown","eaa4d272":"markdown","7df84524":"markdown","a7f04736":"markdown"},"source":{"e5c07050":"# Importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#models\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()\n\n# ----------------------- Helper functions -------------------\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef eval_model(model, X_train, y_train, X_test, y_test):\n    _ = model.fit(X_train, y_train)\n    print(\"Train rmse : \", rmse(y_train, model.predict(X_train)))\n    print(\"Test rmse : \", rmse(y_test, model.predict(X_test)))","806e0483":"data = pd.read_csv('\/kaggle\/input\/abalone-dataset\/abalone.csv')\n\n# add target(age) to dataset [rings + 1.5 = age ]\ndata['age'] = data.Rings + 1.5\n\n # remove rings variable\ndata.drop('Rings', axis=1, inplace=True)\n\nprint(\"Data loaded Successfully!\")","135ea275":"data.sample(5)","dd3831ce":"print(\"No. of rows : \", data.shape[0])\nprint(\"No. of columns : \", data.shape[1])","aa9f3f32":"data.describe()","1288f113":"data.isna().mean().to_frame(name='% of missing values') # No missing values","455d5695":"data.nunique().to_frame(\"# of unique values\")","436640da":"# train test split\n\ntrain, test = train_test_split(data, test_size=0.25, random_state=1)\nprint('Train data points :', len(train))\nprint('Test data points :', len(test))","636312c0":"numerical_features = [\"Length\", 'Diameter', 'Height','Whole weight',\n                      'Shucked weight', 'Viscera weight', 'Shell weight']\n\ncategorical_feature = \"Sex\"\n\nfeatures = numerical_features + [categorical_feature]\n\ntarget = 'age'","a69342e8":"fig, axes = plt.subplots(ncols=2,figsize=(16, 5))\n\ntrain[target].plot.hist(color='blue', ax=axes[0])\naxes[0].set(title=\"Train\")\n\ntest[target].plot.hist(color='blue', ax=axes[1])\naxes[1].set(title=\"Test\")\n\nplt.tight_layout()\nplt.show()","bce78e3c":"fig, axes = plt.subplots(4,2,figsize=(16, 14))\naxes = np.ravel(axes)\n\nfor i, c in enumerate(numerical_features):\n    hist = train[c].plot(kind = 'hist', ax=axes[i], title=c, color='blue', bins=30)\n    \nplt.tight_layout()\nplt.show()","d22f4fb7":"fig, axes = plt.subplots(4,2,figsize=(16, 14))\naxes = np.ravel(axes)\n\nfor i, c in enumerate(numerical_features):\n    hist = train[c].plot(kind = 'box', ax=axes[i],color='blue', vert=False)\n    axes[i].set_title(c, fontsize=15)\n    \nplt.tight_layout()\nplt.show()","dab1e9ef":"t = train[categorical_feature].value_counts(normalize=True)\nt.plot(kind='pie',\n       figsize=(5,5),\n       title=categorical_feature,\n       ylabel=\"\",\n       autopct=\"%.2f\",\n       fontsize=14)\nplt.tight_layout()\nplt.show()","ef8ff16f":"plt.figure(figsize=(14,6))\nsns.heatmap(train.corr(method='pearson'),\n            annot=True,\n            cbar=False,\n            cmap='Blues')\nplt.show()","f34d79d9":"fig, axes = plt.subplots(4,2,figsize=(16, 18))\naxes = np.ravel(axes)\n\nfor i, c in enumerate(numerical_features):\n    _ = sns.scatterplot(x=train[c],\n                        y=train[target],\n                        ax=axes[i],\n                        color='blue')\n    \n    axes[i].set_title(f\"{c} Vs age\",\n                      fontsize=14, \n                      fontweight='bold')\n    \n    axes[i].set_xlabel(c, fontsize=15)\n    axes[i].set_ylabel('age', fontsize=15)\n    \nplt.tight_layout()\nplt.show()","53bd34fe":"fig = plt.figure(figsize=(10,8))\nax = plt.axes(projection='3d')\n\nax.set_xlabel('Height of abalone (mm)')\nax.set_ylabel('Length of abalone (mm)')\nax.set_zlabel('age')\nax.scatter3D(train['Height'],\n             train['Length'], \n             train[target],\n             c='blue',\n             alpha=0.1);\n\nplt.show()","f209a52e":"train.Sex = train.Sex.replace({\"M\":1, \"I\":0, \"F\":-1})\ntest.Sex = test.Sex.replace({\"M\":1, \"I\":0, \"F\":-1})","95788f3c":"idx = train.loc[train.Height>0.4].index\ntrain.drop(idx, inplace=True)\n\nidx = train.loc[train['Viscera weight']>0.6].index\ntrain.drop(idx, inplace=True)\n\nidx = train.loc[train[target]>25].index\ntrain.drop(idx, inplace=True)","1ba6144e":"X_train = train[features]\ny_train = train[target]\n\nX_test = test[features]\ny_test = test[target]\n\nX_train.head()","9ac8063f":"models = {'linear_regression':LinearRegression(),\n         \n         'lasso':Lasso(random_state=1),\n         \n         'decision_tree':DecisionTreeRegressor(random_state=1),\n         \n         'random_forest':RandomForestRegressor(random_state=1),\n         \n         'xgboost':XGBRegressor(random_state=1),\n        }","66d4dd6f":"# \nfor key, regressor in models.items():\n    print(key)\n    eval_model(regressor, X_train, y_train, X_test, y_test)\n    print(\"\\n------------------------------------------\")","0c24ffaf":"# Linear regression\nlr_params = {'fit_intercept':[True,False]}\n\n# Lasso\nlasso_params = {'alpha': [1e-4, 1e-3, 1e-2, 1, 10, 100]}\n\n# Decision tree\ndt_params =  {'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],\n            'min_samples_split': [5, 10, 20, 30, 40, 50],\n            'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n            'max_leaf_nodes': [8, 16, 32, 64, 128,256]}\n\n# Random Forest\nrf_params = {'bootstrap': [True, False],\n             'max_depth': [2, 5, 10, 20, None],\n             'max_features': ['auto', 'sqrt'],\n             'min_samples_leaf': [1, 2, 4],\n             'min_samples_split': [2, 5, 10],\n             'n_estimators': [100, 150, 200, 250]}\n\n# XGBoost\nxgb_params = {'n_estimators':[100, 200, 300] , \n             'max_depth':list(range(1,10)) , \n             'learning_rate':[0.006,0.007,0.008,0.05,0.09] ,\n             'min_child_weight':list(range(1,10))}","484e26b8":"params = [lr_params, lasso_params, dt_params, rf_params, xgb_params]\n\n# searching Hyperparameters\ni=0\nfor name, model in models.items():\n    print(name)\n    regressor = RandomizedSearchCV(estimator = model,\n                                   n_iter=10,\n                                   param_distributions = params[i],\n                                   cv = 3,\n                                   scoring = 'neg_root_mean_squared_error')\n    \n    search = regressor.fit(X_train, y_train)\n    \n    print('Best params :',search.best_params_)\n    print(\"RMSE :\", -search.best_score_)\n    i+=1\n    print()","65210ee7":"rf_params = {'n_estimators': 200, \n             'min_samples_split': 2,\n             'min_samples_leaf': 4, \n             'max_features': 'sqrt', \n             'max_depth': None, \n             'bootstrap': True}\n\nmodel = RandomForestRegressor(random_state=1, **rf_params)\n\nmodel.fit(X_train, y_train)","9a6befd8":"import pickle\nwith open(\"model.pkl\", \"wb\") as f:\n    pickle.dump(model, f)","49141a37":"print(\"Train rmse : \", rmse(y_train, model.predict(X_train)))\nprint(\"Test rmse : \", rmse(y_test, model.predict(X_test)))","f6b93748":"df = pd.DataFrame([features, model.feature_importances_]).T\ndf.columns = ['feature', 'importance']\ndf.sort_values(\"importance\", ascending=False)","6ea16068":"y_pred = model.predict(X_test)\n\nfig = plt.figure(figsize=(10, 6))\nplt.scatter(range(y_test.shape[0]), y_test, color='red', label='y_true')\nplt.scatter(range(y_test.shape[0]), y_pred, color='blue', label='y_pred')\nplt.legend()\nplt.show()","8f865caa":"plt.figure(figsize=(10,5))\nplt.hist(y_pred-y_test, bins=30)\nplt.show()","47278217":"def predict_age(x):\n    x = pd.DataFrame([x], columns=features)\n    age = model.predict(x)\n    return round(age[0],2)","9bf0891b":"with open(\"model.pkl\", 'rb') as f:\n    model = pickle.load(f)\n    \n# Random sample from test set\nex = [0.295 , 0.225 , 0.08  , 0.124 , 0.0485, 0.032 , 0.04  , 0.]\n\nprint(\"Estimated age : \",predict_age(ex))","edf5705b":"## Thank you..!! ##","a4adf767":"<h1 style=\"color:green;\">Evaluation<\/h1>","a354976c":"### Data Sample","1d0bc296":"### Statistical summary of features","4c8da944":"### Removing outliers","0d48cdd0":"### Scatter plot\n#### Relation with target column","291e6a18":"<a id=\"3.3\"><\/a>\n## 3.4 Final Modeling\n### Random forest is performing better : Selected","8f851973":"# Train test split\n<p style=\"font-size: 20px\">Train test split is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.The motivation is quite simple: you should separate your data into train and test splits to prevent your model from overfitting and to accurately evaluate your model.<b> It is also a good practice to only use train data to perform further analysis.<\/b><br><br>\n    <b>Train :<\/b> 75% of data<br>\n    <b>Test :<\/b> 25% of data<br>\n<\/p>","f2d35794":"### Dimensions","51112c85":"### Feature importance","5d8f90b3":"## Scatter plot","7e57687f":"### Save the model\n<p style=\"font-size: 18px\">Saving model for deployment step..<\/p>","a6c20e1a":"### Unique values","d52d1817":"<p style=\"font-size: 18px\">Observation : Some features has a linear relationship with target.<\/p>","1db89db5":"### Feature separation","f7c08083":"# Table of contents\n\n- [Exploratory data analysis](#2)\n\n- [Modeling](#3)\n    \n    - [Feature Engineering](#3.0)\n\n    - [Initial Modeling](#3.1) \n    \n    - [Hyperparameter tuning](#3.2)\n    \n    - [Final Modeling](#3.3)\n    \n- [Evaluation](#4)","7facd715":"<p style=\"font-size: 18px\">Observation : <br>Most of our features contain outliers.<br> 'Height' feature as extreme outliers.<\/p>","4584562d":"## Variable separation","98f3e762":"<a id=\"3.2\"><\/a>\n## 3.3 Hyperparameter tuning","bea465de":"<h1 align=\"center\">Abalone age prediction<\/h1>\n\n# What is Abalone?\n<p style=\"font-size: 18px\">Abalone is a popular choice of seafood\u2014a shellfish to be precise\u2014that lives in cold coastal waters around the world. Biologically, abalone is a mollusk belonging to the Gastropoda class. In plain English, this means that abalone is technically <b>a type of marine snail<\/b>.<\/p>\n\n<img src=\"https:\/\/i2.wp.com\/www.tastyislandhawaii.com\/images10\/kccfm\/kona_abalone_pregrill.jpg\" width=\"600px\">\n\n# How to determine the age of an abalone?\n<p style=\"font-size: 18px\">Scientific studies on abalones require knowing the age of an abalone, but the process of determining age is complicated. It involves measuring the number of layers of shell (\u201crings\u201d) that make up the abalone\u2019s shell. This is done by taking a sample of shell, staining it and counting the number of rings under the microscope.<b>This process is tedious and time-consuming.<\/b><\/p>\n\n# How machine learning can solve this problem?\n<p style=\"font-size: 18px\">As we know, Age is a number and we have data that contains the physical measurements of abalones and their ages. We can build a machine learning model that can predict the age of abalone given its physical measurements like weight, height, etc.<\/p>\n\n---","2ba4fde5":"<a id=\"3.1\"><\/a>\n<h2>3.2 Initial Modeling<\/h2>\n<p style=\"font-size: 18px\"> Since it is a regression problem we can use the <b>Root mean squared error<\/b> as the performance metric.\n    <img src=\"https:\/\/www.datascienceland.com\/media\/uploads\/2020\/12\/15\/rmse\">\n<\/p>\n\n### Base models","f71670c3":"### Missing values","d2252751":"<a id=2><\/a>\n<h1 style=\"color: green;\">2. Exploratory data analysis<\/h1>\n<p style=\"font-size: 20px\">Exploratory Data Analysis or EDA is very crucial for the success of all data science projects. It is an approach to analyze and understand the various aspects of the data.\n<\/p>\n<img src=\"https:\/\/static.wixstatic.com\/media\/704a47_602aa053d04d4d54a4a8aaf75fbfea01~mv2.png\/v1\/fill\/w_740,h_335,al_c,q_90\/704a47_602aa053d04d4d54a4a8aaf75fbfea01~mv2.webp\" width=\"700px\">","42fb3a88":"### Label encoding","a68a0181":"## Read the data","128def0b":"<a id=3><\/a>\n<h1 style=\"font-size: 25px; color:green\">3. Modeling<\/h1>","20ff8cd8":"### Box plot (Outliers)\n<p style=\"font-size: 18px\">Outliers are data points that are far from other data points. In other words, they\u2019re unusual values in a dataset.<\/p>","e9d0498d":"## Pearson Correlation","06fd39db":"## Error distribution","8ecb9613":"## Target distribution","fa03fa01":"### Pie chart : Categorical feature `sex`","94f3ddbf":"### Height Vs Length Vs age","985d43c3":"<b style=\"font-size: 20px; color: green\"> About the data set<\/b>\n<p style=\"font-size: 18px\">\n    Data comes from an original (non-machine-learning) study : The Population Biology of Abalone (_Haliotis_ species) in Tasmania.<br>\n    From the original data examples with missing values were removed, and the ranges of the continuous values have been scaled by dividing with 200.\n<\/p>\n\n## Variable discription\n<p style=\"font-size: 18px\">\n<b>`Sex`:<\/b> M (male), F (female), I (infant)<br>\n<b>`Length`:<\/b> longest shell measurement (in mm)<br>\n<b>`Diameter`:<\/b> measurement perpendicular to legnth (in mm)<br>\n<b>`Height`:<\/b> with meat in shell (in mm)<br>\n<b>`Whole weight`:<\/b> the whole abalone (in grams)<br>\n<b>`Shucked weight`:<\/b> weight of the meat (in grams)<br>\n<b>`Viscera weight`:<\/b> gut weight after bleeding (in grams)<br>\n<b>`Shell weight`:<\/b> after being dried (in grams)<br>\n<b>`Rings`:<\/b> + 1.5 gives the age in years (the value to predict)\n <\/p>\n \n ---","eaa4d272":"<a id=\"3.0\"><\/a>\n<h2>3.1 Feature Engineering<\/h2>\n<p style=\"font-size: 18px\">Feature engineering is the general term for the process of creating and manipulating features, so that a good predictive model can be created.<\/p>","7df84524":"## Distribution of numerical features","a7f04736":"### Prediction on single data point"}}