{"cell_type":{"2aeba55a":"code","d7c1a88d":"code","445089a8":"code","b0ac7e04":"code","505da1bd":"code","49d025ac":"code","5464154a":"code","9b35fe0b":"code","1824b90b":"code","78e82b33":"code","f1de5fcf":"code","65b92a14":"code","e7faf101":"code","385b7cb9":"code","f59ea089":"code","13245de8":"code","a7929fc4":"code","3fb359a6":"code","f4eb5127":"code","481a8f6d":"code","a0661aac":"markdown"},"source":{"2aeba55a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nfrom matplotlib.colors import rgb2hex\nsns.set_style(\"darkgrid\")\nplt.style.use('seaborn')","d7c1a88d":"from sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","445089a8":"from tensorflow.keras.layers import Dense, Input, Dropout, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPool1D, add, AveragePooling1D\nimport tensorflow as tf\nfrom tensorflow.keras import models, optimizers as opts","b0ac7e04":"df = pd.read_csv('..\/input\/electric-motor-temperature\/measures_v2.csv')\ndf.shape","505da1bd":"df.describe()","49d025ac":"df.isnull().sum().sort_values(ascending=False)","5464154a":"target_features = ['motor_speed', 'torque', 'stator_yoke', 'stator_tooth', 'stator_winding']\ninput_features = ['u_q', 'u_d', 'i_q', 'i_d', 'coolant', 'ambient']","9b35fe0b":"def seq_data_preprocessing(data):\n    std_scaler = StandardScaler()\n    data = pd.DataFrame(std_scaler.fit_transform(data), columns=list(data.columns))\n    \n    return data, std_scaler","1824b90b":"def seq_data_inverse_process(data, scaler, columns):\n    \n    data = scaler.inverse_transform(data)\n    if columns:\n        data = pd.DataFrame(data, columns=columns)\n    \n    return data","78e82b33":"df_cpy = df.copy()","f1de5fcf":"def split_sequence_train_test(data, test_size, seq_len, batch_size, input_cols, target_cols, groupby_col='profile_id', random_state=42):\n        \n    X, y = data[input_cols], data[target_cols]\n    X, X_scaler = seq_data_preprocessing(X)\n    y, y_scaler = seq_data_preprocessing(y)\n    \n    data = pd.concat([X, y, data[groupby_col]], axis=1)\n    data_grpd = {pid: df_ for pid, df_ in data.groupby(groupby_col)}\n    \n    sequential_data = []\n    \n    for group_id, group_df in data_grpd.items():\n        batch = []\n        group_df.index = np.arange(len(group_df))\n        batch_no = 0\n                \n        while True:\n            start = np.random.choice(group_df.index, replace=False)\n            end = start + seq_len\n            \n            if end > len(group_df)-1:\n                continue\n                \n            seq_X, seq_y = group_df.loc[start : end-1, input_cols].values, group_df.loc[end, target_cols].values\n            sequential_data.append([seq_X, seq_y])\n            batch_no += 1\n#             print(f\"--> batch no.{batch_no} for profile id {group_id} ready\")\n            \n            if batch_no >= batch_size:\n                break\n    print(\"! Sequence build complete\")\n\n    X, y = zip(*sequential_data)\n    X, y = np.array(X), np.array(y)\n    \n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    print(\"! Train test split complete\")\n\n    return (X_train, X_test, y_train, y_test, X_scaler, y_scaler)","65b92a14":"training_history = []","e7faf101":"%%time\n\nseq_len = 128\nbatch_size = 500\n\nX_train, X_test, y_train, y_test, X_scaler, y_scaler = split_sequence_train_test(df, test_size=0.2, random_state=42, seq_len=seq_len, batch_size=batch_size, input_cols=input_features, target_cols=target_features)","385b7cb9":"X_train.shape","f59ea089":"def cnn_model_builder(seq_len, n_features, n_targets, kernel_size, activation=None):\n    # input shape: window_size x len(x_cols)\n    x = Input((seq_len, n_features), name='input_62')\n    y = Conv1D(filters=1024, kernel_size=kernel_size, padding='same', activation='relu')(x)\n\n    y = Conv1D(filters=512, kernel_size=kernel_size, padding='same',\n                      dilation_rate=2, activation='relu')(y)\n    y = Conv1D(filters=256, kernel_size=kernel_size, padding='same',\n                      dilation_rate=2, activation='relu')(y)\n    \n#     y = AveragePooling1D(pool_size=2, strides=1, padding='same')(y)\n\n    y = Conv1D(filters=256, kernel_size=kernel_size, padding='same',\n                      dilation_rate=2, activation='relu')(y)\n    \n    y = Conv1D(filters=128, kernel_size=kernel_size, padding='same',\n                      dilation_rate=2, activation='relu')(y)\n    \n    shortcut = Conv1D(filters=128, kernel_size=1,\n                             dilation_rate=2, padding='same')(x)\n    y = add([shortcut, y])\n\n    y = MaxPool1D(pool_size=32)(y)\n    y = Flatten()(y)\n    y = Dense(64, name='dense_128', activation=activation)(y)\n    y = Dropout(rate=0.05)(y)\n    y = Dense(n_targets, name=f'dense_{n_targets}', activation=activation)(y)\n\n    model = models.Model(inputs=x, outputs=y)\n    model.compile(optimizer=opts.Adam(), loss='mse', metrics=['MeanSquaredError', 'mae'])\n    \n    return model","13245de8":"cnn_model = cnn_model_builder(seq_len, n_features=6, n_targets=5, kernel_size=2)\ncnn_model.summary()","a7929fc4":"%%time\nepochs = 150\n\nmodel_es_callback = tf.keras.callbacks.EarlyStopping(\n#     filepath='..\/output\/',\n#     save_weights_only=True,\n    monitor='val_mean_squared_error',\n    min_delta = 0.0001,\n    patience = 10,\n    verbose = 1,\n    mode='min',\n    restore_best_weights=True)\n\n\nhistory = cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[model_es_callback])\ntraining_history.append(history)","3fb359a6":"def train_test_visualizer(X_test, y_test, history, target_cols, y_scaler, n_cols=3):\n        \n    epochs = np.arange(1, len(history.history['loss']) + 1)\n    y_pred = history.model.predict(X_test)\n    n_rows = int(np.ceil(y_pred.shape[-1] \/ n_cols))\n    \n    y_test = seq_data_inverse_process(y_test, y_scaler, target_cols)\n    y_pred = seq_data_inverse_process(y_pred, y_scaler, target_cols)\n    \n    mse = history.history['mean_squared_error']\n    val_mse = history.history['val_mean_squared_error']\n    \n    fig1, ax = plt.subplots(figsize=(4 * n_cols, 3))\n    \n    ax.plot(epochs, mse, 'go', label='Training MSE', alpha=0.5)\n    ax.plot(epochs, val_mse, 'r-', label='Validation MSE')\n    ax.set_ylabel('MSE')\n    ax.set_xlabel('No. epoch')    \n    ax.legend()\n    \n    fig1.suptitle('Training Stage')\n    \n    \n    \n    fig2, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows))\n    r2_total = r2_score(y_test, y_pred)\n    model_mse = np.min(history.history['val_mean_squared_error'])\n    \n    for i, (ax, col) in enumerate(zip(axes.flatten(), target_cols)):\n\n        r2_col = r2_score(y_test.loc[:, col], y_pred.loc[:, col])\n        mse = mean_squared_error(y_test.loc[:, col], y_pred.loc[:, col])\n        rmse = np.sqrt(mse)\n        \n        sns.scatterplot(x=y_pred.loc[:, col], y=y_test.loc[:, col], label=f'R2: {r2_col:.2}\\nMSE: {mse:.2}\\nRMSE: {rmse:.2}', ax=ax, alpha=0.3)\n    #     if i % n_cols == 0:\n    #         ax.set_ylabel('Ground Truth')\n    #     else:\n    #         ax.set_ylabel('')\n        ax.set_ylabel('Ground Truth')\n        ax.set_xlabel('Predicted')\n        ax.set_title(col)\n        ax.legend()\n\n    fig2.suptitle(f'Testing Stage --> Total R2={r2_total:.5}; Total MSE={model_mse:.3}')\n    \n    \n    plt.tight_layout()","f4eb5127":"train_test_visualizer(X_test, y_test, training_history[-1], target_features, y_scaler, n_cols=3)","481a8f6d":"training_history[-1].model.save('.\/')","a0661aac":"# Briefly about the task\n\nIn this notebook I have built a **CNN based Deep Learning architecture** which **predicts the following 5 target features**:\n* Predicted Stator Readings\n    * Stator tooth temperature (stator_tooth)\n    * Stator winding temperature (stator_winding)\n    * Stator yoke temperature (stator_yoke)\n    \n* Predicted Rotor Readings\n    * Motor Speed (motor speed)\n    * Torque (torque)\n    \nThese are the outputs of the model which accepts the following as the **input features**:\n* Voltage q-component (u_q)\n* Voltage d-component (u_d)\n* Current d-component (i_q)\n* Current q-component (i_d)\n* Coolant temperature (coolant)\n* Ambient temperature (ambient)"}}