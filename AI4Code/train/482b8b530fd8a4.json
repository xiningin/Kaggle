{"cell_type":{"3f0bd551":"code","7ccee9db":"code","b4d5e15d":"code","b6fec564":"code","7e538c3f":"code","9e802842":"code","d6b455d6":"code","79ea4268":"code","9598f2ea":"code","80de9dee":"code","1158440f":"code","dd3879bd":"code","db95ff9b":"code","d00a7893":"code","3967db76":"markdown","9603d499":"markdown","134253f7":"markdown","eb81d87c":"markdown","5f4c9712":"markdown","e5927aa7":"markdown","52a30bab":"markdown","4e3d9a7f":"markdown","f2091a97":"markdown","818c7f1b":"markdown","c25df2a6":"markdown","984f544e":"markdown","bd74d502":"markdown","4e501e87":"markdown","91836998":"markdown","21adf70c":"markdown","852f7414":"markdown"},"source":{"3f0bd551":"from IPython.display import Image\nImage(\"\/kaggle\/input\/nlpimage\/nlpsearch_engine.jpg\")","7ccee9db":"# ============================================================================\n# PART I: LOAD THE DATA\n# ============================================================================\n## Read worddic from all CORD-19 papers\n\nimport pickle\n\n## Load pickle file worddic\npickle_in = open('\/kaggle\/input\/darvirian\/worddic_all_200407.pkl', 'rb')\nworddic = pickle.load(pickle_in)\n\n## Load pickle file worddic\npickle_in = open('\/kaggle\/input\/darvirian\/df.pkl', 'rb')\ndf = pickle.load(pickle_in)\n\n## Load pickle file sentences\npickle_in = open('\/kaggle\/input\/darvirian\/sentences_200407.pkl', 'rb')\nsentences = pickle.load(pickle_in)","b4d5e15d":"# ============================================================================\n# PART II: The Search Engine\n# ============================================================================\nimport re\nfrom collections import Counter\n\ndef search(searchsentence):\n    # split sentence into individual words\n    searchsentence = searchsentence.lower()\n    # split sentence in words and keep characters as in worddic\n    words = searchsentence.split(' ')\n    words = [re.sub(r'[^a-zA-Z.]', '', str(w)) for w in words]\n\n    # temp dictionaries\n    enddic = {}\n    idfdic = {}\n    closedic = {}\n\n    # remove words if not in worddic \n    # (keep only the words that are in the dictionary)\n    words = [word for word in words if word in worddic.keys()]\n    numwords = len(words)\n\n    # metrics fullcount_order and fullidf_order: \n    # sum of number of occurences of all words in each doc (fullcount_order) \n    # and sum of TF-IDF score (fullidf_order)\n    for word in words:\n        # print(word)\n        for indpos in worddic[word]:\n            # print(indpos)\n            index = indpos[0]\n            amount = len(indpos[1])\n            idfscore = indpos[2]\n            # check if the index is already in the dictionary: \n            # add values to the keys\n            if index in enddic.keys():\n                enddic[index] += amount\n                idfdic[index] += idfscore\n            # if not, just make a two new keys and store the values\n            else:\n                enddic[index] = amount\n                idfdic[index] = idfscore\n    fullcount_order = sorted(enddic.items(), \n                             key=lambda x: x[1], \n                             reverse=True\n                            )\n    fullidf_order = sorted(idfdic.items(), \n                           key=lambda x: x[1], \n                           reverse=True\n                          )\n\n    # metric combocount_order: \n    # percentage of search words (as in dict) that appear in each doc\n    # (and is it a reason to give these docs more relevance)\n    alloptions = {k: worddic.get(k) for k in words}\n    comboindex = [item[0] for worddex in alloptions.values() for item in worddex]\n    combocount = Counter(comboindex) # count the time of each index\n    for key in combocount:\n        combocount[key] = combocount[key] \/ numwords\n    combocount_order = sorted(combocount.items(), \n                              key=lambda x: x[1], \n                              reverse=True\n                             )\n\n    # metric closedic: if words appear in same order as in search\n    if len(words) > 1:\n        x = [index[0] for record in [worddic[z] for z in words] for index in record]\n        y = sorted(list(set([i for i in x if x.count(i) > 1])))\n\n        # dictionary of documents \n        # and all positions (for docs with more than one search word in it)\n        closedic = {}\n        for wordbig in [worddic[x] for x in words]:\n            for record in wordbig:\n                if record[0] in y:\n                    index = record[0]\n                    positions = record[1]\n                    try:\n                        closedic[index].append(positions)\n                    except:\n                        closedic[index] = []\n                        closedic[index].append(positions)\n        # Index add to comprehension:\n        # closedic2 = [record[1] for wordbig in [worddic[x] \n        # for x in words] for record in wordbig if record[0] in y]\n\n        # metric: fdic number of times \n        # search words appear in a doc in descending order\n        # TODO check\n        x = 0\n        fdic = {}\n        for index in y: # list with docs with more than one search word\n            csum = []            \n            for seqlist in closedic[index]:\n                while x > 0:\n                    secondlist = seqlist # second word positions\n                    x = 0\n                    # first and second word next to each other (in same order)\n                    sol = [1 for i in firstlist if i + 1 in secondlist]\n                    csum.append(sol)\n                    fsum = [item for sublist in csum for item in sublist] \n                    fsum = sum(fsum) \n                    fdic[index] = fsum\n                    fdic_order = sorted(fdic.items(), \n                                        key=lambda x: x[1], reverse=True)\n                while x == 0:\n                    firstlist = seqlist # first word positions \n                    x = x + 1 \n    else:\n        fdic_order = 0\n\n    # TODO another metric for if they are not next to each other but still close\n\n    return(searchsentence, \n           words, \n           fullcount_order, \n           combocount_order, \n           fullidf_order, \n           fdic_order\n          )","b6fec564":"# =============================================================================\n# PART III: Rank and return (rule based)\n# =============================================================================\nimport pandas as pd\nimport time \n\n# Find sentence of search word(s)\ndef search_sentence(doc_number, search_term):\n    sentence_index = []\n    search_list = search_term.split()\n    for sentence in sentences[doc_number]:\n        for search_word in search_list:\n            if search_word.lower() in sentence.lower():\n                # df.Sentences[doc_number].index(sentence)\n                sentence_index.append(sentence) \n    return sentence_index\n\ndef rank(term):\n\n    start_time = time.time()\n    # get results from search\n    results = search(term)\n    disp_search_words = results[1]\n    # get metrics\n    # number of search words found in dictionary\n    num_search_words = len(results[1]) \n    # number of search words (as in dict) in each doc (in descending order)\n    num_score = results[2] \n    # percentage of search words (as in dict) in each doc (in descending order)\n    per_score = results[3] \n    # sum of tfidf of search words in each doc (in ascending order)\n    tfscore = results[4] \n    order_score = results[5] # fidc order\n\n    # list of documents in order of relevance\n    final_candidates = []\n\n    # no search term(s) not found\n    if num_search_words == 0:\n        print('Search term(s) not found')\n\n    # single term searched (as in dict): return the following 5 scores\n    if num_search_words == 1:\n        # document numbers\n        num_score_list = [l[0] for l in num_score] \n        # take max 3 documents from num_score\n        num_score_list = num_score_list[:3] \n        # add the best percentage score\n        num_score_list.append(per_score[0][0]) \n        # add the best tf score\n        num_score_list.append(tfscore[0][0]) \n        # remove duplicate document numbers\n        final_candidates = list(set(num_score_list)) \n\n\n    # more than one search word (and found in dictionary)\n    if num_search_words > 1:\n\n        # rule1: doc with high fidc order_score (>1) \n        # and 100% percentage search words (as in dict) on no. 1 position\n        first_candidates = []\n\n        # first candidate(s) comes from fidc order_score (with value > 1)\n        for candidates in order_score:\n            if candidates[1] >= 1:\n                first_candidates.append(candidates[0])\n\n        second_candidates = []\n\n        for match_candidates in per_score:\n            # if all words are in a document: add to second_candidates\n            # TODO check why per_score sometimes > 1 (change to >=1 ?)\n            if match_candidates[1] == 1:\n                second_candidates.append(match_candidates[0])\n        # first final candidates have the highest score of \n        # search words next to each other \n        # and all search words (as in dict) in document  \n        for match_candidates in first_candidates:\n            if match_candidates in second_candidates:\n                final_candidates.append(match_candidates)\n\n        # rule2: add max 4 other words \n        # with order_score greater than 1 (if not yet in final_candiates)\n        t3_order = first_candidates[0:3]\n        for each in t3_order:\n            if each not in final_candidates:\n                final_candidates.insert(len(final_candidates), \n                                        each\n                                       )\n\n        # rule3: add 2 top td-idf results to final_candidates\n        final_candidates.insert(len(final_candidates), \n                                tfscore[0][0]\n                               )\n        final_candidates.insert(len(final_candidates), \n                                tfscore[1][0]\n                               )\n\n        # rule4: next add four other high percentage score \n        # (if not yet in final_candiates)\n        # the first 4 high percentages scores \n        # (if equal to 100% of search words in doc)\n        t3_per = second_candidates[0:3] \n        for each in t3_per:\n            if each not in final_candidates:\n                final_candidates.insert(len(final_candidates), each)\n\n        # rule5: next add any other no. 1 result in num_score, \n        # per_score, tfscore and order_score (if not yet in final_candidates)\n        othertops = [num_score[0][0], \n                     per_score[0][0], \n                     tfscore[0][0], \n                     order_score[0][0]\n                    ]\n        for top in othertops:\n            if top not in final_candidates:\n                final_candidates.insert(len(final_candidates), top)\n\n    # top results: sentences with search words, \n    # paper ID (and documet number), authors and abstract\n    df_results = pd.DataFrame(columns=['Title', \n                                       'Paper_id', \n                                       'Document_no', \n                                       'Authors', \n                                       'Abstract', \n                                       'Sentences'\n                                      ]\n                             )\n    for index, results in enumerate(final_candidates):\n        # if index < 5:\n        df_results.loc[index+1, 'Title'] = df.title[results]\n        df_results.loc[index+1, 'Paper_id'] = df.paper_id[results]\n        df_results.loc[index+1, 'Document_no'] = results\n        df_results.loc[index+1, 'Authors'] = df.authors[results]\n        df_results.loc[index+1, 'Abstract'] = df.abstract[results]\n        search_results = search_sentence(results, term)\n        # remove duplicate sentences\n        unique_search_results = list(dict.fromkeys(search_results))\n        df_results.loc[index+1, 'Sentences'] = unique_search_results\n    \n    end_time = time.time()\n    # print final candidates\n    print('\\nFound search words:', disp_search_words)\n    print('Number of documents found:', len(final_candidates))\n    print('Processing Time: ', round(end_time - start_time, 2), ' s')\n    # print('Ranked papers (document numbers):', final_candidates)        \n        \n    return final_candidates, df_results\n\n\n","7e538c3f":"# truncate words\/phases up to length sz\n# for hoover tool to be a bit tidy \n# when showing the sentence count, paper ID, paper title, and abstract\ndef truncate_str(input_str,sz):\n    if len(input_str) < sz:\n      return input_str\n    else :\n      return input_str[:sz] + ' ...'\n\ndef truncate_palatte(desc_num_sentence, \n                     desc_paper_id, \n                     desc_paper_title, \n                     desc_paper_abstract,\n                     desc_paper_sentences\n                    ):\n    sz = len(desc_num_sentence)\n    if sz < 256:\n        source = ColumnDataSource(data=dict(paper_id=desc_paper_id, \n                                            title=desc_paper_title, \n                                            abstract=desc_paper_abstract, \n                                            sentences=desc_paper_sentences, \n                                            paper=[x for x in range(0,sz)], \n                                            count=desc_num_sentence, \n                                            color=viridis(sz)\n                                            )  \n                                 )   \n    else :\n        sz = 256 - 1\n        source = ColumnDataSource(data=dict(paper_id=desc_paper_id[:sz], \n                                            title=desc_paper_title[:sz], \n                                            abstract=desc_paper_abstract[:sz], \n                                            sentences=desc_paper_sentences[:sz],\n                                            paper=[x for x in range(0,sz)], \n                                            count=desc_num_sentence[:sz], \n                                            color=viridis(sz)\n                                            )  \n                                 )\n \n    return source\n    \nimport numpy as np\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.palettes import viridis\nfrom bokeh.io import show, output_notebook\n\ndef bokeh_interactive_plot(rank_result):\n\n    #collect lists for hoover tool visualisation \n    paper_id = list(rank_result['Paper_id'])\n    paper_title = list(rank_result['Title'])\n    paper_abstract = list(rank_result['Abstract'])\n    paper_sentences = list(rank_result['Sentences'])\n\n    # sort according to decreasing sentence counts\n    num_sentence_per_paper = [len(rank_result['Sentences'][x]) \n                              for x in range(1,len(rank_result)+1)\n                             ]\n    descending_sort_idx = np.array(num_sentence_per_paper).argsort()[::-1]\n\n    sort_idx = descending_sort_idx\n    desc_num_sentence = [num_sentence_per_paper[sort_idx[x]] \n                         for x in range(len(sort_idx))\n                        ]\n    desc_paper_id = [paper_id[sort_idx[x]] for x in range(len(sort_idx))]\n    # displaying  paper_title up to 200 characters\n    desc_paper_title = [truncate_str(str(paper_title[sort_idx[x]]),200) \n                        for x in range(len(sort_idx))\n                       ]\n    # displaying paper_abstract up to 400 characters\n    desc_paper_abstract = [truncate_str(str(paper_abstract[sort_idx[x]]),400) \n                           for x in range(len(sort_idx))\n                          ]\n    # displaying paper_sentences up to 600 characters\n    desc_paper_sentences = [truncate_str(str(paper_sentences[sort_idx[x]]),600) \n                           for x in range(len(sort_idx))\n                          ]\n\n    # Bokeh's mapping of column names and data lists\n    source = truncate_palatte(desc_num_sentence, \n                              desc_paper_id, \n                              desc_paper_title, \n                              desc_paper_abstract,\n                              desc_paper_sentences\n                             )\n    \n    sz = len(desc_num_sentence)\n    x_val = [x for x in range(0,sz)]\n    y_val = desc_num_sentence\n\n    p = figure(x_range=(-int(max(x_val)\/6),max(x_val)+int(max(x_val)\/2)), \n               y_range=(-int(max(y_val)\/2),max(y_val)+int(max(y_val)\/5)), \n               plot_height=750, \n               plot_width=950, \n               title=\"Sentence Counts per paper\",\n               toolbar_location=None, \n               tools=\"\"\n              )\n\n    # Render and show the vbar plot\n    p.vbar(x='paper', top='count', width=0.9, color='color', source=source)\n    # Hover tool referring to our own data field using @\n    p.add_tools(HoverTool(tooltips=[(\"count\", \"@count\"),\n                                    (\"ID\", \"@paper_id\"),\n                                    (\"title\", \"@title\"),\n                                    (\"abstract\", \"@abstract\"),\n                                    (\"sentences\", \"@sentences\")\n                                   ]\n                         )\n               )\n\n    # Set to output the plot in the notebook\n    output_notebook()\n    # Show the plot\n    return show(p, notebook_handle=True)","9e802842":"papers, rank_result = rank('Real-time tracking whole genomes mechanism coordinating rapid dissemination information online GISAID NCBI GenBank MN908947 MN996532 AY278741 KY417146 MK211376 development diagnostics therapeutics Chloroquine track variations virus time')\nbokeh_interactive_plot(rank_result)","d6b455d6":"papers, rank_result = rank('Wuhan seafood market geographic temporal sample sets distribution genomic differences phylogenetic strain circulation genetic lineages multi-lateral agreements Nagoya Protocol')\nbokeh_interactive_plot(rank_result)","79ea4268":"papers, rank_result = rank('Evidence animal livestock market infected zoonotic transmission field surveillance genetic sequencing ACE2 receptor binding reservoir epidemic  pathogenesis')\nbokeh_interactive_plot(rank_result)","9598f2ea":"papers, rank_result = rank('Evidence farmers infected role origin zoonotic transmission pathogenesis human-to-human transmission')\nbokeh_interactive_plot(rank_result)","80de9dee":"papers, rank_result = rank('Surveillance wildlife livestock farms SARS-CoV-2 other coronaviruses SARS-CoV MERS-CoV SARS-CoV-2 HKU1 NL63 OC43 229E Southeast Asia Indonesia Malaysia Singapore Philippines East Timor Brunei Cambodia Laos Myanmar Thailand Vietnam')\nbokeh_interactive_plot(rank_result)","1158440f":"papers, rank_result = rank('Experimental infections host range pathogen ACE2 receptor')\nbokeh_interactive_plot(rank_result)","dd3879bd":"papers, rank_result = rank('Animal host evidence RaTG13 Rhinolophus affinis bats ferrets cats pangolins')\nbokeh_interactive_plot(rank_result)","db95ff9b":"papers, rank_result = rank('Socioeconomic behavioral risk factors healthcare services business closure unemployment')\nbokeh_interactive_plot(rank_result)","d00a7893":"papers, rank_result = rank('Sustainable risk reduction strategies community-based measures regular handwashing quarantine')\nbokeh_interactive_plot(rank_result)","3967db76":"### Part 5: Questions","9603d499":"### Introduction\n\nCreated for Kaggle CORD-19-research-challenge: **What do we know about virus genetics, origin, and evolution?**\n\nThe code below is inspired by [NLP Search Engine](https:\/\/www.kaggle.com\/amitkumarjaiswal\/nlp-search-engine). The customisation of the code for the CORD-19 Research Challenge can be found in [darvirian github repository](https:\/\/github.com\/HenryBol\/darvirian\/).\n\nThe main python code of the Darvirian search engine is provided:\n* [Darvirian_Search_Engine_main.py](https:\/\/github.com\/HenryBol\/darvirian\/blob\/master\/Darvirian_Search_Engine_main.py)\n\nThe flowchart of Darvirian search engine is displayed as follows.","134253f7":"* Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\n\nFor better quality of results, we add the keywords: *online* *GISAID* *NCBI* *GenBank* *MN908947* *MN996532* *AY278741* *KY417146* *MK211376* *Chloroquine* ","eb81d87c":"* Socioeconomic and behavioral risk factors for this spill-over\n\nFor better quality of results, we add the keywords: *healthcare* *services* *business* *closure* *unemployment* ","5f4c9712":"Not shown in the flowchart diagram above is the visualisation, where we use [bokeh library](https:\/\/bokeh.org\/) for interactive plotting.","e5927aa7":"* Experimental infections to test host range for this pathogen.\n\nFor better quality of results, we add the keywords: *ACE2* *receptor*","52a30bab":"* Sustainable risk reduction strategies\n\nFor better quality of results, we add the keywords: *community-based* *measures* *regular* *handwashing* *quarantine*","4e3d9a7f":"* Animal host(s) and any evidence of continued spill-over to humans\n\nFor better quality of results, we add the keywords: *RaTG13* *Rhinolophus* *affinis* *bat*  *ferrets* *cats* *pangolins*","f2091a97":"### Part 2: The Search Engine\n\nObjective: to create word search which takes multiple words (sentence) and finds documents that contain these words along with metrics for ranking:\n\nOutput: *searchsentence*, *words*, *fullcount_order*, *combocount_order*, *fullidf_order*, *fdic_order*\n1. *searchsentence* : original sentence to be searched\n2. *words* : words of the search sentence that are found in the dictionary (worddic)\n3. *fullcount_order* : number of occurences of search words\n4. *combocount_order* : percentage of search terms\n5. *fullidf_order* : sum of TD-IDF scores for search words (in ascending order)\n6. *fdic_order* : exact match bonus: word ordering score\n\nFor a better quality of results, we add selected keywords provided by a domain-knowledge expert.","818c7f1b":"* Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n\nFor better quality of results, we add the keywords: *zoonotic* *transmission* *pathogenesis* *human-to-human* *transmission*","c25df2a6":"### Part 1: Loading the data\n\nThe details of preprocessing of data are provided in full at [Darvirian_Search_Engine_main.py](https:\/\/github.com\/HenryBol\/darvirian\/blob\/master\/Darvirian_Search_Engine_main.py). \n\nFor cleaning of data we have adopted the code by [xhlulu](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv). The cleaned csv files are available at [darvirian github repository](https:\/\/github.com\/HenryBol\/darvirian\/). \n\nThe pre-processed data that are available at [darvirian github repository](https:\/\/github.com\/HenryBol\/darvirian\/) are also provided at the [data repository of this notebook](https:\/\/www.kaggle.com\/josephambrosepagaran\/darvirian-searchengine\/#data).\n\nFor simplicity, only the much needed data for searching and visualiation are loaded as shown in the following lines of code.","984f544e":"* Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\n\nFor better quality of results, we add the keywords: *Wuhan* *seafood* *market* *phylogenetic* *genetic* *lineages*.","bd74d502":"### Part 3: Rank and Rule Based\n\nCreate a simple rule based rank and return function with the following rules:\n\n* rule (1) : doc with high fidc order_score (>1) and 100% percentage search words (as in dict) on no. 1 position\n* rule (2) : add max 4 other words with order_score greater than 1 (if not yet in final_candiates)\n* rule (3) : add 2 top td-idf results to final_candidates\n* rule (4) : next add four other high percentage score (if not yet in final_candiates) the first 4 high percentages scores (if equal to 100% of search words in doc)\n* rule (5) : next add any other no. 1 result in num_score, per_score, tfscore and order_score (if not yet in final_candidates)","4e501e87":"* Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n\nFor better quality of results, we add the keywords: *SARS-CoV* *MERS-CoV* *SARS-CoV-2* *HKU1* *NL63* *OC43* *229E* *Indonesia* *Malaysia* *Singapore* *Philippines* *East Timor* *Brunei* *Cambodia* *Laos* *Myanmar* *Thailand* *Vietnam*","91836998":"### Part 4: Visualisation\n\nWe use [bokeh](https:\/\/bokeh.org\/) interactive plot package. For the hoover tool, the ColumnDataSource() includes the paper ID, title, abstract, sentence counts and the color palette. In order for the pop-up visuals displayed when mouse hoover, *truncate_str()* function is written. And also because the color palette is limited to 256 colors, we added a *truncate_palatte()* function to display only 256 colors and not more.","21adf70c":"* Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over. \n\nFor better quality of results, we add the keywords: *market* *ACE2* *zoonotic* *transmission* *pathogenesis*. ","852f7414":"### Developed by  [Henry Bol MSc](https:\/\/www.linkedin.com\/in\/henrybol\/), [Yao Yao PhD](mailto:yaocong111@gmail.com), and [Joseph Ambrose Pagaran PhD](https:\/\/www.linkedin.com\/in\/joseph-ambrose-p-04b466ba\/)"}}