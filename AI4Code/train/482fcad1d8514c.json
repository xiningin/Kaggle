{"cell_type":{"b1236188":"code","acda7ff5":"code","6df7261b":"code","7ccf32b9":"code","bb146e5b":"code","89b4eae9":"code","316c6492":"code","efa7e736":"code","9664b229":"code","343b784f":"code","b72a07d7":"code","be992e97":"code","eda5c18e":"code","41dfc9ba":"code","c6589ac0":"code","92b05c6d":"code","380eae36":"code","3d67a449":"code","8bf47742":"code","00f2bef6":"code","c2d732ca":"code","cb346d00":"code","c9e35112":"code","1e1ff000":"code","de42f3f4":"code","515a4be6":"code","551268c3":"code","3c5285fe":"code","0ec48612":"code","bec5bce1":"code","17bd92d9":"code","1dc7d1a7":"code","858197df":"code","feb0d25c":"code","394e6167":"code","98726857":"code","1543d9fa":"code","8dddcab4":"code","fda5d4cc":"code","f8f48b9a":"code","21928345":"code","48a7c8ed":"code","16bffc15":"code","da120e03":"code","80e397fd":"code","8075b1cc":"code","cf018211":"code","54244bfe":"code","fff62449":"code","957c2270":"code","e2271475":"markdown","cd9a2dc3":"markdown","6dc96a82":"markdown","07cfbd42":"markdown","65ecd63c":"markdown","c6bf819d":"markdown","a83773d1":"markdown","e770090c":"markdown","c6eefd54":"markdown","46fa9a16":"markdown","6c2da512":"markdown","0b90c8e4":"markdown","d79472e8":"markdown","3154f86b":"markdown","0f84f251":"markdown","ad8fc193":"markdown","13d0c1ee":"markdown","b916b8fe":"markdown","897bfbad":"markdown","615eacd7":"markdown","4f03d47a":"markdown","51a3ffa9":"markdown","199b8979":"markdown"},"source":{"b1236188":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acda7ff5":"import pandas as pd\ndf = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')\ndf.head()","6df7261b":"df.shape","7ccf32b9":"df.info()","bb146e5b":"len(df[\"Geography\"].unique())","89b4eae9":"len(df[\"Surname\"].unique())","316c6492":"len(df[\"Exited\"].unique())","efa7e736":"# Get unique count for each variable\ndf.nunique()","9664b229":"# Drop the columns as explained above\ndf = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)","343b784f":"print(\"Min age in dataset\", min(df[\"Age\"].unique()))\nprint(\"Max age in dataset\", max(df[\"Age\"].unique()))","b72a07d7":"print(\"Min NumOfProducts in dataset\", min(df[\"NumOfProducts\"].unique()))\nprint(\"Max NumOfProducts in dataset\", max(df[\"NumOfProducts\"].unique()))","be992e97":"print(\"Min CreditScore in dataset\", min(df[\"CreditScore\"].unique()))\nprint(\"Max CreditScore in dataset\", max(df[\"CreditScore\"].unique()))","eda5c18e":"print(\"Min Tenure in dataset\", min(df[\"Tenure\"].unique()))\nprint(\"Max Tenure in dataset\", max(df[\"Tenure\"].unique()))","41dfc9ba":"print(\"Min Tenure in dataset\", min(df[\"Exited\"].unique()))\nprint(\"Max Tenure in dataset\", max(df[\"Exited\"].unique()))","c6589ac0":"df.head()","92b05c6d":"print(\"Shape of the dataset \",df.shape)","380eae36":"df.describe()","3d67a449":"df.isnull().sum()","8bf47742":"df1 = df.copy()","00f2bef6":"import matplotlib.pyplot as plt\nimport seaborn as sns","c2d732ca":"corr = df.corr()\nplt.figure(figsize = (12,12))\nsns.heatmap(corr  , linewidths= 0.01 , linecolor= \"white\" , cmap= \"coolwarm\" , annot = True).set_title(\"Correlation\".upper())","cb346d00":"size  = df[\"IsActiveMember\"].value_counts(sort =True)\ncolors = [\"magenta\",\"mediumslateblue\"]\nlabels = [\"Yes\",\"No\"]\nexplode = (0, 0.1)\nplt.figure(figsize=(10 , 8))\nplt.pie(size,colors=colors,autopct='%1.1f%%',shadow=True,startangle = 270 ,explode= explode, labels=labels)\nplt.title(\"Active and Non active member\")\nplt.show()","c9e35112":"labels = 'active', 'non active' \ncolors = [\"magenta\",\"mediumslateblue\"]\n\nsizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots(figsize=(10, 8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90 , colors = colors)\nax1.axis('equal')\nplt.title(\"Proportion of customer active and non active\", size = 20)\nplt.show()","1e1ff000":"plt.figure(figsize=(8,4))\nsns.countplot(x='IsActiveMember', data= df , palette= \"magma\",hue = \"Gender\")","de42f3f4":"plt.figure(figsize=(8,4))\nsns.countplot(x='IsActiveMember', data= df , palette= \"icefire\",hue = \"Geography\")","515a4be6":"plt.figure(figsize=(8,4))\nsns.countplot(x='IsActiveMember', data= df , palette= \"YlOrRd\",hue = \"Exited\")","551268c3":"num_projects=df.groupby('Geography').count()\nplt.bar(num_projects.index.values, num_projects['Gender'] , color = \"slateblue\" , edgecolor = \"navy\" ,linewidth = 5 )\nplt.xlabel('Geography')\nplt.ylabel('Gender')\nplt.show()","3c5285fe":"num_projects=df.groupby('NumOfProducts').count()\nplt.bar(num_projects.index.values, num_projects['Gender'] , color = \"slateblue\" , edgecolor = \"navy\" ,linewidth = 5 )\nplt.xlabel('NumOfProducts')\nplt.ylabel('Gender')\nplt.show()","0ec48612":"import numpy as np\nbins = [ 0, 18, 24, 35, 60,92, 100]\nlabels = [\"Unknown\",'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ndf[\"age\"] = df[\"Age\"]\ndf['AgeGroup'] = pd.cut(df[\"Age\"], bins, labels = labels)\ndf[\"AgeGroup\"].isnull().sum()\n\nage_mapping = {\"Unknown\":0,'Teenager': 1, 'Student': 2, 'Young Adult': 3, 'Adult': 4, 'Senior': 5}\ndf[\"AgeGroup\"].fillna(value = 'Unknown' ,inplace = True)\n\n\ndf['AgeGroup'] = df['AgeGroup'].map(age_mapping).astype(\"int\")\n","bec5bce1":"print(df[\"AgeGroup\"].unique())","17bd92d9":"label = {'France':1, 'Germany':2, 'Spain':3}\ndf.replace({'Geography':label}, inplace = True)\ndf.head()","1dc7d1a7":"label = {'Female':0, 'Male':1}\ndf.replace({'Gender':label}, inplace = True)\ndf.head()","858197df":"df['BalanceSalaryRatio'] = df.Balance\/df.EstimatedSalary\nsns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df , palette= \"bone\")\nplt.ylim(-1, 5)","feb0d25c":"df['TenureByAge'] = df.Tenure\/(df.age)\nsns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df , palette = \"Blues\" )\nplt.ylim(-1, 1)\nplt.show()","394e6167":"\ndf['CreditScoreGivenAge'] = df.CreditScore\/(df.age)\ndf.head()","98726857":"\ncontinuous_vars = ['CreditScore',  'AgeGroup', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n                   'TenureByAge','CreditScoreGivenAge']\ncat_vars = ['HasCrCard', 'IsActiveMember','Geography', 'Gender']\ndf = df[['Exited'] + continuous_vars + cat_vars]\ndf.head()","1543d9fa":"\nminVec = df[continuous_vars].min().copy()\nmaxVec = df[continuous_vars].max().copy()\ndf[continuous_vars] = (df[continuous_vars]-minVec)\/(maxVec-minVec)\ndf.head()","8dddcab4":"def DfPrepPipeline(df_predict,df_Cols,minVec,maxVec):\n    # Add new features\n    df_predict['BalanceSalaryRatio'] = df_predict.Balance\/df_predict.EstimatedSalary\n    df_predict['TenureByAge'] = df_predict.Tenure\/(df_predict.Age - 18)\n    df_predict['CreditScoreGivenAge'] = df_predict.CreditScore\/(df_predict.Age - 18)\n    # Reorder the columns\n    continuous_vars = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','BalanceSalaryRatio',\n                   'TenureByAge','CreditScoreGivenAge']\n    cat_vars = ['HasCrCard','IsActiveMember',\"Geography\", \"Gender\"] \n    df_predict = df_predict[['Exited'] + continuous_vars + cat_vars]\n    # Change the 0 in categorical variables to -1\n    df_predict.loc[df_predict.HasCrCard == 0, 'HasCrCard'] = -1\n    df_predict.loc[df_predict.IsActiveMember == 0, 'IsActiveMember'] = -1\n    # One hot encode the categorical variables\n    lst = [\"Geography\", \"Gender\"]\n    remove = list()\n    for i in lst:\n        for j in df_predict[i].unique():\n            df_predict[i+'_'+j] = np.where(df_predict[i] == j,1,-1)\n        remove.append(i)\n    df_predict = df_predict.drop(remove, axis=1)\n    # Ensure that all one hot encoded variables that appear in the train data appear in the subsequent data\n    L = list(set(df_Cols) - set(df_predict.columns))\n    for l in L:\n        df_predict[str(l)] = -1        \n    # MinMax scaling coontinuous variables based on min and max from the train data\n    df_predict[continuous_vars] = (df_predict[continuous_vars]-minVec)\/(maxVec-minVec)\n    # Ensure that The variables are ordered in the same way as was ordered in the train set\n    df_predict = df_predict[df_Cols]\n    return df_predict\n","fda5d4cc":"# Split Train, test data\ndf_train = df.sample(frac=0.7 ,random_state=200)\ndf_test = df.drop(df_train.index)\nprint(len(df_train))\nprint(len(df_test))","f8f48b9a":"df_test.head()","21928345":"#Support functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform\n\n# Fit models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# Scoring functions\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n","48a7c8ed":"# Function to give best model score and parameters\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\ndef get_auc_scores(y_actual, method,method2):\n    auc_score = roc_auc_score(y_actual, method); \n    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n    return (auc_score, fpr_df, tpr_df)","16bffc15":"log_primal = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=250, multi_class='auto',n_jobs=None, \n                                penalty='l2', random_state=None, solver='lbfgs',tol=1e-05, verbose=0, warm_start=False)\nlog_primal.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nprint(\"Classification Report for Logistic Regression\")\nprint(classification_report(df_test.Exited, log_primal.predict(df_test.loc[:, df_test.columns != 'Exited'])))","da120e03":"\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(df_train.loc[:, df_train.columns != 'Exited'])\nlog_pol2 = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=300, multi_class='auto', n_jobs=None, \n                              penalty='l2', random_state=None, solver='liblinear',tol=0.0001, verbose=0, warm_start=False)\nlog_pol2.fit(df_train_pol2,df_train.Exited)\nprint(\"Classification Report for logistic regression with pol 2 kernel\")\n\nprint(classification_report(df_train.Exited,  log_pol2.predict(df_train_pol2)))","80e397fd":"\nSVM_RBF = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf', max_iter=-1, probability=True, \n              random_state=None, shrinking=True,tol=0.001, verbose=False)\nSVM_RBF.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nprint(\"Classification Report for SVM with RBF Kernel\")\n\nprint(classification_report(df_train.Exited,  SVM_RBF.predict(df_train.loc[:, df_train.columns != 'Exited'])))","8075b1cc":"\nSVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',  max_iter=-1,\n              probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)\nSVM_POL.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nprint(\"Classification Report for SVM with Pol Kernel\")\n\nprint(classification_report(df_train.Exited,  SVM_POL.predict(df_train.loc[:, df_train.columns != 'Exited'])))","cf018211":"\nRF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',max_depth=8, max_features=6, max_leaf_nodes=None,min_impurity_decrease=0.0,\n                            min_impurity_split=None,min_samples_leaf=1, min_samples_split=3,min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n                            oob_score=False, random_state=None, verbose=0,warm_start=False)\nRF.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nprint(\"Classification Report for Random Forest classifier\")\n\nprint(classification_report(df_train.Exited,  RF.predict(df_train.loc[:, df_train.columns != 'Exited'])))","54244bfe":"\nXGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=7,\n                    min_child_weight=5, missing=None, n_estimators=20,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1)\nXGB.fit(df_train.loc[:, df_train.columns != 'Exited'],df_train.Exited)\nprint(\"Classification Report for Extreme Gradient Boost Classifier\")\n\nprint(classification_report(df_train.Exited,  XGB.predict(df_train.loc[:, df_train.columns != 'Exited'])))","fff62449":"# Compare Algorithms\nimport pandas\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels = []\nmodels.append(('Logistic Regression', log_primal))\n#models.append(('Logistic Regression for pol 2 kernel', poly2))\nmodels.append(('SVM for RBF kernel', SVM_RBF))\nmodels.append(('SVM for POL kernel', SVM_POL))\nmodels.append(('Random Forest Classifier', RF))\nmodels.append(('XGB', XGB))\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    cv_results = model_selection.cross_val_score(model, df_train.loc[:, df_train.columns != 'Exited'], df_train.Exited , cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","957c2270":"# boxplot algorithm comparison\nfig = plt.figure(figsize =(20,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","e2271475":" Arrange columns by data type for easier manipulation","cd9a2dc3":"Lastly we introduce a variable to capture credit score given age to take into account credit behaviour visavis adult life","6dc96a82":"# Data Visualisation  ","07cfbd42":"# Algorithm Comparision","65ecd63c":"[Link to Git Repository](https:\/\/github.com\/Apurva-tech\/IET.git)","c6bf819d":"# SVM with RBF Kernel","a83773d1":"# Splitting Data Into Test and Train sets ( 0.7 )","e770090c":"Adding new column BalanceSalaryRatio to scale data","c6eefd54":"# Data Analysis","46fa9a16":"# Imputing Data","6c2da512":"Number of unique values for categorical data :","0b90c8e4":"# Treating the categorical Data","d79472e8":"# Logistic regression with pol 2 kernel","3154f86b":"Given that tenure is a 'function' of age, we introduce a variable aiming to standardize tenure over age:\n","0f84f251":"# The best Algorithm is Extreme Gradient Boost Classifier and Random forest classifier","ad8fc193":"# Random Forest classifier","13d0c1ee":"# Feature Engineering","b916b8fe":"The data is already imputed","897bfbad":"# SVM with Pol Kernel","615eacd7":"# Load Dataset","4f03d47a":"# Logistic Regression Model ","51a3ffa9":"minMax scaling the continuous variables","199b8979":"# Extreme Gradient Boost Classifier"}}