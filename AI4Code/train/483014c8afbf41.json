{"cell_type":{"01575564":"code","463895fd":"code","2f913390":"code","830e0680":"code","c69f5c12":"code","08450007":"code","28d97397":"code","03b95e23":"code","d58d6b85":"code","75e8a109":"code","4128e69b":"code","5a28a037":"code","9040e097":"code","3bf03704":"code","fad9c91e":"code","8b9b9e23":"code","bd0f12f2":"code","0b48d5c5":"code","0d4dbfbb":"code","0b37aaa7":"code","e81cc618":"code","4793d67b":"code","a75bb4c6":"code","74247db2":"code","e043ad8c":"code","db71d068":"code","802cfc93":"code","f2abab68":"code","b58775bc":"code","fa735c80":"markdown","04430fb6":"markdown","b5c52013":"markdown","e2983598":"markdown","c6df3660":"markdown","1357e3ae":"markdown","54947398":"markdown","44509e84":"markdown"},"source":{"01575564":"import os, re, gc, warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import wasserstein_distance as wd\nfrom tqdm import tqdm_notebook\nwarnings.filterwarnings(\"ignore\")\ngc.collect()","463895fd":"DATA_DIR = '..\/input\/'\nFILES={}\nfor fn in os.listdir(DATA_DIR):\n    FILES[ re.search( r'[^_\\.]+', fn).group() ] = DATA_DIR + fn\n\ntrain = pd.read_csv(FILES['train'],index_col='id')\ntest = pd.read_csv(FILES['test'],index_col='id')\n    \nCAT_COL='wheezy-copper-turtle-magic'\nCATS = sorted(train[CAT_COL].unique())\nR = int(np.e*1e7)","2f913390":"from sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n# from sklearn.mixture import GaussianMixture\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n# from sklearn.linear_model import LogisticRegression","830e0680":"def fit_set(y, X, T, k, reg):\n\n    oof=pd.Series(0.5,index=X.index,dtype='float')\n    preds=pd.DataFrame(index=T.index,dtype='float')\n    y_,X_,T_=[],[],[]\n    \n    for i in tqdm_notebook( CATS ):\n        y_i=y[X[CAT_COL]==i]\n        X_i=X[X[CAT_COL]==i].drop(CAT_COL,axis=1).dropna(axis=1)\n        T_i=T[T[CAT_COL]==i].drop(CAT_COL,axis=1).dropna(axis=1)\n        \n        o,p = fit_one_model(y_i, X_i, T_i, k, reg)\n\n        oof[o.index]=o\n        preds = pd.concat([preds,p])\n        X_i[CAT_COL]=i\n        T_i[CAT_COL]=i\n        \n        y_.append(y_i)\n        X_.append(X_i)\n        T_.append(T_i)\n    \n    preds = preds.dropna()\n    df_y = pd.concat([y for y in y_])\n    df_X = pd.concat([x for x in X_])\n    df_T = pd.concat([t for t in T_])        \n        \n    print(f\"\\n\\nOut-of-fold ROC AUC for entire labeled dataset: {roc_auc_score(y.values,oof.values):.5f}\")\n    return df_y, df_X, df_T, oof, preds","c69f5c12":"def fit_one_model(y, X:pd.DataFrame, T:pd.DataFrame, k:int=15, reg=.5):\n    # Pick a classifier\n    # clf = LogisticRegression(solver='liblinear',penalty='l2',C=1.0)\n    clf = QuadraticDiscriminantAnalysis(reg_param=reg, store_covariance=False, tol=1e-4)    \n    # clf = GaussianMixture(n_components = 2,\n    #                       covariance_type = 'full',#\u2018full\u2019\u2018tied\u2019\u2018diag\u2019\u2018spherical\u2019\n    #                       max_iter = 100,\n    #                       n_init = 100,\n    #                       init_params = 'random', #\u2018kmeans\u2019\u2018random\u2019\n    #                       tol = 1e-3,\n    #                       reg_covar = 1e-6,\n    #                       random_state = R,\n    #                       warm_start = True,\n    #                       verbose = 0,#0,1,2,\n    #                       verbose_interval = 10,\n    #                      )\n    oof=pd.Series(0.5,index=X.index, dtype='float') \n    p_fold=[]\n \n    i=0\n    skf = StratifiedKFold(n_splits=k, random_state=R, shuffle=True)\n    for tr_idx, te_idx in skf.split(X, y):\n        i+=1\n        clf.fit(X.iloc[tr_idx],y.iloc[tr_idx])\n        oof[X.index[te_idx]]= clf.predict_proba(X.iloc[te_idx])[:,1]\n        p_fold.append(pd.Series(  clf.predict_proba(T)[:,1], index=T.index, name='fold_'+str(i)  ))\n    \n    preds = pd.concat([fold for fold in p_fold],axis=1)\n    # no k-fold training\n#         clf.fit(X,y)\n#         preds = pd.concat([preds,\n#                            pd.Series(clf.predict_proba(T)[:,1],\n#                                      index=T.index,\n#                                      name='fold_'+str(i))], axis=1)\n\n\n    #  Summary output for each of 512 models    \n#     print(f\"{len(X.columns)} features used\\\n#     \\tIn-fold classifier score: {clf.score(X,y):.5f}\\\n#     \\tOut-of-fold ROC AUC: {roc_auc_score(y,oof.values):.5f}\")\n\n    return oof, preds","08450007":"def update_dataset(y, X, T, o=None, p=None, iter_n=0):\n    y_, X_, T_=[],[],[]\n    for i in tqdm_notebook(CATS):\n        idx_x = (X[CAT_COL]==i)\n        idx_t = (T[CAT_COL]==i)\n        y_i = y[ idx_x ]\n        X_i = X[ idx_x ].dropna(axis=1)\n        T_i = T[ idx_t ].dropna(axis=1)\n\n        if iter_n==1:\n            y_i, X_i, T_i = first_pass(y_i, X_i, T_i)\n        else:\n            o_i = o[ idx_x ]\n            p_i = p[ idx_t ]\n            y_i, X_i, T_i = iterative_filter_augment(y_i, X_i, T_i, o_i, p_i, iter_n)\n            \n        X_i[CAT_COL]=i\n        T_i[CAT_COL]=i\n        y_.append(y_i)\n        X_.append(X_i)\n        T_.append(T_i)\n    y_new = pd.concat([y for y in y_])\n    X_new = pd.concat([x for x in X_])\n    T_new = pd.concat([t for t in T_])\n    return y_new, X_new, T_new","28d97397":"from sklearn import manifold\n\ndef first_pass(y, X, T):\n    # AUGMENTATION\n    # Add 1 synthetic feature from low variance features\n    low_var_cols = X.loc[:,(X.var()<2)].columns\n    # Sum of all the absolute values of the low variance features\n    X['synthetic'] = X[low_var_cols].abs().sum(axis=1)\n    T['synthetic'] = T[low_var_cols].abs().sum(axis=1)   \n    \n    # Remove features with low variance \/ low signal-to-noise\n    X = X.drop(low_var_cols, axis=1)\n    T = T.drop(low_var_cols, axis=1)  \n\n    # FILTERING\n    # Normalize all columns but WCTM\n    scl = StandardScaler()\n    scl.fit( pd.concat([X, T]) )\n    X = pd.DataFrame(scl.transform(X),index=X.index, columns=X.columns)\n    T = pd.DataFrame(scl.transform(T),index=T.index, columns=T.columns)\n\n    # use generic column names, so the resulting dataframe is minimized\n    # X.columns = np.arange(X.shape[1])\n    # T.columns = np.arange(T.shape[1])\n    return y, X, T","03b95e23":"def iterative_filter_augment(y, X, T, o, p, iter_n): \n    # Remove samples with large distance between prediction and label\n    drop_samples = ((y-o).abs().sort_values()[:int(.01*y.shape[0])]).index\n    y = y.drop(drop_samples)\n    X = X.drop(drop_samples)\n    \n    # Add samples from Test set which have confident out-of-fold predictions\n    q = p.quantile(.9,axis=1)\n    \n    # get idx by threshold\n    # idx_new = q[(q>.95) | (q<.05)].index\n    # get idx by balanced %\n    idx_new = pd.concat([ q.sort_values()[:int(iter_n*.11*q.shape[0])] , q.sort_values(ascending=False)[:int(iter_n*.11*q.shape[0])] ]).index\n    \n    # idx_new = idx_new[~idx_new.isin(X.index)]\n    X = pd.concat([X, T.loc[idx_new]],axis=0).loc[~X.index.duplicated(keep='first')]\n    y = pd.concat([y, q.loc[idx_new].round().astype(int)],axis=0).loc[~y.index.duplicated(keep='first')]\n    return y, X, T","d58d6b85":"import matplotlib.pyplot as plt\ndef show_preds(p:pd.DataFrame):\n    fig = plt.figure(figsize = (20,5))\n    p.quantile(.1,axis=1).hist(bins=20,alpha=.3,color='green')\n    p.quantile(.9,axis=1).hist(bins=20,alpha=.3,color='red')\n    p.quantile(.5,axis=1).hist(bins=20,alpha=.3,color='yellow')\n    display(f'p: {p.shape}    (green=10th, yellow=50th, red=90th percentile)')\n    return","75e8a109":"from sklearn import metrics\ndef compare_oof_target(o:pd.Series):  \n    fig = plt.figure(figsize = (20,5))\n    train['target'].hist(bins=20,alpha=1,color='red')\n    o.hist(bins=20,alpha=.5,color='green')\n    display(f'oof: {o.shape}    train:{train.shape}    (red=original train target, green=oof predictions)')\n    \n    fig = plt.figure(figsize = (20,10))\n    fpr, tpr, thresholds = metrics.roc_curve(y, o)\n    auc = metrics.auc(fpr, tpr)\n\n    plt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc)\n    plt.legend()\n    plt.title('ROC curve')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.grid(True)","4128e69b":"y=train['target']\nX=train.drop('target',axis=1)\nT=test","5a28a037":"%%time\ny,X,T=update_dataset(y,X,T,iter_n=1)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","9040e097":"%%time\ny, X, T, o, p = fit_set(y,X,T,k=15, reg=.5)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","3bf03704":"show_preds(p)","fad9c91e":"compare_oof_target(o)","8b9b9e23":"%%time\ny,X,T=update_dataset(y,X,T,o,p,iter_n=2)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","bd0f12f2":"%%time\ny, X, T, o, p = fit_set(y,X,T,k=13,reg=.3)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","0b48d5c5":"show_preds(p)","0d4dbfbb":"compare_oof_target(o)","0b37aaa7":"%%time\ny,X,T=update_dataset(y,X,T,o,p,iter_n=3)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","e81cc618":"%%time\ny, X, T, o, p = fit_set(y,X,T,k=11,reg=.1)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","4793d67b":"show_preds(p)","a75bb4c6":"compare_oof_target(o)","74247db2":"%%time\ny,X,T=update_dataset(y,X,T,o,p,iter_n=4)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","e043ad8c":"%%time\ny, X, T, o, p = fit_set(y,X,T,k=11,reg=.03)\ndisplay(f'y:{str(y.shape)}    X:{str(X.shape)}    T:{T.shape}')","db71d068":"show_preds(p)","802cfc93":"compare_oof_target(o)","f2abab68":"preds=p.quantile(.5,axis=1).dropna()\npreds.name='target'\npreds.hist(bins=3)\npreds.shape","b58775bc":"sub = pd.read_csv(FILES['sample'],index_col='id')\nsub.update(preds)\nsub.to_csv('submission.csv',index=True)","fa735c80":"## Training Results","04430fb6":"# Score testing in a notebook made for tinkering\n\n1. [512 models](#512-Models), one for each value of 'wheezy-copper-turtle-magic' \n2. [Model specification](#Model-specification)\n    1. Classifier selection\n    2. k-Fold\n\n4. [Training Data Modification](#Training-Data-Modification)\n    1. [First-pass only Training Data Modification](#First-pass-Training-Data-Modification)\n        1. Add 1 synthetic feature composed from low variance features\n        2. Remove features with low variance \/ low signal-to-noise\n    2. [Iterative Data Modification](#Iterative-Data-Modification)\n        1. Remove samples from Training set with large distance between prediction and label\n        2. Add samples from Test set which have confident out-of-fold predictions\n\n5. [Training Results](#Training-Results)\n6. [Submission](#Submission)","b5c52013":"## 512 Models","e2983598":"## First-pass only Training Data Modification","c6df3660":"## Training Data Modification","1357e3ae":"## Iterative Training Data Modification","54947398":"## Model specification","44509e84":"## Submission"}}