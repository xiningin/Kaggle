{"cell_type":{"61f5f92f":"code","15835582":"code","460e8c2f":"code","01364939":"code","75fa16e2":"code","be8772ef":"code","cada583a":"code","518c1168":"code","8ba926e8":"code","4a9fa92a":"code","f1a94028":"code","3b5536aa":"code","8081e50b":"code","6d8b30c5":"code","72399a74":"code","28ec8e6b":"code","0972f41e":"code","108c1711":"code","82cdf58f":"code","d861a340":"code","c22c4a5c":"code","349a593e":"code","f866aa10":"code","3dd15864":"code","975f6b3d":"code","fa6d438b":"code","f5a9f4a2":"code","502cad06":"code","69cb0985":"code","df7de748":"code","28491884":"code","efef484a":"code","b4d481a7":"code","50c65d60":"code","1c6e87b2":"code","a2c23d3f":"code","fede49c0":"code","c5509b3a":"code","7e52232b":"code","17c720a1":"code","f2de1efe":"code","941da877":"code","605130d2":"code","cc17dbb6":"code","88e10bee":"code","aae21489":"code","bdbbe323":"code","81de9a6d":"code","bc005a43":"code","2d2ee802":"code","ea88790e":"code","52f08d1b":"code","da2a0a4a":"code","69f28fcf":"code","b24fe3b0":"code","bcb2e74a":"code","6cc8a421":"code","1a3c5b43":"code","1f9260b7":"code","4da2590c":"code","1275ea0f":"code","1ee3abc1":"code","dafe61df":"code","2059a569":"code","801da990":"code","accea128":"code","34cf1222":"markdown","8c14a39e":"markdown","72250570":"markdown","e4b3f368":"markdown","2392d2ce":"markdown","8126f00b":"markdown","324ea624":"markdown","c49ae01b":"markdown","b4f5f39d":"markdown","6e18fbe9":"markdown","f532283d":"markdown","470bea02":"markdown","ff54deb3":"markdown","dbbdca0b":"markdown","82210692":"markdown","6f55f835":"markdown","12c16176":"markdown","d42de486":"markdown","9d59aad0":"markdown","cbf4ece6":"markdown","aa7088ca":"markdown","a5677725":"markdown","cc17a8a5":"markdown","eb0294b5":"markdown","63826dd0":"markdown","27da67bb":"markdown","8e169b95":"markdown","bff95c67":"markdown","7fea4bf1":"markdown","1406aae0":"markdown"},"source":{"61f5f92f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.manifold import TSNE\nfrom sklearn.manifold import MDS\nfrom sklearn.manifold import Isomap\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder","15835582":"df = pd.read_csv ('..\/input\/horse-colic\/horse.csv')","460e8c2f":"dfTrain,dfTest = train_test_split(df, test_size=0.3, random_state=0)","01364939":"dfTrain.describe()","75fa16e2":"dfTrain.drop(columns = ['cp_data', 'hospital_number'], axis = 1, inplace = True)\ndfTest.drop(columns = ['cp_data', 'hospital_number'], axis = 1, inplace = True)","be8772ef":"#The features lesion1, 2 and 3 will give us trouble because the way they are formated doesn't make any sense. \n#Therefore, we will create a new feature that counts the number of lesions the horse has to replace them.\ndfTrain.loc[dfTrain['lesion_1'] > 0, 'lesion_1'] = 1\ndfTrain.loc[dfTrain['lesion_2'] > 0, 'lesion_2'] = 1\ndfTrain.loc[dfTrain['lesion_3'] > 0, 'lesion_3'] = 1\n\ndfTrain['num_lesions'] = dfTrain['lesion_1'] + dfTrain['lesion_2'] + dfTrain['lesion_3']\ndfTrain = dfTrain.drop(columns = ['lesion_1','lesion_2', 'lesion_3'], axis = 1)\ndfTrain['num_lesions'].value_counts()","cada583a":"dfTest.loc[dfTest['lesion_1'] > 0, 'lesion_1'] = 1\ndfTest.loc[dfTest['lesion_2'] > 0, 'lesion_2'] = 1\ndfTest.loc[dfTest['lesion_3'] > 0, 'lesion_3'] = 1\n\ndfTest['num_lesions'] = dfTest['lesion_1'] + dfTest['lesion_2'] + dfTest['lesion_3']\ndfTest = dfTest.drop(columns = ['lesion_1','lesion_2', 'lesion_3'], axis = 1)","518c1168":"def new_punctuation(df): #This manual ordinal encoder was done looking at the documentation to ensure they follow the correct scale (for ex: more pain -> bigger number)\n    df['surgery'] = df['surgery'].map({'yes':1,'no':2}).astype('float64')\n    df['age'] = df['age'].map({'adult':1,'young':2}).astype('float64')\n    df['temp_of_extremities'] = df['temp_of_extremities'].map({'normal':1,'warm':2,'cool':3,'cold':4}).astype('float64')\n    df['peripheral_pulse'] = df['peripheral_pulse'].map({'increased':2,'normal':1,'reduced':3,'absent':4}).astype('float64')\n    df['mucous_membrane'] = df['mucous_membrane'].map({'dark_cyanotic':6,'bright_red':5,'pale_cyanotic':4,'pale_pink':3,'bright_pink':2,'normal_pink':1}).astype('float64')\n    df['capillary_refill_time'] = df['capillary_refill_time'].map({'more_3_sec':3, '3':2, 'less_3_sec':1}).astype('float64')\n    df['pain'] = df['pain'].map({'extreme_pain':5, 'severe_pain':4, 'mild_pain':3, 'depressed':2, 'alert':1}).astype('float64')\n    df['peristalsis'] = df['peristalsis'].map({'absent':4, 'hypomotile':3, 'normal':2, 'hypermotile':1}).astype('float64')\n    df['abdominal_distention'] = df['abdominal_distention'].map({'severe':4,'moderate':3,'slight':2,'none':1}).astype('float64')\n    df['nasogastric_tube'] = df['nasogastric_tube'].map({'significant':3, 'slight':2, 'none':1}).astype('float64')\n    df['nasogastric_reflux'] = df['nasogastric_reflux'].map({'more_1_liter':3, 'less_1_liter':2, 'none':0}).astype('float64')\n    df['rectal_exam_feces'] = df['rectal_exam_feces'].map({'absent':4, 'decreased':3, 'increased':2, 'normal':1}).astype('float64')\n    df['abdomen'] = df['abdomen'].map({'distend_large':5, 'distend_small':4, 'firm':3, 'other':2, 'normal':1}).astype('float64')\n    df['abdomo_appearance'] = df['abdomo_appearance'].map({'serosanguious':3, 'cloudy':2, 'clear':1}).astype('float64')\n    df['outcome'] = df['outcome'].map({'euthanized':3, 'died':2, 'lived':1}).astype('float64')\n    df['surgical_lesion'] = df['surgical_lesion'].map({'yes':1,'no':0}).astype('float64')\n    return df","8ba926e8":"dfTrain = new_punctuation(dfTrain)\ndfTest = new_punctuation(dfTest)\ndfTrain.dtypes","4a9fa92a":"y_test = dfTest['outcome']\ndfTest.drop(columns = ['outcome'], axis = 1, inplace = True)","f1a94028":"plt.figure(figsize=(24,12))\nsns.heatmap(dfTrain.corr(),cmap='magma_r',annot=True)","3b5536aa":"fig,ax = plt.subplots(3,1,figsize=(15,15))\nsns.lineplot(x=dfTrain['rectal_temp'],y=dfTrain.outcome,ax=ax[0],color='r') #We visualize 3 features with different levels of correlation with outcome\nsns.lineplot(x=dfTrain['rectal_exam_feces'],y=dfTrain.outcome,ax=ax[1],color='b')\nsns.lineplot(x=dfTrain['peripheral_pulse'],y=dfTrain.outcome,ax=ax[2],color='g')","8081e50b":"chosen_cols = ['rectal_temp', 'pulse', 'respiratory_rate', 'packed_cell_volume', 'outcome']\nsns.pairplot(dfTrain[chosen_cols], hue='outcome', palette = 'viridis'); #We can see a clear correlation for example with high packed cell volume meaning no survival (the same with pulse not with resprate)","6d8b30c5":"sns.pairplot(dfTrain[chosen_cols], kind=\"kde\"); #This graph helps us show where most values are concentrated in some numerical features","72399a74":"sns.countplot(data=dfTrain, x='pain', hue = 'outcome') #We can see how the bigger the pain, the less the chance of survival","28ec8e6b":"sns.countplot(data=dfTrain, x='mucous_membrane', hue = 'outcome') #We can see how the worse the circulation-> less the chance of survival","0972f41e":"sns.countplot(data=dfTrain, x='capillary_refill_time', hue = 'outcome') #We can see how the worse the circulation-> less the chance of survival","108c1711":"sns.countplot(data=dfTrain, x='peristalsis', hue = 'outcome') #the lesser the activity on the horses gut the lesser the chance of survival","82cdf58f":"sns.countplot(data=dfTrain, x='abdominal_distention', hue = 'outcome') #the more distended the abdomen the lesser the chance of survival (it means more pain like the documentation says)","d861a340":"sns.countplot(data=dfTrain, x='age', hue = 'outcome') #surprisingly, the age of the horse doesn't have a big impact on the outcome of the surgery\n#however younger horses tend to survive less","c22c4a5c":"dfTrain.dtypes","349a593e":"dfTrain.isna().sum() #There are no columns that have null values in the test dataframe but not in the train dataframe","f866aa10":"#We start finding which columns to eliminate\nfor col in dfTrain.columns:\n  if dfTrain[col].isna().sum() > 120: #More than 50% missing values\n    print('Column ' + col + ' --> NULL VALUES: ' + str(dfTrain[col].isna().sum()) + ' --> Correlation with target of ' + str(dfTrain.corr()['outcome'][col]))","3dd15864":"#We drop columns nasogastric_reflux_ph and abdomo_protein as they don't hold a significant correlation with the target and they have > 50% of null values in the train dataset\n#Filling their null values would cause more harm than good\ndfTrain = dfTrain.drop(columns = ['abdomo_protein', 'nasogastric_reflux_ph'], axis = 1)\ndfTest = dfTest.drop(columns = ['abdomo_protein', 'nasogastric_reflux_ph'], axis = 1)","975f6b3d":"dfTrain.shape","fa6d438b":"plt.figure(figsize=(16,8))\nsns.heatmap(dfTrain.isnull(), cbar=False)","f5a9f4a2":"#We will use different approaches to fill the missing values on categorical and numerical variables","502cad06":"#We create vectors for the categorical and the numerical features that have missing values\ncat_features = ['surgery' , 'age', 'temp_of_extremities', 'peripheral_pulse',\n       'mucous_membrane', 'capillary_refill_time', 'pain', 'peristalsis',\n       'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', \n       'rectal_exam_feces', 'abdomen', 'abdomo_appearance']\n       \nnum_features = ['rectal_temp', 'pulse', 'respiratory_rate', 'packed_cell_volume', 'total_protein']","69cb0985":"dfTrain.dtypes","df7de748":"#We will use a KNNImputer to imput the categorical features\ndfTestOrig = dfTest\nKNNimpTR = KNNImputer(n_neighbors=1)\ndfTrain = pd.DataFrame(KNNimpTR.fit_transform(dfTrain),columns = dfTrain.columns)\nKNNimpTS = KNNImputer(n_neighbors=1)\ndfTest = pd.DataFrame(KNNimpTS.fit_transform(dfTest),columns = dfTest.columns)","28491884":"plt.figure(figsize=(16,8))\nsns.heatmap(dfTrain.isnull(), cbar=False) #The last row has null on all the categorical variables so there's no way to fill it with the KNN imputer","efef484a":"dfTrain.isna().sum() #No missing values remain!","b4d481a7":"dfTrain[num_features].boxplot()","50c65d60":"sns.boxplot(dfTrain['pulse'])","1c6e87b2":"sns.boxplot(dfTrain['respiratory_rate'])","a2c23d3f":"sns.boxplot(dfTrain['packed_cell_volume']) #We can ignore these small outliers","fede49c0":"Q1 = dfTrain['respiratory_rate'].quantile(0.15)\nQ3 = dfTrain['respiratory_rate'].quantile(0.85)\nIQR = Q3 - Q1\nbig_outliers = dfTrain['respiratory_rate'] > (Q3 + 1.5 * IQR)\ndfTrain[big_outliers] #We will drop these rows with outliers)?????????????????????????????????????????????????????????? -> not for now","c5509b3a":"dfTrain[big_outliers].index","7e52232b":"dfTrain.drop(labels=dfTrain[big_outliers].index, axis=0, inplace = True)","17c720a1":"Q1 = dfTrain['pulse'].quantile(0.25)\nQ3 = dfTrain['pulse'].quantile(0.75)\nIQR = Q3 - Q1\nbig_outliers_2 = dfTrain['pulse'] > (Q3 + 1.5 * IQR)\ndfTrain[big_outliers_2] #We will drop these rows with outliers??????????????????????????????????????????????????????????? -> Not for now","f2de1efe":"dfTrain.drop(labels=dfTrain[big_outliers_2].index, axis=0, inplace = True)","941da877":"#We also standardize the categorical variables despite it isn't necessary\ntarget = dfTrain['outcome']\ndfTrain = dfTrain.drop(columns = ['outcome'], axis = 1) #We extract the target to avoid standardizing it\nfeatures = dfTrain.columns\nscaler = StandardScaler()\nscaledTrain = scaler.fit_transform(dfTrain)\nscaledTest = scaler.transform(dfTest) #we transform the test set with the model trained on the train set\ndfTrain = pd.DataFrame(data=scaledTrain, columns=features)\ndfTest = pd.DataFrame(data=scaledTest, columns=features) #We rebuild to a dataframe format\"\"\"","605130d2":"dfTrain.hist(bins=22, figsize=(15, 15))","cc17dbb6":"print(dfTrain['pulse'].skew()) #We will fix variables with a skewness > 0.5\nprint(dfTrain['respiratory_rate'].skew())\nprint(dfTrain['total_protein'].skew())\nprint(dfTrain['packed_cell_volume'].skew())\nprint(dfTrain['rectal_temp'].skew())","88e10bee":"sns.distplot(dfTrain[\"pulse\"] , color = \"b\", hist_kws={\"alpha\": 0.4});","aae21489":"sns.distplot(dfTrain[\"respiratory_rate\"] , color = \"b\", hist_kws={\"alpha\": 0.4});","bdbbe323":"sns.distplot(dfTrain[\"total_protein\"] , color = \"b\", hist_kws={\"alpha\": 0.4});","81de9a6d":"sns.distplot(dfTrain[\"packed_cell_volume\"] , color = \"b\", hist_kws={\"alpha\": 0.4}); #This distribution looks less skewed (more gaussian)","bc005a43":"cols_not_normal = ['pulse', 'respiratory_rate', 'total_protein']\n\nPT = PowerTransformer()\n\nPTx_train = PT.fit_transform(dfTrain)\nPTx_test = PT.transform(dfTest)\n\ndfTrain2 = pd.DataFrame(data=PTx_train, columns=features)\ndfTest2 = pd.DataFrame(data=PTx_test, columns=features)\n\ndfTrain[cols_not_normal] = dfTrain2[cols_not_normal]\ndfTest[cols_not_normal] = dfTest2[cols_not_normal]\n\ndfTrain.insert(21,\"outcome\",target.values) #we add the target feature again","2d2ee802":"dfTrain.hist(bins=22, figsize=(15, 15))","ea88790e":"print(dfTrain['pulse'].skew())\nprint(dfTrain['respiratory_rate'].skew())\nprint(dfTrain['total_protein'].skew()) #Skewness fixed!","52f08d1b":"dfTrain.corr()['outcome']['total_protein'] #No correlation!","da2a0a4a":"dfTrain.drop(columns=['total_protein'], axis = 1, inplace = True)\ndfTest.drop(columns=['total_protein'], axis = 1, inplace = True)","69f28fcf":"featuresVIS = ['temp_of_extremities', 'peripheral_pulse',\n       'mucous_membrane', 'capillary_refill_time', 'pain', 'peristalsis',\n       'abdominal_distention', 'nasogastric_tube', 'nasogastric_reflux', \n       'rectal_exam_feces', 'abdomen', 'abdomo_appearance', 'rectal_temp', \n       'pulse', 'respiratory_rate', 'packed_cell_volume']\n\nx = dfTrain.loc[:,featuresVIS].values\n\ny = dfTrain.loc[:,'outcome'].values\n\nx = StandardScaler().fit_transform(x)\n\ndfaux = dfTrain.copy()\n\ncolors_data = {1: 'y', 2: 'b', 3: 'r'}","b24fe3b0":"pca = PCA().fit(x)\nplt.plot(np.cumsum(pca.explained_variance_ratio_)) # cumsum computes the cumulative sum\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","bcb2e74a":"pca = PCA(n_components = 10)  # Proyectar 22 dimensiones a 10\npca10 = pca.fit_transform(x)\n#print(dfTrain.shape)\n#print(pca10.shape)\n#print(pca.components_)\n#print(pca.explained_variance_)","6cc8a421":"pca = PCA(n_components = 2)  # Proyectar 22 dimensiones a 2\nprojected = pca.fit_transform(x)\n#print(dfTrain.shape)\n#print(pca10.shape)\n#print(pca.components_)\n#print(pca.explained_variance_)","1a3c5b43":"plt.scatter(projected[:, 0], projected[:, 1],\n            c = dfTrain['outcome'], edgecolor = 'none', alpha = 0.9,\n            cmap = plt.cm.get_cmap('viridis', 3))\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.colorbar()","1f9260b7":"y_train = dfTrain['outcome']\nx_train = dfTrain.drop(columns = ['outcome'], axis = 1)\nx_test = dfTest","4da2590c":"lr = LogisticRegression(random_state=0)\nparam_grid={\"C\":np.logspace(-3,3,10)}\ngrid = GridSearchCV(lr, param_grid, cv=5, verbose=0)\ngrid_search=grid.fit(x_train, y_train)\nprint('The best value found for the hyperparameter C is ' + str(grid_search.best_params_['C']))\nprint('The best result on the training set using 5-Fold CV was ' + str(grid_search.best_score_))\ny_pred = grid_search.predict(x_test)\nprint('The best result predicting the test set was ' + str(accuracy_score(y_test, y_pred)))\nconfusion_matrix(y_test, y_pred)","1275ea0f":"gnb = GaussianNB()\nparam_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\ngrid = GridSearchCV(gnb, param_grid, cv=5, verbose=0)\ngrid_search=grid.fit(x_train, y_train)\nprint('The best value found for the hyperparameter var_smoothing is ' + str(grid_search.best_params_['var_smoothing']))\nprint('The best result on the training set using 5-Fold CV was ' + str(grid_search.best_score_))\ny_pred = grid_search.predict(x_test)\nprint('The best result predicting the test set was ' + str(accuracy_score(y_test, y_pred)))\nconfusion_matrix(y_test, y_pred)","1ee3abc1":"knn = KNeighborsClassifier()\nk_range = list(range(1, 16))\nparam_grid = dict(n_neighbors=k_range)\ngrid = GridSearchCV(knn, param_grid, cv=5, verbose=0)\ngrid_search=grid.fit(x_train, y_train)\nprint('The best value found for the hyperparameter number of neighbors is ' + str(grid_search.best_params_['n_neighbors']))\nprint('The best result on the training set using 5-Fold CV was ' + str(grid_search.best_score_))\ny_pred = grid_search.predict(x_test)\nprint('The best result predicting the test set was ' + str(accuracy_score(y_test, y_pred)))\nconfusion_matrix(y_test, y_pred)","dafe61df":"rf = RandomForestClassifier(n_jobs=-1, random_state= 0)\nparam_grid = { \n    'n_estimators': [100,200,300,400],\n    'max_depth' : [2,3,4,6,8]\n}\ngrid = GridSearchCV(rf, param_grid, cv=5, verbose=0)\ngrid_search=grid.fit(x_train, y_train)\n#print('The best value found for the hyperparameter var_smoothing is ' + str(grid_search.best_params_['var_smoothing']))\nprint('The best result on the training set using 5-Fold CV was ' + str(grid_search.best_score_))\ny_pred = grid_search.predict(x_test)\nprint('The best result predicting the test set was ' + str(accuracy_score(y_test, y_pred)))\nconfusion_matrix(y_test, y_pred)","2059a569":"grid_search.best_params_","801da990":"mlpc = MLPClassifier(random_state=0, verbose = 0)\nparam_grid = [\n    {'hidden_layer_sizes': [(100,100,100,100),(100,100,100,100,100),(100,100,100,100,100,100), (100,100,100,100,100,100,100)],\n     'alpha': [0.0001,0.001,0.01,0.1],\n     'early_stopping': [True],\n     'learning_rate_init' : [0.001,0.01,0.1]\n     }\n]\ngrid = GridSearchCV(mlpc, param_grid, cv=5, verbose=0)\ngrid_search=grid.fit(x_train, y_train)\nprint('The best result on the training set using 5-Fold CV was ' + str(grid_search.best_score_))\ny_pred = grid_search.predict(x_test)\nprint('The best result predicting the test set was ' + str(accuracy_score(y_test, y_pred)))\nconfusion_matrix(y_test, y_pred)","accea128":"grid_search.best_params_","34cf1222":"**LINEAR MODELS**","8c14a39e":"\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34","72250570":"**MLP Classifier**","e4b3f368":"We will delete the outliers","2392d2ce":"**NON-LINEAR MODELS**","8126f00b":"# **FILLING MISSING VALUES**","324ea624":"**PCA DOESN`T REALLY HELP US ON THIS DATASET SINCE WE CAN'T EXPLAIN MOST OF THE VARIANCE WITH A SMALLER NUMBER OF DIMENSIONS NOR OBSERVE CLEAR DIFFERENT GROUPS OF HORSES**","c49ae01b":"Since total_protein doesn't improve its skewness (its distribution is totally abnormal) and its correlation with the target is inexistent, we decide to eliminate it.","b4f5f39d":"![caballo.gif](attachment:01881a5e-29b9-4c58-a664-91be5f6c0986.gif)","6e18fbe9":"# **PCA??**","f532283d":"# **FEATURE ENGINEERING**","470bea02":"# **Let's start!**","ff54deb3":"# **THE END!**","dbbdca0b":"# **LOADING THE DATAFRAME**","82210692":"Now lets take a glance at the correlation between features to decide how we will fill the missing values or nulls.","6f55f835":"# **MODEL SELECTION**","12c16176":"# **OUTLIER DETECTION (and treatment?)**","d42de486":"**KNN Classifier**","9d59aad0":"**Thanks for reading this kernel!**","cbf4ece6":"**PREPROCESSING IS DONE!** \ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34\ud83d\udc34","aa7088ca":"**Logistic Regression**","a5677725":"**Naive Bayes**","cc17a8a5":"**PCA**","eb0294b5":"# **NORMALIZATION OF THE VARIABLES**","63826dd0":"# **TRANSFORMATION OF VARIABLES**","27da67bb":"BEFORE WE START FILLING MISSING VALUES, LET'S DECIDE WHICH FEATURES ARE REDUNDANT OR HAVE TO BE ELIMINATED","8e169b95":"**Random Forest Classifier** - OUR BEST MODEL!","bff95c67":"We will be looking to predict if a horse can survive a surgery. This is a full project including preprocessing + EDA + model selection and hyperparameter tuning among others.","7fea4bf1":"# **DATA VISUALIZATION**","1406aae0":"LETS FIRST FILL THE MISSING VALUES OF THE CATEGORICAL FEATURES"}}