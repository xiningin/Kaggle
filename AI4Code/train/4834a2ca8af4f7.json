{"cell_type":{"dac19971":"code","475e9bc0":"code","53bbf023":"code","1d5a0829":"code","2e77dcac":"code","f0829f85":"code","1cfe854d":"code","dc140192":"code","6b903f3e":"code","1f67005c":"code","d31440b5":"code","40bf0cc7":"code","0086add6":"code","51279a87":"code","9f7dfc33":"code","30430402":"code","3010c0cc":"markdown","98d91d63":"markdown","07467548":"markdown","0aefee89":"markdown"},"source":{"dac19971":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import mode, skew, kurtosis, entropy\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas(tqdm_notebook)\n\n# Any results you write to the current directory are saved as output.","475e9bc0":"train = pd.read_csv(\"..\/input\/santander-value-prediction-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/santander-value-prediction-challenge\/test.csv\")\n\ntransact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\ny = np.log1p(train[\"target\"]).values","53bbf023":"test[\"target\"] = train[\"target\"].mean()","1d5a0829":"all_df = pd.concat([train, test]).reset_index(drop=True)\nall_df.columns = all_df.columns.astype(str)\nprint(all_df.shape)","2e77dcac":"cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n        '6619d81fc', '1db387535', \n        'fc99f9426', '91f701ba2', '0572565c2', '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'\n       ]","f0829f85":"pattern_1964666 = pd.read_csv('..\/input\/pattern-found\/pattern_1964666.66.csv')\npattern_1166666 = pd.read_csv('..\/input\/pattern-found\/pattern_1166666.66.csv')\npattern_812666 = pd.read_csv('..\/input\/pattern-found\/pattern_812666.66.csv')\npattern_2002166 = pd.read_csv('..\/input\/pattern-found\/pattern_2002166.66.csv')\npattern_3160000 = pd.read_csv('..\/input\/pattern-found\/pattern_3160000.csv')\npattern_3255483 = pd.read_csv('..\/input\/pattern-found\/pattern_3255483.88.csv')","1cfe854d":"pattern_1964666.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_1166666.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_812666.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_2002166.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_3160000.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)\npattern_3255483.drop(['Unnamed: 0','value_count'],axis=1,inplace=True)","dc140192":"pattern_1166666.rename(columns={'8.50E+43': '850027e38'},inplace=True)","6b903f3e":"l=[]\nl.append(pattern_1964666.columns.values.tolist())\nl.append(pattern_1166666.columns.values.tolist())\nl.append(pattern_812666.columns.values.tolist())\nl.append(pattern_2002166.columns.values.tolist())\nl.append(pattern_3160000.columns.values.tolist())\nl.append(pattern_3255483.columns.values.tolist())","1f67005c":"def _get_leak(df, cols,extra_feats, lag=0):\n    f1 = cols[:((lag+2) * -1)]\n    f2 = cols[(lag+2):]\n    for ef in extra_feats:\n        f1 += ef[:((lag+2) * -1)]\n        f2 += ef[(lag+2):]\n    \n    d1 = df[f1].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d1.to_csv('extra_d1.csv')\n    d2 = df[f2].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'}) \n\n    d2['pred'] = df[cols[lag]]\n#     d2.to_csv('extra_d2.csv')\n    #d2 = d2[d2.pred != 0] ### to make output consistent with Hasan's function\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    d4 = d1[~d1.duplicated(['key'], keep=False)]\n    d5 = d4.merge(d3, how='inner', on='key')\n    \n    d6 = d1.merge(d5, how='left', on='key')\n    d6.to_csv('extra_d6.csv')\n    \n    return d1.merge(d5, how='left', on='key').pred.fillna(0)","d31440b5":"def compiled_leak_result():\n    \n    max_nlags = len(cols)-2\n    train_leak = train[[\"ID\", \"target\"] + cols]\n    train_leak[\"compiled_leak\"] = 0\n    train_leak[\"nonzero_mean\"] = train[transact_cols].apply(\n        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n    )\n    scores = []\n    leaky_value_counts = []\n    leaky_value_corrects = []\n    leaky_cols = []\n    \n    for i in range(max_nlags):\n        c = \"leaked_target_\"+str(i)\n        print('Processing lag', i)\n        train_leak[c] = _get_leak(train, cols,l, i)\n        \n        leaky_cols.append(c)\n        train_leak = train.join(\n            train_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n            on=\"ID\", how=\"left\"\n        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n        zeroleak = train_leak[\"compiled_leak\"]==0\n        train_leak.loc[zeroleak, \"compiled_leak\"] = train_leak.loc[zeroleak, c]\n        leaky_value_counts.append(sum(train_leak[\"compiled_leak\"] > 0))\n        _correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n        leaky_value_corrects.append(_correct_counts*1.0\/leaky_value_counts[-1])\n        print(\"Leak values found in train\", leaky_value_counts[-1])\n        print(\n            \"% of correct leaks values in train \", \n            leaky_value_corrects[-1]\n        )\n        tmp = train_leak.copy()\n        tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n        print('Na count',tmp.compiled_leak.isna().sum())\n        scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n        print(\n            'Score (filled with nonzero mean)', \n            scores[-1]\n        )\n    result = dict(\n        score=scores, \n        leaky_count=leaky_value_counts,\n        leaky_correct=leaky_value_corrects,\n    )\n    return train_leak, result","40bf0cc7":"train_leak, result = compiled_leak_result()","0086add6":"result = pd.DataFrame.from_dict(result, orient='columns')\nresult.T","51279a87":"result.to_csv('train_leaky_stat.csv', index=False)","9f7dfc33":"train_leak.head()","30430402":"best_score = np.min(result['score'])\nbest_lag = np.argmin(result['score'])\nprint('best_score', best_score, '\\nbest_lag', best_lag)","3010c0cc":"We take time series columns from [here](https:\/\/www.kaggle.com\/johnfarrell\/giba-s-property-extended-result)","98d91d63":"Updated this Kernel by Jiazhen Xi with some of the new patterns I found. Using 6 of the 40 those patterns. Hope this helps everyone and provide them with some sort of help in this dying stage of the competition.","07467548":"> Please go through Giba's post and kernel  to underrstand what this leak is all about\n> https:\/\/www.kaggle.com\/titericz\/the-property-by-giba (kernel)\n> https:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/61329 (post)\n> \n> Also, go through this Jiazhen's kernel which finds more columns to exploit leak\n> https:\/\/www.kaggle.com\/johnfarrell\/giba-s-property-extended-result\n> \n> I just exploit data property in brute force way and then fill in remaining by row non zero means! This should bring everyone on level-playing field.\n> \n> **Let the competition begin! :D**\n\n> ### Just some small modifications from [original baseline](https:\/\/www.kaggle.com\/tezdhar\/breaking-lb-fresh-start)~\n>- The leak rows are calculated separately on train\/test set\n>- Calculated the leaky values, correctness, for each lag\n>- Hope this can help to do some *lag_selection*\n\n>### Update leak process codes to Dmitry Frumkin's *fast* [version](https:\/\/www.kaggle.com\/dfrumkin\/a-simple-way-to-use-giba-s-features-v2)\n>- The result of Dmitry's original function and result of Hasan's function seem slightly different\n>- Modified to make the output consistent with Hasan's function (Seems better score)","0aefee89":"Updating this function on the basis of the hint provided by Paradox [here](http:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/61472#363394)."}}