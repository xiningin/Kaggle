{"cell_type":{"ab5e6cd5":"code","3f3d4f5f":"code","a6262f51":"code","61948309":"code","a4f9d2f9":"code","e0a47f3e":"code","de71de8e":"code","4fa51d3c":"code","8fa747f8":"code","844c9821":"code","202b871c":"code","2eeb3054":"code","c28175b3":"code","25575584":"code","93d2f2bd":"code","817a3384":"code","5b139095":"code","18e2c90b":"code","5c336d12":"code","be426aea":"code","d13ccfd8":"code","bbd24dfe":"code","b5c32295":"code","af14a433":"code","fe6010cc":"code","7898e5ab":"code","bcf55e37":"code","7a7c0573":"markdown","6e2a494d":"markdown","647b8d3a":"markdown","2579b5e1":"markdown"},"source":{"ab5e6cd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f3d4f5f":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_concat = pd.concat([df_train, df_test], axis = 0).reset_index(drop = True)","a6262f51":"print('Train size : ', len(df_train))\nprint('Test size : ', len(df_test))\nprint('Full size : ', len(df_concat))","61948309":"df_train.head()","a4f9d2f9":"# Detecting null values and records\n\nnulls = pd.DataFrame(np.c_[df_concat.isnull().sum(), (df_concat.isnull().sum()\/ len(df_concat))*100],\n                     columns = ['# of nulls', '% of nulls'],\n                     index = df_concat.columns)\nnulls","e0a47f3e":"# Dropping keyword and location\n\nfor df in [df_train, df_test, df_concat]:\n    df.drop(columns = ['location', 'keyword', 'id'], inplace = True)","de71de8e":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport string\n\nnltk.download('stopwords')\nnltk_stopwords = stopwords.words('english')\n\nporter = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef stemSentence(sentence):\n    token_words = word_tokenize(sentence)\n    stem_sentence = []\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\ndef lemSentence(sentence):\n    token_words = word_tokenize(sentence)\n    lem_sentence = []\n    for word in token_words:\n        lem_sentence.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n        lem_sentence.append(\" \")\n    return \"\".join(lem_sentence)","4fa51d3c":"# Example of Stemming\nprint(porter.stem(\"studies\"))\nprint(porter.stem(\"studying\"))","8fa747f8":"# Example of Lemmatization\nprint(wordnet_lemmatizer.lemmatize('studies', pos=\"v\"))\nprint(wordnet_lemmatizer.lemmatize('studying', pos=\"v\"))","844c9821":"# Creating clean function\n\ndef clean(message, lem=True):\n    # Remove ponctuation\n    message = message.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove numbers\n    message = message.translate(str.maketrans('', '', string.digits))\n    \n    # Remove stop words\n    message = [word for word in word_tokenize(message) if not word.lower() in nltk_stopwords]\n    message = ' '.join(message)\n    \n    # Lemmatization (root of the word)\n    if lem:\n        message = lemSentence(message)\n    \n    return message","202b871c":"print(clean(\"Hello my name is Cedric and I'm 24 years old. I love red cars. I'm better then Hamilton.\", False))\nprint(clean(\"Hello my name is Cedric and I'm 24 years old. I love red cars. I'm better then Hamilton.\", False))\nprint(clean(\"Hello my name is Cedric and I'm 24 years old. I love red cars. I'm better then Hamilton.\", True))","2eeb3054":"df_concat['cleaned_text'] = df_concat.text.apply(lambda x: clean(x, True))","c28175b3":"df_concat.head()","25575584":"df_train_cleaned = df_concat[:df_train.shape[0]]\ndf_test_cleaned = df_concat[df_train.shape[0]:]","93d2f2bd":"BERT_MODEL_HUB = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2'\ndisc = 'Base_uncased'","817a3384":"# Bert Tokenizer\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization\nFullTokenizer = tokenization.FullTokenizer","5b139095":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nbert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)\nprint('Bert layer is ready to use!')","18e2c90b":"to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\ntokenizer = FullTokenizer(vocabulary_file, to_lower_case)","5c336d12":"s = \"Hello my name is Cedric and I love red cars\"\nprint('Tokenized version of {} is : \\n {} '.format(s, tokenizer.tokenize(s)))","be426aea":"df_concat.head()","d13ccfd8":"def tokenize_messages(message):\n    return tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(message) + ['[SEP]'])\n\ndf_concat['tokenized_messages'] = df_concat.cleaned_text.apply(lambda x: tokenize_messages(x))\n\ndf_concat.head(2)","bbd24dfe":"# Max length of message\nmax_len = len(max(df_concat.tokenized_messages, key = len))\nprint(\"Max len is :\", max_len)","b5c32295":"from tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.optimizers import SGD\n\nclass MessageClassifier:\n    \n    def __init__(self, tokenizer, bert_layer, max_len, lr = 0.0001,\n                 epochs = 15, batch_size = 32,\n                 activation = 'sigmoid', optimizer = 'SGD',\n                 beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07,\n                 metrics = 'accuracy', loss = 'binary_crossentropy'):  \n        \n        self.tokenizer = tokenizer\n        self.bert_layer = bert_layer\n        self.max_len = max_len\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.activation = activation\n        self.optimizer = optimizer       \n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.metrics = metrics\n        self.loss = loss\n        \n    \n    def encode(self, texts):\n        \n        all_tokens = []\n        masks = []\n        segments = []\n        \n        for text in texts:\n            tokenized = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + self.tokenizer.tokenize(text) + ['[SEP]'])\n            len_zeros = self.max_len - len(tokenized)\n            \n            padded = tokenized + [0] * len_zeros\n            mask = [1] * len(tokenized) + [0] * len_zeros\n            segment = [0] * self.max_len\n            \n            all_tokens.append(padded)\n            masks.append(mask)\n            segments.append(segment)\n        \n        return np.array(all_tokens), np.array(masks), np.array(segments)\n    \n        \n    def make_model(self):\n        \n        # Shaping the inputs to our model\n        input_ids = Input(shape = (self.max_len, ), dtype = tf.int32, name = 'input_ids')\n        input_mask = Input(shape = (self.max_len, ), dtype = tf.int32, name = 'input_mask')\n        segment_ids = Input(shape = (self.max_len, ), dtype = tf.int32,  name = 'segment_ids')\n\n        pooled_output, sequence_output = bert_layer([input_ids, input_mask, segment_ids] )\n\n        clf_output = sequence_output[:, 0, :]\n        \n        out = tf.keras.layers.Dense(1, activation = self.activation)(clf_output)\n        \n        model = Model(inputs = [input_ids, input_mask, segment_ids], outputs = out)\n        \n        optimizer = SGD(learning_rate = self.lr)\n\n        model.compile(loss = self.loss, optimizer = self.optimizer, metrics = [self.metrics])\n        \n        print('Model is compiled with {} optimizer'.format(self.optimizer))\n        \n        return model\n       \n    \n    def train(self, x):    \n        \n        checkpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', save_best_only = True)\n            \n        model = self.make_model()\n        \n        X = self.encode(x['cleaned_text'])\n        Y = x['target']\n        \n        model.fit(X, Y, shuffle = True, validation_split = 0.2, \n                  batch_size=self.batch_size, epochs = self.epochs,\n                  callbacks=[checkpoint])\n                \n        print('Model is fit!')\n        \n        \n    def predict(self, x):\n        \n        X_test_encoded = self.encode(x['cleaned_text'])\n        model = tf.keras.models.load_model('model.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n        y_pred = model.predict(X_test_encoded)\n        \n        return y_pred","af14a433":"classifier = MessageClassifier(tokenizer = tokenizer, bert_layer = bert_layer,\n                              max_len = max_len, lr = 0.0001,\n                              epochs = 1, activation = 'sigmoid',\n                              batch_size = 32, optimizer = 'SGD',\n                              beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07)","fe6010cc":"classifier.train(df_train_cleaned)","7898e5ab":"y_pred = np.round(classifier.predict(df_test_cleaned))","bcf55e37":"sample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nids = sample_sub.id\nfinal_submission = pd.DataFrame(np.c_[ids, y_pred.astype('int')], columns = ['id', 'target'])\nfinal_submission.to_csv('final_submission_v5.0.csv', index = False)\nfinal_submission.head()","7a7c0573":"# BERT","6e2a494d":"# Data cleaning","647b8d3a":"# Creating the model","2579b5e1":"# Ideas to make this classifier better\n\n- Check that every comment is in english --> Otherwise user translator \/ different stimming process\n- Using Part of Speech to make the algorithm what is the role of each word"}}