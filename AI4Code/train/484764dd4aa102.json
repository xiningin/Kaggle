{"cell_type":{"4845c777":"code","df22811d":"code","a06d06a4":"code","2925e72d":"code","6afe9c22":"code","1f912775":"code","fcb54d9e":"code","c05a77d3":"code","3bdf879f":"code","e3787828":"code","83d47ef3":"code","cb3d31a1":"code","72721572":"code","56178f97":"code","9424b59a":"code","825bbdb6":"code","5cf7094a":"code","aba22aeb":"code","7c6e858c":"code","fa1a3bbc":"code","0fcce9e8":"code","4d5605ea":"code","f0b3bc79":"code","e930bb14":"code","5712ffe0":"code","7826726b":"code","7226fcb1":"code","fa65dcf9":"code","d7507172":"code","74e5636c":"code","a9c58f0b":"code","902367ff":"code","40595525":"code","49880c56":"code","1fcad5b0":"code","5948368a":"code","93bb4258":"code","6b6b5b3d":"code","d4fffb2b":"code","78eb0d7f":"code","ad1f3048":"code","6f4d1a3c":"code","7ac3d7f3":"code","30913dc4":"code","4eae3d2b":"code","9285da8b":"code","6613d4f6":"code","a4d2b4e5":"code","a172a27d":"code","e29397ca":"markdown","c2168072":"markdown","900bff5e":"markdown","963f31d3":"markdown","f2915000":"markdown","e24d57ac":"markdown","2c8825b4":"markdown","410c2f1f":"markdown","647d97c2":"markdown","6b62736c":"markdown","6dbf1a6d":"markdown","b18b4fd9":"markdown","bf5f0c11":"markdown","d76fcd4b":"markdown","3c3f3829":"markdown","b71f27dc":"markdown","710d4a99":"markdown"},"source":{"4845c777":"#REQUIRED LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,Lasso\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.utils.extmath import softmax\n# import pickle\n# from sklearn.externals import joblib\n\n# import pandas_profiling as pp\n\nwarnings.filterwarnings('ignore')\ngc.enable()\n%matplotlib inline","df22811d":"#CHECKING ALL AVAILABLE FILES\npath='..\/input\/tabular-playground-series-dec-2021\/'\ndata_files=list(os.listdir(path))\ndf_files=pd.DataFrame(data_files,columns=['file_name'])\ndf_files['size_in_mb']=df_files.file_name.apply(lambda x: round(os.path.getsize(path+x)\/(1024*1024),4))\ndf_files['type']=df_files.file_name.apply(lambda x:'file' if os.path.isfile(path+x) else 'directory')\ndf_files['file_count']=df_files[['file_name','type']].apply(lambda x: 0 if x['type']=='file' else len(os.listdir(path+x['file_name'])),axis=1)\n\nprint('Following files are available under path:',path)\ndisplay(df_files)","a06d06a4":"#ALL CUSTOM FUNCTIONS\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n#     print('DataFrame shape')\n#     print('rows:',df_fa.shape[0])\n#     print('cols:',df_fa.shape[1])\n    col_list=['null','unique_count','data_#type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.at[col,'mean']=round(df_fa[col].mean(),4)\n            df.at[col,'median']=round(df_fa[col].median(),4)\n            df.at[col,'mode']=round(df_fa[col].mode()[0],4)\n            df.at[col,'std']=round(df_fa[col].std(),4)\n            df.at[col,'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.at[col,'sample_values']=list(df_fa[col].unique())\n#     display(df_fa.head())      \n    return(df.fillna('-'))\n\n\ndef feature_compare(df_fa,df_ft):\n    print('Train DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    \n    print('Test DataFrame shape')\n    print('rows:',df_ft.shape[0])\n    print('cols:',df_ft.shape[1])\n    \n    col_list=['null','unique_count','data_type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=pd.MultiIndex.from_product([df_train.columns,['train','test']],names=['features','dataset']),columns=col_list)\n   \n    df.loc[(slice(None),['train']),'null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'null']=list([len(df_ft[col][df_ft[col].isnull()]) for i,col in enumerate(df_ft.columns)])+['-']\n    \n    \n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['train']),'unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'unique_count']=list([len(df_ft[col].unique()) for i,col in enumerate(df_ft.columns)])+['-']\n    \n    df.loc[(slice(None),['train']),'data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'data_type']=list([df_ft[col].dtype for i,col in enumerate(df_ft.columns)])+['-']\n    \n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.loc[([col],['train']),'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.loc[([col],['train']),'mean']=round(df_fa[col].mean(),4)\n            df.loc[([col],['train']),'median']=round(df_fa[col].median(),4)\n            df.loc[([col],['train']),'mode']=round(df_fa[col].mode()[0],4)\n            df.loc[([col],['train']),'std']=round(df_fa[col].std(),4)\n            df.loc[([col],['train']),'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.loc[([col],['train']),'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.loc[([col],['train']),'sample_values']=str(list(df_fa[col].unique()))\n        \n        \n    for i,col in enumerate(df_ft.columns):            \n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.loc[([col],['test']),'max\/min']=str(round(df_ft[col].max(),2))+'\/'+str(round(df_ft[col].min(),2))\n            df.loc[([col],['test']),'mean']=round(df_ft[col].mean(),4)\n            df.loc[([col],['test']),'median']=round(df_ft[col].median(),4)\n            df.loc[([col],['test']),'mode']=round(df_ft[col].mode()[0],4)\n            df.loc[([col],['test']),'std']=round(df_ft[col].std(),4)\n            df.loc[([col],['test']),'skewness']=round(df_ft[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.loc[([col],['test']),'max\/min']=str(df_ft[col].max())+'\/'+str(df_ft[col].min())\n        df.loc[([col],['test']),'sample_values']=str(list(df_ft[col].unique()))\n        \n    return(df.fillna('-'))\n\n#EXTENDING RIDGE CLASSIFIER WITH PREDICT PROBABILITY FUNCITON\n\nclass RidgeClassifierwithProba(RidgeClassifier):\n    def predict_proba(self, X):\n        d = self.decision_function(X)\n        d_2d = np.c_[-d, d]\n        return softmax(d_2d)\n\n    \n#PREDICTION FUNCTIONS\n\ndef type_predictor(X,y,test,iterations,model,model_name):  \n\n    df_preds=pd.DataFrame()\n    df_preds_x=pd.DataFrame()\n    k=1\n    splits=iterations\n    avg_score=0\n\n    #CREATING STRATIFIED FOLDS\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n    print('\\nStarting KFold iterations...')\n    for train_index,test_index in skf.split(X,y):\n        df_X=X[train_index,:]\n        df_y=y[train_index]\n        val_X=X[test_index,:]\n        val_y=y[test_index]\n       \n\n    #FITTING MODEL\n        model.fit(df_X,df_y)\n\n    #PREDICTING ON VALIDATION DATA\n        col_name=model_name+'xpreds_'+str(k)\n        preds_x=pd.Series(model.predict(val_X))\n        df_preds_x[col_name]=pd.Series(model.predict(X))\n\n    #CALCULATING ACCURACY\n        acc=accuracy_score(val_y,preds_x)\n        print('Iteration:',k,'  accuracy_score:',acc)\n        if k==1:\n            score=acc\n            best_model=model\n            preds=pd.Series(model.predict(test))\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds\n        else:\n            preds1=pd.Series(model.predict(test))\n            preds=preds+preds1\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds1\n            if score<acc:\n                score=acc\n                best_model=model\n        avg_score=avg_score+acc        \n        k=k+1\n    print('\\n Best score:',score,' Avg Score:',avg_score\/splits)\n    #TAKING AVERAGE OF PREDICTIONS\n    preds=preds\/splits\n    \n    print('Saving test and train predictions per iteration...')\n    df_preds.to_csv(model_name+'.csv',index=False)\n    df_preds_x.to_csv(model_name+'_.csv',index=False)\n    x_preds=df_preds_x.mean(axis=1)\n    del df_preds,df_preds_x\n    gc.collect()\n    return preds,best_model,x_preds ","2925e72d":"%%time\n#READING TRAIN DATASET\n\ndf_train=pd.read_csv(path+'train.csv')\n\n#READING TEST DATASET AND SUBMISSION FILE\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_submission.csv')","6afe9c22":"%%time\n#UNDERSTANDING TRAIN AND TEST DATASET USING FEATURE BY FEATURE COMPRISON\npd.set_option('display.max_rows', None)\nfeature_compare(df_train,df_test)","1f912775":"gc.collect()","fcb54d9e":"%%time\n#CHECKING TRAIN AND TEST DATASET MEMORY USAGE BEFORE DOWNCASTING\nprint('\\ntrain dataset data usage information before downcasting\\n')\ndf_train.info(memory_usage='deep',max_cols=1)\nprint('\\ntest dataset data usage information before downcasting\\n')\ndf_test.info(memory_usage='deep',max_cols=1)\n\n\n#DOWNCASTING TRAIN DATASET\nfor column in df_train.columns:\n    if df_train[column].dtype == \"float64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"float\")\n    if df_train[column].dtype == \"int64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"integer\")\n        \n#DOWNCASTING TEST DATASET\nfor column in df_test.columns:\n    if df_test[column].dtype == \"float64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"float\")\n    if df_test[column].dtype == \"int64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"integer\")\n        \n#CHECKING TRAIN AND TEST DATASET MEMORY USAGE AFTER DOWNCASTING\nprint('train dataset data usage information after downcasting\\n')\ndf_train.info(memory_usage='deep',max_cols=1)\nprint('\\ntest dataset data usage information after downcasting\\n')\ndf_test.info(memory_usage='deep',max_cols=1)","c05a77d3":"%%time\n#CREATING A FEATURE LIST EXCLUDING ID AND TARGET\nfeatures=[col for col in df_train.columns if col!='Id' and col!='Cover_Type' and col!='Soil_Type7' and col!='Soil_Type15']\ndf_fs=feature_summary(df_train[features])\nbinary_features=[col for i,col in enumerate(df_fs.index) if df_fs.iloc[i,1]==2]\nnonbinary_features=[col for i,col in enumerate(df_fs.index) if df_fs.iloc[i,1]!=2]\n\nprint('Total features excluding ID and Cover_Type(target feature):',len(features))\nprint('Total binary features:',len(binary_features))\nprint('Total Non binary features:',len(nonbinary_features))","3bdf879f":"%%time\n#VISUALIZING TRAIN AND TEST FEATURE DISTRIBUTION\nplt.figure()\nfig, ax = plt.subplots(4,3 ,figsize=(20,20))\n\nfor i,feature in enumerate(nonbinary_features):\n    plt.subplot(4, 3,i+1)\n    sns.kdeplot(data=df_train[feature],x=df_train[feature],color='red', label='train')\n    plt.axvline(x=df_train[feature].mean(),color='yellow',linestyle='--',label='train mean')\n    sns.kdeplot(df_test[feature],x=df_test[feature],color='grey',label='test')\n    plt.axvline(x=df_test[feature].mean(),color='orange',linestyle='--',label='test mean')\n    plt.xlabel(feature,color='blue')\n\n    plt.legend(loc=1,fontsize='x-small')\n    \n    \nplt.show();","e3787828":"gc.collect()","83d47ef3":"#Cover types description from original Forest Cover Type Prediction competition\ndata={1:'Spruce\/Fir',2:'Lodgepole Pine',3:'Ponderosa Pine',4:'Cottonwood\/Willow',5:'Aspen',6:'Douglas-fir',7:'Krummholz'}\ndf_cover_type=pd.DataFrame(list(data.items()),columns=['Cover_Type','Cover_Description'])\n\n\ndf=df_train[['Id','Cover_Type']].groupby('Cover_Type').count().reset_index().sort_values(by='Cover_Type')\ndf.columns=['Cover_Type','Observation Count']\ndf=df.merge(df_cover_type,on='Cover_Type',how='left')\ndf['Cover_Type_Desc']=[str(df.loc[i,'Cover_Type'])+'-'+df.loc[i,'Cover_Description'] for i in df.index]\n#CREATING VISUALIZATION\nfig=px.pie(df,names='Cover_Type_Desc',values='Observation Count',color='Cover_Type_Desc',hole=0.6,\n           color_discrete_sequence=px.colors.qualitative.Plotly,width=700,height=700)\n           \n\nfig.update_layout(\n                    {'paper_bgcolor':'#FEFBF3'},\n                    title={\n                        'text': \"Cover_Type Distribution\",\n                        'x':0.5,\n                        'font_color':\"red\"},\n    \n                    font_color=\"blue\",\n                    legend_title=\"Cover_Type\",\n                    legend_title_font_color=\"green\",\n                    legend=dict(\n                                yanchor=\"top\",\n                                y=0.99,\n                                xanchor=\"right\",\n                                x=0.05\n                                )\n\n                )\n\n\nfig.show()","cb3d31a1":"%%time\n#CORRELATION CHECK CATEGORICAL FEATURES\ncorr = df_train[features+['Cover_Type']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\n# plt.rcParams.update({'font.size': 12})\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.1)\nplt.title('CORRELATION MAP',color='blue',fontsize=12)\nplt.show()","72721572":"del corr\ngc.collect()","56178f97":"%%time\nX=df_train[features]\n\npca = PCA(n_components=2,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2'])\nprincipalDf['Cover_Type']=df_train['Cover_Type']\n\nfig = plt.figure(figsize=(15,15))\nsc=plt.scatter(x=principalDf['principal_component_1'], y=principalDf['principal_component_2'],c=principalDf['Cover_Type'],cmap='Accent')\nplt.legend(*sc.legend_elements(),bbox_to_anchor=(1.05, 1), loc=2)\nplt.title('2D Visualization of train Dataset',color='blue',fontsize=12)\nplt.show()","9424b59a":"%%time\nX=df_train[features]\n\npca = PCA(n_components=3,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2','principal_component_3'])\nprincipalDf['Cover_Type']=df_train['Cover_Type']\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection = '3d')\n\nax.set_xlabel(\"principal_component_1\")\nax.set_ylabel(\"principal_component_2\")\nax.set_zlabel(\"principal_component_3\")\n\nsc=ax.scatter(xs=principalDf['principal_component_1'], ys=principalDf['principal_component_2'],\n              zs=principalDf['principal_component_3'],c=principalDf['Cover_Type'],cmap='Accent')\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\nplt.title('3D Visualization of train Dataset',color='blue',fontsize=12)\nplt.show()","825bbdb6":"del X\ngc.collect()","5cf7094a":"#Dropping Cover_Type 5 as there is only on record from this category\nprint('Train dataset shape before dropping Cover_Type 5',df_train.shape)\n\ndf_train.drop(df_train[df_train[\"Cover_Type\"] == 5].index, axis=0, inplace=True)\n\nprint('Train dataset shape after dropping Cover_Type 5',df_train.shape)","aba22aeb":"#checking Aspect feature before correction\nprint('Train dataset: Aspect feature summary before correction')\ndisplay(feature_summary(df_train[['Id','Aspect']]))\n\nprint('Test dataset: Aspect feature summary before correction')\ndisplay(feature_summary(df_test[['Id','Aspect']]))","7c6e858c":"%%time\n#Correcting Aspect Range.if value is less than 0 adding 360 to it and if value is greater than 359 subtracting 360 from it\ndf_train[\"Aspect\"][df_train[\"Aspect\"] < 0] += 360\ndf_train[\"Aspect\"][df_train[\"Aspect\"] > 359] -= 360\n\ndf_test[\"Aspect\"][df_test[\"Aspect\"] < 0] += 360\ndf_test[\"Aspect\"][df_test[\"Aspect\"] > 359] -= 360","fa1a3bbc":"#checking Aspect feature after correction\nprint('Train dataset: Aspect feature summary after correction')\ndisplay(feature_summary(df_train[['Id','Aspect']]))\n\nprint('Test dataset: Aspect feature summary after correction')\ndisplay(feature_summary(df_test[['Id','Aspect']]))","0fcce9e8":"#checking hillshade features before correction\nprint('Train dataset hillshade features summary before correction')\ndisplay(feature_summary(df_train[['Id','Hillshade_9am','Hillshade_Noon','Hillshade_3pm']]))\n\nprint('Test dataset hillshade features summary before correction')\ndisplay(feature_summary(df_test[['Id','Hillshade_9am','Hillshade_Noon','Hillshade_3pm']]))","4d5605ea":"%%time\n#Correcting Hillshade features\ndf_train.loc[df_train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ndf_test.loc[df_test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ndf_train.loc[df_train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ndf_test.loc[df_test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ndf_train.loc[df_train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ndf_test.loc[df_test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ndf_train.loc[df_train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ndf_test.loc[df_test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ndf_train.loc[df_train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ndf_test.loc[df_test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ndf_train.loc[df_train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ndf_test.loc[df_test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","f0b3bc79":"#checking hillshade features after correction\nprint('Train dataset hillshade features summary after correction')\ndisplay(feature_summary(df_train[['Id','Hillshade_9am','Hillshade_Noon','Hillshade_3pm']]))\n\nprint('Test dataset hillshade features summary after correction')\ndisplay(feature_summary(df_test[['Id','Hillshade_9am','Hillshade_Noon','Hillshade_3pm']]))","e930bb14":"#Creating list of Soil_Type and Wilderness_Area columns\nst_features=[col for col in df_train.columns if str(col).startswith('Soil_Type')]\nwa_features=[col for col in df_train.columns if str(col).startswith('Wilderness_Area')]","5712ffe0":"%%time\n#SIMPLE FEATURE ENGINEERING, CREATING SOME AGGREGATION FEATURES\ndf_train['sumst']=df_train[st_features].sum(axis=1)\ndf_test['sumst']=df_test[st_features].sum(axis=1)\n\ndf_train['meanst']=df_train[st_features].mean(axis=1)\ndf_test['meanst']=df_test[st_features].mean(axis=1)\n\ndf_train['stdst'] = df_train[st_features].std(axis=1)\ndf_test['stdst'] = df_test[st_features].std(axis=1)\n\ndf_train['maxst'] = df_train[st_features].max(axis=1)\ndf_test['maxst'] = df_test[st_features].max(axis=1)\n\ndf_train['minst'] = df_train[st_features].min(axis=1)\ndf_test['minst'] = df_test[st_features].min(axis=1)\n\ndf_train['kurtst'] = df_train[st_features].kurtosis(axis=1)\ndf_test['kurtst'] = df_test[st_features].kurtosis(axis=1)\n\ndf_train['sumwa']=df_train[wa_features].sum(axis=1)\ndf_test['sumwa']=df_test[wa_features].sum(axis=1)\n\ndf_train['meanwa']=df_train[wa_features].mean(axis=1)\ndf_test['meanwa']=df_test[wa_features].mean(axis=1)\n\ndf_train['stdwa'] = df_train[wa_features].std(axis=1)\ndf_test['stdwa'] = df_test[wa_features].std(axis=1)\n\ndf_train['maxwa'] = df_train[wa_features].max(axis=1)\ndf_test['maxwa'] = df_test[wa_features].max(axis=1)\n\ndf_train['minwa'] = df_train[wa_features].min(axis=1)\ndf_test['minwa'] = df_test[wa_features].min(axis=1)\n\ndf_train['kurtwa'] = df_train[wa_features].kurtosis(axis=1)\ndf_test['kurtwa'] = df_test[wa_features].kurtosis(axis=1)\n\ndf_train['sumnb']=df_train[nonbinary_features].sum(axis=1)\ndf_test['sumnb']=df_test[nonbinary_features].sum(axis=1)\n\ndf_train['meannb']=df_train[nonbinary_features].mean(axis=1)\ndf_test['meannb']=df_test[nonbinary_features].mean(axis=1)\n\ndf_train['stdnb'] = df_train[nonbinary_features].std(axis=1)\ndf_test['stdnb'] = df_test[nonbinary_features].std(axis=1)\n\ndf_train['maxnb'] = df_train[nonbinary_features].max(axis=1)\ndf_test['maxnb'] = df_test[nonbinary_features].max(axis=1)\n\ndf_train['minnb'] = df_train[nonbinary_features].min(axis=1)\ndf_test['minnb'] = df_test[nonbinary_features].min(axis=1)\n\ndf_train['kurtnb'] = df_train[nonbinary_features].kurtosis(axis=1)\ndf_test['kurtnb'] = df_test[nonbinary_features].kurtosis(axis=1)\n\n\nagg_features= ['sumst','meanst','stdst','maxst','minst','kurtst','sumwa','meanwa','stdwa','maxwa','minwa','kurtwa',\n               'sumnb','meannb','stdnb','maxnb','minnb','kurtnb']","7826726b":"# Manhhattan distance to Hydrology\ndf_train['manhhattan_dist_hydro'] = np.abs(df_train['Horizontal_Distance_To_Hydrology']) + np.abs(df_train['Vertical_Distance_To_Hydrology'])\ndf_test['manhhattan_dist_hydro'] = np.abs(df_test['Horizontal_Distance_To_Hydrology']) + np.abs(df_test['Vertical_Distance_To_Hydrology'])\n\n# Euclidean distance to Hydrology\ndf_train['ecldn_dist_hydro'] = (df_train['Horizontal_Distance_To_Hydrology']**2 + df_train['Vertical_Distance_To_Hydrology']**2)**0.5\ndf_test['ecldn_dist_hydro'] = (df_test['Horizontal_Distance_To_Hydrology']**2 + df_test['Vertical_Distance_To_Hydrology']**2)**0.5","7226fcb1":"hydro_features=['manhhattan_dist_hydro','ecldn_dist_hydro']","fa65dcf9":"#filling null values with zero\ndf_train['ecldn_dist_hydro'].fillna(0,inplace=True)\ndf_test['ecldn_dist_hydro'].fillna(0,inplace=True)","d7507172":"gc.collect()","74e5636c":"%%time\nscaler = StandardScaler()\nX = scaler.fit_transform(df_train[features+agg_features+hydro_features])\ntest = scaler.transform(df_test[features+agg_features+hydro_features])\ny=df_train['Cover_Type'].values","a9c58f0b":"gc.collect()","902367ff":"#FINAL DATASET SHAPES\nX.shape,y.shape,test.shape","40595525":"%%time\nmodel=RidgeClassifier()\nprint('Ridge Classifier parameters:\\n',model.get_params())\n\nridge_predictions,best_ridge_model,ridge_preds=type_predictor(X,y,test,15,model,'RC')","49880c56":"gc.collect()","1fcad5b0":"df_submission['Cover_Type']=ridge_predictions\ndf_submission['Cover_Type']=round(df_submission['Cover_Type'],0)\ndf_submission['Cover_Type']=df_submission['Cover_Type'].astype('int8')\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('ridge_submission.csv',index=False)\ndf_submission.head(10)","5948368a":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features+hydro_features\ndf_feature_impt['importance']=best_ridge_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,30))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance Ridge Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","93bb4258":"gc.collect()","6b6b5b3d":"# %%time\n# model=DecisionTreeClassifier()\n# print('Ridge Classifier parameters:\\n',model.get_params())\n\n# dtree_predictions,best_dtree_model,dtree_preds=type_predictor(X,y,test,15,model,'DTC')","d4fffb2b":"# gc.collect()","78eb0d7f":"# df_submission['Cover_Type']=dtree_predictions\n# df_submission['Cover_Type']=round(df_submission['Cover_Type'],0)\n# df_submission['Cover_Type']=df_submission['Cover_Type'].astype('int8')\n# #SAVING LGBM PREDICTIONS\n# df_submission.to_csv('dtree_submission.csv',index=False)\n# df_submission.head(10)","ad1f3048":"# df_feature_impt=pd.DataFrame()\n# df_feature_impt['features']=features+agg_features+hydro_features\n# df_feature_impt['importance']=best_dtree_model.feature_importances_\n\n# df_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\n# plt.figure(figsize = (15,30))\n# ax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\n# plt.title('Feature importance Decision Tree Model',color='blue',fontsize=12)\n# ax.bar_label(ax.containers[0]);","6f4d1a3c":"%%time\nlgbm_params = {\n    'objective' : 'multiclass',\n   'device_type': 'gpu'\n}\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=type_predictor(X,y,test,15,model,'LGB')","7ac3d7f3":"gc.collect()","30913dc4":"df_submission['Cover_Type']=lgb_predictions\ndf_submission['Cover_Type']=round(df_submission['Cover_Type'],0)\ndf_submission['Cover_Type']=df_submission['Cover_Type'].astype('int8')\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('lgb_submission.csv',index=False)\ndf_submission.head(10)","4eae3d2b":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features+hydro_features\ndf_feature_impt['importance']=best_lgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance LGB Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","9285da8b":"%%time\nxgb_params = {\n   'tree_method': 'gpu_hist', \n   'gpu_id': 0, \n   'predictor': 'gpu_predictor', \n}\n\nmodel=xgb.XGBClassifier(**xgb_params)\nprint('XGB parameters:\\n',model.get_params())\n\nxgb_predictions,best_xgb_model,XGBpreds=type_predictor(X,y,test,15,model,'XGB')","6613d4f6":"gc.collect()","a4d2b4e5":"df_submission['Cover_Type']=xgb_predictions\ndf_submission['Cover_Type']=round(df_submission['Cover_Type'],0)\ndf_submission['Cover_Type']=df_submission['Cover_Type'].astype('int8')\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('xgb_submission.csv',index=False)\ndf_submission.head(10)","a172a27d":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features+hydro_features\ndf_feature_impt['importance']=best_xgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance Best XGB Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","e29397ca":"<h2 id=\"CUP\">Data Clean up and Feature Tuning<\/h2>\nCorrecting feature ranges and dropping rows with 'Cover_Type' 5 as there is only one record with this type.\n\nCredit for these improvements goes to below notebook:\n\nhttps:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering\n\n<h4>Observations<\/h4>\n<ul>\n    <li><b>Aspect<\/b> is the compass direction that a terrain faces. Here, It is expressed in degrees. All the values from 0 to 359 are present. Besides, there are some values greater than 359 and some smaller than 0. It will be better If we make all the values in this column lie in the range (0, 359). Moreover, all the values in this column lies in the range (-33, 407), train dataset max value 407 and min value -33, whereas for test dataset max value 400 and min value -33. Please check <a href=\"#FeatureSummary\">feature summary<\/a> section. So adding 360 to angles smaller than 0 and subtracting 360 from angles greater than 359 will do the work.<\/li>\n    <li>Hillshading computes surface illumination as values from 0 to 255 based on a given compass direction to the sun (azimuth) and a certain altitude above the horizon (altitude). Hillshades are often used to produce maps that are visually appealing. In both train and test datasets, there are certain rows with <b>hillshade<\/b> value more than 255 or less than 0. They must be the result of recording error and should be relpaced with an appropriate value. Perhaps, values less than 0 refer to the darkest shade and replacing them with 0 should be fine. Similarly, we can assume that hillshade values more than 255 refer to the brightest shades and a value of 255 should be good replacement.<\/li>\n \n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","c2168072":"<h2 id=\"FeatureSummary\">Understanding Train and Test dataset features<\/h2>\nUnderstanding Train and Test dataset features in comparative view, using basic statistical measures.\n\n<h4>Observations<\/h4>\n<ul>\n    <li>No missing values in train or test dataset<\/li>\n    <li>Features <b>Soil_Type7 and Soil_Type15<\/b> are zero in both train and test datasets<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","900bff5e":"<h2 id=\"Ridge\">Ridge Classifier<\/h2>\nStarting with base Ridge model, without any hyperparameter tuning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","963f31d3":"<h2 id=\"Normalization\">Normalizing dataset<\/h2>\nUsing Standard Scaler to normalize dataset\n\n<br><a href=\"#Approach\">back to main menu<\/a>\n","f2915000":"\n<table align=\"left\">\n    <caption><b>THE WILDERNESS AREA DETAILS<\/b><\/caption>\n    <tr><th>Wilderness type<\/th><th>Description<\/th><\/tr>\n    <tr><td>1<\/td><td>Rawah Wilderness Area.<\/td><\/tr>\n    <tr><td>2<\/td><td>Neota Wilderness Area.<\/td><\/tr>\n    <tr><td>3<\/td><td>Comanche Peak Wilderness Area.<\/td><\/tr>\n    <tr><td>4<\/td><td>Cache la Poudre Wilderness Area.<\/td><\/tr>\n<\/table>","e24d57ac":"<b>Problem Statement:<\/b>  The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original <a href=\"https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview\">Forest Cover Type Prediction<\/a> competition.\n\n<b>Problem type:<\/b> A multi-class classification problem.\n\n<b>Evaluation matrix:<\/b> Submissions are evaluated on <b>multi-class classification accuracy<\/b>.","2c8825b4":"<h2 id=\"AggFeatures\">Creating Aggregated features<\/h2>\nCreating aggregated features\n<h4>Observation<\/h4>\n<li>Creating aggregated features for Soil_Type, Wilderness_Area and Nonbinary columns separately<\/li>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","410c2f1f":"<table align=\"left\">\n    <caption><b>TRAIN and TEST DATASET FEATURES<\/b><\/caption>\n    <tr><th>Feature<\/th><th>Feature description<\/th><\/tr>\n    <tr><td>Elevation<\/td><td>Elevation in meters.<\/td><\/tr>\n    <tr><td>Aspect<\/td><td>Aspect in degrees <a href=\"https:\/\/www.photopills.com\/sites\/default\/files\/tutorials\/2014\/2-azimuth-elevation.jpg\">azimuth<\/a> (The azimuth is the angle between North, measured clockwise around the observer's horizon.)<\/td><\/tr>\n    <tr><td>Slope<\/td><td>Slope in degrees.<\/td><\/tr>\n    <tr><td>Horizontal_Distance_To_Hydrology<\/td><td>Horz Dist to nearest surface water features.<\/td><\/tr>\n    <tr><td>Vertical_Distance_To_Hydrology<\/td><td>Vert Dist to nearest surface water features.<\/td><\/tr>\n    <tr><td>Horizontal_Distance_To_Roadways<\/td><td>Horz Dist to nearest roadway.<\/td><\/tr>\n    <tr><td>Hillshade_9am (0 to 255 index)<\/td><td>Hillshade index at 9am, summer solstice.<\/td><\/tr>\n    <tr><td>Hillshade_Noon (0 to 255 index)<\/td><td>Hillshade index at noon, summer solstice.<\/td><\/tr>\n    <tr><td>Hillshade_3pm (0 to 255 index)<\/td><td>Hillshade index at 3pm, summer solstice.<\/td><\/tr>\n    <tr><td>Horizontal_Distance_To_Fire_Points<\/td><td>Horz Dist to nearest wildfire ignition points.<\/td><\/tr>\n    <tr><td>Wilderness_Area (4 binary columns, 0 = absence or 1 = presence)<\/td><td>Wilderness area designation.<\/td><\/tr>\n    <tr><td>Soil_Type (40 binary columns, 0 = absence or 1 = presence)<\/td><td>Soil Type designation.<\/td><\/tr>\n    <tr><td>Cover_Type (7 types, integers 1 to 7)<\/td><td>Forest Cover Type designation.<\/td><\/tr>\n<\/table>","647d97c2":"<h2 id=\"XGB\">XGBClassifier<\/h2><font color='red'>(work in progress)<\/font>\n\nSimple XGBClassifier without any hyperparameter tunning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","6b62736c":"<h2 id=\"Corr\">Correlation Check<\/h2>\nLets check if there are any correlated features. If two features are highly correlated we can remove one of the feature.\nThis will help in dimentionality reduction.\n\n<h4>Observations<\/h4>\n<ul>\n    <li><\/li>\n   \n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","6dbf1a6d":"<h2 id=\"Target\">Understanding Cover_Type (target) feature distribution<\/h2>\nLets visualize Cover_Type (target) feature.\n\n<h4>Observation<\/h4>\n \n\n<br><a href=\"#Approach\">back to main menu<\/a>","b18b4fd9":"<h2 id=\"TrainVisual\">Visualizating Training dataset<\/h2>\nWe are making use of PCA, dimentionality reduction technique to Visualize Training dataset.<br>\nVisualization is also helpful in understanding any grouping or patterns within dataset.\n<h4>Observation<\/h4>\n3D plot shows some pattern or grouping along different planes\n\n<br><a href=\"#Approach\">back to main menu<\/a>","bf5f0c11":"<h2 id=\"Approach\">Approach to the problem<\/h2>\nIdea is to develop a generalized approach for solving any multiclass classification problem\n<ol>\n    <li>Performing exploratory data analysis (EDA) and Data Preparation (DP).<\/li>\n    <ol>\n        <li><a href=\"#FeatureSummary\">Understanding Train and Test dataset features (EDA)<\/a><\/li>\n        <li><a href=\"#Downcasting\">Down Casting Train and Test datasets (DP)<\/a><\/li>\n        <li><a href=\"#Target\">Understanding Cover_Type (target) feature distribution (EDA)<\/a><\/li>\n        <li><a href=\"#Corr\">Correlation check (EDA)<\/a><\/li>\n        <li><a href=\"#TrainVisual\">Visualizing Training dataset (EDA)<\/a><\/li>\n        <li><a href=\"#CUP\">Data Clean up and Feature Tuning(DP)<\/a><\/li>\n    <\/ol>\n    <li>Feature Engineering.<\/li>\n    <ol>\n        <li><a href=\"#AggFeatures\">Creating Aggregated features<\/a><\/li>\n    <\/ol>\n    <li>Training Linear,Gradient Boost and Ensemble models.<\/li>\n    <ol>\n        <li><a href=\"#Ridge\">Ridge Classifier<\/a><\/li>\n        <li><a href=\"DTC\">DecisionTree Classification<\/a><\/li>\n        <li><a href=\"#LGBM\">LGBM Classification<\/a><\/li>\n        <li><a href=\"#XGB\">XGBClassifer<\/a><\/li>\n    <\/ol>\n <\/ol>\n\n<h4>Observations<\/h4>\n<ul>\n    <li> Cross Validation score for LGBM (0.9288525) is better than Ridge (0.89618075) for 10 iterations<\/li>\n    <li> But on public leaderboard Ridge (0.88672) and LGBM (0.89817) scores have higher variance from cross validation scores<\/li>\n    <li> Cross Validation score for Ridge (0.89611824), DecisionTree (0.94623975007), LGBM (0.9325144932) and corresponding public leaderboard scores Ridge (0.88771), DecisionTree (0.92391), LGBM (0.91971) for 15 iterations<\/li>\n    <li> We are getting best score with DecisionTree classifier without any feature turning or feature engineering<\/li>\n    <li> Now lets try some feature engineering and feature tuning<\/li>\n<\/ul>","d76fcd4b":"<table align=\"left\">\n    <caption><b>SOIL TYPE DETAILS<\/b><\/caption>\n    <tr><th>Soil Type<\/th><th>Description<\/th><\/tr>\n    <tr><td>1<\/td><td>Cathedral family - Rock outcrop complex, extremely stony.<\/td><\/tr>\n    <tr><td>2<\/td><td>Vanet - Ratake families complex, very stony.<\/td><\/tr>\n    <tr><td>3<\/td><td>Haploborolis - Rock outcrop complex, rubbly.<\/td><\/tr>\n    <tr><td>4<\/td><td>Ratake family - Rock outcrop complex, rubbly.<\/td><\/tr>\n    <tr><td>5<\/td><td>Vanet family - Rock outcrop complex complex, rubbly.<\/td><\/tr>\n    <tr><td>6<\/td><td>Vanet - Wetmore families - Rock outcrop complex, stony.<\/td><\/tr>\n    <tr><td>7<\/td><td>Gothic family.<\/td><\/tr>\n    <tr><td>8<\/td><td>Supervisor - Limber families complex.<\/td><\/tr>\n    <tr><td>9<\/td><td>Troutville family, very stony.<\/td><\/tr>\n    <tr><td>10<\/td><td>Bullwark - Catamount families - Rock outcrop complex, rubbly.<\/td><\/tr>\n    <tr><td>11<\/td><td>Bullwark - Catamount families - Rock land complex, rubbly.<\/td><\/tr>\n    <tr><td>12<\/td><td>Legault family - Rock land complex, stony.<\/td><\/tr>\n    <tr><td>13<\/td><td>Catamount family - Rock land - Bullwark family complex, rubbly.<\/td><\/tr>\n    <tr><td>14<\/td><td>Pachic Argiborolis - Aquolis complex.<\/td><\/tr>\n    <tr><td>15<\/td><td>unspecified in the USFS Soil and ELU Survey.<\/td><\/tr>\n    <tr><td>16<\/td><td>Cryaquolis - Cryoborolis complex.<\/td><\/tr>\n    <tr><td>17<\/td><td>Gateview family - Cryaquolis complex.<\/td><\/tr>\n    <tr><td>18<\/td><td>Rogert family, very stony.<\/td><\/tr>\n    <tr><td>19<\/td><td>Typic Cryaquolis - Borohemists complex.<\/td><\/tr>\n    <tr><td>20<\/td><td>Typic Cryaquepts - Typic Cryaquolls complex.<\/td><\/tr>\n    <tr><td>21<\/td><td>Typic Cryaquolls - Leighcan family, till substratum complex.<\/td><\/tr>\n    <tr><td>22<\/td><td>Leighcan family, till substratum, extremely bouldery.<\/td><\/tr>\n    <tr><td>23<\/td><td>Leighcan family, till substratum - Typic Cryaquolls complex.<\/td><\/tr>\n    <tr><td>24<\/td><td>Leighcan family, extremely stony.<\/td><\/tr>\n    <tr><td>25<\/td><td>Leighcan family, warm, extremely stony.<\/td><\/tr>\n    <tr><td>26<\/td><td>Granile - Catamount families complex, very stony.<\/td><\/tr>\n    <tr><td>27<\/td><td>Leighcan family, warm - Rock outcrop complex, extremely stony.<\/td><\/tr>\n    <tr><td>28<\/td><td>Leighcan family - Rock outcrop complex, extremely stony.<\/td><\/tr>\n    <tr><td>29<\/td><td>Como - Legault families complex, extremely stony.<\/td><\/tr>\n    <tr><td>30<\/td><td>Como family - Rock land - Legault family complex, extremely stony.<\/td><\/tr>\n    <tr><td>31<\/td><td>Leighcan - Catamount families complex, extremely stony.<\/td><\/tr>\n    <tr><td>32<\/td><td>Catamount family - Rock outcrop - Leighcan family complex, extremely stony.<\/td><\/tr>\n    <tr><td>33<\/td><td>Leighcan - Catamount families - Rock outcrop complex, extremely stony.<\/td><\/tr>\n    <tr><td>34<\/td><td>Cryorthents - Rock land complex, extremely stony.<\/td><\/tr>\n    <tr><td>35<\/td><td>Cryumbrepts - Rock outcrop - Cryaquepts complex.<\/td><\/tr>\n    <tr><td>36<\/td><td>Bross family - Rock land - Cryumbrepts complex, extremely stony.<\/td><\/tr>\n    <tr><td>37<\/td><td>Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.<\/td><\/tr>\n    <tr><td>38<\/td><td>Leighcan - Moran families - Cryaquolls complex, extremely stony.<\/td><\/tr>\n    <tr><td>39<\/td><td>Moran family - Cryorthents - Leighcan family complex, extremely stony.<\/td><\/tr>\n    <tr><td>40<\/td><td>Moran family - Cryorthents - Rock land complex, extremely stony.<\/td><\/tr>\n<\/table>","3c3f3829":"<h2 id=\"Downcasting\">Down Casting Training and Testing datasets<\/h2>\nChecking possibility for down casting dataset datatypes. This will help in reducing overall dataset size.\n\n<h4>Observations<\/h4>\n<ul>\n    <li>We have only one data type in datasets, i.e., int64<\/li>\n    <li>It is always a good idea to reduce overall dataset size by finding correct datatypes<\/li>\n    <li>With downcasting able to reduce training dataset size from 1.7 GB to 259.4 MB<\/li>\n    <li>With downcasting able to reduce testing dataset size from 419.6 MB to 63.9 MB<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","b71f27dc":"<h2 id=\"LGBM\">LGBM Classification<\/h2>\n\nSimple LGBMClassifier without any hyperparameter tunning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","710d4a99":"<h2 id=\"DTC\">DecisionTree Classification<\/h2>\n\nSimple DecisionTree Classification without any hyperparameter tunning.\n\n<I><font color='red'>Note: Commented the code to reduce overall execution time<\/font><\/I>\n\n<br><a href=\"#Approach\">back to main menu<\/a>"}}