{"cell_type":{"6af25508":"code","0b63342d":"code","230830ab":"code","b9abbdb2":"code","d2c6ae02":"code","fda7e432":"code","fc84cf3a":"code","5e00cd0d":"code","60e05da2":"code","e404d3aa":"code","ded28fec":"code","0eab8b13":"code","ae28c15d":"code","c997bfd3":"code","a31b1f67":"code","b474c311":"code","2e8db927":"code","63293085":"code","0e3443fe":"code","b83904c7":"code","e088c5dd":"code","c7dfea04":"code","c5eb2a4c":"code","664f5138":"code","b64dcf43":"code","304ff6df":"code","00a9875c":"code","22addfc9":"code","fb8657df":"code","db2c473b":"code","195fb23b":"code","72ac9fd0":"code","5746075f":"code","4369dfc9":"code","dd835a5f":"code","56b4901a":"code","74ca7e7e":"code","4bb33c0f":"code","cfc1a4a8":"code","529514de":"code","2b5c2ef8":"code","419ba50c":"code","5bfa27b1":"code","b3d13072":"code","9f1386d8":"code","5a03a3c9":"code","e5e98fa8":"code","665599b2":"code","75a4b5c2":"code","f5fddb4b":"code","c70575ed":"code","2e085cca":"code","f4da0f20":"code","41e66691":"code","172badc6":"code","9c0dc105":"code","1f6c299e":"code","ed5139f1":"code","1307f070":"code","5b477be3":"code","65c190ec":"code","b55fed93":"code","6cac8bd0":"code","92ad2d2c":"code","78b0abcf":"markdown","61cce4c4":"markdown","95aa18a6":"markdown","5e5eb066":"markdown","9ccc0b61":"markdown","409f6027":"markdown","9f33b890":"markdown","3d782980":"markdown","0906f391":"markdown","d09d0534":"markdown","53076287":"markdown","5b9396f4":"markdown","9712d280":"markdown","767d35dd":"markdown","7ff1022e":"markdown","cd8645b5":"markdown","068a8e3e":"markdown","accd71c8":"markdown","4df9d683":"markdown","b7d5e201":"markdown","54fcf540":"markdown","91e49db1":"markdown","10de53c1":"markdown","e825d9bc":"markdown","349d2ccd":"markdown","0663ccac":"markdown","06e1b077":"markdown","bda3951c":"markdown"},"source":{"6af25508":"import pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nnp.random.seed(seed=217)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0b63342d":"train = pd.read_csv(\"..\/input\/costa-rican-household-poverty-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/costa-rican-household-poverty-prediction\/test.csv\")","230830ab":"train ","b9abbdb2":"test","d2c6ae02":"train[train.age >= 18].groupby('idhogar').transform(\"count\")","fda7e432":"train_test = train.append(test, ignore_index = True)\ntrain_test","fc84cf3a":"train_test['Id'].value_counts()","5e00cd0d":"train_test['idhogar'].value_counts()","60e05da2":"#check if multiple columns can be 1 at the same time\n#all the columns should only contains binary data 1 or 0\ndef is_mutually_exclusive(dataframe,attributes):\n    res_s = dataframe[attributes].sum(axis=1)\n    res_s = res_s[res_s > 1]\n    if res_s.size == 0:\n        print(attributes,'are mutually exclusive')\n    else:\n        print(attributes,'are not mutually exclusive')","e404d3aa":"suspected_columns_in_ohe = ['pared', 'piso', 'techo', 'abastagua', 'sanitario', \n                            'energ', 'elimbasu','epared', 'etecho', 'eviv', \n                            'estadocivil', 'parentesco','instlevel','tipovivi','lugar']\nelectricity_source = ['public','planpri', 'noelec', 'coopele']\nfor group_title in suspected_columns_in_ohe:\n    group = [col for col in train_test if col.startswith(group_title)]\n    is_mutually_exclusive(train_test, group)\nis_mutually_exclusive(train_test, electricity_source)","ded28fec":"missing = pd.DataFrame(train_test.isnull().sum()).rename(columns={0:'total'})\nmissing['Percent']=missing['total']\/len(train_test)\nmissing.sort_values('Percent',ascending=False).head(10)","0eab8b13":"train_test.loc[((train_test['age']>19) | (train_test['age']<7)) & (train_test['rez_esc'].isnull()), 'rez_esc']=0\n","ae28c15d":"missing_rez_esc = train_test[train_test['rez_esc'].isnull() == True]\nmissing_rez_esc = missing_rez_esc[['rez_esc','age','escolari']]\nprint(missing_rez_esc.to_string())","c997bfd3":"train_test['rez_esc']=train_test['rez_esc'].fillna(0)","a31b1f67":"over_rez_esc = train_test[train_test['rez_esc'] > 5]\nover_rez_esc= over_rez_esc[['rez_esc','age','escolari']]\nprint(over_rez_esc.to_string())","b474c311":"train_test.loc[train_test['rez_esc'] > 5, 'rez_esc'] = 8 - 7 - 0","2e8db927":"train_test.groupby('v18q')['v18q1'].apply(lambda x:x.isnull().sum())","63293085":"train_test['v18q1']=train_test['v18q1'].fillna(0)","0e3443fe":"import matplotlib.pyplot as plt\nown_variables=[x for x in train_test if x.startswith('tipo')]\ntrain_test.loc[train_test['v2a1'].isnull(),own_variables].sum().plot.bar()\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)","b83904c7":"train_test['v2a1']=train_test['v2a1'].fillna(0)","e088c5dd":"missing_meaneduc = train_test[train_test['meaneduc'].isnull() == True]\nmissing_meaneduc = missing_meaneduc[['idhogar','age','escolari','meaneduc','SQBmeaned']]\nprint(missing_meaneduc.to_string())","c7dfea04":"missing_meaneduc = missing_meaneduc[missing_meaneduc['age']>=18]\nmissing_meaneduc = missing_meaneduc[['idhogar', 'escolari']]\nmissing_meaneduc = missing_meaneduc.groupby(by=[\"idhogar\"]).mean()","c5eb2a4c":"result = pd.merge(train_test,missing_meaneduc , left_on=\"idhogar\", right_index=True, how=\"left\", sort=False)\nresult","664f5138":"result['meaneduc']=result['meaneduc'].fillna(result['escolari_y'])","b64dcf43":"missing_meaneduc_result = result[result['meaneduc'].isnull() == True]\nmissing_meaneduc_result = missing_meaneduc_result[['idhogar','age','escolari_x','meaneduc','SQBmeaned']]\nprint(missing_meaneduc_result.to_string())","304ff6df":"result['meaneduc']=result['meaneduc'].fillna(0)","00a9875c":"result['SQBmeaned']=result['SQBmeaned'].fillna(result['meaneduc']*result['meaneduc'])","22addfc9":"missing_meaneduc_result = result[result['SQBmeaned'].isnull() == True]\nmissing_meaneduc_result = missing_meaneduc_result[['idhogar','age','escolari_x','meaneduc','SQBmeaned']]\nprint(missing_meaneduc_result.to_string())","fb8657df":"train_test['meaneduc'] = result['meaneduc']\ntrain_test['SQBmeaned'] = result['SQBmeaned']","db2c473b":"missing = pd.DataFrame(train_test.isnull().sum()).rename(columns={0:'total'})\nmissing['Percent']=missing['total']\/len(train_test)\nmissing.sort_values('Percent',ascending=False).head(10)","195fb23b":"print(train_test.dtypes.to_string())","72ac9fd0":"train_test.select_dtypes('object').head()","5746075f":"mapping_dict={'yes':1,\"no\":0}\ntrain_test['dependency'] = train_test['dependency'].replace(mapping_dict).astype(np.float64)\ntrain_test['edjefe'] = train_test['edjefe'].replace(mapping_dict).astype(\"int\")\ntrain_test['edjefa'] = train_test['edjefa'].replace(mapping_dict).astype(\"int\")\ntrain_test['edjef'] = np.max(train_test[['edjefa','edjefe']], axis=1)\n","4369dfc9":"print(train_test.dtypes.to_string())","dd835a5f":"train_test.select_dtypes('object').head()","56b4901a":"# convert to int if float but only has integer value\ntrain_test['v18q1'] = train_test['v18q1'].astype(\"int\")\ntrain_test['rez_esc'] = train_test['rez_esc'].astype(\"int\")","74ca7e7e":"from sklearn.preprocessing import LabelEncoder\ndef do_features(df):\n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    \n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n\n    # Drop id's\n    #df.drop(['Id'], axis=1, inplace=True)\n    \n    return df\ntrain_test = do_features(train_test)","4bb33c0f":"# convert one hot encoded fields to label encoding\nfrom sklearn.preprocessing import LabelEncoder\ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele'] #do for electricity also\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df\ntrain_test = convert_OHE2LE(train_test)","cfc1a4a8":"def convert_geo2aggs(df_):\n    cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\n    cols_nums = ['age', 'meaneduc', 'dependency', \n                 'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n                 'bedrooms', 'overcrowding']\n\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n    \n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\n# add some aggregates by geography\ntrain_test = convert_geo2aggs(train_test)","529514de":"train_test","2b5c2ef8":"\ntrain_test['num_over_18'] = 0\ntrain_test['num_over_18'] = train_test[train_test.age >= 18].groupby('idhogar').transform(\"count\")\ntrain_test['num_over_18'] = train_test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\ntrain_test['num_over_18'] = train_test['num_over_18'].fillna(0)\n# add some extra features, these were taken from another kernel\ndef extract_features(df):\n    # add the number of people over 18 in each household\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['v2a1_to_r4t3'] = df['v2a1']\/df['r4t3'] # rent to people in household\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms'] # rooms per person\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize'] # rent to household size\n    df['rent_to_over_18'] = df['v2a1']\/df['num_over_18']\n    # some households have no one over 18, use the total rent for those\n    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n    \nextract_features(train_test)       ","419ba50c":"train_test","5bfa27b1":"# drop duplicated columns\nneedless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n                 'mobilephone', 'female', ]\n\n#instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n\n#needless_cols.extend(instlevel_cols)\n\ntrain_test = train_test.drop(needless_cols, axis=1)","b3d13072":"test_final = train_test.loc[train_test[\"Target\"].isnull()]\ntest_final = test_final.drop(['Target'], axis=1)\ntest_final = test_final.fillna(0)\ntest_final.to_csv(r'test_final_v1.csv', index = False)\ntest_final","9f1386d8":"train_final = train_test.loc[train_test[\"Target\"].notnull()]\ntrain_final = train_final.fillna(0)\ntrain_final.to_csv(r'train_final_v1.csv', index = False)\ntrain_final","5a03a3c9":"cols_with_missing = [col for col in train_final.columns \n                                 if train_final[col].isnull().any()]\ncols_with_missing","e5e98fa8":"train_final.loc[train_final['agg18_estadocivil1_MEAN'].isnull()]","665599b2":"train_test.to_csv(r'train_test.csv', index = False)","75a4b5c2":"# train_test = pd.read_csv(\"..\/input\/costa-rica\/train_test.csv\") # check for cleanliness Version 14\n\ntrain = train_test.loc[~train_test['Target'].isnull()]\ntest = train_test.loc[train_test['Target'].isnull()]\n\ntest = test.drop(['Target'], axis=1)\n\noriginal_test = pd.read_csv(\"..\/input\/costa-rican-household-poverty-prediction\/test.csv\")\n\ntest_ids = original_test.Id","f5fddb4b":"# Need test id when exporting prediction result\n# test_ids = test.Id\n\n# But Id and idhogar are not needed during model building\ntrain.drop(columns = [\"Id\", \"idhogar\"], inplace = True)\ntest.drop(columns = [\"Id\", \"idhogar\"], inplace = True)\n\ntrain_length = train.shape[0]\ntest_length = test.shape[0]","c70575ed":"# Our subjects of interest are heads of household\n# Hence, for training, we will only be using these subjects\n\n# But for test data, we have to predict for both heads and non-heads,\n# except that results for non-heads are not graded.\n# Hence, we treat all the test data like heads of household for prediction purposes.\n\nX = train[train.parentesco1 == 1].copy()\ny = X['Target'] - 1\ny = y.astype('int')\nX.drop(['Target', \"parentesco1\"], axis=1, inplace = True)\n\ntest.drop(columns = [\"parentesco1\"], inplace = True)","2e085cca":"# Since our training data is so imbalanced, we shall not allocate the same weight to all target classes\n# Instead, target classes that are more \"rare\" shall get higher weight\n# y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)\ny_weights = class_weight.compute_sample_weight('balanced', y, indices=None)\n\n# Let's take a look\n# Indeed, target class 1, which is the rarest, gets the highest weight\n# Then weight decreases as the Target class becomes more common\n# print(pd.DataFrame(dict(Target = y_train+1, Weight = y_train_weights)).drop_duplicates().sort_values(by = [\"Target\"]).reset_index(drop = True))\nprint(pd.DataFrame(dict(Target = y, Weight = y_weights)).drop_duplicates().sort_values(by = [\"Target\"]).reset_index(drop = True))","f4da0f20":"def split_data(train, y, sample_weight=None, test_percentage=0.20):   \n    # pick some random households to use for the test data\n    test_idx = np.random.choice(train.index, size = int(train.shape[0] * test_percentage), replace = False)\n    \n    X_test = train.loc[test_idx]\n    y_test = y.loc[test_idx]\n\n    X_train = train.loc[~train.index.isin(test_idx)]\n    y_train = y.loc[~train.index.isin(test_idx)]\n    \n    if sample_weight is not None:\n        y_train_weights = sample_weight[~train.index.isin(test_idx)]\n        return X_train, y_train, X_test, y_test, y_train_weights\n    \n    return X_train, y_train, X_test, y_test","41e66691":"train_X, train_y, val_X, val_y, train_y_weights = split_data(X, y, y_weights, test_percentage = 0.30)","172badc6":"def evaluate_macroF1_xgb(predictions, truth):  \n    pred_labels = predictions.argmax(axis=1)\n    truth = truth.get_label()\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1) \n\nxgb_fit_params={\"early_stopping_rounds\":500,\n                \"eval_metric\" : evaluate_macroF1_xgb, \n                'verbose': False,\n               }\n\nxgb_ = xgb.XGBClassifier(random_state=217, n_jobs=-1)\nxgb_fit_params[\"eval_set\"] = [(val_X,val_y)]\nmodel = xgb_.fit(train_X, train_y, sample_weight = train_y_weights, **xgb_fit_params)\nuseless_xgb_cols = train_X.columns[model.feature_importances_ == 0].tolist()","9c0dc105":"rf_ = RandomForestClassifier(random_state=217, n_jobs=-1)\nrf_.fit(X, y)\nuseless_rf_cols = X.columns[rf_.feature_importances_ == 0].tolist()","1f6c299e":"def evaluate_macroF1_lgb(truth, predictions):  \n    pred_labels = predictions.reshape(4,-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nlgb_fit_params={\"early_stopping_rounds\":500,\n                \"eval_metric\" : evaluate_macroF1_lgb, \n                'verbose': False,\n               }\n\nlgb_fit_params[\"eval_set\"] = [(val_X,val_y)]\nlgb_ = lgb.LGBMClassifier(random_state = 217, n_jobs=-1)\nmodel = lgb_.fit(train_X, train_y, sample_weight = train_y_weights, **lgb_fit_params)\nuseless_lgb_cols = X.columns[model.feature_importances_ == 0].tolist()","ed5139f1":"print(\"XGBClassifier eliminated {} features\".format(len(useless_xgb_cols)))\nprint(\"RandomForestClassifier eliminated {} features\".format(len(useless_rf_cols)))\nprint(\"LGBMClassifier eliminated {} features\".format(len(useless_lgb_cols)))","1307f070":"xgb_opt_parameters = {'n_estimators':300, 'learning_rate':0.15, 'max_depth':35, 'eta':0.15, \n                      'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, \n                      'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n\nrf_opt_parameters = {\"max_depth\": None, \"n_estimators\": 500, \"min_impurity_decrease\": 1e-3, \n                    \"min_samples_leaf\": 2, \"class_weight\": \"balanced\"}\n\nlgb_opt_parameters = {'max_depth':45, \"learning_rate\":0.15, 'objective':'multiclass', 'silent':True, #'metric':evaluate_macroF1_lgb, \n                      'n_estimators':300, 'class_weight':'balanced','colsample_bytree': 0.89, 'min_child_samples':90, \n                      'num_leaves':20, 'subsample':0.96}","5b477be3":"xgbs = []\nfor i in range(0, 16):\n    xgb_ = xgb.XGBClassifier(random_state=217+i, n_jobs=-1, **xgb_opt_parameters)\n    train_X, train_y, val_X, val_y, train_y_weights = split_data(X.drop(columns = useless_xgb_cols), y, sample_weight= y_weights)\n    xgb_fit_params[\"eval_set\"] = [(val_X,val_y)]\n    xgb_.fit(train_X, train_y, sample_weight = train_y_weights, **xgb_fit_params)\n    xgbs.append(xgb_)\n\nrfs = []\nfor i in range(0, 8):\n    rf = RandomForestClassifier(random_state=217+i, n_jobs=-1, **rf_opt_parameters)  \n    rf.fit(X.drop(useless_rf_cols, axis=1), y)\n    rfs.append(rf)\n    \nlgbs = []\nfor i in range(0, 9):\n    lgb_ = lgb.LGBMClassifier(random_state=217+i, n_jobs=-1, **lgb_opt_parameters)\n    train_X, train_y, val_X, val_y  = split_data(X.drop(columns = useless_lgb_cols), y)\n    lgb_fit_params[\"eval_set\"] = [(val_X,val_y)]\n    lgb_.fit(train_X, train_y, **lgb_fit_params)\n    lgbs.append(lgb_)","65c190ec":"xgb_predictions_hard_test = []\nfor i in range(0, len(xgbs)):\n    xgb_predictions_hard_test.append(xgbs[i].predict(test.drop(columns = useless_xgb_cols ).fillna(0)))\n    \nrf_predictions_hard_test = []\nfor i in range(0, len(rfs)):\n    rf_predictions_hard_test.append(rfs[i].predict(test.drop(columns = useless_rf_cols ).fillna(0)))\n    \nlgb_predictions_hard_test = []\nfor i in range(0, len(lgbs)):\n    lgb_predictions_hard_test.append(lgbs[i].predict(test.drop(columns = useless_lgb_cols).fillna(0)))  \n    \nxgb_predictions_soft_test = []\nfor i in range(0, len(xgbs)):\n    xgb_predictions_soft_test.append(xgbs[i].predict_proba(test.drop(columns = useless_xgb_cols ).fillna(0)))\n    \nrf_predictions_soft_test = []\nfor i in range(0, len(rfs)):\n    rf_predictions_soft_test.append(rfs[i].predict_proba(test.drop(columns = useless_rf_cols ).fillna(0)))\n    \nlgb_predictions_soft_test = []\nfor i in range(0, len(lgbs)):\n    lgb_predictions_soft_test.append(lgbs[i].predict_proba(test.drop(columns = useless_lgb_cols ).fillna(0)))\n","b55fed93":"def generate_results(settings):\n    num_of_xgbs = settings[0]\n    num_of_rfs = settings[1]\n    num_of_lgbs = settings[2]\n    voting_type = settings[3]\n    \n    if voting_type == \"Hard\":\n        all_predictions = []\n        if len(xgb_predictions_hard_test[0:num_of_xgbs]) != 0:\n            all_predictions.extend(xgb_predictions_hard_test[0:num_of_xgbs])\n        if len(rf_predictions_hard_test[0:num_of_rfs]) != 0:\n            all_predictions.extend(rf_predictions_hard_test[0:num_of_rfs])\n        if len(lgb_predictions_hard_test[0:num_of_lgbs]) != 0:\n            all_predictions.extend(lgb_predictions_hard_test[0:num_of_lgbs])\n        combined_predictions = np.vstack(all_predictions).transpose()\n        combined_predictions = pd.DataFrame(combined_predictions).mode(axis = 1)\n        combined_predictions = combined_predictions.median(axis = 1).astype('int')\n    \n    if voting_type == \"Soft\":\n        xgbs_predictions = sum(xgb_predictions_soft_test[0:num_of_xgbs])\n        rfs_predictions = sum(rf_predictions_soft_test[0:num_of_rfs])\n        lgbs_predictions = sum(lgb_predictions_soft_test[0:num_of_lgbs])\n        combined_predictions = sum([xgbs_predictions, rfs_predictions, lgbs_predictions])\n        combined_predictions = combined_predictions.argmax(axis = 1)\n\n    return combined_predictions","6cac8bd0":"results = generate_results((7, 8, 9, 'Hard'))","92ad2d2c":"submission = pd.DataFrame(test_ids)\nsubmission[\"Target\"] = results + 1\nsubmission.to_csv(\"submission.csv\", index = False)","78b0abcf":"See if there is any data inconsistency for all the columns in the dataset.","61cce4c4":"there are 10340 households in the dataset","95aa18a6":"The rez_esc that are still missing are mostly by individual aged 18-19. I assumed they are graduated from school or won't be attending school anymore. Hence, years behind in school are not applicable to them as well and we can fill their 'rez_esc' as 0.","5e5eb066":"saw a lot of columns with similar name, check if they are mutually exclusive\/ one hot encoded data.","9ccc0b61":"v18q1 is Number of tablets household owns. \nv18q is binary data showing if a household owns a tablet.","409f6027":"check empty data for every column ","9f33b890":"* No empty 'v2a1' when tipovivi2 and tipovivi3 ==1. \n* Empty 'v2a1' are mostly coming from tipovivi1 menaning that owns a fully paid house so dont need to pay rent\n* assume that the individual dont need to pay rent for the others empty value","3d782980":"* As we can see, 'dependency', 'edjefe' and 'edjefa' are having inconsistent data.\n* dependency, Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n* edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n* edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0","0906f391":"# Feature Engineering ","d09d0534":"id is unique","53076287":"notice that 'Target' with NaN is from test data","5b9396f4":"from the data we understand when the head of household is a male, 'edjefe' will be a number value whereas 'edjefa' will be a 'no'\nand when the head of household is a female, 'edjefa' will be a number value whereas 'edjefe' will be a 'no'","9712d280":"Accodint to the host : \"In the example of v2a1 you should first filter by tipoviv3. Most households that don't have information will be households that do not rent or are not paying loans (tipoviv2). If a household is tipoviv3 and does not have information then you need to make a choice about the treatment of the variable either assume 0, impute a value or not use the variable. Basically in those cases we don't know the value.\"","767d35dd":"See which are the rows with empty 'meaneduc' ","7ff1022e":"23856 rows and 142 columns for test data, consistent with the description from host","cd8645b5":"Only one individual has 'rez_esc' > 5. \nThis is an invalid data because years behind in school is 99 years but the individual is only 8 years old.\nWe shall replace the rez_esc with '8 - 7 - escolari'. We assumed that age - 7 give the years of education a person should have.","068a8e3e":"check for id and idhodgar uniqueness. ","accd71c8":"# Data Cleaning","4df9d683":"rez_esc -> Years behind in school\n\nAccording to the Host \"This variable is only collected for people between 7 and 19 years of age and it is the difference between the years of education a person should have and the years of education he\/she has. it is capped at 5.\"","b7d5e201":"Now all the missing data are replaced with meaningful value. we can start to do eda and feature engineering.","54fcf540":"# Feature Selection","91e49db1":"most of them are between 17-19 that didn't fill in this column, maybe they don't think that they are adult yet. So let's fill in those household with 18+ adults","10de53c1":"From the above discussion we can understand that 'yes = 1 and no = 0' for 'dependency', 'edjefe' and 'edjefa'","e825d9bc":"there exist 2 household that don't have any adult. We fill in thier 'meaneduc' with 0.","349d2ccd":"all are mutually exclusive. or one hot encoded data. Can change to label encoding when doing feature engineering if we are building decision tree.","0663ccac":"all 25468 missing data in 'v18q1' is when 'v18q' equals to 0. meaning that the household dont own a tablet and also means that the number of tablets the household owns is 0 ('v18q1' = 0). Hence, fill in the empty data in 'v18q1' to 0. ","06e1b077":"![image.png](attachment:image.png)\n![image.png](attachment:image.png)","bda3951c":"9557 rows and 143 columns for train data, consistent with the description from host"}}