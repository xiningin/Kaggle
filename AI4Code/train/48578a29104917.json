{"cell_type":{"56a309ec":"code","45285331":"code","370e7e30":"code","2e9843ba":"code","38aa1932":"code","97a128e9":"code","7c9cce0d":"code","847a4434":"code","72af89e0":"code","75fe9354":"code","b0fd5083":"code","8e37a672":"code","92efb153":"code","1a281da0":"code","cec3c452":"markdown","d8e9e656":"markdown","8bb4a856":"markdown","41287ce8":"markdown","89275e29":"markdown","9284ffce":"markdown","32344abe":"markdown","cb4debb4":"markdown"},"source":{"56a309ec":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport datetime\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.callbacks import TensorBoard,EarlyStopping","45285331":"# Let us the check the version\nprint(\"Tensorflow version : \", tf.__version__)\n\n# if you set the GPU, it will show here\nprint(\"Number of GPUs available are :\",tf.config.experimental.list_logical_devices(\"GPU\")) \n\n# Set the seed to replicate the result\nfrom numpy.random import seed\nseed(27)\n\n# load the data set\ndataset = keras.datasets.fashion_mnist\n(train_X, train_Y),(test_x,test_y)=dataset.load_data()\n\n# We have 10 labels, so let us create an array to access the class names based on the labler number\nclassNames = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Check the size of the datasets\nprint(\"Train dataset size :\", len(train_X))\nprint(\"Test  dataset size :\", len(test_x))\n\n# Let us check the shape of training image\nprint(\"shape  :\", train_X[0].shape)","370e7e30":"# let us plot some image randomly and it's label\nplt.figure(figsize=(10,10))\nfor i in range(16):\n  plt.subplot(4,4,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  randomNumber = random.randint(0,len(train_X))     \n  plt.imshow(train_X[randomNumber], cmap='gray')\n  plt.xlabel(classNames[train_Y[randomNumber]]) ","2e9843ba":"# We are working on classification problem, so let us normalize the pixel values before passing them to the model\ntrain_X = train_X\/255.0\ntest_x = test_x\/255.0\n\n# Clear the logs from the previous runs if any\n!rm -rf .\/logs\n\n# create the model and assign the leyers\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(28,28))) # input layer \nmodel.add(Dense(512,activation='relu')) # hidden layer\nmodel.add(Dense(10,activation='softmax')) # output layer\nmodel.summary()","38aa1932":"opt = Adam(learning_rate=0.001) # default value is 0.001\nmodel.compile(optimizer=opt,\n              loss=SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n# we are setting time to directory name to distinguish\nlog_dir = \"logs\/fit\/\"+datetime.datetime.now().strftime(\"%Y%M%D-%H%M%S\") \n\ntensorboard_cb = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# We will add \"EarlyStopping\" to the callbacks list for enabling the early stopping of the model\nearlystopping_cb = EarlyStopping(     \n    monitor='val_loss',  # we are monitoring the validation loss, so our model stops training if the validation loss is started increasing\n    min_delta=0, # minimum acceptable threshold for monitor value\n    patience=3, # Number of epochs with no improvement after which training will be stopped\n    verbose=0, \n    mode='auto', # if we set to auto, it will set to minimum in case of loss, it will set to maximum in case of accuracy \n    baseline=None, \n    restore_best_weights=True  # restores the best weight\n)\n\n# We will add the callbacks to change the default behaviour of the model.\nmodel.fit(x=train_X, \n          y=train_Y, \n          epochs=25, \n          validation_split=0.2, \n          callbacks=[tensorboard_cb,earlystopping_cb])","97a128e9":"# we need to call this to load the tensorboard\n%load_ext tensorboard\n%tensorboard --logdir logs\/fit","7c9cce0d":"testLoss, testAccuracy = model.evaluate(test_x, test_y, verbose=2)\nprint(\"Test loss :\", testLoss)\nprint(\"Test accuracy :\", testAccuracy)","847a4434":"predicted = model.predict(test_x)\npredicted.shape","72af89e0":"classNames[np.argmax(predicted[177])]","75fe9354":"plt.imshow(test_x[177])","b0fd5083":"def plotImage(i, predictedArray, trueLabel, img):\n  trueLabel, img = trueLabel[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predictedLabel = np.argmax(predictedArray)\n  if(predictedLabel == trueLabel):\n    color = 'blue'\n  else:\n    color = 'red'\n\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(classNames[predictedLabel],\n                                100*np.max(predictedArray),\n                                classNames[trueLabel]),\n                                color=color)","8e37a672":"# Plot the first X test images, their predicted labels, and the true labels.\n# Color correct prediction in blue and incorrect prediction in red.\nnumRows = 4\nnumCols = 4\nnum_images = numRows*numCols\nplt.figure(figsize=(2*2*numCols, 2*numRows))\nfor i in range(num_images):\n  plt.subplot(numRows, 2*numCols, 2*i+1)\n  plotImage(i, predicted[i], test_y, test_x)\nplt.tight_layout()\nplt.show()","92efb153":"# Save the entire model as a SavedModel.\n!mkdir -p savedModel\nmodel.save('savedModel\/myModel')","1a281da0":"#Loading the model from saved location\nloadedModel = tf.keras.models.load_model('savedModel\/myModel')\n\n# Check its architecture\nloadedModel.summary()","cec3c452":"# **Model Performance**","d8e9e656":"# **Model Training**","8bb4a856":"# **Model Creation**","41287ce8":"# **Tensor Board**","89275e29":"**Load the required libraries**","9284ffce":"# **Exploratory Analyis**","32344abe":"# **Classification of Fashion-MNIST using Deep Learning**\n\n[Fashion-MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist) dataset is used in this article for solving a  classification problem using the Deep Learning.\n\n\nThis dataset has a training set of 60,000 examples and a test set of 10,000 examples.  Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n\n\n**Each training and test examples is assigned to one of the following lables:**\n\n0\t- T-shirt\/top\n\n1\t- Trouser\n\n2\t- Pullover\n\n3\t- Dress\n\n4\t- Coat\n\n5\t- Sandal\n\n6\t- Shirt\n\n7\t- Sneaker\n\n8\t- Bag\n\n9 - Ankle boot\n","cb4debb4":"Compile method links the optimizer, loss function and metrics to the model.\n\nWe will use \"SparseCategoricalCrossentropy\" because this is a multi class problem ( we have to classify into ten different labels in this dataset )\n\nHere we are using the \"Adam\" optimizer. The learning rate in Adam optimizer is adapted as learning unfolds ( in case of Stochastic gradient descent the learning rate does not change during the training ). Adam method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradient. Adam realizes the benifits of both AdaGrad( Adaptive Gradient Algorithm ) and RMSProp ( Root Mean Square Propagation )."}}