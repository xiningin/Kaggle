{"cell_type":{"666f4350":"code","c247d84f":"code","4b428e85":"code","c9558ae4":"code","7ed7d002":"code","faca2fb3":"code","85a93b9a":"code","b0e42ec1":"code","05219ffa":"code","8796d5d1":"code","3b162e0f":"code","a06f2f6a":"code","b3210579":"code","d5fbca05":"code","64bfb128":"code","f53689f3":"code","b54e7135":"code","debe72ca":"code","8284aeca":"code","c5564c7e":"code","b6ba1d91":"markdown","f369351b":"markdown","68c47445":"markdown","12e4c9a7":"markdown","59855003":"markdown","af488a51":"markdown","03dd2f54":"markdown","f4f08746":"markdown","67cee789":"markdown","4fc8627a":"markdown","3888232d":"markdown","b73e6a37":"markdown","1a422e00":"markdown","0482e803":"markdown","3c8795ad":"markdown","dc21cbd6":"markdown","0818507f":"markdown","3991d43f":"markdown"},"source":{"666f4350":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","c247d84f":"dataset = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\nX = dataset.iloc[:, :-1].values # features\ny = dataset.iloc[:, -1].values # target","4b428e85":"X[:5]","c9558ae4":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'), [3])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n# we don't need to del the one dummy varible column in the dataframe for linear regression\n# but for other model we need to use n-1 dummy columns if there are n unique values in a particular column\n# hence to avoid confussion we can del the first column of dummy columns created. ","7ed7d002":"print(X)\n# The dummy variables are always created in the first columns.","faca2fb3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","85a93b9a":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","b0e42ec1":"y_pred = regressor.predict(X_test)","05219ffa":"y_pred","8796d5d1":"np.set_printoptions(precision=2) # only two decimals after point\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","3b162e0f":"import statsmodels.api as sm","a06f2f6a":"# Building the optimal model using Backward Elimination\nimport statsmodels.api as sm\nX = np.append(arr = np.ones((50, 1)).astype(float), values = X, axis = 1)\nprint(X)\nX_opt = np.array(X[:, [0, 1, 2, 3, 4, 5]], dtype=float)\n","b3210579":"X_opt","d5fbca05":"model = sm.OLS(endog = y, exog = X_opt)\nregressor_OLS = model.fit()\nregressor_OLS.summary()","64bfb128":"X_opt = X[:, [0, 1, 3, 4, 5]]\nX_opt = np.array(X[:, [0, 1, 3, 4, 5]], dtype=float)\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","f53689f3":"X_opt = np.array(X[:, [0,3, 4, 5]], dtype=float)\n#X_opt = X[:, [0, 3, 4, 5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","b54e7135":"X_opt = np.array(X[:, [0, 3, 5]], dtype=float)\n#X_opt = X[:, [0, 3, 5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","debe72ca":"\n\nX_opt = np.array(X[:, [0,3]], dtype=float)\n#X_opt = X[:, [0, 3]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","8284aeca":"print(regressor.predict([[ 0, 0, 160000, 130000, 300000]]))","c5564c7e":"print(regressor.coef_)\nprint(regressor.intercept_)","b6ba1d91":"y_pred is an numpy array of one row","f369351b":"**R squared \u2013 It tells about the goodness of the fit. It ranges between 0 and 1. The closer the value to 1, the better it is. It explains the extent of variation of the dependent variables in the model. However, it is biased in a way that it never decreases(even on adding variables).**\n\n\n**Adj Rsquared \u2013 This parameter has a penalising factor(the no. of regressors) and it always decreases or stays identical to the previous value as the number of independent variables increases. If its value keeps increasing on removing the unnecessary parameters go ahead with the model or stop and revert.**\n\n\n**F statistic \u2013 It is used to compare two variances and is always greater than 0. It is formulated as v12\/v22. In regression, it is the ratio of the explained to the unexplained variance of the model.\nAIC and BIC \u2013 AIC stands for Akaike\u2019s information criterion and BIC stands for Bayesian information criterion Both these parameters depend on the likelihood function L.**\n\n\n**Skew \u2013 Informs about the data symmetry about the mean.**\n\n\n**Kurtosis \u2013 It measures the shape of the distribution i.e.the amount of data close to the mean than far away from the mean.**\n\n\n**Omnibus \u2013 D\u2019Angostino\u2019s test. It provides a combined statistical test for the presence of skewness and kurtosis.**\n\n\n**Log-likelihood \u2013 It is the log of the likelihood function.**","68c47445":"# Significance level - Backward elimination\n\nWe have different techniques to find out the features which have the maximum effect on the output.\n\nHere we are going to look at the Backward elimination.\n\nIn this process we need to add one column of ones in the starting of the column.\n\nIn backward elimination we delete the value one by one whose significance level is less.\n\ni.e In general we have a P-value and a significance level\n\nP_value = 1 - (minus) significane level\n\nor in other terms\n\np_value+ significance level = 1\n\nif P_value is high significance level is less.\n\nHence we will be deleating features one by one whose P_value is high which means it has less significance level.\n\nBy eliminating process we get to the values which are of most significance","12e4c9a7":"# Like this notebook then upvote it.\n\n\n# Need to improve it then comment below.\n\n\n# * Enjoy Machine Learning.","59855003":"In the output above we have all the predicted values from model on the left side and the real values on the right side. ","af488a51":"## Importing the libraries","03dd2f54":"## Getting the final linear regression equation with the values of the coefficients","f4f08746":"## Training the Multiple Linear Regression model on the Training set","67cee789":"## Predicting the Test set results","4fc8627a":"### Making a single prediction (for example the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = 'California')","3888232d":"For better understanding of current noebook for beginners go through the links:\n\n [1.1 Data Preprocessing](http:\/\/www.kaggle.com\/saikrishna20\/data-preprocessing-tools)\n\n\n[1.2 Simple linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-2-simple-linear-regression) \n\nIt basically tells u about the preprocessing & Linear Regression which will help u in understanding this notebook better","b73e6a37":"# 1.3 Multiple Linear Regression & Backward Elimination.","1a422e00":"Therefore, the equation of our multiple linear regression model is:\n\n$$\\textrm{Profit} = 86.6 \\times \\textrm{Dummy State 1} - 873 \\times \\textrm{Dummy State 2} + 786 \\times \\textrm{Dummy State 3} - 0.773 \\times \\textrm{R&D Spend} + 0.0329 \\times \\textrm{Administration} + 0.0366 \\times \\textrm{Marketing Spend} + 42467.53$$\n\n**Important Note:** To get these coefficients we called the \"coef_\" and \"intercept_\" attributes from our regressor object. Attributes in Python are different than methods and usually return a simple value or an array of values.","0482e803":"## Importing the dataset","3c8795ad":"## Splitting the dataset into the Training set and Test set","dc21cbd6":"## Encoding categorical data","0818507f":"the varibale whose p value is greater of all and is more than significance level 0.05 is deleted as it means it has less significance on the outcome.","3991d43f":"Therefore, our model predicts that the profit of a Californian startup which spent 160000 in R&D, 130000 in Administration and 300000 in Marketing is $ 181566,92.\n\n**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array. Simply put:\n\n$1, 0, 0, 160000, 130000, 300000 \\rightarrow \\textrm{scalars}$\n\n$[1, 0, 0, 160000, 130000, 300000] \\rightarrow \\textrm{1D array}$\n\n$[[1, 0, 0, 160000, 130000, 300000]] \\rightarrow \\textrm{2D array}$\n\n**Important note 2:** Notice also that the \"California\" state was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the second row of the matrix of features X, \"California\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, not the last three ones, because the dummy variables are always created in the first columns."}}