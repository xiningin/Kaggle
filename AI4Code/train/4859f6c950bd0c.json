{"cell_type":{"1bbc0291":"code","733bfde4":"code","5fcc901b":"code","ad68d61a":"code","1ec9aed7":"code","7b4669ce":"code","1e3f7ce4":"code","6895c2d8":"code","014b68d0":"code","7e16587b":"code","a52527c5":"code","8652a504":"code","3e9d7902":"code","2305e8f8":"code","c1339c6f":"code","4a09611e":"code","ee3e230a":"code","28f0d44a":"code","974b69e0":"code","2845c022":"code","cdf3ad1f":"code","73acc97f":"code","b9b80db0":"code","1664bbc5":"code","a19a7147":"code","aa305f4b":"code","a7656d03":"code","8dcbb6d2":"code","81f4e2b3":"code","cf871a2c":"code","963a6d22":"code","a4945e69":"code","887c6902":"code","d10e4f03":"code","0bb40325":"code","32bb65ae":"code","22892e3c":"code","e0e254bc":"code","db1b9bcd":"code","a26356ea":"code","4fc69d82":"code","96af9e6f":"code","fcdbee7a":"code","e4edf809":"code","7a86b889":"code","e26e3d08":"code","0d9b6caf":"code","090eccc9":"code","3d74690e":"code","ef165692":"code","2b096084":"code","51132920":"code","083eec48":"code","d6cc4563":"code","5cdcc73b":"code","95766aa0":"code","ec31cb0e":"code","e86fde17":"code","41805a5a":"code","5646c09a":"code","ac4031cf":"code","b72640b1":"code","2b42e3ed":"code","e5a4760d":"code","88792ab7":"code","9752c5b2":"code","88c9e8e9":"code","81bd6bfd":"markdown","77439b45":"markdown","edc469f1":"markdown","fad9d9fe":"markdown","3fe3cc5e":"markdown","eb68d245":"markdown","ad5a5857":"markdown","fb84737e":"markdown","2e44b2ef":"markdown","ce878e80":"markdown","8cb63e0f":"markdown","294db7d8":"markdown","f646587e":"markdown","cc6355e6":"markdown","5118b8d8":"markdown","7d965abe":"markdown","a0e287a6":"markdown","88151d25":"markdown","41352c8c":"markdown","9953325b":"markdown","ce851f4e":"markdown","ea77e5fa":"markdown","7396d5d0":"markdown","915569e9":"markdown","fbb2ae4d":"markdown","261dc087":"markdown","593df177":"markdown","c1e566da":"markdown","a1395c16":"markdown","9c381aef":"markdown","405dad73":"markdown","e9258e9e":"markdown","495c0f6e":"markdown","11047489":"markdown","139e92bb":"markdown","98b8f7f6":"markdown","004ebcbd":"markdown"},"source":{"1bbc0291":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","733bfde4":"df = pd.read_csv(\"..\/input\/uber-and-lyft-dataset-boston-ma\/rideshare_kaggle.csv\")\ndf.head()","5fcc901b":"x = df[\"temperatureMin\"]\ny = df[\"temperatureMax\"]\nplt.scatter(x, y, c =\"blue\")\nplt.show()","ad68d61a":"from sklearn.linear_model import LinearRegression\nX= np.array(df[\"temperatureMin\"]).reshape(-1, 1)\ny= df[\"temperatureMax\"]\nreg = LinearRegression().fit(X, y)\nreg","1ec9aed7":"reg.score(X,y)","7b4669ce":"reg.get_params(deep=True)","1e3f7ce4":"reg.predict(np.array([32]).reshape(-1,1))","6895c2d8":"pd. get_option(\"display.max_columns\")\ndf.head()","014b68d0":"df.info()","7e16587b":"df.describe()","a52527c5":"columns = [\"id\",\"month\",\"day\",\"hour\",\"distance\",\"surge_multiplier\",\"cab_type\",\"latitude\",\"longitude\",\"temperature\",\"precipIntensity\",\"precipProbability\",\"humidity\",\"windSpeed\",\"windGust\",\"dewPoint\",\"pressure\",\"ozone\",\"price\"]\nanalysis_df = df[columns]","8652a504":"analysis_df.head()","3e9d7902":"! pip install chart_studio","2305e8f8":"import plotly.express as px","c1339c6f":"df =analysis_df.groupby(by=[\"month\"]).size().reset_index(name=\"counts\")\ndf","4a09611e":"px.bar(data_frame=df, x=\"month\", y=\"counts\", color=\"month\", barmode=\"group\")","ee3e230a":"df =analysis_df.groupby(by=[\"day\"]).size().reset_index(name=\"counts\")\ndf","28f0d44a":"px.bar(data_frame=df, x=\"day\", y=\"counts\", color=\"day\", barmode=\"group\")","974b69e0":"df =analysis_df.groupby(by=[\"hour\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"hour\", y=\"counts\", color=\"hour\", barmode=\"group\")","2845c022":"df =analysis_df.groupby(by=[\"month\",\"cab_type\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"month\", y=\"counts\", color=\"cab_type\", barmode=\"group\")","cdf3ad1f":"df =analysis_df.groupby(by=[\"day\",\"cab_type\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"day\", y=\"counts\", color=\"cab_type\", barmode=\"group\")","73acc97f":"df =analysis_df.groupby(by=[\"hour\",\"cab_type\"]).size().reset_index(name=\"counts\")\npx.bar(data_frame=df, x=\"hour\", y=\"counts\", color=\"cab_type\", barmode=\"group\")","b9b80db0":"# import matplotlib.pyplot as plt\n# from mpl_toolkits.basemap import Basemap","1664bbc5":"# plt.figure(figsize=(8, 8))\n# m = Basemap(projection='ortho', resolution=None, lat_0=analysis_df[\"latitude\"].mean(), lon_0=analysis_df[\"longitude\"].mean())\n# m.bluemarble(scale=0.5);","a19a7147":"# lat = analysis_df[\"latitude\"].values\n# lon = analysis_df[\"longitude\"].values\n# price = analysis_df[\"price\"].values\n# # temperature = analysis_df[\"temperature\"].values","aa305f4b":"# fig = plt.figure(figsize=(8, 8))\n# m = Basemap(projection='lcc', resolution=\"c\", \n#             lat_0=analysis_df[\"latitude\"].mean(),\n#             lon_0=analysis_df[\"longitude\"].mean(),\n#             width=1E6, height=1.2E6)\n# m.shadedrelief()\n# m.drawcoastlines(color='gray')\n# m.drawcountries(color='gray')\n# m.drawstates(color='gray')\n\n# # 2. scatter city data, with color reflecting population\n# # and size reflecting area\n# m.scatter(lon, lat, latlon=True,\n#           c=price, s=temperature,\n#           cmap='Reds', alpha=0.5)\n\n# # 3. create colorbar and legend\n# plt.colorbar(label=r'$\\rm price$')\n# plt.clim(3, 7)\n\n# # make legend with dummy points\n# for a in [15, 40, 57]:\n#     plt.scatter([], [], c='k', alpha=0.5, s=a,\n#                 label=str(a) + ' km$^2$')\n# plt.legend(scatterpoints=1, frameon=False,\n#            labelspacing=1, loc='lower left');","a7656d03":"analysis_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",\n    s=analysis_df['price'], label=\"price\",\n    c=\"temperature\", cmap=plt.get_cmap(\"jet\"),\n    colorbar=True, alpha=0.4, figsize=(10,7),\n)\nplt.legend()\nplt.show()","8dcbb6d2":"import plotly.figure_factory as ff","81f4e2b3":"import random\nrandom.seed(100)","cf871a2c":"viz_df = analysis_df.sample(frac =.25)\n","963a6d22":"fig = ff.create_distplot([viz_df[\"price\"].dropna().values.tolist()], group_labels=['distplot'])\nfig.show()","a4945e69":"plt.figure(figsize=(16,8))\nplt.scatter(x=analysis_df[\"distance\"], y=analysis_df[\"price\"], s=analysis_df[\"surge_multiplier\"], alpha=0.5)\nplt.show()","887c6902":"surge_data = analysis_df[analysis_df[\"surge_multiplier\"]>2.0]\nplt.figure(figsize=(16,8))\nplt.scatter(x=surge_data[\"distance\"], y=surge_data[\"price\"], marker='o', c='r',s=surge_data[\"surge_multiplier\"], alpha=0.5)\nplt.show()","d10e4f03":"f = plt.figure(figsize=(19, 15))\nplt.matshow(analysis_df.corr(), fignum=f.number)\nplt.xticks(range(analysis_df.select_dtypes(['number']).shape[1]), analysis_df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\nplt.yticks(range(analysis_df.select_dtypes(['number']).shape[1]), analysis_df.select_dtypes(['number']).columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16);","0bb40325":"import seaborn as sns\nsns.set(style = \"darkgrid\")\n\nfig = plt.figure(figsize=(16,9))\nax = fig.add_subplot(111, projection = '3d')\n\nx = surge_data['distance']\ny = surge_data['price']\nz = surge_data['surge_multiplier']\n\nax.set_xlabel(\"distance\")\nax.set_ylabel(\"price\")\nax.set_zlabel(\"surge_multiplier\")\n\nax.scatter(x, y, z)\n\nplt.show()","32bb65ae":"df = analysis_df[(analysis_df[\"hour\"]>=22) | (analysis_df[\"hour\"]<=4)]\nplt.figure(figsize=(16,9))\nsns.scatterplot(data = df, x = \"distance\",\n                y = \"price\", hue = \"cab_type\", size = \"surge_multiplier\")\nplt.show()","22892e3c":"\nsns.scatterplot(data = surge_data, x = \"distance\",\n                y = \"price\", hue = \"cab_type\", size = \"surge_multiplier\")\nplt.show()","e0e254bc":"linear_df = analysis_df[[\"distance\",\"surge_multiplier\",\"price\"]]","db1b9bcd":"linear_df.dropna(inplace = True)","a26356ea":"#separate the other attributes from the predicting attribute\nx = linear_df.drop(\"price\",axis=1)\n#separte the predicting attribute into Y for model training \ny = linear_df[\"price\"]","4fc69d82":"# importing train_test_split from sklearn\nfrom sklearn.model_selection import train_test_split\n# splitting the data\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","96af9e6f":"# importing module\nfrom sklearn.linear_model import LinearRegression\n# creating an object of LinearRegression class\nLR = LinearRegression()\n# fitting the training data\nLR.fit(x_train,y_train)","fcdbee7a":"y_prediction =  LR.predict(x_test)","e4edf809":"# importing r2_score module\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n# predicting the accuracy score\nscore=r2_score(y_test,y_prediction)\nprint('r2 socre is ',score)\nprint('mean_sqrd_error is==',mean_squared_error(y_test,y_prediction))\nprint('root_mean_squared error of is==',np.sqrt(mean_squared_error(y_test,y_prediction)))","7a86b889":"import statsmodels.api as sm\nfrom scipy import stats\n\nX2 = sm.add_constant(x)\nest = sm.OLS(y, X2)\nest2 = est.fit()\nprint(est2.summary())","e26e3d08":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nprint(tf.__version__)","0d9b6caf":"train_dataset = linear_df.sample(frac=0.8, random_state=0)\ntest_dataset = linear_df.drop(train_dataset.index)","090eccc9":"import seaborn as sns\nsns.pairplot(train_dataset,diag_kind='kde')","3d74690e":"train_features = train_dataset.copy()\ntest_features = test_dataset.copy()\n\ntrain_labels = train_features.pop('price')\ntest_labels = test_features.pop('price')","ef165692":"normalizer = preprocessing.Normalization(axis=-1)\nnormalizer.adapt(np.array(train_features))\nprint(normalizer.mean.numpy())","2b096084":"linear_model = tf.keras.Sequential([\n    normalizer,\n    layers.Dense(units=1)\n])","51132920":"linear_model.predict(train_features[:10])","083eec48":"linear_model.layers[1].kernel","d6cc4563":"linear_model.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')","5cdcc73b":"%%time\nhistory = linear_model.fit(\n    train_features, train_labels, \n    epochs=10,\n    # suppress logging\n    verbose=2,\n    # Calculate validation results on 20% of the training data\n    validation_split = 0.2)","95766aa0":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","ec31cb0e":"import matplotlib.pyplot as plt\ndef plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [MPG]')\n  plt.legend()\n  plt.grid(True)","e86fde17":"plot_loss(history)","41805a5a":"def plot_data(x, y):\n  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n  plt.plot(x, y, color='k', label='Predictions')\n  plt.xlabel('Horsepower')\n  plt.ylabel('MPG')\n  plt.legend()","5646c09a":"!pip install pyspark","ac4031cf":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SQLContext\nsc= SparkContext()\nsqlContext = SQLContext(sc)\ndf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('..\/input\/uber-and-lyft-dataset-boston-ma\/rideshare_kaggle.csv')\ndf.take(1)","b72640b1":"df.cache()\ndf.printSchema()","2b42e3ed":"from pyspark.sql.types import IntegerType\ndf = df.withColumn(\"price\", df[\"price\"].cast(IntegerType()))","e5a4760d":"from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols = [\"distance\",\"surge_multiplier\"], outputCol = 'features')\nvhouse_df = vectorAssembler.transform(df)\nvhouse_df = vhouse_df.select(['features',\"price\"])\nvhouse_df.show(3)","88792ab7":"vhouse_df.printSchema()\n","9752c5b2":"splits = vhouse_df.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]","88c9e8e9":"# from pyspark.ml.regression import LinearRegression\n# lr = LinearRegression(featuresCol = 'features', labelCol=\"price\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n# lr_model = lr.fit(train_df)\n# print(\"Coefficients: \" + str(lr_model.coefficients))\n# print(\"Intercept: \" + str(lr_model.intercept))\n","81bd6bfd":"# Profiles of The Authors\n* [SAMPRIT CHATTERJEE](http:\/\/people.stern.nyu.edu\/schatter\/cv8psc.htm)","77439b45":"# Mean Squared Error (MSE)\nThe most common metric for regression tasks is MSE. It has a convex shape. It is the average of the squared difference between the predicted and actual value. Since it is differentiable and has a convex shape, it is easier to optimize.\nMSE penalizes large errors.\n\n# Mean Absolute Error (MAE)\nThis is simply the average of the absolute difference between the target value and the value predicted by the model. Not preferred in cases where outliers are prominent.\nMAE does not penalize large errors.","edc469f1":"# Different Terms\n\n## $R^2$ (coefficient of determination)\n![image.png](attachment:4207271b-0c0e-43b6-b5f4-ee05106d877e.png)\n\nThis equation above said that the variability in target can be analyzed by regression in two mutually exclusive terms. \n* One part is the variability of leftover of regression analysis.\n* The other part is the variability included in regression. \n\nThis equation suggests the importance of $R^2$ analysis.\n![image.png](attachment:f8a0e7a7-3eff-477a-8810-72f18f6df490.png)\n\nIt estimates the population proportion of variability in y accounted for by the best linear combination of the predictors. Values closer to 1 indicate a good deal of predictive\npower of the predictors for the target variable, while values closer to 0 indicate\nlittle predictive power. \n\nIt simply measures the corelation between the predicted value and the actual value.\n\nOne word of causion Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts makers of the model to add even more variables. This is called overfitting and can return an unwarranted high R-squared value. **Adjusted R-squared** is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.\n![image.png](attachment:ad92a617-34ee-49ad-9c5f-0c079e4b59bd.png)\n\nIt suggest that unless the p value is too high. both are going to be more or less same. **Adjusted R-squared** is a trade-off between  the complexity of the model and  the strength of the fit.\n\nThe only parameter left unaccounted for in the estimation scheme is the variance of the errors $\\sigma^2$. An unbiased estimate is provided by the residual mean square,\n![image.png](attachment:566ce7b1-a7f5-43c4-9faf-76076749f930.png)\n\nRecallingh that $\\sigma$ will be normally distributed. This means that, roughly speaking, 95% of the time an observed y value falls within \u00b12$\\sigma$ of the expected response.\n","fad9d9fe":"## Geolocation Analysis","3fe3cc5e":"# Coding Problem -1 \n\nTest the Understanding\n\n* predicting Maxtemperature from Min temperature values. To see if they are correlative.","eb68d245":"# Linear Regression\nIn linear regression we will require 2 set of variables. One is the dependent variable(termed as $\\widehat{y}$) and second the independent variable(termed as $\\widehat{x}$). Mostly the task relies upon correctly predicting the $\\widehat{y}$ with the help of a combination of $\\widehat{x}$.\n\nTypically, a regression analysis is used for one (or more) of three purposes: \n1. modeling the relationship between x and y;\n2. prediction of the target variable (forecasting); \n3. and testing of hypotheses.   *taken from [here](http:\/\/docentes.uto.edu.bo\/jzamoranoe\/wp-content\/uploads\/Chatterjee2013_Regresion.pdf)*\n\n","ad5a5857":"It is more or less uniform. Now analyze the same with uber\/Lyft to see the market cap of both the companies for this time period.","fb84737e":"# Visualization of Corelation","2e44b2ef":"Inference:","ce878e80":"# Price Analysis","8cb63e0f":"# Important Note About Regression\n* The word **associated** \u2014 we cannot say that a change in the target variable is caused by a change in the predictor, only that they are associated with each other. That is, correlation does not imply causation.\n\n* The regression coefficient does not say anything about that marginal relationship. That means we can have an equation where your expendeture increases with your income. But it is certainly not so. If interested in relationship with target and specific variable. Run regression alone with the variable.\n ","294db7d8":"## Linear Model\n","f646587e":"# Analysis Of data \n## sklearen framework","cc6355e6":"## Meaning of $R^2$\nIf R\u00b2 is high (say 1), then the model represents the variance of the dependent variable.\nIf R\u00b2 is very low, then the model does not represent the variance of the dependent variable and regression is no better than taking the mean value, i.e. you are not using any information from the other variables.\nA Negative R\u00b2 means you are doing worse than the mean value. It can have a negative value if the predictors do not explain the dependent variables at all such that RSS ~ TSS.","5118b8d8":"This data here is only a snapshot in time. The data might and will change with time so statistics is not truth rather it is a random process being studied.","7d965abe":"## Visualization\n### 1. Time Trend Analysis","a0e287a6":"#  CHECKING ASSUMPTIONS USING RESIDUAL PLOTS\n\n1. A plot of the **residuals versus the fitted values**. This plot should have no pattern to it; that is, no structure should be apparent. Certain kinds of structure indicate potential problems:\n>(a) A point (or a few points) isolated at the top or bottom, or left or right. In addition, often the rest of the points have a noticeable \"tilt\" to them.These isolated points are unusual points, and can have a strong effect on the regression. They need to be examined carefully, and possibly\nremoved from the data set.<br>\n>(b) An impression of different heights of the point cloud as the plot is examined from left to right. This indicates potential heteroscedasticity (nonconstant variance). \n\n2. Plots of the residuals versus each of the predictors. Again, a plot with no apparent structure is desired. \n\n3. If the data set has a time structure to it, residuals should be plotted versus time. Again, there should be no apparent pattern. If there is a cyclical structure, this indicates that the errors are not uncorrelated, as they are supposed to be (that is, there is potentially autocorrelation in the errors). \n\n4. A normal plot of the residuals. This plot assesses the apparent normality of the residuals, by plotting the observed ordered residuals on one axis and the expected positions (under normality) of those ordered residuals on the other. The plot should look like a straight line (roughly). Isolated points once again represent unusual observations, while a curved line indicates that the errors are probably not normally distributed, and tests and intervals might not be trustworthy. \n","88151d25":"Inference:","41352c8c":"# Dealing With Categorical Variable to predict Continous Variable\nthis is a\nquestion of the comparison of means.\nwe focus on models with\nonly categorical predictors, which are termed analysis of variance (ANOVA)\nmodels.","9953325b":"Wow, we don't have any booking data for almost 12 days. It might be due to some reasons or the dataset we are working with is randomized in these way to deal with. ","ce851f4e":"# Model Building\n1. Including unnecessary predictors in the model (what is sometimes called overfitting) complicates descriptions of the process. Using such models tends to lead to poorer predictions because of the additional unnecessary noise. Further, a more complex representation of the true regression relationship is less likely to remain stable enough to be useful for future prediction than is a simpler one. \n\n2. . Omitting important effects (underfitting) reduces predictive power, biases estimates of effects for included predictors, and results in less understanding of the process being studied. \n\n3. Violations of assumptions should be addressed, so that least squares estimation is justified.","ea77e5fa":"# Practical Guide To Linear Regression\n\n### (MetaData of the NoteBook): Statistics and Machinelearning is not just forking and pasting some algorithms to get a job.\n\n### `I strongly believe by understanding these concepts clearly anyone can master the topics. Hence I tried to aggregate all the relevent information under these Notebooks. And gives it a nice R&D development touch and tried to build a scalable Engineering solution out of the Notebook.`\nI hope you will enjoy the notebook. \n<br>\n[Satyajit Maitra](https:\/\/www.linkedin.com\/in\/satyajit-maitra-b51b58150\/)","7396d5d0":"so there are almost 46 columns with numerical value we will hand-picked a few to explain a regression model to predict **price**.","915569e9":"# ESTIMATION USING LEAST SQUARES\nThe true regression function represents the expected relationship between the\ntarget and the predictor variables, which is unknown. A primary goal of a\nregression analysis is to estimate this relationship, or equivalently, to estimate\nthe unknown parameters $\\beta$. This requires a data-based rule, or criterion,\nthat will give a reasonable estimate.\n\nThe target of least-square method is to minimize this fucntion:\n> ![image.png](attachment:e9035519-a4df-4ac9-a639-03e1aac7f249.png)\nThe difference between the fitted and actual value is termed as residual.\n\nAnalytically the solution for the regression coefficient can be found from this equation:\n> ![image.png](attachment:7efcceaf-beb0-4b16-a537-7fa30bc99502.png)\n\nIt implise that our Hypothesis for analytical approach of linear regression:\n> ![image.png](attachment:1645281e-5112-49fc-bd0f-02e1dad308ee.png)\n\nNow to be able to use certain analytical approach into linear regression we do have to follow certian assumptions :\n> 1. The expected value of the errors is zero (E{ei) \u2014 0 for all i). That is, it\ncannot be true that for certain observations the model is systematically\ntoo low, while for others it is systematically too high. A violation of this\nassumption will lead to difficulties in estimating $\\beta$\n\n> 2. The variance of the errors is constant (V(ei) = $\\sigma^2$\n for all i). That is,\nit cannot be true that the strength of the model is more for some parts\nof the population (smaller $\\sigma$) and less for other parts (larger $\\sigma$). This\nassumption of constant variance is called **homoscedasticity**, and its violation (nonconstant variance) is called **heteroscedasticity**. A violation of\nthis assumption means that the least squares estimates are not as efficient\nas they could be in estimating the true parameters, and better estimates\nare available.\n\n> 3. The errors are normally distributed. This is needed if we want to construct any confidence or prediction intervals, or hypothesis tests, which we usually do. If this assumption is violated, hypothesis tests and confidence and prediction intervals can be very misleading. \n\n> 4. The errors are uncorrelated with each other. That is, it cannot be true\nthat knowing that the model underpredicts y (for example) for one particular observation says anything at all about what it does for any other observation. This violation most often occurs in data that are ordered in\ntime (time series data), where errors that are near each other in time are\noften similar to each other (such time-related correlation is called autocorrelation). Violation of this assumption can lead to very misleading\nassessments of the strength of the regression.","fbb2ae4d":"# References:\n## Books I am following:\n* [Handbook of Regression Analysis](http:\/\/docentes.uto.edu.bo\/jzamoranoe\/wp-content\/uploads\/Chatterjee2013_Regresion.pdf)\n\n* [Linear Regression Analysis](http:\/\/www.manalhelal.com\/Books\/geo\/LinearRegressionAnalysisTheoryandComputing.pdf)\n\n* [LINEAR MODELS IN\nSTATISTICS](http:\/\/www.utstat.toronto.edu\/~brunner\/books\/LinearModelsInStatistics.pdf)\n\n## Notes I am following:\n* https:\/\/www.mit.edu\/~6.s085\/notes\/lecture3.pdf\n* http:\/\/web.nchu.edu.tw\/~numerical\/course1012\/ra\/Applied_Regression_Analysis_A_Research_Tool.pdf\n\n* https:\/\/rufiismada.files.wordpress.com\/2012\/02\/regression__linear_models_in_statistics.pdf\n\n* http:\/\/www.stat.cmu.edu\/~hseltman\/309\/Book\/chapter9.pdf\n* https:\/\/www.coursera.org\/learn\/machine-learning\/home\/week\/2\n\n\n","261dc087":"# HYPOTHESIS TESTS\nThere are two types of hypothesis tests related to the regression coefficients\nof immediate interest.\n1. Do any of the predictors provide predictive power for the target variable? This is a test of the overall significance of the regression,\n![image.png](attachment:41729bf8-db29-423d-9220-df7fefbce7cc.png)\n\n2. Given the other variables in the model, does a particular predictor provide additional predictive power? This corresponds to a test of the significance of an individual coefficient, \n![image.png](attachment:279399c7-6dda-421f-b643-9489a0ebae09.png)\n","593df177":"We will work with a set of samplesin these analysis.","c1e566da":"## Check the $R^2$","a1395c16":"so we have more orders in the month of december. good . Now let us move to visualize the pattern of order per day basis. ","9c381aef":"see a special scatter plot to visualize the different effect of surging","405dad73":"Our predictor is 60% accurate in understanding the target. ","e9258e9e":"So one thing is very clear in this aspet that uber is far ahead of lyft in getting orders in all aspects of time.","495c0f6e":"Though this notebook is all about linear regression and i have choosen only the numeric variable (ordinal and ratio) as independent variables. Still we are going to plot some graphs to analyze the data better. ","11047489":"# The Model\nThe data consist of n sets of observations ${x_{1i}, x_{2i},..., x_{ni}, y_{i}}$, which represent a random sample from a larger population. It is assumed that these\nobservations satisfy a linear relationship,\n$$ y_{i} = \\beta_{0} + \\beta_{1}{x_{1i}+ \\beta_{1}{x_{1i}+ \\varepsilon _{i} $$\n\n\nIt is important to recognize that this, or any statistical model, is not\nviewed as a true representation of reality; rather, the goal is that the model\nbe a useful representation of reality. A model can be used to explore the relationships between variables and make accurate forecasts based on those relationships even if it is not the \"truth.\" \n\nFurther, any statistical model is only\ntemporary, representing a provisional version of views about the random process being studied. Models can, and should, change, based on analysis using\nthe current model, selection among several candidate models.","139e92bb":"# Important terms:\n* Cook's distance D is also equivalent to the change in the predicted values from the full data and the fitted value obtained by deleting the observation of outlier.\n* masking effect: This occurs when several unusual observations are all in the same region of the (X, y) space. When this happens, the diagnostics, which all focus on changes in the regression when a single point is deleted, fail, since the presence of the other nearby unusual observations means that the fitted regression changes very little if one is omitted.\n\n* One benefit of taking logs of the response variable is that it can address certain kinds of heteroscedasticity. A situation where the variability of the response variable reflects multiplicative, rather than additive errors, can be accommodated by taking logs. Consider a true relationship that has the form ","98b8f7f6":"Next Step add Apache Spark and explain the whole notebook.","004ebcbd":"# Different Hypothesis\n\nA predictor that does not add significantly to model fit should have an estimated slope coefficient that is not significantly different from 0, and is thus identified by a small t-statistic. t-statistics are not effective in identifying important predictors when the two variables are highly correlated.\n\nColinearity is measureds with F-statistics The F-statistic has the form\n![image.png](attachment:4778cf51-006a-4fa9-bbd7-4559a9e1ebe6.png)\n\nA related issue is that of\ncollinearity (sometimes somewhat redundantly referred to as multicollinearity), which refers to the situation when (some of) the predictors are highly\ncorrelated with each other. Predicting variables that are highly correlated\nwith each other can lead to instability in the regression coefficients, and as\na result the t-statistics for the variables can be deflated. \n\nNote that while collinearity can have a\nlarge effect on regression coefficients and associated t-statistics, it does not\nhave a large effect on overall measures of fit like the overall F-test or R2.\n\nSolution to this is, A diagnostic to determine this in general is the variance inflation factor\n(VIF) for each predicting variable.\nThere are two terms important related to model selection.\nAIC and BIC choose the one with minimum value.\n\nA final way of comparing models is from a directly predictive point of\nview. Since a rough 95% prediction interval is \u00b12$\\sigma$, a useful model from a\npredictive point of view is one with small $\\sigma$, suggesting choosing a model that\nhas small a while still being as simple as possible."}}