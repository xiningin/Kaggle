{"cell_type":{"48896da6":"code","c14be4ef":"code","57a04baa":"code","9d9d3f5e":"code","31d287a5":"code","ccff9159":"code","dbd1b518":"code","b36de9ed":"code","8169a3e9":"code","251bfdcf":"code","3716c36a":"code","89bcd962":"code","f3a2f371":"code","4faecabd":"code","b39f6ebd":"code","03b2640e":"code","9ba25de3":"code","ab6573f9":"code","0a63c595":"code","07fd24e1":"code","6f969b50":"code","4e6aa334":"code","3f2543f5":"code","e724644c":"code","ccf5e684":"code","af9df187":"code","f8b8f45c":"code","1bad9c7c":"code","1a81614b":"code","7b6402c0":"markdown","a2f4ae0f":"markdown","d9c99937":"markdown","ec0ff487":"markdown","45b88218":"markdown","ad73b763":"markdown","76980059":"markdown","927894a8":"markdown","b6dd20aa":"markdown","7b4bb644":"markdown","5642b29f":"markdown","09bd8928":"markdown","dd8cc8d4":"markdown","ead98163":"markdown","f8e4bd92":"markdown","08056260":"markdown","c675f0f5":"markdown","6a388b7d":"markdown"},"source":{"48896da6":"import pandas as pd\n\ntrain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv' # this is the path to the Iowa data that you will use\npd_train = pd.read_csv(train_file_path)\n\ntest_file_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv' # this is the path to the Iowa data that you will use\npd_test = pd.read_csv(test_file_path)\n\n# Run this code block with the control-enter keys on your keyboard. Or click the blue botton on the left\nprint('Input files has been read !!!')","c14be4ef":"pd_train.head()","57a04baa":"pd_fields = pd_train.columns\npd_fields","9d9d3f5e":"#['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType',\n#'HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual',\n#'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu',\n#'GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']\ncategorical_fields = pd_train.columns[pd_train.dtypes == 'object'].values\ncategorical_fields","31d287a5":"na_values = pd_train.isna().sum()\nna_values[na_values != 0]","ccff9159":"na_test_values = pd_test.isna().sum()\nna_test_values[na_test_values != 0]","dbd1b518":"import numpy as np","b36de9ed":"# Before transforming Categorical to Numerical values, assign a value for NA values\nfor field in categorical_fields:\n    pd_train[field].fillna('---',inplace=True)\n    pd_test[field].fillna('---',inplace=True)","8169a3e9":"all_possible_values ={}\ndef convertToNumeric(field):\n    inverse_map = {}\n    \n    all_possible_values[field] = np.unique(np.concatenate([pd_train[field].unique(), pd_test[field].unique()]))\n    for value in enumerate(all_possible_values[field]):\n        inverse_map[value[1]] = value[0]    \n    \n    pd_train[field] = pd_train[field].map(inverse_map)\n    pd_test[field]  = pd_test[field].map(inverse_map)\n    \n# Now, transform to Numerical values\nfor field in categorical_fields:\n    convertToNumeric(field)\n    \n#all_possible_values    ","251bfdcf":"numerical_fields = set(pd_fields) - set(categorical_fields)\n\n# Assign mean value to NA provided at Numerical fields\nfor field in numerical_fields:\n    # print(field)\n    pd_train[field].fillna(pd_train[field].mean(),inplace=True)\n    if field != 'SalePrice':\n        pd_test [field].fillna(pd_train[field].mean(),inplace=True)","3716c36a":"na_values = pd_train.isna().sum()\nna_values[na_values != 0]","89bcd962":"na_test_values = pd_test.isna().sum()\nna_test_values[na_test_values != 0]","f3a2f371":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","4faecabd":"# Train\nx_values = pd_train.drop(['Id','SalePrice'], axis=1).values\ny_values = pd_train['SalePrice'].values\n\nprint (\"x_values.shape = \", x_values.shape)\nprint (\"y_values.shape = \", y_values.shape)\n\n# Test\nx_test = pd_test.drop('Id',axis=1).values\n\nprint (\"x_test.shape = \", x_test.shape)","b39f6ebd":"x_scaler = StandardScaler()\nx_scaler.fit(x_values)\nx_values_scaled = x_scaler.transform(x_values)\nx_test_scaled   = x_scaler.transform(x_test)\n\n#mean_sale_price = y_values.mean()\n\ny_values_scaled = y_values\n\n#y_scaler = StandardScaler()\n#y_scaler.fit(y_values)\n#y_values_scaled = y_scaler.transform(x_values)\n#y_test_scaled   = y_scaler.transform(y_test)","03b2640e":"x_train, x_val, y_train, y_val = train_test_split(x_values_scaled, y_values_scaled, test_size = 0.1)\n\nprint (\"x_train.shape = \", x_train.shape)\nprint (\"y_train.shape = \", y_train.shape)\nprint (\"x_val.shape = \", x_val.shape)\nprint (\"y_val.shape = \", y_val.shape)","9ba25de3":"import tensorflow as tf\nimport matplotlib.pyplot as plt","ab6573f9":"# Define Dense Neuronal Network in TensorFlow\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(100,activation='relu'))\nmodel.add(tf.keras.layers.Dense(50,activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(50,activation='relu'))\nmodel.add(tf.keras.layers.Dense(10,activation='relu'))\nmodel.add(tf.keras.layers.Dense(10,activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))","0a63c595":"def tensorflow_loss_function_test():\n    # TensorFlow tests to learn how to implement 'custom_loss_function'\n    y_true = tf.constant([1.,2.,3,40000,5,6,7,8,9,100000000])\n    y_pred = y_true * 2\n\n    y_diff = tf.abs(tf.math.log(1 + y_true) - tf.math.log(1 + y_pred))  \n    print(\"y_diff_count = \", y_diff.shape)\n    print(\"\")\n    print(\"val = \", tf.keras.backend.get_value(y_diff))\n    print(\"\")\n    sorted_value, sorted_indexes = tf.nn.top_k(tf.negative(y_diff), k = tf.cast(0.7 * tf.cast(tf.size(y_diff),tf.float32),tf.int32), sorted=True)\n    sorted_value = tf.negative(sorted_value)\n    print(\"sorted_diff_count = \", sorted_value.shape)\n    print(\"sorted_val = \", tf.keras.backend.get_value(sorted_value))","07fd24e1":"def custom_loss_function(y_true, y_pred):    \n    #print(\"y_true = \", tf.keras.backend.get_value(y_true))\n    y_diff = tf.abs(tf.math.log(1+y_true) - tf.math.log(1+y_pred))    \n    return tf.reduce_sum(tf.square(y_diff)) \/ tf.cast(tf.size(y_diff), tf.float32)\n\n    # START : Test \n    \n    #num_values = tf.size(y_diff)\n    #print(\"y_diff_size = \", y_diff.shape)\n    \n    # Do not use the 5% percentile values with greater Error to avoid Outliers !!!\n    # Use negative to work with minimum values, avoid maximum values for Outlayers ! \n    # num_values_for_loss = tf.cast(0.95 * tf.cast(tf.size(y_diff),tf.float32),tf.int32)\n    #num_values_for_loss = 10\n    #y_diff_sorted_trunc, indexes_sorted = tf.nn.top_k(tf.negative(y_diff), k = num_values_for_loss, sorted=True)\n    # Non necessary to change sign, because later is 'squared'\n    #y_diff_sorted_trunc = tf.negative(y_diff_sorted_trunc)\n    \n    # END : Test \n        \n    \n    ","6f969b50":"model.compile(optimizer='adam',\n              loss=custom_loss_function)\nhistory = model.fit(x_train,y_train,\n                    batch_size=32,\n                    epochs=250,\n                    validation_data=(x_val,y_val))","4e6aa334":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n#plt.yscale('log')\nplt.show()","3f2543f5":"history = model.fit(x_values_scaled,y_values_scaled,\n                    batch_size=32,\n                    epochs=100)","e724644c":"y_pred = model.predict(x_values)\ny_pred = np.ravel(y_pred)\n\ny_diff = np.abs(np.log(1+y_values) - np.log(1+y_pred))\n\nsort_index = np.argsort(y_diff)\n\nmax_index = int(0.95 * np.size(y_values))\nindex_2_consider = sort_index[:max_index]","ccf5e684":"print(\"min_diff_value =\", y_diff[sort_index[0]])\nprint(\"max_diff_value =\", y_diff[sort_index[sort_index.size-1]])\nprint(\"\")\nprint(\"max_diff_value_after_trunc =\", y_diff[sort_index[max_index-1]])","af9df187":"x_values_scaled_no_outliers = x_values_scaled[index_2_consider]\ny_values_scaled_no_outliers = y_values_scaled[index_2_consider]\nx_values_scaled_no_outliers.shape","f8b8f45c":"history = model.fit(x_values_scaled_no_outliers,y_values_scaled_no_outliers,\n                    batch_size=32,\n                    epochs=100)","1bad9c7c":"pd_result_test = pd.DataFrame()\npd_result_test['Id'] = pd_test['Id']\npd_result_test['SalePrice'] = model.predict(x_test_scaled)","1a81614b":"pd_result_test.to_csv('Submission.csv',index = False)","7b6402c0":"# Analyze provided data","a2f4ae0f":"# Data scaling","d9c99937":"# Avoid Outliers from Calibration & Re-calibrate","ec0ff487":"# Analyze data quality (Is there NA values ?)","45b88218":"# Id is useless for Price predictions (drop it!)","ad73b763":"# Read Data to Pandas dataframes","76980059":"# Tensor Flow Regression","927894a8":"# Check all NA values has been converted correctly","b6dd20aa":"# Evatuate Test set & Kaggle Submission","7b4bb644":"# Now that the ANN has been defined - Calibrate to All train values","5642b29f":"# Fill NA for Numerical fields","09bd8928":"This Neuronal Network is **overfitting** the input data !!! ","dd8cc8d4":"In stead of analyzing correlation between variables, let's use a brute force Algorithm obtaining information from Numerical fields and from Categorical fields (after converting them to Numerical values to be able to use its data)","ead98163":"# Create Training and Validation sets (using sklearn)","f8e4bd92":"# Plot convergence graph","08056260":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","c675f0f5":"Use a custom defined loss function to avoid high dependecny on outliers","6a388b7d":"# Transform categorical fields to numerical values"}}