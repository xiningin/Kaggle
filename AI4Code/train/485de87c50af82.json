{"cell_type":{"30e98c9b":"code","4fb1b977":"code","ce99c2fe":"code","15082fc0":"code","e196f34a":"code","ef4c4a0a":"code","7d8870c4":"code","e7389a53":"code","e7d9655d":"code","ebbc0e66":"code","c82695ac":"code","5dc6e013":"code","e5fcb4b8":"code","d784aee7":"code","1c37ead0":"code","b2ce5faf":"code","280cd9e9":"code","a84c735a":"code","8a2343a7":"code","0e7dcd3c":"code","059bebfb":"code","fa514107":"code","cb795dcc":"code","02b5ca51":"code","d296d2d9":"code","411420cf":"code","4ecf2cf6":"code","7264b7f9":"code","3fd19006":"code","bf1071a2":"code","26fef356":"code","847c871d":"code","13d82061":"code","579bc812":"code","0531d50e":"code","504ef962":"code","9be7d80f":"code","01d83f95":"code","162db361":"code","8702b6e1":"code","ebea21da":"code","68da468d":"code","91275522":"code","6d2e40cd":"code","c3235065":"code","9b2e4108":"code","db881571":"code","4cbefa6b":"code","6c15a61f":"code","050601d3":"code","c0cbcf5c":"code","501edbbf":"code","a70711c1":"code","30baba8c":"code","d15dad7f":"code","2a4bf0ef":"code","3401daeb":"code","c7776cf1":"code","94a55cac":"markdown","619e5de6":"markdown","71b39be2":"markdown","4b39a6fd":"markdown","c273ee64":"markdown","eaf91aa9":"markdown","13b55003":"markdown"},"source":{"30e98c9b":"import numpy as np\nimport pandas as pd\nimport os\nimport nltk\nimport spacy\nfrom wordcloud import WordCloud\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem import WordNetLemmatizer\nimport time\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.metrics import AUC\nfrom sklearn.metrics import confusion_matrix, classification_report","4fb1b977":"#loading training and testing dataframes\ntrain_data = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\ntest_data = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin-1')","ce99c2fe":"#preview of training dataframe\ntrain_data.head(5)","15082fc0":"#preview of testing dataframe\ntest_data.head(5)","e196f34a":"#Non-Null Count and dtype of training dataframe\ntrain_data.info()","ef4c4a0a":"#Non-Null Count and dtype of testing dataframe\ntest_data.info()","7d8870c4":"#descriptive statistics for training dataframe\ntrain_data.describe()","e7389a53":"#descriptive statistics for testing dataframe\ntest_data.describe()","e7d9655d":"def preprocess_dataframe(dataframe, name=None):\n    \n    \"\"\"\n    Function to preprocess dataframe: removes redundant columns, converts dates to datetime type, creates new columns for mentions and hashtags\n    \n    Parameters\n    ----------\n    dataframe: Pandas Dataframe\n        a dataframe to preprocess\n    name: str, default=None\n        The name to assign to a dataframe\n    \n    Returns\n    -------\n    dataframe: Pandas Dataframe\n        a preprocessed dataframe\n    \"\"\"\n    \n    dataframe = dataframe.drop(columns=['UserName', 'ScreenName', 'Location'])\n    dataframe['TweetAt'] = pd.to_datetime(dataframe['TweetAt'])\n    dataframe['mentions'] = pd.Series([[word for word in tweet.split() if word.startswith('@')] for tweet in dataframe['OriginalTweet'].values])\n    dataframe['hashtags'] = pd.Series([[word for word in tweet.split() if word.startswith('#')] for tweet in dataframe['OriginalTweet'].values])\n    \n    if name!=None:\n        dataframe.name = name\n    \n    return dataframe","ebbc0e66":"#preprocessing training and testing dataframes\ntrain_df = preprocess_dataframe(train_data, name='train')\ntest_df = preprocess_dataframe(test_data, name='test')","c82695ac":"def sentiment_countplot(data, title, figsize=(8, 5)):\n    \n    \"\"\"\n    Function that creates countplots for sentiments in a dataframe\n    \n    Parameters\n    ----------\n    data: Pandas dataframe\n        a dataframe for which to visualize sentiments\n    title: str\n        Title of the the plot\n    figsize: tuple, default=(8, 5)\n        The size of the figure\n    \"\"\"\n    fig = plt.figure(figsize=(8, 5))\n    sns.set_palette(\"RdYlGn\")\n    ax = sns.countplot(data=data,\n                  x='Sentiment',\n                  order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'])\n    ax.set_title(title)\n    total = data.shape[0]\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{}%'.format(int(np.round(height\/total*100))),\n                ha=\"center\") ","5dc6e013":"#Distribution of sentiments in training dataframe\nsentiment_countplot(train_df, 'Count of Sentiments in Train Data')","e5fcb4b8":"#Distribution of sentiments in testing dataframe\nsentiment_countplot(test_df, 'Count of Sentiments in Test Data')","d784aee7":"#Distribution of tweet counts by sentiment over time in training dataframe\ntrain_df.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='area', figsize=(10, 5))\nplt.title('Count of Tweets in 2020')\nplt.ylabel('Tweet Count')","1c37ead0":"#Distribution of tweet counts by sentiment over time in testing dataframe\ntest_df.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='area', figsize=(10, 5))\nplt.title('Count of Tweets in 2020')\nplt.ylabel('Tweet Count')","b2ce5faf":"REPLACE_BY_SPACE = re.compile('[\/(){}\\[\\]\\|,;&-_]') #punctuation to replace","280cd9e9":"def preprocess_text(text):\n    \n    \"\"\"\n    Function to preprocess text: removes links, punctuation, spaces, non-alpha words and stop_words\n    \n    Parameters\n    ----------\n    text: str\n        a string to be preprocessed\n        \n    Returns\n    -------\n    text: str\n        a preprocessed string\n    \"\"\"\n    text = text.lower()                                    #lowercase\n    text = re.sub(r\"http\\S+\", \"\", text)                    #replace links with \"\"\n    text = re.sub(r\"\\@\\S+\", \"\", text)                      #replace mentions with \"\"\n    text = re.sub(r\"#\\S+\", \"\", text)                       #replace hashtags with \"\"\n    text = re.sub(r\"won\\'t\", \"would not\", text)            #deal with contractions\n    text = re.sub(r\"n\\'t\", \" not\", text)                   #deal with contractions\n    text = REPLACE_BY_SPACE.sub(' ', text)                 #replace punctuation with space\n    text = [word.strip() for word in text.split()]         #strip space from words\n    text = [word for word in text if len(word)>2]          #removing words less than 2 characters\n    text = [word for word in text if word!='amp']          #removing twitter amp\n    text = ' '.join(text)\n    return text","a84c735a":"#preprocessing text column in train and test dataframes\ntrain_df['Tweet'] = train_df['OriginalTweet'].apply(preprocess_text)\ntest_df['Tweet'] = test_df['OriginalTweet'].apply(preprocess_text)","8a2343a7":"def generate_wordcloud(data, mode='Tweet', sentiments='all'):\n    \n    \"\"\"\n    \n    Function that generates a wordcloud for a givens sentiment from a dataframe containing a text column\n    \n    Parameters\n    ----------\n    data: Pandas DataFrame\n        a pandas dataframe with a text column\n    mode: str, default='Tweet'\n        name of column in dataframe\n    sentiments: str, default='all'\n        The sentiment type for which to generate a wordcloud.\n        Must be one of ['all', 'positive', 'negative']\n    filter_common: boolean, default=False\n        Removes \n    \"\"\"\n    \n    \n    df = data.copy()\n    \n    if sentiments=='positive':\n        df = df[df.Sentiment.isin(['Positive', 'Extremely Positive'])]\n    if sentiments=='negative':\n        df = df[df.Sentiment.isin(['Negative', 'Extremely Negative'])]\n    \n     \n#     if mode=='OriginalTweet':\n#         text = ' '.join([i for i in text if not i.lower().startswith('#') and not i.lower().startswith('@') and not i.lower().startswith('https')])\n    if mode=='Tweet':\n        text = df[mode].str.split(' ').values\n        text = ' '.join([' '.join(i) for i in text])\n        text = text.strip()\n    else:\n        text = df[mode].values\n        text = ' '.join([' '.join(i) for i in text])\n        text = text.strip()\n\n    \n    cloud = WordCloud().generate(text)\n    plt.figure()\n    plt.imshow(cloud)\n    try:\n        plt.title(data.name)\n    except:\n        pass","0e7dcd3c":"#plotting wordcloud for tweets of positive sentiments in training and testing dataframes\nfor df in [train_df, test_df]:\n    generate_wordcloud(df, mode='Tweet', sentiments='positive')","059bebfb":"#plotting wordcloud for tweets of negative sentiments in training and testing dataframes\nfor df in [train_df, test_df]:\n    generate_wordcloud(df, mode='Tweet', sentiments='negative')","fa514107":"#plotting wordcloud for tweets of all sentiments in training and testing dataframes\nfor df in [train_df, test_df]:\n    generate_wordcloud(df, mode='Tweet', sentiments='all')","cb795dcc":"#plotting wordcloud of mentions of positive sentiments in training and testing dataframes\nfor df in [train_df, test_df]:\n    generate_wordcloud(df, mode='mentions', sentiments='positive')","02b5ca51":"#plotting wordcloud of mentions of negative sentiments in training and testing dataframes\nfor df in [train_df, test_df]:\n    generate_wordcloud(df, mode='mentions', sentiments='negative')","d296d2d9":"#plotting wordcloud of mentions of all sentiments in training and testing dataframes\nfor df in [train_df, test_df]:\n    generate_wordcloud(df, mode='mentions', sentiments='all')","411420cf":"def get_top_grams(dataframe, sentiment, n_grams=2, top=10):\n    \n    \"\"\"\n    Function that generates the top n_grams from a text column of dataframe that correspond to\n    a particular sentiment\n    \n    Parameters\n    ----------\n    dataframe: Pandas dataframe\n        dataframe with a text column\n    sentiments: str\n        The sentiment type for which to generate the top n_grams\n        Must be one of ['all', 'negative', 'positive']\n    n_grams: int, default=2\n        The number of grams to generate\n    top: int, default=10\n        The number of most common words to display\n    \"\"\"\n    \n    sentiments = ['Positive', 'Extremely Positive', 'Neutral', 'Negative', 'Extremely Negative']\n    \n    if sentiments!='all':\n        if sentiment=='positive':\n            sentiments = ['Positive', 'Extremely Positive']\n        if sentiment=='negative':\n            sentiments = ['Negative', 'Extremely Negative']\n\n    df = dataframe[dataframe['Sentiment'].isin(sentiments)]['Tweet'].str.split()\n    \n    text = [word for words_list in df.values for word in words_list]\n    \n    grams = nltk.ngrams(text, n=n_grams)\n    \n    dist = nltk.FreqDist(grams)\n    \n    print(dist.most_common(top))","4ecf2cf6":"#displaying top biggrams for positive tweets in training dataframe\nget_top_grams(train_df, 'positive')","7264b7f9":"#displaying top bigrams for negative tweets in training dataframe\nget_top_grams(train_df, 'negative')","3fd19006":"#displaying top bigrams of positive tweets in testing dataframe\nget_top_grams(test_df, 'positive')","bf1071a2":"#display top bigrams of negative tweets in testing dataframe\nget_top_grams(test_df, 'negative')","26fef356":"#calculating number of unique words\nunique_words = set([word for word_list in train_df['Tweet'].str.split().values for word in word_list])\nnum_unique_words = len(unique_words)\nprint(num_unique_words)","847c871d":"MAX_NB_WORDS = 20000 #maximum number of words to take from corpus\nTokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, oov_token='<oov>') #initializing tokenizer\nTokenizer.fit_on_texts(train_df['Tweet'].values) #fitting tokenizer on training_datase","13d82061":"word_to_ind = Tokenizer.word_index #extracting word to index mapping from tokenzier","579bc812":"#displaying word to index mapping\nword_to_ind","0531d50e":"# getting text sequences from training and testing dataframes\nX_train = Tokenizer.texts_to_sequences(train_df['Tweet'].values)\nX_test = Tokenizer.texts_to_sequences(test_df['Tweet'].values)","504ef962":"# calculating maximum length of sequences among both training and testing dataframes\nMAXLEN = max([len(x) for x in X_train] + [len(x) for x in X_test])","9be7d80f":"#adding padding of zeros to obtain uniform length for all sequences\nX_train_padded = sequence.pad_sequences(X_train, maxlen=MAXLEN)\nX_test_padded = sequence.pad_sequences(X_test, maxlen=MAXLEN)","01d83f95":"#encoding sentiment labels\nY_train = train_df['Sentiment'].values\nY_test = test_df['Sentiment'].values\nencoder = LabelEncoder()\nY_train = encoder.fit_transform(Y_train)\nY_test = encoder.transform(Y_test)","162db361":"#one-hot-encoding sentiment labels\nY_train_enc = to_categorical(Y_train)\nY_test_enc = to_categorical(Y_test)","8702b6e1":"print(MAXLEN)\nprint(MAX_NB_WORDS)\nprint(Y_train_enc.shape)","ebea21da":"# defining embedding dimension\nEMBEDDING_DIM = 32\nLSTM_NODES = 128","68da468d":"#building sequential neural network\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAXLEN, mask_zero=True))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(LSTM_NODES, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(Y_train_enc.shape[1], activation='softmax'))","91275522":"#displaying model architecture\nmodel.summary()","6d2e40cd":"#defining pr-auc metric\nauc = AUC(curve='PR')","c3235065":"#compiling model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy', auc])","9b2e4108":"#training model\nhistory = model.fit(X_train_padded, Y_train_enc, validation_data=(X_test_padded, Y_test_enc), epochs=5, batch_size=256, use_multiprocessing=True, shuffle=True)","db881571":"#evaluating model on test set\nY_pred = model.predict(X_test_padded)\nY_pred = np.argmax(Y_pred, axis=1)","4cbefa6b":"#extract labels from encoder\nlabels = list(encoder.classes_)","6c15a61f":"#calculate and plot confusion matrix\ncm = confusion_matrix(Y_test, Y_pred)\nsns.heatmap(cm, annot=True, xticklabels=labels, yticklabels=labels, fmt='g')","050601d3":"#printing classification report\nprint(classification_report(Y_test, Y_pred, target_names=labels))","c0cbcf5c":"#mapping 5 classes to 3 more specific classes\nmapping = {\n    \"Extremely Positive\": \"Positive\",\n    \"Extremely Negative\": \"Negative\",\n    \"Positive\": \"Positive\",\n    \"Neutral\": \"Neutral\",\n    \"Negative\": \"Negative\"\n}\n\n#encoding sentiment labels\n\nY_train = train_df['Sentiment'].values\nY_test = test_df['Sentiment'].values\n\nY_train = list(map(mapping.get, Y_train))\nY_test = list(map(mapping.get, Y_test))\n\nencoder = LabelEncoder()\nY_train = encoder.fit_transform(Y_train)\nY_test = encoder.transform(Y_test)\n\nY_train_enc = to_categorical(Y_train)\nY_test_enc = to_categorical(Y_test)","501edbbf":"#building sequential neural network\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAXLEN, mask_zero=True))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(LSTM(LSTM_NODES, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(Y_train_enc.shape[1], activation='softmax'))","a70711c1":"#compiling model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy', auc])","30baba8c":"#fitting model\nhistory = model.fit(X_train_padded, Y_train_enc, validation_data=(X_test_padded, Y_test_enc), epochs=5, batch_size=256, use_multiprocessing=True, shuffle=True)","d15dad7f":"#evaluating model on test set\nY_pred = model.predict(X_test_padded)\nY_pred = np.argmax(Y_pred, axis=1)","2a4bf0ef":"#extract labels from encoder\nlabels = list(encoder.classes_)","3401daeb":"#calculate and plot confusion matrix\ncm = confusion_matrix(Y_test, Y_pred)\nsns.heatmap(cm, annot=True, xticklabels=labels, yticklabels=labels, fmt='g')","c7776cf1":"#printing classification report\nprint(classification_report(Y_test, Y_pred, target_names=labels))","94a55cac":"## **Model Construction**","619e5de6":"### Trying again with 3 classes instead of 5","71b39be2":"## **Exploratory Data Analysis**","4b39a6fd":"### Loading Libraries","c273ee64":"### Loading Data","eaf91aa9":"### Model Evaluation","13b55003":"### Text Preprocessing"}}