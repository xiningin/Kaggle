{"cell_type":{"5359b6cd":"code","6043177d":"code","8efc57ce":"code","8eb6f240":"code","69e29f2a":"code","e3646d39":"code","5f57d5b7":"code","15ec415c":"code","37a76eef":"code","173c935b":"code","5daeb26f":"code","c09207b3":"code","86434c7b":"code","e4279f7a":"code","ea0d04b7":"code","dea93d7f":"code","3f833cfb":"code","817a6bd5":"code","420843db":"code","b55b4f4d":"code","0d2b9a49":"code","b0832012":"code","67a4f40c":"code","f96f27eb":"code","e31dd1a8":"code","678f0024":"code","68672b3c":"code","7dde6d8a":"code","b8c8bb78":"markdown","811bd9ec":"markdown","27e9bcd6":"markdown","71b92144":"markdown","af282807":"markdown","a2e7366e":"markdown","05a81f8d":"markdown","34272ee3":"markdown"},"source":{"5359b6cd":"import gc\nfrom functools import partial\nfrom pathlib import Path\n\nfrom fastai.text import *\nfrom fastai.callbacks import *\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# pd.set_option('display.max_colwidth', 200)\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.min_rows', 100)\n# pd.set_option('display.max_rows', 100)\n\nhome = Path(\".\")\ninput_dir = Path(\"\/kaggle\/input\/google-quest-challenge\/\")\n\n!mkdir -p ~\/.fastai\/models\/\n!cp -R ..\/input\/fastai-wt103-models\/* ~\/.fastai\/models\/\n!cp ~\/.fastai\/models\/wt103-fwd\/itos_wt103.pkl ~\/.fastai\/models\/wt103-bwd\/itos_wt103.pkl","6043177d":"# The metric used in the competition\nfrom scipy.stats import spearmanr\n\nclass AvgSpearman(Callback):\n    def on_epoch_begin(self, **kwargs):\n        self.preds = None\n        self.target = None\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        if self.preds is None or self.target is None:\n            self.preds = last_output.cpu()\n            self.target = last_target.cpu()\n        else:\n            self.preds = np.append(self.preds, last_output.cpu(), axis=0)\n            self.target = np.append(self.target, last_target.cpu(), axis=0)\n    \n    def on_epoch_end(self, last_metrics, **kwargs):\n        spearsum = 0\n        for col in range(self.preds.shape[1]):\n            spearsum += spearmanr(self.preds[:,col], self.target[:,col]).correlation\n        res = spearsum \/ (self.preds.shape[1] + 1)\n        return add_metrics(last_metrics, res)","8efc57ce":"raw_test = pd.read_csv(input_dir\/\"test.csv\"); raw_test.tail(3)","8eb6f240":"raw_train = pd.read_csv(input_dir\/\"train.csv\"); raw_train.tail(3)","69e29f2a":"# pd.get_dummies(raw_train, columns=class_labels)","e3646d39":"# just to be sane\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)","5f57d5b7":"labels = pd.read_csv(input_dir\/\"sample_submission.csv\").columns[1:].to_list()\nassert len(labels) == 30\ntext_cols = [\"question_title\", \"question_body\", \"answer\"] + [\"host\", \"category\", \"question_user_name\", \"question_user_page\", \"answer_user_page\", \"answer_user_name\"]","15ec415c":"train_df = raw_train.iloc[np.random.permutation(len(raw_train))]\ntrain_lm_df = raw_train.append(raw_test, ignore_index=True, sort=False)","37a76eef":"m_code = re.compile(r\"(\\n(?:[a-z  ][\\s\\S]*?(?: = |{|\\()[\\s\\S]+?)+?\\n)\")\ncode = re.compile(r\"(  [\\s\\S]+?\\n){2,}\",)","173c935b":"# train_lm_df.loc[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"question_body\"]] = train_lm_df[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"question_body\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n# train_lm_df.loc[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"answer\"]] = train_lm_df[train_lm_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"answer\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n\n# train_df.loc[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"question_body\"]] = train_df[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"question_body\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n# train_df.loc[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"answer\"]] = train_df[train_df.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"answer\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n\n# raw_test.loc[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"question_body\"]] = raw_test[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"question_body\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))\n# raw_test.loc[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"]), [\"answer\"]] = raw_test[raw_test.category.isin([\"STACKOVERFLOW\", \"TECHNOLOGY\"])][\"answer\"].apply(lambda x: m_code.sub(\" xxcodeblock \", x))","5daeb26f":"BS = 256","c09207b3":"# data for the language models\ntokenizer = Tokenizer(SpacyTokenizer, 'en')\nprocessor = [TokenizeProcessor(tokenizer=tokenizer, mark_fields=True), NumericalizeProcessor()]\n\nlm_label_list = (TextList.from_df(train_lm_df, \".\", text_cols, processor=processor)\n                 .split_by_rand_pct(0.1, seed=42)\n                 .label_for_lm())\n\ndata_lm = lm_label_list.databunch(bs=BS)\ndata_lm_bwd = lm_label_list.databunch(bs=BS, backwards=True)","86434c7b":"# data for classifiers\nvocab = data_lm.vocab\nBSC = 120\n\nclas_label_list = (TextList.from_df(train_df, \".\", text_cols, vocab=vocab, processor=processor)\n                   .split_by_rand_pct(0.2, seed=42)\n                   .label_from_df(cols=labels)\n                   .add_test(TextList.from_df(raw_test, \".\", text_cols, vocab=vocab, processor=processor)))\n\ndata_clas = clas_label_list.databunch(bs=BSC)\ndata_clas_bwd = clas_label_list.databunch(bs=BSC, backwards=True)","e4279f7a":"lr = 1e-02\nlr *= BS\/48\nmoms = (0.8, 0.7)\nwd=0.1\ndrop_mult = 0.5\n\ndef fit_lm(data, epochs=10, head_epochs=5, prefix=\"fwd\"):\n    learn = language_model_learner(data, AWD_LSTM, drop_mult=drop_mult,\n                                   metrics=[accuracy, Perplexity()],\n                               )\n    learn = learn.to_fp16()\n    learn.fit_one_cycle(head_epochs, slice(lr), moms=moms, wd=wd)\n    learn.unfreeze()\n    learn.save(f\"{prefix}_lm_learn_1\")\n    learn = learn.load(f\"{prefix}_lm_learn_1\")\n    learn.fit_one_cycle(epochs, slice(lr\/100, lr\/2), moms=moms, wd=wd,\n                        callbacks=[SaveModelCallback(learn, monitor=\"perplexity\", mode=\"min\", name=\"best_model\"),]\n                        )\n    learn.save_encoder(f\"{prefix}_enc\")\n    learn.save(f\"{prefix}_lm_model\")\n    return learn","ea0d04b7":"# learn = language_model_learner(data_lm, AWD_LSTM, config, drop_mult=1.0,\n#                                metrics=[accuracy, Perplexity()],\n#                                )\n# learn.lr_find()\n# learn.recorder.plot()","dea93d7f":"# 1\/0\n# learn.purge();\n# gc.collect()","3f833cfb":"learn = fit_lm(data_lm)","817a6bd5":"learn = fit_lm(data=data_lm_bwd, prefix=\"bwd\")","420843db":"lr = 5e-02\nlr *= BSC\/48  # Scale learning rate by batch size\nmoms = (0.8, 0.7)\nwd=0.1\n\ndef fit(data, prefix=\"fwd\", epochs=20, epochs_1=2):\n    learn = text_classifier_learner(data, AWD_LSTM,\n                                    pretrained=False,\n                                    metrics=[AvgSpearman()],\n                                    ).to_fp16()\n    learn.load_encoder(f\"{prefix}_enc\");\n    learn.fit_one_cycle(epochs_1, lr, moms=moms, wd=wd)\n\n    learn.freeze_to(-2)\n    learn.save(\"learn\")\n    learn = learn.load(\"learn\")\n    learn.fit_one_cycle(2, slice(lr\/(2.6**4),lr), moms=moms, wd=wd)\n\n    learn.freeze_to(-3)\n    learn.save(\"learn\")\n    learn = learn.load(\"learn\")\n    learn.fit_one_cycle(2, slice(lr\/2\/(2.6**4),lr\/2), moms=moms, wd=wd)\n\n    learn.unfreeze()\n    learn.save(f\"{prefix}_learn\")\n    learn = learn.load(f\"{prefix}_learn\")\n    learn.fit_one_cycle(epochs, slice(lr\/10\/(2.6**4),lr\/10), moms=moms, wd=wd,\n                        callbacks=[SaveModelCallback(learn, monitor=\"avg_spearman\", mode=\"max\", name=\"best_model\")]\n                        )\n    learn.save(f\"{prefix}_learn_4\")\n    learn = learn.load(f\"{prefix}_learn_4\")\n    return learn","b55b4f4d":"learn = fit(data_clas)","0d2b9a49":"learn_bwd = fit(data=data_clas_bwd, prefix=\"bwd\")","b0832012":"sample_submission = pd.DataFrame(columns=[\"qa_id\"]+labels)","67a4f40c":"def spearm(preds, target):\n    spearsum = 0\n    for col in range(preds.shape[1]):\n        spearsum += spearmanr(preds[:,col], target[:,col]).correlation\n    return spearsum \/ (preds.shape[1] + 1)","f96f27eb":"preds, target = learn.get_preds(DatasetType.Valid, ordered=True)\npreds_b, _ = learn_bwd.get_preds(DatasetType.Valid, ordered=True)\nspearm((preds+preds_b)\/2, target)","e31dd1a8":"test_preds, _ = learn.get_preds(DatasetType.Test, ordered=True)\ntest_preds_b, _ = learn_bwd.get_preds(DatasetType.Test, ordered=True)\npreds_avg = (test_preds+test_preds_b)\/2","678f0024":"sample_submission.loc[:, \"qa_id\"] = raw_test[\"qa_id\"]\n# sample_submission.loc[:, labels] = test_preds\nsample_submission.loc[:, labels] = preds_avg\nsample_submission.loc[:, labels] = np.clip(sample_submission.loc[:, 1:], 0.00001, 0.999999)","68672b3c":"sample_submission.to_csv(\"submission.csv\", index=False)","7dde6d8a":"sample_submission.tail()","b8c8bb78":"## Fine tune LM","811bd9ec":"# Train Classifier","27e9bcd6":"# Submission","71b92144":"**Changes**\n\nV52\nReturn of backwards model\n\nV51\n\nRound of categorical labels\n\nV48\n\nSpacy, only fwd\n\nV32\n\nreplace code blocks\n\nV31\n\nSentencePiece + backwards, test dataset uses the same tokenizer\n\nV30\n\nBackwards model\n\nV29\n\nSentencePiece tokenizer","af282807":"What didn't work:\n\n1. Rounding. Metrics is a correlation\n2. Codeblocks tokenizing (replacing entire blocks with a token)\n3. Removing columns separator","a2e7366e":"# LM\n## Tokenize code","05a81f8d":"This notebook is to train fastai classifier with transfer learning, i.e. ULMFIT with default Spacy tokenizer.\n\nI am using AWD-LSTM. Most other notebooks use transformers like BERT or XL, which potentially yield better results, but are slower.\n\nSources\nhttps:\/\/www.kaggle.com\/melissarajaram\/roberta-fastai-huggingface-transformers","34272ee3":"# Preprocess"}}