{"cell_type":{"5a3f39c4":"code","23a6f7dd":"code","2df9acee":"code","87fe98f1":"code","2c6f39c9":"code","ecba3736":"code","0e09c197":"code","1b494773":"code","9d345e6e":"code","b8d71f13":"code","e2f14509":"code","42aef085":"code","a329a78f":"code","31282927":"code","906e30ae":"code","a38671f0":"code","c12102af":"code","ea703020":"code","4a97fa11":"code","3dcc598d":"code","ff3bca94":"code","87927bbc":"code","30d4ba19":"code","57d91f77":"code","368adab0":"code","0009ffad":"code","c5272f4d":"code","dedf7126":"code","4a3bd7ab":"code","cd203e33":"code","a02749a1":"code","7aa28b12":"code","7d1d063e":"code","9c494e07":"code","a579fb32":"code","4b64c425":"code","55ea4900":"code","a32cc2b4":"code","06c2cc51":"code","d1db21ba":"code","76ed7010":"code","2543e034":"code","836c98e6":"code","b48e3e3e":"code","54442bc8":"code","0855702e":"code","13fb4200":"code","a1e23557":"code","cedba516":"code","a8063a7e":"code","7d2a16dc":"code","467436df":"code","5cd64cce":"code","44721d9d":"code","2c0ef2ec":"code","1644328c":"code","bebbc3df":"code","4a8bc4e3":"code","6417194e":"code","3790b07e":"code","a9777884":"code","3ea4a794":"code","d5f216b6":"code","050a3aeb":"code","a604b9f0":"code","30198d9c":"code","58d6f3f2":"code","d03db262":"code","5df3d949":"code","5d174c99":"markdown","493c2520":"markdown","f4974110":"markdown","a84e4bdd":"markdown","72fb3252":"markdown","7f83e807":"markdown","741329ba":"markdown","56cd5ac8":"markdown","e61ee97c":"markdown","a3686821":"markdown","29b70f88":"markdown","42867e2a":"markdown","704da599":"markdown","6775f1c6":"markdown","cebadec6":"markdown"},"source":{"5a3f39c4":"# Importing Libraries like numpy for mathematical calculation, pandas for dataframes, matplotlib for drawing data graps etc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import metrics\nfrom pandas.plotting import scatter_matrix","23a6f7dd":"# Reading the Real Estate.csv data into notebook.","2df9acee":"real=pd.read_csv(\"..\/input\/real-estate\/Real Estate.csv\")\nreal","87fe98f1":"real.shape # total no of rows and cols.","2c6f39c9":"real.size # total no of elements in data set","ecba3736":"real.head() # printing top 5 rows from dataset.","0e09c197":"real.tail() # printing last 5 rows from dataset.","1b494773":"real.dtypes","9d345e6e":"# Using Distribution Plot to get a sense how the variables are distributed.","b8d71f13":"sns.distplot(real['total_rooms']) # by default the distribution plot uses histogram and fit a KDE(kernal density estimate)on it.\nreal['total_rooms'].mean()# the middle of the bell curve of distplot will give mean of the col here we can see mena is 2635. when we compare with the graph value it'll be the same.","e2f14509":"sns.distplot(real['housing_median_age']) # as we can see in the graph the data of the 'housing_median_age' is distributed soo much as I understand that the data is cahging form 18 to 35 as median age.\nreal['housing_median_age'].mean()","42aef085":"sns.distplot(real['median_income']) # as we can see in the graph the data is uniformaly distributed and we can see the clear bell curve is formaing in the data\nprint(\"mean value: \",real['median_income'].mean())\nprint(\"maximum value: \",real['median_income'].max()) # as their may be some outliers in the 'median_income' col because with the \n# max() it is showing maximum value is 15.0001 and when we look at the graph the maximum value is more than 25.\nprint(\"minimum value\",real['median_income'].min())","a329a78f":"sns.distplot(real['median_house_value'])\nreal['median_house_value'].mean()","31282927":"# As we know we have one categorical col in our Data Set which is \" ocean_proximity \" so,for categorical col \n# we can't use Distribution Plot. For this we will use some other plols.","906e30ae":"real.head(0)","a38671f0":"real['ocean_proximity'].value_counts() # Here we can see that ocean_proximity has 5 categories","c12102af":"figsize=(25,25)\nsns.barplot(x='ocean_proximity',y='housing_median_age',data=real )# so, from the bar we can understand that housing age is more \n# in ISLAND(which is 43 )  as compared to others categories.\n","ea703020":"figsize=(25,25)\nsns.barplot(x='ocean_proximity',y='median_income',data=real)","4a97fa11":"real.hist(figsize=(25,25),bins=50)\n# A histogram is used when we have to check the  distribution of a continuous variable in a dataset\n# here from this histogram we analysed that their is lot of data change rate in longitude,population,total bedrooms,total rooms.\n#and in case of housing_median_age the data is not varing so much in it'starting phase but at the middle it is slightly \n# chnaging and when it comes to end the graph is incresing too much it menas their may be some outliers in the col.\n# for checking outliers we will use different method.","3dcc598d":"real.plot.scatter(x = 'total_rooms', y = 'total_bedrooms')","ff3bca94":"sns.jointplot(x='total_rooms',y='total_bedrooms',data=real,kind='reg')\n#so, here we are finding he best fit line by using jointplot and kind ='reg', what this graph is showing is that: the best fit \n# line passs through those data points which are having minimum distance from that line. the line can be in any direction.","87927bbc":"# sns.pairplot(real,hue='ocean_proximity',palette='rainbow')","30d4ba19":"# sns.pairplot(real,hue='median_house_value',palette='rainbow')","57d91f77":"scatter_matrix(real,figsize=(25,25),alpha=0.9,diagonal=\"kde\",marker=\"o\") #understanding how one col is related to other col using scatter matrix.....\n# Scatter plots shows how much one variable is affected by another or the relationship between them\n# with the help of dots in two dimensions. \n# Scatter plots are very much like line graphs in the concept that they use horizontal and vertical axes to plot data points.","368adab0":"# so, from the above scatter matrix plot we can say that some columns are highly related to each other like: (median_income and housing_median_age),\n# (median_house_value and housing_median_age) and many more col... we can see which box has higher density of datapoints their \n# respective coumns are highely related to each others.....","0009ffad":"# Displaying the above data in the form of heatmap","c5272f4d":"sns.set(context='paper',font='monospace')\nreal_cor_matrix=real.corr()\nfig,axe=plt.subplots(figsize=(12,8))\ncmap=sns.diverging_palette(220,10,center='light',as_cmap=True)\nsns.heatmap(real_cor_matrix,vmax=1,square=True,cmap=cmap,annot=True)","dedf7126":"# from above heatmap we understand that col total_rooms and total_bedrooms ,total_rooms and households , similarly total_bedrooms\n# and populations are highly related because we can see that right side color identifier line that 0.8 and above is highly related \n# and similarly less than 0.4 are not soo much related to each other.","4a3bd7ab":"plt.boxplot(real[\"total_rooms\"],showmeans=True)\nplt.show()","cd203e33":"# box plot is mainly use to detect outliers in any particular column. the red line in the box shows the mean vlaue of the column\n# and here in this column box plot detected outliers form 23,000 because from the range of (20,000 - 40,000) the datapoints are \n# scattering too much hence they are outliers in the dataset.","a02749a1":"print(\"mean value : \",real['total_rooms'].mean())\nprint(\"minimum value  : \",real['total_rooms'].min())\nprint(\"maximum value   : \",real['total_rooms'].max())\n# as we can see here also the minimum value is 2 and respective to it maximum value is 39320 and mean is 2635 , hence if we take \n# difference datapoints from mean value to maximum vlaue theri will be soo much of differences in values so, those values are called\n# outliers in the dataset.","7aa28b12":"def getOutliers(dataframe,column):\n    column = \"total_rooms\" \n    des = dataframe[column].describe()\n    desPairs = {\"count\":0,\"mean\":1,\"std\":2,\"min\":3,\"25\":4,\"50\":5,\"75\":6,\"max\":7}\n    Q1 = des[desPairs['25']]\n    Q3 = des[desPairs['75']]\n    IQR = Q3-Q1\n    lowerBound = Q1-1.5*IQR #finding lower bound\n    upperBound = Q3+1.5*IQR #finding upper bound\n    print(\"(IQR = {})Outlier are anything outside this range: ({},{})\".format(IQR,lowerBound,upperBound))\n    data = dataframe[(dataframe [column] < lowerBound) | (dataframe [column] > upperBound)]\n    print(\"Outliers out of total = {} are \\n {}\".format(real[column].size,len(data[column])))\n    outlierRemoved = real[~real[column].isin(data[column])] #remove the outliers from the dataframe\n    return outlierRemoved","7d1d063e":"df_outliersRemoved = getOutliers(real,\"total_rooms\")","9c494e07":"real.isnull().sum() # isnull() displyes null values respective to it's columns\n# here their ar 207 null values in total_bedrooms columns","a579fb32":"real['total_bedrooms'].mean()\n# real['total_bedrooms'].mode()\n# real['total_bedrooms'].max()\n# real['total_bedrooms'].min()","4b64c425":"real['total_bedrooms'].fillna(value=np.mean(real['total_bedrooms']),inplace=True)\nreal.isnull().sum()","55ea4900":"from sklearn.preprocessing import LabelEncoder\nlabelEncoder = LabelEncoder()\nreal[\"ocean_proximity\"] = labelEncoder.fit_transform(real[\"ocean_proximity\"])\nreal[\"ocean_proximity\"].value_counts()\n# real[\"ocean_proximity\"].head()\nreal.head()","a32cc2b4":"x=real.iloc[:,[0,1,2,3,4,5,6,7,8]] # independent variables(cols)\ny=real.iloc[:,[-1]] # dependent variables(cols)","06c2cc51":"x.head(0)# Independent columns in x","d1db21ba":"y.head(0)# Dependent column in y (Target variable)","76ed7010":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)","2543e034":"x_train.head() #Independent variable training dataset (80%)\n# print(x_train.shape)\n# print(x_train.size)","836c98e6":"x_test.head() #Independent variable testing dataset (20%)\nprint(x_test.shape)\nprint(x_test.size) # displaying shape and size of x_test(20%) data","b48e3e3e":"y_train.head() #Dependent variable training dataset (80%)\n# print(y_train.shape)\n# print(y_train.size)","54442bc8":"y_test.head() #Dependent variable testing dataset (20%)\nprint(y_test.shape)\nprint(y_test.size) # Displaying shape and size of y_test(20%) data","0855702e":"# Applying Linear Regression model without Standardizing the data and checking the accuracy.....","13fb4200":"from sklearn.linear_model import LinearRegression #importing linear regression from sklearn...\nlm=LinearRegression() # creating object of model..","a1e23557":"lm.fit(x_train,y_train)# providing traing data to model.......","cedba516":"pred=lm.predict(x_test)\npred","a8063a7e":"from sklearn.metrics import r2_score\nr2_score(y_test,pred)","7d2a16dc":"# appling Linear regression model with Standardizing the data.","467436df":"from sklearn.preprocessing import StandardScaler\nindependent_scaler = StandardScaler()\nx_train = independent_scaler.fit_transform(x_train)\nx_test = independent_scaler.transform(x_test)\nx_train","5cd64cce":"lm.fit(x_train,y_train)# providing traing data to model.......","44721d9d":"pred=lm.predict(x_test)# after fitting into the model priniting predicted values\npred\nc=pd.DataFrame(pred)\nc.head()","2c0ef2ec":"y_test.head() # printing original value and if we compare with predicted value in above cell some of the values are closer but in some case the difference of values is very high.\n# hence, we can say that Linear Regression model gives the minimum accuracy in this case.","1644328c":"print(np.sqrt(metrics.mean_squared_error(y_test,pred))) # printing Root Mean Squared Error (RMSE)","bebbc3df":"from sklearn.metrics import r2_score # checking the accuracy with with Adjusted R2\nr2_score(y_test,pred)","4a8bc4e3":"from sklearn.tree import DecisionTreeRegressor# importing decision tree form sklearn lib....","6417194e":"dtReg = DecisionTreeRegressor(max_depth=9) # creating obj of decision tree and providing max_depth =9\ndtReg.fit(x_train,y_train)# fitting training data to the model.","3790b07e":"dtReg_y_pred = dtReg.predict(x_test)# after fitting into the model priniting predicted values\ndtReg_y_pred\ne=pd.DataFrame(dtReg_y_pred)\ne.head()","a9777884":"y_test.head() # printing original value and if we compare with predicted value in above cell their is a lot of difference in the predicted values and the original values,\n# hence, we can say that Decision Tree Regression model is not fit in this case(dataset).","3ea4a794":"print(np.sqrt(metrics.mean_squared_error(y_test,dtReg_y_pred))) # printing Root Mean Squared Error (RMSE)","d5f216b6":"from sklearn.metrics import r2_score# checking the accuracy with with Adjusted R2\nr2_score(y_test,dtReg_y_pred)","050a3aeb":"from sklearn.ensemble import RandomForestRegressor # importing Random forest from sklearn module.","a604b9f0":"rfReg = RandomForestRegressor(30)# creating object of random forest \nrfReg.fit(x_train,y_train)# fitting training data in random forest","30198d9c":"rfReg_y_pred = rfReg.predict(x_test)# after fitting into the model priniting predicted values\nrfReg_y_pred\nd=pd.DataFrame(rfReg_y_pred) # converting the data into dataframe so that, we can get the data in right format.\nd.head()","58d6f3f2":"y_test.head() # printing original value and if we compare with predicted value in above cell the values are quite closer,\n# hence, we can say that Random Forest Regressor model gives the maximum accuracy.","d03db262":"print(np.sqrt(metrics.mean_squared_error(y_test,rfReg_y_pred))) # printing Root Mean Squared Error (RMSE)","5df3d949":"from sklearn.metrics import r2_score # checking the accuracy with with Adjusted R2\nr2_score(y_test,rfReg_y_pred)","5d174c99":"# Using Decision Tree Regression*************\n","493c2520":"Here with the Linear Regression model I got the maximum accuracy of 65%","f4974110":"Working with Outliers...","a84e4bdd":"S0, here with Using Decision Tree Regression we got 73% accuracy which not good. Let's try with another ML algorithms.","72fb3252":"## Spliting data into tarin and test...","7f83e807":"# Using Linear Regression Model Algorithm*","741329ba":"# Data Analysis and visualization.......","56cd5ac8":"## Handling Missing values in the data set....\n","e61ee97c":"By using the Random Forest Regressor we gain a maximum accuracy of 81% which is good accurcy...","a3686821":"# Using Random Forest Regressor *","29b70f88":"#                 **House Price Prediction**\n![house_i1.png](attachment:house_i1.png)","42867e2a":"Here, I am working with Real Estate dataset. Based on these features: \n\nlongitude, latitude, housing_median_age, total_rooms,\ttotal_bedrooms, population, households, median_income, ocean_proximity.\nwe have to predict House price.\n\n\n\nAt very first I am doing Data Analysis , Data visualization , treating null values\nin the columns , detecting and removing outliers from the data set.(This part is called data cleaning)\n\nAs we can see in **this dataset our target variable is continious so, this is a Regression Problem and can also see their is a labeled input and output so, this is a supervised machine learning problem.**\n\nAfter cleaning the data I applied 3 Machine Learning ALgorithm:\n1: LINEAR REGRESSION MODEL (with or without standardization ) , Accuracy= 65%\n2: DECISION TREE REGRESSION , Accuracy = 73%\n3: RANDOM FOREST REGRESSOR , Accuracy = 81%\n\nHere, for this dataset DECISION TREE REGRESSION And RANDOM FOREST REGRESSOR are giving accuracy more than 70 %","704da599":"## Using LABEL ENCODER for ctegorical column \" ocean_proximity \"","6775f1c6":"![Thank-you.jpg](attachment:Thank-you.jpg)","cebadec6":"So,till now we understand that the last col in dataset which is \" median_house_value \" values are continous so, it is a regression problem and theri is one Categorica Col. Categorical Cols are those which can further be categorised into multiple categories based on thier values.. so, here the Categorical feature\/column is \" ocean_proximity \"."}}