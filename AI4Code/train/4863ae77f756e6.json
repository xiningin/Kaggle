{"cell_type":{"4208193b":"code","970ea808":"code","715edbb4":"code","d3ebfc2c":"code","63f6dc55":"code","7cf0a94b":"code","bb791b17":"code","314def72":"code","4a1dd799":"code","ea2aa7a8":"markdown","30a6e185":"markdown","ba4cb79a":"markdown","e9038f67":"markdown","e994b145":"markdown","aa84f8c4":"markdown","30e8afc1":"markdown","11da2a7f":"markdown","b9128a4f":"markdown","2bc51e79":"markdown","50e0aa5c":"markdown","e30d70d4":"markdown","c0b09687":"markdown"},"source":{"4208193b":"import pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint","970ea808":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","715edbb4":"train_data = train.text.values\ntrain_labels = train.target.values\ntest_data = test.text.values","d3ebfc2c":"%%time\nmodule_url = 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/4'\nembed = hub.KerasLayer(module_url, trainable=False, name='USE_embedding')","63f6dc55":"def build_model(embed):\n    model = Sequential([\n        Input(shape=[], dtype=tf.string),\n        embed,\n        Dense(128, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","7cf0a94b":"model = build_model(embed)\nmodel.summary()","bb791b17":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_data, train_labels,\n    validation_split=0.2,\n    epochs=20,\n    callbacks=[checkpoint],\n    batch_size=32\n)","314def72":"model.load_weights('model.h5')\ntest_pred = model.predict(test_data)","4a1dd799":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","ea2aa7a8":"# Acknowldegements\n\n- Built upon from: https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-train-a-universal-sentence-encoder\n- Edits: Varied epochs, learning rate to see their impacts\n- Inference: More epochs (20) improved LB","30a6e185":"Create convenient names for the variables we will be using for training and inference.","ba4cb79a":"# Inference","e9038f67":"Don't forget that the latest model might not be the best! Instead, the best is the one we saved as `model.h5`; let's load it and run prediction on `test_data`.","e994b145":"First load all the CSV files we will need","aa84f8c4":"# About this kernel\n\n[Universal Sentence Encoder](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4) is a model created and publicly made available by Google. Built in Tensorflow, it was trained to embed any type of sentences or short paragraphs so that the meaning is as much preserved as possible; so that it can be finetuned for classification tasks specifically.\n\nThis implementation is extremely pleasant to use, since the input is simply the string, and the output is just the 512-dimensional encoded sentence; no preprocessing is needed. It is also fully-trainable, and uses an almost state-of-the-art architecture (namely, pre-BERT transformers (nice [comparison in this blog post](https:\/\/blog.floydhub.com\/when-the-best-nlp-model-is-not-the-best-choice\/)).\n\nSince this competition is dedicated for beginners to get started, I feel this is a perfect example of using a novel technology, but packaged in a gentle and correctly abstracted API (as opposed to the horrors of [1000 lines of tensorflow code](https:\/\/github.com\/google-research\/bert\/blob\/master\/modeling.py) that needs to be understood before modifying BERT). Here, instead, **I'm only showing you some 50 lines of codes to get all up and running**; and only ~15 lines to setup the model!\n\n## Summary\n\nThis kernel serves as a short and straightforward introduction to the process of:\n1. Loading a trained model from [Tensorflow hub](https:\/\/tfhub.dev\/).\n2. Building a `Sequential` Keras model by using the trained model as a layer.\n3. Training the newly created Keras model, and perform inference.","30e8afc1":"Let's check if the model looks the way we want:","11da2a7f":"Build a simple sequential model in Keras, with just a few lines. Note that the `Input` here is a tf.string; usually you will see integer inputs followed by an `Embedding` layer; those are needed for RNNs or CNNs, but here it is all taken care of internally by the USE; in other words, the `embed` layer you just loaded is internally tokenizing the strings, convert them to integers, then map them using an embedding.\n\nIf none of those words make any sense, worry not! USE was designed to be easily understood and directly used as is, so you don't have to get into the low-level implementation details, and can focus on using it as a tool in your Keras model, or use it as is.","b9128a4f":"Finally, we round the predictions, set them to integer, update the `submission` dataframe, and save it as CSV... Oof!","2bc51e79":"# Load data and model","50e0aa5c":"# Train the model","e30d70d4":"Let's get started with the training step! We'll use 20% of the data to validate the results, and only save the model that has the lowest loss on that 20% data.","c0b09687":"Finally, load the Universal Sentence Encoder from tfhub.dev (make sure Internet is enabled!)."}}