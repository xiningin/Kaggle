{"cell_type":{"f22e50d5":"code","f0a8de8a":"code","c623bc4d":"code","92162004":"code","656fb520":"code","0be03ca0":"code","b61c2e28":"code","14f660f2":"code","cfa477de":"code","7e2c8046":"code","7d09ff01":"code","87324cb6":"code","dc34d804":"code","afd59c79":"code","3562fa4a":"code","7a1ba4c9":"code","487c8298":"code","165e5771":"code","3ab6b257":"code","ae3ca97a":"code","bc2cada2":"code","bccbd5c7":"code","fdf4eac7":"code","8f76344c":"code","5a5d621e":"code","0c53cb6f":"code","7aaa17bd":"code","39f0abb7":"markdown","87d78283":"markdown","29c71a94":"markdown","aa5cdaad":"markdown","f80b3a14":"markdown","aa72ed44":"markdown","aea242f6":"markdown","1a3bd86a":"markdown","9a2d95a0":"markdown","ff8ae31b":"markdown","a50c2161":"markdown","aaff4d76":"markdown","5153c569":"markdown","a57ac9bc":"markdown","5db2f14a":"markdown","12a93094":"markdown","61c85151":"markdown","a7435960":"markdown","17e017fa":"markdown","ea375223":"markdown","d6357930":"markdown","9c16c507":"markdown","a51a091d":"markdown","f37838a4":"markdown","8a2c87c4":"markdown","05f6ff17":"markdown","cfaa6c9c":"markdown"},"source":{"f22e50d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","f0a8de8a":"data = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')\ndata.head()","c623bc4d":"data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True, axis=1)\ndata.columns = ['label','message']\ndata.head()","92162004":"data.describe()","656fb520":"data.groupby(['label']).describe()","0be03ca0":"sns.countplot(data.label)","b61c2e28":"data.label.replace({'ham':0,'spam':1}, inplace=True)","14f660f2":"plt.figure(figsize=(12,8))\ndata[data.label==0].message.apply(len).plot(kind='hist',alpha=0.6,bins=35,label='Ham messages')\ndata[data.label==1].message.apply(len).plot(kind='hist', color='red',alpha=0.6,bins=35,label='Ham messages')\n\nplt.legend()\nplt.xlabel(\"Message Length\")\nplt.show()","cfa477de":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(data[data.label == 0].message))\nplt.imshow(wc , interpolation = 'bilinear')","7e2c8046":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(data[data.label == 1].message))\nplt.imshow(wc , interpolation = 'bilinear')","7d09ff01":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(data.message)\ncorpus[:5]\nfrom collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","87324cb6":"sns.barplot(x=list(most_common.values()),y=list(most_common.keys()))","dc34d804":"def get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize = (16,9))\nmost_common_bi = get_top_text_ngrams(data.message,10,2)\nmost_common_bi = dict(most_common_bi)\nsns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))","afd59c79":"plt.figure(figsize = (16,9))\nmost_common_bi = get_top_text_ngrams(data.message,10,3)\nmost_common_bi = dict(most_common_bi)\nsns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))","3562fa4a":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', '\u00fc', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","7a1ba4c9":"textFeatures = data['message'].copy()\ntextFeatures = textFeatures.apply(text_process)\nvectorizer = TfidfVectorizer(\"english\")\nfeatures = vectorizer.fit_transform(textFeatures)\n\nX_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.3, random_state=111)\n","487c8298":"clf = LogisticRegressionCV(cv=10, random_state=0)\nclf.fit(X_train, y_train)\nclf_prediction = clf.predict(X_test)\nlg_acc = accuracy_score(y_test,clf_prediction)\nprint('\\n')\nprint('Accuracy of Naive Bayes =',accuracy_score(y_test,clf_prediction))\nprint('\\n-----------------\\n')\nprint(classification_report(y_test,clf_prediction))","165e5771":"cf_matrix = confusion_matrix(y_true=y_test, y_pred=clf_prediction)\ngroup_names = ['True pos','False Pos','False Neg','True neg']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(15,10))\n\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label');","3ab6b257":"mnb = MultinomialNB(alpha=0.2)\nmnb.fit(X_train, y_train)\nmnb_prediction = mnb.predict(X_test)\nmnb_acc=accuracy_score(y_test,mnb_prediction)\nprint('\\n')\nprint('Accuracy of Naive Bayes =',accuracy_score(y_test,mnb_prediction))\nprint('\\n-----------------\\n')\nprint(classification_report(y_test,mnb_prediction))","ae3ca97a":"cf_matrix = confusion_matrix(y_true=y_test, y_pred=mnb_prediction)\ngroup_names = ['True pos','False Pos','False Neg','True neg']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(15,10))\n\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label');","bc2cada2":"svm = SVC(kernel='sigmoid', gamma=1.0)\nsvm.fit(X_train, y_train)\nsvm_prediction = svm.predict(X_test)\nsvm_acc = accuracy_score(y_test,svm_prediction)\nprint('\\n')\nprint('Accuracy of Naive Bayes =',accuracy_score(y_test,svm_prediction))\nprint('\\n-----------------\\n')\nprint(classification_report(y_test,svm_prediction))","bccbd5c7":"cf_matrix = confusion_matrix(y_true=y_test, y_pred=svm_prediction)\ngroup_names = ['True pos','False Pos','False Neg','True neg']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(15,10))\n\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label');","fdf4eac7":"rf = RandomForestClassifier(n_estimators=50, max_depth=None, n_jobs=-1)\nrf.fit(X_train, y_train)\nrf_prediction = rf.predict(X_test)\nrf_acc = accuracy_score(y_test,rf_prediction)\nprint('\\n')\nprint('Accuracy of Naive Bayes =',accuracy_score(y_test,rf_prediction))\nprint('\\n-----------------\\n')\nprint(classification_report(y_test,rf_prediction))","8f76344c":"cf_matrix = confusion_matrix(y_true=y_test, y_pred=rf_prediction)\ngroup_names = ['True pos','False Pos','False Neg','True neg']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(15,10))\n\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label');","5a5d621e":"acc = np.argmax([lg_acc,mnb_acc,svm_acc,rf_acc])\nclassification = {0:'Logistic Regression',1:'Naive bayes',2:'Support Vector Classifcation',3:'Random Forest Classifier'}\nprint('Best classifier based on accuracy is {}'.format(classification[acc]))","0c53cb6f":"print('Logistic Regression  is ',precision_score(y_test, clf_prediction))\nprint('\\n')\nprint('Naive Bayes is ',precision_score(y_test, mnb_prediction))\nprint('\\n')\nprint('Support Vector Machine Regression is ',precision_score(y_test, svm_prediction))\nprint('\\n')\nprint('Random Forest Classifer is ',precision_score(y_test, rf_prediction))","7aaa17bd":"print('Logistic Regression  is ',recall_score(y_test, clf_prediction))\nprint('\\n')\nprint('Naive Bayes is ',recall_score(y_test, mnb_prediction))\nprint('\\n')\nprint('Support Vector Machine Regression is ',recall_score(y_test, svm_prediction))\nprint('\\n')\nprint('Random Forest Classifer is ',recall_score(y_test, rf_prediction))","39f0abb7":"<a id = 'model'><\/a>\n# <center>Model Building<\/center>","87d78283":"<a id='conclusion'><\/a>\n# <center>Conclusion<\/center>","29c71a94":"# <center>Email Spam Classifer<\/center>\n## <center>Clasify the Collection of SMS messages to tagged as spam or legitimate<\/center>","aa5cdaad":"## Content\n * [***Introduction***](#introduction)\n * [***Data Visualization***](#visualization)\n   * [***Distribution of text***](#distribution)\n   * [***Wordcloud***](#wordcloud)\n   * [***N-grams visualization***](#ngram)\n * [***Data pre-processing***](#preprocess)\n * [***Model Building***](#model)\n   * [***Logistic Regression***](#logistic)\n   * [***Naive Bayes***](#naive)\n   * [***Support Vector Machine***](#support)\n   * [***Random Forest Classifer***](#random)\n * [***Conclusion***](#conclusion)","f80b3a14":"## Bi-gram","aa72ed44":">   To understand **Precision and Recall Trade Off**: \n>    - To fully evaluate the effectiveness of a model, we must examine both precision and recall.<br> <\/t>Unfortunately, precision and recall are often in tension. That is, improving precision typically reduces recall and vice versa.","aea242f6":"<a id = 'introduction'><\/a>\n# <Center>Introduction<\/center>\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. <br> The files contain one message per line. Each line is composed by two columns: **Label** contains the label (ham or spam) and **message** contains the raw text.","1a3bd86a":"## WORDCLOUD FOR TEXT THAT IS NOT SPAM","9a2d95a0":"<a id='naive'><\/a>\n## Naive Bayes","ff8ae31b":"> ### Considering overall performance of *** Precision and Recall***\n>  - Naive Bayes is Best Model.\n\n> ### Considering ***Precision***\n> - Random Forest is Best Model.\n\n> ### Considering ***Recall***\n> - Naive Bayes is best model.\n\n","a50c2161":"## Tri-gram","aaff4d76":"We have Tried **Logistic Regression, Naive Bayes, Support Vector Machine and Random Forest classifier** to compare which model perform best.","5153c569":"## Uni-gram","a57ac9bc":"<a id = 'preprocess'><\/a>\n# <center>Data Pre-Processing<\/center>","5db2f14a":"<a id='wordcloud'><\/a>\n## <center>WORDCLOUD<\/center>","12a93094":"> Based on accuracy of models **Naive Bayes** outperform other classifier.","61c85151":"<a id='Support'><\/a>\n## Support Vector Machine","a7435960":" >   But in our case we wish to more focus on reducing **False Positive**.\n >   - we want to select a model which doesn't classify Normal(Ham) messages as Spam messages because this could cause more   harm as important messages can be classified as spam and it can cause loss.\n >\n >   - so, In our case we should select model whose **precision** is better and also considering **Recall**.","17e017fa":"### Recall of Models","ea375223":"<a id='distribution'><\/a>\n## Distribution of Text\nLength of data is analysis","d6357930":"<a id ='visualization'><\/a>\n# <center>Data Visualization<\/center>\nBasic exploration of data to understand data.","9c16c507":"> ### Overall We should Select Either Naive Bayes model or Logistic Regession in Spam Classsifier Use Case.\n","a51a091d":"<a id='ngram'><\/a>\n## <center>N-gram visualization<\/center>","f37838a4":"### precision of Models","8a2c87c4":"<a id= 'logistic'><\/a>\n## Logistic Regression","05f6ff17":"## WORDCLOUD FOR TEXT THAT IS SPAM","cfaa6c9c":"<a id='random'><\/a>\n## Random Forest Classifier"}}