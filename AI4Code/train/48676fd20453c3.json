{"cell_type":{"75418124":"code","0817f00e":"code","7b5a7982":"code","9eb0ecfe":"code","7addee8d":"code","9bf4e373":"code","a5d9b927":"code","aafa28c5":"code","6a54342e":"code","a8521420":"code","d6299c55":"code","20053fa0":"code","355f907e":"code","e8275d5f":"code","2122ca8b":"code","174cd104":"code","3e92e5ff":"code","e862c82c":"code","141cc463":"code","f844a55d":"code","e7ce2d62":"code","7a158ecd":"code","b12f9f9f":"code","d49f928d":"code","ec36953e":"code","f1d2cde1":"code","5be82337":"code","31da0d3e":"code","765f0d49":"code","c793ee92":"code","fd6d9171":"code","432daee8":"code","bcc88f9e":"code","4520c673":"code","752db268":"code","4607b039":"code","f9bc66fa":"code","c7c34f5a":"code","3dcad855":"code","e020f5c7":"code","5acdeabb":"code","0233cf81":"code","47b016a1":"code","2864cff4":"code","5e6bb1cb":"code","e1edc780":"code","ff902081":"code","bd86c130":"code","c2834338":"code","6cbef8a9":"code","5b91af3d":"code","3151b9a8":"code","31a083dd":"code","1f3e765f":"code","0ad2e892":"code","0c94e3a6":"code","5e2a2e08":"markdown","7309ccac":"markdown","33e65a7c":"markdown","1cdab0be":"markdown","ff758ee3":"markdown","9109b4fe":"markdown","6bc16428":"markdown","5a75069b":"markdown","1cdb9378":"markdown","509c4097":"markdown","b7ffa4e4":"markdown","a67bd724":"markdown","3662df70":"markdown","03616db6":"markdown","32a4ff95":"markdown","f6ea4ca2":"markdown","b6381f67":"markdown","34672f15":"markdown","b663d5c4":"markdown","2837db7d":"markdown"},"source":{"75418124":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0817f00e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nmovies=pd.read_csv('\/kaggle\/input\/all-us-movies-imdb-from-1972-to-2016\/movies_imdb_us_1972_2016_cs.csv')","7b5a7982":"#checking the data.\nmovies.info()","9eb0ecfe":"movies.isnull().sum()","7addee8d":"movies.columns.values","9bf4e373":"movies.dropna(subset=['genre','votes','cast'],inplace=True)","a5d9b927":"movies.duplicated(['title','year','director']).sum()\ndup=movies[movies.duplicated(['title','year','director'])]\nmovies.drop_duplicates(subset=['title','year','director'],inplace=True,keep='first')","aafa28c5":"u=movies[movies['rating'].isna() & movies['gross'].isna()]\nmovies.drop(u.index, axis=0,inplace=True)\nyr=movies[movies['year']>2019]\nmovies.drop(yr.index, axis=0,inplace=True)","6a54342e":"#numerical or categorical\nnumerical_cols = [col for col in movies.columns if movies[col].dtype != 'object']\ncategorical_cols = [col for col in movies.columns if movies[col].dtype == 'object']\n","a8521420":"numerical_cols","d6299c55":"categorical_cols","20053fa0":"eda_num=movies[numerical_cols].describe()\neda_cat=movies[categorical_cols].describe()","355f907e":"eda_num","e8275d5f":"eda_cat","2122ca8b":"#No of unique director?\nmovies.director.nunique()","174cd104":"#genre frequency?\ngenre_freq=pd.DataFrame(movies.genre.value_counts())\n\ngenre_freq.reset_index(inplace=True)\ngenre_freq.columns=['genre','freq']\ngenre_freq.head(10)\n","3e92e5ff":"movies.reset_index(inplace = True, drop = True)\n\n#lets seperate the movies with no ratings. We will try to predict their ratings later after fitting a model.\nmovies.isnull().sum()\nwithout_ratings=movies[movies['rating'].isna()]\nmovies.drop(without_ratings.index, axis=0,inplace=True)","e862c82c":"#profiling\nimport pandas_profiling\nmovies.profile_report()","141cc463":"#lets count number of movies done by a director.\n#Data Analysis\n\ndirector_name_value_counts = movies.director.value_counts()\ndirector_name_value_counts  = pd.DataFrame(director_name_value_counts).reset_index().rename(columns = {'index': 'Director', 'director':'director_name_value_counts'})\n\nmovies=pd.merge(movies,director_name_value_counts,left_on='director',right_on='Director',how='left')\nmovies = movies.drop(columns = 'director')","f844a55d":"movies['main_genre'] = movies.genre.str.split('|').str[0]\n\n#no of unique main genre\nmovies.main_genre.nunique()","e7ce2d62":"#converting main genre column to numerical\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nmovies['main_genre'] = le.fit_transform(movies.main_genre)\n","7a158ecd":"#genre count\ngenre_value_counts = movies.genre.value_counts()\ngenre_value_counts  = pd.DataFrame(genre_value_counts).reset_index().rename(columns = {'index': 'genre', 'genre':'genre_value_counts'})\n\nmovies=pd.merge(movies,genre_value_counts,left_on='genre',right_on='genre',how='left')\n","b12f9f9f":"\nmovies['actor_1'] = movies.cast.str.split('|').str[0]\nmovies['actor_2'] = movies.cast.str.split('|').str[1]\nmovies['actor_3'] = movies.cast.str.split('|').str[2]\nmovies['actor_4'] = movies.cast.str.split('|').str[3]\nmovies['Less_than_five_actors']=movies.cast.str.split('|').str[4].isna()","d49f928d":"#convering to numerical\nmovies['Less_than_five_actors'] = le.fit_transform(movies.Less_than_five_actors)\n\n\n#actors value counts\nactor1_name_value_counts = movies.actor_1.value_counts()\nactor1_name_value_counts  = pd.DataFrame(actor1_name_value_counts).reset_index().rename(columns = {'index': 'actor', 'actor_1':'actor1_name_value_counts'})\n\nmovies=pd.merge(movies,actor1_name_value_counts,left_on='actor_1',right_on='actor',how='left')\nmovies = movies.drop(columns = ['actor_1','actor'])\n\nactor2_name_value_counts = movies.actor_2.value_counts()\nactor2_name_value_counts  = pd.DataFrame(actor2_name_value_counts).reset_index().rename(columns = {'index': 'actor', 'actor_2':'actor2_name_value_counts'})\n\nmovies=pd.merge(movies,actor2_name_value_counts,left_on='actor_2',right_on='actor',how='left')\nmovies = movies.drop(columns = ['actor_2','actor'])\n\nactor3_name_value_counts = movies.actor_3.value_counts()\nactor3_name_value_counts  = pd.DataFrame(actor3_name_value_counts).reset_index().rename(columns = {'index': 'actor', 'actor_3':'actor3_name_value_counts'})\n\nmovies=pd.merge(movies,actor3_name_value_counts,left_on='actor_3',right_on='actor',how='left')\nmovies = movies.drop(columns = ['actor_3','actor'])\n\nactor4_name_value_counts = movies.actor_4.value_counts()\nactor4_name_value_counts  = pd.DataFrame(actor4_name_value_counts).reset_index().rename(columns = {'index': 'actor', 'actor_4':'actor4_name_value_counts'})\n\nmovies=pd.merge(movies,actor4_name_value_counts,left_on='actor_4',right_on='actor',how='left')\nmovies = movies.drop(columns = ['actor_4','actor'])\n","ec36953e":"movies['certificate'].fillna(movies['certificate'].mode()[0],inplace=True)\n\nmovies['certificate'] = le.fit_transform(movies.certificate)","f1d2cde1":"movies = movies.drop(columns = ['genre','Director','cast','title','gross'])\nmovies['runtime'].fillna(movies['runtime'].mean(),inplace=True)\nmovies['actor2_name_value_counts'].fillna(0,inplace=True)\nmovies['actor3_name_value_counts'].fillna(0,inplace=True)\nmovies['actor4_name_value_counts'].fillna(0,inplace=True)","5be82337":"movies.profile_report()","31da0d3e":"#modelling\n\nmoviesR = movies.copy() #lets keep our original movies for reference. Here moviesR is for Regression model\nmoviesC = movies.copy() #Here moviesC is for classification model\n","765f0d49":"from sklearn.model_selection import train_test_split\ny = moviesR.pop('rating')\nX = moviesR\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 42)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns, index=X_train.index)\n\nX_test = pd.DataFrame(scaler.transform(X_test.values), columns = X_train.columns, index = X_test.index)\n","c793ee92":"#removing variables with high colinearity\ndef correlation(movies, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = movies.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in movies.columns:\n                    del movies[colname] # deleting the column from the movies\ncorrelation(X_train,0.90)","fd6d9171":"#importing the required libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n\n# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)            # running RFE\nrfe = rfe.fit(X_train, y_train)\n\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))\ncol_rfe = X_train.columns[rfe.support_]\ncol_rfe\nX_train.columns[~rfe.support_]\n\n#Creating a X_train dataframe with rfe varianles\nX_train_rfe = X_train[col_rfe]\n","432daee8":"#Simple Linear Regression\n# Adding a constant variable for using the stats model\nimport statsmodels.api as sm\nX_train_rfe_constant = sm.add_constant(X_train_rfe)\nlm = sm.OLS(y_train,X_train_rfe_constant).fit()   # Running the linear model\n\n#Let's see the summary of our linear model\nprint(lm.summary())\n\nX_test_rfe = X_test[col_rfe]\nX_test_rfe_constant = sm.add_constant(X_test_rfe)\n\ny_pred_linear = lm.predict(X_test_rfe_constant)\ny_pred_linear.values\ny_pred_linear.min(), y_pred_linear.max()\n\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_pred_linear, y_test)\n\n","bcc88f9e":"#SVM\nfrom sklearn.svm import SVR\nsvr_rbf = SVR(kernel='rbf', gamma=0.1)\nsvr_lin = SVR(kernel='linear', gamma='auto')\nsvr_poly = SVR(kernel='poly', gamma='auto', degree=3)\n\n#SVM with RBF\nsvr_rbf.fit(X_train_rfe, y_train)\ny_pred_svm_rbf = svr_rbf.predict(X_test_rfe)\n\ny_pred_svm_rbf\ny_pred_svm_rbf.min(), y_pred_svm_rbf.max()\nmean_squared_error(y_pred_svm_rbf, y_test)","4520c673":"#SVM linear\nsvr_lin.fit(X_train_rfe, y_train)\ny_pred_svm_lin = svr_lin.predict(X_test_rfe)\nmean_squared_error(y_pred_svm_lin, y_test)\n\ndata = [['Linear regression', 0.90], ['RBF_SVM', 0.88],['SVM_linear',0.90]] \nMSE_table = pd.DataFrame(data, columns = ['Model', 'mse']) \n\n#SVM_poly\nsvr_poly.fit(X_train_rfe, y_train)\ny_pred_svm_poly = svr_poly.predict(X_test_rfe)\nmean_squared_error(y_pred_svm_poly, y_test)\n\nMSE_table = MSE_table.append({'Model':'SVM_poly', 'mse':1.01}, ignore_index=True)","752db268":"MSE_table","4607b039":"#ensemble models\n#Gradient Boosting with Hyper Parameter Tuning\n\nfrom sklearn import ensemble\nn_trees=200\ngradientboost = ensemble.GradientBoostingRegressor(loss='ls',learning_rate=0.04,n_estimators=n_trees,max_depth=4)\ngradientboost.fit(X_train_rfe,y_train)\n\ny_pred_gb=gradientboost.predict(X_test_rfe)\nerror=gradientboost.loss_(y_test,y_pred_gb) ##Loss function== Mean square error\nprint(\"MSE:%.3f\" % error)\nMSE_table = MSE_table.append({'Model':'GradientBoostingRegressor', 'mse':0.605}, ignore_index=True)","f9bc66fa":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'loss' : ['ls'],\n    'max_depth' : [3, 4, 5],\n    'learning_rate' : [0.01, 0.001],\n    'n_estimators': [100, 200, 500]\n}\n# Create a based model\ngb = ensemble.GradientBoostingRegressor()\n# Instantiate the grid search model\ngrid_search_gb = GridSearchCV(estimator = gb, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search_gb.fit(X_train_rfe, y_train)\ngrid_search_gb.best_params_\ngrid_search_gb_pred = grid_search_gb.predict(X_test_rfe)\nmean_squared_error(y_test.values, grid_search_gb_pred)","c7c34f5a":"#Random Forest with Hyper Parameter Tuning\nfrom sklearn.ensemble import RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators = 500)\nrf_regressor.fit(X_train_rfe, y_train)\nrf_pred = rf_regressor.predict(X_test_rfe)\nmean_squared_error(rf_pred, y_test)\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [90, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100,200,300,400, 500,600, 1000]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search_rf.fit(X_train_rfe, y_train)\ngrid_search_rf.best_params_\n\ny_grid_pred_rf = grid_search_rf.predict(X_test_rfe)\nmean_squared_error(y_grid_pred_rf, y_test.values)\n\n\nMSE_table = MSE_table.append({'Model':'RandomForestRegressor', 'mse':0.61}, ignore_index=True)\n","3dcad855":"#XGBoost with Hyperparameter tuning\nimport xgboost as xgb\nxg_model = xgb.XGBRegressor(n_estimators = 500)\nxg_model.fit(X_train_rfe, y_train)\n\nresults = xg_model.predict(X_test_rfe)\nmean_squared_error(results, y_test.values)\n","e020f5c7":"xg_model.score(X_train_rfe, y_train)","5acdeabb":"from sklearn.metrics import r2_score\nr2_score(y_test, results)","0233cf81":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [3, 4],\n    'learning_rate' : [0.1, 0.01, 0.05],\n    'n_estimators' : [100, 500, 1000]\n}\n# Create a based model\nmodel_xgb= xgb.XGBRegressor()\n# Instantiate the grid search model\ngrid_search_xgb = GridSearchCV(estimator = model_xgb, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\ngrid_search_xgb.fit(X_train_rfe, y_train)\ngrid_search_xgb.best_params_\ny_pred_xgb = grid_search_xgb.predict(X_test_rfe)\nmean_squared_error(y_test.values, y_pred_xgb)","47b016a1":"MSE_table = MSE_table.append({'Model':'XGBoost', 'mse':mean_squared_error(y_test.values, y_pred_xgb)}, ignore_index=True)\n","2864cff4":"feature_importance = grid_search_xgb.best_estimator_.feature_importances_\nsorted_importance = np.argsort(feature_importance)\npos = np.arange(len(sorted_importance))\nplt.figure(figsize=(12,5))\nplt.barh(pos, feature_importance[sorted_importance],align='center')\nplt.yticks(pos, X_train_rfe.columns[sorted_importance],fontsize=15)\nplt.title('Feature Importance ',fontsize=18)\nplt.show()","5e6bb1cb":"#Building a Classification Model\n\ny_train_classification = y_train.copy()\ny_train_classification = pd.cut(y_train_classification, bins=[1, 3, 6, float('Inf')], labels=['Flop Movie', 'Average Movie', 'Hit Movie'])\n\ny_test_classification = y_test.copy()\ny_test_classification = pd.cut(y_test_classification, bins=[1, 3, 6, float('Inf')], labels=['Flop Movie', 'Average Movie', 'Hit Movie'])\n\n\nX_train_rfe_classification = X_train_rfe.copy()\nX_test_rfe_classification = X_test_rfe.copy()\n","e1edc780":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogit_model = LogisticRegression(solver = 'saga', random_state = 0)\nlogit_model.fit(X_train_rfe_classification, y_train_classification)\n\ny_logit_pred = logit_model.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_logit_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_logit_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_logit_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_logit_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_logit_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","ff902081":"#Support Vector Classifier with Linear, Polynomial, RBF\n\nfrom sklearn.svm import SVC\nsvc_linear_model = SVC(kernel='linear', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)\n\nsvc_linear_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_linear_pred = svc_linear_model.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_linear_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_linear_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_linear_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_linear_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_linear_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))\n","bd86c130":"from sklearn.svm import SVC\nsvc_poly_model = SVC(kernel='poly', C=100, gamma= 'scale', degree = 3, decision_function_shape='ovo', random_state = 42)\n\nsvc_poly_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_poly_pred = svc_poly_model.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_poly_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_poly_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_poly_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_poly_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_poly_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","c2834338":"from sklearn.svm import SVC\nsvc_rbf_model = SVC(kernel='rbf', C=100, gamma= 'scale', decision_function_shape='ovo', random_state = 42)\n\nsvc_rbf_model.fit(X_train_rfe_classification, y_train_classification)\ny_svc_rbf_pred = svc_rbf_model.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_svc_rbf_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_svc_rbf_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_svc_rbf_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_svc_rbf_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_svc_rbf_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","6cbef8a9":"#Ensemble Models\n#Random Forest Classifier with Hyper Parameter tuning\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [90,100],#list(range(90,100)),\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500, 1000],\n    'random_state' :[0]\n}\n# Create a based model\nrf_model_classification = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search_rf_model_classificaiton = GridSearchCV(estimator = rf_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search_rf_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)\n\ny_rf_classification_pred = grid_search_rf_model_classificaiton.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_rf_classification_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_rf_classification_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_rf_classification_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_rf_classification_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_rf_classification_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","5b91af3d":"#Gradient Boost Classifier with Hyper Parameter Tuning\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [10, 50, 90],\n    'max_features': [3],\n    'min_samples_leaf': [3],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 500],\n    'learning_rate' : [0.01, 0.2],\n    'random_state' : [0]\n}\n# Create a based model\ngbc_model_classification = GradientBoostingClassifier()\n# Instantiate the grid search model\ngrid_search_gbc_model_classificaiton = GridSearchCV(estimator = gbc_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search_gbc_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)\ny_gbc_model_pred = grid_search_gbc_model_classificaiton.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_gbc_model_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_gbc_model_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_gbc_model_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_gbc_model_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_gbc_model_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","3151b9a8":"#XG Boost Classifier with Hyper Parameter Tuning\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n     'objective' : ['multi:softmax', 'multi:softprob'],\n     'n_estimators': [100, 500, 1000],\n     'random_state': [0]\n}\n# Create a based model\nxgb_model_classification = XGBClassifier()\n# Instantiate the grid search model\ngrid_search_xgb_model_classificaiton = GridSearchCV(estimator = xgb_model_classification, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search_xgb_model_classificaiton.fit(X_train_rfe_classification, y_train_classification)\n\ny_xgb_classification_pred = grid_search_xgb_model_classificaiton.predict(X_test_rfe_classification)\n\nfrom sklearn import metrics\ncount_misclassified = (y_test_classification != y_xgb_classification_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test_classification, y_xgb_classification_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprecision = metrics.precision_score(y_test_classification, y_xgb_classification_pred, average= 'macro')\nprint('Precision: {:.2f}'.format(precision))\nrecall = metrics.recall_score(y_test_classification, y_xgb_classification_pred, average= 'macro')\nprint('Recall: {:.2f}'.format(recall))\nf1_score = metrics.f1_score(y_test_classification, y_xgb_classification_pred, average = 'macro')\nprint('F1 score: {:.2f}'.format(f1_score))","31a083dd":"#classification model result\n\ndata = {'Classification Model' : ['Logistic Regression', 'SVC_Linear', 'SVC_Poly', 'SVC_RBF', 'Random forest', 'Gradient Boosting','XGBoost'],\n        'MisClassifications':[596,647,516,523,456,441,430],\n        'Accuracy' : [0.68,0.65,0.72,0.72,0.75,0.76,0.77],\n        'Precision' : [0.43,0.22,0.46,0.46,0.49,0.49,0.50],\n        'Recall' : [0.38,0.33,0.44,0.44,0.47,0.48,0.49],\n        'F1-Score' : [0.37,0.26,0.44,0.45,0.47,0.48,0.49]\n        }\nAccuracy_table = pd.DataFrame(data) ","1f3e765f":"feature_importance = grid_search_gbc_model_classificaiton.best_estimator_.feature_importances_\nsorted_importance = np.argsort(feature_importance)\npos = np.arange(len(sorted_importance))\nplt.figure(figsize=(12,5))\nplt.barh(pos, feature_importance[sorted_importance],align='center')\nplt.yticks(pos, X_train_rfe.columns[sorted_importance],fontsize=15)\nplt.title('Feature Importance ',fontsize=18)\nplt.show()","0ad2e892":"Accuracy_table","0c94e3a6":"MSE_table","5e2a2e08":"After looking in to all the metrics almost we have seen that XGBRegressor has given the best results with mean squared error of 0.57. The Feature Importance given by this model is shown above.","7309ccac":"Converting Categoricals to Numericals to feed our model.","33e65a7c":"Profile report.","1cdab0be":"Exploratory analysis of numerical and categorical columns.","ff758ee3":"Let's tune some parameters.","9109b4fe":"Lets try to find some other variables which might effect a movie rating. like reputation\/popularity of an actor.Here there's no way we can find the popularity of an actor.So, let's count the number of movies done by each actor'actress'.","6bc16428":"So, the data has 9940 entries. 'certificate' and 'gross' column has most number of 'NA'. ","5a75069b":"So, the most US movies are of drama genre and director Woody Allen has directed most number of movies in this period.","1cdb9378":"Rating->(1-3)-> Flop Movie Rating->(3-6)-> Average Movie Rating->(6-10)-> Hit Movie","509c4097":"## MSE=0.8991","b7ffa4e4":"Drop duplicates.","a67bd724":"Linear Regression","3662df70":"As we see that the XGBoost with Hyper Parameter seems to give us the best Results.","03616db6":"Profile report again after missing value treatmenta and adding few new columns.","32a4ff95":"Linear,Polynomial and RBF SVM","f6ea4ca2":"From the exploratory analysis of numerical columns it is clear that most of the US films are less than 113 mins with average duration being 105 mins. And the average rating a movie gets is 6.4, most of the films are between 5.7-7.1 rating. So, we can say most of the US films are average or of just above average ratings. Also, most of the movies earns between 3-4 million dollars.","b6381f67":"Copying the data for regression and classification.","34672f15":"Hmmmm. So, we have problem. We have two columns with high number of missing values and also we have columns with high cardinality. 'gross' column has the most number of missing values. let's delete this column and certificate column we will try to fill by mode value.","b663d5c4":"Classification model building. \nTo Build a classification Model I would like to reuse the preprocessed data from the Regression Model. However I am going to replace the target variable and create a new target variable for our classification Model.","2837db7d":"Let's fill the missing certificates with mode."}}