{"cell_type":{"0978abe3":"code","8a8b04c4":"code","c3f00cde":"code","e8564230":"code","91bd5bd2":"code","e336acb7":"code","7ddf4dc4":"code","61adb3f1":"code","2817cc02":"code","4b67557c":"code","72aa6688":"code","dea5b554":"code","20e978e9":"code","9f99a0ec":"code","f3476ea3":"code","c401d762":"code","a39a9a47":"code","2937ac0f":"code","fe7e86cf":"code","e3ad0e32":"code","59e91f3e":"code","8ad11842":"code","70ff823b":"code","59bcfb8d":"code","a5671a4a":"code","4c3206c8":"code","b90db60a":"code","5c3cc7ff":"code","8d688e97":"code","a4c908f2":"code","0ef3650e":"code","e50ad041":"code","480a64d1":"code","23edc15b":"markdown","c6d3871c":"markdown","d91a7a2e":"markdown","f49cc447":"markdown","de0b7827":"markdown","fdc09929":"markdown","b3a42e38":"markdown","91294fa1":"markdown","2c7027d3":"markdown","95f62ac5":"markdown","b5d2a0a7":"markdown","02fbb865":"markdown","f5b5d862":"markdown","1f0976bf":"markdown","a4ab0202":"markdown","3e7e41ab":"markdown","d95a421c":"markdown","4e96918a":"markdown","df1db96b":"markdown","071e1be2":"markdown","98c9adb3":"markdown","32387e92":"markdown","398bb852":"markdown","44157b4d":"markdown","a10ea697":"markdown","0b48d0b5":"markdown","a35127f2":"markdown","7da1e753":"markdown","0346d626":"markdown","6e3376cc":"markdown","b0238c00":"markdown","be34c42c":"markdown","694aeff2":"markdown","43cdf2e5":"markdown","1554a324":"markdown","c10ff395":"markdown","343ee11a":"markdown","b2b09c45":"markdown","da72c5c7":"markdown","ec6930b7":"markdown","1cf1fdd8":"markdown","fbda6cb8":"markdown","80943918":"markdown","8461540a":"markdown","b4c9963e":"markdown","6cf15480":"markdown","91361ed4":"markdown","0ab435ff":"markdown","cee9991f":"markdown","8e0ba229":"markdown","a6de59ac":"markdown","d1597bf7":"markdown","789339a3":"markdown","017c9e3c":"markdown","65ab7ceb":"markdown","5ac235d0":"markdown","549961ba":"markdown"},"source":{"0978abe3":"import numpy as np\nimport pandas as pd\n\ndata = pd.read_csv(\"\/kaggle\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\")\ndf = pd.DataFrame(data)","8a8b04c4":"df = df.dropna(subset = ['Season', 'Radiant temperature (C)', 'Thermal sensation',\n                         'Thermal comfort', 'Thermal preference'])\ndf['Country'].value_counts()","c3f00cde":"df = df[['Country', 'Season', 'Radiant temperature (C)', 'Thermal sensation', \n         'Thermal preference', 'Thermal comfort', 'Air temperature (C)', \n         'Relative humidity (%)', 'Clo', 'Air velocity (m\/s)','Met', 'Age', \n         'Cooling startegy_building level', 'Sex']]\ndf = df[df['Country'] == 'Australia']\ndf_Aus = df[df['Cooling startegy_building level'] == 'Air Conditioned']\ndf_Aus = df_Aus.dropna()\ndf_Aus['Season'].value_counts()","e8564230":"df_Aus['Thermal comfort'] = df_Aus['Thermal comfort'].map(lambda x : int(x))\ndf_Aus['Thermal sensation'] = df_Aus['Thermal sensation'].map(lambda x : int(x))\n\n# df_Aus = df_Aus.round({'Thermal sensation': 0, 'Thermal comfort': 0})","91bd5bd2":"d = {'cooler': -1, 'warmer': 1, 'no change': 0}\ndf_Aus['Thermal preference'] = df_Aus['Thermal preference'].map(d)\ndf_Aus['Thermal preference'].head()","e336acb7":"df_Aus['Preferred thermal sensation'] = df_Aus['Thermal sensation'] + (df_Aus['Thermal preference'] * (6 - df_Aus['Thermal comfort']))\ndf_Aus['Preferred thermal sensation'] = df_Aus['Preferred thermal sensation'].map(lambda x: int(max(min(x,3),-3)))\ndf_Aus['Preferred thermal sensation'].head()","7ddf4dc4":"df_Aus['Preferred thermal sensation'].value_counts()","61adb3f1":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\nlength = len(df_Aus['Preferred thermal sensation'])\nneutral_thermal = [0 for i in range(length)]\ncategories = np.array([-3,-2,-1,0,1,2,3])\ncm = confusion_matrix(neutral_thermal, df_Aus['Preferred thermal sensation'])\nnp.seterr(divide='ignore', invalid='ignore')\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.imshow(cm_normalized, interpolation='nearest', cmap='Reds')\nplt.title('Normalized Thermal Sensation Difference Matrix')\nplt.colorbar()\ntick_marks = np.arange(len(categories))\nplt.xticks(tick_marks,categories)\nplt.yticks(tick_marks,categories)\nplt.tight_layout()\nplt.ylabel('Neutral thermal sensation')\nplt.xlabel('Preferred thermal sensation')\nplt.tight_layout()\n\nprint(f\"Weighted f1 score: {round(f1_score(neutral_thermal, df_Aus['Preferred thermal sensation'], average='weighted'), 3)} (3dp)\")","2817cc02":"df_Spring = df_Aus[df_Aus['Season'] == 'Spring']\ndf_Summer = df_Aus[df_Aus['Season'] == 'Summer']\ndf_Autumn = df_Aus[df_Aus['Season'] == 'Autumn']\ndf_Winter = df_Aus[df_Aus['Season'] == 'Winter']","4b67557c":"df_Aus = pd.get_dummies(df_Aus.drop(columns = ['Country', 'Season', \n                                               'Cooling startegy_building level']))\ndf_Aus.head()","72aa6688":"response = df_Aus[['Thermal sensation', 'Thermal preference', 'Thermal comfort', \n                   'Preferred thermal sensation']].reset_index(drop=True)\ndf_Aus = df_Aus.drop(columns = ['Thermal sensation', 'Thermal preference', 'Thermal comfort', \n                                'Preferred thermal sensation'])","dea5b554":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, target_train, target_test = train_test_split(df_Aus, response['Thermal sensation'], test_size=0.3, random_state=2)","20e978e9":"# Future work: convert to tensorflow for TPU support\n\n# from xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\nimport time\n\n# To time how long this hyperparameter tuning takes\nstart = time.time()\n\nmodels = [RandomForestClassifier(), KNeighborsClassifier(), DecisionTreeClassifier(), \n          MLPClassifier(), AdaBoostClassifier(), SVC(), DummyClassifier()]\nmodelnames = {models[0]:'model_rf', models[1]:'model_kn', models[2]:'model_dt', \n              models[3]:'model_mlp', models[4]:'model_abc', \n              models[5]:'model_svc', models[6]:'model_dc'}\n\nparametersgrid = {'model_xgb': {'learning_rate': np.arange(0.01, 0.5, 0.02).tolist(),\n                                'n_estimators': range(1000, 8000, 1000),\n                                'max_depth': range(1, 20, 1),\n                                'min_child_weight': range(1, 20, 1),\n                                'gamma': np.arange(0, 0.7, 0.1).tolist(),\n                                'subsample': np.arange(0.4, 1, 0.1).tolist(),\n                                'colsample_bytree': np.arange(0.4, 1, 0.1).tolist(),\n                                'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]}, \n                  'model_rf': {'n_estimators': [1, 10, 20, 40, 70, 100, 150, \n                                                200, 250, 300, 500],  \n                               'min_samples_leaf': range(1, 20, 1), \n                               'min_samples_split': range(1, 10, 1)}, \n                  'model_kn': {'n_neighbors': [1, 10, 20, 40, 70, 100, 150, \n                                               200, 250, 300, 500],  \n                               'leaf_size': range(5, 100, 5),\n                               'p': [1,2],\n                               'weights': ['uniform', 'distance']}, \n                  'model_dt': {'min_samples_leaf': range(5, 100, 5),  \n                               'min_samples_split': range(5, 100, 5)}, \n                  'model_mlp': {'activation': ['identity', 'logistic', 'tanh', 'relu'],  \n                                'solver': ['lbfgs', 'sgd', 'adam'], \n                                'max_iter': [2000]}, \n                  'model_abc': {'n_estimators': [1, 10, 20, 40, 50, 70, 80, 100, 150,\n                                                 200, 250, 300, 500],  \n                                'algorithm': ['SAMME', 'SAMME.R']}, \n                  'model_svc': {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  \n                                'C': np.arange(0.2, 10, 0.2).tolist()}, \n                  'model_dc': {'random_state': [2]}}\n\n\nfor model in models:\n    modelname = modelnames[model]\n    print(f'Tuning {modelname}: ')\n    grid = RandomizedSearchCV(model, parametersgrid[modelname], refit = True, verbose = 3, \n                              n_jobs = -1, n_iter = 20)\n    grid.fit(features_train, target_train) \n    globals()[f'{modelname}_bestparams'] = grid.best_params_ \n    grid_predictions = grid.predict(features_test)\n    globals()[f'{modelname}_f1score'] = f1_score(target_test, grid_predictions, \n                                                 average = 'weighted')\n    globals()[f'{modelname}_kfoldscore'] = grid.best_score_\n    \n    # This 2s sleep is here to ensure that the outputs are in the correct order,\n    # because the output for each loop used to overlap with each other.\n    time.sleep(2)\n\n\nfor model in models:\n    modelname = modelnames[model]\n    print(f\"{modelname} best parameters: {globals()[f'{modelname}_bestparams']}\")\n    print(f\"{modelname} best f1 score: {globals()[f'{modelname}_f1score']}\")\n    print(f\"{modelname} best k-fold score: {globals()[f'{modelname}_kfoldscore']}\" + '\\n')\n    \ntemp = (time.time() - start)\nhours = int(temp\/\/3600)\ntemp = temp - 3600*hours\nminutes = int(temp\/\/60)\nseconds = round(temp - 60*minutes, 3)\nprint(f'Time taken: {hours}hrs, {minutes}mins, {seconds}secs')\n","9f99a0ec":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.ensemble import RandomForestClassifier\n\nau_model = RandomForestClassifier(n_estimators = 40, min_samples_split = 4, \n                                  min_samples_leaf = 15)\nau_model.fit(features_train, target_train)\nkf = KFold(n_splits=5)\ny_pred = au_model.predict(features_test)\n\nweightedf1 = f1_score(target_test, y_pred, average = 'weighted', labels=np.unique(y_pred))\nprint(f'weighted f1 score: {round(weightedf1, 2)}')\n\nscores = cross_val_score(au_model, features_train, target_train, cv=kf)\n\navg_score = np.mean(scores)\nprint(f'kfold score: {round(avg_score, 2)}')\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(target_test, y_pred, labels=np.unique(y_pred)))\n\nfrom tabulate import tabulate\nprint(tabulate({'Variables': list(df_Aus.columns), 'Coeff': au_model.feature_importances_}, \n               headers=\"keys\"))","f3476ea3":"rfdata = {'Parameters': list(df_Aus.columns), 'Coefficient': au_model.feature_importances_}\nrfdatadf = pd.DataFrame(rfdata)\nrfdatadf = rfdatadf.sort_values('Coefficient')\nrfdatadf.plot.barh(x = 'Parameters', y = 'Coefficient', \n                   title = 'Random Forest Classifier Features Importances')","c401d762":"# Dropping the 'Season' parameter as it is no longer important. I.e all data entries in df_Summer will already definitely be in the Summer Season.\ndf_Summer = df_Summer.drop(columns = ['Season'])\ndf_Autumn = df_Autumn.drop(columns = ['Season'])\ndf_Winter = df_Winter.drop(columns = ['Season'])","a39a9a47":"def plot_cm(observed, predicted):\n    categories = np.array([-3,-2,-1,0,1,2,3])\n    cm = confusion_matrix(observed, predicted, labels = categories)\n    cm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    np.set_printoptions(precision=2)\n    plt.imshow(cm_normalized, interpolation='nearest', cmap='Reds')\n    plt.title('Normalized Classification Error Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(categories))\n    plt.xticks(tick_marks,categories)\n    plt.yticks(tick_marks,categories)\n    plt.tight_layout()\n    plt.ylabel('Observed values')\n    plt.xlabel('Predicted values')\n    plt.tight_layout()\n    print(f\"Weighted f1 score: {round(f1_score(observed, predicted, average='weighted'), 3)} (3dp)\")","2937ac0f":"df_Summer_train = df_Summer[['Radiant temperature (C)', 'Air temperature (C)', \n                             'Relative humidity (%)', 'Clo', 'Air velocity (m\/s)', \n                             'Met', 'Age', 'Sex']].reset_index(drop=True)\ndf_Summer_train = pd.get_dummies(df_Summer_train)\ndf_Summer_pred = au_model.predict(df_Summer_train)\n\ndf_Summer_test = df_Summer['Thermal sensation']\n\nplot_cm(df_Summer_test, df_Summer_pred)\n\ncm = confusion_matrix(df_Summer_test, df_Summer_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\nprint(cm_normalized)","fe7e86cf":"df_Autumn_train = df_Autumn[['Radiant temperature (C)', 'Air temperature (C)',\n                             'Relative humidity (%)', 'Clo', 'Air velocity (m\/s)',\n                             'Met', 'Age', 'Sex']].reset_index(drop=True)\ndf_Autumn_train = pd.get_dummies(df_Autumn_train)\ndf_Autumn_train['Sex_Male'] = 0\ndf_Autumn_pred = au_model.predict(df_Autumn_train)\n\ndf_Autumn_test = df_Autumn['Thermal sensation']\n\nplot_cm(df_Autumn_test, df_Autumn_pred)\n\ncm = confusion_matrix(df_Autumn_test, df_Autumn_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\nprint(cm_normalized)","e3ad0e32":"df_Winter_train = df_Winter[['Radiant temperature (C)', 'Air temperature (C)',\n                             'Relative humidity (%)', 'Clo', 'Air velocity (m\/s)',\n                             'Met', 'Age', 'Sex']].reset_index(drop=True)\ndf_Winter_train = pd.get_dummies(df_Winter_train)\ndf_Winter_pred = au_model.predict(df_Winter_train)\n\ndf_Winter_test = df_Winter['Thermal sensation']\n\nplot_cm(df_Winter_test, df_Winter_pred)\n\ncm = confusion_matrix(df_Winter_test, df_Winter_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\nprint(cm_normalized)","59e91f3e":"au_model_rfdefault = RandomForestClassifier(n_estimators = 40, min_samples_split = 4, \n                                            min_samples_leaf = 1, random_state = 2)\nau_model_rfdefault.fit(features_train, target_train)\n\ndf_Summer_pred = au_model_rfdefault.predict(df_Summer_train)\ndf_Winter_pred = au_model_rfdefault.predict(df_Winter_train)\ndf_Autumn_pred = au_model_rfdefault.predict(df_Autumn_train)","8ad11842":"plot_cm(df_Summer_test, df_Summer_pred)\ncm = confusion_matrix(df_Summer_test, df_Summer_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint(cm_normalized)","70ff823b":"plot_cm(df_Autumn_test, df_Autumn_pred)\ncm = confusion_matrix(df_Autumn_test, df_Autumn_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint(cm_normalized)","59bcfb8d":"plot_cm(df_Winter_test, df_Winter_pred)\ncm = confusion_matrix(df_Winter_test, df_Winter_pred)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint(cm_normalized)","a5671a4a":"# Calculates how many observed values are less than predicted and how many are more than predicted\ndef countdiff(observed, predicted):\n    under_count = 0\n    over_count = 0\n    observed = observed.tolist()\n    predicted = predicted.tolist()\n    for i in range(len(observed)):\n        if observed[i] < predicted[i]:\n            under_count += 1\n        elif observed[i] > predicted[i]:\n            over_count += 1\n    under_perc = round(under_count*100\/len(observed),3)\n    over_perc = round(over_count*100\/len(observed), 3)\n    return (under_perc, over_perc)\n\nsummercountdiff = countdiff(df_Summer_test, df_Summer_pred)\nautumncountdiff = countdiff(df_Autumn_test, df_Autumn_pred)\nwintercountdiff = countdiff(df_Winter_test, df_Winter_pred)\n\n\nprint(f'Summer: {summercountdiff[0]}% observed values less than predicted, {summercountdiff[1]}% observed values more than predicted.')\nprint(f'Autumn: {autumncountdiff[0]}% observed values less than predicted, {autumncountdiff[1]}% observed values more than predicted.')\nprint(f'Winter: {wintercountdiff[0]}% observed values less than predicted, {wintercountdiff[1]}% observed values more than predicted.')","4c3206c8":"N = 3\nind = np.arange(N)  \nwidth = 0.27\n\nfig, ax = plt.subplots(figsize=(8, 6))\nplt.title('Graph of Differences in Observed and Predicted Values', size = 15)\nyvals = [summercountdiff[0], autumncountdiff[0], wintercountdiff[0]]\nrects1 = ax.bar(ind, yvals, width, color='r')\nzvals = [summercountdiff[1], autumncountdiff[1], wintercountdiff[1]]\nrects2 = ax.bar(ind+width, zvals, width, color='g')\n\nax.set_ylabel('Percentages')\nax.set_xticks(ind+(0.5*width))\nax.set_xticklabels( ('Spring', 'Autumn', 'Winter') )\nax.legend( (rects1[0], rects2[0]), ('Observed < Predicted', 'Observed > Predicted') )\n\ndef autolabel(rects):\n    for rect in rects:\n        h = rect.get_height()\n        ax.text(rect.get_x()+rect.get_width()\/2, 1.0*h, f'{round(h,2)}%',\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","b90db60a":"from sklearn.model_selection import train_test_split\nfeatures_train1, features_test1, target_train1, target_test1 = train_test_split(df_Aus, response['Preferred thermal sensation'], test_size=0.3, random_state=2)","5c3cc7ff":"au_model_rfdefault.fit(features_train1, target_train1)\n\ndf_Summer_pred1 = au_model_rfdefault.predict(df_Summer_train)\ndf_Winter_pred1 = au_model_rfdefault.predict(df_Winter_train)\ndf_Autumn_pred1 = au_model_rfdefault.predict(df_Autumn_train)\ndf_Summer_test1 = df_Summer['Preferred thermal sensation']\ndf_Winter_test1 = df_Winter['Preferred thermal sensation']\ndf_Autumn_test1 = df_Autumn['Preferred thermal sensation']","8d688e97":"plot_cm(df_Summer_test1, df_Summer_pred1)\ncm = confusion_matrix(df_Summer_test1, df_Summer_pred1)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint(cm_normalized)","a4c908f2":"plot_cm(df_Autumn_test1, df_Autumn_pred1)\ncm = confusion_matrix(df_Autumn_test1, df_Autumn_pred1)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint(cm_normalized)","0ef3650e":"plot_cm(df_Winter_test1, df_Winter_pred1)\ncm = confusion_matrix(df_Winter_test1, df_Winter_pred1)\ncm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nprint(cm_normalized)","e50ad041":"summercountdiff1 = countdiff(df_Summer_test1, df_Summer_pred1)\nautumncountdiff1 = countdiff(df_Autumn_test1, df_Autumn_pred1)\nwintercountdiff1 = countdiff(df_Winter_test1, df_Winter_pred1)\n\n\nprint(f'Summer: {summercountdiff1[0]}% observed values less than predicted, {summercountdiff1[1]}% observed values more than predicted.')\nprint(f'Autumn: {autumncountdiff1[0]}% observed values less than predicted, {autumncountdiff1[1]}% observed values more than predicted.')\nprint(f'Winter: {wintercountdiff1[0]}% observed values less than predicted, {wintercountdiff1[1]}% observed values more than predicted.')","480a64d1":"N = 3\nind = np.arange(N)  \nwidth = 0.27\n\nfig, ax = plt.subplots(figsize=(8, 6))\nplt.title('Graph of Differences in Observed and Predicted Values', size = 15)\nyvals = [summercountdiff1[0], autumncountdiff1[0], wintercountdiff1[0]]\nrects1 = ax.bar(ind, yvals, width, color='r')\nzvals = [summercountdiff1[1], autumncountdiff1[1], wintercountdiff1[1]]\nrects2 = ax.bar(ind+width, zvals, width, color='g')\n\nax.set_ylabel('Percentages')\nax.set_xticks(ind+(0.5*width))\nax.set_xticklabels( ('Spring', 'Autumn', 'Winter') )\nax.legend( (rects1[0], rects2[0]), ('Observed < Predicted', 'Observed > Predicted') )\n\ndef autolabel(rects):\n    for rect in rects:\n        h = rect.get_height()\n        ax.text(rect.get_x()+rect.get_width()\/2, 1.0*h, f'{round(h,2)}%',\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","23edc15b":"We can then look at the last objective of this notebook, which is to determine if the seasons affect preferred thermal sensation of occupants. We can use the same model in the previous section for this section as well, but train it to predict preferred thermal sensation instead of perceived thermal sensation.","c6d3871c":"Training the model, using the best hyperparameters found during hyperparameter tuning earlier on.","d91a7a2e":"### For Winter:","f49cc447":"![Australia.png](attachment:Australia.png)\nFig. 1. Graph of feature importances of the Australia thermal sensation model. Taken from BPS5229 Group Project.","de0b7827":"While the confusion matrices can give us a rough idea of how the errors in classification look like, we also need to calculate the percentage of observed values that are less than predicted, as well as the percentage of observed values that are more than predicted.","fdc09929":"## 5.1. Training of the Same Random Forest Model for Preferred Thermal Sensation","b3a42e38":"Similar to confusion matrices for the perceived thermal sensation, it is hard to tell spot any glaring differences for preferred thermal sensation from just looking at the confusion matrices. However, when looking at the percentages of over and under predictions, we can see a difference between the seasons. For Summer, there is a greater number of observed values that are **less** than predicted values (Observed < Predicted) as compared to observed values that are **more** than predicted values (Observed > Predicted). This shows that for Summer, the preferred thermal sensation appears to be slightly lower than expected.\n\nThe opposite is true for Autumn and Winter, where there is a greater number of observed values that are **more** than predicted values (Observed > Predicted) as compared to observed values that are **less** than predicted values (Observed < Predicted). This shows that for Autumn and Winter, the preferred thermal sensation appears to be slighter higher than expected.\n\nThis result is in line with the original hypothesis, that the colder seasons such as Autumn and Winter affect the preferred thermal sensation in a different way than the hotter seasons such as Summer. In this case, it appears that in the colder seasons of Autumn and Winter, Australians tend to seek for hotter thermal sensations, and in warmer season of Summer, Australians tend to seek for colder thermal sensations. \n\nHowever, it is very important to note that the percentages are well within the model margins of error, as the model accuracy is not that good for Australia.","91294fa1":"# 5. Objective (3) - Effect of Seasons on Preferred Thermal Sensation","2c7027d3":"### References:\n\nChaudhuri, Tanaya & Soh, Yeng & Bose, Sumanta & Xie, Lihua & Li, Hua. (2016). On assuming Mean Radiant Temperature equal to Air Temperature during PMV-based Thermal Comfort Study in Air-conditioned Buildings. 10.1109\/IECON.2016.7793073.  \nShahzad, Sally & Rijal, Hom. (2019). Preferred vs neutral temperatures and their implications on thermal comfort and energy use: Workplaces in Japan, Norway and the UK. Energy Procedia. 158. 3113-3118. 10.1016\/j.egypro.2019.01.1007. ","95f62ac5":"# 3. Objective (1) - Difference Between Neutral Thermal Sensation and Preferred Thermal Sensation","b5d2a0a7":"## 5.2. Comparing how Seasons Differ from Predicted Preferred Thermal Sensation Values","02fbb865":"### For Autumn:","f5b5d862":"## 1.2. Effect of Seasons on Perceived and Preferred Thermal Sensations\n\nNow that we have established the difference between perceived and preferred thermal sensation, we can then examine how seasons affect both perceived and preferred thermal sensations **indoors**. During the different seasons, the thermal sensation outdoors will vary greatly, as each season brings about very differing environmental parameters such as air temperature, wind velocity etc etc. Thus, when going indoors into a thermal environment that is controlled by the building's air conditioning unit, will seasons still affect perceived and preferred thermal sensation, **given that the building's thermal environment has stayed the same across all seasons**.\n\nFor example, in Summer, the weather outside could be so hot that upon entering an air-conditioned building, the occupant might want an especially cooling sensation (imagine the pleasures of eating an icecream on a sweltering hot day), and thus his preferred thermal sensation might be lower than in Winter, when the occupant is already so cold outdoors than he\/she would want something cozy and warm indoors instead, and thus his preferred thermal sensation then might be higher.\nDifferent seasons could also affect the occupant's perception of what is perceived neutral thermal sensation, even if the thermal environment of the building stays exactly the same across all seasons. For example, the occupant could have gotten used to the heat of Summer and has subconsiously adjusted his perceived neutral thermal sensation accordingly.","1f0976bf":"From our results, we can observe that:  \n1. Neutral thermal sensation and Preferred thermal sensation are different\n2. Perceived thermal sensation does not follow our hypothesis  \n**a.** Although there are differences between seasons, both Summer and Winter exhibit similar trends, which is not in agreement with the hypothesis  \n**b.** The differences in perceived thermal sensation are quite minimal and could potentially just be errors cause by inaccuracy in the model\n\n3. Preferred thermal sensation follows hypothesis, but percentages well within margins of error  \n**a.** The colder seasons (Winter and Autumn) exhibit similar differences as compared to predicted values  \n**b.** The warmer season (Summer) exhibits differences to the predicted values that are opposite of the differences in **a**.  \n**c.** The differences in preferred thermal sensation are quite minimal and could potentially just be errors cause by inaccuracy in the model\n\nWhile the hypothesis predicted warmer and colder seasons to have significant but opposite effects on Perceived thermal sensation and Preferred thermal sensation, the results for Perceived thermal sensation disagrees with this hypothesis as Summer and Winter exhibit similar effects on Perceived thermal sensation. \n\nFor Preferred thermal sensation, results appear to be in agreement with the hypothesis, but the results are still within the margin of error for the model. The results could be because during colder seasons, occupants might want to feel warmer and cozier indoors to get a respite from the cold, while during hotter seasons, occupants might want to feel indoor conditions that are more cooling than usual ,to get a respite from the heat.\n\nHowever, at this stage, these are just promising but inconclusive results. Through this study, we are not able to confidently say that seasons affect either the Perceived thermal sensation or the Preferred thermal sensation. Instead, more work needs to be done in the future, such as to create a better model that is better able to predict Perceived thermal sensation and Preferred thermal sensation. A more accurate way of calculating Preferred thermal sensation also has to be devised and verified for its accuracy. With a more accurate model, we can then be more confident that any differences in results between seasons is because of the seasons and not because of errors in the model. \n\nHowever, Australia dataset appears to be a challenge for models to predict accurately. From the Group Project to this Individual Project, the Australia dataset has continued to throw unexpected results, and have poor accuracy for any models tried, as compared to other countries such as India and Brazil where the process is much smoother. One guess would be that the Australia data is very unbalanced with too many data points with a thermal sensation of 0. This is just an educated guess, but there is need to figure out what the issues really are to rectify them.","a4ab0202":"# 4. Objective (2) - Effect of Seasons on Perceived Thermal Sensation","3e7e41ab":"### For Autumn:","d95a421c":"Now that we have a (hopefully) more accurate model for perceived 'Thermal sensation' values in Australia, we can then proceed to compare how the perceived thermal sensations for the different seasons differ from the predicted values. Remember that the model was not allowed to know which data entry came from which season, and as such, it cannot adapt to take into account the differences caused by the different seasons. Thus, if there are indeed differences caused by the seasons, then it should be seen as the difference between the observed and predicted values of thermal sensation for that season.","4e96918a":"## 4.2. Comparing how Seasons Differ from Predicted Perceived Thermal Sensation Values","df1db96b":"Now, to test the observed value of perceived thermal sensations in each season against the predicted value of perceived thermal sensation from the model. As mentioned earlier, the differences between observed and predicted should be the effect of the season on perceived thermal sensation.","071e1be2":"**This notebook expands on the group project done by Comfort Group 1, more specifically the thermal comfort model created for Australia.**","98c9adb3":"After some basic data cleaning as detailed above, we can then look at the seasons that are represented in the data. As we can see from the value counts, only 3 seasons are represented. Although Spring is not represented in this dataset, we can still continue to study the effects of the other 3 seasons.","32387e92":"# 6. Conclusion","398bb852":"The first thing that needs to be done is to check if all data entries have both Thermal sensation and Thermal preference data. The Thermal comfort field is also important, as it shows the magnitude of the thermal preference. For example, a thermal preference of 'cooler' and a thermal comfort of 1 would suggest that the person prefers a much cooler thermal sensation than what he\/she currently has. This will be further explained in a later part of the notebook.\n\nAnother important thing to note is that not all data entries have recorded Radiant Temperature values. While the Mean Radiant Temperature (MRT) is often assumed to be equal to Air Temperature during thermal comfort studies in air-conditioned buildings, studies have found that this assumption could be erronous and lead to significant error in determination of thermal comfort (Chaudhuri et al., 2016).\n\nAs such, we will first be filtering out the data entries that do not contain Thermal sensation, Thermal preference, Thermal comfort and Radiant temperature. Of course, as we are examining the impact of seasons, the Season should also be present.","44157b4d":"### For Autumn:","a10ea697":"Similar to the previous section, we can now test the observed value of preferred thermal sensations in each season against the predicted value of preferred thermal sensation from the model. The differences between observed and predicted should be the effect of the season on preferred thermal sensation.","0b48d0b5":"### For Winter:","a35127f2":"### For Summer:","7da1e753":"### For Summer:","0346d626":"## 1.1. Neutral Thermal Sensation vs Preferred Thermal Sensation#\n\nIt is commonly assumed that a perceived neutral thermal sensation of 0 is also a comfortable thermal sensation, but recent studies have refuted that claim, showing how the preferred thermal sensation is not always a neutral thermal sensation (e.g. Shahzad & Rijal, 2019). This is as occupants might still prefer a change in thermal sensation even though he\/she is already currently experiencing a neutral thermal sensation. For example, an occupant might recognise room temperature as a neutral thermal sensation, but prefer to be in an air-conditioned room which he\/she recognises to be colder than neutral thermal sensation. \n\nAs such, the perceived thermal sensation is not neccessarily the same as the occupant's preferred thermal sensation. When designing a model for indoor thermal comfort, it is important to consider both preferred thermal sensation and perceived thermal sensation. In our Group Project, we only looked at the 'Thermal sensation' as the response variable, but this is only the perceived thermal comfort and not the preferred thermal comfort.","6e3376cc":"The weighted f1 scores for all 3 seasons are all above 0.8, and the confusion matrices shows that the model is no longer predicting everything as 0, which is a welcome sight!\n\nIt is hard to tell spot any glaring differences from just looking at the confusion matrices. However, when counting the percentages of over and under predictions, we can see a difference between the seasons. For Summer and Winter, there is a greater number of observed values that are **less** than predicted values (Observed < Predicted) as compared to observed values that are **more** than predicted values (Observed > Predicted). This shows that for both Summer and Winter, the perceived thermal sensation appears to be slightly lower than expected. The opposite is true for Autumn, where there is a greater number of observed values that are **more** than predicted values (Observed > Predicted) as compared to observed values that are **less** than predicted values (Observed < Predicted). This shows that for Autumn, the perceived thermal sensation appears to be slighter higher than expected.\nThis result goes against the initial hypothesis, as Summer and Winter are completely opposite seasons, with one being the hottest season and one being the coldest season. Yet, both seasons exhibit the same behavior in terms of perceived thermal sensation.\n\nIt is also important to note that the percentages are well within the model margins of error, as the model accuracy is not that good for Australia.","b0238c00":"Before we start with the analysis, we need to first import the data from the provided Ashrae dataset. ","be34c42c":"### 1.3.1. Objectives\n1) Is there a difference between neutral thermal sensation and preferred thermal sensation?  \n2) Do seasons affect the **perceived** thermal sensation?  \n3) Do seasons affect the **preferred** thermal sensation?","694aeff2":"## 4.3. Rerunning With Default 'min_samples_leaf' Value","43cdf2e5":"Now that the 'Thermal preference' is mapped accordinly, we can now proceed with the calculation of the 'Preferred thermal sensation'. As mentioned earlier, the 'Preferred thermal sensation' is bound by -3 and 3, just like the perceived 'Thermal sensation'.","1554a324":"I am using RandomizedSearchCV instead of GridSearchCV this time for model hyperparameter tuning, as RandomizedSearchCV is much faster. However, in the interest of accuracy, I have set it to run 20 iterations instead of the default 10. As such, some of the models will still take quite a while to tune (My run took 10mins).   \n\\* **Do note that if you do not have time to run this cell, I have recorded the results of my run below. Due to the nature of RandomizedSearchCV, the results of my run will most likely be different from yours.**\n\nThrough this model hyperparameter tuning, we are able to determine which model is the most accurate for predicting our data, using the k-fold score. While the f1score only lets us know the accuracy of 1 pre-defined train set in predicting 1 pre-defined test set, k-fold instead estimates how the model is expected to perform in general when used to make predictions on data not used during the training of the model. As such, k-fold is arguably the more important score when making prediction models as we are doing, and we will be using it to determine which model is the best for each of the countries.\n\n**Future work:**\n* Use TensorFlow instead of Scikit-learn so that TPU acceleration can be used.  \n* Although XGBoost is probably the best ML algorithm to use here, I have chosen **not** to use it as it requires a long time to tune the model properly, which would take far too long on Kaggle. XGBoost has been one of the most popular ML algorithms used on Kaggle due to its efficiency and performance. Without going into too much details, XGBoost is a decision-tree based ensemble ML algorithm, and it provides increased performance over the RandomForest algorithm as the XGBoost algorithm also uses an optimised Gradient Boosting algorithm to avoid overfitting and minimise the errors in sequential models.","c10ff395":"### For Winter:","343ee11a":"We then look to complete the first objective of this notebook, which is to determine if there is a difference between neutral thermal sensation and preferred thermal sensation. To help us with this, the dataset has 3 parameters which we can use, namely 'Thermal sensation', which is the perceived thermal sensation, 'Thermal preference', and 'Thermal comfort'.\n\nHowever, as 'Thermal preference' has been recorded as just either 'cooler', 'warmer', or 'no change', it is not possible to know the magnitude of the thermal preference. Does the person with a cooler 'Thermal preference' prefer something slightly cooler, or something much cooler than his current thermal sensation? This is where 'Thermal comfort' comes into the picture, as it has been recorded on a scale from 1 to 6, with 1 being the least thermal comfortable and 6 being the most thermal comfortable. It is thus assumed that the further a person's 'Thermal comfort' differs from the ideal value of 6, the further the occupant is from his\/her ideal thermal sensation, and thus would have a greater magnitude of 'Thermal preference'. For example, a person with a 'Thermal preference' of 'cooler' and a 'Thermal comfort' of 1 would want a thermal sensation much cooler than what he\/she is currently experiencing, while a person with a 'Thermal preference' of 'cooler' and a 'Thermal comfort' of 5 would want a thermal sensation that is only slightly cooler than what he\/she is currently experiencing.\n\nAs such, it is proposed that the 'Preferred thermal sensation' be based on 3 parameters, 1) the current perceived thermal sensation, 2) the direction of thermal preference (e.g. cooler, warmer or none), and 3) the magnitude of thermal preference (based on extent of deviation from ideal thermal comfort of 6).  \nWe can then estimate a 'Preferred thermal sensation' as such:  \n\n'Thermal sensation' + 'Direction' * (6 - 'Thermal comfort')  \n\nWhere 'Direction' is 1 if thermal preference is 'warmer', -1 if thermal preference is 'cooler', and 0 if thermal preference is 'no change'.\nIf the calculated 'Preferred thermal sensation' is smaller than -3 or greater than 3, it will be set as -3 or 3 respectively.\n\n**Example 1:**  \n'Thermal sensation': 0, 'Thermal preference': cooler, 'Thermal comfort': 1  \n'Preferred thermal sensation': 0 + (-1)*(6 - 1) = -5 = -3\n\n**Example 2:**  \n'Thermal sensation': -2, 'Thermal preference': warmer, 'Thermal comfort': 3  \n'Preferred thermal sensation': -2 + (1)*(6 - 3) = 1\n\n\\* It is important to note that this method of calculating 'Preferred thermal sensation' has not been verified, and might not be the most accurate way of calculation. Future work needs to be done to find the most accurate way of calculating 'Preferred thermal sensation' from perceived 'Thermal sensation', 'Thermal preference' and 'Thermal comfort'.","b2b09c45":"## 1.3. Hypothesis\nGiven that the thermal environment of a building stays constant throughout all seasons:  \n1) The different seasons will affect both perceived and preferred thermal sensation  \n2) Colder seasons such as Autumn and Winter will have opposite effects on perceived and preferred thermal sensation as compared to warmer seasons such as Spring and Summer ","da72c5c7":"Next, we will need to round all the thermal sensation and thermal comfort values to integers for the sake of uniformity, as some recorded values are not in integers.","ec6930b7":"Here, we can see that Australia has the highest number of data entries with the required information as listed above. We will thus be proceeding with Australia as we have already created an intial thermal sensation model for it, so we know which parameters from the Ashrae dataset are important for predicting thermal sensation (just to reiterate, this is an extension of our group project).","1cf1fdd8":"## 3.2. Comparison of Neutral Thermal Sensation and Preferred Thermal Sensation","fbda6cb8":"The second objective of this notebook is to determine if the seasons play a part in affecting perceived 'Thermal sensation'. For example, during spring, an occupant could be satisfied with his thermal sensation in his office, but during winter, even though thermal conditions in the office have not changed, the person might perceive his office to be colder\/hotter than usual.\n\nIn order to study this, we first need to split up the dataset into seasons.","80943918":"# 1. Introduction","8461540a":"Now that we have our calculated preferred thermal sensation values, we can then compare it against the neutral thermal sensation value of 0, to determine if there are any differences. If the preferred thermal sensation is the same as neutral thermal sensation, then a large number of preferred thermal sensation values should be 0.\n\nWe first start with a confusion matrix to visualise how different the two values are. Although the confusion matrix is more conventionally used to visualise errors in classification predictions, it can also be repurposed to see the differences between 2 series of data. This is also true for model scores such as f1_score, which can also be used for this purpose. The weighted f1 score is used as it is better suited for datasets with label inbalance (alot of thermal sensation of 0 values in this dataset), as compared to micro and macro f1 scores.","b4c9963e":"### Recorded best parameter, f1 score, k-fold score for all tuned models:\n#### Please note that these results will probably be different from yours due to the nature of RandomizedSearchCV\n\nmodel_rf best parameters: {'n_estimators': 40, 'min_samples_split': 4, 'min_samples_leaf': 15}  \nmodel_rf best f1 score: 0.33491258199813545  \nmodel_rf best k-fold score: 0.4807843137254902\n\nmodel_kn best parameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 500, 'leaf_size': 70}  \nmodel_kn best f1 score: 0.29449477002010743  \nmodel_kn best k-fold score: 0.4768627450980392\n\nmodel_dt best parameters: {'min_samples_split': 45, 'min_samples_leaf': 95}  \nmodel_dt best f1 score: 0.29449477002010743  \nmodel_dt best k-fold score: 0.4768627450980392\n\nmodel_mlp best parameters: {'solver': 'sgd', 'max_iter': 2000, 'activation': 'logistic'}  \nmodel_mlp best f1 score: 0.29449477002010743  \nmodel_mlp best k-fold score: 0.4768627450980392\n\nmodel_abc best parameters: {'n_estimators': 1, 'algorithm': 'SAMME.R'}  \nmodel_abc best f1 score: 0.29449477002010743  \nmodel_abc best k-fold score: 0.476078431372549\n\nmodel_svc best parameters: {'kernel': 'linear', 'C': 5.8}  \nmodel_svc best f1 score: 0.29449477002010743  \nmodel_svc best k-fold score: 0.4768627450980392\n\nmodel_dc best parameters: {'random_state': 2}  \nmodel_dc best f1 score: 0.3150189299802674  \nmodel_dc best k-fold score: 0.3035294117647059\n\nTime taken: 0hrs, 9mins, 49.922secs","6cf15480":"### For Summer:","91361ed4":"While the confusion matrices can give us a rough idea of how the errors in classification look like, we also need to calculate the percentage of observed values that are less than predicted, as well as the percentage of observed values that are more than predicted.","0ab435ff":"As we can see from the scores above, the RandomForestClassifier (model_rf) has the highest f1 and k-fold scores. This shows that the RandomForestClassifier model should be the most accurate (out of all the models tested) when predicting thermal sensation. As such, we will be proceeding with the RandomForestClassifier for the model.\n\nAlthough the k-fold score is really not great at 0.48, it is still much higher than the baseline Dummy Classifier, and it is already a substantial improvement over the Australia model created during the Group Project, which had a k-fold score of 0.38, while the Brazil and India models created during the Group Project had much higher k-fold scores of 0.65 and 0.56 respectively. \n\nThis suggests that the relatively lower accuracy could be due to the Australia data, but this is not in the scope of this notebook.","cee9991f":"We start by comparing the perceived 'Thermal sensation' between the seasons against the whole dataset. As out group project did not use Radiant Temperature in our model (there weren't enough data entries for other countries containing that parameter), it is neccessary to create a new model taking Radiant Temperature into account. It is not only one of the parameters of thermal comfort as outline by Ashrae 55 guidelines, but studies such as Chaudhuri et al. (2016) have shown the importance of this parameter. Chaudhuri et al. (2016) found that while Radiant Temperature is very often assumed by other studies to be the same as Air Temperature, this incorrect assumption actually causes significant inaccuracy in the modelling of predicted thermal comfort.\n\nThe model will be trained with the combined data containing all 4 seasons. The perceived 'Thermal sensation' of each season will be compared against the model, to check if it is different from the predicted values. The 'Country' and 'Cooling startegy_building level' parameters are removed for this model as it contains all the same values as 'Australia' and 'Air Conditioned' respectively. The 'Season' parameter is removed so that the model is unable to know which data entry is for which season. This allows us to clearly see the differences between seasons later on.","8e0ba229":"Splitting the dataset into 70% train and 30% test:","a6de59ac":"The confusion matrices for all 3 seasons look very weird. Almost all the predicted values are 0s, which could be because of the large number of thermal sensation = 0 in the Australia dataset. A quick check of the classification report when training the model shows that the model is indeed predicting almost everything as having a thermal sensation of 0, which is definitely not acceptable. Thus, this could be a limitation of the RandomForestClassifier model. Even though the k-fold scores of the RandomForestClassifier is higher than the other models, it does not always mean that it is the most appropriate model to use.\n\nOne thing that I noticed is that the min_samples_leaf hyperparameter is abnormally high at 15. Usually a high min_samples_leaf value results in reduced variance and higher levels of bias, which is what we are observing in the current RandomForestClassifier model. As such, I tried to use the default value of 1 for min_samples_leaf instead. Hopefully the model with a lower min_samples_leaf value will exhibit greater variance in the prediction, instead of just predicting almost everything to have a thermal sensation of 0.","d1597bf7":"In the case of Australia (Fig. 1), past results have shown that Air temperature, Relative humidity, Clo, Air velocity, Met, Age, Cooling strategy, and Sex have non-negligible importance in the prediction of thermal sensation values in Australia. As such, we will also be focusing on these parameters as the explanatory variables (together with Season, Radiant temperature as mentioned earlier). How the addition of Radiant Temperature affects the model will have to be determined in a new model.\n\nThe filtering of cooling strategy to only include 'Air Conditioned' is extremely important here. This is as we are working under the assumption that the indoor thermal environment is not affected in anyway by the outdoor thermal environment that is being drastically affected by the seasons. As such, an 'Air Conditioned' cooling strategy ensures that the indoors thermal environment is kept independent and isolated from the outdoor thermal environment. Other cooling strategies such as 'Naturally Ventilated' or 'Mixed Mode' could be affected by the outdoors thermal environment.","789339a3":"# 2. Preparation of Dataset","017c9e3c":"From the confusion matrix and weighted f1 score, we see that the neutral thermal sensation of 0 and the 'Preferred thermal sensation' actually differ quite significantly. Only around 45% of the 'Preferred thermal sensation' is actually 0, and the other half differs from the neutral thermal sensation.\n\nAlthough a weighted f1 score of 0.624 is not bad, it still shows that there are differences between the two. Thus we can conclude that there is indeed a difference between the neutral thermal sensation of 0 and the 'Preferred thermal sensation', although the difference is not that big.","65ab7ceb":"## 4.1. Improving on the Australia Model used for the Group Project","5ac235d0":"To prepare for the calculation of the 'Preferred thermal sensation', we first need to map the 'Thermal preference' such that 'cooler' is -1, 'warmer' is 1, and 'no change' is 0.","549961ba":"## 3.1. Calculation of Preferred Thermal Sensation"}}