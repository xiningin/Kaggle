{"cell_type":{"8250cc9b":"code","bde57375":"code","6b5caf07":"code","eebc5d5a":"code","0ccaf26d":"code","452e95f0":"code","60e9bb28":"code","0585b7d0":"code","68316250":"code","2b0b0a61":"code","365b3ff2":"code","23e56f83":"code","8fa9c88b":"code","ebb58279":"code","51887e20":"code","98b41f9c":"code","0d3741af":"code","718b1124":"code","82705265":"code","b2d2f595":"code","e39c8cb2":"code","6bb3b9b0":"code","8232c666":"code","55901ee2":"code","314b0dd1":"code","b63b440a":"code","a9759871":"code","64e32f80":"code","eee5dff8":"code","62953434":"code","2f23b0c9":"code","1054dfed":"code","a7e48d9d":"code","02e14c80":"code","153c254f":"code","6c33a20c":"code","e3c1635b":"code","fb8f9c91":"markdown","4c925be3":"markdown","2b3fa353":"markdown","177a288a":"markdown","aa0459f8":"markdown","fbd34eaa":"markdown","5a3fcf65":"markdown","45642fac":"markdown","0cb02ee6":"markdown","d56087eb":"markdown","8ad5b00c":"markdown","7a1fea0e":"markdown","9bf435f2":"markdown","71928b19":"markdown","682f00a7":"markdown","eb1646f1":"markdown","76af1af2":"markdown"},"source":{"8250cc9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bde57375":"data = pd.read_csv('..\/input\/turkish cyberbullying.csv')","6b5caf07":"data.head()","eebc5d5a":"data.tail(10)","0ccaf26d":"data.info()","452e95f0":"data.describe()","60e9bb28":"y = data.cyberbullying.values\nx = data.message.values","0585b7d0":"print(x)\nprint(len(x))","68316250":"x.shape\ny.shape","2b0b0a61":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42)","365b3ff2":"print(len(x_train))\nprint(len(x_test))","23e56f83":"veri = x.copy()","8fa9c88b":"veri.shape","ebb58279":"from nltk.corpus import stopwords\nstop = stopwords.words(\"turkish\")","51887e20":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(min_df=5,stop_words=stop,ngram_range=(1,3))\nvectorizer.fit(veri)","98b41f9c":"BoW = vectorizer.transform(veri)\nrepr(BoW)","0d3741af":"feature_names = vectorizer.get_feature_names()\nprint(\"100 ile 110 aras\u0131ndaki de\u011ferler:\\n{}\".format(feature_names[100:110]))","718b1124":"from sklearn.feature_extraction.text import TfidfVectorizer\nfor min_df in [1,2,3,4,5,6]:\n    for n_gram in [(1,1),(1,2),(1,3),(2,3)]:\n        tf_vectorizer = TfidfVectorizer(min_df=min_df, stop_words=stop,ngram_range=n_gram)\n        veri1 = tf_vectorizer.fit_transform(veri)\n        best = veri1.max(axis=0).toarray().ravel()\n        sort_by_tfidf = best.argsort()\n        feature_names = np.array(tf_vectorizer.get_feature_names())\n        print(\"Vocabularies using min_df={} and n_gram={} with highest tfidf: \\n{}\".format(min_df, n_gram, feature_names[sort_by_tfidf[-20:]]))\n        print(\"The number of vocabularies: {}\".format(len(tf_vectorizer.vocabulary_)))\n        sort_by_tfidf = np.argsort(tf_vectorizer.idf_)\n        print(\"Vocabularies with lowest idf:\\n{}\".format(feature_names[sort_by_tfidf[:20]]))\n        print('-----------------------------------')","82705265":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","b2d2f595":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score","e39c8cb2":"from sklearn.pipeline import Pipeline","6bb3b9b0":"\nLinearSVC_count = Pipeline([\n    ('countvectorizer',CountVectorizer()),\n    ('LinearSVC',LinearSVC(max_iter=1000))\n])\n\nLinearSVC_tfidf = Pipeline([\n        ('tfidfvectorizer', TfidfVectorizer()),\n        ('LinearSVC', LinearSVC(max_iter=1000))\n])","8232c666":"Naive_count = Pipeline([\n        ('countvectorizer', CountVectorizer()),\n        ('multinomialnb', MultinomialNB())\n])\n\nNaive_tfidf = Pipeline([\n        ('tfidfvectorizer', TfidfVectorizer()),\n        ('multinomialnb', MultinomialNB())\n])","55901ee2":"Decision_count = Pipeline([\n        ('countvectorizer', CountVectorizer()),\n        ('decisiontreeclassifier', DecisionTreeClassifier())\n])\n\nDecision_tfidf = Pipeline([\n        ('tfidfvectorizer', TfidfVectorizer()),\n        ('decisiontreeclassifier', DecisionTreeClassifier())\n])","314b0dd1":"RandomForest_count = Pipeline([\n        ('countvectorizer', CountVectorizer()),\n        ('randomforestclassifier', RandomForestClassifier(n_estimators=100))\n])\n\nRandomForest_tfidf = Pipeline([\n        ('tfidfvectorizer', TfidfVectorizer()),\n        ('randomforestclassifier', RandomForestClassifier(n_estimators=100))\n])","b63b440a":"#\u0130lk olarak b\u00fcnyesinde Count Vectorizer olan parametreler.\nparameters_of_svc_count = [ \n    {\n        'LinearSVC__C': [0.01, 0.1, 1, 10, 100], \n        'countvectorizer__min_df': [1,3,5], \n        'countvectorizer__stop_words': [None, stop],\n        'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n    } \n]\n\nparameters_general_count = [ \n    {\n        'countvectorizer__min_df': [1,3,5], \n        'countvectorizer__stop_words': [None, stop],\n        'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n    }\n]","a9759871":"parameters_of_svc_tfidf = [ \n    {\n        'LinearSVC__C': [0.01, 0.1, 1, 10, 100], \n        'tfidfvectorizer__min_df': [1,3,5], \n        'tfidfvectorizer__stop_words': [stop],\n        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n    } \n]\n\nparameters_of_general_tfidf = [ \n    {\n        'tfidfvectorizer__min_df': [1,3,5], \n        'tfidfvectorizer__stop_words': [stop],\n        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n    }\n]","64e32f80":"x_train.shape","eee5dff8":"for models, parameters, name in zip([LinearSVC_count, Naive_count, Decision_count, RandomForest_count],\n                                    [parameters_of_svc_count, parameters_general_count, parameters_general_count, parameters_general_count],\n                                    [\"LinearSVC\",\"Multinomial NB\",\"Decision Tree\",\"Random Forest\"]):\n\n    grid = GridSearchCV(models, parameters, cv=5)\n    grid.fit(x_train, y_train)\n    print(\"Model ismi: \"+ name)\n    print(\"En iyi cross-validation score: {:.2f}\".format(grid.best_score_ * 100))\n    print(\"En iyi parametreler: \", grid.best_params_)\n    \n    y_train_pred = grid.predict(x_train)\n    print(confusion_matrix(y_train, y_train_pred))\n\n    \n    final_model = grid.best_estimator_\n    final_test_prediction = final_model.score(x_test, y_test)\n    print(\"Test score: {:.2f}%\".format(final_test_prediction * 100))    \n    print(\"--------------------------\")","62953434":"for models, parameters, name in zip([LinearSVC_tfidf, Naive_tfidf, Decision_tfidf, RandomForest_tfidf],\n                                    [parameters_of_svc_tfidf, parameters_of_general_tfidf, parameters_of_general_tfidf, parameters_of_general_tfidf],\n                                    [\"LinearSVC\",\"Multinomial Naive Bayes\",\"Decision Tree\",\"Random Forest\"]):\n\n    grid = GridSearchCV(models, parameters, cv=5)\n    grid.fit(x_train, y_train)\n    print(\"Model ismi: \"+ name)\n    print(\"En iyi cross-validation score: {:.2f}\".format(grid.best_score_ * 100))\n    print(\"En iyi parametreler: \", grid.best_params_)\n    \n    y_train_pred = grid.predict(x_train)\n    print(confusion_matrix(y_train, y_train_pred))\n    \n    final_model = grid.best_estimator_\n    final_test_prediction = final_model.score(x_test, y_test)\n    print(\"Test score: {:.2f}%\".format(final_test_prediction * 100))    \n    print(\"--------------------------\")","2f23b0c9":"# En iyi sonu\u00e7 veren parametrelerimizi de yap\u0131n\u0131n i\u00e7erisine ekliyoruz.\nbest_pipeline = Pipeline([\n    ('tfidfvectorizer', TfidfVectorizer(min_df=1,stop_words=stop,ngram_range=(1,2))),\n    ('LinearSVC', LinearSVC(C=10,max_iter=1000))\n])","1054dfed":"best_pipeline.fit(x_train,y_train)","a7e48d9d":"best_pipeline.steps","02e14c80":"final_test_prediction = best_pipeline.score(x_test,y_test)\nprint(\"Final test score:\",final_test_prediction)\n\ny_test_pred = best_pipeline.predict(x_test)\nprint(\"Confusion matrix\\n\",confusion_matrix(y_test,y_test_pred))","153c254f":"from sklearn.externals import joblib\n\njoblib.dump(best_pipeline,\"latest_model.pkl\")","6c33a20c":"best_model = joblib.load(\"latest_model.pkl\")\nfinal_test_prediction = best_model.score(x_test,y_test)\nprint(\"Final Test Score:\",final_test_prediction)","e3c1635b":"best_model.predict(['gayet iyi','salak','bu ne amk','g\u00fczelsin','siktir','hava \u00e7ok g\u00fczel'])","fb8f9c91":"\u015eimdi modellerimizi \u00e7al\u0131\u015ft\u0131rmaya ba\u015flayal\u0131m. \u0130lk olarak CountVectorizer i\u00e7eren modeller.","4c925be3":"T\u00fcrk\u00e7e'de bulunan '**Stopwords**'(gereksiz kelimeler)'den kurtulmam\u0131z gerekiyor \u00e7\u00fcnk\u00fc bu kelimeler classification yaparken i\u015fimize yaramayacak kelimeler. O nedenle bu kelimelerden kurtulal\u0131m!","2b3fa353":"Modelde kullanaca\u011f\u0131m\u0131z metotlar\u0131, vectorizerlar\u0131 da kullanarak tek tek tan\u0131tmaya ba\u015fl\u0131yorum.","177a288a":"\u015eimdi parametrelerimizi tan\u0131tal\u0131m. Metotlar\u0131n i\u00e7inden yaln\u0131zca LinearSVC hyper parameter olarak 'C' parametresini al\u0131yor. O nedenle LinearSVC parametrelerini ayr\u0131, kalan 3 metotun parametrelerini ayr\u0131 alaca\u011f\u0131m.","aa0459f8":"Dosyay\u0131 load ederek deneyelim.","fbd34eaa":"Modelimizi e\u011fitirken kullanaca\u011f\u0131m\u0131z metotlar\u0131m\u0131z\u0131 import ediyoruz.","5a3fcf65":"\u00c7al\u0131\u015fmamda Pipeline yap\u0131s\u0131 kullanaca\u011f\u0131m. Pipeline'\u0131 bilmeyenler i\u00e7in => https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html","45642fac":"**G\u0130R\u0130\u015e**\n\n* Bu projemizde Turkish cyberbullying datas\u0131n\u0131 kullanarak; T\u00fcrk\u00e7e hakaret i\u00e7eren c\u00fcmleleri tespit edebilen bir model e\u011fitece\u011fiz. Bu model i\u00e7inde CountVectorizer ve TFIDF Vectorizer'lar\u0131 ile tan\u0131\u015f\u0131p, Pipeline yap\u0131s\u0131n\u0131 kullanmay\u0131 \u00f6\u011frenece\u011fiz. En son olarak da modelimizi Joblib ile '.pkl' uzant\u0131l\u0131 bir dosyaya kaydedip, daha sonraki kullan\u0131mlar\u0131m\u0131zda yaln\u0131zca bu dosyay\u0131 \u00e7a\u011f\u0131rarak i\u015flem yapaca\u011f\u0131z.\n\n* Benim ilk NLP ve ilk T\u00fcrk\u00e7e kernel'\u0131m olacak bu \u00e7al\u0131\u015fma. Dolay\u0131s\u0131yla \u00e7ok heyecal\u0131y\u0131m. O zaman ba\u015flayal\u0131m!","0cb02ee6":"G\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi; dosyam\u0131z\u0131n i\u00e7inde bulunan final score'u \u00f6n\u00fcm\u00fcze geldi!\n\n\u015eimdi kendi yazd\u0131\u011f\u0131m\u0131z bir ka\u00e7 \u00f6rnekle classification yapabiliyor muyuz test edelim!","d56087eb":"\u015eimdi de TFIDF vectorizer'\u0131na g\u00f6re sonu\u00e7 alal\u0131m.","8ad5b00c":"Bu modelimizi bir dosya i\u00e7erisine save etmemiz gerekiyor \u015fu an. Dosyan\u0131n ad\u0131n\u0131 en 'latest_model' yapaca\u011f\u0131m.","7a1fea0e":"G\u00f6rd\u00fc\u011f\u00fcn\u00fcz gibi hakaret i\u00e7eren ifadeler 1 nolu (hakaret i\u00e7eren ifadeler) hakaret i\u00e7ermeyen ifadeler 0 nolu (hakaret edilmeyen) class'\u0131m\u0131za classify yap\u0131lm\u0131\u015f oldu. Modelimiz gayet sa\u011fl\u0131kl\u0131 \u00e7al\u0131\u015f\u0131yor.","9bf435f2":"**CountVectorizer**","71928b19":"En iyi test score'u veren modelimiz TFIDF vectorizer'\u0131 ile LinearSVC modeli oldu. En iyi parametreler: C=10, min_df=1, stop_words=word, ngram_range=(1,2). Score: %90.15\n\nEn iyi sonu\u00e7 ald\u0131\u011f\u0131m\u0131z modeli parametreleri ve vectorizer'\u0131yla birlikte yeni bir pipeline yap\u0131s\u0131nda tutal\u0131m. Yeni gelen de\u011ferler direk bu yeni yaratt\u0131\u011f\u0131m\u0131z yap\u0131ya gidecek ve orada kontrol ettirilecek.\n\nEn iyi sonu\u00e7 ald\u0131\u011f\u0131m\u0131z modelimizin pipeline yap\u0131s\u0131:","682f00a7":"Train - Test Split","eb1646f1":"**SONU\u00c7**\n* Modelimizi en iyi \u015fekilde e\u011fittik ve bir c\u00fcmle i\u00e7erisinde hakaret olup olmad\u0131\u011f\u0131n\u0131 bulan bir model geli\u015ftirdik. Son b\u00f6l\u00fcmde yapt\u0131\u011f\u0131m\u0131z testlerde de a\u00e7\u0131k\u00e7a g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi; modelimiz gayet g\u00fczel \u015fekilde \u00e7al\u0131\u015f\u0131yor. Predict ederken kulland\u0131\u011f\u0131m kaba-saba ve k\u00fcf\u00fcrl\u00fc ifadeler i\u00e7in sizden \u00f6z\u00fcr dilerim ancak modelimi test etmem i\u00e7in zorunluydu bu :) Umar\u0131m siz de benim kadar \u00e7ok \u015fey \u00f6\u011frenmi\u015fsinizdir. Yorumlar\u0131n\u0131z benim i\u00e7in \u00e7ok k\u0131ymetli, l\u00fctfen yorum yapmaya \u00e7ekinmeyin!","76af1af2":"**TFIDF Vectorizer**"}}