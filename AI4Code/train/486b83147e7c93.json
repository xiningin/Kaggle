{"cell_type":{"3ece4977":"code","ce6da469":"code","c3eb0d8a":"code","9a48b524":"code","8e479590":"code","d8a5d807":"code","e1e8ce69":"code","d3a4b4b9":"code","f4bcdaa8":"code","1395a0db":"code","acd013a0":"code","45fb4ee8":"code","6ccba0ee":"code","2f636d26":"code","34ffed9a":"code","1029d284":"code","cf80eabc":"code","34e81918":"code","552427b0":"code","98766ef0":"code","9ec3a4d4":"code","3697bc0f":"code","2935dcc8":"code","40afbc6e":"code","bce30464":"code","608f8345":"code","e0c940ba":"code","9e07d322":"code","47fc2601":"code","b20a0260":"code","77d3b24e":"code","566cbfb4":"code","4e5d8e74":"code","95584f32":"code","0d418e45":"code","52c19230":"code","e8192211":"code","4880b932":"code","d30115b3":"code","847d428d":"code","b5d26be5":"markdown","87f21338":"markdown","65dbcc96":"markdown","5c2a3eb4":"markdown","c366a2ac":"markdown","77f0bcf5":"markdown","16dbbfdc":"markdown","bcccef76":"markdown","d7b86190":"markdown","3d74e8ae":"markdown","d8e5856d":"markdown","1d802db5":"markdown","eed3ae24":"markdown","e637027a":"markdown","ae18107b":"markdown","3ada8044":"markdown","8189cfee":"markdown","7beca544":"markdown","5c532e51":"markdown"},"source":{"3ece4977":"!pip install \/kaggle\/input\/tensorboard-220\/tensorboard-2.2.0-py3-none-any.whl -q\n!pip install \/kaggle\/input\/pytorch-lightning\/pytorch_lightning-0.9.0-py3-none-any.whl -q","ce6da469":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom typing import List, Dict, Optional\nfrom pytorch_lightning import Callback\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import RepeatedKFold\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nimport random\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom typing import Dict, Union\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping","c3eb0d8a":"def set_seed(seed: int = 666) -> None:\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nset_seed()","9a48b524":"sub = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","8e479590":"train_features.shape","d8a5d807":"train_features.head()","e1e8ce69":"train_features.cp_type.value_counts(normalize=True)","d3a4b4b9":"control_group = train_features.loc[train_features.cp_type == 'ctl_vehicle', 'sig_id']\ntrain_targets_scored.loc[train_targets_scored['sig_id'].isin(control_group)].sum()[1:].sum()","f4bcdaa8":"test_features.cp_type.value_counts(normalize=True)","1395a0db":"train_features.cp_time.value_counts(normalize=True)","acd013a0":"train_features.cp_dose.value_counts(normalize=True)","45fb4ee8":"plt.hist(train_targets_scored.mean())\nplt.title('Distribution of mean target in each target column');","6ccba0ee":"train_targets_scored.mean().min(), train_targets_scored.mean().mean(), train_targets_scored.mean().max()","2f636d26":"no_control_target = train_targets_scored.loc[~train_targets_scored['sig_id'].isin(control_group)]","34ffed9a":"plt.hist(no_control_target.mean())\nplt.title('Distribution of mean target in each target column without control group');","1029d284":"no_control_target.mean().min(), no_control_target.mean().mean(), no_control_target.mean().max()","cf80eabc":"plt.plot(train_features.loc[train_features['sig_id'] == 'id_79fb45fe7', [col for col in train_features if 'g-' in col]].values.reshape(-1, 1));\nplt.title('g- value of id_79fb45fe7');","34e81918":"plt.plot(sorted(train_features.loc[train_features['sig_id'] == 'id_79fb45fe7', [col for col in train_features if 'g-' in col]].values.reshape(-1, 1)))\nplt.title('sorted g- value of id_79fb45fe7');","552427b0":"s = pd.DataFrame({'sig_id': test_features['sig_id'].values})","98766ef0":"s[train_targets_scored.columns[1:]] = 0.0001","9ec3a4d4":"control_group = test_features.loc[test_features.cp_type == 'ctl_vehicle', 'sig_id']","3697bc0f":"s.loc[s['sig_id'].isin(control_group), train_targets_scored.columns[1:]] = 0","2935dcc8":"s.to_csv('basic_submission.csv', index=False)","40afbc6e":"train_features = pd.concat([train_features, pd.get_dummies(train_features['cp_time'], prefix='cp_time')], axis=1)\ntrain_features = pd.concat([train_features, pd.get_dummies(train_features['cp_dose'], prefix='cp_dose')], axis=1)\ntrain_features = pd.concat([train_features, pd.get_dummies(train_features['cp_type'], prefix='cp_type')], axis=1)\n# train_features = train_features.loc[train_features['cp_type'] != 'ctl_vehicle']\ntrain_features = train_features.drop(['cp_type', 'cp_time', 'cp_dose'], axis=1)\n# train_targets_scored = train_targets_scored.loc[train_targets_scored['sig_id'].isin(train_features['sig_id'])]","bce30464":"class MoADataset(Dataset):\n    def __init__(\n        self,\n        data,\n        targets = None,\n        targets1 = None,\n        mode = 'train'\n    ):\n        \"\"\"\n\n        Args:\n        \"\"\"\n\n        self.mode = mode\n        self.data = data\n        self.targets = targets\n        self.targets1 = targets1\n\n    def __getitem__(self, idx: int) -> Dict[str, np.array]:\n        data = self.data[idx]\n        if self.targets is not None:\n            target = self.targets[idx]\n            target1 = self.targets1[idx]\n        else:\n            target = np.zeros((206,))\n            target1 = np.zeros((402,))\n            \n        sample = {'data': torch.tensor(data).float(),\n                  'target': torch.tensor(target).float(),\n                  'target1': torch.tensor(target1).float()}\n\n        return sample\n\n    def __len__(self) -> int:\n        return len(self.data)","608f8345":"class MoADataModule(pl.LightningDataModule):\n    def __init__(self, hparams: Dict,\n                 train_data, train_targets, train_targets1,\n                 valid_data, valid_targets, valid_targets1):\n        super().__init__()\n        self.hparams = hparams\n        self.train_data = train_data\n        self.train_targets = train_targets\n        self.train_targets1 = train_targets1\n        self.valid_data = valid_data\n        self.valid_targets = valid_targets\n        self.valid_targets1 = valid_targets1\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None):\n\n        \n        self.train_dataset = MoADataset(data=self.train_data.iloc[:, 1:].values,\n                                        targets=self.train_targets.iloc[:, 1:].values,\n                                        targets1=self.train_targets1.iloc[:, 1:].values)\n        self.valid_dataset = MoADataset(data=self.valid_data.iloc[:, 1:].values,\n                                        targets=self.valid_targets.iloc[:, 1:].values,\n                                        targets1=self.valid_targets1.iloc[:, 1:].values)\n\n    def train_dataloader(self):\n        train_loader = torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=1024,\n            num_workers=0,\n            shuffle=True,\n        )\n        return train_loader\n\n    def val_dataloader(self):\n        valid_loader = torch.utils.data.DataLoader(\n            self.valid_dataset,\n            batch_size=1024,\n            num_workers=0,\n            shuffle=False,\n        )\n\n        return valid_loader\n\n    def test_dataloader(self):\n        return None\n","e0c940ba":"n_h_layers = 2048\nlearning_rate = 1e-3\ncriterion = nn.BCEWithLogitsLoss()\n\nclass Net(nn.Module):\n    def __init__(self, n_in, n_h, n_out, n_out1):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(n_in, n_h)\n        self.fc2 = nn.Linear(n_h, math.ceil(n_h\/4))\n        self.fc3 = nn.Linear(math.ceil(n_h\/4), n_out)\n        self.fc4 = nn.Linear(math.ceil(n_h\/4), n_out1)\n        self.bn = nn.BatchNorm1d(n_in)\n        self.bn1 = nn.BatchNorm1d(n_h)\n        self.bn2 = nn.BatchNorm1d(math.ceil(n_h\/4))\n        self.drop = nn.Dropout(0.2)\n        self.n_out = n_out\n        self.selu = nn.SELU()\n        self.sigm = nn.Sigmoid()\n    def forward(self, x, targets, targets1):\n        \n        \n        self.loss = criterion\n        x = self.fc1(self.bn(x))\n        x = self.selu(x)\n        x = self.fc2(self.drop(self.bn1(x)))\n        x = self.selu(x)\n        \n        # scored targets\n        x1 = self.fc3(self.bn2(x))\n        # non scored targets\n        x2 = self.fc4(self.bn2(x))\n        loss = (self.loss(x1, targets) + self.loss(x2, targets1)) \/ 2\n        real_loss = self.loss(x1, targets)\n        # probabilities\n        out = self.sigm(x1)\n        return out, loss, real_loss\n    \n","9e07d322":"class LitMoA(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(LitMoA, self).__init__()\n        self.hparams = hparams\n        self.model = model\n\n    def forward(self, x, targets, targets1, *args, **kwargs):\n        return self.model(x, targets, targets1)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.001)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2)\n\n        return (\n            [optimizer],\n            [{'scheduler': scheduler, 'interval': 'epoch', 'monitor': 'valid_loss'}],\n        )\n\n    def training_step(\n        self, batch: torch.Tensor, batch_idx: int\n    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n        data = batch['data']\n        target = batch['target']\n        target1 = batch['target1']\n        out, loss, real_loss = self(data, target, target1)\n        logs = {'train_loss': loss, 'real_train_loss': real_loss}\n        return {\n            'loss': loss, 'real_train_loss': real_loss,\n            'log': logs,\n            'progress_bar': logs\n        }\n\n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        real_avg_loss = torch.stack([x['real_train_loss'] for x in outputs]).mean()\n        logs = {'train_loss': avg_loss, 'real_train_loss': real_avg_loss}\n        return {'log': logs, 'progress_bar': logs}\n\n    def validation_step(\n        self, batch: torch.Tensor, batch_idx: int\n    ) -> Union[int, Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]]:\n        data = batch['data']\n        target = batch['target']\n        target1 = batch['target1']\n        out, loss, real_loss = self(data, target, target1)\n        logs = {'valid_loss': loss, 'real_valid_loss': real_loss}\n\n        return {\n            'loss': loss, 'real_valid_loss': real_loss,\n            'log': logs,\n            'progress_bar': logs,\n        }\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        real_avg_loss = torch.stack([x['real_valid_loss'] for x in outputs]).mean()\n\n        logs = {'valid_loss': avg_loss, 'real_valid_loss': real_avg_loss}\n        return {'valid_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n","47fc2601":"test_features = pd.concat([test_features, pd.get_dummies(test_features['cp_time'], prefix='cp_time')], axis=1)\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_dose'], prefix='cp_dose')], axis=1)\ntest_features = pd.concat([test_features, pd.get_dummies(test_features['cp_type'], prefix='cp_type')], axis=1)\n# test_features = test_features.loc[test_features['cp_type'] != 'ctl_vehicle']\ntest_features = test_features.drop(['cp_type', 'cp_time', 'cp_dose'], axis=1)\ntest_dataset = MoADataset(data=test_features.iloc[:, 1:].values)\ntest_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size=1024,\n            num_workers=0,\n            shuffle=False,\n        )","b20a0260":"class MetricsCallback(Callback):\n    \"\"\"PyTorch Lightning metric callback.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.metrics = []\n\n    def on_validation_end(self, trainer, pl_module):\n        self.metrics.append(trainer.callback_metrics)","77d3b24e":"hparams = {}","566cbfb4":"rkf = RepeatedKFold(n_splits=6, n_repeats=8, random_state=42)\nn = rkf.get_n_splits()\nall_predictions = None\nscores = []\nfor train_index, valid_index in rkf.split(train_features):\n    \n    net = Net(n_in = 879, n_h = n_h_layers, n_out = 206, n_out1 = 402)\n    # split data\n    train_data, valid_data = train_features.iloc[train_index, :], train_features.iloc[valid_index, :]\n    train_targets_scored_train, train_targets_scored_valid = train_targets_scored.iloc[train_index, :], train_targets_scored.iloc[valid_index, :]\n    train_targets_nonscored_train, train_targets_nonscored_valid = train_targets_nonscored.iloc[train_index, :], train_targets_nonscored.iloc[valid_index, :]\n            \n    model = LitMoA(hparams=hparams, model=net)\n    \n    dm = MoADataModule(hparams=hparams,\n                       train_data=train_data, train_targets=train_targets_scored_train, train_targets1=train_targets_nonscored_train,\n                       valid_data=valid_data, valid_targets=train_targets_scored_valid, valid_targets1=train_targets_nonscored_valid)\n    \n    hparams = {}\n    metrics_callback = MetricsCallback()\n    trainer = pl.Trainer(\n            early_stop_callback=EarlyStopping(monitor='valid_loss', patience=10, mode='min'),\n            checkpoint_callback=ModelCheckpoint(monitor='valid_loss', save_top_k=1, filepath='{epoch}_{valid_loss:.4f}', mode='min'),\n            gpus=1,\n            max_epochs=50,\n            log_save_interval=100,\n            num_sanity_val_steps=0,\n            gradient_clip_val=0.5,\n            weights_summary='full',\n            callbacks = [metrics_callback]\n    )\n    \n    \n    trainer.fit(model, dm)\n    \n    score = metrics_callback.metrics[-1]['real_valid_loss'].item()\n    scores.append(score)\n    \n    predictions = np.zeros((test_features.shape[0], 206))\n    model_inference = model.model\n    model_inference.eval()\n    \n    for ind, batch in enumerate(test_loader):\n        p = model_inference(batch['data'], batch['target'], batch['target1'])[0].detach().cpu().numpy()\n        predictions[ind * 1024:(ind + 1) * 1024] = p\n        \n    if all_predictions is None:\n        all_predictions = predictions\n    else:\n        all_predictions += predictions\n        \nall_predictions = all_predictions \/ n","4e5d8e74":"print(f'Mean score: {np.mean(scores):.4f}. Std score: {np.std(scores):.4f}')","95584f32":"all_predictions.max()","0d418e45":"plt.hist(all_predictions.mean())\nplt.title('Distribution of prediction means');","52c19230":"s = pd.DataFrame({'sig_id': test_features['sig_id'].values})","e8192211":"for col in train_targets_scored.columns[1:].tolist():\n    s[col] = 0","4880b932":"s.loc[:, train_targets_scored.columns[1:]] = all_predictions","d30115b3":"test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ns.loc[s['sig_id'].isin(test_features.loc[test_features['cp_type'] =='ctl_vehicle', 'sig_id']), train_targets_scored.columns[1:]] = 0","847d428d":"s.to_csv('submission.csv', index=False)","b5d26be5":"This looks a bit strange for me, maybe there is some \"magic\" in this dataset?","87f21338":"### Important! Samples with zero targets!","65dbcc96":"The rate of control group is a bit higher in public test data.","5c2a3eb4":"### Interesting values of g- features","c366a2ac":"## Pytorch-lightning","77f0bcf5":"As per description:\n```\ncp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs\n```\n\nLet's check this!","16dbbfdc":"We have a high imbalance: the max target rate is 0.03, the min is very low. We would need to be careful","bcccef76":"If we exclude control group values become higher, but not by much.","d7b86190":"### version history\n\nv12: install pytorch-lightning from source (no need for internet, can sumbit directly). add RepeatedKFold","3d74e8ae":"I'll use both scored and nonscored targets.","d8e5856d":"## General description\n\nNowadays drug disrvery is done using models based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\nIn this dataset we have information about one of the approaches: sample of human cells is treated with the drug and then scientists analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n\nWe have the following information:\n* 772 columns with gene expression data\n* 100 columns with cell viability data\n* binary column showing whether the sample was treated\n* treatment duration and dose\n\n**the re-run dataset has approximately 4x the number of examples seen in the Public test.**\n\nIn this notebook I'll do EDA and build models.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Mechanism_of_action_for_beta_blockers.png)","1d802db5":"### Target values distribution","eed3ae24":"Rates of treatment duration and doze size is roughly the same","e637027a":"## Fixed prediction\n\nLet's make a fixed prediction as a baseline. I'll set predictions for 0.0001 (random arbitrary number) and set control group to zero.","ae18107b":"So we have 23.8k rows in train data - not much, but the number of columns is huge!\n\nFirst of all let's have a look at train data.","3ada8044":"This approach gives 0.034. Not bad right? Now let's build a model.","8189cfee":"Preprocessing - ohe on categorical columns","7beca544":"Indeed! Control group has zero values in targets. I think we can do two things based on this insight:\n* don't use these rows in training\n* for test data set predictions to zero for such rows","5c532e51":"## Data exploration\n\n"}}