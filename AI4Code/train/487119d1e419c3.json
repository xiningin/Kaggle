{"cell_type":{"320a8147":"code","5c694afc":"code","c0c304f1":"code","2a741382":"code","1ac4ff80":"code","e50e750e":"code","db190ebe":"code","bb436a6c":"code","aafab6a3":"code","83935e07":"code","5f968496":"code","bc2f424d":"code","5f069d41":"code","fba93206":"code","546aa8b1":"code","852954f1":"code","8fe170f5":"code","737c1ec8":"code","6e9c1379":"code","d68a623b":"code","a66ab5f5":"code","8b1811d9":"code","4c3c856f":"code","e9996442":"code","64aa1911":"code","15afba63":"code","395711c4":"code","957e1582":"code","8f8facd1":"code","e7330c3b":"code","4be1d16c":"code","6952c461":"code","4fef461d":"code","1d61e361":"code","2a6d69c9":"code","ce3b54f3":"code","a6912719":"code","abdfa64f":"code","7bcc3a22":"code","c5de6cb3":"code","124c7da5":"code","9bfcef91":"code","c718db83":"code","073f156a":"code","84469082":"code","1584b0d0":"code","a3b0de42":"code","6f92185c":"markdown","4a59800f":"markdown","d846fac7":"markdown","5c5a289a":"markdown","ab2120c3":"markdown","2300e1ef":"markdown","7577c95a":"markdown","f403d278":"markdown","ed3e20b8":"markdown","7e02fed5":"markdown","4d5cdbba":"markdown","b78bc9eb":"markdown","214a84d3":"markdown","533172a5":"markdown","56e9210f":"markdown","bf8c41f7":"markdown","451c1fce":"markdown","9a63d3f6":"markdown","425d37a4":"markdown","353cba7f":"markdown","f37fc783":"markdown","cd622a71":"markdown","21c420b1":"markdown","4b8420d9":"markdown","530badbb":"markdown","9f95d623":"markdown","6fe39a35":"markdown","9171f5ea":"markdown","92efda83":"markdown","9386ca5a":"markdown","37f0121e":"markdown"},"source":{"320a8147":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5c694afc":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/black-friday\/train.csv\")\n\n\n#limiting the amount of data we use to keep computation fast \ndf = df.iloc[:]\ndf.shape","c0c304f1":"df.head()","2a741382":"df.describe()","1ac4ff80":"print(\"Data columns\")\nprint(\"--------------------------------------------\")\nprint(pd.DataFrame(df.info()))","e50e750e":"print(df.isnull().sum())","db190ebe":"\ndf.isnull().sum()\/df.isnull().count()*100","bb436a6c":"df = df.drop('Product_Category_3', axis=1)\n\n## also dropping user id category \ndf = df.drop('User_ID', axis=1)\ndf = df.drop('Product_ID', axis=1)","aafab6a3":"df['Marital_Status'].unique()","83935e07":"df['Product_Category_2'].unique()","5f968496":"len(df['Product_Category_2'].unique())","bc2f424d":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer = imputer.fit(pd.DataFrame(df['Product_Category_2']))\n\n\ndf['Product_Category_2'] = imputer.transform(pd.DataFrame(df['Product_Category_2']))\n#data_train['Product_Category_2'] = np.round(data_train['Product_Category_2'])","5f069d41":"imputer = imputer.fit(pd.DataFrame(df['Marital_Status']))\ndf['Marital_Status'] = imputer.transform(pd.DataFrame(df['Marital_Status']))","fba93206":"len(df['Product_Category_2'].unique())","546aa8b1":"df.isnull().sum()\/df.isnull().count()*100","852954f1":"print(\"average purchase for male purchasers = \",\nnp.mean(df['Purchase'].loc[df['Gender'] == 'M']),'\\n')\nprint(\"-\"*115,'\\n')\nprint(\"average purchase for female purchasers = \",\nnp.mean(df['Purchase'].loc[df['Gender'] == 'F']))","8fe170f5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig= plt.figure(figsize=(12,7))\n\n\nsns.set(style=\"darkgrid\")\n\n\nx = pd.DataFrame({\"male average purchase\": [9437], \"Female average purchase\": [8734]})\n\nsns.barplot(data=x)","737c1ec8":"print('Number of Female purchasers = ',df['Gender'][df['Gender'] == 'F'].count())\nprint('Number of male purchasers   = ',df['Gender'][df['Gender'] == 'M'].count())","6e9c1379":"#change gender from 'm' and 'f' to binary \ndf.loc[:, 'Gender'] = np.where(df['Gender'] == 'M', 1, 0)\n\n#renaming some columns \ndf = df.rename(columns={\n                #'Product_ID': 'ProductClass',\n                'Product_Category_1': 'Category1',\n                'Product_Category_2': 'Category2',\n                'City_Category': 'City',\n                'Stay_In_Current_City_Years': 'City_Stay'\n})\n#y = train.pop('Purchase')\n\n","d68a623b":"df.head()","a66ab5f5":"#len(df['ProductClass'].unique())","8b1811d9":"# from sklearn.preprocessing import LabelEncoder\n# L_encoder =  LabelEncoder()\n# for col in ['ProductClass']:    \n#     df.loc[:, col] =L_encoder.fit_transform(df[col])\n# df[['ProductClass']]","4c3c856f":"from sklearn.preprocessing import  OneHotEncoder\ncats = ['Occupation', 'Age', 'City', 'Category1','Category2','City_Stay']\n\n#creating the encoder, fit it to our data \nencoder = OneHotEncoder().fit(df[cats])\n","e9996442":"#generating feature names for our encoded data\nencoder.get_feature_names(cats)","64aa1911":"#building dataframe with encoded catgegoricals \n\n## we use index values from our original data \n## we GENERATE feature names using our encoder\n\nendcoded_data = pd.DataFrame(encoder.transform(df[cats]).toarray(),index=df.index, columns=encoder.get_feature_names(cats))\nendcoded_data.head()","15afba63":"df = pd.concat([df, endcoded_data],sort=False,axis=1)\n\ndf=df.drop(cats, axis=1)","395711c4":"df = df.fillna(0)\ndf.head(15)\n","957e1582":"X = df.drop('Gender',axis=1)\ny = df.pop('Gender')\n\n","8f8facd1":"#X=np.nan_to_num(X)","e7330c3b":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)","4be1d16c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size= 0.25)","6952c461":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train,y_train)","4fef461d":"knn.score(X_test,y_test)","1d61e361":"# parameters = { 'algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n#                'leaf_size' : [18,20,25,27,30,32,34],\n#                'n_neighbors' : [3,5,7,9,10,11,12,13]\n#               }\n\n# from sklearn.model_selection import GridSearchCV\n# gridsearch = GridSearchCV(knn, parameters,verbose=3)\n# gridsearch.fit(X_train,y_train)\n# gridsearch.best_params_","2a6d69c9":"knn = KNeighborsClassifier(algorithm = 'auto', leaf_size =35, n_neighbors =5)\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)\n","ce3b54f3":"dft = pd.read_csv(\"\/kaggle\/input\/black-friday\/test.csv\")\n\ndft = dft.drop('Product_Category_3', axis=1)\n\n## also dropping user id category \ndft = dft.drop('User_ID', axis=1)\ndft = dft.drop('Product_ID', axis=1)\n\n#Product_ID\ndft.head()","a6912719":"dft = dft.iloc[:1]\ndft","abdfa64f":"dft = dft.rename(columns={\n                #'Product_ID': 'ProductClass',\n                'Product_Category_1': 'Category1',\n                'Product_Category_2': 'Category2',\n                'City_Category': 'City',\n                'Stay_In_Current_City_Years': 'City_Stay'\n})\ndft","7bcc3a22":"# dft['ProductClass']='P00248942'\n# dft","c5de6cb3":"#change gender from 'm' and 'f' to binary \ndft['Gender'] = 9851\ndft","124c7da5":"p =pd.DataFrame(encoder.transform(dft[cats]).toarray(),columns=encoder.get_feature_names(cats))\np","9bfcef91":"dft=dft.drop(cats, axis=1)\ndft","c718db83":"dft = pd.concat([dft, p],sort=False,axis=1)\n","073f156a":"df.head(1)","84469082":"dft","1584b0d0":"# dft['ProductClass'] =L_encoder.transform(dft['ProductClass'])\n# p","a3b0de42":"knn.predict(dft)","6f92185c":"# OneHotEncoder for other Classes","4a59800f":"## REPLACING NANS(after OH encoding) with 0","d846fac7":"## dropping categorical data, adding the encoded data\n","5c5a289a":"## Using, and transforming available data to make a prediction, user input will need to be transformed also ","ab2120c3":"# TRAIN TEST SPLIT ","2300e1ef":"# Mean purchase for men Vs women\n###  males buying higher value purchases than females, But the difference not large.","7577c95a":"# FIRST QUICK LOOK AT THE DATA ","f403d278":"## confirming no missing values ","ed3e20b8":"# CREATE THE FINAL INPUT FOR PREDICTION ","7e02fed5":"## we can see that these colums have some null values\n    Product_Category_2           \n    Product_Category_3            ","4d5cdbba":"# HYPER PARAMETER TUNING\n# WITH GridSearchCV ","b78bc9eb":"> ## drop columns","214a84d3":"# MAKING A PREDICTION ","533172a5":"# FITTING OUR MODEL\n","56e9210f":"# THESE ARE THE INPUTS THAT WE REQUIRE FROM THE USER, AFTER TAKING THE INPUT, WE NEED TO DO OUR TRANSFORMATIONS TO GET A PREDICTION\n\nGender \tAge \tOccupation \tCity \tCity_Stay \tMarital_Status \tCategory1 \tCategory2\n\n\n## SINCE THESE ARE CATEGORICAL WE NEED TO PUT A LEGEND SO THAT THE USER CAN PUT THE CORRECT INPUTS","bf8c41f7":"# LABEL ENCODING FOR PRODUCT CLASS","451c1fce":"# number of observations of female individuals vs male ","9a63d3f6":"# IMPUTING \n\n## for Product_Category_2 we can impute the 31.5 % of missing data \n## lets use simple imputer for now, later we can take care of these missing data with other methods as well, to perhaps boost our accuracy ","425d37a4":"> ## Rename similarly","353cba7f":"# HANDLING MISSING DATA ","f37fc783":"# SCALING","cd622a71":"## percentage of missing data:","21c420b1":"## ENCODING CATEGORICAL VARIABLES \n\n","4b8420d9":"## FIRST SCORE","530badbb":"# CREATING the TUNED THE MODEL AGAIN","9f95d623":"## we can delete Product_Category_3 column  because missing data in this columns more than 60% of the observations","6fe39a35":"## IMP*\n> ## encode all required fequres with the exact same encoder","9171f5ea":"# FEATURE ENGINEERING FOR BUILDING MODELS\n","92efda83":"# DEFINING FEATURES AND LABELS ","9386ca5a":"# get prediction ","37f0121e":"# OUR INPUT DATA FOR PREDICTION NEEDS TO LOOK LIKE OUR FEATURES FOR TRAINING\n\n## this means, we have to : \n    drop the same caolumns \n    encode the categorical features in the same way, \n    etc. \n    \n## basically our input data needs to go through all the things we did to our training data, \n## this exercise will also help us find what all inputs to take from the user "}}