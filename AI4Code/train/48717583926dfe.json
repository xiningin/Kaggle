{"cell_type":{"2f6ce3cc":"code","21448b96":"code","9c814699":"code","6cf39fe9":"code","fcaaf760":"code","59299fb3":"code","647bf434":"code","5d62fe3d":"code","1949fd3a":"code","d0e42cff":"code","3309a8aa":"code","f46435c4":"code","f3da66b7":"code","303f0329":"markdown","3459b794":"markdown","e7faca2e":"markdown"},"source":{"2f6ce3cc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","21448b96":"from sklearn import tree, ensemble, metrics, linear_model, preprocessing, model_selection, feature_selection\nfrom plotnine import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9c814699":"# Create class which performs Label Encoding - if required\nclass categorical_encoder:\n    def __init__(self, columns, kind = 'label', fill = True):\n        self.kind = kind\n        self.columns = columns\n        self.fill = fill\n        \n    def fit(self, X):\n        self.dict = {}\n        self.fill_value = {}\n        \n        for col in self.columns:\n            label = preprocessing.LabelEncoder().fit(X[col])\n            self.dict[col] = label\n            \n            # To fill\n            if self.fill:\n                self.fill_value[col] = X[col].mode()[0]\n                X[col] = X[col].fillna(self.fill_value[col])\n                \n        print('Label Encoding Done for {} columns'.format(len(self.columns)))\n        return self\n    def transform(self, X):\n        for col in self.columns:\n            if self.fill:\n                X[col] = X[col].fillna(self.fill_value[col])\n                \n            X.loc[:, col] = self.dict[col].transform(X[col])\n        print('Transformation Done')\n        return X\n    \ndef categorical(df):\n    df['var2'] = df['var2'].replace({'A': 1, 'B': 2, 'C': 3})\n    return df\n\n","6cf39fe9":"test = pd.read_csv('\/kaggle\/input\/test_pavJagI.csv')\ntrain = pd.read_csv('\/kaggle\/input\/train_6BJx641.csv')\n\ntrain['datetime'] = pd.to_datetime(train['datetime'], yearfirst = True)\ntest['datetime'] = pd.to_datetime(test['datetime'], yearfirst = True)","fcaaf760":"# The following approach of merging train and test works for competitions but not during production because in production \n# it is required to replicate the operations performed on the training set on the test set. However, if done correctly, the \n# results will be similar in the approaches.\n\n# Indicators\ntrain['which'] = 1\ntest['which'] = 0\n\n# Merge\ndata = pd.concat([train, test], axis = 0, ignore_index = True)\ndata = data.set_index('datetime')\ndata = data.sort_index()\n\n############  Create New Features - with different lags ################\ndata['temperature_rolling'] = data['temperature'].rolling('24H').mean()\ndata['var1_rolling'] = data['var1'].rolling('24H').mean()\ndata['windspeed_rolling'] = data['windspeed'].rolling('24H').mean()\n\n# Week lagged features\ndata['temperature_rolling_week'] = data['temperature'].rolling('168H').mean()\ndata['var1_rolling_week'] = data['var1'].rolling('168H').mean()\ndata['windspeed_rolling_week'] = data['windspeed'].rolling('168H').mean()\n\n# Other Lags\ndata['temperature_rolling_3d'] = data['temperature'].rolling('72H').mean()\ndata['temperature_rolling_6h'] = data['temperature'].rolling('6H').mean()\ndata = data.reset_index()\n\n\n# y lagged values\ntemp_med = data.set_index('datetime').resample('2M')['electricity_consumption'].mean()\ntemp_med.name = 'y_med_month'\ndata = pd.concat([data.set_index('datetime'),temp_med], \n          axis = 1).fillna(method = 'bfill').reset_index()\n\n# Split Back\ntrain = data.loc[data['which'] == 1, :].drop('which', axis = 1)\ntest = data.loc[data['which'] == 0, :].drop(['which', 'electricity_consumption'], axis = 1)","59299fb3":"# Create Time Series datetime features\ndef ts_features(df, col = 'datetime'):\n    #df['dayofmonth'] = df[col].dt.day\n    df['weekday'] = df[col].dt.dayofweek\n    df['weekend'] = (df[col].dt.dayofweek >= 5)*1\n    df['month'] = df[col].dt.month\n    df['hour'] = df[col].dt.hour\n    df['year'] = df[col].dt.year\n    return df","647bf434":"X_train = ts_features(train).drop(['datetime', 'electricity_consumption', 'ID'], axis = 1)\ny_train = ts_features(train)['electricity_consumption']\n\nX_test = ts_features(test).drop(['ID', 'datetime'], axis = 1)\n\ntest_id = test.ID\ntrain_id = train.ID\n\nX_train = categorical(X_train)\nX_test = categorical(X_test)","5d62fe3d":"X_train.head()","1949fd3a":"X_test = X_test.fillna(method = 'ffill')","d0e42cff":"import lightgbm as lgb\nmodel = lgb.LGBMRegressor(n_estimators = 4000, learning_rate = .02, \n                          max_features = .7, max_depth = 3, subsample = .9).fit(X_train, y_train)\n","3309a8aa":"# Make Submission\nsubmission = pd.DataFrame()\nsubmission['ID'] = test_id\n\n# Take a weighted average of the predictions\nsubmission['electricity_consumption'] = model.predict(X_test)\n\n# Save submission file\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index = None)","f46435c4":"from sklearn import pipeline\nimport xgboost as xgb\nimport catboost as cb\n\nmodel = ensemble.RandomForestRegressor(n_estimators = 250, max_depth = 20, min_samples_leaf = 5,\n                                       n_jobs = 4).fit(X_train, y_train)\nmodel1 = lgb.LGBMRegressor(n_estimators = 4000, learning_rate = .02, max_features = .7, max_depth = 3, subsample = .9).fit(X_train, y_train)\nmodel2 = xgb.XGBRegressor(n_estimators = 2000, learning_rate = .04, max_features = .7, max_depth = 3, subsample = .9).fit(X_train, y_train)\nmodel4 = cb.CatBoostRegressor(n_estimators = 2000, learning_rate = .04, max_depth = 3,\n                              rsm = .7, subsample = .9, silent = True).fit(X_train, y_train)","f3da66b7":"submission = pd.DataFrame()\nsubmission['ID'] = test_id\n\n# Take a weighted average of the predictions\nsubmission['electricity_consumption'] = (model.predict(X_test)+(3*model1.predict(X_test)+(2*model2.predict(X_test))+\\\n                                                               (3*model4.predict(X_test))))\/9\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index = None)","303f0329":"**Ensemble Model Approach**      \n\nHere, the idea is to train several models and to take a weighted average of their predictions. I have chosen the weights by trial and error. ","3459b794":"**Single Model - LightGBM Approach**","e7faca2e":"## 2 Approaches\n1. Single Model(LightGBM)\n2. Ensemble Model(LightGBM+XGBoost+CatBoost+RandomForest)"}}