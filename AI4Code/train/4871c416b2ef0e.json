{"cell_type":{"5289e2da":"code","4d509485":"code","ee7fd13c":"code","46427a80":"code","17a41b23":"code","f4cd2fab":"code","c61d6e66":"code","d1efbbb6":"code","fee056ef":"code","d1667433":"code","8e98452c":"code","1421efe5":"code","d6751e8d":"code","1452b8a8":"code","a1018fb7":"code","275b9fc8":"code","b41cf789":"code","da2d8184":"code","3920ecd3":"code","824bc789":"code","f0774fe9":"code","723f8e05":"code","57d77010":"code","fa5da332":"code","fc35dbf0":"code","775abd10":"code","2d95227c":"code","05c84ec6":"code","a6f503fb":"code","7f0ba560":"code","14ccd372":"code","765cf723":"code","c63e3421":"code","d5406937":"code","786bdc57":"code","fa07c9b6":"markdown"},"source":{"5289e2da":"import pandas as pd\nimport datetime\nimport nltk\nimport spacy\n\nimport re\nfrom time import time \nfrom collections import defaultdict\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\n\ndata = pd.read_json(\"..\/input\/arxivdataset\/arxivData.json\")\nprint(list(data))\ndata.shape\ndata.head()\ndata.isnull().sum()","4d509485":"# ### smaller data. subset: \n# data = data.loc[data[\"year\"]>2016]","ee7fd13c":"data.head()","46427a80":"data = data.dropna().reset_index(drop=True)\ndata.isnull().sum()","17a41b23":"data[\"year\"] = pd.to_datetime(data[\"year\"])\n\n# data = data.sort_values(by=\"year\") # disable for easier comparison\ndata.head()","f4cd2fab":"data_abstract = data['summary'].str.lower()\ndata_abstract.head()","c61d6e66":"x= len(data_abstract)\nprint(x)","d1efbbb6":"nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\n##ORIGL\n# def cleaning(doc):\n#     # Lemmatizes and removes stopwords\n#     # doc needs to be a spacy Doc object\n#     txt = [token.lemma_ for token in doc if not token.is_stop]\n#     # Word2Vec uses context words to learn the vector representation of a target word,\n#     # if a sentence is only one or two words long,\n#     # the benefit for the training is very small\n#     if len(txt) > 2:  \n#         return ' '.join(txt)\n\n##ALT - speedup: \n\ndef cleaning(doc):\n    # Lemmatizes and removes stopwords\n    # doc needs to be a spacy Doc object\n    if len(doc)>2:\n        txt = [token.lemma_ for token in doc if not token.is_stop]\n        # Word2Vec uses context words to learn the vector representation of a target word,\n        # if a sentence is only one or two words long,\n        # the benefit for the training is very small\n        if len(txt) > 2:\n            return ' '.join(txt)","fee056ef":"brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in data['summary'])","d1667433":"t = time()\n\ntxt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n\nprint('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))","8e98452c":"data_clean = pd.DataFrame({'clean': txt})\ndata_clean = data_clean.dropna().drop_duplicates()\ndata_clean.shape","1421efe5":"from gensim.models.phrases import Phrases, Phraser\nsent = [row.split() for row in data_clean['clean']]","d6751e8d":"print(sent[0])","1452b8a8":"phrases = Phrases(sent, min_count= 20, progress_per=10000) # min_count=30\nbigram = Phraser(phrases)","a1018fb7":"sentences = bigram[sent]","275b9fc8":"### get trigrams, by including found bigrams\/phrases\nphrases2 = Phrases(sent, min_count= 20, progress_per=10000) # min_count=30\ntrigram = Phraser(phrases2)\n\nsentences = trigram[sentences]","b41cf789":"sentences[0]","da2d8184":"word_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","3920ecd3":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","824bc789":"import multiprocessing\n\nfrom gensim.models import Word2Vec","f0774fe9":"cores = multiprocessing.cpu_count()","723f8e05":"w2v_model = Word2Vec(min_count=15,  # min_count=20,\n                     window=5,  # window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","57d77010":"t = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","fa5da332":"t = time()\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=25, # epochs=30,\n                report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","fc35dbf0":"w2v_model.init_sims(replace=True)","775abd10":"w2v_model.wv.most_similar(positive=[\"software\"])","2d95227c":"w2v_model.wv.most_similar(positive=[\"engineering\"])","05c84ec6":"w2v_model.wv.similarity(\"technology\", 'era')","a6f503fb":"w2v_model.wv.doesnt_match(['era', 'approach', 'technology'])","7f0ba560":"w2v_model.wv.most_similar(positive=[\"technology\", \"era\"], negative=[\"approach\"], topn=5)","14ccd372":"w2v_model.wv.most_similar(positive=[\"machine_learning\"])","765cf723":"w2v_model.wv.most_similar(positive=[\"ai\"])","c63e3421":"w2v_model.wv.most_similar(positive=[\"artificial_intelligence\",\"ai\",'machine_learning','machine_learn'])","d5406937":"w2v_model.wv.most_similar(positive=[\"attention_mechanism\"])","786bdc57":"w2v_model.wv.most_similar(positive=[\"embedding\"])","fa07c9b6":"* https:\/\/stackoverflow.com\/questions\/35716121\/how-to-extract-phrases-from-corpus-using-gensim"}}