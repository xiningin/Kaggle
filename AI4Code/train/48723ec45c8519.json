{"cell_type":{"774718e3":"code","1d9d58fd":"code","8f5bdccb":"code","1f9c0cc6":"code","3adf9ec1":"code","5cbb4c9c":"code","ec47b409":"code","b93b1a10":"markdown","a903240e":"markdown","2d68c603":"markdown","a56d1e96":"markdown","f5a72f9d":"markdown","a3f4913d":"markdown","d4bdc4fc":"markdown","15cfcfb9":"markdown"},"source":{"774718e3":"import numpy as np\nimport matplotlib.pyplot as plt","1d9d58fd":"v1 = np.array([3,1])\nv2 = np.array([1,3])\nB = np.column_stack((v1, v2))\n\n# We sample from two independent distributions along the axes v1 and v2, the resulting vector is the sum\nX = np.dot(np.random.randn(1000,2), B.T)","8f5bdccb":"plt.figure(figsize=(10,10))\n\nplt.axline((0,0), slope=v1[1]\/v1[0])\nplt.axline((0,0), slope=v2[1]\/v2[0])\n\nplt.scatter(X[:,0], X[:,1],c='r');\nplt.axhline(y=0, color='k');\nplt.axvline(x=0, color='k');\n\nplt.show()","1f9c0cc6":"cov = lambda x: np.dot(x.T, x)\/x.shape[0]\nC = cov(X)\n\nL, Q = np.linalg.eigh(C)\nXQ = np.dot(X,Q)","3adf9ec1":"np.diag(L)","5cbb4c9c":"cov(XQ)","ec47b409":"plt.figure(figsize=(10,10))\n\nplt.axline((0,0), slope=v1[1]\/v1[0])\nplt.axline((0,0), slope=v2[1]\/v2[0])\n\nplt.scatter(X[:,0],X[:,1],c='r', label='X');\nplt.scatter(XQ[:,0],XQ[:,1],c='b', label='XQ');\nplt.axhline(y=0, color='k');\nplt.axvline(x=0, color='k');\n\nplt.legend()\nplt.show()","b93b1a10":"## Dimensionality Reduction\nSince $Q$ is an isomorphic linear transformation, fitting a model to $XQ$ is basically the same as fitting a model to $X$.\n\nHowever, the transformation reorients the data points so that the main axes of variation coincide with the standard coordinate axes (the covariance matrix $D$ is diagonal). All the variation is contained within each axis, or feature, of the transformed matrix $XQ$.\n\nThus, in order to perform dimensionality reduction, we can throw away the features of $XQ$ with small variance, i.e., the columns correponding to elements with small absolute value in $D$.","a903240e":"Lets plot the data points along the axes","2d68c603":"# The Intuition Behind Principal Component Analysis\n\n","a56d1e96":"Lets compare $D$ and the covariance matrix of $XQ$:","f5a72f9d":"Now we will compute $D$, $Q$, and apply the transformation to $X$:","a3f4913d":"## Illustration\nLets generate random points according to a normal distribution and use two given vectors $v_1$ and $v_2$ as the basis for the coordinate system.","d4bdc4fc":"Finally, lets plot the transformed data:","15cfcfb9":"Consider a data table $X$ with $m$ rows and $n$ columns. The rows represent sample points and the columns represent features. Assume the data is normalized so that each column has mean 0 and standard deviation 1.\n\nThe covariance $\\frac{\\sum_{i=1}^{n} X^uX^v}{m}$ between two columns $u$ and $v$ measures the joint variability of the two columns. It can be thought as a measure of how much information is shared by the two columns.\n\nThe covariance matrix $C=X^tX$ is simmetric and can be diagonalized, i.e., it can be written as $C=QDQ^{-1}$, where $D$ is a diagonal $n \\times n$ matrix whose diagonal is composed of the eigenvalues of $C$, and $Q$ is an $n \\times n$ matrix whose columns are the corresponding eingenvectors of $C$.\n\nNo information is lost if we multiply $X$ by a non-singular matrix. Lets consider the transformation of $X$ by $Q$ and analyse the covariance matrix of the result:\n\n$(XQ)^tXQ = Q^tX^tXQ = Q^tCQ = Q^TQD = D$\n\nSince $D$ is diagonal, by applying the transformation $Q$ to $X$, we have completely eliminated the covariance between the columns, all the information is captured around the main axes of the new coordinate system. "}}