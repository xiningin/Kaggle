{"cell_type":{"7e0bf1a2":"code","39f3001c":"code","81b27c1e":"code","4c43a623":"code","d5b0665d":"code","7da3610d":"code","a40954d2":"code","734188d1":"code","2ab322db":"code","e994ba69":"code","1277cdb7":"code","c35af73c":"code","21a97af9":"code","2496ec10":"code","4ea52bdf":"code","8bbb4a8a":"code","9db16474":"code","880254b0":"code","ed49f931":"code","7145144e":"code","2b4fb189":"code","1494e562":"code","25efaf13":"code","712a43a6":"code","51421fcd":"code","0025c835":"code","3cc5b6e5":"code","01758816":"code","87e60b69":"code","a5140be9":"code","ee78e1c1":"code","865fc188":"code","44c50d27":"markdown","d9aff46d":"markdown","f5c208ab":"markdown","db6b6a7b":"markdown","cd878860":"markdown","e10d8bbf":"markdown"},"source":{"7e0bf1a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rnd\nimport sklearn as sk\nfrom tqdm.notebook import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39f3001c":"full_table = (pd.read_csv(\"\/kaggle\/input\/covid19factorsimpact\/fullCOVIDtable.csv\",sep=\";\").\n        drop(columns=\"Unnamed: 0\").\n        loc[:,[\"SCHOOL\",\n               \"WORK\",\n               \"EVENTS\",\n               \"GATHERINGS\",\n               \"TRANSPORTATION\",\n               \"ATHOME\",\n               \"NATIONAL\",\n               \"INTERNATIONAL\",\n               \"INFORMATION\",\n               \"TESTING\",\n               \"TRACING\",\n               \"DAY\",\n               \"COUNTRY\",\n               \"OLD\",\n               \"YOUNG\",\n               \"URBAN\",\n               \"DENSITY\",\n               \"POPULATION\",\n               \"PHYSICIANS\",\n               \"BEDS\",\n               \"REFF\",\n               \"INFECTED\",\n               \"INFECTEDINCREASE\",\n               \"MORTALITY\",\n               \"FATALITYINCREASE\",\n               \"TEMPERATURE\",\n               \"HUMIDITY\",\n               \"WIND\",\n               \"CONTINENT\",\n               \"DATE\"]].\n        dropna()\n       )\n\nfull_table","81b27c1e":"# drop all countries with less than 30 days of useful data\n# create columns representing relevant data in the past by means of shift.\n\ndaysPerCountry = full_table.groupby(by=\"COUNTRY\")[\"REFF\"].count()\ndaysPerCountry[daysPerCountry<30]","4c43a623":"factorsTS = [\"SCHOOL\",\n               \"WORK\",\n               \"EVENTS\",\n               \"GATHERINGS\",\n               \"TRANSPORTATION\",\n               \"ATHOME\",\n               \"NATIONAL\",\n               \"INTERNATIONAL\",\n               \"INFORMATION\",\n               \"TESTING\",\n               \"TRACING\",\n               \"INFECTED\",\n               \"INFECTEDINCREASE\",\n               \"MORTALITY\",\n               \"FATALITYINCREASE\",\n               \"TEMPERATURE\",\n               \"HUMIDITY\",\n               \"WIND\"]\n\nfor factor in factorsTS:\n    for daysInPast in range(1,31):\n        full_table[factor+\"-\"+str(daysInPast)] = full_table.groupby(\"COUNTRY\")[factor].shift(daysInPast)\n        \n        \nfull_table","d5b0665d":"def binReff(reff):\n    binnedReff = 0\n    if reff >= 1 and reff <1.5:\n        binnedReff = 1\n    elif reff>= 1.5 and reff <=2:\n        binnedReff = 2\n    elif reff>2:\n        binnedReff = 3\n        \n    return binnedReff\n\n\ndef binMortality(mort):\n    binMort = 0\n    if mort <=1:\n        binMort = 1\n    elif mort <=5:\n        binMort = 5\n    elif mort <=10:\n        binMort = 10\n    elif mort >10:\n        binMort = 15\n        \n    return binMort","7da3610d":"full_table[\"REFFBINNED\"] = full_table[\"REFF\"].apply(binReff)\nfull_table[\"MORTALITYBINNED\"] = full_table[\"MORTALITY\"].apply(binMortality)","a40954d2":"full_table.to_csv(\"preppedData.csv\",sep=\";\")","734188d1":"rnd.seed(574638)\n\ncountryList = full_table[\"COUNTRY\"].drop_duplicates().to_list()\n\nprint(countryList)\n\nrnd.shuffle(countryList)\n\nprint(\"\\nShuffled country list :\")\nprint(countryList)\n\ntestCountries = countryList[0:20]\nvalidationCountries = countryList[20:30]\ntrainingCountries = countryList[30:]","2ab322db":"factors = [\n    \"SCHOOL\",\n    \"WORK\",\n    \"EVENTS\",\n    \"GATHERINGS\",\n    \"TRANSPORTATION\",\n    \"ATHOME\",\n    \"NATIONAL\",\n    \"INTERNATIONAL\",\n    \"INFORMATION\",\n    \"TESTING\",\n    \"TRACING\",\n    \"DAY\",\n    \"OLD\",\n    \"YOUNG\",\n    \"URBAN\",\n    \"DENSITY\",\n    \"POPULATION\",\n    \"PHYSICIANS\",\n    \"BEDS\",\n    \"INFECTED\",\n    \"INFECTEDINCREASE\",\n    \"FATALITYINCREASE\",\n    \"TEMPERATURE\",\n    \"HUMIDITY\",\n    \"WIND\"\n]\n\nfor factorTS in factorsTS:\n    for daysInPast in range(1,31):\n        factors += [factor+\"-\"+str(daysInPast)]\n\n\ntestSet = full_table[full_table[\"COUNTRY\"].isin(testCountries)].dropna()\nX_testPropagation = testSet.loc[:,factors+[\"MORTALITY\"]].to_numpy()\nX_testMortality = testSet.loc[:,factors].to_numpy()\nY_testPropagation = testSet[\"REFFBINNED\"].to_numpy()\nY_testMortality = testSet[\"MORTALITYBINNED\"].to_numpy()\n\nvalidationSet = full_table[full_table[\"COUNTRY\"].isin(validationCountries)].dropna()\nX_valPropagation = validationSet.loc[:,factors+[\"MORTALITY\"]].to_numpy()\nX_valMortality = validationSet.loc[:,factors].to_numpy()\nY_valPropagation = validationSet[\"REFFBINNED\"].to_numpy()\nY_valMortality = validationSet[\"MORTALITYBINNED\"].to_numpy()\n\n\ntrainingSet = full_table[full_table[\"COUNTRY\"].isin(trainingCountries)].dropna()\nX_trainPropagation = trainingSet.loc[:,factors+[\"MORTALITY\"]].to_numpy()\nX_trainMortality = trainingSet.loc[:,factors].to_numpy()\nY_trainPropagation = trainingSet[\"REFFBINNED\"].to_numpy()\nY_trainMortality = trainingSet[\"MORTALITYBINNED\"].to_numpy()\n","e994ba69":"X_trainPropagation.shape","1277cdb7":"from sklearn import tree","c35af73c":"hyperTreeParamGrid = {\n    \"max_depth\" : [None,3,5,10,15,30],\n    \"min_samples_split\" : [2,3,5,7,10,20,50],\n    \"min_samples_leaf\" : [1,5,10,20,50],\n    \"max_leaf_nodes\" : [None, 2,5,10,15],\n    \"criterion\" : [\"gini\",\"entropy\"]\n}","21a97af9":"from tqdm.notebook import tqdm","2496ec10":"treeClassifiers = pd.DataFrame(columns=[\n    \"max_depth\",\n    \"min_samples_split\",\n    \"min_samples_leaf\",\n    \"max_leaf_nodes\",\n    \"criterion\",\n    \"train_score\",\n    \"val_score\"\n])\n\ncounter = tqdm(total=2100)\nfor max_depth in hyperTreeParamGrid[\"max_depth\"] :\n    for min_samples_split in hyperTreeParamGrid[\"min_samples_split\"] :\n        for min_samples_leaf in hyperTreeParamGrid[\"min_samples_leaf\"] :\n            for max_leaf_nodes in hyperTreeParamGrid[\"max_leaf_nodes\"] :\n                for criterion in hyperTreeParamGrid[\"criterion\"] :\n                    clf = tree.DecisionTreeClassifier(\n                       max_depth = max_depth,\n                       min_samples_split = min_samples_split,\n                       min_samples_leaf = min_samples_leaf,\n                       max_leaf_nodes = max_leaf_nodes, \n                       criterion = criterion\n                    )\n                    \n                    clf = clf.fit(X_trainPropagation,Y_trainPropagation)\n                    train_score  = clf.score(X_trainPropagation,Y_trainPropagation)\n                    val_score = clf.score(X_valPropagation,Y_valPropagation)\n                    \n                    treeClassifiers = treeClassifiers.append({\n                        \"max_depth\":max_depth,\n                        \"min_samples_split\":min_samples_split,\n                        \"min_samples_leaf\":min_samples_leaf,\n                        \"max_leaf_nodes\":max_leaf_nodes,\n                        \"criterion\":criterion,\n                        \"train_score\":train_score,\n                        \"val_score\":val_score\n                    }, ignore_index=True)\n                    \n                    counter.update(1)\n\ntreeClassifiers","4ea52bdf":"treeClassifiers.to_csv(\"trainedTrees.csv\")","8bbb4a8a":"topTreeParams = treeClassifiers[treeClassifiers[\"val_score\"]==treeClassifiers[\"val_score\"].max()]\n\nfirstBest = min(topTreeParams.index) \n\nclf = tree.DecisionTreeClassifier(\n    max_depth = topTreeParams.loc[firstBest,\"max_depth\"],\n    min_samples_split = topTreeParams.loc[firstBest,\"min_samples_split\"],\n    min_samples_leaf = topTreeParams.loc[firstBest,\"min_samples_leaf\"],\n    max_leaf_nodes = topTreeParams.loc[firstBest,\"max_leaf_nodes\"], \n    criterion = topTreeParams.loc[firstBest,\"criterion\"]\n)\n\nclf = clf.fit(X_trainPropagation,Y_trainPropagation)\ntrain_score = clf.score(X_trainPropagation,Y_trainPropagation)\nval_score = clf.score(X_valPropagation,Y_valPropagation)\ntest_score = clf.score(X_testPropagation,Y_testPropagation)\n\nprint(train_score)\nprint(val_score)\nprint(test_score)","9db16474":"from sklearn.ensemble import GradientBoostingClassifier","880254b0":"hyperGradientParamGrid = {\n    \"loss\":[\"deviance\"],\n    \"n_estimators\":[10,20,25,50],\n    \"min_samples_split\" : [2],\n    \"min_samples_leaf\" : [50],\n    \"max_leaf_nodes\" : [None],\n}","ed49f931":"# Bufort suggests xgboost, catboost\ngradientClassifiers = pd.DataFrame(columns=[\n    \"loss\",\n    \"n_estimators\",\n    \"min_samples_split\",\n    \"min_samples_leaf\",\n    \"max_leaf_nodes\",\n    \"train_score\",\n    \"val_score\"\n])\n\ncounter = tqdm(total=4)\nfor loss in hyperGradientParamGrid[\"loss\"] :\n    for n_estimators in hyperGradientParamGrid[\"n_estimators\"] :\n            for min_samples_split in hyperGradientParamGrid[\"min_samples_split\"] :\n                for min_samples_leaf in hyperGradientParamGrid[\"min_samples_leaf\"] :\n                    for max_leaf_nodes in hyperGradientParamGrid[\"max_leaf_nodes\"] :\n                            clf = GradientBoostingClassifier(\n                                loss=loss,\n                                n_estimators=n_estimators,\n                                min_samples_split=min_samples_split,\n                                min_samples_leaf=min_samples_leaf,\n                                max_leaf_nodes=max_leaf_nodes\n                            )\n\n                            clf = clf.fit(X_trainPropagation,Y_trainPropagation)\n                            train_score  = clf.score(X_trainPropagation,Y_trainPropagation)\n                            val_score = clf.score(X_valPropagation,Y_valPropagation)\n\n                            gradientClassifiers = gradientClassifiers.append({\n                                \"loss\":loss,\n                                \"n_estimators\":n_estimators,\n                                \"min_samples_split\":min_samples_split,\n                                \"min_samples_leaf\":min_samples_leaf,\n                                \"max_leaf_nodes\":max_leaf_nodes,\n                                \"train_score\":train_score,\n                                \"val_score\":val_score\n                            }, ignore_index=True)\n                            counter.update(1)\n                            \ngradientClassifiers","7145144e":"gradientClassifiers.to_csv(\"trainedPropagationGradBoost.csv\")","2b4fb189":"topGradParams = gradientClassifiers[gradientClassifiers[\"val_score\"]==gradientClassifiers[\"val_score\"].max()]\n\nfirstBest = min(topGradParams.index) \n\n\nclf = GradientBoostingClassifier(\n    loss=topGradParams.loc[firstBest,\"loss\"],\n    n_estimators=topGradParams.loc[firstBest,\"n_estimators\"],\n    min_samples_split=topGradParams.loc[firstBest,\"min_samples_split\"],\n    min_samples_leaf=topGradParams.loc[firstBest,\"min_samples_leaf\"],\n    max_leaf_nodes=topGradParams.loc[firstBest,\"max_leaf_nodes\"]\n)\n\nclf = clf.fit(X_trainPropagation,Y_trainPropagation)\ntrain_score = clf.score(X_trainPropagation,Y_trainPropagation)\nval_score = clf.score(X_valPropagation,Y_valPropagation)\ntest_score = clf.score(X_testPropagation,Y_testPropagation)\n\nprint(train_score)\nprint(val_score)\nprint(test_score)","1494e562":"from sklearn.neural_network import MLPClassifier\n# bufort suggests tabnet https:\/\/pypi.org\/project\/tabnet\/","25efaf13":"nFeats = X_trainPropagation.shape[1]\nprint(nFeats)","712a43a6":"inLayerN = X_trainPropagation.shape[1]\n\nhyperMLP = {\n    \"alpha\":[1e-3,1,1e4],\n    \"solver\":[\"lbfgs\",\"adam\"],\n    \"layers\":[(inLayerN),(inLayerN,inLayerN),(inLayerN,int(inLayerN\/2))]\n    \n}","51421fcd":"mlpClassifiers = pd.DataFrame(columns=[\n    \"alpha\",\n    \"solver\",\n    \"layers\",\n    \"train_score\",\n    \"val_score\"\n])\n\ncounter = tqdm(total=18)\n\nfor alpha in hyperMLP[\"alpha\"] :\n    for solver in hyperMLP[\"solver\"] :\n        for layers in hyperMLP[\"layers\"] :\n            clf = MLPClassifier(\n                alpha = alpha,\n                solver=solver,\n                hidden_layer_sizes=layers\n            )\n\n            clf = clf.fit(X_trainPropagation,Y_trainPropagation)\n            train_score  = clf.score(X_trainPropagation,Y_trainPropagation)\n            val_score = clf.score(X_valPropagation,Y_valPropagation)\n\n            mlpClassifiers = mlpClassifiers.append({\n                \"alpha\":alpha,\n                \"solver\":solver,\n                \"layers\":layers,\n                \"train_score\":train_score,\n                \"val_score\":val_score\n            }, ignore_index=True)\n\n\n            counter.update(1)\n\nmlpClassifiers","0025c835":"mlpClassifiers.to_csv(\"trainedMLP.csv\")","3cc5b6e5":"topMLPParams = mlpClassifiers[mlpClassifiers[\"val_score\"]==mlpClassifiers[\"val_score\"].max()]\n\nfirstBest = min(topMLPParams.index) \n\nclf = MLPClassifier(\n    alpha = topMLPParams.loc[firstBest,\"alpha\"],\n    solver=topMLPParams.loc[firstBest,\"solver\"],\n    hidden_layer_sizes=topMLPParams.loc[firstBest,\"layers\"]\n)\n\nclf = clf.fit(X_trainPropagation,Y_trainPropagation)\ntrain_score = clf.score(X_trainPropagation,Y_trainPropagation)\nval_score = clf.score(X_valPropagation,Y_valPropagation)\ntest_score = clf.score(X_testPropagation,Y_testPropagation)\n\nprint(train_score)\nprint(val_score)\nprint(test_score)","01758816":"from sklearn.linear_model import RidgeClassifier","87e60b69":"hyperRidge = {\n    \"alpha\":[1e-3,1e-2,1e-1,1,10,100,1000,10000,1e5,1e6,1e7]}","a5140be9":"# Bufort suggests xgboost, catboost\nridgeClassifiers = pd.DataFrame(columns=[\n    \"alpha\",\n    \"train_score\",\n    \"val_score\"\n])\n\ncounter = tqdm(total=len(hyperRidge[\"alpha\"]))\nfor alpha in hyperRidge[\"alpha\"] :\n    clf = RidgeClassifier(\n        alpha = alpha\n    )\n\n    clf = clf.fit(X_trainPropagation,Y_trainPropagation)\n    train_score  = clf.score(X_trainPropagation,Y_trainPropagation)\n    val_score = clf.score(X_valPropagation,Y_valPropagation)\n\n    ridgeClassifiers = ridgeClassifiers.append({\n        \"alpha\":alpha,\n        \"train_score\":train_score,\n        \"val_score\":val_score\n    }, ignore_index=True)\n    counter.update(1)\n\nridgeClassifiers","ee78e1c1":"ridgeClassifiers.to_csv(\"trainedRidge.csv\")","865fc188":"topRidgeParams = ridgeClassifiers[ridgeClassifiers[\"val_score\"]==ridgeClassifiers[\"val_score\"].max()]\n\nfirstBest = min(topRidgeParams.index) \n\nclf = RidgeClassifier(\n    alpha=topRidgeParams.loc[firstBest,\"alpha\"],\n)\n\nclf = clf.fit(X_trainPropagation,Y_trainPropagation)\ntrain_score = clf.score(X_trainPropagation,Y_trainPropagation)\nval_score = clf.score(X_valPropagation,Y_valPropagation)\ntest_score = clf.score(X_testPropagation,Y_testPropagation)\n\nprint(train_score)\nprint(val_score)\nprint(test_score)","44c50d27":"# Ridge Classifier","d9aff46d":"## Gradient Boosted Trees","f5c208ab":"# Load Data & Preprocess","db6b6a7b":"# Decision trees","cd878860":"# Multi Layer Peceptron","e10d8bbf":"# Ensemble Methods"}}