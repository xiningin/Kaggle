{"cell_type":{"1a0177bb":"code","454e9a23":"code","2b9ff462":"code","e4b27f09":"code","12e8cebc":"code","5e829369":"code","68c0d852":"code","c499da05":"code","93693565":"code","b6e68cb1":"code","9306531b":"code","0e5f783a":"code","6c31cea0":"markdown","efb723c7":"markdown","c711f77b":"markdown","34c5e4ba":"markdown","d865288e":"markdown","66232fc0":"markdown","169078a1":"markdown","928ff099":"markdown","552dae8a":"markdown"},"source":{"1a0177bb":"import os\nimport subprocess\n\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport pytesseract","454e9a23":"file = \"..\/input\/receipts\/2018\/us\/restaurant\/shakeshack_20181208_004.pdf\"\nimage = convert_from_path(file)[0]\nimage","2b9ff462":"width, height = image.size\nw_scale = 1000\/width\nh_scale = 1000\/height\n\nocr_df = pytesseract.image_to_data(image, output_type='data.frame') \\\n            \n\nocr_df = ocr_df.dropna() \\\n               .assign(left_scaled = ocr_df.left*w_scale,\n                       width_scaled = ocr_df.width*w_scale,\n                       top_scaled = ocr_df.top*h_scale,\n                       height_scaled = ocr_df.height*h_scale,\n                       right_scaled = lambda x: x.left_scaled + x.width_scaled,\n                       bottom_scaled = lambda x: x.top_scaled + x.height_scaled)\n\nfloat_cols = ocr_df.select_dtypes('float').columns\nocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\nocr_df[:20]","e4b27f09":"%%bash\n\ncd \/kaggle\/working\ngit clone https:\/\/github.com\/microsoft\/unilm.git\ncd unilm\/layoutlm\npip install .","12e8cebc":"os.chdir('\/kaggle\/working\/unilm\/layoutlm\/examples\/seq_labeling')\nsubprocess.run('.\/preprocess.sh')  # gets data and formats it","5e829369":"%%bash\n\nls data\necho \"\"\ncat data\/labels.txt","68c0d852":"! python run_seq_labeling.py  --data_dir data \\\n                            --model_type layoutlm \\\n                            --model_name_or_path \/kaggle\/input\/layoutlm\/layoutlm-base-uncased \\\n                            --do_lower_case \\\n                            --max_seq_length 512 \\\n                            --do_train \\\n                            --num_train_epochs 100 \\\n                            --logging_steps 10 \\\n                            --save_steps -1 \\\n                            --output_dir output \\\n                            --labels data\/labels.txt \\\n                            --per_gpu_train_batch_size 16 \\\n                            --per_gpu_eval_batch_size 16 \\\n                            --seed 12","c499da05":"%%bash\n\n# cut test files to a single image\nmkdir .\/data\/single\n(sed -n '4074,4146p' <  data\/test.txt) > data\/single\/test.txt\n(sed -n '4074,4146p' <  data\/test_box.txt) > data\/single\/test_box.txt\n(sed -n '4074,4146p' <  data\/test_image.txt) > data\/single\/test_image.txt\n\nmkdir .\/output\/single","93693565":"! python run_seq_labeling.py  --data_dir data\/single \\\n                              --model_type layoutlm \\\n                              --model_name_or_path output \\\n                              --do_lower_case \\\n                              --max_seq_length 512 \\\n                              --output_dir output \\\n                              --labels data\/labels.txt \\\n                              --per_gpu_eval_batch_size 16 \\\n                              --do_predict \\\n                              --seed 12","b6e68cb1":"from glob import glob\nfrom IPython.display import Image, display\n\n\n# for img in glob('data\/testing_data\/images\/*.png'):\n#     print(img)\n#     display(Image(img))\n\ndisplay(Image('data\/testing_data\/images\/87125460.png'))","9306531b":"!cat output\/test_predictions.txt","0e5f783a":"!rm -rf \/kaggle\/working\/unilm","6c31cea0":"## Editor's Note, August 2021\nThere has been a lot of progress with LayoutLM in the past year. Most notably it has been integrated into the <b>transformers<\/b> library. You can find documentation at https:\/\/huggingface.co\/transformers\/v4.3.3\/model_doc\/layoutlm.html. I recommend following the examples there instead of the code in this notebook.\n\n<br>\n\n<img src=\"https:\/\/nanonets.com\/blog\/content\/images\/2020\/06\/Screen-Shot-2020-06-01-at-11.42.39-AM.png\" alt=\"receipt\" width=\"450\" height=\"450\" \/>\n\n\n<br>\nThis notebook shows how to find and label key fields of receipts and other similar documents such as invoices or request forms. Documents like these have a visual arrangement of tokens in addition to the token content itself. The arrangement presents an opportunity to use the position of tokens on a document as an additional input.\n\n<br> \n[LayoutLM](https:\/\/github.com\/microsoft\/unilm\/tree\/master\/layoutlm) gives us a highly effective way to jointly model the interaction between text and layout information for scanned documents. It was developed by Microsoft and has recently been integrated into the [Transformers](https:\/\/huggingface.co\/transformers) library. The integration is a great development in that it simplifies using LayoutLM by leaps and bounds. \n\nI'm going to use the original Microsoft repo to show more detail. Model weights and files are not in the repo, just links. You can find them in this [LayoutLM dataset](https:\/\/www.kaggle.com\/jpmiller\/layoutlm) as well.\n\nNote: The first part of the pipeline, getting OCR tokens and coordinates, will use one of the receipts from the attached dataset. The second part, training and inference, will be done on another dataset. This is because we need labeled data including coordinates.","efb723c7":"Training runs with the following command and options. The [attached dataset](https:\/\/www.kaggle.com\/jpmiller\/layoutlm) has pretrained weights and model files.","c711f77b":"# Inference\n\nInference, or prediction, is straightforward with the command line (once all the prep is done). Make sure to use the tuned model files produced by training.","34c5e4ba":"# OCR\n\nBefore applying the model for inference we need to get a receipt into digital form. The pytesseract library forms a wrapper around Tesseract which is an excellent tool for getting our tokens and coordinates. It reads images so first we have to get our pdf to image form. Conversion is easy with pdf2image.","d865288e":"# Model Tuning\n\nThe LayoutLM model has been pretrained on 11M documents using BERT uncased models (base and large) along with the token coordinates. We can use the pretrained model to label parts of other document sets with only minimal tuning.\n\nWe'll switch now to an annotated dataset as mentioned above. We can use the [FUNSD](https:\/\/guillaumejaume.github.io\/FUNSD\/) dataset to tune the model. Below are two examples of FUNSD documents.\n\n<img src=\"http:\/\/guillaumejaume.github.io\/FUNSD\/img\/two_forms.png\" width=\"600\">\n\nThe preprocessing script ran above gets the data in the right format for modeling. The key dtaa files for training are train.txt, train_box.txt, train_image.txt, and labels.txt. Here are the files and the labels. Each token will receive one of the named labels (header, question, or answer) or 'O' if it is something else.","66232fc0":"Here's a document from the test set followed by the labels from the model. You can see it does a pretty good job of identifying \"questions\" and \"answers\" on this form.","169078a1":"Mmmmm, bacon cheese fries.... The code below feeds the image to tesseract and scales the coordinates to 1000x1000. This is my favorite part.","928ff099":"# Model setup\n\nThere are a few things to do to prepare the environment. I won't set it up for mixed precision training to avoid wrestling with the existing Kaggle environment. For clean installs you can follow directions at the repo and enjoy faster training with the --fp16 option.","552dae8a":"Have fun with the dataset!"}}