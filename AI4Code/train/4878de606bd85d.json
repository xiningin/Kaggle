{"cell_type":{"3a83e370":"code","9c07c5fa":"code","4fe34b48":"code","e883220e":"code","4fc448b2":"code","88adbea0":"code","0b4e4ec7":"code","26a29ce7":"code","ec6819bb":"code","5940fcd2":"code","6b97a5c2":"code","0ba5cfce":"code","150dfa0b":"code","75bb6f27":"code","edd5b2cb":"code","cecd9119":"code","d229eb8a":"code","755338d3":"code","ca004bee":"code","cb84c0eb":"code","9f6d3ed6":"code","58ef4731":"markdown","d722baa2":"markdown","efdc2ef3":"markdown","13062a82":"markdown","e27b7108":"markdown","24f75c8e":"markdown"},"source":{"3a83e370":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9c07c5fa":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\n\nfrom keras.layers import Dense,LSTM\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer","4fe34b48":"train = pd.read_csv('\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', sep=\"\\t\")","e883220e":"train.head()","4fc448b2":"test.head()","88adbea0":"print (train[\"Phrase\"][0])\nprint (train[\"Phrase\"][1])\nprint (train.shape)","0b4e4ec7":"\n\nimport seaborn as sns\n\nsns.countplot(x='Sentiment', data=train)","26a29ce7":"\ntokenizer = Tokenizer()\n\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\ntokenizer.fit_on_texts(full_text)\n\nX_train = tokenizer.texts_to_sequences(train['Phrase'].values)\nX_test = tokenizer.texts_to_sequences(test['Phrase'].values)","ec6819bb":"max_len = 50\nX_train = pad_sequences(X_train, maxlen = max_len)\nX_test = pad_sequences(X_test, maxlen = max_len)\nlen(X_train)","5940fcd2":"X_train = np.array(X_train)\nX_test = np.array(X_test)","6b97a5c2":"X_train.shape","0ba5cfce":"y = train['Sentiment']","150dfa0b":"vocab_size = len(tokenizer.word_index) + 1\nvocab_size","75bb6f27":"# TEST\nvocabulary_size = len(tokenizer.word_counts)\nvocabulary_size = vocabulary_size + 1\nseq_len = X_train.shape[1]\nmodel_test = Sequential()\nmodel_test.add(Embedding(input_dim = vocabulary_size, output_dim = 2, input_length=seq_len))\nmodel_test.compile('rmsprop', 'mse')\noutput_array = model_test.predict(X_train)\nprint (output_array.shape)\nout1 = pd.DataFrame(output_array[0])\nout1.tail()","edd5b2cb":"vocabulary_size = len(tokenizer.word_counts)\nvocabulary_size = vocabulary_size + 1\nseq_len = X_train.shape[1]\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, 25, input_length=seq_len))\nmodel.add(LSTM(150, return_sequences=True)) # to stack LSTM we need return seq \nmodel.add(LSTM(150))\nmodel.add(Dense(150, activation='relu'))\n\nmodel.add(Dense(5, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","cecd9119":"from keras.utils import to_categorical\ny = to_categorical(y, num_classes=5)\n\ny.shape","d229eb8a":"# fit model\nmodel.fit(X_train, y, batch_size=256, epochs=10,verbose=1)","755338d3":"sub = pd.read_csv('\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv')\nsub.head()","ca004bee":"pred = model.predict(X_test, batch_size = 256, verbose = 1)\npred[0]","cb84c0eb":"import matplotlib.pyplot as plt\npredictions = np.round(np.argmax(pred, axis=1)).astype(int)\nplt.hist(predictions, normed=False, bins=5)","9f6d3ed6":"sub['Sentiment'] = predictions\nsub.to_csv(\"submission.csv\", index=False)","58ef4731":"### continue ...","d722baa2":"# Train","efdc2ef3":"# X and y","13062a82":"* LSTM layer take a 3D tensor with shape (batch_size, timesteps, input_dim).\n* in this case the correct shape is generated from the embedding layer\n* in this case the timesteps is 50 words\n\nHow embedding works in Keras\ninput_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\noutput_dim: int >= 0. Dimension of the dense embedding. indicates the size of the embedding vectors\neach input integer is used as the index to access a table that contains all posible vectors. That is the reason why it needs to specify the size of the vocabulary as the first argument (so the table can be initialized).Once the network has been trained, we can get the weights of the embedding layer,and can be thought as the table used to map integers to embedding vectors.the underlying automatic differentiation engines (e.g., Tensorflow or Theano) manage to optimize these vectors associated to each input integer just like any other parameter.\n\nit also possible to use pretained embedding https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html","e27b7108":"# Model","24f75c8e":"# Submission"}}