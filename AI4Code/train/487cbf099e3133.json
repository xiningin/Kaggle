{"cell_type":{"b725d1ed":"code","80dbcb6f":"code","52523600":"code","987d7f1f":"code","2b17ca4d":"code","2a4eb4ea":"code","19f765c6":"code","93122ad8":"code","738445a5":"code","ac8c9c9a":"code","ec55f02e":"code","e1d700ab":"code","8975f2db":"code","48bcfa9b":"code","eacd378c":"code","74a2376a":"code","324b7b40":"code","ca489f73":"code","28cf88be":"code","4f4493e5":"code","7455199c":"code","2f7ba5ed":"code","a2c93a67":"code","76dc3b3d":"code","4bd84791":"code","298fa324":"code","07caff18":"code","f86e079e":"code","290d1ae7":"code","b71e6935":"code","3dc19de8":"code","425b9b88":"code","8007333e":"code","4ad7874a":"code","d97dfd0f":"code","b600b872":"code","8c67557e":"code","51415fa5":"code","d451603e":"code","731d0c74":"code","22f9cd66":"code","7090e86d":"code","e5c2f6d4":"code","a89b16f8":"code","d4ca4dfd":"code","32914b05":"code","8337d5dd":"code","0250e76b":"code","f8d4872c":"code","70b2ff1a":"code","8151d700":"code","efd1e918":"code","4262bfb0":"code","d27ae8cb":"code","02da2ed5":"code","8edd3880":"code","699e660d":"code","c2916f30":"code","e29ce289":"code","25d8cb94":"code","e99c1e23":"code","fd347619":"code","777daf90":"code","0a4e330c":"code","e5ba2dd4":"code","04157111":"code","e0e08a74":"code","becf30e2":"code","8a06cef3":"code","e3b8edeb":"code","9e521d70":"code","07f8a78a":"code","626b7bf9":"code","d85d722f":"code","3cbea7db":"code","b6fb8efe":"code","cc4939ac":"code","5cee6ccc":"code","23cb38f2":"code","6d1bd1ad":"code","87e1c72d":"code","24cfc187":"code","262a881a":"code","3ae13448":"code","9021d255":"code","3cbbf04a":"code","9ac330c8":"code","383704ad":"code","65debce6":"code","de519d38":"code","c9f0f7fd":"code","80f69c94":"code","2ebd9c98":"code","44916d9f":"code","486299f1":"code","97923749":"code","326fccde":"code","163a9058":"code","99b15cb2":"code","19fb505b":"code","bc229a8f":"markdown","c36b1ee5":"markdown","ac1a6a1a":"markdown","779ff390":"markdown","aa47e0e5":"markdown","0ad12283":"markdown","ee102cf4":"markdown","78a4c056":"markdown","ee925a49":"markdown","d8a325ab":"markdown","8627f209":"markdown","c8fd2c44":"markdown","d065972c":"markdown","cf28d87c":"markdown","2812ac53":"markdown","729d551f":"markdown","301c6a79":"markdown","11e3c1af":"markdown","511f073a":"markdown","75897f34":"markdown","471302fa":"markdown","e2d1ec8e":"markdown","ed26fc3b":"markdown","9a6515fe":"markdown","846730da":"markdown"},"source":{"b725d1ed":"import numpy as np \nimport pandas as pd\n#\uc2dc\uac01\ud654\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings \nwarnings.filterwarnings(action='ignore')\n\nimport os","80dbcb6f":"PATH = '..\/input\/kakr-4th-competition\/'\nos.listdir(PATH)","52523600":"# csv\uc6a9\ub7c9 \ud30c\uc77c\uc740 \uc801\uc9c0\ub9cc dataframe\ubcc0\ud658 \uc2dc, \uc22b\uc790\ub294 int64(2^64),float64 \ucd5c\ub300\uce58\ub85c \ubc14\ub00c\uba70 \uc6a9\ub7c9\uc774 \uc99d\uac00 \n# ex. dtype\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 : age - int 64\n# int16, int32, int64, bool, float32...\ntrain_dtype = {\n    \"age\":np.int32\n}\n\ntrain = pd.read_csv(os.path.join(PATH,'train.csv'), dtype=train_dtype)\ntest = pd.read_csv(os.path.join(PATH,'test.csv'))\n# sample_submission = pd.read_csv(os.path.join(PATH,'sample_submission.csv'))\n# test = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/test.csv')","987d7f1f":"label = train['income']\ndel train['income']","2b17ca4d":"label","2a4eb4ea":"# \ub77c\ubca8 \uac12 \uc778\ucf54\ub529\nlabel = label.map(lambda x: 1 if x == '>50K' else 0)","19f765c6":"label","93122ad8":"del train['id']\ndel test['id']","738445a5":"tmp_train = train.copy()\ntmp_test  = test.copy()","ac8c9c9a":"tmp_train.head() \n# tmp_train.sample(10)","ec55f02e":"# age int64 > int32 \n# null\uc774 \uc788\ub294 \uceec\ub7fc\uc758 type\uc740 float\ntmp_train.info()","e1d700ab":"msno.matrix(tmp_train)","8975f2db":"# \uc218\uce58\ub9cc \ucd9c\ub825. object \uc81c\uc678\n# \ucd5c\ub300 \ucd5c\uc18c \ucc28\uc774 \ud070 \uac83 \ud655\uc778\ntmp_train.describe()","48bcfa9b":"tmp_test.head()","eacd378c":"tmp_train.capital_gain == 99999","74a2376a":"# capital gain\uc774 99999\uc778 \uac83\uc744 \ucc3e\uc544\ub77c\n# train.loc[\uc870\uac74\uc774 true\uc77c \ub54c,:]\ntmp_train.loc[tmp_train.capital_gain == 99999,:]\n# train.loc[train.capital_gain == 99999,'age']","324b7b40":"tmp_train[tmp_train.apply(lambda x: \"?\" in list(x), axis=1)]","ca489f73":"tmp_train.occupation.value_counts()","28cf88be":"has_na_columns = ['workclass', 'occupation', 'native_country']","4f4493e5":"train['workclass'] == \"?\"","7455199c":"tmp_train[has_na_columns] == '?'","2f7ba5ed":"(tmp_train[has_na_columns] == '?').sum()","a2c93a67":"tmp_train['workclass'].mode()\n# tmp_train['workclass'].mode()[0]\n# tmp_train['age'].mean()","76dc3b3d":"for c in has_na_columns:\n    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n    tmp_test.loc[test[c]   == '?', c] = test[c].mode()[0]","4bd84791":"(tmp_train[has_na_columns] == '?').sum()","298fa324":"tmp_train['capital_gain'].plot.hist()","07caff18":"tmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_gain']  = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_gain'].plot.hist()","f86e079e":"# 0\ubcf4\ub2e4 \ud074 \uacbd\uc6b0\ntmp_train[tmp_train['log_capital_gain']>0]['log_capital_gain'].plot.hist()","290d1ae7":"train['capital_loss'].plot.hist()","b71e6935":"tmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_loss'].plot.hist()","3dc19de8":"tmp_train = tmp_train.drop(columns=['capital_loss', 'capital_gain'])\ntmp_test  = tmp_test.drop(columns=['capital_loss', 'capital_gain'])","425b9b88":"tmp_train.head()","8007333e":"from sklearn.model_selection import train_test_split\n\ntmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label, \n                                                          test_size=0.3,\n                                                          random_state=2020,\n                                                          shuffle=True,\n                                                          stratify=label)","4ad7874a":"tmp_train.head()","d97dfd0f":"# tmp_train = tmp_train.reset_index","b600b872":"# \uc778\ub371\uc2a4 \ucd08\uae30\ud654 \n# drop=True \uae30\uc874 index \uc0ad\uc81c\ntmp_train = tmp_train.reset_index(drop=True)\ntmp_valid = tmp_valid.reset_index(drop=True)\ntmp_test  = tmp_test.reset_index(drop=True)","8c67557e":"tmp_train.head()","51415fa5":"tmp_train.dtypes","d451603e":"tmp_train.dtypes[0] #age","731d0c74":"tmp_train.dtypes[1] #workclass","22f9cd66":"tmp_train.dtypes.index","7090e86d":"tmp_train.dtypes.index[0]","e5c2f6d4":"tmp_train.columns[0]","a89b16f8":"tmp_train.dtypes[1] == 'O'\n# tmp_train.dtypes[1] == '1' #\uc65c \uc5d0\ub7ec\uac00 \ub0a0\uae4c ","d4ca4dfd":"cat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] \nnum_columns = [c for c in tmp_train.dtypes.index if c not in cat_columns]\n\nprint('\ubc94\uc8fc\ud615 \ubcc0\uc218: \\n{}\\n\\n \uc218\uce58\ud615 \ubcc0\uc218: \\n{}\\n'.format(cat_columns, num_columns))","32914b05":"# \ubaa8\ub378\uc774 \ud559\uc2b5\ud558\ub294 \uae30\uc900\uc740 train\uc774\uc5b4\uc57c \ud55c\ub2e4(\ub370\uc774\ud130 \ub098\ub208 \ud6c4 \uc0ac\uc6a9\ud574\uc57c unseen\uc131\ub9bd)\n# \ud45c\uc900\ud3b8\ucc28\ub294 1, \ud3c9\uade0\uc740 0\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\n# tmp_train[num_columns] = scaler.fit(tmp_train[num_columns])\n# scaler.transform(tmp_train[num_columns])\ntmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\ntmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])","8337d5dd":"# train\uc758 \uacbd\uc6b0 \ud45c\uc900\ud3b8\ucc28\ub294 1, \ud3c9\uade0\uc740 0 \uc774\ub2e4. train\uae30\uc900\uc73c\ub85c \ud588\uae30\uc5d0 valid, test\ub294 \ub2e4\ub974\uac8c \ub098\uc634\ntmp_train.describe()","0250e76b":"tmp_valid.describe()","f8d4872c":"tmp_test.describe()","70b2ff1a":"from sklearn.preprocessing import OneHotEncoder\n\n# train, valid\uc5d0 \uc5c6\ub294 \ubc94\uc8fc\uac00 test\uc5d0 \uc788\uc744 \uc218\ub3c4 \uc788\ub2e4. \uadf8\ub7ec\uba74 \ub098\uc911\uc5d0 \uc5d0\ub7ec\ub098\ubbc0\ub85c \uc5ec\uae30\uc11c \ud569\uccd0\uc11c \ubcc0\ud658. \ntmp_all = pd.concat([tmp_train, tmp_valid, tmp_test])\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(tmp_all[cat_columns]) #cat_columns \uc744 \ubc00\uc5b4\ub123\uc5c8\ub2e4","8151d700":"tmp_train.shape, tmp_valid.shape, tmp_test.shape, tmp_all.shape","efd1e918":"# \ubca1\ud130\ub85c \ud45c\ud604\ub41c \ubc94\uc8fc\uc758 \uc21c\uc11c\nohe.categories_","4262bfb0":"ohe_columns = list()\nfor lst in ohe.categories_:\n    ohe_columns += lst.tolist()","d27ae8cb":"ohe_columns","02da2ed5":"# len(ohe_columns)\uc758 \uc21c\uc11c\uc640 ohe.transform(tmp_train[cat_columns]).shape \uc21c\uc11c\uac00 \uac19\ub2e4. \nlen(ohe_columns)","8edd3880":"tmp_all[cat_columns]","699e660d":"ohe.transform(tmp_train[cat_columns]).shape","c2916f30":"new_train_cat = pd.DataFrame(ohe.transform(tmp_train[cat_columns]), columns=ohe_columns)\nnew_valid_cat = pd.DataFrame(ohe.transform(tmp_valid[cat_columns]), columns=ohe_columns)\nnew_test_cat  = pd.DataFrame(ohe.transform(tmp_test[cat_columns]), columns=ohe_columns)","e29ce289":"new_train_cat.head()","25d8cb94":"cat_columns","e99c1e23":"# 18234 axis=0\n# 14 axis=1\ntmp_train.shape","fd347619":"tmp_train = pd.concat([tmp_train, new_train_cat], axis=1)\ntmp_valid = pd.concat([tmp_valid, new_valid_cat], axis=1)\ntmp_test = pd.concat([tmp_test, new_test_cat], axis=1)\n\n# \uae30\uc874 \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc81c\uac70\ntmp_train = tmp_train.drop(columns=cat_columns)\ntmp_valid = tmp_valid.drop(columns=cat_columns)\ntmp_test = tmp_test.drop(columns=cat_columns)","777daf90":"tmp_train.head()","0a4e330c":"tmp_y_train = y_train\ntmp_y_valid = y_valid","e5ba2dd4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import f1_score","04157111":"tmp_train","e0e08a74":"tmp_y_train","becf30e2":"lr = LogisticRegression()\n\nlr.fit(tmp_train, tmp_y_train)\n\ny_pred = lr.predict(tmp_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","8a06cef3":"svc = SVC()\n\nsvc.fit(tmp_train, tmp_y_train)\n\ny_pred = svc.predict(tmp_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","e3b8edeb":"rf = RandomForestClassifier()\n\nrf.fit(tmp_train, tmp_y_train)\n\ny_pred = rf.predict(tmp_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","9e521d70":"xgb = XGBClassifier(tree_method='gpu_hist')\n\nxgb.fit(tmp_train, tmp_y_train)\n\ny_pred = xgb.predict(tmp_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","07f8a78a":"lgb = LGBMClassifier(tree_method='gpu_hist')\n\nlgb.fit(tmp_train, tmp_y_train)\n\ny_pred = lgb.predict(tmp_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","626b7bf9":"def preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    for c in has_na_columns:\n        tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n        tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n        tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n    \n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    #numpy\ub85c \ubc18\ud658\n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","d85d722f":"# f1 score\ndef xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = (y > threshold).astype(int) \n    return 'f1', f1_score(t, y_bin, average='micro')","3cbea7db":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-pyfrom sklearn.model_selection \n\nfrom sklearn.model_selection import StratifiedKFold\n\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)","b6fb8efe":"# Generate 'indices' to split data into training and test set.\nskf.split?","cc4939ac":"val_scores = list()\noof_pred = np.zeros((test.shape[0],))\n\n# StratifiedKFold\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)): #index\ub97c \uaebc\ub0b8\ub2e4. \n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    # n_estimators=2000000 tree\ub97c \uba87\uac1c \ub9cc\ub4e4 \uac83\uc778\uc9c0, \uc124\uc815\ud558\uba74 \uc54c\uc544\uc11c \uaebc\uc9d0\n#     clf = XGBClassifier(n_estimators=2000000, tree_method='gpu_hist')\n    clf = XGBClassifier(tree_method='gpu_hist')\n\n    # \ubaa8\ub378 \ud559\uc2b5\n    # xgb_f1\ud568\uc218\n    # early_stopping_rounds=100 100\ubc88 \ub3d9\uc548 \uc131\ub2a5\uc774 \ub098\uc544\uc9c0\uc9c0 \uc54a\uc73c\uba74 stop\n    # verbose 100\ubc88\uc5d0 1\ubc88 \ucd9c\ub825\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","5cee6ccc":"(test.shape[0],)","23cb38f2":"np.zeros((test.shape[0], ))","6d1bd1ad":"val_scores = list()\noof_pred = np.zeros((test.shape[0], )) #\ud1b5 \ub9cc\ub4e4\uae30#######\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[: , 1] \/ n_splits  #######\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","87e1c72d":"oof_pred","24cfc187":"oof_pred.shape","262a881a":"(oof_pred > 0.5).astype(int)","3ae13448":"int(True), int(False)","9021d255":"bool(1), bool(0)","3cbbf04a":"bool(5), bool(0)","9ac330c8":"# Logistic Regression\nval_scores = list()\noof_pred = np.zeros((test.shape[0], )) #######\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = LogisticRegression() #######\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n#             eval_set = [[x_valid, y_valid]], \n#             eval_metric = 'logloss', #######       \n#             early_stopping_rounds = 100,\n#             verbose = 100,  \n           )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[: , 1] \/ n_splits  #######\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","383704ad":"# LightGBM\nval_scores = list()\noof_pred = np.zeros((test.shape[0], )) #######\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = LGBMClassifier(tree_method='gpu_hist') #######\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = 'logloss', #######       \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[: , 1] \/ n_splits  #######\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","65debce6":"val_scores = list()\n\n# np.zeros((train.shape[0], )) vector\n# np.zeros((train.shape[0], 1)) matrix\nnew_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(4)] #\uc138\ub85c\ub85c \ubd99\ub294\ub2e4\nnew_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(4)]\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    print(f\"Fold {i} Start\")\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758 ########\n    clfs = [LogisticRegression(), \n            RandomForestClassifier(), \n            XGBClassifier(tree_method='gpu_hist'), \n            LGBMClassifier(tree_method='gpu_hist')]\n    \n    for model_idx, clf in enumerate(clfs):\n        clf.fit(x_train, y_train)\n        \n        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1) #reshape(-1, 1)\ubca1\ud130\ub85c \ub098\uc624\uae30\uc5d0 \ucc28\uc6d0\uc744 \ub9de\ucdb0\uc90c\n        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) \/ n_splits","de519d38":"new_x_train_list","c9f0f7fd":"new_x_test_list","80f69c94":"new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None)\nnew_label = label\nnew_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n\nnew_train.shape, new_label.shape, new_test.shape","2ebd9c98":"# XGBClassifier\nval_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","44916d9f":"# LGBMClassifier\nval_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = LGBMClassifier(tree_method='gpu_hist') #######\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = 'logloss', #######       \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","486299f1":"import os\nos.listdir(\"\/kaggle\/input\/kakr-4th-competition\/\")","97923749":"submit = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv\")","326fccde":"submit.head()","163a9058":"submit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)","99b15cb2":"submit.head()","19fb505b":"submit.to_csv('stacking_submit.csv', index=False)","bc229a8f":"#### 6) \uc778\ucf54\ub529\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc218\uce58\ud615 \ubcc0\uc218\ub85c \uc778\ucf54\ub529 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubc94\uc8fc\ud615 \ubcc0\uc218\uc5d0\ub294 Onehot Encoding\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.","c36b1ee5":"#### 5) \ub370\uc774\ud130 \ucabc\uac1c\uae30\n##### 1. Train, Valid, Test Set\n* Train Data : \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294\ub370 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \uc54c\uace0 \uc788\ub294 \ud559\uc2b5\ud560 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Valid Data : \ud559\uc2b5\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \ud559\uc2b5\ud558\uc9c0 \uc54a\uc744 \ub370\uc774\ud130, \ubaa8\ub378 \uac80\uc99d\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Test Data : \ud559\uc2b5\ud55c \ubaa8\ub378\ub85c \uc608\uce21\ud560 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uc608\uce21\ud560 \ub370\uc774\ud130, \ubbf8\ub798 \ub370\uc774\ud130)","ac1a6a1a":"#### 6) \uc2a4\ucf00\uc77c\ub9c1\nScikit-learn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \uc788\ub294 Standard Scaler\ub97c \uc0ac\uc6a9\ud574\uc11c \uc218\uce58\ud615 \ubcc0\uc218\ub4e4\uc758 \ud45c\uc900\ud654\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.","779ff390":"##### capital_gain","aa47e0e5":"## \ub370\uc774\ud130 \ud655\uc778\n* id\n* age : \ub098\uc774\n* workclass : \uace0\uc6a9 \ud615\ud0dc\n* fnlwgt : \uc0ac\ub78c \ub300\ud45c\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uac00\uc911\uce58 (final weight\uc758 \uc57d\uc790)\n* education : \uad50\uc721 \uc218\uc900\n* education_num : \uad50\uc721 \uc218\uc900 \uc218\uce58\n* marital_status: \uacb0\ud63c \uc0c1\ud0dc\n* occupation : \uc5c5\uc885\n* relationship : \uac00\uc871 \uad00\uacc4\n* race : \uc778\uc885\n* sex : \uc131\ubcc4\n* capital_gain : \uc591\ub3c4 \uc18c\ub4dd\n* capital_loss : \uc591\ub3c4 \uc190\uc2e4\n* hours_per_week : \uc8fc\ub2f9 \uadfc\ubb34 \uc2dc\uac04\n* native_country : \uad6d\uc801\n* income : \uc218\uc775 (\uc608\uce21\ud574\uc57c \ud558\ub294 \uac12)\n    * >50K : 1\n    * <=50K : 0","0ad12283":"#### 4) Log \ubcc0\ud658\ncapital_gain \ubcc0\uc218\uc640 capital_loss \ubcc0\uc218\uc758 \ubd84\ud3ec\uac00 \ud55c\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \ud615\ud0dc\uc774\ubbc0\ub85c Log \ubcc0\ud658\uc744 \ud1b5\ud574 \ubd84\ud3ec\uc758 \ud615\ud0dc\ub97c \uc870\uc815\ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.","ee102cf4":"#### 2) \uc11c\ud3ec\ud2b8 \ubca1\ud130 \uba38\uc2e0(rbf \ucee4\ub110)","78a4c056":"##### \ub370\uc774\ud130 \ucabc\uac1c\uae30, Train -> (Train, Valid)\n- train_test_split \ud30c\ub77c\ubbf8\ud130 \n    - test_size  (float): Valid(test)\uc758 \ud06c\uae30\uc758 \ube44\uc728\uc744 \uc9c0\uc815\n    - random_state (int): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \ub0b4\ubd80\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ub09c\uc218 \uac12 (\ud574\ub2f9 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub9e4\ubc88 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4.)\n    - shuffle     (bool): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \uc11e\uc744\uc9c0 \uc720\ubb34\n    - stratify   (array): Stratify\ub780, \ucabc\uac1c\uae30 \uc774\uc804\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc744 \ucabc\uac1c\uace0 \ub098\uc11c\ub3c4 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc124\uc815\ud574\uc57c\ud558\ub294 \uac12\uc785\ub2c8\ub2e4. \ud074\ub798\uc2a4 \ub77c\ubca8\uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.\n        - ex) \uc6d0\ubcf8 Train \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc774 (7:3) \uc774\uc5c8\ub2e4\uba74, \ucabc\uac1c\uc5b4\uc9c4 Train, Valid(test) \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\ub3c4 (7:3)\uc774 \ub429\ub2c8\ub2e4. \ub2f9\uc5f0\ud788 \ubd84\ub958 \ub370\uc774\ud130\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","ee925a49":"### 6. \uacb0\uacfc \ub9cc\ub4e4\uae30","d8a325ab":"### 4_1. OOF \uc559\uc0c1\ube14 \uc2e4\uc2b5 (Logistic Regression, LightGBM, 20\ubd84\uac04)","8627f209":"#### 1) CSV \ud30c\uc77c \ubd88\ub7ec\uc624\uae30","c8fd2c44":"### 2. Scikit-Learn \ubd84\ub958 \ubaa8\ub378 \uc0ac\uc6a9\ud574\ubcf4\uae30\nScikit-Learn\uc758 \uae30\ubcf8 \ubd84\ub958 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\uac01 \ubaa8\ub378\uc758 \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc740 \ub300\ud68c \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc778 f1_score\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","d065972c":"### 4. OOF(Out-Of-Fold) \uc559\uc0c1\ube14\nk-Fold\ub97c \ud65c\uc6a9\ud574\uc11c \ubaa8\ub378 \uac80\uc99d \ubc0f \uac01 \ud3f4\ub4dc\uc758 \uacb0\uacfc\ub97c \uc559\uc0c1\ube14\ud558\ub294 OOF \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4.","cf28d87c":"#### 4) XGBoost","2812ac53":"#### 1) \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378","729d551f":"\ud29c\ub2dd: nni library, hyper opt, keras  \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd \ucc3e\uc544\ubcf4\uae30 ","301c6a79":"#### 5) LightGBM","11e3c1af":"### 3. k-Fold Cross Validation\n\uba3c\uc800 1. \uc5d0\uc11c \uc815\ub9ac\ud55c \uc804\ucc98\ub9ac \ud504\ub85c\uc138\uc2a4\ub97c \ud558\ub098\uc758 \ud568\uc218\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.","511f073a":"##### capital loss","75897f34":"### 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac","471302fa":"#### 2) 2 Stage Meta Model \ud559\uc2b5\nnew_train, new_test\uc5d0 \ub4e4\uc5b4\uc788\ub294 \ubcc0\uc218\ub294 \ubaa8\ub450 \uc218\uce58\ud615 \ubcc0\uc218\uc774\ubbc0\ub85c Standard Scaling\ub9cc \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.<br>\n\uc0c8\ub85c \uc0dd\uc131\ud55c \ub370\uc774\ud130 new_train, new_test \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 2 Stage Meta Model\uc744 \ud559\uc2b5\ud558\uace0 \uacb0\uacfc\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.","e2d1ec8e":"ID \uceec\ub7fc\uc740 \ud589\uc758 \uc2dd\ubcc4\uc790\ub85c \ud544\uc694 \uc5c6\ub294 \uceec\ub7fc\uc774\ubbc0\ub85c \uc0ad\uc81c\ud558\uaca0\uc2b5\ub2c8\ub2e4. ","ed26fc3b":"### 5. Stacking \uc559\uc0c1\ube14\n2 stage \uc559\uc0c1\ube14\uc778 Stacking \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4. Stacking \uc559\uc0c1\ube14\uc740 \uc218\uc2ed\uac1c\uc758 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubaa8\uc544 2 stage \ubaa8\ub378\ub85c \ud559\uc2b5 \ud6c4 \uacb0\uacfc\ub97c \ub0b4\ub294 \uc559\uc0c1\ube14 \ubc29\uc2dd\uc785\ub2c8\ub2e4.\n\n#### 1) 1 stage \uacb0\uacfc \ubaa8\uc73c\uae30\nStacking \uc559\uc0c1\ube14\uc744 \uc9c4\ud589\ud560 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc(train, test)\ub97c \ubaa8\uc74d\ub2c8\ub2e4. ","9a6515fe":"#### 3) \uacb0\uce21\uce58 \ucc98\ub9ac\n\uc774\uc804 \ud0dc\uc9c4\ub2d8 \uac15\uc758\uc5d0\uc11c 'workclass', 'occupation', 'native_country' \uceec\ub7fc\uc5d0 \uacb0\uce21\uce58\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. <br>\n\uc77c\ubc18\uc801\uc778 \uacb0\uce21\uce58\uc640 \ub2e4\ub974\uac8c '?'\ub85c \ud45c\ud604\ub418\uc5b4\uc788\ub294 \uac12\ub4e4\uc740 \ud574\ub2f9 \uceec\ub7fc\uc758 \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\n##### \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uacbd\uc6b0 \uac00\uc7a5 \uac04\ub2e8\ud558\uac8c \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc9c0\ub9cc, \ub2e4\ub978 \uceec\ub7fc\uc744 \ud544\ud130\ub9c1\ud574\uc11c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ex) education_num \ub4f1\n\n#### sklearn iterativeimputer","846730da":"#### 3) \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8"}}