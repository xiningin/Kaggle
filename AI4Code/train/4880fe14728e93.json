{"cell_type":{"20ad4f0f":"code","15709605":"code","a34b1a17":"code","5f256c28":"code","05ca64d2":"code","ea3e1c59":"code","f90d80f8":"code","4e8a8e50":"code","a9c76fb0":"code","40daf997":"code","0f8fae20":"code","11aa1a6b":"code","105a9795":"code","734a2852":"code","e361ad66":"code","1b390e8d":"code","4873165c":"code","54acf20c":"code","9eb25382":"code","a3fdb659":"code","fb8d59b0":"code","17e23a65":"code","f1d034c0":"markdown","1c63067d":"markdown","b343a559":"markdown","ede13d01":"markdown","39d8bd28":"markdown","8b3c5767":"markdown","363b7772":"markdown","11027b12":"markdown","af92bf2c":"markdown","a4ef3988":"markdown","7b83044d":"markdown","1249686a":"markdown","272a2f2b":"markdown","f7b2d876":"markdown","8127e37c":"markdown","724f1afa":"markdown","24bd1de2":"markdown","28eaacb1":"markdown"},"source":{"20ad4f0f":"#import libraries\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport warnings\nwarnings.simplefilter('ignore')\n#read train data and test data\ndata = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","15709605":"data","a34b1a17":"#fill missing values drastically \ndata = data.fillna(method='bfill')\ndata = data.fillna(method='ffill')\ntest = test.fillna(method='bfill')\ntest = test.fillna(method='ffill')\n#make LabeleEncoder into le as an instance\nle = LabelEncoder()\n#LabelEncode the row named \"Sex\"\nle = le.fit(data['Sex'])\ndata['Sex'] = le.transform(data['Sex'])\ntest['Sex'] = le.transform(test['Sex'])\n#Change Cabin numbers like C-100 ,A-200 into A , C.  \ndata['Cabin']=data['Cabin'].str[0:1]\ntest['Cabin']=test['Cabin'].str[0:1]\n#Encode 3 columns into categorical features. \nle = LabelEncoder()\nle = le.fit(data['Sex'])\ndata['Sex'] = le.transform(data['Sex'])\ntest['Sex'] = le.transform(test['Sex'])\nle = le.fit(data['Cabin'])\ndata['Cabin'] = le.transform(data['Cabin'])\ntest['Cabin'] = le.transform(test['Cabin'])\nle = le.fit(data['Embarked'])\ndata['Embarked'] = le.transform(data['Embarked'])\ntest['Embarked'] = le.transform(test['Embarked'])","5f256c28":"x_train= data.iloc[:,[2,4,5,6,7,9,10,11]].astype(\"int64\")\ny_train= data.iloc[:,[1]].astype(\"int64\")","05ca64d2":"x_train","ea3e1c59":"y_train","f90d80f8":"check_df=pd.concat([x_train,y_train],axis=1)\ncheck_df.corr()","4e8a8e50":"x_test=test.iloc[:,[1,3,4,5,6,8,9,10]]\nx_test","a9c76fb0":"#Normalize data from 0 to 1 by using StandardScaler\nx_train = StandardScaler().fit_transform(x_train)\nx_test = StandardScaler().fit_transform(x_test)","40daf997":"# XGBoost Regressor\nxgbm = xgb.XGBRegressor(max_depth=50)\nxgbm.fit(x_train, y_train)\nmodel_score1=xgbm.score(x_train, y_train)\nprint(\"The score on traing data of this machine learning model is \",model_score1,\"!\")","0f8fae20":"#Let's fit and predict.\nforest = RandomForestClassifier(n_estimators=200, random_state=2, max_depth=300)\nforest.fit(x_train, y_train)\nmodel_score2=forest.score(x_train, y_train)\nprint(\"The score on traing data of this machine learning model is \",model_score2,\"!\")","11aa1a6b":"compare_i=pd.DataFrame([\"Random Forest\",\"XGBoost\"])\ncompare_c=pd.DataFrame([model_score2,model_score1])\npd.concat([compare_i,compare_c],axis=1)\n             ","105a9795":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras as kr\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Activation, Flatten,BatchNormalization, Activation,MaxPool2D,Dropout\nfrom tensorflow.keras.optimizers import Adam","734a2852":"np.unique(y_train)","e361ad66":"x_train = x_train.astype('float32')\/255\ny_train = kr.utils.to_categorical(y_train, 2)","1b390e8d":"#Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using Keras.\nmodel3 = Sequential()\nmodel3.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(Dense(32, activation='relu'))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(2, activation='sigmoid'))\nmodel3.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nmodel3.summary()","4873165c":"#model fitting\ndl_result1=model3.fit(x_train, y_train,batch_size=50, epochs=20)","54acf20c":"#Evaluate the deep learning model by using two metrics, loss and accuracy.\nmetrics = ['accuracy']\n#show the evaluation result by using matoplot.\nplt.figure(figsize=(10, 5))\n#Use \"For Loop\".\nfor i in range(len(metrics)):\n    metric = metrics[i]\n    #set subplots to show the result\n    plt.subplot(1, 2, i+1)\n    #Titles of subplots are \"loss\" and \"accuracy\"\n    plt.title(metric) \n    plt_result1 = dl_result1.history[metric] \n    #plot them all\n    plt.plot(plt_result1, label='Deep Learning model') \n    plt.legend() \nplt.show()","9eb25382":"y_pred=forest.predict(x_test)","a3fdb659":"y_pred2= pd.DataFrame(y_pred)\ny_pred2[\"Survived\"]= y_pred2[0]\ny_pred2=y_pred2.iloc[:,[1]].astype(\"int\")\ny_pred2\n","fb8d59b0":"#Connect test data and prediction data \nresult=pd.concat([test,y_pred2],axis=1)\nresult=result.iloc[:,[0,11]]\nresult","17e23a65":"#make an output csv file as submission.csv\nresult.to_csv('submission_rfc.csv', index=False)\nprint('submission_file was saved!')","f1d034c0":"### Step5 :Let's make a prediction!?","1c63067d":"### Step4 :Let's fit this remodeled dataset into 3 models.","b343a559":"#### Ummm. I decided to use whole columns.\u00b6","ede13d01":"<HR>","39d8bd28":"### Step2 : Remodel datasets.\u00b6","8b3c5767":"### Step1 : Preparation for prediction","363b7772":"### Deep Learning model works a little bit in this case. \n### Anyway, Random Forest Classifier is the best model in 3 models. ","11027b12":"<HR>","af92bf2c":"<HR>","a4ef3988":"<HR>","7b83044d":"<HR>","1249686a":"<HR>","272a2f2b":"### Step3: Check the correlation in the dataset.","f7b2d876":"## Let's get started with Titanic Machine Learning Competition by using <font color=\"Green\">XGBoost<\/font> and <font color=\"Blue\">Random Forest Classifier<\/font> and <font color=\"gold\">Deep Learning<\/font>. Which one is better in this case?","8127e37c":"#### How about Deep Learning models? Okay, give it a try. ","724f1afa":"#### Then,Random Forest Classifier is better?!<HR>","24bd1de2":"\n#### Thanks for reading my notebook. Feel free to comment :-)","28eaacb1":"### Step6 :Let's compile a submission file."}}