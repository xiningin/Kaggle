{"cell_type":{"91f2ddcc":"code","7df1592e":"code","d8e42d64":"code","67526cbc":"code","2ae58b7a":"code","babe32da":"code","092f6314":"code","45e1cb14":"code","cdee1a7a":"code","9c3d8016":"code","fb99da46":"code","c5bf235a":"code","085db423":"code","12e8d363":"code","bc5f17bf":"code","3a1f12d0":"code","0db43a8e":"code","b8e0dbcf":"code","5fa6d55a":"code","57175141":"markdown","40fbfea7":"markdown","4219f79b":"markdown","ac104b0b":"markdown","87f17867":"markdown","4499b7fe":"markdown","aff7a19b":"markdown","05d5b98e":"markdown","05001038":"markdown","377c1b6e":"markdown","90ca5b10":"markdown","641843a0":"markdown","f5d211f3":"markdown","ad1c2960":"markdown","b7f5d0c5":"markdown","51426230":"markdown","f1c34a13":"markdown","b3c6b387":"markdown","671689dd":"markdown","e2e31d8d":"markdown","40550054":"markdown","ba7bde54":"markdown","2fc3b5c4":"markdown","1e12ed5d":"markdown"},"source":{"91f2ddcc":"import ipywidgets as widgets\nimport matplotlib.pyplot as plt \nfrom math import exp,factorial,sqrt\n%pylab inline","7df1592e":"# plot the pdf of 1\/n(\\sum_{i=1}^n X_i), X_i~U_{a,b}\ndef uniform_sample_counts(a, b, k, n):\n    X = random.uniform(a-(a+b)\/2, b-(a+b)\/2, [k,n])  # generate a k by n matrix of uniform random numbers\n    S = sum(X, axis=0)\/sqrt(k)\n    return S\n\ndef uniform_plot_hist(s,k,h):\n    a = s[0]\n    b = s[1]\n    if h > 0:\n        n = h\n        counts = uniform_sample_counts(a,b,k,n)\n        hist(counts, bins=40, normed=1, label=\"Histogram of emphirical means\");\n    \ndef uniform_mean_pdf(s,n,h):\n    a = s[0]\n    b = s[1]\n    d = 10.0\/1000;\n    x = linspace(-4.99,5,1000)\n    if a < b:\n        y = (1.0*(x>=(a-b)\/2))*(1.0*(x<=(b-a)\/2))\/(b-a)\n        z = y\n        for j in range(2,n+1):\n            t = [item\/(j-1) for item in z for i in range(j-1)]\n            z = [0,] + convolve(y,t).tolist()\n            z = [i*d for i in z]\n            z = sum(reshape(z,(1000,j)),axis=1)\n        sc = int(n\/sqrt(n))\n        rem = n\/sqrt(n)-sc\n        z = [item\/(rem+sc) for item in z for i in range(sc+random.binomial(1,rem))]\n        x = linspace(-d*len(z)\/2,d*len(z)\/2,len(z))\n        close()\n        plot(x, z,label=\"Distribution of the mean\")\n        xlim([-5,5])\n        title('PDF and histogram of ${Z}_n$ with n=%d'%n)\n        xlabel('$x$')\n        ylabel('$f_{S_n}(x)$')\n        \n        var = (b-a)**2\/12\n        p = linspace(-5,5,1000)\n        q = [exp(-i**2\/(2*var))\/(sqrt(2*pi*var)) for i in p]\n        plot(p,q,label=\"Gaussian distribution\")\n        uniform_plot_hist(s,n,h)\n        xlim([-5,5])\n        plt.legend()\n        grid()\n        \n        \nw=widgets.FloatRangeSlider(\n    description = \"[a, b]\",\n    value=[2, 8],\n    min=0.02,\n    max=9.98,\n    step=0.01,\n    continuous_update = False)\nwidgets.interact(\n    uniform_mean_pdf,\n    s=w,\n    n=widgets.IntSlider(min=1, max=10,description='n:', step=1, value=1),\n    h=widgets.IntSlider(min=0, max=10000,description='samples :', step=50, value=0))","d8e42d64":"# plot the pdf of 1\/n(\\sum_{i=1}^n X_i), X_i~Exp_{lam}\ndef exp_sample_counts(lam,k,n):\n    X= random.exponential(1.0\/lam,[k,n])-1.0\/lam  # generate a k by n matrix of uniform random numbers\n    S=sum(X,axis=0)\/sqrt(k)\n    return S\n\ndef exp_plot_hist(lam,k,h):\n    lam\n    if h>0:\n        n=h\n        counts=exp_sample_counts(lam,k,n)\n        hist(counts,bins=40,normed=1,label=\"Histogram of emphirical means\")\n    \ndef exponential_mean_pdf(lam,n,h):\n    d=0.01\n    x=linspace(d,5,500)\n    z=[(lam**n)*((i*sqrt(n))**(n-1))*exp(-lam*(i*sqrt(n)))\/(factorial(n-1))*sqrt(n) for i in x]\n    x=linspace(d-n\/(sqrt(n)*lam),5-n\/(sqrt(n)*lam),500)\n    close()\n    plot(x, z, label=\"Distribution of the Mean\")\n    #plot([1.0\/lam, 1.0\/lam], [0, 2], 'g--', linewidth = 2.0)\n    title('PDF and histogram of ${Z}_n$ with n=%d $\\lambda$=%1.2f'%(n,lam), fontsize = 20)\n    xlabel('$x$', fontsize = 20)\n    ylabel('$f_{S_n}(x)$', fontsize = 20)\n    \n    var = 1.0\/(lam**2)\n    p = linspace(-5,5,1000)\n    q = [exp(-i**2\/(2*var))\/(sqrt(2*pi*var)) for i in p]\n    plot(p,q,label=\"Gaussian distribution\")\n    xlim([-5,5])\n    ylim([0,1.3])\n    grid()\n    \n    exp_plot_hist(lam,n,h)\n\nwidgets.interact(\n    exponential_mean_pdf,\n    lam=widgets.FloatSlider(min=1, max=3,description='$\\lambda$:', step=0.1, value=2),\n    n=widgets.IntSlider(min=1, max=30,description='n:', step=1, value=1),\n    h=widgets.IntSlider(min=0, max=10000,description='s:', step=50, value=0))","67526cbc":"from scipy.stats import norm, uniform\nimport numpy as np","2ae58b7a":"def Sample_Mean(n,r):\n# n: sample size\n# r: number of experiments\n\n    figure(figsize=(20,10))\n    xlim([1,n])\n    ylim([-1, 1])\n    grid()\n    \n    x = list(range(1,n+1))\n    z = 1.0\/np.sqrt(x)\n    plot(x, z,'k--')\n    plot(x, negative(z), 'k--')\n    \n    for i in range(r):\n        y = random.normal(0, 1, n)\n        m = divide(cumsum(y), x)\n        plot(x, m, alpha=0.5)","babe32da":"widgets.interact(Sample_Mean,\n                 n=widgets.IntSlider(min=10, max=1000,description='sample size', step=10, value=100),\n                 r=widgets.IntSlider(min=1, max=10,description='experiments', step=1, value=5))","092f6314":"@widgets.interact(n=(1,30))\ndef Normal_Mean(n):\n# n: sample size\n# s: number of experiments\n    figure(figsize=(20,10))\n    title('histogram of sample means with sample size n=%d'%n,fontsize = 15)\n    xlabel('$\\overline{X}$',fontsize = 15)\n    ylabel('frequency', fontsize = 15)\n    grid()\n\n    s = 100000\n    \n    x = linspace(-4,4,1000)\n    #y = [norm.pdf(i,0,1) for i in x]\n    y = [uniform.pdf(i,0,1) for i in x]\n    plot(x,y)\n    \n    #X = random.normal(0,1,[n,s])\n    X= random.uniform(0,1,[n,s])\n    M = sum(X,axis=0)\/n\n    hist(M,bins=40,normed=1)","45e1cb14":"def Normal_Variance(n,df):\n# n: sample size\n# s: number of experiments\n# df: degree of freedom\n#     df=0: calculate \"raw\" variance \n#     df=1: calculate unbiased variance \n    figure(figsize=(20,10))\n    xlim([0,4])\n    \n    s = 1000000\n    X = random.normal(0,1,[n,s])\n    V = var(X,axis=0,ddof=df)\n    v = mean(V)\n    \n    plot([v,v], [0, 3], 'r--', linewidth = 2.0)\n    hist(V,bins=60,normed=1);\n\n    plot([1,1], [0, 3], 'g:', linewidth = 2.0)\n    ylabel('frequency', fontsize = 15)\n    grid()","cdee1a7a":"@widgets.interact(n=(2,20))\ndef Raw_Variance(n):\n    Normal_Variance(n,0)\n    title('histogram of \"raw\" sample variance with sample size n=%d'%n,fontsize = 15)\n    xlabel('\"$S^2$\"', fontsize = 15)\n","9c3d8016":"@widgets.interact(n=(2,20))\ndef Unbiased_Variance(n):\n    Normal_Variance(n,1)\n    title('histogram of unbiased sample variance with sample size n=%d'%n,fontsize = 15)\n    xlabel('$S^2$', fontsize = 15)","fb99da46":"def Normal_SD(n):\n# n: sample size\n# s: number of experiments\n\n    figure(figsize=(20,10))\n    xlim([0,3])\n    title('histogram of sample standard deviation with sample size n=%d'%n,fontsize = 15)\n    xlabel('$\\hat{\\sigma}$', fontsize = 15)\n    \n    s = 1000000\n    X = random.normal(0,1,[n,s])\n    V = np.sqrt(var(X,axis=0,ddof=1))\n    v = np.mean(V)\n    \n    plot([v,v], [0, 3], 'r--', linewidth = 2.0)\n    hist(V,bins=60,normed=1);\n\n    plot([1,1], [0, 3], 'g:', linewidth = 2.0)\n    ylabel('frequency', fontsize = 15)\n    grid() ","c5bf235a":"widgets.interact(\n    Normal_SD,\n    n = widgets.IntSlider(min=2, max=10,description='n=', step=1, value=2))","085db423":"from IPython.display import display\nfrom scipy.stats import binom","12e8d363":" def hypothesisTesting(n,option,sig_level):\n    pmf = binom.pmf(range(n+1),n=n,p=0.5)\n    plt.figure(figsize=(12,8))\n    plt.plot(range(n+1),pmf)\n    plt.xlabel(\"Number of Heads\",fontsize=18)\n    plt.ylabel(\"Probability\",fontsize=18)\n    plt.title(\"The Binomial distribution under the null hypothesis\",fontsize=18)\n    plt.show()\n    if option==\"p > 0.5\":\n        k=binom.ppf(1-sig_level,n=n,p=0.5)\n        print(\"Reject null hypothesis if number of heads is more than {}\".format(k))\n    elif option==\"p < 0.5\":\n        k = binom.ppf(sig_level,n=n,p=0.5)-1\n        print(\"Reject null hypothesis if number of heads is less than {}\".format(k+1))\n    elif option==\"p \u2260 0\":\n        k1 = binom.ppf(1-sig_level\/2,n=n,p=0.5)+1\n        k2 = binom.ppf(sig_level\/2,n=n,p=0.5)-1\n        print(\"Reject null hypothesis if number of heads lies outside {} and {}\".format(k2,k1))\n\nstyle = {'description_width': 'initial'}\nw_opt = widgets.Dropdown(options=[\"p > 0.5\",\"p < 0.5\",\"p \u2260 0\"],description=\"Alternate Hypothesis:\",style=style)\nw_sig_level =  widgets.FloatSlider(value =0.05, min = 0., max = 0.5 , step=0.05, description=\"Significance level:\",style=style)\nw_n = widgets.IntSlider(value = 20, min = 10, max = 100, step = 5, style = style)\nv = widgets.interact(hypothesisTesting,n=w_n,option = w_opt,sig_level=w_sig_level)\ndisplay(v)","bc5f17bf":"from scipy.stats import t","3a1f12d0":"def sample(p = 0.5):\n    Y = np.random.rand(1)\n    if Y>=p:\n        return np.random.normal(10,2)\n    else:\n        return np.random.normal(12,2)","0db43a8e":"plt.figure(figsize=(12,8))\nplt.hist(np.asarray([sample(0.2) for _ in range(10000)]),50,density=True)\nplt.xlabel(\"X\",fontsize=18)\nplt.ylabel(\"Probability\",fontsize=18)\nplt.show()","b8e0dbcf":"def Z_test(n,test_type):\n    samples = np.asarray([sample(0.2) for _ in range(n)])\n    sample_mean = np.mean(samples)\n    print(\"Sample mean:{:.4f}\".format(sample_mean))\n    mean = 10\n    sigma = 2\n    z = (sample_mean - mean)*np.sqrt(n)\/sigma\n    print(\"z-score:{:.4f}\".format(z))\n    if test_type==\"\u03bc > \u03bc under null hypothesis\":\n        p = 1 - norm.cdf(z)\n        print(\"p-value: {:.6f}\".format(p))\n    #elif test_type==\"\u03bc < \u03bc under null hypothesis\":\n     #   p = norm.cdf(z)\n      #  print(\"p-value : {}\".format(p))\n    elif test_type==\"\u03bc \u2260 \u03bc under null hypothesis\":\n        p = 2*(1-norm.cdf(np.abs(z)))\n        print(\"p-value: {}\".format(p))\n\nw_opt = widgets.Dropdown(\n    options=[\"\u03bc > \u03bc under null hypothesis\",\"\u03bc < \u03bc under null hypothesis\",\"\u03bc \u2260 \u03bc under null hypothesis\"],\n    description = \"Test type\"\n    )\nw_n = widgets.IntSlider(value = 20, min = 10, max = 1000, step = 10)\nv = widgets.interact(Z_test,n=w_n,test_type = w_opt)\ndisplay(v)","5fa6d55a":"def T_test(n,test_type):\n    samples = np.asarray([sample(0.2) for _ in range(n)])\n    sample_mean = np.mean(samples)\n    S = np.std(samples,ddof=1)\n    print(\"Sample mean:{:.4f}\".format(sample_mean))\n    mean = 10\n    t_score = (sample_mean - mean)*np.sqrt(n)\/S\n    print(\"t-score:{:.4f}\".format(t_score))\n    if test_type==\"\u03bc > \u03bc under null hypothesis\":\n        p = 1 - t.cdf(t_score,n-1)\n        print(\"p-value: {:.6f}\".format(p))\n    # elif test_type==\"\u03bc < \u03bc under null hypothesis\":\n    #    p = t.cdf(t_score,n-1)\n    #    print(\"p-value : {}\".format(p))\n    elif test_type==\"\u03bc \u2260 \u03bc under null hypothesis\":\n        p = 2*(1-t.cdf(np.abs(t_score,n-1)))\n        print(\"p-value: {}\".format(p))\n\nw_opt = widgets.Dropdown(\n    options=[\"\u03bc > \u03bc under null hypothesis\",\"\u03bc < \u03bc under null hypothesis\",\"\u03bc \u2260 \u03bc under null hypothesis\"],\n    description = \"Test type\"\n    )\nw_n = widgets.IntSlider(value = 20, min = 10, max = 1000, step = 10)\nv = widgets.interact(T_test,n=w_n,test_type = w_opt)\ndisplay(v)","57175141":"We do the same test, but now assume that we do not know the variance beforehand, so we use the t-statistic. Observe how the p-values change with the number of samples.","40fbfea7":"## T-Test","4219f79b":"## Hypothesis Testing ","ac104b0b":"# Intoduction to Statistics 2","87f17867":"### Unbiased Variance Estimator\n\nNext we apply the Bessel correction, where instead of normalizing by $n$, we normalize by $n-1$. We show experimentally that this estimator is unbiased.\n\nAgain, the underlying distribution is standard Normal. The red line shows the expectation of the unbiased sample variance and the blue line shows the true distribution variance. And we can see that those two lines overlap for all $n$.","4499b7fe":"## Variance Estimation\n\nNext we estimate the variance. \n\n### Raw (biased) Estimator\n\nWe start with the raw estimate that uses the intuitive normalization by $n$. We show experimentally that its expected value is $\\frac{n-1}n\\sigma^2$.\n\nIn this example the underlying distribution is standard Normal. The green line shows the true distribution variance, here 1. The red line shows the average of the \"raw\" (biased) sample variance. Observe that as you change $n$, the red line is roughly at $\\frac{n-1}n$.","aff7a19b":"The null hypothesis is that the $X$ is a Gaussian random variable with mean 10, the variance of this distribution is given as 4.The code given below allows us to vary the number of samples and calculate the p-values, for different cases of alternate hypothesis. Observe how the p-value changes with the number of samples.","05d5b98e":"### Distribution of the sample mean\n\nNext consider the distribution of the sample mean $\\overline X$. In this example the underlying distribution is either uniform or standard Normal (you can comment the code to choose). You can see that as the sample size $n$ increases the distribution of $\\overline X$ becomes uniform, as predicted by the central limit theorem. If you sample from the normal distribution then $\\overline X$ is exactly normal for any $n$. ","05001038":"We continue to use the standard normal as the underlying distribution. The green line shows the distribution's true standard deviation $\\sigma=1$. The red line shows the average of the sample standard deviation (square root of unbiased sample variance). Observe that the sample standard deviation underestimates $\\sigma$, but this underestimate shrinks as you increase $n$.","377c1b6e":"To see what the distribution of $X$ looks like, let us generate 10000 samples and plot an estiamte of the distribution. ","90ca5b10":"The code below plots the probability distribution function (PDF) of $S_n$ and $\\mathcal{N}(0,\\sigma^2)$ when the $X_i$ follow exponential distribution with parameter $\\lambda$. Again observe the high convergence speed.","641843a0":"Many a times we draw samples from the real world, and it is a common practice to assume the distribution to be Gaussian. In this section we will use the z-test to test this hypothesis.\n\nConsider a random variable $X$ given by\n$$X = Y*Z_1+(1-Y)*Z_2$$\nWhere $Y$ is a Bernoulli random variable ,and $Z_1$ and $Z_2$ are Gaussian random variables.Sampling $X$ is same as sampling $Z_1$ or $Z_2$ with probability $p$ and $1-p$ respectively.These kinds of models are called Gaussian Mixture Models. The following code generates a sample of $X$.","f5d211f3":"The following graph shows how the sample mean $\\overline X$ converges to the distribution mean $\\mu$. The underlying distribution is standard normal. $n$ is the number of samples, and $r$ is the number of experiments.\n\nRecall that the sample mean's expected value is always $\\mu$, which here is 0, and that its standard deviation is $\\frac\\sigma{\\sqrt n}$, which here is $\\frac1{\\sqrt n}$. The dashed black line shows this value and its negation. \n\nObserve:\n* When the sample size $n$ increases, all curves get closer to the distribution mean 0.\n* The $r$ sample means are typically bounded between the positive and negative standard deviations.","ad1c2960":"## Sampling\n\n**Non-Representative Sampling**\n\n * Convenience Sampling: Pick samples that are most convenient, like the top of a shelf or people that can be easily approached.\n\n * Haphazard Sampling: Pick samples without thinking about it. This often gives the illusion take you are picking out samples at random.\n\n * Purposive Sampling: Pick samples for a specific purpose. An example is to focus on extreme cases. This can be useful but is limited because it doesn't allow you to make statements about the whole population.\n\n**Representative Sampling**\n\n * Simple Random Sampling: Pick samples (psuedo)randomly.\n\n * Systematic Sampling: Pick samples with a fixed interval. For example every 10th sample (0, 10, 20, etc.).\n\n * Stratified Sampling: Pick the same amount of samples from different groups (strata) in the population.\n\n * Cluster Sampling: Divide the population into groups (clusters) and pick samples from those groups.","b7f5d0c5":"The code below plots the probability distribution function (PDF) of $S_n$ and $\\mathcal{N}(0,\\sigma^2)$ when $X_i$ follows uniform distribution on $[a,b]$. Observe how quickly the distribution of $S_n$ converges to normal. ","51426230":"In the last kernel, we have looked at how to estimate parameters and statistical measures such as sample mean and variance. Hypothesis testing is a method by which we can quantify the quality of our estimates. An important step in this method is defining the null and alternate hypothesis clearly. By doing so we can exactly interpret the results of the test.","f1c34a13":"## Conclusion\n\nI will be doing ANOVA in my data analysis with Python kernel. Here is the [link](https:\/\/www.kaggle.com\/fazilbtopal\/exploratory-data-analysis-with-python). ","b3c6b387":"## Standart Error\n\nThe Standard Error (SE) measures how spread the distribution is from the sample mean.\n\n$$ SE = \\frac{\\sigma}{\\sqrt{n}}$$","671689dd":"We can use hypothesis testing to see whether a coin is biased or not. Given a coin we can toss it $n$ times and count the number of heads we get. The null hypothesis is that the coins are unbiased, which means, $P(Heads)=P(Tails)=0.5$. The code below displays critical values for different alternate hypothesis. You can vary $n$ and significance level and see the change in the critical values.","e2e31d8d":"## Central Limit Theorem","40550054":"This section demonstrates the Central Limit Theorem by comparing\n$$S_n=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n \\left({X_i-\\mu}\\right)$$\nwith the normal distribution $\\mathcal{N}(0,\\sigma^2)$\nwhere $X_i$ are iid random variables and $\\mu=E[X_i]$, $\\sigma^2=V(X_i)$.","ba7bde54":"## Estimating the standad deviation\n\nWe apply the standard standard-deviation estimator and show that on average it underestimates $\\sigma$. ","2fc3b5c4":"## Z-Test","1e12ed5d":"### Estimating the Mean\n\nWe demonstrate how the sample mean approximates the distribution mean. \n\n#### Convergence of Sample Mean\n\nIf we take $n$ samples, $X_1,\\ldots, X_n$, the sample mean $\\overline X=\\frac{X_1+\\ldots+X_n}n$ converges to the distribution mean $\\mu$. The following program demonstrate that."}}