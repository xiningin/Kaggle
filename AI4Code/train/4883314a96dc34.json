{"cell_type":{"0a009714":"code","d5dbce4f":"code","e294e60e":"code","06e42cb2":"code","bb73c4d1":"code","0a9821ef":"code","d5f8b632":"code","d0c0a53b":"code","aa6183f1":"code","6f98f981":"code","973fcbaa":"code","c3ac37aa":"code","38b3626b":"code","e3f50590":"code","6da74b3d":"code","8d26c279":"code","175d43a3":"code","f38fd024":"code","7e6c0c7c":"code","954a7488":"code","38be8bb9":"code","532ae326":"code","32c9d5f5":"code","95647a39":"code","8526631b":"code","2f433016":"code","95a118fc":"code","db3f6347":"code","0ba62bcb":"code","2b77cac2":"code","bb2e9dac":"code","e1bb4ed9":"code","6b413c0d":"code","cb748edb":"code","abe305a2":"code","99284785":"code","6515b1a7":"code","d2b1edaf":"code","13b49dfe":"code","df3823f2":"code","60cd8b37":"code","57e55b1d":"code","b6491f1b":"code","78d58b54":"code","e89167d4":"markdown","72553d40":"markdown","dacbe2a9":"markdown","e31694a7":"markdown","056014e0":"markdown","ca524e02":"markdown","a0df510e":"markdown","dff4a80f":"markdown","dcc79ac6":"markdown","c62d7072":"markdown","60e37d12":"markdown","c700ed28":"markdown","36bb0280":"markdown","869e3fa1":"markdown","2c42f920":"markdown","f2ad6521":"markdown","02950f22":"markdown","3b07c04e":"markdown","e62ee963":"markdown","2afc940a":"markdown","3a982999":"markdown","0fc4f097":"markdown","5b7bd025":"markdown","6cebbf4b":"markdown","b9c57fe1":"markdown","2b430642":"markdown","59696950":"markdown","03848cc7":"markdown","ca24c173":"markdown","18f606eb":"markdown","ed00df0b":"markdown","f6b90afa":"markdown","ee4ff8a0":"markdown","d66e8455":"markdown","6ef96691":"markdown","d204a230":"markdown","129f3515":"markdown"},"source":{"0a009714":"# Load libraries\nimport sys\nimport scipy\nimport numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nfrom pandas.plotting import scatter_matrix\nfrom math import sqrt\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression, Ridge, LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport pickle","d5dbce4f":"# get working directory\n! pwd","e294e60e":"# load data\nclimbing_data = pd.read_csv('\/kaggle\/input\/mount-rainier-weather-and-climbing-data\/climbing_statistics.csv')\nweather_data = pd.read_csv('\/kaggle\/input\/mount-rainier-weather-and-climbing-data\/Rainier_Weather.csv')","06e42cb2":"climbing_data.info()\nweather_data.info()","bb73c4d1":"# Merge 2 datasets\njoined_data = pd.merge(climbing_data, weather_data, how=\"left\", on=[\"Date\"])\njoined_data.head()","0a9821ef":"# Remove missing values\njoined_data.isna().sum()\njoined_data = joined_data.dropna()\njoined_data.isna().sum()","d5f8b632":"# Remove rows where success percentage > 1.00\njoined_data = joined_data[joined_data['Success Percentage'] <= 1]  \njoined_data.describe()","d0c0a53b":"# Feature selection\ndata = joined_data.drop(columns=[\"Date\", \"Attempted\",\"Succeeded\", \"Battery Voltage AVG\"])\ndata.info()","aa6183f1":"# dummify \"Route\"\ndummy_Route = pd.get_dummies(data['Route'])\ndummify_data = pd.concat([data, dummy_Route], axis = 1)\ndata = dummify_data.drop(columns=[\"Route\"])\ndata.info()","6f98f981":"# Alternative to dummify, is to code \"Route\"\n\n# Change route into codes\ndata[\"Route\"] = data[\"Route\"].astype(\"category\")\ndata[\"Route_code\"] = data[\"Route\"].cat.codes\ndata[\"Route_code\"].describe() # if min is -1, then there is NA\n\n# double check there are same no of unique Route names and Route codes\ndata[\"Route\"].describe() == data[\"Route\"].cat.codes.astype(\"category\").describe()\n\n# View Route code dictionary\ncode = data[\"Route\"].astype('category')\ncode_dictionary = dict(enumerate(code.cat.categories))\nprint(code_dictionary)","973fcbaa":"# describe\ndata.describe()","c3ac37aa":"# head\ndata.head()","38b3626b":"# shape\ndata.shape","e3f50590":"# data types\ndata.dtypes","6da74b3d":"# attributes\nattributes = data.dtypes.index\nprint(attributes)","8d26c279":"# Correlation Coefficient Matrix Heatmap\ncorrelation = data.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(correlation, xticklabels=correlation.columns.values, yticklabels=correlation.columns.values)","175d43a3":"# Covariance Coefficient Matrix Heatmap\ncovariance = data.cov()\nplt.figure(figsize=(15, 10))\nsns.heatmap(covariance, xticklabels=covariance.columns.values, yticklabels=covariance.columns.values)","f38fd024":"# Dependent variable -- 'Success Percentage'\nsns.countplot(data['Success Percentage'])","7e6c0c7c":"# Distribution of attribute -- \"Success Percentage\"\nf = plt.figure(figsize=(20,4))\nf.add_subplot(1,2,1)\nsns.distplot(data['Success Percentage'])\nf.add_subplot(1,2,2)\nsns.boxplot(data['Success Percentage'])","954a7488":"# histogram of all attributes\ndata.hist(figsize=(10,6), bins = 10)\nplt.show()","38be8bb9":"#box plot\nplt.figure(figsize=(20,6))\nsns.boxplot(data = data)","532ae326":"# Line plot\nplt.plot(data['Temperature AVG'])\nplt.title('Line plot: Temperature AVG')\nplt.ylabel('Temperature AVG')\nplt.show()","32c9d5f5":"# Visualize succeeded vs attempted climbs per route\ndata.loc[joined_data[\"Succeeded\"]==1]\n\n# create succeeded climbs dataset\nsucceded_data = joined_data[[\"Route\",\"Succeeded\"]].groupby(\"Route\").sum().reset_index()\nsucceded_data.columns = [\"Route\", \"Succeeded\"]\n#succeded_data.head()\n\n# create attempted climbs dataset\nattempts_data = joined_data[[\"Route\",\"Attempted\"]].groupby(\"Route\").sum().reset_index()\nattempts_data.columns=[\"Route\", \"Attempted\"]\n#attempts_data.head()\n\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nSuccessAttempt_data = [go.Bar(x=attempts_data.Route,\n               y=attempts_data.Attempted, name = \"Attempted climb\"),\n        go.Bar(x=succeded_data.Route,\n               y=succeded_data.Succeeded, name = 'Successful climb'),]\n\nlayout = go.Layout(barmode='stack', title = 'Sucesssful vs Attempted climbs')\n\nfig = go.Figure(data=SuccessAttempt_data, layout=layout)\niplot(fig)","95647a39":"# Visualize % succeess rate per route\nsuccess_rate_data = pd.merge(attempts_data, succeded_data, how=\"left\", on=[\"Route\"])\nsuccess_rate_data[\"Success Percentage\"] = (success_rate_data.Succeeded \/ success_rate_data.Attempted * 100)\n#success_rate_data.head(10)\n\nimport plotly.express as px\nfig = px.bar(success_rate_data, x = \"Route\", y = \"Success Percentage\")\nfig.show()","8526631b":"# scatter plot of 2 features\nscatter_x = data['Success Percentage']\nscatter_y = data['Temperature AVG']\nplt.scatter(scatter_x, scatter_y)\nplt.title('Relationship between climb success and temperature')\nplt.xlabel('% successful climbs')\nplt.ylabel('Average temperature')\nplt.show()","2f433016":"# scatter plot matrix\nscatter_matrix(data, figsize=(10, 10))\nplt.show()","95a118fc":"# Create x (independent, input) + y (dependent, output) variables\nx = data.drop(columns=['Success Percentage'])\ny = data['Success Percentage']\n\n# Split train\/validation datasets (80-20%)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=7)\n\n# dimensions of train\/test set\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","db3f6347":"# Prepare models\nmodels = []\n\n# classification\n#models.append(('KNN', KNeighborsClassifier()))\n#models.append(('CART', DecisionTreeClassifier()))\n#models.append(('SVM', SVC(gamma='auto')))\n#models.append(('RF', RandomForestClassifier(n_estimators=100, max_features=3)))\n#models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n#models.append(('LDA', LinearDiscriminantAnalysis()))\n#models.append(('NB', GaussianNB()))\n\n# regression\nmodels.append(('RFregressor', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('KNNregressor', KNeighborsRegressor()))\nmodels.append(('LinearR', LinearRegression()))","0ba62bcb":"# Evaluate each model's accuracy on the validation set\nprint('Cross Validation Score: RMSE & SD')\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\tcv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='neg_root_mean_squared_error')\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint('%s: %.3f (%.3f)' % (name, -cv_results.mean(), cv_results.std()))\n    \n# Visualize model comparison\nplt.boxplot(results, labels=names)\nplt.title('Model Comparison: Cross Validation Score (RMSE)')\nplt.show()","2b77cac2":"print('Train Set Performance Metrics: RMSE & MAE')\nfor name, model in models:\n    trained_model = model.fit(x_train, y_train)\n    y_train_pred = trained_model.predict(x_train)\n    print('%s: %.3f (%.3f)' % (name, sqrt(mean_squared_error(y_train, y_train_pred)), (mean_absolute_error(y_train, y_train_pred))))","bb2e9dac":"print('Test Set Performance Metrics: RMSE & MAE')\nfor name, model in models:\n    trained_model = model.fit(x_train, y_train)\n    y_test_pred = trained_model.predict(x_test)\n    print('%s: %.3f (%.3f)' % (name, sqrt(mean_squared_error(y_test, y_test_pred)), (mean_absolute_error(y_test, y_test_pred))))","e1bb4ed9":"# Train model\nRFregressor = RandomForestRegressor().fit(x_train, y_train)\n\n\n# Predict y on train set\ny_train_pred = RFregressor.predict(x_train)\n\n# Train set performance metrics\nprint('Train Set Performance Metrics: RMSE')\nprint('%.3f' % (sqrt(mean_squared_error(y_train, y_train_pred))))\n\n# Predict y on test set\ny_test_pred = RFregressor.predict(x_test)\n\n# Test set performance metrics\nprint('Train Set Performance Metrics: RMSE')\nprint('%.3f' % (sqrt(mean_squared_error(y_test, y_test_pred))))","6b413c0d":"# Visualize predicted vs actual mountain climb 'Success Percentage' -- if perfect, then a diagonal line\nplt.scatter(y_test, y_test_pred, alpha = 0.5)\nplt.xlabel('Actual')\nplt.ylabel('Predictions')\nplt.show()","cb748edb":"# Min-Max norm all features\nminmax_scaler = MinMaxScaler()\nx_norm = minmax_scaler.fit_transform(x)\nx = pd.DataFrame(x_norm, columns=x.columns)\nx.head()","abe305a2":"# Split train\/validation datasets (80-20%)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=7)\n\n# dimensions of train\/test set\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","99284785":"# Evaluate each model's accuracy on the validation set\nprint('Cross Validation Score: RMSE & SD')\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = KFold(n_splits=10, random_state=7, shuffle=True)\n\tcv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='neg_root_mean_squared_error')\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint('%s: %.3f (%.3f)' % (name, -cv_results.mean(), cv_results.std()))\n    \n# Visualize model comparison\nplt.boxplot(results, labels=names)\nplt.title('Model Comparison: Cross Validation Score (RMSE)')\nplt.show()","6515b1a7":"print('Train Set Performance Metrics: RMSE & MAE')\nfor name, model in models:\n    trained_model = model.fit(x_train, y_train)\n    y_train_pred = trained_model.predict(x_train)\n    print('%s: %.3f (%.3f)' % (name, sqrt(mean_squared_error(y_train, y_train_pred)), (mean_absolute_error(y_train, y_train_pred))))","d2b1edaf":"print('Test Set Performance Metrics: RMSE & MAE')\nfor name, model in models:\n    trained_model = model.fit(x_train, y_train)\n    y_test_pred = trained_model.predict(x_test)\n    print('%s: %.3f (%.3f)' % (name, sqrt(mean_squared_error(y_test, y_test_pred)), (mean_absolute_error(y_test, y_test_pred))))","13b49dfe":"# Train model\nRFregressor = RandomForestRegressor().fit(x_train, y_train)\n\n\n# Predict y on train set\ny_train_pred = RFregressor.predict(x_train)\n\n# Train set performance metrics\nprint('Train Set Performance Metrics: RMSE')\nprint('%.3f' % (sqrt(mean_squared_error(y_train, y_train_pred))))\n\n# Predict y on test set\ny_test_pred = RFregressor.predict(x_test)\n\n# Test set performance metrics\nprint('Train Set Performance Metrics: RMSE')\nprint('%.3f' % (sqrt(mean_squared_error(y_test, y_test_pred))))","df3823f2":"# Visualize predicted vs actual mountain climb 'Success Percentage' -- if perfect, then a diagonal line\nplt.scatter(y_test, y_test_pred, alpha = 0.5)\nplt.xlabel('Actual')\nplt.ylabel('Predictions')\nplt.show()","60cd8b37":"# Random Forest Regressor (RFregressor)\n#Create dictionary of hyperparameters that we want to tune\nRFR_params = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n# Create new RandomForestRegressor object using GridSearch\ngrid_RFR = GridSearchCV(RandomForestRegressor(), RFR_params, cv=3)\n\n#Fit the model\nbest_model_RFR = grid_RFR.fit(x_train, y_train)\n\n# Print the value of best hyperparameters\n#print('Best n_neighbors:', best_model_RFR.best_estimator_.get_params()['n_neighbors'])\n#print('Best n_neighbors:', best_model_RFR.best_estimator_.get_params()['weights'])\n#print('Best n_neighbors:', best_model_RFR.best_estimator_.get_params()['metric'])\n#print('Best leaf_size:', best_model_RFR.best_estimator_.get_params()['leaf_size'])\n#print('Best p:', best_model_RFR.best_estimator_.get_params()['p'])\nprint(best_model_RFR.best_params_)","57e55b1d":"# Predict y on train set\ny_train_pred_2 = best_model_RFR.predict(x_train)\n\n# Train set performance metrics\nprint('Train Set Performance Metrics: RMSE')\nprint('%.3f' % (sqrt(mean_squared_error(y_train, y_train_pred_2))))\n\n# Predict y on test set\ny_test_pred_2 = best_model_RFR.predict(x_test)\n\n# Test set performance metrics\nprint('Train Set Performance Metrics: RMSE')\nprint('%.3f' % (sqrt(mean_squared_error(y_test, y_test_pred_2))))","b6491f1b":"# Visualize predicted vs actual mountain climb 'Success Percentage' -- if perfect, then a diagonal line\nplt.scatter(y_test, y_test_pred_2, alpha = 0.5)\nplt.xlabel('Actual')\nplt.ylabel('Predictions')\nplt.show()","78d58b54":"# Save model to disk\nFinalModel_RandomForestRegressor = 'FinalModel.sav'\npickle.dump(best_model_RFR, open(FinalModel_RandomForestRegressor, 'wb'))","e89167d4":"**Ways data can be preprocessed:**\n* Remove missing values\n* Remove outliers\n* Remove duplicates\n* Feature selection\n* Feature engineering\n* Feature scaling\n    * **Numerical** data = standardize or normalize\n    * **Categorical** data = one-hot encoding or dummify\n* Group data into clusters\n    * Cluster by an attribute (e.g. age, price)\n    * Cluster using k-means","72553d40":"**5 Levels of ML Model Iteration:**\n1. Fitting Parameters\n2. Tuning Hyperparameters\n3. Feature Engineering\n\n---","dacbe2a9":"# Modeling","e31694a7":"## Understand data with descriptive statistics","056014e0":"Performance metrics on ***train set***:","ca524e02":"# Import Libraries","a0df510e":"### Evaluate Model Performance (3)","dff4a80f":"### Optimize Models by Fitting Parameters (2)","dcc79ac6":"#### *EXTRA: deep error check on individual model*","c62d7072":"Split *rescaled* dataset into train\/test set:","60e37d12":"Performance metrics on ***test set***:","c700ed28":"Split dataset into train\/test set:","36bb0280":"## Iteration (1)","869e3fa1":"Random Forest Regressor had the best metrics in previous iterations, therefore let's only tune the hyperparameters for this one model.","2c42f920":"# Preprocess Data (1)","f2ad6521":"Performance metrics on ***train set***:","02950f22":"### Multivariate plots to understand relationship between attributes","3b07c04e":"The aim is to predict success rate (%) in reaching the Mount Rainier peak given (1) the route and (2) the weather condition.","e62ee963":"Performance metrics on ***test set***:","2afc940a":"**Model Types**\n\n**- Linear models:**\n* Logistic Regression (LR)\n* Linear Discriminant Analysis (LDA)\n\n**- Nonlinear models:**\n* K-Nearest Neighbors (KNN)\n* Classification and Regression Trees (CART)\n* Gaussian Naive Bayes (NB)\n* Support Vector Machines (SVM)\n* Ridge Regression (RR)\n\n**- Bagging ensemble models:**\n* Random Forest (RF)\n\n---","3a982999":"Merge the climbing and weather datasets into 1 dataset.","0fc4f097":"### Evaluate Model Performance (2)","5b7bd025":"## Iteration (3)","6cebbf4b":"### Preprocess Data (2)\n#### Normalization (Min-Max)","b9c57fe1":"# Exploratory Data Analysis (EDA)","2b430642":"### Evaluate Model Performance (1)","59696950":"## Understand data with visualization","03848cc7":"# Load dataset","ca24c173":"#### *EXTRA: deep error check on individual model*","18f606eb":"## Iteration (2)","ed00df0b":"---","f6b90afa":"### Optimize Models by Tuning Hyperparameters (3)","ee4ff8a0":"### Optimize Models by Fitting Parameters (1)\n\nTrain models on train set to find the best parameters with cross validation & get the first performance measures on the validation set\n\nFind different model performance metrics part of *scikit learn* here: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html","d66e8455":"# Save final model","6ef96691":"**Final Notes**\n* The best model was Random Forest Regressor. \n* We dummified the Route, normalized the features, tuned parameters and hyperparamters.\n* The RMSE of the model's performance in predicting the % successf rate of reaching the peak of Mt Rainier was only improved from 0.439 to 0.436. The dataset is too small (1889 total rows) for room for extensive further optimization and tuning and improvements. \n* In the future, we could try with a much larger dataset. ","d204a230":"## Preliminary exploration","129f3515":"### Univariate plots to understand each individual attribute"}}