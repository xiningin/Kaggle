{"cell_type":{"ea174cb5":"code","a3f507d1":"code","f6130b18":"code","fc34bbe2":"code","e40f9670":"code","afdbc173":"code","ca5a93b2":"code","fa5ccb12":"code","eacf8f54":"code","b978b2ee":"code","66ee2205":"code","a2283902":"code","dd4c37d8":"code","3a279fb2":"code","43289ad9":"code","401c3cc5":"code","666b0d26":"code","3fe2f960":"code","7e260831":"code","3310c42e":"code","1c1b3375":"code","f9579059":"code","b3ecbaac":"code","881832ea":"code","664845d5":"code","c473b498":"code","b7e23624":"code","667031e3":"code","0c8805ba":"code","5b212fab":"code","d6974404":"code","7570730d":"code","ff642dbe":"code","dec2b61b":"code","6f27873b":"code","d173416d":"code","4d85d669":"markdown","19189224":"markdown","505472dd":"markdown","6d8c8d88":"markdown","506f6dba":"markdown","cbfb33f5":"markdown","e2157311":"markdown","d4f231b4":"markdown"},"source":{"ea174cb5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\nimport pickle","a3f507d1":"# Load the train data\ntrain_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntrain_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")","f6130b18":"# Print the shape and sizee of data\nprint(\"Number of rows in train_transaction data {:,} and number of columns in train_transaction data {:,} \".format(train_transaction.shape[0], train_transaction.shape[1]))\nprint(\"Number of rows in train_id data {:,} and number of columns in train_id data {:,} \".format(train_id.shape[0], train_id.shape[1]))\n","fc34bbe2":"train_id.head()","e40f9670":"# Check for NaN values\ntrain_id.isnull().sum()","afdbc173":"# Print %age of null values\ntotal_rows = train_id.shape[0]\ncolumns_to_drop = []\nfor cols in train_id.columns:\n  # check null values\n  null_count = train_id[cols].isnull().sum()\n  if null_count > 0:\n    perc_null_values = float(null_count)*100\/total_rows\n    print(\"{} column has {:.3f}% null values\".format(cols, perc_null_values))\n\n    if perc_null_values > 50:\n      columns_to_drop.append(cols)","ca5a93b2":"columns_to_drop","fa5ccb12":"# drop the columns to drop\ntrain_id.drop(columns_to_drop, axis=1, inplace=True)","eacf8f54":"# fill  the rest of nan with default values\ntrain_id.fillna(-999, inplace=True)","b978b2ee":"train_id.info()","66ee2205":"# label encode the categorical columns\ncat_cols = [cols for cols in train_id.columns if train_id[cols].dtype == 'object']\n\n# Label Encoder object\nle = LabelEncoder()\nfor col in cat_cols:\n  train_id[col] = train_id[col].astype('str')\n  train_id[col] = le.fit_transform(train_id[col])","a2283902":"train_id.shape","dd4c37d8":"# print percentage of null values in the transaction data\n# Print %age of null values\ntotal_rows = train_transaction.shape[0]\ncolumns_to_drop = []\nfor cols in train_transaction.columns:\n  # check null values\n  null_count = train_transaction[cols].isnull().sum()\n  if null_count > 0:\n    perc_null_values = float(null_count)*100\/total_rows\n    print(\"{} column has {:.3f}% null values\".format(cols, perc_null_values))\n\n    if perc_null_values > 50:\n      columns_to_drop.append(cols)","3a279fb2":"len(columns_to_drop)","43289ad9":"# drop the columns to drop\ntrain_transaction.drop(columns_to_drop, axis=1, inplace=True)","401c3cc5":"# Substitute default values in the rest of the null values\ntrain_transaction.fillna(-999, inplace=True)","666b0d26":"# Encode the categorical cols\ncat_cols2 = [cols for cols in train_transaction.columns if train_transaction[cols].dtype == 'object']\nfor cols in cat_cols2:\n  le = LabelEncoder()\n  train_transaction[cols] = train_transaction[cols].astype('str')\n  train_transaction[cols] = le.fit_transform(train_transaction[cols])","3fe2f960":"train_transaction.isnull().sum()","7e260831":"train_transaction.head()","3310c42e":"train_id.head()","1c1b3375":"# merge both the dataset\nfinal_data = pd.merge(train_id, train_transaction, on='TransactionID', how='inner')","f9579059":"final_data.head()","b3ecbaac":"# split the dataset into train features and target varaible\nX = final_data.drop('isFraud', axis=1)\ny = final_data['isFraud']","881832ea":"X.head()","664845d5":"# split the tdata\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","c473b498":"xgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)","b7e23624":"# Print the accuracy score\nprint(xgb_clf.score(X_test, y_test))","667031e3":"# Compute ROC AUC Score\nprediction_probability = xgb_clf.predict_proba(X_test)\nprint(roc_auc_score(y_test, prediction_probability[ : , 1]))","0c8805ba":"# Compute ROC AUC Score\nprediction = xgb_clf.predict(X_test)\nprint(f1_score(y_test, prediction))","5b212fab":"# print the feature importance\nfeatures = X.columns\nfeature_imp = xgb_clf.feature_importances_\n\nfeat_imp_df = pd.DataFrame({'Features' : features, 'Feature_Importance' : feature_imp}).sort_values(by='Feature_Importance', ascending=False)\n\n# Plot the feature_importance : only top 10\nplt.figure(figsize=(12,10))\nplt.barh(y=feat_imp_df['Features'].iloc[ : 10], width=feat_imp_df['Feature_Importance'].iloc[ : 10])\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Features\")\nplt.show()","d6974404":"pca = PCA(n_components=0.90)\nX_red = pca.fit_transform(X)","7570730d":"imp_col_no_after_pca = np.argmax(pca.components_)\nimp_col_after_pca = X.columns[imp_col_no_after_pca]\n\nprint(\"The feature with most variance after PCA {}\".format(imp_col_after_pca))","ff642dbe":"X_train, X_test, y_train, y_test = train_test_split(X_red, y, test_size=0.2, random_state=0)","dec2b61b":"xgb_clf.fit(X_train, y_train)","6f27873b":"xgb_clf.score(X_test, y_test)","d173416d":"# Compute ROC AUC Score\nprediction_probability = xgb_clf.predict_proba(X_test)\nprint(roc_auc_score(y_test, prediction_probability[ : , 1]))","4d85d669":"**ROC_AUC Score has reduced significantly after reducing the dimesionality of the dataset.**","19189224":"# Import Libraries","505472dd":"**Almost every column has NaN values.**","6d8c8d88":"# XGBoost Model","506f6dba":"# Data Analysis","cbfb33f5":"# PCA","e2157311":"# Train XGBoost Model on the lower dimension dataset","d4f231b4":"**Reduce the dimensionality using PCA**"}}