{"cell_type":{"23fd33af":"code","e53d99e9":"code","362e3e63":"code","8e48bd27":"code","c6ec5b50":"code","8d1c0bdc":"code","a05c518d":"code","94cca76f":"code","d095c352":"code","34315d66":"code","bbfbe79c":"code","3aebcf18":"code","e094b5d3":"code","d815202a":"code","66c7d26b":"code","ba248ef6":"code","9c6876f5":"code","22945487":"code","652f89da":"code","6b2c9b88":"code","34c44966":"code","eb495bd5":"code","d05a2e98":"code","5a947e91":"code","26f6621d":"code","87395277":"code","e8ede1fb":"code","87bbf7aa":"code","ef5f39ac":"code","6d7437bc":"code","a05c6798":"code","ad14ff3f":"code","d9538ced":"code","3898ce29":"code","31222490":"code","a7ecf23b":"code","db005c1e":"code","7ea80b3f":"code","df911607":"code","a09d9a8a":"code","65787533":"code","a717ae26":"markdown"},"source":{"23fd33af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler \nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings('ignore')","e53d99e9":"# Take a look at the data\ndf = pd.read_csv(\"..\/input\/falldata\/falldeteciton.csv\", sep=\",\")\nprint(df.head(10))","362e3e63":"# Data dimensionality [rows, columns]\nprint(df.shape)","8e48bd27":"# Check data quality\ndf.info()","c6ec5b50":"# Describe the dataframe columns\n# We will discard activity column as that is a nominal attribute\ndf.iloc[:,1:7].describe()","8d1c0bdc":"d = df[\"ACTIVITY\"].value_counts().sort_index()\nprint(d)","a05c518d":"# Pie chart, where the slices will be ordered and plotted counter-clockwise:\ndict = {0:'Standing', 1:'Walking', 2:'Sitting', 3:'Falling', 4:'Cramps', 5:'Running'}\nresp = list(dict.keys())\nlabels = list(dict.values())\nsizes = [d[0], d[1], d[2], d[3], d[4], d[5]]\nexplode = (0, 0, 0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, explode = explode, autopct='%1.1f%%', startangle = 90, counterclock=False, shadow=False)\nax1.axis('equal')\nplt.show()","94cca76f":"# Visualize with Bar chart\n#plt.bar(labels, d)\n#plt.show()\nsns.set(style=\"darkgrid\")\nax = sns.countplot(y='ACTIVITY', data=df)\nax.set_yticklabels(labels);","d095c352":"# Histograms\ndf.iloc[:,1:7].hist(bins=10,figsize=(15, 15))\nplt.show()","34315d66":"# Density\ndf.iloc[:,1:7].plot(kind='density', subplots=True, layout=(3,3), sharex=False, figsize=(15, 15))\nplt.show()","bbfbe79c":"# Create pivot_table\ncolum_names = ['TIME','SL','EEG','BP','HR','CIRCLUATION']\ndf_pivot_table = df.pivot_table(colum_names,\n               ['ACTIVITY'], aggfunc='median')\nprint(df_pivot_table)","3aebcf18":"# Correlation matrix\ntmp = df.drop('ACTIVITY', axis=1)\ncorrelations = tmp.corr()\nprint(correlations)\n# Plot figsize\nfig, ax = plt.subplots(figsize=(15, 11))\n# Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n# Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(correlations, cmap=colormap, annot=True, fmt=\".2f\")\nax.set_xticklabels(\n    colum_names,\n    rotation=45,\n    horizontalalignment='right'\n);\nax.set_yticklabels(colum_names);","e094b5d3":"#temp = df.iloc[:,[1,2,5,6]]\n#temp.describe()\n# Correlation matrix\ntemp = df.drop(['ACTIVITY', 'EEG', 'BP'], axis=1)\ncorrelations = temp.corr()\nprint(correlations)\n# Plot figsize\nfig, ax = plt.subplots(figsize=(8, 6))\n# Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n# Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(correlations, cmap=colormap, annot=True, fmt=\".2f\")\nax.set_xticklabels(\n    ['TIME','SL','HR','CIRCLUATION'],\n    rotation=45,\n    horizontalalignment='right'\n);\nax.set_yticklabels(['TIME','SL','HR','CIRCLUATION']);","d815202a":"sns.set(style='ticks')\nsns.pairplot(tmp)","66c7d26b":"# Use boxplot to do the outlier analysis for the dataset feature variables\n# Boxplot for 'TIME'\nsns.boxplot(y=df['TIME'], x=df['ACTIVITY'])","ba248ef6":"# Use boxplot to do the outlier analysis for the dataset feature variables\n# Boxplot for 'SL'\nax = sns.boxplot(y=df['SL'], x=df['ACTIVITY'])","9c6876f5":"# Use boxplot to do the outlier analysis for the dataset feature variables\n# Boxplot for 'EEG'\nax = sns.boxplot(y=df['EEG'], x=df['ACTIVITY'])","22945487":"# Use boxplot to do the outlier analysis for the dataset feature variables\n# Boxplot for 'HR'\nax = sns.boxplot(y=df['HR'], x=df['ACTIVITY'])","652f89da":"# Use boxplot tdfo do the outlier analysis for the dataset feature variables\n# Boxplot for 'BP'\nax = sns.boxplot(y=df['BP'], x=df['ACTIVITY'])","6b2c9b88":"# Use boxplot to do the outlier analysis for the dataset feature variables\n# Boxplot for 'CIRCLUATION'\nax = sns.boxplot(y=df['CIRCLUATION'], x=df['ACTIVITY'])","34c44966":"# Remove outliers from dataset df\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\ndf_out = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf_out.shape","eb495bd5":"# Create a new column called Decision. This column will contain the values of 0 = 'No Fall', 1 = 'Fall' using following rule: \n# Activity Value : 3 --> Fall, else --> No Fall\n\ndecision = []\nfor i in df_out['ACTIVITY']:\n    if i == 3:\n        decision.append('1')\n    else: \n        decision.append('0')\ndf_out['DECISION'] = decision\nprint(df_out.head(10))","d05a2e98":"df_out['DECISION'].value_counts().sort_index()","5a947e91":"# Split the dataset into x and y : x -> Feature variables, y -> Class variable\nX = df_out.iloc[:,1:7]\ny = df_out['DECISION']\nprint(X.shape)\nprint(y.shape)","26f6621d":"print(X.head(10))","87395277":"print(y.head(10))","e8ede1fb":"# Split dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)\n\n# Apply standard scaling to get optimized result\nsc = StandardScaler()\n#sc = MinMaxScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","87bbf7aa":"# Proceed with Modelling. We will use the following algorithms and check for best accuracy:\n\n#  1. Logistic Regression\n#  2. Decision Trees\n#  3. K-Nearest Neighbors\n#  4. Naive Bayes\n#  5. Random Forests\n#  6. Support Vector Machines\n#  7. Stochastic Gradient Decent Classifier","ef5f39ac":"# Perform Logistic Regression Classifier\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\n\n# Print confusion matrix and accuracy score\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nlr_class_report = classification_report(y_test, lr_predict) \nprint(lr_conf_matrix)\nprint('Accuracy Score :', '%.2f' %lr_acc_score)\nprint('Classification Report :')\nprint(lr_class_report)","6d7437bc":"# Perform Decision Trees Classifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ndt_predict = dt.predict(X_test)\n\n# Print confusion matrix and accuracy score\ndt_conf_matrix = confusion_matrix(y_test, dt_predict)\ndt_acc_score = accuracy_score(y_test, dt_predict)\ndt_class_report = classification_report(y_test, dt_predict) \nprint(dt_conf_matrix)\nprint('Accuracy Score :', '%.2f' %dt_acc_score)\nprint('Classification Report :')\nprint(dt_class_report)","a05c6798":"# Perform K-Nearest Neighbors Classifier\nknn = KNeighborsClassifier()\nknn.fit(X_train,y_train)\nknn_predict = knn.predict(X_test)\n\n# Print confusion matrix and accuracy score\nknn_conf_matrix = confusion_matrix(y_test, knn_predict)\nknn_acc_score = accuracy_score(y_test, knn_predict)\nknn_class_report = classification_report(y_test, knn_predict) \nprint(knn_conf_matrix)\nprint('Accuracy Score :', '%.2f' %knn_acc_score)\nprint('Classification Report :')\nprint(knn_class_report)","ad14ff3f":"# Perform Naive Bayes Classifier\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnb_predict = nb.predict(X_test)\n\n# Print confusion matrix and accuracy score\nnb_conf_matrix = confusion_matrix(y_test, nb_predict)\nnb_acc_score = accuracy_score(y_test, nb_predict)\nnb_class_report = classification_report(y_test, nb_predict) \nprint(nb_conf_matrix)\nprint('Accuracy Score :', '%.2f' %nb_acc_score)\nprint('Classification Report :')\nprint(nb_class_report)","d9538ced":"# Perform Random Forest Classifier\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\nrf_predict = rf.predict(X_test)\n\n# Print confusion matrix and accuracy score\nrf_conf_matrix = confusion_matrix(y_test, rf_predict)\nrf_acc_score = accuracy_score(y_test, rf_predict)\nrf_class_report = classification_report(y_test, rf_predict)\nprint(rf_conf_matrix)\nprint('Accuracy Score :','%.2f' %rf_acc_score)\nprint('Classification Report :')\nprint(rf_class_report)","3898ce29":"# Perform SVM Classifier\nsvc = SVC()\nsvc.fit(X_train,y_train)\nsvc_predict = svc.predict(X_test)\n\n# Print confusion matrix and accuracy score\nsvc_conf_matrix = confusion_matrix(y_test, svc_predict)\nsvc_acc_score = accuracy_score(y_test, svc_predict)\nsvc_class_report = classification_report(y_test, svc_predict)\nprint(svc_conf_matrix)\nprint('Accuracy Score :','%.2f' %svc_acc_score)\nprint('Classification Report :')\nprint(svc_class_report)","31222490":"# Perform SGDC\nsgdc = SGDClassifier()\nsgdc.fit(X_train,y_train)\nsgdc_predict = sgdc.predict(X_test)\n\n# Print confusion matrix and accuracy score\nsgdc_conf_matrix = confusion_matrix(y_test, sgdc_predict)\nsgdc_acc_score = accuracy_score(y_test, sgdc_predict)\nsgdc_class_report = classification_report(y_test, sgdc_predict)\nprint(sgdc_conf_matrix)\nprint('Accuracy Score :','%.2f' %sgdc_acc_score)\nprint('Classification Report :')\nprint(sgdc_class_report)","a7ecf23b":"# From the above results it looks like KNN and Random Forest models are giving best accuracy result for our model\n#\n# We will implement following to get the mean accuracy of these two models:\n#       a. K Fold Cross Validation\n#       b. Stratified K Fold Cross Validation","db005c1e":"# K Fold CV with Random Forest classifier\n\nscore = cross_val_score(rf, X, y, cv=10)\nprint(score)\nprint('Mean accuracy :')\nprint('%.2f' %score.mean())","7ea80b3f":"# K Fold CV with KNN classifier\n\nscore = cross_val_score(knn, X, y, cv=10)\nprint(score)\nprint('Mean accuracy :')\nprint('%.2f' %score.mean())","df911607":"# Set up Stratified K Fold Cross Validation with n_splits=10\n\nskf = StratifiedKFold(n_splits=10, random_state=None)\nskf.get_n_splits(X,y)","a09d9a8a":"# SKFCV for Random Forest classifier\n\naccuracy=[]\n\nfor train_index, test_index in skf.split(X, y):\n    #print('Train :' , train_index, 'Test : ', test_index)\n    X1_train, X1_test = X.iloc[train_index], X.iloc[test_index]\n    y1_train, y1_test = y.iloc[train_index], y.iloc[test_index]\n    \n    rf.fit(X1_train, y1_train)\n    prediction = rf.predict(X1_test)\n    score = accuracy_score(prediction, y1_test)\n    accuracy.append(score)\n    \nprint(accuracy)\nprint('Mean accuracy :')\nprint('%.2f' %np.array(accuracy).mean())","65787533":"# SKFCV for K Nearest Neighbour classifier\n\naccuracy=[]\n\nfor train_index, test_index in skf.split(X, y):\n    #print('Train :' , train_index, 'Test : ', test_index)\n    X2_train, X2_test = X.iloc[train_index], X.iloc[test_index]\n    y2_train, y2_test = y.iloc[train_index], y.iloc[test_index]\n    \n    knn.fit(X2_train, y2_train)\n    prediction = knn.predict(X2_test)\n    score = accuracy_score(prediction, y2_test)\n    accuracy.append(score)\n    \nprint(accuracy)\nprint('Mean accuracy :')\nprint('%.2f' %np.array(accuracy).mean())","a717ae26":"From above result we can see there is no null value in any of the columns."}}