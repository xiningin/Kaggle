{"cell_type":{"491cfef2":"code","1a75b7f1":"code","11822826":"code","77284a66":"code","f1089161":"code","6545f524":"code","0e877f8a":"code","a8fa0b58":"code","baf8680c":"code","0fc387f4":"code","f29ea44a":"code","3aa51709":"code","65d084cc":"code","97f1923a":"code","5807cc56":"code","929171a9":"code","5ca55ff8":"code","84a216be":"code","ea0b9ae8":"code","da5c044e":"code","d5024240":"code","4e0ce8be":"code","c1b72a49":"code","7ac316b9":"code","5b21193a":"markdown","7109e49e":"markdown","3174b6bc":"markdown","c17d7ccc":"markdown","4e5e8c61":"markdown","7ea9bc78":"markdown","3cb90ec3":"markdown","a96ba62d":"markdown","28589ae4":"markdown","bfb7f271":"markdown"},"source":{"491cfef2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS \nfrom nltk.corpus import stopwords \nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom textblob import TextBlob\nimport string\nfrom collections import Counter\nimport folium\n# Any results you write to the current directory are saved as output.","1a75b7f1":"auspol_df=pd.read_csv('..\/input\/auspol2019.csv')\nauspol_df.head()","11822826":"#Loading the geocode data\nloc_geo_df=pd.read_csv('..\/input\/location_geocode.csv')\nloc_geo_df.head()","77284a66":"#combine both the dataset on the name column\ntweets_df=auspol_df.merge(loc_geo_df,how='inner',left_on='user_location',right_on='name')\ntweets_df.head()","f1089161":"#checking the data types\ntweets_df.isnull().sum()","6545f524":"#drop the missing values \ntweets_df=tweets_df.dropna()","0e877f8a":"#Drop the unneccessary columns whihch are not helpful in the analysis\ntweets_df=tweets_df.drop(['id','user_id','user_screen_name',\"user_created_at\",\"name\"],axis=1)\ntweets_df.head()","a8fa0b58":"#lets explore created_at column\ntweets_df['created_at'] =  pd.to_datetime(tweets_df['created_at'])\ncnt_srs = tweets_df['created_at'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.7, color='Blue')\nplt.xticks(rotation='vertical')\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=10)\nplt.title(\"Number of tweets according to dates\")\nplt.show()","baf8680c":"tweets_df['created_at'] =  pd.to_datetime(tweets_df['created_at'])\ntweets_df_day=tweets_df.loc[(tweets_df['created_at'].dt.month== 5) & (tweets_df['created_at'].dt.day==18), 'created_at']\ncnt_srs_hour=tweets_df_day.dt.hour.value_counts()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs_hour.index, cnt_srs_hour.values, alpha=0.7, color='Blue')\nplt.xticks(rotation='vertical')\nplt.xlabel('Hour', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=10)\nplt.title(\"Number of tweets hour by hour on 18th May\")\nplt.show()","0fc387f4":"#Daywise Distribution\ntweets_df['created_at'] =  pd.to_datetime(tweets_df['created_at'])\ncnt_srs = tweets_df['created_at'].dt.weekday_name.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.7, color='Blue')\nplt.xticks(rotation='vertical')\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=10)\nplt.title(\"Number of tweets according to day\")\nplt.show()","f29ea44a":"#most favourite and retweeted tweet\nprint(f\" Maximum number of retweets {tweets_df.retweet_count.max()}\")\nprint(f\" Maximum number of favorites {tweets_df.favorite_count.max()}\")","3aa51709":"#checking the tweet which has the maximum retweet count\ntweets_df.loc[tweets_df['retweet_count']==6622.0,['full_text','user_name']].values","65d084cc":"#lets see the tweet which has the maximum retweet count\ntweets_df.loc[tweets_df['favorite_count']==15559.0,['full_text','user_name']].values","97f1923a":"#Top tweeters along with user names\ntweets_df.user_name.value_counts()[:10,]","5807cc56":"plt.figure(figsize=(14,7))\ncnt_user_location = tweets_df['user_location'].value_counts()\ncnt_user_location.reset_index()\ncnt_user_location = cnt_user_location[:10,]\nsns.barplot(cnt_user_location.index, cnt_user_location.values,data=tweets_df)\nplt.xticks(rotation='vertical')\nplt.show()","929171a9":"#using the latitudes and longtiudes to determine the exact location of the user\ntweets_df['count'] = 1\ntweets_df[['lat', 'long', 'count']].groupby(['lat', 'long']).sum().sort_values('count', ascending=False).head(10)","5ca55ff8":"def generateBaseMap(default_location=[-25.734968,134.489563], default_zoom_start=4):\n    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n    return base_map","84a216be":"from folium.plugins import HeatMap\nbase_map = generateBaseMap()\nHeatMap(data=tweets_df[['lat', 'long', 'count']].groupby(['lat', 'long']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(base_map)\nbase_map","ea0b9ae8":"stop = set(stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = WordNetLemmatizer()\ndata_samples=tweets_df['user_description']\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean = [clean(doc).split() for doc in data_samples]  \nnorm_doc = [clean(doc) for doc in data_samples]","da5c044e":"plt.figure(figsize=(14,6))\nwordcloud = WordCloud().generate(str(norm_doc))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","d5024240":"#wword cloud on the full text\ndata_samples1=tweets_df['full_text']\ndoc_clean = [clean(doc).split() for doc in data_samples1]  \nnorm_doc = [clean(doc) for doc in data_samples1]\nplt.figure(figsize=(14,6))\nwordcloud = WordCloud().generate(str(norm_doc))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","4e0ce8be":"# from tetxblob import TextBlob\ntweets_df['sentiment'] = data_samples1.apply(lambda tweet: TextBlob(tweet).sentiment.polarity)","c1b72a49":"#mapping the polarity values to different labels\ntweets_df['Polarity']=pd.cut(tweets_df['sentiment'],[-np.inf, -.01, .01, np.inf],\n                             labels=['Negative','Neutral','Positive'])\ntweets_df[['sentiment','Polarity']].head(10)","7ac316b9":"norm_corpus_df = pd.DataFrame({'Document': data_samples1, \n                          'Category': tweets_df['Polarity']})\nnorm_corpus_df = norm_corpus_df[['Document', 'Category']]\nnorm_corpus_df.head()","5b21193a":"Since the election were held on the 18th of the may in australia we can see large number of tweets around 5000 when compared to the other days","7109e49e":"* We can see that words like view,own,social,justice were mostly effecting the user description and the words ausspol,ausvote are the most occured word in the tweets","3174b6bc":"we can see large volume of tweets were occured in the 9 to 12 in the morning on the day of elections","c17d7ccc":"The dataset comprises of over 180,000 tweets on australian elections between the time 10.05.2019 and 20.05.2019.The dataset columns are\n\n* created_at: Date and time of tweet creation\n* id: Unique ID of the tweet\n* full_text: Full tweet text\n* retweet_count: Number of retweets\n* favorite_count: Number of likes\n* user_id: User ID of tweet creator\n* user_name: Username of tweet creator\n* user_screen_name: Screen name of tweet creator\n* user_description: Description on tweet creator's profile\n* user_location: Location given on tweet creator's profile\n* user_location: Location given on tweet creator's profile \n* And loacation_geocode.csv contains latitude and longitude of the user.","4e5e8c61":"We can observe that maximum retweet count and the most favourite tweet are same which is made by sara A.carter","7ea9bc78":"**Normalize the data and apply the word cloud on user description**","3cb90ec3":"Here also we can observe that larger number of tweets were made on the saturday which the election day.","a96ba62d":"**sentiment analysis using text blob**","28589ae4":"Let look at the more granular level at what time most of the tweets were occured on 18th of may","bfb7f271":"# Australian elections EDA"}}