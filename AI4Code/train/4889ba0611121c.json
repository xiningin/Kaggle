{"cell_type":{"6ad2f218":"code","ff742f74":"code","20ad5a71":"code","8c983e5c":"code","83c47877":"code","29d5673f":"code","9c9e350c":"code","ea895550":"code","951bfdef":"code","526d0ee7":"code","7cf25410":"code","f4e57a95":"code","cfda9252":"code","aaad0766":"code","5ea33b0f":"code","b9d1107d":"code","c2a07b58":"code","dfb2cde6":"code","8035d98a":"code","295ffda9":"code","da491ebb":"code","a030857d":"code","06507b1a":"code","f425fa05":"code","695b2060":"code","bf45c236":"code","9fff8ebd":"code","4cf77053":"code","a153cf88":"code","5c415ace":"code","2c69d6a8":"code","8daec278":"code","b5d2aa89":"code","f047ce74":"code","93f4872f":"code","4e1dfa56":"code","88a55656":"code","8d4f199c":"code","4a954e63":"code","945bcb1d":"code","e5e0e52d":"code","519a33cc":"code","c0cb6768":"code","30c6cea6":"code","4cf8abe2":"code","c7260ec8":"code","2405aaf3":"code","27ad7849":"code","cd99f412":"code","2b119913":"code","3a0aa0c8":"code","744b938c":"code","d26da8e1":"code","a7935a0d":"code","14a66c33":"code","4c22e3a4":"code","7c1a3c3c":"code","2b5f9a42":"code","6fcae6d7":"code","ae44b1a1":"code","e4b77dcd":"code","14a54105":"code","4d06f219":"code","810f2875":"code","0b1a2152":"code","fbafe983":"code","72c6725c":"code","5e2e70af":"code","c3d09a4d":"code","cc253cca":"code","160e1e62":"markdown","496910db":"markdown","87615b46":"markdown","4139a802":"markdown","0fdae288":"markdown","53e7903b":"markdown","b20ee300":"markdown","7a6be875":"markdown","28357783":"markdown","f99ee8c9":"markdown","215ed7f0":"markdown","daab9b85":"markdown","e6ae9c90":"markdown","08381568":"markdown","e66a31ec":"markdown","e5756483":"markdown","4f188355":"markdown","be700d39":"markdown","e63385e8":"markdown","c0837562":"markdown","a0e16206":"markdown","3e21179f":"markdown","3d5e0401":"markdown","ab5289de":"markdown","ceb73638":"markdown","8e9c69b6":"markdown","fbdf6bf7":"markdown","14eab3f2":"markdown","746dc6e7":"markdown","8606b71e":"markdown","bc58aa5d":"markdown","338e8040":"markdown","ad924a01":"markdown","b6a81524":"markdown","1e7fdd33":"markdown","bcaa89ef":"markdown","6983423b":"markdown","eacf34f9":"markdown","b637e806":"markdown","64384a47":"markdown","bf9cc5b6":"markdown","56c72667":"markdown","a7fbcca6":"markdown","368614f9":"markdown","9057b64b":"markdown","88f546c4":"markdown","3a0ea698":"markdown","11c5158b":"markdown","055ab049":"markdown","189db6be":"markdown","1c8d5255":"markdown","7ca8a59a":"markdown","38b90c0f":"markdown","31bbfad0":"markdown","7b302e6f":"markdown","6ee47e2d":"markdown","be85b295":"markdown","51dd50d0":"markdown"},"source":{"6ad2f218":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff742f74":"# importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics as ms\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost.sklearn import XGBClassifier\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import RandomizedSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier","20ad5a71":"#Train Data\n\ntrain_data = pd.read_csv('\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntrain_data.head(5)","8c983e5c":"# Test Data\n\ntest_data = pd.read_csv('\/kaggle\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')\ntest_data.head(5)","83c47877":"print(train_data.shape)\nprint(test_data.shape)","29d5673f":"train_data.info()","9c9e350c":"test_data.info()","ea895550":"train_data.isnull().sum()","951bfdef":"test_data.isnull().sum()","526d0ee7":"Categorical_Feature = ['Loan_ID', 'Gender' , 'Married' , 'Dependents' , 'Education' , 'Self_Employed' , 'Loan_Amount_Term' , 'Property_Area', 'Credit_History']\nNumerical_Feature = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n\nfor i in Categorical_Feature:\n  train_data[i].fillna(train_data[i].mode()[0], inplace=True)\n  test_data[i].fillna(test_data[i].mode()[0], inplace=True)\n\nfor j in Numerical_Feature:\n  train_data[j] = train_data[j].replace(np.nan , train_data[j].median())\n  test_data[j] = test_data[j].replace(np.nan , test_data[j].median())\n\nprint(\"count of null values for train_data\")\nprint(train_data.isnull().sum())\nprint(\" \")\nprint(\"Count of null values for test_data\")\nprint(test_data.isnull().sum())","7cf25410":"Objecttype_feature_list = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n\nprint(\"Unique value in train_data\")\nfor i in Objecttype_feature_list:\n  print(\"Count of unique values in column \", i, \"is\", train_data[i].nunique(), \"which are\", train_data[i].unique())\n\nprint(\" \")\nprint(\" \")\n\nprint(\"Unique value in test_data\")\nfor i in Objecttype_feature_list:\n  print(\"count of unique values in column \", i, \"is\", test_data[i].nunique(), \"which are\", test_data[i].unique())\n","f4e57a95":"train_data_LoanID = train_data['Loan_ID']\ntrain_data = train_data.drop(columns=['Loan_ID'])\ntrain_data_encoded = pd.get_dummies(train_data,drop_first=True)\n\ntest_data_LoanID = test_data['Loan_ID']\ntest_data = test_data.drop(columns=['Loan_ID'])\ntest_data_encoded = pd.get_dummies(test_data,drop_first=True)\n","cfda9252":"train_data_encoded.columns","aaad0766":"test_data_encoded.columns","5ea33b0f":"# saving loan status and dropping it from train dataset.\ntrain_Loan_status=train_data_encoded['Loan_Status_Y']\ntrain_data_encoded=train_data_encoded.drop('Loan_Status_Y',axis=1)","b9d1107d":"eda_train_data_encoded = train_data_encoded\neda_test_data_encoded = test_data_encoded","c2a07b58":"eda_train_data_encoded.head(2)","dfb2cde6":"eda_test_data_encoded.head(2)","8035d98a":"eda_train_data_encoded.describe()","295ffda9":"# checking skewness\n\nfor i in eda_train_data_encoded:\n  print('skewness of',i,'is',eda_train_data_encoded[i].skew())","da491ebb":"eda_train_data_encoded['Loan_Status_Y'] = train_Loan_status","a030857d":"eda_train_data_encoded['Loan_Status_Y'].value_counts()","06507b1a":"eda_train_data_encoded['Loan_Status_Y'].hist(grid = False)","f425fa05":"eda_train_data_encoded['Education_Not Graduate'].hist(grid = False)","695b2060":"counts, bin_edges = np.histogram(eda_train_data_encoded['ApplicantIncome'], bins=20, density = True)\npdf = counts\/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['ApplicantIncome'], bins=20, density = True)\npdf_1 = counts_1\/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of ApplicantIncome')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","bf45c236":"counts, bin_edges = np.histogram(eda_train_data_encoded['LoanAmount'], bins=20, density = True)\npdf = counts\/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['LoanAmount'], bins=20, density = True)\npdf_1 = counts_1\/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of LoanAmount')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","9fff8ebd":"counts, bin_edges = np.histogram(eda_train_data_encoded['Loan_Amount_Term'], bins=20, density = True)\npdf = counts\/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['Loan_Amount_Term'], bins=20, density = True)\npdf_1 = counts_1\/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of Loan_Amount_Term')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","4cf77053":"eda_train_data_encoded['Loan_Amount_Term'].value_counts().sort_values().plot(kind = 'barh')\nplt.show()","a153cf88":"counts, bin_edges = np.histogram(eda_train_data_encoded['CoapplicantIncome'], bins=20, density = True)\npdf = counts\/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['CoapplicantIncome'], bins=20, density = True)\npdf_1 = counts_1\/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of CoapplicantIncome')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","5c415ace":"sns.countplot('Loan_Status_Y', hue='Education_Not Graduate', data=eda_train_data_encoded)\nplt.xlabel(\"Education_Not Graduate and Loan_Status_Y\")\nplt.ylabel(\"Count\")\n\nplt.title(\"count plot for Education_Not Graduate w.r.t. Loan_Status_Y\")\nplt.show()","2c69d6a8":"sns.countplot('Loan_Status_Y', hue='Gender_Male', data=eda_train_data_encoded)\nplt.xlabel(\"Gender_Male and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Gender_Male w.r.t. Loan_Status_Y\")\nplt.show()","8daec278":"sns.countplot('Loan_Status_Y', hue='Self_Employed_Yes', data=eda_train_data_encoded)\nplt.xlabel(\"Self_Employed_Yes and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Self_Employed_Yes w.r.t. Loan_Status_Y\")\nplt.show()","b5d2aa89":"sns.violinplot(x=\"Loan_Status_Y\", y=\"ApplicantIncome\", data=eda_train_data_encoded, size=8)\nplt.show()","f047ce74":"np.mean(eda_train_data_encoded['ApplicantIncome'])","93f4872f":"plt.title('box plot')\nsns.boxplot(x='Loan_Status_Y',y='LoanAmount', data=eda_train_data_encoded)\nplt.show()","4e1dfa56":"sns.violinplot(x=\"Loan_Status_Y\", y=\"LoanAmount\", data=eda_train_data_encoded, size=8)\nplt.show()","88a55656":"sns.countplot('Loan_Status_Y', hue='Married_Yes', data=eda_train_data_encoded)\nplt.xlabel(\"Married_Yes and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Married_Yes w.r.t. Loan_Status_Y\")\nplt.show()","8d4f199c":"sns.countplot('Loan_Status_Y', hue='Credit_History', data=eda_train_data_encoded)\nplt.xlabel(\"Credit_History and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Credit_History w.r.t. Loan_Status_Y\")\nplt.show()","4a954e63":"plt.scatter(eda_train_data_encoded['LoanAmount'], eda_train_data_encoded['Loan_Amount_Term'])\nplt.show()","945bcb1d":"plt.scatter(eda_train_data_encoded['LoanAmount'], eda_train_data_encoded['ApplicantIncome'])\nplt.show()","e5e0e52d":"Norm_train_data = train_data_encoded\nNorm_test_data = test_data_encoded","519a33cc":"\nlst = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']\nfor i in lst:\n  Norm_train_data[i]=((Norm_train_data[i]-Norm_train_data[i].min())\/(Norm_train_data[i].max()-Norm_train_data[i].min()))\n  Norm_test_data[i]=((Norm_test_data[i]-Norm_test_data[i].min())\/(Norm_test_data[i].max()-Norm_test_data[i].min()))\n","c0cb6768":"Norm_train_data.head(5)","30c6cea6":"Norm_test_data.head(2)","4cf8abe2":"after_eda_train_data = Norm_train_data\nafter_eda_test_data = Norm_test_data","c7260ec8":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='ApplicantIncome')\nplt.show()","2405aaf3":"IQR = (np.percentile(after_eda_train_data['ApplicantIncome'], 75)) - (np.percentile(after_eda_train_data['ApplicantIncome'], 25))\nmin = ((np.percentile(after_eda_train_data['ApplicantIncome'], 25)) - 1.5 * IQR)\nmax = ((np.percentile(after_eda_train_data['ApplicantIncome'], 75)) + 1.5 * IQR)\n  \nfor j in range(len(after_eda_train_data['ApplicantIncome'])):\n  if (after_eda_train_data['ApplicantIncome'][j]>=max):\n    after_eda_train_data['ApplicantIncome'][j]=max\n  elif (after_eda_train_data['ApplicantIncome'][j]<=min):\n    after_eda_train_data['ApplicantIncome'][j]=min\n","27ad7849":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='ApplicantIncome')\nplt.show()","cd99f412":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='LoanAmount')\nplt.show()","2b119913":"IQR = (np.percentile(after_eda_train_data['LoanAmount'], 75)) - (np.percentile(after_eda_train_data['LoanAmount'], 25))\nmin = ((np.percentile(after_eda_train_data['LoanAmount'], 25)) - 1.5 * IQR)\nmax = ((np.percentile(after_eda_train_data['LoanAmount'], 75)) + 1.5 * IQR)\n  \nfor j in range(len(after_eda_train_data['LoanAmount'])):\n  if (after_eda_train_data['LoanAmount'][j]>=max):\n    after_eda_train_data['LoanAmount'][j]=max\n  elif (after_eda_train_data['LoanAmount'][j]<=min):\n    after_eda_train_data['LoanAmount'][j]=min\n","3a0aa0c8":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='LoanAmount')\nplt.show()","744b938c":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='CoapplicantIncome')\nplt.show()","d26da8e1":"IQR = (np.percentile(after_eda_train_data['CoapplicantIncome'], 75)) - (np.percentile(after_eda_train_data['CoapplicantIncome'], 25))\nmin = ((np.percentile(after_eda_train_data['CoapplicantIncome'], 25)) - 1.5 * IQR)\nmax = ((np.percentile(after_eda_train_data['CoapplicantIncome'], 75)) + 1.5 * IQR)\n  \nfor j in range(len(after_eda_train_data['CoapplicantIncome'])):\n  if (after_eda_train_data['CoapplicantIncome'][j]>=max):\n    after_eda_train_data['CoapplicantIncome'][j]=max\n  elif (after_eda_train_data['CoapplicantIncome'][j]<=min):\n    after_eda_train_data['CoapplicantIncome'][j]=min\n","a7935a0d":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='CoapplicantIncome')\nplt.show()","14a66c33":"after_eda_train_data.corr()","4c22e3a4":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"var\"] = after_eda_train_data.columns\nvif[\"VIF\"] = [variance_inflation_factor(after_eda_train_data.values, i) for i in range(after_eda_train_data.shape[1])]\nvif2=vif.sort_values(by=['VIF'], ascending=False)\nvif2.reset_index(inplace = True)\nprint(vif2)","7c1a3c3c":"m_train_data = after_eda_train_data\nm_test_data = after_eda_test_data","2b5f9a42":"y = m_train_data['Loan_Status_Y']","6fcae6d7":"m_train_data = m_train_data.drop('Loan_Status_Y', axis=1)","ae44b1a1":"m_train_data.head()","e4b77dcd":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(m_train_data, y, stratify=y, test_size=0.2, random_state=1)\nprint(train_X.shape, val_X.shape, train_y.shape, val_y.shape )","14a54105":"from sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier","4d06f219":"clf = CatBoostClassifier(loss_function='MultiClass', depth=5, iterations= 350, l2_leaf_reg= 1, learning_rate= 0.02)\nclf.fit(train_X,train_y)\n\ntr_pred=clf.predict(train_X)\nval_pred=clf.predict(val_X)\nprint (\"Accuracy for train is for \",ms.accuracy_score(train_y,tr_pred))\nprint (\"Accuracy for val is for \",ms.accuracy_score(val_y,val_pred))\n\ncatboost_val_acc = ms.accuracy_score(val_y,val_pred)","810f2875":"confusion_matrix(val_y,val_pred)","0b1a2152":"# predicting test data\n\ny_test_pred=clf.predict(m_test_data)\ntest_sub=pd.DataFrame(y_test_pred,columns=['Loan_Status'])\ntest_sub['Loan_ID']=test_data_LoanID","fbafe983":"test_sub['Loan_Status']= test_sub['Loan_Status'].map({0: 'N' , 1: 'Y'})\ntest_sub=test_sub[['Loan_ID','Loan_Status']]\n\ntest_sub.head(2)","72c6725c":"#test_sub.to_csv('catbboost_submission.csv',index=False)\n#The test score for catboost classifier is : 0.7847222222222222.\n","5e2e70af":"# Always scale the input. The most convenient way is to use a pipeline.\n\nalph=[0.001,0.01,0.1,1,10,100]\nacc=[]\n\nfor i in alph:\n  clf = make_pipeline(StandardScaler(), SGDClassifier(loss='log', max_iter=1000, tol=1e-3, class_weight=\"balanced\", alpha=i ))\n  clf.fit(train_X, train_y)\n  val_pred=clf.predict(val_X)\n  acc.append(ms.accuracy_score(val_y,val_pred))\n  print (\"Accuracy for \",i, ms.accuracy_score(val_y,val_pred))\n  \nsgd_val_acc = np.max(acc)\nprint(sgd_val_acc)","c3d09a4d":"r_cfl=RandomForestClassifier(random_state=42,n_jobs=-1)\nr_cfl=RandomForestClassifier(n_estimators=40,random_state=42,n_jobs=-1, max_depth=4)\nr_cfl.fit(train_X, train_y)\ntr_pred=r_cfl.predict(train_X)\nval_pred=r_cfl.predict(val_X)\nprint (\"Accuracy for train is for \",i, ms.accuracy_score(train_y,tr_pred))\nprint (\"Accuracy for val is for \",i, ms.accuracy_score(val_y,val_pred))\n\nrandom_forest_val_acc = ms.accuracy_score(val_y,val_pred)","cc253cca":"df=pd.DataFrame(data=[['CatBoostClassifier Model',catboost_val_acc], ['SGDClassifier',sgd_val_acc], ['RamdomForestClassifier',random_forest_val_acc]], columns=['Model','Validation accuracy'])\ndf","160e1e62":"Here we can observe that we have positive correlation between LoanAmount and ApplicantIncome.","496910db":"CoapplicantIncome","87615b46":"From above graph we can say that our data is imbalanced.","4139a802":"Loan_Amount_Term","0fdae288":"ApplicantIncome","53e7903b":"### correlation Matrix","b20ee300":"LoanAmount and ApplicantIncome","7a6be875":"Credit_History","28357783":"As we can see that Education column have impact on Loan_Status. People, who are\neducated have higher chance of getting the loan. ","f99ee8c9":"LoanAmount","215ed7f0":"\n*   High skewness means -->  skewness is less than -1 or skewnwss is greater that 1\n\n*   Moderate skewness means --> skewness is in range [-1,-0.5] or skewness is in range [0.5,1].\n\n*   nearly symmetric means --> skewness is in range [-0.5,0.5]\n\nObservations:\n\nHighly skewed features are - Gender_Male,, Dependents_1, Dependents_2, Dependents_3+, Education_Not Graduate, Self_Employed, ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History\n\n\nModerate skewed features are - Married_Yes, Property_Area_Urban\n\n\nsymmetric skewness fatures are - Property_Area_Semiurban\n","daab9b85":"### univariate analysis","e6ae9c90":"## Handling Null Values","08381568":"LoanAmount","e66a31ec":"### Bivariate Analysis","e5756483":"As we can see the validation accuracy of catboost is greater than the accuracy of SGD Classifier.","4f188355":"The Cedit_History for 1, which means meeting guidelines have higher chances of loan approval","be700d39":"\nAs we have already seen in the scatter plat - EDA part, there is little correlation between LoanAmount and Loan_Amount_Term. \nHere also we can see vif for LoanAmount is higher.\nBut We cannot remove LoanAmount feature because it is important for our modeling.","e63385e8":"Null value columns in train dataset are :\n\n['Gender' , 'Married' , 'Dependents' ,\n'Self_Employed' , 'LoanAmount' , 'Loan_Amount_Term' , 'Credit_History']","c0837562":"CoapplicantIncome","a0e16206":"As we can see that both train_data and test_data dist plot looks similar, means having similar kind of distributions","3e21179f":"As we can see that range is similar for Loan_Status 'Y' and 'N'. and We have outliers in LoanAmount. ","3d5e0401":"## SGD Classifier","ab5289de":"## Ramdom Forest","ceb73638":"## EDA","8e9c69b6":"\nHere we are replacing null values of categorical_feature with mode and null values of numerical_feature with median.","fbdf6bf7":"Null Value columns in test dataset are :\n\n ['Gender' , 'Dependents' , 'Self_Employed' , 'LoanAmount' , 'Loan_Amount_Term' , 'Credit_History']","14eab3f2":"ApplicantIncome","746dc6e7":"## Normalization","8606b71e":"# Feature Engineering ","bc58aa5d":"As we can see the chances of married people is high in Loan approval ","338e8040":"unique values for categorical features in train and test datasets\n\n","ad924a01":"## CatBoost","b6a81524":"{'Not Graduate': 1 , 'Graduate': 0}, as we can see count of 0 is greater than \ncount of 1 - means no of people who applied for Loan are mostly Educated people.\n","1e7fdd33":"### VIF (Variance Inflation factor)","bcaa89ef":"Here we can observe that there is little correlation between LoanAmount and Loan_Amount_Term. For less no of months we have less Loan Amount","6983423b":"Married_Yes - {'No': 0 , 'Yes': 1}","eacf34f9":"Applicant Income","b637e806":"Education_Not Graduate {'Not Graduate': 1 , 'Graduate': 0}","64384a47":"\nThe describe() function is used to generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. Refer https:\/\/www.w3resource.com\/pandas\/dataframe\/dataframe-describe.php#:~:text=DataFrame%20%2D%20describe()%20function,dataset's%20distribution%2C%20excluding%20NaN%20values.&text=The%20percentiles%20to%20include%20in,fall%20between%200%20and%201.\n","bf9cc5b6":"Catboost is the best model of all.","56c72667":"LoanAmount","a7fbcca6":"As we can see that both train_data and test_data dist plot looks similar, means having similar kind of distributions. \nwe can see that the graph follows the power law distributions \nVery few people have higher Income","368614f9":"## Encoding","9057b64b":"As we can see that most of the people have income in very short range. The graph is looking like Gaussian with skewness. Values are centered around 5403","88f546c4":"LoanAmount and Loan_Amount_Term","3a0ea698":"# Summary","11c5158b":"## EDA and Outliers ","055ab049":"# Data preprocessing","189db6be":"Gender_Male - {'Male': 1 , 'Female': 0}","1c8d5255":"As we can see that Gender column has an impact on Loan_Status. Count of male is greater than the count of female for Loan_status_Y. ","7ca8a59a":"As we can see that both train_data and test_data dist plot looks similar, means having similar kind of distributions","38b90c0f":"As we can see the counts, our dataset is imbalanced dataset. Count of 1' is nearly twice as of 0","31bbfad0":"Self_Employed_Yes - {'No': 0 , 'Yes': 1}","7b302e6f":"# Model","6ee47e2d":"For Encoding I am using one hot encoding because the count of unique values are less than 5.","be85b295":"As we can see that count of non-self_Employed is much greater than the self_employed, means people who are self_Employed have lower chance of getting the Loan","51dd50d0":"Most People take Term of loan for 360 Months."}}