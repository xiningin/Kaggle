{"cell_type":{"18f4ec98":"code","3b607d4a":"code","f60f0cb2":"code","585af492":"code","3e0d16a3":"code","94daeb39":"code","378f5592":"code","98813612":"code","f04b9594":"code","aad59754":"code","6432f438":"code","5128b4e1":"code","b8fd9d0f":"code","a9056f22":"code","ea70a1e3":"code","3eac8cea":"code","f53592b8":"code","fac24eef":"code","9fae9bef":"code","5ebba61f":"code","f5fa253f":"code","c1a20fd1":"code","c2e9105a":"code","ea58a5ea":"code","ea8377fe":"code","fda24a5b":"markdown","882ce839":"markdown","974ae3fe":"markdown","a8b40236":"markdown","0876ca73":"markdown","08dc79c2":"markdown","afcbee3c":"markdown","f7f96738":"markdown","15a61c94":"markdown","78c80144":"markdown","17158ba8":"markdown","ead4617d":"markdown","c8a957a9":"markdown","672e1e42":"markdown","968b0e60":"markdown","6c2bbc6e":"markdown","44d3ef95":"markdown","a75ae67c":"markdown"},"source":{"18f4ec98":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRS=81","3b607d4a":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","f60f0cb2":"data=pd.concat([train, test], sort = True).reset_index(drop=True)\ndata.info()","585af492":"def replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col', 'Sir']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme', 'Lady', 'Dona']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n    \ndata['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndata['Title']=data.apply(replace_titles, axis=1)","3e0d16a3":"data.groupby(['Title', 'Pclass'])['Age'].median()","94daeb39":"data['Age']=data.groupby(['Title', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\ndata['Fare'].fillna(data['Fare'].median(), inplace=True)","378f5592":"data['Family_Size']=data['SibSp']+data['Parch']\ndata['Alone']=[1 if x==0 else 0 for x in data['SibSp']+data['Parch']]","98813612":"data['Surname'] =data.Name.str.extract(r'([A-Za-z]+),', expand=False)\ndf=data[data.duplicated(['Ticket','Surname'], keep=False)]\n\ndfg=df.groupby(['Ticket','Surname'])[['Survived']].max().fillna(0.5)\ndata=pd.merge(data, dfg.rename(columns={'Survived':'FamSurvived'}), how='left', on=['Ticket','Surname'])\ndata['FamSurvived'].fillna(0.5, inplace=True)","f04b9594":"data['Age_T']=PowerTransformer().fit_transform(data[['Age']])\ndata['Fare_T']=PowerTransformer().fit_transform(data[['Fare']])","aad59754":"data = pd.get_dummies(data, columns=['Title'], drop_first=True)","6432f438":"train = data[:len(train)]\ntest = data[len(train):].drop(['Survived'],axis=1)","5128b4e1":"data.groupby(['Sex', 'Pclass'])['Survived'].mean()","b8fd9d0f":"train_1=train.loc[(train.Sex=='male')]\ntest_1=test.loc[(test.Sex=='male')]\n\ntrain_2=train.loc[((train.Pclass<=2) & (train.Sex=='female'))]\ntest_2=test.loc[((test.Pclass<=2) & (test.Sex=='female'))]\n\ntrain_3=train.loc[((train.Pclass>2) & (train.Sex=='female'))]\ntest_3=test.loc[((test.Pclass>2) & (test.Sex=='female'))]","a9056f22":"col_final=['Age_T', 'Fare_T', 'Alone', 'Family_Size', 'FamSurvived', 'Title_Miss', 'Title_Mr', 'Title_Mrs']","ea70a1e3":"param_lgb ={'n_estimators': [100, 500, 1000, 2000],\n             'max_depth':[1,2,3,4,5],\n             'num_leaves': [2,4,6,8,10], \n             'min_child_samples ': [2,5,10,15,20], \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample':[0.4,0.5,0.6,0.7,0.8,0.9,1],\n             'colsample_bytree':[0.4,0.5,0.6,0.7,0.8,0.9,1],\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\n\nparam_log={'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 3, 5, 7, 10, 15, 20, 25, 30, 50], \n                 'penalty':['l1', 'l2', 'elasticnet', 'none'],\n                 'class_weight': [1, 3, 10],\n                 'max_iter': [10000]}\n\n\nparam_rf=  {'n_estimators': [10 ,25,50,100,125,250,500,1000],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [1,2,3,4,5,10],\n               'min_samples_split': [2, 5, 10, 15, 25],\n               'min_samples_leaf': [1, 2, 5, 10]}\n\nparamsStack= {'final_estimator__multi_class':['ovr'],\n              'final_estimator__max_iter':[10000], \n                     'final_estimator__solver':['saga'],\n                     'final_estimator__penalty':['l1', 'l2', 'elasticnet', 'none'],\n                     'final_estimator__l1_ratio':[0.5],\n                     'final_estimator__C':np.linspace(start = 0.0001, stop = 10, num = 100)}\n\nsearchParamsModels={'n_iter':50,'scoring':'accuracy','cv':10,'refit':True,'random_state':RS,'verbose':0,'n_jobs':-1}\n\nclf_lgb=lgb.LGBMClassifier(random_state=RS, silent=True, metric='None')\nclf_log=LogisticRegression(random_state = RS, solver='saga', l1_ratio=0.5)\nclf_rf=RandomForestClassifier(random_state = RS)\nstackM=LogisticRegression(random_state=RS)\n\nl_param=[param_log,param_rf,param_lgb]\nl_clf=[clf_log,clf_rf,clf_lgb]","3eac8cea":"def tune_stack_predict(X_train,y_train,X_test,listModels,listSearchParamM,stackModel,searchParamsModels,listSearchParamS):\n    models_local=[]\n    X_test_c=X_test.copy()\n    \n    for clf, param in zip(listModels,listSearchParamM):\n        #search model param\n        rs_clf=RandomizedSearchCV(estimator=clf, param_distributions=param, **searchParamsModels)\n        rs_clf.fit(X_train, y_train)\n        \n        #model name first 5 char\n        rs_clf_name=type(clf).__name__[0:5]\n        \n        #pred test \n        y_pred=rs_clf.best_estimator_.predict(X_test)\n        \n        #add pred to test\n        X_test_c[rs_clf_name]=np.asarray(y_pred)\n        \n        #add best model to list for stack and return\n        models_local.append([rs_clf_name,rs_clf.best_estimator_])\n        \n     \n    #init stack\n    stack_clf=StackingClassifier(estimators=models_local, final_estimator=stackModel,cv=10, verbose=0)\n    \n    #search stack param\n    rs_stack=RandomizedSearchCV(estimator=stack_clf, param_distributions=listSearchParamS, **searchParamsModels)\n    rs_stack.fit(X_train,y_train)\n    \n    #stack name first 5 char\n    rs_stack_name=type(stack_clf).__name__[0:5]\n  \n    #add stack pred to test\n    y_pred=rs_stack.best_estimator_.predict(X_test)\n    X_test_c[rs_stack_name]=np.asarray(y_pred)\n\n    #add stack to list for return\n    models_local.append([rs_stack_name,rs_stack.best_estimator_])\n    \n    return X_test_c, models_local","f53592b8":"#x1,m1=tune_stack_predict(train_1[col_final], train_1['Survived'],test_1[col_final],l_clf,l_param,stackM,searchParamsModels,paramsStack)\n#x3,m3=tune_stack_predict(train_3[col_final], train_3['Survived'],test_3[col_final],l_clf,l_param,stackM,searchParamsModels,paramsStack)\n","fac24eef":"#m1[3][1].get_params(False)","9fae9bef":"#m3[3][1].get_params(False)","5ebba61f":"\nparam_meta1={'cv': 10,\n 'estimators': [['Logis',\n   LogisticRegression(C=0.4, class_weight=10, l1_ratio=0.5, max_iter=10000,\n                      penalty='l1', random_state=81, solver='saga')],\n  ['Rando',\n   RandomForestClassifier(max_depth=3, min_samples_leaf=5, min_samples_split=5,\n                          n_estimators=25, random_state=81)],\n  ['LGBMC',\n   LGBMClassifier(colsample_bytree=1, max_depth=3, metric='None',\n                  min_child_samples =10, min_child_weight=10.0, n_estimators=500,\n                  n_jobs=4, num_leaves=6, random_state=81, reg_alpha=2,\n                  reg_lambda=50, subsample=0.6)]],\n 'final_estimator': LogisticRegression(C=3.3334, l1_ratio=0.5, max_iter=10000, multi_class='ovr',\n                    random_state=81, solver='saga'),\n 'n_jobs': None,\n 'passthrough': False,\n 'stack_method': 'auto',\n 'verbose': 0}\n\nparam_meta3={'cv': 10,\n 'estimators': [['Logis',\n   LogisticRegression(C=30, class_weight=1, l1_ratio=0.5, max_iter=10000,\n                      penalty='none', random_state=81, solver='saga')],\n  ['Rando',\n   RandomForestClassifier(max_depth=10, max_features='sqrt', min_samples_leaf=2,\n                          min_samples_split=15, n_estimators=1000,\n                          random_state=81)],\n  ['LGBMC',\n   LGBMClassifier(colsample_bytree=0.5, max_depth=1, metric='None',\n                  min_child_samples =15, min_child_weight=0.01, n_estimators=1000,\n                  n_jobs=4, num_leaves=6, random_state=81, reg_alpha=0,\n                  reg_lambda=50, subsample=0.6)]],\n 'final_estimator': LogisticRegression(C=3.3334, l1_ratio=0.5, max_iter=10000, multi_class='ovr',\n                    random_state=81, solver='saga'),\n 'n_jobs': None,\n 'passthrough': False,\n 'stack_method': 'auto',\n 'verbose': 0}","f5fa253f":"stack_clf1=StackingClassifier(**param_meta1)\nstack_clf3=StackingClassifier(**param_meta3)","c1a20fd1":"stack_clf1.fit(train_1[col_final],train_1['Survived'])\nstack_clf3.fit(train_3[col_final],train_3['Survived'])","c2e9105a":"y_pred_1=stack_clf1.predict(test_1[col_final])\ny_pred_3=stack_clf3.predict(test_3[col_final])","ea58a5ea":"y_pred_1=pd.DataFrame({'PassengerId':test_1['PassengerId'],'Survived':y_pred_1 }, dtype=int)\ny_pred_2=pd.DataFrame({'PassengerId':test_2['PassengerId'],'Survived':1 }, dtype=int)\ny_pred_3=pd.DataFrame({'PassengerId':test_3['PassengerId'],'Survived':y_pred_3 }, dtype=int)","ea8377fe":"y_sub=pd.concat([y_pred_1,y_pred_2,y_pred_3], axis=0).sort_values('PassengerId')\ny_sub.to_csv(\"sub.csv\",index=False)","fda24a5b":"* **Add title feature**\n<br>Just extract and assign Title from Name","882ce839":"* **Impute Age, Fare**\n<br>Imputing Age by Title and Pclass is more precisely\n<br>Only one missing value for fare, so just use median","974ae3fe":"## **Loading datasets**","a8b40236":"* **Add FamSurvived**\n<br>Saw this feature on a lot of notebooks. The idea is to check, if other family members actually survived. \n    * 0: no family member survived\n    * 1: at least one survived\n    * 0.5: unknown or alone","0876ca73":"* **Keep final features**","08dc79c2":"* **Tune, train, stack and predict**\n<br>No need to train a model for class 2[](http:\/\/) since survival in this class seems almost certain\n<br>Comment out, cause this takes some time...","afcbee3c":"## Training","f7f96738":"## Feature engineering\n* Add extra features\n* Impute missing values\n* Power transform features\n* Encode categorical features","15a61c94":"* train_1: men (including childs)\n* train_2: women Pclass 1+2\n* train_3: women Pclass 3","78c80144":"* **Dummy encoding**","17158ba8":"* **Tuning the Meta-\/Classifier**","ead4617d":"* **Split data into train, test**","c8a957a9":"## **Result**\n\n* **Making submission**","672e1e42":"* **PowerTransform Age, Fare**\n<br>Make these features more Gaussian-like","968b0e60":"Join both datasets together to make sure, that everything what we do to train-data also happens to test-data","6c2bbc6e":"* **Split train, test into 3 separate datasets**\n<br> There's a big difference in survial rates for Sex\/Pclass, so let's try separate models for men\/women","44d3ef95":"* **Add Family_Size, Alone**\n<br>Family_Size = Parch + SibSp\n<br>Alone, if Family_Size=0","a75ae67c":"* **Final Hyperparamter**"}}