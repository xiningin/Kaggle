{"cell_type":{"f3b04857":"code","48617243":"code","f83a4d7e":"code","48ba750d":"code","5c44bef7":"code","2d3106ee":"code","c5da72f6":"code","e9811832":"code","ca50a170":"code","22ec9a7a":"code","efd30594":"code","323ec13c":"code","98d9862b":"code","342d0bd8":"code","29e89d5d":"code","b2c2b9b1":"code","332e51d6":"code","08f6072a":"code","c0609869":"code","cbbc3d6e":"code","58f0f347":"code","80d99c41":"code","e9243e5c":"code","45ce8474":"code","0c18d124":"code","433bfef0":"markdown","5ddd57f8":"markdown","0a64d726":"markdown","0e26e6f9":"markdown","1eb48de7":"markdown","6ec26b78":"markdown","16c8a57b":"markdown","99e5c9b1":"markdown","363ef09c":"markdown"},"source":{"f3b04857":"!pip install -q efficientnet","48617243":"import math, re, os\nimport tensorflow as tf, tensorflow.keras.backend as K\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nfrom tensorflow.keras.applications import VGG19, ResNet50V2, ResNet101V2, ResNet152V2, DenseNet201, \\\n                                            DenseNet169, NASNetLarge, InceptionResNetV2\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport pandas as pd\nimport cv2\nfrom tqdm.notebook import tqdm\n\nprint(\"Tensorflow version \" + tf.__version__)","f83a4d7e":"# tf.keras.backend.set_floatx('float64')\ntf.keras.backend.floatx()","48ba750d":"WITH_801_CLASSES = False # \u5305\u542b801\u985e\n\nclasses = ''\nwith open('\/kaggle\/input\/2021-sun-bank-summer\/training data dic.txt') as f:\n    classes = f.read()\nclasses = classes.split('\\n')\nif WITH_801_CLASSES:\n    classes.append('isnull')\n    \nlen(classes)","5c44bef7":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set.\n    # On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nXLA_ACCELERATE = False\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","2d3106ee":"def f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","c5da72f6":"# P = {}\n# P['ORG_SIZE'] = (400, 400)\n# P['IMAGE_SIZE'] = (400, 400) # (H, W) (1024, 1024)=>8 (512, 512)=>16 (400, 400) (400, 120)\n# P['BATCH_SIZE'] = 16 * strategy.num_replicas_in_sync\n# P['EPOCHS'] = 25; P['EPOCHS2'] = 0\n# ### sparse_categorical_crossentropy mse mae mape msle kullback_leibler_divergence logcosh poisson\n# P['BEST_LOSS'] = (tf.keras.losses.Huber(delta=3., reduction=\"auto\", name=\"huber_loss\"), 'mae', 'mape')\n# # P['USING_REDUCE'] = None\n# P['USING_REDUCE'] = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3,\n#                                                          min_lr=1e-5, verbose=1)\n# P['USING_LR_SCHEDULE'] = False\n# P['LR'] = {\n#     'START': 1e-5,\n#     'MAX': 1e-4 * strategy.num_replicas_in_sync,\n#     'MIN': 1e-5,\n#     'RAMPUP_EPOCHS': 5,\n#     'SUSTAIN_EPOCHS': 0,\n#     'EXP_DECAY': .9,\n# }\n# # P['EPOCHS'] = 200; P['EPOCHS2'] = 0\n# # P['USING_LR_SCHEDULE'] = False\n# # P['LR'] = {\n# #     'START': 1e-5,\n# #     'MAX': 1e-4 * strategy.num_replicas_in_sync,\n# #     'MIN': 1e-5,\n# #     'RAMPUP_EPOCHS': 5,\n# #     'SUSTAIN_EPOCHS': 0,\n# #     'EXP_DECAY': .998,\n# # }\n# # P['VAL_PROCESS'] = {'NORMAL': True}\n# # P['VAL_PROCESS'] = {'SKIP_VALIDATION': True}\n# P['VAL_PROCESS'] = {'SPLIT_VAL': 0.3 }\n# # P['VAL_PROCESS'] = {\"FOLDS\": {'NUM': 5, 'BEST_MODEL': True}}\n# P['DATASET'] = {\n# #     'TRAIN': ({'PATH': 'foot-images\/tfrecords\/train', 'NUM': 998, 'MERGE': True},),\n# #     'VAL': ({'PATH': 'foot-images\/tfrecords\/dev', 'NUM': 424, 'MERGE': True},),\n# #     'TEST': ({'PATH': 'foot-images\/tfrecords\/test', 'NUM': 1000, 'MERGE': True},),\n    \n#     'TRAIN': (\n# #         {'PATH': 'foot-images\/tfrecords\/train_14230', 'NUM': 14230, 'MERGE': True},\n# #         {'PATH': 'foot-images\/tfrecords\/train_1W_bnd', 'NUM': 12929, 'MERGE': True},\n# #         {'PATH': 'foot-images\/tfrecords\/train_10W', 'NUM': 142300, 'MERGE': True},\n#         {'PATH': 'foot-images\/tfrecords\/train_10W_bnd', 'NUM': 129240, 'MERGE': True},\n# #         {'PATH': 'foot-images-100w\/train_100W', 'NUM': 1423000, 'MERGE': True},\n# #         {'PATH': 'foot-images-100w-bnd\/train_100W_bnd', 'NUM': 142300, 'MERGE': True}, # 1292271\n#     ),\n#     'VAL': (\n#     ),\n#     'TEST': (\n# #         {'PATH': 'foot-images\/tfrecords\/test', 'NUM': 1000, 'MERGE': True},\n#         {'PATH': 'foot-images\/tfrecords\/test_tta', 'NUM': 1000, 'MERGE': True},\n# #         {'TEST': 'EDSR_test', 'NUM': 1000, 'MERGE': False},\n#     ),\n# }\n# P['DO_AUG'] = {\n# #                 'rot_shr_zoom_shift': {'classification': True}\n# #                 'rot_shr_zoom_shift': {'height_shift': 1, 'width_shift': 1}\n# #                 'flip_left_right': True,\n# #                 'flip_up_down': True,\n\n# #                 'cutmix_mixup': True,\n# #                 'rgb_to_grayscale': True,\n# #                 'random_brightness': {'max_delta': 0.5}, # \u4eae\u5ea6 -0.5 ~ 0.5\n# #                 'random_contrast': {'lower': 0.8, 'upper': 3}, # \u5c0d\u6bd4 0.8 ~ 3\n# #                 'random_hue': {'max_delta': 0.2}, # \u8272\u76f8 0 ~ 0.2\n# #                 'random_saturation': {'lower': 0, 'upper': 4}, # \u98fd\u548c 0 ~ 4\n# #                 'gaussian_noise': {'stddev': 0.05},\n# }\n# # efn.EfficientNetB7 DenseNet201 ResNet152V2 NASNetLarge InceptionResNetV2\n# P['MODELS'] = (\n# #     {'MODEL': (efn.EfficientNetB7, 'noisy-student')},\n# #     {'MODEL': (efn.EfficientNetB6, 'noisy-student')},\n# #     {'MODEL': (efn.EfficientNetB5, 'noisy-student')},\n#     {'MODEL': (DenseNet201, 'imagenet')},\n# #     {'MODEL': (ResNet152V2, 'imagenet')},\n# )\n# P['OUTPUT_ACTIVATION'] = 'sigmoid' # None, \"softmax\", \"tanh\", \"sigmoid\"\n# P['IMAGE_FORMAT'] = 'sigmoid' # '0-255', 'sigmoid', 'tanh' sigmoid=>0-1 tanh=>-1~1\n# P['DO_TTA'] = True\n# P['OPTIMIZERS'] = tf.keras.optimizers.SGD(lr=1e-2, momentum=0.9, decay=0.0, nesterov=False) # Adam\n# P['RANDOM_SEED'] = 256\n# P['LOAD_BEST_MODEL'] = True\n# P['CLASSES'] = ('x1', 'y1', 'x2', 'y2')\n# P['SPECIAL_LIST'] = {\n# #     'top': (0, 1), 'down': (2, 3)\n# }\n# DO_ACCURACY_MAX = False\n# P['INPUT_SUBMISSION'] = '\/kaggle\/input\/foot-images\/input_submission.csv'\n# P['TEST_IMAGES_PATH'] = '\/kaggle\/input\/foot-images\/test\/images'\n# P['DO_ONEHOT'] = False\n\n\n\n\n\nP = {}\nP['ORG_SIZE'] = (512, 512)\nP['IMAGE_SIZE'] = (384, 384) # (H, W) (1024, 1024)=>8 (512, 512)=>16\nP['BATCH_SIZE'] = 32 * strategy.num_replicas_in_sync\nP['EPOCHS'] = 100\nP['EPOCHS2'] = 0\n# P['VAL_PROCESS'] = {'NORMAL': True}\nP['VAL_PROCESS'] = {'SKIP_VALIDATION': True}\n# P['VAL_PROCESS'] = {'SPLIT_VAL': 0.3}\n# P['VAL_PROCESS'] = {'FOLDS': {'NUM': 5}}\nP['DATASET'] = {\n    'TRAIN': (\n        {'PATH': '2021-sun-bank-summer\/tfrecords\/train', 'NUM': 45279, 'MERGE': True},\n#         {'PATH': '2021-sun-bank-summer\/tfrecords\/train_gen', 'NUM': 31200, 'MERGE': True},\n    ),\n    'VAL': (\n        {'PATH': '2021-sun-bank-summer\/tfrecords\/dev', 'NUM': 19406, 'MERGE': True},\n    ),\n    \n    'TEST': (\n        {'PATH': '2021-sun-bank-summer\/tfrecords\/dev', 'NUM': 19406, 'MERGE': True},\n    ),\n}\nP['DO_AUG'] = {\n                'rot_shr_zoom_shift': { 'classification': True },\n#                 'rot_shr_zoom_shift': { 'height_shift': 1, 'width_shift': 1 },\n#                 'flip_left_right': True,\n#                 'flip_up_down': True,\n\n                'cutmix_mixup': True,\n#                 'rgb_to_grayscale': True,\n#                 'random_brightness': { 'max_delta': 0.5 }, # \u4eae\u5ea6 -0.5 ~ 0.5\n#                 'random_contrast': { 'lower': 0.8, 'upper': 3 }, # \u5c0d\u6bd4 0.8 ~ 3\n#                 'random_hue': { 'max_delta': 0.2 }, # \u8272\u76f8 0 ~ 0.2\n#                 'random_saturation': { 'lower': 0, 'upper': 4 }, # \u98fd\u548c 0 ~ 4\n#                 'gaussian_noise': { 'stddev': 0.05 },\n}\nP['LR'] = {\n    'START': 1e-5,\n    'MAX': 1e-4 * strategy.num_replicas_in_sync,\n    'MIN': 1e-5,\n    'RAMPUP_EPOCHS': 5,\n    'SUSTAIN_EPOCHS': 0,\n    'EXP_DECAY': .95,\n}\nP['OUTPUT_ACTIVATION'] = 'softmax' # \"None\", \"softmax\", \"tanh\", \"sigmoid\"\nP['IMAGE_FORMAT'] = 'sigmoid' # '0-255', 'sigmoid', 'tanh' sigmoid=>0-1 tanh=>-1~1\n### sparse_categorical_crossentropy mse mae mape msle kullback_leibler_divergence logcosh poisson\nP['BEST_LOSS'] = ('categorical_crossentropy',) # f1\n# efn.EfficientNetB7 DenseNet201 ResNet152V2\nP['MODELS'] = (\n    {'MODEL': (efn.EfficientNetB7, 'noisy-student')},\n#     {'MODEL': (efn.EfficientNetB6, 'noisy-student')},\n#     {'MODEL': (efn.EfficientNetB5, 'noisy-student')},\n#     {'MODEL': (DenseNet201, 'imagenet')},\n#     {'MODEL': (DenseNet169, 'imagenet')},\n#     {'MODEL': (ResNet152V2, 'imagenet')},\n#     {'MODEL': (ResNet101V2, 'imagenet')},\n#     {'MODEL': (ResNet50V2, 'imagenet')},\n#     {'MODEL': (InceptionResNetV2, 'imagenet')},\n#     {'MODEL': (efn.EfficientNetB7, 'imagenet')},\n#     {'MODEL': (efn.EfficientNetB6, 'imagenet')},\n#     {'MODEL': (efn.EfficientNetB5, 'imagenet')},\n)\nP['USING_REDUCE'] = None\n# P['USING_REDUCE'] = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3,\n#                                                          min_lr=1e-5, verbose=1)\nP['DO_TTA'] = False\nP['USING_LR_SCHEDULE'] = True\nP['OPTIMIZERS'] = tf.keras.optimizers.Adam(1e-5) # Adam SGD\nP['RANDOM_SEED'] = 256\nP['LOAD_BEST_MODEL'] = True\nP['CLASSES'] = classes\nP['SPECIAL_LIST'] = {\n#     'top': (0, 1), 'down': (2, 3)\n}\nDO_ACCURACY_MAX = True\nP['INPUT_SUBMISSION'] = '\/kaggle\/input\/2021aoi\/upload_sample.csv'\nP['TEST_IMAGES_PATH'] = None\nP['DO_ONEHOT'] = True","e9811832":"def make_dataset(name):\n    num = 0\n    filenames = []\n    for data in P['DATASET'][name]:\n#         print(name, data)\n        path = data['PATH']\n        files = tf.io.gfile.glob('{}{}\/*.tfrecord'.format(KaggleDatasets().get_gcs_path(\n            path.split('\/')[0]), path[path.find('\/'):]))\n#             print('files:', len(files))\n        if data['MERGE']:\n            num += data['NUM']\n#             print(num)\n            filenames.extend(files)\n#         else:\n#             filenames.append(files)\n    return num, filenames\n\n\nNUM_TRAINING_IMAGES = 0\nTRAINING_FILENAMES = []\nNUM_TRAINING_IMAGES, TRAINING_FILENAMES = make_dataset('TRAIN')\nprint('NUM_TRAINING_IMAGES:', NUM_TRAINING_IMAGES)\nprint('TRAINING_FILENAMES:', len(TRAINING_FILENAMES))\n\n\nNUM_VALIDATION_IMAGES = 0\nVALIDATION_FILENAMES = []\nNUM_VALIDATION_IMAGES, VALIDATION_FILENAMES = make_dataset('VAL')\nprint()\nprint('NUM_VALIDATION_IMAGES:', NUM_VALIDATION_IMAGES)\nprint('VALIDATION_FILENAMES:', len(VALIDATION_FILENAMES))\n    \n\nNUM_TEST_IMAGES = 0\nTEST_FILENAMES = []\nNUM_TEST_IMAGES, TEST_FILENAMES = make_dataset('TEST')\nprint()\nprint('NUM_TEST_IMAGES:', NUM_TEST_IMAGES)\nprint('TEST_FILENAMES:', len(TEST_FILENAMES))","ca50a170":"def lrfn(epoch):\n    if epoch < P['LR']['RAMPUP_EPOCHS']:\n        lr = (P['LR']['MAX'] - P['LR']['START']) \/ P['LR']['RAMPUP_EPOCHS'] * epoch + P['LR']['START']\n    elif epoch < P['LR']['RAMPUP_EPOCHS'] + P['LR']['SUSTAIN_EPOCHS']:\n        lr = P['LR']['MAX']\n    else:\n        lr = (P['LR']['MAX'] - P['LR']['MIN']) * P['LR']['EXP_DECAY'] ** \\\n            (epoch - P['LR']['RAMPUP_EPOCHS'] - P['LR']['SUSTAIN_EPOCHS']) + P['LR']['MIN']\n    return lr\n\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\nrng = [i for i in range(P['EPOCHS'])]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","22ec9a7a":"# numpy and matplotlib defaults\nnp.set_printoptions(threshold=15, linewidth=80)\n\n\n# def batch_to_numpy_images_and_labels(data):\n#     images, labels = data\n#     numpy_images = images.numpy()\n#     numpy_labels = labels.numpy()\n#     if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n#         numpy_labels = [None for _ in enumerate(numpy_images)]\n#     # If no labels, only image IDs, return None for labels (this is the case for test data)\n#     return numpy_images, numpy_labels\n\n\n# def title_from_label_and_target(label, correct_label):\n#     if correct_label is None:\n#         return P['CLASSES'][label], True\n#     correct = (label == correct_label)\n#     return \"{} [{}{}{}]\".format(P['CLASSES'][label], 'OK' if correct else 'NO',\n#                                     u\"\\u2192\" if not correct else '',\n#                                 P['CLASSES'][correct_label] if not correct else ''), correct\n\n\n# def display_one_flower(image, title, subplot, red=False, titlesize=16):\n#     plt.subplot(*subplot)\n#     plt.axis('off')\n#     plt.imshow(image)\n#     if len(title) > 0:\n#         plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2),\n#                   color='red' if red else 'black',\n#                   fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n#     return (subplot[0], subplot[1], subplot[2]+1)\n\n\n# def display_batch_of_images(databatch, predictions=None, figsize=13.0):\n#     \"\"\"This will work with:\n#     display_batch_of_images(images)\n#     display_batch_of_images(images, predictions)\n#     display_batch_of_images((images, labels))\n#     display_batch_of_images((images, labels), predictions)\n#     \"\"\"\n#     # data\n#     images, labels = batch_to_numpy_images_and_labels(databatch)\n#     if labels is None:\n#         labels = [None for _ in enumerate(images)]\n    \n#     # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n#     rows = int(math.sqrt(len(images)))\n#     cols = len(images)\/\/rows\n        \n#     # size and spacing\n#     FIGSIZE =  figsize\n#     SPACING = 0.1\n#     subplot=(rows,cols,1)\n#     if rows < cols:\n#         plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n#     else:\n#         plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n#     # display\n#     for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n#         title = '' if label is None else P['CLASSES'][label]\n#         correct = True\n#         if predictions is not None:\n#             title, correct = title_from_label_and_target(predictions[i], label)\n#         # magic formula tested to work from 1x1 to 10x10 images\n#         dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 \n#         subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n#     # layout\n#     plt.tight_layout()\n#     if label is None and predictions is None:\n#         plt.subplots_adjust(wspace=0, hspace=0)\n#     else:\n#         plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n#     plt.show()\n\n\n# def display_confusion_matrix(cmat, score, precision, recall):\n#     plt.figure(figsize=(15,15))\n#     ax = plt.gca()\n#     ax.matshow(cmat, cmap='Reds')\n#     ax.set_xticks(range(len(P['CLASSES'])))\n#     ax.set_xticklabels(P['CLASSES'], fontdict={'fontsize': 7})\n#     plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n#     ax.set_yticks(range(len(P['CLASSES'])))\n#     ax.set_yticklabels(P['CLASSES'], fontdict={'fontsize': 7})\n#     plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n#     titlestring = \"\"\n#     if score is not None:\n#         titlestring += 'f1 = {:.3f} '.format(score)\n#     if precision is not None:\n#         titlestring += '\\nprecision = {:.3f} '.format(precision)\n#     if recall is not None:\n#         titlestring += '\\nrecall = {:.3f} '.format(recall)\n#     if len(titlestring) > 0:\n#         ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right',\n#                                                'verticalalignment':'top', 'color':'#804040'})\n#     plt.show()\n    \n    \ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    if validation != None:\n        ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","efd30594":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0), [3,3] )\n    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0), [3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape(tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],\n                                       axis=0), [3,3])\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape(tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],\n                                         axis=0), [3,3])\n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\n\ndef rot_shr_zoom_shift_name(image, name, labels):\n    image, labels = rot_shr_zoom_shift(image, labels)\n    return image, name, labels\n\n\ndef rot_shr_zoom_shift(image, labels):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = P['IMAGE_SIZE'][0]\n    XDIM = DIM % 2 # fix for size 331\n    \n    trans = P['DO_AUG'].get('rot_shr_zoom_shift')\n    if trans.get('classification'):\n        rot = 15. * tf.random.normal([1], dtype='float32')\n        shr = 5. * tf.random.normal([1], dtype='float32') \n        h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ 10.\n        w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ 10.\n        h_shift = 16. * tf.random.normal([1], dtype='float32')\n        w_shift = 16. * tf.random.normal([1], dtype='float32') \n#     else:\n#         rot = 0. * tf.cast([1], dtype='float32')\n#         shr = 0. * tf.cast([1], dtype='float32')\n#         h_zoom = 1.0 * tf.cast([1], dtype='float32')\n#         w_zoom = 1.0 * tf.cast([1], dtype='float32')\n#         h_shift = 0. * tf.cast([1], dtype='float32')\n#         w_shift = 0. * tf.cast([1], dtype='float32')\n        \n#         if trans.get('rotation'):\n#     #         rot = 15. * tf.random.normal([1], dtype='float32')\n#             # image = tf.io.read_file(\"..\/input\/foot-images\/test\/images\/image_0001.png\")\n#             # image = tf.image.decode_jpeg(image)\n#             # bounding_boxes = tf.constant([[0.1, 0.2, 0.5, 0.3], [0.5, 0.3, 0.9, 0.1]])\n#             rot = 0. * tf.cast([1], dtype='float32')\n\n#         if trans.get('shear'):\n#             shr = 5. * tf.random.normal([1], dtype='float32')\n\n#         if trans.get('height_zoom'):\n#     #         h_zoom = tf.cast([0.5], dtype='float32')\n#             h_zoom = 1 - tf.random.uniform(shape=[1]) \/ 2\n#             labels *= [1, 1 + (1-h_zoom[0])\/2, 1, 1 - (1-h_zoom[0])\/2]\n\n#         if trans.get('width_zoom'):\n#     #         w_zoom = tf.cast([0.5], dtype='float32')\n#             w_zoom = 1 - tf.random.uniform(shape=[1]) \/ 2\n#             if labels[0] <= 60:\n#                 s1 = 1 + 0.25\n#             else:\n#                 s1 = 1 - 0.25\n#             if labels[2] <= 60:\n#                 s3 = 1 + 0.25\n#             else:\n#                 s3 = 1 - 0.25\n#             labels *= [s1, 1, s3, 1]\n        \n#         if trans.get('height_shift'):\n#             h_shift = trans.get('height_shift') * tf.random.normal([1], dtype='float32')\n#             labels += [0, 1, 0, 1] * h_shift\n\n#         if trans.get('width_shift'):\n#             #trans.get('width_shift') * tf.random.normal([1], dtype='float32')\n#             w_shift = 10. * tf.cast([1], dtype='float32')\n#             labels += [-1, 0, -1, 0] * w_shift\n            \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot, shr, h_zoom, w_zoom, h_shift, w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM\/\/2+XDIM+1, DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]])\n    d = tf.gather_nd(image, tf.transpose(idx3))\n    return tf.reshape(d, [DIM,DIM,3]), labels\n\n\ndef cutmix(image, label, PROBABILITY=1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = P['IMAGE_SIZE'][0]\n    num_classes = len(P['CLASSES'])\n    batch_size = P['BATCH_SIZE']\n    imgs = []; labs = []\n    for j in range(batch_size):\n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        Px = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([], 0, batch_size), tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM), tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM), tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * Px\n        ya = tf.math.maximum(0,y-WIDTH\/\/2)\n        yb = tf.math.minimum(DIM,y+WIDTH\/\/2)\n        xa = tf.math.maximum(0,x-WIDTH\/\/2)\n        xb = tf.math.minimum(DIM,x+WIDTH\/\/2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH\/DIM\/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j], num_classes)\n            lab2 = tf.one_hot(label[k], num_classes)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs), (batch_size, DIM, DIM, 3))\n    label2 = tf.reshape(tf.stack(labs), (batch_size, num_classes))\n    return image2, label2\n\n\ndef mixup(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = P['IMAGE_SIZE'][0]\n    num_classes = len(P['CLASSES'])\n    batch_size = P['BATCH_SIZE']\n    imgs = []; labs = []\n    for j in range(P['BATCH_SIZE']):\n        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n        Px = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([], 0, batch_size), tf.int32)\n        a = tf.random.uniform([],0,1) * Px # this is beta dist with alpha=1.0\n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        # MAKE CUTMIX LABEL\n        if len(label.shape) == 1:\n            lab1 = tf.one_hot(label[j], num_classes)\n            lab2 = tf.one_hot(label[k], num_classes)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(batch_size, DIM,DIM, 3))\n    label2 = tf.reshape(tf.stack(labs),(batch_size, num_classes))\n    return image2, label2\n\n\ndef transform_cutmix_mixup(image, label):\n    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n    DIM = P['IMAGE_SIZE'][0]\n    num_classes = len(P['CLASSES'])\n    batch_size = P['BATCH_SIZE']\n    SWITCH = 0.5\n    CUTMIX_PROB = 0.666\n    MIXUP_PROB = 0.666\n    \n    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n#     image2, label2 = cutmix(image, label['classes'], CUTMIX_PROB)\n#     image3, label3 = mixup(image, label['classes'], MIXUP_PROB)\n    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n    image3, label3 = mixup(image, label, MIXUP_PROB)\n    imgs = []; labs = []\n    for j in range(batch_size):\n        Px = tf.cast( tf.random.uniform([],0,1) <= SWITCH, tf.float32)\n        imgs.append(Px * image2[j,] + (1-Px) * image3[j,])\n        labs.append(Px * label2[j,] + (1-Px) * label3[j,])\n        \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image4 = tf.reshape(tf.stack(imgs),(batch_size, DIM, DIM, 3))\n    label4 = tf.reshape(tf.stack(labs),(batch_size, num_classes))\n#     label['classes'] = label4\n    return image4, label4","323ec13c":"LABELED_TFREC_FORMAT = {\n    \"image\/encoded\": tf.io.FixedLenFeature([], tf.string)\n}\n\nactivation = P['OUTPUT_ACTIVATION']\nif activation == \"softmax\":\n    LABELED_TFREC_FORMAT[\"image\/object\/class\/label\"] = tf.io.FixedLenFeature([], tf.int64)\n    LABELED_TFREC_FORMAT[\"image\/object\/class\/isContain\"] = tf.io.FixedLenFeature([], tf.int64)\nelse:\n    LABELED_TFREC_FORMAT[\"image\/object\/bbox\/xmin\"] = tf.io.FixedLenFeature([], tf.float32)\n    LABELED_TFREC_FORMAT[\"image\/object\/bbox\/ymin\"] = tf.io.FixedLenFeature([], tf.float32)\n    LABELED_TFREC_FORMAT[\"image\/object\/bbox\/xmax\"] = tf.io.FixedLenFeature([], tf.float32)\n    LABELED_TFREC_FORMAT[\"image\/object\/bbox\/ymax\"] = tf.io.FixedLenFeature([], tf.float32)\n\n\ndef sigmoid2tanh(x):\n    return 2 * x - 1\n\n\ndef thanh2sigmoid(x):\n    return (x + 1) \/ 2\n\n\ndef set_arr_value(arr, value, odd=True):\n    p = 1 if odd else 0\n    for idx, i in enumerate(arr):\n        if idx % 2 == p:\n            arr[idx] *= value\n\n\ndef onehot(image, label):\n#     label['classes'] = tf.one_hot(label['classes'], len(P['CLASSES']))\n#     label['isContain'] = tf.one_hot(label['isContain'], 2)\n#     return image, label\n    return image, tf.one_hot(label, len(P['CLASSES']))\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    if P['IMAGE_FORMAT'] == '0-255':\n        image = tf.cast(image, tf.float32)\n    if P['IMAGE_FORMAT'] == 'sigmoid':\n        image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    elif P['IMAGE_FORMAT'] == 'tanh':\n        image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n        image = sigmoid2tanh(image)\n    image = tf.image.resize(image, (P['IMAGE_SIZE'][0], P['IMAGE_SIZE'][1]),\n                            method=tf.image.ResizeMethod.BICUBIC)\n    return image\n\n\ndef get_labels(example):\n    if P['OUTPUT_ACTIVATION'] != 'softmax':\n        x1 = tf.cast(example['image\/object\/bbox\/xmin'], tf.float32)\n        y1 = tf.cast(example['image\/object\/bbox\/ymin'], tf.float32)\n        x2 = tf.cast(example['image\/object\/bbox\/xmax'], tf.float32)\n        y2 = tf.cast(example['image\/object\/bbox\/ymax'], tf.float32)\n        \n        label = [x1, y1, x2, y2]\n        if len(P['SPECIAL_LIST']) > 0:\n            return [label[i] for i in SPECIAL]\n        else:\n            return label\n    else:\n        label = tf.cast(example['image\/object\/class\/label'], tf.int64)\n        return label\n#         isContain = tf.cast(example['image\/object\/class\/isContain'], tf.int64)\n#         return {'classes': label, 'isContain': isContain}\n\n\ndef read_labeled_name_tfrecord(example):\n    LABELED_TFREC_FORMAT[\"image\/filename\"] = tf.io.FixedLenFeature([], tf.string)\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    return decode_image(example['image\/encoded']), example['image\/filename'], get_labels(example)\n\n\ndef read_labeled_tfrecord(example):\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    return decode_image(example['image\/encoded']), get_labels(example)\n\n\ndef load_dataset(filenames, named=False, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n    # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order) \n    dataset = dataset.map(read_labeled_name_tfrecord if named else read_labeled_tfrecord,\n                          num_parallel_calls=AUTO)\n    return dataset\n\n\ndef data_aug(image, labels):\n    if P['DO_AUG'].get('rgb_to_grayscale'):\n        image = tf.image.rgb_to_grayscale(image)\n        image = tf.image.grayscale_to_rgb(image)\n        \n    if P['DO_AUG'].get('random_brightness'):\n        # \u4eae\u5ea6 -0.5~0.5\n        image = tf.image.random_brightness(image, max_delta=P['DO_AUG'].\\\n                                           get('random_brightness')['max_delta']) \n    \n    if P['DO_AUG'].get('random_contrast'):\n        # \u5c0d\u6bd4 0.8~3\n        image = tf.image.random_contrast(image, lower=P['DO_AUG'].get('random_contrast')['lower'],\n                                         upper=P['DO_AUG'].get('random_contrast')['upper'])\n    \n    if P['DO_AUG'].get('random_hue'):\n        # \u8272\u76f8 0~0.2\n        image = tf.image.random_hue(image, max_delta=P['DO_AUG'].get('random_hue')['max_delta'])\n    \n    if P['DO_AUG'].get('random_saturation'):\n        # \u98fd\u548c 0~4\n        image = tf.image.random_saturation(image, lower=P['DO_AUG'].get('random_contrast')['lower'],\n                                           upper=P['DO_AUG'].get('random_contrast')['upper']) \n    \n    if P['OUTPUT_ACTIVATION'] != 'softmax':\n        x1 = labels[0]\n        y1 = labels[1]\n        x2 = labels[2]\n        y2 = labels[3]\n\n    if P['DO_AUG'].get('flip_left_right'):\n        if float(np.random.rand(1)) > 0.5:\n            if P['OUTPUT_ACTIVATION'] == 'softmax':\n                pass\n            elif P['OUTPUT_ACTIVATION'] == 'tanh':\n                x1 = sigmoid2tanh(1 - thanh2sigmoid(x1))\n                x2 = sigmoid2tanh(1 - thanh2sigmoid(x2))\n                labels = [x1, y1, x2, y2]\n            elif P['OUTPUT_ACTIVATION'] == 'sigmoid':\n                x1 = 1 - x1\n                x2 = 1 - x2\n                labels = [x1, y1, x2, y2]\n            else: # None\n                x1 = P['IMAGE_SIZE'][1] - x1\n                x2 = P['IMAGE_SIZE'][1] - x2\n                labels = [x1, y1, x2, y2]\n            image = tf.image.flip_left_right(image) # \u5de6\u53f3\n    \n    if P['DO_AUG'].get('flip_up_down'):\n        if float(np.random.rand(1)) > 0.5:\n            if P['OUTPUT_ACTIVATION'] == 'softmax':\n                pass\n            elif P['OUTPUT_ACTIVATION'] == 'tanh':\n                y1 = sigmoid2tanh(1 - thanh2sigmoid(y1))\n                y2 = sigmoid2tanh(1 - thanh2sigmoid(y2))\n                labels = [x1, y1, x2, y2]\n            elif P['OUTPUT_ACTIVATION'] == 'sigmoid':\n                y1 = 1 - y1\n                y2 = 1 - y2\n                labels = [x1, y1, x2, y2]\n            else: # None\n                y1 = P['IMAGE_SIZE'][0] - y1\n                y2 = P['IMAGE_SIZE'][0] - y2\n                labels = [x1, y1, x2, y2]\n            image = tf.image.flip_up_down(image) # \u4e0a\u4e0b\n    \n    if P['DO_AUG'].get('gaussian_noise'):\n        # Adding Gaussian noise\n        noise = tf.random.normal(shape=(*P['IMAGE_SIZE'], 3), mean=0.0,\n                                 stddev=P['DO_AUG'].get('gaussian_noise')['stddev'], dtype=tf.float32)\n        image = tf.add(image, noise)\n    return image, labels\n            \n            \ndef transform_label(image, label):\n    lab = [label[i] for i in range(len(label))]\n    if P['OUTPUT_ACTIVATION'] == 'tanh': # -1 ~ 1\n        set_arr_value(lab, 1 \/ P['ORG_SIZE'][1], False)\n        set_arr_value(lab, 1 \/ P['ORG_SIZE'][0], True)\n        lab = [sigmoid2tanh(i) for i in lab]\n        return image, lab\n    elif P['OUTPUT_ACTIVATION'] == 'sigmoid': # 0 ~ 1\n        set_arr_value(lab, 1 \/ P['ORG_SIZE'][1], False)\n        set_arr_value(lab, 1 \/ P['ORG_SIZE'][0], True)\n        return image, lab\n    else: # None 0 ~ P['IMAGE_SIZE']\n        set_arr_value(lab, 1 \/ P['ORG_SIZE'][1] * P['IMAGE_SIZE'][1], False)\n        set_arr_value(lab, 1 \/ P['ORG_SIZE'][0] * P['IMAGE_SIZE'][0], True)\n        return image, lab\n\n\ndef get_training_dataset(dataset):\n    if P['DO_AUG'].get('rot_shr_zoom_shift'):\n        dataset = dataset.map(rot_shr_zoom_shift, num_parallel_calls=AUTO)\n    if P['OUTPUT_ACTIVATION'] != 'softmax':\n        dataset = dataset.map(transform_label, num_parallel_calls=AUTO)\n    dataset = dataset.map(data_aug, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    if P['DO_AUG'].get('cutmix_mixup'):\n        dataset = dataset.batch(P['BATCH_SIZE'])\n        dataset = dataset.map(transform_cutmix_mixup, num_parallel_calls=AUTO)\n        dataset = dataset.unbatch()\n    seed = P['RANDOM_SEED']\n    dataset = dataset.shuffle(seed)\n    dataset = dataset.batch(P['BATCH_SIZE'])\n\n    if not P['DO_AUG'].get('cutmix_mixup') and P['DO_ONEHOT']:\n        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\ndef get_validation_dataset(dataset):\n    if P['DO_TTA'] and P['DO_AUG'].get('rot_shr_zoom_shift'):\n        dataset = dataset.map(rot_shr_zoom_shift, num_parallel_calls=AUTO)\n        \n    if P['OUTPUT_ACTIVATION'] != 'softmax':\n        dataset = dataset.map(transform_label, num_parallel_calls=AUTO)\n        \n    dataset = dataset.batch(P['BATCH_SIZE'])\n    if P['DO_ONEHOT']:\n        # we must use one hot like augmented train data\n        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\ndef get_test_dataset():\n    dataset = load_dataset(TEST_FILENAMES, named=True, ordered=True)\n    if P['DO_TTA'] and P['DO_AUG'].get('rot_shr_zoom_shift'):\n        dataset = dataset.map(rot_shr_zoom_shift_name, num_parallel_calls=AUTO)\n    dataset = dataset.batch(P['BATCH_SIZE'])\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\n# # counting number of unique ids in dataset\n# def count_data_items(dataset):\n#     if dataset == None:\n#         return 0\n#     dataset = dataset.map(lambda image, x1, y1, x2, y2: x1) #label: label)\n#     return len(next(iter(dataset.batch(100000))))","98d9862b":"def show_img(img, label):\n#     print(img.shape)\n    if P['OUTPUT_ACTIVATION'] != \"softmax\":\n        even = label[::2]\n        odd = label[1::2]\n        for x, y in zip(even, odd):\n            img = cv2.circle(img, (x, y), 5, (255, 0, 0), -1)\n    else:\n        print(label)\n    plt.axis(\"off\")\n    plt.imshow(img)\n#     plt.show()\n\n\ndef plot_it(num):\n    fig = plt.figure(figsize=(12, 6), dpi=100)\n    for idx, (img, label) in enumerate(get_training_dataset(load_dataset(TRAINING_FILENAMES)).unbatch()\\\n                                        .take(num)):\n    # for (img, labels) in get_validation_dataset(load_dataset(TRAINING_FILENAMES)).unbatch().take(10):\n        plt.subplot(2, 5, idx+1)\n        lab = label.numpy()\n#         print(lab)\n        if P['OUTPUT_ACTIVATION'] == \"softmax\":\n            if P['DO_ONEHOT']:\n                pass\n            else:\n                print(P['CLASSES'][lab])\n        elif P['OUTPUT_ACTIVATION'] == 'tanh':\n            lab = [thanh2sigmoid(i) for i in lab]\n            set_arr_value(lab, P['IMAGE_SIZE'][1], False)\n            set_arr_value(lab, P['IMAGE_SIZE'][0], True)\n        elif P['OUTPUT_ACTIVATION'] == 'sigmoid':\n            set_arr_value(lab, P['IMAGE_SIZE'][1], False)\n            set_arr_value(lab, P['IMAGE_SIZE'][0], True)\n        else: # None\n            pass\n        show_img(img.numpy(), lab)\n\n\nif len(P['SPECIAL_LIST']) > 0:\n    for k in P['SPECIAL_LIST']:\n        SPECIAL = P['SPECIAL_LIST'][k]\n#         print(SPECIAL)\n        plot_it(5)\nelse:\n    SPECIAL = None\n    plot_it(10)","342d0bd8":"FULL_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES\nNUM_FULL_FILENAMES = NUM_TRAINING_IMAGES + NUM_VALIDATION_IMAGES\n\n\ndef get_train_vail_dataset():\n    global P, FULL_FILENAMES, NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES\n    train_dataset = None\n    val_dataset = None\n    if P['VAL_PROCESS'].get('SKIP_VALIDATION'):\n        train_dataset = get_training_dataset(load_dataset(FULL_FILENAMES))\n        val_dataset = None\n        NUM_TRAINING_IMAGES = NUM_TRAINING_IMAGES + NUM_VALIDATION_IMAGES\n        NUM_VALIDATION_IMAGES = 0\n    elif P['VAL_PROCESS'].get('SPLIT_VAL'):\n        TRAINING_RATE = 1 - P['VAL_PROCESS'].get('SPLIT_VAL')\n        full_dataset = load_dataset(FULL_FILENAMES, ordered=True)\n        num = NUM_TRAINING_IMAGES + NUM_VALIDATION_IMAGES\n        NUM_TRAINING_IMAGES = int(TRAINING_RATE * num)\n        NUM_VALIDATION_IMAGES = int((1-TRAINING_RATE) * num)\n\n        full_dataset = full_dataset.shuffle(P['RANDOM_SEED'])\n        train_dataset = full_dataset.take(NUM_TRAINING_IMAGES)\n        val_dataset = full_dataset.skip(NUM_TRAINING_IMAGES)\n        train_dataset = get_training_dataset(train_dataset)\n        val_dataset = get_validation_dataset(val_dataset)\n        del full_dataset\n    elif P['VAL_PROCESS'].get('FOLDS'):\n        pass\n    elif P['VAL_PROCESS'].get('NORMAL'):\n        train_dataset = get_training_dataset(load_dataset(TRAINING_FILENAMES))\n        val_dataset = get_validation_dataset(load_dataset(VALIDATION_FILENAMES, ordered=True))\n    else:\n        raise 'Error!'\n    \n    steps_per_epoch = NUM_TRAINING_IMAGES \/\/ P['BATCH_SIZE']\n    print('steps_per_epoch:', steps_per_epoch)\n    print('Training images: {}'.format(NUM_TRAINING_IMAGES))\n    print('Validation images: {}'.format(NUM_VALIDATION_IMAGES))\n    print('Test images: {}'.format(NUM_TEST_IMAGES))\n    return train_dataset, val_dataset, steps_per_epoch","29e89d5d":"# def f1(y_true, y_pred):\n#     def recall(y_true, y_pred):\n#         \"\"\"Recall metric.\n\n#         Only computes a batch-wise average of recall.\n\n#         Computes the recall, a metric for multi-label classification of\n#         how many relevant items are selected.\n#         \"\"\"\n#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#         recall = true_positives \/ (possible_positives + K.epsilon())\n#         return recall\n\n#     def precision(y_true, y_pred):\n#         \"\"\"Precision metric.\n\n#         Only computes a batch-wise average of precision.\n\n#         Computes the precision, a metric for multi-label classification of\n#         how many selected items are relevant.\n#         \"\"\"\n#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#         precision = true_positives \/ (predicted_positives + K.epsilon())\n#         return precision\n#     precision = precision(y_true, y_pred)\n#     recall = recall(y_true, y_pred)\n#     return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n\ndef binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n    References:\n        https:\/\/arxiv.org\/pdf\/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed\n\n\ndef categorical_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Softmax version of focal loss.\n           m\n      FL = \u2211  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n          c=1\n      where m = number of classes, c = class and o = observation\n    Parameters:\n      alpha -- the same as weighing factor in balanced cross entropy\n      gamma -- focusing parameter for modulating factor (1-p)\n    Default value:\n      gamma -- 2.0 as mentioned in the paper\n      alpha -- 0.25 as mentioned in the paper\n    References:\n        Official paper: https:\/\/arxiv.org\/pdf\/1708.02002.pdf\n        https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/backend\/categorical_crossentropy\n    Usage:\n     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred: A tensor resulting from a softmax\n        :return: Output tensor.\n        \"\"\"\n\n        # Scale predictions so that the class probas of each sample sum to 1\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n\n        # Clip the prediction value to prevent NaN's and Inf's\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        # Calculate Cross Entropy\n        cross_entropy = -y_true * K.log(y_pred)\n\n        # Calculate Focal Loss\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n\n        # Sum the losses in mini_batch\n        return K.sum(loss, axis=1)\n\n    return categorical_focal_loss_fixed\n\n\ndef display(history, do_val_loss, num=''):\n    PREFIX = 'val_' if do_val_loss else ''\n    new_best_loss = PREFIX + 'loss'\n    total = len(P['BEST_LOSS']) + 1\n    display_training_curves(history.history['loss'], history.history[PREFIX + 'loss'], 'loss' + str(num),\n                            int(str(total) + '1' + '1') )\n    for idx, loss in enumerate(P['BEST_LOSS'][1:]):\n        if callable(loss): # \u5982\u679c\u662ffunction\uff0c\u5c07\u540d\u5b57\u53d6\u51fa\n            loss = loss.__name__\n        display_training_curves(history.history[loss], history.history[PREFIX + loss], loss + str(num),\n                                int(str(total) + '1' + str(idx+2)) )\n        \n    index = np.argmin(history.history[new_best_loss])\n    print('minest {}: {:.5f}'.format(new_best_loss, history.history[new_best_loss][index]))\n\n\ndef get_model(modeling, weights, optimizers):\n    rnet = modeling(\n        input_shape=(P['IMAGE_SIZE'][0], P['IMAGE_SIZE'][1], 3),\n        weights=weights,\n        include_top=False\n    )\n    # trainable rnet\n    rnet.trainable = True\n    \n    output = None\n    if len(P['SPECIAL_LIST']) > 0:\n        len_cls = len([P['CLASSES'][i] for i in SPECIAL])\n    else:\n        len_cls = len(P['CLASSES'])\n    if P['OUTPUT_ACTIVATION'] == 'softmax':\n        act = 'softmax'\n    elif P['OUTPUT_ACTIVATION'] == 'tanh':\n        act = 'tanh'\n    elif P['OUTPUT_ACTIVATION'] == 'sigmoid':\n        act = 'sigmoid'\n    else: # None\n        act = 'linear'\n    print('act:', act)\n    \n    output = tf.keras.layers.Dense(len_cls, activation=act, dtype='float32')\n    model = tf.keras.Sequential([\n        rnet,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        output\n    ])\n    \n#     x = rnet.layers[-1].output\n#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n#     x1 = tf.keras.layers.Dense(len_cls, activation=act, dtype='float32', name='classes')(x)\n#     x2 = tf.keras.layers.Dense(1, activation='sigmoid', name='isContain')(x)\n#     model = tf.keras.models.Model(inputs=rnet.input, outputs=[x1, x2]) \n\n    losses = list(P['BEST_LOSS'])\n    model.compile(optimizer=optimizers, loss=losses.pop(0), metrics=losses)\n#     ### mse, binary_crossentropy\n#     model.compile(optimizer=optimizers, loss={'classes': losses.pop(0), \n#                                               'isContain': 'binary_crossentropy'},\n#                   metrics={'classes': losses, 'isContain': 'acc'})\n    model.summary()\n    return model\n\n\ndef pandas_confusion_matrix(model, val_dataset, do_display=True):\n    images_ds = val_dataset.map(lambda image, label: image)\n    labels_ds = val_dataset.map(lambda image, label: label).unbatch()\n    probs = model.predict(images_ds)\n    probs = np.argmax(probs, axis=-1)\n    labels = next(iter(labels_ds.batch(len(probs)))).numpy() # get everything as one batch\n    labels = np.argmax(labels, axis=-1)\n    if do_display:\n        print(pd.crosstab(labels, probs, rownames=['label'], colnames=['predict']))\n    return labels, probs","b2c2b9b1":"import pandas as pd\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.models import load_model\nimport gc\n\n\nif not DO_ACCURACY_MAX:\n    MODE = 'min'\n    if not P['VAL_PROCESS'].get('SKIP_VALIDATION'):\n        new_best_loss = 'val_' + 'loss'\n    else:\n        new_best_loss = 'loss'\nelse:\n    MODE = 'max'\n    if not P['VAL_PROCESS'].get('SKIP_VALIDATION'):\n        new_best_loss = 'val_' + 'accuracy'\n    else:\n        new_best_loss = 'accuracy'\n\nif P['OUTPUT_ACTIVATION'] == 'softmax':\n    P['BEST_LOSS'] = list(P['BEST_LOSS'])\n    P['BEST_LOSS'].extend(['accuracy'])\n    P['BEST_LOSS'] = tuple(P['BEST_LOSS'])\nprint(\"P['OUTPUT_ACTIVATION']:\", P['OUTPUT_ACTIVATION'])\nprint(\"P['BEST_LOSS']:\", P['BEST_LOSS'])\nprint('new_best_loss:', new_best_loss)\n\n\ndef do_train(modling, weight, optimizer, train_dataset, val_dataset, steps_per_epoch, f, file):\n    K.clear_session()\n    CALLBACKS = []\n    with strategy.scope():\n        model = get_model(modling, weight, optimizer)\n#     use_model = ModelCheckpoint(filepath=file, monitor='classes_accuracy', save_weights_only=True,\n#                                 save_best_only=True, mode=MODE, period=1, verbose=1)\n    use_model = ModelCheckpoint(filepath=file, monitor=new_best_loss, save_weights_only=True,\n                                save_best_only=True, mode=MODE, period=1, verbose=1)\n    if P['LOAD_BEST_MODEL']:\n        print('LOAD_BEST_MODEL')\n        CALLBACKS.append(use_model)\n    if P['USING_LR_SCHEDULE']:\n        print('USING_LR_SCHEDULE')\n        CALLBACKS.append(lr_callback)\n    if P['USING_REDUCE']:\n        print('USING_REDUCE')\n        CALLBACKS.append(P['USING_REDUCE'])\n    history = model.fit(train_dataset, steps_per_epoch=steps_per_epoch, epochs=P['EPOCHS'],\n                        callbacks=CALLBACKS, validation_data=val_dataset)\n#     if P['VAL_PROCESS'].get('SKIP_VALIDATION'):\n#         display(history, False, '' if f == 0 else str(f+1))\n#         if P['OUTPUT_ACTIVATION'] == 'softmax':\n#             pandas_confusion_matrix(model, get_validation_dataset(load_dataset(FULL_FILENAMES)))\n#     else:\n#         display(history, True, '' if f == 0 else str(f+1))\n#         if P['OUTPUT_ACTIVATION'] == 'softmax':\n#             pandas_confusion_matrix(model, val_dataset)\n    del train_dataset, val_dataset, model\n    gc.collect()\n    return history\n\n\ndef train_model(modling, weight, optimizer, idx, special_key):\n    global strategy, P\n    \n    if P['VAL_PROCESS'].get('FOLDS'):\n        print(\"NUM_TRAINING_IMAGES: {}\".format(NUM_TRAINING_IMAGES))\n        print(\"NUM_VALIDATION_IMAGES: {}\".format(NUM_VALIDATION_IMAGES))\n        print('NUM_FULL_FILENAMES:', NUM_FULL_FILENAMES)\n        print(\"NUM_TEST_IMAGES: {}\".format(NUM_TEST_IMAGES))\n        histories = []\n#         print('Start training %i folds' % folds)\n        kfold = KFold(P['VAL_PROCESS']['FOLDS']['NUM'], shuffle=True, random_state=P['RANDOM_SEED'])\n        for f, (trn_ind, val_ind) in enumerate(kfold.split(FULL_FILENAMES)):\n            print()\n            print('#' * 35); print('############ FOLD ', f+1,' #############'); print('#' * 35);\n            TRAIN_FILES = [FULL_FILENAMES[fi] for fi in trn_ind]\n            VAL_FILES = [FULL_FILENAMES[fi] for fi in val_ind]\n            steps_per_epoch = NUM_FULL_FILENAMES \/\/ P['VAL_PROCESS']['FOLDS']['NUM'] \/\/ P['BATCH_SIZE']\n            print('steps_per_epoch:', steps_per_epoch)\n            print('len(TRAIN_FILES):', len(TRAIN_FILES))\n            print('len(VAL_FILES):', len(VAL_FILES))\n            file = \"model{}_{}{}_fold{}.h5\".format(idx+1, modling.__name__, special_key, f+1)\n            train_dataset = get_training_dataset(load_dataset(TRAIN_FILES))\n            val_dataset = get_validation_dataset(load_dataset(VAL_FILES, ordered=True))\n            history = do_train(modling, weight, optimizer, train_dataset, val_dataset, steps_per_epoch, f,\n                               file)\n            histories.append(history)\n        \n        max_acc = -9999\n        model_num = -1\n        for i, h in enumerate(histories):\n            for acc in h.history[new_best_loss]:\n                if acc > max_acc:\n                    max_acc = acc\n                    model_num = i\n        print('\\n\\n')\n        print('all maxest acc:', max_acc)\n        print('all maxest model num:', model_num+1)\n    else:\n        file = \"model{}_{}{}.h5\".format(idx+1, modling.__name__, special_key)\n        train_dataset, val_dataset, steps_per_epoch = get_train_vail_dataset()\n        history = do_train(modling, weight, optimizer, train_dataset, val_dataset, steps_per_epoch, -1, file)","332e51d6":"if len(P['SPECIAL_LIST']) > 0:\n    for k in P['SPECIAL_LIST']:\n        SPECIAL = P['SPECIAL_LIST'][k]\n        print('SPECIAL:', SPECIAL)\n        \n        for idx, dic in enumerate(P['MODELS']):\n            train_model(dic['MODEL'][0], dic['MODEL'][1], P['OPTIMIZERS'], idx, '_' + k)\nelse:\n    for idx, dic in enumerate(P['MODELS']):\n        train_model(dic['MODEL'][0], dic['MODEL'][1], P['OPTIMIZERS'], idx, '')","08f6072a":"# # Train2\n# if not P['VAL_PROCESS'].get('SKIP_VALIDATION') and P['EPOCHS2'] > 0:\n#     history = model.fit(\n#         get_training_dataset(load_dataset(FULL_FILENAMES)),\n#         steps_per_epoch=steps_per_epoch,\n#         epochs=P['EPOCHS2'],\n#         callbacks=[best_model]\n#     )\n#     display(history, False)","c0609869":"# if not P['VAL_PROCESS'].get('SKIP_VALIDATION'):\n#     # since we are splitting the dataset and iterating separately on images and labels, order matters.\n#     cmdataset = get_validation_dataset(ordered=True)\n#     images_ds = cmdataset.map(lambda image, label: image)\n#     labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n#    # get everything as one batch\n#     cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()\n#     m = model.predict(images_ds)\n#     m2 = model2.predict(images_ds)\n#     scores = []\n#     for alpha in np.linspace(0,1,100):\n#         cm_probabilities = alpha * m + (1-alpha) * m2\n#         cm_predictions = np.argmax(cm_probabilities, axis=-1)\n#         scores.append(f1_score(cm_correct_labels, cm_predictions, labels=range(len(P['CLASSES'])),\n#                                average='macro'))\n        \n#     print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n#     print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)\n#     plt.plot(scores)\n#     best_alpha = np.argmax(scores) \/ 100\n#     cm_probabilities = best_alpha * m + (1-best_alpha) * m2\n#     cm_predictions = np.argmax(cm_probabilities, axis=-1)\n# else:\n#     best_alpha = 0.51\n#     \n# best_alpha = 0.5\n# print('best_alpha:', best_alpha)","cbbc3d6e":"# def predict_it(probabilities, modling, weight, num, file):\n#     with strategy.scope():\n#         model = get_model(modling, weight, P['OPTIMIZERS'])\n#     print('loading model {}'.format(file))\n#     model.load_weights(file)\n#     if probabilities is None:\n#         probabilities  = 1 \/ num * model.predict(test_images_ds)\n#     else:\n#         probabilities += 1 \/ num * model.predict(test_images_ds)\n#     return probabilities\n\n\n# def predict_process(probabilities, special_key):\n#     if P['VAL_PROCESS'].get('FOLDS'):\n#         for idx, dic in enumerate(P['MODELS']):\n#             for f in range(P['VAL_PROCESS']['FOLDS']['NUM']):\n#                 file = \"model{}_{}{}_fold{}.h5\".format(idx+1, dic['MODEL'][0].__name__, special_key, f+1)\n#                 probabilities = predict_it(probabilities, dic['MODEL'][0], dic['MODEL'][1],\n#                                            P['VAL_PROCESS']['FOLDS']['NUM'], file)\n#     else:\n#         for idx, dic in enumerate(P['MODELS']):\n#             file = \"model{}_{}{}.h5\".format(idx+1, dic['MODEL'][0].__name__, special_key)\n#             probabilities = predict_it(probabilities, dic['MODEL'][0], dic['MODEL'][1], len(P['MODELS']),\n#                                        file)\n#     return probabilities\n\n\n# # since we are splitting the dataset and iterating separately on images and ids, order matters.\n# test_ds = get_test_dataset()\n\n# print('Computing predictions...')\n# print(\"P['MODELS']:\", len(P['MODELS']))\n# test_images_ds = test_ds.map(lambda image, name, label: image)\n# if len(P['SPECIAL_LIST']) > 0:\n#     probabilities_list = []\n#     for k in P['SPECIAL_LIST']:\n#         probabilities = None\n#         SPECIAL = P['SPECIAL_LIST'][k]\n#         print('SPECIAL:', k, SPECIAL)\n#         probabilities = predict_process(probabilities, '_' + k)\n#         probabilities_list.append(probabilities)\n#     probabilities = np.hstack(probabilities_list)\n# else:\n#     probabilities = None\n#     probabilities = predict_process(probabilities, '')\n\n\n# print('probabilities:', probabilities.shape)","58f0f347":"# print('Generating submission.csv file...')\n# test_ids_ds = test_ds.map(lambda image, name, label: name).unbatch()\n# test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n# # np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'],\n# #             delimiter=',', header='image_id,label', comments='')\n\n\n# if P['OUTPUT_ACTIVATION'] == 'softmax':\n#     predictions = np.argmax(probabilities, axis=-1)\n#     print('predictions len:', len(predictions))\n\n#     df = pd.DataFrame({'ID': test_ids, 'Label': predictions})\n# #     df = pd.DataFrame(predictions)\n# else:\n#     # df = pd.DataFrame({'col1': test_ids, 'col2': probabilities_lst})\n#     df = pd.DataFrame(probabilities)\n#     df['image_id'] = test_ids\n#     df.columns = ['x1', 'y1', 'x2', 'y2', 'images']\n#     df = df[['images', 'x1', 'y1', 'x2', 'y2']]\n\n\n# df.to_csv('probabilities.csv', index=None, encoding='utf-8-sig')\n# df.head()","80d99c41":"# df_new = pd.read_csv(P['INPUT_SUBMISSION'])\n\n\n# if P['OUTPUT_ACTIVATION'] == 'softmax':\n#     for idx, rows in tqdm(df_new.iterrows()):\n#         row = df[df.ID.isin([rows.ID])]\n#         if len(row) > 1:\n#             print('Error!')\n#         df_new.loc[idx, 'Label'] = row.Label.values\n# else:\n#     df_new['x1'] = 0\n#     df_new['y1'] = 0\n#     df_new['x2'] = 0\n#     df_new['y2'] = 0\n    \n#     for idx, rows in tqdm(df_new.iterrows()):\n#         row = df[df.images.isin([rows.images])]\n#         if len(row) > 1:\n#             print('Error!')\n#         df_new.loc[idx, 'x1'] = row.x1.values\n#         df_new.loc[idx, 'y1'] = row.y1.values\n#         df_new.loc[idx, 'x2'] = row.x2.values\n#         df_new.loc[idx, 'y2'] = row.y2.values\n    \n#     if P['OUTPUT_ACTIVATION'] == 'tanh':\n#         df_new.x1 = thanh2sigmoid(df_new.x1) * 120\n#         df_new.y1 = thanh2sigmoid(df_new.y1) * 400\n#         df_new.x2 = thanh2sigmoid(df_new.x2) * 120\n#         df_new.y2 = thanh2sigmoid(df_new.y2) * 400\n#     elif P['OUTPUT_ACTIVATION'] == 'sigmoid':\n#         df_new.x1 *= 120\n#         df_new.y1 *= 400\n#         df_new.x2 *= 120\n#         df_new.y2 *= 400\n#     else: # None\n#         df_new.x1 = df_new.x1 \/ P['IMAGE_SIZE'][1] * 120\n#         df_new.y1 = df_new.y1 \/ P['IMAGE_SIZE'][0] * 400\n#         df_new.x2 = df_new.x2 \/ P['IMAGE_SIZE'][1] * 120\n#         df_new.y2 = df_new.y2 \/ P['IMAGE_SIZE'][0] * 400\n#         pass\n\n\n# df_new.to_csv('submission.csv', index=None, encoding='utf-8-sig')\n# df_new.head()","e9243e5c":"if P['TEST_IMAGES_PATH']:\n    df_tmp = df_new[:10]\n    fig = plt.figure(figsize=(12, 6), dpi=100)\n    for idx, (images, x1, y1, x2, y2) in enumerate(zip(df_tmp.images, df_tmp.x1, df_tmp.y1, df_tmp.x2,\n                                                       df_tmp.y2)):\n        plt.subplot(2, 5, idx+1)\n        img = cv2.imread('{}\/{}'.format(P['TEST_IMAGES_PATH'], images))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        lab = [int(x1), int(y1), int(x2), int(y2)]\n        show_img(img, lab)","45ce8474":"str = '..\/input\/2021-sun-bank-summer\/train\/train\/10022_\u0398\u00fb\u00c7.jpg'\n\nimg = cv2.imread(str)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = cv2.resize(img, (P['IMAGE_SIZE'][0], P['IMAGE_SIZE'][1]))\nimg = img.astype(\"float32\") \/ 255.0\nplt.imshow(img)\n\n# print(img.shape)\nimg = np.expand_dims(img, axis=0)\nprint(img.shape)","0c18d124":"%%time\n\nprint(np.max(model.predict(img)))\nprint(classes[np.argmax(model.predict(img))])","433bfef0":"# Training Model","5ddd57f8":"# Configurations","0a64d726":"# Visualization","0e26e6f9":"# Custom LR schedule","1eb48de7":"# Show Images","6ec26b78":"# Datasets Functions","16c8a57b":"# Predictions","99e5c9b1":"# Data Augmentation","363ef09c":"## Load Model into TPU"}}