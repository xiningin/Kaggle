{"cell_type":{"b0497a81":"code","aa979c9c":"code","a75c1da5":"code","3a158b6c":"code","a3ffd06d":"code","e01cf044":"code","9dce1dc1":"code","991adcac":"code","f093e730":"code","f71c7989":"code","95307b0e":"code","674939e1":"code","5b1b9f94":"code","c8d6114c":"markdown","738fdbb5":"markdown","0e11369f":"markdown","89a30afa":"markdown"},"source":{"b0497a81":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import randint as randint\nfrom scipy.stats import uniform \nimport lightgbm as lgbm\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score","aa979c9c":"def load_data(data_path, **kwargs):\n    return pd.read_csv(data_path, header=0, delimiter=kwargs['delimiter'])\n \ndef clean_titanic_dataset(base_data_org, new_data_org, create_dummy=False):\n    \"\"\"\n    Cleaning Titanic Dataset.\n    \n    base_data: is the training data\n    new_data: is any validation or test data\n    For cleaning the base_data just use base_data,base_data\n    P.S. I think using pipeline should be a better way to implement chain of cleaning\/transforming data\n    I will consider it in the next versions. \n    There is no guarantee that new data set has all values, as a result we need to go through all\n    columns and have a strategy for missing values.\n    \"\"\"\n    \n    base_data = base_data_org.copy(deep=True)\n    new_data = new_data_org.copy(deep=True)\n    \n    # Input values should be pandas dataframe:\n    if (type(base_data) != type(pd.DataFrame())) or (type(new_data) != type(pd.DataFrame())):\n        print(\"Input data should be pandas dataframe.\")\n        return None\n        \n    # PassengerId is the key component. If data does not have PassengerId attribute\n    # or it has missing value, the function should terminate gracefully and give\n    # let user know the problem.\n    if ('PassengerId' not in base_data) or ('PassengerId' not in new_data):\n        print(\"PassengerId attribute is missing.\")\n        return None\n    \n    if (base_data.PassengerId.isnull().sum() != 0) or (new_data.PassengerId.isnull().sum() != 0):\n        print(\"PassengerId cannot have missing value.\")\n        return None\n\n    # Survived attribute. (It should be Ok for now)\n    \n    # Pclass \n    # There is not any missing value for Pclass, however, there is no guarantee \n    # for upcoming datasets.\n    \n    for item in new_data.loc[new_data.Pclass.isnull(),:].iterrows():\n            fare = item[1][\"Fare\"]\n            PassengerId = item[1][\"PassengerId\"]\n            if (fare != fare):\n                new_data.loc[new_data[\"PassengerId\"]==PassengerId,\"Pclass\"] = base_data.Pclass.mode()\n                new_data.loc[new_data[\"PassengerId\"]==PassengerId,\"Fare\"] = base_data.Fare.median()\n            else:    \n                pclass_impute = (base_data.groupby('Pclass')['Fare'].median() - fare).abs().idxmin()\n                new_data.loc[new_data[\"PassengerId\"]==PassengerId,\"Pclass\"] = pclass_impute          \n  \n    new_data[\"Pclass\"] = new_data[\"Pclass\"].astype(int)   \n   \n    # I assume name and sex will be provided.\n        \n    # SibSp\n    new_data.loc[new_data.SibSp != new_data.SibSp, \"SibSp\"] = base_data.SibSp.mode()[0]\n\n    # Parch\n    new_data.loc[new_data.Parch != new_data.Parch, \"Parch\"] = base_data.Parch.mode()[0]\n    \n    # Adding Family Size and Family Type\n    new_data[\"Family_size\"] = new_data[\"SibSp\"] + new_data[\"Parch\"] +1\n    new_data[\"Family_type\"] = \"Alone\"\n    new_data.loc[new_data[\"Family_size\"] > 1, \"Family_type\"] = \"Small\"\n    new_data.loc[new_data[\"Family_size\"] > 4, \"Family_type\"] = \"Big\"\n        \n    # Fare\n    # TODO: impute it based on Pclass.\n    new_data.loc[new_data.Fare != new_data.Fare, \"Fare\"] = base_data.Fare.median()\n    \n    # New feature based on: https:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688\n    new_data['Adj_fare'] = new_data['Fare'] \/ new_data['Family_size']\n    \n    # Creating new attribute to address Cabin Assignment.\n    new_data[\"Cabin_assigned\"] = 1\n    new_data.loc[new_data.Cabin != new_data.Cabin,\"Cabin_assigned\"] = 0\n    \n    # Impute missing values of Embarked with mode.\n    # TODO: use other factors, mode is not accurate.\n    new_data.loc[new_data.Embarked != new_data.Embarked,\"Embarked\"] = base_data.Embarked.mode()[0]\n\n    # Create a title column    \n    for my_data in [base_data,new_data]:\n        my_data[\"Title\"] = my_data.Name.str.extract('\\w+,\\s+([a-zA-z\\s]+)\\s*\\..*')\n        my_data[\"Last_Name\"] = my_data.Name.str.extract('(\\w+),\\s+[a-zA-z\\s]+\\s*\\..*')\n\n    title_set = set(base_data.Title.unique())\n    title_set.update(['Other','Special'])\n    \n    # Replace Misc. titles\n    mrs = [\"Mrs\",\"Mme\"]\n    miss = [\"Mlle\",\"Miss\",\"Ns\"]\n    other = [\"Capt\",\"Col\",\"Major\",\"Dr\",\"Rev\",\"Mlle\",\"Mme\",\"Ms\",\"Ms\",]\n    special = [\"Lady\", \"Don\", \"Dona\", \"Sir\", \"the Countess\", \"Jonkheer\"]     \n\n    for my_data in [base_data,new_data]:\n        my_data[\"Title\"].replace(mrs,\"Mrs\", inplace=True)\n        my_data[\"Title\"].replace(miss,\"Miss\", inplace=True)\n        my_data[\"Title\"].replace(other,\"Other\", inplace=True)\n        my_data[\"Title\"].replace(special,\"Special\", inplace=True)\n   \n    Titles = [\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Other\",\"Special\"]\n\n    # Any other title other than what we have seen in training data,\n    # will be other. \n    new_data.loc[new_data[\"Title\"].apply(lambda x: x not in title_set),\"Title\"] = \"Other\"\n\n    # Impute missing values of Age with median of training data\n    # See your previous R kernel for other methods of imputing Age.\n    # new_data.loc[new_data.Age != new_data.Age, \"Age\"] = base_data.Age.median()\n    \n    new_data[\"Sex_numerical\"] = new_data[\"Sex\"].map({\"male\": 1, \"female\": 0}).astype(int)   \n    new_data[\"Age_missing\"] = 0\n    new_data.loc[new_data.Age.isnull(),\"Age_missing\"] = 1\n            \n    # estimating missing age based on Title and Pclass:\n    for title in Titles:\n        for pclass in range(1,4):\n            try:\n                agg_df = base_data.groupby(['Title','Pclass'])['Age'].median()\n                age_to_impute = np.asscalar(agg_df.loc[[(title, pclass)]].values)\n                new_data.loc[(new_data['Age'].isnull()) & (new_data['Title'] == title) & (new_data['Pclass'] == pclass), 'Age'] = age_to_impute\n            except:\n                agg_df = base_data.groupby(['Title'])['Age'].median()\n                age_to_impute = np.asscalar(agg_df.loc[[(title)]].values)\n                new_data.loc[(new_data['Age'].isnull()) & (new_data['Title'] == title), 'Age'] = age_to_impute\n\n    new_data[\"Child\"] = 0\n    new_data.loc[new_data[\"Age\"]<18,\"Child\"] = 1\n\n        \n    # Adding two extra features:\n    # Reference: https:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688\n    new_data['Fsize_age'] = new_data['Family_size'] + new_data['Age']\/70\n    new_data['Adj_fare'] = new_data['Fare'] \/ new_data['Family_size']\n\n    if create_dummy:\n        ## Converting categorical values to dummies\n        new_data = pd.concat([new_data, pd.get_dummies(new_data[\"Embarked\"], prefix='Embarked')], axis=1)\n        new_data = pd.concat([new_data, pd.get_dummies(new_data[\"Title\"], prefix='Title')], axis=1)\n        new_data = pd.concat([new_data, pd.get_dummies(new_data[\"Pclass\"], prefix='Pclass')], axis=1)\n        new_data = pd.concat([new_data, pd.get_dummies(new_data[\"Family_type\"], prefix='Ftype')], axis=1)\n\n    return new_data\n\ndef plot_feature_importance(model,X_train,model_name,num_features):\n    fig, axes = plt.subplots(nrows = 1, ncols = 1, sharex=\"all\", figsize=(10,3))\n    indices = np.argsort(model.feature_importances_)[::-1][:num_features]\n    g = sns.barplot(y = X_train.columns[indices][:num_features],\n                    x = model.feature_importances_[indices][:num_features] , orient='h')\n    g.set_xlabel(\"Relative importance\",fontsize=12)\n    g.set_ylabel(\"Features\",fontsize=12)\n    g.tick_params(labelsize=9)\n    g.set_title(model_name + \" feature importance\");\n    \n    \ndef evaluate_model_with_test_data(test_set,features,model):\n    Y_test = test_set[\"Survived\"]\n    X_test = test_set[features]\n    return accuracy_score(Y_test, model.predict(X_test)) \n\ndef retrain_with_whole_data(data,model,features):\n    Y_train = data[\"Survived\"]\n    X_train = data[features]\n    return model.fit(X_train,Y_train)\n\ndef predict_test_data(model, data, features):\n    X_test_prediction = data[features]\n    return pd.Series(model.predict(X_test_prediction).astype(int), name=\"Survived\")\n\n","a75c1da5":"## Loading data\ntrain_data = load_data('..\/input\/titanic\/train.csv', delimiter = ',')\ntest_data = load_data('..\/input\/titanic\/test.csv', delimiter = ',')","3a158b6c":"# Cleaning data\ntr_data_c = clean_titanic_dataset(train_data,train_data, create_dummy=True)\nts_data_c = clean_titanic_dataset(train_data,test_data, create_dummy=True)\nIDtest = ts_data_c[\"PassengerId\"]","a3ffd06d":"# Setting Index Column to PassengerId\ntr_data_c.set_index(\"PassengerId\",inplace=True)\nts_data_c.set_index(\"PassengerId\",inplace=True)","e01cf044":"# Adding one more features: Family Survival\n# The idea is mentioned:\n# https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83\n# https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3\n# https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever\n# https:\/\/www.kaggle.com\/c\/titanic\/discussion\/57447#latest-592673\n# https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818\n# Following implementation is according to: https:\/\/www.kaggle.com\/mauricef\/titanic\n\ndf = pd.concat([tr_data_c, ts_data_c], axis=0, sort=False)\n\n# Choose Woman or child\ndf['Is_woman_child'] = ((df.Title == 'Master') | (df.Sex == 'female'))\n\n# Computing family survival rate other than the individual\nfamily = df.groupby(df.Last_Name).Survived\ndf['Family_total_count'] = family.transform(lambda s: s[df.Is_woman_child].fillna(0).count())\ndf['Family_total_count'] = df.mask(df.Is_woman_child, df.Family_total_count - 1, axis=0)\ndf['Family_survived_count'] = family.transform(lambda s: s[df.Is_woman_child].fillna(0).sum())\ndf['Family_survived_count'] = df.mask(df.Is_woman_child, df.Family_survived_count - df.Survived.fillna(0), axis=0)\ndf['Family_survival_rate'] = (df.Family_survived_count \/ df.Family_total_count.replace(0, np.nan))\ndf['Is_single_traveler'] = df.Family_total_count == 0\ndf.Family_survival_rate.fillna(0, inplace=True);","9dce1dc1":"train_data_cleaned, test_data_cleaned = df.loc[tr_data_c.index], df.loc[ts_data_c.index]   \n# train_data_cleaned: total train data\n# test_data_cleaned: competition test data\n# train_set: subset of train_data to train prediction models\n# test_set: subset of train_data to test the models. ","991adcac":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=914)\nfor train_index, test_index in split.split(train_data_cleaned, train_data_cleaned[[\"Sex\"]]):\n    train_set = train_data_cleaned.iloc[train_index]\n    test_set = train_data_cleaned.iloc[test_index]","f093e730":"# Inspired by: https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3\n\nlgbm_grid_search = False\nlgbm_feature_names = [                   \n                   'Sex_numerical'\n                 , 'Family_survival_rate'\n                 , 'Is_single_traveler'\n                 #, 'Adj_fare'\n                 #, 'Pclass_1'\n                 #, 'Pclass_2'\n                 #, 'Pclass_3'\n                 #, 'Title_Master'\n                 #, 'Title_Miss'\n                 #, 'Title_Mr'\n                 #, 'Ftype_Small'\n                 #, 'Ftype_Big'        \n                     ]\n\nY_train = train_set[\"Survived\"]\nX_train = train_set[lgbm_feature_names]\n\n\nif lgbm_grid_search:\n    fit_params = {\"early_stopping_rounds\" : 100, \n                 \"eval_metric\" : 'auc', \n                 \"eval_set\" : [(X_train,Y_train)],\n                 'eval_names': ['valid'],\n                 'verbose': 0,\n                 'categorical_feature': 'auto'}\n\n    param_test = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n                  'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000],\n                  'num_leaves': randint(6, 200), \n                  'min_child_samples': randint(100, 500), \n                  'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n                  'subsample': uniform(loc = 0.2, scale = 0.8), \n                  'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7,8,9,10],\n                  'colsample_bytree': uniform(loc = 0.4, scale = 0.6),\n                  'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n                  'reg_lambda': [0, 0.01, 0.1, 0.3, 1, 5, 10, 20, 50, 100]}\n\n    lgbm_clf = lgbm.LGBMClassifier(random_state = 914, silent = True, metric = 'None', n_jobs = 16)\n    grid_search = RandomizedSearchCV(\n        estimator = lgbm_clf, param_distributions = param_test, \n        n_iter = 2000,\n        scoring = 'accuracy',\n        cv = 5,\n        refit = True,\n        random_state = 914,\n        verbose = True)\n\n    grid_search.fit(X_train, Y_train, **fit_params)\n    opt_parameters = grid_search.best_params_\n    print(grid_search.best_params_)\nelse:\n    opt_parameters =     {\n        'colsample_bytree': 0.951896848025216,\n        'learning_rate': 0.2,\n        'max_depth': 10,\n        'min_child_samples': 102,\n        'min_child_weight': 0.01,\n        'n_estimators': 400,\n        'num_leaves': 102,\n        'reg_alpha': 2,\n        'reg_lambda': 0.5,\n        'subsample': 0.4194694182848429}\n      \nlgbm_best = lgbm.LGBMClassifier(**opt_parameters)\nlgbm_model_best_ = lgbm_best.fit(X_train,Y_train)    ","f71c7989":"# plot feature importance\nplot_feature_importance(lgbm_model_best_,X_train,'model_name',3)","95307b0e":"evaluate_model_with_test_data(test_set,lgbm_feature_names,lgbm_model_best_)","674939e1":"# Retrain with all data and predict the test data\nlgbm_on_training = retrain_with_whole_data(train_data_cleaned,lgbm_model_best_,lgbm_feature_names)\ntest_Survived_lgbm = predict_test_data(lgbm_on_training, test_data_cleaned,lgbm_feature_names)","5b1b9f94":"results = pd.concat([IDtest,test_Survived_lgbm],axis=1)\nresults.to_csv(\"submission.csv\",index=False)","c8d6114c":"## Classifier","738fdbb5":"There are many valuable kernels and discussions on Kaggle, which significantly helped me to think about the problem from different perspectives. I tried many kernels and investigated the ideas. Many Kernels have data exploratory analysis, data preprocessing\/cleaning, feature extraction, and developing prediction models. All of these kernels are valuable and provide an insight into the dataset. \nThis kernel is not about exploratory data analysis or developing a prediction model, although I added some model in the end. It is about providing functions to clean data and provide a data frame ready for ML models, including most of the features that so far has been used in different kernels and discussions. Many kagglers gave a thought, worked, and implemented them. I am compiling them in one place.\nIn the main cleaning data function (clean_titanic_dataset), I assume we do not have access to test data for cleaning, and imputing missing values.\n\nKernels that were extremely helpful in developing this kernel (the list is not complete, will be updated):    \n[Titanic WCG+XGBoost - 0.84688](https:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688) by [cdeotte](https:\/\/www.kaggle.com\/cdeotte)         \n[~200 lines | Randomized Search + LGBM : 82.3%](https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3) by [vincentlugat](https:\/\/www.kaggle.com\/vincentlugat)       \n[titanic](https:\/\/www.kaggle.com\/mauricef\/titanic) by [mauricef](https:\/\/www.kaggle.com\/mauricef)    \n\n------\nFeel free to copy and edit this kernel.\n","0e11369f":"### Utility Functions","89a30afa":"### LGBM"}}