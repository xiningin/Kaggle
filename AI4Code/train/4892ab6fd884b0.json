{"cell_type":{"202e1476":"code","499f9252":"code","59db67de":"code","ff55f41f":"code","67465b1a":"code","4328107c":"code","19f3c10e":"code","c9af3323":"code","96947858":"code","b15b7b8d":"code","099c9a6b":"code","6f558b11":"code","b6f414b1":"code","04312fcb":"code","ebde3720":"code","3b053730":"code","b2299e1c":"code","c39b9c4b":"code","17728243":"code","38bc9037":"code","2181a5be":"code","7b150062":"markdown","2126abb7":"markdown","aee972fc":"markdown","642e527f":"markdown","af39c4f1":"markdown","e0671e67":"markdown","58fe16b2":"markdown","b744616f":"markdown","f43be814":"markdown","c1294a4e":"markdown","4d236dd6":"markdown","36ce2436":"markdown","ae8b7575":"markdown","bcbfe366":"markdown","a41fa29f":"markdown","e29067bd":"markdown","a5e35a16":"markdown","545b9f90":"markdown","b0352b6f":"markdown","0797aba6":"markdown","1994e900":"markdown"},"source":{"202e1476":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected = True)\nimport plotly.graph_objs as go\n\nfrom sklearn.metrics import confusion_matrix\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","499f9252":"data = pd.read_csv(\"..\/input\/pulsar_stars.csv\")","59db67de":"data.head() #using pandas head function for first 5 component","ff55f41f":"# information about dataset\ndata.info()","67465b1a":"y = data.target_class # target class chosen as y values\nx_data = data.drop([\"target_class\"],axis = 1) \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values # normalization x values","4328107c":"\nf,ax = plt.subplots(figsize=(7,7))\n\nsns.heatmap(x.corr(),annot = True ,linewidths = .4,ax = ax)","19f3c10e":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42) #20% test and 80% train\n# ","c9af3323":"# implement lr with sklearn\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train,y_train)\n\nform = lr.score(x_test,y_test)*100\nprint(\"Random Forrest accuracy : {0:.2f}%\".format(form))\n","96947858":"from sklearn.metrics import confusion_matrix\n# Calculation of counfusion matrix\ny_pred = lr.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n# and plotting\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths=.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","b15b7b8d":"from sklearn.neighbors import KNeighborsClassifier\n# implement knn with sklearn\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train,y_train)\n\nform = knn.score(x_test,y_test)*100\nprint(\"KNN accuracy : {0:.2f}%\".format(form))","099c9a6b":"# Calculation of counfusion matrix\ny_pred = knn.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n# and plotting\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths=.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","6f558b11":"from sklearn.svm import SVC\n# implement svm with sklearn\nssvm = SVC(random_state=42)\nssvm.fit(x_train,y_train)\nform = ssvm.score(x_test,y_test)*100\nprint(\"SVM Accuracy : {0:.2f}%\".format(form))","b6f414b1":"# Calculation of counfusion matrix\ny_pred = ssvm.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n# and plotting\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths=.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","04312fcb":"from sklearn.tree import DecisionTreeClassifier\n# implement decision tree with sklearn\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nform = dt.score(x_test,y_test)*100\nprint(\"Decision Tree Accuracy : {0:.2f}%\".format(form))","ebde3720":"# Calculation of counfusion matrix\ny_pred = dt.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n# and plotting\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths=.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","3b053730":"from sklearn.ensemble import RandomForestClassifier\n# implement random forest with sklearn\nrf = RandomForestClassifier(n_estimators=200,random_state=42)\nrf.fit(x_train,y_train)\nform = rf.score(x_test,y_test)*100\nprint(\"Random Forrest accuracy : {0:.2f}%\".format(form))","b2299e1c":"# Calculation of counfusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n# and plotting\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths=.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","c39b9c4b":"from sklearn.naive_bayes import GaussianNB\n# implement naive bayes with sklearn\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nform = nb.score(x_test,y_test)*100\nprint(\"Naive Bayes accuracy : {0:.2f}%\".format(form))","17728243":"# Calculation of counfusion matrix\ny_pred = nb.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\n# and plotting\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidths=.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","38bc9037":"#score list and name list created\nscores = [lr.score(x_test,y_test)*100,ssvm.score(x_test,y_test)*100,knn.score(x_test,y_test)*100,\n    dt.score(x_test,y_test)*100,rf.score(x_test,y_test)*100,nb.score(x_test,y_test)*100]\nnames = [\"Logistic Regression\",\"SVM\",\"KNN\",\"Decision Tree\",\"Random Forest\",\"Naive Bayes\"]","2181a5be":"fig = {\n  \"data\": [\n    {\n      \"values\": scores,\n      \"labels\": names,\n      \"domain\": {\"x\": [0, .5]},\n      \"name\": \"Score Rates\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .3,\n      \"type\": \"pie\"\n    },],\n  \"layout\": {\n        \"title\":\"Classifier Algorithms Score Rates\",\n        \"annotations\": [\n            { \"font\": { \"size\": 20},\n              \"showarrow\": False,\n              \"text\": \"Classifier Rates\",\n                \"x\": 0.20,\n                \"y\": 1\n            },\n        ]\n    }\n}\niplot(fig)","7b150062":"1. Introduction\n2. Read and Overview Data\n3. Data Preparation\n4. Logistic Regression\n5. KNN\n6. SVM\n7. Recision Tree\n8. Random Forrest\n9. Naive Bayes\n10. Compare Scores\n11. Conclusion","2126abb7":"# 5.  KNN","aee972fc":"* We will compare scores with pie chard\n* Use piechard because see the accuracy rate between each other","642e527f":"* Modules","af39c4f1":"# 10. Compare Scores","e0671e67":"# Contents\n","58fe16b2":"# 7. Decision Tree ","b744616f":"# 2. Read and Overview Data\n","f43be814":"# 3. Data Preparation","c1294a4e":"* first 5 component of data","4d236dd6":"# 6. SVM","36ce2436":"# 11. Conclusion\n* Classifications applied to data\n* Score values obtained and compared\n* According to this data, the most efficient result was KNN classification","ae8b7575":"everything looks fine","bcbfe366":"# 1. Introduction\n\nIn this kernel we will do the determination of pulsar star according to the Predicting a Pulsar Star dataset. We will use mechine learning classifiers while determining. These classifiers are Logistic Regression, KNN, SVM, Naive Bayes, Decision Tree and Random Forrest. Finally we will compare them according to scores and Confusion Matrix","a41fa29f":"# 4. Logistic Regression ","e29067bd":"* Then we will x and y values split as test and train with sklearn module","a5e35a16":"# 9. Naive Bayes ","545b9f90":"# 8. Random Forest  ","b0352b6f":"* Transactions complated we can start the algorithms","0797aba6":"* Choose x and y values for machine learning algorithms\n* y values are binary result\n* x values are features","1994e900":"* Read dataset with pandas"}}