{"cell_type":{"c5684b7f":"code","40780d74":"code","daded206":"code","05af81d2":"code","ceee6c6f":"code","0c6e435f":"code","0d8344f4":"code","f5626ac3":"code","f449ae3c":"code","9440549f":"code","faf15739":"code","f15d89cd":"code","1694d9bb":"code","f823bc47":"code","274dfd2d":"code","d00a414e":"code","18001043":"code","61e29be2":"code","5c69147b":"code","c35d1279":"code","433fb729":"code","499b49e1":"code","4ca33e2c":"code","8dff5a19":"code","4ab34a9a":"code","5f260a86":"code","c308e8b0":"code","1bfadea2":"code","a0907688":"code","ded17c13":"code","500464cd":"code","15041c20":"code","e184bfe3":"code","bf3b1203":"code","52acfd72":"code","13f4efeb":"code","4ac4e5b1":"code","79382ed1":"code","2bf69729":"code","baaba03b":"code","1881933e":"code","4f4eaa2e":"code","6b62d30c":"code","451c1304":"code","09224fce":"markdown","ef1a4a29":"markdown","ee4d6f6c":"markdown","577b0fde":"markdown","a75a5d04":"markdown","cf16e439":"markdown","dca6bd35":"markdown","48904903":"markdown","d7769195":"markdown","75876c73":"markdown","70e997f4":"markdown","b9441d75":"markdown","9f1cc46f":"markdown","586f4960":"markdown","bda14ee5":"markdown","f44844f2":"markdown","23e47bab":"markdown","79da86c3":"markdown","a896face":"markdown","a9b48312":"markdown","1511b9eb":"markdown","6a19fd98":"markdown","fbba50d5":"markdown","0c672d43":"markdown","ebc1dfac":"markdown","2fe1c548":"markdown","fa4d169f":"markdown","5bb93104":"markdown","ecbab53d":"markdown","9cc9dd31":"markdown","d2e072a0":"markdown","f5be9d66":"markdown"},"source":{"c5684b7f":"import pandas as pd\nimport numpy as np\n\n#!pip uninstall scikit-learn\n#!pip install scikit-learn==0.13\n\nfrom nltk.corpus import stopwords\nimport re\n\n\nfrom sklearn.model_selection import train_test_split # Split Data \nfrom imblearn.over_sampling import SMOTE # Handling Imbalanced\n\n# for Model Building\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score # Performance Metrics  \n\n\n# for Visualization \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom termcolor import cprint\nimport seaborn as sns\nimport warnings   \n\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","40780d74":"data=pd.read_csv('..\/input\/tweets\/Tweets.csv')","daded206":"data.head()","05af81d2":"data.tail()","ceee6c6f":"data.info()","0c6e435f":"data.describe()","0d8344f4":"data.shape","f5626ac3":"data.isnull().sum()","f449ae3c":"cprint(\"Total number of sentiments of tweets :\",'blue')\nprint(data.airline_sentiment.value_counts())\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'airline_sentiment', data = data)\nax.set_title(label = 'Total number of sentiments of tweets', fontsize = 22)\nplt.show()","9440549f":"ax.set_title(label = 'Total number of sentiments of tweets:')\ncolors=sns.color_palette('husl',10)\npd.Series(data['airline_sentiment']).value_counts().plot(kind='pie',colors=colors,labels=['Negative','Neutral','Postive'],explode=[0.05,0.02,0.04],shadow=True,autopct='%.2f',fontsize=12,figsize=(6,6),title=\"Total Tweets for Each Sentiment\")\nplt.show()","faf15739":"colors=sns.color_palette('hls',10)\npd.Series(data['airline']).value_counts().plot(kind=\"bar\",color=colors,figsize=(10,8),fontsize=10,rot=0,title='Total No. of Tweets of Airline')\nplt.xlabel('Airlines',fontsize=13)\nplt.ylabel('No.of Tweets',fontsize=13)","f15d89cd":"cprint(\"Total number of tweets for each airline :\",'blue')\nprint(data.groupby('airline')['airline_sentiment'].count())\n\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'airline', data = data, palette = 'pastel')\nax.set_title(label = 'Total number of tweets for each airline', fontsize = 20)\nplt.show()\n\ncprint(\"Total number of sentiment tweets for each airline :\",'blue')\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nfor i in airlines :\n    print('{} : \\n'.format(i),data.loc[data.airline == i].airline_sentiment.value_counts())","1694d9bb":"cprint('Reasons Of Negative Tweets :','blue')\nprint(data.negativereason.value_counts())\n\nplt.figure(figsize = (24, 12))\nsns.countplot(x = 'negativereason', data = data, palette = 'hls')\nplt.title('Reasons Of Negative Tweets About Airlines', fontsize = 22)\nplt.show()","f823bc47":"NR_Count=data['negativereason'].value_counts()\ndef NCount(Airline):\n    airlineName =data[data['airline']==Airline]\n    count= airlineName['negativereason'].value_counts()\n    Unique_reason= data['negativereason'].unique()\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\n\ndef Plot_Reason(airline):\n    a= NCount(airline)\n    count=a['count']\n    Id = range(1,(len(a)+1))\n    plt.bar(Id,count, color=['darkviolet','yellow','blue','lime','pink','crimson','gold','cyan','orange','purple'])\n    plt.xticks(Id,a['Reasons'],rotation=90)\n    plt.title('Count of Reasons for '+ airline)\n    \nplt.figure(2,figsize=(16, 14))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    Plot_Reason(i)","274dfd2d":"positive=data[data['airline_sentiment']=='positive'].text\nneutral=data[data['airline_sentiment']=='neutral'].text\nnegative=data[data['airline_sentiment']=='negative'].text","d00a414e":"plt.figure(figsize=(24,20))\nworld_cloud_postive=WordCloud(min_font_size=3,max_words=3200,width=1600,height=720).generate(\"\".join(positive))\nplt.imshow(world_cloud_postive,interpolation='bilinear')\nax.grid(False)","18001043":"plt.figure(figsize=(24,12))\nworld_cloud_neutral=WordCloud(min_font_size=3,max_words=3200,width=1600,height=720).generate(\" \".join(neutral))\nplt.imshow(world_cloud_neutral,interpolation='bilinear')\nax.grid(False)","61e29be2":"plt.figure(figsize = (24,12)) \nworldcould_neg = WordCloud(min_font_size = 3,  max_words = 3200 , width = 1600 , height = 720).generate(\" \".join(negative))\nplt.imshow(worldcould_neg,interpolation = 'bilinear')\nax.grid(False)","5c69147b":"# convert Sentiments to 0,1,2\ndef convert_Sentiment(sentiment):\n    if  sentiment == \"positive\":\n        return 2\n    elif sentiment == \"neutral\":\n        return 1\n    elif sentiment == \"negative\":\n        return 0","c35d1279":"# Apply convert_Sentiment function\ndata.airline_sentiment = data.airline_sentiment.apply(lambda x : convert_Sentiment(x))","433fb729":"data.airline_sentiment","499b49e1":"# Remove html tag\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\n# Remove stop words\ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n    return text\n\n# Remove url  \ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# Remove punct\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove @username\ndef remove_username(text):\n    return re.sub('@[^\\s]+','',text)\n\n# Remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n# Decontraction text\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text  \n\n# Seperate alphanumeric\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ndef char(text):\n    substitute = re.sub(r'[^a-zA-Z]',' ',text)\n    return substitute\n\n# combaine negative reason with  tweet (if exsist)\ndata['processedtext'] = data['negativereason'].fillna('') + ' ' + data['text'] \n# 1. Remove HTML tag\ndata['processedtext'] = data['processedtext'].apply(lambda x : remove_html(x))\n# 2. Remove Alpha Numeric\ndata['processedtext'] = data['processedtext'].apply(lambda x : seperate_alphanumeric(x))\n# 3. Remove Special characters\ndata['processedtext'] = data['processedtext'].apply(lambda x : unique_char(cont_rep_char,x))\ndata['processedtext'] = data['processedtext'].apply(lambda x : remove_emoji(x))\ndata['processedtext'] = data['processedtext'].apply(lambda x : remove_url(x))\n\ndata['processedtext'] = data['processedtext'].apply(lambda x : remove_username(x))\n\n# 4.removal of stop words\ndata['processedtext'] = data['processedtext'].apply(lambda x : remove_stopwords(x))\n\ndata['processedtext'] = data['processedtext'].apply(lambda x : decontraction(x))\ndata['processedtext'] = data['processedtext'].apply(lambda x : char(x))\n\n# 5.convert to lower case\ndata['processedtext'] = data['processedtext'].apply(lambda x : x.lower())\n","4ca33e2c":"# result\ndata['processedtext']","8dff5a19":"X = data['processedtext']\ny = data['airline_sentiment']","4ab34a9a":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfid = TfidfVectorizer()\nX_final =  tfid.fit_transform(X)","5f260a86":"smote = SMOTE()\nx_sample,y_sample = smote.fit_resample(X_final,y)","c308e8b0":"X_train , X_test , y_train , y_test = train_test_split(x_sample , y_sample , test_size=0.25,random_state=3)","1bfadea2":"random_forest_classifier = RandomForestClassifier()\nrandom_forest_classifier.fit(X_train,y_train)","a0907688":"random_forest_classifier_prediction =  random_forest_classifier.predict(X_test)","ded17c13":"accuracy_score(random_forest_classifier_prediction,y_test)","500464cd":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)","15041c20":"xgb_prediction =  xgb.predict(X_test)","e184bfe3":"accuracy_score(xgb_prediction,y_test)","bf3b1203":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)","52acfd72":"gbc_prediction =  gbc.predict(X_test)","13f4efeb":"accuracy_score(gbc_prediction,y_test)","4ac4e5b1":"svm = SVC()\nsvm.fit(X_train,y_train)","79382ed1":"svm_prediction =  svm.predict(X_test)","2bf69729":"accuracy_score(svm_prediction,y_test)","baaba03b":"des_tree_classifier = DecisionTreeClassifier()\ndes_tree_classifier.fit(X_train,y_train)","1881933e":"des_tree_classifier_prediction=des_tree_classifier.predict(X_test)","4f4eaa2e":"accuracy_score(des_tree_classifier_prediction,y_test)","6b62d30c":"cr = classification_report(y_test, random_forest_classifier_prediction)","451c1304":"print(\"Classification Report:\\n----------------------\\n\", cr)\n\ncm = confusion_matrix(y_test,random_forest_classifier_prediction)\n\n\n# plot confusion matrix \nplt.figure(figsize=(10,6))\nsentiment_classes = ['Negative', 'Neutral', 'Positive']\nsns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n            xticklabels=sentiment_classes,\n            yticklabels=sentiment_classes)\nplt.title('Confusion matrix', fontsize=16)\nplt.xlabel('Actual label', fontsize=12)\nplt.ylabel('Predicted label', fontsize=12)\nplt.show()","09224fce":"## The distribution of sentiment across all the tweets","ef1a4a29":"##  Solving data imbalance using SMOTE","ee4d6f6c":"## World could of Neutral sentiments","577b0fde":"## Model Score - Accuracy","a75a5d04":"## Plot the word cloud graph of tweets for positive and negative sentiment separately.","cf16e439":"## Import the libraries","dca6bd35":"There are 14640 rows and 15 columns","48904903":"## Model Score - Accuracy","d7769195":"## The distribution of Sentiment of tweets for each airline ","75876c73":"##  Load dataset","70e997f4":"## Split Text Of Sentiments","b9441d75":"## Fit Model - Decision Tree","9f1cc46f":"# VISUALIZE MODEL PERFORMENCE","586f4960":"#### Background and Context\n\nTwitter possesses 330 million monthly active users, which allows businesses to reach a broad population and connect with customers without intermediaries. On the other hand, there\u2019s so much information that it\u2019s difficult for brands to quickly detect negative social mentions that could harm their business.\n\nThat's why sentiment analysis\/classification, which involves monitoring emotions in conversations on social media platforms, has become a key strategy in social media marketing.\n\nListening to how customers feel about the product\/service on Twitter allows companies to understand their audience, keep on top of what\u2019s being said about their brand and their competitors, and discover new trends in the industry.\n\n#### Data Description:\n\nA sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n\n \n#### Dataset:\n\nThe dataset has the following columns:\n\ntweet_id                                                           \nairline_sentiment                                               \nairline_sentiment_confidence                               \nnegativereason                                                   \nnegativereason_confidence                                    \nairline                                                                    \nairline_sentiment_gold                                              \nname     \nnegativereason_gold\nretweet_count\ntext\ntweet_coord\ntweet_created\ntweet_location \nuser_timezone","bda14ee5":"## EDA","f44844f2":"## Data shape and description","23e47bab":"## Fit Model -  XGBClassifier","79da86c3":"## Text pre-processing and cleaning\n\n a. Html tag removal.\n \n b. Tokenization.\n \n c. Remove the numbers.\n \n d. Removal of Special Characters and Punctuations.\n \n e. Removal of stopwords\n \n f. Conversion to lowercase.\n \n g. Lemmatize or stemming.\n \n h. Join the words in the list to convert back to text string in the data frame. (So that each row\n          contains the data in text format.)\n          \n i. Print the first 5 rows of data after pre-processing.\n ","a896face":"## Model Score - Accuracy","a9b48312":"## Using TfidfVectorizer","1511b9eb":"## World could of Negative sentiments","6a19fd98":"Null values are observed in dataset.","fbba50d5":"## Model Score - Accuracy","0c672d43":"SVM and Random Forest Classifier gave best accuracy score. RF Classifier also gave good F Score , Precision and Recall.","ebc1dfac":"## Fit Model - Gradient Boosting Classifier","2fe1c548":"## Fit Model - Support vector machine","fa4d169f":"## Reasons Of Negative Tweets Per AirLines","5bb93104":"## Split dataset","ecbab53d":"## Fit Model - Random Forest","9cc9dd31":"## Distribution of all the negative reasons. ","d2e072a0":"## Model Score - Accuracy","f5be9d66":"## The distribution of all tweets among each airline."}}