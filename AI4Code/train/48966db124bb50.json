{"cell_type":{"dc1e8ae2":"code","b8310acd":"code","bab41053":"code","e62f7253":"code","1fec23ef":"code","0159534d":"code","6113db77":"code","54bca11b":"code","808fe28f":"code","dca416f9":"code","8c14d575":"code","24994fdc":"code","05f89453":"code","6cfbefb2":"code","863ef813":"code","ccd59881":"code","1a90b4bc":"code","b72a3bc8":"code","1df26481":"code","ae75a35e":"code","44b91a0f":"code","c7ec2100":"code","49f0fedc":"code","853bacd9":"code","ec801161":"code","15fafadc":"code","bde23185":"code","f30a6ca1":"code","763304bf":"code","d08f6af4":"code","c815de2f":"code","8ca75bbe":"code","acd289aa":"code","b1d72031":"code","43159c7d":"code","cb2e74c9":"code","bf1952ea":"code","8f1caa85":"code","f73eb99b":"code","b906195a":"code","9bddddad":"code","c70c5b8c":"code","b41c35e0":"code","9edfa6f3":"code","b26c7a71":"code","99b526e9":"code","a9be1a42":"code","403ca390":"code","cd7ad281":"code","82190ca7":"code","feef887f":"code","db70a618":"code","3db90d43":"code","45c93e96":"markdown","8fe3656a":"markdown","4a66001b":"markdown","3dbcb839":"markdown","285c0d62":"markdown","d78ec818":"markdown","09595f83":"markdown"},"source":{"dc1e8ae2":"# Import necessary library packages\n\nimport cv2 \nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport pandas_profiling as pp\nimport tensorflow as tf\nimport time\n\nfrom skimage.io import imread\nfrom sklearn import metrics, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Convolution2D, GlobalAveragePooling2D, Activation, Dense, BatchNormalization, Dropout, MaxPooling2D, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.python.keras.engine.base_layer import Layer","b8310acd":"# Define directories for training and test images\n# Display the training and sample submission CSV file\n\ntrain_dir = '\/kaggle\/input\/severstal-steel-defect-detection\/train_images'\ntest_dir = '\/kaggle\/input\/severstal-steel-defect-detection\/test_images'\ntrain_df = pd.read_csv('\/kaggle\/input\/severstal-steel-defect-detection\/train.csv')\nsubmission_df = pd.read_csv('\/kaggle\/input\/severstal-steel-defect-detection\/sample_submission.csv')","bab41053":"# Display dataframe for training images\n\n# pd.set_option('display.max_rows', None)\ntrain_df.head(len(train_df))","e62f7253":"# Generate a profile report of the training dataset\n\npp.ProfileReport(train_df)","1fec23ef":"# Summary of the training dataset\n\ntrain_df.info()","0159534d":"# Display dataframe for testing images\n\n# pd.set_option('display.max_rows', None)\nsubmission_df.head(len(submission_df))","6113db77":"# Read a sample training image with its information displayed\n\ndisplay(train_df.iloc[1111])\nimage = imread(train_dir + '\/' + train_df['ImageId'][1111])\n# image = imread(train_dir + '\/' + '282cd397d.jpg')\n\n#The size of original image is (256, 1600)\n\nplt.figure(figsize = None) \nplt.imshow(image)\nplt.show()","54bca11b":"# Display the dimension information for a sample training image\n\ninput_shape = image.shape\ninput_shape","808fe28f":"# Read multiple samples of training images\n\ntraining = []\ntraining.append(imread(train_dir + '\/' + train_df['ImageId'][1111]))\ntraining.append(imread(train_dir + '\/' + train_df['ImageId'][2222]))\ntraining.append(imread(train_dir + '\/' + train_df['ImageId'][3333]))\ntraining.append(imread(train_dir + '\/' + train_df['ImageId'][4444]))\ntraining.append(imread(train_dir + '\/' + train_df['ImageId'][5555]))\n\nlabels = [] \nlabels.append(train_df['ClassId'][1111])\nlabels.append(train_df['ClassId'][2222])\nlabels.append(train_df['ClassId'][3333])\nlabels.append(train_df['ClassId'][4444])\nlabels.append(train_df['ClassId'][5555])\n\n# Resized training images\n\nplt.figure(figsize = [25,4]) \n\nfor x in range(0,5):\n    plt.subplot(3, 3,x + 1)\n    plt.subplots_adjust(hspace = 1.0)\n    plt.imshow(training[x])\n    plt.title(\"Defect Class: {}\".format(labels[x]))\n    x += 1\n    \nplt.show()","dca416f9":"# Creating the dict with classId and Encoded pixels and group all together\n# Creating the dict\n# train_df['ClassId_EncodedPixels'] = train_df.apply(lambda row: [row['ClassId'], row['EncodedPixels']], axis = 1)\n\n# Grouping together\n# grouped_EncodedPixels = train_df.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\n\n# Display the respective information of the training dataset\nprint('Number of unique images: %s' % len(train_df['ImageId'].drop_duplicates()))\nprint('Number of images having at least one defect: %s' % len(train_df[train_df['EncodedPixels'] != -1]['ImageId'].unique()))\nprint('Total training samples: %s' % len(train_df))","8c14d575":"# Plot the bar graph the number of training images for each class\n\ntrain_df[\"ClassId\"].value_counts().plot(kind = 'barh')","24994fdc":"# Obtain the number of training images for each class\n\nclass_3, class_1, class_4, class_2 = train_df['ClassId'].value_counts()\nprint(\"Number of Class 1 defects: \", class_1)\nprint(\"Number of Class 2 defects: \", class_2)\nprint(\"Number of Class 3 defects: \", class_3)\nprint(\"Number of Class 4 defects: \", class_4)","05f89453":"# Function for getting the training ImageId in array form\n\ndef get_ImageArray():\n    \n    # Looping counter\n    image_index = 0\n    \n    # Obtain the total number of images in training set\n    total_image_rows = len(train_df['ImageId'])\n    \n    # Create an empty array named labels for inserting the ImageId\n    images = []\n    \n    # Append each image into the images array and iterate until the total number of training images\n    for image_index in range(total_image_rows):\n        img_arr = cv2.imread(train_dir + '\/' + train_df['ImageId'][image_index])\n        img_arr = cv2.resize(img_arr, (120,120))\n        images.append(img_arr)\n        image_index += 1\n        \n    # Return the labels as the function output\n    return images","6cfbefb2":"# Assigning the output of get_ImageArray() function to variable images\n\nimages = get_ImageArray()","863ef813":"# Function for getting the ClassId in array form\n\ndef get_ClassId():\n    \n    # Looping counter\n    class_index = 0\n    \n    # Obtain the total number of images in training set\n    total_class_rows = len(train_df['ClassId'])\n    \n    # Create an empty array named labels for inserting the ClassId\n    class_labels = []\n    \n    # Append each label into the labels array and iterate until the total number of training images\n    for class_index in range(total_class_rows):\n        class_labels.append(train_df['ClassId'][class_index])\n        class_index += 1\n        \n    # Return the labels as the function output\n    return class_labels","ccd59881":"# Assigning the output of get_ClassId() function to variable class_labels\n\nclass_labels = get_ClassId()","1a90b4bc":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n\nencoder = LabelEncoder()\n\nX = np.array(images)\nX = X\/255\n\nY = encoder.fit_transform(class_labels)\nY = to_categorical(Y)","b72a3bc8":"# Divide the training images into training set and test set\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.15, shuffle = True)","1df26481":"# Obtain the splitting results for the training set and test set\n\nprint(\"x_train shape:\", X_train.shape)\nprint(\"x_valid shape:\", X_valid.shape)\nprint(\"y_train shape:\", Y_train.shape)\nprint(\"y_valid shape:\", Y_valid.shape)","ae75a35e":"# https:\/\/colab.research.google.com\/github\/google\/automl\/blob\/master\/efficientnetv2\/tfhub.ipynb#scrollTo=9f3yBUvkd_VJ\n\nimport itertools\nimport os\n\nimport matplotlib.pylab as plt\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nprint('TF version:', tf.__version__)\nprint('Hub version:', hub.__version__)\nprint('Physical devices:', tf.config.list_physical_devices())\n\ndef get_hub_url_and_isize(model_name, ckpt_type, hub_type):\n  if ckpt_type == '1k':\n    ckpt_type = ''  # json doesn't support empty string\n  else:\n    ckpt_type = '-' + ckpt_type  # add '-' as prefix\n  \n  hub_url_map = {\n    'efficientnetv2-b0': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b0\/{hub_type}',\n    'efficientnetv2-b1': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b1\/{hub_type}',\n    'efficientnetv2-b2': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b2\/{hub_type}',\n    'efficientnetv2-b3': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b3\/{hub_type}',\n    'efficientnetv2-s':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-s\/{hub_type}',\n    'efficientnetv2-m':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-m\/{hub_type}',\n    'efficientnetv2-l':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-l\/{hub_type}',\n\n    'efficientnetv2-b0-21k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b0-21k\/{hub_type}',\n    'efficientnetv2-b1-21k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b1-21k\/{hub_type}',\n    'efficientnetv2-b2-21k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b2-21k\/{hub_type}',\n    'efficientnetv2-b3-21k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b3-21k\/{hub_type}',\n    'efficientnetv2-s-21k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-s-21k\/{hub_type}',\n    'efficientnetv2-m-21k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-m-21k\/{hub_type}',\n    'efficientnetv2-l-21k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-l-21k\/{hub_type}',\n    'efficientnetv2-xl-21k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-xl-21k\/{hub_type}',\n\n    'efficientnetv2-b0-21k-ft1k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b0-21k-ft1k\/{hub_type}',\n    'efficientnetv2-b1-21k-ft1k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b1-21k-ft1k\/{hub_type}',\n    'efficientnetv2-b2-21k-ft1k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b2-21k-ft1k\/{hub_type}',\n    'efficientnetv2-b3-21k-ft1k': f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b3-21k-ft1k\/{hub_type}',\n    'efficientnetv2-s-21k-ft1k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-s-21k-ft1k\/{hub_type}',\n    'efficientnetv2-m-21k-ft1k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-m-21k-ft1k\/{hub_type}',\n    'efficientnetv2-l-21k-ft1k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-l-21k-ft1k\/{hub_type}',\n    'efficientnetv2-xl-21k-ft1k':  f'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-xl-21k-ft1k\/{hub_type}',\n      \n    # efficientnetv1\n    'efficientnet_b0': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/{hub_type}\/1',\n    'efficientnet_b1': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b1\/{hub_type}\/1',\n    'efficientnet_b2': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b2\/{hub_type}\/1',\n    'efficientnet_b3': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b3\/{hub_type}\/1',\n    'efficientnet_b4': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b4\/{hub_type}\/1',\n    'efficientnet_b5': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b5\/{hub_type}\/1',\n    'efficientnet_b6': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b6\/{hub_type}\/1',\n    'efficientnet_b7': f'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/{hub_type}\/1',\n  }\n  \n  image_size_map = {\n    'efficientnetv2-b0': 224,\n    'efficientnetv2-b1': 240,\n    'efficientnetv2-b2': 260,\n    'efficientnetv2-b3': 300,\n    'efficientnetv2-s':  384,\n    'efficientnetv2-m':  480,\n    'efficientnetv2-l':  480,\n    'efficientnetv2-xl':  512,\n  \n    'efficientnet_b0': 224,\n    'efficientnet_b1': 240,\n    'efficientnet_b2': 260,\n    'efficientnet_b3': 300,\n    'efficientnet_b4': 380,\n    'efficientnet_b5': 456,\n    'efficientnet_b6': 528,\n    'efficientnet_b7': 600,\n  }\n  \n  hub_url = hub_url_map.get(model_name + ckpt_type)\n  image_size = image_size_map.get(model_name, 224)\n  return hub_url, image_size\n\n\ndef get_imagenet_labels(filename):\n  labels = []\n  with open(filename, 'r') as f:\n    for line in f:\n      labels.append(line.split('\\t')[1][:-1])  # split and remove line break.\n  return labels","44b91a0f":"hub_url = 'gs:\/\/cloud-tpu-checkpoints\/efficientnet\/v2\/hub\/efficientnetv2-b0\/feature-vector'\ndo_fine_tuning = True\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape = [224, 224, 3]),\n    hub.KerasLayer(hub_url, trainable = do_fine_tuning),\n    tf.keras.layers.Dropout(rate = 0.2),\n    tf.keras.layers.Dense(4, activation = 'softmax'),\n])","c7ec2100":"model.summary()","49f0fedc":"model.compile(\n  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n  metrics=['accuracy'])","853bacd9":"steps_per_epoch = 189\nvalidation_steps = 34","ec801161":"num_epochs = 25 #@param {type:\"integer\"}","15fafadc":"history = model.fit(X_train, Y_train, epochs = num_epochs, steps_per_epoch = steps_per_epoch, validation_data = (X_valid, Y_valid), validation_steps = validation_steps, batch_size = 32, verbose = 1).history","bde23185":"# Plot the graphs for the metrics of interest\n\nacc = history['accuracy']\nval_acc = history['val_accuracy']\nloss = history['loss']\nval_loss = history['val_loss']\n\nepochs = range(1,len(acc) + 1)\n\nplt.plot(epochs, acc,'r',label = 'Training Accuracy')\nplt.plot(epochs,val_acc,'b',label = 'Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs,loss,'r',label = 'Training loss')\nplt.plot(epochs,val_loss,'b',label = 'Validation Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\n\nplt.show()","f30a6ca1":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model\n# print(model.metrics_names)\n# compute predictions\n\nresults = model.evaluate(X_valid, Y_valid)","763304bf":"# Function for getting the testing ImageId in array form\n\ndef get_testImageArray():\n    \n    # Looping counter\n    image_index = 0\n    \n    # Obtain the total number of images in training set\n    total_image_rows = len(submission_df['ImageId'])\n    \n    # Create an empty array named labels for inserting the ImageId\n    test_images = []\n    \n    # Append each image into the images array and iterate until the total number of training images\n    for image_index in range(total_image_rows):\n        testimg_arr = cv2.imread(test_dir + '\/' + submission_df['ImageId'][image_index])\n        testimg_arr = cv2.resize(testimg_arr, (120,120))\n        test_images.append(testimg_arr)\n        image_index += 1\n        \n    # Return the labels as the function output\n    return test_images","d08f6af4":"X_test = get_testImageArray()","c815de2f":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n\nencoder = LabelEncoder()\n\nX_test = np.array(X_test)\nX_test = X_test\/255","8ca75bbe":"pred = model.predict(X_test)\npredictions = np.argmax(pred, axis = 1)","acd289aa":"predictions","b1d72031":"classes = ['1','2','3','4']\nclasses = np.array(classes)\npred_name = classes[predictions]\nprint(pred_name)","43159c7d":"submission_df['ClassId'] = pred_name","cb2e74c9":"submission_df.to_csv('.\/submission.csv', index = False)\nsubmission_df","bf1952ea":"# Display dataframe for testing images\n\npd.set_option('display.max_rows', None)\nsubmission_df.head(len(submission_df))","8f1caa85":"def mask_to_rle(image):\n    # The input image is expressed as a numpy array\n    # Masked pixels are expressed as value '1' where the background pixels will be set as '0'\n    # Returns run length as string formatted\n    \n    pixels = image.flatten() # Decoded pixels after flattening the masked image\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1 # This record is the position where the bit value starts to change, here +1 is for position adjustment\n\n    runs[1::2] -= runs[::2] # runs[1::2] = runs[1::2] - runs[::2]\n    # For example, the result before running is runs = [1 3 13 15] \n    # Each number in runs represents the position where the pixel value changes\n    # The result after running is runs = [ 1  2 13  2]\n    # Means that starting from the first position, a total of 2 bits are the same, so use 3 - 1 to get 2.\n    # Means that starting from the 13th position, a total of 2 bits are the same, so use 15 - 13 to get 2.\n    # Correspond to the two 11 at the head and the end above\n    \n    # Python sequence slice addresses can be written as a[start:end:step] and any of start, stop or end can be dropped. \n    # For example, a[::3] is every third element of the sequence.\n    return ' '.join(str(x) for x in runs)\n \ndef rle_to_mask(mask_rle, shape = (256, 1600)):\n    # The mask_rle is the run-length as string formated (start length)\n    # image.shape is the dimension of the image in array form\n    # Returns a masked image expressed in numpy array, with masked pixels as '1' and background pixels as '0'\n\n    s = mask_rle.split()\n    # starts is the list of the position numbers of the changed pixels\n    starts, lengths = [np.asarray(x, dtype = int) for x in (s[0:][::2], s[1:][::2])]  \n    # s[0:][::2] - This is a list of position numbers of changed pixels, for example, we have encoded pixels as [1, 13]\n    # s[1:][::2] - This is a list of the length of the same pixel (recording the consecutive equivalent pixels behind each changed pixel separately Continuous length of value)\n    \n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype = np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)","f73eb99b":"EncodedPixels = train_df['EncodedPixels'][22]\nEncodedPixels","b906195a":"print(len(EncodedPixels.split()))\nEncodedPixels.split()","9bddddad":"rle = list(map(int, EncodedPixels.split()))\nrle","c70c5b8c":"pixel,pixel_count = [],[]\n[pixel.append(rle[i]) if i % 2 == 0 else pixel_count.append(rle[i]) for i in range(0, len(rle))]\nprint('Pixel starting points:\\n',pixel)\nprint('Pixel counting:\\n', pixel_count)","b41c35e0":"rle_pixels = [list(range(pixel[i],pixel[i] + pixel_count[i])) for i in range(0, len(pixel))]\nprint('rle_pixels\\n:', rle_pixels)","9edfa6f3":"rle_mask_pixels = sum(rle_pixels,[]) \nprint('rle mask pixels:\\n', rle_mask_pixels)","b26c7a71":"# Read a sample training image with its information displayed\n\ndisplay(train_df.iloc[22])\nimage = imread(train_dir + '\/' + train_df['ImageId'][22])\nprint(image.shape)\n\nplt.figure(figsize=None) #The size of original image is (1600, 256)\nplt.imshow(image)\nplt.show()","99b526e9":"mask_img = np.zeros((256*1600,1), dtype = int)","a9be1a42":"mask_img[rle_mask_pixels] = 255","403ca390":"image = train_dir + '\/' + train_df['ImageId'][22]\n\nl,b = cv2.imread(image).shape[0], cv2.imread(image).shape[1]\nmask = np.reshape(mask_img, (b, l)).T","cd7ad281":"plt.imshow(mask)","82190ca7":"mask_to_rle(rle_to_mask(train_df['EncodedPixels'].iloc[0])) == train_df['EncodedPixels'].iloc[0]","feef887f":"mask_to_rle(rle_to_mask('1 1')) == '1 1'","db70a618":"rle_to_mask(train_df['EncodedPixels'].iloc[1111])","3db90d43":"mask_to_rle(rle_to_mask(train_df['EncodedPixels'].iloc[1111]))","45c93e96":"# References","8fe3656a":"**2. Train-test Split**","4a66001b":"# Severstal Steel Dataset","3dbcb839":"**1. Data Visualisation**","285c0d62":"1. Main Reference Link 1: https:\/\/medium.com\/analytics-vidhya\/severstal-steel-defect-detection-2d2e836855c2\n2. Main Reference link 2: https:\/\/medium.com\/@saivenkat_\/a-detailed-case-study-on-severstal-steel-defect-detection-can-we-detect-and-classify-defects-in-2844402392cc\n3. Main Reference Link 3: https:\/\/medium.com\/@guildbilla\/steel-defect-detection-image-segmentation-using-keras-dae8b4f986f0\n4. Main Reference Link 4: https:\/\/www.kaggle.com\/kenmatsu4\/visualize-steel-defect\n5. Main Reference Link 5: https:\/\/www.kaggle.com\/yasserhessein\/severstal-steel-defect-detection-with-xception\n6. Main Reference Link 6: http:\/\/faculty.neu.edu.cn\/me\/songkc\/Research.html\n7. Main Reference Link 7: https:\/\/zhuanlan.zhihu.com\/p\/92547750\n8. Secondary Reference Link 1: https:\/\/github.com\/nikhilroxtomar\/Multiclass-Segmentation-in-Unet\n9. Secondary Reference Link 2 (for multiclass classification): https:\/\/www.kaggle.com\/lsind18\/gemstones-multiclass-classification-cnn , https:\/\/www.kaggle.com\/yemregundogmus\/car-forecast-with-multi-class-classification\n10. Supportive Reference Link 1: https:\/\/www.kaggle.com\/arjunrao2000\/beginners-guide-efficientnet-with-keras\/notebook\n11. Supportive Reference Link 2: https:\/\/colab.research.google.com\/drive\/1vzEDAX-3ol7gcZ7qmKuwn8zUld524sUZ#scrollTo=e4wFQJkBiFWE https:\/\/blog.roboflow.com\/how-to-train-efficientnet\/\n12. Supportive Reference Link 3: https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114\n13. Supportive Reference Link 4: https:\/\/towardsdatascience.com\/building-convolutional-neural-networks-in-python-using-keras-7e4652f6456f\n\n**Image Segmentation** <br>\n1. Reference Link 1: https:\/\/medium.com\/analytics-vidhya\/generating-masks-from-encoded-pixels-semantic-segmentation-18635e834ad0 <br> \n2. Reference Link 2: https:\/\/www.kaggle.com\/paulorzp\/run-length-encode-and-decode <br>\n3. Reference Link 3: https:\/\/www.programmersought.com\/article\/51974806761\/ <br>\n4. Reference Link 4: https:\/\/developer.nvidia.com\/blog\/building-image-segmentation-faster-using-jupyter-notebooks-from-ngc\/","d78ec818":"**3. Methodology 1 - EffcientNetV2**","09595f83":"**4. Run-length Encoding**"}}