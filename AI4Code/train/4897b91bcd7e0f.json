{"cell_type":{"6782e894":"code","3780f651":"code","763675aa":"code","db190f9e":"code","08337e12":"code","2f39b7ed":"code","5b979da5":"code","5f1a00ac":"code","15a2f746":"code","b68ab1ec":"code","7ff3792c":"code","d8349f7f":"code","ab397395":"code","60169390":"code","2afeed8d":"markdown","e31ed36b":"markdown","bd4e64ba":"markdown","3bb024ba":"markdown","4361e74b":"markdown","c8560844":"markdown","9990d348":"markdown"},"source":{"6782e894":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","3780f651":"df = pd.read_csv('..\/input\/bookcrossing-dataset\/Books Data with Category Language and Summary\/Preprocessed_data.csv')\ndf","763675aa":"df.columns","db190f9e":"df.info()","08337e12":"#Dropping records includes NaN values\ndf.dropna(inplace=True)","2f39b7ed":"#Managing NaN values of dataset\ndf.isna().sum()","5b979da5":"df_ir = df.loc[(df[\"country\"] == \"iran\"), ['user_id','isbn','rating']]\ndf_ir","5f1a00ac":"user_ids = df_ir[\"user_id\"].unique().tolist()\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuserencoded2user = {i: x for i, x in enumerate(user_ids)}\nbook_ids = df_ir[\"isbn\"].unique().tolist()\nbook2book_encoded = {x: i for i, x in enumerate(book_ids)}\nbook_encoded2book = {i: x for i, x in enumerate(book_ids)}\ndf_ir[\"user\"] = df_ir[\"user_id\"].map(user2user_encoded)\ndf_ir[\"book\"] = df_ir[\"isbn\"].map(book2book_encoded)\n\nnum_users = len(user2user_encoded)\nnum_books = len(book_encoded2book)\ndf_ir[\"rating\"] = df_ir[\"rating\"].values.astype(np.float32)\n\n# min and max ratings will be used to normalize the ratings\nmin_rating = min(df_ir[\"rating\"])\nmax_rating = max(df_ir[\"rating\"])\n\nprint(\"Number of users: {}, Number of Books: {}, Min rating: {}, Max rating: {}\".format(num_users, num_books, min_rating, max_rating))\n","15a2f746":"df_ir= df_ir.sample(frac=1, random_state=42)\nx = df_ir[[\"user\", \"book\"]].values\n\n# Normalize the targets between 0 and 1 (it's easier to train)\ny = df_ir[\"rating\"].apply(lambda x: (x - min_rating) \/ (max_rating - min_rating)).values\n\n# training and validating on 80%\/20%.\nx_train = df_ir[['user_id', 'isbn']].values\ny = df_ir['rating'].values\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\nx_train.shape, x_val.shape, y_train.shape, y_val.shape","b68ab1ec":"class RecommenderNet(keras.Model):\n    def __init__(self, num_users, num_books, embedding_size, **kwargs):\n        super(RecommenderNet, self).__init__(**kwargs)\n        self.num_users = num_users\n        self.num_books = num_books\n        self.embedding_size = embedding_size\n        self.user_embedding = layers.Embedding(\n            num_users,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.user_bias = layers.Embedding(num_users, 1)\n        self.book_embedding = layers.Embedding(\n            num_books,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.book_bias = layers.Embedding(num_books, 1)\n\n    def call(self, inputs):\n        user_vector = self.user_embedding(inputs[:, 0])\n        user_bias = self.user_bias(inputs[:, 0])\n        book_vector = self.book_embedding(inputs[:, 1])\n        book_bias = self.book_bias(inputs[:, 1])\n        dot_user_book = tf.tensordot(user_vector, book_vector, 2)\n        # Add all the components (including bias)\n        x = dot_user_book + user_bias + book_bias\n        # The sigmoid activation forces the rating to between 0 and 1\n        return tf.nn.sigmoid(x)\n\nEMBEDDING_SIZE = 50\n    \nmodel = RecommenderNet(num_users, num_books, EMBEDDING_SIZE)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.000002))","7ff3792c":"history = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=64,\n    epochs=5,\n    verbose=1,\n    validation_data=(x_val, y_val),\n)","d8349f7f":"book_df = df.loc[(df[\"country\"] == \"iran\") & (df[\"Category\"] != \"9\"), ['isbn','book_title','Category']]\nbook_df","ab397395":"# Top recommendations for a random user\n\nuser_id = df_ir.user_id.sample(3).iloc[0]\nbooks_read_by_user = df_ir[df_ir.user_id == user_id]\nbooks_not_read = book_df[~book_df[\"isbn\"].isin(books_read_by_user.isbn.values)][\"isbn\"]\nbooks_not_read = list(set(books_not_read).intersection(set(book2book_encoded.keys())))\n\nbooks_not_read = [[book2book_encoded.get(x)] for x in books_not_read]\n\nuser_encoder = user2user_encoded.get(user_id)\n\nuser_book_array = np.hstack(([[user_encoder]] * len(books_not_read), books_not_read))\\\n\nratings = model.predict(user_book_array).flatten()\n\ntop_ratings_indices = ratings.argsort()[-10:][::-1]\n\nrecommended_book_ids = [book_encoded2book.get(books_not_read[x][0]) for x in top_ratings_indices]\n\nprint(\"Showing recommendations for user: {}\".format(user_id))\nprint(\"====\" * 9)\nprint(\"Books with high ratings from user\")\nprint(\"----\" * 8)\ntop_books_user = ( books_read_by_user.sort_values(by=\"rating\", ascending=False).head(5).isbn.values)\nbook_df_rows = book_df[book_df[\"isbn\"].isin(top_books_user)]\nfor row in book_df_rows.itertuples():\n    print(row.book_title, \":\", row.Category)\n\nprint(\"----\" * 8)\nprint(\"Top 10 Books recommendations\")\nprint(\"----\" * 8)\nrecommended_books = book_df[book_df[\"isbn\"].isin(recommended_book_ids)]\nfor row in recommended_books.itertuples():\n    print(row.book_title, \":\", row.Category)","60169390":"#accuracy of this Recommendation System\nprint('Accuracy: %.2f' % (model.evaluate(x,y)*100))","2afeed8d":"* **Importing Libraries and defining dataframe**","e31ed36b":"the following recommendation system was developed for a subset of this dataset,the users who voted in Iran.It can be expanded to another region or whole of the dataset.The only reason for this approach was to reducing the time of processing.","bd4e64ba":"*  **Preprocessing**","3bb024ba":"* **Recommendation System**","4361e74b":"# **Recommendation System using Deep learning (Collaborative Based)**","c8560844":"The Dataset used in this notebook contains 278,858 users voted 271,379 books in total 1,149,780 ratings.It has user features like user id,age and their region. The features for each book are isbn, year of publication, publisher, its cover image and so on.\n\nKeras library in Python was used for designing neural network to recommend books to users based on similarities with others.","9990d348":"Eventhough the csv file that I used was preprocessed, I checked null values and dataframe info."}}