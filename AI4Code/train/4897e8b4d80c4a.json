{"cell_type":{"c62bd187":"code","42772968":"code","059bed7c":"code","d726e019":"code","0eb9fe04":"code","2c504c06":"code","caa5d682":"code","bb3b5f4d":"code","9ee25d17":"code","38332680":"code","1049dd9f":"code","57ce9d0e":"code","731af3b3":"code","2b441cd1":"code","e92dbf7b":"code","49352fb9":"code","5c6ac05c":"code","e7557096":"code","1849c765":"code","d602f17e":"code","059c13f9":"code","a20b3563":"code","d2c1e86e":"code","737be996":"markdown","147fb56d":"markdown","871c22cf":"markdown","8a63da56":"markdown","3e7f2262":"markdown","e5861145":"markdown","d1e59e6c":"markdown","f3ac2b24":"markdown","d458ad8c":"markdown","c4c1ed36":"markdown","e538b821":"markdown","92ee0bd1":"markdown"},"source":{"c62bd187":"import numpy as np\nimport pandas as pd\nimport os\nfrom numba import jit\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import neighbors\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import NearestNeighbors\nimport librosa, librosa.display\nimport builtins\nfrom sklearn.ensemble import RandomForestRegressor\nimport eli5\nimport shap\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn import metrics\nfrom IPython.display import HTML\nimport json\nimport altair as alt\nfrom collections import Counter","42772968":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","059bed7c":"train.shape, test.shape","d726e019":"train.head()","0eb9fe04":"plt.hist(train[train.columns[10]]);\nplt.title(f'Distribution of {train.columns[10]}');","2c504c06":"plt.hist(train.mean(1));\nplt.title('Distribution of mean values of train columns');","caa5d682":"plt.hist(train.std(1));\nplt.title('Distribution of standard deviations of train columns');","bb3b5f4d":"train['target'].value_counts()","9ee25d17":"X = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\ny = train['target']\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","38332680":"@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https:\/\/www.kaggle.com\/c\/microsoft-malware-prediction\/discussion\/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc \/= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), len(set(y.values))))\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test), oof.shape[1]))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] \/= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    ","1049dd9f":"params = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.1,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 5,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 0.8\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb',\n                                                                                  eval_metric='auc', plot_feature_importance=True, verbose=50, n_estimators=200)","57ce9d0e":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['target'] = result_dict_lgb['prediction'][:, 1]\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","731af3b3":"train['wheezy-copper-turtle-magic'].nunique(), test['wheezy-copper-turtle-magic'].nunique()","2b441cd1":"sorted(train['wheezy-copper-turtle-magic'].unique()) == sorted(test['wheezy-copper-turtle-magic'].unique())","e92dbf7b":"plt.hist(train.loc[train['target'] == 0, 'wheezy-copper-turtle-magic'], color='r', bins=512, label='0');\nplt.hist(train.loc[train['target'] == 1, 'wheezy-copper-turtle-magic'], color='g', bins=512, label='1');\nplt.hist(test['wheezy-copper-turtle-magic'], color='b', bins=512, label='1');\nplt.title('Distribution of wheezy-copper-turtle-magic');\nplt.legend()","49352fb9":"col_part_names = [col.split('-') for col in train.columns]\ncol_part_names = [i for j in col_part_names for i in j]\nCounter(col_part_names).most_common(10)","5c6ac05c":"some_cols = [i[0] for i in Counter(col_part_names).most_common() if i[1] > 4]","e7557096":"test['target'] = -1\nlen_train = train.shape[0]\nscaler = StandardScaler()\nall_data = pd.concat([train, test], axis=0, sort=False, ignore_index=True).reset_index(drop=True)\nfor c in some_cols:\n    more_such_cols = [col for col in all_data.columns if c in col]\n    all_data[f'{c}_mean'] = all_data[more_such_cols].mean(1)\n    all_data[f'{c}_min'] = all_data[more_such_cols].min(1)\n    all_data[f'{c}_max'] = all_data[more_such_cols].max(1)\n    all_data[f'{c}_std'] = all_data[more_such_cols].std(1)\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\nall_data[cols] = scaler.fit_transform(all_data[cols])\n#frequencies\nall_data['wheezy-copper-turtle-magic_count'] = all_data.groupby(['wheezy-copper-turtle-magic'])['id'].transform('count')","1849c765":"train = all_data[:len_train].reset_index(drop=True)\ntest = all_data[len_train:].reset_index(drop=True)","d602f17e":"train = pd.concat([train, pd.get_dummies(train['wheezy-copper-turtle-magic'], prefix='wctm')], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['wheezy-copper-turtle-magic'], prefix='wctm')], axis=1)","059c13f9":"X = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id', 'target'], axis=1)\ny = train['target']\nn_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","a20b3563":"params = {'num_leaves': 1024,\n          'min_child_samples': 10,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.1,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 5,\n          \"subsample\": 1.0,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 1.0,\n          'min_sum_hessian_in_leaf': 10,\n          'num_threads': -1\n         }\n\n\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb',\n                                                                                  eval_metric='auc', plot_feature_importance=True, verbose=500, n_estimators=10000)","d2c1e86e":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub['target'] = result_dict_lgb['prediction'][:, 1]\nsub.to_csv(\"submission_1.csv\", index=False)\nsub.head()","737be996":"I'm going to take some of these columns and calculate statistics based on them.","147fb56d":"Loading libraries and preparing functions","871c22cf":"It seems that this is a categorical feature! And values in train and test are the same! Let's create some features based on it!\n\nAnother idea is scaling features, it seems that it works quite well.\n\nAlso let's try looking at column names. They are quite interesting - they contain several words separated by \"-\". Let's have a look.","8a63da56":"# General information\nIn this kernel I work with Instant Gratification challenge. This is a binary classification problem with immediate \"Stage 2\".\n\nI'll do some EDA and basic modelling and then use feature engineering to improve the model.","3e7f2262":"## Basic model","e5861145":"## Data overview","d1e59e6c":"It seems that the data was normalized.","f3ac2b24":"We have a balanced dataset!","d458ad8c":"We have more than 200 columns which seem to be anonymized. Let's have a quick look at them.","c4c1ed36":"The column seem to have a normal distribution. This reminds me of the recent Santander competition...","e538b821":"Training function:","92ee0bd1":"## Feature engineering\n\nWe can see that there is one feature which has much higher importance that other features: `wheezy-copper-turtle-magic`."}}