{"cell_type":{"c15354ca":"code","02f6efca":"code","c6efd01d":"code","e10fd0f4":"code","af63720b":"code","5517add6":"code","afa5d7cd":"code","0474ce83":"code","95469735":"code","f0fa4c93":"code","ac15d009":"code","9de3128c":"code","80d1b289":"code","daf6a692":"code","0ac9af8f":"code","86b267da":"code","a7d16c08":"code","6221f790":"code","7242de7a":"code","339b9a67":"code","2004bf00":"code","c4133485":"code","49d5abde":"code","ac275f5f":"code","1d2ec9e3":"code","c23a055e":"code","ea6f6765":"code","3e91b7bc":"code","4cb8b52f":"code","109d33aa":"code","84a4f13b":"code","0a0efcef":"code","6d0610d1":"code","407f7a47":"code","cc34bc9a":"code","5a01db4f":"code","f426265b":"code","1a4907a1":"code","9e208010":"code","48279e51":"code","293eae72":"code","9cc95900":"code","c9de11b6":"code","28d1be89":"code","da37a724":"code","ae94452b":"code","782f0da8":"code","d71dad9c":"code","eb831846":"code","5fbf6f29":"code","5537356b":"code","45558ea7":"code","fc3b98d7":"code","f56cd9f0":"code","566d0b55":"code","2408effd":"code","3d60c953":"code","f0b07e09":"code","97633213":"code","46a9697a":"code","b917b779":"code","7991238b":"code","cfa7d64c":"code","929e9b3a":"code","2c931de3":"code","1e63274e":"code","e9bc8547":"code","cc80e1ee":"code","3361340c":"code","5774cfdf":"code","f85cd3ac":"code","2cfbc631":"code","75524425":"markdown","92de4d5e":"markdown","6d791489":"markdown","cb08e62d":"markdown","ddb1a785":"markdown","fa265558":"markdown","804c28e7":"markdown","e5f839ac":"markdown","625ab2df":"markdown","e41d15b2":"markdown","ecb701a7":"markdown","9238a1b8":"markdown","edce9b7c":"markdown","6c2a60e0":"markdown","1ddc87c5":"markdown","1619d992":"markdown","f47af19e":"markdown","e358876e":"markdown","3c1d3b7f":"markdown","96fb0171":"markdown","d5253369":"markdown","ab443d62":"markdown","cac38a9a":"markdown","adfb1e84":"markdown","42f021e7":"markdown","78ac3ce9":"markdown","5759b1d8":"markdown","6c6553a2":"markdown","e582672a":"markdown","143d50cc":"markdown","2e8fe5de":"markdown","1f74f94a":"markdown","2f229c0e":"markdown","ffc675d2":"markdown","f4be0c05":"markdown","e55e8220":"markdown","9e7b1eea":"markdown","e5b57166":"markdown","969f0b3d":"markdown","cf7dc449":"markdown","d5b3e4d2":"markdown","3a0ca3fc":"markdown","10c07f52":"markdown"},"source":{"c15354ca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import (accuracy_score, f1_score, roc_curve, auc, precision_recall_curve, \n                             classification_report, confusion_matrix, roc_auc_score)\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier, early_stopping\n\n%matplotlib inline","02f6efca":"df = pd.read_csv('..\/input\/income-classification\/income_evaluation.csv', engine='python')\ndf.head()","c6efd01d":"df['ID'] = (df.index)\ndf.head()","e10fd0f4":"df.info()","af63720b":"df.columns","5517add6":"columns = list(df.columns)\nfor i in range(len(columns)):\n    col = columns[i]\n    col = col.replace(' ', '')\n    columns[i] = col\n    \ndf.columns = columns\nprint(df.columns)","afa5d7cd":"for col in df.columns:\n    print(df[col].unique())","0474ce83":"objects = df.select_dtypes(include='object')\nfor col in objects:\n    df[col] = objects[col].map(lambda x: x[1:])\n    print(df[col].unique())","95469735":"df.info()","f0fa4c93":"def edu(x):\n    x['num_status'] = (x['education'], x['education-num'])\n    \n    return x\n\ncopy = df.copy().apply(lambda x: edu(x), axis=1)\nprint(np.sort(copy['num_status'].unique()))","ac15d009":"df.drop(['fnlwgt', 'education'], axis=1, inplace=True)","9de3128c":"objects = df.select_dtypes(include='object')\nfor col in objects:\n    le = LabelEncoder()\n    df[col] = le.fit_transform(df[col])","80d1b289":"df.head()","daf6a692":"df[['capital-gain', 'capital-loss', 'hours-per-week']].describe()","0ac9af8f":"df['net_gain'] = df['capital-gain'] - df['capital-loss']","86b267da":"df.drop(['capital-gain', 'capital-loss'], axis=1, inplace=True)","a7d16c08":"df.hist(column=['net_gain', 'hours-per-week'], figsize=(10, 4))","6221f790":"df.value_counts(subset=['hours-per-week'], sort=False)","7242de7a":"df.value_counts(subset=['net_gain'], sort=False)","339b9a67":"income_drops = df[df['net_gain'] == df['net_gain'].max()].index\nhour_drops = df[df['hours-per-week'] == df['hours-per-week'].max()].index\n\ndf.drop(income_drops, axis=0, inplace=True)\ndf.drop(hour_drops, axis=0, inplace=True)\n\ndf.head()","2004bf00":"df.shape","c4133485":"X = df.drop(['income'], axis=1)\ny = df[['ID', 'income']]","49d5abde":"y['income'].value_counts()","ac275f5f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y['income'], random_state=42)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","1d2ec9e3":"train_id = y_train['ID']\ntest_id = y_test['ID']\n\ny_train = y_train['income']\ny_test = y_test['income']\nX_train.drop('ID', axis=1, inplace=True)\nX_test.drop('ID', axis=1, inplace=True)\n\ny_test.head()","c23a055e":"X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1748, stratify=y_train, random_state=42)\n\nprint(X_tr.shape, X_val.shape, y_tr.shape, y_val.shape)","ea6f6765":"ada = AdaBoostClassifier(random_state=42)\ngbc = GradientBoostingClassifier(random_state=42)\nlgb = LGBMClassifier(random_state=42)\ncat = CatBoostClassifier(random_state=42, silent=True)\nxgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')","3e91b7bc":"models = [ada, gbc, lgb, cat, xgb]\nselection = pd.DataFrame(columns=['name', 'accuracy'])\n\nfor model in models:\n    name = model.__class__.__name__\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict(X_val)\n    accuracy = np.sum((y_val == y_pred)) \/ y_val.shape[0]\n    \n    selection = selection.append({'name': name, 'accuracy': accuracy}, ignore_index=True)\n\nselection","4cb8b52f":"xgb.fit(X_train, y_train)\n\ny_pred = xgb.predict(X_test)\ny_pred_proba = xgb.predict_proba(X_test)[:, 1]","109d33aa":"precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\nprint(precision.shape, recall.shape, thresholds.shape)","84a4f13b":"# first 5 elements\nprint(\"First 5 elements of precision:\", precision[:5])\nprint(\"First 5 elements of recall:\", recall[:5])\nprint(\"First 5 elements of threshold:\", thresholds[:5])\n\nprint(\"Last 5 elements of precision:\", precision[-5:])\nprint(\"Last 5 elements of recall:\", recall[-5:])\nprint(\"Last 4 elements of threshold:\", thresholds[-4:])","0a0efcef":"metric_df = pd.DataFrame(columns=['precision', 'recall', 'threshold'])\nmetric_df['precision'] = precision[:-1]\nmetric_df['recall'] = recall[:-1]\nmetric_df['threshold'] = thresholds\n\nmetric_df.head()","6d0610d1":"th_def = 0.5\npred_def = (y_pred_proba >= th_def).astype(np.int64)\ntp = np.sum(((y_test == 1) & (pred_def == 1)))\nfp = np.sum(((y_test == 0) & (pred_def == 1)))\ntn = np.sum(((y_test == 0) & (pred_def == 0)))\nfn = np.sum(((y_test == 1) & (pred_def == 0)))\npr_def = tp \/ (tp + fp)\nre_def = tp \/ (tp + fn)\nprint('Precision at the threshold %.4f: %.4f' % (th_def, pr_def))\nprint('Recall at the threshold %.4f: %.4f' % (th_def, re_def))\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precision[:-1], '-', color='blue', label='precision')\nplt.plot(thresholds, recall[:-1], '--', color='green', label='recall')\n\nplt.plot([0, th_def], [pr_def, pr_def], ':', color='red')\nplt.plot([th_def, th_def], [0, pr_def], ':', color='red')\nplt.plot([th_def], [pr_def], ':', marker='o', color='red')\nplt.plot([0, th_def], [re_def, re_def], ':', color='red')\nplt.plot([th_def, th_def], [0, re_def], ':', color='red')\nplt.plot([th_def], [re_def], ':', marker='o', color='red')\n\nplt.grid(True)\nplt.xlabel('Threshold')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.legend()\nplt.show()","407f7a47":"conmat = confusion_matrix(y_test, y_pred)\nprint(y_test.value_counts())\nprint(conmat)","cc34bc9a":"condf = pd.DataFrame(columns=['Actually Positive', 'Actually Negative'], index=['Predicted Positive', 'Predicted Negative'])\nfor i in range(2):\n    for j in range(2):\n        condf.iloc[i, j] = conmat.T[(1-i), (1-j)]\ncondf","5a01db4f":"# set the custom threshold level\nthreshold = 0.4\n\n# obtain array of the new predicted class\nnew_pred = (y_pred_proba >= threshold).astype(np.int64)\n\npred_df = pd.DataFrame([y_test.value_counts(), pd.Series(y_pred).value_counts(), pd.Series(new_pred).value_counts()]).T\npred_df.columns = ['True values', 'threshold 0.5', 'threshold '+str(threshold)]\npred_df","f426265b":"test_df = pd.DataFrame(test_id, columns=['ID'])\ntest_df['th05'] = y_pred\ntest_df['th04'] = new_pred\ntest_df['true'] = y_test\ntest_df.head()","1a4907a1":"tp04 = test_df[(test_df['th04'] == 1) & (test_df['true'] == 1)].shape[0]\ntn04 = test_df[(test_df['th04'] == 0) & (test_df['true'] == 0)].shape[0]\nfp04 = test_df[(test_df['th04'] == 1) & (test_df['true'] == 0)].shape[0]\nfn04 = test_df[(test_df['th04'] == 0) & (test_df['true'] == 1)].shape[0]\n\ncon04 = pd.DataFrame(columns=['Actually Positive', 'Actually Negative'], index=['Predicted Positive', 'Predicted Negative'])\ncon04.iloc[0, 0] = tp04\ncon04.iloc[0, 1] = fp04\ncon04.iloc[1, 0] = fn04\ncon04.iloc[1, 1] = tn04\n\ncon04","9e208010":"pr05 = condf.iloc[0, 0] \/ (condf.iloc[0, 0] + condf.iloc[0, 1])\nre05 = condf.iloc[0, 0] \/ (condf.iloc[0, 0] + condf.iloc[1, 0])\n\npr04 = tp04 \/ (tp04 + fp04)\nre04 = tp04 \/ (tp04 + fn04)\n\nprint('Precision at 0.5: %.4f, at 0.4: %.4f' % (pr05, pr04))\nprint('Recall at 0.5: %.4f, at 0.4: %.4f' % (re05, re04))","48279e51":"fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n\nprint(fpr.shape, tpr.shape, roc_thresholds.shape)\nprint(fpr[-5:])\nprint(tpr[-5:])\nprint(roc_thresholds[-5:])","293eae72":"print(thresholds.shape)\nprint(roc_thresholds.shape)","9cc95900":"thdf1 = pd.DataFrame(thresholds, columns=['threshold'])\nthdf1['recall'] = recall[:-1]\n\nthdf2 = pd.DataFrame(roc_thresholds, columns=['threshold'])\nthdf2['tpr'] = tpr\n\njoin = pd.merge(left=thdf1, right=thdf2, on='threshold', how='inner')","c9de11b6":"join.head(10)","28d1be89":"join.tail(10)","da37a724":"fprtpr = pd.DataFrame(columns=['threshold', 'fpr', 'tpr'])\nfprtpr['threshold'] = roc_thresholds\nfprtpr['fpr'] = fpr\nfprtpr['tpr'] = tpr","ae94452b":"default = fprtpr[fprtpr['threshold'] >= 0.5]['threshold'].min() # the threshold closest to 0.5\nfpr_def = fprtpr[fprtpr['threshold'] == default]['fpr'].iloc[0]\ntpr_def = fprtpr[fprtpr['threshold'] == default]['tpr'].iloc[0]\nprint('FPR at the threshold %.4f: %.4f' % (default, fpr_def))\nprint('TPR at the threshold %.4f: %.4f' % (default, tpr_def))\n\nplt.figure(figsize=(10, 6))\nplt.plot(roc_thresholds, fpr, '-', color='blue', label='fpr')\nplt.plot(roc_thresholds, tpr, '--', color='green', label='tpr')\n\nplt.plot([0, default], [fpr_def, fpr_def], ':', color='red')\nplt.plot([default, default], [0, fpr_def], ':', color='red')\nplt.plot([default], [fpr_def], ':', marker='o', color='red')\nplt.plot([0, default], [tpr_def, tpr_def], ':', color='red')\nplt.plot([default, default], [0, tpr_def], ':', color='red')\nplt.plot([default], [tpr_def], ':', marker='o', color='red')\n\nplt.grid(True)\nplt.xlabel('Threshold')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.legend()\nplt.show()","782f0da8":"# roc curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, '-', color='blue', label='ROC curve')\nplt.plot([0, 1], [0, 1], '--', color='black')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.grid(True)\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.legend()\nplt.show()","d71dad9c":"roc_auc1 = roc_auc_score(y_test, y_pred_proba)\nroc_auc2 = auc(fpr, tpr)\n\nprint('ROC AUC by roc_auc_score: %.4f, ROC AUC by auc: %.4f' % (roc_auc1, roc_auc2))\nprint(roc_auc1 == roc_auc2)","eb831846":"metric_df.sort_values(by='threshold', ascending=True, inplace=True)\nprint(metric_df.head())\n\nplt.figure(figsize=(10, 6))\nplt.plot(metric_df['threshold'])\nplt.grid(True)\nplt.show()","5fbf6f29":"def f1manual(p, r):\n    return (2*p*r) \/ (p + r)\n\nf1s = list()\nfor i in range(len(metric_df)):\n    p = metric_df['precision'].iloc[i]\n    r = metric_df['recall'].iloc[i]\n    f1_ = f1manual(p, r)\n    f1s.append(f1_)\nf1s = np.array(f1s)\n\nprint(precision.shape, f1s.shape)","5537356b":"metric_df['f1'] = f1s\nf1_max = metric_df['f1'].max()\nth_max = metric_df[metric_df['f1'] == f1_max]['threshold'].iloc[0]\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precision[:-1], '--', color='blue', label='precision')\nplt.plot(thresholds, recall[:-1], '--', color='green', label='recall')\nplt.plot(thresholds, f1s, '-', color='red', label='F1')\n\nplt.plot([0, th_max], [f1_max, f1_max], ':', color='black')\nplt.plot([th_max, th_max], [0, f1_max], ':', color='black')\nplt.plot([th_max], [f1_max], ':', marker='o', color='black', label='Maximum F1 score = (%.4f)' % f1_max)\n\nplt.grid(True)\nplt.xlabel('Threshold')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.legend()\nplt.show()","45558ea7":"# Let's check out the f1 score when both precision and recall are equal\nequal = metric_df[metric_df['precision'] == metric_df['recall']]\nequal","fc3b98d7":"def accmanual(y_true, y_score, threshold=0.5):\n    pred = (y_score >= threshold).astype(np.int64)\n    accuracy = np.sum(y_true == pred) \/ y_true.shape[0]\n    \n    return accuracy","f56cd9f0":"# Let's compare the F1 score and accuracy at the default threshold and the maximum-f1 threshold.\npr_max = metric_df[metric_df['threshold'] == th_max]['precision'].iloc[0]\nre_max = metric_df[metric_df['threshold'] == th_max]['recall'].iloc[0]\n\n# accuracy\nacc_def = accmanual(y_test, y_pred_proba, threshold=0.5)\nacc_max = accmanual(y_test, y_pred_proba, threshold=th_max)\n\nprint(\"Threshold %.4f - F1: %.4f, Accuracy: %.4f\" % (0.5, f1manual(pr_def, re_def), acc_def))\nprint(\"Threshold %.4f - F1: %.4f, Accuracy: %.4f\" % (th_max, f1manual(pr_max, re_max), acc_max))","566d0b55":"def hitratio(y_true, y_score):\n    pr, re, th = precision_recall_curve(y_true, y_score)\n    labels = pd.DataFrame(list(y_true), columns=['true'])\n    \n    hr = []\n    for i in range(th.shape[0]):\n        threshold = th[i]\n        new_label = (y_score >= threshold).astype(np.int64)\n        labels['predicted'] = new_label\n        \n        tp = labels[(labels['true'] == 1) & (labels['predicted'] == 1)].shape[0]\n        fp = labels[(labels['true'] == 0) & (labels['predicted'] == 1)].shape[0]\n        tn = labels[(labels['true'] == 0) & (labels['predicted'] == 0)].shape[0]\n        fn = labels[(labels['true'] == 1) & (labels['predicted'] == 0)].shape[0]\n        \n        hit = (tp + tn) \/ (tp + fp + tn + fn)\n        hr.append(hit)\n    \n    return np.array(hr, dtype=np.float64)","2408effd":"hit_ratio = hitratio(y_test, y_pred_proba)\nprint(thresholds.shape)\nprint(hit_ratio.shape)","3d60c953":"metric_df['hit'] = hit_ratio\nhit_max = metric_df['hit'].max()\nth_hit_max = metric_df[metric_df['hit'] == hit_max]['threshold'].iloc[0]\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, precision[:-1], '--', color='blue', label='precision')\nplt.plot(thresholds, recall[:-1], '--', color='green', label='recall')\nplt.plot(thresholds, hit_ratio, '-', color='red', label='Hit Ratio')\n\nplt.plot([0, th_hit_max], [hit_max, hit_max], ':', color='black')\nplt.plot([th_hit_max, th_hit_max], [0, hit_max], ':', color='black')\nplt.plot([th_hit_max], [hit_max], ':', marker='o', color='black', label='Maximum Hit ratio = (%.4f)' % hit_max)\n\nplt.grid(True)\nplt.xlabel('Threshold')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.legend()\nplt.show()","f0b07e09":"# Confusion matrix when the hit ratio achieves the maximum\nhit_pred = (y_pred_proba >= th_hit_max).astype(np.int64)\npreds = pd.DataFrame(columns=['y_pred', 'hit_pred'])\n\nhit_cm = pd.DataFrame(columns=['Actually Positive', 'Actually Negative'], index=['Predicted Positive', 'Predicted Negative'])\nhit_cm.iloc[0, 0] = np.sum((y_test == 1) & (hit_pred == 1))\nhit_cm.iloc[0, 1] = np.sum((y_test == 0) & (hit_pred == 1))\nhit_cm.iloc[1, 0] = np.sum((y_test == 1) & (hit_pred == 0))\nhit_cm.iloc[1, 1] = np.sum((y_test == 0) & (hit_pred == 0))\n\nhit_cm","97633213":"f1_max = metric_df['f1'].max()\nth_def = 0.5\nth_f1 = metric_df[metric_df['f1'] == f1_max]['threshold'].iloc[0]\nprec_max = metric_df[metric_df['threshold'] == th_f1]['precision'].iloc[0]\nrec_max = metric_df[metric_df['threshold'] == th_f1]['recall'].iloc[0]\n\npred_def = (y_pred_proba >= th_def).astype(np.int64)\ntp = np.sum(((y_test == 1) & (pred_def == 1)))\nfp = np.sum(((y_test == 0) & (pred_def == 1)))\ntn = np.sum(((y_test == 0) & (pred_def == 0)))\nfn = np.sum(((y_test == 1) & (pred_def == 0)))\npr_def = tp \/ (tp + fp)\nre_def = tp \/ (tp + fn)\nf1_def = f1manual(pr_def, re_def)\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, '-', color='blue', label='PR curve AUC = %.4f' % auc(recall, precision))\nplt.plot([0, 1], [1, 0], '--', color='black')\n\nplt.plot([0, rec_max], [prec_max, prec_max], ':', color='red')\nplt.plot([rec_max, rec_max], [0, prec_max], ':', color='red')\nplt.plot([rec_max], [prec_max], ':', marker='o', color='red', label='Maximum f1 score = (%.4f)' % f1_max)\n\nplt.plot([0, re_def], [pr_def, pr_def], ':', color='green')\nplt.plot([re_def, re_def], [0, pr_def], ':', color='green')\nplt.plot([re_def], [pr_def], ':', marker='o', color='green', label='Default f1 score = (%.4f)' % f1_def)\n\nplt.grid(True)\nplt.xlabel('recall')\nplt.ylabel('precision')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.legend()\nplt.show()","46a9697a":"models = [ada, gbc, lgb, cat, xgb]\nselection = pd.DataFrame(columns=['name', 'ROC-AUC'])\n\nfor model in models:\n    name = model.__class__.__name__\n    model.fit(X_tr, y_tr)\n    proba = model.predict_proba(X_val)[:, 1]\n    rocauc = roc_auc_score(y_val, proba)\n    \n    selection = selection.append({'name': name, 'ROC-AUC': rocauc}, ignore_index=True)\n\nselection","b917b779":"cat.get_all_params()","7991238b":"import optuna\n\ncat_features = ['workclass', 'education-num', 'marital-status', 'occupation',\n               'relationship', 'race', 'sex', 'native-country']\n\ndef objective(trial):\n    param = {\n        'eval_metric': 'Logloss',\n        'grow_policy': 'Lossguide',\n        'max_leaves': trial.suggest_int('max_leaves', 5, 50),\n        'max_depth': trial.suggest_int('max_depth', 3, 25),\n        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 5),\n        'iterations': trial.suggest_int('iterations', 800, 4000),\n        'subsample': trial.suggest_loguniform('subsample', 0.5, 1.0),\n        'random_state': 42,\n        'silent': True\n    }\n\n    model = CatBoostClassifier(**param)\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=500, cat_features=cat_features)\n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    roc_auc = roc_auc_score(y_val, y_pred_proba)\n\n    return roc_auc\n\nstudy = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=2022))\nstudy.optimize(objective, n_trials=150)\n\ncat_best = study.best_trial\ncat_best_params = cat_best.params\nprint('score: {0}, params: {1}'.format(cat_best.value, cat_best_params))","cfa7d64c":"cat_final = CatBoostClassifier(**cat_best_params, grow_policy='Lossguide', \n                               eval_metric='Logloss', random_state=42, silent=True)\ncat_final.fit(X_train, y_train, cat_features=cat_features)\n\ncat_pred_proba = cat_final.predict_proba(X_test)[:, 1]","929e9b3a":"cat_precision, cat_recall, cat_thresholds = precision_recall_curve(y_test, cat_pred_proba)\ncat_f1 = f1manual(cat_precision, cat_recall)\n\ncat_metric = pd.DataFrame(columns=['precision', 'recall', 'thresholds', 'f1'])\ncat_metric['precision'] = cat_precision[:-1]\ncat_metric['recall'] = cat_recall[:-1]\ncat_metric['thresholds'] = cat_thresholds\ncat_metric['f1'] = cat_f1[:-1]\n\ncat_metric.sort_values(by='thresholds', ascending=True, inplace=True)\nprint(cat_metric.head())\nprint(cat_metric.tail())","2c931de3":"# See how the precision, recall and f1 score vary along threshold levels\ncat_f1_max = cat_metric['f1'].max()\nth_argmax = cat_metric[cat_metric['f1'] == cat_f1_max]['thresholds'].iloc[0]\npr_argmax = cat_metric[cat_metric['f1'] == cat_f1_max]['precision'].iloc[0]\nre_argmax = cat_metric[cat_metric['f1'] == cat_f1_max]['recall'].iloc[0]\nprint('Argmax - threshold: %.4f, precision: %.4f, recall: %.4f | F1 max: %.4f' % (th_argmax, pr_argmax, re_argmax, cat_f1_max))\n\nplt.figure(figsize=(10, 6))\nplt.plot(cat_thresholds, cat_precision[:-1], '--', color='blue', label='Precision')\nplt.plot(cat_thresholds, cat_recall[:-1], '--', color='green', label='Recall')\nplt.plot(cat_thresholds, cat_f1[:-1], '-', color='red', label='F1')\n\nplt.plot([0, th_argmax], [cat_f1_max, cat_f1_max], ':', color='black')\nplt.plot([th_argmax, th_argmax], [0, cat_f1_max], ':', color='black')\nplt.plot([th_argmax], [cat_f1_max], ':', marker='o', color='black', label='Maximum F1 score = (%.4f)' % cat_f1_max)\n\nplt.xlabel('Thresholds')\nplt.grid(True)\nplt.legend()\nplt.show()","1e63274e":"# plot ROC curve and compute the ROC AUC\ncat_fpr, cat_tpr, cat_roc_thresholds = roc_curve(y_test, cat_pred_proba)\ncat_roc_auc = auc(cat_fpr, cat_tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(cat_fpr, cat_tpr, '-', color='blue', label=('ROC curve (AUC = %.4f)' % cat_roc_auc))\nplt.plot([0, 1], [0, 1], '--', color='black')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.grid(True)\nplt.legend()\nplt.show()","e9bc8547":"# plot Precision - Recall curve, and Precision - Recall along all threshold levels\ncat_pr_auc = auc(cat_recall, cat_precision)\n\nth_def = 0.5\ncat_pred_def = (cat_pred_proba >= th_def).astype(np.int64)\ncat_tp = np.sum(((y_test == 1) & (pred_def == 1)))\ncat_fp = np.sum(((y_test == 0) & (pred_def == 1)))\ncat_tn = np.sum(((y_test == 0) & (pred_def == 0)))\ncat_fn = np.sum(((y_test == 1) & (pred_def == 0)))\ncat_pr_def = cat_tp \/ (cat_tp + cat_fp)\ncat_re_def = cat_tp \/ (cat_tp + cat_fn)\ncat_f1_def = f1manual(cat_pr_def, cat_re_def)\n\nplt.figure(figsize=(8, 6))\nplt.plot(cat_recall, cat_precision, '-', color='blue', label='PR curve AUC = %.4f' % cat_pr_auc)\nplt.plot([0, 1], [1, 0], '--', color='black')\n\nplt.plot([0, re_argmax], [pr_argmax, pr_argmax], ':', color='red')\nplt.plot([re_argmax, re_argmax], [0, pr_argmax], ':', color='red')\nplt.plot([re_argmax], [pr_argmax], ':', marker='o', color='red', label='Maximum f1 score = (%.4f)' % cat_f1_max)\n\nplt.plot([0, cat_re_def], [cat_pr_def, cat_pr_def], ':', color='green')\nplt.plot([cat_re_def, cat_re_def], [0, cat_pr_def], ':', color='green')\nplt.plot([cat_re_def], [cat_pr_def], ':', marker='o', color='green', label='Default f1 score = (%.4f)' % cat_f1_def)\n\nplt.plot([0, 1], [1, 0], '--', color='black')\nplt.xlabel('recall')\nplt.ylabel('precision')\nplt.xlim([-0.005, 1.005])\nplt.ylim([-0.005, 1.005])\nplt.grid(True)\nplt.legend()\nplt.show()","cc80e1ee":"# So let's set the threshold at the argmax making the F1 maximum, and compare the confusion matrix\ndefault_pred = cat_final.predict(X_test)\nopt_pred = (cat_pred_proba >= th_argmax).astype(np.int64)\n\ndefault_tp = np.sum((y_test == 1) & (default_pred == 1))\ndefault_fp = np.sum((y_test == 0) & (default_pred == 1))\ndefault_fn = np.sum((y_test == 1) & (default_pred == 0))\ndefault_tn = np.sum((y_test == 0) & (default_pred == 0))\n\nopt_tp = np.sum((y_test == 1) & (opt_pred == 1))\nopt_fp = np.sum((y_test == 0) & (opt_pred == 1))\nopt_fn = np.sum((y_test == 1) & (opt_pred == 0))\nopt_tn = np.sum((y_test == 0) & (opt_pred == 0))\n\ndefault_cm = pd.DataFrame(columns=['Actually Positive', 'Actually Negative'], index=['Predicted Positive', 'Predicted Negative'])\ndefault_cm.iloc[0, 0] = default_tp\ndefault_cm.iloc[0, 1] = default_fp\ndefault_cm.iloc[1, 0] = default_fn\ndefault_cm.iloc[1, 1] = default_tn\n\nopt_cm = pd.DataFrame(columns=['Actually Positive', 'Actually Negative'], index=['Predicted Positive', 'Predicted Negative'])\nopt_cm.iloc[0, 0] = opt_tp\nopt_cm.iloc[0, 1] = opt_fp\nopt_cm.iloc[1, 0] = opt_fn\nopt_cm.iloc[1, 1] = opt_tn","3361340c":"# confusion matrix obtained from the default threshold\ndefault_cm","5774cfdf":"# confusion matrix after threshold adjustment\nopt_cm","f85cd3ac":"# confusion matrix from the model above we have used: LGBM\ncondf","2cfbc631":"# compare the hit ratio of three confusion matrices.\ndefault_hit = (default_cm.iloc[0, 0] + default_cm.iloc[1, 1]) \/ default_cm.sum().sum()\nopt_hit = (opt_cm.iloc[0, 0] + opt_cm.iloc[1, 1]) \/ opt_cm.sum().sum()\nlgb_hit = (condf.iloc[0, 0] + condf.iloc[1, 1]) \/ condf.sum().sum()\n\nprint('Hit ratio - CatBoost & 0.5: %.4f | CatBoost & optimized: %.4f | LGBM: %.4f | LGBM max hit ratio: %.4f' % (default_hit, opt_hit, lgb_hit, hit_max))","75524425":"# Model Selection","92de4d5e":"<ul style=\"font-size:11pt\">\n    <li>According to the ROC AUC, CatBoost scored the highest. So, I'll use it and optimize hyper-parameters using the Optuna<\/li>\n<\/ul>","6d791489":"<ul style=\"font-size:11pt\">\n    <li>From the figures above, we can see that the fpr and tpr move in the same direction as the threshold level varies. Also, we can infer that the threshold level moves from 1 to 0 on the ROC curve plot.<\/li>\n    <li>Now, what does the 45-degree line on the roc curve plot imply? On the 45-degree line, TPR is equal to FPR. Mathematically, $\\frac{TP}{TP + FN} = \\frac{FP}{TN + FP}$, and if we solve the equation, we obtain that $TP(TN + FP) = FP(TP + FN) \\Leftrightarrow TP \\cdot TN = FP \\cdot FN$. This implies that the model classifies actual labels at the 50% accuracy, thus this model is no better than an unbiased coin. This is an undesirable and terrible model.<\/li>\n    <li>What if the ROC curve goes closer to the upper-left axis? Let's assume that the ROC curve sticks to the upper and left axes. Then, FPR equals to 0 when TPR equals to 1. This means that the false negative and the false positive are both 0, and none of observations are misclassified, thus this model is the perfect model.<\/li>\n    <li>To sum up, the ROC curve has <strong>area under curve(AUC)<\/strong> ranging from 0.5 to 1, and the closer to 1 the ROC AUC is, the better the model is. Let's compute the ROC AUC of the model we've trained.<\/li>\n<\/ul>","cb08e62d":"<p style='font-size:11pt'>Well, the maximum value of capital gain, $99,999 and the 99 working hours per week seem to be anomalies. Let's check this more specifically. Before that, let's aggregate the capital gain and the capital loss, by subtracting the loss from the gain.<\/p>","ddb1a785":"## Evaluation","fa265558":"<ul style=\"font-size:11pt\">\n<li>As we can see, precision and recall move in an opposite way to each other. Also, at the default threshold 0.5, the precision value is a bit higher than the recall value. If the threshold is moved closer to 1, precision will become higher while recall will become lower.<\/li>\n\n<li>Before that, what are the 'Precision' and 'Recall' exactly? Let's talk about them and other metrics now.<\/li>\n<\/ul>","804c28e7":"## 5. Hit Ratio\n\n<ul style=\"font-size:11pt\">\n    <li>The term 'Hit' refers to the number of observations that are classified correctly. Mathematically, the hit equals to <strong>the sum of true positive and true negative<\/strong>.<\/li>\n    <li>Then the 'Hit ratio' refers to the ratio of the number of correctly classified observations divided by the total observations. Thus, maximizing the hit ratio is equivalent to minimizing the misclassifying rate, and hit ratio is also a greater-the-better metric.<\/li>\n    <li>Let's see how the hit ratio varies along the thresholds.<\/li>\n<\/ul>","e5f839ac":"<p style=\"font-size:11pt\">Each of precision and recall has one more element than the threshold array. So let's check the first and the last 5 elements of the precision and recall. For the threshold, let's check the first 5 and the last 4.<\/p>","625ab2df":"<ul style=\"font-size:11pt\">\n    <li>Let's give the ID for each observation. It will be used later.<\/li>\n<\/ul>","e41d15b2":"# Classification metric study\n\n<p style=\"font-size:11pt\">While learning ML, I got curious about metrics for performance of a ML model. I'm going to get started with the metrics used in classification. In this notebook, I'm gonna try to use metrics as many as possible, study interpretation of each of them and how to use them properly.<\/p>\n\n<p style=\"font-size:11pt\">For this project, I'll use the income evaluation dataset, whose purpose is to predict whether each worker would receive income over 50k or not, using socio-demographic features as predictors.<\/p>","ecb701a7":"<p style=\"font-size:11pt\">Now, it is certain that the education feature corresponds to the education-num column one-to-one. Thus, let's drop the education feature as well. After that, convert the string variables to integer using Label encoder.<\/p>","9238a1b8":"<ul style=\"font-size:11pt\">\n    <li>As we can see, in reality, F1 score is not maximized when the precision and recall are equal to each other. In this case, it is maximized at the neighborhood.<\/li>\n    <li> The figure above implies that we need to set an appropriate threshold other than 0.5, especially when the target value is imbalanced. Let's recall the formulas of the precision and recall.<\/li>\n    $$ Precision = \\frac{TP}{TP + FP} \\quad and \\quad Recall = \\frac{TP}{TP + FN} $$\n    <li>The difference between these two values attributes to the difference between the <strong>False positive<\/strong> and <strong>False negative<\/strong>. The precision increases as the false positive, the number of observations misclassified to be positive, decreases, and the recall increases increases as the false negative, the number of observations misclassified to be negative, decreases.<\/li>\n    <li>So, it depends on the purpose of the ML model to determine which is more important, or which misclassification is to be more avoided in other words. For example, if you are training a model to determine whether a patient is should be diagnosed as cancer(1) or not(0), it is much more critical not to misclassify those who are highly likely to have cancer(1) to the non-cancer group(0). In that case, the false negative should be minimized, thus the recall should be maximized.<\/li>\n    <li>In this problem, there are less people who earns income over 50k, and let's say we are trying to identify the high-income group to impose high income tax. The, it is more important not to misclassify the one with high income to the low-income group. Thus we should care more about the recall than about the precision here as well. Therefore, we need to focus on the false negative and set the threshold to minimize the number of the false negatives.<\/li>\n<\/ul>","edce9b7c":"<ul style=\"font-size:11pt\">\n    <li>Both precision and recall are computed from this confusion matrix.<\/li>\n    <li>Specifically, precision is computed as the ratio between the true positive and the sum of the <em>true positive<\/em> and the <em>false positive<\/em>. Mathematically, $$ Precision = \\frac{TP}{TP + FP} $$ \n        <ul style=\"font-size:11pt\">\n            <li>The sum of the true positive and false positive equals to the total number of observations predicted as positive.<\/li>\n            <li>Therefore, we can infer that the precision represents 'how correcly the observations are classified as positive', intuitively.<\/li>\n        <\/ul>\n    <\/li>\n    <li>While recall is computed as the ratio between the true positive and the sum of the <em>true positive<\/em> and the <em>false negative<\/em>. Mathematically, $$ Recall = \\frac{TP}{TP + FN} $$\n        <ul style=\"font-size:11pt\">\n            <li>The sum of the true positive and false negative equals to the total number of observations whose true labels are positive.<\/li>\n            <li>Therefore, the recall represents 'how much the correctly classified observations cover the entire positive class'.<\/li>\n            <li>Recall is also called as <strong>Sensitivity<\/strong> or <strong>True Positive Rate (TPR)<\/strong>.<\/li>\n        <\/ul>\n    <\/li>\n    <li>Besides, there are other metrics computed from a confusion matrix.        \n        <ul style=\"font-size:11pt\">\n            <li><strong>Specificity<\/strong> or <strong>True Negative Rate (TNR)<\/strong> represents how much the correctly classified observations cover the entire negative class. Mathematically, $$ \\frac{TN}{TN + FP} $$<\/li>\n            <li>Finally, <strong>False Positive Rate<\/strong> equals to (1 - specificity), or $$ \\frac{FP}{TN + FP} $$<\/li>\n        <\/ul>\n    <\/li>\n    <li>It is remarkable that the predicted class of each observation is determined by the <strong>threshold<\/strong>, which is set to 0.5 by default here. This implies that if the threshold is moved to another point, the value in every cell(TP, FP, FN, TN) of the confusion matrix also changes. For example, if the threshold is set to the lower point than 0.5, say 0.3, then more observations would be classified to be positive, thus TP and FP would both increase, while TN and FN would both decrease. This is why the precision and recall vary as the threshold moves.<\/li>\n    <li>Let's adjust the threshold level and see how the confusion matrix changes. In order to adjust the threshold level, it is required to use the 'predict_proba' method. It is also required to make the confusion matrix manually.<\/li>\n<\/ul>","6c2a60e0":"<ul style=\"font-size:11pt\">\n    <li>ROC curve, which stands for Receiver Operating Characteristic, is a curve plotted by the FPR and TPR(recall).<\/li>\n    <li>We've already discussed that the precision and recall varies as the threshold level moves, and that the precision and recall are derived from the confusion matrix. Consolidating those facts, we can infer that FPR varies as the threshold level moves as well.<\/li>\n    <li>To sum up, ROC curve plots how FPR and TPR changes over all threshold levels.<\/li>\n    <li>The ROC curve can be obtained from the roc_curve module from the Scikit-learn library.<\/li>\n<\/ul>","1ddc87c5":"<ul style=\"font-size:11pt\">\n    <li>Now, we've seen how the precision and recall are computed, and confirmed that a confusion matrix also differs as a threshold level moves. Furthermore, we can see that the precision and recall move in the opposite direction as well.<\/li>\n    <li>Next, let me introduce alternative metrics that are derived from the precision, recall, and FPR.<\/li>\n<\/ul>","1619d992":"<ul style=\"font-size:11pt\">\n    <li>It can be inferred that the top-left cell here corresponds to the true negative. So let's transform it to the same form as the example above.<\/li>\n    <li>According to the Scikit-learn documentation, $i$th row and $j$th column corresponds to the true label being $i$th class and the predicted label being $j$th class. So here, the rows represent the true values and the columns represent the predicted values. Thus, we need to transpose the matrix before transformation.<\/li>\n<\/ul>","f47af19e":"# Train - Test split","e358876e":"<p style='font-size:11pt'><\/p>","3c1d3b7f":"<p style=\"font-size:11pt\">Since the last 5 elements are all 1 for the precision, and the last 5 ones of the recall converges to 0, it may be okay to drop the last element from each array, in order to make the shape equal to that of the threshold array. After that, let's plot and see how the precision and recall varies as the threshold moves.<\/p>","96fb0171":"<p style=\"font-size:11pt\">Let's check the same thing for string variables, and do the same cleaning if it is necessary.<\/p>","d5253369":"# Metric Study","ab443d62":"## 6. Precision-Recall Curve\n\n<ul style=\"font-size:11pt\">\n    <li>Similar to the ROC curve, the Precision-Recall curve refers to a curve plotted by the recall and precision over all levels of threshold - the recall on $x$-axis and the precision on $y$-axis.<\/li>\n    <li>Although the precision and recall moves in the opposite direction for one model, when we compare several models, we can determine which model scores higher precision\/recall under given recall\/precision.<\/li>\n    <li>A model with higher precision under the given recall is a model whose misclassification rate is lower when the coverage rate of the true positives over the entire actual positives are equal. This implies that the model shows a better performance.<\/li>\n    <li>Furthermore, the statement 'a model with higher precision\/recall under the given recall\/precision' is equivalent to that 'a model with larger area under the Precision-recall curve.'<\/li>\n    <li>So we can use the Precision-recall curve AUC as another precision metric. Similar to the ROC AUC, this is a greater-the-better metric ranging from 0.5 to 1 as well.<\/li>\n<\/ul>","cac38a9a":"<ul style=\"font-size:11pt\">\n    <li>However, the hit ratio only cares about the total sum of misclassified values. In other words, it does not care the difference of relative importance between the false negative and the false positive.<\/li>\n<\/ul>","adfb1e84":"<ul style=\"font-size:11pt\">\n    <li>As we can see, by adjusting the threshold to the level maximizing the F1 score, although the hit ratio has decreased a little bit, we got lower false negatives, which suits our purpose for this model.<\/li>\n    <li>This is the end of this study, and thanks for reading this long notebook. Next time, I'll study the performance metrics used in regression.<\/li>\n<\/ul>","42f021e7":"# Conclusion\n\n<ul style=\"font-size:11pt\">\n    <li>We have explored several mostly used performance metrics of a classification model other than the accuracy.<\/li>\n    <li>The first and the most important concept is a (decision) threshold, whether an observation is classified to be positive(1) or negative(0) is determined by the threshold, and thus the other metrics are all vary as this threshold level moves.<\/li>\n    <li>There are some metrics that gives value at a specific threshold: precision, recall, and f1 score, while there are some metric that gives value all over the entire possible thresholds: ROC curve, Precision-Recall curve and the area under each curve(AUC).<\/li>\n    <li>Therefore, in my opinion, it is recommended to use the ROC AUC or the Precision-Recall AUC for model selection and hyper-parameter optimization, and then set the appropriate threshold to obtain other metrics such as the F1 score, so that you can minimize the number of misclassified observations.<\/li>\n    <li>So, I'll start over from the model selection, hyper-parameter optimization and evaluation again, following the process I have mentioned.<\/li>\n<\/ul>","78ac3ce9":"<p style=\"font-size:11pt\">Now let's check the capital gain and loss, and the working hours per week.<\/p>","5759b1d8":"# Train - Validation Split","6c6553a2":"<p style=\"font-size:11pt\">Here, since the XGB scored the highest, I'm going to use it.<\/p>","e582672a":"## 4. F1 score\n\n<ul style=\"font-size:11pt\">\n    <li>The F1 score is defined to be the <strong>harmonic mean<\/strong> of the precision and recall.<\/li>\n    <li>As you know, there are three well-known means of numbers: arithmetic mean, geometric mean, and harmonic mean, each of which is computed following:\n        <ul style=\"font-size:11pt\">\n            <li>Arithmetic mean: $$ \\frac{a+b}{2} $$<\/li>\n            <li>Geometric mean: $$ \\left( ab \\right)^{\\frac{1}{2}} = \\sqrt{ab} $$<\/li>\n            <li>Harmonic mean: $$ \\frac{2}{\\frac{1}{a} + \\frac{1}{b}} = \\frac{2ab}{a+b} $$<\/li>\n        <\/ul>\n    <\/li>\n    <li>These three means have a relationship such that $A \\geq G \\geq H$, where $A$ denotes the arithmetic mean, $G$ denotes the geometric mean, and $H$ denotes the harmonic mean.<\/li>\n    <li>What remarkable is that the three means are equal when $a = b$, and this also implies that both the geometric mean and harmonic mean reaches the maximum when $a = b$.<\/li>\n    <li>Finally, the F1 score is a greater-the-better metric, and it ranges from 0 to 1.<\/li>\n<\/ul>","143d50cc":"## 3. ROC curve","2e8fe5de":"<p style=\"font-size:11pt\">Hooray! We're done with the EDA and feature preprocessing!<\/p>","1f74f94a":"<ul style=\"font-size:11pt\">\n    <li>From this, we can confirm that even though shape of thresholds from two modules are different, they are basically computed based on the same process.<\/li>\n    <li>Now, let's plot the fpr and tpr over all threshold levels, and then plot the roc curve.<\/li>\n<\/ul>","2f229c0e":"<ul style=\"font-size:11pt\">\n    <li>As you can see, about 400 more observations are classified to be positive as we moved the threshold from 0.5 to 0.4. To check it more clearly, let's make a new confusion matrix. It is time to use the ID.<\/li>\n<\/ul>","ffc675d2":"## 1. Thresholds\n\n<p style=\"font-size:11pt\">\n    <ul style=\"font-size:11pt\">\n        <li>Threshold refers to a value such that governs the choice to turn a projected probability or scores into a class label (cite: <a href=\"https:\/\/deepchecks.com\/glossary\/classification-threshold\/\">Deep Checks<\/a>). For binary classification, it is set to 0.5 by default, which means that if an observation is predicted to be 1 with a probability over 0.5, say 0.75, then its predicted class becomes 1.<\/li>\n        <li>This implies that whether an observation is predicted as 0 or 1 may vary as the threshold value changes. For example, let's assume that a researcher tries to predict one observation whose true label equals to 1 and say its predicted probability of being 1 equals to 0.55. Then, if the threshold is set to 0.5, it is classified as 1, and accuracy of the model equals to 100%. However, if the threshold is set to a higher value, say 0.6, then it is classified as 0 and the accuracy becomes 0 as well.<\/li>\n        <li>In short, in my opinion, threshold is the most important concept to learn, especailly for classification, because it is the problem-specific value, and even the performance of a model can be affected by this value. Threshold values can be obtained from <strong>precision_recall_curve<\/strong> module of the Scikit-learn metrics.<\/li>\n        <li>Then how to set the threshold properly? It depends on many factors, such as the structure of dataset, especially the distribution of a target variable(in other words, imbalance of the target). I'd like to explain it later, while I talk about other metrics.<\/li>\n    <\/ul>\n<\/p>","f4be0c05":"<p style='font-size:11pt'>Well, there exist more than one hundred observations at the net gain of $\\$$99,999, and it seems to be the upper bound of income. Furthermore, the value of $\\$$99,999 is very far away from other values. Thus, let's drop the observations with the net gain 99,999. Also, for the working hour feature, 99-hour working looks like the upper bound, thus let's drop them as well.<\/p>","e55e8220":"## 2. Confusion matrix, Precision, and Recall\n\n<p style=\"font-size:11pt\">\n    <ul style=\"font-size:11pt\">\n        <li>As we saw above, the metrics called <strong>precision and recall<\/strong> both vary as the threshold level moves, and there is a trade-off between them.<\/li>\n        <li>Now, it's time to learn how the precision and the recall are calculated. To know it, we need to talk about the confusion matrix first. The confusion matrix is a $ 2 \\times 2 $ matrix showing how many observations are classified correctly. The two rows of the matrix correspond to the predicted results and the two columns correspond to the true values. See an example below (I got it from <a href=\"https:\/\/glassboxmedicine.com\/2019\/02\/17\/measuring-performance-the-confusion-matrix\/\">Glass Box<\/a>).\n            <img src=\"https:\/\/glassboxmedicine.files.wordpress.com\/2019\/02\/confusion-matrix.png?w=816\">\n        <\/li>\n        <li>So, the top-left cell contains the observations whose true value and the predicted value are all equal to 1(positive), and the number of them is called the <strong>True Positive<\/strong>.<\/li>\n        <li>Next, the top-right cell contains the observations whose true value equals to 0(negative) but the predicted value is 1. In other words, the observations in the cell are <em>misclassified to be positive<\/em> and the number of them is called the <strong>False Positive<\/strong>, which implies that their labels predicted to be \"positive\" are \"false\".<\/li>\n        <li>Similarly, observations in the bottom-left cell are <em>misclassified to be negative<\/em> and the number of them is called the <strong>False Negative<\/strong>, which implies that their labels predicted to be \"negative\" are \"false\".<\/li>\n        <li>Finally, observations in the bottom-right cell are correctly classified as negative, and the number of them is called the <strong>True Negative<\/strong>.<\/li>\n        <li>Let's see the confusion matrix of the model we have trained.<\/li>\n    <\/ul>\n<\/p>","9e7b1eea":"<p style=\"font-size:11pt\">Let's just use accuracy as a metric here.<\/p>","e5b57166":"## Model Selection\n\n<p style=\"font-size:11pt\">I'll use ROC AUC here now.<\/p>","969f0b3d":"<p style=\"font-size:11pt\">Fortunately, there seems to no missing values. The 'fnlwgt' variable is a weight that is given by a researcher arbitrarily, so let's drop it. Also, the 'education' variable and the 'education-num' variable seem to correspond to each other one-to-one. Let's take a look at them.<\/p>\n\n<p style=\"font-size:11pt\">However, there seems to be a blank in front of the head of each column's name except for the age column. So let's modify them.<\/p>","cf7dc449":"<ul style=\"font-size:11pt\">\n    <li>Let's see if both threshold arrays, and both recall arrays share values.<\/li>\n<\/ul>","d5b3e4d2":"# Feature - Target Split","3a0ca3fc":"<ul style=\"font-size:11pt\">\n    <li>The roc_auc_score method computes the area under the ROC curve directly using the true value and predicted probability.<\/li>\n    <li>While the auc method just computes the area under the curve generated by inputs. For example, if the inputs are the recall and precision, auc computes another performance metric called 'Precision-Recall AUC.' I'll explain it later.<\/li>\n<\/ul>","10c07f52":"<ul style=\"font-size:11pt\">\n    <li>As we've expected, the number of the true positives and the false positives increased as we lower the threshold level. Let's compute precision and recall and compare them to the ones under the threshold 0.5.<\/li>\n<\/ul>"}}