{"cell_type":{"b95c7212":"code","a86cf86e":"code","f590c929":"code","99f168ef":"code","db54eeac":"code","3cbe2b84":"code","9715b512":"code","63f2ee54":"code","e295c7e0":"code","e2b36b9b":"code","7a45faa1":"code","11b58fb4":"code","6e63448e":"code","6bcae450":"code","db89784f":"code","b7e43943":"code","b715b855":"code","7b3d29df":"code","5d1b26bf":"code","e16b8bd0":"code","86954b34":"code","5f15c226":"markdown","25d2a03d":"markdown","0f7ab5ad":"markdown","5d2671b4":"markdown","8e3f3354":"markdown","b15d9ab6":"markdown","83e6a3d6":"markdown","7b529dd3":"markdown","86724608":"markdown","aea8a35a":"markdown","b9759906":"markdown","49d882d9":"markdown","b15e683c":"markdown","4a2415e0":"markdown"},"source":{"b95c7212":"# Kaggle input\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a86cf86e":"# Import Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2 # OpenCV\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","f590c929":"# Get data\n\nLABELS = ('PNEUMONIA','NORMAL')\nIMG_SIZE=150\ndef get_data(data_dir):\n    data = []\n    for label in LABELS:\n        path = os.path.join(data_dir, label)\n        class_num = LABELS.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE)) # Reshaping Images\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","99f168ef":"train_raw = get_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest_raw = get_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval_raw = get_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')","db54eeac":"# Training Data\ncount = []\nfor i in train_raw:\n    if(i[1] == 1):\n        count.append(\"Normal\")\n    else:\n        count.append('Pneumonia')\n\nsns.countplot(count)","3cbe2b84":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\n\nfor i, ax in enumerate(axes.flat):\n#     if (train_raw[i][1] == 1):\n        img = train_raw[i][0]\n        ax.imshow(img, cmap='gist_heat')\n        ax.set_title(\"Normal\")\nfig.tight_layout()    \nplt.show()\n\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\n\nfor i, ax in enumerate(axes.flat):\n#     if (train_raw[i][1] == 0):\n        img = train_raw[-(i+1)][0]\n        ax.imshow(img, cmap='gist_heat')\n        ax.set_title(\"Pneumonia\")\nfig.tight_layout()\nplt.show()\n\n","9715b512":"#Splitting data\n\nX_train = []\ny_train = []\n\nX_val = []\ny_val = []\n\nX_test = []\ny_test = []\n\nfor image, label in train_raw:\n    X_train.append(image)\n    y_train.append(label)\n    \nfor image, label in val_raw:\n    X_val.append(image)\n    y_val.append(label)\n    \nfor image, label in test_raw:\n    X_test.append(image)\n    y_test.append(label)","63f2ee54":"# Normalize the data\nX_train = np.array(X_train) \/ 255\nX_val = np.array(X_val) \/ 255\nX_test = np.array(X_test) \/ 255","e295c7e0":"# Resize data for deep learning \nX_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_train = np.array(y_train)\n\nX_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_val = np.array(y_val)\n\nX_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_test = np.array(y_test)\n","e2b36b9b":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)\n","7a45faa1":"np.random.seed = 18\nNUM_EPOCHS = 18\n\nmodel = Sequential()\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (160,160,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 128 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","11b58fb4":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                            patience = 2,\n                                            verbose=1,\n                                            factor=0.3,\n                                            min_lr=0.000001)\n\nhistory = model.fit(datagen.flow(X_train, y_train, batch_size = 32),\n                    epochs = NUM_EPOCHS,\n                    validation_data = datagen.flow(X_val, y_val),\n                    callbacks = [learning_rate_reduction]\n                   )","6e63448e":"print(\"Loss of the model is - \" , model.evaluate(X_test,y_test)[0])\nprint(\"Accuracy of the model is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","6bcae450":"epochs = [i for i in range(NUM_EPOCHS)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Training & Validation Loss\")\nplt.show()","db89784f":"y_preds = model.predict_classes(X_test)\ny_preds = y_preds.reshape(1,-1)[0]\ny_preds","b7e43943":"# Classification report\nprint(classification_report(y_test, y_preds, target_names= ['Pneumonia (0)', 'Normal (1)']))","b715b855":"# Confusion matrix\nplt.figure(figsize = (8,8))\nsns.heatmap(confusion_matrix(y_test, y_preds), annot=True, fmt='', xticklabels=['Pneumonia','Normal'], yticklabels=['Pneumonia','Normal'])","7b3d29df":"correct = np.nonzero(y_preds == y_test)[0]\nincorrect = np.nonzero(y_preds != y_test)[0]\ncorrect[:10], incorrect[:10]","5d1b26bf":"def plot_preds(correct, incorrect, X_test, y_test, n=4, corr=True):\n    \"\"\"\n    Prints correct or incorrect predictions.\n    \"\"\"\n    labels=['pneumonia', 'normal']\n    \n    plt.figure(figsize=(10,15))\n    \n    # Correct\n    if corr:\n        print(\"Correct predictions:\\n\")\n        i=0\n        for c in correct[:n]:\n            plt.subplot(((n+1)\/\/2)+1, 2, i+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.imshow(X_test[c])\n            plt.title(f'Predicted: {labels[y_preds[c]]}, Actual: {labels[y_test[c]]}')\n            plt.tight_layout()\n            i+=1\n\n    # Incorrect\n    else:\n        print(\"Incorrect predictions:\\n\")\n        i=0\n        for inc in incorrect[:n]:\n            plt.subplot(((n+1)\/\/2)+1, 2, i+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.imshow(X_test[inc])\n            plt.title(f'Predicted: {labels[y_preds[inc]]}, Actual: {labels[y_test[inc]]}')\n            plt.tight_layout()\n            i+=1","e16b8bd0":"plot_preds(correct, incorrect, X_test, y_test)","86954b34":"plot_preds(correct, incorrect, X_test, y_test, corr=False)","5f15c226":"# Splitting, and normalizing data\n\nWe will:\n* Split the data into X (features) and y (labels)\n* Normalize the data from 0-255 to 0-1 for quicker convergence by Convolutional Neural Network\n* Resize data to shape for deep learning","25d2a03d":"# Training analysis","0f7ab5ad":"# Model analysis","5d2671b4":"# Training model\n\nWe will use a convolutional neural network to solve this problem.\n\n<img src=\"https:\/\/miro.medium.com\/max\/2000\/1*vkQ0hXDaQv57sALXAJquxA.jpeg\"\n     alt=\"cnn-explained\"\n     style=\"float: left; margin-right: 10px;\" \/>\n\nSource: https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","8e3f3354":"# Loading Dataset","b15d9ab6":"# Pneumonia Classifier\n\n**Name:** Pneumonia Classifier\n\n**Author:** Sharome Burton\n\n**Date:** 07\/25\/2021\n\n**Description:** Machine learning model used to determine the presence of pneumonia in a patient given an image of a chest X-ray.\n\n**Kaggle:** https:\/\/www.kaggle.com\/sharomeethan\/pneumonia-classifier-cnn-acc-91\n\n**GitHub:** https:\/\/github.com\/koulkoudakis\/pneumonia-classifier-CNN\/tree\/main\n\n<img src=\"https:\/\/i.imgur.com\/jZqpV51.png\"\n     alt=\"pneumonia-examples\"\n     style=\"left; margin-right: 10px;\" \/>\n\n\n## 1. Problem definition\n> How accurately can we identify pneumonia in the lungs of a patient given their chest X-ray?\n\n## 2. Data\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal).\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n   \n* `\/test` - the test set\n* `\/train` - the training set\n* `\/val` - the validation set\n\n\nCitation: http:\/\/www.cell.com\/cell\/fulltext\/S0092-8674(18)30154-5\n\nData: https:\/\/data.mendeley.com\/datasets\/rscbjbr9sj\/2\n\n## 3. Evaluation \n\n> **Goal:** Detect pneumonia in a patient from their medical imagery at >90% accuracy\n\nGuide: https:\/\/www.kaggle.com\/kashyapgohil\/pneumonia-detection-using-cnn\n\n","83e6a3d6":"Data appears imbalanced, with Pneumonia cases outnumbering number cases 3:1.","7b529dd3":"# Importing tools","86724608":"# Data augmentation\n In order to avoid overfitting, we must artificially expand our training dataset with small transformations (without changing the label) called augmentation techniques. Some popular augmentation techniques are grayscales, flips, random crops, translations, and rotations.\n \nBy applying some of these augmentations, we are able to eventually produce a more robust model.","aea8a35a":"# Conclusion\n\nModel shows a reasonable precision of **92%** and f-1 score of **0.92**. Additional preprocessing of the images and tuning of the model could be performed to improve this score, but it has passed the evaluation metric for this version.","b9759906":"# Predictions","49d882d9":"# Exploratory Data Analysis (EDA)","b15e683c":"## Correct Predictions","4a2415e0":"## Incorrect Predicitons"}}