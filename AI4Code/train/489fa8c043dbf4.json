{"cell_type":{"dfe8612f":"code","ca07f425":"code","f3ba8066":"code","437c914a":"code","f1e40d5d":"code","48cd0901":"code","e22ba583":"code","66a7bc98":"code","3c6452cf":"code","92211dda":"code","9c7849b8":"code","c993046d":"code","e96fbe94":"code","dcf80eaf":"code","525252a6":"code","f9bbd294":"code","a50b167d":"code","6852cf77":"code","60ef22ff":"code","387f8c11":"code","bab99127":"code","d773dbcc":"code","194fa37c":"code","421ff32a":"code","925b56aa":"code","ffecbdf5":"code","0f8947d0":"code","d1b546e9":"code","fb710c6f":"code","9f48411d":"code","f9560f28":"code","a2593cfb":"code","61859625":"code","abaa4644":"code","45cc8106":"code","3de2e099":"code","f069f4a1":"code","e0d2293b":"code","7ed814d0":"code","8f4bc6af":"code","a6214977":"code","7b82d12e":"code","db39ce5c":"code","3685f49d":"markdown","37bdeac5":"markdown","0e3c14c1":"markdown","38a3558f":"markdown","8c0a0749":"markdown","bee72829":"markdown","cbd265ef":"markdown","422ee3da":"markdown","a300ee95":"markdown","2298e297":"markdown","5e8bfc98":"markdown","d31f9fcb":"markdown","19527f37":"markdown","f0bfa189":"markdown"},"source":{"dfe8612f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ca07f425":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.arima_model import ARMA, ARIMA","f3ba8066":"data = pd.read_csv(\"..\/input\/sales_data_sample.csv\", encoding = \"unicode_escape\")","437c914a":"data.info()","f1e40d5d":"data.columns","48cd0901":"data.isnull().sum()","e22ba583":"data.describe()","66a7bc98":"data.head(2)","3c6452cf":"data[\"ORDERDATE\"] = data[\"ORDERDATE\"].astype(\"datetime64[ns]\")\ndata[\"ORDERDATE\"].max(), data[\"ORDERDATE\"].min()","92211dda":"plt.figure(figsize =(50,8))\nmean_group = data.groupby([\"ORDERDATE\"])[\"SALES\"].mean()\nplt.plot(mean_group)\nplt.title(\"Time series average\")\nplt.show()","9c7849b8":"plt.figure(figsize =(50,8))\nmean_group = data.groupby([\"ORDERDATE\"])[\"SALES\"].median()\nplt.plot(mean_group)\nplt.title(\"Time series median\")\nplt.show()","c993046d":"plt.figure(figsize =(50,8))\nmean_group = data.groupby([\"ORDERDATE\"])[\"SALES\"].std()\nplt.plot(mean_group)\nplt.title(\"Time series standard deviation\")\nplt.show()","e96fbe94":"#data.set_index(\"ORDERDATE\", inplace = True)","dcf80eaf":"data.index","525252a6":"forcast = data[[\"ORDERDATE\",\"SALES\"]]\nforcast = forcast.sort_values(\"ORDERDATE\").reset_index()\nforcast = forcast.drop(\"index\", axis = True)\nforcast[\"ORDERDATE\"] = forcast[\"ORDERDATE\"].astype(\"datetime64[ns]\")\nforcast.head()","f9bbd294":"forcast[\"ORDERDATE\"].min(), forcast[\"ORDERDATE\"].max()","a50b167d":"leng = len(forcast[\"ORDERDATE\"])\nleng","6852cf77":"forcast.set_index(\"ORDERDATE\", inplace = True)\nforcast.tail()","60ef22ff":"forcast.index","387f8c11":"model = AR(forcast)\nmodel_fit = model.fit()","bab99127":"#make prediction\n#import statsmodels.tsa.ar_model.ARResultsWrapper.predict \nyhat = model_fit.predict(start = 2800, end = 2823)","d773dbcc":"yhat.columns = [\"SALES\"]\nyhat.head()","194fa37c":"#plt.plot(forcast[\"SALES\"], color = \"blue\", label = 'Actual')\nplt.plot(yhat, color = \"blue\", label = \"predicted\")\nplt.show()","421ff32a":"mod2 = ARMA(forcast[\"SALES\"], order = (2,1))\nres2 = mod2.fit(disp=False)","925b56aa":"yhat2 = res2.predict(start = 2823,end = 3000)","ffecbdf5":"#plt.plot(test_forcast, color='blue', label='Original')\nplt.plot(yhat, color='red', label='AR')\nplt.plot(yhat2, color='blue', label='ARMA')\nplt.legend(loc='best')\nplt.title('Comparing two models')\nplt.show(block=False)","0f8947d0":"mvg_avg = forcast.rolling(window = 12). mean()\nmvg_std = forcast.rolling(window = 12). std()\n\n#orig = plt.plot(forcast, color='blue', label='Original')\nmean = plt.plot(mvg_avg, color='red', label='Rolling Mean')\nstd = plt.plot(mvg_std, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","d1b546e9":"y = forcast['SALES'].resample('MS').mean()\nforcast[\"2003\":].head()","fb710c6f":"#Plot to see sales data visually\ny.plot(figsize = (50,6))\nplt.show()","9f48411d":"from statsmodels.tsa.stattools import adfuller\nresult = adfuller(y)\nprint(\"ADF Statistics\", result[0])\nprint(\"P value\", result[1])","f9560f28":"print('Results of Dickey Fuller Test:')\ndftest = adfuller(forcast['SALES'], autolag='AIC')\n\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","a2593cfb":"#from statsmodels.tsa.seasonal import seasonal_decompose\n#decompose = seasonal_decompose(y)\n\n#plt.title(\"Original plot\")\n#plt.legend(loc = \"best\" )\n\n#trend = decompose.trend\n#plt.show()\n#plt.plot(trend, label = \"Trend\")\n#plt.title(\"Trend Plot\")\n#plt.legend(loc = \"best\")\n\n#seasonal = decompose.seasonal\n#plt.show()\n#plt.plot(seasonal, label = \"Seasonal\")\n#plt.title(\"Seasonal Plot\")\n#plt.legend(loc = \"best\")\n\n#residual = decompose.resid\n#plt.show()\n#plt.plot(residual, label = \"Residual\")\n#plt.legend(loc = \"best\")","61859625":"#Transformation\nforcast[\"SALES_log\"] = np.log(forcast[\"SALES\"])\n#forcast[\"log_difference\"] = forcast[\"SALES_log\"] - forcast[\"SALES_log\"].shift(1)\nforcast_diff = forcast[\"SALES_log\"]\n#forcast[\"log_difference\"].dropna().plot()","abaa4644":"#moving_avg = forcast_diff.rolling(12).mean()\n#plt.plot(forcast_diff)\n#plt.plot(moving_avg, color='red')","45cc8106":"from statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(forcast[\"SALES\"], nlags=10)\nlag_pacf = pacf(forcast[\"SALES\"], nlags=10, method='ols')\n\n#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(forcast[\"SALES\"])),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(forcast[\"SALES\"])),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(forcast[\"SALES\"])), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(forcast[\"SALES\"])), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()            ","3de2e099":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(forcast[\"SALES\"], order=(2,1,0))\nresults_AR = model.fit(disp=-1)\nplt.plot(forcast[\"SALES\"])\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - forcast[\"SALES\"])**2))\nprint('Plotting AR model')","f069f4a1":"model = ARIMA(forcast[\"SALES\"], order=(0,1,2))\nresults_AR = model.fit(disp=-1)\nplt.plot(forcast[\"SALES\"])\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - forcast[\"SALES\"])**2))\nprint('Plotting AR model')","e0d2293b":"# AR+I+MA = ARIMA model\nmodel = ARIMA(forcast[\"SALES\"], order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(forcast[\"SALES\"])\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - forcast[\"SALES\"])**2))\nprint('Plotting ARIMA model')","7ed814d0":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","8f4bc6af":"#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum.head())","a6214977":"predictions_ARIMA_log = pd.Series(forcast['SALES'].iloc[0], index=forcast.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()\n","7b82d12e":"predictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(forcast[\"SALES\"])\nplt.plot(predictions_ARIMA)","db39ce5c":"results_ARIMA.plot_predict(1,264) \n","3685f49d":"For a Time series to be stationary, its ADCF test should have:\n\n1. p-value to be low (according to the null hypothesis)\n\n2. The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\nFrom the above ADCF test result, we see that p-value(at max can be.0) is very negligible. Also critical values are  very close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is  stationary","37bdeac5":"<strong> MA(Moving Average) <\/strong>","0e3c14c1":"<h3> As the data follows some seasonality issue, so we need to remove those patterns using transformation and seasonal differentiation <\/h3>","38a3558f":"Taking a look at the furniture sales data for the year 2013","8c0a0749":"<strong> Prediction and reverse transformation<\/strong>","bee72829":"<strong> Autoregression Model<\/strong>\nAn autoregressive model is when a value from a time series is regressed on previous values from that same time series. for example, y(t) or y(t-1). \n\nThe equation of autoregression is given by \n\ny(t) = b(0) +b(1) y(t-1) +b(2)y(t-2) + b(3)y(t-3)+ ......\n\n","cbd265ef":"<h3> Syntax <\/h3>\n<strong>class statsmodels.tsa.ar_model.AR(endog, dates=None, freq=None, missing='none')[source]\nAutoregressive AR(p) model<\/strong>\n\n<strong>Parameters:\t<\/strong>\nendog (array-like) \u2013 1-d endogenous response variable. The independent variable.\n\ndates (array-like of datetime, optional) \u2013 An array-like object of datetime objects. If a pandas object is given for endog or exog, it is assumed to have a DateIndex.\n\nfreq (str, optional) \u2013 The frequency of the time-series. A Pandas offset or \u2018B\u2019, \u2018D\u2019, \u2018W\u2019, \u2018M\u2019, \u2018A\u2019, or \u2018Q\u2019. This is optional if dates are given.\n\nmissing (str) \u2013 Available options are \u2018none\u2019, \u2018drop\u2019, and \u2018raise\u2019. If \u2018none\u2019, no nan checking is done. If \u2018drop\u2019, any observations with nans are dropped. If \u2018raise\u2019, an error is raised. Default is \u2018none.\u2019","422ee3da":"<strong> Different time series forcasting method. <\/strong>\n1. Autoregression (AR)\n2. Moving Average (MA)\n3. Autoregressive Moving Average (ARMA)\n4. Autoregressive Integrated Moving Average (ARIMA)\n5. Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n6. Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\n7. Vector Autoregression (VAR)\n8. Vector Autoregression Moving-Average (VARMA)\n9. Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\n10. Simple Exponential Smoothing (SES)\n11. Holt Winter\u2019s Exponential Smoothing (HWES)\n\nFor more information visit [](http:\/\/)https:\/\/machinelearningmastery.com\/time-series-forecasting-methods-in-python-cheat-sheet\/","a300ee95":"<strong> Seasonal differentiation <\/strong>\nIn seasonal differencing, instead of calculating the difference between consecutive values, we calculate the difference between an observation and a previous observation from the same season. For example, an observation taken on a Monday will be subtracted from an observation taken on the previous Monday. Mathematically it can be written as:\n\nyt\u2018 = yt \u2013 y(t-n)\n\nn=7\n","2298e297":"From the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model.","5e8bfc98":"Time series is a collection of data points collected at certain time intervals. They are analyzed to determine the long tern trend to forcast future. It is one of the most important part for any industries like Banking, Retail, Healthcare etc. It helps analyst and service chain manager to take effective decision.","d31f9fcb":"<strong>Checking Stationarity<\/strong>  <h3> \n<h3> What is stationary <\/h3>Stationarity\u2019 is one of the most important concepts you will come across when working with time series data. A stationary series is one in which the properties \u2013 mean, variance and covariance, do not vary with time.\n\n\nIn time-series analysis we must check whether the data follow any treand, seanality, or residual effects or not, and if there is any such effect then our first step is to remove them. \n<strong>Augmented Dickey-Fuller(ADF) <\/strong>statistics is one of the most popular statistics technique to check where the data is stationary or non-stationary. It uses a autoregressive model optimize and optimizes an information criteria across multiple different lag value.\n The Hypotesis in this case is that the time series can be represented by a unit root, that is non stationary(has some time dependent structure). The alternative hypothesis is that the time series is stationary.","19527f37":" <strong>  Time Series Forcasting  <\/strong> : <strong> ARIMA Model <\/strong>\n ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n\n<strong>Number of AR (Auto-Regressive) terms (p)<\/strong> AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)\u2026.x(t-5).\n\n<strong>Number of MA (Moving Average) terms (q)<\/strong>: MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)\u2026.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n\n<strong>Number of Differences (d)<\/strong>: These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\nAn importance concern here is how to determine the value of \u2018p\u2019 and \u2018q\u2019. We use two plots to determine these numbers. Lets discuss them first.\n\n<strong>Autocorrelation Function (ACF)<\/strong>: It is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019t2\u2019 with series at instant \u2018t1-5\u2019\u2026\u2019t2-5\u2019 (t1-5 and t2 being end points).\n\n<strong>Partial Autocorrelation Function (PACF):<\/strong> This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\nThe ACF and PACF plots for the TS after differencing can be plotted as:","f0bfa189":"#Resampling the datetime data. Here we use the start of each month as the timestamp and take the average daily sales value for a particular month since working with the current datetime data becomes tricky\n"}}