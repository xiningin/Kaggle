{"cell_type":{"4dd2b882":"code","9726a4cd":"code","8f169647":"code","eb1c9098":"code","3c648092":"code","5841fdde":"code","d59fd117":"code","ebae0de9":"code","566575b3":"code","608bcd83":"code","c4f86dfe":"code","2f8c9fb0":"code","f0a4f6c5":"code","2fcffd18":"code","89917e8b":"code","6f8d7815":"code","7f0e43f2":"code","6819e6d4":"code","6da9a02b":"code","edcaaad6":"code","986443d8":"code","b5cfe4c0":"code","643e1cae":"code","247aa473":"code","af4a44bf":"code","f3fcd62f":"code","2948a411":"code","91d9515f":"code","e3de5955":"code","ff19af29":"code","64ec7300":"code","759487c3":"code","3b96472b":"code","360ba4ff":"code","463a4e06":"markdown","bbb02404":"markdown","2f1725a8":"markdown","5f25924b":"markdown","ffbd59d8":"markdown","e3b2cfa1":"markdown","a29bafc4":"markdown"},"source":{"4dd2b882":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport nltk\nimport string\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfrom scipy.spatial.distance import cosine\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nrandom_seed = 42\ntotal_documents = 10000","9726a4cd":"fake_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","8f169647":"fake_df.head()","eb1c9098":"fake_df.info()","3c648092":"true_df.head()","5841fdde":"true_df.info()","d59fd117":"fake_df['true'] = 0\ntrue_df['true'] = 1","ebae0de9":"df = fake_df.append(true_df, ignore_index=True)\ndf","566575b3":"df['text'] = df['text'].str.strip()\n#df.loc[df[df['text'] == ''].index].drop()\ndf.drop(df[df['text']==''].index, axis='index', inplace=True)","608bcd83":"df = df.sample(n=total_documents, replace=False, random_state=random_seed)\ndf","c4f86dfe":"df.text = df.text.str.lower().str.translate(str.maketrans({char : '' for char in string.punctuation}))","2f8c9fb0":"df.text.apply(word_tokenize)","f0a4f6c5":"def remove_stop_words(words, lang, domain_words=None):\n    if domain_words == None:\n        domain_words = []\n        \n    stop_words = nltk.corpus.stopwords.words(lang)\n    \n    return [i for i in words if i not in stop_words and i not in domain_words and not i.isdigit()]","2fcffd18":"df['tokens'] = df.text.apply(word_tokenize).apply(remove_stop_words, args=('english', ))","89917e8b":"stemmer = PorterStemmer()\ndf.tokens = df.tokens.apply(lambda x : [stemmer.stem(word) for word in x])","6f8d7815":"df.tokens.str.join(' ')","7f0e43f2":"cv = CountVectorizer(max_features=1000)\nbag_of_words_df = pd.DataFrame(cv.fit_transform(df.tokens.str.join(' ')).toarray(), columns=cv.get_feature_names())\nbag_of_words_df","6819e6d4":"def compute_cos_sim(df, loc1, loc2):\n    return cosine(df.iloc[loc1, :], df.iloc[loc2, :])","6da9a02b":"compute_cos_sim(bag_of_words_df, 1, 2)","edcaaad6":"compute_cos_sim(bag_of_words_df, 0, 2)","986443d8":"for i in range(0, 11):\n    print(f'i = {i} cos = {compute_cos_sim(bag_of_words_df, 0, i)}')","b5cfe4c0":"# Normalizing to use KMeans with cossine\nlen = np.sqrt((bag_of_words_df ** 2).sum(axis=1)).to_numpy()\nlen = len.reshape(len.shape[0], -1)\nbag_of_words_df = bag_of_words_df \/ len\n\n# Dropping NaN values from bag_of_words_df and df\ndf.drop(df.iloc[bag_of_words_df[bag_of_words_df['21st'].isna()].index, :].index, axis='index', inplace=True)\nbag_of_words_df.dropna(inplace=True) ","643e1cae":"bag_of_words_df.isna()","247aa473":"bag_of_words_df[np.any(bag_of_words_df.isna(), axis=1)]","af4a44bf":"inertias = []\nsilhouettes_samples = []\nsilhouettes_scores = []\n\ntesting_k = range(2, 11)\n\nfor k in testing_k:\n    print(f'Computing for k={k}')\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(bag_of_words_df)\n    inertias.append(kmeans.inertia_)\n    silhouettes_samples.append(silhouette_samples(bag_of_words_df, kmeans.labels_, metric='cosine'))\n    silhouettes_scores.append(silhouette_score(bag_of_words_df, kmeans.labels_, metric='cosine'))\n","f3fcd62f":"plt.plot(testing_k, inertias)","2948a411":"plt.plot(testing_k, silhouettes_scores)","91d9515f":"k = np.argmax(silhouettes_scores) + testing_k[0]\nprint(f\"We're choosing k as {k}.\")","e3de5955":"kmeans = KMeans(n_clusters=k)\nkmeans.fit(bag_of_words_df)\nsilhouette = silhouette_samples(bag_of_words_df, kmeans.labels_, metric='cosine')\nlabels = kmeans.labels_","ff19af29":"df['label'] = labels\ndf['silhouette'] = silhouette","64ec7300":"df = df.sort_values(['label', 'silhouette'], ascending=False)\ndf","759487c3":"df[df['label'] == 0].head()","3b96472b":"df[df['label'] == 1].head()","360ba4ff":"df[df['label'] == 2].head()","463a4e06":"_Esse notebook foi originalmente minha **Atividade 1 da mat\u00e9ria de Minera\u00e7\u00e3o de Dados N\u00e3o Estruturados** - SCC0287 - no ano de 2021, segundo semestre com o Professor Ricardo Marcondes Marcacini._\n\n_O objetivo dela \u00e9 estudar a cria\u00e7\u00e3o de Word Embeddings utilizando Bag-of-Words (uma das t\u00e9cnicas mais elementares de NLP). Al\u00e9m disso, dever\u00edamos fazer um pr\u00e9-processamento b\u00e1sico do texto (removendo stopwords e aplicando steamming) e depois agrupar os textos em clusters e selecionando o n\u00famero de clusters com base na m\u00e9trica da silhueta._","bbb02404":"## 5. Mostrar os top-n documentos de cada cluster","2f1725a8":"## 1. Achar um DataSet","5f25924b":"## 0. Imports","ffbd59d8":"## 3. k-Means com similaridade cosseno","e3b2cfa1":"## 4. Calcular a Silhueta ","a29bafc4":"## 2. Pr\u00e9-processar usando Bag-of-Words e Stemmer"}}