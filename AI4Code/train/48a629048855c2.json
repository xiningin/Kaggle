{"cell_type":{"0ff3b511":"code","c6966e47":"code","a381e02f":"code","83d5afd1":"code","5c5c0e44":"code","d60ee047":"code","157dd212":"code","f0e8e7b6":"code","94d02bea":"code","a9047641":"code","bfacf1d2":"code","56160a0b":"code","8f08afcf":"code","3c51158d":"code","41c76bb2":"code","f7f3506f":"code","6a733a71":"code","890fcb37":"code","18fedf14":"code","983e4fa8":"code","f9f99cc6":"code","4c2c7952":"code","b302f326":"code","0a004b62":"markdown","0154e332":"markdown","df8f1d3e":"markdown","4ad7cbd3":"markdown","c97b9f8b":"markdown","183c3cce":"markdown","61e9f7a6":"markdown"},"source":{"0ff3b511":"## imports \n\nimport pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c6966e47":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>div.output_scroll { height: 70em; }<\/style>\"))","a381e02f":"sns.set(style= 'darkgrid', \n       color_codes=True,\n       font = 'Arial',\n       font_scale= 1.5,\n       rc={'figure.figsize':(12,8)})","83d5afd1":"train = pd.read_csv(\"..\/input\/physics-vs-chemistry-vs-biology\/dataset\/train.csv\")\ntest = pd.read_csv(\"..\/input\/physics-vs-chemistry-vs-biology\/dataset\/test.csv\")","5c5c0e44":"train.head()","d60ee047":"train.shape","157dd212":"test.shape","f0e8e7b6":"words_per_comments_train = train.Comment.map(lambda x : len(x.split()))","94d02bea":"sns.distplot(words_per_comments_train)\nplt.title(\"Train: Distribution of words per comment\")\nplt.xlabel(\"Word count\")","a9047641":"min(words_per_comments_train),max(words_per_comments_train)","bfacf1d2":"sns.countplot(train.Topic)\nplt.title(\"Train: Count plot of Tpoics\")","56160a0b":"words_per_comments_test = test.Comment.map(lambda x : len(x.split()))","8f08afcf":"sns.distplot(words_per_comments_test)\nplt.title(\"Test: Distribution of words per comment\")\nplt.xlabel(\"Word count\")","3c51158d":"sns.countplot(test.Topic)","41c76bb2":"## Create the vectorizer \ntfidf = TfidfVectorizer(stop_words='english')\n\n## fit the vectorizer on train data\ntfidf.fit(train.Comment)","f7f3506f":"features = tfidf.transform(train.Comment).toarray()\nfeatures.shape","6a733a71":"label = train.Topic ","890fcb37":"clf = MultinomialNB()","18fedf14":"clf.fit(features, label)","983e4fa8":"test_actual = test.Topic","f9f99cc6":"test_features = tfidf.transform(test.Comment).toarray()\ntest_features.shape","4c2c7952":"test_prediction = clf.predict(test_features) ","b302f326":"test_score = 100*(metrics.accuracy_score(test_actual , test_prediction ))\n\nprint(test_score)","0a004b62":"## Physics vs Chemistry vs Biology\n\n#### This notebook covers basic EDA on the dataset and a baseline model using machine learning approach","0154e332":"## Set up","df8f1d3e":"> #### Note: This was the baseline score, various other machine learning\/deep learning models can be developed to achieve better accuracy ","4ad7cbd3":"### Check performence on test data","c97b9f8b":"### Make predictions on test data","183c3cce":"## EDA","61e9f7a6":"## Model developement\n\n#### Here we have developed a very simple model without any text preprocessing, just to check the baseline performance  "}}