{"cell_type":{"f6cea6c7":"code","cbf8a9e5":"code","e90f5040":"code","09ed01eb":"code","327109c3":"code","cc92ab00":"code","d92289ce":"code","3aa0366c":"code","2abe1a29":"code","a5c62d49":"code","f20fbcb7":"code","7fb61dda":"code","8c138d47":"code","fca2099d":"code","34385e7f":"code","412d5f27":"code","4a6bfd94":"code","de738199":"code","c7f983cb":"code","387a08e6":"code","63403406":"code","4059d242":"code","2b33c44a":"code","da7aa4e4":"code","d251925a":"code","af0b5b0a":"code","d6e7ddaf":"code","8b7791e6":"markdown","607133d1":"markdown","74687588":"markdown","5bd7651c":"markdown","54684151":"markdown","bad83f53":"markdown","ea4211f7":"markdown","24b88cf4":"markdown","23d5fad7":"markdown","8293d252":"markdown","bf63b622":"markdown","25ea2574":"markdown"},"source":{"f6cea6c7":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","cbf8a9e5":"#!pip install pyspark","e90f5040":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\ndf = spark.read.csv('..\/input\/bank.csv', header = True, inferSchema = True)\ndf.printSchema()","09ed01eb":"import pandas as pd\npd.DataFrame(df.take(5), columns=df.columns).transpose()","327109c3":"df.groupby('deposit').count().toPandas()","cc92ab00":"numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\ndf.select(numeric_features).describe().toPandas().transpose()","d92289ce":"numeric_data = df.select(numeric_features).toPandas()\naxs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())\n","3aa0366c":"df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\ncols = df.columns\ndf.printSchema()","2abe1a29":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n\ncategoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n\nstages = []\n\nfor categoricalCol in categoricalColumns:\n    \n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    \n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    \n    stages += [stringIndexer, encoder]\n\n\n","a5c62d49":"label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n\nstages += [label_stringIdx]\n\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n\nstages += [assembler]","f20fbcb7":"from pyspark.ml import Pipeline\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df)\ndf = pipelineModel.transform(df)\nselectedCols = ['label', 'features'] + cols\ndf = df.select(selectedCols)\ndf.printSchema()","7fb61dda":"pd.DataFrame(df.take(5), columns=df.columns).transpose()","8c138d47":"train, test = df.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","fca2099d":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","34385e7f":"import matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","412d5f27":"trainingSummary = lrModel.summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","4a6bfd94":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","de738199":"predictions = lrModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","c7f983cb":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', evaluator.evaluate(predictions))","387a08e6":"from pyspark.ml.classification import DecisionTreeClassifier\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\ndtModel = dt.fit(train)\npredictions = dtModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","63403406":"evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","4059d242":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nrfModel = rf.fit(train)\npredictions = rfModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","2b33c44a":"evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","da7aa4e4":"from pyspark.ml.classification import GBTClassifier\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\npredictions = gbtModel.transform(test)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","d251925a":"evaluator = BinaryClassificationEvaluator()\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","af0b5b0a":"print(gbt.explainParams())","d6e7ddaf":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 60])\n             .addGrid(gbt.maxIter, [10, 20])\n             .build())\ncv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\ncvModel = cv.fit(train)\npredictions = cvModel.transform(test)\nevaluator.evaluate(predictions)","8b7791e6":"**Decision Tree Classifier**\n\nDecision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions.","607133d1":"**Evaluate Logistic Regression**","74687588":"**Random Forest Classifier**","5bd7651c":"**Creating Logistic Regression Model**","54684151":"**Gradient-Boosted Tree Classifier**","bad83f53":"**Introduction**\n\nApache Spark, once a component of the Hadoop ecosystem, is now becoming the big-data platform of choice for enterprises. It is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface.\n\nLink :- https:\/\/towardsdatascience.com\/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n","ea4211f7":"**Exploring The Data**\n\nWe will use the same data set when we built a Logistic Regression in Python, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes\/No) to a term deposit. The dataset can be downloaded from Kaggle.\n\n","24b88cf4":"**Analyzing Data**","23d5fad7":"**Preparing Data for Machine Learning**\n\nThe process includes Category Indexing, One-Hot Encoding and VectorAssembler\u200a\u2014\u200aa feature transformer that merges multiple columns into a vector column.\n\nCode given below indexes each categorical column using the StringIndexer, then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row. \n\nWe use the StringIndexer again to encode our labels to label indices. \n\nNext, we use the VectorAssembler to combine all the feature columns into a single vector column.","8293d252":"**Precision and Recall**","bf63b622":"We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column.","25ea2574":"**Pipeline**\n\nWe use Pipeline to chain multiple Transformers and Estimators together to specify our machine learning workflow. A Pipeline\u2019s stages are specified as an ordered array."}}