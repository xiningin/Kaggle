{"cell_type":{"0d41d358":"code","dc9b1a4d":"code","55d78f24":"code","4a38331c":"code","89c509d6":"code","1d8b2ce4":"code","03979e92":"code","defb3acc":"code","06f000c6":"code","cad94714":"code","b28d0809":"code","28d27985":"code","55fef262":"code","f07c1fb1":"code","217ce6e5":"code","a3f33986":"code","86c0a95e":"code","991a0281":"code","d4313e6a":"code","6068e688":"code","ba6ddc0f":"code","17d9876b":"code","20d7b923":"code","5c6f4df6":"code","d60f4b48":"code","e6b5c503":"code","2ac873a1":"code","1add0a1c":"code","2dcf8433":"code","9f23b564":"code","3de6640b":"code","61762f75":"code","1f8328ca":"code","69692f76":"code","d821ed60":"code","a6e79fea":"code","3edd912a":"code","c424a0be":"code","0c6ca325":"code","1a2b9790":"code","8c4949eb":"code","ca3b90ec":"code","b61f0d10":"code","284f471f":"code","5b3f7743":"code","fcf10c6b":"code","85b3c110":"code","411399b3":"code","f6fd9782":"code","7d03e339":"code","251e1179":"code","203aeb70":"code","43a74e47":"code","be0b10d0":"code","5ce28257":"code","4aac88ea":"code","5c71ab13":"code","9bd6427e":"code","4669992f":"code","0ad72a9a":"code","878a0583":"code","1a6f3b9e":"code","826cb660":"code","11cf8979":"code","d7044352":"code","224a9a2c":"code","3454aff8":"code","9f997d05":"code","024c1d49":"code","e37a2c42":"code","8ce91c60":"code","6f51bdfa":"code","19a7d014":"code","0fa71557":"code","a5a55911":"code","fc4691a5":"code","8def90af":"code","b9bee43c":"code","e37bfaa9":"code","e4e0be6a":"code","2a102e5a":"code","3f82446c":"code","48239d2d":"code","3e6966fc":"code","9111a785":"code","406ef1df":"markdown","d2fdb884":"markdown","ed07a178":"markdown","0ed167cb":"markdown","679b4291":"markdown","82f39f95":"markdown","04a78166":"markdown","f7bdec00":"markdown","005e0099":"markdown","2ce3d83c":"markdown","579543d7":"markdown","fd27b6e2":"markdown","eb904bc6":"markdown","239909b9":"markdown","cbee150d":"markdown","30dfccbe":"markdown","bb5881a4":"markdown","40298a42":"markdown","1947987e":"markdown","6c5d9074":"markdown","a9f80e46":"markdown","7397ac45":"markdown","28d91c7f":"markdown","e7132dcc":"markdown","dc08b22b":"markdown"},"source":{"0d41d358":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc9b1a4d":"%config Completer.use_jedi = False","55d78f24":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n#mterics \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import roc_auc_score\n\nimport string\nimport nltk\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nimport gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\n\nimport os\nimport pickle \nfrom tqdm import tqdm\n","4a38331c":"#Using the sqlite table to read data\ncon = sqlite3.connect('\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite')\n# Filtering only Positive and Negative Reviews i.e not taking into consideration those reviews with score=3\nfiltered_data= pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3\"\"\",con)\n#Given reviews with score>3 a postive rating \"1\" and reviews with score <3 a negative \"0\"\n#constucting a function\ndef partition(x):\n    if x<3:\n        return 0\n    return 1\n#Changing reviews with score<3 to be negative \"0\" and score > 3 to be positive \"1\"\nactualscore=filtered_data['Score']\nPositve_Negative=actualscore.map(partition)\nfiltered_data['Score']=Positve_Negative\nprint(\"No. of data points in Dataset:\",filtered_data.shape)\nfiltered_data.head(3)\n","89c509d6":"display= pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3 AND UserId ='AR5J8UI46CURR' ORDER BY ProductId \"\"\",con)\ndisplay","1d8b2ce4":"#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values(\"ProductId\",axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last')","03979e92":"#Deduplication entries\nfinal_data=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\nfinal_data.shape","defb3acc":"#Checking to see how much % of data still remains\nprint('The total data remain after cleaning data ',(final_data['Id'].size*1.0)\/(filtered_data['Id'].size*1.0)*100)","06f000c6":"final_data[final_data['HelpfulnessNumerator']>final_data['HelpfulnessDenominator']]","cad94714":"display=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3 AND Id=44737 OR Id=64422\nORDER BY ProductID\"\"\",con)\ndisplay.head()","b28d0809":"final_data=final_data[final_data['HelpfulnessNumerator']<=final_data['HelpfulnessDenominator']]\nfinal_data.shape","28d27985":"# before starting the next phase of text preprocessing lets see the no. of entries left\nprint(final_data.shape)\n#How many positive and negative reviews are present in our dataset?\nfinal_data['Score'].value_counts()\nsns.countplot(final_data['Score'])\nplt.show()\nsns.countplot(data=final_data,y='Score')","55fef262":"#finding HTML tags in sentences\nimport re\ni=0\nfor review in final_data['Text'].values:\n    if (len(re.findall('<.*?>',review))):\n        print(review)\n        break\n    i=i+1\nprint(i)","f07c1fb1":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop = set(stopwords.words('english')) # creating set of stopwords\nprint(stop) # it will to show all the stopwords in NLTK\nexcluding_stop = ['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\nprint('****'*15)\n\nstop = [word for word in stop if word not in excluding_stop]\nprint(' ')\nprint(stop)\nsnowstem=nltk.stem.SnowballStemmer('english') # intialising the snowball stemmer\nprint(' ')\nprint('****'*15)\nprint(\"base_word of tasty:\", snowstem.stem('tasty')) # it will tell us the base word or do stemming\n\n# creating a function to clean the word of any html-tags. The function will remove the html tag and evrything between them with \"1\" space \ndef cleanhtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\n# creating a function to clean the punctuation or special characters .The function will create the punctuation with empty string\ndef cleanpunc(sentence):\n    cleaned= re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned= re.sub(r'[.|,|)|(|\\|\/]',r'',cleaned)\n    return cleaned\n ","217ce6e5":"# printing some random reviews\nsent_0 = final_data['Text'].values[6]\nprint(sent_0)\nprint(\"=\"*50)\n\nsent_1000 = final_data['Text'].values[1000]\nprint(sent_1000)\nprint(\"=\"*50)\n\nsent_1500 = final_data['Text'].values[1500]\nprint(sent_1500)\nprint(\"=\"*50)\n\nsent_4900 = final_data['Text'].values[4900]\nprint(sent_4900)\nprint(\"=\"*50)","a3f33986":"import re    #Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\n# remove urls from text python : *https:\/\/stackoverflow.com\/a\/40823105\/4084039*\nsent_0=re.sub(r'https\\S+','',sent_0)\nsent_1000=re.sub(r'https\\S+','',sent_1000)\nsent_1500=re.sub(r'https\\S+','',sent_1500)\nsent_4900=re.sub(r'http\\S+', '', sent_4900)\n\nprint(sent_0)","86c0a95e":"pip install beautifulsoup4","991a0281":"pip install lxml","d4313e6a":"from bs4 import BeautifulSoup\nsoup=BeautifulSoup(sent_0,'lxml')\ntext=soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup=BeautifulSoup(sent_1000,'lxml')\ntext=soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup=BeautifulSoup(sent_1500,'lxml')\ntext=soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup=BeautifulSoup(sent_4900,'lxml')\ntext=soup.get_text()\nprint(text)\nprint(\"=\"*50)\n","6068e688":"import os\nimport re\ndef decontracted(phrase):\n    phrase=re.sub(r\"won't\",'will not',phrase)\n    phrase=re.sub(r\"can\\'t\",'can not',phrase)\n    phrase=re.sub(r\"n\\'t\",'not',phrase)\n    phrase=re.sub(r\"\\'re\",'are',phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","ba6ddc0f":"#rremove words with no. python  # https:\/\/stackoverflow.com\/a\/18082370\/4084039\nsent_0=re.sub(\"\\S\\d\\S\",\"\",sent_0).strip()\nprint(sent_0)\nprint(\"=\"*50)\n\n#remove specail charachter # https:\/\/stackoverflow.com\/a\/5843547\/4084039\nsent_1500=re.sub('[^A-Za-z0-9]+',' ',sent_1500)\nprint(sent_1500)\n","17d9876b":"#Combining all the statement above\nfrom tqdm import tqdm\npreprocessed_review=[]\nfor sentence in tqdm(final_data['Text'].values):\n    sentence=re.sub(r\"http\\S+\", \"\",sentence)\n    sentence=BeautifulSoup(sentence,'lxml').get_text()\n    sentence=decontracted(sentence)\n    sentence=re.sub(\"\\S*\\d\\S*\",\"\",sentence).strip()\n    sentence=re.sub('[^A-Za-z]+', ' ', sentence)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentence=' '.join(e.lower() for e in sentence.split() if e.lower() not in stop)\n    preprocessed_review.append(sentence.strip())\n    ","20d7b923":"from tqdm import tqdm\npreprocessed_summary=[]\nfor sentence in tqdm(final_data['Summary'].values):\n    sentence=re.sub(r\"http\\S+\", \"\",sentence)\n    sentence=BeautifulSoup(sentence,'lxml').get_text()\n    sentence=decontracted(sentence)\n    sentence=re.sub(\"\\S*\\d\\S*\",\"\",sentence).strip()\n    sentence=re.sub('[^A-Za-z]+', ' ', sentence)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentence=' '.join(e.lower() for e in sentence.split() if e.lower() not in stop)\n    preprocessed_summary.append(sentence.strip())\n    ","5c6f4df6":"pip install bs4","d60f4b48":"#SENTENCES Containing HTML tags\nimport re\ni=0;\nfor sentence in final_data['Text'].values:\n    if (len(re.findall('<.*?>',sentence))):\n        break;\n    print(\"=\"*50)\n    print(i,\")>>\",sentence)\n    i=i+1\nprint(\"=\"*50)","e6b5c503":"#adding a column of CleanedText which displays the data after pre-processing of the review\nfinal_data['Cleaned Text']=preprocessed_review\nfinal_data['Cleaned_Summary']=preprocessed_summary\n\n#print(final_data[['Text','Clean_text']])\n#final_data['Cleaned Text']=final_data['Cleaned Text'].str.decode('UTF-8')","2ac873a1":"# converting time in unit=sec\nfinal_data['Time']=pd.to_datetime(final_data['Time'],unit='s')\nfinal_data.head()","1add0a1c":"final_data=final_data.sort_values('Time',axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\nfinal_data.head()","2dcf8433":"#Store the final table into an SQLITE table for future\nconnection=sqlite3.connect('final12.sqlite')\nc=connection.cursor\nconnection.text_factory=str\nfinal_data.to_sql('Reviews',connection,schema=None,if_exists='replace',index=True,index_label=True,chunksize=None,dtype=None)\nconnection.close()","9f23b564":"# taking equal no. of negative and positive data points\ndata_pos=final_data[final_data['Score']==1].sample(n=60000)\ndata_neg=final_data[final_data['Score']==0].sample(n=57000)\nfinal=pd.concat([data_pos,data_neg])\nfinal.tail()","3de6640b":"y= final['Score']\nX = final['Cleaned Text']\nprint(\"Shape of X\",X.shape)\nprint(\"Shape of y\",y.shape)","61762f75":"from sklearn.model_selection import train_test_split\nX_tr ,X_test,y_tr,y_test = train_test_split(X,y,test_size=.30,random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(X_tr, y_tr, test_size=.30, random_state=0)\nprint('Shape of X_train is :',X_train.shape)\nprint('Shape of y_train is :',y_train.shape)\nprint(\"****\"*6)\nprint('Shape of X_Cv is :',X_cv.shape)\nprint('Shape of y_cv is :',y_cv.shape)\nprint(\"****\"*6)\nprint('Shape of X_test is :',X_test.shape)\nprint('Shape of y_test is :',y_test.shape)\nprint(\"****\"*6)\nprint('Shape of X_tr is :',X_tr.shape)\nprint('Shape of y_tr is :',y_tr.shape)","1f8328ca":"from sklearn.feature_extraction.text import CountVectorizer\n# *Converting text into vectors using BOW\nbow_count_vect=CountVectorizer()\nbow_count_vect.fit(X_train)\n# Using the fited CountVectorizer to convert text to vectors\nXtrain_bow=bow_count_vect.transform(X_train)\nXcv_bow=bow_count_vect.transform(X_cv)\nXtest_bow=bow_count_vect.transform(X_test)\nprint(\"After vectorizations\")\nprint(Xtrain_bow.shape, y_train.shape)\nprint(Xcv_bow.shape, y_cv.shape)\nprint(Xtest_bow.shape, y_test.shape)\n#print(X_tr_bow.shape, y_tr.shape)","69692f76":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler(with_mean=False)\nscaler.fit(Xtrain_bow)\nX_tr_bow=scaler.transform(Xtrain_bow)\nX_test_bow=scaler.transform(Xtest_bow)\nX_cv_bow=scaler.transform(Xcv_bow)\nprint(\"After standardising\")\nprint(X_tr_bow.shape, y_train.shape)\nprint(X_cv_bow.shape, y_cv.shape)\nprint(X_test_bow.shape, y_test.shape)\n#print(X_tr_bow.shape, y_tr.shape)","d821ed60":"import math\nfrom sklearn.model_selection import cross_val_score\nC =[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]#C=1\/lambda\nauc_train=[]\nauc_cv=[]\nfor c in C:\n    model_logreg=LogisticRegression(penalty='l2',C=c)\n    model_logreg.fit(X_tr_bow,y_train)\n    \n    y_pred_train=model_logreg.predict_proba(X_tr_bow)[:,1]\n    auc_train.append(roc_auc_score(y_train,y_pred_train))\n    \n    y_pred_cv=model_logreg.predict_proba(X_cv_bow)[:,1]\n    auc_cv.append(roc_auc_score(y_cv,y_pred_cv))\n    \n    auc_score_cv =roc_auc_score(y_cv,y_pred_cv)\n    print(c,\"-------->\",auc_score_cv )\n    \noptimal_C=C[auc_cv.index(max(auc_cv))]\nc=[math.log(x) for x in C]\nprint('Optimal C  is :',optimal_C)\nfig =plt.figure()\nplt.plot(c,auc_train,color='green',label='AUC train')\nplt.plot(c,auc_cv,color='red',label='AUC test')\nplt.legend(loc='best')\nplt.xlabel('log(alpha')\nplt.ylabel('AUC')\nplt.show()","a6e79fea":"#Finding optimal_alpha by using 10-fold_cv\nfrom sklearn.model_selection import cross_val_score\nalpha_C=[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]\ncv_scores=[]\n#perfrom 10-fold cv\nfor i in tqdm(alpha_C):\n    model_logreg=LogisticRegression(penalty='l2',C=i)\n    scores=cross_val_score(model_logreg,X_tr_bow,y_train,cv=10)\n    cv_scores.append(scores.mean())\n#Changing to misclassification error\nMSE=[1 - x for x in cv_scores]\n\n#determing best alpha\noptimal_C=alpha_C[MSE.index(min(MSE))]\naplha_C=[math.log(x) for x in alpha_C]\nprint('\\nThe optimal number of alpha is ',optimal_C)\n#Plot the misclassfication error vs alpha\nplt.plot(aplha_C,MSE,marker='*')\nplt.title(\"Misclassification Error vs alpha\")\nplt.xlabel('value of alpha')\nplt.ylabel('Misclassification Error')\nplt.show()","3edd912a":"optimal_C","c424a0be":"#ROC for labmda=\nlr=LogisticRegression(penalty='l2',C=optimal_C)\nlr.fit(X_tr_bow,y_train)\npredi=lr.predict_proba(X_test_bow)[:,1]\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, predi)\npred=lr.predict_proba(X_tr_bow)[:,1]\nfpr2,tpr2,thresholds2=metrics.roc_curve(y_train,pred)\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,predi)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","0c6ca325":"#Confusion matrix using heat map for test data\nfrom sklearn.metrics import  confusion_matrix\nlogreg=LogisticRegression(penalty='l2',C=optimal_C)\nlogreg.fit(X_tr_bow,y_train)\npred=logreg.predict(X_test_bow)\nimport seaborn as sns\nconf_mat=confusion_matrix(y_test,pred)\nclass_label = [\"negative\", \"positive\"]\ndf=pd.DataFrame(conf_mat,columns=class_label,index=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","1a2b9790":"Weights_before=logreg.coef_\n\nX_e=X_tr_bow\nprint('Shape of X_e before adding noise ',X_e.shape)\nX_e.data=X_e.data+np.random.normal(loc=0,scale=0.0001,size=X_e.data.shape)\nprint('Shape of X_e.data',X_e.data.shape)\nprint('Shape of X_e after adding noise',X_e.shape)","8c4949eb":"#Training Logistic regression With X_e\nlr_e=LogisticRegression(penalty='l2',C=optimal_C)\nlr_e.fit(X_e,y_train)\nW_after=lr_e.coef_","ca3b90ec":"#list(Weights_before[0])","b61f0d10":"#to eliminate divisible by zero error we will add 10^-6 to W_before and W_after\nWeights_before+=10**-6\nW_after+=10**-6\n\nper_vectors=[]\n\nprint(len(Weights_before[0]))\n\nfor i in range(len(Weights_before[0])):\n    val=W_after[0][i]-Weights_before[0][i]\n    val\/=Weights_before[0][i]\n    per_vectors.append(val)\n    \nOriginal_per_vect=np.absolute(per_vectors)\nper_vectors=sorted(np.absolute(per_vectors))[::-1]\n#percentage change iin vectors \nper_vectors[:10]\n    ","284f471f":"type(per_vectors)","5b3f7743":"#calculating percentile from 0 to 100:\nfor i in range(11): #https:\/\/www.geeksforgeeks.org\/numpy-percentile-in-python\/ \n    print(str(i*10)+'th percentile = '+str(np.percentile(per_vectors,i*10)))","fcf10c6b":"#there is sudden rise in percentile from 90 to 100\n#calculating percentile from 9 to 100:\n\nfor i in range(90,101): #https:\/\/www.geeksforgeeks.org\/numpy-percentile-in-python\/ \n    print(str(i)+'th percentile = '+str(np.percentile(per_vectors,i)))","85b3c110":"#there is sudden rise in percentile from 99 to 100\n#calculating percentile from 99 to 100:\n\nfor i in range(1,11): #https:\/\/www.geeksforgeeks.org\/numpy-percentile-in-python\/ \n    print(str(99+(10**-1)*i)+'th percentile = '+str(np.percentile(per_vectors,99+(10**-1)*i)))","411399b3":"#finding features from 99.9th percentile to 100th percentile\nprint('features from 99.9th percentile to 100th percentile')\noriginal_per_vect=Original_per_vect.tolist()\nall_features=bow_count_vect.get_feature_names()\n#for i in range(1,11): #https:\/\/www.geeksforgeeks.org\/numpy-percentile-in-python\/ \n    #index=original_per_vect.index(np.percentile(per_vectors,99.9+(10**-2)*i))\n    #print(all_features[index])\n#indx=original_per_vect.index(index)\n#print(all_features[indx])","f6fd9782":"weight=logreg.coef_\n#x = np.array([3, 1, 2])\n#np.argsort(x)\n#array([1, 2, 0])\npos_index=np.argsort(weight)[:,::-1]\nneg_index=np.argsort(weight)\n","7d03e339":"### Top 10 important features of positive class from\nprint('Top 10 important features of positive class ')\nfor i in list(pos_index[0][:10]):\n    print(all_features[i])","251e1179":"### Top 10 important features of positive class from\nprint('Top 10 important features of neagtive class ')\nfor i in list(neg_index[0][:10]):\n    print(all_features[i])","203aeb70":"from sklearn.feature_extraction.text import TfidfVectorizer\n# Conveting text into vectors using Tf_idf\n\ntf_idf_vect=TfidfVectorizer(ngram_range=(1,2))  #taking both unigram and bigram\ntf_idf_vect.fit(X_train)\n# Using the transrom fucntion TfidfVectorizer  convert text to vectors\nXtrain_tf_idf=tf_idf_vect.transform(X_train)\nXcv_tf_idf   =tf_idf_vect.transform(X_cv)\nXtest_tf_idf =tf_idf_vect.transform(X_test)\nprint(\"After vectorizations\")\nprint(Xtrain_tf_idf.shape, y_train.shape)\nprint(Xcv_tf_idf.shape, y_cv.shape)\nprint(Xtest_tf_idf.shape, y_test.shape)","43a74e47":"import math\nfrom sklearn.model_selection import  cross_val_score\nC =[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]#C=1\/lambda\nauc_train=[]\nauc_cv=[]\nfor c in C:\n    model_logreg=LogisticRegression(penalty='l2',C=c)\n    model_logreg.fit(Xtrain_tf_idf,y_train)\n    \n    y_pred_train=model_logreg.predict_proba(Xtrain_tf_idf)[:,1]  #[:,1] its used for saving ony positve class\n    auc_train.append(roc_auc_score(y_train,y_pred_train))\n    \n    y_pred_cv=model_logreg.predict_proba(Xcv_tf_idf)[:,1]\n    auc_cv.append(roc_auc_score(y_cv,y_pred_cv))\n    \n    auc_score_cv=roc_auc_score(y_cv,y_pred_cv)\n    print(c,\"-------->\",auc_score_cv )\n\noptimal_c=C[auc_cv.index(max(auc_cv))]\nc=[math.log(x) for x in C]\nprint(\"Optimal c:\",optimal_c)\nfig=plt.figure()\nplt.plot(c,auc_train,color='green',label='Auc_Train')\nplt.plot(c,auc_cv,color='red',label='Auc_Test')\nplt.legend(loc='best')\nplt.xlabel('log(alpha')\nplt.ylabel('AUC')\nplt.show()\n","be0b10d0":"#Finding optimal_alpha by using 10-fold_cv\nfrom sklearn.model_selection import cross_val_score\nalpha_C=[0.00001,0.0001,0.001,0.01,0.1,1,2.1,10,100,1000,10000,100000]\ncv_score_tfidf=[]\n#perform 10-foldcv\nfor i in tqdm(alpha_C):\n    model_logreg_tfidf=LogisticRegression(penalty='l2',C=i)\n    score_tfidf=cross_val_score(model_logreg_tfidf,Xtrain_tf_idf,y_train,cv=10)\n    cv_score_tfidf.append(score_tfidf.mean())\n    \n#Changing to misclassification error\nMSE=[1-x for x in cv_score_tfidf]\n\n#determine  best alpha\noptimal_C_tfidf=alpha_C[MSE.index(min(MSE))]\nprint(\"Optimal c:\",optimal_C_tfidf)\nalpha_C=[math.log(x) for x in alpha_C]\n#Plot the misclassfication error vs alpha\nplt.plot(alpha_C,MSE,marker='*')\nplt.title(\"Misclassification Error vs alpha\")\nplt.xlabel('value of alpha')\nplt.ylabel('Misclassification Error')\nplt.show()","5ce28257":"# Roc for allpha =1000\nmodel=LogisticRegression(penalty='l2',C=optimal_C_tfidf)\nmodel.fit(Xtrain_tf_idf,y_train)\n\ny_test_pred=model.predict_proba(Xtest_tf_idf)[:,1]\nfpr1,tpr1,thresholds1=metrics.roc_curve(y_test,y_test_pred)\n\ny_train_pred=model.predict_proba(Xtrain_tf_idf)[:,1]\nfpr2,tpr2,thresholds2=metrics.roc_curve(y_train,y_train_pred)\n\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(fpr1,tpr1,label=\"Test Roc,auc=\"+str(roc_auc_score(y_test,y_test_pred)))\nax.plot(fpr2,tpr2,label=\"Train Roc,Auc=\"+str(roc_auc_score(y_train,y_train_pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()\n","4aac88ea":"#Confusion matrix using heat map for test data\nfrom sklearn.metrics import  confusion_matrix\nlr=LogisticRegression(penalty='l2',C=optimal_C_tfidf)\nlr.fit(Xtrain_tf_idf,y_train)\npred=lr.predict(Xtest_tf_idf)\n\nimport seaborn as sns\ncon_mat=confusion_matrix(y_test,pred)\nclass_label=['negative','positive']\ndf=pd.DataFrame(con_mat,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","5c71ab13":"all_features = tf_idf_vect.get_feature_names()\nweight=lr.coef_\n#x = np.array([3, 1, 2])\n#np.argsort(x)\n#output >> array([1, 2, 0])\npos_index=np.argsort(weight)[:,::-1]\nneg_index=np.argsort(weight)","9bd6427e":"### Top 10 important features of positive class from\nprint('Top 10 important features of positive class ')\nfor i in list(pos_index[0][:10]):\n    print(all_features[i])","4669992f":"#Top 10 important features of negative class -=\nprint(\"top 10 negative features:\")\nfor i in list(neg_index[0][0:10]):\n    print(all_features[i])","0ad72a9a":"i=0\nlist_of_Xtr=[]\nfor sent in X_train:\n    list_of_Xtr.append(sent.split())\nprint(\"len of list_of_Xtr :\",len(list_of_Xtr))\n\ni=0\nlist_of_Xtest=[]\nfor sent in X_test:\n    list_of_Xtest.append(sent.split())\nprint(\"len of list_of_Xtest :\",len(list_of_Xtest))\n\ni=0\nlist_of_Xcv=[]\nfor sent in X_cv:\n    list_of_Xcv.append(sent.split())\nprint(\"len of list_of_Xcv :\",len(list_of_Xcv))\n","878a0583":"# To Train the word2vec model using train data\nw2v_model=Word2Vec(list_of_Xtr,min_count=2,size=50,workers=4)\n\n#min_count :means if a word doesnt occur atleast 5 times don't create word2vec\n# vector_size :is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n#workers : the last of the major parameters (full list here) is for training parallelization, to speed up training:","1a6f3b9e":"w2v_words=list(w2v_model.wv.vocab)\nprint(\"number of words that occured minimum 2 times \",len(w2v_words))\nprint('='*50)\nprint(\"sample words \", w2v_words[0:50])","826cb660":"print(w2v_model.wv['great'])\nprint('='*50)\nprint(len(w2v_model.wv['great']))","11cf8979":"print(w2v_model.wv.most_similar('great'))\nprint('='*50)\nprint(w2v_model.wv.most_similar('worst'))","d7044352":"#Avg W2Vec on train data \n#computing average word2vec for each reviews\nsent_vectors_train=[]\nfor sent in tqdm(list_of_Xtr):\n    sent_vec=np.zeros(50)\n    count_words=0\n    for word in sent:\n        if word in w2v_words:\n            vec=w2v_model.wv[word]\n            sent_vec+=vec\n            count_words+=1\n    if count_words!=0:\n        sent_vec\/=count_words\n    sent_vectors_train.append(sent_vec)\nprint(len(sent_vectors_train))\nprint(len(sent_vectors_train[0]))\n","224a9a2c":"#Avg W2Vec on test data \n#computing average word2vec for each reviews\nsent_vectors_test=[]\nfor sent in tqdm(list_of_Xtest):\n    sent_vec=np.zeros(50)\n    count_words=0\n    for word in sent:\n        if word in w2v_words:\n            vec=w2v_model.wv[word]\n            sent_vec+=vec\n            count_words+=1\n    if count_words!=0:\n        sent_vec\/=count_words\n    sent_vectors_test.append(sent_vec)\nprint(len(sent_vectors_test))\nprint(len(sent_vectors_test[0]))","3454aff8":"#Avg W2Vec on cv data \n#computing average word2vec for each reviews\nsent_vectors_cv=[]\nfor sent in tqdm(list_of_Xcv):\n    sent_vec=np.zeros(50)\n    count_words=0\n    for word in sent:\n        if word in w2v_words:\n            vec=w2v_model.wv[word]\n            sent_vec+=vec\n            count_words+=1\n    if count_words!=0:\n        sent_vec\/=count_words\n    sent_vectors_cv.append(sent_vec)\nprint(len(sent_vectors_cv))\nprint(len(sent_vectors_cv[0]))","9f997d05":"x_train_avg=sent_vectors_train\nx_test_avg=sent_vectors_test\nx_cv_avg=sent_vectors_cv\n\nscaler =StandardScaler(with_mean=False)\nscaler.fit(x_train_avg)\n\nx_train_avg=scaler.transform(x_train_avg)\nx_test_avg=scaler.transform(x_test_avg)\nx_cv_avg=scaler.transform(x_cv_avg)\n\nprint(len(x_train_avg))\nprint(len(x_train_avg[0]))\n\nprint(len(x_test_avg))\nprint(len(x_test_avg[0]))\n\nprint(len(x_cv_avg))\nprint(len(x_cv_avg[0]))","024c1d49":"#Finding optimal_alpha by using 10-fold_cv\nfrom sklearn.model_selection import cross_val_score\nalpha_C=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000,100000]\ncv_score_avg_w2v=[]\n#perform 10-foldcv\nfor i in tqdm(alpha_C):\n    lr=LogisticRegression(penalty='l2',C=i)\n    score_avg_W2v=cross_val_score(lr,x_train_avg,y_train,cv=10)\n    cv_score_avg_w2v.append(score_avg_W2v.mean())\n    \n#Changing to misclassification error\nMSE=[1-x for x in cv_score_avg_w2v]\n\n#determine  best alpha\noptimal_C_avg_W2v=alpha_C[MSE.index(min(MSE))]\nprint(\"Optimal c:\",optimal_C_avg_W2v)\nalpha_C=[math.log(x) for x in alpha_C]\n#Plot the misclassfication error vs alpha\nplt.plot(alpha_C,MSE,marker='*')\nplt.title(\"Misclassification Error vs alpha\")\nplt.xlabel('value of alpha')\nplt.ylabel('Misclassification Error')\nplt.show()","e37a2c42":"# Roc \nlr =LogisticRegression(penalty='l2',C=optimal_C_avg_W2v)\nlr.fit(x_train_avg,y_train)\n\ny_test_pred=lr.predict_proba(x_test_avg)[:,1]\nfpr1,tpr1,thresholds1=metrics.roc_curve(y_test,y_test_pred)\n\ny_train_pred=lr.predict_proba(x_train_avg)[:,1]\nfpr2,tpr2,thresholds2=metrics.roc_curve(y_train,y_train_pred)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_test_pred)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_train_pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","8ce91c60":"#Confusion matrix using heatmap for test data\nfrom sklearn.metrics import confusion_matrix\nlr=LogisticRegression(penalty='l2',C=optimal_C_avg_W2v)\nlr.fit(x_train_avg,y_train)\ny_predict=lr.predict(x_test_avg)\n\ncon_matrix=confusion_matrix(y_test,y_predict)\nclass_label=[\"negative\", \"positive\"]\ndf=pd.DataFrame(con_matrix,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","6f51bdfa":"from sklearn.model_selection import train_test_split\nX_tr ,X_test,y_tr,y_test = train_test_split(X,y,test_size=.30,random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(X_tr, y_tr, test_size=.30, random_state=0)\nprint('Shape of X_train is :',X_train.shape)\nprint('Shape of y_train is :',y_train.shape)\nprint(\"****\"*6)\nprint('Shape of X_Cv is :',X_cv.shape)\nprint('Shape of y_cv is :',y_cv.shape)\nprint(\"****\"*6)\nprint('Shape of X_test is :',X_test.shape)\nprint('Shape of y_test is :',y_test.shape)\nprint(\"****\"*6)\nprint('Shape of X_tr is :',X_tr.shape)\nprint('Shape of y_tr is :',y_tr.shape)","19a7d014":"i=0\nlist_of_Xtr=[]\nfor sent in X_train:\n    list_of_Xtr.append(sent.split())\nprint(\"len of list_of_Xtr :\",len(list_of_Xtr))\n\ni=0\nlist_of_Xtest=[]\nfor sent in X_test:\n    list_of_Xtest.append(sent.split())\nprint(\"len of list_of_Xtest :\",len(list_of_Xtest))\n\ni=0\nlist_of_Xcv=[]\nfor sent in X_cv:\n    list_of_Xcv.append(sent.split())\nprint(\"len of list_of_Xcv :\",len(list_of_Xcv))\n","0fa71557":"# To Train the word2vec model\nw2v_model=gensim.models.Word2Vec(list_of_Xtr,min_count=5,size=50,workers=4)\n#min_count :means if a word doesnt occur atleast 5 times don't create word2vec\n# vector_size :is the number of dimensions (N) of the N-dimensional space that gensim Word2Vec maps the words onto.\n#workers : the last of the major parameters (full list here) is for training parallelization, to speed up training:\nw2vwords=list(w2v_model.wv.vocab)","a5a55911":"tf_idf_model=TfidfVectorizer()\ntf_idf_model.fit_transform(X_train)\n#creating a dictinory with a word  as a key and the idf as value\ndictionary=dict(zip(tf_idf_model.get_feature_names(),list(tf_idf_model.idf_)))\n\ndictionary","fc4691a5":"tfidf_feat=tf_idf_model.get_feature_names()\n\ntf_idf_sent_vect_train=[]\nrow=0\nfor sent in tqdm(list_of_Xtr):\n    sent_vec=np.zeros(50)\n    weight_sum=0\n    for word in sent:\n        if word in w2vwords and word in tfidf_feat:\n            vec=w2v_model.wv[word]\n            #tf_idf=final_tf_idf[row,tfidf_feat.index(word)]\n            # to reduce the computation we are computing \n            #dictionary[word]:idf value of the word in whole corpus \n            #dictionary['aberfoyl get'] output:>>11.263449588064944\n            #sent.count(word):tfvalues of word in this reviews\n            tf_idf=dictionary[word]*(sent.count(word)\/len(sent))\n            #sent.count(word) gives us the count of word in sentence and\n            #then we're dividing this term by total number of words in sent that gives us tf value.\n            sent_vec+=(vec*tf_idf)\n            weight_sum+=tf_idf\n    if weight_sum!=0:\n        sent_vec\/=weight_sum\n    tf_idf_sent_vect_train.append(sent_vec)\n    row+=1\n            ","8def90af":"print(len(tf_idf_sent_vect_train))\nprint(len(tf_idf_sent_vect_train[0]))","b9bee43c":"tfidf_feat=tf_idf_model.get_feature_names()\n\ntf_idf_sent_vect_test=[]\nrow=0\nfor sent in tqdm(list_of_Xtest):\n    sent_vec=np.zeros(50)\n    weight_sum=0\n    for word in sent:\n        if word in w2vwords and word in tfidf_feat:\n            vec=w2v_model.wv[word]\n            #tf_idf=final_tf_idf[row,tfidf_feat.index(word)]\n            # to reduce the computation we are computing \n            #dictionary[word]:idf value of the word in whole corpus \n            #dictionary['aberfoyl get'] output:>>11.263449588064944\n            #sent.count(word):tfvalues of word in this reviews\n            tf_idf=dictionary[word]*(sent.count(word)\/len(sent))\n            #sent.count(word) gives us the count of word in sentence and\n            #then we're dividing this term by total number of words in sent that gives us tf value.\n            sent_vec+=(vec*tf_idf)\n            weight_sum+=tf_idf\n    if weight_sum!=0:\n        sent_vec\/=weight_sum\n    tf_idf_sent_vect_test.append(sent_vec)\n    row+=1\n            ","e37bfaa9":"print(len(tf_idf_sent_vect_test))\nprint(len(tf_idf_sent_vect_test[0]))","e4e0be6a":"tf_idf_sent_vect_cv=[]\nrow=0\nfor sent in tqdm(list_of_Xcv):\n    sent_vec=np.zeros(50)\n    weight_sum=0\n    for word in sent:\n        if word in w2vwords and word in tfidf_feat:\n            vec=w2v_model.wv[word]\n            #tf_idf=final_tf_idf[row,tfidf_feat.index(word)]\n            # to reduce the computation we are computing \n            #dictionary[word]:idf value of the word in whole corpus \n            #dictionary['aberfoyl get'] output:>>11.263449588064944\n            #sent.count(word):tfvalues of word in this reviews\n            tf_idf=dictionary[word]*(sent.count(word)\/len(sent))\n            #sent.count(word) gives us the count of word in sentence and\n            #then we're dividing this term by total number of words in sent that gives us tf value.\n            sent_vec+=(vec*tf_idf)\n            weight_sum+=tf_idf\n    if weight_sum!=0:\n        sent_vec\/=weight_sum\n    tf_idf_sent_vect_cv.append(sent_vec)\n    row+=1","2a102e5a":"print(len(tf_idf_sent_vect_cv))\nprint(len(tf_idf_sent_vect_cv[0]))","3f82446c":"X_train_tf_df_w2vec=tf_idf_sent_vect_train\nX_test_tf_df_w2vec=tf_idf_sent_vect_test\nX_cv_tf_df_w2vec=tf_idf_sent_vect_cv\n\nC_ = [10**-3, 10**-2, 10**0, 10**2,10**3,10**4]#C=1\/lambda\nauc_train=[]\nauc_cv=[]\nfor c in C_:\n    lr=LogisticRegression(penalty='l2',C=c)\n    lr.fit(X_train_tf_df_w2vec,y_train)\n    \n    y_pred_cv=lr.predict_proba(X_cv_tf_df_w2vec)[:,1]\n    auc_cv.append(roc_auc_score(y_cv,y_pred_cv))\n    \n    y_pred_train=lr.predict_proba(X_train_tf_df_w2vec)[:,1]\n    auc_train.append(roc_auc_score(y_train,y_pred_train))\n    \n    auc_score_cv=roc_auc_score(y_cv,y_pred_cv)\n    print(c,\"-------->\",auc_score_cv )\n\noptimal_c_tfidf_w2v=C_[auc_cv.index(max(auc_cv))]\nc=[math.log(x) for x in C_]\nprint(\"Optimal c:\",optimal_c_tfidf_w2v)\nfig=plt.figure()\nplt.plot(c,auc_train,color='green',label='Auc_Train')\nplt.plot(c,auc_cv,color='red',label='Auc_Test')\nplt.legend(loc='best')\nplt.xlabel('log(alpha')\nplt.ylabel('AUC')\nplt.show()\n    ","48239d2d":"#Roc \nlr=LogisticRegression(penalty='l2',C=optimal_c_tfidf_w2v)\nlr.fit(X_train_tf_df_w2vec,y_train)\n    \ny_pred_test=lr.predict_proba(X_test_tf_df_w2vec)[:,1]\nfpr1,tpr1,thresholds1=metrics.roc_curve(y_test,y_pred_test)\n\ny_pred_train=lr.predict_proba(X_train_tf_df_w2vec)[:,1]\nfpr2,tpr2,thresholds2=metrics.roc_curve(y_train,y_pred_train)\n\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_pred_test)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_pred_train)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","3e6966fc":"#Confusion matrix using Heat map for test data\n#Confusion matrix using heatmap for test data\nfrom sklearn.metrics import confusion_matrix\nlr=LogisticRegression(penalty='l2',C=optimal_c_tfidf_w2v)\nlr.fit(X_train_tf_df_w2vec,y_train)\ny_predic=lr.predict(X_test_tf_df_w2vec)\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns    \nconf_matrix=confusion_matrix(y_test,y_predic)\nclass_label=['negative','postive']\ndf=pd.DataFrame(conf_matrix,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","9111a785":"# Please compare all your models using Prettytable library\nfrom prettytable import PrettyTable    \nx = PrettyTable()\nx.field_names = [\"Vectorizer\",\"Regularization\", \"Feature engineering\", \"Hyperameter\", \"AUC\"]\nx.add_row([\"BOW\",\"l2\",\"Not featured\",0.0001,0.9398])\nx.add_row([\"TFIDF\",\"l2\",\"Not featured\",10000,0.9688])\nx.add_row([\"Avg W2v\",\"l2\",\"Not featured\",10,0.9184])\nx.add_row([\"TFIDF Avg W2v\",\"l2\",\"Not featured\",1,0.8971])\n\nprint(x)","406ef1df":"# Exploratory Data Analysis\n\n### Data Deduplication{The most important part of cleaning data}\nIt was observes that(as shown in table below) that the reviews data had many deduplicates entries .Hence it is neccessary to remove the deduplication in order to get unbaised results for the Analysis of the data.\nFollowing is the example given below","d2fdb884":"# Apply Logistic Regression on BOW","ed07a178":"# Word2Vec","0ed167cb":"# 1. Apply Logistic Regression with L2 regularization on Tfidf","679b4291":"# Importing important library","82f39f95":"YOU SHOULD NOT DO LIKE THIS\n1. THE VOCABULARY SHOULD BUILT ONLY WITH THE WORDS OF TRAIN DATA\n    * vectorizer = CountVectorizer()\n    * x_train_bow = vectorizer.fit_transform(X_train)\n    * x_cv_bow = vectorizer.fit_transform(X_cv)\n    * x_test_bow = vectorizer.fit_transform(X_test)\n    \n2. DATA LEAKAGE PROBLEM: IF WE DO LIKE THIS WE ARE LOOKING AT THE TEST DATA BEFORE MODELING\n    * vectorizer = CountVectorizer()\n    * X_bow = vectorizer.fit_transfomr(X)\n    * X_train, X_test, y_train, y_test = train_test_split(X_bow, Y, test_size=0.33)\n    \n3. YOU SHOULD PASS THE PROBABILITY SCORES NOT THE PREDICTED VALUES\n    * y_pred = neigh.predict(X)\n    * roc_auc_score(y_ture,y_pred)","04a78166":"**As it can be seen above the same user has multiple reviews of the same values for HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary and Text and on doing analysis it was found that ProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies, 8.82-Ounce Packages (Pack of 8)**\n\n**ProductId=B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so on**\n\n**It was inferred after analysis that reviews with same parameters other than ProductId belonged to the same product just having different flavour or quantity. Hence in order to reduce redundancy it was decided to eliminate the rows having same parameters.**\n\n**The method used for the same was that we first sort the data according to ProductId and then just keep the first similar product review and delelte the others. for eg. in the above just the review for ProductId=B000HDL1RQ remains. This method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product**","f7bdec00":"**Observation:** It was seen that in 2 rows given above the value of HelpfullnessNumerator is Greater than HelpfullnessDenominator which is not practically hence such rows are too removed from dataset","005e0099":"## 1. Apply Logistic Regression with L2 regularization on BOW ","2ce3d83c":"# Readind Data from Sqlite database","579543d7":"# Sorting dataset based on 'Time' feature","fd27b6e2":"# Text Preprocessing\n**Now we have finished deduplication. Now our data requires some preprocessing before we go on futher anlysis and make the prediction model Hence in the preprocessing phase we do the following steps given below**\n\n1. Begin by removing html tags\n1. Removing any punctution or limited set of special character:like ,or . or # etc\n1. Check the words is made up of english letters and is not alpha-numeric\n1. Check to see if the length of the words is greater than 2 (as it was research that there is no adjective in 2 letter)\n1. Convert the words to lowercase\n1. Remove stopwords\n1. Snowball stemming the word(it is observed that Snoball stemming is better that Porter stemming)\n\n***After this we will collect the words and will use to describe positive and negative reviews.***","eb904bc6":"# Feature Importance on BOW,","239909b9":"\n\nprint(\"Total no. of sentences containing html tags\",i)\n#Set of stopwords\n#stop_words=set(stopwords.word('english'))\n\n# we are removing the words from the stop words list: 'no', 'nor', 'not' for semantic meanig in bigrams and trigram\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',  'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',  'weren', \\\n            'won', \"won't\", 'wouldn'])\nstemming=nltk.stem.SnowballStemmer('english')\n\n#creating funtion to remove html tags from words \n#https:\/\/stackoverflow.com\/questions\/9662346\/python-code-to-remove-html-tags-from-a-string\n#1st:Using regrex\ndef cleanhtml(sentencs):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentencs)\n    return cleantext\n#OR\n#2ndUsing Beautiful\nfrom bs4 import BeautifulSoup\ncleantext=BeautifulSoup(sentence,\"lxml\").text\n\n##function to clean the word of any punctuation or special characters\ndef cleanpunc(sentence):\n    #clean =re.sub('[^A-Za-z0-9]+', '', sentence)\n    cleanp=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    clean_punc=re.sub(r'[.|,|)|(|\\|\/]',r' ',cleanp)\n    return clean_punc\nprint('stop words are :',stopwords)\nprint(\"*********************\")\nprint('base word for tasty is :',stemming.stem('tasty'))\n","cbee150d":"1. First we split whole preprocessed review data to train ,cross validation and test \n2. Then we applied logistic regression on Bag of Words vectoriser for l2 regularisation then measured best hyperparameter and plotted ROC curve and heatmap on test data \n3. After that we wrote down top 10 postive and negative features \n4. Second we applied logistic regression on TFIDF vectoriser for l2 regularisation ,then measured best lambda value and performed same process like Bag of words. \n5. For Avg Word 2 vec we perform w2v on train data and used models and words of train data to cv and test data \n6. Then after we applied logistic regression on Avg Word2vec for l2 regularisation \n7. Just like Avg word2vec we applied logistic regression on TFIDF avgW2vec \n","30dfccbe":"# Assignment 5: Apply Logistic Regression\n1. Apply Logistic Regression on these feature sets\n    * Review text, preprocessed one converted into vectors using (BOW)\n    * Review text, preprocessed one converted into vectors using (TFIDF)\n    * Review text, preprocessed one converted into vectors using (AVG W2v)\n    * Review text, preprocessed one converted into vectors using (TFIDF W2v)\n\n1. Hyper paramter tuning (find best hyper parameters corresponding the algorithm that you choose)\n    * Finding the best hyper parameter which will give the maximum AUC value\n                OR\n    * Finding the best hyper paramter using k-fold cross validation or simple cross validation data\n                OR\n    * Use gridsearch cv or randomsearch cv or you can also write your own for loops to do this task of hyperparameter tuning\n\n1. Pertubation Test\n    * Get the weights W after fit your model with the data X.\n    * Add a noise to the X (X' = X + e) and get the new data set X' (if X is a sparse matrix, X.data+=e)\n    * Fit the model again on data X' and get the weights W'\n    * Add a small eps value(to eliminate the divisible by zero error) to W and W\u2019 i.e W=W+10^-6 and W\u2019 = W\u2019+10^-6\n    * Now find the % change between W and W' (| (W-W') \/ (W) |)*100)\n    * Calculate the 0th, 10th, 20th, 30th, ...100th percentiles, and observe any sudden rise in the values of percentage_change_vector\n    * Ex: consider your 99th percentile is 1.3 and your 100th percentiles are 34.6, there is sudden rise from 1.3 to 34.6, now calculate the 99.1, 99.2, 99.3,..., 100th percentile values and get the proper value after which there is sudden rise the values, assume it is 2.5\n    * Printing the feature names whose % change is more than a threshold x(in our example it's 2.5)\n\n1. Sparsity\n    * Calculate sparsity on weight vector obtained after using L1 regularization\n    \n1. Feature importance\n    * Get top 10 important features for both positive and negative classes separately.\n\n1. Feature engineering\n    * To increase the performance of your model, we can also experiment with with feature engineering like :\n        * Taking length of reviews as another feature.\n        * Considering some features from review summary as well.\n\n1. Representation of results\n    * ploting the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure.\n    * Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n    * Along with plotting ROC curve, you need to print the confusion matrix with predicted and original labels of test data points. Please visualize your confusion matrices using seaborn heatmaps.\n\n1. Conclusion\n    * summarize the results at the end of the notebook, summarize it in the table format. To print out a table please refer to this prettytable library link\n\n\n1. There will be an issue of data-leakage if you vectorize the entire data and then split it into train\/cv\/test.\n2. To avoid the issue of data-leakag, make sure to split your data first and then vectorize it.\n3. While vectorizing your data, apply the method fit_transform() on you train data, and apply the method transform() on cv\/test data.\n","bb5881a4":"# Loadig the data \nThe dataset is available in two forms\n1. .csv form\n2. Sqlite Database\n\nIn order to load the database ,I have use the Sqlite Database as it easier to query the data and visualise the data sufficienlty . Here as we only want to get the global sentiment of the recommnedation(Positive\/Negative), We will purposefully ignore all the scores equal to 3 ,If the Score is above 3 then the reviews will be \"Positive\",Otherwise it will \"Negative\".","40298a42":"                           Or","1947987e":"# Applying Logistic Regression with L2 regularization on TFIDF W2V,","6c5d9074":"# Feature Importance on TFIDF","a9f80e46":"# \n#the code will take time to run \nif not os.path.isfile('finalsqlite'):\n    final_string=[]\n    all_postive_words=[]\n    all_negative_words=[]\n    for i , sentence in enumerate(tqdm(final_data['Text'].values)):\n        filter_sentence=[]\n        sent=cleanhtml(sentence)\n        for words in sent.split():\n            # we have used cleanpunc(w).split(), one more split function here because consider w=\"abc.def\", cleanpunc(w) will return \"abc def\"\n            # if we dont use .split() function then we will be considring \"abc def\" as a single word, but if you use .split() function we will get \"abc\", \"def\"\n            for clean_words in cleanpunc(words).split():\n                if ((clean_words.isalpha())&(len(clean_words)>2)):\n                    if (clean_words.lower() not in stopwords):\n                        s = (stemming.stem(clean_words.lower())).encode(encoding='UTF-8')\n                        filter_sentence.append(s)\n                        if (final_data['Score'].values)[i]==1:\n                            #list of all words used to describe positive reviews\n                            all_postive_words.append(s)  \n                            \n                        if (final_data['Score'].values)[i]==0:\n                            #list of all words used to describe negative reviews\n                            all_negative_words.append(s)  \n                    else:\n                        continue\n                else:\n                    continue\n        str1=b\" \".join(filter_sentence)#final string of cleaned words\n        #print(\"***********************************************************************\")\n        final_string.append(str1)\n        i+=1\n     ","7397ac45":"# Converting text into vectors using Avg W2V","28d91c7f":"#  Performing pertubation test (multicollinearity check) on BOW","e7132dcc":"   Another way of finding optimal_C","dc08b22b":"# Logistic Regression on  Amazon Fine food Review \nThe Amazon fine food dataset consists of reviews of fine foods from amazon\nAll data in one sqlite database. 568,454 food reviews Amazon users left up to October 2012\nData includes:\n\n* Number of reviews :  568,454 reviews\n* Number of user : 256,059 users\n* Total Number of Producst : 74,258 products\n* Time Span of taking Reviews : from Oct 1999 - Oct 2012\n* Number of Columns\/fields : 10\n\n***Attributes Information :***\n1. Id : Row Id\n1. ProductId :Unique identifier for the product (74258 unique values)\n1. UserId :Unqiue identifier for the user (256059 unique values)\n1. ProfileName : Profile name of the user (218418 unique values )\n1. HelpfulnessNumerator : Number of users who found the review helpful \n1. HelpfulnessDenominator : Number of users who indicated whether they found the review helpful or not \n1. Score : Rating between 1 and 5\n1. Time : Timestamp for the review\n1. Summary : Brief summary of the review (295744 unique values)\n1. Text : Text of the review (393579 unique values)\n\n> **Objective :** *Given a review ,determine whether a review is postive or negative*\n\n**Q].** *How to determine if a review is postive or negative?*\n\n**Ans.** We can use the Score\/rating. A rating of 4 or 5 could be considered as positive review & A rating of 1 or 2 can be consider as negative review . A rating of 3 can be consider as neutral and can be ignored . This is the approximation and proxy way to approach way of determing the polarity(positivity\/negativity) of reviews."}}