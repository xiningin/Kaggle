{"cell_type":{"79da3e7a":"code","d68f8593":"code","b65a515f":"code","560dee10":"code","7fcc973d":"code","7675c609":"code","0c86d4ca":"code","408421e4":"code","b9b08baa":"code","aaf4da85":"code","6e0a803f":"code","94d4216a":"code","f26c09ad":"code","8ed4f92c":"code","1a436636":"code","6e3f9439":"code","44b57110":"code","3f6151a4":"code","9c2e56f0":"code","27384389":"code","90a6ff01":"code","25b1ada1":"code","0b236c68":"code","5c38bca1":"code","b1008ddc":"code","c2894331":"code","f6408978":"code","936a9566":"code","2064ede4":"code","e6e12f30":"code","07254a2a":"code","895e5a9c":"code","820b16ed":"code","e87f5478":"code","26e4375e":"code","88b99dee":"code","f3029159":"code","c1916d36":"code","d8a7e1a1":"code","07cce0c7":"code","a620e20b":"code","593c0733":"code","64051aa8":"code","795c1541":"code","e84864f1":"code","97245b00":"code","2405707d":"code","5acb1ec1":"code","24a0a175":"code","e054599f":"code","5df47293":"code","7def87e2":"code","af85d330":"code","b022912a":"code","5f24d411":"code","2f4bc1f3":"code","7db386bb":"code","c12130e0":"code","619836d1":"code","ac5f412e":"code","74fa6db0":"code","21ba1065":"code","ff131cf2":"code","fdc9af25":"code","925350f3":"code","c91e1aeb":"code","fdb4d0f5":"code","b4a9f045":"code","5c5fd0d3":"code","4fd5e803":"code","9d33282b":"code","b3f6b682":"code","3c7e5c8b":"code","222966df":"code","a79e1662":"code","5de5e24c":"code","3471b6e3":"code","e62ab80f":"code","512578e1":"code","e4887011":"code","fe171e08":"code","9923f3a0":"code","63f7a3b3":"code","d07c87f9":"code","6eee32f8":"code","a28182c8":"code","436cad7b":"code","07f30637":"code","73c12f05":"markdown","258f3f2e":"markdown","29ad7128":"markdown","31c1b61a":"markdown","64990803":"markdown","c8551ddb":"markdown","af45855b":"markdown","0a8c4e68":"markdown","aef7b8e4":"markdown","509c02fe":"markdown","00453395":"markdown","30b344c2":"markdown","5d6f7d20":"markdown","278476c5":"markdown","90464bc1":"markdown","99258d81":"markdown","46b847bf":"markdown","208ca765":"markdown","6e33f10a":"markdown","c2584b23":"markdown","7c72b9ee":"markdown","680938fa":"markdown","0ff18610":"markdown","dfd21a9c":"markdown","dccd5a71":"markdown","a41fb7a4":"markdown","fd689270":"markdown","c595b80a":"markdown","aab0c8c9":"markdown","06f32c20":"markdown","6ceabfea":"markdown","fda488fd":"markdown","2e1192be":"markdown","fe478607":"markdown"},"source":{"79da3e7a":"# Import required libraries\nimport os\nimport gc\nimport sys\nimport json\nimport random\nfrom pathlib import Path\n\nimport cv2 # CV2 for image manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\n\nfrom imgaug import augmenters as iaa\n\nimport seaborn as sns\nimport matplotlib.image as mpimg\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold, KFold","d68f8593":"!pip install tensorflow==1.5\n!pip install keras==2.1.5\n\nimport tensorflow\nprint(tensorflow.__version__)\nimport keras\nprint(keras.__version__)","b65a515f":"!ls \/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/","560dee10":"%%time\nwith open('\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/label_descriptions.json', 'r') as file:\n    label_desc = json.load(file)\nsample_sub_df = pd.read_csv('\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/sample_submission.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train.csv')","7fcc973d":"train_df.head()","7675c609":"sample_sub_df.head()","0c86d4ca":"print(f'Shape of training dataset: {train_df.shape}')","408421e4":"print(f'# of images in training set: {train_df[\"ImageId\"].nunique()}')\nprint(f'# of images in test set: {sample_sub_df[\"ImageId\"].nunique()}')","b9b08baa":"pd.DataFrame([train_df['Height'].describe(), train_df['Width'].describe()]).T.loc[['max', 'min', 'mean']]","aaf4da85":"image_shape_df = train_df.groupby(\"ImageId\")[\"Height\", \"Width\"].first()","6e0a803f":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\nax1.hist(image_shape_df['Height'], bins=100)\nax1.set_title(\"Height distribution\")\nax2.hist(image_shape_df['Width'], bins=100)\nax2.set_title(\"Width distribution\")\nplt.show()","94d4216a":"plt.figure(figsize = (70,7))\nmin_height = list(set(train_df[train_df['Height'] == train_df['Height'].min()]['ImageId']))[0]\nplt.imshow(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{min_height}.jpg'))\nplt.grid(False)\nplt.show()","f26c09ad":"plt.figure(figsize = (70,7))\nmax_height = list(set(train_df[train_df['Height'] == train_df['Height'].max()]['ImageId']))[0]\nplt.imshow(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{max_height}.jpg'))\nplt.grid(False)\nplt.show()","8ed4f92c":"plt.figure(figsize = (70,7))\nmin_width = list(set(train_df[train_df['Width'] == train_df['Width'].min()]['ImageId']))[0]\nplt.imshow(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{min_width}.jpg'))\nplt.grid(False)\nplt.show()","1a436636":"plt.figure(figsize = (70,7))\nmax_width = list(set(train_df[train_df['Width'] == train_df['Width'].max()]['ImageId']))[0]\nplt.imshow(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{max_width}.jpg'))\nplt.grid(False)\nplt.show()","6e3f9439":"area_df = pd.DataFrame()\narea_df['ImageId'] = train_df['ImageId']\narea_df['area'] = train_df['Height'] * train_df['Width']\nmin_area = list(set(area_df[area_df['area'] == area_df['area'].min()]['ImageId']))[0]\nmax_area = list(set(area_df[area_df['area'] == area_df['area'].max()]['ImageId']))[0]","44b57110":"plt.figure(figsize = (70,7))\nplt.imshow(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{min_area}.jpg'))\nplt.grid(False)\nplt.show()","3f6151a4":"plt.figure(figsize = (70,7))\nplt.imshow(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{max_area}.jpg'))\nplt.grid(False)\nplt.show()","9c2e56f0":"num_classes = len(label_desc['categories'])\nnum_attributes = len(label_desc['attributes'])\nprint(f'Total # of classes: {num_classes}')\nprint(f'Total # of attributes: {num_attributes}')","27384389":"categories_df = pd.DataFrame(label_desc['categories'])\nattributes_df = pd.DataFrame(label_desc['attributes'])\ncategories_df","90a6ff01":"pd.set_option('display.max_rows', 300)\nattributes_df","25b1ada1":"def plot_images(size=12, figsize=(12, 12)):\n    # First get some images to be plotted\n    image_ids = train_df['ImageId'].unique()[:12]\n    images=[]\n    \n    for image in image_ids:\n        images.append(mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{image}.jpg'))\n    \n    # Plot images in groups of 4 images\n    n_groups = 4\n    \n    count = 0\n    for index in range(size \/\/ 4):\n        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=figsize)\n        for row in ax:\n            for col in row:\n                col.imshow(images[count])\n                col.axis('off')\n                count += 1\n        plt.show()\n    gc.collect()","0b236c68":"plot_images()","5c38bca1":"def create_mask(size):\n    image_ids = train_df['ImageId'].unique()[:size]\n    images_meta=[]\n\n    for image_id in image_ids:\n        img = mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{image_id}.jpg')\n        images_meta.append({\n            'image': img,\n            'shape': img.shape,\n            'encoded_pixels': train_df[train_df['ImageId'] == image_id]['EncodedPixels'],\n            'class_ids':  train_df[train_df['ImageId'] == image_id]['ClassId']\n        })\n\n    masks = []\n    for image in images_meta:\n        shape = image.get('shape')\n        encoded_pixels = list(image.get('encoded_pixels'))\n        class_ids = list(image.get('class_ids'))\n        \n        # Initialize numpy array with shape same as image size\n        height, width = shape[:2]\n        mask = np.zeros((height, width)).reshape(-1)\n        \n        # Iterate over encoded pixels and create mask\n        for segment, (pixel_str, class_id) in enumerate(zip(encoded_pixels, class_ids)):\n            splitted_pixels = list(map(int, pixel_str.split()))\n            pixel_starts = splitted_pixels[::2]\n            run_lengths = splitted_pixels[1::2]\n            assert max(pixel_starts) < mask.shape[0]\n            for pixel_start, run_length in zip(pixel_starts, run_lengths):\n                pixel_start = int(pixel_start) - 1\n                run_length = int(run_length)\n                mask[pixel_start:pixel_start+run_length] = 255 - class_id * 4\n        masks.append(mask.reshape((height, width), order='F'))  # https:\/\/stackoverflow.com\/questions\/45973722\/how-does-numpy-reshape-with-order-f-work\n    return masks, images_meta","b1008ddc":"def plot_segmented_images(size=12, figsize=(14, 14)):\n    # First create masks from given segments\n    masks, images_meta = create_mask(size)\n    \n    # Plot images in groups of 4 images\n    n_groups = 4\n    \n    count = 0\n    for index in range(size \/\/ 4):\n        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=figsize)\n        for row in ax:\n            for col in row:\n                col.imshow(images_meta[count]['image'])\n                col.imshow(masks[count], alpha=0.75)\n                col.axis('off')\n                count += 1\n        plt.show()\n    gc.collect()","c2894331":"plot_segmented_images()","f6408978":"categories_df = pd.DataFrame(label_desc.get('categories'))\nattributes_df = pd.DataFrame(label_desc.get('attributes'))","936a9566":"print(f'# of categories: {len(categories_df)}')\nprint(f'# of attributes: {len(attributes_df)}')","2064ede4":"categories_df.head()","e6e12f30":"attributes_df.head()","07254a2a":"category_map, attribute_map = {}, {}\nfor cat in label_desc.get('categories'):\n    category_map[cat.get('id')] = cat.get('name')\nfor attr in label_desc.get('attributes'):\n    attribute_map[attr.get('id')] = attr.get('name')","895e5a9c":"train_df['ClassId'] = train_df['ClassId'].map(category_map)\ntrain_df['ClassId'] = train_df['ClassId'].astype('category')","820b16ed":"sns.set(style='darkgrid')\nfig, ax = plt.subplots(figsize = (10,10))\nsns.countplot(y='ClassId',data=train_df , ax=ax, order = train_df['ClassId'].value_counts().index)\nfig.show()","e87f5478":"IMAGE_ID = '000b3ec2c6eaffb491a5abb72c2e3e26'","26e4375e":"# Get the an image id given in the training set for visualization\nvis_df = train_df[train_df['ImageId'] == IMAGE_ID]\nvis_df['ClassId'] = vis_df['ClassId'].cat.codes\nvis_df = vis_df.reset_index(drop=True)\nvis_df","88b99dee":"plt.figure(figsize = (110,11))\nimage = mpimg.imread(f'\/kaggle\/input\/imaterialist-fashion-2020-fgvc7\/train\/{IMAGE_ID}.jpg')\nplt.grid(False)\nplt.imshow(image)\nplt.plot()","f3029159":"train_df[train_df['ImageId'] == IMAGE_ID]","c1916d36":"segments = list(vis_df['EncodedPixels'])\nclass_ids = list(vis_df['ClassId'])\nmasks = []\nfor segment, class_id in zip(segments, class_ids):\n    \n    height = vis_df['Height'][0]\n    width = vis_df['Width'][0]\n    # Initialize empty mask\n    mask = np.zeros((height, width)).reshape(-1)\n    \n    # Iterate over encoded pixels and create mask\n    splitted_pixels = list(map(int, segment.split()))\n    pixel_starts = splitted_pixels[::2]\n    run_lengths = splitted_pixels[1::2]\n    assert max(pixel_starts) < mask.shape[0]\n    for pixel_start, run_length in zip(pixel_starts, run_lengths):\n        pixel_start = int(pixel_start) - 1\n        run_length = int(run_length)\n        mask[pixel_start:pixel_start+run_length] = 255 - class_id * 4\n\n    mask = mask.reshape((height, width), order='F')\n    masks.append(mask)","d8a7e1a1":"def plot_individual_segment(*masks, image, figsize=(110, 11)):\n    plt.figure(figsize = figsize)\n    plt.imshow(image)\n    for mask in masks:\n        plt.imshow(mask, alpha=0.6)\n    plt.axis('off')\n    plt.show()","07cce0c7":"plot_individual_segment(masks[0], image=image)","a620e20b":"plot_individual_segment(masks[1], image=image)","593c0733":"plot_individual_segment(masks[2], image=image)","64051aa8":"plot_individual_segment(masks[3], image=image)","795c1541":"plot_individual_segment(masks[4], image=image)","e84864f1":"plot_individual_segment(masks[5], image=image)","97245b00":"plot_individual_segment(masks[6], image=image)","2405707d":"plot_individual_segment(masks[6], image=image)","5acb1ec1":"print(f'Segments that do not have attributes: {train_df[\"AttributesIds\"].isna().sum()\/len(train_df) * 100} %')","24a0a175":"train_df[['ImageId', 'EncodedPixels', 'Height', 'Width', 'ClassId']].isna().sum()","e054599f":"train_df","5df47293":"train_df['ClassId'] = train_df['ClassId'].cat.codes","7def87e2":"train_df","af85d330":"train_df = train_df.drop('AttributesIds', axis=1)","b022912a":"image_df = train_df.groupby('ImageId')['EncodedPixels', 'ClassId'].agg(lambda x: list(x))\nsize_df = train_df.groupby('ImageId')['Height', 'Width'].mean()\nimage_df = image_df.join(size_df, on='ImageId')\n\nprint(\"Total images: \", len(image_df))\nimage_df.head()","5f24d411":"import os\nfrom pathlib import Path\n!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n\n!rm -rf .git # to prevent an error when the kernel is committed\n!rm -rf images assets # to prevent displaying images at the bottom of a kernel","2f4bc1f3":"!wget --quiet https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5\n!ls -lh mask_rcnn_coco.h5\n\nCOCO_WEIGHTS_PATH = 'mask_rcnn_coco.h5'","7db386bb":"# !cat \/kaggle\/working\/Mask_RCNN\/mrcnn\/model.py","c12130e0":"DATA_DIR = Path('\/kaggle\/input\/imaterialist-fashion-2020-fgvc7')\nROOT_DIR = Path('\/kaggle\/working')","619836d1":"import sys","ac5f412e":"# sys.path = sys.path[:-1]","74fa6db0":"# sys.path.append(ROOT_DIR\/'Mask_RCNN')\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","21ba1065":"class FashionConfig(Config):\n    \"\"\"Configuration for training on the toy shapes dataset.\n    Derives from the base Config class and overrides values specific\n    to the toy shapes dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"class\"\n\n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images\/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + len(categories_df)  # background + 46 classes\n\n    # Use small images for faster training. Set the limits of the small side\n    # the large side, and that determines the image shape.\n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n\n    # Use smaller anchors because our image and objects are small\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n\n    # Reduce training ROIs per image because the images are small and have\n    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n    TRAIN_ROIS_PER_IMAGE = 32\n\n    # Use a small epoch since the data is simple\n    STEPS_PER_EPOCH = 100\n\n    # use small validation steps since the epoch is small\n    VALIDATION_STEPS = 5\n    \nconfig = FashionConfig()\nconfig.display()","ff131cf2":"class FashionDataset(utils.Dataset):\n    def __init__(self, df):\n        super().__init__(self)\n        \n        self.IMAGE_SIZE = 256\n        \n        # Add classes\n        for cat in label_desc['categories']:\n            self.add_class('fashion', cat.get('id'), cat.get('name'))\n        \n        # Add images\n        for i, row in df.iterrows():\n            self.add_image(\"fashion\", \n                           image_id=row.name, \n                           path=str(DATA_DIR\/'train'\/row.name) + '.jpg', \n                           labels=row['ClassId'],\n                           annotations=row['EncodedPixels'], \n                           height=row['Height'], width=row['Width'])\n            \n    def _resize_image(self, image_path):\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (self.IMAGE_SIZE, self.IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n        return img\n        \n    def load_image(self, image_id):\n        return self._resize_image(self.image_info[image_id]['path'])\n    \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path'], [x for x in info['labels']]\n    \n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n                \n        mask = np.zeros((self.IMAGE_SIZE, self.IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n        labels = []\n        \n        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n            annotation = [int(x) for x in annotation.split(' ')]\n            \n            for i, start_pixel in enumerate(annotation[::2]):\n                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n\n            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n            sub_mask = cv2.resize(sub_mask, (self.IMAGE_SIZE, self.IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n            \n            mask[:, :, m] = sub_mask\n            labels.append(int(label)+1)\n            \n        return mask, np.array(labels)","fdc9af25":"dataset = FashionDataset(image_df)\ndataset.prepare()\n\nfor i in range(6):\n    image_id = random.choice(dataset.image_ids)\n\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=4)","925350f3":"# Training on a very small subset of the data for fast results for now.\n# image_df = image_df.iloc[0:2]","c91e1aeb":"# This code partially supports k-fold training, \n# you can specify the fold to train and the total number of folds here\nFOLD = 0\nN_FOLDS = 2\n\nkf = KFold(n_splits=N_FOLDS, random_state=42, shuffle=True)\nsplits = kf.split(image_df) # ideally, this should be multilabel stratification\n\ndef get_fold():    \n    for i, (train_index, valid_index) in enumerate(splits):\n        if i == FOLD:\n            return image_df.iloc[train_index], image_df.iloc[valid_index]\n        \ntrain_df, valid_df = get_fold()\n\ntrain_dataset = FashionDataset(train_df)\ntrain_dataset.prepare()\n\nvalid_dataset = FashionDataset(valid_df)\nvalid_dataset.prepare()","fdb4d0f5":"print(train_df.shape)\nprint(valid_df.shape)","b4a9f045":"# Note that any hyperparameters here, such as LR, may still not be optimal\nLR = 1e-4\nEPOCHS = [1, 6, 8]\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","5c5fd0d3":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)\n\n# Load weights trained on MS COCO, but skip layers that\n# are different due to the different number of classes\n# See README for instructions to download the COCO weights\nmodel.load_weights(COCO_WEIGHTS_PATH, by_name=True,\n                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n                            \"mrcnn_bbox\", \"mrcnn_mask\"])","4fd5e803":"augmentation = iaa.Sequential([\n    iaa.Fliplr(0.5) # only horizontal flip here\n])","9d33282b":"%%time\nmodel.train(train_dataset, valid_dataset,\n            learning_rate=LR*2, # train heads with higher lr to speedup learning\n            epochs=EPOCHS[0],\n            layers='heads',\n            augmentation=None)\n\nhistory = model.keras_model.history.history","b3f6b682":"# %%time\n# model.train(train_dataset, valid_dataset,\n#             learning_rate=LR,\n#             epochs=EPOCHS[1],\n#             layers='all',\n#             augmentation=augmentation)\n\n# new_history = model.keras_model.history.history\n# for k in new_history: history[k] = history[k] + new_history[k]","3c7e5c8b":"# %%time\n# model.train(train_dataset, valid_dataset,\n#             learning_rate=LR\/5,\n#             epochs=EPOCHS[2],\n#             layers='all',\n#             augmentation=augmentation)\n\n# new_history = model.keras_model.history.history\n# for k in new_history: history[k] = history[k] + new_history[k]","222966df":"epochs = range(EPOCHS[0])\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(131)\nplt.plot(epochs, history['loss'], label=\"train loss\")\nplt.plot(epochs, history['val_loss'], label=\"valid loss\")\nplt.legend()\nplt.subplot(132)\nplt.plot(epochs, history['mrcnn_class_loss'], label=\"train class loss\")\nplt.plot(epochs, history['val_mrcnn_class_loss'], label=\"valid class loss\")\nplt.legend()\nplt.subplot(133)\nplt.plot(epochs, history['mrcnn_mask_loss'], label=\"train mask loss\")\nplt.plot(epochs, history['val_mrcnn_mask_loss'], label=\"valid mask loss\")\nplt.legend()\n\nplt.show()","a79e1662":"best_epoch = np.argmin(history[\"val_loss\"]) + 1\nprint(\"Best epoch: \", best_epoch)\nprint(\"Valid loss: \", history[\"val_loss\"][best_epoch-1])","5de5e24c":"import glob","3471b6e3":"glob_list = glob.glob(f'\/kaggle\/working\/class*\/mask_rcnn_class_{best_epoch:04d}.h5')\nmodel_path = glob_list[0] if glob_list else ''","e62ab80f":"sample_df = sample_sub_df","512578e1":"class InferenceConfig(FashionConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\nassert model_path != '', \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","e4887011":"# Convert data to run-length encoding\ndef to_rle(bits):\n    rle = []\n    pos = 0\n    for bit, group in itertools.groupby(bits):\n        group_list = list(group)\n        if bit:\n            rle.extend([pos, sum(group_list)])\n        pos += len(group_list)\n    return rle","fe171e08":"# Since the submission system does not permit overlapped masks, we have to fix them\ndef refine_masks(masks, rois):\n    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n    mask_index = np.argsort(areas)\n    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n    for m in mask_index:\n        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n        union_mask = np.logical_or(masks[:, :, m], union_mask)\n    for m in range(masks.shape[-1]):\n        mask_pos = np.where(masks[:, :, m]==True)\n        if np.any(mask_pos):\n            y1, x1 = np.min(mask_pos, axis=1)\n            y2, x2 = np.max(mask_pos, axis=1)\n            rois[m, :] = [y1, x1, y2, x2]\n    return masks, rois","9923f3a0":"IMAGE_SIZE = 256","63f7a3b3":"def resize_image(image_path):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n    return img","d07c87f9":"%%time\nsub_list = []\nmissing_count = 0\nfor i, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n    image = resize_image(str(DATA_DIR\/'test'\/row['ImageId']) + '.jpg')\n    result = model.detect([image])[0]\n    if result['masks'].size > 0:\n        masks, _ = refine_masks(result['masks'], result['rois'])\n        for m in range(masks.shape[-1]):\n            mask = masks[:, :, m].ravel(order='F')\n            rle = to_rle(mask)\n            label = result['class_ids'][m] - 1\n            sub_list.append([row['ImageId'], ' '.join(list(map(str, rle))), label, np.NaN])\n    else:\n        # The system does not allow missing ids, this is an easy way to fill them \n        sub_list.append([row['ImageId'], '1 1', 23, np.NaN])\n        missing_count += 1","6eee32f8":"sample_sub_df.columns","a28182c8":"submission_df = pd.DataFrame(sub_list, columns=sample_df.columns.values)\nprint(\"Total image results: \", submission_df['ImageId'].nunique())\nprint(\"Missing Images: \", missing_count)\nsubmission_df.head()","436cad7b":"submission_df.to_csv(\"submission.csv\", index=False)","07f30637":"for i in range(9):\n    image_id = sample_df.sample()['ImageId'].values[0]\n    image_path = str(DATA_DIR\/'test'\/image_id)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    result = model.detect([resize_image(image_path)])\n    r = result[0]\n    \n    if r['masks'].size > 0:\n        masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n        for m in range(r['masks'].shape[-1]):\n            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n        \n        y_scale = img.shape[0]\/IMAGE_SIZE\n        x_scale = img.shape[1]\/IMAGE_SIZE\n        rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n        \n        masks, rois = refine_masks(masks, rois)\n    else:\n        masks, rois = r['masks'], r['rois']\n        \n    visualize.display_instances(img, rois, masks, r['class_ids'], \n                                ['bg']+label_names, r['scores'],\n                                title=image_id, figsize=(12, 12))","73c12f05":"### Let's see the class wise distribution of segments in training dataset","258f3f2e":"## Plotting 7th Segment with ClassId: \"sleeve\"","29ad7128":"## Plotting 3rd Segment with ClassId: \"pants\"","31c1b61a":"## Now let's plot each segment in a separate image","64990803":"### Image with maximum area","c8551ddb":"## Data Preparation and modeling","af45855b":"## Plotting 4th Segment with ClassId: \"top, t-shirt, sweatshirt\"","0a8c4e68":"## Analysing Categories and Attributes","aef7b8e4":"## Plotting 2nd Segment: ClassId: \"shoe\"","509c02fe":"Let's check of missing values in training dataset for columns other than \"AttributeIds\"","00453395":"## Plotting 6th Segment with ClassId: \"sleeve\"","30b344c2":"### Now let's visualize an image with all its classes and attributes","5d6f7d20":"# In Progress... Stay Tuned!","278476c5":"### Image with minimum width","90464bc1":"## Plotting 5th Segment with ClassId: \"pocket\"","99258d81":"### Image with minimum height","46b847bf":"### Image with minimum area","208ca765":"Some of the segments have no attributes. Let's check how many such segment exists in training dataset.","6e33f10a":"## Plotting a few training images without any masks","c2584b23":"### Image size analysis in training dataset","7c72b9ee":"Let's load the COCO dataset weights to our Model.","680938fa":"## Details about Classes and Attributes","0ff18610":"### Height and Width destribution of training images","dfd21a9c":"### Image with maximum height","dccd5a71":"Reference: https:\/\/github.com\/matterport\/Mask_RCNN\/blob\/master\/samples\/shapes\/train_shapes.ipynb","a41fb7a4":"### Image with maximum width","fd689270":"From above table, this image has 8 segmentes and a few attributes. Let's visualize all of them!","c595b80a":"## Training","aab0c8c9":"So there are 46 categories (classes) and 294 attributes. Let's see some of the categories and attributes","06f32c20":"Drop attributeIds for simplicity for now. TODO: Need to take this in consideration once the basic model is ready with ClassId","6ceabfea":"## Let's first the plot the plain image","fda488fd":"## Plotting a few images with given segments","2e1192be":"## Plotting 8th segment with Class \"neckline\"","fe478607":"## Plotting 1st Segment: ClassId: \"Shoe\" and no attributes "}}