{"cell_type":{"ab4cf2b7":"code","902891fd":"code","4d0ca79a":"code","975fa398":"code","896fcd3c":"code","65f9d2a7":"code","b37e5200":"code","01c6074a":"code","362c9503":"code","76999f2e":"code","636c783a":"code","6356beb5":"code","b749359d":"code","753cd897":"code","40758ad4":"code","61e142bd":"code","a3ebd154":"code","df1e9840":"code","415821b7":"code","55c37bfc":"code","5d2ba482":"code","5661d803":"code","65867f44":"code","5f94c3d3":"code","4638c4ea":"code","fd566a29":"code","ea1ac99b":"code","fb890bc3":"code","6ac0b862":"code","34e41a03":"code","cb59b615":"code","49cded0e":"code","10478d3b":"code","2a09e213":"code","695f18ed":"code","75641601":"code","fd8c1590":"code","15469ff1":"code","5e04f755":"code","fcaca062":"code","733afeb7":"code","f57cecea":"code","f166095b":"code","2f00d68a":"code","c44feba3":"code","aad339fd":"code","5dd6e836":"code","a641a4e4":"code","5004e4f9":"code","328fdc95":"code","221c15d7":"code","b139af1d":"code","114e3395":"code","819ce085":"code","ee6ef3fc":"markdown","b13dc24e":"markdown","5db10533":"markdown","93f72664":"markdown","4c8aa8a7":"markdown","bea0aa43":"markdown","0295f356":"markdown","bf955270":"markdown","1dabdff5":"markdown","1e1c5d9b":"markdown","57396686":"markdown","84854f4a":"markdown","917f3dca":"markdown","a2556bbc":"markdown","4d0b239f":"markdown","f11ebe9a":"markdown","25af526f":"markdown","10363182":"markdown","a80559ad":"markdown","9f071e2f":"markdown","a4ed2de6":"markdown","caf1eafc":"markdown","e2b5c7c1":"markdown","6e9dcef3":"markdown","9a9a55c1":"markdown","6add91dc":"markdown","5138ead1":"markdown"},"source":{"ab4cf2b7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_colwidth', -1)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport re\n\n#Plot\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport cufflinks as cf\ncf.go_offline()\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.manifold import TSNE\n\n# Keras NLP\nfrom keras.preprocessing import sequence, text\n\n# Keras training\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n\n#\u00a0RNN\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\n\n# print\nfrom tqdm import tqdm","902891fd":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","4d0ca79a":"train[train['target']==0].head(5)","975fa398":"train[train['target']==1].head(5)","896fcd3c":"# remove punc\ndef remove_punc(question):\n    return question.replace(\"?\", \"\").replace(\"!\", \"\").replace(\",\", \"\").replace(\";\", \"\").replace(\".\", \"\").replace(\"\\\\\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n","65f9d2a7":"nb_train = train.shape[0]\n\nprint(\"There are %s questions in the training set.\" % nb_train)\nprint(\"There are %s questions in the testing set.\\n\" % test.shape[0])\n\nnb_insincere = train[train['target']==1].shape[0]\nprint(\"There are {} insincere questions in the training set. ({}% total)\".format(nb_insincere, round(100*nb_insincere\/nb_train,2)))","b37e5200":"def question_size(question):\n    return len(question.split(\" \"))\n\ntrain['question_size'] = train[\"question_text\"].apply(question_size)\ntest['question_size'] = test[\"question_text\"].apply(question_size)","01c6074a":"# Sampled histograms\nto_plot = pd.DataFrame({'train': train['question_size'].sample(frac=0.01),\n                   'test': test['question_size'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram',\n              histnorm='probability',\n              title='Train\/test questions size distribution (from samples)',\n              filename='cufflinks\/basic-histogram')","362c9503":"# Sampled histograms\nto_plot = pd.DataFrame({'train_insincere': train[train['target']==1]['question_size'].sample(frac=0.05),\n                   'train_sincere': train[train['target']==0]['question_size'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram',\n              histnorm='probability',\n              title='Insincere\/sincere questions size distribution (from samples)',\n              filename='cufflinks\/basic-histogram')","76999f2e":"#do better here\ntrain['number_questions'] = train[\"question_text\"].apply(lambda x:x.count('? '))\ntrain['number_statements'] = train[\"question_text\"].apply(lambda x:x.count('. '))\n#train['number_statements'] = train[\"question_text\"].apply(lambda x:x.count(\"[^.]{15,}.\"))","636c783a":"# Sampled histograms\nto_plot = pd.DataFrame({'train_insincere': train[train['target']==1]['number_statements'].sample(frac=0.05),\n                   'train_sincere': train[train['target']==0]['number_statements'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram',\n              histnorm='probability',\n              title='Train\/test, number of statements in questions distribution (from samples)',\n              filename='cufflinks\/basic-histogram')","6356beb5":"punc_list = ['\\\\', '?', '.', ';', ',', '-']\n\ndef average_word_length(question):\n    words = re.sub(\"|\".join(punc_list), \"\", question).split(\" \")\n    return np.mean([len(w) for w in words])\n\ntrain['average_word_length'] = train[\"question_text\"].apply(average_word_length)","b749359d":"# Sampled histograms\nto_plot = pd.DataFrame({'train_insincere': train[train['target']==1]['average_word_length'].sample(frac=0.01),\n                   'train_sincere': train[train['target']==0]['average_word_length'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram', histnorm='probability',\n              title='Train\/test, average word length in questions distribution (from samples)', filename='cufflinks\/basic-histogram')","753cd897":"import nltk\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\ndef nb_stop_words(question):\n    words = re.sub(\"|\".join(punc_list), \"\", question).split(\" \")\n    return len([w for w in words if w in eng_stopwords])\n\ntrain['nb_stop_words'] = train[\"question_text\"].apply(nb_stop_words)","40758ad4":"# Sampled histograms\nto_plot = pd.DataFrame({'train_insincere': train[train['target']==1]['nb_stop_words'].sample(frac=0.01),\n                   'train_sincere': train[train['target']==0]['nb_stop_words'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram', histnorm='probability',\n              title='Train\/test, number of stopwords in questions distribution (from samples)', filename='cufflinks\/basic-histogram')","61e142bd":"def nb_up_case_word(question):\n    return question.count()\n\ntrain['nb_up_case_word'] = train[\"question_text\"].apply(lambda x:len(re.findall('[A-Z][a-z]+ ',x)))","a3ebd154":"# Sampled histograms\nto_plot = pd.DataFrame({'train_positive': train[train['target']==1]['nb_up_case_word'].sample(frac=0.01),\n                   'train_negative': train[train['target']==0]['nb_up_case_word'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram', histnorm='probability',\n              title='Train\/test, number of First names in questions distribution (from samples)', filename='cufflinks\/basic-histogram')","df1e9840":"def get_question_type(question):\n    question = question.lower()\n    question_types = [\"why\", \"who\", \"what\", \"when\", \"how\", \"can\", \"do\"]\n    \n    for qt in question_types:\n        if question.startswith(qt):\n            return qt\n    \n    return \"other\"\n\ntrain[\"question_type\"] = train[\"question_text\"].apply(get_question_type)","415821b7":"train[\"sum\"] = train.groupby([\"target\"]).qid.count()","55c37bfc":"train_grouped = train.groupby([\"question_type\", \"target\"],as_index=False).qid.count()\n\ntrain_grouped[\"sum\"] = train_grouped.groupby(\"target\").qid.transform(np.sum)\ntrain_grouped[\"qid\"] = train_grouped[\"qid\"] \/ train_grouped[\"sum\"]\n\ntrain_pivoted = train_grouped.pivot(index=\"question_type\", columns=\"target\", values=\"qid\")","5d2ba482":"train_pivoted.iplot(kind='bar',\n              title='Train\/test, type of questions distribution (from samples)', filename='cufflinks\/bar-chart-row')","5661d803":"#Build vocabulay\nctv = CountVectorizer(analyzer='word',token_pattern=r\"\\w{1,}'?[t]?\", ngram_range=(1, 1), stop_words = 'english')\n\n# Get vocabulary\ntrain['question_text_no_punc'] = train['question_text'].apply(remove_punc)\nctv.fit(train['question_text_no_punc'])\n\ntrain_pos = ctv.transform(train[train['target']==1]['question_text_no_punc'])\ntrain_neg = ctv.transform(train[train['target']==0]['question_text_no_punc'])","65867f44":"train_pos = train_pos.sum(axis=0) \/ train_pos.sum()\ntrain_neg = train_neg.sum(axis=0) \/ train_neg.sum()\n\ntrain_diff = train_pos - train_neg\n\ntrain_diff = train_diff.tolist()[0]","5f94c3d3":"inv_voc = {v:k for k,v in ctv.vocabulary_.items()}\ntup_list = [(value, inv_voc[ind]) for (ind, value) in enumerate(train_diff)]","4638c4ea":"most_sincere_words = sorted(tup_list)\nmost_insincere_words = sorted(tup_list, reverse=True)","fd566a29":"[pos_scores, pos_words] = zip(*most_insincere_words)\n[neg_scores, neg_words] = zip(*most_sincere_words)\n\ntrace1 = go.Bar(\n    y=list(reversed(pos_words[:300])),\n    x=list(reversed(pos_scores[:300])),\n    orientation = 'h',\n    marker=dict(\n        color='rgba(244, 80, 65, 0.6)'\n    ),\n)\ntrace2 = go.Bar(\n    y=list(reversed(neg_words[:300])),\n    x=list(reversed(neg_scores[:300])),\n    orientation = 'h',\n    marker=dict(\n        color='rgba(134, 244, 66, 0.6)'\n    ),\n)\n\nfig = tools.make_subplots(rows=1, cols=2)\n\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace1, 1, 2)\n\nfig['layout'].update(height=8000, title='Words that are more used in sincere(left)\/insincere(right) questions')\npy.iplot(fig, filename='simple-subplot-with-annotations')","ea1ac99b":"# user embeddings to find words that don't exist\nEMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt' # https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))","fb890bc3":"def nb_typos(question):\n    real_words = embeddings_index.keys()\n    typos=[w for w in remove_punc(question.lower()).split(\" \") if w not in real_words]\n    \n    return len(typos)\n    \ntrain[\"nb_typos\"] = train[\"question_text\"].apply(nb_typos)","6ac0b862":"# Sampled histograms\nto_plot = pd.DataFrame({'train_positive': train[train['target']==1]['nb_typos'].sample(frac=0.01),\n                   'train_negative': train[train['target']==0]['nb_typos'].sample(frac=0.01)})\n\nto_plot.iplot(kind='histogram', histnorm='probability',\n              title='Train\/test, number of typos in questions distribution (from samples)', filename='cufflinks\/basic-histogram')","34e41a03":"# match and highlight in 2D\nEMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n","cb59b615":"top_sincere_words = most_sincere_words[:1000]\ntop_insincere_words = most_insincere_words[:1000]","49cded0e":"words_to_keep = top_sincere_words + top_insincere_words\nemb = np.array([list(embeddings_index[word[1]]) + [abs(word[0])\/word[0], word[1]] for word in words_to_keep if word[1] in embeddings_index ])","10478d3b":"emb.shape","2a09e213":"X = emb\nX_embedded = TSNE(n_components=2).fit_transform(X[:,:300])\nX_embedded.shape","695f18ed":"# Create a trace\ntrace = go.Scatter(\n    x = X_embedded[:,0],\n    y = X_embedded[:,1],\n    mode = 'markers',\n    marker=dict(\n        color= ['rgb(51, 206, 111)' if val=='-1.0' else 'rgb(244, 86, 66)'  for val in list(X[:,300])]\n    ),\n    text = X[:,301]\n)\n\ndata = [trace]\nlayout = go.Layout(title='top sincere\/insincere words according to their embeddings (projected on 2D by T-SNE)')\nfig = go.Figure(data=data, layout=layout)\n# Plot and embed in ipython notebook!\npy.iplot(fig, filename='basic-scatter')\n\n# or plot with: plot_url = py.plot(data, filename='basic-line')","75641601":"train_count = train.groupby('target').count()\ntrain_count.qid[0] \/ (train_count.qid[0] + train_count.qid[1])","fd8c1590":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.question_text, train.target, \n                                                  stratify=train.target, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)\n\n","15469ff1":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xvalid) + list(xtrain))\n\ntrain_tfv =  tfv.transform(xtrain)\nvalid_tfv =  tfv.transform(xvalid)","5e04f755":"train_tfv","fcaca062":"valid_tfv","733afeb7":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(train_tfv, ytrain)\npredictions = clf.predict_proba(valid_tfv)","f57cecea":"# Plot different F1 scores according to threshold\nx = np.linspace(0,1, num=25)\ndef pred(threshold):\n    return [0 if (y<threshold) else 1 for y in predictions[:,1]]\n\nscores = [f1_score(yvalid, pred(xx)) for xx in x]\n\n# Create a trace\ntrace = go.Scatter(\n    x = x,\n    y = scores\n)\n\ndata = [trace]\n\npy.iplot(data, filename='basic-line')","f166095b":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","2f00d68a":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)","c44feba3":"# Plot different F1 scores according to threshold\nx = np.linspace(0,1, num=35)\ndef pred(threshold):\n    return [0 if (y<threshold) else 1 for y in predictions[:,1]]\n\nscores = [f1_score(yvalid, pred(xx)) for xx in x]\n\n# Create a trace\ntrace = go.Scatter(\n    x = x,\n    y = scores\n)\n\ndata = [trace]\n\npy.iplot(data, filename='basic-line')","aad339fd":"# Reload embeddings even if already loaded (for each section to be independant)\nEMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n","5dd6e836":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.question_text, train.target, \n                                                  stratify=train.target, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)\n","a641a4e4":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","5004e4f9":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","328fdc95":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","221c15d7":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","b139af1d":"model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=10, verbose=1, validation_data=(xvalid_pad, yvalid_enc))","114e3395":"predictions = model.predict_proba(xvalid_pad)","819ce085":"# Plot different F1 scores according to threshold\nx = np.linspace(0,1, num=100)\ndef pred(threshold):\n    return [0 if (y<threshold) else 1 for y in predictions[:,1]]\n\nscores = [f1_score(yvalid, pred(xx)) for xx in x]\n\n# Create a trace\ntrace = go.Scatter(\n    x = x,\n    y = scores\n)\n\ndata = [trace]\n\npy.iplot(data, filename='basic-line')","ee6ef3fc":"Seems like a nice feature","b13dc24e":"## LSTM\n","5db10533":"##\u00a0Average word length","93f72664":"# Train\/valid","4c8aa8a7":"### Read Embeddings","bea0aa43":"##\u00a0TFIDF and Logistic Regression","0295f356":"## CNN","bf955270":"## Number of stopwords","1dabdff5":"# CountVectorizer","1e1c5d9b":"No real diff here","57396686":"Note: insincere questions are longer on average !","84854f4a":"##\u00a0Question types","917f3dca":"A genuine question involves more 'what' (and 'how') and less 'why', this makes sense as 'why' is usually followed by a statement. ","a2556bbc":"## Number of typos","4d0b239f":"## Datasets size","f11ebe9a":"## Questions size","25af526f":"### Sentences preprocessing: sentences to seq of token vector","10363182":"# TFIDF + Logistic regression","a80559ad":"## First names","9f071e2f":"## Number of sentences\/questions in each question","a4ed2de6":"We still see the difference but its less interesting than the sentence size","caf1eafc":"###\u00a0Split validation\/train","e2b5c7c1":"## Embedding visualisations","6e9dcef3":"Note: Imbalanced dataset","9a9a55c1":"# Some EDA","6add91dc":"##\u00a0Term frequency comparison training positive\/negative","5138ead1":"# Discover the dataset with these intuitive visual insights !"}}