{"cell_type":{"50e48a4e":"code","ab843160":"code","1140933b":"code","144774b9":"code","ed3fa65f":"code","ae798c77":"code","982f13c9":"code","bd769e52":"code","dc10f987":"code","d33b147a":"code","bfc64088":"code","ccf44ece":"code","497e65c2":"code","703b6ebb":"code","d9f5674d":"code","1da29c2d":"code","8db62af0":"code","d7bcae61":"code","bc566f7f":"code","773dee6f":"code","5f8f4969":"code","098307bd":"code","3d0a949f":"code","d96ed6fd":"code","aa8497ef":"code","9d06c5ce":"code","d1f7f8f9":"code","743199fc":"code","26c01ae4":"code","47bf8750":"code","133d5f05":"code","780709e2":"code","1eb178e3":"code","d7e0cb9e":"code","b5458b80":"code","bc963b36":"code","33b3ba61":"code","f6066d92":"code","9b2a4cd3":"markdown","83c33290":"markdown","8028e26c":"markdown","5a12d189":"markdown","a19056db":"markdown","62a41561":"markdown","ca11f5ff":"markdown","84172877":"markdown","cf26ff0f":"markdown","149c3b6d":"markdown","b7cfad94":"markdown","b238c3e6":"markdown","cb133a0d":"markdown","5fb11cf4":"markdown"},"source":{"50e48a4e":"from sklearn import decomposition\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport scikitplot as skplt\n\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ab843160":"df_raw = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ndf_raw.head(5)","1140933b":"df_raw.describe()","144774b9":"df_raw.isnull().sum().head(100)","ed3fa65f":"# Define the dataset\n\ndf = df_raw.fillna(-99).dropna()\nnew_columns=[]\nfor i in df.columns:\n    if i != 'Patient ID':\n        if i != 'SARS-Cov-2 exam result':\n            if i != 'Patient addmited to regular ward (1=yes, 0=no)':\n                if i != 'Patient addmited to semi-intensive unit (1=yes, 0=no)':\n                    if i != 'Patient addmited to intensive care unit (1=yes, 0=no)':\n                        new_columns.append(i)\n        \nX0 = df[new_columns]\n# Define features\nX1 = pd.get_dummies(X0)  # Creating dummy variables  \nX1.head(5)","ae798c77":"# Check for balance\nFIGURE_SIZE = (8,8)\ndf['SARS-Cov-2 exam result'].value_counts().plot.pie(figsize=FIGURE_SIZE)","982f13c9":"# Define encoded target \ny = df[['SARS-Cov-2 exam result']]\ny = y.replace({'negative': 0, 'positive': 1})\ny","bd769e52":"# Run the Recursive feature elimination\nlogregHasCovid = LogisticRegression()\nn_features_to_select = 45\nrfe = RFE(logregHasCovid, n_features_to_select)\nrfe = rfe.fit(X1, y.values.ravel())\n# print(rfe.support_)\n# print(rfe.ranking_)","dc10f987":"# Define new columns from RFE\ncolumns_retained_RFE_HasCovid = X1.columns[rfe.get_support()].values\ncolumns_retained_RFE_HasCovid","d33b147a":"# Define new training features\nX = X1[columns_retained_RFE_HasCovid]\nX.head(4)","bfc64088":"# Train logistic regression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48)\n\nlogregHasCovid = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nlogregHasCovid.fit(X_train, y_train)\ny_pred = logregHasCovid.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logregHasCovid.score(X_test, y_test)))","ccf44ece":"# Results with confusion matrix\nFIGURE_SIZE = (8,8)\nconfusion_matrix0 = confusion_matrix(y_test, y_pred)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=FIGURE_SIZE)","497e65c2":"# Results with classification_report\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","703b6ebb":"# Results with ROC\nlogit_roc_auc = roc_auc_score(y_test, logregHasCovid.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logregHasCovid.predict_proba(X_test)[:,1])\nplt.figure(figsize=FIGURE_SIZE)\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","d9f5674d":"# del X, X0, X1, y","1da29c2d":"df = df_raw.fillna(-99).dropna()\nnew_columns2=[]\nfor i in df.columns:\n    if i != 'Patient ID':\n        if i != 'Patient addmited to regular ward (1=yes, 0=no)':\n            if i != 'Patient addmited to semi-intensive unit (1=yes, 0=no)':\n                if i != 'Patient addmited to intensive care unit (1=yes, 0=no)':\n                    new_columns2.append(i)\n        \nX = df[new_columns2]\nX = pd.get_dummies(X)  # Creating dummy variables  \n\nX.head(5)\nX.columns","8db62af0":"# Check for balance\ndf['Patient addmited to regular ward (1=yes, 0=no)'].value_counts().plot.pie(figsize=FIGURE_SIZE)","d7bcae61":"y = df[['Patient addmited to regular ward (1=yes, 0=no)']]","bc566f7f":"logregRegular = LogisticRegression()\nn_features_to_select = 24\nrfe = RFE(logregRegular, n_features_to_select)\nrfe = rfe.fit(X, y.values.ravel())\n# print(rfe.support_)\n# print(rfe.ranking_)","773dee6f":"columns_retained_RFE_Regular = X.columns[rfe.get_support()].values\ncolumns_retained_RFE_Regular","5f8f4969":"X = X[columns_retained_RFE_Regular]\nX.head(4)","098307bd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nlogregRegular = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\nlogregRegular.fit(X_train, y_train)\ny_pred = logregRegular.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logregRegular.score(X_test, y_test)))","3d0a949f":"confusion_matrix1 = confusion_matrix(y_test, y_pred)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=FIGURE_SIZE)\n","d96ed6fd":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","aa8497ef":"logit_roc_auc = roc_auc_score(y_test, logregRegular.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logregRegular.predict_proba(X_test)[:,1])\nplt.figure(figsize=FIGURE_SIZE)\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","9d06c5ce":"# Check for balance\ndf['Patient addmited to semi-intensive unit (1=yes, 0=no)'].value_counts().plot.pie(figsize=FIGURE_SIZE)","d1f7f8f9":"# Reset variables\ny = df[['Patient addmited to semi-intensive unit (1=yes, 0=no)']]\nX = df[new_columns2]\nX = pd.get_dummies(X)  # Creating dummy variables  \n\n# Recursive Feature Elimination (RFE)\nlogregSemiIntensive = LogisticRegression()\nn_features_to_select = 24\nrfe = RFE(logregSemiIntensive, n_features_to_select, verbose=False)\nrfe = rfe.fit(X, y.values.ravel())\n# print(rfe.support_)\n# print(rfe.ranking_)","743199fc":"\n\n# Replace the dataset with new features\ncolumns_retained_RFE_SemiIntensive = X.columns[rfe.get_support()].values\n\nX = X[columns_retained_RFE_SemiIntensive]\n\n\n# Split train and test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Train model\nlogregSemiIntensive = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\nlogregSemiIntensive.fit(X_train, y_train)\ny_pred = logregSemiIntensive.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logregSemiIntensive.score(X_test, y_test)))\n\n","26c01ae4":"confusion_matrix2 = confusion_matrix(y_test, y_pred)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=FIGURE_SIZE)\n\nprint(classification_report(y_test, y_pred))\n\nlogit_roc_auc = roc_auc_score(y_test, logregSemiIntensive.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logregSemiIntensive.predict_proba(X_test)[:,1])\nplt.figure(figsize=FIGURE_SIZE)\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","47bf8750":"# Check for balance\ndf['Patient addmited to intensive care unit (1=yes, 0=no)'].value_counts().plot.pie(figsize=FIGURE_SIZE)","133d5f05":"y = df[['Patient addmited to intensive care unit (1=yes, 0=no)']]\nX = df[new_columns2]\nX = pd.get_dummies(X)  # Creating dummy variables  \n\n# Recursive Feature Elimination (RFE)\nlogregIntensive = LogisticRegression()\nn_features_to_select = 40\nrfe = RFE(logregIntensive, n_features_to_select)\nrfe = rfe.fit(X, y.values.ravel())\n# print(rfe.support_)\n# print(rfe.ranking_)","780709e2":"\n\n# Replace the dataset with new features\ncolumns_retained_RFE_Intensive = X.columns[rfe.get_support()].values\n\nX = X[columns_retained_RFE_Intensive]\n\n\n# Split train and test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Train model\nlogregIntensive = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\nlogregIntensive.fit(X_train, y_train)\ny_pred = logregIntensive.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logregIntensive.score(X_test, y_test)))\n","1eb178e3":"confusion_matrix3 = confusion_matrix(y_test, y_pred)\nskplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=FIGURE_SIZE)\n\nprint(classification_report(y_test, y_pred))\n\n\nlogit_roc_auc = roc_auc_score(y_test, logregIntensive.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logregIntensive.predict_proba(X_test)[:,1])\nplt.figure(figsize=FIGURE_SIZE)\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","d7e0cb9e":"# Function for evaluating\ndef evaluate_sample(sample_number):\n    X = df[new_columns2]\n    X = pd.get_dummies(X)  # Creating dummy variables  \n\n    X1 = X[columns_retained_RFE_Regular].values[sample_number].reshape(1, -1)\n    X2 = X[columns_retained_RFE_SemiIntensive].values[sample_number].reshape(1, -1)\n    X3 = X[columns_retained_RFE_Intensive].values[sample_number].reshape(1, -1)\n\n    print(f'Probabilities for regular treatment. No: {logregRegular.predict_proba(X1)[0][0]:.4f},  Yes: {logregRegular.predict_proba(X1)[0][1]:.4f}')\n    print(f'Probabilities for semi intensive treatment. No: {logregSemiIntensive.predict_proba(X2)[0][0]:.4f},  Yes: {logregSemiIntensive.predict_proba(X2)[0][1]:.4f}')\n    print(f'Probabilities for intensive treatment. No: {logregIntensive.predict_proba(X3)[0][0]:.4f},  Yes: {logregIntensive.predict_proba(X3)[0][1]:.4f}')\n    \n    print('\\n.............Conclusion: ........................\\n')\n\n    if logregIntensive.predict_proba(X3)[0][1] < 0.51 and logregIntensive.predict_proba(X3)[0][1] > 0.1:\n        print('Patient with moderate probability of receiving intensive treatment.')     \n    elif logregIntensive.predict_proba(X3)[0][1] > 0.51:\n        print('Patient with high probability of receiving intensive treatment.')   \n        \n    elif logregSemiIntensive.predict_proba(X2)[0][1] < 0.51 and logregSemiIntensive.predict_proba(X2)[0][1] > 0.1:\n        print('Patient with moderate probability of receiving semi intensive treatment.')        \n    elif logregSemiIntensive.predict_proba(X2)[0][1] > 0.51:\n        print('Patient with high probability of receiving semi intensive treatment.') \n        \n    elif logregRegular.predict_proba(X1)[0][1] > 0.1 and logregRegular.predict_proba(X1)[0][1] < 0.50:\n        print('Patient with moderate probability of receiving treatment in the regular ward.') \n    elif logregRegular.predict_proba(X1)[0][1] > 0.51:\n        print('Patient with high probability of receiving treatment in the regular ward.')    \n        \n    else:\n        print('Patient without the need for hospitalization.')","b5458b80":"evaluate_sample(5)","bc963b36":"evaluate_sample(5226)","33b3ba61":"evaluate_sample(5009)","f6066d92":"evaluate_sample(111)","9b2a4cd3":"Again, the results showed good accuracy for negative results, but low accuracy for positive results. This is expected in view of the high imbalance of the dataset with a very low incidence of positive targets, as well as the large amount of null values in the dataset. ","83c33290":"### Model for the target: 'Patient addmited to semi-intensive unit (1=yes, 0=no)'**","8028e26c":"### Model for the target: 'Patient addmited to intensive care unit (1=yes, 0=no)'**","5a12d189":"Now let's test some samples.","a19056db":"# Discussion\n\nIn this work we used 2 algorithms that brought simplicity, flexibility and speed to the execution. Which were the algorithms: logistic regression and RFE (Recursive Feature Elimination). Both have advantages reported below.  \n\n## Advantages of Logistic Regression:\n\nIt is a widely used technique because it is very efficient, does not require too many computational resources, it\u2019s highly interpretable, it\u2019s easy to regularize, and it outputs well-calibrated predicted probabilities.\n\nLogistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. Therefore Feature Engineering plays an important role in regards to the performance of Logistic Regression.\n\nBecause of its simplicity and the fact that it can be implemented relatively easy and quick, Logistic Regression is also a good baseline that you can use to measure the performance of other more complex Algorithms.\nReference: https:\/\/towardsdatascience.com\/real-world-implementation-of-logistic-regression-5136cefb8125\n\n\n## Advantages of Recursive Feature Elimination\n\nRecursive Feature Elimination (RFE) as its title suggests recursively removes features, builds a model using the remaining attributes and calculates model accuracy. RFE is able to work out the combination of attributes that contribute to the prediction on the target variable (or class). \nReference: https:\/\/medium.com\/@aneesha\/recursive-feature-elimination-with-scikit-learn-3a2cbdf23fb7  \n\n\n# Conclusion\n\nThe results showed good accuracy for negative results, but low accuracy for positive results. This is expected in view of the high imbalance of the dataset with a very low incidence of positive targets, as well as the large amount of null values in the dataset.\nWe believe that with new datasets with less null values and more positive values in the targets, the generated model will have better performance.\n\nThe main advantage of the routine presented here is that it can easily be adapted to new datasets, since it practically does not require manual readjustment. The algorithm itself will reevaluate the features as well as retrain the models.\n\n\n\n**I hope you enjoyed!   \nLovingly from Brazil to help the world fight the pandemic.**\n","62a41561":"#  Topic: Diagnosis of COVID-19 and its clinical spectrum\nAI and Data Science supporting clinical decisions.\n\n# Introduction\n\n### Background\n\nThe World Health Organization (WHO) characterized the COVID-19, caused by the SARS-CoV-2, as a pandemic on March 11, while the exponential increase in the number of cases was risking to overwhelm health systems around the world with a demand for ICU beds far above the existing capacity, with regions of Italy being prominent examples.\n\nBrazil recorded the first case of SARS-CoV-2 on February 26, and the virus transmission evolved from imported cases only, to local and finally community transmission very rapidly, with the federal government declaring nationwide community transmission on March 20.\n\nUntil March 27, the state of S\u00e3o Paulo had recorded 1,223 confirmed cases of COVID-19, with 68 related deaths, while the county of S\u00e3o Paulo, with a population of approximately 12 million people and where Hospital Israelita Albert Einstein is located, had 477 confirmed cases and 30 associated death, as of March 23. Both the state and the county of S\u00e3o Paulo decided to establish quarantine and social distancing measures, that will be enforced at least until early April, in an effort to slow the virus spread.\n\nOne of the motivations for this challenge is the fact that in the context of an overwhelmed health system with the possible limitation to perform tests for the detection of SARS-CoV-2, testing every case would be impractical and tests results could be delayed even if only a target subpopulation would be tested.\n\n### Dataset\n\nThe dataset contains anonymized data from patients seen at the Hospital Israelita Albert Einstein, at S\u00e3o Paulo, Brazil, and who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.\n\nAll data were anonymized following the best international practices and recommendations. All clinical data were standardized to have a mean of zero and a unit standard deviation.\n\n# Objective\n\nThe aim of this notebook is to provide resources and insights, through data science, to performe the tasks proposed by Kaggle:\n\n### TASK 1\n\u2022 Predict confirmed COVID-19 cases among suspected cases.\nBased on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive\/negative)?\n\n### TASK 2\n\u2022 Predict admission to general ward, semi-intensive unit or intensive care unit among confirmed COVID-19 cases.\nBased on the results of laboratory tests commonly collected among confirmed COVID-19 cases during a visit to the emergency room, would it be possible to predict which patients will need to be admitted to a general ward, semi-intensive unit or intensive care unit?\n\n\n## Strategy adopted\n\nIn this work we will make use of logistic regression with recursive feature elimination from the sklearn library.\n\n\n# Code development for insights\n\nFirst, let's import the necessary libraries.\n","ca11f5ff":"The results showed good accuracy for negative results, but low accuracy for positive results. This is expected in view of the high imbalance of the dataset with a very low incidence of positive targets, as well as the large amount of null values in the dataset. ","84172877":"And check our dataset","cf26ff0f":"# Recursive Feature Elimination (RFE) and training model for Task 2\n\nFor the task 2 we will perform the same strategy.\nFirst applay the RFE and after train the logistic regression classificator for each of the targets, that are: \n* 'Patient addmited to regular ward (1=yes, 0=no)', \n* 'Patient addmited to semi-intensive unit (1=yes, 0=no)'\n* 'Patient addmited to intensive care unit (1=yes, 0=no)'.   \n   \nAfter training each model, it will be possible to make the inference separately and from the probabilities provided by each of them, classify the sample as belonging to one of the groups.\n","149c3b6d":"### Model for the target: 'Patient addmited to regular ward (1=yes, 0=no)'**","b7cfad94":"# Task 1\n\n\u2022 Predict confirmed COVID-19 cases among suspected cases. Based on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive\/negative)?","b238c3e6":"## Inferences\n\nNow for the inferences of data samples we will create a function that analyzes the probabilities and evaluates in which class of care the individual fits.","cb133a0d":"# Recursive Feature Elimination (RFE) and training model for Task 1\n\nAs we said before, in this work we will make use of logistic regression with recursive feature elimination from the sklearn library.   \nFirst we pepare our features and labels, after we perform the RFE and train the models.","5fb11cf4":"## References\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression   \nhttps:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8   \nhttps:\/\/www.kaggle.com\/residentmario\/automated-feature-selection-with-sklearn   \n"}}