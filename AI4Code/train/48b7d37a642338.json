{"cell_type":{"b014ba94":"code","5686b57c":"code","dc272dfd":"code","21afb9a5":"code","4d1893ab":"code","9ad58539":"code","c6d4d7c8":"code","269f226a":"code","7a1099d9":"code","8b7c7757":"code","955e72a9":"code","624cc19f":"code","e1cd0940":"code","82e06c9f":"code","051386ad":"code","79a34eb7":"code","fb93ae71":"code","6335855f":"code","fb7a6de4":"code","6c6181f6":"code","a5f196fa":"code","8d15a9b6":"code","8e2fddfc":"code","b774889a":"code","88031e46":"code","b10d9efa":"code","dd1e7e7a":"code","b8183e80":"code","bc35c1e6":"code","f465c348":"code","6b81cac0":"code","6aad8947":"code","ce582b62":"code","863544db":"code","a9e6703c":"code","0de64e63":"code","a1f66939":"code","7da39922":"markdown","f2c7cd11":"markdown","b0c76bfd":"markdown","5265ed17":"markdown","7b47d5be":"markdown","190f10a7":"markdown","af341bef":"markdown","100f29ce":"markdown","b2a00015":"markdown"},"source":{"b014ba94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5686b57c":"import tensorflow as tf\nimport transformers\nimport missingno as msno\nimport re\nimport spacy\nimport nltk\nfrom wordcloud import WordCloud","dc272dfd":"from sklearn.model_selection import KFold\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom transformers import TFAutoModel,AutoTokenizer,TFAutoModelForSequenceClassification\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_palette('husl')","21afb9a5":"df = pd.read_csv(\"\/kaggle\/input\/schopenhauer-work-corpus\/Schopenhauer_works_corpus.csv\")\ndf.tail()","4d1893ab":"world = df[(df['book_title']=='The World As Will And Idea (Vol. 1 of 3)')].reset_index(drop=True)\nworld.head()","9ad58539":"#8th row. And 5th column, text_clean \n\ndf.iloc[8,4]","c6d4d7c8":"print(f'length of df : {len(df)}')","269f226a":"#Code by Shubham Kumar https:\/\/www.kaggle.com\/eiann1509\/commonlit-fine-tuning-with-roberta-base \n\ndef my_plot(df,row):\n  idx=0\n  j=0\n  feat=['publishing_date','Unnamed: 0']\n  plt.rcParams['figure.figsize'] = (15,5)\n  fig,axes=plt.subplots(row,2)\n  plt.subplots_adjust(top = 1.95)  \n  for i in range(row):\n      axes[i,j].axvline(df[feat[idx]].mean(), linestyle=':', linewidth=2)\n      sns.kdeplot(df[feat[idx]],color='red',ax=axes[i,j])   \n      axes[i,j].set_title(feat[idx])\n      j+=1\n      sns.violinplot(df[feat[idx]],color='red',ax=axes[i,j])     \n      axes[i,j].set_title(feat[idx])\n      idx+=1\n      j=0","7a1099d9":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","8b7c7757":"my_plot(df,2)","955e72a9":"plt.figure(figsize=(20,10))\nplt.xticks(rotation=90)\nsns.countplot(df['publishing_date'][df['publishing_date'].notnull()]);","624cc19f":"df=df[['text_clean','publishing_date']]                 \nprint(f'range of publishing_date values : ({df.publishing_date.min()},{df.publishing_date.max()})')","e1cd0940":"df=df.rename(columns={'text_clean':'text'})\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words(\"english\")","82e06c9f":"df['publishing_date'].skew","051386ad":"cloud=WordCloud(background_color = 'black',stopwords=stopwords,max_words=200,max_font_size = 40,scale=3).generate(str(df['text']))\n\ntitle='word count'\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(top = 2.25)\nfig.suptitle(title, fontsize = 20)\nplt.imshow(cloud);","79a34eb7":"#Be careful because last column name is text_clean. I changed too upper case to avoid ambiguity  \n\ndef Text_clean(text):\n  pattern=re.compile(\"[^a-zA-Z]|https?:\/\/\\S+|www\\.\\S+\")\n  return pattern.sub(r' ',text)","fb93ae71":"x_data=df['text'].apply(lambda text:Text_clean(text).strip())","6335855f":"x_data=[\" \".join(data.split()) for data in x_data]\nx_data[:10]","fb7a6de4":"#using spacy we do lemmatization and singularization\ntrain_data=[]\nnlp=spacy.load('en_core_web_sm')\nfor data in x_data:\n  doc = nlp(data)\n  train_data.append(\" \".join([str(token.lemma_) for token in doc]))","6c6181f6":"train_data=[' '.join([word for word in data.split() if '-PRON-'!=word]) for data in train_data]\ntrain_data=[' '.join([word for word in data.split() if word not in stopwords]) for data in train_data]\ny_data=df['publishing_date'].values","a5f196fa":"x_data[:2],y_data[:2]","8d15a9b6":"train_data[:2],y_data[:2]","8e2fddfc":"#Dataset prep for linear reg and Roberta\n\nclass DATASET:\n  def __init__(self,train_data,y_data):\n    \n    self.train_data=train_data\n    self.y_data=y_data\n  \n  def __call__(self,pad_sequences,train_test_split,model_name,roberta_tokenizer):\n\n    if model_name=='LR':\n\n      x_train,x_test,y_train,y_test=train_test_split(self.train_data,self.y_data,test_size=0.3)\n      tfidf=TfidfVectorizer(analyzer='word', ngram_range=(1,3))\n      table_c=tfidf.fit_transform(list(x_train)+list(x_test))\n      train_table_data=tfidf.transform(x_train)\n      test_table_data=tfidf.transform(x_test)\n      \n      return train_table_data,test_table_data,y_train,y_test\n\n    elif model_name=='roberta':\n      sequences=[]\n      length=[]\n      for text in self.train_data:\n        tokens=roberta_tokenizer.encode(text,add_special_tokens=True, truncation=True)\n        sequences.append(tokens)\n\n      \n      roberta_data=pad_sequences(sequences,maxlen=200,padding='pre',value=roberta_tokenizer.encode('<pad>')[1])  #roberta_tokenizer.encode('<pad>')[1] is the token value for padding\n      return roberta_data,self.y_data","b774889a":"roberta_tokenizer=AutoTokenizer.from_pretrained('roberta-base')","88031e46":"data=DATASET(train_data,y_data)\nx_train_data,x_test_data,y_train_data,y_test_data=data(pad_sequences,train_test_split,'LR',roberta_tokenizer)\nx_roberta_data,y_roberta_data=data(pad_sequences,train_test_split,'roberta',roberta_tokenizer)","b10d9efa":"print(f'for linear reg training sample set: {x_train_data.shape,y_train_data.shape} and for roberta whole dataset : {x_roberta_data.shape,y_roberta_data.shape}')","dd1e7e7a":"class Linear_Model(tf.keras.Model):\n  def __init__(self,x_train_data,y_train_data):\n    self.x_train_data=x_train_data\n    self.y_train_data=y_train_data\n    self.lreg=LinearRegression()\n\n  def linear_regression_result(self,x_test_data):\n    self.lreg.fit(self.x_train_data,self.y_train_data)     #train                    \n    \n    #predict\n    return self.lreg.predict(x_test_data)","b8183e80":"linear_model=Linear_Model(x_train_data,y_train_data)\nlr_y_pred=linear_model.linear_regression_result(x_test_data)","bc35c1e6":"#performance metric evaluation on linear regression\nfrom sklearn.metrics import mean_squared_error\nprint(f'RMSE Score {mean_squared_error(lr_y_pred,y_test_data,squared=False)}')","f465c348":"class Custom_roberta(tf.keras.Model):\n\n  def __init__(self):\n    super(Custom_roberta,self).__init__()\n    self.roberta_model = TFAutoModelForSequenceClassification.from_pretrained('roberta-base',output_hidden_states=False, output_attentions=False, num_labels=1)\n\n  def call(self,input_ids):\n    x=self.roberta_model(input_ids)\n    \n\n    return x","6b81cac0":"def loss_func(y_true,y_pred):  #root mean sqruared error (RMSE) \n  return tf.sqrt(tf.reduce_mean(tf.square(y_pred-y_true)))","6aad8947":"#with fit function use simple scheduler\n#constant lr for first 10 epochs and then lr is decreased exponentially \n\ndef schedule(epochs,lr):\n    if epochs<10:\n        return lr\n        \n    else:\n        return lr * tf.math.exp(-0.01)","ce582b62":"#train_Size of the data for training and dev\n\ntrain_size=int(0.8*(len(x_roberta_data)))","863544db":"x_roberta_data.shape,y_roberta_data.shape","a9e6703c":"#k folds\ndef train_in_folds(x_roberta_data,y_roberta_data,folds):\n\n    # initiate the kfold class from model_selection module\n    kf = KFold(n_splits=folds,shuffle=True)\n    \n    for (fold, (train_index, test_index)) in enumerate(kf.split(x_roberta_data)):\n        print(f'for fold : {fold+1}\\n')\n        x_train,x_test=x_roberta_data[train_index],x_roberta_data[test_index]\n        y_train,y_test=y_roberta_data[train_index],y_roberta_data[test_index]\n\n        x_test,y_test=tf.convert_to_tensor(x_test),tf.convert_to_tensor(y_test)\n        x_train,y_train=tf.convert_to_tensor(x_train),tf.convert_to_tensor(y_train)\n        \n\n        model=Custom_roberta()\n        optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n        callback_1=tf.keras.callbacks.LearningRateScheduler(schedule)\n        model.compile(optimizer=optimizer,loss='mse',metrics=[RootMeanSquaredError()])\n        callback_2=tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',mode='min',restore_best_weights=True,patience=2)\n        \n\n        r=model.fit(x_train,y_train,batch_size=10,validation_data=(x_test,y_test),epochs=6,callbacks=[callback_1,callback_2],verbose=1)\n        print(f\"best rmse : {np.min(r.history['val_root_mean_squared_error']):.4f}\")\n        print('\\n')","0de64e63":"train_in_folds(x_roberta_data,y_roberta_data,folds=5)","a1f66939":"\n#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Shubham Kumar @eiann1509 for the script' )","7da39922":"#RoBERTa Base","f2c7cd11":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 20px; background: #001f3f;\"><i><b style=\"color:orange;\">The pinnacle of Schopenhauer's philosophical thought<\/b><\/i><\/h1><\/center>\n\n\"The World as Will and Representation marked the pinnacle of Schopenhauer's philosophical thought; he spent the rest of his life refining, clarifying, and deepening the ideas presented in this work without any fundamental changes. The first edition was met with near-universal silence. The second edition of 1844 similarly failed to attract any interest. At the time, post-Kantian German academic philosophy was dominated by the German Idealists\u2014foremost among them G. W. F. Hegel, whom Schopenhauer bitterly denounced as a \"charlatan\". It was not until the publication of his Parerga and Paralipomena in 1851 that Schopenhauer began to see the start of the recognition that eluded him for so long.\"\n\nhttps:\/\/en.wikipedia.org\/wiki\/The_World_as_Will_and_Representation","b0c76bfd":"#Code by Shubham Kumar https:\/\/www.kaggle.com\/eiann1509\/commonlit-fine-tuning-with-roberta-base","5265ed17":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 20px; background: #001f3f;\"><i><b style=\"color:orange;\">The world as Representation is The \u2018objectification\u2019 of the Will<\/b><\/i><\/h1><\/center>\n\n\"Schopenhauer identifies the thing-in-itself the inner essence of everything as will: a blind, unconscious, aimless striving devoid of knowledge, outside of space and time, and free of all multiplicity. The world as representation is, therefore, the \u2018objectification\u2019 of the will. Aesthetic experiences release a person briefly from his endless servitude to the will, which is the root of suffering. True redemption from life, Schopenhauer asserts, can only result from the total ascetic negation of the \u2018will to life.\u2019 Schopenhauer notes fundamental agreements between his philosophy, Platonism, and the philosophy of the ancient Indian Vedas.\"\n\nhttps:\/\/en.wikipedia.org\/wiki\/The_World_as_Will_and_Representation","7b47d5be":"#We can clearly see that transformer model outperforming simple linear model in terms of performance in terms of RMSE metric.\n\n#I don't know since this is my First RoBERTa","190f10a7":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 20px; background: #001f3f;\"><i><b style=\"color:orange;\">The World as Will and Representation<\/b><\/i><\/h1><\/center>\n\n\n\"The World as Will and Representation (German: Die Welt als Wille und Vorstellung) is the central work of the German philosopher Arthur Schopenhauer. The first edition was published in late 1818, with the date 1819 on the title-page. A second, two-volume edition appeared in 1844: volume one was an edited version of the 1818 edition, while volume two consisted of commentary on the ideas expounded in volume one. A third expanded edition was published in 1859, the year prior to Schopenhauer's death. In 1948, an abridged version was edited by Thomas Mann.\"\n\n\"In the summer of 1813, Schopenhauer submitted his doctoral dissertation\u2014On the Fourfold Root of the Principle of Sufficient Reason\u2014and was awarded a doctorate from the University of Jena. After spending the following winter in Weimar, he lived in Dresden and published his treatise On Vision and Colours in 1816. Schopenhauer spent the next several years working on his chief work, The World as Will and Representation. Schopenhauer asserted that the work is meant to convey a \"single thought\" from various perspectives. He develops his philosophy over four books covering epistemology, ontology, aesthetics, and ethics. Following these books is an appendix containing Schopenhauer\u2019s detailed Criticism of the Kantian Philosophy.\"\n\nhttps:\/\/en.wikipedia.org\/wiki\/The_World_as_Will_and_Representation","af341bef":"#Linear Regression model","100f29ce":"![](https:\/\/philosophymaps.files.wordpress.com\/2014\/10\/schopenhauer.png)philosophymaps.wordpress.com","b2a00015":"#Word frequency text_clean visualization in word cloud"}}