{"cell_type":{"529e684f":"code","1d963042":"code","cec5fbc4":"code","c1f25184":"code","fa5a6032":"code","912c05fe":"code","6446af64":"code","41905fe5":"code","5473fe99":"code","3224984c":"code","ca69afc7":"code","c7471a7f":"code","fa25eae7":"code","80a8172f":"code","d88cf0d5":"code","4224d946":"code","48c1b142":"code","c5c7df33":"code","49878521":"code","eb99bcbd":"code","0215aa24":"code","8c1174de":"code","57376b5d":"code","4a27a863":"code","2e0d6def":"code","1e864209":"code","30b743f6":"code","354c31bc":"code","f0e9779d":"code","4dfe2571":"code","3c6768c4":"code","2e19a813":"code","21b1e256":"code","061554eb":"code","42064863":"code","285a4182":"code","4638c883":"code","29e04c3b":"code","9da9ba86":"code","d3be6e46":"code","48ed44bb":"code","65eb0b71":"code","326e587a":"code","710e4475":"code","4ddbb32f":"code","f5f8bf5b":"code","24673dd5":"code","4ca02f81":"code","e7829abd":"code","9506b942":"code","294e7529":"code","cc5e76a6":"code","e37a55d4":"code","ba896d2c":"code","dcf9c768":"code","c78f9496":"code","c8a68385":"code","c85e979c":"code","cb0980f0":"code","3d2756c5":"code","572d5216":"code","948f3b6e":"code","3219aa3e":"code","458965dd":"code","b0b65df7":"code","6ae5e595":"code","04bf93c7":"code","919a5b43":"markdown","10a5ec08":"markdown","2903f153":"markdown","f6cef1c9":"markdown","990a720b":"markdown","bb8b8ee3":"markdown","ab24a835":"markdown","475ae469":"markdown","8bd653d2":"markdown","aaebac1e":"markdown","5f93d8e2":"markdown","0bbafd94":"markdown","9b07cafd":"markdown","4f9b918a":"markdown","3ca5a00c":"markdown","9a8c74fa":"markdown","0a96768a":"markdown","c1e65af1":"markdown","e8aa6601":"markdown","70d1edc6":"markdown","d3c40c0f":"markdown","cd7d21d8":"markdown","397fbf97":"markdown","0e79980c":"markdown","471d4b36":"markdown","ca971600":"markdown","b520a28d":"markdown","3fa6748b":"markdown","fc0b2dc3":"markdown"},"source":{"529e684f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport re\nimport time\nimport warnings\nimport sqlite3\nfrom sqlalchemy import create_engine # database connection\nimport csv\nimport os\nwarnings.filterwarnings(\"ignore\")\nimport datetime as dt\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nimport os\n","1d963042":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(train.head())\nprint('**'* 50)\nprint(test.head())","cec5fbc4":"print(train.info())\nprint('**'* 50)\nprint(test.info())","c1f25184":"n_train = train.shape[0]\nn_test = test.shape[0]\ny = train['SalePrice'].values\nprint(train['SalePrice'].value_counts())\n#print(y.value_counts())\ndata = pd.concat((train, test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)","fa5a6032":"print(data.head())\nprint(data.shape)","912c05fe":"plt.figure(figsize=(30,10))\nsns.heatmap(train.corr(),cmap='coolwarm',annot = True)\nplt.show()\n","6446af64":"sns.pairplot(train, palette='rainbow')","41905fe5":"counts, bin_edges = np.histogram(train['YearBuilt'], bins=10, density = True)\npdf = counts\/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\nplt.show();","5473fe99":"sns.lmplot(x='YearBuilt',y='SalePrice',data=train)\n","3224984c":"counts, bin_edges = np.histogram(train['YearBuilt'], bins=10, \n                                 density = True)\npdf = counts\/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\n\n\n\n\nplt.show();","ca69afc7":"plt.figure(figsize=(16,8))\nsns.boxplot(x='GarageCars',y='SalePrice',data=train)\nplt.show()","c7471a7f":"counts, bin_edges = np.histogram(train['GarageCars'], bins=10, \n                                 density = True)\npdf = counts\/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\nplt.show();","fa25eae7":"plt.figure(figsize=(16,8))\nsns.barplot(x='GarageArea',y = 'SalePrice',data=train, estimator=np.mean)\nplt.show()","80a8172f":"counts, bin_edges = np.histogram(train['GarageArea'], bins=10, \n                                 density = True)\npdf = counts\/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\nplt.show();","d88cf0d5":"plt.figure(figsize=(16,8))\nsns.barplot(x='FullBath',y = 'SalePrice',data=train)\nplt.show()","4224d946":"counts, bin_edges = np.histogram(train['FullBath'], bins=10, \n                                 density = True)\npdf = counts\/(sum(counts))\nprint(pdf);\nprint(bin_edges);\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)\nplt.show();","48c1b142":"sns.lmplot(x='1stFlrSF',y='SalePrice',data=train)\n\n","c5c7df33":"sns.boxplot(x='1stFlrSF',y='SalePrice',data=train)","49878521":"data = data[['LotArea','Street', 'Neighborhood','Condition1', 'Condition2','BldgType','HouseStyle','OverallCond', 'Heating','CentralAir','Electrical','1stFlrSF','2ndFlrSF','BsmtHalfBath','FullBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','GarageCars','GarageArea','PoolArea']]","eb99bcbd":"data.info()","0215aa24":"data['BsmtHalfBath'] = data['BsmtHalfBath'].fillna(data['BsmtHalfBath'].mean())\ndata['GarageCars'] = data['GarageCars'].fillna(data['GarageCars'].mean())\ndata['GarageArea'] = data['GarageArea'].fillna(data['GarageArea'].mean())\n#data['Electrical']=data['Electrical'].fillna(' ')\n\n","8c1174de":"# Categorical boolean mask\ncategorical_feature_mask = data.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols = data.columns[categorical_feature_mask].tolist()\nprint(categorical_cols)\nprint(\"number of categorical features \",len(categorical_cols))\n\n# i in range(len(categorical_cols)):\n # data[i]=data[i].fillna(' ')\n#data['Electrical'] = data['Electrical'].fillna(' ')\n\n","57376b5d":"data = pd.get_dummies(data, columns=categorical_cols)","4a27a863":"data.info()","2e0d6def":"data.shape","1e864209":"train =data[:n_train]\ntest = data[n_train:]\nprint(train.info())\nprint(test.shape)","30b743f6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.1, random_state=101)","354c31bc":"# we are going to scale to data\n\ny_train= y_train.reshape(-1,1)\ny_test= y_test.reshape(-1,1)\nprint(X_train.info())\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.fit_transform(X_test)\ny_train = sc_X.fit_transform(y_train)\ny_test = sc_y.fit_transform(y_test)","f0e9779d":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nprint(lm)","4dfe2571":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)\/(C.sum(axis=1))) = [[1\/3, 3\/7]\n    #                           [2\/3, 4\/7]]\n\n    # ((C.T)\/(C.sum(axis=1))).T = [[1\/3, 2\/3]\n    #                           [3\/7, 4\/7]]\n    # sum of row elements = 1\n    \n    B =(C\/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C\/C.sum(axis=0)) = [[1\/4, 2\/6],\n    #                      [3\/4, 4\/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","3c6768c4":"# print the intercept\nprint(lm.intercept_)","2e19a813":"print(lm.coef_)\n","21b1e256":"predictions = lm.predict(X_test)\npredictions= predictions.reshape(-1,1)\n#plot_confusion_matrix(y_test, predictions)","061554eb":"plt.figure(figsize=(15,8))\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()\n","42064863":"from sklearn import metrics","285a4182":"print('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n#print(log_loss(y_test, predictions))\n","4638c883":"from sklearn import ensemble\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error, r2_score","29e04c3b":"params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)","9da9ba86":"clf_pred=clf.predict(X_test)\nclf_pred= clf_pred.reshape(-1,1)","d3be6e46":"print('MAE:', metrics.mean_absolute_error(y_test, clf_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, clf_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, clf_pred)))","48ed44bb":"plt.figure(figsize=(15,8))\nplt.scatter(y_test,clf_pred, c= 'brown')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()\nplt.plot(y_test,clf_pred, c= 'blue')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()","65eb0b71":"from sklearn.tree import DecisionTreeRegressor\ndtreg = DecisionTreeRegressor(random_state = 100)\ndtreg.fit(X_train, y_train)\n","326e587a":"dtr_pred = dtreg.predict(X_test)\ndtr_pred= dtr_pred.reshape(-1,1)","710e4475":"print('MAE:', metrics.mean_absolute_error(y_test, dtr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, dtr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_pred)))","4ddbb32f":"plt.figure(figsize=(15,8))\nplt.scatter(y_test,dtr_pred,c='green')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()\nplt.plot(y_test,clf_pred, c= 'blue')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()","f5f8bf5b":"from sklearn.svm import SVR\nsvr = SVR(kernel = 'rbf')\nsvr.fit(X_train, y_train)\n","24673dd5":"svr_pred = svr.predict(X_test)\nsvr_pred= svr_pred.reshape(-1,1)","4ca02f81":"print('MAE:', metrics.mean_absolute_error(y_test, svr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, svr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))","e7829abd":"plt.figure(figsize=(15,8))\nplt.scatter(y_test,svr_pred, c='red')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()\nplt.plot(y_test,clf_pred, c= 'blue')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()","9506b942":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 1500, random_state = 0)\nrfr.fit(X_train, y_train)\n","294e7529":"rfr_pred= rfr.predict(X_test)\nrfr_pred = rfr_pred.reshape(-1,1)","cc5e76a6":"print('MAE:', metrics.mean_absolute_error(y_test, rfr_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, rfr_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rfr_pred)))","e37a55d4":"plt.figure(figsize=(15,8))\nplt.scatter(y_test,rfr_pred, c='orange')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()\nplt.plot(y_test,clf_pred, c= 'blue')\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()","ba896d2c":"error_rate=np.array([metrics.mean_squared_error(y_test, predictions),metrics.mean_squared_error(y_test, clf_pred),metrics.mean_squared_error(y_test, dtr_pred),metrics.mean_squared_error(y_test, svr_pred),metrics.mean_squared_error(y_test, rfr_pred)])\n\n\nprint(min(metrics.mean_squared_error(y_test, predictions),min(metrics.mean_squared_error(y_test, clf_pred),min(metrics.mean_squared_error(y_test, svr_pred),metrics.mean_squared_error(y_test, dtr_pred)))))","dcf9c768":"plt.figure(figsize=(16,5))\nprint(error_rate)\nplt.plot(error_rate)\nplt.scatter(error_rate,range(1,6))\nseed = 7\n# prepare models\nmodels = ['SVM','RANDOM_FOREST','LR','BGT','DT']\n\n","c78f9496":"a = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","c8a68385":"test_id = a['Id']\nprint(test_id.shape)\n#making dataframe \na = pd.DataFrame(test_id, columns=['Id'])","c85e979c":"test = sc_X.fit_transform(test)","cb0980f0":"test.shape","3d2756c5":"test_prediction_svr=svr.predict(test)\ntest_prediction_svr= test_prediction_svr.reshape(-1,1)","572d5216":"test_prediction_svr","948f3b6e":"test_prediction_svr =sc_y.inverse_transform(test_prediction_svr)\ntest_prediction_svr","3219aa3e":"test_pred_svr = pd.DataFrame(test_prediction_svr, columns=['SalePrice'])\n#test_pred_svr","458965dd":"test_pred_svr.head()","b0b65df7":"result = pd.concat([a,test_pred_svr], axis=1)","6ae5e595":"result.head()","04bf93c7":"result.to_csv('..\/submission.csv',index=False)","919a5b43":"<a id=\"1\"><\/a> <br>\n# **Random Forest Regression **","10a5ec08":"<a id=\"1\"><\/a> <br>\n### **Train Test Split **\nNow let's split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model.","2903f153":"**Box plot for GarageCars feature**","f6cef1c9":"# Model Comparison","990a720b":"## **confusion ,Recall,Precision matrix function for performance checking**","bb8b8ee3":"## **Onehot encoding on categorical data**","ab24a835":"<a id=\"1\"><\/a> <br>\n# **Linear Regression**\n\nLet's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can't use.","475ae469":"**We can say the best working model by loking MSE rates The best working model is Support Vector Machine.**\nWe are going to see the error rate. which one is better?\n","8bd653d2":"# Data Visualization on training + test set","aaebac1e":"We are going to convert train ad test data. And We are going to drop SalePrice column for predict.","5f93d8e2":"<a id=\"1\"><\/a> <br>\n# **Decision Tree Regression **","0bbafd94":"**Heatmap for train set**","9b07cafd":"# Feature Engineering","4f9b918a":"Now we will use test data .","3ca5a00c":"**CDF And Pdf for yearbuilt feature**","9a8c74fa":"**Pair plot**","0a96768a":"We have to convert all columns into numeric or categorical data.","c1e65af1":"<a id=\"1\"><\/a> <br>\n# **Gradient Boosting Regression **\n\nGradient Boosting trains many models in a gradual, additive and sequential manner. The major difference between AdaBoost and Gradient Boosting Algorithm is how the two algorithms identify the shortcomings of weak learners (eg. decision trees). While the AdaBoost model identifies the shortcomings by using high weight data points, gradient boosting performs the same by using gradients in the loss function (y=ax+b+e , e needs a special mention as it is the error term). The loss function is a measure indicating how good are model\u2019s coefficients are at fitting the underlying data. A logical understanding of loss function would depend on what we are trying to optimise. We are trying to predict the sales prices by using a regression, then the loss function would be based off the error between true and predicted house prices.","e8aa6601":"<a id=\"1\"><\/a> <br>\n### **Model Evaluation **\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.","70d1edc6":"<a id=\"1\"><\/a> <br>\n### **Creating and Training the Model **","d3c40c0f":"\n**Let's get started!**\n\n\nBuilding  my first Kernel, Hope you like it ! \n\n### Import Libraries","cd7d21d8":"**prediction with SVM Model**","397fbf97":"<a id=\"1\"><\/a> <br>\n### **Predictions from our Model **\nLet's grab predictions off our test set and see how well it did!","0e79980c":"\nThe decision tree is a simple machine learning model for getting started with regression tasks.\n\n**Background**\nA decision tree is a flow-chart-like structure, where each internal (non-leaf) node denotes a test on an attribute, each branch represents the outcome of a test, and each leaf (or terminal) node holds a class label. The topmost node in a tree is the root node. (see here for more details).\nNot suited for large dataset because of it complexity\n","471d4b36":"LMPLOT for yearbuilt","ca971600":"<a id=\"1\"><\/a> <br>\n### **Regression Evaluation Metrics**\n\nHere are three common evaluation metrics for regression problems:\n\n**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n\n1\ud835\udc5b\u2211\ud835\udc56=1\ud835\udc5b|\ud835\udc66\ud835\udc56\u2212\ud835\udc66\u0302 \ud835\udc56|\n \n**Mean Squared Error (MSE)** is the mean of the squared errors:\n\n1\ud835\udc5b\u2211\ud835\udc56=1\ud835\udc5b(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\u0302 \ud835\udc56)2\n \n \n**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n\n1\ud835\udc5b\u2211\ud835\udc56=1\ud835\udc5b(\ud835\udc66\ud835\udc56\u2212\ud835\udc66\u0302 \ud835\udc56)\n \n**Comparing these metrics**:\n\nMAE is the easiest to understand, because it's the average error.\nMSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\nRMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\nAll of these are loss functions, because we want to minimize them.","b520a28d":"<a id=\"1\"><\/a> <br>\n# **Support Vector Machine Regression **","3fa6748b":"What is the data trying to say to us ?","fc0b2dc3":"**Filling NULL values**"}}