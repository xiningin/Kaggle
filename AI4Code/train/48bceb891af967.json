{"cell_type":{"d0baa1b9":"code","ba941235":"code","6af634f4":"code","b77d43bc":"code","12c189e1":"code","c75b75e3":"code","80a953c4":"code","5ed3c58e":"code","4239109a":"code","454bc12e":"code","9b562326":"code","0e6a80a9":"code","11d287ca":"code","ae69dfae":"code","943ea2e2":"code","a5638e20":"code","e6f466c6":"code","6c1c1efa":"code","a9f61708":"code","dfa32a72":"code","2012331c":"markdown","8d4960a6":"markdown","3b203eff":"markdown","5f53ede5":"markdown","caad728b":"markdown","78abbdff":"markdown","6e8fbdf2":"markdown","80f02390":"markdown","f78eb625":"markdown"},"source":{"d0baa1b9":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","ba941235":"import warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as tlp\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings","6af634f4":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","b77d43bc":"from typing import Dict\n\nfrom tempfile import gettempdir\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom l5kit.configs import load_config_data\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\nfrom l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.geometry import transform_points\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\nfrom prettytable import PrettyTable\nfrom pathlib import Path\n\nimport os","12c189e1":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager(None)\n# get config\ncfg = {\n    'format_version': 4,\n    'model_params': {\n        'history_num_frames': 15,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [331, 331],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    'train_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 8,\n        'shuffle': True,\n        'num_workers': 1\n    },\n    \n    'test_data_loader': {\n        'key': 'scenes\/test.zarr',\n        'batch_size': 12,\n        'shuffle': False,\n        'num_workers': 1\n    }\n\n}\n\nTRAIN_MODE = False","c75b75e3":"# ===== INIT DATASET\nif TRAIN_MODE:\n    train_cfg = cfg[\"train_data_loader\"]\n    rasterizer = build_rasterizer(cfg, dm)\n    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n    train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], \n                                  batch_size=train_cfg[\"batch_size\"], \n                                 num_workers=train_cfg[\"num_workers\"])\n    print(train_dataset)","80a953c4":"def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https:\/\/arxiv.org\/abs\/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https:\/\/ngc.nvidia.com\/catalog\/model-scripts\/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width \/ 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https:\/\/arxiv.org\/abs\/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\n\ndef _resnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet18(pretrained=False, progress=True, **kwargs):\n    r\"\"\"ResNet-18 model from\n    `\"Deep Residual Learning for Image Recognition\" <https:\/\/arxiv.org\/pdf\/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n","5ed3c58e":"import torch\nimport torch.nn as nn\n\nclass LyftModel(nn.Module):\n    def __init__(self, cfg):\n        super(LyftModel, self).__init__()\n        # load pre-trained Conv2D model\n        self.backbone = resnet18(pretrained=False, progress=False)\n        self.backbone.load_state_dict(\n            torch.load(\n                '..\/input\/resnet18\/resnet18.pth'\n            )\n        )\n        # change input channels number to match the rasterizer's output\n        num_history_channels = (cfg['model_params']['history_num_frames']+1) * 2\n        num_in_channels = 3 + num_history_channels\n        self.backbone.conv1 = nn.Conv2d(\n            num_in_channels,\n            self.backbone.conv1.out_channels,\n            kernel_size=self.backbone.conv1.kernel_size,\n            stride=self.backbone.conv1.stride,\n            padding=self.backbone.conv1.padding,\n            bias=False\n        )\n        \n        # change output size to (X, Y) * number of future states\n        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n        self.backbone.fc = nn.Linear(in_features=512, out_features=num_targets)        \n    \n    def forward(self, x):\n        # Forward pass\n        return self.backbone(x)","4239109a":"def build_model(cfg: Dict) -> torch.nn.Module:\n    # load pre-trained Conv2D model\n    model = LyftModel(cfg)\n    ckpt = torch.load('..\/input\/lyftmodelall\/resnet18_15hist_331px.pth', map_location=torch.device('cpu'))\n    model.load_state_dict(ckpt)\n    return model","454bc12e":"build_model(cfg)","9b562326":"class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches \/\/ 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '\/' + fmt.format(num_batches) + ']'","0e6a80a9":"def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n    # Train\n    batch_time = AverageMeter('Time', ':6.1f')\n    data_time = AverageMeter('Data', ':6.1f')\n    losses = AverageMeter('Loss', ':6.1e')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, data_time, losses],\n        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n    )\n    criterion = nn.MSELoss(reduction=\"none\")\n    model.train()\n    end = time.time()\n    best = 1000\n    for i, data in enumerate(train_loader):\n        data_time.update(time.time()-end)\n        inputs = data[\"image\"].to(device)\n        target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n        targets = data[\"target_positions\"].to(device)\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(inputs).reshape(targets.shape)\n        loss = criterion(outputs, targets)\n        # not all the output steps are valid, but we can filter them out from the loss using availabilities\n        loss = loss * target_availabilities\n        loss = loss.mean()\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        losses.update(loss.item(), inputs.size(0))\n        scheduler.step(metrics=loss)\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if i % 50 == 0:\n            progress.display(i)\n            if losses.avg< best:\n                best = losses.avg \n                xm.save(model.state_dict(), \"model.bin\")\n                xm.master_print(f'Model Saved.{best}')\n        \n    del loss\n    del outputs\n    gc.collect()\n    \n    ","11d287ca":"from torch.utils.data.distributed import DistributedSampler\nimport torch_xla.debug.metrics as met\nWRAPPED_MODEL = xmp.MpModelWrapper(build_model(cfg))\n\ndef _run():\n    TRAIN_BATCH_SIZE = 8\n    EPOCHS = 1\n    xm.master_print('Starting Run ...')\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Train Loader Created.')\n    \n    num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size())\n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n    xm.master_print('Done Model Loading.')\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n                                                           factor=0.8, patience=3, \n                                                           verbose=False, \n                                                           threshold=0.0001, min_lr=0.00001)\n    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n    \n    for epoch in range(EPOCHS):\n        para_loader = tlp.ParallelLoader(train_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Training ...')\n        train_loop_fn(para_loader.per_device_loader(device),\n                      model,  \n                      optimizer, \n                      device, \n                      scheduler, \n                      epoch\n                     )\n        \n        xm.master_print(\"Finished training epoch {}\".format(epoch))\n        if epoch == EPOCHS-1:\n            xm.master_print('Saving Model ..')\n            xm.save(model.state_dict(), \"model.bin\")\n            xm.master_print('Model Saved.')\n            \n    if METRICS_DEBUG:\n      xm.master_print(met.metrics_report(), flush=True)","ae69dfae":"import time\nfrom torch.nn import functional as F\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    _run()\n\nFLAGS={}\nif TRAIN_MODE:\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","943ea2e2":"import numpy as np\nimport os\nimport torch\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom typing import Dict\n\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.evaluation import write_pred_csv\nfrom l5kit.rasterization import build_rasterizer","a5638e20":"DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}\/single_mode_sample_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}\/multi_mode_sample_submission.csv\"\n\n# Training notebook's output.","e6f466c6":"model = LyftModel(cfg)\nckpt = torch.load('..\/input\/lyftmodelall\/resnet18_15_331px.bin')\nmodel.load_state_dict(ckpt)","6c1c1efa":"# ===== INIT DATASET\ntest_cfg = cfg[\"test_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Test dataset\/dataloader\ntest_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\ntest_mask = np.load(f\"{DIR_INPUT}\/scenes\/mask.npz\")[\"arr_0\"]\ntest_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\ntest_dataloader = DataLoader(test_dataset,\n                             shuffle=test_cfg[\"shuffle\"],\n                             batch_size=test_cfg[\"batch_size\"],\n                             num_workers=test_cfg[\"num_workers\"])\n\n\nprint(test_dataloader)","a9f61708":"def _test():\n    \n\n    future_coords_offsets_pd = []\n    timestamps = []\n    agent_ids = []\n    device = 'xla:0'\n    print(f\"device: {device} ready!\")\n    model = LyftModel(cfg)\n    ckpt = torch.load('..\/input\/lyftmodelall\/effnet0l2binay_368.bin')\n    model.load_state_dict(ckpt)\n    model = model.to(device)\n    model.eval()\n    with torch.no_grad():\n        dataiter = tqdm(test_dataloader)\n\n        for data in dataiter:\n            inputs = data[\"image\"].to(device)\n            target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n            targets = data[\"target_positions\"].to(device)\n\n            outputs = model(inputs).reshape(targets.shape)\n\n            future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n            timestamps.append(data[\"timestamp\"].numpy().copy())\n            agent_ids.append(data[\"track_id\"].numpy().copy())\n    write_pred_csv('submission.csv',\n               timestamps=np.concatenate(timestamps),\n               track_ids=np.concatenate(agent_ids),\n               coords=np.concatenate(future_coords_offsets_pd))","dfa32a72":"def _mp_fn(rank, flags):\n#     torch.set_default_tensor_type('torch.FloatTensor')\n    _test()\n\nFLAGS={}\nif not TRAIN_MODE:\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","2012331c":"Due to the fact that the following steps take way too long, they are commented out.","8d4960a6":"## Model Define: Efficientnet-b0 + L2norm + Binary Head","3b203eff":"Config is where you can make your changes to have different `model_architecture`, `history_step_size`, `history_num_frames`, `batch_size`, etc. Inspect `cfg` for more details.","5f53ede5":"### Baseline Model\n\nThis baseline model is adopted from [Lyft's example](https:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/agent_motion_prediction\/agent_motion_prediction.ipynb) on their l5kit repo.","caad728b":"### Importing PyTorch and l5kit","78abbdff":"### Installing l5kit","6e8fbdf2":"### TEST MODE","80f02390":"### Loading the data","f78eb625":"### Prepare data path and config file"}}