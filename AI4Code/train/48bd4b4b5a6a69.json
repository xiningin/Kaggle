{"cell_type":{"535e8bbc":"code","c48215f0":"code","8a92e47f":"code","f4c2976d":"code","abdb45bc":"code","388daeee":"code","7277967e":"code","732077ea":"code","055ee477":"code","883a573f":"code","c80d8683":"code","e5cad826":"code","46bab9be":"code","82085aba":"code","bb7f7ed4":"code","71517108":"code","3ca1192d":"code","404f65b3":"code","4f4d1499":"code","9afa01d8":"code","d062c343":"code","36d7940a":"code","a7e051d1":"code","1948e64b":"code","e5e84e2b":"code","0241c063":"code","ff33c9e5":"code","63dac341":"code","22a834be":"code","eb72007e":"code","ba2e809d":"code","2457c73f":"code","b60013b1":"code","15eb935a":"code","ab7b2e7e":"code","e3623ee1":"code","e0a50daa":"code","331f4a88":"code","7f1c6143":"code","af2ef495":"code","337dbebd":"code","ed4304b8":"code","ba4df0ed":"code","0079ea41":"code","387ab502":"code","127bd391":"code","445319db":"code","58f8461d":"code","4a8b1a03":"code","48a78c69":"code","21ac8455":"code","a0b6934c":"code","5fc0dcef":"code","2e259f24":"code","cf754fa0":"code","a61fa8a6":"code","3356fe32":"code","ef738c61":"code","9970b564":"code","055c3234":"code","ce5e068f":"code","421d9c44":"code","f66ed76f":"code","7bfc8b1d":"code","24fe826b":"code","78f8e038":"code","81365c3c":"code","40e472ba":"code","eeeee621":"code","60c4f59d":"code","2fa539e2":"code","8ef4a2a6":"code","34c038ea":"code","57c18375":"code","7a602c79":"code","15254f01":"code","0e5de173":"code","4651b219":"code","8b0ac7b1":"code","67c89c15":"code","2f373c68":"code","20c55f31":"code","2546a660":"code","829d00c1":"code","27e4a7fb":"code","c1afe761":"code","1e09d469":"code","a500f875":"code","0f0b5f81":"code","9b72c142":"code","8406628d":"code","c45175d2":"code","245081b8":"code","b8653d24":"code","462edf6f":"code","a8f461d2":"code","8b8e4c62":"code","7a1c6026":"code","a309121b":"code","3f3e383d":"code","c5ffcda1":"code","0b52bf22":"code","2127a9eb":"code","2ec32e62":"code","049cc1f6":"code","53393ff6":"code","ba0e0393":"code","039f61b8":"code","8d84ec14":"code","57a0559e":"code","ab9146f9":"code","cca60ad4":"code","e5981f10":"code","b30b970a":"code","a4c40ff1":"code","c1e2feef":"markdown","437628fd":"markdown","9661251c":"markdown","ee71299b":"markdown","03395dc1":"markdown","52ab307d":"markdown","341dec8d":"markdown","c0828c58":"markdown","760e3724":"markdown","7cd338aa":"markdown","6a111ff5":"markdown","a6a83595":"markdown","656c4e32":"markdown","824db048":"markdown","c8c3087e":"markdown","175e1382":"markdown","d3025f95":"markdown","956ce448":"markdown","65d63dda":"markdown","4508fb47":"markdown","dd236b36":"markdown","28f07e9e":"markdown","433b86d7":"markdown"},"source":{"535e8bbc":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport sys\n\n# Bibliteca para Visualiza\u00e7\u00e3o\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Biblioteca para Manipula\u00e7\u00e3o de Dados\nimport pandas as pd\nimport numpy as np\n\n# Biblioteca para An\u00e1lise e modelagem de s\u00e9ries temporais\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Imports para formata\u00e7\u00e3o dos gr\u00e1ficos\nimport matplotlib.cbook\nimport matplotlib as m\nm.rcParams['axes.labelsize'] = 14\nm.rcParams['xtick.labelsize'] = 12\nm.rcParams['ytick.labelsize'] = 12\nm.rcParams['text.color'] = 'k'\n\n# Imports para cria\u00e7\u00e3o e valida\u00e7\u00e3o dos modelos temporais\nfrom statsmodels.tsa.arima_model import ARIMA\nimport sklearn\nfrom sklearn.metrics import mean_squared_error \nimport itertools\n\n# Import para Padroniza\u00e7\u00e3o dos dados\nfrom sklearn.preprocessing import StandardScaler\n\n# Import utilizado para Feature Selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFECV\n\n# Import utilizado para an\u00e1lise de MultiColinearidade dos Dados\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Import para An\u00e1lise de Estacionaridade nos Modelos de S\u00e9ries Temporais\nfrom statsmodels.tsa.stattools import adfuller\n\n# Import para obter os feriados usados no Modelo SARIMAX Ex\u00f3geno\nimport holidays\n\n# Import Modelo de S\u00e9ries Temporais Multivariado\nfrom statsmodels.tsa.vector_ar.var_model import VAR\n\n# Imports para o modelo Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Import para aplicar Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\n# Import para definir os KFold do CV\nfrom sklearn.model_selection import RepeatedKFold\n\n# Import para Otimiza\u00e7\u00e3o de HiperParametros\nfrom sklearn.model_selection import GridSearchCV\n\n# Import para o modelo de Regress\u00e3o Linear\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport statsmodels.stats as sms\n\nfrom matplotlib.pylab import rcParams \nrcParams['figure.figsize'] = 20,10\nmatplotlib.style.use('ggplot')\n\n%matplotlib inline","c48215f0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8a92e47f":"# Import Dataset de Treino.\ndf_train = pd.read_csv('\/kaggle\/input\/dataset_training.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/dataset_training.csv')","f4c2976d":"# Unificando o df_train e df_test pois l\u00e1 na frente iremos utilizar s\u00e9ries temporais e os dados a serem previstos \n# devem ser os mais atuais poss\u00edvel.\ndf_full = pd.concat([df_train, df_test])\ndf_full = df_full.sort_values(['date'], ascending=True)\ndf_full.head()","abdb45bc":"df_train.shape","388daeee":"df_test.shape","7277967e":"df_full.shape","732077ea":"# Considerando a vari\u00e1vel rv1 como a TARGET do dataset\ncolumn_target = 'rv1'","055ee477":"# Analisando as informa\u00e7\u00f5es de cada dado\ndf_full.info()","883a573f":"# Resumo estat\u00edstico dos dados\ndf_full.describe()","c80d8683":"def formata_dados(dataset):\n    # Convertendo a coluna date para o formato DateTime\n    dataset['date'] = pd.to_datetime(dataset['date'], format='%Y-%m-%d %H:%M:%S')\n     \n    # Aplicando o Split de alguns dados do campo Date  Obs.: O Campo Day_of_week vai ser transformado em inteiro.\n    dataset['Month'] = dataset['date'].dt.month\n    dataset['day'] = dataset['date'].dt.day\n    dataset['hour'] = dataset['date'].dt.hour\n    dataset['Day_of_week'] = dataset['date'].dt.dayofweek\n\n    # Renomeando a vari\u00e1vel WeekStatus para Weekend. Essa vari\u00e1vel ter\u00e1 valores 0 e 1. {0 : Weekday , 1: Weekend}\n    dataset.rename(columns={'WeekStatus':'Weekend'}, inplace=True)\n\n    # Segunda Feira = 0 ... Sabado = 5, Domingo = 6\n    dataset['Weekend'] = 0\n    dataset.loc[(dataset.Day_of_week == 5) | (dataset.Day_of_week == 6), 'Weekend'] = 1    \n    \n    # Padronizando o nome das colunas para Lower\n    dataset.columns = map(str.lower, dataset.columns)\n    \n    return dataset","e5cad826":"df_train = formata_dados(df_train)\ndf_test = formata_dados(df_test)\ndf_full = formata_dados(df_full)","46bab9be":"df_full.columns ","82085aba":"df_full.head(5)","bb7f7ed4":"df_full.hist(figsize=(10,10));","71517108":"# Quantidade de dados coletados Weekend e WeekDay\nfig, ax = plt.subplots(figsize = (10,6))\n\nsns.countplot(df_full['weekend'])\n\nax.set_title('Dados Coletados Weekday vs Weekend')\nax.set_ylabel('Quantidade')\nax.set_xlabel('0: Weekday \/ 1: Weekend');","3ca1192d":"fig, ax = plt.subplots(figsize = (10,6))\n\ndf_full.groupby('weekend').mean()[column_target].plot(kind='bar')\n\nax.set_title('Gasto M\u00e9dio Energia Weekday vs Weekend')\nax.set_ylabel('Volume Energia')\nax.set_xlabel('0: Weekday \/ 1: Weekend');","404f65b3":"df_full.groupby('day_of_week').mean()[column_target]","4f4d1499":"fig, ax = plt.subplots(figsize = (10,6))\n\ndf_full.groupby('day_of_week').mean()[column_target].plot(kind='bar')\n\nax.set_title('M\u00e9dia Energia Gasta por Dia')\nax.set_ylabel('Volume Energia')\nax.set_xlabel('Dia Semana');","9afa01d8":"corrmat = df_full.corr()\ntop_corr_features = corrmat.index\n\nplt.figure(figsize=(20,20))\nplt.title('\\nHeatmap Correla\u00e7\u00e3o de Vari\u00e1veis\\n', fontsize=18)\n\ng=sns.heatmap(df_full[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d062c343":"# Variavel nsm e hour s\u00e3o altamente correlacionadas. Drop Coluna Hour.\ndf_train.drop('hour', axis=1, inplace=True)\ndf_test.drop('hour', axis=1, inplace=True)\ndf_full.drop('hour', axis=1, inplace=True)","36d7940a":"# Variavel rv1 e rv2 s\u00e3o altamente correlacionadas e possuem a mesma informa\u00e7\u00e3o. Vamos Dropar uma das colunas.\ndf_train.drop('rv2', axis=1, inplace=True)\ndf_test.drop('rv2', axis=1, inplace=True)\ndf_full.drop('rv2', axis=1, inplace=True)","a7e051d1":"# Definindo a variavel date como Index do Dataset. \n# Essa opera\u00e7\u00e3o transforma os dados em s\u00e9ries possibilitando a an\u00e1lise como Series Temporais.\ndf_full.index = df_full['date']\ndf_full = df_full.drop('date', 1)\ndf_full.head(5)","1948e64b":"df_train.index = df_train['date']\ndf_train = df_train.drop('date', 1)\n\ndf_test.index = df_test['date']\ndf_test = df_test.drop('date', 1)","e5e84e2b":"# Utilizando Random Forest Regressor para identificar as melhores vari\u00e1veis preditoras\nX = df_full.loc[:, df_full.columns != column_target]\ny = df_full.loc[:, df_full.columns == column_target]\nmodel2 = RandomForestRegressor()\nrfecv2 = RFECV(estimator=model2, cv=4)\nm_rfecv = rfecv2.fit(X,y)","0241c063":"# Plotando o resultado do feature selection com RandomForestRegressor\nplt.figure()\nplt.title(\"\\n Feature Selection com RandomForestRegressor\\n\")\nplt.xlabel(\"\\nN\u00famero de Features Consideradas\")\nplt.ylabel(\"\\nCross validation score (# Classifica\u00e7\u00f5es corretas)\")\nplt.plot(range(1, len(m_rfecv.grid_scores_) + 1), m_rfecv.grid_scores_)\nplt.show()\n\n# Print dos resultados\nprint(\"\\nVari\u00e1veis Preditoras:\", X.columns[:-1])\nprint(\"\\nVari\u00e1veis Selecionadas: %s\" % m_rfecv.support_)\nprint(\"\\nRanking dos Atributos: %s\" % m_rfecv.ranking_)\nprint(\"\\nN\u00famero de Melhores Atributos: %d\" % m_rfecv.n_features_)","ff33c9e5":"# Analisando Multicolinearidade entre os dados\nvif = pd.DataFrame()\nvif['Feature']= df_full.loc[:, df_full.columns != column_target].columns\nvif['VIF Factor'] = [variance_inflation_factor(df_full.loc[:, df_full.columns != column_target].values, i) for i in range(df_full.loc[:, df_full.columns != column_target].shape[1])]\n\nvif.round(1).head(100)","63dac341":"# Pegando a data a cada X dias\ndef qtd_data(df_series, qtd):\n    list_date = []\n    control = 0\n    \n    for i in df_series.index.values:\n        if control > qtd:\n            control = 1        \n    \n        if control == 0 or control == qtd:\n            list_date.append(i)\n            control = control + 1\n        else:\n            control = control + 1        \n    \n    return list_date","22a834be":"df_full_series_Dia = df_full[column_target].resample('D').mean()","eb72007e":"mean_ = [np.mean(df_full_series_Dia[:x]) for x in range(len(df_full_series_Dia))]\nmean_series_Dia = pd.Series(mean_)\nmean_series_Dia.index = df_full_series_Dia.index\n\nfig, ax = plt.subplots(figsize = (12,4))\nplt.plot(df_full_series_Dia, label = 'Gasto Di\u00e1rio')\nplt.plot(mean_series_Dia, label = 'M\u00e9dia')\nplt.legend()\nplt.xticks(rotation = 90)\nplt.xticks(qtd_data(df_full_series_Dia, 7))\n\nax.set_title('Gasto M\u00e9dio Energia Di\u00e1rio');","ba2e809d":"matplotlib.style.use('ggplot')\n\nfig, ax = plt.subplots(figsize = (12,12))\n\nres = seasonal_decompose(df_full_series_Dia)\n\nplt.subplot(411)\nplt.plot(res.observed, label = 'S\u00e9rie Original')\nplt.legend(loc = 'best')\nplt.xticks(rotation = 90)\nplt.xticks(qtd_data(df_full_series_Dia, 7))\nplt.title('An\u00e1lise Gasto M\u00e9dio Energia Di\u00e1rio')\n\nplt.subplot(412)\nplt.plot(res.trend, label = 'Tend\u00eancia')\nplt.legend(loc = 'best')\nplt.xticks(rotation = 90)\nplt.xticks(qtd_data(df_full_series_Dia, 7))\n\nplt.subplot(413)\nplt.plot(res.seasonal, label = 'Sazonalidade')\nplt.legend(loc = 'best')\nplt.xticks(rotation = 90)\nplt.xticks(qtd_data(df_full_series_Dia, 7))\n\nplt.subplot(414)\nplt.plot(res.resid, label = 'Res\u00edduos')\nplt.legend(loc = 'best')\nplt.xticks(rotation = 90)\nplt.xticks(qtd_data(df_full_series_Dia, 7))\n\nplt.tight_layout();","2457c73f":"# Fun\u00e7\u00e3o para testar a estacionaridade\ndef testa_estacionaridade(serie, tipo):\n    if tipo is None:\n        tipo = ''\n    else:\n        tipo = '('+tipo+')'\n    \n    \n    # Calcula estat\u00edsticas m\u00f3veis\n    rolmean = serie.rolling(window = 12).mean()\n    rolstd = serie.rolling(window = 12).std()\n\n    # Plot das estat\u00edsticas m\u00f3veis\n    orig = plt.plot(serie, color = 'blue', label = 'Original')\n    mean = plt.plot(rolmean, color = 'red', label = 'M\u00e9dia M\u00f3vel')\n    std = plt.plot(rolstd, color = 'black', label = 'Desvio Padr\u00e3o')\n    plt.legend(loc = 'best')\n    plt.title('Estat\u00edsticas M\u00f3veis - M\u00e9dia e Desvio Padr\u00e3o ' + tipo)\n    plt.xticks(rotation = 45)\n    plt.show()\n    \n    # Teste Dickey-Fuller:\n    # Print\n    print('\\nResultado do Teste Dickey-Fuller:\\n')\n\n    # Teste\n    dfteste = adfuller(serie, autolag = 'AIC')\n\n    # Formatando a sa\u00edda\n    dfsaida = pd.Series(dfteste[0:4], index = ['Estat\u00edstica do Teste',\n                                               'Valor-p',\n                                               'N\u00famero de Lags Consideradas',\n                                               'N\u00famero de Observa\u00e7\u00f5es Usadas'])\n\n    # Loop por cada item da sa\u00edda do teste\n    for key, value in dfteste[4].items():\n        dfsaida['Valor Cr\u00edtico (%s)'%key] = value\n\n    # Print\n    print (dfsaida)\n    \n    # Testa o valor-p\n    print ('\\nConclus\u00e3o:')\n    if dfsaida[1] > 0.05:\n        print('\\nO valor-p \u00e9 maior que 0.05 e, portanto, n\u00e3o temos evid\u00eancias para rejeitar a hip\u00f3tese nula.')\n        print('Essa s\u00e9rie provavelmente n\u00e3o \u00e9 estacion\u00e1ria.')\n    else:\n        print('\\nO valor-p \u00e9 menor que 0.05 e, portanto, temos evid\u00eancias para rejeitar a hip\u00f3tese nula.')\n        print('Essa s\u00e9rie provavelmente \u00e9 estacion\u00e1ria.')","b60013b1":"testa_estacionaridade(df_full_series_Dia, 'Diario')","15eb935a":"df_train.shape[0]","ab7b2e7e":"df_test.shape","e3623ee1":"# Split do df_full em 2 novos dfs:\n# Irei substituir as informa\u00e7\u00f5es no df_train e df_test, por\u00e9m deixando os dados com a msm qtd q antes.\ndf_train = df_full.iloc[0:df_train.shape[0]+1]\ndf_test = df_full.iloc[df_train.shape[0]: ]","e0a50daa":"df_train_series_Dia = df_train[column_target].resample('D').mean()\ndf_test_series_Dia = df_test[column_target].resample('D').mean()","331f4a88":"# Fun\u00e7\u00e3o para medir o desempenho do modelo\ndef performance(y_true, y_pred): \n    mse = ((y_pred - y_true) ** 2).mean()\n    mape = np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n    return( print('MSE das previs\u00f5es \u00e9 {}'.format(round(mse, 2))+\n                  '\\nRMSE das previs\u00f5es \u00e9 {}'.format(round(np.sqrt(mse), 2))+\n                  '\\nMAPE das previs\u00f5es \u00e9 {}'.format(round(mape, 2))))","7f1c6143":"# Vamos definir p, d e q para que tenham valores entre 0 e 2 e testaremos as combina\u00e7\u00f5es.\np = d = q = range(0, 2)\n# Lista de combina\u00e7\u00f5es de p, d, q\npdq = list(itertools.product(p, d, q))\npdq","af2ef495":"# Lista de combina\u00e7\u00f5es dos hiperpar\u00e2metros sazonais P, D e Q\n# Estamos usando List Comprehension\n# 7 representa a sazonalidade\nseasonal_pdq = [(x[0], x[1], x[2], 7) for x in list(itertools.product(p, d, q))]\nseasonal_pdq","337dbebd":"print('\\nExemplos de Combina\u00e7\u00f5es dos Hiperpar\u00e2metros Para o Modelo SARIMA:\\n')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[3], seasonal_pdq[4]))","ed4304b8":"# Grid Search\n#warnings.filterwarnings(\"ignore\")\n\n# Menor valor poss\u00edvel para a estat\u00edstica AIC (nosso objetivo na otimiza\u00e7\u00e3o do modelo)\nlowest_aic = sys.maxsize\nlowest = ''\n\n# Loop\nfor param in pdq:\n    \n    for param_seasonal in seasonal_pdq:\n        try:\n            # Cria o modelo com a combina\u00e7\u00e3o dos hiperpar\u00e2metros\n            mod = sm.tsa.statespace.SARIMAX(df_train_series_Dia,\n                                            order = param,\n                                            seasonal_order = param_seasonal,\n                                            enforce_stationarity = False,\n                                            enforce_invertibility = False)\n            \n            # Treina o modelo\n            results = mod.fit()\n            \n            # Print\n            print('SARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n            \n            # Coleta o menor valor de AIC\n            if lowest_aic >  results.aic:\n                lowest = 'SARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic)\n                lowest_aic = results.aic\n        except:\n            continue\n\nprint (\"\\nModelo com Menor Valor de AIC: \" + lowest)","ba4df0ed":"# Treina o modelo com a melhor combina\u00e7\u00e3o de hiperpar\u00e2metros\nmodelo_sarima = sm.tsa.statespace.SARIMAX(df_train_series_Dia,\n                                             order = (0, 0, 1),\n                                             seasonal_order = (0, 1, 1, 7),\n                                             enforce_stationarity = False,\n                                             enforce_invertibility = False)","0079ea41":"# Treinamento (Fit) do modelo\nmodelo_sarima_fit = modelo_sarima.fit()","387ab502":"# Sum\u00e1rio do modelo\nprint(modelo_sarima_fit.summary())","127bd391":"df_test_series_Dia.index.values.min()","445319db":"df_test_series_Dia.index.values.max()","58f8461d":"# Vamos fazer previs\u00f5es dos dados do TESTE\nsarima_predict = modelo_sarima_fit.get_prediction(start = pd.to_datetime('2016-04-23'), \n                                                       end = pd.to_datetime('2016-05-27'),\n                                                       dynamic = False)","4a8b1a03":"# Intervalo de confian\u00e7a\nsarima_predict_conf = sarima_predict.conf_int()\nsarima_predict_conf","48a78c69":"rcParams['figure.figsize'] = 20,8\n\n# Plot dos valores observados\nax = df_full_series_Dia.plot(label = 'Valores Observados', color = '#2574BF')\n\n# Plot dos valores previstos\nsarima_predict.predicted_mean.plot(ax = ax, \n                                     label = 'Previs\u00f5es SARIMA(0, 0, 1)x(0, 1, 1, 7)', \n                                     alpha = 0.7, \n                                     color = 'red') \n\n# Plot do intervalo de confian\u00e7a\nax.fill_between(sarima_predict_conf.index,\n                # lower sales\n                sarima_predict_conf.iloc[:, 0],\n                # upper sales\n                sarima_predict_conf.iloc[:, 1], color = 'k', alpha = 0.1)\n\n# T\u00edtulos e Legendas\nplt.title('Previs\u00e3o de Consumo M\u00e9dio Energia Por Dia com Modelo ARIMA Sazonal Dados Teste')\nplt.xlabel('Data')\nplt.ylabel('M\u00e9dia Consumo')\nplt.legend()\nplt.show()","21ac8455":"# Calculando a performance\nsarima_results_treino = performance(df_test_series_Dia, sarima_predict.predicted_mean)\nsarima_results_treino","a0b6934c":"feriados = pd.Series()\n\n# Como nosso dataset aparenta ser da B\u00e9lgica, baseado na informa\u00e7\u00e3o do aeroporto, estamos procurando feriados no ano.\nfor i, feriado in holidays.Belgium(years = [2016]).items():\n    feriados[i] = feriado","5fc0dcef":"feriados_df = pd.DataFrame(feriados)","2e259f24":"# Reset do index para ajustar os nomes das colunas\nferiados_df.reset_index(level = 0, inplace = True)\nferiados_df.columns = ['data_feriado', 'feriado']\n# Visualiza\nferiados_df.head()\nferiados_df['data_feriado'] = pd.to_datetime(feriados_df['data_feriado'])","cf754fa0":"# Fun\u00e7\u00e3o\ndef adiciona_feriado(x):\n    \n    # Aplica a regra\n    batch_df = feriados_df.apply(lambda y: 1 if (x['data'] == y['data_feriado']) else None, axis=1)\n    \n    # Limpa valores nulos\n    batch_df = batch_df.dropna(axis = 0, how = 'all')  \n    \n    # Se estiver vazio, preenche com 0\n    if batch_df.empty:\n        batch_df = 0\n    else: \n        batch_df = batch_df.to_string(index = False)\n        \n    return batch_df","a61fa8a6":"# Cria um dataframe a partir da s\u00e9rie\nFrame_means = pd.DataFrame(df_train_series_Dia)\n\n# Reset do \u00edndice para ajustar as colunas (podia ter feito tudo isso em um comando, ms didaticamente deixamos assim)\nFrame_means.reset_index(level = 0, inplace = True)\n\n# Ajusta o nome das colunas\nFrame_means.columns = ['data', 'rv1']\nFrame_means.head()\n\n# Aplicamos a fun\u00e7\u00e3o e criamos a coluna feriado\nFrame_means['feriado'] = Frame_means.apply(adiciona_feriado, axis = 1)\n\n# Convertendo a coluna feriado para inteiro\nFrame_means['feriado'] = pd.to_numeric(Frame_means['feriado'], downcast = 'integer')\n\n# Vamos definir a order_date como \u00edndice\nFrame_means.set_index(\"data\", inplace = True)","3356fe32":"df_train.columns","ef738c61":"# E somente o feriado (mais uma constante requerida pelo statsmodels) na s\u00e9rie de feriado\nexog_var_treino = sm.add_constant(Frame_means['feriado'])\nexog_var_treino","9970b564":"# Grid Search\nwarnings.filterwarnings(\"ignore\")\n\n# Menor valor poss\u00edvel para a estat\u00edstica AIC (nosso objetivo na otimiza\u00e7\u00e3o do modelo)\nlowest_aic = sys.maxsize\nlowest = ''\n\n# Loop\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            \n            # Cria o modelo com a combina\u00e7\u00e3o dos hiperpar\u00e2metros\n            mod = sm.tsa.statespace.SARIMAX(df_train_series_Dia,\n                                            exog_var_treino,\n                                            order = param,\n                                            seasonal_order = param_seasonal,\n                                            enforce_stationarity = False,\n                                            enforce_invertibility = False)\n            \n            # Treina o modelo\n            results = mod.fit()\n            \n            # Print\n            print('SARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n            \n            # Coleta o menor valor de AIC\n            if lowest_aic >  results.aic:\n                lowest = 'SARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic)\n                lowest_aic = results.aic\n        except:\n            continue\n\nprint (\"\\nModelo com Menor Valor de AIC: \" + lowest)","055c3234":"# Treina o modelo com a melhor combina\u00e7\u00e3o de hiperpar\u00e2metros\nmodelo_sarima_v2 = sm.tsa.statespace.SARIMAX(df_train_series_Dia,\n                                             exog_var_treino,\n                                             order = (0, 0, 1),\n                                             seasonal_order = (0, 1, 1, 7),\n                                             enforce_stationarity = False,\n                                             enforce_invertibility=False)","ce5e068f":"# Treinamento (Fit) do modelo\nmodelo_sarima_v2_fit = modelo_sarima_v2.fit()","421d9c44":"# Sum\u00e1rio do modelo\nprint(modelo_sarima_v2_fit.summary())","f66ed76f":"df_train_series_Dia.index.values.min()","7bfc8b1d":"df_train_series_Dia.index.values.max()","24fe826b":"# Vamos fazer previs\u00f5es um passo a frente\nsarima_predict_2_treino = modelo_sarima_v2_fit.get_prediction(start = pd.to_datetime('2016-01-20'), \n                                                       end = pd.to_datetime('2016-03-19'),\n                                                       exog = exog_var_treino['20160120':'20160319'],\n                                                       dynamic = True)\n# Intervalo de confian\u00e7a\nsarima_predict_conf_2_treino = sarima_predict_2_treino.conf_int()\nsarima_predict_conf_2_treino","78f8e038":"rcParams['figure.figsize'] = 20,8\n\n# Plot dos valores observados\nax = df_train_series_Dia.plot(label = 'Valores Observados', color = '#2574BF')\n\n# Plot dos valores previstos\nsarima_predict_2_treino.predicted_mean.plot(ax = ax, \n                                     label = 'Previs\u00f5es SARIMA(0, 0, 1)x(0, 1, 1, 7) com vari\u00e1vel Ex\u00f3gena', \n                                     alpha = 0.7, \n                                     color = 'red') \n\n# Plot do intervalo de confian\u00e7a\nax.fill_between(sarima_predict_conf_2_treino.index,\n                # lower sales\n                sarima_predict_conf_2_treino.iloc[:, 0],\n                # upper sales\n                sarima_predict_conf_2_treino.iloc[:, 1], color = 'k', alpha = 0.1)\n\n# T\u00edtulos e Legendas\nplt.title('Previs\u00e3o de Consumo Energia com Modelo ARIMA Sazonal Dados Treino')\nplt.xlabel('Data')\nplt.ylabel('Media Consumo Energia')\nplt.legend()\nplt.show()","81365c3c":"# Calculando a performance Dados Treino\nsarima_results_2_treino = performance(df_train_series_Dia, sarima_predict_2_treino.predicted_mean)\nsarima_results_2_treino","40e472ba":"# Cria um dataframe a partir da s\u00e9rie\nFrame_means_teste = pd.DataFrame(df_test_series_Dia)\n\n# Reset do \u00edndice para ajustar as colunas (podia ter feito tudo isso em um comando, ms didaticamente deixamos assim)\nFrame_means_teste.reset_index(level = 0, inplace = True)\n\n# Ajusta o nome das colunas\nFrame_means_teste.columns = ['data', 'rv1']\nFrame_means_teste.head()\n\n# Aplicamos a fun\u00e7\u00e3o e criamos a coluna feriado\nFrame_means_teste['feriado'] = Frame_means_teste.apply(adiciona_feriado, axis = 1)\n\n# Convertendo a coluna feriado para inteiro\nFrame_means_teste['feriado'] = pd.to_numeric(Frame_means_teste['feriado'], downcast = 'integer')\n\n# Vamos definir a order_date como \u00edndice\nFrame_means_teste.set_index(\"data\", inplace = True)","eeeee621":"# E somente o feriado (mais uma constante requerida pelo statsmodels) na s\u00e9rie de feriado\nexog_var_teste = sm.add_constant(Frame_means_teste['feriado'])\nexog_var_teste","60c4f59d":"exog_var_teste.shape","2fa539e2":"df_test_series_Dia.shape","8ef4a2a6":"df_test_series_Dia.index.values.min()","34c038ea":"df_test_series_Dia.index.values.max()","57c18375":"df_test_series_Dia","7a602c79":"exog_var_teste","15254f01":"# Vamos fazer previs\u00f5es um passo a frente\nsarima_predict_2_teste = modelo_sarima_v2_fit.get_prediction(start = pd.to_datetime('2016-03-19'), \n                                                       end = pd.to_datetime('2016-05-28'),\n                                                       exog = exog_var_teste, #['20160320':'20160527'],\n                                                       dynamic = True)\n# Intervalo de confian\u00e7a\nsarima_predict_conf_2_teste = sarima_predict_2_teste.conf_int()\nsarima_predict_conf_2_teste","0e5de173":"rcParams['figure.figsize'] = 20,8\n\n# Plot dos valores observados\nax = df_full_series_Dia.plot(label = 'Valores Observados', color = '#2574BF')\n\n# Plot dos valores previstos\nsarima_predict_2_teste.predicted_mean.plot(ax = ax, \n                                     label = 'Previs\u00f5es SARIMA(0, 0, 1)x(0, 1, 1, 7) com vari\u00e1vel Ex\u00f3gena', \n                                     alpha = 0.7, \n                                     color = 'red') \n\n# Plot do intervalo de confian\u00e7a\nax.fill_between(sarima_predict_conf_2_teste.index,\n                # lower sales\n                sarima_predict_conf_2_teste.iloc[:, 0],\n                # upper sales\n                sarima_predict_conf_2_teste.iloc[:, 1], color = 'k', alpha = 0.1)\n\n# T\u00edtulos e Legendas\nplt.title('Previs\u00e3o de Consumo Energia com Modelo ARIMA Sazonal Dados Teste')\nplt.xlabel('Data')\nplt.ylabel('Media Consumo Energia')\nplt.legend()\nplt.show()","4651b219":"# Calculando a performance Dados Teste\nsarima_results_2_teste = performance(df_test_series_Dia, sarima_predict_2_teste.predicted_mean)\nsarima_results_2_teste","8b0ac7b1":"df_train_Dia_mean = df_train.resample('D').mean()\ndf_test_Dia_mean = df_test.resample('D').mean()","67c89c15":"df_train_Dia_mean.describe()","2f373c68":"df_train_Dia_mean_Stand = df_train_Dia_mean.copy()\n\n# Colunas que desejo aplicar a Padroniza\u00e7\u00e3o\ncols = ['appliances', 'lights', 't1', 'rh_1', 't2', 'rh_2', 't3', 'rh_3', 't4',\n       'rh_4', 't5', 'rh_5', 't6', 'rh_6', 't7', 'rh_7', 't8', 'rh_8', 't9',\n       'rh_9', 't_out', 'press_mm_hg', 'rh_out', 'windspeed', 'visibility',\n       'tdewpoint', 'nsm']\n\nfor i in cols:\n    scale = StandardScaler().fit(df_train_Dia_mean[[i]])\n    \n    df_train_Dia_mean_Stand[i] = scale.transform(df_train_Dia_mean[[i]])\n\ndf_train_Dia_mean_Stand.head(2)","20c55f31":"df_test_Dia_mean_Stand = df_test_Dia_mean.copy()\n\n# Colunas que desejo aplicar a Padroniza\u00e7\u00e3o\ncols = ['appliances', 'lights', 't1', 'rh_1', 't2', 'rh_2', 't3', 'rh_3', 't4',\n       'rh_4', 't5', 'rh_5', 't6', 'rh_6', 't7', 'rh_7', 't8', 'rh_8', 't9',\n       'rh_9', 't_out', 'press_mm_hg', 'rh_out', 'windspeed', 'visibility',\n       'tdewpoint', 'nsm']\n\nfor i in cols:\n    scale = StandardScaler().fit(df_test_Dia_mean[[i]])\n    \n    df_test_Dia_mean_Stand[i] = scale.transform(df_test_Dia_mean[[i]])\n\ndf_test_Dia_mean_Stand.head(2)","2546a660":"model = VAR(endog = df_train_Dia_mean_Stand,\n            freq = df_train_Dia_mean_Stand.index.inferred_freq)\n\nmodel_fit = model.fit()","829d00c1":"# model_fit.plot_forecast(3);\ndf_test_Dia_mean_Stand.values.shape","27e4a7fb":"pred_var = model_fit.forecast(y=df_test_Dia_mean_Stand.values, steps=70)\npred_var","c1afe761":"model_fit.summary()","1e09d469":"pred = pd.DataFrame(index=range(0,len(pred_var)),columns=[df_train_Dia_mean_Stand.columns])\npred\n\n#len(pred_var)\n\nfor j in range(0,len(df_train_Dia_mean_Stand.columns)):\n    for i in range(0, len(pred_var)):\n        pred.iloc[i][j] = pred_var[i][j]","a500f875":"pred_var","0f0b5f81":"df_forecast = pd.DataFrame(pred_var, index=df_test_Dia_mean_Stand.index, columns=df_test_Dia_mean_Stand.columns + '_2d')","9b72c142":"fig, ax = plt.subplots(figsize = (10,10))\nax.plot(df_test_Dia_mean_Stand['rv1'])\nax.plot(df_forecast['rv1_2d']);","8406628d":"performance(df_forecast['rv1_2d'], df_test_Dia_mean_Stand['rv1'])","c45175d2":"#X = df_full.loc[:, df_full.columns != column_target]\n#y = df_full.loc[:, df_full.columns == column_target]\n\nX_train = df_train_Dia_mean_Stand.loc[:, df_train_Dia_mean_Stand.columns != column_target]\nY_train = df_train_Dia_mean_Stand.loc[:, df_train_Dia_mean_Stand.columns == column_target]\n\nX_test = df_test_Dia_mean_Stand.loc[:, df_test_Dia_mean_Stand.columns != column_target]\nY_test = df_test_Dia_mean_Stand.loc[:, df_test_Dia_mean_Stand.columns == column_target]","245081b8":"X_train.shape\n#Y_train.shape","b8653d24":"X_test.shape\n#Y_test.shape","462edf6f":"# GRID SEARCH para identificar os melhores parametros.\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n\n# Grid de par\u00e2metros\nparam_grid = {'learning_rate': [0.1, 0.01, 0.001],\n              'max_depth': [4, 5, 6],\n              'min_samples_leaf': [3, 4, 5],\n              'subsample': [0.3, 0.5, 0.7],\n              'n_estimators': [400, 700, 1000, 2000, 3000]\n              }\n\n# Regressor\nest = GradientBoostingRegressor()\n\n# Modelo criado com GridSearchCV\ngs_cv = GridSearchCV(est, param_grid, scoring = 'neg_mean_squared_error', n_jobs = 4, return_train_score=True).fit(X_train, Y_train)\n\n# Imprime os melhors par\u00e2metros\nprint('Melhores Hiperpar\u00e2metros: %r' % gs_cv.best_params_)","a8f461d2":"gs_cv.best_params_","8b8e4c62":"#est = GradientBoostingRegressor()\n\n#params = {'min_samples_leaf': 3}\n#est.set_params(**gs_cv.best_params_)\n#est.fit(X_train, Y_train)","7a1c6026":"est = GradientBoostingRegressor(n_estimators = 6000, max_depth =8, learning_rate = 0.001, min_samples_leaf=4, subsample=0.3)\nest.fit(X_train, Y_train)","a309121b":"yhat = est.predict(X_test)\nyhat","3f3e383d":"predy = pd.DataFrame(index=range(0,len(yhat)), columns=['rv1_pred'])\n\nj = 0\nfor i in range(0, len(yhat)):\n    predy.iloc[i][j] = yhat[i]","c5ffcda1":"predy","0b52bf22":"predy.index = Y_test.index","2127a9eb":"fig, ax = plt.subplots(figsize = (20,8))\nax.plot(df_full_series_Dia, color = \"green\", label = 'Valores Observados')\nax.plot(pd.Series(predy['rv1_pred']), color = 'red', label = 'Valores Previstos Gradient Boosting')\nplt.legend();","2ec32e62":"performance(pd.Series(predy['rv1_pred']), pd.Series(Y_test['rv1']))","049cc1f6":"X_train = df_train_Dia_mean_Stand.loc[:, df_train_Dia_mean_Stand.columns != column_target]\nY_train = df_train_Dia_mean_Stand.loc[:, df_train_Dia_mean_Stand.columns == column_target]\n\nX_test = df_test_Dia_mean_Stand.loc[:, df_test_Dia_mean_Stand.columns != column_target]\nY_test = df_test_Dia_mean_Stand.loc[:, df_test_Dia_mean_Stand.columns == column_target]","53393ff6":"modelo = LinearRegression()\nmodelo_result = modelo.fit(X_train, Y_train)","ba0e0393":"yhat2 = modelo_result.predict(X_test)","039f61b8":"# output list  \n# function used for removing nested  \n# lists in python.  \ndef reemovNestings(yhat2): \n    \n    for i in yhat2: \n        if type(i) == list: \n            reemovNestings(i) \n        else: \n            output.append(i) \n    \n    return output","8d84ec14":"output = []\nyhat2 = reemovNestings(yhat2.tolist())","57a0559e":"yhat2","ab9146f9":"predy2 = pd.DataFrame(index=range(0,len(yhat2)), columns=['rv1_pred'])\n\nj = 0\nfor i in range(0, len(yhat2)):\n    predy2.iloc[i][j] = yhat2[i]","cca60ad4":"predy2.index = Y_test.index","e5981f10":"predy2","b30b970a":"fig, ax = plt.subplots(figsize = (20,8))\nax.plot(df_full_series_Dia, color = \"green\", label = 'Valores Observados')\nax.plot(pd.Series(predy2['rv1_pred']), color = 'red', label = 'Valores Previstos Gradient Boosting')\nplt.legend();","a4c40ff1":"performance(pd.Series(predy2['rv1_pred']), pd.Series(Y_test['rv1']))","c1e2feef":"### REGRESS\u00c3O LINEAR MULTILA","437628fd":"### Compreendendo o resultado:\n\n- Azul: A s\u00e9rie completa (df_full_series_Dia)\n\n- Vermelho: Dados Previstos pelo modelo.\n\nFoi informado dados do df_test_series_Dia para o modelo aplicar a previs\u00e3o.","9661251c":"Importando bibliotecas que ser\u00e3o utilizadas durante o desenvolvimento.","ee71299b":"# Sele\u00e7\u00e3o de Atributos com Random Forest","03395dc1":"O resultado da decomposi\u00e7\u00e3o da S\u00e9rie nos mostra que existe sazonalidade no conjunto de dados. Devido a esse fator, iremos realizar o primeiro treinamento do modelo utilizando o modelo **SARIMA**.","52ab307d":"### Primeiras manipula\u00e7\u00f5es no Dataset Treino","341dec8d":"# An\u00e1lise Parcial:\n\n##### Comparando o resultado nos dados de Teste do Modelo SARIMA x Modelo SARIMA Com Vari\u00e1vel Ex\u00f3gena, o resultado foi:\n\n- Modelo SARIMA: MSE: 1.19 | RMSE: 1.09 | MAPE: 3.37 | AIC: 321.170\n- Modelo SARIMA Com Vari\u00e1vel Ex\u00f3gena: MSE: 1.17 | RMSE: 1.08 | MAPE: 3.45 | AIC: 324.607\n\nCom A vari\u00e1vel Ex\u00f3gena de Feriados na B\u00e9lgica tivemos um aumento do AIC. O resultado n\u00e3o foi t\u00e3o bom.\n\nSe fosse necess\u00e1rio escolher entre os 2 modelos acima, o modelo escolhido seria o SARIMA com menor AIC.","c0828c58":"# An\u00e1lise Explorat\u00f3ria","760e3724":"# An\u00e1lise S\u00e9rie Temporal","7cd338aa":"# MODELO SARIMA COM VARI\u00c1VEL EX\u00d3GENA","6a111ff5":"###### Os dados est\u00e3o todos em Escalas diferentes um do outro. Isso dificulta o treinamento de alguns modelos.\n###### Irei aplicar a Padroniza\u00e7\u00e3o dos dados de modo que a m\u00e9dia de todos os dados se aproximem o m\u00e1ximo de 0 e o desvio padr\u00e3o como 1.","a6a83595":"# Avaliando todos os resultados.\n\n\nSARIMA\n\n- AIC:191.85300524599475\n- MSE das previs\u00f5es \u00e9 1.73\n- RMSE das previs\u00f5es \u00e9 1.32\n- MAPE das previs\u00f5es \u00e9 3.92\n\nSARIMA COM VARIAVEL EX\u00d3GENA\n\n- AIC:195.8530052510325\n- MSE das previs\u00f5es \u00e9 2.29\n- RMSE das previs\u00f5es \u00e9 1.51\n- MAPE das previs\u00f5es \u00e9 4.66\n\nVAR MULTIVARIAVEL\n\n- MSE das previs\u00f5es \u00e9 2.79\n- RMSE das previs\u00f5es \u00e9 1.67\n- MAPE das previs\u00f5es \u00e9 4.97\n\nGRADIENT BOOSTING REGRESSOR\n\n- MSE das previs\u00f5es \u00e9 2.21\n- RMSE das previs\u00f5es \u00e9 1.49\n- MAPE das previs\u00f5es \u00e9 4.68\n\nREGRESS\u00c3O LINEAR\n\n- MSE das previs\u00f5es \u00e9 13.76\n- RMSE das previs\u00f5es \u00e9 3.71\n- MAPE das previs\u00f5es \u00e9 13.97\n\n### Diante o Cen\u00e1rio, caso o objetivo seja prever o consumo em um determinado dia, seria utilizado o Modelo SARIMA, pois o SARIMA com Vari\u00e1vel Ex\u00f3gena, n\u00e3o apresentou um ganho representativo e teve uma queda no AIC.\n\n### Caso o objetivo seja prever o consumo di\u00e1rio sem levar em conta um dia especifico, o Modelo Gradient Boosting seria melhor recomendado devido a taxa do RMSE ser relativamente menor.","656c4e32":"### ESTOU ME BASEANDO QUE A VARI\u00c1VEL TARGET SEJA A VARI\u00c1VEL rv1.","824db048":"# Inicio An\u00e1lise Preditiva","c8c3087e":"# MODELO VAR MULTIVARIADO","175e1382":"## Irei realizar o estudo agora utilizando o Modelo Gradient Boosting para prever o consumo m\u00e9dio de energia Di\u00e1rio.","d3025f95":"Podemos observar que a s\u00e9rie de gasto M\u00e9dio de Energia Di\u00e1rio possui uma m\u00e9dia Constante. Vamos verificar se \u00e9 uma s\u00e9rie estacion\u00e1ria ou n\u00e3o utilizando o modelo de Dickey Fuller.\n\n#### Vamos Decompor a S\u00e9rie em 3 partes: Tend\u00eancia, Sazonalidade e Res\u00edduo.","956ce448":"T > Temperatura\n\nRH >  Umidade Relativa (Relative Humidity)\n\nT_out > Temperatura do lado de fora da casa\n\nRH_out > Umidade do lado de fora da casa\n\nmm_hg > Unidade de press\u00e3o","65d63dda":"## Modelo VAR com Multivariaveis n\u00e3o foi capaz de prever o consumo m\u00e9dio de energia Di\u00e1rio. Resultado muito ruim.","4508fb47":"## GRADIENT BOOSTING","dd236b36":"### Realizando um agrupamento Di\u00e1rio para ter um paradigma de como est\u00e1 o consumo m\u00e9dio energia total.","28f07e9e":"Observa\u00e7\u00f5es:\n\n- A S\u00e9rie n\u00e3o possui nenhuma tend\u00eancia.\n- Na Semana entre 08-02 at\u00e9 22-02, parece ter ocorrido algum evento que elevou o gasto m\u00e9dio.\n- Parece existir uma sazonalidade nos dados SEMANAL (a cada 7 dias), baseado em como o gr\u00e1fico foi apresentado.\n- Os Res\u00edduos n\u00e3o seguem um padr\u00e3o.","433b86d7":"O Gasto m\u00e9dio de energia no FDS e em dias de semana representam o mesmo gasto. Tomando como base que possui 5 dias Weekday e apenas 2 Weekend, o gasto nos FDS s\u00e3o relativamente maiores baseado na propor\u00e7\u00e3o da distribui\u00e7\u00e3o dos dados."}}