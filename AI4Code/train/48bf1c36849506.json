{"cell_type":{"10f2ecf6":"code","9a660221":"code","933cf06f":"code","7973aece":"code","c09d4008":"code","08bef65e":"code","3ae0441e":"code","f3cf10cc":"code","abcc9c2e":"code","a161b680":"code","fafc3ee8":"code","0496527e":"code","68ee7e6e":"code","bd1e6ef5":"code","fb4bceb0":"code","7aff1350":"code","1d6757e1":"code","a706eb8a":"code","f9518501":"code","4e593a2e":"code","1b2cfe7b":"code","df1c5b0c":"code","9aeb364c":"code","bccf9e56":"code","2583b127":"code","7b7d4b15":"code","efbec096":"code","787bd2f6":"code","1154989c":"code","79ee3162":"code","3ebbd8bb":"code","4346162b":"code","b67337e4":"code","b58713f4":"code","53b893cd":"code","b44a03d0":"code","c996b415":"code","4353b3ca":"code","fa785d13":"code","ea2aeeae":"code","0059a5b4":"code","b6220a68":"code","b19eb09c":"code","0fc717c6":"code","fba14ece":"code","1479df29":"code","220b1301":"code","69b5843b":"code","df622c76":"code","6b9afed0":"code","f875901b":"code","ad7559a2":"code","4626fb66":"code","39267979":"code","def6aa1e":"code","dbd12586":"code","685ece75":"code","e69c9fb4":"code","4234dadb":"code","011bd54d":"code","b0162bbb":"code","c76dcea4":"code","72aab953":"code","1bd51d78":"code","d6e143bc":"code","dd2c240f":"code","cbfa78d5":"code","6138e5a1":"code","65a2b723":"code","9baef746":"code","59220363":"code","a66d6f81":"code","315d9cc0":"code","55522fb7":"code","682ec89c":"code","9b79ca59":"code","f431d741":"code","2d334f69":"code","7b9551c8":"code","1c3a01ca":"code","e25202f4":"code","fdc0c371":"code","84b366a7":"code","29303f27":"code","3c9c578a":"code","d3194e33":"code","1c2b100f":"code","c9bccbcb":"code","d3c8e5eb":"code","6ae6d2c8":"code","446d0664":"code","c908a5c0":"code","a177a066":"code","1c81f70d":"code","f682e4b3":"code","b72fd656":"code","474c5bbc":"code","188def2b":"code","332bc27f":"code","03dd2f98":"code","c6a56f8f":"code","afcd2367":"code","7530c3de":"code","ef69fce8":"code","903eb15b":"code","15dbe047":"code","ea5a8c33":"code","bf91f32a":"code","42c08eea":"code","7669ee6c":"code","c3b2bdfc":"code","7795f017":"code","5fbd8665":"code","0185ce5b":"code","7731482a":"code","cca611b6":"code","340fe981":"code","f9ad38dc":"code","b10f271f":"code","63a09169":"code","77b79bb9":"code","720deb32":"code","ffdf47cf":"code","283a89a7":"code","d33417d2":"code","3a4f887d":"code","11f6443d":"code","6990bdbd":"code","ce3f34d2":"code","da5fe9b7":"code","a7c1f21c":"code","b379310b":"code","35f22580":"code","52f71b48":"code","91945254":"code","37df1cac":"code","1693f718":"code","e6b2c527":"code","760023e5":"code","5d42af6d":"code","ed5b9747":"code","693e7c3b":"code","5cd77168":"markdown","32467e88":"markdown","27daa033":"markdown","4167e739":"markdown","dc487c4d":"markdown","7817770b":"markdown","24f7daae":"markdown","0cf204e3":"markdown","6c6c0fff":"markdown","9a21561a":"markdown","4bbc909a":"markdown","5dceca70":"markdown","7b49819d":"markdown","fb6224de":"markdown","4b49e2b4":"markdown","30dce750":"markdown","15d24056":"markdown","ec098e52":"markdown","9f9d844b":"markdown","9ac9715a":"markdown","d980f3af":"markdown","22cb6528":"markdown","b8f472e0":"markdown","56421827":"markdown","e67a5e65":"markdown","af01507a":"markdown","89a9d6a0":"markdown","f6a59d6c":"markdown","ae710d06":"markdown","26c37c3e":"markdown","b9c096a2":"markdown","ac9658ce":"markdown","046c0fd2":"markdown","a8a455be":"markdown","2f2f8141":"markdown"},"source":{"10f2ecf6":"import time\nimport re\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport folium\n\nfrom geopy.distance import geodesic\nfrom geopy.distance import great_circle\nfrom geopy.geocoders import Nominatim","9a660221":"# Jupyter Notebook Interactive \nfrom IPython.display import display\nfrom ipywidgets import interact, widgets, IntSlider\nfrom IPython.html.widgets import *","933cf06f":"%matplotlib inline \nsns.set_style('dark') # Seaborn ploting style\npd.set_option('display.expand_frame_repr', False) # Expand the dataframe width to display all columns","7973aece":"# Import training dataset\ntrain_dataset_path = \"..\/input\/nytaxi-dataset\/train.csv\"\nmain_df = pd.read_csv(train_dataset_path)","c09d4008":"# Import testing dataset\ntest_dataset_path = \"..\/input\/nytaxi-dataset-taxi\/test.csv\"\ntest_df = pd.read_csv(test_dataset_path)","08bef65e":"# Import weather dataset\nweather_df_path = '..\/input\/nytaxi-dataset\/taxi_weather_data.csv'\nweather_df = pd.read_csv(weather_df_path)","3ae0441e":"print('main_df shape: \\n{}\\n'.format(main_df.shape))\nprint('test_df shape: \\n{}\\n'.format(test_df.shape))\nprint('weather_df shape: \\n{}\\n'.format(weather_df.shape))","f3cf10cc":"print('test_df is: {}% of the main_df'.format(round(len(test_df) \/ len(main_df), 2) * 100))","abcc9c2e":"print('main_df columns: \\n{}\\n'.format(list(main_df.columns)))\nprint('test_df columns: \\n{}\\n'.format(list(test_df.columns)))\nprint('weather_df columns: \\n{}\\n'.format(list(weather_df.columns)))","a161b680":"print('main_df columns types: \\n{}\\n'.format(main_df.dtypes))\nprint('test_df columns types: \\n{}\\n'.format(test_df.dtypes))\nprint('weather_df columns types: \\n{}\\n'.format(weather_df.dtypes))","fafc3ee8":"print('main_df columns types: \\n{}\\n'.format(main_df.info()))\nprint('test_df columns types: \\n{}\\n'.format(test_df.info()))\nprint('weather_df columns types: \\n{}\\n'.format(weather_df.info()))","0496527e":"print('main_df columns types: \\n{}\\n'.format(main_df.isnull().sum()))\nprint('test_df columns types: \\n{}\\n'.format(test_df.isnull().sum()))\nprint('weather_df columns types: \\n{}\\n'.format(weather_df.isnull().sum()))","68ee7e6e":"print('main_df statistics: \\n{}\\n'.format(main_df.describe()))\nprint('test_df statistics: \\n{}\\n'.format(test_df.describe()))\nprint('weather_df statistics: \\n{}\\n'.format(weather_df.describe()))","bd1e6ef5":"print('main_df categorical statistics: \\n{}\\n'.format(main_df.describe(include=['object']))) # Display categorical (dtype = 'object') statistics\nprint('test_df categorical statistics: \\n{}\\n'.format(test_df.describe(include=['object']))) \nprint('weather_df categorical statistics: \\n{}\\n'.format(weather_df.describe(include=['object']))) ","fb4bceb0":"print('main_df heaoverview: \\n{}\\n'.format(main_df.head()))\nprint('test_df overview: \\n{}\\n'.format(test_df.head()))\nprint('weather_df overview: \\n{}\\n'.format(weather_df.head()))","7aff1350":"# Display the distribution of the trip duration with the mean\/ median\/ 25% & 75% percentiles\nplt.figure(figsize=(50, 10))\n\nsns.distplot(main_df.trip_duration.values, bins= 100)\n\nplt.axvline(main_df.trip_duration.mean())\nplt.axvline(main_df.trip_duration.median(), linestyle = '--', c = 'r')\nplt.axvline(main_df.trip_duration.quantile(0.25), linestyle = ':', c = 'g')\nplt.axvline(main_df.trip_duration.quantile(0.75), linestyle = ':', c = 'g')\nplt.margins(0.02)\n\nplt.show()","1d6757e1":"# Visualize the trip duration distribution\n\nfig, ax = plt.subplots(ncols=2, figsize= (20,6))\nsns.distplot(main_df.trip_duration.values, bins= 100, ax= ax[0])\nsns.distplot(np.log(main_df.trip_duration.values), bins= 100, ax= ax[1])\n\nax[0].set_title('Trip Duration')\nax[0].set_xlabel('Trip Duration Samples')\n\nax[1].set_title('Log (Trips Count)')\nax[1].set_xlabel('Trip Duration Samples')\n\nplt.show()","a706eb8a":"# Focus on the main_df.trip_duration feature\n\ndef trip_duration_focus():\n    print('Trip Duration Count: {}'.format(len(main_df.trip_duration)))\n    print('Trip Duration Max: {}'.format(main_df.trip_duration.max()))\n    print('Trip Duration Min: {}'.format(main_df.trip_duration.min()))\n    print('Trip Duration Mean: {}'.format(main_df.trip_duration.mean()))\n    print('Trip Duration Variance: {}'.format(main_df.trip_duration.var()))\n    print('Trip Duration Standard Deviation: {}'.format(main_df.trip_duration.std()))\n    print('Trip Duration Interquartile Range: {}'.format(stats.iqr(main_df.trip_duration)))\n    print('Trip Duration For The First 5 Values: {}'.format(main_df.trip_duration.sort_values().head().values))\n    print('Trip Duration For The Last 5 Values: {}'.format(main_df.trip_duration.sort_values().tail().values))\n    \ntrip_duration_focus()","f9518501":"# Decide on which approach to take for initial outliers clearning \n\n# Removing outliers based on mean & 2 * std\nm, s = main_df.trip_duration.mean(), main_df.trip_duration.std()\nmean_main_df = main_df[(main_df.trip_duration <= m + 2 * s) & (main_df.trip_duration >= m - 2 * s)].copy()\n\nbefore_df, after_df = len(main_df.trip_duration), len(mean_main_df.trip_duration)\nprint('Mean (2 * STD) approach - Original dataset lenght is: {}. New dataset length is: {}. Diference count is: {} - {:.2f}%'.format(before_df, after_df, before_df - after_df, round((before_df - after_df) \/ before_df, 3) * 100))\n\n# Remove outliers based on median & 1.5 * iqr\nq1, q2, iqr = np.percentile(main_df.trip_duration, 25), np.percentile(main_df.trip_duration, 75), stats.iqr(main_df.trip_duration)\nmedian_main_df = main_df[(main_df.trip_duration >= q1 - (iqr * 1.5)) & (main_df.trip_duration <= q2 + (iqr * 1.5))].copy()\n\nbefore_df, after_df = len(main_df.trip_duration), len(median_main_df.trip_duration)\nprint('Median (1.5 * IQR) approach - Original dataset lenght is: {}. New dataset length is: {}. Diference count is: {} - {:.2f}%'.format(before_df, after_df, before_df - after_df, round((before_df - after_df) \/ before_df, 3) * 100))\n\n# Remove outliers based on median & 2 * iqr\nmedian_2_main_df = main_df[(main_df.trip_duration >= q1 - (iqr * 2)) & (main_df.trip_duration <= q2 + (iqr * 2))].copy()\n\nbefore_df, after_df = len(main_df.trip_duration), len(median_2_main_df.trip_duration)\nprint('Median (2 * IQR) approach - Original dataset lenght is: {}. New dataset length is: {}. Diference count is: {} - {:.2f}%\\n\\n'.format(before_df, after_df, before_df - after_df, round((before_df - after_df) \/ before_df, 3) * 100))\n\n# Visualization of the trip duration distribution when using log \/ mean & std \/ median & iqr to make a decision on which approach to take.\nbp_bins = int(np.sqrt(len(main_df.trip_duration)))\n\nfig, ax = plt.subplots(ncols=5, figsize= (45,6))\nsns.distplot(main_df.trip_duration.values, bins= bp_bins, ax= ax[0])\nsns.distplot(main_df.trip_duration.values[:5000], bins= bp_bins, ax= ax[1])\nsns.distplot(mean_main_df.trip_duration.values, bins= 100, ax= ax[2])\nsns.distplot(median_main_df.trip_duration.values, bins= 100, ax= ax[3])\nsns.distplot(median_2_main_df.trip_duration.values, bins= 100, ax= ax[4])\n\nax[0].set_title('Normal - Trip Duration Distribution')\nax[0].set_xlabel('Trip Duration Samples')\n\nax[1].set_title('Normal (5000 Samples Zoom In) - Trip Duration Distribution')\nax[1].set_xlabel('Trip Duration Samples')\nax[1].set_xlim(0,5000)\n\nax[2].set_title('(Mean -\/+ 2 * STD) - Trip Duration Distribution')\nax[2].set_xlabel('Logged Trip Duration Samples')\n\nax[3].set_title('(Median -\/+ 1.5 * IQR) - Trip Duration Distribution')\nax[3].set_xlabel('Logged Trip Duration Samples')\n\nax[4].set_title('(Median -\/+ 2 * IQR) - Trip Duration Distribution')\nax[4].set_xlabel('Logged Trip Duration Samples')\n\nplt.show()","4e593a2e":"# Delete all unwanted objects\ndel median_2_main_df, median_main_df, mean_main_df, q1, q2, iqr, m, s, before_df, after_df, bp_bins","1b2cfe7b":"# Exclude the outliers\n\nbefore_df = len(main_df.trip_duration)\n\nm = main_df.trip_duration.mean()\ns = main_df.trip_duration.std()\n\nmain_df = main_df[main_df.trip_duration <= m + 2 * s]\nmain_df = main_df[main_df.trip_duration >= m - 2 * s]\n\nafter_df = len(main_df.trip_duration)\n\n# About 2000 rows were removed from the original dataset\nprint('Original dataset lenght is: {}. New dataset length is: {}. Diference is: {}'.format(before_df, after_df, before_df - after_df))\n\n# Check the trip duration again\ntrip_duration_focus()","df1c5b0c":"# Excluding trips that are less than 60 seconds\n\nless_60_df = len(main_df[main_df.trip_duration < 60]) \n\nmain_df = main_df[~(main_df.trip_duration < 60)]\nafter_df = len(main_df.trip_duration) \n\n# About 8562 rows were removed from the original dataset\nprint('Original dataset lenght is: {}. New dataset length is: {}. Diference is: {}'.format(less_60_df, after_df, after_df - less_60_df))\n\n# Check the trip duration again\ntrip_duration_focus()","9aeb364c":"# Remove [passenger_count == 0 \/ 7 \/ 9]\n\nprint('length of the main_df where the [passenger_count == 0 \/ 7 \/ 8 \/ 9] is: {}\\n'.format(len(main_df.loc[(main_df.passenger_count == 0) | (main_df.passenger_count == 7) | (main_df.passenger_count == 8) | (main_df.passenger_count == 9)])))\nprint('Passengers distribution is: \\n{}\\n'.format(main_df.passenger_count.value_counts()))\nprint('Passengers distribution is: \\n{}\\n'.format(main_df.loc[(main_df.passenger_count == 0) | (main_df.passenger_count == 7) | (main_df.passenger_count == 8) | (main_df.passenger_count == 9), ['pickup_datetime', 'passenger_count', 'trip_duration']].head().sort_values('trip_duration', ascending = True)))\n\n# Delete the rows where [passenger_count == 0 \/ 7 \/ 9]\nmain_df = main_df.loc[~((main_df.passenger_count == 0) | (main_df.passenger_count == 7) | (main_df.passenger_count == 8) | (main_df.passenger_count == 9))].copy() ","bccf9e56":"# Visualize the updated trip duration distribution\n\nfig, ax = plt.subplots(ncols=2, figsize= (20,6))\nsns.distplot(main_df.trip_duration.values, bins= 100, ax= ax[0])\nsns.distplot(np.log(main_df.trip_duration.values), bins= 100, ax= ax[1])\n\nax[0].set_title('Trip Duration Distribution')\nax[0].set_xlabel('Trip Duration Samples')\n\nax[1].set_title('Transformed - Log (Trip Duration)')\nax[1].set_xlabel('Transformed Trip Duration Samples')\n\nplt.show()","2583b127":"# Convert main_df dates columns into datetime type in all dataframes\nmain_df.pickup_datetime = pd.to_datetime(main_df.pickup_datetime)\nmain_df.dropoff_datetime = pd.to_datetime(main_df.dropoff_datetime)\ntest_df.pickup_datetime= pd.to_datetime(test_df.pickup_datetime)\nweather_df.pickup_datetime = pd.to_datetime(weather_df.pickup_datetime)\n\n# Check if they were converted\nprint('datetime type columns in main_df, pickup_datetime: {}  and dropoff_datetime: {}.\\nTotal datetime dtypes are: {}\\n'.format(main_df.pickup_datetime.dtype, main_df.dropoff_datetime.dtype, sum(main_df.dtypes == 'datetime64[ns]')))\nprint('datetime type columns in test_df, pickup_datetime: {}.\\nTotal datetime dtypes are: {}\\n'.format(test_df.pickup_datetime.dtype, sum(test_df.dtypes == 'datetime64[ns]')))\nprint('datetime type columns in weather_df, pickup_datetime: {}.\\nTotal datetime dtypes are: {}\\n'.format(weather_df.pickup_datetime.dtype, sum(weather_df.dtypes == 'datetime64[ns]')))","7b7d4b15":"# Function to convert 'int64' to 'int32'\ndef convert_int64_32(df):\n    for i in df.columns:\n        if df[i].dtype == 'int64':\n            df[i] = df[i].astype('int32')","efbec096":"convert_int64_32(main_df)\nconvert_int64_32(test_df)\nconvert_int64_32(weather_df)","787bd2f6":"# Function to convert 'float64' to 'float32'\ndef convert_float64_32(df):\n    for i in df.columns:\n        if df[i].dtype == 'float64':\n            df[i] = df[i].astype('float32')","1154989c":"convert_float64_32(main_df)\nconvert_float64_32(test_df)\nconvert_float64_32(weather_df)","79ee3162":"# Convert the store_flag_dict to 1 & 0\nstore_flag_dict = {'Y': 1, 'N': 0}\nmain_df.store_and_fwd_flag.replace(store_flag_dict, inplace=True)\nmain_df.store_and_fwd_flag = main_df.store_and_fwd_flag.astype('int32')","3ebbd8bb":"# Sort dataframe based on datetime (year > month > day > time)\ndef reorganize_df_firstround(df):\n    df.sort_values('pickup_datetime', inplace= True) # Sort dataframe based on datetime\n    df.reset_index(inplace=True) # Reset indexe dataframes \n    df.drop('index', axis= 1, inplace= True) # Dropping unneccesary columns\n    df.drop('id', axis= 1, inplace= True) # Dropping unneccesary columns\n    \n# Reset the index\ndef reorganize_df_comman(df):\n    df.reset_index(inplace=True) # Reset indexe dataframes \n    df.drop('index', axis= 1, inplace= True) # Dropping unneccesary columns","4346162b":"reorganize_df_firstround(main_df) # Calling reorganize dataframe function on main_df\nreorganize_df_firstround(test_df) # Calling reorganize dataframe function on test_df","b67337e4":"# check dates (pickup\/ dropoff) if they are not the same while the pickup time is less than 23:59\nlen(main_df.loc[(main_df.pickup_datetime.dt.date != main_df.dropoff_datetime.dt.date) & (main_df.pickup_datetime.dt.hour <= 23) & (main_df.dropoff_datetime.dt.minute <= 59)])","b58713f4":"# Check initial main_df dataset cleaning effect\nprint('main_df new length: {}\\n'.format(len(main_df))) # original length is: 1458644\nprint('main_df new info: {}\\n'.format(main_df.info())) # original size is: 122+ MB","53b893cd":"# Histogram Plotting\n\nplt.figure(figsize=(12,6))\n#bp_bins = int(np.sqrt(len(main_df.trip_duration)))\n\nplt.hist(main_df.trip_duration, bins= 100)\nplt.xlabel('Trip Duration Samples')\nplt.ylabel('Count of Trip')\nplt.title('Trip Duration Histogram')\nplt.margins(0.02)\n\nplt.show()","b44a03d0":"# ECDF Plotting\n\nplt.figure(figsize=(12,6))\n\nx = np.sort(main_df.trip_duration.values)\ny = range(len(main_df))\n\nplt.plot(x, y, marker = '.', linestyle = 'none')\nplt.xlabel('Trip Duration Samples')\nplt.ylabel('Count of Trip Duration')\nplt.title('Trip Duration Histogram')\nplt.margins(0.02)\n\nplt.show()\n\n# Alternative plotting code using Seaborn library\n# sns.scatterplot(x = range(main_df.shape[0]), y = np.sort(main_df.trip_duration.values), data= main_df)","c996b415":"# ECDF Plotting with Percentiles\n\nplt.figure(figsize=(12,6))\n\npercentiles = np.array([10, 25, 50, 75, 99])\npercentiles_vars = np.percentile(main_df.trip_duration, percentiles)\n\nx = np.sort(main_df.trip_duration.values)\ny = np.arange(1, len(main_df.trip_duration) + 1) \/ len(main_df.trip_duration)\n\nplt.plot(x, y, '--')\nplt.plot(percentiles_vars, percentiles \/ 100, marker = 'o', markersize = 10, linestyle = 'none')\n\nplt.xlabel('Trip Duration Samples')\nplt.ylabel('Count of Trip Duration')\nplt.title('Trip Duration Histogram')\nplt.margins(0.02)\n\nplt.show()","4353b3ca":"plt.figure(figsize=(12,6))\n\nsns.boxplot(y = np.log(main_df.trip_duration), data= main_df)\nplt.show()","fa785d13":"print('trip_duration minimum is: {}'.format(main_df.trip_duration.min()))\nprint('trip_duration maximum is: {}'.format(main_df.trip_duration.max()))\nprint('trip_duration mean is: {}'.format(round(main_df.trip_duration.mean(), 2)))\nprint('trip_duration variance is: {}'.format(round(main_df.trip_duration.var(), 2)))\nprint('trip_duration std is: {}'.format(round(main_df.trip_duration.std(), 2)))\nprint('trip_duration median is: {}'.format(round(main_df.trip_duration.median(), 2)))\nprint('trip_duration Interquartile Range (IQR) is: {}'.format(round(stats.iqr(main_df.trip_duration), 2)))\n\nprint('log(trip_duration) variance is: {}'.format(round(np.log(main_df.trip_duration).var(), 2)))","ea2aeeae":"# Display scatter plot using pickup\/ dropoff lat and long points as given in the training dataset\n\nplt.style.use('dark_background')\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(15,10))\n\nmain_df.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude', color='yellow', s=.02, alpha=.6, subplots=True, ax=ax1)\nax1.set_title(\"Pickups\", color= 'white')\nax1.axes.tick_params(color= 'white', labelcolor= 'white')\nax1.set_facecolor('black')\n\nmain_df.plot(kind='scatter', x='dropoff_longitude', y='dropoff_latitude', color='yellow', s=.02, alpha=.6, subplots=True, ax=ax2)\nax2.set_title(\"Dropoffs\", color= 'white')\nax2.axes.tick_params(color= 'white', labelcolor= 'white')\nax2.set_facecolor('black') ","0059a5b4":"# .copy() used to avoid warning while doing operations on the new dataframe\n# Display scatter plot using pickup\/ dropoff lat and long points for NY city block only\n\nwest, south, east, north = -74.03, 40.63, -73.77, 40.85 # NY city block\n\n# Create new dataframe excluding the points that are not within the NY city block range\nfiltered_main_df = main_df.loc[(main_df.pickup_longitude > west) & (main_df.pickup_longitude < east) & \n                               (main_df.dropoff_longitude > west) & (main_df.dropoff_longitude < east) & \n                               (main_df.pickup_latitude < north) & (main_df.pickup_latitude > south) & \n                               (main_df.dropoff_latitude < north) & (main_df.dropoff_latitude > south)].copy()\n\nreorganize_df_comman(filtered_main_df) # Reset the index\n\n# Check df before\/after lengths \nprint('New dataset size is: \\n{}\\n'.format(len(filtered_main_df)))\nprint('The new dataset is less by: \\n{}\\n'.format(len(main_df) - len(filtered_main_df)))","b6220a68":"# NY city area borders\nborders = { 'manhattan':{ 'min_lng':-74.0479, 'min_lat':40.6829, 'max_lng':-73.9067, 'max_lat':40.8820 },\n            'queens':{ 'min_lng':-73.9630, 'min_lat':40.5431, 'max_lng':-73.7004, 'max_lat':40.8007 },\n            'brooklyn':{ 'min_lng':-74.0421, 'min_lat':40.5707, 'max_lng':-73.8334, 'max_lat':40.7395 },\n            'bronx':{ 'min_lng':-73.9339, 'min_lat':40.7855, 'max_lng':-73.7654, 'max_lat':40.9176 },\n            'staten_island':{ 'min_lng':-74.2558, 'min_lat':40.4960,  'max_lng':-74.0522, 'max_lat':40.6490 },\n            'airport_JFK':{ 'min_lng':-73.8352, 'min_lat':40.6195, 'max_lng':-73.7401, 'max_lat':40.6659},\n            'airport_EWR':{ 'min_lng':-74.1925, 'min_lat':40.6700, 'max_lng':-74.1531, 'max_lat':40.7081 },\n            'airport_LaGuardia':{ 'min_lng':-73.8895, 'min_lat':40.7664, 'max_lng':-73.8550, 'max_lat':40.7931 } }\n\n# Function - Differentiate areas based on LAT and LON\ndef points_classifier(lat,lng):\n    if lat >= borders['manhattan']['min_lat'] and lat <= borders['manhattan']['max_lat'] and lng >= borders['manhattan']['min_lng'] and lng <= borders['manhattan']['max_lng']:\n        return 'Manhattan'\n    elif lat >= borders['queens']['min_lat'] and lat <= borders['queens']['max_lat'] and lng >= borders['queens']['min_lng'] and lng <= borders['queens']['max_lng']:\n        return 'Queens'\n    elif lat >= borders['brooklyn']['min_lat'] and lat <= borders['brooklyn']['max_lat'] and lng >= borders['brooklyn']['min_lng'] and lng <= borders['brooklyn']['max_lng']:\n        return 'Brooklyn'\n    elif lat >= borders['bronx']['min_lat'] and lat <= borders['bronx']['max_lat'] and lng >= borders['bronx']['min_lng'] and lng <= borders['bronx']['max_lng']:\n        return 'Bronx'\n    elif lat >= borders['staten_island']['min_lat'] and lat <= borders['staten_island']['max_lat'] and lng >= borders['staten_island']['min_lng'] and lng <= borders['staten_island']['max_lng']:\n        return 'Staten Island'\n    else:\n        return 'Unknown'\n\n# Function - Differentiate airports from cities\ndef is_airport(lat, lng):\n    if lat >= borders['airport_JFK']['min_lat'] and lat <= borders['airport_JFK']['max_lat'] and lng >= borders['airport_JFK']['min_lng'] and lng <= borders['airport_JFK']['max_lng']:\n        return 'JFK Airport'\n    elif lat >= borders['airport_EWR']['min_lat'] and lat <= borders['airport_EWR']['max_lat'] and lng >= borders['airport_EWR']['min_lng'] and lng <= borders['airport_EWR']['max_lng']:\n        return 'EWR Airport'\n    elif lat >= borders['airport_LaGuardia']['min_lat'] and lat <= borders['airport_LaGuardia']['max_lat'] and lng >= borders['airport_LaGuardia']['min_lng'] and lng <= borders['airport_LaGuardia']['max_lng']:\n        return 'La Guardia Aiport'\n    else:\n        return 'City'\n    \n# Create new columns for pickup and dropoff area \nfiltered_main_df['pickup_area'] = filtered_main_df.apply(lambda x: points_classifier(x['pickup_latitude'], x['pickup_longitude']), axis = 1)\nfiltered_main_df['dropoff_area'] = filtered_main_df.apply(lambda x: points_classifier(x['dropoff_latitude'], x['dropoff_longitude']), axis = 1)\n\n# Create new column to differentiate airports from cities\nfiltered_main_df['airport_pickup'] = filtered_main_df.apply(lambda x: is_airport(x['pickup_latitude'], x['pickup_longitude']), axis = 1)\nfiltered_main_df['airport_dropoff'] = filtered_main_df.apply(lambda x: is_airport(x['dropoff_latitude'], x['dropoff_longitude']), axis = 1)","b19eb09c":"filtered_main_df.head()","0fc717c6":"plt.style.use('dark_background')\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(15,10))\n\nfiltered_main_df.plot(kind='scatter', x='pickup_longitude', y='pickup_latitude', color='yellow', s=.02, alpha=.6, subplots=True, ax=ax1)\nax1.set_title(\"Pickups\", color= 'white')\nax1.axes.tick_params(color= 'white', labelcolor= 'white')\nax1.set_facecolor('black')\n\nfiltered_main_df.plot(kind='scatter', x='dropoff_longitude', y='dropoff_latitude', color='orange', s=.02, alpha=.6, subplots=True, ax=ax2)\nax2.set_title(\"Dropoffs\", color= 'white')\nax2.axes.tick_params(color= 'white', labelcolor= 'white')\nax2.set_facecolor('black') ","fba14ece":"# Convert the pickup LAT & LON to array (float32) to plot points on map  \npickup_loc = np.array(filtered_main_df[['pickup_latitude', 'pickup_longitude']], dtype= 'float32') # Has to be 'pickup_latitude', 'pickup_longitude' for the map plotting\nprint(len(pickup_loc) - 1420792)","1479df29":"# Best views are: 'Stamen Terrain', 'Stamen Toner', 'Cartodb Positron'\n# Due to the computing time, I will plot 2000 points only.\nnymap = folium.Map(location=[40.745208740234375, -73.98473358154298], zoom_start= 12, control_scale= True, tiles='Cartodb dark_matter')\n\nfor i in range( 0, 2000):\n    folium.CircleMarker(pickup_loc[i], radius=0.01, fill= True, opacity=0.5, color='yellow').add_to(nymap)\n    \nnymap","220b1301":"# Storing the LAT & LON for pickup\/ dropoff with 3 decimal points only into new columns\nfiltered_main_df['pickup_latitude_round3'] = filtered_main_df.pickup_latitude.apply(lambda x: round(x,3))\nfiltered_main_df['pickup_longitude_round3'] = filtered_main_df.pickup_longitude.apply(lambda x: round(x,3))\nfiltered_main_df['dropoff_latitude_round3'] = filtered_main_df.dropoff_latitude.apply(lambda x: round(x,3))\nfiltered_main_df['dropoff_longitude_round3'] = filtered_main_df.dropoff_longitude.apply(lambda x: round(x,3))","69b5843b":"filtered_main_df.head()","df622c76":"# Scatter plot with the new rounded LAT & LON\nplt.style.use('dark_background')\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(15,10))\n\nfiltered_main_df.plot(kind='scatter', x='pickup_longitude_round3', y='pickup_latitude_round3', color='yellow', s=.02, alpha=.6, subplots=True, ax=ax1)\nax1.set_title(\"Pickups\", color= 'white')\nax1.axes.tick_params(color= 'white', labelcolor= 'white')\nax1.set_facecolor('black')\n\nfiltered_main_df.plot(kind='scatter', x='dropoff_longitude_round3', y='dropoff_latitude_round3', color='orange', s=.02, alpha=.6, subplots=True, ax=ax2)\nax2.set_title(\"Dropoffs\", color= 'white')\nax2.axes.tick_params(color= 'white', labelcolor= 'white')\nax2.set_facecolor('black') ","6b9afed0":"def scatter_plotting(df, p_lng, p_lat, d_lng, d_lat, p_cap, d_cap):\n    \n    plt.style.use('dark_background')\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(15,10))\n\n    df.plot(kind='scatter', x= p_lng, y= p_lat, color='yellow', s=.02, alpha=.6, subplots=True, ax=ax1)\n    ax1.set_title(p_cap, color= 'white')\n    ax1.axes.tick_params(color= 'white', labelcolor= 'white')\n    ax1.set_facecolor('black')\n\n    df.plot(kind='scatter', x= d_lng, y= d_lat, color='orange', s=.02, alpha=.6, subplots=True, ax=ax2)\n    ax2.set_title(d_cap, color= 'white')\n    ax2.axes.tick_params(color= 'white', labelcolor= 'white')\n    ax2.set_facecolor('black') ","f875901b":"scatter_plotting(filtered_main_df.loc[(filtered_main_df.airport_pickup == 'JFK Airport') | (filtered_main_df.airport_dropoff == 'JFK Airport')] , 'pickup_longitude_round3', 'pickup_latitude_round3', 'dropoff_longitude_round3', 'dropoff_latitude_round3', 'JFK Airport Pickups', 'JFK Airport Dropoffs') \t\nscatter_plotting(filtered_main_df.loc[(filtered_main_df.airport_pickup == 'La Guardia Aiport') | (filtered_main_df.airport_dropoff == 'La Guardia Aiport')] , 'pickup_longitude_round3', 'pickup_latitude_round3', 'dropoff_longitude_round3', 'dropoff_latitude_round3', 'La Guardia Aiport Pickups', 'La Guardia Aiport Dropoffs') \t\nscatter_plotting(filtered_main_df.loc[(filtered_main_df.pickup_area == 'Manhattan') | (filtered_main_df.dropoff_area == 'Manhattan')] , 'pickup_longitude_round3', 'pickup_latitude_round3', 'dropoff_longitude_round3', 'dropoff_latitude_round3', 'Manhattan Pickups', 'Manhattan Dropoffs') \t\nscatter_plotting(filtered_main_df.loc[(filtered_main_df.pickup_area == 'Queens') | (filtered_main_df.dropoff_area == 'Queens')] , 'pickup_longitude_round3', 'pickup_latitude_round3', 'dropoff_longitude_round3', 'dropoff_latitude_round3', 'Queens Pickups', 'Queens Dropoffs') \t\nscatter_plotting(filtered_main_df.loc[(filtered_main_df.pickup_area == 'Brooklyn') | (filtered_main_df.dropoff_area == 'Brooklyn')] , 'pickup_longitude_round3', 'pickup_latitude_round3', 'dropoff_longitude_round3', 'dropoff_latitude_round3', 'Brooklyn Pickups', 'Brooklyn Dropoffs') \t\nscatter_plotting(filtered_main_df.loc[(filtered_main_df.pickup_area == 'Bronx') | (filtered_main_df.dropoff_area == 'Bronx')] , 'pickup_longitude_round3', 'pickup_latitude_round3', 'dropoff_longitude_round3', 'dropoff_latitude_round3', 'Bronx Pickups', 'Bronx Dropoffs') \t","ad7559a2":"filtered_main_df.columns","4626fb66":"# Convert pickup and dropoff points to array to retreive the distance in KM faster\npick_drop_points = filtered_main_df[['pickup_latitude_round3', 'pickup_longitude_round3', 'dropoff_latitude_round3', 'dropoff_longitude_round3' ]].values","39267979":"# Check the new array slicing \nprint(len(pick_drop_points))\nprint(pick_drop_points[0])\nprint(pick_drop_points[0][0:2])\nprint(pick_drop_points[0][2:4])\nprint(pick_drop_points[0][0])\nprint(pick_drop_points[0][1])","def6aa1e":"# Create empty list to append the new distance points\ndist_result = []\n\n# Calculating the distance function\ndef calc():\n    for i in range(0, len(pick_drop_points)):\n        pickpoint = (pick_drop_points[i][0], pick_drop_points[i][1])\n        droppoint = (pick_drop_points[i][2], pick_drop_points[i][3])\n        dist_result.append(geodesic(pickpoint, droppoint).km)\n\n# Executing the function\ncalc()","dbd12586":"# To make sure dist_result & filtered_main_df are in the same length\nprint('dist_result array length is: {}'.format(len(dist_result)))\nprint('filtered_main_df length is: {}\\n'.format(len(filtered_main_df)))","685ece75":"filtered_main_df['est_distance'] = np.around(dist_result, decimals=1)\nfiltered_main_df['est_distance'] = filtered_main_df['est_distance'].astype('float32')","e69c9fb4":"print('estimated distance head overview: \\n{}\\n'.format(filtered_main_df.est_distance.sort_values().head()))\nprint('estimated distance tail overview: \\n{}\\n'.format(filtered_main_df.est_distance.sort_values().tail()))","4234dadb":"filtered_main_df.loc[filtered_main_df.est_distance == 0, 'est_distance'].count() # The count of observations where 'est_distance == 0'","011bd54d":"filtered_main_df.loc[filtered_main_df.est_distance == 0].head()","b0162bbb":"# Above points (LAT & LON) were randomly checked and manually checked using Google maps. Based on that, I have decided to remove these points as they are invalid.\n\nfiltered_main_df = filtered_main_df.loc[~(filtered_main_df.est_distance.values == 0)].copy() # Remove the observations where 'est_distance == 0'\nreorganize_df_comman(filtered_main_df) # Reset the index","c76dcea4":"filtered_main_df.drop(['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'], axis= 1, inplace= True)","72aab953":"# Extracting year, month, day, time from the pickup_datetime column\nfiltered_main_df['year_pick'] = filtered_main_df['pickup_datetime'].dt.year.astype('int32')\nfiltered_main_df['month_pick'] = filtered_main_df['pickup_datetime'].dt.month.astype('int32')\nfiltered_main_df['day_pick'] = filtered_main_df['pickup_datetime'].dt.day.astype('int32')\nfiltered_main_df['weekday_pick'] = filtered_main_df['pickup_datetime'].dt.weekday.astype('int32')\nfiltered_main_df['time_pick'] = filtered_main_df['pickup_datetime'].dt.time\nfiltered_main_df['hour_pick'] = filtered_main_df['pickup_datetime'].dt.hour.astype('int32')\n\n# Format the trip duration from seconds to HH:MM:SS format\nfiltered_main_df['trip_dur_formated'] = pd.to_datetime(filtered_main_df['trip_duration'], unit='s').dt.strftime('%H:%M:%S')","1bd51d78":"# Weekdays VS Weekends\ndef weekdays_weekends(x):\n    return 'Weekend' if (x == 5) or (x == 6) else 'Weekday'\n\n# Define the taxi type\ndef taxi_type(x):\n    return 'Limousine \/ Van' if x > 4 else 'Regular Taxi'\n\n# Define rush hours\ndef rush_hours(x):\n    return 'Rush Hours' if (x >= 7 and x <= 10) or (x >= 16 and x <= 19) else 'Normal'\n\n# Define night surcharge USD 0.5\ndef night_surcharge(x):\n    return 0.5 if (x >= 20 or x < 6) else 0\n\n# Define peak charges USD 1\ndef peak_charges(x):\n    return 1 if (x >= 16 and x < 20) else 0\n\n# Extract the season\ndef season(x):\n    if (x >= 1 and x < 3):\n        return 'Winter'\n    if (x >= 3 and x < 6):\n        return 'Spring'\n    if x >= 6:\n        return 'Summer'\n    else:\n        return 'Unknown'\n    \n# Part of the day   \ndef part_of_the_day(x):\n    if (x >= 5 and x < 12):\n        return 'Morning'\n    if x == 12:\n        return 'Noon'\n    if (x > 12 and x < 17):\n        return 'Afternoon'\n    if (x >= 17 and x < 20):\n        return 'Evening'\n    if (x >= 20 and x <= 23):\n        return 'Night'\n    if (x >= 0 and x <= 4):\n        return 'Midnight'\n    else:\n        return 'Unknown'\n\n# Creating new columns for the new features\nfiltered_main_df['weekdays_weekends'] = filtered_main_df.weekday_pick.apply(weekdays_weekends).astype('category') # Add weekends\/ weekdays to the dataframe\nfiltered_main_df['taxi_type'] = filtered_main_df.passenger_count.apply(taxi_type).astype('category') # # Add van types to the dataframe\nfiltered_main_df['avg_speed'] = round(filtered_main_df.est_distance \/ (filtered_main_df.trip_duration \/ 3600)).astype('int32') # Add average speed to the dataframe\nfiltered_main_df['season'] = filtered_main_df.month_pick.apply(season).astype('category') # Add seasons hours to the dataframe\n\nfiltered_main_df['rush_hours'] = filtered_main_df.hour_pick.apply(rush_hours).astype('category') # Add rush hours to the dataframe\nfiltered_main_df['night_charges'] = filtered_main_df.hour_pick.apply(night_surcharge).astype('float32') # Add night charges to the dataframe\nfiltered_main_df['peak_charges'] = filtered_main_df.hour_pick.apply(peak_charges).astype('int32') # Add peak hours to the dataframe\nfiltered_main_df['day_part'] = filtered_main_df.hour_pick.apply(part_of_the_day).astype('category') # Add part of the day hours to the dataframe\n\n# Fastest approach (Monday=0, Tuesday=1, Wednesday=2, Thursday=3, Friday=4, Saturday=5, Sunday=6) select the rows where the [day_pick = 5 or 6] and assign 0 to peak_charges \nfiltered_main_df.loc[(filtered_main_df.day_pick == 6) | (filtered_main_df.day_pick == 5), 'peak_charges'] = 0","d6e143bc":"filtered_main_df.head()","dd2c240f":"print('average speed head values overview: \\n{}\\n'.format(filtered_main_df.avg_speed.sort_values().head()))\nprint('average speed tail values overview: \\n{}\\n'.format(filtered_main_df.avg_speed.sort_values().tail()))\n\nprint('average speed head with features overview: \\n{}\\n'.format(filtered_main_df[['pickup_datetime', 'dropoff_datetime', 'pickup_latitude_round3', 'pickup_longitude_round3', 'dropoff_latitude_round3', 'dropoff_longitude_round3', 'rush_hours', 'est_distance', 'trip_duration', 'avg_speed']].sort_values('avg_speed').head()))\nprint('average speed tail with features overview: \\n{}\\n'.format(filtered_main_df[['pickup_datetime', 'dropoff_datetime', 'pickup_latitude_round3', 'pickup_longitude_round3', 'dropoff_latitude_round3', 'dropoff_longitude_round3', 'rush_hours', 'est_distance', 'trip_duration', 'avg_speed']].sort_values('avg_speed').tail()))\n\nprint('count of the observations where average speed is 0: {}'.format(len(filtered_main_df.loc[filtered_main_df.avg_speed == 0])))","cbfa78d5":"print('Observations count where estimated distance less than 0.2: {}'.format(len(filtered_main_df[filtered_main_df.est_distance < 0.2])))","6138e5a1":"#Based on the random check on LAT\/LON pickup and dropoff points on Google map, I can tell that all checked points are invalid. Action: Remove observations where estimated distance is less that 0.2\n\nfiltered_main_df = filtered_main_df.loc[~(filtered_main_df.est_distance.values < 0.2)].copy() # Remove the observations where 'est_distance < 0.2'\nreorganize_df_comman(filtered_main_df) # Reset the index","65a2b723":"# Quick overview (details are on the top) \nprint('Weather dataset length: \\n{}\\n'.format(len(weather_df)))\nprint('Weather dataset first 5 rows: \\n{}\\n'.format(weather_df.head()))\nprint('Weather dataset available months: \\n{}\\n'.format(weather_df.pickup_datetime.dt.month.unique()))\nprint('Weather dataset data types: \\n{}\\n'.format(weather_df.dtypes))\nprint('Weather dataset missing values: \\n{}\\n'.format(weather_df.isnull().sum()))","9baef746":"weather_df = weather_df.loc[weather_df.pickup_datetime.dt.month <= 6] # Extract the first 6 months only\nweather_df.fillna(method= 'ffill', inplace= True)\n\nw_conditions_replacment = {'Overcast': 1, 'Partly Cloudy': 2, 'Clear': 3, 'Mostly Cloudy': 4, 'Light Rain': 5, 'Scattered Clouds': 6, 'Heavy Rain': 7, 'Rain': 8, 'Light Snow': 9, 'Snow': 10, 'Heavy Snow': 11, 'Light Freezing Fog': 12, 'Haze': 13, 'Light Freezing Rain': 14, 'Fog': 15, 'Unknown': 16}\nweather_df.weather_condition.astype(str).replace(w_conditions_replacment, inplace= True) # Replace conditions strings with numeric values","59220363":"weather_df['new'] = (weather_df.pickup_datetime.dt.month.astype(str) + weather_df.pickup_datetime.dt.day.astype(str) + weather_df.pickup_datetime.dt.hour.astype(str)).astype('int32') # Create new columns combines month\/dateday\/hour in weather dataframe with int32 type\n\nweather_df.drop_duplicates(subset='new', inplace= True) # Remove duplicated rows based on the 'new' column values\nreorganize_df_comman(weather_df) # Reset dataframe index, assign new indexes to avoid errors","a66d6f81":"num_array_weather = np.array(weather_df['new'], dtype= 'int32') # Store new values into numpy array of type Int32\nnum_array_weather_data = np.array(weather_df[['temperature', 'rain', 'snow', 'weather_condition']], dtype= 'int32') # Store the rest of values (temperature, rain, snow, conds) into numpy array of type Int32","315d9cc0":"# Process filtered_main_df 'new' columns for processing \nfiltered_main_df['new'] = (filtered_main_df.month_pick.astype(str) + filtered_main_df.day_pick.astype(str) + filtered_main_df.hour_pick.astype(str)).astype('int32') # Create new columns combines month\/dateday\/hour in main dataframe with int32 type\nnum_array_main_df = np.array(filtered_main_df['new'], dtype= 'int32') # Store the new values (month\/dateday\/hour) codes into numpy array of type Int32","55522fb7":"# Initiate empty lists\ntemperature_list = []\nrain_list = []\nsnow_list = []\nweather_condition_list = []\n\n# Add weather data into lists \ndef add_weather_data(mdh):\n    try:\n        indx = np.where(num_array_weather == mdh)[0][0]\n        temperature_list.append(num_array_weather_data[indx][0])\n        rain_list.append(num_array_weather_data[indx][1])\n        snow_list.append(num_array_weather_data[indx][2])\n        weather_condition_list.append(num_array_weather_data[indx][3])\n        return mdh\n    except:\n        temperature_list.append(np.nan)\n        rain_list.append(np.nan)\n        snow_list.append(np.nan)\n        weather_condition_list.append(np.nan)  \n        return mdh\n\nfiltered_main_df.new = filtered_main_df.new.apply(add_weather_data) # Add weather data into the main dataframe","682ec89c":"# Create new columns in main dataframe and inserting weather data into them\ndef add_weather_data_to_main_df():\n    filtered_main_df['temperature'] = temperature_list\n    filtered_main_df['rain'] = rain_list\n    filtered_main_df['snow'] = snow_list\n    filtered_main_df['weather_condition'] = weather_condition_list\n    \nadd_weather_data_to_main_df() # Call creating columns\/ inserting weather data","9b79ca59":"#filtered_main_df.loc[(filtered_main_df.temperature == -99) & (filtered_main_df.rain == -99) & (filtered_main_df.snow == -99) & (filtered_main_df.weather_condition == -99), ['temperature', 'rain', 'snow', 'weather_condition']] #= np.NaN # Replace -99 (missing) values with np.NaN\nfiltered_main_df.fillna(method= 'ffill', inplace= True) # Fill the missing values with the previous ones","f431d741":"# Weather columns converted to float64 when replaced the -99 with np.NaN. Re convert the data type to 'int32'\nfiltered_main_df.temperature = filtered_main_df.temperature.astype('int32')\nfiltered_main_df.rain = filtered_main_df.rain.astype('int32')\nfiltered_main_df.snow = filtered_main_df.snow.astype('int32')\nfiltered_main_df.weather_condition = filtered_main_df.weather_condition.astype('int32')","2d334f69":"# Reverse the weather conditions values (numeric to string) and convert the type to category\ndef weather_conditions_to_categories():\n    new_w_conditions_replacment = {1: 'Overcast', 2: 'Partly Cloudy', 3: 'Clear', 4: 'Mostly Cloudy', 5: 'Light Rain', 6: 'Scattered Clouds', 7: 'Heavy Rain', 8: 'Rain', 9: 'Light Snow', 10: 'Snow', 11: 'Heavy Snow', 12: 'Light Freezing Fog', 13: 'Haze', 14: 'Light Freezing Rain', 15: 'Fog', 16: 'Unknown'}\n    filtered_main_df.weather_condition.replace(new_w_conditions_replacment, inplace=True)\n    filtered_main_df.weather_condition.astype('category')\n\nweather_conditions_to_categories() # Call function to update the weather_condition columns\n#filtered_main_df.weather_condition = filtered_main_df.weather_condition.astype('category')","7b9551c8":"pd.set_option('display.max_columns', 35)\nfiltered_main_df.head()","1c3a01ca":"###########################################################################################################################################################################\n# Deleting \/ dropping unwanted objects \/ columns\ndel weather_df, weather_df_path, temperature_list, rain_list, snow_list, weather_condition_list, num_array_weather, num_array_weather_data, pickup_loc, west, south, east, north, before_df, after_df, less_60_df, m, s, percentiles, percentiles_vars, x # Remove weather related objects\n###########################################################################################################################################################################","e25202f4":"# Visualize the distribution of the trip duration, estimated distance and average speed\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(ncols=3, figsize= (30,6))\n\nsns.distplot(filtered_main_df.trip_duration.values, bins= 50, ax= ax[0])\nsns.distplot(filtered_main_df.est_distance.values, bins= 50, ax= ax[1])\nsns.distplot(filtered_main_df.avg_speed.values, bins= 30, ax= ax[2])\n\nax[0].set_title('Trip Duration Distribution')\nax[0].set_xlabel('Trip Duration Samples')\n\nax[1].set_title('Estimated Distance Distribution')\nax[1].set_xlabel('Estimated Distance Samples')\n\nax[2].set_title('Average Speed Distribution')\nax[2].set_xlabel('Average Speed Samples')\n\nplt.show()","fdc0c371":"filtered_main_df.columns","84b366a7":"# Prepare variables mapping\nmap_weekdays = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\nmap_weekdays_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nmap_months = {1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun'}","29303f27":"# Annotation function based on the number of trips in filtered_main_df. \ndef annotations(hight_number, style):\n    total = len(filtered_main_df)\n    if style == 'both':\n        for p in _.patches:\n            height = p.get_height()\n            _.text(p.get_x()+p.get_width()\/2., height + hight_number, '{} ({}%)'.format(int(height), int(round(height\/total, 2)*100)), ha=\"center\", va='center', fontsize=10)\n    else:\n        for p in _.patches:\n            height = p.get_height()\n            _.text(p.get_x()+p.get_width()\/2., height + hight_number, '{}%'.format(int(round(height\/total, 2)*100)), ha=\"center\", va='center', fontsize=10)","3c9c578a":"# Count plot passengers, months, weekdays, seasons\n\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(ncols=4, figsize= (50,8))\n\n_ = sns.countplot(filtered_main_df.passenger_count, ax= ax[0])\nannotations(25000, 'both')\n\n_ = sns.countplot(filtered_main_df.month_pick.replace(map_months), ax= ax[1])\nannotations(5000, 'both')\n\n_ = sns.countplot(filtered_main_df.weekday_pick.replace(map_weekdays), order= map_weekdays_order, ax= ax[2])\nannotations(5000, 'both')\n\n_ = sns.countplot(filtered_main_df.season, order=['Winter', 'Spring', 'Summer'], ax= ax[3])\nannotations(15000, 'both')\n\nax[0].set_title('Passenger(s)')\nax[0].set_xlabel('Number of Passengers')\nax[0].set_ylabel('Trips Count')\n\nax[1].set_title('Months (Jan-Jun 2016)')\nax[1].set_xlabel('Months Samples')\nax[1].set_ylabel('Trips Count')\n\nax[2].set_title('Weekdays')\nax[2].set_xlabel('Weekdays')\nax[2].set_ylabel('Trips Count')\n  \nax[3].set_title('Seasons (Summer Includes The Month of Jun Only)')\nax[3].set_xlabel('Seasons')\nax[3].set_ylabel('Trips Count')\n\nplt.show()","d3194e33":"# Count plot weekdays\/weekends, taxi types, rush hours, part of the day\n\nfig, ax = plt.subplots(ncols=4, figsize= (50,8))\n\n_ = sns.countplot(filtered_main_df.weekdays_weekends, ax= ax[0])\nannotations(18000, 'both')\n\n_ = sns.countplot(filtered_main_df.taxi_type, ax= ax[1])\nannotations(20000, 'both')\n\n_ = sns.countplot(filtered_main_df.rush_hours, ax= ax[2])\nannotations(15000, 'both')\n\n_ = sns.countplot(filtered_main_df.day_part, order = ['Morning', 'Noon', 'Afternoon', 'Evening', 'Night', 'Midnight'], ax= ax[3])\nannotations(6000, 'both')\n\nax[0].set_title('Weekdays Vs. Weekends (Weekends are Saturday & Sunday)')\nax[0].set_xlabel('Weekdays Vs. Weekends')\nax[0].set_ylabel('Trips Count')\n\nax[1].set_title('Taxi Type (Regular & Limousine)')\nax[1].set_xlabel('Taxi Type')\nax[1].set_ylabel('Trips Count')\n\nax[2].set_title('Rush Hours (7:00-10:00 AM & 4:00-7:00 PM)')\nax[2].set_xlabel('Rush Hours')\nax[2].set_ylabel('Trips Count')\n  \nax[3].set_title('Part of The Day')\nax[3].set_xlabel('Part of The Day')\nax[3].set_ylabel('Trips Count')\n\nplt.show()","1c2b100f":"plt.figure(figsize= (30, 8))\n\n_ = sns.countplot(filtered_main_df.weather_condition)\n\ntotalLen = len(filtered_main_df)\nannotations(12572,'both')\n\n_.set_title('Number of Trips Based on Weather Conditions')\n_.set_xlabel('Weather Conditions')\n_.set_ylabel('Trips Count')\n_.set_xticklabels(_.get_xticklabels(), rotation=90)\n\nplt.show()","c9bccbcb":"# Formula to automatically select the best height value for plotting annotations\nint((filtered_main_df.weather_condition.value_counts().max() \/ filtered_main_df.weather_condition.value_counts().min()) * 2)","d3c8e5eb":"# Count plot pickups \/ dropoffs points\n\nfig, ax = plt.subplots(ncols=4, figsize= (50,8))\n\n_ = sns.countplot(filtered_main_df.pickup_area, ax= ax[0])\nannotations(20000, 'both')\n\n_ = sns.countplot(filtered_main_df.dropoff_area, ax= ax[1])\nannotations(20000, 'both')\n\n_ = sns.countplot(filtered_main_df.airport_pickup, ax= ax[2])\nannotations(20000, 'both')\n\n_ = sns.countplot(filtered_main_df.airport_dropoff, ax= ax[3])\nannotations(20000, 'both')\n\nax[0].set_title('Pickup Areas')\nax[0].set_xlabel('Pickup Areas')\nax[0].set_ylabel('Trips Count')\n\nax[1].set_title('Dropoff Areas')\nax[1].set_xlabel('Dropoff Areas')\nax[1].set_ylabel('Trips Count')\n\nax[2].set_title('Pickups From Cities \/ Airports')\nax[2].set_xlabel('Pickup')\nax[2].set_ylabel('Trips Count')\n\nax[3].set_title('Dropoff To Cities \/ Airports')\nax[3].set_xlabel('Dropoff')\nax[3].set_ylabel('Trips Count')\n\nplt.show()","6ae6d2c8":"# Count plot passengers, months, weekdays, seasons\n\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(ncols=3, figsize= (50,12))\n\n_ = sns.countplot(filtered_main_df.passenger_count, hue= filtered_main_df.airport_pickup, ax= ax[0])\nannotations(25000, '')\n\n_ = sns.countplot(filtered_main_df.month_pick.replace(map_months), hue= filtered_main_df.airport_pickup, ax= ax[1])\nannotations(5000, '')\n\n_ = sns.countplot(filtered_main_df.weekday_pick.replace(map_weekdays), order= map_weekdays_order, hue= filtered_main_df.airport_pickup, ax= ax[2])\nannotations(5000, '')\n\nax[0].set_title('Passenger(s)')\nax[0].set_xlabel('Number of Passengers')\nax[0].set_ylabel('Trips Count')\n\nax[1].set_title('Months (Jan-Jun 2016)')\nax[1].set_xlabel('Months Samples')\nax[1].set_ylabel('Trips Count')\n\nax[2].set_title('Weekdays')\nax[2].set_xlabel('Weekdays')\nax[2].set_ylabel('Trips Count')\n  \nplt.show()","446d0664":"filtered_main_df.head()","c908a5c0":"# Check the relation between the trip duration and the estimated distance considering the rush hours.\nplt.figure(figsize=(20,10))\nsns.scatterplot(data = filtered_main_df, x = 'trip_duration', y = 'est_distance', hue = 'rush_hours', size= 'avg_speed', palette='Set2')","a177a066":"# Check the relation between the trip duration and the estimated distance considering the number of passengers rush hours, and weekends\/ weekdays.\nsns.relplot(x = 'trip_duration', y = 'est_distance', hue = 'rush_hours', size = 'avg_speed', col = 'passenger_count', row = 'weekdays_weekends', palette = 'bright', data = filtered_main_df)","1c81f70d":"# Check the relation between trip duration and average speed considering the number of passengers rush hours, estimated distance, and weekends\/ weekdays.\nsns.relplot(x = 'trip_duration', y = 'avg_speed', hue = 'rush_hours', size = 'est_distance', col = 'passenger_count', row = 'weekdays_weekends', palette = 'bright', data = filtered_main_df)","f682e4b3":"# Check the relation between estimated distance  and average speed considering the number of passengers rush hours, trip duration, and weekends\/ weekdays.\nsns.relplot(x = 'est_distance', y = 'avg_speed', hue = 'rush_hours', size = 'trip_duration', col = 'passenger_count', row = 'weekdays_weekends', palette = 'bright', data = filtered_main_df)","b72fd656":"filtered_main_df[['trip_duration', 'est_distance', 'avg_speed']].corr()","474c5bbc":"# Get LAT & LON in one column with '+' to seperate them\nlat_lon_temp = filtered_main_df.pickup_latitude_round3.astype(str) + '+' + filtered_main_df.pickup_longitude_round3.astype(str)\n\n# Create new dataframe to store the value_counts results\ntop_locations = pd.DataFrame()\ntop_locations['points'] = lat_lon_temp.value_counts().index[:500]\ntop_locations['counts'] = lat_lon_temp.value_counts().values[:500]\n\n# Seperate the 'points' column into 'lat' and 'lon' columns\ntop_locations['lat'] = top_locations.points.str.split('+').str.get(0)\ntop_locations['lon'] = top_locations.points.str.split('+').str.get(1)\n\n# Drop 'points' columns\ntop_locations.drop('points', axis= 1, inplace= True)","188def2b":"# I get error here because of the timout failure. On my local machine works fine\n\n# Collect the addresses using geolocator function\n'''\naddresses = []\n\nlat_lon_points = top_locations[['lat', 'lon']].values\ngeolocator = Nominatim(user_agent=\"nytaxi-analysis-project\")\n\nfor i in lat_lon_points:\n    v = geolocator.reverse((i), timeout=50)\n    addresses.append(v[0])\n'''","332bc27f":"# Get the length excluding duplicated addresses\n\n\n'''\nprint('Length of extracted addresses: {}'.format(len(addresses)))\nprint('Length of extracted addresses excluding duplicated ones: {}'.format(len(set(addresses))))\n'''","03dd2f98":"'''\ntop_locations['address'] = addresses\ntop_locations['post_code'] = top_locations.address.str.split(',').str.get(-2)\ntop_locations['short_address'] = top_locations.address.str.split(',').str.get(0) + ', ' + top_locations.address.str.split(',').str.get(1) + ', ' + top_locations.address.str.split(',').str.get(2)\n'''","c6a56f8f":"'''\npd.set_option('max_colwidth', 200)\ntop_locations[:5]\n'''","afcd2367":"# Check missing values and manually collect it from Google to update the cell. Also looking at the address column, you will find the postcode as 10001\n\n'''\nprint(top_locations.post_code.isnull().sum())\nprint(top_locations[top_locations.post_code.isnull()])\n'''","7530c3de":"# Postcode cleaning function \n\n'''\ndef cleanPostcode(x):\n    \n    if 'NY' in x: #x.contains('NY', nan= False):\n        x = x.replace('NY', '')\n    elif 'New York' in x: \n        x = x.replace('New York', '9999')\n    elif '-' in x:\n        x = x.split('-')[0]\n    elif ':' in x:\n        x = x.split(':')[0]\n    \n    return x\n\n# Clean postcodes\ntop_locations.post_code = top_locations.post_code.apply(lambda x: cleanPostcode(x)).str.strip()\n\n# Create array with [Index : Postcode]\nindxValues = [[27 , 10153], [45 , 10011], [111 , 10003], [119 , 10036], [173 , 10119], [254 , 10020],\n              [284 , 10001], [289 , 10002], [303 , 10019], [331 , 10010], [370 , 10017], [406 , 10010], [429 , 10154]]\n\n# Run for loop for the indexes of the 9999 postcodes within indxValues array and assign the new postcodes values\nfor i in indxValues:\n    top_locations.loc[i[0], 'post_code'] = i[1]\n    \n# Looking at the above graph, I can see two postcodes that are wrong. I will assign the new postcodes using Google map\ntop_locations.loc[[81,366], 'post_code'] = 10014\ntop_locations.loc[[253,436], 'post_code'] = 10016\n'''","ef69fce8":"# Matching postcodes and sum the counts\n'''\ndef finalTopLoc(postcode):\n    try:\n        finalCount = top_locations.loc[top_locations.post_code == postcode, 'counts'].sum()\n        return finalCount\n    except:\n        return 0\n    \n# Convert post_code and counts columns to int32 \ntop_locations.post_code = top_locations.post_code.astype('int32')\ntop_locations.counts = top_locations.counts.astype('int32')\n\n# Create new dataframe to store the unique postcodes along with the counts sum\nfinalLoc_df = pd.DataFrame({'postcode' : top_locations.post_code.value_counts().keys().values}) # Creating the dataframe on the fly\nfinalLoc_df['finalCount'] = finalLoc_df.postcode.apply(lambda x: finalTopLoc(x)).astype('int32') # Matching the postcodes and sum the counts using the finalTopLoc function\nfinalLoc_df = finalLoc_df.sort_values('finalCount', ascending= False).reset_index(drop= True) # Reorder the values based on counts sum, reset the index and assign it to the created dataframe\n'''","903eb15b":"# Plot the final top locations based on the sum of postcode counts\n\n'''\n\nplt.figure(figsize = (25,8))\n\n_ = sns.barplot(x= 'postcode', y= 'finalCount', order= finalLoc_df.postcode, palette=\"Blues_d\", data= finalLoc_df)\n_.set_title('Top locations based on postcode', weight='bold', pad= 50)\n_.set_xlabel('Postcode', weight='bold', labelpad= 20)\n_.set_ylabel('finalCount', weight='bold', labelpad= 20)\n_.set_xticklabels(_.get_xticklabels(), rotation=90)\n\ntotal = float(finalLoc_df.finalCount.sum()) \n\nfor p in _.patches:\n    height = p.get_height()\n    _.text(p.get_x()+p.get_width()\/2., height + 5800, '{}  ({}%)'.format(int(height), int(round(height\/total, 2)*100)), ha=\"center\", va='center', fontsize=10, rotation = 90)\n'''","15dbe047":"fig, ax = plt.subplots(ncols=4, figsize= (50,8))\n\nfiltered_main_df.hour_pick.value_counts().sort_index().plot(ax = ax[0])\nfiltered_main_df.groupby(['hour_pick', 'passenger_count']).size().unstack('passenger_count').plot(ax = ax[1])\nfiltered_main_df.groupby(['hour_pick', 'weekdays_weekends']).size().unstack('weekdays_weekends').plot(ax = ax[2])\nfiltered_main_df.groupby(['hour_pick', 'taxi_type']).size().unstack('taxi_type').plot(ax = ax[3])\n\nax[0].set_title('Hourly Pickup Trend (Over 6 Months)')\nax[0].set_xlabel('Hours')\nax[0].set_xticks(ticks= filtered_main_df.hour_pick.value_counts().sort_index().keys())\n\nax[1].set_title('Hourly Pickup Trend By Passengers Count (Over 6 Months)')\nax[1].set_xlabel('Hours')\nax[1].set_xticks(ticks= filtered_main_df.hour_pick.value_counts().sort_index().keys())\n\nax[2].set_title('Hourly Pickup Trend By Weekends (Over 6 Months)')\nax[2].set_xlabel('Hours')\nax[2].set_xticks(ticks= filtered_main_df.hour_pick.value_counts().sort_index().keys())\n\nax[3].set_title('Hourly Pickup Trend By Taxi Types (Over 6 Months)')\nax[3].set_xlabel('Hours')\nax[3].set_xticks(ticks= filtered_main_df.hour_pick.value_counts().sort_index().keys())\n\nplt.show()","ea5a8c33":"plt.figure(figsize=(50,8))\n\nfiltered_main_df.pickup_datetime.dt.date.value_counts().sort_index().plot()\n\nplt.title('Trips By Date ( Over 6 Months)')\nplt.xlabel('Dates')\nplt.ylabel('Trip Counts')\nplt.xticks(label= filtered_main_df.pickup_datetime.dt.date.value_counts().sort_index().keys(), rotation = 90)\nplt.margins(0.005)\n\nplt.show()","bf91f32a":"plt.figure(figsize=(20,8))\n\nfiltered_main_df.loc[(filtered_main_df.month_pick == 1) & (filtered_main_df.day_pick == 23), 'hour_pick'].value_counts().sort_index().plot()\n\nplt.title('Trips By Date ( Over 6 Months)')\nplt.xlabel('Dates')\nplt.ylabel('Trip Counts')\nplt.xticks(label = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\nplt.margins(0.005)\n\nplt.show()","42c08eea":"print(filtered_main_df.loc[(filtered_main_df.month_pick == 1) & (filtered_main_df.day_pick == 23), ['pickup_datetime', 'hour_pick', 'rain', 'snow']].head())\nprint('')\nprint(filtered_main_df.loc[(filtered_main_df.month_pick == 1) & (filtered_main_df.day_pick == 23), ['pickup_datetime', 'hour_pick', 'rain', 'snow']].tail())\nprint('')\nprint('Total trips in 23-1-2016:  {} trips'.format(len(filtered_main_df[(filtered_main_df.month_pick == 1) & (filtered_main_df.day_pick == 23)])))","7669ee6c":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'day_part']).size().unstack('day_part').plot(ax = ax)\n\nax.set_title('Trips By Date & Part of The Day ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","c3b2bdfc":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'rush_hours']).size().unstack('rush_hours').plot(ax = ax)\n\nax.set_title('Trips By Date & Rush Hours ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","7795f017":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'taxi_type']).size().unstack('taxi_type').plot(ax = ax)\n\nax.set_title('Trips By Date & Taxi Type ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","5fbd8665":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'pickup_area']).size().unstack('pickup_area').plot(ax = ax)\n\nax.set_title('Trips By Date & Pickup Areas ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","0185ce5b":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'airport_pickup']).size().unstack('airport_pickup').plot(ax = ax)\n\nax.set_title('Trips By Date & Pickup Areas ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","7731482a":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'vendor_id']).size().unstack('vendor_id').plot(ax = ax)\n\nax.set_title('Trips By Date & Vendor ID ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","cca611b6":"months = {1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun'}\n\nfig, ax = plt.subplots(ncols=1, figsize= (15,8))\n\nfiltered_main_df.pickup_datetime.dt.month.value_counts().sort_index().plot()\n\nax.set_title('Trips Over 6 Months')\nax.set_xlabel('Months')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.month.value_counts().keys())\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \n\nplt.show()","340fe981":"# Temprary convert back 'store_and_fwd_flag' column\nstore_flag_dict_flip = {1: 'Y', 0: 'N'}\nfiltered_main_df.store_and_fwd_flag = filtered_main_df.store_and_fwd_flag.replace(store_flag_dict_flip)","f9ad38dc":"fig, ax = plt.subplots(ncols=1, figsize= (50,8))\n\nfiltered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'store_and_fwd_flag']).size().unstack('store_and_fwd_flag').plot(ax = ax)\n\nax.set_title('Storing Data Status (Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Trips')\nplt.margins(0.005)\n                \nplt.show()","b10f271f":"# Annotation function for the customized sns.barplot function \ndef annotationsCustom(plotVar, totalLength, style):\n \n    if style == 'both':\n        for p in plotVar.patches:\n            height = p.get_height()\n            plotVar.text(p.get_x()+p.get_width()\/2., height + (height * 0.02), '{} ({}%)'.format(int(height), int(round(height\/totalLength, 2)*100)), ha=\"center\", va='center', fontsize=10)\n    else:\n        for p in plotVar.patches:\n            height = p.get_height()\n            plotVar.text(p.get_x()+p.get_width()\/2., height + (height * 0.02), '{}%'.format(int(round(height\/totalLength, 2)*100)), ha=\"center\", va='center', fontsize=10)\n\n            \n# Customized sns.barplot function\ndef getGroupby(df, col1, col2, titleVar, xLabelVar, yLabelVar, figSize, colNum):\n    \n    # Check if col2 is a list or a string\n    if (colNum > 1) & (type(col2) == list):\n        \n        # Create the figure and number of columns\n        fig, ax = plt.subplots(ncols= colNum, figsize= figSize)\n        \n        # Loop through the col2 list by taking the index and calling the variable using col2[colIndx]\n        for colIndx in range(len(col2)):\n\n            tempVar = df.groupby(col1)[col2[colIndx]].mean()\n            x = tempVar.index\n            y = tempVar.values\n\n            _ = sns.barplot(x, y, ax = ax[colIndx])\n            \n            # Calling annotation function\n            totalLength = len(tempVar)\n            annotationsCustom(plotVar= _, totalLength= totalLength, style= 'both')\n\n            ax[colIndx].set_title(titleVar[colIndx], weight = 'bold', pad = 15)\n            \n            # Check if axes are list or string\n            if type(xLabelVar) == list:\n                ax[colIndx].set_xlabel(xLabelVar[colIndx])\n            else:\n                ax[colIndx].set_xlabel(xLabelVar)\n\n            if type(yLabelVar) == list:\n                ax[colIndx].set_ylabel(yLabelVar[colIndx])\n            else:\n                ax[colIndx].set_ylabel(yLabelVar)   \n\n            # CAN NOT ADD plt.show() in this part of the foor loop\n    else:\n        \n        plt.figure(figsize= figSize)\n        \n        tempVar = df.groupby(col1)[col2].mean()\n        x = tempVar.index\n        y = tempVar.values\n\n        _ = sns.barplot(x, y)\n        annotationsCustom(plotVar= _, totalLength= totalLength, style= 'both')\n\n        _.set_title(titleVar, weight = 'bold', pad = 15)\n        _.set_xlabel(xLabelVar)\n        _.set_ylabel(yLabelVar)\n\n    plt.show()\n        \n        \n        \n# Plot - Compare betweem storing data statuses 'Y' and 'N' with the mean of vender ID, trip_duration,        \n# Calling customized sns.barplot function\ngetGroupby(df = filtered_main_df,\n           col1= 'store_and_fwd_flag', \n           col2=['trip_duration', 'est_distance', 'avg_speed', 'vendor_id'],\n           titleVar=['Storing Data Status Based on Trip Duration', 'Storing Data Status Based on Estimated Distance', 'Storing Data Status Based on Average Speed', 'Storing Data Status Based on Vendor ID'], \n           xLabelVar='Storing Data Status', \n           yLabelVar=['Trip Duration Average', 'Distance Average', 'Speed Average', 'Vendor ID'], \n           figSize=(50, 8), \n           colNum=4)","63a09169":"fig, ax = plt.subplots(ncols=1, figsize= (50,12))\n\n#filtered_main_df.groupby([filtered_main_df.pickup_datetime.dt.date, 'temperature']).size().unstack('temperature').plot(ax = ax)\nsns.lineplot(x = filtered_main_df.pickup_datetime.dt.date, y = 'temperature', data = filtered_main_df, ax = ax)\n\nax.set_title('Daily Temperature ( Over 6 Months)')\nax.set_xlabel('Dates')\nax.set_xticks(ticks= filtered_main_df.pickup_datetime.dt.date.value_counts().keys())\nplt.xticks(rotation = 90)\nax.set_ylabel('Temperature')\nax.margins(0.005)                \n\nplt.show()","77b79bb9":"fig, ax = plt.subplots(ncols=1, figsize= (50,12))\n\nfiltered_main_df.groupby('temperature').size().plot(ax = ax)\n#sns.lineplot( x = filtered_main_df.temperature.value_counts().keys(), y = filtered_main_df.temperature.value_counts().values, data = filtered_main_df, ax = ax)\n\nax.set_title('Daily Temperature with Number of Trips (Over 6 Months)')\nax.set_xlabel('Temperatures')\nax.set_xticks(ticks= filtered_main_df.temperature.sort_values().unique())\n#plt.xticks(rotation = 90)\nax.set_ylabel('Temperature')\nax.margins(0.005)                \n\nplt.show()","720deb32":"fig, ax = plt.subplots(ncols=1, figsize= (50,12))\n\nfiltered_main_df.groupby(['temperature', 'taxi_type']).size().unstack('taxi_type').plot(ax = ax)\n\nax.set_title('Daily Temperature with Number of Trips (Over 6 Months)')\nax.set_xlabel('Temperatures')\nax.set_xticks(ticks= filtered_main_df.temperature.sort_values().unique())\n#plt.xticks(rotation = 90)\nax.set_ylabel('Temperature')\nax.margins(0.005)                \n\nplt.show()","ffdf47cf":"fig, ax = plt.subplots(ncols=1, figsize= (50,12))\n\nfiltered_main_df.groupby(['temperature', 'rush_hours']).size().unstack('rush_hours').plot(ax = ax)\n\nax.set_title('Daily Temperature with Number of Trips (Over 6 Months)')\nax.set_xlabel('Temperatures')\nax.set_xticks(ticks= filtered_main_df.temperature.sort_values().unique())\n#plt.xticks(rotation = 90)\nax.set_ylabel('Temperature')\nax.margins(0.005)                \n\nplt.show()","283a89a7":"# Replace digits with the name of each month\nmonths = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'No', 12:'Dec'}\n\n# Extract key\/ values from .value_counts() and display the results into dataframe style\na = filtered_main_df.rush_hours.value_counts().sort_index().keys()\nb = filtered_main_df.rush_hours.value_counts().sort_index().values\nc = round(filtered_main_df.rush_hours.value_counts(normalize=True).sort_index(), 3).values*100\nprint('Rush Hours: \\n{}\\n'.format(pd.DataFrame(data= list(zip(a, b, c)), columns=['rush_hours','counts', 'percentage']).set_index('rush_hours')))\n\na = filtered_main_df.month_pick.value_counts().sort_index().keys().map(months)\nb = filtered_main_df.month_pick.value_counts().sort_index().values\nc = round(filtered_main_df.month_pick.value_counts(normalize=True).sort_index(), 3).values*100\nprint('Monthly Distribution: \\n{}\\n'.format(pd.DataFrame(data= list(zip(a, b, c)), columns=['months','counts', 'percentage']).set_index('months')))\n\na = filtered_main_df.passenger_count.value_counts().sort_index().keys()\nb = filtered_main_df.passenger_count.value_counts().sort_index().values\nc = round(filtered_main_df.passenger_count.value_counts(normalize=True).sort_index(), 3).values*100\nprint('Number of Passenger Distribution: \\n{}\\n'.format(pd.DataFrame(data= list(zip(a, b, c)), columns=['number_of_passengers', 'counts', 'percentage']).set_index('number_of_passengers')))\n\na = filtered_main_df.taxi_type.value_counts().sort_index().keys()\nb = filtered_main_df.taxi_type.value_counts().sort_index().values\nc = round(filtered_main_df.taxi_type.value_counts(normalize=True).sort_index(), 3).values*100\nprint('Regular taxi VS Van taxi: \\n{}\\n'.format(pd.DataFrame(data= list(zip(a, b, c)), columns=['taxi_types', 'counts', 'percentage']).set_index('taxi_types')))","d33417d2":"# Interactive plotting \n\ndef plotmonth(month):\n    \n    data = filtered_main_df[filtered_main_df.month_pick == month]\n    circleSize = data.avg_speed\n    colors = data.passenger_count.map({1: 'skyblue', 2: 'coral', 3: 'green', 4: 'gold', 5: 'palegreen', 6: 'brown'})\n    \n    data.plot.scatter('trip_duration', 'est_distance', s = circleSize, c = colors, alpha = 0.8, linewidths = 1, edgecolors = 'white', figsize=(25,12))\n    \n    plt.show()\n    \n#plotmonth(1)\n\n# Interactive plotting is not working. Needs to check the nodejs and npm packages first.\ninteract(plotmonth, month = widgets.IntSlider(min=1, max=6, step=1, value=1))","3a4f887d":"from sklearn.cluster import MiniBatchKMeans","11f6443d":"filtered_main_df.columns","6990bdbd":"# Prepare the coordinates \ncords = np.vstack((filtered_main_df[['pickup_latitude_round3', 'pickup_longitude_round3']], filtered_main_df[['dropoff_latitude_round3', 'dropoff_longitude_round3']])) # np.vstack function takes tuple","ce3f34d2":"# Randomize the cords\ncordsSamples = np.random.permutation(len(cords))[:500000]","da5fe9b7":"# Train the MiniBatchKMeans and fit it on the cordsSamples\nminiKmean = MiniBatchKMeans(n_clusters= 50, batch_size= 10000).fit(cords[cordsSamples])","a7c1f21c":"# Predict the LAT and LON points and assign the clusters into their new columns\nfiltered_main_df['pickup_cluster'] = miniKmean.predict(filtered_main_df[['pickup_latitude_round3', 'pickup_longitude_round3']])\nfiltered_main_df['dropoff_cluster'] = miniKmean.predict(filtered_main_df[['dropoff_latitude_round3', 'dropoff_longitude_round3']])","b379310b":"filtered_main_df.head()","35f22580":"# Scatter plot with the new rounded LAT & LON\nplt.style.use('dark_background')\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(18,10))\n\nfiltered_main_df.plot(kind='scatter', x='pickup_longitude_round3', y='pickup_latitude_round3', c= 'pickup_cluster', s=.1, alpha=.2, subplots=True, ax=ax1)\nax1.set_title(\"Pickups\", color= 'white')\nax1.axes.tick_params(color= 'white', labelcolor= 'white')\nax1.set_facecolor('black')\nax1.grid(False)\n\nfiltered_main_df.plot(kind='scatter', x='dropoff_longitude_round3', y='dropoff_latitude_round3', c= 'dropoff_cluster', s=.1, alpha=.2, subplots=True, ax=ax2)\nax2.set_title(\"Dropoffs\", color= 'white')\nax2.axes.tick_params(color= 'white', labelcolor= 'white')\nax2.set_facecolor('black') \nax2.grid(False)","52f71b48":"cols = ['pickup_longitude_round3', 'pickup_latitude_round3']\ncordsGroupedBy_MeanSpeed = filtered_main_df.groupby(cols)['avg_speed'].mean().reset_index()\ncordsGroupedBy_CountVid = filtered_main_df.groupby(cols)['vendor_id'].count().reset_index()\ncordsStats = pd.merge(cordsGroupedBy_MeanSpeed, cordsGroupedBy_CountVid, on= cols)\ncordsStats = cordsStats[cordsStats.vendor_id > 100]","91945254":"plt.style.use('dark_background')\nfig, ax = plt.subplots(ncols=1, figsize= (10,14))\n\nax.scatter( x=filtered_main_df.pickup_longitude_round3.values, y= filtered_main_df.pickup_latitude_round3.values, c= 'white', s=.5, alpha=.2)\nax.scatter(x= cordsStats.pickup_longitude_round3.values, y= cordsStats.pickup_latitude_round3.values, c= cordsStats.avg_speed.values, cmap='YlOrBr', s=5, alpha=.8, vmin=1, vmax=8)\n\nax.set_title(\"Average Speed\", color= 'white')\nax.axes.tick_params(color= 'white', labelcolor= 'white')\nax.set_facecolor('black')\nax.grid(False)\n\nplt.show()","37df1cac":"cordsGroupedBy_MeanTripDur = filtered_main_df.groupby(cols)['trip_duration'].mean().reset_index()\ncordsStats_TripDur = pd.merge(cordsGroupedBy_MeanTripDur, cordsGroupedBy_CountVid, on= cols)\ncordsStats_TripDur = cordsStats_TripDur[cordsStats_TripDur.vendor_id > 100]\n\nplt.style.use('dark_background')\nfig, ax = plt.subplots(ncols=1, figsize= (10,14))\n\nax.scatter( x=filtered_main_df.pickup_longitude_round3.values, y= filtered_main_df.pickup_latitude_round3.values, c= 'white', s=.5, alpha=.2)\nax.scatter(x= cordsStats_TripDur.pickup_longitude_round3.values, y= cordsStats_TripDur.pickup_latitude_round3.values, c= cordsStats_TripDur.trip_duration.values, cmap='YlOrBr', s=5, alpha=.8, vmin=1, vmax=8)\n\nax.set_title(\"Trip Duration\", color= 'white')\nax.axes.tick_params(color= 'white', labelcolor= 'white')\nax.set_facecolor('black')\nax.grid(False)\n\nplt.show()","1693f718":"cordsGroupedBy_MeanEstDis = filtered_main_df.groupby(cols)['est_distance'].mean().reset_index()\ncordsStats_MeanEstDis = pd.merge(cordsGroupedBy_MeanEstDis, cordsGroupedBy_CountVid, on= cols)\ncordsStats_MeanEstDis = cordsStats_MeanEstDis[cordsStats_MeanEstDis.vendor_id > 100]\n\nplt.style.use('dark_background')\nfig, ax = plt.subplots(ncols=1, figsize= (10,14))\n\nax.scatter( x=filtered_main_df.pickup_longitude_round3.values, y= filtered_main_df.pickup_latitude_round3.values, c= 'white', s=.5, alpha=.2)\nax.scatter(x= cordsStats_MeanEstDis.pickup_longitude_round3.values, y= cordsStats_MeanEstDis.pickup_latitude_round3.values, c= cordsStats_MeanEstDis.est_distance.values, cmap='YlOrBr', s=5, alpha=.8, vmin=1, vmax=8)\n\nax.set_title(\"Estimated Distance\", color= 'white')\nax.axes.tick_params(color= 'white', labelcolor= 'white')\nax.set_facecolor('black')\nax.grid(False)\n\nplt.show()","e6b2c527":"mapped_rain = {0: 'No Rain ', 1: 'Raining'} \nmapped_snow = {0: 'No Snow ', 1: 'Snowing'}\nmapped_pickup_dropoff_airport = {'City': 'Not Airport'}\n\nfiltered_main_df.weekday_pick.replace(map_weekdays, inplace= True)\nfiltered_main_df.month_pick.replace(map_months, inplace= True)\nfiltered_main_df.rain.replace(mapped_rain, inplace= True)\nfiltered_main_df.snow.replace(mapped_snow, inplace= True)\nfiltered_main_df.airport_pickup.replace(mapped_pickup_dropoff_airport, inplace= True)","760023e5":"filtered_main_df.columns","5d42af6d":"# Prepare the columns for training dataset\ntrain_cols = ['vendor_id', 'passenger_count', 'store_and_fwd_flag', 'trip_duration', 'pickup_area', 'dropoff_area', 'airport_pickup', 'airport_dropoff', 'pickup_latitude_round3', 'pickup_longitude_round3', 'dropoff_latitude_round3', 'dropoff_longitude_round3', 'est_distance', 'year_pick', 'month_pick', 'day_pick', 'weekday_pick', 'hour_pick', 'weekdays_weekends', 'taxi_type', 'avg_speed', 'season', 'rush_hours', 'day_part', 'temperature', 'rain', 'snow', 'weather_condition', 'pickup_cluster', 'dropoff_cluster']","ed5b9747":"# Create training dataset and assign the columns \ntrain_df = filtered_main_df[train_cols].copy()","693e7c3b":"# Check\ntrain_df.head()","5cd77168":"# 2.3. Dataset quick overview","32467e88":"# NOTE\n1. Based on NYC Tai website, night surcharge = USD 0.50 after 8:00 PM and before 6:00 AM\n2. peak hours weekdays = USD 1.00 Monday to Friday after 4:00 PM and before 8:00 PM\n3. Tolls are on the bridges and tunnels. Cost is here https:\/\/beta.costtodrive.com\/tolls-on-popular-bridges-and-tunnels\/\n4. The latitude of Times Square, Manhattan, NY, USA is 40.758896, and the longitude is -73.985130. Any point falls out of less than NT city LAT and LON will assume that it has at least 1 toll\n5. NY (entire city) 40.7128\u00b0 N, 74.0060\u00b0 W\n6. Manhattan 40.7831\u00b0 N, 73.9712\u00b0 W\n7. Manhattan island area size: 59.1 km\u00b2\n8. Brooklyn 40.6782\u00b0 N, 73.9442\u00b0 W \n9. Brooklyn land area: 180 km\u00b2\n10. LAT ( North <-> South ), LON ( West <-> East )\n11. Use http:\/\/www.mapcoordinates.net\/en to see LAT & LON real time and apply knowledge to this project to seperate Manhattan island from Brooklyn and other cities to add tolls details \n12. NY central park LAT 40.7903 LON -73.9597\n\n# Action\n1. Try to figure out if there's tunnel or bridge during the trip either by geomap or by trip distance (i.e. trip > 6 KM)\n2. Add new column for tolls (1 and 0)\n3. Add toll cost column\n\n# Weekdays\n1. Monday=0, Tuesday=1, Wednesday=2, Thursday=3, Friday=4, Saturday=5, Sunday=6","27daa033":"# 3.4. Training dataset LAT & LON visualization (scatter plot)","4167e739":"# 1.3. Import Datasets","dc487c4d":"# 3.3. Training dataset visualization (on tip_duration distribution)","7817770b":"# 4.2. Extract year, month, date, weekday and hours.","24f7daae":"# 2.1. Understand the training and the testing datasets shape","0cf204e3":"# 4.3. Extract taxi type, rush hours, night charges, peak hours and avg speed.","6c6c0fff":"# 5.5.1. What are the top 10 pickup places?","9a21561a":"# Note\n\n1. Initial outliers were removed.\n2. New columns for the LAT & LON with 3 decimal points for better visualisation and data processing (in the next step 'feature engineering')\n3. New columns for pickup\/ dropoff areas\n4. New columns for pickup\/ dropoff airports","4bbc909a":"# 1. Importing\n    \n# 1.1. Importing libraries","5dceca70":"# 4. Features Extraction\n    4.1. Generate distance estimation based on LAT & LON points\n    4.2. Extract year, month, date, weekday and hours.\n    4.3. Extract taxi type, rush hours, night charges, peak hours and avg speed.\n    4.4. Add NY city weather dataset to the training dataset\n        4.4.1. Weather dataset preperation\n        4.4.2. Add to filtered_main_df","7b49819d":"#######################################################################################################################################################################################################################################################################################################\n# check dates (pickup\/ dropoff) if they are not the same while the pickup time is less than 23:59\n\nprint('main_df lenght is: {}'.format(len(main_df)))\nprint('updated main_df lenght is: {}'.format(len(main_df) - len(main_df.loc[(main_df.pickup_datetime.dt.date != main_df.dropoff_datetime.dt.date) & (main_df.pickup_datetime.dt.hour <= 23) & (main_df.dropoff_datetime.dt.minute <= 59)])))\nprint('Lenght of the observations that are incorrect: {}\\n'.format(len(main_df.loc[(main_df.pickup_datetime.dt.date != main_df.dropoff_datetime.dt.date) & (main_df.pickup_datetime.dt.hour <= 23) & (main_df.dropoff_datetime.dt.minute <= 59)])))\nprint('Example of the observations that are incorrect (dates are not the same while the time is less than 23:59): \\n{}\\n'.format(main_df.loc[(main_df.pickup_datetime.dt.date != main_df.dropoff_datetime.dt.date) & (main_df.pickup_datetime.dt.hour <= 23) & (main_df.dropoff_datetime.dt.minute <= 59)].head()))\n\n# Remove the incorrect dates observations\nmain_df = main_df.loc[~((main_df.pickup_datetime.dt.date != main_df.dropoff_datetime.dt.date) & (main_df.pickup_datetime.dt.hour <= 23) & (main_df.dropoff_datetime.dt.minute <= 59))].copy()\n########################################################################################################################################################################################################################################################################################################","fb6224de":"# 2.2. Dataset description including categorical observations","4b49e2b4":"# 4.4. Add NY city weather dataset to the training dataset\n        4.4.1. Weather dataset preperation\n        4.4.2. Add to filtered_main_df","30dce750":"1. Importing\n    1.1. Importing libraries\n    1.2. Import Datasets\n    1.3. Set default options\n\n2. Initial Discovery\n    2.1. Understand the training and the testing datasets shape\n    2.2. Dataset description including categorical dataset distribution - main_df.describe(include=[\u2018O\u2019])\n    2.3. Dataset quick overview\n    2.4. Observation and notes\n\n3. Cleaning & Optimizing Training Dataset\n    3.1. Treat unrealistic values and outliers\n    3.2. Treat missing values and optimize data types for better memory usage\n    3.3. Training dataset visualization (on tip_duration distribution)\n    3.4. Training dataset LAT & LON visualization (scatter plot)\n    3.5. Define NY city LAT \/ LON and visualize it (scatter plot)\n    3.6. Display NY interactive map using folium library\n\n4. Features Extraction\n    4.1. Generate distance estimation based on LAT & LON points\n    4.2. Extract year, month, date, weekday and hours.\n    4.3. Extract peak hours, night charges, taxi type and avg speed.\n        4.3.1. Dataset unrealistic \/ outlier points\n    4.4. Add NY city weather dataset to the training dataset\n        4.4.1. Weather dataset preperation\n        4.4.2. Add to filtered_main_df\n\n5. Analysis\n    5.1. Treat outliers    \n    5.2. Univariate analysis (distplot, countplot)\n    5.3. Bi-variate analysis (barplot, violinplot, regplot)\n    5.4. Multivariate analysis (corr(), heatmap)\n    5.5. Questions (Graphical Representation) :\n            5.5.1. What are the top 10 pickup places?\n            5.5.2. What are the least 10 pickup places?\n            5.5.3. What are the top 10 dropoff places?\n            5.5.4. What are the least 10 dropoff places?\n            5.5.5. Trends across seasons (months)\n            5.5.6. What is busiest month, weekday, hour?\n            5.5.7. Does the weather condition affect the trip duration?\n            5.5.8. Rush hours VS regular hours?\n            5.5.9. Taxi vendor comparison\n    5.6. Questions (Statistical Tests) :\n            5.5.1. What are the top 10 pickup places?\n            ********************\n\n8. Feature Engineering\n    8.1. Select features\n    8.2. Vectorize features using OneHotEncoding()\n    8.3. Scaling numeric features using MinMaxScaler() link\n\n7. Machine Learning link\n    7.1. Build and select model\n    7.2. Model evaluation\n    7.3. Model hyper-parameters tuning\n    7.4. Prediction","15d24056":"# 3.2. Treat missing values and optimize data types for better memory usage\n    weather_df dataset notes: -  \n    - Only 5 values are missing in 'temp'. It will be treated seperately.\n    - 'conds' column data type needs to be changed to numeric instead of 'object'.  ","ec098e52":"# 2. Initial Discovery \n    2.1. Understand the training and the testing datasets shape\n    2.2. Dataset description including categorical dataset distribution - main_df.describe(include=[\u2018O\u2019])\n    2.3. Dataset quick overview\n    2.4. Observation and notes\n    2.5. Training dataset visualization (on tip_duration distribution)\n    2.6. Training dataset LAT & LON visualization (scatter plot)","9f9d844b":"# 4.1. Generate distance estimation based on LAT & LON points","9ac9715a":"# Observation: \n    Based on the above data, I can see that the number of trips is much lower than the usual. It might be because of the snow, or\/ and because of the taxi strike in New York during this period based on google search.","d980f3af":"# 3.1. Treat unrealistic values and outliers","22cb6528":"# 3. Cleaning & Optimizing Training Dataset\n    3.1. Treat unrealistic values and outliers\n    3.2. Treat missing values and optimize data types for better memory usage\n    3.3. Training dataset visualization (on tip_duration distribution)\n    3.4. Training dataset LAT & LON visualization (scatter plot)\n    3.5. Define NY city LAT \/ LON and visualize it (scatter plot)\n    3.6. Display NY interactive map using folium library","b8f472e0":"# Testing LAT and LON with round(3)","56421827":"# Create clusters (Neighborhoods) using MiniBatchKMeans","e67a5e65":"# 3.6. Display NY interactive map using folium library","af01507a":"# Quick observation on Tip Duration:\n1. Unrealistis tirp duration (i.e. 1 second and 3526282 seconds)\n2. Variance is big: 27430691.1\n3. The spread of the data is big: 5237.4\n\n# Graphs observation\n1. From visualizing the trip duration distribution, the approach will be - mean (2 * std) - or - median ( 2 * iqr).\n2. Statistically, it's always recommended to use the median instead of the mean when the data is not normally distributed with long tail. The mean is affected with outliers.\n3. However, if we take the meida approach, it means that we will remove valiable data (i.e. all trips greater than 40 mins will be removed which means, some of the airports trips will be removed)\n4. I will go with the mean approach in this analysis and I will apply the same analysis on the second approach (median). \n\n# Action:\n1. To exclude the data that lies outside 2 standard deviations from the mean assuming they are outliers\n2. Remove observations where trip_duration < 60 seconds\n3. Reset the index","89a9d6a0":"# Machine Learning Part","f6a59d6c":"# Trip Duration Prediction Analysis & ML Preparation\n  Project Steps","ae710d06":"# 3.5. Define NY city LAT \/ LON and visualize it (scatter plot)","26c37c3e":"# 5. Analysis\n    5.1. Treat outliers    \n    5.2. Univariate analysis (distplot, countplot)\n    5.3. Bi-variate analysis (barplot, violinplot, regplot)\n    5.4. Multivariate analysis (corr(), heatmap)\n    5.5. Questions :\n            5.5.1. What are the top 10 pickup places?\n            5.5.2. What are the least 10 pickup places?\n            5.5.3. What are the top 10 dropoff places?\n            5.5.4. What are the least 10 dropoff places?\n            5.5.5. Trends across seasons (months)\n            5.5.6. What is busiest month, weekday, hour?\n            5.5.7. Does the weather condition affect the trip duration?\n            5.5.8. Rush hours VS regular hours?\n            5.5.9. Taxi vendor comparison","b9c096a2":"# 5.5. Questions :\n    5.5.1. What are the top 10 pickup places?\n    5.5.2. What are the least 10 pickup places?\n    5.5.3. What are the top 10 dropoff places?\n    XXXXX 5.5.4. What are the least 10 dropoff places?\n    5.5.5. Trends across seasons (months)\n    5.5.6. What is busiest month, weekday, hour?\n    5.5.7. Does the weather condition affect the trip duration?\n    5.5.8. Rush hours VS regular hours?\n    5.5.9. Taxi vendor comparison","ac9658ce":"# Notes\n\n1. I will take the results of the LAT & LON with 3 decimal points as it's more accurate when I manually run a random check on Google maps.\n2. All observations where average speed is 0, distance is always less than 1 KM. using the LAT & LON on Google maps, it shows that both locations are closed to each other and it is mostly red (busy route)\n3. I have decided to update '0' to '1'","046c0fd2":"# 1.2. Set default options","a8a455be":"2.4. Observation and notes\n\n    1. test_df is 43.0% of the main_df, almost half of the training dataset.\n\n    2. There are 2 columns dont exist in the test_df, 'dropoff_datetime' and 'trip_duration'which we will be predict.\n\n    3. main_df : \n    \n        A) Focus on trip_duration as this is the feature we are going to work with and predict.\n        B) 'pickup_datetime', 'dropoff_datetime' dtype is 'object'. Needs to be 'datetime' type.\n        C) Rest of columns needs to be checked and optimized by converting 'int64' to 'int32' and 'float64' to 'float32'.\n        D) 'store_and_fwd_flag' dtype is 'object'. Since its just 'Y' and 'N', I will replace them with '1' & '0' and convert it to 'int32'.\n        E) No missing values\n        F) memory usage: 122.4+ MB. Optimizing the dtypes will definitely reduce the usage of memory.\n        G) 'passenger_count' contains '0', '7', '8' and '9' values. Needs to be checked and remove it\/ update it when applicable based on the rest of the observations.\n\n    4. test_df : \n    \n        A) 'pickup_datetime' dtype is 'object'. Needs to be 'datetime' type.\n        B) Rest of columns needs to be checked and optimized by converting 'int64' to 'int32' and 'float64' to 'float32'.\n        C) 'store_and_fwd_flag' dtype is 'object'. Since its just 'Y' and 'N', I will replace them with '1' & '0' and convert it to 'int32'.\n        D) No missing values\n        E) 'passenger_count' contains '0' value. Needs to be checked and remove it\/ update it when applicable based on the rest of the observations. \n\n    5. weather_df : ( will be processed later )\n        \n        A) 'pickup_datetime' dtype is 'object'. Needs to be 'datetime' type.\n        B) Rest of columns needs to be checkedd and optimized by converting 'int64' to 'int32' and 'float64' to 'float32'.\n     ** C) 'conds' dtype is 'object' that contains only 16 unique values repeated over 10000 time. Best practice here is to convert it to 'category' type. However, I have decided to replace existing values with numerics for future calculations.\n     ** D) 'tempm' column is missing 5 values. Since its tempratures column. I have decided to take the value of the previous day\/ hour value using 'ffill' method.\n\n    6. Reorder the samples\/ observations based on the 'pickup_datetime' \n\n    7. I will drop the 'id' column from both main_df and test_df.\n    \n\n\n** weather_df will be treated separately.","2f2f8141":"# Observations\n1. Where estimated distance = 0, the LAT\/LNG for both pickup and dropoff are the same.\n2. Since these observations were invalid, I decided to remove them."}}