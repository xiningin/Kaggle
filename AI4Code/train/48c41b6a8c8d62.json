{"cell_type":{"e6c402ba":"code","164932d5":"code","b503e8fe":"code","69f6ecda":"code","3fce60b8":"code","bc823746":"code","8aeccef2":"code","da60d5a5":"code","5fa2dda4":"code","7420228a":"code","c5f53e98":"code","7a75d328":"code","24e73df6":"code","5f90fee9":"code","215c757f":"code","e9ce4d3f":"code","419b3d30":"code","512826ca":"code","d93a4426":"code","2c1fbb43":"markdown"},"source":{"e6c402ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","164932d5":"import h2o\n#connecting to cluster\nh2o.init(strict_version_check=False)","b503e8fe":"data_csv = \"\/kaggle\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\"\ndata = h2o.import_file(data_csv)","69f6ecda":"data.describe()","3fce60b8":"data.rename(columns={\"PAY_0\": \"PAY_1\"}) #for consistency\ndata.rename(columns={'default.payment.next.month': \"DEFAULT\"}) #easier\n\ncols_names = data.columns #because we know the data type for all the columns (they are all ints)\ncols_names","bc823746":"not_categorical = ['ID',\n 'LIMIT_BAL',\n  'AGE',\n 'BILL_AMT1',\n 'BILL_AMT2',\n 'BILL_AMT3',\n 'BILL_AMT4',\n 'BILL_AMT5',\n 'BILL_AMT6',\n 'PAY_AMT1',\n 'PAY_AMT2',\n 'PAY_AMT3',\n 'PAY_AMT4',\n 'PAY_AMT5',\n 'PAY_AMT6']\n\ntarget = \"DEFAULT\"\n\ncategorical = [item for item in cols_names if item not in not_categorical and item != target]\ncategorical","8aeccef2":"data.head()","da60d5a5":"#Onehot encoding (as labels are already encoded as numbers)\n\ndata_onehot = pd.get_dummies(data.as_data_frame(), columns=categorical)\ndata_onehot.head()","5fa2dda4":"#Drop the ID column\n\ndata_onehot = data_onehot.drop(columns=['ID'])","7420228a":"data_onehot.columns","c5f53e98":"#Creating equally sized bins for age - 5 categories\n\nprint(data_onehot['AGE'].describe())\n\n#add age bins to make it all-inclusive - in case new data may come\n\ndata_onehot['AGE_BINS'] = pd.qcut(data_onehot['AGE'], 5)\n\n#Add age bins for ages (0, 20.999] and (79.0, ) - even though there may be no data for this in the present dataset, it is important to do this in case we have future data\n\ndata_onehot['AGE_BINS_(0, 20.999]'] = 0 #in the same format as after one hot encoding (doing this two cells later)\ndata_onehot['AGE_BINS_(79.0, )'] = 0\n","7a75d328":"data_onehot.head() #it works!","24e73df6":"#Now we use one hot encoding for these categories\n\ndata_age = pd.get_dummies(data_onehot, columns=['AGE_BINS'])\ndata_age = data_age.drop(columns=['AGE'])\ndata_age.head()","5f90fee9":"#some statistical featurs\n\nbill_amt_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\npay_amt_cols = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n\n#mean of Bill_amt and Pay_amt, max, min, std, var\n\ndata_age['BILL_AMT_MEAN'] = data_age[bill_amt_cols].mean(axis=1)\ndata_age['PAY_AMT_MEAN'] = data_age[pay_amt_cols].mean(axis=1)\n\ndata_age['BILL_AMT_MAX'] = data_age[bill_amt_cols].max(axis=1)\ndata_age['PAY_AMT_MAX'] = data_age[pay_amt_cols].max(axis=1)\n\ndata_age['BILL_AMT_MIN'] = data_age[bill_amt_cols].min(axis=1)\ndata_age['PAY_AMT_MIN'] = data_age[pay_amt_cols].min(axis=1)\n\ndata_age['BILL_AMT_MED'] = data_age[bill_amt_cols].median(axis=1)\ndata_age['PAY_AMT_MED'] = data_age[pay_amt_cols].median(axis=1)\n\ndata_age['BILL_AMT_STD'] = data_age[bill_amt_cols].std(axis=1)\ndata_age['PAY_AMT_STD'] = data_age[pay_amt_cols].std(axis=1)\n\ndata_age['BILL_AMT_VAR'] = data_age[bill_amt_cols].var(axis=1)\ndata_age['PAY_AMT_VAR'] = data_age[pay_amt_cols].var(axis=1)\n\n\ndata_age.head()","215c757f":"#some new variables\n\n#payment fraction of bill statement\nfor i in range(1, 7):        \n    data_age['PAY_FRAC_' + str(i)] = data_age[pay_amt_cols[i-1]] \/ data_age[bill_amt_cols[i-1]]\ndata_age = data_age.fillna(0)\n\n\n#fraction of credit limit used (bill_amt \/ limit_bal)\nfor i in range(1, 7):        \n    data_age['USED_CREDIT' + str(i)] = data_age[bill_amt_cols[i-1]] \/ data_age['LIMIT_BAL']\ndata_age = data_age.fillna(0)\n\n\ndata_age.head()","e9ce4d3f":"data_age['PAY_FRAC_1'].max()\n\n\n\n#There are 540. Three simple ways to deal: delete feature, delete rows, set to zero. Have to test.\n\n#Setting to zero\n\nfor i in range (1, 7):\n    #print(len(data_age[data_age['PAY_FRAC_' + str(i)] == np.inf])) #0 of them are -np.inf\n    data_age['PAY_FRAC_' + str(i)] = data_age['PAY_FRAC_' + str(i)].replace({np.inf: 0})\n    #print(len(data_age[data_age['PAY_FRAC_' + str(i)] == np.inf]))","419b3d30":"#Scaling\n\n#Using standard scalar scaling\n#Multiple methods such as min-max scaling, standard scaling, etc. All have different advantages and depend on the distribution of data.\n#Can always change this in the next iterations of the ML pipeline. Trial and error process.\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\nscaled_features = data_age.copy()\n\ncol_names = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4' ,'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4' ,'PAY_AMT5', 'PAY_AMT6', 'PAY_FRAC_1', 'PAY_FRAC_2', 'PAY_FRAC_3', 'PAY_FRAC_4', 'PAY_FRAC_5', 'PAY_FRAC_6', 'USED_CREDIT1', 'USED_CREDIT2', 'USED_CREDIT3', 'USED_CREDIT4', 'USED_CREDIT5', 'USED_CREDIT6']\nfeatures = scaled_features[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n\nscaled_features[col_names] = features\nscaled_features","512826ca":"scaled_df = pd.DataFrame(scaled_features, columns=['LIMIT_BAL', 'BILL_AMT1', 'PAY_AMT1', 'USED_CREDIT1'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n\nax1.set_title('Before Scaling')\nsns.kdeplot(data_age['LIMIT_BAL'], ax=ax1) #kernel density estimate plot (non-parametric way to estimate the probability density function of a random variable.)\nsns.kdeplot(data_age['BILL_AMT1'], ax=ax1)\nsns.kdeplot(data_age['PAY_AMT1'], ax=ax1)\nsns.kdeplot(data_age['USED_CREDIT1'], ax=ax1)\nax2.set_title('After Standard Scaler')\nsns.kdeplot(scaled_df['LIMIT_BAL'], ax=ax2)\nsns.kdeplot(scaled_df['BILL_AMT1'], ax=ax2)\nsns.kdeplot(scaled_df['PAY_AMT1'], ax=ax2)\nsns.kdeplot(scaled_df['USED_CREDIT1'], ax=ax2)\nplt.show()","d93a4426":"scaled_features.columns","2c1fbb43":"We can see here how the data is scaled. Now, we have the dataframe *scaled_features.*"}}