{"cell_type":{"7971c146":"code","28ca90d6":"code","f17ddd4c":"code","5bc89c20":"code","7529773f":"code","7252221c":"code","344f8d55":"code","462f084d":"code","3667d97b":"code","316bdda8":"code","ed3619dc":"code","96514055":"code","645604d4":"code","542f9d1d":"code","41d40032":"code","ae5ea91d":"code","29692500":"code","a44397f8":"code","b99bc350":"code","fe6a66d8":"code","7ab61b92":"code","6270f419":"code","8688fa6c":"code","8c26760b":"code","9fb1bfc2":"code","abce93c7":"code","e9fc1738":"code","8cb42e25":"code","be25cd44":"code","800d594d":"code","e7cd2450":"code","7dde6756":"code","00d6f7bc":"code","310982b8":"code","5ca71da0":"markdown","526ec3f7":"markdown","46c5dc27":"markdown","c4ecebd0":"markdown","e8b9f615":"markdown","5f110b56":"markdown","719e6afb":"markdown","1c733c6d":"markdown","d2a184b2":"markdown","a179227e":"markdown","ec0fbc86":"markdown","0f1799a8":"markdown","541eedff":"markdown","bc6f7856":"markdown","bb45acd5":"markdown","812812c3":"markdown","8ee25d86":"markdown","caac9727":"markdown","2ca38fb3":"markdown","dcc4e251":"markdown","6d0981ce":"markdown","64dd3f6f":"markdown","88f07859":"markdown"},"source":{"7971c146":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nsys.path.append('..\/input\/shopee-competition-rgr')","28ca90d6":"%%bash\nmkdir -p .\/src && \\\ncp ..\/input\/shopee-competition-rgr\/*.py .\/src","f17ddd4c":"import gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nimport cv2\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch import nn\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Subset, DataLoader\n\nfrom src.config import CFG\nfrom src.dataset import ShopeeDataset\nfrom src.loss import Mish, replace_activations\nfrom src.model import ShopeeCNNModel\nfrom src.train import train_fn, eval_fn\nfrom src.transforms import get_train_transforms, get_test_transforms\nfrom src.utils import read_dataset\n\nimport warnings\nwarnings.filterwarnings('ignore')","5bc89c20":"BASE_DIR = \"..\/input\/shopee-product-matching\"","7529773f":"train = pd.read_csv(f'{BASE_DIR}\/train.csv')\ntest = pd.read_csv(f'{BASE_DIR}\/test.csv')","7252221c":"train.posting_id = train.posting_id.str.replace('train_', '')","344f8d55":"train.head(3)","462f084d":"train.label_group.value_counts()[:10]","3667d97b":"ax = plt.axes()\nsns.boxplot(train.label_group.value_counts(), ax=ax)\nax.set_xlabel(\"Count\")\nax.set_ylabel(\"Labels groups\")","316bdda8":"duplicated_labels= train[\"label_group\"].value_counts()[:20]\nplt.xticks(range(len(duplicated_labels)), duplicated_labels.index, rotation=90)\nplt.bar(range(len(duplicated_labels)), duplicated_labels.values)\nplt.show()","ed3619dc":"labels_to_examine = duplicated_labels.iloc[np.random.randint(0, len(duplicated_labels), 3)].to_frame()\nlabels_to_examine","96514055":"len(train), train[\"image\"].nunique()","645604d4":"def visualize_similar_imgs(random=False, COLS=6, ROWS=4, base_path=BASE_DIR):\n    root = f'{base_path}\/train_images'\n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            if random: row = np.random.randint(0,len(train))\n            else: row = COLS*k + j\n            name = train.iloc[row,1]\n            title = train.iloc[row,3]\n            title_with_return = \"\"\n            for i,ch in enumerate(title):\n                title_with_return += ch\n                if (i!=0)&(i%20==0): title_with_return += '\\n'\n                img = cv2.imread(str(Path(root).joinpath(name)))\n            plt.subplot(1,COLS,j+1)\n            plt.title(title_with_return)\n            plt.axis('off')\n            plt.imshow(img)\n    plt.show()","542f9d1d":"visualize_similar_imgs()","41d40032":"to_visualize = 5\n\ndef add_newlines_to_title(title: str):\n    max_chars = 15\n    idxs = [i*max_chars for i in range(0, len(title) \/\/ max_chars)]\n    for i in idxs:\n        title = title[:i]+'\\n'+title[i:]\n    return title+'\\n'\n\ndef visualize_dupl_images():\n    for k, (lg, dup_n) in enumerate(labels_to_examine.iterrows()):\n        plt.figure(figsize=(20,5))\n        samples = train[train.label_group==lg][:to_visualize]\n        title = samples.loc[:,'title'].values\n        names = samples.loc[:,'image'].values\n        for j in range(to_visualize):\n            img_path = str(Path(BASE_DIR).joinpath(f'train_images\/{names[j]}'))\n            img = cv2.imread(img_path)\n            plt.subplot(1,to_visualize,j+1)\n            img_title = add_newlines_to_title(title[j])\n            plt.title(img_title)\n            plt.axis('off')\n            plt.imshow(img)\n    plt.show()\n\nvisualize_dupl_images()","ae5ea91d":"target = \"label_group\"\nlabels = train[target]\ndescr = train.title\nphash = train.image_phash\nimages = train.image","29692500":"descr","a44397f8":"phash, phash.nunique(), labels.nunique()","b99bc350":"from PIL import Image\nimport imagehash\nimg_path1 = f'{BASE_DIR}\/train_images\/{images[0]}'\nimg_path2 = f'{BASE_DIR}\/train_images\/{images[1]}'\n\ndef find_img_phashes(img):\n    avg_hash = imagehash.average_hash(img)\n    diff_hash = imagehash.dhash(img)\n    dct_hash = imagehash.phash(img)\n    wavelet_hash = imagehash.whash(img)\n    print('Hashes:')\n    print('AVG: ' + str(avg_hash))\n    print('DIFF: ' + str(diff_hash))\n    print('DCT: ' + str(dct_hash))\n    print('Wavelet: ' + str(wavelet_hash))\n    print('\\nTrue hash: '+phash[0])\n    return avg_hash, diff_hash, dct_hash, wavelet_hash\n\nprint(\"Image 1 perceptual hashes:\")\navg_hash1, diff_hash1, dct_hash1, wavelet_hash1 = find_img_phashes(Image.open(img_path1))\nprint(\"\\nImage 2 perceptual hashes:\")\navg_hash2, diff_hash2, dct_hash2, wavelet_hash2 = find_img_phashes(Image.open(img_path2))\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 10)) \nax[0].imshow(Image.open(img_path1))\nax[1].imshow(Image.open(img_path2))\nif(dct_hash1 == dct_hash2):\n    print(\"\\nThe pictures are perceptually the same !\")\nelse:\n    print(f\"\\nThe pictures are different, distance: {dct_hash1 - dct_hash2}\")","fe6a66d8":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","7ab61b92":"text = ' '.join(descr)\nwordcloud = WordCloud(width=400, height=400, min_font_size=8, max_font_size=64, background_color='white').generate(text)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6270f419":"descr","8688fa6c":"sns.distplot(descr.map(lambda x: x.split(' ')).map(len), axlabel='#words in description')","8c26760b":"def run_training(base_dir):\n    data = pd.read_csv(base_dir)\n\n    present_imgs = os.listdir(f\"{base_dir}\/train_images\/\")\n    data = data[data['image'].isin(present_imgs)]\n    data['image'] = data['image'].apply(lambda x: f\"{base_dir}\/train_images\/\" + x)\n\n    encoder = LabelEncoder()\n    data['label_group'] = encoder.fit_transform(data['label_group'])\n    train_dataset = ShopeeDataset(data, transforms=get_train_transforms())\n\n    torch.cuda.empty_cache()\n    TRAIN_IDXS = int(0.9 * len(train_dataset))\n\n    indices = np.arange(len(train_dataset))\n    train_indices, test_indices = train_test_split(indices, train_size=TRAIN_IDXS)\n\n    train_subset = Subset(train_dataset, train_indices)\n    val_subset = Subset(train_dataset, test_indices)\n\n    train_dataloader = DataLoader(dataset=train_subset,\n                                  batch_size=CFG.batch_size,\n                                  num_workers=CFG.num_workers,\n                                  shuffle=True,\n                                  pin_memory=True,\n                                  drop_last=True)\n    val_dataloader = DataLoader(dataset=val_subset,\n                                batch_size=CFG.batch_size,\n                                shuffle=True,\n                                pin_memory=True,\n                                drop_last=True)\n\n    model = ShopeeCNNModel('efficientnet_b3')\n    model.to(CFG.device)\n\n    existing_layer = torch.nn.SiLU\n    new_layer = Mish()\n    model = replace_activations(model, existing_layer, new_layer)\n\n    lr_start = 1e-2\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr_start)\n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 3, eta_min=lr_start * 1e-4)\n\n    history = {'train': [], 'val': []}\n\n    for i in range(CFG.train_epochs):\n        epoch_loss_train = train_fn(model, train_dataloader, criterion, optimizer, scheduler, i)\n        epoch_loss_val = eval_fn(model, val_dataloader, i)\n        history['train'].append(epoch_loss_train)\n        history['val'].append(epoch_loss_val)\n        torch.save(model.state_dict(), 'arcface_512x512_efficientnet_b3.pt')\n\n    print(history)","9fb1bfc2":"TEST_PATH = f\"{BASE_DIR}\/test.csv\"","abce93c7":"def get_image_embeddings(base_dir, model_name, model_path):\n    embeds = []\n\n    model = ShopeeCNNModel(model_name=model_name)\n    model.to(CFG.device)\n    model.eval()\n\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(CFG.device)\n\n    test_data = pd.read_csv(f\"{base_dir}\/test.csv\")\n\n    test_data['image'] = test_data['image'].apply(lambda x: f\"{base_dir}\/test_images\/\" + x)\n\n    test_dataset = ShopeeDataset(test_data, transforms=get_test_transforms(), is_training=False)\n    image_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n\n    with torch.no_grad():\n        for img, label in tqdm(image_loader):\n            img = img.cuda()\n            label = label.cuda()\n            feat = model(img, label)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","e9fc1738":"def get_image_predictions(df, embeddings, threshold=0.0):\n    if len(df) > 3:\n        KNN = 50\n    else:\n        KNN = 3\n\n    model = NearestNeighbors(n_neighbors=KNN, metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]  # check if the distance is small enough for a 'neighbour'\n        ids = indices[k, idx]  # select indices in KNN dataframe that match the distance req for item k\n        posting_ids = df['posting_id'].iloc[ids].values  # obtain posting ids from indices, including identity\n        predictions.append(posting_ids)\n\n    del model, distances, indices\n    gc.collect()\n    return predictions","8cb42e25":"def get_text_predictions(df, df_cu, max_features = 20_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)\/\/CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds","be25cd44":"KNN_DISTANCE_THRESH = 0.21\nMAX_TEXT_TOKENS = 15_000","800d594d":"test_data = pd.read_csv(f\"{BASE_DIR}\/test.csv\")","e7cd2450":"df,df_cu,image_paths = read_dataset(BASE_DIR)\n\nimage_embeddings = get_image_embeddings(BASE_DIR, CFG.model_name2, CFG.model_path2)\nimage_predictions = get_image_predictions(df, image_embeddings, threshold = KNN_DISTANCE_THRESH)\ntext_predictions = get_text_predictions(df, df_cu, max_features = MAX_TEXT_TOKENS)","7dde6756":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x))","00d6f7bc":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis = 1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","310982b8":"pd.read_csv('submission.csv').head()","5ca71da0":"### What text appears to be most frequently used?","526ec3f7":"# Explore product descriptions","46c5dc27":"## Before we start, I'd like to thank these guys for their insightful and much inspiring work:\n[Chris Deotte](https:\/\/www.kaggle.com\/cdeotte) - on awesome introductions to ML with CUDA, found in the top of the Shopee competition notebooks\n\n[ragnar](https:\/\/www.kaggle.com\/ragnar123) - https:\/\/www.kaggle.com\/ragnar123\/shopee-efficientnetb3-arcmarginproduct, https:\/\/www.kaggle.com\/ragnar123\/shopee-inference-efficientnetb1-tfidfvectorizer\n\n[Mr_KnowNothing](https:\/\/www.kaggle.com\/tanulsingh077) - https:\/\/www.kaggle.com\/tanulsingh077\/pytorch-metric-learning-pipeline-only-images\n\n[Parth Dhameliya](https:\/\/www.kaggle.com\/parthdhameliya77) - https:\/\/www.kaggle.com\/parthdhameliya77\/pytorch-eca-nfnet-l0-image-tfidf-inference","c4ecebd0":"#### Since the hidden test set contains approx. 70k samples, it requires GPU optimized ML to keep iterations relatively quick","e8b9f615":"### This notebook will walk you through the competition. We will perform EDA, have a look at images and explore perceptual hashing, peek into textual data. \n### For modeling part we will create a basic training pipelines with PyTorch and RAPIDS and do inference with separate models for image and text data.","5f110b56":"# Explore the image data","719e6afb":"### The most frequent words can provide some noise to model and we should handle them accordingly","1c733c6d":"# Inference","d2a184b2":"## Text data","a179227e":"### There are quite a few samples where description is either too short or too long, giving the average length of ~8.\n### Smart way to concatenate image- and text-based predictions with these looks beneficial","ec0fbc86":"# Explore our target","0f1799a8":"### Let's examine perceptual hashing","541eedff":"### Both images and descriptions give clear scent of duplication\n### Images with same product contain the product with some distortions (rotation, brightness, gamma transforms), noisy objects. There are complete duplicates as well.\n### Descriptions contain name of the product with some extra text, which mildly contributes to higher algorithm performance","bc6f7856":"# Shopee - Price Match Guarantee competition.\n## We are given a set of products, each represented by the image and text description. The goal is to discover products that should have equal price.\n#### **NOTE**: the goal is quite different from looking for simply duplicated items, which is brilliantly explained by [Roman Glushko](https:\/\/www.kaggle.com\/glushko) in [this discussion](https:\/\/www.kaggle.com\/c\/shopee-product-matching\/discussion\/236496)","bb45acd5":"### Visualization of these duplicates can provide us with insights into features to account for","812812c3":"### But! DCT is tolerant to minor transformations that we have a lot in our data (acc. to http:\/\/www.hackerfactor.com\/blog\/?\/archives\/432-Looks-Like-It.html).\n### This implies using additional perceptual hashes and some voting rule?!","8ee25d86":"### Interesting! Identical images denote different label groups. \n### This suggests that description will play quite a role when deciding whether two products have the same price","caac9727":"### If the phash is identical - the images are complete copies. We have a plenty of such cases","2ca38fb3":"# Cooking the model","dcc4e251":"## Image data","6d0981ce":"#### Different hashing techniques use different image features. Maybe we can guess the hash that was used?","64dd3f6f":"### So, our perceptual hashing algorithm is DCT","88f07859":"### Image perceptual hash should be similar (not identical!) if the image is considered similar"}}