{"cell_type":{"35e0ed6b":"code","65367790":"code","6502e027":"code","f3676527":"code","31312464":"code","833a8d34":"code","692f22f4":"code","1530343a":"code","333164f9":"code","628d9b09":"code","fa214910":"code","6eee4e44":"code","d9285e75":"code","e2f2a628":"code","fb3851b5":"code","bf15986e":"code","a9a92703":"code","de4cd713":"code","1cf264d2":"code","d0928411":"code","d33ed218":"code","73de1904":"code","4625038d":"code","074ee206":"code","303ae1ab":"code","71f0497a":"code","b7182336":"code","16c4c307":"code","d103a2d8":"markdown","d3e8ccb3":"markdown","8fbf58d6":"markdown","7a37f769":"markdown","5b738a83":"markdown","50ad729a":"markdown","495375ec":"markdown","85009bcb":"markdown","37f577d5":"markdown","0bcd44bd":"markdown","1a474bbf":"markdown","7d9f7980":"markdown","0da1e73d":"markdown","69bbf75c":"markdown","9352a386":"markdown","8d0e05f5":"markdown","5872bf41":"markdown"},"source":{"35e0ed6b":"import os\nimport gc\nimport abc\nimport torch\nimport random\nimport shutil\nimport numpy as np\nimport pandas as pd\n\nfrom torch import nn\nfrom pathlib import Path\nfrom transformers.file_utils import ModelOutput\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import get_cosine_schedule_with_warmup, AdamW, AutoModel, AutoTokenizer","65367790":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything()","6502e027":"class TrainingDataset(Dataset):\n    def __init__(self, text_excerpts, targets):\n        self.text_excerpts = text_excerpts\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx],\n                  'target': self.targets[idx]}\n        return sample","f3676527":"def transform_targets(targets):\n    targets = targets.astype(np.float32).reshape(-1, 1)\n    return targets","31312464":"def create_training_dataloader(data, batch_size, shuffle, num_workers=4):\n    text_excerpts = data['excerpt'].tolist()\n    targets = transform_targets(data['target'].to_numpy())\n    dataset = TrainingDataset(text_excerpts=text_excerpts, targets=targets)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False)\n    return dataloader","833a8d34":"def split_into_kfolds(data, n_splits, shuffle, random_state):\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    data['bins'] = pd.cut(data['target'], bins=num_bins, labels=False)\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n    for train_indices, valid_indices in kf.split(X=data, y=data['bins'].tolist()):\n        yield data.iloc[train_indices], data.iloc[valid_indices]","692f22f4":"class Metric:\n    def __init__(self):\n        self.sse = 0\n        self.num_samples = 0\n    \n    def update(self, targets, predictions):\n        self.sse += np.sum(np.square(targets - predictions))\n        self.num_samples += len(targets)\n        \n    def get_rmse(self):\n        rmse = np.sqrt(self.sse \/ self.num_samples)\n        return rmse","1530343a":"class Monitor:\n    def __init__(self, num_patient_epochs):\n        self.num_patient_epochs = num_patient_epochs\n        self.best_epoch_num = None\n        self.best_score = np.inf\n        self.best_model = None\n        \n    def early_stopping(self, current_epoch_num):\n        return True if current_epoch_num > self.best_epoch_num + self.num_patient_epochs else False\n    \n    def update_best_model(self, current_epoch_num, current_score, current_model, current_tokenizer, save_path):\n        if current_score < self.best_score:\n            self.best_epoch_num = current_epoch_num\n            self.best_score = current_score\n#             self.best_model = current_model\n            # Save model and tokenizer\n            shutil.rmtree(save_path, ignore_errors=True)\n            os.makedirs(save_path)\n            torch.save(current_model.state_dict(), save_path \/ 'model.pth')\n            current_tokenizer.save_pretrained(save_path)","333164f9":"class KfoldMonitor:\n    def __init__(self):\n        self.fold_monitor = {}\n        \n    def update(self, fold, monitor):\n        self.fold_monitor[fold] = monitor\n        \n    def get_mean_fold_score(self):\n        mean_cross_validation_score = np.mean([fold_monitor.best_score for fold_monitor in self.fold_monitor.values()])\n        return mean_cross_validation_score\n        ","628d9b09":"def clear_cuda():\n    gc.collect()\n    torch.cuda.empty_cache()","fa214910":"def train(dataloader, model, tokenizer, padding, max_length, optimizer, device, scheduler=None):\n    clear_cuda()\n    model.train()\n    model.to(device)\n    epoch_loss = 0\n    for batch_num, batch in enumerate(dataloader):\n        # Forward Propagation\n        inputs = tokenizer(batch['text_excerpt'], padding=padding, truncation=True, max_length=max_length, return_tensors=\"pt\")\n        inputs = {key:value.to(device) for key, value in inputs.items()}\n        targets = batch['target'].to(device)\n        outputs = model(**inputs, labels=targets)\n        epoch_loss += outputs.loss.item()\n        # Backpropagation\n        outputs.loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        optimizer.zero_grad()\n    average_epoch_loss = epoch_loss\/len(dataloader)\n    return model, average_epoch_loss","6eee4e44":"def evaluate(dataloader, model, tokenizer, padding, max_length, device):\n    clear_cuda()\n    model.eval()\n    model.to(device)\n    epoch_loss = 0\n    metric = Metric()\n    for batch_num, batch in enumerate(dataloader):\n        # Forward Propagation\n        inputs = tokenizer(batch['text_excerpt'], padding=padding, truncation=True, max_length=max_length,return_tensors=\"pt\")\n        inputs = {key:value.to(device) for key, value in inputs.items()}\n        targets = batch['target'].to(device)\n        with torch.no_grad():\n            outputs = model(**inputs, labels=targets)\n        epoch_loss += outputs.loss.item()\n        targets = targets.detach().cpu().numpy()\n        predictions = outputs.logits.detach().cpu().numpy()\n        metric.update(targets=targets, predictions=predictions)\n    average_epoch_loss = epoch_loss\/len(dataloader)\n    return average_epoch_loss, metric","d9285e75":"def train_and_evaluate(fold, epoch_num, train_dataloader, valid_dataloader, model, tokenizer, padding, max_length, optimizer, device, scheduler, monitor, save_path):\n    clear_cuda()\n    model.to(device)\n    for batch_num, batch in enumerate(train_dataloader):\n        model.train()\n        # Forward Propagation\n        inputs = tokenizer(batch['text_excerpt'], padding=padding, truncation=True, max_length=max_length, return_tensors=\"pt\")\n        inputs = {key:value.to(device) for key, value in inputs.items()}\n        targets = batch['target'].to(device)\n        outputs = model(**inputs, labels=targets)\n        # Backpropagation\n        outputs.loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        optimizer.zero_grad()\n        # Evaluate\n        if batch_num  % 10 == 0:\n            model.eval()\n            metric = Metric()\n            for _, batch in enumerate(valid_dataloader):\n                # Forward Propagation\n                inputs = tokenizer(batch['text_excerpt'], padding=padding, truncation=True, max_length=max_length,return_tensors=\"pt\")\n                inputs = {key:value.to(device) for key, value in inputs.items()}\n                targets = batch['target'].to(device)\n                with torch.no_grad():\n                    outputs = model(**inputs, labels=targets)\n                targets = targets.detach().cpu().numpy()\n                predictions = outputs.logits.detach().cpu().numpy()\n                metric.update(targets=targets, predictions=predictions)\n            print(f'Fold: {fold}, epoch: {epoch_num}, Iteration num: {batch_num}, RMSE: {metric.get_rmse()}')\n            monitor.update_best_model(current_epoch_num=epoch_num, current_score=metric.get_rmse(),\n                                      current_model=model, current_tokenizer=tokenizer, save_path=save_path)\n    return monitor","e2f2a628":"class RegressorOutput(ModelOutput):\n    loss = None\n    logits = None\n    hidden_states = None\n    attentions = None","fb3851b5":"class RobertaPoolerRegressor(nn.Module):\n    def __init__(self, model_path, apply_sqrt_to_loss):\n        super(RobertaPoolerRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        self.loss_fn = nn.MSELoss()\n        self.apply_sqrt_to_loss = apply_sqrt_to_loss\n    \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        roberta_outputs = self.roberta(input_ids=input_ids, \n                                       attention_mask=attention_mask)\n        pooler_output = roberta_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        if self.apply_sqrt_to_loss:\n            loss = torch.sqrt(self.loss_fn(labels, logits)) if labels is not None else None\n        else:\n            loss = self.loss_fn(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","bf15986e":"class AttentionHead(nn.Module):\n    def __init__(self, hidden_dim):\n        super(AttentionHead, self).__init__()\n        self.W = nn.Linear(hidden_dim, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, x):\n        attention_scores = self.V(torch.tanh(self.W(x)))\n        attention_scores = torch.softmax(attention_scores, dim=1)\n        attentive_x = attention_scores * x\n        attentive_x = attentive_x.sum(axis=1)\n        return attentive_x","a9a92703":"class RobertaLastHiddenStateRegressor(nn.Module):\n    def __init__(self, model_path):\n        super(RobertaLastHiddenStateRegressor, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.roberta.config.hidden_size)\n        self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n        self.loss_fn = nn.MSELoss()\n    \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        roberta_outputs = self.roberta(input_ids=input_ids,\n                                       attention_mask=attention_mask)\n        last_hidden_state = roberta_outputs['last_hidden_state']\n        attentive_vector = self.head(last_hidden_state)\n        attentive_vector = self.dropout(attentive_vector)\n        logits = self.regressor(attentive_vector)\n        loss = torch.sqrt(self.loss_fn(labels, logits)) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","de4cd713":"class Experiment(abc.ABC):\n    def __init__(self, save_name, run_on_sample, n_splits=5, random_state=42, batch_size=8, num_epochs=3, num_patient_epochs=1):\n        # Everything common across experiments is declared here\n        self.full_train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n        self.random_state = random_state\n        self.run_on_sample = run_on_sample\n        if run_on_sample:\n            self.full_train_data = self.full_train_data.sample(frac=0.25, random_state=self.random_state)\n        self.save_name = save_name\n        self.n_splits = n_splits\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n        self.num_patient_epochs = num_patient_epochs\n        self.kfold_monitor = KfoldMonitor()\n        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    \n    @abc.abstractmethod\n    def get_experiment_params(self):\n        raise NotImplementedError\n            \n    def run(self):\n        clear_cuda()\n        for fold, (train_data, valid_data) in enumerate(split_into_kfolds(data=self.full_train_data, n_splits=self.n_splits,\n                                                                              shuffle=True, random_state=self.random_state)):\n            clear_cuda()\n            train_dataloader = create_training_dataloader(data=train_data, batch_size=self.batch_size, shuffle=True)\n            valid_dataloader = create_training_dataloader(data=valid_data, batch_size=self.batch_size, shuffle=False)\n            model, tokenizer, padding, max_length, optimizer, scheduler = self.get_experiment_params()\n            monitor = Monitor(num_patient_epochs=self.num_patient_epochs)\n            save_path = Path(f'{self.save_name}\/fold_{fold}')\n            for epoch_num in range(self.num_epochs):\n                monitor = train_and_evaluate(fold=fold, epoch_num=epoch_num, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,\n                                             model=model, tokenizer=tokenizer, padding=padding, max_length=max_length,\n                                             optimizer=optimizer, device=self.device, scheduler=scheduler, monitor=monitor, save_path=save_path)\n            del model\n            self.kfold_monitor.update(fold=fold, monitor=monitor)\n            print('----------------------------------------------------')\n            \n        mean_cross_validation_score = np.mean([fold_monitor.best_score for fold_monitor in self.kfold_monitor.fold_monitor.values()])\n        print(f'Mean cross validation score: {mean_cross_validation_score}')       ","1cf264d2":"class Experiment1(Experiment):\n    def __init__(self):\n        super(Experiment1, self).__init__(save_name='experiment_1', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/maunish-clrp-model\/clrp_roberta_base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaLastHiddenStateRegressor(model_path=model_path)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = 'max_length'\n        max_length = 256\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=10 * ((len(experiment_1.full_train_data) \/\/ experiment_1.batch_size) +1 ))\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \nexperiment_1 = Experiment1()\nexperiment_1.run()","d0928411":"class Experiment2(Experiment):\n    def __init__(self):\n        super(Experiment2, self).__init__(save_name='experiment_2', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/commonlit-data-download\/roberta-base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaLastHiddenStateRegressor(model_path=model_path)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = 'max_length'\n        max_length = 256\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=10 * ((len(experiment_1.full_train_data) \/\/ experiment_1.batch_size) +1 ))\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \n    \nexperiment_2 = Experiment2()\nexperiment_2.run()","d33ed218":"class Experiment3(Experiment):\n    def __init__(self):\n        super(Experiment3, self).__init__(save_name='experiment_3', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/maunish-clrp-model\/clrp_roberta_base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaPoolerRegressor(model_path=model_path, apply_sqrt_to_loss=False)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = 'max_length'\n        max_length = 256\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=10 * ((len(experiment_1.full_train_data) \/\/ experiment_1.batch_size) +1 ))\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \n    \nexperiment_3 = Experiment3()\nexperiment_3.run()","73de1904":"class Experiment4(Experiment):\n    def __init__(self):\n        super(Experiment4, self).__init__(save_name='experiment_4', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/maunish-clrp-model\/clrp_roberta_base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaLastHiddenStateRegressor(model_path=model_path)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = True\n        max_length = None\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=10 * ((len(experiment_1.full_train_data) \/\/ experiment_1.batch_size) +1 ))\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \n    \nexperiment_4 = Experiment4()\nexperiment_4.run()","4625038d":"class Experiment5(Experiment):\n    def __init__(self):\n        super(Experiment5, self).__init__(save_name='experiment_5', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/maunish-clrp-model\/clrp_roberta_base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaLastHiddenStateRegressor(model_path=model_path)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = 'max_length'\n        max_length = 256\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = None\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \nexperiment_5 = Experiment5()\nexperiment_5.run()","074ee206":"class Experiment6(Experiment):\n    def __init__(self):\n        super(Experiment6, self).__init__(save_name='experiment_6', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/maunish-clrp-model\/clrp_roberta_base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaPoolerRegressor(model_path=model_path, apply_sqrt_to_loss=True)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = 'max_length'\n        max_length = 256\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=10 * ((len(experiment_1.full_train_data) \/\/ experiment_1.batch_size) +1 ))\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \nexperiment_6 = Experiment6()\nexperiment_6.run()","303ae1ab":"class Experiment7(Experiment):\n    def __init__(self):\n        super(Experiment7, self).__init__(save_name='experiment_7', run_on_sample=True)\n\n    def get_experiment_params(self):        \n        model_path = '..\/input\/commonlit-data-download\/roberta-base'\n        tokenizer_path = '..\/input\/commonlit-data-download\/roberta-base'\n        model = RobertaPoolerRegressor(model_path=model_path, apply_sqrt_to_loss=False)\n        model.to(self.device)\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        padding = True\n        max_length = None\n        optimizer = AdamW(params=model.parameters(), lr=2e-5, weight_decay=0.01)\n        scheduler = None\n        return model, tokenizer, padding, max_length, optimizer, scheduler\n    \nexperiment_7 = Experiment7()\nexperiment_7.run()","71f0497a":"pd.options.display.max_colwidth = None","b7182336":"results = pd.DataFrame([{'experiment': 'experiment_1',\n                         'description': ['Pretrained on Goodreads', 'Attention on Last hidden states', 'Tokenizer max_len: 256', 'Cosine scheduler with warmup'],\n                         'sample': experiment_1.run_on_sample,\n                         'score': experiment_1.kfold_monitor.get_mean_fold_score()},\n                        \n                        {'experiment': 'experiment_2',\n                         'description': ['Base model from Huggingface', 'Attention on Last hidden states', 'Tokenizer max_len: 256', 'Cosine scheduler with warmup'],\n                         'sample': experiment_2.run_on_sample,\n                         'score': experiment_2.kfold_monitor.get_mean_fold_score()},\n                        \n                        {'experiment': 'experiment_3',\n                         'description': ['Pretrained on Goodreads', 'Pooler output with MSE loss', 'Tokenizer max_len: 256', 'Cosine scheduler with warmup'],\n                         'sample': experiment_3.run_on_sample,\n                         'score': experiment_3.kfold_monitor.get_mean_fold_score()},\n                        \n                        {'experiment': 'experiment_4',\n                         'description': ['Pretrained on Goodreads', 'Attention on Last hidden states', 'Tokenizer max_len: None', 'Cosine scheduler with warmup'],\n                         'sample': experiment_4.run_on_sample,\n                         'score': experiment_4.kfold_monitor.get_mean_fold_score()},\n                        \n                        {'experiment': 'experiment_5',\n                         'description': ['Pretrained on Goodreads', 'Attention on Last hidden states', 'Tokenizer max_len: 256', 'No scheduler'],\n                         'sample': experiment_5.run_on_sample,\n                         'score': experiment_5.kfold_monitor.get_mean_fold_score()},\n                        \n                        {'experiment': 'experiment_6',\n                         'description': ['Pretrained on Goodreads', 'Pooler output with RMSE loss', 'Tokenizer max_len: 256', 'No scheduler'],\n                         'sample': experiment_6.run_on_sample,\n                         'score': experiment_6.kfold_monitor.get_mean_fold_score()},\n                        \n                        {'experiment': 'experiment_7',\n                         'description': ['Base model from Huggingface', 'Pooler output with MSE loss', 'Tokenizer max_len: None', 'No scheduler'],\n                         'sample': experiment_7.run_on_sample,\n                         'score': experiment_7.kfold_monitor.get_mean_fold_score()}])","16c4c307":"results","d103a2d8":"## Vignesh Model","d3e8ccb3":"# Different parts","8fbf58d6":"# Context:\nI trained a simple regressor based on Roberta base and achieved a LB score of 0.526. I was not extremely happy but it was better than my previous models so I was OK. But later on I came across Maunish's notebook in which he does almost the same thing but with some addtional small components like adding scheduler and so on and his model scored 0.479 which is a very significant difference. Congratulations to Maunish on it. I wanted to identify which components from Maunish contributed towards improved performance of his algorithm. Therefore in this notebook I would like to start with Maunish's model and progressively remove components to match that of mine and identify which additional components from him contribute to imporved performance.\n\nMaunish's finetuner notebook : https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune  \nMaunish's inference notebook: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-inference  \nMy finetuner notebook: https:\/\/www.kaggle.com\/vigneshbaskaran\/commonlit-easy-transformer-finetuner  \nMy inference notebook: https:\/\/www.kaggle.com\/vigneshbaskaran\/commonlit-easy-finetuner-inference  \n\nThe important components in an algorithm:\n\n\n|      Component     |                      Maunish                      |            Vignesh            |   |\n|:------------------:|:-------------------------------------------------:|:-----------------------------:|:-:|\n|  Pretrained model  | Pretrained with additional data on top of ROBERTA |  Simple ROBERTA base from HF  |   |\n|      Optimizer     |              AdamW with weight decay              |   AdamW with no weight decay  |   |\n|      Scheduler     |                  Cosine Annealing                 |          No scheduler         |   |\n| Model architecutre |       Last hidden state with attention head       | Pooler with no attention head |   |\n|        Loss        |                      SQRT MSE                     |              MSE              |   |\n|      Tokenizer     |                    max_len: 256                   |        max_len: default       |   |\n|  train_valid_split |              Stratified KFold on bins             |          Simple KFold         |   |\n|                    |                                                   |                               |   |\n\n\n# Plan\n1. Start by recreating Maunish's work - Make sure I can get his score of ~ 0.479\n2. Sequentially replace Maunish's components with my components and track how the validation score changes\n3. Finally make sure when every component of Maunish's work is replaced with my component I get my old score of ~ 0.526\n4. Summarize the findings","7a37f769":"# Summarizing results","5b738a83":"# Experiment 6\nMaunish's Original Algorithm. **Regressor modified with MSE loss**. Details:\n1. Pretrained model: Additional pretraining done on Goodreads data\n2. Regressor: On top of **Pooler output** with **RMSE** loss (Same as Experiment 3 but instead of MSE loss using RMSE loss)\n3. Tokenizer: max_length: 256\n4. Scheduler: cosine schedule with warmup","50ad729a":"# Experiment 5\nMaunish's Original Algorithm. **Scheduler removed**. Details:\n1. Pretrained model: Additional pretraining done on Goodreads data\n2. Regressor: On top of Last hidden state - vectorized by attention\n3. Tokenizer: max_length: 256\n4. Scheduler: **None**","495375ec":"# Common parts\n## Dataset and Dataloader","85009bcb":"## Maunish Model","37f577d5":"# Experiment 1\nMaunish's Original Algorithm. **No modification**. Details:\n1. Pretrained model: Additional pretraining done on Goodreads data\n2. Regressor: On top of Last hidden state - vectorized by attention\n3. Tokenizer: max_length: 256\n4. Scheduler: cosine schedule with warmup","0bcd44bd":"# Experiment 7\nMy original algorithm **No modification**. Details:\n1. Pretrained model: No additional pretraining. Simply downloaded from HF\n2. Regressor: On top of **Pooler output** with **MSE** loss\n3. Tokenizer: max_length: None\n4. Scheduler: None","1a474bbf":"# Experiment 4\nMaunish's Original Algorithm. **Max length of tokenizer modified**. Details:\n1. Pretrained model: Additional pretraining done on Goodreads data\n2. Regressor: On top of Last hidden state - vectorized by attention\n3. Tokenizer: max_length: **None**\n4. Scheduler: cosine schedule with warmup","7d9f7980":"# Recreating Maunish's model\n1. Configuration: `{'lr': 2e-5, 'wd': 0.01, 'batch_size': 16, 'validated_every_n_iteration': 10, 'max_len': 256, 'epochs': 3, 'nfolds': 5, 'seed': 42}`\n2. Seeds everything\n3. Bins the data\n4. KFold split based on bins\n5. Define Dataset\n6. Define model: Roberta + Attention head on last hidden state\n7. Optimizer: `optim.AdamW(model.parameters(),lr=2e-5,weight_decay=0.01)`\n8. LR scheduler: `lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))`","0da1e73d":"# Experiment 2\nMaunish's Original Algorithm. Modified the pretrained model. Details:\n1. Pretrained model: **Base model from Huggingface**\n2. Regressor: On top of Last hidden state - vectorized by attention\n3. Tokenizer: max_length: 256\n4. Scheduler: cosine schedule with warmup","69bbf75c":"# Experiment 3\nMaunish's Original Algorithm. **Regressor modified with MSE loss**. Details:\n1. Pretrained model: Additional pretraining done on Goodreads data\n2. Regressor: On top of **Pooler output** with **MSE** loss  \n3. Tokenizer: max_length: 256\n4. Scheduler: cosine schedule with warmup","9352a386":"## Metric, EarlyStopping, Saver, Monitor","8d0e05f5":"## Training and Validation loop","5872bf41":"# Training"}}