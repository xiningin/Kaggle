{"cell_type":{"e562a7e0":"code","2412ffb2":"code","9b698dbe":"code","c60a0c31":"code","4daf63a0":"code","0f75abb2":"code","c33d3ebc":"code","330869e6":"code","25a18687":"code","b024c130":"code","54c50ccc":"code","8079d6c2":"code","f08fbce1":"code","7064d289":"code","478094d6":"code","6a0126b4":"code","688a417e":"code","81939060":"code","ed74f889":"code","20b32bdf":"code","07013557":"code","fd98c6f9":"code","ee65cd35":"code","83c9eff2":"code","3c587fd1":"code","b7a24b21":"code","d26ab247":"code","afcc2e1d":"code","4c945272":"code","e63facfb":"code","c2b06959":"code","8ba9d90d":"code","3c180f3a":"code","ad3534f4":"code","1415890a":"code","23fe58f6":"code","9a1ccf81":"code","f00fdf82":"markdown","2a5ada83":"markdown","6e6df2d6":"markdown","ac15b613":"markdown","e5e1eae7":"markdown","e369ff27":"markdown","7868f3a7":"markdown","0e527b7e":"markdown","0b89df39":"markdown","15736a67":"markdown","770b94b4":"markdown","d10f5818":"markdown","0937fc7d":"markdown","a6b20033":"markdown","c2c283fa":"markdown","383fc5a2":"markdown","35de261b":"markdown","80e1f041":"markdown","756d8a44":"markdown","e4960d08":"markdown","767f31a2":"markdown","4de6541f":"markdown","6ad9232d":"markdown","700a20ce":"markdown","f8cb44b6":"markdown","304d2549":"markdown","74ac26b5":"markdown"},"source":{"e562a7e0":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2412ffb2":"data_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata_train.head()","9b698dbe":"data_train.describe(include='all')","c60a0c31":"full_data = data_train.append(data_test)\nfull_data.count()","4daf63a0":"full_data.isnull().sum()","0f75abb2":"full_data['Embarked'].describe()","c33d3ebc":"full_data_preprocessed = full_data.copy()\nfull_data_preprocessed['Embarked'] = full_data['Embarked'].fillna('S')\nfull_data_preprocessed['Embarked'].isnull().sum()","330869e6":"full_data['Fare'].median()","25a18687":"full_data_preprocessed['Fare'] = full_data['Fare'].fillna(full_data['Fare'].median())\nfull_data_preprocessed['Fare'].isnull().sum()","b024c130":"women_count = data_train.loc[data_train.Sex == 'female']['Survived'].count()\nwomen_survived = data_train.loc[data_train.Sex == 'female'][\"Survived\"].sum()\nwomen_survived_perc = data_train.loc[data_train.Sex == 'female'][\"Survived\"].sum() \/ data_train.loc[data_train.Sex == 'female'][\"Survived\"].count()\nwomen_count, women_survived, women_survived_perc","54c50ccc":"man_count = data_train.loc[data_train.Sex == 'male']['Survived'].count()\nman_survived = data_train.loc[data_train.Sex == 'male']['Survived'].sum()\nman_survived_perc = data_train.loc[data_train.Sex == 'male']['Survived'].sum() \/ data_train.loc[data_train.Sex == 'male']['Survived'].count()\nman_count, man_survived, man_survived_perc","8079d6c2":"# Creating new DataFrame to store preprocessed values\ndata_preprocessed = data_train[['Survived']]\n\n#Mapping Sex\ndata_preprocessed['IsFemale'] = data_train['Sex'].map({'male':0, 'female':1})\ndata_preprocessed","f08fbce1":"Pclass_count = data_train['Pclass'].count()\nPclass_1_count = data_train.loc[data_train['Pclass'] == 1]['Pclass'].count()\nPclass_2_count = data_train.loc[data_train['Pclass'] == 2]['Pclass'].count()\nPclass_3_count = data_train.loc[data_train['Pclass'] == 3]['Pclass'].count()\nPclass_1_perc = Pclass_1_count \/ Pclass_count\nPclass_2_perc = Pclass_2_count \/ Pclass_count\nPclass_3_perc = Pclass_3_count \/ Pclass_count\n\nPclass_1_perc, Pclass_2_perc, Pclass_3_perc","7064d289":"Pclass_1_survived_perc = data_train.loc[data_train['Pclass'] == 1]['Survived'].sum() \/ Pclass_1_count \nPclass_2_survived_perc = data_train.loc[data_train['Pclass'] == 2]['Survived'].sum() \/ Pclass_2_count\nPclass_3_survived_perc = data_train.loc[data_train['Pclass'] == 3]['Survived'].sum() \/ Pclass_3_count\nPclass_1_survived_perc, Pclass_2_survived_perc, Pclass_3_survived_perc","478094d6":"data_preprocessed['Pclass'] = data_train['Pclass']\ndata_preprocessed","6a0126b4":"data_train['Survived'].loc[data_train['Embarked']== 'S'].sum() \/ data_train['Survived'].loc[data_train['Embarked']== 'S'].count()","688a417e":"data_train['Survived'].loc[data_train['Embarked']== 'C'].sum() \/ data_train['Survived'].loc[data_train['Embarked']== 'C'].count()","81939060":"data_train['Survived'].loc[data_train['Embarked']== 'Q'].sum() \/ data_train['Survived'].loc[data_train['Embarked']== 'Q'].count()","ed74f889":"data_preprocessed['Embarked'] = data_train['Embarked']\ndata_preprocessed","20b32bdf":"# Group SibSp and Parch\ndata_train_1 = data_train.copy()\ndata_train_1['Family_nr'] = data_train_1['SibSp'] + data_train_1['Parch']\ndata_train_1['IsAlone'] = 0\ndata_train_1.loc[data_train_1['Family_nr'] == 0, 'IsAlone'] = 1  \ndata_train_1[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False)","07013557":"data_preprocessed['IsAlone'] = data_train_1['IsAlone']\ndata_preprocessed","fd98c6f9":"### NOT WORKING BEFORE FIXING NULL VALUES ###\n\n#data_train = data_train.copy()\n#data_train_1['AgeGroup'][data_train_1['Age']<=9] = '0-9'\n#data_train_1['AgeGroup'][data_train_1['Age']>=10 & data_train_1['Age']<=15] = '10-15'\n#data_train_1['AgeGroup'][data_train_1['Age']>=16 & data_train_1['Age']<=54] = '16-54'\n#data_train_1['AgeGroup'][data_train_1['Age']>=55] = '55+'\n#data_train_1['AgeGroup'][data_train_1['Age']>=14 & data_train_1['Age']<=15] = '0-14'\n#data_train_1","ee65cd35":"sns.distplot(data_train['Fare'])","83c9eff2":"data_train['TicketFreq'] = data_train.groupby('Ticket')['Ticket'].transform('count')\ndata_train['PassengerFare'] = data_train['Fare'] \/ data_train['TicketFreq']\n\ndata_train.head()","3c587fd1":"data_train[data_train['Ticket'] == '373450']['Ticket'].count()","b7a24b21":"data_train['FareGroup'][data_train['Fare']<=50] = '0-50'\ndata_train['FareGroup'][data_train['Fare']>50 & data_train['Fare']<=100] = '50-100'\ndata_train['FareGroup'][data_train['Fare']>100] = '100+'\ndata_train.loc[data_train['FareGroup'] == '0-50']['Survived'].sum() \/ data_train.loc[data_train['FareGroup'] == '0-50'].count()","d26ab247":"data_preprocessed['Fare'] = data_train.loc[:,'Fare']","afcc2e1d":"# Declaring inputs and target\nx = data_preprocessed.iloc[:,1:]\ny = data_preprocessed['Survived']\nx.shape, y.shape","4c945272":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = x\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['Features'] = variables.columns\nvif\n\n### VIF = 1 -> PERFECT\n### 1 < VIF < 10 -> OK\n### VIF > 10 -> TO REMOVE","e63facfb":"# Encoding\ndata_to_encode = data_preprocessed.iloc[:,1:-1]\ndummies = pd.get_dummies(data_to_encode, drop_first=True)\ndummies","c2b06959":"# Normalizing\n\n# Seleziono colonne\n#dati_to_normalize = dati_with_dummies.drop(['Avg_ODP_MIN'], axis = 1)\n\n# Salvo nomi colonne\n#cols= dati_to_normalize.columns.values \n\n# MinMax scaler\n# Uso MixMax scaler cos\u00ec \u00e8 pi\u00f9 facile interpretare i risultati dopo\n#scaler = MinMaxScaler()\n#scaler.fit(dati_to_normalize)\n#dati_norm = scaler.transform(dati_to_normalize)\n#dati_norm = pd.DataFrame(dati_norm, columns = cols)\n#dati_norm","8ba9d90d":"#Feature selection\nfrom sklearn.feature_selection import f_regression\np_values = pd.DataFrame(dummies, columns=['Features'])\np_values['p_value'] = f_regression(x,y)[1]\np_values['p_value'] = p_values['p_value'].round(3)\np_values.sort_values(by=['p_value'])\n\n### If p_value > 0.05 the feature is not statistically relevant and should be removed ###","3c180f3a":"# Train Test split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","ad3534f4":"Logit = LogisticRegression(random_state=0).fit(x_train, y_train)\ny_hat = Logit.predict(x_train)\n#y_hat","1415890a":"def confusion_matrix(data,actual_values,model):\n        \n        # Confusion matrix \n        \n        # Parameters\n        # ----------\n        # data: data frame or array\n            # data is a data frame formatted in the same way as your input data (without the actual values)\n            # e.g. const, var1, var2, etc. Order is very important!\n        # actual_values: data frame or array\n            # These are the actual values from the test_data\n            # In the case of a logistic regression, it should be a single column with 0s and 1s\n            \n        # model: a LogitResults object\n            # this is the variable where you have the fitted model \n            # e.g. results_log in this course\n        # ----------\n        \n        #Predict the values using the Logit model\n        pred_values = model.predict(data)\n        # Specify the bins \n        bins=np.array([0,0.5,1])\n        # Create a histogram, where if values are between 0 and 0.5 tell will be considered 0\n        # if they are between 0.5 and 1, they will be considered 1\n        cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n        # Calculate the accuracy\n        accuracy = (cm[0,0]+cm[1,1])\/cm.sum()\n        # Return the confusion matrix and the accuracy\n        return cm, accuracy","23fe58f6":"# Confusion matrix with train data\ncm = confusion_matrix(x_train,y_train,Logit)\ncm","9a1ccf81":"# Confusion matrix with test data\ncm = confusion_matrix(x_test,y_test,Logit)\ncm","f00fdf82":"## 3.0.1 Filling Age <a class=\"anchor\" id=\"filling-age\"><\/a>","2a5ada83":"Female had a greater chance to survive. The feature is useful and we should keep it. We should map it for future modeling","6e6df2d6":"# 3.2 Pclass <a class=\"anchor\" id=\"pclass\"><\/a>\nDoes Pclass influence the chance of survival?","ac15b613":"# 3.1 Sex <a class=\"anchor\" id=\"sex\"><\/a>\nLet's find if male of female had better chance of survival","e5e1eae7":"# 1. Introduction <a class=\"anchor\" id=\"introduction\"><\/a>","e369ff27":"# 3.5 Age <a class=\"anchor\" id=\"age\"><\/a>\nLet's try to group Age","7868f3a7":"# 3.6 Fare <a class=\"anchor\" id=\"fare\"><\/a>","0e527b7e":"Class 1 had better chance then 2 and 3, so we should keep the feature **Pclass**.\nWe should keep the actual values (1,2,3) aswell to preserve the order.","0b89df39":"Passangers with family members were most likely to survive -> include \"IsAlone\"","15736a67":"## 3.0 Missing values <a class=\"anchor\" id=\"missing-values\"><\/a>\nLet's see if we have missing values","770b94b4":"Let's now analyze more in depth each feature","d10f5818":"The valuable features have now no missing values!","0937fc7d":"# 3.3 Embarked <a class=\"anchor\" id=\"embarked\"><\/a>","a6b20033":"# 4 Modeling <a class=\"anchor\" id=\"modeling\"><\/a>","c2c283fa":"# 4.1 Logistic Regression <a class=\"anchor\" id=\"logistic-regression\"><\/a>","383fc5a2":"# 3.4 SibSp and Parch <a class=\"anchor\" id=\"sibsp-parch\"><\/a>\nHow **SibSp** and **Parch** influence the chance of survival?","35de261b":"Passangers in different **Pclass** had different chance of survival, so we should keep this feature. Later we will need to get dummies from this values","80e1f041":"## 3.0.2 Filling Embarked <a class=\"anchor\" id=\"filling-embarked\"><\/a>","756d8a44":"This is my first competition on Kaggle after and online course on data science. Looking forward to practice what i've learned","e4960d08":"### Checking multicollinearity","767f31a2":"# 2. Loading Data <a class=\"anchor\" id=\"loading\"><\/a>","4de6541f":"* **Age** has many missing values but could be a useful feature. We should try to predict the age of missing values based on other features\n* **Cabin** has too many missing values and not much value, so it should be removed.\n* **Embarked** has only 2 missing values that can be easily filled with the mean or median\n* **Fare**  has only 1 missing value that can be easily filled with the mean or median","6ad9232d":"Median is 'S', so let's fill empty data with it","700a20ce":"## 3.0.3 Filling Fare <a class=\"anchor\" id=\"filling-fare\"><\/a>","f8cb44b6":"# Table of contents \n* [1. Introduction](#introduction)\n* [2. Loading Data](#loading)\n* [3. Data Preprocessing](#data-preprocessing)\n * [3.0 Missing Values](#missing-values)\n   * [3.0.1 Filling Age](#filling-age)\n   * [3.0.2 Filling Embarked](#filling-embarked)\n   * [3.0.3 Filling Fare](#filling-fare)\n * [3.1 Sex](#sex)\n * [3.2 Pclass](#pclass)\n * [3.3 Embarked](#embarked)\n * [3.4 SibSp and Parch](#sibsp-parch)\n * [3.5 Age](#age)\n * [3.6 Fare](#fare)\n* [4 Modeling](#modeling)\n * [4.1 Logistic Regression](#logistic-regression)","304d2549":"# 3. Data Preprocessing <a class=\"anchor\" id=\"data-preprocessing\"><\/a>\nIn this section we will prepare the data for the modeling phase","74ac26b5":"Let's fill with the median"}}