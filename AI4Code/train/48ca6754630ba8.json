{"cell_type":{"83e81d28":"code","6e6224d9":"code","06abe152":"code","faf058ff":"code","2dc16c4c":"code","6fd81aa0":"code","72dd0935":"code","c5a35e6a":"code","a04a55c4":"code","bced4bd1":"code","54120334":"code","886187e9":"code","4bb289ac":"code","130e1406":"code","e0908b1f":"code","2bdf14a2":"code","5e85328e":"code","4b84c746":"code","dcc23edf":"code","832d778a":"code","5ea1cfc5":"code","5cba1e8c":"code","1f34b199":"code","b07fe024":"code","53b8f6d0":"code","1198b2d4":"code","9eebeff8":"code","b316d65a":"code","98b6c3ff":"code","5274cd74":"code","e711bf90":"code","2269d837":"code","58ff58ff":"code","56120b89":"code","c86d5954":"code","29df172c":"code","9ba03c42":"code","82fbb660":"code","82d1ee9e":"markdown","2dfb09a5":"markdown","ab0dc354":"markdown","068fa7f4":"markdown","4b27beaa":"markdown","3c958887":"markdown","ff687145":"markdown","60f9468e":"markdown","9879e839":"markdown","2ecd4257":"markdown","b8d5b19d":"markdown","0b4f3b63":"markdown","9ef6634f":"markdown","eebef906":"markdown","4e75ccf0":"markdown"},"source":{"83e81d28":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom tqdm.auto import tqdm\nimport collections\n\nimport os\n\nfrom pathlib import Path\n\nimport json\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nsns.set_palette('Set3_r')\n\npd.set_option(\"display.max_rows\", 20, \"display.max_columns\", None)\n\nprint(os.listdir('..\/input\/'))\n        \nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = Warning)","6e6224d9":"train = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\nprint(train.shape)\ntrain.head()","06abe152":"test = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nprint(test.shape)\ntest.head()","faf058ff":"sub = pd.read_csv('\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv')\nprint(sub.shape)\nsub.head()","2dc16c4c":"external_hindi1 = pd.read_csv('\/kaggle\/input\/mlqa-hindi-processed\/mlqa_hindi.csv')\nprint(external_hindi1.shape)\nexternal_hindi1.head(2)","6fd81aa0":"external_hindi2 = pd.read_csv('\/kaggle\/input\/mlqa-hindi-processed\/xquad.csv')\nprint(external_hindi2.shape)\nexternal_hindi2.head(2)","72dd0935":"print('External hindi dataset...')\nexternal_hindi = pd.concat([external_hindi1, external_hindi2])\nprint(external_hindi.shape)\nexternal_hindi.head()","c5a35e6a":"print('External Tamil dataset...')\nexternal_tamil = pd.read_csv('\/kaggle\/input\/squad-translated-to-tamil-for-chaii\/squad_translated_tamil.csv')\nexternal_tamil['language'] = 'tamil'\nprint(external_tamil.shape)\nexternal_tamil.head(2)","a04a55c4":"print('Combined External dataset...')\nexternal_df = pd.concat([external_hindi, external_tamil])\nexternal_df = external_df.sample(frac = 1).reset_index(drop = True)\nprint(external_df.shape)\nexternal_df.head()","bced4bd1":"del external_hindi1, external_hindi2, external_hindi, external_tamil\ngc.collect()","54120334":"fig, (ax1, ax2) = plt.subplots(1, 2)\nsns.countplot(x = 'language', data = external_df, ax = ax1).set_title('External Dataset Language Counts')\nfor p in ax1.patches:\n    ax1.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nsns.countplot(x = 'language', data = test, ax = ax2).set_title('Test Language Counts')\nfor p in ax2.patches:\n    ax2.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","886187e9":"import yaml\n\nhparams = {\n    'DEVICE': 'TPU',\n    'EPOCHS': 2,\n    'MODEL_2': '..\/input\/jplu-tf-xlm-roberta-large',\n    'N_FOLDS': 2,\n    'SEED': 777,\n    'VERBOSE': 1,\n    'BATCH_SIZE': 32,\n    'MAX_LENGTH': 512,\n    'DOC_STRIDE': 128\n    \n}","4bb289ac":"import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nimport transformers\nfrom transformers import AutoTokenizer, TFXLMRobertaForQuestionAnswering, TFXLMRobertaModel\n\nprint(tf.__version__)\nprint(transformers.__version__)","130e1406":"SEED = hparams['SEED']\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","e0908b1f":"model_checkpoint = hparams['MODEL_2']\nbatch_size = hparams['BATCH_SIZE']","2bdf14a2":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_special_tokens = True)\nprint(tokenizer)","5e85328e":"train['context_num_tokens'] = train['context'].apply(lambda x: len(tokenizer(x)['input_ids']))\ntrain['context_num_tokens'].max()","4b84c746":"train['context_num_tokens'].hist()","dcc23edf":"train = train.sample(frac = 1, random_state = 2021).reset_index(drop = True)\nprint(train.shape)\ntrain.head()","832d778a":"#Split data to folds\nn_folds = hparams['N_FOLDS']\ntrain['kfold'] = -1\n\nskf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = SEED)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X = train, y = train['language'].values)):\n    train.loc[val_idx, 'kfold'] = fold\ntrain.head(2)","5ea1cfc5":"DEVICE = 'TPU'\n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","5cba1e8c":"max_length = hparams['MAX_LENGTH'] #The maximum length of a feature (question and context)\ndoc_stride = hparams['DOC_STRIDE'] #The authorized overlap between two part of the context when splitting it if needed.\n\npad_on_right = tokenizer.padding_side == \"right\"","1f34b199":"def prepare_training(examples):\n    examples['question'] = [q.lstrip().rstrip('?') for q in examples['question']] #remove leading white space\n    \n    #Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    #in one example possible giving several features when a context is long, each of those features having a\n    #context that overlaps a bit the context of the previous feature.\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    #Since one example might give us several features if it has a long context, we need a map from a feature to\n    #its corresponding example. This key gives us just that.\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #The offset mappings will give us a map from token to character position in the original context. This will\n    #help us compute the start_positions and end_positions.\n    \n    offset_mapping = tokenized_examples.pop('offset_mapping')\n    \n    tokenized_examples['start_positions'] = []\n    tokenized_examples['end_positions'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples['input_ids'][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        \n        sequence_ids = tokenized_examples.sequence_ids(i)\n        \n        #One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples.loc[sample_index, 'answer_text']\n        start_char = examples.loc[sample_index, 'answer_start']\n        \n        # If no answers are given, set the cls_index as answer.\n        if start_char is None:\n            tokenized_examples['start_positions'].append(cls_index)\n            tokenized_examples['end_positions'].append(cls_index)\n        else:\n            # Start\/end character idx of the answer in the text.\n            end_char = start_char + len(answers)\n            \n             #Start token idx of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n            #Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples['start_positions'].append(cls_index)\n                tokenized_examples['end_positions'].append(cls_index)\n            else:\n                #Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                #Note: we could go after the last offset if the answer is the last word (edge case).\n                \n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples['start_positions'].append(token_start_index - 1)\n                \n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples['end_positions'].append(token_end_index + 1)\n\n    return tokenized_examples","b07fe024":"def prepare_validation(examples):\n    examples['question'] = [q.lstrip() for q in examples['question']]\n    examples['question'] = [q.rstrip('?') for q in examples['question']]\n    \n    tokenized_examples = tokenizer(\n                list(examples['question' if pad_on_right else 'context'].values),\n                list(examples['context' if pad_on_right else 'question'].values),\n                truncation = 'only_second' if pad_on_right else 'only_first',\n                max_length = max_length,\n                stride = doc_stride,\n                return_overflowing_tokens = True,\n                return_offsets_mapping = True,\n                padding = 'max_length'\n            )\n    \n    sample_mapping = tokenized_examples.pop('overflow_to_sample_mapping')\n    \n    #id column from the dataset\n    tokenized_examples['example_id'] = []\n\n    for i in range(len(tokenized_examples['input_ids'])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples['example_id'].append(examples.loc[sample_index, 'id'])\n        tokenized_examples['offset_mapping'][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples['offset_mapping'][i])\n        ]\n\n    return tokenized_examples","53b8f6d0":"def build_tf_dataset(df, batch_size = 4, flag = 'train'):\n    \n    if flag == 'train':\n        features = prepare_training(df)\n    else:\n        features = prepare_validation(df)\n    \n    input_ids = features['input_ids']\n    attn_masks = features['attention_mask']\n    \n    if flag == 'train':\n        start_positions = features['start_positions']\n        end_positions = features['end_positions']\n        train_dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks, start_positions, end_positions))\n        train_dataset = train_dataset.map(lambda x1, x2, y1, y2: ({'input_ids': x1, 'attention_mask': x2}, {'start_positions': y1, 'end_positions': y2}))\n        train_dataset = train_dataset.batch(batch_size)\n        train_dataset = train_dataset.shuffle(1000)\n        train_dataset = train_dataset.prefetch(AUTO)\n        \n        return train_dataset, features\n    \n    elif flag == 'valid':\n        dataset = tf.data.Dataset.from_tensor_slices((input_ids, attn_masks))\n        dataset = dataset.map(lambda x1, x2: ({'input_ids': x1, 'attention_mask': x2}))\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(buffer_size = AUTO)\n        \n        return dataset, features","1198b2d4":"def build_model():\n    roberta = TFXLMRobertaModel.from_pretrained(model_checkpoint)\n    \n    input_ids = tf.keras.layers.Input(shape = (max_length, ), name = 'input_ids', dtype = tf.int32)\n    attention_mask = tf.keras.layers.Input(shape = (max_length, ), name = 'attention_mask', dtype = tf.int32)\n    \n    embeddings = roberta(input_ids = input_ids, attention_mask = attention_mask)[0]\n    \n    x1 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x1 = tf.keras.layers.Dense(1, use_bias = False)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax', name = 'start_positions')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(embeddings) \n    x2 = tf.keras.layers.Dense(1, use_bias = False)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax', name = 'end_positions')(x2)\n\n    model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = [x1, x2])\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)\n    \n    model.compile(loss = [loss, loss], optimizer = optimizer)\n\n    return model","9eebeff8":"from transformers import logging\n\nlogging.set_verbosity(40)","b316d65a":"# K.clear_session()\n# with strategy.scope():\n#     model = build_model()\n    \n# print(model.summary())","98b6c3ff":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","5274cd74":"def post_process_predictions(examples, features, start, end, n_best_size = 20, max_answer_length = 30):\n    \n    all_start_logits, all_end_logits = start, end\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples['id'])}\n    features_per_example = collections.defaultdict(list)\n    \n    for i, feature in enumerate(features['example_id']):\n        features_per_example[example_id_to_index[feature]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features['input_ids'])} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in examples.iterrows():\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example['context']\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features['offset_mapping'][feature_index]\n\n            # Update minimum null prediction.\n            cls_index = features['input_ids'][feature_index].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key = lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        #if not squad_v2:\n        #    predictions[example[\"id\"]] = best_answer[\"text\"]\n        #else:\n        answer = best_answer[\"text\"] \n        predictions[example['id']] = answer\n\n    return predictions","e711bf90":"t1= time()\nprint(f\" time starting {time()-t1}\")\nstart_probs, end_probs = [], []\njaccard_scores=[]\nfold =1\nepochs = hparams['EPOCHS']\ntrain_df = train[train['kfold'] != fold]\nvalid_df = train[train['kfold'] == fold]\n#concat external_df to train_df for training, no change in valid_df\ntrain_df = pd.concat([train_df.iloc[:, 1:-1], external_df])\ntrain_df = train_df.reset_index(drop = True)\nvalid_df = valid_df.reset_index(drop = True) \ntrain_dataset, train_enc = build_tf_dataset(train_df, batch_size = batch_size, flag = 'train')\nvalid_dataset, valid_enc = build_tf_dataset(valid_df, batch_size = batch_size, flag = 'valid')\nprint(f\" built all datasets {time()-t1}\")\n\nK.clear_session()\nwith strategy.scope():\n    model = build_model()\n    \nprint(f\"built model {time()-t1} \")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(f'qa_model_{fold + 1}.h5', verbose = 1, monitor = 'loss', mode = 'min', save_best_only = True, \n                                                save_weights_only = True)\n\nhistory = model.fit(train_dataset, \n                        epochs = 4, \n                        batch_size = batch_size,\n                        callbacks = [checkpoint],\n                        verbose = 1\n                        )\nprint('Predicting valid dataset...')\nstart_pred, end_pred = model.predict(valid_dataset, batch_size = batch_size, verbose = 1)\nprint('Post-process predictions...')\nvalid_preds = post_process_predictions(valid_df, valid_enc, start_pred, end_pred)\n    \nscore = []\nfor idx in range(len(valid_df)):\n    str1 = valid_df['answer_text'].values[idx]\n    str2 = valid_preds[valid_df.loc[idx, 'id']]\n    score.append(jaccard(str1, str2))\nprint(f'Jaccard Score for fold {fold + 1}: {np.mean(score)}')\njaccard_scores.append(np.mean(score))\n    \nstart_probs.append(start_pred)\nend_probs.append(end_pred)\n\n","2269d837":"# strart_probs, end_probs = [], []\n# epochs = hparams['EPOCHS']\n# jaccard_scores = []\n\n\n# for i, fold in enumerate(range(n_folds)):\n#     print('=======' * 15)\n#     print(f\"Fold: {fold + 1}\")\n#     print('=======' * 15)\n#     train_df = train[train['kfold'] != fold]\n#     valid_df = train[train['kfold'] == fold]\n    \n#     #concat external_df to train_df for training, no change in valid_df\n#     train_df = pd.concat([train_df.iloc[:, 1:-1], external_df])\n\n#     train_df = train_df.reset_index(drop = True)\n#     valid_df = valid_df.reset_index(drop = True)    \n#     train_dataset, train_enc = build_tf_dataset(train_df, batch_size = batch_size, flag = 'train')\n#     valid_dataset, valid_enc = build_tf_dataset(valid_df, batch_size = batch_size, flag = 'valid')\n    \n    \n#     K.clear_session()\n#     with strategy.scope():\n#         model = build_model()\n#     if i == 0:\n#         print(model.summary())\n    \n#     checkpoint = tf.keras.callbacks.ModelCheckpoint(f'qa_model_{fold + 1}.h5', verbose = 1, monitor = 'loss', mode = 'min', save_best_only = True, \n#                                                 save_weights_only = True)\n\n#     history = model.fit(train_dataset, \n#                         epochs = 3, \n#                         batch_size = batch_size,\n#                         callbacks = [checkpoint],\n#                         verbose = 1\n#                         )\n#     print('Predicting valid dataset...')\n#     start_pred, end_pred = model.predict(valid_dataset, batch_size = batch_size, verbose = 1)\n#     print('Post-process predictions...')\n#     valid_preds = post_process_predictions(valid_df, valid_enc, start_pred, end_pred)\n    \n#     score = []\n#     for idx in range(len(valid_df)):\n#         str1 = valid_df['answer_text'].values[idx]\n#         str2 = valid_preds[valid_df.loc[idx, 'id']]\n#         score.append(jaccard(str1, str2))\n#     print(f'Jaccard Score for fold {fold + 1}: {np.mean(score)}')\n#     jaccard_scores.append(np.mean(score))\n    \n#     strart_probs.append(start_pred)\n#     end_probs.append(end_pred)\n    \n#     del train_dataset, valid_dataset, model\n#     gc.collect()","58ff58ff":"hparams['JAC_SCORES'] = jaccard_scores\n\nwith open(r'hparams.yaml', 'w') as f:\n    yaml.dump(hparams, f)","56120b89":"test_dataset, test_enc = build_tf_dataset(test, batch_size = batch_size, flag = 'valid')","c86d5954":"start_probs, end_probs = [], []\nstart_pred, end_pred = model.predict(test_dataset, batch_size = batch_size, verbose = 1)\nprint(start_pred.shape, end_pred.shape)\nstart_probs.append(start_pred)\nend_probs.append(end_pred)","29df172c":"# start_probs, end_probs = [], []\n# for fold in range(n_folds):\n#     with strategy.scope():\n#         model = build_model()\n#     print('Loading trained model weights...')\n#     model.load_weights(f'qa_model_{fold + 1}.h5')\n#     print(f'Predicting testset - Fold: {fold + 1}...')\n#     start_pred, end_pred = model.predict(test_dataset, batch_size = batch_size, verbose = 1)\n#     print(start_pred.shape, end_pred.shape)\n#     start_probs.append(start_pred)\n#     end_probs.append(end_pred)","9ba03c42":"test_start_probs, test_end_probs = np.mean(start_probs, axis = 0), np.mean(end_probs, axis = 0)\npredictions = post_process_predictions(test, test_enc, test_start_probs, test_end_probs)","82fbb660":"sub_df = pd.DataFrame({'id': list(predictions.keys()), 'PredictionString': list(predictions.values())})\nsub_df.to_csv('.\/submission.csv', index = False)\nsub_df.head()","82d1ee9e":"# Fine Tune Model","2dfb09a5":"### Ref:\n- https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/\n- https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n- https:\/\/www.kaggle.com\/wchowdhu\/hands-on-nli-w-transformers-m-bert-xlm-roberta#Data-Exploration-and-Analysis","ab0dc354":"# Set TPU","068fa7f4":"# Competition Overview:\n\nIn this competition, the goal is to predict answers to real questions about Wikipedia articles. You will use chaii-1, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators. ","4b27beaa":"# Metrics","3c958887":"- The context length is too long, it has to be split into pieces before processing\n- Usually in NLP tasks long documents are truncated but in QA tasks truncating 'context' would lead to loss of answer\n- To avoid this, the long context is split into many input features each of length less than the max_length parameter\n- And if answer is at the split, we use overlapping of split features which is controlled by the parameter doc_stride","ff687145":"Save the hyperparameters and the validation scores for prediction notebook","60f9468e":"# Huggingface TF XLM RoBerta\nXLM-RoBERTa is a scaled cross lingual sentence encoder. It is trained on 2.5T of data across 100 languages data filtered from Common Crawl. XLM-R achieves state-of-the-arts results on multiple cross lingual benchmarks.","9879e839":"# Predict on Test Data","2ecd4257":"\n# Competition Rules:\n- CPU Notebook <= 5 hours run-time\n-GPU Notebook <= 5 hours run-time\n-Internet access disabled\n- Freely & publicly available external data is allowed, including pre-trained models\n- Submission file must be named submission.csv","b8d5b19d":"### External Datasets\n- This Notebook uses @rhtsingh's hindi dataset: [Hindi External](https:\/\/www.kaggle.com\/rhtsingh\/external-data-mlqa-xquad-preprocessing\/data)\n- For Tamil, the dataset i have created: [Tamil External](https:\/\/www.kaggle.com\/msafi04\/squad-translated-to-tamil-for-chaii)","0b4f3b63":"# Competition Metrics:\nThe metric in this competition is the word-level Jaccard score\n\n`def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))`","9ef6634f":"# TF XLM RoBerta Model","eebef906":"# Post Process Predictions","4e75ccf0":"Thanks to Kaggle and fellow Kagglers for the all the learnings, nothing beats doing and learning!!!"}}