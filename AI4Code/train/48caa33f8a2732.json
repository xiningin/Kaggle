{"cell_type":{"97afdb73":"code","301d4911":"code","09ad7dd5":"code","daca59c0":"code","64eaf58b":"code","38912360":"code","0f86c369":"code","48c892a6":"code","1a0440b0":"code","8f0061a8":"code","5b1ecb5a":"code","f5e00060":"code","39622f68":"code","5c995dd6":"code","7b6c34e7":"code","44e8893c":"code","c02d5513":"code","8d2cec0f":"code","881b2290":"code","2c2250aa":"code","deaba8c8":"code","b50c07ae":"code","1322264c":"code","2ada5d1f":"code","bb9b8a2f":"code","d54aed0b":"code","ad7085d4":"code","a14f20fa":"code","69f75540":"markdown","915f906e":"markdown","aeafbf77":"markdown","60821288":"markdown","3786ea5e":"markdown","ac6411c4":"markdown","481a8b7a":"markdown","dde0a085":"markdown","db330adf":"markdown","33f84b21":"markdown","9815c427":"markdown","5b7fa698":"markdown","ef0dbba2":"markdown","0bd6af3b":"markdown","397b530c":"markdown"},"source":{"97afdb73":"import numpy as np \nimport pandas as pd\n\ndf_train = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\ndel(df_train['id'])\ndf_train.head()","301d4911":"df_valid = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\ndel(df_valid['id'])\ndf_valid.head()","09ad7dd5":"df_test = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\ndel(df_test['id'])\ndf_test.head()","daca59c0":"import plotly.express as px\nimport plotly.graph_objects as go\n\ncols = [col for col in df_train.columns]\ncols.remove('comment_text')\ntoxic_cats = {}\n\nfor i in cols:\n    i1 = i.capitalize()\n    i1 = i1.replace(\"_\", \" \")\n    toxic_cats[i1] = df_train[i].value_counts()[1]\n\n\n\n\n\nfig = px.bar(x=toxic_cats.values(), y=toxic_cats.keys(), text=toxic_cats.values(),\n             width=700, height=400, title='N\u00ba of comments per toxicity level',\n             color=toxic_cats.values(),\n             labels={'x': 'N\u00ba of comments', 'y': 'Level'})\nfig.update_layout(barmode='stack', yaxis={'categoryorder':'total ascending'})\n\nwith_toxic = {}\n\nfor i in cols:\n    i1 = i.capitalize()\n    i1 = i1.replace(\"_\", \" \")\n    with_toxic[i1] = sum(np.where((df_train['toxic'] == df_train[i]) & (df_train['toxic'] == 1),\n                                   True, False))\n\nfig = px.bar(x=with_toxic.values(), y=with_toxic.keys(), text=with_toxic.values(),\n             width=700, height=400, title='N\u00ba of comments per toxicity level',\n             color=with_toxic.values(),\n             labels={'x': 'N\u00ba of comments', 'y': 'Level'})\nfig.update_layout(barmode='stack', yaxis={'categoryorder':'total ascending'})\nfig.show()","64eaf58b":"fig = px.pie(values=toxic_cats.values(), names=toxic_cats.keys(), width=700, height=400,\n            title=\"Distribution of comments' toxicity categories\")\nfig.show()","38912360":"fig = go.Figure(data=[\n    go.Bar(y=[a for a in toxic_cats.values()], x=[a for a in toxic_cats.keys()],\n           name='Total', marker_color='purple'),\n    go.Bar(y=[a for a in with_toxic.values()], x=[a for a in with_toxic.keys()],\n          name='Toxic as well', marker_color='yellow')\n])\n\nfig.update_layout(title='Are comments in other categories in toxic as well?', barmode='group', xaxis={'categoryorder':'total descending'})\n\n\nfig.show()","0f86c369":"toxic_bfr = df_train.toxic.value_counts()[1]\n\nfor i in range(len(df_train)):\n    if df_train.loc[i,'toxic'] == 0 and (df_train.loc[i, 'obscene'] == 1 or\n                                         df_train.loc[i, 'severe_toxic'] == 1 or\n                                         df_train.loc[i, 'threat'] == 1 or\n                                         df_train.loc[i, 'insult'] == 1 or\n                                         df_train.loc[i, 'identity_hate'] == 1):\n        df_train.loc[i,'toxic'] = 1\n        \ntoxic_after = df_train.toxic.value_counts()[1]\ntoxic_comments = toxic_after - toxic_bfr\nprint('There are %i new toxic comments.' %toxic_comments)","48c892a6":"import gc \n\ndel(df_train['obscene'])\ndel(df_train['identity_hate'])\ndel(df_train['insult'])\ndel(df_train['threat'])\ndel(df_train['severe_toxic'])\n\ngc.collect()","1a0440b0":"languages_val = {a:b for a,b in zip(df_valid['lang'].unique(), df_valid['lang'].value_counts())}\nlanguages_val['Spanish'] = languages_val.pop('es')\nlanguages_val['Italian'] = languages_val.pop('it')\nlanguages_val['Turkish'] = languages_val.pop('tr')\n\n\nfig = px.pie(values=languages_val.values(), names=languages_val.keys(), width=700, height=400,\n            title=\"Distribution of comments' languages in validation data\")\nfig.show()","8f0061a8":"languages_test = {a:b for a,b in zip(df_test['lang'].unique(), df_test['lang'].value_counts())}\nlanguages_test['Spanish'] = languages_test.pop('es')\nlanguages_test['Italian'] = languages_test.pop('it')\nlanguages_test['Turkish'] = languages_test.pop('tr')\nlanguages_test['Russian'] = languages_test.pop('ru')\nlanguages_test['French'] = languages_test.pop('fr')\nlanguages_test['Portuguese'] = languages_test.pop('pt')\n\nfig = px.pie(values=languages_test.values(), names=languages_test.keys(), width=700, height=400,\n            title=\"Distribution of comments' languages in testing data\")\nfig.show()","5b1ecb5a":"print(\"There are %.2f%% toxic comments in the training data.\"%(df_train['toxic'].value_counts()[1]\/df_train['toxic'].value_counts()[0]*100))","f5e00060":"print(\"There are %.2f%% toxic comments in the validation data.\"%(df_valid['toxic'].value_counts()[1]\/df_valid['toxic'].value_counts()[0]*100))","39622f68":"print(\"The validation dataframe represents a %.2f%% of the training data.\" %(df_valid.shape[0]\/(df_train.shape[0]+df_valid.shape[0])))","5c995dd6":"del(df_valid['lang'])\ndel(df_valid['comment_text'])\ndf_valid = df_valid.rename(columns={'translated':'comment_text'})\n\ngc.collect()","7b6c34e7":"df = pd.concat([df_train, df_valid], ignore_index=True, axis=0)\n\ndf","44e8893c":"from sklearn.model_selection import train_test_split\n\nX = df['comment_text']\ny = df['toxic']\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y,\n                                                       random_state=1,\n                                                       train_size=0.8\n                                                      )","c02d5513":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvec = TfidfVectorizer(decode_error='ignore',stop_words='english', max_df=0.8, max_features=1600)\nx_train = vec.fit_transform(x_train).todense()\nx_train = pd.DataFrame(x_train, columns=vec.get_feature_names())","8d2cec0f":"x_valid = vec.transform(x_valid).todense()\nx_valid = pd.DataFrame(x_valid, columns=vec.get_feature_names())","881b2290":"del(df)","2c2250aa":"print(\"There are %.2f%% toxic comments in train data.\"%(y_train.sum()\/len(y_train)*100))","deaba8c8":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=1)\n\nx_train, y_train = sm.fit_resample(x_train, y_train)","b50c07ae":"x_train.shape","1322264c":"x_train.tail()","2ada5d1f":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\n\nmodel = XGBClassifier(n_estimators=800,\n                      use_label_encoder=False,\n                      learning_rate=0.1,\n                      max_depth=6,\n                      gamma=1,\n                      scale_pos_weight=7,\n                      random_state=1)\n\nmodel.fit(x_train, y_train)\n\npreds = model.predict(x_valid)\n\nf1 = f1_score(preds, y_valid)\n\nprint(\"F1 Score: %.4f\" %f1)\n","bb9b8a2f":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(y_valid, preds, labels=model.classes_, normalize='true')\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                             display_labels=model.classes_)\ndisp.plot() ","d54aed0b":"x_test = vec.transform(df_test['translated']).todense()\nx_test = pd.DataFrame(x_test, columns=vec.get_feature_names())","ad7085d4":"preds_test = model.predict(x_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'id': df_test.index,\n                       'toxic': preds_test})\noutput.to_csv('submission.csv', index=False)\n\noutput.head()","a14f20fa":"output['toxic'].value_counts()","69f75540":"# Predictions\n------","915f906e":"# Training our Model\n-----------","aeafbf77":"The training dataset is not well balanced (there are way more non-toxic comments than toxic ones). We will use SMOTE to add new toxic comments. ","60821288":"# Data Analysis\n------------------","3786ea5e":"If you came this far, thank you so much. As I said before, please let me know if you liked it and tell me how can I improve this solution!","ac6411c4":"# Preprocessing\n-------","481a8b7a":"## Test","dde0a085":"## Train","db330adf":"We already imported a dataset translated to english using Yandex.Translate, so we will use only the translated comments.","33f84b21":"It's a huge dataset, so we will delete some columns and dataframes to save RAM Memory (I've already allocated more memory that I can count to solve it, please let me know if you have some tips on how to save more RAM memory).","9815c427":"## Validation","5b7fa698":"# Introduction\n-------\nNowadays, it's difficult to chat online without reading some toxic comments. To solve this problem, an option is to social medias forbid this harmful content. One way to do so is to create machine learning algorithms that can tell us if a comment is or isn't toxic. That's our goal in this Notebook.\n\nGiven a training dataset, we will build a model to predict if a english comment is toxic. But there is another problem - the testing dataset has comments in differents languages. A solution is to translate them to English, and that's what https:\/\/www.kaggle.com\/kashnitsky has already done for us using the Yandex.Translate's API. We will analyse the data, preprocess the comments and use a EDA Model to reach our goal. So, let's start! Please let me know if you liked the notebook and please comment below how can I improve it! ","ef0dbba2":"# Importing Data\n-------","0bd6af3b":"We can clearly see the relation between toxic and other categories, so we will replace the comments that are classified as non-toxic to toxic if they are included in other level of toxicity.","397b530c":"Our validation dataframe represents only 0.03% of training data, and the toxic comments are disproportionate distributed between both dataframes. So we will need to join them and split them randomly to have a more accurate result."}}