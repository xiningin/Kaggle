{"cell_type":{"7121a9e7":"code","4f48655f":"code","b394ba93":"code","3646d876":"code","6179634d":"code","6e05b1b1":"code","b99011bf":"code","39a63e5b":"code","23f9d29e":"code","895c5256":"code","5623cc84":"code","b0ea983f":"code","c54879a7":"code","0de2b417":"code","9618546a":"code","6c6c1087":"code","91c33cad":"code","00547556":"code","989a7370":"code","a62c370c":"code","4995a365":"code","4812b80b":"code","163c55e3":"code","650922ab":"code","e2e05e6f":"code","817fe950":"markdown","093f2015":"markdown","4821f7f7":"markdown","065cc82a":"markdown","acf2565e":"markdown","d64a952b":"markdown","cdc463e3":"markdown","fc91e66a":"markdown","5de587cb":"markdown"},"source":{"7121a9e7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nimport seaborn as sns\n\n%matplotlib inline","4f48655f":"df = pd.read_csv('..\/input\/bmwcsv\/bmw.csv')\ndf.head()","b394ba93":"df.info()","3646d876":"print(df.transmission.unique())\nprint(df.model.unique())\nprint(df.fuelType.unique())","6179634d":"df.corr()","6e05b1b1":"plt.figure(figsize=(10,10))\nsns.set_style(\"darkgrid\")\nsns.boxplot(data=df[['price','mileage']])\nplt.show","b99011bf":"plt.figure(figsize=(10,10))\nsns.set_style(\"darkgrid\")\nsns.boxplot(data=df['mpg'])\nplt.xlabel(\"mpg\")\nplt.show","39a63e5b":"plt.figure(figsize=(10,10))\nplt.scatter(x=df['year'], y=df['price'])\nplt.show","23f9d29e":"plt.figure(figsize=(10,10))\nplt.scatter(x=df['mileage'], y=df['price'])\nplt.show","895c5256":"plt.figure(figsize=(10,10))\nplt.scatter(x=df['engineSize'], y=df['price'])\nplt.show","5623cc84":"plt.figure(figsize=(16,5))\nsns.set_style(\"darkgrid\")\nsns.stripplot(x=\"model\", y=\"price\", data=df)\nplt.show","b0ea983f":"df.describe()","c54879a7":"plt.figure(figsize=(16,5))\nsns.set_style(\"darkgrid\")\nsns.stripplot(x=\"transmission\", y=\"price\", data=df)\nplt.show","0de2b417":"plt.figure(figsize=(16,5))\nsns.set_style(\"darkgrid\")\nsns.stripplot(x=\"fuelType\", y=\"price\", data=df)\nplt.show","9618546a":"features= ['year','transmission','mileage','fuelType','engineSize']\n\ndf2= df.drop(['model','tax','mpg'], axis=1)\nprint(df2)","6c6c1087":"df2= df.drop(['model','tax','mpg'], axis=1)\n\nd = {'Diesel':0, 'Petrol':1, 'Other':2 , 'Hybrid':3, 'Electric':4}\ndf2['fuelType'] = df2['fuelType'].map(d)\n\nd1 ={'Automatic':0, 'Manual':1, 'Semi-Auto':2}\ndf2['transmission'] = df2['transmission'].map(d1)\n\nprint(df2)","91c33cad":"X = df2[['year','mileage','transmission','fuelType','engineSize']]\ny = df2['price']\n\nfrom sklearn import linear_model\n\nregr = linear_model.LinearRegression()\n\nregr.fit(X,y)\n\nprint(regr.coef_)\n\nprdictedprice = regr.predict([[2021,100000,2,0,2]])\n\nprint(prdictedprice)","00547556":"import statsmodels.formula.api as smf\n\nmodel = smf.ols('price ~ year + mileage + transmission + fuelType + engineSize', data=df2)\n\nresults =model.fit()\n\nprint(results.summary())","989a7370":"from scipy import stats","a62c370c":"X = df['mileage']\ny = df['price']\n\nslope, intercept, r, p ,std_err = stats.linregress(X,y)\n\ndef myfunc(X):\n    return slope*X  + intercept\n\nmymodel = list(map(myfunc, X))\n\nfig, ax =plt.subplots()\n\nax.scatter(X,y)\nax.plot(X, mymodel)\nfig.set_size_inches(15,8)\nfig.show\n\nprint(\"r value of given problem:\", r)\nprint(\"p value of given problem:\", p)\nprint(std_err)\nprint(slope, intercept)","4995a365":"from sklearn import linear_model\n\nX = df[['mileage', 'year']]\ny = df['price']\n\nregr = linear_model.LinearRegression()\n\nregr.fit(X,y)\n\nprint(regr.coef_)\nprint(regr.predict([[100000,2021]]))\n\n\ntest_df = df.loc[7000:,['mileage','year']]\n\ny_test = df.loc[7000:,'price']\n\nX_test = test_df[['mileage','year']]\n\ny_pred = regr.predict(X_test)\n\nprint(y_pred)\n\nfrom sklearn.metrics import r2_score\n\nr2 = r2_score(y_test, y_pred)\n\nprint(r2)\n\nfrom sklearn.metrics import mean_squared_error\n\nMSE = mean_squared_error(y_test, y_pred)\n\nprint(MSE)","4812b80b":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nX=df2[['year','transmission','mileage','fuelType','engineSize']]\ny= df2['price']\n\n# selecting training and testing data\ntrain_X, val_X,train_y, val_y = train_test_split(X,y, random_state=0)\n\n#model training and fiting data\nmodel = DecisionTreeRegressor(random_state=0)\n\nmodel.fit(train_X,train_y)\n\n#predicting and validaiton model\nval_predictions = model.predict(val_X)\n\nmae = mean_absolute_error(val_y,val_predictions)\n\nprint(mae)","163c55e3":"X = df2[['year','mileage','transmission','fuelType','engineSize']]\ny = df2['price']\n\nfrom sklearn.feature_selection import mutual_info_regression\n\nmiscores = mutual_info_regression(X,y, random_state=0)\n\nmiscores = pd.Series(miscores, name=\"MI_SCORE\", index=X.columns)\nprint(miscores)","650922ab":"# Optimization for number of leaf nodes\n\ndef get_mae(max_leaf_nodes,train_X, val_X,train_y, val_y):\n        nmodel = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n        nmodel.fit(train_X,train_y)\n        val_predict = nmodel.predict(val_X)\n        mae = mean_absolute_error(val_y,val_predict)\n        print(mae)\n        \n        \n# getting various MAE from decision tree regressor\n \nfor max_leaf_nodes in [10,30,50,80,100,150,200]:\n    get_mae(max_leaf_nodes,train_X,val_X,train_y,val_y)\n    print(max_leaf_nodes)","e2e05e6f":"from xgboost import XGBRegressor\n\nX=df2[['year','transmission','mileage','fuelType','engineSize']]\ny= df2['price']\n\ntrain_X, val_X, train_y, val_y = train_test_split(X,y, random_state=0)\n\n\nmy_model1 = XGBRegressor(n_estimators=1000, learning_rate=0.05,  random_state=0)\n\nmy_model1.fit(train_X,train_y)\n\npreds = my_model1.predict(val_X)\n\n#mae\n\nMAE = mean_absolute_error(val_y, preds)\n\nprint(MAE)","817fe950":"# As per above results at **leaf_nodes=100** we have a least **MAE= 3374.9724**","093f2015":"# Model 6","4821f7f7":"# Model 2","065cc82a":"# Model 3","acf2565e":"# Model 1\n","d64a952b":"# Model 4","cdc463e3":"# I have tested various regression models for car pricing prediction dataset, below code represents EDA, Feature Selection and Model training \n\nDo check it out and looking forward to your review on the same!","fc91e66a":"# Model 5","5de587cb":"# Model 7"}}