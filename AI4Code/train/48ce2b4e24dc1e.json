{"cell_type":{"c9cab497":"code","4d26cc5d":"code","497d81cc":"code","db91592d":"code","57abdd2f":"code","5fcd5018":"code","810d7ee7":"code","dbb9ba5d":"code","5ff2bee7":"code","4d2a1345":"code","b2697e88":"code","d4d1e0fb":"code","140ba696":"code","c3451931":"code","e4e13d24":"code","103041fe":"code","cfa79f61":"code","b501d5ef":"code","9a773417":"code","2d13a463":"code","ce3e0a24":"code","3b7907f1":"code","c49b41ea":"code","b2b46e7e":"code","c3a81e53":"code","b795a97c":"code","c337c215":"code","aefc38a6":"code","e864b2ae":"code","2240a305":"code","12b07bd0":"code","ef28429d":"code","d1fb330c":"code","bf8ad5be":"code","2efe3ea7":"code","56d33a8f":"code","2ee2e629":"code","69b7b91b":"code","9f5274fb":"code","0335d86b":"code","67f584ba":"code","28864fbe":"code","0ec20b22":"code","179aa95a":"code","c46c45be":"code","a58fe2c7":"code","8558c652":"code","a8e94411":"code","7660014f":"code","82ce2cec":"code","5909081f":"code","a2b10b9e":"markdown","54751883":"markdown","1582c524":"markdown","9e7c9cc5":"markdown","b102a03f":"markdown","7b331787":"markdown","7925b60b":"markdown"},"source":{"c9cab497":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d26cc5d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n# from sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom datetime import datetime\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","497d81cc":"# \uc635\uc158 \uc124\uc815\npd.options.display.max_columns = 1000","db91592d":"# random seed fixed\nnp.random.seed(5)","57abdd2f":"train = pd.read_csv('\/kaggle\/input\/dacon-bio-optical-data-analysis\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/dacon-bio-optical-data-analysis\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/dacon-bio-optical-data-analysis\/sample_submission.csv')\ndisplay(train, test)","5fcd5018":"# \ubc94\uc704\ub294 \uc0ac\ub78c\uc5d0 \ub530\ub77c \ub2e4\uc18c \ucc28\uc774\uac00 \uc788\uc73c\ub098, \ub300\uccb4\ub85c 380\u223c780nm\uc774\ub2e4. \ub2e8\uc0c9\uad11\uc778 \uacbd\uc6b0 700\u223c610nm\ub294 \ube68\uac15, 610\u223c590nm\ub294 \uc8fc\ud669, 590\u223c570nm\ub294 \ub178\ub791,\n# 570\u223c500nm\ub294 \ucd08\ub85d, 500\u223c450nm\ub294 \ud30c\ub791, 450\u223c400nm\ub294 \ubcf4\ub77c\ub85c \ubcf4\uc778\ub2e4.\n# \ube68\uac15\ubcf4\ub2e4 \ud30c\uc7a5\uc774 \uae34 \ube5b\uc744 \uc801\uc678\uc120, \ubcf4\ub77c\ubcf4\ub2e4 \ud30c\uc7a5\uc774 \uc9e7\uc740 \ube5b\uc744 \uc790\uc678\uc120\uc774\ub77c\uace0 \ud55c\ub2e4.\n# \ube5b\uc758 \uc9c4\ub3d9\uc218 = c \/ \ud30c\uc7a5\n# E = h\u03bd = hc \/ \u03bb (h=\ud50c\ub791\ud06c\uc0c1\uc218)","810d7ee7":"plt.figure(figsize=(15, 6))\nsns.distplot(train['hhb'])\nplt.show()","dbb9ba5d":"plt.figure(figsize=(15, 6))\nsns.distplot(train['hbo2'])\nplt.show()","5ff2bee7":"plt.figure(figsize=(15, 6))\nsns.distplot(train['ca'])\nplt.show()","4d2a1345":"plt.figure(figsize=(15, 6))\nsns.distplot(train['na'])\nplt.show()","b2697e88":"np.max(train['hhb'])","d4d1e0fb":"# # \uad6c\uac04 binding\n# bins = list(range(0, 25, 3))\n# bins","140ba696":"# bins_label = [str(x) + \"_\" + str(x+3) for x in bins]\n# bins_label","c3451931":"# train['hhb_level'] = pd.cut(train['hhb'], bins, right=False, labels=bins_label[:-1])\n# train","e4e13d24":"# \uacb0\uce21\uce58 \ubcf4\uac04\ubc95 \uc0ac\uc6a9\ndst_columns = [k for k in train.columns if 'dst' in k] # \uacb0\uce21\uce58\uac00 \uc788\ub294 _dst \uce7c\ub7fc\ub4e4\ub9cc \ucd94\ucd9c\ntrain_dst = train[dst_columns] # train data\uc5d0\uc11c \uacb0\uce21\uce58\uac00 \uc788\ub294 _dst \uce7c\ub7fc\ub4e4\ub9cc \ucd94\ucd9c\ntest_dst = test[dst_columns] # test data\uc5d0\uc11c \uacb0\uce21\uce58\uac00 \uc788\ub294 _dst \uce7c\ub7fc\ub4e4\ub9cc \ucd94\ucd9c","103041fe":"# train data \ubcf4\uac04\n# train[dst_columns] = train_dst.interpolate(axis=1, limit_direction='both')\ntrain[dst_columns] = train_dst.interpolate(axis=1)\nprint(train.isnull().sum().sum())\ntrain = train.fillna(0) # -1\nprint(train.isnull().sum().sum())","cfa79f61":"# test data \ubcf4\uac04 \n# test[dst_columns] = test_dst.interpolate(axis=1, limit_direction='both')\ntest[dst_columns] = test_dst.interpolate(axis=1)\nprint(test.isnull().sum().sum())\ntest = test.fillna(0) # -1\nprint(test.isnull().sum().sum())","b501d5ef":"# \ub610 \ub2e4\ub978 \ubcc0\uc218 \uc0dd\uc131\nsrc_list = [k for k in train.columns if 'src' in k]\ndst_list = [k for k in train.columns if 'dst' in k]\n# display(src_list, dst_list)","9a773417":"# src - dst = gap_feature \uce7c\ub7fc \ucd94\uac00\ngap_feature_names=[]\n\nfor i in range(650, 1000, 10):\n    gap_feature_names.append(str(i) + '_gap')\n\nalpha = pd.DataFrame(np.array(train[src_list]) - np.array(train[dst_list]), columns=gap_feature_names, index=train.index)\nbeta = pd.DataFrame(np.array(test[src_list]) - np.array(test[dst_list]), columns=gap_feature_names, index=test.index)\n\ntrain = pd.concat([train, alpha], axis=1)\ntest = pd.concat([test, beta], axis=1)\n\nprint(train.shape, test.shape)","2d13a463":"# src \/ dst = gap ratio \uce7c\ub7fc \ucd94\uac00\nepsilon=1e-10\n\nfor dst_col, src_col in zip(dst_list, src_list):\n    dst_val = train[dst_col]\n    src_val = train[src_col] + epsilon\n    delta_ratio = dst_val \/ src_val\n    train[dst_col + '_' + src_col + '_ratio'] = delta_ratio\n    \n    dst_val = test[dst_col]\n    src_val = test[src_col] + epsilon\n    \n    delta_ratio = dst_val \/ src_val\n    test[dst_col + '_' + src_col + '_ratio'] = delta_ratio\n    \nprint(train.shape, test.shape)","ce3e0a24":"# rho \ubd84\ud3ec \ud655\uc778\ntrain['rho'].value_counts()","3b7907f1":"# rho\uc758 \ud1b5\uacc4\ub7c9 \ucd94\uac00\nrho = train.drop(['id', 'hhb', 'hbo2', 'ca', 'na'], axis=1).groupby('rho').agg(['mean', 'std', 'max', 'min']).reset_index()\nrho.columns = [x[0] + '_' + x[1] for x in rho.columns]\nrho = rho.rename({'rho_' : 'rho'}, axis=1)\nrho","c49b41ea":"# rho\uc640 merge\ntrain = pd.merge(train, rho, on='rho', how='left')\ntest = pd.merge(test, rho, on='rho', how='left')\ntrain.shape, test.shape","b2b46e7e":"# \ud478\ub9ac\uc5d0 \ubcc0\ud658\n# DFT with numpy\nalpha_real = train[dst_list].copy()\nalpha_imag = train[dst_list].copy()\n\nbeta_real = test[dst_list].copy()\nbeta_imag = test[dst_list].copy()\n\nfor i in tqdm(alpha_real.index):\n    alpha_real.iloc[i] = alpha_real.iloc[i].copy() - alpha_real.iloc[i].copy().mean()\n    alpha_imag.iloc[i] = alpha_imag.iloc[i].copy() - alpha_real.iloc[i].copy().mean()\n    \n    alpha_real.iloc[i] = np.fft.fft(alpha_real.iloc[i], norm='ortho').real\n    alpha_imag.iloc[i] = np.fft.fft(alpha_imag.iloc[i], norm='ortho').imag\n\n    \nfor i in tqdm(beta_real.index):\n    beta_real.iloc[i] = beta_real.iloc[i].copy() - beta_real.iloc[i].copy().mean()\n    beta_imag.iloc[i] = beta_imag.iloc[i].copy() - beta_imag.iloc[i].copy().mean()\n    \n    beta_real.iloc[i] = np.fft.fft(beta_real.iloc[i], norm='ortho').real\n    beta_imag.iloc[i] = np.fft.fft(beta_imag.iloc[i], norm='ortho').imag\n    \nreal_part = []\nimag_part = []\n\nfor col in dst_list:\n    real_part.append(col + '_fft_real')\n    imag_part.append(col + '_fft_imag')\n    \nalpha_real.columns = real_part\nalpha_imag.columns = imag_part\nalpha = pd.concat([alpha_real, alpha_imag], axis=1)\n\nbeta_real.columns = real_part\nbeta_imag.columns = imag_part\nbeta = pd.concat([beta_real, beta_imag], axis=1)","c3a81e53":"# NaN \ud655\uc778\nalpha_real.isnull().sum().sum(), alpha_imag.isnull().sum().sum(), alpha.isnull().sum().sum(), beta.isnull().sum().sum()","b795a97c":"# numpy.fft \ub294 \uace0\uc18d \ud478\ub9ac\uc5d0 \ubcc0\ud658\uc744 \uc774\uc6a9\ud55c \uc774\uc0b0 \ud478\ub9ac\uc5d0 \ubcc0\ud658\uc744 \uacc4\uc0b0\ud574\uc90d\ub2c8\ub2e4.\n# \uc2e4\uc218\ubd80\uc640 \ud5c8\uc218\ubd80\ub85c \ub098\uc624\ub294 \uacb0\uacfc\ubb3c\uc744 \uac01\uac01 \ubcc0\uc218\ub85c \ucd94\uac00\ud574\uc90d\ub2c8\ub2e4.\ntrain = pd.concat([train, alpha], axis=1)\ntest = pd.concat([test, beta], axis=1)\n\nprint(train.shape, test.shape)","c337c215":"# \uac01 \ud589 src, dst\uc758 \ud1b5\uacc4\ub7c9 \ucd94\uac00\ntrain['src_mean'] = train.apply(lambda x : np.mean(x[src_list]), axis=1)\ntrain['src_median'] = train.apply(lambda x : np.median(x[src_list]), axis=1)\ntrain['src_std'] = train.apply(lambda x : np.std(x[src_list]), axis=1)\ntrain['src_sum'] = train.apply(lambda x : np.sum(x[src_list]), axis=1)\ntrain['src_max'] = train.apply(lambda x : np.max(x[src_list]), axis=1)\n\ntrain['dst_mean'] = train.apply(lambda x : np.mean(x[dst_list]), axis=1)\ntrain['dst_median'] = train.apply(lambda x : np.median(x[dst_list]), axis=1)\ntrain['dst_std'] = train.apply(lambda x : np.std(x[dst_list]), axis=1)\ntrain['dst_sum'] = train.apply(lambda x : np.sum(x[dst_list]), axis=1)\ntrain['dst_max'] = train.apply(lambda x : np.max(x[dst_list]), axis=1)\n\ntrain.head(2)","aefc38a6":"# \uac01 \ud589 src, dst\uc758 \ud1b5\uacc4\ub7c9 \ucd94\uac00\ntest['src_mean'] = test.apply(lambda x : np.mean(x[src_list]), axis=1)\ntest['src_median'] = test.apply(lambda x : np.median(x.loc[src_list]), axis=1)\ntest['src_std'] = test.apply(lambda x : np.std(x[src_list]), axis=1)\ntest['src_sum'] = test.apply(lambda x : np.sum(x[src_list]), axis=1)\ntest['src_max'] = test.apply(lambda x : np.max(x[src_list]), axis=1)\n\ntest['dst_mean'] = test.apply(lambda x : np.mean(x[dst_list]), axis=1)\ntest['dst_median'] = test.apply(lambda x : np.median(x[dst_list]), axis=1)\ntest['dst_std'] = test.apply(lambda x : np.std(x[dst_list]), axis=1)\ntest['dst_sum'] = test.apply(lambda x : np.sum(x[dst_list]), axis=1)\ntest['dst_max'] = test.apply(lambda x : np.max(x[dst_list]), axis=1)\n\ntest.head(2)","e864b2ae":"# \ub2e4\uc2dc \ube7c\uc8fc\ub294 \uce7c\ub7fc \ucd94\uac00\ntrain['src_mean_dst_mean_gap'] = train['src_mean'] - train['dst_mean']\ntrain['src_median_dst_median_gap'] = train['src_median'] - train['dst_median']\ntrain['src_sum_dst_sum_gap'] = train['src_sum'] - train['dst_sum']\ntrain['src_max_dst_max_gap'] = train['src_max'] - train['dst_max']\n\ntest['src_mean_dst_mean_gap'] = test['src_mean'] - test['dst_mean']\ntest['src_median_dst_median_gap'] = test['src_median'] - test['dst_median']\ntest['src_sum_dst_sum_gap'] = test['src_sum'] - test['dst_sum']\ntest['src_max_dst_max_gap'] = test['src_max'] - test['dst_max']","2240a305":"# \ub2e4\uc2dc \ube7c\uc8fc\ub294 \uce7c\ub7fc \ucd94\uac00\ntrain['src_mean_dst_mean_gap_ratio'] = train['src_mean'] \/ (train['dst_mean']+epsilon)\ntrain['src_median_dst_median_gap_ratio'] = train['src_median'] \/ (train['dst_median']+epsilon)\ntrain['src_sum_dst_sum_gap_ratio'] = train['src_sum'] \/ (train['dst_sum']+epsilon)\ntrain['src_max_dst_max_gap_ratio'] = train['src_max'] \/ (train['dst_max']+epsilon)\n\ntest['src_mean_dst_mean_gap_ratio'] = test['src_mean'] \/ (test['dst_mean']+epsilon)\ntest['src_median_dst_median_gap_ratio'] = test['src_median'] \/ (test['dst_median']+epsilon)\ntest['src_sum_dst_sum_gap_ratio'] = test['src_sum'] \/ (test['dst_sum']+epsilon)\ntest['src_max_dst_max_gap_ratio'] = test['src_max'] \/ (test['dst_max']+epsilon)\n\ntrain.shape, test.shape","12b07bd0":"# dst 10 \ub2e8\uc704\ub85c \ube7c\uc8fc\ub294 \uce7c\ub7fc \ucd94\uac00\n    \nfor i in range(650, 990, 10) :\n    col = \"{}_dst\".format(i)\n    col_10 = \"{}_dst\".format(i+10)\n    train[\"{}_dst-{}_dst\".format(i+10, i)] = train[col_10] - train[col]\n    test[\"{}_dst-{}_dst\".format(i+10, i)] = test[col_10] - test[col]\n    train[\"{}_dst\/{}_dst\".format(i+10, i)] = train[col_10] \/ train[col]\n    test[\"{}_dst\/{}_dst\".format(i+10, i)] = test[col_10] \/ test[col]\n    \n    train[\"{}_dst-{}_dst_rho\".format(i+10, i)] = (train[col_10] - train[col]) \/ train.loc[i, 'rho']\n    test[\"{}_dst-{}_dst_rho\".format(i+10, i)] = (test[col_10] - test[col]) \/ test.loc[i, 'rho']\n    train[\"{}_dst\/{}_dst_rho\".format(i+10, i)] = (train[col_10] \/ train[col]) \/ train.loc[i, 'rho']\n    test[\"{}_dst\/{}_dst_rho\".format(i+10, i)] = (test[col_10] \/ test[col]) \/ test.loc[i, 'rho']\n    \n    \n    col = \"{}_src\".format(i)\n    col_10 = \"{}_src\".format(i+10)\n    train[\"{}_src-{}_src\".format(i+10, i)] = train[col_10] - train[col]\n    test[\"{}_src-{}_src\".format(i+10, i)] = test[col_10] - test[col]    \n    train[\"{}_src\/{}_src\".format(i+10, i)] = train[col_10] \/ train[col]\n    test[\"{}_src\/{}_src\".format(i+10, i)] = test[col_10] \/ test[col]\n\n    \ntrain.tail(3)","ef28429d":"# dst 20 \ub2e8\uc704\ub85c \ube7c\uae30\nfor i in range(650, 990, 10) :\n    if (i+20) <= 990 :\n        col = \"{}_dst\".format(i)\n        col_20 = \"{}_dst\".format(i+20)\n        train[\"{}_dst-{}_dst\".format(i+20, i)] = train[col_20] - train[col]\n        test[\"{}_dst-{}_dst\".format(i+20, i)] = test[col_20] - test[col]\n        train[\"{}_dst\/{}_dst\".format(i+20, i)] = train[col_20] \/ train[col]\n        test[\"{}_dst\/{}_dst\".format(i+20, i)] = test[col_20] \/ test[col]","d1fb330c":"# dst 30 \ub2e8\uc704\ub85c \ube7c\uae30\nfor i in range(650, 990, 10) :\n    if (i+30) <= 990 :\n        col = \"{}_dst\".format(i)\n        col_30 = \"{}_dst\".format(i+30)\n        train[\"{}_dst-{}_dst\".format(i+30, i)] = train[col_30] - train[col]\n        test[\"{}_dst-{}_dst\".format(i+30, i)] = test[col_30] - test[col]\n        train[\"{}_dst\/{}_dst\".format(i+30, i)] = train[col_30] \/ train[col]\n        test[\"{}_dst\/{}_dst\".format(i+30, i)] = test[col_30] \/ test[col]","bf8ad5be":"# dst 40 \ub2e8\uc704\ub85c \ube7c\uae30\nfor i in range(650, 990, 10) :\n    if (i+40) <= 990 :\n        col = \"{}_dst\".format(i)\n        col_40 = \"{}_dst\".format(i+40)\n        train[\"{}_dst-{}_dst\".format(i+40, i)] = train[col_40] - train[col]\n        test[\"{}_dst-{}_dst\".format(i+40, i)] = test[col_40] - test[col]\n        train[\"{}_dst\/{}_dst\".format(i+40, i)] = train[col_40] \/ train[col]\n        test[\"{}_dst\/{}_dst\".format(i+40, i)] = test[col_40] \/ test[col]","2efe3ea7":"# \ud761\uc218\uc728 \uacf5\uc2dd \uc774\uc6a9 \uce7c\ub7fc\nfor i in range (650, 1000, 10) :\n    train['Absorption_' + str(i)] = (np.log(train[str(i) + '_src'] \/ train[str(i) + '_dst']) \/ train['rho'])\n    test['Absorption_' + str(i)] = (np.log(test[str(i) + '_src'] \/ test[str(i) + '_dst']) \/ test['rho'])\n    \n    train['Absorption_' + str(i)] = (np.exp(train[str(i) + '_src'] - train[str(i) + '_dst']) \/ train['rho'])\n    test['Absorption_' + str(i)] = (np.exp(test[str(i) + '_src'] - test[str(i) + '_dst']) \/ test['rho'])\n\ntrain.tail(3)","56d33a8f":"# rho\ub85c \ub098\ub220\uc8fc\ub294 \uce7c\ub7fc --> \uac70\ub9ac \ud45c\uc900\ud654\n\nall_data = pd.concat([train, test], sort=False) # \ub370\uc774\ud130 \ubb36\uc5b4\uc8fc\uae30\n\nall_data_dst = all_data.loc[:, '650_dst':'990_dst']\nall_data_dst['rho'] = all_data['rho']\nall_data_dst = all_data.iloc[:len(train), :]\n\nmedian = all_data_dst.groupby('rho').median().reset_index()\nmedian = median.drop(['rho'], axis=1)\nrho_median = median.mean(axis=1).reset_index()\n\nall_data['rho'] = all_data['rho'].replace(10, 1\/rho_median.iloc[0, 1])\nall_data['rho'] = all_data['rho'].replace(15, 1\/rho_median.iloc[1, 1])\nall_data['rho'] = all_data['rho'].replace(20, 1\/rho_median.iloc[2, 1])\nall_data['rho'] = all_data['rho'].replace(25, 1\/rho_median.iloc[3, 1])\n\nfor i in range(650, 1000, 10) :\n    all_data[str(i) + 'div'] = (all_data[str(i) + '_dst'] * all_data['rho']) \/ all_data[str(i) + '_src']\n    \nall_data.tail(3)","2ee2e629":"# \ub2e4\uc2dc \uc6d0\ub798 \ub370\uc774\ud130\ub85c\ntrain = all_data.iloc[:len(train), :]\ntest = all_data.iloc[len(train):, :].drop(['hhb', 'hbo2', 'ca', 'na'], axis=1)\ntrain.shape, test.shape","69b7b91b":"# data split\nX_train = train.drop(['id', 'hhb', 'hbo2', 'ca', 'na'], axis=1)\ny_train = train[['hhb', 'hbo2', 'ca', 'na']]\ntest = test.set_index('id')\n\ny_train_1 = y_train['hhb'].copy()\ny_train_2 = y_train['hbo2'].copy()\ny_train_3 = y_train['ca'].copy()\ny_train_4 = y_train['na'].copy()\n\nX_train.shape, test.shape, y_train.shape","9f5274fb":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n              (thour, tmin, round(tsec, 2)))","0335d86b":"# LGBM \ubaa8\ub378 \uc815\uc758\n# I actually used n_estimators 6000, 6000, 6000, 7000\nmodel_hhb = LGBMRegressor(random_state=12, num_leaves=110, n_estimators=600, learning_rate=0.05, boosting_type='dart',\n                           colsample_bytree=0.4, subsample=1, metric='regression_l1', boost_from_average=True)\n\nmodel_hbo2 = LGBMRegressor(random_state=13, num_leaves=110, n_estimators=600, learning_rate=0.05, boosting_type='dart',\n                           colsample_bytree=0.4, subsample=1, metric='regression_l1', boost_from_average=True)\n\nmodel_ca = LGBMRegressor(random_state=14, num_leaves=110, n_estimators=600, learning_rate=0.05, boosting_type='dart',\n                         colsample_bytree=0.4, subsample=1, metric='regression_l1', boost_from_average=True)\n\nmodel_na = LGBMRegressor(random_state=15, num_leaves=110, n_estimators=700, learning_rate=0.05, boosting_type='dart',\n                           colsample_bytree=0.5, subsample=1, metric='regression_l1', boost_from_average=True)","67f584ba":"# LGBM Cross Validation\n\nfolds = 10\n\nlgb_cv_sum_hhb = 0\nlgb_cv_sum_hbo2 = 0\nlgb_cv_sum_ca = 0\nlgb_cv_sum_na = 0\n\nlgb_pred_hhb = []\nlgb_pred_hbo2 = []\nlgb_pred_ca = []\nlgb_pred_na = []\n\nlgb_fpred_hhb = []\nlgb_fpred_hbo2 = []\nlgb_fpred_ca = []\nlgb_fpred_na = []\n\navreal = y_train\n\ntrain_time = timer(None)\n\nkf = KFold(n_splits=folds, random_state=95, shuffle=True)\n\nfor i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n    start_time = timer(None)\n    Xtrain, Xval = X_train.iloc[train_index], X_train.iloc[val_index]\n    ytrain, yval = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    print('\\n Fold %02d hhb' % ((i + 1)))\n    model_hhb.fit(Xtrain, ytrain.iloc[:, 0], eval_set=(Xval, yval.iloc[:, 0]), verbose=2000)\n    print('\\n Fold %02d hbo2' % ((i + 1)))\n    model_hbo2.fit(Xtrain, ytrain.iloc[:, 1], eval_set=(Xval, yval.iloc[:, 1]), verbose=2000)\n    print('\\n Fold %02d ca' % ((i + 1)))\n    model_ca.fit(Xtrain, ytrain.iloc[:, 2], eval_set=(Xval, yval.iloc[:, 2]), verbose=2000)\n    print('\\n Fold %02d na' % ((i + 1)))\n    model_na.fit(Xtrain, ytrain.iloc[:, 3], eval_set=(Xval, yval.iloc[:, 3]), verbose=2000)\n\n              \n    lgb_scores_val_hhb = model_hhb.predict(Xval)\n    lgb_scores_val_hbo2 = model_hbo2.predict(Xval)\n    lgb_scores_val_ca = model_ca.predict(Xval)\n    lgb_scores_val_na = model_na.predict(Xval)\n    \n    \n    lgb_MAE_hhb = mean_absolute_error(yval.iloc[:, 0], lgb_scores_val_hhb)\n    lgb_MAE_hbo2 = mean_absolute_error(yval.iloc[:, 1], lgb_scores_val_hbo2)\n    lgb_MAE_ca = mean_absolute_error(yval.iloc[:, 2], lgb_scores_val_ca)\n    lgb_MAE_na = mean_absolute_error(yval.iloc[:, 3], lgb_scores_val_na)\n    \n    lgb_MAE = np.mean([lgb_MAE_hhb, lgb_MAE_hbo2, lgb_MAE_ca, lgb_MAE_na])\n    \n    print('\\n Fold %02d LGBM MAE: %.6f' % ((i + 1), lgb_MAE))\n    \n    lgb_y_pred_hhb = model_hhb.predict(test)\n    lgb_y_pred_hbo2 = model_hbo2.predict(test)\n    lgb_y_pred_ca = model_ca.predict(test)\n    lgb_y_pred_na = model_na.predict(test)\n    \n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        lgb_fpred_hhb = lgb_pred_hhb + lgb_y_pred_hhb\n        lgb_fpred_hbo2 = lgb_pred_hbo2 + lgb_y_pred_hbo2\n        lgb_fpred_ca = lgb_pred_ca + lgb_y_pred_ca\n        lgb_fpred_na = lgb_pred_na + lgb_y_pred_na\n    else:\n        lgb_fpred_hhb = lgb_y_pred_hhb\n        lgb_fpred_hbo2 = lgb_y_pred_hbo2\n        lgb_fpred_ca = lgb_y_pred_ca\n        lgb_fpred_na = lgb_y_pred_na\n    lgb_pred_hhb = lgb_fpred_hhb\n    lgb_pred_hbo2 = lgb_fpred_hbo2\n    lgb_pred_ca = lgb_fpred_ca\n    lgb_pred_na = lgb_fpred_na\n    \n    lgb_cv_sum_hhb = lgb_cv_sum_hhb + lgb_MAE_hhb\n    lgb_cv_sum_hbo2 = lgb_cv_sum_hbo2 + lgb_MAE_hbo2\n    lgb_cv_sum_ca = lgb_cv_sum_ca + lgb_MAE_ca\n    lgb_cv_sum_na = lgb_cv_sum_na + lgb_MAE_na\n\ntimer(train_time)\n\nlgb_cv_score = (np.mean([lgb_cv_sum_hhb, lgb_cv_sum_hbo2, lgb_cv_sum_ca, lgb_cv_sum_na]) \/ folds)\n\nprint('\\n Average LGBM MAE:\\t%.6f' % lgb_cv_score)\nlgb_score = round(lgb_cv_score, 6)\n\nlgb_mpred_hhb = lgb_pred_hhb \/ folds\nlgb_mpred_hbo2 = lgb_pred_hbo2 \/ folds\nlgb_mpred_ca = lgb_pred_ca \/ folds\nlgb_mpred_na = lgb_pred_na \/ folds","28864fbe":"# XGBoost \ubaa8\ub378 \uc815\uc758\n# I actually used n_estimators 20000, 20000, 20000, 22000\nmodel_hhb = XGBRegressor(random_state=25, max_depth=5, n_estimators=200, learning_rate=0.05, booster='dart', tree_method='gpu_hist',\n                           colsample_bytree=0.4, subsample=1, metric='regression_l1', boost_from_average=True)\n\nmodel_hbo2 = XGBRegressor(random_state=27, max_depth=5, n_estimators=200, learning_rate=0.05, booster='dart', tree_method='gpu_hist',\n                           colsample_bytree=0.4, subsample=1, metric='regression_l1', boost_from_average=True)\n\nmodel_ca = XGBRegressor(random_state=21, max_depth=5, n_estimators=200, learning_rate=0.05, booster='dart', tree_method='gpu_hist',\n                         colsample_bytree=0.4, subsample=1, metric='regression_l1', boost_from_average=True)\n\nmodel_na = XGBRegressor(random_state=91, max_depth=5, n_estimators=220, learning_rate=0.05, booster='dart', tree_method='gpu_hist',\n                           colsample_bytree=0.5, subsample=1, metric='regression_l1', boost_from_average=True)","0ec20b22":"# XGB Cross Validation\n\nfolds = 10\n\nxgb_cv_sum_hhb = 0\nxgb_cv_sum_hbo2 = 0\nxgb_cv_sum_ca = 0\nxgb_cv_sum_na = 0\n\nxgb_pred_hhb = []\nxgb_pred_hbo2 = []\nxgb_pred_ca = []\nxgb_pred_na = []\n\nxgb_fpred_hhb = []\nxgb_fpred_hbo2 = []\nxgb_fpred_ca = []\nxgb_fpred_na = []\n\navreal = y_train\n\ntrain_time = timer(None)\n\nkf = KFold(n_splits=folds, random_state=13, shuffle=True)\n\nfor i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n    start_time = timer(None)\n    Xtrain, Xval = X_train.iloc[train_index], X_train.iloc[val_index]\n    ytrain, yval = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    print('\\n Fold %02d hhb' % ((i + 1)))\n    model_hhb.fit(Xtrain, ytrain.iloc[:, 0], eval_set=[(Xval, yval.iloc[:, 0])], eval_metric='mae',\n              early_stopping_rounds=500, verbose=5000)\n    print('\\n Fold %02d hbo2' % ((i + 1)))\n    model_hbo2.fit(Xtrain, ytrain.iloc[:, 1], eval_set=[(Xval, yval.iloc[:, 1])], eval_metric='mae',\n              early_stopping_rounds=500, verbose=5000)\n    print('\\n Fold %02d ca' % ((i + 1)))\n    model_ca.fit(Xtrain, ytrain.iloc[:, 2], eval_set=[(Xval, yval.iloc[:, 2])], eval_metric='mae',\n              early_stopping_rounds=500, verbose=5000)\n    print('\\n Fold %02d na' % ((i + 1)))\n    model_na.fit(Xtrain, ytrain.iloc[:, 3], eval_set=[(Xval, yval.iloc[:, 3])], eval_metric='mae',\n              early_stopping_rounds=500, verbose=5000)\n\n              \n    xgb_scores_val_hhb = model_hhb.predict(Xval)\n    xgb_scores_val_hbo2 = model_hbo2.predict(Xval)\n    xgb_scores_val_ca = model_ca.predict(Xval)\n    xgb_scores_val_na = model_na.predict(Xval)\n    \n    \n    xgb_MAE_hhb = mean_absolute_error(yval.iloc[:, 0], xgb_scores_val_hhb)\n    xgb_MAE_hbo2 = mean_absolute_error(yval.iloc[:, 1], xgb_scores_val_hbo2)\n    xgb_MAE_ca = mean_absolute_error(yval.iloc[:, 2], xgb_scores_val_ca)\n    xgb_MAE_na = mean_absolute_error(yval.iloc[:, 3], xgb_scores_val_na)\n    \n    xgb_MAE = np.mean([xgb_MAE_hhb, xgb_MAE_hbo2, xgb_MAE_ca, xgb_MAE_na])\n    \n    print('\\n Fold %02d XGB MAE: %.6f' % ((i + 1), xgb_MAE))\n    \n    xgb_y_pred_hhb = model_hhb.predict(test)\n    xgb_y_pred_hbo2 = model_hbo2.predict(test)\n    xgb_y_pred_ca = model_ca.predict(test)\n    xgb_y_pred_na = model_na.predict(test)\n    \n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        xgb_fpred_hhb = xgb_pred_hhb + xgb_y_pred_hhb\n        xgb_fpred_hbo2 = xgb_pred_hbo2 + xgb_y_pred_hbo2\n        xgb_fpred_ca = xgb_pred_ca + xgb_y_pred_ca\n        xgb_fpred_na = xgb_pred_na + xgb_y_pred_na\n    else:\n        xgb_fpred_hhb = xgb_y_pred_hhb\n        xgb_fpred_hbo2 = xgb_y_pred_hbo2\n        xgb_fpred_ca = xgb_y_pred_ca\n        xgb_fpred_na = xgb_y_pred_na\n    xgb_pred_hhb = xgb_fpred_hhb\n    xgb_pred_hbo2 = xgb_fpred_hbo2\n    xgb_pred_ca = xgb_fpred_ca\n    xgb_pred_na = xgb_fpred_na\n    \n    xgb_cv_sum_hhb = xgb_cv_sum_hhb + xgb_MAE_hhb\n    xgb_cv_sum_hbo2 = xgb_cv_sum_hbo2 + xgb_MAE_hbo2\n    xgb_cv_sum_ca = xgb_cv_sum_ca + xgb_MAE_ca\n    xgb_cv_sum_na = xgb_cv_sum_na + xgb_MAE_na\n\ntimer(train_time)\n\nxgb_cv_score = (np.mean([xgb_cv_sum_hhb, xgb_cv_sum_hbo2, xgb_cv_sum_ca, xgb_cv_sum_na]) \/ folds)\n\nprint('\\n Average XGB MAE:\\t%.6f' % xgb_cv_score)\nxgb_score = round(xgb_cv_score, 6)\n\nxgb_mpred_hhb = xgb_pred_hhb \/ folds\nxgb_mpred_hbo2 = xgb_pred_hbo2 \/ folds\nxgb_mpred_ca = xgb_pred_ca \/ folds\nxgb_mpred_na = xgb_pred_na \/ folds","179aa95a":"# CatBoost \ubaa8\ub378 \uc815\uc758\n# I actually used n_estimators 20000, 20000, 20000, 22000\nmodel_hhb = CatBoostRegressor(random_state=95, learning_rate=0.03, iterations=200, verbose=5000, loss_function='MAE', task_type=\"GPU\", depth=6)\nmodel_hbo2 = CatBoostRegressor(random_state=96, learning_rate=0.03, iterations=200, verbose=5000, loss_function='MAE', task_type=\"GPU\", depth=6)\nmodel_ca = CatBoostRegressor(random_state=97, learning_rate=0.03, iterations=200, verbose=5000, loss_function='MAE', task_type=\"GPU\", depth=6)\nmodel_na = CatBoostRegressor(random_state=98, learning_rate=0.03, iterations=220, verbose=5000, loss_function='MAE', task_type=\"GPU\", depth=6)","c46c45be":"# Catboost Cross Validation\n\nfolds = 10\n\ncat_cv_sum_hhb = 0\ncat_cv_sum_hbo2 = 0\ncat_cv_sum_ca = 0\ncat_cv_sum_na = 0\n\ncat_pred_hhb = []\ncat_pred_hbo2 = []\ncat_pred_ca = []\ncat_pred_na = []\n\ncat_fpred_hhb = []\ncat_fpred_hbo2 = []\ncat_fpred_ca = []\ncat_fpred_na = []\n\navreal = y_train\n\ntrain_time = timer(None)\n\nkf = KFold(n_splits=folds, random_state=95, shuffle=True)\n\nfor i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n    start_time = timer(None)\n    Xtrain, Xval = X_train.iloc[train_index], X_train.iloc[val_index]\n    ytrain, yval = y_train.iloc[train_index], y_train.iloc[val_index]\n    \n    print('\\n Fold %02d hhb' % ((i + 1)))\n    model_hhb.fit(Xtrain, ytrain.iloc[:, 0], eval_set=(Xval, yval.iloc[:, 0]), early_stopping_rounds=500)\n    print('\\n Fold %02d hbo2' % ((i + 1)))\n    model_hbo2.fit(Xtrain, ytrain.iloc[:, 1], eval_set=(Xval, yval.iloc[:, 1]), early_stopping_rounds=500)\n    print('\\n Fold %02d ca' % ((i + 1)))\n    model_ca.fit(Xtrain, ytrain.iloc[:, 2], eval_set=(Xval, yval.iloc[:, 2]), early_stopping_rounds=500)\n    print('\\n Fold %02d na' % ((i + 1)))\n    model_na.fit(Xtrain, ytrain.iloc[:, 3], eval_set=(Xval, yval.iloc[:, 3]), early_stopping_rounds=500)\n\n              \n    cat_scores_val_hhb = model_hhb.predict(Xval)\n    cat_scores_val_hbo2 = model_hbo2.predict(Xval)\n    cat_scores_val_ca = model_ca.predict(Xval)\n    cat_scores_val_na = model_na.predict(Xval)\n    \n    \n    cat_MAE_hhb = mean_absolute_error(yval.iloc[:, 0], cat_scores_val_hhb)\n    cat_MAE_hbo2 = mean_absolute_error(yval.iloc[:, 1], cat_scores_val_hbo2)\n    cat_MAE_ca = mean_absolute_error(yval.iloc[:, 2], cat_scores_val_ca)\n    cat_MAE_na = mean_absolute_error(yval.iloc[:, 3], cat_scores_val_na)\n    \n    cat_MAE = np.mean([cat_MAE_hhb, cat_MAE_hbo2, cat_MAE_ca, cat_MAE_na])\n    \n    print('\\n Fold %02d CatBoost MAE: %.6f' % ((i + 1), cat_MAE))\n    \n    cat_y_pred_hhb = model_hhb.predict(test)\n    cat_y_pred_hbo2 = model_hbo2.predict(test)\n    cat_y_pred_ca = model_ca.predict(test)\n    cat_y_pred_na = model_na.predict(test)\n    \n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        cat_fpred_hhb = cat_pred_hhb + cat_y_pred_hhb\n        cat_fpred_hbo2 = cat_pred_hbo2 + cat_y_pred_hbo2\n        cat_fpred_ca = cat_pred_ca + cat_y_pred_ca\n        cat_fpred_na = cat_pred_na + cat_y_pred_na\n    else:\n        cat_fpred_hhb = cat_y_pred_hhb\n        cat_fpred_hbo2 = cat_y_pred_hbo2\n        cat_fpred_ca = cat_y_pred_ca\n        cat_fpred_na = cat_y_pred_na\n    cat_pred_hhb = cat_fpred_hhb\n    cat_pred_hbo2 = cat_fpred_hbo2\n    cat_pred_ca = cat_fpred_ca\n    cat_pred_na = cat_fpred_na\n    \n    cat_cv_sum_hhb = cat_cv_sum_hhb + cat_MAE_hhb\n    cat_cv_sum_hbo2 = cat_cv_sum_hbo2 + cat_MAE_hbo2\n    cat_cv_sum_ca = cat_cv_sum_ca + cat_MAE_ca\n    cat_cv_sum_na = cat_cv_sum_na + cat_MAE_na\n\ntimer(train_time)\n\ncat_cv_score = (np.mean([cat_cv_sum_hhb, cat_cv_sum_hbo2, cat_cv_sum_ca, cat_cv_sum_na]) \/ folds)\n\nprint('\\n Average CatBoost MAE:\\t%.6f' % cat_cv_score)\ncat_score = round(cat_cv_score, 6)\n\ncat_mpred_hhb = cat_pred_hhb \/ folds\ncat_mpred_hbo2 = cat_pred_hbo2 \/ folds\ncat_mpred_ca = cat_pred_ca \/ folds\ncat_mpred_na = cat_pred_na \/ folds","a58fe2c7":"sub.head(2)","8558c652":"# lightgbm\nsub_lgb = sub.copy()\nsub_lgb['hhb'] = lgb_mpred_hhb\nsub_lgb['hbo2'] = lgb_mpred_hbo2\nsub_lgb['ca'] = lgb_mpred_ca\nsub_lgb['na'] = lgb_mpred_na\nsub_lgb.head(3)","a8e94411":"# xgboost\nsub_xgb = sub.copy()\nsub_xgb['hhb'] = xgb_mpred_hhb\nsub_xgb['hbo2'] = xgb_mpred_hbo2\nsub_xgb['ca'] = xgb_mpred_ca\nsub_xgb['na'] = xgb_mpred_na\nsub_xgb.head(3)","7660014f":"# catboost\nsub_cat = sub.copy()\nsub_cat['hhb'] = cat_mpred_hhb\nsub_cat['hbo2'] = cat_mpred_hbo2\nsub_cat['ca'] = cat_mpred_ca\nsub_cat['na'] = cat_mpred_na\nsub_cat.head(3)","82ce2cec":"# Blending - lgb + xgb + cat\nsub.iloc[:, 1:] = 0.4*sub_lgb.iloc[:, 1:] + 0.4*sub_xgb.iloc[:, 1:] + 0.2*sub_cat.iloc[:, 1:]\nsub.head()","5909081f":"sub.to_csv('Submission_Dacon_bio_Ensemble_lgb_xgb_cat.csv', index=False)","a2b10b9e":"## EDA","54751883":"## Preprocessing & Feature Engineering","1582c524":"id : \uad6c\ubd84\uc790\n\nrho : \uce21\uc815 \uac70\ub9ac (\ub2e8\uc704: mm)\n\nsrc : \uad11\uc6d0 \uc2a4\ud399\ud2b8\ub7fc (650 nm ~ 990 nm)\n\ndst : \uce21\uc815 \uc2a4\ud399\ud2b8\ub7fc (650 nm ~ 990 nm)\n\nhhb : \ub514\uc625\uc2dc\ud5e4\ubaa8\uae00\ub85c\ube48 \ub18d\ub3c4\n\nhbo2 : \uc625\uc2dc\ud5e4\ubaa8\uae00\ub85c\ube48 \ub18d\ub3c4\n\nca : \uce7c\uc298 \ub18d\ub3c4\n\nna : \ub098\ud2b8\ub968 \ub18d\ub3c4","9e7c9cc5":"## Data Load","b102a03f":"## Modeling & KFold","7b331787":"I participated in 'Bio Optical Data Analysis AI Competition' held by DACON, a Korean data science\/machine learning competition platform with this code.\n\nUsing this code, I was able to get in the top 23.\n\nA description of the data and competition rules are attached to the address of Dacon below.\n\nI'm still a student studying machine learning, so there may be some inefficient code. I hope you understand that.\n\nThank you.\n\nAddress: https:\/\/dacon.io\/competitions\/official\/235608\/overview\/","7925b60b":"## Ensemble"}}