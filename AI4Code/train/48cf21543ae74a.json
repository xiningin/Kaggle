{"cell_type":{"34a0c532":"code","688c07b7":"code","d42b4e11":"code","1e74c6b4":"code","fd3b3463":"code","fbd41c23":"code","9c13e661":"code","287ae42c":"code","44d7ae1f":"code","8e23f288":"code","90f80f36":"code","b22eb50b":"code","0473efc5":"code","3f7941cd":"code","c30d783e":"code","7fb706c2":"code","abfbd18d":"code","e932af29":"markdown","bd630641":"markdown","9f6af1ed":"markdown","50fb5634":"markdown","1a8dd8c0":"markdown","38003296":"markdown","7d76bbd6":"markdown"},"source":{"34a0c532":"!pip install ..\/input\/nfl-lib\/timm-0.1.26-py3-none-any.whl\n!tar xfz ..\/input\/nfl-lib\/pkgs.tgz\n# for pytorch1.6\ncmd = \"sed -i -e 's\/ \\\/ \/ \\\/\\\/ \/' timm-efficientdet-pytorch\/effdet\/bench.py\"\n!$cmd","688c07b7":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\nfrom tqdm import tqdm\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)","d42b4e11":"video_labels = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/train_labels.csv').fillna(0)\nvideo_labels_with_impact = video_labels[video_labels['impact'] > 0]\nfor row in tqdm(video_labels_with_impact[['video','frame','label']].values):\n    frames = np.array([-4,-3,-2,-1,1,2,3,4])+row[1]\n    video_labels.loc[(video_labels['video'] == row[0]) \n                                 & (video_labels['frame'].isin(frames))\n                                 & (video_labels['label'] == row[2]), 'impact'] = 1\nvideo_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\nvideo_labels = video_labels[video_labels.groupby('image_name')['impact'].transform(\"sum\") > 0].reset_index(drop=True)\nvideo_labels['impact'] = video_labels['impact'].astype(int)+1\nvideo_labels['x'] = video_labels['left']\nvideo_labels['y'] = video_labels['top']\nvideo_labels['w'] = video_labels['width']\nvideo_labels['h'] = video_labels['height']\nvideo_labels.head()","1e74c6b4":"'''\nnp.random.seed(0)\nvideo_names = np.random.permutation(video_labels.video.unique())\nvalid_video_len = int(len(video_names)*0.2)\nvideo_valid = video_names[:valid_video_len]\nvideo_train = video_names[valid_video_len:]\nimages_valid = video_labels[ video_labels.video.isin(video_valid)].image_name.unique()\nimages_train = video_labels[~video_labels.video.isin(video_valid)].image_name.unique()\nimages_all = video_labels[ video_labels.video.isin(video_names)].image_name.unique()\n'''","fd3b3463":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ndf_folds = video_labels[['image_name']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\ndf_folds = df_folds.groupby('image_name').count()\ndf_folds.loc[:, 'video'] = video_labels[['image_name', 'video']].groupby('image_name').min()['video']\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['video'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x \/\/ 20}').values.astype(str),\n)\n\ndf_folds.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","fbd41c23":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact > 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact > 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","9c13e661":"uniq_video = video_labels.video.unique()\nvideo_dir = '\/kaggle\/input\/nfl-impact-detection\/train'\nout_dir = 'train_images'\n!mkdir -p $out_dir\nfor video_name in uniq_video:\n    mk_images(video_name, video_labels, video_dir, out_dir)","287ae42c":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.RandomSizedCrop(min_max_height=(500, 720), height=720, width=720, p=0.5),\n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n                                     val_shift_limit=0.2, p=0.9),\n                A.RandomBrightnessContrast(brightness_limit=0.2, \n                                           contrast_limit=0.2, p=0.9),\n            ], p=0.9),\n            A.ToGray(p=0.01),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Transpose(p=0.5),\n            A.JpegCompression(quality_lower=85, quality_upper=95, p=0.2),\n            A.OneOf([\n                A.Blur(blur_limit=3, p=1.0),\n                A.MedianBlur(blur_limit=3, p=1.0)\n            ],p=0.1),\n            A.Resize(height=512, width=512, p=1),\n            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )","44d7ae1f":"TRAIN_ROOT_PATH = 'train_images'\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        if self.test or random.random() > 0.33:\n            image, boxes, labels = self.load_image_and_boxes(index)\n        elif random.random() > 0.5:\n            image, boxes, labels = self.load_cutmix_image_and_boxes(index)\n        else:\n            image, boxes, labels = self.load_mixup_image_and_boxes(index)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    break\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        #print(f'{TRAIN_ROOT_PATH}\/{image_id}')\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}\/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        records = self.marking[self.marking['image_name'] == image_id]\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        labels = records['impact'].values\n        return image, boxes, labels\n    \n    def load_mixup_image_and_boxes(self, index):\n        image, boxes, labels = self.load_image_and_boxes(index)\n        r_image, r_boxes, r_labels = self.load_image_and_boxes(random.randint(0, self.image_ids.shape[0] - 1))\n        return (image+r_image)\/2, np.vstack((boxes, r_boxes)).astype(np.int32), np.concatenate((labels, r_labels))\n    \n\n    def load_cutmix_image_and_boxes(self, index, imsize=720):\n        \"\"\" \n        This implementation of cutmix author:  https:\/\/www.kaggle.com\/nvnnghia \n        Refactoring and adaptation: https:\/\/www.kaggle.com\/shonenkov\n        \"\"\"\n        w, h = imsize, imsize\n        s = imsize \/\/ 2\n    \n        xc, yc = [int(random.uniform(imsize * 0.25, imsize * 0.75)) for _ in range(2)]  # center x, y\n        indexes = [index] + [random.randint(0, self.image_ids.shape[0] - 1) for _ in range(3)]\n\n        result_image = np.full((imsize, imsize, 3), 1, dtype=np.float32)\n        result_boxes = []\n        result_labels = np.array([], dtype=np.int)\n\n        for i, index in enumerate(indexes):\n            image, boxes, labels = self.load_image_and_boxes(index)\n            if i == 0:\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif i == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif i == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, max(xc, w), min(y2a - y1a, h)\n            elif i == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n            result_image[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            padw = x1a - x1b\n            padh = y1a - y1b\n\n            boxes[:, 0] += padw\n            boxes[:, 1] += padh\n            boxes[:, 2] += padw\n            boxes[:, 3] += padh\n\n            result_boxes.append(boxes)\n            result_labels = np.concatenate((result_labels, labels))\n\n        result_boxes = np.concatenate(result_boxes, 0)\n        np.clip(result_boxes[:, 0:], 0, 2 * s, out=result_boxes[:, 0:])\n        result_boxes = result_boxes.astype(np.int32)\n        index_to_use = np.where((result_boxes[:,2]-result_boxes[:,0])*(result_boxes[:,3]-result_boxes[:,1]) > 0)\n        result_boxes = result_boxes[index_to_use]\n        result_labels = result_labels[index_to_use]\n        \n        return result_image, result_boxes, result_labels\n    ","8e23f288":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n    marking=video_labels,\n    transforms=get_train_transforms(),\n    test=False,\n)\n\nvalidation_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n    marking=video_labels,\n    transforms=get_valid_transforms(),\n    test=True,\n)","90f80f36":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","b22eb50b":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'.\/{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}\/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader, fold):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}\/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                print('Save the model')\n                self.save(f'{self.base_dir}\/best-checkpoint-{str(self.epoch).zfill(3)}epoch-fold{str(fold)}.bin')\n                #for path in sorted(glob(f'{self.base_dir}\/best-checkpoint-*epoch.bin'))[:-3]:\n                #    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}\/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n\n                loss, _, _ = self.model(images, boxes, labels)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}\/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n\n            self.optimizer.zero_grad()\n            \n            loss, _, _ = self.model(images, boxes, labels)\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","0473efc5":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 4 \n    n_epochs = 1\n    lr = 0.0002\n    folder = 'effdet5-models'\n    verbose = True\n    verbose_step = 1\n    step_scheduler = False\n    validation_scheduler = True\n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )","3f7941cd":"def get_net():\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    checkpoint = torch.load('..\/input\/nfl-models\/efficientdet_d5-ef44aea8.pth')\n    net.load_state_dict(checkpoint)\n    config.num_classes = 2\n    config.image_size = 512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    return DetBenchTrain(net, config)\n\n","c30d783e":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_training(num_fold):\n    seed_everything(2003)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    #device = torch.device('cuda:0')\n    \n    \n    for fold_number in range(num_fold):\n        if fold_number != 0:\n            continue\n        print('Fold: {}'.format(fold_number + 1))\n\n        train_dataset = DatasetRetriever(\n            image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n            marking=video_labels,\n            transforms=get_train_transforms(),\n            test=False,\n        )\n\n        validation_dataset = DatasetRetriever(\n            image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n            marking=video_labels,\n            transforms=get_valid_transforms(),\n            test=True,\n        )\n\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=TrainGlobalConfig.batch_size,\n            sampler=RandomSampler(train_dataset),\n            pin_memory=False,\n            drop_last=True,\n            num_workers=TrainGlobalConfig.num_workers,\n            collate_fn=collate_fn,\n        )\n        val_loader = torch.utils.data.DataLoader(\n            validation_dataset, \n            batch_size=TrainGlobalConfig.batch_size,\n            num_workers=TrainGlobalConfig.num_workers,\n            shuffle=False,\n            sampler=SequentialSampler(validation_dataset),\n            pin_memory=False,\n            collate_fn=collate_fn,\n        )\n        \n        net = get_net()\n        net.to(device)\n\n        fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n        fitter.fit(train_loader, val_loader, fold_number)","7fb706c2":"run_training(num_fold=5)","abfbd18d":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n# !mv * \/tmp\/","e932af29":"## Albumentations","bd630641":"# Data Preparation","9f6af1ed":"# Stratified K-Fold","50fb5634":"## Fitter","1a8dd8c0":"# Create Images from Videos","38003296":"# EfficientDet [Train] Mixup + Cutmix + Stratified K-Fold\nThis notebook is mainly based on [2Class Object Detection Training](https:\/\/www.kaggle.com\/its7171\/2class-object-detection-training), with addition to mixup, cutmix, and stratified k-fold capability. Some winners in the previous object detection competition ([Global Wheat Detection](https:\/\/www.kaggle.com\/c\/global-wheat-detection)) use the combination of mixup, cutmix and stratified k-fold in their solution. So I guess, these tools would be useful in this competition as well. The implementation of cutmix and mixup is based on this [kernel](https:\/\/www.kaggle.com\/kaushal2896\/data-augmentation-tutorial-basic-cutout-mixup). \n\n## Please kindly upvote this kernel if you find it useful","7d76bbd6":"## Dataset"}}