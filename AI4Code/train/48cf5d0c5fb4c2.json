{"cell_type":{"0c8da605":"code","e0fc4fe4":"code","eb2dff99":"code","fb3c765f":"code","00541cb1":"code","53910104":"code","779f2283":"code","1aa91d37":"code","cdbda3a2":"code","3d68dbc3":"code","bbfefd6a":"code","9d4328d1":"code","5fee23cf":"code","986e5364":"code","1e5cead1":"code","454ca088":"code","ba1c81e6":"code","23a486b3":"code","ef68ce5c":"code","c4143374":"code","9de5e895":"code","d7eceb98":"code","c5844818":"code","ee5e399b":"code","15162242":"code","93c34c82":"code","d703e510":"code","a00f5c45":"code","a8a0e585":"code","79da4041":"code","181e0623":"code","933be960":"code","0bb16345":"code","68515a9e":"code","d1ec5565":"code","523d6e9a":"code","eece1fd6":"code","9e8c8e9a":"code","2dea964d":"code","a442ebe1":"code","5ce4999a":"code","da754fbf":"code","c43287df":"code","fe785714":"code","3410bc19":"code","68eb8a12":"code","624ae27e":"code","f1f37977":"code","d57299c8":"code","4b382169":"code","94370028":"code","d688366e":"code","c23dd52f":"code","0e9bb6e9":"code","b6aa620b":"code","bbbeb444":"code","28e66d1b":"code","82bff1f2":"code","70c2e1c5":"code","d9b60d41":"code","c3db434a":"code","7bb60779":"code","ca67a279":"code","2c1941ec":"code","a35d255d":"code","425cb371":"code","a362cee1":"code","d21742b9":"code","f4eab03d":"code","cd5a56a4":"code","1238ca37":"code","af53ab72":"code","a3fde986":"code","9c796a49":"code","32b5a4d0":"code","353c3827":"code","47ed71a7":"code","a6e24874":"code","1ba16a4c":"code","72787e6f":"code","b01b84de":"code","cf324a58":"code","d2d2080a":"code","b911752d":"code","d1a68379":"code","c8153768":"code","5898139c":"code","dcf6f0b0":"code","516a9a7a":"code","c20566dd":"code","908a8ddd":"markdown","c7b943ba":"markdown","ac796905":"markdown","fb61c736":"markdown","452f7152":"markdown","a44dcc37":"markdown","02d2ee8d":"markdown","b1544964":"markdown"},"source":{"0c8da605":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0fc4fe4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport statsmodels.api as sm\nimport warnings\nimport missingno as msno\nfrom datetime import date\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import KNNImputer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.simplefilter(\"ignore\")","eb2dff99":"pd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","fb3c765f":"df_ = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf = df_.copy()\ndf.head()","00541cb1":"df.columns = [col.upper() for col in df.columns]","53910104":"def check_df(dataframe, head=5, tail = 5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head ######################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail ######################\")\n    print(dataframe.tail(tail))\n    print(\"##################### NA ########################\")\n    print(dataframe.isnull().sum())\n    print(\"################### Quantiles ###################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","779f2283":"check_df(df)","1aa91d37":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n    It gives the names of categorical, numerical and categorical but cardinal variables in the data set.\n    Note: Categorical variables with numerical appearance are also included in categorical variables.\n    Parameters\n    ------\n        dataframe: dataframe\n                The dataframe from which variable names are to be retrieved\n        cat_th: int, optional\n                Class threshold for numeric but categorical variables\n        car_th: int, optinal\n                Class threshold for categorical but cardinal variables\n    Returns\n    ------\n        cat_cols: list\n                Categorical variable list\n        num_cols: list\n                Numeric variable list\n        cat_but_car: list\n                Categorical view cardinal variable list\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = total number of variables\n        num_but_cat is inside cat_cols.\n        The sum of 3 lists with return is equal to the total number of variables: cat_cols + num_cols + cat_but_car = number of variables\n    \"\"\"\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","cdbda3a2":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","3d68dbc3":"cat_cols","bbfefd6a":"num_cols","9d4328d1":"num_cols = [col for col in num_cols if col not in \"PASSENGERID\"]","5fee23cf":"num_cols","986e5364":"def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","1e5cead1":"def check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False","454ca088":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","ba1c81e6":"def grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n        \n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index","23a486b3":"def cat_summary(dataframe, col_name, plot=False):\n    import pandas as pd\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()","ef68ce5c":"for col in cat_cols:\n    cat_summary(df, col, plot=True)","c4143374":"def num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()","9de5e895":"num_summary(df, num_cols, True)","d7eceb98":"def target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")","c5844818":"for col in num_cols:\n    target_summary_with_num(df, \"SURVIVED\", col)","ee5e399b":"df.corr()\nf, ax = plt.subplots(figsize=[18, 13])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap=\"magma\")\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","15162242":"df.isnull().sum()","93c34c82":"(df.isnull().sum() \/ df.shape[0] * 100).sort_values(ascending=False)","d703e510":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis = 1, keys = ['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns","a00f5c45":"missing_values_table(df)","a8a0e585":"msno.bar(df);\nplt.show()","79da4041":"def pairplot(dataset, target_column):\n    sns.set(style=\"ticks\")\n    sns.pairplot(dataset, hue=target_column)\n    plt.show()\n\npairplot(df, 'SURVIVED')","181e0623":"for col in num_cols:\n    print(col, check_outlier(df, col))","933be960":"for col in num_cols:\n    print(col, replace_with_thresholds(df, col))","0bb16345":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","68515a9e":"num_cols","d1ec5565":"num_cols = [col for col in num_cols if col not in \"PASSENGERID\"]","523d6e9a":"cat_cols","eece1fd6":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","9e8c8e9a":"binary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]","2dea964d":"binary_cols","a442ebe1":"for col in binary_cols:\n    label_encoder(df, col)","5ce4999a":"df.head()","da754fbf":"df[\"NEW_CABIN_BOOL\"] = df[\"CABIN\"].notnull().astype('int')","c43287df":"df[\"NEW_NAME_COUNT\"] = df[\"NAME\"].str.len()","fe785714":"df[\"NEW_NAME_WORD_COUNT\"] = df[\"NAME\"].apply(lambda x: len(str(x).split(\" \")))","3410bc19":"df[\"NEW_NAME_DR\"] = df[\"NAME\"].apply(lambda x: len([x for x in x.split() if x.startswith(\"Dr\")]))","68eb8a12":"df['NEW_TITLE'] = df.NAME.str.extract(' ([A-Za-z]+)\\.', expand=False)","624ae27e":"df[\"NEW_FAMILY_SIZE\"] = df[\"SIBSP\"] + df[\"PARCH\"] + 1","f1f37977":"df.loc[((df['SIBSP'] + df['PARCH']) > 0), \"NEW_IS_ALONE\"] = \"NO\"\ndf.loc[((df['SIBSP'] + df['PARCH']) == 0), \"NEW_IS_ALONE\"] = \"YES\"","d57299c8":"df.loc[(df['AGE'] < 18), 'NEW_AGE_CAT'] = 'young'\ndf.loc[(df['AGE'] >= 18) & (df['AGE'] < 56), 'NEW_AGE_CAT'] = 'mature'\ndf.loc[(df['AGE'] >= 56), 'NEW_AGE_CAT'] = 'senior'","4b382169":"df.loc[(df['SEX'] == 'male') & (df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'youngmale'\ndf.loc[(df['SEX'] == 'male') & ((df['AGE'] > 21) & (df['AGE']) <= 50), 'NEW_SEX_CAT'] = 'maturemale'","94370028":"df.loc[(df['SEX'] == 'male') & (df['AGE'] > 50), 'NEW_SEX_CAT'] = 'seniormale'\ndf.loc[(df['SEX'] == 'female') & (df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'youngfemale'\ndf.loc[(df['SEX'] == 'female') & ((df['AGE'] > 21) & (df['AGE']) <= 50), 'NEW_SEX_CAT'] = 'maturefemale'\ndf.loc[(df['SEX'] == 'female') & (df['AGE'] > 50), 'NEW_SEX_CAT'] = 'seniorfemale'","d688366e":"df.head()","c23dd52f":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","0e9bb6e9":"num_cols = [col for col in num_cols if \"PASSENGERID\" not in col]","b6aa620b":"for col in num_cols:\n    print(col, check_outlier(df, col))","bbbeb444":"for col in num_cols:\n    replace_with_thresholds(df, col)","28e66d1b":"for col in num_cols:\n    print(col, check_outlier(df, col))","82bff1f2":"missing_values_table(df)","70c2e1c5":"df.drop(\"CABIN\", inplace=True, axis=1)","d9b60d41":"remove_cols = [\"TICKET\", \"NAME\"]","c3db434a":"df.drop(remove_cols, inplace=True, axis=1)","7bb60779":"df[\"AGE\"] = df[\"AGE\"].fillna(df.groupby(\"NEW_TITLE\")[\"AGE\"].transform(\"median\"))","ca67a279":"df[\"NEW_AGE_PCLASS\"] = df[\"AGE\"] * df[\"PCLASS\"]","2c1941ec":"df.loc[(df['AGE'] < 18), 'NEW_AGE_CAT'] = 'young'\ndf.loc[(df['AGE'] >= 18) & (df['AGE'] < 56), 'NEW_AGE_CAT'] = 'mature'\ndf.loc[(df['AGE'] >= 56), 'NEW_AGE_CAT'] = 'senior'","a35d255d":"df.loc[(df['SEX'] == 'male') & (df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'youngmale'\ndf.loc[(df['SEX'] == 'male') & ((df['AGE'] > 21) & (df['AGE']) <= 50), 'NEW_SEX_CAT'] = 'maturemale'\ndf.loc[(df['SEX'] == 'male') & (df['AGE'] > 50), 'NEW_SEX_CAT'] = 'seniormale'","425cb371":"df.loc[(df['SEX'] == 'female') & (df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'youngfemale'\ndf.loc[(df['SEX'] == 'female') & ((df['AGE'] > 21) & (df['AGE']) <= 50), 'NEW_SEX_CAT'] = 'maturefemale'\ndf.loc[(df['SEX'] == 'female') & (df['AGE'] > 50), 'NEW_SEX_CAT'] = 'seniorfemale'","a362cee1":"binary_cols","d21742b9":"for col in binary_cols:\n    df = label_encoder(df, col)","f4eab03d":"df.head()","cd5a56a4":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=False)\n    return dataframe","1238ca37":"cats_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]","af53ab72":"cat_cols","a3fde986":"df = one_hot_encoder(df, cats_cols)","9c796a49":"df.head()","32b5a4d0":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","353c3827":"num_cols = [col for col in num_cols if \"PASSENGERID\" not in col]","47ed71a7":"useless_cols = [col for col in df.columns if df[col].nunique() == 2 and\n                (df[col].value_counts() \/ len(df) < 0.01).any(axis=None)]","a6e24874":"useless_cols","1ba16a4c":"df.drop(useless_cols, axis=1, inplace=True)","72787e6f":"df.head()","b01b84de":"df.drop([\"NEW_TITLE\", \"NEW_IS_ALONE\", \"NEW_SEX_CAT\"], axis=1, inplace=True)","cf324a58":"scaler = RobustScaler()","d2d2080a":"df[num_cols] = scaler.fit_transform(df[num_cols])\ndf[num_cols].head()","b911752d":"y = df[\"SURVIVED\"]\nX = df.drop([\"PASSENGERID\", \"SURVIVED\"], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)","d1a68379":"reg_model = LinearRegression().fit(X_train, y_train)\nreg_model.intercept_\nreg_model.coef_\ny_pred = reg_model.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred))\nreg_model.score(X_train, y_train)\nnp.mean(np.sqrt(-cross_val_score(reg_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","c8153768":"pre_model = LGBMRegressor().fit(X, y)\nfeature_imp = pd.DataFrame({'Feature': X.columns, 'Value': pre_model.feature_importances_})\nfeature_imp.sort_values(\"Value\", ascending=False)","5898139c":"pre_model = RandomForestRegressor().fit(X, y)\nfeature_imp = pd.DataFrame({'Feature': X.columns, 'Value': pre_model.feature_importances_})\nfeature_imp.sort_values(\"Value\", ascending=False)","dcf6f0b0":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          ]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","516a9a7a":"rf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\naccuracy_score(y_pred, y_test)","c20566dd":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                      ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(rf_model, X_train)","908a8ddd":"To summarize this function, first the categorical variables were selected, then the numerical but categorical ones were selected. At the beginning of the run, **cat_th** was specified. This means categorical threshold. Even if a variable is numeric if it has less than 10 classes, that variable is a categorical variable for us. In other words, if the number of classes in a variable is less than 10, it can be a categorical variable even if it is numeric. Then we set the cardinal threshold. If a categorical variable has more than 20 classes and its type is categorical, it appears categorical and is cardinal.\n\nWe build the **cat_cols** list from scratch. Here were categorical variables with a numeric look. We add these too. There were also high cardinal variables in **cat_cols**, we select categorical but non-cardinal variables and remove them.\n\n**num_cols** refers to numeric columns. Here, those whose type is different from object, namely int, float ones, will be included in **num_cols**. We will also subtract numeric and categorical ones.","c7b943ba":"Previously, we looked at whether there is a missing value in our data set, but now we take the output of the missing observations in the variables in our data set. We see that there are missing values in the variables \"**AGE**\", \"**CABIN**\" and \"**EMBARKED**\" and how many.","ac796905":"The **num_but_cat** variable is actually inside the **cat_cols** variable. Printed for reporting purposes.","fb61c736":"We quickly displayed the frequency of the deficiencies in the data set, the names of the variables with missing data, and the rate of these deficiencies.","452f7152":"If a categorical variable has two classes and these classes are encoded as 1-0, it is called binary encoding. If it has more than two classes, it is called label encoding. We call the LabelEncoder algorithm and assign it to a variable named \"**le**\". Then, we encode the categorical variable in the data set by saying transform the \"**SEX**\" variable in our data set to \"**le**\". When we forget which values we gave 1-0 in these conversion processes, we can see our values by using the \"**inverse_transform**\" method.","a44dcc37":"We define a function called \"**missing_values_table**\" for a first look at the missing data. Returns the variables with the names of missing values named \"**dataframe**\" in the defined argument of this function and \"**na_columns**\" in the second argument. In the first line, there is an argument in which the names of the variables with missing values in this data set are selected when the relevant data set is defined. Then we create an argument named \"**n_miss**\" to find the number of missing values. Under the \"**n_miss**\" argument, we define an argument named \"**ratio**\" that gives a ratio for missing values. Finally, we convert them into a dataset called \"**missing_df**\" and combine them with the \"**concat**\" operation.","02d2ee8d":"Since the **\"PASSENGERID\"** variable is numeric, it came to the **num_cols** variable. We can leave out the **\"PASSENGERID\"**.","b1544964":"If we define the above function, we have set the index information to **False** by default because we do not want the index information for now. grab_outliers is the first analysis function for us. If it is less than the lower limit or greater than the upper limit, bring them. If this place is greater than 10, get a head. The head argument can be given by default."}}