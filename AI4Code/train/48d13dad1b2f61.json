{"cell_type":{"11191809":"code","214d14b3":"code","14ecc708":"code","e55f1f34":"code","e13a3fca":"code","7bf7255d":"code","47a38907":"code","216ce2af":"code","dd2ea012":"code","50c4b4fc":"markdown","ab987172":"markdown","7c82c8cb":"markdown","a3f3eb64":"markdown","9fdee7a1":"markdown","f262ed4b":"markdown"},"source":{"11191809":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras_preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","214d14b3":"path = '..\/input\/final-flowers-course-project-dataset\/newFlowers'","14ecc708":"train_datagen = ImageDataGenerator(rescale=1.\/255, validation_split=0.2, rotation_range=40,\n                  width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,\n                  horizontal_flip=True, fill_mode='nearest')\n\ntrain_gen = train_datagen.flow_from_directory(path, target_size=(150,150),\n                class_mode='categorical',batch_size=126, subset='training')\n\nval_gen = train_datagen.flow_from_directory(path, target_size=(150,150),\n                class_mode='categorical', batch_size=126, subset='validation')","e55f1f34":"labels = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\nfor i in range(15):\n    if i%5==0:\n        fig, ax = plt.subplots(ncols=5, figsize=(15,15))\n    img, lbl = train_gen.next()\n    ax[i%5].imshow(img[2])\n    ax[i%5].set_title(labels[np.argmax(lbl[2])])\n    ax[i%5].grid(False)\n    ax[i%5].axis(False)    ","e13a3fca":"model = tf.keras.Sequential([\n\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3), padding='same'),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n#     tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n#     tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n#     tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.3),\n    \n#     tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n#     tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.3),\n    \n    \n#     tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n#     tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.3),\n    \n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Dense(5, activation='softmax')     \n])\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","7bf7255d":"model.summary()","47a38907":"history = model.fit(train_gen, epochs=100, validation_data=val_gen)","216ce2af":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.grid(axis='both')\n\nplt.show() ","dd2ea012":"# an outlier in this dataset. im telling you DO NOT RUN THIS CELL\n\npath = '..\/input\/flowers-recognition\/flowers\/daisy\/34500610132_9921740f71_n.jpg'\nimg = plt.imread(path)\nplt.imshow(img)","50c4b4fc":"Accuracy would have increased a little bit by increasing epochs but I guess we will never know.","ab987172":"(To be honest GPU option is a life saver.)","7c82c8cb":"NOTE: This kernel is originally based on the [Flowers Recognition](http:\/\/https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition). But this dataset had some bad data(some .py and .pyc files).\n\nKudos to [Rishit Chaudhary](http:\/\/https:\/\/www.kaggle.com\/rishitchs) who removed the bad data and some bad pictures as well. So I am using his [Cleaned Dataset](http:\/\/https:\/\/www.kaggle.com\/rishitchs\/final-flowers-course-project-dataset).\n\nBut OC [Alexander Mamaev](http:\/\/https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition) thank you very much for the awesome dataset","a3f3eb64":"# Reading and Preparing data","9fdee7a1":"# Modeling and Training","f262ed4b":"# Visualizing some of Data"}}