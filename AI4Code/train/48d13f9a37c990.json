{"cell_type":{"4e116397":"code","14731641":"code","a37887f0":"code","17da8339":"code","03915e97":"code","da10977f":"code","15e2d95f":"code","f38168a1":"code","e9d0e28c":"code","8bceebfd":"code","1c347c72":"markdown"},"source":{"4e116397":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14731641":"import time\nimport tensorflow as tf\nimport tensorflow_hub as hub","a37887f0":"def load_dataset(file_path, num_samples):\n    df = pd.read_csv(file_path, usecols=[6, 9], nrows=num_samples)\n    df.columns = ['rating', 'title']\n\n    text = df['title'].tolist()\n    text = [str(t).encode('ascii', 'replace') for t in text]\n    text = np.array(text, dtype=object)[:]\n    \n    labels = df['rating'].tolist()\n    labels = [1 if i>=4 else 0 if i==3 else -1 for i in labels]\n    labels = np.array(pd.get_dummies(labels), dtype=int)[:] \n\n    return labels, text\n    ","17da8339":"tmp_labels, tmp_text = load_dataset('..\/input\/amazon-fine-food-reviews\/Reviews.csv', 568454)\ntmp_text.shape","03915e97":"from sklearn.model_selection import train_test_split","da10977f":" X_train, X_test, y_train, y_test= train_test_split(tmp_text, tmp_labels, test_size = 0.4)","15e2d95f":"hub_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1\", output_shape=[50], \n                           input_shape=[], dtype=tf.string, name='input', trainable=False)\n\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(3, activation='softmax', name='output'))\nmodel.compile(loss='categorical_crossentropy',\n                  optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","f38168a1":"len(X_train), len(X_test)","e9d0e28c":"\nprint(\"Training the model ...\")\nhistory = model.fit(X_train, y_train, batch_size = 128, epochs=6, verbose=1,\n              validation_data=(X_test,y_test),)\n    ","8bceebfd":"model.save('model.h5')","1c347c72":"Here i use pretrained word embedding form tensflow hub. Here the embedding taked from tensorflow hub . the embedding text has 128 dimension and corpus is 700B. \nLink : ##https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim50\/1    |(Token based text embedding trained on English Google News 7B corpus.)\n         ##https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1   |(Token based text embedding trained on English Google News 200B corpus.)\n"}}