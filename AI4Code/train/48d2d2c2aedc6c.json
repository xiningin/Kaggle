{"cell_type":{"2f2ea0c4":"code","b587777c":"code","1b357ae8":"code","de2d5421":"code","4d988c68":"code","1ab2d489":"code","aa46ba1a":"code","48ea42b4":"code","833410eb":"code","d3991627":"code","ddca297e":"code","e291ec9f":"code","ff68671d":"code","cd6f6ee0":"code","455ec305":"code","221bac17":"code","ef0273e7":"code","1a664dc4":"code","9be30254":"code","be760ae6":"code","aca06b4c":"code","aaf64005":"code","75a23dc9":"code","90ec175e":"code","7a1c23d5":"code","6c5f3a4d":"code","a9e5bfb3":"code","5ffc7232":"code","9c17c581":"code","1d2b0532":"code","72060652":"code","e67eb535":"code","299c03be":"code","a2946b0f":"code","19aac590":"code","1a047c7a":"code","0c0e631f":"code","5fa14745":"code","5a95d49c":"code","c8a0f03c":"code","daf8f9f5":"code","9f24b572":"code","abb4e576":"code","342fa2d5":"code","a15e7d39":"code","26c1a50d":"code","b6443687":"code","bf38a4fa":"code","a14b907e":"code","6a9a378e":"code","a0429cae":"code","6721a988":"code","3a1d0abd":"code","bb88664a":"code","26bc5fea":"code","72d8bc3c":"code","924859a0":"code","b6c0a625":"code","3ba85b2c":"code","a65199fd":"code","630eed4f":"code","e4791ed6":"code","5fdab42f":"code","dbfef1c6":"code","b7c25d4c":"code","4055fccb":"code","4cfd5705":"code","c3bf7806":"code","c2794876":"code","d1ed8379":"code","39fe8f6b":"code","ffcf372c":"code","fdea363f":"code","c1e39785":"markdown","ceac3f08":"markdown","2c6554ae":"markdown","94b65dee":"markdown","d304e3ca":"markdown","f16754b7":"markdown","79b29604":"markdown","6c36d425":"markdown","c2f206de":"markdown","2d39deae":"markdown","9c3f0f84":"markdown","8b9c658e":"markdown","07eea73a":"markdown","0047a55a":"markdown","0996c33f":"markdown","dd03d76b":"markdown","e631f4d4":"markdown","3cfcd6ca":"markdown","d40c81ad":"markdown","d567c9b3":"markdown","5ec7c843":"markdown","53abceff":"markdown","00a6a3f7":"markdown","804f20e0":"markdown","83b92712":"markdown","0437af08":"markdown"},"source":{"2f2ea0c4":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# set the graphs to show in the jupyter notebook\n%matplotlib inline\n\n# set seabor graphs to a better style\nsns.set(style=\"ticks\")","b587777c":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1b357ae8":"# Uploading datasets i.e \"Customers\",\"Transaction\",\"Product Heirarchy\"\nCustomers = pd.read_csv(\"\/kaggle\/input\/retail-case-study-data\/Customer.csv\")\nTransaction = pd.read_csv(\"\/kaggle\/input\/retail-case-study-data\/Transactions.csv\")\nProduct_heirarchy = pd.read_csv(\"\/kaggle\/input\/retail-case-study-data\/prod_cat_info.csv\")","de2d5421":"Customers.shape","4d988c68":"Customers.head(10)","1ab2d489":"Transaction.shape","aa46ba1a":"Transaction.head()","48ea42b4":"Product_heirarchy.shape","833410eb":"Product_heirarchy.head()","d3991627":"#Renaming column \"prod_sub_cat_code\" to \"prod_subcat_code\" so that datasets merged easily Transaction and Product_heirarchy\nProduct_heirarchy.rename(columns = {\"prod_sub_cat_code\":\"prod_subcat_code\"},inplace = True)","ddca297e":"#Merging Datasets transaction and product_heirarchy with the help of left join and assigning it to variable Data\nData= pd.merge(left = Transaction,\n        right = Product_heirarchy,\n        on = [\"prod_cat_code\",\"prod_subcat_code\"],\n        how = \"left\",\n        )\nData","e291ec9f":"Data.isnull().sum()","ff68671d":"#Merging Datasets Data and Customers and assigning it to a final variable called Customer_Final\nCustomer_Final = pd.merge(left = Data,\n                         right = Customers,\n                         left_on = \"cust_id\",\n                         right_on = \"customer_Id\",\n                         how = \"left\")\nCustomer_Final","cd6f6ee0":"Customer_Final.head()","455ec305":"Customer_Final.shape","221bac17":"Transaction.shape","ef0273e7":"print('''Rows of both \"Customer_Final\" & \"Transaction\" are same. That means all transactions done are present\nin Customers_Final\n'''\n)","1a664dc4":"Customer_Final.dtypes","9be30254":"Customer_Final.isnull().sum()","be760ae6":"#Converting \"DOB\" and \"Tran_date\" dtype from object to dates\nCustomer_Final[\"DOB\"] = pd.to_datetime(Customer_Final[\"DOB\"],format = \"%d-%m-%Y\")\nCustomer_Final[\"tran_date\"] = pd.to_datetime(Customer_Final[\"tran_date\"])","aca06b4c":"Customer_Final[\"DOB\"].head()","aaf64005":"Customer_Final[\"tran_date\"].head()","75a23dc9":"#Checking For Duplicates\nCustomer_Final.duplicated().sum()","90ec175e":"#dropping duplicates\nCustomer_Final.drop_duplicates(inplace=True)","7a1c23d5":"Customer_Final.duplicated().sum()","6c5f3a4d":"#Column names and corresponding Datatypes\nCustomer_Final.columns","a9e5bfb3":"#Column names and corresponding Datatypes\nCustomer_Final.dtypes","5ffc7232":"#Top 10 observations\nCustomer_Final.head(10)","9c17c581":"#Bottom ten observations\nCustomer_Final.tail(10)","1d2b0532":"#using Customer_Final.describe() to describe the data where we can see count,mean,std,min,25%,50%,75%,max for continuous variables present in the data\nCustomer_Final.describe()","72060652":"#using quantile function to describe 0 = min , 0.25 = Q1 , 0.5 = Q2, 0.5 = median , 0.75 = Q3 , 1 = max\nquant = Customer_Final.quantile([0, 0.25, 0.5, 0.75, 1])\nQ1 = quant.loc[0.25]\nQ3 = quant.loc[0.75]\nMin = quant.loc[0]\nMax = quant.loc[1]\nMedian = quant.loc[0.5]","e67eb535":"Frequency_tables = Customer_Final.loc[:,Customer_Final.dtypes == \"object\"].describe()\nFrequency_tables","299c03be":"Continuos_variable = Customer_Final.loc[:,[\"prod_subcat_code\",\"prod_cat_code\",\"Qty\",\"Rate\",\"Tax\",\"total_amt\"]]","a2946b0f":"Continuos_variable.columns","19aac590":"for var in  Continuos_variable.columns:\n    Continuos_variable[var].plot(kind=\"hist\")\n    plt.title(var)\n    plt.show()","1a047c7a":"Categorical_variables = Customer_Final.loc[:,Customer_Final.dtypes == \"object\"]\nCategorical_variables","0c0e631f":"plt.figure(figsize=(8,8))\nsns.countplot(Categorical_variables[\"Gender\"])\nplt.show()","5fa14745":"plt.figure(figsize=(8,8))\nsns.countplot(Categorical_variables[\"Store_type\"])\nplt.xlabel(\"Store Type\")\nplt.show()","5a95d49c":"plt.figure(figsize=(8,8))\nsns.countplot(Categorical_variables[\"prod_cat\"])\nplt.xlabel(\"Product Category\")\nplt.show()","c8a0f03c":"plt.figure(figsize=(8,8))\nCategorical_variables.groupby(\"prod_subcat\")[\"prod_subcat\"].count().plot(kind = \"barh\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Product sub-category\")\nplt.show()","daf8f9f5":"Customer_Final.sort_values(by=\"tran_date\")","9f24b572":"Start_date = Customer_Final[\"tran_date\"].min()","abb4e576":"End_date = Customer_Final[\"tran_date\"].max()","342fa2d5":"print(\"Time period of the available transaction data is from \" + pd.Timestamp.strftime(Start_date,format = \"%d-%m-%Y\") + \" to \" + pd.Timestamp.strftime(End_date,format = \"%d-%m-%Y\"))","a15e7d39":"#Count of transactions where the total amount of transaction was negative\nnegative_transaction = Customer_Final.loc[Customer_Final[\"total_amt\"] < 0 , \"transaction_id\"].count()","26c1a50d":"print(\"Count of transactions where the total amount of transaction was negative is\" , negative_transaction)","b6443687":"#Groupby the dataset on the basis of \"Gender\" and \"prod_cat\"\nPopular_products = Customer_Final.groupby([\"Gender\",\"prod_cat\"])[[\"Qty\"]].sum().reset_index()\nPopular_products.pivot(index = \"Gender\",columns = \"prod_cat\",values = \"Qty\")","bf38a4fa":"Cust_Grp = Customer_Final.groupby([\"city_code\"])[\"customer_Id\"].count().sort_values(ascending = False)\nCust_Grp","a14b907e":"plt.figure(figsize = (8,5))\nCust_Grp.plot(kind = \"bar\")\nplt.xlabel(\"City Code\")\nplt.ylabel(\"No. of Customers\")\nplt.yticks(np.arange(0 , 3500, step = 500))\nplt.show()","6a9a378e":"Percentage = round((Cust_Grp[4.0]\/Cust_Grp.sum()) * 100,2)\nPercentage","a0429cae":"print(\"City code 4.0 has the maximum customers and the percentage of customers from the city is \",Percentage)","6721a988":"Customer_Final.groupby([\"Store_type\"])[\"Qty\",\"Rate\"].sum().sort_values(by=\"Qty\",ascending = False)","3a1d0abd":"print(\"e-Shop sells the maximum products by value and by quantity\")","bb88664a":"Store_group = round(Customer_Final.pivot_table(index = \"prod_cat\", columns = \"Store_type\", values = \"total_amt\",  aggfunc = \"sum\"),2)\nStore_group","26bc5fea":"#the total amount earned from the \"Electronics\" and \"Clothing\" categories from Flagship Stores\nTotal_amt = Store_group.loc[[\"Clothing\",\"Electronics\"],\"Flagship store\"].sum()\nprint(\"the total amount earned from the Electronics and Clothing categories from Flagship Stores is \", Total_amt)","72d8bc3c":"Gender_group = round(Customer_Final.pivot_table(index = \"prod_cat\", columns = \"Gender\", values = \"total_amt\",  aggfunc = \"sum\"),2)\nGender_group","924859a0":"Male_amt = Gender_group.loc[\"Electronics\",\"M\"].sum()\nMale_amt","b6c0a625":"print(\"the total amount earned from Males the Electronics category is \", Male_amt)","3ba85b2c":"#Creating a Datafram that does not have negative Transactions of customers\nPos_Trans= Customer_Final.loc[Customer_Final[\"total_amt\"]>0,:]\nPos_Trans","a65199fd":"# Creating a dataframe that contains unique possitive transactions\nUnique_Trans = Pos_Trans.groupby([\"customer_Id\",\"prod_cat\",\"prod_subcat\"])[\"transaction_id\"].count().reset_index()\nUnique_Trans","630eed4f":"# Now finding the customers having unique transactions greater than 10\nUnique_trans_count = Unique_Trans.groupby(\"customer_Id\")[\"transaction_id\"].count().reset_index()\nUnique_trans_count","e4791ed6":"Unique_trans_count[Unique_trans_count[\"transaction_id\"]>10]","5fdab42f":"print(\"There are no unique transactions greater than 10\")","dbfef1c6":"now = pd.Timestamp('now') \nCustomer_Final['DOB'] = pd.to_datetime(Customer_Final['DOB'], format='%m%d%y') # 1 \nCustomer_Final['DOB'] = Customer_Final['DOB'].where(Customer_Final['DOB'] < now, Customer_Final['DOB'] - np.timedelta64(100, 'Y')) # 2 \nCustomer_Final['AGE'] = (now - Customer_Final['DOB']).astype('<m8[Y]')","b7c25d4c":"Customer_Final['Age_cat'] = pd.cut(Customer_Final['AGE'],bins=[24,35,46,57],labels=['25-35','36-46','47-57'],include_lowest=True)","4055fccb":"Customer_Final","4cfd5705":"# grouping the dataframe 'customer_final' on the basis of 'Age_cat' and 'prod_cat' \nCustomer_25_35 = Customer_Final.groupby(['Age_cat','prod_cat'])['total_amt'].sum()\nCustomer_25_35","c3bf7806":"Customer_25_35.loc[\"25-35\" , [\"Books\" , \"Electronics\"]]","c2794876":"print(\"Total amount spent on 'Electronics' and 'Books' product categories is\", \n      Customer_25_35.loc['25-35',['Books','Electronics']].sum().round(2))","d1ed8379":"Customer_Final","39fe8f6b":"# filtering out data that belongs to the 'age_cat' = 25-35 \nCustomer_total_amount_25_35 = Customer_Final[Customer_Final['Age_cat']=='25-35']\nCustomer_total_amount_25_35","ffcf372c":"# getting all the data with transaction date between 1st Jan 2014 to 1st Mar 2014? \ntotal_amount = Customer_total_amount_25_35[(Customer_total_amount_25_35['tran_date'] >='2014-01-01') & (Customer_total_amount_25_35['tran_date'] <='2014-03-01')]\ntotal_amount","fdea363f":"print('The total amount spent by customers aged 25-35 between 1st Jan 2014 to 1st Mar 2014 is', total_amount['total_amt'].sum())","c1e39785":"#### b. Count of transactions where the total amount of transaction was negative","ceac3f08":"# Customer Analysis For Retail","2c6554ae":"### 11. For all customers aged between 25 - 35, find out:\n","94b65dee":"### 7. Which store type sells the maximum products by value and by quantity?","d304e3ca":"### 2. Prepare a summary report for the merged data set.","f16754b7":"### 8. What was the total amount earned from the \"Electronics\" and \"Clothing\" categories from Flagship Stores?\n","79b29604":"#### a. What was the total amount spent for \u201cElectronics\u201d and \u201cBooks\u201d product categories?","6c36d425":"## Import necessary libraries","c2f206de":"#### Bar chart for Categorical Variables","2d39deae":"### 4. Calculate the following information using the merged dataset :","9c3f0f84":"#### Products popular in Males are :\n- Books\n- Clothing\n- Electronics\n- Home and Kitchen\n \n#### Products popular in Females are :\n- Bags\n- Footwear\n","8b9c658e":"### 10. How many customers have more than 10 unique transactions, after removing all transactions which have any negative amounts?","07eea73a":"### 9. What was the total amount earned from \"Male\" customers under the \"Electronics\" category?\n","0047a55a":"### 1. Merge the datasets Customers, Product Hierarchy and Transactions as Customer_Final. Ensure to keep all customers who have done transactions with us and select the join type accordingly.","0996c33f":"## BUSINESS PROBLEM:\n### A Retail store is required to analyze the day-to-day transactions and keep a track of its customers spread across various locations along with their purchases\/returns across various categories.\n\n### Create a report and display the below calculated metrics, reports and inferences.\n","dd03d76b":"#### a. Time period of the available transaction data","e631f4d4":"### b. Top\/Bottom 10 observations","3cfcd6ca":"### b. What was the total amount spent by these customers between 1st Jan, 2014 to 1st Mar, 2014?","d40c81ad":"### 3. Generate histograms for all continuous variables and frequency bars for categorical variables.","d567c9b3":"### d. Frequency tables for all the categorical variables","5ec7c843":"### a. Get the column names and their corresponding data types ","53abceff":"#### as we have to deal with customers aged between 25-35, so creating new column 'Age_cat'","00a6a3f7":"#### Histograms for continuous variables","804f20e0":"### c. \u201cFive-number summary\u201d for continuous variables (min, Q1, median, Q3 and max)","83b92712":"### 6. Which City code has the maximum customers and what was the percentage of customers from that city?","0437af08":"### 5. Analyze which product categories are more popular among females vs male customers"}}