{"cell_type":{"18a767dd":"code","8b886f52":"code","f2b1c794":"code","4c46fdcf":"code","a70c5874":"code","dd965888":"code","bb0b6c7b":"code","64abb3dd":"code","2036df0e":"code","29b4db3f":"code","ac1ec4b3":"code","90b0ce6c":"code","e462f08e":"code","59d9a6ad":"code","9dcf1365":"code","95b02795":"code","00c7a6d5":"code","e0738e2a":"code","36179bd0":"code","408ebd38":"code","76210e69":"code","db22d9ec":"code","f6b4f16a":"code","ee3344f7":"code","5e8f4179":"code","ffad6552":"code","8a1f8d20":"code","541625f2":"code","e2093842":"code","1f086600":"code","7d352e5c":"code","9013627e":"code","3ac7f5f1":"code","9881f82f":"code","a79ed515":"code","13ee1693":"code","b2efb139":"code","2bd38895":"code","5e11d8bc":"code","aeba57b0":"code","e65b1be4":"code","9fc0cd42":"code","f4562026":"code","1b035947":"code","b611e1ad":"code","0925d85a":"code","424f393d":"code","6c82a8f2":"code","0743dafc":"code","0650cbe7":"code","8fb34ac0":"code","b14d31f4":"code","adf59e0d":"code","6583abf6":"markdown","3689512a":"markdown","f266f43e":"markdown","1f5d2f0a":"markdown","c3ff6a96":"markdown","2d93954f":"markdown","b96f754d":"markdown","cca60788":"markdown","b5293020":"markdown","0c1b8f5a":"markdown","2acd2d92":"markdown","ca60ff9d":"markdown","9ee4ba2e":"markdown","0a71499a":"markdown","b7202196":"markdown","9b0ea4c7":"markdown","dabbb7bf":"markdown","85c1112b":"markdown","0fe92eb5":"markdown","0a8d3797":"markdown","d156f142":"markdown"},"source":{"18a767dd":"from IPython.core.display import HTML\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nHTML(\n    \"\"\"\n    <style>\n        div.text_cell_render h1{\n            color: white;\n        }\n        h1 {\n           font-family: \"Arial Black\", sans-serif;\n           font-size: 1.5em;\n           letter-spacing: -1px;\n           background-color: black;\n           color: white;\n           text-align:center;\n         }\n    <\/style>\n    \"\"\"\n)","8b886f52":"# Check the Hardware\n\n!nvidia-smi","f2b1c794":"# Setup\n\nimport sys\n\n!cp ..\/input\/rapids\/rapids.0.17.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n!rm \/opt\/conda\/envs\/rapids.tar.gz\n\nsys.path += [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"]\nsys.path += [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"]\nsys.path += [\"\/opt\/conda\/envs\/rapids\/lib\"]\n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","4c46fdcf":"# importing\n\nimport os\nimport math\nimport numpy as np\nnp.random.seed(42) # why not 42?\n\nimport pandas as pd; print(\"Pandas Version: \", pd.__version__)\nimport cudf; print(\"cuDF Version: \", cudf.__version__)","a70c5874":"# DataFrame creation using Pandas\n\ndf = pd.DataFrame()\ndf[\"key\"] = [0, 0, 2, 2, 3]\ndf[\"value\"] = [float(i + 10) for i in range(5)]\nprint(df)","dd965888":"# DataFrame creation using cuDF\n\ncdf = cudf.DataFrame()\ncdf[\"key\"] = [0, 0, 2, 2, 3]\ncdf[\"value\"] = [float(i + 10) for i in range(5)]\nprint(cdf)","bb0b6c7b":"# Sample Aggregation check\n\nprint(\"Sum of all Values (Pandas): \", df[\"value\"].sum())\nprint(\"Sum of all Values (cuDF): \", cdf[\"value\"].sum())","64abb3dd":"# cuDF DataFrame creation using Pandas DataFrame\n\nptoc = cudf.DataFrame.from_pandas(df)\nprint(ptoc)","2036df0e":"# View data\n\ncdf.head(2)","29b4db3f":"# Sort data\n\ncdf.sort_values(by=\"value\", ascending=\"False\")","ac1ec4b3":"# Selecting\nprint_u = lambda s: print(\"\\n\" + \"\\u0332\".join(s) + \"\\n\")\n\nprint_u(\"Selection by Column:\")\nprint(cdf['key'])\n\nprint_u(\"Selection by Label:\")\nprint(cdf.loc[2:5, ['value']])\n\nprint_u(\"Selection by Position:\")\nprint(cdf.iloc[2:5])\n\nprint_u(\"Selection by Boolean Indexing (Direct):\")\nprint(cdf[cdf.key < 1])\n\nprint_u(\"Selection by Boolean Indexing (Query):\")\nprint(cdf.query(\"key < 1\"))\n\nprint_u(\"Selection by Boolean Indexing (Query with keyword):\")\ncudf_comparator = 1\nprint(cdf.query(\"key < @cudf_comparator\"))","90b0ce6c":"# String Methods\ns = cudf.Series(['A', 'b', 'Baca', None, 'CABA', 'dog'])\n\nprint_u(\"Lower:\")\nprint(s.str.lower())\n\nprint_u(\"Upper:\")\nprint(s.str.upper())","e462f08e":"# Missing data\nprint(s.fillna(\"NULL\"))","59d9a6ad":"# Operations\n\nprint_u(\"Describe:\")\nprint(cdf.describe())\n\nprint_u(\"Statistics:\")\nprint(cdf.value.mean(), cdf.value.var())\n\n\"\"\"\nprint_u(\"ApplyMap:\")\n\ndef complex_math_transform(num):\n    return math.cos(num) * 3 \/ 9\n\nprint(cdf.value.applymap(complex_math_transform))\n\"\"\"\n\nprint_u(\"Concat:\")\nprint(cudf.concat([s, s]))\n\nprint_u(\"Append:\")\nprint(s.append(s))","9dcf1365":"# Join\n\ndf_a = cudf.DataFrame()\ndf_a['key'] = ['a', 'b', 'c', 'd', 'e']\ndf_a['vals_a'] = [float(i + 10) for i in range(5)]\n\ndf_b = cudf.DataFrame()\ndf_b['key'] = ['a', 'c', 'e']\ndf_b['vals_b'] = [float(i+100) for i in range(3)]\n\nmerged = df_a.merge(df_b, on=['key'], how='left') # LEFT JOIN\nprint(merged)","95b02795":"# Grouping\n\ncdf['agg_col1'] = [1 if x % 2 == 0 else 0 for x in range(len(cdf.value))]\ncdf['agg_col2'] = [1 if x % 3 == 0 else 0 for x in range(len(cdf.value))]\n\n# Grouping and then applying the sum function to the grouped data\nprint(cdf.groupby('agg_col1').sum())\n\nprint(\"\\n\")\n\n# Grouping and applying statistical functions to specific columns, using agg.\nprint(cdf.groupby('agg_col1').agg({'key':'max', 'value':'mean'}))\n\nprint(\"\\n\")\n\n# Multiple Grouping\nprint(cdf.groupby(['agg_col1', 'agg_col2']).agg({'key':'max', 'value':'mean'}))","00c7a6d5":"# Time Series\n\ndate_df = cudf.DataFrame()\ndate_df['date'] = pd.date_range('11\/20\/2018', periods=72, freq='D')\ndate_df['value'] = np.random.sample(len(date_df))\n\n## query before 23rd Nov 2018\n\nconstraint = pd.to_datetime('2018-11-23')\nprint(date_df.query(\"date < @constraint\"))\n\nprint(\"\\n\")\n\n## extract date, month, year etc.. \n\ndate_df['day'] = date_df.date.dt.day\ndate_df['month'] = date_df.date.dt.month\nprint(date_df.head())","e0738e2a":"# Converting Data Representation\n\n## Converting a cuDF DataFrame to a pandas DataFrame.\nprint(cdf.to_pandas(), end=\"\\n\\n\")\n\n## Converting a cuDF DataFrame to a numpy ndarray.\nprint(cdf.as_matrix(), end=\"\\n\\n\")\n\n## Converting a cuDF Series to a numpy ndarray.\nprint(s.to_array(), end=\"\\n\\n\")","36179bd0":"# Getting Data In\/Out\n\n## Writing to CSV by first sending data to host \ncdf.to_pandas().to_csv(\"foo.csv\", index=False)\n\ncdf = cudf.read_csv('foo.csv')\nprint(cdf)","408ebd38":"# Performance\n\na = np.random.rand(10000000) # 10 million values\ndf = pd.DataFrame()\ncdf = cudf.DataFrame()","76210e69":"%time df['a'] = a\n%time cdf['a'] = a","db22d9ec":"%%timeit\ndf['a'].sum()","f6b4f16a":"%%timeit\ncdf['a'].sum()","ee3344f7":"# deleting earlier created variables \ndel cdf, df, s, ptoc, df_a, df_b, merged, a","5e8f4179":"# Example: Haversine distance\n\n# importing\nfrom math import cos, sin, asin, sqrt, pi, atan2\n\nimport time\nimport cudf\nimport numpy as np\nfrom numba import cuda\n\nnp.random.seed(42)\ndata_length = 1000\n\n# cuDF DataFrame\ndf = cudf.DataFrame()\ndf['lat1'] = np.random.normal(10, 1, data_length)\ndf['lon1'] = np.random.normal(10, 1, data_length)\ndf['lat2'] = np.random.normal(10, 1, data_length)\ndf['lon2'] = np.random.normal(10, 1, data_length)","ffad6552":"# Kernel Function\n\ndef haversine_distance_kernel(lat1, lon1, lat2, lon2, out):\n    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(lat1, lon1, lat2, lon2)):\n        x_1 = pi\/180 * x_1\n        y_1 = pi\/180 * y_1\n        x_2 = pi\/180 * x_2\n        y_2 = pi\/180 * y_2\n        \n        dlon = y_2 - y_1\n        dlat = x_2 - x_1\n        a = sin(dlat\/2)**2 + cos(x_1) * cos(x_2) * sin(dlon\/2)**2\n        \n        c = 2 * asin(sqrt(a)) \n        r = 6371 # Radius of earth in kilometers\n        \n        out[i] = c * r","8a1f8d20":"df = df.apply_rows(haversine_distance_kernel,\n                   incols=['lat1', 'lon1', 'lat2', 'lon2'],\n                   outcols=dict(out=np.float64),\n                   kwargs=dict())\n\nprint(df.head())","541625f2":"# Kernel Function (same as above)\n\ndef haversine_distance_kernel(lat1, lon1, lat2, lon2, out):\n    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(lat1, lon1, lat2, lon2)):\n        x_1 = pi\/180 * x_1\n        y_1 = pi\/180 * y_1\n        x_2 = pi\/180 * x_2\n        y_2 = pi\/180 * y_2\n        \n        dlon = y_2 - y_1\n        dlat = x_2 - x_1\n        a = sin(dlat\/2)**2 + cos(x_1) * cos(x_2) * sin(dlon\/2)**2\n        \n        c = 2 * asin(sqrt(a)) \n        r = 6371 # Radius of earth in kilometers\n        \n        out[i] = c * r","e2093842":"acdf = df.apply_chunks(haversine_distance_kernel,\n                        incols=['lat1', 'lon1', 'lat2', 'lon2'],\n                        outcols=dict(out=np.float64),\n                        kwargs=dict(),\n                        chunks=16,\n                        tpb=8)\n\nprint(acdf.head())","1f086600":"df = cudf.DataFrame(\n        {\n            \"stock\": [1] * 5 + [2] * 5 + [3] * 5,\n            \"price\": [np.random.randint(0, 100) for _ in range(15)],\n        }\n     )\n\n# group by 'stock'\ngrouped_df = df.groupby(\"stock\", method=\"cudf\")","7d352e5c":"# kernel function\n\ndef rolling_avg(price, avg):\n    win_size = 3\n    for i in range(cuda.threadIdx.x, len(price), cuda.blockDim.x):\n        if i < win_size - 1:\n            # If there is not enough data to fill the window,\n            # take the average to be NaN\n            avg[i] = np.nan\n        else:\n            total = 0\n            for j in range(i - win_size + 1, i + 1):\n                total += price[j]\n            avg[i] = total \/ win_size","9013627e":"# Compute moving avgs on all groups\nresults = grouped_df.apply_grouped(rolling_avg,\n                                   incols=['price'],\n                                   outcols=dict(avg=np.float64))\n\nprint(results.query(\"stock == 2\"))","3ac7f5f1":"del df, acdf, haversine_distance_kernel, results, rolling_avg","9881f82f":"## Importing\nimport cudf; print('cuDF Version:', cudf.__version__)\n\nimport sklearn; print('Scikit-Learn Version:', sklearn.__version__)\nimport cuml; print('cuML Version:', cuml.__version__)","a79ed515":"## CUDA Datasets\ncuxr, cuyr = cuml.datasets.regression.make_regression()\ncuxc, cuyc = cuml.datasets.classification.make_classification()\n\nprint(f\"Regression Dataset Shape: {(cuxr.shape, cuyr.shape)}\")\nprint(f\"Classification Dataset Shape: {(cuxc.shape, cuyc.shape)}\")","13ee1693":"## Numpy Datasets from CUDA datasets\nimport cupy as cp\n\nnpxr, npyr = cuxr.copy_to_host(), cuyr.copy_to_host()\nnpxc, npyc = cp.asnumpy(cuxc), cp.asnumpy(cuyc)","b2efb139":"## Linear Regression\n\nfrom cuml.linear_model import LinearRegression as CULinearRegression\nfrom sklearn.linear_model import LinearRegression as SKLinearRegression\n\nculr = CULinearRegression(fit_intercept = True, normalize = False, algorithm = \"eig\").fit(cuxr, cuyr)\nsklr = SKLinearRegression(fit_intercept=True, normalize=False).fit(npxr, npyr)\n\nprint(f\"CUML Coefficients: {list(culr.coef_)}, Intercept: {culr.intercept_}\")\nprint(f\"SK Coefficients: {sklr.coef_}, Intercept: {sklr.intercept_}\")","2bd38895":"## Logistic Regression\n\nfrom cuml.linear_model import LogisticRegression as CULogisticRegression\nfrom sklearn.linear_model import LogisticRegression as SKLogisticRegression\n\nculr = CULogisticRegression().fit(cuxc, cuyc)\nsklr = SKLogisticRegression().fit(npxc, npyc)\n\nprint(f\"CUML Coefficients: {culr.coef_},\\n\\n Intercept: {culr.intercept_}\")\nprint(\"\\n\")\nprint(f\"SK Coefficients: {sklr.coef_},\\n\\n Intercept: {sklr.intercept_}\")","5e11d8bc":"## Ridge Regression\n\nfrom cuml.linear_model import Ridge as CURidge\nfrom sklearn.linear_model import Ridge as SKRidge\n\ncur = CURidge(alpha=1.0).fit(cuxr, cuyr)\nskr = SKRidge(alpha=1.0).fit(npxr, npyr)\n\nprint(f\"CUML Coefficients: {list(cur.coef_)}, Intercept: {cur.intercept_}\")\nprint(f\"SK Coefficients: {skr.coef_}, Intercept: {skr.intercept_}\")","aeba57b0":"## Lasso Regression\n\nfrom cuml.linear_model import Lasso as CULasso\nfrom sklearn.linear_model import Lasso as SKLasso\n\ncul = CULasso(alpha=1.0).fit(cuxr, cuyr)\nskl = SKLasso(alpha=1.0).fit(npxr, npyr)\n\nprint(f\"CUML Coefficients: {list(cul.coef_)}, Intercept: {cul.intercept_}\")\nprint(f\"SK Coefficients: {skl.coef_}, Intercept: {skl.intercept_}\")","e65b1be4":"## ElasticNet\n\nfrom cuml.linear_model import ElasticNet as CUElasticNet\nfrom sklearn.linear_model import ElasticNet as SKElasticNet\n\ncuen = CUElasticNet().fit(cuxr, cuyr)\nsken = SKElasticNet(alpha=1.0, l1_ratio=0.5).fit(npxr, npyr)\n\nprint(f\"CUML Coefficients: {cuen.coef_}, Intercept: {cuen.intercept_}\")\nprint(f\"SK Coefficients: {sken.coef_}, Intercept: {sken.intercept_}\")","9fc0cd42":"## Mini Batch SGD Classifier\n\nfrom cuml.linear_model import MBSGDClassifier\nfrom sklearn.linear_model import SGDClassifier\n\ncusgd = MBSGDClassifier(loss='squared_loss').fit(cuxc, cuyc)\nsksgd = SGDClassifier(loss='squared_loss').fit(npxc, npyc)\n\nprint(f\"CUML Coefficients: {cusgd.coef_}, Intercept: {cusgd.intercept_}\")\nprint(\"\\n\")\nprint(f\"SK Coefficients: {sksgd.coef_}, Intercept: {sksgd.intercept_}\")","f4562026":"## Mini Batch SGD Regressor\n\nfrom cuml.linear_model import MBSGDRegressor\nfrom sklearn.linear_model import SGDRegressor\n\ncusgd = MBSGDRegressor(loss='squared_loss').fit(cuxr, cuyr)\nsksgd = SGDRegressor(loss='squared_loss').fit(npxr, npyr)\n\nprint(f\"CUML Coefficients: {list(cusgd.coef_)}, Intercept: {cusgd.intercept_}\")\nprint(\"\\n\")\nprint(f\"SK Coefficients: {sksgd.coef_}, Intercept: {sksgd.intercept_}\")","1b035947":"## Random Forest (Classifier\/Regressor)\nfrom cuml.ensemble import RandomForestClassifier as cuRFC\nfrom cuml.ensemble import RandomForestRegressor as cuRFR\n\ncurfc = cuRFC().fit(cuxc, cuyc)\ncurfr = cuRFR().fit(cuxr, cuyr)\n\nprint(f\"CUML RFClassifier Score: {curfc.score(cuxc, cuyc)}\")\nprint(f\"CUML RFRegressor Score: {curfr.score(cuxr, cuyr)}\")","b611e1ad":"## SVM (Classifier\/Regressor)\nfrom cuml.svm import SVC\nfrom cuml.svm import SVR\n\ncurfc = SVC(kernel='poly', degree=2, gamma='auto', C=1).fit(cuxc, cuyc)\ncurfr = SVR(kernel='poly', degree=2, gamma='auto', C=1).fit(cuxr, cuyr)","0925d85a":"## Nearest Neighbors (Classifier\/Regressor)\nfrom cuml.neighbors import KNeighborsClassifier\nfrom cuml.neighbors import KNeighborsRegressor\n\nfrom cuml.datasets import make_blobs\nfrom cuml.preprocessing.model_selection import train_test_split\n\nX, y = make_blobs(n_samples=100, centers=5, n_features=10)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80)\n\ncuknnc = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\ncuknnr = KNeighborsRegressor(n_neighbors=10).fit(X_train, y_train)\n\nprint(f\"KNN Classifer Prediction: {cuknnc.predict(X_test)}\")\nprint(f\"KNN Regressor Prediction: {cuknnr.predict(X_test)}\")","424f393d":"## K-Means Clustering\nfrom cuml.cluster import KMeans\n\nX = np.asarray([[1.0, 1.0], [1.0, 2.0], [3.0, 2.0], [4.0, 3.0]], dtype=np.float32)\n\nkmeans = KMeans(n_clusters=2).fit(X)\nprint(f\"Labels: {kmeans.labels_}\")\nprint(f\"Cluster Centers:\\n {kmeans.cluster_centers_}\")","6c82a8f2":"## DBSCAN\nfrom cuml.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=1.0, min_samples=2).fit(X)\nprint(f\"Labels: {dbscan.labels_}\")","0743dafc":"## PCA\nfrom cuml.decomposition import PCA\n\npca = PCA(n_components=1)\npca.fit(X)\n\nprint(f'components: {pca.components_}')\nprint(f'explained variance ratio: {pca.explained_variance_ratio_}')","0650cbe7":"## TSNE\nfrom cuml import TSNE\n\ntsne = TSNE(n_components=1)\ntsne.fit(X)","8fb34ac0":"## HoltWinters\nfrom cuml import ExponentialSmoothing\n\ndata = cudf.Series([1, 2, 3, 4, 5, 6,\n                    7, 8, 9, 10, 11, 12,\n                    2, 3, 4, 5, 6, 7,\n                    8, 9, 10, 11, 12, 13,\n                    3, 4, 5, 6, 7, 8, 9,\n                    10, 11, 12, 13, 14],\n                    dtype=np.float64)\n\ncu_hw = ExponentialSmoothing(data, seasonal_periods=12)\ncu_hw.fit()\n\ncu_pred = cu_hw.forecast(4)\nprint('Forecasted points:\\n', cu_pred)","b14d31f4":"## ARIMA\nfrom cuml.tsa.arima import ARIMA\n\n# Create seasonal data with a trend, a seasonal pattern and noise\nn_obs = 100\nnp.random.seed(12)\nx = np.linspace(0, 1, n_obs)\n\npattern = np.array([[0.05, 0.0], [0.07, 0.03], [-0.03, 0.05], [0.02, 0.025]])\nnoise = np.random.normal(scale=0.01, size=(n_obs, 2))\ny = (np.column_stack((0.5*x, -0.25*x)) + noise + np.tile(pattern, (25, 1)))\n\n# Fit a seasonal ARIMA model\narima = ARIMA(y, (0,1,1), (0,1,1,4), fit_intercept=False)\narima.fit()\n\n# Forecast\ncu_pred = arima.forecast(10)\nprint('Forecasted points:\\n', cu_pred)","adf59e0d":"## Grid Search\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\n\n# binary classification\nX, y = datasets.make_hastie_10_2(n_samples=12000)\n\n# hyperparameters\nparameters = {'kernel':('linear', 'rbf'), 'C':range(1, 10)}\n\n# cuML SVC\ncusvc = cuml.svm.SVC()\n\nclf = GridSearchCV(cusvc, parameters)\nclf.fit(X, y)\n\nprint(f\"Best Parameters : {clf.best_params_}\")","6583abf6":"Built based on the Apache Arrow columnar memory format, **cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data**.\n\n`cuDF provides a pandas-like API` that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.","3689512a":"### Advanced UDFs: apply_grouped\n\nApply a kernel function over the grouped chunk.","f266f43e":"## Classification\/Regression <a class=\"anchor\" id=\"cr\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>\n\n<ul>\n<li>Linear Regression<\/li>\n<li>Logistic Regression<\/li>\n<li>Ridge Regression<\/li>\n<li>Lasso Regression<\/li>\n<li>ElasticNet Regression<\/li>\n<li>Mini Batch SGD Classifier<\/li>\n<li>Mini Batch SGD Regressor<\/li>\n<li>Stochastic Gradient Descent<\/li>\n<li>Random Forest<\/li>\n<li>Forest Inferencing<\/li>\n<li>Coordinate Descent<\/li>\n<li>Quasi-Newton<\/li>\n<li>Support Vector Machines<\/li>\n<li>Nearest Neighbors Classification<\/li>\n<li>Nearest Neighbors Regression<\/li><\/ul>","1f5d2f0a":"# cuML <a class=\"anchor\" id=\"ml\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>","c3ff6a96":"## User Defined Functions with cuDF <a class=\"anchor\" id=\"2df\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>","2d93954f":"cuGraph is a collection of graph analytics that process data found in cuDF Dataframes. cuGraph aims to provide a NetworkX-like API that will be familiar to data scientists, so they can now build GPU-accelerated workflows more easily.\n\nPlease refer to this github notebooks for understanding cuGraph:\n\nhttps:\/\/github.com\/Quansight\/scipy-2019-rapids-tutorial\/tree\/master\/cugraph","b96f754d":"## Dimensionality Reduction and Manifold Learning <a class=\"anchor\" id=\"drml\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>\n\n<ul>\n<li>Principal Component Analysis<\/li>\n<li>Truncated SVD<\/li>\n<li>UMAP<\/li>\n<li>Random Projections<\/li>\n<li>TSNE<\/li><\/ul>","cca60788":"## Introduction to cuDF <a class=\"anchor\" id=\"1df\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>","b5293020":"Sometimes, the built-in methods of `cudf.DataFrame` don't do exactly what we want. We need to write a custom function (also known as a user defined function) to apply over the DataFrame.\n\ncuDF\u2019s DataFrame class has two primary methods that let users run custom Python functions on GPUs: `apply_rows` and `apply_chunks`.\n\nMore advanced example of applying a user defined function on a grouped DataFrame using `apply_grouped`.","0c1b8f5a":"## Time Series <a class=\"anchor\" id=\"ts\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>\n\n<ul>\n<li>HoltWinters<\/li>\n<li>ARIMA<\/li>\n<\/ul>","2acd2d92":"# cuDF <a class=\"anchor\" id=\"df\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>","ca60ff9d":"\nIn the example above:\n* The processing was spread across 15 CUDA blocks.\n* Within each block, 64 separate threads were used for computation.\n* In this case, most threads in a block handled one element from the input array, but some threads have to deal with two elements, because there are 1000 rows and 960 threads (15 blocks * 64 threads per block)\n\n> `apply_rows` handled all of this for us!","9ee4ba2e":"## Grid Search CV <a class=\"anchor\" id=\"gscv\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>\n\n\n**Can normally pass in the numpy array's to the cuML models and the model can be given as input to the GridSearchCV**","0a71499a":"### apply_rows\n\n`apply_rows` processes each of the DataFrame rows independently in parallel. Under the hood, the `apply_rows` method will optimally divide the long columns into chunks, and assign chunks into different GPU blocks for parallel computation.\n\nIn order to use `apply_rows`, we need to write a **kernel function**. A kernel function is a function that will be executed on each row of the DataFrame set the output value for each row. The execution order of rows is arbitrary, so each execution of the function MUST be independent of other execution.","b7202196":"# SetUp Rapids <a class=\"anchor\" id=\"sr\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>","9b0ea4c7":"## Clustering <a class=\"anchor\" id=\"clu\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>\n\n<li>K-Means Clustering<\/li>\n<li>DBSCAN<\/li>\n<\/ul>","dabbb7bf":"# cuGraph <a class=\"anchor\" id=\"gr\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>","85c1112b":"# Table of Contents  <a class=\"anchor\" id=\"toc\"><\/a>\n\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n    <li style=\"list-style: outside none none !important;\"><a href=\"#wr\">1- What is Rapids?<\/a><\/li>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#sr\">2- SetUp Rapids<\/a><\/li>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#df\">3- cuDF<\/a>\n        <ul>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#1df\">3.1- Introduction to cuDF<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#2df\">3.2- User Defined Functions with cuDF<\/a><\/li>\n        <\/ul>\n    <\/li>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#ml\">4- cuML<\/a>\n        <ul>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#cr\">4.1- Classification\/Regression<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#clu\">4.2- Clustering<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#drml\">4.3- Dimensionality Reduction and Manifold Learning<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#ts\">4.4- Time Series<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#gscv\">4.5- Grid Search CV<\/a><\/li>\n        <\/ul>    \n    <\/li>\n    <li style=\"list-style: outside none none !important;\"><a href=\"#gr\">5- cuGraph<\/a><\/li>\n<\/ul>\n<\/div>","0fe92eb5":"### apply_chunks\n\nIn the section above, the data was generally split into single-element chunks. `apply_chunks` is a more general version of `apply_rows` that gives us control over how the data is chunked on the GPU. We can specify how to divide the long array, map each of the array chunks to different GPU blocks to process (using the `chunks` argument) and assign the number of threads per block (using the `tpb` argument).\n\nApplying kernels with `apply_chunks` is very similar to applying kernels with `apply_rows`. Except, when we call `apply_chunks`, we must also provide:\n\n* The chunk size chunks as an integer or `cudf.Series` of integer offsets\n* The number of threads per block, tpb. Note that tpb can be omitted, but in that case, it defaults to 1 thread per block, which is very inefficient. We recommend always setting this argument.","0a8d3797":"# Introduction to RAPIDS\n\n***by*** <a href=\"https:\/\/linkedin.com\/in\/harishvutukuri\/\">Harish Vutukuri<\/a> (28 December 2020)","d156f142":"# What is RAPIDS? <a class=\"anchor\" id=\"wr\"><\/a>\n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fe\/Green-Up-Arrow.svg\" style=\"width:20px;hight:20px;float:left\">&nbsp;Back to the table of contents<\/a>\n\nRAPIDS is a \u201csuite of open source software libraries and APIs\u201d grouped together for the purpose of providing users the ability to `\u201cexecute end-to-end data science and analytics pipelines entirely on GPUs.\u201d`\n\n**RAPIDS utilizes <em>NVIDIA CUDA\u00ae<\/em> primitives for low-level compute optimization, and exposes GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.**\n\n**RAPIDS also includes support for multi-node, multi-GPU deployments, enabling vastly accelerated processing and training on much larger dataset sizes.**\n\nRAPIDS had its start from the [Apache Arrow](https:\/\/arrow.apache.org\/) and [GoAi](http:\/\/gpuopenanalytics.com\/) projects based on a columnar, in-memory data structure that delivers efficient and fast data interchange with flexibility to support complex data models.\n\n## Libraries and APIs Overview\n\n* **cuDF** \u2014 pandas-like dataframe manipulation library\n* **cuML** \u2014 collection of ML libraries that will provide GPU versions of algorithms available in scikit-learn\n* **cuGraph** \u2014 network-X like graphing API\n\n## Integration With Deep Learning Libraries\n\nThrough it\u2019s Apache Arrow roots, RAPIDS provides native array_interface support so data can be seamlessly pushed to deep learning frameworks that accept array_interface (e.g. PyTorch, Chainer) or work with DLPack and MXNet.\n\n## Overview\n\n![image.png](attachment:image.png)"}}