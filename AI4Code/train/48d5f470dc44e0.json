{"cell_type":{"4b36a07a":"code","7a79ce49":"code","7f61694e":"code","2de6ac61":"code","70d878ed":"code","8cd9f5c8":"code","6a6d2468":"code","cc008de1":"code","64bc10e0":"code","af03f92b":"code","1098dbc9":"code","6b6d1f2e":"code","c32eb4cd":"code","95bd9e94":"code","d164a267":"code","b8ea1b59":"code","418fb6d5":"code","bd81323c":"code","2a49b7ef":"code","5fe9fd0d":"code","fde9d717":"code","a8da1ebe":"code","597532af":"code","1e7e66d2":"code","5227d397":"code","bec5ae36":"code","480ed7ee":"code","48fb734f":"code","aa14121c":"code","d4d34102":"code","0f772b71":"code","987a7557":"code","e3f89af8":"code","d4c27b5c":"code","9fe74def":"code","cc110a38":"code","52dc3020":"code","65d21488":"code","d6a056d2":"code","f8a1c60a":"code","bf3e7cdb":"markdown","924babcf":"markdown","bcf39363":"markdown","d4b4f209":"markdown","f1a1cdc3":"markdown","4a72c001":"markdown"},"source":{"4b36a07a":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils import plot_model\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import BatchNormalization\nimport tensorflow as tf\nimport keras\nfrom keras.constraints import unit_norm\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.layers import Input, Embedding,Flatten,concatenate, Conv1D, Bidirectional,Dropout\nfrom keras.models import load_model\nfrom numpy.testing import assert_allclose","7a79ce49":"# reading data set\ndata = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')","7f61694e":"# removing a noisy data point\ndata = data[data.textID != '12f21c8f19']\ndata","2de6ac61":"# removing empty rows\ndata['text'].replace('', np.nan, inplace=True)\ndata.dropna(subset=['text'], inplace=True)\ndata.reset_index(drop=True, inplace=True)","70d878ed":"data['text'] = data['text'].apply(lambda x: \" \".join(x.split()))\ndata['selected_text'] = data['selected_text'].apply(lambda x: \" \".join(x.split()))\n","8cd9f5c8":"data = data.astype({\"text\": str, \"selected_text\": str, 'sentiment': str})\ndata","6a6d2468":"\nx_train,x_test = train_test_split(data, test_size = 0.05, random_state=42)\nx_train,x_cv = train_test_split(x_train, test_size = 0.2, random_state = 42)\n\nprint(\"x_train shape is\", x_train.shape)\nprint(\"x_cv shape is\", x_cv.shape)\nprint(\"x_test shape is\", x_test.shape)\n","cc008de1":"# index reset.\nx_train.reset_index(inplace = True, drop = True)\nx_cv.reset_index(inplace = True, drop = True)\nx_test.reset_index(inplace = True, drop = True)","64bc10e0":"# https:\/\/stackoverflow.com\/questions\/31749448\/how-to-add-percentages-on-top-of-bars-in-seaborn\nfig,ax = plt.subplots(figsize = (15,5), nrows =1, ncols = 3)\nax = ax.flatten()\nsns.countplot(x_train.sentiment, ax = ax[0], order = ['neutral', 'positive', 'negative'])\ntotal = x_train.shape[0]\nfor p in ax[0].patches:\n    height = p.get_height()\n    ax[0].text(p.get_x()+p.get_width()\/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100\/total),\n            ha=\"center\") \nsns.countplot(x_cv.sentiment, ax = ax[1], order = ['neutral', 'positive', 'negative'])\ntotal = x_cv.shape[0]\nfor p in ax[1].patches:\n    height = p.get_height()\n    ax[1].text(p.get_x()+p.get_width()\/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100\/total),\n            ha=\"center\") \nsns.countplot(x_test.sentiment, ax = ax[2], order = ['neutral', 'positive', 'negative'])\ntotal = x_test.shape[0]\nfor p in ax[2].patches:\n    height = p.get_height()\n    ax[2].text(p.get_x()+p.get_width()\/2.,\n            height + 4,\n            '{:1.2f}%'.format(height*100\/total),\n            ha=\"center\") ","af03f92b":"x_train","1098dbc9":"print(x_train.shape)\nprint(x_cv.shape)\nprint(x_test.shape)","6b6d1f2e":"#https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n# tokenizing text to sequences and padding.\n\n\ntext_tokenizer = Tokenizer(char_level =True)\ntext_tokenizer.fit_on_texts(list(x_train['text']))\nvocab_size_1 = len(text_tokenizer.word_index) + 1\n# integer encode the documents\nprint(\"vocab size is:\",vocab_size_1)\n\ntrain_text = text_tokenizer.texts_to_sequences(list(x_train['text']))\ncv_text = text_tokenizer.texts_to_sequences(list(x_cv['text']))\ntest_text = text_tokenizer.texts_to_sequences(list(x_test['text']))\n\ntrain_select_text = text_tokenizer.texts_to_sequences(list(x_train['selected_text']))\ncv_select_text = text_tokenizer.texts_to_sequences(list(x_cv['selected_text']))\ntest_select_text = text_tokenizer.texts_to_sequences(list(x_test['selected_text']))\n\n\nmax_length = 141 # max length of a tweet\n\ntrain_text = pad_sequences(train_text, maxlen=max_length, padding='post')\ncv_text =  pad_sequences(cv_text, maxlen=max_length, padding='post')\ntest_text = pad_sequences(test_text, maxlen = max_length, padding = 'post')\n\n\n\n\nprint(\"no. of rows sequences in train:\",len(train_text))\nprint(\"no. of rows of sequences in validataion:\", len(cv_text))\nprint(\"max length of sequences\",max_length)","c32eb4cd":"# sample datapoint\ni = 1\nprint('text:')\nprint(x_train.loc[i,'text'])\nprint('sequence of text:')\nprint(text_tokenizer.texts_to_sequences([x_train.loc[i,'text']]))\nprint('sequence of text after padding:')\nprint(train_text[i])\nprint('select text:')\nprint(x_train.loc[i,'selected_text'])\nprint('sequence of select text:')\n\nprint(train_select_text[i])","95bd9e94":"# tokenizing sentiment.\nsentiment_tokenizer = Tokenizer()\nsentiment_tokenizer.fit_on_texts(x_train['sentiment'])\nvocab_size_2 = len(sentiment_tokenizer.word_index) +1\n\ntrain_sentiment = sentiment_tokenizer.texts_to_sequences(x_train['sentiment'])\ncv_sentiment = sentiment_tokenizer.texts_to_sequences(x_cv['sentiment'])\ntest_sentiment = sentiment_tokenizer.texts_to_sequences(x_test['sentiment'])\n\n\nprint(sentiment_tokenizer.word_index)","d164a267":"# https:\/\/stackoverflow.com\/questions\/7100242\/python-numpy-first-occurrence-of-subarray\n# creating new target variables.\ndef target_creation(tweets, sub_tweets):\n    \"\"\"\n    inputs:\n    tokenized tweet and tokenized selected_text.\n    \n    action:\n    calculates start and end index of subtweet within tweet.\n    \n    output:\n    returns start and end indices.\n    \n    \n    \"\"\"\n    \n    start = np.zeros(tweets.shape, dtype = 'int32')\n    end = np.zeros(tweets.shape, dtype = 'int32')\n    \n    for i in range(tweets.shape[0]):\n        \n            \n        a=tweets[i]\n        b = sub_tweets[i]\n        for j in range(len(a)):\n            if (a[j:j+len(b)]==b).all():\n                break\n\n        start[i,j] = 1\n        end[i,j+len(sub_tweets[i])] = 1\n       \n    \n    return start,end","b8ea1b59":"# coverting lists to array\ntrain_select_text = np.array(train_select_text)\ncv_select_text = np.array(cv_select_text)\ntest_select_text = np.array(test_select_text)\n\ntrain_sentiment = np.array(train_sentiment)\ncv_sentiment =np.array(cv_sentiment)\ntest_sentiment =np.array(test_sentiment)","418fb6d5":"# checking whether all created targets are correct.\n\ntrain_start,train_end = target_creation(train_text,train_select_text)\ncount = 0\nfor i in range(x_train.shape[0]):\n\n    \n    if (train_text[i][np.argmax(train_start[i]):np.argmax(train_end[i])]==train_select_text[i]).all():\n\n        count+=1\n    else:\n        print(len(train_text[i][np.argmax(train_start[i]):np.argmax(train_end[i])]))\n        print(len(train_text[i]))\n        print(len(train_select_text[i]))\n        print(train_text[i][np.argmax(train_start[i]):np.argmax(train_end[i])])\n        print(train_text[i])\n        print(train_select_text[i])\n\n        print(x_train.loc[i])\n        print(i)\n\nif count ==x_train.shape[0]:\n    print('all targets are correct')\nelse:\n    print(count,'targets are correct')","bd81323c":"# checking whether all created targets are correct.\n\ncv_start,cv_end = target_creation(cv_text,cv_select_text)\ncount = 0\nfor i in range(x_cv.shape[0]):\n\n    \n    if (cv_text[i][np.argmax(cv_start[i]):np.argmax(cv_end[i])]==cv_select_text[i]).all():\n\n        count+=1\n    else:\n        print(len(cv_text[i][np.argmax(cv_start[i]):np.argmax(cv_end[i])]))\n        print(len(cv_text[i]))\n        print(len(cv_select_text[i]))\n        print(cv_text[i][np.argmax(cv_start[i]):np.argmax(cv_end[i])])\n        print(cv_text[i])\n        print(cv_select_text[i])\n        print(x_cv.loc[i])\n        print(i)\n        \n    \n    \nif count ==x_cv.shape[0]:\n    print('all targets are correct')\nelse:\n    print(count,'targets are correct')","2a49b7ef":"# checking whether all targets are correct.\ntest_start,test_end = target_creation(test_text,test_select_text)\ncount = 0\nfor i in range(x_test.shape[0]):\n\n    \n    if (test_text[i][np.argmax(test_start[i]):np.argmax(test_end[i])]==test_select_text[i]).all():\n\n        count+=1\n    else:\n        print(len(test_text[i][np.argmax(test_start[i]):np.argmax(test_end[i])]))\n        print(len(test_text[i]))\n        print(len(test_select_text[i]))\n        print(test_text[i][np.argmax(test_start[i]):np.argmax(test_end[i])])\n        print(test_text[i])\n        print(test_select_text[i])\n        print(x_test.loc[i])\n        print(i)\n\n        \nif count ==x_test.shape[0]:\n    print('all targets are correct')\nelse:\n    print(count,'targets are correct')","5fe9fd0d":"#loading char glove vectors\nchar2vec = {}\nwith open('..\/input\/glove840b300dchar\/glove.840B.300d-char.txt') as f:\n    for line in f:\n        values = line.split()\n        char = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        char2vec[char] = coefs\n        \nprint('no. of char vectors',len(char2vec))\nprint('chars covered in the model', list(char2vec.keys()))","fde9d717":"# creating embedding matrix\n\nvocab =text_tokenizer.word_index\nembedding_matrix = np.zeros((len(vocab) + 1, 300))\nfor word, i in vocab.items():\n    vector = char2vec.get(word)\n\n    if vector is not None:\n        embedding_matrix[i] = vector\n","a8da1ebe":"# padding select text sequences\ntrain_select_text = pad_sequences(train_select_text, maxlen=max_length, padding='post')\ncv_select_text =  pad_sequences(cv_select_text, maxlen=max_length, padding='post')\ntest_select_text = pad_sequences(test_select_text, maxlen = max_length, padding = 'post')\n","597532af":"# https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n\n\ndef build_model(n1,n2,n3,n4,drop,mode,bidir=False):\n\n    \"\"\"\n    inputs:\n    \n    n1: no. of units in first layer if mode is 'lstm' else no. of filters in conv layer\n    n2: no. of units in second layer if mode is 'lstm' else kernel size in conv layer\n    n3: no. of neurons in first dense layer\n    n4: no. of neurons in second dense layer\n    mode: lstm\/conv\n    bidir: normal lstm or bidirectional lstm\n    drop: dropout rate\n    \n    action:\n    \n    creates a neural network based on given inputs\n    \n    output:\n    \n    returns the model\n    \n    \"\"\"\n    \n    keras.backend.clear_session()\n    \n    i1 = Input(shape=(141,), dtype='int32')\n    e = Embedding(vocab_size_1, 300, weights=[embedding_matrix],  trainable=False )(i1)\n    if(mode=='lstm'):\n        if bidir:\n            x1 = Bidirectional(keras.layers.LSTM(n1, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(e)\n            x1 = Bidirectional(keras.layers.LSTM(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) ))(e)\n        else:\n            x1 = keras.layers.LSTM(n1, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(e)\n            x1 = keras.layers.LSTM(n2, return_sequences=True, kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001) )(e)\n\n    elif(mode=='conv'):\n        x1=Conv1D(n1,n2,activation = 'relu',)(e)\n        x1=Conv1D(n1\/2,n2,activation = 'relu',)(x1)\n    x1 = Flatten()(x1)\n    \n    i2 = Input(shape=(1,), dtype='int32')\n    e = Embedding(vocab_size_2, 2, )(i2)\n    x2 = Flatten()(e)\n    con = concatenate([x1,x2])\n    con = BatchNormalization()(con) \n    \n    x1 = keras.layers.Dense(n1, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(con)\n    x1 = BatchNormalization()(x1) \n    x1 = Dropout(drop)(x1)\n    x1 = keras.layers.Dense(n2, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(x1)\n    x1 = Dropout(drop)(x1)\n\n    x2 = keras.layers.Dense(n1, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(con)\n    x2 = BatchNormalization()(x2) \n    x2 = Dropout(drop)(x2)\n\n    x2 = keras.layers.Dense(n2, activation = 'relu', kernel_initializer='he_uniform',kernel_constraint=unit_norm(),kernel_regularizer=regularizers.l2(0.0001))(x2)\n    x2 = Dropout(drop)(x2)\n    \n    output1 = keras.layers.Dense(141, activation = 'softmax')(x1)\n    output2 = keras.layers.Dense(141, activation = 'softmax')(x2)\n\n    model = keras.models.Model(inputs =[i1,i2], outputs = [output1,output2] )\n\n    opt = keras.optimizers.Adam(lr=3e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0,clipnorm=1)\n\n    model.compile(optimizer = opt, loss = 'categorical_crossentropy' )\n\n\n    return model\n","1e7e66d2":"model = build_model(128,64,32,16,0.3,'lstm',False )\n","5227d397":"# visualizing the model\nplot_model(model, show_shapes = False)\n","bec5ae36":"# model checkpoint to save best model.\nfilepath = \"\/kaggle\/working\/best_model.h5\" \ncheckpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n\n","480ed7ee":"model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =20, batch_size = 32, callbacks= [checkpoint])","48fb734f":"model = build_model(256,4,32,16,0.1,'conv' )\n","aa14121c":"plot_model(model, show_shapes = False)\n","d4d34102":"model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =20, batch_size = 32, callbacks= [checkpoint])","0f772b71":"model = build_model(128,64,64,32,0.3,'lstm',True )\n","987a7557":"plot_model(model, show_shapes = False)\n","e3f89af8":"model.fit([train_text,train_sentiment],[train_start,train_end], validation_data = ([cv_text,cv_sentiment], [cv_start, cv_end]),epochs =40, batch_size = 32, callbacks= [checkpoint])","d4c27b5c":"#https:\/\/stackoverflow.com\/questions\/51700351\/valueerror-unknown-metric-function-when-using-custom-metric-in-keras\n# loading the bestmodel out of three.\nbest_model = load_model(\".\/best_model.h5\",)","9fe74def":"test_start,test_end= best_model.predict([test_text,test_sentiment])","cc110a38":"# metric\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","52dc3020":"# calculating the jaccard score.\n\nscore = 0\nfor i in range(x_test.shape[0]):\n    \n    if np.argmax(test_start[i])<=np.argmax(test_end[i]):\n    \n        score = score + jaccard(x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1],x_test.selected_text[i])\n    else:\n        score = score + jaccard(x_test.text[i],x_test.selected_text[i])\n\n\n    \nprint(score\/x_test.shape[0])        ","65d21488":"# prediction examples.\n\nfor i in range(0,x_test.shape[0],100):\n    print('text:', x_test.text[i])\n    print('selected text:', x_test.selected_text[i])\n    print('sentiment:',x_test.sentiment[i] )\n    print('predicted:',x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1])\n    print('jaccard score:', jaccard(x_test.selected_text[i], x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1]))\n    print('#################################\\n')\n    #score = score + jaccard(x_test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])+1],x_test.selected_text[i])\n\n","d6a056d2":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n\n# tokenizing test set.\ntest_text = text_tokenizer.texts_to_sequences(test['text'])\ntest_text = pad_sequences(test_text, maxlen=max_length, padding='post')\ntest_sentiment = sentiment_tokenizer.texts_to_sequences(test['sentiment'])\n\n# coverting lists to array.\ntest_text = np.array(test_text)\ntest_sentiment = np.array(test_sentiment)\n\n# predicting using best model.\n\npreds = []\ntest_start,test_end= best_model.predict([test_text,test_sentiment])\n\nfor i in range(test.shape[0]):\n    preds.append(test.text[i][np.argmax(test_start[i]):np.argmax(test_end[i])])","f8a1c60a":"# creating submission file.\nsubmission = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\nsubmission['selected_text'] = preds\n\nsubmission.to_csv('submission.csv', index = False)","bf3e7cdb":"## Libraries","924babcf":"## Modelling","bcf39363":"## Char Embeddings","d4b4f209":"## Splitting data into train cv and test","f1a1cdc3":"## Test Data Predictions","4a72c001":"## Vectorization"}}