{"cell_type":{"691bfa60":"code","7008bed7":"code","82f67f22":"code","e309bfd4":"code","e2f05294":"code","c3deddbc":"code","3dd3d7a3":"code","8507252a":"code","6cd71a0d":"code","a14e6eff":"code","94d6f037":"code","3accdc5f":"code","ef8be826":"code","cb719e7d":"code","459784ae":"code","1d9aacde":"code","8ab0b71e":"code","b29eba4b":"code","531e5fca":"code","cbff24c4":"code","75c154a4":"code","81faadc9":"code","d62c8e04":"code","a45dcd83":"code","3c1304c9":"code","97e2a0bd":"code","2e096e8d":"code","26f32a29":"code","1ae7e028":"code","a729bdac":"code","15115131":"code","e2e732fc":"code","4837c2d9":"code","65083c83":"code","66796bdc":"code","5052038e":"code","25419d36":"code","f341d69b":"markdown","54c32dc0":"markdown","41516262":"markdown","c7be4223":"markdown","bb29b745":"markdown","ea744d8a":"markdown","0132b86b":"markdown","161b3adc":"markdown","6edb11b0":"markdown","8ba01235":"markdown","5cd7741f":"markdown","125f28e0":"markdown","de0789ae":"markdown","bc4f08f0":"markdown","f08e8211":"markdown","2dbbfd8a":"markdown","7fed5396":"markdown","631a4ba3":"markdown","006361c8":"markdown","1ad1a773":"markdown","640ef343":"markdown","cb494cf2":"markdown","da77ae8f":"markdown","963dcca2":"markdown","d0eea9a4":"markdown","72e01520":"markdown","db9efb94":"markdown","fad0d36f":"markdown","1b5b25da":"markdown","78de0871":"markdown","ac2b2046":"markdown","91bff1d4":"markdown"},"source":{"691bfa60":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n%matplotlib inline","7008bed7":"sms = pd.read_csv('..\/input\/spam.csv', encoding='latin-1')\nsms.head()","82f67f22":"sms = sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\nsms = sms.rename(columns = {'v1':'label','v2':'message'})","e309bfd4":"sms.groupby('label').describe()","e2f05294":"sms['length'] = sms['message'].apply(len)\nsms.head()","c3deddbc":"mpl.rcParams['patch.force_edgecolor'] = True\nplt.style.use('seaborn-bright')\nsms.hist(column='length', by='label', bins=50,figsize=(11,5))","3dd3d7a3":"text_feat = sms['message'].copy()","8507252a":"def text_process(text):\n    \n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    \n    return \" \".join(text)","6cd71a0d":"text_feat = text_feat.apply(text_process)","a14e6eff":"vectorizer = TfidfVectorizer(\"english\")","94d6f037":"features = vectorizer.fit_transform(text_feat)","3accdc5f":"features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3, random_state=111)","ef8be826":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score","cb719e7d":"svc = SVC(kernel='sigmoid', gamma=1.0)\nknc = KNeighborsClassifier(n_neighbors=49)\nmnb = MultinomialNB(alpha=0.2)\ndtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\nlrc = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=31, random_state=111)\nabc = AdaBoostClassifier(n_estimators=62, random_state=111)\nbc = BaggingClassifier(n_estimators=9, random_state=111)\netc = ExtraTreesClassifier(n_estimators=9, random_state=111)","459784ae":"clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc, 'AdaBoost': abc, 'BgC': bc, 'ETC': etc}","1d9aacde":"def train_classifier(clf, feature_train, labels_train):    \n    clf.fit(feature_train, labels_train)","8ab0b71e":"def predict_labels(clf, features):\n    return (clf.predict(features))","b29eba4b":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))","531e5fca":"df = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score'])\ndf","cbff24c4":"df.plot(kind='bar', ylim=(0.9,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","75c154a4":"def stemmer (text):\n    text = text.split()\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words += (stemmer.stem(i))+\" \"\n    return words","81faadc9":"text_feat = text_feat.apply(stemmer)","d62c8e04":"features = vectorizer.fit_transform(text_feat)","a45dcd83":"features_train, features_test, labels_train, labels_test = train_test_split(features, sms['label'], test_size=0.3, random_state=111)","3c1304c9":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))","97e2a0bd":"df2 = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score2'])\ndf = pd.concat([df,df2],axis=1)\ndf","2e096e8d":"df.plot(kind='bar', ylim=(0.85,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","26f32a29":"lf = sms['length'].as_matrix()\nnewfeat = np.hstack((features.todense(),lf[:, None]))","1ae7e028":"features_train, features_test, labels_train, labels_test = train_test_split(newfeat, sms['label'], test_size=0.3, random_state=111)","a729bdac":"pred_scores = []\nfor k,v in clfs.items():\n    train_classifier(v, features_train, labels_train)\n    pred = predict_labels(v,features_test)\n    pred_scores.append((k, [accuracy_score(labels_test,pred)]))","15115131":"df3 = pd.DataFrame.from_items(pred_scores,orient='index', columns=['Score3'])\ndf = pd.concat([df,df3],axis=1)\ndf","e2e732fc":"df.plot(kind='bar', ylim=(0.85,1.0), figsize=(11,6), align='center', colormap=\"Accent\")\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Distribution by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","4837c2d9":"from sklearn.ensemble import VotingClassifier","65083c83":"eclf = VotingClassifier(estimators=[('BgC', bc), ('ETC', etc), ('RF', rfc), ('Ada', abc)], voting='soft')","66796bdc":"eclf.fit(features_train,labels_train)","5052038e":"pred = eclf.predict(features_test)","25419d36":"print(accuracy_score(labels_test,pred))","f341d69b":"### What have we forgotten? Message length!","54c32dc0":"Looks like the lengthy is the message, more likely it is a spam. Let's not forget this","41516262":"Now drop \"unnamed\" columns and rename v1 and v2 to \"label\" and \"message\"","c7be4223":"### Stemmer","bb29b745":"Looks like ensemble classifiers are not doing as good as expected.","ea744d8a":"We are using ensemble algorithms here, but what about ensemble of ensembles? Will it beat NB?","0132b86b":"Now define our tex precessing function. It will remove any punctuation and stopwords aswell.","161b3adc":"Let's read the data from csv file","6edb11b0":"Let's make functions to fit our classifiers and make predictions","8ba01235":"Parametres are based on notebook:\n[Spam detection Classifiers hyperparameter tuning][1]\n\n\n  [1]: https:\/\/www.kaggle.com\/muzzzdy\/d\/uciml\/sms-spam-collection-dataset\/spam-detection-classifiers-hyperparameter-tuning\/","5cd7741f":"### Final verdict - well tuned NaiveBayes is your friend in spam detection.","125f28e0":"And beforehand i want to thank Jose Portilla for his magnificent \"Python for Data Science and Machine Learning\" course on Udemy , which helped me to dive into ML =)","de0789ae":"Let's append our message length feature to the matrix we fit into our classifiers","bc4f08f0":"Let's create new data frame. We'll need a copy later on","f08e8211":"Now let's import bunch of classifiers, initialize them and make a dictionary to itereate through","2dbbfd8a":"### Voting classifier","7fed5396":"Now iterate through classifiers and save the results","631a4ba3":"Now let's create new feature \"message length\" and plot it to see if it's of any interest","006361c8":"Define our stemmer function","1ad1a773":"Better but nope.","640ef343":"It is said that stemming short messages does no goot or even harm predictions. Let's try this out.","cb494cf2":"Intresting that \"Sorry, I'll call later\" appears only 30 times here =)","da77ae8f":"This time everyone are doing a little bit worse, except for LinearRegression and RandomForest. But the winner is still MultinominalNaiveBayes.","963dcca2":"Goal of this notebook to test several classifiers on the data set with different features ","d0eea9a4":"First of all neccesary imports","72e01520":"### Text processing and vectorizing our meddages","db9efb94":"### Let's begin","fad0d36f":"Looks like mostly the same . Ensemble classifiers doing a little bit better, NB still got the lead.","1b5b25da":"###  Classifiers and predictions","78de0871":"First of all let's split our features to test and train set","ac2b2046":"Let's look into our data","91bff1d4":"Stem, split, fit - repeat... Predict!"}}