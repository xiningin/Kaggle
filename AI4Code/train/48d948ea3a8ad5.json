{"cell_type":{"f2ec1663":"code","2f6c443f":"code","6012450f":"code","1b3d3ece":"code","97942bf2":"code","2da40ec9":"code","211d1e80":"code","1adfff4d":"code","68ac8dca":"code","b9820d56":"code","94f3d7d5":"code","6d744410":"code","1637e9c6":"code","49e67c7e":"code","0afd12ec":"code","65f9894e":"code","a351fa01":"code","603a254c":"code","83227f1c":"code","1cc887c2":"code","b4cc7f0f":"code","453d34c5":"code","5bd11477":"code","0341cf3b":"code","6852495b":"code","d65fbd43":"code","00313761":"code","8d920128":"code","6b12a898":"code","3b58b73b":"code","54e1016f":"code","d18a9e0e":"code","4ff86311":"code","9c99d74e":"code","4caa5ab7":"code","5030ed3f":"code","97dd63de":"code","a928f302":"code","8501ebaf":"code","5524645a":"code","e5f91812":"code","de96e7cd":"code","93d750c5":"code","f5084661":"code","689ec864":"code","4ab47512":"markdown","8d6edde3":"markdown","c95d169e":"markdown","f8ebb72e":"markdown","81fd9fec":"markdown","de3518b5":"markdown","8ee6d34e":"markdown","7449be8e":"markdown","d91784f0":"markdown","0f4dbf31":"markdown","d8196b37":"markdown","385777f4":"markdown","c4a2f557":"markdown","dec10468":"markdown","078f4872":"markdown","46392364":"markdown","871ca147":"markdown","564c14db":"markdown","a89d342f":"markdown","c05f4ed7":"markdown","4c8acce6":"markdown","b057b3da":"markdown","69a52642":"markdown","68079d31":"markdown","be6b10fe":"markdown"},"source":{"f2ec1663":"import sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge ,Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures\nfrom sklearn.metrics import mean_squared_log_error as msle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n%matplotlib inline","2f6c443f":"train=pd.read_csv('..\/input\/bike-sharing-demand\/train.csv')\ntest=pd.read_csv('..\/input\/bike-sharing-demand\/test.csv')","6012450f":"train.head()","1b3d3ece":"test.head()","97942bf2":"train.drop(['casual','registered'],1,inplace=True)","2da40ec9":"train.info()","211d1e80":"train.describe()","1adfff4d":"#from data describtion we can extract categorcal data and numerical data","68ac8dca":"categorical_cols=['season','holiday','workingday','weather']\nnumerical_cols=['temp','atemp','humidity','windspeed']\nlabel='count'","b9820d56":"train.isnull().sum()","94f3d7d5":"train.duplicated().sum()","6d744410":"train['datetime'] = pd.to_datetime(train['datetime'])\ntest['datetime'] = pd.to_datetime(test['datetime'])\ntrain['Month']=pd.DatetimeIndex(train['datetime']).month\ntest['Month']=pd.DatetimeIndex(test['datetime']).month\ntrain['Year']=pd.DatetimeIndex(train['datetime']).year\ntest['Year']=pd.DatetimeIndex(test['datetime']).year\ntrain['WeekDay']=pd.DatetimeIndex(train['datetime']).weekday\ntest['WeekDay']=pd.DatetimeIndex(test['datetime']).weekday\ntrain['Hour']=pd.DatetimeIndex(train['datetime']).hour\ntest['Hour']=pd.DatetimeIndex(test['datetime']).hour\n# train=train.drop(columns=['datetime'])\n# test=test.drop(columns=['datetime'])\ntrain.head()","1637e9c6":"for col in train.drop(columns='count').columns:\n    print(f'For column {col}')\n    print('------------------')\n    print(train[col].value_counts())\n    print('\\n')","49e67c7e":"categorical_cols.extend(['Month','WeekDay','Hour'])","0afd12ec":"# def encodetime(train,test,col,label):\n#     d3=train[[col,label]].groupby(col).mean()\n#     d3.sort_values(by='count',ascending=False)\n#     plt.scatter(x=d3.index,y=d3['count'])\n#     d3=d3.sort_values(by='count')\n#     d3['w']=np.arange(train[col].nunique())\n#     dic=dict(zip(d3.index,d3['w']))\n#     train[col]=train[col].map(dic)\n#     test[col]=test[col].map(dic)\n\n","65f9894e":"enc_col_names = [\"WeekDay\",'Year','Month']\nenc_df = train[enc_col_names]\ntrain.drop(enc_col_names, inplace=True, axis=1)\nenc_df.head()","a351fa01":"from sklearn import preprocessing\n\nord_enc = preprocessing.OrdinalEncoder(dtype=int)\nord_enc_df = pd.DataFrame(ord_enc.fit_transform(enc_df), columns= enc_col_names)\nord_enc_df.head()","603a254c":"train = pd.concat([train, ord_enc_df], axis=1)\ntrain","83227f1c":"# encodetime(train,test,'Year',label)","1cc887c2":"# encodetime(train,test,'Month',label)","b4cc7f0f":"# encodetime(train,test,'Hour',label)","453d34c5":"# encodetime(train,test,'WeekDay',label)","5bd11477":"# def boxplot(x,y,**kwargs):\n#     sns.boxplot(x=x,y=y)\n#     x=plt.xticks(rotation=90)\n# f=pd.melt(train,id_vars=['count'],value_vars=categorical_cols)\n# g=sns.FacetGrid(f,col='variable',col_wrap=2,sharex=False)\n# g.map(boxplot,'value','count')\n\nfig,ax=plt.subplots(figsize=(15,8))\n#Box plot for Temp_windspeed_humidity_outliers\nsns.boxplot(data=train[categorical_cols])\nax.set_title('catigorical data')\nplt.show()","0341cf3b":"fig,ax=plt.subplots(figsize=(15,8))\n#Box plot for Temp_windspeed_humidity_outliers\nsns.boxplot(data=train[numerical_cols])\nax.set_title('numerical_cols')\nplt.show()","6852495b":"sns.pairplot(train[numerical_cols])","d65fbd43":"f, ax = plt.subplots(figsize=(15, 15))\ncorr = train[[*numerical_cols,'count']].corr()\nsns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True)","00313761":"f, ax = plt.subplots(figsize=(15, 15))\ncorr = train.corr()\nsns.heatmap(corr,cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, annot = True)","8d920128":"sns.displot(train[label] , kde=True, height=8.27, aspect=11.7\/8.27)\nsns.displot(np.log(train[label]) , kde=True, height=8.27, aspect=11.7\/8.27)","6b12a898":"def trans(x,l1=0.3,l2=0):\n    if l1!=0:\n        return ((x+l2)**l1-1)\/l1\n    else:\n        return np.log(x+l2)\ndef rev_trans(x,l1=0.3,l2=0):\n    return (x*l1+1)**(1\/l1)-l2\n\nz=train[label].apply(trans)   \nsns.displot(z , kde=True, height=8.27, aspect=11.7\/8.27)","3b58b73b":"#using box cox transform on the label column","54e1016f":"#train=pd.get_dummies(train,columns=['season','weather'])\n#test=pd.get_dummies(test,columns=['season','weather'])","d18a9e0e":"#removing atemp column as it has high correlation with temp column\nx=train.drop(['count','atemp','datetime'],1) \nxtest=test.drop(['atemp','datetime'],1)\ny=train['count']\nxt,xv,yt,yv=train_test_split(x,y,test_size=0.2,random_state=101)","4ff86311":"#redundunt function .. useful if you decide not to use label transformation function\ndef redun(x):\n    return x","9c99d74e":"def mk_model_RF(xt1,xv1,yt,yv,md=None,func1=redun,func2=redun,mss=2,n_est=100,al=0):\n    \n    ytt=yt.apply(func1)\n    yvt=yv.apply(func2)\n    model=RandomForestRegressor(max_depth=md, random_state=0,min_samples_split=mss,n_estimators=n_est,ccp_alpha=al)\n    model.fit(xt1,ytt)\n    ypt=np.apply_along_axis(func2,arr=model.predict(xt1),axis=0)\n    ypv=np.apply_along_axis(func2,arr=model.predict(xv1),axis=0)\n    print('training r2:',r2_score(yt,ypt))\n    print('Validation r2:',r2_score(yv,ypv))\n    print('training rmsle:',np.sqrt(msle(yt,ypt)))\n    print('validation rmsle:',np.sqrt(msle(yv,ypv)))\n    return model","4caa5ab7":"mk_model_RF(xt,xv,yt,yv)","5030ed3f":"mk_model_RF(xt,xv,yt,yv,func1=trans,func2=rev_trans,n_est=500,md=20)","97dd63de":"def mk_model_xgb(xt,xv,yt,yv,func1=redun,func2=redun,lr=1,min_child_weight =25,colsample_bytree = 0.8,md=None):\n    model =XGBRegressor( colsample_bytree = colsample_bytree, learning_rate = lr,min_child_weight =min_child_weight, max_depth=md )\n    ytt=yt.apply(func1)\n    model.fit(xt,ytt)\n    ypt=np.apply_along_axis(func2,arr=model.predict(xt),axis=0)\n    ypv=np.apply_along_axis(func2,arr=model.predict(xv),axis=0)\n    print('training r2:',r2_score(yt,ypt))\n    print('Validation r2:',r2_score(yv,ypv))\n    print('training rmsle:',np.sqrt(msle(yt,ypt)))\n    print('validation rmsle:',np.sqrt(msle(yv,ypv)))\n    return model","a928f302":"_=mk_model_xgb(xt,xv,yt,yv,func1=trans,func2=rev_trans,lr=0.2,min_child_weight =20,colsample_bytree = 0.8,md=20)","8501ebaf":"model=XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.2,min_child_weight =20, max_depth=20).fit(x,y.apply(trans))\nmodel","5524645a":"xtest","e5f91812":"model.predict(xtest)","de96e7cd":"yp=np.apply_along_axis(rev_trans,arr=model.predict(xtest),axis=0)","93d750c5":"plt.hist(yp)","f5084661":"test['count']=yp","689ec864":"test[['datetime', 'count']].to_csv('\/kaggle\/working\/submission.csv', index=False)","4ab47512":"# Encoding time data ordered by mean of label in each category ","8d6edde3":"# Model Training and Testing","c95d169e":"# EDA","f8ebb72e":"# visualizing Catergorical columns","81fd9fec":"## Using Random Forest Regressor","de3518b5":"# prediction Distripution","8ee6d34e":"# visualizing numiric coumns","7449be8e":"# Encoding Weather and Season (One Hot Encoding)","d91784f0":"# Loading Data","0f4dbf31":"# Transforming Label distribution","d8196b37":"## using label transformation gives better rmsle","385777f4":"using Log results in left skewed distribution","c4a2f557":"# Data describtion","dec10468":"## Using XGradient Boosting Regressor","078f4872":"# Spliting Data into Train and Validation","46392364":"XGradient Boosting Regressor yielded better results so we trained the model with whole data using these parameters","871ca147":"# Produce Submission File","564c14db":"seems like there is no abnormal or weird values in the minimum or maximum values","a89d342f":"seems like there is no null values","c05f4ed7":"# Adding timestamp data","4c8acce6":"datetime - hourly date + timestamp\n\nseason -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n\nholiday - whether the day is considered a holiday\n\nworkingday - whether the day is neither a weekend nor holiday\n\nweather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n\ntemp - temperature in Celsius\n\natemp - \"feels like\" temperature in Celsius\n\nhumidity - relative humidity\n\nwindspeed - wind speed\n\ncasual - number of non-registered user rentals initiated\n\nregistered - number of registered user rentals initiated\n\ncount - number of total rentals","b057b3da":"# Applying inverse transformation","69a52642":"we have to drop the 2 columns 'casual' and 'registerd' from the training dataset as they are not present in the test dataset","68079d31":"adding month,weekday and hour data","be6b10fe":"# Team members:"}}