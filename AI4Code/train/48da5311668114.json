{"cell_type":{"6a79490b":"code","04003317":"code","9fa8a526":"code","f82f6254":"code","5c0cc2c3":"code","e9f725cf":"code","1932b904":"code","91ef1718":"code","28f51327":"code","a0750d65":"code","e7281b5e":"markdown","1f9c70fa":"markdown","cd8e05e3":"markdown","f472d465":"markdown","05f9c03d":"markdown","b814c332":"markdown","9d8bd9d2":"markdown","8a1e17d1":"markdown","62d40951":"markdown","3b5f90fc":"markdown","b191c683":"markdown","c9ccce4d":"markdown","f487b983":"markdown"},"source":{"6a79490b":"import math, random, time\nimport numpy as np, pandas as pd, sklearn\nfrom typing import List\n\nfrom dataclasses import dataclass, field\nimport nltk; nltk.download('punkt'); nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom scipy.spatial import distance\n\n!pip install compress_json\nimport pickle, gzip, compress_json\npickle.HIGHEST_PROTOCOL\nimport os, json, urllib.request, tarfile\n\nfrom collections import OrderedDict","04003317":"def getData() -> None:\n    urls = ['https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/comm_use_subset.tar.gz', \n            'https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/noncomm_use_subset.tar.gz', \n            'https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/custom_license.tar.gz', \n            'https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/biorxiv_medrxiv.tar.gz']\n\n    # Create data directory\n    try:\n        os.mkdir('.\/data')\n        print('Directory created!')\n    except FileExistsError:\n        print('Directory already exists!')\n\n    #Download all files\n    print('Downloading CORD-19 database...')\n    \n    for i in range(len(urls)):\n        urllib.request.urlretrieve(urls[i], '.\/data\/file'+str(i)+'.tar.gz')\n        print('Downloaded file '+str(i+1)+'\/'+str(len(urls)))\n        tar = tarfile.open('.\/data\/file'+str(i)+'.tar.gz')\n        tar.extractall('.\/data')\n        tar.close()\n        print('Extracted file '+str(i+1)+'\/'+str(len(urls)))\n        os.remove('.\/data\/file'+str(i)+'.tar.gz')\n        \n    print('Done!')","9fa8a526":"@dataclass\nclass Document:\n    \"\"\"Data structure to store and interact with CORD-19 documents.\"\"\"\n\n    paper_ID : str\n    title    : str = ''\n    authors  : list = field(default_factory=list)\n    text     : str = ''\n\n    def get_freq(self, word : str) -> int:\n        \"\"\"Returns the frequency of a given word in the text.\"\"\"\n        return self.text.count(word)\n\n    def vectorize(self, tokenizer, stopwords, dim : int) -> List[str]:\n        \"\"\"Creates a string vector of a given dimension.\"\"\"\n        # tokenizer :: str -> List<str>\n        words = tokenizer(self.text)\n        # Remove duplicate words to reduce computation time\n        words = list(set(words))\n        # Lowercase in lieu of stemming\n        for word in words:\n            word.lower()\n        # Remove duplicates\n        words = list(set(words))\n\n        # Remove all punctuation, numeric characters, etc.\n        def valid_word(word, stopwords) -> bool:\n            \"\"\"Determines if a word should be included in the string vector.\"\"\"\n            return not(word in stopwords) and (word.isalpha())\n        words = [word for word in words if valid_word(word, stopwords)]\n        \n        # Sort all words by descending frequency\n        words.sort(reverse=True, key=lambda w : self.get_freq(w))\n\n        # Take only the first 'dim' words, with the paper ID\n        words.insert(0, self.paper_ID)\n        return words[:dim + 1]","f82f6254":"def extract() -> dict:\n    \"\"\"extract :: None -> dict\n    \n       > Extracts the contents of the CORD-19 database\n       > Represents all data as a 'Document' object\n       > Returns cumulative dictionary\n    \"\"\"\n\n    # Initialize empty dictionary\n    docs = {}\n\n    print('Parsing documents...'); ct = 0\n\n    #Iterate through all files in the data directory\n    for subdir, dirs, files in os.walk('.\/data'):\n        for file in files:\n            ct += 1\n            if ct == 11000:\n                print('1\/3 of corpus parsed.')\n            if ct == 22000:\n                print('2\/3 of corpus parsed.')\n            with open(os.path.join(subdir, file)) as f:\n                data_extract = json.load(f)\n                \n                paper_ID = data_extract['paper_id']\n                title    = data_extract['metadata']['title']\n                authors  = data_extract['metadata']['authors']\n                text     = ''\n\n                try:\n                    for body in data_extract['abstract']:\n                        text += body['text']\n                except:\n                    pass\n                for body in data_extract['body_text']:\n                    text += body['text']\n                # Append the abstract and paper content together for vocabulary\n\n                # Add the current document to the running dictionary\n                docs[paper_ID] = Document(paper_ID, title, authors, text)\n\n    print('Done!')\n    return docs","5c0cc2c3":"class InvertedIndex:\n    \"\"\"Word-level inverted index data structure.\"\"\"\n    \n    def __init__(self, tokenizer, stopwords : list=[]):\n        \n        self.tokenizer = tokenizer\n        self.stopwords = set(stopwords)\n        \n        self.index = {}\n        # key : word, val : List<paper_ID>\n\n        # Used for memoizing the size of document frequencies\n        self.intersections = {}\n        \n    def add_doc(self, doc : Document):\n        \"\"\"Incorporates the text of a document into the index.\"\"\"\n\n        # tokenizer :: str -> List<str>\n        words = self.tokenizer(doc.text)\n        #Lowercase in lieu of stemming\n        for word in words:\n            word.lower()\n        ID = doc.paper_ID\n        position = 0\n\n        for word in words:\n            if word in self.stopwords:\n                pass\n            elif not(word in self.index.keys()):\n                self.index[word] = [ID]\n            else: \n                self.index[word].append(ID)\n                position += 1\n\n    def freq_table(self) -> None:\n        \"\"\"Store the frequency of each word in a dict.\"\"\"\n\n        self.freq = {}\n        for word in self.index.keys():\n            self.freq[word] = len(self.index[word])","e9f725cf":"class Indexed_K_Means:\n    \"\"\"Data structure for modified K-Means clustering algorithm.\"\"\"\n\n    def __init__(self, k=10, max_iter=25):\n        self.num_clusters = k\n        self.max_iter = max_iter\n\n    def seeds(self, data):\n        \"\"\"Choose random centroids to start K-Means.\"\"\"\n        return random.sample(data, self.num_clusters)\n\n    def sem_sim(self, w1: str, w2 : str, idx) -> float:\n        \"\"\"Computes the semantic similarity of two words.\"\"\"\n\n        # Don't want to include stopwords\n        if (w1 not in idx.index) or (w2 not in idx.index):\n            return 0\n\n        # Check memoization array for precomputations\n        if w1 not in idx.intersections.keys():\n            idx.intersections[w1] = {}\n        if w2 not in idx.intersections[w1].keys():\n            # If this is the first time comparing w1\/w2, compute the intersect\n            w1_docs = idx.index[w1]; w2_docs = idx.index[w2]\n            idx.intersections[w1][w2] = len(w1_docs.intersection(w2_docs))\n\n        # Return the sem sim using index attributes\n        return 2 * idx.intersections[w1][w2] \/ idx.freq[w1] + idx.freq[w2]\n\n    def sim(self, vec1 : list, vec2 : list, idx) -> float:\n        \"\"\"Computes the similarity between two string vectors.\"\"\"\n        s = 0; dim = min(len(vec1), len(vec2))\n        for i in range(1, dim):\n            # Sum the sem similarity of each component in the vectors\n            s += self.sem_sim(vec1[i],vec2[i], idx)\n\n        # Normalize to dimension of vector\n        return ((1\/dim) * s)\n\n    def fit(self, data, idx):\n        \"\"\"Clusters given data into num_clusters.\"\"\"\n\n        print('Training K-Means model...')\n\n        # Randomly select seed centroids\n        self.centroids = self.seeds(data)\n        print('Seed found!')\n\n        # Create initial prototype clusters\n        self.classifications = {}\n        for label in range(self.num_clusters):\n            self.classifications[label] = []\n\n        # Iterate algorithm for specified number of times\n        for i in range(self.max_iter):\n\n            print('Cluster round '+str(i))\n\n            self.make_clusters(data, idx)\n\n            self.update_centroids()\n\n        print('Done!')\n\n    def make_clusters(self, data, idx) -> None:\n\n        for vec in data:\n            # Keep track of running maximum similarity\n            if len(vec) <= 49:\n                continue\n            best_sim = -math.inf; best_label = 0\n\n            # Check each centroid to see if it's most similar\n            for i in range(self.num_clusters):\n                curr_sim = self.sim(vec, self.centroids[i], idx)\n                if curr_sim > best_sim:\n                    # Update values\n                    best_sim = curr_sim\n                    best_label = i\n            # Add vector to list by cluster label\n            clus = self.classifications[best_label]\n            clus.append(vec)\n            self.classifications[best_label] = clus\n\n    def update_centroids(self) -> None:\n        \"\"\"Selects new mean centroids based on current clusters.\"\"\"\n\n        # Delete all previous centroids\n        prev_centroids = self.centroids.copy()\n        self.centroids.clear()\n        for cluster in self.classifications.keys():\n            if len(self.classifications[cluster]) == 0:\n                new_cent = []\n                new_cent.append(prev_centroids[cluster])\n\n            else:\n                new_cent = []\n                dim = 50\n                for i in range(1, dim):\n                    # Randomly sample each component from the cluster's vectors\n                    vec = random.sample(self.classifications[cluster], 1)[0]\n                    new_cent.append(vec[i])\n            # Add the new centroid to the running list\n            self.centroids.append(new_cent)\n\n    def predict(self) -> int:\n        \"\"\"Returns list of predicted cluster labels.\"\"\"\n        \n        # Pretty self-explanatory, but I'm glad you're reading the comments!\n        return self.classifications","1932b904":"def organize(doc_list : List[List[str]]):\n    \"\"\"\n       Vectorizes, clusters and creates topic models for documents in \n       the corpus.\n    \"\"\"\n\n    # Instantiate the Index data structure\n    idx = InvertedIndex(\n        nltk.word_tokenize,\n        nltk.corpus.stopwords.words('english'))\n    \n    ### IF YOU WANT TO RECOMPILE THE INVERTED INDEX, UNCOMMENT BELOW UNTIL 'END'\n    #\n    print('Compiling inverted index...')\n    start_time = time.time()\n    \n    ct = 0\n    for doc in doc_list.values():\n        print(ct)\n        idx.add_doc(doc)\n        ct += 1\n    \n    print(\"Done! Indexing took %s seconds. ---\" % (time.time() - start_time))\n    \n    # Save index in Pickle for later reference (no recomputation)\n    idx_file = gzip.open('idx_file', 'ab') \n    pickle.dump(idx.index, idx_file)                      \n    idx_file.close()\n    #\n    ### END ####################################################################\n\n    # Read inverted index from Pickle\n    print('Importing inverted index...')\n    idx_file = gzip.open('idx_file', 'rb')\n    inv_index = pickle.load(idx_file)\n    idx.index = inv_index\n    print('Done!')\n\n    # Record the frequency of each word\n    idx.freq_table()\n    # Convert word usage to sets for intersection calculation\n    for word in idx.index.keys():\n        idx.index[word] = set(idx.index[word])\n\n    ### IF YOU WANT TO RECOMPILE THE STRING VECTORS, UNCOMMENT BELOW UNTIL 'END'\n    #\n    print('Vectorizing documents...')\n    vec_list = []; ct = 0\n    # Compile all feature vectors for documents in corpus\n    for doc in doc_list.values():\n        vec_list.append(doc.vectorize(\n            nltk.word_tokenize,\n            nltk.corpus.stopwords.words('english'),\n            50)) # dim = 50\n        print(ct); ct += 1\n    print('Done!')\n    #\n    vector_file = open('vectors', 'w')\n    json.dump(vec_list, vector_file)                      \n    vector_file.close()\n    #\n    ### END ####################################################################\n\n    # Import precomputed vectors from JSON\n    print('Importing string vectors...')\n    vector_file = open('vectors', 'r')\n    vecs = json.load(vector_file)\n    vec_list = vecs\n    print('Done!')\n\n    ### IF YOU WANT TO RECOMPILE THE CLUSTERS, UNCOMMENT BELOW UNTIL 'END'\n    #\n    print('Clustering string vectors...')\n    start_time = time.time()\n    # Cluster data using K-Means\n    KMeans = Indexed_K_Means(max_iter=25)\n    KMeans.fit(vec_list, idx)\n    # Attach labels to string vectors\n    clusters = KMeans.predict()\n    print(\"Done! Clustering took %s seconds. ---\" % (time.time() - start_time))\n    \n    cluster_file = open(\"cluster_file.json.gz\", 'wt') \n    compress_json.dump(clusters, \"cluster_file.json.gz\")                      \n    cluster_file.close()\n    #\n    ### END ####################################################################\n\n    # Import precomputed clusters from JSON\n    print('Importing clusters...')\n    cluster_file = gzip.open(\"cluster_file.json.gz\", 'rt')\n    clusters = json.load(cluster_file)\n    print('Done!')\n\n    # Compile the keywords for each cluster, store in a dictionary\n    # Keyed by cluster ID, values are lists of vectors\n    keywords = {}\n    for ID, cluster in clusters.items():\n        keywords[ID] = []\n        for vec in cluster:\n            keywords[ID].extend(vec[1:])\n        # Remove all duplicate keywords\n        keywords[ID] = list(OrderedDict.fromkeys(keywords[ID]))\n\n    return idx, keywords, clusters","91ef1718":"def retrieve(keywords, clusters, queries, idx):\n    \"\"\"Retrieves the top 100 documents and sorts by similarity.\"\"\"\n    \n    def sem_sim(w1: str, w2 : str, idx) -> float:\n        \"\"\"Computes the semantic similarity of two words.\"\"\"\n\n        # If a query term is not seen anywhere in the vocabulary, return 0 rank.\n        #\n        # In the worst case, the query will be complete nonsense, and the model\n        # will just return 100 results which may or may not be correlated.\n        if (w1 not in idx.index) or (w2 not in idx.index):\n            return 0\n\n        if w1 not in idx.intersections.keys():\n            idx.intersections[w1] = {}\n        if w2 not in idx.intersections[w1].keys():\n            w1_docs = idx.index[w1]; w2_docs = idx.index[w2]\n            idx.intersections[w1][w2] = len(w1_docs.intersection(w2_docs))\n\n        return 2 * idx.intersections[w1][w2] \/ idx.freq[w1] + idx.freq[w2]\n\n    def word_sim(q : str, vec : List[str], idx, doc_level : bool=False) -> float:\n        \"\"\"Computes the similarity between two string vectors.\"\"\"\n\n        # 'doc_level' refers to whether or not we are calculating relevance with\n        # respect to cluster keywords or document vectors. In the case of \n        # document-level ranking, we want to weight less frequent words more \n        # highly.\n\n        if (len(q) == 0) or (len(vec) == 0):\n            return 0\n\n        # Measure the edit distance between each vector\n        s = 0; dim = len(vec)\n        for i in range(0, dim):\n            if doc_level:\n                # Weight less frequent words\n                weight = (i + 1 \/ dim)\n            else:\n                # No inherent ordering in words, equal weight\n                weight = 1\n            # Return the weighted similarity\n            s += sem_sim(q,vec[i], idx) * weight\n\n        return ((1\/(dim + 1)) * s)\n\n    # Initialize list for return\n    result_list = []\n\n    # Iterate through each query in the list\n    for q in queries:\n        \n        # Preprocess the query into separate, stemmed words\n        q_words = nltk.word_tokenize(q)\n        for word in q_words:\n            word.lower()\n        for stop in nltk.corpus.stopwords.words('english'):\n            if stop in q_words:\n                q_words.remove(stop)\n\n        # Method of ranking relevancy\n        def rank(strings : List[str], idx, doc_level : bool=False):\n            r = 0\n            for word in q_words:\n                r += word_sim(word, strings, idx, doc_level)\n            return r\n\n        # Sort each cluster by relevancy of its keywords\n        relevant = list(keywords.keys())\n        relevant.sort(\n            reverse=True, key=lambda x : rank(keywords[x], idx))\n\n        n = 0\n        # Iterate until all 100 results are compiled\n        print('Compiling results...')\n        while n < 100:\n            counter = 0\n            # Sort the docs in each cluster by relevance to query\n            clusters[relevant[counter]].sort(\n                reverse=True, key=lambda x : rank(x[1:], idx))\n            docs = clusters[relevant[counter]]\n            # If there are not enough results in the current cluster...\n            if len(relevant[counter]) < (100 - n):\n                result_list.extend([doc[0] for doc in docs])\n                # ...move on to the next cluster\n                counter += 1\n                # If there ARE enough, just add all of them\n            else:\n                result_list.extend([doc[0] for doc in docs[:(100 - n)]])\n            n = len(result_list)\n\n        # Print results\n        print_results(q, result_list)\n        # Add query result to running list\n        result_list.append(result_list)\n\n    # Return list of lists of results\n    return result_list","28f51327":"def print_results(query : str, results : List[str]) -> None:\n    \"\"\"Prints query results in readable format.\"\"\"\n\n    print('Your search for \"' + query + '\" returned the following results:')\n    for i in range(100):\n        print(str(i+1)+'. ' + results[i])","a0750d65":"def main():\n\n    #Initialize the random seed for ML applications\n    random.seed(1595)\n\n    # Load the CORD-19 database\n    getData()\n\n    # Extract relevant info and create list of Docs\n    doc_list = extract()\n\n    # Extract and organize the data\n    idx, keywords, clusters = organize(doc_list)\n\n    # ========================= INPUT QUERIES HERE ============================#\n    q = ['coronavirus origin',\n         'coronavirus response to weather changes',\n         'coronavirus immunity'\n    ]\n    #==========================================================================#\n\n    # Information retrieval given queries\n    results = retrieve(keywords, clusters, q, idx)\n\nmain()","e7281b5e":"# Organization\n___\n\n`organize` simply instantiates and trains the index and clustering algorithm, saving the results in JSON format for later reference. This prevents us from having to retrain every time we search.","1f9c70fa":"# Data Extraction\n___\n\nNow that we have the Document data structure implemented, we simply iterate over the corpus of documents and instantiate it as a Python object. `extract` initializes an empty dictionary and populates it with each document, keyed by the paper ID. It returns the full dictionary, allowing constant lookup time for document attributes as long as we have its ID.","cd8e05e3":"# Loading the Data\n___\n\nHere we parse the JSON contents of the CORD-19 database through Amazon Web Services. We save each of the ~33,000 documents in a newly created data directory. Every stage of our code includes status messages to update the user on runtime progress.","f472d465":"# Retrieval - Vector Space Model\n___\n\nTo rank our search results, we simply tokenize the query and compute its similarity to all of the vectors in the string-vector decision space. The results are sorted, the top 100 most relevant are chosen to be presented, and returned in a list of lists.","05f9c03d":"# Printing and Presenting Results\n___\n\nWe included a simple function to print the search results in an aesthetically pleasing manner. It ranks the results from 1-100.","b814c332":"# Thank you!\n\nThanks for checking out our project! We appreciate all feedback you might have!","9d8bd9d2":"# Indexing\n___\n\nWe also implemented a word-level inverted index as a Python class. The index itself is a dictionary stored as an attribute of the class, while other methods provide greater functionality.\n\n* `add_doc` takes a single Document and incorporates all of its text into the index, saving each occurrence of a word in the relevant space in the `self.index` dictionary. It tokenizes the text into words, applies a blanket lowercase, and removes all English stopwords from NLTK.\n* `freq_table` constructs a new dictionary to store all of the document frequencies of each word. Not only does this turn what would be a linear computation into constant-time lookup, but it allows us to transmute the index dictionary in ways that will aid us later on down the line.","8a1e17d1":"## CORDsearch: Clustering CORD-19 by Inverted Index-Modified K-means\nDeveloped by Cassi Larose, Paula Choconta and Connor Jordan for BIOL1595\/2595 @ Brown University.\n___\n\n### Overview & Acknowledgement\nCORDsearch utilizes a variant of K-Means Clustering augmented for use on string-vectors. This method is largely pioneered by Taeho Jo of Hongik University, and is directly adapted from his 2008 paper [\"Inverted Index based Modified Version of K-Means Algorithm for Text Clustering\"](https:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.148.1963&rep=rep1&type=pdf).\n\nCORDsearch takes plain-text queries as input and returns the top 100 most relevant search results from the COVID-19 Open Research Database (CORD-19). It utilizes a pre-computed inverted index, pre-trained K-Means Clustering model, and pre-trained LDA topic model to group documents by relevancy.","62d40951":"# Dependencies","3b5f90fc":"# String Vectors - What Are They?\n___\n\nThe difficulty of clustering typical numerical vectors is the so-called 'curse of dimensionality'; the more total information we account for, the sparser each vector becomes and the less information any individual vector can convey. Numerical vectors used in the K-Means Algorithm reserve each dimension for a given term, and for each vector the corresponding component is its TF-IDF score.\n\nString vectorization takes a different, more abstract approach to defining the decision space. Each basis vector, instead of a constant term, is a ranking of document frequency. The components of each vector are the most frequent words in the document, sorted by that frequency. These vectors are extremely dense, allowing us to choose our dimension and cut down on runtime while still maintaining a reasonable vector space.\n___\n\nHowever, this means we need to define a new, robust decision metric and processes to facilitate K-Means on this new type of vector.\n\n## Semantic Similarity\n___\n\nUsing document frequency for each word, ($df(w)$), we can define the *semantic similiarity* between two words as\n$$ss(w_i, w_j) = \\frac{2df(w_i, w_j)}{df(w_i) + df(w_j)},$$\nwhere $df(w_i, w_j)$ represents the number of times both words occur in the same document.\n\nThis allows us to define another metric, *vector similarity*, between two string vectors:\n$$sim(\\frac{1}{d}\\sum_{k=1}^d w_{ik}, w_{jk}).$$\n\nBecause vector similarity obeys the triangle inequality, it is a robust distance metric over our decision space! The downside is its complexity, even with the pre-computed index at our fingertips. Calculating the numerator of semantic similarity, the overlap between two documents, requires us to turn the occurrence lists of both words into sets, removing duplicates, and finding the length of the intersection of those two sets. Doing this hundreds of thousands of times is not practical and would make training take over 15 hours. So, we memoize!\n\n## Memoization\n___\n\nMemoization involves creating an empty array and filling it in whenever you finish a subtask. That way, if you've already computed something, you can retrieve that result in constant lookup time. For `sem_sim`, our implementation of semantic similarity, we store calculations in the `idx.intersections` 2D array, which is really a nested dictionary. This storage makes computation extremely efficient and actually gets faster over time.","b191c683":"# Document Extraction\n___\n\nWe define our own 'Document' data structure to better organize the dataset and introduce additional functionality. They're implemented as a dataclass for the sake of readability, and come packaged with two methods to be utilized later.\n* `get_freq` takes a word as input and returns its frequency in the document.\n* `vectorize` converts a given document into a 'string vector' as detailed in the Jo paper linked in the overview. This process will be elaborated upon soon.","c9ccce4d":"# Modified K-Means\n___\n\nWe can use vector similarity as our method of clustering string vectors. The only question left is how we compute our centroids, the 'average' vectors that define each cluster. We employed a stochastic method, randomly selecting each $i$th component of the 'mean' vector from the $i$th index of all the vectors in that cluster.\n![image.png](attachment:image.png)\nAfter fitting the clusters to our data set, we can return the clusters in dictionary form, keyed by their cluster ID.","f487b983":"# Main Method\n___\n\nHere's where it all comes together! We simply call all of the functions we defined above to cluster and rank a given list of queries."}}