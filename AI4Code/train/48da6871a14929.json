{"cell_type":{"14276f6b":"code","46c3156b":"code","5e45c02c":"code","86771cbc":"code","f553f1a9":"code","f14b25fd":"code","de1bf7f0":"code","54cdd062":"code","04387cca":"code","c69fe548":"code","7526ab9b":"markdown","8e8c05b6":"markdown","b4906974":"markdown","07be2e63":"markdown","4bd53612":"markdown","a6e51daf":"markdown","ba10289c":"markdown","7196e5f2":"markdown"},"source":{"14276f6b":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","46c3156b":"# Reading the dataset\nraw_train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\nraw_test = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\n\ntrain = raw_train.drop(['id','target'], axis = 1)\ntest = raw_test.drop('id', axis = 1)\n\ntarget = raw_train.target\nid_train = raw_train.id\nid_test = raw_test.id","5e45c02c":"selected_features = ['f34', 'f71', 'f55', 'f80', 'f8', 'f27', 'f43', 'f91', 'f50', 'f22', 'f41', 'f97', 'f25', 'f57', 'f81', 'f96', 'f66', 'f17', 'f44', 'f21', 'f30', 'f60', 'f9', 'f54', 'f40', 'f42', 'f62', 'f94', 'f49', 'f47', 'f82', 'f26', 'f32', 'f79', 'f19', 'f20', 'f31', 'f11', 'f33', 'f10', 'f7', 'f90', 'f24', 'f3', 'f63', 'f37', 'f23', 'f13', 'f84', 'f67', 'f70', 'f12', 'f58', 'f16', 'f4', 'f51', 'f56', 'f46', 'f2', 'f1', 'f89', 'f69', 'f53', 'f74', 'f0', 'f64', 'f99', 'f92']\nprint(f\"Selected Features: {selected_features}\")","86771cbc":"# The number 2 is just a threshold to split\ndata = train[selected_features].copy()\nh_skew = data.loc[:,data.skew() >= 2].columns  # with Skewed \nl_skew = data.loc[:,data.skew() < 2].columns   # Bimodal\n\n# Skewed distrubutions\ntrain['median_h'] = train[h_skew].median(axis=1)\ntest['median_h'] = test[h_skew].median(axis=1)\n\ntrain['var_h'] = train[h_skew].var(axis=1)\ntest['var_h'] = test[h_skew].var(axis=1)\n\n# Bimodal distributions\ntrain['mean_l'] = train[l_skew].mean(axis=1)\ntest['mean_l'] = test[l_skew].mean(axis=1)\n\ntrain['std_l'] = train[l_skew].std(axis=1)\ntest['std_l'] = test[l_skew].std(axis=1)\n\ntrain['median_l'] = train[l_skew].median(axis=1)\ntest['median_l'] = test[l_skew].median(axis=1)\n\ntrain['skew_l'] = train[l_skew].skew(axis=1)\ntest['skew_l'] = test[l_skew].skew(axis=1)\n\ntrain['max_l'] = train[l_skew].max(axis=1)\ntest['max_l'] = test[l_skew].max(axis=1)\n\ntrain['var_l'] = train[l_skew].var(axis=1)\ntest['var_l'] = test[l_skew].var(axis=1)\n\nraw_train = train.copy()\nraw_test = test.copy()","f553f1a9":"# Scaling and Nomalization\ntransformer_high_skew = make_pipeline(\n    StandardScaler(), \n    MinMaxScaler(feature_range=(0, 1))\n)\n\ntransformer_low_skew = make_pipeline(\n    StandardScaler(),\n    MinMaxScaler(feature_range=(0, 1))\n)\n\nnew_cols = train.columns[-8:]\nh_skew = train.iloc[:,:100].loc[:, train.skew() >= 2].columns\nl_skew = train.iloc[:,:100].loc[:, train.skew() < 2].columns\n\ntransformer_new_cols = make_pipeline(\n    StandardScaler(),\n    MinMaxScaler(feature_range=(0, 1))\n)\n\npreprocessor = make_column_transformer(\n    (transformer_high_skew, l_skew),\n    (transformer_low_skew, h_skew),\n    (transformer_new_cols, new_cols),\n)","f14b25fd":"# Some parameters to config \nEPOCHS = 840\nBATCH_SIZE = 2048 \nACTIVATION = 'swish'\nLEARNING_RATE = 0.000265713\nFOLDS = 10","de1bf7f0":"# Seed \nmy_seed = 42\ndef seedAll(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \nseedAll(my_seed)\n\n# -----------------------------------------------------------------\ndef load_model(name:str):\n    early_stopping = callbacks.EarlyStopping(\n        patience=20,\n        min_delta=0,\n        monitor='val_loss',\n        restore_best_weights=True,\n        verbose=0,\n        mode='min', \n        baseline=None,\n    )\n\n    plateau = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=7, \n            verbose=0,\n            mode='min')\n\n    model = keras.Sequential([\n        layers.Dense(96, activation = ACTIVATION, input_shape = [train.shape[1]]),      \n        layers.Dense(48, activation =ACTIVATION), \n        layers.Dense(32, activation =ACTIVATION),\n        layers.Dense(1, activation='sigmoid'),\n    ])\n\n    model.compile(\n        optimizer= keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n        loss='binary_crossentropy',\n        metrics=['AUC'],\n    )\n    \n    return model, early_stopping, plateau","54cdd062":"preds_valid_f = {}\npreds_test = []\ntotal_auc = []\nf_scores = []\n\nkf = StratifiedKFold(n_splits=FOLDS,random_state=0,shuffle=True)\nfor fold,(train_index, valid_index) in enumerate(kf.split(train,target)):\n    X_train,X_valid = train.loc[train_index], train.loc[valid_index]\n    y_train,y_valid = target.loc[train_index], target.loc[valid_index]\n\n    # Preprocessing\n    index_valid  = X_valid.index.tolist()\n    test  = raw_test.copy()\n    \n    X_train = preprocessor.fit_transform(X_train)\n    X_valid = preprocessor.transform(X_valid)\n    test = preprocessor.transform(test)\n      \n    # Model\n    model, early_stopping, plateau  = load_model('version1')\n    history = model.fit(  X_train, y_train,\n                validation_data = (X_valid, y_valid),\n                batch_size = BATCH_SIZE, \n                epochs = EPOCHS,\n                callbacks = [early_stopping, plateau],\n                shuffle = True,\n                verbose = 0\n              )\n    preds_valid = model.predict(X_valid).reshape(1,-1)[0] \n    preds_test.append(model.predict(test).reshape(1,-1)[0])\n    \n    #  Saving  scores to plot the end  \n    scores = pd.DataFrame(history.history)\n    scores['folds'] = fold\n    if fold == 0:\n        f_scores = scores \n    else: \n        f_scores = pd.concat([f_scores, scores], axis  = 0)\n        \n    # Concatenating valid preds\n    preds_valid_f.update(dict(zip(index_valid, preds_valid)))\n\n    # Getting score for a fold model\n    fold_auc = roc_auc_score(y_valid, preds_valid)\n    print(f\"Fold {fold} roc_auc_score: {fold_auc}\")\n\n    # Total auc\n    total_auc.append(fold_auc)\n\nprint(f\"mean roc_auc_score: {np.mean(total_auc)}, std: {np.std(total_auc)}\")","04387cca":"for fold in range(f_scores['folds'].nunique()):\n    history_f = f_scores[f_scores['folds'] == fold]\n\n    fig, ax = plt.subplots(1, 2, tight_layout=True, figsize=(14,4))\n    fig.suptitle('Fold : '+str(fold), fontsize=14)\n        \n    plt.subplot(1,2,1)\n    plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n    plt.legend(fontsize=15)\n    plt.grid()\n    \n    plt.subplot(1,2,2)\n    plt.plot(history_f.loc[:, ['auc', 'val_auc']],label= ['auc', 'val_auc'])\n    plt.legend(fontsize=15)\n    plt.grid()\n    \n    print(\"Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()));","c69fe548":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsub['target'] = np.mean(preds_test, axis = 0)\nsub.to_csv('submission.csv', index=False)\nsub.head()","7526ab9b":"# Deep Learning Model optimization\n\n## Credits\n* Model idea goes to: https:\/\/www.kaggle.com\/javiervallejos\/simple-nn-with-good-results-tps-nov-21\n* First improvements made by: https:\/\/www.kaggle.com\/adityasharma01\/simple-nn-tps-nov-21  \n* Great feature engineering: https:\/\/www.kaggle.com\/sfktrkl\/tps-nov-2021-nn-2\n\n## What's the improvement?\nOptimization of the model size and the number of folds.\n","8e8c05b6":"# Importing Libraries and Loading datasets","b4906974":"# Neural Network","07be2e63":"# Outcomes","4bd53612":"## Mutual information\n\nThis work were made by **\u015eAFAK T\u00dcRKELI** \ud83d\udc4f","a6e51daf":"# Model","ba10289c":"## Split the dataset and add new columns","7196e5f2":"## Feature Engineering\n\nSplit the dataset by distribution of each column and add some basic columns (mea, std, var, mean, etc)."}}