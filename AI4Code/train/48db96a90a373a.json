{"cell_type":{"0085f3c1":"code","f8d8a595":"code","a7ff69c1":"code","61f0c393":"code","5c2fd920":"code","2104cf1b":"code","53b508b8":"code","44202865":"code","0a2abf5f":"code","7b66b8ba":"code","ccfb4b23":"code","f1b447c8":"code","5ec9f8cc":"code","f011577b":"code","5af75261":"code","ce0428ac":"code","f2e8e59e":"code","25bdbece":"code","40d56384":"code","3d6b01bf":"code","c7d15806":"markdown","4a91fb8b":"markdown","6a50a2c3":"markdown","b5d30d21":"markdown","7e1c009e":"markdown","28fb3244":"markdown","49407fbc":"markdown","cb81448c":"markdown","a8e0eb24":"markdown","4194c28a":"markdown","f3a911a0":"markdown","7569f700":"markdown","b22c24e2":"markdown","fcb8bf6b":"markdown","ebdcea62":"markdown","2005d6cd":"markdown","688f5fc7":"markdown","95f0554d":"markdown","d8e317a4":"markdown","e62d5805":"markdown","51d676e5":"markdown","9274c142":"markdown","e634e2e6":"markdown","81ad12a6":"markdown","a0ce8923":"markdown","938d1c74":"markdown","bfecff3d":"markdown","f889605e":"markdown","5476b9b5":"markdown","6828ed93":"markdown","7925bf03":"markdown","fa905f9e":"markdown","b01017bb":"markdown","04162487":"markdown","fb65fb3f":"markdown"},"source":{"0085f3c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8d8a595":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nimport itertools\nimport os\nimport shutil\nimport random\nimport glob\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n%matplotlib inline","a7ff69c1":"train_path = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set'\n# valid_path = 'data\/dogs-vs-cats\/valid'\ntest_path = '..\/input\/cat-and-dog\/test_set\/test_set'","61f0c393":"train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=10)\ntest_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=10, shuffle=False)","5c2fd920":"imgs, labels = next(train_batches)\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","2104cf1b":"plotImages(imgs)\nprint(labels)    ","53b508b8":"vgg16_model = tf.keras.applications.vgg16.VGG16()","44202865":"vgg16_model.summary()","0a2abf5f":"type(vgg16_model)","7b66b8ba":"model = Sequential()\nfor layer in vgg16_model.layers[:-1]:\n    model.add(layer)","ccfb4b23":"for layer in model.layers:\n    layer.trainable = False","f1b447c8":"model.add(Dense(units=2, activation='softmax'))","5ec9f8cc":"model.summary()","f011577b":"model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])","5af75261":"model.fit(x = train_batches, \n          steps_per_epoch = len(train_batches),\n          epochs = 5,\n          verbose = 2\n         )","ce0428ac":"test_imgs, test_labels = next(test_batches)\nplotImages(test_imgs)\nprint(test_labels)","f2e8e59e":"predictions = model.predict(x = test_batches, steps = len(test_batches), verbose = 0)","25bdbece":"# def plot_confusion_matrix(cm, classes,\n#                           normalize=False,\n#                           title='Confusion matrix',\n#                           cmap=plt.cm.Blues):\n#     \"\"\"\n#     This function prints and plots the confusion matrix.\n#     Normalization can be applied by setting `normalize=True`.\n#     \"\"\"\n#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n#     plt.title(title)\n#     plt.colorbar()\n#     tick_marks = np.arange(len(classes))\n#     plt.xticks(tick_marks, classes, rotation=45)\n#     plt.yticks(tick_marks, classes)\n\n#     if normalize:\n#         cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n#         print(\"Normalized confusion matrix\")\n#     else:\n#         print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n#     thresh = cm.max() \/ 2.\n#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n#         plt.text(j, i, cm[i, j],\n#             horizontalalignment=\"center\",\n#             color=\"white\" if cm[i, j] > thresh else \"black\")\n\n#     plt.tight_layout()\n#     plt.ylabel('True label')\n#     plt.xlabel('Predicted label')","40d56384":"# test_batches.class_indices","3d6b01bf":"cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\ncm_plot_labels = ['cat','dog']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","c7d15806":"Let's see how to train the fine-tuned VGG16 model to classify images as cats or dogs.","4a91fb8b":"Let's first check out a batch of training data using the plotting function","6a50a2c3":"to be continue...\nwill share next kernal publically soon\n\nlink to next [part](https:\/\/www.kaggle.com\/bavalpreet26\/keras4\/edit)","b5d30d21":"In contrast, recall how much simpler the CNN was that we worked with in the last [notebook](https:\/\/www.kaggle.com\/bavalpreet26\/cnn-tutorial-keras-nb2). VGG16 is much more complex and sophisticated and has many more layers than our previous model.","7e1c009e":"we\u2019ll use the `Adam` optimizer with a learning rate of `0.0001`, `categorical_crossentropy` as our loss, and `\u2018accuracy\u2019` as our metric.","28fb3244":"We\u2019re now going to create a confusion matrix so we can visualize our predictions.","49407fbc":"Now see how to use the fine-tuned VGG16 model that we trained above to predict on images of cats and dogs in our test set.","cb81448c":"![image](https:\/\/neurohive.io\/wp-content\/uploads\/2018\/11\/vgg16.png)","a8e0eb24":"To understand what preprocessing is needed for images that will be passed to a VGG16 model, we can look at the [VGG16 paper](https:\/\/arxiv.org\/pdf\/1409.1556.pdf).","4194c28a":"# Part - 2","f3a911a0":"The pre-trained model we'll be working with to classify images of cats and dogs is called VGG16, which is the model that won the 2014 ImageNet competition.\n\nIn the ImageNet competition, multiple teams compete to build a model that best classifies images from the ImageNet library. The ImageNet library houses thousands of images belonging to 1000 different categories.\n\nWe\u2019ll import this VGG16 model and then fine-tune it using Keras. The fine-tuned model will not classify images as one of the 1000 categories for which it was trained on, but instead it will only work to classify images as either cats or dogs.\n\nNote that dogs and cats were included in the ImageNet library from which VGG16 was originally trained. Therefore, the model has already learned the features of cats and dogs. Given this, the fine-tuning we'll do on this model will be very minimal","7569f700":"Now, we have replicated the entire `vgg16_model` (excluding the output layer) to a new `Sequential` model, which we've just given the name `model`.\n\nNext, we\u2019ll iterate over each of the layers in our new `Sequential` model and set them to be <font color='red'>non-trainable<\/font>. This freezes the weights and other trainable parameters in each layer so that they will not be trained or updated when we later pass in our images of cats and dogs.","b22c24e2":"For now, we\u2019re going to go through a process to convert the `Functional` model to a `Sequential` model, so that it will be easier for us to work with given our current knowledge.\n\nWe first create a new model of type `Sequential`. We then iterate over each of the layers in `vgg16_model`, except for the last layer, and add each layer to the new `Sequential` model.","fcb8bf6b":"# Part-3 Predict","ebdcea62":"#### Plot Predictions with a Confusion Matrix","2005d6cd":"Now, let's begin building our model.","688f5fc7":"Under the 2.1 Architecture section, we can see that the authors stated that, \"The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\"\n\nThis is the preprocessing that was used on the original training data, and therefore, this is the way we need to process images before passing them to VGG16 or a fine-tuned VGG16 model.\n\nThis processing is what is causing the underlying color data to look distorted","95f0554d":"We now call model.predict to have the model predict on the test data.","d8e317a4":"Recall that this is the same test set we used in a previous [notebook](https:\/\/www.kaggle.com\/bavalpreet26\/cnn-tutorial-keras-nb2) to test the model we built from scratch, and the color in the images appears to be distorted due to the VGG16 preprocessing we discussed previously.","e62d5805":"We pass in the test set, test_batches, and set steps to be then length of test_batches, steps specifies how many batches to yield from the test set before declaring one prediction round complete.","51d676e5":"# Part-1 Building A Fine - Tuned Model","9274c142":"### Train A Fine-Tuned Neural Network With TensorFlow's Keras API","e634e2e6":"The original trained VGG16 model, along with its saved weights and other parameters, is now downloaded.\n\nWe can check out a summary of the model just to see what the architecture looks like.","81ad12a6":"We can now check out a `summary` of our model and see that everything is exactly the same as the original `vgg16_model`, except for now, the output layer has only `2` nodes, rather than 1000, and the number of `trainable parameters` has drastically decreased since we froze all the parameters in the earlier layers.","a0ce8923":"Inspired from [this channel](https:\/\/www.youtube.com\/watch?v=oDHpqu52soI&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL&index=13)","938d1c74":"Now, we\u2019ll train the model using model.fit().\n\nNote that the call to `fit()` is exactly the same as it was when we used it on the original CNN we built from scratch in a previous [notebook](https:\/\/www.kaggle.com\/bavalpreet26\/cnn-tutorial-keras-nb2), except for we're only running 5 epochs this time","bfecff3d":"When we [previously](https:\/\/www.kaggle.com\/bavalpreet26\/cnn-tutorial-keras-nb2) inspected these images, we briefly discussed that the color data was skewed as a result of preprocessing the images using the tf.keras.applications.vgg16.preprocess_input function.","f889605e":"If we check out the type of model `vgg16_model` is, we see that it is of type `Model`, which is from the Keras\u2019 `Functional` API.","5476b9b5":"Picking up with the code, we\u2019ll first get a batch of test samples and their corresponding labels from the test set, and plot them to see what the data looks like","6828ed93":"We can see that the model incorrectly predicted only few samples.So proving this model to be much more capable of generalizing than the previous CNN we built from scratch.","7925bf03":"Notice that the last `Dense` layer of VGG16 has `1000` outputs. These outputs correspond to the 1000 categories in the ImageNet library.\n\nSince we\u2019re only going to be classifying two categories, cats and dogs, we need to modify this model in order for it to do what we want it to do, which is to only classify cats and dogs.\n\nBefore we do that, note that the type of Keras models we\u2019ve been working with so far in this series have been of type `Sequential`.","fa905f9e":"## VGG16 And ImageNet","b01017bb":"Looking at the results from training, we can see just after 5 epochs, we have some pretty outstanding results, especially when you compare it to the results we got from our original model.\n\nOur accuracy starts off at 96% and goes over 99% in just 5 epochs.","04162487":"The reason we don\u2019t want to retrain these layers is because, as mentioned earlier, cats and dogs were already included in the original ImageNet library. So, VGG16 already does a nice job at classifying these categories. We only want to modify the model such that the output layer understands only how to classify cats and dogs and nothing else. Therefore, we don\u2019t want any re-training to occur on the earlier layers.\n\nNext, we add our new output layer, consisting of only 2 nodes that correspond to cat and dog. This output layer will be the only trainable layer in the model.","fb65fb3f":"We\u2019ve not yet worked with the more sophisticated Functional API, although we will work with it in later notebooks using the MobileNet model."}}