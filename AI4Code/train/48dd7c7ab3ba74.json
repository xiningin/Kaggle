{"cell_type":{"f9a60a1a":"code","f7334f2f":"code","354730f1":"code","ef2bc570":"code","b3b0aaae":"code","0ecce19b":"code","fcd34d77":"code","407e4084":"code","31abeb92":"code","020dbaf3":"code","c6837883":"code","c908dd3d":"code","7d73fca2":"markdown","2cddef1a":"markdown","c624a2d0":"markdown","6d8c14f3":"markdown","1fe1aa17":"markdown","23ccc59a":"markdown","694d659b":"markdown","accf9bde":"markdown","5e629276":"markdown","adc5f0d6":"markdown","75996514":"markdown"},"source":{"f9a60a1a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score","f7334f2f":"train_df = pd.read_csv('..\/input\/assignment2\/train.csv')\ntest_df = pd.read_csv('..\/input\/assignment2\/eval.csv')\n\ntrain = train_df.copy()\ntest = test_df.copy()","354730f1":"print(train.isna().sum())\nprint(train.shape)\nprint(test.shape)\nprint(train.sum())\n","ef2bc570":"fig = train.copy()\nfig = fig.drop(['id', 'title', 'esrb_rating'], axis=1)\n\nfig = fig.hist(figsize=(16,20))\nplt.show()","b3b0aaae":"train.head()","0ecce19b":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n\ntrain.drop(['id', 'title', 'console'], axis=1, inplace=True)\ntest.drop(['id', 'console'], axis=1, inplace=True)\n\nX = train.drop(['esrb_rating'], axis = 1)\ny = train['esrb_rating']\ntest.head()","fcd34d77":"# OPTIMAL \nfrom sklearn.linear_model import LogisticRegression\n\nscore = 0\nfor i in range(500):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    log_reg = LogisticRegression(solver='newton-cg', C=100)\n    log_reg.fit(X_train, y_train)\n\n    y_pred = log_reg.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    if acc > score:\n        score = acc\n        final_pred = log_reg.predict(test) \n        submission = pd.DataFrame({'id':test_df['id'], 'esrb_rating': final_pred})\n        print(acc)\n\nsubmission.to_csv('log.csv', index=False)","407e4084":"## OPTIMAL\nfrom sklearn.svm import SVC\n\nscore = 0\nfor i in range(500):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    svm = SVC(C=10, gamma=0.1)\n    svm.fit(X_train, y_train)\n\n    y_pred = svm.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    if acc > score:\n        score = acc\n        final_pred = svm.predict(test) \n        submission = pd.DataFrame({'id':test_df['id'], 'esrb_rating': final_pred})\n        print(acc)\n\nsubmission.to_csv('svm.csv', index=False)","31abeb92":"## OPTIMAL\nfrom sklearn.tree import DecisionTreeClassifier\n\nscore = 0\nfor i in range(500):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    dec = DecisionTreeClassifier(criterion = 'entropy', max_features = 15)\n    dec.fit(X_train, y_train)\n\n    y_pred = dec.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    if acc > score:\n        score = acc\n        final_pred = dec.predict(test) \n        submission = pd.DataFrame({'id':test_df['id'], 'esrb_rating': final_pred})\n        print(acc)\n\nsubmission.to_csv('decision.csv', index=False)","020dbaf3":"## OPTIMAL\nfrom sklearn.ensemble import RandomForestClassifier\n\nscore = 0\nfor i in range(500):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    ran_for = RandomForestClassifier(max_depth=25, min_samples_split=10, n_estimators=50)\n    ran_for.fit(X_train, y_train)\n\n    y_pred = ran_for.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    if acc > score:\n        score = acc\n        final_pred = ran_for.predict(test) \n        submission = pd.DataFrame({'id':test_df['id'], 'esrb_rating': final_pred})\n        print(acc)\n\nsubmission.to_csv('ran_for.csv', index=False)","c6837883":"## OPTIMAL\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscore = 0\nfor i in range(500):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    knn = KNeighborsClassifier(n_neighbors = 7, weights = 'distance', metric = 'manhattan')\n    knn.fit(X_train, y_train)\n\n    y_pred = knn.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    if acc > score:\n        score = acc\n        final_pred = knn.predict(test) \n        submission = pd.DataFrame({'id':test_df['id'], 'esrb_rating': final_pred})\n        print(acc)\n        \nsubmission.to_csv('knn.csv', index=False)","c908dd3d":"# import xgboost as xgb\n\n# # Instantiate the XGBClassifier: xg_cl\n# xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n\n# # Fit the classifier to the training set\n# xg_cl.fit(X_train, y_train)\n\n# # Predict the labels of the test set: preds\n# y_pred = xg_cl.predict(X_test)\n\n# acc = accuracy_score(y_test, y_pred)\n# print(acc)\n\n# final_pred = neigh.predict(test) \n# submission = pd.DataFrame({'id':test_df['id'], 'esrb_rating': final_pred})\n# submission.to_csv('xgb.csv', index=False)","7d73fca2":"# Imports","2cddef1a":"# XGBoost","c624a2d0":"# Logistic Regression","6d8c14f3":"# Random Forest","1fe1aa17":"# Notebook Requirements\nIn order to ensure that you possess understanding of the concepts covered so far, you must show the work required to generate your solution file. This is accomplished by constructing a jupyter notebook in which you will show all of the steps required to build your model, train your model, and generate predictions against the provided test data.\n\nTo receive full credit for this assignment, your notebook must conform to the following requirements:\n\n* [2 point] You must load the data from the provided CSV files.\n\n* [2 point] You must check for missing values within the training data and, if required, describe and implement an approach to handle those missing values.\n\n* [2 point] You must check for outliers within the training data and, if required, describe and implement an approach to handle those outliers.\n\n* [2 point] You must describe any data transformations or feature engineering that are required and provide an explanation as to \"why\" each is being done.\n\n* [2 point] You must build and train a Logistic Regression model on the training data and evaluate its performance on a set of validation data\nYou must generate a distribution of validation scores, as well as summary statistics for this distribution (using the pandas describe() method)\n\n* [2 point] You must build and train an Support Vector Machine on the training data and evaluate its performance on a set of validation data\nYou must generate a distribution of validation scores, as well as summary statistics for this distribution (using the pandas describe() method)\n\n* [2 point] You must build and train a Decision Tree model on the training data and evaluate its performance on a set of validation data\nYou must generate a distribution of validation scores, as well as summary statistics for this distribution (using the pandas describe() method)\n\n* [2 point] You must build and train a Random Forest model on the training data and evaluate its performance on a set of validation data\nYou must generate a distribution of validation scores, as well as summary statistics for this distribution (using the pandas describe() method)\n\n* [2 point] You must build and train a K Nearest Neighbors model on the training data and evaluate its performance on a set of validation data\nYou must generate a distribution of validation scores, as well as summary statistics for this distribution (using the pandas describe() method)\n\n* [2 points] You must select the best model that you have generated and use that model to predict the target vector for the test data.\nYou must save this this target vector to your submission.csv file and print the contents of your submission.csv file within the notebook\n","23ccc59a":"# K Nearest Neighbors","694d659b":"# Data Explore","accf9bde":"# Data Preparation","5e629276":"# Support Vector Machine","adc5f0d6":"# Decision Tree","75996514":"# Reading in Data"}}