{"cell_type":{"6918e33e":"code","9d4ac018":"code","6a73fa34":"code","60af07b4":"code","f6990041":"code","ed580b72":"code","d3d79aff":"code","900f22f9":"code","4fe3c51d":"code","f98b585e":"code","0339fb83":"code","642dbb09":"code","c49db2da":"code","afdadd43":"code","30cd5050":"code","d485dd1a":"code","6277a81b":"code","01005a87":"code","2e084583":"code","5b09e92f":"code","c00ed9ad":"code","e68b27fc":"code","39ba3045":"code","ae85b773":"code","c967e23f":"code","690d17f3":"code","9ca355f4":"code","fa0034d4":"code","e773d9bf":"code","7a52e463":"code","7caa7a2b":"code","81342654":"code","fa9edf94":"code","c4352995":"code","67c27764":"code","3f479de8":"code","3965dfab":"code","8374ed90":"code","1fe09f8e":"code","22c71deb":"code","673833be":"code","79c77c61":"code","38ff1d31":"code","1cee89a7":"code","e442202c":"code","374c47f8":"code","e05f6110":"code","6c017ba9":"code","f1cda362":"code","03268c71":"code","05c918dd":"code","acfcfa9c":"code","1b470820":"code","2186b83a":"code","fb034081":"code","96ce451d":"code","1f07a613":"code","a5ddfd2d":"code","3372a837":"code","897d5fb5":"code","808f1a17":"code","e90ce82d":"code","a21c1847":"code","be30a9ab":"code","c1f19d55":"code","879e594b":"markdown","94d44ce9":"markdown","95b21c79":"markdown","ca8c6159":"markdown","3ae2abfe":"markdown","b62874d7":"markdown","9db7d449":"markdown","0c5f440c":"markdown","d43a0ab2":"markdown","1f16935f":"markdown","673ee6f6":"markdown","25513733":"markdown","fdd4913d":"markdown","7aec15bc":"markdown","cd753e97":"markdown","37345ac9":"markdown","a38f6de3":"markdown","228e4dbd":"markdown","5be26ebf":"markdown","1861c24b":"markdown","2b33f3e2":"markdown","34a7f995":"markdown","b8ea251f":"markdown","49057559":"markdown","434bbdd9":"markdown","8356e321":"markdown","3d877649":"markdown","f4efa62f":"markdown","732c31ff":"markdown","fc94c370":"markdown","c337a1a9":"markdown","95ef7eb0":"markdown","638a52ec":"markdown"},"source":{"6918e33e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d4ac018":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nsns.set(style=\"whitegrid\")","6a73fa34":"data='\/kaggle\/input\/income-classification\/income_evaluation.csv'\ndf=pd.read_csv(data)","60af07b4":"#print the shape\nprint('The shape of the dataset : ' , df.shape)","f6990041":"df.head()","ed580b72":"col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\ndf.columns = col_names\n\ndf.columns","d3d79aff":"df.dtypes","900f22f9":"df.describe()","4fe3c51d":"#check for missing value\n\ndf.isnull().sum()","f98b585e":"categorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :\\n\\n', categorical)","0339fb83":"df[categorical].head()","642dbb09":"for var in categorical: \n    \n    print(df[var].value_counts())","c49db2da":"# check for missing values\n\ndf['income'].isnull().sum()","afdadd43":"# view number of unique values\n\ndf['income'].nunique()","30cd5050":"# view the frequency distribution of values\n\ndf['income'].value_counts()","d485dd1a":"# view percentage of frequency distribution of values\n\ndf['income'].value_counts()\/len(df)","6277a81b":"# visualize frequency distribution of income variable\n\nf,ax=plt.subplots(1,2,figsize=(18,8))\n\nax[0] = df['income'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Income Share')\n\n\n#f, ax = plt.subplots(figsize=(6, 8))\nax[1] = sns.countplot(x=\"income\", data=df, palette=\"Set1\")\nax[1].set_title(\"Frequency distribution of income variable\")\n\nplt.show()","01005a87":"# check number of unique labels \n\ndf.workclass.nunique()","2e084583":"# view frequency distribution of values\n\ndf.workclass.value_counts()","5b09e92f":"# replace '?' values in workclass variable with `NaN`\n\ndf['workclass'].replace(' ?', np.NaN, inplace=True)","c00ed9ad":"# again check the frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()","e68b27fc":"f, ax = plt.subplots(figsize=(10, 6))\nax = df.workclass.value_counts().plot(kind=\"bar\", color=\"green\")\nax.set_title(\"Frequency distribution of workclass variable\")\nax.set_xticklabels(df.workclass.value_counts().index, rotation=30)\nplt.show()","39ba3045":"# check number of unique labels\n\ndf.occupation.nunique()","ae85b773":"# view unique labels\n\ndf.occupation.unique()","c967e23f":"# view frequency distribution of values\n\ndf.occupation.value_counts()","690d17f3":"# replace '?' values in occupation variable with `NaN`\n\ndf['occupation'].replace(' ?', np.NaN, inplace=True)","9ca355f4":"# again check the frequency distribution of values\n\ndf.occupation.value_counts()","fa0034d4":"# check number of unique labels\n\ndf.native_country.nunique()","e773d9bf":"# view unique labels \n\ndf.native_country.unique()","7a52e463":"# check frequency distribution of values\n\ndf.native_country.value_counts()","7caa7a2b":"# replace '?' values in native_country variable with `NaN`\n\ndf['native_country'].replace(' ?', np.NaN, inplace=True)\n# again check the frequency distribution of values\n\ndf.native_country.value_counts()","81342654":"df[categorical].isnull().sum()","fa9edf94":"numerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :\\n\\n', numerical)","c4352995":"df[numerical].head()","67c27764":"df[numerical].isnull().sum()","3f479de8":"df['age'].nunique()","3965dfab":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nax = sns.distplot(x, bins=10, color='blue')\nax.set_title(\"Distribution of age variable\")\nplt.show()","8374ed90":"# plot correlation heatmap to find out correlations\n\ndf.corr().style.format(\"{:.4}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","1fe09f8e":"X=df.drop(['income'],axis=1)\n\ny=df['income']","22c71deb":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","673833be":"#check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","79c77c61":"categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n\ncategorical","38ff1d31":"numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n\nnumerical","1cee89a7":"# print percentage of missing values in the categorical variables in training set\n\nX_train[categorical].isnull().mean()","e442202c":"# print categorical variables with missing data\n\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, (X_train[col].isnull().mean()))","374c47f8":"for df2 in [X_train, X_test]:\n    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)","e05f6110":"# check missing values in categorical variables in X_train\n\nX_train[categorical].isnull().sum()","6c017ba9":"# check missing values in categorical variables in X_test\n\nX_test[categorical].isnull().sum()","f1cda362":"# check missing values in X_train\n\nX_train.isnull().sum()","03268c71":"# check missing values in X_test\n\nX_test.isnull().sum()","05c918dd":"# preview categorical variables in X_train\n\nX_train[categorical].head()","acfcfa9c":"# import category encoders\n\nimport category_encoders as ce","1b470820":"# encode categorical variables with one-hot encoding\n\nencoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n                                 'race', 'sex', 'native_country'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","2186b83a":"X_train.head()","fb034081":"X_train.shape","96ce451d":"X_test.head()","1f07a613":"X_test.shape","a5ddfd2d":"cols = X_train.columns\nfrom sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])","3372a837":"# import Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# instantiate the classifier \n\nrfc = RandomForestClassifier(random_state=0)\n\n\n\n# fit the model\n\nrfc.fit(X_train, y_train)\n\n\n\n# Predict the Test set results\n\ny_pred = rfc.predict(X_test)\n\n#Check accuracy score\n\n\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with Random Forest (with 10 decision tree ) : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\n","897d5fb5":"# instantiate the classifier with n_estimators = 100\n\nrfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nrfc_100.fit(X_train, y_train)\n\n\n\n# Predict on the test set results\n\ny_pred_100 = rfc_100.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with Random Forest (with 100 decision tree ) : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))\n","808f1a17":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_predict_dt = dt.predict(X_test)\nacc_dt = metrics.accuracy_score(y_predict_dt,y_test)\nprint('The accuracy of the Decision Tree is', acc_dt)","e90ce82d":"sv = SVC() #select the algorithm\nsv.fit(X_train,y_train) # we train the algorithm with the training data and the training output\ny_predict_svm = sv.predict(X_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_predict_svm,y_test)\nprint('The accuracy of the SVM is:', acc_svm)","a21c1847":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)","be30a9ab":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","c1f19d55":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","879e594b":"**Scikit-Learn (sklearn) \u2192 Commonly used open source machine learning library**","94d44ce9":"# 4. Exploratory data analysis\n\nNow, I will explore the data to gain insights about the data.","95b21c79":"The above df.describe() command presents statistical properties in vertical form.","ca8c6159":"* There are 9 categorical variables in the dataset.\n\n* The categorical variables are given by workclass, education, marital_status, occupation, relationship, race, sex, native_country and income.","3ae2abfe":"# 3. Import dataset","b62874d7":"# Findings\n\nWe can see that the dataset contains 9 character variables and 6 numerical variables.\nincome is the target variable.","9db7d449":"**The expected accuracy increases with number of decision-trees in the model.**","0c5f440c":"**As expected, younger people make less money as compared to senior people.**","d43a0ab2":"# Find numerical variables","1f16935f":"**As a final check, I will check for missing values in X_train and X_test.**","673ee6f6":"We can see that there are 32561 instances and 15 attributes in the data set.","25513733":"**We can see that there are no missing values in the income target variable.**","fdd4913d":"We can see that there are lot more private workers than other category of workers.","7aec15bc":" **Declare feature vector and target variable**","cd753e97":"**One Hot Encoding means that categorical variables are represented as binary.**","37345ac9":"# **1. The problem statement**\n\nIn this kernel, I try to make predictions where the prediction task is to determine whether a person makes over 50K a year. I implement Random Forest Classification, Decision Tree and SVM with Python and Scikit-Learn. So, to answer the question, I build a Random Forest classifier, Decision Tree and SVM to predict whether a person makes over 50K a year.","a38f6de3":"* Percentage of frequency distribution of values","228e4dbd":"**The two unique values are <=50K and >50K.**","5be26ebf":"**We can see that males make more money than females in both the income categories.**","1861c24b":"Now, we will check the frequency distribution of categorical variables.","2b33f3e2":"**Similarly, I will take a look at the X_test set.**","34a7f995":"**I will do feature engineering on different variables.**\n\n**First, I will show the categorical and numerical variables separately in the training set.**","b8ea251f":"**Explore relationship between age and income variables**","49057559":"# Rename column names\n\nWe can see that the dataset does not have proper column names. The column names contain underscore. We should give proper names to the columns. I will do it as follows:-","434bbdd9":"**We can see that there are no missing values in X_train and X_test.**","8356e321":"We can see that native_country column contains relatively large number of labels as compared to other columns. I will check for cardinality after train-test split.","3d877649":"**We can see that there is no strong correlation between variables.**","f4efa62f":"# Types of variables\n\nIn this section, I segregate the dataset into categorical and numerical variables.\n\nThere are a mixture of categorical and numerical variables in the dataset.\n\nCategorical variables have data type object. Numerical variables have data type int64.\n\nFirst of all, I will explore categorical variables.","732c31ff":"# 2. Import libraries","fc94c370":"**We now have X_train dataset ready to be fed into the Random Forest classifier.**","c337a1a9":"# Explore income target variable","95ef7eb0":"Split data into separate training and test set","638a52ec":"We can see that there are no missing values in the numerical variables."}}