{"cell_type":{"66216ed4":"code","40067074":"code","437931ac":"code","f6043a63":"code","d7a24f07":"code","49d95c33":"code","000e9b14":"code","7f412d25":"code","eaf38b21":"code","0f2c06a1":"code","5d2c91b6":"code","63755b97":"code","9310050f":"code","0fcd1727":"code","1d7816fb":"code","fe1c4461":"code","39d057bc":"code","50593919":"code","a0249a11":"code","5ad91a49":"code","cf98a3d4":"code","1409fd7a":"code","4c38db79":"code","a06217ca":"code","3ffea2d0":"code","911f0143":"code","7244c30e":"code","77d98fd2":"code","10e8cfc0":"code","eda4d651":"code","bd689b9e":"code","44116b6d":"code","bec3f3a4":"code","c6913bed":"code","41151e93":"code","6dcd1cef":"code","64af0963":"markdown","ae64e020":"markdown","a6b72941":"markdown","77db8282":"markdown","93ad42c2":"markdown","e52417f6":"markdown","d9261bed":"markdown","9e987002":"markdown","8aa05d14":"markdown","2bc70ac8":"markdown","7e26c0f6":"markdown","3365a88c":"markdown","2be203fb":"markdown","fe9934ef":"markdown","b0903abd":"markdown","f6628444":"markdown","ecd0f62a":"markdown","1a1a9249":"markdown","6def732b":"markdown","413f16e6":"markdown","94668b06":"markdown","f72a0491":"markdown","1ebb3b5e":"markdown"},"source":{"66216ed4":"import sys; \nsys.path.insert(0,'..\/input\/timm-nfnet')\nimport timm","40067074":"! pip install fastai==1.0.61","437931ac":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader","f6043a63":"import pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom pathlib import Path\n\nfrom fastai.vision import *\nfrom fastai.metrics import error_rate\n\nfrom PIL import Image","d7a24f07":"inputs=Path(\"..\/input\/digit-recognizer\")\nos.listdir(inputs)","49d95c33":"# load training data and explore the first three rows\ntrain=pd.read_csv(inputs\/\"train.csv\")\ntrain.head(3)","000e9b14":"# load test data and explore the first three rows\ntest=pd.read_csv(inputs\/\"test.csv\")\ntest.head(3)","7f412d25":"# tfms can be passed directly to define a DataBunch object (see below) which is then associated with a model to begin training.\ntfms = get_transforms(do_flip=False) # if True the image is randomly flipped\ntr=Path(\"..\/train\")\nte=Path(\"..\/test\")","eaf38b21":"for index in range(10):\n    try:\n        os.makedirs(tr\/str(index))\n    except:\n        pass","0f2c06a1":"sorted(os.listdir(tr))","5d2c91b6":"try:\n    os.makedirs(te)\nexcept:\n    pass\n","63755b97":"for index, row in train.iterrows():\n    \n    label,digit = row[0], row[1:]\n    \n    filepath = tr\/str(label)\n    filename = f\"{index}.jpg\"\n    \n    digit = digit.values\n    digit = digit.reshape(28,28)\n    digit = digit.astype(np.uint8)\n    \n    img = Image.fromarray(digit)\n    img.save(filepath\/filename)\n    \n    ","9310050f":"for index, digit in test.iterrows():\n\n    filepath = te\n    filename = f\"{index}.jpg\"\n    \n    digit = digit.values\n    digit = digit.reshape(28,28)\n    digit = digit.astype(np.uint8)\n    \n    img = Image.fromarray(digit)\n    img.save(filepath\/filename)","0fcd1727":"def displayRandomImagesFromEveryFolder(directory=tr, samplesPerDigit=5):\n\n    fig = plt.figure(figsize=(5,10))\n    \n    for rowIndex in range(1, 10):\n        subdirectory = str(rowIndex)\n        path = directory\/subdirectory\n        images = os.listdir(path)\n        for sampleIndex in range(1,samplesPerDigit+1):\n            randomNumber = random.randint(0, len(images)-1)\n            image = Image.open(path\/images[randomNumber])\n            ax = fig.add_subplot(10, 5, samplesPerDigit*rowIndex + sampleIndex)\n            ax.axis(\"off\")\n            \n            plt.imshow(image, cmap='gray')\n            \n    \n    plt.show()\n    \ndisplayRandomImagesFromEveryFolder()","1d7816fb":"data = ImageDataBunch.from_folder(path=\"..\/train\",test=\"..\/test\",ds_tfms=tfms, valid_pct=0.2,bs=32,size=24).normalize(imagenet_stats)","fe1c4461":"data.show_batch(rows=3 ,figsize=(5,5))","39d057bc":"print(data.classes)","50593919":"class NFNetModel(nn.Module):\n    \n    def __init__(self, num_classes=10, model_name='nfnet_f1', pretrained=False):\n        super(NFNetModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        self.model.head.fc = nn.Linear(self.model.head.fc.in_features, num_classes)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","a0249a11":"model = NFNetModel()","5ad91a49":"learn = Learner(data, model, metrics=accuracy)","cf98a3d4":"# find optimal learning rate and plot the graph\nlearn.lr_find()\n# plot loss vs. learning rate\nlearn.recorder.plot()","1409fd7a":"learn.fit_one_cycle(10)","4c38db79":"learn.save(\"501\")","a06217ca":"learn.unfreeze()\nlearn.fit_one_cycle(10,max_lr=1e-3)","3ffea2d0":"learn.save(\"502\")","911f0143":"interp=ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(8,8))","7244c30e":"interp.most_confused(min_val=3)","77d98fd2":"interp.plot_top_losses(9,figsize=(7,7))","10e8cfc0":"class_score,y=learn.get_preds(DatasetType.Test)","eda4d651":"probs= class_score[0].tolist()\n[f\"{index}: {probs[index]}\" for index in range(len(probs))]","bd689b9e":"class_score=np.argmax(class_score,axis=1)","44116b6d":"class_score[0].item()","bec3f3a4":"samplesub=pd.read_csv(inputs\/\"sample_submission.csv\")\nsamplesub.head()","c6913bed":"ImageId = [os.path.splitext(path)[0] for path in os.listdir(te)]\nImageId = [int(path) for path in ImageId]\nImageId = [ID+1 for ID in ImageId]\nImageId[:5]","41151e93":"subs=pd.DataFrame({\"ImageId\":ImageId,\"Label\":class_score})","6dcd1cef":"subs.to_csv(\"submission.csv\",index=False)\nsubs.head(3)","64af0963":"**Next, we can figure out what ideal learning rates are:**","ae64e020":"# What is Fastai?\n> Fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library.","a6b72941":"**We have to try and get the dataset into a folder format, from the existing format, which will make it easier to use fastai's functions.**","77db8282":"# Submission\n\nNow, creating the submission file based on the example given (which should contain ImageId and Label):","93ad42c2":"**From this, we can see which two numbers are confused most and the number of times.**","e52417f6":"# *Please upvote the kernel if you find it useful*","d9261bed":"**These are the images which had the highest loss, that is the biggest difference between the probability of being corect and actually being correct.**","9e987002":"# Display images ","8aa05d14":"# Prepare Data\n**Currently, it is not even an image, just a 0s and 1s, as seen from the training set. Using the functions below, we can convert them into images:**","2bc70ac8":"# NFNET Overview\n![ImageNet](https:\/\/miro.medium.com\/max\/1400\/1*CjpipU_oChc899f_Esjpyg.png)\n \nNFNET's are Convolutional Residual Style Networks that have no batch normalization build in them. But without the batch normalization usually networks are not performing so well or cannot scale to larger batch sizes however NFNET builds networks that scale to large batch sizes and are more efficient than previous state-of-the-art methods. The training latency vs accuracy graph shows that NFnets are 8.7\u00d7 times faster than EffNet-B7 for the same top-1 accuracy score trained on ImageNet. ","7e26c0f6":"**We can clearly see that the learning rate is most effective at 1e-03, but let's try without a predefined learning rate:**","3365a88c":"# Importing the necessary libraries:","2be203fb":"**99% accuracy, not bad at all.**\n\n**Saving this model:**","fe9934ef":"# Model \n\nNFNets are a family of modified ResNets that achieves competitive accuracies without batch normalization. To do so, it applies 3 different techniques:\n* Modified residual branches and convolutions with Scaled Weight Standardization\n* Adaptive Gradient Clipping\n* Architecture optimization for improved accuracy and training spee","b0903abd":"**To get a set of transforms with default values that work pretty well in a wide range of tasks, it's often easiest to use get_transforms.**                                                                                                     \n* **tfms is just a parameter used later during training, which is initalized here.** \n* **tr and te are paths to be used.**","f6628444":"# Prediction","ecd0f62a":"**The dataset has been converted into images!**\n\n**We can move on to getting the data from folders, and seperating them into training and validation sets. Also normalization is very important to make sure all values lie between 0 and 1.**\n\n**It turns out that the PosixPath is not iterated by ImageDataBunch in Kaggle.So we can change the path created by pathlib library which was a PosixPath object to just a string which specifies the path to the training and testing directories, so in our case train path = \"..\/train\" and test path = \"..\/test\"**","1a1a9249":"**These are the probabilities that the image is any of these numbers. But we don't want that. We only want the highest probability:**","6def732b":"**The data has been successfully extracted from the folders.**\n\n**We can also check what classes exist:**","413f16e6":"# Image transforms","94668b06":"# Results\n\n**We can interpret our results as well:**","f72a0491":"# Read the train and test datasets:","1ebb3b5e":"**Now, let's try with the optimal learning rates:**"}}