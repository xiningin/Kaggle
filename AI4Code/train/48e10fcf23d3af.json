{"cell_type":{"763667bd":"code","baf75082":"code","05077bd0":"code","0227b43e":"code","705432f3":"code","a8dc3d8f":"code","c4d0bfc6":"code","a5604789":"code","a47f0bd1":"code","945f1024":"code","f553a051":"code","bd337163":"code","0d24d7a2":"code","dab7234c":"code","aa634926":"code","65ac5601":"code","72f8a501":"code","2ee07bf4":"code","dbd8206d":"code","6f17b0d1":"code","943568eb":"code","a6362709":"code","ff18c50b":"code","b2b30627":"code","ef57e58c":"code","f61bd678":"code","1da07658":"code","d734ad5c":"code","b8eafa13":"code","ad691216":"code","31d479dd":"code","471756ab":"code","c609bbdf":"code","88daa045":"code","6fffa117":"code","273aedcf":"code","11e1d2ac":"markdown","73a565ef":"markdown","fb34e139":"markdown","b583ddd1":"markdown","3e2e6d42":"markdown","f15c3162":"markdown","ef276983":"markdown","cfb75dfd":"markdown","5771f636":"markdown","029fd50c":"markdown","682b3f8e":"markdown","4900e9bf":"markdown","11a471e6":"markdown","dd2cdf7c":"markdown","31fd9663":"markdown","4ffe64bd":"markdown","4f34f0e4":"markdown","cc6d689d":"markdown","4c61b218":"markdown","a4167cde":"markdown","b27f7940":"markdown","2f2677ac":"markdown","fa61e2ed":"markdown","9868505c":"markdown","d313965f":"markdown","f0e65a3a":"markdown"},"source":{"763667bd":"pip install style","baf75082":"from IPython.display import Image\nImage(\"..\/input\/imageml\/ML.jpg\", width = \"900px\")","05077bd0":"import style\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as pyo\nimport matplotlib.pyplot as plt\n\n","0227b43e":"Input_Data = pd.read_csv(\"..\/input\/rainfall-in-india\/rainfall in india 1901-2015.csv\")\nInput_Data = Input_Data.fillna(Input_Data.mean())\nInput_Data.info()\nInput_Data.head()\nInput_Data.describe()","705432f3":"def _SUBDIVISION(column,division):\n        Year = []\n        annual = []\n        for i in range(len(column)):\n            if column[i] == division:\n                if 'nan'!= str(Input_Data[\"ANNUAL\"][i]) and 'NAN' != str(Input_Data[\"ANNUAL\"][i]):\n                    Year.append(Input_Data[\"YEAR\"][i])\n                    annual.append((Input_Data[\"ANNUAL\"][i]))\n        return Year,annual\ncolors = [\"#808000\",\"#FF0000\",\"#0000FF\",\"#808000\",\"#800080\",\"#008000\",\"#800000\",\"#A52A2A\",\"#FFA500\",\"#000000\",\"#151B54\",\"#FBB917\",\"#806517\",\"#C11B17\",\n          \"#810541\",\"#F6358A\",\"#808000\",\"#FF0000\",\"#0000FF\",\"#808000\",\"#800080\",\"#008000\",\"#800000\",\"#A52A2A\",\"#FFA500\",\"#000000\",\"#151B54\",\"#FBB917\",\n          \"#806517\",\"#C11B17\",\"#810541\",\"#F6358A\",\"#808000\",\"#FF0000\",\"#0000FF\",\"#808000\"]\ncountry = Input_Data[\"SUBDIVISION\"].unique()\nfor i in range(len(country)):\n    Year, annual = _SUBDIVISION(Input_Data[\"SUBDIVISION\"],str(country[i]))\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize']=(25,5)\n    plt.plot(Year, annual, color=colors[i] ,alpha = 0.7, marker='o', linestyle='dashed',linewidth=2, markersize=12)\n    plt.title(str(country[i])+\" Annual Rainfall\\n\", size= 25)\n    plt.xlabel(\"Year\", size= 25)\n    plt.ylabel(\"Rainfall in\\nCenti Meter\", size= 30)\n    plt.show()\n    ","a8dc3d8f":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"SUBDIVISION\", y=\"ANNUAL\", data=Input_Data, width=0.9,linewidth=3)\nax.set_xlabel('Subdivision',fontsize=30)\nax.set_ylabel('Annual Rainfall (in mm)',fontsize=30)\nplt.title('Annual Rainfall in Subdivisions in India',fontsize=40)\nax.tick_params(axis='x',labelsize=20,rotation=90)\nax.tick_params(axis='y',labelsize=20,rotation=0)\nplt.grid()\nplt.ioff()","c4d0bfc6":"Input_Data[['YEAR', 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].groupby(\"YEAR\").sum().plot(figsize=(11,7));\n","a5604789":"\nInput_Data[['SUBDIVISION', 'Jan-Feb', 'Mar-May',\n       'Jun-Sep', 'Oct-Dec']].groupby(\"SUBDIVISION\").sum().plot.barh(stacked=True,figsize=(16,8));","a47f0bd1":"plt.figure(figsize=(11,4))\nsns.heatmap(Input_Data[['Jan-Feb','Mar-May','Jun-Sep','Oct-Dec','ANNUAL']].corr(),annot=True)\nplt.show()","945f1024":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np\ndivision_data = np.asarray(Input_Data[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']])\n\nX = None; y = None\nfor i in range(division_data.shape[1]-3):\n    if X is None:\n        X = division_data[:, i:i+3]\n        y = division_data[:, i+3]\n    else:\n        X = np.concatenate((X, division_data[:, i:i+3]), axis=0)\n        y = np.concatenate((y, division_data[:, i+3]), axis=0)\n        \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","f553a051":"temp = Input_Data[['SUBDIVISION','JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[Input_Data['YEAR'] == 2010]\n\ndata_2010 = np.asarray(temp[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[temp['SUBDIVISION'] == 'TELANGANA'])\n\nX_year_2010 = None; y_year_2010 = None\nfor i in range(data_2010.shape[1]-3):\n    if X_year_2010 is None:\n        X_year_2010 = data_2010[:, i:i+3]\n        y_year_2010 = data_2010[:, i+3]\n    else:\n        X_year_2010 = np.concatenate((X_year_2010, data_2010[:, i:i+3]), axis=0)\n        y_year_2010 = np.concatenate((y_year_2010, data_2010[:, i+3]), axis=0)","bd337163":"#terst 2015\ntemp = Input_Data[['SUBDIVISION','JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[Input_Data['YEAR'] == 2015]\n\ndata_2015 = np.asarray(temp[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[temp['SUBDIVISION'] == 'TELANGANA'])\n\nX_year_2015 = None; y_year_2015 = None\nfor i in range(data_2015.shape[1]-3):\n    if X_year_2015 is None:\n        X_year_2015 = data_2015[:, i:i+3]\n        y_year_2015 = data_2015[:, i+3]\n    else:\n        X_year_2015 = np.concatenate((X_year_2015, data_2015[:, i:i+3]), axis=0)\n        y_year_2015 = np.concatenate((y_year_2015, data_2015[:, i+3]), axis=0)","0d24d7a2":"temp = Input_Data[['SUBDIVISION','JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[Input_Data['YEAR'] == 2005]\n\ndata_2005 = np.asarray(temp[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[temp['SUBDIVISION'] == 'TELANGANA'])\n\nX_year_2005 = None; y_year_2005 = None\nfor i in range(data_2005.shape[1]-3):\n    if X_year_2005 is None:\n        X_year_2005 = data_2005[:, i:i+3]\n        y_year_2005 = data_2005[:, i+3]\n    else:\n        X_year_2005 = np.concatenate((X_year_2005, data_2005[:, i:i+3]), axis=0)\n        y_year_2005 = np.concatenate((y_year_2005, data_2005[:, i+3]), axis=0)","dab7234c":"from sklearn import linear_model\n\n# linear model\nreg = linear_model.ElasticNet(alpha=0.5)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n# print (mean_absolute_error(y_test, y_pred))","aa634926":"#2005\ny_year_pred_2005 = reg.predict(X_year_2005)\n\n\n#2010\ny_year_pred_2010 = reg.predict(X_year_2010)\n    \ny_year_pred_2015 = reg.predict(X_year_2015)\n\nprint (\"MEAN 2005\")\nprint (np.mean(y_year_2005),np.mean(y_year_pred_2005))\nprint (\"Standard deviation 2005\")\nprint (np.sqrt(np.var(y_year_2005)),np.sqrt(np.var(y_year_pred_2005)))\n\n\nprint (\"MEAN 2010\")\nprint (np.mean(y_year_2010),np.mean(y_year_pred_2010))\nprint (\"Standard deviation 2010\")\nprint (np.sqrt(np.var(y_year_2010)),np.sqrt(np.var(y_year_pred_2010)))\n\n\nprint (\"MEAN 2015\")\nprint (np.mean(y_year_2015),np.mean(y_year_pred_2015))\nprint (\"Standard deviation 2015\")\nprint (np.sqrt(np.var(y_year_2015)),np.sqrt(np.var(y_year_pred_2015)))\n\n\n","65ac5601":"from sklearn.svm import SVR\n\n# SVM model\nclf = SVR(kernel='rbf', gamma='auto', C=0.5, epsilon=0.2)\nclf.fit(X_train, y_train) \ny_pred = clf.predict(X_test)\nprint (mean_absolute_error(y_test, y_pred))","72f8a501":"#2005\ny_year_pred_2005 = reg.predict(X_year_2005)\n\n\n#2010\ny_year_pred_2010 = reg.predict(X_year_2010)\n    \ny_year_pred_2015 = reg.predict(X_year_2015)\n\nprint (\"MEAN 2005\")\nprint (np.mean(y_year_2005),np.mean(y_year_pred_2005))\nprint (\"Standard deviation 2005\")\nprint (np.sqrt(np.var(y_year_2005)),np.sqrt(np.var(y_year_pred_2005)))\n\n\nprint (\"MEAN 2010\")\nprint (np.mean(y_year_2010),np.mean(y_year_pred_2010))\nprint (\"Standard deviation 2010\")\nprint (np.sqrt(np.var(y_year_2010)),np.sqrt(np.var(y_year_pred_2010)))\n\n\nprint (\"MEAN 2015\")\nprint (np.mean(y_year_2015),np.mean(y_year_pred_2015))\nprint (\"Standard deviation 2015\")\nprint (np.sqrt(np.var(y_year_2015)),np.sqrt(np.var(y_year_pred_2015)))\n\nplt.rcParams['figure.figsize']=(23,10)\nplt.bar([np.mean(y_year_2005)],[np.mean(y_year_pred_2005)],label = \"Year-2005\")\nplt.bar([np.mean(y_year_2010)],[np.mean(y_year_pred_2010)],label = \"Year-2010\")\nplt.bar([np.mean(y_year_2015)],[np.mean(y_year_pred_2015)],label = \"Year-2015\")\n\nplt.legend()\nplt.show()","2ee07bf4":"from keras.models import Model\nfrom keras.layers import Dense, Input, Conv1D, Flatten\n\n# NN model\ninputs = Input(shape=(3,1))\nx = Conv1D(64, 2, padding='same', activation='elu')(inputs)\nx = Conv1D(128, 2, padding='same', activation='elu')(x)\nx = Flatten()(x)\nx = Dense(128, activation='elu')(x)\nx = Dense(64, activation='elu')(x)\nx = Dense(32, activation='elu')(x)\nx = Dense(1, activation='linear')(x)\nmodel = Model(inputs=[inputs], outputs=[x])\nmodel.compile(loss='mean_squared_error', optimizer='adamax', metrics=['mae'])\nmodel.summary()\nmodel.fit(x=np.expand_dims(X_train, axis=2), y=y_train, batch_size=64, epochs=10, verbose=1, validation_split=0.1, shuffle=True)\ny_pred = model.predict(np.expand_dims(X_test, axis=2))\nprint (mean_absolute_error(y_test, y_pred))","dbd8206d":"#2005\ny_year_pred_2005 = reg.predict(X_year_2005)\n\n\n#2010\ny_year_pred_2010 = reg.predict(X_year_2010)\n    \ny_year_pred_2015 = reg.predict(X_year_2015)\n\nprint (\"MEAN 2005\")\nprint (np.mean(y_year_2005),np.mean(y_year_pred_2005))\nprint (\"Standard deviation 2005\")\nprint (np.sqrt(np.var(y_year_2005)),np.sqrt(np.var(y_year_pred_2005)))\n\n\nprint (\"MEAN 2010\")\nprint (np.mean(y_year_2010),np.mean(y_year_pred_2010))\nprint (\"Standard deviation 2010\")\nprint (np.sqrt(np.var(y_year_2010)),np.sqrt(np.var(y_year_pred_2010)))\n\n\nprint (\"MEAN 2015\")\nprint (np.mean(y_year_2015),np.mean(y_year_pred_2015))\nprint (\"Standard deviation 2015\")\nprint (np.sqrt(np.var(y_year_2015)),np.sqrt(np.var(y_year_pred_2015)))\nplt.rcParams['figure.figsize']=(23,10)\nplt.bar([np.mean(y_year_2005)],[np.mean(y_year_pred_2005)],label = \"Year-2005\")\nplt.bar([np.mean(y_year_2010)],[np.mean(y_year_pred_2010)],label = \"Year-2010\")\nplt.bar([np.mean(y_year_2015)],[np.mean(y_year_pred_2015)],label = \"Year-2015\")\n\nplt.legend()\nplt.show()","6f17b0d1":"# spliting training and testing data only for telangana\nORISSA = np.asarray(Input_Data[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[Input_Data['SUBDIVISION'] == 'ORISSA'])\n\nX = None; y = None\nfor i in range(ORISSA.shape[1]-3):\n    if X is None:\n        X = ORISSA[:, i:i+3]\n        y = ORISSA[:, i+3]\n    else:\n        X = np.concatenate((X, ORISSA[:, i:i+3]), axis=0)\n        y = np.concatenate((y, ORISSA[:, i+3]), axis=0)\n        \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)","943568eb":"from sklearn import linear_model\n\n# linear model\nreg = linear_model.ElasticNet(alpha=0.5)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nprint (mean_absolute_error(y_test, y_pred))","a6362709":"#2005\ny_year_pred_2005 = reg.predict(X_year_2005)\n\n\n#2010\ny_year_pred_2010 = reg.predict(X_year_2010)\n    \ny_year_pred_2015 = reg.predict(X_year_2015)\n\nprint (\"MEAN 2005\")\nprint (np.mean(y_year_2005),np.mean(y_year_pred_2005))\nprint (\"Standard deviation 2005\")\nprint (np.sqrt(np.var(y_year_2005)),np.sqrt(np.var(y_year_pred_2005)))\n\n\nprint (\"MEAN 2010\")\nprint (np.mean(y_year_2010),np.mean(y_year_pred_2010))\nprint (\"Standard deviation 2010\")\nprint (np.sqrt(np.var(y_year_2010)),np.sqrt(np.var(y_year_pred_2010)))\n\n\nprint (\"MEAN 2015\")\nprint (np.mean(y_year_2015),np.mean(y_year_pred_2015))\nprint (\"Standard deviation 2015\")\nprint (np.sqrt(np.var(y_year_2015)),np.sqrt(np.var(y_year_pred_2015)))\nplt.rcParams['figure.figsize']=(23,10)\nplt.bar([np.mean(y_year_2005)],[np.mean(y_year_pred_2005)],label = \"Year-2005\")\nplt.bar([np.mean(y_year_2010)],[np.mean(y_year_pred_2010)],label = \"Year-2010\")\nplt.bar([np.mean(y_year_2015)],[np.mean(y_year_pred_2015)],label = \"Year-2015\")\n\nplt.legend()\nplt.show()","ff18c50b":"from sklearn.svm import SVR\n\n# SVM model\nclf = SVR(kernel='rbf', gamma='auto', C=0.5, epsilon=0.2)\nclf.fit(X_train, y_train) \ny_pred = clf.predict(X_test)\nprint (mean_absolute_error(y_test, y_pred))","b2b30627":"#2005\ny_year_pred_2005 = reg.predict(X_year_2005)\n\n\n#2010\ny_year_pred_2010 = reg.predict(X_year_2010)\n    \ny_year_pred_2015 = reg.predict(X_year_2015)\n\nprint (\"MEAN 2005\")\nprint (np.mean(y_year_2005),np.mean(y_year_pred_2005))\nprint (\"Standard deviation 2005\")\nprint (np.sqrt(np.var(y_year_2005)),np.sqrt(np.var(y_year_pred_2005)))\n\n\nprint (\"MEAN 2010\")\nprint (np.mean(y_year_2010),np.mean(y_year_pred_2010))\nprint (\"Standard deviation 2010\")\nprint (np.sqrt(np.var(y_year_2010)),np.sqrt(np.var(y_year_pred_2010)))\n\n\nprint (\"MEAN 2015\")\nprint (np.mean(y_year_2015),np.mean(y_year_pred_2015))\nprint (\"Standard deviation 2015\")\nprint (np.sqrt(np.var(y_year_2015)),np.sqrt(np.var(y_year_pred_2015)))\nplt.rcParams['figure.figsize']=(23,10)\nplt.bar([np.mean(y_year_2005)],[np.mean(y_year_pred_2005)],label = \"Year-2005\")\nplt.bar([np.mean(y_year_2010)],[np.mean(y_year_pred_2010)],label = \"Year-2010\")\nplt.bar([np.mean(y_year_2015)],[np.mean(y_year_pred_2015)],label = \"Year-2015\")\n\nplt.legend()\nplt.show()","ef57e58c":"model.fit(x=np.expand_dims(X_train, axis=2), y=y_train, batch_size=64, epochs=10, verbose=1, validation_split=0.1, shuffle=True)\ny_pred = model.predict(np.expand_dims(X_test, axis=2))\nprint (mean_absolute_error(y_test, y_pred))","f61bd678":"#2005\ny_year_pred_2005 = reg.predict(X_year_2005)\n\n\n#2010\ny_year_pred_2010 = reg.predict(X_year_2010)\n    \ny_year_pred_2015 = reg.predict(X_year_2015)\n\nprint (\"MEAN 2005\")\nprint (np.mean(y_year_2005),np.mean(y_year_pred_2005))\nprint (\"Standard deviation 2005\")\nprint (np.sqrt(np.var(y_year_2005)),np.sqrt(np.var(y_year_pred_2005)))\n\n\nprint (\"MEAN 2010\")\nprint (np.mean(y_year_2010),np.mean(y_year_pred_2010))\nprint (\"Standard deviation 2010\")\nprint (np.sqrt(np.var(y_year_2010)),np.sqrt(np.var(y_year_pred_2010)))\n\n\nprint (\"MEAN 2015\")\nprint (np.mean(y_year_2015),np.mean(y_year_pred_2015))\nprint (\"Standard deviation 2015\")\nprint (np.sqrt(np.var(y_year_2015)),np.sqrt(np.var(y_year_pred_2015)))\nplt.rcParams['figure.figsize']=(23,10)\nplt.bar([np.mean(y_year_2005)],[np.mean(y_year_pred_2005)],label = \"Year-2005\")\nplt.bar([np.mean(y_year_2010)],[np.mean(y_year_pred_2010)],label = \"Year-2010\")\nplt.bar([np.mean(y_year_2015)],[np.mean(y_year_pred_2015)],label = \"Year-2015\")\n\nplt.legend()\nplt.show()","1da07658":"district = pd.read_csv(\"..\/input\/rainfall-in-india\/district wise rainfall normal.csv\",sep=\",\")\ndistrict = district.fillna(district.mean())\ndistrict.info()","d734ad5c":"ap_data = district[district['STATE_UT_NAME'] == 'ANDHRA PRADESH']\nap_data[['DISTRICT', 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']].groupby(\"DISTRICT\").mean()[:40].plot.barh(stacked=True,figsize=(18,8));","b8eafa13":"# testing and training for the complete data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\ndivision_data = np.asarray(district[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',\n       'AUG', 'SEP', 'OCT', 'NOV', 'DEC']])\n\nX = None; y = None\nfor i in range(division_data.shape[1]-3):\n    if X is None:\n        X = division_data[:, i:i+3]\n        y = division_data[:, i+3]\n    else:\n        X = np.concatenate((X, division_data[:, i:i+3]), axis=0)\n        y = np.concatenate((y, division_data[:, i+3]), axis=0)\n        \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ad691216":"temp = district[['DISTRICT','JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL','AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[district['STATE_UT_NAME'] == 'ANDHRA PRADESH']\nhyd = np.asarray(temp[['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL','AUG', 'SEP', 'OCT', 'NOV', 'DEC']].loc[temp['DISTRICT'] == 'HYDERABAD'])\n# print temp\nX_year = None; y_year = None\nfor i in range(hyd.shape[1]-3):\n    if X_year is None:\n        X_year = hyd[:, i:i+3]\n        y_year = hyd[:, i+3]\n    else:\n        X_year = np.concatenate((X_year, hyd[:, i:i+3]), axis=0)\n        y_year = np.concatenate((y_year, hyd[:, i+3]), axis=0)","31d479dd":"from sklearn import linear_model\n\n# linear model\nreg = linear_model.ElasticNet(alpha=0.5)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n#print (mean_absolute_error(y_test, y_pred))","471756ab":"y_year_pred = reg.predict(X_year)\nprint (\"MEAN Hyderabad\")\nprint (np.mean(y_year)),(np.mean(y_year_pred))\nprint (\"Standard deviation hyderabad\")\nprint (np.sqrt(np.var(y_year))),(np.sqrt(np.var(y_year_pred)))","c609bbdf":"from sklearn.svm import SVR\n\n# SVM model\nclf = SVR(gamma='auto', C=0.1, epsilon=0.2)\nclf.fit(X_train, y_train) \ny_pred = clf.predict(X_test)\nprint (mean_absolute_error(y_test, y_pred))\ny_year_pred = reg.predict(X_year)\nprint (\"MEAN Hyderabad\")\nprint (np.mean(y_year)),(np.mean(y_year_pred))\nprint (\"Standard deviation hyderabad\")\nprint (np.sqrt(np.var(y_year))),(np.sqrt(np.var(y_year_pred)))","88daa045":"model.fit(x=np.expand_dims(X_train, axis=2), y=y_train, batch_size=64, epochs=10, verbose=1, validation_split=0.1, shuffle=True)\ny_pred = model.predict(np.expand_dims(X_test, axis=2))\nprint (mean_absolute_error(y_test, y_pred))","6fffa117":"y_year_pred = reg.predict(X_year)\nprint (\"MEAN Hyderabad\")\nprint (np.mean(y_year)),(np.mean(y_year_pred))\nprint (\"Standard deviation hyderabad\")\nprint (np.sqrt(np.var(y_year))),(np.sqrt(np.var(y_year_pred)))","273aedcf":"from IPython.display import Image\nImage(\"..\/input\/comparison-on-algo\/rsz_screenshot_from_2020-05-19_13-40-16.png\", width = \"900px\")","11e1d2ac":"# OBSERVATIONS","73a565ef":"# ANN FOR HYDERABAD","fb34e139":"# LINEAR MODEL FOR ORISSA","b583ddd1":"# PREDICTIONS","3e2e6d42":"# SPLITTING TEST AND TRAINING DATA FOR HYDERABAD","f15c3162":"# Prediction Observations\n# Training on complete dataset\n# Algorithm\t    MAE\nLinear Regression\t          94.94821727619338\nSVR\t                          127.74073860203839\nArtificial neural nets\t       85.2648713528865\n# Training on HYDERABAD dataset\n# Algorithm\t     MAE\nLinear Regression\t            70.61463829282977\nSVR\t                            90.30526775954294\nArtificial neural nets\t        59.95190786532157","ef276983":"# Documentation\n## Pandas\n## Pandas is mainly used for machine learning in form of dataframes. Pandas allow importing data of various file formats such as csv, excel etc. Pandas allows various data manipulation operations such as groupby, join, merge, melt, concatenation as well as data cleaning features such as filling, replacing or imputing null values.\n## Link:https:\/\/pandas.pydata.org\/docs\/\n## Matplotlib\n## Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+.\n## Link:https:\/\/matplotlib.org\/3.2.1\/contents.html\n## Plotly\n## Plotly's Python graphing library makes interactive, publication-quality graphs. Examples of how  to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.\n## Link:https:\/\/plotly.com\/python\/\n## Seaborn\n## Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n## Link:https:\/\/seaborn.pydata.org\/","cfb75dfd":"Linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model. However, the term is also used in time series analysis with a different meaning. In each case, the designation \"linear\" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible.","5771f636":"# CONCLUSION\nVarious visualizations of data are observed which helps in implementing the approaches for prediction.\nPrediction of amount of rainfall for both the types of dataset.\nObservations indicates machine learning models won't work well for prediction of rainfall due to fluctutaions in rainfall.","029fd50c":"> For prediction we formatted data in the way, given the rainfall in the last three months we try to predict the rainfall in the next consecutive month.\n> For all the experiments we used 80:20 training and test ratio.\n> Linear regression\n> SVR\n> Artificial neural nets\n> > Testing metrics: We used Mean absolute error to train the models.\n> We also shown the amount of rainfall actually and predicted with the histogram plots.\n> We did two types of trainings once training on complete dataset and other with training with only ORISSA data\n> All means are standard deviation observations are written, first one represents ground truth, second one represents predictions.","682b3f8e":"# SUPPORT VECTOR ","4900e9bf":"# SVM FOR HYDERABAD","11a471e6":"# SVM FOR ORISSA","dd2cdf7c":"# Training on complete dataset\n# Algorithm\t        MAE\n*Linear Regression\t                 96.94\nSVR\t                                 126.136\nArtificial neural \nnetworks\t                          88.2648**\n# Training on ORISSA dataset\n# Algorithm\t    MAE\nLinear Regression\t            97.207798\nSVR\t                            126.24075954294\nArtificial neural networks\t    75.2648\n* Neural Networks performs better than SVR etc.\n* Observed MAE is very high which indicates machine learning models won't work well for prediction of rainfall.\n* ORISSA data has a single pattern that can be learned by models, rather than learning different patterns of all states. so has high accuracy.\n* Analysed individual year rainfall patterns for 2005, 2010, 2015.\n* Approximately close means, noticed less standard deviations.**","31fd9663":" # Month Wise Graph analysis","4ffe64bd":"# Support Vector Machine\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification and regression challenges. We plot\u00a0each data item as a point in n-dimensional space (where n is number of features) with the value of each feature being the value of a particular coordinate. Then, we\u00a0perform classification by finding the hyper-plane that\u00a0differentiates the two classes very well. A Support Vector Machine models the situation by creating a\u00a0feature space, which is a finite-dimensional\u00a0vector space, each dimension of which represents a \"feature\" of a particular object. The goal of the SVM is to train a model that assigns new unseen objects into a particular category. It achieves this by creating a linear partition of the feature space into two categories. Based on the features in the new unseen objects, it places an object \"above\" or \"below\" the separation plane, leading to a categorization. It is non-probabilistic, because the features in the new objects fully determine its location in feature space and there is no stochastic element involved. A subset of training data lies on Biased and Unbiased Hyper planes","4f34f0e4":"A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature. Neural networks can adapt to changing input; so the network generates the best possible result without needing to redesign the output criteria. ","cc6d689d":"# From the graph its analysed that july,aug,sep has increase in rainfall","4c61b218":"# LINEAR MODEL","a4167cde":"# DISTRICT WISE ","b27f7940":"# Overview on Machine Learning","2f2677ac":"# ANN FOR ORISSA","fa61e2ed":"# Comparison of some algorithms!","9868505c":"# NEURAL NETS","d313965f":"# Observations\n*It is observed that in Andhra Pradesh, annual rainfall depends more in the months of january, febuary.\nIt also shows that if there is rainfall in months march, april, may then there is less amount of rainfall in the months june, july, august, september.*\n# Predictions\n*We used the same types of models and evaluation metrics used for the above dataset.\nWe also tested the amount of rainfall in hyderabad by models trained on complete dataset and andhra pradesh dataset.*","f0e65a3a":"# SPLITTING TEST AND TRAINING DATA FOR ORISSA"}}