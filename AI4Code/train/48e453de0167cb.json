{"cell_type":{"40cb6627":"code","fb1a3ebd":"code","52e57fd9":"code","1e37e661":"code","2e1c27ac":"code","55673ebf":"code","d0af2dd3":"code","41fba8fb":"code","6ef04b5f":"code","f1f35217":"code","25b96748":"code","2ace2f23":"code","28f1a4a1":"code","76d3660b":"code","c3ccec1f":"code","ec924c61":"markdown","b427dbb8":"markdown","688f7970":"markdown","a3b610ed":"markdown","e43717f1":"markdown","e309f687":"markdown","38edd101":"markdown","9794198b":"markdown","d7478dc0":"markdown","a37843ab":"markdown"},"source":{"40cb6627":"import numpy as np\nimport pandas as pd","fb1a3ebd":"def tocsv(df, filename='tocsv.csv', index = False):\n    df.to_csv(filename, index=index)","52e57fd9":"pd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)","1e37e661":"df_train = pd.read_csv('..\/input\/train.csv') \ndf_test = pd.read_csv('..\/input\/test.csv')\n\nid_col = df_test['Id']","2e1c27ac":"print(f\"Train shape: {df_train.shape}\")\nprint(f\"Test shape: {df_test.shape}\")","55673ebf":"%matplotlib inline\np = df_train.SalePrice.hist()\nt = p.set_title(\"SalePrice distribution\")","d0af2dd3":"y_train = np.log10(df_train.SalePrice)\nX_train = df_train.drop('SalePrice', axis=1)\nX_test = df_test\nX = pd.concat([X_train, X_test])","41fba8fb":"categoricals = X_train.select_dtypes(include='object').columns\nnumericals = X_train.select_dtypes(exclude='object').columns\nprint(f'{len(categoricals)} categorical features')\nprint(f'{len(numericals)} numerical features')","6ef04b5f":"X[categoricals].isna().sum().sort_values(ascending=False)","f1f35217":"X[categoricals] = X[categoricals].fillna(\"absent\")","25b96748":"X[numericals].isna().sum().sort_values(ascending=False)","2ace2f23":"X = pd.get_dummies(X)\n","28f1a4a1":"from sklearn.impute import SimpleImputer\nimp_mean = SimpleImputer(strategy='mean')\nimp_mean.fit(X)\nX_imp = imp_mean.transform(X)\n\nX_imp = pd.DataFrame(X_imp)","76d3660b":"X_train_model = X_imp[0:1460]\nX_test_model = X_imp[1460:]","c3ccec1f":"from sklearn import linear_model\n#regr = linear_model.Lasso(alpha=0.1)\nregr = linear_model.LinearRegression()\nregr = regr.fit(X_train_model, y_train)\nout = regr.predict(X_test_model)\n\nout = 10**out\n\nout = pd.DataFrame(out,columns=['SalePrice'])\n\nout.insert(0,\"Id\", id_col) \nout\ntocsv(out)","ec924c61":"## Data cleaning\/preprocessing","b427dbb8":"My name is Martin C. I am submitting this solution as part of my application at turingly.\n\nThis notebooks is structured as follows:\n- Introduction\n- Data cleaning\/preprocessing\n- Modeling\/prediction","688f7970":"For the categorical features, most of the \"missing\" values are not actually missing. E.g. PoolQC is NA when there is no pool. That does not mean that data is missing. We replace the \"missing\" values with the string \"absent\":","a3b610ed":"## Introduction\nIn this notebook we look at the kaggle problem titled \"House Prices: Advanced Regression Techniques\". We are given data about houses and are asked to predict the saleprices of the houses. There are 1460 training examples and 1459 test examples. The data has 80 columns (i.e. 80 features), including \n-    LotArea: Lot size in square feet\n-    Street: Type of road access\n-    Alley: Type of alley access\n-    LotShape: General shape of property\n-    LandContour: Flatness of the property\n-    BldgType: Type of dwelling  \n-    OverallQual: Overall material and finish quality\n-    OverallCond: Overall condition rating\n-    YearBuilt: Original construction date\n  ","e43717f1":"### Import data","e309f687":"Now for the rest of the features (numerical) we simply use the mean value. This is far from optimal obviously, but it is simple. ","38edd101":"Categorical features that have missing data:","9794198b":"### Imputation ","d7478dc0":"Now use one-hot encoding for the categorical variables:","a37843ab":"### Dealing with missing data "}}