{"cell_type":{"c39a6b17":"code","0b32dd76":"code","3e910af6":"code","55c94326":"code","d9a7d742":"code","91d7f982":"code","8d3872cd":"code","72abbe26":"code","9b12c891":"code","024adf77":"code","c0fbdccb":"code","e8c84363":"code","4ceeccd9":"code","e8ec2952":"code","106ed027":"code","7eb89459":"code","f96d8640":"code","ac2b86e9":"code","c02f532a":"code","cc7ea7f0":"code","bb901322":"code","a5bc8937":"code","7c841d05":"code","69b78c01":"code","98687fe9":"code","44c2ba50":"code","8a3229ad":"code","bd1c2143":"code","36dec4db":"code","07dbf794":"code","6548efad":"code","d57e9697":"code","a90d12e3":"code","d2790ac9":"code","22eeace0":"code","764e613b":"code","3fa116a5":"code","1422c8d9":"code","48ffc9ca":"code","b35af28e":"code","731b878f":"code","6cb22aec":"code","c7a99736":"markdown","c8c63358":"markdown","b5fbcd04":"markdown","69064615":"markdown","6d899ce4":"markdown","aa68caa3":"markdown","5888f5ba":"markdown","106549d7":"markdown","fbfd9d6b":"markdown","2dfff9f4":"markdown","78efefdc":"markdown","b2a0860e":"markdown","0db54e24":"markdown","c47f8c11":"markdown","eec69c12":"markdown","5975f722":"markdown","1d488b61":"markdown","347cf562":"markdown","37e6c43a":"markdown","3ffad26b":"markdown","089a1d43":"markdown","d8cf0508":"markdown","7a427967":"markdown","1009a2fe":"markdown","56aad9b6":"markdown","3f186895":"markdown","a89bfe8c":"markdown","a8695f32":"markdown"},"source":{"c39a6b17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b32dd76":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","3e910af6":"# fetching train.csv and test.csv\ntrain_df=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_df.head()","55c94326":"# lets print these\nprint(train_df.columns)\nprint(test_df.columns)","d9a7d742":"# lets take deep observation about data\nprint(train_df.info())\nprint(test_df.info())","91d7f982":"#lets check for null values in both datset\nprint(train_df.isnull().sum())\nprint('\\t\\t\\t')\nprint(test_df.isnull().sum())","8d3872cd":"# lets describe more about data\ntrain_df.describe(include='all')","72abbe26":"#lets see relation bw different columns\ncorr_matrix=train_df.corr()\nplt.figure(figsize=(15,6))\nsns.heatmap(corr_matrix,annot=True)","9b12c891":"fig,ax=plt.subplots(2,2,figsize=(15,10))\nsns.countplot(train_df['Sex'],ax=ax[0][0])\nsns.countplot(train_df['Embarked'],ax=ax[0][1])\nsns.countplot(train_df['Pclass'],ax=ax[1][0])\nsns.countplot(train_df['SibSp'],ax=ax[1][1])\n\n\nax[0][0].set_title('Total no of male and female')\nax[0][1].set_title('Embarked distribution')\nax[1][0].set_title('Passenger class distribution')\nax[1][1].set_title('Sibling or spouse Distribution')\n\n","024adf77":"fig,ax=plt.subplots(1,2,figsize=(15,5))\nsns.distplot(train_df['Age'],hist=True,ax=ax[0])\nsns.distplot(train_df['Fare'],hist=True,ax=ax[1])\n\n\nax[0].set_title('Age distribution')\nax[1].set_title('Fare distribution')\n\n\n","c0fbdccb":"plt.figure(figsize=(15,15))\nsns.pairplot(train_df)","e8c84363":"# check for null value\ntrain_df.isnull().sum()","4ceeccd9":"#we will replace the age columns by finding the mean of age with respect to Gender as well as passenger class \ntrain_gp=train_df.groupby(['Sex','Pclass'])['Age'].mean()\nprint(train_gp)\ntest_gp=test_df.groupby(['Sex','Pclass'])['Age'].mean()\nprint(test_gp)\n\n\n","e8ec2952":"# this function will fill null value with the desire mean value\ndef fillAgeNa(df):\n    for i in range(len(df)) : \n        if pd.isnull(df.loc[i, \"Age\"]):\n            if (df.loc[i,'Sex']=='female') and (df.loc[i,'Pclass']==1) :\n                df.loc[i,'Age']=37\n            elif(df.loc[i,'Sex']=='female') and (df.loc[i,'Pclass']==2) :\n                 df.loc[i,'Age']=26\n            elif(df.loc[i,'Sex']=='female') and (df.loc[i,'Pclass']==3):\n                 df.loc[i,'Age']=22\n            elif(df.loc[i,'Sex']=='male') and (df.loc[i,'Pclass']==1):\n                 df.loc[i,'Age']=40\n            elif(df.loc[i,'Sex']=='male') and (df.loc[i,'Pclass']==2):\n                 df.loc[i,'Age']=30\n            elif(df.loc[i,'Sex']=='male') and (df.loc[i,'Pclass']==3):\n                 df.loc[i,'Age']=25\n    return df\n            \n                \n    ","106ed027":"ndf=train_df.copy()\ntrain_df=fillAgeNa(ndf)\ntrain_df.isnull().sum()\n#similarly for test data we will fill like\nndf=test_df.copy()\ntest_df=fillAgeNa(ndf)\ntrain_df['Embarked'].fillna('S',inplace=True)","7eb89459":"# now check for null values\ntrain_df.isnull().sum()","f96d8640":"# filling null value in fare column with mean\ntest_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)\ntest_df.isnull().sum()\ntest_df.shape","ac2b86e9":"# dropping cabin columns\ntrain_data=train_df.drop('Cabin',axis=1)\ntest_data=test_df.drop('Cabin',axis=1)\ntest_data.shape","c02f532a":"train_data.drop(['PassengerId','Name','Ticket'],inplace=True,axis=1)\ntest_data.drop(['PassengerId','Name','Ticket'],inplace=True,axis=1)","cc7ea7f0":"#perform encoding for categorical variable\nencd1=pd.get_dummies(train_data[['Sex','Embarked']],drop_first=True)\nencd2=pd.get_dummies(test_data[['Sex','Embarked']],drop_first=True)","bb901322":"# now lets concat the encoded categorical data\ntrain_data=pd.concat([train_data,encd1],axis=1)\ntest_data=pd.concat([test_data,encd2],axis=1)\n","a5bc8937":"print(train_data.head(2))\nprint(test_data.head(2))","7c841d05":"# now we will count the size of family\ntrain_data['FamilySize']=train_data['SibSp']+train_data['Parch']+1\ntest_data['FamilySize']=test_data['SibSp']+test_data['Parch']+1\n","69b78c01":"# here we are apply lambda function \ntrain_data['Isalone']=train_data['FamilySize'].apply(lambda x : 1 if x>1 else 0)\ntest_data['Isalone']=test_data['FamilySize'].apply(lambda x : 1 if x>1 else 0)","98687fe9":"from sklearn.preprocessing import LabelEncoder","44c2ba50":"train_data","8a3229ad":"# now lets drop unnecessry columns like Sex and Embarked\ntrain_data.drop(['Sex','Embarked','FamilySize'],axis=1,inplace=True)\ntest_data.drop(['Sex','Embarked','FamilySize'],axis=1,inplace=True)\n","bd1c2143":"scaler=MinMaxScaler()\nscaler.fit(train_data[['Fare']])\ntrain_data['Fare']=scaler.transform(train_data[['Fare']])\ntrain_data['Age']=scaler.fit_transform(train_data[['Age']])\ntest_data['Fare']=scaler.fit_transform(test_data[['Fare']])\ntest_data['Age']=scaler.fit_transform(test_data[['Age']])","36dec4db":"# here we are applying lamda function to check if sibling or spouse is present or not\ntrain_data['SibSp']=train_data['SibSp'].apply(lambda x: 1 if x>0 else 0)\ntest_data['SibSp']=test_data['SibSp'].apply(lambda x: 1 if x>0 else 0)","07dbf794":"# lets create one mopre important feature by multiplying age with pclass\ntrain_data['nw']=train_data['Age']*train_data['Pclass']\ntest_data['nw']=test_data['Age']*test_data['Pclass']","6548efad":"print(train_data.head(2))\nprint(test_data.head(2))","d57e9697":"# now everything looks good lets divide our data in x_train and y_train\nX_train=train_data.drop('Survived',axis=1)\ny_train=train_data[['Survived']]\nprint('shape of x train and y train')\nprint(X_train.shape,y_train.shape)\nX_test=test_data\nprint('shape of x test')\nprint(X_test.shape)\n","a90d12e3":"model1=RandomForestClassifier()\nmodel2=XGBClassifier()\nmodel3=LogisticRegression()\nmodel4=SVC(kernel='poly',gamma=1,C=0.1)\nmodel5=KNeighborsClassifier(n_neighbors=23,leaf_size=23,p=1)\n#model.fit(X_train,y_train)\nmodel=[model1,model2,model3,model4,model5]","d2790ac9":"c=0\nfor m in model:\n    c+=1\n    m.fit(X_train,y_train)\n    accur=round(m.score(X_train,y_train)*100,2)\n    print('Model',c)\n    print('accuracy =',accur)","22eeace0":"''' param_grid={'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}'''\n#grid = RandomizedSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 3) \n#grid.fit(X_train, y_train)","764e613b":"#grid.best_params_","3fa116a5":"model=RandomForestClassifier(n_estimators= 2000,\n  min_samples_split= 5,\n  min_samples_leaf= 2,\n  max_features= 'sqrt',\n  max_depth= None)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\ny_pred=pd.Series(y_pred)\ny_pred=y_pred.apply(lambda x: 1 if x else 0)\naccur=round(model.score(X_train,y_train)*100,2)\naccur\n","1422c8d9":"# sample submission file look like this\ndataframe=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ndataframe.head()","48ffc9ca":"# here we will create submission file according to the given sample\nsubm=pd.concat([test_df['PassengerId'],y_pred],axis=1)\nsubm.rename(columns={'PassengerId':'PassengerId',0:'Survived'},inplace=True)","b35af28e":"# lets verify our submission file with sample file\nsubm.head()","731b878f":"subm['Survived'].value_counts()","6cb22aec":"subm.to_csv('submission.csv',index=False)","c7a99736":"As we can see that train data has 177 missing age columns and 687 missing cabin columns.\nfor test data age columns have 86 as well as cabin columns have 327 missing values.\nwe would handle these values later.","c8c63358":"# Model Training","b5fbcd04":"### Here, we are doing hyper parameter optimization on our Random forest algorithm to increase accuracy and improve model","69064615":"## lets check for columns","6d899ce4":"# Submission file","aa68caa3":"As we can clearly see the accuracy of Random forest is very good for train data so we perform classification on random forest","5888f5ba":"Importing important libraries","106549d7":"we will coment out this as i find best params already so to save time i comment out on parameter tuning","fbfd9d6b":"we will make another feature as if person is alone or not","2dfff9f4":"As we can see that test data does not contain survived columns as we have to predict that.","78efefdc":"lets see the data now","b2a0860e":"## Now that everything is sorted lets scale our data ","0db54e24":"From the above plot we  can observe some points on train data before doing feature engineering.\n1.There are approx 300 female and 600 male on ship.\n2.Mostly above 600 belong to embarked (S) and 2nd most are C.\n3.approx 500 of them belongs to pclass-3 and approx 200 each from class 1 and 2.\n4.More than 600 people didnt have sibsp.\nThese data will be very useful for feature engineering process.","c47f8c11":"As we can see that there are 891 entries in train.csv and there are 418 enteries in test.csv","eec69c12":"# Exploratory Data Analysis","5975f722":"# **Titanic**\n\n![](https:\/\/cdn.britannica.com\/s:700x500\/79\/4679-050-BC127236\/Titanic.jpg)\n#### There has never been universal agreement over the number of lives lost in the sinking of the Titanic. Beginning with the first news reports of the disaster, inquirers have found it unwise to trust the original passenger and crew lists, which were rendered inaccurate by such factors as misspellings, omissions, aliases, and failure to count musicians and other contracted employees as either passengers or crew members. Agreement was made more difficult by the international nature of the disaster, essentially involving a British-registered liner under American ownership that carried more than 2,000 people of many nationalities. Immediately after the sinking, official inquiries were conducted by a special committee of the U.S. Senate (which claimed an interest in the matter on the grounds of the American lives lost) and the British Board of Trade (under whose regulations the Titanic operated). The figures established by these hearings are as follows:\n\n### U.S. Senate committee: 1,517 lives lost\n\n### British Board of Trade: 1,503 lives lost\n\n### Confusion over these figures was immediately aggravated by the official reports of these inquiries to the U.S. Senate and the British Parliament; these reports revised the numbers to 1,500 and 1,490, respectively. The figures have been revised, officially and unofficially, so many more times since 1912 that most researchers and historians concede that they will never know how many of the people sailing on the Titanic died.","1d488b61":"As we can see age has 177 null columns it means we have to fill data very significantly as it is one of the important feature.\nwe will do feature engineering on both test as well as train data side by side.","347cf562":"### This is My first Submission for kaggle compete.So score is not that much good if you have any advice or suggestion you can comment.\n# If You like it plese motivate me by upvoting. Thank you.","37e6c43a":"### After doing that much our data is pretty sorted and lets check how our data looks","3ffad26b":"### Handling categorical value such as SEX and EMBARKED","089a1d43":"## Model selection\n## As there are lots of model to apply classification we are using some of the model and the model which perform better will be applied\n### 1.Random Forest\n### 2. XGBoost classifier\n### 3.Support vector Machine classifier\n### 4.Logistics Regression\n### 5.KNeighbors classifier\n","d8cf0508":" Now our age column is fully sorted now. and we will have to look for cabin columns","7a427967":"Now we can say that it is accurate way to fill null data in age columns.","1009a2fe":"# Feature Engineering","56aad9b6":"### Saving the output file","3f186895":"cabin is drop as it is not that much important for our prediction and it also contains bunch of null data","a89bfe8c":"Now we will train our model on this data","a8695f32":"Locate the dataset in kaggle directory"}}