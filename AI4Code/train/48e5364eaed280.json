{"cell_type":{"5bb49034":"code","e5f3333f":"code","c65e6cbb":"code","0d6861d0":"code","491a5dba":"code","2960c92c":"code","2eb75925":"code","48c49326":"code","2f90f946":"code","f54571fb":"code","43494a7f":"code","464bde3c":"code","ba4ad54f":"code","18cb294d":"code","3594f2db":"code","eaf920d7":"code","c476b020":"code","9430dee8":"code","e3e25b1b":"code","7bced3af":"code","39a8e0b4":"code","b50d6942":"code","ef2778ad":"markdown","e1d3d534":"markdown","b6d38ab6":"markdown","ece75198":"markdown","ffdba160":"markdown","6c1a8f36":"markdown","7c679e1f":"markdown","66a6dd1f":"markdown","ad1b51d8":"markdown","9fe8b1ec":"markdown"},"source":{"5bb49034":"import os\nimport glob\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e5f3333f":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\n\nset_seed(42)","c65e6cbb":"import sys\nsys.path.append('..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D')\nfrom efficientnet_pytorch_3d import EfficientNet3D","0d6861d0":"path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\ntrain_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\nprint('Num of train samples:', len(train_data))\ntrain_data.head()\nimg_size = 256","491a5dba":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\ndef load_3d_dicom_images(scan_id, split = \"train\"):\n    \"\"\"\n    we will use some heuristics to choose the slices to avoid any numpy zero matrix (if possible)\n    \"\"\"\n    flair = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/FLAIR\/*.dcm\"))\n    t1w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1w\/*.dcm\"))\n    t1wce = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T1wCE\/*.dcm\"))\n    t2w = sorted(glob.glob(f\"{path}\/{split}\/{scan_id}\/T2w\/*.dcm\"))\n    \n    \n    flair_img = np.array([dicom2array(a) for a in flair[len(flair)\/\/2 - 25:len(flair)\/\/2 + 25]]).T\n    \n    if flair_img.shape[-1] < 50:\n        n_zero = 50 - flair_img.shape[-1]\n        flair_img = np.concatenate((flair_img, np.zeros((img_size, img_size, n_zero))), axis = -1)\n    #print(flair_img.shape)\n        \n    \n    \n    t1w_img = np.array([dicom2array(a) for a in t1w[len(t1w)\/\/2 - 25:len(t1w)\/\/2 + 25]]).T\n    if t1w_img.shape[-1] < 50:\n        n_zero = 50 - t1w_img.shape[-1]\n        t1w_img = np.concatenate((t1w_img, np.zeros((img_size, img_size, n_zero))), axis = -1)\n    #print(t1w_img.shape)\n    \n    \n    t1wce_img = np.array([dicom2array(a) for a in t1wce[len(t1wce)\/\/2 - 25:len(t1wce)\/\/2 + 25]]).T\n    if t1wce_img.shape[-1] < 50:\n        n_zero = 50 - t1wce_img.shape[-1]\n        t1wce_img = np.concatenate((t1wce_img, np.zeros((img_size, img_size, n_zero))), axis = -1)\n    #print(t1wce_img.shape)\n    \n    \n    t2w_img = np.array([dicom2array(a) for a in t2w[len(t2w)\/\/2 - 25:len(t2w)\/\/2 + 25]]).T\n    if t2w_img.shape[-1] < 50:\n        n_zero = 50 - t2w_img.shape[-1]\n        t2w_img = np.concatenate((t2w_img, np.zeros((img_size, img_size, n_zero))), axis = -1)\n    #print(t2w_img.shape)\n    \n    return np.concatenate((flair_img, t1w_img, t1wce_img, t2w_img), axis = -1)","2960c92c":"load_3d_dicom_images(\"00000\").shape","2eb75925":"slices = load_3d_dicom_images(\"00000\")\nprint(slices.shape)","48c49326":"def plot_imgs(imgs, cols=20, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(64,64)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i in range(cols):\n        img = imgs[:,:,i]\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n    \nplot_imgs(slices)","2f90f946":"# frames = []\n# for i in range(200):\n#     frames.append(np.array(slices[:,:,i], dtype = np.uint8))\n\n# fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n# out = cv2.VideoWriter('\/kaggle\/working\/out_video.mp4', fourcc, 15, (200,200))\n# for i in range(len(frames)):\n#     c_frame =  cv2.cvtColor(frames[i],cv2.COLOR_GRAY2RGB)\n#     out.write(c_frame)\n    \n# out.release()","f54571fb":"# # the video play doesn't work, you can download it to view\n\n# from IPython.display import HTML\n# from base64 import b64encode\n\n# def play(filename):\n#     html = ''\n#     video = open(filename,'rb').read()\n#     src = 'data:video\/mp4;base64,' + b64encode(video).decode()\n#     html += '<video width=1000 controls autoplay loop><source src=\"%s\" type=\"video\/mp4\"><\/video>' % src \n#     return HTML(html)\n\n# play('\/kaggle\/working\/out_video.mp4')","43494a7f":"# let's write a simple pytorch dataloader\n\n\nclass BrainTumor(Dataset):\n    def __init__(self, path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification', split = \"train\", validation_split = 0.0):\n        # labels\n        train_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n        self.labels = {}\n        brats = list(train_data[\"BraTS21ID\"])\n        mgmt = list(train_data[\"MGMT_value\"])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        if split == \"valid\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            self.ids = self.ids[:int(len(self.ids)* validation_split)] # first 20% as validation\n        elif split == \"train\":\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            self.ids = self.ids[int(len(self.ids)* validation_split):] # last 80% as train\n        else:\n            self.split = split\n            self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]\n            \n    \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        imgs = load_3d_dicom_images(self.ids[idx], self.split)\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,) * 200, (0.5,) * 200)])\n        imgs = transform(imgs)\n        \n        if self.split != \"test\":\n            label = self.labels[self.ids[idx]]\n            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n        else:\n            return torch.tensor(imgs, dtype = torch.float32)","464bde3c":"# testing the dataloader\ntrain_dataset = BrainTumor()\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=8)\n# val_dataset = BrainTumor(split=\"valid\")\n# val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=8)","ba4ad54f":"for img, label in train_loader:\n    print(img.shape)\n    print(label.shape)\n    break\n\n# for img, label in val_loader:\n#     print(img.shape)\n#     print(label.shape)\n#     break","18cb294d":"model = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr = 0.0001)\nn_epochs = 2","3594f2db":"# let's train\ngpu = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(gpu)\n\nfor epoch in range(n_epochs):  # loop over the dataset multiple times\n\n    train_loss = []\n    best_pres = 10000\n    model.train()\n    for i, data in tqdm(enumerate(train_loader, 0)):\n        x, y = data\n        \n        x = torch.unsqueeze(x, dim = 1)\n        x = x.to(gpu)\n        y = y.to(gpu)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(x)\n        loss = criterion(outputs, y)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        train_loss.append(loss.item())\n    avg_train = sum(train_loss) \/ len(train_loss)\n    print(f\"epoch {epoch+1} train: {avg_train}\")\n\n#     running_loss = []\n#     best_pres = 10000\n#     model.eval()\n#     for i, data in tqdm(enumerate(val_loader, 0)):\n\n#         x, y = data\n        \n#         x = torch.unsqueeze(x, dim = 1)\n#         x = x.to(gpu)\n#         y = y.to(gpu)\n\n#         # forward\n#         outputs = model(x)\n#         loss = criterion(outputs, y)\n\n#         # print statistics\n#         running_loss.append(loss.item())\n#     avg_pred = sum(running_loss) \/ len(running_loss)   \n#    print(f\"epoch {epoch+1} val: {avg_pred}\")\n    if avg_train < best_pres:\n        print('save model...')\n        best_pres = avg_train\n        torch.save(model.state_dict(),'best_loss.pt')","eaf920d7":"# model = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n# model.to(gpu)\n# checkpoint = torch.load(f\"best_loss.pt\")\n# model.load_state_dict(checkpoint)\n# model.eval()","c476b020":"# class test_BrainTumor(Dataset):\n#     def __init__(self, path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification', split = \"test\"):\n#         # labels\n#         train_data = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n#         self.labels = {}\n#         brats = list(train_data[\"BraTS21ID\"])  \n#         self.split = split\n#         self.ids = [a.split(\"\/\")[-1] for a in sorted(glob.glob(path + f\"\/{split}\/\" + \"\/*\"))]   \n#     def __len__(self):\n#         return len(self.ids)\n    \n#     def __getitem__(self, idx):\n#         imgs = load_3d_dicom_images(self.ids[idx], self.split)\n#         transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,) * 200, (0.5,) * 200)])\n#         imgs = transform(imgs)\n#         return torch.tensor(imgs, dtype = torch.float32)","9430dee8":"# test_dataset = test_BrainTumor(split = \"test\")\n# test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=8)","e3e25b1b":"# submission = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv\")","7bced3af":"# y_pred = []\n# ids = []\n\n# for e, batch in enumerate(test_loader):\n#     print(f\"{e}\/{len(test_loader)}\", end=\"\\r\")\n#     with torch.no_grad():\n#         tmp_pred = np.zeros((batch.shape[0], ))\n#         tmp_res = torch.sigmoid(model(batch.to(gpu))).cpu().numpy().squeeze()\n#         tmp_pred += tmp_res\n#         y_pred.extend(tmp_pred)\n#         ids.extend(batch[\"id\"].numpy().tolist())","39a8e0b4":"# submission = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred})\n# submission.to_csv(\"submission.csv\", index=False)","b50d6942":"# submission","ef2778ad":"### **Training**","e1d3d534":"### **MRI Slice Loading\/Processing**","b6d38ab6":"### **Data Loader**","ece75198":"**Inference**","ffdba160":"# RSNA-MICCAI Brain Tumor Radiogenomic Classificationn - **An approach with PyTorch EfficientNet 3D**\n\n## **Problem Description**:\n\nThere are structural multi-parametric MRI (mpMRI) scans for different subjects, in DICOM format. The exact mpMRI scans included are:\n\n* Fluid Attenuated Inversion Recovery (FLAIR)\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\n`train_labels.csv` - file contains the target **MGMT_value** for each subject in the training data **(e.g. the presence of MGMT promoter methylation)**.\n\nSo, it's a binary classification problem.\n\n## **A EfficientNet3D solution**:\n\n* For each patient, we consider 4 sequences (FLAIR, T1w, T1Gd, T2), and for each of those sequences we take 50 slices from the middle, and stack them, to get 50 x 4 = 200 slices. We resize the slices in shape (200, 200).\n\n* Construct an efficientnet-3d in pytorch with input shape (200, 200, 200).\n\n* Perform binary classification.\n","6c1a8f36":"### **Importing EfficientNet-3D**","7c679e1f":"### **Inspecting Labels**","66a6dd1f":"### **Model: EfficientNet-3D B0**","ad1b51d8":"### **Importing libraries**","9fe8b1ec":"### **Visualization**"}}