{"cell_type":{"5f19a910":"code","893c6735":"code","3f76ed93":"code","02ce349b":"code","76e9d022":"code","79a61434":"code","77c888ee":"code","b31b3756":"code","8facafcc":"code","fa144a54":"code","03bf033d":"code","138af96d":"code","3e2a7864":"code","0b91c7a4":"code","11595c7c":"code","10f442e0":"code","6f621abc":"code","7769c90b":"code","9ff8b229":"code","06af3929":"code","fbd9677e":"markdown","ae7cfb88":"markdown","2f9bec52":"markdown","8ea78d1c":"markdown","66091e49":"markdown","8c915e83":"markdown","458ae4b8":"markdown","3d475f28":"markdown","0ea57f69":"markdown","b7ce1234":"markdown","a89879fa":"markdown"},"source":{"5f19a910":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","893c6735":"model_data = pd.read_csv(\"\/kaggle\/input\/mp-feature-selection\/fs_data.csv\")\nmodel_data.head()","3f76ed93":"model_data.info()","02ce349b":"x = model_data.drop(['corona_result'],axis=1)\ny = model_data['corona_result']\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3,random_state=42)","76e9d022":"X_train.to_csv(\"X_train.csv\",index=False)\nX_test.to_csv(\"X_test.csv\",index=False)\ny_train.to_csv(\"y_train.csv\",index=False)\ny_test.to_csv(\"y_test.csv\",index=False)","79a61434":"X_train = pd.read_csv(\".\/X_train.csv\")\nX_test = pd.read_csv(\".\/X_test.csv\")\ny_train = pd.read_csv(\".\/y_train.csv\")\ny_test = pd.read_csv(\".\/y_test.csv\")","77c888ee":"def eval_metr(y_pred):\n  cm1 = confusion_matrix(y_test,y_pred)\n  unique_label = np.unique([y_test, y_pred])\n  cmtx = pd.DataFrame(\n    confusion_matrix(y_test, y_pred, labels=unique_label), \n    index=['true:{:}'.format(x) for x in unique_label], \n    columns=['pred:{:}'.format(x) for x in unique_label]\n  )\n  print(cmtx)\n  specificity1 = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n  print('\\nSpecificity : ', specificity1)\n  recall = cm1[1,1]\/(cm1[1,1]+cm1[1,0])\n  print(\"\\nRecall: \",recall)\n  precision = cm1[1,1]\/(cm1[0,1]+cm1[1,1])\n  print('\\nPrecision : ', precision)\n  accuracy = (cm1[0,0]+cm1[1,1])\/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n  print('\\nAccuracy : ', accuracy)","b31b3756":"from sklearn.linear_model import LogisticRegression\n\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train,y_train)\ny_pred=logistic_regression.predict(X_test)\nprint(classification_report(y_test, y_pred))","8facafcc":"eval_metr(pd.DataFrame(y_pred))","fa144a54":"from matplotlib import pyplot\nli = list(X_train.columns)\nimportance = logistic_regression.coef_[0]\nfor i,v in enumerate(importance):\n    print('%s, Score: %.5f' % (li[i],v))\npyplot.bar([x for x in range(len(importance))], importance)\npyplot.show()","03bf033d":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\ngnb_pred=gnb.predict(X_test)\nprint(classification_report(y_test, gnb_pred))","138af96d":"eval_metr(pd.DataFrame(gnb_pred))","3e2a7864":"from sklearn.inspection import permutation_importance\n\nimps = permutation_importance(gnb, X_test, y_test)\nimportances = imps.importances_mean\nstd = imps.importances_std\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(10, 7))\nplt.title(\"Feature importances\")\nplt.bar(range(X_test.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xlim([-1, X_test.shape[1]])\nplt.show()","0b91c7a4":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)\nrfc_pred=rfc.predict(X_test)\nprint(classification_report(y_test, rfc_pred))","11595c7c":"eval_metr(pd.DataFrame(rfc_pred))","10f442e0":"plt.barh(X_train.columns.values.tolist(),rfc.feature_importances_)","6f621abc":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(max_depth=3,random_state=42, objective='binary:logistic', eval_metric= \"logloss\")\nxgb.fit(X_train,y_train)\nxgb_pred=xgb.predict(X_test)\nprint(classification_report(y_test, xgb_pred))","7769c90b":"eval_metr(pd.DataFrame(xgb_pred))","9ff8b229":"plt.barh(X_train.columns.values.tolist(),xgb.feature_importances_)","06af3929":"# import pickle\n# model_path = 'model.pkl'\n# model_pickle = open(model_path, 'wb')\n# pickle.dump(rfc, model_pickle)\n# model_pickle.close()","fbd9677e":"### Based On the above results Creating a pickle file to save the best model results","ae7cfb88":"### Data Loading","2f9bec52":"#### Naive Bayes","8ea78d1c":"#### Evaluation Metric Function","66091e49":"#### Random Forest ","8c915e83":"### Data Modeling\n\n> *Data Partition (70:30 split)*","458ae4b8":"#### Logistic Regression","3d475f28":"## Step 4: Data Modeling\n<br>\n\n> **By: Sana Shaikh & Omkar Patil**","0ea57f69":"#### XGBoost","b7ce1234":"### Evaluation Parameters\n\n> ![Formulae](https:\/\/raw.githubusercontent.com\/mykeysid10\/Sem-6-Risk-Prediction-of-Covid-19-Cases\/main\/ML%20Source%20Code\/Confusion%20Matrix.jpg)\n> 1. **Sensitivity (Recall)** : *It is the ability of a test to correctly classify an individual as COVID Positive patient.*\n> 2. **Specificity** : *It is the ability of a test to correctly classify an individual as COVID Negative patient.*\n> 3. **Precision** :  *It is the ratio of correctly predicted positive observations to the total predicted positive observations.*","a89879fa":"### Conclusion\n\n> \n> ![Evaluation Parameters](https:\/\/raw.githubusercontent.com\/mykeysid10\/Sem-6-Risk-Prediction-of-Covid-19-Cases\/main\/ML%20Source%20Code\/Model-Evaluation-Parameters.png)\n> \n> 1. *In this scenerio best model must have type 2 error as low as possible and type 1 error should be low.*\n> 2. *XGBoost performs slightly better than other 3 models.*"}}