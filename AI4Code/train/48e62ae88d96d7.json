{"cell_type":{"9db2fbe5":"code","2240c6bd":"code","42355053":"code","6b4acd17":"code","c6c280ef":"code","23a29e49":"code","4912c376":"code","da2d4a7c":"code","b6a04f7f":"code","b63ad9ea":"code","ef7bcdeb":"code","e6c747b9":"code","5df624a5":"code","8dc832ab":"code","c8f5dc43":"code","3fa30ce3":"code","58234df9":"code","c88b1333":"code","f84512ca":"code","e6fdf65c":"code","6a137434":"code","513bf577":"code","c8a10d14":"code","58a25783":"markdown","1a6dc405":"markdown","815b0229":"markdown","e479f742":"markdown","bc0c453c":"markdown","4f6acf97":"markdown","e1a5d7b0":"markdown","186ad827":"markdown","cdbe43a7":"markdown","d3adc9d6":"markdown","29dc874b":"markdown","00375b98":"markdown","54528192":"markdown","199b7cce":"markdown","c6a9fbc8":"markdown","17e50c35":"markdown","f1a9bf60":"markdown","ad54f7dc":"markdown","67ea6079":"markdown"},"source":{"9db2fbe5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy # linear algebra\nimport pandas  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn\nimport matplotlib\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn import decomposition\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport math\nfrom tensorflow import keras\nimport math\nimport scipy\nstate=72\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2240c6bd":"df = pandas.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\n#df2 = pandas.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv\")","42355053":"df.describe(include = \"all\")","6b4acd17":"print(df.isna().sum().sort_values(ascending=False))","c6c280ef":"df.shape","23a29e49":"df.head(10)","4912c376":"seaborn.histplot(data = df, x = \"output\",stat = \"count\",bins=2,discrete=True)\nprint(\"Skewness = {0}\".format(df[\"output\"].skew()))","da2d4a7c":"fig, axes = matplotlib.pyplot.subplots(1,3, figsize = (15,5))\nmatplotlib.pyplot.subplots_adjust(wspace=0.3)\n\n\nplot1 = seaborn.kdeplot(data = df.loc[df[\"output\"]==0], x = \"age\",hue=\"sex\", ax = axes [0])\nplot1.set_title (\"Pacient Age Low Prob\")\n\nplot2 = seaborn.kdeplot(data = df.loc[df[\"output\"]==1], x = \"age\", hue=\"sex\", ax = axes[1])\nplot2.set_title (\"Pacient Age High Prob\")\n\nplot3 = seaborn.histplot (data = df, x=\"sex\",stat=\"count\",ax=axes[2], discrete = True)\nplot3.set_title (\"Number of People by Sex\")\n\nprint(\"Median Age for Patients with Lower Heart Attack Probability: {0:0.2f}\".format(df.loc[df[\"output\"]==0][\"age\"].median()))\nprint(\"Median Age for Patients with Higher Heart Attack Probability: {0:0.2f}\\n\\n\".format(df.loc[df[\"output\"]==1][\"age\"].median()))","b6a04f7f":"fig, axes = matplotlib.pyplot.subplots(1,2, figsize = (10,5))\nplot1 = seaborn.histplot(data = df.loc[df[\"output\"]==0], x = \"cp\", discrete = True, ax = axes[0])\nplot1.set_title(\"Chest Pain People Low Prob\")\nplot2 = seaborn.histplot(data = df.loc[df[\"output\"]==1], x = \"cp\", discrete = True, ax = axes[1])\nplot2.set_title(\"Chest Pain People High Prob\")\nprint(\"Average Chest Pain for Patients with Low Probability of Heart Attack: {0:0.2f}\".format(df.loc[df[\"output\"]==0][\"cp\"].mean()))\nprint(\"Average Chest Pain for Patients with High Probability of Heart Attack: {0:0.2f}\\n\\n\".format(df.loc[df[\"output\"]==1][\"cp\"].mean()))","b63ad9ea":"fig, axes = matplotlib.pyplot.subplots(figsize = (8,5))\nplot1 = seaborn.kdeplot(data = df, x = \"thalachh\", hue = \"output\")\nplot1.set_title(\"Average Heart Rate\")\nprint(\"Average Heart Rate High Prob. Heart Attack: {0:0.2f}\\nAverage Heart Rate Low Prob Heart Attack: {1:0.2f}\\n\".format(df.loc[df[\"output\"]==1][\"thalachh\"].mean(),df.loc[df[\"output\"]==0][\"thalachh\"].mean()))","ef7bcdeb":"cat_columns = list()\nnum_columns = list()\nfor column in df.columns:\n    if(df[column].dtype != \"object\"):\n        num_columns.append(column)\n    else:\n        cat_columns.append(column)\nprint(len(cat_columns))","e6c747b9":"corr_data = df.corr()\nprint(corr_data[\"output\"].sort_values(ascending=False)[1:4])\nprint(corr_data[\"output\"].sort_values(ascending=True)[:3])\ncorr_data.style.background_gradient(cmap='coolwarm', axis=None).set_precision(6)","5df624a5":"X_columns = df.columns.tolist()\nX_columns = X_columns[:-1]\nY_columns = \"output\"\n\nX = df[X_columns]\nY = df[Y_columns]\nstd_scaler = preprocessing.StandardScaler()\n\nX_scaled = pandas.DataFrame(std_scaler.fit_transform(X), columns = X_columns)\npca = decomposition.PCA(n_components = 10)\nprincipalComponents = pca.fit_transform(X_scaled)\ndf_principal = pandas.DataFrame(data = principalComponents\n             , columns = [\"PC1\", \"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"])\nprint(\"Total Variance of Principal Components: {0}\".format(pca.explained_variance_ratio_.sum()))","8dc832ab":"\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X_scaled, Y, test_size=0.3, random_state=state)\n\n\nrand_list = {\"C\": scipy.stats.uniform(2, 30)}\n\nclf = sklearn.linear_model.LogisticRegression (penalty = \"l1\",solver=\"saga\", random_state=state)\n\nrand_search = sklearn.model_selection.RandomizedSearchCV(clf, param_distributions = rand_list, n_iter = 20, cv = 3, scoring = \"f1\", random_state=state) \nrand_search.fit(X_train, Y_train) \nprint(rand_search.best_params_)\n\npandas.DataFrame(rand_search.cv_results_).head(5)","c8f5dc43":"model = sklearn.linear_model.LogisticRegression (penalty = \"l2\",solver=\"saga\", C = rand_search.best_params_.get(\"C\",20), random_state=state)\nmodel.fit(X_train, Y_train)\nprint(\"\\nF1 Score: {0:0.2f}\\n\".format(sklearn.metrics.f1_score(Y_test,model.predict(X_test))))\nprint(sklearn.metrics.classification_report (Y_test, model.predict(X_test)))\nsklearn.metrics.plot_confusion_matrix(model, X_test, Y_test, cmap = matplotlib.pyplot.cm.Blues, normalize = \"true\")","3fa30ce3":"fig, axes = matplotlib.pyplot.subplots(1, figsize = (10,8))\nfor i in range(0,13):\n    print(\"Feature: {0} - {1:0.4f}\".format(X_scaled.columns.tolist()[i],model.coef_[0][i]))\nplot1 = seaborn.barplot(x = X_train.columns.tolist(),y = model.coef_[0], palette=['red' if (x > 0.7 or x < -0.7) else 'blue' for x in model.coef_[0]], ax = axes)\n\n#plot1.set_xticklabels(X_train.columns)\n","58234df9":"from sklearn import ensemble\n\nrand_list = {\n            \"max_depth\":[None,1,2],\n            \"min_samples_split\":[2,3,4]}\n\nclf = ensemble.RandomForestClassifier(random_state=state)\n#f1 = sklearn.metrics.make_scorer(sklearn.metrics.f1_score)\nrand_search = sklearn.model_selection.RandomizedSearchCV(clf, param_distributions = rand_list, n_iter = 20, cv = 3, scoring = \"f1\",return_train_score=True, error_score = \"raise\", random_state=state) \nrand_search.fit(X_train, Y_train) \nprint(rand_search.best_params_)\n\n\npandas.DataFrame(rand_search.cv_results_).head(5)","c88b1333":"model2 = ensemble.RandomForestClassifier(max_depth = rand_search.best_params_.get(\"max_depth\"), min_samples_split = rand_search.best_params_.get(\"min_samples_split\"), random_state=state)\nmodel2.fit(X_train,Y_train)\nprint(\"\\nF1 Score: {0:0.2f}\\n\".format(sklearn.metrics.f1_score(Y_test,model2.predict(X_test))))\nprint(sklearn.metrics.classification_report (Y_test, model2.predict(X_test)))\nsklearn.metrics.plot_confusion_matrix(model2, X_test, Y_test,cmap = matplotlib.pyplot.cm.Blues, normalize = \"true\")","f84512ca":"fig, axes = matplotlib.pyplot.subplots(1, figsize = (10,6))\nfor i in range(0,13):\n    print(\"Feature: {0} - {1:0.4f}\".format(X_scaled.columns.tolist()[i],model2.feature_importances_[i]))\nplot1 = seaborn.barplot(x = X_train.columns.tolist(),y = model2.feature_importances_, palette=['red' if (x > 0.125) else 'blue' for x in model2.feature_importances_])\n\n#plot1.set_xticklabels(X_train.columns)","e6fdf65c":"rand_list = {\n            \"n_neighbors\":[5,6,7,8,9,10],\n            \"algorithm\":[\"ball_tree\",\"kd_tree\",\"brute\"]}\n\nclf = sklearn.neighbors.KNeighborsClassifier()\n\nrand_search = sklearn.model_selection.RandomizedSearchCV(clf, param_distributions = rand_list, n_iter = 20, cv = 3, scoring = \"f1\",return_train_score=True, error_score = \"raise\",random_state=state) \nrand_search.fit(X_train, Y_train) \nprint(rand_search.best_params_)\n\npandas.DataFrame(rand_search.cv_results_).head(5)\n","6a137434":"model3 = sklearn.neighbors.KNeighborsClassifier(n_neighbors = rand_search.best_params_.get(\"n_neighbors\"), algorithm = rand_search.best_params_.get(\"algorithm\"))\nmodel3.fit(X_train,Y_train)\nprint(\"\\nF1 Score: {0:0.2f}\\n\".format(sklearn.metrics.f1_score(Y_test,model3.predict(X_test))))\nprint(sklearn.metrics.classification_report (Y_test, model3.predict(X_test)))\nsklearn.metrics.plot_confusion_matrix(model3, X_test, Y_test,cmap = matplotlib.pyplot.cm.Blues, normalize = \"true\")","513bf577":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=numpy.linspace(.1, 1.0, 5), scoring = None):\n    \n    \n    if axes is None:\n        _, axes = matplotlib.pyplot.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True, scoring = scoring)\n        \n        \n        \n    train_scores_mean = numpy.mean(train_scores, axis=1)\n    train_scores_std = numpy.std(train_scores, axis=1)\n    test_scores_mean = numpy.mean(test_scores, axis=1)\n    test_scores_std = numpy.std(test_scores, axis=1)\n    fit_times_mean = numpy.mean(fit_times, axis=1)\n    fit_times_std = numpy.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n    \n\n    return matplotlib.pyplot","c8a10d14":"fig, axes = matplotlib.pyplot.subplots(3,3,figsize = (16,12))\nplot_learning_curve(model, \"Learning Curves Logistic\", X_scaled, Y, axes=axes[:, 0], ylim=(0.7, 1.01),n_jobs=4, scoring=\"f1\")\nmatplotlib.pyplot.subplots_adjust(hspace=0.4)\n\nplot_learning_curve(model2, \"Learning Curves Random Florest\", X_scaled, Y, axes=axes[:, 1], ylim=(0.7, 1.01),n_jobs=4, scoring=\"f1\")\nmatplotlib.pyplot.subplots_adjust(hspace=0.4)\n\nplot_learning_curve(model3, \"Learning Curves K Neighbors\", X_scaled, Y, axes=axes[:, 2], ylim=(0.7, 1.01),n_jobs=4, scoring=\"f1\")\nmatplotlib.pyplot.subplots_adjust(hspace=0.4)","58a25783":"A simple Logistic Regression model already shows fairly good results using all the features. We are using a L1 penalty in an atempt to minimize the weights of useless features and usind 3 folds in the Cross-Validation as the test samples is already quite small. Also using Hyperparameter Tuning for the Regularization parameter.","1a6dc405":"The maximum Heart Rate of the Patients also shows promising factor to detect a possible Heart Attack ","815b0229":"We can see that both Logistic Regression and Random Florest have similiar Weight's importance. However the Random Florest Model determines that \"maximum heart rate achieved\" and the \"number of major vessels\" are also very important features.","e479f742":"# Feature Engineering","bc0c453c":"Due to the higher score, the model that better suits this Dataset is Random Florest!","4f6acf97":"A K Nearest Neighbors with Hyperparameter Tuning has a lower F1 score, of up to 0.84.","e1a5d7b0":"The Random Florest model shows a higher score for this dataset. However, Logistic Regression shows a higher scalability to a much larger dataset. In this case we are only conserned with the score of the model. K Nearest Neighbors does have a higher scalability than Random Florest and it does seem to tend to a similar score, however more samples are needed to confirm this","186ad827":"Let's try one last model - K Nearest Neighbors","cdbe43a7":"As we can see men are both the major sex in the group of patients with higher prob. of a Heart Attack and in the group of people with lower prob. of Heart Attack, which leads to the conclusion of men being the principal victim of a Heart Attack. Moreover, people in their 50s are very likely to experience some signs of Heart Atack.","d3adc9d6":"Let's compare it to a Random Florest model","29dc874b":"Simiarly, patients with higher probability of Heart Attack showed a higher level of chest pain.","00375b98":"The features here dont show a Linear Correlation with the output variable. With that, choosing the features is a bit harder. \n* We can try to apply PCA and see which principal components have the majority of the variance. Due to the smal number of features, PCA isnt a good option as we will need most of the features to accomplish a variance value of around 90%\n* Since we have a small number of features we can simply choose them all and test\n","54528192":"We can see that the Logistic Regression model considers \"Sex\" and \"Chest Pain\" as the most importante features for the problem. That confirms what we saw earlier.","199b7cce":"We can see right from the start that are Y variable, the variable we want to be able to predict, is a little bit Skewed. Its a minimal difference but we with later see how this sightly impacts our models","c6a9fbc8":"All the data is numeric. No need to apply any type of Label Enconding","17e50c35":"This model, with Hyperparameter Tuning, has an increse of 0.02, average, in the F1 Score in comparision with Logistic Regression.There is still the problem of the model predicting more False Negatives than False Positves due to higher number of samples of Class \"1\"","f1a9bf60":"We will plot the Correlation Matrix in order to see any type of Linear Correlation between features. Since this is a Classification problem, most likely there will be low value of Linear Correlation","ad54f7dc":"We can see Logistic Regression has a solid F1 Score of 0.8, failling more when predicting False Negatives. Also because there are usually more \"1\" in the test data than \"0\"","67ea6079":"# Data Analysis"}}