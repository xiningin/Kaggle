{"cell_type":{"63498446":"code","855c5fcd":"code","3c8e547a":"code","703d7180":"code","d29f9603":"code","3f5a1fda":"code","58c95afc":"code","8ef0b540":"code","343e6499":"code","bdbd9d8d":"code","083b066a":"code","ce799575":"code","6dc2dbaf":"code","6259d260":"code","a2c6bb64":"code","9792a6dc":"code","52f25ba8":"code","1d8f7197":"code","2487c1a3":"code","42f42c0f":"code","e4dd5be3":"code","53f5b339":"code","9e56f041":"code","0cac952d":"code","66c0b72c":"code","b3605e2f":"code","43d6a114":"code","24e73067":"code","f01867f0":"code","c76c019e":"code","310dd28f":"code","6f43460c":"code","1fb9ab7c":"code","eaac24c4":"code","c8878f0c":"code","96010711":"code","71d748eb":"code","ac4cdcb2":"code","cec372e5":"code","cfe052e1":"code","8077ccdc":"code","c3199c53":"code","cce32190":"markdown","267c6c1a":"markdown","cc5eaa83":"markdown","d2414697":"markdown","b42d9751":"markdown","1c163036":"markdown","adbe43d1":"markdown","d812b002":"markdown","d5be20b3":"markdown","5dad4c8e":"markdown","63ee4dbe":"markdown","83ccad37":"markdown","e798d8dd":"markdown","e805733f":"markdown","5f67641d":"markdown","ce1728e3":"markdown"},"source":{"63498446":"# Check TensorFlow version\n\nimport tensorflow as tf\nprint(tf.__version__)\n","855c5fcd":"#Lists the available physical devices as CPU or GPU or TPU or any other pluggable\n\ntf.config.list_physical_devices()\n","3c8e547a":"!nvidia-smi -L","703d7180":"# Clear any logs from previous runs\n!rm -rf .\/training_logs\/ \n!mkdir .\/training_logs\/","d29f9603":"# Download Ngrok to tunnel the tensorboard port to an external port\n!wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n\n# Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\nimport os\nimport multiprocessing\n\n\npool = multiprocessing.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir .\/training_logs\/ --host 0.0.0.0 --port 6006 &\",\n                        \".\/ngrok http 6006 &\"\n                        ]]","3f5a1fda":"# Get helper functions file\n\n!wget https:\/\/raw.githubusercontent.com\/mrdbourke\/tensorflow-deep-learning\/main\/extras\/helper_functions.py\n","58c95afc":"# Import series of helper functions for the notebook (we've created\/used these in previous notebooks)\n\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys\n","8ef0b540":"# Get TensorFlow Datasets\n\nimport tensorflow_datasets as tfds\n","343e6499":"# Availability check\n\ndatasets_list = tfds.list_builders() # gets all available TFDS datasets\nprint(\"food101\" in datasets_list)\n","bdbd9d8d":"# Load the food101 dataset using tfds.load function\n\n(train_data, test_data), ds_info = tfds.load(\n    name=\"food101\", # target dataset to get from TFDS\n    split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n    shuffle_files=True, # shuffle files on download?\n    as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n    with_info=True\n    )\n","083b066a":"# review some example\n\ntfds.visualization.show_examples(train_data,ds_info)\n","ce799575":"# Prinout dataset detailed info\n\nprint('Number of class' , ds_info.features[\"label\"].num_classes)\nprint('Shape', ds_info.features.shape)\nprint('Dtype', ds_info.features.dtype)\n","6dc2dbaf":"# Get class names\n\nclass_names = ds_info.features[\"label\"].names\nclass_names\n","6259d260":"# Take one sample off the training data\n\ntrain_one_sample = train_data.take(1) # samples are in format (image_tensor, label)\ntrain_one_sample\n","a2c6bb64":"# Output info about our training sample\n\nfor image, label in train_one_sample:\n  print(f\"\"\"\n  Image shape : {image.shape}\n  Image dtype : {image.dtype}\n  Target class from Food101 : {label}\n  Class name : {class_names[label.numpy()]}\"\"\")","9792a6dc":"# What does an image tensor from TFDS's Food101 look like?\nimage","52f25ba8":"# Plot an image tensor\nimport matplotlib.pyplot as plt\nplt.imshow(image)\nplt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\nplt.axis(False);","1d8f7197":"# Converts image datatype from 'uint8' -> 'float32' and \n# reshapes image to [img_shape, img_shape, color_channels]\n\ndef preprocess_img(image, label, img_shape=224):\n    image = tf.image.resize(image, [img_shape, img_shape]) # reshape\n    return tf.cast(image, tf.float32), label # converts img -> tensor\n","2487c1a3":"# Print image, shape, dypes for before and after preprocessing\n\npreprocessed_img = preprocess_img(image, label)[0]\nprint(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\nprint(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")\n","42f42c0f":"# We can still plot our preprocessed image as long as we \n# divide by 255 (for matplotlib capatibility)\nplt.imshow(preprocessed_img\/255.)\nplt.title(class_names[label])\nplt.axis(False);","e4dd5be3":"# Map train_data with processing function (& parallize) it\n\ntrain_data = train_data.map(\n    map_func=preprocess_img,\n    num_parallel_calls=tf.data.AUTOTUNE\n)\n\n#Reshuffle & Batch the data & prefecth it\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Map prepreprocessing function to test data\ntest_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Turn test data into batches (don't need to shuffle)\ntest_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)\n","53f5b339":"# Review train & test dataset\n\ntrain_data, test_data\n","9e56f041":"# Create TensorBoard callback\n\nfrom helper_functions import create_tensorboard_callback\n\n# Create ModelCheckpoint callback to save model's progress\n\ncheckpoint_path = \"model_checkpoints\/cp.ckpt\" # saving weights requires \".ckpt\" extension\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    checkpoint_path,\n    montior=\"val_acc\", # save the model weights with best validation accuracy\n    save_best_only=True, # only save the best weights\n    save_weights_only=True, # only save model weights (not whole model)\n    verbose=0 # don't print out whether or not model is being saved\n)  ","0cac952d":"# data augmentation to preprocess the given dataset\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.models import Sequential\n\n# Creating a sequential model to randomize the preprocessing image for multi-dimentional augmentation\n\ndata_augmentation=Sequential([\n  preprocessing.RandomFlip(\"horizontal\"), \n  preprocessing.RandomRotation(0.2), \n  preprocessing.RandomHeight(0.2), \n  preprocessing.RandomWidth(0.2), \n  preprocessing.RandomZoom(0.2),\n])","66c0b72c":"input_shape=(224,224,3)\n\n# Create base model\nbase_model = tf.keras.applications.EfficientNetB4(include_top=False)\nbase_model.trainable = False # freezes base model layers\n\n# Create Functional model \ninputs = layers.Input(shape=input_shape, name=\"input_layer\")\n\n# set base_model to inference mode only \nx = base_model(inputs, training=False)\n\nx = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n\n# want one output neuron per class & Separate the output as an activation layer\noutputs = layers.Dense(len(class_names), activation = \"softmax\")(x) \n\nmodel = tf.keras.Model(inputs, outputs)\n\n# Compile the model\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are not one-hot\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n","b3605e2f":"# Check out our model\n\nmodel.summary()\n","43d6a114":"# Fit the model with callbacks\n\nfeature_extract=model.fit(\n        train_data,\n        epochs=3,\n        steps_per_epoch=len(train_data),\n        validation_steps=int(0.15*len(test_data)),\n        validation_data=test_data,\n        callbacks=[\n                   create_tensorboard_callback(\n                        dir_name=\"training_logs\", \n                        experiment_name=\"efficientb4_101_classes_all_data_feature_extract\"),\n                   model_checkpoint\n                  ]\n)\n","24e73067":"results_feature_extract_model=model.evaluate(test_data)\nresults_feature_extract_model","f01867f0":"# Create ModelCheckpoint callback to save best model during fine-tuning\n\ncheckpoint_path = \"fine_tune_checkpoints\/\"\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    checkpoint_path,\n    save_best_only=True,\n    monitor=\"val_loss\"\n)\n","c76c019e":"# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n\nearly_stopping=tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", # watch the val loss metric\n    patience=3 # if val loss decreases for 3 epochs in a row, stop training\n)\n","310dd28f":"# Creating learning rate reduction callback\n\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n    patience=3,\n    verbose=1, # print out when learning rate goes down \n    min_lr=1e-7\n)\n","6f43460c":"for layer in model.layers:\n  layer.trainable = True # set all layers to trainable\n  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) ","1fb9ab7c":"# Compile the model\n# Sparse_categorical_crossentropy loss fn for labels that aren't one-hot encoded\n# 10x lower learning rate than the default\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              metrics=[\"accuracy\"]\n             )\n","eaac24c4":"model.summary()","c8878f0c":"# Start to fine-tune (all layers)\n\nfine_tune = model.fit(\n    train_data,\n    epochs=100, # fine-tune for a maximum of 100 epochs\n    steps_per_epoch=len(train_data),\n    validation_data=test_data,\n    validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n    callbacks= [\n                # saves only the best model during training\n                create_tensorboard_callback(\"training_logs\",\"efficientb4_101_classes_all_data_fine_tuning\"),\n                early_stopping, # stop model after X epochs of no improvements\n                reduce_lr\n                ]\n)\n","96010711":"history=model.evaluate(test_data)","71d748eb":"import os\n\n# Create a function to save an model\ndef save_model(model, suffix=None):\n    # Create a model directory pathname with current time\n    model_path = suffix + '.h5' #h5 is like .pkl or .joblib\n    print(f'Saving model to: {model_path}...')\n    model.save(model_path)\n    return model_path","ac4cdcb2":"# Save the model\nsaved = save_model(model, suffix='.\/Food Vision')","cec372e5":"# Create a function to load the model\nimport tensorflow_hub as hub\n\ndef load_model(model_path):\n    print(f'Loading model from: {model_path}...')\n    model = tf.keras.models.load_model(\n        model_path, \n        custom_objects={\"KerasLayer\": hub.KerasLayer})\n    return model","cfe052e1":"loaded_model = load_model(saved)\nloaded_model","8077ccdc":"loaded_model.evaluate(test_data)","c3199c53":"! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","cce32190":"## Create model callbacks\nIncluding:\n- Tensorboard\n- Modelcheckpoint","267c6c1a":"## fine-tuning our model's layers ","cc5eaa83":"# Project Name: Food Vision\n\n        - Identifies the name of the food with an image input\n        \n- Author : Saravanan R        \n- Reg. No: 39110904\n        ","d2414697":"## Creating model for feature extraction","b42d9751":"## Saving and loading a model","1c163036":"## workaround to access tensorboard","adbe43d1":"#### Creating more callback functions","d812b002":"## Download & load Food101 dataset","d5be20b3":"## Batch & prepare dataset","5dad4c8e":"### Check GPU availability","63ee4dbe":"## Fitting back the model to fine-tune the model (Transfer learning)","83ccad37":"#### Set class_names","e798d8dd":"### Fitting the model for feature extraction","e805733f":"## Processing data","5f67641d":"## Explore & visulized dataset ","ce1728e3":"### Download custom helpers functions"}}