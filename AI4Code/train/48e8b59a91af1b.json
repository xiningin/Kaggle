{"cell_type":{"1a317290":"code","23c30fec":"code","4fb95f74":"code","5221a3ee":"code","c7369e16":"code","04d1319e":"code","9d22a2ad":"code","16208322":"code","21c27ca2":"code","a9790043":"code","56a8328a":"code","409a3b01":"code","cae03eb8":"code","0ac81bc9":"code","1d700ad1":"code","85b8b64b":"code","06ede529":"code","1c623b7a":"code","ac330f39":"code","1277776a":"code","044851ac":"code","d7682736":"code","5a09d856":"code","d8a83947":"code","9f6d6510":"code","e9ef088e":"code","edc5535b":"code","ab5921cc":"code","76547964":"code","08229d6b":"code","2d50d9ce":"code","d8cb75c7":"code","780b413b":"code","760b188f":"code","e3cb498b":"code","0ea8223b":"code","1d3174e6":"code","6ddb6aa1":"code","08950729":"code","ff5a39a0":"code","854a0440":"code","4db07654":"code","0efcd0e2":"code","aa186d43":"code","6ec7bc05":"code","f8217bd5":"code","8936e7bd":"code","d061ec45":"code","3041436a":"code","edd995ab":"code","275d1412":"code","73eb747c":"code","7cd2ad8b":"code","9f876547":"code","e08d41e4":"markdown","b85f6f71":"markdown","37d1c95b":"markdown","eeb8664e":"markdown","e038b9c1":"markdown","931e7578":"markdown","38223474":"markdown","c0b67cc7":"markdown","69a4b0d8":"markdown","ac26e60e":"markdown","e9246c07":"markdown","291c5796":"markdown","a0753d86":"markdown","9e989fa3":"markdown","6b4258c7":"markdown","1cde4d94":"markdown","41fbfb8e":"markdown","8fd8c23d":"markdown","64b87b7f":"markdown","46cc8d0b":"markdown","3756d092":"markdown","e3dbc6a7":"markdown","afb99020":"markdown","f39af477":"markdown","d19a1ff5":"markdown","b66b78e3":"markdown","acff8a94":"markdown","772253fd":"markdown"},"source":{"1a317290":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors","23c30fec":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.shape","4fb95f74":"train.head()","5221a3ee":"train[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');","c7369e16":"train[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","04d1319e":"# we have no missing values\ntrain.isnull().any().any()","9d22a2ad":"print('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","16208322":"train['target'].value_counts()","21c27ca2":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","a9790043":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nn_fold = 10\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","56a8328a":"def train_model(X, X_test, y, params, folds=folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        # print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=2000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=500,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","409a3b01":"# A lot of people are using logreg currently, let's try\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","cae03eb8":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr_repeated, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)","0ac81bc9":"eli5.show_weights(model, top=40)","1d700ad1":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","85b8b64b":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","06ede529":"perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, top=50)","1c623b7a":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(perm).feature if 'BIAS' not in i]\nX_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","ac330f39":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr1, prediction_lr1, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","1277776a":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","044851ac":"explainer = shap.LinearExplainer(model, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","d7682736":"sfs1 = SFS(model, \n           k_features=(10, 15), \n           forward=True, \n           floating=False, \n           verbose=0,\n           scoring='roc_auc',\n           cv=folds,\n          n_jobs=-1)\n\nsfs1 = sfs1.fit(X_train, y_train)","5a09d856":"fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n\nplt.ylim([0.8, 1])\nplt.title('Sequential Forward Selection (w. StdDev)')\nplt.grid()\nplt.show()","d8a83947":"top_features = list(sfs1.k_feature_names_)\nX_train = train[top_features]\nX_test = test[top_features]\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","9f6d6510":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e9ef088e":"lr = linear_model.LogisticRegression(solver='liblinear', max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2'],\n                  'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n                  'solver': ['newton-cg', 'sag', 'lbfgs']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","edc5535b":"lr = linear_model.LogisticRegression(solver='liblinear', max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","ab5921cc":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","76547964":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\noof_gnb, prediction_gnb, scores_gnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gnb)","08229d6b":"from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier()\n\nparameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n                 }\n\ngrid_search = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","2d50d9ce":"abc = AdaBoostClassifier(**grid_search.best_params_)\noof_abc, prediction_abc, scores_abc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=abc)","d8cb75c7":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier()\n\nparameter_grid = {'n_estimators': [10, 50, 100, 1000],\n                  'max_depth': [None, 3, 5, 15]\n                 }\n\ngrid_search = GridSearchCV(etc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\netc = ExtraTreesClassifier(**grid_search.best_params_)\noof_etc, prediction_etc, scores_etc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=etc)","780b413b":"from sklearn.gaussian_process import GaussianProcessClassifier\ngpc = GaussianProcessClassifier()\noof_gpc, prediction_gpc, scores_gpc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=gpc)","760b188f":"from sklearn.svm import SVC\nsvc = SVC(probability=True, gamma='scale')\n\nparameter_grid = {'C': [0.001, 0.01, 0.1, 1.0, 10.0],\n                  'kernel': ['linear', 'poly', 'rbf'],\n                 }\n\ngrid_search = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nsvc = SVC(probability=True, gamma='scale', **grid_search.best_params_)\noof_svc, prediction_svc, scores_svc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=svc)","e3cb498b":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\n\nparameter_grid = {'n_neighbors': [2, 3, 5, 10, 20],\n                  'weights': ['uniform', 'distance'],\n                  'leaf_size': [5, 10, 30]\n                 }\n\ngrid_search = GridSearchCV(knc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nknc = KNeighborsClassifier(**grid_search.best_params_)\noof_knc, prediction_knc, scores_knc = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=knc)","0ea8223b":"from sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()\n\nparameter_grid = {'alpha': [0.0001, 1, 2, 10]\n                 }\n\ngrid_search = GridSearchCV(bnb, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nbnb = BernoulliNB(**grid_search.best_params_)\noof_bnb, prediction_bnb, scores_bnb = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=bnb)","1d3174e6":"sgd = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\n\nparameter_grid = {'loss': ['log', 'modified_huber'],\n                  'penalty': ['l1', 'l2', 'elasticnet'],\n                  'alpha': [0.001, 0.01],\n                  'l1_ratio': [0, 0.15, 0.5, 1.0],\n                  'learning_rate': ['optimal', 'invscaling', 'adaptive']\n                 }\n\ngrid_search = GridSearchCV(sgd, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nsgd = linear_model.SGDClassifier(eta0=1, tol=0.0001, **grid_search.best_params_)\noof_sgd, prediction_sgd, scores_sgd = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=sgd)","6ddb6aa1":"plt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'LogisticRegression': scores})\nscores_df['GaussianNB'] = scores_gnb\nscores_df['AdaBoostClassifier'] = scores_abc\nscores_df['ExtraTreesClassifier'] = scores_etc\nscores_df['GaussianProcessClassifier'] = scores_gpc\nscores_df['SVC'] = scores_svc\nscores_df['KNeighborsClassifier'] = scores_knc\nscores_df['BernoulliNB'] = scores_bnb\nscores_df['SGDClassifier'] = scores_sgd\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);","08950729":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr, prediction_lr, _ = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","ff5a39a0":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = (prediction_lr + prediction_svc) \/ 2\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","854a0440":"plt.hist(prediction_lr, label='logreg');\nplt.hist(prediction_svc, label='svc');\nplt.hist((prediction_lr + prediction_svc) \/ 2, label='blend');\nplt.title('Distribution of out of fold predictions');\nplt.legend();","4db07654":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(2)\n\nX_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\n\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)","0efcd0e2":"cor = pd.DataFrame(X_train_poly).corrwith(y_train)","aa186d43":"sc = []\nfor i in range(10, 510, 10):\n    top_corr_cols = list(cor.abs().sort_values().tail(i).reset_index()['index'].values)\n    X_train_poly1 = X_train_poly[:, top_corr_cols]\n    X_test_poly1 = X_test_poly[:, top_corr_cols]\n    oof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)\n    sc.append(scores)","6ec7bc05":"plt.figure(figsize=(12, 8));\nplt.plot([np.mean(i) for i in sc]);\nplt.xticks(range(50), range(10, 510, 10), rotation=45);\nplt.title('Top poly features vs CV');","f8217bd5":"top_corr_cols = list(cor.abs().sort_values().tail(200).reset_index()['index'].values)\nX_train_poly1 = X_train_poly[:, top_corr_cols]\nX_test_poly1 = X_test_poly[:, top_corr_cols]","8936e7bd":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_poly, prediction_lr_poly, scores = train_model(X_train_poly1, X_test_poly1, y_train, params=None, model_type='sklearn', model=model)","d061ec45":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr_poly\nsubmission.to_csv('submission_poly.csv', index=False)\n\nsubmission.head()","3041436a":"X_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nX_train['300'] = X_train.std(1)\nX_test['300'] = X_test.std(1)\nscaler = StandardScaler()\nX_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\nX_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_1, prediction_lr_1, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model)","edd995ab":"X_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nX_train['300'] = X_train.std(1)\nX_test['300'] = X_test.std(1)\nscaler = StandardScaler()\nX_train[X_train.columns[:-1]] = scaler.fit_transform(X_train[X_train.columns[:-1]])\nX_test[X_train.columns[:-1]] = scaler.transform(X_test[X_train.columns[:-1]])\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_1, prediction_lr_1_repeated, scores = train_model(X_train.values, X_test.values, y_train, params=None, model_type='sklearn', model=model, folds=repeated_folds)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = prediction_lr_1_repeated\nsubmission.to_csv('repeated_nn_features.csv', index=False)\n\nsubmission.head()","275d1412":"X_train = train.drop(['id', 'target'], axis=1)\nX_test = test.drop(['id'], axis=1)\nmain_cols = X_train.columns.tolist()","73eb747c":"neigh = NearestNeighbors(3, n_jobs=-1)\nneigh.fit(X_train)\n\ndists, _ = neigh.kneighbors(X_train, n_neighbors=3)\nmean_dist = dists.mean(axis=1)\nmax_dist = dists.max(axis=1)\nmin_dist = dists.min(axis=1)\n\nX_train['300'] = X_train.std(1)\nX_train = np.hstack((X_train, mean_dist.reshape(-1, 1), max_dist.reshape(-1, 1), min_dist.reshape(-1, 1)))\n\ntest_dists, _ = neigh.kneighbors(X_test, n_neighbors=3)\n\ntest_mean_dist = test_dists.mean(axis=1)\ntest_max_dist = test_dists.max(axis=1)\ntest_min_dist = test_dists.min(axis=1)\n\nX_test['300'] = X_test.std(1)\nX_test = np.hstack((X_test, test_mean_dist.reshape(-1, 1), test_max_dist.reshape(-1, 1), test_min_dist.reshape(-1, 1)))","7cd2ad8b":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\noof_lr_2, prediction_lr_2, scores = train_model(X_train, X_test, y_train, params=None, model_type='sklearn', model=model)","9f876547":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = (prediction_lr_1 + prediction_lr_2) \/ 2\nsubmission.to_csv('blend.csv', index=False)\n\nsubmission.head()","e08d41e4":"# General information\n\nIn Don't Overfit! II competition we have a binary classification task. 300 columns, 250 training samples and 79 times more samples in test data! We need to be able to build a model without overfitting.\n\nIn this kernel I'll write the following things:\n\n* EDA on the features and trying to get some insights;\n* Using permutation importance to select most impactful features;\n* Comparing various models: bayer classification, linear models, tree based models;\n* Trying various approaches to feature selection including taking top features from eli5 and shap;\n* Hyperparameter optimization for models;\n* Feature generation;\n* Other things;\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*vuZxFMi5fODz2OEcpG-S1g.png)\n\n*Work still in progress*","b85f6f71":"## Modelling","37d1c95b":"Submitting `prediction_lr` gives 0.847 on leaderboard.","eeb8664e":"### Permutation importance\nThere is also another way of using eli5 - we could have a look at permutation importance. It works in the following way:\n* We fit a model;\n* We randomly shuffle one column of validation data and calculate the score;\n* If the score dropped significantly, it means that the feature is important;\n\nYou can read more about this approach here: https:\/\/www.kaggle.com\/dansbecker\/permutation-importance","e038b9c1":"We can see that There are several features with highly positive weights and more features with negative weights. In fact there are only 32 features, which are important according to ELI5. Let's try using only them for the submission!","931e7578":"## Basic modelling","38223474":"## SHAP\n\nAnother interesting tool is SHAP. It also provides explanations for a variety of models.","c0b67cc7":"Let's try generating some features!","69a4b0d8":"CV increased a bit!","ac26e60e":"Sadly blend gives 0.831 on LB. Again no luck.","e9246c07":"And this gives 0.811 on leaderboard. Overfitting! It seems that feature selection isn't the best approach. Let's try building various models!","291c5796":"## ELI5 and permutation importance\n\nELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm.","a0753d86":"So, parameters for logreg are optimal, let's try other models","9e989fa3":"The number of polynomial features is ~45k which is too much. We need some way to select some of them. Let's try use correlations with target.","6b4258c7":"Wow, we got improvement from 0.7226 to 0.7486 on CV! But this submission gives 0.845 on leaderboard. So it decreases score slightly. Let's try other things!","1cde4d94":"We can see that logistic regression is superior to most other models. Only SVC is comparable. It seems that other models either overfit or can't work on this small dataset.\n\n\nLet's try submitting a blend of them!","41fbfb8e":"From this overview we can see the following things:\n* target is binary and has some disbalance: 36% of samples belong to 0 class;\n* values in columns are more or less similar;\n* columns have std of 1 +\/- 0.1 (min and max values are 0.889, 1.117 respectively);\n* columns have mean of 0 +\/- 0.15 (min and max values are -0.2, 0.1896 respectively);","8fd8c23d":"It could be difficult to interpret this plot when you see it for the first time. It shows how features impact predictions. For example for feature 33 low values have a negative impact on model predictions (zero is more likely), and high values have a positive impace (ones are more likely). Feature 217 has an opposite effect: low values have a positive impact and high values have a negative impact.\n\nBut we will need to select features manually... let's use a library for that!","64b87b7f":"### Adding statistics","46cc8d0b":"Wow, if we select columns by permutation importance, CV score drops significantly. It seems it doesn't work well in out case.","3756d092":"Let's have a look at correlations now!","e3dbc6a7":"### Polynomial Features","afb99020":"### Adding distance features","f39af477":"## Data exploration","d19a1ff5":"### Mlextend SequentialFeatureSelector","b66b78e3":"We can see that correlations between features are lower that 0.3 and the most correlated feature with target has correlation of 0.37. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target.","acff8a94":"Score became much lower. So this is also a bad idea","772253fd":"Not suprisingly we overfit."}}