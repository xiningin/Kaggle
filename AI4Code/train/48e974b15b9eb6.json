{"cell_type":{"cae06b8e":"code","94f7faa8":"code","b1c8922c":"code","28b0da47":"code","8ba62bec":"code","a218db51":"code","8348e7fd":"code","b5b9d305":"code","49d59058":"code","2e8304f9":"code","fd59939a":"code","9363a541":"code","5350a942":"code","0c049b27":"code","79d01dc1":"code","849d9fb3":"code","d613347b":"code","9276ac2f":"code","203f7977":"code","6eb4d172":"code","989d0657":"code","6b73580a":"code","55570650":"code","949a57d4":"code","b53ad709":"code","be99dd49":"code","03976e37":"code","b02714d6":"code","1e874981":"code","e75ccf48":"code","b4c75367":"code","23f08cb3":"code","9ea27fc8":"code","f360f220":"code","54911963":"code","cbc93928":"code","ca896be6":"code","fc99d368":"code","cf95bcbc":"code","d1010203":"code","8706aa57":"code","661887f6":"code","2afe5272":"code","87116298":"code","708fc91b":"code","20c4f46f":"code","7054babe":"code","a210f936":"code","d144f7d5":"code","c351be38":"code","7a2609b9":"code","e8c5d7f1":"code","56496716":"code","d9f9623b":"code","d5761f9c":"code","4a06c7e1":"code","0fe68f98":"code","e1bdfad6":"markdown","07596826":"markdown","4a1c4653":"markdown","104d9e82":"markdown","96296f30":"markdown","b61105d8":"markdown","da4dc4c9":"markdown","13056698":"markdown","a2761b71":"markdown","e6d528c0":"markdown","2323bd26":"markdown","28ad827e":"markdown","1ed035eb":"markdown","3a6434ec":"markdown","f5733c1a":"markdown","c5a1627d":"markdown","7f10c7e9":"markdown","dfcf8a3c":"markdown","464a0565":"markdown","9a084e95":"markdown","7965ac30":"markdown","0116ca60":"markdown","d0c966c2":"markdown","58f0ec9b":"markdown","da4bc6b4":"markdown","ebd3d0f9":"markdown","3ad9ca83":"markdown","d65c8fdf":"markdown","fed9a82b":"markdown"},"source":{"cae06b8e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport PIL\nfrom PIL import Image\n\nimport cv2\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision import transforms\n\nimport torch.utils.data\nfrom torch.utils.data import Dataset, DataLoader\nimport random","94f7faa8":"os.listdir(\"\/kaggle\/input\/global-wheat-detection\")\n\ntrain_dir = \"\/kaggle\/input\/global-wheat-detection\/train\"\ntest_dir = \"\/kaggle\/input\/global-wheat-detection\/test\"\n\ndf_train=pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/train.csv\")","b1c8922c":"print(df_train.head())\nprint(df_train.shape)","28b0da47":"print(len(df_train['image_id'].unique()))\nprint((df_train.shape[0])\/len(df_train['image_id'].unique()))","8ba62bec":"print(\"Image_id v\/s # of bounding boxes\")\nprint(df_train['image_id'].value_counts())","a218db51":"print(\"Height\")\nprint(df_train['height'].value_counts())\nprint(\"Width\")\ndf_train['width'].value_counts()","8348e7fd":"list_image_ids_df = list(df_train['image_id'].unique())\nlist_image_ids_dir = os.listdir(\"\/kaggle\/input\/global-wheat-detection\/train\")\n#list_image_ids_df.sort() == list_image_ids_dir.sort()\n#sorted(list_image_ids_df,key = lambda x: x,reverse=False)","b5b9d305":"len(list_image_ids_dir)- len(list_image_ids_df)","49d59058":"#There is a difference of 49 images i.e. there are 49 images in training directory for which there are no bounding boxes indicating no wheatheads detected","2e8304f9":"for col in df_train.columns:\n    if sum(df_train[col].isnull())==1:\n        print(col+\" has null values\")\n    else:\n        print(col+\" no null values\")","fd59939a":"df_train['x0'] = df_train['bbox'].map(lambda x: x[1:-1].split(\",\")[0]).astype(float)\ndf_train['y0'] = df_train['bbox'].map(lambda x: x[1:-1].split(\",\")[1]).astype(float)\ndf_train['w'] = df_train['bbox'].map(lambda x: x[1:-1].split(\",\")[2]).astype(float)\ndf_train['h'] = df_train['bbox'].map(lambda x: x[1:-1].split(\",\")[3]).astype(float)\ndf_train['x1'] = df_train['x0'] + df_train['w']\ndf_train['y1'] = df_train['y0'] + df_train['h']","9363a541":"df_train.head()","5350a942":"for col in df_train.columns:\n    if sum(df_train[col].isnull())==1:\n        print(col+\" has null values\")\n    else:\n        print(col+\" no null values\")","0c049b27":"df_train.dtypes","79d01dc1":"cols_to_be_selected = ['image_id','x0','y0','x1','y1']\ndf1_train = df_train[cols_to_be_selected]","849d9fb3":"val_percentage = 0.2\nnum_val_images = int(len(df1_train['image_id'].unique())*val_percentage)\nnum_train_images = len(df1_train['image_id'].unique()) - num_val_images\nlist_val_imageid = list(df1_train['image_id'].unique())[(-1)*num_val_images:]\nlist_train_imageid = list(df1_train['image_id'].unique())[:num_train_images]","d613347b":"print(\"Number of validation images: \",num_val_images)\nprint(\"Number of training images: \",num_train_images)\nprint(num_val_images + num_train_images)","9276ac2f":"df2_val = df1_train.loc[df1_train['image_id'].isin(list_val_imageid),:]\ndf2_train = df1_train.loc[df1_train['image_id'].isin(list_train_imageid),:]","203f7977":"def get_transform():\n    list_transforms = []\n    # converts the input image, which should be a PIL image, into a PyTorch Tensor\n    list_transforms.append(transforms.ToTensor())\n    \n    #keeping space for augmentations in future\n    \n    return transforms.Compose(list_transforms)","6eb4d172":"class GlobalWheatDetectionDataset(torch.utils.data.Dataset):\n    # first lets start with __init__ and initialize any objects\n    def __init__(self,input_df,input_dir,transforms=None):\n        \n        self.df=input_df\n        \n        self.list_images = list(self.df['image_id'].unique())\n        \n        self.image_dir=input_dir\n        \n        self.transforms = transforms\n    \n    # next lets define __getitem__\n    # very important to note what it returns for EACH image:\n    # I. image - a PIL image of size (H,W) for ResNet50 FPN image should be scaled\n    # II. target - a dictionary containing the following fields\n    # A. boxes as FloatTensor of dimensions - N,4 where N = # of bounding boxs within an image \n    # and 4 columns include [x0,y0,x1,y1]\n    # B. labels as Int64Tensor of dimension - N\n    # C. area as Int64Tensor of dimension - N\n    # D. iscrowd as UInt8Tensor of dimension - N\n    # III. image_id \n    \n    def __getitem__(self,idx):\n        \n        # II. target\n        # Preparation for (A) boxes\n        # FloatTensor of dimensions - N,4 where N = # of bounding boxs within an image and 4 columns include [x0,y0,x1,y1]\n        \n        cols_to_be_selected =['x0','y0','x1','y1']\n        img_id = self.list_images[idx]\n        bboxes_array = np.array(self.df.loc[self.df['image_id']==img_id,cols_to_be_selected])\n        boxes = torch.tensor(bboxes_array, dtype=torch.float32)\n        \n        # Preparation for (B) labels\n        # Int64Tensor of dimension - N\n        num_boxes = self.df.loc[self.df['image_id']==img_id].shape[0]\n        labels = torch.ones(num_boxes, dtype=torch.int64)\n        \n        # Preparation for (C) area\n        # dimension - N, int64tensor\n        area = torch.tensor(np.array((self.df['x1']-self.df['x0'])*(self.df['y1']-self.df['y0'])), dtype=torch.int64)\n        \n        # Preparation for (D) iscrowd\n        # dimension - N, Uint8tensor\n        iscrowd = torch.zeros(num_boxes, dtype=torch.uint8)\n        \n        # Combining everything\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        # I. Input image\n        # Specifications: A.RGB format B. scaled (0,1) C. size (H,W) D. PIL format\n        \n        img = cv2.imread(self.image_dir+\"\/\"+img_id+\".jpg\")\n        img_RGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img_scaled = img_RGB\/255.0\n        img_final = img_scaled\n        \n        if self.transforms is not None:\n            img_final = self.transforms(img_final)\n        \n        # III. image_id\n        \n        \n        return img_final, target, img_id\n    \n    # next lets define __len__    \n    def __len__(self):\n        \n        return len(self.df['image_id'].unique())","989d0657":"train_dataset = GlobalWheatDetectionDataset(df2_train,train_dir,get_transform())\nval_dataset = GlobalWheatDetectionDataset(df2_val,train_dir,get_transform())","6b73580a":"def collate_fn(batch):\n    return tuple(zip(*batch))","55570650":"train_dataloader = DataLoader(train_dataset, batch_size=16,shuffle=False, num_workers=4,collate_fn=collate_fn)\n#val_dataloader = DataLoader(val_dataset, batch_size=8,shuffle=False, num_workers=4,collate_fn=collate_fn)","949a57d4":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","b53ad709":"#Showing a sample\nimage,targets,image_id = train_dataset[0]\n# Converting the A.)images to cuda or device \nimage = image.to(device)\n# To see why this is needed especially in a GPU environment, try running [print(img.device) for img in images]\n\n# Converting the B.)images to cuda or device","be99dd49":"boxes = targets['boxes'].cpu().numpy().astype(np.int32)\nsample = image.permute(1,2,0).cpu().numpy().astype(np.float32)\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (255, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","03976e37":"# load a model; pre-trained on COCO\n#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","b02714d6":"def get_instance_objectdetection_model(num_classes,path_weight):\n    # load an instance segmentation model pre-trained on COCO\n    create_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=False)\n\n    # get the number of input features for the classifier\n    in_features = create_model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    create_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    create_model.load_state_dict(torch.load(path_weight,map_location=torch.device('cpu')))\n\n    return create_model","1e874981":"path_weight= \"\/kaggle\/input\/fasterrcnn\/fasterrcnn_resnet50_fpn_best.pth\"","e75ccf48":"num_classes = 2\n# Why 2 classes - background and wheat-heads\nmodel = get_instance_objectdetection_model(num_classes,path_weight)","b4c75367":"torch.cuda.empty_cache()","23f08cb3":"# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","9ea27fc8":"mode=\"validation\"","f360f220":"if mode==\"training\":\n    model.train()\n    model.to(device)\n\n    num_epochs = 5\n\n    itr = 1\n\n    for epoch in range(num_epochs):\n        #loss_hist.reset()\n        loss_sum = 0\n        num_iterations = 0\n        for images, targets, image_ids in train_dataloader:\n\n            images = list(image.to(device,dtype=torch.float) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)   ##Return the loss\n\n            losses = sum(loss for loss in loss_dict.values())\n            loss_value = losses.item()\n\n            #loss_hist.send(loss_value)  #Average out the loss\n            loss_sum = loss_sum + loss_value\n            num_iterations = num_iterations + 1\n\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            if itr % 5 == 0:\n                print(f\"Iteration #{itr} loss: {loss_value}\")\n\n            itr += 1\n\n        # update the learning rate\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        if num_iterations>0:\n            loss_avg_value = loss_sum\/num_iterations\n\n        print(\"Epoch\"+ \"#\"+str(epoch)+\" loss: \"+str(loss_avg_value))","54911963":"os.listdir(\"\/kaggle\/input\/gwd-customtrained-fasterrcnn-resnet-50-fpn-01\")","cbc93928":"#torch.save(model.state_dict(), '\/kaggle\/working\/customtrained_fasterrcnn_resnet50_fpn.pth')","ca896be6":"path_trained_weight = \"\/kaggle\/input\/gwd-customtrained-fasterrcnn-resnet-50-fpn-01\/customtrained_fasterrcnn_resnet50_fpn.pth\"\nnum_classes=2\ntrained_model = get_instance_objectdetection_model(num_classes,path_trained_weight)","fc99d368":"# os.chdir(\"\/kaggle\/working\/\")","cf95bcbc":"# from IPython.display import FileLink\n# FileLink(r'customtrained_fasterrcnn_resnet50_fpn.pth')","d1010203":"torch.cuda.empty_cache()","8706aa57":"val_dataloader = DataLoader(val_dataset, batch_size=8,shuffle=False, num_workers=2,collate_fn=collate_fn)","661887f6":"trained_model.eval()\ntrained_model.to(device)\n\n\nimages, targets, image_ids = next(iter(val_dataloader))\n\nimages = list(image.to(device,dtype=torch.float) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[2].permute(1,2,0).cpu().numpy()\n\nfig,ax = plt.subplots(1,1,figsize=(16,8))\n\nfor box in boxes:\n    cv2.rectangle(sample, (box[0],box[1]),(box[2],box[3]),(255,0,0),3)\n    ax.set_axis_off()\n    ax.imshow(sample)","2afe5272":"class GlobalWheatDetectionTestDataset(torch.utils.data.Dataset):\n    # first lets start with __init__ and initialize any objects\n    def __init__(self,input_df,input_dir,transforms=None):\n        \n        self.df=input_df\n        \n        self.list_images = list(self.df['image_id'].unique())\n        \n        self.image_dir=input_dir\n        \n        self.transforms = transforms\n    \n    # next lets define __getitem__\n    # very important to note what it returns for EACH image:\n    # I. image - a PIL image of size (H,W) for ResNet50 FPN image should be scaled\n    \n    # II. image_id \n    \n    def __getitem__(self,idx):\n        \n        # II. image_id\n        img_id = self.list_images[idx]\n        # I. Input image\n        # Specifications: A.RGB format B. scaled (0,1) C. size (H,W) D. PIL format\n        \n        img = cv2.imread(self.image_dir+\"\/\"+img_id+\".jpg\")\n        img_RGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img_scaled = img_RGB\/255.0\n        img_final = img_scaled\n        \n        if self.transforms is not None:\n            img_final = self.transforms(img_final)\n        \n        \n        \n        \n        return img_final, img_id\n    \n    # next lets define __len__    \n    def __len__(self):\n        \n        return len(self.df['image_id'].unique())","87116298":"df_test=pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/sample_submission.csv\")","708fc91b":"df_test.head()","20c4f46f":"# df_test['x0'] = df_test['bbox'].map(lambda x: x[1:-1].split(\",\")[0]).astype(float)\n# df_test['y0'] = df_test['bbox'].map(lambda x: x[1:-1].split(\",\")[1]).astype(float)\n# df_test['w'] = df_test['bbox'].map(lambda x: x[1:-1].split(\",\")[2]).astype(float)\n# df_test['h'] = df_test['bbox'].map(lambda x: x[1:-1].split(\",\")[3]).astype(float)\n# df_test['x1'] = df_test['x0'] + df_test['w']\n# df_test['y1'] = df_test['y0'] + df_test['h']","7054babe":"test_dataset = GlobalWheatDetectionTestDataset(df_test,test_dir,get_transform())","a210f936":"test_dataloader = DataLoader(test_dataset, batch_size=8,shuffle=False, num_workers=2,collate_fn=collate_fn)","d144f7d5":"detection_threshold = 0.45","c351be38":"def format_prediction_string(boxes, scores): ## Define the formate for storing prediction results\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","7a2609b9":"device = torch.device('cuda')","e8c5d7f1":"## Lets make the prediction\nresults=[]\ntrained_model.eval()\nimages = []\noutputs =[]\nfor images_, image_ids in test_dataloader:    \n\n    images = list(image.to(device,dtype=torch.float) for image in images_)\n    outputs = trained_model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()    ##Formate of the output's box is [Xmin,Ymin,Xmax,Ymax]\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32) #Compare the score of output with the threshold and\n        scores = scores[scores >= detection_threshold]                    #slelect only those boxes whose score is greater\n                                                                          # than threshold value\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]         #Convert the box formate to [Xmin,Ymin,W,H]\n        \n        \n            \n        result = {                                     #Store the image id and boxes and scores in result dict.\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        \n        results.append(result)              #Append the result dict to Results list\n\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","56496716":"#os.chdir(\"\/kaggle\/working\")","d9f9623b":"test_df.to_csv('submission.csv', index=False)","d5761f9c":"# from IPython.display import FileLink\n# FileLink(r'submission_1.csv')","4a06c7e1":"sample = images[1].permute(1,2,0).cpu().numpy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\n\nboxes = boxes[scores >= detection_threshold].astype(np.int32)","0fe68f98":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","e1bdfad6":"## 7. Inference","07596826":"With this we have the dataframes in place. Now lets start preparing the data which we need for FastRCNN Pytorch model\nFor this lets refer to the documentation:\nhttps:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html","4a1c4653":"Just to be sure, lets check for any null values in dataframe","104d9e82":"Validation","96296f30":"Lets now define the dataset","b61105d8":"Below notebooks helped in a great manner to construct the below code:\n\n1. https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train by Peter\n2. https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-inference by Peter\n3. https:\/\/www.kaggle.com\/aryaprince\/getting-started-with-object-detection-with-pytorch by @PA\n4. https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html PyTorch tutorial\n5. https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html ","da4dc4c9":"## References","13056698":"Now lets create corresponding dataframes","a2761b71":"Thus we have 3373 images. And we are looking at an average of approximately 44 bounding boxes in the image\n=> Thus approximately 44 wheat heads are present in an image\n\nLets also have a look at the distribution in terms of max & min","e6d528c0":"## 5. Model Building","2323bd26":"Taking the below excerpt from the model finetuning pytorch tutorial available","28ad827e":"![image.png](attachment:image.png)","1ed035eb":"Checking  image sizes","3a6434ec":"Thats everthing for the dataset","f5733c1a":"All images have height,width = 1024,1024","c5a1627d":"# Outline\n\n**0. References **\n\n**1. Pipeline **\n\n**2. Importing necessary packages**\n\n**3. Basic EDA **\n\n**4. Data Preparation **\n\n**5. Model **\n\n**6. Training**\n\n**7. Inference**","7f10c7e9":"Now, that the different sets of checks are done lets start with data preparation","dfcf8a3c":"Lets also check whether image_ids in df_train csv and list of images in training folder are same","464a0565":"## 3. Basic EDA","9a084e95":"## 4. Data Preparation","7965ac30":"So we are looking at 147793 entries in the training excel -> corresponding to the total number of bounding boxes overall\nThis may\/may not be equal to the number of images -- lets check this\nIt will be equal only if each image has one and only one bounding box","0116ca60":"![image.png](attachment:image.png)","d0c966c2":"\nThe [torchvision reference scripts for training object detection, instance segmentation and person keypoint detection](https:\/\/github.com\/pytorch\/vision\/tree\/v0.3.0\/references\/detection) allows for easily supporting adding new custom datasets.\nThe dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n\nThe only specificity that we require is that the dataset `__getitem__` should return:\n\n* image: a PIL Image of size (H, W)\n* target: a dict containing the following fields\n    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n    * (optionally) `masks` (`UInt8Tensor[N, H, W]`): The segmentation masks for each one of the objects\n    * (optionally) `keypoints` (`FloatTensor[N, K, 3]`): For each one of the `N` objects, it contains the `K` keypoints in `[x, y, visibility]` format, defining the object. `visibility=0` means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt `references\/detection\/transforms.py` for your new keypoint representation\n\nIf your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n\nAdditionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.","58f0ec9b":"With this in mind lets build the class with three methods in place:\n1. __init__\n2. __getitem__\n3. __len__","da4bc6b4":"## 6. Training","ebd3d0f9":"## 1. Pipeline","3ad9ca83":"We know the structure df_train from the given data description:\n1. image_id - the unique image ID\n2. width, height - the width and height of the images\n3. bbox - a bounding box, formatted as a Python-style list of [xmin, ymin, width, height]\n\nbbox is something of high interest to us. We need to structure bbox in a way that can be used further","d65c8fdf":"Problem statement: to detect wheatheads present in images of wheats","fed9a82b":"## 2. Importing Packages"}}