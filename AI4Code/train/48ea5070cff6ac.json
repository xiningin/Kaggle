{"cell_type":{"ead021e5":"code","d19b42d7":"code","2c200757":"code","06fb89dd":"code","d92b8dc6":"code","5a7a0f8f":"code","36ef11b9":"code","76cc1f52":"code","0e1ec7cf":"code","ce1eabcf":"code","38aefaf5":"code","7a6cddb6":"code","8ca990be":"code","1039b60a":"code","0c852294":"code","f38ff2db":"code","5db51d00":"code","6e7b61c4":"code","93a8cdd6":"code","fcbc484d":"code","1dbc0ecc":"code","008ee6cf":"code","bfb4d600":"code","3acfd870":"code","deeecbae":"code","c32cd8d8":"code","a282d04b":"code","7d76beb0":"code","4d15fe03":"code","528bcc02":"code","e3188b52":"code","16047ab1":"code","b60b5c95":"code","b2a5c71d":"code","938b8c48":"code","41b114b2":"code","9e28ee93":"code","59694813":"code","6a3cfd82":"code","0f4c0386":"code","d20e3949":"code","82e11134":"code","d43c000f":"code","7a9a5878":"code","c9f0b5ca":"code","52e5a1c2":"code","d234b1ed":"code","33565b15":"code","684f1a8c":"code","c24a1cbd":"code","b6877977":"code","c5b20d1b":"code","7f88aea4":"code","8d1dae55":"code","fc520dd6":"code","a7c8be8a":"code","c3709606":"code","517ad5b2":"code","a211afba":"code","fab74302":"code","60d66410":"code","1aec738f":"code","37def530":"code","c9a2cca1":"code","0373dcb4":"markdown","0b54dedd":"markdown","88ca2af9":"markdown","640de6fb":"markdown","f70505b8":"markdown","2b19896a":"markdown","a5a10c4e":"markdown","3fd30251":"markdown","ad368293":"markdown","81fb91e7":"markdown"},"source":{"ead021e5":"#pip install afinn","d19b42d7":"#pip install vaderSentiment","2c200757":"#pip install shap","06fb89dd":"#nltk packages\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n#Pandas\nimport pandas as pd\nimport numpy as np\n#enable display of all columns in notebook\npd.options.display.max_columns = 999 \nnp.random.seed(12345)\n\n#re\nimport re\n\n#sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import r2_score\n\n#string\nimport string\n\n\n#sentiment packages\nfrom textblob import TextBlob\n\n#Used in creating ngrams\nimport math\n\n#xgboost\nimport shap\nimport xgboost as xgb\n\n#plt\nimport matplotlib.pyplot as plt\n#enables display of plots in notebook\n%matplotlib inline \n\n#stats\nfrom scipy import stats","d92b8dc6":"#from google.colab import drive\n#drive.mount(\"\/content\/gdrive\")","5a7a0f8f":" ##%cd \/content\/gdrive\/My Drive\/ML","36ef11b9":"##train = pd.read_csv(\"train.csv\")\n##test = pd.read_csv(\"test.csv\")\n##sample_submission = pd.read_csv(\"sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","76cc1f52":"train[train['text'].isna()]","0e1ec7cf":"train.drop(314, inplace = True)\ntrain = train.reset_index(drop = True)","ce1eabcf":"train.info()\ntrain.head()","38aefaf5":"#Prepare column names for xgboost\ntrain.columns = train.columns.str.strip()\ntest.columns = test.columns.str.strip()","7a6cddb6":"#Define a function to calculate jaccard score\ndef jaccard(str1,str2):\n    a=str1.lower().split(\" \")\n    b=str2.lower().split(\" \")\n    c=set(a)&set(b)\n    prop=len(c)\/(len(a)+len(b)-len(c))\n    return(prop)","8ca990be":"#To save time, remove neutral from the train set.\n# pos_train = train[train['sentiment'] == 'positive']\n# neg_train = train[train['sentiment'] == 'negative']\nneutral_train = train[train['sentiment'] == 'neutral']\ntrain = train[train['sentiment'] != 'neutral']\nneutral_test = test[test['sentiment'] == 'neutral']\ntest = test[test['sentiment'] != 'neutral']","1039b60a":"train.head()","0c852294":"test.head()","f38ff2db":"# Create jaccard score for texts in train set with neutral sentiment\nneutral_train.apply(lambda x: jaccard(x['text'], x['selected_text']), axis = 1).hist()","5db51d00":"def remove_url(text):\n    url_pattern = re.compile(\"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", flags=re.UNICODE)\n    return url_pattern.sub(r'', text)","6e7b61c4":"train['text']=train['text'].apply(lambda x:remove_url(x))\ntest['text']=test['text'].apply(lambda x:remove_url(x))","93a8cdd6":"#Check the distribution of proportion of selected_text versus text_n_words in train set\ntrain['text_n_words'] = train['text'].apply(str).apply(lambda x: len(x.split(\" \")))\ntrain['sel_text_n_words'] = train['selected_text'].apply(str).apply(lambda x: len(x.split(\" \")))\ntrain['prop_sel_text_len'] = train['sel_text_n_words']\/train['text_n_words']\ntrain['prop_sel_text_len'].hist()","fcbc484d":"#Number of string in text\ndef find_str(text):\n  result = re.split('[!?,.]',text)\n  count = len(result)\n  return count\n\n#List of prepositions\nprep = ['about', 'below', 'excepting', 'off', 'toward', 'above', 'beneath', 'on', 'under', 'across', 'from','onto',\n'underneath', 'after','between', 'in', 'out', 'until', 'against', 'beyond' , 'outside', 'up' , 'along', 'but', 'inside','over',\n'upon', 'among','by','past', 'around', 'concerning', 'regarding', 'with', 'at', 'despite','into', 'since', 'within',\n'down', 'like', 'through','without', 'before', 'during', 'near', 'throughout', 'behind', 'except', 'of', 'to', 'for']\n\ndef preposition(sentence):\n  words = sentence.split()\n  prep_num = 0\n  for x in prep:\n      prep_num += words.count(x)\n  return prep_num","1dbc0ecc":"#Create features for train set before running ngrams to save time\ntrain['text_sent_blob'] = train['text'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)#Blog score\n##af = Afinn()\n##train['text_sent_afinn'] = train['text'].apply(str).apply(lambda x: af.score(x.lower()))#Afinn score\n##analyser = SentimentIntensityAnalyzer()\n##train['text_sent_varder'] = train['text'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])#Varder score\ntrain['text_n_str'] = train['text'].apply(str).apply(lambda x: find_str(x.lower()))#Number of string in text\ntrain['text_n_uq_words'] = train['text'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))#Number of unique words\ntrain['text_n_uq_chars'] = train['text'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))#Number of unique characters\ntrain['text_n_prepositions'] = train['text'].apply(str).apply(lambda x: preposition(x.lower()))#Number of prepositions","008ee6cf":"train.head()","bfb4d600":"#Create features for test set\ntest['text_n_words'] = test['text'].apply(str).apply(lambda x: len(x.split(\" \")))\ntest['text_sent_blob'] = test['text'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)#Blog score\n##af = Afinn()\n##test['text_sent_afinn'] = test['text'].apply(str).apply(lambda x: af.score(x.lower()))#Afinn score\n##analyser = SentimentIntensityAnalyzer()\n##test['text_sent_varder'] = test['text'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])#Varder score\ntest['text_n_str'] = test['text'].apply(str).apply(lambda x: find_str(x.lower()))#Number of string in text\ntest['text_n_uq_words'] = test['text'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))#Number of unique words\ntest['text_n_uq_chars'] = test['text'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))#Number of unique characters\ntest['text_n_prepositions'] = test['text'].apply(str).apply(lambda x: preposition(x.lower()))#Number of prepositions","3acfd870":"test.head()","deeecbae":"#Create ngrams for a line\ndef create_ngrams(line):\n  words = line['text'].split()\n  # subsets = [words[i:j+1] for i in range(len(words)) for j in range(i,len(words))] #Create subset for whole train set\n  subsets = [words[i:j+1] for i in range(len(words)) for j in range(i,int(math.ceil(len(words)\/2)))] #Create subset for only the rows with subsetsLen(ngrams)\/Len(original text) <= 0.5\n  return subsets","c32cd8d8":"%%time\n#\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01\uff01It takes 30 mins to run...\n#Create ngrams subsets for train set\ntrain_subsets = pd.DataFrame()\ntrain_temp = pd.DataFrame()\nfor i in range(len(train)):\n  ngrams_lines = create_ngrams(train.iloc[i])\n  train_temp = pd.DataFrame([train.iloc[i]]*(len(ngrams_lines)))#Create the new lines and \n  train_temp['ngram'] = list(map(lambda x: \" \".join(words for words in x),ngrams_lines))#Combine the new lines with their ngrams\n  train_subsets = train_subsets.append(train_temp,ignore_index=True)#Append new lines with ngrams to train_subsets","a282d04b":"train_temp = train\ntrain_temp['ngram'] = train_temp['text']\ntrain_subsets = train_subsets.append(train_temp,ignore_index=True) #Append original train set to get subsetsLen(ngrams)\/Len(original text) = 1","7d76beb0":"#Update the two columns created before\ntrain_subsets['sel_text_n_words'] = train_subsets['ngram'].apply(str).apply(lambda x: len(x.split(\" \")))\ntrain_subsets['prop_sel_text_len'] = train_subsets['sel_text_n_words']\/train_subsets['text_n_words']","4d15fe03":"len(train_subsets)","528bcc02":"train_subsets.head()","e3188b52":"train_subsets.tail()","16047ab1":"%%time\ntest_subsets = pd.DataFrame()\ntest_temp = pd.DataFrame()\nfor i in range(len(test)):\n  ngrams_lines = create_ngrams(test.iloc[i])\n  test_temp = pd.DataFrame([test.iloc[i]]*(len(ngrams_lines)))#Create the new lines and \n  test_temp['ngram'] = list(map(lambda x: \" \".join(words for words in x),ngrams_lines))#Combine the new lines with their ngrams\n  test_subsets = test_subsets.append(test_temp,ignore_index=True)#Append new lines with ngrams to test_subsets","b60b5c95":"test_temp = test\ntest_temp['ngram'] = test_temp['text']\ntest_subsets = test_subsets.append(test_temp,ignore_index=True) #Append original test set to get subsetsLen(ngrams)\/Len(original text) = 1","b2a5c71d":"#Update the two columns created before\ntest_subsets['sel_text_n_words'] = test_subsets['ngram'].apply(str).apply(lambda x: len(x.split(\" \")))\ntest_subsets['prop_sel_text_len'] = test_subsets['sel_text_n_words']\/test_subsets['text_n_words']","938b8c48":"len(test_subsets)","41b114b2":"test_subsets.head()","9e28ee93":"test_subsets.tail()","59694813":"%%time\n#It takes 6 mins\n\n#Jaccard score\ntrain_subsets['jaccard'] = train_subsets.apply(lambda x: jaccard(x['ngram'], x['selected_text']), axis = 1) # Create jaccard score for each row\n#Blob score\ntrain_subsets['sel_text_sent_blob'] = train_subsets['ngram'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)\ntrain_subsets['dif_text_sent_blob'] = train_subsets['text_sent_blob'] - train_subsets['sel_text_sent_blob']\n#Afinn score\n##train_subsets['sel_text_sent_afinn'] = train_subsets['ngram'].apply(str).apply(lambda x: af.score(x.lower()))\n##train_subsets['dif_text_sent_afinn'] = train_subsets['text_sent_afinn'] - train_subsets['sel_text_sent_afinn']\n#Varder score\n##train_subsets['sel_text_sent_varder'] = train_subsets['ngram'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])\n##train_subsets['dif_text_sent_varder'] = train_subsets['text_sent_varder'] - train_subsets['sel_text_sent_varder']\n#Proportion of number of string of ngrams\ntrain_subsets['sel_text_n_str'] = train_subsets['ngram'].apply(str).apply(lambda x: find_str(x.lower()))\ntrain_subsets['prop_sel_text_n_str'] = train_subsets['sel_text_n_str'] \/ train_subsets['text_n_str']\n#Number of unique words of ngrams\ntrain_subsets['sel_text_n_uq_words'] = train_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))\ntrain_subsets['prop_sel_text_n_uq_words'] =  train_subsets['sel_text_n_uq_words']\/train_subsets['text_n_uq_words']\n#Number of unique characters of ngrams\ntrain_subsets['sel_text_n_uq_chars'] = train_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))\ntrain_subsets['prop_sel_text_n_uq_chars'] = train_subsets['sel_text_n_uq_chars']\/train_subsets['text_n_uq_chars'] \n#Number of prepositions\ntrain_subsets['sel_text_n_prepositions'] = train_subsets['ngram'].apply(str).apply(lambda x: preposition(x.lower()))\ntrain_subsets['prop_sel_text_n_prepositions'] = train_subsets['sel_text_n_prepositions']\/train_subsets['text_n_prepositions']","6a3cfd82":"train_subsets.head()","0f4c0386":"%%time\n#It takes 1 mins\n\n#Blob score\ntest_subsets['sel_text_sent_blob'] = test_subsets['ngram'].apply(str).apply(lambda x: TextBlob(x.lower()).sentiment.polarity)\ntest_subsets['dif_text_sent_blob'] = test_subsets['text_sent_blob'] - test_subsets['sel_text_sent_blob']\n#Afinn score\n##test_subsets['sel_text_sent_afinn'] = test_subsets['ngram'].apply(str).apply(lambda x: af.score(x.lower()))\n##test_subsets['dif_text_sent_afinn'] = test_subsets['text_sent_afinn'] - test_subsets['sel_text_sent_afinn']\n#Varder score\n##test_subsets['sel_text_sent_varder'] = test_subsets['ngram'].apply(str).apply(lambda x: analyser.polarity_scores(x.lower())[\"compound\"])\n##test_subsets['dif_text_sent_varder'] = test_subsets['text_sent_varder'] - test_subsets['sel_text_sent_varder']\n#Proportion of number of string of ngrams\ntest_subsets['sel_text_n_str'] = test_subsets['ngram'].apply(str).apply(lambda x: find_str(x.lower()))\ntest_subsets['prop_sel_text_n_str'] = test_subsets['sel_text_n_str'] \/ test_subsets['text_n_str']\n#Number of unique words of ngrams\ntest_subsets['sel_text_n_uq_words'] = test_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(x.strip().split())))\ntest_subsets['prop_sel_text_n_uq_words'] = test_subsets['sel_text_n_uq_words']\/test_subsets['text_n_uq_words'] \n#Number of unique characters of ngrams\ntest_subsets['sel_text_n_uq_chars'] = test_subsets['ngram'].apply(str).apply(lambda x: len(np.unique(list(x.replace(\" \", \"\")))))\ntest_subsets['prop_sel_text_n_uq_chars'] = test_subsets['sel_text_n_uq_chars']\/test_subsets['text_n_uq_chars']\n#Number of prepositions\ntest_subsets['sel_text_n_prepositions'] = test_subsets['ngram'].apply(str).apply(lambda x: preposition(x.lower()))\ntest_subsets['prop_sel_text_n_prepositions'] = test_subsets['sel_text_n_prepositions']\/test_subsets['text_n_prepositions']","d20e3949":"test_subsets.head()","82e11134":"train_subsets.to_csv('train_subsets.csv',index = False)\ntest_subsets.to_csv('test_subsets.csv',index = False)","d43c000f":"# %cd C:\\Users\\77548\\Desktop\\tweet-sentiment-extraction\n# train_subsets = pd.read_csv(\"train_subsets_without_url.csv\")\n# test_subsets = pd.read_csv(\"test_subsets_without_url.csv\")","7a9a5878":"train_subsets_pos =  train_subsets[train_subsets['sentiment'] == 'positive']\ntest_subsets_pos =  test_subsets[test_subsets['sentiment'] == 'positive']\ntrain_subsets_neg =  train_subsets[train_subsets['sentiment'] == 'negative']\ntest_subsets_neg =  test_subsets[test_subsets['sentiment'] == 'negative']","c9f0b5ca":"train_subsets_pos['ID'] = train_subsets_pos.index + 1\ntest_subsets_pos['ID'] = test_subsets_pos.index + 1\ntrain_subsets_neg['ID'] = train_subsets_neg.index + 1\ntest_subsets_neg['ID'] = test_subsets_neg.index + 1","52e5a1c2":"def score_to_numeric(x):\n    if x=='negative':\n        return 0\n    if x=='positive':\n        return 1","d234b1ed":"train_subsets_pos['sentiment'] = train_subsets_pos['sentiment'].apply(score_to_numeric)\ntest_subsets_pos['sentiment'] = test_subsets_pos['sentiment'].apply(score_to_numeric)\ntrain_subsets_neg['sentiment'] = train_subsets_neg['sentiment'].apply(score_to_numeric)\ntest_subsets_neg['sentiment'] = test_subsets_neg['sentiment'].apply(score_to_numeric)","33565b15":"y = 'jaccard'\n# X = [name for name in train_subsets.columns if name not in [y, 'ID', 'textID','text', 'selected_text','ngram']\nX = [name for name in train_subsets.columns if name not in [y, 'ID', 'textID','text', 'selected_text','ngram']]\nprint('y =', y)\nprint('X =', X)","684f1a8c":"train_subsets_pos[X + [y]].describe()","c24a1cbd":"train_subsets_neg[X + [y]].describe()","b6877977":"np.random.seed(12345) # set random seed for reproducibility\nsplit_ratio = 0.7     # 70%\/30% train\/test split\n\n# execute split_pos\nsplit_pos = np.random.rand(len(train_subsets_pos)) < split_ratio\ntrain_pos = train_subsets_pos[split_pos]\ntest_pos = train_subsets_pos[~split_pos]\n\n# summarize split_pos\nprint('Train_pos data rows = %d, columns = %d' % (train_pos.shape[0], train_pos.shape[1]))\nprint('Test_pos data rows = %d, columns = %d' % (test_pos.shape[0], test_pos.shape[1]))\n\n# execute split_neg\nsplit_neg = np.random.rand(len(train_subsets_neg)) < split_ratio\ntrain_neg = train_subsets_neg[split_neg]\ntest_neg = train_subsets_neg[~split_neg]\n\n# summarize split_neg\nprint('Train_neg data rows = %d, columns = %d' % (train_neg.shape[0], train_neg.shape[1]))\nprint('Test_neg data rows = %d, columns = %d' % (test_neg.shape[0], test_neg.shape[1]))","c5b20d1b":"dtrain_pos = xgb.DMatrix(train_pos[X], train_pos[y])\ndtest_pos = xgb.DMatrix(test_pos[X], test_pos[y])\ndtrain_neg = xgb.DMatrix(train_neg[X], train_neg[y])\ndtest_neg = xgb.DMatrix(test_neg[X], test_neg[y])","7f88aea4":"#Negative dataset model training\nbase_y_pos = train_pos[y].mean()\n\n# tuning parameters\nparams = {\n    'objective': 'reg:linear', \n    'bagging_fraction': 0.8768575337571937,\n    'colsample_bytree': 0.9933592930641432,\n    'feature_fraction': 0.816825176108506,\n    'gamma': 0.05587328363633812,\n    'learning_rate': 0.19879098664834996,\n    'max_depth': 6,\n    'min_child_samples': 9,\n    'num_leaves': 7,\n    'reg_alpha': 0.11806338517600543,\n    'reg_lambda': 0.23269341544465222,\n    'subsample': 0.6,\n    'base_score': base_y_pos,                       # calibrate predictions to mean of y \n    'seed': 12345                               # set random seed for reproducibility\n}\n\n# watchlist is used for early stopping\nwatchlist_pos = [(dtrain_pos, 'train'), (dtest_pos, 'eval')]\n\n# train model\nxgb_model_pos = xgb.train(params,                   # set tuning parameters from above                   \n                      dtrain_pos,                   # training data\n                      1000,                     # maximum of 1000 iterations (trees)\n                      evals=watchlist_pos,          # use watchlist for early stopping \n                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in rmse \n                      verbose_eval=True)\n\n\n","8d1dae55":"#Negative dataset model training\nbase_y_neg = train_neg[y].mean()\n\n# tuning parameters\nparams = {\n    'objective': 'reg:linear', \n    'bagging_fraction': 0.8768575337571937,\n    'colsample_bytree': 0.9933592930641432,\n    'feature_fraction': 0.816825176108506,\n    'gamma': 0.05587328363633812,\n    'learning_rate': 0.19879098664834996,\n    'max_depth': 6,\n    'min_child_samples': 9,\n    'num_leaves': 7,\n    'reg_alpha': 0.11806338517600543,\n    'reg_lambda': 0.23269341544465222,\n    'subsample': 0.6,\n    'base_score': base_y_neg,                       # calibrate predictions to mean of y \n    'seed': 12345                               # set random seed for reproducibility\n}\n\n# watchlist is used for early stopping\nwatchlist_neg = [(dtrain_neg, 'train'), (dtest_neg, 'eval')]\n\n# train model\nxgb_model_neg = xgb.train(params,                   # set tuning parameters from above                   \n                      dtrain_neg,                   # training data\n                      1000,                     # maximum of 1000 iterations (trees)\n                      evals=watchlist_neg,          # use watchlist for early stopping \n                      early_stopping_rounds=50, # stop after 50 iterations (trees) without increase in rmse \n                      verbose_eval=True)\n","fc520dd6":"predictions_pos = xgb_model_pos.predict(dtest_pos)\npredictions_neg = xgb_model_neg.predict(dtest_neg)","a7c8be8a":"xgb.plot_importance(xgb_model_pos,importance_type='weight')\nxgb.plot_importance(xgb_model_neg,importance_type='weight')","c3709606":"dtest_subsets_pos = xgb.DMatrix(test_subsets_pos[X])\ndtest_subsets_neg = xgb.DMatrix(test_subsets_neg[X])","517ad5b2":"prediction_test_pos = xgb_model_pos.predict(dtest_subsets_pos)\nprediction_test_neg = xgb_model_neg.predict(dtest_subsets_neg)","a211afba":"test_subsets_pos['Jaccard'] = prediction_test_pos\ntest_subsets_neg['Jaccard'] = prediction_test_neg","fab74302":"test_subsets_pos.head()","60d66410":"test_subsets_neg.head()","1aec738f":"test_submission_pos = test_subsets_pos.sort_values('Jaccard', ascending=False).drop_duplicates(['textID'])\ntest_submission_neg = test_subsets_neg.sort_values('Jaccard', ascending=False).drop_duplicates(['textID'])","37def530":"test_submission_pos = test_submission_pos[['textID','ngram']]\ntest_submission_neg = test_submission_neg[['textID','ngram']]\ntest_submission_pos = test_submission_pos.rename(columns = {'ngram':'selected_text'})\ntest_submission_neg = test_submission_neg.rename(columns = {'ngram':'selected_text'})\ntest_submission = test_submission_pos.append(test_submission_neg)\nneutral_submission = neutral_test[['textID','text']]\nneutral_submission = neutral_submission.rename(columns = {'text':'selected_text'})\nsubmission = test_submission.append(neutral_submission,ignore_index=True)\nsample_submission = sample_submission.drop(columns=['selected_text'])\nsample_submission = pd.merge(sample_submission,submission,on='textID',how='left')","c9a2cca1":"sample_submission.to_csv('submission.csv', index = False)","0373dcb4":"- textID - unique ID for each piece of text\n- text - the text of the tweet\n- sentiment - the general sentiment of the tweet\n- selected_text - [train only] the text that supports the tweet's sentiment","0b54dedd":"The jaccard scores for texts in train set with neutral sentiment are close to 1, so we use original text to represent the forecasting result.","88ca2af9":"## Xgboost","640de6fb":"Reference: https:\/\/www.kaggle.com\/nkoprowicz\/a-simple-solution-using-only-word-counts","f70505b8":"### Load Packages","2b19896a":"Check Missing Values","a5a10c4e":"### Load Dataset","3fd30251":"Create features\n","ad368293":"Remove URL from text","81fb91e7":"Create Features"}}