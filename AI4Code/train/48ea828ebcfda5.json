{"cell_type":{"62b3b2ae":"code","09b707b3":"code","a6ea91d0":"code","1ff1f750":"code","ca4ec5fc":"code","189aae62":"code","715c3acf":"code","198e2187":"code","15552d3d":"code","6df7fcb2":"code","ecfb8271":"code","6a615acd":"code","351acffe":"code","a893a6a4":"code","fc771288":"code","02435c42":"code","bd00017b":"code","b1a9093e":"code","1ad41e03":"code","27a7357a":"code","c6eac3ab":"code","a105fa04":"code","886bc290":"code","500f3ae1":"code","e4bf8468":"code","c3e02c48":"code","a57a105f":"code","2f90fbda":"code","84b131cd":"code","30f87ddf":"code","69615064":"code","11df358a":"code","321dd24d":"code","ca08fabb":"code","cfcee91f":"code","4805b360":"code","bed975cf":"code","f38c2a6d":"code","342196c3":"code","b4689a9a":"code","9dae4c66":"code","61fcefa2":"code","596ee130":"code","b99ecb36":"code","6a6e0c93":"code","7f37ff85":"code","eb99da7e":"code","cdba315c":"code","5845868c":"code","2e8df9d1":"code","bf3d0861":"code","82dab531":"code","471f4307":"code","9926d032":"code","3195f747":"code","8fa9add8":"code","e8b51cc7":"code","1c955c3a":"code","dd825dcf":"code","12fee494":"code","1d034338":"code","6b26d3ae":"code","62674ead":"code","4c6acc0f":"code","4f23bc52":"code","9bfb858e":"code","89bc1da5":"code","51b05999":"code","e06a9f2d":"code","1886fd4e":"code","a5f8f4c6":"code","d43db182":"code","f7707f81":"code","b116e722":"code","599dc5dc":"code","ea72ca16":"code","02a2ea30":"code","09f740e2":"code","90a18723":"code","8d7d6b78":"code","b63fd5bb":"code","a5231e38":"code","a9e225d3":"code","7e399367":"code","5bf7aaf0":"code","a37117c4":"code","a33073f4":"code","45018867":"code","7ee5fa51":"code","1591e162":"code","1c128563":"code","3885db8b":"code","e1821ccd":"code","ab67bc3a":"code","97005932":"code","e671d6f8":"code","81faba1a":"code","4b57af5b":"code","ce86e2fb":"code","08bf184c":"code","7fd1ffce":"code","4f11db61":"code","ea032ffa":"code","fe7221af":"code","ad0e7187":"code","d015417b":"code","8a9cc48a":"code","cdfdefc4":"code","86324275":"code","5341714a":"markdown","b1f252c8":"markdown","be2719d7":"markdown","f72932db":"markdown","3b65a81e":"markdown","456f6af1":"markdown","2b860ac4":"markdown","06b91513":"markdown","b580df95":"markdown","4d5b2bb5":"markdown","cde9011d":"markdown","daeca4a8":"markdown","1b4d2bea":"markdown","a90e0db0":"markdown","afbd582c":"markdown","6d854467":"markdown","245384f5":"markdown","1cc13781":"markdown","b8f11239":"markdown","86afe512":"markdown","decc9978":"markdown","126b7121":"markdown","02f270d0":"markdown","32a7ae52":"markdown","8619f22e":"markdown","f28501fc":"markdown","8eb57825":"markdown","cf91efbd":"markdown","6cc593ac":"markdown","c3cd8815":"markdown","9830bd32":"markdown","31569bcd":"markdown","f2cecd8f":"markdown","9f124af7":"markdown","1c711e9f":"markdown"},"source":{"62b3b2ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09b707b3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\ncolor = sns.color_palette()\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\npd.set_option('display.max_columns', None)","a6ea91d0":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","1ff1f750":"train_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop('Id', axis = 1, inplace = True)\ntest.drop('Id', axis = 1, inplace = True)","ca4ec5fc":"print('Train', train.shape)\nprint('Test', test.shape)","189aae62":"sns.scatterplot(train['GrLivArea'], train['SalePrice'])","715c3acf":"# Lets remove the values where the area is very big but the price is surprisingly low\n\nouts = train[(train['GrLivArea']>4000) & (train['SalePrice']<200000)]\ntrain.drop(outs.index, inplace = True)","198e2187":"sns.distplot(train['SalePrice'], fit = norm)","15552d3d":"stats.probplot(train['SalePrice'], plot = plt)\nplt.show()","6df7fcb2":"train['SalePrice'] = np.log1p(train['SalePrice'])","ecfb8271":"sns.distplot(train['SalePrice'], fit = norm)\nplt.show()\nstats.probplot(train['SalePrice'], plot = plt)\nplt.show()","6a615acd":"n_train = train.shape[0]\nn_test = test.shape[0]\ntarget = train.SalePrice.values\n\nall_data = pd.concat((train, test)).reset_index(drop = True)\nall_data.drop('SalePrice', axis = 1, inplace = True)\n\nprint('Shape of complete data ', all_data.shape)","351acffe":"all_data.head()","a893a6a4":"train.isna().sum()","fc771288":"miss = (all_data.isna().sum()\/len(all_data)).to_frame()\nmiss.rename(columns = {0:'Missing Values'}, inplace = True)\n\nmiss = miss[miss['Missing Values']>0]\nmiss = miss.sort_values(by = 'Missing Values', ascending = False)\n","02435c42":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation = 90)\nsns.barplot(miss.index, miss['Missing Values'],)","bd00017b":"correlations = train.corr()['SalePrice'].to_frame()\ncorrelations.sort_values(by = 'SalePrice', ascending = False).style.background_gradient()","b1a9093e":"miss.index","1ad41e03":"# Ex\tExcellent\n# Gd\tGood\n# TA\tAverage\/Typical\n# Fa\tFair\n# NA\tNo Pool\n\nall_data['PoolQC'].value_counts()","27a7357a":"#Lets convert the NA to NO Pool\nall_data['PoolQC'].fillna('None', inplace = True)","c6eac3ab":"all_data['MiscFeature'].value_counts()","a105fa04":"all_data['MiscFeature'].fillna('None', inplace = True)","886bc290":"all_data['Alley'].value_counts()","500f3ae1":"all_data['Alley'].fillna('None', inplace = True)","e4bf8468":"all_data['Fence'].fillna('None', inplace = True)","c3e02c48":"all_data['FireplaceQu'].fillna('None', inplace = True)","a57a105f":"fillers = all_data.groupby('Neighborhood')['LotFrontage'].median()","2f90fbda":"a = all_data[all_data['LotFrontage'].isna()]","84b131cd":"all_data['LotFrontage'] =  all_data['LotFrontage'].fillna(all_data['Neighborhood'].map(fillers))","30f87ddf":"for col in 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType':\n    all_data[col].fillna('None', inplace = True)","69615064":"for  col in 'GarageYrBlt', 'GarageArea','GarageCars':\n    all_data[col].fillna(0, inplace = True)","11df358a":"for col in 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath':\n    all_data[col].fillna(0, inplace = True)","321dd24d":"for col in 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1' ,'BsmtFinType2':\n    all_data[col].fillna('None', inplace = True)\n","ca08fabb":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","cfcee91f":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","4805b360":"all_data = all_data.drop(['Utilities'], axis=1)","bed975cf":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","f38c2a6d":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","342196c3":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","b4689a9a":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","9dae4c66":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","61fcefa2":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","596ee130":"all_data.isna().sum()","b99ecb36":"all_data[\"AllSF\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\nall_data['Area'] = all_data['LotArea']*all_data['LotFrontage']\nall_data['Area_log'] = np.log1p(all_data['Area'])","6a6e0c93":"num_cols = all_data.columns[all_data.dtypes != 'object']","7f37ff85":"c = []\nfor col in num_cols:\n    if(all_data[col].nunique() < 15):\n        c.append(col)\n","eb99da7e":"c.remove('PoolArea')","cdba315c":"for col in c:\n    all_data[col] = all_data[col].astype(str)\n ","5845868c":"cats = all_data.columns[all_data.dtypes == 'object']","2e8df9d1":"ordinals = ['MSSubClass','ExterQual','LotShape','BsmtQual','BsmtCond','BsmtExposure',\n            'BsmtFinType1', 'BsmtFinType2','HeatingQC','Functional','FireplaceQu','KitchenQual', \n            'GarageFinish','GarageQual','GarageCond','PoolQC','Fence']\n\norders=[ ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180','190'],\n         ['Fa','TA','Gd','Ex'],  ['Reg','IR1' ,'IR2','IR3'], \n         ['None','Fa','TA','Gd','Ex'],  ['None','Po','Fa','TA','Gd','Ex'], \n         ['None','No','Mn','Av','Gd'], \n         ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], \n         ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n         ['Po','Fa','TA','Gd','Ex'],  ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n         ['None','Po','Fa','TA','Gd','Ex'],  ['Fa','TA','Gd','Ex'],\n         ['None','Unf','RFn','Fin'],  ['None','Po','Fa','TA','Gd','Ex'],\n         ['None','Po','Fa','TA','Gd','Ex'],\n         ['None','Fa','Gd','Ex'],  ['None','MnWw','GdWo','MnPrv','GdPrv'] ]","bf3d0861":"from sklearn.preprocessing import OrdinalEncoder","82dab531":"for i in range(len(orders)):\n    oe = OrdinalEncoder(categories={0:orders[i]})\n    all_data.loc[:,ordinals[i]] = oe.fit_transform(all_data.loc[:,ordinals[i]].values.reshape(-1,1))","471f4307":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","9926d032":"for i in ordinals:\n   all_data[i] = all_data[i].astype('str')","3195f747":"nums = all_data.columns[all_data.dtypes != 'object']\nskews = all_data[nums].apply(lambda x : skew(x.dropna())).sort_values(ascending = False)\nskewness = pd.DataFrame({'Skewness':skews})","8fa9add8":"skewness = skewness[abs(skewness)>0.75]","e8b51cc7":"from scipy.special import boxcox1p","1c955c3a":"skewed_features = skewness.index","dd825dcf":"skewed_features\nlam = 0.15\n\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lam)","12fee494":"rest = set(all_data.columns).difference(ordinals)\nlen(rest)","1d034338":"OHE_encoded = pd.get_dummies(all_data[rest],drop_first=True)","6b26d3ae":"OHE_encoded.head()","62674ead":"df = pd.concat((OHE_encoded, all_data[ordinals]), axis = 1)","4c6acc0f":"df.head()","4f23bc52":"for col in df:\n    df[col] = pd.to_numeric(df[col])","9bfb858e":"df.shape","89bc1da5":"train = df[:n_train]\ntest = df[n_train:]","51b05999":"train.shape","e06a9f2d":"test.shape","1886fd4e":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVR","a5f8f4c6":"!pip install pycaret[full]\n","d43db182":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = .2)","f7707f81":"def rmse_cv(model):\n    kf = KFold(n_splits = 6, shuffle = True, random_state = 42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, target, scoring ='neg_mean_squared_error', cv = kf))\n    rmse_val = np.sqrt(-cross_val_score(model, train.values, target, scoring = 'neg_mean_squared_error', cv = kf))\n    \n    return \"Train \",rmse, \"Validation\", rmse_val","b116e722":"lasso =  make_pipeline(RobustScaler(), Lasso(alpha = 0.0002144127223451149, max_iter=200000))","599dc5dc":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0010936129372972772, l1_ratio=0.21322431030386174, random_state=3, max_iter= 200000)) ","ea72ca16":"KRR = KernelRidge(alpha=0.015, kernel='polynomial', degree=1, coef0=0.5)\n","02a2ea30":"GBoost = GradientBoostingRegressor(n_estimators=1070, max_depth = 3, min_samples_split =  0.9900745088899041, \n                                      min_samples_leaf = 0.0024212450940811515, max_features = 'log2',\n                                      learning_rate = 0.23500000000000001, loss = 'huber',\n                                      criterion  = 'mse')\n","09f740e2":"model_xgb = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.9, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.191, max_delta_step=0, max_depth=2,\n             min_child_weight=3, monotone_constraints='()',\n             n_estimators=180, n_jobs=-1, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=123, reg_alpha=0.001,\n             reg_lambda=0.4, scale_pos_weight=13.8, subsample=0.7,\n             tree_method='auto', validate_parameters=1, verbosity=0)","90a18723":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","8d7d6b78":"model_svr = SVR()\nmodel_svr.get_params()","b63fd5bb":"score = rmse_cv(lasso)\nprint(\"\\nLasso score on Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\nLasso score on Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))","a5231e38":"score = rmse_cv(ENet)\nprint(\"\\nElasticNet score on Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\nElasticNet score on Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))\n","a9e225d3":"score = rmse_cv(KRR)\nprint(\"\\nKernel Ridge score on Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\nKernel Ridge score Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))\n","7e399367":"score = rmse_cv(GBoost)\nprint(\"\\nGradient Boosting scoreon Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\nGradient Boosting score Test aSet: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))\n\n","5bf7aaf0":"score = rmse_cv(model_xgb)\nprint(\"\\nXgboost Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\nXgboost Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))\n","a37117c4":"score = rmse_cv(model_lgb)\nprint(\"\\LGM Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\LGM Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))","a33073f4":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn import metrics","45018867":"from skopt import space\nfrom skopt import gp_minimize\nfrom sklearn.metrics import mean_squared_error","7ee5fa51":"from hyperopt import hp, fmin,tpe, Trials\nfrom hyperopt.pyll import scope\n","1591e162":"restore_these_settings = np.geterr()\n\ntemp_settings = restore_these_settings.copy()\ntemp_settings[\"over\"] = \"ignore\"\ntemp_settings[\"under\"] = \"ignore\"\n\nnp.seterr(**temp_settings)\n","1c128563":"#params = {'alpha': np.arange(0,1,0.001)}\n\nparam_space = {\n                \n               \n    'loss' : hp.choice('loss', ['ls', 'lad', 'huber', 'quantile']),\n    'learning_rate' : scope.float(hp.quniform('learning_rate',0.1,1, 0.005)),\n    'n_estimators' : scope.int(hp.quniform('n_estimators', 20,4000,10)),\n    'criterion' : hp.choice('criterion', ['friedman_mse','mse']),\n     'min_samples_split' : hp.uniform('min_samples_split', 0.0,1.0, ),\n    'min_samples_leaf' : scope.float(hp.uniform('min_samples_leaf', 0.0,0.5)),\n     'max_depth' : scope.int(hp.quniform('max_depth', 2,20,1)),\n    'max_features' : hp.choice('max_features',['auto', 'sqrt', 'log2',])\n   \n}\n\n","3885db8b":"def optimize(params, x, y):\n    #params = dict(zip(param_names, params))\n    model = GradientBoostingRegressor(**params)\n    kf = KFold(n_splits=5)\n    accuracies = []\n    \n    for train_idx, test_idx in kf.split(X=x):\n        \n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        x_test = x[test_idx]\n        y_test = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(x_test)\n        \n        fold_acc = mean_squared_error(preds, y_test)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n        \n        ","e1821ccd":"from functools import partial\n!pip install scikit-opt\n","ab67bc3a":"optimization_function = partial(\n    optimize,\n    x = train.values,\n    y = target)\n\ntrials = Trials()","97005932":"results = fmin(fn = optimization_function, \n                     space = param_space, \n                      max_evals = 100,\n                     trials=trials,\n                    algo = tpe.suggest,\n                     verbose = 10)\nprint(results)\n","e671d6f8":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","81faba1a":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmse_cv(averaged_models)\nprint(\"\\Averaged base models score Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\Averaged base models score Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))\n\n","4b57af5b":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    \n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","ce86e2fb":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmse_cv(stacked_averaged_models)\nprint(\"\\Stacking base models score Training Set: {:.4f} ({:.4f})\\n\".format(score[1].mean(), score[1].std()))\nprint(\"\\Stacking base models score Test Set: {:.4f} ({:.4f})\\n\".format(score[3].mean(), score[3].std()))\n","08bf184c":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","7fd1ffce":"stacked_averaged_models.fit(train.values, target)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(target, stacked_train_pred))","4f11db61":"od = train.columns[train.dtypes == 'object']\nfor col in od:\n    train[col] = train[col].astype('float')\n    test[col] = test[col].astype('float')","ea032ffa":"model_xgb.fit(train, target)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(target, xgb_train_pred))","fe7221af":"model_lgb.fit(train, target)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(target, lgb_train_pred))","ad0e7187":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(target,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))\n","d015417b":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n","8a9cc48a":"ensemble","cdfdefc4":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('ensemble.csv',index=False)","86324275":"sub","5341714a":"#### Bayesian Optimization - Hyperopt","b1f252c8":"### Correlation ","be2719d7":"#### MSZoning","f72932db":"#### KitchenQual","3b65a81e":"## Preprocessing","456f6af1":"### LotFrontage","2b860ac4":"#### Fence","06b91513":"#### MiscFeature","b580df95":"### Box Cox Transformation of (highly) skewed features\n\n","4d5b2bb5":"#### 'GarageYrBlt', 'GarageArea','GarageCars'","cde9011d":"#### MasVnrArea and MasVnrType :","daeca4a8":"#### 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'","1b4d2bea":"#### MSSubClass","a90e0db0":"#### PoolQC","afbd582c":"#### Exterior1st and Exterior2nd ","6d854467":"#### SaleType","245384f5":"#### BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2","1cc13781":"## Pycaret for tuning","b8f11239":"## Ensembling StackedRegressor, XGBoost and LightGBM","86afe512":"## Missing Values","decc9978":"### Model Tuning","126b7121":"#### Outliers","02f270d0":"Lets use Elastic Net for this purpose","32a7ae52":"#### Target","8619f22e":"## More features engeneering\u00b6\n","f28501fc":"Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, \nthis feature won't help in predictive modelling. We can then safely remove it.","8eb57825":"#### Functional","cf91efbd":"## Feature Engineering","6cc593ac":"### Base models scores\n","c3cd8815":"#### Alley","9830bd32":"Linear models work best if the distribution is linear. Lets use log transformation to do that.","31569bcd":"#### 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType':","f2cecd8f":"### Skewed features\n\n","9f124af7":"#### FireplaceQu","1c711e9f":"#### Electrical"}}