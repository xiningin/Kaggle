{"cell_type":{"6672331d":"code","a555f7fc":"code","a4d84bda":"code","9000a53f":"code","0f4a7fa7":"code","d9eb6e41":"code","1a40b70d":"code","952a0476":"code","31b5fa8b":"code","0ef2facc":"code","7539366b":"code","8e4d0bd6":"code","5ae3b6b7":"code","9b9ca622":"code","c9f6c228":"code","3dd7cc2b":"code","b4a9ebe7":"code","b0564bf0":"code","6a7eba04":"code","0fe70470":"code","0c0e8795":"code","21f4f363":"code","1744edfc":"code","dd5e59c5":"code","37cd20e7":"code","4f72785a":"code","9da65252":"code","f2269482":"code","13024e92":"markdown","9b5349f2":"markdown","cb25c6d9":"markdown","dc26d0cc":"markdown","8b10dc23":"markdown","8de59af3":"markdown","316bd7e7":"markdown","7e61a1d6":"markdown","5c9a0535":"markdown","cb507527":"markdown","9e1afa0f":"markdown","c7d8a5d0":"markdown","89c023c2":"markdown","df14acd1":"markdown","8d92b5ce":"markdown","f00774db":"markdown","c5103fc9":"markdown","d850be19":"markdown","4ea1c422":"markdown","f2326ff3":"markdown","39a3b5d8":"markdown","79f5658c":"markdown","e1ed2be2":"markdown","eb6f8e5c":"markdown","93e6f43d":"markdown","d622f402":"markdown","34a623e8":"markdown","d0a9b1fe":"markdown","630f51a9":"markdown","fc53efcb":"markdown","cf993291":"markdown"},"source":{"6672331d":"#importing dataset from kaggle:\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a555f7fc":"# Importing necessary libraries:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import dendrogram\n# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go","a4d84bda":"df=pd.read_csv('\/kaggle\/input\/young-people-survey\/responses.csv')\ndf.head()","9000a53f":"mapping=pd.read_csv('\/kaggle\/input\/young-people-survey\/columns.csv')\nfinal=df[['Movies','Horror','Comedy','Romantic','Sci-fi','War','Documentary','Action','History','Countryside, outdoors','Celebrities','Science and technology','Art exhibitions','Alcohol','Healthy eating','Prioritising workload','Workaholism','Number of friends','Interests or hobbies','Internet usage','Energy levels','Finances','Socializing','Entertainment spending','Spending on gadgets','Spending on healthy eating','Age']]\nfinal.info()","0f4a7fa7":"final.isna().sum()","d9eb6e41":"final['Alcohol'].replace(['never', 'social drinker','drink a lot'], [0,3,5],inplace=True)\nfinal['Internet usage'].replace(['few hours a day', 'most of the day', 'less than an hour a day','no time at all'], [5,3,1,0],inplace=True)","1a40b70d":"final.fillna(df.median(),inplace=True)\nfinal['Alcohol']=final['Alcohol'].fillna(final['Alcohol'].median())\nfinal=final.astype(int)","952a0476":"final.describe()","31b5fa8b":"list=['Movies','Horror','Comedy','Romantic','Sci-fi','War','Documentary','Action','History','Countryside, outdoors','Celebrities','Science and technology','Art exhibitions','Alcohol','Healthy eating','Prioritising workload','Workaholism','Number of friends','Interests or hobbies','Internet usage','Energy levels','Finances','Socializing','Entertainment spending','Spending on gadgets','Spending on healthy eating','Age']\nmapping.loc[mapping['short'].isin(list)].reset_index(drop=True)","0ef2facc":"final.head()","7539366b":"sns.set(rc={'figure.figsize':(15,10)})\nax=sns.heatmap(final.corr(),center=0.00,cmap=sns.diverging_palette(255, 133,as_cmap=True))\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","8e4d0bd6":"sns.barplot(final['Romantic'],final['Science and technology'],palette=\"GnBu_d\");","5ae3b6b7":"sns.barplot(final['Energy levels'],final['Number of friends'],palette=sns.dark_palette(\"purple\"));","9b9ca622":"sns.lineplot(final['Age'],final['Prioritising workload'],palette=\"GnBu_d\");","c9f6c228":"sns.kdeplot(final['Art exhibitions'],final['Science and technology']);","3dd7cc2b":"from sklearn.cluster import KMeans\nx=final\nwcss = []\nfor i in range(2, 9):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 3000, n_init = 10,random_state=0)\n    km.fit(x)\n    wcss.append(km.inertia_)\n    \nplt.plot(range(2, 9), wcss)\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()","b4a9ebe7":"km = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 1000, n_init = 30)\nkm.fit(x)\ny_means=km.predict(x)\n","b0564bf0":"silhouette_score(x,y_means)","6a7eba04":"x=final[['Science and technology','Countryside, outdoors','Art exhibitions']]\n\nscaler=StandardScaler()\nscaler.fit(x)\nwcss = []\n#x=scaler.transform(x)\nfor i in range(2, 6):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 1000, n_init = 10,random_state=0)\n    km.fit(x)\n    y_means=km.predict(x)\n    s=silhouette_score(x,y_means)\n    wcss.append(s)\nplt.plot(range(2, 6), wcss)\nplt.title('No. of clusters vs Silhouette Score', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('Silhouette_Score')\nplt.show()","0fe70470":"km = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 1000, n_init = 30,random_state=2)\nkm.fit(x)\ny_means=km.predict(x)\nfinal['y_means']=y_means\nfinal.groupby('y_means').mean()","0c0e8795":"def configure_plotly_browser_state():\n  import IPython\n  display(IPython.core.display.HTML('''\n        <script src=\"\/static\/components\/requirejs\/require.js\"><\/script>\n        <script>\n          requirejs.config({\n            paths: {\n              base: '\/static\/base',\n              plotly: 'https:\/\/cdn.plot.ly\/plotly-1.5.1.min.js?noext',\n            },\n          });\n        <\/script>\n        '''))\n  \ntrace1 = go.Scatter3d(\n    x= final['Science and technology'],\n    y= final['Countryside, outdoors'],\n    z= final['Art exhibitions'],\n    mode='markers',\n     marker=dict(\n        color = final['y_means'], \n        size= 10,\n        line=dict(\n            color= final['y_means'],\n            width= 12\n        ),\n        opacity=0.9\n     )\n)\ndg = [trace1]\n\nlayout = go.Layout(\n    title = '3D visualization of clusters',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    ),\n \n)\nconfigure_plotly_browser_state()\n\nfig = go.Figure(data = dg, layout = layout)\n\nfig.show()","21f4f363":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))\nplt.title('Dendrogam', fontsize = 20)\nplt.xlabel('Individuals')\nplt.ylabel('Ecuclidean Distance')\nplt.show()","1744edfc":"from sklearn.cluster import AgglomerativeClustering\nx=final[['Science and technology','Countryside, outdoors','Art exhibitions']]\nhc = AgglomerativeClustering(n_clusters = 4, affinity = 'euclidean')\ny_hc = hc.fit_predict(x)","dd5e59c5":"silhouette_score(x,y_hc)","37cd20e7":"Art = input(\" Enter your score here: \")","4f72785a":"Outdoors = input(\" Enter your score here: \")","9da65252":"Science = input(\" Enter your score here: \")","f2269482":"test=[[Science,Outdoors,Art]]\ny_means=km.predict(test)\ny_means[0]\nif y_means[0]==2:\n  print(\"You are identified to be an all rounder!\")\nelif y_means[0]==3:\n  print(\"You are identified to be a geek!\")\nelif y_means[0]==1:\n  print(\"You are identified to be a hopeless romantic!\")\nelse:\n  print(\"You are identified to be an outcast!\")","13024e92":"Next we will take a look at the summary statistics of our data.","9b5349f2":"We can see that there is some correlation between quite a few of the variables but mostly less than 0.2. Therefore we are not going to be removing any variables as none seem completely redundant. Let us take a look at the relationship between some of the more correlated variables.","cb25c6d9":"The value is lower than what we obtained from K- Means. With that we have come to an end of our analysis with heirarchical clustering. With that we decide to use KNN. There is a high chance there is not too many well distinguised clusters in our dataset and clearly using all the variables we have selected here was not a suitable choice. With that we come to an end of our analysis.\n\n## What kind of person are you!?:\nIn this final section we will classify you into one of the 4 classes we have formed! Fill in the values below!\n\nFirst, rate how much you like doing art or visiting exhibitions etc. from 1-5, one being strongly disliking  and 5 being really liking it.\n","dc26d0cc":"For handling our missing values we will be using the median to fill in our gaps(median is a good measure considering that all our data is actually categorical data).","8b10dc23":"## Clustering analysis:\nFor our analysis we will be trying K-Means clustering as well asagglomerative clustering. We will be trying both models and will select the one  that works well. We also have an idea of how many clusters we want\/ expecting. We want around 4-6 clusters. This is because of the variables we have chosen and the aprroximate number of personality types we can estimate to have. \nWe will start of with KNN.\n## KNN clustering:\nFirstly, we will determine the exact number of clusters we want to input using the elbow method.","8de59af3":"One point we can note is that it does not look like a regular clustering algorithm graph mostly due to the fact that all our variables are categorical and every point in our 3D space will be occupied.\nWith that we have come to the end of our analysis using KNN. Next we will take a look at our data using agglomarative clustering(hierarchical clustering).","316bd7e7":"### Loading the data and libraries:","7e61a1d6":"Straight away we can see the negative relation between their choice of movies of Romance genre of a person and their inclination for science and technology.","5c9a0535":"Now we will use heirarchical clustering and check our silhouette score.","cb507527":"Straight away we can see that this score is bad. A score of 0 essentially means the boundary between the clusters are touching and the distance is nill. This is a fairly discouraging statistics. Next, let us try adjusting our variables to see if we get better results. The extremely low Silhouette score could be due to a few reasons, this includes the possibility that there are no clusters as such. It could also be because there input variables are poorly chosen or our model is poorly optimized. Perhaps some other models could work better.","9e1afa0f":"Right away we can see that as age increases so does priority for workload. With that, we have come to the end of the EDA and preprocessing. ","c7d8a5d0":"Similarly we can see the positive relationship between the number of friends and energy levels.","89c023c2":"Now, we will take a look at the heat map for the correlation between the various variables and check for relationships.","df14acd1":"Next, we will convert all our values into numerical values.","8d92b5ce":"And finally, rate how much you are interested in science and technology.","f00774db":"## Exploratory data analysis and data pre-processing:\nFirst let us take a look at the entire data set.","c5103fc9":"We can see that most of the values are numerical values(integers actually). Another point we can notice is that quite a few values are missing from a lot of the columns. We can take a deeper look at that soon. Also we notice that there are two objects and from looking at the head. We can see that they too heirichal objects and can easily be translated into numerical values.Let us take a look at the missing values.","d850be19":"Plotting the density graphs we can see that a person more inclined towards science and technology often tends to be less so interested in art. Let us see how the clustering algorithms work and what conclusions we can make from them.","4ea1c422":"## Heirarchical clustering:\nHere on, we will be using our data set which has been used for K-means clustering. We will proceed to prioritize this algorithm  and conduct further analysis on this method in the case that the silhouette score obtained for heirarchical clustering is higher than that of K means.\nFirst, we will take a look at the dendogram produced using this algorithm.","f2326ff3":"Let us take a look at the first 5 rows of the  final dataframe.","39a3b5d8":"One thing that is done for the convinience of the dataset is that a lot of the column names have been reduced to the following.Given below is the mapping for the varialbes we will be using.","79f5658c":"Below, we have tried fewer variables that we consider important and indicative of the individuals personality and tried a combination of them with varying parameters. We will be also plotting the Silhouette score vs the number of clusters to better choose how many clusters we will be using.","e1ed2be2":"We can see some interesting patterns.This is particularly notable with our variables we have chosen to cluster and we can possibly cluster into reasonable groups. We can see that for group 2 has a strong inclination towards outdoor activities as well as a very strong inclination towards the arts, healthy eating and has multiple interests while working quite hard while being quite social. Seems like quite the all rounder. \n\nTaking a look at the group 3, we can straight away see an interest for science and technology as well as an interest in Sci-Fi,action,War movise etc while also being quite responsible. We can categorize as the geek.\n\nLooking at group 1, the characteristics that stand out are the increased scores for romantic movies as well as Celebrities(the difference may seem low but running the model multiple times there is almost always an inevitable celebrity\/romantic category with a strong distaste for science and tech with some other significant differences. We can bring them under the category hopeless romantic.\n\nTh final category, group 0, the stand out are the increased affinity for horror movies and alcohol while having the least energy,  we can aptly name them outcasts.\n\n\nFinally, let us take a look at the plot of our three attributes used for our analysis in a 3D interactive map done using plotly.","eb6f8e5c":"Next, rate how much you enjoy the outdoors:","93e6f43d":"To measure how well our model performed we will be using Silhouette score as shown below( it has a range of -1 to 1)","d622f402":"What we can see is that there are 150 variables and using all of them is going to be extremely confusing, redundant and ineffective for analysis. So, instead, the first thing we do is handpick a few(27) I would love to use for this project to make my clutering analysis. Let us take a look at the variables chosen along with their info.","34a623e8":"Looking at the graph above we decide to choose 5 clusters for our final analysis.","d0a9b1fe":"With some tweaking we see that our silhouette score has improved considerably. It is not great still but much better and possibly the clusters hold meaning.\n\nLet us take the group by the clusters and see if we can recognize patterns in our clusters.","630f51a9":"# What kind of person are you? A clustering analysis:\nFor this project I found an extremely interesting dataset on Kaggle which is a survey of young people with a wide range of variables(150 of them to be precise) from movie tastes to health habits.  I will be performing a clustering analysis to group people based on a number of variables and finally label them myself based on my intuition. This is my first clustering project and I hope to gain a deeper understanding on clustering. I personally find this dataset very interesting because there is such a wide, interesting collection of variables which lets me choose  variables and customize the analysis to my liking and proceed from there. This is my first clustering analysis! So any input is appreciated.\n\n## The data:\nIn 2013, students of the Statistics class at FSEV UK were asked to invite their friends to\nparticipate in this survey.\n\nThe data file (responses.csv) consists of 1010 rows and 150 columns (139\ninteger and 11 categorical).\nFor convenience, the original variable names were shortened in the\ndata file. See the columns.csv file if you want to match the data with the original names.\nThe data contain missing values.\nThe survey was presented to participants in both electronic and written form.\nThe original questionnaire was in Slovak language and was later translated\ninto English.\nAll participants were of Slovakian nationality, aged between 15-30.\nThe variables can be split into the following groups:\n\nMusic preferences (19 items)\n\nMovie preferences (12 items)\n\nHobbies & interests (32 items)\n\nPhobias (10 items)\n\nHealth habits (3 items)\n\nPersonality traits, views on life, & opinions (57 items)\n\nSpending habits (7 items)\n\nDemographics (10 items)\n\n## The Questions:\nThese are the questions in the survey I have decided to pursue with in my analysis:\nMOVIE PREFERENCES\n1. I really enjoy watching movies.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n2. Horror movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n3. Comedies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n4. Romantic movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n5. Sci-fi movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n6. War movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n7. Documentaries: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n8. Action movies: Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)\n\n9. History: Not interested 1-2-3-4-5 Very interested (integer)\n\n10. Outdoor activities: Not interested 1-2-3-4-5 Very interested (integer)\n\n11. Celebrity lifestyle: Not interested 1-2-3-4-5 Very interested (integer)\n\n12. Science and technology: Not interested 1-2-3-4-5 Very interested (integer)\n\n13. Drinking: Never - Social drinker - Drink a lot (categorical)\n\n14. I live a very healthy lifestyle.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n15. I try to do tasks as soon as possible and not leave them until last minute.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n16. I often study or work even in my spare time.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\nI look at things from all different angles before I go ahead.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n17. I have lots of friends.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n18. I am always full of life and energy.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n19. I have many different hobbies and interests.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n20. How much time do you spend online?: No time at all - Less than an hour a day - Few hours a day - Most of the day (categorical)\n\n21. I save all the money I can.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n22. I spend a lot of money on partying and socializing.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n23. I spend a lot of money on gadgets.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n24. I will hapilly pay more money for good, quality or healthy food.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n25. Age: (integer)\n\n26. Art: Not interested 1-2-3-4-5 Very interested (integer)\n\n27. I enjoy meeting new people.: Strongly disagree 1-2-3-4-5 Strongly agree (integer)\n\n","fc53efcb":"With that we have come to an end to this clustering analysis!\nI hope you enjoyed it despite its rather poor outcome. There is much to be improved on and much left wanted. However, it has been a learning experience for me as this is my first time doing a clustering analysis. Now that I have completed the project I better understand how clusterin works and how clustering doesn't( I believe I personally learnt a bit more of the latter in this case). It is also sometimes said clustering is not suitable for categorical variables and perhaps that could be a reason as well as to why it performed as given.\n Thank you for taking the time to go through this!","cf993291":"One observation of particular note is the age. The maximum is 30 and mean is 20.43 with the minimum of 15. This tells us the age range and that this dataset is towards the younger crowd."}}