{"cell_type":{"02e69692":"code","3f237cad":"code","594380a1":"code","f616fa49":"code","f6632abd":"code","abfb006c":"code","d2d27576":"code","671f8aae":"code","81f9236e":"code","408a4ed0":"code","7f365d14":"code","42056568":"code","d275f12c":"code","b3861ce6":"code","c09ae6a8":"code","cfaf9fc2":"code","32f5e979":"code","6ca68364":"code","c40e99b7":"code","b9ef4866":"code","3bd70790":"code","83ce4515":"code","41a73f9e":"code","40cb07b6":"code","9223413e":"code","fe8c99ef":"code","415fe72a":"markdown","6bc01e15":"markdown","9a128156":"markdown","fba0b4f9":"markdown","6ca65a8f":"markdown","d944fccb":"markdown","922ac50d":"markdown","45dbf8fa":"markdown","d75ce66b":"markdown","82e8f657":"markdown","708de20c":"markdown","06510775":"markdown","684f56d3":"markdown"},"source":{"02e69692":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.max_columns = 9999 #Making sure all columns appear\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f237cad":"FILEPATH_TRAIN = '\/kaggle\/input\/santander-value-prediction-challenge\/train.csv'\nFILEPATH_TEST = '\/kaggle\/input\/santander-value-prediction-challenge\/test.csv'","594380a1":"train = pd.read_csv(FILEPATH_TRAIN, sep=',', engine='c') #Specify sep when using C engine\ntest = pd.read_csv(FILEPATH_TEST, sep=',', engine='c')\nprint(\"Shape of train:\",train.shape, '\\n',\"Shape of test:\",test.shape)","f616fa49":"train.head()","f6632abd":"train.describe()","abfb006c":"import matplotlib.pyplot as plt\nimport seaborn as sns","d2d27576":"plt.figure(figsize=(9, 7))\nplt.scatter(range(train.shape[0]), np.sort(train['target'].values))\nplt.xlabel('Index', fontsize=14)\nplt.ylabel('Target', fontsize=14)\nplt.title(\"Target Distribution\", fontsize=14)\nplt.show()","671f8aae":"plt.figure(figsize=(10,8))\nsns.distplot(train['target'].values, bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","81f9236e":"plt.figure(figsize=(10,8))\nsns.distplot(np.log1p(train['target'].values), bins=50, kde=False)\nplt.xlabel('Target', fontsize=11)\nplt.title('Target Histogram', fontsize=11)\nplt.show()","408a4ed0":"train.isnull().sum()","7f365d14":"train.isnull().sum().any()","42056568":"unique_vals = train.nunique().reset_index() #This drops NaN values by default\nunique_vals.columns = [\"Name\", \"Uniqueness\"]\nconst_d = unique_vals[unique_vals[\"Uniqueness\"]==1]\nconst_d.shape","d275f12c":"str(const_d.Name.tolist())","b3861ce6":"#Ignore any warnings that arise\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy.stats import spearmanr","c09ae6a8":"labels = []\nvalues = []\n\nfor col in train.columns:\n    if col not in [\"ID\", \"target\"]:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train['target'].values)[0])\n\ncorrelation_df = pd.DataFrame({'column_label':labels, 'correlation_val':values})        \ncorrelation_df = correlation_df.sort_values(by='correlation_val')\n\ncorrelation_df = correlation_df[(correlation_df['correlation_val']>0.1) | (correlation_df['correlation_val']<-0.1)]","cfaf9fc2":"index = np.arange(correlation_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,25))\nrec = ax.barh(index, np.array(correlation_df.correlation_val.values), color='r')\nax.set_yticks(index) #Set Y to index value of the df\nax.set_yticklabels(correlation_df.column_label.values, rotation='horizontal') #Define horizontal bar graph\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","32f5e979":"import seaborn as sns\n\ncolumns = correlation_df[(correlation_df['correlation_val']>0.11) | (correlation_df['correlation_val']<-0.11)].column_label.tolist()\n\ntmp = train[columns]\ncomat = tmp.corr(method='spearman') #Since we used spearman coefficient\nfig, ax = plt.subplots(figsize=(30,30))\n\nsns.heatmap(comat, square=True, cmap=\"RdYlGn\", annot=True)\nplt.title(\"Correlation Heatmap\", fontsize=18)\nplt.show()","6ca68364":"tr_x = train.drop(const_d.Name.tolist()+ [\"ID\", \"target\"], axis=1)\nte_x = test.drop(const_d.Name.tolist()+[\"ID\"], axis=1)\ntr_y = np.log1p(train['target'].values)","c40e99b7":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(tr_x, tr_y)","b9ef4866":"#Plot Importance factor\nfeatures = tr_x.columns.values\nimportance = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importance)[::-1][:20]\n\nplt.figure(figsize=(14,14))\nplt.title(\"Feature Importances\")\nplt.bar(range(len(indices)), importance[indices], color=\"b\", yerr=std[indices])\nplt.xticks(range(len(indices)), features[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","3bd70790":"import lightgbm as lgb","83ce4515":"def run_lgb(train_x, train_y, val_x, val_y, test_x):\n    parameters = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'num_leaves': 30,\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7,\n        'bagging_frequency': 5,\n        'bagging_seed': 2018,\n        'verbosity': -1\n    }\n    \n    lgtrain = lgb.Dataset(train_x, label=train_y)\n    lgval = lgb.Dataset(val_x, label=val_y)\n    evals_result = {}\n    model = lgb.train(parameters, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=200, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_x, num_iteration=model.best_iteration)\n    \n    return pred_test_y, model, evals_result","41a73f9e":"from sklearn.model_selection import KFold","40cb07b6":"k_fold = KFold(n_splits=5, shuffle=True, random_state=2020)\npred_test_final = 0\n\nfor d_ind, v_ind in k_fold.split(tr_x):\n    \n    d_x, v_x = tr_x.loc[d_ind, :], tr_x.loc[v_ind, :]\n    d_y, v_y = tr_y[d_ind], tr_y[v_ind]\n    pred_test, model, evals_result = run_lgb(d_x, d_y, v_x, v_y, te_x)\n    pred_test_final += pred_test\n    \npred_test_final \/= 5\npred_test_final = np.expm1(pred_test_final)","9223413e":"final_df = pd.DataFrame({\"ID\":test[\"ID\"].values, \"target\":pred_test_final})\nfinal_df.to_csv(\"submission.csv\", index=False)","fe8c99ef":"#Feature importance for LightGBM\nfig, ax = plt.subplots(figsize=(14,20))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n#ax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=16)\nplt.show()","415fe72a":"**Plotting of the target variable**","6bc01e15":"256 Unique columns are present","9a128156":"**Best way to display a right skewed distribution is to use a log scale**","fba0b4f9":"**No null values. Always a good sign**","6ca65a8f":"**Spearman correlation is better to use due to the fact that it is computed based on ranks and this data is not linear where we can use Pearson correlation**","d944fccb":"**Sources:**\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-santander-value\n\nhttps:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/59128\n\nhttps:\/\/en.wikipedia.org\/wiki\/Ordinal_data\n\nhttps:\/\/support.minitab.com\/en-us\/minitab-express\/1\/help-and-how-to\/modeling-statistics\/regression\/supporting-topics\/basics\/a-comparison-of-the-pearson-and-spearman-correlation-methods\/#:~:text=The%20Pearson%20correlation%20evaluates%20the%20linear%20relationship%20between%20two%20continuous%20variables.&text=The%20Spearman%20correlation%20coefficient%20is,evaluate%20relationships%20involving%20ordinal%20variables.\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.nunique.html\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.spearmanr.html\n\nhttps:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html\n\nhttps:\/\/matplotlib.org\/3.1.0\/tutorials\/colors\/colormaps.html\n\nhttps:\/\/www.geeksforgeeks.org\/matplotlib-pyplot-xlim-in-python\/#:~:text=The%20xlim()%20function%20in,limits%20of%20the%20current%20axes.&text=Parameters%3A%20This%20method%20accept%20the,set%20the%20xlim%20to%20right.\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.expm1.html","922ac50d":"**K fold cross validation for predictions in test set**","45dbf8fa":"**The Heatmap of Correlation**","d75ce66b":"**Print the anonymised columns**","82e8f657":"**Creating Submission file**","708de20c":"**Baseline Light GBM** (TODO:Tune it)","06510775":"**Random values are the column names which mean the columns are anonymous**","684f56d3":"**Much better**"}}