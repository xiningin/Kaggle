{"cell_type":{"6980ea45":"code","be521978":"code","40708b15":"code","e63da3e9":"code","bea8522e":"code","c733563c":"code","df5ca50b":"code","c2840e85":"code","92ac9ad3":"code","4c9ead85":"code","10c3700b":"code","09d86d69":"code","56e8a67d":"code","0c006b5e":"code","8ec1941c":"code","5daff213":"code","4e384ac3":"code","2300e92d":"code","4e6bda12":"code","7be0b2ac":"code","cb3ea6d3":"code","3d2e4625":"code","5fdee815":"code","6d34dd8f":"code","f4aa7e1c":"code","188cb9ca":"code","2cfe805f":"code","2643950c":"code","ca40d9f1":"code","d83ba8b1":"code","be124800":"code","4e2daf51":"code","72c54ec2":"code","57381458":"code","c6196172":"code","cc54df0e":"code","10b057ca":"markdown","44f70921":"markdown","44f1e499":"markdown","4f9f33f3":"markdown","5f71dae3":"markdown","1f5ad67a":"markdown","fcb9e2f7":"markdown","fe6d1da4":"markdown","3778684a":"markdown","07e18ea3":"markdown","12127c6c":"markdown","def20e78":"markdown","bbcccb7a":"markdown","4082014d":"markdown"},"source":{"6980ea45":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import time for training details\nfrom time import time\nt0 = time()\n\nimport warnings\nwarnings.filterwarnings('ignore')","be521978":"df_heart= pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","40708b15":"df_heart_row_count, df_heart_column_count=df_heart.shape\nprint('Total number of rows:', df_heart_row_count)\nprint('Total number of columns:', df_heart_column_count)","e63da3e9":"df_heart.info()","bea8522e":"df_heart.describe()","c733563c":"df_heart.isna().sum()","df5ca50b":"df_heart.head().iloc[:5]","c2840e85":"print (\"Unique values are:\\n\",df_heart.nunique())","92ac9ad3":"df_heart.target.value_counts()","4c9ead85":"df_heart.sex.value_counts()","10c3700b":"# check gender distribution in the dataset\ngender_dist = df_heart.groupby('sex')[['sex']].count()\ncolors_list = ['blue', 'darksalmon']\nplt.pie(gender_dist['sex'], labels = ['Female','Male'], autopct = '%1.1f%%', colors=['#ffadad','#89c2d9'],shadow=True, startangle=90,explode=[0.1, 0.005])\nplt.title(\"Gender distribution\",color = 'black', fontsize = '14')\nplt.show()","09d86d69":"#Getting an idea about the distribution of gender \np = sns.countplot(data=df_heart, x = 'sex', palette='RdBu_r')","56e8a67d":"df_ht = ['sex','cp','restecg','fbs','ca','slope','thal', 'target']\n\nfig, axs = plt.subplots(4, 2, figsize=(10,20))\naxs = axs.flatten()\n\n# iterate through each column of df_catd and plot\nfor i, col_name in enumerate(df_ht):\n    sns.countplot(x=col_name, data=df_heart, ax=axs[i], hue =df_heart['target'], palette= 'RdBu_r')\n    plt.title(\"Bar chart of\")\n    axs[i].set_xlabel(f\"{col_name}\", weight = 'bold')\n    axs[i].set_ylabel('Count', weight='bold')","0c006b5e":"#correlation map for features\nf,ax = plt.subplots(figsize=(12, 12))\nax.set_title('Correlation map for variables')\nsns.heatmap(df_heart.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax,cmap=\"coolwarm_r\")","8ec1941c":"#Getting an idea about the distribution of gender with cholestrol \np = sns.barplot(data=df_heart, x = 'sex',y='chol', palette='RdBu_r')","5daff213":"cp_dist = pd.DataFrame(df_heart.groupby(by=['sex','target','cp'],axis=0)['age'].count())\ncp_dist.columns = ['Total Count']\ncp_dist.index.names = ['Gender','Heart Disease 0\/1','Chest Pain Type']\ncp_dist","4e384ac3":"sns.set(style=\"whitegrid\")\np = sns.countplot(data=df_heart, x = 'cp', hue='target', palette='RdBu_r')","2300e92d":"df_heart['trestbps'].plot(kind='hist',density=True,figsize=(4,4))\ndf_heart['trestbps'].plot(kind='density')\nplt.xlabel('restbp')\nplt.ylabel('density value')\nplt.title('Resting Blood pressure value')","4e6bda12":"df_heart['ca'].plot(kind='hist',density=True,figsize=(4,4))\ndf_heart['ca'].plot(kind='density')\nplt.xlabel('chol')\nplt.ylabel('density')\nplt.title('ca')","7be0b2ac":"#Defining X and y\nX = df_heart.drop(['target'], axis=1)\ny = df_heart['target']\n\n# creating dataset split for prediction\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42) # 80-20 split\n\n# Checking split \nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_test:', X_test.shape)\nprint('y_test:', y_test.shape)","cb3ea6d3":"# 1. Using Random Forest Classifier\nt0 = time()\n# Load random forest classifier \nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a random forest Classifier\nclf = RandomForestClassifier(n_jobs=2, random_state=0)\n\n# Train the Classifier\/fitting the model\nclf.fit(X_train, y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_rf = round(clf.score(X_test,y_test) * 100, 2)\nrf_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', rf_time)\n\n#Print Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"coolwarm_r\")","3d2e4625":"#2. Gaussian Naive Bayes Classifier\nt0 = time()\n#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n# Train the Classifier\/fitting the model\ngnb.fit(X_train, y_train)\n\n# predict the response\ny_pred = gnb.predict(X_test)\nacc_gnb = round(gnb.score(X_test,y_test) * 100, 2)\ngnb_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Gaussian Na\u00efve Bayes Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', gnb_time)\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"coolwarm_r\")","5fdee815":"#import Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(max_depth=10)\n\n# Train the Classifier\/fitting the model\nclf = clf.fit(X_train,y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_dt = round(clf.score(X_test,y_test) * 100, 2)\ndt_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import accuracy_score \n\n# evaluate accuracy\nprint (\"Decision Tree Accuracy:\", metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', dt_time)\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"coolwarm_r\")","6d34dd8f":"#kNN\nimport sys, os\n\n# Import kNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Train the Classifier\/fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test,y_test) * 100, 2)\nknn_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"kNN Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', knn_time)\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"coolwarm_r\")","f4aa7e1c":"#Support Vector Machines trial\nimport sys, os\n\n#Import svm model\nfrom sklearn import svm\nfrom sklearn.svm import SVC\n\n#Create a svm Classifier\nclf = SVC(C=1, kernel='rbf')\n\n# Train the Classifier\/fitting the model\nclf.fit(X_train, y_train)\n\n# predict the response\ny_pred = clf.predict(X_test)\nacc_svm = round(clf.score(X_test,y_test) * 100, 2)\nsvm_time=(round(time() - t0, 3))\n# evaluate accuracy\nprint(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,\"%\")\nprint('Training time', svm_time)\n\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\nsns.heatmap(cm, annot=True,cmap=\"coolwarm_r\")","188cb9ca":"# visualizing accuracies for all ML Algorithms using Matplotlib\npredictors_group = ('Random Forest', 'GaussianNB', 'DecisionTree','kNN','SVM')\nx_pos = np.arange(len(predictors_group))\naccuracies1 = [acc_rf, acc_gnb, acc_dt,acc_knn, acc_svm]\n    \nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color='blue')\nplt.grid(False)\nplt.xticks(x_pos, predictors_group, rotation='vertical')\nplt.ylabel('Accuracy (%)')\nplt.title('Classifier Accuracies')\nplt.show()","2cfe805f":"#printing top three accuracies with training time\n\nprint('Decision Tree:', acc_dt,'%','with', dt_time,'s')\nprint('Random Forest:', acc_rf,'%','with', rf_time,'s')\nprint('GaussianNB:',acc_gnb,'%','with', gnb_time,'s')","2643950c":"# importing the model for prediction\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# creating list of tuple wth model and its name  \nmodels = []\nmodels.append(('DT',DecisionTreeClassifier()))\nmodels.append(('RF',RandomForestClassifier()))\nmodels.append(('GNB',GaussianNB()))","ca40d9f1":"# Import Cross Validation \nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3, random_state=42, shuffle=True)\nkf.get_n_splits(X)\n# print(kf)\n\nacc = []   # All Algorithm\/model accuracies\nnames = []    # All model name\n\nfor name, model in models:\n    \n    acc_of_model = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy') # kFolds =5 without shuffling\n    \n    acc.append(acc_of_model) # appending Accuray of different model to acc List\n    \n    names.append(name)# appending name of models\n    Acc =name,round(acc_of_model.mean()*100,2) # printing Output \n    print(Acc)","d83ba8b1":"# Plotting all accuracies together for comparison\n# using the values for accuracies from previous steps\n# these accuracies might vary slightly with each run. Hence,these need to be updated here before plotting\n\nlabels = ['Decision Tree', 'Random Forest','Gaussian NB']\nNoCV =[81.97, 83.61,86.89] # accuracy before Cross Validation\nCV=[70.25, 78.09, 80.16] # accuracy after Cross Validation \n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.25  # the width of the bars\n\nf, ax = plt.subplots(figsize=(10,8)) \np1 = ax.bar(x - width\/2, CV, width, label='After Cross Validation', color='#89c2d9')\np2 = ax.bar(x + width\/2, NoCV, width, label='Before Cross Validation', color='#ffadad')\n\n# Add some text for labels and title \nax.set_ylabel('Accuracies')\nax.set_title('Accuracy comparison')\nax.set_xticks(x)\nplt.xticks()\nax.set_xticklabels(labels)\nax.legend(loc='best')\nplt.show()","be124800":"#splitting dataframe to create a new dataframe & test algorithm for female group separately\n\n# on basis of column value for gender\n# using dataframe.groupby() function to create dataframe df_women\ndf_women = df_heart[df_heart['sex'] == 0]","4e2daf51":"df_women.info()","72c54ec2":"print (df_women)","57381458":"print('df_women:', df_women.shape)","c6196172":"#Defining X and y\nXw = df_women.drop(['target'], axis=1)\nyw = df_women['target']\n\n# creating dataset split for prediction\nfrom sklearn.model_selection import train_test_split\nXw_train, Xw_test , yw_train , yw_test = train_test_split(Xw,yw,test_size=0.2,random_state=42) # 80-20 split\n\n# Checking split \nprint('Xw_train:', Xw_train.shape)\nprint('yw_train:', yw_train.shape)\nprint('Xw_test:', Xw_test.shape)\nprint('yw_test:', yw_test.shape)","cc54df0e":"#Gaussian Naive Bayes Classifier\nt0 = time()\n#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n# Train the Classifier\/fitting the model\ngnb.fit(Xw_train, yw_train)\n\n# predict the response\nyw_pred = gnb.predict(Xw_test)\nacc_gnb = round(gnb.score(Xw_test,yw_test) * 100, 2)\ngnb_time=(round(time() - t0, 3))\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# evaluate accuracy\nprint(\"Gaussian Naive Bayes Accuracy:\",metrics.accuracy_score(yw_test, yw_pred)*100,\"%\")\nprint('Training time', gnb_time)\ncm = pd.DataFrame(confusion_matrix(yw_test, yw_pred))\nsns.heatmap(cm, annot=True, cmap=\"coolwarm_r\")","10b057ca":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Importing & Reading the dataset \ud83d\udcdd <\/centre><\/strong><\/h3>","44f70921":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>What does this dataset contain? <\/centre><\/strong><\/h3>","44f1e499":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Dataset split for prediction \u23f3 <\/centre><\/strong><\/h3>","4f9f33f3":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>More plots to understand the distribution of the data \ud83d\udd0d<\/centre><\/strong><\/h3>","5f71dae3":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Importing Libraries & Packages \ud83d\udcda <\/centre><\/strong><\/h3>","1f5ad67a":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Plotting correlation map for features <\/centre><\/strong><\/h3>","fcb9e2f7":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Visualizing Accuracies for all ML Algorithms \ud83d\udcca <\/centre><\/strong><\/h3>","fe6d1da4":"<blockquote>This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to\nthis date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0). <\/blockquote>","3778684a":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Cross Validation <\/centre><\/strong><\/h3>","07e18ea3":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Applying machine learning algorithms<\/centre><\/strong><\/h3>","12127c6c":"![Heart Disease Prediction(1).jpg](attachment:c21b8c5a-6518-4b4d-a19b-ba6f77803212.jpg)","def20e78":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Checking for missing values \u270f\ufe0f <\/centre><\/strong><\/h3>","bbcccb7a":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Plots to understand the distribution of the data \ud83d\udd0d<\/centre><\/strong><\/h3>","4082014d":"### <h3 style=\"background-color:#cad2c5;color:#800f2f;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;\"><strong><centre>Effect of different attributes on target outcome\u270f\ufe0f <\/centre><\/strong><\/h3>"}}