{"cell_type":{"ea43c226":"code","64632419":"code","2afc2cc2":"code","c75df0d1":"code","031b311e":"code","8afa4943":"code","fb1672f7":"code","9f711cb6":"code","11c41ac5":"code","ba881fbe":"code","3c7bc679":"code","dbaa3989":"code","e626f3a2":"code","5ab768ad":"code","bf92594c":"code","10c958e9":"code","19ef0c70":"code","8e4d8484":"code","e4c8fe6a":"code","b0ef71a8":"code","f66a8459":"code","0b561397":"code","70d0be94":"code","1d453850":"code","ede272df":"code","78987e36":"code","e91db6eb":"code","3c0a0302":"code","a71921be":"code","0bb97472":"code","b2ed32e0":"code","5c2c0a59":"code","31a6dd44":"markdown","306c1de7":"markdown","d3686059":"markdown","5da1599e":"markdown","e8c0a1b1":"markdown","8a906ab6":"markdown","28b7e478":"markdown","ce2f470e":"markdown","3226e667":"markdown","405fc7ab":"markdown","ef05a506":"markdown","6feb07f2":"markdown","9bead052":"markdown","955b5586":"markdown","f4cc059a":"markdown","18301303":"markdown","5bd03726":"markdown","b00221a2":"markdown","1f0c39e9":"markdown"},"source":{"ea43c226":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#For statistics\nfrom scipy import stats\n#For DataProcessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n#For Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\n\n#For Neural Network with TensorFlow\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom __future__ import absolute_import, division, print_function\n\nimport pathlib\nimport os\nprint(os.listdir(\"..\/input\"))\n\nprint('TensorFlow version is:',tf.__version__)\n\n# Any results you write to the current directory are saved as output.","64632419":"#Read the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n#print the dtypes in train to see how we can separate the quantitative from the qualitative\nprint(\"All dtypes in train are\",train.dtypes.unique())\n\n#separate the data into quantitative and qualitative\nquantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\ntarget = 'SalePrice'\nqualitative = [f for f in train.columns if train.dtypes[f] == 'object']\n\n#For categorical data, we assign a value 'MISSING' to it to make it a new category\nfor c in qualitative:\n    if train[c].isnull().any():\n        train[c] = train[c].fillna('MISSING')\n    if test[c].isnull().any():\n        test[c] = test[c].fillna('MISSING')\n\n\ndef rank_qual(frame, feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = frame[feature].unique()\n    ordering.index = ordering['val'].values\n    ordering.pop('val')\n    ordering['spmean'] = frame[[feature,'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values(by = 'spmean')\n    ordering['rank'] = range(1,ordering.shape[0]+1)\n    return ordering\n\ndef encode_rank(df,feature,ordering):\n    \"\"\"encode a categoraical feature based on rank\n        the encoded feature becomes a new column featire+'_E' added to df\n    df: DataFrame, the DataFrame with categorical feature \n    feature: str, the column name to encode\n    ordering: DataFrame, a dictionary in df form that maps categoraical values to ranks\n    \n    \"\"\"\n    ordering = ordering['rank'].to_dict()\n    for f_val, rank in ordering.items():\n        df.loc[df[feature] == f_val, feature+'_E'] = rank\n        \nqual_encoded=[]        \nfor feature in qualitative:\n    \n    ordering = rank_qual(train,feature)\n\n    encode_rank(train,feature,ordering)\n    encode_rank(test,feature,ordering)\n    qual_encoded.append(feature +'_E') \n    \n\n\n\n\n\n\n# def encode(frame, feature):\n#     ordering = pd.DataFrame()\n#     ordering['val'] = frame[feature].unique()\n#     ordering.index = ordering['val']\n#     ordering['spmean'] = frame[[feature,target]].groupby(feature).mean()[target]\n#     ordering = ordering.sort_values(by = 'spmean')\n#     ordering['rank'] = range(1,ordering.shape[0]+1)\n#     ordering = ordering['rank'].to_dict()\n#     for f_val, rank in ordering.items():\n#         frame.loc[frame[feature] == f_val, feature+'_E'] = rank\n# qual_encoded=[]\n# for f in qualitative:\n#     encode(train,f)\n#     qual_encoded.append(f+'_E') \n    \n#the error used by this competition is the root-mean-squared error of log\ndef error(actual, hat):\n    return np.sqrt(np.sum(np.square(np.log(actual)-np.log(hat)))\/len(actual))\n\n\n","2afc2cc2":"#Check the missing\ndef barplot_missing(df):\n    #barplot and list the total number and percentage\n    total = df.isnull().sum(axis=0)\n    percent = 100*total \/df.isnull().count()\n    missing = pd.concat([total, percent], axis =1, keys = ['Total','Percent(%)'])\n    missing = missing[missing['Total']>0]\n    missing = missing.sort_values(by='Total', ascending=False)\n    missing['Percent(%)'].plot.bar()\n    return missing\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nmissing = barplot_missing(train)\nprint('All missing data in training set are quantitative. Well done!' if(missing.index).all() in quantitative else 'Some qual_encoded in train is still missing. Check it!')\nplt.subplot(1,2,2)\nmissing = barplot_missing(test)\nprint('All missing data in testing set are quantitative. Well done!' if (missing.index).all() in quantitative else 'Some qual_encoded in test is still missing. Check it!')","c75df0d1":"#Check the qual_encoded columns in testing data\nmissing_qual_E = [ c for c in missing.index if c in qual_encoded]\nmissing.loc[missing_qual_E]","031b311e":"#Check MSZoning column to see if the unique values in training and testing datasets are the same\nprint('Unique values in training data for MSZoning:',train['MSZoning'].unique())\nprint('Unique values in test data for MSZoning:',test['MSZoning'].unique())\n#check to see if there's any NaN in train and test\nprint(train['MSZoning'].isnull().any(),test['MSZoning'].isnull().any())","8afa4943":"#Assign avg rank to each missing encoded feature\nmean = train[missing_qual_E].mean()\nfor c in missing_qual_E:\n    avg = mean[c]\n    test.loc[:,c]=test[c].fillna(avg)\n","fb1672f7":"ax1 = plt.subplot(1,2,1)\nmissing_num_train=barplot_missing(train)\nplt.ylabel('Percent(%)')\n\nplt.subplot(1,2,2,sharey=ax1)\nmissing_num_test= barplot_missing(test)\n","9f711cb6":"#check if any LotArea is zero\nprint(np.where(train['LotArea']==0))\nprint(np.where(test['LotArea']==0))\n\n#check the min value of LotArea, LotFrontage\nprint('The min of LotArea\/LotFrontage in train is:' ,train['LotArea'].min(),'\/', train['LotFrontage'].min(),'\\n',\n'The min of LotAreaLot\/Frontage in test is:',test['LotArea'].min(),'\/', test['LotFrontage'].min())\n#train.loc[np.where(train['LotArea']==0)[0],['LotArea','LotFrontage']]","11c41ac5":"def intersection(s1,s2):\n    return list(set(s1)&set(s2))\n\nLotArea_Positive = np.where(train['LotArea']>0)\nLotFrontage_missing = np.where(train['LotFrontage'].isnull())\n\ncondition = intersection(LotArea_Positive[0],LotFrontage_missing[0])\nprint('The number of rows that  LotArea>0 but LotFrontage is missing is:',len(condition) )\n#train.loc[condition, ['LotArea','LotFrontage']]","ba881fbe":"plt.figure(figsize=(15,5))\nax1 = plt.subplot(1,3,1)\nsns.scatterplot(train['LotFrontage'],train['SalePrice'])\n\nax2 =plt.subplot(1,3,2,sharey=ax1)\nsns.scatterplot(train['LotArea'],train['SalePrice'])\n\n\nax3= plt.subplot(1,3,3)\nsns.scatterplot(train['LotFrontage'],train['LotArea'])\n\nplt.figure(figsize=(15,5))\nax4 = plt.subplot(1,3,1)\nsns.scatterplot(np.log(train['LotFrontage']),np.log(train['SalePrice']))\n\nax5 =plt.subplot(1,3,2)\nsns.scatterplot(np.log(train['LotArea']),np.log(train['SalePrice']))\n\n\nax6= plt.subplot(1,3,3)\nsns.scatterplot(np.log(train['LotFrontage']),np.log(train['LotArea']))\n","3c7bc679":"fr = pd.concat([train['LotArea'], train['LotFrontage'],\n                train['SalePrice'],\n                train['LotConfig_E'],train['LotShape_E'],\n                train['LotConfig'], train['LotShape']],axis=1)\nfr['LotArea_log'] = np.log(train['LotArea'])\nfr['LotFrontage_log'] = np.log(train['LotFrontage'])\nfr['SalePrice_log'] = np.log(train['SalePrice'])\nnumeric = ['LotArea','LotFrontage','SalePrice','LotConfig_E','LotShape_E','LotArea_log','LotFrontage_log','SalePrice_log']\nfr[numeric].head()\ncorr = fr[numeric].corr()\nsns.heatmap(corr, cmap='RdBu_r',vmin=-1,vmax=1)\n","dbaa3989":"df1=train[['LotArea','LotFrontage','LotConfig','LotShape']].dropna()\ndf2=test[['LotArea','LotFrontage','LotConfig','LotShape']].dropna()\n\ndf1['LotArea_log'] = np.log(df1['LotArea'])\ndf2['LotArea_log'] = np.log(df2['LotArea'])\n\ndf1['LotFrontage_log'] = np.log(df1['LotFrontage'])\ndf2['LotFrontage_log'] = np.log(df2['LotFrontage'])\ndf = pd.concat(\n[df1,df2]\n,axis=0,ignore_index=True )\n\ndf.tail()","e626f3a2":"\nregr = linear_model.LinearRegression()\nX = df['LotArea_log'].values.reshape(-1,1)\nY = df['LotFrontage_log'].values.reshape(-1,1)\n# print(X.reshape(-1,1).shape)\n# print(Y)\nregr.fit(X,Y)\nprint('Coefficients:',regr.coef_)\nprint('R^2:',regr.score(X,Y))\n\nsns.lmplot(data=df,  x='LotFrontage_log', y='LotArea_log',\n           truncate=True,fit_reg=True)\n","5ab768ad":"\ndef fill_with_lm(regr,df,missing_feature, expl_vars,if_log_log=True):\n    \"\"\"\n    missing_feature: str, the name of the missing feature to fill with linear model\n    expl_vars: list of str, the \n    \"\"\"\n    #in training set\n    missing_rows = np.where(df[missing_feature].isnull())[0]\n    x = df.loc[missing_rows,expl_vars]\n    if if_log_log:\n        x = np.log(x).values\n\n    #assign the values inplace\n    try:\n        y = regr.predict(x.reshape(-1,1))\n        if if_log_log:\n            y=np.exp(y)\n        df.loc[missing_rows,missing_feature] = y\n        print('Missing data in',missing_feature,'is fileed with linear model.')\n    except:\n        print('No filling is done.')\n\n#fill the data with linear model\nfill_with_lm(regr,train,'LotFrontage',['LotArea'])\nfill_with_lm(regr,test,'LotFrontage',['LotArea'])\n\n\n#fill_with_lm(regr,train,'LotFrontage',['LotArea'])\n\n","bf92594c":"sns.distplot(train['GarageArea'])\nprint('training set has no missing GarageArea'\n      if np.array_equal(np.where(train['GarageYrBlt'].isnull()),  np.where(train['GarageArea']==0))\n     else 'training set has missing GarageArea')\nprint('testing set has no missing GarageArea'\n      if np.array_equal(np.where(test['GarageYrBlt'].isnull()),  np.where(test['GarageArea']==0))\n          else'training set has missing GarageArea')\nprint('In testing set:')\nyr_is_missing=np.where(test['GarageYrBlt'].isnull())\narea_is_zero = np.where(test['GarageArea']==0)\narea_is_not_zero = np.where(test['GarageArea']!=0)\n\nprint(len(yr_is_missing[0]),'rows have GarageYrBlt is NaN.')\nprint(len(area_is_zero[0]),'rows have GarageArea ==0.')\n","10c958e9":"#the index in testing data where they do have a garage but GarageYrBlt is unknown\nyr_unknown_index=[i for i  in yr_is_missing[0] if i in area_is_not_zero[0]]\nYr_avg = train['GarageYrBlt'].mean()\ntest.loc[yr_unknown_index, 'GarageYrBlt'] = Yr_avg\n\n#train['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)\n# test['GarageYrBlt'] = test['GarageYrBlt'].fillna(0)","19ef0c70":"print(train['MasVnrArea'].describe())\nplt.subplot(1,2,1)\nsns.distplot(train['MasVnrArea'].dropna(),kde=False)\nprint(test['MasVnrArea'].describe())\nplt.subplot(1,2,2)\nsns.distplot(test['MasVnrArea'].dropna(),kde=False)","8e4d8484":"_ = barplot_missing(train)\nplt.figure()\n_ = barplot_missing(test)","e4c8fe6a":"missing_quant = [c for c in quantitative if test[c].isnull().any()]\nhandled = ['LotFrontage','GarageYrBlt','MasVnrArea']\n#features in testing set that are still missing\nunhandled = [f for f in missing_quant if f not in handled]\nunhandled_mean = train[unhandled].mean()\nunhandled_mean\nfor f in unhandled:\n     test[f] = test[f].fillna(unhandled_mean[f])\n        \ntest[unhandled].isnull().sum()\n#we assign the average value to each of them ","b0ef71a8":"\n#Choose quantitative and encoded qualitative data for training and testing\nfeatures = quantitative + qual_encoded\ntrain_data = train[features]\ntest_data = test[features]\n\n\n","f66a8459":"_=barplot_missing(test_data)\nplt.figure()\n_=barplot_missing(train_data)\n","0b561397":"#fill zeros to all \ntrain_data= train_data.fillna(0)\ntest_data= test_data.fillna(0)","70d0be94":"#Normalize the data\ntrain_stats = train_data.describe()\ntrain_stats = train_stats.transpose()\n\n\n\ndef norm(x):\n  return (x - train_stats.loc[:,'mean']) \/ train_stats.loc[:,'std']\n\nnormed_train_data = norm(train_data)\nnormed_test_data = norm(test_data)    ","1d453850":"#check the normed data before training the model and giving predicition\nprint(normed_test_data.isnull().any().any(),\nnormed_train_data.isnull().any().any())","ede272df":"train_dataset = normed_train_data\ntrain_labels = train['SalePrice'] \ndef build_model():\n  model = keras.Sequential([\n    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation=tf.nn.relu),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mean_squared_logarithmic_error',\n                optimizer=optimizer,\n                metrics=['mean_squared_logarithmic_error'])\n  return model\n\n#buili the model\nmodel = build_model()\n\n\n\n# Display training progress by printing a single dot for each completed epoch\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nEPOCHS = 1500;\n\n#set earlystop when val_loss is not improving, patienece is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n\nhistory = model.fit(\n  normed_train_data.values, train_labels.values,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[\n              early_stop,\n              PrintDot()])\n\n","78987e36":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\n","e91db6eb":"def plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n  \n\n  \n  plt.figure(figsize=(12,8))\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error [$MPG^2$]')\n  plt.plot(hist['epoch'], hist['mean_squared_logarithmic_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_squared_logarithmic_error'],\n           label = 'Val Error')\n  plt.ylim([0,.1])\n  plt.legend()\n\n\nplot_history(history)\n\nplt.hlines(.028, 0, 1500,'b')\nplt.show()\nhist.tail()","3c0a0302":"train_predictions = model.predict(normed_train_data).flatten()\n\n\nplt.scatter(train_labels, train_predictions)\nplt.xlabel('True Values [SalePrice]')\nplt.ylabel('Predictions [SalePrice]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\nplt.plot([0, 10**6], [0, 10**6])\nplt.plot(train['SalePrice'].mean(),train['SalePrice'].mean(),'ko')","a71921be":"train_predictions = model.predict(normed_train_data).flatten()\nprint('the error on the training data is',error(train_predictions,train['SalePrice']))","0bb97472":"test_predictions = model.predict(normed_test_data).flatten()\nsns.distplot(test_predictions)\nsns.distplot(train['SalePrice'])","b2ed32e0":"print(np.isnan(test_predictions).any())\nlen(test_predictions)","5c2c0a59":"submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission['SalePrice'] = test_predictions\nsubmission.isnull().any().any()\nsubmission.to_csv(\"submission_1_030419.csv\",index=False)","31a6dd44":"# The three numeric features missing in the traning set has nonnegligible fraction, especially the LotFrontage. We should deal with these three more carefully than assigning zeros to them","306c1de7":"![](http:\/\/)### We notice significant linear realation between the log of LotArea and LotFrontage. This agrees with our knowledge of area = length**2\n### However to be more careful, let's exam all other variable with keywords 'Lot' to validate this simple assumption\n","d3686059":"## We use linear regression to fit LotFrontage_log. \n### We have the option to combine both training set and testing set.","5da1599e":"## We notice that numbers for the missing encoded data are really small. It's very likely they are missing qualitative data before encoding. But we have handled them with assigning 'MISSING' to the qualitative values! Let's find out what it is.","e8c0a1b1":"we see that only LotArea_log show significant linear relation with LotFrontage_log","8a906ab6":"## We have handled the categorical missing data and encoded them in both training and testing sets. But let's look at the bar plot of missing columns again before we move on!","28b7e478":"Handled the rest of the missing data in test\n","ce2f470e":"# 1.For LotFrontage that has ~15% missing data, we would like to see whether they're missing. We know it should be related to LotArea.\n\n## First we would like to see if a missing LotFrontage means no lot. We check if there's any row that has zero LotArea   ","3226e667":"We see that the coefficient is close  to .5\nWe can now assign values to the missing LotFrontage with the linear regression","405fc7ab":"## 2.For GarageYrBlt, we suspect the missing data mean there's no garage at all. We check this by plotting out the GarageArea distribution and also compare the rows where GarageArea=0 with rows  where GarageYrBlt = NaN.","ef05a506":"Now it's clear where these small fraction of missing data are from. They are complete in the training set, but are missed in the testing set. \n### In this case, we decide to set the encoded values to be the average for them.","6feb07f2":"## Our strategy is to approximate the LotFrontage with LotArea. Let's plot out their relations and also look at their relations with SalePrice.","9bead052":"# 3.For MasVnrArea, its distribution shows more than 50% of zeros. \n## We handle this with a more simple way by assinging it to the most probable value 0 because the loss fraction is very small.\n","955b5586":"# Build the model with tensorflow","f4cc059a":"## It seems every house has a parking lot. Therefore we assume the missing LotFrontage are missing data because their corresponding LotArea is not missing","18301303":"## Our suspection is mostly confrimed, but there are two anomalies. We identified two rows that have garages but their GarageYrBlt is unknown. We assume it is missing.\n\n## For these two data we decide to set their GarageYrBlt to the average value of the training set. Because they do have a Garage, but the data of year built is missing.\n\n## We set all others to 0 to account for the fact that it has no garage. This approach should be fine with neural network because the model can adapt to nonlinear behavior. Setting them to zero should amount to similar effect of setting another one-hot feature.\n\n# We will hande setting to zero altogether in the end","5bd03726":"# The minimal data preprocessing from my other kernel:","b00221a2":"Regression of LotFrontage with LotArea","1f0c39e9":"## To our surprise, although the traing set is properly encoded, the testing set does still have missing data in encoded columns!  Let's  see what they are."}}