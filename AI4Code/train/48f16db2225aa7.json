{"cell_type":{"9b416621":"code","a3b781d8":"code","df0999d6":"code","5bb5c5e0":"code","5e436b2c":"code","fba6f1a4":"code","1608edde":"code","c6e563de":"code","4d7adf6c":"code","d74ed033":"code","fbac88fb":"code","8162f3f3":"code","fe600562":"code","59dbec09":"code","7534d83a":"code","a8828769":"code","656d77a2":"code","b5c8a7e4":"code","098c39d3":"code","556ed4bd":"code","516d91db":"code","2b8eea75":"code","7354ca0f":"code","ba428675":"code","6dd3881d":"code","9aefd9f7":"code","849ff1ab":"code","9d0ea89e":"code","dad00f9e":"code","bdb3eba7":"code","be5ab1e7":"code","67c873c8":"code","b3a0b94c":"code","54728e79":"code","709396ed":"code","793bedad":"code","31d0b2ad":"code","46a9a176":"code","9cf07af3":"code","ca9e6322":"code","3a52804b":"code","40daa17f":"code","fa73cc90":"code","204e67b7":"code","6ce3cf9b":"code","64495e8e":"code","cf3934a4":"code","db1ea4d4":"code","894f8a61":"code","4585f9cc":"code","78d49b6f":"code","0f381bc0":"code","663430c3":"code","b38a9a32":"code","cc1e9ca8":"code","aa7daaff":"code","d26e01e6":"code","d4094da4":"code","fcb96cec":"markdown","caf7ea76":"markdown","ee5ae7e9":"markdown","e454ea44":"markdown","a48c50c3":"markdown","50e23c42":"markdown","ddc6379c":"markdown","0c956428":"markdown","f8cee516":"markdown","0591f227":"markdown","03f42cb3":"markdown","2389e8fe":"markdown","2ffc45cb":"markdown","bd819983":"markdown","69c49413":"markdown","f0021d27":"markdown","132a5dbf":"markdown","e53ccaac":"markdown","fb4c511f":"markdown","e01fab0a":"markdown","c0f02dbc":"markdown","cf6f6f5e":"markdown","f14bb368":"markdown","8d4722f9":"markdown","57a23b82":"markdown","656ef708":"markdown","f62e6116":"markdown","0963c78b":"markdown","cbf7727f":"markdown","762eacd0":"markdown","48556d7c":"markdown","3f70af09":"markdown","d76e44d7":"markdown"},"source":{"9b416621":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(palette=sns.color_palette('Set2',9))\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, StratifiedKFold, GridSearchCV,RandomizedSearchCV\n\nfrom xgboost import XGBRFClassifier,XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\n\nfrom scipy.stats import uniform,randint","a3b781d8":"# !pip install seaborn --upgrade\n# import os\n# os._exit(00)","df0999d6":"print(sns.__version__)","5bb5c5e0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5e436b2c":"train_path='\/kaggle\/input\/titanic\/train.csv'\ntest_path='\/kaggle\/input\/titanic\/test.csv'\ntitanic_train=pd.read_csv(train_path)\ntitanic_test=pd.read_csv(test_path)","fba6f1a4":"titanic_train.head(10)","1608edde":"titanic_test.head()","c6e563de":"titanic_train.info()","4d7adf6c":"titanic_test.info()","d74ed033":"def plot_mutihist(num_row,num_col,hist_columns):\n\n    f,axs=plt.subplots(num_row,num_col,squeeze=True,figsize=(20,10))\n    for i in range(len(hist_columns)%num_col,num_col):\n        axs[num_row-1,i].remove()\n    for i in range(len(hist_columns)):\n        sns.histplot(data=titanic_train,x=hist_columns[i], ax=axs[i\/\/num_col,i%num_col])\n    f.tight_layout()","fbac88fb":"titanic_hist_columns=['Pclass','Sex','Age','SibSp','Parch','Fare']\nplot_mutihist(num_row=2,num_col=4,hist_columns=titanic_hist_columns)","8162f3f3":"def plot_countplot_hue(num_row,num_col,count_columns):\n\n    f,axs=plt.subplots(num_row,num_col,squeeze=True,figsize=(15,10))\n    for i in range(len(count_columns)%num_col,num_col):\n        axs[num_row-1,i].remove()\n    for i in range(len(count_columns)):\n        sns.countplot(data=titanic_train,x=count_columns[i],hue='Survived',ax=axs[i\/\/num_col,i%num_col])","fe600562":"titanic_count_columns=['Pclass','Sex','SibSp','Parch']\nplot_countplot_hue(num_row=2,num_col=4,count_columns=titanic_count_columns)","59dbec09":"titanic_heatmap_columns=['Survived','Age','SibSp','Parch','Fare','Pclass']\ncmap = sns.diverging_palette(240, 10, as_cmap=True)\nsns.heatmap(titanic_train[titanic_heatmap_columns].corr(), cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) ","7534d83a":"sns.catplot(x='Sex',y='Survived',kind='bar',hue='Pclass',data=titanic_train)","a8828769":"f,axs=plt.subplots(1,4,squeeze=True,figsize=(20,5))\nsns.barplot(x='SibSp',y='Survived',data=titanic_train,ax=axs[0])\nsns.barplot(x='Parch',y='Survived',data=titanic_train,ax=axs[1])\n\ntitanic_train['Family']=titanic_train['SibSp']+titanic_train['Parch']+1\nsns.barplot(x='Family',y='Survived',data=titanic_train,ax=axs[2])\ndef groupfamily(x):\n    if x==1:\n        return 1\n    elif (2<=x)&(x<=4):\n        return 2\n    elif (5<=x)&(x<=6):\n        return 3\n    else:\n        return 4\ntitanic_train['Family_group']=titanic_train['Family'].apply(groupfamily)\n\nsns.barplot(x='Family_group',y='Survived',data=titanic_train,ax=axs[3])","656d77a2":"AGE_BINS=[-1,5,17,20,24,28,32,40,48,100]\ntitanic_train['Age_cat']=pd.cut(titanic_train['Age'],AGE_BINS,labels=[0,1,2,3,4,5,6,7,8])\n\nf,axs=plt.subplots(1,3,squeeze=True,figsize=(22,7))\n\nsns.histplot(data=titanic_train, x=\"Age\", bins='auto',kde=True, hue=\"Survived\",ax=axs[0])\naxs[0].plot(AGE_BINS, np.ones(len(AGE_BINS)) ,'rv')\n\nsns.barplot(x='Age_cat',y='Survived',data=titanic_train,ax=axs[1])\n\nsns.countplot(data=titanic_train, x=\"Age_cat\", hue=\"Survived\",ax=axs[2])","b5c8a7e4":"FARE_BINS=[-1,7.35, 7.82, 8, 10, 13, 23,30, 45, 80, 150, 1000]\ntitanic_train['Fare_cat']=pd.cut(titanic_train['Fare'],bins=FARE_BINS,labels=[0,1,2,3,4,5,6,7,8,9,10])\n\nf,axs=plt.subplots(1,3,squeeze=True,figsize=(22,7))\n\nsns.histplot(data=titanic_train, x=\"Fare\",bins='auto',kde=True, hue=\"Survived\",ax=axs[0])\naxs[0].plot(FARE_BINS,np.ones(len(FARE_BINS)),'rv')\naxs[0].set_ylim(0,70)\naxs[0].set_xlim(0,500)\n\nsns.barplot(x='Fare_cat',y='Survived',data=titanic_train,ax=axs[1])\n\nsns.countplot(data=titanic_train, x=\"Fare_cat\", hue=\"Survived\",ax=axs[2])","098c39d3":"sns.countplot(data=titanic_train, x=\"Embarked\", hue=\"Survived\")","556ed4bd":"titanic_train['Cabin_t']=titanic_train['Cabin'].str.get(i=0)\ntitanic_train['Cabin_t'].fillna('X',inplace=True)\nf,(ax1,ax2)=plt.subplots(1,2,figsize=(12,5))\nsns.countplot(data=titanic_train, x=\"Cabin_t\", hue=\"Survived\",ax=ax1)\nsns.countplot(data=titanic_train, x=\"Cabin_t\", hue=\"Pclass\",ax=ax2)\nax1.set_ylim(0,50)\nax2.set_ylim(0,50)","516d91db":"titanic_train['Cabin_t'] = titanic_train['Cabin_t'].replace(['A', 'B', 'C','T'], 'ABCT')\ntitanic_train['Cabin_t'] = titanic_train['Cabin_t'].replace(['D', 'E'], 'DE')\ntitanic_train['Cabin_t'] = titanic_train['Cabin_t'].replace(['F', 'G'], 'FG')","2b8eea75":"f,ax1=plt.subplots(figsize=(12,5))\nsns.countplot(data=titanic_train, x=\"Cabin_t\", hue=\"Pclass\",ax=ax1)\nax1.set_ylim(0,200)","7354ca0f":"titanic_train['Name_m']=titanic_train['Name'].str.split(pat=', ',n=1,expand=True)[1].str.split(pat='.',n=1,expand=True)[0]\n\n\ndef Name_transform(x):\n    if x=='Mr':\n        return 'Mr'\n    elif x=='Mrs':\n        return 'Mrs'\n    elif x=='Miss':\n        return 'Miss'\n    else:\n        return 'etc'\ntitanic_train['Name_M']=titanic_train['Name_m'].apply(Name_transform)","ba428675":"f,ax1=plt.subplots(1,1,squeeze=True,figsize=(10,5))\nsns.countplot(data=titanic_train, x=\"Name_M\", hue=\"Survived\",ax=ax1)","6dd3881d":"titanic_train['Ticket_Freq']=titanic_train.groupby('Ticket')['Ticket'].transform('count')","9aefd9f7":"titanic_train['Ticket_Freq']\nsns.countplot(data=titanic_train, x=\"Ticket_Freq\", hue=\"Survived\")","849ff1ab":"titanic_train=pd.read_csv(train_path)\ntitanic_test=pd.read_csv(test_path)\ntarget=titanic_train['Survived']\nId=titanic_test[['PassengerId']]","9d0ea89e":"class NullTransformer(BaseEstimator,TransformerMixin):\n\n    def fit(self,df,y=None):\n        return self\n    def transform(self,df):\n        \n    ##fill Embarked with most frequently variable 'S'    \n        df['Embarked'].fillna('S',inplace=True)  \n        \n    ##fill Cabin with 'X'\n        df['Cabin'].fillna('X',inplace=True)     \n        \n    ##fill Fare with median value of same Pclass\n        missing_Fare_index=list(df['Fare'][df['Fare'].isnull()].index)     \n        \n        for index in missing_Fare_index:\n            if df['Pclass'][index]==1:\n                df['Fare'].iloc[index]=df.groupby('Pclass')['Fare'].median()[1]\n            elif df['Pclass'][index]==2:\n                df['Fare'].iloc[index]=df.groupby('Pclass')['Fare'].median()[2]\n            elif df['Pclass'][index]==3:\n                df['Fare'].iloc[index]=df.groupby('Pclass')['Fare'].median()[3]\n            \n    ##fill Age with median value of same Pclass    \n        index_NaN_age = list(df[\"Age\"][df[\"Age\"].isnull()].index)             \n\n        for index in index_NaN_age :\n            if df['Pclass'][index]==1:\n                df['Age'].iloc[index]=df.groupby('Pclass')['Age'].median()[1]\n            elif df['Pclass'][index]==2:\n                df['Age'].iloc[index]=df.groupby('Pclass')['Age'].median()[2]\n            elif df['Pclass'][index]==3:\n                df['Age'].iloc[index]=df.groupby('Pclass')['Age'].median()[3]\n                \n        return df","dad00f9e":"class FeatureExtraction(BaseEstimator,TransformerMixin):\n\n    def fit(self,df,y=None):\n        return self\n    def transform(self,df):\n        \n    ## create Family Features   \n        df['Family']=df['SibSp']+df['Parch']+1  \n        def groupfamily(x):\n            if x==1:\n                return 1\n            elif (2<=x)&(x<=4):\n                return 2\n            elif (5<=x)&(x<=6):\n                return 3\n            else:\n                return 4\n        df['Family']=df['Family'].apply(groupfamily)\n        \n        df.drop(['SibSp','Parch','PassengerId'],inplace=True,axis=1)\n        if 'Survived' in list(df.keys()):\n            df.drop(['Survived'],inplace=True,axis=1)\n                     \n    ## extract Ticket Frequency and prefix\n        df['Ticket_freq']=df.groupby('Ticket')['Ticket'].transform('count')\n        df.drop(['Ticket'],inplace=True,axis=1)\n        \n    ## extract Cabin prefix\n        df['Cabin']=df['Cabin'].str.get(i=0)                            \n        df['Cabin'] = df['Cabin'].replace(['A', 'B', 'C','T'], 'ABCT')\n        df['Cabin'] = df['Cabin'].replace(['D', 'E'], 'DE')\n        df['Cabin'] = df['Cabin'].replace(['F', 'G'], 'FG')\n        \n    ## extract Name title\n        df['Name']=df['Name'].str.split(pat=', ',n=1,expand=True)[1].str.split(pat='.',n=1,expand=True)[0]      \n        def Name_transform(x):\n            if x=='Mr':\n                return 'Mr'\n            elif x=='Mrs':\n                return 'Mrs'\n            elif x=='Miss':\n                return 'Miss'\n            else:\n                return 'etc'\n        df['Name']=df['Name'].apply(Name_transform)     \n        \n     ## Binning Age, Fare\n        AGE_BINS=[-1,5,17,20,24,28,32,40,48,100]\n        df['Age_cat']=pd.cut(df['Age'],AGE_BINS,labels=[0.,1.,2.,3.,4.,5.,6.,7.,8.])\n        \n        FARE_BINS=[-1,7.35, 7.82, 8, 10, 13, 23,30, 45, 80, 150, 1000]\n        df['Fare_cat']=pd.cut(df['Fare'],bins=FARE_BINS,labels=[0.,1.,2.,3.,4.,5.,6.,7.,8.,9.,10.])\n        df.drop(['Age','Fare'],inplace=True,axis=1)\n        \n    ## Fare to log scale \n       # df[\"Fare\"] = df[\"Fare\"].map(lambda x: np.log(x) if x>0 else 0)\n    \n        return df","bdb3eba7":"attribs=['Pclass','Name','Sex','Age','Ticket','Fare','Cabin','Embarked','SibSp','Parch']\nnum_attribs=['Pclass','Age_cat','Fare_cat',\"Ticket_freq\"]\ncat_attribs=['Name','Sex','Cabin','Embarked','Family']\n\npipeline1=Pipeline([\n        ('NT',NullTransformer()),\n        ('FE',FeatureExtraction())\n])\n\ntrain=pipeline1.fit_transform(titanic_train)\ntest=pipeline1.transform(titanic_test)\n\ntrain=pd.get_dummies(train,columns=cat_attribs)\ntest=pd.get_dummies(test,columns=cat_attribs)\n","be5ab1e7":"train=train.to_numpy()\ntest=test.to_numpy()","67c873c8":"kfolds=StratifiedKFold(n_splits=3)    \n\nscores=[]\n\n##Base models\nsvc=SVC(random_state=42)\nrdf=RandomForestClassifier(random_state=42)\nada=AdaBoostClassifier(random_state=42)\nlog=LogisticRegression(random_state=42)\ngrb=GradientBoostingClassifier(random_state=42)\ndct=DecisionTreeClassifier(random_state=42)\next=ExtraTreesClassifier(random_state=42)\nxgb=XGBRFClassifier(random_state=42)\nxgbc=XGBClassifier(random_state=42)\n\nmodels=[svc,rdf,ada,log,grb,dct,ext,xgb,xgbc]\n\nfor model in models:\n    scores.append(cross_val_score(model,train,target,cv=kfolds,n_jobs=-1))\n    scores_mean=np.mean(scores,axis=1)","b3a0b94c":"columns_name=[]\nfor model in models:\n    columns_name.append(model.__class__.__name__)\n\nscores_df=pd.DataFrame(np.array(scores).T,columns=columns_name)\nscores_mean_tosort=pd.Series(np.array(scores_mean).T,index=columns_name)\nscores_mean_sorted=scores_mean_tosort.sort_values(ascending=False)\n\nf, ax1=plt.subplots(figsize=(10,5))\nsns.barplot(data=scores_df,orient='h',order=list(scores_mean_sorted.index))","54728e79":"scores_mean_sorted","709396ed":"def grid_search(estimator,param,X,y):\n\n    grid=GridSearchCV(estimator, param, n_jobs=-1, cv=kfolds, return_train_score=True)\n    grid.fit(X,y)\n\n    grid_results=grid.cv_results_\n    #for mean_score,param in zip(grid_results['mean_test_score'],grid_results['params']):\n        #print(mean_score,param)\n    \n    print('\\nBestParams and Score : \\n',grid.best_params_,'\\n',grid.best_score_)\n    \n    return grid.best_estimator_ , grid, estimator","793bedad":"def random_grid_search(estimator,param_d,X,y,n):\n\n    grid=RandomizedSearchCV(estimator, param_d, n_jobs=-1, cv=kfolds, return_train_score=True, n_iter=n)\n    grid.fit(X,y)\n\n    grid_results=grid.cv_results_\n    #for mean_score,param in zip(grid_results['mean_test_score'],grid_results['params']):\n        #print(mean_score,param)\n    \n    print('\\nBestParams and Score : \\n',grid.best_params_,'\\n',grid.best_score_)\n    \n    return grid.best_estimator_ , grid, estimator","31d0b2ad":"param_grid = [\n    {'C':[0.6, 0.8,1,1.15,1.2,1.23,1.4], 'kernel':['rbf'], 'gamma':[0.1] }   \n]\nsvc_best, svc_grid, svc=grid_search(SVC(random_state=42, probability=True),param_grid,X=train,y=target)","46a9a176":"param_grid = [\n    {'C': uniform(0.9,0.3), 'kernel':['rbf'], 'gamma':[0.1] }   \n]\nsvc_best, svc_grid, svc =random_grid_search(SVC(random_state=42, probability=True),param_grid,X=train,y=target,n=20)","9cf07af3":"param_grid = [\n    {'learning_rate':[0.1], 'n_estimators':[30,50,100,200,240], 'max_depth':[2,3,4] }   \n]\ngbc_best, gbc_grid, gbc =grid_search(GradientBoostingClassifier(random_state=42),param_grid,X=train,y=target)","ca9e6322":"param_grid = [\n    {'learning_rate':[0.1], 'n_estimators': randint(80,110), 'max_depth':[3] }   \n]\ngbc_best, gbc_grid, gbc =random_grid_search(GradientBoostingClassifier(random_state=42),param_grid,X=train,y=target,n=20)","3a52804b":"param_grid = [\n    {'penalty':['l1','l2'],'C':[0.1,0.8,0.9,1,1.1,1.2,1.3,5], 'tol':[1e-4,1e-3],'solver':['liblinear']}   \n]\nlog_best, log_grid, log =grid_search(LogisticRegression(random_state=42),param_grid,X=train,y=target)","40daa17f":"param_grid = [\n    {'penalty':['l1'],'C': uniform(0.7,0.3), 'tol':[1e-4],'solver':['liblinear']}   \n]\nlog_best, log_grid, log =random_grid_search(LogisticRegression(random_state=42),param_grid,X=train,y=target,n=20)","fa73cc90":"param_grid = [\n    {'n_estimators':[30,50,70,100,200,300],'max_depth':[2,3,4,5], 'gamma':[0.1]}   \n]\nxgb_best, xgb_grid, xgb=grid_search(XGBRFClassifier(random_state=42),param_grid,X=train,y=target)","204e67b7":"param_grid = [\n    {'n_estimators':randint(20,60),'max_depth':[4], 'gamma':[0.1]}   \n]\nxgb_best, xgb_grid, xgb =random_grid_search(XGBRFClassifier(random_state=42),param_grid,X=train,y=target,n=20)","6ce3cf9b":"param_grid = [\n    {'n_estimators':[30,50,100,200],'max_depth':[2,3,4], 'gamma':[0.1]}   \n]\nxgbc_best, xgbc_grid, xgbc =grid_search(XGBClassifier(random_state=42),param_grid,X=train,y=target)","64495e8e":"param_grid = [\n    {'n_estimators': randint(70,120),'max_depth':[3], 'gamma':[0.1]}   \n]\nxgbc_best, xgbc_grid, xgbc =random_grid_search(XGBClassifier(random_state=42),param_grid,X=train,y=target,n=20)","cf3934a4":"param_grid = [\n    {'n_estimators': [30,50,100,200,300],'learning_rate':[0.9,1.0,1.1,1.2,1.3]}   \n]\nada_best, ada_grid, ada =grid_search(AdaBoostClassifier(random_state=42),param_grid,X=train,y=target)","db1ea4d4":"param_grid = [\n    {'n_estimators': randint(20,50),'learning_rate':[1.3]}   \n]\nada_best, ada_grid, ada =random_grid_search(AdaBoostClassifier(random_state=42),param_grid,X=train,y=target,n=20)","894f8a61":"param_grid = [\n    {'n_estimators': [30,50,100,200,300],'max_depth':[2,3,4,5], 'criterion':['gini','entropy']}   \n]\nrdf_best, rdf_grid, rdf =grid_search(RandomForestClassifier(random_state=42),param_grid,X=train,y=target)","4585f9cc":"param_grid = [\n    {'n_estimators': randint(200,400),'max_depth':[5], 'criterion':['gini']}   \n]\nrdf_best, rdf_grid, rdf =random_grid_search(RandomForestClassifier(random_state=42),param_grid,X=train,y=target,n=20)","78d49b6f":"param_grid = [\n    {'n_estimators': [30,50,100,200,300],'max_depth':[2,3,4,5], 'criterion':['gini','entropy']}   \n]\next_best, ext_grid, ext =grid_search(ExtraTreesClassifier(random_state=42),param_grid,X=train,y=target)","0f381bc0":"param_grid = [\n    {'n_estimators': randint(80,150),'max_depth':[5], 'criterion':['gini']}   \n]\next_best, ext_grid, ext =random_grid_search(ExtraTreesClassifier(random_state=42),param_grid,X=train,y=target,n=20)","663430c3":"models=[\n        ('svc', svc_best),\n        ('log', log_best), \n        ('xgb',xgb_best),\n        ('gbc', gbc_best),\n        ('xgbc', xgbc_best),\n        ('ada', xgbc_best),\n        ('rdf',rdf_best),\n        ('ext',ext_best)\n    \n]\n\nvot_hard = VotingClassifier(estimators=models, voting='hard', n_jobs=-1)\n\nvot_hard.fit(train,target)\n\nvot_soft = VotingClassifier(estimators=models, voting='soft', n_jobs=-1)\n\nvot_soft.fit(train,target)\n\nstack=StackingClassifier(estimators=models,cv=kfolds,n_jobs=-1,stack_method='predict_proba')\n\nstack.fit(train,target)\n\nprediction_hard=vot_hard.predict(test)\nprediction_soft=vot_soft.predict(test)\nprediction_stack=stack.predict(test)\n","b38a9a32":"kfolds=StratifiedKFold(n_splits=3)    \n\nscores=[]\n\nmodels=[vot_hard,vot_soft,stack]\n\nfor model in models:\n    scores.append(cross_val_score(model,train,target,cv=kfolds))\n    scores_mean=np.mean(scores,axis=1)","cc1e9ca8":"scores_mean","aa7daaff":"Survived_hard = pd.Series(prediction_hard, name=\"Survived\")\nresult_hard = pd.concat([Id,Survived_hard],axis=1)\nresult_hard.to_csv(\"submission_hard.csv\",index=False)\n\nSurvived_soft = pd.Series(prediction_soft, name=\"Survived\")\nresult_soft = pd.concat([Id,Survived_soft],axis=1)\nresult_soft.to_csv(\"submission_soft.csv\",index=False)\n\nSurvived_stack = pd.Series(prediction_stack, name=\"Survived\")\nresult_stack = pd.concat([Id,Survived_stack],axis=1)\nresult_stack.to_csv(\"submission_stack.csv\",index=False)","d26e01e6":"from sklearn.decomposition import PCA\n\n\npca=PCA(n_components=0.879)\ntrain_pca=pca.fit_transform(train,target)\ntest_pca=pca.transform(test)\n\nsvc_pca=SVC(C=1.0, kernel='rbf', gamma=0.1,  probability=True)\n\n\n\n\ncv_pca=cross_val_score(svc_pca,train_pca,target,n_jobs=-1, cv=kfolds)\nsvc_pca.fit(train_pca,target)\nprediction_pca=svc_pca.predict(test_pca)\n\nSurvived_pca = pd.Series(prediction_pca, name=\"Survived\")\nresult_pca = pd.concat([Id,Survived_pca],axis=1)\nresult_pca.to_csv(\"submission_pca.csv\",index=False)\n\n\"\"\"\ntrain\ntest\ntarget\n=SVC()\nparam_grid_pca=[\n    {\n        'kpca':['linear','poly','rbf']\n    }\n]\n\"\"\"","d4094da4":"cv_pca","fcb96cec":"## 3.1 Data skimming","caf7ea76":"### 3.3.1 Pclass, Sex","ee5ae7e9":"### RandomForest","e454ea44":"## 5.2 Hyperparameter Tuning","a48c50c3":"## 3.2 Feature analysis","50e23c42":"### GradientBoosting","ddc6379c":"## 5.3 Ensemble & Prediction","0c956428":"A,B,C,T are all 1st class\n\/ D,E are 80% 1st class\n\/ F,G are 2nd\/3rd class","f8cee516":"split family by 1\/2,3,4\/5,6\/7,8,11","0591f227":"### ExtraTree","03f42cb3":"### 3.3.5 Cabin","2389e8fe":"# 1. Import Modules","2ffc45cb":"### 3.3.2 SibSp, Parch","bd819983":"### 3.3.3 Age, Fare","69c49413":"# 5.Modeling","f0021d27":"One-hot-encoding features","132a5dbf":"Binning Age","e53ccaac":"# 4.Feature Engineering","fb4c511f":"### SVC","e01fab0a":"### 3.3.7 Ticket","c0f02dbc":"# 2. Load Data & Scanning","cf6f6f5e":"### LogisticRegression","f14bb368":"### XGRFBoost","8d4722f9":"## 5.1 Model evaluation","57a23b82":"Hey let me introduce my own notebook. I'm beginner in Kaggle and machine-learning\n\n> I got the score 0.78 at public LB with ths kernel\nand used ensemble & stacking method for prediction,\nalso used full pipeline to extract and change features\n\nbest regards.","656ef708":"Binning Fare","f62e6116":"One-hot-encoding","0963c78b":"# 3. Exploratory Data analysis","cbf7727f":"### XGBoost","762eacd0":"### 3.3.4 Embarked","48556d7c":"### ADABoost","3f70af09":"People who bought same tickets might be family\/friends\/etc","d76e44d7":"### 3.3.6 Name"}}