{"cell_type":{"1377144c":"code","8723816b":"code","e34d94f3":"code","af0f0179":"code","5e39ab48":"code","fd2c8974":"code","b47126c1":"code","b424030a":"code","2cb5d39f":"code","58cae73d":"code","4d5088ef":"code","9a633283":"code","e8438e26":"code","ff9ad9da":"code","a37e93b7":"code","32ca20ee":"code","19929bd9":"code","9e6cdbd1":"code","06d5b71e":"code","d090cf19":"code","c344f1e9":"code","43e39002":"code","c3231190":"code","5ac70f4b":"code","5d696af5":"code","1e4c2224":"code","429d826b":"code","0d9a0d65":"code","ad1277b0":"code","7f2a9db3":"code","bc6ce4eb":"code","185fe70b":"code","1c443ce7":"code","915feb8b":"code","06048506":"code","72b33eb6":"code","28d46898":"code","3b9e1058":"code","a4489303":"code","5e61712b":"code","465c4297":"code","b4e048cf":"code","73b10af5":"code","b4a7d291":"code","77661af7":"code","1e25d1cb":"code","29ca0ed8":"code","082e0c04":"code","595ba51d":"code","a8db1689":"code","62bf0a83":"markdown","61fa6829":"markdown","541617ad":"markdown","855ba7c3":"markdown","d2995e0e":"markdown","1baff2de":"markdown","5f4c5f13":"markdown","add656c0":"markdown","7143e395":"markdown","5fa09da5":"markdown","94f5bd22":"markdown","5d1c15ea":"markdown","03e27286":"markdown","b4839014":"markdown","fda8a848":"markdown","a8ce504d":"markdown","eb1fcea3":"markdown","6614aa90":"markdown","a79c4fa1":"markdown","42101121":"markdown","2dadc506":"markdown","f14c4cba":"markdown","87a3b810":"markdown"},"source":{"1377144c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score","8723816b":"data = pd.read_csv('..\/input\/data.csv')","e34d94f3":"data=data.drop('Unnamed: 32',axis=1)","af0f0179":"data.head()","5e39ab48":"data.apply(lambda x: sum(x.isnull()))","fd2c8974":"print(data.shape[0])\ndata.apply(lambda x : len(x.unique()))","b47126c1":"plt.figure(1)\ndata['diagnosis'].value_counts(normalize=True).plot.bar( title= 'dependant variable')","b424030a":"def dist(variable):\n    plt.subplot(222)\n    ax1=plt.subplot(221)\n    sns.distplot(data[variable]);\n    ax2=plt.subplot(222)\n    sns.distplot(np.log1p(data[variable]));\n    ax2=plt.subplot(223)\n    mms = MinMaxScaler()\n    sns.distplot(mms.fit_transform(data[variable].values.reshape(-1,1)))","2cb5d39f":"fig, ax = plt.subplots(figsize=(20,20)) \nsns.heatmap(data.corr(),cmap=sns.diverging_palette(220, 20, as_cmap=True))","58cae73d":"data = data.drop('radius_mean',axis=1)\ndata = data.drop('perimeter_mean',axis=1)\ndata = data.drop('area_mean',axis=1)\ndata = data.drop('perimeter_worst',axis=1)\ndata = data.drop('area_worst',axis=1)\ndata = data.drop('radius_se',axis=1)\ndata = data.drop('perimeter_se',axis=1)","4d5088ef":"dist('smoothness_mean')","9a633283":"dist('texture_mean')","e8438e26":"dist('compactness_mean')","ff9ad9da":"dist('concavity_mean')","a37e93b7":"dist('concave points_mean')","32ca20ee":"dist('symmetry_mean')","19929bd9":"dist('fractal_dimension_mean')","9e6cdbd1":"dist('texture_se')","06d5b71e":"dist('area_se')","d090cf19":"dist('smoothness_se')","c344f1e9":"dist('compactness_se')","43e39002":"dist('concavity_se')","c3231190":"dist('concave points_se')","5ac70f4b":"dist('symmetry_se')","5d696af5":"dist('fractal_dimension_se')","1e4c2224":"dist('radius_worst')","429d826b":"dist('texture_worst')","0d9a0d65":"dist('smoothness_worst')","ad1277b0":"dist('compactness_worst')","7f2a9db3":"dist('concavity_worst')","bc6ce4eb":"dist('concave points_worst')","185fe70b":"dist('symmetry_worst')","1c443ce7":"dist('fractal_dimension_worst')","915feb8b":"data_outliers_removed = data.copy()","06048506":"ax = sns.boxplot(y=\"texture_mean\",  data=data_outliers_removed, linewidth=2.5)\ndescription = data_outliers_removed.texture_mean.describe()\nQ1 = description[4]\nQ3 = description[6]\noutliers_low = Q1 - (1.5 * (Q3-Q1))\noutliers_high = Q3 + (1.5 * (Q3-Q1))\nprint(outliers_low,outliers_high)","72b33eb6":"numerical = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","28d46898":"for i in numerical:\n    if i in data.columns:\n        print (i + ' : ' + str(data_outliers_removed[i].mean()))","3b9e1058":"for i in numerical:\n    if i in data.columns:\n        description = data_outliers_removed[i].describe()\n        Q1 = description[4]\n        Q3 = description[6]\n        outliers_low = Q1 - (1.5 * (Q3-Q1))\n        outliers_high = Q3 + (1.5 * (Q3-Q1))\n        median = data_outliers_removed[i].median()\n        temp_high = data_outliers_removed[i]>outliers_high\n        temp_low = data_outliers_removed[i]>outliers_low\n        data_outliers_removed.loc[temp_high == True,i]= median\n        data_outliers_removed.loc[temp_low == True,i]= median","a4489303":"for i in numerical:\n    if i in data.columns:\n        print (i + ' : ' + str(data_outliers_removed[i].mean()))","5e61712b":"for i in numerical:\n    if i in data.columns:\n        data_outliers_removed[i] = np.log1p(data_outliers_removed[i])\n        data[i] = np.log1p(data[i])","465c4297":"data = data.drop('id',axis=1)\ndata_outliers_removed = data_outliers_removed.drop('id',axis=1)","b4e048cf":"def prediction(x,y,regressor):\n    le = LabelEncoder()\n    y=le.fit_transform(y)\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33)\n    regressor.fit(x_train,y_train)\n    y_pred = regressor.predict(x_test)\n    return accuracy_score(y_test,y_pred)    ","73b10af5":"LR = LogisticRegression()","b4a7d291":"prediction(data_outliers_removed.drop('diagnosis',axis=1),data['diagnosis'],LR)","77661af7":"prediction(data.drop('diagnosis',axis=1),data['diagnosis'],LR)","1e25d1cb":"le = LabelEncoder()\nx = data.drop('diagnosis',axis=1)\ny = le.fit_transform(data['diagnosis'])\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.33)\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n                       scoring='%s_macro' % score)\n    clf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n\n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()\n    #print(accuracy_score(y_pred = y_pred, y_true = y_test),precision_score(y_pred = y_pred, y_true = y_test),recall_score(y_pred = y_pred, y_true = y_test))\n\n# Note the problem is too easy: the hyperparameter plateau is too flat and the\n# output model is the same for precision and recall with ties in quality","29ca0ed8":"x = data.drop('diagnosis',axis=1)\ny = le.fit_transform(data['diagnosis'])\nclf = SVC(kernel='linear', C=1000)\nscores = cross_val_score(clf, x, y, cv=10, scoring='accuracy')\nscores.mean()","082e0c04":"x = data.drop('diagnosis',axis=1)\ny = le.fit_transform(data['diagnosis'])\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=4)\nmetrics = pd.DataFrame(index = ['accuracy','precision','recall'],\n                       columns = ['Tree','SVM'])\ndef crossval(model,parameters):\n    clf = GridSearchCV(model, parameters)\n    clf.fit(X_train,y_train)\n    y_pred = clf.best_estimator_.predict(X_test)\n    accuracy = np.average(cross_val_score(clf, X_test, y_test, scoring='accuracy'))\n    precision = np.average(cross_val_score(clf, X_test, y_test, scoring='precision'))\n    recall = np.average(cross_val_score(clf, X_test, y_test, scoring='recall'))\n    f1= np.average(cross_val_score(clf, X_test, y_test, scoring='f1'))\n    if model==svm:\n        metrics.loc['accuracy','SVM'] = accuracy\n        metrics.loc['precision','SVM'] = precision\n        metrics.loc['recall','SVM'] = recall\n    if model==tree:\n        metrics.loc['accuracy','Tree'] = accuracy\n        metrics.loc['precision','Tree'] = precision\n        metrics.loc['recall','Tree'] = recall\n    return accuracy,precision,recall,f1,clf.best_estimator_,metrics","595ba51d":"svm = SVC()\ntree= DecisionTreeClassifier()\nparameters = {'kernel':('linear', 'rbf'), 'C':(1,10,100),'gamma': (1,2,3,'auto'),'decision_function_shape':('ovo','ovr'),'shrinking':(True,False)}\naccuracy,precision,recall,f1,model,metrics = crossval(svm,parameters)\nprint(metrics)\nparameters = {'max_depth':(1,6,12,15)}\naccuracy,precision,recall,f1,model,metrics = crossval(tree,parameters)\nprint(metrics)","a8db1689":"fig,ax = plt.subplots(figsize = (10,5))\nmetrics.plot(kind='barh', ax=ax)","62bf0a83":"### 3.1 Using cross validation and fitting SVM, Decision Tree Classifier","61fa6829":"### 1.9 Converting numerical variables to the log of each variable","541617ad":"## 1.0 Analyzing the dataset to understand the data","855ba7c3":" <h3><center>There are two main classifications of tumors. One is known as benign and the other as malignant. A benign tumor is a tumor that does not invade its surrounding tissue or spread around the body. A malignant tumor is a tumor that may invade its surrounding tissue or spread around the body.<\/center><\/h3>\n \n  <h3><center>- This notebook is used to classify a breast cancer patient by wheather it is malignant or benign<\/center><\/h3>","d2995e0e":" <h1><center><span style=\"color:red\">END<\/span> <\/center><\/h1>","1baff2de":"### 1.6 Excluding one variable that is correlated","5f4c5f13":"## 2.0 Model Training and Prediction","add656c0":"### 1.2 Unique Values","7143e395":"### 2.4 Using support vector machine by using different parameters","5fa09da5":"### 1.7 Analyzing the distribution of each numerical variable ","94f5bd22":"### 1.10 Removing the ID variable","5d1c15ea":"### 1.3 Comparing the valus in the dependant variable","03e27286":" <h1><center><span style=\"color:red\">Breast Cancer Classification<\/span> <\/center><\/h1>","b4839014":"### 2.3 Performance with outliers","fda8a848":"### 1.4 Analyzing the variables, after normalizing the variables and after log transforming the variable","a8ce504d":"### 2.1 Using Logistic Regression to predict the model's preformance","eb1fcea3":"### 1.8 Create a copy of the dataset to predict the model performance before and after remove outliers","6614aa90":"### 2.2  performance without outliers","a79c4fa1":"### 3.2 Accuracy, Precision and Recall values of SVM and Decision Tree Classifier","42101121":"## 3.0 Comparing different techniques and optimizing the algorithm","2dadc506":"### 1.5 Analyzing the correlation of variables ","f14c4cba":"### Since We are dealing with two different types of cancers and not True or False, I believe that our prediction has a good accuracy score and can be improved further","87a3b810":"### 1.1 Null values"}}