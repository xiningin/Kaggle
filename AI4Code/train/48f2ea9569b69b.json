{"cell_type":{"e22bee18":"code","42f0b6e8":"code","f57a1189":"code","5d0dfeee":"code","df2a6a30":"code","78176137":"code","bf5dca3c":"code","1c2e59c1":"code","d87d6fc6":"code","2bb07433":"code","27dc4a94":"code","7a4c352c":"code","60f973cd":"code","5770f776":"code","c439071f":"code","3f762f11":"code","78244da6":"code","1347e257":"code","dfcd73e4":"code","62edea32":"code","f9579fa1":"code","a074d00a":"code","9399ddb6":"code","4e6985b7":"code","8936fc4a":"code","7785c8d8":"code","6ee41834":"code","11236c13":"code","e976ad35":"code","51eab339":"code","e7161387":"code","e5044917":"code","c8af3089":"code","64825f83":"code","72bd5403":"code","8533d899":"code","7d3b2993":"code","b3c3acc5":"code","ac8e4ef0":"code","59230c01":"code","73990e60":"code","c82cfebd":"code","6ef414cc":"code","8f9dfb4e":"code","7ee4cbfa":"code","c7007a35":"code","c3757287":"code","e3a0ae7f":"code","61c61440":"code","81f4ae96":"markdown","73758a36":"markdown","018cbed4":"markdown"},"source":{"e22bee18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42f0b6e8":"# loading packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f57a1189":"# loading data\ndata = pd.read_csv('\/kaggle\/input\/datasets_33080_43333_car data.csv')\ndata.head()","5d0dfeee":"data.info()","df2a6a30":"data.isnull().sum()","78176137":"# taking object datatypes only\ndata.select_dtypes(include= np.object).head()","bf5dca3c":"# creating a function for getting unique values from dataset\ndef get_unique_values(dataset):\n    df = dataset.select_dtypes(include = np.object)\n    cols = list(df.columns)\n    for i in cols:\n        print('{}: {}'.format(i,df[i].unique()), '\\n')","1c2e59c1":"get_unique_values(data)","d87d6fc6":"data['Owner'].unique()","2bb07433":"data['Year'].unique()","27dc4a94":"# dropping unnecessary attributes\ndata.drop(columns=['Car_Name'], axis = 1,inplace= True)\ndata.head()","7a4c352c":"# considering only numeric attributes\ndata.iloc[:, 1:4].describe().T","60f973cd":"# data distribution using histograms\nplt.style.use('seaborn')\ndata.iloc[:, 1:4].hist(figsize = (12,12))\nplt.show()","5770f776":"## scatter plot for linearity check and spread on target variable\/attribute\nfor i in list(data.columns)[2:4]:\n    sns.relplot(x = i, y = 'Selling_Price', data= data)\n    plt.show()","c439071f":"## plotting by category\n\nfor i in list(data.columns)[2:4]:\n    for j in list(data.select_dtypes(include= np.object).columns):\n        sns.relplot(x = i, y = 'Selling_Price',hue = j,  data= data)\n    plt.show()","3f762f11":"## correlation\ndata.iloc[:, [0,2,3]].corr()","78244da6":"# to check yearly change\nfor i in list(data.columns)[1:4]:\n    sns.lineplot(x = 'Year', y = i, data = data, color=\"coral\")\n    plt.show()","1347e257":"# to check yearly change with category wise\nfor i in list(data.columns)[1:4]:\n    sns.lineplot(x = 'Year', y = i, data = data, hue = 'Fuel_Type' ,color=\"coral\")\n    plt.show()","dfcd73e4":"# to check yearly change with category wise\nfor i in list(data.columns)[1:4]:\n    sns.lineplot(x = 'Year', y = i, data = data, hue = 'Seller_Type' ,color=\"coral\")\n    plt.show()","62edea32":"# to check yearly change with category wise\nfor i in list(data.columns)[1:4]:\n    sns.lineplot(x = 'Year', y = i, data = data, hue = 'Transmission' ,color=\"coral\")\n    plt.show()","f9579fa1":"sns.barplot(x = 'Fuel_Type', y = 'Selling_Price', data = data, order = ['Diesel', 'Petrol', 'CNG'])\nplt.show()","a074d00a":"data.columns","9399ddb6":"list(data.columns)[4:len(list(data.columns))]","4e6985b7":"# selling price on categorical data\nfor i in list(data.columns)[4:len(list(data.columns))]:\n    sns.barplot(x = i, y = 'Selling_Price', data= data, )\n    plt.show()","8936fc4a":"# change categorical values into numeric by getting dummies\ndata = pd.get_dummies(data,columns= ['Fuel_Type','Seller_Type', 'Transmission'], drop_first= True)\ndata.head()","7785c8d8":"X_data = data.drop(columns= 'Selling_Price', axis = 1)\ny_data = data['Selling_Price']","6ee41834":"# here I m using extra tree classifier but you can use decision tree or randomforest too\nfrom sklearn.ensemble import ExtraTreesRegressor\net = ExtraTreesRegressor()\net.fit(X_data, y_data)","11236c13":"# check feature importance values\net.feature_importances_","e976ad35":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(et.feature_importances_, index=X_data.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.show()","51eab339":"# converting into X and y arrays\nX = X_data.iloc[:].values\ny = y_data.iloc[:].values","e7161387":"# splitting the data into train and test\n# random state is any orbitary number and it will help you to get same reslut as am I when you run this notebook.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state= 10)","e5044917":"# scaling the data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","c8af3089":"# models that I want to perform \n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","64825f83":"# linear regression\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)","72bd5403":"# training scores and testing scores\nprint(\"training score :{}\".format(lr_model.score(X_train, y_train)))\nprint(\"testing score :{}\".format(lr_model.score(X_test, y_test)))","8533d899":"# predictions\ny_pred = lr_model.predict(X_test)\ny_pred","7d3b2993":"# error metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nprint('mae score : {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred)))","b3c3acc5":"# decision tree\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train, y_train)","ac8e4ef0":"print(\"training score :{}\".format(dt_model.score(X_train, y_train)))\nprint(\"testing score :{}\".format(dt_model.score(X_test, y_test)))","59230c01":"y_pred = dt_model.predict(X_test)\ny_pred","73990e60":"print('mae score : {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred)))","c82cfebd":"# Randomforest model\nrf_model = RandomForestRegressor()\nrf_model.fit(X_train, y_train)","6ef414cc":"print(\"training score :{}\".format(rf_model.score(X_train, y_train)))\nprint(\"testing score :{}\".format(rf_model.score(X_test, y_test)))","8f9dfb4e":"y_pred = rf_model.predict(X_test)\ny_pred","7ee4cbfa":"print('mae score : {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred)))","c7007a35":"# xgboost\nimport xgboost as xgb\nxg = xgb.XGBRegressor()\nxg.fit(X_train, y_train)","c3757287":"print(\"training score :{}\".format(xg.score(X_train, y_train)))\nprint(\"testing score :{}\".format(xg.score(X_test, y_test)))","e3a0ae7f":"y_pred = xg.predict(X_test)\ny_pred","61c61440":"print('mae score : {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('mse score : {}'.format(mean_squared_error(y_test, y_pred)))\nprint('rmse score : {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('R2 score : {}'.format(r2_score(y_test, y_pred)))","81f4ae96":"- Note : the objective of ****variable importance**** is to give an idea of important attributes but depending on business lines we have to take decision whether we have to go with all attributes or drop some of them.","73758a36":"### Xgboost : The Showman","018cbed4":"### Note : compare the error metrics on each model for better understanding."}}