{"cell_type":{"9b4197e1":"code","8a3880bc":"code","c2b24765":"code","1e4490c7":"code","7b844f0d":"code","c41a1618":"code","341ca173":"code","84cef727":"code","6dd4d545":"code","7c82ce57":"code","78d0eaad":"code","291940bd":"code","ba3e9195":"code","d15d3453":"markdown","0d0d264a":"markdown","c67db0c6":"markdown"},"source":{"9b4197e1":"# Libraries \nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\n#pd.set_option('display.max_columns', 70) # Since we're dealing with moderately sized dataframe,\n#pd.set_option('display.max_rows', 13)# max 13 columns and rows will be shown","8a3880bc":"# Read\ndf = pd.read_csv(r\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head()\n","c2b24765":"#Data Manipulation\n#Replacing spaces with null values in total charges column\ndf['TotalCharges'] = df[\"TotalCharges\"].replace(\" \",np.nan)\n\n#Dropping null values from total charges column which contain .15% missing data \ndf = df[df[\"TotalCharges\"].notnull()]\ndf = df.reset_index()[df.columns]\n\n#convert to float type\ndf[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)\n\n#replace 'No internet service' to No for the following columns\nreplace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n                'TechSupport','StreamingTV', 'StreamingMovies']\nfor i in replace_cols : \n    df[i]  = df[i].replace({'No internet service' : 'No'})\n    \n#replace values\ndf[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})\n\n#Tenure to categorical column\ndef tenure_lab(df) :\n    \n    if df[\"tenure\"] <= 12 :\n        return \"Tenure_0-12\"\n    elif (df[\"tenure\"] > 12) & (df[\"tenure\"] <= 24 ):\n        return \"Tenure_12-24\"\n    elif (df[\"tenure\"] > 24) & (df[\"tenure\"] <= 48) :\n        return \"Tenure_24-48\"\n    elif (df[\"tenure\"] > 48) & (df[\"tenure\"] <= 60) :\n        return \"Tenure_48-60\"\n    elif df[\"tenure\"] > 60 :\n        return \"Tenure_gt_60\"\ndf[\"tenure_group\"] = df.apply(lambda df:tenure_lab(df),\n                                      axis = 1)\n\n#Separating churn and non churn customers\nchurn     = df[df[\"Churn\"] == \"Yes\"]\nnot_churn = df[df[\"Churn\"] == \"No\"]\n\n#Separating catagorical and numerical columns\nId_col     = ['customerID']\ntarget_col = [\"Churn\"]\ncat_cols   = df.nunique()[df.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\nnum_cols   = [x for x in df.columns if x not in cat_cols + target_col + Id_col]","1e4490c7":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n#customer id col\nId_col     = ['customerID']\n#Target columns\ntarget_col = [\"Churn\"]\n#categorical columns\ncat_cols   = df.nunique()[df.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols   = [x for x in df.columns if x not in cat_cols + target_col + Id_col]\n#Binary columns with 2 values\nbin_cols   = df.nunique()[df.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    df[i] = le.fit_transform(df[i])\n    \n#Duplicating columns for multi value columns\ndf = pd.get_dummies(data = df,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(df[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_df_og = df.copy()\ndf = df.drop(columns = num_cols,axis = 1)\ndf = df.merge(scaled,left_index=True,right_index=True,how = \"left\")","7b844f0d":"df.head()","c41a1618":"df1= df[[\n       'PhoneService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling',\n       'Churn', 'MultipleLines_No', 'MultipleLines_No phone service',\n       'MultipleLines_Yes', 'InternetService_DSL',\n       'InternetService_Fiber optic', 'InternetService_No',\n       'Contract_Month-to-month', 'Contract_One year', 'Contract_Two year',\n       'PaymentMethod_Bank transfer (automatic)',\n       'PaymentMethod_Credit card (automatic)',\n       'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check',\n       'tenure_group_Tenure_0-12', 'tenure_group_Tenure_12-24',\n       'tenure_group_Tenure_24-48', 'tenure_group_Tenure_48-60',\n       'tenure_group_Tenure_gt_60', 'tenure', 'MonthlyCharges',\n       'TotalCharges']]\n# df1 without personal details\ndf2= df[[ 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n       'PhoneService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling',\n       'Churn', 'MultipleLines_No', 'MultipleLines_No phone service',\n       'MultipleLines_Yes', 'InternetService_DSL',\n       'InternetService_Fiber optic', 'InternetService_No',\n       'Contract_Month-to-month', 'Contract_One year', 'Contract_Two year',\n       'PaymentMethod_Bank transfer (automatic)',\n       'PaymentMethod_Credit card (automatic)',\n       'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check',\n       'tenure_group_Tenure_0-12', 'tenure_group_Tenure_12-24',\n       'tenure_group_Tenure_24-48', 'tenure_group_Tenure_48-60',\n       'tenure_group_Tenure_gt_60', 'tenure', 'MonthlyCharges',\n       'TotalCharges']]\n#df2 with everything","341ca173":"Y = df2.Churn.values\nY1 = df1.Churn.values\nY","84cef727":"cols = df2.shape[1]\nX = df2.loc[:, df2.columns != 'Churn']\nX1 = df1.loc[:, df1.columns != 'Churn']\nX.columns;","6dd4d545":"Y.shape\nY1.shape","7c82ce57":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train1, X_test1,Y_train1,Y_test1 = train_test_split(X1, Y1, test_size=0.33, random_state=99)\n#Without weather\nsvc = SVC()\nsvc.fit(X_train1, Y_train1)\nY_pred = svc.predict(X_test1)\nacc_svc1 = round(svc.score(X_test1, Y_test1) * 100, 2)\nacc_svc1\nf1_svc1=f1_score(Y_test1, Y_pred, average='macro')\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train1, Y_train1)\nY_pred = knn.predict(X_test1)\nacc_knn1 = round(knn.score(X_test1, Y_test1) * 100, 2)\nacc_knn1\nf1_knn1=f1_score(Y_test1, Y_pred, average='macro')\n\n\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train1, Y_train1)\nY_pred = logreg.predict(X_test1)\nacc_log1 = round(logreg.score(X_train1, Y_train1) * 100, 2)\nacc_log1\nf1_logreg1=f1_score(Y_test1, Y_pred, average='macro')\n\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train1, Y_train1)\nY_pred = gaussian.predict(X_test1)\nacc_gaussian1 = round(gaussian.score(X_test1, Y_test1) * 100, 2)\nacc_gaussian1\nf1_gaussian1=f1_score(Y_test1, Y_pred, average='macro')\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train1, Y_train1)\nY_pred = perceptron.predict(X_test1)\nacc_perceptron1 = round(perceptron.score(X_test1, Y_test1) * 100, 2)\nacc_perceptron1\nf1_perceptron1=f1_score(Y_test1, Y_pred, average='macro')\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train1, Y_train1)\nY_pred = linear_svc.predict(X_test1)\nacc_linear_svc1 = round(linear_svc.score(X_test1, Y_test1) * 100, 2)\nacc_linear_svc1\nf1_linear_svc1=f1_score(Y_test1, Y_pred, average='macro')\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train1, Y_train1)\nY_pred = sgd.predict(X_test1)\nacc_sgd1 = round(sgd.score(X_test1, Y_test1) * 100, 2)\nacc_sgd1\nf1_sgd1=f1_score(Y_test1, Y_pred, average='macro')\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train1, Y_train1)\nY_pred = decision_tree.predict(X_test1)\nacc_decision_tree1 = round(decision_tree.score(X_test1, Y_test1) * 100, 2)\nacc_decision_tree1\nf1_decision_tree1=f1_score(Y_test1, Y_pred, average='macro')\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train1, Y_train1)\nY_pred = random_forest.predict(X_test1)\nrandom_forest.score(X_train1, Y_train1)\nacc_random_forest1 = round(random_forest.score(X_test1, Y_test1) * 100, 2)\nacc_random_forest1\nf1_random_forest1=f1_score(Y_test1, Y_pred, average='macro')\n","78d0eaad":"# Support Vector Machines\nX_train, X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.33, random_state=99)\n#with weather condition\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_test, Y_test) * 100, 2)\nacc_svc\nf1_svc=f1_score(Y_test, Y_pred, average='macro')\n\n#KNN\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test, Y_test) * 100, 2)\nacc_knn\nf1_knn=f1_score(Y_test, Y_pred, average='macro')\n\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\nf1_logreg=f1_score(Y_test, Y_pred, average='macro')\n\n# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_test, Y_test) * 100, 2)\nacc_gaussian\nf1_gaussian=f1_score(Y_test, Y_pred, average='macro')\n\n# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_test, Y_test) * 100, 2)\nacc_perceptron\nf1_perceptron=f1_score(Y_test, Y_pred, average='macro')\n\n# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_test, Y_test) * 100, 2)\nacc_linear_svc\nf1_linear_svc=f1_score(Y_test, Y_pred, average='macro')\n\n# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_test, Y_test) * 100, 2)\nacc_sgd\nf1_sgd=f1_score(Y_test, Y_pred, average='macro')\n\n# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_test, Y_test) * 100, 2)\nacc_decision_tree\nf1_decision_tree=f1_score(Y_test, Y_pred, average='macro')\n\n# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_test, Y_test) * 100, 2)\nacc_random_forest\nf1_random_forest=f1_score(Y_test, Y_pred, average='macro')\n","291940bd":"models = pd.DataFrame({\n    'Without Personal Data': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Accuracy': [acc_svc1, acc_knn1, acc_log1, \n              acc_random_forest1, acc_gaussian1, acc_perceptron1, \n              acc_sgd1, acc_linear_svc1, acc_decision_tree1],\n    'F1-Score' : [f1_svc, f1_knn1, f1_logreg1, \n              f1_random_forest1, f1_gaussian1, f1_perceptron1, \n              f1_sgd1, f1_linear_svc1, f1_decision_tree]})\nsorted_without=models.sort_values(by='Accuracy', ascending=False)\n\nmodels = pd.DataFrame({\n    'With Personal Data': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Accuracy': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree],\n    'F1-Score' : [f1_svc, f1_knn, f1_logreg, \n              f1_random_forest, f1_gaussian, f1_perceptron, \n              f1_sgd, f1_linear_svc, f1_decision_tree]})\nsorted_with=models.sort_values(by='Accuracy', ascending=False)\n\n","ba3e9195":"from IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n    \ndisplay_side_by_side(sorted_without,sorted_with)\n","d15d3453":"# Customer Turnover \nIn the customer management lifecycle, customer churn refers to a decision made by the customer about ending the business relationship. It is also referred as loss of clients or customers.\n\n![Which customers stay and which leave?](http:\/\/www.bsaitechnosales.com\/wp-content\/uploads\/2018\/03\/customer_churn_telecom_service.jpg)\n\n\nGDPR came in effect from May 25th, 2018 onwards in Europe. \n\n![GDPR](https:\/\/zdnet1.cbsistatic.com\/hub\/i\/2017\/11\/15\/be5d1ea8-0ad7-45e6-8588-e2c7eafecd79\/bb902409c29dce984f60ae162b67f420\/istock-gdpr-concept-image.jpg)\n\nOne of the fundamental issues addressed in GDPR is whether the data holder has a legitimate reason for holding the data. Telecom companies should only use as much data as is required to get a task done. Data minimisation if referred in five separate chapters. Therefore, it is impossible to comply with the GDPR without applying the concepts.\n\nI noticed that the given data holds five variables that could be considered personal. Namely the gender, senior citizenship status, partner, dependent, and tenure. Especially the gender and whether the customer has a partner or a dependent, were interesting variables. There is also some speculation how senior status affects the likelihood of voluntary churn. \n\n### My primary question which I hope to have an answer to is the following: \n\n#### 1) To what degree do personal details of gender, senior citizenship status, existence of a partner, existance of a dependent, and tenure affect the model's predictive power of predicting churn?\n\n","0d0d264a":"### Results\n\nGiven the accuracy and F1-score, it seems that exclusion of five variables does not affect the performance of predicting churn significantly. \n\nConsequently, is it necessary to include personal information in predicting customer turnover probabilities? Or is the infromation already present in other features? What about knowing whether the person is classified as a senior - does it increase or decrease the likelihood for voluntary customer turnover? \n\nFinally, if the data doesn't purposefully serve , then are the companies justified to collect and store this data in case they operate within the European Union? \n\n\n\n","c67db0c6":"Credits for data preprocessing go to Pavan Raj and his [Kernel](https:\/\/www.kaggle.com\/pavanraj159\/telecom-customer-churn-prediction)"}}