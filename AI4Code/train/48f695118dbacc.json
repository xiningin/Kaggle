{"cell_type":{"a79e3250":"code","22ba26d1":"code","ad5afe39":"code","8b71fa1b":"code","bdfb1049":"code","8725e64c":"code","74e0281c":"code","0c97d4f2":"code","aa2d16e0":"code","d57c21a9":"code","f130ed68":"code","a069d799":"code","b0ec4fce":"markdown","0b2019f2":"markdown","64436357":"markdown","d3ccb991":"markdown","29dc0498":"markdown","74e5dc4e":"markdown","5b54e991":"markdown"},"source":{"a79e3250":"import json\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport tensorflow.keras.layers as L\nimport tensorflow as tf","22ba26d1":"# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","ad5afe39":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True))\n\ndef build_model(embed_size, seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128):\n    inputs = L.Input(shape=(seq_len, 3))\n\n    embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n\n    hidden = gru_layer(hidden_dim, dropout)(reshaped)\n    hidden = gru_layer(hidden_dim, dropout)(hidden)\n    hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n    # Since we are only making predictions on the first part of each sequence, we have\n    # to truncate it\n    truncated = hidden[:, :pred_len]\n    \n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    model.compile(tf.keras.optimizers.Adam(), loss='mse')\n    \n    return model","8b71fa1b":"def pandas_list_to_array(df):\n    \"\"\"\n    Inputs:\n        df: dataframe of shape (x, y), containing list of length l\n    Return:\n        np.array of shape (x, l, y)\n    \"\"\"\n    \n    return np.transpose(\n        np.array(\n            df.values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","bdfb1049":"def preprocess_inputs(df, token2int, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return pandas_list_to_array(\n        df[cols].applymap(lambda seq: [token2int[x] for x in seq])\n    )","8725e64c":"data_dir = '\/kaggle\/input\/stanford-covid-vaccine\/'\ntrain = pd.read_json(data_dir + 'train.json', lines=True)\ntest = pd.read_json(data_dir + 'test.json', lines=True)\nsample_df = pd.read_csv(data_dir + 'sample_submission.csv')","74e0281c":"train_sn05 = train[train.signal_to_noise > 0.5]\ntrain_sn10 = train[train.signal_to_noise > 1]\ntrain_sn15 = train[train.signal_to_noise > 1.5]\ntrain_sn20 = train[train.signal_to_noise > 2]\ntrain_sn25 = train[train.signal_to_noise > 2.5]\ntrain_sn30 = train[train.signal_to_noise > 3]\ntrain_sn35 = train[train.signal_to_noise > 3.5]\ntrain_sn40 = train[train.signal_to_noise > 4.0]\n\n# train_noisy is used for noisy test data\ntrain_noisy = train[train.signal_to_noise < 0.5]\n\nfor i, train_sn in enumerate([train_sn05, train_sn10, train_sn15, train_sn20, train_sn25, train_sn30, train_sn35, train_sn40]):    \n    print(f'the length of train_sn{(i+1)*5} is {len(train_sn)}')\n    \nprint(f'the length of train_noisy is {len(train_noisy)}')","0c97d4f2":"# We will use this dictionary to map each character to an integer\n# so that it can be used as an input in keras\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ntrain_inputs = preprocess_inputs(train, token2int)\ntrain_labels = pandas_list_to_array(train[pred_cols])\n\ntrain_inputs_05 = preprocess_inputs(train_sn05, token2int)\ntrain_inputs_10 = preprocess_inputs(train_sn10, token2int)\ntrain_inputs_15 = preprocess_inputs(train_sn15, token2int)\ntrain_inputs_20 = preprocess_inputs(train_sn20, token2int)\ntrain_inputs_25 = preprocess_inputs(train_sn25, token2int)\ntrain_inputs_30 = preprocess_inputs(train_sn30, token2int)\ntrain_inputs_35 = preprocess_inputs(train_sn35, token2int)\ntrain_inputs_40 = preprocess_inputs(train_sn40, token2int)\n\ntrain_labels_05 = pandas_list_to_array(train_sn05[pred_cols])\ntrain_labels_10 = pandas_list_to_array(train_sn10[pred_cols])\ntrain_labels_15 = pandas_list_to_array(train_sn15[pred_cols])\ntrain_labels_20 = pandas_list_to_array(train_sn20[pred_cols])\ntrain_labels_25 = pandas_list_to_array(train_sn25[pred_cols])\ntrain_labels_30 = pandas_list_to_array(train_sn30[pred_cols])\ntrain_labels_35 = pandas_list_to_array(train_sn35[pred_cols])\ntrain_labels_40 = pandas_list_to_array(train_sn40[pred_cols])\n\ntest_inputs = preprocess_inputs(train_noisy, token2int)\ntest_labels = pandas_list_to_array(train_noisy[pred_cols])","aa2d16e0":"model = build_model(embed_size=len(token2int))\nmodel.summary()","d57c21a9":"train_sn_list = [(train_inputs_05, train_labels_05,),\n                (train_inputs_10, train_labels_10,),\n                (train_inputs_15, train_labels_15,),\n                (train_inputs_20, train_labels_20,),\n                (train_inputs_25, train_labels_25,),\n                (train_inputs_30, train_labels_30,),\n                (train_inputs_35, train_labels_35,),\n                (train_inputs_40, train_labels_40,)]\n\nloss_list = []\n\nfor i, (train_inputs, train_labels) in enumerate(train_sn_list):\n    print('=========================================')\n    print(f'SN={(i+1)*0.5}')\n    model = build_model(embed_size=len(token2int))\n    history = model.fit(\n    train_inputs, train_labels, \n    batch_size=64,\n    epochs=60,\n    verbose=2,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n        tf.keras.callbacks.ModelCheckpoint(f'model_sn{(i+1)*5}.h5')\n    ],\n    validation_split=0.2\n)\n    loss_list.append(min(history.history['val_loss']))","f130ed68":"# minimum val_loss of each model\nfor i, loss in enumerate(loss_list):\n    print(f'SN={(i+1)*0.5}: val_loss = {loss}')","a069d799":"# loss for noisy test data\nfor i in range(8):\n    model.load_weights(f'.\/model_sn{(i+1)*5}.h5')\n    results = model.evaluate(test_inputs, test_labels, batch_size=8, verbose=0)\n    print(f'SN={(i+1)*0.5}: test_loss = {results}')","b0ec4fce":"## Load and preprocess data","0b2019f2":"## Helper functions and useful variables","64436357":"According to this great notebook https:\/\/www.kaggle.com\/alelafe\/openvaccine-gru-lstm-noiselevel, signal_to_noise is really useful to eliminate noisy data.\n\nBut, eliminating noisy data is good for predicting noisy data.\n\nTo answer the question, I decided to perform some experiments with this notebook.\n\n\n1.  **I made 8 train data with signal_to_noise cutoff = 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0**\n2.  **I used train data with SN < 0.5 as noisy test dataset.**\n\n\nIf I made a mistake, please point it out to me.\n\nThis notebook is based on https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model .\n\nVersion 7: I revised some mistakes, the values of sn cutoff = 3.5 and 4.0 in Results were wrong.","d3ccb991":"## Build and train model","29dc0498":"|  SN  |  0.5  |  1.0  |  1.5  |  2.0  |  2.5  |  3.0  | 3.5  |  4.0  |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n|  train size  |  2212  |  2096  |  1999  |  1889  |  1771  |  1649  | 1497  |  1350  |\n|  min val_loss  |  0.0759  |  0.0680  |  0.0651  |  0.0614  |  0.0601  |  0.0580  | 0.0581  |  0.0569  |\n    |  test_loss  |  8.91  |  8.86  |  8.88  |  8.93  |  9.01  |  8.90  | 8.96  |  8.98  |\n","74e5dc4e":"# Results: val_loss & test_loss","5b54e991":"The table above is all the results from this notebook. Signal_to_noise cutoff didn't affect the values of test_loss significantly. I think that noisy data were not properly measured, so it is really difficult to predict them. Therefore, cutoff by signal_to_noise value may not cause overfitting to public leader board. Because it is difficult to accurately predict outliers. In this competition, using outliers to predict outliers may not be effective. "}}