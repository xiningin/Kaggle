{"cell_type":{"56b94f3b":"code","759f06c6":"code","2d7f7de8":"code","b232be04":"code","9ea616ae":"code","af72b2a9":"code","6df29f27":"code","3bf0aab0":"code","df9f22b9":"code","1869fdaf":"code","f368fb4d":"code","cc4077d6":"code","3ef3ab89":"code","dab42dbf":"code","6d8712ea":"code","59daf6d3":"code","3a791c80":"code","b3921761":"code","20473e9f":"code","87e1914a":"code","9bae30ee":"code","18cfcf8c":"code","86bedebd":"code","2371cc77":"code","80dfe503":"code","817e7103":"code","50af8673":"code","ee7f034d":"code","15dee7dd":"code","1a4c8c0e":"code","0880aec3":"code","4ff194a4":"code","45d1cbbd":"code","e7dae8cf":"code","70ff191e":"code","01b0dc40":"code","53d23f94":"code","761d0d63":"code","fcb2b83d":"code","14f42d12":"code","8a4a787c":"code","9fdf3fe8":"code","364bcaf9":"markdown","d512978a":"markdown","f2cdde6c":"markdown","7ec96822":"markdown","92ea9f65":"markdown","314d6d63":"markdown","82004fb0":"markdown","afd5e4f3":"markdown","0131ffb8":"markdown","93b39552":"markdown","806ed263":"markdown","661db495":"markdown","85948698":"markdown","d67b90fb":"markdown","69522233":"markdown","b8f8edab":"markdown","fd24b789":"markdown","da3da380":"markdown","8ac3ffef":"markdown","c17c5977":"markdown","979ad5df":"markdown","a7187dd7":"markdown","ea9be0c8":"markdown"},"source":{"56b94f3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%run \"..\/input\/titanicprescripts\/prescripts.ipynb\"\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import LabelEncoder\n# Importing Dependencies\n%matplotlib inline\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\nimport missingno as msno\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n   \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","759f06c6":"Read_Datasources()\ntrain_data","2d7f7de8":"MissingColumnPercent(train_data)","b232be04":"MissingColumnPercent(test_data)","9ea616ae":"Merge_TrainandTest()\nmerge","af72b2a9":"#Percentage of missing values in merge dataset\nMissingColumnPercent(merge)","6df29f27":"#Verifying if there are any multiple entries recorded in the dataset\nDuplicate_check()","3bf0aab0":"#PClass\n\nCategorical_Distribtion(\"Pclass\")","df9f22b9":"#Name\n\nprint(\"Duplicates count:\",len(merge[merge.duplicated([\"Name\"])]))\nmerge.query(\"Name=='Connolly, Miss. Kate'\")","1869fdaf":"merge[\"Name\"].nunique()","f368fb4d":"\nName_conversion()","cc4077d6":"Categorical_Distribtion(\"Name\")","3ef3ab89":"#sex\nCategorical_Distribtion(\"Sex\")","dab42dbf":"#Cabin\n\n# Well it is very clear missing perecentage is of 77% in Cabin column,through the number we get influenced to drop column.\n# But again from other end of story, we may can take this values to be no cabin allotted passengers.\n\ncabin_impute()\n","6d8712ea":"#Cabin\nCategorical_Distribtion(\"Cabin\")","59daf6d3":"#Ticket\n\n# Column Ticket contains some alphanumerical values,as this might add piece of infrormation. \n# Better we columnise with important details\n\n\ncolumnise_ticket()","3a791c80":"Categorical_Distribtion(\"Ticket\")","b3921761":"#Embarked\nCategorical_Distribtion(\"Embarked\")","20473e9f":"#Since mode of distribution is towards S category in either of respones, filling the nulls with respective value\nmerge[\"Embarked\"].fillna(\"S\",inplace=True)\nCategorical_Distribtion(\"Embarked\")","87e1914a":"sns.boxplot(merge[\"Fare\"])\n# Filling Nulls with median as data is highly skewed\nmerge[\"Fare\"].fillna(merge[\"Fare\"].median(),inplace=True)","9bae30ee":"sns.distplot(merge[\"Fare\"])","18cfcf8c":"MissingColumnPercent(merge)","86bedebd":"# Age\n\n    \nNumerical_Distribution(\"Age\")    ","2371cc77":"# Imputing Age column by simple Mean\n\nmerge[\"Age_im_mean\"] = merge['Age'].replace(np.NaN,merge['Age'].mean())\n","80dfe503":"# Imputing Age column by  simple median \n\nmerge[\"Age_im_median\"] = merge['Age'].replace(np.NaN,merge['Age'].median())","817e7103":"# custom imputation\n\ncustom_simple_impute() #label Age_ci","50af8673":"# Sibsp and Parch\n# Merging Sibsp and Parch and creating new variable called 'Family_size' and Create buckets of single, small, medium, and large and then put respective values into them.\n\nFamily_Binning()","ee7f034d":"Categorical_Distribtion(\"Family_size\")\n","15dee7dd":"# Age\n\nAge_Binning() #output Age_binned","1a4c8c0e":"Categorical_Distribtion(\"Age_binned\")","0880aec3":"# Fare\n\nFare_Binning()","4ff194a4":"Categorical_Distribtion(\"Fare_binned\")","45d1cbbd":"   \ndata_types(merge)","e7dae8cf":"    \ntransform_columns()","70ff191e":"data_types(merge)","01b0dc40":"#Let's split the train and test set to feed machine learning algorithm.\ntest_train_split()","53d23f94":"# Now initialize all the classifiers object.\n\nmodels_train()","761d0d63":"# Create a function that returns mean cross validation score for different models. Here we used Kfold validation process\n\nmodel_evaluation()","fcb2b83d":" sorted_tunned_scores= model_tuning()","14f42d12":"tuned_score()","8a4a787c":"model_retrain()","9fdf3fe8":"model_pred_sub()","364bcaf9":"## Observations:\n\n* Looks dataset is mix match of numerical and ordinal variables, hence it is suggested to anlayze the dataset compehensively and compose to model requirements.\n\n* To support faster analysis and persistent data structure w.r.t train and test datasets, will merge together.","d512978a":"# Exploratory Data Analysis","f2cdde6c":"Here we can reference survival ratio of Miss, Mrs tend to be higher, also Mr have high non survival count.","7ec96822":"Survived Rate for Pclass 1 is very less compared to others, either indicating facilities provided here are shortage or due to high member count. ","92ea9f65":"# Model Evaluation","314d6d63":"# Binning\n","82004fb0":"# Data Preparation","afd5e4f3":"As several titles represented, which is addded information of profession let us columnise them based on bukets they may fit in. \n\nProfessionals like Dr, Rev, Col, Major, Capt will be put into 'Officer' bucket.\nTitles such as Dona, Jonkheer, Countess, Sir, Lady, Don were usually entitled to the aristocrats.\nWe would also replace Mlle and Ms with Miss and Mme by Mrs as these are French titles.","0131ffb8":"In the above tabel, LR, KNN have the highest cross validation accuracy among the remaining models.","93b39552":"## Data Insource and Inspection","806ed263":"Survival ratio is higher for age group less than 20, also can infer that survival ratio decreases as age increases. To analyze the key points from age, let us divide column values to various bukets. But before let us complete and correct the values.\n\nAs part of anlalysis, age data is seem missing at random and have no correlation with observed variables. Let us impute these values by trying some of most commonly used techniques.","661db495":"From above we observe Decision Tree and Random Forest highest accuracy, however we cant justify these are top performeing models on dataset as evaluation is yet to be done.","85948698":"Though there are outliers in the data, there might be passengers who have charged high fare as per the choice and accomodations they were interested in. So to make vaulable piece of information let us bin this column.","d67b90fb":"# Columns Distribution Analysis","69522233":"As sum of classes purposedly made to validate model, we may forgo this synopsis.","b8f8edab":"# Missing Columns","fd24b789":"Found there are no duplicate records in entire dataset and in primary identifier column PassengerID","da3da380":"# Model Building ","8ac3ffef":"# Imputation Methods","c17c5977":"# Tuning Model Hyperparameters","979ad5df":"From above, can be inferred survival ratio is high among females.","a7187dd7":"# Model Re-train","ea9be0c8":"# Model Prediction & Submission \n"}}