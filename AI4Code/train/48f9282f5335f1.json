{"cell_type":{"7d6f967a":"code","515aa9e2":"code","b410036b":"code","9dab58a4":"code","a9c0afba":"code","89812909":"code","0a481698":"code","6a05a594":"code","ce3c5c50":"code","da2db3a9":"code","e268d381":"code","31163f79":"code","a134dfec":"code","6e229ff2":"code","66f8f59b":"code","335148eb":"code","515db556":"code","00c7ad00":"code","be7b0dfc":"code","6e663a23":"markdown","8660e1c0":"markdown","3c70c9fe":"markdown","4995142c":"markdown","a92124fe":"markdown","6288f392":"markdown","c4f56c8e":"markdown","9c15936d":"markdown","910f8222":"markdown","ef238775":"markdown","9d82b238":"markdown","5a59cdd8":"markdown","8525af25":"markdown","622109a6":"markdown","c77484e1":"markdown","2777143a":"markdown","f2c9ad9c":"markdown"},"source":{"7d6f967a":"!pip install -q --upgrade tf-nightly==1.14.1-dev20190312 #tf-nightly-gpu==1.14.1-dev20190312\n!pip install -q tfp-nightly==0.7.0.dev20190312\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.python import tf2\nif not tf2.enabled():\n    import tensorflow.compat.v2 as tf\n    tf.enable_v2_behavior()\n    assert tf2.enabled()\n\n# import tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\n\ntfk = tf.keras\ntfkl = tf.keras.layers\ntfpl = tfp.layers\ntfd = tfp.distributions\n\nfrom tensorflow.keras.utils import plot_model\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.manifold import TSNE\nimport os\n\n%matplotlib inline\nnp.random.seed(0)\ntf.random.set_seed(0)\nprint(tf.__version__)\nprint(tfp.__version__)","515aa9e2":"### Utility Functions\n## Plots\n# Plot Feature Projection [credit: https:\/\/www.kaggle.com\/shivamb\/semi-supervised-classification-using-autoencoders]\ndef tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2, random_state=0)\n    X_t = tsne.fit_transform(x1)\n#     plt.figure(figsize=(12, 8))\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud', s=2)\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Fraud', s=2)\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n    \n# Plot Keras training history\ndef plot_loss(hist):\n    plt.plot(hist.history['loss'])\n    plt.plot(hist.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.yscale('log',basey=10)\n    plt.show()\n    ","b410036b":"raw_data = pd.read_csv(\"..\/input\/creditcard.csv\")\ndata, data_test = train_test_split(raw_data, test_size=0.25)","9dab58a4":"data.loc[:,\"Time\"] = data[\"Time\"].apply(lambda x : x \/ 3600 % 24)\ndata.loc[:,'Amount'] = np.log(data['Amount']+1)\n\ndata_test.loc[:,\"Time\"] = data_test[\"Time\"].apply(lambda x : x \/ 3600 % 24)\ndata_test.loc[:,'Amount'] = np.log(data_test['Amount']+1)\n# data = data.drop(['Amount'], axis = 1)\nprint(data.shape)\ndata.head()","a9c0afba":"non_fraud = data[data['Class'] == 0].sample(1000)\nfraud = data[data['Class'] == 1]\n\ndf = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df.drop(['Class'], axis = 1).values\nY = df[\"Class\"].values\n\ntsne_plot(X, Y, \"original.png\")","89812909":"def dense_layers(sizes):\n    return tfk.Sequential([tfkl.Dense(size, activation=tf.nn.leaky_relu) for size in sizes])\n\noriginal_dim = X.shape[1]\ninput_shape = X[0].shape\nintermediary_dims = [20, 10, 8]\nlatent_dim = 2\nbatch_size = 128\nmax_epochs = 1000\n\n# prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1),\n#                         reinterpreted_batch_ndims=1)\n\nprior = tfd.MultivariateNormalDiag(\n        loc=tf.zeros([latent_dim]),\n        scale_identity_multiplier=1.0)\n\nencoder = tfk.Sequential([\n    tfkl.InputLayer(input_shape=input_shape, name='encoder_input'),\n    dense_layers(intermediary_dims),\n    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None),\n    tfpl.MultivariateNormalTriL(latent_dim, \n                           activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n], name='encoder')\n\nencoder.summary()\nplot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n\ndecoder = tfk.Sequential([\n    tfkl.InputLayer(input_shape=[latent_dim]),\n    dense_layers(reversed(intermediary_dims)),\n    tfkl.Dense(tfpl.IndependentNormal.params_size(original_dim), activation=None),\n    tfpl.IndependentNormal(original_dim),\n], name='decoder')\n\ndecoder.summary()\nplot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n\nvae = tfk.Model(inputs=encoder.inputs,\n                outputs=decoder(encoder.outputs[0]),\n                name='vae_mlp')\n\nnegloglik = lambda x, rv_x: -rv_x.log_prob(x)\n\nvae.compile(optimizer=tf.keras.optimizers.Nadam(), \n            loss=negloglik)\n\nvae.summary()\nplot_model(vae,\n           to_file='vae_mlp.png',\n           show_shapes=True)","0a481698":"x = data.drop([\"Class\"], axis=1)\ny = data[\"Class\"].values\n\nx_norm, x_fraud = x.values[y == 0], x.values[y == 1]\n\nx_norm_sample = x_norm[np.random.randint(x_norm.shape[0], size=100000), :]\nx_norm_train_sample, x_norm_val_sample = train_test_split(x_norm_sample, test_size=0.2)","6a05a594":"tf_train = tf.data.Dataset.from_tensor_slices((x_norm_train_sample, x_norm_train_sample)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))\ntf_val = tf.data.Dataset.from_tensor_slices((x_norm_val_sample, x_norm_val_sample)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))","ce3c5c50":"checkpointer = ModelCheckpoint(filepath='bestmodel.h5', verbose=0, save_best_only=True)\nearlystopper = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.005, patience=20, verbose=0, restore_best_weights=True)\n\nhist = vae.fit(tf_train,\n               epochs=max_epochs,\n               shuffle=True,\n               verbose=0,\n               validation_data=tf_val,\n               callbacks=[checkpointer, earlystopper])\n\nplot_loss(hist)\n","da2db3a9":"# vae.load_weights('bestmodel.h5', by_name=True)","e268d381":"reconstruct_samples_n = 100\n\ndef reconstruction_log_prob(eval_samples, reconstruct_samples_n):\n    encoder_out = encoder(eval_samples)\n    encoder_samples = encoder_out.sample(reconstruct_samples_n)\n    return np.mean(decoder(encoder_samples).log_prob(eval_samples), axis=0)","31163f79":"latent_x_mean = encoder(X).mean()\nplt.scatter(latent_x_mean[:, 0], latent_x_mean[:, 1], c=Y, cmap='RdYlGn_r', s=2)\nplt.title('latent means')\nplt.ylabel('mean[1]')\nplt.xlabel('mean[0]')\nplt.show()\n# tsne_plot(latent_x_mean, data_sample[\"Class\"].values, \"raw.png\")","a134dfec":"latent_x_std = encoder(X).stddev()\nplt.scatter(latent_x_std[:, 0], latent_x_std[:, 1], c=Y, cmap='RdYlGn_r', s=2)\nplt.title('latent standard deviations')\nplt.ylabel('stddev[1]')\nplt.xlabel('stddev[0]')\nplt.show()","6e229ff2":"latent_x = encoder(X).sample()\nplt.scatter(latent_x[:, 0], latent_x[:, 1], c=Y, cmap='RdYlGn_r', s=2)\nplt.title('latent vector samples')\nplt.ylabel('z[1]')\nplt.xlabel('z[0]')\nplt.show()","66f8f59b":"x_log_prob = reconstruction_log_prob(X, reconstruct_samples_n)\nax = plt.hist([x_log_prob[Y==0], x_log_prob[Y==1]], 60)\nplt.title('reconstruction log probability')\nplt.ylabel('frequency')\nplt.xlabel(\"log p(x|x')\")\nplt.show()","335148eb":"fpr, tpr, thresh = roc_curve(Y, -x_log_prob)\nauc = roc_auc_score(Y, -x_log_prob)\n\nplt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\nplt.title('VAE roc curve - training')\nplt.ylabel('True Positive Rate')\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc='best')\nplt.show()","515db556":"from sklearn import svm\nclf = svm.SVC(gamma='scale')\nclf.fit(X, Y)\nauc = roc_auc_score(Y, clf.predict(X))\n\nplt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\nplt.title('SVM roc curve - training')\nplt.ylabel('True Positive Rate')\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc='best')\nplt.show()","00c7ad00":"x_test_log_prob = reconstruction_log_prob(data_test.drop(['Class'], axis = 1).values, reconstruct_samples_n)\ntest_y = data_test[\"Class\"].values\n\nfpr, tpr, thresh = roc_curve(test_y, -x_test_log_prob)\nauc = roc_auc_score(test_y, -x_test_log_prob)\n\nplt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\nplt.title('VAE roc curve - test')\nplt.ylabel('True Positive Rate')\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc='best')\nplt.show()","be7b0dfc":"auc = roc_auc_score(test_y, clf.predict(data_test.drop(['Class'], axis = 1).values))\n\nplt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\nplt.title('SVM roc curve - test')\nplt.ylabel('True Positive Rate')\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc='best')\nplt.show()","6e663a23":"## 6. Evaluation on Test Set\nEvaluate VAE and a trivial SVM on the previously reserved test set. VAE outperforms the simple SVM.","8660e1c0":"The training stops when the validation losses fail to decrease for 20 consecutive epochs.","3c70c9fe":"## 3. Train a Variational Autoencoder\nWe train a VAE with 100k in-sample non-fraud transactions. As this is for exploratory and illustration purpose, the hidden layer design was by trial and error. \n\nThe prior for the latent variables was set to be a random unit multivariate normal vector of the latent dimention. The latent dimension is set to 2 so that it give some intuitive illustrations as you will see soon. \n\nThe output of the encoder, the latent distribution parameters, was deliberated chosen to be multiviarate normal with non-zero covariance because I noticed it had subsequent impact on the separation of normal transactions from fraud transactions, suggesting that the covariance of fraud transactions may have patterns. As a result, there are 5 distribution parameters to be learnt (2 mean values + 3 covariance values from the lower triangle of the 2-buy-2 covariance matrix)\n\nThe output of the decoder, the data distribution parameters, follow feature-independent normal distributions. This choice is important. Most of the examples I could find online were applied to binary images such as the MNIST dataset where the output would follow independent bernoulli distributions. Here the data are real-valued and generally follow normal distributions, hence it only makes sense to model the output with normal distributions or alike. Another important implication of having the right distribution is that it will give the corresponding log probability loss during training. It would not make sense to train a real-valued normal distribution using binary cross entropy, for example.","4995142c":"<h1 align=\"center\">Fraud Detection with Variational Autoencoder<\/h1>","a92124fe":"## 1. Raw Dataset\n\nRead the data, split the data into training set and test set. All trials for model design and hyperparameter selection were based solely on the \"data\" training set. The \"data_test\" test set was never used until the final evaluation of classifiers before the initial commit of this report.","6288f392":"## 2. Visualize Preprocessed Transaction Features\nSample 1000 non-fraud transactions from the training set and plot with all fraudulent transactions in the training set. The T-SNE plot shows that after Time and Amount preprocessing, fraud transactions (in red) seem to be adequately seperated from non-fraud transactions (in green) in this particular projection.","c4f56c8e":"## 7. Limitations \n* The VAE hidden layer design has room for improvement.\n* Hyperparameter tuning can be added.","9c15936d":"## 5. VAE vs SVM\nLet's compare this with a trivial supervised SVM classifier. I am curious if SVM would outperform VAE, since the preprocessed input seemed to give good separation between frauds and non-frauds. Turns out it did not. VAE for the win!","910f8222":"Looking at the log of the reconstruction probability, fraud transactions in orange are clearly separeted from normal transaction in most cases despite some overlaps. Normal transactions have higher reconstruction probability, of course.\n\nNow, let's take the negative reconstruction log probability, and draw a ROC curve across the range to see how it would perform if we were to build a threshold-based fraud detector on it.","ef238775":"## 4. Visualize Latent Representations \nT-SNE plots of latent distribution parameters and samples for the previously sampled data. Notice how the clustering are very pronounced across all plots.","9d82b238":"In my [previous naive attempt](https:\/\/www.kaggle.com\/hone5com\/fraud-detection-with-one-class-adversarial-nets) at applying autoencoders to fraud detection, I trained a simple autoencoder with one hidden layer on each of the encoder side and decoder side. The autoencoder was asymmetrical and overcomplete, i.e. the hidden layer dimensions and the latent dimension were asymetrical and larger than the input size. Even so, it seemed to perform alright by piping the latent encoding through a simple linear classifer. An adversarial net was also tried out, but only as a classifier alternative to the linear benchmark. This may be something I would revisit in the future.\n\nIn this new attempt, using the same credit card data by ULB machine learning group, I explore applying Variational Autoencoders (VAE) to the same problem. Functionally, Variational Autoencoders also try to reconstruct the input, but with the additional contraint that the latent representation learns the probability distribution parameters of the input rather than an arbitrary encoding of the input, hance the \"Variational\". What's good about learning the distribution? One, by explicitly modeling the data and noise generation processes, VAE can learn to separate the two, making it more robust. Two, the latent space can become more interpreble if disentanglement constraints are applied. Three, you can generate new samples by sampling latent vectors and pipe them through the decoder. There are more points and I will leave the technical details to the true professionals. You can find the resources that helped me below.\n\nPlease feel free to point out any inadequacy. Thank you.\n\n<sup>1<\/sup> [Building Autoencoders in Keras - Keras Blog](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)  \n<sup>2<\/sup> [Variational Autoencoders with Tensorflow Probability Layers - Medium](https:\/\/medium.com\/tensorflow\/variational-autoencoders-with-tensorflow-probability-layers-d06c658931b7)  \n<sup>3<\/sup> [Tensorflow Probability VAE Example](https:\/\/github.com\/tensorflow\/probability\/blob\/master\/tensorflow_probability\/examples\/vae.py)  \n<sup>4<\/sup> [Google Colab VAE Interactive Example](https:\/\/colab.research.google.com\/github\/tensorflow\/probability\/blob\/master\/tensorflow_probability\/examples\/jupyter_notebooks\/Probabilistic_Layers_VAE.ipynb#scrollTo=9clSiUTiT3G1)  \n<sup>5<\/sup> [An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2, 1-18.](https:\/\/pdfs.semanticscholar.org\/0611\/46b1d7938d7a8dae70e3531a00fceb3c78e8.pdf)\n\n\n### Contents \n\n1. Raw Dataset\n2. Visualize Preprocessed Transaction Features\n3. Train a Variational Autoencoder\n4. Visualize Latent Representations \n5. VAE vs SVM\n6. Evaluation on Test Set\n7. Limitations ","5a59cdd8":"Above 0.96 in training set, pretty decent!","8525af25":"Notice how the fraud transactions's standard deviations in red are much more scattered and tend to have higher values in both axes. This is inline with the intuition that fraud transactions tend to be erratic.","622109a6":"There is a clear separation between the fraud and non-fraud transactions around the origin [0,0] mean vector, a sign that the VAE is learning something meaningful.","c77484e1":"Define a function that would perform Monte Carlo on inputs to compute the reconstruction probability (given the output data distribution, what would be the probability to generate the original input?). I sample 100 times for each input.","2777143a":"Following my previous attempt, I transform the Time field to time-of-day to account for intraday seasonality. The Amount field is transformed to log scale, with the intuition that the scale of magnitute of a transaction could be a more relevant feature for fraud than linear amounts.","f2c9ad9c":"The sampled latent vector z. The clustering and scattering are consistent with previous observations."}}