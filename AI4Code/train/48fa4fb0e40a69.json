{"cell_type":{"0ca8c25f":"code","cb69b68f":"code","a76285ce":"code","9e7cd082":"code","ebe46de2":"code","f6667756":"code","6caee98c":"code","e89cc1a8":"code","dd78139a":"code","2ce5a265":"code","1c7ef791":"code","b53fc08a":"code","6c406c96":"code","3c0f197f":"code","e18b64a0":"code","872a4b3a":"code","9e924c10":"code","c02c0629":"code","004904d3":"code","135c0405":"code","dfab508d":"code","36488bdb":"code","b988c29b":"code","e2689153":"code","6cff3d63":"code","259eb016":"code","75085981":"code","86b97c3c":"code","5f190405":"code","644a1d3f":"code","d66088da":"code","760ec02a":"code","0d4a51f7":"code","b42a22ef":"code","67b0be11":"code","815a1475":"code","4b14994f":"code","7aecae77":"code","734bb9b7":"code","40c87b5c":"code","0e7ae22a":"code","6ba9399f":"code","57d55de6":"code","2e6d1d03":"code","d62bc0fd":"code","7861bbc8":"code","02eeb7b0":"code","704a3b88":"code","864c2753":"code","d942ef32":"code","03837949":"code","6163fb07":"code","0443b0e0":"code","8a2b42bf":"code","d926eb49":"code","9e5f6bbe":"code","53e71668":"code","308112d1":"code","1e3d65e6":"code","5e331bf7":"code","6253bd4f":"code","12e3ddbd":"code","0fba5851":"code","ef64e7f5":"code","2add65fc":"code","1dbd6727":"code","7833f2cb":"code","5be6e97a":"markdown","63811e84":"markdown","71468853":"markdown","a1fbea18":"markdown","fe063a53":"markdown","95256bd4":"markdown","34610094":"markdown","1ab7513e":"markdown","0a84dece":"markdown","2f97fca6":"markdown","84b9dfa7":"markdown","1d95d835":"markdown","c45ba148":"markdown","9f33ef57":"markdown","0892f830":"markdown","2ae688bf":"markdown","eff05ca7":"markdown","21354b12":"markdown","4ba96302":"markdown","6fa23bf9":"markdown","e57d1531":"markdown","e2f44d1f":"markdown","e8e1551c":"markdown","ffe9119e":"markdown","8cb878dc":"markdown","6a235439":"markdown","53967a15":"markdown","cacfd2cb":"markdown","8a30b559":"markdown","3f82442f":"markdown","0e721a7d":"markdown","f2e001b7":"markdown","10bd8181":"markdown","9c4cfa2f":"markdown","178def73":"markdown"},"source":{"0ca8c25f":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas_profiling as pd_prof\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set()\n\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n","cb69b68f":"poke_data = pd.read_csv('..\/input\/Pokemon.csv')","a76285ce":"poke_data.info()","9e7cd082":"poke_data.head()","ebe46de2":"poke_data = pd.read_csv('..\/input\/Pokemon.csv',index_col='#')","f6667756":"poke_data.head()","6caee98c":"poke_data.describe()","e89cc1a8":"#pd_prof.ProfileReport(poke_data)","dd78139a":"poke_data.Legendary.replace({True:1,False:0},inplace=True)","2ce5a265":"poke_data.describe()","1c7ef791":"p = poke_data.hist(figsize = (20,20))","b53fc08a":"poke_data['Legendary'].value_counts()","6c406c96":"poke_data['Type 1'].value_counts()","3c0f197f":"poke_data['Type 2'].value_counts()","e18b64a0":"type_1_list = list(poke_data['Type 1'].value_counts().index)\ntype_2_list = list(poke_data['Type 2'].value_counts().index)","872a4b3a":"type_1_list.sort()==type_2_list.sort()","9e924c10":"dummy_type_1 = pd.get_dummies(poke_data['Type 1'])\ndummy_type_2 = pd.get_dummies(poke_data['Type 2'])","c02c0629":"dummy_final = pd.DataFrame(index=poke_data.index)\nfor column_name in type_2_list:\n    dummy_final[column_name] = dummy_type_1[column_name] + dummy_type_2[column_name]","004904d3":"dummy_final.head()","135c0405":"dummy_final.describe()","dfab508d":"dummy_final.info()","36488bdb":"# I have a dataframe 'df' like this \n\n# Id    v1    v2\n# 0     A     0.23\n# 1     B     0.65\n# 2     NaN   0.87\n\n# If I use this function\n\n# df1 = get_dummies(df)\n# df1\n\n# Id    v1_A    v1_B    v2\n# 0     1       0       0.23\n# 1     0       1       0.65\n# 2     0       0       0.87 .","b988c29b":"poke_data_new = pd.concat([poke_data,dummy_final],sort=False,axis=1)","e2689153":"poke_data_new.head()","6cff3d63":"poke_data_new.drop(['Type 1','Type 2'],axis=1,inplace=True)\npoke_data_new.info()","259eb016":"poke_data_new['Generation'].value_counts()","75085981":"plt.figure(figsize=(20,20))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(poke_data.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","86b97c3c":"poke_data_new.columns","5f190405":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nnumerical =  pd.DataFrame(sc_X.fit_transform(poke_data_new[['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def',\n       'Speed']]),columns=['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def',\n       'Speed'],index= poke_data_new.index\n        )","644a1d3f":"#numerical\npoke_clean_standard = poke_data_new.copy(deep=True)\npoke_clean_standard[['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def',\n       'Speed']] = numerical[['Total', 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def',\n       'Speed']]","d66088da":"poke_clean_standard.head()","760ec02a":"poke_clean_standard.describe()","0d4a51f7":"x = poke_clean_standard.drop([\"Legendary\",\"Name\"],axis=1)\ny = poke_clean_standard.Legendary","b42a22ef":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y,random_state = 2,test_size=0.4,stratify=y)","67b0be11":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","815a1475":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","4b14994f":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))\n","7aecae77":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')\n","734bb9b7":"#Setup a knn classifier with k neighbors\n#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(7)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","40c87b5c":"y_pred = knn.predict(X_test)","0e7ae22a":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","6ba9399f":"from sklearn.metrics import f1_score\nf1_score(y_test,y_pred)","57d55de6":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","2e6d1d03":"from sklearn.metrics import matthews_corrcoef\nprint(matthews_corrcoef(y_test,y_pred))","d62bc0fd":"from sklearn.metrics import roc_curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=7) ROC curve')\nplt.show()","7861bbc8":"from imblearn.over_sampling import SMOTE","02eeb7b0":"poke_clean_standard.Legendary.value_counts()","704a3b88":"sm = SMOTE(random_state=2, ratio = 'minority')\nx_train_res, y_train_res = sm.fit_sample(X_train, y_train)","864c2753":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(x_train_res,y_train_res)\n    \n    train_scores.append(knn.score(x_train_res,y_train_res))\n    test_scores.append(knn.score(X_test,y_test))","d942ef32":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","03837949":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","6163fb07":"knn = KNeighborsClassifier(2)\n\nknn.fit(x_train_res,y_train_res)\nknn.score(X_test,y_test)","0443b0e0":"y_pred = knn.predict(X_test)","8a2b42bf":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","d926eb49":"from sklearn.metrics import f1_score\nf1_score(y_test,y_pred)","9e5f6bbe":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","53e71668":"from sklearn.metrics import matthews_corrcoef\nprint(matthews_corrcoef(y_test,y_pred))","308112d1":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek(ratio='auto')\nx_train_res, y_train_res = smt.fit_sample(X_train, y_train)\n","1e3d65e6":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(x_train_res,y_train_res)\n    \n    train_scores.append(knn.score(x_train_res,y_train_res))\n    test_scores.append(knn.score(X_test,y_test))","5e331bf7":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","6253bd4f":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","12e3ddbd":"knn = KNeighborsClassifier(2)\n\nknn.fit(x_train_res,y_train_res)\nknn.score(X_test,y_test)","0fba5851":"y_pred = knn.predict(X_test)","ef64e7f5":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","2add65fc":"from sklearn.metrics import f1_score\nf1_score(y_test,y_pred)","1dbd6727":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","7833f2cb":"from sklearn.metrics import matthews_corrcoef\nprint(matthews_corrcoef(y_test,y_pred))","5be6e97a":"#### So it is visible that get_dummies function converts Nans to 0 and doesn't form a separate column for them like it does for other categories. Our problem has been solved thanks to get_dummies()","63811e84":"#### Let's check now","71468853":"## Bivariate EDA","a1fbea18":"#### Now iterating on the name of the categories present in one of the columns (any one because both columns have same categories), the values of the columns having same name would be added together and stored in a third dataframe with the same index as the initial dataframe.","fe063a53":"## ROC Curve","95256bd4":"## Result visualisation","34610094":"#### No doubt the data would require to be scaled before modelling can take place. The max values of all columns are varying widely.\n#### Apart from that none of the statistics seem to be unrealistic like negative values etc. so lets move forward.","1ab7513e":"## Confusion Matrix","0a84dece":"# SMOTE Tomek Method","2f97fca6":"#### Now we can concatenate this dataframe with the initial dataframe and drop the type 1 and type 2 columns","84b9dfa7":"#### Noticable that none of the factors seem to have a correlation with the target value higher than 0.70","1d95d835":"#### This # column seems to be useful as the index for this dataframe, Let's load the dataframe again with # as index","c45ba148":"#### We should replace the bool values in Legendary column with binary values","9f33ef57":"#### Now let's do a basic check on this!","0892f830":"# Matthew Correlation Coefficient Score","2ae688bf":"### Unique Values in each column","eff05ca7":"#### Let's do a summary statistic analysis for the data","21354b12":"#### By printing the info we can see that this new dataframe doesn't contain any Nan values whereas we had seen that the type 2 column had Nans. So what just happened? Where did the Nan values go?\n\n#### Let's try to understand using a very raw example that I tried to depict below. ","4ba96302":"#### Column 'Total' is the Sum of Attack, Sp. Atk, Defense, Sp. Def, Speed and HP","6fa23bf9":"#### Perfect we'll be using this one from now. ","e57d1531":"# SMOTE for Class Imbalance","e2f44d1f":"#### To check whether the steps we took are correct, summary stats are printed for the new dataframe. None of the columns contains a max value greater than one. Hence there's nothing to worry about.","e8e1551c":"#### Some of the factors like Sp. Atk, Defence seem to be skewed.","ffe9119e":"### If get_dummies is directly applied to change this categorical data to binary, the function will produce columns with the same name because of the presence of same categories in both columns and due to the nature of get_dummies function. This would cause high dimensionality and unnecessary redundancy.\n#### To deal with this problem dummies of the two columns are created separately.","8cb878dc":"## Initial EDA","6a235439":"# Classification Report","53967a15":"#### Great class imbalance!","cacfd2cb":"#### Two columns have the same categories. This may cause issues with one hot encoding. Let's try to solve for this.\n#### Beginning by checking what values the two columns contain and are they same.","8a30b559":"#### This shows that the values in both the type columns are same. That means no column has a different category.","3f82442f":"#### Initially only one column seems to have null values, 'Type 2'. Going by the data description it is the second column to depict the type of pokemon so it is acceptable to be null at some places because it is not necessary that every pokemon would have more than one type.","0e721a7d":"#### The best result is captured at k = 7 hence 7 is used for the final model ","f2e001b7":"#### Finally let's see the column Generation","10bd8181":"# Yet To Be Updated","9c4cfa2f":"## Skewness\n\nA ***left-skewed distribution*** has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That\u2019s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA ***right-skewed distribution*** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That\u2019s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n\n![](https:\/\/www.statisticshowto.datasciencecentral.com\/wp-content\/uploads\/2014\/02\/pearson-mode-skewness.jpg)\n\n\n#### to learn more about skewness\nhttps:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/skewed-distribution\/","178def73":"# F1 Score"}}