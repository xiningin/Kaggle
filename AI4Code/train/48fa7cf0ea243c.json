{"cell_type":{"9074773a":"code","60295a16":"code","28b43e2c":"code","a04aee1d":"code","83862983":"code","aecab4f8":"code","4638d3e1":"code","64522ac2":"code","a6a0e036":"code","1f41c491":"code","b2ac146b":"code","5a006cbc":"code","604e1148":"code","dcec76b0":"code","81d30bf2":"code","514f9fcd":"markdown","99f6fc90":"markdown","44029435":"markdown","ac33b97f":"markdown","0ebf1eb5":"markdown","22c01124":"markdown","55ddb559":"markdown","864d59fb":"markdown"},"source":{"9074773a":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom sklearn.metrics import f1_score\nimport torchtext\nfrom tqdm import tqdm, tqdm_notebook\nfrom nltk import word_tokenize\nimport random\nfrom torch import optim","60295a16":"text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize, fix_length=70)\nqid = torchtext.data.Field()\ntarget = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\ntrain = torchtext.data.TabularDataset(path='..\/input\/train.csv', format='csv',\n                                      fields={'question_text': ('text',text),\n                                              'target': ('target',target)})\ntest = torchtext.data.TabularDataset(path='..\/input\/test.csv', format='csv',\n                                     fields={'qid': ('qid', qid),\n                                             'question_text': ('text', text)})","28b43e2c":"text.build_vocab(train, test, min_freq=3)\nqid.build_vocab(test)","a04aee1d":"glove = torchtext.vocab.Vectors('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')\ntqdm_notebook().pandas() ","83862983":"text.vocab.set_vectors(glove.stoi, glove.vectors, dim=300)","aecab4f8":"class TextCNN(nn.Module):\n    \n    def __init__(self, lm, padding_idx, static=True, kernel_num=128, fixed_length=50, kernel_size=[2, 5, 10], dropout=0.2):\n        super(TextCNN, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.embedding = nn.Embedding.from_pretrained(lm)\n        if static:\n            self.embedding.weight.requires_grad = False\n        self.embedding.padding_idx = padding_idx\n        self.conv = nn.ModuleList([nn.Conv2d(1, kernel_num, (i, self.embedding.embedding_dim)) for i in kernel_size])\n        self.maxpools = [nn.MaxPool2d((fixed_length+1-i,1)) for i in kernel_size]\n        self.fc = nn.Linear(len(kernel_size)*kernel_num, 1)\n        \n    def forward(self, input):\n        x = self.embedding(input).unsqueeze(1)  # B X Ci X H X W\n        x = [self.maxpools[i](torch.tanh(cov(x))).squeeze(3).squeeze(2) for i, cov in enumerate(self.conv)]  # B X Kn\n        x = torch.cat(x, dim=1)  # B X Kn * len(Kz)\n        y = self.fc(self.dropout(x))\n        return y","4638d3e1":"def search_best_f1(true, pred):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in np.arange(0.1, 0.501, 0.01):\n        tmp[1] = f1_score(true, np.array(pred)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    return tmp[2], delta\n\ndef training(epoch, model, loss_func, optimizer, train_iter):\n    e = 0\n    \n    while e < epoch:\n        train_iter.init_epoch()\n        losses, preds, true = [], [], []\n        for train_batch in tqdm(list(iter(train_iter)), 'epcoh {} training'.format(e)):\n            model.train()\n            x = train_batch.text.cuda()\n            y = train_batch.target.type(torch.Tensor).cuda()\n            true.append(train_batch.target.numpy())\n            model.zero_grad()\n            pred = model.forward(x).view(-1)\n            loss = loss_function(pred, y)\n            preds.append(torch.sigmoid(pred).cpu().data.numpy())\n            losses.append(loss.cpu().data.numpy())\n            loss.backward()\n#             clip_grad_norm_(model.parameters(), 2)\n            optimizer.step()\n        train_f1, alpha_train = search_best_f1([j for i in true for j in i], [j for i in preds for j in i])\n        print('epcoh {:02} - train_loss {:.4f} - train f1 {:.4f} - delta {:.4f}'.format(\n                            e, np.mean(losses), train_f1, alpha_train))\n                \n        e += 1\n    return alpha_train\n                ","64522ac2":"random.seed(1234)\nbatch_size = 512\ntrain_iter = torchtext.data.BucketIterator(dataset=train,\n                                               batch_size=batch_size,\n                                               shuffle=True,\n                                               sort=False)","a6a0e036":"def init_network(model, method='xavier', exclude='embedding', seed=123):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    for name, w in model.named_parameters():\n        if not exclude in name:\n            if 'weight' in name:\n                if method is 'xavier':\n                    nn.init.xavier_normal_(w)\n                elif method is 'kaiming':\n                    nn.init.kaiming_normal_(w)\n                else:\n                    nn.init.normal_(w)\n            elif 'bias' in name:\n                nn.init.constant_(w, 0.0)\n            else: \n                pass\n\ndef print_model(model, ignore='embedding'):\n    total = 0\n    for name, w in model.named_parameters():\n        if not ignore or ignore not in name:\n            total += w.nelement()\n            print('{} : {}  {} parameters'.format(name, w.shape, w.nelement()))\n    print('-------'*4)\n    print('Total {} parameters'.format(total))","1f41c491":"text.fix_length = 70\nmodel = TextCNN(text.vocab.vectors, padding_idx=text.vocab.stoi[text.pad_token], kernel_size=[1, 2, 3, 5], kernel_num=128, static=False, fixed_length=text.fix_length, dropout=0.1).cuda()\ninit_network(model)\noptimizer = optim.Adam(params=model.parameters(), lr=1e-3)\nloss_function = nn.BCEWithLogitsLoss()\nprint_model(model, ignore=None)","b2ac146b":"alpha = training(3, model, loss_function, optimizer, train_iter)","5a006cbc":"def predict(model, test_list):\n    pred = []\n    with torch.no_grad():\n        for test_batch in test_list:\n            model.eval()\n            x = test_batch.text.cuda()\n            pred += torch.sigmoid(model.forward(x).view(-1)).cpu().data.numpy().tolist()\n    return pred","604e1148":"test_list = list(torchtext.data.BucketIterator(dataset=test,\n                                    batch_size=batch_size,\n                                    sort=False,\n                                    train=False))","dcec76b0":"preds = predict(model, test_list)\nsub = pd.DataFrame()\nsub['qid'] = [qid.vocab.itos[j] for i in test_list for j in i.qid.view(-1).numpy()]\nsub['prediction'] = (preds > alpha).astype(int)\nsub.head()","81d30bf2":"sub.to_csv(\"submission.csv\", index=False)","514f9fcd":"### Network","99f6fc90":"### Training Strategy","44029435":"### Predict","ac33b97f":"### Network Init","0ebf1eb5":"### Load Corpus","22c01124":"### Batch Set and Train\/Validation Split","55ddb559":"### Build Vocabulary","864d59fb":"### Load Pretrained Language Model"}}