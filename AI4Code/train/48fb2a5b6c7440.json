{"cell_type":{"bda501cb":"code","3cf5306c":"code","864b6dfd":"code","6e700937":"code","16cd4c55":"code","f23fa10b":"code","6d7956ff":"code","4917f4de":"code","ed72c0e3":"code","ee4bbbc8":"code","9dd5bdc4":"code","742bc494":"code","179113be":"code","fe5dc2e3":"code","4a706417":"code","d9633526":"code","c310a18d":"code","213a634e":"code","88d3a747":"code","e47793a8":"code","776fc12b":"code","26468692":"markdown","a8b35b8a":"markdown","2d0c3771":"markdown","3cbdb715":"markdown","7f7c3385":"markdown","6036a944":"markdown","11b81f97":"markdown","4d4d75d9":"markdown","1d9a4a2b":"markdown","24620bdd":"markdown","484df09a":"markdown","70790b6a":"markdown"},"source":{"bda501cb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom statistics import mode, mean\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","3cf5306c":"data = pd.read_csv('..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')","864b6dfd":"fig, ax = plt.subplots(figsize=(9,9))\n\nsns.heatmap(data.corr(), square=True, annot=True, cbar=False,  ax=ax);\n# and we can see that here no height correlation","6e700937":"data.isnull().sum()","16cd4c55":"data = data.dropna(axis='rows', thresh=15)\ndata.isnull().sum()","f23fa10b":"data[\"education\"]=data[\"education\"].fillna(mode(data[\"education\"]))\ndata[\"BPMeds\"]=data[\"BPMeds\"].fillna(mode(data[\"BPMeds\"]))\n\ndata[\"cigsPerDay\"]=data[\"cigsPerDay\"].fillna((data[\"cigsPerDay\"].mean()))\ndata[\"totChol\"]=data[\"totChol\"].fillna((data[\"totChol\"].mean()))\ndata[\"BMI\"]=data[\"BMI\"].fillna((data[\"BMI\"].mean()))\ndata[\"heartRate\"]=data[\"heartRate\"].fillna((data[\"heartRate\"].mean()))\ndata[\"glucose\"]=data[\"glucose\"].fillna(data[\"glucose\"].mean())","6d7956ff":"data.isnull().any()","4917f4de":"for col in data.columns[:-1]:\n    pd.crosstab(data[col], data.TenYearCHD).plot(kind='bar')\n    plt.xlabel(col)","ed72c0e3":"data = data.drop(columns='currentSmoker')","ee4bbbc8":"X = data[['male','age','education','cigsPerDay','BPMeds','prevalentStroke','prevalentHyp','diabetes','totChol','sysBP','diaBP','BMI','heartRate','glucose']]\ny = pd.Series(data['TenYearCHD'])","9dd5bdc4":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)","742bc494":"scaler = StandardScaler()\nX_train = scaler.fit_transform(x_train)\nX_test = scaler.transform(x_test)","179113be":"np.around(X_train.mean(axis = 0), 10)","fe5dc2e3":"X_train.std(axis = 0)","4a706417":"model = LogisticRegression()","d9633526":"model.fit(X_train, y_train);","c310a18d":"labels = model.predict(X_test)","213a634e":"accuracy_score(y_test, labels)","88d3a747":"acc = np.array([])\nfor i in range(0, 100, 10):\n    y_pred_new_threshold = (model.predict_proba(X_test)[:, 1]>= i\/100).astype(int)\n    newscore = accuracy_score(y_test, y_pred_new_threshold)\n    acc = np.append(acc, [y_pred_new_threshold])\nacc = acc.astype(int)\nacc = acc.reshape(10,-1)","e47793a8":"i=0\nfor l in acc:\n    print('***', i, '***')\n    matrix = confusion_matrix(y_test, l)\n    print('\\n', matrix)\n    print(classification_report(y_test, l))\n    i+=1","776fc12b":"logit_roc_auc = roc_auc_score(y_test, labels)\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","26468692":"![image.png](attachment:image.png)","a8b35b8a":"We need to normalize our data, and shift the mean to the origin. This is important to get accurate results because of the nature of the logistic equation. This is done by the normalize method.\nStandardScaler transforms the data in such a manner that it has mean as 0 and standard deviation as 1. In short, it standardizes the data. ","2d0c3771":"Delete from data that rows where >=2 no data available.","3cbdb715":"In some situations it is possible to maximize either recall or precision at the expense of another metric. For example, when pre-screening patients for follow \u2014 up, we would probably like to get a review of about 1.0-we want to find all patients who actually have the disease and we can accept low accuracy if the cost of follow-up is not significant. However, there is a simpler metric that takes into account both accuracy and recall, and so you can aim to maximize this number to make the model better. This F1-score that is a harmonic mean of precision and recall.\n**And now we see that F-1-score of model = 86%**.\nSo it is better result who don`t have disease. And for creatind better model to need more data.","7f7c3385":"Check that the mean of each feature (column) is 0","6036a944":"### Research model ###","11b81f97":"So we have categorical data:\n- education\n- BPMeds\nTo fill data by mode.\n\nContinuous data:\n- cigsPerDay\n- totChol\n- BMI\n- heartRate\n- glucose\nTo fill data by mean.","4d4d75d9":"Only 'currentSmoker' not be a good predictor of the outcome.","1d9a4a2b":"### Clearing data ###","24620bdd":"### Train model ###","484df09a":"Check that the std of each feature (column) is 1","70790b6a":"It is the best score in this model and you will see why we stay on it:\nCreating an array containing labels depending on the specified threshold and looking at the results classification_report."}}