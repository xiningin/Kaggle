{"cell_type":{"f0552ff8":"code","d9012290":"code","1d506bde":"code","16271672":"code","090dcb95":"code","5ef39e45":"code","70a8c531":"code","fe5fde34":"code","cdf757ee":"code","fe1361b2":"code","1b21f381":"code","7a31e9ac":"code","0c277d64":"code","19e1ea08":"code","efc057bd":"code","1962666c":"code","cac0e004":"code","96088de9":"code","d7758136":"code","46182a52":"code","0000b9e9":"code","166c9c1f":"code","651e335f":"code","89ab51c8":"code","1222d988":"code","bb15ec0e":"code","b2d67b60":"markdown","8453cc20":"markdown","243edd63":"markdown","beb0f481":"markdown","17e606fb":"markdown"},"source":{"f0552ff8":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.mlab as mlab\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(action='ignore')","d9012290":"heart_df=pd.read_csv(\"..\/input\/framingham-heart-study-dataset\/framingham.csv\")\nheart_df.drop(['education'],axis=1,inplace=True)\nheart_df.head()","1d506bde":"heart_df.isnull().sum()","16271672":"count=0\nfor i in heart_df.isnull().sum(axis=1):\n    if i>0:\n        count=count+1\nprint('Total number of rows with missing values is ', count)\nprint('since it is only',round((count\/len(heart_df.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')","090dcb95":"heart_df.dropna(axis=0,inplace=True)\ndata = heart_df.copy()","5ef39e45":"table=pd.crosstab(data.male,data.TenYearCHD)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.xlabel('Gender')","70a8c531":"table=pd.crosstab(data.currentSmoker,data.TenYearCHD)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.xlabel('Current Smoker')","fe5fde34":"%matplotlib inline\nplt.figure(figsize=(20,18))\npd.crosstab(data.cigsPerDay,data.TenYearCHD).plot(kind='bar')\nplt.xlabel('Cigs per Day')","cdf757ee":"import seaborn as sns\nsns.countplot(x='TenYearCHD',data=data)\n\ndata.TenYearCHD.value_counts()","fe1361b2":"data.columns","1b21f381":"columns = ['male', 'age', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n       'diaBP', 'BMI', 'heartRate', 'glucose']\nX = data[columns]\nX","7a31e9ac":"y = data[['TenYearCHD']]\ny","0c277d64":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nsmote = SMOTE(random_state = 0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nos_data_X,os_data_y = smote.fit_sample(X_train, y_train)","19e1ea08":"os_data_X","efc057bd":"os_data_y","1962666c":"# we can Check the numbers of our data\nprint(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of no subscription in oversampled data\",len(os_data_y[os_data_y['TenYearCHD']==0]))\nprint(\"Number of subscription\",len(os_data_y[os_data_y['TenYearCHD']==1]))\nprint(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['TenYearCHD']==0])\/len(os_data_X))\nprint(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['TenYearCHD']==1])\/len(os_data_X))","cac0e004":"col = os_data_X.columns\ncol","96088de9":"from sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\n\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(os_data_X, os_data_y.values.ravel())\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(os_data_X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","d7758136":"#RFE has helped us choose the following columns\ncols = ['male', 'age', 'currentSmoker', 'cigsPerDay', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'sysBP', 'heartRate']","46182a52":"import statsmodels.api as sm\nlogit_model = sm.Logit(y,X)\nresult = logit_model.fit()\nprint(result.summary2())","0000b9e9":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\nos_data_X = os_data_X[cols]\nX_test = X_test[cols]","166c9c1f":"logreg = LogisticRegression()\nlogreg.fit(os_data_X, os_data_y)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)","651e335f":"print('Train\/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","89ab51c8":"confusion_matrix(y_test, y_pred)","1222d988":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","bb15ec0e":"# 10-fold cross-validation logistic regression\nlogreg = LogisticRegression()\n# Use cross_val_score function\n# We are passing the entirety of X and y since cross validation takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg, X, y, cv=10, scoring='roc_auc')\n\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % -scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())","b2d67b60":"# Recursive Feature Elimination","8453cc20":"# K-fold Cross Validation","243edd63":"# Logistic Regression Model Fitting","beb0f481":"Goal: predict whether a patient has 10-year risk of future coronary heart disease (CHD).","17e606fb":"# SMOTE"}}