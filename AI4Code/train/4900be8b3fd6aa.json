{"cell_type":{"b7969c75":"code","7f136433":"code","445fe6fd":"code","433bd5a0":"code","d7845318":"code","3cee1731":"code","9bfbdeed":"code","85b80cc3":"code","da375461":"code","267b8e6c":"code","2e823b76":"code","34ad5a6b":"code","22de9fc0":"code","2108fcc6":"code","6848d57a":"code","861e2531":"code","d896ff04":"code","7d001df7":"code","f080dcee":"code","c6553bec":"code","c2d0f087":"code","609e6318":"code","e895331a":"code","81cb9b97":"code","121df58c":"code","c97255f3":"code","c3602ecc":"code","c920be94":"code","3cfd13d4":"code","c755348e":"code","d4f90d70":"code","f31700af":"code","5f83c56a":"code","3f5d9dfa":"code","4f09fcf9":"code","d313a7e4":"code","984e92ba":"code","b1dc5709":"code","5114d6bc":"code","d113e1ed":"code","03a16f57":"code","e0d4eca9":"code","03f043f8":"code","79e0a175":"code","e0f5a145":"code","326360e9":"markdown","62d21000":"markdown","3f1d8d68":"markdown","769caf1b":"markdown","5a3f4208":"markdown","d29a1076":"markdown","9e485202":"markdown","85daa4e3":"markdown"},"source":{"b7969c75":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm","7f136433":"\"\"\"posted code to generate csv: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\"\"\"\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n    \ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n        \n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","445fe6fd":"biorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\nbio_files = load_files(biorxiv_dir)\nbio_df = generate_clean_df(bio_files)","433bd5a0":"pmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)","d7845318":"comm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\ncomm_files = load_files(comm_dir)\ncomm_df = generate_clean_df(comm_files)","3cee1731":"noncomm_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\nnoncomm_files = load_files(noncomm_dir)\nnoncomm_df = generate_clean_df(noncomm_files)","9bfbdeed":"data = pd.concat([bio_df,pmc_df,comm_df,noncomm_df])","85b80cc3":"\"\"\"I have created a list of indecies for social articles, i created this list using LDA tagging\"\"\"\nsocial_index=pd.read_csv('\/kaggle\/input\/index-social\/index_social.csv')\nsocial_index","da375461":"data2=data.iloc[social_index['Unnamed: 0'],:].copy()","267b8e6c":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras","2e823b76":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel","34ad5a6b":"stop_words={'ourselves', 'hers', 'between', 'yourself', 'but', 'again', \\\n            'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with',\\\n            'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such',\\\n            'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who',\\\n            'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we',\\\n            'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\\\n            'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above',\\\n            'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', \\\n            'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will',\\\n            'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over',\\\n            'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', \\\n            'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which',\\\n            'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', \\\n            'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than',\\\n            ')','(',',',';','et','al','.'   }","22de9fc0":"\"\"\"\nhttps:\/\/pythonhealthcare.org\/category\/natural-language-processing\/\nHere we process the data in the following ways:\n  1) change all text to lower case\n  2) tokenize (breaks text down into a list of words)\n  3) remove punctuation and non-word text\n  4) find word stems (e.g. runnign, run and runner will be converted to run)\n  5) removes stop words (commonly occuring words of little value, e.g. 'the')\n\"\"\"\nstemming = PorterStemmer()\nstops = stop_words\n\ndef clean_text(raw_text):\n    \"\"\"This function works on a raw text string, and:\n        1) changes to lower case\n        2) tokenizes (breaks text down into a list of words)\n        3) removes punctuation and non-word text\n        4) finds word stems\n        5) removes stop words\n        6) rejoins meaningful stem words\"\"\"\n    \n    # Convert to lower case\n    text = raw_text.lower()\n    \n    # Tokenize\n    tokens = nltk.word_tokenize(text)\n    \n    # Keep only words (removes punctuation + numbers)\n    # use .isalnum to keep also numbers\n    token_words = [w for w in tokens if w.isalpha()]\n    \n    # Stemming\n    stemmed_words = [stemming.stem(w) for w in token_words]\n    \n    # Remove stop words\n    meaningful_words = [w for w in stemmed_words if not w in stops]\n      \n    # Return cleaned data\n    return meaningful_words\ndef apply_cleaning_function_to_list(X):\n    cleaned_X = []\n    for element in X:\n        cleaned_X.append(clean_text(element))\n    return cleaned_X\n\n\"\"\"\nThe frequency of all words is counted. Words are then given an index number so\nthat th emost commonly occurring words hav ethe lowest number (so the \ndictionary may then be truncated at any point to keep the most common words).\nWe avoid using the index number zero as we will use that later to 'pad' out\nshort text.\n\"\"\"\n\ndef training_text_to_numbers(text, cutoff_for_rare_words = 1):\n    \"\"\"Function to convert text to numbers. Text must be tokenzied so that\n    test is presented as a list of words. The index number for a word\n    is based on its frequency (words occuring more often have a lower index).\n    If a word does not occur as many times as cutoff_for_rare_words,\n    then it is given a word index of zero. All rare words will be zero.\n    \"\"\"\n    # Flatten list if sublists are present\n    if len(text) > 1:\n        flat_text = [item for sublist in text for item in sublist]\n    else:\n        flat_text = text\n    \n    # get word freuqncy\n    fdist = nltk.FreqDist(flat_text)\n\n    # Convert to Pandas dataframe\n    df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n    df_fdist.columns = ['Frequency']\n\n    # Sort by word frequency\n    df_fdist.sort_values(by=['Frequency'], ascending=False, inplace=True)\n\n    # Add word index\n    number_of_words = df_fdist.shape[0]\n    df_fdist['word_index'] = list(np.arange(number_of_words)+1)\n    # Convert pandas to dictionary\n    word_dict = df_fdist['Frequency'].to_dict()\n    \n    # Use dictionary to convert words in text to numbers\n    text_numbers = []\n    for string in text:\n        string_numbers = [word for word in string if 2<word_dict[word]<10000]\n        text_numbers.append(string_numbers)\n    \n    return (text_numbers, df_fdist)","2108fcc6":"data2['cleaned_text'] = apply_cleaning_function_to_list(data2.text)\nnumbered_text, dict_df = training_text_to_numbers(data2['cleaned_text'].values)\ndata2=data2.assign(numbered_text=numbered_text)","6848d57a":"#1 sentence split\ndef sentance(text):\n    sentences=list()\n    sentence_list=nltk.sent_tokenize(text)\n    for sent in sentence_list:\n        if sent.find('copyright') != -1:\n            continue\n        elif sent.find('https') != -1:\n            continue\n        else:             \n            sentences.append(sent)\n    return(sentences)\ndata2['sentences']= [ sentance(i) for i in data2.text ]","861e2531":"data2['sentences2']= [' '.join(i) for i in data2.sentences ]","d896ff04":"sentences=list()\nfor lst in data2.sentences:\n    for sent in lst:\n        sentences.append(sent)\nsentences2 = apply_cleaning_function_to_list(sentences)","7d001df7":"from gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(data2.numbered_text))]\n#model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n#model.save('model')","f080dcee":"model=Doc2Vec.load('\/kaggle\/input\/doc2vec-model\/model')","c6553bec":"#to create a new vector\nvector = model.infer_vector(apply_cleaning_function_to_list(['ethical and social'])[0])\n\n# to find the siilarity with vector\nmodel.similar_by_vector(vector)\n\n# to find the most similar word to words in 2 document\nmodel.wv.most_similar(documents[1][0])\n\n#find similar documents to document 1\nsim_docs=model.docvecs.most_similar(1)","c2d0f087":"# Create Dictionary\nid2word = corpora.Dictionary(data2.numbered_text)\n# Create Corpus\ntexts = data2.numbered_text\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=5, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","609e6318":"# Print the Keyword in the 10 topics\nprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","e895331a":"# Visualize the topics\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","81cb9b97":"from gensim.models import Word2Vec\nmodel = Word2Vec(data2.numbered_text, min_count=1,size= 50,workers=3, window =3, sg = 1, seed=1)\nwords = list(model.wv.vocab)","121df58c":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as pyplot\nX = model[model.wv.vocab]\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\npyplot.scatter(result[:, 0], result[:, 1])","c97255f3":"words = list(model.wv.vocab)\nfor i, word in enumerate(words):\n\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))","c3602ecc":"model.most_similar(positive=['social'], topn=10)","c920be94":"import heapq\nmaximum_frequncy = max(dict_df['Frequency'])\ndict_df['Frequency']=dict_df['Frequency']\/maximum_frequncy\n\n#https:\/\/stackabuse.com\/text-summarization-with-nltk-in-python\/\ndef sentance_summary(text):\n    sentence_scores = {}\n    sentence_list=nltk.sent_tokenize(text)\n    for sent in sentence_list:\n        if sent.find('copyright') != -1:\n            continue\n        else:             \n            for word in clean_text(sent):\n                if word in dict_df.index:\n                    if len(sent.split(' ')) < 30:\n                        if sent not in sentence_scores.keys():\n                            sentence_scores[sent] = dict_df[dict_df.index==word][\"Frequency\"].tolist()[0]\n                        elif word == 'copyright':\n                            sentence_scores[sent] += -10\n                        else:\n                            sentence_scores[sent] += dict_df[dict_df.index==word][\"Frequency\"].tolist()[0]\n    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n    summary = ' '.join(summary_sentences)\n    return(summary)","3cfd13d4":"data3=data2.reset_index()\ndata3['summary']= [ sentance_summary(i) for i in data3.sentences2 ]","c755348e":"data3['abstract'][sim_docs[0][0]]","d4f90d70":"data3['summary'][sim_docs[0][0]]","f31700af":"import spacy\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = spacy.load(\"en_core_web_sm\")\ndata2['clean_text']=[' '.join(i) for i in data2.numbered_text]","5f83c56a":"#https:\/\/www.kaggle.com\/shivamb\/spacy-text-meta-features-knowledge-graphs\nfrom collections import Counter \nfeats = ['char_count', 'word_count', 'word_count_cln',\n       'stopword_count', '_NOUN', '_VERB', '_ADP', '_ADJ', '_DET', '_PROPN',\n       '_INTJ', '_PUNCT', '_NUM', '_PRON', '_ADV', '_PART', '_amod', '_ROOT',\n       '_punct', '_advmod', '_auxpass', '_nsubjpass', '_ccomp', '_acomp',\n       '_neg', '_nsubj', '_aux', '_agent', '_det', '_pobj', '_prep', '_csubj',\n       '_nummod', '_attr', '_acl', '_relcl', '_dobj', '_pcomp', '_xcomp',\n       '_cc', '_conj', '_mark', '_prt', '_compound', '_dep', '_advcl',\n       '_parataxis', '_poss', '_intj', '_appos', '_npadvmod', '_predet',\n       '_case', '_expl', '_oprd', '_dative', '_nmod']\nclass AutomatedTextFE:\n    def __init__(self):\n        self.pos_tags = ['NOUN', 'VERB', 'ADP', 'ADJ', 'DET', 'PROPN', 'INTJ', 'PUNCT',\\\n                         'NUM', 'PRON', 'ADV', 'PART']\n        self.dep_tags = ['amod', 'ROOT', 'punct', 'advmod', 'auxpass', 'nsubjpass',\\\n                         'ccomp', 'acomp', 'neg', 'nsubj', 'aux', 'agent', 'det', 'pobj',\\\n                         'prep', 'csubj', 'nummod', 'attr', 'acl', 'relcl', 'dobj', 'pcomp', \\\n                         'xcomp', 'cc', 'conj', 'mark', 'prt', 'compound', 'dep', 'advcl',\\\n                         'parataxis', 'poss', 'intj', 'appos', 'npadvmod', 'predet', 'case',\\\n                         'expl', 'oprd', 'dative', 'nmod']\n\ndef _spacy_cleaning(doc):\n    toks = [t for t in doc if (t.is_stop == False)]\n    toks = [t for t in toks if (t.is_punct == False)]\n    words = [t.lemma_ for token in toks]\n    return \" \".join(words)\ndef _spacy_features(df):\n    df[\"clean_text\"] = df[c].apply(lambda x : _spacy_cleaning(x))\n    df[\"char_count\"] = df[textcol].apply(len)\n    df[\"word_count\"] = df[c].apply(lambda x : len([_ for _ in x]))\n    df[\"word_count_cln\"] = df[\"clean_text\"].apply(lambda x : len(x.split()))\n    df[\"stopword_count\"] = df[c].apply(lambda x : len([_ for _ in x if _.is_stop]))\n    df[\"pos_tags\"] = df[c].apply(lambda x : dict(Counter([_.head.pos_ for _ in x])))\n    df[\"dep_tags\"] = df[c].apply(lambda x : dict(Counter([_.dep_ for _ in x])))\n    return df \n\nclass AutomatedTextFE:\n    def __init__(self, df, textcol):\n        self.df = df\n        self.textcol = textcol\n        self.c = \"spacy_\" + textcol\n        self.df[self.c] = self.df[self.textcol].apply( lambda x : nlp(x))\n        \n        self.pos_tags = ['NOUN', 'VERB', 'ADP', 'ADJ', 'DET', 'PROPN', 'INTJ', 'PUNCT',\\\n                         'NUM', 'PRON', 'ADV', 'PART']\n        self.dep_tags = ['amod', 'ROOT', 'punct', 'advmod', 'auxpass', 'nsubjpass',\\\n                         'ccomp', 'acomp', 'neg', 'nsubj', 'aux', 'agent', 'det', 'pobj',\\\n                         'prep', 'csubj', 'nummod', 'attr', 'acl', 'relcl', 'dobj', 'pcomp', \\\n                         'xcomp', 'cc', 'conj', 'mark', 'prt', 'compound', 'dep', 'advcl',\\\n                         'parataxis', 'poss', 'intj', 'appos', 'npadvmod', 'predet', 'case',\\\n                         'expl', 'oprd', 'dative', 'nmod']\n        \n    def _spacy_cleaning(self, doc):\n        tokens = [token for token in doc if (token.is_stop == False)\\\n                  and (token.is_punct == False)]\n        words = [token.lemma_ for token in tokens]\n        return \" \".join(words)\n        \n    def _spacy_features(self):\n        self.df[\"clean_text\"] = self.df[self.c].apply(lambda x : self._spacy_cleaning(x))\n        self.df[\"char_count\"] = self.df[self.textcol].apply(len)\n        self.df[\"word_count\"] = self.df[self.c].apply(lambda x : len([_ for _ in x]))\n        self.df[\"word_count_cln\"] = self.df[\"clean_text\"].apply(lambda x : len(x.split()))\n        \n        self.df[\"stopword_count\"] = self.df[self.c].apply(lambda x : \n                                                          len([_ for _ in x if _.is_stop]))\n        self.df[\"pos_tags\"] = self.df[self.c].apply(lambda x :\n                                                    dict(Counter([_.head.pos_ for _ in x])))\n        self.df[\"dep_tags\"] = self.df[self.c].apply(lambda x :\n                                                    dict(Counter([_.dep_ for _ in x])))\n        \n    def _flatten_features(self):\n        for key in self.pos_tags:\n            self.df[\"_\" + key] = self.df[\"pos_tags\"].apply(lambda x : \\\n                                                           x[key] if key in x else 0)\n        \n        for key in self.dep_tags:\n            self.df[\"_\" + key] = self.df[\"dep_tags\"].apply(lambda x : \\\n                                                           x[key] if key in x else 0)\n                \n    def generate_features(self):\n        self._spacy_features()\n        self._flatten_features()\n        self.df = self.df.drop([self.c, \"pos_tags\", \"dep_tags\", \"clean_text\"], axis=1)\n        return self.df\n    \ndef spacy_features(df, tc):\n    fe = AutomatedTextFE(df, tc)\n    return fe.generate_features()","3f5d9dfa":"textcol = \"clean_text\"\nfeats_df=spacy_features(data2, textcol)","4f09fcf9":"# document level spacy labeling\nNoun=[]\nVerb=[]\nfor article in data2.spacy_clean_text:\n    #ents = [(e.text, e.start_char, e.end_char, e.label_) for e in article.ents]\n    rel=[(token.text, token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\\\n         for token in article]\n    rel = pd.DataFrame(rel,columns =[\"TEXT\",'DEP','HEAD TEXT','HEAD POS','CHILDREN']) \n    #Get verbs and no null children\n    rel2=rel[([(i!=[]) for i in rel['CHILDREN']]) &  (rel['HEAD POS'] =='VERB')]\n    tmp=[[(i, row[1][0]) for i in  row[1][1]] for row in rel2[[\"HEAD TEXT\",\"CHILDREN\"]].iterrows() ]\n    for i in tmp:\n        for n,v in i:\n            Noun.append(n.text)\n            Verb.append(v)\n","d313a7e4":"tmp=pd.DataFrame([Noun, Verb]).transpose()\ntmp.columns=['Noun','Verb']\ntmp3=tmp.groupby(['Noun','Verb']).size().reset_index()","984e92ba":"tmp3=tmp3[(tmp3[0]>5) & (tmp3[0]<10)]","b1dc5709":"!pip install pyvis","5114d6bc":"Noun=tmp3.groupby('Noun', as_index = False).size().reset_index()\nVerb=tmp3.groupby('Verb', as_index = False).size().reset_index()","d113e1ed":"words=Verb[Verb[0]>500]['Verb'].tolist() + Noun[Noun[0]>500]['Noun'].tolist()\ntmp4=tmp3[(tmp3['Noun'].isin(words) | tmp3['Verb'].isin(words) )]\nwords=Verb[Verb[0]<10]['Verb'].tolist() + Noun[Noun[0]<=10]['Noun'].tolist()\ntmp5=tmp3[(tmp3['Noun'].isin(words) | tmp3['Verb'].isin(words) )]","03a16f57":"# pypi\nimport networkx\nfrom networkx import (\n    draw,\n    DiGraph,\n    Graph,\n)\n%matplotlib inline","e0d4eca9":"from pyvis.network import Network","03f043f8":"tmp3[\"Weight\"]=tmp3[0]\nsources = tmp3['Noun']\ntargets = tmp3['Verb']\nweights = tmp3['Weight']\n\ngot_net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", \\\n                  notebook=True)\n# set the physics layout of the network\ngot_net.barnes_hut()\n\nedge_data = zip(sources, targets, weights)\n\nfor e in edge_data:\n    src = e[0]\n    dst = e[1]\n    w = e[2]\n\n    got_net.add_node(str(src), str(src), title=src)\n    got_net.add_node(dst, dst, title=dst)\n    got_net.add_edge(str(src), str(dst), value=w)\n\nneighbor_map = got_net.get_adj_list()\n\n# add neighbor data to node hover data\nfor node in got_net.nodes:\n    node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join(neighbor_map[node[\"id\"]])\n    node[\"value\"] = len(neighbor_map[node[\"id\"]])\n    \ngot_net.show(\"Article3.html\")","79e0a175":"from pprint import pprint as print\nfrom gensim.summarization import summarize","e0f5a145":"print(summarize(data3.sentences2[sim_docs[0][0]], ratio=0.05 ))","326360e9":"# **HEAPQ Document Summary**","62d21000":"# **Keep Social Articles**","3f1d8d68":"# **Tokenize Documents**","769caf1b":"# Doc2Vec","5a3f4208":"# **Spacy **","d29a1076":"# **Word2Vec Representation of Documents**","9e485202":"# **Load All Articles**","85daa4e3":"# **LDA Analysis of Articles**"}}