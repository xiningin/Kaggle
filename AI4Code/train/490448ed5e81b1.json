{"cell_type":{"10d21341":"code","bdc9beca":"code","ed1d3716":"code","d04ee861":"code","9623c7d0":"markdown","5a5396d1":"markdown"},"source":{"10d21341":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras as k\nimport matplotlib.pyplot as plt\nimport threading\nimport cv2\nimport gym\nimport random\nimport time","bdc9beca":"tf.enable_eager_execution()\nEPISODE,running_score,G_t_step = 59000,190,37500000\n","ed1d3716":"class A3C_Atari:\n\n    def __init__(self, game_name, lr, n_workers, n_actions, action_space, NMaxEp, frequency, gamma):\n        self.lr = lr\n        self.game_name = game_name\n        self.n_actions = int(n_actions)\n        self.action_space = action_space\n        self.n_workers = int(n_workers)\n        self.model = self.Model()\n        self.NMaxEp = NMaxEp\n        self.frequency = frequency\n        self.gamma = gamma\n\n    def Model(self):\n        input_ = k.layers.Input(shape=(80, 80, 4))\n        conv1 = k.layers.Conv2D(32, kernel_size=(4, 4), strides=(4, 4),\n                                kernel_initializer=k.initializers.glorot_normal(),\n                                activation=k.activations.relu, padding='valid')(input_)\n        conv2 = k.layers.Conv2D(16, kernel_size=(4, 4), strides=(2, 2),\n                                kernel_initializer=k.initializers.glorot_normal(),\n                                activation=k.activations.relu, padding='valid')(conv1)\n\n    \n\n        dense1 = k.layers.Flatten()(conv2)\n        dense2 = k.layers.Dense(256, activation=k.activations.relu,\n                                kernel_initializer=k.initializers.glorot_normal(),\n                                bias_initializer=k.initializers.glorot_normal())(dense1)\n        actions = k.layers.Dense(self.n_actions, activation=k.activations.softmax,\n                                 kernel_initializer=k.initializers.glorot_normal(),\n                                 bias_initializer=k.initializers.glorot_normal())(dense2)                     #Actor\n        value = k.layers.Dense(1, activation=None,\n                               kernel_initializer=k.initializers.glorot_normal(),\n                               bias_initializer=k.initializers.glorot_normal())(dense2)                       #Critic      \n        \n        print(input_.shape)\n\n        model = k.Model(inputs=input_, outputs=[actions, value])\n        model.compile(optimizer=k.optimizers.Adam(self.lr), loss=[self.custom_loss(),k.losses.mse],\n                      loss_weights=[1, 0.5])\n\n        model.load_weights('model_breakout_6.h5')\n\n        model.summary()\n        return model\n\n    def proccess_input(self, state):\n        state = state[35:195:2, 0:160:2, :]\n        state = cv2.resize(state, (80, 80), interpolation=cv2.INTER_CUBIC)\n        state = 0.299 * state[:, :, 0] + 0.587 * state[:, :, 1] + 0.114 * state[:, :, 2]                #Intersting way to convert my coloured images to Grayscale.\n        state = state \/ 255.0\n        return state\n\n    def custom_loss(self):\n        def loss_fn(y_true, y_pred):\n            y_pred = tf.clip_by_value(y_pred, 0.00001, 0.99999)\n            entropy = -tf.reduce_mean(tf.reduce_sum(y_pred * tf.log(y_pred),axis=0))\n            policy_loss = -tf.reduce_mean(y_true * tf.log(y_pred))\n            loss = policy_loss - 0.001 * entropy\n            return loss\n\n        return loss_fn\n\n\n    def train(self):\n\n        envs = [gym.make(self.game_name) for i in range(self.n_workers)]\n        lock = threading.Lock()\n        workers = [threading.Thread(target=self.run_thread, daemon=True, args=(envs[i], i, lock)) for i in\n                   range(self.n_workers)]\n        for worker in workers:\n            worker.start()\n            time.sleep(0.1)\n        [worker.join() for worker in workers]\n\n    def update(self, states, actions, rewards, done):\n\n        #print(done)\n        #print(rewards)\n        Q = []\n        if done[-1] == True:\n            R = 0\n        else:\n            R = self.model.predict(np.expand_dims(states[-1], axis=0))[1][0][0]\n        for r in reversed(rewards):\n            R = self.gamma * R + r\n            Q.append(R)\n\n        Q = np.flip(Q)\n       # print(Q)\n        #Q -= np.mean(Q)\n        #std = np.std(Q)\n        #if std != 0 :\n         #   Q \/= std\n\n\n        V = self.model.predict(np.asarray(states))[1]\n        print(Q)\n\n        advantage = np.zeros([len(rewards), self.n_actions])\n        input = np.empty([len(rewards),80,80,4])\n\n        for i in range(len(rewards)):\n            advantage[i][actions[i]] = Q[i] - V[i][0]\n            input[i] = states[i]\n\n        self.model.fit(input, [advantage, Q], verbose=0)\n\n    def run_thread(self, env, i, lock):\n        global EPISODE, running_score, G_t_step\n        while EPISODE < self.NMaxEp and running_score < 300:\n            EPISODE += 1\n            t,t_step, score , prevlives = 0, 0, 0, 5\n            state = self.proccess_input(env.reset())\n            state = np.stack([state] * 4, axis=2)\n            state_list, reward_list, action_list, done_list, probability_list = [], [], [], [], []\n            while prevlives > 0:\n                t_step += 1\n                G_t_step += 1\n                lock.acquire()\n                probability = np.clip(self.model.predict(np.expand_dims(state, axis=0))[0][0], 0.00001, 0.99999)\n                #if i == 1:\n                    \n                     #env.render()\n                time.sleep(0.01)\n                lock.release()\n                action = np.random.choice(self.action_space, 1, p=probability)\n                next_state, reward, done, info = env.step(action[0])\n                next_state = self.proccess_input(next_state)\n                next_state = np.append(state[:, :, 1:], np.expand_dims(next_state, axis=2), axis=2)\n                if info['ale.lives']-prevlives == -1:\n                    prevlives -= 1\n                    reward = -0.25\n                    done = True\n                state_list.append(state)\n                reward_list.append(reward)\n                action_list.append(action_space.index(action))\n                done_list.append(done)\n                probability_list.append(probability)\n                score += reward\n                state = next_state\n                if (t_step-t == self.frequency  or done == True):\n                    state_list.append(state)\n                    lock.acquire()\n                    if len(action_list):\n                        self.update(state_list, action_list, reward_list, done_list)\n                    lock.release()\n                    state_list, action_list, reward_list, done_list = [], [], [], []\n                    t=t_step\n\n            lock.acquire()\n            running_score = 0.95 * running_score + 0.05 * score\n            print('EPISODE : ', (EPISODE), 'G_tstep : ', (G_t_step), 'running score : ', (running_score),\n                      'score :',\n                      (score), 't_step : ', (t_step))\n            if EPISODE % 50 == 0:\n                self.model.save('model_breakout_7.h5')\n                print('model saved to disc')\n                print(random.sample(probability_list,10))\n            lock.release()\n","d04ee861":"env=gym.make('BreakoutDeterministic-v4').env\nprint(env.action_space.n)\nprint(env.unwrapped.get_action_meanings())\naction_space = [1,2,3]\nn_actions = len(action_space)\nenv.close()\nagent = a3c_atari(lr=0.0001,game_name='BreakoutDeterministic-v4',gamma=0.99,n_actions=n_actions,action_space = action_space,n_workers=8,NMaxEp=200000,frequency=5)\nagent.train()","9623c7d0":"Sorry to not have provided the model_breakout_6.h5 AND model_breakout_7.h5 files. But do grasp the code it is easily understandable.","5a5396d1":"Applying Reinforcement Learning to basic tasks has been quite a hot topic of interest in the last decade, especially for the second part.One of the basic steps is to begin implementing different algorithms related to it,to basic Games. Classic Arcade Game Enviornments have achieved a special attention towards themselves as a test bed for these kind of algorithms.\n\n**A3C (Asynchronous Advantage Actor Critic)**\n\nIt's hard to get your state of the art algorithm working,this is because getting any algorithm to work requires some good choices for hyperparameters, and I have to do all of these experiments over my lappy.\n\nTHE A3C algorithm can be essentially described as using policy gradients with a function approximator, where the function approximator is a deep neural network and the authors use a clever method to try and ensure the agent explores the state space well.Must admit I am in love with the idea.With the A3C algorithm,use many agents, all exploring the state space simultaneously. The hope is that the different agents will be in different parts of the state space, and thus give uncorrelated updates to the gradients.\n\nProviding the link to all the tutorials I went through is not possible, but I found [pdf](https:\/\/drive.google.com\/file\/d\/1dFfDv-alQs6E_wyRmd3F2ZHKCVxhEIxY\/view) concise and useful,so to learn the basics give it a look. I have tried my level best that my code is easy to understand for those who know the theory behind the Algorithm.\n"}}