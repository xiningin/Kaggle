{"cell_type":{"f5225801":"code","29c85305":"code","95917f43":"code","f58e2046":"code","41ecb91e":"code","e8ae6f36":"code","4012f571":"code","6613ec53":"code","364942ff":"code","6fc2086c":"code","0dd9cd83":"code","3d4ebbd6":"code","01afa8d0":"code","fd793806":"code","13bcb92c":"code","ceaac3e3":"code","cb3058c3":"code","fa5b4c15":"code","6de048d9":"code","114b04a8":"code","5719c5ca":"code","05c803c1":"code","c84c49ab":"markdown","fd3d1a7c":"markdown","921a3269":"markdown","ec91c34d":"markdown","3d7385b8":"markdown"},"source":{"f5225801":"import pandas as pd\nimport pickle as pkl\nimport random\n\n# Which tokenizer to use? TweetTokenizer is more robust than the vanilla tokenizer, but then,\n# will the intelligence of tokenization matter in the long run when trained using DL?\nfrom nltk.tokenize import word_tokenize, TweetTokenizer\ntokenizer = TweetTokenizer(preserve_case = False)\n\nfrom gensim.models import Word2Vec, KeyedVectors","29c85305":"movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\nmovie_lines = pd.read_csv(\"..\/input\/movie-corpus\/movie_lines.txt\", sep = \"\\+\\+\\+\\$\\+\\+\\+\", engine = \"python\", index_col = False, names = movie_lines_features)\n\n# Using only the required columns, namely, \"LineID\" and \"Line\"\nmovie_lines = movie_lines[[\"LineID\", \"Line\"]]\n\n# Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\nmovie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)","95917f43":"movie_lines.head()","f58e2046":"movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\nmovie_conversations = pd.read_csv(\"..\/input\/movie-corpus\/movie_conversations.txt\", sep = \"\\+\\+\\+\\$\\+\\+\\+\", engine = \"python\", index_col = False, names = movie_conversations_features)\n\n# Again using the required feature, \"Conversation\"\nmovie_conversations = movie_conversations[\"Conversation\"]","41ecb91e":"movie_conversations.head()","e8ae6f36":"# This instruction takes lot of time, run it only once.\n#conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() for u in c.strip().strip('[').strip(']').split(',')] for c in movie_conversations]","4012f571":"with open(\"conversations.pkl\", \"wb\") as handle:\n    pkl.dump(conversation, handle)","6613ec53":"with open(\"..\/input\/processed-conversations\/conversatons.pkl\", \"rb\") as handle:\n    conversation = pkl.load(handle)","364942ff":"# Calculate the dialogue length statistics\n\ndialogue_lengths = [len(dialogue) for dialogue in conversation]\npd.Series(dialogue_lengths).describe()","6fc2086c":"# Generate 50 sample pairs - 14\/03\/2019\nindices = random.sample(range(len(conversation)), 50)\nsample_context_list = []\nsample_response_list = []\n\nfor index in indices:\n    \n    response = conversation[index][-1]\n        \n    context = \"FS: \" + conversation[index][0] + \"\\n\"\n    for i in range(1, len(conversation[index]) - 1):\n        \n        if i % 2 == 0:\n            prefix = \"FS: \"\n        else:\n            prefix = \"SS: \"\n            \n        context += prefix + conversation[index][i] + \"\\n\"\n        \n    sample_context_list.append(context)\n    sample_response_list.append(response)\n\nwith open(\"cornell_movie_dialogue_sample.csv\", \"w\") as handle:\n    for c, r in zip(sample_context_list, sample_response_list):\n        handle.write('\"' + c + '\"' + \"#\" + r + \"\\n\")\n","0dd9cd83":"def generate_pairs(conversation):\n    \n    context_list = []\n    response_list = []\n    \n    for dialogue in conversation:\n        \n        response = word_tokenize(dialogue[-1])\n        \n        context = word_tokenize(dialogue[0])\n        for index in range(1, len(dialogue) - 1):\n            context += word_tokenize(dialogue[index])\n        \n        context_list.append(context)\n        response_list.append(response)\n        \n    return context_list, response_list","3d4ebbd6":"context_list, response_list = generate_pairs(conversation)","01afa8d0":"def train_test_split(X,Y):\n    \n    # Population indices to sample from.\n    pop_indices = [i for i in range(len(X))]\n    \n    # Randomly split the dataset into test and train with a 80%-20% split\n    #test_indices = random.sample(pop_indices, int(0.2 * len(X)))\n    test_indices = random.sample(pop_indices, 1000)\n    train_indices = list(set(pop_indices) - set(test_indices))\n\n    X_test = [X[i] for i in test_indices]\n    Y_test = [Y[i] for i in test_indices]\n    \n    X_train = [X[i] for i in train_indices]\n    Y_train = [Y[i] for i in train_indices]\n    \n    #Add negative samples to the test list\n    X_test += X_test\n    Y_test += Y_test[::-1]\n    \n    return X_train, Y_train, X_test, Y_test","fd793806":"X_train, Y_train, X_test, Y_test = train_test_split(context_list, response_list)","13bcb92c":"with open(\"X_train.pkl\", \"wb\") as handle:\n    pkl.dump(X_train, handle)\nwith open(\"Y_train.pkl\", \"wb\") as handle:\n    pkl.dump(Y_train, handle)\nwith open(\"X_test.pkl\", \"wb\") as handle:\n    pkl.dump(X_test, handle)\nwith open(\"Y_test.pkl\", \"wb\") as handle:\n    pkl.dump(Y_test, handle)","ceaac3e3":"corpus = []\nfor c in conversation:\n    context = []\n    \n    for s in c:\n        context += word_tokenize(s)\n    \n    corpus.append(context)","cb3058c3":"with open(\"corpus.pkl\", \"wb\") as handle:\n    pkl.dump(corpus, handle)","fa5b4c15":"# Read the corpus text file\nwith open(\"corpus.pkl\", \"rb\") as handle:\n    corpus = pkl.load(handle)","6de048d9":"# min_count = 1, says not to ignore any word occurence\n# size = 50, is the size of word embedding\nmodel = Word2Vec(corpus, min_count = 1, size = 50, sg = 1)","114b04a8":"with open(\"embeddings.kv\", \"wb\") as handle:\n    model.wv.save(handle)","5719c5ca":"embeddings = KeyedVectors.load(\"embeddings.kv\")","05c803c1":"embeddings['Can']","c84c49ab":"## Generate corpus using the conversation list","fd3d1a7c":"As observed above, the mean dialogue length is approximately 4 which is pretty less and we can take only the last utterence as the response. Yet to figure out a way to handle the larger ones though.","921a3269":"## Create context and response pairs","ec91c34d":"## Create a list of dialogues\n\nWe join two different files namely `movie_lines.tsv` and `movie_conversations.tsv` to finally produce a list of dialogues. This list is further stored as a `pickle` file for further processing.","3d7385b8":"\n## Create word embeddings using Gensim"}}