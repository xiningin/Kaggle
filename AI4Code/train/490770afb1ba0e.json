{"cell_type":{"46482b62":"code","ace77960":"code","1bdec10e":"code","2bef92af":"code","cd3732d4":"code","b1d96832":"code","4cd6fb0b":"code","d77d0ea4":"code","c1c2b457":"code","2140d7f7":"code","da63a164":"code","bebcb8e8":"code","ccc6ee20":"code","077b2f77":"code","331396e6":"code","c5f6fc39":"code","0bc3e9d8":"code","f1e0b05b":"code","9d7ae7dc":"markdown","7987da43":"markdown","4388d91d":"markdown","95fdccc3":"markdown","27789b13":"markdown","e502edb4":"markdown","3fc42b07":"markdown","c5590748":"markdown","2ccce42a":"markdown","099be4b1":"markdown","18421a4a":"markdown","d25c34e3":"markdown"},"source":{"46482b62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ace77960":"df = pd.read_csv('..\/input\/spam-ham\/spam_ham_dataset.csv')\ndf.head()","1bdec10e":"import string\nimport nltk\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt","2bef92af":"lemmatizer = WordNetLemmatizer() \nstop_words = set(stopwords.words('english')) ","cd3732d4":"def text_preprocessing(msg):\n    msg = msg.lower()\n    nopunc =[char for char in msg if char not in string.punctuation]\n    nopunc=''.join(nopunc)\n    token = word_tokenize(nopunc)\n    \n    #remove word:subject\n    if 'subject' in token:\n        token.remove('subject')\n        \n    word = [word for word in token if word.isalnum()]    \n    \n    final_msg = list()\n  \n    for w in word: \n        if w not in stop_words: \n            final_msg.append(w)\n    \n    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in final_msg] \n    return lemmas ","b1d96832":"msg = df['text'][1]\nprint(msg)\nf = text_preprocessing(msg)\nprint(f)","4cd6fb0b":"tokens = list()\nfor i in range(len(df['text'])):\n    tokens.append(text_preprocessing(df['text'][i]))\n    \nprint(tokens[:10])","d77d0ea4":"print(len(tokens))","c1c2b457":"bow=list()\nfor l in tokens:\n    for w in l:\n        if w not in bow:\n            bow.append(w)\n            \nprint(len(bow))","2140d7f7":"freq = np.zeros(shape=(len(df['text']),len(bow)))\n\nfor i in range(len(df['text'])):\n    token_list=tokens[i]\n    for j in range(len(bow)):\n        freq[i][j]=token_list.count(bow[j])\n\nfeature_df = pd.DataFrame(freq, columns =bow) ","da63a164":"feature_df.head()","bebcb8e8":"from sklearn.model_selection import train_test_split\nX = df['text']\ny = df['label_num']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20)","ccc6ee20":"print(X_train.head(5))\nprint(X_test.head(5))\n#print(X_test[:1])\nprint(y_test.head(5))","077b2f77":"def NB(subj):\n    tok = text_preprocessing(subj)\n    freq_vector = np.zeros(len(bow))\n    for i in range(len(bow)):\n        freq_vector[i] = tok.count(bow[i])\n    \n    spam = 0\n    ham = 0\n    for i in y_train:\n        if i==0:\n            ham += 1\n        else:\n            spam += 1\n    print('Span = ',spam,'\\nHam = ',ham)\n    \n    p_spam = spam\/len(y_train)\n    p_ham = ham\/len(y_train)\n    print('Prob of spam = ',p_spam,'\\nProb of ham = ',p_ham)\n    \n    y_train_list=y_train.tolist()\n    #print(y_train_list)\n\n    t_spam = 0\n    t_ham = 0\n    prob_spam = 1\n    prob_ham = 1\n    \n    for i in range(len(bow)):\n        yes = 0\n        no = 0\n        for j in range(len(y_train_list)):\n            if y_train_list[j] == 1 and freq_vector[i] == freq[j][i]:\n                yes += 1\n            elif y_train_list[j] == 0 and freq_vector[i] == freq[j][i]:\n                no += 1\n        if prob_ham == 0 or prob_spam == 0:\n            prob_ham = 1\n            prob_spam = 1\n            \n        prob_ham = float(prob_ham) * float((no\/ham))\n        prob_spam = float(prob_spam) * float((yes\/spam))  \n        \n    print(prob_ham,prob_spam)\n\n    t_ham = prob_ham * p_ham  \n    t_spam = prob_spam * p_spam\n    \n    denominator = t_ham + t_spam\n    pred_ham = t_ham\/denominator\n    pred_spam = t_spam\/denominator\n    \n    print(\"HAM|X = \",pred_ham)\n    print(\"SPAM|X = \",pred_spam)\n\n    if pred_ham < pred_spam:\n        return 1\n    else:\n        return 0","331396e6":"y_test_sample = y_test[:10]\nX_test_sample = X_test[:10]\n\ny_pred = []\nprint(len(y_test_sample),len(y_pred),len(X_test_sample))\n\nfor i in X_test_sample:\n    y_pred.append(NB(i))\n    #print(i)\nprint(\"Predicted values = \",y_pred,\"\\n\")\n","c5f6fc39":"print('Actual values = ',y_test_sample)\nprint(len(y_test_sample),len(y_pred))","0bc3e9d8":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(list(y_test_sample),y_pred)\ntn,fp,fn,tp = confusion_matrix(list(y_test_sample),y_pred).ravel()","f1e0b05b":"# Accuracy\nacc = (tp+tn)\/(tp+tn+fp+fn)\n\n# Precision\np_ham = tn\/(tn+fn)\np_spam = tp\/(tp+fp)\np_avg = (p_ham+p_spam)\/2\n\n# Recall\nr_ham = tn\/(tn+fp)\nr_spam = tp\/(tp+fn)\nr_avg = (r_ham+r_spam)\/2\n\n# F-measure\nf_m = (2*p_avg*r_avg)\/(p_avg+r_avg)\n\nprint(\"Accuracy : \", acc)\nprint(\"Precision : \", p_avg)\nprint(\"Recall : \", r_avg)\nprint(\"F-measure : \", f_m)","9d7ae7dc":"## Confusion matrix","7987da43":"## Data splitting","4388d91d":"### Sample testing","95fdccc3":"## Loading the Dataset","27789b13":"## Import libraries","e502edb4":"## Text Preprocessing\n\n1. Converting string to lower case\n2. Removing punctuations\n3. Removing stop words","3fc42b07":"### Constructing the frequency matrix for BOW","c5590748":"## Testing","2ccce42a":"## Constructing the bag of words","099be4b1":"## Token words","18421a4a":"## Naive Bayes Classifier","d25c34e3":"## Metrics"}}