{"cell_type":{"4cef22eb":"code","7b85d2eb":"code","d86bfe92":"code","ec08297d":"code","55b6d19a":"code","f22f1bd7":"code","e7a15b16":"code","4440478c":"code","e316cb57":"code","2b2179eb":"code","77cb1030":"code","b17f6c95":"code","bc7c4643":"code","73b2ec7c":"code","fe57e8e9":"code","6ccdc105":"code","1865efd1":"code","0c9f0c38":"code","f5a14cb5":"code","152d882f":"code","5f3fb160":"code","aad59b5f":"code","65cb7f3f":"code","d1dd7bd2":"code","8c1ae0be":"code","8a11108f":"code","d67eff02":"code","41361f08":"code","3ca3b470":"code","3522ddd7":"code","c1ef252c":"code","bc920350":"code","4032392f":"code","9a45a4bb":"code","70ed93ef":"code","40827d57":"code","395ae1cf":"code","c5d14a9e":"code","a303e7dd":"code","42bff597":"code","43489bf3":"code","dac5d39d":"code","d0fd1610":"code","f2b2b618":"code","26a5163f":"code","86f3dc99":"code","346e93f9":"code","a9fcc0ac":"code","c454f707":"code","ab0b5a38":"code","b2c8ca19":"code","634a1707":"code","cbd53e55":"code","61df21f3":"code","fd68dda2":"code","4e425f19":"code","8d9ba66e":"code","fe3e94f8":"code","37cb7cbf":"code","f6172a67":"code","c8338418":"code","ba41c073":"markdown","498854a2":"markdown","af067f53":"markdown","483cc133":"markdown","0f4fb752":"markdown","acee74b9":"markdown","1c33b801":"markdown","8bf5c552":"markdown","0bcfce40":"markdown","23a19478":"markdown","962a335f":"markdown","568e6ae2":"markdown","c6f2a7d0":"markdown","d14be184":"markdown","dca6f1c3":"markdown","7c1bd71d":"markdown","e4cee750":"markdown","d4dd4784":"markdown","2a86c05a":"markdown","475ca772":"markdown","692a51ad":"markdown","ff71a98d":"markdown","7e91ef88":"markdown","4fdac6f5":"markdown","b5a0d196":"markdown","97dc3591":"markdown","770c1191":"markdown","1b8a7b37":"markdown","06e9a3da":"markdown","d16a55bd":"markdown","23ab9390":"markdown","50cafbd2":"markdown","71bac042":"markdown","458553fb":"markdown","571b304d":"markdown","fbe465ec":"markdown","4dcd2570":"markdown","b7315341":"markdown","916c79c1":"markdown","b7eee127":"markdown","917036a3":"markdown","a0f0b702":"markdown","701789ac":"markdown","5e8cd928":"markdown","f967a8cf":"markdown","0a2b8bfe":"markdown","44263a48":"markdown","bada1cc4":"markdown","7f855b11":"markdown","e70ea5bf":"markdown","81c21cf1":"markdown","662e23af":"markdown","1fa8c6ab":"markdown","d842df90":"markdown","6c2470c8":"markdown","36b76e93":"markdown","3f190b8b":"markdown","c5244e1e":"markdown","6130346a":"markdown","52a47df1":"markdown","b7b91a05":"markdown"},"source":{"4cef22eb":"%matplotlib inline","7b85d2eb":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avanc\u00e9s\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","d86bfe92":"all_dfs = pd.read_excel('..\/input\/penalty-statistics-20192020\/19-20XL_V2.xlsx', sheet_name=None)\ndf = pd.concat(all_dfs, ignore_index=True)","ec08297d":"# Dimension du dataset\nprint(np.shape(df))","55b6d19a":"df.info()","f22f1bd7":"df.count()","e7a15b16":"df.Scored.value_counts()\/df.Scored.count()","4440478c":"df['PT sub'].value_counts()","e316cb57":"df['Team taking pen'].value_counts()\/df['Team taking pen'].count()","2b2179eb":"sns.boxplot(x=\"Scored\", y=\"Minute\", data=df)","77cb1030":"fig = sns.FacetGrid(df, hue=\"Scored\", aspect=3, palette=\"Set2\") # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"Minute\", shade=True)\nfig.add_legend()","b17f6c95":"df.columns","bc7c4643":"# On \u00e9limine les colonnes non pertinentes pour la pr\u00e9diction\n# On \u00e9limine Last Goal et Team Win car ce sont des valeurs qui sont influanc\u00e9es par le la convertion du penalty ou non non\npenalty = df.drop(['REF', 'Competition', 'Last Goal', 'Team win','Date','Journee', 'Home team', 'Away team', 'Pen taker'], axis=1)","73b2ec7c":"df.count()","fe57e8e9":"df.info()","6ccdc105":"penalty['Team taking pen'] = penalty['Team taking pen'].map({\"Home\":0, \"Away\":1})\npenalty['LtD game-changing pen'] = penalty['LtD game-changing pen'].map({\"YES\":1, \"NO\":0})\npenalty['DtW game-changing pen'] = penalty['DtW game-changing pen'].map({\"YES\":1, \"NO\":0})\npenalty['L no game-changing'] = penalty['L no game-changing'].map({\"YES\":1, \"NO\":0})\npenalty['W no game-changing'] = penalty['W no game-changing'].map({\"YES\":1, \"NO\":0})\npenalty['Scored'] = penalty['Scored'].map({\"YES\":1, \"NO\":0})\npenalty['PT sub'] = penalty['PT sub'].map({\"YES\":1, \"NO\":0})","1865efd1":"penalty = pd.get_dummies(data=penalty, columns=['PT position'])","0c9f0c38":"penalty[['Minute']].describe()","f5a14cb5":"sns.kdeplot(titanic.Minute, color='red')","152d882f":"from sklearn import preprocessing","5f3fb160":"minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\npenalty[['Minute']] = minmax.fit_transform(penalty[['Minute']])","aad59b5f":"sns.distplot(penalty.Minute, color='red')","65cb7f3f":"scaler = preprocessing.StandardScaler()\npenalty[['Minute']] = scaler.fit_transform(penalty[['Minute']])","d1dd7bd2":"sns.distplot(penalty.Minute, color='red')","8c1ae0be":"penalty.head()","8a11108f":"X = penalty.drop(['Scored'], axis=1)\ny = penalty.Scored","d67eff02":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","41361f08":"print(X_train.shape)\nprint(X_test.shape)","3ca3b470":"from sklearn import svm","3522ddd7":"clf = svm.LinearSVC()\nclf.fit(X_train,y_train)","c1ef252c":"y_lr = clf.predict(X_test)","bc920350":"# Importation des m\u00e9thodes de mesure de performances\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score","4032392f":"print(confusion_matrix(y_test,y_lr))","9a45a4bb":"print(accuracy_score(y_test,y_lr))","70ed93ef":"print(classification_report(y_test, y_lr))","40827d57":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","395ae1cf":"print(classification_report(y_test, y_rf))","c5d14a9e":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","a303e7dd":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","42bff597":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'n_estimators', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","43489bf3":"from sklearn import model_selection","dac5d39d":"param_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)","d0fd1610":"rf_gs.fit(X_train, y_train)","f2b2b618":"print(rf_gs.best_params_)","26a5163f":"rf2 = rf_gs.best_estimator_","86f3dc99":"y_rf2 = rf2.predict(X_test)","346e93f9":"print(classification_report(y_test, y_rf2))","a9fcc0ac":"importances = rf2.feature_importances_\nindices = np.argsort(importances)","c454f707":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.title('Importance des caracteristiques')","ab0b5a38":"X = penalty.drop(['Scored'], axis=1)\ny = df.Scored\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","b2c8ca19":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_sample(X_train, y_train)","634a1707":"y_train.value_counts()","cbd53e55":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","61df21f3":"print(classification_report(y_test, y_rf))","fd68dda2":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","4e425f19":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\nprint(xgb.score(X_test,y_test))","8d9ba66e":"from sklearn import metrics\ny_xgb = xgb.predict(X_test)\n\nprint(classification_report(y_test, y_xgb))\n\ncm = metrics.confusion_matrix(y_test, y_xgb)\nprint(cm)","fe3e94f8":"X = penalty.drop(['Scored'], axis=1)\ny = penalty.Scored\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","37cb7cbf":"df.Scored.value_counts()","f6172a67":"from xgboost import XGBClassifier\nxgb = XGBClassifier(scale_pos_weight=384\/110)\nxgb.fit(X_train,y_train)\ny_xgb = xgb.predict(X_test)","c8338418":"print(classification_report(y_test, y_xgb))\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)","ba41c073":"## Sur\u00e9chantillonnage","498854a2":"- REF : id\n- Competition : Championnat dans lequel le match c'est jou\u00e9\n- Journee : Journee de championnat\n- Date\n- Home Team : Equipe jouant \u00e0 domicile\n- Away Team : Equipe jouant \u00e0 l'exterieur\n- Team taking pen : Equipe (domicile\/exterieur) qui obtient le penalty\n- Pen taker : joueur tirant le penalty\n- Minute Ltd : Minute durant laquelle le penalty est tir\u00e9\n- LtD game-changing pen : Le penalty permet \u00e0 l'\u00e9quipe de revenir \u00e0 \u00e9galit\u00e9 (ex : 1-2)\n- DtW game-changing pen : Le penalty permet \u00e0 l'\u00e9quipe de mener au score (ex : 1-1)\n- L no game-changing : Le penalty permet \u00e0 l'\u00e9quipe de revenir au score mais est toujours men\u00e9e (l'\u00e9quipe est men\u00e9e par au moins 2 buts d'\u00e9carts avant que le penalty soit tir\u00e9 \/ ex : 0-2)\n- W no game-changing : Le penalty permet \u00e0 l'\u00e9quipe d'accro\u00eetre son avance (l'\u00e9quipe est d\u00e9j\u00e0 devant au score avant que le penalty soit tir\u00e9 \/ ex : 3-2)\n- Scored : Penalty convertie ou non\n- PT sub : Tireur est un remplacant ou non\n- PT position : Poste du tireur\n- Team win : L'\u00e9quipe a gagn\u00e9 le match \n- Last Goal : Dernier but du match ou non","af067f53":"## Encodage binaire des donn\u00e9es qualitatives (one hot encoding)","483cc133":"En comparant les valeurs pr\u00e9dites et les valeurs r\u00e9elles, on a plusieurs possibilit\u00e9s :\n- *Vrais positifs* (VP ou TP) : on pr\u00e9dit \"oui\" et la valeur attendue est \"oui\"\n- *Vrais n\u00e9gatifs* (VN ou TN) : on pr\u00e9dit \"non\" et la valeur attendue est \"non\"\n- *Faux positifs* (FP) : on pr\u00e9dit \"oui\" et la valeur attendue est \"non\"\n- *Faux n\u00e9gatifs* (FN) : on pr\u00e9dit \"non\" et la valeur attendue est \"oui\"","0f4fb752":"Les r\u00e9sultats sont sensiblement les m\u00eames que pour le sur echantillonage","acee74b9":"On teste les for\u00eats al\u00e9atoires avec les donn\u00e9es sur\u00e9chantillonn\u00e9es :","1c33b801":"On va utiliser une am\u00e9lioration de la m\u00e9thode XGBoost, sans sur\u00e9chantillonage","8bf5c552":"On peut normaliser les valeurs min et \u00e0 max (valeurs ramen\u00e9es entre 0 et 1) :","0bcfce40":"# Analyse d'un dataset : Statistique sur les p\u00e9naltys en Europe","23a19478":"On utilise le param\u00e8tre *scale_pos_weight* pour donner plus d'impact aux erreurs commises sur la classe minoritaire :","962a335f":"## Importations des librairies courantes","568e6ae2":"Cr\u00e9er les jeux d'apprentissage et de test","c6f2a7d0":"## R\u00e9gression logistique","d14be184":"## Cr\u00e9ation des jeux d'apprentissage et de test","dca6f1c3":"Parmi les hyperparam\u00e8tres de l'algorithme qui peuvent avoir un impact sur les performances, on a :\n- **n_estimators** : le nombre d'arbres de d\u00e9cision de la for\u00eat al\u00e9atoire\n- **min_samples_leaf** : le nombre d'\u00e9chantillons minimum dans une feuille de chaque arbre\n- **max_features** : le nombre de caract\u00e9ristiques \u00e0 prendre en compte lors de chaque split","7c1bd71d":"La m\u00e9thode SMOTE (Synthetic Minority Oversampling TEchnique) consiste \u00e0 synth\u00e9tiser des \u00e9l\u00e9ments pour la classe minoritaire, \u00e0 partir de ceux qui existent d\u00e9j\u00e0. Elle fonctionne en choisissant au hasard un point de la classe minoritaire et en calculant les k-voisins les plus proches pour ce point. Les points synth\u00e9tiques sont ajout\u00e9s entre le point choisi et ses voisins.","e4cee750":"Ici on a choisi des valeurs pour le nombres d'arbres dans la for\u00eat al\u00e9atoire (*'n_estimators'*) et le nombre minimum d'\u00e9chantillons pour une feuille.","d4dd4784":"On peut \u00e9galement utiliser le *StandardScaler* pour ramener la moyenne \u00e0 0 et l'\u00e9cart type \u00e0 1 :","2a86c05a":"Dans ce cas l\u00e0 les penalties non convertis sont en partie reconnus mais ca reste tr\u00e8s faible","475ca772":"78% des penalties ont \u00e9t\u00e9 convertis durant la saison 19-20","692a51ad":"Eliminer les colonnes non pertinentes pour la pr\u00e9diction (on peut utiliser une liste de colonnes dans drop), et placer le r\u00e9sultat dans la variable penalty :","ff71a98d":"## Conditionnement des donn\u00e9es","7e91ef88":"Nous voyons bien ici que les r\u00e9sultats sont \u00e9rron\u00e9s, ca me parait faux que la caract\u00e9ristique la plus importante pour qu'un penalty soit marqu\u00e9 soit la minute \u00e0 laquelle le penalty a \u00e9t\u00e9 tir\u00e9.","4fdac6f5":"On lance l'entrainement :","b5a0d196":"On peut visualiser ces degr\u00e9s d'importance avec un graphique \u00e0 barres par exemple :","97dc3591":"<img src=\"https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png\">","770c1191":"On s\u00e9pare le dataset en deux parties :\n- un ensemble d'apprentissage (entre 70% et 90% des donn\u00e9es), qui va permettre d'entra\u00eener le mod\u00e8le\n- un ensemble de test (entre 10% et 30% des donn\u00e9es), qui va permettre d'estimer la pertinence de la pr\u00e9diction","1b8a7b37":"## Interpr\u00e9tation des param\u00e8tres","06e9a3da":"Seulement 33 tireurs \u00e9taient des remplacants au d\u00e9but du match","d16a55bd":"L'attribut *feature_importances_* renvoie un tableau du poids de chaque caract\u00e9ristique dans la d\u00e9cision :","23ab9390":"## Ajustement des hyperparam\u00e8tres (Random Forests)","50cafbd2":"On a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage (en \"ajoutant\" des donn\u00e9es) :","71bac042":"On utilise la fonction get_dummies de Pandas pour transformer les colonnes multimodales (par exemple '\u00a8PT position') en plusieurs colonnes binaires (par exemple 'ST' dont les valeurs sont 1 si le tireur joue au poste de Striker (Buteur) et 0 sinon) :","458553fb":"Resultat toujours insuffisant, il reconnait quasiment que les penalties marqu\u00e9s","571b304d":"On va r\u00e9\u00e9quilibrer le dataset en sur-\u00e9chantillonnant la classe minoritaire :","fbe465ec":"il ne manque aucune donn\u00e9e","4dcd2570":"Afficher la matrice de confusion :","b7315341":"### Mise \u00e0 l'\u00e9chelle des donn\u00e9es quantitatives","916c79c1":"On teste les for\u00eats al\u00e9atoires :","b7eee127":"## Lecture du fichier","917036a3":"On s\u00e9lectionne le meilleur estimateur :","a0f0b702":"N\u00e9anmoins cette mesure peut \u00eatre fauss\u00e9e dans certains cas, en particulier si le nombre de 0 et de 1 est d\u00e9s\u00e9quilibr\u00e9.\nOn a donc d'autres estimateurs :\n- la **pr\u00e9cision** est le nombre de pr\u00e9dictions positives correctes sur le nombre total de pr\u00e9dictions positives : *precision = VP\/(VP+FP)*\n- la **sensibilit\u00e9** (*recall*) est le nombre de pr\u00e9dictions positives sur le nombre effectif de \"oui\" : *recall = VP:(VP+FN)*\n- le **score F1** est la moyenne pond\u00e9r\u00e9e de la pr\u00e9cision et de la sensibilit\u00e9 : *f1-score = 2xprecisionxrecall\/(precision+recall)*","701789ac":"La plupart des algorithmes ont besoin de donn\u00e9es num\u00e9riques, et n'acceptent pas les cha\u00eenes de caract\u00e8res :","5e8cd928":"<img src=\"https:\/\/i.stack.imgur.com\/gKyb9.png\">","f967a8cf":"validation_curve permet de tracer la courbe du score sur un ensemble d'apprentissage et sur un ensemble de test (cross validation), en faisant varier un param\u00e8tre, par exemple n_estimators :","0a2b8bfe":"Cela ne marche pas car il ne prend encore une fois pas en compte les penalies rat\u00e9s","44263a48":"La linear regression ne marche pas du tout. Je ne sais pas dire si c'est une erreur de configuration (que je n'arrive pas \u00e0 trouver) o\u00f9 si c'est normal (m\u00e9thode pas appropri\u00e9e)","bada1cc4":"On peut voir les param\u00e8tres s\u00e9lectionn\u00e9s et le score :","7f855b11":"La pertinence (ou accuracy) mesure le nombre de bonnes pr\u00e9dictions sur le nombre total d'observations","e70ea5bf":"On reconstitue les jeux de donn\u00e9es sans \u00e9chantillonnage :","81c21cf1":"# Mesures de performance","662e23af":"52% des penalties ont \u00e9t\u00e9 en faveur de l'\u00e9quipe \u00e0 domicile","1fa8c6ab":"Les r\u00e9sultats sont un peu mieux r\u00e9partie ici, il reconnait plus de penalties manqu\u00e9s par contre il reconnait beaucoup moins de penalties r\u00e9ussis ","d842df90":"Appliquer une r\u00e9gression logistique pour classifier sur l'ensemble de test","6c2470c8":"## Chiffres et Visualisation des donn\u00e9es","36b76e93":"La matrice de confusion permet de compter les vrais positifs, faux positifs, ...","3f190b8b":"## XGBoost pond\u00e9r\u00e9","c5244e1e":"Il n'y a aucune donn\u00e9e manquante","6130346a":"On cr\u00e9e donc de \"fausses donn\u00e9es\" (mais \"vraisemblables\") pour l'apprentissage","52a47df1":"## Importance des caract\u00e9ristiques","b7b91a05":"## Extreme Gradient Boosting : XGBoost avec sur\u00e9chantillonage SMOTE"}}