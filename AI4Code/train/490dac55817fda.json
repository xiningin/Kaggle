{"cell_type":{"97088a72":"code","cf95bf8a":"code","1c325156":"code","5ba8236b":"code","b2be877f":"code","3841b557":"code","1f6c3175":"code","3d7627f7":"code","c6c40be1":"code","23cebf05":"code","053aa52b":"code","430f48f5":"code","e051d87f":"code","6379f5cd":"code","4a0b2f97":"code","de68d590":"code","c6b026ec":"code","0e6064e0":"code","0ccba268":"code","dd815a59":"code","7ccd4ca1":"code","102c92f8":"code","0e7cbda0":"code","e1312a8f":"code","37cca909":"code","73d04ac2":"code","46392f82":"code","6deb4da5":"code","24f10579":"code","91279238":"code","4d3a31ea":"code","a9be9c02":"code","47bab313":"code","b7f3042c":"markdown","5cea6137":"markdown","28a652a7":"markdown","b7d1c647":"markdown","126da94e":"markdown","1488f277":"markdown","37a79140":"markdown","c686564d":"markdown","5843231a":"markdown","f3732c42":"markdown","7e4478b4":"markdown","606962e7":"markdown","672840f9":"markdown","e472ef07":"markdown","5359f026":"markdown","d70a4576":"markdown","c38dd7f3":"markdown","d95f73fe":"markdown","90f53611":"markdown","57c7812a":"markdown","c37c4cf4":"markdown","8658b352":"markdown","8b60fcab":"markdown","750cc0f5":"markdown","011a0f62":"markdown","75e167fa":"markdown","ac56a537":"markdown","13218915":"markdown","19b56e62":"markdown","159ae5b6":"markdown","328c89ff":"markdown","cba8b3b3":"markdown","cd3791ef":"markdown","cce7d191":"markdown","c9e2bebc":"markdown","ebeda8a9":"markdown","3a158369":"markdown"},"source":{"97088a72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# string label to categorical values\nfrom sklearn.preprocessing import LabelEncoder\n\n# cross validation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n# limit warning\nimport warnings  \nwarnings.filterwarnings('ignore')","cf95bf8a":"# import data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","1c325156":"# convert objects (strings) into numbers\nprint(train['SaleCondition'].unique())\nfor i in range(train.shape[1]):\n    if train.iloc[:,i].dtypes == object:\n        lbl = LabelEncoder()\n        lbl.fit(list(train.iloc[:,i].values) + list(test.iloc[:,i].values))\n        train.iloc[:,i] = lbl.transform(list(train.iloc[:,i].values))\n        test.iloc[:,i] = lbl.transform(list(test.iloc[:,i].values))\n\nprint(train['SaleCondition'].unique())","5ba8236b":"# keep ID for submission\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# split data for training\ny_train = train['SalePrice']\nX_train = train.drop(['Id','SalePrice'], axis=1)\nX_test = test.drop('Id', axis=1)\n\n# dealing with missing data\nXmat = pd.concat([X_train, X_test])","b2be877f":"# before nan removals\nmsno.matrix(df=Xmat, figsize=(12, 8), color=(0.5,0,0))","3841b557":"# after nan removals\nXmat = Xmat.drop(['LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\nXmat = Xmat.fillna(Xmat.median())\nmsno.matrix(df=Xmat, figsize=(12, 8), color=(0.5,0,0))","1f6c3175":"# add a new feature 'total sqfootage'\nXmat['TotalSF'] = Xmat['TotalBsmtSF'] + Xmat['1stFlrSF'] + Xmat['2ndFlrSF']","3d7627f7":"# normality check for the target\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nax = sns.distplot(y_train)","c6c40be1":"# log-transform (log(1 + x)) the dependent variable for normality\ny_train = np.log1p(y_train)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nax = sns.distplot(y_train)","23cebf05":"# restore original form\nX_train = Xmat[:X_train.shape[0]]\nX_test = Xmat[X_train.shape[0]:]\n\n# interaction terms\nX_train[\"Interaction\"] = X_train[\"TotalSF\"] * X_train[\"OverallQual\"]\nX_test[\"Interaction\"] = X_test[\"TotalSF\"] * X_test[\"OverallQual\"]\n\nX_train.head()","053aa52b":"X_test.head()","430f48f5":"print(y_train[:10])","e051d87f":"# LightGBM\nimport lightgbm as lgb","6379f5cd":"# example hyperparameters for light GBM\nlgbParams = {\"num_leaves\": 41, # model complexity\n         \"min_data_in_leaf\": 10, # minimal number of data in one leaf\n         \"objective\":'regression', # regression, binary, ...\n         \"metric\": 'rmse', # mae, auc, binary_logloss, ...\n         \"num_iterations\": 5000, # number of boosting iterations \n         \"max_depth\": -1, # max depth for tree model\n         \"max_bin\": 255, # max number of bins that features are bucketed\n         \"learning_rate\": 0.005, # learning rate\n         \"num_iterations\": 500, # number of boosting iterations\n         \"boosting\": \"gbdt\", # dart, goss, ...\n         \"feature_fraction\": 0.9, # selected proportion of feature on each iteration\n         \"bagging_fraction\": 0.9, # selected proportion of data without resampling\n         \"bagging_freq\": 3, # perform bagging at every k iteration\n         \"lambda_l1\": 0.1, # L1 regularization\n         \"verbosity\": -1, # <0, fatal; 0: error, 1: info, >1: debug\n         \"random_state\": 42, # random state\n        }","4a0b2f97":"## Visualize feature importance\n\n# make a LightGBM dataset\ntrainX, testX, trainY, testY = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\nd_train = lgb.Dataset(trainX, trainY)\nd_eval = lgb.Dataset(testX, testY, reference=d_train)\n\n# model training\nLGBmodel = lgb.train(lgbParams, d_train, valid_sets=d_eval, early_stopping_rounds=100, verbose_eval=1000)\n\n# feature importance\nimportance = LGBmodel.feature_importance()\nranking = np.argsort(-importance)\nfig, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=importance[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()","de68d590":"# KFold cross-validation\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# placeholders\ny_pred_lgb = np.zeros(np.shape(X_train)[0])\npredictions_lgb = np.zeros(np.shape(X_test)[0])\nfor train_idx, test_idx in kf.split(X_train):\n    # train, test split\n    trainX, testX = X_train.iloc[train_idx], X_train.iloc[test_idx]\n    trainY, testY = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n    # make a LightGBM dataset\n    d_train = lgb.Dataset(trainX, trainY)\n    d_eval = lgb.Dataset(testX, testY, reference=d_train)\n    \n    # model training\n    LGBmodel = lgb.train(lgbParams, d_train, valid_sets=d_eval, early_stopping_rounds=100, verbose_eval=1000)\n    \n    # cross validation score\n    y_pred_lgb[test_idx] = LGBmodel.predict(testX, num_iteration=LGBmodel.best_iteration) \n    \n    # prediction on test data\n    predictions_lgb += LGBmodel.predict(X_test) \/ n_folds","c6b026ec":"# CV score\ncvscore = np.sqrt(((y_pred_lgb - y_train.values) ** 2).mean()) # Root mean squared error\nprint(\"CV score = \" + str(cvscore))","0e6064e0":"import xgboost as xgb","0ccba268":"# example hyperparameters for XGB\nxgbParams = {'learning_rate': 0.005,\n             'max_depth': 10,\n             'sabsample': 0.9,\n             'colsample_bytree': 0.9,\n             'n_estimators': 1000,\n             'objective': 'reg:linear',\n             'eval_metric': 'rmse',\n             'alpha': 0.1,\n             'verbosity': 0,\n            }","dd815a59":"# make a LightGBM dataset\ntrainX, testX, trainY, testY = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\nd_train = xgb.DMatrix(trainX, trainY)\nd_eval = xgb.DMatrix(testX, testY)\nwatchlist = [(d_eval, 'eval'), (d_train, 'train')]\nn_rounds = 5000\n\n# model training\nXGBmodel = xgb.train(xgbParams, d_train, n_rounds, watchlist, early_stopping_rounds=100, verbose_eval=1000)\n\n# feature importance\nimportance = np.asarray(list(XGBmodel.get_score(importance_type='gain').values()))\nranking = np.argsort(-importance)\nfig, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=importance[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()","7ccd4ca1":"# placeholders\ny_pred_xgb = np.zeros(np.shape(X_train)[0])\npredictions_xgb = np.zeros(np.shape(X_test)[0])\nfor train_idx, test_idx in kf.split(X_train):\n    # train, test split\n    trainX, testX = X_train.iloc[train_idx], X_train.iloc[test_idx]\n    trainY, testY = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n    # make a LightGBM dataset\n    d_train = xgb.DMatrix(trainX, trainY)\n    d_eval = xgb.DMatrix(testX, testY)\n    watchlist = [(d_eval, 'eval'), (d_train, 'train')]\n    \n    # model training\n    XGBmodel = xgb.train(xgbParams, d_train, n_rounds, watchlist, early_stopping_rounds=100, verbose_eval=1000)\n    \n    # cross validation score\n    y_pred_xgb[test_idx] = XGBmodel.predict(xgb.DMatrix(testX)) \n    \n    # prediction on test data\n    predictions_xgb += XGBmodel.predict(xgb.DMatrix(X_test)) \/ n_folds","102c92f8":"# CV score\ncvscore = np.sqrt(((y_pred_xgb - y_train.values) ** 2).mean()) # Root mean squared error\nprint(\"CV score = \" + str(cvscore))","0e7cbda0":"from catboost import CatBoostRegressor","e1312a8f":"Catmodel = CatBoostRegressor(iterations=2000, \n                          learning_rate = 0.005,\n                          use_best_model = True,\n                          eval_metric = 'RMSE',\n                          loss_function = 'RMSE',\n                          boosting_type = 'Ordered', # or Plain\n                          verbose = 0)                                     ","37cca909":"# train, test split\ntrainX, testX, trainY, testY = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n\n# model training\nCatmodel.fit(trainX, trainY, eval_set=[(trainX, trainY)], early_stopping_rounds=100)\n\n# feature importance\nimportance = Catmodel.get_feature_importance()\nranking = np.argsort(-importance)\nfig, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=importance[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()","73d04ac2":"# placeholders\ny_pred_cat = np.zeros(np.shape(X_train)[0])\npredictions_cat = np.zeros(np.shape(X_test)[0])\nfor train_idx, test_idx in kf.split(X_train):\n    # train, test split\n    trainX, testX = X_train.iloc[train_idx], X_train.iloc[test_idx]\n    trainY, testY = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n    # model training\n    Catmodel.fit(trainX, trainY, eval_set=[(testX, testY)], early_stopping_rounds=100)\n    \n    # cross validation score\n    y_pred_cat[test_idx] = Catmodel.predict(testX) \n    \n    # prediction on test data\n    predictions_cat += Catmodel.predict(X_test) \/ n_folds","46392f82":"# CV score\ncvscore = np.sqrt(((y_pred_cat - y_train.values) ** 2).mean()) # Root mean squared error\nprint(\"CV score (CatBoost) = \" + str(cvscore))","6deb4da5":"X1 = pd.DataFrame({'LGB': y_pred_lgb, 'XGB': y_pred_xgb, 'CAT': y_pred_cat})\nX1.head(7)","24f10579":"# linear model\nfrom sklearn.linear_model import BayesianRidge\n\n# fitting\nlinearModel1 = BayesianRidge()\nlinearModel1.fit(X1, y_train)\n\n# prediction\nX2 = pd.DataFrame({'LGB': predictions_lgb, 'XGB': predictions_xgb, 'CAT': predictions_cat})\npredictions1 = linearModel1.predict(X2)","91279238":"# Neural network (multi-layer perceptron)\nfrom sklearn.neural_network import MLPRegressor\nMLPmodel = MLPRegressor(random_state=1220, activation='relu', solver='sgd', learning_rate='adaptive', tol=1e-06, hidden_layer_sizes=(250, ))\n\n# Support vector machine (NuSVR)\nfrom sklearn.svm import NuSVR\nSVRmodel = NuSVR(kernel='rbf', degree=4, gamma='auto', nu=0.59, coef0=0.053, tol=1e-6)\n\n# z-scoring: not necessary for Gradient boosting models \nX_train = (X_train - np.mean(X_train)) \/ np.std(X_train)\nX_test = (X_test - np.mean(X_test)) \/ np.std(X_test)\n\n# placeholders\ny_pred_nn = np.zeros(np.shape(X_train)[0])\npredictions_nn = np.zeros(np.shape(X_test)[0])\ny_pred_svr = np.zeros(np.shape(X_train)[0])\npredictions_svr = np.zeros(np.shape(X_test)[0])\nfor train_idx, test_idx in kf.split(X_train):\n    # train, test split\n    trainX, testX = X_train.iloc[train_idx], X_train.iloc[test_idx]\n    trainY, testY = y_train.iloc[train_idx], y_train.iloc[test_idx]\n    \n    # model training\n    MLPmodel.fit(trainX, trainY)\n    SVRmodel.fit(trainX, trainY)\n    \n    # cross validation score\n    y_pred_nn[test_idx] = MLPmodel.predict(testX) \n    y_pred_svr[test_idx] = SVRmodel.predict(testX) \n    \n    # prediction on test data\n    predictions_nn += MLPmodel.predict(X_test) \/ n_folds\n    predictions_svr += SVRmodel.predict(X_test) \/ n_folds","4d3a31ea":"# CV score\ncvscore = np.sqrt(((y_pred_nn - y_train.values) ** 2).mean()) # Root mean squared error\nprint(\"CV score (Neural network) = \" + str(cvscore))\ncvscore = np.sqrt(((y_pred_svr - y_train.values) ** 2).mean()) # Root mean squared error\nprint(\"CV score (SVR) = \" + str(cvscore))","a9be9c02":"# second Stacking ================================\nX3 = pd.DataFrame({'MLP': y_pred_nn, 'SVR': y_pred_svr})\n\nlinearModel2 = BayesianRidge()\nlinearModel2.fit(X3, y_train)\n\n# prediction\nX4 = pd.DataFrame({'MLP': predictions_nn, 'SVR': predictions_svr})\npredictions2 = linearModel2.predict(X4)\n\n# final stacking (Gradient boosts + NN + SVR) ==========\nX5 = pd.DataFrame({'LGB+XGB+CAT': linearModel1.predict(X1), 'NN+SVR': linearModel2.predict(X3)})\n\nlinearModel3 = BayesianRidge()\nlinearModel3.fit(X5, y_train)\n\n# prediction\nX4 = pd.DataFrame({'LGB+XGB+CAT': predictions1, 'NN+SVR': predictions2})\npredictions = linearModel3.predict(X4)","47bab313":"# Don't forget that we log-transform our target. So we need to get it back!\npredictions = np.expm1(predictions)\n\n# submission\nsubmission = pd.DataFrame({\n    \"Id\": test_ID,\n    \"SalePrice\": predictions\n})\nsubmission.to_csv('houseprice_stacked.csv', index=False)","b7f3042c":"### Submission","5cea6137":"# How to use Light GBM","28a652a7":"## Data cleaning\nFirst, let me quickly clean and organize data such that we can play it around with Light GBM. The following processes are based on [another kernel of mine: HousePrice - data cleaning & visualization](https:\/\/www.kaggle.com\/code1110\/houseprice-data-cleaning-visualization).","b7d1c647":"## Step 2: set hyperparameters\nThe Light GBM is a complex model, which has many hyperparameters (model parameters to be set before the learning process begins) to be tuned. It is very intimidating to see [all parameters in Light GBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html). \n\nHowever, the same official website picks up important hyperparameters to make our lives easier for [parameters tuning](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html).\n\nAt the end of your day, you would have something like the following.","126da94e":"Let's see our CV score.","1488f277":"# How to use XGBoost\n\n## Step 1: import ","37a79140":"## Step 2: set hyperparameters","c686564d":"Alright, now we are ready to use the Light GBM!","5843231a":"## Step 1: import \nThis is how to import Light GBM library.","f3732c42":"### Deal with NaNs","7e4478b4":"## Step 3: model fitting (extracting feature importance)","606962e7":"#### Reference\n**Light GBM**\n- [Ke et al (2017) LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)\n- [GitHub: microsoft\/LightGBM](https:\/\/github.com\/microsoft\/LightGBM)\n\n**XGBoost**\n- [Using XGBoost in Python](https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python)\n- [GitHub: dmlc\/xgboost](https:\/\/github.com\/dmlc\/xgboost\/tree\/master\/demo\/guide-python)\n- [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)\n\n**CatBoost**\n- [GitHub: catboost\/catboost](https:\/\/github.com\/catboost\/catboost)\n- [Python package training parameters](https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html)","672840f9":"### Load data","e472ef07":"### Label encoding","5359f026":"### Add a new feature","d70a4576":"### Force our target to be normally distributed","c38dd7f3":"## Step 2: set hyperparameters","d95f73fe":"### cv score","90f53611":"> ## Step 4: model fitting (for submission)\nNormally we can get a better result by averaging model predictions from multiple cross-validation sets. This is what we will do here.","57c7812a":"Thank you very much for reading this kernel so far! I appreciate your feedback:)","c37c4cf4":"## Step 4: model fitting (for submission)","8658b352":"\nIt seems that \"TotalSF\", \"LotArea\", \"GrLivArea\" are important to determine a house price. This is an intuitive result!","8b60fcab":"# Stacking\nIt is normally a better idea to use stacking with different types of models like NN + SVM + GradientBoosting, but here let's use stacking for the three Gradient Boosting models.","750cc0f5":"### Libraries","011a0f62":"## Appendix: try other Gradient Boosting algorithms\nLightGBM is great in many ways but there are some situations where other Gradient Boosting algorthms (XGBoost & CatBoost) prevail. \nThe way to use them is also kind of unique, so let's briefly have a look one by one.","75e167fa":"## Step 3: model fitting (extracting feature importance)","ac56a537":"We can actually use Light GBM to see feature importance, which is helpful to pick up features predictive to our target. \n\nNote that we need to build a **LightGBM dataset**.","13218915":"Basically you can change these parameters based on your demands. Here are some proposals from the official websites.\n\n\n### For Faster Speed\n\n*     Use bagging by setting **bagging_fraction** and **bagging_freq**\n*     Use feature sub-sampling by setting **feature_fraction**\n*     Use small **max_bin**\n*     Use **save_binary** to speed up data loading in future learning\n*     Use parallel learning, refer to [Parallel Learning Guide](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html)\n\n### For Better Accuracy\n\n*     Use large **max_bin** (may be slower)\n*     Use small **learning_rate** with large **num_iterations**\n*     Use large **num_leaves** (may cause over-fitting)\n*     Use bigger training data\n*     Try **dart**\n\n### Deal with Over-fitting\n\n*     Use small **max_bin**\n*     Use small **num_leaves**\n*     Use **min_data_in_leaf** and **min_sum_hessian_in_leaf**\n*     Use bagging by set **bagging_fraction** and **bagging_freq**\n*     Use feature sub-sampling by set **feature_fraction**\n*     Use bigger training data\n*     Try **lambda_l1**, **lambda_l2** and **min_gain_to_split** for regularization\n*     Try **max_depth** to avoid growing deep tree\n\n","19b56e62":"### cv score","159ae5b6":"Their predicitons look similar:D Let's combine them to make one prediction!","328c89ff":"## Step 3: model fitting (extracting feature importance)","cba8b3b3":"### Predicted values from the three models","cd3791ef":"See our cleaned data!","cce7d191":"## Step 4: model fitting (for submission)","c9e2bebc":"## More stacking\nAs mentioned earlier, stacking works the best when different kinds of models are combined. Here we use NN and SVM together with the stacked Gradient Boosting models!","ebeda8a9":"# How to use CatBoost\n## Step 1: import","3a158369":"This kernel is all about **Light GBM**. Light GBM is excellent because it is\n\n- **Fast**\n- **Memory efficient**\n- **Scalable**\n\nand most importantly for Kagglers, you can expect **high prediction performance**!\n\nThanks to these amazing features, you can find the appearance of Light GBM in almost all the Kaggle competitions, especially when the task is a regression like this one.\n\nIn this kernel I surmmarize how to use Light GBM. Using it is certainly intimidating for the first time as the way to use it is slightly different from the conventional ways like sklearn.\n\nI myself often forget how to use Light GBM, so I certainly come back to this kernel for the reminder...:D"}}