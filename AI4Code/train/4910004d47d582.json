{"cell_type":{"a38683b5":"code","793b948e":"code","374ba140":"code","e047a9cb":"code","95d1f51d":"code","5e9c8446":"code","41fcd3d9":"code","9d7ca6d1":"code","ce8bf33c":"code","5084f7b1":"code","1f569617":"code","a5c37087":"code","3db086ed":"code","dd476afc":"code","23911eea":"code","328d22e9":"code","281321d2":"code","90ef55bd":"code","1bae18ca":"code","e3644b00":"code","4b3216b6":"code","9752e5e8":"code","a5a855f2":"code","bab31933":"code","9a2e3ed3":"code","e913b128":"code","2678134d":"code","b0e03252":"code","af7eedd3":"code","765886f2":"code","d66670ad":"code","0034adc5":"code","38a6d379":"code","19454ebe":"code","bf7b9412":"code","d0338d42":"code","066fae38":"code","7dbd8a1e":"code","242eda3a":"code","2c448576":"code","d9b9d356":"code","7e2558ae":"code","4b202681":"code","2fc853f0":"code","6b97d288":"code","095ac853":"code","29f44b82":"code","58df25ec":"code","2d35f016":"code","a033fba7":"code","c916c42e":"code","2477cfe3":"code","18601f34":"code","a39353df":"code","00141c25":"code","23873ba7":"code","dbd14888":"code","914e46b3":"code","1299c5c3":"code","2dd8f02b":"code","b48205b5":"markdown","10d6b848":"markdown","2bf1b7df":"markdown","3000bfdc":"markdown","d09b89b4":"markdown","41a2917a":"markdown","9b4b898a":"markdown","0aba01e9":"markdown","0f200604":"markdown","fcdc770d":"markdown","ef2ad991":"markdown","51a3394b":"markdown","eaf62dcb":"markdown","e6206d97":"markdown","51b7eceb":"markdown","b8c77c8d":"markdown","d044aa0e":"markdown","f6d49f71":"markdown","689420e8":"markdown","c1f420a0":"markdown","1e5ece3d":"markdown","7acda02a":"markdown","d84130b3":"markdown","2a0f68dc":"markdown","b37bb306":"markdown","d19ceb95":"markdown","a258a1ff":"markdown","7536a1ce":"markdown","5f98f787":"markdown","05117036":"markdown","0b890b86":"markdown","50186f6e":"markdown","ecd945cb":"markdown","85f46465":"markdown","1087ea90":"markdown"},"source":{"a38683b5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats","793b948e":"RS = 42","374ba140":"# \/kaggle\/input\/machine-predictive-maintenance-classification\/predictive_maintenance.csv\ndf_original=pd.read_csv('\/kaggle\/input\/machine-predictive-maintenance-classification\/predictive_maintenance.csv')","e047a9cb":"df_original.head()","95d1f51d":"df_original.rename(columns = {'Air temperature [K]': 'Air_temperature', 'Process temperature [K]': 'Process_temperature', 'Rotational speed [rpm]': 'Rotational_speed', 'Torque [Nm]': 'Torque', 'Tool wear [min]': 'Tool_wear'}, inplace = True)","5e9c8446":"df_original.shape","41fcd3d9":"df_original['UDI'].nunique()","9d7ca6d1":"df_original['Product ID'].nunique()","ce8bf33c":"df_original.drop(['UDI', 'Product ID', 'Failure Type'], axis = 1, inplace = True)","5084f7b1":"from sklearn.model_selection import train_test_split\n\nX = df_original.drop('Target', axis = 1)\nY = df_original['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = RS, test_size = 0.05, stratify = Y)","1f569617":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","a5c37087":"df = X_train.join(y_train, how = 'inner')\ndf.sort_index(axis = 0, inplace = True)","3db086ed":"Duplicate = df[df.duplicated(keep = 'first')]\nprint('Duplicate Entries: ', Duplicate.shape)","dd476afc":"df.info()","23911eea":"discrete_feature = [feature for feature in df.columns if len(df[feature].unique())<25 and feature not in ['Target']]\nprint('Discrete variables count: {}'.format(len(discrete_feature)))","328d22e9":"discrete_feature","281321d2":"for feature in discrete_feature:\n    plt.figure(figsize=(10,5))\n    plot=sns.countplot(df[feature])\n\n    total = len(df[feature])\n    for p in plot.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        plot.annotate(percentage, (x, y), size = 10)\n\n    plt.show() ","90ef55bd":"for i in discrete_feature:\n        plt.figure(figsize=(10,5))\n        sns.countplot(df[i],hue=df['Target'])\n        plt.show()","1bae18ca":"continuous_feature = [feature for feature in df.columns if df[feature].dtypes != 'O' and feature not in discrete_feature+['Target']]\n\nprint(\"Number of numerical features: \", len(continuous_feature))","e3644b00":"continuous_feature","4b3216b6":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","9752e5e8":"from scipy.stats import shapiro\n\nfor feature in continuous_feature:\n    data = df.copy()\n    DataToTest = data[feature]\n    stat, p = shapiro(DataToTest)\n    print(feature)\n    print('stat = %.2f, p = %.30f' % (stat, p))\n    \n    if p > 0.05:\n        print('Normal distribution')\n        print()\n    else:\n        print('Not a normal distribution')\n        print()","a5a855f2":"for i in continuous_feature:\n    f,axes=plt.subplots(1,2,figsize=(15,6))\n    sns.boxplot(x='Target', y=i, data= df, ax=axes[0])\n    df.groupby(by=['Target'])[i].mean().reset_index().sort_values(i,ascending=True).plot(x='Target',y=i,kind='bar',ax=axes[1])","bab31933":"for feature in continuous_feature:\n    \n    data = df.copy()\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","9a2e3ed3":"corr = df.corr()\nf, ax = plt.subplots(figsize=(12, 10))\nmask = np.triu(np.ones_like(corr, dtype=bool))[1:, :-1]\ncorr = corr.iloc[1:,:-1].copy()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)\nplt.yticks(rotation=0)\nplt.show()","e913b128":"X_train_final, X_validation, y_train_final, y_validation = train_test_split(X_train, y_train,  random_state = RS, test_size = 0.20, stratify = y_train)\n# X_train_final.sort_index(axis = 0, inplace = True)\n# X_validation.sort_index(axis = 0, inplace = True)\n# y_train_final.sort_index(axis = 0, inplace = True)\n# y_validation.sort_index(axis = 0, inplace = True)","2678134d":"from sklearn.preprocessing import OrdinalEncoder\n\nord = ['L', 'M', 'H']\nordinal_encoder = OrdinalEncoder(categories = [ord])\nX_train_final['Type'] = ordinal_encoder.fit_transform(X_train_final[['Type']])\nX_validation['Type'] = ordinal_encoder.transform(X_validation[['Type']])","b0e03252":"ordinal_encoder.categories_","af7eedd3":"from sklearn.preprocessing import FunctionTransformer\n\nto_transform = ['Air_temperature', 'Process_temperature', 'Rotational_speed', 'Tool_wear']\n\ntransformer = FunctionTransformer(np.log1p)\nX_train_final[to_transform] = transformer.fit_transform(X_train_final[to_transform])\nX_validation[to_transform] = transformer.transform(X_validation[to_transform])","765886f2":"from sklearn.preprocessing import StandardScaler\n\nto_scale = ['Air_temperature', 'Process_temperature', 'Rotational_speed', 'Torque', 'Tool_wear']\n\nscaler = StandardScaler()\nX_train_final[to_scale] = scaler.fit_transform(X_train_final[to_scale])\nX_validation[to_scale] = scaler.transform(X_validation[to_scale])","d66670ad":"X_train_final.head()","0034adc5":"from sklearn.feature_selection import VarianceThreshold\n\nconstant_filter = VarianceThreshold(threshold = 0.01)\nconstant_filter.fit(X_train_final)","38a6d379":"constant_filter.get_support().sum()","19454ebe":"X_train_T = X_train_final.T\nX_train_T.duplicated().sum()","bf7b9412":"df = X_train_final.copy()\nvif = pd.Series(np.linalg.inv(df.corr().values).diagonal(),index=df.columns,\n          name='VIF').abs().sort_values(ascending=False).round(2)\ndf = pd.cut(vif.round(1),[0,1,5,10,20,30,40,float('inf')]).value_counts().sort_index()\ndf.index = df.index.map(str)\n\nplt.subplots(figsize=(20, 10))\nplt.bar(x=df.index, height=df)","d0338d42":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\n\nsel = RFE(RandomForestRegressor(n_estimators = 100, random_state = 0, n_jobs = -1))\nsel.fit(X_train_final, y_train_final)\nsel.get_support()","066fae38":"features = X_train_final.columns[sel.get_support()]\nfeatures","7dbd8a1e":"X_train_sel = pd.DataFrame(sel.transform(X_train_final), columns = features)\nX_train_sel.shape, y_train_final.shape","242eda3a":"from imblearn.over_sampling import SMOTE\noversample = SMOTE(random_state = RS)\nX_train_upsampled, y_train_upsampled = oversample.fit_resample(X_train_sel, y_train_final)","2c448576":"X_train_upsampled.shape, y_train_upsampled.shape","d9b9d356":"X_validation_sel = pd.DataFrame(sel.transform(X_validation), columns = features)\nX_validation_sel.shape, y_validation.shape","7e2558ae":"def fit_models(model, X_train, X_test, y_train, y_test):\n    \n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    \n    accuracy = metrics.accuracy_score(y_test,pred)  \n    precision = metrics.precision_score(y_test,pred)   \n    recall = metrics.recall_score(y_test,pred)  \n    f1 = metrics.f1_score(y_test,pred)\n    loss = metrics.log_loss(y_test,pred)\n    \n    return accuracy, precision, recall, f1, loss","4b202681":"from scipy import stats\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nlr = LogisticRegression(random_state = RS)\ngnb = GaussianNB()\nknn = KNeighborsClassifier()\nsvm = SVC(random_state = RS)\ndT_gini = DecisionTreeClassifier(random_state = RS)\ndT_entropy = DecisionTreeClassifier(random_state = RS)\nbgcl = BaggingClassifier(n_estimators=100,random_state = RS)\nabcl = AdaBoostClassifier(n_estimators=100, random_state = RS)\ngbcl = GradientBoostingClassifier(n_estimators = 100,random_state = RS)\nrfcl = RandomForestClassifier(n_estimators = 100, random_state = RS)\nxgbcl = XGBClassifier(n_estimators = 100, random_state = RS)\n\nresult = {}\n\nfor model,name in zip([lr, gnb, knn, svm, dT_gini, dT_entropy, bgcl, abcl, gbcl, rfcl, xgbcl],\n                     ['Logistic Regression', 'Gaussian NB', 'KNN', 'SVC', 'Decision Tree(gini)',\n                     'Decision Tree(entropy)', 'Bagging Classifier', 'Adaptive Boosting', \n                      'Gradient Boosting', 'Random Forest Classifier', 'XGB Classifier']):\n    result[name] = fit_models(model,X_train_upsampled, X_validation_sel, y_train_upsampled, y_validation)","2fc853f0":"result1 = pd.DataFrame(np.array(list(result.values())),    # make a dataframe out of the metrics from result dictionary \n                       columns= ['ACCURACY', 'PRECISION', 'RECALL', 'F1-SCORE', 'LOG LOSS'], \n                       index= result.keys())   # use the model names as index\n\nresult1.index.name = 'Model'   # name the index of the result1 dataframe as 'Model'\n\nresult1 ","6b97d288":"xgb_clf = XGBClassifier(n_estimators = 100, random_state = RS)\nxgb_clf.fit(X_train_upsampled, y_train_upsampled)","095ac853":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n\npred = xgb_clf.predict(X_validation_sel)\nreport = pd.DataFrame(classification_report(y_validation, pred, output_dict=True)).T\nreport","29f44b82":"params={'n_estimators':[500,700,800,900],\n       'max_depth':[1,3,5,7,9],\n       'learning_rate':[0.01,0.1],\n        'objective':['binary:logistic'],\n        'eval_metric':['error']\n       }","58df25ec":"xgb_clf_hp = XGBClassifier(use_label_encoder=False)\n\ngrd_xgb = GridSearchCV(xgb_clf_hp, param_grid=params, cv=2)","2d35f016":"grd_xgb.fit(X_train_upsampled, y_train_upsampled)","a033fba7":"grd_xgb.best_params_","c916c42e":"xgb_clf_hp = XGBClassifier(**grd_xgb.best_params_)","2477cfe3":"xgb_clf_hp.fit(X_train_upsampled, y_train_upsampled)","18601f34":"pred = xgb_clf_hp.predict(X_validation_sel)\nreport = pd.DataFrame(classification_report(y_validation, pred, output_dict=True)).T\nreport","a39353df":"X_test.shape, y_test.shape","00141c25":"X_test['Type'] = ordinal_encoder.transform(X_test[['Type']])","23873ba7":"X_test[to_transform] = transformer.transform(X_test[to_transform])","dbd14888":"X_test[to_scale] = scaler.transform(X_test[to_scale])","914e46b3":"X_test_sel = pd.DataFrame(sel.transform(X_test), columns = features)\n","1299c5c3":"pred = xgb_clf.predict(X_test_sel)\nreport = pd.DataFrame(classification_report(y_test, pred, output_dict=True)).T\nreport","2dd8f02b":"pred = xgb_clf_hp.predict(X_test_sel)\nreport = pd.DataFrame(classification_report(y_test, pred, output_dict=True)).T\nreport","b48205b5":"***Checking for constant, quasi constant features***","10d6b848":"#### Applying Transformations","2bf1b7df":"***Hyperparameter tuning***","3000bfdc":"Only these features contribute in predicting.","d09b89b4":"#### Renaming feature names for convenience","41a2917a":"There are no missing values in the dataset","9b4b898a":"## Testing our model with new data","0aba01e9":"***Checking for multicollinearity***","0f200604":"#### Scaling the dataset","fcdc770d":"XGB Classifier outperforms the other models. Let us build our models using XGB Classifier.","ef2ad991":"***OBSERVATIONS***\n- Process_temperature and Air_temperature are highly positively correlated\n- Torque and Rotational_speed are highly negatively correlated.\n\nIn Feature Selection we will check whether these features are really required for predicting the Target.","51a3394b":"### Prediction by original model","eaf62dcb":"- Except Torque all other features are not normally distributed. \n\n- Let us apply log transformation for the skewed features.","e6206d97":"### SMOTE","51b7eceb":"## FEATURE ENGINEERING","b8c77c8d":"There are no constant and quasi constant features in the dataset.","d044aa0e":"There are no duplicate features in the dataset","f6d49f71":"## MODEL BUILDING","689420e8":"#### Checking for duplicate entries","c1f420a0":"## FEATURE SELECTION","1e5ece3d":"- Generally VIF > 10 is considered as high, None of the features have VIF value greater than 10.","7acda02a":"#### Discrete features","d84130b3":"Dropping 'UDI', 'Product ID' and 'Failure Type'","2a0f68dc":"***Checking for duplicate features***","b37bb306":"Some of the features are skewed.","d19ceb95":"#### Train test split","a258a1ff":"### Prediction by hyperparameter tuned model","7536a1ce":"- Rotational_speed and Torque have outliers in them.\n\n- We will be using machine learning models that are robust to outliers","5f98f787":"***Feature selection by Recursive Feature Elimination***","05117036":"### XGB Classifier","0b890b86":"L, M, or H for low (50% of all products), medium (30%) and high (20%) as product quality.\n\nThe dataset has 60% of low type product.","50186f6e":"#### Numerical features","ecd945cb":"There are no duplicate entries in the dataset.","85f46465":"#### Checking for outliers","1087ea90":"## EXPLORATORY DATA ANALYSIS "}}