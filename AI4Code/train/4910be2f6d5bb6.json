{"cell_type":{"d4be42df":"code","d993caca":"code","d825f4bf":"code","d716aa27":"code","75f6c761":"code","3908dc82":"code","cb0f50b0":"code","77903eaf":"code","8aedcf54":"code","9872ae77":"code","452c995e":"code","1c24de2c":"code","5216a4b3":"code","06f73d3f":"code","413f81ee":"code","0dae94eb":"code","6bbeef2c":"code","a7e5a5b0":"code","a0d4882b":"code","9526ac3e":"code","f22a92c2":"code","92b66f5b":"code","51a8f6a4":"code","49e3a64c":"code","8beea6e3":"code","f14efabc":"code","e444ddf0":"code","3ac7e66c":"code","535d00e3":"code","62a8e12b":"markdown","72638579":"markdown","4a4a3314":"markdown","9a898b83":"markdown","e3af4ea3":"markdown","ae1756ce":"markdown","d2e2fab0":"markdown","f18aa124":"markdown","b2382f0a":"markdown"},"source":{"d4be42df":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d993caca":"from typing import Dict, Any, Union\n\nfrom pathlib import Path\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\n\nimport torch.utils.data as D\nfrom torch.utils.data.dataset import Dataset, IterableDataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom transformers import PreTrainedModel\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\n\nimport yaml, gc","d825f4bf":"BASE_PATH = Path('\/kaggle\/input\/commonlit-t5-large')\nDATA_PATH = Path('\/kaggle\/input\/commonlitreadabilityprize\/')\nassert DATA_PATH.exists()\nMODELS_PATH = Path(BASE_PATH\/'best_models')\nassert MODELS_PATH.exists()","d716aa27":"train_df = pd.read_csv(DATA_PATH\/'train.csv')\ntest_df = pd.read_csv(DATA_PATH\/'test.csv')\nsample_df = pd.read_csv(DATA_PATH\/'sample_submission.csv')\n\ndef remove_unnecessary(df):\n    df.drop(df[df['target'] == 0].index, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    \nremove_unnecessary(train_df)","75f6c761":"class Config(): \n    NUM_FOLDS = 6\n    NUM_EPOCHS = 3\n    BATCH_SIZE = 16\n    MAX_LEN = 248\n    MODEL_PATH = BASE_PATH\/'lm'\n#     TOKENIZER_PATH = str(MODELS_PATH\/'roberta-base-0')\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    SEED = 1000\n    NUM_WORKERS = 2\n    MODEL_FOLDER = MODELS_PATH\n    model_name = 't5-large'\n    svm_kernels = ['rbf']\n    svm_c = 5\n\ncfg = Config()","3908dc82":"train_df['normalized_target'] = (train_df['target'] - train_df['target'].mean()) \/ train_df['target'].std()","cb0f50b0":"model_path = MODELS_PATH\nassert model_path.exists()","77903eaf":"!ls {MODELS_PATH}","8aedcf54":"class AttentionHead(nn.Module):\n    \n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        \n        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n        self.final_layer = nn.Linear(hidden_dim, num_targets)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.hidden_layer(features))\n        score = self.final_layer(att)\n        attention_weights = torch.softmax(score, dim=1)\n        return attention_weights","9872ae77":"from transformers import T5EncoderModel\n\nclass CommonLitModel(nn.Module):\n    def __init__(self):\n        super(CommonLitModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = T5EncoderModel.from_pretrained(cfg.MODEL_PATH, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector","452c995e":"def load_model(i):\n    inference_model = CommonLitModel()\n    inference_model = inference_model.cuda()\n    inference_model.load_state_dict(torch.load(str(model_path\/f'{i + 1}_pytorch_model.bin')))\n    inference_model.eval();\n    return inference_model","1c24de2c":"def convert_to_list(t):\n    return t.flatten().long()\n\nclass CommonLitDataset(nn.Module):\n    def __init__(self, text, test_id, tokenizer, max_len=128):\n        self.excerpt = text\n        self.test_id = test_id\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return {'input_ids': convert_to_list(encode['input_ids']),\n                'attention_mask': convert_to_list(encode['attention_mask']),\n                'id': self.test_id[idx]}\n    \n    def __len__(self):\n        return len(self.excerpt)","5216a4b3":"!ls {MODELS_PATH}\/tokenizer-1","06f73d3f":"from transformers import T5Tokenizer\n\ntokenizers = []\nfor i in range(1, cfg.NUM_FOLDS):\n    tokenizer_path = MODELS_PATH\/f\"tokenizer-{i}\"\n    print(tokenizer_path)\n    assert(Path(tokenizer_path).exists())\n    tokenizer = T5Tokenizer.from_pretrained(str(tokenizer_path))\n    tokenizers.append(tokenizer)","413f81ee":"def create_dl(df, tokenizer):\n    text = df['excerpt'].values\n    ids = df['id'].values\n    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n    return DataLoader(ds, \n                      batch_size = cfg.BATCH_SIZE,\n                      shuffle=False,\n                      num_workers = 1,\n                      pin_memory=True,\n                      drop_last=False\n                     )","0dae94eb":"def get_cls_embeddings(dl, transformer_model):\n    cls_embeddings = []\n    with torch.no_grad():\n        for input_features in tqdm(dl, total=len(dl)):\n            _, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n            embedding_out = context_vector.detach().cpu().numpy()\n            cls_embeddings.extend(embedding_out)\n    return np.array(cls_embeddings)","6bbeef2c":"num_bins = int(np.ceil(np.log2(len(train_df))))\ntrain_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\nbins = train_df['bins'].values","a7e5a5b0":"def rmse_score(X, y):\n    return np.sqrt(mean_squared_error(X, y))","a0d4882b":"%%time\n\ntrain_target = train_df['normalized_target'].values\n\ndef calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)\n\nfinal_scores = []\nfinal_rmse = []\nfor j, tokenizer in enumerate(tokenizers):\n    print('Model', j)\n    test_dl = create_dl(test_df, tokenizer)\n    train_dl = create_dl(train_df, tokenizer)\n    transformer_model = load_model(j)\n    transformer_model.cuda()\n    X = get_cls_embeddings(train_dl, transformer_model)\n    y = train_target\n    X_test = get_cls_embeddings(test_dl, transformer_model)\n    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n    scores = []\n    rmse_scores = []\n    for kernel in cfg.svm_kernels:\n        print('Kernel', kernel)\n        kernel_scores = []\n        kernel_rmse_scores = []\n        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n\n            print('Fold', k, train_idx.shape, valid_idx.shape)\n            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_valid, y_valid = X[valid_idx], y[valid_idx]\n            model.fit(X_train, y_train)\n            prediction = model.predict(X_valid)\n            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n            print('rmse_score', kernel_rmse_scores[k])\n            kernel_scores.append(model.predict(X_test))\n        scores.append(calc_mean(kernel_scores))\n        rmse_scores.append(calc_mean(kernel_rmse_scores))\n    final_scores.append(calc_mean(scores))\n    final_rmse.append(calc_mean(rmse_scores))\n    del transformer_model\n    torch.cuda.empty_cache()\n    del tokenizer\n    gc.collect()\nprint('FINAL RMSE score', np.mean(np.array(final_rmse)))","9526ac3e":"final_scores_bck = final_scores","f22a92c2":"final_scores_bck","92b66f5b":"# (train_df['target'] - cfg.train_target_mean) \/ cfg.train_target_std\nfinal_scores = np.array(final_scores) * train_df['target'].std() + train_df['target'].mean()","51a8f6a4":"final_scores","49e3a64c":"def calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)","8beea6e3":"target_mean = train_df['target'].mean()\nfinal_scores_flat = calc_mean(final_scores).flatten()\nfinal_scores_mean = final_scores_flat.mean()\ntarget_mean, np.array(final_scores).mean()","f14efabc":"mean_diff = target_mean - final_scores_mean\nmean_diff, mean_diff \/ len(final_scores)","e444ddf0":"sample_df['target'] = final_scores_flat\n# sample_df['target'] = len(final_scores) \/ np.sum(1 \/ np.array(final_scores), axis=0) # harmonic mean\nsample_df","3ac7e66c":"pd.DataFrame(sample_df).to_csv('submission.csv', index=False)","535d00e3":"!cat submission.csv","62a8e12b":"#### Ensure the mean of the prediction equals the mean of the training data","72638579":"### Read Existing Models","4a4a3314":"### Configuration","9a898b83":"#### Extract Number of Bins","e3af4ea3":"#### Training","ae1756ce":"#### Extract Embeddings","d2e2fab0":"### Folders and Dataframes","f18aa124":"This is an inference notebook with the `t5-large` model. You can achieve decent results with this model and choose to use the provided model in an ensemble of yours.","b2382f0a":"### DataSet and Tokenizers"}}