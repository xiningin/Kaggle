{"cell_type":{"208285d9":"code","c9834d13":"code","aaae1155":"code","1b56fdca":"code","dd5ed27c":"code","d3258926":"code","4c8afbf8":"code","880d4aa9":"code","47ee15b1":"code","a9c74a56":"code","7d992e30":"code","3739f62c":"code","87af2713":"code","98339d84":"code","c1aac28e":"code","7d69a8b3":"code","359ea829":"code","effbf5de":"code","6164aba5":"code","5e293d42":"code","14926eaf":"code","5baeb9a2":"code","7678c164":"code","297aec8b":"code","3742a5b4":"code","60d40c25":"code","4c28a99d":"code","5f30de77":"code","8c5a5f5e":"code","acb02754":"code","31759319":"code","c350866e":"code","ff652e4c":"code","b257758d":"code","82b2394a":"code","b3ec06a7":"code","6b5487f8":"code","6f7d4357":"code","124ddd71":"code","c5ec55a5":"code","eab542e8":"code","29c59b16":"code","b400362d":"code","8fc2b136":"code","af26ce37":"code","000c8482":"code","b5c6ab08":"code","4e08377e":"code","291c3bfd":"code","e18ee7a7":"code","cb8a7f68":"code","fe79c3a8":"code","71e70970":"code","5b25463a":"code","62a84b2b":"code","16a62dc5":"code","7d66141d":"code","1bfa7f89":"code","9b3533b1":"code","85ad4782":"markdown","f4b67aaf":"markdown","4b159bb7":"markdown","5af9fbe9":"markdown","d5ab42af":"markdown","06c68a92":"markdown","1f98ba77":"markdown","46ccb3c7":"markdown","069beac6":"markdown","c049b2eb":"markdown","809e209c":"markdown","aa35d9eb":"markdown","2b5d53bc":"markdown","f6351ec3":"markdown","a76fc704":"markdown","ef28d9e4":"markdown","08b08e70":"markdown","545be215":"markdown","2b61904a":"markdown","4be07b8e":"markdown","6fef413d":"markdown","6a36d7fe":"markdown","44dff714":"markdown","800e2ee8":"markdown","20b2ce07":"markdown","c9293d30":"markdown","294877aa":"markdown","b476adf3":"markdown","43512281":"markdown","8e23f9f6":"markdown","6e217d0c":"markdown","102ac7ce":"markdown"},"source":{"208285d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9834d13":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve","aaae1155":"df=pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","1b56fdca":"df.head()","dd5ed27c":"df.info()","d3258926":"df.describe()","4c8afbf8":"sns.heatmap(df.isnull(),cmap='viridis')","880d4aa9":"df['Outcome'].value_counts()","47ee15b1":"labels=['True','False']\nexplode=[0.03,0.03]\ncolor=['pink','lightgreen']","a9c74a56":"f,ax = plt.subplots(1,2,figsize = (15, 7))\n_=df.Outcome.value_counts().plot.bar(ax=ax[0],cmap='viridis')\n_=df.Outcome.value_counts().plot.pie(ax=ax[1],labels=labels,autopct='%.2f%%',colors=color,explode=explode)","7d992e30":"sns.pairplot(df,hue='Outcome')","3739f62c":"plt.figure(figsize=(10,6))\nsns.heatmap(df.corr(),annot=True,cmap='plasma',linecolor='black',linewidths=0.01)","87af2713":"fig, ax = plt.subplots(4,2,figsize=(16,16))\nsns.distplot(df.Age,bins=20, ax=ax[0,0]) \nsns.distplot(df.Pregnancies,bins=20,ax=ax[0,1]) \nsns.distplot(df.Glucose,bins=20,ax=ax[1,0]) \nsns.distplot(df.BloodPressure,bins=20,ax=ax[1,1]) \nsns.distplot(df.SkinThickness,bins=20,ax=ax[2,0])\nsns.distplot(df.Insulin,bins=20,ax=ax[2,1])\nsns.distplot(df.DiabetesPedigreeFunction,bins=20,ax=ax[3,0]) \nsns.distplot(df.BMI,bins=20,ax=ax[3,1]) ","98339d84":"plt.figure(figsize=(15,6))\nsns.countplot('Pregnancies',hue='Outcome',data=df,palette='viridis')\nplt.legend(loc='upper right',labels=['False','True'])","c1aac28e":"data=df.copy(deep=True)\ndata[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n      'BMI', 'DiabetesPedigreeFunction']]=data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction']].replace(0,np.NaN)","7d69a8b3":"data.isnull().sum()","359ea829":"data=data.fillna(data.mean())","effbf5de":"plt.figure(figsize=(10,6))\nsns.heatmap(data.corr(),annot=True,cmap='plasma',linecolor='black',linewidths=0.01)","6164aba5":"fig,ax=plt.subplots(4,2,figsize=(16,16))\nsns.distplot(data['Pregnancies'],ax=ax[0,0],bins=20)\nsns.distplot(data['Glucose'],ax=ax[0,1],bins=20)\nsns.distplot(data['BloodPressure'],ax=ax[1,0],bins=20)\nsns.distplot(data['SkinThickness'],ax=ax[1,1],bins=20)\nsns.distplot(data['Insulin'],ax=ax[2,0],bins=20)\nsns.distplot(data['BMI'],ax=ax[2,1],bins=20)\nsns.distplot(data['DiabetesPedigreeFunction'],ax=ax[3,0],bins=20)\nsns.distplot(data['Age'],ax=ax[3,1],bins=20)","5e293d42":"X=data.drop('Outcome',axis=1)\ny=data['Outcome']","14926eaf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","5baeb9a2":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","7678c164":"knn=KNeighborsClassifier()\nparams={'n_neighbors':range(1,21),'p':[1,2,3,4,5,6,7,8,9,10],\n        'weights':['distance','uniform'],'leaf_size':range(1,21)}","297aec8b":"gs_knn=GridSearchCV(knn,param_grid=params,cv=10,n_jobs=-1)","3742a5b4":"gs_knn.fit(X_train,y_train)\ngs_knn.best_params_","60d40c25":"prediction=gs_knn.predict(X_test)","4c28a99d":"acc_knn=accuracy_score(y_test,prediction)\nprint(acc_knn)\nprint(confusion_matrix(y_test,prediction))","5f30de77":"probability=gs_knn.predict_proba(X_test)[:,1]","8c5a5f5e":"fpr_knn,tpr_knn,thresh=roc_curve(y_test,probability)","acb02754":"plt.figure(figsize=(12,6))\nplt.plot(fpr_knn,tpr_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='0.5')\nplt.plot([1,1],c='0.5')","31759319":"roc_auc_score(y_test,probability)*100","c350866e":"log_reg=LogisticRegression()\nparams={'C':[0.01,0.1,1,10],'max_iter':[100,300,600]}","ff652e4c":"gs_lr=GridSearchCV(log_reg,param_grid=params,n_jobs=-1,cv=10)","b257758d":"gs_lr.fit(X_train,y_train)\ngs_lr.best_params_","82b2394a":"prediction=gs_lr.predict(X_test)","b3ec06a7":"acc_lr=accuracy_score(y_test,prediction)\nprint(acc_lr)\nprint(confusion_matrix(y_test,prediction))","6b5487f8":"probability=gs_lr.predict_proba(X_test)[:,1]","6f7d4357":"fpr_lr,tpr_lr,thresh=roc_curve(y_test,probability)","124ddd71":"plt.figure(figsize=(14,6))\nplt.plot(fpr_lr,tpr_lr)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='0.5')\nplt.plot([1,1],c='0.5')","c5ec55a5":"roc_auc_score(y_test,probability)*100","eab542e8":"dtr=DecisionTreeClassifier()\nparams={'max_features':[\"auto\", \"sqrt\", \"log2\"],'min_samples_leaf':range(1,11),'min_samples_split':range(1,11)}","29c59b16":"gs_dtr=GridSearchCV(dtr,param_grid=params,n_jobs=-1,cv=5)","b400362d":"gs_dtr.fit(X_train,y_train)\ngs_dtr.best_params_","8fc2b136":"prediction=gs_dtr.predict(X_test)","af26ce37":"acc_dtr=accuracy_score(y_test,prediction)\nprint(acc_dtr)\nprint(confusion_matrix(y_test,prediction))","000c8482":"probability=gs_dtr.predict_proba(X_test)[:,1]","b5c6ab08":"fpr_dtr,tpr_dtr,thresh=roc_curve(y_test,probability)","4e08377e":"plt.figure(figsize=(14,6))\nplt.plot(fpr_dtr,tpr_dtr)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='0.5')\nplt.plot([1,1],c='0.5')","291c3bfd":"roc_auc_score(y_test,probability)*100","e18ee7a7":"rfc=RandomForestClassifier()\nparams={'n_estimators':[100,300,500],'min_samples_leaf':range(1,11)}","cb8a7f68":"gs_rfc=GridSearchCV(rfc,param_grid=params,n_jobs=-1,cv=5)","fe79c3a8":"gs_rfc.fit(X_train,y_train)\ngs_rfc.best_params_","71e70970":"prediction=gs_rfc.predict(X_test)","5b25463a":"acc_rfc=accuracy_score(y_test,prediction)\nprint(acc_rfc)\nprint(confusion_matrix(y_test,prediction))","62a84b2b":"probability=gs_rfc.predict_proba(X_test)[:,1]","16a62dc5":"fpr_rfc,tpr_rfc,thresh=roc_curve(y_test,probability)","7d66141d":"plt.figure(figsize=(14,6))\nplt.plot(fpr_rfc,tpr_rfc)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='0.5')\nplt.plot([1,1],c='0.5')","1bfa7f89":"roc_auc_score(y_test,probability)*100","9b3533b1":"report=pd.DataFrame({'Model':['KNeighborsClassifier','LogisticRegression','DecisionTreeClassifier','RandoForestClassifier'],\n                    'Score':[acc_knn,acc_lr,acc_dtr,acc_rfc]})\nreport.sort_values(by='Score',ascending=False)","85ad4782":"## Let's check the distribution again","f4b67aaf":"# Scaling the data","4b159bb7":"# Exploratory data analysis","5af9fbe9":"# Classifying the data\n\n## Hyperparameter Tuning:\n\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters are learned. The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss.\nFor more info : https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization\n\n\n## Reciever Operating Characteristics\n\nA receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n\nThe ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection in machine learning. The false-positive rate is also known as probability of false alarm and can be calculated as (1 \u2212 specificity). It can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from \u2212 \u221e to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis. \nFor more info: https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic","d5ab42af":"# LogisticRegression","06c68a92":"* Most people have been pregnant twice max\n* Looks like higher the number of pregnancies, higher the chances of being diabetic.","1f98ba77":"# Data Visualization","46ccb3c7":"# Distribution Plot","069beac6":"# Some more Exploratory Data Analysis","c049b2eb":"# Splitting Data","809e209c":"# DecisionTreeClassifier","aa35d9eb":"## Outcome","2b5d53bc":"## Let's checkout the heatmap now","f6351ec3":"# Splitting the data","a76fc704":"## Replacing NaN values (if any) with zero","ef28d9e4":"# Importing libraries","08b08e70":"# Feature Engineering","545be215":"We are good to go","2b61904a":"# Replacing null values with mean","4be07b8e":"# Plotting a heatmap","6fef413d":"# Reading the data","6a36d7fe":"# Checking for null values","44dff714":"## Here we'll be performing:\n* Exploratory data analysis\n* Data Visualization\n* Feature Engineering\n* Classification","800e2ee8":"# Comparing the accuracies","20b2ce07":"# KNeighborsClassifier","c9293d30":"# Looking for Relationships","294877aa":"OK!. Here we find there is good correlation between :\n* SkinThickness and BMI\n* Age and Pregnancy\n* Glucose and Outcome\n* Glucose and Insulin","b476adf3":"# RandomForestClassifier","43512281":"That's a bit hard to interpret.\nLet's try another approach","8e23f9f6":"# Which attributes heavily affect the outcome?","6e217d0c":"# PIMA Diabetics Classification\n\nDiabetes mellitus (DM), commonly known as diabetes, is a group of metabolic disorders characterized by a high blood sugar level over a prolonged period of time. Symptoms often include frequent urination, increased thirst, and increased appetite. If left untreated, diabetes can cause many complications. Acute complications can include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcers, damage to the nerves, damage to the eyes and cognitive impairment.\n\nDiabetes is due to either the pancreas not producing enough insulin, or the cells of the body not responding properly to the insulin produced. There are three main types of diabetes mellitus:\n\n* Type 1 diabetes results from the pancreas's failure to produce enough insulin due to loss of beta cells. This form was previously referred to as \"insulin-dependent diabetes mellitus\" (IDDM) or \"juvenile diabetes\". The loss of beta cells is caused by an autoimmune response. The cause of this autoimmune response is unknown.\n* Type 2 diabetes begins with insulin resistance, a condition in which cells fail to respond to insulin properly. As the disease progresses, a lack of insulin may also develop. This form was previously referred to as \"non insulin-dependent diabetes mellitus\" (NIDDM) or \"adult-onset diabetes\". The most common cause is a combination of excessive body weight and insufficient exercise.\n* Gestational diabetes is the third main form, and occurs when pregnant women without a previous history of diabetes develop high blood sugar levels.","102ac7ce":"## Count of total number of pregnancies"}}