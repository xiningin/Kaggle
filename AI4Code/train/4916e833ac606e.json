{"cell_type":{"643114f1":"code","f4d2b302":"code","ce1c3f29":"code","2cc9be30":"code","e9658b48":"code","a65e6609":"code","5ac23c53":"code","1bf12ab1":"code","6806f150":"code","0473a97e":"code","7a4b8dac":"code","1b448b26":"markdown"},"source":{"643114f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4d2b302":"df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf","ce1c3f29":"df['Date'] = pd.to_datetime(df.Date)\ndf.describe()","2cc9be30":"print(df.shape)\ndf.info()","e9658b48":"df.dropna(subset=['RainTomorrow'] , inplace=True)","a65e6609":"df.info()","5ac23c53":"input_col = df.columns.tolist()[1 : -1]\ntarget_col = 'RainTomorrow'","1bf12ab1":"df[input_col].info()\n","6806f150":"numerical_col = df[input_col].select_dtypes(np.number).columns.tolist()\ncategorical_col = df[input_col].select_dtypes('object').columns.tolist()\nnumerical_col , categorical_col","0473a97e":"import plotly.express as px\nfig = px.scatter_matrix(df[numerical_col] ,width=1600, height=2000)\nfig.show()","7a4b8dac":"df[numerical_col].isna().sum()","1b448b26":"# How to get started?\nAny regression or classification dataset problem in general needs to follow the following steps :\n- Cleaning the data\n- Exploring the data and finding out relations with different features \n- Finding out the input\/target columns \n- Differentiating the numerical columns and categorical columns \n- imputing\n- scaling \n- encoding\n- model selection\n- train the model \/ make predictions \n- save the model \/ submit the predictions"}}