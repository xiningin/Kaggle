{"cell_type":{"f3ee71f2":"code","5670f3a5":"code","2a956dee":"code","25a6f167":"code","3d8d5d31":"code","2f96b8c6":"code","2e91b2e1":"code","750d4754":"code","b0b4eeab":"code","e432bb55":"code","d76f52f5":"code","5d9c2d2c":"code","56018586":"code","244c8058":"code","c8b7cef5":"code","ec0eceaa":"code","eb87171f":"code","231be899":"code","9886baa9":"code","bbd94b97":"code","3184b459":"code","ff8a35ef":"code","e44506b3":"code","1e3231ae":"code","f818f02b":"code","6ac71cab":"code","e4c9e83a":"code","ea64ddb8":"code","0b9a346b":"code","87a9d55e":"code","4107fafa":"code","6fcb1850":"code","275be21b":"code","038076bd":"code","7ec70fc4":"code","8b5677db":"code","0799508c":"code","a232afd3":"code","ea7e86e5":"code","8a41b319":"code","832a79ce":"code","0478d0ce":"code","90eb324e":"code","89cd945d":"code","370176de":"code","0f3d6c63":"markdown","b6c51d9e":"markdown","f5508ee6":"markdown","6884dae8":"markdown","db2207a9":"markdown","11301ccd":"markdown","8207d6a5":"markdown","42a5662f":"markdown","f5a1d848":"markdown","fdeeffdb":"markdown","efdd6ed5":"markdown","7c7299f2":"markdown","fce5a3e3":"markdown","95067b21":"markdown","574277fd":"markdown","75a02a9e":"markdown","338e6b6a":"markdown","4dedd628":"markdown","6a55f8d5":"markdown","b44cd6d7":"markdown","0f5556e2":"markdown","a18095ed":"markdown","169945b3":"markdown","b8db61b3":"markdown","0f5bca49":"markdown","8cf0bfd2":"markdown","423f0b7c":"markdown","e0f3e1af":"markdown","c875c865":"markdown","24ac911f":"markdown","523fb077":"markdown","4e19318e":"markdown","ed355c44":"markdown","5b741d3a":"markdown","2af96574":"markdown","f0e1b9e9":"markdown"},"source":{"f3ee71f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Plotting function\n%matplotlib inline\nimport seaborn as sns #Plotting Aesthetics\nsns.set_style('ticks')\nimport os #For importing the data\n\n#For Accuracy and Prediction\nfrom sklearn.metrics import confusion_matrix # For Machine Learning Accuracy Representation\nimport itertools\nfrom sklearn import metrics \n\n#Cleaning Data\nfrom sklearn.preprocessing import LabelEncoder\n\n#Scaling Data\nfrom sklearn.preprocessing import StandardScaler\n\n#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#Model Selection and Data Splitting\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n\n#Classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import RidgeClassifier\n\n\n# For Importing data\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5670f3a5":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","2a956dee":"train.info()\nprint('----------------------------------------')\ntest.info()","25a6f167":"train.columns","3d8d5d31":"train_new=train.drop(['PassengerId','Cabin','Ticket','Name'],axis=1)\ntest_new=test.drop(['PassengerId','Cabin','Ticket','Name'],axis=1)","2f96b8c6":"train_new['Age'].fillna((train['Age'].mean()), inplace=True)\ntrain_new['Embarked'].fillna('', inplace=True)\ntest_new['Age'].fillna((test['Age'].mean()), inplace=True)\ntest_new['Embarked'].fillna('', inplace=True)\ntest_new['Fare'].fillna(test['Fare'].mean(),inplace=True)","2e91b2e1":"train_new.info()\nprint('----------------------------------------')\ntest_new.info()","750d4754":"#plot multiple histograms with Seaborn\nsurvived = train_new[train.Survived == 0]\nsns.distplot(train_new['Age'],  kde=False,color='blue', label='Survived')\n\nnot_survived = train_new[train.Survived == 1]\nsns.distplot(not_survived['Age'], kde=False,color='red', label='Not Survived')\n\nplt.legend(prop={'size': 12})\nplt.title('Age of Titanic Passengers', fontsize=18)\nplt.xlabel('Age', fontsize=16)\nplt.ylabel('Count', fontsize=16)","b0b4eeab":"sns.countplot(train_new['Survived'],palette=\"GnBu_d\")\nplt.title('Titanic Passenger Survival')","e432bb55":"sns.countplot(train_new['Sex'],palette=\"GnBu_d\")\nplt.title('Sex of Titanic Passengers')","d76f52f5":"sns.countplot(train_new['Pclass'],hue=train_new['Survived'],palette=\"GnBu_d\")\nplt.title('Survival by Class of Titanic Passengers')","5d9c2d2c":"sns.countplot(train_new['Embarked'],hue=train_new['Pclass'],palette=\"GnBu_d\")\nplt.title('Embarked by Class of Titanic Passengers')","56018586":"sns.countplot(train_new['Sex'],hue=train_new['Pclass'],palette=\"GnBu_d\")\nplt.title('Sex by Class of Titanic Passengers')","244c8058":"sns.scatterplot(train_new['Age'],train_new['Fare'],train_new['Sex'],palette=['red','blue'])\nplt.title('Fare by Age and Sex')\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.show()","c8b7cef5":"le=LabelEncoder()\ntrain_new['Embarked_Code']=le.fit_transform(train_new['Embarked'])\ntest_new['Embarked_Code']=le.transform(test_new['Embarked'])\ntrain_new['Sex_Code']=le.fit_transform(train_new['Sex'])\ntest_new['Sex_Code']=le.transform(test_new['Sex'])\ntrain_new","ec0eceaa":"train_new.info()","eb87171f":"sns.heatmap(train_new.corr(),annot=True,cmap='Blues')","231be899":"train_new.columns","9886baa9":"train_accuracy= []\naccuracy_list = []\nalgorithm = []\n\nX_train, X_test, y_train, y_test = train_test_split(train_new[['Pclass','Age','SibSp','Parch', 'Fare','Embarked_Code', 'Sex_Code']],\n                                                    train_new['Survived'],test_size=0.3, random_state=0)","bbd94b97":"scaler=StandardScaler()\nX_train_scaled=scaler.fit_transform(X_train)\nX_test_scaled=scaler.transform(X_test)","3184b459":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion Matrix',\n                          cmap=plt.cm.BuGn):\n\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","ff8a35ef":"Log_param={'C':[0.005,0.01,0.1,0.5,1],'solver':['lbfgs', 'liblinear', 'sag'],'max_iter':[100,500,1000],'multi_class':['ovr']}\nLog_Reg=LogisticRegression()\nLog_parm=GridSearchCV(Log_Reg, Log_param, cv=5)\nLog_parm.fit(X_train_scaled, y_train)\ny_reg=Log_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",Log_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(Log_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_reg)))\ncm = metrics.confusion_matrix(y_test, y_reg)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='Logistic Regression')\naccuracy_list.append(metrics.accuracy_score(y_test, y_reg)*100)\ntrain_accuracy.append(Log_parm.score(X_train_scaled, y_train))\nalgorithm.append('Logistic Regression')","e44506b3":"SVC_param={'kernel':['sigmoid','rbf','poly'],'C':[0.005,0.01,0.1,0.5,1,1.25],'decision_function_shape':['ovr'],'random_state':[0]}\nSVC_pol=SVC()\nSVC_parm=GridSearchCV(SVC_pol, SVC_param, cv=5)\nSVC_parm.fit(X_train_scaled, y_train)\ny_pol=SVC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",SVC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(SVC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_pol)))\ncm = metrics.confusion_matrix(y_test, y_pol)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='SVM')\ntrain_accuracy.append(SVC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_pol)*100)\nalgorithm.append('SVM')","1e3231ae":"error = []\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=i, p=2,\n                     weights='uniform')\n    K_NN.fit(X_train_scaled, y_train)\n    pred_i = K_NN.predict(X_test_scaled)\n    error.append(np.mean(pred_i != y_test))","f818f02b":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","6ac71cab":"K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=14, p=2,\n                     weights='uniform')\nK_NN.fit(X_train_scaled, y_train)\ny_KNN=K_NN.predict(X_test_scaled)\nprint(\"Train Accuracy {0:.3f}\".format(K_NN.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_KNN)))\ncm = metrics.confusion_matrix(y_test, y_KNN)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='KNN')\ntrain_accuracy.append(K_NN.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_KNN)*100)\nalgorithm.append('KNN')","e4c9e83a":"RFC_param={'max_depth':[1,2,3,4,5],'n_estimators':[10,25,50,100,150],'criterion':['entropy','gini'],\n           'ccp_alpha':[0,0.01,0.1],'max_features':[0.5,'auto']}\nRFC=RandomForestClassifier()\nRFC_parm=GridSearchCV(RFC, RFC_param, cv=5)\nRFC_parm.fit(X_train_scaled, y_train)\ny_RFC=RFC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",RFC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RFC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RFC)))\ncm = metrics.confusion_matrix(y_test, y_RFC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='RFC')\ntrain_accuracy.append(RFC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_RFC)*100)\nalgorithm.append('Random Forest')","ea64ddb8":"GBC_parma={'loss':['deviance','exponential'],'n_estimators':[10,25,50,100,150],'learning_rate':[0.1,0.25, 0.5, 0.75],\n          'criterion':['friedman_mse'], 'max_features':[None],'max_depth':[1,2,3,4,5,10],'ccp_alpha':[0,0.01,0.1]}\nGBC = GradientBoostingClassifier()\nGBC_parm=GridSearchCV(GBC, GBC_parma, cv=5)\nGBC_parm.fit(X_train_scaled, y_train)\ny_GBC=GBC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",GBC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(GBC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_GBC)))\ncm = metrics.confusion_matrix(y_test, y_GBC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='GBC')\ntrain_accuracy.append(GBC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_GBC)*100)\nalgorithm.append('GBC')","0b9a346b":"RC_parma={'solver':['svd','lsqr','cholesky'],'alpha':[0,0.5,0.75,1,1.5,2],'normalize':[True,False]}\nRC=RidgeClassifier()\nRC_parm=GridSearchCV(RC, RC_parma, cv=5)\nRC_parm.fit(X_train_scaled, y_train)\ny_RC=RC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",RC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RC)))\ncm = metrics.confusion_matrix(y_test, y_RC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='Ridge Classifier')\ntrain_accuracy.append(RC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_RC)*100)\nalgorithm.append('Ridge Classifier')","87a9d55e":"X_feat=train_new[['Pclass','Age', 'SibSp', 'Parch', 'Fare','Embarked_Code', 'Sex_Code']]\ny_feat=train_new['Survived']","4107fafa":"#Feature Selection\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(X_feat,y_feat)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_feat.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(3,'Score'))  #print 10 best features","6fcb1850":"train_accuracy_fet= []\naccuracy_list_fet = []\nalgorithm_fet = []\nX_train_fet, X_test_fet, y_train_fet, y_test_Fet = train_test_split(train_new[['Pclass', 'Fare', 'Sex_Code']],\n                                                    train_new['Survived'],test_size=0.3, random_state=0)","275be21b":"X_train_fet_scaled=scaler.fit_transform(X_train_fet)\nX_test_fet_scaled=scaler.transform(X_test_fet)","038076bd":"Log_param={'C':[0.005,0.01,0.1,0.5,1],'solver':['lbfgs', 'liblinear', 'sag'],'max_iter':[100,500,1000],'multi_class':['ovr']}\nLog_Reg=LogisticRegression()\nLog_parm=GridSearchCV(Log_Reg, Log_param, cv=5)\nLog_parm.fit(X_train_fet_scaled, y_train)\ny_reg=Log_parm.predict(X_test_fet_scaled)\nprint(\"The best parameters are \",Log_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(Log_parm.score(X_train_fet_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_reg)))\ncm = metrics.confusion_matrix(y_test, y_reg)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='Logistic Regression')\naccuracy_list_fet.append(metrics.accuracy_score(y_test, y_reg)*100)\ntrain_accuracy_fet.append(Log_parm.score(X_train_fet_scaled, y_train))\nalgorithm_fet.append('Logistic Regression')","7ec70fc4":"SVC_param={'kernel':['sigmoid','rbf','poly'],'C':[0.005,0.01,0.1,0.5,1,1.25],'decision_function_shape':['ovr'],'random_state':[0]}\nSVC_pol=SVC()\nSVC_parm=GridSearchCV(SVC_pol, SVC_param, cv=5)\nSVC_parm.fit(X_train_fet_scaled, y_train)\ny_pol=SVC_parm.predict(X_test_fet_scaled)\nprint(\"The best parameters are \",SVC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(SVC_parm.score(X_train_fet_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_pol)))\ncm = metrics.confusion_matrix(y_test, y_pol)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='SVM')\ntrain_accuracy_fet.append(SVC_parm.score(X_train_fet_scaled, y_train))\naccuracy_list_fet.append(metrics.accuracy_score(y_test, y_pol)*100)\nalgorithm_fet.append('SVM')","8b5677db":"error = []\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=i, p=2,\n                     weights='distance')\n    K_NN.fit(X_train_fet_scaled, y_train)\n    pred_i = K_NN.predict(X_test_fet_scaled)\n    error.append(np.mean(pred_i != y_test))","0799508c":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","a232afd3":"K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=12, p=2,\n                     weights='distance')\nK_NN.fit(X_train_fet_scaled, y_train)\ny_KNN=K_NN.predict(X_test_fet_scaled)\nprint(\"Train Accuracy {0:.3f}\".format(K_NN.score(X_train_fet_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_KNN)))\ncm = metrics.confusion_matrix(y_test, y_KNN)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='KNN')\ntrain_accuracy_fet.append(K_NN.score(X_train_fet_scaled, y_train))\naccuracy_list_fet.append(metrics.accuracy_score(y_test, y_KNN)*100)\nalgorithm_fet.append('KNN')","ea7e86e5":"RFC_param={'max_depth':[1,2,3,4,5],'n_estimators':[10,25,50,100,150,200],'criterion':['entropy','gini'],\n           'ccp_alpha':[0,0.01,0.1],'max_features':[0.5,'auto']}\nRFC=RandomForestClassifier()\nRFC_parm=GridSearchCV(RFC, RFC_param, cv=5)\nRFC_parm.fit(X_train_fet_scaled, y_train)\ny_RFC=RFC_parm.predict(X_test_fet_scaled)\nprint(\"The best parameters are \",RFC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RFC_parm.score(X_train_fet_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RFC)))\ncm = metrics.confusion_matrix(y_test, y_RFC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='RFC')\ntrain_accuracy_fet.append(RFC_parm.score(X_train_fet_scaled, y_train))\naccuracy_list_fet.append(metrics.accuracy_score(y_test, y_RFC)*100)\nalgorithm_fet.append('Random Forest')","8a41b319":"GBC_parma={'loss':['deviance','exponential'],'n_estimators':[10,25,50,100,150],'learning_rate':[0.1,0.25, 0.5, 0.75],\n          'criterion':['friedman_mse'], 'max_features':[None],'max_depth':[1,2,3,4,5,10],'ccp_alpha':[0,0.01,0.1]}\nGBC = GradientBoostingClassifier()\nGBC_parm=GridSearchCV(GBC, GBC_parma, cv=5)\nGBC_parm.fit(X_train_fet_scaled, y_train)\ny_GBC=GBC_parm.predict(X_test_fet_scaled)\nprint(\"The best parameters are \",GBC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(GBC_parm.score(X_train_fet_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_GBC)))\ncm = metrics.confusion_matrix(y_test, y_GBC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='GBC')\ntrain_accuracy_fet.append(GBC_parm.score(X_train_fet_scaled, y_train))\naccuracy_list_fet.append(metrics.accuracy_score(y_test, y_GBC)*100)\nalgorithm_fet.append('GBC')","832a79ce":"RC_parma={'solver':['svd','lsqr','cholesky'],'alpha':[0,0.5,0.75,1,1.5,2],'normalize':[True,False]}\nRC=RidgeClassifier()\nRC_parm=GridSearchCV(RC, RC_parma, cv=5)\nRC_parm.fit(X_train_fet_scaled, y_train)\ny_RC=RC_parm.predict(X_test_fet_scaled)\nprint(\"The best parameters are \",RC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RC_parm.score(X_train_fet_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RC)))\ncm = metrics.confusion_matrix(y_test, y_RC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Dead', 'Survived'],\n                          title='Ridge Classifier')\ntrain_accuracy_fet.append(RC_parm.score(X_train_fet_scaled, y_train))\naccuracy_list_fet.append(metrics.accuracy_score(y_test, y_RC)*100)\nalgorithm_fet.append('Ridge Classifier')","0478d0ce":"f,ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=accuracy_list,y=algorithm,palette = sns.dark_palette(\"blue\",len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithm\")\nplt.title('Algorithm Test Accuracy No Feature Selection')\nplt.show()","90eb324e":"f,ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=accuracy_list_fet,y=algorithm_fet,palette = sns.dark_palette(\"blue\",len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithm\")\nplt.title('Algorithm Test Accuracy Feature Selection')\nplt.show()","89cd945d":"#Setting test data up\nTesting_set=test_new[['Pclass', 'Fare', 'Sex_Code']]\nTesting_set_scaled=scaler.transform(Testing_set)","370176de":"#Predicting and Submission\nID = test['PassengerId']\ntest_prediction = K_NN.predict(Testing_set_scaled)\n\nSubmission = pd.DataFrame({ 'PassengerId' : ID, 'Survived': test_prediction })\nSubmission.to_csv('submission.csv', index=False)","0f3d6c63":"<a id=\"10\"><\/a>\n# Evaluation of Accuracy","b6c51d9e":"<b> Next lets look at if class and embarked had an impact of survival. First we will use a correlation matrix to observe the relationships. However, we need to transform our categorical values to numerical representations using LabelEncoder.<\/b>","f5508ee6":"<a id=\"3\"><\/a>\n# Cleaning the Data","6884dae8":"<b> Even though there are more males, it is interesting to observe that the fare was higher for females between the ages of 20 to 50. <\/b>","db2207a9":"<a id=\"6\"><\/a>\n# Feature Scaling the Data\nFirst the data needs to be preprocessed by using a scaler. We will Use Standard Scaler which will transform our data to have a mean of 0 <b>($\\mu$)<\/b> and a variance of 1 <b>($s$)<\/b>. The equation looks like this <b> $z$ = $\\frac{x-\\mu}{s}$ <\/b>.","11301ccd":"### Gradient Boosting Classifier","8207d6a5":"<a id=\"1\"><\/a>\n# Importing Libraries","42a5662f":"### K-Nearest Neighbors (KNN)","f5a1d848":"### Random Forest Classifier","fdeeffdb":"<a id=\"7\"><\/a>\n# Machine Learning Implementation (No Feature Selection)\n\nGridSearchCV provides a way of throwing a ton of parameters at the models and it will select the best parameters that provide the highest prediction accuracy.\n\nThe Algorithms used are:\n\n- Logistic Regression\n- Support Vector Machines\n- K-Nearest Neighbors\n- Random Forest\n- Gradient Boosting\n- Ridge Classifier\n\n## <u> Logistic Regression <\/u> \nA model that is used statistically for binary dependent variables based on the probability of an event occuring. This can be further extended for several variables in a classification setting for multi-class prediction.\n    \n## <u> Support Vector Machines (SVM) <\/u>\nCommonly used for classification tasks, SVM's function by a Kernel which draws points on a hyperplane and uses a set of vectors to separate data points. This separation of data points creates a decision boundary for where a new data point can be predicted for a specific class label. \n\n## <u> K-Nearest Neighbors <\/u>\nSimply, an algorithm that clusters the data and by a measure of distance to the 'k' nearest points votes for a specific class prediction.\n\n## <u> Random Forest <\/u> \nAn ensemble method that estimates several weak decision trees and combines the mean to create an uncorrelated forest at the end. The uncorrelated forest should be able to predict more accurately than an individual tree.\n\n## <u> Gradient Boosting <\/u>\nSimilar to Random Forest, Gradient Boosting builds trees one at a time then ensembles them as each one is built.\n\n## <u> Ridge Classifier <\/u>\nNormalizes data then treats problem as a multi-output regression task.","efdd6ed5":"<a id=\"5\"><\/a>\n# <b>Splitting the Training Data <\/b>\nTrain test split is a simple function in the Sklearn library that will allow us to easily split our dependent variable (Survived) from our indepdent variables as shown in the correlation matrix. The idea here is to scale the data, perform a decomposition technique (LDA) then visualize the results to see how the two classes separate. If they separate well then we will move onto a machine learning with several algorithms. If not, then we will take a step back look at using a feature selector to pull out the most relevant features.","7c7299f2":"### Support Vector Machine","fce5a3e3":"## Table of Contents\n[0.Feature Meaning](#0) <br\/>\n[1.Importing Libraries](#1) <br\/>\n[2.Importing Data](#2) <br\/>\n[3.Cleaning the Data](#3) <br\/>\n[4.Exploring Data](#4) <br\/>\n[5.Splitting the Data](#5) <br\/>\n[6.Feature Scaling (Normalization)](#6) <br\/>\n[7.Machine Learning without Feature Selection](#7) <br\/>\n[8.Feature Selection](#8) <br\/>\n[9.Machine Learning (Feature Selection)](#9) <br\/>\n[10.Evaluation of Acuracy](#10) <br\/>\n[11.Discussion](#11) <br\/>\n[12.Submission](#12) <br\/>\n","95067b21":"First, we need to select what our best value of K will be then we can implement that into our code. Cross-validation is not used here.","574277fd":"<b> Thanks for sticking through to the end please upvote and comment if this was helpful!\n    ![image.png](attachment:image.png)","75a02a9e":"<b>Upon inspection of our correlation matrix it seems that the most correlated value the negative relationship between sex and survival which as we all know from the story of the Titanic is that it were women and children who were given most priority towards rescue so this is not shocking. However, the good news is that we have plenty of features to use for machine learning. To summarize the analysis most passengers died on the titanic. Age as a factor was pretty distributed as seen in the histogram,most passengers embarked from South Hampton and most passengers were 3rd class. <\/b>","338e6b6a":"<b>Now the NaN strings will be replaced with either the mean if it is a numerical value or an empty string if it is a string value.","4dedd628":"<a id=\"8\"><\/a>\n# Feature Selection\nWe see that we obtained accuracyies around 80% using the full data set. For the fun of it we will try to use a feature selection with the top three features based off $Chi^2$ score to see if that provides better prediction accuracy.","6a55f8d5":"### For Confusion Matrices","b44cd6d7":"<b>As we can see for Sex, male=1 and female=0 and for Embarked C=1,Q=2,S=3.","0f5556e2":"## The Titatnic\n![image.png](attachment:image.png)\nThe tragic event of the Titanic occured over 100 years ago, being one of the most famous incidence in maritime history. Many did not survive while many lived to tell the tale of what occured the evening of the ships impact onto the iceberg. However, the lives and memories are not forgotten and are still important for beginners who want to learn data science. The reason for that is because the basic data that is provided are simple features to observe such as age, sex and class which can be explored than used for machine learning tasks. First we will do a basic exploratory data analysis then perform machine learning tasks by using all relevant features then using the KBest algorithm to provide the three best features and compare all models. The best predicting model will then be used on the testing data set. <b> Note this notebook was implemented without looking too much into what others have done. There are higher accuracies that predict around 99% but I wanted to try it my own way. This notebook can take some time to run due to using GridSearchCV on both Random Forest and Gradient Boosting Classifiers.<\/b>","a18095ed":"<a id=\"0\"><\/a>\n# Feature Meaning\n- survival as 0 = No and 1= Yes\n- pclass as Ticket Class as 1st, 2nd or 3rd\n- Sex as male or female\n- Age in years\n- Sibsp is # of sibling or spouses\n- parch is # of parents\/children\n- ticket is ticket #\n- fare is passenger fare\n- cabin is cabin #\n- embarked location of embarkment C = Cherbourg, Q = Queenstown, S = Southampton","169945b3":"<b> First lets look at the columns and see what our variables are.","b8db61b3":"<b> Lets start by removing irrelevant columns shown below.","0f5bca49":"### Ridge Classifier","8cf0bfd2":"<a id=\"2\"><\/a>\n# Importing Data","423f0b7c":"<a id=\"11\"><\/a>\n# Evaluation of Accuracy\n- It is interesting to observe the variance of characteristics of the passengers on board provided by the data.\n- At a basic EDA we see sex, fare and class were correlated to ones survival.\n- When using machine learning for prediction of passenger survival, comparisons should be made between taking relevant features and a feature selected algorithmic approach.\n- In this case feature selection seemed to improve the accuracies for both KNN and Gradient Boosting.\n- Further analsysis is neccessary and feature engineering may provide better results.","e0f3e1af":"<a id=\"4\"><\/a>\n# Exploratory Data Analysis","c875c865":"### Logistic Regression","24ac911f":"<b>Now we can check again if we were sucessful. <\/b>","523fb077":"The top scoring features were Fare, Sex and Class which were all the most correlated features to survival in the correlation matrix. We will re-run the models with these. Again the data will be split then re-scaled.","4e19318e":"<b> We see that in both data sets, age and embarked have some missing values ,while just in the testing set fare needs to be filled. To make life easier and to see how are columns are labeled we will generate the labels below for simple copy and paste.<\/b>","ed355c44":"<a id=\"12\"><\/a>\n# Model Implementation for Testing Data Set\nThe model that provided the highest prediction accuracy was the KNN trained on the feature selected data. Therefore, that will be used for the submission results with favored parameters.","5b741d3a":"<b>By visually exploring the data we can look at the direct relationship between some of our features and make decisions on how we may want to include or exclude them for machine learning prediction. <\/b>","2af96574":"We see that k=14 or 15 provides the lowest number of error, therefore we will select that.","f0e1b9e9":"<a id=\"9\"><\/a>\n# Machine Learning Implementation (Feature Selected)"}}