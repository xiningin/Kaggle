{"cell_type":{"7d7a9c89":"code","ddc50801":"code","44de2988":"code","6847113d":"code","c320601d":"code","27c0168a":"code","c9194aa9":"code","7c727e57":"code","25496709":"markdown","a23f8bd6":"markdown","377bfe0b":"markdown","e3cf1524":"markdown","ce880a4e":"markdown"},"source":{"7d7a9c89":"import pandas as pd\nimport numpy as np\n\nDATA_DIR = '..\/input\/tabular-playground-series-nov-2021\/'\n\ntrain = pd.read_csv(DATA_DIR + 'train.csv').set_index('id')\ny = train.pop('target').values\n\ntest = pd.read_csv(DATA_DIR + 'test.csv').set_index('id')\nsample = pd.read_csv(DATA_DIR + 'sample_submission.csv').set_index('id')\n\nno_features = test.shape[1]","ddc50801":"from sklearn.preprocessing import QuantileTransformer, StandardScaler, RobustScaler, MinMaxScaler\nfrom gc import collect\n\ndata = {}\n\n# This was no help at all\n#print('Fitting quantiles transformer: Normal')\nnorm = QuantileTransformer(output_distribution='normal', n_quantiles=1000)\nnorm.fit(pd.concat([train, test]))\ndata['norm'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\nprint('Fitting standard scaler')\nnorm = StandardScaler()\nnorm.fit(pd.concat([train, test]))\ndata['z'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\nprint('Fitting robust scaler')\nnorm = RobustScaler()\nnorm.fit(pd.concat([train, test]))\ndata['robust'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\n# This was no help at all\nprint('Fitting min-max')\nnorm = MinMaxScaler(feature_range=(-1, 1))\nnorm.fit(pd.concat([train, test]))\ndata['min-max'] = {\n    'train': norm.transform(train),\n    'test': norm.transform(test),\n}\n\ndel(train)\ncollect();","44de2988":"import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import BatchNormalization, Dense, Dropout, GaussianNoise, Input, Attention, Add, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.optimizers import Adam\n\ndef get_model(depth, noise, lr):\n    tf.random.set_seed(42)\n    np.random.seed(42)\n    \n    features = 2 ** (depth + 2)\n    \n    # Input\n    inputs = Input(no_features)\n\n    # Add noise to the continuous\n    x = GaussianNoise(noise)(inputs)\n    \n    # Build out some blocks\n    for _ in range(0, depth):\n        x = Dense(features, activation='swish')(x)\n        if features > 8:\n            features \/= 8\n    \n    out = Dense(1, activation='sigmoid')(x)\n    \n    # Build\n    model = Model(inputs=inputs, outputs=out, name='perceptomanic')\n\n    # Optimiser\n    opt = Adam(learning_rate=lr)\n\n    # Metrics\n    auc = tf.keras.metrics.AUC(name='auc')\n    \n    # Compile\n    model.compile(loss='binary_crossentropy', \n                  optimizer=opt,\n                  metrics=[auc])\n    \n    return model\n","6847113d":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport optuna\n\ndef nn(trial):\n    '''\n    '''\n    params = {\n        'noise': trial.suggest_float('noise', 0.0, 0.06),\n        'lr': trial.suggest_float('lr', 0.0001, 0.1),\n        'depth': trial.suggest_int('depth', 1, 8),\n    }\n    norm_method = trial.suggest_categorical('norm_method', ['robust'])\n    \n    kf = StratifiedKFold(10, shuffle=True, random_state=42)\n    estimates = []\n    y_hat_validation = np.zeros(len(y)) + np.NaN\n    \n    # callbacks\n    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=False, mode='min', restore_best_weights=True)\n    callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=False, mode='min')\n    \n    for train_index, test_index in kf.split(data[norm_method]['train'], y):\n        # Train model\n        model = get_model(**params)\n\n        # Split out test and train\n        X_train = data[norm_method]['train'][train_index]\n        y_train = y[train_index]\n\n        X_test = data[norm_method]['train'][test_index]\n        y_test = y[test_index]\n\n        # Fit\n        model.fit(x=X_train, \n                  y=y_train, \n                  batch_size=4096, \n                  epochs=1000, \n                  validation_data=(X_test, y_test), \n                  verbose=False, \n                  callbacks=[callback_early_stopping, callback_lr])\n\n        # Predict\n        y_hat_validation[test_index] = model.predict(X_test).squeeze()\n        \n        # Check for early stop\n        not_null = ~np.isnan(y_hat_validation)\n        score = roc_auc_score(y[not_null], y_hat_validation[not_null])\n        if score < 0.735:\n            print('Early stopping')\n            return score\n        \n        del(model)\n        \n    score = roc_auc_score(y, y_hat_validation)\n    \n    return score\n","c320601d":"study = optuna.create_study(study_name='Find me some params dude', direction='maximize')","27c0168a":"study.optimize(nn, timeout=60*60*6, gc_after_trial=True, show_progress_bar=True)\n\nprint('Number of finished trials: {}'.format(len(study.trials)))\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\n\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))","c9194aa9":"def nn_run(params):\n    '''\n    '''\n    \n    norm_method = params.pop('norm_method')\n    \n    kf = StratifiedKFold(10, shuffle=True, random_state=42)\n    estimates = []\n    y_hat_validation = np.zeros(len(y)) + np.NaN\n    \n    # callbacks\n    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=False, mode='min', restore_best_weights=True)\n    callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=False, mode='min')\n    \n    for train_index, test_index in kf.split(data[norm_method]['train'], y):\n        # Train model\n        model = get_model(**params)\n\n        # Split out test and train\n        X_train = data[norm_method]['train'][train_index]\n        y_train = y[train_index]\n\n        X_test = data[norm_method]['train'][test_index]\n        y_test = y[test_index]\n\n        # Fit\n        model.fit(x=X_train, \n                  y=y_train, \n                  batch_size=4096, \n                  epochs=1000, \n                  validation_data=(X_test, y_test), \n                  verbose=True, \n                  callbacks=[callback_early_stopping, callback_lr])\n\n        # Predict\n        y_hat_validation[test_index] = model.predict(X_test).squeeze()\n        estimates.append(model.predict(data[norm_method]['test']).squeeze())\n        \n        print('\\n')\n        \n        del(model)\n        \n    print(roc_auc_score(y, y_hat_validation))\n    \n    return estimates\n\nestimates = nn_run(study.best_trial.params)","7c727e57":"sample['target'] = np.vstack(estimates).T.mean(axis=1)\nsample.to_csv('simple_af_nn.csv')","25496709":"## Model\nThis is our basic model structure. A few additions that worked for me...\n- Adding an attention layer between the raw and noised features\n- Using add to incorporate residuals","a23f8bd6":"## Now lets go generate some normalisations for us to play with\nI have taken out a few examples here and commented more. Robust and Z score seem to provide the best results for me.\n\nNote: I am cheating here and using the test set to help scale the training set. This is one of those things that's cool in a Kaggle competition but probably not advisable in the wild.","377bfe0b":"# Keras \/ Optuna Starter\n\nThe intention of this notebooks is to provide a simple example of how a basic Keras structure can be trained with Optuna. With a bit of experimentation to the architecture it should be fairly easy to get a nice score for this point in the competition. Hopefully this provides a more accessible point for those wishing to experiment with NNs for this project.\n## Grab our data","e3cf1524":"### Now run it","ce880a4e":"## Optuna Search"}}