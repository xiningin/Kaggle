{"cell_type":{"266c8bc9":"code","081d48a1":"code","8c32f699":"code","ee45f744":"code","c6318549":"code","b42c4735":"code","4672f07c":"code","a1e079b0":"code","84695c72":"code","81b50cd3":"code","5ade5ab6":"code","8b53eb44":"code","4e368c00":"code","9e648a4b":"code","096b6784":"code","ba15cb95":"code","038c7255":"code","c690f1ff":"code","4195ce2a":"code","fce23ad1":"code","855c1530":"code","36f65694":"code","e7df2131":"code","f00e70b4":"code","00353fc5":"markdown","15b3a1df":"markdown","c6c01a53":"markdown","c463a774":"markdown","40187430":"markdown","fc081001":"markdown","8b71e3f5":"markdown","785d99d6":"markdown","8ac22524":"markdown","ab97558a":"markdown","8f27c8cf":"markdown","a768e5f0":"markdown","c8784c9a":"markdown","d06656c9":"markdown","7e16f817":"markdown","ac02a35e":"markdown","d319126b":"markdown","00a8fbab":"markdown","55b96047":"markdown","20538a78":"markdown","fa19b25f":"markdown","45ebe854":"markdown","c28f79bf":"markdown","4dd62532":"markdown","23716829":"markdown"},"source":{"266c8bc9":"# Print List of files (our train and test data) in Kaggle's file explorer\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","081d48a1":"# Importing Pandas an Numpy Libraries to use on manipulating our Data\nimport pandas as pd\nimport numpy as np\n\n# To Preproccesing our data\nfrom sklearn.preprocessing import LabelEncoder\n\n# To fill missing values\nfrom sklearn.impute import SimpleImputer\n\n# To Split our train data\nfrom sklearn.model_selection import train_test_split\n\n# To Visualize Data\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# To Train our data\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\n\n# To evaluate end result we have\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\n\n\n# We are importing our Data with Pandas Library\n# We use \"Coronary_artery.csv\" \ndf = pd.read_csv(\"\/kaggle\/input\/heartdisease2\/heart-2.csv\")\n","8c32f699":"# Prints first 5 row in Data\ndf.head()","ee45f744":"# Print number of rows in data\nprint(\"Rows:\", len(df))","c6318549":"#\u00a0Prints Summary of Numerical Data\ndf.describe()","b42c4735":"#\u00a0Prints Summary of Categorical Data\ndf.describe(include=[np.object])","4672f07c":"numerical_column = df.select_dtypes(exclude=\"object\").columns.tolist()\ncategorical_column = df.select_dtypes(include=\"object\").columns.tolist()\nprint(\"Numerical Columns:\", numerical_column)\nprint(\"****************\")\nprint(\"Categorical Columns:\", categorical_column)","a1e079b0":"df[categorical_column].describe()","84695c72":"# Get column names have less than 10 more than 2 unique values\nto_one_hot_encoding = [col for col in categorical_column if df[col].nunique() <= 10 and df[col].nunique() > 2]\n\n# Get Categorical Column names thoose are not in \"to_one_hot_encoding\"\nto_label_encoding = [col for col in categorical_column if not col in to_one_hot_encoding]\n\nprint(\"To One Hot Encoding:\", to_one_hot_encoding)\nprint(\"To Label Encoding:\", to_label_encoding)\n","81b50cd3":"df.isnull().sum()","5ade5ab6":"# We will use built in pandas function \"get_dummies()\" to simply to encode \"to_one_hot_encoding\" columns\none_hot_encoded_columns = pd.get_dummies(df[to_one_hot_encoding])\none_hot_encoded_columns","8b53eb44":"# Label Encoding\n\nlabel_encoded_columns = []\n# For loop for each columns\nfor col in to_label_encoding:\n    # We define new label encoder to each new column\n    le = LabelEncoder()\n    # Encode our data and create new Dataframe of it, \n    # notice that we gave column name in \"columns\" arguments\n    column_dataframe = pd.DataFrame(le.fit_transform(df[col]), columns=[col] )\n    # and add new DataFrame to \"label_encoded_columns\" list\n    label_encoded_columns.append(column_dataframe)\n\n# Merge all data frames\nlabel_encoded_columns = pd.concat(label_encoded_columns, axis=1)\nlabel_encoded_columns","4e368c00":"# Copy our DataFrame to X variable\nX = df.copy()\n\n# Droping Categorical Columns,\n# \"inplace\" means replace our data with new one\n# Don't forget to \"axis=1\"\nX.drop(categorical_column, axis=1, inplace=True)\n\n# Merge DataFrames\nX = pd.concat([X, one_hot_encoded_columns, label_encoded_columns], axis=1)\nprint(\"All columns:\", X.columns.tolist())\nX","9e648a4b":"# Define Y (This is the value we will predict)\ny = df[\"target\"]\n\n# Droping \"class\" from X\nX.drop([\"target\"], axis=1, inplace=True)\nX","096b6784":"# You can specify test size\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","ba15cb95":"# Define Random Forest Model\nrf = RandomForestClassifier(n_estimators=100)\n\n# We fit our model with our train data\nrf.fit(X_train, y_train)\n\n# Then predict results from X_test data\npred_rf = rf.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\nprint(\"Predicted:\", pred_rf[0:10])\nprint(\"Actual:\", y_test[0:10])","038c7255":"# Define Decision Tree Model\ndt = DecisionTreeClassifier()\n# We fit our model with our train data\ndt.fit(X_train, y_train)\n# Then predict results from X_test data\npred_dt = dt.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\nprint(\"Predicted:\", pred_dt[0:10])\nprint(\"Actual:\", y_test[0:10])","c690f1ff":"# Define Logistic Regression Model\nlog = LogisticRegression()\n# We fit our model with our train data\nlog.fit(X_train, y_train)\n# Then predict results from X_test data\npred_log = log.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\nprint(\"Predicted:\", pred_log[0:10])\nprint(\"Actual:\", y_test[0:10])","4195ce2a":"# Define Bernouilli Naive Bias Model\nbnb = BernoulliNB()\n# We fit our model with our train data\nbnb.fit(X_train, y_train)\n# Then predict results from X_test data\npred_bnb = bnb.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\nprint(\"Predicted:\", pred_bnb[0:10])\nprint(\"Actual:\", y_test[0:10])","fce23ad1":"# Define Gaussian Naive Bias Model\ngnb = GaussianNB()\n# We fit our model with our train data\ngnb.fit(X_train, y_train)\n# Then predict results from X_test data\npred_gnb = gnb.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\nprint(\"Predicted:\", pred_gnb[0:10])\nprint(\"Actual:\", y_test[0:10])","855c1530":"# Define KNN Model\nknn = KNeighborsClassifier(n_neighbors=3, metric=\"minkowski\")\n# We fit our model with our train data\nknn.fit(X_train, y_train)\n# Then predict results from X_test data\npred_knn = knn.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\nprint(\"Predicted:\", pred_knn[0:10])\nprint(\"Actual:\", y_test[0:10])","36f65694":"# Define XGBoost Model\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n# We fit our model with our train data\nxgb.fit(\n    X_train, y_train,\n    #\u00a0That means if model don't improve it self in 5 rounds, it will stop learning\n    # So you can save your time and don't overtrain your model.\n    early_stopping_rounds=5,\n    # We provide Test data's to evaluate model performance\n    eval_set=[(X_test, y_test)],\n    verbose=False\n )\n# Then predict results from X_test data\npred_xgb = xgb.predict(X_test)\n\n# See First 10 Predictions and They Actual Values\n# print(\"Predicted:\", pred_xgb[0:10])\nprint(\"Actual:\", y_test[0:10])","e7df2131":"# Confusion Matrixes\n# First parameter is actual value\n# second parameter is value that we prediceted\n\n# Random Forest \ncm_rf = confusion_matrix(y_test, pred_rf)\n# Desicion Tree\ncm_dt = confusion_matrix(y_test, pred_dt)\n# Logistic Regression\ncm_log = confusion_matrix(y_test, pred_log)\n# Bernouilli Naive Bias\ncm_bnb = confusion_matrix(y_test, pred_bnb)\n# Gaussian Naive Bias\ncm_gnb = confusion_matrix(y_test, pred_gnb)\n# KNN (K-Nearest Neighbors)\ncm_knn = confusion_matrix(y_test, pred_knn)\n# XGBoost \ncm_xgb = confusion_matrix(y_test, pred_xgb)\n\nprint(\"***********************\")\nprint(\"Confusion Matrixes\")\nprint(\"***********************\")\nprint(\"Random Forest:\\n\", cm_rf)\nprint(\"Desicion Tree:\\n\", cm_dt)\nprint(\"Logistic Regression:\\n\", cm_log)\nprint(\"Bernouilli Naive Bias:\\n\", cm_bnb)\nprint(\"Gaussian Naive Bias:\\n\", cm_gnb)\nprint(\"KNN (K-Nearest Neighbors):\\n\", cm_knn)\nprint(\"XGBoost:\\n\", cm_xgb)","f00e70b4":"# Accuracy Scores\n# First parameter is actual value\n# second parameter is value that we prediceted\n\n# Random Forest \nacc_rf = accuracy_score(y_test, pred_rf)\n# Desicion Tree\nacc_dt = accuracy_score(y_test, pred_dt)\n# Logistic Regression\nacc_log = accuracy_score(y_test, pred_log)\n# Bernouilli Naive Bias\nacc_bnb = accuracy_score(y_test, pred_bnb)\n# Gaussian Naive Bias\nacc_gnb = accuracy_score(y_test, pred_gnb)\n# KNN (K-Nearest Neighbors)\nacc_knn = accuracy_score(y_test, pred_knn)\n# XGBoost \nacc_xgb = accuracy_score(y_test, pred_xgb)\n\nprint(\"***********************\")\nprint(\"Accuracy Scores\")\nprint(\"***********************\")\nprint(\"Random Forest:\", acc_rf)\nprint(\"Desicion Tree:\", acc_dt)\nprint(\"Logistic Regression:\", acc_log)\nprint(\"Bernouilli Naive Bias:\", acc_bnb)\nprint(\"Gaussian Naive Bias:\", acc_gnb)\nprint(\"KNN (K-Nearest Neighbors):\", acc_knn)\nprint(\"XGBoost:\", acc_xgb)","00353fc5":"There is no missing column :) Now we can proceed to One Hot Encoding and Label Encoding","15b3a1df":"# Compare Models Perfomance \u26a1\ufe0f\u2b50\ufe0f\nIn this section we will compare performance of models which we train in previous section. We will use \"confusion_matrix\" and \"accuracy_score\" from sklearn library. Then we choose best model for our data.","c6c01a53":"# Logistic Regression \ud83d\udcc8\nNow we will speed things little up. [Sklearn docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n","c463a774":"# Gaussian Naive Bias\n[Sklearn docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)","40187430":"# Spliting Columns for One Hot Encoding and Label Encoding \u270d\ufe0f\nThere is 6 categorical column in dataset. We will transform columns that have less than 10 and more then 2 unique values, with One Hot Encoder (We define new column for each value).\n\nWe will transfor rest of columns with Label Encoding (We dont define new columns instead of that we give a numerical value for each unique label).\n\n* We are not using One Hot Encoding on columns have more then 10 unique value because of It will effect performance very badly. \n* We are not using One Hot Encoding on 2 unique values because it will cause \"Dummy Variable\" proglem. You can Google it for know more  \n","fc081001":"We will investigate performance of our model  later on \ud83e\uddd0","8b71e3f5":"# KNN (K-Nearest Neighbors) \ud83c\udfd8\nKNN is based on calculation of nearest elements to our data. KNN takes some arguments \"n_neighbors\" and \"metric\". \"n_neighbors\" is how many nearest neighbors will be used to predict result. \"metric\" is method to which method to used to calculate distance to neighbors. You can check [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html) for more info.","785d99d6":"# One Hot Encoding and Label Encoding \ud83d\udd25","8ac22524":"# Investigating Missing Values \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\nIn this section we will search for missing columns. If our data has missing columns it can produce error while we training our data. \n","ab97558a":"#\u00a0Desicion Tree \ud83c\udf33\nDecision Tree is our second algoritm. To learn more chekout [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html).","8f27c8cf":"# Bernouilli Naive Bias\n[Sklearn docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html)","a768e5f0":"# Performance of Models and Last Words \ud83c\udfaf\nAs you see XGBoost, Logistic Regression and Bernouilli Naive Bias: give us best results. Any way feel free to comment below. Have a nice day :)\n\n ","c8784c9a":"# Spliting Numerical and Categorical Values \u2702\ufe0f\nWe will split categorical (String) and numerical values. selec_dtypes function selecet column in our DataFrame includes or exculudes type we define. \"object\" stands for Sting values inside our DataFrame.\n\n* When we include type of \"object\" we select String Columns\n* When we exclude type of \"object\" we select Numerical Columns","d06656c9":"# It's Time to TRAIN!!! \ud83d\udcaa\nNow we will train our data with several machine learning models. We will try several models and find which is best for us. We will use **Classifiers** to predict. You should use Calssifiers for predicting nominal values (\"True or False\" or \"Category A, Category B, C ..\" like types) and you should use Reggressors for predicting numerical values (exp. Companie's Salary). In this we will use these algoritms:\n\n* Random Forest (It's my favorite)\n* Desicion Tree\n* Logistic Regression Classifier\n* Bernouilli Naive Bias\n* Gaussian Naive Bias\n* KNN (K-Nearest Neighbors)\n* XGBoost (It's new and have acurate predictions)","7e16f817":"# XGBoost \ud83d\ude80\nXGBoost is a library based on gradient boosting. It's very accurate and fast. But you need some knowledge to benefit it's briliant features. In this example I will show basic ussage og XGBoost. You should checkout [it's own documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/) or [Kaggle Tutorial](https:\/\/www.kaggle.com\/alexisbcook\/xgboost) for more information.","ac02a35e":"Now it's time to split our data to train and test","d319126b":"# Split Data to Train and Test \ud83d\udc5f\nWe successfuly complete data manipulation. There is one more step to Train our data with Machine Learning. We will split data to Train and Test groups. It's important because we use Test group to see our Machine Learning Models performance\n\nFirst we will split data to two parts X and Y. \n* X is our data without target column\n* Y is our target to predict","00a8fbab":"# Accuracy Scores \ud83e\udd47","55b96047":"# Confusion Maxtrixes \ud83d\udcd0","20538a78":"# Bring Data Together \ud83d\udcd6\nNow we will birng all modified data to one single DataFrame then we examine our data ","fa19b25f":"# Exploring Categorical Columns \ud83d\udd26\nIn this section we will look for our Categorical (String) columns. In here we will see unique row which is unique values column has. We will use that info to split our categorical columns to encoding later on.","45ebe854":"# \ud83d\udd25 Machine Learning for Beginners With 7 Diffrent Models \ud83d\udcaa\nThis notbook covers the basics of Machine Learning for beginners. We will estimate whether patients have a **Heart Attack or Not**. We will use [this dataset](https:\/\/www.kaggle.com\/mehmetzahitylmaz\/heartdisease2) to make predictions. We will proceed with these steps:\n* Import Libraries and Datasets \ud83d\udd0c\n* Exploring Dataset \ud83d\udd0d\n* Spliting Numerical and Categorical Values \u2702\ufe0f\n* Exploring Categorical Columns \ud83d\udd26\n* Spliting Columns for One Hot Encoding and Label Encoding \u270d\ufe0f\n* Investigating Missing Values \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\n* Bring Data Together \ud83d\udcd6\n* Split Data to Train and Test \ud83d\udc5f\n* Training our models \ud83d\udcaa\n* Compare Models Perfomance \u26a1\ufe0f\u2b50\ufe0f\n* Performance of Models and Last Words \ud83c\udfaf","c28f79bf":"# Import Libraries and Datasets \ud83d\udd0c\nIn this section we import libraries and import DataFrames. We use \"Coronary_artery.csv\" because it provides us good example to preproccessing data.","4dd62532":"#\u00a0Random Forest \ud83c\udf32\nRandom forest is our first algoritm to try. It can take few arguments one of them is n_estimators. You can play with n_estimators to get different predictions. But don't forget that if you increase it too much you can ended up with overtrained model which we don't realy want :) For more information you can see [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).","23716829":"# Exploring Dataset \ud83d\udd0d\n* In this section we will look at our train DataFrame. head() function print first 5 rows in our DataFrame"}}