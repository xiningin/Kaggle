{"cell_type":{"9dd24eb1":"code","beef0418":"code","5d51c117":"code","19de5b4f":"code","e4cf6eb8":"code","9a829a33":"code","b424eaf6":"code","08252d7f":"code","a72cb308":"code","ecc4164f":"code","7006af92":"code","04a5d601":"code","2e6ba282":"code","4ac3cbf4":"code","7227e089":"code","e41f4de3":"code","507239af":"code","a9a3c24e":"code","4305f939":"code","c7889fc7":"code","6f0ade60":"code","9e1ca305":"code","4c46e4ff":"code","14e5987c":"code","34554707":"code","067fb3dc":"code","3f8c2216":"code","64d98adb":"code","45403c3f":"code","2ee710f0":"markdown","ce08ff74":"markdown","45f014d6":"markdown","af9dde3b":"markdown","6d6b871c":"markdown","6fa482cd":"markdown","1f014189":"markdown","fde0645c":"markdown","279bdc18":"markdown","736a6118":"markdown","9dc3caa5":"markdown","20ab52aa":"markdown","3a93c5a3":"markdown","085a0250":"markdown","ee406036":"markdown","7ba0843c":"markdown","c43b244d":"markdown","7ceb0820":"markdown","5c533818":"markdown","72f53740":"markdown","2fbdad15":"markdown","58925860":"markdown","2b22c71e":"markdown","b845222f":"markdown","bd2a3e28":"markdown","d986eebd":"markdown","4ade5bee":"markdown","c401ead9":"markdown","f6018ddb":"markdown","d4562226":"markdown","f637d2f2":"markdown","392d17e8":"markdown","e5a7b56b":"markdown"},"source":{"9dd24eb1":"import pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nimport shap\n","beef0418":"os.listdir('..\/input\/data-science-bowl-2019')\n","5d51c117":"%%time\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count',\n             'event_code','title' ,'game_time', 'type', 'world','timestamp']\ntrain=pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv',usecols=keep_cols)\ntrain_labels=pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv',\n                         usecols=['installation_id','game_session','accuracy_group'])\ntest=pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv',usecols=keep_cols)\nsubmission=pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","19de5b4f":"train.shape,train_labels.shape","e4cf6eb8":"x=train_labels['accuracy_group'].value_counts()\nsns.barplot(x.index,x)","9a829a33":"not_req=(set(train.installation_id.unique()) - set(train_labels.installation_id.unique()))","b424eaf6":"train_new=~train['installation_id'].isin(not_req)\ntrain.where(train_new,inplace=True)\ntrain.dropna(inplace=True)\ntrain['event_code']=train.event_code.astype(int)","08252d7f":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    return df","a72cb308":"time_features=['month','hour','year','dayofweek','weekofyear']\ndef prepare_data(df):\n    df=extract_time_features(df)\n    \n    df=df.drop('timestamp',axis=1)\n    #df['timestamp']=pd.to_datetime(df['timestamp'])\n    #df['hour_of_day']=df['timestamp'].map(lambda x : int(x.hour))\n    \n\n    join_one=pd.get_dummies(df[['event_code','installation_id','game_session']],\n                            columns=['event_code']).groupby(['installation_id','game_session'],\n                                                            as_index=False,sort=False).agg(sum)\n\n    agg={'event_count':sum,'game_time':['sum','mean'],'event_id':'count'}\n\n    join_two=df.drop(time_features,axis=1).groupby(['installation_id','game_session']\n                                                   ,as_index=False,sort=False).agg(agg)\n    \n    join_two.columns= [' '.join(col).strip() for col in join_two.columns.values]\n    \n\n    join_three=df[['installation_id','game_session','type','world','title']].groupby(\n                ['installation_id','game_session'],as_index=False,sort=False).first()\n    \n    join_four=df[time_features+['installation_id','game_session']].groupby(['installation_id',\n                'game_session'],as_index=False,sort=False).agg(mode)[time_features].applymap(lambda x: x.mode[0])\n    \n    join_one=join_one.join(join_four)\n    \n    join_five=(join_one.join(join_two.drop(['installation_id','game_session'],axis=1))). \\\n                        join(join_three.drop(['installation_id','game_session'],axis=1))\n    \n    return join_five\n\n","ecc4164f":"\njoin_train=prepare_data(train)\ncols=join_train.columns.to_list()[2:-3]\njoin_train[cols]=join_train[cols].astype('int16')\n\n","7006af92":"join_test=prepare_data(test)\ncols=join_test.columns.to_list()[2:-3]\njoin_test[cols]=join_test[cols].astype('int16')","04a5d601":"cols=join_test.columns[2:-12].to_list()\ncols.append('event_id count')\ncols.append('installation_id')","2e6ba282":"df=join_test[['event_count sum','game_time mean','game_time sum',\n    'installation_id']].groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=join_test[cols].groupby('installation_id',as_index=False,\n                               sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=join_test[['title','type','world','installation_id']].groupby('installation_id',\n         as_index=False,sort=False).last().drop('installation_id',axis=1)\n\ndf_four=join_test[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])\n","4ac3cbf4":"final_train=pd.merge(train_labels,join_train,on=['installation_id','game_session'],\n                                         how='left').drop(['game_session'],axis=1)\n\n#final_test=join_test.groupby('installation_id',as_index=False,sort=False).last().drop(['game_session','installation_id'],axis=1)\nfinal_test=(df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)","7227e089":"df=final_train[['event_count sum','game_time mean','game_time sum','installation_id']]. \\\n    groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=final_train[cols].groupby('installation_id',as_index=False,\n                                 sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=final_train[['accuracy_group','title','type','world','installation_id']]. \\\n        groupby('installation_id',as_index=False,sort=False). \\\n        last().drop('installation_id',axis=1)\n\ndf_four=join_train[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])\n\n\n\nfinal_train=(df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)","e41f4de3":"final_train.shape,final_test.shape","507239af":"len(set(final_train.columns) & set(final_test.columns))","a9a3c24e":"final=pd.concat([final_train,final_test])\nencoding=['type','world','title']\nfor col in encoding:\n    lb=LabelEncoder()\n    lb.fit(final[col])\n    final[col]=lb.transform(final[col])\n    \nfinal_train=final[:len(final_train)]\nfinal_test=final[len(final_train):]\n\n\n    \n","4305f939":"X_train=final_train.drop('accuracy_group',axis=1)\ny_train=final_train['accuracy_group']","c7889fc7":"def model(X_train,y_train,final_test,n_splits=3):\n    scores=[]\n    pars = {\n        'colsample_bytree': 0.8,                 \n        'learning_rate': 0.08,\n        'max_depth': 10,\n        'subsample': 1,\n        'objective':'multi:softprob',\n        'num_class':4,\n        'eval_metric':'mlogloss',\n        'min_child_weight':3,\n        'gamma':0.25,\n        'n_estimators':500\n    }\n\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    y_pre=np.zeros((len(final_test),4),dtype=float)\n    final_test=xgb.DMatrix(final_test.drop('accuracy_group',axis=1))\n\n\n    for train_index, val_index in kf.split(X_train, y_train):\n        train_X = X_train.iloc[train_index]\n        val_X = X_train.iloc[val_index]\n        train_y = y_train[train_index]\n        val_y = y_train[val_index]\n        xgb_train = xgb.DMatrix(train_X, train_y)\n        xgb_eval = xgb.DMatrix(val_X, val_y)\n\n        xgb_model = xgb.train(pars,\n                      xgb_train,\n                      num_boost_round=1000,\n                      evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n                      verbose_eval=False,\n                      early_stopping_rounds=20\n                     )\n\n        val_X=xgb.DMatrix(val_X)\n        pred_val=[np.argmax(x) for x in xgb_model.predict(val_X)]\n        score=cohen_kappa_score(pred_val,val_y,weights='quadratic')\n        scores.append(score)\n        print('choen_kappa_score :',score)\n\n        pred=xgb_model.predict(final_test)\n        y_pre+=pred\n\n    pred = np.asarray([np.argmax(line) for line in y_pre])\n    print('Mean score:',np.mean(scores))\n    \n    return xgb_model,pred","6f0ade60":"xgb_model,pred=model(X_train,y_train,final_test,5)","9e1ca305":"sub=pd.DataFrame({'installation_id':submission.installation_id,'accuracy_group':pred})\nsub.to_csv('submission.csv',index=False)\n","4c46e4ff":"fig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgb_model, max_num_features=50, height=0.5, ax=ax,importance_type='gain')\nplt.show()","14e5987c":"fig, ax = plt.subplots(figsize=(10,10))\nxgb.plot_importance(xgb_model, max_num_features=50, height=0.5, ax=ax,importance_type='weight')\nplt.show()","34554707":"shap_values = shap.TreeExplainer(xgb_model).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\")","067fb3dc":"shap.summary_plot(shap_values[3], X_train)","3f8c2216":"shap.summary_plot(shap_values[0], X_train)","64d98adb":"X_train,X_test,y_train,y_test=train_test_split(X_train,y_train,test_size=.1)\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","45403c3f":"threshold = np.sort(model.feature_importances_)[40:]\nfor thresh in threshold:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = cohen_kappa_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, cohen kappa score: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n    \n    \n    ","2ee710f0":"There are three methods to measure feature_importances in xgboost.They are :\n- `weight` : The total number of times this feature was used to split the data across all trees.\n- `Cover` :The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits.\n- `Gain` : The average loss reduction gained when using this feature for splitting in trees.\n\nWe used `Gain` in the above example and the model says when it used `event_code_2030` the loss on average was reduced by 8%.","ce08ff74":"Here we will use `StratifiedKFold` and `xgboost` model to train and make prediction.\n","45f014d6":"### <font size=4 color='violet'> Reading and understanding our data<\/font>","af9dde3b":"In this step,we will \n-  prepare train by merging  our train to train_labels.This will be our `final_train`.\n-  prepare the test by selecting last row of each installation_id ,game_session as we have only 1000 rows in `sample_submission`.The last accuracy group for each installation id is taken as the accuracy group of the child.\n","6d6b871c":"For example,this figure expains the feature importance and it's influence on different class.\n- for feature `event_code_3020` its SHAP value is high for `class 3` means it influences predicting class 3 is more than any other class.","6fa482cd":"<font size=6 color='violet'>Introduction<\/font>\n\n![](http:\/\/www.gpb.org\/sites\/www.gpb.org\/files\/styles\/hero_image\/public\/blogs\/images\/2018\/08\/07\/maxresdefault.jpg?itok=gN6ErLyU)","1f014189":"Here we can see that the variables are ranked in the descending order.\n- The most important variable `event_code_3020`.\n- Lower value of `event_code_3020` has a high and positive impact on the model predicting `class 3` .\n- Lower value of `event_code_3020` the model tends to classify it to `class 3`","fde0645c":"We can see that this data contains full history of the installation,ie each time a child has played the game a unique game_session identifier is generated and the attributes related to the game is stored.The atttributes are:\n\nThe data provided in these files are as follows:\n- `event_id` - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.\n- `game_session` - Randomly generated unique identifier grouping events within a single game or video play session.\n- `timestamp` - Client-generated datetime\n- `event_data` - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise - fields are determined by the event type.\n- `installation_id` - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- `event_count` - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n- `event_code` - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n- `game_time` - Time in milliseconds since the start of the game session. Extracted from event_data.\n- `title` - Title of the game or video.\n- `type` - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- `world` - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media.\n\n We will not consider `specs.csv`,it contains description of events in natural language.","279bdc18":"Just making sure that all the columns in our `final_train` and `final_test` is the same,except accuracy_group.The instersection should return `54`.","736a6118":"Summary plot for `class 3`","9dc3caa5":"## <font size=5 color='violet'>Feature Selection<\/font>\n\nWe will use    module of xgboost to plot the feature importances and see what features  our model think are important for making prediction.\n","20ab52aa":"### <font size=4 color='violet'>Interpreting our model with Confidence<\/font>\n\n\n\n`SHAP` is a powerful tool for interpreting our model with more confidence,It makes the process simple and understandable.We will try SHAP in this section for interpret our model.","3a93c5a3":"After making our prediction we will make our submission to `submission.csv`.","085a0250":"<font size=3 color='violet'>Extracting time features<\/font>\n","ee406036":"Similarly for `class0`","7ba0843c":"<font color='red' size=4>If you think this kernel was helpful,please don't forget to click on the upvote button,that helps a lot.<\/font>","c43b244d":"## <font size=5 color='violet'> Data Preparation<\/font>","7ceb0820":"### Under Construction!!!","5c533818":"- It seems that  we have to group dafaframe by `installation_id` to form a proper trainable dataframe.\n- We will apply the same to form out test set.","72f53740":"YES ! It's done..","2fbdad15":"## <font size=5 color='violet'> Making our submission<\/font>","58925860":"Next,we will define a `prepare_data` funtion to prepare our train and test data.For the we will do the following steps:\n-   extract `hour_of_day` from timestamp and drop timestamp column,this indicated the hour of day in which is child playes the game.\n-   We will do an on_hot encoding on `event_code` and group the dataframe by installation_id and game_session.\n-   We will define an `agg` dictionary to define the the aggregate functions to be performed after grouping the dataframe\n-   For variables 'type','world' and 'title' we will the the first value,as it is unique for every installation_id,game_session pair.\n-   Atlast, we will join all these togethor and return the dataframe.\n","2b22c71e":"##  <font size=5 color='violet'> Importing required libraries<\/font>","b845222f":"<font size=5 color='violet'>Evaluation<\/font>\n\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\n$$w_{i,j} = \\frac{\\left(i-j\\right)^2}{\\left(N-1\\right)^2}$$\n\nWe will use `cohen_kappa_score` which is available in `sklearn.metrics` to calculate the score.","bd2a3e28":"### <font size=4 color='violet'>Select from Features<\/font>\n\nWe will use sklearn `SelectFromFeatures` to select relevent features.","d986eebd":"## <font size=4 color='violet'> Label Encoding<\/font>\n- We will concat out final_train and final_test to form `final`.\n- We will label encode the categorical variables.\n- We will split them back to final_train and final_test.","4ade5bee":"\n\n\nIn this dataset, we are provided with game analytics for the PBS KIDS Measure Up! app. In this app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments. Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter.\n\nThe intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment. Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices. In the training set, you are provided the full history of gameplay data. In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n    3: the assessment was solved on the first attempt\n    2: the assessment was solved on the second attempt\n    1: the assessment was solved after 3 or more attempts\n    0: the assessment was never solved\n\n","c401ead9":"<font color='blue' size=4>If you think this kernel was helpful,please don't forget to click on the upvote button,that helps a lot.<\/font>","f6018ddb":"Hope you can see the differnce between them.","d4562226":"When we considered weight,the model says that is used `game_time mean` 1035 times to split the data across the trees.\n\nhmmm...so how what can we conclude from above figures?\nWe will find out....","f637d2f2":"In this we will prepare the data and make it in a trainable form.For that we will do the following steps :\n- first,we will find the installation ids which are in `train.csv` and which are not in `train_labels.csv`.These installations won't  be of much use to us because `train_labels.csv` contains the the target label,ie `accuracy group`.We will first identify them and remove those rows.","392d17e8":"## <font size=5 color='violet'> XGBoost with StratifiedKFold<\/font>","e5a7b56b":"We need to look into it further and evaluate..."}}