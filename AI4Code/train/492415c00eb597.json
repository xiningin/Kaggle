{"cell_type":{"046c2cf1":"code","4bf72846":"code","7d9bed5e":"code","0b4158d6":"code","092e0619":"code","02ec5414":"code","463fb11c":"code","ba3cab7a":"code","ee197cd1":"code","21fc8ec1":"code","4fd36049":"code","807e8e62":"code","c217d523":"code","8799a36c":"code","7cfcc349":"code","0790392a":"code","c188d9c1":"code","60b24b52":"code","c1d1b2a2":"code","7f6ed1d2":"code","6f158674":"code","bd875d62":"code","e54a69b1":"code","26311a5c":"code","a794b2e9":"code","00af39ad":"code","470f71e1":"code","7da37b85":"code","5efc6107":"code","f7430d55":"code","523cc7de":"code","392b3658":"code","552ce618":"code","29e2d972":"code","00e985bd":"code","66a8dd9f":"code","35f9f946":"code","5e64d3f6":"code","f26a4306":"code","3710cd4a":"code","42c00dfd":"code","f608dda8":"code","595b3865":"code","c8a7f791":"code","d3271ce8":"code","13cd9409":"code","7f8146b8":"code","42021ea7":"code","23f1b1ba":"code","4214b538":"code","a3a5ee53":"code","29e794e4":"code","48a7c115":"code","e766787f":"code","a86e5a26":"code","9101c9b5":"code","3246591d":"code","f2fb33d6":"code","7df7e4e2":"code","9e472238":"code","3b88b603":"code","c32102a1":"code","4fa32d8b":"code","25f39bb4":"code","7be6c445":"code","b9ae504c":"code","2639ac1a":"code","295ad716":"code","b0e20498":"code","7bd85bf7":"code","1ff636b0":"code","544db929":"code","2ccd9c79":"code","8fab58c6":"code","67c92203":"code","23cf8df4":"code","5d55dbd5":"code","e3c66cbc":"code","65ab3180":"code","3c174719":"code","190eeadf":"code","93f83a24":"code","e30b1b7b":"code","50418ad0":"code","6bcd26aa":"code","3c598260":"code","8e10cd86":"code","df0248db":"code","49e0b64c":"markdown","894ea0df":"markdown","bc5a3bce":"markdown","8016ae9e":"markdown","1402e3ee":"markdown","26ea6a8f":"markdown","18f87e92":"markdown","900140fa":"markdown","cc1598d1":"markdown","8fac38f5":"markdown","803fff81":"markdown","589eb7f0":"markdown","5c215ec1":"markdown","4d1693d0":"markdown","6d5c11a5":"markdown","3c3f0b3b":"markdown","cdb4addc":"markdown","fd1243f8":"markdown","e679a40f":"markdown","f0455e6c":"markdown","3b615de0":"markdown","8f78f638":"markdown","48ce1d52":"markdown","379b072b":"markdown","13f80758":"markdown","99f596a4":"markdown","54071621":"markdown","2d00c75d":"markdown","74aca718":"markdown","4ad69b3b":"markdown","24713753":"markdown","8fc3f84f":"markdown","e4635849":"markdown","6f999b9a":"markdown","47907a0a":"markdown","9eb83ccd":"markdown","766edc1e":"markdown","18067300":"markdown","1b108439":"markdown","c4530744":"markdown","1a269ecb":"markdown","024ee900":"markdown","4fb7a7d5":"markdown","bc119e9c":"markdown","1b811bed":"markdown","b0d7d24e":"markdown","304f995c":"markdown","1e642cab":"markdown","ece26607":"markdown"},"source":{"046c2cf1":"import os\nimport math\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport json\n\nfrom scipy import stats\nfrom scipy import linalg\n\nimport statsmodels.api as sm\nimport statsmodels.stats.stattools as sms\nfrom statsmodels.formula.api import ols\n\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn import neighbors\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nimport missingno as msno\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n# set style\nsns.set_style('whitegrid')\n# overriding font size and line width\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n# map visualization\nimport folium\nfrom folium.plugins import HeatMap\n\n# don't print matching warnings\nimport warnings\nwarnings.filterwarnings('ignore') \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nprint(os.listdir(\"..\/input\"))","4bf72846":"# local functions which are in a seperate python file\n#\ndef stepwise_selection(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05, verbose=True):\n    \"\"\"\n    Perform a forward-backward feature selection based on p-value from statsmodels.api.OLS\n\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features\n\n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression for the details\n    \"\"\"\n\n    included = list(initial_list)\n    while True:\n        changed = False\n        # forward step\n        excluded = list(set(X.columns) - set(included))\n        new_pval = pd.Series(index=excluded)\n\n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n\n        best_pval = new_pval.min()\n\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed = True\n\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        # null if pvalues is empty\n        worst_pval = pvalues.max()\n\n        if worst_pval > threshold_out:\n            changed = True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n\n    return included\n\n\ndef display_heatmap(data):\n    \"\"\"\n    Display a heatmap from a given dataset\n\n    :param data: dataset\n    :return: g (graph to display)\n    \"\"\"\n\n    # Set the style of the visualization\n    # sns.set(style = \"white\")\n    sns.set_style(\"white\")\n\n    # Create a covariance matrix\n    corr = data.corr()\n\n    # Generate a mask the size of our covariance matrix\n    mask = np.zeros_like(corr)\n    mask[np.triu_indices_from(mask)] = None\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(15, 12))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(240, 10, sep=20, n=9, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    g = sns.heatmap(corr, cmap=cmap, mask=mask, square=True)\n\n    return g\n\n\ndef display_jointplot(data, columns):\n    \"\"\"\n    Display seaborn jointplot on given dataset and feature list\n\n    :param data: dataset\n    :param columns: feature list\n    :return: g\n    \"\"\"\n\n    sns.set_style('whitegrid')\n\n    for column in columns:\n        g = sns.jointplot(x=column, y=\"price\", data=data, dropna=True,\n                          kind='reg', joint_kws={'line_kws': {'color': 'red'}})\n\n    return g\n\n\ndef display_plot(data, vars, target, plot_type='box'):\n    \"\"\"\n    Generates a seaborn boxplot (default) or scatterplot\n\n    :param data: dataset\n    :param vars: feature list\n    :param target: feature name\n    :param plot_type: box (default), scatter, rel\n    :return: g\n    \"\"\"\n\n    # pick one dimension\n    ncol = 3\n    # make sure enough subplots\n    nrow = math.floor((len(vars) + ncol - 1) \/ ncol)\n    # create the axes\n    fig, axarr = plt.subplots(nrows=nrow, ncols=ncol, figsize=(20, 20))\n\n    # go over a linear list of data\n    for i in range(len(vars)):\n        # compute an appropriate index (1d or 2d)\n        ix = np.unravel_index(i, axarr.shape)\n\n        feature_name = vars[i]\n\n        if plot_type == 'box':\n            g = sns.boxplot(y=feature_name, x=target, data=data, width=0.8,\n                            orient='h', showmeans=True, fliersize=3, ax=axarr[ix])\n\n        # elif plot_type == 'scatter':\n        else:\n            g = sns.scatterplot(x=feature_name, y=target, data=data, ax=axarr[ix])\n\n        # else:\n        #     col_name = vars[i]\n        #     g = sns.relplot(x=feature_name, y=target, hue=target, col=col_name,\n        #                     size=target, sizes=(5, 500), col_wrap=3, data=data)\n\n    return g\n\n\ndef map_feature_by_zipcode(zipcode_data, col):\n    \"\"\"\n    Generates a folium map of Seattle\n    :param zipcode_data: zipcode dataset\n    :param col: feature to display\n    :return: m\n    \"\"\"\n\n    # read updated geo data\n    king_geo = \"cleaned_geodata.json\"\n\n    # Initialize Folium Map with Seattle latitude and longitude\n    m = folium.Map(location=[47.35, -121.9], zoom_start=9,\n                   detect_retina=True, control_scale=False)\n    # tiles='stamentoner')\n\n    # Create choropleth map\n    m.choropleth(\n        geo_data=king_geo,\n        name='choropleth',\n        data=zipcode_data,\n        # col: feature of interest\n        columns=['zipcode', col],\n        key_on='feature.properties.ZIPCODE',\n        fill_color='OrRd',\n        fill_opacity=0.9,\n        line_opacity=0.2,\n        legend_name='house ' + col\n    )\n\n    folium.LayerControl().add_to(m)\n\n    # Save map based on feature of interest\n    m.save(col + '.html')\n\n    return m\n\n\ndef measure_strength(data, feature_list, target):\n    \"\"\"\n    Calculate a Pearson correlation coefficient and the p-value to test for non-correlation.\n\n    :param data: dataset\n    :param feature_list: feature list\n    :param target: feature name\n    :return:\n    \"\"\"\n\n    print(\"Pearson correlation coefficient R and p-value \\n\\n\")\n\n    for k, v in enumerate(feature_list):\n        r, p = stats.pearsonr(data[v], data[target])\n        print(\"{0} <=> {1}\\t\\tR = {2} \\t\\t p = {3}\".format(target, v, r, p))\n\n\ndef heatmap_features_by_loc(data, feature):\n    \"\"\"\n    Generates a heatmap based on lat, long and a feature\n\n    :param data: dataset\n    :param feature: feature name\n    :return:\n    \"\"\"\n    max_value = data[feature].max()\n\n    lat = np.array(data.lat, dtype=pd.Series)\n    lon = np.array(data.long, dtype=pd.Series)\n    mag = np.array(data[feature], dtype=pd.Series) \/ max_value\n\n    d = np.dstack((lat, lon, mag))[0]\n    heatmap_data = [i for i in d.tolist()]\n\n    hmap = folium.Map(location=[47.55, -122.0], zoom_start=10, tiles='stamentoner')\n\n    hm_wide = HeatMap(heatmap_data,\n                      min_opacity=0.7,\n                      max_val=max_value,\n                      radius=1, blur=1,\n                      max_zoom=1,\n                      )\n\n    hmap.add_child(hm_wide)\n\n    return hmap\n","7d9bed5e":"# We can import the above function from a seperate python file:\n#\n# import function_filename as f\n#\n# you can check out the the documentation for the rest of the autoreaload modes\n# by apending a question mark to %autoreload, like this:\n# %autoreload?\n#\n# %load_ext autoreload\n# %autoreload 2","0b4158d6":"# read data and read date correctly\n#\ndataset = pd.read_csv(\"..\/input\/kc_house_data.csv\", parse_dates = ['date'])","092e0619":"dataset.shape","02ec5414":"dataset.dtypes","463fb11c":"# Display all missing data\n#\nmsno.matrix(dataset);","ba3cab7a":"# Handling Null values for view\n#\ndataset.view.fillna(0, inplace=True)","ee197cd1":"# Handling yr_renovated\n# - create new column 'is_renovated' and 'yr_since_renovation'\n# - if sqft_living15 > sqft_living set renovated\n# - drop yr_renovated\n#\nimport datetime\ncur_year = datetime.datetime.now().year\n\ndef calc_years(row):\n    return cur_year - row['yr_renovated'] if row['yr_renovated'] > 0 else 0\n\ndef set_renovated(row):\n    return 1 if row['yr_since_renovation'] > 0 or row['sqft_living'] != row['sqft_living15'] else 0\n\n# Set yr_renovated to int\ndataset.yr_renovated.fillna(0, inplace = True)\n# now I can convert yr_renovated to int\ndataset.yr_renovated = dataset.yr_renovated.astype('int64')\n\ndataset['yr_since_renovation'] = dataset.apply(calc_years, axis = 1)\n\n# Create category 'is_renovated'\ndataset['is_renovated'] = dataset.apply(set_renovated, axis=1)\n# Binning\nbins = [0., 1950., 1980., 1990., 2000., 2015.]\nnames = ['never', 'before 1980', '1980-1989', '1990-1999', '2000-2015']\ndataset['yr_renov_bins'] = pd.cut(dataset['yr_renovated'], bins, labels=names, right=False)\ndataset.yr_renov_bins.fillna('never', inplace=True)\n\ndataset.drop(columns=['yr_renovated'], inplace=True)","21fc8ec1":"print(cur_year)","4fd36049":"dataset.yr_built.shape","807e8e62":"dataset.yr_built.value_counts()","c217d523":"# While are at it, lets convert yr_built to house_age and drop yr_built\n#\ndataset['house_age'] = cur_year - dataset.yr_built\n# dataset.drop(columns=['yr_built'], inplace=True)","8799a36c":"dataset.house_age.value_counts()","7cfcc349":"# To answer this question, it's best to build a new variable (feature engineering) ...\ndataset['yr_built_cat'] = dataset['house_age'].apply(lambda x: ('old' if x >= 50 else 'middle-aged') if x >= 15 else 'modern')\n\n# ... and turn it into a category\ndataset['yr_built_cat'] = pd.Categorical(dataset['yr_built_cat'], categories = ['old', 'middle-aged', 'modern'])\ndataset.head(2)","0790392a":"dataset.yr_built_cat.value_counts()","c188d9c1":"# What is the percential of NaN in waterfront?\n#\nprint(dataset.waterfront.isnull().sum() \/ dataset.shape[0])","60b24b52":"# Because the percential is about 10% we set the NaN values to zero\n#\ndataset.waterfront.fillna(0, inplace=True)","c1d1b2a2":"msno.matrix(dataset);","7f6ed1d2":"dataset.shape","6f158674":"# Handling sqft_basement\n#\ndef calc_basement(row):\n    \"\"\"\n    Calculate basement sqft based on difference sqft_living and sqft_above\n    Deals at the same time with the '?' string\n    \n    :param row: feature (column)\n    :return: value (sqft)\n    \"\"\"\n    return row['sqft_living'] - row['sqft_above'] if row['sqft_above'] < row['sqft_living']  else 0\n\ndataset.sqft_basement = dataset.apply(calc_basement, axis = 1)","bd875d62":"# sort dataset by date and reset index (Do I have a good reason for it? No.)\n#\ndataset = dataset.sort_values(by = ['date'])\ndataset = dataset.reset_index(drop=True)","e54a69b1":"display_heatmap(dataset);","26311a5c":"dataset.head()","a794b2e9":"dataset.columns","00af39ad":"dataset['zipcode'] = dataset['zipcode'].astype(int)","470f71e1":"cols = ['bedrooms', 'bathrooms', 'sqft_above', 'sqft_basement', 'sqft_living15', \n        'sqft_lot15', 'yr_since_renovation', 'house_age', 'zipcode']","7da37b85":"ncol = 3 # pick one dimension\nnrow = math.floor((len(cols)+ ncol-1) \/ ncol) # make sure enough subplots\nfig, axarr = plt.subplots(nrows=nrow, ncols=ncol, figsize=(20, 20)) # create the axes\n\nfor i in range(len(cols)): # go over a linear list of data\n    ix = np.unravel_index(i, axarr.shape) # compute an appropriate index (1d or 2d)\n\n    name = cols[i]\n    dataset.plot(kind='scatter', x=name, y='price', ax=axarr[ix], label=name) \n\nplt.tight_layout()\nplt.show();\n# plt.savefig('pics\/scatter_plot_1.png', dpi = 320)","5efc6107":"dataset.sqft_lot15.value_counts(bins=10, sort=False)","f7430d55":"dataset.sqft_living15.value_counts(bins=10, sort=False)","523cc7de":"dataset.bedrooms.value_counts(bins=10, sort=False)","392b3658":"dataset.price.value_counts(bins=10, sort=False)","552ce618":"# 'house_age', 'sqft_basement', 'sqft_above', 'sqft_living15',  'sqft_lot15', 'yr_since_renovation'\n#\ncontinous_vars = ['sqft_living15',  'sqft_lot15', 'house_age', 'yr_since_renovation']","29e2d972":"display_jointplot(dataset, continous_vars)","00e985bd":"measure_strength(dataset, continous_vars, 'price')","66a8dd9f":"discrete_vars = ['grade', 'condition', 'view', 'floors', 'bedrooms', 'bathrooms']","35f9f946":"# Display box-and-whisker plot\n#\ndisplay_plot(dataset, discrete_vars, 'price')","5e64d3f6":"measure_strength(dataset, discrete_vars, 'price')","f26a4306":"# # Set zipcode type to string (folium)\n# dataset['zipcode'] = dataset['zipcode'].astype('str')\n\n# # get the mean value across all data points\n# zipcode_data = dataset.groupby('zipcode').aggregate(np.mean)\n# zipcode_data.reset_index(inplace = True)","3710cd4a":"# # count number of houses grouped by zipcode\n# #\n# dataset['count'] = 1\n# t = dataset.groupby('zipcode').sum()\n# t.reset_index(inplace = True)\n# t = t[['zipcode', 'count']]\n# zipcode_data = pd.merge(zipcode_data, t, on='zipcode')\n\n# # drop count from org dataset\n# dataset.drop(['count'], axis = 1, inplace = True)","42c00dfd":"# # Get geo data file path\n# geo_data_file = os.path.join('data', '..\/input\/king_county_wa_zipcode_area.geojson')\n\n# # load GeoJSON\n# with open(geo_data_file, 'r') as jsonFile:\n#     geo_data = json.load(jsonFile)\n    \n# tmp = geo_data\n\n# # remove ZIP codes not in geo data\n# geozips = []\n# for i in range(len(tmp['features'])):\n#     if tmp['features'][i]['properties']['ZIPCODE'] in list(zipcode_data['zipcode'].unique()):\n#         geozips.append(tmp['features'][i])\n\n# # creating new JSON object\n# new_json = dict.fromkeys(['type','features'])\n# new_json['type'] = 'FeatureCollection'\n# new_json['features'] = geozips\n\n# # save uodated JSON object\n# open(\"..\/input\/cleaned_geodata.json\", \"w\").write(json.dumps(new_json, sort_keys=True, indent=4, separators=(',', ': ')))","f608dda8":"# map_feature_by_zipcode(zipcode_data, 'count')","595b3865":"# map_feature_by_zipcode(zipcode_data, 'price')","c8a7f791":"# Get the top 5 zipcode by price\n#\n# zipcode_data.nlargest(5, 'price')['zipcode']","d3271ce8":"# Initialize Folium Map with Seattle latitude and longitude\n\n# from folium.plugins import HeatMap\n\n# max_val = dataset.price.max()\n\n# lat = np.array(dataset.lat, dtype=pd.Series)\n# lon = np.array(dataset.long, dtype=pd.Series)\n# mag = np.array(dataset.price, dtype=pd.Series)\n\n# d = np.dstack((lat, lon, mag))[0]\n# heatmap_data = [i for i in d.tolist()]\n\n# m = folium.Map(location=[47.35, -121.9], zoom_start=9, detect_retina=True, control_scale=False)\n# HeatMap(heatmap_data, radius=1, blur=1).add_to(m)\n# m","13cd9409":"dataset.plot(kind=\"scatter\", x=\"long\", y=\"lat\", figsize=(16, 8), c=\"price\", \n             cmap=\"gist_heat_r\", colorbar=True, sharex=False);\nplt.show();","7f8146b8":"sns.relplot(x=\"sqft_living15\", y=\"price\", hue=\"price\", col=\"is_renovated\", \n            size=\"price\", sizes=(5, 500), col_wrap=3, data=dataset);","42021ea7":"dataset.is_renovated.value_counts()","23f1b1ba":"# get statistics for houses which are renovated\ndf_is_renovated = dataset[dataset['is_renovated'] == 1.0]\n\nsubset = ['price', 'bedrooms', 'floors', 'sqft_living15', 'sqft_lot15']\nis_renovated_descriptives = round(df_is_renovated[subset].describe(), 2)\nis_renovated_descriptives","4214b538":"df_not_renovated = dataset[dataset['is_renovated'] == 0.0]\n\nsubset = ['price', 'bedrooms', 'floors', 'sqft_living15', 'sqft_lot15']\nnot_renovated_descriptives = round(df_not_renovated[subset].describe(), 2)\nnot_renovated_descriptives","a3a5ee53":"is_renovated_descriptives.price.median()","29e794e4":"not_renovated_descriptives.price.median()","48a7c115":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x='is_renovated', y='price', hue='is_renovated', data=dataset, palette=\"PuBu_r\")\n\n# add title, legend and informative axis labels\nax.set_title('\\nMedian Prices depending on if House is renovated\\n', fontsize=14, fontweight='bold')\nax.set(ylabel='Price', xlabel='Is Renovated')\nax.legend(loc=2);","e766787f":"dataset['price'][dataset.is_renovated.max()] - dataset['price'][dataset.is_renovated.min()]","a86e5a26":"sns.relplot(x=\"sqft_living15\", y=\"price\", hue=\"price\", col=\"condition\",\n            size=\"price\", sizes=(5, 500), col_wrap=3, data=dataset);","9101c9b5":"# plot this dataframe with seaborn\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x='condition', y='price', hue='yr_built_cat', data=dataset, palette=\"PuBu_r\")\n\n# add title, legend and informative axis labels\nax.set_title('\\nMedian Prices depending on Condition and Age of Houses\\n', fontsize=14, fontweight='bold')\nax.set(ylabel='Price', xlabel='Condition')\nax.legend(loc=2);","3246591d":"sns.relplot(x=\"sqft_living15\", y=\"price\", hue=\"price\", col=\"grade\", \n            size=\"price\", sizes=(5, 500), col_wrap=3, data=dataset);","f2fb33d6":"# plot this dataframe with seaborn\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x='grade', y='price', hue='yr_built_cat', data=dataset, palette=\"PuBu_r\")\n\n# add title, legend and informative axis labels\nax.set_title('\\nMedian Prices depending on Condition and Age of Houses\\n', fontsize=14, fontweight='bold')\nax.set(ylabel='Price', xlabel='Grade')\nax.legend(loc=2);","7df7e4e2":"dataset['condition'] = dataset['condition'].astype('category', ordered = True)\ndataset['waterfront'] = dataset['waterfront'].astype('category', ordered = True)\ndataset['is_renovated'] = dataset['is_renovated'].astype('category', ordered = False)\ndataset['view'] = dataset['view'].astype('category', ordered = False)\n\n# Create category 'has_basement'\ndataset['has_basement'] = dataset.sqft_basement.apply(lambda x: 1 if x > 0 else 0)\ndataset['has_basement'] = dataset.has_basement.astype('category', ordered = False)","9e472238":"# Set dummies (we may want to add zipcode as well)\ncat_columns = ['floors', 'view', 'condition', 'waterfront', 'is_renovated', 'has_basement']\n\nfor col in cat_columns:\n    dummies = pd.get_dummies(dataset[col])\n    dummies = dummies.add_prefix(\"{}_\".format(col))\n    \n    dataset.drop(col, axis=1, inplace=True)\n    dataset = dataset.join(dummies)","3b88b603":"# replace the '.' in the column name\nfor col in dataset.columns:\n    if col.find('.') != -1: \n        dataset.rename(columns={col: col.replace('.', '_')}, inplace=True)","c32102a1":"# dropping id and date\ndataset.drop(['id', 'date', 'lat', 'long'], axis = 1, inplace = True)","4fa32d8b":"dataset.head()","25f39bb4":"dataset.describe()","7be6c445":"# Using MinMax\n#\nminmax_df = dataset[['house_age', 'yr_since_renovation', 'zipcode']]\n\nscaler = preprocessing.MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(minmax_df)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['house_age', 'yr_since_renovation', 'zipcode'])","b9ae504c":"# Using Robust for price and sqft\n#\nrobust_df = dataset[['price', 'sqft_above', 'sqft_living15', 'sqft_lot15']]\n\nscaler = preprocessing.RobustScaler()\nrobust_scaled_df = scaler.fit_transform(robust_df)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['price', 'sqft_above', 'sqft_living15', 'sqft_lot15'])","2639ac1a":"dataset_ols = pd.concat([dataset[['grade', 'bedrooms', 'bathrooms', 'condition_3', 'condition_4', \n                                  'condition_5']], minmax_scaled_df, robust_scaled_df], axis=1)","295ad716":"dataset_ols.head()","b0e20498":"ols_results = []\nif len(ols_results) != 1:\n    ols_results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value', 'normality (JB)']]","7bd85bf7":"features = ['grade', 'bedrooms', 'bathrooms', 'house_age', 'yr_since_renovation', 'sqft_above',\n            'sqft_living15', 'sqft_lot15', 'zipcode', 'condition_3', 'condition_4', 'condition_5']","1ff636b0":"def run_ols_regression(store_results, data, target, feature, show_plots=False):\n    \"\"\"\n    Run ols model, prints model summary, displays plot_regress_exog and qqplot\n    \n    :param data: dataset\n    :param target: target feature name\n    :param feature: feature name\n    :return:\n    \"\"\"\n    \n    formula = target + '~' + feature\n    model = ols(formula=formula, data=data).fit()\n\n    df = pd.DataFrame({feature: [data[feature].min(), data[feature].max()]})\n    pred = model.predict(df)\n\n    if show_plots:\n        print('Regression Analysis and Diagnostics for formula: ', formula)\n        print('\\n')\n\n        fig = plt.figure(figsize=(16, 8))\n        fig = sm.graphics.plot_regress_exog(model, feature, fig=fig)\n        plt.show();\n\n        residuals = model.resid\n        fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n        fig.show();\n    \n    # append all information to results\n    store_results.append([feature, model.rsquared, model.params[0], model.params[0],\n                        model.pvalues[1], sms.jarque_bera(model.resid)[0]])\n","544db929":"for feature in features:\n    run_ols_regression(ols_results, dataset_ols, 'price', feature)","2ccd9c79":"pd.DataFrame(ols_results)","8fab58c6":"y = dataset_ols['price']\nX = dataset_ols.drop(['price'], axis=1)","67c92203":"result = stepwise_selection(X, y, verbose = True)\nprint('resulting features:')\nprint(result)","23cf8df4":"pred = '+'.join(features)\nformula = 'price~' + pred","5d55dbd5":"model = ols(formula=formula, data=dataset_ols).fit()\nmodel.summary()","e3c66cbc":"y = dataset_ols['price']\nX = dataset_ols.drop(['price', 'yr_since_renovation', 'sqft_above', 'condition_3', 'condition_4'], axis=1)","65ab3180":"X_int = sm.add_constant(X)\nmodel = sm.OLS(y, X_int).fit()\nmodel.summary()","3c174719":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))","190eeadf":"# Fitting the model to the training data\nlinreg = LinearRegression().fit(X_train, y_train)\n\n# Calc preditors on the train and test set\ny_hat_train = linreg.predict(X_train)\ny_hat_test = linreg.predict(X_test)","93f83a24":"# Calc residuals\ntrain_residuals = y_hat_train - y_train\ntest_residuals = y_hat_test - y_test","e30b1b7b":"# Calc MSE (Mean Squared Error)\ntrain_mse = mean_squared_error(y_train, y_hat_train)\ntest_mse = mean_squared_error(y_test, y_hat_test)\nprint('Train Mean Squarred Error:', train_mse)\nprint('Test Mean Squarred Error:', test_mse)","50418ad0":"fig = plt.figure(figsize=(16, 8))\nsns.distplot(y_test - y_hat_test, bins=100);\n# sns.distplot(test_residuals, bins=50)","6bcd26aa":"train_error = []\ntest_error = []\n\nfor t in range(5, 95):\n    train_temp = []\n    test_temp = []\n    for i in range(100):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=t\/100)\n        linreg.fit(X_train, y_train)\n\n        y_hat_train = linreg.predict(X_train)\n        y_hat_test = linreg.predict(X_test)\n\n        train_temp.append(mean_squared_error(y_train, y_hat_train))\n        test_temp.append(mean_squared_error(y_test, y_hat_test))\n    \n    # save average train\/test errors\n    train_error.append(np.mean(train_temp))\n    test_error.append(np.mean(test_temp))\n\nfig = plt.figure(figsize=(16, 12))\nplt.scatter(range(5, 95), train_error, label='training error')\nplt.scatter(range(5, 95), test_error, label='testing error')\n\nplt.legend()\nplt.show()","3c598260":"from sklearn.model_selection import cross_val_score\n\ncv_5_results  = np.mean(cross_val_score(linreg, X, y, cv=5, scoring=\"neg_mean_squared_error\"))\ncv_10_results = np.mean(cross_val_score(linreg, X, y, cv=10, scoring=\"neg_mean_squared_error\"))\ncv_20_results = np.mean(cross_val_score(linreg, X, y, cv=20, scoring=\"neg_mean_squared_error\"))","8e10cd86":"print(cv_5_results, cv_10_results, cv_20_results)","df0248db":"print('Measure of the quality of an estimator - values closer to zero are better\\n\\n')\nprint('MAE: ', metrics.mean_absolute_error(y_test, y_hat_test))\nprint('MSE: ', metrics.mean_squared_error(y_test, y_hat_test))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_hat_test)))","49e0b64c":"### Regression Evaluation","894ea0df":"## Normalize dataset","bc5a3bce":"dataset.info()","8016ae9e":"# Correlation Matrix\n#\ndataset.corr()","1402e3ee":"---\n## Answers\n1. Zip Code (neighborhood) can be an indicator for house prices \n    (see the top 5 zip codes 98039, 98004, 98040, 98112, 98102).\n2. Housing density in condery is less an indicator for the house price.\n3. Regards grade and condition of the house I believe that with the data given we have too little informations and therefore is inconclusive.","26ea6a8f":"### Data Science using OSEMN\n\nThe first main step is to obtain all needed informations and import all needed libraries. Here I included the geojson data file from King County and extracted the needed data to help me visuzlize housing data. Next comes the handling of data which is either missing or just not clean. By going through the data and addressing the bits and peices ie NaN etc, helps me understand the data and hopfully can derive some statistics and visualizations. this follows a classifications and scaling of features. Finishing with the interpretation of what I can find.\n\n1. Obtaining data\n2. Scrubbing data\n3. Exploring data\n4. Modeling data \n5. Interpreting results\n","18f87e92":"Looks like that renovating can pay off by about $120,000, but there is no garantie for it.\n\n---","900140fa":"### Import all needed libraries ","cc1598d1":"## Investigating some of the outliers in numbers","8fac38f5":"Houses which lay in the 3-5 catergory of condition (especially condition 4 for modern homes) seem to have higher price than older homes.\n\n---","803fff81":"---\nIterating over the feature seem not to make any improvements on our R-squared value.\n\n##### Furthermore:\n- The regression output shows that most variables are statistically significant with **p-values** less than 0.05. \n- With regards to the **coefficients**, most variables are positively correlated with the price, only (lower) grades, a renovation status years back and the building year are negatively correlated.\n\n#### Final thoughts:\nThe grade of a house has more impact on the price if a house is older than 15 years and even more so if older than 50 years. That said the grade reflects to some extend the size of a house (cabin vs mansoin) but mostly the condition the house is in (poor, average all the way to luxury). The condition of a house has some effect on the price if it is in the range 3 - 5 on all houses no matter the age. One can make the argument that the conidition of a house is coralated wheather or not it is renovated. Renovation has a effect on the price of the house of a median average of $120,000. It is not known what kind of investment one has to make in order to gain such return.\n","589eb7f0":"f.display_jointplot(dataset, discrete_vars)","5c215ec1":"### Dropping features\n\nFinally we shall drop eature we are still carrying around but for sure are not needed.","4d1693d0":"### Residual Histogram","6d5c11a5":"num = 100\ntrain_err = []\ntest_err = []\n\nfor i in range(num):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n    linreg.fit(X_train, y_train)\n\n    y_hat_train = linreg.predict(X_train)\n    y_hat_test = linreg.predict(X_test)\n    \n    train_err.append(mean_squared_error(y_train, y_hat_train))\n    test_err.append(mean_squared_error(y_test, y_hat_test))\n    \nplt.scatter(list(range(num)), train_err, label='Training Error')\nplt.scatter(list(range(num)), test_err, label='Testing Error')\nplt.legend();","3c3f0b3b":"### Cleaning basement feature","cdb4addc":"## Sanity Check using sklearn","fd1243f8":"It lloks like that renovation will affect the price","e679a40f":"## Cross Validation","f0455e6c":"___\n\n### Get a gerneral overview via scatter plot","3b615de0":"---\n# Answering question  #3\n\n### Relational plots\nLets visualize the relationship of price and sqft_living15 by grade and condition to hopefuly get a deeper inside","8f78f638":"**Notes:** \n    * Cross referencing on trulia, there are houses with a high sqft_living15 as well a price at $24,000,000\n    * And as it turns out the number of bathrooms can be 8 or even 9, and it looks like that it might have some effect on the price\n    * But 30+ bedrooms is an outlier, they as well looks like can have some effect on the price \n    * How does age (yr_built) and sqft_living coerlate?\n    * It looks like that zipcodes are coerlated to price","48ce1d52":"We shall drop 'yr_since_renovation', 'sqft_above', 'bedrooms', 'condition_3', 'condition_4' \nfrom the feature list","379b072b":"Initial observation (based on the darker colours\/higher and lower values):\n    * In general there are very few strong correlations (around +\/-0.7 and beyond)\n    * price correlates to sqft_living\/15 and grade\n    * grade correlates with sqft_above\n    * house_age correlates with bathrooms, floors, grade, sqft_above\n    * we can consolidate sqft_living, sqft_living15 and sqft_above","13f80758":"###### Observation:\n    * The most pricey zipcode 98039 seems to be also one of the less densly populated zipcode.\n    * The housing density is focused around Seattle ","99f596a4":"#### Description of what can be found in the dataset\n\n+ **ida** notation for a house\n+ **date** Date house was sold\n+ **price** Price is prediction target\n+ **bedrooms** Number of Bedrooms\/House\n+ **bathrooms** Number of bathrooms\/bedrooms\n+ **sqft_living** square footage of the home\n+ **sqft_lot** square footage of the lot\n+ **floors** Total floors (levels) in house\n+ **waterfront** House which has a view to a waterfront\n+ **view** Has been viewed\n+ **condition** How good the condition is ( Overall )\n+ **grade** overall grade given to the housing unit, based on King County grading system (see below)\n+ **sqft_above** square footage of house apart from basement\n+ **sqft_basement** square footage of the basement\n+ **yr_built** Built Year\n+ **yr_renovated** Year when house was renovated\n+ **zipcode** zip\n+ **lat** Latitude coordinate\n+ **long** Longitude coordinate\n+ **sqft_living15** Living room area in 2015 (implies-- some renovations) This might or might not have affected the lot size area\n+ **sqft_lot15** lotSize area in 2015 (implies-- some renovations)\n\nhttp:\/\/www5.kingcounty.gov\/sdc\/FGDCDocs\/resbldg_extr_faq.htm\n##### BLDGGRADE\nBuildling grade (Source: King County Assessments)\n+ Value - Definition\n+ 0     - Unknown\n+ 1     - Cabin\n+ 2     - Substandard\n+ 3     - Poor\n+ 4     - Low\n+ 5     - Fair\n+ 6     - Low Average\n+ 7     - Average\n+ 8     - Good\n+ 9     - Better\n+ 10    - Very Good\n+ 11    - Excellent\n+ 12    - Luxury\n+ 13    - Mansion\n+ 20    - Exceptional Properties\n","54071621":"___\n\n### Categorize Data\n\nWe need to create dummy vars for our categorical variables. **One-hot encoding** shall do the trick. ","2d00c75d":"### Do your regression model","74aca718":"Simpler representation, but in this cae more effective. And brings the point across.","4ad69b3b":"## Regression Model Validation","24713753":"### Investigate Discrete Variables\n","8fc3f84f":"msno.matrix(dataset)","e4635849":"# Waterfront - Level Up:\n# We could try to determine by lat\/long if a house is at the waterfront or not by \n# implementing k-nearest neighbor \n# https:\/\/stackabuse.com\/k-nearest-neighbors-algorithm-in-python-and-scikit-learn\/","6f999b9a":"### Collecting basic informations about the data set","47907a0a":"Regarding to [trulia](https:\/\/www.trulia.com\/for_sale\/53033_c\/price;d_sort\/) most of the so called outliers seen in the plot above seem legit (as of 11\/02\/2018).\n","9eb83ccd":"### Go through the selection process","766edc1e":"## Obtaining Data","18067300":"---\n### Investigating Continuos Variables in relationship to price","1b108439":"### Get the big picture","c4530744":"## Concat normalized data and selected feature into new dataframe","1a269ecb":"### Check out each feature by itself","024ee900":"abs(dataset.corr()) > 0.7","4fb7a7d5":"___\n\n# Scrubbing Data\n\n\n## Cleaning Data","bc119e9c":"___\n\n# Answer #1\n\n### Visualize house prices and density by zipcode\n\nDue to missing data we can't run the cells below.\n","1b811bed":"___\n\n## Regression Model","b0d7d24e":"sns.catplot(x=\"is_renovated\", y=\"price\", data=dataset, height=4, aspect=2)\nplt.title('\\nIs Renovated vs. Price\\n', fontweight='bold')\nplt.xlabel('Is Renovated')\nplt.ylabel('Price');","304f995c":"Grade reflects in the price more in older houses, especially houses older than 50 years.\n\n**Conclusion:**\n- Whether you renovate or not is a matter of the outcome you desire. But a simple home improvement seems to help with the selling price.\n- The condition your house is in is important, especially you want to make sure you are in the category 3-5.\n- The grade given for your house reflects in the price on older houses and therefore especially important.","1e642cab":"# Answer #2\n\nLocation, location, location. Waterfront properties are by far the most expensive once.","ece26607":"# Questions:\n\n\n1. Is location of a house by zipcode\/neighborhood an indicator for the house price? \n2. Do have zipcodes (neighborhoods) with the higher housing density an effect on selling price?\n3. Does grade, condition and renovation of a house reflect in the price?"}}