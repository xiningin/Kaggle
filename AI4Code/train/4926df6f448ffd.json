{"cell_type":{"6128e681":"code","39119aa7":"code","941ce4fe":"code","fbd5f50b":"code","74a179be":"code","0b0b566a":"code","91f79f8c":"code","85b54667":"code","3a834a2d":"code","85b162e4":"code","1841a40d":"code","f83239e2":"code","15e6d4a6":"code","760b6f27":"code","a4371b94":"code","f63f1851":"code","c41bc26d":"code","110b7109":"code","1f2001a4":"code","2176e972":"code","cc52d629":"code","6f99dcae":"code","2976f54e":"code","e42c1833":"code","313f70ab":"code","36612513":"code","bdcac307":"code","346a62a5":"code","e3296cf1":"code","8a00d43c":"code","ef7ab3d3":"code","a741265e":"code","1cee517d":"code","6423e5ba":"code","99939ac1":"code","e43dc055":"code","e9c71581":"code","3d7d764b":"code","bcca826c":"code","89fd4d0e":"code","fc6fe8e6":"code","db5bfaad":"code","c02af184":"code","0ce6b913":"code","a14ac595":"code","48a66bb2":"code","9df5b620":"code","9ad638f8":"code","3d4a7515":"code","9f353a05":"code","ff379170":"code","1e330ca9":"code","f4181e71":"code","2c35bd14":"code","3bd5f30c":"code","7832cdd7":"code","5c5dc3cf":"code","da69b856":"code","6951a622":"code","cb8e01c5":"code","185bec2f":"code","731a5982":"code","5990e81d":"code","3e971fcc":"code","b1f0fe41":"code","54da0be1":"code","de4ce764":"code","4d713ad8":"code","83a7c01c":"code","11f017c7":"code","e54d3a44":"code","44f45ab2":"code","aca465b9":"code","d11d3217":"code","d03d0718":"code","c95a7e2b":"code","97e3ff4f":"code","a479600d":"code","810bfdff":"code","bb1683d2":"code","47f59bfe":"code","a33ca442":"code","0a4a7419":"code","0e4b7b22":"code","8dd08b9b":"code","847c1219":"code","cf851376":"code","3fa7b145":"code","64d3e8a5":"code","d945e376":"code","31c5776f":"code","832f47b0":"code","0d8d81fd":"code","7782ba2a":"code","4a3ed6a4":"code","cc085fb8":"code","d36797cb":"code","7fc46974":"code","338691d2":"code","5dd8fd4e":"code","dd527a83":"code","33eeaa00":"markdown","715621e6":"markdown","9fb25df4":"markdown","68b69c0c":"markdown","560c3514":"markdown","bd035f95":"markdown","98b1087f":"markdown","dfb1d8f9":"markdown","d843d385":"markdown","0e99a905":"markdown","eb930c9e":"markdown","c15404ad":"markdown","53f993fc":"markdown","563134f8":"markdown","29e0ff17":"markdown","996aa2d9":"markdown","7d506261":"markdown","a36d40b8":"markdown","ce7129c2":"markdown","09693bd7":"markdown","150297d6":"markdown","7fefd554":"markdown","7c761a57":"markdown","554cafb3":"markdown","f8a96a8e":"markdown","1ae11bb2":"markdown","4d0ec5b8":"markdown","c82a5acb":"markdown","ca18ed94":"markdown","37267ec4":"markdown","5e7ea1b6":"markdown","9f968941":"markdown","e4a19be4":"markdown","78899d6e":"markdown","ee63fd8b":"markdown","ccd50aeb":"markdown","6f5db057":"markdown","5f011ada":"markdown","6a905177":"markdown","3552c27b":"markdown","599fbd65":"markdown","0406a12f":"markdown","c107500c":"markdown","6db75c78":"markdown","46c5c990":"markdown","0ce51b01":"markdown","c44f47c5":"markdown","8dce9519":"markdown","61a8c688":"markdown","fe0f10cc":"markdown","c5830028":"markdown","0a8da50f":"markdown","5cf3d4b2":"markdown","cc6816a4":"markdown","37e2e208":"markdown","b4a19645":"markdown","17bc9ac7":"markdown","360cbbce":"markdown","775cb4cf":"markdown","c5cbe89a":"markdown","585f430b":"markdown","4e665d3b":"markdown","9ab808a4":"markdown","52359edb":"markdown","ff131ab3":"markdown","fb11ca5e":"markdown","8184742c":"markdown","a17a3dfe":"markdown","8358a23d":"markdown","80da97b2":"markdown","d28cbb64":"markdown","9af11823":"markdown","ebd9c4b3":"markdown","96220834":"markdown","da4cc359":"markdown","694c63be":"markdown","90f95b95":"markdown","a2a89708":"markdown","45de586e":"markdown","b8d9a45f":"markdown","c0b2e8b5":"markdown","5a7ea12c":"markdown","ee5609ce":"markdown","a92092a1":"markdown","41f013eb":"markdown","0c289a75":"markdown","f746d154":"markdown","c59f6661":"markdown"},"source":{"6128e681":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")","39119aa7":"data = pd.read_csv('\/kaggle\/input\/youtube-new\/GBvideos.csv') ","941ce4fe":"data.head()","fbd5f50b":"data.shape","74a179be":"def make_label_encoder(original_feature , new_feature) : \n    enc  = LabelEncoder()\n    enc.fit(data[original_feature])\n    data[new_feature] = enc.transform(data[original_feature])\n    data.drop([original_feature],axis=1, inplace=True)","0b0b566a":"def make_countplot(feature) :\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"prism\", 3)) ","91f79f8c":"def make_kdeplot(feature) : \n    sns.kdeplot(data[feature], shade=True)","85b54667":"def make_pie(feature) : \n    plt.pie(data[feature].value_counts(),labels=list(data[feature].value_counts().index),\n        autopct ='%1.2f%%' , labeldistance = 1.1,explode = [0.05 for i in range(len(data[feature].value_counts()))] )\n    plt.show()","3a834a2d":"data.columns","85b162e4":"for col in data.columns : \n    print('Unique values for Column {0}     is      {1}'.format(col ,len(data[col].unique())))","1841a40d":"make_label_encoder('video_id' , 'video_id Code')","f83239e2":"data.head()","15e6d4a6":"data['trending_date'].unique()","760b6f27":"len(data['trending_date'].unique())","a4371b94":"year_list = []\nmonth_list = []\nday_list = []\nfor x in range(data.shape[0]) :\n    year_list.append(data['trending_date'][x][:2])\n    month_list.append(data['trending_date'][x][6:])\n    day_list.append(data['trending_date'][x][3:5])","f63f1851":"data.insert(16,'Year',year_list)\ndata.insert(17,'Month',month_list)\ndata.insert(18,'Day',day_list)","c41bc26d":"data.head()","110b7109":"ax = sns.countplot(x=\"Year\", data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\",3))","1f2001a4":"ax = sns.countplot(x=\"Month\", data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\",3))","2176e972":"ax = sns.kdeplot(data['Day'], shade=True)","cc52d629":"data.drop(['trending_date'],axis=1, inplace=True)","6f99dcae":"year_dict = {'17': 0,'18':1}\ndata['Year'] = data['Year'].map(year_dict)\n","2976f54e":"data.head()","e42c1833":"len(data['title'].unique())","313f70ab":"all_words = []\nfor x in range(data.shape[0]) : \n    all_words = all_words  +  data['title'][x].split()","36612513":"len(all_words)","bdcac307":"all_words_series = pd.Series(all_words)","346a62a5":"top_words =  all_words_series.value_counts()[:30] \ntop_words","e3296cf1":"top_words = list(top_words.index)","8a00d43c":"top_words","ef7ab3d3":"top_words.remove('-')\ntop_words.remove('|')\ntop_words.remove('The')\ntop_words.remove('&')\ntop_words.remove('the')\ntop_words.remove('of')\ntop_words.remove('and')\ntop_words.remove('in')\ntop_words.remove('on')\ntop_words.remove('a')\ntop_words.remove('with')\ntop_words.remove('In')\ntop_words.remove('To')\ntop_words.remove('A')","a741265e":"top_words","1cee517d":"l1 = []\nl2 = []\nl3 = []\nl4 = []\nl5 = []\nl6 = []\nl7 = []\nl8 = []\nl9 = []\nl10 = []\nl11 = []\nl12 = []\nl13 = []\n\nthis_list = []\nfor x in range(data.shape[0]) : \n    this_list  =  data['title'][x].split()\n    \n    if ('Video)' in this_list) or ('Video]' in this_list)  : \n        l1.append(1)\n    else : \n        l1.append(0)\n        \n    if ('(Official' in this_list) or ( 'Official'in this_list)  or ( '[Official' in this_list)  : \n        l2.append(1)\n    else : \n        l2.append(0)\n        \n    if 'Trailer' in this_list : \n        l3.append(1)\n    else : \n        l3.append(0)\n\n    if 'You' in this_list : \n        l4.append(1)\n    else : \n        l4.append(0)\n    if 'Last' in this_list : \n        l5.append(1)\n    else : \n        l5.append(0)\n    if 'Star' in this_list : \n        l6.append(1)\n    else : \n        l6.append(0)\n    if 'My' in this_list : \n        l7.append(1)\n    else : \n        l7.append(0)\n    if 'Me' in this_list : \n        l8.append(1)\n    else : \n        l8.append(0)\n    if  'I'  in this_list : \n        l9.append(1)\n    else : \n        l9.append(0)\n    if 'to' in this_list : \n        l10.append(1)\n    else : \n        l10.append(0)\n    if '2018' in this_list : \n        l11.append(1)\n    else : \n        l11.append(0)\n    if 'Music' in this_list : \n        l12.append(1)\n    else : \n        l12.append(0)\n \n    if 'ft.' in this_list : \n        l13.append(1)\n    else : \n        l13.append(0)","6423e5ba":"len(l12)","99939ac1":"data.insert(18,'word 1',l1)\ndata.insert(19,'word 2',l2)\ndata.insert(20,'word 3',l3)\ndata.insert(21,'word 4',l4)\ndata.insert(22,'word 5',l5)\ndata.insert(23,'word 6',l6)\ndata.insert(24,'word 7',l7)\ndata.insert(25,'word 8',l8)\ndata.insert(26,'word 9',l9)\ndata.insert(27,'word 10',l10)\ndata.insert(28,'word 11',l11)\ndata.insert(29,'word 12',l12)\ndata.insert(30,'word 13',l13)","e43dc055":"data.drop(['title'],axis=1, inplace=True)\n","e9c71581":"data.head()","3d7d764b":"len(data['channel_title'].unique())","bcca826c":"make_label_encoder('channel_title' , 'channel Code')","89fd4d0e":"data.head()","fc6fe8e6":"data['category_id'].unique()","db5bfaad":"len(data['category_id'].unique())","c02af184":"make_countplot('category_id')","0ce6b913":"len(data['publish_time'].unique())","a14ac595":"data['publish_time'][:10]","48a66bb2":"publish_quarter = []\n\nfor x in range(data.shape[0]):\n    publish_hour = int(str(data['publish_time'][x])[11:13])\n    if publish_hour >=0 and publish_hour < 6  : \n        publish_quarter.append(1)\n    elif publish_hour >=6 and publish_hour < 12  : \n        publish_quarter.append(2)\n    elif publish_hour >=12 and publish_hour < 18  : \n        publish_quarter.append(3)\n    else: \n        publish_quarter.append(4)","9df5b620":"len(publish_quarter)","9ad638f8":"data.insert(30,'Publish Quarter',publish_quarter)","3d4a7515":"data.head()","9f353a05":"make_label_encoder('publish_time' , 'publish time code')","ff379170":"data.head()","1e330ca9":"len(data['tags'].unique())","f4181e71":"data['tags'][:10]","2c35bd14":"data['tags'][10]","3bd5f30c":"make_label_encoder('tags' , 'tags code')","7832cdd7":"data.head()","5c5dc3cf":"len(data['views'].unique())","da69b856":"data['views'].min()","6951a622":"data['views'].max()","cb8e01c5":"make_kdeplot('views')","185bec2f":"def feature_sectors(data , maxx , feature , new_featre):\n    step = (maxx- data[feature].min())\/10\n    new_list = []\n    minn = data[feature].min()\n    for x in range(data.shape[0]) : \n        if data[feature][x] <= (minn + step):\n            new_list.append(1)\n        elif data[feature][x] <= (minn + (2*step)):\n            new_list.append(2)            \n        elif data[feature][x] <= (minn + (3*step)):\n            new_list.append(3)            \n        elif data[feature][x] <= (minn + (4*step)):\n            new_list.append(4)            \n        elif data[feature][x] <= (minn + (5*step)):\n            new_list.append(5)            \n        elif data[feature][x] <= (minn + (6*step)):\n            new_list.append(6)            \n        elif data[feature][x] <= (minn + (7*step)):\n            new_list.append(7)            \n        elif data[feature][x] <= (minn + (8*step)):\n            new_list.append(8)            \n        elif data[feature][x] <= (minn + (9*step)):\n            new_list.append(9)            \n        else:\n            new_list.append(10)            \n    data.insert(data.shape[1], new_featre , new_list)   \n    \n","731a5982":"feature_sectors(data ,50000000 , 'views' , 'views sector')","5990e81d":"data.head()","3e971fcc":"make_countplot('views sector')","b1f0fe41":"data.drop(['views sector'],axis=1, inplace=True)","54da0be1":"feature_sectors(data ,1000000 , 'views' , 'views sector')","de4ce764":"data.head()","4d713ad8":"make_countplot('views sector')","83a7c01c":"data.drop(['views sector'],axis=1, inplace=True)","11f017c7":"feature_sectors(data ,10000000 , 'views' , 'views sector')","e54d3a44":"data.head()","44f45ab2":"make_countplot('views sector')","aca465b9":"make_kdeplot('likes')","d11d3217":"feature_sectors(data ,500000 , 'likes' , 'likes sectors')","d03d0718":"make_countplot('likes sectors')","c95a7e2b":"make_kdeplot('dislikes')","97e3ff4f":"feature_sectors(data ,50000 , 'dislikes' , 'dislikes sectors')","a479600d":"make_countplot('dislikes sectors')","810bfdff":"data.drop(['dislikes sectors'],axis=1, inplace=True)","bb1683d2":"feature_sectors(data ,10000 , 'dislikes' , 'dislikes sectors')","47f59bfe":"make_countplot('dislikes sectors')","a33ca442":"data.head()","0a4a7419":"make_kdeplot('comment_count')","0e4b7b22":"feature_sectors(data ,10000 , 'comment_count' , 'comment_count sectors')","8dd08b9b":"make_countplot('comment_count sectors')","847c1219":"data.drop(['thumbnail_link'],axis=1, inplace=True)","cf851376":"make_label_encoder('comments_disabled','comments_disabled code')\nmake_label_encoder('ratings_disabled','ratings_disabled code')\nmake_label_encoder('video_error_or_removed','video_error_or_removed')","3fa7b145":"data.head()","64d3e8a5":"len(data['description'].unique())","d945e376":"data.drop(['description'],axis=1, inplace=True)","31c5776f":"make_countplot('category_id')","832f47b0":"len(data['category_id'].unique())","0d8d81fd":"data.info()","7782ba2a":"X = data.drop(['category_id'], axis=1, inplace=False)\ny = data['category_id']","4a3ed6a4":"X.shape","cc085fb8":"y.shape","d36797cb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","7fc46974":"GBCModel = GradientBoostingClassifier(n_estimators=100,max_depth=3,random_state=33) \nGBCModel.fit(X_train, y_train)","338691d2":"print('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))","5dd8fd4e":"DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='entropy',max_depth=20,random_state=33) #criterion can be entropy\nDecisionTreeClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('DecisionTreeClassifierModel Train Score is : ' , DecisionTreeClassifierModel.score(X_train, y_train))\nprint('DecisionTreeClassifierModel Test Score is : ' , DecisionTreeClassifierModel.score(X_test, y_test))","dd527a83":"#Calculating Prediction\ny_pred = DecisionTreeClassifierModel.predict(X_test)\ny_pred_prob = DecisionTreeClassifierModel.predict_proba(X_test)\nprint('Predicted Value for DecisionTreeClassifierModel is : ' , y_pred[:20])","33eeaa00":"ok the graph ","715621e6":"how it looks now","9fb25df4":"how data looks like now","68b69c0c":"I think 500 thousands is a suitable number to divide on it","560c3514":"so we need to convert it to Series so we can easily count repeated words ","bd035f95":"let's see how it looks like now","98b1087f":"great accuracy , let's use it in prediction ","dfb1d8f9":"unique data is an important thing to know about each feature , so let;s know the umber of unique values for each feature ","d843d385":"how data looks like","0e99a905":"\nalso we can handle the Year value , to make it either 0 or 1 , instead of 17 & 18 , so it may be easier for the model in calculation ","eb930c9e":"let's have a look at it ","c15404ad":"____\n\nok , several important & less important features , how is the dimension ? ","53f993fc":"ok , I think this is kinda good as a representative for the data \n\n______\n\n\nwe need to make something similar in likes\n","563134f8":"almost there is no repeated values here , how about min & max values for views of these videos","29e0ff17":"ok , not a very great accuracy , how about Decision Tree Classifier , it might be helpful","996aa2d9":"so we'll not be able to encode it , since it contains many unusual charracters , so we'll have to drop it","7d506261":"ok , we might not need to make that word processing as we did in the title , let's just convert it using labelencoder","a36d40b8":"how data looks now","ce7129c2":"___\n\n# Viewership Numbers\n\now we need to focus in important numbers like : views, likes & dislikes\n\nlet's start with views","09693bd7":"here we go they are 16 words \n\nbut since there are repeating words here which are Video) , Video] , also 3 different types of official , so they are 13 words\n\nnow we'll make 13 lists , each one will be either 1 if the title contain this word , or 0 if it doesn't , then we'll insert them in the dataset later","150297d6":"ok looks better, now how looks like","7fefd554":"let's draw it","7c761a57":"_____\n\nok how about tags ? ","554cafb3":"so we'll make a new Series which is the most repeated 30 words here","f8a96a8e":"____\n\nhow about comment count","1ae11bb2":"from 39K row , there is only 205 unique value , so we'll need it \n\nnow let's make a smart step , we can extract new features from treding_date , by getting the year , month & date of it to use them as separate 3 new features ","4d0ec5b8":"I think we can reduce it a little bit , let's drop it first","c82a5acb":"_______\n\n\n# More Data Processing\n\nok , let's continue in the rest of features\n\nhow about the channel_title","ca18ed94":"____\n\nnow how about trending_date , we need to know more about it","37267ec4":"\n______\n\n# Important Functions \n\nso we'll need to define important functions which will be used here","5e7ea1b6":"how it looks","9f968941":"ok , how many unique values ? \n","e4a19be4":"perfect , let's check the length of any of these lists","78899d6e":"now we can safely drop the title feature","ee63fd8b":"____\n\nnow the description , how many unique values here ? \n","ccd50aeb":"then we can open the dataset","6f5db057":"_____\n\nas shown here , some features will be used with its original values , & some will need some processing\n\n\nlet's start with video_id . \n\nsince video_id generally refer to something useless , which will not be helpful in training , but here we can see that there is 3272 unique values from almost 39K sample size , which mean there is a huge number of repeating video_id , so this feature will be important in training\n\nwe'll have to convert it using labelencoder from sklearn , to use code instead of it \n","5f011ada":"great. we need to have a look for the test score","6a905177":"ok , we need to get only the index (the words themselves)","3552c27b":"& make it with 10 thousands","599fbd65":"exactly the length of our dataset , now le'ts insert them in the dataset","0406a12f":"ok looks fine , how many unique values are there ? ","c107500c":"how it looks like ? ","6db75c78":"so it's clear that majority of videos within 800 to 30 Million\n\n\nso we'll make an important function to make 10 sectors of this feature (or any feature) , depend on the max value we define","46c5c990":"let's see a random tag","0ce51b01":"great , how many words we have now  ? ","c44f47c5":"great , let's be sure the length of the list is equal to data length","8dce9519":"____\n\n# Building the Model\n\n\nhow about using Gradient Boosting Classifier , with 100 estimators & 3 depth ","61a8c688":"how data looks like","fe0f10cc":"so it's year them month then day then housr , minute , second . .\n\nI think the most important part of data here is the House of publishing , since the day , month & year is somehow related to trending date which we already included\n\nso let's make something useful with it , we'll use the publishing hour to know which quarter of the day it published , & use it as an important feature","c5830028":"well, not very representative m since almost all sectors less than 50 Million , lets drop this sector , & make a new sector with only 1 million views as max","0a8da50f":"how the data looks like","5cf3d4b2":"\n_______\n\n\n# Data Processing \n\nso we'll need to start with processing the features one after one , the list of features is : ","cc6816a4":"kinda suitable , let's do it in dislikes","37e2e208":"how it looks","b4a19645":"so we can encode publish time now","17bc9ac7":"____\n\now the thubnail link will be useless for us","360cbbce":"let's graph it","775cb4cf":"more than 3 thousand unique values , how this data looks like ? ","c5cbe89a":"____\n\n\ncool  , now let's see how countplot of different years , months & days","585f430b":"not very well , 1 Million is too small for that , let's use 10 million instead , but first drop that sector feature","4e665d3b":"now to insert it","9ab808a4":"______\n\n\n# Words Processing\n\nwe have here a big point which related to the title of the video \n\nthe title contain several different words , which might be important for training , but also we can never just use labelencoder to convert unique values , we need to make something more smart\n\nfirst let's know the amount of unique values","52359edb":"great , now let's have another look","ff131ab3":"great , now we are ready , let's have a final look ","fb11ca5e":"now let's have a look to the data","8184742c":"how it looks ","a17a3dfe":"the different is HUGE , from 800 to 400 Million , even drawing it will not be easy","8358a23d":"now we can add them to the data","80da97b2":"as we get all what we need from trending_date , we can drop it now","d28cbb64":"majority of them in 10th & 24th category , ok , nothing to be done with it\n\n_____\n\nhow about publish time","9af11823":"what are their shapes ? ","ebd9c4b3":"and graph","96220834":"and have a look now","da4cc359":"\n____\n\nOK , we'll make a HUGE list contain all words separately of all titles , then we'll focus only on most repeated words , as they are important keywords","694c63be":"may be 50 thousand is suitable","90f95b95":"we are lucky , no nulls \n\n_____\n\n# Data Splitting\n\nfirst we'll need to define X & y\n","a2a89708":"let's repeat it with 10 thousand","45de586e":"as we see , there are many meaningless words , which looks like stopwords , so we have to drop them to avoid any misleading in training as they refer to nothing in the meaning ","b8d9a45f":"also we have to convert comment_disabled , rating_disabled , video_error_or_removed , using label encoder to be either 1 or 0","c0b2e8b5":"___\n\n\ngreat , now let's use it to make 10 sectors for the feature views , with max 50 Million , & check how it looks like","5a7ea12c":"so this is the output , which we'll need to classify the data depend on it , how many categories we have here","ee5609ce":"let's split it using sklearn","a92092a1":"we can encode it directly now","41f013eb":"now the last feature : category_id","0c289a75":"how about category_id","f746d154":"how it looks now","c59f6661":"# Youtube Category Classification\nBy : Hesham Asem\n____\n\n\nin this dataset , we have a dataset for viewership habits for specific most famous videos on Youtube , for Great Britain on 2017 , 2018\n\nwe need to make a classification model , so it can detect which category of the video depend on viewership habits & video details\n\ndataset here : \nhttps:\/\/www.kaggle.com\/datasnaek\/youtube-new#GBvideos.csv\n\n____\n\n\nlet's first import needed libraries\n"}}