{"cell_type":{"549efa13":"code","64cc16df":"code","43065006":"code","8f7457c9":"code","79c85d07":"code","a798936f":"code","00eac232":"code","080b9152":"code","6be107fe":"code","a82db53a":"code","048995a0":"code","44909c0f":"code","257d3e36":"code","06140203":"code","34682339":"code","e373a3ea":"code","d2f55882":"code","2a2775ff":"code","482c9447":"code","5300e0d0":"code","59c48d2e":"code","a64fbebc":"code","36ac6ecb":"code","ec08b1eb":"code","70b1a0bc":"markdown","ca298207":"markdown","be9b2f67":"markdown","20fca6a4":"markdown","9381ab92":"markdown","14c8115e":"markdown","7b30805f":"markdown","28b7b76a":"markdown","14c54401":"markdown","8acb503d":"markdown","4673c55b":"markdown","5104d694":"markdown","6f3760cd":"markdown"},"source":{"549efa13":"import pandas as pd\nfrom sklearn.datasets import load_wine\nwine_dataset = load_wine()\nwine = pd.DataFrame(wine_dataset.data, columns=wine_dataset.feature_names)\n\nwine['quality'] = wine_dataset.target\nwine.head()\n","64cc16df":"wine = wine[wine.quality !=2]","43065006":"wine.quality.value_counts()","8f7457c9":"X = wine.drop(columns='quality')\nY = wine['quality']","79c85d07":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","a798936f":"from sklearn.linear_model import LogisticRegression","00eac232":"model = LogisticRegression(max_iter = 1000)\nmodel.fit(X_train, Y_train)","080b9152":"pred = model.predict(X_test)\npred","6be107fe":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report, auc, roc_curve\nconfusion_matrix(pred, Y_test)","a82db53a":"precision_score(Y_test, pred)","048995a0":"recall_score(Y_test, pred)","44909c0f":"f1_score(pred,Y_test)","257d3e36":"classification_report(pred, Y_test)","06140203":"# a = auc(pred, Y_test)\nfrom sklearn.metrics import roc_auc_score\nfpr, tpr, thres = roc_curve(Y_test,  pred)\nplt.scatter(fpr, tpr)\nthres","34682339":"roc_auc_score(pred, Y_test)","e373a3ea":"\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\ny_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(Y_test,  y_pred_proba)\nauc = roc_auc_score(Y_test, pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","d2f55882":"plt.scatter(fpr, tpr)\nplt.show()","2a2775ff":"\n# Print ROC curve\nimport numpy as np\nplt.plot(fpr,tpr)\nplt.show() \n\n# Print AUC\nauc = np.trapz(tpr,fpr)\nprint('AUC:', auc)","482c9447":"# from sklearn.feature_selection import SelectKBest, f_regression\n# from sklearn.pipeline import make_pipeline\n# anova_filter = SelectKBest(f_regression, k=3)\n# anova_svm = make_pipeline(anova_filter, model)\n# anova_svm.fit(X_train, Y_train)","5300e0d0":"# pred = anova_svm.predict(X_test)","59c48d2e":"# from sklearn.linear_model import RidgeClassifierCV\n# model = RidgeClassifierCV()","a64fbebc":"# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import uniform\n# logistic = LogisticRegression(solver='liblinear',multi_class='ovr')\n# distributions = dict(class_weight=['balanced'],penalty=['l2', 'l1'],max_iter=[10,50,100,300,500,900])\n# clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n# search = clf.fit(X_train,Y_train)\n# search.best_params_","36ac6ecb":"# model = LogisticRegression(solver='liblinear',multi_class='ovr',penalty='l2',max_iter=300,class_weight='balanced')","ec08b1eb":"# model.fit(X_train, Y_train)","70b1a0bc":"**To predict A discrete outcome.Logistic Regression measures the relationship between the dependent variable (our label, what we want to predict) and the one or more independent variables (our features), by estimating probabilities using it\u2019s underlying logistic function\nThese probabilities must then be transformed into binary values in order to actually make a prediction. This is the task of the logistic function, also called the sigmoid function. The Sigmoid-Function is an S-shaped curve that can take any real-valued number and map it into a value between the range of 0 and 1, but never exactly at those limits. This values between 0 and 1 will then be transformed into either 0 or 1 using a threshold classifier.**","ca298207":"![image.png](attachment:image.png)","be9b2f67":"![image.png](attachment:image.png)","20fca6a4":"# Logistic Regression","9381ab92":"# Maximum Likelihood Estimation is a general approach to estimating parameters in statistical models.","14c8115e":"The precision tells us that it predicted wine quality==0, 92 % of the times","7b30805f":"# Recall = (TP\/(TP + FN) === (12\/(12 + 0))","28b7b76a":"# Precision = (TP\/(TP + FP)) ===  (12\/(12 + 1)) ","14c54401":"* * * **The F1 Score is the 2*((precision * recall)\/(precision+recall)). It is also called the F Score or the F Measure. Put another way, the F1 score conveys the balance between the precision and the recall.**","8acb503d":"**What is the AUC-ROC curve?\nThe Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the \u2018signal\u2019 from the \u2018noise\u2019. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.**","4673c55b":"**The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.The first big difference is that you calculate accuracy on the predicted classes while you calculate ROC AUC on predicted scores. That means you will have to find the optimal threshold for your problem. Moreover, accuracy looks at fractions of correctly assigned positive and negative classes.**","5104d694":" Our model classifies wine quality==0 100% of the time ","6f3760cd":"![image.png](attachment:image.png)"}}