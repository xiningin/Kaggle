{"cell_type":{"becaa6bb":"code","837deee9":"code","824f7d0e":"code","b2527e78":"code","693ecaa5":"code","ff679c89":"code","9827896d":"code","034c8745":"code","efb490ad":"code","b76771fa":"code","b8ba8825":"code","1c6f336a":"code","d6a9ea58":"code","4871a77a":"code","04a056c2":"markdown","a7831e27":"markdown","b547fd77":"markdown","8d0f598e":"markdown"},"source":{"becaa6bb":"import os\n\ndef load_data(path):\n    \"\"\"\n    Load dataset\n    \"\"\"\n    input_file = os.path.join(path)\n    with open(input_file, \"r\") as f:\n        data = f.read()\n\n    return data.split('\\n')","837deee9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","824f7d0e":"codes = load_data('..\/input\/deciphering-rnn\/cipher.txt')\nplaintext = load_data('..\/input\/deciphering-rnn\/plaintext.txt')","b2527e78":"codes[:5]","693ecaa5":"plaintext[:5]","ff679c89":"from keras.preprocessing.text import Tokenizer\n\ndef tokenize(x):\n    \"\"\"\n    Tokenize x\n    :param x: List of sentences\/strings to be tokenized\n    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n    \"\"\"\n    x_tk = Tokenizer(char_level=True)\n    x_tk.fit_on_texts(x)\n    \n    return x_tk.texts_to_sequences(x), x_tk","9827896d":"import numpy as np\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    # TODO: Implement\n    if length is None:\n        length = max([len(sentence) for sentence in x])\n    return pad_sequences(x, maxlen=length, padding='post')","034c8745":"def preprocess(x, y):\n    \"\"\"\n    Preprocess x and y\n    :param x: Feature List of sentences\n    :param y: Label List of sentences\n    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n    \"\"\"\n    preprocess_x, x_tk = tokenize(x)\n    preprocess_y, y_tk = tokenize(y)\n\n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n\n    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\npreproc_code_sentences, preproc_plaintext_sentences, code_tokenizer, plaintext_tokenizer =\\\n    preprocess(codes, plaintext)\n\nprint('Data Preprocessed')","efb490ad":"preproc_code_sentences[0]","b76771fa":"from keras.layers import GRU, Input, Dense, TimeDistributed\nfrom keras.models import Model\nfrom keras.layers import Activation\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\n\n\ndef simple_model(input_shape, output_sequence_length, code_vocab_size, plaintext_vocab_size):\n    \"\"\"\n    Build and train a basic RNN on x and y\n    :param input_shape: Tuple of input shape\n    :param output_sequence_length: Length of output sequence\n    :param code_vocab_size: Number of unique code characters in the dataset\n    :param plaintext_vocab_size: Number of unique plaintext characters in the dataset\n    :return: Keras model built, but not trained\n    \"\"\"\n    # Build the layers\n    learning_rate = 1e-3\n\n    input_seq = Input(input_shape[1:])\n    rnn = GRU(64, return_sequences=True)(input_seq)\n    logits = TimeDistributed(Dense(plaintext_vocab_size))(rnn)\n\n    model = Model(input_seq, Activation('softmax')(logits))\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    return model\n\n\n# Reshaping the input to work with a basic RNN\ntmp_x = pad(preproc_code_sentences, preproc_plaintext_sentences.shape[1])\ntmp_x = tmp_x.reshape((-1, preproc_plaintext_sentences.shape[-2], 1))","b8ba8825":"# Train the neural network\nsimple_rnn_model = simple_model(\n    tmp_x.shape,\n    preproc_plaintext_sentences.shape[1],\n    len(code_tokenizer.word_index)+1,\n    len(plaintext_tokenizer.word_index)+1)\n","1c6f336a":"simple_rnn_model.fit(tmp_x, preproc_plaintext_sentences, batch_size=32, epochs=5, validation_split=0.2)","d6a9ea58":"def logits_to_text(logits, tokenizer):\n    \"\"\"\n    Turn logits from a neural network into text using the tokenizer\n    :param logits: Logits from a neural network\n    :param tokenizer: Keras Tokenizer fit on the labels\n    :return: String that represents the text of the logits\n    \"\"\"\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n\nprint('`logits_to_text` function loaded.')\n\nprint(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], plaintext_tokenizer))","4871a77a":"plaintext[0]","04a056c2":"## Padding\nWhen batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n\nMake sure all the cipher sequences have the same length and all the plaintext sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https:\/\/keras.io\/preprocessing\/sequence\/#pad_sequences) function.","a7831e27":"# Deciphering Code with character-level RNNs\n\nThis notebook is a part of my learning journey which I've been documenting from Udacity's Natural Language Processing Nanodegree program, which helped me a lot to learn and excel advanced NLP stuff such as PySpark. Thank you so much Udacity for providing such quality content. \n\n\nIn this notebook, we'll look at how to build a recurrent neural network and train it to decipher strings encrypted with a certain cipher.\n# Loading Dataset\nThe dataset we have consists of 10,000 encrypted phrases and the plaintext version of each encrypted phrase.","b547fd77":"## Model Overview: Character-Level RNN\nThe model we will use here is a character-level RNN since the cipher seems to work on the characer level. In a machine translation scenario, a word-level RNN is the more common choice.\n\nA character-level RNN will take as input an integer referring to a specific character and output another integer. To be able to get our model to work, we'll need to preprocess our dataset in the following steps:\n 1. Isolating each character as an array element (instead of an entire phrase, or word being the element of the array)\n 1. Tokenizing the characters so we can turn them from letters to integers and vice-versa\n 1. Padding the strings so that all the inputs and outputs can fit in matrix form\n \nTo visualize this processing, let's assume either our source sequences (`codes` in this case) or target sequences (`plaintext` in this case) look like this (a list of strings):\n\n![list_1.png](attachment:list_1.png)\n\nSince this model will be working on the character level, we'll need to separate each string into a list of characters (implicitly done by the tokenizer in this notebook):\n\n![list_2.png](attachment:list_2.png)\n\nThen, the process of tokenization will turn each character into an integer.  Note that when you're working on the a word-level RNN (as in most machine translation examples), the tokenizer will assign an integer to each word rather than each letter, and each cell would represent a word rather than a character.\n\n![list_3.png](attachment:list_3.png)\n\nMost machine learning platforms expect the input to be a matrix rather than a list of lists. To turn the input into a matrix, we need to find the longest member of the list, and pad all shorter sequences with 0. Assuming 'and two' is the longest sequence in this example, the matrix ends up looking like this:\n\n![padded_list.png](attachment:padded_list.png)\n\n## Preprocessing\nFor a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n\nWe can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better.\n\nTurn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https:\/\/keras.io\/preprocessing\/text\/#tokenizer) function. Since we're working on the character level, make sure to set the `char_level` flag to the appropriate value. ","8d0f598e":"## Preprocess Pipeline"}}