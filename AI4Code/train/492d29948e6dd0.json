{"cell_type":{"dfeb02f1":"code","281f69d5":"code","81164a32":"code","62cf2426":"code","4875bbfb":"code","59af76eb":"code","8cc2b485":"code","6a4e7520":"code","2c8b3594":"code","07518ead":"code","68147910":"code","b47de1fe":"code","d2cb89fe":"code","231020b8":"code","a2d7911d":"code","6894ae95":"code","faf2a9ae":"code","898fc2c8":"code","855972be":"code","d3291c39":"code","5ae6c417":"code","075d357c":"code","44beab53":"code","fcb61ba5":"code","03fbe7dc":"code","eb373492":"markdown","09e907e2":"markdown","8385d7cb":"markdown","031ab302":"markdown","798a3b20":"markdown","0addb3fa":"markdown"},"source":{"dfeb02f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","281f69d5":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","81164a32":"#Reading the data\ndata_train = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\",index_col='Id')\ndata_test = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\",index_col='Id')","62cf2426":"#printing data\ndata_train.head()","4875bbfb":"data_train.tail()","59af76eb":"cols = data_train.columns.to_list()\nprint(cols)","8cc2b485":"#describe our data\ndata_train.describe(include=\"all\")","6a4e7520":"data_train.info()","2c8b3594":"#check for missing data\n\nnull_columns = data_train.columns[data_train.isnull().any()==True].to_list()\ndata_train[null_columns].isnull().sum().to_frame()\n","07518ead":"data_test.head()","68147910":"data_test.describe()","b47de1fe":"#check for missing data\n\nnull_columns_train = data_test.columns[data_test.isnull().any()==True].to_list()\ndata_test[null_columns_train].isnull().sum().to_frame()","d2cb89fe":"#Removing rows with missing sales price\nX = data_train.dropna(subset=['SalePrice'], axis=0)\ny = X.SalePrice\n\n#Removing SalePrice from X\nX.drop('SalePrice', axis=1, inplace=True)","231020b8":"#getting validation test from data_train\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, test_size=0.2,random_state =0)\nX_train.shape, X_test.shape","a2d7911d":"low_card_cols = [item for item in X_train.columns if X_train[item].nunique()<10 and X_train[item].dtype == 'object']\n\n#select numerical columns\nnum_cols = [item for item in X_train.columns if X_train[item].dtype in ['int64', 'float64']]\n\n#selected columns only\ntotal_cols = low_card_cols + num_cols\n\nX_Train = X_train[total_cols].copy()\nX_Value = X_test[total_cols].copy()\nX_Test = data_test[total_cols].copy()","6894ae95":"X_Test.head()","faf2a9ae":"X_Train.head(), X_Value.head(), X_Test.head()","898fc2c8":"#converting categorical to numeric data, one hot encoding\n\nX_Train = pd.get_dummies(X_Train)\nX_Value = pd.get_dummies(X_Value)\nX_Test  = pd.get_dummies(X_Test)\n\n#for sample, see our data\nX_Train.head()","855972be":"#let's align our data to ensure concurrency\n\nX_Train, X_Value = X_Train.align(X_Value, join='left', axis=1)\nX_Train, X_Test = X_Train.align(X_Test, join='left', axis=1)\n","d3291c39":"from xgboost import XGBRegressor","5ae6c417":"xgb =  XGBRegressor(n_estimators=1000, learning_rate=0.05)\nxgb.fit(X_Train, y_train)","075d357c":"y_pred = xgb.predict(X_Value)","44beab53":"#MAE\nfrom sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(y_pred, y_test)\nprint(\"mean_absolute_error : \", mae)","fcb61ba5":"prediction = xgb.predict(X_Test)","03fbe7dc":"output = pd.DataFrame({'Id': X_Test.index, 'SalePrice': prediction})\noutput.to_csv(\"submission.csv\", index=False)\noutput.head()","eb373492":"### Exploratory Data Analysis","09e907e2":"### Separating Data","8385d7cb":"so we have a lot of missing data to deal with that we'll just get back in second, first let's check the test data if it has any missing values.","031ab302":"### Modelling","798a3b20":"so we have 1168 rows for training and 292 rows for testing","0addb3fa":"\"Cardinality\" means the number of unique values in a column\nSelect categorical columns with relatively low cardinality (convenient but arbitrary)"}}