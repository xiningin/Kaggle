{"cell_type":{"c1b238d6":"code","96a4a57c":"code","99e3f3ca":"code","4947a43f":"code","d10f3a38":"code","3b302b4e":"code","7f933e53":"code","2a7d81d5":"code","cf97e952":"code","af6fc710":"code","01324812":"code","5926dea0":"code","2bc43879":"code","c433f6c3":"code","55943140":"code","c6caee9b":"code","c6f01a79":"code","7e8d6dce":"code","5a79a7a3":"code","9b2d25b2":"code","ab225a03":"code","f916194b":"code","989d8b8f":"code","ffeba7ed":"code","978c3cf9":"code","211fa5c7":"code","407b06f3":"code","5943729f":"code","5fc85603":"code","81f6bfe0":"code","1cbe2320":"code","847b242f":"code","9b5bc032":"code","26497201":"code","4bcd64d6":"code","9f16a51d":"code","a6b72945":"code","710f6f2d":"code","4a23676b":"code","00da218f":"code","c51d8ce3":"code","b78b035d":"code","2372b705":"code","813750f1":"code","41603cec":"code","9097ae9c":"code","98b294bb":"code","92e69b0d":"code","7087dc1e":"code","978bb501":"code","0283c6e6":"code","67a8019a":"code","fcc9667d":"code","e02276bd":"code","3ae7d683":"code","f01b9aac":"code","b026ee68":"code","7ff1be56":"code","fbe80a8f":"code","d326ae85":"code","82a4ca44":"code","169f0992":"code","15f1075f":"code","f928df5d":"code","ff382c21":"code","06c55310":"code","1403e05e":"code","4e04e77b":"markdown","eae30580":"markdown","875f1f4e":"markdown","38a1fa0d":"markdown","4ff4c4fc":"markdown","8e53046c":"markdown","dc1464d0":"markdown","bfd6831d":"markdown","74ea64c9":"markdown","5a352060":"markdown","8ba4e1d2":"markdown","72f08048":"markdown","8483e6c0":"markdown","6eb44b7e":"markdown","2e0b8a6e":"markdown","8df7b6c1":"markdown","8968355e":"markdown","320b0ffb":"markdown","7d1af820":"markdown","5117ba8f":"markdown","be3cbe90":"markdown","fedcc99a":"markdown","5f6f1838":"markdown","d177effd":"markdown","1e1b883f":"markdown","7c3afe47":"markdown","761056ec":"markdown","10a0485d":"markdown","8a936131":"markdown","ebe260c4":"markdown","16234365":"markdown","f3fe664a":"markdown","e0b2bc44":"markdown","fbb72983":"markdown","ebba076a":"markdown","a5ef60d8":"markdown","21dbeb63":"markdown","6294112e":"markdown","12eaaf0c":"markdown","ec90a87d":"markdown","81b42020":"markdown","6ade24f0":"markdown","86ce92fb":"markdown","b75b84e5":"markdown","19a1133b":"markdown"},"source":{"c1b238d6":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","96a4a57c":"# Import the numpy and pandas package\n\nimport numpy as np\nimport pandas as pd\n","99e3f3ca":"# Read the given CSV file, and view some sample records\n\nmedical = pd.read_csv('..\/input\/insurance\/insurance.csv')\nmedical.head()","4947a43f":"#Determining the number of rows and columns\nmedical.shape","d10f3a38":"medical.describe()  #summary of all the numeric columns in the dataset","3b302b4e":"medical.info()  #Datatypes of each column","7f933e53":"#Checking missing values\nmedical.isnull().sum()","2a7d81d5":"#Mapping\nmedical['sex'] = medical['sex'].map({'male': 0, 'female': 1})\nmedical['smoker'] = medical['smoker'].map({'yes': 1, 'no': 0})\nmedical.head()","cf97e952":"#Import necessary libraies\nimport matplotlib.pyplot as plt\nimport seaborn as sns","af6fc710":"#Binning the age column.\nbins = [17,35,55,1000]\nslots = ['Young adult','Senior Adult','Elder']\n\nmedical['Age_range']=pd.cut(medical['age'],bins=bins,labels=slots)","01324812":"medical.head()","5926dea0":"# I can check the number of unique values is a column\n# If the number of unique values <=40: Categorical column\n# If the number of unique values in a columns> 50: Continuous\n\nmedical.nunique().sort_values()","2bc43879":"#Pairplot of all numerical variables\nsns.pairplot(medical, vars=[\"age\", 'bmi','children','charges'],hue='smoker',palette=\"husl\")\nplt.show()","c433f6c3":"plt.figure(figsize=(25, 16))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'smoker', y = 'charges', data = medical)\nplt.title('Smoker vs Charges',fontweight=\"bold\", size=20)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'children', y = 'charges', data = medical,palette=\"husl\")\nplt.title('Children vs Charges',fontweight=\"bold\", size=20)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'sex', y = 'charges', data = medical, palette= 'husl')\nplt.title('Sex vs Charges',fontweight=\"bold\", size=20)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'region', y = 'charges', data = medical,palette=\"bright\")\nplt.title('Region vs Charges',fontweight=\"bold\", size=20)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'Age_range', y = 'charges', data = medical, palette= 'husl')\nplt.title('Age vs Charges',fontweight=\"bold\", size=20)\nplt.show()\n","55943140":"plt.figure(figsize=(12,6))\nsns.barplot(x='region', y='charges', hue='sex', data=medical, palette='Paired')\nplt.show()","c6caee9b":"plt.figure(figsize=(12,6))\nsns.barplot(x = 'region', y = 'charges',hue='smoker', data=medical, palette='cool')\nplt.show()","c6f01a79":"plt.figure(figsize=(12,6))\nsns.barplot(x='region', y='charges', hue='children', data=medical, palette='Set1')\nplt.show()","7e8d6dce":"plt.figure(figsize=(12,6))\nsns.violinplot(x = 'children', y = 'charges', data=medical, hue='smoker', palette='inferno')\nplt.show()","5a79a7a3":"#Heatmap to see correlation between variables\nplt.figure(figsize=(12, 8))\nsns.heatmap(medical.corr(), cmap='RdYlGn', annot = True)\nplt.title(\"Correlation between Variables\")\nplt.show()","9b2d25b2":"medical.head()","ab225a03":"# # Get the dummy variables for region and age range\nregion=pd.get_dummies(medical.region,drop_first=True)\nAge_range=pd.get_dummies(medical.Age_range,drop_first=True)\nchildren= pd.get_dummies(medical.children,drop_first=True,prefix='children')\n","f916194b":"# Add the results to the original bike dataframe\nmedical=pd.concat([region,Age_range,children,medical],axis=1)\nmedical.head()","989d8b8f":"#Drop region and age range as we are created a dummy\nmedical.drop(['region', 'Age_range', 'age','children'], axis = 1, inplace = True)\nmedical.head()","ffeba7ed":"# Now lets see the number of rows and columns\nmedical.shape","978c3cf9":"#Now lets check the correlation between variables again\n#Heatmap to see correlation between variables\nplt.figure(figsize=(15, 10))\nsns.heatmap(medical.corr(), cmap='YlGnBu', annot = True)\nplt.show()","211fa5c7":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\n#np.random.seed(0)\nmedical_train, medical_test = train_test_split(medical, train_size = 0.7, random_state = 100)","407b06f3":"print(medical_train.shape)\nprint(medical_test.shape)","5943729f":"from sklearn.preprocessing import MinMaxScaler","5fc85603":"medical.head()","81f6bfe0":"#Instantiate an object\nscaler = MinMaxScaler()\n\n#Create a list of numeric variables\nnum_vars=['bmi','charges']\n\n#Fit on data\nmedical_train[num_vars] = scaler.fit_transform(medical_train[num_vars])\nmedical_train.head()","1cbe2320":"#Divide the data into X and y\ny_train = medical_train.pop('charges')\nX_train = medical_train","847b242f":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n","9b5bc032":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 8)             # running RFE\nrfe = rfe.fit(X_train, y_train)","26497201":"#List of variables selected\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","4bcd64d6":"#Columns where RFE support is True\ncol = X_train.columns[rfe.support_]\ncol","9f16a51d":"#Columns where RFE support is False\nX_train.columns[~rfe.support_]","a6b72945":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]\n","710f6f2d":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)\n","4a23676b":"# Running the linear model \nlm = sm.OLS(y_train,X_train_rfe).fit()","00da218f":"print(lm.summary())","c51d8ce3":"#Drop the constant term B0\nX_train_rfe = X_train_rfe.drop(['const'], axis=1)","b78b035d":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2372b705":"#Drop children_5\nX_train_new1 = X_train_rfe.drop([\"children_5\"], axis = 1)\n","813750f1":"#Build a model\nX_train_lm1 = sm.add_constant(X_train_new1)\nlm1 = sm.OLS(y_train,X_train_lm1).fit()\nprint(lm1.summary())","41603cec":"#Drop the constant term B0\nX_train_lm1 = X_train_lm1.drop(['const'], axis=1)","9097ae9c":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_lm1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","98b294bb":"X_train_new2 = X_train_lm1.drop(['children_4'], axis=1)","92e69b0d":"#Build a model\nX_train_lm2 = sm.add_constant(X_train_new2)\nlm2 = sm.OLS(y_train,X_train_lm2).fit()\nprint(lm2.summary())","7087dc1e":"#Drop the constant term B0\nX_train_lm2 = X_train_lm2.drop(['const'], axis=1)","978bb501":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_lm2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0283c6e6":"X_train_new3 = X_train_lm2.drop(['children_3'], axis=1)","67a8019a":"#Build a model\nX_train_lm3 = sm.add_constant(X_train_new3)\nlm3 = sm.OLS(y_train,X_train_lm3).fit()\nprint(lm3.summary())","fcc9667d":"#Drop the constant term B0\nX_train_lm3 = X_train_lm3.drop(['const'], axis=1)","e02276bd":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_lm3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3ae7d683":"X_train_lm3=sm.add_constant(X_train_lm3)\nX_train_lm3.head()","f01b9aac":"#y train predicted\ny_train_pred = lm3.predict(X_train_lm3)","b026ee68":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","7ff1be56":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nplt.figure(figsize=(14,7))\nsns.distplot((y_train - y_train_pred), bins = 20)\nplt.title('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)  # X-label\nplt.show()","fbe80a8f":"#Create a list of numeric variables\nnum_vars=num_vars=['bmi','charges']\n\n#Fit on data\nmedical_test[num_vars] = scaler.transform(medical_test[num_vars])\nmedical_test.head()","d326ae85":"#Dividing into X_test and y_test\ny_test = medical_test.pop('charges')\nX_test = medical_test\nX_test.describe()","82a4ca44":"X_train_new3.columns","169f0992":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new3.columns]\n\n# Adding a constant variable \nX_test_new1 = sm.add_constant(X_test_new)\nX_test_new1.head()","15f1075f":"# Making predictions\ny_pred = lm3.predict(X_test_new1)\n","f928df5d":"#Evaluate R-square for test\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred)","ff382c21":"#Adjusted R^2\n#adj r2=1-(1-R2)*(n-1)\/(n-p-1)\n\n#n =sample size , p = number of independent variables\nn = X_test.shape[0]\np = X_test.shape[1]\n\n\nAdj_r2=1-(1-0.75783003115855)*(n-1)\/(n-p-1)\nprint(Adj_r2)","06c55310":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.figure(figsize=(15,8))\nplt.scatter(y_test,y_pred,color='blue')\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)     # Y-label\nplt.show()","1403e05e":"#Regression plot\nplt.figure(figsize=(14,8))\nsns.regplot(x=y_test, y=y_pred, ci=68, fit_reg=True,scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\nplt.show()","4e04e77b":"### Visualising Categorical Variables\n\nAs you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.","eae30580":"#  Reading and Understanding the Data\n\nLet's start with the following steps:\n\n1. Importing data using the pandas library\n2. Understanding the structure of the data","875f1f4e":"## Results: \n\n* From the regression analysis, we \ufb01nd that region and gender do not bring signi\ufb01cant difference on charges.\n\n* Age, BMI, number of children and smoking are the ones that drive the charges\n\n* Smoking seems to have themost in\ufb02uence on the medical charges","38a1fa0d":"##### Rebuilding the model","4ff4c4fc":"### Finding R-squared and Adjusted R-Squared for Test set","8e53046c":"From above plot we can see that, Smoking has the highest impact on medical costs, even though the costs are growing with age, bmi and children. Also people who have children generally smoke less","dc1464d0":"### Rescaling the Features","bfd6831d":"#### Recursive Feature Elimination","74ea64c9":"#### Applying the scaling on the test sets","5a352060":"#### Min- Max scaling","8ba4e1d2":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","72f08048":"##### Lets drop children_4 due to its p value","8483e6c0":"**We can see Error terms are normally distributed**","6eb44b7e":"## Step 8: Model Evaluation","2e0b8a6e":"### Let's inspect the various aspects of our dataframe","8df7b6c1":"Notice that sex and smoker are categorical features with 2 different values each? Let's convert them to a numerical feature--to be specific boolean values. Using the map function, we'll set these values:\n\nMale = 0; Female = 1\nNon-somker = 0; Smoker = 1","8968355e":"### Heatmap","320b0ffb":"* ### No missing values in the dataset","7d1af820":" ##### Taking into account certain factors (sex, smoking, having children) let's see how it changes by region","5117ba8f":"\n### We can see that the equation of our best fitted line is:\n\n### $ charges=         0.3826\\times smoker+   0.077   \\times Senior adult + 0.149 \\times Elder + 0.176 \\times bmi + 0.024 \\times children_2  $","be3cbe90":"## Interpretation\n\n* We got all the p values within the acceptable range. A low p-value (< 0.05) indicates that you can reject the null hypothesis. And all the p values we got are less than 0.05.\n\n* A rule of thumb commonly used in practice is if a VIF is > 10, you have high multicollinearity. In our case, with values less than 5, we are in good shape, and can proceed with our regression.\n\n* R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 \u2013 100% scale.\n\n* The adjusted R-squared adjusts for the number of terms in the model.\n\n* Also we can see in the residual analysis, the error terms are normally distributed and centered at 0, which indicates that the models predictions are correct.\n\n* We can also see the coefficient of smoker is more 0.386 which means, if the smoker increases by one unit the medical charges increases by 0.386 units.\n\n* bmi and elder people also have considerably high coefficients","fedcc99a":"#### Dividing into X_test and y_test\n","5f6f1838":"## Step 4: Splitting the Data into Training and Testing Sets","d177effd":"## Step 5: Building a Linear Model","1e1b883f":"### Now our model is good with p-values and VIF under the acceptable range","7c3afe47":"## Step 2: Data Visualization","761056ec":"##### Visualising the fit on the test set","10a0485d":"## Step 7: Making Predictions","8a936131":"## Step 3: Data Preparation","ebe260c4":"As we can see from these barplots the highest charges due to smoking are still in the Southeast but the lowest are in the Northeast. People in the Southwest generally smoke more than people in the Northeast, but people in the Northeast have higher charges by gender than in the Southwest and Northwest overall. And people with children tend to have higher medical costs overall as well","16234365":"##### From the plot above we can see that.\n\n#### 1. Medical Charges are more for smoker than the non smoker.\n\n#### 2. Medical Charges are more in Southeast Region\n\n#### 3. Senior Adults are charged more","f3fe664a":"# Introduction\n\nMedical expenses is one of the major recurring expenses in a human life. Its a common knowledge that one life style and various physical parameters dictates diseases or ailments one can have and these ailments dictates medical expanses. According various studies, major factors that contribute to higher expenses in personal medical care include smoking, aging, BMI. In this study, we aims to find a correlation between personal medical expenses and different factors, and compare them. Then we use the prominent attributes as predictors to predict medical expenses by creating linear regression models.","e0b2bc44":"##### Lets drop children 3","fbb72983":"#### Now lets drop the variable having high p value. We can see children_5 has high p value.","ebba076a":"By analysing all the plots above, we can see that,\n- A trend of increasing charges can be observed from the age vs charge plot. \n- BMI seems to hold some correlation to charges. \n- There seems to be a strong correlation between smoking and medical charges","a5ef60d8":"![image.png](attachment:image.png)","21dbeb63":"### Identify Continuous and Categorical Variables","6294112e":"### Dividing into X and Y sets for the model building","12eaaf0c":"* Train R^2 :  0.723\n\n* Train Adjusted R^2 : 0.722\n\n* Test R^2: 0.762\n\n* Test Adjusted R^2: 0.749\n\n* Difference in R^2 between train and test: 3.9%\n\n* Difference in adjusted R^2 between Train and test:  2.7 % which is less than 5%\n\n## Yes! Its a best model\n\n","ec90a87d":"## Step 6: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","81b42020":"### RFE\n","6ade24f0":"### Building model using statsmodel, for the detailed statistics","86ce92fb":"# Problem Statement\n\nFor a health insurance company to make money, it needs to collect more in yearly premiums than it spends on medical care to its beneficiaries. As a result, insurers invest a great deal of time and money in developing models that accurately forecast medical expenses for the insured population.\n\nMedical expenses are difficult to estimate because the most costly conditions are rare and seemingly random. Still, some conditions are more prevalent for certain segments of the population. For instance, lung cancer is more likely among smokers than non-smokers, and heart disease may be more likely among the obese.\n\n\nThe goal of this analysis is to use patient data to estimate the average medical care expenses for such population segments. These estimates can be used to create actuarial tables that set the price of yearly premiums higher or lower, depending on the expected treatment costs.","b75b84e5":"### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables,  to visualise which variables are most correlated to the target variable 'charges'.","19a1133b":"# Final Result Comparison between Train model and Test: "}}