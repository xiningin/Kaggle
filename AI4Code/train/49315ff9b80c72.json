{"cell_type":{"e926f001":"code","08c8546c":"code","b655fcf2":"code","c738a513":"code","94ed28c1":"code","99a90041":"code","da94008a":"code","f41a1423":"code","dfcd8b4a":"code","88b2e349":"code","e31ac64a":"code","656c0ca6":"code","1ad76ba2":"code","d3b90d42":"code","461a2dba":"code","cead3a9c":"code","d4fcaee4":"code","a321f4c4":"code","fac17bd2":"code","1ae1856b":"code","194c6ee4":"code","f9b355bd":"code","3a5b9dde":"code","083e8621":"code","b211e82c":"code","429eb11c":"code","f7654584":"code","e634b53b":"code","9fcb0156":"code","648c437e":"code","c26c8ee3":"code","5ede2597":"code","8b8cd9b3":"code","a2fbb9f2":"markdown","e5b86116":"markdown","6989f8f2":"markdown","be031a4f":"markdown","81156733":"markdown","455f30af":"markdown","f83c552f":"markdown","6da40844":"markdown","18e0fde0":"markdown","6aef3bf2":"markdown","de614016":"markdown","26f8d30a":"markdown","bc43bd5a":"markdown","7baafc98":"markdown","ad0a1ecf":"markdown","2840af65":"markdown","d7cd0d9b":"markdown","7297cfc1":"markdown","18814bff":"markdown","a957d919":"markdown","836a951e":"markdown"},"source":{"e926f001":"import pandas as pd\nimport numpy as np\nimport ast\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","08c8546c":"#~~~HELPER FUNCTIONS~~~\n\n'''The fix_column function uses the str_repl function to \nclean the columns that have values with intervals'''\ndef str_repl(j, str2find, str2repl, n):\n    find = j.find(str2find)\n    i = find != -1\n    while find != -1 and i != n:\n        find = j.find(str2find, find + 1)\n        i += 1\n    if i == n:\n        return j[:find] + str2repl + j[find+len(str2find):]\n    return j\n\ndef fix_column(data:pd.DataFrame,col:str):\n    y = []\n    for j in data[col]:\n        try:\n            j = str_repl(j,\"[\",\"]\",2)\n        except AttributeError:\n            j = j\n        y.append(j)\n    return y\n\n'''get average of features with interval values'''\ndef agg_func(data:pd.DataFrame,col:str)->None:\n    x = []\n    for i in data[col]:\n        try:\n            new = ast.literal_eval(i) \n            val = np.mean(new)\n        except:\n            val = i\n        x.append(val)\n    return x\n\n'''checks null values and proportion of null values per feature'''\ndef check_proportion_null(df:pd.DataFrame):\n    dff = pd.DataFrame()\n    proportion = []\n    summing = []\n    for i in df.columns:\n        summ = df[i].isnull().sum()\n        summing.append(summ)\n        missing =  summ\/(df.shape[0])*100\n        missing = missing.round(2).astype(str) + \"%\"   \n        proportion.append(missing)\n    dff['columns'] = df.columns.to_list()\n    dff['sum_null_values'] = summing\n    dff['proportion_null_values'] = proportion\n    return dff","b655fcf2":"district = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\ncols = ['pct_black\/hispanic','pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw']\nfor i in cols:\n    district[i] = fix_column(district,i)\n\ndistrict['pct_black\/hispanic'] = agg_func(district,'pct_black\/hispanic')\ndistrict['pct_free\/reduced'] = agg_func(district,'pct_free\/reduced')\ndistrict['pp_total_raw'] = agg_func(district,'pp_total_raw')\ndistrict.head()","c738a513":"check_proportion_null(district)","94ed28c1":"'''district_id is the only column that doesn't have missing values.\nWe create a new df without the district_id column and check whether there are rows that have missing values\nin all remaining columns. We drop these rows from our district data'''\ndistrict_df = district[district.columns.drop('district_id')]\n\ndistrict = district[~district_df.isnull().all(axis=1)]\ndistrict.head()","99a90041":"check_proportion_null(district)","da94008a":"product = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")\nproduct.rename(columns = {'LP ID':'lp_id'},inplace = True)\n\n'''creating a new column with the 3 main categories of Primary Essential Functions; LC,CM,SDO'''\nz = []\nfor i in product['Primary Essential Function']:\n    try:\n        i = i.split('-')[0]\n    except:\n        i = i\n    z.append(i)\n    \nproduct['Essential_category'] = z\nproduct.head()","f41a1423":"check_proportion_null(product)","dfcd8b4a":"schools = []\nfor i, (dirpath, dirnames, filenames) in enumerate(os.walk(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\")):\n    for file in filenames:\n        f = dirpath + \"\/\"+file\n        data = pd.read_csv(f,index_col=False)\n        district_id = file.replace(\".csv\",\"\")\n        data['district_id'] = district_id\n        schools.append(data)\n\nengagement = pd.concat(schools)\nengagement = engagement.reset_index(drop=True)\nengagement['time'] = pd.to_datetime(engagement['time'])\nengagement['day'] = engagement['time'].dt.day_name()\nengagement['month'] = engagement['time'].dt.month_name()\nengagement.head()","88b2e349":"check_proportion_null(engagement)","e31ac64a":"'''drop all rows where engagement index is null, because a propotion > 20% could skew the data \nand also because we have about 21 million rows of data'''\nengagement = engagement[engagement['engagement_index'].notna()]\n\"\"\"we drop the remainig rows where lp_id is null as its a unique product identifier\"\"\"\nengagement = engagement[engagement['lp_id'].notna()]\ncheck_proportion_null(engagement)","656c0ca6":"'''This step involves merging the 3 dataframes i.e. district,product and engagement to one, \nbased on columns LP ID (product and engagement) and district_id (district and engagement)'''\ndf = pd.merge(product,engagement,left_on = 'lp_id',right_on = 'lp_id')\ndistrict['district_id'] = district['district_id'].astype(str)\ndata = pd.merge(df,district,left_on = 'district_id', right_on = 'district_id')\ndata.rename(columns={'Product Name':'Product_Name','Provider\/Company Name':'Provider_Name'},inplace=True)\ndata = data[~data['county_connections_ratio'].isnull()]\n#data.to_csv('merged.csv',index=False)\ndata.head()","1ad76ba2":"#~~~Plotting functions~~~\n#pie chart plot\ndef plot_pie(data:pd.DataFrame,col:str,n:int)->None:\n    data[col].value_counts().plot(kind = 'pie',autopct = '%1.1f%%',explode=[0.05]*n,\n                                colormap = 'summer', figsize = (10,10)).legend()\n    plt.title(f'Distribution of {col}',size=15)\n    plt.show()\n    \n#distribution plot\ndef plot_distribution(data:pd.DataFrame,col1:str,col2:str)->None:\n    dist = data.groupby([col1,col2]).size().reset_index().pivot(columns = col2,index = col1,values=0)\n    dist.plot(kind = 'bar',stacked = True,colormap = 'RdYlGn')\n    plt.show()\n\ndef count_plot(data:pd.DataFrame,col:str):\n    sns.countplot(y=col,data=data,order=data[col].value_counts().index,palette=\"RdYlGn\",linewidth=3)\n    plt.title(f'Count of {col}', size=15)\n    plt.show()\n\ndef top_n(data:pd.DataFrame,col:str,n:int):\n    sns.countplot(y=col,data = data, order=data[col].value_counts().head(n).index,palette='RdYlGn',linewidth=3)\n    plt.title(f'{col} top {n}',size = 15)\n    plt.show()\n    \ndef plot_main(data:pd.DataFrame,col:str,labels:list,n)->None:\n    values = []\n    for i in labels:\n        x = data[col].str.count(i).sum()\n        values.append(x)\n\n    plt.figure(figsize = (10,10))    \n    plt.pie(values,labels = labels,explode = [0.05]*n,autopct = '%1.1f%%',pctdistance=0.85)\n    centre_circle = plt.Circle((0,0),0.5,fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n    plt.title(f'{col} distribution',size=15)\n    plt.legend()\n    plt.show()\n\ndef plot_bar(data:pd.DataFrame,col1:str,col2:str):\n    df=data.sort_values(by=col1,ascending=False)\n    plt.figure(figsize=(12,6))\n    plt.subplot(1,2,1)\n    ax=sns.barplot(df.index,df[col1],palette='RdYlGn',dodge=False)\n    ax.set_xticklabels(df.index,rotation=80)\n    plt.title(f'Distribution of {col1}')\n    \n    plt.subplot(1,2,2)\n    ax=sns.barplot(df.index,df[col2],palette='RdYlBu',dodge=False)\n    ax.set_xticklabels(df.index,rotation=80)\n    plt.title(f'Distribution of {col2}')\n    plt.show()","d3b90d42":"dist = district.groupby('locale').agg({'state':'count','pct_black\/hispanic':'mean','pct_free\/reduced':'mean','pp_total_raw':'mean'}).reset_index()\ndist.rename(columns={'state':'#school_districts_per_locale','pct_black\/hispanic':'pct_black\/hispanic_mean',\n                     'pct_free\/reduced':'pct_free\/reduced_mean',},inplace=True)\n\nfig,ax = plt.subplots(2,2,sharey=True,figsize = (15,10))\nax = ax.flatten()\n    \ng1 = sns.barplot(dist['#school_districts_per_locale'],dist['locale'],palette='RdYlGn',dodge=False,orient = 'h',ax=ax[0])\ng1.set(ylabel = None)\ng1.set(title = (f'No of school districts per locale'))\n\ng2 = sns.barplot(dist['pct_black\/hispanic_mean'],dist['locale'],palette='RdYlBu',dodge=False,orient = 'h',ax=ax[1])\ng2.set(ylabel = None)\ng2.set(title = (f'% of students that identify as black or hispanic per locale'))\n    \ng3 = sns.barplot(dist['pct_free\/reduced_mean'],dist['locale'],palette='RdYlBu',dodge=False,orient = 'h',ax=ax[2])\ng3.set(ylabel = None)\ng3.set(title = (f'% of students eligible for free or reduced-price lunch per locale'))\n    \ng4 = sns.barplot(dist['pp_total_raw'],dist['locale'],palette='RdYlGn',dodge=False,orient = 'h',ax=ax[3])\ng4.set(ylabel = None)\ng4.set(title = (f'Per-pupil total expenditure (sum of local and federal expenditure) per locale'))\n    \nfig.text(0.04, 0.5, 'Locale', va='center', rotation='vertical',size=15)\nplt.show()","461a2dba":"plot_pie(district,'locale',4)","cead3a9c":"count_plot(district,\"locale\")","d4fcaee4":"count_plot(district,'state')","a321f4c4":"#distribution of locales per state\nplot_distribution(district,'state','locale')","fac17bd2":"cols = ['Product Name','Provider\/Company Name','Essential_category','Primary Essential Function']\nfor i in cols:\n    print(f'Number of unique values in product feature {i}:',product[i].nunique())","1ae1856b":"#Plot only the 3 main sectors for better analysis\nlabels = ['PreK-12','Higher Ed','Corporate']\nplot_main(product,'Sector(s)',labels,3)","194c6ee4":"#Top Providers\ntop_n(product,'Provider\/Company Name',10)","f9b355bd":"top_n(product,'Primary Essential Function',15)","3a5b9dde":"plot_pie(product,'Essential_category',4)","083e8621":"time_series = data.groupby('time').agg({'engagement_index':'mean','pct_access':'mean'}).reset_index()\ntime_series.set_index('time',inplace=True)\ncols_plot = ['engagement_index','pct_access']\naxes = time_series[cols_plot].plot(alpha=0.8, linestyle='solid',\n                          title='Time Series plot of Engagement index and % access',grid=True,\n                          figsize=(10,8), subplots=True, sharey=False,legend=False)\n[ax.legend(loc=1) for ax in plt.gcf().axes]\nplt.show()","b211e82c":"col1_df = data[data.Product_Name.isin(['Google Docs'])]\ncol2_df = data[data.Product_Name.isin(['Google Classroom'])]\ncol3_df = data[data.Product_Name.isin(['YouTube'])]\ncol4_df = data[data.Product_Name.isin(['Canvas'])]\ncol5_df = data[data.Product_Name.isin(['Schoology'])]\n\ncol1_dff = col1_df.groupby('time')['engagement_index'].mean().reset_index()\ncol1_dff.rename(columns = {'engagement_index':'Google Docs'},inplace=True)\ncol2_dff = col2_df.groupby('time')['engagement_index'].mean().reset_index()\ncol2_dff.rename(columns = {'engagement_index':'Google Classroom'},inplace=True)\ncol3_dff = col3_df.groupby('time')['engagement_index'].mean().reset_index()\ncol3_dff.rename(columns = {'engagement_index':'YouTube'},inplace=True)\ncol4_dff = col4_df.groupby('time')['engagement_index'].mean().reset_index()\ncol4_dff.rename(columns = {'engagement_index':'Canvas'},inplace=True)\ncol5_dff = col5_df.groupby('time')['engagement_index'].mean().reset_index()\ncol5_dff.rename(columns = {'engagement_index':'Schoology'},inplace=True)","429eb11c":"dff = col1_dff.merge(col2_dff, how='left', left_on='time', right_on='time').merge(col3_dff, how='left', left_on='time', right_on='time').merge(\n    col4_dff, how='left', left_on='time', right_on='time').merge(col5_dff, how='left', left_on='time', right_on='time')\ndff.set_index('time',inplace=True)\ndff.plot()","f7654584":"plt.figure(figsize=(12,6))\n\ng  = sns.lineplot(data=dff,palette = 'mako_r')\ng.legend(loc = 'center right',bbox_to_anchor = (1.25,0.5),ncol=1,title = 'Learning Tool',)\nplt.show()","e634b53b":"def time_series_plot(df:pd.DataFrame,col1:str,col2:str,col3:str,col4:str,col5:str):\n    col1_df = df[df.Product_Name.isin([col1])]\n    col2_df = df[df.Product_Name.isin([col2])]\n    col3_df = df[df.Product_Name.isin([col3])]\n    col4_df = df[df.Product_Name.isin([col4])]\n    col5_df = df[df.Product_Name.isin([col5])]\n    \n    col1_dff = col1_df.groupby('time')['engagement_index'].mean().reset_index()\n    col1_dff.rename(columns = {'engagement_index':f'{col1}'},inplace=True)\n    col2_dff = col2_df.groupby('time')['engagement_index'].mean().reset_index()\n    col2_dff.rename(columns = {'engagement_index':f'{col2}'},inplace=True)\n    col3_dff = col3_df.groupby('time')['engagement_index'].mean().reset_index()\n    col3_dff.rename(columns = {'engagement_index':f'{col3}'},inplace=True)\n    col4_dff = col4_df.groupby('time')['engagement_index'].mean().reset_index()\n    col4_dff.rename(columns = {'engagement_index':f'{col4}'},inplace=True)\n    col5_dff = col5_df.groupby('time')['engagement_index'].mean().reset_index()\n    col5_dff.rename(columns = {'engagement_index':f'{col5}'},inplace=True)\n    \n    dff = col1_dff.merge(col2_dff, how='left', left_on='time', right_on='time').merge(col3_dff, how='left', left_on='time', right_on='time').merge(\n    col4_dff, how='left', left_on='time', right_on='time').merge(col5_dff, how='left', left_on='time', right_on='time')\n    dff.set_index('time',inplace=True)\n    \n    plt.figure(figsize=(12,6))\n    g  = sns.lineplot(data=dff,palette = 'mako_r')\n    g.legend(loc = 'center right',bbox_to_anchor = (1.25,0.5),ncol=1,title = 'Learning Tool')\n    g.set(title='Engagement index of top 5 learning tools in 2020')\n    plt.show()","9fcb0156":"time_series_plot(data,'Google Docs','Google Classroom','YouTube','Canvas','Schoology')","648c437e":"'''get top 10 tools overall per engagement index mean'''\ntop_products = data.groupby('Product_Name')['engagement_index'].mean().reset_index()\ntop_products.sort_values(by='engagement_index',ascending=False).head(10)","c26c8ee3":"def page_load_states(col1:str,col2:str,col3:str,col4:str):\n    df = data.groupby(['state','Product_Name'])['engagement_index'].mean().reset_index()\n    #df['engagement_index'] = df['engagement_index'].round(2)\n    df = df[df['Product_Name'].isin(['Google Docs','Google Classroom','YouTube','Canvas','Schoology',\n                               'Meet','Kahoot','YouTube','Google Forms','Google Drive','Seesaw : The Learning Journal'])]\n    df = df.sort_values(by='engagement_index',ascending=False)\n    \n    fig,ax = plt.subplots(2,2,sharey=True,figsize = (15,10))\n    ax = ax.flatten()\n    \n    x = df[df.state.isin([col1])]\n    g1 = sns.barplot(x.engagement_index,x.Product_Name,palette='RdYlGn',dodge=False,orient = 'h',ax=ax[0])\n    g1.set(xlabel = None)\n    g1.set(ylabel = None)\n    g1.set(title = (f'Average engagement index of the top 10 tools in {col1} state'))\n    \n    y = df[df.state.isin([col2])]\n    g2 = sns.barplot(y.engagement_index,y.Product_Name,palette='RdYlBu',dodge=False,orient = 'h',ax=ax[1])\n    g2.set(xlabel = None)\n    g2.set(ylabel = None)\n    g2.set(title = (f'Average engagement index of the top 10 tools in {col2} state'))\n    \n    xy = df[df.state.isin([col3])]\n    g3 = sns.barplot(xy.engagement_index,xy.Product_Name,palette='RdYlBu',dodge=False,orient = 'h',ax=ax[2])\n    g3.set(xlabel = None)\n    g3.set(ylabel = None)\n    g3.set(title = (f'Average engagement index of the top 10 tools in {col3} state'))\n    \n    z = df[df.state.isin([col4])]\n    g4 = sns.barplot(z.engagement_index,z.Product_Name,palette='RdYlGn',dodge=False,orient = 'h',ax=ax[3])\n    g4.set(xlabel = None)\n    g4.set(ylabel = None)\n    g4.set(title = (f'Average engagement index of the top 10 tools in {col4} state'))\n    \n    fig.text(0.5, 0.04, 'Engagement index',ha='center',size=15)\n    plt.show()\n    \n\npage_load_states('Illinois', 'Utah', 'Wisconsin', 'North Carolina')\npage_load_states('Missouri','Washington', 'Connecticut', 'Massachusetts')","5ede2597":"filtered = data.groupby(['Product_Name','county_connections_ratio'])['engagement_index'].mean().reset_index()\nfiltered = filtered[filtered['Product_Name'].isin(['Google Docs','Google Classroom','YouTube','Canvas','Schoology',\n                               'Meet','Kahoot','YouTube','Google Forms','Google Drive','Seesaw : The Learning Journal'])]\nfiltered = filtered.sort_values(by='engagement_index',ascending=False)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nsns.barplot(x=\"locale\", y=\"engagement_index\",hue = \"county_connections_ratio\" ,data=data,palette = 'rainbow')\nplt.title('Average engagement index per locale given county connections ratio')\n\nplt.subplot(1,2,2)\nsns.barplot(x=\"Product_Name\", y=\"engagement_index\",hue = \"county_connections_ratio\" ,data=filtered,palette = 'summer_r')\nplt.xticks(rotation=80)\nplt.title('Average engagement index per top learning products given county connections ratio')\nplt.show()","8b8cd9b3":"months = data.groupby('month').agg({'pct_access':'mean','engagement_index':'mean'})\ndays = data.groupby('day').agg({'pct_access':'mean','engagement_index':'mean'})\nplot_bar(months,'pct_access','engagement_index')\nplot_bar(days,'pct_access','engagement_index')","a2fbb9f2":"***Most of the school districts are located in the suburbs.***","e5b86116":"The Learning & Curriculum category has the most product tools","6989f8f2":"<a id=\"subsection-three\"><\/a>\n## Merged data\ud83d\udfe2\n\n<a id=\"subsection-three-one\"><\/a>\n### Time series plot of engagement index and % access\ud83d\udcc9  ","be031a4f":"<a id=\"subsection-one\"><\/a>\n## Districts\ud83d\udd35","81156733":"* The engagement index represents the ***mean of the total page-load events per one thousand students of a given product and on a given day***\n* The states were selected by order of occurrence in the district dataframe. \n* Of these 8 states, google docs was the most popular learning tool.\n* Illionois had the highest average engagement index with google docs having an average of approximately 17000 page-load events per 1000 students\n* North Carolina State had the lowest engagement index, with google docs having an average of approximately 3500 page-load events per 1000 students","455f30af":"* Suburbs have the highest number of school districts but have a lower % of students that identify as   black or hispanic, especially when compared to Cities.\n* Cities have the highest % of students eligible for free or reduced-price lunch per locale and have  a considerably high per pupil total expenditure","f83c552f":"# LearnPlatform\ud83c\udfc6: COVID-19\ud83d\ude37 Impact on Digital Learning \n\nThe challenge involves exploring:\n* the state of digital learning in 2020 and \n* how the engagement of digital learning relates to factors such as district demographics, broadband access, and state\/national level policies and events.\n\n**Table of Contents**\n\n* [Loading packages\ud83d\udcda](#section-one)\n* [Loading the data\ud83d\udcd8\ud83d\udcd7\ud83d\udcd2](#section-two)\n* [Merge data\ud83d\udcc1](#section-three)\n* [Exploratory Data Analysis\ud83d\udcca](#section-four)\n    * [Districts \ud83d\udd35](#subsection-one)\n    * [Products \ud83d\udfe1](#subsection-two)\n    * [Merged data\ud83d\udfe2](#subsection-three)\n        * [Time series plot of engagement index and % access\ud83d\udcc9](#subsection-three-one)\n        * [Engagement vs district demographics\ud83d\udcc8](#subsection-three-two)\n        * [Engagement vs internet access\ud83d\udcc9](#subsection-three-three)","6da40844":"<a id=\"section-three\"><\/a>\n# Merge Files\ud83d\udcc1","18e0fde0":"Primary Essential Function  represents the basic function of the product. \nProducts are first labeled as one of these three categories: \n* LC = Learning & Curriculum\n* CM = Classroom Management\n* SDO = School & District Operations.","6aef3bf2":"<a id=\"section-four\"><\/a>\n# Exploratory Data Analysis\ud83d\udcca","de614016":"* The county connection ratio represents the ratio of residential fixed high-speed connections over 200 kbps in at least one direction\/households\n* Only school districts in the rural locale have county connection ratio of range [1,2], as seen in the catplot\n* Google Classroom has the highest average engagement index of around 4500 for school districts that have county connection ratio of range [1,2]","26f8d30a":"<a id=\"section-one\"><\/a>\n# Loading packages\ud83d\udcda","bc43bd5a":"<a id=\"section-two\"><\/a>\n# Loading the data \ud83d\udcd8\ud83d\udcd7\ud83d\udcd2","7baafc98":"* pct_access represents the % of students in the district that have at least one page-load event of a given product and on a given day have at least one page-load event of a given product and on a given day\n* engagement_index represents the total page-load events per one thousand students of a given product and on a given day\n* COVID 19 was declared a pandemic by WHO in March, as a result, schools began to close so as to curb the spread of the virus\n* In the United States, the summer holiday in 2020 began on 20 June and ended on 22 September. This explains why the engagement index and % access dropped during summer as students were on a break from learning. \n","ad0a1ecf":"<a id=\"subsection-three-three\"><\/a>\n### Engagement vs internet access\ud83d\udcc9","2840af65":"**District Information data**\n\nThe district file districts_info.csv includes information about the characteristics of school districts, including data from NCES (2018-19), FCC (Dec 2018), and Edunomics Lab. In this data set, we removed the identifiable information about the school districts. We also used an open source tool ARX (Prasser et al. 2020) to transform several data fields and reduce the risks of re-identification. For data generalization purposes some data points are released with a range where the actual value falls under. ***Additionally, there are many missing data marked as 'NaN' indicating that the data was suppressed to maximize anonymization of the dataset.***\n\n|Name | Description |\n| --- | --- |\n|district_id | The unique identifier of the school district |\n|state | The state where the district resides in |\n|locale | NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See Locale Boundaries User's Manual for more information. |\n|pct_black\/hispanic | Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data |\n|pct_free\/reduced | Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data |\n|countyconnectionsratio | ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version). See FCC data for more information. |\n|pptotalraw | Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district. |","d7cd0d9b":"The most occurence of school districts per locale per state are:\n* Connecticut and Utah, for Suburbs\n* Connecticut, for Rurals\n* California, for Cities and \n* Utah, for towns","7297cfc1":"<a id=\"subsection-three-two\"><\/a>\n### Engagement vs district demographics\ud83d\udcc8","18814bff":"<a id=\"subsection-two\"><\/a>\n## Products\ud83d\udfe1","a957d919":"**Engagement data**\n\nThe engagement data are aggregated at school district level, and each file in the folder engagement_data represents data from one school district. The 4-digit file name represents **district_id which can be used to link to district information in district_info.csv.** The lp_id can be used to link to product information in product_info.csv.\n\n| Name | Description |\n| --- | --- |\n|time | date in \"YYYY-MM-DD\"|\n|lp_id | The unique identifier of the product |\n|pct_access |Percentage of students in the district have at least one page-load event of a given product and on a given day |\n|engagement_index | Total page-load events per one thousand students of a given product and on a given day |","836a951e":"**Product information data**\n\nThe product file products_info.csv includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy. Data were labeled by our team. Some products may not have labels due to being duplicate, lack of accurate url or other reasons.\n\n|Name | Description | \n| --- | --- |\n|LP ID | The unique identifier of the product |\n|URL | Web Link to the specific product |\n|Product Name | Name of the specific product |\n|Provider\/Company Name | Name of the product provider |\n|Sector(s) | Sector of education where the product is used |\n|Primary Essential Function | The basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled |"}}