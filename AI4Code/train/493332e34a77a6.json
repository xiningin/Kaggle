{"cell_type":{"604a715c":"code","299da000":"code","8bf9831a":"code","17df3cd0":"code","1607f251":"code","82c8049f":"code","64ab0c71":"code","f7c452a9":"code","757e957d":"code","0f64c3d1":"markdown","e4bf1065":"markdown"},"source":{"604a715c":"'''\nImporting packages\n\n'''\n\n\nimport numpy as np \nimport pandas as pd \nimport re \nimport nltk \nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer","299da000":"\ndef preprocess_string(str_arg):\n    '''\n    input: str_arg --> Takes string to clean\n    output: cleaned_str --> Gives back cleaned string\n    This fuction cleans the text in the mentioned ways as comments after the line.This has been copied from some other kernel.\n\n    '''\n    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n    \n    return cleaned_str # Returning the preprocessed string in tokenized form","8bf9831a":"'''\nThis code block is for reading and cleaning data.\n\n'''\nimport_df = pd.read_csv('..\/input\/flipkart-products\/flipkart_com-ecommerce_sample.csv')\n# Reading relevant data\nimport_df['product_category_tree'] = import_df['product_category_tree'].apply(lambda x : x.split('>>')[0][2:].strip())\n# Category processing. (Check data to understand)\ntop_fiv_gen = list(import_df.groupby('product_category_tree').count().sort_values(by='uniq_id',ascending=False).head(5).index)\n# Taking only top 5 categories for example sake\nprocessed_df = import_df[import_df['product_category_tree'].isin(top_fiv_gen)][['product_category_tree','description']]\n# Selecting only relevant columns\nprocessed_df['description'] = processed_df['description'].astype('str').apply(preprocess_string)\n# Cleaning strings\ncat_list = list(processed_df['product_category_tree'].unique())\n# Creating a list of categories for later use\nprint(cat_list)\n# Printing the list of top 5 categories\nle = preprocessing.LabelEncoder()\ncategory_encoded=le.fit_transform(processed_df['product_category_tree'])\nprocessed_df['product_category_tree'] = category_encoded\n# Encoding the product category","17df3cd0":"'''\nThis code block is for spliting train test data\n\n'''\nX_train, X_test, y_train, y_test = train_test_split(processed_df['description'],processed_df['product_category_tree'],test_size=0.2)","1607f251":"'''\nThis code block is for converting the training data to vectorized form\n\n'''\nvect = CountVectorizer(stop_words = 'english')\n# Removing stop words\nX_train_matrix = vect.fit_transform(X_train) \n# Converting the train data","82c8049f":"'''\nThis code block is for training vectorized data and predicting & scoring test data\n\n'''\nclf=MultinomialNB()\n# Defining model\nclf.fit(X_train_matrix, y_train)\n# Fitting to multinomial NB model \nprint(clf.score(X_train_matrix, y_train))\n# Scoring the trained model (Expected to be above 95 percent)\nX_test_matrix = vect.transform(X_test) \n# Converting the test data\nprint (clf.score(X_test_matrix, y_test))\n# Scoring for the test data\npredicted_result=clf.predict(X_test_matrix)\nprint(classification_report(y_test,predicted_result))\n# Printing score ","64ab0c71":"'''\nThis code block is for converting the training data to Tf-Idf form\n\n'''\nvectorizer = TfidfVectorizer(stop_words = 'english')\n# Removing stop words\nX_train_tfidf = vectorizer.fit_transform(X_train)\n# Converting the train data","f7c452a9":"'''\nThis code block is for training, predicting & scoring test data\n\n'''\nclf2=MultinomialNB()\n# Defining model\nclf2.fit(X_train_tfidf, y_train)\n# Fitting to multinomial NB model \nprint(clf2.score(X_train_tfidf, y_train))\n# Scoring the trained model (Expected to be above 95 percent)\nX_test_tfidf = vectorizer.transform(X_test) \n# Converting the test data\nprint (clf2.score(X_test_tfidf, y_test))\n# Printing score ","757e957d":"'''\nTesting Block: Test your sting. Replace the 'car' string to test\n'''\nle.inverse_transform(clf.predict(vect.transform(['car'])))","0f64c3d1":"# Product Category Prediction\n\n##### Hi, \n###### Welcome to this repository! The objective of this repository is to best understand Naive Bayes . The data used in this repo has been taken from Kaggle (link below). \nhttps:\/\/www.kaggle.com\/PromptCloudHQ\/flipkart-products\n\n\n### About project Mechanic of Machine Learning:\nI am a mechanical engineer by education. Now, I want to deep dive in the world of Machine Learning, hence the name, mechanic of ML :D. I have taken up this project to understand the in-depth mathematics involved in regularly used ML algorithms. Under this project, I will be sharing useful material and links as I explore this domain. The objective is to learn and spread the same. Stay tuned to my GitHub for updates!\n\n### Business Case: \nAn online retailer wants to classify data based on description provided by seller. Generate a model to facilitate this ask. \n### Notebook objectives:\n* To understand and implement naive bayes \n\n\n### Assumptions:\n\n* Only five category data is considered from the total set \n\n### References:\n* GDA and NB: https:\/\/www.youtube.com\/watch?v=nt63k3bfXS0\n* NB: https:\/\/www.youtube.com\/watch?v=O2L2Uv9pdDA\n* Gaussian NB:https:\/\/www.youtube.com\/watch?v=H3EjCKtlVog\n* Building NB from scratch: https:\/\/towardsdatascience.com\/na%C3%AFve-bayes-from-scratch-using-python-only-no-fancy-frameworks-a1904b37222d\n* Notes and source code: https:\/\/github.com\/ArindamRoy23\/Product-Prediction_GDA-NB_Mechanic-of-ML\n","e4bf1065":"### Conclusion \n Naive Bayes works very well for this data set with an above 99% accuracy. This is a business ready model to deploy. "}}