{"cell_type":{"4359c60e":"code","7042808e":"code","56f61ff7":"code","3e057871":"code","9e9b77b6":"code","8dd33386":"code","8723603e":"code","a62b22e9":"code","30790a33":"code","a4de9d25":"code","e9efed14":"code","0118ee81":"code","bb30dfbc":"code","bc36fa2f":"code","61e6acc8":"code","dd1e8061":"code","79df9166":"code","a7e33964":"code","5d2e2494":"code","211fcb26":"code","f287df70":"code","18d72cfb":"code","c6f97a22":"code","0a5fda1a":"code","68f2bbf5":"code","3b240d19":"code","99536f4b":"code","261a17f0":"code","129beace":"code","36b1c802":"code","a9c4bcf1":"code","e0419972":"code","62009ef9":"code","52f05456":"code","a6d88295":"code","2993eb55":"code","105bdd13":"code","33a928e1":"code","b656da25":"code","2c9d186d":"code","66b89ac8":"code","c057a8e1":"code","8ffab175":"code","8b0ae197":"code","4d45910c":"code","dbd8b468":"code","fd89c4f9":"code","5436009c":"code","91fff843":"code","a1c6ea1d":"code","1c0035ca":"code","c411aa55":"code","8901b9ad":"code","fe96044a":"code","f4d8d8cb":"code","606c88d6":"code","e93fe052":"code","168efd39":"code","5f5ed4a6":"code","3e14162b":"code","3049b80f":"code","07460c8e":"code","25e4bb8e":"code","428b2630":"code","12f8b2dd":"markdown","b010d449":"markdown","2cf25db5":"markdown","0a43650e":"markdown","841d63cb":"markdown","19e797c7":"markdown","a3dcd62e":"markdown","2cad8ce1":"markdown","56d84f2e":"markdown","4e106cf7":"markdown","4761c1ee":"markdown","bef92aab":"markdown","6924b79d":"markdown","0d44da1a":"markdown","23474ac7":"markdown","2b6b6671":"markdown","91fa4722":"markdown","c4fb27ed":"markdown","73a18f2d":"markdown","2893ce10":"markdown","cf7c9666":"markdown","7a879614":"markdown","e7cf4ac1":"markdown","ebe21fd5":"markdown","13e993fd":"markdown","a787ce96":"markdown","3cd36336":"markdown","dc4ffcb7":"markdown","df5f53a1":"markdown","a268dd22":"markdown","9a39ae4a":"markdown","6c7f5122":"markdown","c82ede1c":"markdown","d1104151":"markdown","29f64f75":"markdown","e67a93cd":"markdown"},"source":{"4359c60e":"import pandas as pd\nimport numpy as np\nimport keras\nnp.random.seed(2)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport itertools","7042808e":"# Function to plot Confusion Matrix (to be used later).\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","56f61ff7":"data=pd.read_csv('..\/input\/creditcard.csv')","3e057871":"data.head()","9e9b77b6":"data.tail()","8dd33386":"# To check the count of fraudulent and normal transactions\nsns.countplot(data['Class'],facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3), label = \"Count\")","8723603e":"# Now Checking actual number of fraudulent transactions\nfraud_indices=np.array(data[data.Class==1].index)\nno_records_fraud=len(fraud_indices)\nnormal_indices=np.array(data[data.Class==0].index)\nno_records_normal=len(normal_indices)\n\nprint(\"No. of Fraudulent Transaction is {} and No. of Normal Transaction is {}\".format(no_records_fraud, no_records_normal))","a62b22e9":"# To see the actual distribution of data \nsns.pairplot(data, hue = 'Class', vars = ['V1', 'V2', 'V3', 'V15', 'V18','Amount'] )","30790a33":"sns.kdeplot(data['Amount'],shade=True)","a4de9d25":"# To see the the actual distribution of Amount\n\nfig=sns.FacetGrid(data,hue='Class',aspect=4)\nfig.map(sns.kdeplot,'Amount',shade=True)\noldest=data['Amount'].max()\nfig.set(xlim=(0,oldest))\nfig.add_legend()","e9efed14":"sns.scatterplot(x = 'Amount', y = 'V1',hue='Class',  data = data)\n","0118ee81":"dataset2 = data.drop(columns = ['Class'])\n\n","bb30dfbc":"dataset2.corrwith(data.Class).plot.bar(\n        figsize = (20, 10), title = \"Correlation with Class\", fontsize = 20,\n        rot = 45, grid = True)","bc36fa2f":"\ndata['normalized_amount']=StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n# Dropping the actual Amount column from the dataset.\ndata=data.drop(['Amount'],axis=1)","61e6acc8":"# To check the dataset for changed column\ndata.head()","dd1e8061":"# I think Time is the irrelevant column so we are dropping the Time column from dataset.\ndata=data.drop(['Time'],axis=1)","79df9166":"data.head()","a7e33964":"# Assigning X and Y \nX=data.iloc[:,data.columns!='Class']\ny=data.iloc[:,data.columns=='Class']","5d2e2494":"X.head()","211fcb26":"y.head()","f287df70":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","18d72cfb":"X_train.shape","c6f97a22":"X_test.shape","0a5fda1a":"# As we have to supply the X test,X_Train,ytest,y_train into deep learning models so we have to convert it into numpy arrays.\nX_train = np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test=np.array(y_test)","68f2bbf5":"from keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense","3b240d19":"model = Sequential([\n     #First Layer\n     Dense(units=16, input_dim=29, activation='relu'),\n      #Second Layer\n     Dense(units=24,activation='relu'),\n     Dropout(0.5),\n      #Third Layer\n     Dense(20,activation='relu'),\n     #Fourth Layer\n     Dense(24,activation='relu'),\n     #Fifth Layer\n     Dense(1,activation='sigmoid')  \n    \n    \n])\n\nmodel.summary()","99536f4b":"model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train, batch_size=15, epochs=5)","261a17f0":"score=model.evaluate(X_test,y_test)\nprint(score)","129beace":"y_pred=model.predict(X_test)\ny_test=pd.DataFrame(y_test)","36b1c802":"cnf_matrix=confusion_matrix(y_test,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","a9c4bcf1":"y_pred=model.predict(X)\ny_test=pd.DataFrame(y)\ncnf_matrix=confusion_matrix(y_test,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","e0419972":"X=data.iloc[:,data.columns!='Class']\ny=data.iloc[:,data.columns=='Class']","62009ef9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","52f05456":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest=RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train,y_train.values.ravel())","a6d88295":"y_pred=random_forest.predict(X_test)","2993eb55":"cnf_matrix=confusion_matrix(y_test,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","105bdd13":"y_pred=random_forest.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","33a928e1":"X=data.iloc[:,data.columns!='Class']\ny=data.iloc[:,data.columns=='Class']","b656da25":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)\n","2c9d186d":"from sklearn.tree import DecisionTreeClassifier\ndecc=DecisionTreeClassifier()\ndecc.fit(X_train,y_train.values.ravel())","66b89ac8":"y_pred=decc.predict(X_test)","c057a8e1":"decc.score(X_test,y_test)","8ffab175":"cnf_matrix=confusion_matrix(y_test,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","8b0ae197":"y_pred=decc.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","4d45910c":"fraud_indices=np.array(data[data.Class==1].index)\nno_records_fraud=len(fraud_indices)\nprint(no_records_fraud)","dbd8b468":"normal_indices=data[data.Class==0].index","fd89c4f9":"random_normal_indices=np.random.choice(normal_indices,no_records_fraud,replace=False)\nrandom_normal_indices=np.array(random_normal_indices)\nprint(len(random_normal_indices))","5436009c":"under_sample_indices=np.concatenate([fraud_indices,random_normal_indices])\nprint(len(under_sample_indices))","91fff843":"under_sample_data=data.iloc[under_sample_indices,:]","a1c6ea1d":"under_sample_data.head()","1c0035ca":"X_undersample=under_sample_data.iloc[:,under_sample_data.columns!='Class']\ny_undersample=under_sample_data.iloc[:,under_sample_data.columns=='Class']","c411aa55":"\nX_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample, test_size = 0.3, random_state=0)","8901b9ad":"X_train = np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test=np.array(y_test)","fe96044a":"model = Sequential([\n     Dense(units=16, input_dim=29, activation='relu'),\n     Dense(units=24,activation='relu'),\n     Dropout(0.5),\n     Dense(20,activation='relu'),\n     Dense(24,activation='relu'),\n     Dense(1,activation='sigmoid')  \n    \n    \n])\n\nmodel.summary()","f4d8d8cb":"model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train, batch_size=15, epochs=5)","606c88d6":"y_pred=model.predict(X_test)\ny_expected=pd.DataFrame(y_test)\n\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","e93fe052":"y_pred=model.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","168efd39":"\nX_resample,y_resample=SMOTE().fit_sample(X,y.values.ravel())","5f5ed4a6":"y_resample=pd.DataFrame(y_resample)\nX_resample=pd.DataFrame(X_resample)\n\n","3e14162b":"X_train, X_test, y_train, y_test = train_test_split(X_resample, y_resample, test_size = 0.3, random_state=0)","3049b80f":"X_train = np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test=np.array(y_test)","07460c8e":"model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train, batch_size=15, epochs=5)","25e4bb8e":"y_pred=model.predict(X_test)\ny_expected=pd.DataFrame(y_test)\n\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","428b2630":"y_pred=model.predict(X)\n\ncnf_matrix=confusion_matrix(y,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","12f8b2dd":"# Welcome to My Kernel","b010d449":"# Confusion Matrix for entire dataset","2cf25db5":"# Applying Undersampling","0a43650e":"There is a significant improvement in reducing down the false negatives to 35 but still we have to reduce this number to bare minimum.","841d63cb":"# Plotting Confusion matrix for entire dataset","19e797c7":"# Deep Neural Network","a3dcd62e":"# Confusion Matrix for Entire dataset","2cad8ce1":"# Conclusion\nSince, our goal was to reduce the false negatives which we have reduced significantly to 2 using SMOTE oversampling technique.Further we can also increase the epochs and can try above applied machine learning algo to improve the result and reduce the false negative to zero.","56d84f2e":"As all the features from V1 to V28 are already normalized, so we just have to normalize the Amount","4e106cf7":"# Applying random Forest Classifier","4761c1ee":"Our goal is to reduce the false negative to bare minimum as it indicates number of fraudulent transaction which model predicted as normal transaction which is a very serious error because our model is not able to identify the fraudulent transactions.","bef92aab":"This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.\n\nThe new instances are not just copies of existing minority cases; instead, the algorithm takes samples of the feature space for each target class and its nearest neighbors, and generates new examples that combine features of the target case with features of its neighbors. This approach increases the features available to each class and makes the samples more general.\n\nSMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases. For example, suppose you have an imbalanced dataset where just 1% of the cases have the target value A (the minority class), and 99% of the cases have the value B. To increase the percentage of minority cases to twice the previous percentage, you would enter 200 for SMOTE percentage in the module's properties.\n\n<img src=\"https:\/\/image.ibb.co\/iEudQq\/smote-1.png\">","6924b79d":"As the number of fraudulent transactions are very less in comparison to normal transactions we are not able to see fraudulent transactions.","0d44da1a":"# Setting Optimizer and Loss Function\n\nOnce our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.\n\n**Loss Function** : \nWe define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (=2 classes) called the binary_crossentropy\".\n\nThe most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss.\n\nI choosed **Adam optimizer** because it combines the advantages of two other extensions of stochastic gradient descent. Specifically:\n\n**1. Adaptive Gradient Algorithm (AdaGrad)** that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n\n**2. Root Mean Square Propagation (RMSProp)** that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n\nAdam realizes the benefits of both **AdaGrad** and **RMSProp**.\n\nAdam is a popular algorithm in the field of deep learning because it achieves good results fast.\n\nThe metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation).\n\nI have used 5 epochs and batch size of 15","23474ac7":"# Applying Keras Sequential model on undersampled dataset","2b6b6671":"# Confusion Matrix for Entire dataset","91fa4722":"# Applying Keras Sequential Model on Oversampled dataset","c4fb27ed":"# Finding Correlation with target variable","73a18f2d":"2.**Checking the last five entries to get the idea of data distribution from top to end.**","2893ce10":"# What is the Class Imbalance Problem?\nIt is the problem in machine learning where the total number of a class of data (positive) is far less than the total number of another class of data (negative). This problem is extremely common in practice and can be observed in various disciplines including fraud detection, anomaly detection, medical diagnosis, oil spillage detection, facial recognition, etc.\n\n# Why is it a problem?\nMost machine learning algorithms and works best when the number of instances of each classes are roughly equal. When the number of instances of one class far exceeds the other, problems arise. This is best illustrated below with an example.\n\nGiven a dataset of transaction data, we would like to find out which are fraudulent and which are genuine ones. Now, it is highly cost to the e-commerce company if a fraudulent transaction goes through as this impacts our customers trust in us, and costs us money. So we want to catch as many fraudulent transactions as possible.\n\nIf there is a dataset consisting of 10000 genuine and 10 fraudulent transactions, the classifier will tend to classify fraudulent transactions as genuine transactions. The reason can be easily explained by the numbers. Suppose the machine learning algorithm has two possibly outputs as follows:\n\nModel 1 classified 7 out of 10 fraudulent transactions as genuine transactions and 10 out of 10000 genuine transactions as fraudulent transactions.\nModel 2 classified 2 out of 10 fraudulent transactions as genuine transactions and 100 out of 10000 genuine transactions as fraudulent transactions.\nIf the classifier\u2019s performance is determined by the number of mistakes, then clearly Model 1 is better as it makes only a total of 17 mistakes while Model 2 made 102 mistakes. However, as we want to minimize the number of fraudulent transactions happening, we should pick Model 2 instead which only made 2 mistakes classifying the fraudulent transactions. Of course, this could come at the expense of more genuine transactions being classified as fraudulent transactions, but will be a cost we can bear for now. Anyhow, a general machine learning algorithm will just pick Model 1 than Model 2, which is a problem. In practice, this means we will let a lot of fraudulent transactions go through although we could have stopped them by using Model 2. This translates to unhappy customers and money lost for the company.\n\n**How to tell the machine learning algorithm which is the better solution?\nTo tell the machine learning algorithm (or the researcher) that Model 2 is better than Model 1, we need to show that Model 2 above is better than Model 1 above. For that, we will need better metrics than just counting the number of mistakes made.\n\nWe introduce the concept of True Positive, True Negative, False Positive and False Negative:\n\n1. True Positive (TP) \u2013 An example that is positive and is classified correctly as positive<br>\n2. True Negative (TN) \u2013 An example that is negative and is classified correctly as negative<br>\n3. False Positive (FP) \u2013 An example that is negative but is classified wrongly as positive<br>\n4. False Negative (FN) \u2013 An example that is positive but is classified wrongly as negative<br>\n\n**Sampling based approaches**\nThis can be roughly classified into two categories:\n\nOversampling, by adding more of the minority class so it has more effect on the machine learning algorithm\nUndersampling, by removing some of the majority class so it has less effect on the machine learning algorithm\n\n**1. Undersampling :** By undersampling, we could risk removing some of the majority class instances which is more representative, thus discarding useful information. This can be illustrated as follows: \n<img src=\"https:\/\/image.ibb.co\/cuD6Vq\/Undersampling-580x197.png\">\n\nHere the green line is the ideal decision boundary we would like to have, and blue is the actual result. On the left side is the result of just applying a general machine learning algorithm without using undersampling. On the right, we undersampled the negative class but removed some informative negative class, and caused the blue decision boundary to be slanted, causing some negative class to be classified as positive class wrongly.","cf7c9666":"Our model not able to identify all the fraudulent transactions for entire dataset also as number of fals negative is 122","7a879614":"**if like please UPVOTE the kernel and dont forget to give your valuable suggestions.**","e7cf4ac1":"In this dataset we have to identify fraudulent transaction and it is basically a anomaly detection problem.\n\n**Anomaly detection** is a technique used to identify unusual patterns that do not conform to expected behavior, called outliers. It has many applications in business, from intrusion detection (identifying strange patterns in network traffic that could signal a hack) to system health monitoring (spotting a malignant tumor in an MRI scan), and from fraud detection in credit card transactions to fault detection in operating environments.\n\nThis dataset is suffered from the problem of imbalanced dataset as number of fraudulent transactions are very few in comparison of non fraudulent transactions this is the reason that most of the machine learning and deep learning models will not provide satisfactory results and\/or unable to identify fraudulent transasctions.\n\n**Observation from the dataset :**\n1. The dataset consists of numerical values from the 28 \u2018Principal Component Analysis (PCA)\u2019 transformed features, namely V1 to V28. Furthermore, there is no metadata about the original features provided, so pre-analysis or feature study could not be done.\n2. The \u2018Time\u2019 and \u2018Amount\u2019 features are not transformed data.\n3.There is no missing value in the dataset.\n\nSo, my approach\/workflow for solving this problem is detailed below :\n1. EDA\n2. Finding correlation of attributes with target variable\n3. Preprocessing the data\n4. Apply Deep Neural Network\n5. Apply Machine Learning Classifiers (Random forest, Decision Tree Classifier)\n6. Apply Undersampling\n7. Apply Deep Neural network to check the accuracy and false negatives.\n8. Apply SMOTE - Oversampling\n9. Apply Deep Neural network to check the accuracy and false negatives.\n10. Final remarks\n","ebe21fd5":"# Exploratory Data Analysis (EDA)","13e993fd":"# Splitting data into Train and Test set\n\nI am splitting the data into 70% of the data into training set and 30% of the data into test set.","a787ce96":"**Model Definition:**\n\nI used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n\nThe first is the sequential layer. It takes 16 units it is a Positive integer, it specifies dimensionality of the output space and the activation function used in this layer is relu\n\n**'relu'** is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n**'sigmoid'** The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n\nIn second layer I have used 24 units and used activation function relu.\n\n**Dropout** is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\nAs the dataset is large I have opted for 0.5 dropout.\n\nIn third layer I have used 20 units and used activation function relu.\n\nIn fourth layer I have used 24 units and used activation function relu.\n\nIn last layer output should be 1 so i have used 1 and used activation function sigmoid.","3cd36336":"# Importing Essential Libraries","dc4ffcb7":"# Confusion Matrix","df5f53a1":"Still there is a significantly more number of false negatives so we have to apply undersampling and oversampling techniques to see that the performance of the model will improve or not.","a268dd22":"# Preprocessing","9a39ae4a":"For the entire dataset the false negative reduced to 35 in comparison to deep learning model having false negatives 122 in case of entire dataset. Still there is a chance of further improvement.","6c7f5122":"1. **Checking the actual data how it looks like by looking at top 5 rows of the dataset. **","c82ede1c":"# Synthetic Minority Over-sampling Technique (SMOTE)\nUsing a machine learning algorithm out of the box is problematic when one class in the training set dominates the other. SMOTE solves this problem. In this tutorial I'll walk you through how SMOTE works and then how the SMOTE function code works.\n<img src=\"https:\/\/image.ibb.co\/iZjL5q\/SMOTE.png\">","d1104151":"# Confusion Matrix for Entire dataset","29f64f75":"# Applying Decision Tree Classifier","e67a93cd":"**It seems that there are very few Fraudulent Transactions in comparison to Normal Transactions.**\n"}}