{"cell_type":{"96fa9155":"code","e684dc9c":"code","3e51a585":"code","e91ef36e":"code","2dd4c9cf":"code","f396ba41":"code","fe34789f":"code","0442558a":"code","789c303a":"code","693d68d4":"code","c5f68b9d":"code","101cecbf":"code","9d02d1da":"code","21851547":"code","23c38371":"code","e3035a1a":"code","1f5780c1":"code","34297179":"code","129dc73b":"code","cb1584b4":"code","96558b39":"code","7458a0b5":"code","8d897705":"code","06fd7930":"code","78f43588":"code","4b818764":"code","18c06d61":"code","b69c7955":"code","4e761618":"code","b7c4014e":"code","47b342d3":"code","4cb4527b":"code","01967947":"code","1a8ab320":"code","51163e9c":"code","48cc9077":"code","4b934da8":"code","abe18026":"code","e5a7e3d6":"code","29b8ac0a":"code","5be154e1":"code","52e84fa1":"code","4840ad7b":"code","65c1ee9d":"code","e3f7fd97":"code","ea005a7d":"code","22fc0d1d":"markdown","3e6555d9":"markdown","739d5a7a":"markdown","45748a7b":"markdown","80666d0b":"markdown","02f32d58":"markdown","86dee0af":"markdown","b8b79b5d":"markdown","ef8198e5":"markdown","12c673d5":"markdown","bf38d2e8":"markdown","afd9575b":"markdown","94ba57c6":"markdown","5cc1d29b":"markdown","c946cbc3":"markdown","5b866b37":"markdown"},"source":{"96fa9155":"import math # Matematica\nimport pandas as pd # Pre-processamento\nimport numpy as np # Algebra linear\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn.metrics as metrics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn # ignora alguns avisos irritantes(das bibliotescas sklearn e seaborn)","e684dc9c":"# Lendo a base de dados\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3e51a585":"train","e91ef36e":"test","2dd4c9cf":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Obtem os paramentros ajustados pela fun\u00e7\u00e3o\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} e sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Exibe a distribui\u00e7\u00e3o atual\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} e $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequencia')\nplt.title('Distribui\u00e7\u00e3o do Pre\u00e7o de Venda')\n\n# Tamb\u00e9m obtem o QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","f396ba41":"# Normaliza\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","fe34789f":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Obtem os paramentros ajustados pela fun\u00e7\u00e3o\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Exibe a distribui\u00e7\u00e3o atual\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequencia')\nplt.title('Distribui\u00e7\u00e3o do Pre\u00e7o de Venda')\n\n# Tamb\u00e9m obtem o QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","0442558a":"# Concatena as bases de dados para facilitar o pr\u00e9-processamento\ndata = pd.concat([train, test])","789c303a":"data.set_index('Id', inplace = True)","693d68d4":"data.isnull().sum()[:50]","c5f68b9d":"data.info()","101cecbf":"data['PoolQC'].fillna('None', inplace=True) # Valor NaN significa 'Sem Pscina'\ndata['MiscFeature'].fillna('None', inplace=True) # Valore NaN significa 'Sem Funcionalidade diversa'\ndata['Alley'].fillna('None', inplace=True) # Valor NaN significa 'Sem acesso ao beco'\ndata['Fence'].fillna('None', inplace=True) # Valor NaN significa 'Sem cerca'\ndata['FireplaceQu'].fillna('None', inplace=True) # Valor NaN significa 'Sem Lareira'","9d02d1da":"# Como a \u00e1rea de cada rua conectada \u00e0 propriedade da casa, muito provavelmente tem uma \u00e1rea semelhante a outras casas em seu bairro, podemos preencher os valores que faltam pela mediana da 'LotFrontage' do bairro.\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","21851547":"# GarageType, GarageFinish, GarageQual e GarageCond: Repondo dados faltantes por None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')","23c38371":"# GarageYrBlt, GarageArea e GarageCars : Repondo dados faltantes por 0 (Pois Sem garagem = sem carros na garagem.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)","e3035a1a":"# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath e BsmtHalfBath : Dados faltantes provavelmente s\u00e3o zeros (pois o lugar pode n\u00e3o ter por\u00e3o)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)","1f5780c1":"# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 e BsmtFinType2 : Para todas essas categorias que tem rela\u00e7\u00e3o com por\u00e3o, NaN ssignifica que pode n\u00e3o ter por\u00e3o.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')","34297179":"# MasVnrArea e MasVnrType : Provavelmente Na significa 'sem folheado de alvenaria'. Nos podemos colocar a \u00e1rea como 0 e o tipo como None. \ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)","129dc73b":"# MSZoning (Classifica\u00e7\u00e3o geral de zoneamento) : 'RL' \u00e9 de longe o valor mais comum. Nesse caso nos podemos substituir  Na por 'RL'\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])","cb1584b4":"# Utilities : Para essa caracteristica categorica todos os valores s\u00e3o \"AllPub\", exceto um \"NoSeWa\" e 2 NA. Como a casa com 'NoSewa' est\u00e1 no conjunto de treinamento, esse recurso n\u00e3o ajudar\u00e1 na modelagem preditiva. Podemos ent\u00e3o remov\u00ea-lo tranquilamente.\ndata = data.drop(['Utilities'], axis=1)","96558b39":"# Functional : A descri\u00e7\u00e3o dos dados diz que NA significa typical\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")","7458a0b5":"# Electrical : Esse tem apenas um valor Na. Como a caracteristica mais comum \u00e9 'SBrkr', podemos substituir o valor faltante por este.\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])","8d897705":"# KitchenQual: Apenas um valor Na. Como a caracteristica mais comum \u00e9 'TA', podemos substituir o valor faltante por este.\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])","06fd7930":"# Exterior1st e Exterior2nd : Denovo, como Exterior 1 e 2 tem apenas um valor faltante. Vamos substituir esse valor pelo valor mais comum da coluna.\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])","78f43588":"# SaleType : Preenchendo denovo com o valor mais comum: \"WD\"\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","4b818764":"# MSSubClass : Na provavelmente significa Sem aula de constru\u00e7\u00e3o. Vamos substituilos por None\ndata['MSSubClass'] = data['MSSubClass'].fillna(\"None\")","18c06d61":"data.info()","b69c7955":"# Transformando algumas vari\u00e1veis numericas que na verdade s\u00e3o categoricas\n\n# MSSubClass=Sem aula de constru\u00e7\u00e3o\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\n\n\n# Mudando OverallCond para variavel categorica\ndata['OverallCond'] = data['OverallCond'].astype(str)\n\n\n# Ano e mes vendido transformados para vari\u00e1veis categoricas\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","4e761618":"# Aplicando Label Encoding em algumas vari\u00e1veis categ\u00f3ricas que podem conter informa\u00e7\u00f5es em seu conjunto de ordena\u00e7\u00e3o\n\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# Processa as colunas, e aplica LabelEncoder para vari\u00e1veis categoricas\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[c].values)) \n    data[c] = lbl.transform(list(data[c].values))\n\n# Formato        \nprint('Shape data: {}'.format(data.shape))","b7c4014e":"# Representa as vari\u00e1veis categoricas de uma forma num\u00e9rica\ndata = pd.get_dummies(data)\nprint(data.shape)","47b342d3":"train = data[:1460]\ntest = data[1460:].drop('SalePrice', axis = 1)","4cb4527b":"train.shape, test.shape","01967947":"X = train.drop('SalePrice', axis = 1)\ny = train['SalePrice']","1a8ab320":"# Divide a base em treino e valida\u00e7\u00e3o\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)","51163e9c":"xgb = XGBRegressor(booster='gbtree', colsample_bylevel=1,\n                   colsample_bynode=1,colsample_bytree=0.6,\n                   gamma=0, importance_type='gain',\n                   learning_rate=0.01, max_delta_step=0,\n                   max_depth=4,min_child_weight=1.5,\n                   n_estimators=2500, n_jobs=1, nthread=None,\n                   objective='reg:linear', reg_alpha=0.4640,\n                   reg_lambda=0.6, scale_pos_weight=1,\n                   silent=None, subsample=0.8, verbosity=1)","48cc9077":"lgbm = LGBMRegressor(objective='regression',\n                    num_leaves=4,\n                    learning_rate=0.01,\n                    n_estimators=11000,\n                    max_bin=200,\n                    bagging_fraction=0.75,\n                    bagging_freq=5,\n                    bagging_seed=7,\n                    feature_fraction=0.4)","4b934da8":"gboost = GradientBoostingRegressor(n_estimators=3000, \n                                   learning_rate=0.05,\n                                   max_depth=4, \n                                   max_features='sqrt',\n                                   min_samples_leaf=15, \n                                   min_samples_split=10, \n                                   loss='huber', \n                                   random_state =5)","abe18026":"xgb.fit(X_train, y_train)\nlgbm.fit(X_train, y_train, eval_metric='rmsle')\ngboost.fit(X_train, y_train)","e5a7e3d6":"pred2 = xgb.predict(X_test)\npred3 = lgbm.predict(X_test)\npred4 = gboost.predict(X_test)","29b8ac0a":"print('Erro m\u00e9dio logar\u00edtmico de raiz quadrado test (XGB) = ' + str(math.sqrt(metrics.mean_squared_log_error(y_test, pred2))))\nprint('Erro m\u00e9dio logar\u00edtmico de raiz quadrado test (LGBM) = ' + str(math.sqrt(metrics.mean_squared_log_error(y_test, pred3))))\nprint('Erro m\u00e9dio logar\u00edtmico de raiz quadrado test (GBoost) = ' + str(math.sqrt(metrics.mean_squared_log_error(y_test, pred4))))","5be154e1":"lgbm.fit(X, y)   # 0.12269 \nxgb.fit(X ,y)    # 0.12495\ngboost.fit(X, y) # 0.12333","52e84fa1":"prediction_lgbm =  np.expm1(lgbm.predict(test))\nprediction_xgb = np.expm1(xgb.predict(test))\nprediction_gboost = np.expm1(gboost.predict(test))","4840ad7b":"\"\"\"\nprediction = ( prediction_lgbm * 0.38 + prediction_gboost * 0.35 + prediction_xgb * 0.27)   # 0.12006\nprediction = ( prediction_lgbm * 0.4 + prediction_gboost * 0.35 + prediction_xgb * 0.25)    # 0.12007\nprediction = ( prediction_lgbm * 0.45 + prediction_gboost * 0.35 + prediction_xgb * 0.2)    # 0.12012\nprediction = ( prediction_lgbm * 0.55 + prediction_gboost * 0.45)                           # 0.12061\nprediction = ( prediction_lgbm * 0.45 + prediction_gboost * 0.55)                           # 0.12069\nprediction = ( prediction_gboost * 0.15 + prediction_lgbm * 0.7 + prediction_gboost * 0.15) # 0.12086\nprediction = ( prediction_gboost * 0.2 + prediction_lgbm * 0.5 + prediction_gboost * 0.3)   # 0.12154\nprediction = ( prediction_lgbm * 0.55 + prediction_xgb * 0.45)                              # 0.12155\n\"\"\"","65c1ee9d":"prediction = ( prediction_lgbm * 0.38 + prediction_gboost * 0.36 + prediction_xgb * 0.26)   # 0.12006","e3f7fd97":"# Tranforma o resultado em um DataFrame\nsubmission = pd.DataFrame({\"Id\": test.index,\"SalePrice\": prediction})","ea005a7d":"# Transforma o DataFrame em um arquivo csv para ser submetido no kaggle.\nsubmission.to_csv('submission.csv', index=False)","22fc0d1d":"#### Se o notebook tiver sido \u00fatil, d\u00ea um upvote!","3e6555d9":"### Trabalhando com Valores Faltantes\n\nOlhando o arquivo data_description.txt (vem junto com a base de dados) podemos fazer varios ajustes nos dados: ","739d5a7a":"## Ajusta e valida os modelos","45748a7b":"### Normalizando os dados","80666d0b":"## Treino e Previs\u00e3o (Usando toda a base de dados)","02f32d58":"* GradientBoostingRegressor","86dee0af":"* XGBRegressor","b8b79b5d":"No missing values","ef8198e5":"## Engenharia de Recursos","12c673d5":"# Informa\u00e7\u00f5es iniciais\nNeste notebook \u00e9 apresentado uma resolu\u00e7\u00e3o do desafio \"House Prices: Advanced Regression Techniques\". Como modelo de aprendizado de maquina foram utilizados: \n* LGBMRegressor\n* XGBRegressor\n* GradientBoostingRegressor\n\n**Se esse notebook lhe for \u00fatil, d\u00ea um upvote! :)**\n\nSe encontrar algum problema no c\u00f3digo, ou tiver alguma d\u00favida\/dica a deixe nos comentarios.\n\nOBS: Essa \u00e9 a vers\u00e3o em Portugu\u00eas de um outro notebook que eu mesmo fiz. Objetivo desse \u00e9 apenas facilitar o aprendizado, mesmo pra quem ainda n\u00e3o tenham boa familiaridade com ingl\u00eas.\n\nLink do notebook em Ingl\u00eas:\n\n* https:\/\/www.kaggle.com\/katchupalvarenga\/house-prices-top-10-on-leaderboard","bf38d2e8":"# Importa\u00e7\u00f5es","afd9575b":"## Submiss\u00e3o","94ba57c6":"# Modelo","5cc1d29b":"#  Pr\u00e9-processamento de Dados","c946cbc3":"# Fontes:\n    \n1. https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling\n2. https:\/\/www.kaggle.com\/fedi1996\/house-prices-data-cleaning-viz-and-modeling","5b866b37":"* LGBMRegressor"}}