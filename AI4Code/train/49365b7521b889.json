{"cell_type":{"c76ddd9e":"code","5676087c":"code","1fe2c6cb":"code","1ac4c567":"code","aa3a38e4":"code","d4779970":"code","94a9c7bc":"code","f70d5e17":"code","8022b47c":"code","d364e905":"code","cdde779d":"code","da686eb5":"code","3561b80a":"code","23a795b5":"code","61b5e80a":"code","1ed8b8e5":"code","fd61a628":"code","ccf04379":"code","edca6aa0":"code","a7528cc6":"code","4c55cd09":"code","642ec64e":"code","910f7e45":"code","d3479d70":"code","6cade07b":"code","3e8d15d6":"code","e5fd2685":"code","cc54aaa9":"code","7dfd464a":"code","bad41ce9":"code","fe08a0dd":"code","02dba317":"code","32794a7a":"code","b735d0af":"code","547e9968":"code","66d777e2":"code","7207e25d":"code","4352c0cb":"code","6c3d793e":"code","4b37a079":"code","01a2e43b":"code","585dc154":"code","5eb45350":"code","3e0f6536":"code","7646f40f":"code","bb34ea9f":"code","c7e4a6af":"code","1086a7ad":"code","32e66f7c":"code","85d6ed8e":"code","59b57495":"code","c16642f6":"code","6ab19a7d":"code","2340ac40":"code","e2fd638c":"code","c3c6a102":"code","b72b9bce":"code","83c41c20":"code","7d0c10aa":"code","9e16ddfc":"code","db1ffa94":"code","42ed820d":"code","6c03b1bd":"code","ac18694a":"code","3ff33828":"code","cdf18b2d":"code","5abd7c6a":"code","f833fc4a":"code","8b6f1ccd":"code","4286e8a8":"code","3c3345ba":"code","18d21842":"code","15dbd945":"code","7d1dfd13":"code","3c10ab6f":"code","95bc64c3":"code","883b57e9":"code","260d3905":"code","6c07f8cf":"code","761845b9":"code","29ed382b":"code","a949219e":"code","274fe8d0":"code","9f82b130":"code","6619065c":"code","1ac477b4":"code","a8472bd5":"code","4ea84d3e":"code","f709cf60":"code","edd9fdcb":"code","4aef9b4e":"code","dc824144":"code","238952ff":"code","3bde0e49":"code","7a2d2030":"code","26c9e69f":"code","438d402a":"code","4df62a0c":"code","05526d94":"code","f8cd2d00":"code","aaa1c788":"code","a8876a7c":"code","097dbd0f":"code","c9acb6c6":"code","d64407dd":"code","bb3c0f0d":"code","f4282964":"code","6901e560":"code","054d5baa":"code","6130765a":"code","a4e67c1a":"code","2a12976c":"code","9597c554":"code","2f3da2b9":"code","88ff9a0c":"code","4cc22885":"code","423e246e":"code","eadaabeb":"code","6688255a":"code","cd0c4a6f":"code","ef25901d":"code","cc49f194":"code","a0504d06":"code","859e9bb8":"code","b373c79f":"code","fb1788ec":"code","c8230ffc":"code","b44cf66c":"code","72c67619":"code","434662bf":"code","b680b1a4":"code","e3d9dabd":"code","ad39af90":"code","c4e17d23":"code","e4e89253":"code","7dfcf5bb":"markdown","47c26c8c":"markdown","d131c00c":"markdown","d7b9ac1f":"markdown","b8dba494":"markdown","275fdc8c":"markdown","f037b31c":"markdown","338c2b5b":"markdown","56d78794":"markdown","dbcf0c63":"markdown","e82c5dea":"markdown","5f3c5bc4":"markdown","8c572691":"markdown","9342afac":"markdown","081c8176":"markdown","e08fefe2":"markdown","dd84f211":"markdown","7613c1cb":"markdown","e7fcc989":"markdown","9bc63376":"markdown","018c5a04":"markdown","592bd6a2":"markdown","0f9109a8":"markdown","a7238114":"markdown","253507c0":"markdown","7423c8e5":"markdown","b56b20b3":"markdown","39493e75":"markdown","b05bc5c6":"markdown","cb0bfb27":"markdown","7bbdef28":"markdown","beafd17f":"markdown","51c1ac80":"markdown","012f0baa":"markdown","ef614d1a":"markdown","2e8e47fb":"markdown","dd277413":"markdown","98ca8b9c":"markdown","09c31f4e":"markdown","656c8266":"markdown","c8d738c5":"markdown","ca4384a0":"markdown","6887b83f":"markdown","b5a6d606":"markdown"},"source":{"c76ddd9e":"import pandas as pd # package for high-performance, easy-to-use data \n#structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\nimport missingno as msno #checking missing values\ncolor = sns.color_palette()\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.offline as offline\noffline.init_notebook_mode()\nfrom pylab import rcParams\n\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, roc_auc_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\n\n# import cufflinks and offline mode\nimport cufflinks as cf\ncf.go_offline()\n\n# from sklearn import preprocessing\n# # Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","5676087c":"data=pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head(5)","1fe2c6cb":"data.info()\ndata.describe().T","1ac4c567":"msno.bar(data, color = 'r', figsize = (10,8))   ","aa3a38e4":"#Reassign target\ndata.Attrition.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)\n# Drop useless feat\ndata = data.drop(columns=['StandardHours', \n                          'EmployeeCount', \n                          'Over18',\n                        ])\ndata.head(5)","d4779970":"attrition = data[(data['Attrition'] != 0)]\nno_attrition = data[(data['Attrition'] == 0)]\n\n#COUNT\ntrace = go.Bar(x = (len(attrition), len(no_attrition)), y = ['Yes_attrition', 'No_attrition'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=['gold', 'lightskyblue'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Attrition Count')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)\n\n#PERCENTAGE\ntrace = go.Pie(labels = ['No_attrition', 'Yes_attrition'], values = data['Attrition'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightskyblue','gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Attrition Distribution')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","94a9c7bc":"plt.figure(figsize=(20,20))\ncorr = data.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(20,20))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr,  cmap=\"RdYlGn\", annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","f70d5e17":"data_num=data.select_dtypes(include='number')\ndata_num.head()","8022b47c":"data_obj=data.select_dtypes(include='object')\ndata_obj.head()","d364e905":"plt.subplots(figsize=(10,10))\nsns.countplot(data.Age)","cdde779d":"sns.scatterplot(x='Age',y='MonthlyIncome',data=data)","da686eb5":"sns.countplot(x='Attrition',hue='PerformanceRating',data=data)","3561b80a":"plt.subplots(figsize=(10,8))\nsns.countplot(data.Education)","23a795b5":"plt.subplots(figsize=(10,8))\nsns.countplot(data.NumCompaniesWorked)","61b5e80a":"plt.subplots(figsize=(10,8))\nsns.countplot(data.PercentSalaryHike)","1ed8b8e5":"plt.subplots(figsize=(6,8))\nsns.countplot(x='BusinessTravel', data=data)","fd61a628":"plt.subplots(figsize=(10,8))\nsns.countplot(x='MaritalStatus', data=data)","ccf04379":"print(data.groupby(['Gender','MaritalStatus'])['MaritalStatus'].count())\nprint(data.groupby('Gender')['Gender'].count())","edca6aa0":"plt.figure(figsize=(8,8))\nplt.pie(data['JobRole'].value_counts(),labels=data['JobRole'].value_counts().index,autopct='%.2f%%');\nplt.title('Job Role Distribution',fontdict={'fontsize':22});","a7528cc6":"plt.subplots(figsize=(10,8))\nfig = plt.gcf()\nfig.set_size_inches(20,14)\nsns.countplot(x='JobRole', hue='Gender',data=data)\nplt.title('Job Role Between Male and Female')","4c55cd09":"plt.figure(figsize=(8,8))\nplt.pie(data['EducationField'].value_counts(),labels=data['EducationField'].value_counts().index,autopct='%.2f%%')","642ec64e":"plt.subplots(figsize=(10,8))\nsns.countplot(x='Department', data=data)","910f7e45":"plt.subplots(figsize=(10,8))\nsns.countplot(x='Department', hue='Attrition',data=data)","d3479d70":"plt.figure(figsize=(12, 9))\nsns.boxplot(x='PercentSalaryHike',y='Age',data=data,palette='winter')","6cade07b":"sns.set(font_scale=1)\nsns.boxplot(x='JobRole',y='MonthlyIncome',data=data)\nplt.xticks(rotation=90)","3e8d15d6":"sns.boxplot(x='EducationField',y='MonthlyIncome',data=data)\nplt.xticks(rotation=90)","e5fd2685":"data.corr()['Attrition'].sort_values(ascending=False)","cc54aaa9":"data.groupby(by='JobRole')[\"PercentSalaryHike\",\"YearsAtCompany\",\"TotalWorkingYears\",\"YearsInCurrentRole\",\"WorkLifeBalance\"].mean()","7dfd464a":"data.head()","bad41ce9":"data_obj=data.select_dtypes(include='object')\ndata_obj.head()","fe08a0dd":"from sklearn.preprocessing import LabelEncoder","02dba317":"le= LabelEncoder()","32794a7a":"categorical_col=[]\nfor col in data.columns:\n    if data[col].dtype== object and data[col].nunique()<=50:\n        categorical_col.append(col)\nprint(categorical_col)","b735d0af":"for col in categorical_col:\n    data[col]=le.fit_transform(data[col])","547e9968":"data.shape","66d777e2":"from sklearn.model_selection import train_test_split","7207e25d":"X= data.drop('Attrition',axis=1)\ny=data['Attrition']","4352c0cb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)","6c3d793e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_auc_score,roc_curve\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 33)\nprint(\"Train Set Size : \",X_train.shape)\nprint(\"Train Target Set Size : \",y_train.shape)\nprint(\"Test  Set Size : \",X_test.shape)\nprint(\"Test  Target Set Size : \",y_test.shape)","4b37a079":"from sklearn.tree import DecisionTreeClassifier","01a2e43b":"model= DecisionTreeClassifier()","585dc154":"model.fit(X_train,y_train)","5eb45350":"dir(model) #to select which all parameters are important to us","3e0f6536":"pred= model.predict(X_test)","7646f40f":"from sklearn.metrics import classification_report,confusion_matrix","bb34ea9f":"print(classification_report(y_test,pred))","c7e4a6af":"print(confusion_matrix(y_test,pred))","1086a7ad":"from sklearn.model_selection import RandomizedSearchCV","32e66f7c":"params={\"criterion\":(\"gini\", \"entropy\"),\n        \"splitter\":(\"best\", \"random\"), \n        \"max_depth\":(list(range(1, 20))), \n        \"min_samples_split\":[2, 3, 4], \n        \"min_samples_leaf\":list(range(1, 20))}","85d6ed8e":"tree_randomized= RandomizedSearchCV(model,params,n_iter=100,n_jobs=-1,cv=5,verbose=2)","59b57495":"tree_randomized.fit(X_train,y_train)","c16642f6":"tree_randomized.best_estimator_","6ab19a7d":"model=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                       max_depth=4, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=11, min_samples_split=4,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=42, splitter='random')","2340ac40":"model.fit(X_train,y_train)\npred=model.predict(X_test)","e2fd638c":"print(classification_report(y_test,pred))","c3c6a102":"print(confusion_matrix(y_test,pred))","b72b9bce":"from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor","83c41c20":"print(RandomForestClassifier())\nprint(RandomForestRegressor()) #check HP we can tune","7d0c10aa":"from sklearn.ensemble import RandomForestClassifier \nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning) #to let us that the default value for gridsearch is going to change in future release\nwarnings.filterwarnings('ignore', category=DeprecationWarning) #to let us know tyhe beahviour of gridsearchcv within test","9e16ddfc":"def print_results(results):\n    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n\n    means = results.cv_results_['mean_test_score']\n    stds = results.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n        print('{} (+\/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))","db1ffa94":"rf = RandomForestClassifier()\nparameters = {\n    'n_estimators': [5,50,250], 'max_depth':[2,4,8,16,32,None] #none will let it go as deep as it want\n}\n\ncv = GridSearchCV(rf, parameters, cv=5) #(modelobject, parameter dictionary, how many folds we want cv=5)\ncv.fit(X_train,y_train.values.ravel()) #training lables are stored as vector type, but we need array , hence .ravel()\n\nprint_results(cv)","42ed820d":"cv.best_estimator_","6c03b1bd":"rf= RandomForestClassifier(n_estimators=50,max_depth=32)\nrf.fit(X_train,y_train)\nrf_pred= rf.predict(X_test)\nprint(classification_report(y_test,rf_pred))","ac18694a":"print(confusion_matrix(y_test,rf_pred))","3ff33828":"from sklearn.svm import SVC","cdf18b2d":"SVC()# we only select ones that are imp - C and kernel","5abd7c6a":"dir(SVC)","f833fc4a":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 101)","8b6f1ccd":"clf = svm.SVC()\nclf.fit(X_train,y_train)\n\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf.score(X_test, y_test) * 100))","4286e8a8":"linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\naccuracy_lin_train = linear.score(X_train, y_train)\naccuracy_lin_test = linear.score(X_test, y_test)\nprint('Accuracy Linear Kernel on training set:', accuracy_lin_train*100)\nprint('Accuracy Linear Kernel on testing set:', accuracy_lin_test*100)","3c3345ba":"rbf = svm.SVC(kernel='rbf', gamma=0.1, C=0.1, decision_function_shape='ovo').fit(X_train, y_train)\naccuracy_rbf_train = rbf.score(X_train, y_train)\naccuracy_rbf_test = rbf.score(X_test, y_test)\nprint('Accuracy Radial Basis Kernel on training set:', accuracy_rbf_train*100)\nprint('Accuracy Radial Basis Kernel on testing set:', accuracy_rbf_test*100)","18d21842":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} ","15dbd945":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC","7d1dfd13":"grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)","3c10ab6f":"# May take awhile!\ngrid.fit(X_train,y_train)","95bc64c3":"grid.best_params_","883b57e9":"grid.best_estimator_","260d3905":"grid_predictions = grid.predict(X_test)","6c07f8cf":"print(confusion_matrix(y_test,grid_predictions))","761845b9":"print(classification_report(y_test,grid_predictions))","29ed382b":"data.columns","a949219e":"X= data.drop(['Attrition','BusinessTravel','DailyRate','Department','DistanceFromHome','Education','EmployeeNumber','Gender',\n             'HourlyRate','JobInvolvement','JobLevel','JobRole','JobSatisfaction','MaritalStatus',\n             'MonthlyRate','NumCompaniesWorked','OverTime','RelationshipSatisfaction','StockOptionLevel',\n              'TrainingTimesLastYear'],axis=1)\ny=data['Attrition']","274fe8d0":"data.head()","9f82b130":"X.columns","6619065c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)","1ac477b4":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_auc_score,roc_curve\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 101)\nprint(\"Train Set Size : \",X_train.shape)\nprint(\"Train Target Set Size : \",y_train.shape)\nprint(\"Test  Set Size : \",X_test.shape)\nprint(\"Test  Target Set Size : \",y_test.shape)","a8472bd5":"# Applying Scaling Standardiztion to all of the features in order to bring them into common scale .\n# Standardiztion : is preferred when most of the featues are not following gaussian distribution . \n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = pd.DataFrame(sc.fit_transform(X_train))\nX_test  = pd.DataFrame(sc.fit_transform(X_test))","4ea84d3e":"from sklearn.model_selection import GridSearchCV","f709cf60":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state = 42 )\n\n# Setting Parameters for Logistic Regression . \n\nparams = {    # Regularization Params\n             'penalty' : ['l1','l2','elasticnet'],\n              # Lambda Value \n             'C' : [0.01,0.1,1,10,100]\n         }\n\nlog_reg = GridSearchCV(lr,param_grid = params,cv = 10)\nlog_reg.fit(X_train,y_train)\nlog_reg.best_params_","edd9fdcb":"# Make Prediction of test data \ny_pred = log_reg.predict(X_test)\nprint(classification_report(y_test,y_pred))","4aef9b4e":"plt.rcParams['figure.figsize'] = (6,4)\nclass_names = [1,0]\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)), annot = True, cmap = 'BuGn_r',\n           fmt = 'g')\nplt.tight_layout()\nplt.title('Confusion matrix for Logistic Regression  Model', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","dc824144":"plt.rcParams['figure.figsize'] = (10,6)\n\n# Get predicted probabilites from the model\ny_proba = log_reg.predict_proba(X_test)[:,1]\n\n# display auc value for log_reg\nauc_log_reg = roc_auc_score(y_test,y_pred)\nprint(\"roc_auc_score value for log reg is : \",roc_auc_score(y_test,y_pred))\n\n# Create true and false positive rates\nfpr_log_reg,tpr_log_reg,thershold_log_reg_model = roc_curve(y_test,y_proba)\nplt.plot(fpr_log_reg,tpr_log_reg)\nplt.plot([0,1],ls='--')\n#plt.plot([0,0],[1,0],c='.5')\n#plt.plot([1,1],c='.5')\nplt.title('Reciever Operating Characterstic For Logistic Regregression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","238952ff":"from sklearn.tree import DecisionTreeClassifier \n\ndt = DecisionTreeClassifier(random_state = 42)\n\n\n# Setting Parameters for DecisionTreeClassifier . \n\nparams = {  \n             'criterion'    : [\"gini\", \"entropy\"],\n             'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n              'min_samples_split' :[i for i in range(4,16)],\n              'min_samples_leaf' : [i for i in range(4,16)]\n         }\n\ndt_clf = GridSearchCV(dt,param_grid = params,cv = 10)\ndt_clf.fit(X_train,y_train)\ndt_clf.best_params_","3bde0e49":"# Make Prediction of test data \ny_pred = dt_clf.predict(X_test)\nprint(classification_report(y_test,y_pred))","7a2d2030":"plt.rcParams['figure.figsize'] = (6,4)\nclass_names = [1,0]\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)), annot = True, cmap = 'BuGn_r',\n           fmt = 'g')\nplt.tight_layout()\nplt.title('Confusion matrix for DecisionTreeClassifier   Model', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","26c9e69f":"plt.rcParams['figure.figsize'] = (10,6)\n\n# Get predicted probabilites from the model\ny_proba = dt_clf.predict_proba(X_test)[:,1]\n\ndt_clf_auc_score = roc_auc_score(y_test,y_pred)\n# display auc value for DecisionTreeClassifier\nprint(\"roc_auc_score value for log reg is : \",roc_auc_score(y_test,y_pred))\n\n# Create true and false positive rates\nfpr_dt_clf,tpr_dt_clf,thershold_dt_clf_model = roc_curve(y_test,y_proba)\nplt.plot(fpr_dt_clf,tpr_dt_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.title('Reciever Operating Characterstic For DecisionTreeClassifier ')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","438d402a":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators = 150,min_samples_split = 20,min_samples_leaf = 5,random_state = 42)\nrf_clf.fit(X_train,y_train)\ny_pred = rf_clf.predict(X_test)\n","4df62a0c":"# Make Prediction of test data \ny_pred = rf_clf.predict(X_test)\nprint(classification_report(y_test,y_pred))","05526d94":"plt.rcParams['figure.figsize'] = (6,4)\nclass_names = [1,0]\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)), annot = True, cmap = 'BuGn_r',\n           fmt = 'g')\nplt.tight_layout()\nplt.title('Confusion matrix for RandomForestClassifier   Model', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","f8cd2d00":"plt.rcParams['figure.figsize'] = (10,6)\n\n# Get predicted probabilites from the model\ny_proba = dt_clf.predict_proba(X_test)[:,1]\n\nrf_auc_score = roc_auc_score(y_test,y_pred)\n\n# display auc value for RandomForestClassifier\nprint(\"roc_auc_score value for log reg is : \",roc_auc_score(y_test,y_pred))\n\n# Create true and false positive rates\nfpr_rf_clf,tpr_rf_clf,thershold_rf_clf_model = roc_curve(y_test,y_proba)\nplt.plot(fpr_rf_clf,tpr_rf_clf)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.title('Reciever Operating Characterstic For RandomForestClassifier ')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","aaa1c788":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_jobs = -1)\n\n# set params\n\nparams = {\n             \"n_neighbors\" : [i for i in range(15)],\n               'p' : [1,2] ,\n              'leaf_size' : [i for i in range(15)],\n               \n          }\nknn = GridSearchCV(knn,param_grid = params, cv = 5)\nknn.fit(X_train,y_train)\nknn.best_params_","a8876a7c":"# Make Prediction of test data \ny_pred = knn.predict(X_test)\nprint(classification_report(y_test,y_pred))","097dbd0f":"plt.rcParams['figure.figsize'] = (6,4)\nclass_names = [1,0]\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)), annot = True, cmap = 'BuGn_r',\n           fmt = 'g')\nplt.tight_layout()\nplt.title('Confusion matrix for KNN Algorithm   Model', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","c9acb6c6":"plt.rcParams['figure.figsize'] = (10,6)\n\n# Get predicted probabilites from the model\ny_proba = knn.predict_proba(X_test)[:,1]\n\nknn_auc_score = roc_auc_score(y_test,y_pred)\n\n\n# display auc value for KNN Algorithm\nprint(\"roc_auc_score value for log reg is : \",roc_auc_score(y_test,y_pred))\n\n# Create true and false positive rates\nfpr_KNN,tpr_KNN,thershold_KNN_model = roc_curve(y_test,y_proba)\nplt.plot(fpr_KNN,tpr_KNN)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.title('Reciever Operating Characterstic For KNN Algorithm ')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","d64407dd":"plt.figure(figsize=(10,6))\nplt.title('Reciever Operating Characterstic Curve')\nplt.plot(fpr_log_reg,tpr_log_reg,label='LogisticRegression')\nplt.plot(fpr_dt_clf,tpr_dt_clf,label='DecisionTreeClassifier')\nplt.plot(fpr_rf_clf,tpr_rf_clf,label='RandomForestClassifier')\nplt.plot(fpr_KNN,tpr_KNN,label='KNearestNeighbors ')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","bb3c0f0d":"print(\"Area Under Curve Score values for Different algorithms : \")\nprint(\"LogisticRegression          : \",auc_log_reg)\nprint(\"DecisionTreeClassfier       : \",dt_clf_auc_score)\nprint(\"RandomForest Classifier     : \",rf_auc_score)\nprint(\"KnearestNeighborsClassifier : \",knn_auc_score)","f4282964":"from sklearn.svm import SVC","6901e560":"scaler=StandardScaler()\nscaled_data=scaler.fit_transform(data.drop('Attrition',axis=1))\nX=scaled_data\ny=data['Attrition']","054d5baa":"x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","6130765a":"SVC()# we only select ones that are imp - C and kernel","a4e67c1a":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","2a12976c":"\nclf = svm.SVC()\nclf.fit(X_train,y_train)\n\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf.score(X_test, y_test) * 100))\n","9597c554":"from sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train,y_train)","2f3da2b9":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.1,1,10,100], 'gamma':[1,0.1,0.01,0.001]}","88ff9a0c":"grid = GridSearchCV(SVC(), param_grid, refit = True, verbose=3)\ngrid.fit(X_train, y_train)","4cc22885":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=150)\n\nclf = svm.SVC(C=1,gamma=0.01)\nclf.fit(X_train,y_train)\n\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf.score(X_test, y_test) * 100))","423e246e":"from sklearn.svm import SVC\nsvm = SVC(kernel='linear')\nsvm.fit(X_train,y_train)","eadaabeb":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.1,1,10,100], 'gamma':[1,0.1,0.01,0.001]}","6688255a":"grid = GridSearchCV(SVC(kernel='linear'), param_grid, refit = True, verbose=3)\ngrid.fit(X_train, y_train)","cd0c4a6f":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=150)\n\nclf = svm.SVC(kernel='linear',C=1,gamma=0.01)\nclf.fit(X_train,y_train)\n\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf.score(X_test, y_test) * 100))","ef25901d":"from sklearn.svm import SVC\nsvm = SVC(kernel='rbf')\nsvm.fit(X_train,y_train)","cc49f194":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.1,1,10,100], 'gamma':[1,0.1,0.01,0.001]}","a0504d06":"grid = GridSearchCV(SVC(kernel='rbf'), param_grid, refit = True, verbose=3)\ngrid.fit(X_train, y_train)","859e9bb8":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=150)\n\nclf = svm.SVC(kernel='rbf',C=1,gamma=0.01)\nclf.fit(X_train,y_train)\n\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf.score(X_train, y_train) * 100))\n\nprint('Accuracy of SVC on test set: {:.2f}'.format(clf.score(X_test, y_test) * 100))","b373c79f":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=150)","fb1788ec":"clf = svm.SVC(kernel='precomputed')","c8230ffc":"gram_train = np.dot(X_train, X_train.T)\nclf.fit(gram_train, y_train)","b44cf66c":"gram_test = np.dot(X_test, X_train.T)\nclf.predict(gram_test)","72c67619":"print('Accuracy of SVC on training set: {:.2f}'.format(clf.score(gram_train, y_train) * 100))\nprint('Accuracy of SVC on training set: {:.2f}'.format(clf.score(gram_test, y_test) * 100))","434662bf":"plt.rcParams['figure.figsize'] = (6,4)\nclass_names = [1,0]\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(confusion_matrix(y_test,y_pred)), annot = True, cmap = 'BuGn_r',\n           fmt = 'g')\nplt.tight_layout()\nplt.title('Confusion matrix for Logistic Regression  Model', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","b680b1a4":"from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor","e3d9dabd":"print(GradientBoostingClassifier())\nprint(GradientBoostingRegressor())","ad39af90":"from sklearn.ensemble import GradientBoostingClassifier\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning) #to let us that the default value for gridsearch is going to change in future release\nwarnings.filterwarnings('ignore', category=DeprecationWarning) #to let us know tyhe beahviour of gridsearchcv within test\n","c4e17d23":"gb = GradientBoostingClassifier()\nparameters = {\n    'n_estimators': [5,50,250,500], 'max_depth':[2,4,8,16,32],'learning_rate': [0.01,0.1,1,10,100]\n}\n\n\ncv = GridSearchCV(gb, parameters, cv=5) #(modelobject, parameter dictionary, how many folds we want cv=5)\ncv.fit(X_train,y_train.values.ravel()) #training lables are stored as vector type, but we need array , hence .ravel()\n\nprint_results(cv)","e4e89253":"cv.best_estimator_","7dfcf5bb":"# KNN:","47c26c8c":"# Henceforth, we will be choosing only a few important attributes and check them on other algorithms","d131c00c":"# 7. Is business travelling a part of work life at IBM ?","d7b9ac1f":"# Critical attributes w.r.t Job role","b8dba494":"# 11. Which education field is commonly noticed amongst IBM employees?","275fdc8c":"# Checking co-relation of Attrition with other attributes :","f037b31c":"# let's have a look at numerical and categorical types","338c2b5b":"# 3. What is the relation between performance rating and attrition ?","56d78794":"# 2. What is the relation between Age and monthly income ?","dbcf0c63":"# Decision Tree Classifier :","e82c5dea":"# Visualisations ","5f3c5bc4":"# Gradient Boosting Classifier:","8c572691":"#n_estimators (how many indiviual trees can be built) and max depth(how deep can the tree go ) to consider","9342afac":"# 1. What is the age range of employees at IBM ?","081c8176":"# 50% of employees from each gender are Divorced ","e08fefe2":"# 8. What is the relationship status of IBM employees in general ?","dd84f211":"# Observations:\n1. The average age of employees at IBM is 39, which means while hiring, they prefer candidates with decent work experience and expect higher level of expertise.\n2. The average salary hike for employees is 15% with maximum being 25%. With decent salary hike in the organisation, employees tend to stay longer at the company and tend to enjoy long-term benefits with job security. This means, IBM rewards it's employees for their performance. This is proporational to employee satisfaction.\n3. However , the average Employee satisfaction stands at 2.7 out of 5.\n4. Most of the employees who get into IBM have worked with 2 or 3 companies in the past.\n5. On an average, an employee has worked at IBM for around 11 years and there seems to be an outlier - wherein an employee has worked for 38 years.\n6. It takes around 2 years for an IBM employee to bag his\/her next promotion at the workplace.","7613c1cb":"# Observations from co-relation matrix:\n1. Age and Total working years seem to have a good correlation of around 68%. \n2. Job level and Monthly income have around 95% co-relation ,which is evident. It also has around 51% correlation with Age, 78% with Total Working years and 53% with years spent in the company.\n3. Percent Salary hike and Performance Rating have around 7&%.\n4. years at the company seem to have a strong 77% co-relation with Years in current role and Years with current manager.\n","e7fcc989":"# 12. Which department has maximum employees employed with them ?","9bc63376":"# 5. With how many companies have the employees worked in the past?","018c5a04":"# 15. What is the montly income as per the job role ?","592bd6a2":"NOW, WE WILL BE TUNING THE HYPERPARAMETERS OF DECISION TREE USING RANDOMIZED SEARCH CROSS VALIDATION METHOD FOR IMPROVING THE ACCURACY OF THE MODEL.","0f9109a8":"# Applying Linear SVM","a7238114":"# Decision Tree Classifier","253507c0":"# Logistic Regression:","7423c8e5":"# 16. What is the monthly income as per the Education field ?","b56b20b3":"# Encoding the categorical columns  ","39493e75":"# 6. How many employees receive what percent salary hike at IBM ?","b05bc5c6":"# SVM:","cb0bfb27":"# 4. What level of education does IBM employees generally have ?","7bbdef28":"# 9. What are the most active job roles at IBM ?\n","beafd17f":"# 13. Which department witnesses maximum Attrition ?","51c1ac80":"# Apply Gram Matrix","012f0baa":"# 10. Does a particular Gender dominate a Job role ?","ef614d1a":"# Support Vector Machine ","2e8e47fb":"# Grid Search on Linear SVM","dd277413":"# Grid Search ","98ca8b9c":"# Grid Search on rbf SVM","09c31f4e":"# Appyling RBF SVM","656c8266":"# Random Forest Classifier :","c8d738c5":"# Observations:\n1. In Attribution distribution diagram, it can be seem around 83.9% or 1233 (out of 1470 employees) dont think of leaving the organisation or are not at the risk of losing their employment.\n2. Around 16.1% or 237 (out of 1470 employees) are either thinking of leaving the organisation or are at the risk of losing their employment.","ca4384a0":"# 14. Which age range demands what kind of Salary hike ?","6887b83f":"# Random Forest Classifier:","b5a6d606":"# Grid Search"}}