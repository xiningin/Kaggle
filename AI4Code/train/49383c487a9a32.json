{"cell_type":{"cda849e1":"code","804cb4db":"code","f1118e7e":"code","508c36ea":"code","b935b8ca":"code","7caa8cb2":"code","858efe23":"code","38cc5e3b":"code","120b2ebf":"code","b752c241":"code","005a3174":"code","0956fded":"code","d4e66f7c":"code","73d18754":"code","1b2cc03a":"code","f71bb503":"code","db49e46f":"code","bf62a311":"code","4be7bbb2":"code","9f1dea08":"code","91ccc721":"code","71db1fa9":"code","f2ee24d1":"code","fd86ddc3":"code","01a20824":"code","2ba191d3":"code","d60305b2":"code","810e7fd4":"code","0541305f":"code","a92b9d37":"code","71580944":"code","3864f0eb":"code","97aa1ffe":"code","ca3ff728":"code","37b87e71":"code","db9b5a23":"code","bf97086f":"code","06f6a2ce":"code","c4915544":"code","aa15e76b":"code","ae6752f0":"code","8679051d":"markdown","a96eb28b":"markdown","d63716ab":"markdown","6cc2f9e9":"markdown","dc8dc12d":"markdown","b3dce88c":"markdown","5f629999":"markdown","2b9e6682":"markdown","43ffa7f9":"markdown","f4a94b94":"markdown","cd65a693":"markdown","152e8118":"markdown","a6f62fa7":"markdown","3b19fc89":"markdown","99796d09":"markdown","d47c2365":"markdown"},"source":{"cda849e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt # Visualisation\n%matplotlib inline\n\nfrom wordcloud import WordCloud\n\nfrom sklearn import preprocessing\nfrom sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n","804cb4db":"# import the csv file into a pandas dataframe with read_csv\ndata = pd.read_csv('..\/input\/train.csv', index_col='PassengerId')\ndata.head()","f1118e7e":"data.info()","508c36ea":"data.describe(include='all')","b935b8ca":"data['title'] = data['Name'].apply(lambda name: name.split(', ')[1].split('.')[0])\ndata['title'].value_counts()","7caa8cb2":"text_title = ' '.join(data['title'].fillna('').values)\nwordcloud = WordCloud(max_font_size=None, background_color='white',\n                      width=1200, height=1000).generate(text_title)\nplt.imshow(wordcloud)\nplt.title('Top Title')\nplt.axis(\"off\")","858efe23":"data.drop(['Name','Ticket','Cabin'], axis=1, inplace = True)\ndata.dropna(axis=0, subset=['Age'], inplace = True)\ndata.describe(include='all')","38cc5e3b":"data.isnull().values.any()","120b2ebf":"data['Embarked'].fillna('S', inplace = True) \ndata.head()","b752c241":"data.isnull().values.any()","005a3174":"se = preprocessing.LabelEncoder()\nse.fit(data['Sex'])\nprint(list(se.classes_))\ndata['Sex_encode'] = se.transform(data['Sex']) \n\nee = preprocessing.LabelEncoder()\nee.fit(data['Embarked'])\nprint(list(ee.classes_))\ndata['Embarked_encode'] = ee.transform(data['Embarked']) \n\nte = preprocessing.LabelEncoder()\nte.fit(data['title'])\nprint(list(te.classes_))\ndata['title_encode'] = te.transform(data['title']) ","0956fded":"data.head()","d4e66f7c":"data.drop(['Sex','Embarked','title'], axis=1, inplace = True)\ndata.head()","73d18754":"Y = data.iloc[:,0].values\nX = data.iloc[:,1:].values.astype('float64') # convertion of all type to float in order to don't have DataConvertionWarning during data procession","1b2cc03a":"print(\"Y shape {}\".format(Y.shape))\nprint(\"X shape {}\".format(X.shape))","f71bb503":"std_scale = preprocessing.StandardScaler().fit(X)\nX_scaled = std_scale.transform(X)","db49e46f":"pca = decomposition.PCA(n_components=2)\npca.fit(X_scaled)","bf62a311":"print(pca.explained_variance_ratio_)\nprint(pca.explained_variance_ratio_.sum())","4be7bbb2":"X_projected = pca.transform(X_scaled)\n\nplt.scatter(X_projected[:, 0], X_projected[:, 1],\n    c= Y)\nplt.show()","9f1dea08":"pcs = pca.components_\n\nfor i, (x, y) in enumerate(zip(pcs[0, :], pcs[1, :])):\n    plt.plot([0, x], [0, y], color='k')\n    plt.text(x, y, data.columns[i+1].replace(\"_encode\",\"\"), fontsize='14')\n\nplt.plot([-0.7, 0.7], [0, 0], color='grey', ls='--')\n\nplt.plot([0, 0], [-0.7, 0.7], color='grey', ls='--')\n\n\nplt.show()","91ccc721":"model = XGBClassifier()\nmodel.fit(X, Y)\n# feature importance\nprint(model.feature_importances_)\n# plot\nfeature_names = [f.replace(\"_encode\",\"\") for f in data.columns[1:]]\nplt.bar(feature_names, model.feature_importances_)\nplt.show()","71db1fa9":"plot_importance(model)","f2ee24d1":"corr = data.corr()\nlabels = [f.replace(\"_encode\",\"\") for f in data.columns]\nplt.figure(figsize=(10, 10))\nplt.imshow(corr, cmap='RdYlGn', interpolation='none', aspect='auto')\nplt.colorbar()\nplt.xticks(range(len(corr)), labels, rotation='vertical')\nplt.yticks(range(len(corr)), labels);\nplt.suptitle('Titanic Correlations Heat Map', fontsize=15, fontweight='bold')\nplt.show()","fd86ddc3":"xgb_clf = XGBClassifier()\n#clf.fit(X, Y)\nscores = cross_val_score(xgb_clf, X, Y, cv=5)\nprint(\"XGB: {:.4f}\".format(scores.mean()))","01a20824":"rf_clf = RandomForestClassifier(n_estimators = 100)\n#clf.fit(X, Y)\nscores = cross_val_score(rf_clf, X, Y, cv=5)\nprint(\"RandomForest {:.4f}\".format(scores.mean()))","2ba191d3":"et_clf = ExtraTreesClassifier(n_estimators = 1000)\n#clf.fit(X, Y)\nscores = cross_val_score(et_clf, X, Y, cv=5)\nprint(\" ExtraTrees {:.4f}\".format(scores.mean()))","d60305b2":"ab_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 5),n_estimators = 1000)\n#clf.fit(X, Y)\nscores = cross_val_score(ab_clf, X, Y, cv=5)\nprint(\"AdaBoost {:.4f}\".format(scores.mean()))","810e7fd4":"gb_clf = GradientBoostingClassifier(n_estimators = 100)\n#clf.fit(X, Y)\nscores = cross_val_score(gb_clf, X, Y, cv=5)\nprint(\"GradientBoosting {:.4f}\".format(scores.mean()))","0541305f":"test = pd.read_csv('..\/input\/test.csv', index_col='PassengerId')\ntest.head()","a92b9d37":"test.describe(include='all')","71580944":"X_age = np.delete(X,1,1)\ny_age = X[:,1]\nX_fare = np.delete(X,4,1)\ny_fare = X[:,4]","3864f0eb":"xgb_age = XGBRegressor()\nxgb_age.fit(X_age,y_age)\npredictions = xgb_age.predict(X_age)\nprint(explained_variance_score(predictions,y_age))","97aa1ffe":"xgb_fare = XGBRegressor()\nxgb_fare.fit(X_fare,y_fare)\npredictions = xgb_fare.predict(X_fare)\nprint(explained_variance_score(predictions,y_fare))","ca3ff728":"test['title'] = test['Name'].apply(lambda name: name.split(', ')[1].split('.')[0])\ntest['title'].unique()","37b87e71":"test.replace('Dona','Don', inplace = True)\ntest['title'].unique()","db9b5a23":"test.drop(['Name','Ticket','Cabin'], axis=1, inplace = True)\ntest['Sex_encode'] = se.transform(test['Sex']) \ntest['Embarked_encode'] = ee.transform(test['Embarked'])\ntest['title_encode'] = te.transform(test['title'])\ntest.drop(['Sex','Embarked','title'], axis=1, inplace = True)\ntest.describe(include='all')\n","bf97086f":"without_age = test[test['Age'].isnull()]\nfor idx,row in without_age.iterrows():\n    age = xgb_age.predict([np.delete(row.values,1,0)])\n    test.at[idx, 'Age'] = age\n\n    \nwithout_fare = test[test['Fare'].isnull()]\nfor idx,row in without_fare.iterrows():\n    fare = xgb_fare.predict([np.delete(row.values,4,0)])\n    test.at[idx, 'Fare'] = fare","06f6a2ce":"test.describe(include='all')","c4915544":"model = GradientBoostingClassifier(n_estimators = 100)\nmodel.fit(X, Y)\ny_pred = model.predict(test.values)","aa15e76b":"df = pd.DataFrame(y_pred, columns=['Survived'])\ndf.index.name='PassengerId'\ndf.index = test.index\ndf.to_csv('submission.csv', header=True)","ae6752f0":"df.head(20)","8679051d":"Now, we have remve all empy data, we are ready to explore the dataset. First, we will encode the Label data with sklearn preprocessing LabelEncoder","a96eb28b":"## <span style=\"color:#5F9EA0\">Step 3: Data Exploration<\/span>\n### <span style=\"color:#5F9EA0\">convert data for exploration<\/span>","d63716ab":"We clean the data, but we have still some empty data in Embarked. Southampton is the most common value, so I fill the empty Embarked with this value","6cc2f9e9":"<span style=\"color:\t#A9A9A9\">Usefull python librairies import<\/span>","dc8dc12d":"GradientBoosting gie the best score from the train dataset","b3dce88c":"    Now, we will read the test data to participate to the competetions","5f629999":"## <span style=\"color:#5F9EA0\">Step 2: Feature Analysis<\/span>\n### <span style=\"color:#5F9EA0\">Clean the data<\/span>\nData are missing for:\n- Age (714\/891)\n- Cabin (204\/891)\n- Embarked (889\/891)\nName will be not use in this analysis","2b9e6682":"## <span style=\"color:#5F9EA0\">Step 1: Data Cleaning<\/span>\n### <span style=\"color:#5F9EA0\">Import the csv file and look the content<\/span>","43ffa7f9":"Now, we will clean the dataset. Name is not usefull, except perhaps to know the embarked port from kind of name (if we could detect the nationality). Ticket also don't had any inforamtion (Embarked?). I remove cabin also, for my first try. There is room for improvement with this data. We will clean also row without age..","f4a94b94":"| **Variable** | **Definition**                             | **Key**                                        |\n| :----------: | :----------------------------------------- | ---------------------------------------------- |\n|   survival   | Survival                                   | 0 = No, 1 = Yes                                |\n|    pclass    | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n|     sex      | Sex                                        |                                                |\n|     Age      | Age in years                               |                                                |\n|    sibsp     | # of siblings \/ spouses aboard the Titanic |                                                |\n|    parch     | # of parents \/ children aboard the Titanic |                                                |\n|    ticket    | Ticket number                              |                                                |\n|     fare     | Passenger fare                             |                                                |\n|    cabin     | Cabin number                               |                                                |\n|   embarked   | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |","cd65a693":"# <span style=\"color:#5F9EA0\">Titanic Survival Prediction<\/span>\n--------------------------------","152e8118":"The GradientBoosting classifier wihtout optimization gives the best result.","a6f62fa7":"Now, we will prepare the data for processing","3b19fc89":"Dona is a new title, so we can replace Dona by Don","99796d09":"We will try differents classifiers\n* XGBoost\n* RandomForest\n* ExtraTrees\n* AdaBoost\n* GradientBoosting\n\n","d47c2365":"In the test dataset, some age data are missing and one fFare also, we will make regressor to guess the missing values"}}