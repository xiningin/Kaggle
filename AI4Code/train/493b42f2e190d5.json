{"cell_type":{"c8d9a988":"code","652653fc":"code","d5ee40d5":"code","88f97c68":"code","1b2d0741":"code","981a2422":"code","e6f834c1":"code","b6a911f8":"code","af104dee":"code","bdcc0007":"code","bed118bb":"code","b051262d":"markdown","8e29443e":"markdown","e8bdb836":"markdown","70c8e129":"markdown","5d9486bf":"markdown","df81bdde":"markdown","528aeaa1":"markdown","2fe5f507":"markdown","5ef92f95":"markdown","2cd3f312":"markdown","24ce26bb":"markdown","1f42bbd0":"markdown","35a2e24c":"markdown"},"source":{"c8d9a988":"import tensorflow as tf\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image","652653fc":"def tensor_to_image(tensor):\n\n    tensor = tensor*255\n    tensor = np.array(tensor, dtype=np.uint8)\n    if np.ndim(tensor)>3:\n        assert tensor.shape[0] == 1\n        tensor = tensor[0]\n    return PIL.Image.fromarray(tensor)","d5ee40d5":"content_path_orig = '..\/input\/gan-getting-started\/photo_jpg\/00068bc07f.jpg'\nstyle_path_orig = '..\/input\/gan-getting-started\/monet_jpg\/000c1e3bff.jpg'","88f97c68":"def load_img(path_to_img):\n    max_dim = 256\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img","1b2d0741":"def imshow(image, title=None):\n    # convert tensor elements to 0 - 255 values\n    image = image * 255\n    \n    # Remove batch dimension of tensor\n    if len(image.shape) > 3:\n        out = np.squeeze(image, axis=0)\n     \n    out = out.astype('uint8')\n    plt.imshow(out)\n    if title:\n        plt.title(title)\n    plt.imshow(out)","981a2422":"plt.figure(figsize=(10,10))\n\ncontent_image_orig = load_img(content_path_orig)\nstyle_image_orig = load_img(style_path_orig)\n\nplt.subplot(1, 2, 1)\nimshow(content_image_orig, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image_orig, 'Style Image')","e6f834c1":"import tensorflow_hub as hub\n# This is version is updated from the original notebook (\/2 vs. \/1)\n# Results are greatly improved\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/2')","b6a911f8":"stylized_image_orig = hub_module(tf.constant(content_image_orig), tf.constant(style_image_orig))[0]\nplt.figure(figsize=(15,15))\n\nplt.subplot(1, 3, 1)\nimshow(content_image_orig, 'Content Image')\n\nplt.subplot(1, 3, 2)\nimshow(stylized_image_orig, 'Result')\n\nplt.subplot(1, 3, 3)\nimshow(style_image_orig, 'Style Image')","af104dee":"style_path_flip = '..\/input\/gan-getting-started\/photo_jpg\/00068bc07f.jpg'\ncontent_path_flip = '..\/input\/gan-getting-started\/monet_jpg\/000c1e3bff.jpg'\n\ncontent_image_flip = load_img(content_path_flip)\nstyle_image_flip = load_img(style_path_flip)\n\nstylized_image_flip = hub_module(tf.constant(content_image_flip), tf.constant(style_image_flip))[0]\nplt.figure(figsize=(15,15))\n\nplt.subplot(1, 3, 1)\nimshow(content_image_flip, 'Content Image')\n\nplt.subplot(1, 3, 2)\nimshow(stylized_image_flip, 'Result')\n\nplt.subplot(1, 3, 3)\nimshow(style_image_flip, 'Style Image')","bdcc0007":"from os import listdir\nfrom os.path import isfile, join\n\n\nphoto_path = '..\/input\/gan-getting-started\/photo_jpg\/'\nphoto_files = [f for f in listdir(photo_path) if isfile(join(photo_path, f))]\n\nmonet_path = '..\/input\/gan-getting-started\/monet_jpg\/'\nmonet_files = [f for f in listdir(monet_path) if isfile(join(monet_path, f))]\nmonet_count = len(monet_files)","bed118bb":"from random import randrange\n\n\n# Get random photo\ncontent_path1 = photo_path + photo_files[randrange(len(photo_files))]\nstyle_path1 = monet_path + monet_files[randrange(len(monet_files))]\n\ncontent_image1 = load_img(content_path1)\nstyle_image1 = load_img(style_path1)\n\ncontent_image2 = style_image1\nstyle_image2 = content_image1\n\nstylized_image1 = hub_module(tf.constant(content_image1), tf.constant(style_image1))[0]\nstylized_image2 = hub_module(tf.constant(content_image2), tf.constant(style_image2))[0]\n\nplt.figure(figsize=(20,20))\nplt.subplots_adjust(bottom=0.3, top=0.7, hspace=0.25, right=0.8, wspace=0.3)\n\nplt.subplot(2, 3, 1)\nimshow(content_image1, 'Content Image')\n\nplt.subplot(2, 3, 2)\nimshow(stylized_image1, 'Result')\n\nplt.subplot(2, 3, 3)\nimshow(style_image1, 'Style Image')\n\nplt.subplot(2, 3, 4)\nimshow(content_image2, 'Content Image')\n\nplt.subplot(2, 3, 5)\nimshow(stylized_image2, 'Result')\n\nplt.subplot(2, 3, 6)\nimshow(style_image2, 'Style Image')","b051262d":"# Explore Further\n\nThe code below selects a random photo and a random Monet and displays results for when Monet is the style and for when his work is the content. To learn more on video -- the BBC television series \"Fake or Fortune\" Season 1, Episode 1 investigates a painting that might be by Monet (available on at least one major streaming service in the U.S. and possibly available in other countries too).","8e29443e":"### Import and configure modules","e8bdb836":"### Create a simple function to display an image:","70c8e129":"## Fast Style Transfer using TF-Hub\n\n### Let's download the [TensorFlow Hub style transfer model](https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/2) -- just 81.66 MB.\n### There are versions for mobile devices too!","5d9486bf":"### Choose a content image and a style image (1st Photo and 1st Monet):","df81bdde":"# Neural style transfer\n\nThis notebook is based on the first portion of the 2018 TensorFlow Authors [Neural Style Transfer](https:\/\/www.tensorflow.org\/tutorials\/generative\/style_transfer) tutorial, the notebook from which was released under the [Apache License, version 2.0](https:\/\/www.apache.org\/licenses\/LICENSE-2.0). I have made a few modifications, mostly related to data loading from a local filepath vs. a web url.  You do not need a GPU or TPU to get good results fast.","528aeaa1":"### Define a function to load an image and limit its maximum dimension to 256 pixels.","2fe5f507":"Wow! Now that we have a modern photographer on the cliffs instead of Monet, we get to see the breaking waves and the clouds in the sky. Those weren't in Monet's painting, or were they?","5ef92f95":"### Select random photo and random Monet, then stylize and display","2cd3f312":"### Run the model and display result","24ce26bb":"### Load image filenames","1f42bbd0":"## Is it a Monet?\n\nIf Claude Monet had set up his easel in the field of the content image, would he have painted something like the result image?  Although the color palette looks pretty close to the style image, the style itself seems off.  In Monet's painting (which might be *At Val-Saint-Nicholas near Dieppe (Morning)*, 1897 from the Phillips Collection in Washington, DC), the sea, sky, and cliffs are indistinct. Yet, in the result image, the mountains, clouds, and field are fairly detailed. Have we gotten it wrong?\n\n<p><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/38\/Monet_-_Valle_Buona%2C_Near_Bordighera%2C_1884.jpg\/640px-Monet_-_Valle_Buona%2C_Near_Bordighera%2C_1884.jpg\" style=\"width:512px\" \/><\/p>\n\nThis is Monet's *Valle Buona Near Bordighera*, 1884 (photo credit: Jerry Ward \/ Dallas Museum of Art, via [Wikimedia Commons](https:\/\/commons.m.wikimedia.org\/wiki\/File:Monet_-_Valle_Buona,_Near_Bordighera,_1884.jpg), license [Creative Commons Attribution-ShareAlike 4.0 International](https:\/\/creativecommons.org\/licenses\/by-sa\/4.0\/deed.en)). Actually, the TensorFlow Hub style transfer model predicted very well how Monet might have painted the scene in the content image, but how? Perhaps it's because, the model was trained on arbitrary images which probably included Impressionist paintings, and the Impressionists painted plenty of detailed fields.  So, are we to conclude that content informs style?  Let's flip the script...","35a2e24c":"## Visualize the input"}}