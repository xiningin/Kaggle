{"cell_type":{"190d711c":"code","e733aa38":"code","89651431":"code","83f4ebcf":"code","2e99230d":"code","197a779b":"code","772827ba":"code","db7d598a":"code","056fd614":"code","44de5af1":"code","93425953":"code","d287f8c6":"code","47bef13f":"code","4bd2e82e":"code","9ecd763b":"code","b2d5d5ff":"code","d7c6b1b4":"code","cb96a800":"code","53f83839":"code","446c4353":"code","4445dc32":"code","4f9eb60b":"code","2eeb712b":"code","477e9c07":"markdown","e5bbb7ef":"markdown","cd07a398":"markdown","68c6761f":"markdown","f8ba49b2":"markdown","770a86f4":"markdown","fe7955fb":"markdown","a2df40f6":"markdown","c9bf0790":"markdown","94f16615":"markdown","4c5d7f2e":"markdown","4768ab22":"markdown","24cbc3f5":"markdown","c1b04239":"markdown","9082d504":"markdown","76582e6a":"markdown","0e1e659b":"markdown","4ddc76a9":"markdown","ecf3afc4":"markdown","b95b9f1b":"markdown","26f84b2c":"markdown","1439aa01":"markdown","2a023bf0":"markdown","a88415e9":"markdown","656039ac":"markdown","92b4ff5d":"markdown","5127c641":"markdown","e3c8e18b":"markdown"},"source":{"190d711c":"##################################################\n# Imports\n##################################################\n\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import entropy\n\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '\/kaggle\/input\/image-classification-fashion-mnist'\n\n# Set seed\nnp.random.seed(0)\n\n# Adjust the font type of the plots with the one in the document\nplt.rcParams[\"font.family\"] = \"serif\"","e733aa38":"##################################################\n# Load dataset\n##################################################\n\nx_train = np.load(os.path.join(DATA_BASE_FOLDER, 'train.npy'))\nx_valid = np.load(os.path.join(DATA_BASE_FOLDER, 'validation.npy'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy'))\ny_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))['class'].values\ny_valid = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))['class'].values\ny_labels = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# Plot random images of different classes\nplt.figure(figsize=(25, 5))\nfor idx in range(20):\n    plt.subplot(1, 20, idx + 1)\n    img = x_train[idx].reshape(28, 28)\n    plt.title(f'{y_labels[y_train[idx]]}')\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\nplt.show()","89651431":"x_train_df = pd.DataFrame(x_train)\nx_valid_df = pd.DataFrame(x_valid)\nx_test_df = pd.DataFrame(x_test)","83f4ebcf":"print(x_train_df.isna().sum().sum())","2e99230d":"print(x_train_df.shape, \n      x_valid_df.shape, \n      x_test.shape, \n      y_train.shape, \n      y_valid.shape)","197a779b":"unique, counts = np.unique(y_train, return_counts=True) # Count the unique values\n\ny_class_fr = pd.DataFrame(counts, index = y_labels)\ny_class_fr = y_class_fr.rename(columns={0: \"Count\"})   # Rename the columns\n\ndisplay(y_class_fr.transpose())                     # Display the count\n\ny_class_fr.plot.bar(legend = False)     # Create a barplot","772827ba":"entropy = pd.Series(entropy(x_train))              #compute entropy for each parameter\n\nentropy = entropy.reset_index()                    #reset the index\n\nentropy = entropy.sort_values(ascending = False, by = 0)    #sort the values from the biggest to the smallest\n\nentropy = entropy.rename(columns={0: \"Entropy\", \"index\": \"Index\"})   #rename the columns","db7d598a":"#Scale the features\nScaler = MinMaxScaler()\n\nx_train_scaled = Scaler.fit_transform(x_train_df)\nx_valid_scaled = Scaler.fit_transform(x_valid_df)\nx_test_scaled = Scaler.fit_transform(x_test_df)","056fd614":"# This is the final optimized model, we will use this from the start in order to speed up the computing time\n\nmodel = KNeighborsClassifier(n_neighbors = 4, \n                             weights = 'distance',\n                             algorithm = 'brute', \n                             p = 2)","44de5af1":"# Compute the accuracy of the model\n\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()","93425953":"# Evaluate the model and append the accuracy and speed to the given lists\n# N is the number of observations\n# D is the number of parameters\n# V is the number of values tried\n\ndef execute_model_evaluation (model, x_train, y_train, x_valid, accuracy_list, speed_list):\n    '''\n    input model: sklearn kNN model\n    input x_train: pd.DataFrame of shape (N, D)\n    input y_train: ndarray of shape (N,)\n    input x_valid: pd.DataFrame of shape (N, D)\n    input accuracy_list: list of length (V)\n    input speed_list: list of legnth (V)\n    '''\n\n    # Start the time keeping\n    starting_time = time.process_time()                               \n    \n    # Fit the model with the training dataset\n    model.fit(x_train, y_train)                                       \n        \n    # Predict the new values with the model\n    y_pred = model.predict(x_valid)                                   \n\n    # End the time keeping\n    finishing_time = time.process_time()                              \n    \n    # Compute the time\n    iteration_speed = round(finishing_time - starting_time, 3)        \n    \n    # Append the time to the speed list\n    speed_list.append(iteration_speed)                                \n    \n    # Compute the accuracy\n    iteration_accuracy = round(accuracy(y_pred, y_valid) * 100, 2)    \n    \n    # Append the accuracy to the accuracy list\n    accuracy_list.append(iteration_accuracy)                          ","d287f8c6":"# Create a line plot of accuracy and speed\n# V is the number of values tried for the model\n\ndef accuracy_speed_plot(x_axis, accuracy_list, speed_list):\n    '''\n    input x_axis: ndarray of shape (V,)\n    input accuracy_list: list of length (V)\n    input speed_list: list of length (V)\n    '''\n    fig, (ax1, ax2) = plt.subplots(2,                           # Create two subplots\n                                   sharex = True,               # Remove the common inside x-axis\n                                   figsize=(12,8))              # Adjust the figure size\n\n    fig.suptitle('Accuracy in % (Top) and Speed in s (Down)', fontsize=24, fontfamily = \"serif\")   # Add the plot title\n\n    ax1.plot(x_axis, accuracy_list, linewidth = 2)   # Plot the accuracy list\n\n    ax2.plot(x_axis, speed_list, linewidth = 2)      # Plot the speed list\n    \n    ax1.tick_params(axis='y', labelsize=14)          # Adjust the y label font size\n    ax2.tick_params(axis='y', labelsize=14)          # Adjust the y label font size\n    ax2.tick_params(axis='x', labelsize=16)          # Adjust the x label font size","47bef13f":"# Create a bar plot of accuracy and speed\n# V is the number of values tried for the model\n\ndef accuracy_speed_barplot(x_axis, accuracy_list, speed_list):\n    '''\n    input x_axis: ndarray of shape (V,)\n    input accuracy_list: list of length (V)\n    input speed_list: list of length (V)\n    '''\n    fig, (ax1, ax2) = plt.subplots(2,                          # Create two subplots\n                                   sharex = True,              # Remove the common inside x-axis\n                                   figsize=(12,8))             # Adjust the figure size\n\n    fig.suptitle('Accuracy in % (Top) and Speed in s (Down)', fontsize=24, fontfamily = \"serif\")  # Add the plot title\n\n    ax1.bar(x_axis, accuracy_list)                             # Plot the accuracy list\n\n    ax2.bar(x_axis, speed_list)                                # Plot the speed list\n\n    ax1.tick_params(axis='y', labelsize=14)                    # Adjust the y label font size\n    ax2.tick_params(axis='y', labelsize=14)                    # Adjust the y label font size\n    ax2.tick_params(axis='x', labelsize=16)                    # Adjust the x label font size","4bd2e82e":"# Create a cool table with the accuracy and the speed for each try\n# V is the number of values tried for the model\n\ndef cool_table (values, values_name, accuracy_list, speed_list):\n    '''\n    input values: ndarray of shape (V,)\n    input values_name: string\n    input accuracy_list: list of length (V)\n    input speed_list: list of length (V)\n    output tbl: pd.DataFrame of shape(V, 3)\n    '''\n    \n    # Create a dataframe with the value tested, the accuracy and the speed\n    tbl = pd.DataFrame({'Accuracy': accuracy_list,\n                         'Time': speed_list},\n                      index= values)\n    \n    tbl.index.name = values_name                    # Rename the index\n    \n    return tbl","9ecd763b":"# Inizialize the two kpi lists\naccuracy_features, speed_features = [], []         \n\n#Select the desired number of features we want to test\nfeatures_numbers = np.array([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, x_train_scaled.shape[1]])\n\nfor features_number in features_numbers:\n    \n    # Select the number of features\n    parameters_list = entropy.head(features_number).Index                 \n    \n    # Create a new training df with only that features\n    x_train_param = pd.DataFrame(x_train_scaled)[parameters_list]         \n    \n    # Create a new validation df with only that features\n    x_valid_param = pd.DataFrame(x_valid_scaled)[parameters_list]         \n    \n    execute_model_evaluation(model, \n                             x_train_param, y_train, \n                             x_valid_param, \n                             accuracy_features, speed_features)\n    \n\nfeatures_table = cool_table(features_numbers, '# of Features', \n                            accuracy_features, speed_features)\ndisplay('Time to run the cell: ' + str(round(sum(speed_features), 2)) + ' s')\ndisplay(features_table)\n\naccuracy_speed_plot (features_numbers, \n                     accuracy_features, speed_features)","b2d5d5ff":"# Select the 128 features with the highest entropy\nparameters_list = entropy.head(128).Index                  \n\n# Filter the features selected from the training set\nx_train = pd.DataFrame(x_train_scaled)[parameters_list]    \n\n# Filter the features selected from the validation set\nx_valid = pd.DataFrame(x_valid_scaled)[parameters_list]    ","d7c6b1b4":"accuracy_neighbors, speed_neighbors = [], []\n\nneighbors_fitted = [1, 2, 3, 4, 5, 6, 7, 10, 13, 16]\n\nfor neighbors_trial in neighbors_fitted:\n    \n    model = KNeighborsClassifier(n_neighbors = neighbors_trial,\n                                 weights = 'distance', \n                                 algorithm = 'brute', \n                                 p = 2)\n    \n    execute_model_evaluation(model, \n                             x_train, y_train, \n                             x_valid, \n                             accuracy_neighbors, speed_neighbors)\n\n    \nneighbors_table = cool_table(neighbors_fitted, '# of Neighbors', \n                             accuracy_neighbors, speed_neighbors)\n\ndisplay('Time to run the cell: ' + str(round(sum(speed_neighbors), 2)) + ' s')\ndisplay(neighbors_table)\n\naccuracy_speed_plot (neighbors_fitted, \n                     accuracy_neighbors, speed_neighbors)","cb96a800":"accuracy_weights, speed_weights = [], []\n\nweights_fitted = ['uniform', 'distance']\n\nfor weight_trial in weights_fitted:\n    \n    model = KNeighborsClassifier(n_neighbors = 3, \n                                 weights = weight_trial,\n                                 algorithm = 'brute', \n                                 p = 2)\n    \n    execute_model_evaluation(model, \n                             x_train, y_train, \n                             x_valid, \n                             accuracy_weights, speed_weights)\n    \n\nweight_table = cool_table(weights_fitted, 'Types of Weight', \n                          accuracy_weights, speed_weights)\n\ndisplay('Time to run the cell: ' + str(round(sum(speed_weights), 2)) + ' s')\ndisplay(weight_table)\n\naccuracy_speed_barplot(weights_fitted, \n                       accuracy_weights, speed_weights)","53f83839":"accuracy_algorithm, speed_algorithm = [], []\n\nalgorithms_fitted = ['auto', 'ball_tree', 'kd_tree', 'brute']\n\nfor algorithm_trial in algorithms_fitted:\n    \n    model = KNeighborsClassifier(n_neighbors = 3, \n                                 weights = 'distance', \n                                 algorithm = algorithm_trial,\n                                 p = 2,\n                                 leaf_size = 2)\n    \n    execute_model_evaluation(model, \n                             x_train, y_train, \n                             x_valid, \n                             accuracy_algorithm, speed_algorithm)\n    \nalgorithm_table = cool_table(algorithms_fitted, 'Types of Algorithms', \n                             accuracy_algorithm, speed_algorithm)\n\ndisplay('Time to run the cell: ' + str(round(sum(speed_algorithm), 2)) + ' s')\ndisplay(algorithm_table)\n    \naccuracy_speed_barplot(algorithms_fitted, \n                       accuracy_algorithm, speed_algorithm)","446c4353":"accuracy_leaf_size, speed_leaf_size = [], []\n\nleaf_size_fitted = [1, 2, 4, 8, 16, 32, 64]\n\nfor leaf_size_trial in leaf_size_fitted:\n    \n    model = KNeighborsClassifier(n_neighbors = 3, \n                                 weights = 'distance',\n                                 algorithm = 'brute',\n                                 leaf_size = leaf_size_trial,\n                                 p = 2)\n    \n    execute_model_evaluation(model, \n                             x_train, y_train, \n                             x_valid, \n                             accuracy_leaf_size, speed_leaf_size)\n    \nleaf_table = cool_table(leaf_size_fitted, 'Leaf Size', \n                        accuracy_leaf_size, speed_leaf_size)\n\ndisplay('Time to run the cell: ' + str(round(sum(speed_leaf_size), 2)) + ' s')\ndisplay(leaf_table)\n    \naccuracy_speed_plot(leaf_size_fitted, \n                    accuracy_leaf_size, speed_leaf_size)","4445dc32":"accuracy_p, speed_p = [], []\n\np_fitted = [1, 2]\n\nfor p_trial in p_fitted:\n    \n    model = KNeighborsClassifier(n_neighbors = 3, \n                                 weights = 'distance', \n                                 algorithm = 'brute', \n                                 p = p_trial)\n    \n    execute_model_evaluation(model, \n                             x_train, y_train, \n                             x_valid, \n                             accuracy_p, speed_p)\n    \np_table = cool_table(p_fitted, 'Types of Norm', \n                     accuracy_p, speed_p)\n\ndisplay('Time to run the cell: ' + str(round(sum(speed_p), 2)) + ' s')\ndisplay(p_table)\n    \naccuracy_speed_barplot(p_fitted, \n                       accuracy_p, speed_p)","4f9eb60b":"### We report the final model with all the best features\n\n# We select the 512 parameters with the highest entropy\nparameters_list = entropy.head(512).Index                            \n\nx_train_final = pd.DataFrame(x_train_scaled)[parameters_list]\n\nx_valid_final = pd.DataFrame(x_valid_scaled)[parameters_list]\n\nmodel = KNeighborsClassifier(n_neighbors = 3, \n                             weights = 'distance',\n                             algorithm = 'brute', \n                             p = 2)\n\nstarting_time = time.process_time()                    # Start the time keeping\n\nmodel.fit(x_train_final, y_train)                      # Fit the model with the training dataset\n\ny_pred = model.predict(x_valid_final)                  # Predict the new values with the model\n\nfinishing_time = time.process_time()                   # End the time keeping\n\niteration_speed = finishing_time - starting_time       # Compute the time\n\niteration_accuracy = accuracy(y_pred, y_valid) * 100   # Compute the accuracy\n\nprint ('The model has an accuracy of ' + \n       str(round(iteration_accuracy, 2)) + \n       '% and a computing speed of ' + \n       str(round(iteration_speed, 2)) + \n       ' seconds')\n\nprint('The model is capable of predicting one instance in ' + \n      str(round(iteration_speed\/y_pred.shape[0], 4)) + \n      ' seconds')","2eeb712b":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\nx_test_final = pd.DataFrame(x_test_scaled)[parameters_list]\n\ny_test_pred = model.predict(x_test_final)\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nif y_test_pred is not None:\n    submission['class'] = y_test_pred\nsubmission.to_csv('my_submission.csv', index=False)","477e9c07":"# Sending the submission for the challenge","e5bbb7ef":"![knn.PNG](attachment:18b3ba5a-7cfc-4bca-8eba-19eed2b9a33b.PNG)\n\n*Source: https:\/\/hal.archives-ouvertes.fr\/hal-01657491\/document*\n\nAs you can see from the image, with 1 and 3 neighbors we assign the new data point (the black one) to the blue class, while with 5 and 7 neighbors we assign the new data point to the grey class.\n\n**There is no optimal number of neighbors**, it depends on the specific context and dataset.","cd07a398":"# Welcome to the Fashion-MNIST Challenge!\n\nWebsite reference: https:\/\/github.com\/zalandoresearch\/fashion-mnist\n\n**Author:** Gabriele Carbone\n\n**Date:** 05\/06\/2021","68c6761f":"# Evaluation","f8ba49b2":"# Processing Data\n\nWe transform the datasets into pandas DataFrame because it's easier to work with them","770a86f4":"# Dataset\n\nThe dataset contains 50k train + 10k validation images of 10 different categories ('T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot').\n\nEach image is a 28x28 grayscale, and for simplicity here is flattened into a 784 dimensional vector.","fe7955fb":"We can see that **the accuracy is exactly the same for all the trials,** while the speed change less than 3% between the smallest and the largest trial.\n\nThis proves the indifference that the brute force method has regarding the leaf size (simply because doesn't use trees at all)\n\nIn the final model we will drop entirely this hyperparameter because it's not important to us and we want to improve readibility","a2df40f6":"### Tuning the number of neighbors\nThe number of neighbors computed is the \"k\" in the kNN model. *Having more neighbors means that the model will take in account more observation similar to the input.* So, if we set k = 5 the 5 most similar observations will have a right to \"vote\" and influencing the class assignment to the input, if we set k = 15 the 15 most similar observations have the right to \"vote\" and so on.\n\nExploring this hyperparameter is very important in order to **understand the optimal level of bias-variance trade-off.**\n\nUsually, if we set k too small we will have underfitting, while if we set k too large we will have overfitting.\n\nWe set the search space between 1 and 16 neighbors, and if that is not enough, we will expand the search","c9bf0790":"### Feature Scaling and Final Elaborations\n\nIn this notebook, we will fully test the K Nearest Neighbor model which is very sensible to the scale of the single features\n\nNow, *all the values in the dataset are between 0 and 255*, so  - in theory - we should obtain good results even without scaling, but for good practice we will use a **standard MinMaxScaler** anyway","94f16615":"The first thing we can notice is that **the accuracy increase (although not linearly) with every feature**, so we can say that - in general - all of the pixel are important. But, as we said before, we cannot use the full dataset.\n\nWe select for the final model the 512 features option, because we want a model with at least 80% accuracy, but it can't be too slow\n\nFurthermore, *we can see that the increase in accuracy is very small while the increase in the running time is linearly dependent on the size of the dataset*\n\nFor the sake of speed, during the rest of the tuning, we will use the 128 option","4c5d7f2e":"In our case, **this hyperparameter doesn't seem to have a strong impact**\n\nThe accuracy is more or less the same, while *the speed is slightly increased with the 'distance' option.*\n\nConsidered that, we will use the 'distance' option in the next models","4768ab22":"# Author's Notes\n\nI've tried to optimize not only the model per se, but also the notebook in general:\n- Each new piece of code is commented the first time that it's used\n- The lines have an addition space between them\n    - But, if the lines are somehow connected, then you'll find them together\n- Each function's argument is broke into single lines\n    - But, if the arguments are few or simple I've left them on a single line\n- Through the notebook, I've used similar variable names, only changing the minimum possible to ease the connection between each section of the hyperparameters tuning\n- I've used the easiest plots to read (lineplots or barplots) , this is because I've favoured readibility over complexity. I feel like the informations that I wanted to share were delivered pretty well even with simple plots and I didn't want to add complexity where it wasn't necessary\n- I've added little sections of theoretical stuff to help you understand the notebook even with minimal background\n- I've tried to explain, for each hyperparameter, how it impacts the results and why\n\nThat's all. Let me know if you appreciated it!","24cbc3f5":"### Feature Selection - Entropy Method\n\nWe have a **huge number of features** in the dataset (over 700), so we need to give a priority to them\n\nWe use the metric *entropy* to understand what features might carry more information (alias, we look for the pixel that are more important in the classification process)","c1b04239":"### Tuning the p parameter\nThe p-parameter indicates the way in which the kNN should compute the distances between observations and input\n\nWe will test the two main methods for computing inputs:\n- *p = 1*: also called l1-norm or manatthan distance. Compute the distance, in absolute values, between the input value and the observations.\n- *p = 2*: also called l2-norm or euclidean distance. Compute the distance by taking the root of the sum of all the squared distances between the input value and the observations","9082d504":"### Tuning the leaf size\n\nThe leaf size hyperparameter influences the use of the binary trees with theBallTree algorithm or the KDTree algorithm options active\n\nWith the brute force method applied, this shouldn't affect the performances whatsoever","76582e6a":"### Check for NA values\n\nThere are no observations with NA values","0e1e659b":"We can see that **the model, using the l2-norm, is almost 6x faster than using the l1-norm**\n\nThis is probably due to the fact that computing absolute values is more computationally expensive compared to squaring and rooting numbers if the problem can't be solved using an LP reformulation that lead to linear equations \n\nWhile the accuracy is sligthly higher with the l1-norm, we will use the l2-norm in the final model","4ddc76a9":"![knn - Copia.PNG](attachment:96789107-19c7-4f41-9f62-3382da19147c.PNG)\n\n*Source: https:\/\/hal.archives-ouvertes.fr\/hal-01657491\/document*","ecf3afc4":"### Testing the HyperParameters\nFor both accuracy (as the primary KPI) and speed (the secondary KPI)","b95b9f1b":"### Tuning the type of weight\nkNN is a non-parametric model, so to see the word \"weight\" must be somewhat confusing\n\nThere are two types of weights in kNN:\n- *'uniform'*: this means, substantially, that there are no weights and all the distances in the features are considered with the same importance\n- *'distance'*: this means that the features will be assigned a weight based on their distance from the input. So, the more distant a feature values is the smaller the weight assigned will be the mless important the feature will be\n\nWith the 'distance' option, a feature that have values usually far from the input will have less importance \n\n**Feature scaling is extremely important for this hyperparameter!**","26f84b2c":"We can see that the 'auto' option probably selected the 'kd_tree' option for this dataset, thus the similar results.\n\nThe ball_tree algorithm is almost twice as fast as the kd_tree, while the brute force algorithm is almost 6 times faster than the kd_tree (and 3x faster than the ball_tree). **The brute force algorithm is faster due to the fact that doesn't need to build a binary tree like the other two.**\n\nGiven the obvious disparity in performances, we will use the brute force in the final model","1439aa01":"### Tuning the type of algorithm\nThe type of algorithm is, essentially, **the \"engine\" that the model use** to process the input.\n\nThere are four option for this hyperparameter:\n- *'auto'*: the function will automatically pick the best of the other three options based on the input\n- *'ball_tree'*: will use the ball_tree algorithm. It's an algorithm that uses a binary tree to partition the data. Usually, performs well when the number of dimensions is high but, initially, require a lot of time to create the binary tree. It uses the 'triangle inequality' to create bounds and speed up the process. Complexity: O[DNlog(N)]\n- *'kd_tree'*: will use the kd_tree algorithm. Like the ball_tree, uses binary trees to recursively partition the data into smaller groups. The construction of a kd_tree is very fast because it's done by using only along the data axes. But, kd_tree becomes very slow for higher dimensionality spaces because it suffer from the 'curse of dimensionality'. Complexity: O[DNlog(N)], for larger samples O[DN]\n- *'brute'*: will use a brute force approach. Complexity: O[DN]\n\nThe main difference between the kd_tree and the ball_tree is that *kd_tree partition data along the axes, while the ball_tree partition data in a series of hyper-spheres* (in 2d they would be circles, in 3d they would be sphere, and so on)","2a023bf0":"### Print Shapes\n\nPrint the shapes of the **3 dataset** (x_train_df, x_valid_df, x_test) and **2 target vectors** (y_train, y_valid)","a88415e9":"We note that **the accuracy goes up until 3 neighbors then starts to drop.** This means that if we go over 4 neighbor we start overfitting the model.\n\nAlso, note that the speed is affected only up to the fourth neighbor, after that is constant\n\nConsidered that, we will use 3 neighbors in the next models","656039ac":"# kNN Model\n\nThe Fashion-MNIST Challenge is a **classification problem**, so using a kNN (aka k-Nearest Neighbors) can be a smart approach\n\nkNN is a **non-parametric model** (meaning that doesn't use weights or parameters learning) that, intuitively, given a certain input searches for the most similar k observation and assign a class to the input based on the most popular one among them\n\nFrom a mathematical point of view, the \"similarity\" is often computed using the l1-norm or the l2-norm (also called \"euclidean distance\")\n\nThe main advantage of kNN is the **almost non-existent time to train** (limited to the storing time) BUT **the prediction phase may take a while** because the algorithm needs to compare the input to all the observation in the dataset (for each input!)\n\n## How does it works?\n\nkNN takes as input the parameter values of the observation then computes the distances between the instance's value for a feature and the training set's value for the same feature, for each feature. Then the model process, depending on the norm used, these distances and output a number that represent the general \"distance\" of the the observation from that data point.\n\nThese results are ranked from the smallest (most similar observation of the training set) to the greatest (the least similar one).\n\nFinally, the first k observation (called **\"neighbors\"**) are taken and given the \"right\" to vote for the class of a given instance.","92b4ff5d":"![](https:\/\/robocrop.realpython.net\/?url=https%3A\/\/files.realpython.com\/media\/knn_01_MLgeneral_wide.74e5e2dc1094.png&w=1512&sig=492d6c64473418b06336cebadf7eb78bb7662e12)\n\n*Source: https:\/\/realpython.com\/knn-python\/*\n\nFrom a mathematical point of view, in 2D we can imagine the kNN as **a model to create \"dominance\" maps** (also called \"Voronoi Diagram Visualizations\") were, *if an instance fall inside one of these intervals, then it's assigned to the dominant class.*\n\nFrom the image above, you can see  a very simple case: for example, given that the majority of the data points in the top left side of the graph are stars then we will assign an hypotetichal new data point to \"star\" if it falls in that interval.\n\nkNN is very powerful because it can create **extremely complex boundaries with simple math.**\nFor example, in the image below you can see the complexity of a real case study and how it changes based on the number of neighbors: usually, *if we have fewer neighbors the boundaries are more complex because they are more sensible* (it takes just a single data point to change the class!)","5127c641":"### Classes' Frequency\nThe classes are very balanced, each of the classes has around the same portion of dataset (10%)","e3c8e18b":"### Tuning the Number of Features\nSelecting the most important features (alias the pixels) is the first step in our tuning process\n\nIn general, more features means more information and, as a consequence, more accuracy (at least in the training set...)\n\nBut, to use all features involves to major side effects:\n- **risk of overfitting the model**: as in all situations with a very high number of parameters\n- **a general decrease in speed**: expecially in a model as kNN the number of features is of the utmost importance, given that the time required increase linearly with the dataset used "}}