{"cell_type":{"7aef49f2":"code","d7d6630c":"code","17163efc":"code","fe62f8f5":"code","388b83dd":"code","276f1605":"code","dec2b5a2":"markdown","2eae0645":"markdown","d154d4fe":"markdown","06196aa7":"markdown","ab801dc9":"markdown","affad001":"markdown","f66d51a8":"markdown","fa70e124":"markdown"},"source":{"7aef49f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7d6630c":"pip install tflearn","17163efc":"import nltk\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()\n\nimport numpy\nimport tflearn\nimport tensorflow as tf\nimport random\nimport json\nimport pickle\n\n\nimport json\nwith open('..\/input\/chat-bot-dataset-for-pointer\/intents.json') as file:\n    data = json.load(file)","fe62f8f5":"try:\n    with open(\"data.pickle\", \"rb\") as f:\n        words, labels, training, output = pickle.load(f)\nexcept:\n    words = []\n    labels = []\n    docs_x = []\n    docs_y = []\n\n    for intent in data[\"intents\"]:\n        for pattern in intent[\"patterns\"]:\n            wrds = nltk.word_tokenize(pattern)\n            words.extend(wrds)\n            docs_x.append(wrds)\n            docs_y.append(intent[\"tag\"])\n\n        if intent[\"tag\"] not in labels:\n            labels.append(intent[\"tag\"])\n\n    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n    words = sorted(list(set(words)))\n\n    labels = sorted(labels)\n\n    training = []\n    output = []\n\n    out_empty = [0 for _ in range(len(labels))]\n\n    for x, doc in enumerate(docs_x):\n        bag = []\n\n        wrds = [stemmer.stem(w.lower()) for w in doc]\n\n        for w in words:\n            if w in wrds:\n                bag.append(1)\n            else:\n                bag.append(0)\n\n        output_row = out_empty[:]\n        output_row[labels.index(docs_y[x])] = 1\n\n        training.append(bag)\n        output.append(output_row)\n\n\n    training = numpy.array(training)\n    output = numpy.array(output)\n\n    with open(\"data.pickle\", \"wb\") as f:\n        pickle.dump((words, labels, training, output), f)\n","388b83dd":"from tensorflow.python.framework import ops\nops.reset_default_graph()\n#tf.reset_default_graph()\n\nnet = tflearn.input_data(shape=[None, len(training[0])])\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\nnet = tflearn.regression(net)\n\nmodel = tflearn.DNN(net)\n\ntry:\n    model.load(\"model.tflearn\")\nexcept:\n    model = tflearn.DNN(net)\n    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n    model.save(\"model.tflearn\")","276f1605":"def bag_of_words(s, words):\n    bag = [0 for _ in range(len(words))]\n\n    s_words = nltk.word_tokenize(s)\n    s_words = [stemmer.stem(word.lower()) for word in s_words]\n\n    for se in s_words:\n        for i, w in enumerate(words):\n            if w == se:\n                bag[i] = 1\n            \n    return numpy.array(bag)\n\n\ndef chat():\n    print(\"Start talking with the bot (type quit to stop)!\")\n    while True:\n        inp = input(\"You: \")\n        if inp.lower() == \"quit\":\n            break\n\n        results = model.predict([bag_of_words(inp, words)])\n        results_index = numpy.argmax(results)\n        tag = labels[results_index]\n\n        for tg in data[\"intents\"]:\n            if tg['tag'] == tag:\n                responses = tg['responses']\n\n        print(random.choice(responses))\n\nif __name__ == \"__main__\":        \n chat()","dec2b5a2":"**Now its time to loop through our JSON data and extract the data we want. For each pattern we will turn it into a list of words using nltk.word_tokenizer, rather than having them as strings. We will then add each pattern into our docs_x list and its associated tag into the docs_y list.**","2eae0645":"Bag of Words\nNow that we have loaded in our data and created a stemmed vocabulary it's time to talk about a bag of words. As we know neural networks and machine learning algorithms require numerical input. So out list of strings wont cut it. We need some way to represent our sentences with numbers and this is where a bag of words comes in. What we are going to do is represent each sentence with a list the length of the amount of words in our models vocabulary. Each position in the list will represent a word from our vocabulary. If the position in the list is a 1 then that will mean that the word exists in our sentence, if it is a 0 then the word is nor present. We call this a bag of words because the order in which the words appear in the sentence is lost, we only know the presence of words in our models vocabulary.","d154d4fe":"**Making Predictions\nNow its time to actually use the model! Ideally we want to generate a response to any sentence the user types in. To do this we need to remember that our model does not take string input, it takes a bag of words. We also need to realize that our model does not spit out sentences, it generates a list of probabilities for all of our classes. This makes the process to generate a response look like the following:\n\u2013 Get some input from the user\n\u2013 Convert it to a bag of words\n\u2013 Get a prediction from the model\n\u2013 Find the most probable class\n\u2013 Pick a response from that class**","06196aa7":"**We can add more data to Json file. If you want to edit or add some data you can do.**","ab801dc9":"**Developing a Model\nNow that we have preprocessed all of our data we are ready to start creating and training a model. For our purposes we will use a fairly standard feed-forward neural network with two hidden layers. The goal of our network will be to look at a bag of words and give a class that they belong too (one of our tags from the JSON file).\n\nWe will start by defining the architecture of our model. Keep in mind that you can mess with some of the numbers here and try to make an even better model! A lot of machine learning is trial an error.**","affad001":"**Install Packages\nBefore starting to work on our chatbot we need to download a few python packages. Please note as of writing this these packages will ONLY WORK IN PYTHON 3.6. Hopefully this will be fixed in the future.\n\nWe will simply use pip to install the following:\n- numpy\n- nltk\n- tensorflow\n- tflearn\n\nSimply go to CMD and type: pip install \"package name\". Where you will replace \"package_name\" with all of the entries listed above.******","f66d51a8":"**Loading our JSON Data\nWe will start by importing some modules and loading in our json data. Make sure that your .json file is in the same directory as your python script!******","fa70e124":"**Now our json data will be stored in the variable data.**"}}