{"cell_type":{"71dce820":"code","84faf091":"code","e75529c6":"code","3df28a7c":"code","3723f1cb":"code","f02bcf7b":"code","0c4a50bd":"code","a9fb08b6":"code","78b51bb3":"code","51299950":"code","69a4f00e":"code","b001d549":"code","4a2fca50":"code","33be4f9b":"code","05ea99f3":"code","85c0f64f":"code","4822379b":"code","95adea10":"code","92dbc4a4":"code","1f5309e5":"code","0727304a":"code","d7929d1e":"code","03f43aa5":"code","c39f7a61":"code","7dbd379d":"code","a0d609ff":"code","f6df96dd":"code","5f3c634f":"code","76db39ca":"code","79fd3cce":"code","241f1b28":"markdown","b496a90c":"markdown","d35dc1d9":"markdown","716213bc":"markdown","1f0f6d3a":"markdown","c6530227":"markdown","6baf937f":"markdown","f4845684":"markdown","43cf6f66":"markdown","843c9891":"markdown","d7dfbfca":"markdown","3d6f9a19":"markdown","8db318b4":"markdown","6f3589e1":"markdown","ea1c9a4e":"markdown","4f841060":"markdown","1067f960":"markdown","c5637463":"markdown","7421539f":"markdown","c7558e55":"markdown","e1c45a68":"markdown","13a183dd":"markdown","9159e99e":"markdown","639dc0da":"markdown"},"source":{"71dce820":"#installs\n!pip install jovian --upgrade --quiet\n#Imports\nimport jovian\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split, DataLoader, TensorDataset\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets.utils import download_url\nimport zipfile\n%matplotlib inline","84faf091":"project_name=\"Predicting-Pulsars\" #Project name to commit to on Jovian.ml","e75529c6":"#data_url=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00372\/HTRU2.zip\"\n#download_url(data_url, \".\")","3df28a7c":"# Uncomment as needed.\n#with zipfile.ZipFile(\".\/HTRU2.zip\", 'r') as zip_ref:\n#    zip_ref.extractall(\".\")\n#!rm -rf HTRU2.zip","3723f1cb":"filename = \"..\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\" #Change as needed.\n\ndf = pd.read_csv(filename)\nprint(df.info())\ndf.head()","f02bcf7b":"inputs_df=df.drop(\"target_class\",axis=1)#Easiest way to get inputs- we just need everything but the targets_class \ninputs_arr=inputs_df.to_numpy()\ntargets_df=df[\"target_class\"]#Easiest way to get outputs-need just targets_class\ntargets_arr=targets_df.to_numpy()","0c4a50bd":"#output variables.\ninputs=torch.from_numpy(inputs_arr).type(torch.float64)\ntargets=torch.from_numpy(targets_arr).type(torch.long)\ninputs.shape, targets.shape","a9fb08b6":"#jovian.commit(project=project_name, enviroment=None)","78b51bb3":"dataset=TensorDataset(inputs, targets)","51299950":"num_rows=df.shape[0]\nval_percent = .1 # How much of the dataset \nval_size = int(num_rows * val_percent)\ntrain_size = num_rows - val_size","69a4f00e":"torch.manual_seed(2)#Ensure that we get the same validation each time.\ntrain_ds, val_ds = random_split(dataset, (train_size, val_size))\ntrain_ds[5]","b001d549":"batch_size=200","4a2fca50":"# PyTorch data loaders\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)","33be4f9b":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","05ea99f3":"# get device\ndevice=get_default_device()\ndevice","85c0f64f":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","4822379b":"def logistic_block(in_features=8, out_features=8):\n    layers=[nn.Linear(in_features, out_features),\n            nn.BatchNorm1d(out_features),\n            nn.ReLU(inplace=True)]\n    return nn.Sequential(*layers)","95adea10":"class HTRU2Model(nn.Module):\n    def __init__(self,):\n        super(HTRU2Model,self).__init__()\n        self.fc1 = nn.Linear(8, 16)\n        self.fc2 = nn.Linear(16, 16)\n        self.fc3 = nn.Linear(16, 2)\n        self.softmax = nn.Softmax(dim=1)\n    def forward(self, x):\n        x = x.float()\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n    def training_step(self, batch):\n        inputs, targets = batch \n        out = self(inputs)                  # Generate predictions\n        loss = F.cross_entropy(out, targets) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        inputs, targets = batch \n        out = self(inputs)                    # Generate predictions\n        loss = F.cross_entropy(out, targets)   # Calculate loss\n        acc = accuracy(out, targets)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))   \n\n","92dbc4a4":"model=to_device(HTRU2Model(),device)","1f5309e5":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","0727304a":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=optim.Adam):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n","d7929d1e":"history = [evaluate(model, val_dl)]\nhistory","03f43aa5":"epochs = 10\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","c39f7a61":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","7dbd379d":"time = 8.37","a0d609ff":"#jovian.log_hyperparams(arch='simple', \n#                       epochs=epochs, \n#                       lr=max_lr, \n#                       scheduler='one-cycle', \n #                      weight_decay=weight_decay, \n  #                     grad_clip=grad_clip,\n   #                    opt=opt_func.__name__)\n#jovian.log_metrics(val_loss=history[-1]['val_loss'], \n    #               val_acc=history[-1]['val_acc'],\n     #              train_loss=history[-1]['train_loss'],\n      #             time=time)\n#jovian.commit(project=project_name, enviroment=None,outputs=['Predicting-Pulsars-PyTorch.pth'])","f6df96dd":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\n\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n    \ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","5f3c634f":"plot_losses(history)","76db39ca":"plot_accuracies(history)","79fd3cce":"plot_lrs(history)","241f1b28":"### Transfer to GPU","b496a90c":"## Stats and charts!","d35dc1d9":"Train and add to history\n> Here you can see some progress bar bling!","716213bc":"### Create the dataset.","1f0f6d3a":"### Create the model Class","c6530227":"## Download the dataset and initialize the dataset.\n - if you are on kaggle, add [this](https:\/\/www.kaggle.com\/pavanraj159\/predicting-a-pulsar-star) dataset- avoids redownloading the dataset each time.\n - Otherwise uncomment the next two lines of code.","6baf937f":"## Prepare Dataset for Training\nWe need to convert the dataframe to Pytorch Tensors using numpy arrays.","f4845684":"#### Extract the csv file - if you downloaded the dataset.","43cf6f66":"### Set a batch size.  \nI am going to pick 200, but adjust this to you needs.","843c9891":"# Predicting Pulsar Stars\n#### Use the HTRU 2 dataset to predict pulsars.","d7dfbfca":"### Commit to Jovian(optional)","3d6f9a19":"I am logging all metrics. This will help show impovement over versions, through Jovian's Compare Versions feature.","8db318b4":"### Linear Block","6f3589e1":"### Split the dataset into training and validation","ea1c9a4e":"## Create a Model\nWe are going to be creating a Model with Residual Blocks and Batch Normalization. Roughly based on ResNet5 architecture.","4f841060":"**Get the initial accuracy and loss**","1067f960":"#### Accuracy, evaluation, and fit function","c5637463":"We can now wrap our training and validation data loaders using DeviceDataLoader for automatically transferring batches of data to the GPU (if available).\n","7421539f":"Use the random_split function to split dataset into 2 parts of the desired length","c7558e55":"## Load Data and transfer data to GPU, if available.","e1c45a68":"## Training the Model","13a183dd":"# Commit To Jovian!   (Optional, but useful)","9159e99e":"## Train!\n> | The Parameters are listed below. ","639dc0da":"### Load the data from the .csv file \nWe just need to use the panda library's read_csv() function\n"}}