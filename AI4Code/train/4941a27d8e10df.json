{"cell_type":{"3a3f72b4":"code","8d583f4e":"code","b0606fbf":"code","f818121e":"code","d3c8ef71":"code","e77ad519":"code","cbeb5b3b":"code","7ace7761":"code","4399c9f6":"code","6e494dd8":"code","31b0189b":"code","46811c2b":"code","6c090fbb":"code","ba8d1db9":"code","6923a4a3":"markdown","1f876412":"markdown","029c63a6":"markdown","c1a0e164":"markdown","a12174f4":"markdown","5c54645d":"markdown","a2bf99ec":"markdown","c5b03938":"markdown","8288f939":"markdown","5116a6e3":"markdown","ac9bf19c":"markdown","7e71f6db":"markdown","b85dfc89":"markdown","f18f681a":"markdown","db7f088b":"markdown","d37099b8":"markdown","5bcc5bf7":"markdown","bacac54b":"markdown"},"source":{"3a3f72b4":"#fix kaggle container\n!apt-get install -y libsndfile1","8d583f4e":"import pandas as pd\nimport cffi\nimport torchvision\nimport joblib\nimport librosa\nimport jieba\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b0606fbf":"!git clone https:\/\/github.com\/OpenNMT\/OpenNMT-py\n!mkdir \/kaggle\/working\/data\n!mkdir \/kaggle\/working\/output","f818121e":"train_count = 100000\nval_count = 5000\ntest_count = 5000","d3c8ef71":"with open('\/kaggle\/input\/open_ru.txt', mode='r', encoding='utf-8') as ru_in, open('\/kaggle\/working\/data\/sent_ru_train.txt', mode='w', encoding='utf-8') as ru_out_train, \\\n     open('\/kaggle\/working\/data\/sent_ru_val.txt', mode='w', encoding='utf-8') as ru_out_val, open('\/kaggle\/working\/data\/sent_ru_test.txt', mode='w', encoding='utf-8') as ru_out_test:\n    \n    for i in range(train_count):\n        if i < train_count-1:\n            ru_out_train.write(ru_in.readline().strip() + '\\n')\n        else:\n            ru_out_train.write(ru_in.readline().strip())\n        \n    for i in range(val_count):\n        if i < val_count-1:\n            ru_out_val.write(ru_in.readline().strip() + '\\n')\n        else:\n            ru_out_val.write(ru_in.readline().strip())\n        \n    for i in range(test_count):\n        if i < test_count-1:\n            ru_out_test.write(ru_in.readline().strip() + '\\n')\n        else:\n            ru_out_test.write(ru_in.readline().strip())  ","e77ad519":"with open('\/kaggle\/input\/open_zh.txt', mode='r', encoding='utf-8') as zh_in, open('\/kaggle\/working\/data\/sent_zh_train.txt', mode='w', encoding='utf-8') as zh_out_train, \\\n     open('\/kaggle\/working\/data\/sent_zh_val.txt', mode='w', encoding='utf-8') as zh_out_val, open('\/kaggle\/working\/data\/sent_zh_test.txt', mode='w', encoding='utf-8') as zh_out_test:\n    \n    for i in range(train_count):\n        if i < train_count-1:\n            zh_out_train.write(zh_in.readline().strip() + '\\n')\n        else:\n            zh_out_train.write(zh_in.readline().strip())\n        \n    for i in range(val_count):\n        if i < val_count-1:\n            zh_out_val.write(zh_in.readline().strip() + '\\n')\n        else:\n            zh_out_val.write(zh_in.readline().strip())\n        \n    for i in range(test_count):\n        if i < test_count-1:\n            zh_out_test.write(zh_in.readline().strip() + '\\n')\n        else:\n            zh_out_test.write(zh_in.readline().strip())  ","cbeb5b3b":"!python \/kaggle\/working\/OpenNMT-py\/tools\/learn_bpe.py -i \/kaggle\/working\/data\/sent_zh_train.txt -o \/kaggle\/working\/data\/src.code -s 32000","7ace7761":"!python \/kaggle\/working\/OpenNMT-py\/tools\/learn_bpe.py -i \/kaggle\/working\/data\/sent_ru_train.txt -o \/kaggle\/working\/data\/trg.code -s 32000","4399c9f6":"!python \/kaggle\/working\/OpenNMT-py\/tools\/apply_bpe.py -c \/kaggle\/working\/data\/src.code -i \/kaggle\/working\/data\/sent_zh_train.txt -o \/kaggle\/working\/data\/src-train-bpe.txt\n!python \/kaggle\/working\/OpenNMT-py\/tools\/apply_bpe.py -c \/kaggle\/working\/data\/src.code -i \/kaggle\/working\/data\/sent_zh_val.txt -o \/kaggle\/working\/data\/src-val-bpe.txt\n!python \/kaggle\/working\/OpenNMT-py\/tools\/apply_bpe.py -c \/kaggle\/working\/data\/src.code -i \/kaggle\/working\/data\/sent_zh_test.txt -o \/kaggle\/working\/data\/src-test-bpe.txt\n!python \/kaggle\/working\/OpenNMT-py\/tools\/apply_bpe.py -c \/kaggle\/working\/data\/trg.code -i \/kaggle\/working\/data\/sent_ru_train.txt -o \/kaggle\/working\/data\/tgt-train-bpe.txt\n!python \/kaggle\/working\/OpenNMT-py\/tools\/apply_bpe.py -c \/kaggle\/working\/data\/trg.code -i \/kaggle\/working\/data\/sent_ru_val.txt -o \/kaggle\/working\/data\/tgt-val-bpe.txt","6e494dd8":"!python \/kaggle\/working\/OpenNMT-py\/preprocess.py -train_src \/kaggle\/working\/data\/src-train-bpe.txt -train_tgt \/kaggle\/working\/data\/tgt-train-bpe.txt \\\n    -valid_src \/kaggle\/working\/data\/src-val-bpe.txt -valid_tgt \/kaggle\/working\/data\/tgt-val-bpe.txt -save_data \/kaggle\/working\/data\/ondatr \\\n    -src_vocab_size 32000 -tgt_vocab_size 32000","31b0189b":"!python \/kaggle\/working\/OpenNMT-py\/train.py -data \/kaggle\/working\/data\/ondatr -save_model \/kaggle\/working\/data\/ondatr-trans -world_size 1 \\\n    -gpu_rank 0 --train_steps 5000 -save_checkpoint_steps 5000 --keep_checkpoint 10","46811c2b":"!python \/kaggle\/working\/OpenNMT-py\/translate.py -model \/kaggle\/working\/data\/ondatr-trans_step_5000.pt -src \/kaggle\/working\/data\/src-test-bpe.txt \\\n    -output \/kaggle\/working\/output\/pred.bpe -replace_unk -verbose","6c090fbb":"!sed -i \"s\/@@ \/\/g\" \/kaggle\/working\/output\/pred.bpe","ba8d1db9":"!perl \/kaggle\/working\/OpenNMT-py\/tools\/multi-bleu.perl \/kaggle\/working\/data\/sent_ru_test.txt < \/kaggle\/working\/output\/pred.bpe","6923a4a3":"There are 4M+ sentences in the dataset. Increase the training parameteres below. 5000-6000 is good for validation.","1f876412":"# Translate","029c63a6":"# Introduction","c1a0e164":"We're taking our pretrained weights and translating the input **tokenized** text.","a12174f4":"Preparing the training data for the model.","5c54645d":"This is a baseline model for the **Russian-Chinese Machine Translation Challenge** hosted on _ML Boot Camp_.\n\nhttps:\/\/mlbootcamp.ru\/ru\/round\/26\/tasks\n\nI've made the dataset with OpenSubtitles parallel corpus. Other datasets that I've found so far:\n\n* TED parallel corpus\n* Facebook Matrixnet parallel corpus\n\nIdeas to improve:\n\n* Feed the larger dataset (Zh-Ru parallel corpus).\n* Try to align Chinese and Russian texts (for example this Chinese translation of Ostrovsky's \"How the steel was tempered\" https:\/\/github.com\/hankinghu\/literature-books\/blob\/master\/%E9%92%A2%E9%93%81%E6%98%AF%E6%80%8E%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84.txt).\n* Tweak the model parameters. They are all described in OpenNMT documentation.\n* This is a PyTorch version. There is a TF implementation too. You can try it.\n* Share your ideas.","a2bf99ec":"Here we trainig the model for --train_steps and saving checkpoints to use later.","c5b03938":"They say in all the tutorials that 32000 is a good number to deal with. Don't get it for granted, try to decrease\/increase it.","8288f939":"# Train & test split","5116a6e3":"There are \"@@ \" token separators in the output file. Let's clean them up.","ac9bf19c":"# Detokenization","7e71f6db":"# Preprocessing","b85dfc89":"Byte-Pair encoding is a very cool thing \u2014 it allows you to cover more words including unknown ones with a smaller dictionary. Don't forget to replace the chunk separators after translation.","f18f681a":"# BLEU","db7f088b":"# BPE","d37099b8":"# Make dictionaries","5bcc5bf7":"BLEU is a crucial metric in the field of machine translation. SOTA models do about 30-40% BLEU for a different language pairs nowadays.","bacac54b":"# Train"}}