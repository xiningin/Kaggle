{"cell_type":{"cecf4252":"code","c32de682":"code","03bd3fcd":"code","5e49f95e":"code","9d6aa606":"code","47594340":"code","422083c4":"code","638a6059":"code","b93c85ff":"code","ab9d737f":"code","d907b36d":"code","d3c1b0d7":"code","59c9eb6b":"code","51a6e644":"code","edfddcde":"code","967ddbf4":"code","2b7bfdc3":"code","9d659468":"code","827d62df":"code","0c99fc73":"code","ac179f06":"code","793bc541":"code","56d40915":"code","dc65c2a5":"code","32889d8e":"code","d87c84a9":"code","760829ca":"code","0e057ad9":"code","44357eaa":"code","792dd254":"code","ee38156f":"code","19dac1c7":"code","8b65ac8d":"code","c859534a":"code","2ba3c248":"code","76229a90":"code","92ca6df2":"code","54d3a68d":"code","f83f89a2":"code","625e3178":"code","dd23b77a":"code","48e99053":"code","71e5ff72":"code","2e5a5c70":"code","df44a627":"code","10f68289":"code","d682c7f1":"markdown","09d0526a":"markdown","d60ac1fd":"markdown","4a1819b5":"markdown","0a28887b":"markdown","5098f739":"markdown","afb6002f":"markdown","c9e878e0":"markdown","2270278d":"markdown"},"source":{"cecf4252":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf # Import tensorflow library\nfrom tensorflow import keras # Import Keras Library","c32de682":"train=pd.read_csv(\"..\/input\/Kannada-MNIST\/train.csv\")","03bd3fcd":"test=pd.read_csv(\"..\/input\/Kannada-MNIST\/test.csv\")","5e49f95e":"train.head()","9d6aa606":"test.head()","47594340":"Y_train = train['label']\nX_train = train.drop(columns=['label'])\nX_test = test.drop(columns=['id'])","422083c4":"print(\"x_train shape:\", X_train.shape, \"y_train shape:\", Y_train.shape) ","638a6059":"#X_train=np.array(X_train)\n#Y_train=np.array(Y_train)\n#X_test=np.array(X_test)","b93c85ff":"X_train.values[100]","ab9d737f":"plt.imshow(X_train.values[100].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest') #plt.axis(\"off\")\nplt.show()","d907b36d":"digit_train, counts_train = np.unique(Y_train, return_counts = True)","d3c1b0d7":"plt.bar(digit_train,counts_train,width =0.6)\nplt.title('Distribution of Y_train')\nplt.xlabel('Digit Number')\nplt.ylabel('Counts')\nplt.show()","59c9eb6b":"#Using Standardization Scaler method\nfrom sklearn.preprocessing import StandardScaler \nscaler=StandardScaler()\n#from sklearn.preprocessing import MinMaxScaler\n#scaler=MinMaxScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n\n#X_train_scaled=scaler.fit_transform(X_train.values)\nX_test_scaled=pd.DataFrame(scaler.transform(X_test.values.reshape(len(X_test),784)))","51a6e644":"type(X_train_scaled)","edfddcde":"fig,ax = plt.subplots(1,2)\nax[0].imshow(X_train.values[100].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest') #plt.axis(\"off\")\nax[0].set_title('Unscaled')\nax[1].imshow(X_train_scaled.values[100].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest') #plt.axis(\"off\")\nax[1].set_title('Scaled')\nplt.show()","967ddbf4":"fig_object, ax_object = plt.subplots(1, 10, figsize=(12,5))\nax_object = ax_object.reshape(10,)\n    \nfor i in range(len(ax_object)):\n    ax = ax_object[i]\n    idx=np.argwhere(Y_train.values==i)[0]\n    ax.imshow(X_train.values[idx].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest')\n    ax.set_xlabel(Y_train.values[i])\n    ax.set_title(i)\n    \nplt.show()\n\nfig_object, ax_object = plt.subplots(1, 10, figsize=(12,5))\nax_object = ax_object.reshape(10,)\n      \nfor i in range(len(ax_object)):\n    ax = ax_object[i]\n    idx=np.argwhere(Y_train==i)[0]\n    ax.imshow(X_train_scaled.values[idx].reshape(28,28), cmap = plt.cm.binary, interpolation = 'nearest')\n    ax.set_xlabel(Y_train[i])\n    plt.xlabel(Y_train[i])\n    ax.set_title(i)       \nplt.show()","2b7bfdc3":"from sklearn.model_selection import train_test_split\nX_train,X_val,Y_train,Y_val = train_test_split(X_train_scaled,Y_train,test_size=0.25)","9d659468":"import os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #model will be trained on GPU 1","827d62df":"import keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport gzip\n%matplotlib inline\nfrom keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop","0c99fc73":"X_train = X_train.reshape(-1, 28,28, 1)\nX_test = X_test.reshape(-1, 28,28, 1)\nX_train.shape, X_test.shape","ac179f06":"batch_size = 128\nepochs = 128\ninChannel = 1\nx, y = 28, 28\ninput_img = Input(shape = (x, y, inChannel))","793bc541":"def autoencoder(input_img):\n    #encoder\n    #input = 28 x 28 x 1 (wide and thin)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n\n    #decoder\n    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n    return decoded","56d40915":"autoencoder = Model(input_img, autoencoder(input_img))\nautoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())","dc65c2a5":"autoencoder.summary()","32889d8e":"autoencoder_train = autoencoder.fit(X_train, Y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_val, Y_val))","d87c84a9":"X_train.shape","760829ca":"X_val.shape","0e057ad9":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\n#Import libraries to build the model","44357eaa":"model=Sequential()","792dd254":"model.add(Convolution2D(64,3,3,input_shape=(28,28,1),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))","ee38156f":"model.add(Convolution2D(128,3,3,input_shape=(28,28,1),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))","19dac1c7":"model.add(Flatten())","8b65ac8d":"model.add(Dense(output_dim=256,activation='relu'))\nmodel.add(Dense(output_dim=128,activation='relu'))\nmodel.add(Dense(output_dim=10,activation='sigmoid'))\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","c859534a":"X_train=X_train.reshape(-1,28,28,1)","2ba3c248":"epochs=30\n\n# fits the model on batches with real-time data augmentation:\nhistory=model.fit(x=X_train, y=Y_train, epochs=epochs, batch_size=64, validation_data=(X_val.values.reshape(len(X_val),28,28,1), Y_val))","76229a90":"test_loss, test_acc = model.evaluate(X_val.values.reshape(len(X_val),28,28,1),  Y_val,verbose=2)\nprint('\\nTest accuracy:', test_acc)","92ca6df2":"plt.plot(range(epochs),history.history['accuracy'])\nplt.xlabel('No. of Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.show()\nplt.plot(range(epochs),history.history['loss'])\nplt.xlabel('No. of Epochs')\nplt.ylabel('Loss Value')\nplt.title('Loss Function')\nplt.show()","54d3a68d":"predict = model.predict(X_val.values.reshape(len(X_val),28,28,1))","f83f89a2":"Y_pred=[]\nfor i in range(len(predict)):\n    Y_pred.append(np.argmax(predict[i,:]))","625e3178":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_val,Y_pred)\nprint(cm)","dd23b77a":"acc=0\nfor j in range(len(cm)):\n    acc=acc+cm[j,j]\nprint(acc\/15000*100)","48e99053":"submission=pd.read_csv(\"..\/input\/Kannada-MNIST\/sample_submission.csv\")","71e5ff72":"test_predict = model.predict(X_test_scaled.values.reshape(len(X_test),28,28,1))\nY_predict=np.argmax(test_predict,axis=1)\nsubmission['label']=Y_predict\nsubmission['id']=range(0,len(X_test_scaled))","2e5a5c70":"submission.to_csv('submission.csv',index=False)","df44a627":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(200, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# train the model on the new data for a few epochs\nmodel.fit_generator(...)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfor i, layer in enumerate(base_model.layers):\n    print(i, layer.name)\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 249 layers and unfreeze the rest:\nfor layer in model.layers[:249]:\n    layer.trainable = False\nfor layer in model.layers[249:]:\n    layer.trainable = True\n\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit_generator(...)","10f68289":"shape_x = 28\nshape_y = 28\nnRows,nCols,nDims = X_train_scaled.shape[1:]\ninput_shape = (nRows, nCols, nDims)\nclasses = np.unique(Y_train)\nnClasses = len(classes)","d682c7f1":"**Visulaize the input data**","09d0526a":"Split the dataset","d60ac1fd":"Building Model","4a1819b5":"## AutoEncoder","0a28887b":"Feature Scaling or Standardization","5098f739":"(a) Distribution of training data\n","afb6002f":"Perfomance Metrics","c9e878e0":"Equal distribution of training data","2270278d":"Evaluate the Accuracy"}}