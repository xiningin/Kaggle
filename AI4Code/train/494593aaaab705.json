{"cell_type":{"6660b85e":"code","46c4711d":"code","d19bb6f9":"code","3bf476f4":"code","8c67deeb":"code","2c248223":"code","292a9480":"code","3cb8e702":"code","328339c6":"code","f036b588":"code","fa6fd4f5":"code","347dbca9":"code","0a908666":"code","86300e66":"code","d8f25283":"code","81178dea":"code","e9c8ef88":"code","995ac092":"code","8ad8e22d":"code","1f093a45":"code","90ab6243":"code","6217af9b":"code","fbdde75a":"code","39d9769a":"code","d3fa99ff":"code","4585e0cb":"code","7669a951":"code","96362859":"code","0817816b":"code","35e1e901":"code","4e06a85c":"code","de99a0cb":"code","aba156d4":"code","8d5a0fd1":"code","7406aa99":"code","885be1e7":"code","3fd6e6e6":"code","78df6d97":"code","691998b9":"code","bcafae2f":"markdown","8f1b2f22":"markdown","617509fb":"markdown","a0e75675":"markdown","9df27796":"markdown","ebb217fd":"markdown","e0e80b3f":"markdown","7594584c":"markdown","2e533a33":"markdown","dab50919":"markdown","74c83735":"markdown","8645e125":"markdown","adb087f6":"markdown","f5fe3fac":"markdown","9eeb1718":"markdown","7bb077a4":"markdown","319ec1bc":"markdown","1d619993":"markdown","81945146":"markdown","437a622d":"markdown","952d703e":"markdown","8c3611d1":"markdown","d0ac9537":"markdown","47101c3a":"markdown","f85233d3":"markdown","e5471cf0":"markdown","f715cc00":"markdown","b1ac97b7":"markdown","9307cc8c":"markdown","65edc4fe":"markdown","bbb35349":"markdown","69add232":"markdown","b465922d":"markdown","e777b527":"markdown","0faa979d":"markdown","7357b5c7":"markdown","2b43a577":"markdown","cc29b3b9":"markdown","e249c1d8":"markdown"},"source":{"6660b85e":"import tensorflow as tf\na = tf.constant(5)\na","46c4711d":"with tf.Session() as sess:\n    print(sess.run(a))","d19bb6f9":"with tf.Session() as sess:\n    sess.as_default()\n    print(a.eval())","3bf476f4":"tf.InteractiveSession()\na.eval()","8c67deeb":"# Kh\u1edfi t\u1ea1o m\u1ed9t gi\u00e1 tr\u1ecb Variable trong tensorflow\nv = tf.Variable([1, 2, 3], name = 'vector')\nm_2D = tf.Variable([[1, 2], [3, 4]], name = 'matrix_2D')\nm_nD = tf.Variable(tf.zeros([2, 2, 2]), name = 'matrix_nD')","2c248223":"# Kh\u1edfi t\u1ea1o m\u1ed9t gi\u00e1 tr\u1ecb Variable trong tensorflow\ngv_v = tf.get_variable(initializer = [1, 2, 3], name = 'vector')\ngv_m_2D = tf.get_variable(initializer = [[1, 2], [3, 4]], name = 'matrix_2D')\ngv_m_nD = tf.get_variable(initializer = tf.zeros([2, 2, 2]), name = 'matrix_nD')","292a9480":"# Kh\u1edfi t\u1ea1o t\u1ea5t c\u1ea3 trong 1 l\u1ea7n:\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run([gv_v, gv_m_2D, gv_m_nD]))","3cb8e702":"# Kh\u1edfi t\u1ea1o t\u1ea5t c\u1ea3 trong 1 l\u1ea7n:\nwith tf.Session() as sess:\n    sess.run(tf.variables_initializer([v, m_2D, m_nD]))\n    print(sess.run([v, m_2D, m_nD]))","328339c6":"x = tf.placeholder(tf.float32, shape = [2, 3])\ny = tf.constant([[1], [2], [3]], tf.float32) #l\u01b0u \u00fd ph\u1ea3i ki\u1ec3u d\u1eef li\u1ec7u c\u1ee7a y v\u00e0 x ph\u1ea3i tr\u00f9ng nhau\ny_hat = tf.matmul(x, y)","f036b588":"with tf.Session() as sess:\n    print(sess.run([y_hat], feed_dict = {x: [[1, 2, 3], [4, 5, 6]]}))","fa6fd4f5":"with tf.Session() as sess:\n    print(sess.run([y_hat], feed_dict = {x: [[3, 5, 1], [2, 5, 2]]}))","347dbca9":"t_zeros = tf.zeros([2, 3, 2],tf.float32)\nwith tf.Session() as sess:\n    print(sess.run(t_zeros))","0a908666":"t_origin = tf.constant([[[1, 2], \n                         [3, 4]],\n                        [[5, 6],\n                         [7, 8]]])\nt_zeros = tf.zeros_like(t_origin)\n\nwith tf.Session() as sess:\n    print(sess.run(t_zeros))","86300e66":"t_ones = tf.ones([2, 3, 2],tf.float32)\nt_ones_like = tf.ones_like(t_ones)\nwith tf.Session() as sess:\n    print('tensor ones:' + str(sess.run(t_ones)) + '\\n')\n    print('tensor ones like:' + str(sess.run(t_ones_like)))","d8f25283":"t_eye = tf.eye(3, 3, [1], tf.float32)\nwith tf.Session() as sess:\n    print(sess.run(t_eye))","81178dea":"t_random = tf.random_normal([2, 3], mean = 9, stddev = 2)\nwith tf.Session() as sess:\n    print(sess.run(t_random))","e9c8ef88":"t_rand_pois = tf.random_poisson(lam = 2, shape = [2, 3])\nt_rand_unif = tf.random_uniform(shape = [2, 3], minval = 0, maxval = 2)\n\nwith tf.Session() as sess:\n    print(sess.run([t_rand_pois, t_rand_unif]))","995ac092":"x = tf.constant(1)\ny = tf.constant(2)\nz = tf.add(x, y)\nt = tf.add(x, -y)\nwith tf.Session() as sess:\n    print(sess.run([z, t]))","8ad8e22d":"x = tf.constant(1)\ny = tf.constant(2)\nz = tf.multiply(x, y)\nwith tf.Session() as sess:\n    print(sess.run(z))","1f093a45":"import numpy as np\n\nx = tf.constant(np.arange(12), shape = [3, 4], dtype = tf.float32)\ny = tf.constant(np.arange(16), shape = [4, 4], dtype = tf.float32)\nz = tf.matmul(x, y)\n\nwith tf.Session() as sess:\n    print(sess.run(z))","90ab6243":"import numpy as np\n\nx = tf.constant(np.arange(40), shape = [2, 4, 5], dtype = tf.float32)\ny = tf.constant(np.arange(40), shape = [2, 5, 4], dtype = tf.float32) \nz = tf.matmul(x, y)\n\nwith tf.Session() as sess:\n    print(sess.run(z))","6217af9b":"x = tf.constant([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\ny = tf.reduce_mean(x)\nz = tf.reduce_mean(x, axis = 1, keepdims = True)\nwith tf.Session() as sess:\n    print(sess.run([y, z]))","fbdde75a":"x = tf.constant([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\ny = tf.reduce_min(x)\nz = tf.reduce_min(x, axis = 1, keepdims = True)\nwith tf.Session() as sess:\n    print(sess.run([y, z]))","39d9769a":"x = tf.constant([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], dtype = tf.float32)\ny = tf.exp(x)\nwith tf.Session() as sess:\n    print(sess.run([y]))","d3fa99ff":"x = tf.constant([[[1, -2, 3], [4, 5, -6], [7, -8, 9]]], dtype = tf.float32)\ny = tf.nn.relu(x)\nwith tf.Session() as sess:\n    print(sess.run([y]))","4585e0cb":"x = tf.constant([1, 1, 2, 3, 4, 5, 6, 7, 8], dtype = tf.float32)\ny = tf.nn.softmax(x)\nwith tf.Session() as sess:\n    print(sess.run([y]))","7669a951":"x = tf.constant([1, 1, 2, 3, 4, 5, 6, 7, 8], dtype = tf.float32)\ny = tf.nn.softmax(x)\nwith tf.Session() as sess:\n    print(sess.run(tf.reduce_sum(y)))","96362859":"x = tf.constant([1, 9, 0, 2, 3, 4, 5, 6, 7, 8], dtype = tf.float32)\ny = tf.argmin(x)\nz = tf.argmax(x)\nwith tf.Session() as sess:\n    print(sess.run([y, z]))","0817816b":"x = tf.constant(5)\nif x.graph is tf.get_default_graph():\n    print('x is one part of default graph')","35e1e901":"g = tf.Graph()\n\nwith g.as_default():\n    x = tf.constant(5)\n    if x.graph is g:\n        print('x is one part of default graph')","4e06a85c":"import tensorflow as tf\nx = tf.Variable(2, name = 'x_variabel')\ny = tf.Variable(4, name = 'y_variabel')\nz = tf.multiply(x, y)\n\n#Kh\u1edfi t\u1ea1o m\u1ed9t writer \u0111\u1ec3 l\u01b0u graph m\u1eb7c \u0111\u1ecbnh v\u00e0o \u1ed5 \u0111\u0129a\nwriter = tf.summary.FileWriter('first_graph_logs', tf.get_default_graph())\nwith tf.Session() as sess:\n    #Kh\u1edfi t\u1ea1o to\u00e0n b\u1ed9 c\u00e1c bi\u1ebfn\n    sess.run(tf.global_variables_initializer())\n    #Th\u00eam operation z v\u00e0o graph m\u1eb7c \u0111\u1ecbnh\n    sess.run(z)\n#\u0110\u00f3ng writer    \nwriter.close()","de99a0cb":"import os\nimport urllib.request as req\nimport numpy as np\nimport tensorflow as tf\n\n#set up parameter\nTRAIN = 'iris_training.csv'\nTRAIN_URL = 'http:\/\/download.tensorflow.org\/data\/iris_training.csv'\nTEST = 'iris_testing.csv'\nTEST_URL = 'http:\/\/download.tensorflow.org\/data\/iris_test.csv'\ninput_shape = 4\nn_classes = 3\n\ndef loadfile(filename, link):\n    if not os.path.exists(filename):\n        raw = req.urlopen(link).read().decode('utf-8')\n        with open(filename, 'w') as f:\n            f.write(raw)\n            \n    data = tf.contrib.learn.datasets.base.load_csv_with_header(\n      filename=filename,\n      target_dtype=np.int,\n      features_dtype=np.float32)\n    #normalize bi\u1ebfn d\u1ef1 b\u00e1o theo ph\u00e2n ph\u1ed1i chu\u1ea9n\n    mu = np.mean(data.data, axis = 0)\n    sigma = (np.std(data.data, axis=0))\n    predictor = (data.data - mu) \/ sigma\n    \n    #Chuy\u1ec3n bi\u1ebfn m\u1ee5c ti\u00eau sang d\u1ea1ng onehot endcoder\n#     target = np.eye(len(data.target), n_classes, dtype = np.float32)[data.target]\n    target = data.target\n    return {'predictor': predictor, 'target': target}\n\ntrain = loadfile(TRAIN, TRAIN_URL)\ntest = loadfile(TEST, TEST_URL)\n\n\nX = tf.placeholder(tf.float32, [None, input_shape])\ny = tf.placeholder(tf.int32, [None])","aba156d4":"# filename_queue = tf.train.string_input_producer([TRAIN, TEST])\n# reader = tf.TextLineReader(skip_header_lines = True)\n# key, value = reader.read(filename_queue)\n\n# record_defaults = [[0.], [0.], [0.], [0.], [0.]]\n\n# col1, col2, col3, col4, col5 = tf.decode_csv(\n#     value, record_defaults=record_defaults)\n\n# features = tf.stack([col1, col2, col3, col4])\n\n\n# with tf.Session() as sess:\n#   # Start populating the filename queue.\n#   coord = tf.train.Coordinator()\n#   threads = tf.train.start_queue_runners(coord=coord)\n\n# #   for i in range(120):\n#     # Retrieve a single instance:\n#   example, label = sess.run([features, col5])\n\n#   coord.request_stop()\n#   coord.join(threads)","8d5a0fd1":"#https:\/\/stackoverflow.com\/questions\/46264133\/weights-and-biases-not-updating-in-tensorflow\nweights = {\n    'l1': tf.Variable(tf.random_normal([input_shape, 10])),\n    'l2': tf.Variable(tf.random_normal([10, 15])),                              \n    'l3': tf.Variable(tf.random_normal([15, 20])),\n    'out': tf.Variable(tf.random_normal([20, 1]))\n}\n\n\nbiases = {\n    'l1': tf.Variable(tf.random_normal([1, 10])),\n    'l2': tf.Variable(tf.random_normal([1, 15])),                              \n    'l3': tf.Variable(tf.random_normal([1, 20])),\n    'out': tf.Variable(tf.random_normal([1, 3]))\n}\n                           \n    \ndef neural_network(X):\n    layer1 = tf.nn.relu(tf.add(tf.matmul(X, weights['l1']), biases['l1']))\n    layer2 = tf.nn.relu(tf.add(tf.matmul(layer1, weights['l2']), biases['l2']))\n    layer3 = tf.nn.relu(tf.add(tf.matmul(layer2, weights['l3']), biases['l3']))\n    out = tf.nn.softmax(tf.add(tf.matmul(layer3, weights['out']), biases['out']))\n    return out\n\nnn = neural_network(X)","7406aa99":"learning_rate = 0.5\nglobal_step = tf.Variable(0)\n\nloss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits = nn, labels = y))\ngrad_op = tf.reduce_sum(tf.gradients(loss_op, nn)[0])\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain_op = optimizer.minimize(loss_op, global_step = global_step)","885be1e7":"#T\u00ednh to\u00e1n m\u1ee9c \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a model\nmatch_pred = tf.equal(tf.cast(tf.argmax(nn, 1), tf.int32), y)\nacc_op = tf.reduce_mean(tf.cast(match_pred, tf.float32))","3fd6e6e6":"import time\n#X\u00e2y d\u1ef1ng v\u00f2ng l\u1eb7p fitting model tr\u00ean t\u1eebng batch\nbatch_size = 100 #K\u00edch th\u01b0\u1edbc m\u1ed7i batch.\nn_steps = 2000 #S\u1ed1 l\u01b0\u1ee3ng c\u00e1c l\u01b0\u1ee3t c\u1eadp nh\u1eadt d\u1eef li\u1ec7u.\nprint_every = 100 #Kho\u1ea3ng c\u00e1ch l\u01b0\u1ee3t c\u1eadp nh\u1eadt d\u1eef li\u1ec7u \u0111\u1ec3 in ra k\u1ebft qu\u1ea3 thu\u1eadt to\u00e1n.\n\n#T\u1ea1o h\u00e0m l\u1ea5y batch ti\u1ebfp theo. Khi l\u1ea5y h\u1ebft \u0111\u1ebfn batch cu\u1ed1i c\u00f9ng c\u1ee7a m\u1eabu s\u1ebd shuffle l\u1ea1i m\u1eabu.\ndef next_batch(X, batch_size, index = 0):\n    start = index\n    index += batch_size\n    if index > len(X['predictor']):\n        perm = np.arange(len(X['predictor']))\n        np.random.shuffle(perm)\n        X['predictor'] = X['predictor'][perm]\n        X['target'] = X['target'][perm]\n        start = 0\n        index = batch_size\n    end = index\n    return X['predictor'][start:end], X['target'][start:end], index\n\nwith tf.Session() as sess:\n    #Kh\u1edfi t\u1ea1o to\u00e0n b\u1ed9 c\u00e1c bi\u1ebfn.\n    sess.run(tf.global_variables_initializer())\n    idx = 0\n    for step in range(n_steps):\n        start_time = time.time()\n        batch_x, batch_y, idx = next_batch(train, batch_size = batch_size, index = idx)\n        #Th\u1ef1c thi thu\u1eadt to\u00e1n gradient descent\n        sess.run(train_op, feed_dict = {X: batch_x, y: batch_y})\n        loss = sess.run(loss_op, feed_dict = {X:batch_x, y:batch_y})\n        acc = sess.run(acc_op, feed_dict = {X:batch_x, y:batch_y})\n        grad = sess.run(grad_op, feed_dict = {X:batch_x, y:batch_y})\n        duration = time.time() - start_time\n        if step % print_every == 0:\n            print('Step {}; grads: {};Loss value: {:.8f}; Accuracy: {:.4f}; time: {:.4f} sec'.format(sess.run(global_step), grad, loss, acc, duration))\n#             print(sess.run(weights['l1']))\n            \n    print('Finished training!')\n    print('Loss value in test: {:.4f}; Accuracy in test: {:.4f}'.format(sess.run(loss_op, feed_dict = {X:test['predictor'], y:test['target']}),\n                                                          sess.run(acc_op, feed_dict = {X:test['predictor'], y:test['target']})))","78df6d97":"import os\nimport urllib.request as req\nimport numpy as np\nimport tensorflow as tf\n\n#set up parameter\nTRAIN = 'iris_training.csv'\nTRAIN_URL = 'http:\/\/download.tensorflow.org\/data\/iris_training.csv'\nTEST = 'iris_testing.csv'\nTEST_URL = 'http:\/\/download.tensorflow.org\/data\/iris_test.csv'\ninput_shape = 4\nn_classes = 3\n\ndef loadfile(filename, link):\n    if not os.path.exists(filename):\n        raw = req.urlopen(link).read().decode('utf-8')\n        with open(filename, 'w') as f:\n            f.write(raw)\n    data = tf.contrib.learn.datasets.base.load_csv_with_header(\n      filename=filename,\n      target_dtype=np.int,\n      features_dtype=np.float32)\n    #normalize bi\u1ebfn d\u1ef1 b\u00e1o theo ph\u00e2n ph\u1ed1i chu\u1ea9n\n    mu = np.mean(data.data, axis = 0)\n    sigma = (np.std(data.data, axis=0))\n    predictor = (data.data - mu) \/ sigma\n    \n    #Chuy\u1ec3n bi\u1ebfn m\u1ee5c ti\u00eau sang d\u1ea1ng onehot endcoder\n#     target = np.eye(len(data.target), n_classes, dtype = np.float32)[data.target]\n    target = data.target\n    return {'predictor': predictor, 'target': target}\n\ntrain = loadfile(TRAIN, TRAIN_URL)\ntest = loadfile(TEST, TEST_URL)\n\n\nX = tf.placeholder(tf.float32, [None, input_shape])\ny = tf.placeholder(tf.int32, [None])","691998b9":"#https:\/\/stackoverflow.com\/questions\/46264133\/weights-and-biases-not-updating-in-tensorflow\nweights = {\n    'l1': tf.Variable(tf.random_normal([input_shape, 10])),\n    'l2': tf.Variable(tf.random_normal([10, 15])),                              \n    'l3': tf.Variable(tf.random_normal([15, 20])),\n    'out': tf.Variable(tf.random_normal([20, 1]))\n}\n\n\nbiases = {\n    'l1': tf.Variable(tf.random_normal([1, 10])),\n    'l2': tf.Variable(tf.random_normal([1, 15])),                              \n    'l3': tf.Variable(tf.random_normal([1, 20])),\n    'out': tf.Variable(tf.random_normal([1, 3]))\n}\n                           \n    \ndef neural_network(X):\n    layer1 = tf.nn.relu(tf.add(tf.matmul(X, weights['l1']), biases['l1']))\n    layer2 = tf.nn.relu(tf.add(tf.matmul(layer1, weights['l2']), biases['l2']))\n    layer3 = tf.nn.relu(tf.add(tf.matmul(layer2, weights['l3']), biases['l3']))\n    out = tf.nn.softmax(tf.nn.relu(tf.add(tf.matmul(layer3, weights['out']), biases['out'])))\n    return out\n\nnn = neural_network(X)\n\n\n\nlearning_rate = 0.5\nglobal_step = tf.Variable(0)\n\nloss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n    logits = nn, labels = y))\ngrad_op = tf.reduce_sum(tf.gradients(loss_op, nn)[0])\noptimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\ntrain_op = optimizer.minimize(loss_op, global_step = global_step)\n\n\n\nmatch_pred = tf.equal(tf.cast(tf.argmax(nn, 1), tf.int32), y)\nacc_op = tf.reduce_mean(tf.cast(match_pred, tf.float32))\n\n\n\nimport time\n#X\u00e2y d\u1ef1ng v\u00f2ng l\u1eb7p fitting model tr\u00ean t\u1eebng batch\nbatch_size = 120 #K\u00edch th\u01b0\u1edbc m\u1ed7i batch.\nn_steps = 2000 #S\u1ed1 l\u01b0\u1ee3ng c\u00e1c l\u01b0\u1ee3t c\u1eadp nh\u1eadt d\u1eef li\u1ec7u.\nprint_every = 100 #Kho\u1ea3ng c\u00e1ch l\u01b0\u1ee3t c\u1eadp nh\u1eadt d\u1eef li\u1ec7u \u0111\u1ec3 in ra k\u1ebft qu\u1ea3 thu\u1eadt to\u00e1n.\n\n#T\u1ea1o h\u00e0m l\u1ea5y batch ti\u1ebfp theo. Khi l\u1ea5y h\u1ebft \u0111\u1ebfn batch cu\u1ed1i c\u00f9ng c\u1ee7a m\u1eabu s\u1ebd shuffle l\u1ea1i m\u1eabu.\ndef next_batch(X, batch_size, index = 0):\n    start = index\n    index += batch_size\n    if index > len(X['predictor']):\n        perm = np.arange(len(X['predictor']))\n        np.random.shuffle(perm)\n        X['predictor'] = X['predictor'][perm]\n        X['target'] = X['target'][perm]\n        start = 0\n        index = batch_size\n    end = index\n    return X['predictor'][start:end], X['target'][start:end], index\n\nwith tf.Session() as sess:\n    #Kh\u1edfi t\u1ea1o to\u00e0n b\u1ed9 c\u00e1c bi\u1ebfn.\n    sess.run(tf.global_variables_initializer())\n    idx = 0\n    for step in range(n_steps):\n        start_time = time.time()\n        batch_x, batch_y, idx = next_batch(train, batch_size = batch_size, index = idx)\n        #Th\u1ef1c thi thu\u1eadt to\u00e1n gradient descent\n        sess.run(train_op, feed_dict = {X: batch_x, y: batch_y})\n        loss = sess.run(loss_op, feed_dict = {X:batch_x, y:batch_y})\n        acc = sess.run(acc_op, feed_dict = {X:batch_x, y:batch_y})\n        grad = sess.run(grad_op, feed_dict = {X:batch_x, y:batch_y})\n        duration = time.time() - start_time\n        if step % print_every == 0:\n            print('Step {}; grads: {};Loss value: {:.8f}; Accuracy: {:.4f}; time: {:.4f} sec'.format(sess.run(global_step), grad, loss, acc, duration))\n#             print(sess.run(weights['l1']))\n            \n    print('Finished training!')\n    print('Loss value in test: {:.4f}; Accuracy in test: {:.4f}'.format(sess.run(loss_op, feed_dict = {X:test['predictor'], y:test['target']}),\n                                                          sess.run(acc_op, feed_dict = {X:test['predictor'], y:test['target']})))","bcafae2f":"**4. H\u00e0m relu**\n\nH\u00e0m relu c\u00f3 c\u00f4ng th\u1ee9c l\u00e0 $y = max(0, x)$. \u0110\u00e2y l\u00e0 m\u1ed9t h\u00e0m activation \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng kh\u00e1 ph\u1ed5 bi\u1ebfn trong c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y trong nh\u1eefng n\u0103m g\u1ea7n \u0111\u00e2y \u0111\u1ec3 thay th\u1ebf cho h\u00e0m sigmoid b\u1edfi \u0111\u1ea1o h\u00e0m c\u1ee7a n\u00f3 t\u1ed3n t\u1ea1i h\u1ea7u kh\u1eafp n\u01a1i (tr\u1eeb \u0111i\u1ec3m 0) v\u00e0 khi $x > 0$ \u0111\u1ea1o h\u00e0m ch\u1ec9 nh\u1eadn gi\u00e1 tr\u1ecb l\u00e0 1 n\u00ean s\u1ebd ti\u1ebft ki\u1ec7m chi ph\u00ed t\u00ednh to\u00e1n \u0111\u1ed3ng th\u1ed3i kh\u00f4ng b\u1ecb tri\u1ec7t ti\u00eau. Trong khi h\u1ea1n ch\u1ebf c\u1ee7a \u0111\u1ea1o h\u00e0m sigmoid \u0111\u00f3 l\u00e0 m\u1ed7i l\u1ea7n $x$ thay \u0111\u1ed5i s\u1ebd ph\u1ea3i t\u00ednh l\u1ea1i \u0111\u1ea1o h\u00e0m v\u00e0 s\u1ebd b\u1ecb tri\u1ec7t ti\u00eau khi gi\u00e1 tr\u1ecb c\u1ee7a $x$ v\u00f4 c\u00f9ng l\u1edbn ho\u1eb7c v\u00f4 c\u00f9ng nh\u1ecf. Ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng h\u00e0m `tf.nn.relu()` \u0111\u1ec3 t\u00ednh gi\u00e1 tr\u1ecb relu c\u1ee7a m\u1ed9t tensor.","8f1b2f22":"**2. Min\/Max**\n\nT\u01b0\u01a1ng t\u1ef1 nh\u01b0 c\u00e1ch t\u00ednh trung b\u00ecnh. Ta s\u1ebd s\u1eed d\u1ee5ng h\u00e0m `tf.reduce_min()` ho\u1eb7c `tf.reduce_max()` c\u00f3 c\u00fa ph\u00e1p l\u1ea7n l\u01b0\u1ee3t:\n\n`tf.reduce_max(\n    input_tensor,\n    axis=None,\n    keepdims=None,\n    name=None\n)`\n\n\n`tf.reduce_max(\n    input_tensor,\n    axis=None,\n    keepdims=None,\n    name=None\n)`\n\nK\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 s\u1ebd l\u00e0 gi\u00e1 tr\u1ecb min ho\u1eb7c max theo c\u00e1c chi\u1ec1u.","617509fb":"Ngo\u00e0i ra ta c\u00f2n c\u00f3 `tf.random_poisson()` s\u1eed d\u1ee5ng cho ph\u00e2n ph\u1ed1i poisson v\u00e0 `tf.random_uniform()` s\u1eed d\u1ee5ng cho ph\u00e2n ph\u1ed1i \u0111\u1ec1u. C\u00f3 c\u00fa ph\u00e1p l\u1ea7n l\u01b0\u1ee3t l\u00e0:\n\n`\ntf.random_poisson(\n    lam, #tham s\u1ed1 \u0111\u1eb7c tr\u01b0ng x\u00e1c \u0111\u1ecbnh ph\u00e2n ph\u1ed1i poisson\n    shape, #k\u00edch th\u01b0\u1edbc c\u1ee7a tensor\n    dtype=tf.float32,\n    seed=None,\n    name=None\n)\n`\n\nv\u00e0 \n\n`\ntf.random_uniform(\n    shape, #k\u00edch th\u01b0\u1edbc c\u1ee7a tensor\n    minval=0, #gi\u00e1 tr\u1ecb nh\u1ecf nh\u1ea5t\n    maxval=None, #gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t\n    dtype=tf.float32,\n    seed=None,\n    name=None\n)\n`","a0e75675":"## 2.4. C\u00e1c tensor \u0111\u1eb7c bi\u1ec7t \n\nC\u0169ng gi\u1ed1ng nh\u01b0 numpy, tensor s\u1ebd c\u00f3 nh\u1eefng ki\u1ec3u ma tr\u1eadn \u0111\u1eb7c bi\u1ec7t \u0111\u1ec3 gi\u00fap kh\u1edfi t\u1ea1o tensor nhanh h\u01a1n bao g\u1ed3m: ma tr\u1eadn 0, ma tr\u1eadn 1, ma tr\u1eadn \u0111\u01a1n v\u1ecb, ma tr\u1eadn ng\u1eabu nhi\u00ean,....\n\n**1. zeros tensor**\n\n`tf.zeros(shape = shape, dtype = dtype)` v\u1edbi dtype l\u00e0 ki\u1ec3u bi\u1ebfn v\u00e0 shape l\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a tensor.","9df27796":"**2. one tensor**\n\nHo\u00e0n to\u00e0n t\u01b0\u01a1ng t\u1ef1 nh\u01b0 zeros tensor, one tensor c\u0169ng c\u00f3 c\u00fa ph\u00e1p nh\u01b0 sau: `tf.ones(shape = shape, dtype = dtype)`.","ebb217fd":"* Kh\u1edfi t\u1ea1o m\u1ed9t bi\u1ebfn th\u00f4ng qua h\u00e0m tf.get_variable():\n\nC\u00fa ph\u00e1p:`tf.get_variable(initializer = value, name = name)`. Trong \u0111\u00f3 initializer l\u00e0 gi\u00e1 tr\u1ecb c\u1ee7a bi\u1ebfn v\u00e0 name l\u00e0 t\u00ean c\u1ee7a operation tr\u00ean \u0111\u1ed3 th\u1ecb.","e0e80b3f":"## 2.6. \u0110\u1ed3 th\u1ecb\n\n### 2.6.1. C\u00e1c t\u1ea1o m\u1ed9t \u0111\u1ed3 th\u1ecb\n\n\u0110\u1ed3 th\u1ecb l\u00e0 m\u1ed9t c\u1ea5u ph\u1ea7n kh\u00f4ng th\u1ec3 thi\u1ebfu trong m\u1ed9t model tensorflow. Trong m\u1ed9t \u0111\u1ed3 th\u1ecb s\u1ebd bao g\u1ed3m r\u1ea5t nhi\u1ec1u c\u00e1c operation object, m\u1ed7i m\u1ed9t operation object \u0111\u1ea1i di\u1ec7n cho m\u1ed9t t\u00ednh to\u00e1n.\n\nCh\u00fang ta c\u00f3 2 c\u00e1ch ch\u00ednh \u0111\u1ec3 t\u1ea1o ra m\u1ed9t graph \u0111\u00f3 l\u00e0 th\u00f4ng qua h\u00e0m `tf.Graph()` ho\u1eb7c `tf.get_default_graph()`. \n\nKhi g\u1ecdi h\u00e0m `tf.get_default_graph()` th\u00ec m\u1ed9t graph m\u1eb7c \u0111\u1ecbnh lu\u00f4n \u0111\u01b0\u1ee3c t\u1ea1o ra. Khi \u0111\u00f3 \u0111\u1ec3 th\u00eam m\u1ed9t ph\u1ea7n t\u1eed operation m\u1edbi v\u00e0o graph m\u1eb7c \u0111\u1ecbnh ta ch\u1ec9 c\u1ea7n kh\u1edfi t\u1ea1o operation. Ch\u1eb3ng h\u1ea1n nh\u01b0 b\u00ean d\u01b0\u1edbi ta s\u1ebd t\u1ea1o ra m\u1ed9t operation v\u00e0 ki\u1ec3m tra xem operation n\u00e0y c\u00f3 ph\u1ea3i l\u00e0 m\u1ed9t th\u00e0nh ph\u1ea7n c\u1ee7a graph m\u1eb7c \u0111\u1ecbnh hay kh\u00f4ng.","7594584c":"* Tr\u01b0\u1eddng h\u1ee3p tensor 3D","2e533a33":"H\u00e0m `tf.argmax(x, 1)` s\u1ebd t\u00ecm ra class c\u00f3 x\u00e1c xu\u1ea5t l\u1edbn nh\u1ea5t trong l\u1edbp c\u00e1c class tr\u1ea3 v\u1ec1. \u0110\u00e2y ch\u00ednh l\u00e0 class \u0111\u01b0\u1ee3c d\u1ef1 b\u00e1o. H\u00e0m `tf.equal()` s\u1ebd so s\u00e1nh class d\u1ef1 b\u00e1o c\u00f3 tr\u00f9ng v\u1edbi class th\u1ef1c t\u1ebf (ground truth) hay kh\u00f4ng. acc l\u00e0 t\u1ef7 l\u1ec7 ph\u1ea7n tr\u0103m d\u1ef1 b\u00e1o ch\u00ednh x\u00e1c \u0111\u01b0\u1ee3c t\u00ednh ra t\u1eeb trung b\u00ecnh c\u1ee7a to\u00e0n b\u1ed9 c\u00e1c k\u1ebft qu\u1ea3 so s\u00e1nh \u1edf to\u00e0n b\u1ed9 c\u00e1c quan s\u00e1t.","dab50919":"## 2.3. Placeholder\n\nL\u00e0 c\u00e1c bi\u1ebfn m\u00e0 gi\u00e1 tr\u1ecb ban \u0111\u1ea7u kh\u00f4ng \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o trong l\u00fac t\u1ea1o graph m\u00e0 \u0111\u01b0\u1ee3c truy\u1ec1n v\u00e0o t\u1eeb nh\u01b0 t\u1eadp data input \u0111\u1ec3 ch\u1ea1y m\u1ed9t m\u00f4 h\u00ecnh. Ch\u1eb3ng h\u1ea1n m\u1ed9t m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng d\u1ef1a tr\u00ean m\u1ed9t b\u1ed9 d\u1eef li\u1ec7u c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u1ea1i cho nhi\u1ec1u b\u1ed9 d\u1eef li\u1ec7u kh\u00e1c c\u00f3 c\u00f9ng \u0111\u1eb7c \u0111i\u1ec3m. Khi \u0111\u00f3 gi\u00e1 tr\u1ecb m\u00e0 ta c\u1ea7n truy\u1ec1n v\u00e0o m\u00f4 h\u00ecnh ch\u00ednh l\u00e0 c\u00e1c placeholder. \u0110\u1ed3 th\u1ecb c\u1ee7a m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c gi\u1eef nguy\u00ean (s\u1ed1 l\u1edbp, s\u1ed1 l\u01b0\u1ee3ng \u0111\u01a1n v\u1ecb trong t\u1eebng l\u1edbp, k\u00edch th\u01b0\u1edbc \u0111\u1ea7u v\u00e0o, \u0111\u1ea7u ra,...) ta thu \u0111\u01b0\u1ee3c c\u00e1c h\u1ec7 s\u1ed1 m\u1edbi. H\u00e0m s\u1ed1 \u0111\u1ec3 kh\u1edfi t\u1ea1o placeholder l\u00e0 `tf.placeholder(dtype, shape)`.\nGi\u1ea3 s\u1eed b\u00ean d\u01b0\u1edbi ta kh\u1edfi t\u1ea1o m\u1ed9t placeholder x c\u00f3 k\u00edch th\u01b0\u1edbc 2x3. Ch\u00fang ta c\u1ea7n t\u00ednh ph\u00e9p nh\u00e2n ma tr\u1eadn x v\u1edbi m\u1ed9t ma tr\u1eadn h\u1eb1ng s\u1ed1 y k\u00edch th\u01b0\u1edbc 3x1. Ta l\u00e0m nh\u01b0 sau:","74c83735":"**4. Fitting model**","8645e125":"Khi ta c\u1ea7n thay m\u1ed9t b\u1ed9 d\u1eef li\u1ec7u kh\u00e1c th\u00ec ch\u00fang ta s\u1ebd thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a x trong feed_dict. K\u1ebft qu\u1ea3 \u0111\u1ea7u ra s\u1ebd thay \u0111\u1ed5i nh\u01b0ng k\u00edch th\u01b0\u1edbc c\u1ee7a ma tr\u1eadn \u0111\u1ea7u ra kh\u00f4ng thay \u0111\u1ed5i.","adb087f6":"**3. tensor \u0111\u01a1n v\u1ecb**\n\nTensor \u0111\u01a1n v\u1ecb s\u1ebd c\u00f3 c\u00e1c ph\u1ea7n t\u1eed l\u00e0 m\u1ed9t ma tr\u1eadn \u0111\u01a1n v\u1ecb v\u00e0 \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o th\u00f4ng qua h\u00e0m:\n\n`\ntf.eye(\n    num_rows,\n    num_columns=None,\n    batch_shape=None,\n    dtype=tf.float32,\n    name=None\n)\n`\n\nTrong \u0111\u00f3 num_rows, num_columns l\u1ea7n l\u01b0\u1ee3t l\u00e0 s\u1ed1 d\u00f2ng v\u00e0 s\u1ed1 c\u1ed9t, batch_shape l\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a tensor theo batch. N\u1ebfu batch_shape = [3, 3] th\u00ec tensor s\u1ebd bao g\u1ed3m 3 d\u00f2ng v\u00e0 3 c\u1ed9t trong \u0111\u00f3 m\u1ed9t ph\u1ea7n t\u1eed \u1ee9ng v\u1edbi 1 d\u00f2ng v\u00e0 1 c\u1ed9t l\u00e0 m\u1ed9t ma tr\u1eadn k\u00edch th\u01b0\u1edbc num_rows x num_columns. dtype l\u00e0 ki\u1ec3u bi\u1ebfn v\u00e0 name l\u00e0 t\u00ean c\u1ee7a node. K\u00edch th\u01b0\u1edbc c\u1ee7a tensor khi bi\u1ec3u di\u1ec5n \u0111\u1ebfn t\u1eebng ph\u1ea7n t\u1eed c\u1ee7a ma tr\u1eadn l\u00e0 [batch_shape, num_rows, num_columns].","f5fe3fac":"Ngo\u00e0i ra ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng l\u1ec7nh `eval()` \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 c\u00e1c node trong m\u1ed9t \u0111\u1ed3 th\u1ecb. Nh\u01b0ng tr\u01b0\u1edbc \u0111\u00f3 ta ph\u1ea3i khai b\u00e1o m\u1ed9t session nh\u01b0 l\u00e0 m\u1eb7c \u0111\u1ecbnh \u0111\u1ec3 ch\u1ea1y \u0111\u01b0\u1ee3c graph.","9eeb1718":"Ta nh\u1eadn th\u1ea5y m\u1eb7c d\u00f9 \u0111\u00e3 kh\u1edfi t\u1ea1o gi\u00e1 tr\u1ecb cho a = 5 nh\u01b0ng khi hi\u1ec3n th\u1ecb gi\u00e1 tr\u1ecb c\u1ee7a a = 0. \u0110\u00f3 l\u00e0 v\u00ec ch\u00fang ta m\u1edbi ch\u1ec9 t\u1ea1o ra \u0111\u1ed3 th\u1ecb g\u1ed3m m\u1ed9t operation l\u00e0 a nh\u01b0ng v\u1eabn ch\u01b0a th\u1ef1c thi \u0111\u1ed3 th\u1ecb \u0111\u00f3. Do \u0111\u00f3 a v\u1eabn \u0111ang gi\u1eef gi\u00e1 tr\u1ecb m\u1eb7c \u0111\u1ecbnh l\u00e0 0. Khi th\u1ef1c thi \u0111\u1ed3 th\u1ecb ch\u00fang ta c\u1ea7n t\u1ea1o ra m\u1ed9t session \u0111\u1ec3 run operation a nh\u1eb1m k\u00edch ho\u1ea1t lu\u1ed3ng x\u1eed l\u00fd, khi \u0111\u00f3 gi\u00e1 tr\u1ecb a = 5. L\u01b0u \u00fd trong m\u1ed9t graph th\u00ec m\u1ed9t d\u1eef li\u1ec7u (c\u00f3 th\u1ec3 l\u00e0 h\u1eb1ng, bi\u1ebfn, placeholder) \u0111\u1ec1u l\u00e0 1 operation.","7bb077a4":"**2. X\u00e2y d\u1ef1ng m\u1ea1ng n\u01a1 ron**\n\nTa nh\u1eadn th\u1ea5y c\u00e1c gi\u00e1 tr\u1ecb c\u1ea7n tunning trong m\u00f4 h\u00ecnh l\u00e0 c\u00e1c weights v\u00e0 biases. Do \u0111\u00f3 trong m\u1ea1ng n\u01a1 ron ta s\u1ebd x\u00e2y d\u1ef1ng c\u00e1c layer  d\u1ef1a tr\u00ean ma tr\u1eadn h\u1ec7 s\u1ed1 v\u00e0 vector ch\u1ec7ch. L\u01b0u \u00fd ch\u00fang ta ph\u1ea3i khai b\u00e1o c\u00e1c ma tr\u1eadn h\u1ec7 s\u1ed1 v\u00e0 vector ch\u1ec7ch d\u01b0\u1edbi d\u1ea1ng Variable \u0111\u1ec3 thu\u1eadt to\u00e1n t\u1ed1i \u01b0u c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c \u0111\u00e2y l\u00e0 c\u00e1c gi\u00e1 tr\u1ecb c\u1ea7n c\u1eadp nh\u1eadt.\nM\u1ea1ng n\u01a1 ron s\u1ebd c\u00f3 g\u1ed3m 3 layer v\u1edbi k\u00edch th\u01b0\u1edbc m\u1ed7i layer nh\u01b0 sau:\n\n* layer 1: 10 units v\u1edbi k\u00edch th\u01b0\u1edbc ma tr\u1eadn [input_shape, 10] v\u00e0 h\u00e0m k\u00edch ho\u1ea1t l\u00e0 relu.\n\n* layer 2: 15 units v\u1edbi k\u00edch th\u01b0\u1edbc ma tr\u1eadn [10, 15] v\u00e0 h\u00e0m k\u00edch ho\u1ea1t l\u00e0 relu.\n\n* layer 3: 20 units v\u1edbi k\u00edch th\u01b0\u1edbc ma tr\u1eadn [15, 20] v\u00e0 h\u00e0m k\u00edch ho\u1ea1t l\u00e0 relu.\n\n* output layer: l\u00e0 m\u1ed9t flatten layer (1 vector). Do \u0111\u00f3 k\u00edch th\u01b0\u1edbc ma tr\u1eadn tr\u1ecdng s\u1ed1 l\u00e0 [20, 1]. H\u00e0m relu \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t h\u00e0m k\u00edch ho\u1ea1t v\u00e0 k\u1ebft qu\u1ea3 \u01b0\u1edbc l\u01b0\u1ee3ng x\u00e1c xu\u1ea5t \u0111\u01b0\u1ee3c t\u00ednh th\u00f4ng qua h\u00e0m softmax.\n\nDo output c\u1ee7a layer tr\u01b0\u1edbc s\u1ebd l\u00e0m input c\u1ee7a layer sau n\u00ean ta ho\u00e0n to\u00e0n x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c shape c\u1ee7a input d\u1ef1a v\u00e0o ouput c\u1ee7a layer tr\u01b0\u1edbc. \u0110\u1ec3 th\u1ecfa m\u00e3n c\u00e1c ph\u00e9p nh\u00e2n ma tr\u1eadn th\u1ef1c hi\u1ec7n \u0111\u01b0\u1ee3c th\u00ec c\u00e1c k\u00edch th\u01b0\u1edbc ma tr\u1eadn h\u1ec7 s\u1ed1 \u1edf m\u1ed7i layer ph\u1ea3i \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp sao cho width c\u1ee7a layer li\u1ec1n tr\u01b0\u1edbc ph\u1ea3i b\u1eb1ng height c\u1ee7a layer li\u1ec1n sau. C\u00e1c vector ch\u1ec7ch s\u1ebd c\u00f3 height = 1 v\u00e0 width ph\u1ea3i b\u1eb1ng width c\u1ee7a ma tr\u1eadn tr\u1ecdng s\u1ed1 thu\u1ed9c c\u00f9ng 1 layer.","319ec1bc":"## 2.2. Bi\u1ebfn\n\nTr\u00e1i ng\u01b0\u1ee3c v\u1edbi h\u1eb1ng. Bi\u1ebfn l\u00e0 l\u00e0 gi\u00e1 thay \u0111\u1ed5i trong m\u1ed9t \u0111\u1ed3 th\u1ecb. Th\u00f4ng th\u01b0\u1eddng trong m\u1ea1ng n\u01a1 ron th\u00ec bi\u1ebfn ch\u00ednh l\u00e0 ma tr\u1eadn h\u1ec7 s\u1ed1 c\u1ee7a h\u00e0m loss function. Bi\u1ebfn s\u1ebd lu\u00f4n c\u00f3 gi\u00e1 tr\u1ecb kh\u1edfi t\u1ea1o ban \u0111\u1ea7u \u0111\u1ec3 k\u00edch ho\u1ea1t thu\u1eadt to\u00e1n gradient. \u0110\u1ec3 t\u1ea1o ra m\u1ed9t variable trong tensorflow ta s\u1eed d\u1ee5ng h\u00e0m `tf.Variabel()` ho\u1eb7c `tf.get_variable()`.\n\n* Kh\u1edfi t\u1ea1o m\u1ed9t bi\u1ebfn th\u00f4ng qua h\u00e0m tf.Variabel():\n\nC\u00fa ph\u00e1p: `tf.Variabel(value = value, name = name)`. Trong \u0111\u00f3 value l\u00e0 c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a bi\u1ebfn v\u00e0 name l\u00e0 t\u00ean c\u1ee7a operation th\u1ec3 hi\u1ec7n tr\u00ean \u0111\u1ed3 th\u1ecb.","1d619993":"C\u00e1c bi\u1ebfn sau khi \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o n\u1ebfu mu\u1ed1n s\u1eed d\u1ee5ng \u0111\u01b0\u1ee3c s\u1ebd c\u1ea7n \u0111\u01b0\u1ee3c k\u00edch ho\u1ea1t th\u00f4ng qua 1 session b\u1eb1ng l\u1ec7nh `tf.global_variables_initializer()`.","81945146":"**4.tensor ng\u1eabu nhi\u00ean**\n\nL\u00e0 tensor c\u00f3 c\u00e1c ph\u1ea7n t\u1eed \u0111\u01b0\u1ee3c t\u1ea1o ra m\u1ed9t c\u00e1ch ng\u1eabu nhi\u00ean. Th\u00f4ng th\u01b0\u1eddng l\u00e0 ph\u00e2n ph\u1ed1i `gaussian` v\u1edbi k\u00ec v\u1ecdng v\u00e0 ph\u01b0\u01a1ng sai x\u00e1c \u0111\u1ecbnh. H\u00e0m `tf.random_normal()` s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 t\u1ea1o ra m\u1ed9t tensor ng\u1eabu nhi\u00ean c\u00f3 c\u00fa ph\u00e1p:\n`\ntf.random_normal(\n    shape,\n    mean=0.0,\n    stddev=1.0,\n    dtype=tf.float32,\n    seed=None,\n    name=None\n)\n`\n\nTrong \u0111\u00f3 shape l\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a tensor, mean l\u00e0 trung b\u00ecnh, stddev l\u00e0 \u0111\u1ed9 l\u1ec7ch chu\u1ea9n, gi\u00e1 tr\u1ecb seed l\u00e0 1 s\u1ed1 nguy\u00ean d\u01b0\u01a1ng s\u1ebd qui \u0111\u1ecbnh l\u1ea7n ch\u1ea1y l\u1ea1i sau s\u1ebd \u0111\u01b0a ra k\u1ebft qu\u1ea3 nh\u01b0 l\u1ea7n ch\u1ea1y tr\u01b0\u1edbc. M\u1eb7c \u0111\u1ecbnh c\u1ee7a seed l\u00e0 c\u00e1c k\u1ebft qu\u1ea3 m\u1ed7i l\u1ea7n ch\u1ea1y s\u1ebd kh\u00f4ng t\u00e1i l\u1eadp.","437a622d":"## 2.5. C\u00e1c to\u00e1n t\u1eed \n\n**1. Ph\u00e9p c\u1ed9ng\/ tr\u1eeb**\n\n\u0110\u01b0\u1ee3c th\u1ef1c hi\u1ec7n qua h\u00e0m `tf.add()` nh\u01b0 sau","952d703e":"Trong tr\u01b0\u1eddng h\u1ee3p ch\u1ec9 mu\u1ed1n kh\u1edfi t\u1ea1o c\u00e1c bi\u1ebfn trong danh s\u00e1ch c\u1ee5 th\u1ec3 ta c\u00f3 th\u1ec3 d\u00f9ng h\u00e0m `tf.variables_initializer()`.","8c3611d1":"M\u1ed9t l\u01b0u \u00fd m\u00e0 ch\u00fang ta kh\u00f4ng th\u1ec3 qu\u00ean \u0111\u00f3 l\u00e0 ph\u1ea3i kh\u1edfi t\u1ea1o c\u00e1c bi\u1ebfn b\u1eb1ng l\u1ec7nh `tf.global_variables_initializer()` tr\u01b0\u1edbc khi ch\u1ea1y session \u0111\u1ec3 c\u00e1c bi\u1ebfn \u0111\u01b0\u1ee3c g\u00e1n gi\u00e1 tr\u1ecb. B\u00ean d\u01b0\u1edbi l\u00e0 code c\u1ee7a to\u00e0n b\u1ed9 qu\u00e1 tr\u00ecnh x\u00e2y d\u1ef1ng m\u1ea1ng n\u01a1 ron.","d0ac9537":"**3. Ph\u00e9p nh\u00e2n ma tr\u1eadn**\n\nS\u1eed d\u1ee5ng h\u00e0m `tf.matmul()` c\u00f3 c\u00fa ph\u00e1p nh\u01b0 sau:\n\n`tf.matmul(\n    a,\n    b,\n    transpose_a=False,\n    transpose_b=False,\n    adjoint_a=False,\n    adjoint_b=False,\n    a_is_sparse=False,\n    b_is_sparse=False,\n    name=None\n)`\n\nTrong \u0111\u00f3 a, b l\u1ea7n l\u01b0\u1ee3t l\u00e0 c\u00e1c tensor b\u00ean tr\u00e1i v\u00e0 tensor b\u00ean ph\u1ea3i. L\u01b0u \u00fd l\u00e0 trong tr\u01b0\u1eddng h\u1ee3p tensor 2D th\u00ec s\u1ed1 c\u1ed9t c\u1ee7a a ph\u1ea3i b\u1eb1ng s\u1ed1 d\u00f2ng c\u1ee7a b; tr\u01b0\u1eddng h\u1ee3p tensor 3D th\u00ec k\u00edch th\u01b0\u1edbc c\u1ee7a a ph\u1ea3i b\u1eb1ng k\u00edch th\u01b0\u1edbc c\u1ee7a b khi chuy\u1ec3n v\u1ecb c\u1ee7a 2 chi\u1ec1u cu\u1ed1i c\u00f9ng. C\u00e1c tham s\u1ed1 transpose_a, transpose_b khi \u0111\u01b0\u1ee3c thi\u1ec7t l\u1eadp True (m\u1eb7c \u0111\u1ecbnh False) l\u1ea7n l\u01b0\u1ee3t c\u00f3 \u00fd ngh\u0129a c\u00f3 chuy\u1ec3n v\u1ecb tr\u01b0\u1edbc khi nh\u00e2n hay kh\u00f4ng. C\u00e1c tham s\u1ed1 a_is_sparse, b_is_sparse = True l\u1ea7n l\u01b0\u1ee3t c\u00f3 \u00fd ngh\u0129a coi a, b nh\u01b0 l\u00e0 c\u00e1c bi\u1ec3u di\u1ec5n c\u1ee7a ma tr\u1eadn sparse hay kh\u00f4ng (m\u1eb7c \u0111\u1ecbnh l\u00e0 kh\u00f4ng). B\u00ean d\u01b0\u1edbi l\u00e0 c\u00e1c v\u00ed d\u1ee5:\n\n* Tr\u01b0\u1eddng h\u1ee3p tensor 2D","47101c3a":"Ho\u1eb7c ta c\u00f3 th\u1ec3 t\u1ea1o m\u1ed9t zero tensor c\u00f3 k\u00edch th\u01b0\u1edbc b\u1eb1ng v\u1edbi m\u1ed9t tensor cho tr\u01b0\u1edbc th\u00f4ng qua h\u00e0m \n`tf.zeros_like(original_tensor)`","f85233d3":"**3. exponential**\n\nH\u00e0m m\u0169 c\u01a1 s\u1ed1 t\u1ef1 nhi\u00ean \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ph\u1ed5 bi\u1ebfn trong h\u1ecdc m\u00e1y ch\u1eb3ng h\u1ea1n nh\u01b0 trong h\u1ed3i qui logistic, h\u00e0m softmax, .... Ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng h\u00e0m `tf.exp()` \u0111\u1ec3 t\u00ednh gi\u00e1 tr\u1ecb c\u1ee7a c\u01a1 s\u1ed1 m\u0169 \u0111\u1ed1i v\u1edbi 1 tensor. K\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 l\u00e0 m\u1ed9t tensor ch\u1ee9a c\u00e1c ph\u1ea7n t\u1eed l\u00e0 l\u0169y th\u1eeba c\u01a1 s\u1ed1 t\u1ef1 nhi\u00ean ($e$) c\u1ee7a ph\u1ea7n t\u1eed t\u01b0\u01a1ng \u1ee9ng tr\u00ean tensor g\u1ed1c. C\u00fa ph\u00e1p c\u1ee7a h\u00e0m \u0111\u01a1n gi\u1ea3n nh\u01b0 sau:\n\n`tf.exp(x, name=None)`\n\nL\u01b0u \u00fd r\u1eb1ng x l\u00e0 tensor c\u00f3 ki\u1ec3u \u0111\u1ecbnh d\u1ea1ng n\u1eb1m trong c\u00e1c ki\u1ebfu `half, float32, float64, complex64, complex128`.","e5471cf0":"Khi \u0111\u00f3 m\u1ed9t operation y_hat \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o b\u1eb1ng v\u1edbi ph\u00e9p nh\u00e2n ma tr\u1eadn x v\u00e0 y th\u00f4ng qua h\u00e0m `tf.matmul()`. M\u1ed9t \u0111i\u1ec1u ta d\u1ec5 nh\u1eadn th\u1ea5y \u0111\u00f3 l\u00e0 gi\u00e1 tr\u1ecb c\u1ee7a x kh\u00f4ng \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o m\u1eb7c \u0111\u1ecbnh nh\u01b0 \u0111\u1ed1i v\u1edbi ki\u1ec3u bi\u1ebfn th\u00f4ng th\u01b0\u1eddng (`tf.variable()`) m\u00e0 ch\u1ec9 khi ta k\u00edch ho\u1ea1t \u0111\u1ed3 th\u1ecb t\u1ea1i operation y_hat th\u00ec ch\u00fang ta m\u1edbi truy\u1ec1n v\u00e0o gi\u00e1 tr\u1ecb c\u1ee7a x th\u00f4ng qua tham s\u1ed1 feed_dict c\u00f3 d\u1ea1ng c\u1ee7a m\u1ed9t dictionary trong python.","f715cc00":"K\u1ebft qu\u1ea3 cho th\u1ea5y m\u1ed9t operation khi \u0111\u01b0\u1ee3c t\u1ea1o ra lu\u00f4ng l\u00e0 m\u1ed9t ph\u1ea7n c\u1ee7a graph m\u1eb7c \u0111\u1ecbnh. \n\nKhi kh\u1edfi t\u1ea1o m\u1ed9t graph t\u1eeb h\u00e0m `tf.Graph()` th\u00ec graph \u0111\u00f3 kh\u00f4ng m\u1eb7c \u0111\u1ecbnh. Do \u0111\u00f3 \u0111\u1ec3 th\u00eam m\u1ed9t operation v\u00e0o graph th\u00ec ta ph\u1ea3i chuy\u1ec3n graph v\u1eeba kh\u1edfi t\u1ea1o sang m\u1eb7c \u0111\u1ecbnh b\u1eb1ng h\u00e0m `as_default()` v\u00e0 t\u1ea1o c\u00e1c operation trong graph \u0111\u00f3.","b1ac97b7":"**5. H\u00e0m softmax**\n\n\u0110\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 \u01b0\u1edbc l\u01b0\u1ee3ng x\u00e1c xu\u1ea5t x\u1ea3y ra c\u1ee7a m\u1ed9t class. Ch\u1eb3ng h\u1ea1n ch\u00fang ta c\u00f3 b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i $C$ class. Sau khi thi\u1ebft l\u1eadp m\u1ea1ng n\u01a1 ron ta t\u00ednh \u0111\u01b0\u1ee3c layer cu\u1ed1i c\u00f9ng l\u00e0 vector $\\mathbf{z} \\in \\mathbb{R}^{C}$. Khi \u0111\u00f3 gi\u00e1 tr\u1ecb c\u1ee7a h\u00e0m softmax \u1ee9ng v\u1edbi class $i$ s\u1ebd l\u00e0:\n\n$$a_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^C \\exp(z_j)}, ~~ \\forall i = 1, 2, \\dots, C$$\n\nM\u1ed9t t\u00ednh ch\u1ea5t ta d\u1ec5 nh\u1eadn th\u1ea5y l\u00e0 t\u1ed5ng c\u00e1c gi\u00e1 tr\u1ecb softmax c\u1ee7a to\u00e0n b\u1ed9 c\u00e1c class ph\u1ea3i b\u1eb1ng 1.\n\nsoftmax c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c t\u00ednh to\u00e1n th\u00f4ng qua h\u00e0m `tf.nn.softmax()` v\u1edbi c\u00fa ph\u00e1p:\n\n`tf.nn.softmax(logits, dim=-1, name=None)`\n\nTrong \u0111\u00f3 logits l\u00e0 m\u1ed9t tensor c\u00f3 c\u00e1c ki\u1ec3u bi\u1ebfn `half, float32, float64`.","9307cc8c":"X v\u00e0 y l\u1ea7n l\u01b0\u1ee3t l\u00e0 2 placeholder \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 ch\u1ee9a bi\u1ebfn d\u1ef1 b\u00e1o v\u00e0 v\u00e0 bi\u1ebfn \u0111\u01b0\u1ee3c d\u1ef1 b\u00e1o. K\u00edch th\u01b0\u1edbc c\u1ee7a n\u00f3 c\u00f3 height b\u1eb1ng None ng\u1ee5 \u00fd r\u1eb1ng ta c\u00f3 th\u1ec3 \u0111\u01b0a v\u00e0o bao nhi\u00eau quan s\u00e1t t\u00f9y \u00fd. \u0110i\u1ec1u n\u00e0y thu\u1eadn l\u1ee3i cho fitting model theo c\u00e1c batch_size kh\u00e1c nhau \u1edf b\u01b0\u1edbc 4.","65edc4fe":"### 2.6.2. Xu\u1ea5t \u0111\u1ed3 th\u1ecb tr\u00ean tensorboard\n\nTensorboard l\u00e0m m\u1ed9t c\u00f4ng c\u1ee5 gi\u00fap ta hi\u1ec3n th\u1ecb, theo d\u00f5i v\u00e0 qu\u1ea3n l\u00fd c\u00e1c \u0111\u1ed3 th\u1ecb t\u1eeb m\u1ed9t lu\u1ed3ng x\u1eed l\u00fd d\u1eef li\u1ec7u tensorflow. \u0110\u1ec3 c\u00e0i \u0111\u1eb7t tensorboard ch\u00fang ta g\u00f5 l\u1ec7nh.\n\n`pip install tensorboard=1.8.0`\n\nl\u01b0u \u00fd sau d\u1ea5u b\u1eb1ng l\u00e0 version c\u1ee7a tensorboard. N\u00ean c\u00e0i version c\u1ee7a tensorboard b\u1eb1ng v\u1edbi version c\u1ee7a tensorflow \u0111\u1ec3 tr\u00e1nh conflict. Trong tensorboard c\u00f3 m\u1ed9t v\u00e0i h\u00e0m quan tr\u1ecdng c\u00f3 ch\u1ee9c n\u0103ng nh\u01b0 sau:\n\n* `tf.summary.scalar`: \u0110\u01b0\u1ee3c g\u1eafn v\u00e0o c\u00e1c nodes \u0111\u1ec3 l\u01b0u l\u1ea1i c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a learning rate v\u00e0 loss sau m\u1ed7i l\u1ea7n x\u1eed l\u00fd.\n\n* `tf.summary.histogram`: V\u1ebd bi\u1ec3u \u0111\u1ed3 histogram ph\u00e2n ph\u1ed1i c\u1ee7a c\u00e1c tr\u1ecdng s\u1ed1 v\u00e0 gradients t\u1eeb m\u1ed9t layer c\u1ee5 th\u1ec3.\n\n* `tf.summary.merge_all`: S\u00e1t nh\u1eadp c\u00e1c th\u1ed1ng k\u00ea c\u1ee7a to\u00e0n b\u1ed9 c\u00e1c nodes trong graph v\u00e0o m\u1ed9t b\u1ea3n th\u1ed1ng k\u00ea d\u1eef li\u1ec7u chung.\n\n* `tf.summary.FileWriter`: L\u01b0u to\u00e0n b\u1ed9 c\u00e1c th\u1ed1ng k\u00ea c\u1ee7a \u0111\u1ed3 th\u1ecb v\u00e0o \u1ed5 \u0111\u0129a.\n\nCh\u00fang ta s\u1ebd quan t\u00e2m \u0111\u1ebfn h\u00e0m `tf.summary.FileWriter()` nh\u1ea5t b\u1edfi h\u00e0m n\u00e0y cho ph\u00e9p ta l\u01b0u v\u00e0 \u0111\u1ecdc m\u1ed9t graph tr\u00ean tensorboard. `tf.summary.FileWriter()` c\u00f3 c\u00fa ph\u00e1p nh\u01b0 sau:\n\n`tf.summary.FileWriter(folder directory, graph)`\n\nTrong \u0111\u00f3 folder directory l\u00e0 \u0111\u1ecba ch\u1ec9 c\u1ee7a th\u01b0 m\u1ee5c m\u00e0 ta s\u1ebd l\u01b0u \u0111\u1ed3 th\u1ecb. graph l\u00e0 \u0111\u1ed3 th\u1ecb c\u1ea7n l\u01b0u v\u00e0o disk. L\u01b0u \u00fd \u0111\u1ed3 th\u1ecb n\u00e0y lu\u00f4n ph\u1ea3i \u0111\u1ec3 m\u1eb7c \u0111\u1ecbnh \u0111\u1ec3 c\u00f3 th\u1ec3 th\u00eam c\u00e1c ph\u1ea7n operation v\u00e0o trong n\u00f3.\n\nB\u00ean d\u01b0\u1edbi ch\u00fang ta t\u1ea1o ra m\u1ed9t graph m\u1eb7c \u0111\u1ecbnh v\u00e0 l\u01b0u v\u00e0o \u1ed5 \u0111\u0129a \u1edf folder c\u00f9ng th\u01b0 m\u1ee5c cha v\u1edbi file hi\u1ec7n h\u00e0nh c\u00f3 t\u00ean l\u00e0 `first_graph_logs`:","bbb35349":"Sau khi ch\u1ea1y ch\u01b0\u01a1ng tr\u00ecnh tr\u00ean m\u1ed9t folder m\u1edbi t\u00ean l\u00e0 first_graph_logs \u0111\u01b0\u1ee3c t\u1ea1o trong c\u00f9ng th\u01b0 m\u1ee5c v\u1edbi file hi\u1ec7n h\u00e0nh v\u00e0 l\u01b0u to\u00e0n b\u1ed9 c\u00e1c k\u1ebft qu\u1ea3 x\u1eed l\u00fd v\u00e0 c\u1ea5u tr\u00fac c\u1ee7a graph. \u0110\u1ec3 \u0111\u1ecdc \u0111\u01b0\u1ee3c graph n\u00e0y ta v\u00e0o c\u1eeda s\u1ed1 command line tr\u1ecf t\u1edbi folder cha ch\u1ee9a file hi\u1ec7n h\u00e0nh v\u00e0 g\u00f5 l\u1ec7nh:\n\n`tensorboard --logdir first_graph_logs`\n\n--logdirs l\u00e0 t\u00ean c\u1ee7a tham s\u1ed1 v\u00e0 gi\u00e1 tr\u1ecb ph\u00eda sau l\u00e0 \u0111\u01b0\u1eddng d\u1eabn t\u1edbi folder l\u01b0u tr\u1eef graph. Khi \u0111\u00f3 k\u1ebft ch\u01b0\u01a1ng tr\u00ecnh s\u1ebd hi\u1ec3n th\u1ecb d\u00f2ng logs:\n\n`TensorBoard 1.8.0 at http:\/\/laptopTCC-PC:6006 (Press CTRL+C to quit)`\n\nTa copy \u0111\u01b0\u1eddng link v\u00e0 truy c\u1eadp th\u00f4ng qua tr\u00ecnh duy\u1ec7t \u0111\u1ec3 xem c\u1ea5u tr\u00fac c\u1ee7a graph.\n\n# 3. X\u00e2y d\u1ef1ng m\u1ea1ng n\u01a1 ron  tr\u00ean tensorflow\n\nB\u00ean d\u01b0\u1edbi ta s\u1ebd s\u1eed d\u1ee5ng tensorflow \u0111\u1ec3 x\u00e2y d\u1ef1ng m\u1ed9t m\u1ea1ng n\u01a1 ron network ph\u00e2n lo\u1ea1i c\u00e1c lo\u1ea1i hoa trong t\u1eadp d\u1eef li\u1ec7u iris. \u0110\u00e2y l\u00e0 b\u1ed9 d\u1eef li\u1ec7u bao g\u1ed3m 120 quan s\u00e1t tr\u00ean t\u1eadp train v\u00e0 30 quan s\u00e1t tr\u00ean t\u1eadp test v\u1ec1 d\u1eef li\u1ec7u K\u00edch th\u01b0\u1edbc chi\u1ec1u d\u00e0i v\u00e0 chi\u1ec1u r\u1ed9ng c\u1ee7a c\u00e1nh v\u00e0 t\u00e1n c\u1ee7a 3 lo\u00e0i hoa iris kh\u00e1c nhau. D\u1ef1a v\u00e0o k\u00edch th\u01b0\u1edbc c\u00e1c lo\u00e0i hoa ta s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t model m\u1ea1ng n\u01a1 ron \u0111\u1ec3 ph\u00e2n lo\u1ea1i \u0111\u00fang lo\u00e0i hoa v\u1ec1 nh\u00f3m c\u1ee7a ch\u00fang. \n\nQu\u00e1 tr\u00ecnh x\u00e2y d\u1ef1ng m\u1ea1ng n\u01a1 ron s\u1ebd tr\u1ea3i qua 4 b\u01b0\u1edbc:\n\n1. Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o.\n2. X\u00e2y d\u1ef1ng m\u1ea1ng n\u01a1 ron.\n3. Thi\u1ebft l\u1eadp h\u00e0m loss function v\u00e0 ph\u01b0\u01a1ng ph\u00e1p t\u1ed1i \u01b0u gradient descent.\n4. Fitting m\u00f4 h\u00ecnh v\u00e0 \u0111\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3.\n\nC\u00e1c b\u01b0\u1edbc l\u1ea7n l\u01b0\u1ee3t nh\u01b0 b\u00ean d\u01b0\u1edbi:\n\n**1. Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u **","69add232":"**2. Ph\u00e9p nh\u00e2n**\n\n\u0110\u01b0\u1ee3c th\u1ef1c hi\u1ec7n qua h\u00e0m `tf.multiply()` nh\u01b0 sau","b465922d":"Ki\u1ec3m tra t\u1ed5ng gi\u00e1 tr\u1ecb softmax tr\u1ea3 v\u1ec1.","e777b527":"Ngo\u00e0i ra ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng l\u1ec7nh tf.InteractiveSession() \u0111\u1ec3 t\u1ea1o ra m\u1ed9t session trong tr\u1ea1ng th\u00e1i m\u1eb7c \u0111\u1ecbnh (lu\u00f4n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng) \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 \u0111\u1ed3 th\u1ecb.","0faa979d":"**3. Thi\u1ebft l\u1eadp h\u00e0m loss function v\u00e0 ph\u01b0\u01a1ng ph\u00e1p t\u1ed1i \u01b0u gradient descent**","7357b5c7":"**6. argument min\/max**\n\nH\u00e0m n\u00e0y s\u1ebd tr\u1ea3 v\u1ec1 s\u1ed1 th\u1ee9 t\u1ef1 t\u01b0\u01a1ng \u1ee9ng theo m\u1ed9t chi\u1ec1u n\u00e0o \u0111\u00f3 trong m\u1ed9t tensor c\u00f3 gi\u00e1 tr\u1ecb l\u00e0 l\u1edbn nh\u1ea5t ho\u1eb7c nh\u1ecf nh\u1ea5t. C\u00fa ph\u00e1p:\n\n`tf.argmax(input, axis=None, name=None, dimension=None)`\n\n`tf.argmin(input, axis=None, name=None, dimension=None)`\n\ntrong \u0111\u00f3 input l\u00e0 tensor v\u00e0 axis l\u00e0 chi\u1ec1u \u0111\u1ec3 t\u00ednh argmax ho\u1eb7c argmin. Tr\u01b0\u1eddng h\u1ee3p th\u00f4ng th\u01b0\u1eddng \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng l\u00e0 vector (tensor-1D).","2b43a577":"# 1. Tensorflow \n## 1.1. Tensorflow l\u00e0 g\u00ec?\n\nTensorflow l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n m\u00e3 ngu\u1ed3n m\u1edf l\u00e0m vi\u1ec7c d\u1ef1a tr\u00ean l\u1eadp tr\u00ecnh *lu\u1ed3ng d\u1eef li\u1ec7u* (dataflow) th\u00f4ng qua c\u00e1c nhi\u1ec7m v\u1ee5. \u0110\u00e2y l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n to\u00e1n h\u1ecdc \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u trong c\u00e1c \u1ee9ng d\u1ee5ng c\u1ee7a h\u1ecdc m\u00e1y ch\u1eb3ng h\u1ea1n nh\u01b0 x\u00e2y d\u1ef1ng c\u00e1c m\u1ea1ng n\u01a1 ron, c\u00e1c thu\u1eadt to\u00e1n ph\u00e2n lo\u1ea1i kNN (k-Nearest Neighbor), SVM (Support Vector Machine),.... v\u00e0 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong c\u00e1c d\u1ef1 \u00e1n nghi\u00ean c\u1ee9u c\u0169ng nh\u01b0 s\u1ea3n ph\u00e2m c\u1ee7a google v\u00e0 thay th\u1ebf c\u00e1c th\u01b0 vi\u1ec7n g\u1ea7n \u0111\u00e2y.\n\n## 1.2. \u0110\u1eb7c tr\u01b0ng c\u1ee7a tensorflow.\n\nTensorflow l\u00e0 lu\u1ed3ng c\u1ee7a d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n qua m\u1ed9t \u0111\u1ed3 th\u1ecb t\u00ednh to\u00e1n. Khi x\u00e2y d\u1ef1ng m\u1ed9t model tensorflow ch\u00fang ta th\u01b0\u1eddng t\u00e1ch bi\u1ec7t 2 ph\u1ea7n ri\u00eang r\u1ebd \u0111\u00f3 l\u00e0: \n\n1. X\u00e2y d\u1ef1ng \u0111\u1ed3 th\u1ecb t\u00ednh to\u00e1n:  M\u1ed9t \u0111\u1ed3 th\u1ecb s\u1ebd bao g\u1ed3m c\u00e1c node v\u00e0 c\u00e1c c\u1ea1nh. Node c\u1ee7a \u0111\u1ed3 th\u1ecb s\u1ebd th\u1ec3 hi\u1ec7n ch\u1ee9c n\u0103ng t\u00ednh to\u00e1n (ch\u1eb3ng h\u1ea1n ph\u00e9p c\u1ed9ng, tr\u1eeb, nh\u00e2n, chia,...) v\u00e0 c\u00e1ch c\u1ea1nh th\u1ec3 hi\u1ec7n d\u1eef li\u1ec7u t\u00ednh to\u00e1n, th\u01b0\u1eddng l\u00e0 c\u00e1c d\u1eef li\u1ec7u nhi\u1ec1u chi\u1ec1u hay c\u00f2n g\u1ecdi l\u00e0 tensor \u0111\u01b0\u1ee3c k\u1ebft n\u1ed1i v\u1edbi nhau th\u00f4ng qua c\u00e1c node.\n\n\n2. Th\u1ef1c thi c\u00e1c lu\u1ed3ng t\u00ednh to\u00e1n tr\u00ean \u0111\u1ed3 th\u1ecb: \u0110\u1ed3 th\u1ecb t\u00ednh to\u00e1n m\u00e0 ta c\u00f3 m\u1edbi ch\u1ec9 l\u00e0 m\u1ed9t b\u1ea3n thi\u1ebft k\u1ebf c\u1ee7a m\u00f4 h\u00ecnh. Ch\u00fang ta c\u1ea7n ch\u1ea1y b\u1ea3n thi\u1ebft k\u1ebf \u0111\u00f3 b\u1eb1ng c\u00e1ch k\u00edch ho\u1ea1t m\u1ed9t session \u0111\u1ec3 th\u1ef1c thi c\u00e1c operation c\u1ee7a \u0111\u1ed3 th\u1ecb. C\u00e1c th\u1ef1c thi n\u00e0y s\u1ebd c\u1ea7n \u0111\u01b0\u1ee3c truy\u1ec1n d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o \u0111\u1ec3 l\u00e0m nguy\u00ean li\u1ec7u tr\u1ea3 v\u1ec1 k\u1ebft qu\u1ea3 \u1edf \u0111\u1ea7u ra.\n\n\n## 1.3. Gi\u1edbi thi\u1ec7u tensor.\n\nTensor l\u00e0 m\u1ed9t ki\u1ec3u d\u1eef li\u1ec7u cho ph\u00e9p l\u01b0u tr\u1eef \u0111\u01b0\u1ee3c s\u1ed1 chi\u1ec1u t\u00f9y \u00fd, n\u00f3 c\u00f3 th\u1ec3 l\u00e0 \u0111\u1ea1i l\u01b0\u1ee3ng v\u00f4 h\u01b0\u1edbng, vector, m\u1ea3ng 1 chi\u1ec1u, m\u1ea3ng 2 chi\u1ec1u ho\u1eb7c m\u1ea3ng k\u00edch th\u01b0\u1edbc n chi\u1ec1u. Sau \u0111\u00e2y l\u00e0 c\u00e1c v\u00ed d\u1ee5 v\u1ec1 tensor:\n\n* 0 chi\u1ec1u (\u0111\u1ea1i l\u01b0\u1ee3ng v\u00f4 h\u01b0\u1edbng): 1\n* 1 chi\u1ec1u: [1, 2, 3]\n* 2 chi\u1ec1u: [[1, 2], [3, 4]]\n* 3 chi\u1ec1u: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n\n# 2. C\u00e1c \u0111\u1ed1i t\u01b0\u1ee3ng trong tensor\n\n## 2.1. H\u1eb1ng s\u1ed1\n\nL\u00e0 gi\u00e1 tr\u1ecb c\u1ed1 \u0111\u1ecbnh trong tensorflow \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o th\u00f4ng qua h\u00e0m `tf.constant()`. Ch\u00fang ta s\u1ebd kh\u00f4ng th\u1ec3 thay \u0111\u1ed5i \u0111\u01b0\u1ee3c gi\u00e1 tr\u1ecb c\u1ee7a m\u1ed9t h\u1eb1ng s\u1ed1.","cc29b3b9":"## 2.5. C\u00e1c h\u00e0m \u0111\u1eb7c bi\u1ec7t\n\n**1. Trung b\u00ecnh**\n\n\u0110\u1ec3 t\u00ednh trung b\u00ecnh c\u1ee7a m\u1ed9t tensor ta s\u1ebd s\u1eed d\u1ee5ng h\u00e0m `tf.reduce_mean()`:\n\n`tf.reduce_mean(\n    input_tensor,\n    axis=None,\n    keepdims=None,\n    name=None\n)`\n\ninput_tensor l\u00e0 tensor c\u1ea7n t\u00ednh trung b\u00ecnh. axis l\u00e0 chi\u1ec1u c\u1ea7n t\u00ednh trung b\u00ecnh. N\u1ebfu axis = None th\u00ec m\u1eb7c \u0111\u1ecbnh t\u00ednh trung b\u00ecnh c\u1ee7a to\u00e0n b\u1ed9 c\u00e1c ph\u1ea7n t\u1eed c\u1ee7a tensor m\u00e0 kh\u00f4ng x\u00e9t \u0111\u1ebfn chi\u1ec1u. N\u1ebfu t\u00ednh trung b\u00ecnh theo m\u1ed9t chi\u1ec1u n\u00e0o \u0111\u00f3 th\u00ec s\u1ed1 chi\u1ec1u c\u1ee7a k\u1ebft qu\u1ea3 \u0111\u1ea7u ra s\u1ebd gi\u1ea3m \u0111i 1 do theo chi\u1ec1u \u0111\u00f3 tensor \u0111\u1ec1u c\u00f3 1 ph\u1ea7n t\u1eed \u0111\u1ea1i di\u1ec7n. keepdims = True c\u00f3 t\u00e1c d\u1ee5ng gi\u1eef nguy\u00ean s\u1ed1 chi\u1ec1u so v\u1edbi input__tensor.","e249c1d8":"H\u00e0m `tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)` s\u1ebd t\u00ednh ra h\u00e0m ra gi\u00e1 tr\u1ecb cross entropy gi\u1eefa 2 ph\u00e2n ph\u1ed1i \u0111\u01b0\u1ee3c d\u1ef1 b\u00e1o t\u1eeb m\u1ea1ng n\u01a1 ron (logits) v\u00e0 gi\u00e1 tr\u1ecb th\u1ef1c t\u1ebf (ground truth). Gi\u00e1 tr\u1ecb n\u00e0y c\u00e0ng nh\u1ecf th\u00ec gi\u1eefa 2 ph\u00e2n ph\u1ed1i c\u00e0ng s\u00e1t nhau t\u1ee9c m\u00f4 h\u00ecnh \u0111\u01b0a ra d\u1ef1 b\u00e1o c\u00e0ng chu\u1ea9n x\u00e1c.\nThu\u1eadt to\u00e1n gradient descent m\u00e0 ch\u00fang ta s\u1eed d\u1ee5ng l\u00e0 AdamOptimizer v\u1edbi learning_rate = 0.5. L\u01b0u \u00fd khi ta k\u00edch ho\u1ea1t qu\u00e1 tr\u00ecnh train_op th\u00ec thu\u1eadt to\u00e1n AdamOptimizer s\u1ebd t\u1ef1 \u0111\u1ed9ng t\u00ecm \u0111\u1ebfn c\u00e1c variable v\u00e0 c\u1eadp nh\u1eadt l\u1ea1i variable theo ph\u01b0\u01a1ng gradient descent. Qu\u00e1 tr\u00ecnh n\u00e0y t\u01b0\u01a1ng \u1ee9ng v\u1edbi vi\u1ec7c th\u1ef1c hi\u1ec7n c\u00e1c b\u01b0\u1edbc sau:\n\n* T\u00ednh gradient descent th\u00f4ng qua h\u00e0m `tf.train.compute_gradients()`.\n* C\u1eadp nh\u1eadt c\u00e1c variable theo ph\u01b0\u01a1ng gradient descent th\u00f4ng qua h\u00e0m `apply_gradients()`.\n\nDo \u0111\u00f3 c\u00e1c weights v\u00e0 bias c\u1ee7a m\u1ed7i layer \u0111\u1ec1u \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt sau m\u1ed7i l\u01b0\u1ee3t hu\u1ea5n luy\u1ec7n. Tuy nhi\u00ean c\u00f3 m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p thu\u1eadt to\u00e1n s\u1ebd d\u1eebng l\u1ea1i khi gradient qu\u00e1 nh\u1ecf v\u00e0 accuracy v\u00e0 loss function d\u01b0\u1eddng nh\u01b0 kh\u00f4ng thay \u0111\u1ed5i. Nh\u1eefng tr\u01b0\u1eddng h\u1ee3p n\u00e0y ta c\u1ea7n thay \u0111\u1ed5i t\u1ed1c \u0111\u1ed9 h\u1ecdc, thay \u0111\u1ed5i h\u00e0m loss function ho\u1eb7c \u0111i\u1ec1u ch\u1ec9nh l\u1ea1i c\u1ea5u tr\u00fac m\u1ea1ng n\u01a1 ron."}}