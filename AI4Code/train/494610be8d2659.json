{"cell_type":{"936d46f7":"code","0b3558b4":"code","e7e33627":"code","ac917bcc":"code","69dec14d":"code","f65f9067":"code","77285954":"code","6f31febd":"code","ebb10646":"markdown","bddfa574":"markdown","1dce04c6":"markdown","62f0bd05":"markdown","da025dda":"markdown","b1ced42c":"markdown","d520dbab":"markdown"},"source":{"936d46f7":"# install deep dive\n# Book reference https:\/\/d2l.ai\/\n!pip install --quiet d2l","0b3558b4":"%matplotlib inline\nfrom d2l import mxnet as d2l\nfrom mxnet import autograd, gluon, init, np, npx\nfrom mxnet.gluon import nn\nnpx.set_np()","e7e33627":"n_train, n_test, num_inputs, batch_size=20,100,200,5\ntrue_w, true_b = np.ones((num_inputs,1)) * 0.01, 0.05\ntrain_data = d2l.synthetic_data(true_w, true_b, n_train)\ntrain_iter = d2l.load_array(train_data, batch_size)\ntest_data = d2l.synthetic_data(true_w, true_b, n_test)\ntest_iter=d2l.load_array(test_data, batch_size, is_train=False)","ac917bcc":"def init_params():\n    w = np.random.normal(scale=1, size=(num_inputs, 1))\n    b = np.zeros(1)\n    w.attach_grad()\n    b.attach_grad()\n    return [w, b]","69dec14d":"def l2_penalty(w):\n    return(w**2).sum() \/ 2","f65f9067":"def train(lambd):\n    w, b = init_params()\n    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n    num_epochs, lr = 100,0.003\n    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',\n                            xlim=[5, num_epochs], legend=['train','test'])\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            with autograd.record():\n                # The L2 norm penalty term has been added, and broadcasting\n                # makes `l2_penalty(w)` a vector whose length is `batch_size`\n                l = loss(net(X), y) + lambd * l2_penalty(w)\n            l.backward()\n            d2l.sgd([w, b], lr, batch_size)\n        if(epoch+1) % 5 == 0:\n            animator.add(epoch+1, (d2l.evaluate_loss(net, train_iter, loss),\n                                   d2l.evaluate_loss(net, test_iter, loss)))\n    print('L2 norm of w:', np.linalg.norm(w))","77285954":"train(lambd=0)","6f31febd":"train(lambd=3)","ebb10646":"# Training without Regularization","bddfa574":"# Initialize Parameters","1dce04c6":"# Training Function","62f0bd05":"# Training with Regularization","da025dda":"# Linear Regression Regularization","b1ced42c":"# Create Data\n\n## Use of formula:\n\n![formula.png](attachment:formula.png)","d520dbab":"# L2 Norm Penalty"}}