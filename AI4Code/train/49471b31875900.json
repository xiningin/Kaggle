{"cell_type":{"d87ab197":"code","a8b988a9":"code","05734fee":"code","f72d8475":"code","97d4a3c7":"code","44f08359":"code","726c098e":"code","e0128579":"code","8705459b":"code","a024e271":"code","f687dcc6":"code","fb5c9509":"code","5b74c38f":"code","626407a8":"code","0d600fe1":"code","8c8b2676":"code","ea179a24":"code","60c8c518":"code","c456454c":"code","f740c9b2":"code","0d84f06c":"code","2e8e3405":"code","d5ffa184":"code","39609dba":"code","85d27cc4":"markdown","303438fb":"markdown","5d1a50c6":"markdown","efb447fe":"markdown","985b2e1d":"markdown","94927e27":"markdown","4c9b1985":"markdown","ab23937e":"markdown","0928c435":"markdown","1e206675":"markdown","d6d19910":"markdown","b24390b8":"markdown","527e7b91":"markdown","b838ab94":"markdown","0a0eb378":"markdown","8f6ae307":"markdown","9a4626f9":"markdown","bd4b0f5d":"markdown","b1b8c884":"markdown","23e34eb7":"markdown","90f9fe1c":"markdown","6fbac574":"markdown","815070a0":"markdown","95566db0":"markdown","776664a4":"markdown","289ddb33":"markdown","bcbe02bd":"markdown","bc7ceeb4":"markdown"},"source":{"d87ab197":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold, StratifiedKFold, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_recall_curve,SCORERS\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        data_path = dirname+'\/'\n        print(data_path)\n\n# Any results you write to the current directory are saved as output.","a8b988a9":"train = pd.read_csv(data_path+'train.csv')\n\ntrain.head()","05734fee":"train.describe().T","f72d8475":"test = pd.read_csv(data_path+'test.csv')\n\ntest.head()","97d4a3c7":"test.describe().T","44f08359":"train.groupby('Cover_Type')['Cover_Type'].count().plot.pie(),train.groupby('Cover_Type')['Cover_Type'].count()","726c098e":"print('Number of train rows:',train.shape[0])\nprint('Number of test rows:',test.shape[0])\nprint('Ratio: %0.4f' % (train.shape[0]\/test.shape[0]))\nplt.pie([train.shape[0],test.shape[0]])","e0128579":"#https:\/\/www.kaggle.com\/tunguz\/adversarial-santander\n\n# removed solid type 7 and 15 because of different values an test and train\n# (see https:\/\/www.kaggle.com\/mancy7\/simple-eda)\nav_features = list(set(train.columns)-set(['Id','Cover_Type','Solid_Type7','Solid_Type15']))\n\nav_X = train[av_features]#train.columns[:]]\nav_X['is_test'] = 0\nav_X_test = test[av_features]#test.columns[:]]\nav_X_test['is_test'] = 1\n\nav_train_test = pd.concat([av_X, av_X_test], axis =0)\nav_y = av_train_test['is_test']#.values\nav_train_test = av_train_test.drop(['is_test'],axis=1)\n","8705459b":"# setup KFold\nsplits = 3\n#repeats = 2\nrskf = StratifiedKFold(n_splits=splits, random_state=2019, shuffle=True)\n","a024e271":"\n# log regression\nscores = cross_val_score(LogisticRegression(random_state=2019, solver='lbfgs',max_iter=1000), av_train_test, av_y, cv=rskf, scoring='roc_auc') #'f1'\nprint(\"Log Regression Accuracy (RoC): %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'AV LogReg'))\n\n# random forrest\nscores = cross_val_score(RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2019), av_train_test, av_y, cv=rskf, scoring='roc_auc') #'f1'\nprint(\"Random Forrest Accuracy (RoC): %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'AV RandomForestClas'))\n","f687dcc6":"# https:\/\/www.kaggle.com\/ynouri\/random-forest-k-fold-cross-validation\ndef compute_roc_auc(X, y, index):\n    y_predict = clf.predict_proba(X.iloc[index])[:,1]\n    print(y_predict)\n    fpr, tpr, thresholds = roc_curve(y.iloc[index], y_predict)\n    auc_score_roc = auc(fpr, tpr)\n    # http:\/\/www.davidsbatista.net\/blog\/2018\/08\/19\/NLP_Metrics\/\n    precision, recall, thresholds = precision_recall_curve(y.iloc[index], y_predict)\n    auc_score_prc = auc(recall, precision)\n    \n    return y_predict, auc_score_roc, auc_score_prc","fb5c9509":"# http:\/\/fastml.com\/adversarial-validation-part-one\/\nclf = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2019)\n\nfprs, tprs, scores_roc_train, scores_roc_valid, scores_prc_train, scores_prc_valid = [], [], [], [], [], []\n# https:\/\/github.com\/zygmuntz\/adversarial-validation\/blob\/master\/numerai\/sort_train.py\npredictions = np.zeros(av_y.shape[0])\n\nfor (i_train, i_valid), i in zip(rskf.split(av_train_test,av_y),range(splits)):\n    print('Split', i)\n    clf.fit(av_train_test.iloc[i_train], av_y.iloc[i_train])\n    \n    # score\n    _, auc_score_roc_train, auc_score_prc_train = compute_roc_auc(av_train_test, av_y, i_train)\n    y_predict, auc_score_roc, auc_score_prc = compute_roc_auc(av_train_test, av_y, i_valid)\n    predictions[i_valid] = y_predict\n    \n    scores_roc_train.append(auc_score_roc_train)\n    scores_roc_valid.append(auc_score_roc)\n    scores_prc_train.append(auc_score_prc_train)\n    scores_prc_valid.append(auc_score_prc)\n    \n    # Feature Importance\n    ## https:\/\/towardsdatascience.com\/running-random-forests-inspect-the-feature-importances-with-this-code-2b00dd72b92e\n    clf.score(av_train_test.iloc[i_valid], av_y.iloc[i_valid])\n    rf_feature_importances = pd.DataFrame(clf.feature_importances_,\n                                       index = av_train_test.columns,\n                                       columns=['importance']).sort_values('importance', ascending=False)\n    display(rf_feature_importances.head(10))\n    \n    # Permutation Importance\n    permImp = PermutationImportance(clf, random_state=2021).fit(av_train_test.iloc[i_valid], av_y.iloc[i_valid]) \n    display(eli5.show_weights(permImp, feature_names = av_train_test.columns.tolist()))\n    \nprint('Mean Accuracy roc:', np.mean(scores_roc_valid))\nprint('Mean Accuracy precision recal:', np.mean(scores_roc_valid))\n\nav_train_test['p'] = predictions\n","5b74c38f":"av_train_test['is_test']=av_y\nav_train_test.groupby(['is_test']).describe()[['p']]","626407a8":"train['testalike'] = av_train_test.loc[av_train_test.is_test == 0]['p'].rank(method='first').astype(int)\ntrain['testalike_per_cover_type'] = train.groupby(['Cover_Type'])['testalike'].rank(method='first').astype(int)\n\n# check rank distribution per cover type\ntrain.groupby(['Cover_Type']).describe()[['testalike_per_cover_type']]\n# => looks good","0d600fe1":"train.to_csv('train_av.csv', index=False)","8c8b2676":"# https:\/\/www.kaggle.com\/phsheth\/forestml-eda-and-stacking-evaluation\n\nsns.distplot(train['Elevation'], label = 'train')\nsns.distplot(test['Elevation'], label = 'test')\nplt.legend()\nplt.title('Elevation')\nplt.show()","ea179a24":"sns.distplot(train['Horizontal_Distance_To_Roadways'], label = 'train')\nsns.distplot(test['Horizontal_Distance_To_Roadways'], label = 'test')\nplt.legend()\nplt.title('Horizontal_Distance_To_Roadways')\nplt.show()","60c8c518":"sns.distplot(train['Horizontal_Distance_To_Fire_Points'], label = 'train')\nsns.distplot(test['Horizontal_Distance_To_Fire_Points'], label = 'test')\nplt.legend()\nplt.title('Horizontal_Distance_To_Fire_Points')\nplt.show()","c456454c":"def testalike_split(X,y, val_size, testalike):\n    tr_idx, val_idx = np.split(testalike.argsort(),[-int(len(testalike)*val_size)])\n    \n    train_X = X[tr_idx]\n    train_y = y[tr_idx]\n    val_X = X[val_idx]\n    val_y = y[val_idx]\n    return (train_X, val_X, train_y, val_y), val_size\n\n\ndef simple_split(X,y, val_size):\n    return train_test_split(X, y, random_state = 2020, test_size = val_size), val_size\n\n\ndef run_classification(test_spliter):\n    (train_X, val_X, train_y, val_y), val_size = test_spliter\n    \n    clf = RandomForestClassifier(n_estimators = 100, n_jobs = -1, random_state=2020)\n    clf.fit(train_X, train_y)\n\n    y_hat_val = clf.predict(val_X)\n    y_hat = clf.predict(test_prep)\n    \n    cm = confusion_matrix(val_y, y_hat_val)\n    accuracy = accuracy_score(val_y, y_hat_val)\n    \n    print('Accuracy:', accuracy)\n    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=range(1,8), yticklabels=range(1,8))\n\n    return y_hat\n","f740c9b2":"X = train.drop(['Id','Soil_Type7','Soil_Type15'],axis=1)\ny = X.pop('Cover_Type')\ntestalike = X.pop('testalike')\ntestalike_group = X.pop('testalike_per_cover_type')\n\ntest_prep_ids = test['Id']\ntest_prep = test.drop(['Id','Soil_Type7','Soil_Type15'],axis=1)\n\n\n####  Scaling  ####\nsc = StandardScaler()\nX = sc.fit_transform(X)\ntest_prep = sc.transform(test_prep)\n","0d84f06c":"y_hat = run_classification(testalike_split(X,y, 0.1, testalike_group))\n\noutput = pd.DataFrame({'Id': test_prep_ids,'Cover_Type': y_hat})\noutput.to_csv('probe_testalike_split_10.csv', index=False)","2e8e3405":"y_hat = run_classification(testalike_split(X,y, 0.25, testalike_group))\n\noutput = pd.DataFrame({'Id': test_prep_ids,'Cover_Type': y_hat})\noutput.to_csv('probe_testalike_split_25.csv', index=False)","d5ffa184":"y_hat = run_classification(simple_split(X,y, 0.1))\n\noutput = pd.DataFrame({'Id': test_prep_ids,'Cover_Type': y_hat})\noutput.to_csv('probe_simple_split_10.csv', index=False)","39609dba":"y_hat = run_classification(simple_split(X,y, 0.25))\n\noutput = pd.DataFrame({'Id': test_prep_ids,'Cover_Type': y_hat})\noutput.to_csv('probe_simple_split_25.csv', index=False)","85d27cc4":"Set target of train = 0, set target of test = 1. \nCheck if you can differentiate between train and test data with Logistic Regression or Random Forest. \nIf the algorithms can't differentiate between train and test they seem to have similar characteristics.\n\nWe use roc_auc-Score to measure how well data entrys can be classified as test. 1 means it can perfectly classify the data, 0.5 means it can't classify the data. Hence 0.5 means train and test data are similar.\n\nWe examined at a few metrics (https:\/\/www.kaggle.com\/joatom\/discussing-metrics-on-imbalanced-data) to choose a reasonable metric to classify imbalanced data. We also run the experiments with different metrics all showing the same tendecies.\n","303438fb":"## 1. testalike group split 10%","5d1a50c6":"## Rank the \"test-likelyness\" on train","efb447fe":"These are the LB results:\n- no. 1: 0.752\n- no. 2: 0.739\n- no. 3: 0.743\n- no. 4: 0.731","985b2e1d":"==> The classes are equaly distributed","94927e27":"Seems like RF classifies about 25% of the train data as test data (0.99 at 75%-quartile where *is_test* == 0). We use this 25% as our out-of-adversarial-validation set later.","4c9b1985":"There is only one value for Soil_Type7 and Soil_Type15 in train but two in test. Will remove the feature later. (see https:\/\/www.kaggle.com\/mancy7\/simple-eda) ","ab23937e":"## 4. simple split 25 %","0928c435":"# Conclusions \nThis looks good. The test like validation set no.1 has about the same score on kernel and LB.\nIt might be tempting to use no.3 or no.4 because of the high score (0.86) on the kernel. But they are overfit.\nNo.1 seams to be a good validation set keep in control while feature and model engineering.\n\n\nThank you for reading sofar. So tell us, what are your favorite validation strategies?\n","1e206675":"# Probe different validation sets","d6d19910":"Expand to see code for classification and validation set splitting:","b24390b8":"The most influencial features on our AV predictions have differnt distributions on test and train as expected.","527e7b91":"## How many samples per CoverType","b838ab94":"## 3. simple split 10%","0a0eb378":"# Checking out the data","8f6ae307":"# Intro\nThere are several strategies to train a model to achieve a similar score on the kernels train data and on the test data where the results are shown on the leaderboard (LB). Commonly used are for instance cross validation (CV) \/ kfolding or model ensembling.\nWe will see that in this competition the train and test data sets are not equaly distributed. Therefore we examine Adversarial Validation as a strategy to generate a validation set that is similar to the test data.\n\nDisclaimer:\nPlease keep in mind that this is a beginners notebook and might include some wrong assumptions or conclusions. We are happy if you discuss with us in the comments section.\n\nHere are some references where I first stumbled upon Adverserial Validation (AV). Please upvote the kernels if you like them.\n\nhttp:\/\/fastml.com\/adversarial-validation-part-one\/\nhttps:\/\/www.kaggle.com\/tunguz\/adversarial-santander\n\nHere is another public kernel relevant to this competition concerning AV:\n\nhttps:\/\/www.kaggle.com\/lukeimurfather\/adversarial-validation-train-vs-test-an-update\n\nDiscussion about metrics:\nhttps:\/\/www.kaggle.com\/joatom\/discussing-metrics-on-imbalanced-data\n\nSome plots are taken from here:\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-eda-and-stacking-evaluation\n\nSome basic EDA is taken from here:\nhttps:\/\/www.kaggle.com\/mancy7\/simple-eda","9a4626f9":"# Comparing test and train data","bd4b0f5d":"We will take x % of the most testalike data from train to build our validation set in experiment no.1 and no.2. \nIn experiment no.3 and no.4 we choose a random sample as validation set. We run a simple Random Forest and predict the classes of the different validation sets.\nAfterwards we predict the classes for the test dataset and probe it against the leaderboard.\n\nIf our assumptions are right no.1 and no.2 will score about the same on the LB as the coresponding validation set.\n\n1. RF with 10% validation set of testalike\n2. RF with 25% validation set of testalike\n3. RF with simple 10% train-split\n4. RF with simple 25% train-Split","b1b8c884":"# Probing against the LB","23e34eb7":"We rank the train data on who high the prediction towards test was. In *testalike* wie rank it over the entiry dataset and on *testalike_per_cover_type* we do the ranking per Cover_Type.","90f9fe1c":"LogReg roc_auc accuracy of 75%. (Also tried on scaled data (Standard- and MinMax-Scaler) with same results. \nRandom Forrest roc_auc accurace of 76% \n(Also tried f1-score. Got a 0.99% for both.)\n\nLet's check what features cause the classification.","6fbac574":"There are very few train rows compared to test rows. \n\nWe want to check if the structure of train and test are similar. ","815070a0":"==> high rank means being more *test alike*","95566db0":"Let's have a look add the most influencial features.\n","776664a4":"## General","289ddb33":"## Adversarial Validation","bcbe02bd":"## 0. preprocessing","bc7ceeb4":"## 2. testalike group split 25%\n"}}