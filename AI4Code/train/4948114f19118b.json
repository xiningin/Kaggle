{"cell_type":{"c6c6a738":"code","afcf0063":"code","47cc25c0":"code","c8d20d4a":"code","e40f0635":"code","9c7e6775":"code","9b0b7c39":"code","d204c6ce":"code","005a2fab":"code","206760a3":"code","ee9d2052":"code","0f47634c":"code","284345af":"code","abf2407a":"code","a83449b8":"code","82eb8fdf":"code","83e0fbe0":"code","e7c7e629":"code","b42b17cd":"code","edc68e44":"code","cd38ba1c":"code","1fa5b682":"code","d4f660b6":"code","fcab62ca":"code","aacaabe0":"code","a34979e7":"code","4137ca7c":"code","43b581ef":"code","5fc0834e":"code","79480aa6":"code","d15e7710":"code","5f10418a":"code","d703901f":"code","cf51684c":"code","861cdb5b":"code","3c42a6ed":"code","b20d4f94":"code","33eb5667":"code","af9b2e43":"code","751dac4e":"code","48c76e01":"code","e94c8535":"markdown","3ea44c9e":"markdown","58357027":"markdown","390cc43a":"markdown","a2e603b0":"markdown","1eed50ab":"markdown","66738a79":"markdown","f9744879":"markdown","07c39d87":"markdown","0141c685":"markdown","bdf93e5a":"markdown","b9e138bd":"markdown","9a176953":"markdown"},"source":{"c6c6a738":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import *\nfrom sklearn.neighbors import *\nfrom sklearn.cluster import *\nfrom sklearn.linear_model import *\nfrom sklearn.preprocessing import *\nfrom sklearn.pipeline import make_pipeline\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","afcf0063":"seed=999\nlabel='Pawpularity'\n\nnp.random.seed(seed)","47cc25c0":"df=pd.read_csv('..\/input\/swin-transformer-features-for-pawpularity-contest\/swin_vectors.csv')","c8d20d4a":"df_train,df_test=train_test_split(df,test_size=0.25,random_state=seed,shuffle=False)\n\n\nx_train,x_test=df_train.drop(['Id',label],axis=1),df_test.drop(['Id',label],axis=1)\n\ny_train,y_test=df_train[label],df_test[label]","e40f0635":"alpha=1.","9c7e6775":"def rmse(y_true,y_pred):\n    return np.sqrt(np.mean(np.square(y_true*100-y_pred*100)))","9b0b7c39":"models=[]\n\nscores_train=[]\nscores_val=[]\n\nskf=StratifiedKFold(n_splits=5,random_state=seed)\nfor train_idx,val_idx in skf.split(x_train,y_train):\n    \n    \n    lr=make_pipeline(StandardScaler(),Ridge(alpha=alpha))\n    lr.fit(x_train.loc[train_idx],y_train.loc[train_idx]\/100.)\n    \n    models.append(lr)\n    \n    score=rmse(y_train.loc[train_idx]\/100.,lr.predict(x_train.loc[train_idx]))\n    scores_train.append(score)\n    score=rmse(y_train.loc[val_idx]\/100.,lr.predict(x_train.loc[val_idx]))\n    scores_val.append(score)\n    ","d204c6ce":"print(f'train score : {np.mean(scores_train)} +-{np.std(scores_train)}')\nprint(f'val score : {np.mean(scores_val)} +-{np.std(scores_val)}')","005a2fab":"\npreds_train=[]\npreds_test=[]\nfor m in models:\n    y_pred=m.predict(x_train)\n    preds_train.append(y_pred)\n    y_pred=m.predict(x_test)\n    preds_test.append(y_pred)\n    \npreds_train=np.mean(preds_train,axis=0)\npreds_test=np.mean(preds_test,axis=0)","206760a3":"score_train=rmse(y_train\/100.,preds_train)\nscore_test=rmse(y_test\/100.,preds_test)\n\nprint(f'train score:{score_train}')\nprint(f'test score:{score_test}')","ee9d2052":"def plot_hist(preds_train,preds_test,score_train,score_test):\n    fix,ax=plt.subplots(ncols=2,figsize=(20,5))\n    ax[0].set_title('Training Set')\n    ax[0].set_title(f'Testing Set & Prediction  (rmse:{score_train.astype(\"float16\")})')\n    \n    df_train[label].hist(ax=ax[0],lw=1,ec=\"black\", fc=\"red\", alpha=0.5,label='GroundTruth')\n    ax[0].hist(preds_train*100,lw=1,ec=\"yellow\", fc=\"green\", alpha=0.5,label='Prediction')\n    ax[0].set_xlabel(label)\n    ax[0].set_ylim([0,4000])\n    ax[0].legend()\n    \n    ax[1].set_title(f'Testing Set & Prediction  (rmse:{score_test.astype(\"float16\")})')\n    df_test[label].hist(ax=ax[1],lw=1,ec=\"black\", fc=\"red\", alpha=0.5,label='GroundTruth')\n    ax[1].hist(preds_test*100,lw=1,ec=\"yellow\", fc=\"green\", alpha=0.5,label='Prediction')\n    ax[1].legend()\n    ax[1].set_xlabel(label)\n    ax[1].set_ylim([0,1500])\n    plt.show()","0f47634c":"plot_hist(preds_train,preds_test,score_train,score_test)","284345af":"import tensorflow as tf\nimport sys\nsys.path.append('..\/input\/polynomial\/high-order-layers-master')\nimport high_order_layers.PolynomialLayers as poly\nfrom tensorflow.keras import backend as K","abf2407a":"tf.random.set_seed(seed)","a83449b8":"offset = -0.1\nfactor = 1.5 * 3.14159\nxTest = np.arange(100) \/ 50 - 1.0\nyTest = 0.5 * np.cos(factor * (xTest - offset))\n\nxTrain = tf.random.uniform([1000], minval=-1.0, maxval=1, dtype=tf.float32)\nyTrain = 0.5 * tf.math.cos(factor * (xTrain - offset))","82eb8fdf":"lr1=LinearRegression()\nlr2=make_pipeline(PolynomialFeatures(7),LinearRegression())","83e0fbe0":"lr1.fit(xTrain[:,np.newaxis],yTrain)\nlr2.fit(xTrain[:,np.newaxis],yTrain)","e7c7e629":"y1=lr1.predict(xTest[:,np.newaxis])\ny2=lr2.predict(xTest[:,np.newaxis])","b42b17cd":"fig,ax=plt.subplots(ncols=2,figsize=(10,5))\n\nax[0].scatter(xTest,y1)\nax[0].scatter(xTest,yTest)\nax[0].set_title('linear regression')\n\nax[1].scatter(xTest,y2)\nax[1].scatter(xTest,yTest)\nax[1].set_title('polynomial regression')\n\nplt.show()","edc68e44":"def get_model(x,y,val=False,epochs=5,loss='mse',batch_size=32,order=poly.b3D):\n    nn=tf.keras.Sequential([poly.Polynomial(1,order)])\n    nn.compile(optimizer='adam',loss=loss,metrics=[loss])\n    if val:\n        nn.fit(x,y,validation_split=0.25,batch_size=batch_size,epochs=epochs)\n    else:\n        nn.fit(x,y,batch_size=batch_size,epochs=epochs)\n    return nn","cd38ba1c":"nn=get_model(xTrain[:,np.newaxis],yTrain,epochs=10,batch_size=1)","1fa5b682":"y=nn.predict(xTest[:,np.newaxis])","d4f660b6":"plt.scatter(xTest,y)\nplt.scatter(xTest,yTest)","fcab62ca":"def plot_losses(hist,type_):\n    plt.figure(figsize=(10,5))\n    for i,h in enumerate(hist):\n        plt.plot(h['rmse'],label=f'{i}-order train')\n\n        plt.plot(h['val_rmse'],label=f'{i}-order val')\n    \n    plt.title(type_,loc='right')\n    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=3, fancybox=True, shadow=True)\n    plt.xlabel('iteration')\n    plt.ylabel('rmse')\n    plt.ylim([5,25])\n    plt.show()","aacaabe0":"def cross_entropy(y_true,y_pred):\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y_true)) #Continuous Bernoulli distribution\n\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_true*100 - tf.nn.sigmoid(y_pred)*100)))","a34979e7":"def get_model_complex(x,y,val=False,epochs=5,loss='mse',batch_size=32,order=poly.b2D):\n    nn=tf.keras.Sequential([\n        tf.keras.layers.Dense(32,activation='relu'),\n        poly.Polynomial(1,order),\n    ])\n    \n    nn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n               loss=cross_entropy,metrics=[loss])\n    if val:\n        history=nn.fit(x,y,validation_split=0.25,batch_size=batch_size,epochs=epochs,verbose=0)\n    else:\n        history=nn.fit(x,y,batch_size=batch_size,epochs=epochs,verbose=0)\n    return nn,history","4137ca7c":"def train(type_):\n    hist=[]\n    nns=[]\n    \n    for order in tqdm(order_dict[type_]):\n        nn,history=get_model_complex(x_train,y_train\/100.,val=True,\n                     loss=rmse,batch_size=8,epochs=10,order=order)\n        \n        hist.append(history.history)\n        nns.append(nn)\n        \n    return nns,hist","43b581ef":"def plot_hist_nn(NN):\n    for i,nn in enumerate(NN):\n        print(f'order-{i+1}')\n        preds_train=nn.predict(x_train)\n\n        score_train=rmse(np.array(y_train)\/100.,preds_train).numpy()\n        \n        print(score_train)\n        \n        preds_test=nn.predict(x_test)\n\n        score_test=rmse(np.array(y_test)\/100.,preds_test).numpy()\n\n        plot_hist(tf.nn.sigmoid(preds_train).numpy(),\n                  tf.nn.sigmoid(preds_test).numpy(),score_train,score_test)","5fc0834e":"\norder_dict={\n    'orders_simple':[\n        poly.b1,\n        poly.b2,\n        poly.b3,\n        poly.b4,\n        poly.b5\n    ],\n    \n    'orders_continuous':[\n        poly.b1C,\n        poly.b2C,\n        poly.b3C,\n        poly.b4C,\n        poly.b5C\n    ],\n    'orders_dis_continuous':[\n        poly.b1D,\n        poly.b2D,\n        poly.b3D,\n        poly.b4D,\n        poly.b5D\n    ]}","79480aa6":"type_='orders_simple'","d15e7710":"simple_nn,simple_hist=train(type_)","5f10418a":"plot_losses(simple_hist,type_)","d703901f":"plot_hist_nn(simple_nn)","cf51684c":"type_='orders_continuous'","861cdb5b":"con_nn,con_hist=train(type_)","3c42a6ed":"plot_losses(con_hist,type_)","b20d4f94":"plot_hist_nn(con_nn)","33eb5667":"type_='orders_dis_continuous'","af9b2e43":"discon_nn,discon_hist=train(type_)","751dac4e":"plot_losses(discon_hist,type_)","48c76e01":"plot_hist_nn(discon_nn)","e94c8535":"## Conclusion :  Just ...  I want to add sample weights on 80~100 data","3ea44c9e":"# Ridge","58357027":"## Simple polynomial","390cc43a":"## TF model","a2e603b0":"* For simple data","1eed50ab":"# Test polynomial","66738a79":"### Functions","f9744879":"## Compare linear regression and polynomial regression","07c39d87":"The way I compute rmse having bug, this not the true rmse score","0141c685":"### Data Spliting","bdf93e5a":"## Test polynomial order","b9e138bd":"## Piecewise discontinuous polynomial","9a176953":"## Piecewise continuous polynomial"}}