{"cell_type":{"35e702c8":"code","d56e9c14":"code","1e1920d7":"code","963ff4bb":"code","33fd3804":"code","4f7603bd":"code","84478a84":"code","339df4e2":"code","58f572fd":"code","5a7e5cef":"code","e35fbebe":"code","9349356a":"markdown","5827c58c":"markdown","b0985be3":"markdown","de8ca6e1":"markdown","c80f8cac":"markdown","b69afde9":"markdown","0f76c3f8":"markdown","8ae632e3":"markdown","63099db7":"markdown","447bc19a":"markdown","f4a9349e":"markdown"},"source":{"35e702c8":"print(\"********************** CUDA Version ********************** \\n - \\n\")\n!nvcc --version\nprint(\"********************** CPU Info ********************** \\n - \\n\")\n!cat \/proc\/cpuinfo\nprint(\"********************** CPU Count ********************** \\n - \\n\")\nimport os\nprint(os.cpu_count())\nprint(\"********************** GPU Info ********************** \\n - \\n\")\n!nvidia-smi\nprint(\"********************** Python Version ********************** \\n - \\n\")\n!python -V","d56e9c14":"# clone repo\n!git clone https:\/\/github.com\/wyattowalsh\/sports-analytics.git\n\n# change directory to directory that contains this notebook\n%cd \/kaggle\/working\/sports-analytics\/basketball\/notebooks\/\n\n# install conda\n# !bash sports-analytics\/project_resources\/bash_scripts\/install_conda_in_colab.sh \n\n# install dependencies\n!pip3 install -r ..\/..\/dependencies\/basketball\/data_collection.txt\n!pip3 install --upgrade --force-reinstall dask\n!pip3 install --upgrade --force-reinstall dask-cuda\n!pip3 install 'fsspec>=0.3.3'\n\n# restart kernel\nexit()  ","1e1920d7":"# nba_api dependencies\nfrom nba_api.stats.endpoints import commonplayerinfo\n\n# datascience stack\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\nimport sqlite3 as sql\nimport dask\nfrom dask.distributed import Client, progress, LocalCluster\nimport multiprocessing as mp\nimport dask.array as da\n\n# system utility stack\nimport os\nimport time\nfrom requests.packages.urllib3.exceptions import ProxyError\nimport urllib.error\nimport urllib.request\n\n# change directory to directory that contains this notebook\n%cd \/kaggle\/working\/sports-analytics\/basketball\/notebooks\/\n\n# Move and change permissions as needed, allowing for import\n!mkdir -p ~\/.kaggle\/ && cp ..\/..\/..\/..\/input\/kagglejson\/kaggle.json  ~\/.kaggle\/ && chmod 600 ~\/.kaggle\/kaggle.json\nimport kaggle\n\n!cp ..\/..\/..\/..\/input\/basketball\/basketball.sqlite .\/","963ff4bb":"\n!wget -O http_proxies.txt \"https:\/\/api.proxyscrape.com\/v2\/?request=getproxies&protocol=http&timeout=10000&country=all&ssl=yes&anonymity=all&simplified=true\"\n\nwith open('http_proxies.txt', 'r') as file:\n    proxies = file.read().split('\\n')\nprint(\"Original number of proxies: \", len(proxies))\n\ndef check_proxies(proxy):\n    try:\n        urllib.request.urlopen(\"http:\/\/\" + proxy, timeout = 30)\n        print(\"alive proxy detected\")\n    except:\n        return proxy\n\npool = mp.Pool(64)\ndead_proxies = pool.map(check_proxies, proxies)\n\n[proxies.remove(proxy) for proxy in dead_proxies if proxy in proxies]\nif \"\" in proxies:\n    proxies.remove(\"\")\nprint(\"Number of proxies alive: \", len(proxies))\n","33fd3804":"# Make sure to put appropiate number of workers given info provided in the output of the first cell\ncluster = LocalCluster(n_workers=64) \nc = Client(cluster)\nc","4f7603bd":"proxies = get_proxies()\nwith open('valid_proxies.txt', 'w') as f:\n    for proxy in proxies:\n        f.write(\"%s\\n\" % proxy)\nc.shutdown()\nproxies","84478a84":"def get_current_player_attributes():\n    conn = sql.connect('basketball.sqlite')\n    try:\n        return pd.read_sql(\"SELECT * FROM Player_Attributes\", conn).values\n    except:\n        print(\"Table does not exist\")\n        return None\n# current_player_attributes = get_current_player_attributes()\n\ndef get_player_ids():\n    conn = sql.connect('basketball.sqlite')\n    try:\n        return pd.read_sql(\"SELECT id From Player\", conn).values\n    except:\n        print(\"Table or column does not exist\")\n        return None\n    \ndef get_common_player_info_df(player_id):\n    res = []\n    i = 0\n    j = 0\n    while len(res) < 1 :\n        try:\n            res = commonplayerinfo.CommonPlayerInfor(player_id = player_id, timeout = 30)\n            res_dfs = res.get_data_frames()\n            res_df = pd.merge(res_dfs[0], res_dfs[1], how='left', left_on=[\"PERSON_ID\", \"DISPLAY_FIRST_LAST\"], right_on=['PLAYER_ID', 'PLAYER_NAME'])\n            res_df = res_df.drop(['TimeFrame'], axis=1)\n            print(\"success\")\n        except:\n            try:\n                res = commonplayerinfo.CommonPlayerInfo(player_id=player_id, proxy = \"http:\/\/\" + proxies[i], timeout=30)\n                res_dfs = res.get_data_frames()\n                res_df = pd.merge(res_dfs[0], res_dfs[1], how='left', left_on=['PERSON_ID', 'DISPLAY_FIRST_LAST'], right_on=['PLAYER_ID', 'PLAYER_NAME'])\n                res_df = res_df.drop(['TimeFrame'], axis=1)\n                print(\"success with proxy\")\n            except:\n                if (i + 1) < len(proxies):\n                    print(\"proxy #{} failed. Trying next proxy...\".format(i))\n                    i = i + 1\n                else:\n                    return None\n    return res_df\n\nplayer_ids = get_player_ids()\npool = mp.Pool(48)\ndfs = pool.map(get_common_player_info_df, player_ids[0:49])\ndfs = [df for df in dfs if df != None]\ndf = pd.concat(dfs)\ndf.head()","339df4e2":"res_df","58f572fd":"def get_common_player_info(player_id):\n  with open('valid_proxies.txt', 'r') as file:\n    proxies = file.read().split('\\n')\n  res_dfs = []\n  i = 0\n  proxies_retrieved = False\n  # while response is empty\n  while len(res_dfs) <= 0: \n    # try the request without a proxy\n    try:\n      res_dfs = commonplayerinfo.CommonPlayerInfo(player_id=player_id, timeout=100).get_data_frames()\n      res_df = pd.merge(res_dfs[0], res_dfs[1], how='left', left_on=['PERSON_ID', 'DISPLAY_FIRST_LAST'], right_on=['PLAYER_ID', 'PLAYER_NAME'])\n      res_df = res_df.drop(['TimeFrame'], axis=1)\n      print(\"******* SUCCESS ******* \\n ******* {} ******* \\n\".format(player_id))\n      return res_df\n    # if still fails, then try with proxy\n    except:\n      try:\n        res_dfs = commonplayerinfo.CommonPlayerInfo(player_id=player_id, timeout=150).get_data_frames()\n        res_df = pd.merge(res_dfs[0], res_dfs[1], how='left', left_on=['PERSON_ID', 'DISPLAY_FIRST_LAST'], right_on=['PLAYER_ID', 'PLAYER_NAME'])\n        res_df = res_df.drop(['TimeFrame'], axis=1)\n        print(\"******* SUCCESS ******* \\n ******* {} ******* \\n\".format(player_id))\n        return res_df\n      # if still fails, move on to next proxy, unless out of proxies\n      except:\n        if (i + 1) < len(proxies):\n          i = i + 1\n        # if out of proxies, restart counter and get new proxies\n        else:\n          i = 0\n         # if proxies_retrieved:\n           # print(\"******* FAILURE ****** \\n ****** {} ******* \\n ******* RETURNING NONE ******\".format(player_id))\n         #   return None\n        #  else:\n           # print(\"******* FAILURE ****** \\n ****** {} ******* \\n ******* COLLECTING NEW PROXIES AND TRYING REQUEST AGAIN ******\".format(player_id))\n         #   i = 0\n         #   proxies = get_proxies()\n         #   proxies_retrieved = True\n      ","5a7e5cef":"# Make sure to put appropiate number of workers given info provided in the output of the first cell\ndef main():\n    cluster = LocalCluster(n_workers=20) \n    with Client(address=cluster):\n      conn = sql.connect('..\/..\/..\/..\/input\/basketball\/basketball.sqlite')\n      player_ids = pd.read_sql('SELECT id FROM Player', conn).values #pd.DataFrame(players.get_players()).astype({'id': 'str'})['id'].values\n\n      dfs = []\n      for player_id in player_ids:\n        df = dask.delayed(get_common_player_info(player_id))\n        dfs.append(df)\n\n      dfs = dask.persist(*dfs)\n      dfs = dask.compute(dfs)\n      dfs = [df for df in dfs if df != None]\n      dfs = dask.dataframe.multi.concat(dfs)\n      return dfs\n\nif __name__ == \"__main__\":\n  common_player_info_dfs = main()\n\ncommon_player_info_dfs.head()","e35fbebe":"cluster.shutdown()","9349356a":"### Create Dask Cluster with the Number of Workers Equal to the Number of CPU Cores","5827c58c":"## Get Common Player Information","b0985be3":"## Process `get_proxies()` with Dask then Shutdown Cluster","de8ca6e1":"### Define Functions `get_quick_proxies()` & `get_common_player_info()`\n\nEach function utilizes a ***Dask*** cluster. \n\n`get_quick_proxies()` gets a list of proxies (tested to be alive) more quickly than the function above. This function is used in the case that all proxies found from the above function fail to return responses from stats.nba.com. \n\n`get_common_player_info()` returns dataframe of common player infomation for a certain player. The paradigm here is to distribute jobs (where each job is collecting common player info for a certain player) across a ***Dask*** cluster since all outputs will be the same and can be easily be concatenated. ","c80f8cac":"## Prepare Development Environment\n\n- ### Clone GitHub Repository (if necessary)\n- ### Install Conda package manager (if necessary)\n- ### Install dependencies","b69afde9":"## Collect Data","0f76c3f8":"## Get Proxy Server Addresses","8ae632e3":"### Define Function to Scrape New Proxy List and Return Proxies Tested to be Alive","63099db7":"## View System Information","447bc19a":"## Import Dependencies & Initialize Kaggle","f4a9349e":"### Extract Common Player Information for all Players"}}