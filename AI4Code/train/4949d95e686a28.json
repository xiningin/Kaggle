{"cell_type":{"f7648665":"code","dd58da68":"code","d5fc9f78":"code","74ae1148":"code","3ba03951":"code","87e64b62":"code","6f4bea06":"code","27b8300e":"code","296cb6cc":"code","0225890e":"code","acaf4826":"code","edf5ed64":"code","e02c5d95":"code","d75937b0":"code","d08f74af":"code","ee61c21d":"code","6ce5dc4f":"code","90ec40e8":"code","3509c275":"code","272ee8ee":"code","c0c914f1":"code","c6d27037":"code","b3915134":"code","5c7249d4":"code","09fbae4f":"code","db6c76ef":"code","e84ab2a2":"code","cdca44ed":"code","78bc3bb8":"code","ed834dd4":"code","abde1956":"code","70ea453c":"code","54c52d3e":"code","93f0554c":"code","eef78c43":"code","394832be":"code","44754afe":"code","a0040966":"code","08c9007d":"markdown","d53eef84":"markdown","59571a5d":"markdown","fca6e18d":"markdown","574331ca":"markdown","94809ada":"markdown","e35940b7":"markdown","44f56291":"markdown","8f1fab95":"markdown","0062d60f":"markdown","00bfd3c7":"markdown","c6af9843":"markdown","95b0c907":"markdown","633ca0e4":"markdown","4853962d":"markdown","5d44304c":"markdown","343f07a3":"markdown","773e6e55":"markdown","8569de2f":"markdown","cd77d1e5":"markdown","23f20561":"markdown","ec3f3b6a":"markdown","dca0f1c2":"markdown","a92729a7":"markdown","d0fa95b3":"markdown","d1065558":"markdown","5d52c61b":"markdown","a6c67582":"markdown","2f5f0440":"markdown","3573a799":"markdown"},"source":{"f7648665":"!pip install -q transformers ekphrasis keras-tuner","dd58da68":"import numpy as np\nimport pandas as pd\nimport urllib\nimport statistics\nimport math\nimport pprint\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import (\n    Input,\n    Dense,\n    Embedding,\n    Flatten,\n    Dropout,\n    GlobalMaxPooling1D,\n    GRU,\n    concatenate,\n)\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom transformers import (\n    DistilBertTokenizerFast,\n    TFDistilBertModel,\n    DistilBertConfig,\n)\n\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import Tokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\nfrom ekphrasis.dicts.noslang.slangdict import slangdict\n\nimport kerastuner","d5fc9f78":"def print_metrics(model, x_train, y_train, x_val, y_val):\n    train_acc = dict(model.evaluate(x_train, y_train, verbose=0, return_dict=True))[\n        \"accuracy\"\n    ]\n    val_acc = dict(model.evaluate(x_val, y_val, verbose=0, return_dict=True))[\n        \"accuracy\"\n    ]\n\n    val_preds = model.predict(x_val)\n    val_preds_bool = val_preds >= 0.5\n\n    print(\"\")\n    print(f\"Training Accuracy:   {train_acc:.2%}\")\n    print(f\"Validation Accuracy: {val_acc:.2%}\")\n    print(\"\")\n    print(f\"Validation f1 score: {sklearn.metrics.f1_score(val_preds_bool, y_val):.2%}\")","74ae1148":"# Using DistilBERT:\nmodel_class, tokenizer_class, pretrained_weights = (TFDistilBertModel, DistilBertTokenizerFast, 'distilbert-base-uncased')\n\npretrained_bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n\ndef get_pretrained_bert_model(config=pretrained_weights):\n    if not config:\n        config = DistilBertConfig(num_labels=2)\n\n    return model_class.from_pretrained(pretrained_weights, config=config)\n\n","3ba03951":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","87e64b62":"print(train_df.info())\n\nprint(\"\")\nprint(\"train rows:\", len(train_df.index))\nprint(\"test rows:\", len(test_df.index))","6f4bea06":"print(\"label counts:\")\ntrain_df.target.value_counts()","27b8300e":"print(\"train precentage of nulls:\")\nprint(round(train_df.isnull().sum() \/ train_df.count() * 100, 2))","296cb6cc":"print(\"test precentage of nulls:\")\nprint(round(test_df.isnull().sum() \/ test_df.count() * 100, 2))","0225890e":"# check that we don't have any keywords appearing in one set and not the other\ntrain_keywords = set(train_df[\"keyword\"].dropna())\ntest_keywords = set(test_df[\"keyword\"].dropna())\n\nall_keywords = train_keywords.union(test_keywords)\nunique_test_keywords = all_keywords - train_keywords\nunique_train_keywords = all_keywords - test_keywords\n\nprint(f\"unique_test_keywords: {unique_test_keywords}\")\nprint(f\"unique_train_keywords: {unique_train_keywords}\")","acaf4826":"# We'll use these weights later on to make up for the slightly imbalanced dataset\nclasses = np.unique(train_df[\"target\"])\nclass_weights = sklearn.utils.class_weight.compute_class_weight(\n    \"balanced\", classes=classes, y=train_df[\"target\"]\n)\n\nclass_weights = {clazz : weight for clazz, weight in zip(classes, class_weights)}","edf5ed64":"# Commented out the graceful handling of duplicated because the Kaggle kernel version of statistics.mode()\n# won't handle multimodal results\n\n# Duplicates aren't consistently labeled, so we keep one example of the most frequently occuring label\n# train_df[\"duplicated\"] = train_df.duplicated(subset=\"text\")\n# duplicated_tweets = train_df.loc[lambda df: df[\"duplicated\"] == True, :]\n# aggregated_duplicates = duplicated_tweets.groupby(\"text\", as_index=False).aggregate(\n#     statistics.mode\n# )\n\n# train_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\n# train_df = train_df.append(aggregated_duplicates, ignore_index=True)\n\ntrain_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\nprint(\"train rows:\", len(train_df.index))\nprint(\"test rows:\", len(test_df.index))","e02c5d95":"class TweetPreProcessor:\n    \"\"\"\n    This class does some cleaning and normalization prior to BPE tokenization\n    \"\"\"\n\n    def __init__(self):\n\n        self.text_processor = TextPreProcessor(\n            # terms that will be normalized\n            normalize=[\n                \"url\",\n                \"email\",\n                \"phone\",\n                \"user\",\n                \"time\",\n                \"date\",\n            ],\n            # terms that will be annotated\n            annotate={\"repeated\", \"elongated\"},\n            # corpus from which the word statistics are going to be used\n            # for word segmentation\n            segmenter=\"twitter\",\n            # corpus from which the word statistics are going to be used\n            # for spell correction\n            spell_correction=True,\n            corrector=\"twitter\",\n            unpack_hashtags=False,  # perform word segmentation on hashtags\n            unpack_contractions=False,  # Unpack contractions (can't -> can not)\n            spell_correct_elong=True,  # spell correction for elongated words\n            fix_bad_unicode=True,\n            tokenizer=Tokenizer(lowercase=True).tokenize,\n            # list of dictionaries, for replacing tokens extracted from the text,\n            # with other expressions. You can pass more than one dictionaries.\n            dicts=[emoticons, slangdict],\n        )\n\n    def preprocess_tweet(self, tweet):\n        return \" \".join(self.text_processor.pre_process_doc(tweet))\n    \n    # this will return the tokenized text     \n    def __call__(self, tweet):\n        return self.text_processor.pre_process_doc(tweet)\n    \ntweet_preprocessor = TweetPreProcessor()","d75937b0":"# Have a look at how the TweetProcessor is doing\nfor tweet in train_df[100:120][\"text\"]:\n    print(\"original:  \", tweet)\n    print(\"processed: \", tweet_preprocessor.preprocess_tweet(tweet))\n    print(\"\")","d08f74af":"train_df[\"text\"] = train_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)\ntest_df[\"text\"] = test_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)","ee61c21d":"# Fill NA\ntrain_df[\"keyword\"].fillna(\"\", inplace=True)\ntest_df[\"keyword\"].fillna(\"\", inplace=True)\n\n# remove %20 from keywords\ntrain_df[\"keyword\"] = train_df[\"keyword\"].apply(urllib.parse.unquote)\ntest_df[\"keyword\"] = test_df[\"keyword\"].apply(urllib.parse.unquote)","6ce5dc4f":"x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n    train_df[[\"text\", \"keyword\"]], train_df[\"target\"], test_size=0.3, random_state=42, stratify=train_df[\"target\"]\n)","90ec40e8":"def tokenize_encode(tweets, max_length=None):\n    return pretrained_bert_tokenizer(\n        tweets,\n        add_special_tokens=True,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        return_tensors=\"tf\",\n    )\n\n\n# need to be explicit about the lengths (instead of just specifying padding=True in the tokenizer)\n# otherwise train tweets end up being 71 and validation tweets end up as 70, which causes problems\/warnings\nmax_length_tweet = 72\nmax_length_keyword = 8\n\ntrain_tweets_encoded = tokenize_encode(x_train[\"text\"].to_list(), max_length_tweet) \nvalidation_tweets_encoded = tokenize_encode(x_val[\"text\"].to_list(), max_length_tweet) \n\ntrain_keywords_encoded = tokenize_encode(x_train[\"keyword\"].to_list(), max_length_keyword) \nvalidation_keywords_encoded = tokenize_encode(x_val[\"keyword\"].to_list(), max_length_keyword) \n\ntrain_inputs_encoded = dict(train_tweets_encoded)\ntrain_inputs_encoded[\"keywords\"] = train_keywords_encoded[\"input_ids\"]\n\nvalidation_inputs_encoded = dict(validation_tweets_encoded)\nvalidation_inputs_encoded[\"keywords\"] = validation_keywords_encoded[\"input_ids\"]\n","3509c275":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_tweets_encoded), y_train)\n)\n\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(validation_tweets_encoded), y_val)\n)\n\ntrain_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n    (train_inputs_encoded, y_train)\n)\n\nval_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n    (validation_inputs_encoded, y_val)\n)\n","272ee8ee":"tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n    tokenizer=tweet_preprocessor, min_df=1, ngram_range=(1, 1), norm=\"l2\"\n)\n\ntrain_vectors = tfidf_vectorizer.fit_transform(raw_documents=x_train[\"text\"]).toarray()\nvalidation_vectors = tfidf_vectorizer.transform(x_val[\"text\"]).toarray()","c0c914f1":"# I obtained the value of C by experimenting with LogisticRegressionCV but I'm leaving it out for brevity\nlogisticRegressionClf = LogisticRegression(n_jobs=-1, C=2.78)\nlogisticRegressionClf.fit(train_vectors, y_train)\n\ndef print_metrics_sk(clf, x_train, y_train, x_val, y_val):\n    print(f\"Train Accuracy:         {clf.score(x_train, y_train):.2%}\")\n    print(f\"Validation Accuracy:    {clf.score(x_val, y_val):.2%}\")\n    print(\"\")\n    print(f\"f1 score:               {sklearn.metrics.f1_score(y_val, clf.predict(x_val)):.2%}\")\n\nprint_metrics_sk(logisticRegressionClf, train_vectors, y_train, validation_vectors, y_val)","c6d27037":"feature_extractor = get_pretrained_bert_model()\n\n# Run a forward pass on the tokenized inputs\n# model_outputs = feature_extractor(\n#     train_tweets_encoded[\"input_ids\"], train_tweets_encoded[\"attention_mask\"]\n# )\nmodel_outputs = feature_extractor.predict(\n    train_dataset.batch(32)\n)\n# BERT's sentence representation can be retrieved from a hidden vector at index 0 in the sequence, \n# (where the special token CLS was prepended by the tokenizer)\ntrain_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\n\n# The rest of the sequence contains the embeddings \n# (modified by successive layers of self-attention) for each token\ntrain_word_vectors = model_outputs.last_hidden_state[:, 1:, :]\n\n# And the same again for the validation set\n# model_outputs = feature_extractor(\n#     validation_tweets_encoded[\"input_ids\"], validation_tweets_encoded[\"attention_mask\"]\n# )\nmodel_outputs = feature_extractor.predict(\n    val_dataset.batch(32)\n)\nvalidation_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\nvalidation_word_vectors = model_outputs.last_hidden_state[:, 1:, :]","b3915134":"logisticRegressionClf = LogisticRegression(n_jobs=-1, class_weight=class_weights)\nlogisticRegressionClf.fit(train_sentence_vectors, y_train)\n\nprint_metrics_sk(\n    logisticRegressionClf,\n    train_sentence_vectors,\n    y_train,\n    validation_sentence_vectors,\n    y_val,\n)","5c7249d4":"def create_gru_model() -> keras.Model:\n\n    model = keras.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=train_word_vectors.shape[1:]))\n    model.add(GRU(32, return_sequences=True))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\nmodel = create_gru_model()\n\nhistory = model.fit(\n    train_word_vectors,\n    y_train,\n    validation_data=(validation_word_vectors, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(model, train_word_vectors, y_train, validation_word_vectors, y_val)","09fbae4f":"def create_multi_input_model() -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    tweet_classification_vectors = keras.Input((train_sentence_vectors.shape[1],), name=\"tweets\")\n    tweet_features = Dense(1, activation='relu')(tweet_classification_vectors)    \n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_classification_vectors], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\n\nmodel = create_multi_input_model()\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_sentence_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_sentence_vectors}\n\nhistory = model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\n\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)","db6c76ef":"def create_multi_input_rnn_model() -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\n    tweet_features = GRU(32, return_sequences=True)(tweet_token_embeddings)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    tweet_features = Dense(1, activation='relu')(tweet_features)    \n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\n\nmodel = create_multi_input_rnn_model()\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\n\nhistory = model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)","e84ab2a2":"def create_candidate_model_with_fx(hp: kerastuner.HyperParameters) -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(hp.Choice(\"keyword_units\", values=[1, 8, 16, 32], default=1))(keyword_features)\n\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\n    \n    tweet_features = GRU(hp.Choice(\"GRU_units\", values=[8, 16, 32, 64, 128], default=32), return_sequences=True)(tweet_token_embeddings)\n    tweet_features = Dropout(hp.Float(\"GRU_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    \n    for i in range(hp.Int(\"num_layers\", min_value=0, max_value=3, step=1)):\n        tweet_features = Dense(hp.Choice(\"layer_\" + str(i) + \"_units\", values=[2, 8, 16, 32, 64, 128, 256]), activation=\"relu\")(tweet_features)\n        tweet_features = Dropout(hp.Float(\"layer_\" + str(i) + \"_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\n    \n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\n","cdca44ed":"# Hyperband Tuning\nMAX_EPOCHS = 10\nFACTOR = 3\nITERATIONS = 3\n\nprint(f\"Number of models in each bracket: {math.ceil(1 + math.log(MAX_EPOCHS, FACTOR))}\")\nprint(f\"Number of epochs over all trials: {round(ITERATIONS * (MAX_EPOCHS * (math.log(MAX_EPOCHS, FACTOR) ** 2)))}\")","78bc3bb8":"tuner = kerastuner.Hyperband(\n    create_candidate_model_with_fx,\n    max_epochs=MAX_EPOCHS,\n    hyperband_iterations=ITERATIONS, \n    factor=FACTOR, \n    objective=\"val_accuracy\",\n    directory=\"hyperparam-search\",\n    project_name=\"architecture-hyperband\",\n)\n\ntuner.search(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=10,\n    verbose=1,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=3,\n            restore_best_weights=True,\n        )\n    ],\n)\n","ed834dd4":"# tuner.results_summary()","abde1956":"best_model = tuner.get_best_models()[0]\n# best_model.summary()\nprint(\"\")\nbest_arch_hp = tuner.get_best_hyperparameters()[0]\npprint.pprint(best_arch_hp.values, indent=4)\nprint(\"\")\n\nprint_metrics(best_model, train_inputs, y_train, validation_inputs, y_val)","70ea453c":"# To create a baseline for the simplest possible fine-tuned BERT\ndef create_bert_simple_for_ft():\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n\n    pretrained_bert_model = get_pretrained_bert_model()\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n\n    prediction = Dense(1, activation=\"sigmoid\")(bert_outputs.last_hidden_state[:, 0, :])\n    return keras.Model(inputs=[input_ids, attention_mask], outputs=prediction)\n\nmodel = create_bert_simple_for_ft()\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(\n    train_dataset.batch(32),\n    validation_data=val_dataset.batch(32),\n    class_weight=class_weights,\n    epochs=20,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(\n    model, dict(train_tweets_encoded), y_train, dict(validation_tweets_encoded), y_val\n)\n","54c52d3e":"def create_bert_rnn_for_ft():\n    \n    pretrained_bert_model = get_pretrained_bert_model()\n    \n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\n    tweet_features = GRU(32, return_sequences=True)(bert_token_embeddings)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\nmodel = create_bert_rnn_for_ft()\n\nmodel.fit(\n    train_multi_input_dataset.batch(32),\n    validation_data=val_multi_input_dataset.batch(32),\n    epochs=20,\n    class_weight=class_weights,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=3,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\n)","93f0554c":"def create_model_candidate() -> keras.Model:\n    pretrained_bert_model = get_pretrained_bert_model()\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(best_arch_hp.get(\"keyword_units\"))(keyword_features)\n\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\n    tweet_features = GRU(best_arch_hp.get(\"GRU_units\"), return_sequences=True)(bert_token_embeddings)\n    tweet_features = Dropout(best_arch_hp.get(\"GRU_dropout\"))(tweet_features)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    \n    for i in range(best_arch_hp.get(\"num_layers\")):\n        tweet_features = Dense(best_arch_hp.get(\"layer_\" + str(i) + \"_units\"), activation=\"relu\")(tweet_features)\n        tweet_features = Dropout(best_arch_hp.get(\"layer_\" + str(i) + \"_dropout\"))(tweet_features)\n    \n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n","eef78c43":"model = create_model_candidate()\n\nhistory = model.fit(\n    train_multi_input_dataset.batch(32),\n    validation_data=val_multi_input_dataset.batch(32),\n    epochs=6,\n    class_weight=class_weights,\n    callbacks=[\n        keras.callbacks.EarlyStopping(\n            monitor=\"val_accuracy\", restore_best_weights=True\n        )\n    ],\n)\n\nbest_epoch = len(history.history[\"val_accuracy\"]) - 1\n\nprint_metrics(\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\n)","394832be":"test_tweets_encoded = tokenize_encode(test_df[\"text\"].to_list(), max_length_tweet)\ntest_inputs_encoded = dict(test_tweets_encoded)\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)\n\ntest_keywords_encoded = tokenize_encode(test_df[\"keyword\"].to_list(), max_length_keyword)\ntest_inputs_encoded[\"keywords\"] = test_keywords_encoded[\"input_ids\"]\ntest_multi_input_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)","44754afe":"full_train_dataset = train_multi_input_dataset.concatenate(val_multi_input_dataset)\nmodel = create_model_candidate()\n\nmodel.fit(\n    full_train_dataset.batch(32),\n    epochs=best_epoch,\n    class_weight=class_weights,\n)","a0040966":"preds = np.squeeze(model.predict(test_multi_input_dataset.batch(32)))\npreds = (preds >= 0.5).astype(int)\npd.DataFrame({\"id\": test_df.id, \"target\": preds}).to_csv(\"submission.csv\", index=False)","08c9007d":"## Train-Test Split","d53eef84":"## Tokenize and Encode Test Set","59571a5d":"N.B. Typically one might freeze the base model, train the added classifier for a bit, unfreeze the base model, lower the learning rate and train the whole model again. \n\nHowever Huggingface recommend that training an unfrozen model right from the beginning (with a low learning rate) works better with transformers. \n\nI tried both and there seemed to be no advantage to freeze-unfreeze. Sometimes it even reported an inferior score. However, it's hard to be certain given the large random fluctations between training runs with such a small dataset. I didn't test this with kfold validation which may have yielded more conclusive results. \n\nI have read in some papers that gradual unfreezing of the blocks in the base model can lead to better results.","fca6e18d":"## Multi-Input Classifier with Sentence Vectors & Keywords","574331ca":"# Instantiate Pretrained Bert Model & Tokenizer","94809ada":"## Tokenisation and Encode ","e35940b7":"# Create Submission","44f56291":"## RNN with Attention Embeddings & Keywords","8f1fab95":"## Logistic Regression with BERT Sentence Vectors","0062d60f":"## Create TF-IDF Vectors","00bfd3c7":"## Fine-Tune BERT with Simple Head on Sentence Vector","c6af9843":"## Fine-Tune BERT with RNN on Attention Embeddings and Keywords","95b0c907":"# Architecture Search For Best Classification Head","633ca0e4":"# Imports & Preamble","4853962d":"## Run Classifier","5d44304c":"## Clean Keywords","343f07a3":"# Feature Extraction with BERT","773e6e55":"## Drop Duplicates","8569de2f":"# Load and Examine Data ","cd77d1e5":"# Preprocessing","23f20561":"## Extract Sentence Vectors and Attention Embeddings","ec3f3b6a":"## Helper Function for Assessing Keras Models","dca0f1c2":"# BERT Fine Tuning","a92729a7":"## Train Model","d0fa95b3":"## Save Predictions","d1065558":"# Baseline with Logistic Regression on a TF-IDF Bag of Words","5d52c61b":"## Create TF Dataset","a6c67582":"## RNN with BERT Attention Embeddings ","2f5f0440":"## Fine-Tune BERT with With Best Classification Head","3573a799":"## Clean Tweets"}}