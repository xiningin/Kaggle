{"cell_type":{"5844aa20":"code","cf02e367":"code","64688984":"code","666c6b25":"code","a1feb541":"code","debcc505":"markdown","3c30e643":"markdown","094bc8bc":"markdown","d5c0ba52":"markdown","bc23f4c0":"markdown","bf5ac83c":"markdown","d5b40819":"markdown"},"source":{"5844aa20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf02e367":"data_ts = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')\ndata_ts.shape","64688984":"ch_size = 60_000\nfor i in range(10, 19):\n    data_ts['target'] = 0\n    data_ts.loc[i*ch_size:(i+1)*ch_size, 'target'] = 1\n    data_ts[['target']].to_csv(f'prob_{i-10}.csv', index_label='id')","666c6b25":"import pylab as pl\n\nfpr = [0, 0.4, 1]\ntpr = [0, 0.6, 1]\n\npl.plot(fpr, tpr, 'bo-')\npl.plot([0,1], [0,1], 'k:')\npl.plot([0,fpr[1]], [tpr[1]]*2, 'r:')\npl.plot([fpr[1]]*2, [0,tpr[1]], 'r:')\npl.xlabel('False Positive Rate')\npl.ylabel('True Positive Rate')\npl.title('ROC Curve')\npl.text(fpr[0]+0.05, tpr[0], 'threshold>1', ha='left')\npl.text(fpr[1], tpr[1]+0.1, 'threshold=1', va='bottom', ha='center')\npl.text(fpr[2]-0.05, tpr[2], 'threshold=0', ha='right');\npl.xticks(fpr, ['0',r'$FPR_1$','1'])\npl.yticks(tpr, ['0',r'$TPR_1$','1']);","a1feb541":"AUC_chunks = [0.50276, 0.52120, 0.48360, 0.48896, 0.51060, 0.51020, 0.47949, 0.49088, 0.51229]\n\nratio_finder = lambda AUC: (9*AUC - 3.5) \/ 2\n\n[ratio_finder(x) for x in AUC_chunks]","debcc505":"# Motivation\n\nas several people has discussd it already, data seems to be in chunks and ratio of positive labels in each chunk is different from one another. @AmbrosM has discussed a way to probe testset positive ratios per chunk in his notebook. He uses a good performing submission and replaces the output for diffirent chunks with pre-defined values, then by submitting the new outputs we can get some information about the actual ratio of positive samples in the public part of the test set.\n\nIn this notebook I'm going to show a mathematical way to estimate the ratio. I will be using a different approach when submitting probing submissions and then use the AUC score to approximate positive ratios given some assumptions.\n\nlong story short, here are the ratios (probabilities) for the 9 chunks in the test set:\n\n**(0.51242,\n 0.59540,\n 0.42619,\n 0.45032,\n 0.54770,\n 0.54590,\n 0.40770,\n 0.45896,\n 0.55530)**\n\nI also don't think these can help so much for making a better prediction, given the fact that 25% of targets are flipped and we have already reached 0.75 AUC, but let's have some fun with the mathematics of AUC.\n\nRefer to these notebook and discussions for more info regarding the chunks:\n\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsnov21-012-leaderboard-probing\/notebook\n\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/discussion\/286731","3c30e643":"# Mathematics of AUC\n\nBecause our submission has only 0,1 the ROC curve would look like something like this:","094bc8bc":"It should be noted again that the estimates were made based on two main assumptions:\n\n* The ratios are the same in public and private test-sets\n* The positive ratio over all samples in test set is 0.5\n","d5c0ba52":"Just to remind you how the ROC curve is being calculated. For the given input vecotrs (y_true, y_pred), we know y_true is binary, and without losing generality we can assume y_pred values are in the [0,1] range. Now we would pick different threshoulds as decision threshoulds and assign 0,1 labels to the y_pred. So if we have 100 prediction values in range [0,1] when we chose the threshould as 0.2 we will end up in a vetor like [0,0,0, ..., 1,1,1] and using this new vector we can calculate True Positive Rate and False Positive Rate compared to original target (y_true).\n\nFalse Posiive Rate is the number of false positives devided by total number of actual negative samples, and True Positive Rate is the number of true postives devided by the number of actual positives.\n\n$FPR = \\frac{FP}{N} = \\frac{FP}{FP+TN}$\n\n$TPR = \\frac{TP}{P} = \\frac{TP}{TP+FN}$\n\nBy changing the threshould we would get different values for FPR, TPR and ploting them would give us the ROC (receiver operating characteristic) curve. The area under this curve is typically called AUC (area under curve). It is woth mentioning typically the threshoulds are the unique values in y_pred vector, because for values between those the TPR, FPR would not change. ","bc23f4c0":"# Probing\n\nfor the leaderboard probing part I'm using submissions with 0s,1s. I make 9 submissions in the i-th submissions I put all of the outputs for the i-th chunk to 1 and all the rest to 0s. This way will will get an idea about how much of the actual 1s are in the i-th chunk.","bf5ac83c":"## AUC for the probing submission files\n\nIn our example since our output has only 0s and 1s in y_pred the threshoulds would be [2, 1, 0]. The first and the last threshould would result in 0 and 1 respectively for both FPR and TPR. The main point of interest is when threshould = 1. \n\nLet's call FPR and TPR at thr=1 $FPR_1, TPR_1$ respectively. Given these values we can calculate the AUC as a sum of areas of a triangle and a trapeziod:\n\n$AUC = \\frac{FPR_1 \\times TPR_1}{2} + \\frac{TPR_1 + 1}{2} \\times (1-FPR_1)$\n\nsimplifying it:\n\n$AUC = \\frac{TPR_1 - FPR_1 + 1}{2}$\n\n\nfor the i-th submission, we have 1 on the i-th chunk and 0s on other chunks. Assuming the i-th chunk has $r_i$ ratio of positive samples and ratio $r$ for the whole test set, we can calculate the $FPR_1, TPR_1$ for the i-th chunk:\n\n$TPR_1(i) = \\dfrac{r_i \\times ChunkSize}{r \\times TestSetSize} = \\dfrac{r_i}{9 \\times r}$\n\nsimilarly for $FPR_1$:\n\n$FPR_1(i) = \\dfrac{1-r_i}{9 \\times (1-r)}$\n\n**key assumption:** assuming the whole test set would have the ration of 0.5 of 0s and 1s, i.e. $r=0.5$, we can simplify the above formulas and derive AUC to $r_i$ formula:\n\n$TPR_1(i) = \\dfrac{r_i}{4.5}, FPR_1(i) = \\dfrac{1-r_i}{4.5}$\n\n$AUC(i) = \\dfrac{1}{2}(\\dfrac{r_i}{4.5} - \\dfrac{1-r_i}{4.5} + 1)$\n\nOr:\n\n$AUC(i) = \\dfrac{2r_i+3.5}{9}$\n\nIn other words, the positive ratio in i-th chunk is:\n\n$r_i = \\dfrac{9 AUC(i) - 3.5}{2}$\n\n\nGiven this formula we can estimate the positive ratios for different chunks given the AUC of the submission files:","d5b40819":"## Probing Results\n\nHere are the AUCs for the 9 submissions:\n\n\\[0.50276, 0.52120, 0.48360, 0.48896, 0.51060, 0.51020, 0.47949, 0.49088, 0.51229\\]\n\nWe already can see that there should be more positive samples in the second chunk compared to the third chunk. It should be noted these are from the public leaderboard, and the following discussion is made given the assumption that public\/private leaderboard split is random, i.e. the same ratio will be found in private leaderboard."}}