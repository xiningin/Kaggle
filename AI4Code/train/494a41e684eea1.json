{"cell_type":{"a5a2cf17":"code","086a8994":"code","7aa5146c":"code","f99c2099":"code","f78acfaa":"code","b9d5504a":"code","d00ddd99":"code","eafe4af1":"code","3a09484b":"code","c34a8dc6":"code","3d09a67e":"code","25055a10":"code","614347b2":"code","64f9146e":"code","6ddbc86e":"code","ad563649":"code","339186a6":"code","edad2bec":"code","4570cd93":"code","afdc5406":"code","a789f568":"code","ec7c6e69":"code","980f6ffa":"code","c184b6e2":"code","d3eb8317":"code","df2bf298":"code","4d1a2343":"code","fb365242":"code","a6d7861a":"code","37d029f8":"code","3eaa9c58":"code","88647517":"code","e46c47da":"code","03b7025e":"code","89432a50":"code","3446b605":"code","d7b401a8":"code","2efa4fd2":"code","f767ed41":"code","acd14b6f":"code","e63e93b9":"code","6b146067":"code","f6c96266":"code","36f5f380":"code","04f5acf8":"code","1262edc1":"code","c48e661c":"code","0237b24f":"code","a1394187":"code","71d9ca7f":"code","b88e6879":"code","bc66c659":"code","c4e27461":"code","0e378f6e":"code","ce301d28":"code","83671b38":"code","84ec54ae":"code","5b1cb192":"code","c17e595f":"code","8568182b":"code","226ff454":"code","7feb1270":"code","dd1e7186":"code","e0efdeed":"code","cb1af332":"code","d0a3cfb3":"code","574e4347":"code","b65dd1b5":"code","a44c8f22":"code","72c3f52b":"code","9c8c54f9":"code","f5588b8b":"code","6baecf59":"code","d8b88bbb":"code","89936eb8":"code","33ecfae8":"code","1b4abca0":"code","c062d273":"code","930d0385":"markdown","e6ae011a":"markdown","2153761d":"markdown","1b08743c":"markdown","b978d35b":"markdown","ac2cca2c":"markdown","0204d006":"markdown","75ee6ae6":"markdown","e438e23c":"markdown","8814816d":"markdown","03c07aee":"markdown","355e4a32":"markdown","c0bcedda":"markdown","1dff75d2":"markdown","a03edf0c":"markdown"},"source":{"a5a2cf17":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","086a8994":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7aa5146c":"df = pd.read_csv('\/kaggle\/input\/autoprice\/Automobile price data _Raw_.csv')\ndf.head()","f99c2099":"##Replacing the ? value to Nan before filling it\ndf.replace(to_replace =\"?\", value =np.nan,inplace=True)\ndf.head()","f78acfaa":"##Getting dummy values for columns with classifications and including them in training as well\nmake = pd.get_dummies(df['make'],drop_first=True)\nfueltype = pd.get_dummies(df['fuel-type'],drop_first=True)\naspiration = pd.get_dummies(df['aspiration'],drop_first=True)\nnumofdoors = pd.get_dummies(df['num-of-doors'],drop_first=True)\nbodystyle = pd.get_dummies(df['body-style'],drop_first=True)\ndrivewheels = pd.get_dummies(df['drive-wheels'],drop_first=True)\nenginelocation = pd.get_dummies(df['engine-location'],drop_first=True)\nfuelsystem = pd.get_dummies(df['fuel-system'],drop_first=True)\nenginetype = pd.get_dummies(df['engine-type'],drop_first=True)\ncylno = pd.get_dummies(df['num-of-cylinders'],drop_first=True)\n\ndf.drop(['make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location','fuel-system','engine-type','num-of-cylinders'],axis=1,inplace=True) \n\ndf = pd.concat([df,make,fueltype,aspiration,numofdoors,bodystyle,drivewheels,enginelocation,fuelsystem,enginetype,cylno],axis=1)\n","b9d5504a":"df.head()","d00ddd99":"#Chaning all the object type columns to numeric columns\ndf['normalized-losses'] = df['normalized-losses'].apply(pd.to_numeric)\ndf['bore'] = df['bore'].apply(pd.to_numeric)\ndf['stroke'] = df['stroke'].apply(pd.to_numeric)\ndf['horsepower'] = df['horsepower'].apply(pd.to_numeric)\ndf['peak-rpm'] = df['peak-rpm'].apply(pd.to_numeric)\ndf['price'] = df['price'].apply(pd.to_numeric)","eafe4af1":"#Here we can see we have successfully converted all object columns to integer columns\ndf.info()","3a09484b":"#Replacing the Nan values with mean of the respective columns to get better results\ndf['normalized-losses'] = df['normalized-losses'].replace(to_replace =np.nan,value =df['normalized-losses'].mean()) \ndf['bore'] = df['bore'].replace(to_replace =np.nan,value =df['bore'].mean()) \ndf['stroke'] = df['stroke'].replace(to_replace =np.nan,value =df['stroke'].mean()) \ndf['peak-rpm'] = df['peak-rpm'].replace(to_replace =np.nan,value =df['peak-rpm'].mean()) \ndf['horsepower'] = df['horsepower'].replace(to_replace =np.nan,value =df['horsepower'].mean()) \ndf['price'] = df['price'].replace(to_replace =np.nan,value =df['price'].mean()) ","c34a8dc6":"df.head()","3d09a67e":"#Checking if null values still exist\ndf['ohc'].isnull()","25055a10":"df= df.replace([np.inf, -np.inf], np.nan)","614347b2":"df","64f9146e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('price',axis=1), \n                                                    df['price'], test_size=0.355, \n                                                    random_state=101)","6ddbc86e":"from sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\n\nlm.fit(X_train,y_train)","ad563649":"print('Coefficients: \\n', lm.coef_)","339186a6":"pred1 = lm.predict( X_test)","edad2bec":"\nplt.scatter(y_test,pred1)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.title('Linear Regression')","4570cd93":"from sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, pred1))\nprint('MSE:', metrics.mean_squared_error(y_test, pred1))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred1)))","afdc5406":"sns.distplot((y_test-pred1),bins=50);","a789f568":"from sklearn.metrics import r2_score \naccuracy = r2_score(y_test,pred1)\nprint(np.round(accuracy,decimals=4))","ec7c6e69":"from sklearn import tree","980f6ffa":"clf = tree.DecisionTreeRegressor()\nclf.fit(X_train, y_train)","c184b6e2":"pred2 = clf.predict(X_test)","d3eb8317":"plt.scatter(y_test,pred2)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.title('Decision Tree Regression')","df2bf298":"sns.distplot((y_test-pred2),bins=50);","4d1a2343":"accuracy = r2_score(y_test,pred2)\nprint(np.round(accuracy,decimals=4))","fb365242":"from sklearn.svm import SVR\n\nmodel = SVR()","a6d7861a":"model.fit(X_train,y_train)","37d029f8":"pred3 = model.predict(X_test)","3eaa9c58":"plt.scatter(y_test,pred3)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.title('Support Vector Regression')","88647517":"sns.distplot((y_test-pred3),bins=50);","e46c47da":"accuracy = r2_score(y_test,pred3)\nprint(np.round(accuracy,decimals=4))","03b7025e":"from sklearn.ensemble import RandomForestRegressor","89432a50":"rfr = RandomForestRegressor()","3446b605":"rfr.fit(X_train,y_train)","d7b401a8":"pred_rf = rfr.predict(X_test)","2efa4fd2":"plt.scatter(y_test,pred_rf)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.title('Random Forest Regressor')","f767ed41":"sns.distplot((y_test-pred_rf),bins=50);","acd14b6f":"accuracy = r2_score(y_test,pred_rf)\nprint(np.round(accuracy,decimals=4))","e63e93b9":"from sklearn import linear_model","6b146067":"reg = linear_model.BayesianRidge()\nreg.fit(X_train, y_train)\n","f6c96266":"predic = reg.predict(X_test)","36f5f380":"plt.scatter(y_test,predic)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.title('Bayesian Regression')","04f5acf8":"sns.distplot((y_test-predic),bins=50);","1262edc1":"accuracy = r2_score(y_test,predic)\nprint(np.round(accuracy,decimals=4))","c48e661c":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nn_samples, n_features = 10, 5","0237b24f":"reg1 = make_pipeline(StandardScaler(),\n                     SGDRegressor(max_iter=5000, tol=1e-3))","a1394187":"reg1.fit(X_train, y_train)","71d9ca7f":"pr = reg1.predict(X_test)","b88e6879":"plt.scatter(y_test,pr)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.title('Gradient Descent')","bc66c659":"sns.distplot((y_test-pr),bins=50);","c4e27461":"accuracy = r2_score(y_test,pr)\nprint(np.round(accuracy,decimals=4))","0e378f6e":"#Importing the old dataset again to plot the values perfectly without showing dummy columns to avoid confusion","ce301d28":"df1 = pd.read_csv('\/kaggle\/input\/autoprice\/Automobile price data _Raw_.csv')","83671b38":"df1.head()","84ec54ae":"##Cleaning the dataset by replacing all ? values by using mean and conveerting them to numeric columns","5b1cb192":"df1.replace(to_replace =\"?\", \n                 value =np.nan,inplace=True)","c17e595f":"df1.head()","8568182b":"#Chaning all the object type columns to numeric columns\ndf1['normalized-losses'] = df1['normalized-losses'].apply(pd.to_numeric)\ndf1['bore'] = df1['bore'].apply(pd.to_numeric)\ndf1['stroke'] = df1['stroke'].apply(pd.to_numeric)\ndf1['horsepower'] = df1['horsepower'].apply(pd.to_numeric)\ndf1['peak-rpm'] = df1['peak-rpm'].apply(pd.to_numeric)\ndf1['price'] = df1['price'].apply(pd.to_numeric)","226ff454":"#Replacing the Nan values with mean of the respective columns to get better results\ndf1['normalized-losses'] = df1['normalized-losses'].replace(to_replace =np.nan, \n                 value =df1['normalized-losses'].mean()) \ndf1['bore'] = df1['bore'].replace(to_replace =np.nan, \n                 value =df1['bore'].mean()) \ndf1['stroke'] = df1['stroke'].replace(to_replace =np.nan, \n                 value =df1['stroke'].mean()) \ndf1['peak-rpm'] = df1['peak-rpm'].replace(to_replace =np.nan, \n                 value =df1['peak-rpm'].mean()) \ndf1['horsepower'] = df1['horsepower'].replace(to_replace =np.nan, \n                 value =df1['horsepower'].mean()) \ndf1['price'] = df1['price'].replace(to_replace =np.nan, \n                 value =df1['price'].mean()) ","7feb1270":"df1.info()","dd1e7186":"df1.head()","e0efdeed":"#Let us first see the relation between a few numeric columns to price\nsns.jointplot(x='normalized-losses',y='price',data=df1,kind='scatter')","cb1af332":"sns.jointplot(x='engine-size',y='price',data=df1,kind='scatter',color='b',edgecolor='r')","d0a3cfb3":"sns.jointplot(x='horsepower',y='price',data=df1,kind='scatter',color='g',edgecolor='b')","574e4347":"sns.jointplot(x='stroke',y='price',data=df1,kind='scatter')","b65dd1b5":"sns.jointplot(x='compression-ratio',y='price',data=df1,kind='scatter')","a44c8f22":"sns.jointplot(x='peak-rpm',y='price',data=df1,kind='scatter')","72c3f52b":"sns.jointplot(x='highway-mpg',y='city-mpg',data=df1,kind='scatter',color='r')","9c8c54f9":"sns.jointplot(x='highway-mpg',y='price',data=df1,kind='scatter',color='r',edgecolor='b')","f5588b8b":"sns.countplot(x='num-of-doors',data=df1)","6baecf59":"sns.countplot(x='body-style',data=df1)","d8b88bbb":"sns.barplot(x='body-style',y='price',data=df1)","89936eb8":"sns.barplot(x='num-of-doors',y='price',data=df1)","33ecfae8":"sns.barplot(x='fuel-type',y='price',data=df1)","1b4abca0":"sns.boxplot(x=\"symboling\", y=\"price\",hue='num-of-doors',data=df1)","c062d273":"plt.figure(figsize = (16,5))\n\nsns.heatmap(df1.corr(),annot=True)\n","930d0385":"# Bayesian Regression Model","e6ae011a":"# Support Vector Regression ","2153761d":"# Decision Tree Regression","1b08743c":"### We can observe Random Forest Regressor has R2 Score of 0.9026 ","b978d35b":"# Stochastic Gradient Descent","ac2cca2c":"### We can observe that the R2 Score of Bayesian Model is 0.7965 ","0204d006":"### We can observe that the R2 Score of Decision Tree Regressor is 0.8757% ","75ee6ae6":"### Here, we can observe Support Vector Regression has accuracy of 0.0062 ","e438e23c":"# Linear Regression","8814816d":"# The end","03c07aee":"# Cleaning the data and preparing it for training and testing","355e4a32":"### We can observe that Stockholm Gradient Descent has an R2 Score of 0.8878% ","c0bcedda":"### So, we can see that the R2 Score of Linear Regression is 0.8389","1dff75d2":"# Random Forest Regressor","a03edf0c":"# Exploratory Data Analysis"}}