{"cell_type":{"589f7a35":"code","453351b0":"code","12d9277c":"code","d02c1952":"code","aba3e2e5":"code","1a3b67a8":"code","43185d58":"code","baf58756":"code","d1ef02c7":"code","f85eacfb":"code","58e036ed":"code","1f9cb236":"code","21ae3007":"code","bb639601":"code","bb4a7808":"code","2d5c68ea":"code","93aafbb2":"code","7cfdd021":"code","102c2f42":"code","c835096d":"code","ab252b43":"code","ccf3cc78":"code","e4aa10d4":"code","3a95a560":"code","6597a0f3":"code","4cde2646":"code","ca85b9fd":"code","e2a34a60":"code","e649f47a":"code","c6679d97":"code","6f628cfa":"code","6c9c1d04":"code","a3a14ce5":"code","9646495d":"code","139669df":"code","069386cd":"code","695bca4c":"code","cb111f64":"code","670678a3":"code","103ea909":"code","eb523d5d":"code","50e824e6":"code","5fa3a231":"code","9ce98c89":"code","e42bcf08":"code","389f86aa":"code","36601dca":"code","a0578bb2":"markdown","bc060f50":"markdown","986a1538":"markdown","0bc10383":"markdown","19af8699":"markdown","f3b64eb6":"markdown","01f3048c":"markdown","222d352b":"markdown","9a570ae2":"markdown","bd85eee6":"markdown","20d28628":"markdown","36254606":"markdown","c95ed09e":"markdown","24d2d671":"markdown"},"source":{"589f7a35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","453351b0":"import numpy as np\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Embedding","12d9277c":"reviews = ['nice food',\n        'amazing restaurant',\n        'too good',\n        'just loved it!',\n        'will go again',\n        'horrible food',\n        'never go there',\n        'poor service',\n        'poor quality',\n        'needs improvement']\n\nsentiment = np.array([1,1,1,1,1,0,0,0,0,0])","d02c1952":"?one_hot","aba3e2e5":"one_hot(\"amazing restaurant\",30)","1a3b67a8":"vocab_size = 30\nencoded_reviews = [one_hot(d, vocab_size) for d in reviews]\nprint(encoded_reviews)","43185d58":"max_length = 4\npadded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\nprint(padded_reviews)","baf58756":"embeded_vector_size = 5\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","d1ef02c7":"X = padded_reviews\ny = sentiment","f85eacfb":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nprint(model.summary())","58e036ed":"model.fit(X, y, epochs=50, verbose=0)","1f9cb236":"# evaluate the model\nloss, accuracy = model.evaluate(X, y)\naccuracy","21ae3007":"weights = model.get_layer('embedding').get_weights()[0]\nlen(weights)","bb639601":"weights[13]","bb4a7808":"# !pip install gensim\n# !pip install python-Levenshtein\nimport gensim","2d5c68ea":"df = pd.read_json(\"http:\/\/snap.stanford.edu\/data\/amazon\/productGraph\/categoryFiles\/reviews_Cell_Phones_and_Accessories_5.json.gz\", lines=True)\ndf","93aafbb2":"df.shape","7cfdd021":"review_text = df.reviewText.apply(gensim.utils.simple_preprocess)","102c2f42":"review_text","c835096d":"review_text.loc[0]","ab252b43":"df.reviewText.loc[0]","ccf3cc78":"?gensim.models.Word2Vec","e4aa10d4":"#Build the model\nmodel = gensim.models.Word2Vec(#documents,       #Word list\n                               min_count=2,    #Ignore all words with total frequency lower than this                           \n                               workers=4,       #Number of CPU Cores\n                               vector_size=50,  #Embedding size -  Dimensionality of the feature vectors. - (50, 300)\n                               window=10,        #Maximum Distance between current and predicted word\n                               epochs =10       #Number of iterations over the text corpus\n                              )  \n\n\n# model = gensim.models.Word2Vec(\n#     window=10,\n#     min_count=2,\n#     workers=4,\n# )","3a95a560":"model.build_vocab(review_text, progress_per=1000)","6597a0f3":"model.raw_vocab.items()","4cde2646":"model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)","ca85b9fd":"model.wv['bad']","e2a34a60":"model.save(\".\/word2vec-amazon-cell-accessories-reviews-short.model\")","e649f47a":"model.wv.most_similar(\"bad\")","c6679d97":"model.wv.similarity(w1=\"cheap\", w2=\"inexpensive\")","6f628cfa":"model.wv.similarity(w1=\"great\", w2=\"good\")","6c9c1d04":"import gensim\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors","a3a14ce5":"#wv = api.load('word2vec-google-news-300')","9646495d":"filename = '\/kaggle\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)","139669df":"model.word_vec('king')","069386cd":"# queen = (king - man) + woman\n\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\nprint(result)","695bca4c":"result = model.most_similar(positive=['Paris', 'Italy'], negative=['France'], topn=3)\nprint(result)","cb111f64":"result = model.most_similar(positive=['Delhi', 'France'], negative=['India'], topn=3)\nprint(result)","670678a3":"result = model.most_similar(positive=['samsung', 'phone'], negative=['apple'], topn=3)\nprint(result)","103ea909":"from gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = '\/kaggle\/input\/glove6b\/glove.6B.100d.txt'\nword2vec_output_file = '\/kaggle\/working\/glove.6B.100d.txt.word2vec'\nglove2word2vec(glove_input_file, word2vec_output_file)","eb523d5d":"for dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))","50e824e6":"# load the Stanford GloVe model\nfilename = '\/kaggle\/working\/glove.6B.100d.txt.word2vec'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=False)","5fa3a231":"# calculate: (king - man) + woman = ?\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\nprint(result)","9ce98c89":"result = model.most_similar(positive=['apple', 'milk'], negative=['orange'], topn=3)\nprint(result)","e42bcf08":"result = model.most_similar(positive=['paris', 'italy'], negative=['france'], topn=3)\nprint(result)","389f86aa":"result = model.most_similar(positive=['paris', 'india'], negative=['france'], topn=3)\nprint(result)","36601dca":"result = model.most_similar(positive=['milk', 'cheese'], negative=['curd'], topn=3)\nprint(result)","a0578bb2":"#### Build Vocabulary","bc060f50":"### Finding Similar Words and Similarity between words\nhttps:\/\/radimrehurek.com\/gensim\/models\/word2vec.html","986a1538":"# 1. NLP - Word Embedding Demo[Using Binary Classification Model]\n","0bc10383":"# 2. Create Custom Word Embeddings Using Gensim","19af8699":"### Training the Word2Vec Model\n\nTrain the model for reviews. Use a window of size 10 i.e. 10 words before the present word and 10 words ahead. A sentence with at least 2 words should only be considered, configure this using min_count parameter.\n\nWorkers define how many CPU threads to be used.","f3b64eb6":"#### Train the Word2Vec Model","01f3048c":"# 3. Google Pre-Trained Word2Vec Model \n\nlet\u2019s download a __pre-trained__ model and play around with it. \n\nWe will fetch the Word2Vec model trained on part of the Google News dataset, covering approximately 3 million words and phrases. \n\nSuch a model can take hours to train, but since it\u2019s already available, downloading and loading it with Gensim takes minutes.\n\n#### The model is approximately 2GB, so you\u2019ll need a decent network connection to proceed. (abt 5 mins)\n\n3 million words * 300 features * 4bytes\/feature = ~3.35GB\n\u2026that\u2019s a big matrix!","222d352b":"### Reading and Exploring the Dataset\nThe dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas.\n\nLink to the Dataset: http:\/\/snap.stanford.edu\/data\/amazon\/productGraph\/categoryFiles\/reviews_Cell_Phones_and_Accessories_5.json.gz","9a570ae2":"#### Initialize the model","bd85eee6":"### Further Reading\n\nYou can read about gensim more at https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html\n\nExplore other Datasets related to Amazon Reviews: http:\/\/jmcauley.ucsd.edu\/data\/amazon\/\n\nHow to Develop Word Embeddings in Python with Gensim - https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/","20d28628":"# 4. Stanford\u2019s GloVe Embedding","36254606":"### Save the Model\n\nSave the model so that it can be reused in other applications","c95ed09e":"### Simple Preprocessing & Tokenization\nThe first thing to do for any data science task is to clean the data.\nFor NLP, we apply various processing like converting all the words to lower case, trimming spaces, removing punctuations. \nThis is something we will do over here too.\n\nAdditionally, we can also remove stop words like 'and', 'or', 'is', 'the', 'a', 'an' and convert words to their root forms like 'running' to 'run'.","24d2d671":"## Exercise\n\nTrain a word2vec model on the [Sports & Outdoors Reviews Dataset](http:\/\/snap.stanford.edu\/data\/amazon\/productGraph\/categoryFiles\/reviews_Sports_and_Outdoors_5.json.gz)\nOnce you train a model on this, find the words most similar to 'awful' and find similarities between the following word tuples: ('good', 'great'), ('slow','steady')"}}