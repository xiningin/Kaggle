{"cell_type":{"bfda97ee":"code","251086a9":"code","27eeb9f3":"code","4c1345c5":"code","24f2baa2":"code","a398f204":"code","1fff4d6b":"code","d22efe27":"code","9a41c4a6":"code","8c799182":"code","08d30f00":"code","42306f86":"code","4228b3a4":"code","6c09960d":"code","ab69c198":"markdown","2538ddcd":"markdown","924a6bde":"markdown"},"source":{"bfda97ee":"!pip install torchsummary\n!pip install timm\n!pip install loguru\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torch.nn.functional as F\nfrom torchsummary import summary\nimport torchvision\nimport torchvision.datasets as dataset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nimport gc\nimport copy\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom loguru import logger","251086a9":"class CONFIG:\n    seed = 42\n    model_name = 'tf_efficientnetv2_m_in21k' \n    train_batch_size = 8\n    valid_batch_size = 32\n    img_size = 150\n    epochs = 5\n    learning_rate = 1e-4\n    min_lr = 1e-6\n    weight_decay = 1e-6\n    T_max = 5\n    scheduler = 'CosineAnnealingLR'\n    n_accumulate = 1\n    n_fold = 5\n    target_size = 270\n    device = torch.device( \"cpu\")","27eeb9f3":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","4c1345c5":"transform = transforms.Compose([transforms.Resize(CONFIG.img_size),\n                                transforms.ToTensor()])\n\ntrain_dataset = dataset.ImageFolder(root='..\/input\/100-bird-species\/train',transform=transform)\n\ntest_dataset = dataset.ImageFolder(root='..\/input\/100-bird-species\/test',\n                                            transform=transform)","24f2baa2":"total_class = train_dataset.classes\ntrain_class = train_dataset.class_to_idx\ntest_class = test_dataset.class_to_idx\n\nidx_to_classes = {}\nfor classes in train_class:\n    idx_to_classes[train_class[classes]] = classes","a398f204":"train_loader = DataLoader(train_dataset,\n                        batch_size=CONFIG.train_batch_size,\n                        shuffle=True)\n\nvalid_loader = DataLoader(test_dataset,\n                        batch_size= CONFIG.valid_batch_size,\n                        shuffle=False)","1fff4d6b":"class BirdModel(nn.Module):\n    def __init__(self, model_name, pretrained=True):\n        super(BirdModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=3)\n        for param in self.model.parameters():\n            param.requires_grad = False\n        self.n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(self.n_features, CONFIG.target_size)\n        \n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n    \nmodel = BirdModel(CONFIG.model_name,pretrained=False)\nmodel.to(CONFIG.device)\nmodel.load_state_dict(torch.load(\"..\/input\/trained-effnetv2\/epoch3.pb\"))","d22efe27":"params_to_update = model.parameters()\nfeature_extract = True\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)","9a41c4a6":"def criterion(outputs, targets):\n    return nn.CrossEntropyLoss()(outputs, targets.long())","8c799182":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, labels) in bar:         \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        batch_size = images.size(0)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss = loss \/ CONFIG.n_accumulate\n            \n        loss.backward()\n        \n        if (step + 1) % CONFIG.n_accumulate == 0:\n            optimizer.step()\n\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","08d30f00":"@torch.no_grad()\ndef valid_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, labels) in bar:        \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        PREDS.append(outputs.sigmoid().cpu().detach().numpy())\n        TARGETS.append(labels.cpu().detach().numpy())\n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return epoch_loss, ","42306f86":"def fetch_scheduler(optimizer):\n    if CONFIG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG.T_max, eta_min=CONFIG.min_lr)\n    elif CONFIG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG.T_0, T_mult=1, eta_min=CONFIG.min_lr)\n    elif CONFIG.scheduler == None:\n        return None\n        \n    return scheduler\noptimizer = optim.Adam(params_to_update, lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\nscheduler = fetch_scheduler(optimizer)\n","4228b3a4":"def run(model, optimizer, scheduler, device, num_epochs):    \n    start = time.time()\n    \n    best_model_wts = model.state_dict()\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG.device, epoch=epoch)\n        \n        valid_epoch_loss = valid_one_epoch(model, optimizer, scheduler,\n                                                            dataloader=valid_loader, \n                                                            device=CONFIG.device, epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n\n        best_model_wts = copy.deepcopy(model.state_dict())\n        PATH = \"epoch{:.0f}.pb\".format(epoch)\n        torch.save(model.state_dict(), PATH)\n        print(\"Model Saved\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","6c09960d":"# Uncomment to train the model . I have trained for 3 epochs and loaded those weights \n# model, history = run(model, optimizer, scheduler=scheduler, device=CONFIG.device, num_epochs=3)","ab69c198":"****Referred from notebooks of [Abhay](https:\/\/www.kaggle.com\/abhaymishra3420\/xception-from-scratch) and [Debarshi](https:\/\/www.kaggle.com\/debarshichanda\/efficientnetv2-state-of-the-art)****","2538ddcd":"# **Finetuning efficientnetv2 **","924a6bde":"![Efficientnetv2](https:\/\/serge-m.github.io\/media\/2020-07-01-which-backbone-to-choose\/efficientnet.png)"}}