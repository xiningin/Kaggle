{"cell_type":{"05f565db":"code","b781ef71":"code","46f3932e":"code","68b25723":"code","b22810b0":"code","c32bf4ee":"code","29373a7b":"code","1da6b013":"code","8d15fc3e":"code","e69b3617":"code","85077ca5":"code","668769a0":"code","2ef740ce":"code","3c42106c":"code","6d9b0fd6":"code","8bfcaffd":"code","ce877a67":"code","a2bb1279":"code","437bd590":"code","3d773355":"code","6ef7b132":"code","561e9474":"code","c233464f":"code","df634760":"markdown","45240d64":"markdown","cce7faef":"markdown","4c564043":"markdown","3995a346":"markdown","eab26119":"markdown","3b90169b":"markdown","5807acf3":"markdown","26d142d3":"markdown"},"source":{"05f565db":"# Libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport cv2\nfrom tqdm import tqdm\nfrom colorama import Fore, Back, Style\nr_ = Fore.WHITE\nfrom plotly.offline import iplot\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom skimage.io import imshow, imread, imsave\nfrom skimage.transform import rotate, AffineTransform, warp,rescale, resize, downscale_local_mean\nfrom skimage import color,data\nfrom skimage.exposure import adjust_gamma\nfrom skimage.util import random_noise\n","b781ef71":"# Train \ntrain_labels=pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ntrain_labels.head()","46f3932e":"cmap_plot = plt.get_cmap('jet_r')\nddt = train_labels.target.value_counts().to_frame()\nplt.style.use('fivethirtyeight')\nfig, ax = plt.subplots(1, 1, figsize = (12, 4))\nsns.countplot(data = train_labels, x = 'target', orient = \"v\", palette = 'pastel', ax = ax)\nplt.suptitle(\"Train target distribution\")\nplt.rcParams.update(plt.rcParamsDefault)","68b25723":"train_files = glob.glob(\"..\/input\/seti-breakthrough-listen\/train\" + \"\/*\/*.npy\")\nprint(\"\\t\\t\\t\\t{}{}Number of train files: {}\".format(r_, Back.BLACK, len(train_files)))","b22810b0":"def get_train_filename_by_id(_id: str) -> str:\n    return f\"..\/input\/seti-breakthrough-listen\/train\/{_id[0]}\/{_id}.npy\"\n\ndef show_cadence(filename: str, label: int) -> None:\n    fig, axes = plt.subplots(6, 1, figsize = (16, 10))\n    ax = axes.ravel()\n    arr = np.load(filename)\n    for i in range(6):\n        \n        ax[i].imshow(arr[i].astype(float), interpolation='nearest', aspect='auto')\n        ax[i].text(5, 100, [\"ON\", \"OFF\"][i % 2], bbox={'facecolor': 'white'})\n        if i != 5:\n            ax[i].set_xticks([])\n            \n    fig.text(0.5, -0.02, 'Frequency Range', ha='center', fontsize=18)\n    fig.text(-0.02, 0.5, 'Seconds', va='center', rotation='vertical', fontsize=18)\n\n    plt.suptitle(f\"ID: {os.path.basename(filename)} TARGET: {label}\", fontsize=18)\n    fig.tight_layout()\n    plt.show()","c32bf4ee":"positive_target=train_labels.query(\"target==1\").sample().id.item()\nnegative_target=train_labels.query(\"target==0\").sample().id.item()\nshow_cadence(get_train_filename_by_id(positive_target), 1)\nshow_cadence(get_train_filename_by_id(negative_target), 0)","29373a7b":"#Test\ntest_files = glob.glob('..\/input\/seti-breakthrough-listen\/test' + \"\/*\/*.npy\")\nprint(\"\\t\\t\\t\\t{}{}Number of test files: {}\".format(r_, Back.BLACK, len(test_files)))","1da6b013":"show_cadence(np.random.choice(test_files, 1).item(), None)\nshow_cadence(np.random.choice(test_files, 1).item(), None)","8d15fc3e":"#Libraries\nimport os\nimport sys\nsys.path=['..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master',]+sys.path\nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom efficientnet_pytorch import model as enet\nimport random\nfrom sklearn.model_selection import StratifiedKFold\n","e69b3617":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nrandom_state = set_seed(2021)\n    ","85077ca5":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","668769a0":"class ClassificationDataset:\n    \n    def __init__(self, image_paths, targets): \n        self.image_paths = image_paths\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):      \n        image = np.load(self.image_paths[item]).astype(float)\n\n        targets = self.targets[item]\n                \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","2ef740ce":"df_train=pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ndf_train.head()","3c42106c":"df_train['img_path']=df_train['id'].apply(lambda x:f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')\ndf_train.head()","6d9b0fd6":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n        self.conv1 = nn.Conv2d(6, 3, kernel_size=3, stride=1, padding=3, bias=False)\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.extract(x)\n        x = self.myfc(x)\n        return x","8bfcaffd":"def train(data_loader, model, optimizer, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        inputs = data[\"image\"]\n        targets = data['targets']\n        \n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","ce877a67":"baseline_name = 'efficientnet-b1'\npretrained_model = {\n    baseline_name: '..\/input\/efficientnet-pytorch\/efficientnet-b1-dbc7070a.pth'\n}\nmodels = []\ndevice = \"cuda\"\nepochs = 3\nBatch_Size = 32\nX = df_train.img_path.values\nY = df_train.target.values\nskf = StratifiedKFold(n_splits=5)\nfold = 0\n\nfor train_index, test_index in skf.split(X, Y):\n    \n    model = enetv2(baseline_name, out_dim=1)\n    model.to(device)\n\n    train_images, valid_images = X[train_index], X[test_index]\n    train_targets, valid_targets = Y[train_index], Y[test_index]\n\n    train_dataset = ClassificationDataset(image_paths=train_images, targets=train_targets)\n    valid_dataset = ClassificationDataset(image_paths=valid_images, targets=valid_targets)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=Batch_Size,shuffle=True, num_workers=4)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=Batch_Size,shuffle=False, num_workers=4)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\n    for epoch in range(epochs):\n        train(train_loader, model, optimizer, device=device)\n        predictions, valid_targets = evaluate(valid_loader, model, device=device)\n        roc_auc = metrics.roc_auc_score(valid_targets, predictions)\n        print(f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\")\n        \n    torch.save(model.state_dict(),baseline_name + '-' + str(fold) + '.pt')\n    models.append(model)\n    fold += 1","a2bb1279":"submission=pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsubmission['img_path']=submission['id'].apply(lambda x:f'..\/input\/seti-breakthrough-listen\/test\/{x[0]}\/{x}.npy')","437bd590":"test_dataset=ClassificationDataset(image_paths=submission.img_path.values, targets=submission.target.values)\ntest_loader=torch.utils.data.DataLoader(test_dataset, batch_size=16,shuffle=False,num_workers=4)","3d773355":"sig=torch.nn.Sigmoid()\nouts=[]\nfor model in models:\n    predictions,valid_targets=evaluate(test_loader, model, device=device)\n    predictions=np.array(predictions)[:,0]\n    out=sig(torch.from_numpy(predictions))\n    out=out.detach().numpy()\n    outs.append(out)\n    ","6ef7b132":"pred=np.mean(np.array(outs),axis=0)","561e9474":"submission.target=pred\nsubmission.drop(['img_path'],axis=1,inplace=True)\nsubmission.to_csv('submission.csv', index=False)\n","c233464f":"submission.head()","df634760":"<h2><center>1. Competition Understanding<\/center><\/h2>","45240d64":"![LA3LNj6cu7JFLwe7BKmXnD.jpg](attachment:b142995c-d3bc-475f-83ea-034ff7b00567.jpg)","cce7faef":"#### Introduction\n<ul>\n  <li>In this competition, we have to use our data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call \u201cneedles\u201d) in the haystack of data from the telescope.They have identified some of the hidden needles so that you can train your model to find more.<\/li>\n <li>The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more.<\/li>\n <li>The algorithm that\u2019s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.<\/li>\n   ","4c564043":"<h1><center>SETI Breakthrough Listen - E.T. Signal Search<\/center><\/h1>\n<h2><center>Find extraterrestrial signals in data from deep space<\/center><\/h2>","3995a346":"##### Let's have a look at the data now...\n<h2><center>2. Exploratory Data Analysis<\/center><\/h2>\n","eab26119":"##### Lets kickstart the modelling..\n<h2><center>3. Model: EffiecientNet<\/center><\/h2>","3b90169b":"#### Goal\nThe main objective of the competition is to <b> accurately classify anomalous signals <\/b> in scans of Breakthrough Listen targets using machine learning-based models.","5807acf3":"##### We can infer from the graph above that the data is imbalanced. We will keep this in mind while creating the model for this data.","26d142d3":"#### Now let's visualize the Train and Test Dataset"}}