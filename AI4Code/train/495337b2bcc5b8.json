{"cell_type":{"5a4fd541":"code","a1ecdcf7":"code","c9c18b85":"code","077b5724":"code","ef2994e2":"code","a84d25eb":"code","f4caddd5":"code","075a7f04":"code","aa1f5b91":"code","d597b73f":"code","6872857a":"code","7bc02a10":"code","aa37c5a5":"code","1c8ae676":"code","d6a80c29":"code","4faa7982":"code","fca828b6":"code","2d150bd4":"code","56ea151c":"code","97189a8b":"code","0c2593d1":"code","41c74334":"code","9c01ed0b":"code","c9995070":"code","ebdf43b8":"code","647acfb4":"code","b522c84e":"code","058ff2d7":"code","96e098fa":"code","470d493c":"code","c2140968":"code","c7467eb3":"code","e83a8655":"code","5624c3dd":"code","0731dcf2":"code","b92d5aa2":"markdown","c26f0fb2":"markdown"},"source":{"5a4fd541":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Input\nfrom keras.models import Sequential, save_model\nfrom keras.utils import np_utils\nimport tensorflow as tf\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.metrics import log_loss\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","a1ecdcf7":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntrain = train.drop(columns=['id'])","c9c18b85":"#dic_targ = {\"target\": {\"Class_1\": 0, \"Class_2\": 1,\"Class_3\": 2,\"Class_4\": 3}}\n#train = train.replace(dic_targ)\n# histograms of the variables\nplt.figure(figsize=(20,20))\ntrain.hist()\nplt.show()","077b5724":"test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ntest_id=test.id\ntest = test.drop(columns=['id'])","ef2994e2":"train","a84d25eb":"cat_columns = ['target']\ntarg_dummy=pd.get_dummies(train, prefix_sep=\"_\",\n                              columns=cat_columns)\ntarg_dummy","f4caddd5":"#np.ascontiguousarray()\nX=train.iloc[:,0:50]\ny=targ_dummy.iloc[:,50:55]\ny","075a7f04":"###Check Class Balance\nTotal = y.sum()\nprint (Total)","aa1f5b91":"X.values","d597b73f":"###Check for Blank Values\ny[\"sum\"] = y.sum(axis=1)\nprint(y[y[\"sum\"]==0])\ndel y[\"sum\"]","6872857a":"from sklearn.model_selection import train_test_split\nnp.random.seed(15)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    stratify=y, \n                                                    test_size=0.3)\n\n##### Get Indexes for Training Set\nindexs_list2=y_train.index.values.tolist() \ny_class_df=train['target'].iloc[indexs_list2]","7bc02a10":"###Check Class Balance\nTotal2 = y_train.sum()\nprint (Total2)","aa37c5a5":"\n# define pipeline\n#over = SMOTE(sampling_strategy=0.1)\n####5943,40248,14994, 8815\nsampling_strategy_under = {0: 5943, 1: 25000, 2: 14994, 3:8815}\n#sampling_strategy_over = {0: 12500, 1: 25000, 2: 20000, 3:16000}\n\nunder = RandomUnderSampler(sampling_strategy=sampling_strategy_under)\n#over = SMOTEENN(sampling_strategy=sampling_strategy_over)\n\nX_train, y_train=under.fit_resample(X_train.values, y_train.values)\n#X_train, y_train=over.fit_resample(X_train, y_train)\n\n#X_train=pd.DataFrame(X_train)\n#y_train=pd.DataFrame(y_train)","1c8ae676":"###Check Class Balance\nTotal2 = y_train.sum()\nprint (Total2)","d6a80c29":"num_classes=4\nnum_classes","4faa7982":"num_columns=X_train.shape[1]\nnum_columns","fca828b6":"def create_model(num_columns):\n    model = Sequential()\n    model.add(Input(num_columns))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(units=50, kernel_initializer='glorot_uniform', activation='tanh'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Dense(units=25,activation='tanh'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2) )\n    model.add(Dense(units=num_classes,activation='softmax'))\n    opt = tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[tf.keras.metrics.CategoricalAccuracy()])\n    return model\n    \n#Best drop out .05 , .1, .2, 38 epochs\n#metrics=[tf.keras.metrics.AUC(name='auc')]\n#tf.keras.metrics.AUC(name='auc')\n#tf.keras.metrics.Recall(name='recall')\n#tf.keras.metrics.Precision(name='precision')","2d150bd4":"#Class Penalty Weights\nclass_weights = {0: 1.3,\n                1: 1.,\n                2: 1.,\n                3: 1.2}\n#class_weight=class_weights,","56ea151c":"#reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n#model = create_model(num_columns)\n#history = model.fit(X_train,\n#                  y_train,\n#                  validation_data=(X_test, y_test),\n#                  epochs=60, batch_size=128,\n#                  callbacks=[reduce_lr_loss], verbose=2\n#                 )","97189a8b":"# Plot training & validation accuracy values\n#plt.plot(history.history['categorical_accuracy'])\n#plt.plot(history.history['val_categorical_accuracy'])\n#plt.title('Model accuracy')\n#plt.ylabel('Categorical Accuracy')\n#plt.xlabel('Epoch')\n#plt.legend(['Train', 'Test'], loc='upper left')\n#plt.show()","0c2593d1":"# Plot training & validation accuracy values\n#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.title('Model accuracy')\n#plt.ylabel('Categorical Crossentropy')\n#plt.xlabel('Epoch')\n#plt.legend(['Train', 'Test'], loc='upper left')\n#plt.show()","41c74334":"####Get Length of Test\nl=len(test)-1\nl","9c01ed0b":"##Empty Predictions Set\nss = y.copy()\nss = ss.reset_index()\ndel ss['index']\nss=ss.loc[0:l,:]\nss.loc[:, y.columns] = 0\nss","c9995070":"##Empty Validation Set\nres = y.copy()\nres = res.reset_index ()\nres.loc[:, y.columns] = 0\ndel res['index']\nres","ebdf43b8":"#Class Penalty Weights\n#from sklearn.utils import class_weight\n\n#class_weights = {0: 3.,\n#                1: 1.,\n#                2: 1.3,\n#                3: 2.}\n#class_weight=class_weights,\n#under = RandomUnderSampler(sampling_strategy='not minority')","647acfb4":"tf.random.set_seed(42)\nfor n, (tr, te) in enumerate(StratifiedKFold(n_splits=5, shuffle=True).split(train['target'], train['target'])):\n    print(train['target'][tr].value_counts().sort_index()[0]\/(max(train['target'][tr].value_counts())))\n    print(train['target'][tr].value_counts().sort_index())","b522c84e":"\nN_STARTS = 7\ntf.random.set_seed(42)\n\n####This iterates through starts:\n\nfor seed in range(N_STARTS):\n#####This iteraties through folds n, validation indexes te, and train indexes tr:  \n    for n, (tr, te) in enumerate(StratifiedKFold(n_splits=10, random_state=seed, shuffle=True).split(train['target'], train['target'])):\n        print(f'Fold {n}')\n    \n        model = create_model(num_columns)\n        #checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        #cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n          #                           save_weights_only = True, mode = 'min')\n        \n####This fits the model to each fold and validation set. .values avoids creating a np array:\n######Try Adjustments to Class Balance\n        #sampling_strategy_under = {0: 5943, 1: 25000, 2: 14994, 3:8815}\n        #under = RandomUnderSampler(sampling_strategy=sampling_strategy_under)\n        #X_train, y_train=under.fit_resample(X.values[tr], y.values[tr])\n        model.fit(X.values[tr],\n                  y.values[tr],\n                  validation_data=(X.values[te], y.values[te]),\n                  epochs=35, batch_size=64,\n                  callbacks=[reduce_lr_loss], verbose=2\n                 )\n        \n        #model.load_weights(checkpoint_path)\n####Makes predictions for each fold & seed:\n        test_predict = model.predict(test.values[:, :])\n        val_predict = model.predict(X.values[te])\n####Sum Predictions for Each Epoch     \n        ss.loc[:, y.columns] += test_predict\n        res.loc[te, y.columns] += val_predict\n        print('')\n        \n####After all summed, Divide summed predictions by the number of starts times the number of folds:     \nss.loc[:, y.columns] \/= ((n+1) * N_STARTS)\nres.loc[:, y.columns] \/= N_STARTS","058ff2d7":"####Estimate Validation Loss of Averaged Results\ndef metric(y_true, y_pred):\n    metrics = []\n    for _target in y.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","96e098fa":"print(f'OOF Metric: {metric(y, res)}')","470d493c":"ss","c2140968":"test_id=pd.DataFrame(test_id)\nss\ntest_id","c7467eb3":"ss=pd.merge(test_id, ss, how='inner', left_index=True, right_index=True)\nss=pd.DataFrame(ss)\nss","e83a8655":"ss.columns = ['id','Class_1', 'Class_2', 'Class_3', 'Class_4']","5624c3dd":"ss","0731dcf2":"ss.to_csv('submission.csv', index=False)","b92d5aa2":"# Keras Multiclass Neural Network with Tanh Activation & Blended Shuffled Stratified Runs\n\n**This model performs relatively well for log loss. The downside is it tends to pick one class. Feel free to copy and play around with it. \n\nSpecial thanks to https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2","c26f0fb2":"# Types of Over and Undersampling Using imblearn\n\n1) **Random Under Sampling:** We can use the function RandomUnderSampler(sampling_strategy= 'not minority') or 'majority' to adjust the relationship between the minority class and majority class. We can use dict to input a dictionary with keys corresponding to classes and values corresponding to desired # of samples for each class. Finally, the default is to sample without replacement but that can be altered by utilizing replacement=True. Note: For binary classification, you can input a float with the desired minority \/ majority ratio. \n\n\n**Here we reach a 2 to 1 balance:** \ndefine undersample strategy\n\nundersample = RandomUnderSampler(sampling_strategy='not minority')\n\nsampling_strategy = {0: 10, 1: 15, 2: 20}\n\nrus = RandomUnderSampler(sampling_strategy=sampling_strategy)\n\nfit and apply the transform\n\nX_under, y_under = undersample.fit_resample(X, y)\n\n2) **Random Over Sampling:** \n\nros = RandomOverSampler(random_state=0)\n\nros.fit(X, y)\n\nX_resampled, y_resampled = ros.sample(X, y)\n\n3) **Over Sampling Using SMOTE:** This method takes two minority class points that are close to each other and creates a synthetic point repeatedly until the desired balance is acheived. \n\n**Here we reach a 2 to 1 balance:** \noversample = SMOTE(sampling_strategy='not minority') \nX_over, y_over = oversample.fit_resample(X, y)\n\n\n4) **Nearest Neighbor Under Sampling and SMOTE Over Sampling (SMOTEENN)**: This strategy combines the above approaches to acheive the desired ratio. \n\n**Here we reach a 2 to 1 balance:** \ndefine sampling strategy\nsample = SMOTEENN(sampling_strategy='not minority')\nfit and apply the transform\nX_over, y_over = sample.fit_resample(X, y)\n\n4)Bagging: Bootstrap resampling with replacement from under represented classes. \n"}}