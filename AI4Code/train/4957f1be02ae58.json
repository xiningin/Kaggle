{"cell_type":{"5f5f3d26":"code","b191ae0d":"code","6136fe02":"code","9748268c":"code","38468124":"code","a7509fa8":"code","696aece3":"code","3214f3f4":"code","a879355f":"code","033fa697":"code","de55ce48":"code","a87aeaf3":"code","9ae7588b":"code","fd34fd9b":"code","bf73c7c4":"code","277225de":"code","3e6b25c2":"code","8b2164a1":"code","679cee57":"markdown","7a735ec2":"markdown","a71e5d6b":"markdown","ec7fcae6":"markdown","5202b24e":"markdown","76bfeae7":"markdown","0fd38904":"markdown","0588a4da":"markdown","fcc6fb93":"markdown","de4b73b6":"markdown","468dfc64":"markdown","a13eb6d5":"markdown","19327d74":"markdown","4647f096":"markdown","ddde2612":"markdown","70fbadec":"markdown","a11b2244":"markdown","861fe100":"markdown","1dbe520e":"markdown","e3fd0d2e":"markdown","323c2459":"markdown","45e2808d":"markdown","c50cfb1c":"markdown","94a3d85a":"markdown","7ec43b75":"markdown","2c1a0288":"markdown","d88fd759":"markdown","b31b1b98":"markdown","97232e10":"markdown","1de4c0cb":"markdown","3ba696c5":"markdown","a8105b53":"markdown","b3fcf152":"markdown","760a331e":"markdown","ccfacb79":"markdown","ea9f91fc":"markdown","7115843a":"markdown","13162cf5":"markdown","163d8707":"markdown"},"source":{"5f5f3d26":"import numpy as np\n\nSTREAM_FILE_NAME = \"..\/input\/stream-data-dgim\/stream_data_dgim.txt\"\n\n\nclass StreamSimulator:\n    def __init__(self):\n        self.stream_data = np.loadtxt(fname=STREAM_FILE_NAME, dtype=int)\n        self.stream_len = self.stream_data.shape[0]\n        print(\"Loaded %d binary bits as the data stream\" % self.stream_len)\n\n        self.current_idx = 0\n\n    def next(self) -> (int, int):\n        \"\"\"\n        :return:            1. index of the current bit in the stream (start from 0)\n                            2. the current bit\n        \"\"\"\n        assert self.current_idx <= self.stream_len - 1, \"[INFO] Stream Ends\"\n\n        _bit = self.stream_data[self.current_idx]\n        _bit_idx = self.current_idx\n        self.current_idx += 1\n        return _bit_idx, _bit\n\n    def debug__get_stream_slice(self, start: int = 0, end: int = None) -> np.ndarray:\n        \"\"\"\n        Get the stream data by indices range [start, end)\n        :param start:           [Optional] Start index. If not specified, start from head\n        :param end:             [Optional] End index. If not specified, end till tail (included)\n        :return:                Stream data slice\n        \"\"\"\n        if end is None:\n            return self.stream_data[start:]\n        else:\n            return self.stream_data[start:end]\n\n\nstream_obj = StreamSimulator()\n# print(stream_obj.next())  # (0, 0)\n# print(stream_obj.next())  # (1, 0)\n# print(stream_obj.next())  # (2, 1)","b191ae0d":"import typing\n# from collections import deque\nimport time\n\n# from load_stream import *  # annotate when submit in the notebook\n\nMAX_STREAM_LEN = 1000000\n# WINDOW_SIZE = 1000\n\n\nclass DGIMStreamQuery:\n    def __init__(self, window_size: int):\n        assert window_size > 0, \"[ERROR] Zero Window Size. Positive Integers Expected.\"\n        self.MAX_STREAM_LEN = MAX_STREAM_LEN\n        self.WINDOW_SIZE = window_size\n\n        self._INIT_STREAM_IDX_MAX = -1  # init value (no buckets) of the max idx of bits in the stream\n        self._INIT_STREAM_IDX_MIN_END = -self.WINDOW_SIZE  # init value (no buckets) of the min end idx of buckets\n\n        self.buckets = list()\n\n        self.stream_idx_min_end = self._INIT_STREAM_IDX_MIN_END  # drop buckets iff end_time < stream_idx_min_end\n        self.stream_idx_max = self._INIT_STREAM_IDX_MAX  # warning: overflow problem for large streams\n        self.stream_idx_reset_round_cnt = 0  # rounds of stream indices reset operations\n\n    def _reset_stream_idx(self) -> None:\n        \"\"\"\n        Reset stream indices when all are used up,\n            i.e., current idx (stream_idx_max) = MAX_STREAM_LEN - 1\n            solution: decrease end_idx of each bucket by (MAX_STREAM_LEN - 1)\n        \"\"\"\n        assert [] != self.buckets, \"[ERROR] Internal Error. Steam Indices Reset is Called for No Buckets\"\n\n        # reset stream idx, min end idx and add reset counter\n        self.stream_idx_min_end = self._INIT_STREAM_IDX_MIN_END + 1  # buckets exist\n        self.stream_idx_max = self._INIT_STREAM_IDX_MAX + 1  # buckets exist\n        self.stream_idx_reset_round_cnt += 1\n\n        delta = self.MAX_STREAM_LEN - 1\n        for _bucket in self.buckets:  # reference object\n            _bucket[\"end\"] -= delta\n\n        print(\"[INFO] Stream Indices Reset (%d Times in Total)\" % self.stream_idx_reset_round_cnt)\n\n    @staticmethod\n    def _generate_new_bucket_info(end_idx_in_stream: int, size: int) -> typing.Dict[str, int]:\n        \"\"\"\n        Generate the info data structure of a new (inserted ONLY) bucket\n        :param end_idx_in_stream:   end index of the bucket, in stream ([0, MAX_STREAM_LEN))\n        :param size:                size of the bucket: cnt of 1's = 2^{size}\n        :return:                    info object of the new bucket:\n                                        <dict> {\"end\": <int> same as param, \"size\": <int> same as param}\n        \"\"\"\n        info = {\"end\": end_idx_in_stream, \"size\": size}\n        return info\n\n    def debug__get_stream_len(self) -> int:\n        \"\"\"\n        [DEBUG] API: get the current maximum index of bits in the stream, accumulated by the class (start from 0)\n        :return:                    current length of the stream\n        \"\"\"\n        return self.stream_idx_max\n\n    def debug__print_buckets(self) -> None:\n        \"\"\"\n        [DEBUG] API: print all the buckets, from the earliest to the most recent one\n        \"\"\"\n        print(\"[DEBUG] All Buckets (Past -> Future)\")\n        for _idx, _bucket in enumerate(self.buckets):\n            print(\"\\t[%*d] ? - #%0*d: Size = %d\"\n                  % (int(np.ceil(np.log10(len(self.buckets)))), _idx,\n                     int(np.ceil(np.log10(self.MAX_STREAM_LEN))), _bucket[\"end\"], _bucket[\"size\"]))\n\n    def _drop_due_bucket(self) -> None:\n        \"\"\"\n        Drop due buckets (end_time < stream_idx_min_end),\n            operated within each new bit's arrival, and thus focus on the head bucket only\n        \"\"\"\n        if not self.buckets:  # i.e., [] == self.buckets\n            return\n        earliest_bucket = self.buckets[0]\n        if earliest_bucket[\"end\"] < self.stream_idx_min_end:\n            del self.buckets[0]\n\n    def _compact_buckets(self) -> int:\n        \"\"\"\n        Check the buckets (from the most recent to the earliest) to\n            compact the earliest two buckets if 3 consecutive ones are with the same size,\n            until no such consecutive 3-bucket pairs are found\n        :return:                    number of compaction operations actually conducted\n        \"\"\"\n        crt_bucket_idx = len(self.buckets) - 1  # loop buckets from the most recent to the earliest\n        crt_bucket_idx -= 1  # do not check the most recent one\n        consec_bucket_size = self.buckets[-1][\"size\"]  # to-check size of the buckets\n        consec_bucket_cnt = 1  # number of consecutive buckets with the same size, init=1 cuz of the most recent one\n\n        compact_cnt = 0\n        while crt_bucket_idx >= 0:\n            # [CASE 1] new bucket with a DIFFERENT size from that of the previous (more recent) one\n            if self.buckets[crt_bucket_idx][\"size\"] != consec_bucket_size:\n                break\n\n            # [CASE 2] new bucket with the SAME size as that of the previous (more recent) one\n            consec_bucket_cnt += 1\n\n            # [CASE 2-A] 1 or 2 consecutive buckets now\n            if 3 > consec_bucket_cnt:\n                crt_bucket_idx -= 1\n                continue\n\n            # [CASE 2-B] 3 consecutive buckets with the same size:\n            #   compact the earliest 2 bucket into 1 bigger bucket\n\n            # extract info of the new bucket\n            _prev_bucket_idx = crt_bucket_idx + 1\n            _new_end_idx_in_stream = self.buckets[_prev_bucket_idx][\"end\"]\n            _new_size = self.buckets[crt_bucket_idx][\"size\"] + 1\n\n            # modify the earliest compacted bucket to be the new one\n            self.buckets[crt_bucket_idx][\"end\"] = _new_end_idx_in_stream\n            self.buckets[crt_bucket_idx][\"size\"] = _new_size\n            # delete the semi-earliest bucket\n            del self.buckets[_prev_bucket_idx]\n            # finish compaction\n            compact_cnt += 1\n\n            # set the after-compaction bucket's size as the to-check one\n            consec_bucket_size = self.buckets[crt_bucket_idx][\"size\"]\n            consec_bucket_cnt = 1\n            crt_bucket_idx -= 1\n\n        return compact_cnt\n\n    def next(self, new_bit: int):\n        \"\"\"\n        Push a new bit\n        :param new_bit:             value of the newly pushed bit\n        :return:\n        \"\"\"\n        assert 0 == new_bit or 1 == new_bit, \"[ERROR] Invalid Bit Value. Got %d, Expected 0 or 1.\" % new_bit\n\n        self.stream_idx_max += 1  # move forward stream\n        self.stream_idx_min_end += 1  # move forward stream\n        self._drop_due_bucket()  # drop due buckets (end_time < stream_idx_min_end)\n\n        if 0 == new_bit:\n            return\n        else:\n            crt_idx_stream = self.stream_idx_max\n            new_bucket = self._generate_new_bucket_info(end_idx_in_stream=crt_idx_stream, size=0)\n            self.buckets.append(new_bucket)\n            _ = self._compact_buckets()\n\n        # reset stream index if indices are all used up\n        if self.MAX_STREAM_LEN == self.stream_idx_max + 1:\n            self._reset_stream_idx()\n\n    def count(self, bit_range: int = -1) -> int:\n        \"\"\"\n        Count the number of 1-bits in the given range\n        :param bit_range:           [Optional] range of the counting task,\n                                        given as the number of bits from the most recent one (included),\n                                        if not specified, the range is exactly the window_size\n        :return:                    number of 1's\n        \"\"\"\n        assert -1 == bit_range or bit_range > 0, \\\n            \"[ERROR] Invalid Count Range. Got %d, Expected -1 (All Window Size) or Positive Integers\" % bit_range\n\n        bit_range_real = bit_range if -1 != bit_range else WINDOW_SIZE\n\n        count = 0\n        _earliest_bucket_found = False\n        _stream_idx_min_end = self.stream_idx_max - bit_range_real + 1\n        for _idx, _bucket in enumerate(self.buckets):\n            if _bucket[\"end\"] < _stream_idx_min_end:  # out-of-bound buckets: too early\n                continue\n\n            # in-bound buckets: _bucket[\"end\"] >= _stream_idx_min_end:\n            if _earliest_bucket_found is False:  # first occurrence\n                _earliest_bucket_found = True\n                count += 2 ** (_bucket[\"size\"] - 1)  # add half of the 1's count of the earliest inbound bucket\n            else:  # consecutive occurrence\n                count += 2 ** (_bucket[\"size\"])  # add half of the 1's count of the earliest bucket\n\n        return count\n\n\n# # debug codes\n# WINDOW_SIZE = 1000\n# test_dgim = DGIMStreamQuery(window_size=WINDOW_SIZE)\n# test_slice = 100\n# for _ in range(test_slice):\n#     _, test_new_bit = stream_obj.next()\n#     test_dgim.next(new_bit=test_new_bit)\n# print(stream_obj.debug__get_stream_slice(end=test_slice))\n# test_dgim.debug__print_buckets()","6136fe02":"WINDOW_SIZE = 1000\ndgim = DGIMStreamQuery(window_size=WINDOW_SIZE)\n_cnt_start_dgim = time.perf_counter()\nwhile 1:\n    try:\n        _, _new_bit = stream_obj.next()\n    except AssertionError:\n        break\n    dgim.next(new_bit=_new_bit)\nres_cnt_dgim_all = dgim.count()\ntime_elapsed_dgim = time.perf_counter() - _cnt_start_dgim\nprint(\"In the entire window (size %d), DGIM approximate %d 1's in %.4f seconds\"\n      % (WINDOW_SIZE, res_cnt_dgim_all, time_elapsed_dgim))","9748268c":"res_cnt_dgim_500 = dgim.count(bit_range=500)\nprint(\"In the last %d bits of the window (size %d), DGIM approximate %d 1's\"\n      % (500, WINDOW_SIZE, res_cnt_dgim_500))\n\nres_cnt_dgim_200 = dgim.count(bit_range=200)\nprint(\"In the last %d bits of the window (size %d), DGIM approximate %d 1's\"\n      % (200, WINDOW_SIZE, res_cnt_dgim_200))","38468124":"_cnt_start_real = time.perf_counter()\n_stream_data_all = stream_obj.debug__get_stream_slice()\n_stream_data_window = _stream_data_all[_stream_data_all.shape[0] - WINDOW_SIZE:]\nres_cnt_real_all = np.where(1 == _stream_data_window)[0].shape[0]\ntime_elapsed_real = time.perf_counter() - _cnt_start_real\nprint(\"In the entire window (size %d expected, %d got), the real counter asserts %d 1's in %.4f seconds\"\n      % (WINDOW_SIZE, _stream_data_window.shape[0], res_cnt_real_all, time_elapsed_real))","a7509fa8":"import typing\nimport random\n\nRANGE_RATIO = 10000\nMAX_HASH_FUNC_CNT = 10000\n\n\nclass HashFunctionGenerator:\n    def __init__(self, max_func_cnt: int):\n        self.max_func_cnt = max_func_cnt\n\n        self.rand_nums = []  # require max_func_cnt-1 numbers, since <built-in> hash() is the 1-ST hash function\n        self._generate_random_numbers()\n\n    def _generate_random_numbers(self) -> None:\n        \"\"\"\n        Generate random numbers for XOR operations\n        \"\"\"\n        max_rand_range = self.max_func_cnt * RANGE_RATIO\n        rand_nums = {}\n        rand_num_cnt = 0\n        while rand_num_cnt < self.max_func_cnt - 1:  # <built-in> hash() accounts for one function\n            _rand_num = random.randint(0, max_rand_range)\n            if _rand_num in rand_nums:\n                continue\n            rand_nums[_rand_num] = 1\n            rand_num_cnt += 1\n        self.rand_nums = list(rand_nums.keys())\n\n    def hash(self, value: typing.Any, use_cnt: int = -1) -> typing.List[int]:\n        \"\"\"\n        Hash the given value using given number of hash functions and return the results\n        :param value:           given to-be-hashed value\n        :param use_cnt:         [Optional] number of hash functions to use\n                                    if not specified, all the hash functions will be used\n        :return:                <list> of the given number of <int> hashed values\n        \"\"\"\n        assert -1 == use_cnt or 0 < use_cnt < self.max_func_cnt, \\\n            \"[ERROR] Invalid Number of Hash Functions to Use. \" \\\n            \"Got %d. Expected -1 (All Functions) or Positive Integers\" % use_cnt\n\n        use_cnt_real = use_cnt if -1 != use_cnt else self.max_func_cnt\n\n        hash_values = [hash(value)]\n        for _idx, _num in enumerate(self.rand_nums[:use_cnt_real - 1]):\n            hash_values.append(hash_values[_idx] ^ _num)\n        return hash_values\n\n\nhash_func_family = HashFunctionGenerator(max_func_cnt=MAX_HASH_FUNC_CNT)","696aece3":"import nltk\nfrom nltk.corpus import words\n\n# # download words file (if \"LookupError\" is raised)\n# nltk.download('words')\n\nword_list = words.words() # len=236736, <list>of<str> ['A', ..., 'Aani', 'Ababdeh', ...]","3214f3f4":"# # download words file (if \"LookupError\" is raised)\n# nltk.download('movie_reviews')\n    \nfrom nltk.corpus import movie_reviews\n\nneg_reviews = [] # len=751256 <list>of<str> ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ...]\npos_reviews = [] # len=832564 <list>of<str> ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ...]\n\nfor fileid in movie_reviews.fileids('neg'):\n  neg_reviews.extend(movie_reviews.words(fileid))\nfor fileid in movie_reviews.fileids('pos'):\n  pos_reviews.extend(movie_reviews.words(fileid))","a879355f":"PUNC_MARK_ASCII = [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 58, 59, 60, 61, 62, 63, 64, 91, 92, 93, 94, 95, 96, 123, 124, 125, 126]\nPUNC_MARK_DICT = dict.fromkeys([chr(_id) for _id in PUNC_MARK_ASCII], 1)","033fa697":"from tqdm import tqdm\n\n\nword_list_dict = dict.fromkeys(word_list, 1)\n\n\ndef word_in_dict(in_word:str, use_dict=False) -> int:\n    if in_word in PUNC_MARK_DICT:\n        return -1\n    if use_dict:\n        if in_word in word_list_dict:\n            return 1\n        return 0\n    else:\n        if in_word in word_list:\n            return 1\n        return 0\n\n# negative reviews (<dict> version)\n_judge_start_dict_neg = time.perf_counter()\njudge_res_label_neg = []\nfor _word in tqdm(neg_reviews):\n    _label = word_in_dict(in_word=_word, use_dict=True)\n    judge_res_label_neg.append(_label)\ntime_elapsed_dict_neg = time.perf_counter() - _judge_start_dict_neg\nprint(\"Judgement Finished upon All %d Negative Review Words (Dict) in %.4f seconds\"\n      % (len(neg_reviews), time_elapsed_dict_neg))\n\n# negative reviews (<list> version)\n_judge_start_list_neg = time.perf_counter()\nfor _word in tqdm(neg_reviews):\n    _label = word_in_dict(in_word=_word, use_dict=False)\ntime_elapsed_list_neg = time.perf_counter() - _judge_start_list_neg\nprint(\"Judgement Finished upon All %d Negative Review Words (List) in %.4f seconds\"\n      % (len(neg_reviews), time_elapsed_list_neg))\n\n# positive reviews (<dict> version)\n_judge_start_dict_pos = time.perf_counter()\njudge_res_label_pos = []\nfor _word in tqdm(pos_reviews):\n    _label = word_in_dict(in_word=_word, use_dict=True)\n    judge_res_label_pos.append(_label)\ntime_elapsed_dict_pos = time.perf_counter() - _judge_start_dict_pos\nprint(\"Judgement Finished upon All %d Positive Review Words (Dict) in %.4f seconds\"\n      % (len(pos_reviews), time_elapsed_dict_pos))\n\n# positive reviews (<list> version)\n_judge_start_list_pos = time.perf_counter()\nfor _word in tqdm(pos_reviews):\n    _label = word_in_dict(in_word=_word, use_dict=False)\n    judge_res_label_pos.append(_label)\ntime_elapsed_list_pos = time.perf_counter() - _judge_start_list_pos\nprint(\"Judgement Finished upon All %d Positive Review Words (List) in %.4f seconds\" \n      % (len(pos_reviews), time_elapsed_list_pos))","de55ce48":"import typing\nimport math\nfrom tqdm import tqdm\n\n\n# hash functions are given in var <hash_func_family>, call obj.hash() to get the hashed values\n\nclass BitArray:\n    def __init__(self, size):\n        \"\"\" Create a bit array of a specific size \"\"\"\n        self.size = size\n        # self.bit_array = bytearray(math.ceil(size \/ 8.))\n        self.bit_array = bytearray(int(math.ceil(size \/ 8.)))\n\n    def set(self, n):\n        \"\"\" Sets the nth element of the bit array \"\"\"\n        index = n \/\/ 8\n        position = n % 8\n        self.bit_array[index] = self.bit_array[index] | 1 << (7 - position)\n\n    def get(self, n):\n        \"\"\" Gets the nth element of the bit array \"\"\"\n        index = n \/\/ 8\n        position = n % 8\n        return (self.bit_array[index] & (1 << (7 - position))) > 0\n\n\nclass BloomFilter:\n    def __init__(self, bit_arr_len: int, hash_func_cnt: int):\n        assert 1 <= hash_func_cnt <= MAX_HASH_FUNC_CNT, \\\n            \"[ERROR] Invalid Number of Hash Functions. Got %d. Expected [1, %d]\" \\\n            % (hash_func_cnt, MAX_HASH_FUNC_CNT)\n\n        self.bit_array_len = bit_arr_len  # n, the length of the bit array B, in bits\n        self.hash_func_cnt = hash_func_cnt  # k, the number of hash functions\n        self.bit_array_obj = BitArray(size=bit_arr_len)\n\n    def _add(self, key: typing.Any) -> None:\n        \"\"\"\n        add the given keyword to the bit map\n        :param key:             value of the to-add key\n        \"\"\"\n        # call the function to generate the hit indices in the bit array\n        hit = hash_func_family.hash(value=key, use_cnt=self.hash_func_cnt)\n        \n        # update the bit map values\n        for _idx in hit:\n            self.bit_array_obj.set(_idx % self.bit_array_len)\n\n    def _search(self, key: typing.Any) -> bool:\n        \"\"\"\n        search the bit map to determine whether a keyword is in the bit map\n        :param key:             value of the to-search key\n        :return:                True if is in, False otherwise\n        \"\"\"\n        # call the function to generate the hit indices in the bit array\n        to_hit = hash_func_family.hash(key)\n        for _idx in to_hit:\n            # the corresponding idx in the bit array is 0, i.e., keyword not in\n            if not self.bit_array_obj.get(_idx % self.bit_array_len):\n                return False\n        return True\n\n    def test(self, train_list: list, test_list: list, test_label: list,\n             show_info: bool = True) -> float:\n        \"\"\"\n        test the bloom filter\n        :param train_list:      <list> of train data: dictionary keys\n        :param test_list:       <list> of test data: test keys\n        :param test_label:      <list> of test data labels (ground truth):\n                                    0 if not in, -1 if punctuation marks, 1 if in\n        :param show_info:       whether to show info messages\n        :return:                false positive rate\n        \"\"\"\n        # add the keywords to the bit map\n        if show_info:\n            print(\"Adding Keywords\")\n            for _word in tqdm(train_list):\n                self._add(_word)\n        else:\n            for _word in train_list:\n                self._add(_word)\n\n        # test the filter\n        test_cnt = len(test_list)\n        false_pos_cnt = 0  # predict negative as positive\n        if show_info:\n            print(\"Testing Keywords\")\n            for _idx in tqdm(range(test_cnt)):\n                _word = test_list[_idx]\n                if 1 != test_label[_idx] and self._search(_word) is True:\n                    false_pos_cnt += 1\n        else:\n            for _idx, _word in enumerate(test_list):\n                if 1 != test_label[_idx] and self._search(_word) is True:\n                    false_pos_cnt += 1\n        false_positive_rate = 1. * false_pos_cnt \/ test_cnt\n\n        return false_positive_rate","a87aeaf3":"bloom_filter_neg = BloomFilter(bit_arr_len=2000000, hash_func_cnt=6)\nbloom_filter_pos = BloomFilter(bit_arr_len=2000000, hash_func_cnt=6)\n\n_judge_start_bf_neg = time.perf_counter()\nfalse_pos_neg = bloom_filter_neg.test(\n    train_list=word_list, test_list=neg_reviews, test_label=judge_res_label_neg)\ntime_elapsed_bf_neg = time.perf_counter() - _judge_start_bf_neg\nprint(\"Judgement Finished upon All %d Negative Review Words in %.4f seconds with %.5f False Positive Rate\" \n      % (len(neg_reviews), time_elapsed_bf_neg, false_pos_neg))\n\n_judge_start_bf_pos = time.perf_counter()\nfalse_pos_pos = bloom_filter_pos.test(\n    train_list=word_list, test_list=pos_reviews, test_label=judge_res_label_pos)\ntime_elapsed_bf_pos = time.perf_counter() - _judge_start_bf_pos\nprint(\"Judgement Finished upon All %d Positive Review Words in %.4f seconds with %.5f False Positive Rate\" \n      % (len(pos_reviews), time_elapsed_bf_pos, false_pos_pos))","9ae7588b":"param_test_cnt = 5000\nparam_test_list = pos_reviews[:param_test_cnt]\nparam_test_label = judge_res_label_pos[:param_test_cnt]\n\nbit_arr_len_gp = [100000 * i for i in range(1, 100 + 1, 10)]\nhash_fun_cnt_gp = [i for i in range(1, 100 + 1, 10)]\n\ntry:\n    false_pos_rate_test_res = np.load(\"false_pos_rate.npy\")\nexcept FileNotFoundError:\n    false_pos = np.zeros(shape=(len(bit_arr_len_gp), len(hash_fun_cnt_gp)))\n    for _idx_bal, _b_a_l in enumerate(bit_arr_len_gp):\n        # print(\"#%d bit array len = %d\" % (_idx_bal, _b_a_l))\n        for _idx_hfc in tqdm(range(len(hash_fun_cnt_gp))):\n            _bloom_filter = BloomFilter(bit_arr_len=_b_a_l, hash_func_cnt=hash_fun_cnt_gp[_idx_hfc])\n\n            _false_pos = _bloom_filter.test(\n                train_list=word_list, test_list=param_test_list,\n                test_label=param_test_label, show_info=False)\n            false_pos[_idx_bal, _idx_hfc] = _false_pos\n            # print(_false_pos)\n\n    np.save(\"false_pos_rate.npy\", false_pos)","fd34fd9b":"import matplotlib\nimport matplotlib.pyplot as plt\n# from mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d import axes3d\nimport numpy as np\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\n\n# Reference: https:\/\/matplotlib.org\/mpl_toolkits\/mplot3d\/tutorial.html\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\n\n# Make data.\nX = np.array(bit_arr_len_gp)\nY = np.array(hash_fun_cnt_gp)\nX, Y = np.meshgrid(X, Y)\nZ = np.load(\"false_pos_rate.npy\")\n\n# Plot the surface.\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n\n# Customize the z axis.\n# ax.set_zlim(-1.01, 1.01)\n# ax.zaxis.set_major_locator(LinearLocator(10))\n# ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.set_xlabel(\"Bit Array Length\")\nax.set_ylabel(\"Hash Function Number\")\nax.set_zlabel(\"False Positive Rate\")\n\n# Add a color bar which maps values to colors.\nfig.colorbar(surf, shrink=0.5, aspect=15)\n\nfig.suptitle(\"Bloom Filter Hyper-Param Simple Test\")\n\nplt.show()","bf73c7c4":"word_cnt_pos = {}\nfor _word in pos_reviews:\n    if _word in PUNC_MARK_DICT:\n        continue\n    try:\n        word_cnt_pos[_word] += 1\n    except KeyError:\n        word_cnt_pos[_word] = 1","277225de":"class CountMinSketch:\n    def __init__(self, cnt_width: int, hash_func_cnt: int):\n        assert 1 <= hash_func_cnt <= MAX_HASH_FUNC_CNT, \\\n            \"[ERROR] Invalid Number of Hash Functions. Got %d. Expected [1, %d]\" \\\n            % (hash_func_cnt, MAX_HASH_FUNC_CNT)\n\n        self.cnt_width = cnt_width  # w, the width of the bit count matrix\n        self.hash_func_cnt = hash_func_cnt  # k, the number of hash functions\n        self.cnt_mat_obj = [[0 for _ in range(self.cnt_width)]\n                            for _ in range(self.hash_func_cnt)]\n\n    def _add(self, key: typing.Any) -> None:\n        \"\"\"\n        add the given keyword to the bit map\n        :param key:             value of the to-add key\n        \"\"\"\n        # call the function to generate the hit indices in the bit array\n        hit = hash_func_family.hash(value=key, use_cnt=self.hash_func_cnt)\n\n        # update the bit map values\n        for _hash_idx, _hash_val in enumerate(hit):\n            self.cnt_mat_obj[_hash_idx][_hash_val % self.cnt_width] += 1\n\n    def _retrieve(self, key: typing.Any) -> int:\n        \"\"\"\n        search the bit map to determine whether a keyword is in the bit map\n        :param key:             value of the to-search key\n        :return:                approximated count (min count of all hash functions)\n        \"\"\"\n        # call the function to generate the hit indices in the bit array\n        to_hit = hash_func_family.hash(key, use_cnt=self.hash_func_cnt)\n        min_cnt_val = np.inf\n        for _hash_idx,_hash_val in enumerate(to_hit):\n            # find min of cnt[hash_idx, hash_val]\n            _crt_cnt = self.cnt_mat_obj[_hash_idx][_hash_val % self.cnt_width]\n            min_cnt_val = min(_crt_cnt, min_cnt_val)\n\n        return min_cnt_val\n\n    def test(self, in_word_list: list, gr_word_cnt: dict,\n             show_info: bool = True) -> typing.Dict[str, int]:\n        \"\"\"\n        test the bloom filter\n        :param in_word_list:    <list> of to-count words\n        :param gr_word_cnt:     <dict> of test data cnt (ground truth)\n        :param show_info:       whether to show info messages\n        :return:\n        \"\"\"\n        # add the keywords to the bit map\n        if show_info:\n            print(\"Adding Words\")\n            for _word in tqdm(in_word_list):\n                self._add(_word)\n        else:\n            for _word in in_word_list:\n                self._add(_word)\n\n        # test the filter\n        error_cnt_sum = 0  # \\sum_{words} { abs(aprox_cnt - gr_cnt) }\n        if show_info:\n            print(\"Testing words counts\")\n            for _word, _gr_cnt in tqdm(gr_word_cnt.items()):\n                _apporx_cnt = self._retrieve(key=_word)\n                error_cnt_sum += abs(_apporx_cnt - _gr_cnt)\n        else:\n            for _word, _gr_cnt in gr_word_cnt.items():\n                _apporx_cnt = self._retrieve(key=_word)\n                error_cnt_sum += abs(_apporx_cnt - _gr_cnt)\n\n        return error_cnt_sum\n","3e6b25c2":"bit_arr_len_gp = [100000 * i for i in range(1, 100 + 1, 10)]\nhash_fun_cnt_gp = [i for i in range(1, 100 + 1, 10)]\n\ntry:\n    error_test_res = np.load(\"error_cms.npy\")\nexcept FileNotFoundError:\n    error_sum = np.zeros(shape=(len(bit_arr_len_gp), len(hash_fun_cnt_gp)))\n    for _idx_bal, _b_a_l in enumerate(bit_arr_len_gp):\n        for _idx_hfc in tqdm(range(len(hash_fun_cnt_gp))):\n            _cnt_min_sk = CountMinSketch(cnt_width=_b_a_l, hash_func_cnt=hash_fun_cnt_gp[_idx_hfc])\n\n            _error_sum = _cnt_min_sk.test(\n                in_word_list=pos_reviews, gr_word_cnt=word_cnt_pos,\n                show_info=False)\n            error_sum[_idx_bal, _idx_hfc] = _error_sum\n            # print(_false_pos)\n\n    np.save(\"error_cms.npy\", error_sum)","8b2164a1":"fig = plt.figure()\nax = fig.gca(projection='3d')\n\n# Make data.\nX = np.array(bit_arr_len_gp)\nY = np.array(hash_fun_cnt_gp)\nX, Y = np.meshgrid(X, Y)\nZ = np.load(\"error_cms.npy\")\n\n# Plot the surface.\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n\n# Customize the z axis.\n# ax.set_zlim(-1.01, 1.01)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.set_xlabel(\"Width $w$\")\nax.set_ylabel(\"Depth $d$\")\nax.set_zlabel(\"Abs Error Sum\")\n\n# Add a color bar which maps values to colors.\nfig.colorbar(surf, shrink=0.5, aspect=15)\n\nfig.suptitle(\"Count-Min Sketch Hyper-Param Simple Test\")\n\nplt.show()","679cee57":"### 1. Write a function that accurately determines whether each word in `neg_reviews` and `pos_reviews` belongs to `word_list`.","7a735ec2":"Here we get a data stream (`word_list`) and 2 query lists (`neg_reviews` and `pos_reviews`).","a71e5d6b":"For the same reason, we set words in the English dictionary as `key`s of a `<dict>`. Now, a simply search for the given word in the dictionary results in the required accurate results, where,\n\n+ `1` label: word is in the dictionary\n+ `0` label: word is not in the dictionary\n+ `-1` label: word is a punctuation mark\n\nHere, however, besides the above most efficient way, we ought to conduct `<list>` search to provide execution time benchmark for the following questions.","ec7fcae6":"## **Task1\uff1aDGIM**\n\nDGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the `stream_data_dgim.txt` (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code below.","5202b24e":" ### 2. Implement the bloom filter by yourself and add all words in `word_list` in your bloom filter. Compare the running time difference between linear search on a list and multiple hash computations in a Bloom filter.","76bfeae7":"First, we calculate the false positive rate of several sets of settings,*(even within such a limited amount of data, it takes about 20 min to calculate the results)*","0fd38904":"### Stream data loading:","0588a4da":"The execution results (history value, run without checkpoints) are listed as follows,\n\n| Counter | 1's Count | Execution Time |\n| :-----: | :-------: | :------------: |\n| DGIM   | 508     | 0.1829 s     |\n| Real   | 391     | 0.0008 s     |","fcc6fb93":"# EE359-Coursework 4  Streaming Algorithm","de4b73b6":"Notice that, there are punctuation marks in the review words. Therefore, we ought to filter them out while making judgments (mark as `-1`). Due to efficiency concernes, these marks are stored as `key`s of `<dict>`, which is accessed\/searched via hash functions.","468dfc64":"Now the hash function family is stored in the object `hash_func_family`. We may hash values using a given number of hash functions through `hash_func_family.hash(value=my_value, use_cnt=my_use_cnt)`.","a13eb6d5":"### 1. Write a function that accurately counts the occurrence times of each word in `neg_reviews` or `pos_reviews`.","19327d74":"Bloom Filter Implementation","4647f096":"1. Data Structure\n\n    + Each bucket is represented by a `<dict>`: `{\"end\": <int>, \"len\": <int>}`, where,\n        * `\"end\"` is the end timestamp of the bucket, ranging in `[0, MAX_STREAM_LEN)`,\n            - i.e., the maximum index of the bits in the stream\n            - here, we do not let `\"end\" = actual_index_of_stream mod window_size`\n                + because, if so, each insertion requires index update of all buckets (costly)\n                + instead, we maintain the minimum possible end time of the buckets (drop iff `end_time > min_end_time`)\n        * \"size\" indicates the number of 1's in the bucket: $CNT(1's) = 2^{``size''}$\n    + All buckets' `<dict>s` are placed in a `<list>`, where the earlier the bucket, the smaller its `<list>` index is.\n        * i.e. add new buckets via `<list>.append(<dict>)`\n\n2. Design Implementation <\/br>\n    Here, due to efficiency concerns, `<collections.deque>` is **ATTEMPTED** to be (but **NOT**) used instead of naive `<list>`.\n\n    + APIs of `<deque>` are similar to those of `<list>`. [[Reference]](https:\/\/stackoverflow.com\/a\/8538295), [[Doc]](https:\/\/docs.python.org\/3\/library\/collections.htmlcollections.deque)\n    + Strength: both-end insertion and removal: $\\mathcal{O}(1) \\Leftrightarrow$ $O(n)$ of `<list>` operations at the head\n    + Weakness: random index access: $\\mathcal{O}(1)$ at both ends but $\\mathcal{O}(n)$ in the middle $\\Leftrightarrow$ $\\mathcal{O}(1)$ of `<list>`\n\n","ddde2612":"### 2. Implement the Count-Min sketch by yourself. Set different width $w$ and depth $d$ of the internal data structure of CM-Sketch. Compare the influence of different $w$ and $d$ on the error.\n","70fbadec":"In computing, the count\u2013min sketch (CM sketch) is a probabilistic data structure that serves as a frequency table of events in a stream of data. ","a11b2244":"Still, we ought to filter out all the punctuation marks.","861fe100":"## **Task2: Bloom Filter**\n\nA Bloom filter is a space-efficient probabilistic data structure. Here the task is to implement a bloom filter by yourself. ","1dbe520e":"From which, we may observe that, except for extremely small width $w$ and depth $d$, the overall error is low, indicating satisfactory performance. In other words, the selection of the two hyper-parameters is not that demanding, compared with that of Bloom Filters.","e3fd0d2e":"Then, we may plot in 3D to visualize,","323c2459":"Now the stream data is read from file and stored in the object `stream_obj`. We may retrieve bits through `bit_idx, bit_value = stream_obj.next()`.","45e2808d":"The execution results (history value, run without checkpoints) are listed as follows,\n\n| Review   | Samples Count | Data Sturcture | Execution Time |\n| :-------: | :------------:| :------------: | :------------: |\n| Negative  | 751,256     | `<dict>`     | 0.7880 s     |\n| Negative  | 751,256     | `<list>`     | 1,021 s     |\n| Positive  | 832,564     | `<dict>`     | 1.030 s     |\n| Positive  | 832,564     | `<list>`     | 1,117 s     |","c50cfb1c":"Now use Count-Min Sketch to analyze the influence of different width $w$ and depth $d$ on the error.  \nHere, the error of each setting is calculated as, the sum of absolute value of the difference between the accurate count and the approximate one of each word, i.e.,\n$$error = \\sum_{word \\in WORDS}{abs(CNT_{accurate} - CNT_{apporx})}$$","94a3d85a":"### 3. Use different bit array length $m$ and number of hash functions $k$ to implement the bloom filter algorithm. Then compare the impact of different $m$ and $k$ on the false positive rate.","7ec43b75":"### Data loading:\n\nFrom the NLTK (Natural Language ToolKit) library, we import a large list of English dictionary words, commonly used by the very first spell-checking programs in Unix-like operating systems.","2c1a0288":"From which, we may ambiguously (smoother surface can be drawm from more sets of settings) observe the known optimal solution,\n\n$$k = \\frac{m}{N}\\cdot\\ln(2)$$\n\nwhere, $k$ is the number of hash functions, $m$ is the bit array length, $N$ is the number of keys in the dictionary.","d88fd759":"Here we use the query stream (`neg_reviews` or `pos_reviews`) from task 2.  \n**In practive**, `pos_reviews` is used only.","b31b1b98":"### 2. With the window size 1000, count the number of 1-bits in the last 500 and 200 bits of the bitstream.","97232e10":"Then we load another dataset from the NLTK Corpora collection: `movie_reviews`.\n\nThe movie reviews are categorized between positive and negative, so we construct a list of words (usually called bag of words) for each category.","1de4c0cb":"Count-Min Sketch Implementation","3ba696c5":"### 3. Write a function that accurately counts the number of 1-bits in the current window. Caculate the accuracy of your own DGIM algorithm and compare the running time difference.","a8105b53":"## **Hash Functions Generator**\nBefore we get started with the following hash-based tasks (*Task 2*: Bloom Filter, *Task 3*: Count-Min Sketch), we ought to implement a function to generate whatever number of hash functions, as independent from each other as possible.\n\n+ [[Reference]](http:\/\/www.partow.net\/programming\/hashfunctions\/) General Purpose Hash Function Algorithms\n+ [[Reference]](https:\/\/stackoverflow.com\/questions\/2255604\/hash-functions-family-generator-in-python) Concise but Far from Random\n+ [[Reference]](http:\/\/gregoryzynda.com\/python\/developer\/hashing\/2018\/02\/05\/hashing-functions.html) Generate New Hash Functions Via XOR Hash Value with Another Random Integer","b3fcf152":"### 1. Set the window size to 1000, and count the number of 1-bits in the current window.","760a331e":"Now draw the 3D surface, *(after the exhausting 40-min execution)*","ccfacb79":"The execution results (history value, run without checkpoints) are listed as follows, (with previous results)\n\n| Review   | Samples Count | Data \/ Filter | Execution Time| False Positive Rate |\n| :-------: | :------------:| :-------------: | :---------: | :-----------------: |\n| Negative  | 751,256     | `<dict>`      | 0.7880 s   | 0             |\n| Negative  | 751,256     | `<list>`      | 1,021 s     | 0             |\n| Negative  | 751,256     | Bloom Filter   | 466.8 s  | 0.00000         |\n| Positive  | 832,564     | `<dict>`      | 1.030 s   | 0             |\n| Positive  | 832,564     | `<list>`      | 1,117 s     | 0             |\n| Negative  | 751,256     | Bloom Filter   | 517.6 s  | 0.00000         |\n\nFrom which, we may observe that,\n\n+ The bloom filter ($\\mathcal{O}(k\\times1)$) is much efficient than searching in the `<list>` ($\\mathcal{O}(n)$).\n+ However, the `<built-in> <dict>` is even more faster due to some magic.\n+ Here, since the hyper-parameters are ideal choice, the false positive rate of both bloom filters are almost zero.","ea9f91fc":"For simplicity, we only focus on the first 5,000 samples of `pos_reviews` (832,564 samples in total). It is similar for `neg_reviews`.","7115843a":"Now we add all words in `word_list` into the bloom filter (bit array length is 2000000, use 6 hash functions, ideal choice),","13162cf5":"### DGIM Implementation","163d8707":"## **Task3: Count-Min sketch**\n\n"}}