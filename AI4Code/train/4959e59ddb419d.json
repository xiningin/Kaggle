{"cell_type":{"978c1ef6":"code","1a4115f0":"code","a13f6aab":"code","3fc74046":"code","c77e1271":"code","c2aa9638":"code","11d897d0":"code","50efca59":"code","0d48ab6c":"code","ba785b2c":"code","2e48320e":"code","ed43053e":"code","f4cce244":"code","097d061c":"code","9fce5066":"code","4c1509d9":"code","51c992e8":"code","a84ada47":"code","cb74fbd8":"code","07351bf0":"code","2d1225a1":"code","a0bd8b86":"code","7cfe1bb5":"code","c4f7f490":"markdown","7eb4cd0f":"markdown","b3127242":"markdown","8c1ee396":"markdown","04e4bfaf":"markdown","9b049589":"markdown","1a5404b2":"markdown","fe6a88c4":"markdown","6987a5ed":"markdown","bba22cfe":"markdown","104086eb":"markdown","e2da8bad":"markdown","275255db":"markdown","8f948dee":"markdown","b6660114":"markdown","b3b94450":"markdown","e3779a03":"markdown","4a97314b":"markdown","b15d5b70":"markdown","f50b562f":"markdown","8687c172":"markdown","b2940e57":"markdown","fea96be2":"markdown","e2797fc8":"markdown","4967875d":"markdown","ca3a1651":"markdown","375f9b78":"markdown","f9308fbd":"markdown","dfdaf16d":"markdown","01961c75":"markdown","e7ffa020":"markdown","225f94a9":"markdown","c4e1c247":"markdown","d8b58583":"markdown","57cf0208":"markdown","45a6abfd":"markdown","bce7a9cf":"markdown","fa105db2":"markdown","6cd54081":"markdown","f7cf3bab":"markdown"},"source":{"978c1ef6":"# Importing standard packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","1a4115f0":"#\u00a0Import training and test set \ndf_train = pd.read_csv('..\/input\/random-linear-regression\/train.csv')\ndf_test = pd.read_csv('..\/input\/random-linear-regression\/test.csv')","a13f6aab":"# Join train and test set into one dataframe \ndf = pd.concat((df_train, df_test), axis=0).reset_index(drop=True)","3fc74046":"# Check for Nulls and dtype of dataframe\ndf.info()","c77e1271":"#\u00a0Check for NaNs\ndf.isna().sum()","c2aa9638":"#\u00a0Check for duplicates\ndf.duplicated().sum()","11d897d0":"# Drop NaNs and duplicates\ndf = df.dropna().drop_duplicates()","50efca59":"# Overall statistics\ndf.describe()","0d48ab6c":"# Check for correlation\nsns.heatmap(df.corr(), annot = True, cmap=\"tab20c\");","ba785b2c":"# Plot data\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Relationship between x and y')\nplt.plot(df.iloc[:,0],df.iloc[:,1],'ro', mec= 'k');\nplt.legend(['Dataset']);","2e48320e":"#\u00a0Separate features and target variable\nX = df.iloc[:,:-1]\ny = df.iloc[:,1]","ed43053e":"# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/4, random_state=42)\n# Re-index\nX_train = X_train.reset_index(drop=True) \ny_train = y_train.reset_index(drop=True) \nX_test = X_test.reset_index(drop=True) \ny_test = y_test.reset_index(drop=True) ","f4cce244":"# Add ones to the dataframes\nm,n = X_train.values.shape\no,p = X_test.values.shape\nX_train = pd.concat((pd.DataFrame(np.ones((m, 1)), columns= ['Bias']),X_train),axis=1)\nX_test = pd.concat((pd.DataFrame(np.ones((o, 1)), columns= ['Bias']),X_test),axis=1)","097d061c":"def LR_Cost(X: pd.DataFrame, y: pd.Series, theta: np.array) -> float:\n\n    \"\"\" Return the Mean Squared Error of a linear regression model using vectorised notation. \"\"\"\n\n    # Retrieve feature dimensions\n    m, n = X.shape \n    # Calculate error \n    residual = y - X @ theta\n    # Calculate Cost Function \n    CF_LR = (1\/m) * (residual.T @ residual)\n    # Return Cost Function\n    return CF_LR","9fce5066":"def LS_NE(X: pd.DataFrame, y: pd.Series) -> np.array:\n    \n    \"\"\" Return the maximum likelihood estimate from the normal equations as this maximises the likelihood of the linear regression model. \"\"\"\n\n    # Calculate estimator theta\n    return np.linalg.solve((X.T @ X), X.T @ y)","4c1509d9":"def predict(X: pd.DataFrame, theta_estimate: np.array) -> pd.Series:\n    \n    \"\"\" Return model predictions. \"\"\"\n\n    return X @ theta_estimate","51c992e8":"class LinearRegression():\n    \n    def __init__(self):\n        \n        \"\"\" Initialise parameters. \"\"\"\n        \n        self.theta = None\n    \n    def fit(self, X: pd.DataFrame, y: pd.Series) -> np.array:\n    \n        \"\"\" Return the maximum likelihood estimate from the normal equations as this maximises the likelihood of the linear regression model. \"\"\"\n\n        # Calculate estimator theta\n        self.theta = np.linalg.solve((X.T @ X), X.T @ y)\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n    \n        \"\"\" Return model predictions. \"\"\"\n\n        y_pred = X @ self.theta\n        # Return predictions\n        return y_pred\n    \n    def LR_Cost(self, X: pd.DataFrame, y: pd.Series) -> float:\n\n        \"\"\" Return the Mean Squared Error of a linear regression model using vectorised notation. \"\"\"\n\n        # Retrieve feature dimensions\n        m, n = X.shape \n        # Calculate error \n        residual = y - X @ self.theta\n        # Calculate Cost Function \n        CF_LR = (1\/m) * (residual.T @ residual)\n        # Return Cost Function\n        return CF_LR","a84ada47":"# Instantiate model\npop_prof_model = LinearRegression()","cb74fbd8":"# Fit model to training dataset to obtain estimates\npop_prof_model.fit(X_train, y_train)\n# Obtain predictions for training and test dataset\ny_pred_train = pop_prof_model.predict(X_train)\ny_pred_test = pop_prof_model.predict(X_test)\n#\u00a0Calculate MSE and R2 score on training and test dataset\nMSE_train = pop_prof_model.LR_Cost(X_train, y_train)\nMSE_test = pop_prof_model.LR_Cost(X_test, y_test)\nprint(\"Theta estimates are: {}\".format(pop_prof_model.theta))\nprint(\"Training dataset mean squared error: {}\".format(MSE_train))\nprint(\"Test dataset mean squared error: {}\".format(MSE_test))\nprint(\"Train dataset R2 score: {}\".format(r2_score(y_train,y_pred_train)))\nprint(\"Test dataset R2 score: {}\".format(r2_score(y_test,y_pred_test)))","07351bf0":"# Plot test data against predictive model\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Relationship between x and y')\nplt.plot(X_test.iloc[:,1],y_test,'ro', mec= 'k');\nplt.plot(X_test.iloc[:,1],y_pred_test,'-b', mec= 'k');\nplt.legend(['Dataset', 'Linear Regression']);","2d1225a1":"def feature_scaling(X: pd.DataFrame) -> pd.DataFrame:\n    \n    \"\"\" Normalises the features in X (dataframe) and returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1. \"\"\"\n    \n    # Return normalised data\n    return (X - np.mean(X, axis=0))\/np.std(X, axis=0, ddof=0)","a0bd8b86":"def LR_Cost_Alt(X: pd.DataFrame, y: pd.Series, theta: np.array) -> float:\n    \n    \"\"\" Return the Mean Squared Error of a linear regression model using loops. \"\"\"\n    \n    # Retrieve feature dimensions\n    m, n = X.shape \n    # Initialise error\n    residual = 0\n    # Calculate squared error \n    for i in range(m):\n        residual += (y[i] - np.sum(X[:,i] * theta))**2\n    # Calculate mean squared error\n    CF_LR = np.mean(residual)\n    # Return Cost Function\n    return CF_LR","7cfe1bb5":"def plot_data(data: pd.DataFrame):\n    \n    \"\"\" Plot 2-D dataset. \"\"\"\n    \n    # Split data into X inputs and y outcomes (from dataframe)\n    X = data.iloc[:,0]\n    y = data.iloc[:,1]\n    plt.xlabel('$x$')\n    plt.ylabel('$y$')\n    plt.title('Investigating the relationship between ...')\n    plt.plot(X,y,'ro', mec= 'k')","c4f7f490":"Thanks for reading this notebook. If there are any mistakes or things that need more clarity, feel free to respond in the comment section and I will be happy to reply. \n\nAs always, please leave an upvote - it would also be helpful if you cite this documentation if you are going to use any of the code. \ud83d\ude0a\n\n#CodeWithSid ","7eb4cd0f":"## Splitting dataset","b3127242":"# Summary","8c1ee396":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","04e4bfaf":"# Theta\/Co-efficient Estimates","9b049589":"To apply feature scaling on the feature dataset:","1a5404b2":"A value of 1 (positive) suggests to us that there is an exact correlative match between the feature and the target variable and so a linear regression is a perfect fit for this dataset.","fe6a88c4":"<p> <center> This notebook is in <span style=\"color: green\"> <b> Active <\/b> <\/span> state of development! <\/center> <\/p>  \n<p> <center> Be sure to checkout my other notebooks for <span style=\"color: blue\"> <b> knowledge, insight and laughter <\/b> <\/span>! \ud83e\udde0\ud83d\udca1\ud83d\ude02<\/center> <\/p> ","6987a5ed":"### 4.\n\nCombining all the previous notations, we will write the associated model in the following matrix\/vectorised manner:\n    \n\\begin{gather}\n    \\underline{\\boldsymbol{y}}\n    = Y =\n\\begin{bmatrix}\ny^{1} \\\\ \\ y^{2} \\\\ \\dots \\\\  y^{N}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf(\\boldsymbol x^{1}, \\underline{\\boldsymbol{ \\theta}}) \\\\ f(\\boldsymbol x^{2}, \\underline{\\boldsymbol{\\theta}}) \\\\ \\dots \\\\ f(\\boldsymbol x^{N}, \\underline{\\boldsymbol{ \\theta}})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\theta_{0} + \\theta_{1}x_{1}^{1} \\dots \\theta_{p}x_{p}^{1} \\\\\n\\theta_{0} + \\theta_{1}x_{1}^{2} \\dots \\theta_{p}x_{p}^{2} \\\\ \\dots \\\\\n\\theta_{0} + \\theta_{1}x_{1}^{N} \\dots \\theta_{p}x_{p}^{N}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{1}^{1} & x_{2}^{1} & \\dots & x_{p}^{1} \\\\\n1 & x_{1}^{2} & x_{2}^{2} & \\dots & x_{p}^{2} \\\\\n\\dots  & \\dots  & \\dots  & \\dots & \\dots  \\\\\n1 & x_{1}^{N} & x_{2}^{N} & \\dots & x_{p}^{N} \n\\end{bmatrix}\n\\begin{bmatrix}\n\\theta_0 \\\\  \\theta_1 \\\\ \\dots \\\\  \\theta_p\n\\end{bmatrix}\n= X \\underline{\\boldsymbol{ \\theta}} \n\\end{gather}\n\nwhere:\n- $\\boldsymbol{X} \\in \\mathbb{R}^{N \\times (p+1)}$\n- $\\underline{\\boldsymbol{ \\theta}} \\in \\mathbb{R}^{p+1}$\n- $\\underline{\\boldsymbol{ y}} \\in \\mathbb{R}^{N}$\n\n**Implementation Note:** We store each example as a row in the the $X$ matrix. To take into account the intercept term ($\\hat \\theta_0$), we add an additional first column to $X$ and set it to all ones. This allows us to treat $\\hat \\theta_0$ as simply another 'feature'.","bba22cfe":"![image.png](attachment:8307e1ae-af5e-4791-8c54-728624876e60.png)","104086eb":"# Model Testing and Results","e2da8bad":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","275255db":"# Data Collection","8f948dee":"<center> <img src=\"https:\/\/www.mihaileric.com\/static\/linear_regression_joke-9400ea8c70e0500f1934f7a22c86bc68-b75a8.png\" width=\"550\" height=\"550\" \/> <\/center> ","b6660114":"# Aim","b3b94450":"### 2.\n\nFor every data point $x^{i}$, we will have a corresponding *observation value*. Conventionally, we write the following:\n\n\\begin{gather}\n    \\underline{\\boldsymbol y} = \n\\begin{bmatrix}\n  y^1 \\\\ y^2 \\\\ \\dots \\\\ y^N \n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf(\\boldsymbol x^{1}, \\underline{\\boldsymbol \\theta}) \\\\ f(\\boldsymbol x^{2}, \\underline{\\boldsymbol \\theta}) \\\\ \\dots \\\\ f(\\boldsymbol x^{N}, \\underline{\\boldsymbol \\theta})\n\\end{bmatrix}\n\\in \\mathbb{R}^N\n\\qquad \\text{the outcome\/target variable}\n\\end{gather}\n\nwhere,\n\n\\begin{gather}\n    \\underline{\\boldsymbol x^{i}} = \n\\begin{bmatrix} \nx^{i}_{1} \\\\ x^{i}_{2} \\\\ \\dots \\\\ x^{i}_{p}\n\\end{bmatrix}\n\\in \\mathbb{R}^p;\\ \\forall i \\in {1,...,N}\n\\qquad \\text{a feature of the dataset}\n\\end{gather}\n\nSince our aim is to construct a linear regression of $\\boldsymbol{y}$, we can write the observed datapoints as:\n\n$$y^{i} = f_{LR}(\\boldsymbol x^{i}, \\underline{\\boldsymbol{\\theta}})= \\theta_{0} + \\theta_{1}x_{1}^{i} \\dots \\theta_{p}x_{p}^{i}; \\ \\forall i= 1,...,N$$\n\n**Note:** Python treats row vectors and column vectors the same in computation\/dimensions (as they are $1D$ arrays). Hence, this notation is abitrary as we can easily just transpose the column vector and make it into a row vector and rewrite the definition of $\\boldsymbol x^{i}$ i.e. it is by choice how you lay this out but you must make sure to stay consistent in terms of dimension compatibility. \n\nHere, the only unknowns are the $\\underline{\\boldsymbol{\\theta}}$. Hence, our goal is to estimate $\\underline{\\boldsymbol{\\theta}}$ i.e. calculate the estimates $\\underline{\\boldsymbol{\\hat \\theta}}$, which we call our **parameters** of the dataset. ","e3779a03":"To plot datasets that are 2-dimensional, where the first column is the $x$ variable and the second column is the $y$ variable:","4a97314b":"# Full Linear Regression Model","b15d5b70":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","f50b562f":"### 5.\n\nMathematically speaking, our estimated parameters $\\hat \\theta_{j}$ are learned\/observed after the modelling. \n\nOverall, the process is as follows: \n- We will first be working with the data $\\underline{\\boldsymbol{y}}={X\\underline{\\theta}}$\n- Once we solve this matrix multiplication problem, we will have calculated $\\underline{\\boldsymbol{\\hat \\theta}} = \\{\\hat \\theta_{j}; \\forall j \\in \\{1,\\dots,p\\}\\}$ \n- We can then substitute all $\\hat \\theta_{j}'s$ back into $\\underline{\\boldsymbol{y}}={X\\underline{\\theta}}$  to give us $\\underline{\\boldsymbol{\\hat y}} = {X\\underline{\\hat \\theta}}$\n\nWe can measure the quality of our $\\underline{\\boldsymbol{\\hat \\theta}}$ parameters by minimising the total residual error i.e. sum of the difference between predictions and observations, which is $\\underline{\\boldsymbol{y}}-\\underline{\\boldsymbol{\\hat y}} := \\underline{\\boldsymbol{e}} $. (This will later be constructed into a cost function that we will record). ","8687c172":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","b2940e57":"# Data Processing","fea96be2":"<center> <h1>\ud83d\udcc8 Linear Regression \ud83d\udcc8 <\/h1> <\/center> ","e2797fc8":"## Import Modules","4967875d":"# Extra","ca3a1651":"# Background","375f9b78":"# Data Visualisations","f9308fbd":"### 3.\n\nFor every data point, we will also have a *predicted value*. Similarly, we write the following:\n\n\\begin{gather}\n    \\underline{\\hat{\\boldsymbol y}} = \n\\begin{bmatrix}\n  \\hat y^1 \\\\ \\hat y^2 \\\\ \\dots \\\\ \\hat y^N \n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf(\\boldsymbol x^{1}, \\underline{\\boldsymbol{\\hat \\theta}}) \\\\ f(\\boldsymbol x^{2}, \\underline{\\boldsymbol{\\hat \\theta}}) \\\\ \\dots \\\\ f(\\boldsymbol x^{N}, \\underline{\\boldsymbol{\\hat \\theta}})\n\\end{bmatrix}\n\\in \\mathbb{R}^N\n\\qquad \\text{the predicted datapoints of $\\boldsymbol{y}$}\n\\end{gather}\n\nSince our aim is to construct a linear regression of $\\boldsymbol{y}$, we can write the predicted datapoints as:\n\n$$\\hat y^{i} = f_{LR}(\\boldsymbol x^{i}, \\underline{\\boldsymbol{\\hat\\theta}})= \\hat \\theta_{0} + \\hat \\theta_{1}x_{1}^{i} \\dots \\hat \\theta_{p}x_{p}^{i}; \\ \\forall i= 1,...,N$$","dfdaf16d":"An alternative cost function implementation (that uses a for loop):","01961c75":"Finding the least squares solution to the linear cost function\/MSE can be done via multivariable differentiation techniques. Instead, we will look at the equivalent form; that is, using maximum likelihood estimation, we find the parameters $\\boldsymbol\\theta^{\\mathrm{ML}}$ that maximises the likelihood of our linear regression model:\n    \n$$\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol{{\\theta}})\\,\n$$\nWe know that the maximum likelihood estimator is given by:\n    \n$$\n\\boldsymbol{\\theta}^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\, \n$$\n<br>\nwhich yields us the same answers as the derivation from differentiation of the matrix vector MSE form. ","e7ffa020":"Some comments about the code implementations:\n\n1. Here we have created an entire python class for linear regression to make it similar to a machine learning pipeline format. If you would like to just use the functions by themselves, simply take them from the individual sections.\n2. We have only dealt with 1 feature in this dataset (simple linear regression) - a mult-linear regression may not yield the same accuracy. \n3. We have not actually used the cost function as a means to optimise the model - we have simply used the normal equations to obtain our estimates. The cost function is used as a metric as it is equivalent to the MSE. \n3. Two alternative methods to solve linear regression problems are: Gradient Descent (Batch) which uses the cost function as an objective function and Grid Search (Brute Force). Seperate notebooks will be designated to each of these methods and you will see that all these methods will provide the exact\/near same solutions. ","225f94a9":"For most machine learning models, we would like them to have low bias and low variance - that is, the model should perform well on the training set (low bias) and also the test set, alongside with other new random test sets (low variance). Therefore, to test for bias and variance of our model, we shall split the dataset into training and test set. We will not be tuning any hyperparameters (and thus do not need a validation set). \n\nFor these functions, the $X$ dataset (of features) should have a column 1's as the first column to account for the bias term\/intercept co-efficient. Before this occurs, one should check the order of magnitude of the features - if they differ hugely, one must apply feature scaling before calculating the MSE (this is not needed to run the normal equations however), so that our data is somewhat normally distributed.","c4e1c247":"**Note:** Make sure that you do not remove lots of datapoints due to the NaN or missing value issues. It may be better to impute them.","d8b58583":"# Cost Function","57cf0208":"The least squares finds a solution that minimises the mean squred error of our linear regression model. This happens to also be equivalently known as the Linear Regression Cost Function. \n    \n<br>\nThe cost function, is defined as (in non-matrix vector form):\n\n$$ MSE_{Linear} = J({\\boldsymbol \\theta}) = \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat y - y^{(i)}\\right)^2$$\n\nThis can also be written as (in matrix vector form):\n\n$$ MSE_{Linear} = J({\\boldsymbol \\theta}) = \\frac{1}{m}[(y - X {\\boldsymbol \\theta})^T (y - X {\\boldsymbol \\theta})]$$\n\nThus, we seek:\n\n$$\n\\underline{\\boldsymbol{\\hat \\theta}} = \\{\\underline{\\boldsymbol \\theta} : \n\\underset{\\underline{\\boldsymbol\\theta}}{\\text{min}}\\ {J(\\theta)}\\}\n$$","45a6abfd":"The aim is to provide, from scratch, code implementations for linear regression problems. This will involve both the main functions needed to solve a linear regression and some additional utility functions as well.\n\n**Note**: We will not be diving into in-depth exploratory data analysis, feature engineering etc... in these notebooks and so will not be commenting extensively on things such as skewness, kurtosis, homoscedasticity etc...","bce7a9cf":"- The in-sample MSE (training) and out-sample MSE (test) are quite small in value and very close. \n- The out-sample is slightly higher than the in-sample, which may suggest a higher bias of the model as it is has trained\/more suited to the training dataset.\n- The R2 score is extremely high (near 1) for both training and test set which means that the model nearly explains all the variation in the response variable around its mean.","fa105db2":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","6cd54081":"### 1.\n\nFor this topic, we consider a linear regression problem. The reason this is called 'linear' regression is not because it is in the form $y = mx + c$ but because the $\\theta_{N}$ are linear combinations in $x$ i.e. $y = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2}$ is linear but $y = \\theta_{0} + \\theta_{1}x_{1}^{\\theta_{2}}$ is not.\n\nIn general, the data we have will look like:\n\n$$\\{x_{1}^{i}, x_{2}^{i}, \\dots, x_{p}^{i}, y^{i}\\}_{i=1}^N$$\n\n- $p$ = # of features in the dataset.\n- $N$ = # of training inputs. \n\nOur linear regression problem will be of the form (non-vectorised):\n\n$$\ny = \\underline{\\boldsymbol{x}}^T\\underline{\\boldsymbol\\theta} + \\epsilon = f(\\underline{\\boldsymbol x}, \\underline{\\boldsymbol \\theta}) ,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n$$\n\nwhere $\\underline{\\boldsymbol x}\\in\\mathbb{R}^N$ are inputs and $y\\in\\mathbb{R}$ are noisy observations. The parameter vector $\\underline{\\boldsymbol\\theta}\\in\\mathbb{R}^D$ parametrizes the function i.e. this is what we are trying to estimate.","f7cf3bab":"# Regression Predictions"}}