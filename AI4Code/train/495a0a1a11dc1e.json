{"cell_type":{"10dda1f2":"code","6e86aae7":"code","0606098e":"code","0e6d66e8":"code","393b7cbf":"code","a5647c81":"code","23f9f1ed":"code","a6e6e100":"code","930cd76c":"code","1c8c270e":"code","c0a85e67":"code","4dc15150":"code","d0be593c":"code","25c1ddac":"code","1c3fcb54":"code","bbee1181":"code","efa7c382":"code","72c9d838":"code","6f87aeaf":"code","4578c08a":"code","093c98c6":"code","f78fdb3e":"code","1c7aeaf6":"code","8b1bd9c0":"code","dfcf580d":"code","c0a3aee2":"code","c6980d45":"code","9273a999":"code","4eb3b779":"code","dbbf6359":"code","fce29d2e":"code","f67dc4ae":"code","1608dea4":"code","fd764b14":"code","cf1a9276":"code","227d8f76":"code","1477e21a":"code","454af1f2":"code","1a940b11":"code","b889ef81":"code","6a4233bd":"markdown","6e661e34":"markdown","1beb7642":"markdown","a2c762f3":"markdown","390914ae":"markdown","77fd4bdc":"markdown","5eebaace":"markdown","1787aaed":"markdown","72dfc3d2":"markdown","1739e54c":"markdown","ab9bd073":"markdown","84c6a356":"markdown","86fa721a":"markdown","2c7fcaa8":"markdown","17b0f8fa":"markdown","a415dda5":"markdown"},"source":{"10dda1f2":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport optuna\nfrom imblearn.over_sampling import RandomOverSampler","6e86aae7":"train_data = pd.read_csv(\"..\/input\/song-popularity-kfold\/song_popularity_kfold.csv\")\ntest_data = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","0606098e":"train_data.head(15)","0e6d66e8":"train_data.info()","393b7cbf":"train_data.describe()","a5647c81":"def convert_features_positive(data):\n    for feature in data.columns:\n        data[feature] = data[feature].abs()\n    return data","23f9f1ed":"train_data = convert_features_positive(train_data)\ntest_data = convert_features_positive(test_data)","a6e6e100":"train_data.head()","930cd76c":"test_data.head()","1c8c270e":"useful_features = [feature for feature in train_data.columns if feature not in [\"id\",\"song_popularity\",\"KFold\"]]","c0a85e67":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = XGBClassifier( \n        n_estimators= 10000,\n        random_state=0,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  \n        gpu_id=0,\n        predictor='gpu_predictor',\n        n_jobs = -1,\n    )\n    \n    model.fit(x_train, y_train, \n              early_stopping_rounds=300,\n              eval_metric='auc', \n              eval_set=[(x_valid, y_valid)], verbose=1000)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","4dc15150":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    rdsample = RandomOverSampler()\n\n    x_train, y_train = rdsample.fit_resample(x_train, y_train)\n    x_valid, y_valid = rdsample.fit_resample(x_valid, y_valid)\n    \n    model = XGBClassifier( \n        n_estimators= 10000,\n        random_state=0,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  \n        gpu_id=0,\n        predictor='gpu_predictor',\n        n_jobs = -1,\n    )\n    \n    model.fit(x_train, y_train, \n              early_stopping_rounds=300,\n              eval_metric='auc', \n              eval_set=[(x_valid, y_valid)], verbose=1000)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","d0be593c":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = IterativeImputer(random_state=30)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = XGBClassifier( \n        n_estimators= 10000,\n        random_state=0,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  \n        gpu_id=0,\n        predictor='gpu_predictor',\n        n_jobs = -1,\n    )\n    \n    model.fit(x_train, y_train, \n              early_stopping_rounds=300,\n              eval_metric='auc', \n              eval_set=[(x_valid, y_valid)], verbose=1000)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","25c1ddac":"def objective(trial):\n    roc_score = []\n    final_predictions = []\n    for fold in range(5):\n        \n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n        reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n        n_estimators = trial.suggest_int(\"n_estimators\",100,10000)\n        \n        x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n        x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n        x_test = test_data.copy()\n        x_test = x_test.drop(\"id\",axis=1)\n\n        y_train = x_train[\"song_popularity\"]\n        y_valid = x_valid[\"song_popularity\"]\n\n    #     print(x_train.columns)\n    #     print(x_valid.columns)\n    #     print(x_test.columns)\n\n        x_train = x_train[useful_features]\n        x_valid = x_valid[useful_features]\n\n        imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n\n        x_train = imputer.fit_transform(x_train)\n        x_valid = imputer.transform(x_valid)\n        x_test = imputer.transform(x_test)\n\n        scaler =  StandardScaler()\n        x_train = scaler.fit_transform(x_train)\n        x_valid = scaler.transform(x_valid)\n        x_test = scaler.transform(x_test) \n\n        model = XGBClassifier( \n            n_estimators= 10000,\n            random_state=0,\n            use_label_encoder=False,\n            objective='binary:logistic',\n            tree_method='gpu_hist',  \n            gpu_id=0,\n            predictor='gpu_predictor',\n            n_jobs = -1,\n            learning_rate=learning_rate,\n            reg_lambda=reg_lambda,\n            reg_alpha=reg_alpha,\n            subsample=subsample,\n            colsample_bytree=colsample_bytree,\n            max_depth=max_depth\n        )\n\n        model.fit(x_train, y_train, \n                  early_stopping_rounds=300,\n                  eval_metric='auc', \n                  eval_set=[(x_valid, y_valid)], verbose=1000)\n\n\n        preds_valid = model.predict_proba(x_valid)[:,1]\n        roc_score.append(roc_auc_score(y_valid,preds_valid))\n        \n    return np.mean(roc_score)","1c3fcb54":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)","bbee1181":"best_param_xgboost = study.best_params","efa7c382":"final_predictions = []\nroc_score = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = XGBClassifier( \n        random_state=0,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  \n        gpu_id=0,\n        predictor='gpu_predictor',\n        n_jobs = -1,\n        **best_param_xgboost\n    )\n    \n    model.fit(x_train, y_train, \n              early_stopping_rounds=300,\n              eval_metric='auc', \n              eval_set=[(x_valid, y_valid)], verbose=1000)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)\n    roc_score.append(roc_auc_score(y_valid,preds_valid))\n        \nprint(f\"mean roc_auc_score is {np.mean(roc_score)} and std of roc_auc_score is {np.std(roc_score)}\")","72c9d838":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = KNNImputer()\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = XGBClassifier( \n        n_estimators= 10000,\n        random_state=0,\n        use_label_encoder=False,\n        objective='binary:logistic',\n        tree_method='gpu_hist',  \n        gpu_id=0,\n        predictor='gpu_predictor',\n        n_jobs = -1,\n    )\n    \n    model.fit(x_train, y_train, \n              early_stopping_rounds=300,\n              eval_metric='auc', \n              eval_set=[(x_valid, y_valid)], verbose=1000)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)\n    ","6f87aeaf":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer()\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = RandomForestClassifier(n_estimators=1000)\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","4578c08a":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = CatBoostClassifier(random_state=42,\n                                      verbose=0,\n                                      task_type=\"GPU\",\n                                      devices='0:1')\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","093c98c6":"def objective(trial):\n    final_predictions = []\n    roc_score = []\n    for fold in range(5):\n        #declaring heyper parameters\n        param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"l2_leaf_reg\" : trial.suggest_int(\"l2_leaf_reg\",1,5),\n        \"min_child_samples\" : trial.suggest_categorical(\"min_child_samples\",[1,4,8,16,30]),\n        \"eval_metric\" : \"CrossEntropy\",\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n        \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n        if param[\"bootstrap_type\"] == \"Bayesian\":\n            param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n        elif param[\"bootstrap_type\"] == \"Bernoulli\":\n            param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n        x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n        x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n        x_test = test_data.copy()\n        x_test = x_test.drop(\"id\",axis=1)\n\n        y_train = x_train[\"song_popularity\"]\n        y_valid = x_valid[\"song_popularity\"]\n\n    #     print(x_train.columns)\n    #     print(x_valid.columns)\n    #     print(x_test.columns)\n\n        x_train = x_train[useful_features]\n        x_valid = x_valid[useful_features]\n\n        imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n\n        x_train = imputer.fit_transform(x_train)\n        x_valid = imputer.transform(x_valid)\n        x_test = imputer.transform(x_test)\n\n        scaler =  StandardScaler()\n        x_train = scaler.fit_transform(x_train)\n        x_valid = scaler.transform(x_valid)\n        x_test = scaler.transform(x_test) \n\n        model = CatBoostClassifier(random_state=42,\n                                          verbose=0,\n#                                           task_type=\"GPU\",\n#                                           devices='0:1',\n                                          **param)\n\n        model.fit(x_train, y_train)\n\n\n        preds_valid = model.predict_proba(x_valid)[:,1]\n        roc_score.append(roc_auc_score(y_valid,preds_valid))\n        \n    return np.mean(roc_score)","f78fdb3e":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)","1c7aeaf6":"catbost_best_params = study.best_params","8b1bd9c0":"final_predictions = []\nroc_score = []\nfor fold in range(5):\n    \n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer(missing_values=np.nan,strategy=\"constant\",fill_value=-999)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    \n    model = CatBoostClassifier(random_state=42,\n                                      verbose=0,\n                              **catbost_best_params)\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    roc_score.append(roc_auc_score(y_valid,preds_valid))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)\nprint(f\"mean roc auc score is : {np.mean(roc_score)}\")","dfcf580d":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = SimpleImputer()\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    rdsample = RandomOverSampler()\n\n    x_train, y_train = rdsample.fit_resample(x_train, y_train)\n    x_valid, y_valid = rdsample.fit_resample(x_valid, y_valid)\n    \n    model = CatBoostClassifier(random_state=42,\n                                      verbose=0,\n                                      task_type=\"GPU\",\n                                      devices='0:1')\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","c0a3aee2":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = IterativeImputer(random_state=30)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = CatBoostClassifier(random_state=42,\n                                      verbose=0,\n                                      task_type=\"GPU\",\n                                      devices='0:1')\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","c6980d45":"def objective(trial):\n    final_predictions = []\n    roc_score = []\n    for fold in range(5):\n        #declaring heyper parameters\n        param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"min_child_samples\" : trial.suggest_categorical(\"min_child_samples\",[1,4,8,16,30]),\n        \"eval_metric\" : \"CrossEntropy\",\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n        \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"used_ram_limit\": \"3gb\",\n        'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n    }\n\n        if param[\"bootstrap_type\"] == \"Bayesian\":\n            param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n        elif param[\"bootstrap_type\"] == \"Bernoulli\":\n            param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n        x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n        x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n        x_test = test_data.copy()\n        x_test = x_test.drop(\"id\",axis=1)\n\n        y_train = x_train[\"song_popularity\"]\n        y_valid = x_valid[\"song_popularity\"]\n\n    #     print(x_train.columns)\n    #     print(x_valid.columns)\n    #     print(x_test.columns)\n\n        x_train = x_train[useful_features]\n        x_valid = x_valid[useful_features]\n\n        imputer = IterativeImputer(random_state=30)\n\n        x_train = imputer.fit_transform(x_train)\n        x_valid = imputer.transform(x_valid)\n        x_test = imputer.transform(x_test)\n\n        scaler =  StandardScaler()\n        x_train = scaler.fit_transform(x_train)\n        x_valid = scaler.transform(x_valid)\n        x_test = scaler.transform(x_test) \n\n        model = CatBoostClassifier(random_state=42,\n                                          verbose=0,\n#                                           task_type=\"GPU\",\n#                                           devices='0:1',\n                                          **param)\n\n        model.fit(x_train, y_train)\n\n\n        preds_valid = model.predict_proba(x_valid)[:,1]\n        roc_score.append(roc_auc_score(y_valid,preds_valid))\n        \n    return np.mean(roc_score)","9273a999":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)","4eb3b779":"new_catboost_params = study.best_params","dbbf6359":"final_predictions = []\nroc_score = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    x_train = x_train[useful_features]\n    x_valid = x_valid[useful_features]\n    \n    imputer = IterativeImputer(random_state=30)\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = CatBoostClassifier(random_state=42,\n                                      verbose=0,\n#                                       task_type=\"GPU\",\n#                                       devices='0:1',\n                              **new_catboost_params)\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    roc_score.append(roc_auc_score(y_valid,preds_valid))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)\n    \nprint(np.mean(roc_score))","fce29d2e":"model.feature_importances_","f67dc4ae":"temp_df = pd.DataFrame({\"features names\" : train_data[useful_features].columns,\n              \"feature importance score\" :model.feature_importances_ }).sort_values(by=\"feature importance score\",ascending=False)","1608dea4":"temp_df","fd764b14":"# only considering the features which have score more than 5\nnew_useful_features = temp_df.set_index(keys=\"features names\")[\"feature importance score\"][:-3].index","cf1a9276":"new_useful_features = np.array(new_useful_features)\nnew_useful_features","227d8f76":"final_predictions = []\nfor fold in range(5):\n    x_train = train_data[train_data.KFold != fold].reset_index(drop=True)\n    x_valid = train_data[train_data.KFold == fold].reset_index(drop=True)\n    x_test = test_data.copy()\n    x_test = x_test.drop(\"id\",axis=1)\n    \n    y_train = x_train[\"song_popularity\"]\n    y_valid = x_valid[\"song_popularity\"]\n    \n    x_train = x_train[new_useful_features]\n    x_valid = x_valid[new_useful_features]\n    x_test  = x_test[new_useful_features]\n\n    \n#     print(x_train.columns)\n#     print(x_valid.columns)\n#     print(x_test.columns)\n    \n    imputer = SimpleImputer()\n    \n    x_train = imputer.fit_transform(x_train)\n    x_valid = imputer.transform(x_valid)\n    x_test = imputer.transform(x_test)\n    \n    scaler =  StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test = scaler.transform(x_test) \n    \n    model = CatBoostClassifier(random_state=42,\n                                      verbose=0,\n#                                       task_type=\"GPU\",\n#                                       devices='0:1',\n                                      **catboost_best_params)\n    \n    model.fit(x_train, y_train)\n    \n    \n    preds_valid = model.predict_proba(x_valid)[:,1]\n    print(\"The ROC score after {} fold is {}\".format(fold,roc_auc_score(y_valid,preds_valid)))\n    test_preds = model.predict_proba(x_test)[:,1]\n    final_predictions.append(test_preds)","1477e21a":"target = np.mean(np.column_stack(final_predictions), axis=1)\n\nids = test_data[\"id\"]","454af1f2":"output_data = pd.DataFrame({\"id\":ids,\"song_popularity\":target})","1a940b11":"output_data","b889ef81":"output_data.to_csv(\"submission10.csv\",index=False)","6a4233bd":"# Again Training Catboost with best parameters","6e661e34":"# Finding Best Parameters for XGBoost using Optuna","1beb7642":"# Finding best features for the catboost and again train the model with these features and updated params","a2c762f3":"## Found removing low scoring features decreases the score of the model","390914ae":"# Training CatBoost with SimpleImputer and RandomSampler","77fd4bdc":"# Model-1 :  XGB with Simple Imputer","5eebaace":"# Model-2 : XGB with KNN Imputer","1787aaed":"# Model-3 : Random Forest with KNN Imputer","72dfc3d2":"# Training CatBoost with Iterative Imputer","1739e54c":"# Again training the XGB with best parameters","ab9bd073":"# Training XGBoost with SimpleImputer and RandomOverSampler","84c6a356":"# Training on CatBoost Model","86fa721a":"# Training XGBoost with iterative imputer","2c7fcaa8":"# Again Training Catboost with best params and Simple Imputer","17b0f8fa":"# Finding Best Parameters using optuna for CatBoost as we are now also using iterative imputer","a415dda5":"# finding Catboost best parameters using optuna using simple imputer "}}