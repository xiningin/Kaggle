{"cell_type":{"6f330187":"code","8d25283e":"code","c096ef7e":"code","27fa6802":"code","794838b5":"code","4cf64bdb":"code","6b73bc18":"code","5673465d":"code","45469a12":"code","5569e97e":"code","9cfc9886":"code","3390a547":"code","afb6a2a3":"code","26c7d15b":"code","14c44d77":"code","8df3de45":"code","a213ad55":"code","8e136ce9":"code","5a93e93e":"code","edd5a44e":"code","e934892c":"code","0b64eb17":"code","d313440d":"code","181a9129":"code","eb278d6b":"code","57da713d":"code","d37cfe05":"code","327433fa":"code","63684fad":"code","8758f37c":"code","16b3ccbf":"code","428452ac":"code","f26bfe10":"code","92dd1cb4":"code","30186453":"code","d2cb8391":"code","a834ce82":"code","b314d1ab":"code","1a0ae4bf":"code","1387c006":"code","5cd6efee":"code","ab024f5e":"code","087fb253":"code","0131a745":"code","85329555":"code","24523a92":"code","ff1526e6":"code","43d7829c":"code","98662f79":"code","bd9cf4e2":"code","1bbcf53e":"code","95eb63ca":"code","6340e61c":"code","545ce415":"code","46a13978":"code","a89bc1f1":"code","0907a58a":"code","33e3b29f":"code","f745d62c":"code","b7c4356d":"code","da5747d7":"code","9f92ea9f":"code","3cdd06e0":"code","94991f07":"markdown","7864d824":"markdown","e4daca77":"markdown","162a3da8":"markdown","6bc951c5":"markdown","a96e1f53":"markdown","c4556dbc":"markdown","d5526abf":"markdown","27cc34bd":"markdown","eaa3bc0e":"markdown","792f043a":"markdown","010199c0":"markdown","cbd6ffc0":"markdown","5cbac2ac":"markdown","1b6cc457":"markdown","aa8d9bb4":"markdown","38be5e00":"markdown","b6089398":"markdown","25d3a73d":"markdown","cc934290":"markdown","58145589":"markdown","ecad4ddb":"markdown","08757343":"markdown","bf2cecde":"markdown"},"source":{"6f330187":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rc('figure', figsize=(10,10))","8d25283e":"# Importing the data\ndata = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')","c096ef7e":"# Understanding the properties of the columns\ndata.describe()","27fa6802":"data.info()","794838b5":" # Splitting the data in 3:1 Ratio for Training and Testing\nfrom sklearn.model_selection import train_test_split\nx_data = data[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny_data = data[['Species']]\nx_train,x_test, y_train, y_test = train_test_split(x_data,y_data,test_size = 0.33, random_state = 0)","4cf64bdb":"# Correlation of various values","6b73bc18":"data_tr = pd.concat([x_train,y_train], axis = 1)\nsns.heatmap(data_tr.corr())","5673465d":"# The corr function does not consider the Species column as its non-numeric","45469a12":"# Understanding the categorical values\n\nfigure , ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\nsns.violinplot('Species','SepalLengthCm', data = data_tr\n            ,ax = ax1\n            ,fliersize = 7)\nsns.violinplot('Species','SepalWidthCm', data = data_tr\n            ,ax = ax2\n           ,fliersize = 7)\nsns.violinplot('Species','PetalLengthCm', data = data_tr\n            ,ax = ax3\n           ,fliersize = 7)\nsns.violinplot('Species','PetalWidthCm', data = data_tr\n            ,ax = ax4\n           ,fliersize = 7)","5569e97e":"# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\ndata_tr_speciesCat = data_tr.copy()\nlb_species = LabelEncoder()\ndata_tr_speciesCat['Species'] = lb_species.fit_transform(data_tr_speciesCat['Species'])\ndata_tr_speciesCat.head()","9cfc9886":"plt.rc('figure'\n       , figsize=(20,10))\nsns.heatmap(data_tr_speciesCat.corr()\n            ,cmap = 'Greens')","3390a547":"# But this label encoding will provide weightd to different categories","afb6a2a3":"#One Hot Encoding\n# Trying out One Hot Encoding to see how the corr gets affected","26c7d15b":"data_tr_OHE = data_tr.copy()\ndata_tr_OHE = pd.get_dummies(data_tr_OHE\n                             ,columns = ['Species']\n                             ,prefix = ['Species'])\ndata_tr_OHE.head()","14c44d77":"corr_df = data_tr_OHE.corr()\ncorr_df.drop(['Species_Iris-setosa','Species_Iris-versicolor','Species_Iris-virginica']\n             ,axis = 1\n             ,inplace = True)\ncorr_df.drop(['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\n             ,axis = 0\n             ,inplace = True)\ncorr_df","8df3de45":"sns.heatmap(corr_df\n            ,vmin = -1\n            ,annot = True\n            ,cmap = 'BuPu')","a213ad55":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nX = data_tr_speciesCat[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\nY = data_tr_speciesCat[['Species']]\nlm.fit(X,Y)\nprint('Co-effecient: ',lm.coef_)\nprint('Intercept: ',lm.intercept_)","8e136ce9":"# Predicting values from this model\nyhat = lm.predict(X)","5a93e93e":"ax1 = sns.distplot(data_tr_speciesCat[['Species']], hist = False, color = 'r', label = 'Actual Value')\nsns.distplot(yhat, hist = False, color = 'b', label = 'Fitted Value' , ax = ax1)","edd5a44e":"from sklearn.metrics import mean_squared_error\nprint('MSE :',mean_squared_error(Y,yhat))\nprint('R-Squared :',lm.score(X,Y))","e934892c":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree = 3)\nx_poly = poly_features.fit_transform(X)\npoly_lm = LinearRegression()\npoly_lm.fit(x_poly,Y)","0b64eb17":"print('Co-effecient: ',poly_lm.coef_)\nprint('Intercept: ',poly_lm.intercept_)","d313440d":"# Predicting values uising this model\ny_poly_predict = poly_lm.predict(x_poly)","181a9129":"ax1 = sns.distplot(data_tr_speciesCat[['Species']], hist = False, color = 'r', label = 'Actual Value')\nsns.distplot(y_poly_predict, hist = False, color = 'b', label = 'Fitted Value' , ax = ax1)","eb278d6b":"y_poly_predict[:5]","57da713d":"# Plotting the Predicted values against the actual values\npolyReg_df = pd.DataFrame({})\npolyReg_df['Actual Values'] = data_tr_speciesCat['Species']\npolyReg_df['Predicted Values'] = y_poly_predict\nplt.scatter(polyReg_df['Actual Values'],polyReg_df.index,label = 'Actual Values')\nplt.scatter(polyReg_df['Predicted Values'],polyReg_df.index,label = 'Predicted Values')\nplt.legend()","d37cfe05":"polyReg_round = polyReg_df.copy()\npolyReg_round['Predicted Values'] = np.absolute(np.round(polyReg_df['Predicted Values'],0))\npolyReg_round.head()","327433fa":"ax1 = sns.distplot(polyReg_round['Actual Values'], hist = False, color = 'r', label = 'Actual Value')\nsns.distplot(polyReg_round['Predicted Values'], hist = False, color = 'b', label = 'Fitted Value' , ax = ax1)","63684fad":"print('MSE (non-rounded):',mean_squared_error(Y,y_poly_predict))\nprint('MSE (rounded):',mean_squared_error(Y,polyReg_round['Predicted Values']))\nprint('R-Squared :',poly_lm.score(x_poly,Y))","8758f37c":"# Instead of using the Rounding Off, let's try clustering for the same","16b3ccbf":"from sklearn.cluster import KMeans \nfrom sklearn import metrics \nfrom scipy.spatial.distance import cdist ","428452ac":"# Getting the optimum cluster value\nX = polyReg_df\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \n  \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(X)     \n      \n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ X.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) \/ X.shape[0] \n    mapping2[k] = kmeanModel.inertia_ \n","f26bfe10":"plt.plot(K, distortions, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Distortion') \nplt.title('The Elbow Method using Distortion') \nplt.show() ","92dd1cb4":"# The elbow is at 3\nknn_mod = KMeans(n_clusters = 3)\nknn_mod.fit(polyReg_df)","30186453":"knn_df = polyReg_df.copy()\nknn_df['Clusters'] = knn_mod.labels_\nknn_df['Clusters'].replace({2:1,1:2},inplace = True)","d2cb8391":"knn_df.head()","a834ce82":"plt.scatter(knn_df['Predicted Values'],knn_df.index,c = knn_df['Clusters'],label = 'Predicted Values')\nplt.scatter(knn_df['Actual Values'],knn_df.index,c = 'black', label = 'Actual Values')\n# plt.scatter(df_temp['y_rounded'],df_temp.index,marker = 'x')\nplt.legend()","b314d1ab":"print('MSE :',mean_squared_error(knn_df['Actual Values'],knn_df['Clusters']))","1a0ae4bf":"print('---------------------------------------------------------------------------------------------------------')\nprint(x_test.head())\nprint('---------------------------------------------------------------------------------------------------------')\nprint(y_test.head())\nprint('---------------------------------------------------------------------------------------------------------')","1387c006":"# Encoding the Test Target Data\ny_test_cat = y_test.copy()\nlb_species = LabelEncoder()\ny_test_cat['Species'] = lb_species.fit_transform(y_test_cat['Species'])\ny_test_cat.head()","5cd6efee":"# Fitting Polynomial Features for the Test X data\nx_poly_test = poly_features.fit_transform(x_test)","ab024f5e":"y_poly_test = poly_lm.predict(x_poly_test)","087fb253":"y_val_df = y_test_cat.copy()\ny_val_df['y_predicted'] = y_poly_test","0131a745":"ax1 = sns.distplot(y_test_cat, hist = False, color = 'r', label = 'Actual Value')\nsns.distplot(y_poly_test, hist = False, color = 'b', label = 'Fitted Value' , ax = ax1)","85329555":"plt.scatter(y_val_df['Species'],y_val_df.index,label = 'Predicted Values')\nplt.scatter(y_val_df['y_predicted'],y_val_df.index, label = 'Actual Values')\n# plt.scatter(df_temp['y_rounded'],df_temp.index,marker = 'x')\nplt.legend()","24523a92":"knn_test_mod = KMeans(n_clusters=3)\nknn_test_mod.fit(y_val_df[['y_predicted']])\nknn_pred_df = y_val_df.copy()\nknn_pred_df['Clusters'] = knn_test_mod.labels_","ff1526e6":"plt.scatter(knn_pred_df['Species'],knn_pred_df.index,label = 'Predicted Values')\nplt.scatter(knn_pred_df['y_predicted'],knn_pred_df.index,c = knn_pred_df['Clusters'], label = 'Actual Values')\n# plt.scatter(df_temp['y_rounded'],df_temp.index,marker = 'x')\nplt.legend()","43d7829c":"sns.scatterplot(data=knn_pred_df, x='Species', y='y_predicted', hue='Clusters')","98662f79":"group_data = knn_pred_df.groupby(['Species','Clusters'])\n# for key in gb.groups.keys():\n#     print(key,':',gb.get_group(key).count().values[0])","bd9cf4e2":"# Getting unique values\nspecies_vals = knn_pred_df['Species'].unique()\nclusters_vals = knn_pred_df['Clusters'].unique()","1bbcf53e":"max_vals = {}\nfor key in group_data.groups.keys():\n    max_vals[key[0]] = 0\n\nmap_dict = max_vals.copy()\n\nfor key in group_data.groups.keys():\n#     print(group_data.get_group(key).count().values[0])\n    if max_vals[key[0]] < group_data.get_group(key).count().values[0]:\n        max_vals[key[0]] = group_data.get_group(key).count().values[0]\n        map_dict[key[0]] = key","95eb63ca":"mapping = {}\nfor tup in list(map_dict.values()):\n    mapping[tup[1]] = tup[0]\nmapping","6340e61c":"knn_pred_df['Clusters'].replace(mapping,inplace = True)","545ce415":"sns.scatterplot(data=knn_pred_df, x='Species', y='y_predicted', hue='Clusters')","46a13978":"ax1 = sns.distplot(knn_pred_df['Clusters'], hist = False, color = 'b', label = 'Fitted Value' ,kde_kws=dict(linewidth=5,shade = True,alpha = 0.5))\nsns.distplot(knn_pred_df['Species'], hist = False, color = 'r', label = 'Actual Value', ax = ax1,kde_kws=dict(linewidth=2))","a89bc1f1":"print('MSE :',mean_squared_error(knn_pred_df['Species'],knn_pred_df['Clusters']))","0907a58a":"# Checking the MSE if we used the Rounding Off logic\nprint('MSE :',mean_squared_error(knn_pred_df['Species'],np.absolute(np.round(knn_pred_df['Clusters'],0))))","33e3b29f":"ax1 = sns.distplot(np.absolute(np.round(knn_pred_df['Clusters'],0)), hist = False, color = 'b', label = 'Fitted Value' ,kde_kws=dict(linewidth=5,shade = True,alpha = 0.5))\nsns.distplot(knn_pred_df['Species'], hist = False, color = 'r', label = 'Actual Value', ax = ax1,kde_kws=dict(linewidth=2))","f745d62c":"from sklearn.metrics import accuracy_score","b7c4356d":"acc_score = accuracy_score(knn_pred_df['Species'],knn_pred_df['Clusters'])","da5747d7":"acc_score","9f92ea9f":"from sklearn.metrics import confusion_matrix\nconfusion_mat = pd.DataFrame(confusion_matrix(knn_pred_df['Species'],knn_pred_df['Clusters']),index = [0,1,2], columns = [0,1,2])","3cdd06e0":"confusion_mat","94991f07":"<h3>Distibution plots between the Actual Values and the Predicted Values","7864d824":"**Defining the mapping for the cluster values based on the above Scatter plot to be consistent to the Species label encoding**","e4daca77":"Fitted value behaves a lot differently from the actual values\n<br>\nThe Actual Values display a distribution of degree 3","162a3da8":"<h3> Checking the distribution plot for the predicted and actual values","6bc951c5":"<h2> Polynomial Regression + Multiple Linear Regression","a96e1f53":"Plotting the scatter plots for the Actual Values and the Predicted Values","c4556dbc":"---","d5526abf":"<h> <b>Working on the training data <\/h1>","27cc34bd":"Applying the clustering for this data","eaa3bc0e":"<h1> Working on Test Data","792f043a":"<b>Implications:<\/b>\n<br>\nWhen the predicted values are rounded off, we get 0 error\n<br>\nR-Squared for this model is better then the previous","010199c0":"Looks like the values are fractions since we used Polynomial Regression","cbd6ffc0":"Above distibution graphs display an almost perfect model for trained data\n<br>\nLet's see how the predicted data looks like","5cbac2ac":"<h3>Distibution plots between the Actual Values and the Predicted Values","1b6cc457":"<h2>Polynomial Regression<\/h2>","aa8d9bb4":"Clusters define the section better\n<br>\nLet's try the same 2 methods on the Test data","38be5e00":"1. Iris-setosa : Correlation to all variables\n2. Iris-versicolor : Correlation is not so strong with other variables except for Sepal Width\n3. Iris-virginica : Correlation with Sepal Length, Petal Length and Petal Width","b6089398":"<h3>Both the plots now are in perfect sync","25d3a73d":"Both Clustering and Rounding Off the value provides same results","cc934290":"<h1> Training a Model <\/h1>\n<br>\n<h2>Multiple Linear Regression<\/h2>","58145589":"Plotting the distribution plots again","ecad4ddb":"<h1><center> ACCURACY SCORE : 96%","08757343":"The MSE and R-Squared above are pretty good\n<br>\nBut we can do better","bf2cecde":"Predicted Values are scattered around the Actual Values due to the former being fractions\n<br>The Predicted Values remain within the boundaries of 1 Unit so they might do good when rounded off"}}