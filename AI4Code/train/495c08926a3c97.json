{"cell_type":{"d238915c":"code","794f4846":"code","9822522f":"code","4603f2a4":"code","aa5695d6":"code","fc710986":"code","d39a638c":"code","4df82a0c":"code","d21464e1":"code","d663a16a":"code","f675cd55":"code","281ff2fa":"code","a96bd1e6":"code","28148090":"code","d5e77004":"code","4bae5fda":"code","4d97578d":"code","5a4a054b":"code","9e15dcbe":"code","abbd6e2e":"code","95b5b9f1":"code","48a9d2c8":"code","ebb229ed":"code","b218df6d":"code","17f4f8cc":"code","c7994bcd":"code","4d39530d":"code","8cdff869":"code","35a1e44f":"code","34721284":"code","ba8066cb":"code","2b26ae5b":"code","63610d04":"code","eb1feef2":"code","0af1f235":"code","382aefa8":"code","068812b7":"code","02d06551":"code","b8057f52":"code","2d3fddf7":"code","5631e7b7":"markdown","7ffe1401":"markdown","f737263a":"markdown","6bc3af59":"markdown","54616dff":"markdown","378b8a23":"markdown","f36ed144":"markdown"},"source":{"d238915c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","794f4846":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nsimplefilter(action='ignore', category=DeprecationWarning)\n\n# load data\ndata = pd.read_csv('..\/input\/predicting-a-pulsar-star\/pulsar_stars.csv')\ndata.head()","9822522f":"# concise way to display stats in the data including nulls and skewness\ndef data_stats(df):\n    lines = df.shape[0]\n    d_types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    unique = df.apply(lambda x: x.unique().shape[0])\n    nulls = df.isnull().sum()\n    missing_ratio = (df.isnull().sum()\/lines)*100\n    skewness = df.skew()\n    col_names = ['dtypes', 'counts', 'unique', 'nulls', 'missing_ratio', 'skewness']\n    temp = pd.concat([d_types, counts, unique, nulls, missing_ratio, skewness], axis=1)\n    temp.columns = col_names\n    return temp\n\n\nstats = data_stats(data)\nstats","4603f2a4":"col_names = ['mean_IP', 'std_IP', 'kurt_IP', 'skew_IP', 'mean_DMSNR', 'std_DMSNR', 'kurt_DMSNR', 'skew_DMSNR', 'target_class']\ndata.columns = col_names","aa5695d6":"data.describe()","fc710986":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data['target_class'])","d39a638c":"# pairplot\nsns.pairplot(data, \n             vars=['mean_IP', 'std_IP', 'kurt_IP', 'skew_IP', 'mean_DMSNR', 'std_DMSNR', 'kurt_DMSNR', 'skew_DMSNR'],\n             hue='target_class')","4df82a0c":"# heatmap\nfig = plt.figure(figsize=(12, 10))\nsns.heatmap(data.corr(), annot=True)","d21464e1":"# plot distributions\ndef plot_dists(df, col_1, col_2='target_class'):\n    fig, axis = plt.subplots(1, 2, figsize=(16, 5))\n    \n    sns.distplot(df[col_1], ax=axis[0])\n    axis[0].set_title('distribution of {}. Skewness = {:.4f}'.format(col_1 ,df[col_1].skew()))\n    \n    sns.violinplot(x=col_2, y=col_1, data=data, ax=axis[1], inner='quartile')\n    axis[1].set_title('violin of {}, split by target'.format(col_1))\n    plt.show()\n    \nfor col in data.columns[:-1]:\n    plot_dists(data, col)","d663a16a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.base import clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","f675cd55":"# score function\ndef cv_score(model, features, label, folds):\n    cv=KFold(n_splits=folds, shuffle=True)\n    cv_estimate = cross_val_score(model, features, label, cv=cv, scoring='roc_auc', n_jobs=4)\n    mean = np.mean(cv_estimate)\n    std = np.std(cv_estimate)\n    return mean, std","281ff2fa":"# set up dataframe for estimator evaluation\nindx_estimators = ['LogisticRegression', 'SVC', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 'MPLClassifier']\ncolumn_names = ['roc_auc_mean', 'roc_auc_std']\nresults = pd.DataFrame(columns=column_names, index=indx_estimators)","a96bd1e6":"# initialize models with balanced class_weight as the target is unbalanced\nlog_mod = LogisticRegression(class_weight='balanced')\nsvc_mod = SVC(probability=True, class_weight='balanced')\nada_mod = AdaBoostClassifier()\ngbc_mod = GradientBoostingClassifier()\nrfc_mod = RandomForestClassifier(class_weight='balanced')\nmlp_mod = MLPClassifier()","28148090":"# We need to transform and scale the skewed data. because there is over 1000 samples we can use the quantile transformer.\nfrom sklearn.preprocessing import QuantileTransformer\nX = data[['mean_IP', 'std_IP', 'kurt_IP', 'skew_IP', 'mean_DMSNR', 'std_DMSNR', 'kurt_DMSNR', 'skew_DMSNR']].values\ny = data['target_class'].values\n\nX = QuantileTransformer(output_distribution='normal').fit_transform(X)\n\nprint('Features shape: {}'.format(X.shape))\nprint('Label shape   : {}'.format(y.shape))","d5e77004":"models = [log_mod, svc_mod, ada_mod, gbc_mod, rfc_mod, mlp_mod]\nfor name, mod in zip(indx_estimators, models):\n    mean, std = cv_score(mod, X, y, 10)\n    results.loc[name,'roc_auc_mean'] = mean\n    results.loc[name, 'roc_auc_std'] = std\n    \nresults","4bae5fda":"rf = RandomForestClassifier()\nrf.fit(data[data.columns[:-1]].values, data['target_class'].values)\n\nfeature_import = pd.Series(rf.feature_importances_, index=data.columns[:-1]).sort_values(ascending=False)\nfeature_import\n\nfig = plt.figure(figsize=(18, 8))\nsns.barplot(x=feature_import, y=feature_import.index)\nplt.title('Feature importances')\nplt.xlabel('Score')\nplt.show()","4d97578d":"from sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn import metrics","5a4a054b":"# model comparison dataframe on test data\nindex_cols = ['LogisticRegression', 'SVC', 'AdaBoostClassifier', 'GradientBoosting', 'RandomForestClassifier', 'MPLClassifier']\ncolnames =['roc_auc', 'accuracy', 'precision', 'recall']\nresults_final = pd.DataFrame(columns=colnames, index=index_cols)","9e15dcbe":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)","abbd6e2e":"log_mod = LogisticRegression(class_weight='balanced')\nlog_mod.get_params()","95b5b9f1":"cv=KFold(n_splits=10, shuffle=True)\nparam_grid={'C':[0.1, 1, 10, 100, 1000]}\n\ngrid_lin_mod = GridSearchCV(estimator=log_mod,\n                           param_grid=param_grid,\n                           scoring='roc_auc',\n                           cv=cv, \n                           return_train_score=True,\n                           n_jobs=4,\n                           verbose=1)\n\ngrid_lin_mod.fit(X, y)\nlin_mod_best = grid_lin_mod.best_estimator_\nprint('GridSearchCV Best Score: {:.4f}'.format(grid_lin_mod.best_score_))\nprint('\\nTuned HyperParameter       value')\nprint('C                           {}'.format(grid_lin_mod.best_estimator_.C))","48a9d2c8":"def score_model(probs, threshold):\n    return np.array([1 if x >= threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    mets = metrics.precision_recall_fscore_support(labels, scores)\n    conf = metrics.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    {:6d}             {:6d}'.format(conf[0,0], conf[0,1]))\n    print('Actual negative    {:6d}             {:6d}'.format(conf[1,0], conf[1,1]))\n    print('')\n    print('Accuracy        {:.4f}'.format(metrics.accuracy_score(labels, scores)))\n    print('AUC             {:.4f}'.format(metrics.roc_auc_score(labels, probs[:,1])))\n    print('Macro precision {:.4f}'.format(float((float(mets[0][0]) + float(mets[0][1]))\/2.0)))\n    print('Macro recall    {:.4f}'.format(float((float(mets[1][0]) + float(mets[1][1]))\/2.0)))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   {:6d}         {:6d}'.format(mets[3][0], mets[3][1]))\n    print('Precision  {:.4f}         {:.4f}'.format(mets[0][0], mets[0][1]))\n    print('Recall     {:.4f}         {:.4f}'.format(mets[1][0], mets[1][1]))\n    print('F1         {:.4f}         {:.4f}'.format(mets[2][0], mets[2][1]))\n\ndef plot_auc(labels, probs):\n    fpr, tpr, threshold = metrics.roc_curve(labels, probs[:,1])\n    auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'blue', label = 'AUC = {:.4f}'.format(auc))\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n","ebb229ed":"lin_mod_best.fit(X_train, y_train)\nprobabilities = lin_mod_best.predict_proba(X_test)\n\n# add to results_final\nscore = score_model(probabilities, .5)\nresults_final.loc['LogisticRegression',:] = [metrics.roc_auc_score(y_test, probabilities[:,1]), \n                                            metrics.accuracy_score(y_test, score), \n                                            metrics.precision_score(y_test, score),\n                                            metrics.recall_score(y_test, score)]\n\nprint_metrics(y_test, probabilities, .5)      \nplot_auc(y_test, probabilities)","b218df6d":"svc_mod = SVC(probability=True, class_weight='balanced')\nsvc_mod.get_params()","17f4f8cc":"cv=KFold(n_splits=3, shuffle=True)\nparam_grid = [{'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear']},\n            {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1.0\/50.0, 1.0\/200.0, 1.0\/500.0, 1.0\/1000.0], 'kernel': ['rbf']}]\n\nsvc_class_grid = GridSearchCV(estimator=svc_mod,\n                           param_grid=param_grid,\n                           cv=cv,\n                           scoring='roc_auc',\n                           return_train_score=True,\n                           n_jobs=4,\n                           verbose=1)\n\nsvc_class_grid.fit(X, y)\nsvc_best_params = svc_class_grid.best_estimator_\nprint('GridsearchCV Best Score: ' + str(svc_class_grid.best_score_))\nprint('\\nTested HyperParameters           Values')\nprint('kernal:                              {}'.format(svc_class_grid.best_estimator_.kernel))\nprint('gamma:                               {}'.format(svc_class_grid.best_estimator_.gamma))\nprint('C:                                   {}'.format(svc_class_grid.best_estimator_.C))","c7994bcd":"svc_best_params.fit(X_train, y_train)\nprobabilities = svc_best_params.predict_proba(X_test)\n\n# add to results_final\nscore = score_model(probabilities, .5)\nresults_final.loc['SVC',:] = [metrics.roc_auc_score(y_test, probabilities[:,1]), \n                              metrics.accuracy_score(y_test, score), \n                              metrics.precision_score(y_test, score),\n                              metrics.recall_score(y_test, score)]\n\nprint_metrics(y_test, probabilities, .5)      \nplot_auc(y_test, probabilities)","4d39530d":"ad_clf = AdaBoostClassifier(DecisionTreeClassifier(class_weight='balanced'))\nad_clf.get_params()","8cdff869":"cv=KFold(n_splits=3, shuffle=True)\nparam_grid = {'base_estimator__max_depth' :[1, 2, 5],\n              'base_estimator__min_samples_split' :[2, 3 ,5],\n              'base_estimator__min_samples_leaf' :[2, 3, 5 ,10],\n              'n_estimators' :[10, 20, 50, 100],\n              'learning_rate' :[0.001, 0.01, 0.1, 1]}\n\n\n\nad_clf_grid = GridSearchCV(estimator=ad_clf,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\nad_clf_grid.fit(X, y)\nada_best_params = ad_clf_grid.best_estimator_\nprint('GridsearchCV Best Score: {}'.format(ad_clf_grid.best_score_))\nprint('\\nTested HyperParameters        Values')\nprint('base_estimator__max_depth:         {}'.format(ad_clf_grid.best_estimator_.base_estimator.max_depth))\nprint('base_estimator__min_samples_split: {}'.format(ad_clf_grid.best_estimator_.base_estimator.min_samples_split))\nprint('base_estimator__min_samples_leaf:  {}'.format(ad_clf_grid.best_estimator_.base_estimator.min_samples_leaf))\nprint('n_estimators:                      {}'.format(ad_clf_grid.best_estimator_.n_estimators))\nprint('learning_rate:                     {}'.format(ad_clf_grid.best_estimator_.learning_rate))","35a1e44f":"ada_best_params.fit(X_train, y_train)\nprobabilities = ada_best_params.predict_proba(X_test)\n\n# add to results_final\nscore = score_model(probabilities, .5)\nresults_final.loc['AdaBoostClassifier',:] = [metrics.roc_auc_score(y_test, probabilities[:,1]), \n                                   metrics.accuracy_score(y_test, score), \n                                   metrics.precision_score(y_test, score),\n                                   metrics.recall_score(y_test, score)]\n\nprint_metrics(y_test, probabilities, .5)\nplot_auc(y_test, probabilities)","34721284":"gb_clf = GradientBoostingClassifier()\ngb_clf.get_params()","ba8066cb":"cv=KFold(n_splits=3, shuffle=True)\nparam_grid = {'n_estimators': [10, 20, 50, 100, 500],\n             'max_depth': [3, 5, 8, 15],\n             'min_samples_split': [2, 5, 8, 10],\n             'min_samples_leaf': [1, 2, 3, 5],\n             'learning_rate': [0.001, 0.01, 0.1, 1]}\n\ngb_clf_grid = GridSearchCV(estimator=gb_clf,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\ngb_clf_grid.fit(X, y)\ngb_best_params = gb_clf_grid.best_estimator_\nprint('GridsearchCV Best Score: {}'.format(gb_clf_grid.best_score_))\nprint('\\nTested HyperParameters   Values')\nprint('n_estimators:               {}'.format(gb_clf_grid.best_estimator_.n_estimators))\nprint('max_depth:                  {}'.format(gb_clf_grid.best_estimator_.max_depth))\nprint('min_samples_split:          {}'.format(gb_clf_grid.best_estimator_.min_samples_split))\nprint('min_samples_leaf:           {}'.format(gb_clf_grid.best_estimator_.min_samples_leaf))\nprint('learning_rate:              {}'.format(gb_clf_grid.best_estimator_.learning_rate))","2b26ae5b":"gb_best_params.fit(X_train, y_train)\nprobabilities = gb_best_params.predict_proba(X_test)\n\n# add to results_final\nscore = score_model(probabilities, .5)\nresults_final.loc['GradientBoosting',:] = [metrics.roc_auc_score(y_test, probabilities[:,1]), \n                                           metrics.accuracy_score(y_test, score), \n                                           metrics.precision_score(y_test, score),\n                                           metrics.recall_score(y_test, score)]\n\nprint_metrics(y_test, probabilities, .5)      \nplot_auc(y_test, probabilities)","63610d04":"rf_clf = RandomForestClassifier(class_weight='balanced')\nrf_clf.get_params()","eb1feef2":"cv=KFold(n_splits=3, shuffle=True)\nparam_grid = {'n_estimators': [10, 20, 50, 100, 500],\n             'max_depth': [3, 5, 8, 15, 50],\n             'min_samples_split': [2, 5, 8, 10],\n             'min_samples_leaf': [1, 2, 3, 5]}\n\nrf_clf_grid = GridSearchCV(estimator=rf_clf,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\nrf_clf_grid.fit(X, y)\nrf_best_params = rf_clf_grid.best_estimator_\nprint('GridsearchCV Best Score: {}'.format(rf_clf_grid.best_score_))\nprint('\\nTested HyperParameters   Values')\nprint('n_estimators:               {}'.format(rf_clf_grid.best_estimator_.n_estimators))\nprint('max_depth:                  {}'.format(rf_clf_grid.best_estimator_.max_depth))\nprint('min_samples_split:          {}'.format(rf_clf_grid.best_estimator_.min_samples_split))\nprint('min_samples_leaf:           {}'.format(rf_clf_grid.best_estimator_.min_samples_leaf))","0af1f235":"rf_best_params.fit(X_train, y_train)\nprobabilities = rf_best_params.predict_proba(X_test)\n\n# add to results_final\nscore = score_model(probabilities, .5)\nresults_final.loc['RandomForestClassifier',:] = [metrics.roc_auc_score(y_test, probabilities[:,1]), \n                                           metrics.accuracy_score(y_test, score), \n                                           metrics.precision_score(y_test, score),\n                                           metrics.recall_score(y_test, score)]\n\nprint_metrics(y_test, probabilities, .5)      \nplot_auc(y_test, probabilities)","382aefa8":"mlp_clf = MLPClassifier()\nmlp_clf.get_params()","068812b7":"cv=KFold(n_splits=3, shuffle=True)\nparam_grid = {'hidden_layer_sizes' :[(50, 50, 10), (50, 100, 10), (100,)],\n             'solver' :['lbfgs', 'adam'],\n             'alpha' :[.000001, .00001, .0001, .001],\n             'beta_1' :[.8, .9, .99],\n             'beta_2' :[.8, .9, .99, .999]}\n\nmlp_clf_grid = GridSearchCV(estimator=mlp_clf,\n                           param_grid=param_grid,\n                           cv=cv,\n                           scoring='roc_auc',\n                           return_train_score=True,\n                           n_jobs=4,\n                           verbose=1)\n\nmlp_clf_grid.fit(X, y)\nmlp_best_params = mlp_clf_grid.best_estimator_\nprint('GridSearchCV Best Score:  {}'.format(mlp_clf_grid.best_score_))\nprint('\\nTested HyperParameters   Values')\nprint('hidden_layer_sizes:          {}'.format(mlp_clf_grid.best_estimator_.hidden_layer_sizes))\nprint('solver:                      {}'.format(mlp_clf_grid.best_estimator_.solver))\nprint('alpha:                       {}'.format(mlp_clf_grid.best_estimator_.alpha))\nprint('beta_1:                      {}'.format(mlp_clf_grid.best_estimator_.beta_1))\nprint('beta_2:                      {}'.format(mlp_clf_grid.best_estimator_.beta_2))","02d06551":"mlp_best_params.fit(X_train, y_train)\nprobabilities = mlp_best_params.predict_proba(X_test)\n\n# add to results_final\nscore = score_model(probabilities, .5)\nresults_final.loc['MPLClassifier',:] = [metrics.roc_auc_score(y_test, probabilities[:,1]), \n                                           metrics.accuracy_score(y_test, score), \n                                           metrics.precision_score(y_test, score),\n                                           metrics.recall_score(y_test, score)]\n\nprint_metrics(y_test, probabilities, .5)      \nplot_auc(y_test, probabilities)","b8057f52":"def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n\n    axes.set_title(title)\n    if ylim is not None:\n        axes.set_ylim(*ylim)\n    axes.set_xlabel(\"Training examples\")\n    axes.set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plot learning curve\n    axes.grid(color='k')\n    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes.legend(loc=\"best\")\n    return plt\n\n\ncv=KFold(n_splits=10, shuffle=True)\n\nfig, axes = plt.subplots(3, 2, figsize=(16, 20))\n\ntitle = \"learning Curves (LogisticRegression)\"\nestimator = clone(lin_mod_best)\nplot_learning_curve(estimator, title, X, y, axes=axes[0, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (SVM Classifier)\"\nestimator = clone(svc_best_params)\nplot_learning_curve(estimator, title, X, y, axes=axes[0, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (AdaBoostClassifier)\"\nestimator = clone(ada_best_params)\nplot_learning_curve(estimator, title, X, y, axes=axes[1, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (GradientBoostingClassifier)\"\nestimator = clone(gb_best_params)\nplot_learning_curve(estimator, title, X, y, axes=axes[1, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (RandomForestClassifier)\"\nestimator = clone(rf_best_params)\nplot_learning_curve(estimator, title, X, y, axes=axes[2, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = \"Learning Curves (MPLClassifier)\"\nestimator = clone(mlp_best_params)\nplot_learning_curve(estimator, title, X, y, axes=axes[2, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\nplt.show()\n\n# reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html","2d3fddf7":"results_final","5631e7b7":"https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_map_data_to_normal.html#sphx-glr-auto-examples-preprocessing-plot-map-data-to-normal-py\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py","7ffe1401":"The Column names are a bit unwieldly so first we will deal with them","f737263a":"We will start with LogisticRegression","6bc3af59":"Feature Importances","54616dff":"Data Exploration","378b8a23":"this is sevearly unbalanced we will have to make sure we keep this in mind when modeling.","f36ed144":"Lets create a baseline model score for comparison purposes"}}