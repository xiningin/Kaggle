{"cell_type":{"4c4425cc":"code","99ccb140":"code","4b8fcedf":"code","8d4bf694":"code","df22721c":"code","caa386d7":"code","f40612b2":"code","a1792178":"code","3fc940a1":"code","10ac76ff":"code","d6d78769":"code","6ee8994f":"code","93033b85":"code","7b01f032":"markdown","d4cf07ef":"markdown","f79770a5":"markdown","d9bffeb2":"markdown","5c6905f9":"markdown","a7789a33":"markdown","87c07569":"markdown","ee2f4ba6":"markdown","0cbefe9c":"markdown","eae9baad":"markdown","d81ab7e0":"markdown","5060560c":"markdown"},"source":{"4c4425cc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\n","99ccb140":"data = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ndata.head()","4b8fcedf":"d2 = data.drop(['id', 'target'], axis = 1)\nd1 = d2\nd4 = np.log(abs(d2)+1)\n\nd4 = d4.add_prefix(\"log_\")\n\n\n# our summary stats of original data\nd2['mean'] = d1.mean(axis=1)\nd2['sd'] = d1.std(axis=1)\nd2['var'] = d1.var(axis=1)\nd2['max'] = d1.max(axis=1)\nd2['min'] = d1.min(axis=1)\nd2['median'] = d1.median(axis=1)\nd2['kurt'] = d1.kurt(axis=1)\n\ndataf = pd.concat([d2, d4], axis=1)\n\n\nX_dataf = dataf\ny_dataf = data['target']\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_dataf, y_dataf, test_size = 0.2, random_state=37)\nX_train1, X_val1, y_train1, y_val1 = train_test_split(X_train1, y_train1, test_size=0.25, random_state=37)\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_dataf, y_dataf, test_size = 0.2, random_state=73)\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.25, random_state=73)\n\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X_dataf, y_dataf, test_size = 0.2, random_state=173)\nX_train3, X_val3, y_train3, y_val3 = train_test_split(X_train3, y_train3, test_size=0.25, random_state=173)\n","8d4bf694":"testdata = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\ntestid = testdata['id']\n\ntd2 = testdata.drop(['id'], axis = 1)\ntd1 = td2\ntd4 = np.log(abs(td2)+1)\n\ntd4 = td4.add_prefix(\"log_\")\n\n\n# our summary stats of original data\ntd2['mean'] = td1.mean(axis=1)\ntd2['sd'] = td1.std(axis=1)\ntd2['var'] = td1.var(axis=1)\ntd2['max'] = td1.max(axis=1)\ntd2['min'] = td1.min(axis=1)\ntd2['median'] = d1.median(axis=1)\ntd2['kurt'] = d1.kurt(axis=1)\n\ntdata = pd.concat([td2, td4], axis=1)","df22721c":"# Early stopping parameters\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=30, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n# model structure\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_dim = 207),\n    layers.Dense(units = 256, activation = \"swish\"),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 256, activation = \"relu\"),\n    layers.Dropout(0.25),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 64, activation = \"relu\"),\n    layers.Dropout(0.125),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 16, activation = \"relu\"),\n    layers.Dropout(0.125),\n    layers.BatchNormalization(),\n\n    layers.Dense(units = 1, activation = \"sigmoid\") # binary output\n])\n\n# evaluation methods\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'binary_crossentropy',\n    metrics = ['binary_accuracy']\n)\n\n# training\nhistory = model.fit(X_train1, y_train1,\n                    validation_data = (X_val1, y_val1),\n                    batch_size = 100,\n                    callbacks = [early_stopping],\n                    epochs = 200)","caa386d7":"# plotting evaluation steps\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","f40612b2":"predictions = model.predict(\n   tdata, \n   batch_size = None, \n   verbose = 0, \n   steps = None, \n   callbacks = early_stopping, \n   max_queue_size = 10, \n   workers = 1, \n   use_multiprocessing = True\n).reshape(1,-1)[0]\n\n\nresult1 = pd.DataFrame({'id':testid, 'target': predictions})\nresult1.to_csv('.\/submission1.csv', index=False)","a1792178":"# Early stopping parameters\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=30, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n# model structure\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_dim = 207),\n    layers.Dense(units = 256, activation = \"swish\"),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 256, activation = \"relu\"),\n    layers.Dropout(0.25),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 64, activation = \"relu\"),\n    layers.Dropout(0.125),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 16, activation = \"relu\"),\n    layers.Dropout(0.125),\n    layers.BatchNormalization(),\n\n    layers.Dense(units = 1, activation = \"sigmoid\") # binary output\n])\n\n# evaluation methods\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'binary_crossentropy',\n    metrics = ['binary_accuracy']\n)\n\n# training\nhistory = model.fit(X_train2, y_train2,\n                    validation_data = (X_val2, y_val2),\n                    batch_size = 100,\n                    callbacks = [early_stopping],\n                    epochs = 200)","3fc940a1":"# plotting evaluation steps\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","10ac76ff":"predictions = model.predict(\n   tdata, \n   batch_size = None, \n   verbose = 0, \n   steps = None, \n   callbacks = early_stopping, \n   max_queue_size = 10, \n   workers = 1, \n   use_multiprocessing = True\n).reshape(1,-1)[0]\n\n\nresult1 = pd.DataFrame({'id':testid, 'target': predictions})\nresult1.to_csv('.\/submission2.csv', index=False)","d6d78769":"# Early stopping parameters\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=30, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n# model structure\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_dim = 207),\n    layers.Dense(units = 256, activation = \"swish\"),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 256, activation = \"relu\"),\n    layers.Dropout(0.25),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 64, activation = \"relu\"),\n    layers.Dropout(0.125),\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 16, activation = \"relu\"),\n    layers.Dropout(0.125),\n    layers.BatchNormalization(),\n\n    layers.Dense(units = 1, activation = \"sigmoid\") # binary output\n])\n\n# evaluation methods\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'binary_crossentropy',\n    metrics = ['binary_accuracy']\n)\n\n# training\nhistory = model.fit(X_train3, y_train3,\n                    validation_data = (X_val3, y_val3),\n                    batch_size = 100,\n                    callbacks = [early_stopping],\n                    epochs = 200)","6ee8994f":"# plotting evaluation steps\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","93033b85":"predictions = model.predict(\n   tdata, \n   batch_size = None, \n   verbose = 0, \n   steps = None, \n   callbacks = early_stopping, \n   max_queue_size = 10, \n   workers = 1, \n   use_multiprocessing = True\n).reshape(1,-1)[0]\n\n\nresult1 = pd.DataFrame({'id':testid, 'target': predictions})\nresult1.to_csv('.\/submission3.csv', index=False)","7b01f032":"## Summary features\n\nSummary statistics of the features, say 'mean', 'median', 'range' etc. Can be very telling of the data. Luckily, these are not complicated to include.\n\nWe want these statistics to inform our decision about what the `target` should be. Thus, we need to include extra columns with these summaries as values.","d4cf07ef":"# Features\n\nWe *should* inspect the data first when it comes to this. But as mentioned my focus was exploring simple keras neural net models. Anyhow, if we want to really improve the model, we will ahve to start thinking about both feature engineering and feature selection. As this data is no 'real', it was generated, this may prove tricky, but we'll get creative.\n\nUsually, this is how we gain the best improvements. \n\n## Correlations","f79770a5":"minor, if neglibible improvements.\n\n# Early Stopping\n\nWe have been running for 20 epochs (training sets) when we can see that we often do not improve after just a few epochs. We can set out some 'early stopping' parameters to stop the training early if it looks like it is going nowhere. \n\nThis also helps with preventing 'overfitting'.\n\nWe'll add 'capacity' to our model by adding further layers.","d9bffeb2":"## Non-linear features\n\nMuch alike the polynomial features, we can also use non-linear functions.\nThis most common transformation to use is 'log', whether base2, 10, or the natural log is up to choice. But really, it's whatever you need. \n\nWe can go crazy really, but experience and domain knowledge will best inform these decisions. In essence, you are attempting to transform the data to better reveal any underlying pattern.","5c6905f9":"# Dropout\n\nAnother tool we can use is 'dropout'. Meaning, when evaluating weights, the model will choose some nodes to 'drop'. Essentially, it prevents singular nodes having too much importance. We'll use the deep model, again.","a7789a33":"Looking at the past three runs, we can see that 'deepening' the model has improved it. But only 'just'. We need more!\n\n# Batch Normilization\n\nWe did nothing with the data. Jack-squat. One aspect that often proves helpful to note is \"a large difference is more important than a small difference, (but it depends on the difference)\" - me. When data is not 'normalized', a large difference between two values of a feature may have more 'importance' than say a relatively small (but very important) difference for a model. A way to take this into account is to normalize the data. (Scale each feature such that every feature has the same 'spread' or 'min\/max' etc.)\n\nKeras has a normlization layer that we will use. We'll continue with our 'deep' model","87c07569":"# A Quick Exercise\n\nThis is a 'quickie' in building a Neural Network with Keras, coming off the Kaggle course \"Intro to Deep Learning\"","ee2f4ba6":"We'll build a naive model first, on all features, then we'll look at feature selection and posisble engineering","0cbefe9c":"# Wide and Deep Modelling\n\nor is it 'modeling'? We can develop our model two ways. Widen  (add more units), or deepen (add more layers). \n\nWe'll try both, separately, and then together","eae9baad":"## Polynomial features\n\nPolynomials of 'x' are like: ax^2 + bx + c, etc. to any power and any combination. It would be ridiculous to explore all possible combinations, though looking at just a possible range of powers, say x^2, or x^3, may help create something special. \n\nUsually, when it comes to choices, you would choose either the most significant features or the least significant features. Depends if you are investigating the most obvious patterns or least.\n\n(*I suppose, but please correct me on this*)\n\nThis test, we'll use 'x^2' as an extra feature, for every feature. Then normalize.","d81ab7e0":"Welp! There goes out hopes (but not our dreams). This heatmap confirms that the features do not correlate strongly with eachother. (Thus, 'feature engineering' may be of no help) BUT we can see that some do correlate \"more strongly\" with the `id` (leftmost column) and `target` (bottom row).\n\n*Note, the strengths of any correlations here are exceptionally weak if present at all. We will more than likely just skip this step altogether, but it's extra practice for myself*\n\nFortunately, we remove the `id` feature as it is just a label and the correlations present are 'false', they will only create misleading predictions. However, the correlation with `target` is precisely what we need for 'feature selection'. Let's collect these features","5060560c":"Okay, cool. It \"works\"."}}