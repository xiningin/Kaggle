{"cell_type":{"d38365a2":"code","30bd4fa1":"code","fc3f32b7":"code","1ae7a2a5":"code","7d984ca2":"code","aa7104f2":"code","250651b7":"code","58cc630c":"code","12ee9d19":"code","a051e746":"code","a5c36e63":"code","cdd8e92d":"code","db437874":"code","02f58f3b":"code","1b7dd099":"code","1fd79135":"code","8b9110da":"code","0e89cc73":"code","a7a6276a":"markdown","dae6d921":"markdown","b22cfec6":"markdown","6c136bf7":"markdown"},"source":{"d38365a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","30bd4fa1":"import pandas as pd\nfrom tensorflow.keras.utils import get_file\n\ntry:\n    path = get_file('kddcup.data_10_percent.gz', origin='http:\/\/kdd.ics.uci.edu\/databases\/kddcup99\/kddcup.data_10_percent.gz')\nexcept:\n    print('Error downloading')\n    raise\n    \nprint(path) \n\n# This file is a CSV, just no CSV extension or headers\n# Download from: http:\/\/kdd.ics.uci.edu\/databases\/kddcup99\/kddcup99.html\ndf = pd.read_csv(path, header=None)","fc3f32b7":"# The CSV file has no column heads, so add them\ndf.columns = [\n    'duration',\n    'protocol_type',\n    'service',\n    'flag',\n    'src_bytes',\n    'dst_bytes',\n    'land',\n    'wrong_fragment',\n    'urgent',\n    'hot',\n    'num_failed_logins',\n    'logged_in',\n    'num_compromised',\n    'root_shell',\n    'su_attempted',\n    'num_root',\n    'num_file_creations',\n    'num_shells',\n    'num_access_files',\n    'num_outbound_cmds',\n    'is_host_login',\n    'is_guest_login',\n    'count',\n    'srv_count',\n    'serror_rate',\n    'srv_serror_rate',\n    'rerror_rate',\n    'srv_rerror_rate',\n    'same_srv_rate',\n    'diff_srv_rate',\n    'srv_diff_host_rate',\n    'dst_host_count',\n    'dst_host_srv_count',\n    'dst_host_same_srv_rate',\n    'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate',\n    'dst_host_srv_diff_host_rate',\n    'dst_host_serror_rate',\n    'dst_host_srv_serror_rate',\n    'dst_host_rerror_rate',\n    'dst_host_srv_rerror_rate',\n    'outcome'\n]\n\n# display 5 rows\ndf[0:19289]","1ae7a2a5":"print(\"Read {} rows.\".format(len(df)))\nprint('='*40)\nprint('The number of data points are:', df.shape[0])\nprint('='*40)\nprint('The number of features are:', df.shape[1])\nprint('='*40)\noutput = df['outcome'].values\nlabels = set(output)\nprint('The different type of output labels are:', labels)\nprint('='*125)\nprint('The number of different output labels are:', len(labels))","7d984ca2":"# Data Cleaning\n\n# Checking for NULL values\nprint('Null values in dataset are',len(df[df.isnull().any(1)]))\nprint('='*40)\n\n# Checkng for DUPLICATE values\ndf.drop_duplicates(keep='first', inplace = True)\n\n# For now, just drop NA's (rows with missing values)\ndf.dropna(inplace=True,axis=1) \n\n# stored the data into a pickle file so we can load through\n# df.to_pickle('df.pkl')\n\nprint(\"Read {} rows.\".format(len(df)))","aa7104f2":"# Exploratory data analysis\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import *\n\nplt.figure(figsize=(15,7))\nclass_distribution = df['outcome'].value_counts()\nclass_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in train data')\nplt.grid()\nplt.show()","250651b7":"sorted_yi = np.argsort(-class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', class_distribution.index[i],':', class_distribution.values[i], \n          '(', np.round((class_distribution.values[i]\/df.shape[0]*100), 3), '%)')\n    \n#df.groupby('outcome')['outcome'].count() #this could also be used if you want no-fromatted for above.","58cc630c":"ENCODING = 'utf-8'\n\ndef expand_categories(values):\n    result = []\n    s = values.value_counts()\n    t = float(len(values))\n    for v in s.index:\n        result.append(\"{}:{}%\".format(v,round(100*(s[v]\/t),2)))\n    return \"[{}]\".format(\",\".join(result))\n        \ndef analyze(df):\n    print()\n    cols = df.columns.values\n    total = float(len(df))\n\n    print(\"{} rows\".format(int(total)))\n    for col in cols:\n        uniques = df[col].unique()\n        unique_count = len(uniques)\n        if unique_count>100:\n            print(\"** {}:{} ({}%)\".format(col,unique_count,int(((unique_count)\/total)*100)))\n        else:\n            print(\"** {}:{}\".format(col,expand_categories(df[col])))\n            expand_categories(df[col])","12ee9d19":"# Analyze KDD-99\n\nimport pandas as pd\nimport os\nimport numpy as np\nfrom sklearn import metrics\nfrom scipy.stats import zscore\n\nanalyze(df)","a051e746":"# Encode a numeric column as zscores\ndef encode_numeric_zscore(df, name, mean=None, sd=None):\n    if mean is None:\n        mean = df[name].mean()\n\n    if sd is None:\n        sd = df[name].std()\n\n    df[name] = (df[name] - mean) \/ sd\n    \n# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\ndef encode_text_dummy(df, name):\n    dummies = pd.get_dummies(df[name])\n    for x in dummies.columns:\n        dummy_name = f\"{name}-{x}\"\n        df[dummy_name] = dummies[x]\n    df.drop(name, axis=1, inplace=True)","a5c36e63":"# Now encode the feature vector\n\nencode_numeric_zscore(df, 'duration')\nencode_text_dummy(df, 'protocol_type')\nencode_text_dummy(df, 'service')\nencode_text_dummy(df, 'flag')\nencode_numeric_zscore(df, 'src_bytes')\nencode_numeric_zscore(df, 'dst_bytes')\nencode_text_dummy(df, 'land')\nencode_numeric_zscore(df, 'wrong_fragment')\nencode_numeric_zscore(df, 'urgent')\nencode_numeric_zscore(df, 'hot')\nencode_numeric_zscore(df, 'num_failed_logins')\nencode_text_dummy(df, 'logged_in')\nencode_numeric_zscore(df, 'num_compromised')\nencode_numeric_zscore(df, 'root_shell')\nencode_numeric_zscore(df, 'su_attempted')\nencode_numeric_zscore(df, 'num_root')\nencode_numeric_zscore(df, 'num_file_creations')\nencode_numeric_zscore(df, 'num_shells')\nencode_numeric_zscore(df, 'num_access_files')\nencode_numeric_zscore(df, 'num_outbound_cmds')\nencode_text_dummy(df, 'is_host_login')\nencode_text_dummy(df, 'is_guest_login')\nencode_numeric_zscore(df, 'count')\nencode_numeric_zscore(df, 'srv_count')\nencode_numeric_zscore(df, 'serror_rate')\nencode_numeric_zscore(df, 'srv_serror_rate')\nencode_numeric_zscore(df, 'rerror_rate')\nencode_numeric_zscore(df, 'srv_rerror_rate')\nencode_numeric_zscore(df, 'same_srv_rate')\nencode_numeric_zscore(df, 'diff_srv_rate')\nencode_numeric_zscore(df, 'srv_diff_host_rate')\nencode_numeric_zscore(df, 'dst_host_count')\nencode_numeric_zscore(df, 'dst_host_srv_count')\nencode_numeric_zscore(df, 'dst_host_same_srv_rate')\nencode_numeric_zscore(df, 'dst_host_diff_srv_rate')\nencode_numeric_zscore(df, 'dst_host_same_src_port_rate')\nencode_numeric_zscore(df, 'dst_host_srv_diff_host_rate')\nencode_numeric_zscore(df, 'dst_host_serror_rate')\nencode_numeric_zscore(df, 'dst_host_srv_serror_rate')\nencode_numeric_zscore(df, 'dst_host_rerror_rate')\nencode_numeric_zscore(df, 'dst_host_srv_rerror_rate')\n\n# display 5 rows\n\ndf.dropna(inplace=True,axis=1)\ndf[0:5]\n# This is the numeric feature vector, as it goes to the neural net","cdd8e92d":"# Convert to numpy - Classification\nx_columns = df.columns.drop('outcome')\nx = df[x_columns].values\ndummies = pd.get_dummies(df['outcome']) # Classification\noutcomes = dummies.columns\nnum_classes = len(outcomes)\ny = dummies.values","db437874":"df.groupby('outcome')['outcome'].count()","02f58f3b":"import pandas as pd\nimport io\nimport requests\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Create a test\/train split.  25% test\n# Split into train\/test\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.25, random_state=42)\n\n# Create neural net\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\nmodel.add(Dense(y.shape[1],activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=19)","1b7dd099":"import tensorflow.keras.backend as K\nprint('Learning Rate - ')\nprint(K.eval(model.optimizer.lr)) \nprint('='*50)\nmodel.summary()","1fd79135":"import seaborn as sns\nimport datetime as dt\n\ndef confusion_matrix_func(y_test, y_test_pred):\n    \n    '''\n    This function computes the confusion matrix using Predicted and Actual values and plots a confusion matrix heatmap\n    '''\n    C = confusion_matrix(y_test, y_test_pred)\n    cm_df = pd.DataFrame(C)\n    labels = ['back', 'butter_overflow', 'loadmodule', 'guess_passwd', 'imap', 'ipsweep', 'warezmaster', 'rootkit', \n'multihop', 'neptune', 'nmap', 'normal', 'phf', 'perl', 'pod', 'portsweep', 'ftp_write', 'satan', 'smurf', 'teardrop', 'warezclient', 'land']\n    plt.figure(figsize=(20,15))\n    sns.set(font_scale=1.4)\n    sns.heatmap(cm_df, annot=True, annot_kws={\"size\":12}, fmt='g', xticklabels=labels, yticklabels=labels)\n    plt.ylabel('Actual Class')\n    plt.xlabel('Predicted Class')\n    \n    plt.show()\n\n# calculate roc curve\nfrom sklearn.metrics import *\n#fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_test, pred)\nfrom sklearn import preprocessing\ndef multiclass_roc_auc_score(y_test, pred, average=\"macro\"):\n    lb = preprocessing.LabelBinarizer()\n    lb.fit(y_test)\n    y_test = lb.transform(y_test)\n    pred = lb.transform(pred)\n    return roc_auc_score(y_test, pred, average=average)","8b9110da":"print('Train data')\nprint(x_train.shape)\nprint(y_train.shape)\nprint('='*20)\nprint('Test data')\nprint(x_test.shape)\nprint(y_test.shape)\nprint('='*20)\n\n# Measure accuracy\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score\nprint('Predicting on the test data:')\nstart = dt.datetime.now()\nescore = model.evaluate(x_test, y_test, batch_size=32)\npred = model.predict(x_test)\npred = np.argmax(pred,axis=1)\ny_eval = np.argmax(y_test,axis=1)\n\nvscore = metrics.accuracy_score(y_eval, pred)\n\nrscore = recall_score(y_eval, pred, average='weighted')\n\nascore = precision_score(y_eval, pred, average='weighted')\n\nf1score= f1_score(y_eval, pred, average='weighted') #F1 = 2 * (precision * recall) \/ (precision + recall) for manual\n\nroc_auc_socre = multiclass_roc_auc_score(y_eval, pred)\n\n\nprint('Completed')\nprint('Time taken:',dt.datetime.now()-start)\nprint('='*50)\nprint(\"Validation score: {}\".format(vscore))\nprint('='*50)\nprint(\"Evaluation score: {}\".format(escore))\nprint('='*50)\nprint(\"Recall score: {}\".format(rscore))\nprint('='*50)\nprint(\"Precision score: {}\".format(ascore))\nprint('='*50)\nprint(\"F1 score: {}\".format(f1score))\nprint('='*50)\nprint(\"ROC-AUC score: {}\".format(roc_auc_socre))\n","0e89cc73":"confusion_matrix_func(y_eval, pred)","a7a6276a":"**Encode the feature vector**","dae6d921":"**Train the Neural Network**","b22cfec6":"**Training an Intrusion Detection System with KDD99 Dataset**","6c136bf7":"**Analyzing the Dataset**"}}