{"cell_type":{"3a152387":"code","5d78bfbc":"code","e2af51c7":"code","ef234dd9":"code","301bb2fc":"code","77f844a3":"code","3f39d62e":"code","08fd88a1":"code","af65ed54":"code","308f3e3b":"code","9185eab9":"code","f1b6adec":"code","17c524f0":"code","ffb7d5e9":"code","9a4f4ef2":"code","affd0092":"code","f9d5f62c":"code","bd299fb0":"code","b17e1e36":"code","1a5e8317":"code","867885eb":"code","ee1b387d":"code","384b775a":"code","8b2cbeba":"code","483652f5":"code","3c2edb6b":"code","d7e0a152":"code","745126b4":"code","2961e9c3":"code","ee1db472":"code","826e3a0d":"code","bc528d97":"code","ef19640e":"code","3bbff220":"markdown","16b734f2":"markdown","5223f612":"markdown","84129a6e":"markdown","eaaa88f6":"markdown","77184e6c":"markdown","3c36ab6e":"markdown","9c898688":"markdown","d3b98574":"markdown","f827fd17":"markdown","0f4dfd28":"markdown","82aad01e":"markdown","fd06ac98":"markdown","74264511":"markdown","56fe65fd":"markdown","ebe79e8e":"markdown","83618799":"markdown","8d4d2600":"markdown","2e5b8c86":"markdown","d6f19dda":"markdown","4d845de1":"markdown","1adbb96d":"markdown","798ea905":"markdown","33a55d2b":"markdown","9c34378c":"markdown","9aa42a97":"markdown","eb314f0f":"markdown","a2a8d6da":"markdown"},"source":{"3a152387":"# Data manipulation\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\n\n# Maths\nimport numpy as np\n\n# Patching sklearn\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\n\n# Model Building \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport optuna\nfrom optuna.integration import LightGBMPruningCallback\n\nimport shap\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\n\n\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint\n\n# Lgbm\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\n\n# Settings\nsns.set(rc = {'figure.figsize': (26, 8)})\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style = \"ticks\", rc = custom_params)","5d78bfbc":"# Data loading\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col = 'Id')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col = 'Id')\n\n# Drop Not relevant columns\ndef drop_nrv(df):\n    na_df = pd.DataFrame(df.isna().sum(), \n                         columns = ['Number of NaN'])\n    \n    na_df = na_df.sort_values('Number of NaN', \n                              ascending = False).head(10)\n    \n    na_df['Perc'] = round(na_df['Number of NaN']\/len(train_data.index)*100, 2)\n    \n    # Drop columns that have more than 50% of NaN\n    to_drop = list(na_df[na_df['Perc'] > 50.00].index)\n    na_df['Perc'] = na_df['Perc'].astype(str) + '%'\n    df.drop(to_drop, \n            inplace = True, axis = 1)\n    \n    print(to_drop, ' columns have been removed (> 50% NaN)\\n', sep = '')\n    \n    return na_df, df.astype('float64', errors = 'ignore')\n\nprint('TRAIN DATA\\n')\nmissing_val_train, train_data = drop_nrv(train_data)\nprint(missing_val_train, '\\n\\n', '-'*80, '\\n', sep = '')\n\nprint('TEST DATA\\n')\nmissing_val_test, test_data = drop_nrv(test_data)\nprint(missing_val_test, '\\n\\n', '-'*80, '\\n', sep = '')\n\nprint(f'Train Data rows: {train_data.shape[0]} \\nTrain Data Columns: {train_data.shape[1]}\\n')\nprint(f'Test Data rows: {test_data.shape[0]} \\nTest Data Columns: {test_data.shape[1]}')","e2af51c7":"def obj_int_identifier(train_data, test_data):\n\n    num_unique_val = pd.DataFrame(train_data.nunique(), columns = ['Unique Values']).sort_values(by = 'Unique Values')\n    cat = num_unique_val[num_unique_val['Unique Values']<=25].index\n    cont  = num_unique_val[num_unique_val['Unique Values']>25].index\n    \n    train_data_cont_var = train_data.filter(cont).columns\n    train_data_disc_var = train_data.filter(cat).columns\n    \n    print('Train data total columns: ', \n          len(train_data_cont_var)+len(train_data_disc_var), \n          '\\nContinuous Features: ', len(train_data_cont_var),\n          '\\nDiscrete Features: ', len(train_data_disc_var), '\\n', sep ='')\n    \n    test_data_cont_var = test_data.filter(cont).columns\n    test_data_disc_var = test_data.filter(cat).columns\n\n    print('Test data total columns: ', \n          len(test_data_cont_var)+len(test_data_disc_var), \n          '\\nContinuous Features: ', len(test_data_cont_var),\n          '\\nDiscrete Features: ', len(test_data_disc_var), sep ='')\n    \n    return train_data_cont_var, train_data_disc_var, test_data_cont_var, test_data_disc_var\n\ntrain_data_cont_var, train_data_disc_var, test_data_cont_var, test_data_disc_var = obj_int_identifier(train_data, test_data)","ef234dd9":"# Align Features\ndef discrepancies_check(train, test):\n    dict_train = {}\n    dict_test = {}\n    \n    for el in train_data_disc_var:\n        dict_train[el] = train[el].unique().tolist()\n    \n    for el in test_data_disc_var:\n        dict_test[el] = test[el].unique().tolist()\n    \n    if dict_train.keys() == dict_test.keys():\n        print('Train and Test set have the same discrete features.\\nResults:')\n    else: \n        print('Pay attention, different discrete features in Train and Test!\\n')\n\n    dict_diff = {}\n    train_or_test = {}\n    \n    for key in dict_train.keys():\n        if set(dict_train[key]) ^ set(dict_test[key]) != set():\n            dict_diff[key] = list(set(dict_train[key]) ^ set(dict_test[key]))\n    \n    for key in dict_train.keys():\n        if (set(dict_test[key]) - set(dict_train[key]) != set()) & (set(dict_train[key]) - set(dict_test[key]) != set()):\n            train_or_test[key] = 'Both'        \n        elif set(dict_train[key]) - set(dict_test[key]) != set():\n            train_or_test[key] = 'Train'\n        elif set(dict_test[key]) - set(dict_train[key]) != set():\n            train_or_test[key] = 'Test'        \n        elif set(dict_train[key]) ^ set(dict_test[key]) == set():\n            pass\n        else:\n            print('Pay attention possible errors!')\n    \n    df = pd.DataFrame(index = dict_diff.keys(), columns = ['Discrepancies'])\n    df['Discrepancies'] = dict_diff.values()\n    \n    df1 = pd.DataFrame(index = train_or_test.keys(), columns = ['Where'])\n    df1['Where'] = train_or_test.values()\n    \n    final_df = df.merge(df1, right_index = True, left_index = True)\n    \n    return final_df\n    \ndiscrepancies_check(train_data, test_data)","301bb2fc":"print('Utilities in Train data: ', train_data['Utilities'].unique(), '\\n',\n      'Utilities in Test data: ', test_data['Utilities'].unique(), sep = '')","77f844a3":"print('Utilities in Train data: ', train_data['Electrical'].unique(), '\\n',\n      'Utilities in Test data: ', test_data['Electrical'].unique(), sep = '')","3f39d62e":"train_data[train_data_cont_var].hist(figsize=(26, 20), layout=(5, 4))\n\nplt.tight_layout()\nplt.show()","08fd88a1":"fig, ax = plt.subplots(10, 6, figsize=(26, 40))\n\nfor variable, subplot in zip(train_data_disc_var, ax.flatten()):\n    sns.countplot(x  = train_data[variable], ax=subplot)\n    subplot.figure.tight_layout()\n    for label in subplot.get_xticklabels():\n        label.set_rotation(45)\n\nfig.delaxes(ax[9][3])\nfig.delaxes(ax[9][4])\nfig.delaxes(ax[9][5])","af65ed54":"sns.pairplot(train_data[train_data_cont_var])  \nplt.tight_layout()\nplt.show()","308f3e3b":"sns.heatmap(train_data[train_data_cont_var].corr(), annot=True, annot_kws={\"size\": 15},fmt='.1f')\nplt.show()","9185eab9":"test_data[test_data_cont_var].hist(figsize=(26, 20), layout=(5, 4))\n\nplt.tight_layout()\nplt.show()","f1b6adec":"fig, ax = plt.subplots(10, 6, figsize=(26, 40))\n\nfor variable, subplot in zip(train_data_disc_var, ax.flatten()):\n    sns.countplot(x  = train_data[variable], ax=subplot)\n    subplot.figure.tight_layout()\n    for label in subplot.get_xticklabels():\n        label.set_rotation(45)\n\nfig.delaxes(ax[9][2])\nfig.delaxes(ax[9][3])\nfig.delaxes(ax[9][4])\nfig.delaxes(ax[9][5])","17c524f0":"sns.pairplot(test_data[test_data_cont_var])  \nplt.tight_layout()\nplt.show()","ffb7d5e9":"sns.heatmap(test_data[test_data_cont_var].corr(), annot=True, annot_kws={\"size\": 15},fmt='.1f')\nplt.show()","9a4f4ef2":"fig,ax = plt.subplots(3,2, figsize=(26,20))\n\nsns.boxplot(x=train_data['OverallQual'],\n            y=train_data['SalePrice'], \n            ax=ax[0,0]).set_title('OverallQual and SalePrice',\n                                fontsize = 20)\n\nsns.boxplot(x=train_data['OverallCond'],\n            y=train_data['SalePrice'], \n            ax=ax[1,0]).set_title('OverallCond and SalePrice',\n                                fontsize = 20)\n\nsns.scatterplot(x = train_data['YearBuilt'],\n                y = train_data['SalePrice'],\n                hue = train_data['OverallQual'],\n            ax=ax[0,1]).set_title('YearBuilt and SalePrice',\n                                  fontsize = 20)\n\nsns.scatterplot(x=train_data['GrLivArea'],\n                y=train_data['SalePrice'], \n                hue = train_data['OverallQual'],\n                ax=ax[1,1]).set_title('GrLivArea and SalePrice',\n                                      fontsize = 20)\n\nsns.boxplot(x = train_data['TotRmsAbvGrd'],\n            y = train_data['SalePrice'],\n            ax = ax[2,0]).set_title('TotRmsAbvGrd and SalePrice',\n                                  fontsize = 20)\n\nsns.scatterplot(x=train_data['TotalBsmtSF'],\n                y=train_data['SalePrice'], \n                hue = train_data['OverallQual'],\n                ax=ax[2,1]).set_title('TotalBsmtSF and SalePrice',\n                                      fontsize = 20)\nfig.show()","affd0092":"# For the categorical variable is better to track where the missing values are\ntrain_data[train_data_disc_var] = train_data[train_data_disc_var].fillna('missing')\n\n# The mean is used to fill NaN. As for GarageYrBlt 0 is used in order to track where the missing values are\nmiss_cont = pd.DataFrame(train_data[train_data_cont_var].isna().sum(), columns = ['missing']).sort_values('missing', ascending = False).head(3)\ntrain_data['LotFrontage'] = train_data['LotFrontage'].fillna(train_data['LotFrontage'].mean())\ntrain_data['GarageYrBlt'] = train_data['GarageYrBlt'].fillna(0)\ntrain_data['MasVnrArea'] = train_data['MasVnrArea'].fillna(train_data['MasVnrArea'].mean())\nmiss_cont","f9d5f62c":"# For the categorical variable is better to track where the missing values are\ntest_data[test_data_disc_var] = test_data[test_data_disc_var].fillna('missing')\n\n# The mean is used to fill NaN. As for GarageYrBlt 0 is used in order to track where the missing values are\nmiss_cont = pd.DataFrame(test_data[test_data_cont_var].isna().sum(), columns = ['missing']).sort_values('missing', ascending = False).head(8)\ntest_data['LotFrontage'] = test_data['LotFrontage'].fillna(test_data['LotFrontage'].mean())\ntest_data['GarageYrBlt'] = test_data['GarageYrBlt'].fillna(0)\ntest_data['MasVnrArea'] = test_data['MasVnrArea'].fillna(test_data['MasVnrArea'].mean())\ntest_data['GarageArea'] = test_data['GarageArea'].fillna(test_data['MasVnrArea'].mean())\ntest_data['BsmtUnfSF'] = test_data['BsmtUnfSF'].fillna(test_data['MasVnrArea'].mean())\ntest_data['BsmtFinSF2'] = test_data['BsmtFinSF2'].fillna(test_data['MasVnrArea'].mean())\ntest_data['TotalBsmtSF'] = test_data['TotalBsmtSF'].fillna(test_data['MasVnrArea'].mean())\ntest_data['BsmtFinSF1'] = test_data['BsmtFinSF1'].fillna(test_data['MasVnrArea'].mean())\nmiss_cont","bd299fb0":"print('Residual NanN in Train:',train_data.isna().sum().sum())\nprint('Residual NanN in Test:',test_data.isna().sum().sum())","b17e1e36":"#Check if something went wrong with columns until now\nprint(f'Train Data rows: {train_data.shape[0]} \\nTrain Data Columns: {train_data.shape[1]}\\n')\nprint(f'Test Data rows: {test_data.shape[0]} \\nTest Data Columns: {test_data.shape[1]}')","1a5e8317":"fig,ax = plt.subplots(12,2, figsize=(26,80))\ni = 0\n\nlog_t_train = [  'ScreenPorch', 'LotFrontage', 'EnclosedPorch', 'BsmtFinSF2', 'OpenPorchSF',\n                 'WoodDeckSF', 'MasVnrArea', '2ndFlrSF', 'SalePrice', '1stFlrSF',\n                 'GrLivArea', 'LotArea']\n\nfor col in log_t_train: \n    sns.histplot(data=train_data[train_data_cont_var], \n            x=col,\n            kde=True,\n            ax=ax[i,0]).set_title(f'Original {col}, skew: {round(train_data[train_data_cont_var][col].skew(),4)}',\n                                fontsize = 20)\n\n    sns.histplot(data=np.log1p(train_data[train_data_cont_var]), \n            x=col,\n            kde=True,\n            ax=ax[i,1]).set_title(f'Log transformed {col}, skew: {round(np.log1p(train_data[train_data_cont_var][col]).skew(),4)}',\n                                fontsize = 20)\n    i=i+1\n    ","867885eb":"train_data[log_t_train] = np.log1p(train_data[log_t_train])","ee1b387d":"fig,ax = plt.subplots(11,2, figsize=(26,80))\ni = 0\n\nlog_t_test = [  'ScreenPorch', 'LotFrontage', 'EnclosedPorch', 'BsmtFinSF2', 'OpenPorchSF',\n                 'WoodDeckSF', 'MasVnrArea', '2ndFlrSF', '1stFlrSF',\n                 'GrLivArea', 'LotArea']\n\nfor col in log_t_test: \n    sns.histplot(data=test_data[test_data_cont_var], \n            x=col,\n            kde=True,\n            ax=ax[i,0]).set_title(f'Original {col}, skew: {round(test_data[test_data_cont_var][col].skew(),4)}',\n                                fontsize = 20)\n\n    sns.histplot(data=np.log1p(test_data[test_data_cont_var]), \n            x=col,\n            kde=True,\n            ax=ax[i,1]).set_title(f'Log transformed {col}, skew: {round(np.log1p(test_data[test_data_cont_var][col]).skew(),4)}',\n                                fontsize = 20)\n    i=i+1","384b775a":"test_data[log_t_test] = np.log1p(test_data[log_t_test])","8b2cbeba":"#Check if something went wrong with columns until now\nprint(f'Train Data rows: {train_data.shape[0]} \\nTrain Data Columns: {train_data.shape[1]}\\n')\nprint(f'Test Data rows: {test_data.shape[0]} \\nTest Data Columns: {test_data.shape[1]}')\n\nprint('\\nResidual NanN in Train:',train_data.isna().sum().sum())\nprint('Residual NanN in Test:',test_data.isna().sum().sum())","483652f5":"# This function labels columns and returns the modified data frame\ndef ptp(col, df):\n    \n    le = preprocessing.LabelEncoder()\n    ptp_corr = dict()\n    \n    for name in col:\n        le.fit(df[name].ravel().astype('str'))\n        k = name  \n        c = dict()\n        \n        for el in le.classes_:\n            c[el] = int(le.transform(np.asarray(el).ravel()))\n        \n        ptp_corr[k] = c\n        df[name] = le.transform(df[name].ravel().astype('str'))\n    \n    return ptp_corr, df\n\nmap_p2p1, train_data_lab_enc = ptp(train_data_disc_var, train_data)\nmap_p2p2, test_data_lab_enc = ptp(test_data_disc_var, test_data)\n\n#Check if something went wrong with columns until now\nprint(f'Train Data rows: {train_data_lab_enc.shape[0]} \\nTrain Data Columns: {train_data_lab_enc.shape[1]}\\n')\nprint(f'Test Data rows: {test_data_lab_enc.shape[0]} \\nTest Data Columns: {test_data_lab_enc.shape[1]}')","3c2edb6b":"# Features Encoding\ndef var_encoding(train, test):\n    # Solving the problem of discrepancies between Train and test\n    train['Origin']='TrainData'\n    test['Origin']='TestData'\n    comprehensive = pd.concat([train, test])\n    \n    dummy = train_data_disc_var\n    \n    comprehensive = pd.get_dummies(comprehensive, columns=dummy)\n    \n    new_train = comprehensive[comprehensive['Origin']=='TrainData']\n    new_train.drop('Origin', inplace = True, axis = 1)\n    new_test = comprehensive[comprehensive['Origin']=='TestData']\n    new_test.drop('Origin', inplace = True, axis = 1)\n    new_test.drop('SalePrice', inplace = True, axis = 1)\n    \n    if set(new_train.columns) - set(dummy) == set(new_train.columns):\n        print('Original columns dropped from train_data!')\n    else:\n        print('Some original columns are still present in train_data, please check!')\n        \n    if set(new_test.columns) - set(dummy) == set(new_test.columns):\n        print('Original columns dropped from test_data!\\n')\n    else:\n        print('Some original columns are still present in test_data, please check!\\n')\n    \n    return new_train, new_test\n\nnew_train, new_test = var_encoding(train_data_lab_enc, test_data_lab_enc)\n\n#Check if something went wrong with columns until now\nprint(f'Train Data rows: {new_train.shape[0]} \\nTrain Data Columns: {new_train.shape[1]}\\n')\nprint(f'Test Data rows: {new_test.shape[0]} \\nTest Data Columns: {new_test.shape[1]}')","d7e0a152":"new_train.reset_index(inplace = True)\nidx = new_train['Id']\nnew_train = new_train.drop(['Id'], axis = 1)\n\nX = new_train.drop(['SalePrice'], axis = 1)\ny = new_train['SalePrice']\n","745126b4":"def scaling_feat(train_set, test_set):\n    print(f'Dimensions before scaling: \\ntrain_set: {train_set.shape} \\ntest_set: {test_set.shape}')\n    \n    scaler = StandardScaler()\n\n    train_set_scaled = scaler.fit_transform(train_set)\n    test_set_scaled = scaler.transform(test_set)\n\n    train_set = pd.DataFrame(train_set_scaled, index=train_set.index, columns=train_set.columns)\n    test_set = pd.DataFrame(test_set_scaled, index=test_set.index, columns=test_set.columns)\n    \n    print(f'\\nDimensions after scaling: \\ntrain_set: {train_set.shape} \\ntest_set: {test_set.shape}')\n    \n    return train_set, test_set\n\ntrain_set, test_set = scaling_feat(X, new_test)","2961e9c3":"train_x, validation_x, train_y, validation_y = train_test_split(train_set, \n                                                                y, \n                                                                test_size=0.1,\n                                                                random_state=1505)\n\ntrain_x.columns = train_set.columns\nvalidation_x.columns = train_set.columns","ee1db472":"# Preforming a Grid Search to find the best combination of parameters\n\ngrid = {'iterations': [5000, 10000],\n        'learning_rate': [0.1, 0.05, 0.025],\n        'depth': [2, 3, 6],\n        'l2_leaf_reg': [0.1, 0.25, 0.5]}\n\nfinal_model = CatBoostRegressor(logging_level = 'Silent',\n                                od_type = 'Iter', \n                                od_wait = 100)\n\ngscv = GridSearchCV(estimator = final_model, param_grid = grid, scoring = 'neg_root_mean_squared_error', cv = 5)\n\n# Fitting the model\ngscv.fit(train_x, train_y)\n\n# Estimator with the best performance\nprint(gscv.best_estimator_)\n\n# Best score\nprint(gscv.best_score_)\n\n# Returns the best parameters\nprint(gscv.best_params_)","826e3a0d":"# Cat-Boost Regressor Validation\nparams = gscv.best_params_\n         \ncat = CatBoostRegressor(**params,\n                        random_seed = 1505)\n\ncat_model = cat.fit(train_x,\n                    train_y,\n                    plot = False,\n                    verbose = False)\n\ncatf_pred = cat_model.predict(validation_x)\ncatf_RMSE_score = mean_squared_error(validation_y, catf_pred, squared = True)\ncatf_RMSE_score","bc528d97":"train_p = Pool(train_x)\nval_p = Pool(validation_x)\n\nexplainer = shap.TreeExplainer(cat_model) # insert your model\nshap_values = explainer.shap_values(train_p) # insert your train Pool object\n\nshap.initjs()\nshap.summary_plot(shap_values, train_x)","ef19640e":"# Test CSV Submission\n\ntest_pred = cat_model.predict(test_set)\nsubmission = pd.DataFrame(test_set.index, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","3bbff220":"```{python}\ndef objective(trial, data = X, target = y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, \n                                                        target, \n                                                        test_size=0.2,\n                                                        random_state=1505)\n   \n    param = {\n        'metric': 'rmse', \n        'random_state': 1505,\n        'n_estimators': 20000,\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    \n    est = lgb.early_stopping(1000, first_metric_only=False, verbose=True)\n    logm = lgb.log_evaluation(period=10000, show_stdv=True)\n    \n    model = LGBMRegressor(**param)  \n    model.fit(train_x, \n              train_y,\n              eval_set = [(test_x,test_y)],\n              callbacks = [est, logm])\n    \n    preds = model.predict(test_x)    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse\n```","16b734f2":"```{python}\nparams['cat_smooth'] = params.pop('min_data_per_groups')\n```","5223f612":"## test_data","84129a6e":"<div style=\"display:fill;\n            border-radius:10px;\n            background-color:#246be3;\n            font-family:Verdana;\n            letter-spacing:1px;\n            border: 2px solid #002a6e; \n            text-align:center;\n            color:white; \n            font-size:120%\">\n\n<h2>Feature Enineering<br> \ud83e\uddd1\u200d\ud83c\udfed<\/h2> <\/div>","eaaa88f6":"{'depth': 3, 'iterations': 5000, 'l2_leaf_reg': 0.25, 'learning_rate': 0.025}","77184e6c":"# 1.2 Test Data Visualization","3c36ab6e":"***\n# 3.1 Catboost","9c898688":"```{python}\nparams = study.best_params   \nparams['random_state'] = 1505\nparams['n_estimators'] = 20000 \nparams['metric'] = 'rmse'\n```","d3b98574":"```{python}\nprint(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")\n```","f827fd17":"<div style=\"display:fill;\n            border-radius:10px;\n            background-color:#246be3;\n            font-family:Verdana;\n            letter-spacing:1px;\n            border: 2px solid #002a6e; \n            text-align:center;\n            color:white; \n            font-size:120%\">\n\n<h2>Libraries and Settings<br> \ud83d\udcd3<\/h2> <\/div>\n","0f4dfd28":"# 2.1 Imputing NaN\n\n## train_data","82aad01e":"# 2.3 Variable encoding","fd06ac98":"<h2>\n    <p style=\"text-align:center; font-family:Verdana; letter-spacing:0.5px; font-size:120%\"> Predict sales prices and practice feature engineering, RFs, and gradient boosting \n    <\/p>\n<\/h2> \n    \n\n<center>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png\"> \n<\/center>\n\n<br><br>\n\n**With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. Resources:**\n\n* `train.csv` - the training set\n* `test.csv` - the test set\n* `data_description.txt` - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n* `sample_submission.csv` - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms","74264511":"# 2.2 Transforming Skewed Distributions","56fe65fd":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Ex.2: in Train Electrical has Mix and nan, while in Test these observations are not present.<\/div>","ebe79e8e":"# 1.3 Exploring High Correlations with `SalePrice`","83618799":"```{python}\noptuna.logging.set_verbosity(optuna.logging.ERROR)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n```","8d4d2600":"# 1.1 Train Data Visualization","2e5b8c86":"<div style=\"display:fill;\n            border-radius:10px;\n            background-color:#246be3;\n            font-family:Verdana;\n            letter-spacing:1px;\n            border: 2px solid #002a6e; \n            text-align:center;\n            color:white; \n            font-size:120%\">\n\n<h2>Model training<br> \ud83e\uddd1\u200d\ud83c\udfeb<\/h2> <\/div>","d6f19dda":"# 3.2 LGBMRegressor alternative model with Optuna ","4d845de1":"<h1>\n    <p style=\"text-align:center; font-size:180%\"> House Prices - Advanced Regression Techniques <br><br> \ud83c\udfe1<\/p> \n<\/h1>","1adbb96d":"```{python}\noutput.to_csv('submission.csv', index=False)\n```","798ea905":"```{python}\npreds = np.zeros(new_test.shape[0])\n\nkf = KFold(n_splits=10,\n                     random_state=1505,\n                     shuffle=True)\n\nrmse=[]  # list contains rmse for each fold\nn=0\n\nest = lgb.early_stopping(500, first_metric_only=False, verbose=True)\nlogm = lgb.log_evaluation(period=5000, show_stdv=True)\n\nfor trn_idx, test_idx in kf.split(X, y):\n    X_tr,X_val=X.iloc[trn_idx],X.iloc[test_idx]\n    y_tr,y_val=y.iloc[trn_idx],y.iloc[test_idx]\n    model = LGBMRegressor(**params)\n    model.fit(X_tr,\n              y_tr,\n              eval_set=[(X_val,y_val)],\n              callbacks = [est, logm])\n    preds+=model.predict(new_test)\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1\n\n```","33a55d2b":"## Test Data","9c34378c":"<div style=\"display:fill;\n            border-radius:10px;\n            background-color:#246be3;\n            font-family:Verdana;\n            letter-spacing:1px;\n            border: 2px solid #002a6e; \n            text-align:center;\n            color:white; \n            font-size:120%\">\n\n<h2>Data Loading and EDA <br> \ud83e\udd13<\/h2> <\/div>\n","9aa42a97":"```{python}\n# Save predictions to file\noutput = pd.DataFrame({'Id': test_set.index,\n                       'SalePrice': np.exp(preds)})\n\n# Check format\noutput.head()\n```","eb314f0f":"## Train Data","a2a8d6da":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Ex.1: in Train Utilities has NoSeWa, while in Test Utilities has nan.<\/div>"}}