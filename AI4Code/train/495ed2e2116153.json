{"cell_type":{"9693aeab":"code","dc4e908d":"code","d09bf897":"code","26e97484":"code","ce256dde":"code","1e53b5a6":"code","2ca25c64":"code","bbaa314a":"code","07efe574":"code","2f16f301":"code","efffd7cc":"code","4f6a1abc":"code","22a08ed7":"code","6692c94a":"code","0046673f":"code","0d025547":"code","fb74a0d4":"code","225768a9":"code","9c115c4b":"code","2f30eb16":"code","d83cd0f7":"code","4b3ca5ee":"code","f5ac6010":"code","b8fbebda":"code","4924f78f":"code","7ec49f17":"code","67448727":"code","f76f9c3e":"code","b496e406":"code","5e32d5bb":"code","073225b9":"code","9050a1db":"code","b214ce49":"code","007f8eb0":"code","ac799bb9":"code","c96f6296":"code","d467551e":"code","74afa4c3":"code","de963072":"code","d9d88bf6":"code","5ff3f976":"code","8a3679c4":"code","76dd4fa7":"code","c5325ac1":"code","11715129":"code","f1eb5e58":"code","107a8428":"code","671262cd":"markdown"},"source":{"9693aeab":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns","dc4e908d":"df=pd.read_csv(\"..\/input\/all-the-data-set-needed\/trainhacker1.csv\")","d09bf897":"df.head()","26e97484":"df.tail()","ce256dde":"df.shape# gets the number of rows and columns of a data set","1e53b5a6":"df.info()# gives all the information regarding the dataset","2ca25c64":"df.isnull().sum()#check the null values present","bbaa314a":"\n df['Deal_value'] = df['Deal_value'].str.replace('$','')# replace $  in deal values","07efe574":"df['Weighted_amount'] = df['Weighted_amount'].str.replace('$','')# replace $  in Weighted_amount ","2f16f301":" df['Deal_value'] =df['Deal_value'].astype(float)#to convert to float datatype","efffd7cc":"df['Deal_value']=df['Deal_value'].fillna(df['Deal_value'].mean())#then fill null values with mean","4f6a1abc":"df['Weighted_amount'] =df['Weighted_amount'].astype(float)\ndf['Weighted_amount']=df['Weighted_amount'].fillna(df['Weighted_amount'].mean())#similar for weighted_amount","22a08ed7":"df.isnull().sum()","6692c94a":"sns.pairplot(df)# pairplot that gives the relation between different numerical varibles","0046673f":"corre=df.corr()            #check for correlation :we can seee that deal value and weighted amount are highly correlated and hence any one can be removed if needed\nsns.heatmap(corre)","0d025547":"df.corr()","fb74a0d4":"df.drop([\"Lead_name\",\"Industry\",\"Date_of_creation\",\"Contact_no\",\"POC_name\",\"Designation\",\"Lead_POC_email\",\"Hiring_candidate_role\",\"Lead_source\",\"Last_lead_update\",\"Internal_POC\",\"Resource\"], axis=1, inplace=True)","225768a9":"m=df.groupby(\"Level_of_meeting\")","9c115c4b":"m[\"Success_probability\"].mean()","2f30eb16":"n=df.groupby(\"Geography\")\nn[\"Success_probability\"].count()","d83cd0f7":"n[\"Success_probability\"].mean()","4b3ca5ee":"o=df.groupby(\"Fund_category\")\no[\"Success_probability\"].count()\n","f5ac6010":"o[\"Success_probability\"].mean()","b8fbebda":"o=df.groupby(\"Pitch\")\no[\"Success_probability\"].mean()\n","4924f78f":"df=pd.concat([df,pd.get_dummies(df[\"Pitch\"])],axis=1)\ndf=pd.concat([df,pd.get_dummies(df[\"Fund_category\"])],axis=1)\ndf=pd.concat([df,pd.get_dummies(df[\"Level_of_meeting\"])],axis=1)\n","7ec49f17":"df.drop(\"Pitch\",axis=1,inplace=True)\ndf.drop([\"Fund_category\",\"Level_of_meeting\"],axis=1,inplace=True)","67448727":"df.drop([\"Product_1\",\"Category 4\",\"Level 1\"],axis=1,inplace=True)","f76f9c3e":"df","b496e406":"cor=df.corr()\nsns.heatmap(cor)","5e32d5bb":"sns.heatmap(df.isnull(), yticklabels=False, vmin=0, vmax=1)","073225b9":"#from the data set we can see that locations in US have their stated in uppercase at the end \ndf[['Place', 'State']] = df['Location'].str.split(',', 1, expand=True)#split location to place and state\ndf['State'].fillna('0', inplace = True)\ndf['Geography'].fillna('USA', inplace = True)\ndf.loc[df['State'] == '0', 'Geography'] = 'India'\ndf.drop(['Place', 'State','Location'], axis = 1, inplace = True)","9050a1db":"df.isnull().sum()","b214ce49":"l=[i for i in df.columns if df[i].dtype==\"object\" and i!=\"Deal_title\"]\nl","007f8eb0":"from sklearn.preprocessing import LabelEncoder#label encoding\nfor i in l :\n    \n    df[i] = df[i].astype('category')\nlabel_encoder = LabelEncoder()\nfor col in l:\n    \n    df[col] = label_encoder.fit_transform(df[col])","ac799bb9":"X_train=df.drop(['Success_probability','Deal_title'],axis=1)","c96f6296":"y_train=df['Success_probability']","d467551e":"!pip install xgboost","74afa4c3":"import xgboost\nregressor=xgboost.XGBRegressor()","de963072":"booster=['gbtree','gblinear']\nbase_score=[0.25,0.5,0.75,1]","d9d88bf6":"n_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","5ff3f976":"from sklearn.model_selection import RandomizedSearchCV\nrandom_cv = RandomizedSearchCV(estimator= regressor,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","8a3679c4":"regressor=xgboost.XGBRegressor(base_score=0.75, booster='gblinear', colsample_bylevel=None,\n       colsample_bynode=None, colsample_bytree=None, gamma=None, gpu_id=-1,\n       importance_type='gain', interaction_constraints=None,\n       learning_rate=0.1, max_delta_step=None, max_depth=10,\n       min_child_weight=4, monotone_constraints=None,\n       n_estimators=500, n_jobs=12, num_parallel_tree=None,\n       objective='reg:squarederror', random_state=0, reg_alpha=0,\n       reg_lambda=0, scale_pos_weight=1, subsample=None, tree_method=None,\n       validate_parameters=1, verbosity=None)","76dd4fa7":"regressor.fit(X_train,y_train)","c5325ac1":"df_test=pd.read_csv('..\/input\/testdata-after-feature-enginerring\/hackerearthcompetitiontest2.csv')\n","11715129":"y_pred=regressor.predict(df_test)\ny_pred","f1eb5e58":"pred=pd.DataFrame(y_pred)\nsub_df=pd.read_csv('..\/input\/all-the-data-set-needed\/testhacker1.csv')\ndatasets=pd.concat([sub_df['Deal_title'],pred],axis=1)\ndatasets.columns=['Deal_title','Success_probability']\ndatasets.to_csv('sample2.csv',index=False)","107a8428":"from sklearn.metrics import accuracy_score\n","671262cd":"**Grouping with different categorical variables so that the mean sum etc of a particular group can be seen**"}}