{"cell_type":{"52d69ccc":"code","ccce4da1":"code","d0f1cdf3":"code","15c15330":"code","b8c05222":"code","00d815c9":"code","0897f11e":"code","86f7bb3f":"code","8312b0ee":"code","ddafaffd":"code","d7a24e04":"code","c1f5d59f":"code","908d6e7f":"code","41d2ac3d":"code","2a8a2c42":"code","7f1f8c49":"code","72c70150":"code","492a9104":"code","be718665":"code","feb1a491":"code","248d8c69":"code","b98521f5":"code","ef86887e":"code","8eaef3e6":"code","833bf850":"code","8924d06b":"code","0cc82e1b":"code","da2947c0":"code","41feb8ba":"code","f2079332":"code","b0f17853":"code","df197b36":"code","28bbb43f":"code","4f5dd972":"code","2ae4138c":"code","d29b29b2":"code","70bbc2b6":"code","1932a8e9":"code","2f721c9b":"code","b70a2e8c":"code","83a85471":"code","031384a0":"code","e66d8c09":"code","14dd59fd":"code","0972b9cd":"code","00fdf455":"code","cf368592":"code","1145259c":"code","fbdf7a4b":"code","41b2b307":"code","f206f2ff":"code","6234ee32":"code","bff4ae87":"code","91117266":"code","06fa740a":"code","e2b66455":"code","1f1b2f37":"code","c71b42af":"code","344c6054":"code","9b694229":"code","1a8a9636":"code","cf7800f3":"code","24f03a0f":"code","9af37e52":"code","9f831bf8":"code","1a06e63f":"code","78390c9c":"code","8207ccb8":"code","d69049a9":"code","76b6ac42":"code","79d96245":"code","36d25669":"code","7c19c5e3":"code","ca5802cc":"code","1b516fb9":"code","08fd2f40":"code","007d0117":"code","ea6cbce7":"code","ef3d4a05":"code","82674561":"code","8c53d7f1":"code","43e68657":"code","aa94c90b":"code","3b3eb282":"code","d00e64e5":"code","d67acd34":"code","cb980338":"code","e9a616eb":"code","ff1b873f":"code","1700597d":"code","9874e1ce":"code","a5eca000":"code","2bf56884":"code","b824d994":"code","95ee7c27":"code","dc422d85":"code","157db756":"code","3f2ac8ea":"code","4be78717":"code","2843033b":"code","349ee31a":"code","27c569e7":"code","6bce429b":"code","cdefa164":"code","cbb02cf3":"code","8c8fdd1a":"code","aeef79ea":"code","20f13446":"code","77ae1b2c":"code","051d45a8":"code","2fdca48c":"code","7ae33bd2":"code","aeb51ff9":"code","0fde6a60":"code","99903966":"code","4532097d":"code","28d7803d":"code","79e0294c":"code","1d62d8d8":"code","1b27066f":"code","6c39358f":"code","154cb1b3":"code","11a455e4":"code","2edce6d8":"code","473cc2ae":"code","a5631ea5":"markdown","5acbb53a":"markdown","5988c8cb":"markdown","00ab81b7":"markdown","5c29ea62":"markdown","73aa4641":"markdown","872d171b":"markdown","b93dbf8e":"markdown","ffe3a9bf":"markdown","a01b8713":"markdown","142113d3":"markdown","8adfbff4":"markdown","317e34a1":"markdown","473ae4e7":"markdown","8869529b":"markdown","2c9965b1":"markdown","9f5cace4":"markdown","9f46edfe":"markdown","1fd0d1b9":"markdown","0b02b0c0":"markdown","29611701":"markdown","3495fa17":"markdown","99aa907f":"markdown","e9428e6e":"markdown","50ee3d5e":"markdown","b882180f":"markdown","998dd81a":"markdown","eb07bac8":"markdown","adbee2b9":"markdown","6ca6caa0":"markdown","8b531337":"markdown","e870cb6d":"markdown","cb67bc4e":"markdown","106554d5":"markdown","906c7c71":"markdown"},"source":{"52d69ccc":"from sklearn.metrics import make_scorer, accuracy_score,auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom scipy.stats import norm\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nfrom scipy import stats\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\n","ccce4da1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d0f1cdf3":"warnings.filterwarnings('ignore')\n%matplotlib inline","15c15330":"df_train=pd.read_csv('\/kaggle\/input\/advertsuccess\/Train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/advertsuccess\/Test.csv')","b8c05222":"df_train.head()","00d815c9":"#function for missing data\ndef missing_data(df_train):\n    total = df_train.isnull().sum().sort_values(ascending=False)\n    percent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return(missing_data.head(20))","0897f11e":"missing_data(df_train)","86f7bb3f":"df_train.describe()","8312b0ee":"df_train.dtypes","ddafaffd":"df_train['netgain']=df_train['netgain'].astype('str')","d7a24e04":"df_train.netgain[df_train.netgain == 'True'] = 1\ndf_train.netgain[df_train.netgain == 'False'] = 0","c1f5d59f":"df_train['netgain']=df_train['netgain'].astype('int')","908d6e7f":"df_train=df_train[df_train.airlocation != 'Holand-Netherlands']","41d2ac3d":"df_train.airlocation.value_counts()","2a8a2c42":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(corrmat, vmax=.8, square=True);","7f1f8c49":"plt.figure(figsize=(25,20))\nsns.factorplot(data=df_train,x='netgain',y='ratings',hue='genre')","72c70150":"plt.figure(figsize=(15,10))\nsns.catplot(x='realtionship_status',y='ratings',data=df_train,hue='netgain',height=5,aspect=3,kind='box')\nplt.title('boxplot')","492a9104":"plt.figure(figsize=(15,10))\nsns.relplot(x='ratings', y='average_runtime(minutes_per_week)', data=df_train,\n            kind='line', hue='netgain', col='expensive')","be718665":"plt.figure(figsize=(15,15))\nsns.relplot(x='ratings', y='average_runtime(minutes_per_week)', data=df_train,\n            kind='line')","feb1a491":"plt.figure(figsize=(10,10))\nsns.catplot(x='realtionship_status',y='average_runtime(minutes_per_week)',data=df_train)","248d8c69":"plt.figure(figsize=(10,10))\nsns.catplot(x='realtionship_status',y='average_runtime(minutes_per_week)',hue='netgain',data=df_train)","b98521f5":"plt.figure(figsize=(10,4))\nsns.countplot(x='industry',hue='netgain',data=df_train,order=df_train['industry'].value_counts().sort_values().index);","ef86887e":"plt.figure(figsize=(10,4))\nsns.countplot(x='targeted_sex',data=df_train,order=df_train['targeted_sex'].value_counts().sort_values().index,hue=df_train.netgain);","8eaef3e6":"plt.figure(figsize=(10,4))\nsns.countplot(x='genre',hue='netgain',data=df_train,order=df_train['genre'].value_counts().sort_values().index);","833bf850":"plt.figure(figsize=(15,7))\nsns.countplot(x='expensive',hue='genre',data=df_train,order=df_train['expensive'].value_counts().sort_values().index);","8924d06b":"plt.figure(figsize=(11,5))\nsns.countplot(x='genre',hue='netgain', data=df_train,palette=\"Set1\")\nplt.xlabel('Genre')\nplt.ylabel('Count')\nplt.title('Genre Wise Netgain')\nplt.show()","0cc82e1b":"plt.rcParams['figure.figsize'] = (10, 5)\nsns.violinplot(df_train['airtime'], df_train['netgain'], palette = 'rainbow')\nplt.title('Airtime vs Netgain Score', fontsize = 20)\nplt.show()","da2947c0":"plt.rcParams['figure.figsize'] = (10, 5)\nsns.violinplot(df_train['expensive'], df_train['netgain'], palette = 'rainbow')\nplt.title('expensive vs Netgain Score', fontsize = 20)\nplt.show()","41feb8ba":"plt.figure(figsize=(10,5))\nsns.distplot(df_train['average_runtime(minutes_per_week)'])\nplt.show()","f2079332":"sns.countplot(df_train.airtime,hue=df_train.netgain)","b0f17853":"def scatterplot(x_data, y_data, x_label=\"\", y_label=\"\", title=\"\", color = \"r\", yscale_log=False):\n\n    # Create the plot object\n    _, ax = plt.subplots()\n\n    # Plot the data, set the size (s), color and transparency (alpha)\n    # of the points\n    ax.scatter(x_data, y_data, s = 10, color = color, alpha = 0.75)\n\n    if yscale_log == True:\n        ax.set_yscale('log')\n\n    # Label the axes and provide a title\n    ax.set_title(title)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)","df197b36":"scatterplot(df_train.genre,df_train.ratings,x_label=\"genre\",y_label='airtime',color='blue',yscale_log=False)","28bbb43f":"fig, ax = plt.subplots()\nax.scatter(x = df_train['ratings'], y = df_train['airlocation'])\nplt.ylabel('Airlocation')\nplt.xlabel('Ratings')\nplt.show()","4f5dd972":"# Plotting\nsns.catplot(x='airtime', y='average_runtime(minutes_per_week)', data=df_train, kind='boxen', aspect=2)\nplt.title('Boxen Plot', weight='bold', fontsize=16)\nplt.show()","2ae4138c":"sns.factorplot(data=df_train,x='industry',y='average_runtime(minutes_per_week)')\nplt.title('Factor Plot', weight='bold', fontsize=16)\nplt.show()","d29b29b2":"# Plotting\nsns.catplot(x='expensive', y='ratings', data=df_train, kind='boxen', aspect=2)\nplt.title('Boxen Plot', weight='bold', fontsize=16)\nplt.show()","70bbc2b6":"sns.set()\ncols = ['realtionship_status','industry','genre','targeted_sex','average_runtime(minutes_per_week)','airtime','airlocation','ratings','expensive','money_back_guarantee','netgain']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show()","1932a8e9":"encoded = pd.get_dummies(df_train)","2f721c9b":"encoded.head()","b70a2e8c":"encoded.columns","83a85471":"dependent_all=encoded['netgain']","031384a0":"independent_all=encoded.drop(['id','netgain'],axis=1)","e66d8c09":"x_train,x_test,y_train,y_test=train_test_split(independent_all,dependent_all,test_size=0.3,random_state=100)","14dd59fd":"log =LogisticRegression()\nlog.fit(x_train,y_train)","0972b9cd":"#model on train using all the independent values in df\nlog_prediction = log.predict(x_train)\nlog_score= accuracy_score(y_train,log_prediction)\nprint('Accuracy score on train set using Logistic Regression :',log_score)","00fdf455":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train, log_prediction)","cf368592":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_train,log_prediction)\nprint(\"AUC on train using Logistic Regression :\",metrics.auc(fpr, tpr))","1145259c":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_train, log_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","fbdf7a4b":"from sklearn.metrics import recall_score\nprint('recall_score on train set :',recall_score(y_train, log_prediction))","41b2b307":"from sklearn.metrics import f1_score\nprint('F1_sccore on train set :',f1_score(y_train, log_prediction))","f206f2ff":"#model on train using all the independent values in df\nlog_prediction = log.predict(x_test)\nlog_score= accuracy_score(y_test,log_prediction)\nprint('accuracy score on test using Logisitic Regression :',log_score)","6234ee32":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, log_prediction)","bff4ae87":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test,log_prediction)\nprint(\"AUC on test using Logistic Regression :\",metrics.auc(fpr, tpr))","91117266":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, log_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","06fa740a":"from sklearn.metrics import recall_score\nprint('recall_score on train set :',recall_score(y_test, log_prediction))","e2b66455":"from sklearn.metrics import f1_score\nprint('F1_sccore on train set :',f1_score(y_test, log_prediction))","1f1b2f37":"from sklearn.model_selection import cross_val_score\nlr = LogisticRegression()\nscores = cross_val_score(lr, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","c71b42af":"xgboost = xgb.XGBClassifier(max_depth=3,n_estimators=300,learning_rate=0.05)","344c6054":"xgboost.fit(x_train,y_train)","9b694229":"#XGBoost model on the train set\nXGB_prediction = xgboost.predict(x_train)\nXGB_score= accuracy_score(y_train,XGB_prediction)\nprint('accuracy score on train using XGBoost ',XGB_score)","1a8a9636":"confusion_matrix(y_train, XGB_prediction)","cf7800f3":"fpr, tpr, thresholds = metrics.roc_curve(y_train,XGB_prediction)\nprint(\"AUC on train using XGBoost :\",metrics.auc(fpr, tpr))","24f03a0f":"average_precision = average_precision_score(y_train, XGB_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","9af37e52":"print('recall_score on train set :',recall_score(y_train, XGB_prediction))","9f831bf8":"print('F1_sccore on train set :',f1_score(y_train, XGB_prediction))","1a06e63f":"#XGBoost model on the test\nXGB_prediction = xgboost.predict(x_test)\nXGB_score= accuracy_score(y_test,XGB_prediction)\nprint('accuracy score on test using XGBoost :',XGB_score)","78390c9c":"confusion_matrix(y_test, XGB_prediction)","8207ccb8":"fpr, tpr, thresholds = metrics.roc_curve(y_test,XGB_prediction)\nprint(\"AUC on test using XGBoost :\",metrics.auc(fpr, tpr))","d69049a9":"average_precision = average_precision_score(y_test, XGB_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","76b6ac42":"print('recall_score on test set :',recall_score(y_test, XGB_prediction))","79d96245":"print('F1_sccore on test set :',f1_score(y_test, XGB_prediction))","36d25669":"xg = xgb.XGBClassifier()\nscores = cross_val_score(xg, x_test, y_test, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","7c19c5e3":"rfc2=RandomForestClassifier(n_estimators=300)\nrfc2.fit(x_train,y_train)","ca5802cc":"#model on train using all the independent values in df\nrfc_prediction = rfc2.predict(x_train)\nrfc_score= accuracy_score(y_train,rfc_prediction)\nprint('accuracy Score on train using RandomForest :',rfc_score)","1b516fb9":"confusion_matrix(y_train, rfc_prediction)","08fd2f40":"fpr, tpr, thresholds = metrics.roc_curve(y_train,rfc_prediction)\nprint(\"AUC on train using RandomForest :\",metrics.auc(fpr, tpr))","007d0117":"average_precision = average_precision_score(y_train, rfc_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","ea6cbce7":"print('recall_score on train set :',recall_score(y_train, rfc_prediction))","ef3d4a05":"print('F1_sccore on train set :',f1_score(y_train, rfc_prediction))","82674561":"#model on test using all the indpendent values in df\nrfc_prediction = rfc2.predict(x_test)\nrfc_score= accuracy_score(y_test,rfc_prediction)\nprint('accuracy score on test using RandomForest ',rfc_score)","8c53d7f1":"confusion_matrix(y_test, rfc_prediction)","43e68657":"fpr, tpr, thresholds = metrics.roc_curve(y_test,rfc_prediction)\nprint(\"AUC on test using RandomForest :\",metrics.auc(fpr, tpr))","aa94c90b":"average_precision = average_precision_score(y_test, rfc_prediction)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","3b3eb282":"print('recall_score on test set :',recall_score(y_test, rfc_prediction))","d00e64e5":"print('F1_sccore on test set :',f1_score(y_test, rfc_prediction))","d67acd34":"lr = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(lr, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","cb980338":"dec=DecisionTreeClassifier()","e9a616eb":"dec.fit(x_train,y_train)","ff1b873f":"#model on train using all the independent values in df\ndec_prediction = dec.predict(x_train)\ndec_score= accuracy_score(y_train,dec_prediction)\nprint('Accuracy score on train using Decision Tree :',dec_score)","1700597d":"    print(confusion_matrix(y_train, dec_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,dec_prediction)\n    print(\"AUC on train using DecisionTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, dec_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, dec_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, dec_prediction))\n    ","9874e1ce":"#model on test using all the independent values in df\ndec_prediction = dec.predict(x_test)\ndec_score= accuracy_score(y_test,dec_prediction)\nprint('Accuracy Score on tree using Decision Tree  :',dec_score)","a5eca000":"    print(confusion_matrix(y_test, dec_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,dec_prediction)\n    print(\"AUC on train using DecisionTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, dec_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, dec_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, dec_prediction))\n    ","2bf56884":"lr = DecisionTreeClassifier()\nscores = cross_val_score(lr, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","b824d994":"etc=ExtraTreeClassifier()\netc.fit(x_train,y_train)","95ee7c27":"#model on train using all the independent values in df\netc_prediction = etc.predict(x_train)\netc_score= accuracy_score(y_train,etc_prediction)\netc_score","dc422d85":"    print(confusion_matrix(y_train, etc_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,etc_prediction)\n    print(\"AUC on train using ExtraTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, etc_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, etc_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, etc_prediction))\n    ","157db756":"#model on test using all the independent values in df\netc_prediction = etc.predict(x_test)\netc_score= accuracy_score(y_test,etc_prediction)\netc_score","3f2ac8ea":"    print(confusion_matrix(y_test, etc_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,etc_prediction)\n    print(\"AUC on train using ExtraTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, etc_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, dec_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, etc_prediction))\n    ","4be78717":"lr = ExtraTreeClassifier()\nscores = cross_val_score(lr, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","2843033b":"ada =AdaBoostClassifier(n_estimators=100)","349ee31a":"ada.fit(x_train,y_train)","27c569e7":"#model on train using all the independent values in df\nada_prediction = ada.predict(x_train)\nada_score= accuracy_score(y_train,ada_prediction)\nada_score","6bce429b":"    print(confusion_matrix(y_train, ada_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,ada_prediction)\n    print(\"AUC on train using AdaBoost :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, ada_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, ada_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, ada_prediction))\n    ","cdefa164":"#model on test using all the independent values in df\nada_prediction = ada.predict(x_test)\nada_score= accuracy_score(y_test,ada_prediction)\nprint('accuracy score om test using AdaBoost :',ada_score)","cbb02cf3":"    print(confusion_matrix(y_test, ada_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,ada_prediction)\n    print(\"AUC on test using AdaBoost :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, ada_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, ada_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, ada_prediction))\n    ","8c8fdd1a":"lr = AdaBoostClassifier(n_estimators=100)\nscores = cross_val_score(lr, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","aeef79ea":"bca =BaggingClassifier()\nbca.fit(x_train,y_train)\n#model on train using all the independent values in df\nbca_prediction = bca.predict(x_train)\nbca_score= accuracy_score(y_train,bca_prediction)\nprint('accuracy on train using BaggingClassifier :',bca_score)","20f13446":"    print(confusion_matrix(y_train, bca_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,bca_prediction)\n    print(\"AUC on train using BaggingClassifier :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, bca_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, bca_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, bca_prediction))\n    ","77ae1b2c":"#model on test using all the independent values in df\nbca_prediction = bca.predict(x_test)\nbca_score= accuracy_score(y_test,bca_prediction)\nprint(bca_score)","051d45a8":"    print(confusion_matrix(y_test, bca_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,bca_prediction)\n    print(\"AUC on train using Bagging Classifier :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, bca_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, bca_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, bca_prediction))\n    ","2fdca48c":"lr = BaggingClassifier(n_estimators=100)\nscores = cross_val_score(lr, x_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","7ae33bd2":"ettc=ExtraTreesClassifier()\nettc.fit(x_train,y_train)\n#model on train using all the independent values in df\nettc_prediction = ettc.predict(x_train)\nettc_score= accuracy_score(y_train,ettc_prediction)\nprint('training accuracy using Extratressclassifier',ettc_score)","aeb51ff9":"    print(confusion_matrix(y_train, ettc_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_train,ettc_prediction)\n    print(\"AUC on train using ExtraTree :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_train, ettc_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on train set :',recall_score(y_train, ettc_prediction))\n    print('F1_sccore on train set :',f1_score(y_train, ettc_prediction))\n    ","0fde6a60":"#model on test using all the independent values in df\nettc_prediction =ettc.predict(x_test)\nettc_score= accuracy_score(y_test,ettc_prediction)\nprint('testing accuracy using Extratressclassifier',ettc_score)","99903966":"    print(confusion_matrix(y_test, ettc_prediction))\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,ettc_prediction)\n    print(\"AUC on test using Extratreesclassifier :\",metrics.auc(fpr, tpr))\n    average_precision = average_precision_score(y_test, ettc_prediction)\n    print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n    print('recall_score on test set :',recall_score(y_test, ettc_prediction))\n    print('F1_sccore on test set :',f1_score(y_test, ettc_prediction))\n    ","4532097d":"nn=MLPClassifier()\nnn.fit(x_train,y_train)\nnn_prediction = nn.predict(x_train)\nnn_score= accuracy_score(y_train,nn_prediction)\nprint('accuracy score on train using MLPClassifier :',nn_score)\n#model on test using all the independent values in df\nnn_prediction =nn.predict(x_test)\nnn_score= accuracy_score(y_test,nn_prediction)\nprint('accuracy score on test using MLPClassifier :',nn_score)","28d7803d":"df_test.head()","79e0294c":"encoded = pd.get_dummies(df_test)","1d62d8d8":"encoded","1b27066f":"testing=encoded.drop(['id'],axis=1)","6c39358f":"prediction = xgboost.predict(testing)","154cb1b3":"submission=pd.DataFrame({\"id\":df_test.id,'prediction': prediction})","11a455e4":"# saving the dataframe \nsubmission.to_csv('sample_submission.csv')","2edce6d8":"import pandas as pd\n%matplotlib inline\n#do code to support model\n#\"data\" is the X dataframe and model is the SKlearn object\n\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(x_train.columns, rfc2.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n#plt.figure(figsize=(15,7))\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45,figsize=(15,7))","473cc2ae":"importances = rfc2.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rfc2.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n#plt.figure()\nplt.figure(figsize=(15,7))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), indices)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()\n","a5631ea5":"# Data Visualization\n\nData visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers of the images. This communication is achieved through the use of a systematic mapping between graphic marks and data values in the creation of the visualization","5acbb53a":"# average precision recall score\nAverage precision (AP) summarizes such a plot as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\n<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mtext>AP<\/mtext>\n  <mo>=<\/mo>\n  <munder>\n    <mo data-mjx-texclass=\"OP\">&#x2211;<\/mo>\n    <mi>n<\/mi>\n  <\/munder>\n  <mo stretchy=\"false\">(<\/mo>\n  <msub>\n    <mi>R<\/mi>\n    <mi>n<\/mi>\n  <\/msub>\n  <mo>&#x2212;<\/mo>\n  <msub>\n    <mi>R<\/mi>\n    <mrow>\n      <mi>n<\/mi>\n      <mo>&#x2212;<\/mo>\n      <mn>1<\/mn>\n    <\/mrow>\n  <\/msub>\n  <mo stretchy=\"false\">)<\/mo>\n  <msub>\n    <mi>P<\/mi>\n    <mi>n<\/mi>\n  <\/msub>\n<\/math>\n\nwhere <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msub>\n    <mi>P<\/mi>\n    <mi>n<\/mi>\n  <\/msub>\n<\/math> and  <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msub>\n    <mi>R<\/mi>\n    <mi>n<\/mi>\n  <\/msub>\n<\/math> are the precision and recall at the nth threshold. A pair  <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mo stretchy=\"false\">(<\/mo>\n  <msub>\n    <mi>R<\/mi>\n    <mi>k<\/mi>\n  <\/msub>\n  <mo>,<\/mo>\n  <msub>\n    <mi>P<\/mi>\n    <mi>k<\/mi>\n  <\/msub>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>   is referred to as an operating point.\n","5988c8cb":"<h3 style='padding: 10px'>comparison table<\/h2><table border-style:solid; class='table table-striped'> <thead> <tr> <th>Algorithm Used<\/th> <th>Accuracy Score On Train<\/th> <th>Accuracy Score On Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'>XGBoost Classifier <\/th> <td>0.82141290039491<\/td> <td>0.8207064243665216<\/td><\/tr> \n    <tr> <th scope='row'>Random Forest Classifier<\/th> <td>0.8588196577446249<\/td> <td>0.8050934220629639<\/td><\/tr> <tr> \n    <th scope='row'>Logisitic Regresion<\/th> <td>0.8040258885476086<\/td> <td>0.8033017660609163\n    <\/td><\/tr> <tr><th scope='row'>Decision Tree Classifier<\/th> <td>0.8630978499341817<\/td> <td>0.7999744049142564<\/td><\/tr>\n    <tr><th scope='row'>Extra tree classifier<\/th><td>0.8630978499341817<\/td><td>0.8001023803429741<\/td><\/tr>\n    <tr><th scope='row'>ADA boost classifier<\/th><td>0.8180122860903906<\/td><td>0.8162272843614026<\/td><\/tr>\n    <tr><th scope='row'>Bagging classifier<\/th><td>0.8593132953049584<\/td><td>0.8052213974916816<\/td><\/tr>\n    <tr><th scope='row'>ExtraTreesclassifier<\/th><td>0.8630978499341817<\/td><td>0.8004863066291272<\/td><\/tr>\n    \n        <tr><th scope='row'>NeuralNetwork(MLPClassifier)<\/th><td>0.8099495392716104<\/td><td>0.8033017660609163<\/td><\/tr>\n    <\/tbody> <\/table>","00ab81b7":"\n<h3 >comparing metries [ExtraTree]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.863097<\/td> <td>0.794983<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.764074<\/td> <td>0.6592717<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.56<\/td> <td>0.37<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.573351<\/td> <td>0.428256\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.668173<\/td> <td>0.478854<\/td><\/tr><\/table>","5c29ea62":"# confusion matrix\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.","73aa4641":"# RandomForestClassifier\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set.","872d171b":"# Neural Network(MLPclassifier)","b93dbf8e":"   # F1 Score\nCompute the F1 score, also known as balanced F-score or F-measure\n\nThe F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n\nF1 = 2 * (precision * recall) \/ (precision + recall)","ffe3a9bf":"# DecisionTreeClassifer\n\nA decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n\nDecision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.","a01b8713":"<h3 >comparing metries [Bagging Classifier]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.859258<\/td> <td>0.806117<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.76973<\/td> <td>0.68462<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.55<\/td> <td>0.40<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.59730<\/td> <td>0.45805\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.677110<\/td> <td>0.52283<\/td><\/tr><\/table>","142113d3":"# XGBoost Algorithm\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.","8adfbff4":"# Advertisement Success Dataset","317e34a1":"# Data\n\nData Description\n\nid                                 -Unique id for each row\n\nratings                            -Metric out of 1 which represents how much of the targeted demographic watched the advertisement\n\nairlocation                        -Country of origin\n\nairtime                            -Time when the advertisement was aired\n\naverage_runtime(minutes_per_week)  -Minutes per week the advertisement was aired\n\ntargeted_sex                       -Sex that was mainly targeted for the advertisement\n\ngenre                              -The type of advertisement\n\nindustry                           -The industry to which the product belonged\n\neconomic_status                    -The economic health during which the show aired\n\nrelationship_status                -The relationship status of the most responsive customers to the advertisement\n\nexpensive                          -A general measure of how expensive the product or service is that the ad is discussing.\n\nmoney_back_guarantee               -Whether or not the product offers a refund in the case of customer dissatisfaction.\n\nnetgain [target]                   -Whether the ad will incur a gain or loss when sold","473ae4e7":"# Correlation\nWhen two sets of data are strongly linked together we say they have a High Correlation.\n\nThe word Correlation is made of Co- (meaning \"together\"), and Relation\n\nCorrelation is Positive when the values increase together, and\nCorrelation is Negative when one value decreases as the other increases\nA correlation is assumed to be linear (following a line).\n\ncorrelation examples\nCorrelation can have a value:\n\n1 is a perfect positive correlation\n0 is no correlation (the values don't seem linked at all)\n-1 is a perfect negative correlation\nThe value shows how good the correlation is (not how steep the line is), and if it is positive or negative.","8869529b":"\n<h3 >comparing metries [RandomForest]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.863097<\/td> <td>0.80752<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.76914<\/td> <td>0.676682<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.56<\/td> <td>0.40<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.58818<\/td> <td>0.43267\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.673810<\/td> <td>0.51041<\/td><\/tr><\/table>","2c9965b1":"# Loading the train and test data-set using pandas.read_csv\n\n","9f5cace4":"# One_hot Encoding\nOne hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms \nto do a better job in prediction.\nThe encoding is done using pandas.get_dummies","9f46edfe":"\n<h3 >comparing metries [XGBoost]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.82141<\/td> <td>0.82070<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.69710<\/td> <td>0.686418<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.45<\/td> <td>0.43<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.45767<\/td> <td>0.43598\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.55200<\/td> <td>0.53002<\/td><\/tr><\/table>","1fd0d1b9":"# AUC \nCompute Area Under the Curve (AUC) using the trapezoidal rule","0b02b0c0":"## Train And Test Split ","29611701":"# ExtraTreeClassifier\n\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.","3495fa17":"## Task\nThis is a binary classification problem where you need to predict whether an ad will lead to a netgain","99aa907f":"# BaggingClassifier\n\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.","e9428e6e":"\n<h3 >comparing metries [AdaBoost]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.818615<\/td> <td>0.816227<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.712104<\/td> <td>0.7008396<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.45<\/td> <td>0.43<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.5069587<\/td> <td>0.485651\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.573345<\/td> <td>0.55068<\/td><\/tr><\/table>","50ee3d5e":"# LogisticRegression \n\nLogistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"),","b882180f":"# ExtraTreesClassifier\n\nExtremely Randomized Trees Classifier(Extra Trees Classifier) is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a \u201cforest\u201d to output it\u2019s classification result. In concept, it is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest.","998dd81a":"## feature Enginearing on test data ","eb07bac8":"<h3 >comparing metries [ExtraTreesClassifier]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.863097<\/td> <td>0.801254<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.764074<\/td> <td>0.663161<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.56<\/td> <td>0.38<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.573351<\/td> <td>0.405629\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.6681733<\/td> <td>0.486271<\/td><\/tr><\/table>","adbee2b9":"\n<h3 >comparing metries [Logistic Regression ]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.80402<\/td> <td>0.80330<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.6577<\/td> <td>0.6477<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.40<\/td> <td>0.38<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.3759<\/td> <td>0.35761\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.4798<\/td> <td>0.4546<\/td><\/tr><\/table>","6ca6caa0":"# Data Description:\nTrain.csv : 26049 x 12 [including headers] : training data set\n\nTest.csv : 6514 x 11 [including headers] : test data set","8b531337":"\n<h3 >comparing metries [DecisionTree]<\/h3>\n<table style=\"width:50%\"> <thead> <tr> <th>Metries <\/th> <th> Train<\/th> <th> Test<\/th><\/tr> <\/thead> <tbody> <tr> <th scope='row'> Accuracy Score <\/th> <td>0.863097<\/td> <td>0.79779<\/td><\/tr> \n    <tr><th scope='row'>AUC <\/th> <td>0.76407<\/td> <td>0.66880<\/td><\/tr><tr> <th scope='row'>Average_precision_recall_score<\/th> <td>0.56<\/td> <td>0.38<\/td><\/tr> <tr> \n    <th scope='row'>recall_score <\/th> <td>0.573351<\/td> <td>0.428256\n    <\/td><\/tr> <tr><th scope='row'>F1 Score <\/th> <td>0.668173<\/td> <td>0.49553<\/td><\/tr><\/table>","e870cb6d":"# AdaBoostClassifier\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","cb67bc4e":"# recall score\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\nThe best value is 1 and the worst value is 0.","106554d5":"#  Best Algorithm\n\nFrom the above table we get to known that XGBoost algorithm gives us the best accuracy score and produces less delta value \n\nXGBoost model is to Predict the targeted variable on the test variable ","906c7c71":"Loading the Nesscary libraries required"}}