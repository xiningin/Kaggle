{"cell_type":{"142a7755":"code","0634a727":"code","20031caa":"code","66b6077f":"code","8544be07":"code","7d96765e":"code","2094f018":"code","48bc41e3":"code","fb59ced6":"code","9410d6d2":"code","1aa63e9e":"code","2740e320":"code","3661addf":"code","295c7b25":"code","4d177fa4":"code","831b05ea":"code","a9d7dbab":"code","feb8bf17":"code","9046d468":"code","7ccc6619":"code","39173da8":"code","70ba095c":"code","53667f4c":"markdown","8e08c00c":"markdown","23ac7770":"markdown","af1eebe2":"markdown","bb2702a4":"markdown","fbd4e4d5":"markdown","b9908d8f":"markdown","54660b0d":"markdown","373d1626":"markdown","ad386b74":"markdown","d4e3cb63":"markdown","dd1f8d52":"markdown","08acf315":"markdown","7be5c24f":"markdown","4dc2cd9a":"markdown","734b7e8d":"markdown","a1c3141a":"markdown","bcf14866":"markdown","3fb2adda":"markdown","5b72d517":"markdown","6bb41226":"markdown","87d6a642":"markdown"},"source":{"142a7755":"import pandas as pd\nimport numpy as np\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#Models\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report, roc_curve\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n#Oculto los warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","0634a727":"rcParams['xtick.labelsize'] = 16\nrcParams['figure.figsize'] = (11.7,8.27)","20031caa":"df = pd.read_csv('..\/input\/heart-disease-dataset\/heart.csv')","66b6077f":"df.head()","8544be07":"df.shape","7d96765e":"df.info()","2094f018":"df.isnull().sum()","48bc41e3":"df.duplicated().sum()","fb59ced6":"continuous_features = ['age',\n                      'trestbps',\n                      'chol',\n                      'thalach',\n                      'oldpeak'\n                     ]\nfor feature in continuous_features:\n    print(str(feature)+': '+str(sum(df[feature] < 0)))\n","9410d6d2":"for cf in continuous_features:\n    plt.boxplot(df[cf], vert=False)\n    plt.title(cf)\n    plt.xlabel(xlabel = cf,\n               rotation=90)\n\n    plt.show();","1aa63e9e":"sns.heatmap(df.corr(),\n            annot=True);","2740e320":"window_age = (df['age'].max() - df['age'].min())\/4\ndf['enc_age'] = df['age'].apply(lambda x: 0 if df.loc[x,'age'] < df['age'].min()+(window_age)\\\n                                else 1 if df.loc[x,'age'] < df['age'].min()+(2*window_age)\\\n                                else 2 if df.loc[x,'age'] < df['age'].max()-(window_age)\\\n                                else 3)\n\ndf = df.drop(columns=['age'], axis=1)","3661addf":"df.groupby(by=['enc_age'])['enc_age'].count()\/len(df['enc_age'])","295c7b25":"y = df[\"target\"]\nX = df[['cp','restecg','thalach','slope','enc_age']] #Solo selecciono los que tienen coeficiente de correlaci\u00f3n mayor a 0.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","4d177fa4":"modelos=[{\"name\":\"LogisticRegression\", \"model\":LogisticRegression()},\n         {\"name\":\"KNeighborsClassifier\", \"model\":KNeighborsClassifier()},\n         {\"name\":\"DecisionTreeClassifier\", \"model\":DecisionTreeClassifier()},\n         {\"name\":\"RandomForestClassifier\", \"model\":RandomForestClassifier()},\n         {\"name\":\"SVC\", \"function\":SVC()}\n        ]","831b05ea":"best_params = []\n\nfor i in range(len(modelos)):\n    if modelos[i].get('name') == 'LogisticRegression':\n        parametros={'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                    'C': [100, 10, 1.0, 0.1, 0.01]}\n        \n        grid_search = GridSearchCV(modelos[i].get('model'), param_grid=parametros)\n        grid_search.fit(X_train, y_train)\n        best_params.append({\n            'name':modelos[i].get('name'),\n            'model': modelos[i].get('model'),\n            'best_params':grid_search.best_params_,\n            'best_score':grid_search.best_score_\n        })\n    elif modelos[i].get('name') == 'KNeighborsClassifier':\n        \n        parametros={'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n                    'n_neighbors':[5,6,8,9,10,12,15,20]}\n        \n        grid_search = GridSearchCV(modelos[i].get('model'), param_grid=parametros)\n        grid_search.fit(X_train, y_train)\n        best_params.append({\n            'name':modelos[i].get('name'),\n            'model': modelos[i].get('model'),\n            'best_params':grid_search.best_params_,\n            'best_score':grid_search.best_score_\n        })\n        \n    elif modelos[i].get('name') == 'DecisionTreeClassifier':\n        \n        parametros= {'criterion':['gini', 'entropy'], #Se cambiaron los parametros\n                    'splitter':['best', 'random']}\n        \n        grid_search = GridSearchCV(modelos[i].get('model'), param_grid=parametros)\n        grid_search.fit(X_train, y_train)\n        best_params.append({\n            'name':modelos[i].get('name'),\n            'model': modelos[i].get('model'),\n            'best_params':grid_search.best_params_,\n            'best_score':grid_search.best_score_\n        })\n    \n    elif modelos[i].get('name') == 'RandomForestClassifier':\n        \n        parametros= {'criterion':['gini', 'entropy'],\n                      'n_estimators':[1,5,10,20,50]}\n        \n        grid_search = GridSearchCV(modelos[i].get('model'), param_grid=parametros)\n        grid_search.fit(X_train, y_train)\n        best_params.append({\n            'name':modelos[i].get('name'),\n            'model': modelos[i].get('model'),\n            'best_params':grid_search.best_params_,\n            'best_score':grid_search.best_score_\n        })","a9d7dbab":"best_scores = pd.DataFrame(best_params).sort_values(by='best_score', ascending=False).reset_index(drop=True)\nbest_scores","feb8bf17":"model = best_scores.loc[0,'model']\nparams = best_scores.loc[0,'best_params']\n\nskLearn_model = model.set_params(**params)\nmodel_fit = skLearn_model.fit(X_train, y_train)\n\nmodel_predictions_train = model_fit.predict(X_train)\nmodel_predictions_test = model_fit.predict(X_test)","9046d468":"model_fit.score(X_test,y_test)","7ccc6619":"print(classification_report(y_train, model_predictions_train))","39173da8":"importance = model_fit.feature_importances_","70ba095c":"data = {\n    'Features': X_train.columns,\n    'Importance': importance\n}\npd.DataFrame(data).sort_values(by='Importance', ascending = False)","53667f4c":"**Feature importance**","8e08c00c":"**Competition**","23ac7770":"**Load dataset**","af1eebe2":"### Reporting\n**Bivariate analysis**\n\nThe graph below show the correlation between all numerical features.","bb2702a4":"# Heart Disease\n\n> This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them. The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease. \n\n[Source]('https:\/\/www.kaggle.com\/johnsmith88\/heart-disease-dataset')\n\nA heart attack (Cardiovascular diseases) occurs when the flow of blood to the heart muscle suddenly becomes blocked. Cardiovascular disease is the leading cause of death in the U.S. It\u2019s important to learn about your heart to help prevent it. If you have it, you can live a healthier, more active life by learning about your disease and taking care of yourself.\n\nHeart atta condition manifested by diseased blood vessels, structural problems, and blood clots.\n\n\nWhat you are going to find in this notebook?\n\n\n\u2705 **Data QA**\n\n- Dimensional analysis \n- Data type\n- Nulls values\n- Duplicated values\n- Inconsistences\n- Outliers\n- Data QA conclusions\n\n\ud83d\udcca **Reporting**\n\n- Bivariate analysis\n- Feature engineering\n- Reporting conclusions\n\n\ud83e\udd16 **Machine Learning**\n\n- Develop a \"competition\" betwen differents classification models\n- Evaluate results\n- Select the best and train it\n- Models conclusions\n\n**Plase if you consider usefull pleas help me uptvoting my work.**\n### Imports","fbd4e4d5":"### Data QA\n**Head of Dataset**","b9908d8f":"**Data Type**","54660b0d":"**Dimensional analysis**","373d1626":"**Train the model and make predictions**","ad386b74":"**Constants**","d4e3cb63":"**Metrics**","dd1f8d52":"**Inconsistences to be chequed**\n\n- Negative ages\n- Negative values of blood pressure at rest\n- Negative serum cholesterol values in mg\/dl\n- Negative max heart rate achieved vsalues\n- Negative oldpeak values","08acf315":"**Reporting conclusions**\n- The top 3 feature which has a higher relation with the target are: \n    - cp: 0.43\n    - thalac: 0.42 \n    - slope: 0.35\n- Making some feature engineering at age taht don't improve to much the correlation.","7be5c24f":"### Reporting\n\nSplit dataset into train-test ","4dc2cd9a":"Competitor canditates","734b7e8d":"**NULLS values**","a1c3141a":"**Conclusions**\n- DecisionTreeClassifier is the model selected because of its metrics.\n- Metrics in train and test are awesome.\n- thalach is the feature which has more wigh in this model.","bcf14866":"**Duplicated values**","3fb2adda":"**Data QA conclusions**\n- Dataset has 0 NULL values\n- Dataset has 0 duplicated values\n- Dataset has 0 relevant inconsistences\n- Dataset has a few outliers. In case this features has lot of relevance with the target, some feature engineering is goint to be done.","5b72d517":"**Outliers**","6bb41226":"**Feature engineering**\n\n- age will be grouped into 4 groups divided by age quearters","87d6a642":"**Results**"}}