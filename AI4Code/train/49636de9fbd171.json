{"cell_type":{"dc6431a6":"code","4a23b80f":"code","b02da216":"code","748f1343":"code","8704c048":"code","9cb35711":"code","21481871":"code","e61c662a":"code","a2c14579":"code","25bdfcdf":"code","428f2594":"code","c22d24ea":"code","a7e5274f":"code","44cab237":"code","118d8add":"code","a7e4e069":"code","2b036a08":"code","dcbf5951":"code","568a0e43":"code","dfcabc9c":"code","59c202e3":"code","2d21e8dd":"code","51a91d7e":"code","4e4a7bfc":"code","438f2140":"code","01bbcba3":"code","6a544a31":"code","433a01c2":"code","e62122cd":"code","68094fb2":"code","0a8b0bf6":"code","e0250663":"code","79760450":"code","e5c3a87b":"code","f518de5f":"code","1dda2df1":"code","b9bfd762":"code","f7f484bb":"code","9bff2497":"code","4337b198":"code","beca7139":"code","5dd18eef":"code","d29c37b8":"markdown","4762166b":"markdown","86ffb47b":"markdown","b9ff86ff":"markdown","9e7c80cd":"markdown","392907a7":"markdown","297ac8fe":"markdown","5093fbec":"markdown","afd2f4c7":"markdown","db11579d":"markdown","0896778a":"markdown","ee85b499":"markdown","500eb19c":"markdown","e4027de8":"markdown","c5080733":"markdown","4492961b":"markdown","7212bac8":"markdown","cb1bded9":"markdown","cabe5a42":"markdown","7ea4fe5e":"markdown","a98d4837":"markdown","a7d58933":"markdown","afa5b3d5":"markdown","5c0148ed":"markdown","c78e79f5":"markdown","133cc26a":"markdown","59a6445d":"markdown","69b05d3a":"markdown","66e809ae":"markdown","2b11e00e":"markdown","4c5acd79":"markdown","46c2117e":"markdown","9369aa09":"markdown","566cfa19":"markdown","1c7f0d59":"markdown","30e72cca":"markdown","30dd1c21":"markdown","3723ae62":"markdown","242966fb":"markdown","540f8923":"markdown","44ca07d3":"markdown","f60cbd90":"markdown","d5d5baf5":"markdown","d2bc1632":"markdown","80c9fd26":"markdown"},"source":{"dc6431a6":"!pip install --upgrade pip\n!pip install --upgrade allennlp\n!pip install transformers==4.3.0","4a23b80f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b02da216":"from transformers import BertTokenizer, TFBertModel\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel    \nfrom transformers import (XLMRobertaConfig, XLMRobertaTokenizer, TFXLMRobertaModel)            \n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\n\nos.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning\n\nnp.random.seed(0)","748f1343":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","8704c048":"train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\n\ntrain.head()","9cb35711":"# print out stats about data\n\nmissing_values_count = train.isnull().sum() # we get the number of missing data points per column\nprint(\"Number of missing data points per column:\\n\")\nprint (missing_values_count)","21481871":"# check the number of rows and columns in the training set\n\nprint(\"Number of training data rows: {} \\n\".format(train.shape[0]))\n\nprint(\"Number of training data columns: {} \\n\".format(train.shape[1]))","e61c662a":"train.language.unique()\ntrain.language.value_counts()","a2c14579":"# counts for all classes\ncounts = train['label'].value_counts()\n\nclass_labels = ['Entailment', 'Neutral', 'Contradiction']\n\ncounts_per_class = [counts[0], counts[1], counts[2]]\n\n# counts.plot(kind='barh')\n\nplt.figure(figsize = (10,10))\nplt.pie(counts_per_class,labels = class_labels, autopct = '%1.1f%%')\nplt.show()","25bdfcdf":"from sklearn.model_selection import train_test_split\ntrain, validation = train_test_split(train, stratify=train.label.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)\n\n\ntrain.reset_index(drop=True, inplace=True)\nvalidation.reset_index(drop=True, inplace=True)","428f2594":"# check the number of rows and columns after split\n\nprint(\"Train data: {} \\n\".format(train.shape))\n\nprint(\"Validation data: {} \\n\".format(validation.shape))","c22d24ea":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name) # Save the slow pretrained tokenizer\nsave_path = '.'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path) # Save the loaded tokenizer locally\ntokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=False, strip_accents=False) # Load the fast tokenizer from saved file\n\ntokenizer","a7e5274f":"def plot(df, tokenizer):\n    \"\"\"\n    Plot histogram of lengths of input sequences\n    \"\"\"\n    all_text = df.premise.values.tolist() + df.hypothesis.values.tolist() # list of string texts\n    all_text_tokenized = tokenizer.encode_batch(all_text) # list of encoding objects\n    all_tokenized_len = [len(encoding.tokens) for encoding in all_text_tokenized] # list of token lengths\n       \n    plt.hist(all_tokenized_len, bins=30, alpha=0.5)\n    plt.title(' Histogram of lengths of input sequences')\n    plt.xlabel('Number of tokens')\n    plt.ylabel('Count')\n\n    plt.show()\n\nplot(train, tokenizer)","44cab237":"tokenized_premise = tokenizer.encode_batch(train.premise.values.tolist()) # list of encoding objects\ntrain['premise_seq_length'] = [len(encoding.tokens) for encoding in tokenized_premise] # list of lengths\n    \ntokenized_hypothesis = tokenizer.encode_batch(train.hypothesis.values.tolist()) # list of encoding objects\ntrain['hypothesis_seq_length'] = [len(encoding.tokens) for encoding in tokenized_hypothesis] # list of lengths\n\n# Calculate max and avg sequence length per language\ninfo_per_lang = train.groupby('language').agg({'premise_seq_length': ['mean', 'max', 'count'], 'hypothesis_seq_length': ['mean', 'max', 'count']})\nprint (info_per_lang)","118d8add":"column_name = info_per_lang.columns.values[0] #premise mean column\ninfo_per_lang[column_name].plot(kind='bar')","a7e4e069":"# Configuration\nEPOCHS = 3\nBATCH_SIZE = 64 \nMAX_LEN = 100\nPATIENCE = 1\nLEARNING_RATE = 1e-5","2b036a08":"def encode(df, tokenizer, max_len=50):\n    \n    pairs = df[['premise','hypothesis']].values.tolist()\n\n    tokenizer.enable_truncation(max_len)\n    tokenizer.enable_padding()\n    \n    print (\"Encoding...\")\n    # We'll use encode_batch() as 'BertWordPieceTokenizer' object has no attribute 'batch_encode_plus'\n    enc_list = tokenizer.encode_batch(pairs)\n    print (\"Complete\")\n    \n    input_word_ids = tf.ragged.constant([enc.ids for enc in enc_list], dtype=tf.int32) #shape=[num_examples, max_len])\n    input_mask = tf.ragged.constant([enc.attention_mask for enc in enc_list], dtype=tf.int32) #shape=[num_examples, max_len]\n    input_type_ids = tf.ragged.constant([enc.type_ids for enc in enc_list], dtype=tf.int32) #shape=[num_examples, max_len]\n   \n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask.to_tensor(),\n        'input_type_ids': input_type_ids.to_tensor()}\n    \n    return inputs ","dcbf5951":"train_input = encode(train, tokenizer=tokenizer, max_len=MAX_LEN)","568a0e43":"validation_input = encode(validation, tokenizer=tokenizer, max_len=MAX_LEN)","dfcabc9c":"def build_model(model_name, max_len=50):\n    \n    tf.random.set_seed(12345) # For reproducibility\n    \n    bert_encoder = TFBertModel.from_pretrained(model_name)\n#     bert_encoder = TFAutoModel.from_pretrained(model_name)\n\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    features = bert_encoder([input_word_ids, input_mask, input_type_ids])[0] # shape=(batch_size, max_len, output_size)\n    cls_vector = features[:,0,:] #shape=(batch_size, output_size)\n    output = tf.keras.layers.Dense(3, activation='softmax')(cls_vector) # shape=[batch_size, num_class=3]       \n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","59c202e3":"# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n    model = build_model(model_name, MAX_LEN)\n    model.summary()","2d21e8dd":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheckpoint_filepath='bert_best_checkpoint.hdf5'\n\n# callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)]\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)]\n\ntrain_history = model.fit(x=train_input, y=train.label.values, validation_data=(validation_input, validation.label.values), epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","51a91d7e":"# plot loss history\nplt.plot(train_history.history['loss'], label='train loss')\nplt.plot(train_history.history['val_loss'], label='validation loss')\nplt.title('Average Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","4e4a7bfc":"# plot accuracy history\nplt.plot(train_history.history['accuracy'], label='train accuracy')\nplt.plot(train_history.history['val_accuracy'], label='validation accuracy')\nplt.title('Average Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","438f2140":"del model #to free up space","01bbcba3":"# Resets all state generated by Keras\nK.clear_session()","6a544a31":"PRETRAINED_MODEL_TYPES = {\n    'xlmroberta': (XLMRobertaConfig, TFXLMRobertaModel, XLMRobertaTokenizer, 'jplu\/tf-xlm-roberta-large')\n}\n\nconfig_class, model_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['xlmroberta']\n\n# Download vocabulary from huggingface.co and cache.\n# tokenizer = tokenizer_class.from_pretrained(model_name) \ntokenizer = AutoTokenizer.from_pretrained(model_name) #fast tokenizer\n\ntokenizer","433a01c2":"def encode(df, tokenizer, max_len=50):\n    \n    pairs = df[['premise','hypothesis']].values.tolist() #shape=[num_examples]\n    \n    print (\"Encoding...\")\n    encoded_dict = tokenizer.batch_encode_plus(pairs, max_length=max_len, padding=True, truncation=True, \n                                               add_special_tokens=True, return_attention_mask=True)\n    print (\"Complete\")\n    \n    input_word_ids = tf.convert_to_tensor(encoded_dict['input_ids'], dtype=tf.int32) #shape=[num_examples, max_len])\n    input_mask = tf.convert_to_tensor(encoded_dict['attention_mask'], dtype=tf.int32) #shape=[num_examples, max_len]\n    \n    inputs = {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask}    \n    \n    return inputs","e62122cd":"train_input = encode(train, tokenizer=tokenizer, max_len=MAX_LEN)","68094fb2":"validation_input = encode(validation, tokenizer=tokenizer, max_len=MAX_LEN)","0a8b0bf6":"def build_model(max_len=50):\n    \n    tf.random.set_seed(12345) # For reproducibility\n    \n    # The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\n    encoder = model_class.from_pretrained(model_name)\n#     encoder = TFAutoModel.from_pretrained(model_name)\n    \n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    \n    # Extract final layer vectors\n    output = encoder([input_word_ids, input_mask])[0] # shape=(batch_size, max_len, output_size)\n    # We pass the vector of only the [cls] token (at index=0) to the classification layer\n    sequence_output = output[:,0,:] #shape=(batch_size, output_size)\n   \n    # Add a classification layer\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(sequence_output)  \n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","e0250663":"# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n    model = build_model(MAX_LEN)\n    model.summary()","79760450":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheckpoint_filepath='xlmroberta_best_checkpoint.hdf5'\n\n# callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)]\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)]\n\ntrain_history = model.fit(x=train_input, y=train.label.values, validation_data=(validation_input, validation.label.values), epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","e5c3a87b":"# plot loss history\nplt.plot(train_history.history['loss'], label='train loss')\nplt.plot(train_history.history['val_loss'], label='validation loss')\nplt.title('Average Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","f518de5f":"# plot accuracy history\nplt.plot(train_history.history['accuracy'], label='train accuracy')\nplt.plot(train_history.history['val_accuracy'], label='validation accuracy')\nplt.title('Average Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","1dda2df1":"from sklearn.metrics import classification_report, confusion_matrix\nfrom itertools import product\n\n# The function plot_confusion_matrix() is from scikit-learn\u2019s website to plot the confusion matrix. \n# link: https:\/\/scikit-learn.org\/0.18\/auto_examples\/model_selection\/plot_confusion_matrix.html\ndef plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = np.around(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], 2)\n#         print(\"Normalized confusion matrix\")\n#     else:\n#         print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nvalidation_predictions = [np.argmax(i) for i in model.predict(validation_input)] #predictions\nvalidation_labels = validation.label.values.tolist() #ground truth labels\n\ncm_plot_labels = ['entailment','neutral', 'contradiction']\ncm = confusion_matrix(y_true=validation_labels, y_pred=validation_predictions)\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix Without Normalization')\n# plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix With Normalization', normalize=True)\n\ntarget_class = ['entailment' if label==0 else 'neutral' if label==1 else 'contradiction' for label in validation_labels]\nprediction_class = ['entailment' if label==0 else 'neutral' if label==1 else 'contradiction' for label in validation_predictions]\nprint('\\nClassification Report')\nprint(classification_report(y_true=target_class, y_pred=prediction_class))","b9bfd762":"# function to print accuracy per language\ndef accuracy(x):\n    return round(float(x[2]\/x[1]), 2)*100\n\nvalidation['predictions'] = validation_predictions\n\n# Calculate the total number of examples per language\nlang_counts = validation.language.value_counts().sort_index()\n\n# Calculate the number of correct predictions per language\ntp_per_lang = validation[validation['label'] == validation['predictions']].groupby('language').agg({'language': ['count']}).sort_index()\n\nlang_names = lang_counts.index.tolist()\nlang_tuples = list(zip(lang_names, lang_counts.values.tolist(), tp_per_lang.iloc[:, 0].values.tolist()))\nacc = map(accuracy, lang_tuples)\nfor i, score in enumerate(acc):\n    print (\"Accuracy of {} is {} \".format(lang_tuples[i][0], score))","f7f484bb":"# The model weights (that are considered the best) are loaded into the model.\nmodel.load_weights(checkpoint_filepath)","9bff2497":"#encode the test-input sequences\ntest_input = encode(test, tokenizer=tokenizer, max_len=MAX_LEN)","4337b198":"predictions = [np.argmax(i) for i in model.predict(test_input)]\n# predictions = predictions[:test.shape[0]]","beca7139":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\n\nsubmission.head()","5dd18eef":"submission.to_csv(\"submission.csv\", index = False)","d29c37b8":"The encoded input will include - input word IDs and input masks, as required by the XLM-RoBERTa model.","4762166b":"The XLM-RoBERTa is based on Facebook\u2019s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.","86ffb47b":"The length should be large enough such that we don\u2019t lose much data. Additionally, a very big number would make the model complex.\n\nSince most of the inputs are shorter than 50 words, we can consider length 50 for each input type of hypothesis and premise. \n\nHence we set `MAX_LEN`=100.\n\n*Note*: The `MAX_LEN` hyperparameter can be taken as a parameter to be tuned to get optimal results.","b9ff86ff":"From the above plots we can clearly see that the gap between the training and validation losses has decreased by a large margin and best validation accuracy with XLM-RoBERTa model is 79-80%.","9e7c80cd":"To further evaluate the performance of the XLM-RoBERTa model, we'll generate the confusion matrix and classification report on the validation data.","392907a7":"# Split the Training Data","297ac8fe":"It would also be interesting to look at the number of correct predictions per language.","5093fbec":"Let's now visualize the distribution of class labels over the training data","afd2f4c7":"# Configure TPU Settings","db11579d":"From the above plots, we can see that there is a huge gap between the training and validation losses, which suggests that the M-BERT model is not quite good at generalizing to unseen data. The M-BERT model gives final validation accuracy of around 64%. In the next section, we'll look at another model, namely XLM-RoBERTa, which improves the validation accuracy and is much better at predictions on new data.  ","0896778a":"# Implement M-BERT Model","ee85b499":"We will use the same train-validation split and hyperparameter settings as in the previous BERT model for results to be comparable.","500eb19c":"# Submit the Predictions","e4027de8":"## Encode Input Sequences","c5080733":"## Create and Train Model","4492961b":"That's it! The submission file has been created, for more information on how to submit to the competition, please visit the following [link](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview\/evaluation).\n\n\n\n\n<span style=\"color:blue\">If you found this notebook helpful, please kindly upvote!<\/span>","7212bac8":"# Load Libraries and Dependencies","cb1bded9":"Explore data & drop any incomplete rows of data.\n\nFind how many data points and features are in the original, provided training dataset.","cabe5a42":"Let's now visualize the loss and accuracy history for training and validation sets.","7ea4fe5e":"The dataset contains train and test files that includes premise-hypothesis pairs in fifteen different languages. \n\nThe classification of the relationship between the premise and hypothesis statements is as follows:\n\n- label==`0` for `entailment`\n- label==`1` for `neutral`\n- label==`2` for `contradiction`\n\nYou can look at [competition website](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data) for elaboration on the dataset.","a98d4837":"Let's look at the distribution of languages in the training set.","a7d58933":"## Configure Hyperparameter Settings","afa5b3d5":"We will be splitting the training dataset into two parts - the data we will train the model with and a validation set. We stratify data during train-valid split to preserve the original distribution of the target classes.","5c0148ed":"## Set up M-BERT Tokenizer","c78e79f5":"The model will be trained on the training subset and early-stopping will be applied on validation subset to avoid overfitting. The best model checkpoint will be saved after `EPOCHS` iterations.","133cc26a":"We extract the hidden state vector of the 'CLS' token in the final BERT layer and pass that as input to the classification layer for further training.","59a6445d":"For BERT model, the input is represented in the following format:\n\n`CLS` Premise `SEP` Hypothesis `SEP`\n\nThe `CLS` and `SEP` are special tokens, where `CLS` is used in the beginning of a sequence for sentence-level classification while `SEP` separates the sentence pairs.\n\nWe encode the training data by vectorizing the input strings and applying padding and truncation using `MAX_LEN` value.\n\nThe encoded input will include - input word IDs, input masks, and input type IDs","69b05d3a":"The Multilingual BERT or M-BERT is a single language model pre-trained on the concatenation of monolingual Wikipedia corpora from 104 languages. We will fine-tune this pretrained model on our training dataset to get the predictions for textual entailment recognition.","66e809ae":"## Encode Input Sequences","2b11e00e":"## Set up the Tokenizer","4c5acd79":"We can also calculate the mean and max input sequence lengths per language.","46c2117e":"## Create and Train Model","9369aa09":"# Implement XLM-RoBERTa Model","566cfa19":"# Introduction","1c7f0d59":"# Load CSV Data files with Pandas","30e72cca":"Once we are satisfied with our model's performance, we can get test-data predictions for submission.","30dd1c21":"From the above table, we can see that the length of premise sentences are greater than those of the hypothesis sentences for all languages. \n\nHence, let's visualize the mean sequence length distribution over the languages for the premise inputs.","3723ae62":"Natural Language Inferencing (NLI) is an exciting NLP (Natural Language Processing) problem to identify the semantic relationship between two sentences. Given a hypothesis and premise sentence-pairs, the task is to determine whether the premise `entails` the hypothesis statement, `contradicts` it, or neither (`neutral`). \n\nFor more information on the problem, you can visit [Contradictory, My Dear Watson competition](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview)","242966fb":"Let's look at the sequence length distribution (e.g. number of tokens in a sequence) for the input data. We will need this information later when setting the `max_len` value since a machine learning algorithm requires all the inputs in a batch to have the same length.","540f8923":"From the chart above, we can see that the training data is fairly balanced over the 3 classes.","44ca07d3":"# Data Exploration and Analysis","f60cbd90":"From the histogram above, we can see that majority of the input sequences have less than 50 tokens.","d5d5baf5":"The notebook is a step-by-step tutorial on using Transformer models for Natural Language Inferencing (NLI). This includes how to load, fine-tune, and evaluate M-BERT and XLM-RoBERTa models with Tensorflow.","d2bc1632":"# Generate Predictions on Test Data","80c9fd26":"A pretrained model only performs properly if we feed it an input that was tokenized with the same rules that were used to tokenize its training data. The BERT multilingual model does not perform any normalization on the input (no lower casing, accent stripping, or Unicode normalization). Hence we also follow the same rules when tokenizing input data for our task. For more information on data pre-processing, visit [M-BERT github](https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md)."}}