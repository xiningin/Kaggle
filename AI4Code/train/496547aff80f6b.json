{"cell_type":{"7c807b59":"code","f09d12ab":"code","341d05c5":"code","1dffd018":"code","4cf83990":"code","ba691382":"code","28c0f28e":"code","53f552ee":"code","1117b57c":"code","f4e21002":"code","e9f2d50c":"code","1fe821ca":"code","2d2bb55c":"code","5227290c":"code","ba09da2f":"code","ec878419":"code","ad099ea3":"code","ea0aebbe":"code","5653293f":"code","63d61b65":"code","8af16bd3":"code","fb1e7178":"code","f1290103":"code","7cab1c56":"code","2185da7a":"code","441fd200":"code","bc77a6c1":"code","cad04753":"code","611ccd0f":"code","865895df":"code","f8c2dd05":"code","5f9048c1":"code","8bbcdec4":"code","63229837":"code","50794012":"markdown","7aa24126":"markdown","33fb7590":"markdown","461d94e9":"markdown","ff73664d":"markdown","4860e947":"markdown","4590cd0d":"markdown","6ffa67f1":"markdown","6f9c8456":"markdown","6dd14e29":"markdown","a660f0cd":"markdown","64feddd1":"markdown","9732ffe6":"markdown","792a4bfc":"markdown","1c8c99df":"markdown","2a4cc8f4":"markdown","2e42d202":"markdown","be79e994":"markdown","2aabae14":"markdown","2449b124":"markdown","bae1f759":"markdown","9ed0a7d3":"markdown","4267392a":"markdown","8302bf62":"markdown","015deeff":"markdown","e3a6e3d4":"markdown","9b55b4c4":"markdown","e7d1af03":"markdown","fb14e189":"markdown","f756a4c1":"markdown","abb23262":"markdown"},"source":{"7c807b59":"print(\"Hello, World!\")","f09d12ab":"import math\nmath.sqrt(27)","341d05c5":"# importing the libraries we need\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#########################################################\n## JUST COPY-PASTE THE FOLLOWING\n# Set some viewing options for convience\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\n# set a reasonable size for the figures we create\nplt.rcParams['figure.figsize'] = [15, 8]\n# setting the seed so that we always have the same sample\nnp.random.seed(0)\n##########################################################","1dffd018":"path = \"..\/input\/folkhalsomyndigheten\/Folkhalsomyndigheten_Covid19.xlsx\" # change to where you have put the file\ncovid = pd.read_excel(path, sheet_name='Antal per dag region')\ncovid.head(5)","4cf83990":"covid['\u00d6rebro']","ba691382":"interesting_columns = ['Halland', 'Stockholm']\ncovid[interesting_columns]\n# we could also write covid[['Halland', 'Stockholm']] straight away","28c0f28e":"ing_columns = ['Halland', 'Stockholm']\ncovid[interesting_columns].sum()","53f552ee":"# we use the covid = covid... notation here to \"save\" our change\ncovid = covid.set_index('Statistikdatum')","1117b57c":"sns.lineplot(data=covid['Stockholm'])","f4e21002":"# this will not run in the current state\nimport requests\nheaders = {'Ocp-Apim-Subscription-Key': 'MY_API_TOKEN'}\nr = requests.get('https:\/\/api-extern.systembolaget.se\/product\/v1\/product', headers=headers)\npd.read_json(r.text).to_excel('viner.xlsx')","e9f2d50c":"# Load the data\nfilepath = \"..\/input\/creditcardfraud\/creditcard.csv\"\ndf = pd.read_csv(filepath) # Change to the right filepath for you\n\n# View the first 5 entries\ndf.head(5)","1fe821ca":"# We access a particular column in the dataframe through df['COLUMN_NAME']\ndf['Class']","2d2bb55c":"# We can call the function value_counts to count the number of observations with each given value\nv_count = df['Class'].value_counts()\nprint(v_count)","5227290c":"v_count[0] # 0 -> non-fraudulent, 1 -> fraudulent\n# indices in Python ALWAYS start at 0","ba09da2f":"proportion_nofraud = v_count[0] \/ (v_count[0] + v_count[1])\nproportion_fraud = v_count[1] \/ (v_count[0] + v_count[1])\n\n# We can discuss this print statement another day\nprint(f\"Frauds: {proportion_fraud*100:.2f}%\\nNo Frauds: {proportion_nofraud*100:.2f}%\")","ec878419":"sns.countplot(data=df, x='Class')\nplt.title('Class Distributions')\nplt.xticks(ticks=[0, 1], labels=[\"No Fraud\", \"Fraud\"])","ad099ea3":"# Amount\namount = df['Amount']\nsns.distplot(amount, color='red')\nplt.title(\"Distribution of Transaction Amount\")","ea0aebbe":"# investigate the mean, median and standard deviation\namount_mean = amount.mean()\namount_median = amount.median()\namount_std = amount.std()\n\nprint(f\"The mean amount is {amount_mean:.2f} USD, the median {amount_median:.2f} USD, and the standard deviation {amount_std:.2f} USD\")","5653293f":"# Time\ntime = df['Time']\nsns.distplot(time, color='blue')\nplt.title(\"Distribution of Time\")","63d61b65":"# investigate the mean, median and standard deviation\ntime_mean = time.mean()\ntime_median = time.median()\ntime_std = time.std()\n\nprint(f\"The mean amount is {time_mean:.2f} seconds, the median is{time_median:.2f} seconds, and the standard deviation is {time_std:.2f} seconds\")","8af16bd3":"from sklearn.preprocessing import RobustScaler\n\n# The following notation is a little disconcerting if you don't have any experience \n# with objects in a programming language before. Think of this as creating the function\n# That we'll use.\nrobust_scaler = RobustScaler()\n\n# the RobustScaler function only takes data in column-form so we reshape it\nscaled_amount = robust_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\nscaled_time = robust_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\n# in pandas we can create a new column in the dataframe be simply assigning a data \n# (in the right shape) to a new name. In this case \"scaled_amount\" and \"scaled_time\" do \n# not exist as columns in the dataframe before we create it in the code below.\ndf['scaled_amount'] = scaled_amount\ndf['scaled_time'] = scaled_time","fb1e7178":"df.head(10)","f1290103":"# Creating a new dataframe, scaled_df, with only scaled features\nscaled_df = df.drop(['Time', 'Amount'], axis=1)\nscaled_df.head()","7cab1c56":"# Create a shuffled dataframe\nshuffled_scaled_df = scaled_df.sample(frac=1, random_state=42)\n\n# Use the first 80% of the observations for training\nidx_80 = int(len(shuffled_scaled_df)*0.8) # index of the observation 80% down the dataframe\n\n# .iloc is a function for getting a subsample of a dataframe based on index\n# the :idx_80 means \"up to, but not including, index `idx_80`\"\nscaled_df_train = shuffled_scaled_df.iloc[:idx_80]\n\n# Use the last 20% for testing\n# .iloc[idx_80:] gives us all the observations from `idx_80` to the end\nscaled_df_test = shuffled_scaled_df.iloc[idx_80:]\n\n# Double-check the proportions of of fraud to non-fraud are the same in train and test\ncounts_train = scaled_df_train['Class'].value_counts()\ncounts_test = scaled_df_test['Class'].value_counts()\n\nprop_fraud_train = counts_train[1]\/ (counts_train[0] + counts_train[1])\nprop_fraud_test = counts_test[1]\/ (counts_test[0] + counts_test[1])\n\nprint(f\"Fraud in the training set: {prop_fraud_train*100:.4f}% ({counts_train[1]} in total)\\nFraud in the test set: {prop_fraud_test*100:.4f}% ({counts_test[1]} in total)\")","2185da7a":"# Extract all the observations of fraud.\n# .loc is a function that gives you a subset of the dataframe, in this case where \n# the 'Class' column is equal to 1\nfraud_df = scaled_df_train.loc[scaled_df_train['Class'] == 1]\n\nnum_frauds = len(fraud_df)\n\n# Extract an equal non-fraud observations in our shuffled dataframe\nnon_fraud_df = scaled_df_train.loc[scaled_df_train['Class'] == 0][:num_frauds]\n\n# Glue (concatenate) these two together into one dataframe\nru_df_train = pd.concat([fraud_df, non_fraud_df])\n\n# shuffle the dataframe that we glued together\nru_df_train = ru_df_train.sample(frac=1)\n\nru_df_train","441fd200":"sns.countplot('Class', data=ru_df_train)\nplt.title(\"Equally Distributed Classes\", fontsize=14)","bc77a6c1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create the dependent and independent variables\nX_train = ru_df_train.drop(\"Class\", axis=1)\ny_train = ru_df_train['Class']","cad04753":"logistic_regression = LogisticRegression(penalty=\"l2\", C=1000, solver=\"liblinear\")\nlogistic_regression.fit(X_train, y_train)","611ccd0f":"knear_neighbors = KNeighborsClassifier(n_neighbors=4, weights=\"uniform\")\nknear_neighbors.fit(X_train, y_train)","865895df":"# Separate out the dependent and independent variables in the testing data\nX_test = scaled_df_test.drop(\"Class\", axis=1)\ny_test = scaled_df_test['Class']\n\nlr_predictions = logistic_regression.predict(X_test)\nknn_predictions = knear_neighbors.predict(X_test)","f8c2dd05":"# How well did the models perform\n\n# Number of correct predictions\nlr_correct_preds = sum(y_test == lr_predictions)\nlr_success_rate = lr_correct_preds \/ len(y_test)\n\nknn_correct_preds = sum(y_test == knn_predictions)\nknn_success_rate = knn_correct_preds \/ len(y_test)\n\nprint(f\"Logistic Regression success rate: {lr_success_rate*100:.4f}%\")\nprint(f\"k-Nearest Neighbors success rate: {knn_success_rate*100:.4f}%\")","5f9048c1":"only_nofraud = np.zeros(len(y_test))\nnofraud_correct_preds = sum(y_test == only_nofraud)\nnofraud_success_rate = nofraud_correct_preds \/ len(y_test)\n\nprint(f\"No Fraud model success rate: {nofraud_success_rate*100:.4f}%\")","8bbcdec4":"from sklearn.metrics import confusion_matrix\n\nnofraud_confusion = confusion_matrix(y_test, only_nofraud)\nlr_confusion = confusion_matrix(y_test, lr_predictions)\nknn_confusion = confusion_matrix(y_test, knn_predictions)\n\nexplanation = np.array([[\"True negatives\", \"False positives\"], [\"False negatives\", \"True positives\"]])\n\nprint(f\"A confusion matrix consists of:\\n{explanation}\")\nprint('-'*50)\n\nprint(f\"If we only predict No Fraud:\\n{nofraud_confusion}\")\nprint('-'*50)\n\nprint(f\"The confusion matrix for our logistic regression:\\n{lr_confusion}\")\nprint('-'*50)\n\nprint(f\"The confusion matrix for our k-Nearest Neighbors:\\n{knn_confusion}\")\nprint('-'*50)","63229837":"# Balanced accuracy score for the logistic regression model\nlr_recall_fraud = lr_confusion[1, 1] \/ (lr_confusion[0, 1] + lr_confusion[1, 1]) # true positives \/ (true positives + false positives)\nlr_recall_nofraud = lr_confusion[0, 0] \/ (lr_confusion[0, 0] + lr_confusion[1, 0]) # true negative \/ (true negatives + false negatives)\n\nlr_balanced_score = (lr_recall_fraud + lr_recall_nofraud) \/ 2\n\n# Balanced accuracy score for our k-Nearest Neighbors model\nknn_recall_fraud = knn_confusion[1, 1] \/ (knn_confusion[0, 1] + knn_confusion[1, 1]) # true positives \/ (true positives + false positives)\nknn_recall_nofraud = knn_confusion[0, 0] \/ (knn_confusion[0, 0] + knn_confusion[1, 0]) # true negative \/ (true negatives + false negatives)\n\nknn_balanced_score = (knn_recall_fraud + knn_recall_nofraud) \/ 2\n\n# Balanced accuracy score for the nofraud modell\nnofraud_recall_fraud = 0 # we have no false or true positives, so this is technically undefined\n\nnofraud_recall_nofraud = nofraud_confusion[0, 0] \/ (nofraud_confusion[0, 0] + nofraud_confusion[1, 0]) # true negative \/ (true negatives + false negatives)\n\nnofraud_balanced_score = (nofraud_recall_fraud + nofraud_recall_nofraud) \/ 2\n\nprint(\"Balanced Accuracy Scores:\")\nprint(f\"Logistic Regression: {lr_balanced_score*100:.4f}%\")\nprint(f\"k-Nearest Neighbors: {knn_balanced_score*100:.4f}%\")\nprint(f\"Nofraud model: {nofraud_balanced_score*100:.4f}%\")","50794012":"Lets look at what our dataframe looks like now.","7aa24126":"By this measure, we clearly see that the knn model is our strongest performer, but there is a lot of room for improvement. We'll leave things here for this time, but I leave it as an exercise for the reader to see if, for example, removing outliers from the data can lead to better performance. \n\nThank you for following along this far! Please grab me if you want more help, want to discuss using Python to solve a particular problem in your line of work, or just want to drink coffee with someone. \n\nAny feedback on this workshop or the written material -- bad or good -- is really appreciated. Feedback on things that can be improved is immensely valuable!","33fb7590":"We will leave the covid dataset for now and move on to more interesting pastures, but please try and play around with this data on your own to see what you can do with dataframes! Try loading different sheets from the excel file by changing the `sheet_name` parameter in `read_excel`, compare different regions to each other and maybe see if there is a second dataset, such as temperature, which you can correlate changes to.\n\n## Challenge Time!\n\nNow that you have gotten a glimpse of working with pandas, I'd like you to try working a little on your own (or two by two) and answer a few questions. Systembolaget have an [open API](https:\/\/api-portal.systembolaget.se\/) which lets you pull down product data on their full catalog. They code below does not work on SEB computers because the api is blocked, so I've provided the data as an excel-sheet attached to this notebooks on Kaggle. ","461d94e9":"and if we want to see extract several columns, we can just collect the column names in a *list* using square brackets:","ff73664d":"## Stepping Things Up: Importing Data\n\nIt's taken some time but we are now ready to import some data and do some analysis: lets play with pandas. A dataframe can (roughly) be thought of as a spreadsheet with rows and columns and an index. Today we will be playing with two public datasets: the official Covid statistics from Folkh\u00e4lsomyndigheten, and a dataset on credit card fraud. \n\nAlthough we're all tired of hearing about 'rona, this dataset is both topical and great for showing how we can import data from Excel and work with it using Python. To get the data go to [Folkh\u00e4lsomyndighetens statistics page](https:\/\/www.folkhalsomyndigheten.se\/smittskydd-beredskap\/utbrott\/aktuella-utbrott\/covid-19\/statistik-och-analyser\/bekraftade-fall-i-sverige\/) and click the download link. Move the Excel file to the same folder as the notebook you are currently working in.\n\nTo look at and manipulate this data we will create a *dataframe* called `covid` using the pandas function `read_excel`. To print the top of the dataframe we'll us the function `head`.","4860e947":"By this comparison our models are not great. However, in the real world missing a case of fraud (a *false negative*) is much worse than labelling a legit transaction as fraudulent (a *false positive*). Our metric of success should reflect this. Lets look at how our each of these three approaches performs with regards to our two classes in a so-called confusion matrices.","4590cd0d":"Lets visualise this dataset too. First, lets show the discrepancy between the number of fraud and number of non-fraud using the `countplot` function in `seaborn`.","6ffa67f1":"### k-Nearest Neighbors\nIn just as simple fashion we can train a k-Nearest Neighbors model.","6f9c8456":"### Random Undersampling\n\nThe defining characteristic of our data is that there are very few occurrences of fraud. In many cases classification models perform better when they have been trained on balanced data. One way to force this balance is to employ [random undersampling](https:\/\/en.wikipedia.org\/wiki\/Oversampling_and_undersampling_in_data_analysis) in which we randomly remove observations of the majority class, non-fraud, until we reach parity between the classes. In doing these we do however run the risk of discarding important information as we are only using a small minority of all the observations available to us. \n\nThe code in the next cell creates a new dataframe, `ru_df_train`, which contains all observations of fraud and equal number of randomly chosen non-fraud observations. The plan is to train our model on this dataset. However, it is important to verify that model works well on the real dataset, where fraud is much less prevalent. We will therefore test our model on the data separated out in the previous step.","6dd14e29":"To calculate the proportions all we need to do is:","a660f0cd":"### Visualising the Data\n\nTo continue building an understanding of this dataset we should try to create some visualisations. The two libraries we rely on here are `matplotlib` (which we aliased with `plt`) and `seaborn` (aliased `sns`). Unfortunately, things get a little measy here because `seaborn` is based on `matplotlib` -- acting as a layer on top of it. Roughly, you can say that in this tutorial we'll use `seaborn` to create the the basic graphs and plots while calling on `matplotlib` to make adjustments.\n\nIn the first example we'll just start by showing the number of cases in Stockholm over time using `seaborn`'s `lineplot` function. Since we have already set time as the index, we only need to specify the column and the rest is inferred.\n\nIn the first example we'll just visualise the different in count between Fraud and No Fraud with the `countplot` function and adjust the plots title and ticks.","64feddd1":"## Math Time! \n### Scaling\n\nHow are we feeling so far? Hopefully things have been as clear as they can be, otherwise let me know and I'll see what can improved or changed. \n\nWe are about to take a quite large leap forward, do some statistical work out of necessity, and then train a prediction model. Don't worry to much about it right now but I'm including it for those who are interested.\n\nFirst off, scaling. As mentioned previously all our feature V1-V28 have been transformed (scaled) through PCA. To be able to treat all our features in the same way we need to scale Amount and Time too. To do this we will use a transformation function called `RobustScaler` from the `sklearn.preprocessing` library. It applies \n\n$$f(x) = \\frac{x - \\mu^{0.5}}{\\mu^{0.75} - \\mu^{0.25}}$$\n\nTo each data point $x$, where $\\mu^{q}$ is the $q$-th quantile of the sample for the given feature. Once again, at this point don't worry too much about it. By using quantiles the scaling transformation becomes more robust towards outliers.","9732ffe6":"and press the arrow, or shift-Enter, to execute the code.\n\n### Importing modules and libraries\nIn Jupyter notebooks each cell can contain each text, formated in markdown, or code. \n\nWithout going too deep into the technical details we can say that the code that we write in this notebook is run by a _kernel_, a special program built to execute code. When we start up a new kernel it only loads a small subset of all the features that exist in the language -- the absolute necessities. We can gain access to the wider feature-set by _importing_ so-called modules containing these additional features. \n\nFor example if I want to take the square root of a number in Python I need to import the `math` module and use the function `math.sqrt`:","792a4bfc":"<img src=\"https:\/\/seb.se\/Static\/Images\/SebLogo.svg\" width=\"200\">\n\n# Credit Card Fraud: Intro to Python Workshop\n#### Johan Ramne\n\n\n\n**Note**: This notebook is currently hosted on [Kaggle](https:\/\/www.kaggle.com) and all the code and examples will be executable directly within it at a later date. I urge you to follow the setup steps, get your own notebook up and running, and type out the code examples shown. Wherever you can, try to change things to see if you understand what each line does. Don't be afraid to break things!\n\nThis notebook is intended to be an introduction to the Python language in the form of a workshop. The intended use is as a walk-through to show an example to people working in a financial context. It is based on public data available on [Kaggle](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud) and the analysis done by [Janio Martinez](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets).\n\n## The Tools We'll be Using\n* Python 3 (the code has been tested for 3.8.6)\n* Jupyter Notebooks (The de-facto foundation for data exploration and presentation in Python)\n* Pandas (data manipulation and exploration)\n* NumPy (linear algebra and other numeric operations)\n* matplotlib and seaborn (data visualisation)\n* scikit-learn (Out-of-the-box ML algorithms)\n\n## What We'll Learn\nThis tutorial is mainly meant to serve as an introduction to Python in a (sort-of) real world use case: building a machine learning model that can detect credit card fraud. You are not expected to be able to start doing advanced analysis or creating your own ML models after -- this is just show what is possible and within everyone's reach.\n\nThis tutorial lets you start from absolute scratch. In it we will:\n1. Install a Python distribution (Anaconda3) and a text editor on your SEB computer.\n2. Download a dataset of credit card transactions from Kaggle, a central hub for data scientist from all over the world.\n3. Manipulate and visualise different aspects of the data\n4. Build two machine learning models for identifying credit card fraud.\n\nIf you are interested in learning more after this tutorial I highly recommend three different books, depending what path you want to start down on: \n\n* For a general introduction to Python, Zed Shaw's \"Learn Python the Hard Way\" is a good resource.\n* Wes McKinney, the original author of the pandas package, has written \"Python for Data Analysis\" which gives an excellent overview of NumPy and pandas as a whole. \n* \"Python for Finance\" by Yves Hilpisch can act also act an introduction to pandas within our specific context.\n\n## Step 1: Setup and Installation\nThe easiest way for us to quickly get up and running with Python is through the Anaconda3 distribution. We will be using the [64-bit Individual Edition](https:\/\/repo.anaconda.com\/archive\/Anaconda3-2020.07-Windows-x86_64.exe), download it and start the installer. The default configuration will work fine for our use-case.The Anaconda distribution comes pre-configured with all the packages we'll be using today. In the future you can either use the graphical interface or the `conda` tool to install new packages.\n\nBecause we are installing things on our work computers we need to setup the `conda` tool to download packages via the SEB proxy network. Because I have a complete lack of knowledge when it comes to network security, I'm uncertain about the consequences of publishing SEB's proxy to a public document -- replace `NETWORK.PROXY` with actual proxy in the following commands in the workshop. Open the `Anaconda Powershell prompt` and paste:\n\n```\nconda config --set proxy_servers.http NETWORK.PROXY\nconda config --set proxy_servers.https NETWORK.PROXY\nconda config --set ssl_verify no\n```\nThis sets us up to download any additional Python package that we may need, and to update the packages already included in the Anaconda distribution.\n\n### The Editor: VS Code\nThis is optional but highly recommended. Microsoft's Visual Studio comes in several different flavors but for our needs the open-source free version, [Visual Studio Code](https:\/\/code.visualstudio.com\/) is a great editor. \n\nIf you choose to use VS Code, go to the extensions tab and add the Python extension for proper syntax highlighting and other support features.\n\n## Step 2: Jupyter Notebook\nNow that we have a Python distribution in place and an editor, we are almost ready to write code! That last thing we need is an environment in which we will _execute_ our code. Within data science the de facto solution has been to use so-called **notebooks** to explore data, test different models and document the results. A notebook is a a web application which provides an interactive environment in which you mix freely between code, regular text, graphs and images. We will be using Jupyter Notebook, arguably the most common option, to write our code today. VS Code has built-in support for Jupyter, you just have to choose the \"trust\" option when VS Code asks if you want to run the notebook. If you prefer you can start the `anaconda powershell prompt` application (or `terminal` if you're on a Mac), go the your workspace folder and write\n\n```\njupyter notebook\n```\nand the application will start in your browser.\n\nEither way, choose a work folder and create a file named \"credit-fraud.ipynb\", where \".ipynb\" is the jupyter notebook extension. Within SEB it is recommended that you create a *workspace* folder for all your code in your C drive and call it **ws**.\n\n## Time to Code!\nWe are now finally ready to write some code! Python as a language is designed to be as human-readable as possible and hide complexity from the programmer to a large extent. It is traditional that the first lines of code you write are to make the computer say \"Hello, World!\" and in Python the code for this is:","1c8c99df":"If everything has gone as it should for you have a total of 33 columns. Now we need create a new dataframe, excluding Amount and Time, so that we only have scaled features. ","2a4cc8f4":"In the confusion matrices we see that our kNN model is catching as many counts of fraud as the logistic regression, but with many fewer false positives. A second way of adjusting for the imbalance in the test data is to use a *balanced accuracy score*, where we assign a weight to the class based on its proportion of the total sample -- classifying a fraud correctly gives more than classifying a non-fraud correctly.\n\nThe balanced accuracy score is built on the concept of *recall score*:\n\n$$\\textrm{recall score} = \\frac{\\textrm{true positives}}{\\textrm{true positives} + \\textrm{false positives}},$$\n\ni.e. the proportion of positive labels that were correct. The balanced accuracy score is the average recall score for all class, in our case \"Fraud\" and \"No Fraud\", so the score goes from 0 (worst-case) to 1 (best-case).","2e42d202":"Each column is a feature -- due to the confidential nature of the dataset all features but Time, Amount and Class have been anonymised and transformed through Principle Component Analysis (PCA). We don't need to go into more detail on PCA today. All in all we have:\n\n* Time: Seconds elapsed since the first observation in the dataset\n* Amount: Transaction amount\n* V1 - V28: Anonymous features. Think of this as current balance, number of registered credit cards, length of time as customer, gender etc.\n* Class: If the transaction was fraudulent (1) or not (0).\n\nOur first task is to find out the proportion between fraudulent and non-fraudulent transactions in the dataset. ","be79e994":"Lets double-check that we now have a balanced dataset.","2aabae14":"Modules are the real power within nearly all modern programming languages. There are modules that come built-in with all Python distributions like `math`, and then there are third-party libraries, built by the Python community, to solve different problems. The Anaconda distribution that we have downloaded comes many of the most popular third party libraries. Today we will make us of the following:\n\n* Pandas: Creating and manipulating dataframes\n* Numpy: Linear algebra and array programming\n* Matplotlib + seaborn: Graphs and other visualisations\n* Scikit-learn: Out-of-the-box machine learning algorithms\n\nIn the next code cell we will import with the syntax:\n\n```\nimport <library> as <alias>\n```\n\nThe `as <alias>` is not strictly necessary but allows us to shorten the library name and makes the code easier to read. The alias' used are standard convention. Normally libraries are imported at the at the top of a file or notebook.","2449b124":"Download the dataset, load it into a dataframe and try to figure out:\n1. What product has the highest APK (Alcohol \/ krona)?\n2. What alcoholic beverage has the worst?\n3. What proportion of the beverages are organic? hint: `value_counts` function\n4. Graph the product distribution by type with seaborns `countplot` function\n\nWe'll spend a few minutes figuring this out before moving on stepping up a gear and then finally doing some machine learning.","bae1f759":"To access a particular column in a dataframe `covid` we use the notation `covid['COLUMN NAME']`, for example:","9ed0a7d3":"### Credit Card Fraud data\n\nThe second dataset that we will look at is over credit card fraud in September of 2013. You need to download the data from [Kaggle](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud) to follow along the rest of the tutorial.\n\nWe will call our dataframe `df` and load the data with the pandas function `read_csv`, since the downloaded data is formatted in a .csv file.","4267392a":"This looks great at first glance! But most of you have probably already realised that with so few cases of fraud in the data, any model that predicts \"No Fraud\" most of the time will do well by this metric -- in fact, if we only predict \"No Fraud\":","8302bf62":"### Logistic Regression\nScikit-learn provides a standardised way of applying out-of-box machine learning algorithms to your data. The model is trained by using the `.fit` method of the model that you have imported on the training data. Once the model is trained you can use the `.predict` method to generate predictions.\n\nThe next cell is where all the magic happens!","015deeff":"### Distribution of Amount And Time\n\nLets switch focus to the other non-anonymous features in the dataset: Amount and Time. In the next two code cells we investigate their sample disributions. The `distplot` function in `seaborn` can automatically generate a histogram and sample density estimation for the given variable.","e3a6e3d4":"This is the data that we will ultimately train and test our model with.\n\n \n## Things are Heating Up: Building a Classification model\nNow that we have a firmer grip on the data available to us we can actually start working on the task at hand: classifying credit card fraud. We all know that machine learning is the new black -- AI will take all over our jobs if we're not careful. However, there is \n\n### Training and Testing Data\n\nOne of the core concepts in machine learning is splitting the data available to you between a larger training set, used to train the model and tune parameters, and a smaller testing set, used to test the efficacy of the model. Often a third set -- a validation set -- is also carved out. How to divvy up the data between training and testing (and possibly validation) depends on the task itself and the underlying nature of the data. \n\nIf we assume that there is no trend element to the number of frauds or any of the underlying features, we can shuffle our data and randomly decide if an observation should be used for training or testing. Lets go with that for now. How much data to save for testing purposes is also highly dependent on the task and the data itself, but a very rough rule of thumb is 20-25%.","9b55b4c4":"The number of fraudulent transactions is completely dwarfed by the number of non-fraudulent -- the dataset is highly _imbalanced_. We saved the results of `value_counts` in the variable `v_count` and can access each of the to counts as _indexes_ of `v_count`: `v_count[0]` and `v_count[1]`. Try it yourself!","e7d1af03":"When working with time-series data, such as this dataset, the time is usually used as the *index* of the set. We can tell pandas to use a particular column as index with the function `set_index`. This is essential for making our data visualisation tools easy to use.","fb14e189":"## Testing\nSince we are using < 1000 observations to train our models, running the `.fit` method takes no time at all. In the future, when your training complex models and have access to a huge dataset, it is usually a good idea to only use a subsample of that data while exploring the different approaches available to you.\n\nWe are now ready to test the efficacy of our two models. We will do a bit of \"manual\" work here to make clear what we are measuring, instead of using the built-in function \".score\", but when your playing around yourself this built-in scoring is great!\n\nRemember that we want to test our model on the real data that we have saved for this occasion. It contains very few actual cases of fraud compared to the data we used for training.","f756a4c1":"Dataframes come with loads of useful built-in functions that can be called using a period (.) followed by the function name. For example, if we want to know the total number of cases in Halland and Stockholm, we can do the following:","abb23262":"## Training Models\n\nCongratulations! You've made it to the most exciting part of the workshop. Now that we have all our training and testing data in order, we're ready to finally start training some models. As we are using random under-sampling we do not have that much data to train our models with, which tends to favor keeping the them relatively simple. Today we will play with two types of models:\n\n* [Logistic Regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression): A statistical modelling technique akin to linear regression\n* [k-Nearest Neighbors](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm): a classic lazy learning algorithm\n\nIt is this point that is often the most computationally intensive part of machine learning, in which we look for the parameter and hyper-parameter sets that give the best results for each model. To save some time I have \"tv-kockat\" a tiny bit and found hyper-parameter sets that give reasonable results for both algorithms. \n\nLets start by importing the two models we'll use and separate the training data between the _independent variables_ `X` and the _dependent variable_ `y`."}}