{"cell_type":{"32b66543":"code","14cb933e":"code","0cb7f486":"code","f41ad597":"code","6a46c16c":"code","dd1207a1":"code","2cc73fe6":"code","5214e4fd":"code","ce02869b":"code","b7f70580":"code","080e20f8":"code","6850ec15":"code","d1661b93":"code","dc78c009":"code","625eed4f":"code","bb8423e3":"code","cec4075a":"code","fe5cb62b":"code","52926d0c":"code","d64213fb":"code","1f8ed461":"code","9c295488":"code","ef067daa":"code","6049c6c1":"code","303b4034":"code","3f0e7de4":"code","a5cad818":"code","8c997e65":"code","9d848b6c":"code","57a17d39":"code","33e4d242":"code","88a20b48":"code","2c5211cc":"code","59c212dc":"code","0a5588c3":"code","4d91b013":"code","660d32f6":"code","09e9fc3d":"code","b90bf230":"code","f34413ea":"code","01e21c75":"code","db9e9e2d":"code","a62dc1ab":"code","f7ef4483":"code","8c830452":"code","043506d3":"code","45838359":"code","43f8c60f":"code","09fc8942":"code","6020efc9":"code","d1991d3a":"code","af489a07":"markdown","45941ff0":"markdown","719f71e2":"markdown","3dbee989":"markdown","e51bc252":"markdown","fc37ae9e":"markdown","a4d19e69":"markdown","61ef49ae":"markdown","df59eb12":"markdown","c28ee2ed":"markdown","8d3e6bba":"markdown","52af56d9":"markdown","88eaa392":"markdown","38ca0159":"markdown","49aa678a":"markdown","1b85c5d0":"markdown","69539eae":"markdown","77d2bc24":"markdown","4febbb45":"markdown","f2da21f3":"markdown","4a5584ef":"markdown","ab8853f6":"markdown","64f531ea":"markdown","e049767e":"markdown","bf982dd7":"markdown","a08c9aa1":"markdown","eb2ae297":"markdown","dde002bf":"markdown","351dbbbf":"markdown","ff64de2b":"markdown","f695a7e9":"markdown","d3d1a230":"markdown","c369c67a":"markdown","30bc8039":"markdown","c183288c":"markdown","7cecf4cd":"markdown","f7171350":"markdown","c47da125":"markdown","e9b1da5b":"markdown","184e1ab8":"markdown","adb48985":"markdown","1cfa92b6":"markdown","ec74759f":"markdown","6fbc67a0":"markdown","baaee8f3":"markdown","bde52b45":"markdown","37a5ff8d":"markdown","6719eaa8":"markdown","d85e22f8":"markdown","10cb0b8d":"markdown","a8763d4e":"markdown","5963f252":"markdown","d8001e9a":"markdown","0b9ca147":"markdown","062fb81a":"markdown","1f3e5b8a":"markdown","7b722a7c":"markdown","3ed83208":"markdown"},"source":{"32b66543":"!pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887\n!apt update && apt install -y libsm6 libxext6","14cb933e":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","0cb7f486":"from fastai.imports import *\nfrom fastai.structured import train_cats, proc_df, rf_feat_importance\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom sklearn import metrics","f41ad597":"import seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')","6a46c16c":"# !ls ..\/input","dd1207a1":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","2cc73fe6":"df_train.head()","5214e4fd":"df_train.drop('Id', axis=1, inplace=True)\ndf_test.drop('Id', axis=1, inplace=True)","ce02869b":"df_train.head()\n","b7f70580":"fig ,ax = plt.subplots()\nax.scatter(x=df_train['GrLivArea'],y = df_train['SalePrice'])\nplt.title('Showing Outliers')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","080e20f8":"#Deleting outliers\ndf_train.drop(df_train[(df_train['GrLivArea'] > 4000) & (df_train['SalePrice'] < 200000)].index, inplace=True)","6850ec15":"# check the graph again\nfig ,ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.show()","d1661b93":"sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# #Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.show()\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()\n","dc78c009":"df_train['SalePrice'].head()","625eed4f":"df_train[\"SalePrice\"] = np.log(df_train[\"SalePrice\"])\n\nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# #Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.show()\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","bb8423e3":"# from scipy.special import boxcox1p","cec4075a":"# df_train['SalePrice'] = boxcox1p(df_train['SalePrice'], 0.15)","fe5cb62b":"# #df_train[\"SalePrice\"] = np.log(df_train[\"SalePrice\"])\n\n# sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# # Get the fitted parameters used by the function\n# (mu, sigma) = norm.fit(df_train['SalePrice'])\n# print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# # #Now plot the distribution\n# plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\n# plt.ylabel('Frequency')\n# plt.title('SalePrice distribution')\n# plt.show()\n\n# #Get also the QQ-plot\n# fig = plt.figure()\n# res = stats.probplot(df_train['SalePrice'], plot=plt)\n# plt.show()","52926d0c":"n_train = df_train.shape[0]\nn_test = df_test.shape[0]\ny = df_train.SalePrice.values\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop('SalePrice', axis=1, inplace=True)\nprint(\"Size of all data : {}\".format(all_data.shape))","d64213fb":"all_data.head()","1f8ed461":"train_cats(all_data)","9c295488":"all_data,Alley,nas=proc_df(all_data,'Alley')","ef067daa":"all_data.head()","6049c6c1":"all_data.columns","303b4034":"dd=['BsmtFinSF1_na', 'BsmtFinSF2_na', 'BsmtFullBath_na', 'BsmtHalfBath_na', 'BsmtUnfSF_na', 'GarageArea_na',\n       'GarageCars_na', 'GarageYrBlt_na', 'LotFrontage_na', 'MasVnrArea_na', 'TotalBsmtSF_na']","3f0e7de4":"all_data.drop(dd, axis=1, inplace=True)\n","a5cad818":"all_data.shape","8c997e65":"all_data.head()","9d848b6c":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","57a17d39":"train = all_data[:n_train]\ntest = all_data[n_train:]","33e4d242":"train.head()","88a20b48":"test.head()","2c5211cc":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train),y_train), rmse(m.predict(X_valid),y_valid),\n           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","59c212dc":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 88\nn_trn = len(train) - n_valid\nX_train, X_valid = split_vals(train, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","0a5588c3":"m = RandomForestRegressor(n_jobs=-1, random_state=1)\nm.fit(X_train, y_train)\nprint_score(m)","4d91b013":"m = RandomForestRegressor(n_jobs=-1, random_state=2, n_estimators=46, oob_score=True, max_features=0.5, \n                          max_depth=10)\nm.fit(X_train,y_train)\nprint_score(m)","660d32f6":"m = RandomForestRegressor(n_jobs=-1, random_state=1, n_estimators=25, oob_score=True, max_features=0.6)\nm.fit(X_train,y_train)\nprint_score(m)","09e9fc3d":"m = RandomForestRegressor(n_jobs=-1, random_state=1, n_estimators=40, oob_score=True, max_features=0.6, \n                          )\nm.fit(X_train,y_train)\nprint_score(m)","b90bf230":"m = RandomForestRegressor(n_jobs=-1, random_state=6, n_estimators=60, oob_score=True, max_features=0.5, \n                          max_depth=16, min_samples_leaf=3)\nm.fit(X_train,y_train)\nprint_score(m)","f34413ea":"m = RandomForestRegressor(n_jobs=-1, random_state=6, n_estimators=60, oob_score=True, max_features=0.4, \n                          max_depth=16, min_samples_leaf=3, max_leaf_nodes=450)\nm.fit(X_train,y_train)\nprint_score(m)","01e21c75":"m = RandomForestRegressor(n_jobs=-1, random_state=10, n_estimators=100, oob_score=True, max_features=0.5, \n                          max_depth=14, min_samples_leaf=3, max_leaf_nodes=400, min_impurity_decrease=0.00001)\nm.fit(X_train,y_train)\nprint_score(m)","db9e9e2d":"m = RandomForestRegressor(n_jobs=-1, random_state=10, n_estimators=160, oob_score=True, max_features=0.5, \n                          max_depth=14, min_samples_leaf=2, max_leaf_nodes=400, min_impurity_decrease=0.00001)\nm.fit(X_train,y_train)\nprint_score(m)","a62dc1ab":"m = RandomForestRegressor(n_jobs=-1, random_state=10, n_estimators=160, oob_score=True, max_features=0.5, \n                          max_depth=None, min_samples_leaf=2, max_leaf_nodes=250, min_impurity_decrease=0.00001,\n                          min_impurity_split=None)\n\nm.fit(X_train,y_train)\nprint_score(m)","f7ef4483":"SalePrice = m.predict(test)","8c830452":"df_sample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","043506d3":"df_sample.head()","45838359":"SalePrice = np.exp(SalePrice)","43f8c60f":"df_sample['SalePrice'] = SalePrice","09fc8942":"df_sample.head()","6020efc9":"df_sample.to_csv('Home_price.csv', columns=['Id','SalePrice'], index=False)","d1991d3a":"df_sample.SalePrice.head()","af489a07":"# Feature Engineering","45941ff0":"Write some function which are helpful in calculations, **rmse, print_score, split_vals** function are created. \n\nThe rmse calculate **Root Mean Square Error** for the given data, print_score calculate the rmse of training and testing data and accuracy of training and testing data.\n\nThe data is already randomised hence we don't need to use random split.","719f71e2":"This transformation is same like log transformation.","3dbee989":"### Box-Cox Transformation\n","e51bc252":" After deleting outliers from the data scatter plot showing the data looks like.","fc37ae9e":"**Finally, I give my best effort to build a random forest model** using number of parameters of random forest.\n\nFor this model I have tuned every parameter of the RandomForestRegressor and finally represent model with **OOB_score = 0.89291**. ","a4d19e69":"## Final model","61ef49ae":"Here are some of the links I found helpful while writing kernel\n\n- [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) from this kernel I have learned Box-Cox transformation as well as how combine train and test data and perform analysis.","df59eb12":"# Target Variable","c28ee2ed":"## Drop unnecessary features","8d3e6bba":"## Find outliers","52af56d9":"# Table of Contents","88eaa392":"## Writting necessary functions for shortcut","38ca0159":"## Import liabraries","49aa678a":"The hyperline is not fitted on the given data because values of target variable is too high, overcome this problem by using transformation.That's why I'm applying log transformation.\n\nAfter transformation, we can see how hyperline fits on target variable.","1b85c5d0":"In given data there are total 81 features of home and using these features you are going to predict the home prices.\n\n[Here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) you can find description of all the features. I have decided to delete Id feature from the data because it is not important for prediction.","69539eae":"This model improve the OOB_score as compared with previous model. The **OOB_score** of current model is **0.8908** and previous model OOB_score is **0.8862**.\n\nIn this model added one new parameter named as min_impurity_decrease. This parameter a node will be split if this split induces a decrease of the impurity greater than or equal to this value","77d2bc24":"## Log transformation","4febbb45":"After observing all features of dataframe decided to create one new feature named as **TotalSF**. The **TotalSF** is created by adding **1stFlrSF**, **2ndFlrSF** and **TotalBsmtSF** and added this feature into original dataframe.","f2da21f3":"Python is an amazing language with many libraries. I am going to import the nessary library as we go on.\n\n","4a5584ef":"I am using deep learning liabrary named as fast.ai for this kernel. Fast.ai provide special functions like train_cats, proc_df, etc. The train_cats function change any columns of strings in a panda's dataframe to a column of categorical value and proc_df function converts null values with median as well as separate target variabe from training data.","ab8853f6":"## Tested on test data","64f531ea":"# Overview and cleaning data","e049767e":"In this model added one new parameter named as min_samples_leaf. It means at the end leaf of tree contains minimum 3 data points.\n\nCheck small change in OOB_score of this model compare with previous model. ","bf982dd7":"# Resources","a08c9aa1":"After applying proc_df many na columns are created and this are not useful when training model that's why deleting all na columns from dataframe ","eb2ae297":"Again improvement in the next model as compared with previous model. \n\nWe can see OOB_score is increased while tuning parameters. The **OOB_score** of current model is **0.8928** while **OOB_score** of previous model is **.8908**","dde002bf":"**In statistics, an outlier is an observation point that is distant from other observations.**\n\nThe above definition suggests that outlier is something which is separate\/different from the crowd\n","351dbbbf":"   **A Random Forest Algorithm & Machine Learning Workflow of House Price**","ff64de2b":"## Use fast.ai liabrary function","f695a7e9":"# Model tested on test data","d3d1a230":"The proc_df function separate the responce variable but I'm passing other unnecessary feature(Alley) beacause I'm already separate response variable.\n\nOne more reason for not passing response variable is I have used combined data (train & test data for analysis) and in test data response variable is not present.\n\nIf you perform analysis on only training data then you are able to pass the response variable to the proc_df function.","c369c67a":"I learned a lot of things from my first kernel and I treid not to repeat the mistakes in this kernel. I am excited to share my second kernel with the Kaggle community, and I think my journey of data science can leap from this community. please leave a comment if you have any suggestions to make it better!! \n\nGoing back to the topic of this kernel, I am using visualizations to explain the data, and the  random forest machine learning algorithm used to build model and predict the home prices.","30bc8039":"## Normal distribution on target variable","c183288c":"- To **[fast.ai](https:\/\/www.fast.ai\/)** where I started my deep learning journey.\n- To **[Palash Karmore](https:\/\/www.linkedin.com\/in\/palash-karmore?originalSubdomain=in)**, for motivating as well as give suggestions on my mistake when I'm creating in my data science   journey.  ","7cecf4cd":"## Deleting Outliers","f7171350":"We can see the changes values of n_estimators, max_features and decreases OOB_score of the model. ","c47da125":"## About dataset","e9b1da5b":"After applying feature enginering we are able to build model on data. \n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nRandom Forest Regressor.\n\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\nAccording to Random Forest\ndefinition it is fit for building model on given data. \n\nSo, Let start to build random forest model!","184e1ab8":"Separate the training and testing data and build a model on training data.","adb48985":"# Credits\n","1cfa92b6":"## Combine train & test data","ec74759f":"We need to anaylse both training and testing data. There is no need of separate analysis on training as well as testing data. We can combine both train and test data and perform analysis on it.\n\nWe are able to split train and test data in given format after complete analysis on combined data. \n\n","6fbc67a0":"We can see the how fast.ai liabrary function work on data. \n\nThe train_cats function change any columns of strings in a panda's dataframe to a column of categorical values. \n\nproc_df takes a data frame and splits off the response variable, and changes the dataframe into an entirely numeric dataframe or they can do more than that you can see the fast.ai code on [github](https:\/\/github.com\/fastai\/fastai).","baaee8f3":"We can see the normal distribution on target variable, the graph shows the distribution is right skewed and QQ-plot shows the how hyperline fits on given data.","bde52b45":"Using all features build random forest model with two parameters and print accuracy of training and testing data by calling print_score function.\n","37a5ff8d":"## Split the train and test data","6719eaa8":"      \nBeing a part of Kaggle gives me unlimited access to learn, share and grow as a Data Scientist. In this kernel, I want to solve Home Price Prediction competition, a popular machine learning dataset for beginners. I am going to share how I work with a dataset step by step from data preparation, data analysis and implementing machine learning models. I will also describe the model results along with many other tips. So let's get started.\n\nIf there are any recommendations\/changes you would like to see in this notebook, please leave a comment at the end of this kernel. \nIf you like this notebook or find this notebook helpful, Please feel free to **UPVOTE** and\/or leave a comment.\n\nYou can also Fork and Run this kernel on **Github**\nStay Tuned for More to Come!!","d85e22f8":"We can see at the bottom right two points with extremely large GrLivArea that are of a low price. These values are huge oultliers.\n\nThat's why I decided the delete them from data.","10cb0b8d":"## Tuning hyperparameters","a8763d4e":"![](http:\/\/realsta.in\/wp-content\/uploads\/2016\/01\/best-home-house-builder-civil-contractor-delhi-gurgaon-india-maxwell-builder.jpg)","5963f252":"# Built the model","d8001e9a":" - [Introduction](.\/House%20price%20kaggle%20final.ipynb#Introduction)\n\n     - [Importing liabraries](#Import-liabraries)\n     \n     - [About Dataset](#About-dataset)\n \n - [Overview and Cleaning Data](#Overview-and-cleaning-data)\n \n     - [Find outliers](#Find-outliers)\n     \n     - [Delating outliers](#Deleting-Outliers)\n     \n - [Target Variable](#Target-Variable)\n \n     - [Normal distribution on target variable](#Normal-distribution-on-target-variable)\n     \n     - [Log transformation](#Log-transformation)\n     \n - [Feature Engineering](#Feature-Engineering)\n \n     - [Combine train and test data](#Combine-train-&-test-data)\n     \n     - [Use fast.ai liabrary function](#Use-fast.ai-liabrary-function)\n     \n     - [Drop unnecessary features](#Drop-unnecessary-features)\n     \n     - [split train  and test data](#Split-the-train-and-test-data)\n \n - [Build the model](#Built-the-model)\n \n     - [Writting necessary functions for shortcut](#Writting-necessary-functions-for-shortcut)\n     \n     - [Tuning hyperparameter](#Tuning-hyperparameters)\n     \n     - [Final model](#Final-model)\n     \n - [Model tested on test data](#Model-tested-on-test-data)\n \n     - [Tested on test data](#Tested-on-test-data)\n     \n     - [Build submission file](#Build-submission-file)\n\n - [Credits](#Credits)","0b9ca147":"## Build submission file","062fb81a":"This model is not good while comparing with old model because in this model reduces OOB_score while tuning hyperparameter.\n\nAdded max_leaf_nodes parameter in this model this represents maximum 450 leaf nodes are available in the tree.","1f3e5b8a":"# Introduction\n","7b722a7c":"Build the following model using 6 parameters and see result in the output. We can see in this model I'm using maximum 50% features, build 46 decision trees, keep the maximum depth of each decision tree is 10. \n\nOut-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests\n\nOOB_score defines score of the training dataset obtained using an out-of-bag estimate.\n\nThere is small change in this model by comparing with previous model.","3ed83208":"If increasing trees in the forest as well as increases max_features, there is small change in OOB_score but first model is better than this model."}}