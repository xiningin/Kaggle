{"cell_type":{"55fc6f43":"code","ae0f8222":"code","12f42ff4":"code","f5923488":"code","c8c783f9":"code","6aec5ecd":"code","148a1d37":"code","e0528f6b":"code","82abd190":"code","8772636c":"code","e28e8912":"code","637d06db":"code","869775a5":"code","aa354f45":"code","86e49d85":"code","55cb8ada":"code","79e38478":"code","d3d8297f":"code","4d43e7ff":"code","06e4b1cf":"code","284432f5":"code","9965561a":"code","90985f1e":"code","1337e43b":"code","83b6a8d0":"code","75fa46bd":"code","3aa1bcf4":"code","b44d7da9":"code","b3dfa009":"code","5b507c6e":"code","2211d5c0":"code","deb5e0b6":"code","e1a9070d":"code","ef6d9c70":"code","8c506805":"code","d78c250e":"code","50127770":"code","a6729d4e":"code","3c433657":"code","1ce0e097":"code","c2a2ac56":"code","af10d244":"code","af84d658":"code","11d50ed7":"code","b1e556bf":"code","3c9eaab9":"code","d8715be0":"code","05fa5316":"code","77d8701d":"markdown"},"source":{"55fc6f43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae0f8222":"import tensorflow as tf\n\nimport os\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    print(\"Not connected to a TPU runtime. Using CPU\/GPU strategy\")\n    strategy = tf.distribute.MirroredStrategy()","12f42ff4":"strategy.scope()","f5923488":"# !conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n# !conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n# !conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n# !conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n# !conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n# !conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","c8c783f9":"# !pip install efficientnet","6aec5ecd":"# import efficientnet.keras as efn\nimport os\nimport shutil\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense , Conv2D , Dropout , MaxPooling2D , Flatten, Activation , BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.optimizers import Adam, SGD, Adadelta\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","148a1d37":"random.seed(7)","e0528f6b":"preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n","82abd190":"# base_model = EfficientNetB7(weights='imagenet',include_top=False,input_shape=(600,600,3))\n# base_model.trainable = False\n# base_model.summary()","8772636c":"%cd ..\n%mkdir tmp\n%cd tmp\n%ls\n","e28e8912":"a = os.listdir('..\/input\/efficientnetb7-dataset-augmented\/augmented\/train')\na","637d06db":"# for i in a:\n#     os.mkdir('NewPreprocessed\/{}'.format(i))","869775a5":"shutil.copytree(\"..\/input\/efficientnetb7-dataset-augmented\/augmented\/train\",\"\/kaggle\/tmp\/NewPreprocessed\")","aa354f45":"# from PIL import Image\n# import os, sys\n\n\n# def resize(path,savepath):\n#     for item in os.listdir(path):\n#         item_path = path+\"\/\"+item\n#         im = Image.open(item_path)\n#         imResize = im.resize((600,600), Image.ANTIALIAS)\n#         imResize.save(savepath +\"\/\"+item.split('.')[0]+ '_resized.jpg', 'JPEG')\n\n# for i in a:\n#     path = \"..\/input\/rsna-balenced-dataset\/Processed\/\"+i\n#     savepath = \"NewPreprocessed\/\"+i\n#     resize(path,savepath)\n    ","86e49d85":"%ls","55cb8ada":"for i in a:\n    savepath = \"NewPreprocessed\/\"\n    print(len(os.listdir(savepath+i)))","79e38478":"split_size = .85\ndef split_data(SOURCE, TRAINING, VALIDATION, SPLIT_SIZE):  \n    all_images = os.listdir(SOURCE)\n    print(type(all_images))\n    random.shuffle(all_images)\n    splitting_index = round(SPLIT_SIZE*len(all_images))\n\n    #print(splitting_index)\n    #print(splitting_index+portion)\n\n    train_images = all_images[:splitting_index]\n    valid_images = all_images[splitting_index:]\n    for img in train_images:\n        shutil.copy(os.path.join(SOURCE,img),TRAINING)\n    for img in valid_images:\n        shutil.copy(os.path.join(SOURCE,img),VALIDATION)\n","d3d8297f":"os.mkdir('train')\nos.mkdir('val')","4d43e7ff":"for i in a:\n    source = \"NewPreprocessed\/\"+i\n    train = \"train\/\"+i\n    val = \"val\/\"+i\n    os.mkdir(train)\n    os.mkdir(val)\n    split_data(source, train, val, split_size)","06e4b1cf":"for i in a:\n    train = \"train\/\"+i\n    val = \"val\/\"+i\n    print(len(os.listdir(train)),len(os.listdir(val)))","284432f5":"batch_size = 64\n","9965561a":"# TRAINING_DIR = 'train'\n# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n# train_generator =  train_datagen.flow_from_directory(TRAINING_DIR,\n#                                                       target_size=(600,600),\n#                                                       batch_size=16,\n#                                                      class_mode='categorical',\n#                                                      shuffle = True)\n# VALIDATION_DIR = 'val'\n# validation_datagen = ImageDataGenerator()\n# validation_generator =  validation_datagen.flow_from_directory(VALIDATION_DIR,\n#                                                       target_size=(600,600),\n#                                                       batch_size=16,\n#                                                       class_mode='categorical',\n#                                                       shuffle = True)","90985f1e":"TRAINING_DIR = 'train'\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n                                                      TRAINING_DIR,\n                                                        shuffle = True,\n                                                      image_size=(600,600),\n                                                      batch_size=batch_size,\n                                                        label_mode='categorical')\nVALIDATION_DIR = 'val'\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n                                                      VALIDATION_DIR,\n                                                    shuffle = True,\n                                                      image_size=(600,600),\n                                                    label_mode='categorical',\n                                                      batch_size=batch_size)","1337e43b":"class_names = train_ds.class_names\nclass_names","83b6a8d0":"# plt.figure(figsize=(10, 10))\n# for images, labels in train_ds.take(1):\n#     for i in range(9):\n#         ax = plt.subplot(3, 3, i + 1)\n#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n#         plt.title(class_names[labels[i]])\n#         plt.axis(\"off\")","75fa46bd":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.prefetch(buffer_size=AUTOTUNE)","3aa1bcf4":"# base_model = EfficientNetB7(weights='imagenet',include_top=False,input_shape=(600,600,3))","b44d7da9":"# base_model.trainable = False\n# for i in base_model.layers[-10:]:\n#     i.trainable = True\n#     print(i.trainable)\n# for i in base_model.layers[-20:]:\n#     print(i.trainable)","b3dfa009":"def create_model():\n    inputs = tf.keras.Input(shape=(600, 600, 3))\n    preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n    data_augmentation = tf.keras.Sequential([\n                                              #tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n                                              tf.keras.layers.experimental.preprocessing.RandomRotation(0.03),\n                                            ])\n    \n    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n#     prediction_layer0 = tf.keras.layers.Dense(2048,activation=\"relu\")\n#     prediction_layer1 = tf.keras.layers.Dense(1024,activation=\"relu\")\n    prediction_layer2 = tf.keras.layers.Dense(512,activation=\"relu\")\n    prediction_layer3 = tf.keras.layers.Dense(128,activation=\"relu\")\n    prediction_layer4 = tf.keras.layers.Dense(4,activation=\"softmax\")\n    \n    #x = data_augmentation(inputs)\n    x = preprocess_input(inputs)\n    base_model = EfficientNetB7(weights='imagenet',include_top=False,input_tensor=x)\n    base_model.trainable = False\n    for layer in base_model.layers[-10:]:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n            layer.trainable = True\n    x = global_average_layer(base_model.output)\n#     x = BatchNormalization()(x)\n#     x = Dropout(0.2)(x)\n#     x = prediction_layer0(x)\n#     x = Dropout(0.2)(x)\n#     x = prediction_layer1(x)\n    x = Dropout(0.1)(x)\n    x = prediction_layer2(x)\n    x = Dropout(0.2)(x)\n    x = prediction_layer3(x)\n    outputs = prediction_layer4(x)\n    model = tf.keras.Model(inputs, outputs)\n    return(model)","5b507c6e":"initial_learning_rate = 0.01\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=5000,\n    decay_rate=0.1,\n    staircase=False)\n","2211d5c0":"with strategy.scope():\n    Model = create_model()\n    optimizer = Adam(learning_rate=lr_schedule)\n    Model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n#Model = tf.keras.models.load_model(\"..\/input\/efficientnetb7\/weights\/my_model.h5\")\n\nhistory = Model.fit(train_ds,epochs=50,batch_size=batch_size,validation_data=val_ds) \n\nModel.save('..\/working\/weights\/my_model.h5')","deb5e0b6":"#Model.save('..\/working\/weights\/my_model.h5')","e1a9070d":"# with strategy.scope():\n#     Model = create_model()\n#     optimizer = SGD(learning_rate=lr_schedule)\n#     Model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n# Model = tf.keras.models.load_model(\"..\/input\/efficientnetb7\/weights\/my_model.h5\")\n\n# history = Model.fit(train_ds,epochs=10,batch_size=batch_size,validation_data=val_ds) \n\n# Model.save('..\/working\/weights\/my_model.h5')","ef6d9c70":"# with strategy.scope():\n#     Model = create_model()\n#     optimizer = Adadelta(learning_rate=lr_schedule)\n#     Model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n# Model = tf.keras.models.load_model(\"..\/input\/efficientnetb7\/weights\/my_model.h5\")\n\n# history = Model.fit(train_ds,epochs=6,batch_size=batch_size,validation_data=val_ds) \n\n# Model.save('..\/working\/weights\/my_model.h5')","8c506805":"print(Model.input_shape,Model.output_shape)\n#Model.summary()","d78c250e":"\n# history = Model.fit(train_generator,epochs=40,batch_size=16,validation_data=validation_generator)\n#print(len(base_model.layers))","50127770":"# with strategy.scope():","a6729d4e":"# os.mkdir(\"..\/working\/weights\")\n# Model.save_weights(\"..\/working\/weights\/weights.pt\")","3c433657":"# os.rmdir(\"..\/working\/weights\")","1ce0e097":"#Model = tf.keras.models.load_model(\"..\/input\/efficientnetb7\/weights\/my_model.h5\")","c2a2ac56":"#history = Model.fit(train_ds,epochs=25,batch_size=16,validation_data=val_ds) ","af10d244":"#Model =  tf.keras.models.load_model(\"\/kaggle\/input\/efficientnetweights\/weights\/my_model.h5\")","af84d658":"#Model = tf.keras.models.load_model(\"\/kaggle\/input\/efnetb7-layers-increased-more-trainable-layers\/weights\/my_model.h5\")","11d50ed7":"# Model = create_model()\n#Model.load_weights(\"..\/input\/efficientnetweights\/weights\/\")","b1e556bf":"#Model.evaluate(val_ds)","3c9eaab9":"# def unfreeze_model(Model):\n#     # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n#     for layer in Model.layers[-30:]:\n#         if not isinstance(layer, tf.keras.layers.BatchNormalization):\n#             layer.trainable = True\n\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n#     Model.compile(\n#         optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n#     )\n\n\n# unfreeze_model(Model)\n\n# epochs = 10  # @param {type: \"slider\", min:8, max:50}\n# hist = Model.fit(train_ds, epochs=epochs,batch_size=batch_size, validation_data=val_ds)\n# plot_hist(hist)","d8715be0":"#Model.summary()","05fa5316":"#Model.layers","77d8701d":"Loading and saving"}}