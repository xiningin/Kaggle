{"cell_type":{"f9a9bf72":"code","c6ef3d3e":"code","d5cf9c0b":"code","3b4ff0e0":"code","7b5d8862":"code","b6e24c19":"code","31550e58":"code","f4c0c7f3":"code","9d4abf63":"code","ffd33635":"code","e836557c":"code","82a4b4c9":"code","84c037ab":"code","76672172":"code","70f036d9":"code","58ccd327":"code","86384b22":"code","181d78d6":"code","5dc9c4ed":"code","3b40b478":"code","7dc0045a":"code","b6727ad1":"code","f3acfaee":"code","826513dc":"code","a72ca7f0":"code","215c61bd":"code","a736ab33":"code","ffcf7e5c":"code","f0721af2":"code","c97408b3":"code","d854aba4":"code","6dfa0bc3":"code","c22ea92b":"code","38b069f9":"code","910b14b7":"code","85838f76":"code","3b28c146":"code","a5d595cc":"markdown","a35eaa08":"markdown","2455b53f":"markdown","49e491b4":"markdown","440c5d39":"markdown","74e0480b":"markdown","13c14f42":"markdown","e1359ff5":"markdown","fa8cb1d4":"markdown","f6c46c8f":"markdown","9059652c":"markdown","eeb0f442":"markdown","7c7d9527":"markdown","4914770a":"markdown","bbb9709e":"markdown","be13dff4":"markdown","db8f8fa7":"markdown","edee9add":"markdown","9678a4d5":"markdown","47102317":"markdown","528ab3ee":"markdown","0c52419a":"markdown"},"source":{"f9a9bf72":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\nimport random\nimport os\nimport glob\nimport cv2 \nfrom fastai.vision import *\nfrom fastai import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import plot_confusion_matrix\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(f'Running on device: {device}')\n","c6ef3d3e":"# Set seed fol all\ndef seed_everything(seed=1358):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()","d5cf9c0b":"PATH = Path('..\/input\/planets-dataset\/planet\/planet\/')\ntrain_img = PATH\/'train-jpg'\ntrain_folder = 'train-jpg'\ntest_img = PATH\/'test-jpg'\nmodel_dir = Path('\/kaggle\/working\/')\nbs = 64","3b4ff0e0":"PATH.ls()","7b5d8862":"train_df = pd.read_csv(os.path.join(PATH, 'train_classes.csv'))\n# adding path to the image in our dataframe. \ntrain_df['image_name'] = train_df['image_name'].apply(lambda x: f'{train_folder}\/{x}.jpg')\ntrain_df.head()","b6e24c19":"# Since this is a multi lable task and the labels are given as tags in a single dataframe series\nbiner = MultiLabelBinarizer()\ntags = train_df['tags'].str.split()\ny = biner.fit_transform(tags)\n\nlabels = biner.classes_\nprint('Number of labels: ', len(labels))\nprint(labels)","31550e58":"# Getting the labels into one hot encoded form for EDA ease. \nfor label in labels:\n    train_df[label] = train_df['tags'].apply(lambda x: 1 if label in x.split()  else 0)\n    \ntrain_df.head()","f4c0c7f3":"train_df[labels].sum().sort_values(ascending=False).plot(kind='barh', figsize=(8,8))","9d4abf63":"df_asint = train_df.drop(train_df.columns[[0,1]], axis=1).astype(int)\ncoocc_df = df_asint.T.dot(df_asint)\n\ncoocc_df","ffd33635":"# Confusion matrix. ","e836557c":"#reading images\n\nrandom_imgs = train_df.ix[random.sample(list(train_df.index), 10)][['image_name', 'tags']]\n\nto_read = random_imgs.loc[:, 'image_name'].values\ntags = random_imgs.loc[:, 'tags'].values\n\nimages = [cv2.imread(os.path.join(PATH\/file)) for file in to_read]\nprint(\"Number of images: \", len(images))\nprint(\"Size of an image: \", images[0].shape)","82a4b4c9":"plt.figure(figsize=(25,15))\ncolumns = 5\nfor i, image in enumerate(images):\n    plt.subplot(len(images) \/ columns + 1, columns, i + 1)\n    plt.imshow(image)\n    plt.grid(False)\n    plt.title(tags[i])","84c037ab":"print(f\"Size of Training set images: {len(list(train_img.glob('*.jpg')))}\")\nprint(f\"Size of Test set images: {len(list(test_img.glob('*.jpg')))}\")\n","76672172":"img_size = 128\n\ntfms = get_transforms(do_flip=True,flip_vert=True,p_lighting=0.4,\n                      max_lighting=0.3, max_zoom=1.05, max_rotate=360, xtra_tfms=[flip_lr()])\n\n\n# The datablock API makes things very easy. \n# Im using 1% of the training data to validate the models. \n\nsrc = (ImageList.from_df(train_df, PATH, cols='image_name')\n        .split_by_rand_pct(valid_pct=0.1)\n        .label_from_df(label_delim=' '))\n\n\ndata = (src.transform(tfms,size=img_size,resize_method=ResizeMethod.CROP)\n        .databunch(bs=bs,num_workers=4) \n        .normalize(imagenet_stats)      \n       )","70f036d9":"data","58ccd327":"data.train_ds","86384b22":"data.valid_ds","181d78d6":"data.train_ds[0]","5dc9c4ed":"print(data.train_ds.y[200])\ndata.train_ds.x[200]","3b40b478":"data.show_batch(rows=2)","7dc0045a":"model_1 = models.resnet50\nacc_02 = partial(accuracy_thresh, thresh=0.2)\nf_score = partial(fbeta, thresh=0.2)\n\nlearn = create_cnn(data, model_1, metrics=[acc_02, f_score], model_dir='\/kaggle\/working')","b6727ad1":"learn.lr_find()\nlearn.recorder.plot()","f3acfaee":"learn.fit_one_cycle(5, 0.01)","826513dc":"learn.model_dir = '\/kaggle\/working'\nlearn.save('resnet50-stage1')","a72ca7f0":"model_2 = models.densenet121\nlearn_dense = create_cnn(data, model_2, metrics=[acc_02, f_score], model_dir='\/kaggle\/working')","215c61bd":"learn_dense.fit_one_cycle(5, 0.01)","a736ab33":"learn_dense.save('DenseNet121-stage1')","ffcf7e5c":"learn_dense.unfreeze()\nlearn_dense.lr_find()\nlearn_dense.recorder.plot()","f0721af2":"learn_dense.fit_one_cycle(5, slice(1e-5, 1e-4))","c97408b3":"learn_dense.save('DenseNet121-stage2')","d854aba4":"data_2 = (src.transform(tfms,size=256,resize_method=ResizeMethod.CROP)\n        .databunch(bs=bs,num_workers=4) \n        .normalize(imagenet_stats)      \n       )\n\ndata_2","6dfa0bc3":"model_2 = models.densenet121\nlearn_dense_2 = create_cnn(data_2, model_2, metrics=[acc_02, f_score], model_dir='\/kaggle\/working')","c22ea92b":"learn_dense_2.load('DenseNet121-stage2')","38b069f9":"learn_dense_2.lr_find()\nlearn_dense_2.recorder.plot()","910b14b7":"learn_dense_2.fit_one_cycle(10, 0.01)","85838f76":"learn_dense_2.unfreeze()\nlearn_dense_2.lr_find()\nlearn_dense_2.recorder.plot()","3b28c146":"learn_dense_2.fit_one_cycle(10, slice(1e-5,1e-4))","a5d595cc":"## Importing dependencies and setting file paths","a35eaa08":"Fine-tuning the entire model in stage 2. ","2455b53f":"36432 images for training and 4047 for validating. ","49e491b4":"## Experiment 2\n\n> For the second experiment:\n+ Using DenseNet121 (pretrained on imagenet)\n+ Similar metrics as first experiment. ","440c5d39":"With 5 epochs able to get upto 92% fbeta. ","74e0480b":"Looking at the co-ocurrance for these labels. \n> The combination (primary, clear) has the highest co-ocurrance. Followed by (primary, agriculture)","13c14f42":"Using the usual fast ai to train and evaluate models. ","e1359ff5":"The label primary appears the most in our dataset followed by clear and agriculture. \nAs stated in the data description, primary refers to primary rainforest.\n> Generally speaking, the \"primary\" label was used for any area that exhibited dense tree cover. ","fa8cb1d4":"DenseNet201 performs slighly better than resnet50. \nI ll use this for fine tuning the entire model. ","f6c46c8f":"# Planet: Understanding the Amazon from Space\n## Use satellite data to track the human footprint in the Amazon rainforest\n\nMulticlass classification for classifying satelite images of the Amazon rainforest. \nExploring the data and Fast Ai. ","9059652c":"## Training ","eeb0f442":"## Experiment 3\n\nFine tuning the network further with larger image size. ","7c7d9527":"Picking a point from the train_ds from our databunch gives the Image object which includes its size. \nIt also outputs that the label is Multicategory coupled with its value. ","4914770a":"Train data has 36432 images of size 128x128x3. ","bbb9709e":"## Experiment 1\n\n> For the first experiment: \n> + Using resnet50 (pretrained on imagenet)\n> + fbeta with 0.2 threshold and accuracy as metrics. \n","be13dff4":"Starting with an image size of 128*128 with a few transformations. \n+ Flipping the image vertically and horizontaly. \n+ Changing lighting and contrast. \n+ Rotations. \n+ Zooming. ","db8f8fa7":"Fine-tuning last layers in Stage 1 ","edee9add":"> We have 17 unique labels for this data","9678a4d5":"A random point from the databunch. \nThis time i select the labels from **data.train_ds.y** and image from **data.train_ds.x**\n\nThe databunch containing the train dataset has both x and y components and we can index into them. ","47102317":"> Every experiment would have two stages: \n+ 1st stage: Freezing early layers and only fine-tuning the last few newly added layers.\n+ 2nd stage: Unfreezing all the layers and fine-tuning them. \n\n\nUsing the learning rate finder before every stage. \n","528ab3ee":"Plotting a few random images with there labels to see how the data looks. \nChoose 10 random images from the data. ","0c52419a":"## Little bit of EDA"}}