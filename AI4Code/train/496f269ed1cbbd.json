{"cell_type":{"8a7dfaca":"code","b11912c7":"code","4905d229":"code","5749841c":"code","a8e4814b":"code","19042c52":"code","e0d5d1d4":"code","4602ab64":"code","2cac5d75":"code","255ce74b":"code","20f1f28f":"code","3b8de791":"code","efeb6a6d":"code","5cb8c112":"code","f1e67928":"code","1fd564be":"code","1234478c":"code","df3b8cfc":"code","983c5eb1":"code","bbc5f947":"code","4672125c":"code","f7a97b9e":"code","cec2b8cb":"code","3ce1eec5":"code","7b73083e":"code","c12d243a":"code","c4b27a76":"code","08cdbf20":"code","58d3df67":"code","61ac9ab2":"code","3d8742a7":"code","83ce724c":"code","c54b25c8":"code","fd72efc8":"code","417d9cda":"code","ebca3c76":"code","5255e367":"code","cbace4d0":"code","97208aaf":"code","44b2b8b2":"code","e617cc80":"code","8a270521":"code","4ae48114":"code","1e3684c2":"code","6d38de7a":"code","750ab2d0":"code","0bf96b77":"code","cd0d635c":"code","11d61cac":"code","6c90d542":"code","68bf2e9f":"code","92124be1":"code","75dcef90":"code","6f9528ed":"code","2a47405e":"code","947d8f87":"code","6b2042f0":"code","1998a8e5":"code","4e877812":"markdown","8e35cbf0":"markdown","658148c7":"markdown","1ca5aab8":"markdown","eaa040e4":"markdown","364c98f2":"markdown","6934155d":"markdown","4bdc0825":"markdown","f3a40fa3":"markdown","31d175c1":"markdown","9a642043":"markdown","936bb2e8":"markdown","be0d29dd":"markdown","3fcd30c3":"markdown","4c9f6773":"markdown","040640f8":"markdown","fca6962a":"markdown","31bc4c9a":"markdown","30032733":"markdown","e069cb2e":"markdown","cda56dcb":"markdown","a0bf2264":"markdown","f4acc139":"markdown","a8334a0b":"markdown","0e8c8ffb":"markdown","1e1cce45":"markdown","d41fc105":"markdown","b93f8f9d":"markdown","f0fd6fc6":"markdown","eecacc64":"markdown","8cc44038":"markdown","085abf33":"markdown","3e0e733e":"markdown","825a7ed3":"markdown","75bd03c1":"markdown"},"source":{"8a7dfaca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy","b11912c7":"df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv',encoding='ISO-8859-1')","4905d229":"df.head()","5749841c":"df_tweets=df[['OriginalTweet','Sentiment']]","a8e4814b":"df_tweets.head()","19042c52":"100 * df_tweets['Sentiment'].value_counts()\/len(df_tweets)","e0d5d1d4":"df_tweets.isna().sum()","4602ab64":"# Checking if there are any \"blank\" tweets. If there are, then we'll remove such tweets.\nblanks = []\nfor i, tweet, lb in df_tweets.itertuples():\n    if type(tweet) == str:\n        if tweet.isspace():\n            blanks.append(i)","2cac5d75":"blanks # no blank tweets","255ce74b":"plt.figure(figsize=(12,6))\nsns.histplot(data=df_tweets,x='Sentiment')","20f1f28f":"df2 = df_tweets.copy()","3b8de791":"from nltk.sentiment import SentimentIntensityAnalyzer","efeb6a6d":"sia = SentimentIntensityAnalyzer()","5cb8c112":"sia.polarity_scores(df2['OriginalTweet'][3])","f1e67928":"df2['pos_score'] = df2[\"OriginalTweet\"].apply(lambda x: sia.polarity_scores(x)['pos'])\ndf2['neg_score'] = df2[\"OriginalTweet\"].apply(lambda x: sia.polarity_scores(x)['neg'])\ndf2['neu_score'] = df2[\"OriginalTweet\"].apply(lambda x: sia.polarity_scores(x)['neu'])\ndf2['comp_score'] = df2[\"OriginalTweet\"].apply(lambda x: sia.polarity_scores(x)['compound'])","1fd564be":"df2.head()","1234478c":"plt.figure(figsize=(11,6))\nsns.histplot(data=df2,x='comp_score',kde=True)","df3b8cfc":"plt.figure(figsize=(11,6))\nsns.histplot(data=df2,x='pos_score',kde=True)","983c5eb1":"plt.figure(figsize=(11,6))\nsns.histplot(data=df2,x='neg_score',kde=True)","bbc5f947":"plt.figure(figsize=(11,6))\nsns.histplot(data=df2,x='neu_score',kde=True)","4672125c":"plt.figure(figsize=(11,9))\nsns.histplot(data=df2[df2['Sentiment']=='Neutral'],x='comp_score',kde=True)","f7a97b9e":"plt.figure(figsize=(11,9))\nsns.histplot(data=df2[df2['Sentiment']=='Positive'],x='comp_score',kde=True)","cec2b8cb":"plt.figure(figsize=(11,9))\nsns.histplot(data=df2[df2['Sentiment']=='Extremely Positive'],x='comp_score',kde=True)","3ce1eec5":"df2[(df2['Sentiment']=='Extremely Positive') & (df2['comp_score'] < -0.75)]","7b73083e":"plt.figure(figsize=(11,9))\nsns.histplot(data=df2[df2['Sentiment']=='Negative'],x='comp_score',kde=True)","c12d243a":"plt.figure(figsize=(11,9))\nsns.histplot(data=df2[df2['Sentiment']=='Extremely Negative'],x='comp_score',kde=True)","c4b27a76":"df2[(df2['Sentiment']=='Extremely Negative') & (df2['comp_score'] > 0.75)]","08cdbf20":"X = df_tweets['OriginalTweet']\ny = df_tweets['Sentiment']","58d3df67":"from sklearn.model_selection import train_test_split","61ac9ab2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","3d8742a7":"from sklearn.pipeline import Pipeline","83ce724c":"from sklearn.feature_extraction.text import TfidfVectorizer","c54b25c8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB","fd72efc8":"p1 = Pipeline([('tfidf',TfidfVectorizer()), ('lr',LogisticRegression(max_iter=1000000,solver='saga'))]) # Logistic regression pipeline\np2 =  Pipeline([('tfidf',TfidfVectorizer()), ('lsvc',LinearSVC(random_state=42))]) # Linear SVC regression pipeline\np3 = Pipeline([('tfidf',TfidfVectorizer()), ('svc',SVC(random_state=42))]) # SVC regression pipeline\np4 =  Pipeline([('tfidf',TfidfVectorizer()), ('mnb',MultinomialNB())]) # Multinomial Naive bayes regression pipeline","417d9cda":"p1.fit(X_train,y_train)","ebca3c76":"lr_pred = p1.predict(X_test)","5255e367":"from sklearn.metrics import classification_report,confusion_matrix","cbace4d0":"print(classification_report(y_test,lr_pred))\nprint(confusion_matrix(y_test,lr_pred))","97208aaf":"p2.fit(X_train,y_train)\nlsvc_pred = p2.predict(X_test)\nprint(classification_report(y_test,lsvc_pred))\nprint(confusion_matrix(y_test,lsvc_pred))","44b2b8b2":"p4.fit(X_train,y_train)\nmnb_pred = p4.predict(X_test)\nprint(classification_report(y_test,mnb_pred))\nprint(confusion_matrix(y_test,mnb_pred))","e617cc80":"from sklearn.ensemble import RandomForestClassifier","8a270521":"p5 = Pipeline([('tfidf',TfidfVectorizer()), ('rf',RandomForestClassifier(n_estimators=100,random_state=42))])","4ae48114":"p5.fit(X_train,y_train)\nrf_pred = p5.predict(X_test)\nprint(classification_report(y_test,rf_pred))\nprint(confusion_matrix(y_test,rf_pred))","1e3684c2":"from xgboost import XGBClassifier","6d38de7a":"p6 = Pipeline([('tfidf',TfidfVectorizer()), (\"xgb\",XGBClassifier(random_state=42,booster='dart'))])","750ab2d0":"p6.fit(X_train,y_train)\nxgb_pred = p6.predict(X_test)\nprint(classification_report(y_test,xgb_pred))\nprint(confusion_matrix(y_test,xgb_pred))","0bf96b77":"from xgboost import XGBRFClassifier","cd0d635c":"p7 = Pipeline([('tfidf',TfidfVectorizer()), (\"xgbrf\",XGBRFClassifier(random_state=42,booster='dart'))])\np7.fit(X_train,y_train)\nxgbrf_pred = p7.predict(X_test)\nprint(classification_report(y_test,xgbrf_pred))\nprint(confusion_matrix(y_test,xgbrf_pred))","11d61cac":"from catboost import CatBoostClassifier","6c90d542":"tfidf = TfidfVectorizer()","68bf2e9f":"X_train_vect = tfidf.fit_transform(X_train)\nX_test_vect = tfidf.transform(X_test)","92124be1":"p8 = Pipeline([('tfidf',TfidfVectorizer()), (\"cb\",CatBoostClassifier(random_state=42))])\np8.fit(X_train,y_train)\ncb_pred = p8.predict(X_test)\nprint(classification_report(y_test,cb_pred))\nprint(confusion_matrix(y_test,cb_pred))","75dcef90":"df_test = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv')","6f9528ed":"df_test.head()","2a47405e":"100 * df_test['Sentiment'].value_counts()\/len(df_test)","947d8f87":"X_final = df_test['OriginalTweet']\ny_final = df_test['Sentiment']","6b2042f0":"cb_test_pred = p8.predict(X_final)","1998a8e5":"print(classification_report(y_final,cb_test_pred))\nprint(confusion_matrix(y_final,cb_test_pred))","4e877812":"#### Let's try to analyze these tweets with respect to their sentiment polarity scores (positve score, negative score, neutral score and compound score)","8e35cbf0":"#### 6) XGBOOST","658148c7":"#### Splitting the data into train and test set","1ca5aab8":"## Catboost gave us an accuracy of around 57% on the actual test data as well","eaa040e4":"#### Inference:\nFor positve tweets, we can see a huge peak in comp_score from around 0.5 and the peak diminishes around 0.75. This could be due to the fact that tweets having comp_score of more than 0.5 are classified as \"Highly Positive\" and not just \"Positive\". However, we could see quite a few tweets which have comp_score of more than 0.75 being classified as only \"Positive\" and not \"Highly Positive\", this could be due to the fact that those tweets might be having a very high neu_score or neg_score.","364c98f2":"#### Note:\nWe can notice that the graphs of \"Highly Positive\" and \"Highly Negative\" graphs are almost mirror image of each other, indicating an almost symmetric distribution around the \"Extreme\" values. This was also noticed earlier when we plotted \"neu_score\" ","6934155d":"#### 7) XGBRFBOOST","4bdc0825":"#### After uploading the notebook it was found that SVC model took a lot of time to get trained. Hence, SVC not good for this case. However, code is given below\n##### p3.fit(X_train,y_train)\n##### svc_pred = p3.predict(X_test)\n##### print(classification_report(y_test,svc_pred))\n##### print(confusion_matrix(y_test,svc_pred))","f3a40fa3":"#### Extremely Negative","31d175c1":"#### 8) Catboost","9a642043":"# EDA","936bb2e8":"#### Now, Let's focus on the comp_score of each sentiment","be0d29dd":"## Implementing ML algos","3fcd30c3":"#### Inference:\nIn case of \"Highly Positive\" tweets, we can see a huge peak jump around 0.75 which diminshes at 1 (because 1 is the max score). This peak takes place right off from the point where the peak of \"Positive\" tweets end, proving our assumption (comp_score greater than 0.75 being classified as \"Highly Positive\" tweets) to be correct. We can even spot a few outliers having the comp_score of around -0.75, which could be due to the fact those tweets having high neg_score and neu_score than the pos_score. It could also be the case such tweets are misclassified (possibly due to some error in the initial classifying system)","4c9f6773":"#### Inference:\nWe can see that the peak of this graph is wobbly and is on the negative side. There is no \"one peak\", however, we can say that the region of peak starts from around -0.25 and ends a little bit before -0.75. This could be due to the fact that the statements having comp_score lower than -0.75 are classified as \"Highly Negative\" instead of just \"Negative\"","040640f8":"#### 3) SVC","fca6962a":"#### Inference:\nThe distribution is of the form highly left skewed normal distribution. We can see \"humped peak\" around a neu_score of 0.85. High number of \"absolutely neutral\" tweets are present (around 7000), indicated by a single bar.","31bc4c9a":"#### Inference:\nFor \"Neutral\" tweets, we can see a peak around comp_score of 0, with a symmetric tails. The distribution is normal and symmetric with long tails. However, the tail on the positve side is slightly longer than the one on the left (due to higher number of positve tweets)","30032733":"#### Extremely Positive","e069cb2e":"#### 4) Multinomial NB","cda56dcb":"#### Neutral","a0bf2264":"#### Inference:\nIn case of \"Highly Negative\" tweets, we can see a huge peak jump around -0.75 which diminshes at 1 (because -1 is the min score). This peak takes place right off from the point where the peak of \"Negative\" tweets end, proving our assumption (comp_score less than -0.75 being classified as \"Highly Negative\" tweets) to be correct. We can even spot a few outliers having the comp_score of around +0.75, which could be due to the fact those tweets having high pos_score and neu_score than the pos_score. It could also be the case such tweets are misclassified (possibly due to some error in the initial classifying system)","f4acc139":"#### 5) Random Forest","a8334a0b":"#### 1) Logistic Regression","0e8c8ffb":"The above 4 graphs show that except comp_score all the scores have a high skewed distribution followed\/preceeded by a single long bar (indicating all those scores which are not present in that particular score category)","1e1cce45":"#### Inference:\nThe negative score of tweets seems to distributed in a highly right skewed normal distribution manner, with a slight peak occuring around 0.08 - 0.12. However, we can see that a very high number of tweets (around 17500) have a neg_score of 0 indicating that they are either highly positive, positive or neutral. All the non-negative number of tweets are summed in that one single bar.","d41fc105":"#### Inference:\nThe positive score of tweets seems to distributed in a highly right skewed normal distribution manner, with a slight peak occuring around 0.06 - 0.08. However, we can see that a very high number of tweets (around 16000) have a pos_score of 0 indicating that they are either highly negative, negative or neutral. All the non-positive number of tweets are summed in that one single bar.","b93f8f9d":"#### Negative","f0fd6fc6":"#### Note: We won't be doing EDA on this actual test data as in practical scenario it is not possible to do any EDA on the incoming test data ","eecacc64":"#### Inference:\nWe can notice almost a perfect normal distribution with wide tails. The plot is almost symmetric, with a slightly highert height around the positive region which could be due to the higher number of positive and extremely positive number of tweets.","8cc44038":"#### Creating pipelines (including Tfidf vectorization and then using classification algorithm)","085abf33":"#### Positive","3e0e733e":"#### 2) Linear SVC","825a7ed3":"### Catboost is giving us the best accuracy... let's try it on the actual test set","75bd03c1":"#### Inference:\nWe can see that most of the samples (tweets) have \"positive\" sentiment. Next, are the tweets having \"Negative\" sentiments, after that comes tweets with \"Neutral\" sentiments. Then, we can see the \"extreme cases\" lead by \"Extremely Positive\" sentiments and followed by \"Extremely Negative\" sentiments."}}