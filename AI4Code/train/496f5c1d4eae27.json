{"cell_type":{"e8c28cae":"code","fbe6f099":"code","8e5ee85d":"code","8eed4b70":"code","13b5ccf5":"code","c43e802d":"code","67784c6c":"code","7b344d13":"code","b59e2870":"code","fb6f7bda":"code","737efb09":"code","22f8d948":"code","bda2365c":"code","fc3f94cb":"code","1141fe24":"code","ae5b00da":"code","4e574b92":"code","c598edda":"code","94726c7f":"code","f4f5949a":"code","8101a5d6":"code","f0659448":"code","3cbf9a5d":"code","a3c9e41b":"code","1a93b6a6":"code","d8b8efc8":"code","2f19abcf":"code","97b05947":"code","c0f16077":"code","168552ed":"code","006d21f3":"code","29f876ce":"code","b9c568db":"code","ccd48a0f":"code","743e3530":"code","27211b29":"code","08cc3f9e":"code","cd3a2fbb":"markdown","77f6065f":"markdown","860a7680":"markdown","0c8263ba":"markdown","69acc34d":"markdown","1a9df1e1":"markdown","10f532b7":"markdown","3face4ff":"markdown","3e73547c":"markdown","4490bfce":"markdown","23b7ebf7":"markdown","c325338e":"markdown","23085ebd":"markdown"},"source":{"e8c28cae":"# %matplotlib notebook\n\nimport gc\n\n# Linear Algebra\nimport numpy as np\n\n# Data Processing\nimport pandas as pd\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Stats\nfrom scipy import stats\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, classification_report, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Classifiers\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Stop unnecessary Seaborn warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set()  # Stylises graphs","fbe6f099":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","8e5ee85d":"train_identity = pd.read_csv(f'..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction = pd.read_csv(f'..\/input\/ieee-fraud-detection\/train_transaction.csv')\n# sub = pd.read_csv(f'..\/input\/ieee-fraud-detection\/sample_submission.csv')\n\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')","8eed4b70":"test_identity = pd.read_csv(f'..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction = pd.read_csv(f'..\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","13b5ccf5":"test = reduce_mem_usage(test)","c43e802d":"train.head()","67784c6c":"train.info()","7b344d13":"# test.head()","b59e2870":"# test.info()","fb6f7bda":"fraud = train[train['isFraud'] == 1]","737efb09":"down_train = train[train['isFraud'] == 0].sample(\n    n=int(len(train) \/ 5),\n    random_state=0\n)","22f8d948":"train = down_train.append(fraud)","bda2365c":"train = reduce_mem_usage(train)","fc3f94cb":"train.info()","1141fe24":"qual_cols = (\n    ['ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] +\n    [f'card{n}' for n in range(1, 7)] +\n    [f'M{n}' for n in range(1, 10)] +\n    ['DeviceType' ,'DeviceInfo'] +\n    [f'id_{n}' for n in range(12, 39)]\n)\nprint(f'Qualitative Variables: {qual_cols}')","ae5b00da":"# missing_vals = pd.DataFrame(train[train.columns].isnull().sum() * 100 \/ train.shape[0])\n# missing_vals[missing_vals[0] > 80]","4e574b92":"# train = train.drop(missing_vals[missing_vals[0] > 80].index, axis=1)","c598edda":"print(f'Duplicate Rows: {train.duplicated().sum()}')","94726c7f":"test.columns = [i.replace('-', '_') if 'id' in i else i for i in test.columns]","f4f5949a":"int_cols = (\n    train.loc[:, train.dtypes == np.int8] +\n    train.loc[:, train.dtypes == np.int16] +\n    train.loc[:, train.dtypes == np.int32] +\n    train.loc[:, train.dtypes == np.int32]\n)\nint_cols = int_cols.columns","8101a5d6":"numeric_cols_train = (\n     train.drop(list(qual_cols) + list(int_cols) + ['isFraud'], axis=1).columns\n )\n\nnumeric_cols_test = (\n     test.drop(list(qual_cols) + list(set(int_cols) - {'isFraud'}), axis=1).columns\n )\n\n\nscaler = StandardScaler()\ntrain_numeric_norm = scaler.fit_transform(train[numeric_cols_train])\ntest_numeric_norm = scaler.fit_transform(test[numeric_cols_test])","f0659448":"test_numeric_norm = scaler.fit_transform(test[numeric_cols_train])\ntrain_numeric_norm = scaler.fit_transform(train[numeric_cols_test])","3cbf9a5d":"for i, col in enumerate(numeric_cols_train):\n    train[f'{col}_n'] = train_numeric_norm[:, i]\n    del train[col]\n\nfor i, col in enumerate(numeric_cols_train):\n    test[f'{col}_n'] = test_numeric_norm[:, i]\n    del test[col]","a3c9e41b":"qual_objects = train.select_dtypes(include=['object']).columns\nqual_objects_test = set(train.select_dtypes(include=['object']).columns) - set('isFraud')\nfor col in qual_objects:\n    train[col] = train[col].replace(np.nan, 'nan', regex=True)\n\nfor col in qual_objects_test:\n    test[col] = test[col].replace(np.nan, 'nan', regex=True)","1a93b6a6":"le = LabelEncoder()\n\nfor col in qual_objects:\n    train[col] = le.fit_transform(train[col].astype(str))\n    test[col] = le.fit_transform(test[col].astype(str))","d8b8efc8":"for col in [f'{i}_n' for i in numeric_cols_train]:\n    train[col] = train[col].replace(np.nan, 0, regex=True)\nfor col in [f'{i}_n' for i in numeric_cols_test]:\n    test[col] = test[col].replace(np.nan, 0, regex=True)","2f19abcf":"for col in int_cols:\n    train[col] = train[col].replace(np.nan, -99, regex=True)\n\nfor col in list(set(int_cols) - {'isFraud'}):\n    test[col] = test[col].replace(np.nan, -99, regex=True)","97b05947":"for col in set(qual_cols) - set(qual_objects):\n    train[col] = train[col].replace(np.nan, -99, regex=True)\n    test[col] = test[col].replace(np.nan, -99, regex=True)","c0f16077":"X = train\ny = X['isFraud']\nX = X.drop('isFraud', axis=1)","168552ed":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, train_size=0.8, random_state=0\n)\n\ndel X\ndel y\n\nprint(f'X Train Shape: {X_train.shape}')\nprint(f'X Validation Shape: {X_valid.shape}')\nprint(f'y Train Shape: {y_train.shape}')\nprint(f'y Validation Shape: {y_valid.shape}')","006d21f3":"clf = RandomForestClassifier(random_state=0)\n\nclf.fit(X_train, y_train)\nresults_valid = clf.predict(X_valid)\nresults = clf.predict(test)","29f876ce":"feature_importances = pd.DataFrame(clf.feature_importances_,\n                                   index = train.drop('isFraud', axis=1).columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)\nfeature_importances = feature_importances.reset_index()\nfeature_importances.head(10)","b9c568db":"plt.figure(figsize=(13, 7))\nsns.barplot(\n    x=\"importance\", y='index',\n    data=feature_importances[0:10], label=\"Total\"\n)\nplt.title(\"Random Forest Variable Importance\")\nplt.ylabel(\"Variable\")\nplt.xlabel(\"Importance\")\nplt.show()","ccd48a0f":"print(classification_report(y_valid, results_valid))","743e3530":"results_df = pd.DataFrame()\nresults_df['TransactionID'] = test['TransactionID']\nresults_df['isFraud'] = results","27211b29":"results_df.to_csv('submission.csv', index=False)","08cc3f9e":"rf_auc = roc_auc_score(y_valid, results_valid)\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_valid, clf.predict_proba(X_valid)[:,1])\nplt.figure(figsize=(10, 10))\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label=f'Random Forest AUC: {round(rf_auc, 3)}')\n\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate')\n\nplt.xlim([-0.005, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","cd3a2fbb":"# Helper Functions","77f6065f":"# Categorical Variables","860a7680":"# Label Encoding","0c8263ba":"# Slitting Data","69acc34d":"# Standarizing","1a9df1e1":"# Importing the Data","10f532b7":"## Train Data","3face4ff":"# Missing Values","3e73547c":"# Imports","4490bfce":"# Building Models","23b7ebf7":"## Testing Data","c325338e":"Drop 80%, come back later if we need to.","23085ebd":"# Duplicate Rows"}}