{"cell_type":{"f2bd0851":"code","745e21c7":"code","aa0011b1":"code","cf76f531":"code","de1a2f7e":"code","c4d5f657":"code","ab794058":"code","3f587eb9":"code","df1e02f4":"code","df74ffa3":"code","bb680b4d":"code","1c105902":"code","549485f4":"code","2e73958e":"code","4f6ccee2":"code","334d15a6":"code","e842467c":"code","e7dd0d5f":"code","47782fa7":"code","661ef170":"code","1de7e514":"code","39d41ae4":"code","adf6c32d":"code","131de867":"code","66923749":"code","c6457bd0":"code","6e902dce":"code","f1e03f2a":"code","e7b3163f":"code","ed78be84":"code","9dce7d0b":"code","df4ae9a9":"code","54c30322":"code","4ea815db":"code","9ac1676e":"code","d1d40928":"code","af4fc45b":"code","a3e867c2":"code","a2221f60":"code","9dff4797":"code","b00383e8":"code","5aa9c801":"code","4da553fe":"code","ace36ff3":"code","2f53683d":"code","badccd69":"code","f60db95c":"code","449819a6":"code","fa3ad208":"code","d15c5f6c":"code","dd7152d4":"code","b2927970":"code","f17fae60":"code","059d17ba":"code","cddcded3":"markdown","57725698":"markdown","b0bc2686":"markdown","e6c8ed7c":"markdown","e068d321":"markdown","7630ed51":"markdown","31920346":"markdown","373da5e7":"markdown","bd373dc5":"markdown","d55fb6f9":"markdown","526a4ddc":"markdown","8ddf3165":"markdown","385a2a26":"markdown","b0fee08a":"markdown","3d9cf719":"markdown","4c0d6a31":"markdown","634634c2":"markdown","07827739":"markdown","454d6811":"markdown","a01b2029":"markdown","cc65fc35":"markdown","5d0ce3e9":"markdown","c420b0b1":"markdown","41f8b0a0":"markdown","7be24b26":"markdown","2ab2482b":"markdown","40d7c9a6":"markdown","40dbbadb":"markdown","0fcc57ea":"markdown","3b939a1d":"markdown","2586b479":"markdown","9400d6bb":"markdown","e80c5d0d":"markdown","bd829589":"markdown","6d24900a":"markdown","18025272":"markdown","d90865bd":"markdown","4363cb61":"markdown","c0f3b70f":"markdown","9c8d2642":"markdown","47e3a244":"markdown","599e7981":"markdown","32da4ee3":"markdown","0f44aaaf":"markdown","c4983bf2":"markdown","070b14aa":"markdown","3231755f":"markdown","db667bd0":"markdown","eb225e86":"markdown","c2dd6314":"markdown","4fbbc272":"markdown","3707534a":"markdown","24ad5a9e":"markdown","a4dfc7db":"markdown","fb16bc29":"markdown","7603c4b0":"markdown","697bc772":"markdown","a84a9dc5":"markdown","97f0be4d":"markdown","5d51179f":"markdown","456aece1":"markdown","686f77bd":"markdown","e8b01ea5":"markdown","d52dbbf5":"markdown","eb8b1c2f":"markdown","1d7df644":"markdown","a139122c":"markdown","6ee20d25":"markdown","c75b9a40":"markdown","bb6505f6":"markdown","2ae8acae":"markdown"},"source":{"f2bd0851":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score,GridSearchCV, validation_curve\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler,MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support,f1_score\nimport time\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import mode\nfrom mlxtend.classifier import EnsembleVoteClassifier\nimport warnings\nimport copy\n\n%matplotlib inline\nfrom tabulate import tabulate\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","745e21c7":"#Load data in a pandas DataFrame\n \nsdss_df = pd.read_csv('\/kaggle\/input\/sloan-digital-sky-survey-dr16-70k\/sdss-IV-dr16-70k.csv', skiprows=0)\n#first look at the database\nsdss_df.head()\n ","aa0011b1":"sdss_df.describe()","cf76f531":"q_u_low = sdss_df[\"psfMag_u\"].quantile(0.01)\nq_u_hi  = sdss_df[\"psfMag_u\"].quantile(0.99)\nq_g_low = sdss_df[\"psfMag_g\"].quantile(0.01)\nq_g_hi  = sdss_df[\"psfMag_g\"].quantile(0.99)\nq_r_low = sdss_df[\"psfMag_r\"].quantile(0.01)\nq_r_hi  = sdss_df[\"psfMag_r\"].quantile(0.99)\nq_i_low = sdss_df[\"psfMag_i\"].quantile(0.01)\nq_i_hi  = sdss_df[\"psfMag_i\"].quantile(0.99)\nq_z_low = sdss_df[\"psfMag_z\"].quantile(0.01)\nq_z_hi  = sdss_df[\"psfMag_z\"].quantile(0.99)\nsdss_df_filtered = sdss_df[(sdss_df[\"psfMag_u\"] < q_u_hi) & (sdss_df[\"psfMag_u\"] > q_u_low)& \n                 (sdss_df[\"psfMag_g\"] < q_g_hi) & (sdss_df[\"psfMag_g\"] > q_g_low)&\n                 (sdss_df[\"psfMag_r\"] < q_r_hi) & (sdss_df[\"psfMag_r\"] > q_r_low)& \n                 (sdss_df[\"psfMag_i\"] < q_i_hi) & (sdss_df[\"psfMag_i\"] > q_i_low)& \n                 (sdss_df[\"psfMag_z\"] < q_z_hi) & (sdss_df[\"psfMag_z\"] > q_z_low)]","de1a2f7e":"sdss_df_filtered.info()","c4d5f657":"sdss_df_filtered[\"class\"].unique()\n","ab794058":"sdss_df_filtered = sdss_df_filtered[[ 'objid', 'ra', 'dec', 'psfMag_u','psfMag_g','psfMag_r','psfMag_i','psfMag_z','run','run','rerun','camcol',\"field\",\"redshift\",\"plate\",\"mjd\",\"fiberid\",\"class\"]]","3f587eb9":"\ntarget_count = sdss_df_filtered[\"class\"].value_counts()\nprint(target_count)\n\nprint('\\nProportion galaxies\/stars:', round(target_count[0] \/ target_count[1], 2), ': 1')\nprint('Proportion galaxies\/quasars:', round(target_count[0] \/ target_count[2], 2), ': 1')\nprint('Proportion stars\/quasars:', round(target_count[1] \/ target_count[2], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","df1e02f4":"sdss_df_filtered.columns.values","df74ffa3":"sdss_df_filtered.drop([\"objid\",\"run\",\"rerun\",\"camcol\",\"field\"], axis=1, inplace=True)\nsdss_df_filtered.head()","bb680b4d":"g = sns.PairGrid(sdss_df_filtered,x_vars=['psfMag_u', 'psfMag_g','psfMag_i','psfMag_r','psfMag_z'],\n                 y_vars=[\"redshift\"],hue='class')\ng.map(sns.scatterplot)\ng.add_legend()\n\n ","1c105902":"sns.boxplot(data=sdss_df_filtered, y='redshift', x='class')\nsdss_df_filtered[['redshift','class']].groupby(['class'],as_index=False).mean().sort_values(by='class',ascending=False)","549485f4":"\nX,y= sdss_df_filtered.drop([\"class\"],axis=1),sdss_df_filtered[[\"class\"]]\ny= y.reset_index(drop= True)\nX= X.reset_index(drop= True)\nclass_le = LabelEncoder()\ny['class'] = class_le.fit_transform(y['class'])\n#retrieve LabelEncoder() map rules\nlist(class_le.inverse_transform([0,1,2]))","2e73958e":"class_le = LabelEncoder()\ny['class'] = class_le.fit_transform(y['class'])\n#retrieve LabelEncoder() map rules\nlist(class_le.inverse_transform([0,1,2]))","4f6ccee2":"rus = RandomUnderSampler(random_state =42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\ny_resampled.value_counts()","334d15a6":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=42)\ny_train= y_train.values.ravel()\ny_test= y_test.values.ravel()","e842467c":"stdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\n","e7dd0d5f":"lg = LogisticRegression(C= 100, multi_class=\"ovr\",random_state=1 )\ntraining_start = time.perf_counter()\nlg.fit(X_train_std, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = lg.predict(X_test_std)\nprediction_end = time.perf_counter()\nacc_lg = lg.score(X_test_std,y_test)*100\nlg_train_time = training_end-training_start\nlg_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's  Logistic Regression Classifier's prediction accuracy is: %3.2f\" % (acc_lg))\nprint(\"Time consumed for training: %4.3f seconds\" % (lg_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (lg_prediction_time))","47782fa7":"knn = KNeighborsClassifier(n_neighbors=7)\ntraining_start = time.perf_counter()\nknn.fit(X_train_std, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = knn.predict(X_test_std)\nprediction_end = time.perf_counter()\nacc_knn = knn.score(X_test_std,y_test)*100\nknn_train_time = training_end-training_start\nknn_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knn))\nprint(\"Time consumed for training: %4.3f seconds\" % (knn_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (knn_prediction_time))","661ef170":"rfc = RandomForestClassifier(n_estimators=13,random_state=1)\ntraining_start = time.perf_counter()\nrfc.fit(X_train_std, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = rfc.predict(X_test_std)\nprediction_end = time.perf_counter()\nacc_rfc = rfc.score(X_test_std,y_test)*100\nrfc_train_time = training_end-training_start\nrfc_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's Random Forest Classifier's prediction accuracy is: %3.3f\" % (acc_rfc))\nprint(\"Time consumed for training: %4.3f seconds\" % (rfc_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (rfc_prediction_time))","1de7e514":"svc = SVC(kernel=\"linear\",C= 1.0,decision_function_shape= \"ovr\",random_state=1)\ntraining_start = time.perf_counter()\nsvc.fit(X_train_std, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = svc.predict(X_test_std)\nprediction_end = time.perf_counter()\nacc_svc = svc.score(X_test_std,y_test)*100\nsvc_train_time = training_end-training_start\nsvc_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svc))\nprint(\"Time consumed for training: %4.3f seconds\" % (svc_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (svc_prediction_time))","39d41ae4":"svc2 = SVC(gamma= 0.80,kernel =\"rbf\",decision_function_shape= \"ovr\",random_state=1)\ntraining_start = time.perf_counter()\nsvc2.fit(X_train_std, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = svc2.predict(X_test_std)\nprediction_end = time.perf_counter()\nacc_svc2 = svc2.score(X_test_std,y_test)*100\nsvc2_train_time = training_end-training_start\nsvc2_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svc2))\nprint(\"Time consumed for training: %4.3f seconds\" % (svc2_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (svc2_prediction_time))","adf6c32d":"cov_mat = np.cov(X_train_std.T)\neigen_vals,eigen_vecs= np.linalg.eig(cov_mat)\ntot = sum(eigen_vals)\nvar_exp= [(i\/tot) for i in sorted (eigen_vals,reverse= True)]\ncum_var_exp= np.cumsum(var_exp)\nplt.bar(range(1,len(eigen_vals)+1),var_exp ,alpha= 0.5,align = \"center\",label= \" Individual explained variance\")\nplt.step(range(1,len(eigen_vals)+1),cum_var_exp ,where= \"mid\",label= \"Individual explained variance\")\nplt.xlabel(\"principal component index\")\nplt.ylabel(\"Explained variance ratio\")\nplt.legend(loc= \"best\")\nplt.tight_layout()\nplt.show()\n ","131de867":"pca = PCA(n_components=7)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca= pca.transform(X_test_std)","66923749":"lr2= LogisticRegression(C=100.0,multi_class=\"ovr\",random_state=1)\ntraining_start =time.perf_counter()\nlr2.fit(X_train_pca,y_train)\ntraining_end =time.perf_counter()\nprediction_start =time.perf_counter()\npreds = lr2.predict(X_test_pca)\nprediction_end = time.perf_counter()\nacc_lr2 = lr2.score(X_test_pca,y_test)*100\nlr2_train_time = training_end-training_start\nlr2_prediction_time = prediction_end-prediction_start\nprint(\"Scikit-Learn's  Logistic Regression Classifier's prediction accuracy is: %3.2f\" % (acc_lr2))\nprint(\"Time consumed for training: %4.3f seconds\" % (lr2_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (lr2_prediction_time))\n\n","c6457bd0":"print(f\" the original size of the training set was :{round(X_train_std.nbytes\/1024,2)} kb\")\n\nprint(f\" the size of the reduced set is:{round(X_train_pca.nbytes\/1024,2)} kb\")\n\nprint(f\" a reduction of {round((X_train_std.nbytes-X_train_pca.nbytes)\/1024,2)} kb\")","6e902dce":"test= np.c_[ X_train_pca, y_train ]\ndf_test= pd.DataFrame(test)\n\ndf_test.columns = ['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5', 'PCA_6', 'PCA_7', 'class' ]\n \n\nf = sns.PairGrid(df_test,x_vars=['PCA_2', 'PCA_3','PCA_4','PCA_5','PCA_6','PCA_7'],\n                 y_vars =[\"PCA_1\"],hue = \"class\")\nf.map(sns.scatterplot)\nf.add_legend()","f1e03f2a":"pipe_LR= make_pipeline(RobustScaler(),PCA(n_components=7),LogisticRegression())\npipe_LR.fit(X_train,y_train)\nprint(f\"test accuracy {100* pipe_LR.score(X_test,y_test):2.3f}\")","e7b3163f":"pipe_KNN= make_pipeline(RobustScaler(),PCA(n_components=7),KNeighborsClassifier(n_neighbors=7))\npipe_KNN.fit(X_train,y_train)\nprint(f\"test accuracy {100* pipe_KNN.score(X_test,y_test):2.3f}\")","ed78be84":"pipe_RF= make_pipeline(StandardScaler(),PCA(n_components=7),\n                       RandomForestClassifier(n_estimators=17,n_jobs=2) )\npipe_RF.fit(X_train,y_train)\nprint(f\"test accuracy {100* pipe_RF.score(X_test,y_test):.3f}\")\n","9dce7d0b":"pipe_RF2= make_pipeline(StandardScaler(),\n                       RandomForestClassifier(n_estimators=17) )\npipe_RF2.fit(X_train,y_train)\nprint(f\"test accuracy {100* pipe_RF2.score(X_test,y_test):.3f}\")","df4ae9a9":"pipe_SVM= make_pipeline(StandardScaler(),PCA(n_components=7),SVC(decision_function_shape= \"ovr\",probability= True))\npipe_SVM.fit(X_train,y_train)\nprint(f\"test accuracy {100* pipe_SVM.score(X_test,y_test):.3f}\")\n\n\n ","54c30322":"scores= cross_val_score(estimator= pipe_KNN,X=X_train,y= y_train,cv=10,n_jobs=-1)\nprint(f\"Accuracy scores :  {scores}\")\nprint(f\"CV accuracy: {np.mean(scores):.2f} +\/- {np.std(scores):.2f} \")","4ea815db":"param_range = [0.01,1,10.0,100.0,1000]\n\ntrain_scores, test_scores = validation_curve(estimator= pipe_SVM, X=X_train,y= y_train,param_name=\"svc__C\",param_range=param_range,cv=8)\ntrain_mean= np.mean(train_scores,axis=1)\ntrain_std= np.std(train_scores,axis=1)\ntest_mean= np.mean(test_scores,axis=1)\ntest_std= np.std(test_scores,axis=1)","9ac1676e":"plt.plot(param_range,train_mean,color=\"blue\",marker=\"o\",markersize=5,label= \"training accuracy\")\nplt.fill_between(param_range,train_mean + train_std,train_mean - train_std,alpha=0.15,color=\"blue\")\nplt.plot(param_range,test_mean,color=\"green\",marker=\"s\",linestyle=\"--\",markersize=5,label= \"testing accuracy\")\nplt.fill_between(param_range,test_mean + test_std,test_mean - test_std,alpha=0.15,color=\"green\")\nplt.grid()\nplt.xscale(\"log\")\nplt.legend(loc= \"lower right\")\nplt.xlabel(\"Parameter C\")\nplt.ylabel(\"Accuracy\")\nplt.ylim([0.8,1.0])\nplt.grid(True)\nplt.show()","d1d40928":"pipe_SVM.get_params()","af4fc45b":" \nparam_range_SVM = [100,130,170,300]\nparam_grid_SVM= [{\"svc__C\":param_range_SVM,\"svc__kernel\":[\"linear\"],\"svc__random_state\":[1]},\n                     {\"svc__C\":param_range_SVM,\"svc__kernel\":[\"rbf\"],\"svc__gamma\":[0.1,1,10,100],\n                          \"svc__random_state\":[1],\"svc__probability\": [True] }]\ngs_SVM = GridSearchCV(estimator= pipe_SVM, param_grid= param_grid_SVM,scoring=\"accuracy\",cv=2,refit=True,n_jobs=-1)\ngs_SVM= gs_SVM.fit(X_train,y_train)\nprint(f\" mean accuracy:{gs_SVM.best_score_ :.3f} +- {np.mean(gs_SVM.cv_results_['std_test_score']):.3f} \")\nprint(gs_SVM.best_params_)","a3e867c2":"\n\nparam_range_RF2 = [11,15,18,21]\nparam_grid_RF2= [{'standardscaler': [StandardScaler(),RobustScaler()],\n                  \"randomforestclassifier__n_estimators\":param_range_RF2,\n                      \"randomforestclassifier__criterion\":[\"gini\",\"entropy\"],\n                          \"randomforestclassifier__max_features\":[\"auto\", \"sqrt\", \"log2\"],\"randomforestclassifier__random_state\":[1]}]\ngs_RF = GridSearchCV(estimator= pipe_RF2,param_grid= param_grid_RF2,scoring=\"accuracy\",cv=2,refit=True,n_jobs=-1)\ngs_RF= gs_RF.fit(X_train,y_train)\n\n\nprint(f\" mean accuracy:{gs_RF.best_score_ :.4f} +- {np.mean(gs_RF.cv_results_['std_test_score']):.4f} \")\n \n\nprint(gs_RF.best_params_)","a2221f60":"param_range_LR = [ 1000,2000,5000,10000]\nparam_grid_LR= [{'robustscaler': [StandardScaler(),RobustScaler()],\n                 \"logisticregression__C\":param_range_LR,\"logisticregression__penalty\":[\"l2\",\"l1\"],\n                     \"logisticregression__solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"] ,\"logisticregression__random_state\":[1]},]\ngs_LR = GridSearchCV(estimator= pipe_LR,param_grid= param_grid_LR,scoring=\"accuracy\",cv=2,refit=True,n_jobs=-1)\ngs_LR= gs_LR.fit(X_train,y_train)\n\nprint(f\" mean accuracy:{gs_LR.best_score_ :.3f} +- {np.mean(gs_LR.cv_results_['std_test_score']):.3f} \")\n \n\nprint(gs_LR.best_params_)","9dff4797":"param_range_KNN = [2,3,4,5,8]\nparam_grid_KNN= [{'robustscaler': [StandardScaler(),RobustScaler()],\"kneighborsclassifier__n_neighbors\":param_range_KNN,\n                      \"kneighborsclassifier__weights\":[\"uniform\",  \"distance\"],\"kneighborsclassifier__p\":[2,3, 4 ]}]\ngs_KNN = GridSearchCV(estimator= pipe_KNN,param_grid= param_grid_KNN,scoring=\"accuracy\",cv=2,refit=True,n_jobs=-1)\ngs_KNN= gs_KNN.fit(X_train,y_train)\n\nprint(f\" mean accuracy:{gs_KNN.best_score_ :.3f} +- {np.mean(gs_KNN.cv_results_['std_test_score']):.3f} \")\nprint(gs_KNN.best_params_)","b00383e8":"tree= DecisionTreeClassifier(criterion=\"entropy\",random_state=1, max_depth=1)\npipe_ADA= make_pipeline(StandardScaler(),AdaBoostClassifier(base_estimator=tree,n_estimators=130,learning_rate=0.1,random_state=1))\npipe_ADA.fit(X_train,y_train)\nprint(f\"test accuracy {100* pipe_ADA.score(X_test,y_test):.3f}\")\n\n\n\nparam_ADA= [{'standardscaler': [StandardScaler(),RobustScaler()],\"adaboostclassifier__base_estimator__criterion\":[\"entropy\"],\n             \"adaboostclassifier__n_estimators\": [170,180,190],\"adaboostclassifier__base_estimator__splitter\":[\"best\"],\n             \"adaboostclassifier__base_estimator__max_depth\":[6,7,8],\"adaboostclassifier__base_estimator__min_samples_split\":[6,7,8]}]\ngs_ADA = GridSearchCV(estimator= pipe_ADA,param_grid= param_ADA,scoring=\"f1_micro\",cv=2,refit=True,n_jobs=-1)\ngs_ADA= gs_ADA.fit(X_train,y_train)\n\nprint(f\" mean accuracy:{gs_ADA.best_score_ :.3f} +- {np.mean(gs_ADA.cv_results_['std_test_score']):.3f} \")\nprint(gs_ADA.best_params_)","5aa9c801":"\ngs_objects={\"gs_KNN\":gs_KNN,\"gs_RF\":gs_RF,\"gs_LR\":gs_LR,\"gs_SVM\":gs_SVM,\"gs_ADA\":gs_ADA}\nresults=np.zeros((5,5))\nfor num,el in enumerate(gs_objects ):\n\n    scores =  cross_val_score(gs_objects[el],X_train,y_train,scoring=\"accuracy\",cv=5,n_jobs=-1)\n    for index,score in enumerate(scores):\n        results[num][index]= score\n        \n    print(f\" CV accuracy for the {el} classifier is {np.mean(scores): .3f} +\/- {np.std(scores):.3f}\")\n\nheaders = [\"result 1\", \"result 2\", \"result 3\", \"result 4\", \"result 5\"]\n# tabulate data\ntable = tabulate(results, headers, tablefmt=\"plain\",showindex= [\"gs_KNN\",\"gs_RF\",\"gs_LR\",\"gs_SVM\",\"gs_ADA\"])\n# output\nprint(table)\n","4da553fe":"gs_objects={\"gs_RF\":gs_RF}\nresults=np.zeros((1,5))\ny_pred=gs_RF.predict(X)\nfor num,el in enumerate(gs_objects ):\n    score=f1_score(y, y_pred, average='macro')    \n    print(f\" CV F1 for the {el} classifier is {score: .4f} +\/- {score:.4f}\")\n","ace36ff3":"sns.set_style('white')\ny_pred = gs_RF.predict(X)\nconfmat= confusion_matrix(y_true=y, y_pred= y_pred)\nfig,ax= plt.subplots(figsize=(2.5,2.5))\nax.matshow(confmat,cmap=plt.cm.Blues,alpha=0.3)\nfor i in range(confmat.shape[0]):\n    for j in range (confmat.shape[1]):\n        ax.text(x=j,y=i,\n               s= confmat[i,j],\n               va=\"center\",ha=\"center\")\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title (\"RF\")\nplt.show()","2f53683d":"def evaluate_model_performance(y_true, y_pred,heatmap ='True', ret= \"yes\"):\n    if heatmap == \"True\":\n        cm = confusion_matrix(y_true, y_pred)\n        # Visualizing model performance\n        ax= plt.subplot()\n        sns.heatmap(cm, annot=True, ax = ax, fmt='g'); #annot=True to annotate cells\n\n        # labels, title and ticks\n        ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n        ax.set_title('Confusion Matrix'); \n    precision,recall,fscore,support = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n    #print(f\" precisione: {precision},\\n recall: {recall},\\n f1 score: {fscore},\\n support: {support} \\n\")\n    if ret ==\"no\":\n        print(f\" precisione: {precision},\\n recall: {recall},\\n f1 score: {fscore},\\n support: {support} \\n\")\n        return    \n    return precision,recall,fscore,support\n\n    ","badccd69":"def gridsearch_weight(models,a=3,voting= \"hard\"):\n    if len(models)==0:\n        print(\"no models selected\")\n        return 0\n    col_names =[ \"w\"+ str(i) for i in range(1,len(models)+1) ]\n    n = len(models)\n    print(f\"length vector models {n}\")\n    df  = pd.DataFrame(columns=(*col_names,\"precision\",\"recall\",\"fscore\",\"support\"))\n    pva = tuple([a]*n)\n    weights =   np.ndindex( pva)\n    weights =list(weights)[1:]\n    index = np.arange((a**n)-1)\n    def  inner_func (models,i, weights,df):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            eclf = EnsembleVoteClassifier(clfs=models, weights=weights[i], fit_base_estimators=False,voting= voting)\n            eclf = eclf.fit(X_train, y_train)\n            a,b,c,d = evaluate_model_performance(y, eclf.predict(X),heatmap= \"False\") \n\n        df.loc[i] = [*weights[i], a,b,c,d]\n    for i in index:\n        inner_func(models, i,weights, df)\n    df.head()\n\n \n \n    return df","f60db95c":"evaluate_model_performance(y, gs_ADA.predict(X),heatmap= \"True\",ret =\"no\") ","449819a6":"\ntrained_clfs = [gs_ADA,gs_KNN, gs_LR,gs_SVM,gs_RF]\n\n\n\ndef majority_engine(trained_clfs, x, y):\n    \n    test_matrix = np.empty((x.shape[0], len(trained_clfs)))\n    for (n, clf) in enumerate(trained_clfs):\n        uff=clf.predict_proba(x) \n        test_matrix[:,n]= copy.deepcopy(np.argmax(uff, axis=1))\n    return mode( test_matrix, axis=1)[0]","fa3ad208":"\nlr_ens_val_pred = majority_engine(trained_clfs, X,y)\nevaluate_model_performance(y, lr_ens_val_pred,ret = \"no\") \n ","d15c5f6c":"df_soft=gridsearch_weight(models=trained_clfs,voting= \"soft\")","dd7152d4":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    num_trained = len(trained_clfs)\n    #print(df_soft.loc[df_soft['fscore'].idxmax()][:num_trained ].tolist())\n    \n    eclf = EnsembleVoteClassifier(clfs=trained_clfs, weights=df_soft.loc[df_soft['fscore'].idxmax()][:num_trained ].tolist(), fit_base_estimators=False,voting=\"soft\")\n    eclf = eclf.fit(X_train, y_train)\n    evaluate_model_performance(y, eclf.predict(X),heatmap= \"True\") \n    \n\n","b2927970":"df_hard=gridsearch_weight(models=trained_clfs,voting= \"hard\")","f17fae60":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    num_trained = len(trained_clfs)\n     \n    \n    eclf2 = EnsembleVoteClassifier(clfs=trained_clfs, weights=df_hard.loc[df_hard['fscore'].idxmax()][:num_trained ].tolist(), fit_base_estimators=False,voting=\"hard\")\n    eclf2 = eclf2.fit(X_train, y_train)\n    evaluate_model_performance(y, eclf2.predict(X),heatmap= \"True\") ","059d17ba":"headers =[ \"w\"+ str(i) for i in range(1,len(trained_clfs)+1 )]\nheaders.extend([\"precision\",\"recall\",\"fscore\",\"support\"])\n\n# tabulate data\ntable = tabulate([df_hard.loc[df_hard['fscore'].idxmax()], \n                  df_soft.loc[df_soft['fscore'].idxmax()]], headers, tablefmt=\"plain\",showindex= [\"hard\",\"soft\" ])\n# output\nprint(table)\n\n","cddcded3":"### Pipeline with a SVM classifier\npipeline with a standardscaler(),a pca with 7 components and a  support vector machine estimator with default values for the parameters (kernel= \"rbf\",gamma = \"scale\" and c= 1).","57725698":"### Bagging \n\nBagging, also known as bootstrap aggregating, is  an ensemble learning technique that  share  many traits with the previous method. The main difference is that, while the simple majority\/plurality classifier fits each classifier to the entire training set,in the bagging ensembles each classifier is trained on a randomly extracted subset of the entire training set. The predictions obtained with the classifiers trained on these subsets are then combined  and the final result is taken equal to the mode ,weighted or not, of the prediction set. Bagging was already implemented in our RandomForest classifier in a previous section\n","b0bc2686":"Seems there is a class imbalance that may affect the accuracy of the model,we will deal with it once we have filtered the data and split the dataset into a training set  and an evaluation set. \n","e6c8ed7c":"## K Nearest Neighbors\n\nK-Nearest Neighbor, also known as KNN, is a supervised learning algorithm that can be used for regression as well as classification problems.\nKNN works on the assumption that every data point falling in near to each other  belongs to the same class. In other words it classifies a new data point based on  a similarity score. KNN is also a lazy learner, since it doesn't learn a discriminative function from the training data but memorizes the training dataset instead.\nThe algorithm itself is failry straightforward and can be summarized by the following steps:\n1. Choose the number  *k*  and a distance metric.\n2. Find the k-nearest neighbors of the data record that we want to classify.\n3. Assign the class label by majority vote.\n\nBased on the chosen distance metric, the KNN algorithm finds the *k* examples in the training dataset that are closest(most similar) to the point that we want to classify.The class label of the data point is then determined by a majority vote among its' *k* nearest neighbors.\nThe main advantage of such a memory-based approach is that the classifier immediately adapts as we add new data, the downside though is that the computational complexity grows linearly with the number of examples in the training dataset. Furthermore,as we add new data, the training dataset grow bigger and this may cause storage problems.Another downside is that we do not know the right value of k, hence we are often forced to test the model with different values before being able to pick the right one. For now we will set k= 5.\n\n ","e068d321":"### Evaluation functions definition\nBefore showing how to use ensemble models,we define a new function named evaluate_model_performance that will help us evaluating the results and a function gridsearch_weight which will allow us to optimize the weights parameters in the EnsembleVoteClassifier","7630ed51":"### 2) hard voting\nlet's do the same with with the voting parameter set to \"hard\"","31920346":"As we can see the dataset is now composed by 66266 entries,each one characterized by 17 features. There are no missing values  but there is still 1 feature whose Dtype is \"object\". Let's see how many unique values this feature contains:","373da5e7":"let's now count the number of examples per class:","bd373dc5":"# Data Recovery\nthe dataset we will work on is recovered from the casjobs website through the following SQl query: \n\n ```SELECT TOP 70000\n objid,ra,dec,psfMag_u,psfMag_g,psfMag_r,psfMag_i,psfMag_z, run, rerun, camcol,\n field, class, z as redshift, plate, mjd, fiberid\n FROM  specPhoto \n```\n \n","d55fb6f9":"Let's see how the data are distributed according to the redshift.","526a4ddc":"## Convalidation curves\n\nthe scikit-learn class val_curves allows to plot the testing accuracies((obtained with  astratified k-fold cross convalidation algorithm) as function of the model parameters, for example the inverse regularization parameter C in a SVM model, and provide an easy and immediate way to detect if the model suffer from underfitting or overfitting.For example we can take the SVM Classifier trained before and use the convalidation curves to fine tune it:","8ddf3165":"# Feature Selection\n\nSo far we have limited our action on the dataset. We have dropped features of no interest, encoded a categorical feature and then we applied a normalization algorithm to the features. We have then splitted the data in a training set and a validation set and we have trained some simple machine learning models on our data. However, feature selection is not the only option available to reduce the number of features,another approach is known as feature selection, which can be understood as an approach to data compression with the goal of maintaining most of the relevant information. In practice, feature extraction is not only used to improve storage space or the computational efficiency of the learning algorithm,but can also improve the predictive performance by reducing the curse of dimensionality(although this apply for much greater datasets than the one we have been using).\n Different algorithms have been developed to achieve this goal,among them the most widely known are Principal Component Analysis and Linear Discriminant Analysis. We will focus our attention on the first one.","385a2a26":"as we can see from the plot, the sweet spot for the inverse C parameter lies around 100. However, C is only one of the parameters available, a complete list can be accessed with:","b0fee08a":"### Encoding categorical variables\n\nwe will use the  scikit-learn built-in LabelEncoder() transformer to encode the \"class\" feature","3d9cf719":"### The kernel trick\n\nthe kernel trick is a mathematical trick employed  when we wish to solve nonlinear problems using an SVM by transforming the training data into a higher-dimensional feature space via a mapping function,$\\phi $ ,and train a linear SVM model to classify the data in this new feature space.The greatest problem with this approach is that the construction of the new feature space is computationally very expensive. To avoid the step of calculating the dot product between two transformed points explicitly, a new function,the kernel function, is defined.One of the most widely used kernels is the radial basis function, known also as Gaussian Kernel, which can be easily implemented in scikit-learn by  adding the parameter kernel = \"rbf\" to s SVC classifier. The gamma parameter $\\gamma$ is related to the cut-off of the gaussian Kernel and can be fine tuned to further improve the accuracy of the model. For now let's put it equal to 0.80, we will fine tune it later.\n\nSince our data appear having a globular form on a 2d projection i thought it would have been interesting to see if the kernel trick would improve the overall accuracy","4c0d6a31":"### The regularization parameter  \nWe have passed to the LogisticRegression classifier a value of C=100.0, this is an easy way to improve our model accuracy. The inverse of this parameter, $\\lambda$, is known as the regularization parameter and allows to filter out noise from the data, thus preventing overfitting. Regularization add bias in the data in order to penalize extreme weight values.By increasing the value of $\\lambda$ we push the weights toward 0 and decrease the dependence of our model on the training data. The most common form of regularization is known as L2 regularization","634634c2":"# Model validation and fine-tuning\n\nA fundamental step in building a machine learning model is the process of performance evaluation of our new model on new previously unseen data. Simply put,a model can suffer either by a lack of complexity (**high bias**),thus being unable to capture the complexity of the patterns in the data and correctly predict the class of new elements, or it can overfit the data (**high variance**) when it is too clomplex and does not generalize well on new data. Usually there is a trade-off between bias and variance and fine tuning is required to find the best parameters for our model.\n\nThe classic way of estimating the performance of a model is the holdout method, which consist of splitting the dataset into a training set and a test set, exactly as we have done in the previous sections,however there are more advanced tecniques that allows us to pick the right model for our data. A better yet still simple alternative is to split the dataset into three parts (a training set, a validation set and a test set), train the model on the first one,use the second one for parameter tuning and model selection, and at last use the test dataset to obtain a less biased estimate of the model ability to generalize to new data.\nOne disadvantage of this method is that it may be heavily sensitive to how we split the dataset, hence more robust technique have been developed to mitigate this problem.\n","07827739":"Taking the mode of the classifiers predictions  is not the only way to decide what will be the final prediction. A different method rely on the predicted probabilities for class labels and the final prediction is taken equal to the class that has the largest total probability.\n\nscikit-learn has a built-in VotingClassifier that allows to implement both a class based and a probability based ensemble classifier by specifying the parameter \"voting:{\u2018hard\u2019, \u2018soft\u2019}\" ( default value is\u2019hard\u2019). If we set voting equal to \"hard\" the model uses predicted class labels for majority rule voting,else, if we select \"soft\", the estimator will use the the average predicted probabilities.\n\nOne down-side of using scikit's VotingClassifier is that it requires unfitted models while our estimators were already trained and their parameters were already optimized,thus it should not be a surprise if we achieve a lower accuracy with it.\nA workaround that allows us to implement a maiority voting classifier requires that we import **EnsembleVoteClassifier** from mlxtend.classifier. Another useful feature of both VotingClassifier and EnsembleVoteClassifier is the possibility of setting up  different weights for each classifier and compute the final prediction as the weighted average of the results of each estimator. The parameter \"weight\" can be either left empty,which means that the model will use uniform weight, or fine tuned to further improve the overall accuracy of the model. Let's check which one of the 2 options give the best results:\n\n### 1) soft voting","454d6811":"# Preliminar Data Exploration and Filtering\n","a01b2029":"## Principal Component Analysis\n\nPCA  helps us to identify  patterns hidden in dataset based on the correlation between features. In a nutshell, Pca aims to find the directions of maximum variance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one. The orthogonal axes of this new subspace (known as principal component)  can be interpreted as the directions of maximum variance.\nWhen we use PCA we construct a **d** (number of original features) x **k** ( number of new components)  projection matrix **W**, that will allow us to map the old space to the new subspace through matrix multiplication. The matrix **W** will be constructed by calculating the eigenpairs of the covariance matrix and then selecting the k eigenvectors with the biggest eigenvalues.\nTo see how many eigenvectors  should be picked let's write a short code that will allow us to see the **Explained Variance Ratio**, i.e. the relative weight of the eigenvalue respect to the total.","cc65fc35":"## Scitkit-Learn's Random Forest Classifier\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or the mean\/average prediction (in case of regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting their training set. The random forest algorithm can be summarized in four simple steps:\n   1. Draw a rando bootstrap sample of size n ( randomly choose n examples from the training dataset with no replacement).\n   2. Grow a decision tree from the bootstrap sample. At each node:\n            1. Randomly select d features without replacement.\n            2. Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.\n   3. Repeat the steps 1-2 k times.\n   4. Aggreagate the prediction by each tree to assign the class label by majority vote.\n   \nThe big advantage of random forests over decision trees is that we don't have to worry so much about choosing good hyperparameters since the ensemble model is quite robust to noise from the individual decision trees. The only parameter that we really need to care about is the number of trees,k, that we choose for the random forest, since the larger it is the greater are both the computational cost and the performance of the ensemble. In scikit-learn,as default parameters, the size of the bootstrap is taken equal to the number of examples while the number of features,d, at each split, is taken equal to the square root of the number of features in the training dataset.  \n\nA nice and time saving feature of the scikit-learn is the support to multi-core CPU through the declaration of the parameters n_jobs, which split the load over the chosen number of cores or all the available cores (n_jobs=-1)\n            \n        \n","5d0ce3e9":"let's compare the scores:","c420b0b1":"# Ensemble Learning\n\nEnsemble learning uses multiple algorithms to obtain better predictive performance than any single one of its constituent algorithms could.  Ensemble methods fall into two broad categories,   sequential ensemble techniques and parallel ensemble techniques. Sequential ensemble techniques generate base learners in a sequence, e.g., Adaptive Boosting (AdaBoost). The sequential generation of base learners promotes the dependence between the base learners. The performance of the model is then improved by assigning higher weights to previously misrepresented learners.\n\nIn parallel ensemble techniques, base learners are generated in a parallel format, e.g., random forest. Parallel methods utilize the parallel generation of base learners to encourage independence between the base learners. The independence of base learners significantly reduces the error due to the application of averages.  \nThe majority of ensemble techniques apply a single algorithm in base learning, which results in homogeneity in all base learners. Homogenous base learners refers to base learners of the same type, with similar qualities (such  in Adaboost). Other methods apply heterogeneous base learners, giving rise to heterogeneous ensembles (majority\/plurality classifiers). Heterogeneous base learners are learners of distinct types.\n\n\n","41f8b0a0":"### Features description:\nfrom the official Sloan Digital Sky Survey website,clicking on the \"Schema\" subsection and then on the SpecPhoto table, we can retrieve the description of the features which compose our dataset:  \n   1.  **objid** = Unique SDSS identifier composed from[skyVersion,rerun,run,camcol,field,obj]\n   2.  **ra**    = ra in J2000 from SpecObj\n   3.  **dec**   = dec in J2000 from SpecObj\nRight ascension (ra) and Declination (dec) specify the direction of a point on the celestial sphere in the equatorial coordinate system.\n   4. **psfMag_u** = PSF (Point Spread Function) flux in the u band.The point spread function (PSF) describes the response of an imaging system to a point source or point object\n   5. **psfMag_g** = PSF flux in the g band\n   6. **psfMag_r** = PSF flux in the r band\n   7. **psfMag_i** = PSF flux in the i band\n   8. **psfMag_z** = PSF flux in the z band\n   9. **run**      =  identifies the specific scan.A Run is a length of a strip observed in a single continuous image observing scan, bounded by lines of \u03bc and \u03bd. A strip covers a great circle region from pole to pole; this cannot be observed in one pass. The fraction of a strip observed at one time (limited by observing conditions) is a Run. Runs can (and usually do) overlap at the ends.\n   10. **rerun**   =  A reprocessing of an imaging run. The underlying imaging data are the same, but the software version and\/or calibration are different.\n   11. **camcol**  =  identify  the scanline within the run\n   12. **field**   =  Field number\n   13. **class**   =  object class (galaxy, star or quasar object)\n   14. **redshift**=  the observed Redshift\n   15. **plate**   =  Each spectroscopic exposure employs a large, thin, circular metal plate that positions optical fibers via holes drilled at the locations of the images in the telescope focal plane. These fibers then feed into the spectrographs. Each plate has a unique serial number, which is called plate.\n   16. **mjd**     =  modified Julian Date, i.e. the date at which the data were taken\n   17. **fiberid** =  fiber ID\n   ","7be24b26":"# Introduction to Pipelines\n\nData scientists have often to perform the same steps over and over from data preparation to the final accuracy test. As you have seen in the previous section many cells differ slightly from one another,as the modification often consist in small and time consuming changes,moreover as the number of machine learning models implemented raise we have to keep track of the parameter values we have employed.\nPipelines allow us to streamline this process by compiling the preparation steps while easing the task of model tuning and monitoring. The scikit-learn class Pipeline can be considered as a wrapper around the individual transformers and estimators and it works by sequentially applying a list of transforms and a final estimator. Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to call the fit method.\nAnother advantage of using pipelines is the possibility to cross-validate together  several steps that have been assembled together, while setting different parameters for each one.\nWe will provide few implementation of pipelines as examples","2ab2482b":"Since we have seen that applying PCA does not improve the accuracy of the random forest model, but on the contrary  decrease it noticeably, we implement another pipeline which does not cointains the PCA transformer","40d7c9a6":"the scatterplots tell us that objects classified as Quasars have an higher overall redshift,and,according to the famous Hubble's law, that means they are farther from us than every galaxy and every star sampled. They  are also less represented at the lowest magnitudes in all 5 bands, a consequence of the higher distance between them and us. Galaxies instead show a weak linear relation between their redshift and their magnitude across every band. Lastly stars redshift is too small to extract any meaningfull property from these plots. Let'sa different kind of plot to see if we can infere more from them.","40dbbadb":"### Pipeline with a Random Forest Classifier\nThis time we switch the KNN estimator with a random forest estimator with a number of estimator equal to 11 and n_jobs=2. By looking at the new score we can tell that,in our case, using a PCA transformer before feeding data to the RandomForestClassifier reduce our accuracy score, hence we will not use PCA  with this classifier anymore","0fcc57ea":"### Feature selection\n\nOnce given a look at the descriptions we can easily realize that some of the feautures  collected in our dataset are not related to the target variable \"class\",henceforth we will discard them.\n","3b939a1d":"### Create a train dataset and a test dataset","2586b479":"### KNN fine tuning","9400d6bb":"### support vector machine tuning","e80c5d0d":"### Logistic Regression with PCA\n Later, after the introduction of the concept of pipelines and GridSearch, we will use the informations gained through the Principal Component Analysis to tune our models with a much shorter code.  \nFor now,as an introductory example,we will show how the application of PCA can improve the accuracy of the LogisticRegression model.","bd829589":"as we can see, setting voting on \"hard\" allows us to obtain slightly higher results","6d24900a":"### LogisticRegression fine tuning","18025272":"as we can see the random forest classifier shows consistently higher accuracy scores than the other algorithms so it can be considered the best candidate model. Support vector machine and logistic regression comes respectively second and third with accuracy scores of 96% and 95.5%. Nevertheless,accuracy is just one of the possible perfomance metrics  with which we can evaluate our models.","d90865bd":"# Scope of the Notebook\n\nI started writing this notebook while I was reading Sebastian Raschka's book \"Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow\", as a way to put what I learned on \"paper\" and test the results. As many already know, starting as a novice is often the hardest step, that's the reason I began my path on kaggle with a dataset similar to other datasets previously adopted by other kaggle users but at the same time with a degree of personalization.This choice allowed me to learn from other users notebooks,initially by mimicking what they have done,subsequently by adding my own contributions and code. Since my Aim was to learn as much as possibile about the platform, the markup language, the sql initally needed to retrieve the data, pandas and a bunch of different machine learning models,I didn't give much thought to the idea of reaching the highest score, although I think I reached a good result(f1 score of 98.6% on an unbalanced dataset),thus the notebook assumed more the form of a journal where I reported what I was reading at that time, rather than a short notebook showing how to reach the highest score with the shortest path,and became quite long in the process. On top of that, the need to translate into english my thoughts added another level of difficulty. I am sure there are better ways to do what i have done and that there are errors I have made that needs to be corrected.\nI  owe a debt of gratitude toward Sebastian Raschka for the knowledge he shared through his book and his blog and kaggle user LennartGrosser whose notebook I took as initial example.","4363cb61":"the vast majority of stars have a  redshift close to zero, while a tiny number of them have a slightly positive or negative redshift.","c0f3b70f":"\n### Confusion Matrix\n\n The Confusion matrix is a table layout that makes it easy to see whether the system is confusing between classes. Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). If we ","9c8d2642":"The results shows that the application of the kernel trick does not improve the overall accuracy, but we have yet to optimize the hyperparameters!","47e3a244":"### Pipeline with a LogisticRegression classifier\nwe define a pipeline called pipe_LR composed of a sequence of 2 transformers (RobustScaler() and PCA) and 1 final estimator  with default hyperparameter values (C=1.0,penalty = l2)","599e7981":"let's now plot the data retrieved:","32da4ee3":"# Other evaluation metrics","0f44aaaf":"## 1)Class based Maiority\/Plurality voting\n\nIn maiority voting the final prediction is taken as the mode of the predictions of the single estimators. It is one of the simplest ensemble based models,useful when we have already trained estimators.\n\n\n\n\n\n","c4983bf2":"## GridSearch\n\nWith convalidation curves we improve the performance of the model by tuning a single parameter, though most of the times we want to find the best combination of multiple parameter values. A simple yet effective technique is called Grid Search and it's already implemented in scikit-learn through the **GridSearchCV** class from model_selection. GridSearch works in a fairly simple way: we pass predefined values for hyperparameters to the **GridSearchCV** function which tries all the combinations of the values and evaluates the model for each combination using the Cross-Validation method. We do the passing by defining a dictionary in which we mention a particular hyperparameter along with the values it can take. After using this function we get the accuracy for every combination of hyperparameters and we can choose the one with the best performance","070b14aa":"### Standardization\nStandardScaler() removes the mean and scales the data to unit variance. it's just one of the many possible scaler that scikit-learn provides. we will compare more scaler in the following sections","3231755f":"The plotted matrix shows that our model can correctly distinguish quasars and stars but the accuracy suffer when it has to distinguish between galaxies and quasars.  ","db667bd0":"we can then optimize the weight depending on which score we want to maximize. For example, to maximize the f1 score we must use the values:","eb225e86":"## Support Vector Machine Classifier\n\nA Support Vector Machine (SVM) is a supervised machine learning algorithm that can be employed for both classification and regression purposes. SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes. The data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane, are called \"support vectors\". Because of this, they can be considered the critical elements of a data set. The distance between the hyperplane and the nearest data point from either set is known as the margin. The goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set, giving a greater chance of new data being classified correctly.\n\nFrom the scikit-learn official[ website](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/svm.html) we learn that:\n\n\"SVC and NuSVC implement the \u201cone-versus-one\u201d approach for multi-class classification. In total, n_classes * (n_classes - 1) \/ 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers,the decision_function_shape option allows to monotonically transform the results of the \u201cone-versus-one\u201d classifiers to a \u201cone-vs-rest\u201d decision function of shape (n_samples, n_classes)\"\n\nwe will then set  decision_function_shape= \"ovr\".\n","c2dd6314":"## Logistic Regression\n\n**Logistic Regression** is a machine learning algorithm which is used for classification problems, it is a predictive analysis algorithm and is based on the odds of a certain event to happen, or more precisely on the inverse function of the logit function knows as the **Logistic sigmoid function** or **sigmoid function**, which is the natural logaritm of the odds of a certain event to happen\n\n$$\\begin{equation*}logit(p)= log\\frac{p }{1-p }=z= \\boldsymbol {w}^{T}\\boldsymbol x\\end{equation*}.$$\n\n$$ \\phi(z) = \\frac{1 }{1+ e^{-z} } ,$$   where z is the net input \n$$ z= \\boldsymbol {w}^{T}\\boldsymbol x .$$\nIt can be considered a variant of the Adaline since it differs mainly for the activation function used. In fact, while the Adaline use the identity, the Logistic Regression model use the **sigmoid function**.\nThe logistic regression in his base form is  a linear model for binary classification like the Perceptron or the Adaline, though it can be generalized to multiclass settings in a fairly easy way. One way to do this is to use the OvR technique, which consist of training one classifier per class, where the particular class is treated as the positive and the remaining classes as negatives. Unlabeled data are then assigned the class label with the highest confidence.  \n\n","4fbbc272":"##  Introductory Feature Engineering\n\n ","3707534a":"### The regularization parameter and the Slack variables\nThe Slack variable $\\xi$ was introduced for the first time in 1995 by Vladimir Vapnick in the effort to improve the convergence of the model when dealing with non separable data by  introducing a cost penalization for miclassified data. Through the C parameter, we can control how much weight we assign to misclassified elements,thus affecting the width of the margin and the bias-variance tradeoff.\n","24ad5a9e":"### Pipeline with a K-Nearest-Neighbors estimator\nThis time  we have provided 2 transformers,RobustScaler and PCA, and a KNeighborsClassifier estimator and we have selected n_components=7 and n_neighbors, the number of nearest neighbors equal to 7.","a4dfc7db":"## Adaptive boosting (Adaboost)\n\nAs the official scikit-learn website report, An AdaBoost  classifier is \"a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases and learn from the mistakes of the previous weak learners.\" In boosting ensemble models, the base classifiers are very simple and prone to errors in classification.  The key idea of boosting algorithms is to let the model focus on training examples which are hard to classify by associating to them an higher weight, while at the same time lowering the weight associated with examples which are easy to classify.  Since we have seen the Random Forest classifier performed well in the previous section, we will apply the Adaboost classifier  combined with Decision Tree classifiers for the weak learners","fb16bc29":"### Split train and target columns","7603c4b0":"The barplot and the stepplot indicate that the first 7 principal components accounts for over 98% of the variance,so let's apply PCA to our training and test sets with a number of components equal to 7","697bc772":"There are only 3 unique values that we will encode later with th LabelEncoder. For now let's move the target column \"class\" to the right","a84a9dc5":"### Dealing with Class Imbalance","97f0be4d":"## Filtering\nLet's take a look at the features and what they mean:","5d51179f":"### F1 score\n\nBefore introducing the **F1 score** we need to define 2 new metrics for the evaluation of our model performances: the **Precision** and the **Recall**. Starting from the point of view of a binary classification problem, we can say that the precision score is defined as the number of true positives divided the number of positives, whether they are false or true:\n\n\n$$\\begin{equation*} Precision =  \\frac{TP }{TP+FP }\\end{equation*}.$$\n\nthe Recall, instead, is equal to the number of True positives divided the sum of true positive and false negatives:\n\n$$\\begin{equation*} Recall = \\frac{TP }{FN +TP }\\end{equation*}.$$\n\n\nOptimizing for Recall reduce the number of False negatives, while optimizing for Precision reduce the number of False positive.\n\nTo balance the up- and down-sides of these 2 new metrics often a combination of the two is used. This new score is called \"F1 score\" and is defined as :\n\n$$\\begin{equation*}F1 = 2 \\frac{PRE * REC }{PRE +REC }\\end{equation*}.$$  \n\nIn the multi-class and multi-label case the F1 score  represent the average of the **F1 score** of each class with weighting depending on the value of the average parameter:  \n\n1. average = '**macro**' : Calculate the metrics for each label  and find their unweighted mean. This method does not take label imbalance into account  \n2. average ='**weighted**': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.  \n3. average = '**micro**': Calculate metrics globally by counting the total true positives, false negatives and false positives.\n\n\nAs an example,we will compute the F1 score on the original unbalanced dataset:\n\n\n","456aece1":"## Learning curves","686f77bd":"## EDA","e8b01ea5":"# Introduction: Automatic Spectral Classification of Quasars,Galaxies and Stars in the Sloan Digital Sky Survey\n\n## What's a Quasar?\nWith the exception of the short-lived powerful explosions responsible for supernovae and gamma-ray bursts, quasars (or QSOs) are the brightest objects in the Universe.The word \u201cquasar\u201d  and the acronym \u201cQSO\u201d are short for \u201cquasi-stellar radio source\u201d and \u201cQuasi-stellar object\u201d respectively, due to their \u2018star-like\u2019 appearance.  \nThey are thought to be powered by supermassive black holes (black holes with a mass of more than one billion solar masses) which lie at the center of massive galaxies. However, the light we see from quasars does not come from directly from the black-hole but from a disk of gas and stars called an accretion disk, which surrounds it. Intense heat and light is emitted from this accretion disk, caused by friction produced from the material swirling around, and eventually into, the black hole. Quasars are typically more than 100 times brighter than the galaxies which host them and they emit jets from their central regions which can be larger in extent than the host galaxy.Despite the enormous amount of energy these objects have,they are  relatively small, with the most common size being close to 1.5 light years.  Since they are very bright objects, their lifespan is shortened by their hunger for power,and they dim when the source of their power, the accretion disk, cannot sustain anymore the rate of radiation.This explains why quasars were more common in the early universe, as this energy production ends when the supermassive black hole consumes all of the gas and dust near it, and why many of the brightest quasars are  very distant from us (The highest-redshift quasar known (as of December 2017) was ULAS J1342+0928, with a redshift of 7.54, which corresponds to a comoving distance of approximately 29.36 billion light-years from Earth (these distances are much larger than the distance light could travel in the universe's 13.8 billion year history because space itself has also been expanding).  \nWhen a quasar jet interacts with the gas surrounding the galaxy, radio waves are emitted which can be seen as \u201cradio lobes\u201d by radio telescopes. Even though quasars are intrinsically very bright, we cannot see any quasars in the night sky without using a telescope. This is because the nearest quasars are more than a billion parsecs away, therefore they appear relatively faint in the sky despite their large luminosities and their light is greatly redshifted. Even with small telescopes,the only visible Quasar is 3C 273, discovered in the 50's, which has a 12.9 magnitude. They were first noted as radio sources with no corresponding visible object. Later,with the development of high-resolution imaging from ground-based telescopes and the Hubble Space Telescope, the \"host galaxies\" surrounding the quasars started being detected. \n\n## The Sloan Digital Sky survey\nMore than 750000 quasars have been found (as of August 2020), most from the Sloan Digital Sky Survey. All observed quasar spectra have redshifts between 0.056 and 7.64 (as of 2021). Applying Hubble's law to these redshifts, it can be shown that they are between 600 million and 29.36 billion light-years away (in terms of comoving distance)\nQuasars' luminosities are variable, with time scales that range from months to hours. This means that quasars generate and emit their energy from a very small region, since each part of the quasar would have to be in contact with other parts on such a time scale as to allow the coordination of the luminosity variations. This would mean that a quasar varying on a time scale of a few weeks cannot be larger than a few light-weeks across. The emission of large amounts of power from a small region requires a power source far more efficient than the nuclear fusion that powers stars,compatible only with the conversion of the gravitational potential energy to radiation.\nRadiation from quasars is partially \"nonthermal\" (i.e., not due to black-body radiation), and approximately 10% are observed to also have jets and lobes like those of radio galaxies.Quasars can be detected over the entire observable electromagnetic spectrum, including radio, infrared, visible light, ultraviolet, X-ray and even gamma rays. Most quasars are brightest in their rest-frame ultraviolet wavelength of 121.6 nm Lyman-alpha emission line of hydrogen, but due to the tremendous redshifts of these sources, that peak luminosity has been observed as far to the red as 900.0 nm, in the near infrared. \n\n## The role of Quasars in astrophysics\n\n \nBecause quasars are extremely distant, bright, and small in apparent size, they are useful reference points in establishing a measurement grid on the sky. The International Celestial Reference System (ICRS) is based on hundreds of extra-galactic radio sources, mostly quasars, distributed around the entire sky. Because they are so distant, they are apparently stationary to our current technology, yet their positions can be measured with the utmost accuracy by very-long-baseline interferometry (VLBI).\nHowever, their role as sky candles has been reduced due to the difficulty of obtain a precise estimation of their luminosity.\nThus, until recently, the role of the standard candles of the universe has been taken by Type Ia supernovae with a known brightness. The supernovae extended our reach to when the universe was a third of its current age. Nevertheless,to see farther back, and probe the era when dark energy overtook matter, astronomers need something even more luminous. Recently, in a new study published on nature [Nature](http:\/\/https:\/\/www.nature.com\/articles\/s41550-018-0657-z), a team lead by Guido Risaliti (University of Florence and INAF-Astrophysical Observatory of Arcetri, Italy) and Elisabeta Lusso (Durham University, UK)  proposed to use Quasars to test the \u039bCDM model in the redshift interval between the farthest observed type Ia supernovae and the cosmic microwave background.\nThe key to achieve this result was realizing that there is a relation between the emission of X-rays from quasars and their luminosity who had not been exploited until now due to the limited number of quasars discovered and the poor quality of the observations.Taking advantage of the growing number of known quasars,Risaliti and Lusso removed any sources where disk emission is obscured (by dust or gas) or contaminated (by emission from a fast-flowing black hole jet). Their careful selection results in a much tighter, more useful relation which was  then been applied on the  data collected from the Sloan Digital Sky Survey, the XMM-Newton, Chandra, and Swift space telescopes. In this way, Rosaliti and Lusso have been able to estimate both the distance and the luminosity of up 1,600 quasars,turning them into a novel kind of standard candles.\nThe results shows that the distance modulus\/redshift relation of quasars at z\u2009<\u20091.4 is in agreement with that of supernovae and with the concordance model. However, a deviation from the \u039bCDM model emerges at higher redshift, with a statistical significance of ~4\u03c3. If an evolution of the dark energy equation of state is allowed, the data suggest dark energy density increasing with time.Therefore, to further expande our knowledge of the early life of the universe,it is of fundamental importance to develop algorithms able to  scan the terabyte of data collected everyday by the telescopes around the globe and detect new quasars which can be used as candles.\n\n## The need for machine learning based techniques\n\nThe enourmous amount of data produced everyday cannot by analysed by hand and provide a challenge for the traditional algorithm. Machine Learning based methods fine tuned on already classified samples offer us a solution to the problem of the  swelling of the raw data produced each day.  \n\n### Note:\nSince this is nothing but an educational notebook, I have decided to take only a limited number of sampled from the Sloan Digital Sky Survey and I decided to learn and apply different ML techniques  to this limited subset with the goal to reach a  acceptable classification performance.","d52dbbf5":"there seems to be some outliers as the min row reveal,let's get rid of them!\nWe will remove all the random numbers that lie in the lowest quantile and the highest quantile for each of the sequent columns:","eb8b1c2f":"### Random Forest fine tuning\n","1d7df644":"## Nested cross validation\nthe .score() method give us a good estimate of the model accuracy, but if we want to select a model between different machine learning algorithms it is  raccomended to use another approach called \"Nested cross-validation\". In fact, using this technique reduce the  bias of the error related to the dataset used as test. The algorithm consiste of 2 different loops, one external and one internal, the first of which split the dataset into a training and test dataset, and the second one split the training dataset in a training and validation set.  In scikit-learn we can implement the nested cross validation algorithm by writing something as simple as:\n```scores = cross_val_score(\"yourGridSearchObject\",X_train,y_train,scoring=\"accuracy\",cv=5)```.   \nLet's compare the scores obtained with the previous defined pipelines and  the Nested cross validation algorithm:\n","a139122c":"## K-fold cross-validation and stratified k-fold cross validation\n\nwith K-fold cross-validation we randomly split the training dataset into k different sets, k-1 of them are used for training and the remaining one is used as a validation test.The process is repeated k times, each time using a different set for validation, and the final accuracy value is calculated by taking the mean of the values obtained. An improvement over the standard k-fold cross validation technique is the stratified K-fold cross validation, which provides better  bias and variance estimates, particularly in case of unequal class proportions. The mean difference between stratified k-fold  and the standard k-fold cross validation method is that the first preserve the class proportions in each fold while the latter does not.  \nfortunately Scikit-Learn already has an implementation of stratified k-fold, thus we don't need to implement it from 0.","6ee20d25":"as we can see now the dataset contains an equal number of examples for each  value of the target variable. The number of total element in the dataset is  19725, split equally among Galaxies, Quasars and Stars","c75b9a40":"# Elementary Machine Learning Algorithms\n\nWe will now introduce few examples of the most common and widespread machine learning models. All these models have parameters which can be tweakened to improve the overall accuracy but we will not modify them for now, having taken the decision to focus on the simplest implementation of the algorithm. Later,once we have introduced the concepts of pipelines and Gridsearch, we will investigare further this aspect that we are for now leaving mostly untouched. Before we start training model, let's introduce briefly the concept of accuracy\n\n## Accuracy\n\nThe accuracy is a useful metric with which we are able to quantify the performance of a model and, in a binary classification problem, is equal to the sum of correct prediction divided the total number of predictions:\n\n $$ \\begin{equation*}ACC= \\frac{TP +TN }{FP +FN + TP + TN }\\end{equation*} .$$\n \n \n It is tightly related to the concept of Error, which is instead equal to the number of wrong predictions divided the total number of predictions\n \n $$ \\begin{equation*}ERR= \\frac{FP +FN }{FP +FN + TP + TN } = 1- ACC\\end{equation*}. $$\n \n \nWhen we have more than 2  labels (i.e. in a multiclass classification problem),scikit-learn compute the accuracy as the total number of correct predictions divided by the total number of samples. Due to the way it is defined, Accuracy as an evaluation metrics makes sense only if the class labels are uniformly distributed,since it gives the same weight to every correct prediction.In fact,in case of imbalanced classes,the score may not reflect the actual predictive abilities of our model. Since we have already solved the issue of class imbalance by undersampling the majority class we can calculate the accuracy score without having to worry about a skewed score","bb6505f6":"As you can see,while the prediction accuracy has hardly changed, the time consumed for the training of the model is half of what it was before. Let's take a look now at how the last 6 principal components appear when plotted against the first one","2ae8acae":"### support vector machine"}}