{"cell_type":{"3367d0cb":"code","021e68f2":"code","de273526":"code","d0cd8809":"code","cd0da228":"code","3a4ceddc":"code","a2a81e37":"code","0551d2af":"code","924760b6":"code","338f61a9":"code","009b3e98":"code","9284ad11":"code","4a411e97":"code","32285b68":"code","6f5c9ea8":"code","8aadb292":"code","c701e727":"code","d62b9d14":"code","2ac8fc29":"code","b3f02264":"code","5bb771d9":"code","8f4d5f01":"code","db286bae":"code","ca527bca":"code","ac2229aa":"code","e5a5b94f":"code","05555d48":"code","49a201ae":"code","929a295b":"code","811b4d05":"code","85f663e4":"markdown","43bec929":"markdown","46bc9cb7":"markdown","dd24f5ca":"markdown","c0251788":"markdown","5db63d54":"markdown","6e98e3da":"markdown","0fc61346":"markdown","d13d68a8":"markdown","097be437":"markdown","c2b0ed97":"markdown","b8cec8a3":"markdown","409ba525":"markdown","269d868f":"markdown","4e18c053":"markdown","c573c558":"markdown","ca511631":"markdown","6dea79ce":"markdown","045736f5":"markdown","32288ca7":"markdown","a87edc3a":"markdown","db20166b":"markdown","adeeb089":"markdown","17d3c488":"markdown","841dae0b":"markdown","c90b2dd1":"markdown","084e71ca":"markdown","ad71141e":"markdown","1526c1db":"markdown","857dcfe8":"markdown","3885f7b5":"markdown","ac6cf134":"markdown","bf5b3c9c":"markdown","a005ef7c":"markdown","525967fe":"markdown","19ef8c3a":"markdown","36d00e97":"markdown","ad3faffa":"markdown","b3673780":"markdown","fc1801f5":"markdown","9a6da4bc":"markdown","37ba1d91":"markdown"},"source":{"3367d0cb":"import pandas as pd\nimport seaborn as sns\n\ndf = pd.DataFrame([[ 1792, 231500],\n                   [  1592, 189500],\n                   [   900, 122000],\n                   [   630,  84500],\n                   [  1188, 142000],\n                   [  1200, 151000],\n                   [  1648, 195000],\n                   [  1644, 205000]],\n                    columns=['GrLivArea', 'SalePrice'])\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=df)","021e68f2":"from sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\ny = df['SalePrice'].to_numpy()\nX = df['GrLivArea'].to_numpy().reshape(-1,1)\n\nlinreg = LinearRegression()\nlinreg.fit(X, y)\ny_pred_1 = linreg.predict(X)\n\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=df)\nplt.plot(X, y_pred_1, '-')","de273526":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y, y_pred_1)","d0cd8809":"df2 = pd.DataFrame([[630,  84500],\n                    [900, 122000]], \n                   columns=['GrLivArea', 'SalePrice'])","cd0da228":"y_2 = df2['SalePrice'].to_numpy()\nX_2 = df2['GrLivArea'].to_numpy().reshape(-1,1)\n\nlinreg_2 = LinearRegression()\nlinreg_2.fit(X_2, y_2)\ny_pred_2 = linreg_2.predict(X_2)\n\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=df2)\nplt.plot(X_2, y_pred_2, '-', c='r')","3a4ceddc":"import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nnp.round(mean_squared_error(y_2, y_pred_2),2)","a2a81e37":"y_pred_3 = linreg_2.predict(X)\n\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=df)\nplt.plot(X, y_pred_3, '-', c='r')","0551d2af":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y, y_pred_3)","924760b6":"sns.scatterplot(x='GrLivArea', y='SalePrice', data=df)\nplt.plot(X, y_pred_3, '-', c='r', label = 'Linear Regr (Trained on 2 Data point)')\nplt.plot(X, y_pred_1, '-', label = 'Linear Regr (Trained on All Data point)')\n\nplt.legend()","338f61a9":"from sklearn.linear_model import Ridge\n\nridge_2 = Ridge(alpha=5000)\nridge_2.fit(X_2, y_2)\n\ny_pred_4 = ridge_2.predict(X_2)\n\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=df2)\nplt.plot(X_2, y_pred_4, '-', c='g')","009b3e98":"y_pred_5 = ridge_2.predict(X)\nnp.round(mean_squared_error(y, y_pred_5),2)","9284ad11":"fig = plt.figure()\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=df)\nplt.plot(X, y_pred_3, '-', c='r', label = 'Linear Regr (Trained on 2 Data point)')\nplt.plot(X, y_pred_5, '-', c='g', label='Ridge Regr (Trained on 2 Data point)')\nplt.plot(X, y_pred_1, '-', label = 'Linear Regr (Trained on All Data point)')\n\nplt.legend()","4a411e97":"sns.scatterplot(x='GrLivArea', y='SalePrice', data=df2)\nplt.plot(X_2, y_pred_2, '-', c='r')\nplt.plot(X_2, y_pred_4, '-', c='g')\nplt.plot(X, y_pred_3, '-', c='r', label = 'Linear Regr (Trained on 2 Data point)')\nplt.plot(X, y_pred_5, '-', c='g', label='Ridge Regr (Trained on 2 Data point)')\n\nplt.legend()","32285b68":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6f5c9ea8":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.head()","8aadb292":"train_df = train_df.drop('Id', axis=1)","c701e727":"target = 'SalePrice'\ncategorical_features = []\nnumeric_features = []\nfeatures = train_df.columns.values.tolist()\nfor col in features:\n    if train_df[col].dtype != 'object': \n        if col != target:\n            numeric_features.append(col)\n    else:\n        categorical_features.append(col)\n        \nfor col in numeric_features:\n    mean = train_df[col].mean()\n    train_df[col] = train_df[col].fillna(mean)\n    \nfor col in categorical_features:\n    train_df[col] = train_df[col].fillna('None')\n\n    ","d62b9d14":"train_df.corr()['SalePrice'].sort_values(ascending=False)","2ac8fc29":"top_6_corr=['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', '1stFlrSF', 'FullBath']\nprows = 2\npcols = 3\nfig, axs = plt.subplots(prows, pcols, figsize=(pcols*3.5, prows*3))\nfor r in range(0,prows):\n    for c in range(0,pcols):  \n        i = r*pcols+c\n        col=top_6_corr[i]\n        sns.regplot(x=train_df[col], y=train_df['SalePrice'], ax = axs[r][c])","b3f02264":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])","5bb771d9":"from scipy.stats import skew\nskewed_feats = train_df[numeric_features].apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\nskewed_feats","8f4d5f01":"train_df[skewed_feats] = np.log1p(train_df[skewed_feats])","db286bae":"from sklearn.preprocessing import LabelEncoder\n# Encoding categorical features\nfor col in categorical_features:\n    le = LabelEncoder()\n    le.fit(list(train_df[col].astype(str).values))\n    train_df[col] = le.transform(list(train_df[col].astype(str).values))","ca527bca":"y = train_df['SalePrice']\nX = train_df.drop('SalePrice', axis=1)","ac2229aa":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nalpha = [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000, 10000]\nridge_gscv = GridSearchCV(estimator=Ridge(), \n                                param_grid={'alpha': alpha,\n                                        'fit_intercept': [True, False]},\n                                scoring='neg_mean_squared_error',\n                                cv=5)","e5a5b94f":"ridge_gscv.fit(X, y)\nridge_gscv.best_params_","05555d48":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","49a201ae":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha = ridge_gscv.best_params_['alpha'], \n                        fit_intercept = ridge_gscv.best_params_['fit_intercept'])\nridge.fit(X_train, y_train)","929a295b":"index = []\nfor c in top_6_corr:\n    i = train_df.columns.get_loc(c)\n    index.append(i)\nridge.coef_[index]","811b4d05":"print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))","85f663e4":"$\\hat{Y} = \\hat{\\beta}_{0} + \\sum\\limits_{i=1}^{n}X_{i}\\hat{\\beta}_{i}  $","43bec929":"$ {L}_{OLS}({\\hat\\beta}) = \\sum\\limits_{i=1}^{n}({Y}_i - \\hat{Y}_i)^2$","46bc9cb7":"In return to the small amount of bias we can get significant drop in variance.","dd24f5ca":"$\\hat{\\beta}_{i}$ - slope of the straight line \n\n$\\hat{\\beta}_{0}$ - y-axis intercept","c0251788":"When we have only two data points, the line results in overfit and high variance. ","5db63d54":"The value of ${\\lambda}$ can be any value from 0 to positive infinity.\n\nIf ${\\lambda} = 0$, Ridge Regression = Ordinary Least Square.\n\nWith increasing value of ${\\lambda}$ the slope of the line becomes smaller.\nThe value of SalePrice becomes less and less sensitive to GroundLivingArea.\n\nWe need to choose the value of ${\\lambda}$ that results in lowest variance.","6e98e3da":"When we have lot of measurements we can assume that the straight line makes the best estimate of the relationship between 'Ground Living Area' and 'Sale Price'.\n\nBut what if we have only two data points in training set.","0fc61346":"We get very large mean_squared_error contributed by high variance. Model is overfit to the training data.","d13d68a8":"In contrast Ridge Regression tries to learn parameter - slope and y-axis, such that they minimize the sum of squared residuals + ${\\lambda} * {the slope}^2$.","097be437":"$\\sum\\limits_{j=1}^{m}{\\hat\\beta}_j^2$ adds the penalty to the traditional Ordinary Least Square method.","c2b0ed97":"Linear Regression will try to fit a straight line, since the line overlap the data points this time, mean_squared_error will be zero.","b8cec8a3":"${\\lambda}$ is the severity of the penalty","409ba525":"The main idea behind Ridge Regression is to find a line that does not fit the training data that well, there is a small amount of bias into how the new line fit the training data.","269d868f":"Linear Regression will try to fit a straight line to data,","4e18c053":"## Inspired From:","c573c558":"## Build Ridge Regression model","ca511631":"Ridge Regression has tried to pull some of the coefficients towards zero.","6dea79ce":"Since the data looks relatively linear, we can use Linear Regression - (Ordinary Least Squares) to model relationship between 'Ground Living Area' and 'Sale Price'.","045736f5":"Linear Regression(Ordinary Least Squares) does not provide way to control complexity of the model to avoid overfitting.\n\nRidge Regression is also a Linear Model applies L2 Regularization to control the complexity of the model. \n\nLet us explore more on Ridge Regression in this notebook.\n\nLet us start by collecting a sample House Price data. We collect 'Ground Living Area' and 'Sale Price'","32288ca7":"${\\lambda} * \\sum\\limits_{j=1}^{m}{\\hat\\beta}_j^2$ - this penality causes bias in Ridge Regresson line.","a87edc3a":"## Summary","db20166b":"The value of  ${\\lambda}$ is determined by Cross Validation.","adeeb089":"With Ridge Regression by starting with slightly worst fit on training data can provide better prediction on test data.","17d3c488":"https:\/\/www.youtube.com\/watch?v=Q81RR3yKn30","841dae0b":"Check the coef_ of top 6 correlated features with target variable.","c90b2dd1":"When the sample sizes are relatively small, the Ridge Regression can provide better predictions on the new data by making the predictions less sensitive to the changes in Training Data. ","084e71ca":"Due to this penality, Ridge Regression line is less steeper than Linear Regression line. The effect is SalePrice is less sensitive to change in GroundLivingArea.","ad71141e":"## Idea behind Ridge Regression","1526c1db":"Let us compare both the straight line - models.","857dcfe8":"# Ridge Regression","3885f7b5":"${L}_{ridge}({\\hat\\beta}) = \\sum\\limits_{i=1}^{n}({Y}_i - \\hat{Y}_i)^2 + {\\lambda} * \\sum\\limits_{j=1}^{m}{\\hat\\beta}_j^2$","ac6cf134":"$\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{i}$ are learnt such that these values need to minimize the squared distance between data point and straight line.","bf5b3c9c":"Ordinary Least Squares tries to learn parameter - slope and y-axis, intercept such that they minimize sum of squared residuals (difference between data point and straight line).","a005ef7c":"Ridge Regression adds penalty ${\\lambda} * theslope^{2}$ when learning parameters. This penalty shrinks the values of coefficients learnt.","525967fe":"## How does Ridge Regression Work","19ef8c3a":"$\\hat{Y} = \\hat{\\beta}_{0} + \\sum \\limits _{i=1} ^{n} X_{i}\\hat{\\beta}_{i} $","36d00e97":"## Apply Ridge Regression on complete House Price Dataset","ad3faffa":"Now let us compare all 3 straight lines - models on the original data set.","b3673780":"Now if we try to test this model with our orignal data set.","fc1801f5":"### Fill in missing values","9a6da4bc":"## Evaluate the model","37ba1d91":"Ridge Regression uses same formula as Oridinary Least Squares to make predictions. "}}