{"cell_type":{"a897e972":"code","0a456acb":"code","b3ef38c9":"code","44e8802a":"code","dd698125":"code","dffdfc45":"code","fa96bf23":"code","0bd1ddf2":"code","059df5cb":"code","12b4c97a":"code","471d06dc":"code","e376275b":"code","639bfd9a":"code","24884f16":"code","99d4e3d8":"code","7a6f706c":"code","98e41d5d":"code","be7506f6":"code","d008cd58":"code","9e29250f":"code","eca517ec":"code","f61bef2f":"code","efbe9ced":"code","fc7d16d9":"code","24de0c9a":"code","89e290d2":"code","d7c32895":"code","b79f9943":"markdown"},"source":{"a897e972":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0a456acb":"# Import required librarues\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport catboost as cb\nimport lightgbm as lgb\n\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import accuracy_score\n\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.pipeline import make_pipeline\n\nfrom imblearn.over_sampling import SMOTE","b3ef38c9":"train = pd.read_csv(\"\/kaggle\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/learn-together\/test.csv\")","44e8802a":"# Remove the Labels and make them y\ny = train['Cover_Type']\n\n# Remove label from Train set\nX = train.drop(['Cover_Type'],axis=1)\n\n# Rename test to text_X\ntest_X = test\n\n\n\n# split data into training and validation data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX = X.drop(['Id'], axis = 1)\ntrain_X = train_X.drop(['Id'], axis = 1)\nval_X = val_X.drop(['Id'], axis = 1)\ntest_X = test_X.drop(['Id'], axis = 1)","dd698125":"train_X.describe()","dffdfc45":"val_X.describe()","fa96bf23":"## Harnessing Gaussian Mixture from https:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering","0bd1ddf2":"\n\nprint('> Adding cluster based feature')\nfrom sklearn.mixture import GaussianMixture\n\ngmix = GaussianMixture(n_components=10)\ngmix.fit(test_X)\n\ntrain_X['Test_Cluster'] = gmix.predict(train_X)\nval_X['Test_Cluster'] = gmix.predict(val_X)\ntest_X['Test_Cluster'] = gmix.predict(test_X)","059df5cb":"sns.distplot(train_X['Elevation'], label = 'train_X')\nsns.distplot(val_X['Elevation'], label = 'val_X')\nsns.distplot(test_X['Elevation'], label = 'test_X')\nplt.legend()\nplt.title('Elevation')\nplt.show()","12b4c97a":"sns.distplot(train_X['Aspect'], label = 'train_X')\nsns.distplot(val_X['Aspect'], label = 'val_X')\nsns.distplot(test_X['Aspect'], label = 'test_X')\nplt.title('Aspect')\nplt.legend()\nplt.show()","471d06dc":"sns.distplot(train_X['Horizontal_Distance_To_Hydrology'], label = 'train_X')\nsns.distplot(val_X['Horizontal_Distance_To_Hydrology'], label = 'val_X')\nsns.distplot(test_X['Horizontal_Distance_To_Hydrology'], label = 'test_X')\nplt.title('Horizontal_Distance_To_Hydrology')\nplt.legend()\nplt.show()","e376275b":"sns.distplot(train_X['Vertical_Distance_To_Hydrology'], label = 'train_X')\nsns.distplot(val_X['Vertical_Distance_To_Hydrology'], label = 'val_X')\nsns.distplot(test_X['Vertical_Distance_To_Hydrology'], label = 'test_X')\nplt.title('Vertical_Distance_To_Hydrology')\nplt.legend()\nplt.show()","639bfd9a":"sns.distplot(train_X['Horizontal_Distance_To_Roadways'], label = 'train_X')\nsns.distplot(val_X['Horizontal_Distance_To_Roadways'], label = 'val_X')\nsns.distplot(test_X['Horizontal_Distance_To_Roadways'], label = 'test_X')\nplt.title('Horizontal_Distance_To_Roadways')\nplt.legend()\nplt.show()","24884f16":"sns.distplot(train_X['Hillshade_9am'], label = 'train_X')\nsns.distplot(val_X['Hillshade_9am'], label = 'val_X')\nsns.distplot(test_X['Hillshade_9am'], label = 'test_X')\nplt.title('Hillshade_9am')\nplt.legend()\nplt.show()","99d4e3d8":"### define the classifiers\n### Parameters from :https:\/\/www.kaggle.com\/joshofg\/pure-random-forest-hyperparameter-tuning\n\nclassifier_rf = RandomForestClassifier(n_estimators = 719,\n                                       max_features = 0.3,\n                                       max_depth = 464,\n                                       min_samples_split = 2,\n                                       min_samples_leaf = 1,\n                                       bootstrap = False,\n                                       random_state=42)\nclassifier_xgb = OneVsRestClassifier(XGBClassifier(n_estimators = 719,\n                                                   max_depth = 464,\n                                                   random_state=42))\nclassifier_et = ExtraTreesClassifier(random_state=42)\nclassifier_lg = lgb.LGBMClassifier(silent=True, random_state=42)\nclassifier_adb = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth  = 464,\n                                                                          min_samples_split = 2,\n                                                                          min_samples_leaf = 1,\n                                                                          random_state=42), random_state=42)","7a6f706c":"pipe_rf = make_pipeline(ColumnSelector(cols=(0, 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53)),\n                        classifier_rf)\npipe_xgb = make_pipeline(ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 51, 52)),\n                         classifier_xgb)\npipe_et = make_pipeline(ColumnSelector(cols=(0, 1, 3, 5, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53)),\n                        classifier_et)\n\npipe_lg = make_pipeline(ColumnSelector(cols=(0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 23, 24, 25, 26, 29, 30, 31, 33, 35, 36, 37, 42, 43, 44, 45, 46, 48, 51, 53)),\n                        classifier_lg)\n\n\npipe_adb = make_pipeline(ColumnSelector(cols=(0, 1, 3, 5, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53)),\n                        classifier_adb)\n","98e41d5d":"sm = SMOTE(random_state=2, sampling_strategy = 'all', ratio = {1:8000, 2:8000, 3:3000, 4:3000, 5:3000, 6:3000, 7:3000})\ntrain_X_sm, train_y_sm = sm.fit_sample(train_X, train_y.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(train_X_sm.shape))\nprint('After OverSampling, the shape of train_y: {}'.format(train_y_sm.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(train_y_sm==1)))\nprint(\"After OverSampling, counts of label '2': {}\".format(sum(train_y_sm==2)))\nprint(\"After OverSampling, counts of label '3': {}\".format(sum(train_y_sm==3)))\nprint(\"After OverSampling, counts of label '4': {}\".format(sum(train_y_sm==4)))\nprint(\"After OverSampling, counts of label '5': {}\".format(sum(train_y_sm==5)))\nprint(\"After OverSampling, counts of label '6': {}\".format(sum(train_y_sm==6)))\nprint(\"After OverSampling, counts of label '7': {}\".format(sum(train_y_sm==7)))","be7506f6":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nsclf = StackingCVClassifier(classifiers=[pipe_rf,\n                                         pipe_xgb,\n                                         pipe_et,\n                                         pipe_lg,\n                                         pipe_adb],\n                            use_probas=True,\n                            meta_classifier=classifier_rf)\n\n\n\nlabels = ['Random Forest', 'XGBoost', 'ExtraTrees', 'LightGBM', 'AdaBoost', 'MetaClassifier']\n\n\n\n\nfor clf, label in zip([classifier_rf, classifier_xgb, classifier_et, classifier_lg, classifier_adb, sclf], labels):\n    scores = cross_val_score(clf, train_X_sm, train_y_sm.ravel(),\n                             cv=5,\n                             scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","d008cd58":"sclf.fit(train_X.values, train_y.values.ravel())","9e29250f":"\nval_pred = sclf.predict(val_X.values)","eca517ec":"acc = accuracy_score(val_y, val_pred)\nprint(acc)","f61bef2f":"sclffin = StackingCVClassifier(classifiers=[pipe_rf,\n                                            pipe_xgb,\n                                            pipe_et,\n                                            pipe_lg,\n                                            pipe_adb],\n                            use_probas=True,\n                            meta_classifier=classifier_rf)\n","efbe9ced":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\nprint(\"Before OverSampling, counts of label '2': {}\".format(sum(y==2)))\nprint(\"Before OverSampling, counts of label '3': {}\".format(sum(y==3)))\nprint(\"Before OverSampling, counts of label '4': {}\".format(sum(y==4)))\nprint(\"Before OverSampling, counts of label '5': {}\".format(sum(y==5)))\nprint(\"Before OverSampling, counts of label '6': {}\".format(sum(y==6)))\nprint(\"Before OverSampling, counts of label '7': {}\".format(sum(y==7)))","fc7d16d9":"sm = SMOTE(random_state=2, sampling_strategy = 'all', ratio = {1:8000, 2:8000, 3:3000, 4:3000, 5:3000, 6:3000, 7:3000})\nX_sm, y_sm = sm.fit_sample(X, y.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_sm.shape))\nprint('After OverSampling, the shape of train_y: {}'.format(y_sm.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_sm==1)))\nprint(\"After OverSampling, counts of label '2': {}\".format(sum(y_sm==2)))\nprint(\"After OverSampling, counts of label '3': {}\".format(sum(y_sm==3)))\nprint(\"After OverSampling, counts of label '4': {}\".format(sum(y_sm==4)))\nprint(\"After OverSampling, counts of label '5': {}\".format(sum(y_sm==5)))\nprint(\"After OverSampling, counts of label '6': {}\".format(sum(y_sm==6)))\nprint(\"After OverSampling, counts of label '7': {}\".format(sum(y_sm==7)))","24de0c9a":"sclffin.fit(X_sm, y_sm.ravel())","89e290d2":"test_ids = test[\"Id\"]\ntest_pred = sclffin.predict(test_X.values)","d7c32895":"# Save test predictions to file\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': test_pred})\noutput.to_csv('submission.csv', index=False)","b79f9943":"### Import the Raw Data"}}