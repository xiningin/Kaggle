{"cell_type":{"2c7f83ac":"code","84d06779":"code","c0945a34":"code","c6f2ad37":"code","233eefb4":"code","fc108250":"code","9ae419df":"code","2629e188":"code","899eee9c":"code","0775c1f6":"code","25a84334":"code","eb6ce8bf":"code","cbf29a6b":"code","c542d276":"code","03a8e46b":"code","3bb7b208":"code","15617528":"code","f693e223":"code","5b3c4ef0":"code","f6e3e6f6":"code","cd8c2d4f":"code","f937a253":"code","96c8ec57":"code","1acd417c":"code","038ea8b1":"code","a255b1f8":"code","e7d04cf6":"code","c3748bad":"code","e7473fca":"code","b3730337":"code","33589a55":"code","684f05a3":"code","d55dcfae":"code","a03c0a4b":"code","c1438ed8":"code","2d9ff81f":"code","ecb0bbd0":"code","85dce5b1":"code","03bda5b9":"code","94390cab":"code","91dab131":"code","ebc419a6":"code","a622e886":"code","b52aab44":"code","f76e8862":"code","b2c96c6f":"code","1cc1ad7f":"code","81632a1e":"code","a9da9623":"code","4d6a3952":"code","a3a26bf8":"code","f62b4c36":"code","0b763dcd":"code","eede797b":"code","068f1e6b":"code","ce5b2d08":"code","f0da90e0":"code","0b2c0c2d":"code","5223b9b2":"code","df4e4ff4":"code","d1502209":"code","38b82514":"code","0ef758e8":"code","c2af83b6":"code","714f9af3":"code","edb6d9c8":"code","52703317":"code","5bcaf2a5":"code","784ab12c":"code","2a4638ad":"code","ffc6db98":"code","221abc90":"code","45493064":"code","2b899536":"code","f6ba1fb8":"code","6c3d7ac5":"code","8a10a77a":"code","84793a5f":"code","f5bf94c5":"markdown","a688fba8":"markdown","72535b52":"markdown","26092812":"markdown","28ef2621":"markdown","179f87e5":"markdown","6c86ec71":"markdown","c5a92a4d":"markdown","73e2b418":"markdown","2311e78e":"markdown","574950b8":"markdown","2d76d566":"markdown","76b15396":"markdown","8934e8ef":"markdown","1d73c4a3":"markdown","65408f9b":"markdown","8e7217dd":"markdown","d9f55ae2":"markdown","70c89df2":"markdown","381e3959":"markdown","eb0033b7":"markdown","6f158023":"markdown","104a77db":"markdown","ff20ea96":"markdown","4603abcf":"markdown","11fe9250":"markdown","c8e71575":"markdown","46e7e40b":"markdown","a089ca7b":"markdown","21d87696":"markdown","c6ba1fe4":"markdown","88fc8c95":"markdown","f847cd0d":"markdown","0e8db93a":"markdown","10f76739":"markdown","05177d29":"markdown","74e2134e":"markdown","f988c88e":"markdown","4096a84a":"markdown","f3d3d5fe":"markdown","e4e316c8":"markdown","8bc6869d":"markdown","e08ea8c9":"markdown","3823a0bf":"markdown","9b23aef6":"markdown","009ee88d":"markdown","6e791a0b":"markdown","77d3d279":"markdown","62e7f3a7":"markdown","f26d4372":"markdown","4eaa15d8":"markdown","960484dd":"markdown","f8e286aa":"markdown","b403a4cd":"markdown","7c73e396":"markdown","e1965c64":"markdown","f96fe431":"markdown","5a89bc57":"markdown","919b56a1":"markdown","6b1be8e1":"markdown","00fa69db":"markdown","abb6a805":"markdown","30baabbb":"markdown","2f0922e7":"markdown"},"source":{"2c7f83ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84d06779":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom textblob import TextBlob, Word\nimport collections\nimport re\nimport string\nimport emoji\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import StackingClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","c0945a34":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\ntrain_df.head()","c6f2ad37":"train_df.shape","233eefb4":"train_df[\"target\"].value_counts()","fc108250":"train_df[\"target\"].value_counts(normalize=True)","9ae419df":"countplot = sns.countplot(x=\"target\", data=train_df, palette=\"Set1\")\ncountplot.set_title(\"Real disaster tweets count\")","2629e188":"my_labels=[\"Non-Disaster\", \"Disaster\"]\nplt.pie(train_df['target'].value_counts(), labels=my_labels, colors = [\"Blue\",\"Red\"])\nplt.legend()\nplt.show()","899eee9c":"train_df.drop(columns=['id','keyword','location'], axis=1, inplace=True)\ntest_df.drop(columns=['keyword','location'], axis=1, inplace=True)","0775c1f6":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train_df[train_df.target == 1].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","25a84334":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train_df[train_df.target == 0].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","eb6ce8bf":"stop_words = set(stopwords.words('english'))\ncontraction_map = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n}\n\nall_punctuation = set(string.punctuation)\nall_punctuation.add(\"...\")\nall_punctuation.add('\u2019')\nall_punctuation.add('-')\nall_punctuation.add('\u201c')\nall_punctuation.add('[')\nall_punctuation.add(']')\nall_punctuation.add(' ')","cbf29a6b":"def uncapitalize(text):\n    return text.lower()","c542d276":"def removeEmojis(text):\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    return clean_text","03a8e46b":"def expand_abbr(article):\n    new_article = article\n    for item in contraction_map:\n        if item in article:\n            new_article = article.replace(item,contraction_map[item])\n    return new_article","3bb7b208":"def strip_links(text):\n    link_regex    = re.compile('((https?):((\/\/)|(\\\\\\\\))+([\\w\\d:#@%\/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], ', ')    \n    return text","15617528":"def strip_all_entities(text):\n    entity_prefixes = ['@','#']\n    for separator in  string.punctuation:\n        if separator not in entity_prefixes :\n            text = text.replace(separator,' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if word[0] not in entity_prefixes:\n                words.append(word)\n    return ' '.join(words)","f693e223":"def lemmatize_with_postag(text):\n    sent = TextBlob(text)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    return \" \".join(lemmatized_list)","5b3c4ef0":"def remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return filtered_sentence","f6e3e6f6":"def remove_punctuation(token_list):\n    new_list = []\n    for tok in token_list:\n        if tok not in all_punctuation:\n            new_list.append(tok)\n    final_list = [x for x in new_list if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n    final_sentence = \" \".join(final_list)\n    return final_sentence","cd8c2d4f":"def clean_text(text):\n    text = uncapitalize(text)\n    text = removeEmojis(text)\n    text = expand_abbr(text)\n    text = strip_links(text)\n    text = strip_all_entities(text)\n    text = lemmatize_with_postag(text)\n    cleaned_tokens = remove_stopwords(text)\n    final_text = remove_punctuation(cleaned_tokens)\n    return final_text\nprocessed_train_df = train_df.copy(deep=True)\nprocessed_train_df[\"text\"] = processed_train_df.text.apply(clean_text)","f937a253":"processed_train_df.head()","96c8ec57":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500, width = 1000, height = 500).generate(\" \".join(processed_train_df[processed_train_df.target == 1].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","1acd417c":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500, width = 1000, height = 500).generate(\" \".join(processed_train_df[processed_train_df.target == 0].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","038ea8b1":"def extract_ngrams(text, num):\n    n_grams = ngrams(nltk.word_tokenize(text), num)\n    return [' '.join(grams) for grams in n_grams]","a255b1f8":"disaster_text = \" \".join(processed_train_df[processed_train_df.target == 1].text)","e7d04cf6":"real_one_gram = extract_ngrams(disaster_text, 1)\nreal_one_gram_freq = collections.Counter(real_one_gram)\nreal_one_gram_freq.most_common(15)","c3748bad":"freq_list = real_one_gram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"unigram of real disaster tweets\")","e7473fca":"real_bigram = extract_ngrams(disaster_text, 2)\nreal_bigram_freq = collections.Counter(real_bigram)\nreal_bigram_freq.most_common(15)","b3730337":"freq_list = real_bigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"bigram of real disaster tweets\")","33589a55":"real_trigram = extract_ngrams(disaster_text, 3)\nreal_trigram_freq = collections.Counter(real_trigram)\nreal_trigram_freq.most_common(15)","684f05a3":"freq_list = real_trigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"trigram of real disaster tweets\")","d55dcfae":"non_disaster_text = \" \".join(processed_train_df[processed_train_df.target == 0].text)","a03c0a4b":"non_disaster_one_gram = extract_ngrams(non_disaster_text, 1)\nnon_disaster_one_gram_freq = collections.Counter(non_disaster_one_gram)\nnon_disaster_one_gram_freq.most_common(15)","c1438ed8":"freq_list = non_disaster_one_gram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=45)\nax.set_ylabel(\"count\")\nax.set_title(\"unigram of non-disaster tweets\")","2d9ff81f":"non_disaster_bigram = extract_ngrams(non_disaster_text, 2)\nnon_disaster_bigram_freq = collections.Counter(non_disaster_bigram)\nnon_disaster_bigram_freq.most_common(15)","ecb0bbd0":"freq_list = non_disaster_bigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(18,1))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"bigram of non-disaster tweets\")","85dce5b1":"non_disaster_trigram = extract_ngrams(non_disaster_text, 3)\nnon_disaster_trigram_freq = collections.Counter(non_disaster_trigram)\nnon_disaster_trigram_freq.most_common(15)","03bda5b9":"freq_list = non_disaster_trigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"trigram of non-disaster tweets\")","94390cab":"embedding_df = processed_train_df.copy(deep=True)","91dab131":"word_embeddings = {}\nf = open('\/kaggle\/input\/glove6b\/glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs","ebc419a6":"def get_sentence_vectors(text):\n    sentence_vector = np.zeros((100,))\n    if len(text) == 0:\n        return sentence_vector\n    else:\n        tokens = text.split()\n        for token in tokens:\n            try:\n                sentence_vector += word_embeddings[token]\n            except:\n                pass\n        sentence_vector = sentence_vector\/len(tokens)\n        return sentence_vector","a622e886":"embedding_df[\"text\"] = embedding_df.text.apply(get_sentence_vectors)","b52aab44":"embedding_df.head()","f76e8862":"x_train, x_test, y_train, y_test = train_test_split(embedding_df[\"text\"],embedding_df[\"target\"],test_size=0.2)","b2c96c6f":"print(f\"**********Naive Bayes**********\")\nmodel =  GaussianNB()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","1cc1ad7f":"print(f\"**********Logistic Regression**********\")\nmodel = LogisticRegression()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","81632a1e":"print(f\"**********Random Forest**********\")\nmodel = RandomForestClassifier()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","a9da9623":"print(f\"**********AdaBoost**********\")\nmodel = AdaBoostClassifier()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","4d6a3952":"print(f\"**********XGBoost**********\")\nmodel = XGBClassifier()\nstart = time.time()\nmodel.fit(np.asarray(x_train.to_list()),y_train)\ny_pred = model.predict(np.asarray(x_test.to_list()))\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","a3a26bf8":"from sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nprint(f\"**********SVC**********\")\nmodel = SVC()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","f62b4c36":"print(\"**********Stacking**********\")\nstart = time.time()\nestimators = [(\"xgb\", XGBClassifier()), (\"SVC\",SVC()), (\"rfe\",RandomForestClassifier())]\nfinal_estimator = LinearSVC()\nstacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\nstacking_clf.fit(np.asarray(x_train.to_list()),y_train)\ny_pred = stacking_clf.predict(np.asarray(x_test.to_list()))\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","0b763dcd":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch,logs={}):\n    if(logs.get('accuracy')>0.90):\n      print(\"\\nReached 90% accuracy so cancelling training\")\n      self.model.stop_training=True","eede797b":"callbacks = myCallback()\nann_model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\nann_model.fit(np.asarray(x_train.to_list()), y_train, epochs=1000, validation_data=(np.asarray(x_test.to_list()), y_test), callbacks=[callbacks])\n","068f1e6b":"ann_model.evaluate(np.asarray(x_test.to_list()),y_test)","ce5b2d08":"processed_train_df.head(10)","f0da90e0":"def counter_word(text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","0b2c0c2d":"counter = counter_word(processed_train_df.text)","5223b9b2":"num_words = len(counter)\n#Max number of words in a sequence\nmax_length = 30","df4e4ff4":"x_train, x_test, y_train, y_test = train_test_split(processed_train_df[\"text\"],processed_train_df[\"target\"],test_size=0.3)","d1502209":"tokenizer = Tokenizer(num_words = num_words)\ntokenizer.fit_on_texts(x_train)","38b82514":"word_index = tokenizer.word_index","0ef758e8":"train_sequences = tokenizer.texts_to_sequences(x_train)\ntrain_sequences[:5]","c2af83b6":"train_padded = pad_sequences(\n    train_sequences, maxlen= max_length, padding='post', truncating = 'post'\n)","714f9af3":"test_sequences = tokenizer.texts_to_sequences(x_test)\ntest_sequences[:5]","edb6d9c8":"test_padded = pad_sequences(\n    test_sequences, maxlen= max_length, padding='post', truncating = 'post'\n)","52703317":"print(x_train.head(1))\nprint(train_sequences[0])","5bcaf2a5":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch,logs={}):\n    if(logs.get('accuracy')>=0.85):\n      print(\"\\nReached 85% accuracy so cancelling training\")\n      self.model.stop_training=True\ncallbacks = myCallback()","784ab12c":"lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(num_words, 32, input_length=max_length),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.LSTM(200),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nlstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])","2a4638ad":"history = lstm_model.fit(\n    train_padded,\n    y_train,\n    epochs=30,\n    verbose=1,\n    validation_data=(test_padded, y_test),\n    callbacks=[callbacks]\n)","ffc6db98":"lstm_model.evaluate(test_padded,y_test)","221abc90":"def clean_text(text):\n    text = uncapitalize(text)\n    text = removeEmojis(text)\n    text = expand_abbr(text)\n    text = strip_links(text)\n    text = strip_all_entities(text)\n    text = lemmatize_with_postag(text)\n    cleaned_tokens = remove_stopwords(text)\n    final_text = remove_punctuation(cleaned_tokens)\n    return final_text\n\nprocessed_test_df = test_df.copy(deep=True)\nprocessed_test_df[\"text\"] = processed_test_df.text.apply(clean_text)\n\nfinal_train_df = train_df.copy(deep=True)\nfinal_train_df[\"text\"] = final_train_df.text.apply(clean_text)","45493064":"test_embedding_df = processed_test_df.copy(deep=True)\ntest_embedding_df[\"text\"] = test_embedding_df.text.apply(get_sentence_vectors)\n\nfinal_train_embedding = final_train_df.copy(deep=True)\nfinal_train_embedding[\"text\"] = final_train_embedding.text.apply(get_sentence_vectors)","2b899536":"final_train_embedding.head()","f6ba1fb8":"estimators = [(\"xgb\", XGBClassifier()), (\"SVC\",SVC()), (\"rfe\",RandomForestClassifier())]\nfinal_estimator = LinearSVC()\nstacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\nstacking_clf.fit(np.asarray(final_train_embedding[\"text\"].to_list()),final_train_embedding[\"target\"])\npredictions = stacking_clf.predict(np.asarray(test_embedding_df[\"text\"].to_list()))","6c3d7ac5":"predictions_df = pd.Series(np.array(predictions).flatten()).to_frame()\nresult_df = pd.concat([test_df,predictions_df], axis = 1)\nresult_df.drop(columns=['text'], axis=1, inplace=True)\nresult_df = result_df.rename(columns={0: \"target\"})\nresult_df['target'] = result_df['target'].map(lambda a: int(a))","8a10a77a":"result_df.head()","84793a5f":"result_df.to_csv('result.csv',index=False)","f5bf94c5":"### 4.1.1 Wordcloud for real disaster tweets","a688fba8":"## 2.2 Wordclouds","72535b52":"## 11.2 Convert text to vectors","26092812":"## 11.3 Output predictions","28ef2621":"## 4.3 n-grams of non-disaster tweets","179f87e5":"### 4.3.4 Tri-grams","6c86ec71":"### 2.2.1 Wordcloud for real disaster tweets","c5a92a4d":"### 3.2.4 Remove website urls","73e2b418":"As we can see, there are many noisy words that should not be indicative of disaster. Let's clean this up","2311e78e":"## 10.3 Evaluate LSTM","574950b8":"## 11.1 Preprocess test dataset","2d76d566":"### 3.2.8 Remove punctuations","76b15396":"# 7. Machine learning models","8934e8ef":"## 10.2 Creating model","1d73c4a3":"# 11. Output","65408f9b":"# 9. Preperation for LSTM","8e7217dd":"## 7.4 Random Forest","d9f55ae2":"## 9.2 Create train test split","70c89df2":"### 2.2.2 Wordcloud for non-disaster tweets","381e3959":"# 10. Building LSTM model","eb0033b7":"### 3.2.3 Expand abbreviations","6f158023":"Looks good! Now let's take a look at more visualizations to easier understand our data","104a77db":"Let's create some functions so it'll be easier to implement them down the line","ff20ea96":"## 3.2 Helper functions","4603abcf":"## 7.1 Train test split","11fe9250":"### 3.2.5 Stripping all entities","c8e71575":"## 4.2 n-grams of real disaster tweets","46e7e40b":"### 3.2.7 Remove stopwords","a089ca7b":"### 4.3.3 Bi-grams","21d87696":"# 6. Converting sentences into vectors","c6ba1fe4":"## 2.1 Class distribution","88fc8c95":"## 8.2 Creating model","f847cd0d":"### 3.2.2 Remove emojis","0e8db93a":"### 4.2.2 Uni-grams (most common words)","10f76739":"# 1. Import relevant libraries","05177d29":"## 7.3 Logistic Regression","74e2134e":"## 7.2 Naive Bayes","f988c88e":"# 5. Importing word embeddings (GloVe)","4096a84a":"## 7.6 XGBoost","f3d3d5fe":"### 3.2.6 Lemmantization","e4e316c8":"## 9.3 Word index","8bc6869d":"## 4.1 Wordclouds ","e08ea8c9":"## 7.7 Support Vector Classifier","3823a0bf":"### 3.2.1 Lower-casing text","9b23aef6":"### 4.3.1 Preprocessing","009ee88d":"## 9.1 Create word frequency","6e791a0b":"## 7.5 AdaBoost","77d3d279":"# 4. Further visualizations","62e7f3a7":"### 4.2.3 Bi-grams","f26d4372":"### 4.2.4 Tri-grams","4eaa15d8":"## 8.1 Stop function","960484dd":"## 9.4 Create word sequence","f8e286aa":"## 10.1 Stop function","b403a4cd":"# 2. Exploratory Data Analysis","7c73e396":"### 4.2.1 Preprocessing","e1965c64":"## 7.8 Stacking","f96fe431":"# 3. Preprocessing","5a89bc57":"# 8. Artificial Neural Networks (ANNs)","919b56a1":"## 9.5 Text padding","6b1be8e1":"## 3.3 Cleaning text","00fa69db":"## 3.1 Defining variables","abb6a805":"## 8.3 Evaluate ANN","30baabbb":"### 4.1.2 Wordcloud for non-disaster tweets","2f0922e7":"### 4.3.2 Uni-grams (most common words)"}}