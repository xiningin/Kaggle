{"cell_type":{"dd62f261":"code","f6ac4622":"code","44143633":"code","f964b56d":"code","9a53e0ac":"code","26a111a0":"code","187263a5":"code","93da22fb":"code","fdada099":"code","442cda2f":"code","1176cfc0":"code","7d912607":"code","927d835d":"code","88bcabac":"code","0eeee5ea":"code","e237307c":"code","c50c219f":"code","9b11a5e3":"code","afa97e46":"code","436aef17":"code","f5c2fe70":"code","a30b4000":"code","1dbe4b2c":"markdown","5f54973c":"markdown","976a7f6b":"markdown","5058476e":"markdown"},"source":{"dd62f261":"import pandas as pd\nfrom PIL import Image\nimport torch\nfrom torch import nn, optim\nfrom torchvision.models import googlenet\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport os\nfrom skimage import io\nimport time","f6ac4622":"train_df = pd.read_csv('..\/input\/av-emergency\/train_SOaYf6m\/train.csv')","44143633":"test_df = pd.read_csv('..\/input\/av-emergency\/test_vc2kHdQ.csv')","f964b56d":"train_df.head()","9a53e0ac":"train_df.info()","26a111a0":"test_df.info()","187263a5":"train_df['emergency_or_not'].value_counts()","93da22fb":"model = googlenet(pretrained=True, transform_input=True)","fdada099":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","442cda2f":"model.fc","1176cfc0":"model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 100),\n                         nn.ReLU(),\n                         nn.Dropout(0.8),\n                         nn.Linear(100, 1),\n                        nn.Sigmoid())","7d912607":"model = model.to(device)\nprint(next(model.parameters()).is_cuda)","927d835d":"class LoadCustomData(Dataset):\n    def __init__(self, csv_file, root_path, transform=None):\n        self.annotation = pd.read_csv(csv_file)\n        self.root_path = root_path\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.annotation)\n    \n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_path, self.annotation.iloc[index, 0])\n        image = Image.open(img_path)\n        y_label = torch.tensor(int(self.annotation.iloc[index, 1]))\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return (image.float(), y_label.float())\n    \n    ","88bcabac":"# Transforms\ntransformations = T.Compose([T.Resize(225),\n                             T.RandomHorizontalFlip(0.2),\n                             T.RandomPerspective(),\n                             T.RandomRotation(12),\n                             T.ToTensor(),\n                             T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n                                     )","0eeee5ea":"# Loading train data\ntrain_data = LoadCustomData(csv_file='..\/input\/av-emergency\/train_SOaYf6m\/train.csv', root_path='..\/input\/av-emergency\/train_SOaYf6m\/images', transform=transformations)","e237307c":"train_set, valid_set = torch.utils.data.random_split(train_data, [1500, 146])","c50c219f":"train_loader = DataLoader(train_set, batch_size=64, shuffle=True)","9b11a5e3":"valid_loader = DataLoader(valid_set, batch_size=64, shuffle=True)","afa97e46":"def check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n        \n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions ==y).sum()\n            num_samples += predictions.size(0)\n        print(f'Got {num_correct\/num_samples} with accuracy {float(num_correct)\/float(num_samples) *100}')","436aef17":"optimizer = optim.Adam(model.parameters(), lr=0.00001)\ncriterion = nn.BCELoss()","f5c2fe70":"epochs = 10\nvalid_loss_min = np.Inf # track change in validation loss\n\n\nfor epoch in range(epochs):\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    print('No. of epochs : ', epoch+1)\n    \n    # Training the model\n    for batch_id, (images, label) in enumerate(train_loader):\n        # zero grad \n        optimizer.zero_grad()\n        # prediction of model\n        images = images.to(device)\n        label = label.to(device)\n        \n        output = model(images)\n        # loss \n        loss = criterion(output, label.view(-1,1))\n        # Backprop\n        loss.backward()\n        # updating weights\n        optimizer.step()\n        # Calculation training loss\n        train_loss += loss.item()\n    \n    # Validation model\n    for batch_id, (images, label) in enumerate(valid_loader):\n        model.eval()\n        # prediction of modelediction of model\n        images = images.to(device)\n        label = label.to(device)\n            \n            \n        output = model(images)\n        # Calculating loss\n        valid_loss_model = criterion(output, label.view(-1,1))\n        valid_loss += valid_loss_model.item()\n        \n    # Print training stats\n    train_loss = train_loss \/len(train_loader)\n    valid_loss = valid_loss \/ len(valid_loader)\n    \n    print('Train loss :', train_loss)\n    print('Valid loss :', valid_loss)\n\n    \n    # Save model if validation loss is less then train loss\n    \n    if valid_loss <= valid_loss_min:\n        print('\\nValidation loss decreases  (Saving model): ', valid_loss, 'Valid loss :', valid_loss_min)\n        torch.save(model.state_dict(), 'model.pt')\n        valid_loss_min = valid_loss\n        print('valid_loss_min update :', valid_loss_min)\n        print('------------------------------------------------')","a30b4000":"    check_accuracy(valid_loader, model)","1dbe4b2c":"# Importing Files","5f54973c":"# Model Training","976a7f6b":"# Loading Data","5058476e":"# Defining model"}}