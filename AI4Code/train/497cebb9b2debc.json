{"cell_type":{"7f9b5c9b":"code","9ee5396e":"code","0a75307a":"code","d3cb0d72":"code","39b26c50":"code","f31f71e3":"code","c135300e":"code","4b2db86c":"code","d5062997":"markdown","e59e9367":"markdown","49f048db":"markdown"},"source":{"7f9b5c9b":"import os\nimport json\nimport glob\nimport random\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\n\n\n\nTYPES = [\"FLAIR\", \"T1w\", \"T2w\", \"T1wCE\"]\nWHITE_THRESHOLD = 10 # out of 255\nEXCLUDE = [109, 123, 709]\n\n\ntrain_df = pd.read_csv(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv\")\ntest_df = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv')\ntrain_df = train_df[~train_df.BraTS21ID.isin(EXCLUDE)]\ndef load_dicom(path, size = 224):\n    ''' \n    Reads a DICOM image, standardizes so that the pixel values are between 0 and 1, then rescales to 0 and 255\n    \n    Note super sure if this kind of scaling is appropriate, but everyone seems to do it. \n    '''\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return cv2.resize(data, (size, size))\n\ndef get_all_image_paths(brats21id, image_type, folder='train'): \n    '''\n    Returns an arry of all the images of a particular type for a particular patient ID\n    '''\n    assert(image_type in TYPES)\n    \n    patient_path = os.path.join(\n        \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/%s\/\" % folder, \n        str(brats21id).zfill(5),\n    )\n\n    paths = sorted(\n        glob.glob(os.path.join(patient_path, image_type, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    \n    num_images = len(paths)\n    \n    start = int(num_images * 0.25)\n    end = int(num_images * 0.75)\n\n    interval = 3\n    \n    if num_images < 10: \n        interval = 1\n    \n    return np.array(paths[start:end:interval])\n\ndef get_all_images(brats21id, image_type, folder='train', size=225):\n    return [load_dicom(path, size) for path in get_all_image_paths(brats21id, image_type, folder)]\nIMAGE_SIZE = 128\n\ndef get_all_data_for_train(image_type):\n    global train_df\n    \n    X = []\n    y = []\n    train_ids = []\n\n    for i in tqdm(train_df.index):\n        x = train_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'train', IMAGE_SIZE)\n        label = x['MGMT_value']\n\n        X += images\n        y += [label] * len(images)\n        train_ids += [int(x['BraTS21ID'])] * len(images)\n        assert(len(X) == len(y))\n    return np.array(X), np.array(y), np.array(train_ids)\n\ndef get_all_data_for_test(image_type):\n    global test_df\n    \n    X = []\n    test_ids = []\n\n    for i in tqdm(test_df.index):\n        x = test_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'test', IMAGE_SIZE)\n        X += images\n        test_ids += [int(x['BraTS21ID'])] * len(images)\n\n    return np.array(X), np.array(test_ids)","9ee5396e":"# X_train, X_valid, y_train, y_valid, trainidt_train, trainidt_valid = train_test_split(X, y, trainidt, test_size=0.2, random_state=40)\n\n# split = int(X.shape[0] * 0.8)\n\n# # X_train = X[:split]\n# # X_valid = X[split:]\n\n# # y_train = y[:split]\n# # y_valid = y[split:]\n\n# # trainidt_train = trainidt[:split]\n# # trainidt_valid = trainidt[split:]\n\n# X_train = tf.expand_dims(X_train, axis=-1)\n# X_valid = tf.expand_dims(X_valid, axis=-1)\n\n# y_train = to_categorical(y_train)\n# y_valid = to_categorical(y_valid)\n\n# X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, trainidt_train.shape, trainidt_valid.shape","0a75307a":"# global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n# data_augmentation = tf.keras.Sequential([\n#   tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n#   tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n# ])","d3cb0d72":"# np.random.seed(0)\n# random.seed(12)\n# tf.random.set_seed(12)\n\n# inpt = keras.Input(shape=(128, 128, 1))\n\n# h = keras.layers.experimental.preprocessing.Rescaling(1.\/255)(inpt)\n# # h = data_augmentation(h)\n\n# # convolutional layer!\n# h = keras.layers.Conv2D(32, kernel_size=(3, 3),strides=(1,1), activation=\"relu\", name=\"Conv_1\", padding=\"valid\")(h) \n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# h = keras.layers.Conv2D(64, kernel_size=(3, 3),strides=(1,1), activation=\"relu\", name=\"Conv_2\", padding=\"same\")(h) \n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# # pooling layer\n# h = keras.layers.MaxPool2D(pool_size=(2,2))(h) \n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# # convolutional layer!\n# h = keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", name=\"Conv_3\",padding =\"same\")(h)\n# # h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# # pooling layer\n# # h = keras.layers.MaxPool2D(pool_size=(1,1))(h)\n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# h = keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", name=\"Conv_4\",padding =\"valid\")(h)\n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# h = keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", name=\"Conv_5\",padding =\"same\")(h)\n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# h = keras.layers.MaxPool2D(pool_size=(2,2))(h)\n# h = tf.keras.layers.BatchNormalization(axis=-1)(h)\n# h = keras.layers.Dropout(0.3)(h)   \n\n# h = keras.layers.Flatten()(h) \n# # h = global_average_layer(h)\n# h = keras.layers.Dense(128, activation='relu')(h)   \n\n# output = keras.layers.Dense(2, activation=\"softmax\")(h)\n\n# model = keras.Model(inpt, output)\n\n# from tensorflow.keras.optimizers import SGD\n# # opt = SGD(lr=0.1)\n\n# checkpoint_filepath = 'best_model.h5'\n# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n# filepath=checkpoint_filepath,\n# save_weights_only=False,\n# monitor='val_auc',\n# mode='max',\n# save_best_only=True,\n# save_freq='epoch')\n\n# model.compile(loss='categorical_crossentropy',\n#              optimizer=tf.keras.optimizers.SGD(learning_rate =0.0001),\n#              metrics=[tf.keras.metrics.AUC()])\n\n# history = model.fit(x=X_train, y = y_train, epochs=50, callbacks=[model_checkpoint_callback], validation_data= (X_valid, y_valid))","39b26c50":"X_test, testidt = get_all_data_for_test('T1wCE')","f31f71e3":"file_path = '..\/input\/2dcnn\/best_model.h5'","c135300e":"model_best = tf.keras.models.load_model(filepath=file_path)","4b2db86c":"sample = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv')\n\ny_pred = model_best.predict(X_test)\n\npred = np.argmax(y_pred, axis=1)\n\nresult=pd.DataFrame(testidt)\nresult[1]=pred\n\nresult.columns=['BraTS21ID','MGMT_value']\nresult2 = result.groupby('BraTS21ID',as_index=False).mean()\nresult2['BraTS21ID'] = sample['BraTS21ID']\n# result2['MGMT_value'] = result2['MGMT_value'].apply(lambda x:round(x*10)\/10)\nresult2.to_csv('submission.csv',index=False)\nresult2","d5062997":"# It omits training.\n# Please run the commented out part.","e59e9367":"# Prediction","49f048db":"# training"}}