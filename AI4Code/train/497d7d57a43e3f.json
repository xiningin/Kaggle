{"cell_type":{"15b4272c":"code","6bad4d2d":"code","2dc56a4d":"code","c6c0730f":"code","dc1d35b9":"code","7353764f":"code","63f86288":"code","cd40b77b":"code","f0b1490c":"code","8d8f1e64":"code","cc07f3d4":"code","5129ae9f":"code","b192bc51":"code","d54fd2bd":"code","a2b4352f":"code","941186bf":"code","0340b5f3":"code","27383e6e":"code","8e4c18bd":"code","915dfa40":"code","8de0bbd7":"code","dc4e9f5d":"code","f548ec7d":"code","c75b8c80":"code","9bd65b82":"code","28a7a0d9":"code","2d2b3446":"code","489ab21e":"code","78627f4d":"code","8a15f512":"code","2e605693":"code","205841fb":"code","fbfbace9":"code","29c41718":"code","43d17046":"code","fa6a4ba3":"code","f7f91935":"code","d832392a":"code","74c86238":"code","135f340f":"code","b76b1c6b":"code","89a27c08":"code","e6cbfebf":"markdown","b508833a":"markdown","bb87d7ab":"markdown","766eadc6":"markdown","f869f894":"markdown","1b386577":"markdown","b9608ed6":"markdown","946f6219":"markdown","2575f4da":"markdown","35ca8aa0":"markdown","f1052b86":"markdown","ad7da235":"markdown","77cc9351":"markdown","37b5401c":"markdown","a7db4b17":"markdown","d99dd1f4":"markdown","05ab3d91":"markdown","b3086678":"markdown","93e57b86":"markdown","28306f4e":"markdown","c3e877d0":"markdown","9964e0cd":"markdown","7fc3de65":"markdown","796c0b84":"markdown","7c536663":"markdown","b793b8de":"markdown","6d078fcf":"markdown","f1a27a6f":"markdown","ba948719":"markdown","d90ce628":"markdown","fc778084":"markdown","22e493be":"markdown","982dd246":"markdown","e5c54ce5":"markdown","ece75728":"markdown","1ff4e76a":"markdown","5715612e":"markdown","e99ec749":"markdown","33061ba9":"markdown","d6bb4f41":"markdown","d4b6f91b":"markdown","46fe9266":"markdown","727c68c2":"markdown","74903b4f":"markdown","b6b11845":"markdown","7334d4d7":"markdown","d3fefe95":"markdown","64c09745":"markdown","04296d39":"markdown","4cb232f8":"markdown","2b0b2bbc":"markdown","0143ee51":"markdown","7f2c1ee9":"markdown","b2b67443":"markdown","0a71067b":"markdown","1c475458":"markdown","35637c04":"markdown","a229fe8e":"markdown","af9ffdb0":"markdown","6fba99e0":"markdown","fd813b60":"markdown","5efe6e03":"markdown","f08a128c":"markdown","585a9a50":"markdown","fcd5f14c":"markdown","e755a0d3":"markdown","7b4c81e6":"markdown","38b8cc67":"markdown","57fe7743":"markdown"},"source":{"15b4272c":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nimport os\nfrom time import time\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style=\"darkgrid\")","6bad4d2d":"train = pd.read_csv('..\/input\/dt_funda_train_2018.csv')\ntest = pd.read_csv('..\/input\/dt_funda_test_2018.csv')","2dc56a4d":"print('train set contains {} rows and {} columns'.format(train.shape[0],train.shape[1]))\nprint('test set contains {} rows and {} columns'.format(test.shape[0],test.shape[1]))","c6c0730f":"train.head()","dc1d35b9":"train = train.drop(['flg_missing_n_photos','flg_missing_n_photos_360'],axis=1)\ntest = test.drop(['flg_missing_n_photos','flg_missing_n_photos_360'],axis=1)","7353764f":"print(train.columns)","63f86288":"ID_train = train['id']\ntrain = train.drop(['id'],axis=1)\nID_test = test['id']\ntest = test.drop(['id'],axis=1)","cd40b77b":"#Transform categorical variables type to 'object'\ndef to_categorical(df,feats):\n    \n    for col in feats:\n        if df[col].dtype!='object':\n            df[col] = df[col].astype('object')\n            \n    return df\n\n#the categorical features\ncat_var = ['zipcode','type_of_construction','energy_label','located_on','own_ground']            \ntrain = to_categorical(train,cat_var)","f0b1490c":"#number of flags columns\nn_flag = 12\n\n#flag features\nflag_var = train.iloc[:,-n_flag:].columns\n\n#numerical features\nnum_var = train.iloc[:,0:-n_flag].drop('log_price',axis=1).select_dtypes(exclude='object').columns\n\n#Just in case I need it later\nall_var = train.iloc[:,0:-n_flag].drop('log_price',axis=1).columns","8d8f1e64":"#Some categories were mispelled\ncorrection_dict = { 'A ':'A',\n                    'A':'A',\n                    'A+':'A',\n                    'B ':'B',\n                    'B':'B',\n                    'C ':'C',\n                    'C':'C',\n                    'D ':'D',\n                    'D':'D',\n                    'E':'E',\n                    'E ':'E',\n                    'F ':'F',\n                    'F':'F',\n                    'G ':'G',\n                    'G':'G',\n                    'n':'N'}\n\ntrain['energy_label'] = train['energy_label'].map(correction_dict)\ntest['energy_label'] = test['energy_label'].map(correction_dict)\n#I also transform this for avoiding problems when plotting\ntrain['type_of_construction'] = train['type_of_construction'].map({'Bestaande bouw':'-1','Nieuwbouw':'1','0':'0'})\ntest['type_of_construction'] = test['type_of_construction'].map({'Bestaande bouw':'-1','Nieuwbouw':'1','0':'0'})","cc07f3d4":"plt.figure(figsize=(8,10))\nsns.heatmap(train[flag_var])\n#plt.xticks(rotation=45)","5129ae9f":"n_row = train.shape[0]\nnan_count = train[train[flag_var]==1][flag_var].sum()\nnan_percentage = round(nan_count\/n_row*100).sort_values(ascending=False)\nnan_percentage.plot.bar()","b192bc51":"var_to_plot = 'log_price'\n\nsns.distplot(train[var_to_plot],bins=30,fit=norm)\nplt.xlabel(var_to_plot)\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train[var_to_plot].skew()))\nprint(\"Kurtosis: {}\".format(train[var_to_plot].kurt()))","d54fd2bd":"sns.set(style=\"darkgrid\")\nfig = plt.figure(figsize=(20,20))\nfig.subplots_adjust(hspace=0.4, wspace=0.3)\n\nfor j,var in enumerate(num_var):\n    fig.add_subplot(4,3,j+1)\n    #Drop missing values\n    if 'flg_missing_'+var in flag_var:\n        sns.distplot(train[train['flg_missing_'+var]==0][var],bins=20)   \n    else:\n        sns.distplot(train[var],bins=20)\n    plt.xlabel(var)","a2b4352f":"fig = plt.figure(figsize=(20,20))\nfig.subplots_adjust(hspace=0.4, wspace=0.3)\n\nfor j,var in enumerate(cat_var):\n    fig.add_subplot(3,2,j+1)\n    #Drop missing values\n    if 'flg_missing_'+var in flag_var:\n        sns.countplot(train[train['flg_missing_'+var]==0][var],color='lightsteelblue')   \n    else:\n        sns.countplot(train[var],color='lightsteelblue')\n    plt.xlabel(var)\n    plt.xticks(rotation=60)","941186bf":"fig = plt.figure(figsize=(20,20))\nfig.subplots_adjust(hspace=0.4, wspace=0.3)\n\nfor j,var in enumerate(num_var):\n    fig.add_subplot(5,4,j+1)\n    #Drop missing values\n    if 'flg_missing_'+var in flag_var:\n        sns.scatterplot(train[train['flg_missing_'+var]==0][var],\n                    train[train['flg_missing_'+var]==0]['log_price']) \n    else:\n        sns.scatterplot(train[var],train['log_price']) \n    plt.xlabel(var)","0340b5f3":"fig = plt.figure(figsize=(20,20))\nfig.subplots_adjust(hspace=0.4, wspace=0.3)\n\nfor j,var in enumerate(cat_var):\n    fig.add_subplot(3,2,j+1)\n    #Drop missing values\n    if 'flg_missing_'+var in flag_var:\n        sns.boxplot(train[train['flg_missing_'+var]==0][var],\n                    train[train['flg_missing_'+var]==0]['log_price']) \n    else:\n        sns.boxplot(train[var],train['log_price']) \n    plt.xlabel(var)","27383e6e":"#standarizing data\nfrom sklearn.preprocessing import StandardScaler\nlog_price_scaled = StandardScaler().fit_transform(np.array(train['log_price']).reshape(-1, 1))\noutliers = []","8e4c18bd":"threshold = 4.5\ng = sns.distplot(log_price_scaled,bins=30)\nplt.axvline(x=threshold,c='r')\nplt.axvline(x=-threshold,c='r')\nplt.xlabel('log_price_scaled')\n#indexes of outliers\nindex = np.where(np.absolute(log_price_scaled) > threshold)[0]\noutliers += list(index)","915dfa40":"threshold = 4.5\nn_photos_scaled = StandardScaler().fit_transform(np.array(train['living_area']).reshape(-1, 1))\ng = sns.distplot(n_photos_scaled,bins=30)\nplt.axvline(x=threshold,c='r')\nplt.axvline(x=-threshold,c='r')\nplt.xlabel('living_area_scaled')\n#indexes of outliers\nindex = np.where(np.absolute(n_photos_scaled) > threshold)[0]\noutliers += list(index)","8de0bbd7":"index = train[train['flg_missing_year']==0][train['year']<1700].index\noutliers += list(index)","dc4e9f5d":"fig = plt.figure(figsize=(20,8))\nfig.subplots_adjust(hspace=0.4, wspace=0.3)\nfig.add_subplot(1,2,1)\nplt.scatter(train[train['flg_missing_'+'year']==0]['year'], \n            train[train['flg_missing_'+'year']==0]['log_price'])\nplt.axvline(x=1725,c='r')\n\nfig.add_subplot(1,2,2)\nplt.scatter(train[train['flg_missing_'+'year']==0]['year'].drop(index), \n            train[train['flg_missing_'+'year']==0]['log_price'].drop(index))","f548ec7d":"anomalies = train[train['other_indoor_space']>train['living_area']].index\nanomalies = list(anomalies)","c75b8c80":"fig = plt.figure(figsize=(16,4))\nfig.subplots_adjust(hspace=0.4)\n\nfig.add_subplot(1,2,1)\nsns.scatterplot(train[train['flg_missing_'+'other_indoor_space']==0]['living_area'],\n                train[train['flg_missing_'+'other_indoor_space']==0]['other_indoor_space']) \nsns.scatterplot(train['living_area'].iloc[anomalies],train['other_indoor_space'].iloc[anomalies])\n\nnew_train = train.drop(anomalies)\nfig.add_subplot(1,2,2)\nsns.scatterplot(new_train[train['flg_missing_'+'other_indoor_space']==0]['living_area'],\n                new_train[train['flg_missing_'+'other_indoor_space']==0]['other_indoor_space']) ","9bd65b82":"fig = plt.figure(figsize=(16,4))\nfig.subplots_adjust(hspace=0.4)\n\nfig.add_subplot(1,2,1)\nsns.scatterplot(train[train['flg_missing_'+'other_area']==0]['living_area'],\n                train[train['flg_missing_'+'other_area']==0]['other_area'])\n\nindex = train[train['other_area']>train['living_area']].index\nsns.scatterplot(train['living_area'].iloc[index],train['other_area'].iloc[index])\nanomalies += list(index)\n\nnew_train = train.drop(index)\nfig.add_subplot(1,2,2)\nsns.scatterplot(new_train[train['flg_missing_'+'other_area']==0]['living_area'],\n                new_train[train['flg_missing_'+'other_area']==0]['other_area']) ","28a7a0d9":"anomalies = set(anomalies)\ntrain = train.drop(anomalies)","2d2b3446":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","489ab21e":"y = np.array(train['log_price'])\n\ndef benchmark(y,n_folds=10,n_iters=5):\n    \n    kf = KFold(n_splits=n_folds)\n    cv_scores = []\n    \n    for iter in range(n_iters):\n        for train_index, test_index in kf.split(y):\n\n            n_train = test_index.shape[0]\n            y_pred = np.zeros(n_train) + y[train_index].mean()\n            y_true = y[test_index]\n            cv_scores.append(mean_squared_error(y_true,y_pred))\n            \n    return np.sqrt(cv_scores)\n\nmean_baseline = benchmark(y)\nprint('Benchmark:')\nprint('RMSE score: {}'.format(round(np.mean(mean_baseline),2)))\nprint('Std : {}'.format(np.std(mean_baseline)))","78627f4d":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Imputer\nwarnings.filterwarnings('ignore',category=DeprecationWarning)","8a15f512":"estimator = LinearRegression()\nn_splits = 10\nn_iters = 5\nlr_baseline_train = []\nlr_baseline_test = []\n\nX = train[['living_area','flg_missing_living_area']]\ny = train['log_price']\n\nfor iter in range(n_iters):\n\n    cv = KFold(n_splits=n_splits,shuffle=True)\n    cv_iter = list(cv.split(X)) \n\n    #cross validation\n    for train_index, test_index in cv_iter:\n\n        X_train, X_val = (X.iloc[train_index,:], X.iloc[test_index,:])\n        y_train, y_val = (y.iloc[train_index], y.iloc[test_index])\n\n        #Calculate the average from train set and impute on train and validation set\n        imputer = Imputer(missing_values=0)\n        X_train['living_area'] = imputer.fit_transform(np.array(X_train['living_area']).reshape(-1, 1))\n        X_val['living_area'] = imputer.transform(np.array(X_val['living_area']).reshape(-1, 1))\n\n        #Calculate the scale from train set and impute on train and validation set\n        scaler = RobustScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_val = scaler.transform(X_val)\n\n        #fit the estimator\n        estimator.fit(X_train,y_train)\n\n        #predictions\n        y_pred = estimator.predict(X_train)\n        lr_baseline_train.append(mean_squared_error(y_train,y_pred))\n        y_pred = estimator.predict(X_val)\n        lr_baseline_test.append(mean_squared_error(y_val,y_pred))\n            \nlr_baseline_test = np.sqrt(lr_baseline_test)\nlr_baseline_train = np.sqrt(lr_baseline_train)\n    \nprint('mean train score: {}'.format(np.mean(lr_baseline_train)))\nprint('std test score: {}'.format(np.std(lr_baseline_train)))\nprint('--'*20)\nprint('mean test score: {}'.format(np.mean(lr_baseline_test)))\nprint('std test score: {}'.format(np.std(lr_baseline_test)))","2e605693":"#Let\u00b4s concatenate the data sets\nall_data = pd.concat([train.drop('log_price',axis=1),test],axis=0)\n#save the target in other variable\ny = train['log_price']","205841fb":"for var in all_var:\n    \n    if 'flg_missing_'+var in flag_var:\n        index = all_data[all_data['flg_missing_'+var]==1][var].index\n        all_data.loc[index,var] = -999\n        \nall_data = all_data.drop(flag_var,axis=1)\n#for debugging\nall_data.head()","fbfbace9":"nan_count = train[flag_var].sum(axis=1)\nall_data['nan_count'] = nan_count","29c41718":"all_data['n_photos'] = all_data['n_photos'].apply(lambda x: 1 if x > 0 else 0)\nall_data['n_photos_360'] = all_data['n_photos_360'].apply(lambda x: 1 if x > 0 else 0)","43d17046":"all_data['energy_label'] = all_data['energy_label'].map({ 'A':'1',\n                                                        'B':'2',\n                                                        'C':'3',\n                                                        'D':'4',\n                                                        'E':'5',\n                                                        'F':'6',\n                                                        'G':'7',\n                                                        'N':'8'})","fa6a4ba3":"n_train = train.shape[0]\nrf_train = all_data.iloc[:n_train,:]\ntest = all_data.iloc[n_train:,:]","f7f91935":"#levels with less than threshold observations go into the new category level\nthreshold = 30\ncount = train.groupby('located_on')['located_on'].count()\nall_data['located_on'] = all_data['located_on'].apply(lambda x: '9' if x in count[count<threshold].index else x)","d832392a":"def mean_target_encoding(x_tr, x_val,y,cols):\n    '''\n    Calculate mean target encoding \n    \n    Arguments:\n    ------------\n    x_tr : Pandas.DataFrame\n        dataframe to calculate encoding\n    x_val : Pandas.DataFrame\n        dataframe to map encoding\n    y : Pandas.Serie\n        the target\n    cols : list\n        cols to calculate encoding on\n    \n    Return:\n    ------------\n    x_tr : Pandas.DataFrame\n        encoded dataframe\n    x_val : Pandas.DataFrame\n        encoded dataframe\n\n    '''\n    alpha = 2\n    temp = pd.concat([x_tr,y],axis=1)\n    global_mean = y.mean()\n    \n    for col in cols:\n        \n        means = temp.groupby(col)[y.name].mean()\n        #print(means)\n        n_rows = temp.groupby(col)[col].count()\n        #print(n_rows)\n        encode = (means*n_rows + global_mean*alpha)\/(n_rows+alpha)\n        #print(encode)\n        x_tr[col+'_mean_target'] = x_tr[col].map(encode)\n        x_val[col+'_mean_target'] = x_val[col].map(encode)\n        x_val[col+'_mean_target'].fillna(global_mean,inplace=True)\n        \n    return x_tr, x_val","74c86238":"def freq_encoding(serie_tr, serie_val):\n    '''\n    Calculate the frequency encoding and map the inputs.\n    \n    Arguments:\n    ------------\n    serie_tr : Pandas.Series\n         serie to calculate the encoding on.\n    serie_val : Pandas.Series\n         serie to map the encoding on.\n    \n    Return:\n    ------------\n    serie_tr : Pandas.Series\n        encoded serie\n    serie_val : Pandas.Series\n        encoded serie\n\n    '''\n    \n    n_row = serie_tr.shape[0]\n    freq_dict = {}\n    categories = serie_tr.unique()\n    \n    for category in categories:\n        \n        category_freq = serie_tr[serie_tr==category].count()\/n_row*100\n        freq_dict[category] = category_freq\n    \n    global_mean = np.mean([value for value in freq_dict.values()])\n    #print([value for value in freq_dict.values()])\n    #print(np.mean([value for value in freq_dict.values()]))\n    \n    serie_tr = serie_tr.map(freq_dict)\n    serie_val = serie_val.map(freq_dict)\n    serie_val = serie_val.fillna(global_mean)\n    \n    return serie_tr,serie_val","135f340f":"from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import KFold,cross_val_score,validation_curve,StratifiedKFold","b76b1c6b":"def rmse_cv(estimator,X,y,n_splits,n_iter,mean_target=True,cols_to_encode=[]):\n    '''\n    Evaluate a score by cross-validation.\n    Arguments:\n    ------------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n    X : pandas.DataFrame\n        The data to fit.\n    y : pandas.Series\n        The target variable to try to predict.\n    n_splits : int\n        Number of folds for cross-validation\n    n_iter : int \n        Times to repeat validation.\n    mean_target : bool.    \n    cols_to_encode : list. \n        Columns to encode.\n        \n     Return:\n    ------------- \n    train_scores : list. \n                    Training scores.\n    test_scores : list. \n                    Validation scores.\n    '''\n    train_scores = []\n    test_scores = []\n\n    for i in range(n_iter):\n        \n        cv = KFold(n_splits=n_splits,shuffle=True)\n        cv_iter = list(cv.split(X, y)) \n\n        for train_index, test_index in cv_iter:\n\n            X_train, X_val = (X.iloc[train_index,:], X.iloc[test_index,:])\n            y_train, y_val = (y.iloc[train_index], y.iloc[test_index]) \n\n            if mean_target==True:\n                #Calculate the average of target from train folds and impute on train and validation folds\n                X_train, X_val = mean_target_encoding(X_train,X_val,y_train,cols_to_encode)          \n                #fit the estimator\n            estimator.fit(X_train.drop(cols_to_encode,axis=1).values,y_train.values)\n            #predicts on train folds\n            y_pred = estimator.predict(X_train.drop(cols_to_encode,axis=1).values)\n            train_scores.append(mean_squared_error(y_train,y_pred))\n            #predicts on validation fold\n            y_pred = estimator.predict(X_val.drop(cols_to_encode,axis=1).values)\n            test_scores.append(mean_squared_error(y_val,y_pred))\n            \n        #for debugging\n        #print(X_train.drop(cols_to_encode,axis=1).head())\n\n    test_scores = np.sqrt(test_scores)\n    train_scores = np.sqrt(train_scores)\n    return train_scores, test_scores","89a27c08":"var_to_drop = ['total_area','other_indoor_space','other_area','n_weeks_old','own_ground']\n\nX  = rf_train.drop(var_to_drop,axis=1)\n\n\nrf_model = RandomForestRegressor(200,\n                                 max_depth=8,\n                                 max_features=6,\n                                 verbose=0)\n\nrf_model_train, rf_model_test = rmse_cv(rf_model,X,y,5,10,True,['zipcode'])\n\n\nprint('mean test score: {}'.format(np.mean(rf_model_test)))\nprint('std test score: {}'.format(np.std(rf_model_test)))\nprint('--'*20)","e6cbfebf":"#### 2.3.1. Univariate Detection<a class=\"anchor\" id=\"7-bullet\"><\/a>","b508833a":"### 2.2. Univariate Analysis<a class=\"anchor\" id=\"4-bullet\"><\/a>","bb87d7ab":"With linear regression data must be scaled. I\u00b4ll use RobustScaler class to avoid outlier impact and Imputer class to fill missing values:","766eadc6":"**And the distribution for categorical variables?**","f869f894":"### 2.1. Missing data<a class=\"anchor\" id=\"3-bullet\"><\/a>\nLet\u00b4s visualize the missing value pattern:","1b386577":"Most features present positive skewness like **year**, **total_rooms**, **bedrooms**, **living_area**, **other_area**, **total_area**, **monthly_contrib** or **other_indoor_space**. Therefore, they present extreme values which should be studied to know the real impact they have in the regression.","b9608ed6":"For linear regression the score is RMSE = 0.41.","946f6219":"It seems **year** has a non linear relationship. The oldest and the newest houses looks more expensive that those in the middle.","2575f4da":"These are the boxplots between categorical features and target:","35ca8aa0":"**What can be learnt from these plots?**<br><br>\n-**zipcode** and **located_on** are two features with high cardinality. Unfortunately, they have many categories with few observations which may be problematic when encoding.<br><br>\n-**Own_ground** and **type_of_construction** are binary features.<br><br>\n-**Energy_label** has 8 levels. The N level probably means 'unknow' energy label.","f1052b86":"# 1. Introduction<a class=\"anchor\" id=\"1-bullet\"><\/a>\nI\u00b4ve been given this dataset scrapped from a web page. My goal is to predict house prices given a set of features.","ad7da235":"**When may there be a missing value in this dataset?**<br>\nWhen the user of the web page didn\u00b4t fill some specific field.<br>\nSo I\u00b4m going to drop flags columns for n_photos and n_photos_360, as a missing here means directly zero photos, which is the value they have filled nans. ","77cc9351":"Before starting with EDA I want to correct a variable whose categories were written incorrectly:","37b5401c":"As there are so many missing values I\u00b4m going to create a feature which counts the missings per row:","a7db4b17":"**Which is the distribution of numerical variables?**\n\nMissing values have been dropped from these plots to have a real vision of the distribution of the values I truly know.","d99dd1f4":"These are useful modules I\u00b4ll use along the notebook:","05ab3d91":"Thanks for reading ;)","b3086678":"#### 2.3.1. Correlation Plots<a class=\"anchor\" id=\"17-bullet\"><\/a>","93e57b86":"### 2.3. Bivariate Analysis<a class=\"anchor\" id=\"5-bullet\"><\/a>","28306f4e":"## 2. EDA<a class=\"anchor\" id=\"2-bullet\"><\/a>","c3e877d0":"### 3.3. Tree-based model<a class=\"anchor\" id=\"14-bullet\"><\/a>","9964e0cd":"For **located_on** I was trying mean encoding and frequency encoding, but it didn\u00b4t improve the score. So I decided to join the less frequent classes in one new level.","7fc3de65":"## 3. Model<a class=\"anchor\" id=\"9-bullet\"><\/a>\n","796c0b84":"So my benchmark imputing the mean is a RMSE = 0.65","7c536663":"I drop ID since it is not useful for prediction:","b793b8de":"**What can be learnt from these plots?**","6d078fcf":"To check frequency encoding, I\u00b4ll use this function:","f1a27a6f":"I came across with data from house prices scrapped from a real state web page and I present my work here. I made an Exploratory Data Analysis to get insights about the data and tried a random forest model to predict the log_price.**I would like to get the review of the Kaggle Community to keep learning and be helpful for other beginners.***","ba948719":"I\u00b4ll do a **robust validation** of 10 times 5 or 10-fold cross validation whenever possible (computational limitations).","d90ce628":"I drop features with large percentage of missing values and tune parameters with for loops, by hand. First, I tune max_features parameter, then max_depth and n_estimators. I use 5-fold cross validation:","fc778084":"In many plots can be observed examples whose value is far from the rest of the distribution. They\u00b4ll be analyzed in following sections.","22e493be":"To sum up, we have 5 categorical variables, 11 numerical variables and 12 binary variables. These are the columns names:","982dd246":"Let\u00b4s start for target variable, log_price:","e5c54ce5":"First of all, I need some intuition about the data. I\u00b4m going to look at every column and  fill an Excel spreadsheet with the following fields:\n\n    Variable: Name of the variable\n    Type: Categorical or Numerical\n    Description: my first idea about the meaning of the variable\n    Comment: General comments\n\nIt seems most variable names are self-explanatory. Last ones are flag columns which indicate missing values in other columns. Furthermore, missing values have been imputed with zeroes. This is how the trainset looks like:\n","ece75728":"It seems logic that the area of other_indoor_space has to be smaller than the living_area:","1ff4e76a":"### 3.1. Feature Engineering<a class=\"anchor\" id=\"10-bullet\"><\/a>","5715612e":"### Notebook Content:\n\n* [1.Introduction](#1-bullet)<br>\n* [2. EDA](#2-bullet)<br>\n    * [2.1. Missing data](#3-bullet)\n    * [2.2. Univariate Analysis](#4-bullet)\n    * [2.3. Bivariate Analysis](#5-bullet)\n        * [2.4.1. Correlation plots](#17-bullet)\n        * [2.4.1. Group differences](#18-bullet)\n    * [2.4. Outliers](#6-bullet)\n        * [2.4.1. Univariate Detection](#7-bullet)\n        * [2.4.2. Bivariate Detection](#8-bullet)<br>\n* [3. Model](#9-bullet)<br>\n    *    [3.1. Benchmark](#10-bullet)\n        *    [3.2.1. Imputing mean](#12-bullet)\n        *    [3.2.1. Simple Linear Regression](#13-bullet)\n    *    [3.2. Feature Engineering](#11-bullet)\n    *    [3.3. Tree-based models](#14-bullet)","e99ec749":"#### 2.3.2. Group differences<a class=\"anchor\" id=\"18-bullet\"><\/a>","33061ba9":"I\u00b4m going to do a simple linear regression with the most correlated explanatory feature: 'living_area'. ","d6bb4f41":"For the target:","d4b6f91b":"The same for other_area feature:","46fe9266":"I need some benchmarks to compare my results with. What would be my RMSE if I always predict the mean?","727c68c2":"#### 2.3.2. Bivariate detection<a class=\"anchor\" id=\"8-bullet\"><\/a>","74903b4f":"**What can be learnt from these plots?**<br><br>","b6b11845":"#### 3.1.2. Simple Linear Regression<a class=\"anchor\" id=\"13-bullet\"><\/a>","7334d4d7":"The highest correlation can be easily identified as between **living_area** and target. Although, it is a bit affected by outliers.\nOther slight positive correlation are found in **other_contrib**, **total_rooms**, **bedrooms** and **other_area** which make sense.","d3fefe95":"I\u00b4ll analyze the distribution of every individual variable to get insights about my data.","64c09745":"I define some useful variables:","04296d39":"On the other hand, n_photos_360, n_photos, n_weeks_old and year presents lack of correlation.","4cb232f8":"There is a substancial difference beetween groups in **located_on** and **zipcode** features. Especially in **located_on** where observations in first levels are very different from last ones.\n\nA less pronounced one in **energy_label** and **type of construction**.\n\nIt seems there is no appreciable diferrence in **own_ground** feature.","2b0b2bbc":"Outliers will be selected as those cases falling at the outer ranges of the distribution given a threshold. I\u00b4m looking for desviations of 4.5 std. <br><br>\nI save outliers in a variable, just in case I want to keep them in train part but remove them from validation. (**Finally I didn\u00b4t have time for this, so I keep them in train and validation part**)","0143ee51":"Let\u00b4s load the data:","7f2c1ee9":"**How many rows and features do we have?**","b2b67443":"#### 3.1.1. Mean Imputing<a class=\"anchor\" id=\"12-bullet\"><\/a>","0a71067b":"There are some variables with most of its values missed: total_area,other_area and other_indoor_space. This plot will may be useful latter to find patterns with KMeans.\n","1c475458":"This is the function for mean target encoding. I add a smoothing term because there are some classes with few examples. ","35637c04":"For tree-based models work well to replace missing values by a low negative number, -999, and to eliminate flags columns.","a229fe8e":"As we have some categorical features with high cardinality I\u00b4m going to be focused on tree-based models. Other methods like linear models or KNN needs one-hot encoding and that increases the number of features which tends to overfitting.","af9ffdb0":"For eliminating noise from **n_photos** and **n_photos_360** features I\u00b4ll convert them into binary:","6fba99e0":"Lets change the type for categorical features:","fd813b60":"Let\u00b4s check if there are some **anomalies**. Values which have been entered incorrectly by the user.","5efe6e03":"I drop the anomalies from my train set:","f08a128c":"### 2.4. Outliers and Anomalies<a class=\"anchor\" id=\"6-bullet\"><\/a>","585a9a50":"It has some positive skewness, but I think the assumption of normality for regression can be accepted without more tranformations. It seems there are some extreme values that I\u00b4ll analyze later.","fcd5f14c":"These are the correlation plots between numerical features and target:","e755a0d3":"### 3.1. Benchmark<a class=\"anchor\" id=\"11-bullet\"><\/a>","7b4c81e6":"For validating my models I\u00b4m using a 5-cross validation because it is better for mean target encoding. This is the function I\u00b4ll use.","38b8cc67":"For my categorical feature **energy_label** I\u00b4m trying a Label Encoding as it has few categories and they have some kind of order:","57fe7743":"**What\u00b4s the percentage of missing values per feature?**"}}