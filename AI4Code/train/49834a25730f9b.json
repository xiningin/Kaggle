{"cell_type":{"eb6ead7b":"code","0e0de667":"code","9e217c2f":"code","18b188f5":"code","affe45f4":"code","e5a3a259":"code","fe59f3c9":"code","800c2f8d":"code","07885909":"code","7a486604":"code","55e9ba15":"code","46aaa540":"code","51e4d422":"code","59bc2ca9":"code","c0a929a7":"code","1b7b4dc2":"code","ed7a87bc":"code","60a0e2c2":"code","be44e306":"code","2035a5ad":"code","b9eb6773":"code","529377de":"code","08756678":"code","5e31e722":"code","776eee06":"code","2fc453b1":"code","938ed30d":"code","0bbd31c9":"code","5269d2cf":"code","7887f087":"code","868b12f6":"code","d4a17d4c":"code","8cbeca50":"code","4859421d":"code","1fbb6997":"code","e82940e7":"code","f6ce5b8b":"code","419772b3":"code","3d705128":"code","02a52277":"code","cf25417d":"code","350d305b":"code","d4cdd3f0":"code","2ea90b6c":"code","5ec7da6c":"code","ec0e54cc":"markdown","2072d50a":"markdown","50535973":"markdown","77efe4db":"markdown","9cdd55cc":"markdown","f952df78":"markdown","f862ff3e":"markdown","5e4becbb":"markdown","acc78671":"markdown","03fe017f":"markdown","a00e44b3":"markdown","70519727":"markdown"},"source":{"eb6ead7b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\n%matplotlib inline","0e0de667":"# Read Dataset\n\ndf = pd.read_csv('..\/input\/Car_sales.csv')\ndf.replace('.', np.nan, inplace=True)\ndf = df.dropna()\nprint(df.shape)\ndf.head()","9e217c2f":"# Change 'Passenger' to binary indicators\n# Convert object to float64\n\ndf['Passenger'] = (df['Vehicle type']=='Passenger')\ndf.drop('Vehicle type', inplace=True, axis=1)\n\nfor i in df.columns[2:-2]:\n    df[i] = pd.to_numeric(df[i])","18b188f5":"# Convert 'Latest Launch' to the number of days after 1-Jan-1970\n\ndf['Latest Launch'] = pd.to_datetime(df['Latest Launch'])\ndf['Latest Launch'] = (df['Latest Launch'] - pd.Timestamp(1970, 1, 1)).astype(str)\nfor i in df.index:\n    df.loc[i, 'Latest Launch'] = float(df.loc[i, 'Latest Launch'].split()[0])","affe45f4":"# Convert 'Manufacturer' as binary indicators.\n\nfor i in df['Manufacturer'].unique()[:-1]:\n    df[i] = (df['Manufacturer'] == i)\ndf.head(10)","e5a3a259":"fig = plt.figure(figsize=(10,6))\nax = plt.subplot(111)\nax.set_axisbelow(True)\nplt.hist(df['Sales in thousands'], bins=25, rwidth=0.8)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nplt.tick_params(left=False)\nax.set_xlabel('Sales in thousands', fontsize=16)\nax.set_ylabel('Count', fontsize=16)\nax.set_title('Distribution of Target Variable', fontsize=16)\nplt.grid(axis='y')\nplt.show()","fe59f3c9":"df = df[df['Sales in thousands'] <= 300]\ndf.drop(['4-year resale value', 'Latest Launch'], axis=1, inplace=True)\ndf.shape","800c2f8d":"df.head(10)","07885909":"round(df.iloc[:, 2:].corr(), 4)","7a486604":"X_train, X_test, y_train, y_test = train_test_split(df.drop(['Manufacturer', 'Model', 'Sales in thousands'], axis=1), df['Sales in thousands'], random_state = 0)\nlinreg = LinearRegression().fit(X_train, y_train)\n\nprint('linear model coeff (w): {}'.format(linreg.coef_))\nprint('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\nprint('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))","55e9ba15":"knn = KNeighborsRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(knn.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(knn.score(X_test, y_test)))","46aaa540":"grid_values = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\nkf = KFold(n_splits=5, random_state=13)\ngrid_knn_mse = GridSearchCV(knn, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=kf, iid=False)\ngrid_knn_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_knn_mse.best_params_)\nprint('Grid best score (MSE): ', grid_knn_mse.best_score_)","51e4d422":"knn = grid_knn_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(knn.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(knn.score(X_test, y_test)))","59bc2ca9":"svm = SVR(gamma='scale').fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(svm.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(svm.score(X_test, y_test)))","c0a929a7":"grid_values = {'gamma': [0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10],\\\n               'C': [30, 100, 300, 1000, 3000, 10000, 30000, 100000, 300000, 1000000]}\ngrid_svm_mse = GridSearchCV(svm, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=kf, iid=False)\ngrid_svm_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_svm_mse.best_params_)\nprint('Grid best score (MSE): ', grid_svm_mse.best_score_)","1b7b4dc2":"svm = grid_svm_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(svm.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(svm.score(X_test, y_test)))","ed7a87bc":"dt = DecisionTreeRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(dt.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(dt.score(X_test, y_test)))","60a0e2c2":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(dt.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","be44e306":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_samples_split': [2, 4, 8, 16, 32, 64, 100],\\\n               'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_dt_mse = GridSearchCV(dt, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=kf, iid=False)\ngrid_dt_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_dt_mse.best_params_)\nprint('Grid best score (MSE): ', grid_dt_mse.best_score_)","2035a5ad":"dt = grid_dt_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(dt.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(dt.score(X_test, y_test)))","b9eb6773":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(dt.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","529377de":"rf = RandomForestRegressor(n_estimators=100, random_state=3).fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(rf.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(rf.score(X_test, y_test)))","08756678":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_samples_split': [2, 4, 8, 16, 32, 64, 100],\\\n               'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_rf_mse = GridSearchCV(rf, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=kf, iid=False, return_train_score=False)\ngrid_rf_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_rf_mse.best_params_)\nprint('Grid best score (MSE): ', grid_rf_mse.best_score_)","5e31e722":"cv_results_rf = pd.DataFrame(grid_rf_mse.cv_results_)\ncv_results_rf = cv_results_rf[cv_results_rf['param_max_depth']==grid_rf_mse.best_params_['max_depth']]\nrf_score = np.array(cv_results_rf['mean_test_score']).reshape(8, 7)\nfig = plt.figure(figsize=(10,6))\nax = plt.subplot(111)\nim = ax.imshow(rf_score, cmap='cividis')\ncbar = ax.figure.colorbar(im, ax=ax)\nax.set_xticks(np.arange(7))\nax.set_yticks(np.arange(8))\nax.set_xticklabels(grid_values['min_samples_split'], fontsize=13)\nax.set_yticklabels(grid_values['min_samples_leaf'], fontsize=13)\nfor i in range(7):\n    for j in range(8):\n        text = ax.text(i, j, round(rf_score[j][i]), ha=\"center\", va=\"center\", color=\"w\")\nax.set_xlabel('min_samples_split', fontsize=16)\nax.set_ylabel('min_samples_leaf', fontsize=16)\nax.set_title('Random Forest Grid Search', fontsize=16)\nplt.show()","776eee06":"rf = grid_rf_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(rf.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(rf.score(X_test, y_test)))","2fc453b1":"plt.figure(figsize=(10,6), dpi=80)\nax = plt.subplot(111)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(rf.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nfeature_importance['features'].replace(['Ford         ', 'Honda        ', 'Toyota       '], ['Ford', 'Honda', 'Toyota'], inplace=True)\nbars = plt.barh(feature_importance['features'][:10], feature_importance['importance'][:10])\nplt.tick_params(left=False, bottom=False, labelbottom=False)\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nfor bar in bars:\n    plt.gca().text(bar.get_width() - 0.01, bar.get_y() + bar.get_height()\/3, str(round(bar.get_width(),3)), \n                   ha='center', color='white', fontsize=11)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.xlabel('Random Forest Feature importance (Top 10)', fontsize=16)\nplt.ylabel('Feature name', fontsize=16)\nplt.show()","938ed30d":"#nn = MLPRegressor().fit(X_train, y_train)\n#print('R-squared score (training): {:.3f}'.format(nn.score(X_train, y_train)))\n#print('R-squared score (test): {:.3f}'.format(nn.score(X_test, y_test)))","0bbd31c9":"#grid_values = {'hidden_layer_sizes': np.arange(1, 201),\\\n#               'learning_rate': ['constant', 'invscaling', 'adaptive']}\n#grid_nn_mse = GridSearchCV(nn, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\n#grid_nn_mse.fit(X_train, y_train)\n\n#print('Grid best parameter (min. MSE): ', grid_nn_mse.best_params_)\n#print('Grid best score (MSE): ', grid_nn_mse.best_score_)","5269d2cf":"#nn = MLPRegressor(hidden_layer_sizes=200, learning_rate='invscaling').fit(X_train, y_train)\n#print('R-squared score (training): {:.3f}'.format(nn.score(X_train, y_train)))\n#print('R-squared score (test): {:.3f}'.format(nn.score(X_test, y_test)))","7887f087":"#nn = grid_nn_mse.best_estimator_\n#print('R-squared score (training): {:.3f}'.format(nn.score(X_train, y_train)))\n#print('R-squared score (test): {:.3f}'.format(nn.score(X_test, y_test)))","868b12f6":"xg_reg = xgb.XGBRegressor().fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(xg_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(xg_reg.score(X_test, y_test)))","d4a17d4c":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_child_weight': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_xgb_mse = GridSearchCV(xg_reg, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_xgb_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_xgb_mse.best_params_)\nprint('Grid best score (MSE): ', grid_xgb_mse.best_score_)","8cbeca50":"xg_reg = grid_xgb_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(xg_reg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(xg_reg.score(X_test, y_test)))","4859421d":"gb = GradientBoostingRegressor(random_state=3).fit(X_train, y_train)\nprint('R-squared score (training): {:.3f}'.format(gb.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(gb.score(X_test, y_test)))","1fbb6997":"grid_values = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\\\n               'min_samples_split': [2, 4, 8, 16, 32, 64, 100],\\\n               'min_samples_leaf': [1, 2, 4, 8, 16, 32, 64, 100]}\ngrid_gb_mse = GridSearchCV(gb, param_grid = grid_values, scoring = 'neg_mean_squared_error', cv=5, iid=False)\ngrid_gb_mse.fit(X_train, y_train)\n\nprint('Grid best parameter (min. MSE): ', grid_gb_mse.best_params_)\nprint('Grid best score (MSE): ', grid_gb_mse.best_score_)","e82940e7":"gb = grid_gb_mse.best_estimator_\nprint('R-squared score (training): {:.3f}'.format(gb.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(gb.score(X_test, y_test)))","f6ce5b8b":"plt.figure(figsize=(10,10), dpi=80)\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame(feature_names, columns=['features'])\nfeature_importance['importance'] = pd.DataFrame(gb.feature_importances_)\nfeature_importance.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importance.reset_index(drop=True, inplace=True)\nplt.barh(feature_importance['features'], feature_importance['importance'])\nplt.xlabel('Feature importance')\nplt.ylabel('Feature name')\nplt.show()","419772b3":"stacking = pd.DataFrame(y_train)\nmodels = [linreg, knn, svm, dt, rf, gb, xg_reg]\nfor i in models:\n    stacking[str(i)] = i.predict(X_train)\nstacking.columns = ['Sales in thousands', 'LinReg', 'kNN', 'SVM', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGB']\nstacking.sort_values('Sales in thousands', inplace=True)\nlinreg_s = LinearRegression().fit(stacking.drop('Sales in thousands', axis=1), stacking['Sales in thousands'])\ncoef = linreg_s.coef_\nprint(coef)","3d705128":"prediction = pd.DataFrame(y_test)\nmodels = [linreg, knn, svm, dt, rf, gb, xg_reg]\nfor i in models:\n    prediction[str(i)] = i.predict(X_test)\nprediction.columns = ['Sales in thousands', 'LinReg', 'kNN', 'SVM', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGB']\nprediction['Stacking'] = coef[0]*prediction['LinReg'] + coef[1]*prediction['kNN'] + coef[2]*prediction['SVM']\\\n                       + coef[3]*prediction['Decision Tree'] + coef[4]*prediction['Random Forest']\\\n                       + coef[5]*prediction['Gradient Boosting'] + coef[6]*prediction['XGB']\\\n                       + linreg_s.intercept_\n#prediction.sort_values('Sales in thousands', inplace=True)\nprediction.head()","02a52277":"score = []\nfor i in range(1, 9):\n    score += [((prediction.iloc[:, i] - prediction['Sales in thousands'])**2).mean()]\nbest_model = score.index(min(score))\npd.Series(score, index = ['LinReg', 'kNN', 'SVM', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGB', 'Stacking'])","cf25417d":"fig = plt.figure(figsize=(10, 6))\nplt.plot(np.arange(prediction.shape[0]), prediction['Sales in thousands'], label='True Value', linewidth=3)\nfor i in range(len(prediction.columns[1:])):\n    if i == best_model:\n        plt.plot(np.arange(prediction.shape[0]), prediction.iloc[:, i+1], label=prediction.columns[i+1]+'('+str(round(score[i], 2))+')', linewidth=3)\n    else:\n        plt.plot(np.arange(prediction.shape[0]), prediction.iloc[:, i+1], label=prediction.columns[i+1]+'('+str(round(score[i], 2))+')')\nplt.title('Prediction Results', fontsize=16)\nplt.xticks(fontsize=13)\nplt.xlabel('Instances', fontsize=16)\nplt.yticks(fontsize=13)\nplt.ylabel('Sales in thousands', fontsize=16)\nplt.xlim(0, prediction.shape[0]-1)\nplt.grid()\nplt.legend(loc=2)\nplt.show()","350d305b":"prediction = df.iloc[:, 2:3]\nmodels = [linreg, knn, svm, dt, rf, gb, xg_reg]\nfor i in models:\n    prediction[str(i)] = cross_val_predict(i, df.drop(['Manufacturer', 'Model', 'Sales in thousands'], axis=1), df['Sales in thousands'], cv=5)\nprediction.columns = ['Sales in thousands', 'LinReg', 'kNN', 'SVM', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGB']\nprediction['Stacking'] = np.zeros(prediction.shape[0])\nfor train_index, test_index in kf.split(prediction):\n    linreg_s = LinearRegression().fit(prediction.iloc[train_index, 1:], prediction.iloc[train_index, 0])\n    prediction.iloc[test_index, 8] = linreg_s.predict(prediction.iloc[test_index, 1:])\n#prediction.sort_values('Sales in thousands', inplace=True)\nprediction.head()","d4cdd3f0":"score = []\nfor i in range(1, 9):\n    score += [((prediction.iloc[:, i] - prediction['Sales in thousands'])**2).mean()]\nbest_model = score.index(min(score))\npd.Series(score, index = ['LinReg', 'kNN', 'SVM', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGB', 'Stacking'])","2ea90b6c":"fig = plt.figure(figsize=(10, 6))\nax = plt.subplot(111)\nplt.plot(np.arange(prediction.shape[0]), prediction['Sales in thousands'], label='True Value', linewidth=3)\nfor i in range(len(prediction.columns[1:])):\n    if i != best_model:\n        plt.plot(np.arange(prediction.shape[0]), prediction.iloc[:, i+1], label=prediction.columns[i+1]+'('+str(round(score[i], 2))+')', c='C'+str(i+1))\nfor i in range(len(prediction.columns[1:])):\n    if i == best_model:\n        plt.plot(np.arange(prediction.shape[0]), prediction.iloc[:, i+1], label=prediction.columns[i+1]+'('+str(round(score[i], 2))+')', linewidth=3, c='C'+str(i+1))\nhandles, labels = ax.get_legend_handles_labels()\nhandles = handles[:5] + [handles[-1]] + handles[5:-1]\nplt.legend(handles=handles, loc=9, bbox_to_anchor=(0.5, -0.15), ncol=3)\nplt.title('Prediction Results', fontsize=16)\nplt.xticks(fontsize=13)\nplt.xlabel('Instances', fontsize=16)\nplt.yticks(fontsize=13)\nplt.ylabel('Sales in thousands', fontsize=16)\nplt.xlim(0, prediction.shape[0]-1)\nplt.grid()\n#plt.legend(loc=2)\nplt.show()","5ec7da6c":"df[df['Sales in thousands']>=200].iloc[:, :15]","ec0e54cc":"**Stacking (Experiment)**","2072d50a":"**Decision Tree Regression**","50535973":"---\n\nHere is the prediction results on our test set.","77efe4db":"**sklearn Gradient Boosting Regression**","9cdd55cc":"**kNN Regression**","f952df78":"Baseline Model **LinearRegression**","f862ff3e":"It seems that we have a bad prediction on cars with sales more than 200 thousands. What are they?","5e4becbb":"**Neural Network Regression**","acc78671":"Here is the score on the entire set.\n\n\n(Given by sklearn cross_val_predict)","03fe017f":"**Random Forest Regression**","a00e44b3":"**Support Vector Machine Regression**","70519727":"**XGB Gradient Boosting Regression**"}}