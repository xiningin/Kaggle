{"cell_type":{"533c1aa9":"code","155a2e6a":"code","1ebfed19":"code","cbe15c59":"code","2942dbc7":"code","89cbb1e3":"code","9b638981":"code","d5f81a5d":"code","4b4ba91e":"code","9295e94b":"code","44574c27":"code","64d3a6d0":"code","6683bd6d":"code","0e6465ad":"code","2d9474ba":"code","9c8ffb4a":"code","537cb772":"code","40e30452":"code","ada615b4":"code","fb52e5d5":"code","2c842676":"code","b0c01792":"code","5b54cd65":"code","d2f2345d":"code","34720432":"code","4c5de839":"code","e82c2830":"code","9113914c":"code","7fd576a5":"code","c5659ebf":"markdown","05e2c3b7":"markdown","02f5dc67":"markdown","dc2258f6":"markdown","e9aa6bbc":"markdown","e32b2a3c":"markdown","19dee4e1":"markdown","97c9c143":"markdown","1cb5913c":"markdown","9ed51c1d":"markdown","c31959e0":"markdown","02825aba":"markdown","d731c49c":"markdown","c23229fd":"markdown","b93d3fba":"markdown","586c3fa6":"markdown","cb6b9163":"markdown","b0c9c7ed":"markdown"},"source":{"533c1aa9":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","155a2e6a":"PATH = '.\/'\nTRAIN = '..\/input\/train\/'\nTEST = '..\/input\/test\/'\nLABELS = '..\/input\/train.csv'\nSAMPLE = '..\/input\/sample_submission.csv'","1ebfed19":"name_label_dict = {\n0:  'Nucleoplasm',\n1:  'Nuclear membrane',\n2:  'Nucleoli',   \n3:  'Nucleoli fibrillar center',\n4:  'Nuclear speckles',\n5:  'Nuclear bodies',\n6:  'Endoplasmic reticulum',   \n7:  'Golgi apparatus',\n8:  'Peroxisomes',\n9:  'Endosomes',\n10:  'Lysosomes',\n11:  'Intermediate filaments',\n12:  'Actin filaments',\n13:  'Focal adhesion sites',   \n14:  'Microtubules',\n15:  'Microtubule ends',  \n16:  'Cytokinetic bridge',   \n17:  'Mitotic spindle',\n18:  'Microtubule organizing center',  \n19:  'Centrosome',\n20:  'Lipid droplets',\n21:  'Plasma membrane',   \n22:  'Cell junctions', \n23:  'Mitochondria',\n24:  'Aggresome',\n25:  'Cytosol',\n26:  'Cytoplasmic bodies',   \n27:  'Rods & rings' }","cbe15c59":"nw = 2   #number of workers for data loader\narch = resnet34 #specify target architecture","2942dbc7":"train_names = list({f[:36] for f in os.listdir(TRAIN)})\ntest_names = list({f[:36] for f in os.listdir(TEST)})\ntr_n, val_n = train_test_split(train_names, test_size=0.1, random_state=42)","89cbb1e3":"def open_rgby(path,id): #a function that reads RGBY image\n    colors = ['red','green','blue','yellow']\n    flags = cv2.IMREAD_GRAYSCALE\n    img = [cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags).astype(np.float32)\/255\n           for color in colors]\n    return np.stack(img, axis=-1)","9b638981":"class pdFilesDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.labels = pd.read_csv(LABELS).set_index('Id')\n        self.labels['Target'] = [[int(i) for i in s.split()] for s in self.labels['Target']]\n        super().__init__(fnames, transform, path)\n    \n    def get_x(self, i):\n        img = open_rgby(self.path,self.fnames[i])\n        if self.sz == 512: return img \n        else: return cv2.resize(img, (self.sz, self.sz),cv2.INTER_AREA)\n    \n    def get_y(self, i):\n        if(self.path == TEST): return np.zeros(len(name_label_dict),dtype=np.int)\n        else:\n            labels = self.labels.loc[self.fnames[i]]['Target']\n            return np.eye(len(name_label_dict),dtype=np.float)[labels].sum(axis=0)\n        \n    @property\n    def is_multi(self): return True\n    @property\n    def is_reg(self):return True\n    #this flag is set to remove the output sigmoid that allows log(sigmoid) optimization\n    #of the numerical stability of the loss function\n    \n    def get_c(self): return len(name_label_dict) #number of classes","d5f81a5d":"def get_data(sz,bs):\n    #data augmentation\n    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n                RandomDihedral(tfm_y=TfmType.NO)]\n    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n                aug_tfms=aug_tfms)\n    ds = ImageData.get_ds(pdFilesDataset, (tr_n[:-(len(tr_n)%bs)],TRAIN), \n                (val_n,TRAIN), tfms, test=(test_names,TEST))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    return md","4b4ba91e":"bs = 16\nsz = 256\nmd = get_data(sz,bs)\n\nx,y = next(iter(md.trn_dl))\nx.shape, y.shape","9295e94b":"def display_imgs(x):\n    columns = 4\n    bs = x.shape[0]\n    rows = min((bs+3)\/\/4,4)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        for j in range(columns):\n            idx = i+j*columns\n            fig.add_subplot(rows, columns, idx+1)\n            plt.axis('off')\n            plt.imshow((x[idx,:,:,:3]*255).astype(np.int))\n    plt.show()\n    \ndisplay_imgs(np.asarray(md.trn_ds.denorm(x)))","44574c27":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        \n        return loss.sum(dim=1).mean()","64d3a6d0":"def acc(preds,targs,thresh=0.0):\n    preds = (preds > thresh).int()\n    targs = targs.int()\n    return (preds==targs).float().mean()","6683bd6d":"class ConvnetBuilder_custom():\n    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n                 custom_head=None, pretrained=True):\n        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n        if xtra_fc is None: xtra_fc = [512]\n        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n        self.ps,self.xtra_fc = ps,xtra_fc\n\n        if f in model_meta: cut,self.lr_cut = model_meta[f]\n        else: cut,self.lr_cut = 0,0\n        cut-=xtra_cut\n        layers = cut_model(f(pretrained), cut)\n        \n        #replace first convolutional layer by 4->64 while keeping corresponding weights\n        #and initializing new weights with zeros\n        w = layers[0].weight\n        layers[0] = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n        layers[0].weight = torch.nn.Parameter(torch.cat((w,torch.zeros(64,1,7,7)),dim=1))\n        \n        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n        self.top_model = nn.Sequential(*layers)\n\n        n_fc = len(self.xtra_fc)+1\n        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n\n        if custom_head: fc_layers = [custom_head]\n        else: fc_layers = self.get_fc_layers()\n        self.n_fc = len(fc_layers)\n        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n\n    @property\n    def name(self): return f'{self.f.__name__}_{self.xtra_cut}'\n\n    def create_fc_layer(self, ni, nf, p, actn=None):\n        res=[nn.BatchNorm1d(num_features=ni)]\n        if p: res.append(nn.Dropout(p=p))\n        res.append(nn.Linear(in_features=ni, out_features=nf))\n        if actn: res.append(actn)\n        return res\n\n    def get_fc_layers(self):\n        res=[]\n        ni=self.nf\n        for i,nf in enumerate(self.xtra_fc):\n            res += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n            ni=nf\n        final_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()\n        if self.is_reg: final_actn = None\n        res += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n        return res\n\n    def get_layer_groups(self, do_fc=False):\n        if do_fc:\n            return [self.fc_model]\n        idxs = [self.lr_cut]\n        c = children(self.top_model)\n        if len(c)==3: c = children(c[0])+c[1:]\n        lgs = list(split_by_idxs(c,idxs))\n        return lgs+[self.fc_model]\n    \nclass ConvLearner(Learner):\n    def __init__(self, data, models, precompute=False, **kwargs):\n        self.precompute = False\n        super().__init__(data, models, **kwargs)\n        if hasattr(data, 'is_multi') and not data.is_reg and self.metrics is None:\n            self.metrics = [accuracy_thresh(0.5)] if self.data.is_multi else [accuracy]\n        if precompute: self.save_fc1()\n        self.freeze()\n        self.precompute = precompute\n\n    def _get_crit(self, data):\n        if not hasattr(data, 'is_multi'): return super()._get_crit(data)\n\n        return F.l1_loss if data.is_reg else F.binary_cross_entropy if data.is_multi else F.nll_loss\n\n    @classmethod\n    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n                   pretrained=True, **kwargs):\n        models = ConvnetBuilder_custom(f, data.c, data.is_multi, data.is_reg,\n            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n        return cls(data, models, precompute, **kwargs)\n\n    @classmethod\n    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n        models = ConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n        convlearn=cls(data, models, precompute, **kwargs)\n        convlearn.lsuv_init()\n        return convlearn\n    \n    @property\n    def model(self): return self.models.fc_model if self.precompute else self.models.model\n    \n    def half(self):\n        if self.fp16: return\n        self.fp16 = True\n        if type(self.model) != FP16: self.models.model = FP16(self.model)\n        if not isinstance(self.models.fc_model, FP16): self.models.fc_model = FP16(self.models.fc_model)\n    def float(self):\n        if not self.fp16: return\n        self.fp16 = False\n        if type(self.models.model) == FP16: self.models.model = self.model.module.float()\n        if type(self.models.fc_model) == FP16: self.models.fc_model = self.models.fc_model.module.float()\n\n    @property\n    def data(self): return self.fc_data if self.precompute else self.data_\n\n    def create_empty_bcolz(self, n, name):\n        return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)\n\n    def set_data(self, data, precompute=False):\n        super().set_data(data)\n        if precompute:\n            self.unfreeze()\n            self.save_fc1()\n            self.freeze()\n            self.precompute = True\n        else:\n            self.freeze()\n\n    def get_layer_groups(self):\n        return self.models.get_layer_groups(self.precompute)\n\n    def summary(self):\n        precompute = self.precompute\n        self.precompute = False\n        res = super().summary()\n        self.precompute = precompute\n        return res\n\n    def get_activations(self, force=False):\n        tmpl = f'_{self.models.name}_{self.data.sz}.bc'\n        # TODO: Somehow check that directory names haven't changed (e.g. added test set)\n        names = [os.path.join(self.tmp_path, p+tmpl) for p in ('x_act', 'x_act_val', 'x_act_test')]\n        if os.path.exists(names[0]) and not force:\n            self.activations = [bcolz.open(p) for p in names]\n        else:\n            self.activations = [self.create_empty_bcolz(self.models.nf,n) for n in names]\n\n    def save_fc1(self):\n        self.get_activations()\n        act, val_act, test_act = self.activations\n        m=self.models.top_model\n        if len(self.activations[0])!=len(self.data.trn_ds):\n            predict_to_bcolz(m, self.data.fix_dl, act)\n        if len(self.activations[1])!=len(self.data.val_ds):\n            predict_to_bcolz(m, self.data.val_dl, val_act)\n        if self.data.test_dl and (len(self.activations[2])!=len(self.data.test_ds)):\n            if self.data.test_dl: predict_to_bcolz(m, self.data.test_dl, test_act)\n\n        self.fc_data = ImageClassifierData.from_arrays(self.data.path,\n                (act, self.data.trn_y), (val_act, self.data.val_y), self.data.bs, classes=self.data.classes,\n                test = test_act if self.data.test_dl else None, num_workers=8)\n\n    def freeze(self):\n        self.freeze_to(-1)\n\n    def unfreeze(self):\n        self.freeze_to(0)\n        self.precompute = False\n\n    def predict_array(self, arr):\n        precompute = self.precompute\n        self.precompute = False\n        pred = super().predict_array(arr)\n        self.precompute = precompute\n        return pred","0e6465ad":"sz = 256 #image size\nbs = 64  #batch size\n\nmd = get_data(sz,bs)\nlearner = ConvLearner.pretrained(arch, md, ps=0.5) #dropout 50%\nlearner.opt_fn = optim.Adam\nlearner.crit = FocalLoss()\nlearner.metrics = [acc]","2d9474ba":"learner.summary","9c8ffb4a":"learner.lr_find()\nlearner.sched.plot()","537cb772":"lr = 2e-2\nlearner.fit(lr,1)","40e30452":"learner.unfreeze()\nlrs=np.array([lr\/10,lr\/3,lr])","ada615b4":"learner.fit(lrs\/4,4,cycle_len=2,use_clr=(10,20))","fb52e5d5":"learner.fit(lrs\/4,2,cycle_len=4,use_clr=(10,20))","2c842676":"learner.sched.plot_lr()","b0c01792":"learner.fit(lrs\/8,1,cycle_len=8,use_clr=(5,20))","5b54cd65":"learner.save('ResNet34_256_1')","d2f2345d":"def sigmoid_np(x):\n    return 1.0\/(1.0 + np.exp(-x))\n\npreds,y = learner.TTA()\npreds = np.stack(preds, axis=-1)\npreds = sigmoid_np(preds)\npred = preds.max(axis=-1)","34720432":"th = np.array([0.50,0.56,0.50,0.50,0.50,0.47,0.39,0.56,0.36,0.45,\n               0.41,0.48,0.45,0.44,0.63,0.50,0.38,0.33,0.45,0.41,\n               0.33,0.49,0.43,0.52,0.51,0.48,0.43,0.1])\n\nprint(f1_score(y, pred>0.5, average='macro'),f1_score(y, pred>th, average='macro'))","4c5de839":"preds_t,y_t = learner.TTA(is_test=True)\npreds_t = np.stack(preds_t, axis=-1)\npreds_t = sigmoid_np(preds_t)\npred_t = preds_t.max(axis=-1)","e82c2830":"th_t = np.array([0.565,0.39,0.55,0.345,0.33,0.39,0.33,0.45,0.38,0.39,\n               0.34,0.42,0.31,0.38,0.49,0.50,0.38,0.43,0.46,0.40,\n               0.39,0.505,0.37,0.47,0.41,0.545,0.32,0.1])\n(pred_t > th_t).mean(axis=0)","9113914c":"pred_list = []\nfor line in pred_t:\n    s = ' '.join(list([str(i) for i in np.nonzero(line>th_t)[0]]))\n    pred_list.append(s)","7fd576a5":"sample_df = pd.read_csv(SAMPLE)\nsample_list = list(sample_df.Id)\npred_dic = dict((key, value) for (key, value) \n                in zip(learner.data.test_ds.fnames,pred_list))\npred_list_cor = [pred_dic[id] for id in sample_list]\ndf = pd.DataFrame({'Id':sample_list,'Predicted':pred_list_cor})\ndf.to_csv('protein_classification.csv', header=True, index=False)","c5659ebf":"### Validation score","05e2c3b7":"First, I train only the head of the model while keeping the rest frozen. It allows to avoid corruption of the pretrained weights at the initial stage of training due to random initialization of the head layers. So the power of transfer learning is fully utilized when the training is continued.","02f5dc67":"Since a multiclass multilabel task is considered, there are several things about the model that should be pointed out. First, the SOFTMAX MUST NOT BE USED as an output layer because it encourages a single label prediction. The common output function for multilabel tasks is sigmoid. However, combining the sigmoid with the loss function (like in BCE with logits loss or in Focal loss used in this kernel) allows log(sigmoid) optimization of the numerical stability of the loss function. Therefore, sigmoid is also removed.","dc2258f6":"Save the model for further use or training on higher resolution images.","e9aa6bbc":"Evaluate the score with using 4-fold TTA (test time augmentation).","e32b2a3c":"Next, I unfreeze all weights and allow training of entire model. One trick that I use is differential learning rate: the lr of the head part is still lr, while the middle layers of the model are trained with lr\/3, and the base is trained with even smaller lr\/10. Despite the low-level detectors do not vary much from one image data set to another much, the yellow channel should be trained, and also the images are quite different from ImageNet; therefore, the I decrease the learning rate for first layers only by 10 times. If there was no necessity to train an additional channel and the images were more similar to ImageNet, the learning rates could be [lr\/100,lr\/10,lr]. Another trick is learning rate annealing. Periodic lr increase followed by slow decrease drives the system out of steep minima (when lr is high) towards broader ones (which are explored when lr decreases) that enhances the ability of the model to generalize and reduces overfitting. The length of the cycles gradually increases during training.","19dee4e1":"### Overview\nThe goal of this competition is classification of mixed protein patterns. However, unlike most image labeling tasks, where binary or multiclass labeling is considered, in this competition each image can have multiple labels. Multiclass multilabel task has its own specific affecting the design of the model and the loss function. Moreover, the classified images are quite different from ImageNet; therefore, despite usage of a pretrained model is quite helpful, a substantial retraining of entire model is needed. An additional challenge is 4-chanel input to the model (RGBY), which is different from ones used in most of pretrained models (RGB input).\n\nIn this kernel I will show how to handle the above challenges and get started with this competition. I will begin with using a light ResNet34 model and low-resolution images to have a baseline that can be used later to select higher end models and explore the effect of image resolution on the prediction accuracy. **The validation F1 score of the model is ~0.5-0.6**, and I was able to get 0.437 public LB score by using a model produced in an earlier version of the kernel after I did adjustment of thresholds according to https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification\/discussion\/68678#404591. The problem of low public LB score of the model, which mentioned in the previous version of the kernel, is resulted by a bug in the evaluation metric (https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification\/discussion\/69366#409041) that relies on the order of records in the submission file rather than IDs.","97c9c143":"### Training","1cb5913c":"One of the challenges in this competition is 4-chanel input (RGBY) that limits usage of ImageNet pretrained models taking RGB input. However, the input dataset is too tiny to train even a low capacity model like ResNet34 from scratch. I propose to use the following way to walk around this limitation. The most common way to convert RGBY to RGB is just dropping Y channel while keeping RGB without modification. Therefore, I replace the first convolution layer from 7x7 3->64 to 7x7 **4->64** while keeping weights from 3->64 and setting new initial weighs for Y channel to be zero. It allows using the original weights to initialize the network while giving the opportunity to the model to incorporate Y channel into prediction during the following training (when the first layers of the model are unfreezed). In the following hiden cell I put a code from fast.ai library with adding several lines for the replacment of the first convolutional layer.","9ed51c1d":"**It is very important to keep the same order of ids as in the sample submission** https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification\/discussion\/69366#409041. The competition metric relies only on the order of recods ignoring IDs.","c31959e0":"Similar to validation, additional adjustment may be done based on public LB probing results (https:\/\/www.kaggle.com\/c\/human-protein-atlas-image-classification\/discussion\/68678) to predict approximately the same fraction of images of a particular class as expected from the public LB.","02825aba":"### Loss function and metrics","d731c49c":"I begin with finding the optimal learning rate. The following function runs training with different lr and records the loss. Increase of the loss indicates onset of divergence of training. The optimal lr lies in the vicinity of the minimum of the curve but before the onset of divergence. Based on the following plot, for the current setup the divergence starts at ~0.2, and the recommended learning rate is ~0.02.","c23229fd":"Plot several examples of input images.","b93d3fba":"One of the challenges of this competition is strong data imbalance. Some classes, like \"Nucleoplasm\", are very common, while there is a number of rare classes, like \"Endosomes\", \"Lysosomes\", and \"Rods & rings\". In addition, in tasks of multiclass, and especially in multilabel, classification there is always an issue with data imbalance: if you predict 1 class out of 10, given the same number of examples per each class, you have 1 positive vs. 9 negative examples. So, it is crucial to use a loss function that accounts for it. Recently proposed focal loss (https:\/\/arxiv.org\/pdf\/1708.02002.pdf) has revolutionized one stage object localization method in 2017. It is design to address the issue of strong data imbalance, demonstrating amazing results on datasets with imbalance level 1:10-1000. In particular, it works quite well for image segmentation task in \"Airbus Ship Detection Challenge\": https:\/\/www.kaggle.com\/iafoss\/unet34-dice-0-87 .  The implementation of focal loss is borrowed from https:\/\/becominghuman.ai\/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c .","586c3fa6":"### Submission","cb6b9163":"Instead of 0.5, one can adjust the values of the threshold for each class individually to boost the score. However, it should be done for each model individually.","b0c9c7ed":"### Data"}}